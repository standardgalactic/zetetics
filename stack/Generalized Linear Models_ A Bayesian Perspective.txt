
GENERALIZED 
LINEAR MODELS 
A BAYESIAN PERSPECTIVE 
edited by 
Dipak K. Dey 
The University of Connecticut 
Storrs, Connecticut 
SuJit K. Ghosh 
North Carolina State University 
Raleigh, North Carolina 
Bani K. Mallick 
Texas A&M University 
College Station, Texas 
B MARCEL DEKKER, INC. 
NEw YoRK • BASEL 
I!KKI!R 

Library of Congress Cataloging-in-Publication Data 
Dey, Dipak. 
Generalized linear models : a Bayesian perspective I Dipak K. Dey, Sujit K. 
Ghosh, Bani K. Mallick. 
p. em. -
(Biostatistics ; 5) 
Includes index. 
ISBN 0-824 7-9034-0 (alk. paper) 
!.Linear models (Statistics) 2. Bayesian statistical decision theory. I. Ghosh, 
Sujit K., 1970- II. Mallick, Bani K., 1965- III. Title. IV. Biostatistics (New York, 
N.Y.);5. 
QA276 .D485 2000 
519.5'35-dc21 
This book is printed on acid-free paper. 
Headquarters 
Marcel Dekker, Inc. 
270 Madison Avenue, New York, NY 10016 
tel: 212-696-9000; fax: 212-685-4540 
Eastern Hemisphere Distribution 
Marcel Dekker AG 
Hutgasse 4, Postfach 812, CH-4001 Basel, Switzerland 
tel: 41-61-261-8482; fax: 41-61-261-8896 
World Wide Web 
http://www.dekker.com 
00-024055 
The publisher offers discounts on this book when ordered in bulk quantities. For more 
information, write to Special Sales/Professional Marketing at the headquarters address 
above. 
Copyright © 2000 by Marcel Dekker, Inc. All Rights Reserved. 
Neither this book nor any part may be reproduced or transmitted in any form or by any 
means, electronic or mechanical, including photocopying, microfilming, and recording, or 
by any information storage and retrieval system, without permission in writing from the 
publisher. 
Current printing (last digit) 
10 9 8 7 6 5 4 3 2 1 
PRINTED IN THE UNITED STATES OF AMERICA 

Series Introduction 
The primary objectives of the Biostatistics series are to provide useful reference 
books for researchers and scientists in academia, industry, and government, and 
also to offer textbooks for undergraduate and/or graduate courses in the area of 
biostatistics. This series will provide comprehensive and unified presentations of 
statistical designs, analyses, and interpretations of important applications in bio-
statistics, such as those in biopharmaceuticals. A well-balanced summary will be 
given of current and recently developed statistical methods and interpretations for 
both biostatisticians and researchers/scientists with minimal statistical knowledge 
who are engaged in applied biostatistics. The series is committed to providing easy-
to-understand state-of-the-art references and textbooks. In each volume, statistical 
concepts and methodologies will be illustrated through real examples. 
Generalized linear models (GLMs) have been frequently used in pharmaceuti-
cal research and development, especially in clinical research and development for 
demonstration of the safety and efficacy of a pharmaceutical compound under in-
vestigation. However, the concept for the analysis of GLMs with mixed effects for 
categorical and/or longitudinal data is often misused or misinterpreted due to its 
complexity. This volume provides a comprehensive overview of key statistical con-
cepts and methodologies including the Bayesian approach for analysis of GLMs 
including logistic regression and log-linear models from both a theoretical and a 
practical point of view. In addition, it includes important issues related to model 
diagnostics and variable selection in GLMs. 
This volume serves as an intersection for biostatisticians, practitioners, and re-
searchers/scientists by providing a good understanding of key statistical concepts 
and methodologies for analysis and interpretation of GLMs and GLMs with mixed 
effects as well. This volume is in compliance with good statistics practice (GSP) 
standards for good clinical practice (GCP) as required by most regulatory agencies 
for pharmaceutical research and development. 
Shein-Chung Chow 

This Page Intentionally Left Blank 

Preface 
Generalized Linear Models (GLMs) are widely used as flexible models in which 
a function of the mean response is "linked" to covariates through a linear pre-
dictor and in which variability is described by a distribution in an exponential . 
dispersion family. These models include logistic regression and log-linear models 
for binomial and Poisson counts as well as normal, gamma and inverse-Gaussian 
models for continuous responses. Standard techniques for analyzing censored sur-
vival data such as Cox regression can also be handled within the GLM framework. 
Other topics closely related to GLMs include conditionally independent hierarchical 
models, graphical models, generalized linear mixed models (GLMMs) for estimating 
subject-specific effects, semi-parametric smoothing methods, pharmokinetic models 
and spatio-temporal models. 
GLMs thus provide a versatile statistical modeling framework for medical and 
industrial applications, but questions remain about how the power of these models 
can be safely exploited when training data are limited. This volume demonstrates 
how Bayesian methodology allows complex models (ranging from simple logistic 
regression models to semi-parametric survival models for censored data) to be used 
without fear of the "over-fitting" that can occur with traditional GLM methods 
which are usually based on normal approximation theory. Insight into the nature 
of these complex Bayesian models is provided by theoretical investigations and 
practical implementations. Presupposing only basic knowledge of probability and 
statistics, this volume should be of interest to researchers in statistics, engineering 
and medicine. 
This volume will serve as a comprehensive reference book for practitioners and 
researchers. Each part in the volume has chapters written by an expert in that 
particular topic, and the chapters are carefully edited to ensure that a uniform 
style of notation and presentation is used throughout. As a result, all researchers 
whose work uses GLM theory will find this an indispensable companion to their 
work and it will be the reference volume for this subject for many years to come. 
In particular, each chapter describes how to conceptualize, perform and criticize 
traditional GLMs from a Bayesian perspective. In addition, how to use modern 
computational methods to summarize inferences using simulation is elucidated. 
The primary users of this volume include professionals in statistics and other re-
lated disciplines who work in the pharmaceutical industry, medical centers (includ-
ing public health and epidemiology) and public and private research and academic 
institutions. 
Our hope is that this volume will also help researchers identify areas of impor-
tant future research and open new applications of generalized linear models using 
Bayesian approaches. 
The papers in this volume are divided into six parts: General overview, extension 
of the GLMs, categorical and longitudinal data, semiparametric and nonparametric 
approaches, model diagnostics and variable selection and challenging problems. 
In part I, Gelfand and Ghosh introduce Bayesian analysis of generalized linear 
models from its developments. Sun, Speckman and Tsutakawa describe random 
effects in generalized linear mixed model with fully explained examples. Ibrahim 
and Chen develop methods of prior elicitation and variable selection for generalized 
v 

vi 
Preface 
linear mixed models with an example of pediatric pain data. 
Chapters in Part II of the volume describe several extensions of GLMs. Ferreira 
and Gamerman introduce dynamic modeling approach for GLMs. They also lay out 
computational steps with two applications. Dey and Ravishanker extend GLMs in 
the presence of overdispersion. Both parametric and nonparametric approaches to 
overdispersed GLMs are considered. Nandram proposes Bayesian GLMs for infer-
ence about small areas and describes an application with mortality data of U.S.A. 
Part III concerns modeling categorical and longitudinal data. Modeling dichoto-
mous, polychotomous and count data are quite useful and challenging in the pres-
ence of correlation. In this part, first Chib describes methods for the analysis of 
correlated binary data using latent variables. He also describes three algorithms for 
implementation. Chen and Dey extend this to correlated ordinal data and propose 
algorithms for analysis of such data. Bayesian methods for time series count data are 
described by Ibrahim and Chen with an application to the analysis of pollen count. 
Albert and Ghosh propose and analyze item response modeling for categorical data. 
This part concludes with a case study using Bayesian probit and logit models by 
Landrum and Normand. 
Part IV describes G LMs using rich classes of non parametric and semi parametric 
· approaches. Semiparametric GLMs are considered by Mallick, Denison and Smith 
using Bayesian approaches. The chapter by Basu and Mukhopadhyay presents a 
semiparametric method to model link functions for the binary response data. Next, 
Haro-L6pez, Mallick and Smith develop a data adaptive robust link function. In the -last 
chapter of Part IV, Kuo and Peng present a mixture-model approach to the analysis 
of survival data. 
Part V deals with important issues relating to model diagnostics and variable 
selection in GLMs. In this part, the chapter by Dellaportas, Forster and Ntzoufras 
presents Bayesian variable selection in using Gibbs sampler. Next, Ibrahim and 
Chen describe variable selection methods for Cox models. This part is concluded 
by Dey and Chen on Bayesian model diagnostics for correlated binary data. 
Part VI concludes the volume with challenging problems. Wakefield and Stephens 
develop a case study by incorporating errors-in-variable modeling. Iyengar and Dey 
review parametric and semiparametric approaches for the analysis of compositional 
data. Denison and Mallick describe classification trees from a Bayesian perspective 
and apply the algorithm on a case study problem. In the next chapter, Gelfand, 
Ravishanker and Ecker develop a new modeling and inference method for point-
referenced binary spatial data. The part closes with the chapter by Best and Thomas 
on graphical models and software for GLMs. 
The cooperation of all contributors in the timely preparation of their manuscripts 
is greatly appreciated. We decided early on that it was important to referee and 
critically evaluate the papers which were submitted for inclusion in this volume. For 
this substantial task, we relied on the service of numerous referees to whom we are 
most indebted. Among those whom we wish to acknowledge are Sudipto Banerjee, 
Pabak M ukerjee and Kaushik Patra. 
Finally we thank the editors at Marcel Dekker, Inc. for considering our proposal. 
Our special thanks go to Debosri, Swagata and Mou for their encouragements in 
this project . 
Dipak K. Dey, 
Sujit K. Ghosh 
and 
Bani K. Mallick 
Storrs, CT, USA 
Raleigh, NC, USA 
College Station, TX, USA 

Contents 
I 
General Overview 
1 
1 Generalized Linear Models: A Bayesian View 
3 
A. Gelfand & M. Ghosh 
1. 
Introduction . . . . . . . . . 
3 
2. 
GLMs and Bayesian Models 
4 
2.1 
GLMs . . . . . . 
4 
2.2 
Bayesian Models 
5 
3. 
Propriety of Posteriors . 
8 
4. 
Semiparametric GLMs . 
10 
5. 
Overdispersed Generalized Linear Models 
12 
6. 
Model Determination Approaches . . . . . 
14 
2 Random Effects in Generalized Linear Mixed Models (GLMMs) 
23 
D. Sun, P. L. Speckman & R. /(. Tsutakawa 
1. 
Introduction . . . 
23 
2. 
The Model 
. . . . . . . . . . . . . . 
24 
3. 
Random Effects . . . . . . . . . . . . 
26 
3.1 
Independent Random Effects 
26 
3.2 
Correlated Random Effects . 
26 
3.3 
Strongly Correlated Random Effects 
29 
3.4 
Some Examples of the AR(d) Model 
31 
4. 
Hierarchical GLMMs . . 
31 
5. 
Bayesian Computation . . . . . . . . . . . . 
36 
3 Prior Elicitation and Variable Selection for Generalized Linear 
Mixed Models 
41 
J. Ibrahim & M. H. Chen 
1. 
Introduction ............ . 
2. 
Generalized Linear Mixed Models . 
2.1 
Models .......... . 
2.2 
The Prior Distributions .. 
2.3 
Propriety of the Prior Distribution 
2.4 
Specifying the Hyperparameters . 
2.5 
The Posterior Distribution and its Computation 
3. 
Bayesian Variable Selection 
4. 
Pediatric Pain Data 
5. 
Discussion ......... . 
II Extending the GLMs 
4 Dynamic Generalized Linear Models 
M.A. R. Ferreira & D. Gamerman 
41 
43 
43 
44 
46 
47 
48 
48 
51 
52 
55 
57 
vii 

viii 
Contents 
1. 
2. 
3. 
4. 
5. 
6. 
Introduction . 
Dynamic linear models 
. . . . . . . . . . . 
Definition and first approaches to inference 
3.1 
Linear Bayes Approach ..... 
3.2 
Piecewise Linear Approximation 
3.3 
Posterior Mode Estimation . . 
3.4 
Other Approaches and Models 
MCMC-based Approaches ..... . 
4.1 
Gibbs Sampling ....... . 
4.2 
Metropolis-Hasting Algorithm 
Applications 
. . . . . . . . . . . . . . 
5.1 
Application 1: Meningococcic Meningitis 
5.2 
Application 2: Respiratory Diseases and Level of Pollutants 
Discussions and Extensions ..................... . 
57 
58 
59 
60 
61 
61 
62 
62 
63 
64 
65 
66 
68 
70 
5 Bayesian Approaches for Overdispersion in Generalized Linear Mod-
els 
73 
D. K. Dey & N. Ravishanker 
1. 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . 
73 
2. 
Classes of Overdispersed General Linear Models. . . . 
75 
3. 
Fitting OGLM in the Parametric Bayesian Framework 
78 
3.1 
Model Fitting . . . . . . . . . . . . . . . . . . . 
78 
3.2 
Example: Overdispersed Poisson Regression Model 
79 
3.3 
Model Determination for Parametric OGLM's. . . 
80 
4. 
Modeling Overdispersion in the Nonparametric Bayesian Framework 
81 
4.1 
Fitting DP Mixed GLM and OGLM . . . . . . . . . . . . 
81 
4.2 
Example: Overdispersed Binomial Regression Model . . . 
83 
4.3 
Model Determination for Dirichlet Process Mixed Models 
83 
5. 
Overdispersion in Multistage GLM . . . . . . . . . . . . . . . . . 
84 
6 Bayesian Generalized Linear Models for Inference About Small 
Areas 
89 
B. Nandram 
1. 
Introduction . ........ 
89 
2. 
Logistic Regression Models 
91 
3. 
Poisson Regression Models . 
94 
4. 
Computational Issues ... 
96 
5. 
Models for the U.S. Mortality Data .. 
100 
6. 
Challenges in Small Area Estimation 
102 
7. 
Concluding Remarks . . . . . . . . . 
104 
III Categorical and Longitudinal Data 
111 
7 Bayesian Methods for Correlated Binary Data 
113 
S. Chib 
1. 
Introduction . . . . . . . . . . . 
113 
2. 
The Multivariate Probit Model 
114 
2.1 
Dependence Structures . 
116 
2.2 
Student-t Specification . 
116 
2.3 
Estimation of the MVP Model 
117 

2.4 
Fitting of the Multivariate t-link Model 
3. 
Longitudinal Binary Data ...... . 
3.1 
Probit (or logit) Normal Model .... . 
3.2 
Inference ................ . 
3.3 
Computations for the Probit-Normal Model 
3.4 
Binary Response Hierarchical Model 
3.5 
Other Models ....... . 
4. 
Comparison of Alternative Models 
4.1 
Likelihood Ordinate 
4.2 
Posterior Ordinate 
5. 
Concluding Remarks 
6. 
Appendix ..... . 
6.1 
Algorithm 1 . 
6.2 
Algorithm 2 . 
6.3 
Algorithm 3 . 
8 Bayesian Analysis for Correlated Ordinal Data Models 
M. H. Chen & D. K. Dey 
1. 
Introduction . . . . . . . . . . . . . . . . . . . . . 
2. 
Models ....................... . 
3. 
4. 
5. 
6. 
Prior Distributions and Posterior Computations . 
3.1 
Prior Distributions ... 
3.2 
Posterior Computations 
Model Determination . . . . 
4.1 
Model Comparisons . . 
4.2 
Model Diagnostics ... 
Item Response Data Example . 
Concluding Remarks ..... . 
9 Bayesian Methods for Time Series Count Data 
J. Ibrahim & M. H. Chen 
1. 
2. 
3. 
4. 
5. 
Introduction ........... . 
The Method ........... . 
2.1 
The Likelihood Function . 
2.2 
The Prior Distributions . 
2.3 
Specifying the Hyperparameters 
2.4 
Prior Distribution on the Model Space 
Computation of Model Probabilities 
Example: Pollen Data 
Discussion . . . . . . . . 
10 Item Response Modeling 
J. Albert & M. Ghosh 
1. 
2. 
3. 
4. 
Introduction . . . . . . . 
An Item Response Curve . . . . . . . . . . . . . 
Administering an Exam to a Group of Students . 
Prior Distributions ................ . 
Contents 
ix 
118 
119 
119 
120 
120 
122 
123 
124 
124 
125 
127 
127 
127 
128 
129 
133 
133 
135 
138 
138 
138 
142 
143 
146 
148 
155 
159 
159 
160 
160 
162 
164 
164 
165 
167 
168 
173 
173 
175 
176 
178 
4.1 
Noninformative Priors and Propriety of the Posterior Distri-
bution ................ . 
4.2 
Choosing an Informative Prior . . . 
5. 
Bayesian Fitting of Item Response Models . . 
178 
180 
181 

x 
Contents 
5.1 
Fitting of the Two-parameter Model Using Gibbs Sampling . 181 
5.2 
Implementation of Gibbs Sampling for General F ....... 183 
5.3 
Gibbs Sampling for a Probit Link Using Data Augmentation 184 
5.4 
Bayesian Fitting of the One-parameter Model . 
185 
6. 
Inferences from the Model . 
185 
7. 
Model Checking . . . . . . . . . . . 
186 
7.1 
Bayesian Residuals . . . . . 
186 
7.2 
Posterior Predictive Checks 
187 
8. 
The Mathematics Placement Test Example 
188 
9. 
Further Reading . . . . . . . . . . . . . . . 
191 
11 Developing and Applying Medical Practice Guidelines Following 
Acute Myocardial Infarction: A Case Study Using Bayesian Pro bit 
and Logit Models 
195 
M. B. Landrum & S. Normand 
1. 
Background and Significance ........ . 
2. 
Developing Practice Guidelines ....... . 
2.1 
Elicitation of Appropriateness Ratings 
2.2 
Combining the Angiography Panel Data . 
2.3 
Estimation .......... . 
2.4 
Defining the Standard of Care 
2.5 
Results ......... . 
3. 
Applying the Practice Guidelines ... 
3.1 
Study Population . . . . . . .. 
3.2 
Modeling Adherence to Practice Guidelines 
3.3 
Estimation . . . . . . . . . . . . . . . . . 
3.4 
Profiling Hospitals ............ . 
3.5 
Explaining Variability in Quality of Care 
3.6 
Results 
4. 
Discussion ..................... . 
IV 
Semiparametric Approaches 
195 
197 
197 
198 
199 
199 
200 
200 
200 
202 
203 
203 
204 
205 
209 
215 
12 Semiparametric Generalized Linear Models: Bayesian Approaches217 
B. K. M a/lick, D. G. T. Denison & A. F. M. Smith 
1. 
Introduction . . . . . . . . . . . . . . 
2. 
Modeling the Link Function g .... 
2.1 
Binary Response Regression . 
2.2 
General Regression . . . . . . 
3. 
Modeling the Systematic Part 'fJ ••• 
3.1 
Model with Random Effects . 
3.2 
Model with Deterministic Error . 
4. 
Models Using Curves and Surfaces 
5. 
GLMs using Bayesian MARS 
5.1 
Classical MARS ..... . 
5.2 
Bayesian MARS ..... . 
5.3 
Bayesian MARS for GLMs 
6. 
Examples of Bayesian MARS for GLMs 
6.1 
Motivating Example . 
6.2 
Pima Indian Example ..... . 
217 
218 
218 
219 
219 
220 
220 
220 
221 
221 
222 
224 
224 
224 
225 

Contents 
xi 
13 Binary Response Regression with Normal Scale Mixture Links 
231 
S. Basu C:f S. Mukhopadhyay 
1. 
Introduction 
. . . . . . . . . . . . . . . . . . . 
231 
2. 
The Finite Mixture Model 
. . . . . . . . . . . 
233 
3. 
General Mixtures and a Dirichlet Process Prior 
234 
4. 
Model Diagnostic 
236 
4.1 
Basic Goal . . . . . . . 
236 
4.2 
Diagnostic Tools . . . . 
237 
4.3 
Computational Methods 
237 
5. 
Application: Student Retention at the University of Arkansas 
237 
6. 
Discussion 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 
239 
14 Binary Regression Using Data Adaptive Robust Link Functions 243 
R. Haro-L6pez, B. K. Mallick C:f A. F. M. Smith 
1. 
Introduction . . . . . . . . . . . . . . . . . . . 
243 
2. 
The Binary Regression Model . . . . . . . . . 
244 
3. 
Detection of Outliers and Model Comparison 
248 
4. 
Numerical Illustration 
248 
5. 
Discussion . . . . . . . . . . . . . . . . . . . . 
250 
15 A Mixture-Model Approach to the Analysis of Survival Data 
255 
L. Kuo C:f F. Peng 
1. 
Introduction . . . . . . . . 
255 
2. 
Likelihood . . . . . . . . . 
25 7 
3. 
EM and Monte Carlo EM 
257 
4. 
Gibbs Sampler . 
259 
5. 
Model Selection . . . . . . 
260 
6. 
Example . . . . . . . . . . 
261 
6.1 
EM Algorithm for the Specific Example 
262 
6.2 
Gibbs Samplers for the Specific Example 
264 
6.3 
Numerical Results . . . . . . . . . . . . . 
265 
V 
Model Diagnostics and Variable Selection in G LMs 
271 
16 Bayesian Variable Selection Using the Gibbs Sampler 
273 
P. Dellaportas, J. J. Forster C:f I. Ntzoufras 
1. 
Introduction . . . . . . . . . . . . . . . . . . . . . . 
273 
2. 
Gibbs Sampler Based Variable Selection Strategies 
274 
2.1 
Carlin and Chib's Method . . . . . . . . . . 
275 
2.2 
Stochastic Search Variable Selection . . . . 
276 
2.3 
Unconditional Priors for Variable Selection 
277 
2.4 
Gibbs Variable Selection . . . . . . . . . . . 
277 
2.5 
Summary of Variable Selection Strategies . 
278 
3. 
Illustrative Example: 2 x 2 x 2 Contingency Table 
278 
3.1 
Log-Linear models . . . . . 
280 
3.2 
Logistic Regression Models 
280 
4. 
Discussion . . . . . . . . . . . . . . 
281 
5. 
Appendix: BUGS CODES . . . . . 
282 
5.1 
Code for Log-linear Models for 23 Contingency Table . 
282 
5.2 
Code for Logistic Models with 2 Binary Explanatory Factors 283 

xii 
Contents 
17 Bayesian Methods for Variable Selection in the Cox Model 
287 
J. Ibrahim C:f M. H. Chen 
1. 
Introduction . . . . . . . . . 
287 
2. 
The Method . . . . . . . . . 
289 
2.1 
Model and Notation 
289 
2.2 
Prior Distribution for hb(·) 
289 
2.3 
The Likelihood Function . . 
292 
2.4 
Prior Distribution for the Regression Coefficients 
293 
2.5 
Prior Distribution on the Model Space . . . . . . . 
297 
3. 
Computational Implementation . . . . . . . . . . . . . . . 
299 
3.1 
Computing the Marginal Distribution of the Data . 
299 
3.2 
Sampling from the Posterior Distribution of (f3(m), ~) 
302 
4. 
Example: Simulation Study 
305 
5. 
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
309 
18 Bayesian Model Diagnostics for Correlated Binary Data 
D. K. Dey C:f M. H. Chen 
1. 
Introduction . . . . . . . . . . . . . . . . 
2. 
The Models .............. . 
2.1 
Stratified and Mixture Models 
2.2 
Conditional Models 
2.3 
Multivariate Probit Models .. 
2.4 
Multivariate t-Link Models ... 
3. 
The Prior Distributions and Posterior Computations 
3.1 
Prior Distributions .......... . 
3.2 
Posterior Computations ....... . 
4. 
Model Adequacy for Correlated Binary Data 
5. 
Voter Behavior Data example 
6. 
Concluding Remarks ............. . 
VI Challenging Approaches in GLMs 
313 
313 
314 
314 
314 
315 
315 
316 
316 
317 
320 
324 
325 
329 
19 Bayesian Errors-in-Variables ,Modeling 
331 
J. Wakefield C:f D. Stephens 
1. 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . 
331 
2. 
Illustrative Example: Case-control Study with Deprivation . 
333 
3. 
Classical approaches . . . . . . . . . . . . . . . . . . . . . . 
337 
3.1 
Basic Formulation . . . . . . . . . . . . . . . . . . . 
337 
3.2 
Modeling and Analysis: Classical Extensions and Procedures 
339 
4. 
Bayesian Approaches . . . . 
339 
4.1 
General Framework 
339 
4.2 
Implementation. . . 
340 
4.3 
Previous Work . . . 
340 
5. 
Example revisited . . . . . . 
341 
6. 
Conclusions and Discussion 
342 
20 Bayesian Analysis of Compositional Data 
349 
M. Iyengar C:f D. ](. Dey 
1. 
Introduction . . . . . . . . 
349 
2. 
A Parametric Approach 
351 

Contents 
3. 
4. 
5. 
6. 
7. 
Simulation Based Model Determination 
A Semiparametric Approach ...... . 
Posterior Distributions and Estimation . 
Results ... 
Conclusion ... 
21 Classification Trees 
D. G. T. Denison & B. K. Mallick 
1. 
Introduction . . . . . . . . . . . 
2. 
The Classification Tree Model . 
2.1 
The Basis Functions .. 
2.2 
The Classical Approach 
2.3 
The Bayesian Approach 
3. 
Real data example 
4. 
Discussion ........... . 
xiii 
352 
354 
355 
359 
361 
365 
365 
366 
366 
367 
368 
369 
371 
22 Modeling and Inference for Point-Referenced Binary Spatial Data373 
A. E. Gelfand, N. Ravishanker & M. Ecker 
1. 
Introduction . . . . . . 
373 
2. 
Modeling Details . . . 
375 
3. 
Computational Issues 
378 
4. 
An Illustration . 
380 
5. 
Related Remarks . . . 
384 
23 Bayesian Graphical Models and Software for GLMs 
N. Best & A. Thomas 
387 
1. 
Bayesian Graphical Models and Conditional Independence Structures 387 
1.1 
Computation on Bayesian Graphical Models . . 
1.2 
Constructing Software from Graphical Models . 
2. 
Implementing GLM's Using WinBUGS ... . 
3. 
GLMs with Non-canonical Links ...... . 
4. 
Generalized Linear Mixed Models (GLMMs) . 
4.1 
Exchangeable Random Effects 
4.2 
Correlated Random Effects 
5. 
Polytomous Responses . . . . . . . . . 
5.1 
Ordered Categories ...... . 
6. 
Adding Complexity in GLMs/GLMMs . 
6.1 
Missing Data ...... . 
6.2 
Informative Missing Data .. . 
6.3 
Prediction ........... . 
6.4 
Covariate Measurement Error . 
7. 
General Advice on Modeling Using WinBUGS 
7.1 
Parameterization . . . . . . . . . . . . . 
7.2 
Prior Specification ........... . 
7.3 
Convergence and Posterior Sample Size 
7.4 
Model Checking . . . . . . 
8. 
Extending the WinBUGS Software ...... . 
Index 
388 
389 
389 
391 
391 
392 
392 
394 
395 
396 
396 
396 
397 
397 
398 
398 
400 
401 
402 
402 
407 

Contributors 
xiv 
James Albert 
Sanjib Basu 
Nicky Best 
Ming-Hui Chen 
Siddhartha Chib 
Petros Dellaportas 
David D.T. Dension 
Department of Mathematics and Statistics, 
Bowling Green State University, U.S.A 
albert~bgnet.bgsu.edu 
Division of Statistics, 
Northern Illinois University, U.S.A 
basuGmath.niu.edu 
School of Medicine at St. Mary's, 
Imperial College, U.K. 
n.best~ic.ac.uk 
Department of Mathematical Sciences, 
Worchester Polytechnic Institute, U.S.A 
mhchen~wpi.edu 
John M. Olin School of Business, 
Washington University, U.S.A 
chib~simon.wustl.edu 
Department of Statistics, 
Athens University of Economics and Business, Greece 
petros~aueb.gr 
Department of Mathematics, 
Imperial College, U .K 
d.denison~ma.ic.ac.uk 
Dipak K. Dey 
Department of Statistics, 
University of Connecticut, U.S.A 
dey~stat.uconn.edu 
Mark D. Ecker 
Department of Mathematics, 
University of North Iowa, U.S.A 
ecker~math.uni.edu 
Marco A. R. Ferreira 
Instituto de Matematica, 
Federal University of Rio de Janeiro, Brazil 
marco~dme.ufrj.br 
Jonathan J. Forster 
Department of Statistics, 
Athens University of Economics and Business, Greece 
forster~aueb.gr 

Dani Gamerman 
Alan E. Gelfand 
Malay Ghosh 
Ruben A. Haro-L6pez 
Joseph G. Ibrahim 
Malini Iyengar 
Contributors 
xv 
Instituto de Matematica, 
Federal University of Rio de Janeiro, Brazil 
dani~dme.ufrj.br 
Department of Statistics, 
University of Connecticut, U.S.A 
gelfand~uconnvm.uconn.edu 
Department of Statistics, 
University of Florida, U.S.A 
ghoshm~stat.ufl.edu 
Department of Mathematics, 
Imperial College, U .K 
haro~gauss.rhon.itam.mx 
Department of Biostatistics, 
Harvard University, U.S.A 
d.denison~ma.ic.ac.uk 
Clinical Biostatistics, 
Smith Kline Beecham Pharmaceuticals, U.S.A 
malini~stat.uconn.edu 
Lynn Kuo 
Department of Statistics, 
University of Connecticut, U.S.A 
lynn~stat.uconn.edu 
Mary Beth Landrum 
Department of Health Care Policy, 
Harvard University, U.S.A 
landrum~hcp.med.harvard.edu 
Bani K. Mallick 
Department of Statistics, 
Texas A & M University, U.S.A 
bmallick~stat.tamu.edu 
Saurabh Mukhopadhyay 
Clinical Biostatistics, 
Merck & Co., Inc., U.S.A 
saurabh_mukhopadhyay~merck.com 
Balgobin Nandram 
Department of Mathematical Sciences, 
Worchester Polytechnic Institute, U.S.A 
balnan~wpi.edu 
Sharon Lise-Normand 
Department of Health care policy, 
Harvard University, U.S.A 
sharon~hcp.med.harvard.edu 
Ioannis Ntzoufras 
Department of Statistics, 
Athens University of Economics and Business, Greece 
ioannis~aueb.gr 

xvi 
Contributors 
Fengchun Peng 
Credit Marketing, 
Sears, Roebuck and Co., U.S.A 
peng~stat.uconn.edu 
Nalini Ravishanker 
Department of Statistics, 
University of Connecticut, U.S.A 
nalini~stat.uconn.edu 
Adrian F .M. Smith Queen Mary and Westfield College, 
University of London, U .K 
AFMS.BSU_DOM 
David Stephens 
Department of Mathematics, 
Imperial College, U .K 
d.stephens~ma.ic.ac.uk 
Dong Chu Sun 
Department of Statistics, 
Paul L. Speckman 
Andrew Thomas 
University of Missouri Columbia, U.S.A 
sun~stat.missouri.edu 
Department of Statistics, 
University of Missouri Columbia, U.S.A 
speckman~stat.missouri.edu 
School of Medicine at St. Mary's, 
Imperial College, U.K. 
a.thomas~ic.ac.uk 
Robert K. Tsutakawa Department of Statistics, 
Jon Wakefield 
University ofMissoul'i Columbia, U.S.A. 
tsutakawa®stat.missouri.edu 
School of Medicine at St. Mary's 
Imperial College, U.K. 
j.wakefield®ma.ic.ac.uk 

GENERALIZED 
LINEAR MODELS 

Biostatistics: A Series of References and Textbooks 
Series Editor 
Shein-Chung Chow 
President, U.S. Operations 
StatPius, Inc. 
Yardley, Pennsylvania 
Adjunct Professor 
Temple University 
Philadelphia, Pennsylvania 
1. Design and Analysis of Animal Studies in Pharmaceutical Development, 
edited by Shein-Chung Chow and Jen-pei Liu 
2. Basic Statistics and Pharmaceutical Statistical Applications, James E. De 
Muth 
3. Design and Analysis of Bioavailability and Bioequivalence Studies, Second 
Edition, Revised and Expanded, Shein-Chung Chow and Jen-pei Liu 
4. Meta-Analysis in Medicine and Health Policy, edited by Datene K. Stangl and 
Donald A. Berry 
5. Generalized Linear Models: A Bayesian Perspective, edited by Dipak K. Dey, 
Sujit K. Ghosh, and Bani K. Mallick 
ADDITIONAL VOLUMES IN PREPARATION 
Medical Biostatistics, Abhaya lndrayan and S.B. Sarmukaddam 

Part I 
General Overview 

This Page Intentionally Left Blank 

1 
Generalized Linear Models: A 
Bayesian View 
Alan E. Gelfand 
Malay Ghosh 
ABSTRACT Generalized linear models (GLMs) offer a unifying class of models 
which are widely used in regression analysis. Though initially introduced into the 
community from a classical viewpoint, in the past decade the Bayesian literature 
employing these models has witnessed rapid growth. This is due in part to their at-
tractiveness within familiar hierarchical modeling but as well to the wide availability 
of high speed computing to implement simulation based fitting of these models. The 
objective of this chapter is to provide a brief, somewhat selective, summary and 
overview of this recent literature. In particular, we focus upon the range of proposed 
GLMs, prior specification, propriety of the resultant posterior, semiparametric ap-
proaches and model determination, i.e., model adequacy and model choice. 
1. 
Introduction 
Generalized linear models (GLMs), originally introduced by Neider and Wedder-
burn (1972), provide a unifying family of models that is widely used for regression 
analysis. These models are intended to describe non-normal responses. In particu-
lar, they avoid having to select a single transformation of the data to achieve the 
possibly conflicting objectives of normality, linearity and homogeneity of variance. 
Important examples include the binary and the count data. Over the years, GLMs 
have expanded much in scope and usage, and are currently applied to a very broad 
range of problems which include analysis of multicategory data, dynamic or state-
space extensions of non-normal time series and longitudinal data, discrete time 
survival data, and non-Gaussian spatial processes. 
By now, there are several excellent textbooks discussing inference for GLMs from 
a classical point of view (McCullagh and Neider, 1989; Fahrmeir and Tutz, 1991, 
Lindsey, 1995). These books provide a rich collection of estimation and hypothesis 
testing procedures for various parameters of interest primarily from a frequentist 
point of view. Breslow and Clayton (1993) have extended these models further 
by introducing random effects in addition to the fixed effects. The resulting mod-
els, usually referred to as generalized linear mixed models (GLMM's) have further 
widened the scope of application of GLMs for data analysis. Software routines such 
as PROC MIXED in SAS have facilitated the computations involved in the classical 
implementation of GLMs. 
3 

4 
Gelfand & Ghosh 
Bayesian methods for analyzing GLMs are of more recent origin. A general hi-
erarchical Bayesian (HB) approach for such analysis began with West (1985) and 
Albert (1988), although various special cases were considered earlier. Leonard and 
Novick (1986) considered Bayesian analysis of two-way contingency tables, while 
Albert (1985) considered simultaneous estimation of several Poisson means via a 
hierarchical log-linear model. 
The present article aims to review the Bayesian perspective with regard to GLMs. 
This includes a range of hierarchical model GLM specifications, approaches for 
Bayesian model fitting and techniques for model checking and model choice. In Sec-
tion 2, after a brief introduction to the model and the classical inference procedure, 
we proceed to the discussion of some of the Bayesian models that have appeared 
in the literature. We compare the various priors that have been proposed, and dis-
cuss also the methods required for their implementation. This includes the work of 
Albert (1988), Ibrahim and Laud (1991), Dellaportas and Smith (1993), Zeger and 
Karim (1991), Ghosh, Natarajan, Stroud and Carlin (1998) among others. Section 
3 discusses the propriety of posteriors under the different priors. In particular, we 
discuss the results of Ibrahim and Laud (1991), Natarajan and McCulloch (1995), 
Hobert and Casella (1996), Gelfand and Sahu (1999), Ghosh, Natarajan, Stroud and 
Carlin (1998). Section 4 discusses semiparametric and nonparametric Bayesian pro-
cedures for GLMs. Section 5 briefly looks at overdispersed GLMs. Finally, Section 
6 addresses the issues of model diagnostics and model selection. 
2. 
GLMs and Bayesian Models 
2.1 
GLMs 
Consider measurements (discrete or continuous) for n individuals. For the ith 
individual, the response variable is denoted by Yi, and the corresponding vector of 
covariates is denoted by ~i. Responses may be continuous real variables, or counts 
or binary. Fahrmeir and Tutz (1991) contains many interesting examples of binary 
and count data. As an example of binary data, they consider infection from births 
by caesarean section. The response variable is the occurrence or non-occurrence of 
infection. They consider three dichotomous covariates: (a) planned or unplanned 
caesarean, (b) presence or absence of risk factors such as having diabetes or being 
overweight, and (c) use or nonuse of antibiotics as prophylaxis. An example of 
count data involves the effect of two agents of immuno-activating ability that may 
induce cell differentiation (Piegorsch, Weinberg and Margolin, 1988). As response 
variable, one considers the number of cells that exhibited markers after exposure. 
The covariates are the agents TNF (tumor necrosis factor) and IFN (interferon). It is 
of interest to know whether these agents stimulate cell differentiation independently 
or whether there is an interaction effect. 
There are certain distributional and structural assumptions associated with GLMs. 
The key distributional assumption is that conditional on the fh, the Yi are indepen-
dent with pdf's belonging to the one-parameter exponential family, that is, 
(1) 
where the ()i are unknown, but the a( ¢i)(> 0) are known. The usual structural 
assumption is that ()i = h( ~T b), where h is a strictly increasing sufficiently smooth 
function, b(p x 1) is the vector of unknown regression coefficients, and the ~i(P x 1) 

1. Bayesian GLMs 
5 
are known design vectors of dimension p. The parameters ei are usually referred to as 
the canonical parameters. Important special cases include the binomial distributions 
with success parameters Pi = exp(Oi)/[1+ exp(Oi)], a((Pi) = 1, and the Poisson 
distributions with means Ai = exp(Oi), a(<Pi) = 1. The N(J.ti,a}) distributions are 
also covered by (1) with ei = /-li, a(</Ji) =a-t. The gamma and the inverse Gaussian 
distributions are other important special cases of (1 ). 
The classical estimation procedure for GLMs is maximum likelihood. For simplic-
ity, we assume that the </Ji are known and that XT = (::c1, ···,::en) has rank p. The 
likelihood function is given by 
L(b) oc exp [~ a- 1(</>;){y;h(zTb)- ,P(h(zTb))}] . 
(2) 
The corresponding score vector is 
dlo~~(b) = t a£ 1 ( </Ji){Yi- ¢' (h(::cib))}h' (::cib)::ci, 
i=l 
(3) 
and the Fisher information matrix is 
I(b) = E [- d:~~;:] = XTDV(b)A
2(b)X, 
(4) 
where D = Diag ( a-1 ( </Ji), · · ·, a-1( <Pn)), V(b) = Diag ( 1/J"(h( ::cfb )), · · ·, ¢"( h( ::c~b)), 
and A.(b) = Diag (h'(::cfb), · · ·, h'(::c~b)). 
The maximum likelihood estimators are obtained as iterative solutions of the 
likelihood equations dlo~;(b) = 0. If the log-likelihood £(b)'= logL(b) is concave, 
then the MLE is unique when there exists at least one b within the admissible 
parameter set where f(b) attains the local or global maximum. 
The asymptotic theory of the MLE works in this situation as well. Under mild 
regularity conditions, the MLE b of b is asymptotically N(b, n-1 I- 1(b)). 
2.2 Bayesian Models 
For a Bayesian model associated with the likelihood (2), we require a prior for b. 
A commonly used choice is N(bo, :E), where bo and ::E are known. This prior appears 
for example in Dellaportas and Smith (1993). Then, writing y = (Y1, · · ·, Yn)T, the 
posterior of b is given by 
.. (b/y) oc exp [ t a- 1(</>;){y;h(zTb)- ,P(h(xT b))}- ~(b- bo)TE- 1(b- b0)] • 
(5) 
The above posterior is not analytically tractable. In fact, there does not exist 
any closed form expression for the norming constant. Also, finding posterior means, 
variances etc. by numerical integration is not easy even for moderate p. The most 
convenient approach seems to be the Markov Chain Monte Carlo (MCMC) numer-
ical integration techniques which require generating samples from the posterior. 
They can be implemented in general using the Metropolis-Hastings algorithm, but 
if the posterior is log-concave, then one can also use the adaptive rejection sampling 
approach of Gilks and Wild (1992). 

6 
Gelfand & Ghosh 
With little or no prior information, an alternative is to use noninformative priors. 
This implies that the posterior distribution is essentially the likelihood that the 
Bayesian analysis will be close to a likelihood analysis, possibly attractive to fre-
quentists. One of the commonly used noninformative priors due to Laplace (1812) is 
1rL(b) oc 1. However, the following example due to Laud and Ibrahim (1991), shows 
that such a prior can sometimes lead to an improper posterior. 
Example 1. Suppose n = 1, p = 1, and 
f(Y11xt) = (bxt)-1 exp[-Y1/(bxt)]; y > 0, x1 > 0, b > 0. 
Then if 1rL(b) oc 1 
(6) 
Integration with respect to b over (0, oo) gives f0
00 1rL(bjyt)db = f0
00 z-1 exp ( -'fL]-) dz = 
+oo, so that the posterior is improper. 
Laud and Ibrahim (1991) proposed Jeffreys' prior for this problem given by 
1rJ(b) oc II(b)jll 2 with I(b) given in (4). They provided sufficient conditions under 
which the resulting posterior 1rJ(bly) is proper. 
The likelihood function given in (2) generalizes the normal fixed effects model to 
the one-parameter exponential family. Breslow and Clayton (1993) have extended 
this further to include mixed effects models. For a fixed effects model, under the 
link function g = h-1, we have g(Bi) = ~fb. In contrast, for the random effects 
model, one incorporates the random effects as well, and writes g(Bi) = ~fb+zfui, 
where the Zi are also known and the Ui are i.i.d. N(O, Eu)· Breslow and Clayton 
(1993) advocated penalized quasi-likelihood estimates (PQL) for estimating b. 
We note an example of generalized linear mixed effects model (GLMM), consid-
ered in Crowder (1978), and also in Breslow and Clayton (1993). This concerns 
data on the proportion of seeds that germinated on each of 21 plates arranged 
according to a 2 x 2 factorial layout by seed variety and type of root extract. It 
turns out in this example that the within-group variation exceeds that predicted by 
the binomial sampling theory. The heterogeneity due to plate-to-plate variability is 
accounted for by Crowder (1978) and Breslow and Clayton (1993) by means of a 
GLMM that employs the canonical link setting 
i = 1, · · ·, 21, where b represents the fixed effects associated with seed and the 
extract, and the 'Ui, assumed to be iid N ( 0, o-~), represent random effects associated 
the plates. 
The Bayesian procedure, as before assigns a N(bo, Eb) distribution to b. More 
generally, a hierarchical Bayesian model is considered whereby one assigns distribu-
tions to Eb and O"u. One option is to use an inverse Wishart distribution for Eb and 
inverse gamma distribution for o-~. Such distributions can possibly be improper, but 
care must be exercised in order that the resulting posterior is proper. 
Specifically, let Eb and o-~ have independent priors 
(7) 
and 
( 2) 
( 
a )( 2)-iL-1 
1r O"u 
<X exp --2 
2 
O"u 
2 
• 
O"u 
(8) 
We shall write symbolically Eb "'IW(w ,v) and o-~ "'IG(~, f). 

1. Bayesian GLMs 
7 
The joint posterior of 8 = ( fh, · · ·, On )T, b, lh and o-~ is now given by 
n 
1r(8, b, Eb, o-~ IY) 
oc exp[I: { a- 1 ( ¢ )(yi()i - ¢(0i))}] 
i=l 
X 
X 
(9) 
This posterior is analytically intractable, and one needs numerical integration for 
posterior analysis. Gibbs sampling (Gelfand and Smith, 1990; Gelfand, Hill, Racine-
Poon and Smith, 1990) has proved to be very useful for implementation of the 
Bayesian model fitting. This requires sampling from the full conditionals 
[bl8, Eb, o-~, y]"' N((o-~
2 XT X +E[;
1 )-
1 (o-~
2 XT 8+E[; 1bo), (o-~
2 XT X +E[; 1)- 1]; 
(10) 
[Ebl8, b, o-~, y]"' IW(w + (b- bo)(b- bo)T ,v + p); 
(11) 
(12) 
(13) 
where X = (::c 1, • • ·, ::cn)T. It is easy to generate samples from the normal, inverse 
Wishart and the inverse gamma distributions. The only nonstandard conditionals 
are the 1r(Oi !.), i = 1, · · ·, n, which are known only up to multiplicative constants. 
One can use the Metropolis-Hastings algorithm to generate samples. Alternately, 
one can use the adaptive rejection sampling of Gilks and Wild (1992) since these 
posteriors are log-concave. 
The above model should be contrasted to that of .(\.lbert (1988). Albert (1988) 
begins with the likelihood given in (1), but does not model the ()i as ()i = h(::cTb). 
Instead, he considers independent conjugate priors for the ()i at the first stage, 
namely 
(14) 
An easy calculation shows that E[¢'(0i)] = mi. He models the prior means mi as 
mi = h(::cT b). Thus Albert moves the GLM to the second stage specification, that 
is, for the ()i 's rather than the customary first stage specification for the Yi 's. The 
remaining prior parameter A is a precision parameter that reflects the strength of 
one's prior beliefs about the means mi. As A approaches infinity, the prior distribu-
tion of the ¢' ( ()i) becomes increasingly concentrated about the mean mi, and the 
Bayesian model approaches the first stage specification in (2). 
To complete the prior specification at the second stage, a distribution needs to 
be assigned to b and A. West (1985) assigns a normal distribution to b, and a 
chi-squared distribution to A. Albert (1988) assigns instead the prior ll(b, A) oc 
(1 + A)- 2, that is a priori b and A are independent with b "' uniform (RP), and 
ll(A) oc (1 + A)- 2, a heavy tailed prior with infinite first moment. 
With Albert's model, it is possible to calculate 
This implies 
(15) 

8 
Gelfand & Ghosh 
Again, explicit evaluation of the conditional expectation given in the right hand 
side of ( 16) is difficult. Albert uses three approximations: (a) Laplace's method, (b) 
Quasi-likelihood approach, and (c) Brooks's method. A detailed application is given 
for the binomial-logit hierarchical model. 
Albert's model differs from the one given in (5), (7) and (8) in that he connects the 
regression variables not directly with the parameters of interest, but indirectly with 
the prior means. The approximations that he considers are attractive alternatives 
to direct numerical integration, especially when pis large. On the other hand, these 
approximations do not appear to be simpler than using MCMC integration methods. 
A slight variant of the model given in (1 ), (5), (7) and (8) is due to Zeger and 
Karim ( 1991). They consider a stratified sampling situation where Yii is the response 
of the jth unit in the ith stratum. (In a longitudinal data or repeated measurements 
situation, j denotes the jth measurement on the ith subject). The corresponding 
vector of auxiliary characteristics or covariates is denoted by aJij (p x 1). The one-
parameter exponential family model is now given by 
where the link function h(.) yields h(()ii) = aJ'{jb+ z'{jui, j = 1, · · ·, ni; i = 1, · · ·, m. 
The Ui are iid N(O, Eu)· 
At the final stage of the hierarchical model, one assigns mutually independent 
priors to band Eu with b""' uniform (RP) and Eu ""' IW(1 W, v). This posterior is 
also analytically intractable, and Gibbs sampling can be used to generate samples 
from the necessary conditionals. 
The Zeger-Karim formulation does not include possible error in misspecifying the 
model. Ghosh, Natarajan, Stroud and Carlin (1998) consider slightly more general 
modeling of g(()ij ), namely h(()ij) = aJ'{jb + z'{jui + eij. The errors eij account for 
model misspecification. A special case of the latter model will be discussed in the 
next section for studying the propriety of posteriors. 
3. 
Propriety of Posteriors 
If only proper priors are used, then one necessarily gets proper posteriors. How-
ever, as mentioned earlier, Bayesian analysis often relies on diffuse and flat priors 
which are mostly improper. In such instances, it is imperative to verify the propri-
ety of posteriors. Otherwise, descriptive measures such as moments, quantiles etc. 
of the posteriors do not carry any meaning. Checking the propriety of posteriors is 
all the more important when the Bayesian procedure is implemented via MCMC 
technique as it may so happen that all the full conditionals are proper distributions, 
and yet the posterior is improper. (See Casella and George (1992) for an elementary 
example). 
As shown in the previous section, Laplace's prior does not necessarily lead to a 
proper posterior. The same comment applies to Jeffreys' prior. Ibrahim and Laud 
(1991) have investigated conditions under which Jeffreys' prior leads to proper pos-
teriors for GLMs. We present their main result below. 
THEOREM 1. Consider the likelihood function given in (2), and Jeffreys' prior 
llJ(b) ex: II(b)j112, where I(b) is given in (4). Assume that r(X) = p and the 
likelihood function is bounded. Then, a sufficient condition that the posterior dis-

1. Bayesian GLMs 
9 
tribution ll(bly) is proper is that the integrals 
(17) 
are finite for all i = 1, · · ·, n. 
Ibrahim and Laud (1988) also show that a necessary and sufficient condition 
for llJ to be proper is that f6 [tfJ"(O)]ll 2d0 is finite where e denotes the parameter 
space. Dey, Gelfand and Peng (1997) extend these results to the case ofoverdispersed 
GLMs (see Section 5). Hobert and Casella (1996) have provided conditions ensuring 
propriety of posteriors for normal models. Natarajan and McCulloch (1995) have 
considered a version of a hierarchical model for binary data which ensures the 
propriety of posteriors. 
Ghosh et al. (1998) have considered the model given in (14), but h(()ij) = ~~b+ 
'Ui + eij(j = 1, · · ·, ni; i = 1, · · ·, m). The 'Ui and the eij are mutually independent 
with the 'Ui iid N(O, u~) and the eij are iid N(O, o-2). Also, b, u~ and o-2 are mutually 
independent with b ~uniform (RP), u~ ~ IG (~a, ~g) and o-2 ~ IG (~c, ~d). 
Ghosh et al. (1998) provide sufficient conditions for the propriety of posteriors 
under this model. A slightly more general version of their theorem is proved in 
Ghosh and Natarajan (1998) which we present below. 
THEOREM 2: Assume that f(Yij l()ij) is bounded for all j = 1, · · ·, ni; i = 1, · · ·, m. 
Let S = { ( i, j) : J f(Yij l()ij )d()ij < oo}, and s = cardinality of S. Assume that 
s 2:: 1. Then the posterior 1r(8, b, u~, u21y) is proper if a> 0, c > 0, m + g > 0 and 
s + d > p. 
In a more recent article, Gelfand and Sahu (1999) have linked the issue of pro-
priety of posteriors with Bayesian identifiability. Suppose the Bayesian model is 
denoted by the likelihood L(8; y) and the prior 1r(8). Suppose 8 = (81, 82). Fol-
lowing Dawid (1979), if 1r(82l81,y) = 1r(82l81), we say that 82 is not identifiable. 
This means that if observing data y does not increase our prior knowledge about 
82 given 8 1, then 82 is not identified by the data. Noting that 
(18) 
82 is nonidentifiable if and only if L(81,82;y) is free of 82, that is L(8;82;y) = 
L(81, y). Hence, Dawid's formal definition of non-identifiability is equivalent to lack 
of identifiability of the likelihood. We may also observe that if L(81 , 82; y) is free of 
82, then the posterior 
is proper if and only if both 1r(8dy) and 1r(82l81) are proper. 
To see how (19) works for GLMs suppose that rank (X) = r < p. Then it is 
possible to make a one-to-one transformation from b to ( li, p) such that (2) has the 
alternate representation 
L(6,p) <X exp [~a- 1 (4>;){u;h(a:fu6)- ¢(h(a:ij16))}], 
(20) 
where X~ = ( 
~o1, · · · , ~on) is a r x n matrix of rank r. Thus the likelihood does not 
depend on p. Since propriety of 1r(bly) is equivalent to the propriety of 1r(li, ply), 
it follows from (19) and (20) that 1r(bly) is proper if and only both 1r(lily) and 
1r(plli) are proper. If the latter holds, all we need to verify is that 1r(6jy) is proper. 

10 
Gelfand & Ghosh 
Gelfand and Sahu (1999) have shown that with the canonical link fh = ~fb, if 
rank(X) = r < p, a sufficient condition for a proper 1r(bjy) is that the likelihood is 
bounded, and that at least r of the yi's belong to the interiors of their respective 
domains. 
4. 
Semiparametric GLMs 
Sections 2 and 3 contain discussion of fully parametric Bayesian GLMs. In this 
section, we enrich this class of models by wandering nonparametrically near (in a 
suitable sense) this class. As a result, parts of the modeling are captured parametri-
cally, in particular, the linear regression structure on some monotonically increasing 
transformed scale of the canonical parameters. Other aspects such as the link func-
tion or the distribution of the random effects are specified non parametrically. This is 
an example of what has now become known as semi parametric regression modeling. 
One of the difficulties with Bayesian modeling in the nonparametric case is that, 
unlike its parametric counterpart where the dimension of the parameter space is 
finite, nonparametric modeling requires an "infinite dimensional" parameter. Thus, 
the Bayesian approach, in assuming all unknowns are random, requires an infinite 
dimensional stochastic specification. However, significant advancement of research 
in this area during the past twenty five years or so has provided tractable ways to 
make such specifications. Further, more recent advances in Bayesian computation 
enable the fitting of models incorporating these specifications and even extensions 
of these specifications. 
The various probabilistic specifications which yield Bayesian nonparametric mod-
eling include discrete mixtures, Dirichlet processes, mixtures of Dirichlet processes, 
Polya tree distributions, Gamma processes, extended Gamma processes and Beta 
processes. Gelfand (1998) contains a review of all these. For brevity, we will discuss 
only the discrete mixtures and mixtures of Dirichlet processes in the context of 
conditionally independent hierarchical GLMs. 
We may note that the basic object which we are attempting to model is an 
unknown function, say g( ·). The parametric approach writes g as g( ·, 8), ec8 and 
then places a prior distribution over 8t8. The nonparametric approach assumes 
only that g E g, where g is some class of functions. A Bayesian approach requires 
assigning a prior over the elements of g. In what follows, we illustrate this with 
examples where the elements of g are monotone functions. 
We begin our discussion with mixture models. First we notice that modeling a 
strictly monotone function g is equivalent to modeling a distribution function. For 
instance, if the range of g is R1, then T(g( ·)) with T( z) = k1 exp( k2 z) /[1 + k1 
exp(k2z)] with k1 > 0, k2 > 0 is a df. Similarly, if the range of g is n+, then T(g(·)) 
with T(z) = k1zk 2 (1 + k1zk 2 )-1 with k1 > 0, k2 > 0 is a df. 
The mixture model approach models an unknown df using a dense class of mix-
tures of standard distributions. For instance, Diaconis and Ylvisaker (1985) observe 
that discrete mixture of Beta densities provide a dense class of models for densities 
on [0, 1]. 
As a special case, consider modeling the link function h in a generalized linear 
model. Mallick and Gelfand (1994) suggested modeling h(()) by 
r 
T(h(e)) = 2: w~.IB(T(ho(e); ct, de), 
(21) 
l=l 

1. Bayesian GLMs 
11 
where r is the number of mixands, w.e ~ 0, 2:~= 1 w.e = 1 are the mixing weights, 
IB(u,c,d) = f0u{xe- 1(1- x)d- 1/B(c,d)}dx, and ho is a centering function for h. 
Then (21) provides a generic member of the dense class. Inversion of (21), pro-
vides a generic h. Since h is determined by specification of r, w(r) = ( W1, · · ·, Wr ), 
c(r) = (cb"',cr) and d(r) = (d1,···,dr), introducing a distribution on g re-
quires specification of a distribution of the form f(r )/( w(r), c(r)d(r) lr ). Mallick and 
Gelfand (1994) suggest-fixing c(r) and d(r) to provide a set of Beta densities which 
"blanket" [0, 1], for example c.e = £, d.e = r + 1 - £, f = 0, 1, · · ·, r, but treat w(r) 
as random, given r. As shown by these authors, if w(r) ""'Dirichlet (alr), where 
lr = (1, · · ·1)T, then E[T(h(O))] ~ T(ho(O)), that ish is roughly centered about ho. 
This class of models is easy for inferential purposes. Since finding the posterior of 
h is equivalent to finding the posterior of w(r), all one needs is to generate samples 
from the latter. 
Next we turn to Dirichlet processes. Since the appearance of the classic paper 
of Ferguson (1973), such processes have been used quite extensively for Bayesian 
nonparametric inference. A probability measure G on g is said to follow a Dirichlet 
process with parameter aGo, symbolically written as G""' DP(aGo), if for any mea-
surable partition B1, · · ·, Bm of{/, (G(Bl), · · ·, G(Bm))""' Dirichlet (aGo(Bl), · · ·, 
aGo(Bm)). Here, Go is a specified probability measures, and a is the "precision" pa-
rameter. This name for a is justified since V[Go(B.e)] = Go(B.e)(1- Go(B.e))/(a + 1) 
which decreases in a for every £. 
Computationally, it is most convenient to work with a family of Dirichlet mix-
ture distributions. Let {/(·18), 8f8 C RP} be a parametric family of densities with 
respect to some dominating measure 1-l· Consider the family of probability distribu-
tions :F ={Fa : Gf{l} with densities 
f(y!G) = j f(yl8)dG(8). 
(22) 
Here G(8) is viewed as the conditional distribution of 8 given G. It is assumed 
that G""' DP(aGo), whence f(yjG) arises by mixing with respect to a distribution 
having a Dirichlet process. 
Mukhopadhyay and Gelfand (1997) provide a general discussion of Bayesian in-
ference based on Dirichlet process mixed models. Mixtures of Dirichlet processes 
were first introduced in Antoniak (1974), and have been considered subsequently 
in Lo (1984), Brunner and Lo (1989, 1994), (1991), Escobar (1994), Escobar and 
West (1995), MacEachern and Miiller (1994), Gelfand and Mukhopadhyay (1995), 
and Newton, Czado and Chappell (1996) among others. 
To illustrate the implementation of the semiparametric Bayesian procedure in the 
context of GLMs, we begin with conditionally independent observations Yi with pdf 
given in (1). In the next stage, we model h(Oi) = ~Tb+ui, where the 'Ui are iid from 
G with G""' DP(aGo). Integrating over G, the 'Ui have joint pdf /(u1, · · ·, uniGo, a). 
They are no longer independent, but this joint distribution can be written explicitly. 
To complete the prior specification, one needs to specify Go, and also specify 
a distribution for b and a. Typically, one assigns a N(b0 , E) distribution for b 
and, following Escobar and West (1995), a gamma distribution for a. (Empirical 
experience suggests setting a = 1 may be preferable to adding a hyperprior.) A 
simple choice for Go is a normal distribution. However, the Bayesian model fitting 
can be implemented, at least in principle, for an arbitrary Go and a. 
In order to implement the model fitting, one begins with the joint posterior of b 

12 
Gelfand & Ghosh 
and u = (u1,· · ·,un)T as 
1r(b, u, aiGo, y) 
<X exp [2:~= 1 {yih-1 (~fb+ui) -1/J(h-1 (~fb+ui))}] 
(23) 
x exp t-~ (b- bo)T E-1 (b- bo)] f (u1, .. ·, uniGo, a) 1r (a). 
The MCMC implementation of the Bayesian procedure requires generating sam-
ples from the full conditionals 
(a) 
1r (bju, a, Go, y) 
<X exp J2:7=1 {yih-1 (~f b + ui) - ¢ (h-1 (~f b + ui))}] 
x exp l-~ (b- b0)T E- 1 (b- b0 )) ; 
(24) 
(b) 1r (uilb, 'Uj (j /=i), a, Go, y), a mixed density placing point masses proportional 
to f(Yi jb, 'Uj) at each 'Uj (j /=i) and continuous mass proportional to af(Yi jb, ui) 
!( 'Ui I Go); 
(c) 1r(ajb,u) <X f(u1, ... ,uniGo,a)1r(a). If 1r(a) is gamma, Escobar and West 
(1995) have discussed how to generate samples from ll(ajb, u) by introducing 
an additional parameter, say, 'Y· The reader is referred to their paper for 
details. 
5. 
Overdispersed Generalized Linear Models 
Being based on the one-parameter exponential family of distributions, GLMs as-
sume a known functional relationship between the mean and the variance. This 
makes these models unsuitable for certain applications, especially those where the 
samples are too heterogeneous to be explained by such a simple functional relation-
ship. In such instances, one is naturally led to a wider class of models. 
A popular approach for creating a larger class has been through mixture models. 
For instance, the one parameter exponential family defining the GLM is mixed with 
a two parameter exponential family for the canonical parameter () (or equivalently 
the mean parameter J.t) resulting in a two parameter marginal mixture family for 
the data. The resulting overdispersed family of mixture models no longer belongs 
to the exponential family (e.g. beta-binomial, gamma-Poisson). More importantly, 
since the likelihood depends on the sample size, while the mixture distribution 
does not, the relative overdispersion of the resulting mixture family to the original 
exponential family tends to infinity as the sample size increases. An implication of 
such models is that taking additional observations within a population does not 
increase knowledge regarding heterogeneity across populations. 
A second class of models, usually referred to as exponential dispersion models 
(EDM), arises when a(¢)=¢ so that¢ behaves like a scale parameter. Jorgensen 
(1987) provides an extensive treatment of such models. The resulting two parameter 
family of distributions no longer belongs to the exponential family. 
An alternative approach due to Efron (1986) models overdispersion through so-
called "double-exponential" families. Such families are derived as a saddle point 
approximation to the density of an average of n* random variables from a one 
parameter exponential family with large n*. The parameter n* written as np for 
actual sample size n introduces p as a second parameter in the model along with 
canonical parameter (), Ganio and Schafer (1992) have shown that EDM's can be 

1. Bayesian GLMs 
13 
embedded within Efron's double exponential family, and the associated asymptotic 
inference applies. These asymptotics result in overdispersion relative to the original 
exponential family which tends to a constant as n-+ oo, unlike the mixture case. 
A more general class of models was introduced by Gelfand and Dalal (1990). 
For a given one-parameter exponential family, they introduced a two-parameter 
exponential family where one parameter is the overdispersion parameter in addition 
to the canonical parameter. This model includes Efron's model as a special case, 
and also includes a family discussed in Lindsay (1986). 
Dey, Gelfand and Peng (1997) adopted a Bayesian approach for fitting these mod-
els using Jeffreys' prior. Following the technique of Laud and Ibrahim (1991), they 
also proved the propriety of posteriors under such priors under certain conditions. 
Some of their results are presented below. 
We begin with conditionally independent random variables Yi ( i = 1, · · · , n) such 
that 
(25) 
In the above f is a density with respect to some u-finite measure J.L· Assuming 
that (25) is integrable with respect to Yi, if T(yi) is convex, then for distributions 
with common means, V(yi) increases in Ti. Let y = (Yb · · ·, Yn)T, and define (}i = 
h1(xTb) and Ti = h2(zT a), where h1 and h2 are strictly increasing. The resulting 
likelihood is 
n 
L(b, a; y) = exp[L { BiYi + TiT(yi) - p( Bi, Ti)}]. 
(26) 
i=1 
We shall use the notation p(r,s) = /)pr+s /(8Br8r 8 ). Then straightforward calcula-
tions yield 
(27) 
(28) 
( 8
2 log L) 
~ (1 1)( 
) 
1( T 
) 1 T 
E - IJb·IJ 
= L.-tP ' 
Bi, Ti XijZikU mi b h (zi a). 
J ak 
i=1 
(29) 
Writing xT = (mb ... ' mn), zT = (z1' ... ' Zn), M 9 as the n X n diagonal ma-
trix with (M9)ii = p(2•0)((}i, Ti)(g 1(mTb))2, Mr a diagonal matrix with (M r)ii = 
p(0 •2)((}i, Ti )(h1(zT a)) 2 , and M 9,r a diagonal matrix with (M 9,r )u = p(1•1)((Ji, Ti)U 1 
( m r 
b) hI ( z r 
a)' one gets the Fisher information matrix . 
(30) 
Jeffreys' prior is then given by II(b, a)l 112. 
Suppose we assume that X and Z are of full column rank, and that L(b, a; y) is 
bounded above. Then, the posterior of (b, a) is proper if, for each Yi(i = 1, · · ·, n), 
f f exp[Byi + rT(yi)- p(B, r)](p(2•0 )((}, r)p(0 •2)((}, r) 112d(}dr < oo. 
(31) 
lrle 
This result is proved in Dey et al. (1997). 
Recognizing the limitations of the one parameter exponential family, for exam-
ple, an implicit mean-variance relationship and unimodality, Mukhopadhyay and 

14 
Gelfand & Ghosh 
Gelfand (1997) introduced a Dirichlet process mixed GLM (DPMGLM). These mod-
els provide a more flexible first stage specification, while retaining linear structure 
on a transformed scale. 
6. 
Model Determination Approaches 
With the availability of a wide range of GLMs to consider in analyzing a dataset, 
the problem of model determination becomes critical. Model determination com-
prises model checking- is the model adequate?- and model selection- among a set 
of adequate models, which one is best? 
First we consider model adequacy which has received much less attention in the 
literature than model choice. In providing the probabilistic components of a hi-
erarchical model, we rarely believe that any of the distributions is correct. Those 
specifications further removed from the data are often intentionally made less pre-
cise, not because we believe them to be correct but in order to permit the data 
to drive the inference. However, what is true is apart from model checking. If we 
undertake model criticism we must examine the adequacy of what is specified and 
we must assume proper priors (or else the observed data could not have arisen pn-
der the model). High dimensional models, e.g., those having more parameters than 
data points, as well as very vaguely specified hierarchical models will be difficult to 
criticize. 
A formal Bayesian model adequacy criterion (as in Box, 1980) proposes that the 
marginal density of the data be evaluated at the observations. Large values support 
the model, small values do not. Assessment of the magnitude of this value could 
be facilitated by standardizing, using the maximum value or an average value of 
this density (Berger, 1985). However, a high dimensional density ordinate will be 
difficult to estimate well and hopeless to calibrate. In addition, with hierarchical 
models, failures, such as outliers, mean structure errors, dispersion misspecifica-
tions and inappropriate exchangeabilities, can occur at each hierarchical stage. The 
formal procedure does not provide feedback regarding the adequacy of the stagewise 
specifications. 
Chaloner and Brant (1988), Chaloner (1994) and Weiss (1995), focusing on outlier 
detection suggest posterior-prior comparison. Their strategy is to identify random 
variables whose distribution, a priori, is a standard one. In particular, they choose 
functions of so-called realized residuals. Given the data, the posterior distribution 
of each such function is obtained. If it differs considerably from its associated prior, 
using tail area comparison, a lack of model fit is claimed. For a realized residual itself, 
an outlying observation is asserted. If the entire model specification is correct, such 
comparisons will be successful on average but will fail to recognize the variability 
in the posterior. 
A second approach, referred to as model expansion or elaboration, captures model 
failures by specifying a more complex model using mixtures. Though most often 
used to detect outliers, recently, Albert and Chib (1997) use this approach for other 
model failures, in particular, exchangeability in the direction of partial exchange-
ability. Regardless, the model of interest becomes nested within the expanded or 
full model, so model choice procedures replace model checking to criticize the ade-
quacy of the reduced one. Recent work of Muller and Parmigiani (1995) and Carota, 
Parmigiani and Polson (1993) combines elaboration with posterior-prior comparison 
using the Kullback-Leibler distance between these two distributions for the elab-

1. Bayesian GLMs 
15 
oration parameter. This approach requires, for each sort of failure, a non-unique 
specification of an expanded model. 
A third approach is taken up in Gelman, Meng and Stern (1995) who propose 
a posterior predictive strategy. These authors define a discrepancy measure as a 
function of data and parameters, treating both as unknown in one case, inserting 
the observed data in the other. They then compare the resulting posterior distri-
butions given the observed data. Gelman, et al. dismiss prior predictive checking 
arguing that the prior predictive distribution treats the prior as a true "population 
distribution" whereas the posterior predictive distribution treats the prior as an 
outmoded first guess. However, model checking must examine the acceptability of 
the model fitted to the data. Model parameters must be generated from the prior 
prescribed under the model. Gelman, et al. can be criticized for using the data twice. 
The observed data, through the posterior, suggests values of the parameter which 
are likely under the model. Then, to assess adequacy, the observed data is checked 
against data generated using such parameter values, apparently making it difficult 
to criticize the model. 
A fourth approach is developed in recent work of Hodges (1998). Limited to 
the case where all levels are Gaussian, he reexpresses linear hierarchical models as 
standard linear models with simple covariance structure. He then suggests the use 
of familiar linear models diagnostic tools, e.g., residual plots, added variable plots, 
transformations, collinearity checks, case influence, etc. Ad-hoc method is needed 
in tailoring some of these tools to the hierarchical structure. 
Finally, Dey, Gelfand, Swartz and Vlachos (1998) suggest an approach which is 
entirely simulation based, requiring only the model specification and that, for a 
given data set, one be able to simulate draws from the posterior under the model. 
By replicating a posterior of interest using data replicates obtained under the model, 
the extent of variability in such a posterior can be seen. Then, the posterior obtained 
under the observed data can be compared with this medley of posterior replicates to 
ascertain whether the former is in agreement with them and accordingly, whether it 
is plausible that the observed data come from the proposed model. Such comparison 
can be implemented using a Monte Carlo test. Many such tests can be run, each 
focusing on a potential model failure. 
Turning to model choice, for a collection of models m = 1, 2, · · ·, M, the formal 
Bayesian approach assumes that one is "true" but which is the true one is unknown. 
Assigning prior probabilities Pm that model m is true, the posterior probability of 
model m, is Pr(mly) <X f(ylm)Pm where f(y I m) is the marginal or prior predictive 
density of y under model m. Hence, if Yobs denotes the realized data, the model 
which maximizes f(Yobalm)pm is selected. If Pm = M- 1 for all m, we choose the 
model with the largest f(Yobslm), suggesting the use of this quantity as a general 
screening criterion. When models are compared in pairs, the Bayes factor emerges, 
B = f(Yobslml)/ f(Yobslm2) for say models m1 and m2. B is viewed as a weight of 
evidence; B > 1 supports model m1, B < 1 supports model m2. 
Bayes factors have a wide advocacy in the Bayesian community; see Kass and 
Raftery (1995) for a review. However, they lack interpretation in the case of im-
proper priors which are frequently used in complex hierarchical specifications and 
they are difficult to compute for such models with large datasets (though there 
is much recent discussion, see, e.g., Raftery, 1995). The use of Schwarz's (1978) 
Bayesian information criterion (BIC) as an approximation to the Bayes factor re-
quires the specification of model dimension. Unfortunately in the context of GLMs 
involving mixed effects, the dimension of the model is unclear. Moreover, the asymp-
totics associated with such approximation are invalid when the number of model 

16 
Gelfand & Ghosh 
parameters grows with sample size, as in random effects settings where the number 
of individuals grows large. 
Recently, attractive alternatives have appeared. Gelfand and Ghosh (1998), not-
ing that posterior prediction is often a primary use for a model, suggest a formal 
utility maximization approach for model selection. In particular, their approach 
amounts to obtaining a minimized expected posterior predictive loss for a given 
model and then selecting the model which provides the overall minimum. 
For a version of log scoring (or deviance) loss, the minimization for a given model 
can be done explicitly yielding an expression which can be interpreted as a penal-
ized deviance criterion. The criterion is comprised of a piece which is a Bayesian 
deviance measure and a piece which is interpreted as a penalty for model complex-
ity. The penalty function arises without specifying model dimension or asymptotic 
justification. 
Under the model in (1) with a( <Pi)= ¢/wi, Wi known, the criterion becomes, for 
model m, 
(32) 
In (32), t(y) = yB(y)- ,P(B(y)) where B(J.L) = ,P'-
1 (J.L),J.L~m) = E(YiiYob 8 ,m) and 
t~m) = 
t 
E(t(yi)IYobs' m), and k is a weight which typically does not affect the ordering of 
the models and so may be set to 1 for convenience. Since t(y) is convex, Jensen's 
inequality ensures that each term in the right side of (32) is nonnegative. Gelfand 
and Ghosh clarify that the· first term can be interpreted as a penalty function 
and the second as a goodness-of-fit term. The choice (1) determines t(y), hence 
(32). For instance, in the Possion case t(y) = y logy - y, in the binomal case, 
t(y) = log ( *) + n~y log (n~y). Usual continuity corrections are imposed to ensure 
that t(y) can be calculated for any Yi,obs and that t~m) exists. 
Another, somewhat similar criterion has been discussed in Spiegelhalter, Best 
and Carlin (1998). Motivated by the work of Dempster (1974), their suggestion is 
to obtain the posterior distribution of the log likelihood at the observed data for 
each model and then compare these across models. In particular, for ( 1) they define 
the "Bayesian deviance" D( 8) to be 2:7=1 D( (}i) where 
(33) 
Defining D = E(D(8)Iy) and Pn = D- D(E(BIYobs)), Spiegelhalter, et al. propose 
the criterion 
DIG= D+pn 
(34) 
where DIC denotes Deviance Information Criteria. They argue that D, the poste-
rior expected deviance, summarizes model fit while pn, interpreted as the effective 
number of parameters, measures the complexity of the model. They show that DIC 
generalizes the familiar Akaike Information Criteria (AIC) (Akaike, 1973). 
Both the Gelfand and Ghosh criteria and the DIC are readily computed from 
posterior samples. The BUGS software (Spiegelhalter, et al. 1996) provides a con-
venient package for fitting most Bayesian GLMs, and thus for providing posterior 
samples. 
Finally, informal Bayesian model selection in the case of nested "GLMs can beef-
fected by obtaining the posterior distribution of the discrepancy parameter between 

1. Bayesian GLMs 
17 
the full and reduced models as in Albert and Chib (1997). Exploratory approaches 
using cross validation ideas, applicable to small or even moderate sized datasets are 
discussed in Gelfand, Dey and Chang (1992) and Gelfand (1995). 
Acknowledgement 
The work of the first author was supported in part by NSF grant DMS 96-25383. 
The work of the second author was supported in part by NSF grant SBR 98-10968. 
References 
Akaike, H. (1973). Information theory and an extension of the maximum likelihood 
principle. In Proceedings of International Symposium on Information Theory, 
ed. B.N.Petrov and F. Czaki, Budapest, Academia Kiado, 267-281. 
Albert, J .H. (1985). Simultaneous estimation of Poisson means under exchangeable 
and independence models. Journal of Statistical Computation and Simulation, 
23, 1-14. 
Albert, J .H. (1988). Computational methods using a Bayesian hierarchical gen-
eralized linear model. Journal of the American Statistical Association, 83, 
1037-1044. 
Albert, J .H. and S. Chib (1997). Bayesian tests and model diagnostics in condi-
tionally independent hierarchical models. Journal of the American Statistical 
Association, 92, 916-925. 
Antoniak, C.E. (1974). Mixtures of Dirichlet processes with applications to non-
parametric problems. The Annals of Statistics, 2, 1152-117 4. 
Berger, J .0. (1985). Statistical Decision Theory and Bayesian Analysis. Springer-
Verlag, New York. 
Box, G.E.P. (1980). Sampling and Bayes's inference in scientific modeling (with 
discussion). Journal of the Royal Statistical Society, Series A, 143, 383-430. 
Breslow, N.E. and D.G. Clayton (1993). Approximate inference in generalized lin-
ear mixed models. Journal of the American Statistical Association, 88, 9-25. 
Brunner, L.J. and Lo, A.Y. (1989). Bayes methods for a symmetric unimodal 
density and its mode. The Annals of Statistics, 17, 1550-1566. 
Brunner, L.J. and Lo, A.Y. (1994). Non parametric Bayes methods for directional 
data. The Canadian Journal of Statistics, 22, 401-412. 
Carota, C., G. Parmigiani and N.G. Polson (1997). Diagnostic measures for model 
criticism. Journal of the American Statistical Association, 92, 753-762. 
Casella, G and George, E.I. (1994). Explaining the Gibbs sampler. The American 
Statistician, 46, 167-174. 

18 
Gelfand & Ghosh 
Chaloner, K. (1994). Residual analysis and outliers in Bayesian hierarchical models. 
In: Aspects of Uncertainty, eds. A.F.M. Smith and P.R. Freeman. Chichester, 
U.K., John Wiley, 153-161. 
Chaloner, K. and R. Brant (1988). A Bayesian approach to outlier detection and 
residual analysis. Biometrika, 75, 651-659. 
Crowder, M. (1978). Beta-binomial ANOVA for proportions. Applied Statistics, 
27, 34-37. 
Dawid, A.P. (1979). Conditional independence in statistical theory. Journal of the 
Royal Statistical Society, series B, 41, 1-31. 
Dellaportas, P. and Smith, A.F.M. (1993). Bayesian inference for generalized linear 
and proportional hazards models via Gibbs sampling. Applied Statistics, 42, 
443-459. 
Dempster, A. (1974). The direct use of likelihood for significance testing. In: Pro-
ceedings of Conference on Foundational Questions In Statistical Inference. 
Eds: 0. Barndorff-Nielsen, P. Blaesild and G. Schou, p. 335-352, Department 
of Theoretical Statistics: University of Aarhus. 
Dey, D.K., Gelfand, A.E. and Peng, F. (1997). Overdispersed generalized linear 
models. Journal of Statistical Planning and Inference, 64, 93-107. 
Dey, D.K., Gelfand, A.E., Swartz, T. and Vlachos, P.K. (1998). Simulation based 
model checking for hierarchical models. Test, 7, 325-346. 
Efron, B. (1986). Double exponential families and their use in generalized linear 
regression. Journal of the American Statistical Association, 81, 709-721. 
Escobar, M.D. (1994). Estimating normal means with Dirichlet process priors. 
Journal of the Statistical Planning and Inference, 43, 97-106. 
Escobar, M.D. and West, M. (1995). Bayesian density estimation and inference 
using mixtures. Journal of the American Statistical Association, 90, 577-588. 
Fahrmeir, Land Thtz, G. (1994). Multivariate Statistical Modelling based on Gen-
eralized Linear Models. Springer-Verlag, New York. 
Ferguson, T.S. (1973). A Bayesian analysis of some nonparametric problems. The 
Annals of Statistics, 1, 209-230. 
Ganio, L.M. and Schafer, D.W. (1992). Diagnostics of overdispersion. Journal of 
the American Statistical Association, 87, 795-804. 
Gelfand, A.E. (1995). Model determination using sampling-based methods. In 
Markov Chain Monte Carlo in Practice, eds. W. Gilks et al., London, Chap-
man and Hall, 145-161. 
Gelfand, A.E. (1998). Approaches for semiparametric Bayesian regression. In: 
Asymptotics, Nonparametrics and Time Series. Ed.: S. Ghosh, Marcel Dekker, 
Inc, New York. (to appear). 
Gelfand, A.E. and Dalal S. (1990). A note on overdispersed~ exponential families. 
Biometrika, 77, 55-64. 

1. Bayesian GLMs 
19 
Gelfand, A.E., D.K. Dey and H. Chang (1992). Model determination using pre-
dictive distributions with implementations via sampling-based methods. In: 
Bayesian Statistics 4, eds. J.M. Bernardo et al., Oxford, U.K., Oxford Uni-
versity Press, 147-16r 
Gelfand, A.E. and Ghosh, S.K. (1998). Model choice: a minimum posterior predic-
tive loss approach. Biometrika, 85, 1-11. 
Gelfand, A.E., Hills, S.E., Racine-Poon, A. and Smith, A.F.M. (1990). Illustration 
of Bayesian inference in normal data models using Gibbs sampling. Journal 
of the American Statistical Association, 85, 972-985. 
Gelfand, A. E. and Sahu, S.K. (1999). On the propriety of posteriors and Bayesian 
identifiability in generalized linear models. Journal of the American Statistical 
Association (to appear). 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling-based approaches to calculat-
ing marginal densities. Journal of the American Statistical Association, 85, 
398-409. 
Gelman, A., X-L. Meng and H.S. Stern (1995). Posterior predictive assessment of 
model fitness via realized discrepancies (with discussion). Statistica Sinica, 6, 
733-807. 
Ghosh, M., and Natarajan, K. (1998). Small area estimation: a Bayesian perspec-
tive. Multivariate, Design and Sampling, Ed., S. Ghosh, Marcel Dekker, New 
York (to appear). 
Ghosh, M., Natarajan, K., Stroud, T.W.F. and Carlin, B.P. (1998). Generalized 
linear models for small-area estimation. Journal of the American Statistical 
Association, 93, 273-282. 
Gilks, W.R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Applied Statistics, 41, 337-348. 
Hobert, J.P. and Casella, G. (1996). The effect of improper priors in Gibbs sam-
pling in hierarchical linear mixed models. Journal of the American Statistical 
Association, 91, 1461-1473. 
Hodges, J. (1998). Some algebra and geometry for hierarchical models, applied to 
diagnostics. Journal of the Royal Statistical Society, Series B, 60 (to appear), 
Ibrahim, J .G. and Laud, P.W. (1991). On Bayesian analysis of general linear mod-
els using Jeffreys's prior. Journal of the American Statistical Association, 86, 
981-986. 
Jorgensen, B. (1987). Exponential dispersion models. (with discussion). Journal of 
the Royal Statistical Society, Series B,50, 150-?. 
Kass, R.E. and Raftery, A. E. (1995). Bayes factors. Journal of the American Stat-
istical Association, 90, 773-795. 
Leonard, T. and Novick, M.R. (1986). Bayesian full rank marginalization for two-
way contingency tables. Journal of Educational Statistics, 11, 33-56. 
Lindsay, B. (1986). Exponential family mixture models. The Annals of Statistics, 
14, 124-137. 

20 
Gelfand & Ghosh 
Lindsey, J .K. (1995). Mode/ling Frequency and Count Data. London, Clarendon 
Press. 
Lo, A.Y. (1984). On a class of Bayesian non parametric d~nsity estimation:!, density 
estimates. The Annals of Statistics, 12, 351-357. 
MacEachern, S.N. and Miiller, P. (1994). Estimating mixture of Dirichlet process 
models. Technical Report, Institute of Statistics and Decision Sciences, Duke 
University. 
Mallick, B.K. and Gelfand, A.E. (1994). Generalized linear models with unknown 
link functions. Biometrika, 81, 237-245. 
McCullagh, P. and J .A. Neider (1989). Generalized Linear Models. Chapman and 
Hall, London. 
Meng, X-L. (1994). Posterior predictive p-values. Annals of Statistics, 22, 1142-
1160. 
Mukhopadhyay, S. and Gelfand, A.E. (1997). Dirichlet process mixed generalized 
linear models. Journal of the American Statistical Association, 92, 633-639. 
Miiller, P. and G. Parmigiani (1995). Numerical evaluation of information theoretic 
measures. Inc: Bayesian Statistics and Econometrics: Essays in Honor of A. 
Zellner. Eds: Berry, D.A., Chaloner, K.M., Geweke, J.F., John Wiley, New 
York, 397-406. 
Natarajan, R. and McCulloch, C.E. (1995). A note on the existence of the posterior 
distribution for a class of mixed models for binomial responses. Biometrika, 
82, 639-643. 
Neider, J .A. and Wedderburn, R.W.M. (1972). Generalized linear models. Journal 
of the Royal Statistical Society, Series A, 135, 370-384. 
Newton, M.A., Czado, C. and Chappell, R. (1996). Semiparametric Bayesian in-
ference for binary regression. Journal of the American Statistical Association, 
91, 142-153. 
Piergorsch, W.W., Weinberg, C.R. and Margolin, B.H. (1988). Exploring simple 
independent action in multifactor table of proportions. Biometrics, 44, 595-
603. 
Raftery, A.E. (1995). Hypothesis testing and model selection. In: Markov Chain 
Monte Carlo in Practice. Eds. W. Gilks, et al. London, Chapman and Hall, 
163-187. 
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 
6, 461-464. 
Spiegelhalter, D.J ., Best, N .G. and Carlin, B.P. (1998). Bayesian deviance, the 
effective number of parameters and the comparison of arbitrarily complex 
models. Preprint. 
Weiss, R.E. (1995). Residuals and outliers in repeated measures random effects 
models. Tech. Rpt., Dept. of Biostatistics, UCLA. 

1. Bayesian GLMs 
21 
West, M. (1985). Generalized linear models: scale parameters, outlier accomodation 
and prior distributions. In Bayesian Statistics,2, Oxford, Oxford University 
Press, 531-557. 
Zeger, S.L. and Karim, M.R. (1991). Generalized linear models with random ef-
fects: a Gibbs sampling approach. Journal of the American Statistical Asso-
ciation, 86, 79-86. 

This Page Intentionally Left Blank 

2 
Randotn Effects in Generalized 
Linear Mixed Models ( G LMMs) 
Dongchu Sun 
Paul L. Speckman 
Robert K. Tsutakawa 
ABSTRACT In this chapter, we examine the use of special forms of correlated ran-
dom effects in the generalized linear mixed model (GLMM) setting. A special feature 
of our GLMM is the inclusion of random residual effects to account for lack of fit 
due to extra variation, outliers and other unexplained sources of variation. For ran-
dom effects, we consider, in particular, the correlation structure and improper priors 
associated with the autoregressive (AR) model of Ord {1975) and the conditional 
autoregressive (CAR) model of Besag {1974). We give conditions for the propriety 
of the posterior distribution of the GLMM when the fixed effects have a constant 
improper prior and the random effects have a possibly improper conditional au-
toregressive prior. Several examples of exponential families as well as computational 
details for Markov chain Monte Carlo simulation are also presented. 
1. 
Introduction 
Traditional treatment of random effects in mixed linear and nonlinear models 
generally assumes that these effects are independent following some standard dis-
tributions such as normal or gamma. However, with the advent of Markov chain 
Monte Carlo (MCMC) methods and, in particular, the Gibbs sampler ( cf. Gelfand 
and Smith, 1990), such restrictions are no longer necessary, and a much broader class 
of models, including those with correlated random effects, can be used in practice. 
(See Clayton (1996) for a general review of this recent development.) 
In this chapter we consider generalized mixed linear models with random effects 
having the autoregressive and conditionally autoregressive properties commonly en-
countered in temporal and spatial covariates where one expects similarities among 
closely situated observations. Examples from disease mapping will be used to mo-
tivate these models. 
The computational simplicity of MCMC methods enables one to extend the com-
monly used generalized linear mixed model (GLMM) to one that appends random 
residual effects to the linear term to account for lack of fit. These extra terms al-
low for the minor perturbations and occasional outliers commonly encountered in 
practice. However, the remarkable ease of application of the Gibbs sampler does 
not come without a price: There is potential nonconvergence and other annoying 
23 

24 
Sun, Speckman & Tsutakawa 
problems when using the algorithm, especially in situations where noninformative 
prior distributions are employed. 
In Section 2 we formally define the GLMM with residual effects. Two examples 
are given. One has the normal distribution and the other the gamma distribution, 
with the choice depending on the nature of the observed data. For example when 
the data are Poisson, it is more natural to use the conjugate gamma distribution, 
although the normal may be just as appropriate and simple to use. 
In Section 3 we discuss several forms of correlated random effects including the 
AR process of Ord (1974) and the CAR process of Besag (1974), which are useful in 
describing spatial correlations. We examine the joint distributions associated with 
these processes to get a better understanding of the underlying association implied 
by these models. Of particular interest are distributions that are improper and could 
create problems when used in the GLMMs. 
In Section 4, we consider the incorporation of these spatial random variables 
into the GLMM setting and emphasize the special role of the link function in a 
Bayesian hierarchical framework. In the case where the residual effects are normally 
distributed, the fixed effects have a constant prior and random effects may have an 
improper prior, we give sufficient conditions for the existence of a proper posterior 
distribution of all parameters including the fixed and random effects and variance 
components. 
In Section 5, we summarize the computational details including the full condi-
tional distributions required for the implementation of the Gibbs Sampler. 
2. 
The Model 
Let Y1, ... , Y N be the independent random observations, where Yi has the prob-
ability density 
(1) 
The function Ai(¢J) is commonly of the form Ai(¢J) = ¢Jw;l, where the Wi are 
prespecified weights. It is often assumed that the scale parameter ¢J is known. Con-
sider, for example, the case when the population size in area i is mi with un-
known mortality rate Pi, and Yi is Poisson distributed with mean miPi. This is a 
special case of (1) with ¢J = 1, Ai(¢J) = 1, 1Ji = log(miPi), Bi(1Ji) = exp(7Ji), and 
Ci(Yi; ¢J) = -log(yi !) . When Yi has a binomial distribution with parameters mi 
and Pi, ¢J = 1, Ai( ¢J) = 1, 1Ji = log{p£/(1 -Pi)}, Bi( 1Ji) = mi log{ 1 + exp( 7Ji)}, and 
Ci(Yi; ¢J) = log[mi!/{yi!(mi -yi)!}]. 
Generalized Linear Models. We wish to model the variability in 77i to account for 
various fixed covariates. The natural parameters 7Ji are modeled as 
hi(1Ji)=ziill, 
(2) 
where the hi are known monotone functions, X 1 = (z11 , ... , Zln)t is anN xp design 
matrix and IJ is the vector of fixed effects. Such a model is commonly referred to as 
a generalized linear model (GLM) with canonical parameter 7Ji, scale parameter¢, 
and link function hi (cf. McCullagh and Neider, 1989). (Note that usually there is 
a single link function hi ·= h.) 
Generalized Linear Mixed Models. We now extend the model to include random 
effects as follows. Let 
(3) 

2. Random effects in GLMMs 
25 
where hi is a known monotone function, X 1 = (zn, ... , Zln)t and X 2 = (z21, ... , Z2n)t 
are N x p and N x k design matrices, the p x 1 vector 9 represents fixed effects, 
and Z is a k x 1 vector of random effects. Models given by (1) and (3) are often 
called generalized linear mixed models (GLMMs) and have been widely used m 
many problems such as disease mapping e.g., Breslow and Clayton (1993). 
We can further extend the model to add additional residual effects by taking 
(4) 
Here e = ( e1 , ... , eN )t are residual effects satisfying some restriction such as IE( ei) = 
0 or IE exp( ei) = 1. In addition, Z and e are assumed mutually independent. We 
include random residual effects ei to account for the lack of fit of (3) due to extra 
variation, outliers, and other unexplained sources of variation. Note that the random 
effect ei is quite different from Z in the sense that Z often accounts for some special 
pattern such as random geographical effects and spatial correlation. In addition, the 
number of components of Z is often much smaller than N, the number of residual 
effects ei. By a suitable choice of the design matrix, ( 4) may be encompassed under 
(3), but we do not do this in order to emphasize the separate roles of Z and e. We 
will call the model given by (1) and ( 4) a GLMM as well. 
There are many possible choices for the link functions hi in models (2)-( 4). For 
example, in the mortality setting cited earlier, Yi has the Poisson distribution with 
mean miPi and 'f/i = log(miPi)· One possibility is to take hi(r/i) = ru- log(mi) = 
log(Pi), and a loglinear regression model may be applied. Alternatively, Yi can 
be modeled with a binomial distribution. Then the logit link is canonical, and 
logit(pi) = log{pi/(1- Pi)} = ru - log(mi - e'1i) = hi(r/i), resulting in logistic 
regression. 
The random effects term Z in (3)-( 4) is typically assumed to have a multivariate 
normal distribution. We will discuss in detail the choice of the distribution of Z in 
the next section. 
Distribution of Residual Effects. We will assume that the residual effects ei or 
some monotone functions of ei have distributions belonging to an exponential family 
(1), with known common canonical parameter 1J but unknown scale parameter¢. For 
illustration, we will consider the following two classes of distributions for residual 
effects. 
• Normal Residual Effects. Residual effects ei are independent and identically 
normal with mean 0 and variance 60 • 
• Gamma Residual Effects. The exp(ei) are iid gamma(R, R). Here a random 
variable W has the gamma( a, j3) distribution if W has p.d.f. 
Special cases of these models have appeared previously. Clayton and Kaldor 
(1987) and Waller et al. (1997) use a Poisson-normal model (Poisson for Yi and 
normal for Z) but without the residual term e. This a special case of (3). Ghosh 
et al. (1998) use e in the binomial-normal model and treat spatial effects by taking 
X 2Z = U, with U having a distribution defined by the conditional auto regressive 
CAR(1) model of Besag (197 4). This is a special case of ( 4). In Sun, Tsutakawa, 
Kim and He (1997) and Sun, Tsutakawa and He (1998), Z consists of block-wise 
independent random effects, where each block contains random effects and the ei 
are independent random variables with mean 0 and a common variance. West and 

26 
Sun, Speckman & Tsutakawa 
Aguilar (1997) give another interesting example analysing hospital quality monitors 
with an extra residual term in (1.4). 
Special cases of Poisson-gamma models are found in Clayton and Kaldor (1987) 
and Tsutakawa (1988). Specifically, in Tsutakawa (1988), Z contains independent 
random effects, and the exp( ei) are independent gamma variables with mean 1 and 
a common variance. 
The general model (1) and (2) can be used for both continuous and discrete data. 
A discrete example of ( 4), which motivated much of this work, is studied in Sun, 
Tsutakawa, Kim and He (1998), where a spatio-temporal model for cancer mortality 
data is proposed. For a given gender, let YiJk denote the frequency of deaths from 
some specific cause in the ith region and jth age group during the kth time period, 
i = 1, ... , I; j = 1, ... , J; k = 1, ... , K. Conditionally on the fixed and random 
parameters, assume the Yiik are independent and Poisson with means miikPijk, 
where mijk is the size of the ijkth target population. The model of Sun eta/. takes 
the form 
log(Pijk) = Oj + Zi +(Pi+ Wij)(tk -l) + eijk, 
where ()j is the effect of the jth age group, zi is the effect of the ith region, tk is the 
midpoint of the kth time period, and l = Ef=l tk/ I<. The rate of change over time 
is represented by (JJJ + Wij) for the jth age group in the ith region. Both ()j and J.lj 
are treated as fixed effects, while Zi and Wij are random. The residual effects eij k 
are also random. A detailed description of the distributions of the random effects 
and prior distributions are given in Sun et al. (1998), where disease mapping and 
interpretation of numerical results for male lung cancer in the state of Missouri can 
be found. 
3. 
Random Effects 
3.1 
Independent Random Effects 
Historically, it was common to assume independent random effects for linear 
mixed models, i.e., zl' ... ' ZN are independently and identically N(O, 61) distributed. 
(See Harville (1977).) Typical examples include one-way ANOVA and two-way 
ANOVA models with random effects. Hobert and Casella (1996) gave necessary 
and sufficient conditions for the propriety of the posterior distribution for a class of 
noninformative priors for variances components assuming independence of random 
effects. 
3.2 Correlated Random Effects 
There are many important situations where the random effects should be modeled 
as correlated. Correlated models are especially appropriate for spatial effects. A 
number of related methods are commonly used. 
Direct specification of correlation matrix. If the random effects are linearly or-
dered, as for example with longitudinal data, it may be convenient to specify a 
correlation structure directly. For example, to model correlation decreasing with 
distance, Z = (Z1, ... , Zk)t can be taken to have the MVN(O, :E) distribution, 
where :E = ( O"ij) is the k x k matrix with elements 
0" .. -
rpli-il 
ZJ-
' 
(5) 

2. Random effects in GLMMs 
27 
and r > 0 and p E ( -1, 1) are constants. For MCMC methods with modest size k 
(say, k < 100 or so), it is sometimes feasible to generate Z from the joint conditional 
distribution directly. A number of authors including Cressie and Chan (1989) have 
used the distance between area i and area j to introduce spatial correlation. 
AR models. Again assuming a linear ordering for the components of Z, a com-
monly used structure is the AR(l) model with 
zi = pZi-1 + Ei, i = 2, ... , k, 
(6) 
where p is a constant in ( -1, 1), and the Ei are independent and identically N ( 0, 61) 
distributed. If Z1 f".J N(O, 61/(1 - p2)), the distribution of Z is given by (5) with 
T = 61/(1 - p2 ). 
Ord (1975) proposed a generalized AR(1) model by defining 
k 
zi = p .2:: ci1 z 1 + Ei, 
j=1 
(7) 
where the Cij are fixed constants satisfying cii = 0, and €1, ... 'Ek are iid N(O, 61). 
Here p is a "correlation coefficient," measuring the correlation among Zi in the sense 
that the larger IPI is, the stronger the correlation among the components of Z. For 
example, if the Zi are linearly ordered, one can define their joint distribution by 
assuming 
z1 
= 
pZ2 + E1, 
Zi 
= 
p(Zi-1 + Zi+l) + Ei, i=2, ... ,k-1, 
(8) 
zk 
= 
pZk-1 + fk. 
One advantage of (7) is that the formulation generalizes easily to two or more 
dimensions. Taking C = (Gil) to be the k x k matrix of coefficients, I the k x k 
identity matrix, and 
Wp=I-pC, 
(9) 
model (7) is equivalent to W pZ = ( E1, ... , Ek )t. If W P is nonsingular, Z has 
a multivariate normal distribution with mean zero and covariance matrix E = 
61 ( W~ W P) - 1. A common choice of C is the adjacency matrix A = ( aiJ )k x k, de-
fined by 
a·. _ { 1, if j is adjacent to i, 
(10) 
~3 -
0, otherwise. 
The class of distributions for Z when W P = I - pA has been used in modeling 
random regional effects in disease mapping by Sun, Tsutakawa, Kim and He (1997) 
and random county effects in hunting success rates from a turkey hunting survey in 
the State of Missouri by He and Sun (1998). 
One appealing way to view the prior for Z is through the conditional distributions 
of Zi given Z -i = (ZJ, j =f:. i). For the simple AR(1) prior (6), it can be shown that 
B = 61 E- 1 is a tridiagonal matrix with diagonal elements (1, 1 + p2 , ••• , 1 + p2 , 1) 
and off-diagonal elements -p. It follows easily that Z has the Markov property 
ZdZ-i f".J N(l:p2 (Zi-1 +Zi+l), 1!1p2 ), fori= 2, ... ,k-1, 
that is, the conditional distribution of Zi given the rest depends only on adjacent 
variables. Curiously, the generalized AR prior specified through the adjacency ma-
trix in (8) does not have a similar Markov property. This follows from the fact 

28 
Sun, Speckman & Tsutakawa 
that :E- 1 = 81
1 W~Wp is a banded matrix but is not tridiagonal. Instead, Zi!Z-i 
depends on (Zi-2, Zi-b Zi+1, Zi+2) for 3 :5 i :5 k- 2. 
CAR{l) model. In an effort to use priors with the appealing first-order Markov 
property in spatial modeling, many authors have adopted conditional autoregressive 
or CAR models, which are developed by specifying the conditional distributions 
directly in a (presumably) consistent manner. One popular model takes 
(11) 
where Cij and di > 0 are constants satisfying Cii = 0. This is a special case of 
Besag's (1974) model with 
(12) 
i = 1, ... , k. Suppose B is the k x k matrix with diagonal elements ai and ijth 
off-diagonal elements -ai/3ij. Besag proved that if B is symmetric and positive 
definite, these conditional distributions lead to the joint probability density of Z, 
(13) 
i.e. Z"' MVN(O, 61B- 1 ). In the context considered here, suppose 
B=Bp=D-pC, 
(14) 
where Dis a k x k diagonal matrix with positive elements (d1, ... , dk), and Cis a 
symmetric matrix with Cu = 0. If B P is positive definite, then the joint distribution 
of Z is (13), and the conditional distributions of Zi given Z -i are (11). 
In practice, these models are important because the simple conditional distribu-
tions depending only on neighboring values for the Zi are desirable for Bayesian 
analysis using Markov chain Monte Carlo methods. Here are two important cases. 
Case 1. Assume that C = A, the adjacency matrix, and di = Lj Cij. If p E 
( -1, 1), then B is positive definite and the conditional distribution of Zi given Z -i 
is N(pZi, 81/ni), where ni is the number of neighbors of location i, and Zi is the 
mean of the ni neighboring Z;s. (This corresponds to ai = ni and /3ij = p/ni if j 
is adjacent to i and zero otherwise.) This model was studied in Besag (1975) and 
Ripley (1981). 
Case 2. Assume that C = A, the adjacency matrix, and D = I. Let A1 and Ak 
be the smallest and largest eigenvalues of C. If A1 1 < p < .A; 1 , then B is positive 
definite and the conditional distribution of Zi given Z -i is N(p Lj;ei aij Zj, 81). This 
model was used in Ripley (1988). 
However, there are potential problems in modeling the dependence among the 
Zi through the choice of ( ai, /3ij) in ( 12). One problem, singularity of B, is ad-
dressed further in the next section. Another possible problem is with specifying a 
symmetric matrix B. The specification in Case 1 with conditional variance 81/ni 
seems unrealistic when pis small, since in the limiting case p = 0, the Zi are inde-
pendent but the variances still depend on the number of neighbors. This may not 
make sense near boundaries or in nonregular cases. On the other hand, in Case 2 
the conditional variance of Zi does not depend at all on the number of neighbors. 
As an alternative, suppose we let the conditional distribution of Zi given Z -i be 

2. Random effects in GLMMs 
29 
N(pZi, 61 (1 + p/ni)) and assume 0 5 p ~ 1. Formally, this is equivalent to a CAR 
model with C¥i = (1 + pjni)- 1 and 
13 .. _ { p/ni, if j is adjacent to i, 
' 3 -
0, 
otherwise. 
Unfortunately, the ijth off-diagonal element of B, aif3ij = pj(p + ni), is not equal 
to a j f3J i unless ni = nj. Even in the linearly ordered case, this fails at the boundary 
where n1 = 1 and nj = 2 for 1 < j < k. Thus care must be taken in specifying a 
CAR model. 
3.3 Strongly Correlated Random Effects 
If the determinant IBI is zero, the set of full conditional distributions given by 
(12) is not "compatible," a definition used by Arnold and Press (1989), in the sense 
that there is no joint density of Z consistent with the corresponding conditional 
densities. However, there are situations in practice where a nonpositive definite B 
is desirable. For example, if p---+- 1 in (14) when C = A, the adjacency matrix, and 
D is the diagonal matrix of row sums of C, the model is a Markov random field. 
Clearly B is singular. 
When the matrix B is nonpositive definite, there are two possible interpretations. 
One way is to consider a lower dimensional distribution, in the sense that it is proper 
in certain directions but degenerate in some other directions. For example, let r be 
the rank of B, and let )q, ... , .Ar be the positive eigenvalues of B. Write B = r Art, 
where r = (71, .•. , 7k) is an orthogonal matrix, and A= diag(..:\1, ... , .Ar, 0, ... , 0). 
Let r1 = (71, ... , 7r) and A1 = diag(..:\1, ... , .Ar ). Then B = r1A1ri. Now let U 1 = 
(U1, ... , Ur )t be a vector of independent random variables where Ui """N(O, 61..:\i1 ). 
Then Z = r 1U 1 has a singular normal distribution with mean 0 and covariance 
matrix 61 B-, where B- is a pseudo-inverse of B. We often write this distribution 
as MVN(O, 61B-). The joint distribution has the form 
(15) 
where IBI+ is defined to be n~=1 Ai, the product of all positive eigenvalues of 
B. Note that based on such a singular normal distribution, the full conditional 
distribution of Zi given Z -i is degenerate instead of a normal distribution. The 
distribution of Z is essentially proper on a lower dimensional space, so Z is a vector 
of strongly correlated random effects. 
Alternatively, we can sample an additional random sample U 2 = (Ur+1 , ••• , Uk)t 
from a flat constant density over a k- r dimensional Euclidian space. Now define 
Z = r(Ui, U~)t. We can see that the joint density of Z has the form (15), which 
is improper because B is singular. However, we can formally relate (15) to (12) by 
noting that 
f(ZdZ-o) = f(Z,, ... , Zk) j j_: f(Z,, ... , Zk)dZ;. 
Hobert and Casella (1998) have called this type of relationship "functionally com-
patible," in contrast to one being "compatible." 
Markov random field models. In (14), if C =A= (aij),di = L'¢i aij, and 
p = 1, then the distribution of Z is often called a Markov random field model ( cf. 

30 
Sun, Speckman & Tsutakawa 
Kindermann and Snell (1980)). Such models have been used for modeling spatial 
correlations in disease mapping and other contexts by Besag, York, and Mollie 
(1991) and used by Bernardinelli and Montomoli (1992), Bernardinelli et a/. (1995), 
Carlin and Louis (1996), Waller et al. (1997) and Ghosh et al. (1998) among others. 
A utocorrelated random effects. We next give a class of strongly correlated distri-
butions. Define the backwards difference operator H k to be the k x k matrix 
and let 
0 0 
1 0 
0 
be the lower (k- d) rows of the k-dimensional identity matrix h. Now let the 
structural matrix B in (15) be 
(16) 
Because Bka. has rank k - d, Z has a singular distribution, which we write as 
MVN(O, 61B'kd), where n;;d is a pseudo-inverse of Bkd· 
AR(l) (The first order difference model). When d = 1, the prior on Z is called 
the first order difference or random walk prior. See Clayton (1996). In this case the 
structural matrix has the form 
c 
-1 
0 
0 ... 
0 
0 
0 ) 
-1 
2 
-1 0 ... 
0 
0 
0 
Bk1 = · 
... 
0 
0 
0 
0 
-1 
2 
-1 
0 
0 
0 
0 
0 
-1 
1 
kxk 
AR(2) (The second order difference) Model. 
When d = 2, the prior on Z is 
called the stochastic-trend or second order difference prior. See Clayton (1996). In 
this case the structural matrix has the form 
1 
-2 
1 
0 
0 
0 
0 
0 
0 
0 
-2 
5 
-4 
1 
0 
0 
0 
0 
0 
0 
1 
-4 
6 
-4 1 
0 
0 
0 
0 
0 
Bk2 = 
0 
0 
0 
0 
0 
1 -4 
6 
-4 
1 
0 
0 
0 
0 
0 
0 
1 
-4 
5 
-2 
0 
0 
0 
0 
0 
0 
0 
1 
-2 
1 
kxk 
Note that this model can be obtained by the iterated formula 
zi = 2zi-1 - zi-2 + Ei, i = 3, ... , k. 
Such a second order random effects prior has been used for patient monitoring by 
Berzuini (1996). 

2. Random effects in GLMMs 
31 
3.4 
Some Examples of the AR{d) Model 
To see the differences among the AR( d) models when we change d, in Figure 2.1 
we graph three sample paths of the AR( d) process prior for Z using k == 100, 61 == 1 
and d = 1, 2, 3. Note that the rank of Bkd is k- d. When d == 1, the sample paths 
are simple random walks and locally rough, but sample paths are smoother when 
d?. 2. 
4. 
Hierarchical GLMMs 
Bayesian analysis for the GLMs (1)-(2) and the GLMMs given by (1) and (3) is 
studied in Clayton (1996). We will discuss the GLMMs given by (1) and ( 4). For 
illustration, we will consider normal residual effects ei. A full hierarchical Bayesian 
approach requires the specification of prior distributions for IJ, the variance 6o of 
the distribution of ei, the variance 61 of the distribution of random effects Z, and 
the scale parameter ¢. 
Although the commonly used prior of the fixed effects 0 is normal, we will assume 
a noninformative prior for 0, in particular, one having a constant density. We will 
not give a specific form for the priors of (6o, 61, ¢).Since the prior for 0 is improper, 
and the prior for Z is also improper for a singular B, the joint posterior distribution 
may still be improper. As noted by Hobert and Casella (1996) and Sun, Tsutakawa 
and Speckman (1997), the propriety of the posterior is very important in Bayesian 
computation, especially when Markov chain Monte Carlo methods are used. Sun, 
Tsutakawa and Speckman (1997) considered a one-parameter distribution family 
where the prior for the parameters follows a linear mixed model and found conditions 
for a proper posterior distribution. Here we extend the results to the GLMM model, 
where the observations follow the densities (1) with canonical parameters 7Ji and a 
common scale parameter¢. We will only consider the case where there is a common 
variance component 61 for the whole vector of Z. Some generalizations to block 
random effects can be found in Sun et a/. (1998). We use the following notation. 
Note that for Bi(·) defined in (1), the first derivative B: is a strictly increasing 
function. Let Hi be the inverse function of B:. Note that for any fixed ¢, the 
likelihood function fi(YdrJi, ¢) is bounded by 
Mi(¢) 
-
sup fi(YdrJi, ¢) 
'tJi 
Theorem 4 .. 1 Consider the GLMMs {1) and {4) with normal residual effects ei iid I"V 
N(O, 8o). Assume that 
{a) there exists a subset of{1, ... ,N}, say :ln = (i1, ... ,in), such that 
where F( ·) is the prior distribution for ¢; 
(b) the design matrix xr = (.z1,ip ... , Z1,iJt has ful/ rank p, and x; = (.z2,ip ... , Z2,in)t
has the same rank as the matrix X 2 = (z 2,t, ... , z2,N )t; 
(c) the prior for 0 is a constant and Z follows the density {15); 

32 
Sun, Speckman & Tsutakawa 
100 
50 
0 
-50 
-100 
10 
5 
0 
-5 
-10 
(b) d=2 
0 20 
60 
(a) d=1 
0 20 
100 
60 
1000 
500 
0 
-500 
·1000 
100 
(c) d=3 
0 20 
60 
FIGURE 2.1. Sample paths of CAR (d) models for Z when n = 100. 
100 

2. Random effects in GLMMs 
33 
(d) the rank of (x;t R1X; +B) is k, where R1 =In- Xr{xrt xn- 1 xrt; 
(e) the prior for (8o, 81) satisfies the moment condition, 
1E{ 
~- !(n-p-k) ~- !k 
~- !(n-p)} 
u0 
u1 
+ u0 
< oo. 
(19) 
Then the posterior distribution of( q, ¢>, 9, Z, 8o, 81) given Y = (y1, ... , YN) is proper. 
Proof. Without loss of generality, assume that :Tn = { 1, ... , n} in assumption 
(a) and that 8i has a prior density 9i· The posterior density of (q, 9, Z, 8o, 81) given 
(Y, ¢) is 
N 
II 
I 
) 
1 
-.!.(N-n) 
p(q, 9, Z, 8o, 8dY, ¢>)ex: 
fi("Yi 7Ji, ¢> hi(7Ji)80 
2 
x 
i=1 
N 
1 
II exp [- 2~ {hi(7Ji)- xL9- x~iZ}
2
] II g;(8; )G. 
i=n+l 
0 
j=O 
where 
Here V* = (h1(7Jl), ... , hn(7Jn))t. Let q* = (7]1, ... ,7Jn)t. Using inequality (17) and 
integrating with respect to ( 7Jn+1, ... , 7JN )t, 
N 
n 
p(q*, 9, Z, 8o, 8dY, ¢)ex: II Mi(¢) II /;(Yj I7Jj, ¢)hj(7Jj) G. 
i=n+1 
j=1 
Using arguments similar to those in Sun, Tsutakawa and Speckman (1997), we get 
Therefore, from assumption (e), 
j j p(q*, t/>IY)dq* F(d¢>) 
ex J.fJ_, M;(t/>){.D. J 
f; (Y; jr7;, t/>)hj(~; )d~;} F(dt/>), 
which is finite by (18). 
D 
Remark 4 .. 1 A common prior for the variance components 8i is inverse gamma(ai, bi), 
whose density is 
(20) 
Clearly, when bi > 0, n- p- k + 2ao > 0 and k > 2a1, condition {19) holds. 

34 
Sun, Speckman & Tsutakawa 
Remark 4 .. 2 When the prior of¢> is degenerate, i.e., a known constant as in the 
Poisson or binomial cases, condition (18) becomes 
which is equivalent to the condition, 
J 
exp[Aj(t/>)- 1{Yj1Jj- Bj(1Jj)}]hj(rJj)d1Jj < oo, for j E Jn· 
(21) 
A condition similar to (21) was required for all j in Ghosh et al. (1 997) for propriety 
of the posterior distribution. 
Example 4 .. 1 Suppose fi(Yd1Ji, ¢) is Poisson with mean f..Li = miPi· This is a 
special case of(1) with¢>= 1 and 1Ji = log(miPi)· Let hi(rJi) = rJi-log(mi) = log(pi)· 
Here¢>= 1 and hi(rJi) follows the linear structure (4). Then fi(Yd1Ji, ¢)is bounded 
for any Yi ~ 0, and 
which is finite for Yi > 0. Under assumptions (b)-(e) of Theorem 4 .. 1 , the joint 
posterior distribution of (p1, ... ,pN, {}, Z, Do, D1) is proper. 
Example 4 .. 2 Suppose fi(YdrJi, ¢)is binomial with parameters mi and Pi· This is 
a special case of (1) with¢>= 1 and 1Ji = log{pi/(1- Pi)}. Assume hi(rJi) = 1Ji has 
structure (4). Then fi(YdrJi, ¢> = 1) is bounded in 1Ji for any 0::; Yi ~ mi, and 
which is finite if and only if 0 < Yi < mi. Under assumptions (b)-(e) of Theorem 
4 .. 1, the joint posterior distribution of(pl, ... ,pN,O,Z,Do,Dl) is proper. 
Example 4 .. 3 When Yil(f..Li,o-2)"' N(f..Li,o-2), we have 1Ji = f..Li, ¢> = o-2 , Ai(¢>) = ¢>, 
Bi(rJi) = 1Ji and Ci(Yi, ¢) = -0.5log(¢)- y[ /(2¢). If hi(rJi) = 1Ji, this is a typical 
example of a normal hierarchical model. It is easy to see that Mi(¢) = 1jy'2ii($ and 
J fi(Yd1Ji,¢>)d1Ji = 1. Condition (18) becom~s 
l" q,-!(N-n) F(dtf>) < 00, 
which always holds when N = n and F is a proper prior for ¢. In addition, as-
sumptions (b)-( e) of Theorem 4 .. 1 hold. Then the joint posterior distribution of 
(f..Ll, ... , f..LN, o-2, 9, Z, Do, Dl) is proper. 

2. Random effects in GLMMs 
35 
Example 4 . .4 Suppose }i I(Jli, a) ""'gamma( a, a/ Jli), with density 
Here a is the common shape parameter and /Ji is the mean of }i for given (Jli, a). 
This is a special case of (1) with¢>= a, 7Ji = 1/JJ;,, Ai(¢>) = -1/¢, Bi(7Ji) = log(7Ji) 
and Ci(Yi, ¢>) = a log( a) + (a - 1) log(yi) - log{f( a)}. Choose hi( 7Ji) = log( 7Ji) = 
-log(Jli)· Then 
If N = n and ¢> has a proper prior, condition (18) holds. If assumptions (b)-( e) in 
Theorem 4 .. 1 hold, the joint posterior distribution of (Jl1, ... , Jl N, a, fJ, Z, 8o, 81) is 
proper. 
When ¢> is unknown but }i has a continuous distribution, as in the normal and 
gamma examples, we often choose n = N and :In = {1, ... , N}, so that condition 
(18) becomes 
Remark 4 .. 3 Assumption (d) in Theorem 4 .. 1 is crucial. Otherwise, the results 
may not hold. On the other hand, it is easy to see that the rank of the matrix 
(X~R1X 2 +B) equals k if either the rank of (X 1, X 2) is p + k or the rank of 
(B) = k. The following results can be proved similarly. 
Theorem 4 .. 2 Assume that the rank of (X~R1X 2 +B) < k, where R1 = IN -
X1(XiX1)- 1 Xi, and X 1 and X2 are design matrices based on the full data. Under 
assumption (d), for any proper prior of ( 8o, 81, ¢> ), the posterior is improper. 
Proof. Let G be defined as in the proof of Theorem 4 .. 1, and replace n by N 
and (Xi,X;) by (X1, X 2).l,FromSun, Tsutakawa and Speckman (1997), we know 
that for any given ( 8o, 81), 
f 
f 
GdfJdZ = oo. 
Jrn,r' lJR" 
The results follows. 
0 
The following result can be proved using the same argument as that of Theorem 
4 .. 1. 
Theorem 4 .. 3 Given assumptions (a), (b) and (c) of Theorem 4 .. 1, suppose that 
either condition ( d1) or ( d2) below holds: 
(d1) the rank of (X;t R 1X;) is k and {19) is replaced by 
(22) 

36 
Sun, Speckman & Tsutakawa 
{d2) the rank of B is k and {19) is replaced by 
IE{ 6~ !(n-p)} < 00. 
(23) 
Then the result of Theorem 4 .. 1 still holds. 
5. 
Bayesian Computation 
Bayesian inference for hierarchical GLMMs can be implemented via Markov 
chain Monte Carlo methods such as Gibbs sampling and/or the Metropolis algo-
rithm. We assume that the prior for the variance components Di follows an inverse 
gamma(ai, bi) distribution with density (20). The proof of the following fact is omit-
ted. 
Fact 5 .. 1 The full conditional distributions are as follows. 
1. 8j(q, ¢, Z, 6o, 61),..,., MVNp((XiX 1)-1 Xi(V- X2Z), 6o(XiX 1)-1). 
2. Zj(q, ¢, 8, 6o, 61),..,., MVNA:(M 1X~(V- X 18), hoM 1), where M 1 = (X~X 2 + 
6o61 1B)- 1. 
3. hoi( 'I,¢, 8, Z, 61) ,..,., inverse gamma(ao + ~' bo + !(V -X 18 - X 2Z)t (V 
-X 18- X 2Z)). 
4. (6dq, ¢, 8, Z, 60 ),..,., inverse gamma(a1 + ~' b1 + !zt BZ). 
5. Given (¢,Z,60,61), the 7Jj (or v; = h;(7J;)) are independent. In fact, since 7Jj 
and v; are related by a one-to-one transformation, we can simulate from either 
7Jj or v; , depending on simplicity. The density of 7Jj given ( ¢, 8, Z, Do, 61) is 
and the density of v; given(¢, Z, 6o, 61) is 
where hj 1 is the inverse function of h;. 
6. If the prior for ¢ is degenerate, so is its posterior. If ¢ has the prior density 
g(¢), then its posterior density given (q, 8, Z, 60 , 61) is 
N 
g*(¢) ex g(¢) I1 exp[Ai(¢)- 1{Yi7Ji- Bi(7Ji)} + Ci(Yi; ¢)]. 
i=l 

2. Random effects in GLMMs 
37 
Sampling from a normal or inverse gamma distribution is very simple. In Part 5 
of Fact 5 .. 1, the conditional density of 7Ji or Vi is often log-concave. For sampling 
from a log-concave density, Gilks and Wild's (1992) adaptive method or Berger and 
Sun's (1993) direct method can be used. Here are Poisson and binomial examples. 
Example 4 •• 1 (continued). When hi(7Ji) = 7Ji -log(mi) = log(pi), 
si(7Ji) ex exp [Yi7Ji- e11i- 2~
0 
{7Ji -log(mi)- (zlill + z~iZ)}
2). 
Therefore 
Consequently, the conditional density of 7Ji given ( ¢, Z, 8o, 81) is log-concave. Since 
Vj is a linear transformation of 7Ji, the conditional density of Vj is also log-concave. 
Example 4 •• 2 (continued). When hi(7Ji) = 7Ji = log{pi/(1- pi)}, we have 
[ 
{7Ji - log(mi) - (zt1.tl + zt2.Z)} 2 ) 
si(7Ji) ex exp Yi7Ji- mi log(1 + e71i)-
280 
z 
z 
• 
We can show that 
So the conditional density of 7Ji == Vi given ( ¢, Z, 80 , 81) is log-concave. 
Example 4 .. 3 (continued). When hi ( 7Ji) = 7Ji, we have 
Clearly, the conditional distribution of 7Ji given others is normal with mean 8i(u2 + 
8i)- 1yi + u2(u2 + 8i)- 1(zlit1 + z~iZ) and variance u 28i(u2 + 8i)- 1. 
Example 4 .. 4 (continued). When hi(7Ji) = log(7Ji), we have 
[ 
{log(7J·) 
(zt II + zt Z)} 2 ] 
si(7Ji) ex exp -ayi7Ji +a log(71i)-
z -
2
~~ 
2i 
. 
This conditional density is not necessary logconcave. However, its transformation 
ei = log( 7Ji) has the conditional density 
It is easy to verify that /ir log{ si(ei)} = -ayie€• - 80
1 , which is negative. Con-
sequently, we can simply sample from the logconcave density of ei' then make the 
transformation 7Ji = e€•. 
For numerical illustrations of the Gibbs sampler discussed here, see the binomial 
application used in He and Sun (1998) and the Poisson example given in Sun et al. 
(1998). 

38 
Sun, Speckman & Tsutakawa 
References 
Arnold, B.C. and Press, S.J. (1989). Compatible conditional distributions, Journal 
of the American Statistical Association, 84, 152-156. 
Berger, J.O. and Sun, D. (1993). Bayesian Analysis for the Poly-Weibull Distribu-
tion. Journal of the American Statistical Association, 88, 1412-1418. 
Bernardinelli, L., Clayton, D. and Montomoli, C. (1995). Bayesian estimates of 
disease maps: how important are priors? Statistics in Medicine, 14, 2411-
2431. 
Bernardinelli, L. and Montomoli, C. (1992). Empirical Bay~s versus fully Bayesian 
analysis of geographical variation in disease risk. Statistics in Medicine, 11, 
983-1007. 
Berzuini, C. (1996). Medical Monitoring. In Markov Chain Monte Carlo in Prac-
tice, ed. by W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Chapman and 
Hall, 321-337. 
Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems 
(with discussion). J. Roy. Statist. Soc. Ser. B, 36, 192-236. 
Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician, 24, 179-
195. 
Besag, J., York, J. & Mollie, A. (1991). Bayesian image restoration, with two 
applications in spatial statistics (with discussion). Ann. Inst. Statist. Math., 
43, 1-59. 
Breslow, N.E. and Clayton, D.G. (1993). Approximate inference in generalized 
linear mixed models. Journal of the American Statistical Association, 88, 9-
25. 
Carlin, B.P. & Louis, T.A. (1996). Bayes and Empirical Bayes Methods for Data 
Analysis, London: Chapman and Hall. 
Clayton, D. (1996). Generalized linear mixed models. In Markov Chain Monte 
Carlo in Practice, ed. by W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. 
Chapman and Hall, 275-301. 
Clayton, D. & Kaldor, J. (1987). Empirical Bayes estimates of age-standardized 
relative risks for use in disease mapping. Biometrics, 43, 671-681. 
Cressie, N. & Chan, N.H. (1989). Spatial modeling of regional variables. Journal 
of the American Statistical Association, 84, 393-401. 
Gelfand, A.E. & Smith, A.F.M. (1990). Sampling based approaches to calculating 
marginal densities. Journal of the American Statistical Association, 85, 398-
409. 
Ghosh, M., Natarajan, K., Stroud, T.W.F., & Carlin, B.P. (1998). Generalized 
linear models for small area estimation. Journal of the American Statistical 
Association, 93, 273-282. 
Gilks, W.R. & Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Applied Statistics, 41, 337-348. 

2. Random effects in GLMMs 
39 
Harville, D.A. (1977). Maximum likelihood approaches to variance component esti-
mation and to related problems. Journal of the American Statistical Associa-
tion, 72, 320-338. 
He, Z., and Sun, D. (1998). Hierarchical Bayes estimation of hunting success rates 
with spatial correlations. Revised for Biometrics. 
Hobert, J.P. & Casella, G. (1996). The effect of improper priors on Gibbs sam-
pling in hierarchical linear mixed models. Journal of the American Statistical 
Association, 91, 1461-1473. 
Hobert, J.P. & Casella, G. (1998). Functional compatibility, Markov chains and 
Gibbs sampling with improper posteriors. J. of Computational and Graphical 
Statistics, 7, 42-66. 
Kindermann, R. and Snell, J.L. (1980). Markov Random Fields and Their Appli-
cations. Amer. Math. Soc., Providence. Rl. 
McCullagh, P. and Neider, J.A. (1989). Generalized Linear Models, Chapman & 
Hall, London. 
Ord, K. (1975). Estimation methods for models with spatial interaction. Journal 
of the American Statistical Association, 70, 120-126. 
Ripley, B.D. (1981). Spatial Statistics. Wiley, New York. 
Ripley, B.D. (1988). Statistical Inference for Spatial Processes. Cambridge Univer-
sity Press, Cambridge. 
Sun, D., Tsutakawa, R.K., and He, Z. (1998). Propriety of posteriors with improper 
priors in hierarchical linear mixed models. Submitted 
Sun, D., Tsutakawa, R.K., Kim, H. & He, Z. (1997). Spatio-Time Interaction with 
Disease Mapping. Submitted. 
Sun, D., Tsutakawa, R.K. and Speckman, P.L. (1997). Bayesian inference for CAR 
(1) models with noninformative priors. Biometrika, in press. 
Tsutakawa, R.K. (1988). Mixed model for analysing geographic variability in mor-
tality rates. Journal of the American Statistical Association, 83, 117-130. 
Waller, L.A., Carlin, B.P., Xia, H., & Gelfand, A.E. (1997). Hierarchical spatia-
temporal mapping of disease rates. Journal of the American Statistical Asso-
ciation, 92, 607-617. 
West, M. and Aguilar, 0. (1997). Studies of Quality Monitor Time Series: The VA 
Hospital System, Discussion Paper 97-20a, ISDS, Duke University. 

This Page Intentionally Left Blank 

3 
Prior Elicitation and Variable 
Selection for Generalized Linear 
Mixed Models 
Joseph G. Ibrahim 
Ming-Hui Chen 
ABSTRACT Generalized linear models serve as a useful class of regression models 
for discrete and continuous data. In applications such as longitudinal studies, obser-
vations are typically correlated. The correlation structure in the data is induced by 
introducing a random effect, leading to the generalized linear mixed model (GLMM). 
In this chapter, we propose a class of informative prior distributions for the class of 
GLMM's and investigate their theoretical as well as their computational properties. 
Specifically, we investigate conditions for propriety of the proposed priors for the 
class of GLMM's and show that they are proper under some very general conditions. 
In addition, we examine recent computational methods such as hierarchical centering 
and semi-hierarchical centering for doing Gibbs sampling for this class of models. 
One of the main applications of the proposed priors is variable subset selection. Novel 
computational tools are developed for sampling from the posterior distributions and 
computing the posterior model probabilities. We demonstrate our methodology with 
a real longitudinal dataset. 
1. 
Introduction 
Generalized linear models (McCullagh and Neider 1989; Neider and Wedderburn 
1972) are a unified approach to regression methods. They can be applied to a wide 
array of discrete, continuous, and censored outcomes, and are most commonly used 
when the outcomes are independent. However, in many applications, this indepen-
dence is not a reasonable assumption. This is particularly obvious in longitudinal 
studies, where multiple measurements made on the same individual are likely to be 
correlated. One recent technique for the analysis of such general correlated data is 
the generalized estimating equation approach introduced by Liang and Zeger (1986) 
and Zeger and Liang (1986). This approach has the desirable quality that allows 
for independence between subjects while introducing a correlation structure within 
subjects. A drawback to this approach, however, is that it assumes all subjects have 
the same covariance structure. For continuous outcomes with normal errors, Laird 
and Ware (1982) present the random effects model. In this model, a subject-specific 
4\ 

42 
Ibrahim & Chen 
covariance structure is generated by assuming that each individual has a unique 
set of regression coefficients, the random effects, which are distributed around the 
mean regression coefficients for the population, also known as the fixed effects. There 
may also be regression coefficients which are equal for all individuals. Conditional 
on the random effects, repeated observations on a subject are considered indepen-
dent, while marginalizing over the random effects, a unique covariance structure for 
the observations within each subject is obtained. 
Zeger and Karim (1991) present a generalization of the normal random effects 
model to the class of generalized linear models, generating a generalized linear mixed 
model (GLMM). They frame the model from the Bayesian perspective and fit it 
using a Gibbs sampler. They point out that attempts to fit this model using classical 
(frequentist) techniques are limited by the need for multidimensional numerical 
integrations, except in special cases. As a result of these analytically intractable 
integrations, classical analysis of GLMMs has relied on approximations to maximum 
likelihood techniques (see Breslow and Clayton 1993). 
In this chapter, we address the problem of informative prior elicitation and 
Bayesian variable selection for the class of generalized linear mixed models. Specifi-
cally, we discuss a class of informative prior distributions for the regression param-
eters that are extensions of the priors proposed in Ibrahim, Ryan, and Chen (1998) 
and Chen, Ibrahim, and Yiannoutsos (1999). We also give several attractive the-
oretical and computational properties of the priors, as well as give efficient com-
putational algorithms for computing posterior model probabilities in the variable 
selection problem. 
The construction of the prior distributions is based on the notion of the avail-
ability of historical data from a similar previous study or studies. As is well known, 
historical data are often available in applied research settings where the investigator 
has access to previous studies measuring the same response and covariates as the 
current study. For example, in many cancer and AIDS clinical trials, current stud-
ies often use treatments that are very similar or slight modifications of treatments 
used in previous studies. In carcinogenicity studies, large historical databases exist 
for the control animals from previous experiments. In all of these situations, it is 
natural to incorporate the historical data into the current study by quantifying it 
with a suitable prior distribution on the model parameters. Our proposed method-
ology can be applied to each of these situations as well as in other applications that 
involve historical data. 
The prior specification is based on the notion of specifying a vector of prior 
predictions, y0 , for the response vector of the current study, along with a covariate 
matrix Xo corresponding to yo. Then y0 and Xo are used to specify an automated 
parametric informative prior for the regression coefficients. The quantity Yo can be 
taken as the raw response vector from the historical data, a vector of fitted values 
based on the historical data, a vector obtained from a theoretical prediction model, 
or a vector specified from expert opinion or case-specific information. Thus Yo is 
viewed as a prior prediction for y, the actual data in the current study. Similarly, 
X 0 can be taken as the raw covariate matrix based on the historical data or it can be 
specified in other ways. In any case, taking y0 and Xo to be the raw historical data 
results in a more natural, interpretable, and automated specification. Throughout 
the remainder of this chapter, we will refer to Yo as a prior prediction, though it 
need not be a prediction in any formal sense. 
In Section 2.1, we present notation for the GLMM and give the likelihood. In 
Sections 2.2-2.4, we discuss the priors in detail, giving several theoretical properties. 
In Section 3, we discuss Bayesian variable selection and discuss novel computational 

3. Prior Elicitation and Variable Selection for GLMM's 
43 
methods for implementation. In Section 4, we demonstrate the proposed priors on 
a longitudinal dataset. We conclude this chapter with a brief discussion. 
2. 
Generalized Linear Mixed Models 
2.1 
Models 
First, we define the normal linear random effects model, then introduce random 
effects into generalized linear models. For individual i, with ni repeated measure-
ments, the normal linear random effects model for outcome vector Yi is given by 
Yi = Xd3 + Zibi + ei, 
i = 1, ... , N, 
where Yi is ni x 1, Xi is an ni x p matrix of fixed co variates, {3 is a p x 1 
parameter vector of regression coefficients, commonly referred to as fixed effects in 
these models, Zi is an ni x q matrix of covariates for the q x 1 vector of random 
effects bi, and ei is an ni x 1 vector of errors. We let Yit denote the tth component 
of Yi, t = 1, ... , ni. It is standard in implementations of this model to assume ei and 
bi are independent and that both are distributed Normal, with ei ,...., Nni (0, u 2 InJ 
and 
(1) 
where ! 8 is the s x s identity matrix and N8 (Jl, V) denotes the s-dimensional multi-
variate Normal distribution with mean Jl and covariance matrix V. Throughout the 
chapter, it will be more convenient to work with T = v- 1• Under these assumptions, 
(Yilf3, bi, u 2),...., Nni(Xi/3 + Zibi, u 2 InJ· 
Notice that marginally, 
(Yilf3, u 2 , T)""' Nni(Xif3, ZiTZ[ + u 2 lni), 
(2) 
(3) 
which shows the unique covariance structure for subject i. For convenience, we call 
model (2) the normal random effects model. 
Now suppose the sampling distribution of Yit, t = 1, ... , ni is from the exponential 
family, so that 
where 
P(YitiBit, r) = exp {r [yitOit- g(Oit)] + c(yit, r)} , 
dg( Bit) 
Jlit = E(YitiBit, r) = ~, 
_ 1 d2g(Oit) 
Vit = var(yit I Bit, r) = r 
dO[t 
, 
and r is a scalar dispersion parameter. 
In the generalized linear mixed model, the canonical parameter Bit is related to 
the covariates by 
eit = e("'it) , 
where 'f/it = x~tf3 + zitbi, and x~t and zit are rows of the Xi and Zi matrices, 8(·) is 
a monotonic differentiable function, often referred to as the B-link, and "'it is called 
the linear predictor. Throughout, we write 

44 
Ibrahim & Chen 
where 
(4) 
When fht = 'l]it, then the link is said to be the canonical link. For example, for the 
logistic regression model, we have 
P(YitLB, bi, T) = exp {Yit(X~tf3 + zitbi) -log(1 + exp(x~tf3 + zitbi))} , 
so that T = 1. 
Note that the GLMM imitates the normal random effects model in that we as-
sume that conditional on the random effect bi, the repeated observations on subject 
i are independent. For ease of exposition, we assume that T = To, where To is 
known as To = 1 in logistic and Poisson regression, and denote c(y) = c(y, To) and 
P(Yitlf3,bi) = P(Yitlf3,bi,To) in (4). Letting b = (b1, ... ,bN)', y = (Yu, ... ,YNnN)', 
X = (X1, ... , XN )', Z = (Z1, ... , ZN ), 17 = Xj3 + Zb, the joint density of (y, b) 
based on N subjects for the GLMM is 
N n, 
p(y,b I j3,T) = ITITP(Yitlf3,bi) p(bi IT), 
(5) 
i=l t=l 
where p(bi IT) is the distribution of bi in (1), with T = v- 1 . Letting 1] =X j3 + Zb, 
it will be convenient to write (2) in vector notation as 
p(y, b I j3, T) 
p(y I b, j3) p(b I T) 
= exp {To[y'0(1J)- J'g(0(17))] + J'c(y)} p(b IT) , 
(6) 
where J is a vector of ones, (}( 17) are element wise vectorized versions of those in ( 2), 
and 
N 
p(b IT)= IJ(27r)-qf2 IT 11/ 2 exp {b~Tbi}. 
i=l 
There are several attractive properties of (6). First, it takes within-subject corre-
lation into account while allowing each individual to have a unique correlation struc-
ture and maintaining independence between subjects. Second, the model accommo-
dates unbalanced data, in that response vectors need not be of the same length. 
Similarly, irregularly timed measurements can be fit within this model without any 
adjustment. Finally, posterior distributions or estimates of the random effects have 
interpretive value when the trend of the mean function of individuals is of interest. 
To induce the correlation structure on the responses, we integrate out the random 
effect, which leads to the likelihood 
p(y I j3, T) = f 
p(y, b I j3, T) db, 
JRNq 
where RN q denotes the N q dimensional Euclidean space. 
2. 2 
The Prior Distributions 
(7) 
Informative prior elicitation is a very important part of a Bayesian analysis. We 
propose a class of informative priors for the regression coefficients j3, since these 
parameters are typically of primary inferential interest in these problems. The pro-
posed prior distributions are useful in situations such a:s variable selection and pre-
diction, tikelihood-based frequentist methods for inference with these models is 

3. Prior Elicitation and Variable Selection for GLMM's 
45 
virtually impossible, due to the high dimensional integrations required for integrat-
ing out the random effects (see Zeger and Karim, 1991). Our prior construction for 
{3 is based on the notion of the existence of similar previous studies, i.e., historical 
data, as was motivated in Section 1. For ease of exposition, we will assume one 
previous study as the generalization of the prior to multiple previous studies will 
become immediately clear. Suppose there exist historical data with No subjects that 
yield the noi x 1 response vector Yoi for subject i. Let Xoi be an noi x p matrix 
of fixed covariates, and Zoi be an noi x q matrix of covariates for the q x 1 
vector of random effects boi for subject i, i = 1, 2, ... , N 0 for the historical data. 
Also let bo = (bol, ... , boN0 )', Yo = (You, ... , YoN0n 0 N 0 )', Xo = (Xo1, · .. , XoN0 )', 
and Zo = (Zo1, ... ,ZoN0 ). Finally let Do= (No,Xo,yo,Zo) denote the historical 
data. 
We propose a prior distribution for {3 taking the form 
1f({3 I Do, T, ao) o< fi (l, D; fp(YO<t 1.8, bo;)]"' p(bo; I T) dbo;) , 
(8) 
where P(Yoit I {3,boi,r) is (4) with (Yoit,boi,ro) in place of (yit,bi,r). That is, 
P(Yoit I {3,boi) is the GLMM based on the prior data YOit· The quantity ao is a 
prior parameter which weights the historical data relative to the likelihood function 
of the current study. It is reasonable to restrict a0 to 0 ~ ao ~ 1. The parameter a0 
can also be interpreted as a dispersion parameter which takes into account between 
and within study variability in the historical data. The prior in (6) does not have a 
closed form but it has several attractive theoretical and computational properties 
as shown in Sections 2.3 and 3. In vector notation, the prior can be written as 
1r({3 I Do, T, ao) = [ 
[p(yo I {3, bo)]ao p(bo I T)dbo 
JRNoq 
= f 
exp {ao {ro(y~O(f/o)- J~g(O(f/o))] + J~c(yo)}} p(bo IT) dbo, 
JRNoq 
(9) 
where J0 is a vector of ones and f/o = X 0{3+Z0b0 are elementwise vectorized versions 
of those in (6). 
The single most common choice for the structure of V = r-1 is a scalar with Zi 
a column of ones. In general, it is usual to take V to be unstructured but by choice 
of parameterizations of V one can model rather different covariance structures. For 
example, taking Zi to be the identity matrix and V to be an autoregressive (AR-1) 
structure, one gets a common time series model with independent measurement 
error ei. We can incorporate both of these examples in a single model. We take 
V.= .• o-~Eb, where the (j,j*)th element ofEb has the form O"jj• = piJ-rl, where 
p!J-J I is the correlation between ( biJ, bij•), and -1 ~ p ~ 1. This AR-1 structure for 
V is quite general and appears to be quite suitable in practice for many longitudinal 
datasets. 
The prior specification is thus completed by specifying priors for ( a 0 , u~, p). We 
take these parameters independent a priori. We specify a beta prior for a 0 , an inverse 
gamma prior for u~, denoted JG(6o,{o), and a scaled beta prior for p, denoted 
scbeta(vo, 1/lo). Thus, we propose a joint prior distribution of the form 
1r({3, ao, ut, PI Do) 
o< fi (l, D; fp(yoit I {3, bo;)]"' p(bo; IT) dbo;) 

46 
Ibrahim & Chen 
xago-1(1- ao)>.o-1 x (ul)-(oo+1) exp( -ub"2'Yo) 
x(1 + Pto-1(1- p)1/Jo-1, 
(10) 
where Bo = (60 , "Yo, a 0 , .-\0 , v0 , 1/Jo) are known prior parameters. In our analyses, we 
take vague choices of the prior hyperparameters ()0 . 
2. 3 
Propriety of the Prior Distribution 
It is critical to examine the conditions under which the joint prior in (9) is proper. 
This issue is crucial in Bayesian variable selection (see, for example, Ibrahim, Ryan, 
and Chen (1998) and Chen, Ibrahim, and Yiannoutsos (1999)), as it is well known 
that Bayesian variable selection requires a proper prior distribution. It is also an 
important issue in Bayesian hypothesis testing problems, and in particular, in the 
calculation of Bayes factors and related quantities (see, for example, Berger, 1985, 
pp. 145-157). Here, we establish some very general results concerning the propriety 
of the joint prior distribution of ({3, a 0 , ul, p) for generalized linear mixed models. 
We have the following theorem. 
Theorem 2 .. 1 Assume that 
exp {To [YoitB('I]oit)- g(B('I]oit))] + c(Yoit)} ~ Mo 
(11) 
for 1 ~ t ~ noi, 1 ~ i ::=:; No, where M0 is some finite constant. Suppose that there 
exist Yoitt 1 , Yoi 2t2 , ... , Yoi,t,. (1 ::=:; i1 ~ i2 ~ · · · ~ ip) such that 
1: e''l"l exp {To [Yoi;t;O('I) - g(O('I))] + c(Yoi;t;) }a'l < oo 
(12) 
or 
1: e''l"l' exp {To [Yoi;t;0(7J)- g(0(7J))] + c(Yoi;t;) }a'l < oo 
(13) 
for some to> 0 and j = 1, 2, ... ,p and the corresponding. design matrix (xoi 1t1 , Xoi 2t2 , 
... , Xoi,t,.)' has full rank. Then the joint prior distribution 1r({3, ao, ul, pi Do) is 
proper, i.e., 
1
1 
rX) {
1 f fi ( f IT (p(yoit I {3, boi)r
0 7r(boi IT) dboi) 
-1 lo lo J 
RP i=1 J 
Rq t=1 
xag 0 - 1(1- ao)>.o-1 x (ul)-(oo+1)exp(-ub"21o) 
x(1 + p)vo-1(1- p)1/Jo-1 df3 dao dul dp < oo 
if one of the following conditions is satisfied: 
( i) ao > p, ,\0 > 0, and (12) holds; 
( ii) ao > p/2, ,\0 > 0, and (13) holds. 
(14) 
The proof of the theorem is omitted for brevity. The conditions stated in Theorem 
2 .. 1 are sufficient and they hold for many generalized linear mixed models such as the 

3. Prior Elicitation and Variable Selection for GLMM's 
47 
normal, Poisson, and binomial models. However, when the Yoi are binary responses, 
i.e., Yoit = 0 or 1, we have 
1r({3, ao, ul, pI Do) 
<X ft ( j fJ [ 
F( "'~w9 + z~itbo;)""' (1 - F( .,~it.B + z~itbo;))
1-""' ]"' 
p(bo; IT) dbo;) x ag•-'(1- ao)"'- 1 x (ul)-(6o+l)exp(-ub 2'Yo) 
X (1 + p)vo-1 (1- p)t/Jo-1, 
where F is a cumulative distribution function with 0 < F(rJi) < 1, and F- 1 is called 
a link function. In this binary response case, neither (12) nor (13) will be satisfied. 
But, under some additional regularity conditions on the fixed covariates Xoit, the 
propriety of the prior distribution 1r({3, a 0 , ul, p I Do) can still be established. The 
main result is stated in the next theorem. 
Theorem 2 .. 2 Let coit = 1 ifYoit = 0 and= -lifYoit = 1 and define Io = {(i,t): 
1 ~ t ~ noi, 1 ~ i ~ No, Coit Xoit1 > 0} and Jo = {(i, t) : 1 ~ t ~ noi, 1 ~ i ~ 
No, CoitXoit1 < 0}. Assume that the following conditions are satisfied 
(C1) 10 and J0 are non-empty sets; 
(C2) \1 ({32, · · ·, {3p)' #- 0, 
. ( Ef-2 Xojtlf31) < 
( Ef-2 xowf31) 
mm 
mu 
; 
(j,t)EJo 
Xojt1 
(i,t)Elo 
Xoit1 
(C3) 1: e'•i•idF(u) < oo for some t0 > 0; 
(C4) ao > p, ,\ > 0, 6o > p/2, 
then for the binary responses Yoit 's, (14) holds. 
The proof of this theorem is omitted for brevity. Note that in Theorem 2 .. 2, Xoitl 
may be 1, which corresponds to the inclusion of an intercept in the model. 
2.4 
Specifying the Hyperparameters 
In the context of model selection and estimation of {3 in GLMM, (p, u 2) are 
viewed as nuisance parameters, and therefore we take vague choices for their prior 
hyperparameters. In particular, it is reasonable to take v0 = ..Po = 1 which yields a 
uniform prior for p on [-1, 1). Also, we take 60 --+- p/2 and 'Yo--+- 0 for ul to ensure 
a proper joint prior 1r(/3, ao, ug, piDo)· 
For ao, we recommend that several values of the hyperparameters be chosen 
and sensitivity analyses conducted. For elicitation purposes, it is easier to work 
with the prior mean and variance of ao, given by Jlao = ao/ ( ao + Ao), and u~0 = 
Jla0 (1- Jlao )(ao + ..\o + 1)-1 . From Theorems 2 .. 1 and 2 .. 2, a sufficient condition for 
the propriety of the prior distribution is that a 0 > p for the full model. Therefore, a 

48 
Ibrahim & Chen 
reasonable starting point for the analysis is to choose a 0 = ,\0 = p + 1, which gives 
Jlao = 1/2. Then we conduct several sensitivity analyses within a suitable range 
of the uniform prior, using various values of (Jlao, u~0 ). Small and large values of 
(Jlao, 0"~0 ) should be considered. We do not recommend doing an analysis based on 
one set of proposed values of (Jlao, u~0 ), nor do we propose specifying (Jlao, u~J by 
a one-time automated procedure. The choice of (Jlao, u~0 ) depends on the context of 
the problem and the structure of the historical data. In Section 4, we demonstrate 
several choices of (Jlao, u~0 ) and conduct sensitivity analyses and examine their 
impact on variable selection and estimation of {3. 
2. 5 
The Posterior Distribution and its Computation 
Let D = (N, X, y, Z) denote the data for the current study. Then, the joint 
posterior distribution of ({3, a 0 , u~, p) is given by 
p({3, ao, u~, pI D, Do) 
ex: 
p(y I {3, T)1r({3, ao, u~, pI Do), 
(15) 
where p(y I {3, T) is given by (3) and 1r({3, a 0 , u~, pi Do) is given by (9). Clearly, when 
the prior is proper, then so is the posterior. The posterior in (15) does not have a 
closed form in general, but it has attractive computational properties due to the 
recent development of the Gibbs sampler for generalized linear mixed models; see, for 
example, Gelfand, Sahu, and Carlin (1996), and Gelfand and Sahu (1996). Instead of 
directly sampling ({3, ao, u~, p) from the posterior distribution p(f3, ao, u~, p I D, Do), 
we sample ({3, a0 , u~, p, b, bo) from the joint posterior distribution p({3, ao, u~, p, b, bo I 
D, D0 ), which is proportional to 
p*({3, ao, u~, p, b, bo I D, Do) 
= p(y, b I {3, T)p(yo I {3, bo)]a0 p(bo IT) X ago-1(1- ao)>.o-1 
x(u~)-(oo+1) exp(-ub"2'Yo) x (1 + p)vo-1(1- p)'I/Jo-1, 
(16) 
where p(y, b I {3, T), p(yo I {3, bo), and p(bo I T) are given by (6) and (9), respec-
tively. The complete or semi hierarchical centering reparameterization technique of 
Gelfand, Sahu, and Carlin (1996) or Gelfand and Sahu (1996) is particularly suit-
able for the implementation of the Gibbs sampler for our problem. Here, we note 
that if all the ni's, the noi's and q are the same, we can directly apply the complete 
hierarchical centering reparameterization of Gelfand, Sahu, and Carlin (1996). Oth-
erwise, we can use the semi-hierarchical centering reparameterization of Gelfand 
and Sahu (1996). After the (semi) hierarchical centering reparameterization, effi-
cient algorithms can be adopted to sample the reparametrized parameters, which 
are functions of b, b0 , and {3, and the other parameters including {3, ao, u~, and p. 
To preserve space, we omit the details of these algorithms here. 
3. 
Bayesian Variable Selection 
In this section, we consider Bayesian variable selection procedures for generalized 
linear mixed models. We develop an efficient Monte Carlo approach for computing 
posterior model probabilities. 
We first introduce some necessary notation used for variable subset selection. Let 
M denote the model space. We enumerate the models in M by m = 1, 2, ... , K, 

3. Prior Elicitation and Variable Selection for GLMM's 
49 
where K is the dimension of M and model K denotes the full model. The full model 
is defined here as the model containing all of the available covariates in the study, 
and thus K = 2P. Also, let /3(/C) = (/31, ... , /3p )' denote the regression coefficients 
for the full model, and let f3(m) denote a km X 1 vector of regression coefficients for 
model m with a specific choice of km covariates. We write f3UC) = (f3Cm)', {3(-m)')', 
where {3( -m) is /3(/C) with f3(m) deleted. In addition, we let D(m) and D~m) denote 
the current data and the historical data under model m. 
To elicit the prior distribution on the model space M, we let 
Po(f3Cm) ID~m)) 
= t [" l {D. (LJj [v(Yoit I p<m), bo;lj"' p(bo; I T) dbo;) 
xago-1(1- ao)>.o-1 x (u~)-(oo+l) exp( -u;;2/'o) 
x(l + p)"'- 1(1- p)>P•-•} dao dul dp. 
(17) 
We see that p0(f3Cm)IDam)) is proportional to the marginal prior of f3(m). We propose 
to take the prior probability of model m as 
I Po(f3Cm) IDam)) df3Cm) 
p(m) = 2:~=1 I Po(f3U)ID~n) df3Cn · 
(18) 
This choice for p( m) in (11) is a natural one since the numerator is just the normal-
izing constant of the joint prior of (f3Cm), a0 , u~, p) under model m. The prior model 
probabilities in (11) are based on coherent Bayesian updating. It can be shown that 
p(m) in (11) corresponds to the posterior probability of model m based on the data 
Dam) using a uniform prior for the previous study, p0(m) = 2-P for m E M as 
ao -+ oo. That is, p(m) <X p(m I D~m)), and thus p(m) corresponds to the usual 
Bayesian update of p0(m) using D~m) as the data. 
Using the prior model probability p(m) given in (11) and Bayes theorem, the 
posterior probability of model m is given by 
m D(m) -
p(D(m)lm) p(m) 
p( I 
) - ~~ (D(j)l ') ( ') , 
L...JJ=1 p 
J p J 
(19) 
where p(D(m)lm) denotes the marginal distribution of the data D(m) for the current 
study under model m, which has the following expression: 
p(D(m)jm) = 
1
1 1
00 1
1 J. 
p(y I f3(m), T)1r(/3(m), ao, ul, pI Do)df3(m)daodutdp, 
-1 
0 
0 
Rkm 
where p(y I f3(m),T) and 7r(f3(m),ao,ut,p I Do) are defined by (3) and (9) under 
model m. It is easy to see that p(D(m)lm) is indeed the normalizing constant of the 
joint posterior distribution p(f3(m), a0 , ut, p I D, D0 ) given in (16) under model m. 
Now it can be shown that the posterior probability p(miD(m)) in (12) of model m 
is given by 
(20) 

50 
Ibrahim & Chen 
m = 1, ... , /C, where p(f3(-m) = OlD( /C), D~JC)) is the marginal posterior density of 
{3(-m) for the full model evaluated at {3(-m) = 0. In (16), for notational convenience 
we assume that p(f3(-JC) = OID(IC), Da/C)) = 1. The result in (16) is very attractive 
since it shows that the posterior model probability p(miD(m)) is simply a function 
of the marginal posterior density functions of {3(-m) for the full model evaluated 
at /3(-m) = 0. This formula does not algebraically depend on the prior model 
probability p(m) since it cancels out in the derivation due to the structure of p(m). 
This is an important feature since it allows us to compute the posterior model 
probabilities directly without numerically computing the prior model probabilities. 
This has a clear computational advantage and as a result, allows us to compute 
posterior model probabilities very efficiently. We note that this computational device 
works best if all of the covariates are standardized to have mean 0 and variance 1. 
This is not restrictive since this is a typical transformation taken quite often in 
practice to numerically stabilize the Gibbs sampler. 
Due to the complexity of our model, the analytical evaluation of p(f3(-m) = 
0 ID(JC), DaJC)) does not appear possible. However, we can adopt the importance 
weighted marginal posterior density estimation (IWMDE) method of Chen (1994) to 
estimate these marginal posterior densities. The IWMDE is a Monte Carlo method 
developed by Chen (1994), which is particularly suitable for estimating marginal 
posterior densities when the joint posterior density is known up to a normalizing 
constant. The IWMDE method requires using only one MCMC sample from the 
posterior distribution for the full model, making the computation of complicated 
posterior model probabilities feasible. It directly follows from the IWMDE that a 
simulation consistent estimator of p(f3(-m) = OID(JC)) is given by 
P(/3(-m) = OID(IC)) 
B 
= ~ Ew (!3[1)m) I !3[;)),ao(l),u~(I)'P(I),b(l),bo(l)) 
1=1 
p*(/3~;)), {3(-m) = 0, ao(l), 0'~(1)' P(l), b(l), bo(l) ID(IC), Da/C)) 
X 
P* (/3~~), ao(l), ui(l), P(l), b(l), bo(l) I D(IC), D~JC)) 
where w(f3(-m) I f3(m), a0 , u&, p, b, b0 ) is a completely known conditional density of 
{3(-m) given f3(m), a0 , ul, p, b, and bo, p*(/3, ao, u~, p, b, bo I D, Do) is given in (16) 
for the full model, and {(/3~~), ao(l), u~(l)' P(l), b(1), bo(l)), l = 1, 2, ... , B} is a MCMC 
sample from the joint posterior distribution p(/3, ao, u&, p, b, bo I D, Do). Note that 
the choice of the weight density function w(f3(-m) I f3(m), ao, u&, p, b, bo) is somehow 
arbitrary. However, Chen (1994) showed that the best choice of w is the conditional 
posterior density of {3(-m) given f3(m), a0 , u&, p, b, and bo. For the complete hierar-
chical centering case, after reparameterization the conditional posterior distribution 
of f3(1C) given the other parameters is normal, and thus the closed form of the best w 
is available. For the other cases, we can follow an empirical procedure provided by 
Chen (1994) to select w. As demonstrated in Ibrahim and Chen (1998), it is suffi-
cient to choose w to be conditional density of the p dimensional normal distribution 
with its mean and covariance constructed by the MCMC sample, 
for our variable selection problem. 

3. Prior Elicitation and Variable Selection for GLMM's 
51 
4. 
Pediatric Pain Data 
We illustrate our methodology on a repeated measures data set from Pediatric 
Pain. The response for each of the 58 children with complete data is a two dimen-
sional vector of the total number of nurse visits taken each year over the two-year 
period. The covariance structure is a random intercept model for all models with 
q := 2 and each Z a 2 x 2 identity matrix. The children are classified into one of two 
groups, attenders or distracters (A or D), depending on their style of coping (CS) 
with the pain of the cold. Attenders pay attention to their arm in the water and the 
experimental apparatus during the trial; distracters think about topics unrelated to 
the trial. Prior to the fourth trial, an intervention occurs. The intervention (treat-
ment or TMT) is a short counseling session. Three types of counseling are given: 
counseling to attend (A); counseling to distract (D); or a null counseling without in-
structions (N). IfTMT has any effect, then a priori, CS and TMT were presumed to 
interact. Interest lies in whether the two CS groups have different baseline response 
times, and given that CS has an effect, whether treatment affects the response time. 
Thus, the full model contains seven covariates and an intercept term. The seven co-
variates are age (xl), the two treatment indicator variables (x2 and x3), coping style 
(x4), tolerance (x5), rating (x6), and a coping style by rating interaction (x7). The 
response variable y is the total number of nurse visits, which we model as a Poisson 
GLMM. The dataset contains 33 girls and 18 boys. For the purposes of illustration, 
we use the boys as the historical data, from which we will elicit our prior, and use 
the data for the girls as the current data. Thus, for the Pediatric Pain data, we 
have N = 33, No = 18, and all ni's and noi are equal to 2. Since all the ni's, the 
noi 's and q are the same, we can directly apply the complete hierarchical centering 
reparameterization of Gelfand, Sahu, and Carlin (1996). We used 1,000 iterations 
to "burn in" the Gibbs sampler and then generate 20,000 iterations to obtain the 
estimates of all posterior model probabilities in all the following computations. 
Table 3.1 below gives results for the top three models with o:0 = 10, .A0 = 10. i.e., 
Jlao = .5 and Ua0 = .11. In addition, we take vague priors for ul and p. Specifically, 
for ul, we take (6o,/o) = (.005, .005), and for p, we take a uniform prior on [-1, 1], 
i.e., Vo = 1/Jo = 1. From Table 3.1 1 we see that there is not a dominant top model. The 
table does indicate that treatment, rating, coping style, and rating by coping style 
interaction are important covariates for explaining the number of nurse visits. To 
examine the sensitivity of model selection to the choices of (Jla 0 , Ua0 ), we computed 
posterior model probabilities for several choices of (Jla 0 , Ua 0 ). The results for the 
top model are given in Table 3.2. 
Table 3.1: Posterior Model Probabilities 
for (Jla 01 Ua0 ) = (.5, .11) 
m 
p(miD) 
(x2, X4, X6, X7) 
.119 
(x2, Xg, X4 1 X6 1 X7) 
.111 
(x2, X4 1 X5 1 X6 1 X7) 
.059 

52 
Ibrahim & Chen 
Table 3.2: Posterior Model Probabilities 
for Several Values of (JJa 0 , O"a0 ) 
(Jlao ' 0" ao) 
m 
p(mlD) 
(.5, .078) 
( X2, X4, X6, X7) 
.100 
(.5, .064) 
( X2 1 X4 1 X6 1 X7) 
.085 
( .5, .050) 
( X2, X4, X6, X7) 
.073 
( .91, .027) 
(x2,X3,X4,X5 1 X6 1 X7) 
.046 
From Table 3.2, we see that for several choices of (JJa0 , O"a 0 ), the (x2, x4, x6, 
X7) model obtains the largest posterior probability. The pattern of the posterior 
probability structure for the other models for these choices of prior parameters is 
similar to that of Table 3.1. However, model selection does become sensitive to the 
choice of (Jla 0 , u ao) when we give large weight to the historical data as demonstrated 
in the last line of Table 3.2. Here, we see that the top model is (x2, xa, x4, x5, X6, X7 ). 
In all of the above choices of (Jlao, O"a 0 ), there was not a dominant top model which 
obtained a large posterior probability. Thus, it appears for this dataset that there 
is no clear cut top model, but perhaps two or three adequate models, which all 
contain the covariates treatment, rating, coping style, and rating by coping style 
interaction. 
5. 
Discussion 
We have developed a general class of informative prior distributions for the gen-
eralized linear mixed model and have also implemented some novel computational 
tools. One of the main applications for our methods is variable selection, but the 
priors, as well as the computational methods, can be used for other applications, 
such as in the analysis of developmental toxicology data, or in general situations 
when historical data is available. To date, we do not know of any Bayesian or 
frequentist methods for doing variable selection for this class of models. We have 
proposed novel methods which are computationally feasible. Our prior distributions 
are quite general in that they can be used in any application for which there is his-
torical data available. The priors are semi-automatic in the sense that they take the 
form of a likelihood function based on the historical data, and they require very few 
hyperparameters, thus making them especially attractive for variable selection. In 
addition, the proposed priors are proper under some very general conditions. 
References 
Berger, J. 0. (1985), Statistical Decision Theory and Bayesian Analysis, Second 
Edition, New York: Springer-Verlag. 
Breslow N .E., and Clayton, D. G. (1993). Approximate Inference in Generalized 
Linear Mixed Models. Journal of the American Statistical Association, 88, 
9-25. 
Chen, M.-H. (1994). Importance-weighted Marginal Bayesian Posterior Density 
Estimation. Journal of the American Statistical Association, 89, 818-824. 
Chen, M.-H., Ibrahim, J. G., and Yiannoutsos, C. (1999). Prior Elicitation, Vari-
able Selection, and and Bayesian Computation for Logistic Regression Models. 
Journal of the Royal Statistical Society, Series B, 61, 223-242. 

3. Prior Elicitation and Variable Selection for GLMM's 
53 
Gelfand, A. E. and Sahu, S. K. (1996). Identifiability, Propriety and Parameteri-
zation with regard to Simulation-Based Fitting of Generalized Linear Mixed 
Models. Technical Report #9636, Department of Statistics, University of Con-
necticut. 
Gelfand, A. E., Sahu, S. K., and Carlin) B. P. (1996). Efficient Parametrisations 
for Generalized Linear Mixed Models (with discussion) in Bayesian Statistics 
5, eds. J.M. Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith, Oxford: 
Oxford University Press, 165-180. 
Ibrahim, J. G. and Chen, M.-H. (1998). Prior Distributions and Bayesian Compu-
tation For Proportional Hazards Models. Sankhya, Series B, 60, 48-64. 
Ibrahim, J. G., Ryan, L. M., and Chen, M.-H. (1998). Use of Historical Controls to 
Adjust for Covariates in Trend Tests for Binary Data. Journal of the American 
Statistical Association, 93, 1282-1293. 
Laird, N. M. and Ware, J. H. (1982). Random-effects models for longitudinal data. 
Biometrics, 38, 963-97 4. 
Liang, K.-Y. and Zeger, S.L. (1986). Longitudinal Data Analysis Using Generalized 
Linear Models. Biometrika, 73, 13-22. 
McCullagh, P. and Neider, J. (1989). Generalized Linear Models, 2nd edition, New 
York: Chapman & Hall. 
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalized Linear Models. Jour-
nal of the Royal Statistical Society, Series A, 135, 370-384. 
Zeger, S. L., and Karim, M. R. (1991). Generalized Linear Models With Random 
Effects; A Gibbs Sampling Approach. Journal of the American Statistical As-
sociation, 86, 79-86. 
Zeger, S. L. and Liang, K.-Y. (1986). Longitudinal Data Analysis for Discrete and 
Continuous Outcomes. Biometrics, 42, 121-130. 

This Page Intentionally Left Blank 

Part II 
Extending the G LMs 

This Page Intentionally Left Blank 

4 
Dynamic Generalized Linear 
Models 
Marco A. R. Ferreira 
Dani Gamerman 
ABSTRACT Dynamic Generalized Linear Models are generalizations of the Gener-
alized Linear Models when the observations are time series and the parameters are 
allowed to vary through the time. They have been used increasingly in different areas 
such as epidemiology, econometrics and marketing. Here we make an overview of the 
different statistical methodologies that have been proposed to deal with these models 
from the Bayesian viewpoint. Also, we present some of the challenges involved in the 
estimation process. Finally, two applications in epidemiology are presented showing 
the power of MCMC-based methodologies. 
1. 
Introduction 
Real world often leads to the necessity of nonnormal data analysis. This issue was 
highly enlightened with the introduction of generalized linear models (GLM), clever 
extensions of linear regressions, by Neider and Wedderburn (1972), and the Bayesian 
point of view on this subject can be found in chapter 1. As pointed out there, 
the observations are distributed in the exponential family. Hence, if we denote the 
observations by Yt, t = 1, ... , T then their distribution can be represented through 
the density (or probability function) 
p(y,jO,) "' exp { y,O, ~~b(O,)} 
(1) 
In addition, a suitable link function is introduced relating the mean Jl.t = E(Yt lOt) = 
b'(Ot) to the regressor vector Ft through g(p,t) = Vt = F£(3. Also, it is supposed 
that Y1, ... , YT are independent, conditionally on {3. Many nonnormalities can be 
accommodated in this framework, including for example the Binomial and Poisson 
models for counting data and Gamma distribution for positive continuous data. 
This class of models cannot be used directly in other situations in practice when 
the data consist of a time series, hence dependent observations. For example, in 
epidemiology the analysis of the number of cases of a particular disease through 
the time is clearly nonnormal. The study is important for the definition of control 
policies by showing the trend and seasonal patterns, identifying epidemics, measur-
ing the impact of mass vaccination and so on. Another example in medicine is the 
study of treatments to prevent malaria crisis. Some people submitted to different 
57 

58 
Ferreira & Gamerman 
treatments are accompanied during some time, and each individual has his status 
(0 = no crisis, 1 = crisis) recorded each day. Additionally, values of some regressors 
are known for. each individual, as for example age, sex, education and living area. 
This problem can be thought as a longitudinal study with Bernoulli observations. 
One option often used in practice to analyze time series is the class of dynamic 
linear models, where the observation depends on a set of components and the com-
ponents evolve independently through the time. Generally they assume normal ob-
servations, but there is a generalization to the exponential family formalized by 
West, Harrison and Migon (1985), called dynamic generalized linear models. This 
class can also be seen as a generalization of the generalized linear models, with the 
parameters changing through the time. In this chapter we present a review of the 
literature on dynamic generalized linear models and associated inference and we 
illustrate with two applications. 
The organization of the chapter is as follows: In section 2, the dynamic linear 
models are revised; Section 3. contains the definition of the dynamic generalized 
linear models, and presents initial approaches for inference about these models; 
section 4 presents Markov Chain Monte Carlo based approaches to do inference; in 
section 5 applications illustrating some aspects of the models are shown; Discussion 
and possible extensions are presented in section 6. 
2. 
Dynamic linear models 
Dynamic linear models (DLM), also known as state space models, have been 
widely used to analyze time series. They provide a very flexible framework that 
permits smooth and abrupt changes in the time series generating process and per-
mits the natural accommodation of subjective information. A good reference on 
dynamic linear models from a Bayesian point of view is West and Harrison (1997). 
The DLM is usually formed by an observation equation describing the relationship 
between the observation and regressors, trend, seasonallity and other components 
that takes the form of a multivariate regression 
Yt = Ftf3t + Vt 
(2) 
and a system equation describing the evolution of the vector of regression coefficients 
or state parameters f3t through time 
(3) 
The set of disturbances Vt and Ut are independent and Vt "" N(O, Et), Ut ""N(O, Wt) 
and the model is completed with a prior f31 ""N(a1, Rt). 
The DLM can be seen as a generalization of regression models that allows changes 
in parameter values through time. The components of the model are usually defined 
in blocks, so we have a block describing the trend, another describing the seasonallity 
and so on. 
Let Dt denote the information until time t. Then Do is the prior information and 
Dt = Dt-1 U{Yt} if there is no information out of the sample at timet. Traditionally, 
inference for dynamic models is made sequentially by obtaining for each time t the 
prior, predictive and updated distributions for the system parameter f3t. The first 
two distributions are respectively obtained by 
p(f3t IDt-1) 
J 
p(f3t lf3t-1 )p(f3t-liDt-l)df3t-t 
(4) 

4. Dynamic GLMs 
59 
(5) 
and the last one is obtained via Bayes' theorem as 
(6) 
The last equation involves implicitly an integration to find the normalizing con-
stant. If Ft, Gt, Et and Wt are known matrices then all the integrals in (4), (5) and 
( 6) can be obtained exactly. The resulting algorithm is the so called Kalman Filter. 
But usually Et, Wt and in some cases elements of Ft and Gt are not known, imply-
ing in problems to calculate analytically the integrals. These unknown quantities 
are called hyperparameters and denoted by W here. 
A Bayesian attempt to solve the problem of unknown Wt is the use of discount 
factors (see West and Harrison, 1997), widely used in practice. If 6 E (0, 1) is the 
known discount factor and Ct-1 is the variance of !1t-11Dt-b then the variance of 
PtlDt-1 is defined by Rt = GtCt-1G~J6. Thus, the discount factor is related with 
the increase of uncertainty. It is easy to show that Wt = GtCt-1G~(1- 6)/6 thus 
solving the problem of specification of the unknown value of Wt. 
A classical procedure presented in Harvey (1989) is to integrate expressions with 
respect to Pt· The accumulation of expression (5) through time leads to the predic-
tive likelihood depending only on W. He then maximizes this likelihood to estimate 
W. Inference for state parameters proceeds as before with W replaced by the esti-
mate. 
The extension of DLM to nonlinear state space models has been widely used 
in control engineering. In these models the observational equation is replaced by 
Yt = Ft(Pt) + Vt or by Yt = Ft(Pt, Vt), or the evolution equation is replaced by 
Pt = Gt(Pt-d+ut or by Pt = Gt(Pt-1! Ut) with Vt and Ut still supposed normal. The 
nonlinear state space models are in some sense related to the DGLM's. For example, 
one of the options to do inference in nonlinear state space models is the extended 
Kalman filter and smoother, in which the non-linearities are linearized by Taylor 
expansions. A modification of this approach was proposed by Farhmeir in order 
to draw inference for DGLM's, as we explain in subsection 3.3. More information 
about nonlinear state space models can be found in Anderson and Moore (1979). 
Extensions of DLM to nonnormal data were proposed by West (1981), Meinhold 
and Singpurwalla (1989) and Carlin, Polson and Stoffer (1992), but these extensions 
were to distributions that are in some way linked to the normal distribution, such 
as scale mixtures of normals. 
3. 
Definition and first approaches to inference 
Perhaps a first attempt to analyze counting time series was through the use of 
data transformation, as described in Stevens (1974). This was used to improve the 
normal approximation for the transformed observations and to make the variance 
independent of the location. The disadvantages are the difficulty in interpreting the 
results and the inadequacy of the transformations when the observations are counts 
close to zero. 
The modeling of variance laws, as in Harrison and Stevens (1976), has been 
widely used in applications. In this approach, it is supposed that the variances Et 
can be approximated by a function h(Ft!1t), thus incorporating the dependence on 

60 
Ferreira & Gamerman 
the mean. For example, if the data are univariate Poisson then h(x) = x and if 
the data are multinomial then h(x) = Nt[diag(pt)- PtP~] where Pt is the vector 
of multinomial probabilities. As they use the Kalman filter, they substitute f3t in 
h(Ftf3t) by its best estimate. The main drawbacks are that it still supposes normality 
and does not account for the uncertainty about the mean in the variance. 
West, Harrison and Migon (1985) formalized the extension of dynamic models to 
allow observations in the exponential family, thus defining the Dynamic Generalized 
Linear Models (DGLM). The extension is based on the generalized linear models 
proposed in Neider and Wedderburn (1972), which are well covered in McCullagh 
and Neider (1989). The observation equation (2) is replaced by (1) and the link 
function g(pt) = lit = Fff3t relates the successive means Jl.t = E(YtiOt) = b'(Ot) to 
the state parameters, usually mapping the range of Jl.t into the real line. The model 
generalizes G LM by allowing a different vector of regression coefficients for each 
time and is completed with (3), relating these coefficients through time. 
In the case of DGLM the integrals in ( 4), (5) and (6) cannot be obtained exactly, 
and so the inference cannot be made exactly. Many proposals to solve this problem 
have been presented in literature. In the following subsections we present some of 
them. 
3.1 
Linear Bayes Approach 
West, Harrison and Migon (1985) proposed an approximation based on linear 
Bayes. This idea was also described in Migon (1984) within the context of dynamic 
normal nonlinear models. 
Basically, the distribution of the system errors Ut and the distribution of f3t-11Dt-1 
are specified only by the first and second order moments, namely Ut """' WS(O, Wt) 
and f3t-1IDt-1 """'WS(mt-1,Ct-d, with WS standing for wide sense. Then, the 
prior distribution for f3t is f3tiDt-1 """' WS(at, Rt) where at = Gtmt-1 and Rt = 
GtCt-lG~ + Wt. Here again they suggest the use of discount factors to bypass the 
difficult problem of specification or estimation of Wt. Hence, the prior distribution 
for lit= g(pt) = Fff3t is lltiDt-1"""' WS(It,qt) where It= Ffat and Qt = FfRtFt. 
In order to calculate the posterior for Ot the following conjugate prior for Ot is 
assumed: 
(" ID 
) 
{ rtOt- b(Ot)} 
p ut 
t-1 ex exp 
St 
where rt and Bt are such that E[g(b'(Ot))IDt-d = It and V[g(b'(Ot))IDt-d = Qt· 
Hence, the posterior for Ot is 
It implies that the posterior distribution to lit is lit I Dt """' W S(lt, q;) where It = 
E[g(b'(O))IDt] and q; = V[g(b'(O))IDt]· The next step is to obtain E[f3tlllt, Dt-d 
and V(f3tlllt, Dt- 1]. This cannot be done exactly, and West, Harrison and Migon 
(1985) estimated these moments using linear Bayes, leading to 
E[f3tlllt, Dt-1] =at'+ RtFt(llt- lt)/qt 
and 
V[f3t I lit, Dt-d = Rt- RtFtF: Rt/qt 
Finally, moments of Pt I Dt are calculated using properties E[/Jt I De=E{E[/Jt I Vt,De-t] I De} 
and V[f3tiDt] = V{E[f3tlllt,Dt-diDt} + E{V[f3tlllt, Dt-diDt}, and replacing the 

4. Dynamic GLMs 
61 
conditional moments by their linear Bayes estimators. Then, Pt!Dt "" WS(mt, Ct) 
with 
and 
Ct = Rt- RtFtF:Rt(1- q; /qt)/qt 
Hence, the above development allows sequential analysis of DGLM. The approach of 
West, Harrison and Migon (1985) was the first to treat DGLM,s in general, in a time 
when MCMC techniques were not well known, and in many cases the approximation 
is quite good. 
3.2 Piecewise Linear Approximation 
Kitagawa (1987) presented a method to analyze non-Gaussian and nonlinear 
state-space models using piecewise linear functions. He suggested that the densities 
p(/1t lDt-d, p(/1t lDt), p(/1t I Dr) and the density of Ut are approximated by piecewise 
linear functions. Then, the value of each function in a given grid of points define 
the approximated function. Kitagawa (1987) showed that under the assumption of 
piecewise linearity the integrals in expressions ( 4), (5) and (6) are just sums with 
the same order as the number of points on the grid. It is generally necessary to 
adapt the grid of points through the time to include the region where the densities 
are concentrated, which is not an easy task. In addition, as the dimension of Pt 
increases the number of points rises exponentially and it is very hard to locate the 
grid, thus the procedure becomes prohibitive in terms of computational cost and 
time. 
3.3 Posterior Mode Estimation 
Fahrmeir (1992) proposed a generalization of the extended Kalman filter and 
smoother applying it to multivariate DGLM to estimate the state parameters via 
their posterior mode. The algorithm is applied sequentially and provides an approx-
imation of the posterior mode. Indeed, it can be seen as a simplified Fisher scoring 
algorithm. The use of the posterior mode as estimator is just to avoid integration. In 
order to estimate the hyperparameters a procedure based on an EM-type algorithm 
is proposed. 
In order to estimate the state space parameters, Singh and Roberts (1992) pro-
posed the iterative application of the linear Kalman smoothing to the DG LM with 
modified observation equation 
(7) 
with adjusted observations Yt = Yt (!1t) = Vt +(Yt-J-lt )g' (J-tt) and associated variances 
Vt = Vt(Pt) = b"(Ot)[g1(J-tt)J2 for t = 1, ... , T. These modified observations and 
variances are defined at each iteration by using the value of Pt estimated at the 
previous iteration, by the Kalman filter and smoother. Singh and Roberts (1992) 
estimate Wt = W using a moment-based approach. 
Fahrmeir and Wagenpfeil (1997) also worked on obtaining the posterior mode of 
state parameters to multivariate DGLM. They showed that the algorithm proposed 
by Singh and Roberts (1992) leads to the posterior mode of the state parameters 
conditional on a fixed value of W. They also showed that the Fahrmeir's (1992) 

62 
Ferreira & Gamerman 
generalization of the extended Kalman filter is a particular case of this algorithm 
with just one iteration and a convenient choice of the initial values. They propose 
the use of a procedure based on the generalized cross-validation criterion to estimate 
the hyperparameters W. 
3.4 
Other Approaches and Models 
Friihwirth-Schnatter (1992) worked with finite mixture approximations for the 
predictive density P(YtiDs), t > s of DGLM. The fully exponential method of 
Laplace (Kass, Tierney and Kadane, 1989) and the Gauss-Hermite integration (Nay-
lor and Smith, 1982) are within this class of approximations. 
Harvey and Fernandes (1989) proposed models with conjugate property with 
the level varying through the time and the covariate effects constant. For ex-
ample, to model Poisson observations with covariates they assumed Yt IJ.tt, {3 ""' 
Poisson[J.tt exp(Fff3)] and J.tt-dDt-1 ""' Ga(at-1, bt-d, and by analogy with the 
Kalman Filter they proposed a implicit system equation such that J.ttiDt-1 ""' 
Ga(atlt-1, btlt-1) with atlt-1 = 6at-1 and btlt-1 = 6bt-1, 6 being the discount 
factor. Then, the conjugacy implies J.ttlDt ""' Ga(at, bt) with at = 6at-1 + Yt and 
bt = 6bt-1 + exp(Fff3). In their approach, 6 and f3 are regarded as hyperparameters 
and are estimated by the maximization of the predictive likelihood. 
4. 
MCMC-based Approaches 
Markov Chain Monte Carlo (MCMC) techniques have been widely used to solve 
complex Bayesian inference problems in the 90s. They allow great flexibility in 
defining the model and drawing inference about parameters or functions of the 
parameters and predictions. Basically) a Markov chain is defined with the equilib-
rium distribution given by the posterior distribution of the model parameters. A 
realization of this chain is generated until convergence is reached. After convergence, 
the following iterations of the chain are in the equilibrium and can be used to form 
a sample from the posterior distribution. The main advantage of this approach is 
the possibility of doing full Bayesian analysis of the problem, which means that the 
uncertainty due to the fact that W is unknown is considered, or in other words, it is 
possible to integrate W out in order to draw inference on ({31, ... , f3T ). In addition, 
point and interval estimation of W can be done based on the posterior distribution. 
References about this subject include Gilks et al. (1996) and Gamerman (1997). 
For the DGLM's the posterior distribution is of the form 
where for expository purposes we assume that Wt = Wand Ft and Gt are known, 
Vt. The analysis is no longer sequential, and ( 4), (5) and (6) are not calculated at 
all. The density calculated is the posterior distribution p(f31, . .. , f3T, W IDT) and the 
necessary integrals are performed numerically. 
The simplest MCMC technique is the Gibbs sampling. In this approach the tran-
sitions are defined by the full conditional distributions for each parameter, that 
is, the distribution of the parameter conditional on all other parameters. If we de-
note {f3¢t} = {{3t, ... , f3t-b f3t+ 1, ... , f3T} then the full conditional distribution of 

4. Dynamic GLMs 
63 
f3t is p(f3tl{f3;tt}, Dr, W). Assume also an inverted Wishart prior distribution for 
W, denoted by IW(nw /2, nwSw /2). Then the full conditional for W is obtained 
as 
p(W!{,B},Dr) 
ex 
[}]p(Pt!Pt-1, W)] p(W) 
ex 
[}] IWI_ 1, 2 exp[-~(p,- p,_i)'W- 1(Pt- .6<-i)]] 
x IWI-(nw+P+l)/2 exp[-~tr(w-1 Swl )] 
IWI-(nw+T-l+p+l)/2 exp{ -~tr[w-l(Swl 
2 
T 
+ I)!3t - f3t-d(f3t - f3t-l)')n 
t=2 
Hence, the full conditional for W is also inverted Wishart from which it is easy to 
sample (see for example Devroye, 1986), and so it is said that the distribution is 
available for sampling. In DGLM's, the full conditional for f3t is not available for 
sampling in general. There are some proposals of implementation of Gibbs sam-
pling to particular cases, as we show in subsection 4.1. For applications in general, 
the most indicated approaches seem to be those based on the Metropolis-Hastings 
algorithm, as explained in subsection 4.2. 
4.1 
Gibbs Sampling 
Carlin, Polson and Stoffer (1992) introduced the use of Gibbs Sampler to perform 
inference for nonnormal and nonlinear state-space models, with observational and 
system disturbances distributed in the class of scale mixtures of normals (Andrews 
and Mallows, 1974), which includes for example exponential power, logistic and t 
densities. Although Carlin, Polson and Stoffer (1992) did not work with DGLM, 
the idea motivated a few MCMC-based solutions for DGLM. 
Fahrmeir, Hennevogl and Klemme (1992) proposed Gibbs sampler to analyze 
DGLM using rejection sampling to generate from the full conditionals of f3t. In gen-
eral, a value 13; is generated from a density g( .) and it is accepted with probability 
!(!3;)/ I<t(g({3;)], where f(f3t) is proportional to p(f3tl{/3;tt}, DT, W) and I<t is such 
that I<tg(f3t) 2:: f(f3t), Vf3t· In the DGLM framework the system equation has nor-
mal disturbances, and so there exist mt and Ct such that p(f3tl{f3;tt}, DT, W) oc 
P(Ytlf3t)N(mt, Ct) = f(f3t), Vt. Fahrmeir, Hennevogl and Klemme (1992) then pro-
posed g(f3t) = N(mt, Ct) which sounds natural, but can lead to low acceptance 
rates. 
In the context of counting time series, Carlin and Polson (1992) proposed the use 
of the Gibbs Sampler with latent continuous variables, which could evolve following 
the standard linear and normal observational equations. These latent variables were 
related to the discrete observations through a suitable transformation. For example, 
if y was Bernoulli and y* was the related latent variable then the suitable trans-
formation could be y = 1 if y* > 0 or y = 0 if y* ::; 0 . These latent variables can 
be seen as nuisance parameters and their full conditionals are easy to sample. In 
addition, their inclusion makes the sampling from the full conditionals of f3t very 
easy. The drawbacks are that this approach is not applicable to exponential family 

64 
Ferreira & Gamerman 
in general but only to counting data, and it is difficult to find a suitable trans-
formation mapping the latent variables to the discrete observations in most of the 
applications. 
4.2 
Metropolis-Hasting Algorithm 
To overcome the difficulty in sampling of the full conditional for f3t, some authors 
have been presenting approaches based on the Metropolis-Hastings algorithm, which 
is also a MCMC technique. The idea is to use the Gibbs sampler with a Metropolis-
Hasting step to do the transition for the state parameters. When sampling state 
parameters one at a time, this Metropolis-Hasting step is made up of two steps: 
first a value 13; is generated from a proposal transition density qt(f3}old), f3t); then 
this value is accepted with probability 
. { 1 
1rt ({3;) / qt (/3} old)' {3;) } 
mm 
'7rt(f3}old))/qt(f3;, f3}old)) 
where Trt is the full conditional density of f3t for t = 1, ... , T. Then, one of the 
differences between the approaches presented by different authors is the choice of 
the proposal transition density. 
Another important issue is that the convergence can be very slow when using 
MCMC to analyze state space models as pointed out by Carter and Kohn (1994) 
and Shephard (1994). The approaches to analyze DGLM using the Metropolis-
Hastings algorithm differ also by the ways used to accelerate the convergence of the 
chain. A very related topic is how to update the state parameters. It is possible 
to use a single move step updating each f3t, a multimove step updating /31, ... , f3T 
jointly, or a block move step updating blocks of f3t jointly. As shown in Carter 
and Kohn (1994) the speed of convergence in state space models when using single 
move step is slow because of the high correlation between the state parameters, 
generating highly correlated chains. Carter and Kohn (1994) have shown that the 
multimove step uses the information of that correlation and optimally updates the 
state parameters through Gibbs sampler iterations, raising the speed of convergence. 
In the DGLM context the introduction of the multimove Metropolis-Hastings step 
is not generally optimal because it can result in very low acceptance rates, leading 
the chain to stay in the same point for many iterations thus slowing down the 
convergence. It motivated the introduction of block move step Metropolis-Hastings 
updating in DGLM context by Shephard and Pitt (1997) and by Knorr-Held (1997). 
Knorr-Held (1997) proposes the use of conditional prior proposals with fixed 
blocks. Basically, he uses the full conditional prior distribution p(f3r,s I !3¢r,s, W) to 
generate a proposal value f3;,s for f3r,s = (f3r, ... , f3s), 1 :$ r < s :$ T . As the system 
equation disturbances are normal, this proposal is multivariate normal and it is easy 
to sample from. Indeed, since p(f3r,s l/3¢r,s, W, De) ex: fl:=r P(Yt lf3t) x N( mr,s, Cr,s) for 
some value of mr,s and Cr,s, the acceptance probability simplifies to the likelihood 
ratio 
. { 1 fl:=r P(Yt lf3t) } 
mm 
' 
( ld) 
fl:=r P(Yt lf3t 
0 
) 
It is relatively simple to obtain the proposal transition density and to calculate 
the acceptance probability, thus reducing considerably the computational complex-
ity. The problem with this approach is that it can lead to low acceptance rates, 
depending on the structure of the model and the time series under study. 

4. Dynamic GLMs 
65 
Shephard and Pitt (1997) propose sampling random blocks of the disturbances 
Ur,s = ( Ur, ... , u8 ) using as proposal transition density a second order Taylor ex-
pansion of ht(llt) = log p(Yt !Ot(Vt)) in the full conditional of Ur,s. The proposal 
transition density is then multivariate normal, and is calculated via the definition 
of artificial observations Yt = Ft/3t + VthHvt), t = r + 1, ... , s + 2 where usually 
~-l = -h~'(vt) and the replacement of the equation (1) by 
(8) 
The De Jong and Shephard (1995) simulation smoother is then used on model 
defined by equations (8) and (3) in order to generate the proposal for Ur,s. Shephard 
and Pitt (1997) also provide empirical evidence of gains in computational efficiency 
of randomly defined blocks over deterministic block definition. 
Gamerman (1998) suggests the use of a proposal transition density very similar to 
that of Shephard and Pitt (1997) but instead of block movement he proposes single 
movement with. reparametrization of the model in terms of the system disturbances 
and sampling from these disturbances. The proposal transition density is the full 
conditional distribution of Ut in the model with the modified observational equation 
(7), used by Singh and Roberts (1992) and Fahrmeir and Wagenpfeil (1997). The 
reparametrization rewrites the link function in terms of the system disturbances 
t 
t 
. 
as g(Jtt) =.lit = Ff'l;j=l G -Juj, with Ut ....- N(O, W), t = 2, .. . ,T and u1 "" 
N(a1, R1), 1f Gt = G, "Vt. 
5. 
Applications 
We present here two applications of this methodology to epidemiological data. 
The first is concerned with infectious disease control, when the number of cases of 
meningitis is observed through the time and the main purposes are to verify if the 
risk is below an acceptable level and if there is evidence of an epidemy. We present 
as example an analysis of the monthly series of meningococic meningitis in Rio de 
Janeiro, Brazil, from January 1976 to December 1992. The data was obtained from 
Rio de Janeiro State Health Secretary. 
The other application is the study of relationships between the number of cases 
of a disease, or deaths due to a disease, and possible explanatory variables. This can 
suggest ways to reduce the risk of the disease. We present here a preliminary analysis 
of the relationship between the number of child deaths due to respiratory diseases 
in Sao Paulo, Brazil, from 1st January 1991 to 31st December 1991, obtained from 
Sao Paulo City Health Secretary, and the levels of pollutants, obtained from Sao 
Paulo State Environment Secretary. 
We suppose here that the observations in both examples follow a Poisson dis-
tribution. The models include level, seasonallity and random effects for the first 
example, and level and regression variables for the second example. The random 
effects account for overdispersion, implying that the data dispersion may not be 
adequately explained by the Poisson model. 
We denote Yt the number of disease cases in timet, t = 1, ... , T, and p the peri-
odicity of the series. As usual, we use the logarithmic link function. The logarithm 
of the process mean evolve through the time decomposed into level, seasonallity, 
random effect and regression components as 
Yt 
"' Poisson( At) 

66 
Ferreira & Gamerman 
log At = lit = IJ.t + St + 4>t + x: f3 
11-t = 11-t-l + ult, 
ult"" N(O, WI) 
St = -(Bt-l+ ... + St-p+l) + U2t, 
U2t f'V N(O, W2) 
We complete the model with the following independent prior distributions: 11-1 "" 
N( al' Rl), St I'V N( a2, R2), t = 1, ... 'p- 1, 4>t ,...; N(O, u~), f3 I'V N( mp' Cp), wl I'V 
IG(nw1/2, nw1Sw1/2) and W2"" IG(nw2/2, nw2Sw2/2). 
The parameters {tJ.t}, { St}, { ¢t} and {3 do not have full conditionals available for 
sampling. We use the Metropolis-Hasting step within the Gibbs sampler to make the 
transitions for these parameters, as proposed by Gamerman (1998) and explained in 
subsection 4.2. Empirical evidence from studies with simulated data has shown that 
the methodology works well in estimating the parameters of the model. In addition, 
in the present applications we assessed the convergence by graphical inspection. 
Formal methods of convergence diagnosis, as those proposed by Geweke (1992) and 
Raftery and Lewis (1992) can also be used. 
5.1 Application 1: Meningococcic Meningitis 
We analyze here the monthly series of meningococic meningitis cases recorded in 
Rio de Janeiro city from January 1976 to December 1992, which is shown in figure 
4.1a. The main concerns are: 
• Is there a trend, or in other words, is it necessary to do something to reverse 
a rising movement in the number of cases? 
• Is there seasonal movement, implying need for more stringent surveillance in 
certain months? 
• Is it possible to tell something about the efficiency of the government surveil-
lance? 
• How can one ascertain whether the disease is under control? 
Figure 4.1 b shows the posterior mean and 95% credibility intervals for the level of 
the series. It can be seen that the level fell from January 1976 to January 1982 and 
then rose until January 1990. The movement in the end is not clear, but it seems 
that there is a tendency for stabilization or decay. 
The graph of seasonallity shown in figure 4.1c presents a clear pattern, with 
less number of cases in January, February and March, and more cases in July and 
August. In addition, the seasonal pattern is very stable through the years. 
The efficiency of the government surveillance defines the distribution of the num-
ber of cases. If the government succeeds in isolating the new cases then the occur-
rence of a case in a region does not increase significantly the risk in that region, and 
so the Poisson model can be a good approximation to the process. On the other 
hand, if the control is not efficient then the cases tend to appear in clusters implying 
in an increase of the variance of the number of cases and so in a departure from 
the Poisson process. Moreover, the random effects accommodate this type of depar-
ture: the module of the random effects are larger as the observations have greater 
variability than that expected in a Poisson process. The estimates of the random 
effects are shown in figure 4.1d and it seems that the module of the random effects 

4. Dynamic GLMs 
67 
(a) 
(b) 
1976 
1979 
1982 
1965 
1966 
1991 
1976 
1979 
1962 
1965 
1966 
1991 
(c) 
(d) 
1976 
1979 
1982 
1965 
1966 
1991 
1976 
1979 
1962 
1965 
1966 
1991 
FIGURE 4.1. Summary of posterior inference for application 1: a. original series; b. esti-
mates of the level of the series; c. estimates of the seasonal pattern; d. estimates of the 
random effects. In graphs b, c and d, solid lines represent mean and dashed lines represent 
the 95% credibility limits. 

68 
Ferreira & Gamerman 
TABLE 4.1. Lo arithm of the cross validation redictive densities 
With overdispersion 
Without overdispersion 
With 
Without 
seasonality 
seasonality 
6143.19 
6135.79 
6141.21 
6136.22 
were greater from September 1978 to January 1980 and from January 1987 to July 
1991. Therefore, it seems that the government surveillance was less efficient in these 
periods . 
. In order to get some quantitative indication of the need for seasonality and random 
effects in the model, models with and without seasonality and overdispersion 
were fitted and compared using the cross-validation predictive densities, whose use 
with Monte Carlo techniques is explained in Gelfand (1996). The logarithms of 
these cross validation predictive densities are shown in table 4.1 up to an arbitrary 
constant. Based on this criterion, the model with seasonality and overdispersion is 
prefered. The understanding of the state of control of the disease must be obtained 
through the analysis of all these graphs. In our example, the random effects are 
smaller in 1992 compared to the five previous years and the level seems to stabilize 
or decay in 1992. Therefore the surveillance seems to be more efficient in this year 
than in the previous years. In addition, we can easily obtain the predictive distri-
bution of future observations for each month and compare it with the observation 
itself when it becomes available. This can be used to build a monitoring system to 
trigger an alarm for mass vaccination. Finally, it would be interesting to incorporate 
the information about the vaccination programs to measure their impact. 
5.2 Application 2: Respiratory Diseases and Level of Pollutants 
We present here a preliminary analysis of the relationship between the daily 
number of children deaths due to respiratory diseases and the levels of the pollutants 
N02 (gfm3 ) and CO (ppm), in Sao Paulo, Brazil, from 1st January 1991 to 31st 
December 1991. There are records of other pollutants, but the correlations between 
the levels of the pollutants are very high and it is very difficult to measure the 
effect of each pollutant individually. Therefore, we removed from the model the 
less significant pollutants using an exploratory analysis. We plan to report a fuller 
analysis on this dataset elsewhere. Figure 4.2a presents the daily number of deaths 
ranging from 0 to 4. In our model we do not consider the medium and long term 
effects of the pollutants, and therefore we are measuring the instantaneous impact of 
the pollutants on the number of deaths. We supposed that the vector of regressors 
coefficient in the model is constant, as in the models of Harvey and Fernandes 
(1989), but this assumption can be easily relaxed. 
Figures 4.2c and 4.2d show the histograms of the coefficients of N02 and CO, 
respectively. They are located predominantly in the positive part of the real line. 
Indeed, Pr(f3No2 > OIDt) = 0.98062 and Pr(f3co > O!Dt) = 0.91424 indicating 
high probability of relation between level of pollutants and number of children 
deaths due to respiratory diseases. In addition, the estimated posterior means are 
E(f3No2 !Dt) = 0.00222 and E(f3co!Dt) = 0.06530. 
Figure 4.2b shows the histogram of W1, the variance of the disturbances in the 
system equation for the level. As it would be expected, the distribution is skewed and 

4. Dynamic GLMs 
69 
(a) 
(b) 
0 0 
0 00 
0 
<0 
0 
"¢' 
0 
C\1 
0 
1990 1991 
1991 
1991 
1991 1991 
1991 
1991 
0.0 
0.01 
0.02 
0.03 
Jan 
Feb 
Apr 
Jun 
Jul 
Sep 
Nov 
Jan 
(c) 
(d) 
0 0 
(Y) 
<0 
0 0 
C\1 
8 
C\1 
0 
0 
-0.002 
0.002 
0.006 
-0.1 
0.0 
0.1 
0.2 
FIGURE 4.2. Summary of posterior inference for application 2: a. original series; b. his-
togram of W1 j C. histogram of f3No 2 j d. histogram of f3co. 

70 
Ferreira & Gamerman 
has just one mode. The estimated posterior mean for W1 is E(WdDt) = 0.00758. 
It illustrates that in this approach there is no difficult in estimating the hyperpa-
rameters. 
6. 
Discussions and Extensions 
We presented in this chapter the dynamic generalized linear models, some al-
ternatives of inference and two applications using the Monte Carlo Markov chain 
alternative proposed by Gamerman (1998). This approach allows great flexibility in 
modeling and inference. The models can be stated through a study of the problem 
and there are few mathematical limitations, which means that the modeler is almost 
free to set the best model without thinking about the complexity of inference. 
There are many extensions that can be thought mainly with respect to the models. 
For example, in the second application the model measures the instantaneous impact 
of the level of pollutants on the number of deaths. Transfer functions can be used 
in order to measure the impact of the regressors in medium term. There are many 
functional forms for transfer functions that can be used and that have meaningful 
interpretation in epidemiology. We expect to address this question in a future paper. 
Acknowledgements 
Research was partially supported by grants from CNPq and PRONEX. We thank 
Getulio Silveira for allowing us the use of the dataset of meningococic meningitis 
(application 1) and CEA-IME/USP for allowing us the use of the dataset of respi-
ratory diseases and level of pollutants (application 2). 
References 
Anderson, B. D. 0. and Moore, J. B. (1979), Optimal Filtering, Prentice-Hall, New 
Jersey. 
Andrews, D. F. and Mallows, C. L. (1974). Scale mixtures of normality. Journal 
of the Royal Statistical Society (Ser. B) 36, pp. 99-102. 
Besag, J., York, J. and Mollie, A. (1991). Bayesian image restoration with two 
applications in spatial statistics. Annals of the Institute of Statistical Mathe-
matics 43, pp. 1-59. 
Carlin, P. B., Polson, N. G. and Stoffer, D. S. (1992). A Monte Carlo approach 
to nonnormal and nonlinear state-space modeling. Journal of the American 
Statistical Association 87, 493-500. 
Carter, C. K. and Kohn, R. (1994). On Gibbs sampling for state space models. 
Biometrika 81, 541-53. 
De Jong, P. and Shephard, N. (1995). The simulation smoother for time series 
models. Biometrika 82, 339-50. 

4. Dynamic GLMs 
71 
Devroye, L. (1986). Non-uniform random variate generation. Springer-Verlag, New 
York. 
Fahrmeir, L. (1992). Posterior mode estimation by extended Kalman filtering for 
multivariate dynamic linear models. Journal of the American Statistical As-
sociation 87, pp. 501-9. 
Fahrmeir, L., Hennevogl, W. and Klemme, K. (1992). Smoothing in dynamic gener-
alized linear models by Gibbs sampling. In Advances in GLIM and Statistical 
Modelling, Eds. L. Fahrmeir, B. Francis, R. Gilchrist and G. Tutz, Lecture 
Notes in Statistics 78, pp. 85-90. Springer, New York. 
Fahrmeir, L. and Wagenpfeil, S. (1997). Penalized likelihood estimation and iter-
ative Kalman filtering for non-Gaussian dynamic regression models. Comp. 
Stat. Data Anal. 24, 295-320. 
Fruhwirth-Schnatter, S. (1992). Approximate predictive integrals for dynamic gen-
eralized linear models. In Advances in GLIM and Statistical Mode/ling, Eds. 
L. Fahrmeir, B. Francis, R. Gilchrist and G. Tutz, Lecture Notes in Statistics 
78, pp. 101-6. New York: Springer. 
Gamerman, D. (1997). Markov Chain Monte Carlo: Stochastic Simulation for 
Bayesian Inference. Chapman and Hall, London. 
Gamerman, D. (1998). Markov chain Monte Carlo for dynamic generalized linear 
models. Biometrika 85, pp. 215-227. 
Gelfand, A. E. (1996). Model determination using sampling-based methods, in 
Markov Chain Monte Carlo in Practice. Eds. W. R. Gilks, S. Richardson and 
D.J. Spiegelhalter, Chapman and Hall, London, pp. 145-61. 
Geweke, J. (1992). Evaluating the accuracy of sampling-based approaches to the 
calculation of posterior moments (with discussion). In Bayesian Statistics 4 
(eds. J. M. Bernardo et al.), Oxford University Press, Oxford, pp. 169-93. 
Gilks, W. R., Richardson, S. and Spiegelhalter, D.J. (1996) (eds.). Markov Chain 
Monte Carlo in Practice. Chapman and Hall, London. 
Harrison, P. J. and Stevens, C.F. (1976). Bayes forecasting in action: case studies. 
Research Report 14, Department of Statistics, University of Warwick. 
Harvey, A. C. (1989). Forecasting, Structural Time Series Models and the Kalman 
Filter. Cambridge University Press, Cambridge. 
Harvey, A. C. and Fernandes, C. (1989). Time series models for count data or 
qualitative observations. Journal of Business and Economic Statistics 7, 407-
17. 
Kass, R. E., Tierney, L. and Kadane, J. B. (1989). Fully exponential Laplace ap-
proximations to expectations and variances of non-positive functions. Journal 
of the American Statistical Association 84, 710-16. 
Kitagawa, G. (1987). Non-Gaussian state-space modeling of nonstationary time 
series. Journal of the American Statistical Association 82, 1032-63. 

72 
Ferreira & Gamerman 
Knorr-Held, L. (1997). Hierarchical Mode/ling of Discrete Longitudinal Data - Ap-
plications of Markov Chain Monte Carlo. Herbert Utz Verlag Wissenschaff, 
Miinchen. 
McCullagh, P. and Neider, J. A. (1989). Generalized Linear Models. 2nd ed., Chap-
man and Hall, London. 
Meinhold, R. J. and Singpurwalla, N. D. (1989). Robustification of Kalman filter 
models. Journal of the American Statistical Association 84, 479-86. 
Migon, H. S. (1984). An approach to non-linear Bayesian forecasting problems with 
applications. Unpublished Ph.D. thesis, Department of Statistics, University 
of Warwick. 
Naylor, J. C. and Smith, A. F. M. (1982). Application of a method for the efficient 
computation of posterior distributions. Applied Statistics 31, 214-25. 
Neider, J. A. and Wedderburn, R. W. M. (1972). Generalized Linear Models. Jour-
nal of the Royal Statistical Society (Ser. A) 135, 370-84. 
Raftery, A. E. and Lewis, S. (1996). How many iterations in the Gibbs sampler?. 
In Bayesian Statistics 4 ( eds. J. M. Bernardo et al. ), Oxford University Press, 
Oxford, pp. 763-73. 
Shephard, N. (1994). Partial non-Gaussian state space. Biometrika 81, 115-31. 
Shephard, N. and Pitt, M. K. (1997). Likelihood analysis of non-Gaussian mea-
surement time series. Biometrika 84, 653-67. 
Singh, A. C. and Roberts, G. R. (1992). State space modelling of cross-classified 
time series of counts. International Statistical' Review 60, 321-36. 
Stevens, C. F. (1974). On the variability of demand for families of items. Oper. 
Res. Quart. 25, 411-20. 
West, M (1981). Robust sequential approximate Bayesian estimation. Journal of 
the Royal Statistical Society (Ser. B) 43, 157-66. 
West, M. and Harrison, J .(1997). Bayesian Forecasting and Dynamics Models. 2nd 
ed., Springer-Verlag, New York. 
West, M., Harrison, P.J. and Migon, H.S. (1985). Dynamic generalized linear mod-
els and Bayesian forecasting (with discussion). Journal of the American Stat-
istical Association 80, 73-96 

5 
Bayesian Approaches for 
Overdispersion in Generalized 
Linear Models 
Dipak K. Dey 
Nalini Ravishanker 
ABSTRACT Generalized linear models (GLM's) have been routinely used in statis-
tical data analysis. The evolution of these models as well as details regarding model 
fitting, model checking and inference have been thoroughly documented in McCul-
lagh and Neider (1989). However, in many applications, heterogeneity in the observed 
samples is too large to be explained by the simple variance function which is implicit 
in GLM's. To overcome this, several parametric and nonparametric approaches for 
creating overdispersed generalized linear models (OGLM's) were developed. In this 
article, we summarize recent approaches to OGLM's, with special emphasis given to 
the Bayesian framework. We also discuss computational aspects of Bayesian model 
fitting, model determination and inference through examples. 
1. 
Introduction 
Generalized linear models (GLM) are a standard class of models in contemporary 
statistical data analysis (McCullagh and Neider 1989). The widely available GLIM 
software as well as SPlus facilitate computation under these models. Bayesian fit-
ting of GLM's via Gibbs sampling is discussed in Dellaportas and Smith (1993). In 
GLM's, the underlying distribution of responses is assumed to be of the exponential 
family form, and a link function transformation of its expectation is modeled as a 
linear function of observed co variates, assuming that the variance of the response is a 
specified function of its mean. This allows modeling in various nonnormal situations 
such as the binomial, Poisson, negative binomial etc. However, in many applications, 
such a simple functional relationship is inadequate to handle the heterogeneity in 
the data; a common problem is the so-called overdispersion problem, where the 
variance of the response exceeds the nominal variance (Cox 1983). For instance, 
regression analysis of count data in biomedical research areas such as toxicology, 
epidemiology etc., must handle extra-Poisson variation. Count data analyzed under 
a Poisson assumption (Breslow 1984; Lawless 1987a,b; McCullagh and Neider 1989, 
Sec. 6.2) often exhibit overdispersion. In modeling categorical or ordered categorical 
data in longitudinal studies, toxicity studies, or in biological experimental research, 
the use of binomial or multinomial distributions with overdispersion is encountered. 
73 

74 
Dey & Ravishanker 
Crowder (1985) and Williams (1982) analyzed proportions using overdispersed bi-
nomial distributions. Overdispersion often results from latent heterogeneity, i.e., the 
sample of responses is drawn from a population consisting of many subpopulations. 
Historically, the presence of overdispersion prompted the need to define a wider 
class of models than the GLM. The different approaches include finite mixture 
models (Everitt and Hand 1981; Titterington, Makov and Smith 1985), exponential 
dispersion models, EDM (Jorgensen 1987), Efron's (1986) double exponential family 
models, EDM's embedded within Efron's double exponential family (Ganio and 
Schafer 1992), and generalized linear mixed models, GLMM (Breslow and Clayton 
1993). These classes are described in Section 2. 
Numerous methods have been proposed in the literature for estimation in overdis-
persed GLM's with particular emphasis given to binomial or Poisson models. The 
quasi-likelihood (QL) approach requires specification of the mean-variance relation-
ship rather than a full likelihood function. If y is a random variable with mean ft 
and variance V(~t) (a known function), the QL for y is defined as (Wedderburn 
1974) 
Q(~t; y) =I: f(/)dt. 
For a sequence of independent observations f) = (Yl, · · ·, Yn), the QL is defined as 
Q(jJ,; fl) = 2:7=1 Q(J-ti, Yi), where jJ, = (J-tl, · · ·, ftn)· In general, if jJ, is a function of p 
parameters 8 = (fh, · · ·, Bp), the QL estimates of 8 may be obtained by maximizing 
Q(jJ,; fl) under mild conditions (see McCullagh and Neider 1989). For the exponential 
family, the QL coincides with the log-likelihood, and therefore the estimation of () 
retains full asymptotic efficiency (Neider and Lee 1992). The QL approach for a 
model with a single dispersion parameter in the variance function was discussed 
by Finney (1971), McCullagh and Neider (1989) and Wedderburn (1974). Moore 
and Tsiatis (1989) and Williams (1988) showed that the QL approach generally 
gave consistent estimates of the regression coefficients even under a mispecified 
variance function. Wang (1996) presented a QL approach for ordered categorical 
data with overdispersion and illustrated using fish mortality data from quantal 
response experiments. The extended quasi-likelihood was defined in Neider and 
Pregibon (1987), using saddlepoint arguments. 
There has been some concern over the use of overly simple models for overdis-
persion in GLMs, especially in the context of the widely used binomial and Poisson 
models. To overcome this, it has been proposed that explanatory variables be in-
cluded into the model for dispersion (see Carroll and Ruppert 1982; Efron 1986; 
Jorgensen 1987; McCullagh and Neider 1989; Neider and Pregibon 1987; Smyth 
1989). 
The QL/M approach (also called the pseudolikelihood approach) uses only the 
mean and variance structure implied by a mixture model; the regression param-
eters are estimated by quasi-likelihood while the variance parameter is estimated 
by the method of moments. For Poisson counts, use of a gamma mixing distri-
bution results in a negative binomial distribution for the observed data. The QL 
approach yields asymptotically efficient estimates of the regression coefficients for 
the negative binomial model with fixed shape parameter, although the variance 
estimation by the method of moments is less efficient (see Breslow 1984; Lawless 
1987a; Williams 1982). Carroll and Ruppert (1982) discuss the QL/M approach for 
general heteroscedastic models. The penalized quasi-likelihood (PQL) was proposed 
as an approximate Bayes procedure for GLMM's (Laird 1978; Stiratelli, Laird and 
Ware 1984). 
The marginal quasi-likelihood (MQL) procedure, which is similar to Goldstein's 

5. Overdispersion in GLMs 
75 
(1991) apparoach for GLMM's with nested random effects, is appropriate when 
interest is focused on the marginal relationship between covariates and response 
(Liang, Zeger and Qaqish 1992). Both PQL and MQL may be implemented by stan-
dard software for variance components analysis of normally distributed responses 
and provide shrinkage estimates of random error terms and for estimates of the 
variance components. Breslow and Clayton (1993) used PQL and MQL to analyze 
data on seed germination (Crowder 1978) under an overdispersed binomial assump-
tion and a GLMM with a linear predictor including a random effect. They observed 
that there was little difference between PQL and MQL from a practical viewpoint; 
a major distinction between the two methods is that whereas the regression esti-
mates of the former depend strongly on the estimated variance components under a 
non-identity link, those of the latter do not. The PQL also has a limitation in that 
inferences on variance components are not very accurate and it fails to account for 
the contribution of estimated variance components when assessing uncertainty in 
both fixed and random effects (the latter problem exists in empirical Bayes meth-
ods as well). They also point out that use of PQL and MQL is more natural for 
simple over dispersion problems involving a log-linear, conditionally Poisson model 
than a competing approach in Liang and Waclawiw (1990). Zeger (1988) considered 
a time series model for the random effects in a simple overdispersion model for 
count data. Recent Bayesian procedures for GLLM models have used importance 
sampling ideas (Raghunathan 1994) or Gibbs sampling techniques (Besag, York and 
Mollie 1991; Zeger and Karim 1991). A general estimating equation approach (Dig-
gle, Liang and Zeger 1994) provides nearly efficient estimation relative to maximum 
likelihood estimates in overdispersion problems. Efron (1992) discussed asymetric 
maximum likelihood (AML) estimation of overdispersed generalized linear regression 
models, focusing on Poisson regression with application to an archeological data set. 
The AML approach is easy to implement, does not require explicit specification of 
a model for overdispersion and can be a good starting point for a more complete 
parametric analysis based on quasi-likelihood models, double exponential families or 
the overdispersion models (Gelfand and Dalal1990; Dey, Gelfand and Peng 1997). 
Several tests for overdispersion have been suggested in the literature. A test for 
Poisson overdispersion is given by Bohning (1994). Dean (1992) discussed tests for 
overdispersion with respect to a natural exponential family, which are powerful 
against arbitrary alternative mixture models with just the first two moments of 
the mixture distributions specified. Breslow (1990) derived tests of hypotheses in 
overdispersed Poisson regression and other quasi-likelihood models. 
The format of the rest of this paper is as follows. We review various models for 
handling over dispersion in Section 2, parametric approches for Bayesian model fit-
ting using MCMC techniques in Section 3 and nonparametric approaches in Section 
4. Section 5 discusses extension to multistage modeling. 
2. 
Classes of Overdispersed General Linear Models 
Unfortunately, finite parametric mixtures are awkward to work with and in many 
cases it is unclear how many components should be used. Further, even for a given 
number of components, inconvenient constraints on the model are necessary to 
insure identifiability, when the mixing weights and the parameters of the compo-
nent densities in the mixture are unknown. Although continuous mixtures alleviate 
these problems to a certain extent, once again the choice of a mixing distribution 

76 
Dey & Ravishanker 
is a problem. Mixture models, especially finite mixture models have been most fre-
quently used for creating a larger class of models for overdispersion. For instance, 
the one parameter exponential family defining the GLM is mixed with a two pa-
rameter exponential family for the canonical parameter fJ (or equivalently the mean 
parameter f-t), resulting in a two parameter marginal mixture family for the data. 
For the Poisson case, the gamma mixing distribution has been widely used, result-
ing in a negative binomial distribution for the observed data (Manton, Woodbury, 
and Stallard 1981 and Margolin, Kaplan and Zeiger 1981) while Hinde (1982) pro-
posed a log-normal mixing distribution. Shaked (1980) showed that such mixing 
necessarily inflates the model variance, while Gelfand and Dalal (1990) argued that 
taking additional observations within a population does not provide more informa-
tion about heterogeneity across populations. Finite parametric mixture models have 
another drawback in that the resulting over dispersed family of mixture models will 
no longer be an exponential family and consequently would be awkward to work 
with. In many cases, it is unclear how many components should be used. Further, 
even for a given number of components, inconvenient constraints on the model are 
necessary to insure identifiability when the mixing weights and the parameters of 
the component densities in the mixture are unknown. Although continuous mix-
tures alleviate these problems to a certain extent, once again the choice of a mixing 
distribution is a problem. 
For a given one parameter exponential family, Gelfand and Dalal (1990) consid-
ered a class of two parameter exponential family of models 
f(y I fJ, r) = b(y)eBy+rT(y)-p(B,r) 
(1) 
where depending on whether y is continuous or discrete, f is assumed to be a density 
with respect to Lebesgue measure or counting measure respectively. They showed 
that under the assumption that (1) is integrable over ye y, and T(y) is convex, 
then for a common mean, var(y) increases in r. It is presumed that the natural 
parameter space contains a two dimensional rectangle which, by translation, can be 
taken to contain the line T = 0. The associated one parameter exponential family 
which is obtained at T = 0 has the form 
f(y I fJ) = b(y)eey-x(B) 
(2) 
with x(fJ) = p(fJ, 0). As T increases from 0, var(y) increases relative to the vari-
ance under the associated one parameter exponential family so that r describes the 
over dispersion. 
A GLM is usually developed from the form of the one parameter exponential 
family (2). In particular f-t :: E(y) = x' (fJ), var(y) = x" (fJ) = V(f-t), the variance 
function. Here x' (fJ) is strictly increasing in f) so that f-t and f) are one-to-one (fJ = 
I -1 
(X ) 
(f-t)). A link function g is defined, which is a strictly increasing differentiable 
transformation from f-t to 7JeR1, so that g(f-t) = 7J = x.T [3, where x and [3 are, 
respectively, a p x 1 vector of known explanatory variables and an unknown vector 
of model parameters. 
Efron (1986) presented an alternative approach through so-called double expo-
nential families. Such families are derived as the saddle point approximation to the 
density of an average of n* random variables from a one parameter exponential fam-
ily for large n* . The parameter n* written suggestively by Efron as np, 0 < p < 1 for 
actual sample size n, introduces p as a second parameter in the model along with 
the canonical parameter fJ. The density corresponding to the double exponential 

5. Overdispersion in GLMs 
77 
family is 
j(y 1 fJ, p, n) = c(fJ, p, n )pt enp(By-x(B))+n(l-p)(B(y)y-x(B(y))) 
(3) 
I -1 
where fJ(y) = (X ) 
(y), y may be viewed as an average of n i.i.d. random variables, f) 
is the canonical parameter an~ p is a dispersion parameter. For regression problems, 
Efron assumed that fJ = xT {3 and that p = h(zT a) for a suitable h, where z is a 
known q x 1 vector and a is an unknown parameter vector. Using various expansions, 
~e showed that (3) permits attractive approximation as n increases. Most notably, 
f behaves like (2) with w = n and 4> = p- 1 . 
Model (2) is extended to an exponential dispersion model, EDM (Jorgensen 1987) 
by incorporating a dispersion parameter ¢, to give 
f(y 1 fJ, ¢) = b(y, ¢)ew(By-x(B))Irl>, 
(4) 
where w is a known "sample size"; here var(y) = </>V(p)jn. Whereas (1) is a custom-
ary two parameter exponential family, ( 4) is a one parameter family for each fixed 
</>.The ensuing problem was circumvented in an approximate fashion by Ganio and 
Schafer (1992) who considered the EDM as embedded in Efron's double exponen-
tial family and appealed to the associated asymptotic inference. Unlike the mixture 
case, these asymptotics result in overdispersion relative to the original exponen-
tial family which approaches a constant as n-+ oo (Efron 1986; Gelfand and Dalal 
1990). Specifically, Ganio and Schafer directly set ¢ = h(zT a) in (3). Assuming that 
zT a includes an intercept, they fit double exponential family models by the method 
of maximum likelihood and using an approximation to (3). Note that the Normal 
or Gamma families have scale parameters and hence can be written as EDM's. 
Gelfand and Dalal (1990) argued that an appeal to asymptotics is not neces-
sary to justify these models. Specifically, (1) not only includes Efron's model as a 
special case, but also a family discussed by Lindsay (1986). This is attractive be-
cause retaining the exponential family structure simplifies inference and the relative 
overdispersion behaves as in Efron's model. Note that regardless of n, (3) is of the 
form (1) with T(y) = fJ(y)y- x(fJ(y)), r = n(1- p) and fJ = npfJ. Straightforward 
calculation shows this T(y) is convex so that, in fact, (3) is a special case of (1 ). 
Although Gelfand and Dalal suggested that with the specification of link func-
tions, these two parameters could each be given the usual GLM structure, they did 
not pursue the matter. These models, which are referred to as overdispersed gen-
eralized linear models (OGLM's), were fully examined by Dey et al. (1997). They 
assumed independent responses Yi with associated covariates Xi, and Zi, i = 1, 2, .. , n, 
where the components of x and z need not be exclusive. Let y = (Yl, · • ·, Yn) and 
define f}i = g(xT /3) and Ti = h(zT a) where g and h are strictly increasing. The 
corresponding joint density is from (1) 
n 
f('YI/3, a)= II e(JiYi+TiT(Yi)-p(Bi,ri). 
(5) 
i=l 
The monotonicity of g and h is natural and insures that f}i is monotonic in Xii and 
that Ti is monotonic in Zil, facilitating interpretation. Of course such monotonicity 
does not imply that, e.g., fJi is monotone in each covariate. The form xT j3 allows 
a covariate to enter as a polynomial. If an explanatory variable appears in Xi but 
not in Zi, say xu, then l1f1- = p<2,0)(fJi, Ti)g' (xT /3)f3r. But since p(2,o) and g' are 
strictly positive, Jli is strictly monotone in xu with the sign of f3r determining the 
direction. If the variable appears in Zi as well, its influence on Jli is less clear, since 

78 
Dey & Ravishanker 
now ~ 
involves p(l,l), the covariance between y and T(y). A practical drawback 
to working with (1) is that p(fJ, r) is not available explicitly. While x(fJ) in (2) is 
usually an explicit function of fJ, p(fJ, r) = logJ b(y)e8Y+rT(Y)dy usually requires a 
univariate numerical integration or summation. Dey et al. (1997) mentioned that 
this was not a problem in the practical examples they investigated. Gelfand and 
Dalal (1990) argued that the performance of (1) is not sensitive to the choice of 
T(y). Attention should focus on the specification of f}i and Ti rather than on the 
stochastic mechanism (1 ). 
Dey, Peng and Larose (1995) modeled heterogeneity and overdispersion through 
the notion of a parametrized weighted distribution. Suppose the random variable 
of interest Y is distributed over a population of interest with probability density 
f(ylfJ), where fJ is the underlying parameter of interest. The observed data is now 
a random sample from the following weighted distribution with density function 
fw( IB ) - w(y, r)f(ylfJ) 
y 'T -
EJ[w(y, r)] ' 
(6) 
where the expectation in the denominator is the normalizing constant. The overdis-
persed model is viewed as a perturbation of the original model, so that the latter 
can be formed as a weighted distribution of the form (6). These models are regular 
and parsimonious compared to a GLM and enable the capture of overdispersion 
within an exponential family framework. Section 3 describes fitting (5) in a para-
metric setting while Section 4 describes a nonparametric framework using MCMC 
techniques. 
3. 
Fitting OGLM in the Parametric Bayesian Framework 
Dey et al. (1997) examined inference for OGLM's using Markov chain Monte 
Carlo methods. The advantages of their approach include the familiarity of expo-
nential families, the ready interpretation of model parameters, the unification of 
modeling by absorbing earlier cases and exact inference rather than inference based 
on asymptotic approximations. They adopted a Bayesian perspective in fitting these 
models but used noninformative prior specifications since primary concern lies in 
the modeling incorporated in the likelihood in (5). Hence inference will be close to 
that arising from maximum likelihood though estimates of variability will be exact 
rather than asymptotic and entire posteriors for model parameters would result. Re-
quired Bayesian computation is handled through Markov chain Monte Carlo using 
a Metropolis algorithm resulting in samples essentially from the joint posterior dis-
tribution which may be summarized to provide any desired inference. Such samples 
may also be used as the starting point for sampling from predictive distributions to 
investigate questions of model adequacy and model choice. 
3.1 
Model Fitting 
The Fisher information matrix associated with (5) is of interest in Bayesian model 
fitting since the square root of the determinant of this matrix, as a function of f3 
and <i, is known as Jeffreys' prior and is commonly used as a "noninformati ve" 
specification. Denoting the right side of (5) as L(P, a, y), Dey et al. (1997) showed 

5. Overdispersion in GLMs 
79 
that 
Let X denote the n X p design matrix arising from the x~s, z the n X q de-
sign matrix arising from the z;s, Me an n x n diagonal matrix with (Me)u ::: 
p<2•0)(fh, ri)(g' (xT /;))2 , M,. an nx n diagonal matrix with (M,. )ii = p<0•2)(fh, Ti)(h' (zT &))2 
and Me,,. an n x n diagonal matrix with (Me,,. )u = p(l,l)(fJi, Ti)(g' (xT /3))(h' (zT ii )). 
Then 
(7) 
and Jeffreys' prior is II(/;, ii)l!. Computation of (7) requires calculation of p, p<2•0), 
p<0•2) and p(l,l) which in turn requires numerical integration or summation of the 
form J yeT(y)db(y)eey+'rT(y)dy for various c's and d's. 
Dey et al. showed that the posterior is proper under Jeffreys' prior or a flat 
prior for f; and ii, provided L(/3, a; jj) is log concave. Log concavity of L(/3, 0; Y) 
is discussed in Wedderburn (1976) in the context of properties of MLE's and in 
Dellaportas and Smith (1993) with respect to simplifying Monte Carlo sampling of 
/3. In particular, for OGLM Dey et al established log concavity by verifying a simple 
nonnegative definiteness condition. Letting fJ = g( rJ), 
T = h('r) they considered 
the function a(rJ, 1) = fJ(rJ)Y + r('Y)T(y) - p(fJ(rJ), r(f')). If-~ ;::: 0, -~ ;::: 0 
I 
{}
2
a 
{}
2
a I 
and 
- 8;:a' - 8~~~ 
;::: 0, (5) is log concave. For the canonical case () = 1], T = 
-
fJ-y81J ' 
-
fFi'3 
'' -~ = var(y), -~ = var(T(y)) and the determinant becomes var(y)var(T(y)) 
- cov2(y, T(y)) which is nonnegative whence log concavity holds. 
Sampling based fitting of the OGLM in the Bayesian framework was carried out 
using standard tools. Dey et al. (1997) used a Gaussian proposal to implement a 
Metropolis algorithm (Hastings 1970) although adaptive rejection sampling within 
the Gibbs sampler as in Gilks and Wild (1992) is another possibility. They used 
multiple starts in the vicinity of the MLE for j3 (under T = 0) and at a = 6. 
Evaluation of L(/3, &; Y) requires repeated calculation of the function p(fJi, Ti), which 
fortunately is a routine univariate numerical integration. 
3.2 Example: Overdispersed Poisson Regression Model 
Dey et al. (1997) used as a motivating example data involving damage incidents to 
cargo ships (McCullagh and Neider 1989, p204). For each of 34 ships the aggregate 
months in service were recorded as well as the number of damage incidents over that 
period. Explanatory factors are ship type having 5 levels (A, B, C, D, E), year of 
construction having 4levels (CP1, CP2, CP3, CP4) and period of operation having 
two levels (SPl, SP2). Since the response is a count, McCullagh and Neider proposed 
a Poisson regression presuming that the expected number of damage incidents is 

80 
Dey & Ravishanker 
directly proportional to the aggregate months in service, i.e., the total period of 
risk. Using a canonical link the GLM sets 
I 
(} 
= 
log(aggregate months service)+ f3o +effect due to ship type 
+ 
effect due to year of construction +effect due to service period. 
(8) 
The log( aggregate months service) term is called an offset, its coefficient fixed at 1 
as a result of the proportionality assumption. They also incorporated a dispersion 
parameter¢ in the Poisson model, whose estimate was¢= 1.69, indicating overdis-
persion relative to the standard Poisson density which, intrinsically, has ¢ = 1. 
On examining the nonstandardized residuals Yi -fJ.i under the GLM in (8), Dey et 
al. noted that some observations did not show a good model fit, and this situation 
is not improved through an EDM. The use of OGLM's to fit this data is illustrated 
in Dey et al., a brief summary of which follows. 
Taking ( 1) as the model for (} = xT [3 in (7), Dey et al. considered three spec-
ifications for r. In the simple case (model 1) they set r = 0, which resulted in a 
9 parameter Bayesian GLM for (1). Model 2 incorporated a constant dispersion 
parameter r = ao with the convex function, T(y) = (Y + 1) log(y + 1), yielding a 
10 parameter model. Finally, anticipating that overdispersion might increase with 
exposure, model 3 set r = a0 + a1log( aggregate months service), using the same 
T(y), a 11 parameter model. 
Results from these model fits provide evidence of overdispersion ( r > 0) and, 
in addition, evidence that a 1 > 0, supporting the hypothesis that overdispersion 
increases with exposure to risk. It was also seen that model 3 better gave a better 
fit for larger values of y, which are associated with greater exposure. 
3.3 Model Determination for Parametric OGLM's 
Model determination includes model adequacy and model choice. Formal Bayesian 
model determination proceeds from the marginal predictive distribution of the data 
f(y) evaluated at the observed data, Yobs. Since this high dimensional density is 
hard to estimate well and its value is hard to calibrate it may be preferable, as ar-
gued in Gelfand (1995), to look at alternative predictive distributions, in particular 
univariate ones such as the posterior predictive density at a new Yo, f(Yo IYobs) or the 
cross-validated predictive density of say Yr, f(Yr !Y(r),obs) (which can be compared 
with Yr,obs ). Although the model determination diagnostics are admittedly informal, 
they are in the spirit of widely used classical EDA approaches and are attractive 
since they permit examination of model performance at the level of the individual 
observation. Required calculations may be carried out by sampling f(Yr IY(r),obs) 
which may be achieved using the posterior sampler as described in Gelfand and 
Dey (1994). 
For the cargo data analysis, Dey et al. (1997) adopted a cross-validation approach, 
paralleling widely used classical regression strategy. In particular, they considered 
the proper densities f(Yr I Y(r)), r = 1, 2, ... , 34, where Y(r) denotes fj with Yr 
removed. In fact, they conditioned on the actual observations Y(r),obs creating the 
predictive distribution for Yr under the model and all the data except Yr. For model 
determination this would require comparison, in some fashion, of f(Yr I Y(r),obs) 
with the Tth observation, Yr,obs. Such cross validation is discussed in Gelfand, Dey 
and Chang (1992) and in further references provided therein. 

5. Overdispersion in GLMs 
81 
A natural approach for model adequacy is to draw, for each r, a sample from 
f(Yr I Y(r ),obs), and compare this sample with Yr,obs. In particular, based on this 
sample, it is possible to obtain the .025 and .975 quantiles of f(Yr I Y(r),obs) say 
y and Yr and see how many of the Yr obs belongs to [y , Yrl· Under each of the 
:::...r 
' 
-r 
three models fit to the cargo ship data, at least 27 of the 34 intervals contained 
the corresponding Yr,obs. They also obtained the lower and upper quartiles of f(Yr I 
Y(r),obs) to see how many Yr,obs belonged in their interquartile ranges; they found 
13, 15 and 18 under model 1, model 2 and model 3 respectively. Since we would 
expect half, i.e., 17 under the true model, both model 2 and model 3 perform close 
to expectation though all three models seem adequate. 
A well established tool for model choice is the conditional predictive ordinate 
(CPO), f(Yr,obs I Y(r),obs), a large value of which implies agreement between the 
observation and the model. For comparing models, the ratio dr = ~~~r,o'"/~(r),obs•~i) 
r,obs 
(r),obs• i' 
(or perhaps log dr) indicates support by the rth point for one model versus the other 
(see Pettit and Young, 1990). For the cargo data, model 3 provided the best fit. 
A simple diagnostic with a frequentist flavor, 2:;!1 (Yr,obs- E(t-triY)) 2/34 yielded 
values 6.74, 7.11 and 8.61 respectively for model1, model2 and model3; compared 
with the EDM result of 6. 70, all three models are adequate. All of the foregoing 
calculations are carried out by sampling f(Yr IY(r ),obs) which may be achieved using 
the posterior sampler as described in Gelfand and Dey (1994). 
4. 
Modeling Overdispersion in the Nonparametric 
Bayesian Framework 
In some problems, samples exhibit extra heterogeneity because observations are 
drawn from an overall population which would be more effectively modeled as a 
mixture of subpopulations. Such behavior cannot be captured by univariate ex-
ponential family models and even OGLM's cannot explain all types of hetero-
geneity adequately. To alleviate the problems in finite parametric mixture mod-
els, Mukhopadhyay and Gelfand (1997) captured the specification of overdispersion 
nonparametrically through Dirichlet Process (DP) mixing. They extended GLM's 
to DPMGLM's (Dirichlet process mixed GLM's) and OGLM's to DPMOGLM's, 
with specific emphasis on binomial and Poisson regression mode~s. DPMOGLM's 
afford the possibility of capturing a very broad range of heterogeneity resulting in 
the most flexible GLM's yet proposed and are an alternative to GLMM's (Breslow 
and Clayton 1993). Note that they intentionally retained the GLM aspect with re-
gard to the mean. In theory, although one could DP mix over the coefficients in the 
presumed linear mean structure on a translated scale, these coefficients would vanish 
from the likelihood resulting in a mixture model which would no longer be a linear 
model in any sense. Their structure retains these coefficients and the associated lin-
ear structure to permit the appealing interpretation they provide with regard to the 
relationship between a response variable and a collection of explanatory variables. 
4.1 
Fitting DP Mixed GLM and OGLM 
The basics of DP mixing as described in Blackwell and McQueen (1973), Ferguson 
(1973) and Sethuraman and Tiwari (1982) are extended to mixture modeling as 

82 
Dey & Ravishanker 
follows (for details, see Mukhopadhyay and Gelfand, 1997). Consider {/(·10): 8 E 
e c ~d} to be a parametric family of densities with respect to a dominating 
measure It and consider also the family of probability distributions :F = {FG : G E 
P} with densities 
f(yiG) = j f(yl8)dG(8) 
(9) 
with respect to J-t, so that :F becomes a nonparametric family of mixture distribu-
tions; it is assumed that the mixing distribution G comes from a DP on P, i.e., G rv 
DP(vGo), where v is a precision parameter and Go is a proper base probability 
distribution in P. A semi parametric class of models may be obtained by choosing 
to DP mix with respect to a subset w of 8 instead of 8 itself. Such DP mixture mod-
els have become increasingly popular for modeling when conventional parametric 
models are either hard to fit or impose unreasonably strict constraints on the class 
of distributions (see for e.g. Escobar and West (1993); MacEachern 1992; Bush and 
MacEachern 1993 in the context of hierarchical modeling). 
Mukhopadhyay and Gelfand (1997) extended the GLM's which arises from (2) 
by introducing DP mixed GLM's (DPMGLM's) as follows. In the form of the usual 
GLM's they write x' /3 ~ a+ x' /3, so that the intercept term is written separately; 
under a canonical link fJ =a+ x' /3 and they mix over a. DP mixing over a extends 
the basic GLM model to capture heterogeneity in the population with regard to 
the location of the mean, presuming that the covariate relationship is unaffected 
by such centering. The resulting density for an observation y = (y1 , • • ·, Yn) with 
associated covariate x under this model is, following (9), 
!(Yix, /3, G)= j f(Yix, /3, a)dG(a), 
(10) 
where f(ylx, /3, a) is the model in (2) under the canonical link. They used a vague 
normal prior specification for /3 with G rv DP(vGo) and carried out Gibbs sampling 
for models created under (10). The complete conditional densities for the f3i 's are 
log concave (Dellaportas and Smith 1992) and are sampled using adaptive rejec-
tion sampling (Gilks and Wild 1992). Alternatively, Metropolis steps may be used 
(Tierney, 1994). The latent ai are sampled following the approach of MacEachern 
and Muller (1994). This approach is applicable when a general link function is used 
and two possible forms for 8 are considered, viz. 8 = h(a + x' /3) or 8 = a+ h(x' /3). 
Mukhopadhyay and Gelfand discussed mixing of OGLM's to introduce further 
flexibility into the family of models. Imitating the earlier discussion, they replaced 
8 by a+x' /3 in (1), writing the resultc:nt density as f(ylx, /3, a, r). A Bayesian model 
requires specification of a prior for (/3, a, r). Three possibilities exist: 
and 
f(Yix, /3, Gcx, r) = j f(yjx, /3, a, r)d Gcx(a), 
f(y!x,/3,a,Gr) = j f(ylx,/3,a,r)dGr(r), 
f(ylx, /3, Gcx,r) = j f(ylx, [3, a, r)d Gcx,r(a, r). 
(11) 
(12) 
(13) 
Estimation under all these forms may be handled in the Bayesian framework and 
details are given in Mukhopadhyay and Gelfand (1997). 

5. Overdispersion in GLMs 
83 
4.2 Example: Overdispersed Binomial Regression Model 
Lindsey (1993) analyzed data from an egg hatching experiment conducted using 
72 tanks. Only two covariates are available, temperature ( x1) and salinity ( x2). 
The responses are the number of eggs hatched (yi) out of ni eggs in the ith tank 
( i = 1, · · · , 72). Lindsey observed that, for this data, heterogeneity is too great to be 
explained by a standard binomial regression using these covariates. Mukhopadhyay 
and Gelfand investigated the binomial GLM and their proposed extensions. They 
assumed a flat prior for each coefficient (/31 , /32). With T(y) = y2 , a uniform prior 
on the interval [-1, 1] was chosen for r in each of the OGLM and the DPMOGLM. 
Other prior specifications are the same as described in the general case. 
Using the log CPO ratios, they compared the performance of the fitted models. 
This showed that approximately 96% of the data are better explained by the OGLM 
than the GLM. Again, comparing the DP mixed models with the OGLM, they 
found that approximately 76% and 85% of the data support the DPMGLM and 
the DPMOGLM respectively. In each of these comparisons the average of the log 
GPO's was at least 33.253, lending strong support to their conclusions. Lastly, in 
comparing the DPMOGLM to the DPMGLM, the GPO's were roughly split in half 
and the average of the log GPO's was only 0. 795, suggesting little improvement by 
the former model and adoption of the latter in the interest of parsimony. 
To compare common parameters across different models they plotted the posterior 
densities; the posterior density of r for the OGLM was almost degenerate at 1, 
indicating the inadequacy of the OGLM in explaining the heterogeneity present in 
the data. The posterior density of r under the DPMOGLM appeared to be unimodal 
with mode near 0.7. 
4.3 Model Determination for Dirichlet Process Mixed Models 
Mukhopadhyay and Gelfand showed ex.Plicit computation of f(yo I ,Yobs) and f(yr,obs I ,Yr),obs) 
for a generic DP mixed model in a regression setting with covariate values Xi asso-
ciated with responses, Yi, i = 1, · · ·, n. Under the assumption that the density of 
Yi depends upon a vector of parameters 0 which is partitioned as 0 = ( ij, w ), they 
DP mixed over w, assuming that vis specified. The case of unknown v can be han-
dled in a fully Bayesian way using Gibbs sampling as well. They assumed a specific 
parametric prior on ij, 1r( ij) which could also arise hierarchically say as 1r( iji.:Y) · 1r( .:Y) 
where .:Y is a vector ofhyperparameters. Note that although 1r(,:Y) need not be proper 
to consider posterior or cross-validated predictive densities, it must be proper in 
order that the marginal predictive density be proper and hence interpretable. 
With the above assumptions the nonparametric Bayesian regression model takes 
the form 
n IT f(Yi I xi, ij, G) · f( GIGo, v) · 1r( ij) 
i=l 
fi j f(ydX;, ij,W;) dG(W;) 
i=l 
· f( G!Go, v) · 1r( ij) 
(14) 
Marginalizing the posterior predictive density for Yo at xo over G they obtained 
f(Yolxo, Yobs) = ~ ~ ~ f(Yolxo, ij,wo) · f(wolw) · f(w, ijliiobs)· 
(15) 
Jfi lw lwo 
Assuming the sampling based fitting of (14) has yielded draws (wt, ijt), l = 
1, · · ·, B from f(w, ijiYobs), Monte Carlo integration for (15) is done in two ways. If, 

84 
Dey & Ravishanker 
given (wt, ijf) we draw w~1 from f(wolw), we obtain 
B 
f(Yolxo, Yobs) = B- 1 Lf(Yolxo, qt,w~1)· 
(16) 
1=1 
Alternatively, if we do the innermost integration in (15) we need not sample the 
w~1' obtaining 
!-( 1- - ) 
B- 1(v+ n)- 1 
Yo Xo, Yobs 
= 
B 
n 
L{Lf(Yolxo, qt,wt) + v j f(Yolxo, qt,w)dGo(w)}, 
1=1 i=1 
(17) 
where f is a mixture of distributions. Though (17) would generally be prefer-
able to (16), since exact integration replaces sampling, the unpleasant integral in 
(17) must be computed B times. Also, should we wish to obtain samples from 
f(Yolxo,Yobs) we can do so by drawing y~1 from f(Yolxo,qt,w~1 ) again for l = 
1 · · ·,B. They showed that computations for the cross validation predictive density 
through f(Yr lxr, Y(r),obs) was similar. Denoting the new Yr and the corresponding 
Wr by Yo and wo respectively, they approximated f(Yolxr, Y(r),obs) through a ratio 
of Monte Carlo integrations. Since the Monte Carlo integration for the denominator 
in this ratio can be unstable, they suggest preference for the posterior predictive 
density. 
For the egg hatching data, use of the log CPO ratios showed that approximately 
96% of the data are better explained by the OGLM than the GLM. Again, compar-
ing the DP mixed models with the OGLM, approximately 76% and 85% of the data 
support the DPMGLM and the DPMOGLM respectively. In each of these com-
parisons the average of the log CPO's was at least 33.253, lending strong support 
to these conclusions. Lastly, in comparing the DPMOGLM to the DPMGLM, the 
CPO's were roughly split in half and the average of the log CPO's was only 0.795, 
suggesting little improvement by the former model and adoption of the latter in 
the interest of parsimony. To compare common parameters across different models 
they plotted the posterior densities; the posterior density of r for the OGLM was 
almost degenerate at 1, indicating the inadequacy of the OGLM in explaining the 
heterogeneity present in the data. The posterior density of r under the DPMOGLM 
appeared to be unimodal with mode near 0.7. Although the posterior densities of /31 
and of /32 for all the four models were shifted to the left and were less concentrated, 
they still strongly supported nonzero coefficients. 
5. 
Overdispersion in Multistage GLM 
Models are sometimes formulated as hierarchical or multistage GLM's in which 
case we may incorporate overdispersion or D P mixing at one or more of the stages. 
For illustration, consider two examples: semiparametric nested random effects mod-
els and semiparametric errors in variables models. 
The case of nested normal random effect models was discussed in Goldstein (1986). 
More generally, suppose response Yijk is modeled as a GLM with canonical parame-
ter J.L + ai + /3ij. For instance, Yii k might be a count recorded for the kth child in the 

5. Overdispersion in GLMs 
85 
jth class in the ith school with ai being a random school effect and f3ij being a ran-
dom class effect nested within school. They defined li = p+ai, Pii = p+ai+/3ij and 
the hierarchical model f(Yijk IPij) · f(Pij IIi)·!( li IP ). Here/( li IP) would be extended 
to a GLM involving school level covariates say Xi, i.e., the canonical parameter as-
sociated with li would be p + x(/f:Y). Similarly, f(Pii IIi) would be extended to a 
GLM involving class within school covariates say Wij and having canonical param-
eter li + w~1 j3(P). One can introduce DP mixing at the third stage, i.e., mixing on 
p, based on which the model for li bec_?mes j( 1dxi, jj("Y), G). The Bayesian model 
is completely specified with priors on f3("Y), f3(P) and G. OGLM's could be added, 
perhaps most naturally at the first stage and Gibbs sampling proceeds straightfor-
wardly. 
In the errors-in-variables problem, the class of models described in Carroll (1992) 
may be extended. Mukhopadhyay and Gelfand (1997) modeled the response Yi as 
f(ydxiljj(Y),Gy) = J f(Yilxi,,B(Y),a~Y)) dGy(a~Y)). That is, conditional on a~Y), Yi 
follows a GLM with canonical parameter a~Y) + x~jj(Y) and they DP mix over a~y). 
Suppose a component of Xi say Xli is not directly observable. Its actual level may 
only be known to arise with error around some nominal level or, more generally, in 
place of x1i, a vector Wi of surrogate variables may be observed. Then they model 
Xli as /(Xli lwi, ,B(x), Gx) = J f(xli lwi, jj(x), a~x)) d Gx( a~x)). That is, conditional on 
a~x), x1i follows a GLM with canonical parameter a~x) + w~jj(x) and they DP mix 
over the a~x). This introduces DP mixing at both modeling stages. The Bayesian 
model is completely specified with priors on jj(Y), jj(x), Gy and Gx. They fit this 
model using Gibbs sampling, the details of which may be obtained from their paper. 
Additionally, OGLM's could replace the GLM's in the above discussion, leading to 
an extension of the DPMOGLM to a multistage setup. 
References 
Barndorff-Nielsen, O.E. (1978). Information and Exponential Families in Statis-
tical Theory. John Wiley & Sons, New York. 
Besag, J ., York, J. and Mollie, A. (1991). Bayesian Image Restoration, with Two 
Applications in Spatial Statistics (with discussion). Annals of the Institute of 
Statistical Mathematics, 43, 1-59. 
Bohning, D. (1994). A note on a test for Poisson overdispersion. Biometrika, 81, 
418-419. 
Box, G.E.P. and Tiao, G.C. (1992). Bayesian Inference in Statistical Analysis. 
John Wiley & Sons, New York. 
Breslow, N. (1984). Extra-Poisson Variation in Log-Linear Models. Applied Statis-
tics, 33, 38-44. 
Breslow, N. (1990). Tests of Hypotheses in Overdispersed Poisson Regression and 
Other Quasi-Likelihood Models. Journal of the American Statistical Associa-
tion, 85, 565-571. 
Breslow, N. and Clayton, D. (1993). Approximate inference in Generalized Linear 
Mixed Models. Journal of the American Statistical Association, 88, 9-25. 

86 
Dey & Ravishanker 
Carroll, R.J. and Ruppert, D. (1982). Robust Estimation in Heteroscedastic Linear 
Models. Annals of Statistics, 10, 429-441. 
Cox, D.R. (1983). Some remarks on overdispersion. Biometrika, 70, 269-274. 
Cox, D.R. and Reid, N. (1987). Parameter Orthogonality and Approximate Con-
ditional Inference (with discussion). Journal of the Royal Statistical Society, 
Ser. B, 49, 1-39. 
Crowder, M.J. (1978). Beta-Binomial ANOVA for Proportions. Applied Statistics, 
27, 34-37. 
Crowder, M.J. (1985). Gaussian Estimation for Correlated Binomial data. Journal 
of the Royal Statistical Society, Ser. B, 47, 229-237. 
Dean, C.B. (1992). Testing for Overdispersion in Poisson and Binomial Regression 
Models. Journal of the American Statistical Association, 87, 451-457. 
Dellaportas, P. and Smith, A.F.M. (1993). Bayesian Inference for Generalized Lin-
ear and Proportional Hazards Models via Gibbs Sampling. Applied Statistics., 
42~ 443-460. 
Dey, D.K., Gelfand, A.E. and Peng, F. (1997). Overdispersed Generalized Linear 
Models. Journal of Statistical Planning and Inference, 64, 93-108. 
Dey, D.K., Peng, F. and Larose, D. (1995). Modeling Heterogeneity and Extraneous 
Variation using Weighted Distributions. In Model Oriented Data Analysis, C. 
P. Kitsos and W.G. Muller, eds., Physica Verlag, Heidelberg, 241-249. 
Diggle, P.J ., Liang, K-Y, and Zeger, S.L. (1994). Analysis of Longitudinal Data. 
Oxford University Press, Oxford. 
Efron, B. (1986). Double Exponential Families and Their Use in Generalized Linear 
Regression. Journal of the American Statistical Association, 81, 709-721. 
Efron, B. (1992). Poisson Overdispersion Estimates Based on the Method of Asym-
metric Maximum Likelihood. Journal of the American Statistical Association, 
87, 98-107. 
Everitt, B.S. and Hand, D.J. (1981 ). Finite Mixture Distributions. Chapman and 
Hall, London. 
Finney, D.J. (1971 ). Pro bit Analysis (3rd ed. ). Cambridge, U.K.: Cambridge Uni-
versity Press. 
Ganio, L.M. and Schafer, D.W. (1992). Diagnostics for Overdispersion. Journal of 
the American Statistical Association, 87, 795-804. 
Gelfand, A.E. (1995). Model determination using sampling-based methods. In 
Markov Chain Monte Carlo in Practice, eds. W. Gilks, S. Richardson and 
D. Spiegelhalter. Chapman and Hall, London, 145-161. 
Gelfand, A.E. and Dalal, S.R. (1990). A Note on Overdispersed Exponential Fam-
ilies. Biometrika, 77, 55-64. 
Gelfand, A.E. and Dey, D.K. (1994). Bayesian Model Choice: Asymptotics and 
Exact Calculations. Journal of the Royal Statistical Society, Ser. B, 56, 501-
514. 

5. Overdispersion in GLMs 
87 
Gelfand, A.E., Dey, D.K. and Chang H. (1992). Model Determination Using Pre-
dictive Distributions with Implementation Via Sampling-Based Methods. In 
Bayesian Statistics 4, (J. Bernardo et al. eds.), Oxford University Press, Ox-
ford, 147-167. 
Gilks, W.R. and Wild, P. (1992). Adaptive Rejection Sampling for Gibbs Sampling. 
Journal of the Royal Statistical Society, Ser. C, 41, 337-348 
Goldstein, H. (1991). Nonlinear Multilevel Models, With an Application to Discrete 
Response Data. Biometrika, 78, 45-51. 
Hinde, J. (1982). Compound Poisson Regression Models. in GLIM 82: Proceed-
ings of the International Conference on Generalized Linear Models, ed. R. 
Gilchrist, Berlin: Springer-Verlag, 109-121. 
Jorgensen, B. (1987). Exponential Dispersion Models (with discussion). Journal of 
the Royal Statistical Society, Ser. B, 49, 127-162. 
Laird, N.M. (1978). Empirical Bayes Methods for Two-Way Contingency Tables. 
Biometrika, 65, 581-590. 
Lawless, J.F. (1987a). Negative Binomial and Mixed Poisson Regression. Canadian 
Journal of Statistics, 15, 209-225. 
Lawless, J.F. (1987b). Regression Methods for Poisson Process Data. Journal of 
the American Statistical Association, 82, 808-815. 
Liang, K.Y. and Waclawiw, M.A. (1990). Extension of the Stein estimating pro-
cedure Through the use of Estimating Functions. Journal of the American 
Statistical Association, 85, 435-440. 
Liang, K.Y, Zeger, S.L. and Qaqish, B. (1992). Multivariate Regression Analysis for 
Categorical Data (with discussion). Journal of the Royal Statistical Society, 
Ser.B, 54, 3-40. 
Lindsay, B. (1986). Exponential Family Mixture Models (with least squares esti-
mators). The Annals of Statistics, 14,124-37. 
MacEachern, S.N. and Muller, P. (1994). Estimating Mixture of Dirichlet Process 
Models. Technical Report No. 94-11, Duke University, ISDS. 
Manton, K.G., Woodbury, M.A., and Stallard, E. (1981). A Variance Components 
approach to Categorical Data Models with Heterogeneous Cell Populations: 
Analysis of Spatial Gradients in Lung Cancer Mortality Rates in North Car-
olina counties. Biometrics, 37, 259-269. 
Margolin, B.H., Kaplan, N. and Zeiger, E. (1981). Statistical Analysis of the Ames 
Salmonella Microsome Test. Proceedings of the National Academy of Sciences, 
76, 3779-3783. 
McCullagh, P., and Neider, J.A. (1989). Generalized Linear Models. Chapman & 
Hall, London. 
Moore, D.F. and Tsiatis, A. (1989). Robust Estimation of the Standard Error in 
Moment Methods for Extra-Binomial and Extra-Poisson Variation. Unpub-
lished manuscript. 

88 
Dey & Ravishanker 
Mukhopadhyay, S. and Gelfand, A.E. (1997). Dirichlet Process Mixed Generalized 
Linear Models. Journal of the American Statistical Association, 92, 633-639. 
Neider, J .A. and Lee, Y. (1992). Likelihood, quasi-likelihood and pseudolikelihood: 
Some comparisons. Journal of the Royal Statistical Society, Series B, 54, 273-
284. 
Neider, J.A. and Pregibon, D. (1987). An extended quasi-likelihood function. 
Biometrika, 7 4, 221-232. 
Pettit, L.I. and Young, K.D.S. (1990). Measuring the Effect of Observations on 
Bayes Factors. Biometrika, 77, 455-466. 
Raghunathan, T.E. (1994). Monte Carlo Methods for Exploring Sensitivity to Dis-
tributional Assumptions in a Bayesian Analysis of a Series of 2 x 2 Tables. 
Statistics in Medicine, 1525-1538. 
Shaked, M. (1980). On Mixtures from Exponential Families. Journal of the Royal 
Statistical Society, Ser. B, 42, 192-198. 
Smyth, G.K. (1989). Generalized Linear Models with Varying Dispersion. Journal 
of the Royal Statistical Society, Ser. B, 51, 47-60. 
Stiratelli, R., Laird, N.M. and Ware, J.H. (1984). Random Effects Models for Serial 
Observatiosn with Binary Response. Biometrics, 40, 961-971. 
Titterington, D.M., Makov, U.E. and Smith, A.F.M. (1985). Statistical Analysis 
of Finite Mixture Distributions. J. Wiley & sons, Chichester. 
Wang, Y. (1996). A Quasi-Likelihood Approach for Ordered Categorical Data with 
Overdispersion. Biometrics, 52, 1252-1258. 
Wedderburn, R. W.M. (1974). Quasilikelihood functions, generalized linear models 
and the Gauss-Newton method. Biometrika, 61, 439-47. 
Wedderburn, R. (1976). On the Existence and Uniqueness of the Maximum Likeli-
hood Estimates for Certain Generalized Linear Models. Biometrika, 63, 27-32. 
Williams, D.A. (1982). Extra-Binomial Variation in Logistic Linear Models. Ap-
plied Statistics, 31, 144-148. 
Williams, D.A. (1988). Extra-Binomial Variation in toxicology. In Proceedings of 
the Fourteenth International Biometrics Conference, N amur, Belgium: Bio-
metric Society, 301-313. 
Zeger, S.L. (1988). A Regression Model for Time Series of Counts. Biometrika, 75, 
621-629. 
Zeger, S.L. and Karim, M.R. (1991). Generalized Linear Models with Random 
Effects: A Gibbs Sampling Approach. Journal of the American Statistical As-
sociation, 86, 79-86. 

6 
Bayesian Generalized Linear 
Models for Inference About Small 
Areas 
Balgobin Nandram 
ABSTRACT Small area estimation is concerned with the estimation of parameters 
corresponding to small geographical areas or subpopulations when the underlying 
theme is to pool the data from other areas to estimate the parameters for a particular 
area. The purpose of this work is to present a review of the use of Bayesian generalized 
linear models in small area estimation. In particular, we review recent research that 
use hierarchical logistic and Poisson regression models in small area estimation or are 
potentially useful for small area estimation. Also, we discuss computational issues 
in small area estimation. We present an example of Poisson regression in which 
mortality rates are estimated for 798 U.S. health service areas (small areas) for all 
cancer for white males. We also discuss challenges that confront statisticians using 
small area methodology via generalized linear models. Finally, some remarks are 
made with respect to future research in this area. 
1. 
Introduction 
Small area estimation is concerned with the estimation of parameters correspond-
ing to small geographical areas or subpopulations when the underlying theme is to 
pool the data from other areas to estimate the parameters for a particular area. 
Interest in small area estimation has grown tremendously in recent years, more so 
after the elegant review paper of Ghosh and Rao (1994). More sophisticated models 
are being constructed to take care of many sources of variation, and these models 
can include both discrete data and continuous data. As can be envisioned there is a 
fairly large literature on continuous data models while the literature on discrete data 
models is very scanty. The literature on generalized linear models is relatively large, 
but the literature on Bayesian generalized linear models for small area estimation 
is limited. 
Sample survey designs are usually constructed to provide accuracy at a high 
level of aggregation. But interest is sometimes on very small areas which are not 
well represented by the survey data (i.e., the sample sizes of these small areas are 
inherently small). Most of these small areas tend to be areas for which the survey is 
not designed (or intended) to answer questions about. Thus, direct estimators such 
as design-based estimators (e.g., Cochran 1970) are not available or would have 
89 

90 
Nandram 
unacceptably large standard errors. Some countries (e.g., Canada) are beginning 
to design large-scale surveys to include small areas directly. But, as is expected, 
a complete enumeration of all small areas would be prohibitively expensive. Thus, 
statisticians must rely on methods of estimation not common in survey sampling. 
Hierarchical Bayes (HB) and empirical Bayes (EB) approaches have been exten-
sively used in recent years for small area estimation. These models are particularly 
suitable for a systematic connection of small areas. Therefore, the underlying theme 
in small area estimation is to "borrow strength from the ensemble" and this leads 
to improved precision. An attractive feature of this borrowing of strength is that 
shrinkage towards the grand mean is done adaptively because the estimates for areas 
with large sample sizes are shrunk less than those based on smaller sample sizes. 
General theories, methods and applications of the EB and HB approaches in 
small area estimation are presented by Fay and Herriot (1979), Ghosh and Meeden 
(1986), Ghosh and Lahiri (1987, 1989), Battese, Harter and Fuller (1988), Prasad 
and Rao (1990), Datta and Ghosh (1991), Ghosh and Lahiri (1992), Nandram and 
Sedransk (1993 a,b,c), Stroud (1987, 1991), Malec and Sedransk (1985), Nandram 
(1994), Arora et al. (1997), Nandram (1999), and others. Some of these papers are 
not directly related to small area estimation, the objective there being Bayesian 
predictive inference to estimate an overall finite population quantity such as the 
finite population mean (e.g., Nandram and Sedransk 1993a and Malec and Sedransk 
1985). However, the proposed methodology is useful for inference about small areas 
as well. 
For disease mapping, Marshall (1991) followed an EB approach to provide a local 
shrinkage estimator in which the crude rate is shrunk towards a local, neighborhood, 
rate. He applied this estimator for the analysis of infant mortality rates in Auckland, 
New Zealand for the period of 1977-1985. It is worth noting that in disease mapping 
data exist for all areas, and this is different from survey sampling where not all 
geographical areas are sampled. 
In passing we may note that small area estimation is not limited to survey sam-
pling. Essentially, whenever the underlying theme is a borrowing of strength, one 
is performing small area estimation. See, for example, Hulting and Harville (1991) 
for a more general notion than survey sampling. They pointed out that the small 
areas under consideration need not be geographical regions, and they presented an 
example in which the areas are batches of raw material in an industrial application. 
Another example is estimation and prediction for many time series with most of 
these series being very short. For example, Nandram and Petruccelli (1997) showed 
that there are gains in precision for estimation and forecasting when similar series 
are pooled. For an approach using hierarchical Bayesian multivariate time series 
method directly applicable to small area estimation, see Ghosh et al. (1996). 
Until recently much of the work in small area estimation was restricted to contin-
uous variates. Survey data are often categorical and methods used for continuous 
data are then inappropriate. There is a quick introductory review of the hierarchical 
Bayesian generalized linear models in Gelman et al. (1995, ch. 14) who discussed bi-
nomial, Poisson, multinomial models, and overdispersed models. See also Zeger and 
Karim (1991) who used Gibbs sampling to incorporate random effects in generalized 
linear models. 
Ghosh et al. (1998) provides a unified approach to the analysis of both continuous 
and categorical data through hierarchical Bayesian generalized linear models which 
include logistic regression and Poisson regression. However, their models do not 
include random regression coefficients. Recently, Malec et al. (1997) described a hi-
erarchical Bayesian logistic model including both area-specific and element-specific 

6. GLMs for for Inference about small areas 
91 
auxiliary variables and Christiansen and Morris (1997) and Waller et al. (1997) de-
scribed hierarchical models for Poisson regression including only area-specific aux-
iliary data. (It may be more challenging to include element-specific auxiliary data 
for Poisson regression because it is really each area's rate that is modeled in Poisson 
regression.) 
The rest of the paper is organized as follows. In Section 2 we review hierarchical 
logistic regression models with emphasis on the work of Malec et al. (1997). In 
Section 3 we review hierarchical Poisson regression models with emphasis on the 
work of Christiansen and Morris (1997). In Section 4 we describe computational 
issues, highlighting a theorem of Ghosh et al. (1998). In Section 5 an example on 
the estimation of mortality rates for U.S. health service areas is presented. In Section 
6 we describe challenges for Bayesian statisticians using generalized linear models 
in small area estimation. Section 7 has concluding remarks. Pertinent references are 
listed in Section 8. 
2. 
Logistic Regression Models 
Dempster and Tomberlin (1980) first proposed empirical and hierarchical Bayes 
methods using logistic regression to incorporate random effects model-based infer-
ence for small area binary data. The random effects model permits the data to 
determine a compromise between the classical unbiased estimates which depend on 
the data only in the specific local area, and the fixed effects estimates which pool 
information across areas. They applied this method to census undercount from a 
post enumeration survey. MacGibbon and Tomberlin (1989) showed how to use lo-
gistic regression models to estimate (empirical Bayes) proportions for small areas 
for a multi-stage survey. 
Malec et al. (1993, 1997) describe how to use a predictive approach to estimate 
finite population proportions for small areas using models motivated by Wong and 
Mason (1985), Dempster and Tomberlin (1980) and MacGibbon and Tomberlin 
(1989). Malec et al. (1997) made inferences for finite population proportions such 
as the probability of at least one visit to a doctor within the past 12 months. They 
used data from the National Health Interview Survey (NHIS) for the 50 states and 
the District of Columbia for many subpopulations within these 51 areas. Three 
major difficulties are the very large size of the NHIS sample, the use of models 
with many parameters, and the need to make predictions for many small areas. 
In fact, they made inferences for 72 age/race/sex categories for each U.S. county, 
approximately 216,000 subpopulations. 
Malec et al. (1997) assume that each individual in the population is assigned 
to one of I< mutually exclusive and exhaustive classes which are based on the 
individual's socioeconomic/ demographic status. Let Yikj denote a binary random 
variable for individual j in class k, cluster i where i = 1, ... , L, k = 1, ... , B, 
and j = 1, ... , Nik, and let Pik denote the probability that an individual has the 
characteristic in cluster i and class k. Note that these probabilities are not allowed to 
vary with the individuals although this could have been done if there are covariates 
specific to each individual. Within cluster i and class k, and conditional on Pik, the 
Yikj are assumed to be independent Bernoulli random variables with Pr(Yikj = 1 I 
Pik) = Pik. A column vector of M covariates, Xk = ( x k 1, .... , x kM) t, is assumed to 
be the same for each individual in class k and cluster i. Given Xk and a column 

92 
Nandram 
vector of regression coefficients, f3i = (f3il, ... , f3iM )', they assume that 
( 1) 
Thus, (1) is a linear regression with "dependent variable" logit(Pik) and independent 
variable Xk which does not depend on i. Then, they assume that, conditional on 'I 
and r, the f3i are independently distributed with 
(2) 
where each row of Gi is a subset of the cluster-level covariates (Zil, ... , Zic), not 
necessarily related to Xk, 1] is a vector of regression coefficients, and r is an M x M 
positive definite matrix. The regression in (2) is especially important, because it 
permits correlation between individuals in a cluster, and provides the opportunity 
for increased precision. 
Finally, reference prior distributions are assigned to 'I and r as well, such that 
p( ,, r) ex: constant. 
(3) 
There is no additional computational complexity if one replaces the reference prior 
in (3) with normal and inverse Wishart distributions for 'I and r. However, they 
have used reference prior distributions, because as they stated "published estimates 
are used by many secondary data analysts, and thus there is a need to minimize 
subjectivity." One caveat is that one might need to use proper (or proper diffuse) 
priors at least for r because it is not clear that the joint posterior distributions of 
the parameters are proper with reference prior distributions on , and r. 
In their application (1) is a piecewise linear model, linear in age; that is, 
f3i2Xl5,k + /3i3X25,k + /3i4X55,k + f3i5YkX15.k + f3i6YkX25,k + f3i7Zk 
where Yk and Zk are binary variables with Yk = 1 if class k corresponds to males, 
zk = 1 if class k corresponds to whites, and Xak = max(O, k-a) with age k denoting 
the midpoint of the ages of the individuals in class k [e.g., if class k corresponds 
to black females ages 40- 45, X15,k = max(O, 42.5- 15)]. They discuss the rela-
tionships between logit(Pik) and age (for the four race/sex combinations). Formula 
(2) is a second regression model with the vector f3i as the dependent variable, and 
E(f3it) = Ui111]ll + · · · + Uitc11Jtc1 , where {Uitl, ... , Uitc1 } is a subset of { Zil, ... , Zic}. 
The elements of Gi are county-level covariates, such as county per capita income, 
education level, and so on. 
The objective is to make inference about a finite population proportion, P, for a 
specified small area and subpopulation. In general, 
(4) 
where I is the collection of clusters that define the small area, I< is the collection of 
classes that define the subpopulation, and Nik is the total number of individuals in 
cluster i, class k. If P is the proportion of male Iowans who have made at least one 
visit to a doctor, then I is the 99 counties in Iowa and I< is the collection defined 

6. GLMs for for Inference about small areas 
93 
by the cross-classification of race, age (in 5-year groups), and males. Throughout, 
they assume that L L Nik is known, and for convenience they define 
iE/ kEK 
(5) 
For example, e is the total number of male Iowans who have made at least one visit 
to a doctor. 
Let Sik denote the set of sampled individuals in class k, cluster i that has size 
nik· Then 
iE/ kEK jEBik 
Let y8 denote the vector of sample observations. Because E(likj I Pik) = Pik, the 
posterior expected value of e is 
E(e I Ys) = L L L Yikj + L L L E(Pik I Ys) = 
iE/ kEK jEsik 
iE/ kEK jf/Bik 
(6) 
iE/ kEK j EBik 
iE/ kEK 
where 
Pik = exp{x~/1i}/{1 + exp(x~f1i)}. 
Formula ( 6) defines the hierarchical Bayes point estimator of e. The empirical 
Bayes estimator is the special case of (6) obtained by using only (1) and (2) and 
replacing (fJ, f) with a point estimate, (ij, f). The synthetic estimator is also a 
special case of (6) obtained by taking r = 0 in (2). 
A measure of variability is obtained by using the posterior variance of e. In fact, 
Var(e I Ys) = L L(Nik- nik)E{Pik(l- Pik) I Ys}+ 
iE/ kEK 
Var{L L (Nik- nik)Pik I Ys}. 
iE/ kEK 
(7) 
Malec et al. (1997) investigate the quality of inferences about P in two studies. 
First, they used a Bayesian cross-validation deleting an individual or a county at 
a time. The second is a study that compares their estimates for small geographical 
areas or subpopulations to the true values of the parameters. Both studies permit 
them to validate their models and methods. (For details see Sec. 5.3 of their paper.) 
Malec et al. (1997) compare hierarchical Bayes, empirical Bayes, synthetic and 
randomization based estimates. They found that the hierarchical Bayes estimates 
are very versatile. For some subpopulations, the hierarchical Bayes estimates are 
more variable than the synthetic estimates and less variable than the randomiza-
tion estimates, and for other subpopulations, the hierarchical Bayes estimates are 
similar to the synthetic estimates. However, for large subpopulations, as expected, 
the hierarchical Bayes estimates are close to the randomization-based estimates, and 
the synthetic estimates are farther from the randomization-based estimates than are 
the hierarchical Bayes estimates. 

94 
Nandram 
3. 
Poisson Regression Models 
Hierarchical Poisson models have been used for the analysis of different kinds 
of data. Much of the work on disease mapping starts with a Poisson sampling 
process. Clayton and Kaldor (1987) described empirical Bayes approaches that ac-
count for spatial similarities among neighboring rates. Bernardinelli and Montomoli 
(1992) compared empirical Bayes and hierarchical Bayes methods, the latter be-
ing implemented by Markov chain Monte Carlo (MCMC) methods. Breslow and 
Clayton (1993) used generalized linear mixed models to study the disease map-
ping problem, providing approximation schemes for inference; approximation of 
the marginal quasi-likelihood using Laplace's method leads eventually to estimat-
ing equations based on penalized quasi-likelihood for the mean parameters and 
pseudo-likelihood for the variances. Waller et al. (1997) presented spatia-temporal 
hierarchical Bayesian models to model regional disease rates over space and time 
including space-time interactions. 
Christiansen and Morris (1997) proposed a hierarchical Poisson regression model 
not in connection with survey sampling. But this method can be used to analyze 
mortality rates for small areas when a two-level model is used. The elegance is that, 
unlike current approaches to Bayesian analysis, it does not use MCMC methods, and 
simple but approximate closed form expressions are obtained for credible intervals 
of small area effects. 
Christiansen and Morris (1997) noted that their approach, Poisson regression in-
teractive multilevel modeling (PRIMM), has several advantages over earlier meth-
ods. Unlike some of the other methods, PRIMM provides interval estimates for all of 
the parameters, it has better nominal operating characteristics than most commonly 
used alternatives, and it is much faster than BUGS which is a software normally 
used for hierarchical Bayesian analysis. This makes the procedure versatile because 
it can be used for many statistical applications. 
Let Zi be the observed number of deaths for area i with exposure ni and mortality 
rate Ai, i = 1, ... , k. Then they assume 
(8) 
Ai I T, P i,ij Gamma(er, er-x' P) 
(9) 
where Xi= (xw,xil, .. ~.,xi,r-1)' and P = (f3o,f3I, ... ,f3r-d'· In (9), the random 
variable X"" Gamma(a,b) if f(x) = baxa-1exp(-bx)jf(a),x > 0. (A transforma-
tion made later in their paper leads to (9).) Essentially, this is a negative binomial 
regression model, used to address the issue of overdispersion in Poisson models (e.g., 
see Lawless 1987). 
Observe that (8) and (9) are conjugate leading to exact posterior inference about 
Ai conditional on P and T because, letting z be the vector of the Zi, 
Ai I P, T, z"" Gamma(zi + er, ni + er-x~P). 
For the hyperparameters P and r, they use the prior (after the transformation) 
zoer 
' 
rtl 
1r(P,r) ex ( 
)2 (r,P) cR 
zo + er 
k 
k 
where one can take zo = nomo with no = m.in ni and mo = L zd L ni. The prior 
~ 
i=l 
i=l 
on T is proper and is chosen to ensure that the maximum likelihood estimate of T 

6. GLMs for for Inference about small areas 
95 
is finite. This also ensures a proper posterior distribution for T. Nevertheless, the 
prior for P is improper. However, the joint posterior density for p and T can be 
shown to be proper provided that the number of cases ( c0 ) for which Zi > 0 is at 
least r and that the C0 X r submatrix of X = (x1, ... , Xk)' for these cases is full 
rank (see Christiansen and Morris 1997). 
We describe briefly how Christiansen and Morris (1997) obtained their main 
results (see their paper for details). There are three steps. 
First, after integrating out the Ai from (8) and (9), the likelihood function is 
L(P ) - Ilk rc eT + Zi) (1- B·)Zi B~r 
'T -
f(eT)z·! 
~ 
~ 
i=l 
~ 
(10) 
where, letting /Ji = ex~P, Bi = er j(er + niJJi) are the shrinkage factors. Thus, 
conditional on r, the likelihood function in Pis concave and there is a unique mode 
which can be obtained easily. Let Pr and fir denote the modal estimate and the 
negative Hessian matrix of the loglikelihood function C(p, r) = log(L(P, r)). 
Second, a restricted maximum likelihood (REML) type correction is made which 
takes into account the presence of the nuisance factor p in computing a modal 
estimate ofT. The approximate marginal density, 
(11) 
is maximized with respect to T. 
Third, they made a REML type adjustment to the log density, with respect to T 
in (11), of (P, r), denoted by IR(P, r) where 
k 
r 
r ""' 
IR(P, r) = C(P, r)+(1- 2)r-2log(exp(r)+zo)+ 2k L..)og(exp(r)+eim0 ). (12) 
i=l 
Then, the adjusted loglikelihood function in (12) is maximized to obtain the modal 
estimates jJ and f. 
Christiansen and Morris (1997) argued that asymptotically in k 
( ! ) I z ~ Nr+! { ( ~ ) , E} 
(13) 
with 
~ _ "2 [ iY;
2 ii;
1
- ui/ " ] 
u-UT 
VI 
1 
where v is obtained by differentiating C(P, r) with respect to T and solving the re-
sultingequationfor8/J/8rsetting8/J/8r = 
vanda-; = 
var(r I z) ~ -(821R(P,r)/8r2+
u' ii;1v)-1' all quantities in (13) being evaluated at (/1, f). 
Recalling /Ji = ex~p, it follows immediately that 
E(pi I z) =Pi = ex~P+x~Eux;/2 
and 
Var(pi I z) =a-~, = flr{exp(xiEuxi)- 1} 
where E 11 =a-; (a-;2 H'f1 - vu'). With further approximations on integrals based 
on the adjusted density method (ADM), they establish their main theorem which 
we describe next. 

96 
Nandram 
First, we need some notation. Let w[ = var( T- x~,B), 
EoBi = Ih = ef l(ef + niex',~) = aill(ail + ai2)· 
-2 
~ 
-2 
~ 
ail= wi 1(1- Bi), 
ai2 = wi I Bi, 
bi = cov(x~,B, T- x~P)Iwt, Pi, Ep[ =a-~.+ Pt, E1(Bi) =(ail+ bi)l(an + ai2), and 
for s = 0, 1,2, 
Then assume the distribution (13) for ,8 and T, given the data. For i = 1, ... , k, 
the ADM approximations to the first two moments of Ai, given the data, are 
and 
-2ziPi(E1Bf) + z[ EoB[- (ziiJi- PiE1Bi)2. 
It follows from the theorem that given the data, approximately 
2AiXi 
-~2- .v X2v 
0' )..; 
where f) = x; I a-t. Thus, credible intervals for Ai are obtained. 
(14) 
(15) 
Christiansen and Morris (1997) study the operating characteristics of the credi-
ble interval. They compare their method with six alternative methods. One of these 
uses a MCMC method through the BUGS software. The PRIMM method obtains 
noncoverage probabilities closest to the nominal value of .05. In addition, PRIMM 
has relatively fast computing time, and the computer program (S-PLUS environ-
ment) is publicly available to practitioners through Statlib. Your data set is simply 
read into a standard S-PLUS program which anyone can do quickly without any 
prior knowledge about S-PL US. 
PRIMM is potentially useful for small area statisticians interested in overdis-
persed Poisson models with covariates because answers can be obtained very quickly. 
The simplicity of the model permits quick data analysis within small regions (e.g., 
strata) which contain small areas. While PRIMM assumes a very simple model, 
one may be able to extend it to more complex small area models, a job not to be 
underestimated. 
4. 
Computational Issues 
It is inherently difficult to compute quantities of interest in nonlinear parametric 
problems, thus to simplify the computations approximations are usually used. This 
is true even within the framework of MCMC methods. 
It is worth noting that Albert (1998) discussed computational methods for a 
Bayesian hierarchical generalized linear model. A Bayesian two-stage prior distri-
bution is used, and the posterior distributions of the two hyper-parameters are in-
tractable. The focus of his article is on tractable accurate approximations to these 

6. GLMs for for Inference about small areas 
97 
posterior distributions. In particular, he discussed the Laplace method, a quasi 
likelihood method, and the Brooks method, and found that the Laplace method 
performs best for the binomial-logit hierarchical model. 
However, the current approach is to use a sampling based method. Usually the 
conditional posterior distributions do not exist in closed forms, making the Gibbs 
sampler (Gelfand and Smith 1990) difficult to use. Then one needs to use a version 
of the Metropolis-Hastings algorithm (Chib and Greenberg 1995 and Tierney 1994). 
However, one needs to note that if the nonstandard posterior conditional distribu-
tions are log concave, the Gibbs sampler can be used with the Gilks-Wild algorithm 
(Gilks and Wild 1992). 
With many parameters in the model it becomes necessary to accelerate the 
MCMC. For example, see Nandram and Chen (1996) for a method to accelerate 
the Gibbs sampler for the probit model when latent variables (Albert and Chib 
1993) are used. 
It is always important to demonstrate that the joint posterior distributions of the 
parameters are proper for any model. Ghosh et al. (1998) demonstrate how to do 
this for the generalized linear model applied to small area estimation. 
They started with m strata or local areas. Let Yik denote the minimal suffi-
cient statistic (discrete or continuous) corresponding to the kth unit within the 
ith stratum (k = 1, ... , ni; i = 1, ... , m). The Yik are assumed to be conditionally 
independent with pdf 
where k = 1, ... , ni, i = 1, ... , m. The density (16) is parameterized with respect 
to the canonical parameters (}ik and the scale parameters <Pik(> 0). It is assumed 
that the scale parameters <Pik are known. 
The natural parameters (}ik are first modeled as 
(17) 
where h is a strictly increasing function; the Xik (p x 1) are known design vectors, 
P(P x 1) is the unknown regression coefficient, the Ui are the random effects, and the 
fik are the errors. It is assumed that the Ui and the Eik are mutually independent 
with Ui i}.! N(O, o-~) and Eik i}.! N(O, o-2). 
It appears that (16) and (17) do not form a hierarchical Bayesian model. But it 
is now standard to represent such a model in a hierarchical framework as was done by 
Ghosh et al. (1998). Let Ru = o-;; 2 and R = o-- 2 • Also, let 9 = (en, ... , (}ln 1 , ••• , 
(}m1, ... , (}mnm )'and ·u = ( u1, ... , um)'. Then the hierarchical model is given by 
(I) conditional on 9, p, u, Ru = ru and R = r, the Yik are independent with 
densities given in (16); 
(II) conditional on p, u, Ru = ru and R = r, h((}ik) i}.! N(x~kp + Ui, r- 1 ); 
(III) conditional on p, Ru = ru and R = r, Ui i}.! N(O, r;;- 1 ). 
To complete the hierarchical model, Ghosh et al. (1998) assign the following 
prior top, Ru = ru and R = r: 
(IV) p, Ru = ru and R = rare mutually independent with P"' uniform (RP), (p < 
m), Ru "'Gamma(ta, tb) and R"' Gamma(tc, td). 

98 
Nandram 
In (IV) a random variable Z "' Gamma( a, {3) if Z has pdf 
f(z) = {3a exp( -f3z)za- 1 /f(a), z > 0. 
The model in (I)-(IV) is very similar the one considered by MacGibbon and 
Tomberlin (1989) and Breslow and Clayton (1993). As pointed out by Ghosh et al. 
(1998), this model is not strictly contained in the one considered by Zeger and Karim 
(1991). This is true because Zeger and Karim (1991) consider h(Oik) = xikP + Ui, 
where h( ·) is a strictly increasing function, but this formulation does not include 
possible error in misspecifying this model. In fact, the uncertainty in specifying the 
model in (I)-(IV) consists of two components: (i) the effect of the local area and 
(ii) the error component, permitting the possibility to account for overdispersion by 
introducing an extra variance component. 
Interest is on finding the joint posterior distribution of the g(Oik ), given the data 
Y = (Yu, ... , Y1n 1 , ••• , Ym1, ... , Ymnm )', where g is a strictly increasing function, 
and in particular in finding the posterior means, variances and covariances of these 
parameters. In typical applications, g(Oik) = 1/J'(Oik) = E(Y;,k I (}ik)· 
First, however, one needs to ensure that the joint posterior distribution of the 
eik given y is proper. The most important result in the paper is the theorem that 
establishes this result. Let the support of (}ik be the open interval ( e ik, eik), where 
the lower endpoint of the interval can be -oo, the upper endpoint can be +oo, or 
both. 
The theorem follows: assume a > 0, c > 0, L:i ni - p + d > 0, and m + b > 0. 
Then, if 
1::• exp{[Oy,.- ~(8)]N;k}h'(O)d0 < oo 
(18) 
for all Yik and <Pik (> 0), the joint posterior pdf of the (}ik given y is proper. 
The theorem covers the two important special cases of logistic regression and 
Poisson regression. For the logistic case, 
Y;,k I (}ik "' Binomial( nik, exp( (}ik) / { 1 + exp( (}ik)}) 
and h is the identity function (i.e., the link is canonical). Also, let g( (}ik) = 1/J' ( (}ik )/ nik = 
exp(Oik)/(1 + exp(Oik)]. Then writing Pik = exp(Oik)/(1 + exp(Oik)], (18) reduces to 
J0
1 pft-1(1- Pik)n-y,~e-
1 dpik < oo which requires 1 ~ Yik ~ (nik -1), i.e., excludes 
cases of all failures or all successes. For the Poisson case, 
Y;,k I (}ik "' Poisson( exp( (}ik)). 
Then, if his the canonical link, and g(Oik) = 1/J'(Oik) = exp(Oik), the condition (18) 
reduces to J0
00 
(f~~e-
1 exp( -(ik)d(ik < oo which holds for Yik = 1, 2, .... 
As is apparent, direct evaluation of the joint posterior distribution of the g( (}ik) 
given y involves high-dimensional numerical integration, and is not computationally 
feasible except, of course, by using MCMC methods (e.g., Gibbs sampler, Gelfand 
and Smith 1990). The implementation of the Gibbs sampler requires generating 
samples from the conditional posterior distributions. Let 
h(9) = (h(Ou), ... , h(01n 1 ), ••• , h(Oml), ... , h(Omnm))', 
X== (xu, ... , Xln 1 , ••• , Xml, ... , Xmnm)' 
and X'X be nonsingular. Then the necessary conditional posterior distributions 
based on the hierarchical Bayesian model given in (1)-(IV) are: 

6. GLMs for for Inference about small areas 
99 
(i) pI e,u,ru,r,y.-v N((X'x)- 1(X'h(0)- L:iuiL:kXik),r- 1(X'x)- 1); 
(ii) Ui I e, p, ru, r, y ij! N((rni + ru)- 1 L:k(h((}ik)- x~kp), (rni + ru)- 1 ); 
(iii) R I e,p, u, ru, y I"V Gamma(!(c + L:i L:k(h(Oik)- x~kp- Ui) 2) '!(d+ L:;.n ni)); 
(iv) Ru I e,p, u,r,y I"V Gamma(!(a + L:i ul}, t(b+ L:;.n ni)); 
iid 
(v) (}ik I p, u, ru, r, y '""' 1r(Oik I p, u, ru, r, y) oc 
exp [(Yik(}ik -1/J(Oik))<Pii/- ~(h(Oik)- x~kp- Ui) 2] h'((}ik)· 
Samples can be generated easily from the normal and gamma distributions given 
in (i)-(iv). However, the conditional posterior distribution in (v), (}ik given p, u, ru, r 
and y, is known only up to a multiplicative constant, making it difficult to draw 
samples from this conditional posterior distribution. In the special case where h(z) = 
z for all z, Ghosh et al. (1998) noted that it is straightforward to show that log7r(Oik I 
p, u, r, ru, y) is a concave function of eik· In such cases, one can use the adaptive 
rejection sampling (ARS) scheme of Gilks and Wild (1992). 
It is worth noting here, however, that although the conditional posterior distri-
bution is theoretically log concave, it is possible for the ARS to fail (see Gilks and 
Wild 1992). Instead, one can use a Metropolis step to obtain a Metropolis-Hastings 
algorithm (Chib and Greenberg 1995). The main issue then is how to construct effi-
cient proposal densities; see Nandram et al. (1998) who obtained proposal densities 
for many generalized linear models and Nandram (1998) who obtained proposal 
densities for the three-stage hierarchical multinomial-Dirichlet model. 
Inference about 9, based on (i)-(v), can now be obtained in a straightforward 
manner by performing an output analysis from the Gibbs sampler. That is, E(Oik I 
y), V ( (}ik I y) and Cov( (}ik, (}i' k' I y) ( i, k) :f. ( i', k') can be easily obtained from for-
mulas for iterated conditional expectations and variances. (These are Rao-Blackwellized 
estimates as described by Gelfand and Smith 1990.) 
Ghosh et al. (1998) extend the model in (I)-{IV) in two important directions. The 
first extension covers the analysis of multi-category data. Again they considered m 
strata, and within each stratum, they assumed that several units are selected and 
the responses of individuals within each selected unit are independent, and can be 
classified into J categories. The second extension considers a spatial hierarchical 
Bayesian generalized linear model. In this case the Ui in ( 17) represent variables 
which display spatial structure. In particular, they model the Ui so that a pair of 
contiguous zones would have stronger positive correlation than non-contiguous ones. 
They use the pairwise difference prior (e.g., Besag et al. 1995) on the Ui· Finally, 
they proved a theorem about the propriety of the (}ik in this case. 
Also Ghosh and Natarajan (1998) have generalized the theorem by relaxing the 
conditions. Consider k = 1 and drop this subscript. They assume that f(Yi I (}i) is 
bounded for all i and h'(O) = 1. Let h denote (18), S = {i: h < oo}, and s be 
the cardinality of S. Then s + b > p and a > 0 are the conditions needed for the 
propriety of the posterior distribution. 
Models with random regression coefficients are not covered ·by the theory of Ghosh 
et al. (1998), and the computations are more difficult. Other techniques must be 
considered. Gelfand et al. (1995) describe centering, an important technique which 
provides some useful tricks for computations. In addition, many of the conditional 
posterior distributions do not exist in closed forms, and they may not be log con-
cave. Then, the Metropolis-Hastings algorithm is the obvious choice to perform 
computations, creating a possible need to construct a reasonably accurate proposal 

100 
Nandram 
density for the conditional posterior distributions. For good examples of how these 
approximations can be obtained see the details in Christiansen and Morris (1997), 
N andram et al. (1998). 
5. 
Models for the U.S. Mortality Data 
Pickle et al. (1996, 1997) discussed mixed effects models for estimating mortality 
rates for the eighteen leading causes of death in the U.S. Nandram et al. (1999) 
discussed alternative Bayesian models and methods for producing age specific and 
age adjusted mortality rates for "all cancer" for white males using data from 1988-
92. 
Nandram et al. (1999) use the same geographical units, health service areas 
(HSAs), as used in the Atlas (Pickle et al. 1996). The U.S. is divided up into 805 
HSAs, and excluding those in Alaska and Hawaii, there are 798 HSAs. Each HSA is 
a group of counties, and the numbers of HSAs per state varies quite a bit ranging 
from 1 to 58 with median 16. There are 1 to 20 counties within a HSA with a median 
of about 2. With the exception of NYC, each HSA is at least 250 square miles in 
size. The states are too heterogeneous and counties are too small for any meaning-
ful analysis. For the statistical analysis, there are twelve 'regions'; three of the nine 
Census Divisions were split to achieve greater homogeneity (see discussion on pg. 5 
and Appendix 1 of Pickle et al. 1996). Numbers of deaths by age, race, sex, place 
of residence, and cause of death are based on original death certificates reported to 
the National Center for Health Statistics (NCHS) from which the mortality data 
are obtained. 
Let dij and ni; denote, respectively, the number of deaths and exposure for age 
class j in HSA i (i = 1, ... ,798;j = 1, ... ,10). The age classes are 0-4,5-14,15-
24, ... ,75-84, 85 and up, coded as .25, 1, ... , 9. Assume for fixed Aij that 
Inference is desired for the age specific mortality rate, Aij, and the age adjusted 
10 
rate Ri = La; Aij where the a; are the proportions of people in a standard million 
j=l 
U.S. population. (It is surprising that for the Atlas the standard million is based on 
the 1940 U.S. population.) 
The basis for the analysis in the Atlas is the first order Taylor series approximation 
of log Tij where Tij = dij / nij, the observed age specific mortality rate; i.e., 
(19) 
and 
(20) 
where 
xj = ( 1, decade j, (decade j)2 , (decade j)3 , max { 0, (decade j 
knot )3 }) 
with decade 1 = .25, decade j = j 
1 for j = 2, ... , 10 and the value of the knot is 
6. 

6. GLMs for for Inference about small areas 
101 
Let ri; = Tij if Tij > 0 and 10-6 if Tij = 0, and define Yij = log( ri; ). Denote 
by T[k]j the observed mortality rate for region k, k = 1, ... , 12, and let Wij = dij if 
dij ~ 3 and nijT[kJj if dij < 3. 
N andram et al. (1999) fitted several models. One of them is a Bayesian version 
of the mixed effects model used for constructing the Atlas. They have investigated 
three alternative versions of (19) and (20). First, log(rij) is replaced by Yij and the 
variance term, (nijAij)- 1 is replaced by ¢[k]/Wij· Observing that dij is a sample 
based estimator of nij Aij, Pickle et al. (1997) showed that replacing dij with Wij 
provides better estimates of the Aij. The parameter ¢[k] is added to try to cap-
ture (regional) dispersion that is different from the Poisson distribution assumed in 
deriving (19) (see Efron 1986). Then, the model closest to the one in the Atlas is 
Yij ~ N(x}/i,¢[k]/Wij), 
(bu) 
'Yi = p + .... 
bi2 
(21) 
with independence over i and j where bil and bi 2 are independent with bil ij! 
N(O, D,), D, = diag(61). There are locally uniform prior distributions for P and the 
{ ¢[kJ }, and proper, diffuse (i.e., proper with large variance) prior distributions for 
the components of D,. The model used for the Atlas is, essentially, the model in 
(21) with a separate analysis in each of the 12 regions. 
Nandram et al. (1999) constructed several other models without the approxima-
tion on the Poisson sampling process. Each model uses the same sampling distribu-
tion 
dij lnij, Aij i:d Poisson ( nij Aij). 
(22) 
We describe three of these models. The first model is 
(23) 
I 2 iid 
( 
2) . 
lli u 
~ N 0, u , z = 2, ... , 798. 
There is a locally uniform prior distribution on P, and a proper diffuse prior on u 2 . 
The second model is 
log Aij = xj Pi 
(24) 
,BdO, d ijj N(O, d) 
where the prior distribution on e is locally uniform, and the prior on d is proper 
but diffuse. Note that dis not diagonal. The third model is the one just described 
with (24) replaced by 
(25) 
where 
with a proper, diffuse prior on u 2 • 
Nandram et al. (1999) analyzed the models in (21), (23), (24) and (25) fit sep-
arately within regions, and to the entire population of HSAs. The model in (21) 
was fitted using the simple Gibbs sampler and the models in (23), (24), and (25) 
were fitted using the Metropolis-Hastings sampler. For computational details such 
as construction of proposal densities and centering see Nandram et al. (1998). 

102 
Nandram 
Nandram et al. (1999) used three different measures to assess the fit of the models. 
The first is the posterior expected predicted deviance (EPD), 
(26) 
where dnew is a random vector with distribution 
This is a measure of discrepancy between dobs, the observed vector of the dij, and 
dnew, a set of "new" observations selected from the posterior predictive distribution 
of dnew in (22). If the model and data are concordant (26) should be small. One 
choice of P( ·, ·), based on the Chi-squared statistic, is 
P(dobs, dnew) = L L(df:S _ dijew)2 /(dijew + 0.5). 
i 
j 
Nan dram et al. (1999) give two other choices; see also Waller et al. (1997). 
The second measure that they used to assess the fit of the models is the posterior 
predicted p-value; i.e., 
One choice of checking function T(dnew, .:\),analogous to the Chi-squared discrep-
ancy measure, is 
L ( dij 
nij Aij )2 / nij Aij. 
i,j 
Again they have considered two more checking functions. While the EPD is used 
to rank the models, the p-value is primarily used for goodness of fit. 
The third measure of evaluating the alternative models is to use standardized 
residuals. Let d(ij) denote the set of all d's except the ( ij)th component itself. Then 
define the standardized residual as 
That is, the ( ij)th observed rij is "held out" and compared with its point estimator, 
E(r(ij)ld(ij)), which is evaluated without using the observed dij. 
Of the four models that they considered (i.e., those given by (21 ), (23), (24) 
and (25), fit to all 798 HSAs), the values of the posterior predicted p-value were 
acceptable only for the model (25). Using the expected predicted deviances as the 
criteria, model (25) was best, followed by (24), (23) and (21). They also fit model 
(25) separately in each region. The EPD values for this latter case were almost the 
same as those when (25) was fit to all 798 HSAs; however, the p-values provide 
greater support for (25) fit to all 798 HSAs. 
6. 
Challenges in Small Area Estimation 
First, the problem of model diagnostic is an important one. The most promising 
diagnostic procedures are based on cross-validation (Gelfand et al. 1992 and Gelfand 

6. GLMs for for Inference about small areas 
103 
and Dey 1994) and expected predictive deviances (Ibrahim and Laud 1995). One 
would need to assess distribution of residuals and tail area probabilities to find 
threshold values not based on normality. The problem is exaggerated in small area 
estimation because of the obvious difficulty with small sample sizes and the com-
plexity of the likelihood function for generalized linear models. 
Second, overshrinkage is a serious issue in small area estimation. Louis (1984) and 
Ghosh (1992) used constrained Bayes estimates to avoid overshrinkage. Shen and 
Louis (1998) describe triple-goal estimates in two-stage hierarchical models in which 
the estimation method is linked to an inferential goal via a loss function. Triple-goal 
estimates are necessary because it may be desirable to have a set of estimates that 
produce good ranks, a good parameter histogram and good co-ordinate-specific es-
timates. Motivated by the comments of Thomsen in Ghosh and Rao (1994), Rashid 
and Nandram (1998) used rank-based methods in an attempt to fix the overshrink-
age problem for an error components model (see Battese, Harter and Fuller 1988), 
but it is not clear how to apply it to a Bayesian generalized linear model. 
One obvious way to overcome some overshrinkage is to use several parameters 
in the modeling. Fitting several regressions may be better than fitting a single re-
gression when the coefficients are permitted to share effects (i.e., random regression 
model). This is a difficult problem (both theoretically and computationally) not 
covered by the general theory of Ghosh et al. (1998). But such models are desirable 
because they are potentially useful to reduce the effects of over shrinkage. See the 
comments of Holt in Ghosh and Rao (1994). 
Another method that can help with overshrinkage is the concept of "uncertain 
borrowing" suggested by Malec and Sedransk (1992) for pooling the results from 
several experiments. Instead of assuming all the small area effects are exchangeable, 
it is assumed that subsets of small areas are exchangeable and the composition of 
the partition sets is uncertain. Consoni and Veronese (1995) applied the ideas of 
Malec and Sedransk (1992) to a set of binomial experiments. Evans and Sedransk 
(1998) applied this methodology for pooling subpopulation regressions. However, 
when there are many small areas, this method is difficult to perform because there 
are too many partitions, but one might tol~rate larger partition sets. It is possible 
to perform this method for the simple Poisson model of Christiansen and Morris 
(1997) but it may not be so easy for a disease mapping application. 
This uncertain borrowing method is really to include information of clustering 
about the area effects in the model. It is necessary to incorporate other sources 
of information especially when data are sparse. For example, the area effects may 
be restricted in some set or there may be an order restriction on the effects. One 
example of this order restricted inference is on an application to the age composition 
of a fish population (see Nandram et al. 1997). 
It is possible to provide models that fit better by introducing a scale parameter. 
This is particularly important for binomial or Poisson models in which the mean and 
variance are functionally related making such distribution assumptions doubtful. 
One would like to expand these models to include under-dispersion and especially 
over-dispersion. There is a method that can be used for doing so; see West (1985) 
and Efron (1986). Essentially these authors used Hoeffding's representation of the 
exponential family through the deviance function. One of the main problems is that 
the normalization constant is not unity which increases the computation problem. 
This method can help indirectly to correct for overshrinkage. 
A problem that everyone seems to overlook in small area estimation is that the 
sample sizes from the small areas are random because the survey is usually not 
designed to collect data from the small areas. Not incorporating this randomness can 

104 
Nandram 
potentially lead to optimistic precision especially if the sample sizes are informative 
about the parameters of interest. This randomness should be incorporated into the 
model. Thus, there are further difficulties when the generalized linear model is used 
in small area estimations. In addition, if these small areas are finite populations, 
then the finite population sizes of these areas may also be unknown. This creates 
a difficult problem for Bayesian predictive inference. One way to overcome this 
difficulty is to construct a survey design which incorporates the small areas, but with 
many small areas this will be prohibitively expensive. Otherwise, approximations 
are inevitable. 
The multivariate nature of small area effects is also an important consideration. 
Inference about small area effects is sensitive to the prior specification of these small 
area effects. This is particularly true when a subgroup analysis (whose purpose is to 
identify the areas that have values above or below some cutoff point) or a ranking of 
the small area effects is done. Correlation among the small area effects can markedly 
change the subgroups and the rankings. Thus, in a Bayesian analysis, effort needs 
to be placed on the construction of robust priors for the small area effects. This is 
expected to be more difficult within the generalized linear model framework. 
Finally, we must agree that it is desirable to obtain simple approximate closed 
form expressions for distributions of parameters of interest. The MCMC method can 
be used to check the accuracy of these approximations. One example is Christiansen 
and Morris (1997) on Poisson regression. While the MCMC method provides interval 
estimators in an output analysis, approximate closed form intervals are desirable. 
The difficulty here is that, unless common features can be found, an extensive 
amount of work has to be done for each model. 
7. 
Concluding Remarks 
We have reviewed how generalized linear models are currently used in small area 
estimation. Among many problems we summarize three that exist in small area 
estimation. 
First, the problem of overshrinkage will always exist. Methods now available to 
cope with overshrinkage can work for simple models. Other ideas are needed for 
more complex models. A more applied Bayesian approach rather than a decision-
oriented approach can be useful. 
Second, computation using MCMC methods is perhaps the only way to proceed 
to fit and study the fit of these models. Theorems like those proved by Ghosh et al. 
(1998) are needed for models that do not fall in their framework. This adds credence 
to the use of the Bayesian methodology. 
Third, in the area of diagnostics there should be a search for other checking 
functions and deviance measures. For example, checking functions depending on 
sufficient statistics are no good (see Gelman et al. 1995). Checking functions based 
OJ;l ranks seem to be a reasonable alternative. Also residual plots for generalized 
linear models ought to be calibrated by, for example, using training samples. 
It is sensible to fit many models to each small area data set. We need to study all 
these models carefully to understand their strengths and weaknesses. An alternative 
approach is Bayesian model averaging (BMA); see Hoeting et al. (1998) for an 
elegant tutorial on BMA. It is computationally more difficult to implement BMA; 
the present approach is by using the reversible jump Markov chain (Green 1995). 
Faster computers will be helpful for these problems characterized by complicated 

6. GLMs for for Inference about small areas 
105 
generalized linear models, large data sets and the desire to produce estimates for 
small areas and subpopulations. Also, it is very important to understand the mech-
anism that generates the data before any sensible model can be obtained. The data 
analyst must work with the scientists, and the statisticians working in small area 
estimation with generalized linear models must ride on technology. 
Acknowledgment 
The author thanks Professors Joseph Sedransk and Malay Ghosh for their assis-
tance. The research was supported by a research contract with the National Center 
for Health Statistics, U.S. Department of Health and Human Services. 
References 
Albert, J. H. (1988). Computational methods using a Bayesian hierarchical gen-
eralized linear model. Journal of the American Statistical Association, 83, 
1037-1044. 
Albert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous 
response data. Journal of the American Statistical Association, 88, 669-679. 
Arora, V., Lahiri, P. and Mukherjee, K. (1997). Empirical Bayes estimation of finite 
population means from complex surveys. Journal of the American Statistical 
Association, 92, 1555-1562. 
Battese, G. E., Harter, R. M. and Fuller, W.A. (1988). An error components model 
for prediction of county crop areas using survey and satellite data. Journal of 
the American Statistical Association, 83, 28-36. 
Bernardinelli, L. and Montomoli, C. (1992). Empirical Bayes versus fully Bayesian 
analysis of geographical variation in disease risk. Statistics in Medicine, 11, 
983-1007. 
Besag, J ., Green, P., Higdon, D. and Mengersen, K. (1995) Bayesian computation 
and stochastic systems (with discussion). Statistical Science, 10, 3-66. 
Chib, S. and Greenberg, E. (1995). Understanding the Metropolis-Hastings algo-
rithm. The American Statistician, 49, 327-335. 
Christiansen, C. L. and Morris, C. N. (1997). Hierarchical Poisson regression mod-
eling. Journal of the American Statistical Association, 92, 618-632. 
Clayton, D. and Kaldor, J. (1987). Empirical Bayes estimates of age-standardized 
relative risks for use in disease mapping. Biometrics, 43, 671-681. 
Cochran, W.G. (1977). Sampling Techniques. 3rd edition, New York: Wiley. 
Consoni, G. and Veronese, P. (1995). A Bayesian method for combining results from 
several binomial experiments. Journal of the American Statistical Association, 
90, 935-944. 

106 
Nandram 
Datta, G. and Ghosh, M. (1991). Bayesian prediction in linear models: Applications 
to small area estimation. Annals of Statistics, 19, 1748-1770. 
Dempster, A.P., and Tomberlin, T. J. (1980). The analysis of census undercount 
from a postenumeration Survey. In: Proceedings of the Conference on Census 
Undercount, Arlington, VA, pp. 88-94. 
Efron, B. (1986). Double exponential families and their use in generalized linear 
regression. Journal of the American Statistical Association, 81, 709-721. 
Evans, R. and Sedransk, J. (1998). Methodology for pooling subpopulation regres-
sions when the sample sizes are small and there is uncertainty about which 
subpopulations are similar. Technical Report, Department of Statistics, Case 
Western Reserve University. 
Gelfand, A.E. and Dey, D.K. (1994). Bayesian model choice: Asymptotics and 
exact calculations. Journal of the Royal Statistical Society, B, 56, 501-514. 
Gelfand, A.E., Dey, D.K. and Chang, H. (1992). Model determination using pre-
dictive distributions with implementation via sampling-based methods. In: 
Bayesian Statistics, 9, ( J. Bernardo, et a/., eds.), Oxford University Press, 
Oxford, 147-158. 
Gelfand, A. E., Sahu, S. K., and Carlin, B. P. (1995). Efficient reparametrizations 
for normal linear models. Biometrika, 82, 479-488. 
Gelman, A., Carlin, J ., Stern, H., and Rubin, D. (1995). Bayesian Data Analysis. 
London: Chapman and Hall. 
Ghosh, M. (1992), Constrained Bayes estimation with applications. Journal of the 
American Statistical Association, 87. 533-540. 
Ghosh, M. and Lahiri, P. (1987). Robust empirical Bayes estimation of means 
from stratified samples. Journal of the American Statistical Association, 82, 
1153-1162. 
Ghosh, M. and Meeden, G. (1986). Empirical Bayes estimation in finite population 
sampling. Journal of the American Statistical Association, 81, 739-757. 
Ghosh, M. and Natarajan, K. (1998). Small area estimation: a Bayesian perspec-
tive. In: Multivariate Analysis, Design of Experiments and Sample Surveys, 
Ed. Subir Ghosh, New York: Marcel Dekker, to appear. 
Ghosh, M., Nangia, N., and Kim, D. H. (1996). Estimation of median income of 
four-person families: A Bayesian time series approach. Journal of the Ameri-
can Statistical Association, 91, 1423-1431. 
Ghosh, M., Natarajan, K., Stroud, T. W. F., and Carlin, B. P. (1998). Generalized 
linear models for small area estimation. Journal of the American Statistical 
Association, 93, 273-282. 
Ghosh, M. and Rao, J. N. K. (1994). Small area estimation: An appraisal. Statistical 
Sciences, 9, 55-93. 
Gilks, W. R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Journal of the Royal Statistical Society, Ser. C, 41, 337-348. 

6. GLMs for for Inference about small areas 
107 
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and 
Bayesian model determination. Biometrika, 82, 711-732. 
Hoeting, A. J., Madigan, D., Raftery, A. E., and Volinsky, C. (1998). Bayesian 
model averaging," Technical Report, Department of Statistics, Colorado State 
University. 
Hulting, F. L. and Harville, D. A. (1991) Some Bayesian and non-Bayesian proce-
dures for the analysis of comparative experiments and for small-area estima-
tion: computational aspects, frequentists properties and relationships. Journal 
of the American Statistical Association, 86, 557-568. 
Laud, P. W. and Ibrahim, J .G. (1995). Predictive model selection. Journal of the 
Royal Statistical Society, B 57, 247-262. 
Lawless J. F. (1987). Negative binomial and mixed Poisson regression. Canadian 
Journal of Statistics, 15, 209-225. 
MacGibbon, B. and Tomberlin, T. J. (1989). Small area estimates of proportions 
via empirical Bayes techniques. Survey Methodology, 15, 237-252. 
Malec, D. and Sedransk, J. (1992). Bayesian methodology for combining the results 
from different experiments when the specifications for pooling are uncertain. 
Biometrika, 79, 593-601. 
Malec, D. and Sedransk, J. ( 1985). Bayesian methodology for predictive inference 
for finite population parameters in multistage cluster sampling. Journal of the 
American Statistical Association, 80, 897-902. 
Malec, D., Sedransk, J., Moriarity, C. L., and LeClere, F. B. (1997). Small area 
inference for binary variables in the National Health Interview Survey. Journal 
of the American Statistical Association, 92, 815-826. 
Malec, D., Sedransk, J., and Tompkins, L. (1993).Bayesian predictive inference for 
small areas for binary variables in the National Health Interview Survey. In: 
Case Studies in Bayesian Statistics, eds. C. Gatsonis, J .S. Hodges, R.E. Kass, 
and N.D. Singpurwalla, New York: Springer-Verlag, pp. 377-389. 
Marshall, R. J. (1991). Mapping disease and mortality rates using empirical Bayes 
estimators. Journal of the Royal Statistical Society, C, 40, 283-294. 
Nandram, B. (1994). Bayesian predictive inference for multivariate sample surveys. 
Journal of Official Statistics, 167-179. 
Nandram, B. (1998). A Bayesian analysis of the three-stage hierarchical multino-
mial model. Journal of Statistical Computation and Simulation, 61, 97-126. 
Nandram, B. (1999). Empirical Bayes interval estimation for the finite population 
mean of a small area. Statistica Sinica, to appear. 
Nandram, B. and Chen, M-H. (1996). Reparameterizing the generalized linear 
model to accelerate Gibbs sampler convergence. Journal of Statistical Com-
putation and Simulation, 54, 129-144. 
Nandram, B. and Petruccelli, J. D. (1997). A Bayesian analysis of autoregressive 
time series panel data. Journal of Business and Economic Statistics, 15, 328-
334. 

108 
Nandram 
Nandram, B. and Sedransk, J. (1993a). Bayesian predictive inference for longitu-
dinal sample surveys. Biometrics, 49, 1045-1055. 
Nandram, B. and Sedransk, J. (1993b). Empirical Bayes estimation for the finite 
population mean on the current occasion. Journal of the American Statistical 
Association, 88, 994 -1000. 
Nandram, B. and Sedransk J. (1993c). Bayesian predictive inference for a finite 
population proportion: Two-stage cluster sampling. Journal of the Royal Stat-
istical Society, Ser. B, 55, 399-408. 
Nandram, B., Sedransk, J. and Pickle, L.W. (1998). Bayesian analysis of mortality 
rates for U.S. Health Service Areas. Technical Report, Mathematical Sciences, 
Worcester Polytechnic Institute. 
Nandram, B., Sedransk, J. and Pickle, L.W. (1999). Bayesian analysis of mortality 
rates for U.S. Health Service Areas. Sankhya, Ser. B, to appear. 
Nandram, B., Sedransk, J. and Smith, S. J. (1997). Order restricted Bayesian 
estimation of the age composition of a population of Atlantic cod. Journal of 
the American Statistical Association, 92, 33-40. 
Pickle, L. W., Mungiole, M., Jones, G. K., and White, A. A. (1996). Atlas of United 
States Mortality. Hyattsville, Maryland: National Center for Health Statistics. 
Pickle, L. W., Mungiole, M., Jones, G. K., and White, A. A. (1997). Analysis of 
mapped mortality data by mixed effects models. Technical Report, National 
Center for Health Statistics. 
Prasad, N. G. N. and Rao, J. N. K. (1990). On the estimation of mean squared 
error of small area predictors. Journal of the American Statistical Association, 
85, 163 -171. 
Rashid, M. M. and Nandram, B. (1998). A rank-based predictor for the finite 
population mean of a small area: An application to crop production. Journal 
of Agricultural, Biological, and Environmental Statistics, 3, 201-222 (1998). 
Shen, W. and Louis, T.A. (1998). Triple-goal estimates in two-stage hierarchical 
models. Journal of the Royal Statistical Society, B, 60, 455-472. 
Stroud, T. W. F. (1987). Bayes and empirical Bayes approaches to small area 
estimation. In: Small A rea Statistics, eds. R. Platek, J .N .K. Rao, C.E. Sarndal 
and M.P. Singh, New York: Wiley, pp. 124-137. 
Tierney, L. (1994). Markov chains for exploring posterior distributions (with dis-
cussion). Annals of Statistics, 22, 1701-1762. 
Waller, L., Carlin, B., Xia, H., and Gelfand, A. (1997). Hierarchical spatio-temporal 
mapping of disease rates. Journal of the American Statistical Association, 92, 
607-617. 
West, M. (1985), "Generalized linear models: scale parameters, outlier accommo-
dation and prior distributions. In Bayesian Statistics 2, eds., J. M. Bernardo, 
M.H. Degroot, D.V. Lindley and A. F. M. Smith, North-Holland: Elsevier 
Science Publishers, pp. 531-558. 

6. GLMs for for Inference about small areas 
109 
Wong, G. Y. and Mason, W. M. (1985). The hierarchical logistic regression model 
for multilevel analysis. Journal of the American Statistical Association, 80, 
513-524. 
Zeger, S. L. and Karim, M. R. (1991). Generalized linear models with random 
effects: A Gibbs sampling approach. Journal of the American Statistical As-
sociation, 86, 79-86. 

This Page Intentionally Left Blank 

Part III 
Categorical and Longitudinal 
Data 

This Page Intentionally Left Blank 

7 
Bayesian Methods for Correlated 
Binary Data 
Siddhartha Chib 
ABSTRACT This paper reviews some of the recent developments in the fitting and 
comparison of Bayesian models for correlated binary data. The importance of the 
multivariate probit model is highlighted and Markov chain simulation algorithms for 
the fitting of the multivariate probit, probit normal and hierarchical probit models 
{the last two for longitudinal data) are discussed. The fitting of each model is illus-
trated with the Six Cities Data on the health effects of pollution. The computations 
are conducted using the new Windows™ software package BAYESTAT that has 
been developed by the author for the fitting of these and many other Bayesian mod-
els. The paper also shows how alternative Bayesian models for correlated binary data 
can be compared. Marginal likelihoods from Chib's {1995) method are computed for 
each of five models that reflect different assumptions about the correlation structure 
and the extent of heterogeneity in the sample. Details of the fitting algorithms are 
reported in the Appendix. 
1. 
Introduction 
Statistical methods for correlated, continuous response data have been studied 
for a long time and many sophisticated models and methods are now available. In 
the last ten years or so, increasing attention has been devoted to questions for which 
the standard methods are no longer applicable. For example, in a biostatistical con-
text, one might be interested in the joint probability distribution of several binary 
variables, each representing a particular disease outcome [Betensky and Whitte-
more (1996)]. In an economics problem, one might be interested in the outcomes 
from several related binary financial decisions made by a subject. Correlated binary 
data can also arise from the analysis of a single binary variable. In a longitudinal 
problem, for example, one might observe a time series of binary responses (e.g., an 
indicator of whether the subject visited a physician in a given year) that are likely 
to be correlated. 
The analysis of correlated binary data is an exciting and important area of statis-
tics in which both frequentist and modern Bayesian methods have had much to 
offer. For reasons of tractability, some of the first developments occurred in the 
analysis of longitudinal binary data where subject-specific random effects provided 
a simple means to model intra-cluster correlation [Stiratelli, Laird and Ware (1984)]. 
Three recent surveys covering this material are those of Agresti (1989), Fitzmaurice, 
113 

114 
Chib 
Laird and Rotnitzky (1993) and Pendergast, Gange, Newton, Lindstrom, Palta and 
Fisher (1996). More recently, significant methods for the analysis of general mul-
tivariate correlated data have begun to appear. Leading papers in this field are 
those by Carey, Zeger and Diggle (1993) and Glonek and McCullagh (1995) on the 
multivariate logit, and by Chib and Greenberg (1998) on the multivariate probit. 
One purpose of this paper is to show case the multivariate probit model, along 
with some of its variants, as the canonical model for correlated binary data. This 
model was introduced more than twenty five years ago by Ashford and Sowden 
(1970) in the context of bivariate binary responses and was analyzed under sim-
plifying assumptions on the correlation structure by Amemiya (1972), Ochi and 
Prentice (1984) and Lesaffre and Kaufman (1992). The general version of the model 
was considered to be intractable. The problems, however, have now disappeared 
with the recent work of Chib and Greenberg (1998), which builds on the framework 
of Albert and Chib (1993). An efficient Markov chain Monte Carlo approach for 
both classical and Bayesian estimation and model comparison is now available. In 
addition, software that implements the Chib-Greenberg methods is also available. 
The MVP model provides a relatively straightforward way of modeling corre-
lated binary data. In this model, the marginal probability of each response is given 
by a probit function that depends on covariates and response specific parameters. 
Associations between the binary variables are incorporated by assuming that the 
vector of binary outcomes are a function of correlated Gaussian variables, taking the 
value one if the corresponding Gaussian component is positive and the value zero 
otherwise. This connection with a latent Gaussian random vector means that there-
gression coefficients can be interpreted independently of the correlation parameters 
(unlike the case of log-linear models). The link with Gaussian data is also helpful 
in estimation. By contrast, models based on marginal odds ratios [Connolly and 
Liang (1992)) tend to proliferate nuisance parameters as the number of variables 
increase and they become difficult to interpret and estimate. Finally, the Gaussian 
connection enables generalizations of the model to ordinal outcomes (see Chen and 
Dey (this volume)], mixed continuous and binary outcomes, and also to spatial data 
[Oliveira (1997)]. 
The rest of this chapter is organized as follows. In Section 2 we consider the 
multivariate probit model and some of its variants. In Section 3 we consider mod-
els for clustered binary data and discuss random effect models and Markov-type 
transition models. Section 4 will take up the issue of model comparison and show 
how Bayes factors for competing correlated binary data models can be computed. 
All the methods are illustrated using the new Bayesian software package BA YE-
S TAT (developed by the author) that provides a user-friendly Windows™ based 
environment for the fitting of these and many other Bayesian models. 
2. 
The Multivariate Probit Model 
Let YiJ denote a binary response on the ith observation unit and jth variable, 
and let Yi = (Yi 1, ... , YiJ )', 1 ~ i ~ n, denote the collection of responses on all 
J variables. Also let Xij denote the set of covariates for the jth response and /3j E 
Rki the conformable vector of covariate coefficients. Let j3' = (j3~, ... , !3':,) E Rk, 
k = I: kj denote the complete set of coefficients (one for each response) and let 

7. Correlated Binary Data 
115 
'E = { Uj k} denote a J x J correlation matrix. Finally let 
( x;l 
O' 
O' 
) 
O' 
I 
O' 
O' 
Xi= 
xi2 
O' 
O' 
xiJ 
denote the J x k covariate matrix on the ith subject. In the case that the covariate 
effects are not response specific and the same number of covariates influence the 
response Yij, the covariate matrix is Xi= (Xit, ... , XiJ)'. 
Then, according to the multivariate probit model, the marginal probability that 
Yii = 1 is given by the probit form 
and the probability that Yi = Yi, conditioned on parameters {3, E, and covariates 
Xij , is given by 
pr(Yi = Yil/3, 'E)= pr(Yil/3, 'E)= f ... f ¢J(tl0, 'E) dt, 
(1) 
jAiJ 
jAil 
where ¢J(tl0, 'E) is the density of a J-variate normal distribution with mean vector 
0 and correlation matrix 'E and Aij is the interval 
A .. _ { ( - oo, x~i /3i) 
if Yii = 1 
tJ -
[x~j /3j, oo) 
if Yii = 0. 
(2) 
Note that each outcome is determined by its own set of kj covariates Xij and co-
variate effects /3j . 
The multivariate discrete mass function presented in (1) can be specified in terms 
of latent Gaussian random variables. This alternative formulation also forms the 
basis of the computational scheme that is described below. Let Zi = ( Zil, ... , ZiJ) 
denote a J -variate normal vector and let 
(3) 
Now let Yij be 1 or 0 according to the sign of Zij : 
Yij = l(Zij > 0), 
j = 1, ... , J, 
(4) 
where I(A) is the indicator function of the event A. The probability in (1) may be 
expressed as 
(5) 
where Bij is the interval (0, oo) if Yij = 1 and the interval ( -oo, 0] if Yij = 0. It is 
easy to confirm that this integral reduces to the form given above. It should also 
be noted that due to the threshold specification in ( 4), the scale of Zij cannot be 
identified. As a consequence, the matrix 'E must be in correlation form (with units 
on the main diagonal). 

116 
Chib 
2.1 
Dependence Structures 
One basic question in the analysis of correlated binary data is the following: How 
should correlation between binary outcomes be defined and measured? The point 
of view behind the MVP model is that the correlation is modeled at the level of 
the latent data which then induces correlation amongst the binary outcomes. This 
modeling perspective is both flexible and general. In contrast, attempts to model 
correlation directly (as in the classical literature using marginal odds ratios) invari-
ably lead to difficulties, partly because it is difficult to specify pair-wise correlations 
in general, and partly because the binary data scale is not natural for thinking 
about dependence. 
Within the context of the MVP model, alternative dependence structures are 
easily specified and conceived, due to the connection with Gaussian latent data. 
Some of the possibilities are enumerated below. 
• Unrestricted form. Here 'E is fully unrestricted except for the unit constraints 
on the diagonal. The unrestricted 'E matrix has p* = p(p- 1)/2 unknown 
correlation parameters that must be estimated. 
• Equicorrelated form. In this case, the correlations are all equal and described 
by a single parameter p. This form can be a starting point for the analysis 
when one is dealing with outcomes where are all the pair-wise correlations are 
believed to have the same sign. The equicorrelated model may be also be seen 
as arising from a random effect formulation. 
• Toeplitz form. Under this case, the correlations depend on a single parameter 
p but under the restriction that Corr(Zik, Zil) = plk-ll. This version can 
be useful when the binary outcomes are collected from a longitudinal study 
where it is plausible that the correlation between outcomes at different dates 
will diminish with the lag. In fact, in the context of longitudinal data, the 'E 
matrix can be specified in many other forms with analogy with the correlation 
structures that arise in standard time series ARMA modeling. 
2. 2 
Student-t Specification 
Now suppose that the distribution on the latent Zi is multivariate-t with specified 
degrees of freedom v. This gives rise to a model that may be called the multivariate-
t link model. Albert and Chib (1993) extended the probit link to the t-link in the 
binary response case and provided a simple approach for estimating the result-
ing model. Under the multivariate-t assumption, Zd,B, 'E,..., MVTJ(Xi,B, 'E, v) with 
density 
(6) 
As before, the matrix 'E is in correlation form and the observed outcomes are defined 
by ( 4). The model for the latent Zi may be expressed as a scale mixture of normals 
by introducing a random variable Ai ,..., Gamma(~, ~) and letting 
(7) 
Conditionally on Ai, this model is equivalent to the MVP model. 
Chen and Dey (1998) have further extended this idea by letting Zi follow a general 
scale mixture of normal distributions. 

7. Correlated Binary Data 
117 
2.3 Estimation of the MVP Model 
Consider now the question of inference in the MVP model. We are given a set of 
data on n subjects with outcomes Y = {YJi=l and interest centers on the param-
eters of the model 0 = ({3, 'E) and the posterior distribution 1r({3, 'ElY), given some 
prior distribution 1r({3, :E) on the parameters. 
To begin with, note that the likelihood function of the model is composed of the 
probabilities 
pr(yi l/3, 'E)= { ... { 
¢J(t!O, 'E) dt, i ~ n. 
}Au jAil 
As has been known for some time, these integrals are difficult to evaluate by non-
simulation based methods if~ is unrestricted and J is large. This problem, however, 
has been bypassed entirely by Chib and Greenberg (1998). 
Let u = (u1 2 , U31, u32 , ... , UJJ) denote the J(J- 1)/2 distinct elements of 'E. It 
can be shown that the admissible values of u (that lead to a positive definite 'E 
matrix) form a convex solid body in the hypercube [ -1, 1 ]P. Denote this set by C. 
Now let Z = (Z1, ... , Zn) denote the latent values corresponding to the observed 
data Y = {Yi}i=l· Consider the simulation of the augmented posterior density 
1r({3, u, Zly) 
ex 
1r({3, u)f(ZI/3, 'E)pr(yiZ, {3, 'E) 
n 
ex 
1r({3, u) II ( 
¢J(Zi l/3, 'E)pr(yi IZi, {3, 'E)) , {3 E ~k, u E C 
(8) 
i=l 
where 
J 
pr(YiiZi, {3, 'E)= II {l(Zij > O)l(Yij = 1) + I(zij ~ O)l(Yij = 0)} 
j=l 
due to the fact that pr(yi IZi, {3, 'E) is one if the Zij respect the constraint imposed 
by the observed value of Yij, indicated by the functions in curly braces. 
To sample this posterior density, the following Markov chain Monte Carlo scheme 
is available. Fuller details are in the appendix. 
Algorithm 1 (Chib-Greenberg (1998)): 
1. Sample Zij from the distribution Zij IZi( -j), Yi, {3, 'E for j ~ J and i ~ n, where 
Zi( -j) are the latent values on the ith subject excluding Zij; 
2. Sample {3 from the distribution f31Z, y, 'E; 
3. Sample u from the conditional distribution uiZ, y, {3 using the Metropolis-
Hastings algorithm; 
4. Repeat Steps 1-3 using the most recent values of the conditioning variables. 
This algorithm achieves the intended purpose in that it allows one to sample the 
posterior distribution without requiring the computation of the likelihood function. 
In Step 3, the Metropolis-Hastings algorithm [see Chib and Greenberg (1995)) is 
used to sample the distinct elements of 'E. This method of sampling u is quite 
general and can be used to sample unrestricted covariance matrices under non-
Wishart prior assumptions. 
In the source paper, the algorithm above is tested in a problem where J = 7 
and the dimension of u is twenty one. The method is found to work quickly and 

118 
Chib 
efficiently. The paper also provides some additional discussion about sampling u in 
blocks if the dimension of u is excessively large, say over thirty. It is mentioned that 
estimation of the MVP model when 'E is restricted to be in equicorrelated form 
(say) requires no new analysis because Step 3 is based on the distinct elements of 
'E. Additional restrictions, therefore, simplify the simulation. 
Example: MVP model applied to longitudinal data. Consider a data set on the 
health effects of pollution on young children. The response variable is an indicator 
of wheezing status. The data are collected on 537 children in Stuebenville, Ohio, 
each observed at ages 7, 8, 9 and 10 years (see Chib and Greenberg, 1998 for the 
data). One assumes that the marginal probability of wheeze status of the ith child 
at the jth time point is 
where x1 is the age of the child centered at nine years, x 2 is a binary indicator vari-
able representing the mother's smoking habit during the first year of the study, and 
X3 = x1x2. We are assuming that {3 is constant across categories j. Let the associ-
ation between the binary responses on the ith child be modeled by an unrestricted 
correlation matrix. The prior on {3 = ({3~, {32 , {33 , {34 ) is independent Gaussian with 
a mean of zero and a variance of ten. Also let the prior on the correlations u, a 
six component vector, be proportional to a normal distribution with mean zero and 
covariance equal to the identity matrix. From 10,000 runs of algorithm 1 (coded in 
the software package BAYESTAT), one obtains the following covariate effects and 
posterior distributions of the correlations. 
Prior 
Posterior 
Mean 
Std dev 
Mean 
NSE 
Std dev 
Lower 
Upper 
/31 
0.000 
3.162 
-1.108 
0.001 
0.062 
-1.231 
-0.985 
/32 
0.000 
3.162 
-0.077 
0.001 
0.030 
-0.136 
-0.017 
/33 
0.000 
3.162 
0.155 
0.002 
0.101 
-0.043 
0.352 
f34 
0.000 
3.162 
0.036 
0.001 
0.049 
-0.058 
0.131 
TABLE 7.1. Covariate effects in the Six Cities example: MVP model with unrestricted 
correlations. In the table, NSE denotes the numerical standard error, lower is the 2.5th 
percentile and upper is the 97.5th percentile of the simulated draws. The results are based 
on 10000 draws from Algorithm 1. 
These posterior distributions suggest that an equicorrelated correlation structure 
might be appropriate for these data. A formal comparison of the two correlation 
structures is provided below using marginal likelihoods. 
2.4 
Fitting of the Multivariate t-link Model 
Algorithm 1 is easily modified for the fitting of the multivariate-t link version of 
the MVP model. One simple possibility is to include the { Ai} into the sampling since 
conditioned on the value of Ai, the t-link binary model reduces to the MVP model. 
With this augmentation, one implements Steps 1-3 conditioned on the value of {..\i}. 
The sampling is completed with an additional step involving the simulation of { Ai}. 

7. Correlated Binary Data 
119 
+ 
I 
I 
I 
0.8 
8 
_t_ 
I 
I 
I 
I 
I 
I 
I 
8 
I T 
+ 
I 
0.7 
0.5 
I 
I 
I 
0.4 
I t 
0. 3f-
+ 
+ 
slg21 
slg31 
- I 
I 
I 
I 
I 
8 
I 
I 
I 
I 
I 
T 
slg32 
Column Number 
--*-
I 
I 
I 
I 
I 
I 
8 
I 
I 
I 
I 
I 
I 
T 
+ 
slg41 
-,-
I 
I 
I 
I 8 
I 
I 
I 
I 
I 
T 
* 
slg42 
FIGURE 7.1. Posterior boxplots of the correlations in the Six Cities example: MVP model 
A straightforward calculation shows that the updated full conditional distribution 
of Ai is 
, ·IZ· a 
'C'I 
G 
(v + J v + (Zi- Xif3)'E-
1(Zi- Xif3)) 
. < 
At 
, , tJ, Ll I'V 
amma 
2 
, 
2 
, z _ n, 
which is easily sampled. The modified sampler thus requires little extra coding. 
3. 
Longitudinal Binary Data 
Correlated binary data can arise from a longitudinal study, as in the Six Cities 
example above. For such data, one can use the MVP model or one of the other 
special models that have been designed for longitudinal data. In this section we 
discuss the important classes of models that are available. 
3.1 
Pro bit (or logit) Normal Model 
Consider a sequence of binary measurements Yi = (yil, ... , Yini)', Yij E {0, 1} on 
the ith unit taken at ni specific time points (assumed here to be equally spaced). 
Let Xij denote a set of covariates on the ith subject at the jth time point (j :::; ni)· 
In modeling a sequence of such binary random variables the main issue is about a 
probability model for Yi that incorporates intra-cluster correlation. 
A quite popular means of modeling intra-cluster correlation is via a random effects 
formulation [Stiratelli, Laird and Ware (1984)]. Under this model, the probability 
of a positive response is 
Pr(Yij = llhi) = F(x~j(3 + w~j hi) 
hi 
I'V 
N(O, D), 
(9) 
where Wij is a subset of Xij and hi denotes a vector of random effects. The param-
eters (3 and D are unknown. 

120 
Chib 
3.2 Inference 
For the ni outcomes Yi on the ith subject, the joint probability mass function is 
J { fi [F(x:J,a + w:; b;)j"'; [1- F(x:!,a + w;; b;)r-><;} ¢(hdO, D) dbi 
(10) 
J=l 
which, although quite different from the MVP formulation, is also difficult to eval-
uate. 
In existing work, both in the classical and Bayesian contexts, F is taken to be 
cdf of either the logit or standard normal distributions. Zeger and Karim (1991), 
in early work, provide a Bayesian analysis of the logit case, while the probit case is 
discussed by Albert and Chib (1996) and Chib and Carlin (1997). The latter two 
papers demonstrate that the probit model is considerably easier to handle than the 
logit. All full conditional distributions are tractable and the MCMC sampling does 
not require a Metropolis step. 
3.3 Computations for the Probit-Normal Model 
The most effective strategy for dealing with the probit normal model is to express 
the model in terms of latent Gaussian variables. Let Zit be distributed as normal 
with 
Zij lhi "'N(x~jf3 + w~j hi, 1), 1.:::; j $ ni; 1 $ i $ n, 
(11) 
and write the model for the ith cluster as Zi "'N(Xif3 + Wihi, 1), where Wi is 
a ni x q matrix (under obvious notation). As in the MVP model, let the observed 
response Yij be given by 
{ 
1 if Zi · > 0 
Yij = 
0 if z ·
3
• < 0 
tJ -
Then, it can be seen that the joint probability mass function of Yi from this specifi-
cation reduces to (10). With the introduction of the latent data, one simple MCMC 
strategy, in line with that proposed by Gelfand and Smith (1990) for Gaussian panel 
models with fully observed outcomes, is discussed by Albert and Chib (1996). This 
algorithm was recently revised by Chib and Carlin (1997) with a view to improving 
its mixing properties. The idea behind the modification is to sample f3 and {Zi} in 
one block, marginalized over {hi}· That this is possible is a consequence of the fact 
that hi can be analytically integrated out of the model. Then, for the ith cluster, 
it follows that 
Zi 1!3, D "'N(Xif3, Vi) 
where Vi= WiDWi +I. This model now looks identical to the MVP model with 
a particular form for the covariance matrix. Therefore, conditioned on (y, D) (but 
marginalized over {hi}), one can sample f3 and {Zi} exactly as in Steps 1 and 2 of 
Algorithm 1. Given sampled values of {Zi} and {3, one then samples {hi} from the 
model 
(Zi - Xif3) 
N(Wihi, Vi) 
hi "' Nq(O, D). 
In specifying the prior on D- 1 in this model, a useful approach is to assume that 
D follows the inverse Wishart distribution with mean Do on a0 degrees of freedom. 

FIGURE 7.2. Posterior boxplots in the Six Cities example: Probit normal model 
Then, the prior on n- 1 is Wishart with v0 = a0 - q -1 degrees of freedom and scale 
matrix R 0 = D0
1 f(v0 - 2q- 2). This leads to the following algorithm for sampling 
the probit-normal model. 
Algorithm 2 (Chib-Carlin (1997)): 
1. Sample Zij from the distribution Zij IZi( -j), Yi, {3, D for j :::; J and i :::; n, where 
Zi( -j) are the latent values on the ith subject excluding Zij; 
2. Sample {3 from the distribution f31Z, y, D; 
3. Sample hi from the conditional distribution hdZi, {3, D; 
4. Sample n- 1 from the distribution n- 11{bi}; 
5. Repeat Steps 1-4 using the most recent values of the conditioning variables. 
Example ( contd.). Probit normal model applied to the Six Cities data. Consider 
the application of the probit normal model to the Six Cities data analyzed earlier 
through the multivariate probit model. Assume now that the marginal probability 
of wheeze status of the ith child at the jth time point is 
with Wij = (l,x 2ij), implying that both the intercept and the age effects are 
children-specific. The parameter vector {3 is given the same Gaussian prior as before 
and D : 2 x 2 is assumed apriori to have an inverse Wishart distribution with mean 
of l2 on 15 degrees of freedom. From 10,000 runs of algorithm 2 (coded in the soft-
ware package BAYESTAT), one obtains the following posterior distribution of the 
covariate effects and elements of the matrix D. The posterior distributions indicate 
that the age effect is not significant but that there is considerable heterogeneity in 
the effect across the children in the sample. 

122 
Chib 
3.4 
Binary Response Hierarchical Model 
The random effect model for clustered binary data can be extended to the case 
where all covariate effects are subject-specific. Then, Wij = Xij and the model can 
be written as 
Pr(Yij = 1lhi) = 
F(x~jhi) 
hi 
N(Ai/3, D) 
(12) 
where Ai denotes a k x m matrix consisting of cluster-specific covariates. This 
model may be called a two stage binary response hierarchical model. It is a non-
linear version of the hierarchical models considered by Lindley and Smith (1972). In 
this model, it is not required that the dimension of f3 be smaller than the dimension 
of hi. 
Algorithm 2 has been easily adapted to deal with this model since one can define 
the latent data model as Zi rv N(Xi hi, I) and integrate out hi to produce the 
distribution 
Zi 1/3, D rv N(XiAi/3, v; =I+ XiDXD 
Once again Zi can be sampled from its full conditional distribution as in Step 1 of 
the MVP algorithm and f3 can be sampled as in Step 2 of the MVP algorithm (both 
using XiAi as the new covariate matrix). The remaining steps are similar to those 
in Algorithm 2. The resulting algorithm is given as follows. 
Algorithm 3 (Chih and Carlin (1997)): 
1. Sample Zij from the distribution Zij IZi( -j), Yi, {3, D for j ~ J and i ~ n, where 
Zi( -j) are the latent values on the ith subject excluding Zij; 
2. Sample f3 from the distribution f31Z, y, D; 
3. Sample hi from the conditional distribution hiiZi, {3, D; 
4. Sample n-1 from the distribution n-11{hi}; 
5. Repeat Steps 1-4 using the most recent values of the conditioning variables. 
Example (contd.). To illustrate the fitting of the binary response hierarchical 
model, consider again the Six Cities data set. Now suppose that 
where, as before, x1 is the age covariate and X3 is the interaction between age 
and the mother's smoking indicator. Treat X2i as a covariate that influences the 
subject-specific covariate effects hi and write the second stage mean function as 
( 
1 
X2i 
0 0 
0 0 
) 
Aif3 = 
0 0 
1 
X2i 
0 0 
f3 
0 0 
0 0 
1 
X2i 
where f3 = (/31, ... , /36) is a six dimensional fixed effect and the matrix D is a full six 
dimensional matrix of variances and covariances. Assume that the prior information 
about {3 can be described by a N(O, 101) distribution and that ofD-1 by a Wishart 
with 25 degrees of freedom with a mean equal to the identity matrix. Algorithm 
3, which is coded in the software package BAYESTAT, produces the following 

7. Correlated Binary Data 
123 
+ 
+ 
+ 
+ 
* 
• l ' 
10 
I 
I 
I 
I 
I 
I 
I 
I i! 
I 
I 
I 
I 
++ 88 8 
+ + ... t ... J. 
t 
I 
I 
I 
-2 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-6 
I 
I 
I 
8 
i t 1 
+ 
0 
+ 
-1 
+ 
bel1 
bet2 
bet3 
bet4 
betS 
bel6 
011 
021 
022 
031 
032 
033 
FIGURE 7 .3. Posterior boxplots in the Six Cities example: Probit hierarchical model 
posterior distributions of f3 and D from 10,000 iterations, beyond a burn-in of 500 
cycles. Because the posterior distributions of {34 to /36 are quite dispersed, one can 
conjecture that the random effects bil and bi2 do not follow a hierarchical model 
with mean depending on X2i. There is, however, clear indication of heterogeneity in 
the coefficients across the subjects. 
3.5 
Other Models 
Other models for clustered binary outcomes have been proposed in the literature. 
One class of models is the Markov transition model in which the probability of a 
positive outcome at observation j, conditioned on the past outcomes, depends only 
on the last outcome. The simplest such model is given by 
Pr(YiJ = 1lYil (I:$ j- 1), /3) = F(x~tf3 + ¢>1YiJ-d, 
where YiJ -1 is the value of the response at time point j - 1 and F is the cdf 
of some distribution, usually that of the standard normal or the standard logit. 
Under this model dependence is of the first-order Markov kind. Higher order Markov 
dependence can be modeled by letting 
Other variations on this theme are possible [see Muenz and Rubinstein (1992)). 
These models are easily fit using latent data as discussed above. There are no real 
problems in fitting these models except that one must specify the start-up values of 
the response before the first time point. One possibility is to condition the analysis 
on the first few observations in the observed data. This effectively reduces the sample 
size on each cluster but the cost of doing this is minimal provided the time series is 
sufficiently long. 

124 
Chib 
4. 
Comparison of Alternative Models 
In the discussion above, we have considered the MCMC based fitting of a variety 
of different correlated binary data models. We now turn to the question of how 
these alternative models can be compared on the basis of a given set of data and 
priors. One Bayesian approach is to compute the model marginal likelihood and 
to compare competing models using the ratio of marginal likelihoods (called Bayes 
factors). The model marginal likelihood is defined as the integral of the likelihood 
function with respect to the prior density on the parameters. Specifically, if we let 
Mj denote model j, and f(yiMJ, B) and 1r(BIM1) denote the likelihood function 
and prior density, respectively, then the marginal likelihood is defined as 
m(yiMJ) = j f(y!MJ,B)?r(BIMJ)dB. 
Alternatively, the marginal likelihood, by virtue of being the normalizing constant 
of the posterior density, can be expressed as 
(13) 
This expression is an identity in B. Chib (1995) has called this expression the ba-
sic marginal likelihood identity and has developed a technique for computing the 
marginal likelihood by a two step method: first selecting a high posterior den-
sity point B* (such as the posterior mean) and second computing the ordinate 
n(9* I y, ~) by a· conditional /marginal decomposition, where each conditionaVmarginal 
ordinate is estimated from the MCMC output. Given this estimate of the posterior 
ordinate (denoted ?r( B* IY, M j)), the marginal likelihood estimate on the log scale 
is given by 
log m(yiMJ) =log f(yiMJ, B*) +log 1r(B* IMJ)- log 11-(B* IY, Mj). 
(14) 
4.1 
Likelihood Ordinate 
Estimation of the various correlated binary data models does not require the 
computation of the likelihood function. In the expression of the marginal likelihood, 
however, one must compute the likelihood ordinate at the single point B*. As it turns 
out, the computation of this ordinate for each of the models presented above involves 
an integral that is similar to that which arises in the context of the multivariate 
probit model. In particular, the likelihood contribution f(ydMJ, B*) of the ith 
observation in each model is of the type 
(15) 
where Bij is the interval (0, oo) if YiJ = 1 and the interval ( -oo, 0] if YiJ = 0. In the 
case of the MVP model, J.ti = Xi{3 and Ei = E; in the case of the probit normal 
model, J.ti = Xi{3 and Ei = I+ WiDW~; while in the case of hierarchical model, 
J.ti = XiAif3 and Ei =I+ XiDXi. 
One simple way to compute the likelihood contribution is through an application 
of Chib's method. One can note that the likelihood contribution is the normalizing 

7. Correlated Binary Data 
125 
constant of the truncated normal density f(Zi IYi, J.ti, Ei) and hence the quantity of 
interest is given by 
!( I 
E ) 
f(Zt IJ.ti, Ei) 
Yi /Ji, 
i = /(Z*Iy· u· E·) 
i 
'' ,..,, ' 
' 
where z; is some arbitrarily chosen vector. In this expression the numerator is multi-
variate normal and hence is easily available while the denominator can be computed 
by the approach of Ritter and Tanner (1992) as follows. Write the denominator (by 
extending the argument) as 
and note that the integrand is the Markov transition density of moving from Zi to 
Zt. Hence it can be written as 
J 
f(Zi IYi, J.ti, Ei, Zi) = IT f(zij lzik (k < j), Zil (l > j), Yi, J.ti, Ei) 
j=1 
where each of the terms in the product is a univariate truncated normal density, 
truncated to the interval (0, oo) if Yij = 1 or to the region ( -oo, 0) if Yij = 0. 
Now the integral in ( 16) can be computed by taking a large number of draws of 
Zi from 7r(Zdyi,J..ti, Ei), computing f(ZiiYi,J.ti, Ei, Zi) for each such simulated Zi, 
and averaging the results. 
4.2 
Posterior Ordinate 
The starting point in this computation is a decomposition of the posterior ordinate 
into marginal and conditional ordinates followed by a Monte Carlo estimate of each 
ordinate. In the case of the MVP model, fJ = ({3, u) and the posterior ordinate is 
written as 
1r( 0* IY) = 1r( u* IY )1r({3* IY, E*) 
where 
7r(f3*1y,E*) = J 
7r({3*1Z,y,E*)7r(Ziy,E*)dZ. 
The latter ordinate can be computed by taking draws of Z from 1r(Ziy, E*) and 
averaging the Gaussian density 'lf'({3* IZ, y, E*) over these draws. Specifically, as 
shown in the Appendix under Algorithm 1, 
where ~ = B- 1(Bof3o + 2:7=1 X~E*-
1 Zi), B = Bo + 2:7=1 X~E*-
1 Xi and {30 and 
Bo are the prior hyperparameter values. This ordinate can be averaged over the 
different values of Z that are drawn from 1r(Ziy, E*). How should these draws of 
Z be obtained? A simple approach suggested by Chib is to employ the idea of a 
reduced MCMC run. The reduced run involves running Algorithm 1 with E fixed 
at E*, sampling {3 from f31Z, y, E* and Z from Zly, {3, E*. The draws of Z from 
this run follow the desired distribution. The estimation of the posterior ordinate is 
completed by estimating the posterior ordinate 1r( u* IY) by kernel smoothing applied 
to the draws on u from the full run of Algorithm 1. 

126 
Chib 
This same approach can be applied to the remaining models. For example, in the 
probit normal model,()= (JJ, n- 1) and the posterior ordinate is written as 
where 7r(D-hjy) is obtained by averaging 1r(D*- 1j{bi}) (a Wishart density- see 
Algorithm 2 in the Appendix) over the draws of {hi}, and 1r(jJ* IY, D*) is obtained by 
averaging the Gaussian density 1r(jJ* IY, Z, D*) over draws of Z from the reduced run 
consisting of the distributions JJ!y,Z,D*; Zly,jJ,D*. Precisely the same approach 
works in the case of the binary response hierarchical model. 
Example: Consider the comparison of five alternative models for correlated bi-
nary data. Three of these models are the ones that were considered above. Two other 
models are in the MVP class with restricted correlation matrices. One of the very 
interesting features of marginal likelihoods is that they allow for these comparisons 
amongst non-nested models. In BAYES TAT, the marginal likelihood is computed 
using (3) for each model that is simulated. The results for the five models under 
comparison are reported in Table 2. On the basis of these marginal likelihoods we 
M1 
M2 
M3 
M4 
M5 
In f(Yn I()*) 
-795.1869 
-798.5567 
-804.4102 
-805.0249 
-812.0036 
lnm(Yn) 
-823.9188 
-818.009 
-824.0001 
-829.9736 
-840.6648 
TABLE 7.2. Maximized log-likelihood and log marginal likelihood of five models fit in the 
examples: Mt is MVP with unrestricted correlations; M2 is MVP with an equicorrelated 
correlation; M3 is the MVP with Toeplitz correlation structure; M4 is the Probit normal; 
and Ms is the hierarchical probit. 
conclude that the data tend to support the MVP model with equicorrelated corre-
lations. For completeness, the prior-posterior summary for the best fitting model is 
reproduced in Table 3. 
Prior 
Posterior 
Mean 
Std dev 
Mean 
NSE 
Std dev 
Lower 
Upper 
{31 
0.000 
3.162 
-1.103 
0.001 
0.064 
-1.227 
-0.978 
P2 
0.000 
3.162 
-0.077 
0.001 
0.030 
-0.137 
-0.021 
P3 
0.000 
3.162 
0.158 
0.002 
0.103 
-0.046 
0.361 
P4 
0.000 
3.162 
0.039 
0.001 
0.047 
-0.051 
0.133 
P1 
0.500 
1.000 
0.651 
0.002 
0.039 
0.571 
0.727 
-818.009 
TABLE 7.3. Prior-posterior summary for the equicorrelated MVP model applied to the 
Six Cities data. In the table, NSE denotes the numerical standard error, lower is the 2.5th 
percentile and upper is the 97 .5th percentile of the simulated draws. The results are based 
on 10000 draws from Algorithm 1. 

7. Correlated Binary Data 
127 
5. 
Concluding Remarks 
This chapter has summarized some of the recent developments in the fitting 
and comparison of Bayesian models for correlated binary data. The discussion has 
emphasized the multivariate probit model, which may be applied to clustered and 
non-clustered responses, and the probit-normal and binary response hierarchical 
models (the latter two for use with clustered binary data). We discussed the fitting 
of these models and the issue of model comparisons. The discussion did not explore 
residual diagnostics and model fit issues. Early developments on some relevant ideas 
are contained in Albert and Chib (1995) and Chen and Dey (1998). Another problem 
that is not covered in this paper relates to the question of drop-outs in longitudinal 
data analysis [see Cowles, Carlin and Connett ( 1996)]. 
The comparison of alternative models for correlated binary data is an important 
achievement of the last few years. We have shown how the marginal likelihood of 
each model can be computed based on the output of the MCMC simulations. At 
this time, other approaches, for example, the method of Carlin and Chib (1995), 
have not been applied to compare alternative models for correlated binary data. 
Further developments in these areas will likely occur in the near future. 
6. 
Appendix 
This appendix reports full details of each algorithm discussed above. These algo-
rithms are coded in the software package BAYESTAT that has been developed by 
the author. This software program includes a spreadsheet and graphics environment 
for data entry and analysis and pull-down menus for the specification of a large va-
riety of Bayesian models. This program automatically includes the computation of 
the marginal likelihood by Chib's method for every model that is included in the 
program. 
6.1 
Algorithm 1 
1. Sample Zij from the distribution Zij IZi( -j), Yi, ~' E for j ~ J and i ~ n, where 
Zi(-j) are the latent values on the ith subject excluding Zij and 
In this expression TN denotes the truncated normal distribution and aij and 
bij are the parameters of the conditional distributions Zij IZi( -j), ~' E (ignoring 
truncation). The truncated normal distributions are sampled by the inverse 
cdf method, as described in Devroye ( 1987). 
2. Sample~ from the distribution ~IZ, y, E, where, under the prior~,.... N(~o, B0
1), 

128 
Chib 
3. Sample u from the conditional distribution uiZ,y,~, where the density of the 
latter distribution (up to an irrelevant norming constant) is 
n 
g(u) = 1r(u) IT t/>(ZdXi~, E)I[u E C) 
i=1 
and 1r(u) is the assumed prior on u (say multivariate normal truncated to C). 
To sample g(u), let q(u!J.t, V) denote a multivariate-t density with parameters 
J.t and V defined as the mode and inverse of the negative Hessian, respectively, 
of log g(u). Then: 
(a) Sample a proposal value u' from the density q(u'IJ.t, V) 
(b) Move to u' given the current point u with probability of move 
( 
') _ 
. (1r(u') /(ZI~, E')I[u' E C] q(u!J.t, V) 1) 
a u, u - mm 
1r(u) /(ZI~, E)I[u E C] q(u'IJ.t, V)' 
' 
and stay at u with probability 1- a(u, u'). 
4. Repeat Steps 1-3 using the most recent values of the conditioning variables. 
6.2 Algorithm 2 
1. Sample Zij from the distribution ZiJIZi(-j),Yi,~,D for j ~ J and i ~ n, where 
Zi( -j) are the latent values on the ith subject excluding Zij and 
.. IZ· 
. 
. ~ D"'"' { TN(o,oo)(aiJ,biJ) 
if 
YiJ = 1 
Zz:J 
z(-:J),Yz, ' 
TN(-oo,o])(aij,bij) 
if Yij = 0 
In this expression TN denotes the truncated normal distribution and aij and 
bij are the parameters of the conditional distributions Zij IZi( -j), ~'vi (ignor-
ing truncation). 
2. Sample~ from the distribution ~IZ, y, D, where, under the prior~"'"' N(~o, B0 
1 ), 
~IZ,y, E"'"' Nk(~, B- 1), 
with~= B-
1 (Bo~o + L:?=1 X~V£
1 Zi) and B = Bo + L:?= 1 X~V£
1 Xi. 
3. Sample hi from the conditional distribution hdZi, ~' D where 
... 
-1 
hi IZi, ~' D I'V Nk(hi' Di ) ' 
hi= D£ 1(WiV£ 1(Zi- Xi~) and Di = (D + W~V£
1 Wi)-
1
. 
4. Sample n- 1 from the distribution n- 11{hi} where 
under the assumption that the prior on n- 1 is Wishart(v0 , R 0). 
5. Repeat Steps 1-4 using the most recent values of the conditioning variables. 

7. Correlated Binary Data 
129 
6.3 Algorithm 3 
1. Sample Zij from the distribution Zij IZi(-j), Yi, ~' D for j ~ J and i ~ n, where 
Zi( -i) are the latent values on the ith subject excluding Zij and 
Z"IZ· . . ~ D "-J { 
TN(o,oo)(aij,bij) 
if Yii = 1 
zJ 
z(-J),y,, ' 
TN(-oo,o])(aij,bij) 
if Yii = 0 
In this expression TN denotes the truncated normal distribution and aii and 
bij are the parameters of the conditional distributions Zij IZi( -j), ~' Vf (ignor-
ing truncation). 
2. Sample~ from the distribution ~IZ, y, D, where, under the prior~ "-J N(~o, B0
1), 
with /3 = B- 1 (Bo~o+ 2:?= 1 A~X~v;-
1 Zi) and B = Bo+ 2:?= 1 A~X~v;-
1 AiXi. 
3. Sample hi from the conditional distribution bdZi, ,B, D where 
... 
-1 
bdZi,~,D,.... Nk(bi,Di ) , 
hi= Di 1(D- 1 Ai~ +Xi v;- 1Zi) and Di = (D + X~v;-
1 Xi)-
1
. 
4. Sample n- 1 from the distribution n- 1l{hi} where 
5. Repeat Steps 1-4 using the most recent values of the conditioning variables. 
References 
Agresti, A. (1989). A survey of models for repeated ordered categorical response 
data. Statistics in Medicine, 8: 1209-1224. 
Albert, J. and Chib, S. (1993). Bayesian analysis of binary and polychotomous 
response data. Journal of the American Statistical Association, 88, 669-679. 
Albert, J. and Chib, S. (1995). Bayesian Residual Analysis for Binary Response 
Regression Models. Biometrika, (1995), 82, 747-759. 
Albert, J. and Chib, S. (1996). Bayesian Probit Modeling of Binary Repeated 
Measures Data with an Application to a Cross-Over Trial. In Bayesian Bio-
statistics (eds. D. A. Berry and D. K. Stangl), 577-599, New York: Marcel 
Dekker. 
Amemiya, T. (1972). Bivariate probit analysis: minimum chi-square methods. 
Journal of American statistical association 69, 940-44. 
Ashford, J. R. & Sowden, R. R. (1970). Multivariate probit analysis. Biometrics 
26, 535-46. 

130 
Chib 
Betensky, R A., Whittemore, A. S. (1996). An analysis of correlated multivariate 
binary data: Application to familial cancers of the ovary and breast. Applied 
Statistics, 45: 411-429. 
Carey, V, Zeger, S. L., Diggle, P. (1993). Modelling multivariate binary data with 
alternating logistic regressions. Biometrika, 80: 517-526. 
Carlin, B and Chib, S. (1995). Bayesian Model Choice via Markov Chain Monte 
Carlo. Journal of the Royal Statistical Society, Ser B, 57, 473-484. 
Chen, M-H. and Dey, D.K. (1998). Bayesian analysis of correlated binary responses 
via scale mixture of multivariate normal link functions. Technical report, De-
partment of Statistics, University of Connecticut. 
Chib, S. (1995). Marginal Likelihood From the Gibbs Output. Journal of the Amer-
ican Statistical Association, 90, 1313-1321. 
Chib, S. and Greenberg, E (1995). Understanding the Metropolis-Hastings algo-
rithm. American Statistician, 49, 327-335. 
Chib, S. and Greenberg, E (1998). Analysis ofmultivariateprobit models. Biometrika, 
85, 347-361. 
Chib, S. and Carlin, B (1997). On MCMC Sampling in Hierarchical Longitudinal 
Models. Statistics and Computing, in press. 
Connolly, M. A., Liang, Kung-Yee (1988). Conditional logistic regression models 
for correlated binary data. Biometrika, 75: 501-506. 
Cowles, M. K., Carlin, B. P., and Connett, J. E. (1996). Bayesian Tobit mod-
eling of longitudinal ordinal clinical tral compliance data with nonignorable 
missingness. Journal of the American Statistical Association, 91, 86-98. 
Fitzmaurice, Garrett M., Laird, NanM., Rotnitzky, A. G. (1993). Regression mod-
els for discrete longitudinal responses (Disc: p300-309). Statistical Science, 8: 
284-299. 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling-Based Approaches to Calcu-
lating Marginal Densities. Journal of the American Statistical Association, 85, 
398-409. 
Glonek, G. F. V. and McCullagh, P. (1995). Multivariate logistic models. J. R. 
Statist. Soc. B 57, 533-46. 
Lesaffre, E. and Kaufmann, H. K. (1992). Existence and uniqueness of the max-
imum likelihood estimator for a multivariate probit model. Journal of the 
American Statistical Association, 87: 805-811. 
Lindley, D.V., and Smith, A.F.M. (1972). Bayes estimates for the linear model 
(with discussion). J. Roy. Statist. Soc., Ser. B, 34, 1-41. 
Muenz, L. Rand Rubinstein, L. V. (1985). Markov models for covariate dependence 
of binary sequences. Biometrics, 41: 91-101. 
Ochi, Y., and Prentice, R. L. (1984). Likelihood inference in a correlated probit 
regression model. Biometrika, 71: 531-543. 

7. Correlated Binary Data 
131 
Oliveira, V. (1997). Bayesian prediction of clipped Gaussian random fields. Tech-
nical report. 
Pendergast, J. F., Gange, S. J., Newton, M.A., Lindstrom, M. J., Palta, M. and 
Fisher, M. R. (1996). A survey of methods for analyzing clustered binary 
response data. International Statistical Review, 64: 89-118. 
Ritter, C. and Tanner, M.A. Facilitating the Gibbs stopper and the Griddy-Gibbs 
sampler. Journal of the American Statistical Association, 87, 861-868. 
Stiratelli, R, Laird, N., and Ware, J. H (1984). Random-effects models for serial 
observations with binary response. Biometrics, 40: 961-971. 
Zeger, S. L. and Karim, M.R. (1991), Generalized linear models with random 
effects: A Gibbs sampling approach. Journal of the American Statistical As-
sociation 86, 79-86. 

This Page Intentionally Left Blank 

8 
Bayesian Analysis for Correlated 
Ordinal Data Models 
Ming-Hui Chen 
Dipak K. Dey 
ABSTRACT Bayesian methods are considered for analyzing correlated ordinal data. 
We present a new approach to Bayesian hierarchical generalized linear models using 
a rich class of scale mixture of multivariate normal (SMMVN) link functions. Fully 
parametric classical approaches to these are intractable and thus Bayesian methods 
are pursued using a Markov chain Monte Carlo sampling-based approach. Marginal 
likelihood approaches are used to compare the proposed models and simulation-based 
diagnostic procedures are developed to assess model adequacy. Novel computational 
algorithms to perform the Bayesian analysis are further developed and a real data 
example is used to illustrate the methodologies. 
1. 
Introduction 
There is a growing interest in the statistical literature concerning the modeling 
and analysis of correlated binary data. Prentice (1988) provided a comprehensive 
review of various modeling strategies using generalized linear regression analysis of 
correlated binary data with covariates associated at each binary response. Follow-
ing Liang and Zeger (1986) and Zeger and Liang (1986), Prentice used the general-
ized estimating equation (GEE) approach to obtain consistent and asymptotically 
normal estimators of regression coefficients. In a Bayesian framework, Chib and 
Greenberg (1998) used the multivariate probit (MVP) model for correlated binary 
data, while Chen and Dey (1998) considered general scale mixture of multivariate 
normal (SMMVN) link functions for longitudinal binary responses. However, the 
literature is still sparse in modeling and analyzing correlated or repeated ordinal 
data. A correlated ordinal data problem is not a simple generalization of the one 
for the correlated binary data. More importantly, correlated ordinal data problems 
are encountered in many practical applications. Data obtained from surveys are in-
herently categorical, items in a questionnaire usually consist of two-to-five options 
(e.g., "disagree," "neutral" and "agree"), and two or more responses are typically 
taken from the same individuals. Similarly, cholesterol level (low, medium, high) 
and blood sugar level (low, medium, high) on the same individual are measured 
over the time in a longitudinal fashion. 
In this chapter, we assume that two or more ordinal responses are taken one at 
time for the same subjects or repeated 'ordinal measurements are taken over time 
such as in longitudinal studies. Generalized linear regression methods are considered 
133 

134 
Chen & Dey 
for such correlated ordinal data to study the relation between various covariates and 
the polychotomous outcome measure. 
Cowles, Carlin and Connett (1996) used multivariate tobit models for analyz-
ing longitudinal ordinal data which include correlations among the latent variables. 
However, they considered only the three-levels ordinal responses. Here we extend 
work of Ashford and Sowden (1970) in a very novel way to correlated ordinal mod-
els based on multivariate link functions using a very rich class of scale mixtures of 
normal distributions. Our models are very flexible and include, as a special case, 
multivariate probit (MVP), t-link (MVT), logit (MVL), stable distribution fam-
ily links (MVS), and exponential power distribution family links (MVEP). These 
models are more attractive than random effects models since the exchangeability of 
correlation structure is not required. Our approach produces results with desirable 
features; furthermore, it addresses issues which have not previously been considered. 
The first objective of this chapter is to explore different modeling strategies for 
the analysis of correlated ordinal responses from a Bayesian perspective by incor-
porating scale mixture of multivariate normal distributions as link functions. By 
considering a rich class of link functions in such models (within a parametric frame-
work) this approach unifies all the previous methods as well as allowing enough 
flexibility. As one entertains a collection of such models for a given data set one 
needs to address the problem of model determination, i.e., model comparisons and 
model diagnostics. Specifically, we adopt the Markov chain Monte Carlo (MCMC) 
framework (e.g., Gelfand and Smith, 1990 and Tierney, 1994) to simulate the pos-
terior distribution for proposed models. Due to the nature of correlated ordinal 
data, we use marginal likelihoods (Chib, 1995) to compare proposed models and we 
consider several simulation-based diagnostic procedures to assess model adequacy. 
In this chapter, we also present various efficient computational algorithms for the 
complex simulation problems. For example, we use a reparameterization technique 
of Nandram and Chen (1996) to obtain SMMVN-link reparameterized models. Such 
a reparameterization technique greatly eases the computational burden. We use the 
collapsed Gibbs sampler of Liu (1994) to derive an efficient algorithm for simulating 
from the posterior distribution. One of the nice features in our algorithm is that 
we provide an efficient and nearly automatic Hastings scheme (Hastings, 1970) to 
generate the cutpoints needed to classify the ordinal response data, which is consid-
ered to be a very challenging problem in analyzing the Bayesian generalized linear 
models (see, for example, Cowles, 1996 and Nandram and Chen, 1996). Further-
more, in order to compare different SMMVN-link models, we provide an elegant 
implementation scheme of the data-augmentation-based method of Chib (1995) for 
computing marginal likelihoods. The computational algorithms presented in this 
chapter make the use of SMMVN-link models for analyzing the correlated ordinal 
data very attractive and convenient. 
The rest of the chapter is organized as follows. In Section 2, we discuss the gen-
eral structure of the scale mixture of multivariate normal link models and several 
special cases of this general setting. Section 3 is devoted to the development of the 
prior distributions as well as the distribution theory involved in the posterior cal-
culations. In Section 4, we discuss different methods involved in model comparisons 
and model diagnostics. Here we consider some novel graphical approaches to im-
plement different types of newly developed discrepancy measures and standardized 
latent residuals. In Section 5, we use an item response data example to illustrate 
the methodologies. Finally, Section 6 gives brief concluding remarks. 

8. Correlated Ordinal Data Models 
135 
2. 
Models 
We first introduce some notation which will be used throughout the paper. Sup-
pose that we observe an ordinal (1 through L) response Yij on the ith observations 
and the jth variable and let Xij = (Xii1, Xij2, ... , Xijp;) be the corresponding Pi-
dimensional row regression vector for i = 1, 2, ... , n and j = 1, 2, ... , J. (Note that 
Xii 1 may be 1, which corresponds to an intercept.) Denote Yi = (Yi1, Yi2, ... , YiJ)
1 
and assume that Yi1, Yi2, ... , YiJ are dependent whereas Y1, Y2, ... , Yn are inde-
pendent. Let Yi = (Yil, Yi2, ... , YiJ )' and y = (Y1, Y2, ... , Yn) be the observed data. 
Also let Pi = (f3i 1, f3i 2, ... , PiP;)' be a Pi-dimensional column vector of regression 
coefficients and (3 = (f3i, (3~, ... , (3'., )'. 
In order to set up the scale mixture of multivariate normal (SMMVN) link mod-
els for the correlated ordinal response data, we introduce a J -dimensional (latent) 
random vector w; = ( wi1, w;2, ... , w;J )' such that 
(1) 
where -oo = l}o :::; 1}1 :::; 1}2 :::; I},L- 1 :::; I}L=oo are cutpoints for the jth ordi-
nal response, which divide the real line into L intervals. As explained by Nandram 
. and Chen (1996), we specify 1}1 = 0 to ensure the identifiability of the cutpoint 
parameters. Here, we introduce different sets of cutpoints for different ordinal re-
sponses since in many practical problems, each ordinal response may behave quite 
differently. We further assume that 
N(xif3*, K(A)E*), 
(2) 
and 
A "' 7r(A), 
(3) 
where K(A) is a positive function of one-dimensional positive-valued scale mixing 
variable A, 7r(A) is a mixing distribution which is either discrete or continuous, 
Xi = diag(xil, Xi2, ... , xu), and (3* = ((3;', f3;', ... , (ij')' is a p = 2:,f=1 Pi di-
mensional column vector of regression coefficients corresponding to the cutpoints 
1} = (lj2, 1j3 , ... , I},L- 1)' for j = 1, 2, ... , J. In (2) we further take E* = (p'Ji•) JxJ 
to be a correlation matrix such that pJi = 1 to ensure the identifiability of the 
parameters. Such a w; is sometimes called a tolerance variable since in a bioassay 
setting w; can be a lethal dose of a drug. 
For a special case where E* = lJ, and lJ is the J x J identity matrix, Albert and 
Chib (1993) fitted the Bayesian independent ordinal probit model using the Gibbs 
sampler. Even for this simple independent ordinal probit model, the primary Gibbs 
sampler considered in Albert and Chib (1993) may present challenging problems 
in achieving convergence. In view of that, Cowles (1996) provided an algorithm 
which substantially improves convergence not only for the probit model but also for 
the cumulative logit and complementary log-log link models. Recently, Nandram 
and Chen (1996) proposed an algorithm using reparameterization technique which 
improves convergence even further. For the above general SMMVN-link models, the 
computation is even more challenging since we face two difficult sampling problems, 
i.e., (i) generating cutpoints and (ii) generating correlation matrix. 
To ease the computational burden, we consider the following reparameterization: 
(4) 

136 
Chen & Dey 
for j = 1, 2, ... , J and i = 1, 2, ... , n. With reparameterization (4), the SMMVN-
link models given by (6) and (2) become 
}ij = l, if 'Yj,l-1 :5 Wij < ljl, 
(5) 
and 
(6) 
where the reparameterized cutpoints are -oo = 'YiO :5 'Yj1 = 0 :5 'Yi2 :5 · · · :5 
lj,L-1 = 1 :5 ljL = oo, E = (o-jj• ), O"jj = 6J, and O"jj• = DjDj•PJj• for j =/= j*. 
The models given by (4) and (5) are thus called the SMMVN-link reparameterized 
models. 
Notice that in (4), for each j, we have only L- 3 unknown cutpoints, and in 
(5), E is an unrestricted variance-covariance matrix, which has a great advantage 
in the implementation of MCMC sampling. We also note that reparameterization 
( 4) does not affect the distribution of the scale mixing variable A. That is, we still 
have the same mixing distribution 1r(A) for the mixing variable A. The SMMVN-
link reparameterized models have several attractive features. First, the number of 
unknown cutpoints is reduced by J. Second, all unknown cutpoints 'Yil are between 
0 and 1, i.e., 0 :5 ljl :51 for l = 2,3, ... ,L- 2 and j = 1,2, ... ,J. Third, the 
variance-covariance matrix E for Wi is unrestricted. Fourth, when L = 3, there 
are no unknown cutpoints. Due to these nice features, we use the SMMVN-link 
reparameterized models throughout the rest of this chapter. 
Finally, we note that the distribution of Wi determines the joint distribution of Yi 
through ( 4) and the variance-covariance matrix E captures the correlations among 
the Yij 's. More specifically, we have the joint distribution of the correlated ordinal 
responses given by 
P(Yil = Yil, Yi2 = Yi2, ... , YiJ = YiJI,8, E, ,, A, Xi) 
= L, i ..... L, (2.-I<(A)>/2 \1;\~ 
x exp { _[~<(A;]-l (w;- x;,8)'1;- 1(w;- x;,8)} dw;, 
(7) 
where 1 = ('YL'Y~, ... ,,j)', 'Yi = ('Yj2,'Yj3,···,'Yj,L-2)', and 
Aij = ('Yj,J-1, ljt] if Yii = l, for j = 1, 2, ... , J. 
(8) 
The class of SMMVN-link models is very rich and flexible, which include common 
MVP, MVT, MVL, MVS, and MVEP-link models. A brief explanation of such 
models is given as follows. 
Taking K(A) = 1 and the mixing distribution 1r( {1}) = 1, the SMMVN models 
reduce to the MVP models. Similar to the MVP models, when we take K(A) = 1/A 
and A "' Q(v /2, v /2), i.e., 
1 (V)
11
/ 2 v/2 1 { V 
} 
1r(A) = r (~) "2 
A 
-
exp - 2A , 
the SMMVN-link models give the MVT models. As a special case, the MVT links 
reduce to the multivariate Cauchy (MVC) link when v = 1 and the MVP when 
v-+ 00. 

8. Correlated Ordinal Data Models 
137 
Logit models are widely used to fit binary data (e.g., see Prentice 1988). As 
pointed out by Choy (1995), the SMMVN-link model leads to the MVL model when 
K(A) = 4A2 and A follows an asymptotic Kolmogorov distribution with density 
00 
7r(A) = 7rK(A) = 8 L:)-1)k+1k2Aexp{-2k2A2}. 
(9) 
k=1 
The MVL models are attractive since the exchangeability on the correlation struc-
ture is not required, which is advantageous compared to the random effects type of 
logistic regression models, for example, stratified and mixture models as given in 
Prentice (1988). 
A multivariate stable distribution can be obtained as a scale mixture of multi-
variate normal distributions with K(A) = 2A and the mixing distribution 7r(A) = 
Sp(o:, 1) where the density of the positive stable distribution Sp(o:, 1) in the polar 
form is given by 
7rsP(Aio:, 1) = _o:_A-L=o:+1) 1.
1 
s(u)exp {- s(~)} du, for 0 <a:< 1, 
(10) 
1- a: 
o 
A t-o: 
with s( u) = ( s!?n a7r:u ) 
(sin si~-71": 1ru ) • In our scenario, to obtain a heavy-tailed 
multivariate link model, we consider a symmetric multivariate stable (MVS) distri-
bution SJ(2o:, 0, xifi, E) for Wi, where the characteristic function of SJ(2o:, 0, xifi, E) 
on the natural log scale is given by 
In ¢(t) = i (xd1)'t- (t'Et)
0
, for a: E (1/2, 1), 
with t = (t 1 , ... , tJ )' and i2 = -1. Note that when a:= 1/2, SJ(1, 0, Xi/3, E) is the 
multivariate Cauchy distribution, while 
is a multivariate normal distribution. Therefore, MVC is a special case of MVT as 
well as a special case of MVS, and MVP is a limiting case of MVS. 
Exponential power family distributions play an important role in Bayesian mod-
eling, as indicated in Box and Tiao (1992), where they used a univariate exponential 
power family to model random effects in linear and nonlinear models. Formally, the 
density function of the multivariate exponential power family (MVEP) distribution 
has the form 
7rEP( Wi lxi/3, E, a:) 
= 
CJ IEI-1/ 2 exp {- [co(wi- Xif3)'E- 1(wi- Xi/3)] a}, 
(11) 
for 1/2 ::; a: ::; 1, where a: is called the kurtosis parameter and constants c0 and CJ 
are defined as 
r(23a) 
o:c~/2r(f) 
c0 = 
( 1 ) 
and 
CJ = 
(L) J 12 . 
r 2a 
r 2a 7r 
(12) 
We notice that a MVEP-link model is a special case of the SMMVN-link model 
with K(A) = 1/(2coA) and 7r(A) = (t)J/2 7rsP(Aio:, 1), where 7rsP(Aio:, 1) is defined 
in (10). Further, there are two interesting special cases of the MVEP distributions, 
that is, the multivariate normal (a: = 1) and the multivariate double exponential 
distribution (a: = 1/2). 

138 
Chen & Dey 
3. 
Prior Distributions and Posterior Computations 
In this section we present prior distributions for SMMVN-link models and develop 
algorithms to perform posterior computations for such models. 
3.1 
Prior Distributions 
First, we choose the same prior distribution for the regression coefficient vector 
(3 for all SMMVN-link models presented in Section 2. That is, 
(13) 
where Bo is a precision matrix, f3o is a location parameter vector, and both /3o and Bo 
are prespecified. Typically, we choose [JJ = 0 and Bo = diag (Bu, B12, ... , B1p, B21, B22, ... , 
B2p2, ... , BJJ,BJ2, ... ,BJp.t) where Bit is chosen to be small (e.g., Bjt = 0.01) so that a 
vague prior distribution for (3 is obtained, which ensures that the posterior is driven 
by the data. 
Second, we choose 
(14) 
where Qo is a J x J symmetric and positive definite matrix, WJ(no, Qo) denotes 
the Wishart distribution with degrees of freedom no and mean matrix noQo, and 
no and Qo are prespecified a priori. In our illustrative example, we take no = 11 
and Q0
1 = 0.001ho where ho is the 10-dimensional identity matrix, so that the 
prior is sufficiently diffuse. 
Third, we take independent uniform priors on lj = ('Yj2 1 'Yj3,···,'Yj,L-2)', i.e., 
7ru(lj) 
oc 
1, for 0::; lj2::; lj3::; · · ·::; lj,L-2::; 1, 
for j = 1,2, ... ,J. 
3.2 Posterior Computations 
(15) 
We use Gibbs sampling (e.g., Geman and Geman, 1984 and Gelfand and Smith, 
1990) along with the Metropolis algorithms within Gibbs steps to perform the pos-
terior computation. To sample from the posterior distribution, we need to generate 
(3, E, lj, Wi and Ai from their respective conditional distributions. The technical 
detail of computational implementation is given as follows. 
Let B = Bo + L:~=d~(.Xi)]-
1 x~E-
1 xi and 
P = B- 1 (Bof3o + t[~<(A,Jr 1x:E- 1w;) . 
s=l 
Also let w = (wi, w~, ... , w~)' and A= (.X1, .X2, ... , An)'. Then, given E, w, and A, 
we have 
(16) 
From (5) and (13), we have that the conditional distribution of E- 1 given (3, w, and 
A is a Wishart distribution, that is, 
E- 1 1{3,w,A,y 

8. Correlated Ordinal Data Models 
139 
(17) 
Therefore, generating (3 and E from (32) and (17) is straightforward. We notice that 
without reparameterization (4), we must draw the correlation matrix E* from its 
conditional posterior distribution based on (2). From Chen and Dey (1998), it can 
be seen that generating a correlation matrix is much more difficult than drawing a 
variance-covariance matrix from a Wishart distribution. 
To generate /j and w from their conditional distributions, the primary Gibbs 
sampler considered in Albert and Chib (1993) may present challenging problems 
in achieving convergence as discussed in Section 2. Therefore, we consider a more 
efficient MCMC sampling scheme as follows. Let W(j) = (wlj, w2j, ... , Wnj) 1 and 
let w(-j) denote w with W(j) deleted for j = 1,2, ... ,J. Then, we use a cycle of J 
Gibbs steps to generate /j and w(j) jointly from their conditional distributions for 
j = 1, 2, ... , J in turn. To draw /j and W(j) jointly from the conditional distribution 
hi, w<nlfi, E, w( -j), A, y], we first draw /j from [!i lfi, E, w(-j), A, y], and then draw 
W(j) from [w(j)l/j, (3, E, w(-j), y]. 
Given /j, {3, E, w(-j), and A, the conditional distribution of Wij, the ith compo-
nent of W(j), is a truncated normal over interval Aij given in (7) with the mean and 
variance given as follows: 
(18) 
and 
olj = K(Ai) (ujj- EjjE(_1j)EJj). 
(19) 
In (18) and (19), Wi( -j) is Wi with Wij deleted, Xi( -i) is Xi with the jth row deleted, 
fi( -j) is (3 with fii deleted, E( -j) is E with the jth row and jth column deleted, 
and En = ( 
Uj 1, ... , Uj,; -1, Uj ,j +b ... , Uj J). Therefore, we can use the algorithm of 
Geweke (1991) to generate Wij from the above truncated normal distribution for 
i = 1,2, ... ,n. 
It can be easily observed that given /j, (3, and E, w1j, w2j, ... , Wnj are indepen-
dent. Therefore, the conditional distribution [/j lfi, E, w( -j), A, y] is 
where Pii and O'ij are defined by (18) and (19). 
Generating /j from (20) is a challenging problem. Cowles (1996) proposed a 
Hastings scheme using a truncated normal distribution. By drawing /j simulta-
neously from its conditional distribution (20), Nandram and Chen (1996) devel-
oped an improved algorithm with a proposal density based on the Dirichlet dis-
tribution. However, the latter algorithm works well only when the cell counts, i.e., 

140 
Chen & Dey 
njt = I:?=1 1{t}(Yij ), where the indicator function 1{t}(Yii) = 1 if Yij =I and 0 oth-
erwise, are relatively balanced. More recently, Chen and Schmeiser (1998) suggested 
to use a nearly automatic algorithm, a random-direction interior-point (RDIP) ap-
proach, to generate /j. The RDIP requires the minimum input from a user, but it 
may not be very efficient due to the nature of black-box algorithms. 
In many correlated ordinal problems, the cell counts njt's may be unbalanced 
and some of them are possibly missing; see Section 5 for an example. Therefore, it 
is difficult to apply any of the aforementioned existing algorithms. To remedy this 
problem, we use a Metropolis-Hastings algorithm to generate /J from (20) using a 
transformation technique. Let 
-y· t-1 + e(;1 
lit = 
3
'1 + e(;l 
, I= 2, ... , L- 2 
(21) 
and (j = ((j2, ... , (j,L-2)'. Then, the conditional distribution [(J LB, E, w( -j), A, y] 
is 
(22) 
where 7r(!Jifi,E,w(-j),A,y) is given by (20) and /j is evaluated at /jl = (lj,l-1 + 
e(;1 )/(1 + e(;1 ) for I = 2, 3, ... , L- 2. Instead of directly generating /j from (20), 
we first generate (j from (22) and then use (21) to obtain /J. 
To generate (j, we use a multivariate normal proposal N((j, E(;), where (j is a 
maximizer of the logarithm of the right hand side of (22), which can be obtained 
by using the Nelder-Mead algorithm implemented by O'Neill (1971), and E(; is the 
minus inverse of Hessian matrix of (j, that is, 
Then, following Hastings (1970), our algorithm to generate (j operates as follows: 
Step 1. Let (j be the current value. 
Step 2. Generate a proposal value (J from N((j, E()· 
Step 3. A move from (j to (j is made with probability 
. { 7r((j IP, E, W( -j), A, y) exp 
mm 
7r((J IP, E, W( -j), A, y) exp 
A 
A 
1 
A 
~((J - (j )'E(; ((J - (j 
The novelty of this Metropolis-Hastings algorithm is that (a) unlike the other 
existing algorithms, the parameters in the multivariate normal proposal are speci-
fied within each Gibbs-step in an automatic fashion; (b) the proposal distribution 
roughly has the same shape as the true conditional distribution 7r((J IP, E, w( -j), A, y); 
(c) this new algorithm no longer requires the cell counts nit balanced. Therefore, 
our algorithm is more advantageous and efficient than the existing ones. 

8. Correlated Ordinal Data Models 
141 
The conditional distribution [.Xdfi, E, Wi, y] is 
(23) 
To generate Ai from (23), we need to know the form of the mixing distribution 1r(.X). 
Therefore, we present the random generation algorithms for a few special cases of 
SMMVN-link models. 
For an MVP model, it does not require generating Ai, since 1r({.Xi = 1}) = 1. For 
an MVT model, 7r(.Xd,8, E, Wi, y) in (23) reduces to 
where Q(u, v) denotes a gamma distribution with density 7ro(.XIu, v) ex: _xu-le-v( 
Therefore, sampling Ai from its conditional distribution is trivial. 
For an MVL model, using an appropriate Student t approximation to the logistic 
distribution, Chen and Dey (1998) discovered that a good proposal density for 
1rx(.X) given by (9) is 
(24) 
They further showed that the best choices of v and b are v = 5 and b = . 712 and 
they also provided an efficient way to evaluate the infinite series of 1rx(.X). It is 
interesting to mention that when we take 
2 
(ll 
lJ ) 
.X 
'""" 
I Q 2' 8b2 
, 
where IQ(u, v) is an inverse gamma distribution with pdf 7rzo(.XIu, v) = r(u):u+i e-vf.\, 
A> 0, then 
· 
A '""" 
9£(-XIv, b). 
Therefore, to draw Ai from (23), we can use the Metropolis sampling scheme. Let 
Ai be the current value. Generate 
Then, a move to the proposal point .X£ is made with probability 
• 
{ 1r K (.Xi)/ g L (.X£ lv, b) 1 } 
mm 
7rK(Ai)/gL(.Xdv, b)' 
' 
(26) 
where 7rK(.X) and 9£(Ailv, b) are given in (9) and (24) respectively. 
For an MVS-link model, to draw Ai from (23), Choy (1995) proposed the Sam-
pling/Importance Resampling (SIR) method (Tanner, 1996), and the generalized 
Ratio-of-Uniform algorithm (Wakefield, Gelfand, and Smith, 1991), while Chen and 

142 
Chen & Dey 
Dey (1998) developed the Metropolis algorithm (Metropolis eta/., 1953) with an in-
verse gamma proposal distribution. Here, we present only the Metropolis algorithm. 
Letting Ai be the current value, we draw 
At ~ TQ ( J; 1, ~ [(w;- x;/3)'E- 1(w;- x;/3) + 1]) . 
Then, a move to the proposal point Ai is made with probability 
. { 7rsP(Ai Ia, 1)/7rzg(Ai 11/2, 1/4) 1} 
mm 
7rsP(Ada, 1)/7rzg(Ad1/2, 1/4)' 
· 
In the above Metropolis scheme, the full proposal density is proportional to 
( 4 .. :..;)-f lEI-! exp {-4~
1 
( w; - x;;3)'E- 1(w;- x;/3)} .. za(A;I1/2, 1/4). 
(27) 
(28) 
(29) 
The proposal distribution given in (29) has heavier tails than the conditional dis-
tribution of Ai and works well when a is not far away from 1/2. 
Similar to the MVS-link model, we use a Metropolis algorithm with an inverse 
Gaussian proposal to generate Ai for an MVEP-link model. Letting Ai be the current 
value, we draw 
where 
PEP= (4co(wi- Xifi)'E- 1(wi- Xifi))- 112 and u'EP = ~, 
and the density of the inverse Gaussian distribution, IN(p*, u*), is 
* * 
~ { 
u* (A - p* )
2 
} 
7rz.Af(AIJ.t , u ) = y ~ 
exp 
2p* 2 A 
, for A> 0, 
(30) 
with parameters p* > 0 and u* > 0. Then, a move to the proposal point Ai is made 
with probability 
. { 7rsP(Ai Ia, 1)/7rzg(Ai 11/2, 1/4) 1} 
mm 7rsP(Ada, 1)/7rzg(Ad1/2, 1/4)' 
· 
(31) 
In the Metropolis algorithm, the full proposal density is proportional to 
( c~;) 
112
1EI-1/ 2 exp { -c0 A;(w;- x;;3)'E- 1(w;- x;/3)} 
x (Lr
12 
.. xa(Ad1/2, 1/4), 
which exactly matches with an inverse Gaussian distribution. Finally, we mention 
that an elegant algorithm given in Devroye (1986, p 148) can be used to generate 
A,...., IN(JJE:p, o-E:p)· 
4. 
Model Determination 
Once we have accomplished the first two steps of Bayesian analysis, i.e., construct-
ing probability models and computing the posterior distributions of all parameters 
of interest, using a sampling-based approach, it is natural to compare several pro-
posed models. It is also important to assess the fit of the selected models to the 
data and to own substantive knowledge. A good Bayesian analysis should always 
include at least some model comparisons along with some model diagnostics. 

8. Correlated Ordinal Data Models 
143 
4.1 
Model Comparisons 
In this subsection we consider the problem of accounting for uncertainty about 
model form. Here we are faced with many models within a class of SMMVN-link 
models. Although we may wish to summarize our findings with a single model, 
there are usually many choices to be made. In this context, we consider marginal 
likelihood approach (Chib and Greenberg, 1998) for model comparisons since this 
approach is particularly suitable for the correlated ordinal data models. 
Suppose that there are m models Mt, Mz, ... , Mm in our consideration. For 
model Mi, we let 1r(/3, :E-1, 1 I y, Mi) denote the posterior distribution, which is 
( a :E-1 
I M·)- L(y I /3,:E,I,Mi)7r(/3,:E-1,1) 
7r ,_,, 
' 1 
y, 
~ -
m(yiMi) 
' 
(32) 
where 1 == ( 1i, 12, ... , IJ )'. In (32) the likelihood is 
n 
L(y I /3, :E, I, Mi) == IT L(Yk I /3, E, I, Mi) 
k=1 
and 
where ~i(Ak) and 1ri(Ak) are the scale mixing function and the density function 
of the mixing variable Aj associated with model Mi, while the Akj's are defined 
in (7) based on the observed ordinal responses Ykj. Furthermore, in (32) the prior 
distribution, 1r(/3, E-1, 1), is given by (11), (13), and (15), which is the same across 
all SMMVN-link models, and m(y I Mi) is the marginal likelihood. 
To compare different SMMVN-link models, we calculate the marginal likelihoods 
for each of the models. In fact, it is in the same spirit to use the Bayes factor to 
compare two models (see, Kass and Raftery, 1995), which can be seen through the 
following identity for comparing models Mi and Mi•: 
Bii• == exp {In (m(y I Mi)) -In (m(y I Mi• ))} , 
where Bii• is the Bayes factor. We choose the model which yields the largest 
marginal likelihood m(y I Mi)· 
To estimate the marginal likelihood, we adopt a data-augmentation-based method 
of Chib (1995). Given some point (/3"', I;*, 1"') (typically the posterior means of /3, 
:E, and 1), we have an identity for the marginal likelihood on the natural log scale 
as 
In m(y I Mi) 
n 
== L lnL(yk I /3"', :E"', 1"', Mi) 
k=1 
+ln7r(/3"',:E"'-1,1"') -ln7r(/3"',:E"'-1,1"' I y,Mi)· 
(33) 
To estimate ln m(y I Mi), we need to estimate the first term and the third term of 
(24). Following Chib and Greenberg (1998), we write 
ln1r(/3"',E"'-1,1"' I y,Mi) 

144 
Chen & Dey 
= 
ln1r(,B* I :E*,I*,y,Mi) +ln1r(:E*-1 l1*,y,Mi)+ln7r(l* I y,Mi). 
(34) 
In (34), 
7r(,B* I :E*, I*, y, Mi) = J 
7r(,B* I :E*, I*, w, A, y, Mi) 
X7r(w,A I :E*,I*,y,Mi)dwdA, 
(35) 
1r( w, A I :E*, 1*, y, Mi) is the conditional marginal posterior distribution of w = 
( WlJ w2, ... , Wn)' and A= ( AlJ A2, ... , An)' given :E = :E* and 1 = 1*, 
1r(:E*-1 ll*,y,Mi) = j 1r(:E*-1 I ,B,w,A,I*,y,M;,) 
X1r(,B, w, A II*, y, Mi)d,BdwdA, 
(36) 
where 1r(,B, w, A II*, y, Mi) is the conditional marginal posterior distribution of ,B, 
w, and A given 1 = 1*, and 
1r( 1* I y, Mi) = j 1r( 1* I ,B, :E, w, A, y, Mi) 
X1r(,B, :E-1, w, A I y, Mi),Bd:E- 1dwdA, 
(37) 
where 1r(,B, :E-1, w, A I y, Mi) is the marginal posterior distribution of ,B, I;- 1, w, 
and A. 
To obtain simulation-consistent estimates of (35), (36), and (37), we indepen-
dently generate {(Wl(r),A,l<r>), r = 1,2, ... ,R} from 1t(W, A. I}:*, y*, y, Mi),{/3_g(r),w_g(r), A.2<r>), r = 
1, 2, ... , R} from 1r(,B, w, A II*, y, Mi), and { (,B~r), :E~r), w~r), A~r)), r = 1, 2, ... , R} 
from 1r(,B, :E-1, w, A I y, Mi). Note that all three MCMC outputs are easy to ob-
tain by adopting the MCMC sampling algorithms presented in Section 3.2. Then, 
a simulation-consistent estimate of (35) is 
R 
ir(,B* I :E*,I*,y,Mi)= ~L1r(,B* I :E*,I*,w~r),A~r),y,Mi)· 
(38) 
r=1 
Note that in (38), 
7r(,B* I :E*,I*,w~r),A~r),y,Mi) 
( 
1 )p/2 
{ (,B* _ jJ(r))' B(r)(,B* _ p(r))} 
= 
27r 
IB(r)l1/2 exp 
2 
, 
and B(r) = Bo + 2:~= 1 (~i:i(A1rt)]-
1 x~(:E*)-
1 Xk. A simulation-consistent estimate of 
(36) is 
R 
~("'*-1 I * 
M) 1""" ("'*-1 I a(r) 
(r) ,(r) 
* 
M) 
7r ,l.J 
I , y, 
i = R L....J 7r ,l.J 
~-'2 , W2 , "'2 , I , y, 
i · 
r=1 
(39) 

8. Correlated Ordinal Data Models 
145 
In (39), 
where 
Assume that all sets {i* : 
Yi•j = I, i* = 1, 2, ... , n} for l = 2, 3, ... , L- 1 and 
j = 1, 2, ... , J are not empty. Under the above assumption, a simulation-consistent 
estimate of (37), is given by 
R 
~ ( * I M ) 
1 '""" ( * I a( r) "'( r) 
( r) ' ( r) 
M ) 
7r I 
y, 
i = R L..t 7r I 
tJ3 
, L.13 
, w3 , "'3 
, y, 
i , 
r=1 
(40) 
where 
7r( I* I f3~r)' I;~r)' W~r)' A~r)' y, Mi) 
J L-2 
1 
!1 D. min{wj;J3 : Yi•J =I+ 1}- max{wj;J3 : y;•; =I} 
(
41
) 
for max{ w~:] 3
: Yi•j =I}< 1]1 ~min{ w~:] 3
: Yi•j =I+ 1}, I= 2, 3, •.. , L- 2, and 
j = 1, 2, ... , J. Note that the derivation of equation ( 41) follows from the fact that 
if the assumption, 
max{w~:] 3
: Yi•j =I}< min{w~:) 3
: Yi•j =I+ 1}, 
for I= 2,3, ... ,L- 2 and j = 1,2, ... ,J, holds, the cutpoints Iii's are indepen-
dent. If the above assumption is violated, which is rare in practice, ( 41) still works 
with an obvious adjustment. However, if L or J is large, the above Monte Carlo 
approach may not be efficient because of high dimensionality of the problem. A 
more efficient Monte Carlo method can be obtained by using a sequence of J - 3 
dimensional conditional marginal distributions for I· To explore this point, we let 
I(+J) = ( 1i', 12', ... , 1;')' for j = 1, 2, ... , J. Then, we have 
7r( I* I y, Mi) = 7r( li I y, Mi) 
X7r(l2 I 1(+1)'Y,Mi)"·1r(li I 1(+(J-1))'Y,Mi), 
(42) 
and 
1r(1] I 1(+(J-1))'Y,Mi) = j 1r(I]I/3,E,w,A,I(+u-1))'Y'Mi) 
X1r(/3, E-1, w, AII(+U-1))' y, Mi)df3dE- 1dwdA, 
(43) 
for j = 1,2, ... ,J. Then, similar to (40) and (41), an efficient estimate of1t (yj*IY*(+G-l))•Mi) 
can be obtained by using a random sample generated from 7t(p,:E,w,A I C*( *IY*(+G-
t)),Mi). 
Next, we discuss a Monte Carlo method to estimate the probability L{y~t I p*,:E*,y*, Mi). 
Note that Monte Carlo algorithms proposed by Chib and Greenberg {1998) and 

146 
Chen & Dey 
Chen and Dey (1998) for correlated binary response data problems may not be ap-
plicable here because of simulation inefficiency of their algorithms in high dimension. 
Let :E;t :::: diag(:E*), CL == L(yk I ,B*, :E*, /*, Mi), and c£ == L(yk I ,B*, :E;t, 1* ~Mi)· 
Then, c£ can be evaluated numerically, which may involve a one-dimensional inte-
gral. Letting 
we have 
(44) 
where the expectation is taken with respect to 1r( Wk, A I ,B*, :E;t, y, Mi), which is 
proportional to 1r*(wk, A I ,B*, :E;j, y, Mi)· Then, we use the following steps to obtain 
an estimate of L(yj I ,B*, :E*, /*, Mi): 
Step 1. Generate ( w~r), A(r)) from 1r( Wk, A I ,B*, :E;j, y, Mi) using the Gibbs sampler 
for r = 1, ... , R. The necessary steps required in Gibbs sampling are: 
(i) generate w1r) I A(r-l) 
I"V N (xk,B*, Ki(A1r-l)):E;j) over the constrained 
space AA:t x AA; 2 x · · · X AA;J, 
(ii) generate A(r) from (A I w~r)] using a procedure presented in Section 3.2.3. 
Step 2. Calculate the average 
R"' _ _!:_ ~ 
1r( W~r), A(r) I ,8*, :E*, y, Mi) 
a-
L.....t 
() 
, 
R r=l 1r( w/ , A(r) I ,8*, E;t, y, Mi) 
(45) 
and compute ln L(Yk I ,B*, :E*, 1*, Mi) :::: ln c£ + ln Ra. 
Note that the above approach is indeed the one for estimating ratios of two nor-
malizing constants (see, e.g., Meng and Wong, 1996 and Chen and Shao, 1997). 
Finally we comment that as we can see from the above, computing the marginal 
likelihoods for the correlated ordinal data problems is very different from the one for 
the correlated binary data problems. We also comment that as empirically shown 
by Chen and Dey (1998), the methods that use ratios of normalizing constants (see, 
e.g., Meng and Wong, 1996 and Chen and Shao, 1997) to estimate directly the 
Bayes factors are not efficient for the SMMVN-link models. This is partially due to 
the fact that the posterior distributions in the class of the SMMVN-link models are 
relatively far apart from each other. Therefore, the above proposed methods will 
greatly gain precision of the Monte Carlo estimation for computing the marginal 
likelihoods. 
4. 2 
Model Diagnostics 
In the earlier sections we presented a collection of models and proceeded with 
Bayesian inference. The issue at stake is which model is most adequate for the 
given correlated ordinal data as it is well-known that an inadequate model could 

8. Correlated Ordinal Data Models 
147 
lead to a misleading conclusion. In classical statistics, goodness of fit tests have 
been employed to check the plausibility of the model fit to the data. The classical 
goodness of fit test quantifies the extremeness of a particular discrepancy measure 
by calculating an accumulated tail probability that the model under a specified null 
hypothesis is true. In the classical setup the test statistic and hence the accumulated 
tail probability is a function of both the data and the unknown parameters which are 
specified only under the null hypothesis. As an alternative to the classical goodness-
of-fit test, we develop two Bayesian model checking methods for the correlated 
ordinal data. 
Since we use a class of SMMVN-link models for the correlated ordinal responses 
in a unified fashion, our first approach for model diagnostic is to check the appropri-
ateness of the link function at the second stage of the hierarchical model. Recall that 
in the general structure of the SMMVN-link models, the prior distribution of Ai for 
the ith observation forms the desired link. Thus the appropriateness of the link at 
each observation i, i = 1, 2, ... , n, can be checked by comparing the prior distribu-
tion versus the posterior distribution of Ai 's. Such comparison will detect outlying 
group for a given SMMVN-link model. For example, the MVT models correspond 
to a nuisance parameter Ai from a gamma distribution with prior mean one. Thus, 
if the posterior mean of Ai is quite smaller than one, it indicates variance inflation of 
the normal link. As mentioned in Vounatsou, Smith, and Choy (1996), the posterior 
means of Ai may not exist. In this scenario, we use transformed variables 1/Ji = ln Ai, 
i = 1, 2, ... , n, instead. 
Since the Monte Carlo samples of Ai 's from their posteriors are readily available 
from the MCMC outputs, we can simply calculate 1/Ji and compare the posterior 
and prior distributions of 1/Ji = ln Ai through several quantiles. In spirit, this is 
essentially an exploratory data analysis approach of model checking as indicated in 
Dey, Gelfand, Swartz, and Vlachos (1998). We denote the respective 5-quantiles of 
the prior and posterior distributions of 1/Ji to be 
pr 
( pr 
pr 
pr 
pr 
pr 
)' 
q>.. = q>.,,0.05' q>.,,0.25' q>.,,0.5' q>.,,0.75' q>.,,0.95 
and 
po 
pr 
po 
po 
po 
po 
1 
q>., = (q>.,,0.05' q>.,,0.25' q>.,,0.5' q>.,,0.75' q>.,,0.95) · 
Note that qf~ can be obtained analytically or by using Monte Carlo samples from 
their corresponding distributions while qf~ can be computed by using readily avail-
able Monte Carlo samples from the Gibbs outputs. Now, if individual observations 
are of interest, we define the following observational level discrepancy measure: 
. -11 po 
prll2 
. _ 
D~ -
q>., -
q>., 
, for z- 1, 2, ... , n. 
(46) 
Due to the complexity of SMMVN-link models, it does not appear possible to ana-
lytically derive the distribution of di. Therefore, it is difficult to find a cutoff point 
for Di that discriminates between "good" observation and aberrant value. To over-
come such difficulty, we consider the standardized discrepancy measures of the Di 's. 
Let D = (1/n) 2:?=1 Di and let S(D) be the sample standard deviation of the Di 's. 
Then, we define the standardized observational level discrepancy measure: 
Di -15 
di = S(D) 
(47) 
for i = 1, 2, ... , n. Thus, when ldil > k* (typically, we choose k* = 3), the ith 
observation will be viewed as aberrant. Finally, we notice that since the MVP-link 

148 
Chen & Dey 
model is embedded within MVT-link model, one can choose a large value of the 
degrees of freedom v for the gamma distribution of Ai and do similar comparisons. 
Similar argument works if one thinks that the MVP is also embedded within stable 
family or double exponential family. 
For the purposes of checking the model adequacy and detecting outliers at the 
component level, we use Bayesian latent residuals. By generalizing the univariate 
Bayesian residuals of Albert and Chib (1995), we introduce our latent residuals as 
W" -p·· 
r: •• _ 
ZJ 
ZJ 
'-ZJ-
I 
(J'ij 
(48) 
where J.lij = E(wiiiY) and (J'tj = Var(wiiiY), that is, J.lij and (J'tj are the posterior 
mean and posterior variance of Wij, for j = 1, 2, ... , J, i = 1, 2, ... , n. Note that 
J.lij and (J'ij can be simply calculated by using the readily available Monte Carlo 
samples of Wij 's from the Gibbs outputs. Therefore, no additional MCMC samples 
are needed in order to obtain the latent residuals tij 's. 
Based on the above Bayesian latent residuals, we can use similar tools such as 
boxplots of the posterior distributions of the tij 's to assess the model fitting and 
to detect componentwise outliers. Instead of overlaying posterior boxplots for each 
component considered in Albert and Chib (1995), we suggest to calculate P(ltijl ~ 
/{* I y) for each component and plot P(ltij I ~ I<* I y) versus E(Ynew,ij IY) where the 
expectation is taken with respect to the posterior predictive distribution 1r(Ynew IY). 
Note that to obtain an efficient Monte Carlo estimate of E(Ynew,ij IY), we can use 
the following identity 
E(Ynew,ij IY) = E [E(Ynew,ij IXij, /3j, E, /j, Y)] , 
where the first expectation is taken with respect to the posterior distribution 1r(,Bj, E, ~~ 
and the second expectation is 
(49) 
Note that in ( 49), we denote <I> (?J -Xiif3i) to be 1 when I = L and 0 when I = 
"'(>-.)oii 
0. Several different values of [{*, e.g., /{"' =1,2,3, can be used. We feel that our 
residual plots are more effective because of high dimensionality of the problem. 
Here, we do not consider a multivariate version of Bayesian residuals since the 
multivariate Bayesian residuals suffer loss of identification, making the component 
level interpretation difficult. 
5. 
Item Response Data Example 
The Department of Mathematical Sciences at Worcester Polytechnic Institute 
(WPI) recently conducted a survey. The results from the survey were to be used in 
the renovation of the Master's degree program for secondary teachers. One survey 
question contains ten features (items) of Master's degree programs for secondary 
mathematics teachers and the teachers were asked to identify which features are 

8. Correlated Ordinal Data Models 
149 
TABLE 8.1. Summary of the Data 
Items 
Group 
Response 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
1 
1 
2 
4 
4 
1 
1 
-
1 
1 
2 
2 
-
-
12 
10 
2 
2 
8 
2 
1 
5 
I 
3 
9 
3 
36 
34 
13 
7 
16 
13 
9 
21 
4 
26 
25 
17 
20 
31 
30 
30 
33 
35 
33 
5 
40 
46 
7 
8 
29 
36 
22 
27 
30 
15 
1 
-
-
3 
12 
2 
-
5 
5 
4 
4 
2 
-
2 
6 
8 
5 
-
5 
3 
7 
10 
II 
3 
-
9 
21 
13 
7 
6 
13 
10 
7 
16 
4 
12 
13 
13 
16 
25 
11 
18 
19 
15 
15 
5 
39 
27 
8 
2 
12 
34 
10 
14 
18 
6 
important. Each individual responded in one of "not important/' "somewhat im-
portant," "average important," "important," and "very important" for each item 
(feature). The subjects included teachers from two groups: I. Prospective Students, 
and II. Students who have been part of the WPI program. Prospective Students 
were defined to be one faculty member (usually the department head) from every 
high school mathematics department within a sixty-mile radius of WPI. The sur-
vey was sent to 315 secondary mathematics teachers in Massachusetts in November 
1993 and completed surveys were received from 127 teachers. 
A summary of the data is given in Table 8.1. Note that in Table 8.1, the values 
of the response 1, 2, 3, 4, and 5 correspond "not important," "somewhat impor-
tant", "average important," "important," and "very important," respectively, and 
each entry represents the count. See Ganter, Paulauskas, and Chen (1995) for the 
complete information about this survey. We use this example to demonstrate the 
computational feasibility of the methodologies described in Sections 2 to 4 as well 
as to illustrate how SMMVN-link models can be applied to a real item response 
data problem. 
Let Yii be the response of the jth item from the ith individual and let {3j k to 
denote the intercept for the jth item within group k where k = 1 and k = 2 
correspond to Group I and Group II, respectively. To assess the importance of each 
item, we consider the following mean-ranking measure 
Pik = J.oo tl [4> ( /il- f3ik) - 4> (/i,l-1- f3ik)] 7r(A)dA 
(50) 
0 
1:::1 
V"'(A)O'jj 
V"'(A)O'jj 
for j = 1,2, ... , 10 and k = 1,2. 
For illustrative purposes, we consider three SMMVN-link models to fit the WPI 
survey data. These models are the MVP, MVL, and MVC (i.e., MVT with v = 1). 
These models capture different aspects and features of the SMMVN-link models. 
For example, the MVP and the MVC correspond to the lightest and the heaviest 
tails respectively, while the MVL model is roughly in the "halfway" between the 
MVP and MVC models. In the implementation of the Gibbs sampler, we use the 
Metropolis-Hastings algorithm to generate the cutpoints li in the Metropolis step. 
Note that the NC algorithm cannot be applied in this example because the counts nil 

150 
Chen & Dey 
TABLE 8.2. Bayesian Estimates of the Mean-Ranking Measures J.tjk (Group I): Part (a) 
Item 
Model 
Posterior 
Posterior 
95% HPD 
Mean 
Std Dev 
Interval 
1 
MVP 
4.348 
0.084 
(4.180, 4.508) 
MVL 
4.343 
0.083 
( 4.172, 4.500) 
MVC 
4.369 
0.091 
(4.188, 4.535) 
2 
MVP 
4.465 
0.090 
( 4.283, 4.628) 
MVL 
4.485 
0.081 
( 4.319, 4.634) 
MVC 
4.556 
0.077 
( 4.406, 4.704) 
3 
MVP 
3.146 
0.116 
(2.922, 3.378) 
MVL 
3.192 
0.109 
(2.964, 3.395) 
MVC 
3.257 
0.089 
(3.086, 3.432) 
4 
MVP 
3.227 
0.127 
(2.984, 3.479) 
MVL 
3.208 
0.119 
(2.970, 3.435) 
MVC 
3.255 
0.099 
(3.072, 3.453) 
5 
MVP 
4.098 
0.104 
(3.899, 4.304) 
MVL 
4.127 
0.095 
(3.933, 4.301) 
MVC 
4.200 
0.090 
( 4.030, 4.375) 
are very unbalanced and indeed we have few missing cells (see Table 8.1). Also note 
that this Metropolis-Hastings algorithm works quite well and this algorithm results 
in an acceptance probability of approximately 87% for the MVP model, 86% for the 
MVL model, and 85.5% for the MVC model. We check the convergence of the Gibbs 
sampler using several diagnostic procedures as recommended by Cowles and Carlin 
(1996). After convergence, we generate a large number of Gibbs iterates for various 
Bayesian calculations. It is noteworthy that after the convergence, we calculate the 
autocorrelations for all model parameters and we find that the autocorrelations for 
all f3Jk 's and Jlik 's disappear at lag 5 while the autocorrelations for all cutpoints 
/jl 's disappear at lag 10. 
First, using 50,000 Gibbs iterates after convergence, we compute the posterior 
estimates and 95% highest posterior density (HPD) intervals for the mean-ranking 
measures J-ti k 's for all three models. 

8. Correlated Ordinal Data Models 
151 
TABLE 8.3. Bayesian Estimates of the Mean-Ranking Measures J.tjk (Group 1): Part (b) 
Item 
Model 
Posterior 
Posterior 
95% HPD 
Mean 
Std Dev 
Interval 
6 
MVP 
4.273 
0.098 
( 4.079, 4.462) 
MVL 
4.281 
0.092 
( 4.096, 4.453) 
MVC 
4.298 
0.096 
(4.112, 4.480) 
7 
MVP 
3.852 
0.118 
(3.611, 4.074) 
MVL 
3.807 
0.113 
(3.588, 4.023) 
MVC 
3.813 
0.094 
(3.627, 3.995) 
8 
MVP 
4.050 
0.113 
(3.825, 4.267) 
MVL 
4.023 
0.110 
(3.809, 4.236) 
MVC 
4.028 
0.095 
(3.839, 4.217) 
9 
MVP 
4.159 
0.112 
(3.931, 4.366) 
MVL 
4.156 
0.107 
(3.936, 4.352) 
MVC 
4.168 
0.097 
(3.982, 4.360) 
10 
MVP 
3.705 
0.116 
(3.479, 3.930) 
MVL 
3.693 
0.106 
(3.486, 3.899) 
MVC 
3.689 
0.088 
(3.520, 3.860) 

152 
Chen & Dey 
TABLE 8.4. Bayesian Estimates of the Mean-Ranking Measures J.tik (Group II): Part (a) 
Item 
Model 
Posterior 
Posterior 
95% HPD 
Mean 
Std Dev 
Interval 
1 
MVP 
4.748 
0.068 
( 4.615, 4.876) 
MVL 
4.741 
0.070 
(4.597, 4.866) 
MVC 
4.699 
0.074 
(4.541, 4.820) 
2 
MVP 
4.264 
0.126 
( 4.013, 4.496) 
MVL 
4.318 
0.118 
( 4.084, 4.545) 
MVC 
4.444 
0.122 
( 4.203, 4.670) 
3 
MVP 
3.331 
0.142 
(3.051, 3.607) 
MVL 
3.341 
0.142 
(3.341, 3.625) 
MVC 
3.380 
0.147 
(3.107, 3.669) 
4 
MVP 
2.755 
0.160 
(2.437, 3.057) 
MVL 
2.897 
0.161 
(2.582, 3.208) 
MVC 
3.185 
0.137 
(2.930, 3.472) 
5 
MVP 
3.774 
0.141 
(3.500, 4.056) 
MVL 
3.863 
0.132 
(3.602, 4.123) 
MVC 
4.005 
0.103 
(3.811, 4.214) 
From the results are presented in Tables 8.2 to 8.5, it can be seen that the results 
are not sensitive to the choice of link functions. Based on the 95% HPD intervals 
of the J-ti k 's, the patterns that can be seen throughout the two different groups are 
that Items 1, 2, and 6 are always ranked as the top three features, although not nec-
essarily in the same order. Note that these three features are Broaden mathematical 
thinking (Item 1 ), Connect mathematics to other disciplines and real word problem 
solving (Item 2), and Increase mathematical proficiency (Item 6). Similarly, Items 
5, 7, 8 and 9 are ranked in the middle of the ordering, while Items 3, 4, and 10 are 
ranked at the bottom. 
Second, for the model diagnostics, we plot posterior probabilities P(lfijl 2:: I<"' I y) 
of the absolute values of standardized Bayesian residuals greater than or equal to /{"' 
versus E(Ynew,ij IY) for all three models under consideration. Figure 8.1 gives these 
plots for K* = 2. Several other values of K"' are also tried, but the corresponding 
plots are not presented here due to the nature of similarity. 
From Figure 8.1, it can been seen that all three models fit the data fairly well 
and no aberrant features have been found. 
Third, we calculate the posterior estimate of the covariance matrix I; and find 
that the equicorrelation assumption on the correlation structure is questionable. For 
example, the correlations between Item 1 and the other items are not significant 
because all HPD intervals contain 0 for all three models. However, some other cor-
relations are strongly significant. For example, the HPD intervals for the correlation 
between Item 8 and Item 9 are (.575, .810), (.586, .824), and (.585, .840) for the 
MVP, MVL, and MVC models, respectively. It is interesting to mention that this 

8. Correlated Ordinal Data Models 
153 
TABLE 8.5. Bayesian Estimates of the Mean-Ranking Measures P,jk (Group II): Part (b) 
Item 
Model 
Posterior 
Posterior 
95% HPD 
Mean 
Std Dev 
Interval 
6 
MVP 
4.538 
0.098 
(4.354, 4.731) 
MVL 
4.453 
0.099 
( 4.343, 4. 722) 
MVC 
4.531 
0.108 
( 4.311' 4. 723) 
7 
MVP 
3.439 
0.156 
(3.146, 3. 758) 
MVL 
3.549 
0.153 
(3.254, 3.846) 
MVC 
3.759 
0.137 
(3.482, 4.024) 
8 
MVP 
3.683 
0.160 
(3.363, 3.988) 
MVL 
3.796 
0.154 
(3.494, 4.095) 
MVC 
4.025 
0.127 
(3.774, 4.272) 
9 
MVP 
3.725 
0.161 
(3.409, 4.035) 
MVL 
3.839 
0.153 
(3.528, 4.127) 
MVC 
4.128 
0.136 
(3.869, 4.393) 
10 
MVP 
3.190 
0.153 
(2.883, 3.480) 
MVL 
3.272 
0.143 
(2.996, 3.550) 
MVC 
3.401 
0.134 
(3.157) 3.669) 

154 
Chen & Dey 
MVP Model 
0.06 
p 
0.06 
•u 
iii I 
: • I I I ! i 
.. 
• •• 
iii 
0 
:i 
:il 1 ; 
b 
o.cw. 
a 
b 
. '! 
IJ: ; ! 
i I : 
l 
t 
I 
0.08 
iii 
. I 
i 
~ ~ 
I 
~ 
I 
. ; I I 
:•• = ... 
: ; ~ 
.. 
o.oa 
r· I 
;i : 
I 
1 •• 
I 
i!: : i 
. i ~ 
.. 
:: 
·r: • 
. 0.01 
I : • 
.. 
~-
. 
o.oo 
a.cs 
8.0 
a.cs 
4..0 
4..CS 
cs.o 
Jll(Yne'W') 
MVL Model 
0.06 
p 
0.06 
! I I 
I 
!!! 
i 
II 
•• 1 I 
~ 
.. 
I 
0 
.. I 
lt. 
b 
o.cw. 
•t 
I 
"' 
a 
~ : : 
: 
i 
1·1 
i : 
b 
!I .. 
I 
o.08 
I ; i 
I 
• ,: 
I• 
! t 
I 
t 
• t 
• 
I 
: 
:: 
t. 
o.oa 
; 1*i 
.. 
I 
•• 
.. 
~ f· 
. 0.01 
.. . : 
o.oo 
a.cs 
8.0 
a.cs 
4..0 
4..CS 
cs.o 
lll(Yne'W') 
MVC Model 
0.06 
p 
0.06 
li .. 
:. i 
IIi 
• 
.. 
I il 
I i I 
~ 
II i I 
0 
b 
o.cw. 
a 
! I I 
• al 
b 
1: 
' I: . . . 
I 
0.08 
. 
I 
1 
~~ 
I : I 
I 
! .. 
t. 
o.oa 
·= 
f 
I 
:: 
.. • 
0.01 
.. 
. . 
.... 
0 
• 
.. . a 
o.oo 
a.cs 
8.0 
a.cs 
4..0 
4..CS 
cs.o 
Jll(Yne'W') 
FIGURE 8.1. Plots of Posterior Probabilities P(l£ijl 2: K*ly) versus E(Ynew,ijiY) for the 
MVP, MVL, and MVC models where the symbols • (dot),* (star), o (circle), A (triangle), 
and o (square) correspond to ordinal responses Yij = 1 to Yij = 5 

8. Correlated Ordinal Data Models 
155 
high correlation suggests a possible dimension reduction in survey sampling design. 
When we examine Item 8 and Item 9, we find that Item 8 is Introduce modern 
teaching techniques and program assessment while Item 9 is Present ideas and ap-
plications for use in secondary school classrooms, e.g., hands-on and group learning 
activities. Clearly, Items 8 and 9 are nested with each other, which is the main 
reason why we obtain such a high correlation between these two items. This finding 
also suggests that we either combine these two questions as one item or eliminate 
one of them. 
Finally, we compute the marginal likelihoods for all three models. To obtain 
simulation-consistent estimates of the marginal likelihoods, the Monte Carlo sam-
ple sizes in (38), (39), ( 40), and ( 45) were taken to be R = 10, 000. To obtain 
11'(-y* I y,Mi), we use (42) instead of (41) since J XL= 50 is relatively large. We 
find that the Monte Carlo method given in Section 4.1.1 works well in this example. 
Furthermore, we use a procedure provided by Chib (1995) to compute the simula-
tion standard errors for marginal likelihood estimates. The estimated In m(yiMi) 's 
and the corresponding simulation standard errors in parentheses are -2055.6 (0.9), 
-1933.9 (0.6), and -1899.4 (0.6) for the MVC, MVL, and MVP models, respectively. 
Based on the marginal likelihoods, the MVP model is the best. However, this finding 
may not be important for this particular application because our primary interest 
is to compare the mean ranks among the items under study. 
6. 
Concluding Remarks 
Correlated ordinal data often arise in experiments when two or more measure-
ments are taken at one time for the same subjects or when repeated measurements 
are taken over time. If such correlation is ignored in the model, overstatement or 
understatement of the precision of parameter estimates may result. We have con-
sidered a unified approach in this paper to incorporate the correlation structure, 
using the notion of multivariate generalized linear models. 
Our suggested modeling approach is based on multivariate link functions using a 
very rich class of scale mixtures of normals. Such models are very flexible and include 
all the standard link functions in a generalized linear model scenario. Our data 
analyses include complete Bayesian model fitting along with model comparisons 
and model diagnostics, which are very difficult to implement in a classical setup. 
There are several effective graphical approaches presented in this chapter which give 
more insight to the data analysis. 
There is another advantage of considering our approach over the usual random 
effects model which is based on the assumption of exchangeability. This is clearly 
reflected in our data analysis which shows that the equicorrelation assumption is 
not valid. In addition other advantages of Bayesian modeling over classical approach 
prevail in our studies. This includes more precise influence, exact small sample 
analysis, incorporation of the prior information, and inclusion of a large number of 
covariates. 
Acknowledgement 
Dr. Chen's research was supported by the National Science Foundation under 
Grant No. DMS-9702172. 

156 
Chen & Dey 
References 
Albert, J .H. and Chib, S. (1993). Bayesian Analysis of Binary and Polychotomous 
Response Data. Journal of the American Statistical Association, 88, 669-679. 
Albert, J .H. and Chib, S. (1995). Bayesian Residual Analysis for Binary Response 
Regression Models. Biometrika, 82. 7 47-759. 
Ashford, J .R. and Sowden, R.R. (1970). Multivariate Probit Analysis. Biometrics, 
26, 535-546. 
Box, G.E.P. and Tiao, G.C. (1992). Bayesian Inference in Statistical Analysis. 
Wiley: New York. 
Chen, M.-H. and Dey, D.K. (1998). Bayesian Modeling of Correlated Binary Re-
sponses via Scale Mixture of Multivariate Normal Link Functions. Sankhyii, 
Series A, 60, 322-343. 
Chen, M.-H. and Schmeiser, B.W. (1998). Towards Black-Box Sampling: A Random-
Direction Interior-Point Markov Chain Approach. Journal of Computational 
and Graphical Statistics, 7, 1-22. 
Chen, M.-H. and Shao, Q.M. (1997). On Monte Carlo Methods for Estimating 
Ratios of Normalizing Constants. Annals of Statistics, 25, 1563-1594. 
Chib, S. (1995). Marginal Likelihood from the Gibbs Output. Journal of the Amer-
ican Statistical Association, 90, 1313-1321. 
Chib, S. and Greenberg, E. (1998). Bayesian Analysis of Multivariate Probit Mod-
els. Biometrika, 85, 347-361. 
Cowles, M.K. (1996). Accelerating Monte Carlo Markov Chain Convergence for 
Cumulative-Link Generalized Linear Models. Statistics and Computing, 6, 
101-111. 
Cowles, M.K. and Carlin, B.P. (1996). Markov Chain Monte Carlo Convergence 
Diagnostics: A Comparative Review. Journal of the American Statistical As-
sociation, 91, 883-904. 
Cowles, M.K., Carlin, B.P., Connett, J .E. (1996). Bayesian Tobit Modeling of Lon-
gitudinal Ordinal Clinical Trial Compliance Data with Nonignorable Missing-
ness. Journal of the American Statistical Association, 91, 86-98. 
Choy, S.T.B. (1995). Robust Bayesian Analysis Using Scale Mixture of Normals 
Distributions. Ph.D. Dissertation, Department of Mathematics, Imperial Col-
lege, London. 
Devroye, L. (1986). Non- Uniform Random Variate Generation. Springer-Verlag: 
New York. 
Dey, D.K., Gelfand, A.E., Swartz, T.B., and Vlachos, P.K. (1998). Simulation 
Based Model Checking for Hierarchical Models. Test, 7. 
Ganter, L.G., Paulauskas, K.P., and Chen, M.-H. (1995). An Analysis of Master's 
Degree Programs for Secondary School Mathematics Teachers: A Needs As-
sessment. Technical Report, Department of Mathematical Sciences, Worcester 
Polytechnic Institute. 

8. Correlated Ordinal Data Models 
157 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling Based Approaches to Calculat-
ing Marginal Densities. Journal of the American Statistical Association, 85, 
398-409. 
Geman, S. and Geman, D. (1984). Stochastic Relaxation, Gibbs Distributions and 
the Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 6, 721-741. 
Geweke, J. (1991). Efficient Simulation from the Multivariate Normal and Student-
t Distributions Subject to Linear Constraints. Computing Science and Statis-
tics: Proceedings of the Twenty- Third Symposium on the Interface, 571-578. 
Hastings, W.K. (1970). Monte Carlo Sampling Methods Using Markov Chains and 
Their Applications. Biometrika, 57, 97-109. 
Kass, R.E. and Raftery, A.E. (1995). Bayes Factors. Journal of the American Stat-
istical Association, 90, 773-795. 
Liang, K.-Y. and Zeger, S.L. (1986). Longitudinal Data Analysis Using Generalized 
Linear Models. Biometrika, 73, 13-22. 
Liu, J.S. (1994). The Collapsed Gibbs Sampler in Bayesian Computations with Ap-
plications to a Gene Regulation Problem. Journal of the American Statistical 
Association, 89, 958-966. 
Meng, X.L. and Wong, W.H. (1996). Simulating Ratios of Normalizing Constants 
via a Simple Identity: A Theoretical Exploration. Statistica Sinica, 6, 831-860. 
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E. 
(1953). Equations of state calculations by fast computing machines. Journal 
of Chemical Physics, 21, 1087-1092. 
Nandram, B. and Chen, M.-H. (1996). Accelerating Gibbs Sampler Convergence in 
the Generalized Linear Models via a Reparameterization. Journal of Statistical 
Computation and Simulation, 54, 129-144. 
O'Neill, R. (1971). Algorithm AS47-Function Minimization Using a Simplex Pro-
cedure. Applied Statistics, 20, 338-345. 
Prentice, R.L. (1988). Correlated Binary Regression with Covariate Specific to 
Each Binary Observation. Biometrics, 44, 1033-1048. 
Tanner, M.A. (1996). Tools for Statistical Inference. Third Edition, New York: 
Springer-Verlag. 
Tierney, L. (1994). Markov Chains for Exploring Posterior Distributions (with 
discussions). Annals of Statistics, 22, 1701-1762. 
Vounatsou, P., Smith, A.F.M., and Choy, S.T.B. (1996). Bayesian Robustness for 
Location and Scale Parameters Using Simulation. Technical report, Imperial 
College London. 
Wakefield, J.C., Gelfand, A.E., and Smith, A.F.M. (1991). Efficient Generation of 
Random Variates via the Ratio-of-Uniforms Method. Statistics and Comput-
ing, 1, 129-133. 
Zeger, S.L. and Liang, K.-Y. (1986). Longitudinal Data Analysis for Discrete and 
Continuous Outcomes. Biometrics, 42, 121-130. 

This Page Intentionally Left Blank 

9 
Bayesian Methods for Time Series 
Count Data 
Joseph G. Ibrahim 
Ming-Hui Chen 
ABSTRACT Correlated count data arise often in practice, especially in repeated 
measures situations or instances in which observations are collected over time. In 
this chapter, we consider a parametric model for a time series of counts by con-
structing a likelihood based version of a model similar to that of Zeger (1988). The 
model has the advantage of incorporating both overdispersion and autocorrelation. 
We consider a Bayesian approach and discuss a class of informative prior distribu-
tions for the model parameters that are useful for variable subset selection. The 
prior specification is motivated from the notion of the existence of data from similar 
previous studies, called historical data, which is then quantified into a prior distribu-
tion for the current study. We give theoretical and computational properties of the 
proposed priors, as well as examine properties of the implied posterior distributions. 
In addition, computational methods for sampling from the posterior distribution 
of the parameters and computing posterior model probabilities are discussed. The 
computational methods are based on the idea of hierarchical centering (Gelfand et 
al., 1996), and are quite efficient for sampling from the posterior distribution for the 
models considered here. To compute the posterior model probabilities, only posterior 
samples from the full model are needed to estimate the posterior probabilities for all 
of the possible subset models. The methodology is motivated from a real data set 
involving yearly pollen counts, which is also discussed. 
1. 
Introduction 
Historical data can be very helpful in interpreting the results of the current study. 
However, very few methods exist for the formal incorporation of historical data 
to construct the prior distribution. There is some literature addressing this issue 
for the linear model and generalized linear models. See for example, Ibrahim and 
Laud (1994), Laud and Ibrahim (1995), Bedrick, Christensen, and Johnson (1996), 
Ibrahim, Ryan, and Chen (1998), and Chen, Ibrahim, and Yiannoutsos (1999). In 
all of these papers, the authors assume a univariate independent response variable. 
The literature for informative prior elicitation for models with correlated responses 
is essentially nonexistent. 
In this chapter, we present an extension of the priors in Ibrahim, Ryan, and 
t59 

160 
Ibrahim & Chen 
Chen (1998) and and Chen, Ibrahim, and Yiannoutsos (1999) to time series count 
data. The prior specification is based on the notion of specifying a prior prediction 
Yo for the response vector, y, of the current study, along with a covariate matrix 
Xo corresponding to yo. Then Do == (no, Yo, Xo) is used to specify an automated 
parametric informative prior for the regression coefficients. Thus, Do is the historical 
data. The quantity Yo can be taken as the raw response vector from the historical 
data, a vector of fitted values based on the historical data, a vector obtained from 
a theoretical prediction model, or a vector specified from expert opinion or case-
specific information. Thus y0 can be viewed as a prior "prediction" for y, the actual 
data in the current study. Similarly, Xo can be taken as the raw covariate matrix 
based on the historical data or it can be specified in other ways. In any case, 
taking Do to be the raw historical data results in a more natural, interpretable, and 
automated specification. 
The priors discussed here are attractive for variable subset selection, and thus 
these applications serve as a primary motivation for the priors. Another major ad-
vantage of this methodology is that the time series considered here are virtually 
impossible to fit in the frequentist context, let alone the entire problem of frequen-
tist variable subset selection. If for example, there are k possible covariates, then 
there are 21: models to evaluate in the variable subset selection problem. This is 
a computational nightmare in general, regardless of a frequentist or Bayesian ap-
proach. Aside from the computational issues, the methodology in the frequentist 
paradigm is not well developed for the class of models we consider here. In this 
chapter, the Monte Carlo methods we discuss facilitate a very fast and efficient way 
of computing the posterior model probabilities using only a single posterior sample 
from a single model, that being the full model. Such a procedure has proved to be 
quite feasible and powerful in the model selection context (see for example, Ibrahim 
and Chen (1998) and Chen, Ibrahim, and Yiannoutsos, (1999)). 
The rest of the chapter is organized as follows. In Section 2, we present notation 
for the model, the likelihood, the prior distributions, and the posterior distribu-
tions. Computational techniques for sampling from the posterior are discussed in 
Section 3. In Section 4, we present a real dataset demonstrating the computational 
feasibility and the strength of our method over GEE and likelihood based methods. 
We conclude with some general discussion and some possible extensions in Section 
5. 
2. 
The Method 
2.1 
The Likelihood Function 
Let M denote the model space. We enumerate the models in M by m == 1, 2, ... , IC, 
where IC is the dimension of M and model IC denotes the full model. The full model 
is defined here as the model containing all of the available covariates in the study. 
Letting k denote the number of covariates for the full model, our model space, M, 
then contains 2k models. Also, let f3(JC) == (f3o, f31, ... , f3k )' denote the regression 
coefficients for the full model including an intercept, and let f3(m) denote a km x 1 
vector of regression coefficients for model m with an intercept, and a specific choice 
of km - 1 covariates. We write f3(JC) == (f3(m )', {3( -m )')', where {3( -m) is f3(JC) with 
{3( m) deleted. 
Consider a time series of counts Yt, t == 1, ... , n, where each Yt has corresponding 

9. Bayesian Methods for Time Series 
161 
km x 1 covariate vectqr :v~m) under model m. Under model m, conditional on f3(m) 
and a stationary unobserved process ft, the Yt 's are assumed to be independent 
Poisson random variables with mean At = exp(Et + (x~m))'f3(m)), leading to the 
conditional density 
P(Y I f3<m), f, n<m>) 
n II P(Yt I f3(m), ft) 
t=l 
n IT exp { Yt(ft + (x~m))' {3(m))- exp(Et + (x~m))' {3(m)) -log(yt!)} 
t=l 
exp { y'(E + X(m) {3(m))- J~Q({3(m), E)- J~C(y)}, 
(1) 
where y = (Yb ... ' Yn)'' ( = ( fl' ... 'En), x<m) is the n X km matrix of covariates 
with tth row equal to (x~m))', Jn is ann x 1 vector of ones, and Q(f3(m), E) is ann x 1 
vector with tth element equal to qt:::::: exp { ft + (x~m))'f3(m) }, and C(y) is ann x 1 
vector with jth element log(yi !). Finally, nCm) = ( n, y, X(m)) denotes the data for 
the current study under model m. The latent process ft is assumed to have normal 
distribution with mean 0. In particular, we assume that f has a multivariate normal 
distribution with mean 0 and, covariance matrix u2:E, where the ( i, j)th element 
of :E has the form O'ij = pli-il, where pli-jl is the correlation between ( Ei, fj ), and 
-1 ~ p ~ 1. The unobserved process ft is analogous to a "random effect" in a 
random effects model, with the exception that the latent process is correlated. We 
note that the mean and variance of Et do not depend on t. Zeger (1988) constructs 
a similar model through the mean and covariance of the latent process, which then 
define the estimating equations. He does not specify a parametric distribution for 
the latent process as is done here. 
The joint density of (y, E) can be written as 
p(y, ( I f3<m), u2, p, n<m)) 
(27ru2)-n/2(1 _ p2)-(n-1)/2 
X 
exp { y' (t+ xCm) ,a(m))- J~Q(,B(m)' t)- J~C(y)- 2!2 t'E-1 f}. 
(2) 
To induce the correlation structure on y, we integrate out f from (2) leading to the 
"marginal" likelihood of f3(m), given by 
p(y I {3(m)' 0'2) p, D(m)) =I p(y, ( I {3(m)' 0'2) p, n(m)) dE ' 
(3) 
where p(y, ( I f3(m)' u 2' p, n(m)) is given by (2). The marginal likelihood of f3(m) in (3) 
does not have a closed form, and thus the integral cannot be evaluated analytically. 
The implications of the process ft on the correlation structure in the Yt 's and 
the regression model is as follows. Note first that f; = exp( ft) has a log-normal 
distribution with mean a = exp( !u2) and variance v 2 = exp(2u2) - exp(u2). It 
follows upon integration over ft, that the marginal mean of Yt is given by 
(m) 
1-'t 
E(Yt I {3(m), n(m)) 
exp((x~m))' {3(m)) E(exp(Et)) 
a exp((:r~m))' {3(m)) ' 

162 
Ibrahim & Chen 
so that the intercept in the marginal model is log( a:)+ {30 • In addition, we have 
2 
Var(yt I {3(m)' n(m)) = J.t~m) + v 2 (J.t~m))2 ' 
a: 
and 
so that 
-1 
Corr(yt, Yt+A: ID(m)) = ______ 
__;;;_;___:~------,.--,--
[((J.t~m))_l + v2a:-2))((J.t~~~)-1 + v2a:-2)] 
(4) 
(5) 
From ( 4) and (5), we see that the unobserved process ft allows for overdispersion 
and autocorrelation into Yt· In addition, the degree of overdispersion depends on 
J.tt. The autocorrelation in Yt must be less than or equal to that in ft and the degree 
of autocorrelation in Yt relative to ft decreases as J.tt and v 2 decrease. 
2.2 
The Prior Distributions 
Informative prior elicitation is an important part of a Bayesian analysis, espe-
cially in the problem of variable subset selection since proper prior distributions 
are required to compute posterior model probabilities. Here, we present a class of 
informative priors for the regression coefficients f3(m). The prior construction for 
f3(m) is based on the availability of historical data as motivated in Section 1. Sup-
pose there are N historical data sets and the sample size of the ith historical study 
is noi. Let Yoi denote the noi x 1 vector of time series counts for the ith histori-
cal study and let X a':) denote the noi X km matrix of covariates corresponding to 
the ith historical study. In addition, let Eoi denote the latent process for the ith 
historical study, where Eoi is an noi x 1 vector, i = 1, ... , N, and Eoi has an noi 
dimensional multivariate normal distribution with mean 0 and covariance matrix 
u2Eoi, where Eoi is an noi x noi matrix with (j,j*)th element equal to pli-i*l. 
Finally let D~':) = ( noi, X a':), Yoi) denote the data from the ith historical study 
under model m, and D~m) = (D6~), ... , D6~)) denotes all of the historical data 
under model m. 
The prior distribution for {3(m) for the ith historical study takes the form 
7r(f3Cm) I u2 , p, n6';), aoi) 
ex: j P(Yoi I {3(m), Eoi, aoi) (27ru2)- ~ (1- p2)- (no~-l) 
x exp {-2!2 (t~;Ei)/toi)} dto;, 
(6) 
where 
P(Yoi I {3(m), Eoi, aoi) 
= exp { aoi [Y~i(Eoi + Xa':)f3(m))- J~oiQ({3(m),Eoi)- J~oiC(Yoi)]}, 
(7) 
and aoi is a scalar prior parameter that controls the weight of the ith historical 
study relative to the likelihood of the current study. That is, aoi controls the weight 

9. Bayesian Methods for Time Series 
163 
of the likelihood function based on the ith historical study. Small values of aoi give 
less weight whereas large values give more weight. It is most sensible to restrict aoi 
to 0 ::; aoi ::; 1, since we would not want to weight the historical data more than 
the current data. The parameter aoi can also be interpreted as an overdispersion 
parameter which takes into account the between and within study variability in the 
historical data sets. 
The prior distribution of f3(m) based on all of the historical studies is thus given 
by 
(8) 
where a0 = ( a01, ... , aoN ). From (8), we see that aoi = 0 corresponds to no in-
corporation of historical data, and yields a uniform improper prior for {3. The case 
aoi = 1 implies that the historical data and the current data are weighted equally. 
The prior in (8) does not have a closed form but it has several attractive theoretical 
and computational properties as shown in Section 3. 
The prior specification is completed by specifying priors for ( u2, p, ao). We take 
these parameters to be independent a priori. We specify an inverse gamma prior 
for u2, denoted 10(60 , "Yo), a scaled beta prior for p, denoted scbeta(vo, .,Po), and 
independent identically distributed beta priors for each aoi, denoted beta( ao, Ao). 
Here, (60 , "'(o, v0 , .,P0 , a 0 , .Xo) are specified prior hyperparameters. Thus, the joint prior 
distribution takes the form 
7r(f3(m)' u2, p, ao I D~m)) 
oc fi ( j p(yo; \ f3(m), fo;, ao;) (2.-u2)- ¥ (1- p2)- <··~-•I 
x exp {- 2~2 ('~;EO/ fo;)} dfo;) x (fi agr'(l- ao;)~'-
1) 
x(u2)-(oo+l) exp( -U-2"'fo) (1 + pyo-1(1- p),Po-1. 
(9) 
We see that our joint prior for (f3(m), u2, p, a0 ) clearly does not have a closed form 
in general. However, it has a natural motivation and several appealing interpreta-
tions. One motivation for the prior in (9), is that by taking the a0 random, the 
tails of the marginal prior distribution for f3(m) are heavier than those obtained by 
taking a0 a fixed hyperparameter. In addition, a prior on a0 provides great flexi-
bility and allows us to express our uncertainty about it. By allowing different aoi 's 
for different historical studies, we are able to develop a much more flexible prior 
that can weight each historical study differently. This would certainly be desirable 
if one historical study has a much larger sample size than another historical study. 
Another motivation for (9) is that it mimics the marginal likelihood function of 
f3(m) based on the historical data. If, for example ao = 1, then (9) is precisely the 
marginal likelihood function of f3(m) based on the historical data. Thus, our prior 
can be viewed as a weighted marginal likelihood of f3(m), which is a natural prior 
to consider when such historical data is available. 

164 
Ibrahim & Chen 
2.3 Specifying the Hyperparameters 
In the context of model selection, (p, u2) are viewed as nuisance parameters, and 
therefore we take vague choices for their prior hyperparameters. In particular, it 
is reasonable to take v0 = 1/Jo = 1 which yields a uniform prior for p on [ -1, 1]. 
Also, we take 60 -+ 0 and 'Yo -+ 0, which yields a noninformative prior for u 2 • 
For aoi, we recommend that several values of the hyperparameters be chosen and 
sensitivity analyses conducted. For elicitation purposes, it is easier to work with 
the prior mean and variance of aoi, given by J.Lao = ao/(ao + .Xo), and 0"~ 0 = 
J.La 0(1- J.La 0 )(ao + Ao + 1)- 1• It can be shown that a sufficient condition for the 
propriety of the prior distribution is that a 0 > k + 1 for the full model. Therefore, a 
reasonable starting point for the analysis is to choose a0 = Ao = k + 2, which gives 
J.La 0 = 1/2. Then we conduct several sensitivity analyses within a suitable range 
of the uniform prior, using various values of (J.La 0 , 0"~ 0 ). Small and large values of 
(J.La 0 , 0"~ 0 ) should be considered. We do not recommend doing an analysis based on 
one set of proposed values of (J.Lao, 0"~ 0 ), nor do we propose specifying (J.Lao, 0"~ 0 ) by 
a one-time automated procedure. The choice of (J.Lao, 0"~0 ) depends on the context of 
the problem and the structure of the historical data. In Section 4, we demonstrate 
several choices of (J.Lao, 0"~ 0 ) and conduct sensitivity analyses and examine their 
impact on variable selection. 
2.4 
Prior Distribution on the Model Space 
Let 
p~(/3(m) lD~m)) 
= j { fi ( j p(yo;I.B(m), <o;, ao;)(2.-u2)-¥(1- p2)-~ 
X exp {- 2~2 (<~;l:Q/<0;)} d<o;) x (fi ag;-
1(1- a0;)~"-
1) 
X ( u2)-(;o+1) exp(- !~) (1 + p)""- 1(1- p).P"- 1 }du2 dp dao. 
(10) 
We see that p0(f3(m) ID~m)) is proportional to the marginal prior of f3(m). We propose 
to take the prior probability of model m as 
(11) 
This choice for p( m) in (11) is a natural one since the numerator is just the normal-
izing constant of the joint prior of (/3(m), a0 , u2 , p) under model m. The prior model 
probabilities in ( 11) are based on coherent Bayesian updating and this results in 
several attractive interpretations. Firstly, p( m) in (11) corresponds to the posterior 
probability of model m based on the data D~m) using a uniform prior for the previ-
ous study, p0(m) = 2-k for mE M as a 0 -+ oo. That is, p(m) ex p(m I D~m)), and 
thus p(m) corresponds to the usual Bayesian update of po(m) using D~m) as the 
data. Secondly, as Ao -+ oo, p(m) reduces to a uniform prior on the model space. 

9. Bayesian Methods for Time Series 
165 
Therefore, as .\0 -lo oo, the historical data D~m) have a minimal impact in determin-
ing p(m). On the other hand, p(m) in (11) has a nice theoretical property, which 
greatly eases the computational burden for calculating posterior model probabilities 
using the Markov chain Monte Carlo (MCMC) output. 
3. 
Computation of Model Probabilities 
In this section, we state the theoretical properties of the posterior model prob-
abilities based on the choice of the prior model probabilities p(m) given in (11) 
and then disuss the Monte Carlo implementation procedures to compute posterior 
model probabilities. A key result that is presented is that we give a formula for the 
posterior model probability that does not depend directly on p(m). This is due to 
a cancellation of terms that results from the structure of p(m) given in (11). 
The posterior probability of model misgiven by 
m!D(m) -
p(D(m)jm) p(m) 
p( 
) - "~ (D(j)l') ( ') ' 
LIJ=lp 
J p J 
(12) 
where p(D(m) lm) denotes the marginal distribution of the data D(m) for the current 
study under model m, and p(m) denotes the prior probability of model min (11). 
We first obtain an expression for p(D(m) lm). From (9), the joint prior distribution 
for (f3(m), u 2, p) is given by 
1r(f3(m), (]'2, piD~m)) ex: p~(f3(m), (]'2, p!D~m)) 
= j { fi ( j p(yo;l!3(m), fo;, ao;)(211"<T2)- ¥-(1- P"l-l!!op 
exp {- 2!2 (f~;l:Q/fo;)} dfo;) x (fi agr'(1- ao;)~'-
1) 
X(<T2)-(6o+l) exp( _,.-•ro) (1 + p)"'- 1(1- p)~'- 1 }dao, 
(13) 
where p0(f3(m), u 2, p, I D~m)) represents the unnormalized joint prior density func-
tion of (f3(m), u 2, p, I D~m)) under model m. i.e., the right side of equation (9). Then, 
using (13), the joint posterior distribution of (f3(m), u 2, p) under model m is given 
by 
p(f3(m)' (]'2' pI n(m)' D~m)) 
ex: 
p*(/1(m)' (]'2' pI n(m)' D~m)) 
= p(y I {1(m),u2,p,D(m)) 7r(f3(m),u2,p I D~m)), 
(14) 
where p(y I f3(m),u2,p,D(m)) is given by (2), 1r(f3(m),u2,p I D~m)) is given by (13) 
and p* (f3(m), u2 , p I D(m), D~m)) is an unnormalized joint posterior density function. 
Clearly if the prior is proper, then so is the posterior. Thus 

166 
Ibrahim & Chen 
Recall that f3(JC) = (f3(m)', (3( -m)')' where (3( -m) is f3(JC) with f3(m) deleted. Then, 
it can be shown that the posterior probability p(mjD(m)) in (12) of model m is 
given by 
p((3( -m) = OjD(JC), D(JC)) 
p(miD(m))-
0 
(16) 
-
l:f=lp((3(-j) = Ojfl(IC),D~JC)) ' 
m = 1, ... , K, where f3(JC) = (f3(m)', (3( -m)')', and p(f3( -m) = OjD(JC), D~JC)) is the 
marginal posterior density of (3( -m) evaluated at (3( -m) = 0. In (16), for notational 
convenience we assume that p(f3< -JC) = OID(JC), D~JC)) = 1. The result in (16) is very 
attractive since it shows that the posterior model probability p(miD(m)) is simply 
a function of the marginal posterior density functions of (3( -m) for the full model 
evaluated at (3( -m) = 0. This formula does not algebraically depend on the prior 
model probability p( m) since it cancels out in the derivation due to th(') structure of 
p(m). This is an important feature since it allows us to compute the posterior model 
probabilities directly without numerically computing the prior model probabilities. 
This has a clear computational advantage and as a result, allows us to compute 
posterior model probabilities very efficiently. We note that this computational device 
works best if all of the covariates are standardized to have mean 0 and variance 
1. This is not restrictive since this is a typical transformation taken quite often 
in practice to numerically stabilize the Gibbs sampler and the adaptive rejection 
algorithms. 
Due to the complexity of our model, the analytical evaluation of p(f3(-m) = 
OjD(JC), D~JC)) does not appear possible. We use the Monte Carlo method devel-
oped in Ibrahim and Chen (1998) and Chen, Ibrahim, and Yiannoutsos (1999) 
to compute posterior model probabilities using a single Markov chain Monte Carlo 
(MCMC) output from the full model. The hierarchical centering reparameterization 
technique of Gelfand et al. (1996) is particularly suitable for the implementation 
of MCMC sampling for this problem. This technique is also very useful in develop-
ing an efficient Monte Carlo method for estimating the marginal posterior density 
p(f3( -m) = OjD(JC), D~JC)). 
To this end, consider the following reparameterization: 
17 = c + X(JC) (3(JC) 
(17) 
and 
(18) 
fori= 1, 2, ... , N. Let 1]o = (1701 , ... , fJoN). Now the reparameterized posterior for 
the full model is given by 
p(f3(JC)' u2' p, ao, 1], TJoiD(JC)' D~JC)) ex: u-n(l- p2)-(n-1)/2 
x exp { y' '7- J~ Q( '7) -
2~2 ( '7 - X( X:) ,8)' E-1( 1] -
X(X:) ,8)} 
x (fi exp { ao; [1/o;'lo; - J~,;Qo('lo;)- J~,p(yo;)j}) x (2,.u2)-
2)- i!!..o.&_::1l 
{ 
1 ( 
(IC) )' -1 ( 
(IC) R)} 
x (1 - p 
2 
exp - 2u2 1]oi - Xoi (3 Eoi 1]oi - Xoi 
p 
x (fi agr'(l- ao;)~o-1) (u2)-(6o+l) exp (- ~~) 
X (1 + pyo-1(1 _ p)1flo-l , 
(19) 

9. Bayesian Methods for Time Series 
167 
where Q(17) and Qo(TJoi) are vectors of length n and noi, respectively, with jth ele-
ment equal to exp( 1Ji ), and exp( 1]oij), 1Ji = ( x)K:))' ,B(K:) + £j, and 1]0ij = ( 
x~~))' ,B(K:) + 
£oij· Assume that {CB~~),u~l),P(l),ao(l),1J(l),1JO(l)), l = 1,2, ... ,L} is an MCMC 
sample from the reparameterized posterior p(,B(K:), u 2, p, ao, 1], TJoiD(K:), D~K:)) given 
in (19). Then, from (19) and following the lines of Chen (1994), p(,B( -m) = OjD(K:), 
D~K:)) can be estimated by the conditional marginal density estimation (CMDE) 
method. Gelfand, Smith, and Lee (1992), Chen (1994), and Chen and Shao (1997) 
have shown that the CMDE is the most efficient Monte Carlo method for estimat-
ing marginal posterior densities when a joint posterior density is known up to' a 
normalizing constant. It directly follows from, for example, Chen and Shao (1997) 
that a simulation consistent estimator of p(,B( -m) = OID(K:)) is given by 
where Nk+1-km (,B( -m) = OI,B~i)m), P~~), (B(l) )- 1) is the (k + 1 - km)-dimensional 
conditional normal density function of Nk+1(P~~),(B(l))-
1 ) given ,8~;)) eval,uated 
at ,B( -m) = 0, 
B 
- - 1-
((xCK))'"'- 1xCK) + ~(xCK:))'"'- 1 xCK)) 
(l) -
0"2 
.u(l) 
L.....J 
Oi 
,uOi(l) 
Oi 
' 
(l) 
i=1 
A(K:) 
1 { -1 ( 
(K:) I -1 
~ (K:) I -1 
) } 
,8 
= ~ B(l) 
(X 
) E(l) 17(1) + ~(Xoi ) EOi(l)17oi(l) 
, 
(l) 
z=1 
E(l) is an n x n matrix with (j,j*)th element equal to A{)i*l, and Eoi(l) is an 
noi x noi matrix with (j, j*)th element equal to pt{)i*l. 
There are several advantages to using the above Monte Carlo procedure. First, as 
previously mentioned, it is not required to compute p( m) for each model. Second, we 
need only one random draw from p(,B(K:), u2, p, ao, 1], TJoiD(K:), D~K:)). Third, after we 
obtain an MCMC sample from the posterior distribution of the full model, calculat-
ing f>(,B( -m) = OID(K:)) given by (20) is straightforward and almost free of computa-
'tional time. Fourth, for the purposes of computing posterior model probabilities, it 
is required only to store a ( k + 1 )-dimensional vector P(l) and a ( k + 1) x ( k + 1) ma-
trix B(l) for each MCMC sampling iteration, which will greatly reduce the computer 
storage space. This becomes even more advantageous for cases where multiple pre-
vious studies are available and each noi is large. The above features of our Monte 
Carlo procedure essentially make the computation of Bayesian variable selection 
feasible in the presence of a large number of covariates (say, k > 20). 
4. 
Example: Pollen Data 
Pollen allergy is a common disease causing hay fever and respiratory discomfort 
in approximately 10% of the United States population. Although not a life threat-
ening illness, allergy symptoms seem to be increasingly more troublesome, as well 
as costing society a great deal of money and resources. Therefore, it is becoming 

168 
Ibrahim & Chen 
increasingly important to identify the important covariates that help predict pollen 
levels. 
We consider a real data set in which ragweed pollen was collected daily in Kalama-
zoo, Michigan form 1991 to 1994. Frequentist analyses of these data using standard 
Poisson regression methods have been conducted by Stark et al. (1997). Our aim 
here is not to do a detailed data analysis, but rather demonstrate our Bayesian 
methodology for variable selection. The response variable y, is the pollen count for 
a particular day in the season for a given year. We take the 1991, 1992, and 1993 
data as the historical data and the 1994 data as the current data. The full model 
contains an intercept and seven covariates, which were extensively discussed and 
motivated by Stark et al. (1997). These are x1 = rain, (which is a binary variable 
taking the value 0 if there were at least three hours of steady rain, and 1 otherwise), 
x2 = day in the pollen season , x3 = log( day). In addition, we consider two co vari-
ates that are functions of temperature. These are x 4 which is the lowess smoothed 
function of temperature constructed from a nonparametric estimate of the regres-
sion of pollen count on average temperature, and x5 , which denotes the deviation 
from the daily averages temperature to the lowess line. The final two covariates 
are X6 = windspeed and X7 = cold, (which is a binary variable taking the value 0 if 
the overnight temperature dropped below 50 degrees Fahrenheit, and 1 otherwise). 
Tables 9.1 and 9.2 summarize the response variable and covariate data for the 
four years. 
The analysis in Stark et al. (1997) is based on a Poisson model assuming inde-
pendent counts, and thus it does not introduce a latent process nor does it account 
for the time series structure in the data. It is quite different from the model we 
consider in (2). We model the pollen counts as a Poisson distribution as in equation 
(1) with covariates (x1, 0. 0, x7). The model space M contains 27 models. We specify 
noninformative priors for p and u2 • Specifically, we take a uniform prior for p on 
[-1, 1) (i.e. Vo = 1/Jo = 1) and take u2 ,..... 10(.005, .005). 
Table 9.3 give results for the model with the largest posterior probability based on 
several values of (Jtao, 0" a 0 ). From Table 9.3, we see that the top model in each case is 
(x1, x2, x3, x4, x5 ). In addition, we see that the posterior model probabilities increase 
monotonically as more weight is given to the historical data. When we put very small 
weight on the historical data, such as (Jta 0 , O"a0 ) = (.009, .003) the (x1, x2, X3, X4, xs) 
model still obtains the largest posterior probability, with value .117. When we put 
extremely small weight on the historical data such as (Jtao, O"a0 ) = (.0009, .0003), the 
(x2, x3, X4, xs, x1) model obtains the largest posterior probability, with value .122 
and the (x1, x2, x3, x4, x5) model obtains the fourth largest posterior probability 
with value .101. Thus, we see that model choice is reasonably robust to the choice 
of (Jtao, O"a0 ), consistently yielding the (x1, x2, X3, x4, xs) model as the top model for 
a suitable range of (Jtao, u a 0 ). Based on these analyses, it does not appear that the 
variables x 6 (windspeed) and X7 (coldness of temperature) are important predictors 
of pollen counts. 
5. 
Discussion 
We have discussed a class of informative priors for time series count data that are 
quite natural and useful when historical data is available. The priors have some very 
attractive properties and are proper under some very general conditions. We have 
also discussed computational methods for sampling from the posterior distribution 

9. Bayesian Methods for Time Series 
169 
T.i\:BLE 9.1. Summaries of Variable§ for Pollen Data- Part I 
year 
variable 
range 
mean 
standard 
deviation 
1991 
X2 
1.0- 92.0 
46.5 
27.7 
xs 
0.0 - 4.5 
3.56 
0.92 
x4 
53.0- 73.8 
64.6 
8.2 
X5 
-13.4- 16.1 
0.18 
6.9 
X6 
0.0- 18.0 
11.1 
3.9 
y 
0.0- 377 
43.1 
73.4 
1992 
X2 
1.0- 82.0 
41.7 
23.9 
xs 
0.0- 4.4 
3.4 
0.92 
X4 
46.2- 70.4 
61.2 
7.7 
X5 
-11.4-13.7 
0.46 
6.1 
X6 
4.0- 24.0 
12.8 
3.9 
y 
0.0- 440 
53.9 
86.3 
1993 
X2 
1.0- 87.0 
44.0 
25.3 
xs 
0.0 - 4.5 
3.5 
0.92 
X4 
49.9- 75.2 
62.5 
9.1 
X5 
-12.6- 15.5 
0.16 
6.2 
X6 
0.0- 15.0 
8.6 
3.5 
y 
0- 362 
47.0 
78.8 
1994 
X2 
1.0- 79.0 
40.8 
23.2 
xs 
0.0- 4.4 
3.42 
0.94 
X4 
50.5- 69.3 
62.9 
6.3 
X5 
-10.0- 12.8 
0.30 
6.0 
X6 
4.0- 18.0 
10.47 
2.83 
0- 205 
32.29 
49.1 

170 
Ibrahim & Chen 
TABLE 9.2. Summaries of Variables for Pollen Data - Part II 
year 
variable 
value 
count 
percent 
1991 
Xl 
0 
10 
10.9 
Xl 
1 
82 
89.1 
X7 
0 
27 
29.3 
X7 
1 
65 
70.7 
1992 
X1 
0 
8 
9.9 
X1 
1 
73 
90.1 
X7 
0 
30 
37.0 
X7 
1 
51 
63.0 
1993 
Xl 
0 
11 
12.6 
Xl 
1 
76 
87.3 
X7 
0 
29 
33.3 
X7 
1 
58 
66.7 
1994 
Xl 
0 
6 
8.0 
Xl 
1 
69 
92.0 
X7 
0 
25 
33.3 
X7 
1 
50 
66.7 
TABLE 9.3. Posterior Model Probabilities For Pollen Data 
Model 
(JJao, Uao) 
p(m I n(m)) 
(xl,x2,x3,x4,x5) 
(.5, .11) 
.142 
(xl,x2,xa,x4,x5) 
(.5, .08) 
.290 
(xl,x2,xa,x4,x5) 
( .5, .06) 
.385 
(xl,x2,xa,x4,x5) 
( .5, .05) 
.420 
(x1,x2,x3,x4,x5) 
( .98,.02) 
.421 

9. Bayesian Methods for Time Series 
171 
and for computing posterior model probabilities for variable subset selection. The 
expressions obtained for the posterior model probabilities facilitate a very quick and 
efficient method of calculation. In addition, the algorithms developed for sampling 
from the posterior distribution are quite efficient and feasible even for large data sets 
with a large number of covariates. The examples presented in Section 4 demonstrate 
the feasibility and the power of our methods. The Bayesian approach proposed 
here for this class of models appears to have a clear advantage over frequentist 
based procedures or other Bayesian procedures. Future work includes extending 
our methodology to multivariate discrete response models and multivariate models 
for longitudinal data. 
References 
Bedrick, E. J., Christensen, R., and Johnson, W. (1996). A New Perspective on 
Priors for Generalized Linear Models. journal of the American Statistical As-
sociation, 91, 1450-1460. 
Chen, M.-H. (1994):· Importance-weighted Marginal Bayesian Posterior Density 
Estimation. Journal of the American Statistical Association, 89, 818-824. 
Chen, M.-H., Ibrahim, J .G., and Yiannoutsos, C. (1999). Prior Elicitation, Variable 
Selection, and and Bayesian Computation for Logistic Regression Models. 
Journal of the Royal Statistical Society, Series B, 61, 223-242. 
Chen, M.-H. and Shao, Q.-M. (1997). Performance Study of Marginal Posterior 
Density Estimation via Kullback-Leibler Divergence. Test, A Journal of the 
Spanish Society of Statistics and O.R., 6, in press. 
Gelfand, A.E., Sahu, S.K., and Carlin, B.P. (1996). Efficient Parametrisations for 
Generalized Linear Mixed Models (with discussion). In Bayesian Statistics 
5, eds. J.M. Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith, Oxford: 
Oxford University Press, 165-180. 
Gelfand, A.E., Smith, A.F.M. and Lee, T.M. (1992). Bayesian Analysis of Con-
strained Parameter and Truncated Data Problems Using Gibbs Sampling. 
Journal of the American Statistical Association, 87, 523-532. 
Ibrahim, J.G. and Chen, M-H. (1998). Prior Distributions and Bayesian Compu-
tation for Proportional Hazards Models. Sankhya, Series B, 60, 48-64. 
Ibrahim, J .G., and Laud, P.W. (1994). A Predictive Approach to the Analysis of 
Designed Experiments. Journal of the American Statistical Association, 89, 
309-319. 
Ibrahim, J. G., Ryan, L. M., and Chen, M.-H. (1998). Use of Historical Controls to 
Adjust for Covariates in Trend Tests for Binary Data. Journal of the American 
Statistical Association, 93, 1282-1293. 
Laud, P. W., and Ibrahim, J .G. (1995). Predictive Model Selection. Journal of the 
Royal Statistical Society, Ser.B, 57, 247-262. 
Stark, P. C., Ryan, L. M., McDonald, J. L., Burge, H. A. (1997). Using Meteoro-
logic Data to Predict Daily Ragweed Pollen Levels. Aerobiologia, 13, 177-184. 

172 
Ibrahim & Chen 
Zeger, S.L., (1988). A Regression Model for Time Series of Counts. Biometrika, 
75, 621-629. 

10 
Iten1 Response Modeling 
James Albert 
Malay Ghosh 
ABSTRACT This chapter introduces the Bayesian fitting and checking of a fam-
ily of item response models. The one and two parameter item response models are 
described and the models are illustrated using a mathematics placement exam. The 
choice of prior distributions is discussed. It is shown that some standard noninfor-
mative priors will result in improper posterior distributions, and some guidance is 
provided on the choice of informative priors for the item parameters. Markov chain 
Monte Carlo algorithms are outlined for simulating from the joint posterior dis-
tribution of the item and ability parameters. Bayesian residuals and the posterior 
predictive distribution are used to assess model fit. The methods are used to exam-
ine the difficulty and discrimination characteristics of the items on the mathematics 
placement test. 
1. 
Introduction 
In this chapter, we consider the analysis of test data consisting of a number of 
multiple-choice questions. In the particular example that will be analyzed, 200 be-
ginning college students were given a mathematics placement test. The test consists 
of 35 multiple-choice items on topics in intermediate and college algebra. The pur-
pose of this exam is to assess a student's ability in high school algebra towards the 
goal of placing the student in a suitable mathematics course. The results of the 
exam are used together with other information such as the student's score on the 
ACT and their high school grade point average to recommend the best mathematics 
class for enrollment in the fall semester. 
In designing this mathematics placement test, there are several concerns. One is 
interested in placing items on the exam which have different levels of difficulty. If 
most of the students do very poorly on the exam, then the items are generally too 
difficult, and the results of the exam may not be helpful in accurately assessing the 
students' mathematical ability. A similar problem would be present if the majority 
of students were able to get all of the questions correct. Generally it is desirable to 
have a broad range of performances on the exam, ranging from students who get 
relatively few questions correct to students who get most of the questions correct. 
This wide variation of student performances will make it possible to more accurately 
assess the mathematical abilities of the students. 
If the test is effective in measuring the students' abilities, then the next concern 
is whether the individual items on the test are effective in discriminating among 
173 

174 
Albert & Ghosh 
I 
0.5 
0 
0 
0 
0 
0 
~ 
0 
0 
oo 
0 
~0.4 
0 
0 
0 
a: 
0 
8 
0 
0 
0 0 
0 
0 
w 
0 
0 
80.3 
0 
0 
0 
0 
0 
rn 
~ 
cs:>o 
0 
0 
0 
0 
0 
~ 0.2 
~ 
0.1 
0 
0~--~----~----~----~--~-----~~--~~----~----~--~ 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
PROBABILITY OF CORRECT 
FIGURE 10.1. Scatterplot of observed proportion correct and point-biserial correlation for 
all items on the mathematics placement test. 
students of different abilities. Suppose that one item is incorrectly answered by all of 
the students taking the exam. Then this particular item is useless in distinguishing 
between weak and strong students 
nothing would be lost if this item was removed 
from the exam. In contrast, an "ideal" item would be correctly answered by stu-
dents of above-average mathematics ability and incorrectly answered by students 
of below-average ability. This ideal item probably will not exist, but the most valu-
able test items are those that are strongly positively correlated with the students' 
mathematics proficiency. 
In this example, we are interested in learning about the student's performances 
on the exam. In addition, we wish to learn about the characteristics of each of 
the 35 items that make up the exam. The items can be characterized by their 
difficulty levels, and their discriminatory power to distinguish between students of 
different abilities. One can estimate an item's difficulty by computing the proportion 
of correct responses for all students in the sample. An item's discrimination can be 
measured by the point-biserial correlation between the item's response (1 if correct 
and 0 if incorrect) and the total score on the 35 item test. Figure 10.1 plots the 
observed proportion correct against the point-biserial correlation for all items. Note 
that the items appear to vary considerably with respect to their difficulty and also 
with respect to their discrimination characteristics. 
We can learn about the students' abilities and test characteristics by fitting an 
item response model. This model is a representation of the probability that a student 
answers a particular item correctly. The probability is a function of three unknown 
parameters - a single parameter which describes the student's ability, and two 
parameters which characterize the item's difficulty and discrimination ability. 

10. Item Response Modeling 
175 
This chapter will introduce the basic properties of the item response model, and 
describe the fitting and checking of this model from a Bayesian perspective. Section 
2 introduces the basic model in the simple setting where a single student is taking 
a test with a single item. Section 3 extends this model to the case where n students 
are taking a multiple choice test with k items. A key issue in the construction of 
a Bayesian item response model is the choice of priors. Section 4 gives theoretical 
results which indicate problems with using vague improper priors and give guidance 
on the choice of subjective prior priors on the sets of item parameters. Algorithms 
for the Bayesian fitting of one and two-parameter item response models are outlined 
in Section 5 and methods for criticizing a particular fitted model are described in 
Section 6. The methods are illustrated in Section 7 for the mathematics placement 
dataset. 
2. 
An Item Response Curve 
Suppose a student is taking a test consisting of a single multiple-choice item. 
There are two possible results 
either the student gets the item correct or incorrect. 
We let the variable y denote the observed response y = 1 denotes a correct response 
and y = 0 denotes an incorrect correct. The probability of a correct response is 
represented by the model 
Pr(y = 1) = F(a()- b), 
where() is a parameter describing the ability of the student, a and bare parameters 
that characterize the particular test item, and F is a known cumulative distribution 
function. We describe each of these quantities in turn. 
One basic assumption of this model is that there exists a single quantity, (), called a 
latent trait which represents the intrinsic ability of the student to succeed on the one 
question test. For an algebra item on the math placement test, we are assuming the 
existence of one continuous quantity(), which represents the student's basic aptitude 
in high school algebra. We are unable to directly measure a students aptitude in 
algebra, but we obtain a indication of this trait by the student's performance on 
the one question exam. 
It may be unrealistic to assume that a student's ability can be characteristic 
by means of a single latent trait. For example, in the mathematics test example, 
perhaps there exist two distinct latent traits that underlie a student's exam per-
formance, where one trait is an ability to perform arithmetic and basic algebraic 
operations, and the second trait is an ability to solve mathematical problems using 
algebra. Although the problem of detecting and measuring multiple latent traits 
is interesting, we focus here on the assumption of a single latent trait, since this 
assumption is basic for the majority of applications of item response theory. 
The cumulative distribution function F connects the continuous-valued latent 
trait() with the probability of a correct response on the item. There are two popular 
choices for the function F. IfF is assumed to be the standard logistic distribution 
F( ) 
exp(x) 
x = 1 + exp(x)' 
this leads to the logistic model 
Pr( _ l) _ 
exp(a()- b) 
y -
-
1 + exp( a() - b) 

176 
Albert & Ghosh 
IfF is chosen to be the standard normal cdf, denoted <I>(), then we obtain the probit 
model 
Pr(y = 1) = <I>(aB- b). 
The logistic and normal distributions both are symmetric about zero and have 
similar shapes Thus in typical applications of item response models, the fitted prob-
abilities using the logistic and probit models will be very similar. One advantage of 
the logistic model is its relative ease in interpretation. The logit, or log odds, of the 
probability of a correct response is 
Pr(y = 1) 
log l _ Pr(y = l) = aB - b. 
Here we will focus on the use of the probit model, since there exists an attractive 
Bayesian algorithm for fitting the model in this case. 
The parameters a and b in the model describe the characteristics of the particular 
test item. To help interpret these parameters, we consider the item response function 
!(B) = Pr(y = ljB) = F(aB- b), 
which plots the probability of a correct response as a function of the latent trait B. 
This function is typically plotted for values of 8 between -3 and 3 which corresponds 
to the range of abilities of the population of students. 
The parameter b is called the difficulty parameter. This parameter controls the 
general difficulty level of the particular test item. If one fixes the value of the 
parameter a and increases the value of b, the curve maintains its basic shape but is 
shifted to the right. A test item with a large negative value of b corresponds to an 
easy item where even students with below average abilities (corresponding to small 
values of B) have a high probability of getting the item correct. In contrast, an item 
with a large b is difficult- even strong students (large values of B) have a relatively 
small probability of a correct response. 
The second item parameter a is called the discrimination parameter. This pa-
rameter controls the slope of the item response function. If one considers an item 
response curve with fixed b, the curve becomes more steep for increasing values 
of the parameter a. A steep item response curve corresponds to an item that is 
highly discriminatory between weak and strong students. The probability of a cor-
rect response changes rapidly as the latent trait B increases in an interval about 0. 
In contrast an item with a small value of a has a flat shape. This means that the 
probability of a correct response only changes a small amount as one passes from 
weaker (small B) to stronger (large B) students. An item with a small value of a is 
a relatively poor discriminator between students of varying abilities. 
3. 
Administering an Exam to a Group of Students 
In the previous section, we considered a model for the response of an individual 
to a single test item. In the mathematics placement test example, there are 200 
students taking a test consisting of 35 items. We describe how item response theory 
is used to model this more general data structure. 
In general, suppose that there are n individuals that take an item consisting of k 
items. Each item is scored as correct or incorrect -
we represent these responses by 
the numbers 1 and 0, respectively. Let Yij denote the response of the ith individual 

10. Item Response Modeling 
177 
to the jth item on the test. So the variables Yu, ... , Yik represent the binary responses 
of the first individual to items 1, ... , k of the test, the variables Y21 , ... , y2k denote the 
test responses of the second individual, and so on. We can view the observed data 
as a matrix of n rows and k columns, where the rows correspond to individuals and 
the columns to test items. 
The first fundamental assumption in this model is that a given individual's per-
formance on the test is dependent on a single unknown latent trait or ability ()i. 
We let B1, ... , Bn denote the latent traits of the n individuals. In the math placement 
example, we are assuming that the individuals have distinct latent mathematical 
abilities that are measured on a continuous scale. 
The latent traits of the individuals are not directly observable, but we learn 
about them by means on the responses to the test items. We assume that the 
probability that the ith individual obtains the jth item correctly (Yij = 1) depends 
on the individual's ability ()i and the characteristics of the particular test item. The 
probability is modeled, conditional on a value of the latent trait, as 
(1) 
where F is a known cumulative distribution function and aj and bj are parameters 
specific to the particular item. As in the previous section, the parameter bj measures 
the difficulty of the jth item and aj represents the item's ability to discriminate 
between individuals of different abilities. 
Note that P(Yij = 1) represents the probability that an individual obtains a 
correct response to a single item. To obtain the probability that an individual gets 
a particular sequence of responses Yil, ... , Yik, we need to make the assumption 
of local independence. This assumption means that the set of k responses of the 
individual are conditionally independent, given a value of the latent trait ()i. This 
assumption does not mean that the individual's responses to different items are 
independent. It is very possible that the responses to different items are correlated, 
but this correlation is explained entirely by the latent trait of the individual. Using 
this assumption, the probability of observing an individual's entire sequence of 
responses is given by 
P(Yil, ... , Yik I ()i) = P(Yil I ()i) X ... X P(Yik I Bi) 
= n;=1 F(aj()i- bj)Yii[1- F(aj()i- bj)]l-Yii. 
To combine the data across all individuals, we assume that the responses of the n 
individuals are independent. The unknown parameters here are the vector of latent 
traits () = ( B1, ... , Bn), the vector a = ( a1, ... , ak) of item discrimination parameters, 
and the vector b = (b1, ... ,bk) of item difficulty parameters. The likelihood func-
tion of ((),a, b) is proportional to the probability of the response patterns for all 
individuals which is equal to 
n 
k 
L(B,a, b)= II II F(aj()i- bj)Yii[1- F(aj()i- bj)p-Yii. 
i=1 j=1 
The above model is described as a two-parameter item response model, since 
there are two parameters ai, bi associated with each test item. This model implic-
itly assumes that each item has a unique difficulty level. In addition, since the 
parameters a1 , ... , an are distinct, the model assumes that each item has a unique 
discrimination ability. One way to considerably simplify the two-parameter model 

178 
Albert & Ghosh 
is to assume that the items have equal discriminatory competencies -
without loss 
of generality, we can assume that a1 = ... = ak = 1. The probability of response 
of the ith individual to the jth item for this one-parameter item response model is 
then given by the simpler expression 
When F is chosen to be the logistic distribution, the one-parameter item response 
model is called the Rasch model. In the explanation of the fitting and model check-
ing procedures, we will focus on the two-parameter model and indicate how the 
described methods can be adjusted for the simpler one-parameter model. 
4. 
Prior Distributions 
After the likelihood function has been defined, the next task is to define prior 
distributions for the vector of latent traits () and the item parameter vectors a and 
b. 
4.1 
Noninformative Priors and Propriety of the Posterior 
Distribution 
Much of the existing Bayesian literature on item response models deals with 
noninformative improper priors for { Bi}, { aj} and { bj}. This is especially tempt-
ing when the prior information is vague, and the Bayesian paradigm can often be 
used effectively by the introduction of noninformative priors. However, this leads to 
the possibility of improper posteriors. Proper posteriors are vital for any Bayesian 
analysis, since otherwise, there is no meaning of posterior moments, quantiles and 
credible intervals. The impropriety of posterior is not always easy to recognize un-
less calculations are done analytically. For instance, it may so happen that all of 
the conditionals needed for the implementation of MCMC algorithms such as the 
Gibbs sampler are proper, and yet the posterior of interest is improper. 
For the two-parameter item response model, a general class of priors is given by 
n 
k 
k 
1r( e, a, b) <X II 91i( ei) II 92j(aj) II 93k(bj ), 
(2) 
i=l 
j=l 
j=l 
where the densities {g1i}, {92j}, {93j}, are proper or improper pdf's. The joint 
posterior density of ( (), a, b) is given by 
1r(B, a, bly) <X L(B, a, b)1r(B, a, b). 
We now state several theorems which show that some typical improper priors for 
the parameters will lead to improper posteriors. 
Theorem 1. Consider the prior given in (1.2), where at least one of the g2j is 
improper. Let F(x) = 1- F(x). Then the joint posterior of (B, a, b) is improper. 
Proof: Consider the computation of the marginal posterior density of the item 
parameters (a, b) if we define 

10. Item Response Modeling 
179 
for 1 ~ i ~ n, then 
n 
k 
k 
7r(a, bly) <X IT Ii(a, b) IT Y2j(aj) IT Ysk(bj)· 
i=l 
j:l 
j=l 
For definiteness, assume that g21(al) is improper. Now for each fixed i, Yil = 1 or 
0. Hence, each integral Ji(a,b) contains a term F(a1Bi- b1) or 1- F(a1Bi- bl), 
but not both. In the first case, 
Ji(a,b) > f0
00 F(a1Bi _:_bl)TI7=2[FY•i(ajBi -bj)F1-Yii(ai()i -bj)]Yli(Bi)dBi 
> 
F(-b1) f~oo TI7=2[FY•i(aj()i- bj)F1-Y'i(ai()i- bj)]Yli(Bi)dBi, 
while in the second case 
Ii(a, b) 
~ fo
00 F(al()i- bl) n;=2[FYii(aj()i- bj)F1-Yii(aj()i- bj)]Yli(Bi)d()i 
~ 
F(-bl) f~oo n7=2[FY•i(aj()i- bj)_Fl-Yii(aj()i- bj)]gli(Bi)d()i· 
From above, it follows that for each i, there is a nonnegative lower bound for Ii(a, b) 
which does not involve a1 . Integrating this lower bound with respect to the marginal 
(improper) pdf g21(al) over (0, oo), one gets 
This implies that the posterior density 7r(a, bly) is improper, and hence 7r(B, a, bly) 
is also improper. 
It is interesting to note, however, that all of the full conditional posteriors are 
proper in this case. Thus the impropriety of the posterior may not be detected in the 
implementation of a Gibbs sampling algorithm. To show that the full conditionals 
are proper, notice that 
k 
11'( BdB1( l f:: i), a, b, y) <X IT [FY'i ( ajBi - bj )F1-Y'i ( ai ()i - bi )]gli( ()i) 
j=l 
which is typically integrable with respect to ()i· Similarly, 11'(ajlam(m f:: j),B, b,y) 
and 7r(bilbm(m f:: j),B,a,y) may all be proper. A simple example is a normal or a 
logistic F and gli, Y2j are uniform priors over the real line, while Y3i are uniform 
over (0, oo ). 
Swaminathan and Gifford (1985, p. 353) suggest that flat priors for B, a and 
b make the Bayesian analysis comparable to a likelihood-based analysis. But the 
Bayesian analysis is actually questionable, since the posterior is improper in that 
case. 
The above fact is a consequence of a more general result stated below. 
Theorem 2 Consider the prior 7r(B, a) <X g(a), where g is an arbitrary positive 
function of a (for example g could be a proper pdf for a). Then the posterior 
distribution is always improper. 

180 
Albert & Ghosh 
4.2 
Choosing an Informative Prior 
The theorems in the previous section show that noninformative priors for item 
response problems often lead to improper posteriors. Thus it is desirable to choose 
proper prior distributions to ensure that the posterior distribution is also proper. 
The prior distributions discussed here have two primary purposes. First, the choice 
of a prior distribution on the latent traits resolves particular identification problems 
in the two-parameter model. Second, informative prior distributions placed on the 
item response parameters can be used to reflect the prior belief that the values of 
the item parameters are not extreme. 
The two-parameter item response model, as defined in the previous two sections, 
is overparameterized. Looking at the expression for the probabilities ( 1.1), note that 
we can multiply the latent abilities { ()i} by a given positive constant, and divide all 
of the slope parameters { aj} by the same positive constant and preserve the model. 
One solution to this overparameterization problem is to place a constraint on the 
latent traits, such as 2.:~ 1 ()i = 0. Another solution to this problem is to place a 
known prior distribution on the vector of latent traits. Following common practice, 
we assign ()1 , .. , Bn a normal distribution with mean 0 and standard deviation 1. 
This prior is making the implicit assumption that all of the latent abilities of the 
individuals fall in the interval ( -3, 3). 
As discussed in the previous section, proper priors need to be chosen for the item 
parameters to ensure that the posterior distribution is proper. This will be most 
important in the case where extreme data is observed where students are observed 
to get correct or incorrect answers to every item. In the common situation where 
little prior information is available about the difficulty parameters, one can assume 
that the bj are independent, where bj is distributed N(O, sb), where Sb is chosen to be 
a large value. This choice of prior will result in a proper posterior distribution when 
extreme data is observed, and will have a modest effect on the posterior distribution 
for nonextreme data. 
Proper distributions on the discrimination parameters { aj} can be constructed 
based on prior knowledge about the shape of the item response curve. One assump-
tion implicit in the graphs of the item response curve is that the probability of 
a correct response to an item is an increasing function of the latent trait, which 
means that individuals with higher abilities are more likely to get the item correct. 
The item response curve for item j will be increasing when the slope parameter ai 
is positive. Thus, it is reasonable to state a priori that all of the slope parameters 
are positive with high probability. One can model this prior belief by the choice of 
different prior distributions. One could assume that the parameters a1, ... , a~c are 
independent, where aj is assigned a Gamma distribution with known hyperparam-
eter values. Since the support of the Gamma distribution is on positive values, this 
prior is stating that the parameter is positive with probability one. Alternately, one 
can assign aj a normal distribution with mean I' a and standard deviation Sa. By 
choosing a positive value for the normal mean Jta and an appropriate value for sa, 
one can model the belief that ai is positive with high probability. In the following, 
we will use a normal prior on aj since it leads to a simple fitting procedure. 

10. Item Response Modeling 
181 
5. 
Bayesian Fitting of Item Response Models 
5.1 
Fitting of the Two-parameter Model Using Gibbs Sampling 
The prior distribution discussed in the previous section is summarized as follows: 
• The latent abilities fh, ... , Bn are a random sample from a standard normal 
distribution. 
• The item slope parameters a1 , ... , ak and intercept parameters b1, ... , b k are 
independent random samples from normal(O, sa) and normal(J.tb, Bb) distribu-
tions, respectively. 
Combining this prior density with the likelihood function, the posterior density is 
expressed, up to a proportionality constant, as 
g(B, a, bldata) 
= L(B, a, b) TI?=t ¢(Bi; 0, 1) n;=1 ¢( aj; 0, sa)¢(bj; 0, Bb), 
where ¢(x; J.t, u) denotes the normal density with mean J.t and standard deviation 0'. 
One can fit this Bayesian two-parameter item response model by simulating a 
large sample of values from the joint posterior distribution of the latent abilities 
and the item parameters ((),a, b). This sample of values is generated by simula-
tion from a Markov Chain Monte Carlo (MCMC) algorithm. This algorithm will 
produce a correlated sequence of random variables in which the jth value in the 
sequence, ( ()(i), a<n, b(j)) is dependent on the (j -1 )st value ( ()U -t), aU- 1), bU - 1)). 
The MCMC algorithm is constructed by specifying a starting value for the pa-
rameter vector and stating a method for moving from one simulated iterate in the 
sequence to the next. Under general conditions, the distribution of the jth iterate, 
as j approaches infinity, will converge to the posterior distribution of interest. If a 
large number of iterations are performed, the last group of iterates in the sequence, 
say { ( ()(i), a<n, bU)), j = m0 , ... , m0 + m}, will approximate a dependent sample 
from the posterior distribution. 
One general strategy for the construction of a MCMC algorithm is based on the 
notion of Gibbs sampling. This simulation algorithm partitions the random vari-
able into blocks, and iteratively samples each block of parameters from a posterior 
density which conditions on the most recent simulated values of all parameters not 
included in the block. In this setting suppose that we partition the parameter set 
into the vector of latent abilities () and the vectors of item parameters (a, b). Then 
the Gibbs sampling algorithm will iterate from the posterior distribution [Bia, b] of 
latent abilities conditional on fixed values of the item parameters, and the poste-
rior distribution [a, biB] of item parameters conditional on the latent abilities. We 
examine each conditional posterior distribution below. 
Suppose first that the item parameters are known. Then the posterior density of 
the latent traits, conditional on the item parameters, is given by 
g(Bia, b, data) 

182 
Albert & Ghosh 
Note that the individual latent traits are conditionally independent, with the ith 
latent trait distributed according to the density 
k 
g(Bda, b, data)= IT F(aiei- bi)Yii[1- F(ajBi- bi)p-Yiiq)(Bi; 0, 1). 
j=l 
Next, let's reverse the roles of the two sets of parameters and suppose that the 
latent abilities are known. Then the posterior density of the item parameters, con-
ditional on these latent traits, is given by 
g(a, biB, data) 
We see that this conditional density factors into independent components, where the 
set of parameters corresponding to the jth item, ( aj, bi), is distributed according 
to the density 
g(aj, biiB, data) 
= il?= 1 F(ajBi- bj)Yii(1- F(ajBi- bj)]l-Yiiq)(ai;O,sa)¢(bj;O,sb)· 
The convenient factorizations of the two sets of conditional posterior distribu-
tions suggests the following Gibbs sampling algorithm for simulating from the joint 
posterior distribution. 
1. (Choose starting values.) Choose reasonable initial values for the item param-
eters 
call these values {(a)0),b)0)),j = 1, ... ,k}. These values can be esti-
mated from the data, as illustrated in the discussion of the example below. 
Set the iteration number m = 0. 
· 
2. (Simulate the latent traits.) Simulate independent values of the latent traits 
81 , ... , Bn, where Bi is simulated from the conditional posterior density 
g(B;ja(m), b(m), data) 
= n~ 
F(a~m)(J.- b~m))Yii(1- F(a~m)e.- b~m))] 1 -Yii.+.(B·· 0 1) 
J=l 
J 
z 
J 
J 
z 
J 
'fJ 
z, , 
• 
Denote the vector of simulated latent traits by B = ( B~m), ... , B~ )) . 
3. (Simulate the item parameters.) Simulate independent k sets of the item 
parameters, where ( ai, bj) is simulated from the conditional density 
g( aj, bj IB(m), data) 
= n~=l F( ajO~m) - bj )Yii (1- F( ajO~m) - bj )]l-Yii q)( aj; 0, sa)¢(bj; 0, sb)· 
Denote the simulated item parameters as (alm), blm)), ... , (a~m), b~m)), and the 
vectors a(m) = ( a~m), ... , a~m)), b(m) = (blm), ... , b~m)). 
4. (Iterate.) Update the counter m = m + 1 and return to step 2. 

10. Item Response Modeling 
183 
5.2 Implementation of Gibbs Sampling for General F 
A general algorithm for simulating from the above conditional posterior distri-
butions, for any choice of the link function F, is based on the Metropolis-random 
walk method. To describe this simulation algorithm, suppose that one is interested 
in simulating from the posterior density g which is given by 
g( B) = I< h( B), 
where I< is an unknown proportionality constant. Let B(m) denote the current simu-
lated value from the density g. We propose a new simulated value BP from g, where 
BP is obtained from the current value by adding a normal variate with mean 0 and 
standard deviation c. Using symbolic notation, 
BP = B(m) + cZ, 
where Z is a standard normal variate. Next, we compute an acceptance probability 
PRO B, which is equal to the minimum of 1 and the quotient of the posterior density 
evaluated at the proposal value and the posterior density evaluated at the current 
value. 
. ( 
h(BP) ) 
P ROB= mm 1, h(B(m)) 
. 
To complete the algorithm, we simulate a uniform variate U. If U is smaller than 
P ROB, we accept the proposal value and the next value of B in the sequence is 
B(m+l) = BP. Otherwise, if U 2:: PROB, we reject the proposal value and the next 
simulated value remains at the current value B(m+l) = B(m). 
This basic Metropolis method can be used to simulate from all of the condi-
tional posterior distributions in the fitting algorithm for the two-parameter item 
response model. In step 2 of the algorithm, values of the latent traits B1, ... , Bn are 
simulated independently using parallel Metropolis steps using normal proposal den-
sities with known scale factors c1 , ... , en. Values of the k pairs of item parameters 
( a1, b1), ... , ( a,t, b~c) are simulated independently from a bivariate version of the above 
Metropolis algorithm. Given a current value at the j pair, (a)m), b)m)), a proposal 
pair can be generated by adding normal random variables to each component: 
As in the single variable Metropolis case, the pair (a~, b~) is accepted or rejected as 
the next simulated value in the sequence. The probability that the pair is accepted 
depends on the ratio of the posterior density evaluated at the proposal pair and the 
density evaluated at the current pair. 
In the implementation of this Gibbs sampling/Metropolis algorithm for simulating 
a sample from the posterior density, some care should be taken in the choice of the 
normal scale parameters { ci} and {( Cai, Cbi)}. Generally, it is desirable to choose 
values of these scale parameters so that the acceptance rate in each step of the 
algorithm is between 25-50 percent. This can be accomplished by choosing the 
scale to be approximately twice the standard deviation of the conditional posterior 
distribution being sirp.ulated. In practice, one can perform two runs of the algorithm. 
The first run with default values of the scale parameters can be used to estimate the 
posterior standard deviations of each parameter. The algorithm is then run again 
using new values of the scale parameters determined from these estimated standard 
deviations. 

184 
Albert & Ghosh 
5.3 Gibbs Sampling for a Probit Link Using Data Augmentation 
In the case of a probit link function (F = <I>), an alternative MCMC algorithm 
can be implemented by the use of data augmentation and Gibbs sampling. Corre-
sponding to each binary response Yii, define a continuous variable Zij such that 
and Yij indicates if the sign of Zij is positive or negative: 
Yii = 1 if Zii > 0, and Yii = 0 if Zii ~ 0. 
One can show that this formulation leads to the model Pr(Yii = 1) = <l>(ajfh -
bi ). In the mathematics placement test example, one can interpret Zij as a latent 
mathematics ability, measured on a continuous scale, that underlies the student's 
performance on the particular test item. Note that Zij is not observed, but we get 
indications of the sign of Zij by means of the binary response Yii. 
Suppose that we augment the current set of parameters { fh}, { ai}, { bi} with the 
unobserved vector of latent variables Z = (Zu, ... , Zjk)· The joint posterior density 
of all model parameters and latent variables is given by 
g(Z, (),a, bjdata) 
oc 
TI?:l n;=l [¢(Zij' ffiij' 1)/*(Zij' Yij )] 
where the normal mean ffiij = ai ()i - bj, and I* ( c, d) is equal to one when { c > 
0, d = 1} or { c < 0, d = 0}, and equal to zero otherwise. 
On the surface, the inference problem now appears to be more complicated, since 
the posterior density is a function of n + 2k parameters and nk values of the latent 
variables. However, this new representation of the posterior distribution leads to a 
simple Gibbs sampling scheme for simulating from the joint posterior distribution. 
Suppose that we partition the unknown quantities into three groups, the latent 
variables Z, the ability parameters (), and the item parameters a, b. The posterior 
distribution of each group, conditional on values of the remaining parameters, have 
familiar functional forms and are easy to simulate. 
1. (Conditional distribution of latent variables.) Suppose that values of the abil-
ity parameters and item parameters are held fixed. Then Zu, ... , Znk have 
independent truncated normal distributions. The conditional posterior distri-
bution of Zij is truncated normal with mean ffiij = ai ()i - bj and standard 
deviation 1. The truncation of this posterior distribution depends on the value 
of the corresponding binary observation Yii. If the observation is a success 
(Yii = 1), the truncation of Zij is from the left at 0; if the observation is a 
failure (Yii = 0), the truncation is from the right at 0. 
2. (Conditional distribution of ability parameters.) Suppose that the latent vari-
ables and item parameters are fixed. We can write the model for the latent 
variables as 
Zij + bj = aj()i + eij, 
where eij are independent error terms with a standard normal distribution. 
Note that for fixed values of Z and a, b, this is a normal linear modeL Combin-
ing the normal likelihood with the normal prior, one can show that ()1 , ... , ()n 
have independent normal posterior distributions. 

10. Item Response Modeling 
185 
3. (Conditional distribution of item parameters.) Last, we consider the distribu-
tion of the item parameters { aj, bj} conditional on the values of the latent 
variables and the latent traits. Rewrite the latent variables model as 
z~) = aj8~t)- bj + e;,j. 
Since the values of the latent data { Zij} and the latent traits { 8.;.} are fixed, 
this (for a fixed value of j) can be viewed as a normal linear model with un-
known parameter vector ( aj, bj) and known covariate vector ( 8~ t), -1). Again 
this can be combined with the normal priors on aj and bj to obtain indepen-
dent bivariate normal posterior densities for { ( aj, bj}), j = 1, ... , k. 
The three tractable conditional posterior distributions lead to an attractive Gibbs 
sampling algorithm in the pro bit link case. Each iteration of the algorithm consists of 
three steps. The first step simulates latent variables Z from i,ndependent truncated 
normal distributions conditioning on the current values Qf the item and ability 
parameters. The second step simulates values of the abiVty parameters and the 
third step simulates item parameters. In each of the last tWo steps, simulations are 
based on the posterior distribution that conditions on the latent variables and the 
remaining parameters. 
This algorithm is somewhat more time-consuming to run than the Gibbs sam-
pling/Metropolis algorithm, due to the extra simulation of the latent variables. 
However, the algorithm does not depend on the assignment of scale parameter val-
ues in the Metropolis algorithm that may determine the rate of convergence of 
the algorithm. One attractive feature of the probit data augmentation algorithm is 
that, due to the underlying normal linear model, it is straightforward to generalize 
this algorithm to a multinomial response regression model where the categories are 
ordered or unordered. 
5.4 
Bayesian Fitting of the One-parameter Model 
Since the one-parameter item response model is a special case of the two-parameter 
model with a1 = ... = ak = 1, the Gibbs sampling algorithm outlined in the previ-
ous section can also be used to fit the one-parameter model. The Gibbs algorithm 
will alternately simulate values of the latent traits conditional on values of the item 
difficulty parameters, and simulate values of the item parameters conditional on 
the current values of the latent traits. In the case of a general link function F, the 
Metropolis within Gibbs algorithm can be used. In the case of a probit link function, 
the Gibbs/ data augmentation scheme provides an attractive scheme for fitting this 
model. 
6. 
Inferences from the Model 
In many applications of item response modeling, the focus of the analysis is on 
the characteristics of the test items. The jth item on the test is quantified in terms 
of the slope parameter aj, which describes the discrimination ability of the item, 
and the intercept parameter bj, which describes the item's difficulty. To aid in the 
interpretation of the item characteristics, the slope and intercept parameters can 
be transformed to new parameters which are more easily interpreted. 

186 
Albert & Ghosh 
In the case of a probit link function, one useful description of the discrimination 
ability of the jth item is given by the biserial correlation, defined as 
a· 
r; = vl:a{ 
This quantity measures the correlation between the binary responses {y1j, ... , Yni} 
and the latent traits {t'h, ... , On}· It can be interpreted much like a standard cor-
relation coefficient. If ri is a large positive number, then this particular item is an 
effective discriminator between weak and strong students. In contrast, a value of 
ri near zero indicates that the test item provides little information regarding the 
ability of the student. 
It can be hard to interpret the difficulty parameter bi since it is not expressible 
on the probability scale. An alternative measure of difficulty is the probability Pi 
that a randomly chosen individual from the population obtains a correct response 
to the jth question. In the case where the latent traits { t'Ji} are assumed distributed 
from a standard normal distribution 
( 
-b· 
) 
Pi =<l> ~
3 
• 
1 +a~ 
J 
The discrimination and difficulty characteristics of the jth item can be analyzed 
by inspection of the marginal posterior distributions of the correlation ri and the 
probability Pi. In addition, one is interested in learning about the item characteristic 
curve for the item. For an individual with the latent trait t'J, the probability of a 
correct response on the jth item is given by the probability 
One can learn about this probability by means of its marginal posterior distribution. 
The result of the Bayesian fitting of the item response model is a simulated sample 
{ t'J(m), a(m), b(m)} from the joint posterior distribution of the latent traits and item 
parameters. From this sample, one can obtain a simulated sample from the marginal 
posterior distribution of any function /(a, b) by applying this function to each of 
the simulated parameter values. In particular, one can obtained simulated samples 
for the item correlations { ri} and the item difficulty parameters {Pi}. The posterior 
distributions for these parameters can be summarized by the use of posterior means 
and by probability intervals. 
7. 
Model Checking 
7.1 
Bayesian Residuals 
After a particular item response model has been fit, one is interested in assessing 
the closeness of the observed data with the fitted probabilities from the model. If 
Pii = F(ait'Ji -bi) denotes the probability that individual i responds correctly to the 
jth item, one definition of a residual is the difference between the observed binary 
response Yii and the response probability 
rii = Yii - Pii · 

10. Item Response Modeling 
187 
From a Bayesian perspective, after observing data, the location of the fitted proba-
bility Pij is described by its posterior probability distribution and the observed Yii 
is a constant. Thus the residual rij has a posterior probability distribution which 
can be summarized to learn if this particular observation is not well fit by the 
model. Observations for which the residual distribution is located away from zero 
may indicate some lack-of-fit. 
In practice, it can be difficult to identify unusually large values of rij due to the 
binary nature of the response variable. For the purpose of model checking, it can be 
helpful to group the data by latent ability. Suppose that we group the individuals 
by their estimated latent abilities. For the latent ability class g with midpoint (}0 , we 
can compute the proportion of individuals y0 that answer correctly to a particular 
item. If p0 denotes the probability that an individual with latent ability 80 obtains a 
correct answer, then we can inspect the posterior distributions of the group residuals 
r0 = y0 -p0 • Johnson and Albert (1999), Chapter 6 illustrates the use of these group 
residuals to demonstrate the lack-of-fit of the one-parameter model for a sociological 
application. 
7. 2 
Posterior Predictive Checks 
A different strategy for model checking is based on the posterior predictive dis-
tribution. In the Bayesian model, data y is observed from the sampling density 
P(yiB, a, b), and the parameters have a prior g(B, a, b). Inferences about the param-
eters is based on the posterior density g(B, a, bly). Suppose that the same mathe-
matics placement test is administered to a new sample of n students. Let y denote 
the response matrix of this future sample. The probability function of this future 
data, called the posterior predictive density, is computed by averaging the sampling 
density of y over the posterior distribution of the parameters: 
P(yly) = j P('YIB,a, b)g(B,a, bjy)dadbdB. 
This predictive density represents typical data generated from the fitted model. 
If the observed data y is not representative of data y from the posterior predictive 
distribution, then one has doubt that y is generated from the model. Generally, it 
is hard to detect if y is a typical value from the distribution P(yly) since the data 
values y and y are multidimensional. However, one can often construct a testing 
function T(y) which measures some aspect of the data which may not be consistent 
with the stated model. One can compute the posterior predictive distribution of 
T(y) to see what values ofT are predicted from the model. If the observed value of 
T, T(y), is unusual relative to its posterior predictive distribution, then this casts 
doubt on the suitability of the item response model. One can measure unusualness 
by the computation of a p-value 
P(T(y) ~ T(y)). 
If this posterior predictive p-value is small, then this indicates that it is unlikely 
that the assumed model can generate data like the one that was observed. 
In practice, the posterior predictive distribution is computed by simulation. We 
can simulate one set of future data y by a two-step process: ( 1) simulate parameters 
(},a, b from their posterior distribution and (2) simulate data y from the sampling 
density P(yiB, a, b), where one is conditioning on values of the simulated param-
eters. If this process is repeated a large number of times, one obtains a sample 

188 
Albert & Ghosh 
of simulated values of y, and the posterior predictive distribution of the checking 
function T(y) can be summarized by means of a histogram. The location of the 
observed value T(y) on this histogram is informative about the consistency of the 
observed data with the model. See Gelman, Meng and Stern (119) for a general 
discussion on the use of the posterior predictive distribution in model checking. 
8. 
The Mathematics Placement Test Example 
The two-parameter item response model with a probit link function was fit to 
the mathematics placement exam described in Section 1. The hyperparameters of 
the normal distributions were chosen to be Sa = Sb = 1 and Jla = 0. The Gibbs 
sampling/ data augmentation simulation described in Section 5.2 was run for a total 
of 5000 iterations. We focus on the estimation of the item response curve for each 
of the 35 items on the test. 
0.9 
0.8 
0.7 
~ 0.6 
~ 
~ 0.5 
~ 0.4 
0.3 
0.2 
0.1 
0 
15 
23 
25 
2 
5 
24 
6 
8 4 
19 
35 
7 
30 
11 
20 
14 9 
3 
13 
18 28 26 
33 
29 
34 
1 
27 
10 
1632 
12 
22 
31 
17 
21 
~------J---------~--------~----------~--------~--
-1 
-0.5 
0 
DIFFICULTY 
0.5 
FIGURE 10.2. Scatterplot of posterior means of difficulty parameters {b3 } against posterior 
means of discrimination parameters { aj} for all items. 
To summarize the characteristics of all of the test items, Figure 10.2 plots the 
posterior means of the slope parameters { aj} against the posterior means of the 
intercept parameters { bj} for all items. The plotting symbol used in the figure is 
the item number on the test; the bold numbered items are questions on content in 
intermediate algebra and the remaining points correspond to questions on college 
algebra. This figure confirms the comments made in the initial exploration of the 
data in Section 1. The items appear to differ substantially in terms of their diffi-
culty and discrimination levels. The questions on intermediate algebra appear to be 

10. Item Response Modeling 
189 
ITEMS 
ITEM 10 
I=' 0.8 
I=' 0.8 
····· 
frl 
frl 
····· .... 
~ 0.6 
~ 0.6 
a 
a 
m 0.4 
m 0.4 
0 
0 
a: 
a: 
D. 0.2 
D. 0.2 
0 .. 
0 
-3 
-2 
-1 
0 
1 
2 
3 
-3 
-2 
-1 
0 
1 
2 
3 
LA TENT TRAIT fJ) 
LATENT TRAIT fJ) 
ITEM 15 
ITEM 21 
F=" 0.8 
F=" 0.8 
frl 
frl 
~ 0.6 
~ 0.6 
0 
0 
Q. 
0 
m 0.4 
aro.4 
0 
0 
..... 
a: 
a: 
.... ············ 
-
D. 0.2 
D. 0.2 r-
.... ··········· 
······· 
.... 
0 
····· 
0 
-3 
-2 
-1 
0 
1 
2 
3 
-3 
-2 
-1 
0 
1 
2 
3 
LA TENT TRAIT fJ) 
LATENT TRAIT fJ) 
FIGURE 10.3. Posterior medians and 5th and 95th percentiles of the posterior distribution 
of the probability of a correct response Pj(O) for four items using a two-parameter item 
response model. 
generally easier than the questions on college algebra. The lower right point on the 
graph, corresponding to question 21, has an unusually small estimated discrimina-
tion parameter. 
Let's focus on four items on the test, questions 5, 10, 15, 21, which have distinctive 
characteristics in terms of difficulty and discrimination. 
Figure 10.3 graphs the posterior median of the probability of a correct response 
Pj (B) for a sequence of values of the latent trait () between -3 and 3. In addition 
to the solid line that represents the posterior median, this figure also displays the 
5th and 95th percentiles of the posterior distribution of this probability. The fitted 
response curves of items 5 and 15 correspond to questions that are relatively effective 
in discriminating between students of different ability. In contrast, items 10 and 21, 
with relatively flat item response curves, are poor discriminators. It is interesting 
to note that the confidence bands are wider for extreme values of the latent trait () 
this reflects more uncertainty in estimating the probability of correct response 
for these extreme students. 
The two-parameter model assigns each item a distinctive slope parameter aj, 
which assumes that each item possesses a unique ability to discriminate between 
students of different ability. A far simpler model is the one-parameter item response 
model which assumes that all items have the same discrimination ability. 
This one-parameter model was also fit to the math placement dataset and the 
fitted item response curves for items 5, 10, 15, 21 are displayed in Figure 10.4. 

190 
Albert & Ghosh 
ITEMS 
1- 0.8 
frl 
~ 0.6 a 
~ 0.4 
D. 0.2 
o~----~----_.------~----~ 
-4 
-2 
0 
2 
4 
LATENT TRAIT f)) 
ITEM 15 
1- 0.8 
frl 
~ 0.6 a 
~ 0.4 
D. 
0.2 
o~------~~_.------~----~ 
-4 
-2 
0 
2 
4 
LATENT TRAIT f)) 
ITEM 10 
6 
w 
0.8 
~ 0.6 a 
~ 0.4 
D. 0.2 
o~~~~----~------~----~ 
-4 
1- 0.8 
frl 
~ 0.6 
0 2. 
~ 0.4 
D. 0.2 
-2 
0 
2 
4 
LATENT TRAIT f)) 
ITEM 21 
0~----~~~--------._----~ 
-4 
-2 
0 
2 
4 
LATENT TRAIT f)) 
FIGURE 10.4. Posterior medians and 5th and 95th percentiles of the posterior distribution 
of the probability of a correct response P3(8) for four items using a one-parameter item 
response model. 

10. Item Response Modeling 
191 
These fitted curves are much different in appearance than the corresponding curves 
in Figure 10.3. The curves all have the same shape and differ only in their location. 
Also, note that the confidence bands for the probability of a correct response are 
much narrower than the bands for the fitted two-parameter model, especially for 
the small and large values of the latent trait (). 
Since the fitted item response curves for the one and two-parameter models are so 
different for these four items, this raises the issue of model fit. In particular, is the 
observed dataset consistent with the assumption of a one-parameter model which 
has only a single discrimination parameter? In Figure 10.1, we noted the high vari-
ability of the point-biserial correlations for the 35 items. Is this high variability of 
these observed discriminations consistent with the assumption of a one-parameter 
model? We answer this question by use of the posterior predictive distribution. 
Future samples of test results y are simulated from the posterior predictive distri-
bution f(yjy), where the parameters {(), b} are simulated from the one-parameter 
item response model. For each simulated sample of test results, we compute the set 
of point-biserial correlations, and summarize the variability of these correlations by 
use of a sample standard deviation. One thousand future datasets were simulated 
and Figure 10.5 displays a histogram of the standard deviations of the correlations 
for all of these datasets. Note that a typical value of this standard deviation is 
.06. The observed standard deviation of the point-biserial correlations, .0935, is 
in the extreme right-tail of this distribution. The probability of observing a cor-
relation value at least as large as this value is approximately zero. This indicates 
that the observed variation in the discriminations of the 35 items is inconsistent 
with the assumption of a one-parameter model. This brief analysis suggests that 
the two-parameter model may be more suitable than the one-parameter model for 
describing this mathematics placement dataset. 
9. 
Further Reading 
Although this chapter has focused on the Bayesian fitting of item response mod-
els, there is a broad literature on the classical fitting and description of this class 
of models. Hambleton and Swaminathan (1985), Baker (1992) present general re-
views of classical methods, and van der Linden and Hambleton (1997) illustrate the 
generalization of item response theory for categorical responses. Bock and Aitkin 
(1981) illustrate the use of the EM algorithm for computing maximum likelihood 
estimates of the item parameters. 
Early Bayesian analyses of item response models are found in Swaminathan and 
Gifford, J. A. (1982, 1985) and Tsutakawa and Lin (1986). The Gibbs sampler 
(Gelfand and Smith, 1990), and related Metropolis-Hastings type algorithms (Chib 
and Greenberg, 1995) have revolutionized Bayesian computation. The use of data 
augmentation and Gibbs sampling to fit probit models is described in Albert and 
Chib (1993). Albert (1992) and Bradlow et al (1997) illustrate the use of Gibbs 
sampling for modeling probit item response models and Patz and Junker (1997) 
demonstrate the use of Metropolis within Gibbs simulation algorithms for fitting 
item response curves with a logistic link. Ghosh et al (1997) discuss the choice of 
noninformative prior distributions to ensure propriety of the posterior distribution. 
There is less literature available on model checking of item response curves from a 
Bayesian viewpoint. Gelman et al (1995) give a general discussion of the use of the 
posterior predictive distribution in model criticism, and Johnson and Albert (1998) 

192 
Albert & Ghosh 
140 
120 
100 1-
80 
60 
40 .. 
20 1-
0 
0.03 
J 
0.04 
..--
..--
-
0.05 
r--
1--1--
r--
1--
I--
-
h 
., 
0.06 
0.07 
0.08 
0.09 
0.1 
STANDARD DEVIATION 
FIGURE 10.5. Histogram of the posterior predictive distribution of the standard deviation 
of the future point-biserial correlations based on the one-parameter item response model. 
The observed value of the standard deviation of the correlations is represented by a vertical 
line. 

10. Item Response Modeling 
193 
discuss the use of Bayesian residuals in checking item response models. 
References 
Albert, J. H. (1992). Bayesian estimation of normal ogive item response curves 
using Gibbs sampling. Journal of Educational Statistics, 17, 261-269. 
Albert, J. H. and Chib, S. (1993). Bayesian regression analysis of binary and poly-
chotomous response data. Journal of the American Statistical Association, 88, 
657-667. 
Baker, F. B. (1992). Item response theory: Parameter estimation techniques. New 
York: Marcel Dekker. 
Bock, R. D. and Aitkin, M. (1981). Marginal maximum likelihood estimation of 
item parameters: Application of an EM algorithm. Psychometrika, 46, 443-
459. 
Bradlow, E. T., Wainer, H., and Wang, X. (1997). A Bayesian random effects 
model for testlets. manuscript. 
Chib, S., and Greenberg, E. (1995). Understanding the Metropolis-Hastings algo-
rithm. The American Statistician, 49, 327-335. 
Gelfand, A. E., and Smith, A. F. M. (1990). Sampling-based approaches to calcu-
lating marginal densities. Journal of the American Statistical Association, 85, 
398-409. 
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (1995). Bayesian Data 
Analysis. New York: Chapman and Hall. 
Gelman, A., Meng, X. L., and Stern, H. S. (1996). Posterior predictive assessment 
of model fitness via realized discrepancies. Statistica Sinica, 6, 733-807. 
Ghosh, M., Ghosh, A., Chen, M., and Agresti, A. (1997). Bayesian estimation for 
item response models. manuscript. 
Hambleton, R. and Swaminathan, H. (1985). Item response theory: principles and 
applications. Boston: Kluwer. 
Johnson, V. and Albert, J. (1999). Ordinal data modeling. New York: Springer-
Verlag. 
Patz, R. J. and Junker, B. W. (1997). A straightforward approach to Markov chain 
Monte Carlo methods for item response models. manuscript. 
Swaminathan, H. and Gifford, J. A. (1982). Bayesian estimation in the Rasch 
model. Journal of Educational Statistics, 7, 175-192. 
Swaminathan, H. and Gifford, J. A. (1985). Bayesian estimation in the two-parameter 
logistic model. Psychometrika, 50, 349-364. 
Tsutakawa, R. K. and Lin, H. Y. (1986). Bayesian estimation of item response 
curves. Psychometrika, 51, 251-267. 
van der Linden, W. J. and Hambleton, R. K. (Eds.) (1997). Handbook of modern 
item response theory. New York: Springer-Verlag. 

This Page Intentionally Left Blank 

11 
Developing and Applying Medical 
Practice Guidelines Following 
Acute Myocardial Infarction: A 
Case Study Using Bayesian Probit 
and Logit Models 
Mary Beth Landrum 
Sharon-Lise Normand 
ABSTRACT Measuring the quality of care delivered to patient populations com-
prises one of several foci in health services research. For example, a plethora of 
studies have indicated that there are large regional variations in the use of many 
surgical procedures. Observations such as these raise the question as to whether 
there is overuse (or underuse) of surgical procedures within particular strata. To 
answer this question, however, it is necessary to define an explicit standard of care 
who should receive the surgical procedure and who should not? In this Chapter, us-
ing ratings elicited from a multidisciplinary panel of medical experts, we estimate a 
Bayesian ordinal probit model to build an explicit guideline of care for the appropri-
ateness of coronary angiography (an invasive diagnostic procedure) for 890 clinical 
scenarios following a heart attack. Utilizing the posterior distribution of the appro-
priateness scores that comprise the guideline, we profile 294 hospitals who treated 
5998 Medicare patients who suffered a heart attack. We compare adherence to the 
expert-derived guidelines across hospitals by estimating the relationship between the 
use of angiography and the patients' appropriateness for the procedure. We also ex-
amine provider and patient characteristics that impact adherence using hierarchical 
logistic regression models. While we found that most hospitals did treat patients 
in accordance with expert opinion, we also demonstrated that incorporating uncer-
tainty in guidelines based on expert opinion results in more conservative conclusions 
regarding quality of care. 
1. 
Background and Significance 
Concerns surrounding rising health care costs, an aging US population, and the 
implementation of many new managed health care organizations have prompted 
195 

196 
Landrum & N armand 
health care consumers to question whether the quality of care they receive has 
been compromised. Cardiovascular disease, for example, remains one of the leading 
causes of death in the US and among the costliest. It is estimated in 1998, heart 
attacks and other cardiovascular diseases will cost the United States $274.2 billion 
(Boston Globe, 1998). For Medicare beneficiaries, those over the age of 65, the gov-
ernment becomes the primary health care insurer. The nationwide mean Medicare 
payment to hospitals for acute myocardial infarction (AMI) patients was $9261 in 
1990, compared to an overall mean Medicare hospital payment of $463 for non-AMI 
beneficiaries in the same age range (McClellan, 1995). With so much money at 
stake, the government wants to stem rising costs while not <,:om promising quality of 
care. 
Profiling, the process of comparing the quality of care delivered by medical care 
providers to a normative standard, is central to much of the quality assessment ef-
fort. Providers of medical care can be assessed according to either process-based or 
outcome-based measures. The former relate to the appropriateness of the delivery of 
treatments and other medical processes whereas the latter relate to patient-specific 
outcomes of care, such as mortality or patient functioning, that resulted from treat-
ments. Normand, Glickman, and Gatsonis (1997) and Normand, Glickman, and 
Ryan (1997) describe profiling medical care providers using outcome measures. 
They compare mortality rates after an acute myocardial infarction (AMI) across 
hospitals and discuss many of the statistical issues involved in outcome-based pro-
filing, including risk-adjustment, and the development of performance indices. In 
this Chapter, we present a case-study for profiling medical providers using process-
based measures. Process measures can be more sensitive than outcome measures, as 
poor process does not always lead to poor outcomes (Brook et al., 1996). However, 
process measures are only valid to the extent to which it can be demonstrated that 
adherence to recommended processes of care leads to better patient outcomes. 
A key issue in monitoring quality of care using process-based measures is the need 
to define a standard of care, that is, determining who should receive the treatment 
and who should not. Subsets of patients for whom a test, procedure, or treatment 
is likely to benefit are identified, and providers are then judged according to the ex-
tent to which these patients receive the therapy. A popular approach to establishing 
such clinical guidelines for care, pioneered by the RAND corporation (Park et al., 
1986), is the consensus panel. Judgments, based on the results of published efficacy 
and effectiveness research, regarding the appropriateness of a therapy for subsets of 
patients categorized by their symptoms and prior medical history are elicited from a 
multispecialty panel of experts. Each expert assigns ratings on an ordinal scale that 
describes the extent to which the potential benefits of the therapy outweigh the po-
tential risks. Many methodological questions arise in employing practice guidelines 
developed by an expert panel to monitor quality of care including a) how to com-
bine observed appropriateness ratings into a guideline for care b) how to develop 
performance measures using the guideline to define a standard of care, and c) how 
to account for the uncertainty in the guideline and the performance measures. 
In this chapter, we address these issues through the use of generalized linear 
models and illustrate both the development of practice guidelines based on elicited 
ratings from an expert panel and the subsequent application of the guidelines to 
assess quality of care across providers. We illustrate our methods using practice 
guidelines for the use of coronary angiography, an invasive diagnostic procedure 
used to assess the extent of coronary disease, following a heart attack. 
In section 2, we describe the construction of practice guidelines for coronary 
angiography following an AMI based on elicited appropriateness ratings from an 

11. A case study 
197 
expert panel. We utilize ordinal probit models to combine the elicited ratings into a 
measure of appropriateness for a set of medical indications and to estimate their as-
sociated measures of precision. Albert and Chib (1993), Nandram and Chen (1996), 
and Johnson (1996) have previously discussed ordinal probit models from a Bayesian 
perspective. Our methods represent an extension to the class of models proposed 
by Johnson (1996). 
In section 3, we illustrate an application of the practice guidelines to profile the 
quality of care provided by hospitals based on their adherence to these guidelines. 
We employ hierarchical logistic regression models (Longford, 1993; Gatsonis et al., 
1993, 1995; Normand et al., 1997) to estimate hospital-specific measures of quality 
and to explain variations in quality according to patient and provider characteristics. 
We also discuss the importance of accounting for the uncertainty in the standard 
of care when assessing quality across hospitals. 
Finally, in section 4 , we discuss the advantages of a Bayesian approach to profiling 
the appropriate use of medical technologies, and summarize the policy implications 
of our analysis. 
2. 
Developing Practice Guidelines 
2.1 
Elicitation of Appropriateness Ratings 
An expert panel was convened in Boston, MA, during October of 1995 to update 
the 1992 RAND ratings (Bernstein et al., 1992) for the appropriateness of coronary 
angiography following an AMI. Following a comprehensive review of the literature 
relevant to the benefits and risks of coronary angiography (Bates et al., 1997), a list 
of clinically homogeneous strata, referred to as clinical indications, was developed. 
The clinical indications, which categorize patients in terms of their symptoms, past 
medical history, and results of previous diagnostic tests, were chosen so that patients 
within an indication are homogeneous insofar that angiography is equally appro-
priate or inappropriate. The indications,. numbering 888, were separated into two 
chapters. The first 90 indications, comprising Chapter 1, described clinical scenarios 
for angiography during hospitalization following an AMI; the remaining indications 
described scenarios for angiography after discharge but within 12 weeks of the AMI. 
A panel of nine experts were selected from nominations by specialty societies. 
Panelist were chosen for diversity in geographic location, practice setting, and spe-
cialty. Each expert was asked to rate the appropriateness of the procedure within 
each indication independently without discussion or contact with other panel mem-
bers by answering the following question: 
Do the expected benefits of the procedure outweigh its expected risks by 
a sufficient margin so that the procedure is worth doing? 
A nine-point scale, where a value of 1 denotes that the risks of the procedure greatly 
exceed the benefits, a value of 5 indicates that the benefits and risks are equivalent, 
and a value of 9 denotes that the benefits of the procedure greatly outweigh the 
risks, was employed. 
The results of the baseline elicitation were summarized and discussed at a meeting 
of the panelists. During this meeting the panelists had the opportunity to revise the 
list of indications, at which time two indications were added to Chapter 1. At the 
conclusion of this meeting, the experts confidentially re-rated treatment efficacy in 

198 
Landrum & Normand 
8 
,... 
0 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
lntraclass Correlation 
FIGURE 11.1. Reliability of Expert Ratings. This histogram displays the distribution 
of the 890 indication-specific estimates of the intraclass correlation of reliability. 
each indication. Indication-specific estimates of the intraclass correlation of reliabil-
ity (Fleiss, 1986) of the second round ratings are displayed in Figure 11.1. Estimates 
of the reliability of the expert ratings ranged from 29% to 88%, suggesting clear dif-
ferences among the experts in their beliefs regarding the appropriateness of coronary 
angiography for certain indications following AMI even after panel discussion. 
2.2 Combining the Angiography Panel Data 
The approach commonly employed in the medical literature to combine expert 
ratings into a guideline is to classify indications into mutually exclusive categories 
according to the observed median rating and a measure of disagreement in the 
panel (Bernstein et al., 1992). For example, with 9 experts participating on the 
panel, angiography would be considered appropriate for indications rated with a 
median score greater than or equal to 7, as long as fewer than three panelist rated 
the indication with a score of 3 or below. 
There are many shortcomings to the conventional method for producing guide-
lines for care based on expert appropriateness ratings. First, each indication is 
classified independently, ignoring the repeated nature of the elicited ratings. In ad-
dition, each expert's ratings are weighted equally, ignoring differences in the overall 
aggressiveness of the experts' beliefs regarding the benefits of interventions. Finally, 
only a crude measure of disagreement is considered, leading to an understatement of 
the uncertainty inherent in estimating appropriateness based on a sample of expert 
opinion. 
We developed a Bayesian model-based method for combining the elicited rat-· 
ings into a guideline (Landrum and Normand, 1999) that exploits the structure 
of the data by permitting expert-specific threshold parameters as well as indica-
tion random effects. Let Rir be th~ appropriateness rating of the ith indication, 
i = 1, · · ·, 890, made by the rth expert, r = 1, · · ·, 9. We assumed there exits a 
latent appropriateness trait, J-ti, that describes the degree to which the benefits of 

11. A case study 
199 
the procedure outweigh its risk for patients within clinical strata i. Medical Expert 
r was assumed to have rated indication i by first estimating Jli using 
(1) 
so that Lir is an unobserved continuous measurement of the appropriateness of 
indication i, and fir is the error in the assessment made by Expert r. Expert r then 
assigned indication i to one of I< ordered categories according to his/her assessment 
of the appropriateness of the procedure and to his/her own set of thresholds via 
(2) 
where ar = {ark}, k = 0, 1, 2, · · ·,I< ( aro = -oo, arK = oo) are Expert r's thresh-
old parameters. We assumed that the measurement error, fir, followed a Normal 
distribution with mean zero and variance u;, so that 
P(R;, SkI Jli,O!r,k,O'r) = ib ( O!r,kO'~ Jli) · 
In earlier work (Landrum and Normand, 1999), we estimated three models for the 
measurement error structure a) a model that assumed constant measurement error 
across indications and raters b) a model that allowed for heterogeneity across rates in 
the spread of the measurement error and c) a model that allowed for heterogeneity 
across indications in the spread of the measurement error. We determined that 
a model allowing for rater-heterogeneity in the measurement error provided the 
best fit to the observed appropriateness ratings for coronary angiography and thus 
focused on the rater-heterogeneity model in this chapter. Identifiability was achieved 
by fixing the location and the scale of the prior for the appropriateness parameters, 
Jli ,..... N(O, 1). The model was completed by assuming vague but proper priors for 
the remaining parameters: u; 2 ,..... Gamma(1, 1) and ar,k ,..... Unif( -10, 10), subject 
to order constraints on the ar,k 's. 
2. 3 Estimation 
Gibbs sampling, implemented using specialty functions written in Fortran, was 
employed to fit the regression models. Model fit was assessed with several poste-
rior predictive checks (Gelman et al., 1995) using 100 replicated data sets, and by 
examining the posterior distribution of the log-likelihood given the observed data 
(Dempster, 1974). For details regarding the assessment of convergence and model-fit 
see Landrum and Normand (1999). 
2.4 
Defining the Standard of Care 
Several indices of appropriateness were estimated for each indication, including a 
95% posterior credible interval for Jli· We also estimated the posterior probability 
that the underlying appropriateness level exceeds the common standard for defining 
a procedure to be appropriate, denoted PA,dR. The probability that,angiography 
is appropriate for indication i was estimated using 
9 
A 
I 
n I( (d) 
(d)) 
A 
I 
"""' P A ir R 
A 
I 
"""' 
Jli 
> ar 6 
PA,i R= L 
•9 
where PA,ir R= L 
n 
' 
, 
r=l 
d=l 
(3) 

200 
Landrum & Normand 
where J.l~d) is the dth iterate for J.li, n is the total number of iterates employed for 
inference, ar,6 denotes Expert r's threshold for a cutoff between 6 and 7, and I is the 
indicator function. PA,iriR estimates the probability that the latent appropriateness 
score of indication i exceeded Expert r's threshold for a rating greater than or equal 
to 7. We employed the average of this quantity across the nine experts to estimate 
the probability than an average expert would rate indication i as appropriate. 
We compared model-based estimates of appropriateness to the categorization of 
indications according to the standard methodology employed in the medical litera-
ture: 
A 
{ 
1 if Median(Ri) ~ 7 and Lr I( Rir s; 3) < 3 
AiiRi = 
0 otherwise. 
(4) 
2.5 Results 
Figure 11.2 displays appropriateness estimates for 92 clinical indications describ-
ing the use of angiography after an AMI before discharge from the hospital. The 
92 strata address the potential use of angiography in three time frames: less than 6 
hours after symptom onset, 6 to 12 hours after symptom onset, and more than 12 
hours after symptom onset before discharge. The remaining indications, not shown 
in the figure, describe scenarios for angiography after discharge from the hospital, 
but within 12 weeks of the AMI. Angiography was estimated to be highly appro-
priate (PA,iiR > 0.80) for a large number of indications in all three time frames. In 
addition, there was good separation of the clinical indications into two classes: those 
for which the elicited ratings provided evidence of a potential benefit to angiography, 
and those with little support for the use of angiography. Generally, there was also 
good agreement between the posterior estimates of appropriateness and the binary 
classification of indications according to the standard methodology. The agreement 
between the two estimates was stronger for patients assessed more than 12 hours 
after symptom onset, suggesting less uncertainty among the experts regarding the 
benefits of angiography for these patients. 
3. 
Applying the Practice Guidelines 
3.1 
Study Population 
In this section, we use the practice guidelines for angiography as a yard stick 
against which to assess the quality of care delivered to patients following a heart 
attack. The study population consisted of fee-for-service Medicare beneficiaries be-
tween 65 and 89 years of age discharged alive or dead with a heart attack (Interna-
tional Classification of Diseases, Ninth Revision, Clinical Modification [ICD-9-CM) 
principal diagnosis codes of 410.xx) during the period February 1, 1994 to July 31, 
1995 from hospitals located in one US state. 
Medical record data regarding the index episode of care following the AMI was 
abstracted by two Clinical Data Abstraction Centers who were under contract with 
the Health Care Financing Administration. Abstracted data included dates of hos-
pitalization, admission severity, medications, cardiac history, medical therapies, and 
results of non-invasive and invasive tests. We assigned patients to the first hospital 

11. A case study 
201 
Appropriate 
.J 
0 
Not Appropriate 
curs 
. .I 
oo 
0 
cfJ 
0 
go 
> 12 h 
.t· .I 
c~rs 
.o• 8 
« 
0 
6·12 h 
<6h 
.I 
o rs 
,. 
·' 
0 
r 
0 
I 
I 
I 
I 
I 
I 
0 
20 
40 
60 
80 
100 
Posterior Probability Appropriate 
FIGURE 11.2. Appropriateness Estimates for Chapter 1 Clinical Strata. The pos-
terior probability that angiography is appropriate, P A,i IR, for 92 clinical strata describing 
scenarios for angiography after an AMI before discharge from the hospital. The binary 
classification of these indications according to the standard methodology, AdR, is denoted 
by filled (appropriate) or open (not appropriate) circles. 
to which they were admitted and linked together medical record information from 
transfer hospitals in order to create a complete inpatient episode of care. 
To assess adherence to the expert-derived guidelines for coronary angiography, we 
focused on a subset of the 890 clinical indications rated by the expert panel that de-
fine clinical scenarios for coronary angiography more than 12 hours after symptom 
onset, before discharge from the hospital. We focused on these indications because 
90% of patients in this cohort who underwent angiography in the 90 days following 
their AMI received the procedure in this time frame. The scenarios classify patients 
according to their age, the receipt of thrombolytic therapy, and the presence of 
key cardiac complications, such as persistent chest pain, pulmonary edema, and 
cardiogenic shock. With the consultation of cardiologists on our research team, we 
developed an algorithm to assign patients to an indication according to the clinical 
information abstracted from their medical records. Appropriateness measures were 
than assigned to each patient according to their clinical indication. Finally, we clas-
sified patients who were not candidates for an invasive revascularization procedure 
because of extreme comorbid conditions (terminal illness, hepatic failure, metastatic 
cancer, anoxic brain damage, or a do not resuscitate order) as highly inappropriate 
for coronary angiography. 
We obtained hospital characteristics from a detailed survey of the hospitals in 

202 
Landrum & Normand 
which the patients were treated. Hospital information collected in the survey in-
cluded information regarding the availability of various cardiac services, training 
programs, and participation in cardiac clinical trials. In this case-study, we focused 
on the availability of invasive cardiac services at the hospital to which the pa-
tient was admitted. We classified hospitals into three categories: those with the 
capability to perform invasive revascularization procedures - either percutaneous 
transluminal coronary angioplasty (PTCA) or coronary artery bypass graft surgery 
(CABG), those with the capability to perform coronary angiography, but not PTCA 
or CABG, and those with no invasive cardiac capabilities. 
3.2 Modeling Adherence to Practice Guidelines 
We employed Bayesian hierarchical logistic regression models to study the rela-
tionship between the use of angiography in practice and the appropriateness of the 
procedure according to the expert panel. In the first stage, we modeled the patient's 
log-odds of undergoing angiography at each hospital as a function of the appropri-
ateness of the procedure and patient demographic characteristics (age and sex). In 
the second stage, we linked the hospital-specific effects of appropriateness and pa-
tient characteristics on the propensity to undergo angiography to the availability of 
invasive cardiac procedures at the admitting hospital. 
Let Yii h = 1 if the jth patient admitted to the hth hospital assigned to clinical 
stratum i received angiography, Aijh = 1 if angiography was appropriate for a 
patient presenting with indication i (the true, but unknown state of appropriateness 
according to expert belief). We fitted models of the general form 
logit(P(Yiih = 1jX,,8)) 
(5) 
( 
~olhh ) 
,Bhjz,r,D= 
P 
,82h 
MVN(rzr, D), 
(6) 
where Xij h is a vector of p patient characteristics centered at their mean; Zh is 
a vector of q hospital characteristics; ,82h is a vector of p regression coefficients 
describing the relationship between the patient characteristics contained in X and 
the log-odds of receiving angiography at hospital h; and r is a (p + 2)xq matrix of 
population regression coefficients describing the relationship between the hospital-
specific regression coefficients and the provider characteristics contained in Z. 
Because there is a substantial amount of empirical evidence which suggests that 
provider characteristics impact utilization of invasive procedures such as coronary 
angiography (Pilote et al., 1996; Every et al., 1993), we included provider charac-
teristics in the between-hospital model to examine their ability to explain hospital 
variability, to better specify the model, and to improve the precision of the perfor-
mance indexes. While there is little empirical evidence to suggest nonexchangeability 
in the slope coefficients, we also modeled between-hospital variability in the slope 
parameters as a function of provider characteristics and examined the evidence for 
nonexchageability in our data. 
Aij h, the true state of appropriateness for angiography according to expert opin-
ion, is not known. Rather, we estimated appropriateness for angiography using the 
elicited ratings from a sample of nine experts. To incorporate our uncertainty re-
garding the true state of appropriateness we replaced Aij h in Equation ( 5) with 
P A,ij h IR, the estimated probability, conditional on the observed appropriateness 

11. A case study 
203 
ratings by the expert panel, that the underlying appropriateness level of the proce-
dure for a patient presenting with indication i exceeded the threshold for a rating of 
7 or above (as defined by Equation (3)). To compare our analysis with the standard 
approach, we also fitted models in which we employed Aij h IRi to estimate the true 
appropriateness of the procedure for patient j, where AijhiRi = 1 if indication i 
was classified as an appropriate indication according to the standard method for 
combining elicited ratings into a guideline (as defined by Equation ( 4)). 
Vague, but proper priors were specified for remaining parameters: /kl ,...., N(O, 106), k = 
0, 1, · · ·, p + 1; I= 0, 1, · · ·, q- 1; D,..., Wishart(C,p+ 2), where p+ 2 is the number 
of regression parameters estimated for each hospital. Values for C, our prior guess 
for the order of magnitude of the covariance matrix for the hospital-specific regres-
sion coefficients, were obtained from a similar sample of patients admitted to 168 
hospitals located in a different state. 
3. 3 Estimation 
Gibbs sampling, implemented in BUGS (Gilks et al., 1994), was employed to 
fit the hierarchical regression models. Convergence of the sampler was assessed 
using the Potential Scale Reduction (PSR) statistic proposed by Gelman and Rubin 
(1992). Using this diagnostic, we determined that sampling beyond 10,000 iterations 
would not improve the precision of the model estimates. Inference for functions of 
relevant model parameters were obtained by combining iterates from 5 parallel 
chains after a burn-in period of 5000 iterates. 
The fit of the within-hospital model (Equation (5)) was assessed by examining 
residuals from a patient-level analysis. Goodness-of-fit statistics were also calculated 
on the fit of the first-stage model to several large hospitals. We examined the ad-
equacy of the second-stage model by plotting posterior estimates of the first-stage 
parameters against provider characteristics. 
3.4 
Profiling Hospitals 
We evaluated the quality of care provided at each hospital, in terms of adherence 
to expert opinion regarding the use of coronary angiography, by examining the 
posterior distribution of selected functions of hospital-specific regression parameters. 
Specifically, we focused on profiling both the probability that an appropriate patient 
(a patient for whom Aijh = 1) received treatment, and the relative odds of an 
appropriate patient undergoing therapy compared to a patient not rated appropriate 
by the expert panel (a patient for whom Aijh = 0). The first quantity, the probability 
that an "average" appropriate patient underwent angiography, 
quantifies the rate at which hospitals provided angiography to patients appropriate 
for the procedure according to the practice guideline. Although we would not expect 
all appropriate patients to undergo angiography, we would expect that at Jeast 50% 
of patients considered appropriate should have in fact received the procedure, and 
thus considered hospitals with fi < 0.50 to have provided below standard care. 
While the likelihood of angiography for appropriate patients identifies hospitals 
where underuse of angiography may be a problem, it does not measure the ability of 

204 
Landrum & Normand 
the providers to distinguish between patients who are appropriate for angiography 
from those who are not. To address this issue, we also estimated the odds that an 
appropriate patient received angiography compared to a similar patient not rated 
appropriate for the procedure, 
fh = exp(,Blh ) . 
fh < 1 indicates non-compliance to the guideline at hospital h, in that the probabil-
ity of receiving treatment decreased as a function of appropriateness for treatment. 
We estimated the posterior distribution of each of these two performance mea-
sures by computing each measure conditional on a draw of model parameters from 
the Gibbs sampler. For example, to estimate fh we computed at each iteration d 
after burn-in, O~d) = exp(,B~~)), where ,B~~) is the dth iterate for ,B1h. We then es-
timated the posterior distribution of oh using the empirical distribution of the n 
quantities computed conditional on each draw of model parameters, where n is the 
total number of iterates employed for inference. 
To determine whether the hospital delivered below standard care, we calculated 
the posterior probability that appropriate patients were less likely to receive an-
giography compared to patients who were not rated appropriate using 
P(Oh < 1 I y) = t; t I(exp(,Bi~)) < 1). 
d=1 
We then defined hospitals to have provided below standard care, as measured by 
adherence to the guideline, if this probability was large. 
3. 5 Explaining Variability in Quality of Care 
To explain variability in the quality of care patients received across hospitals, 
we estimated the posterior distribution of selected functions of the second stage-
regression parameters (the elements ofr). For example, to compare adherence across 
hospital types, we estimated a model of the form described by Equations (5) - (6), 
with 
,Bkh = /kO + /k1Z1h + /k2Z2h; k = 0,'' ·, p + 1. 
z1h is a binary variable equal to 1 if the admitting hospital had the capability to 
perform coronary angiography, but not bypass surgery or PTCA, z2h is a binary 
variable equal to 1 if the admitting hospital could perform either bypass surgery or 
PTCA, and p is the number of demographic characteristics contained in the model. 
We then computed selected odds-ratios that describe the average relationships be-
tween angiography, the appropriateness of the patient, and patient demographic 
characteristics at each hospital type. For example, to estimate the odds that an 
appropriate patient admitted to a hospital with bypass or angioplasty capabilities 
received angiography compared to a patient with equivalent demographic charac-
teristics not considered appropriate by the experts, we calculated 
OcABG/PTCA = exp(/10 + /12)· 
Posterior estimates of selected functions such as this were then obtained from the 
empirical distribution of quantities computed at each iteration. 

11. A case study 
205 
TABLE 11.1. Patient and Hospital Characteristics. 
Patient Characteristics 
Mean 
Lower 
Upper 
(n = 5998) 
Quantile 
Quantile 
Age 
76.3 
71 
82 
%Male 
54 
Hospital Characteristics 
Mean 
Min 
Max 
(n = 294) 
Angiography Rate (%) 
45.6 
0 
100.0 
Number of AMI Patients 
20 
1 
144 
Cardiac Services 
%Patients 
%Hospitals 
None 
26 
50 
Cath Only 
13 
13 
CABG/PTCA 
62 
37 
3.6 Results 
Patient Population 
Our patient sample was comprised of virtually all Fee-For-Service Medicare bene-
ficiaries aged between 65 and 89 discharged alive or dead with a principal diagnosis 
of AMI from hospitals located in a single state during the period February 1, 1994 
through July 31, 1995. Patients discharged alive in less than 4 days, patients who 
underwent emergent angiography (received the procedure 0-12 hours after symptom 
onset), patients whose residence was outside the US, and patients who died within 1 
day of admittance were excluded from the analyses. Patients admitted to a hospital 
for which the information regarding the availability of invasive cardiac procedures 
was missing, and patients missing clinical data required to assigned them to a clin-
ical indication were additionally dropped from the analyses, leaving a total sample 
of 5998 patients admitted to 294 hospitals. 
Descriptive statistics regarding the 5998 AMI patients and the 294 hospitals are 
reported in Table 11.1. Fifty percent of the hospitals, to which 26% of the patients 
were admitted, did not have the capability to perform coronary angiography. In 
order for patients admitted to one of these hospitals to have undergone angiog-
raphy, his/her physicians had to determine that the procedure would benefit the 
patient enough to warrant their transfer to a facility capable of performing angiog-
raphy. Thirteen percent of the hospitals could perform coronary angiography, but 
not angioplasty or bypass surgery, while 37% of the hospital, to which over 60% 
of the patients were admitted, had the capabilities of providing all invasive cardiac 
services. 
The distribution of two appropriateness measures, the posterior probability an-
giography was appropriate, PAIR, and the classification of patients as appropriate 
according to the standard methodology, AIR, are displayed in Figure 11.3. Angiog-
raphy was estimated to be highly appropriate for approximately half of the patients. 
However, only 50% of these patients received the procedure before discharge. More-
over, a large fraction ( 42%) of the patients with low estimated probability of being 
appropriate for angiography received the procedure. As discussed in Section 2.5, 

206 
Landrum & Normand 
4000 
4000 
~ 
41.7 
49.5 
.1'3 
41.7 
49.5 
c: 3000 
(1) 3000 
(1) 
~ 
~ 
Q. 
Q. 
0 
0 
2000 
(5 2000 
~ 
~ 
E 
:I 
:I 
z 
z 
1000 
1000 
0 
0 
10 
30 
50 
70 
90 
Not Appropriate Appropriate 
Posterior Probability Appropriate 
Standard Categorization 
FIGURE 11.3. Distribution of Appropriateness Estimates Across Patients. The 
distribution of two appropriateness measures, the posterior probability angiography was 
appropriate, PAIR, and the classification of patients as appropriate according to the stan-
dard methodology, AIR across the 5998 patients. The proportion of patients receiving 
angiography in each interval are reported at the top of each bar. 
there was a great deal of agreement between the two estimates of the appropriate-
ness of angiography in this time frame. 
Modeling Adherence 
We estimated two models of the form described by Equations (5)- (6). To exam-
ine the relationship between appropriateness for angiography and its use in practice, 
we employed two different estimates of the true level of angiography appropriate-
ness: P 
A jR, the estimated probability, conditional on the observed appropriateness 
ratings by the expert panel, that the underlying appropriateness level exceeded the 
threshold for a rating of 7 or above, and AIR, the categorization according to the 
standard methodology. To examine the impact of patient characteristics on quality 
of care, both models adjusted for the patient's age and sex. Finally, both models 
included hospital variables that reflected the hospital's capability to provide inva-
sive cardiac services in the between-hospital model (Equation (6)). The Appendix 
reports posterior summaries corresponding to the two models (Table Al). 
Profiling Hospitals 
Figure 11.4 displays the posterior mean estimates of two hospital-specific mea-
sures of quality, the probability that an average appropriate patient received angiog-
raphy at hospital h, fih, and the odds-ratio comparing the likelihood of angiography 
for an appropriate patient to a patient not rated appropriate by the expert panel, 
Oh. We estimated these quantities fitting a model which adjusted for patient demo-
graphic characteristics and which employed P 
A,i. I R to estimate the true level of 
angiography appropriateness. 

11. A case study 
207 
1.0 
0 
No services 
D. 
Cath Only 
0.8 
X 
CABGIPTCA 
Q) 
1a 
a: 
..c: 
0.6 
~ 
~ 
·c 0.4 
a. e 
a. 
a. 
< 0.2 
0.0 
0 
2 
3 
4 
Odds Ratio 
FIGURE 11.4. Hospital-Specific Estimates of Adherence. Plots ofthe posterior mean 
of the probability that an average appropriate patient received angiography at hospital h, 
flh, versus the posterior mean of the odds-ratio comparing the likelihood of angiography 
for an appropriate patient to a patient not rated appropriate by the expert panel, Uh. 
The labels "No services," "Oath Only," and "CABG/PTCA" refer to hospitals without 
the capacity to perform angiography, hospitals that could perform angiography but not 
bypass surgery or angioplasty, and hospitals providing either bypass surgery or angioplasty, 
respectively. 
Ideally all hospitals would fall in the upper right quadrant of Figure 11.4, indicat-
ing that more than 50% of all appropriate patients received angiography, and that 
the hospital correctly identified appropriate patients for treatment. Most hospitals 
without the capacity to perform coronary angiography tended to correctly choose 
the appropriate patients for transfer to another facility to receive angiography. How-
ever, a small fraction of appropriate patients admitted to these hospitals received 
the procedure, suggesting that hospital of this type should be targeted to reduce 
underuse of angiography in appropriate patients. Hospitals with bypass or angio-
plasty capabilities were more likely to provide angiography to more than 50% of the 
appropriate patients, but also tended to be less discriminating in terms of providing 
angiography to the most appropriate patients according to the expert panel. Thus 
high services hospitals should be targeted to reduce overuse of angiography among 
patients who may not benefit from the procedure. Hospitals with only angiography 
capabilities tended to best comply with the opinions of the expert panel, but still 
tended provide angiography to less than 50% of the appropriate patients. 
Figure 11.5 displays the distribution of posterior estimates that each hospital's 
compliance with the guideline was below standard. The histograms display the 
estimated probability that odds of receiving angiography was lower for appropriate 
patients compared to those not rated appropriate (P(fh < 1)). We compared the 

208 
Landrum & Normand 
II) 
!I 
·a. 
~ 
0 
15 
.Q 
§ 
z 
~ 
~ 
g 
~ 
~ 
~ 
0 
Standard Categorization 
0.0 
0.2 
0.4 
0.6 
0.8 
Posterior Probability Performance 
Below Standard 
II) 
~ 
c. 
:g 
:I: 
0 .... 
Cll 
.Q 
E 
::J z 
Model-Based Categorization 
~ 
~ 
g 
~ 
~ 
~ 
0 
o.o 
0.2 
0.4 
0.6 
0.8 
Posterior Probability Performance 
Below Standard 
FIGURE 11.5. Evaluation of Hospitals. Each histogram displays the distribution of the 
estimated probability that a hospital's compliance to the guideline was below standard 
(P(fh < 1)) across the 294 hospitals. 
distribution of the quality measures estimated using the model-based measure of 
appropriates to estimates obtained using the standard categorization. We conclude 
that compliance with expert opinion was not significantly below standard at any 
of the 294 hospitals (P(fh < 1) < 0.80 for all hospitals). However, as expected, 
incorporating uncertainty in the assessment of appropriateness according to the 
expert panel produced more conservative results than in that employing model-
based estimates of appropriateness led to smaller estimates of the degree of evidence 
for poor quality. 
Explaining Variability in Quality of Care 
Figure 11.6 displays posterior density estimates of the odds ratio comparing the 
likelihood of an appropriate patient (one for whom PA,ijh = 1) receiving angiog-
raphy to a patient of equivalent age and gender who was not rated appropriate 
by the expert panel (PA,ijh = 0) at each hospital type. The relationship between 
a patient's appropriateness for angiography and his/her likelihood to receive the 
procedure was weak but significant at all three hospital types. The relationship 
between appropriateness and receipt of the procedure was strongest for patients 
admitted to hospitals with the capability to perform angiography, but not bypass 
surgery or angioplasty: patients rated appropriate for the procedure were twice as 
likely to receive the procedure compared to a similar patient not rated appropriate 
(95% credible interval for the odds-ratio was equal to (1.3,3.0)). The relationship 
was weakest among patients admitted to hospitals with the capability to perform 
angioplasty or bypass surgery: patients rated appropriate for the procedure were 
only 30% more likely to undergo angiography (95% credible interval for the odds 
ratio was equal to (1.1,1.6)). 
}i'igure 11.6 also displays the relationship between patients' demographic charac-
teristics and their likelihood of undergoing angiography. The age of a patient was 
strongly related to their likelihood of receiving the procedure, and patients of older 

Appropriateness 
None 
CathOnly 
CABG/PTCA 
Odds Ratio 
Age 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
Odds Ratio 
11. A case study 
209 
Gender 
0.0 
0.5 
1.0 
1.5 
2.0 
Odds Ratio 
FIGURE 11.6. Relationship between Hospital and Patient Characteristics and 
the Likelihood of Angiography. Smoothed density estimates of the posterior distribu-
tion of selected odds-ratios describing the average relationships between angiography, the 
appropriateness of the patient, patient demographic characteristics, and the availability of 
cardiac services at the admitting hospital. The three panels plot the estimated odds-ratios 
comparing appropriate (FAIR= 1) versus not appropriate patients (FAIR= 0), older ver-
sus younger patients (an increase in 5 years of age), and males versus females, respectively. 
age were less likely to receive angiography. This relationship did not significantly 
differ among the three types of hospitals. Gender was a less important predictor 
of angiography in these patients. Males patients admitted to hospitals with bypass 
surgery or angioplasty capabilities were significantly more likely to undergo angiog-
raphy compared to females (95% credible interval for the odds-ratio was equal to 
(1.2,1.7)), however gender was less important at the other hospital types. 
Posterior density estimates of the probability of receiving angiography for subsets 
of patients admitted to each hospital type are displayed in Figure 11.7. The age 
of the patient and the invasive cardiac services available at the admitting hospital 
impacted on the likelihood of receiving angiography to a much larger degree than the 
appropriateness of the procedure according to the panel of experts. The likelihood of 
receiving angiography varied from 13% for 80 year old males not rated appropriate 
for the procedure admitted to a hospital without any invasive cardiac services to 
87% for 65 year old males rated appropriate for the procedure and admitted to a 
hospital with the capability to perform bypass surgery or angioplasty. Moreover, an 
80 year old had a greater likelihood of receiving angiography if admitted to a high 
service hospital than a 65 year old admitted to a hospital without the capability to 
perform any invasive cardiac services. 
4. 
Discussion 
In this chapter, we demonstrated an application of Bayesian generalized linear 
models in the area of health policy. First, using ordinal probit models, we combined 
ratings from an expert panel in order to explicitly quantify a medical guideline. 
Second, using hierarchical logistic regression modeling, we profiled hospitals with 

210 
Landrum & Normand 
No Services 
-
Appropriate,llOyr male 
- - • Nol appropriate eo yr male 
••••••• Appropriate, 1111 yr,male 
--
Nol approprialell!l yr male 
~ 
II 
II 
'• 
'• A 
'•II 
'• 
II 
I 
I 
: '" /\ 
: 
y l, 
I I I i\ \ 
,' ', 
i 
\ . 
.;..~_ ... 
\.... ........ 
0.0 
0.2 
0.4 
0.8 
0.8 
1.0 
Probability 
'• 
'I 
'• I 
I 
I 
I 
I 
I 
I 
0.0 
0.2 
Cath Only 
(\ i\ 
\' I 
I ! \ 
I A \ 
I i \ 
~ 
I l \ \ 
/ \.i ... 
0.4 
0.6 
0.8 
1.0 
Probability 
CABG/PTCA 
~ 
Jj 
/li 
:: 
IH 
II 
IH 
:: 
llll 
: 
l!li 
: 
Ill! 
i I 
IHl 
: : 
!Ill 
,'I : \ I! 1\ 
-I..J •l.. ..... o u.. 
0.0 
0.2 
0.4 
0.8 
0.8 
1.0 
Probability 
FIGURE 11.7. Estimate Probability of Angiography for Selected Subsets of Pa-
tients. Smoothed density estimates of the posterior distribution of the likelihood of un-
dergoing angiography for selected subsets of patients. 
respect to their adherence to the medical guideline, explained deviations from the 
guideline, and created several posterior performance indices of policy relevance. We 
were able to implicitly incorporate the precision of the appropriateness ratings by 
using the posterior probability that angiography was appropriate for each clinical 
indication. This represents a major step forward in the quality of care literature 
which currently employs an ali-or-nothing rule. Lastly, by linking our posterior 
estimates of appropriateness provided to us by the expert panel to patient outcomes, 
such as mortality and functioning, we will be able to update the panel ratings using 
the usual prior-to-posterior paradigm in future work. 
Substantively, we found compliance rates to the updated coronary angiography 
practice guidelines among elderly patients with AMI to be acceptable. However, a 
patient's appropriateness for the procedure was only weakly associated with his/her 
likelihood of undergoing the procedure, particularly for patients admitted to hos-
pitals with the capability to perform angiography. While the relationship between 
appropriateness for angiography and its use in this population was weak, age (which 
may be a proxy for unmeasured disease severity) and the availability of invasive 
cardiac services at the admitting hospital were strongly related to the use of an-
giography. Moreover, the availability of the procedure at the admitting hospital 
dominated the effect of age; an 80 year old had a greater likelihood of receiving 
angiography if admitted to a high service hospital than a 65 year old admitted to a 
hospital without any invasive cardiac services. 
We found that hospitals tended to do well according to only one of the dimensions 
of quality we examined. Hospitals with the capability to perform invasive cardiac 
procedures provided angiography to a large proportion of patients likely to bene-
fit from the procedure. However, they did not discriminate well between patients 
according to the potential benefits of the procedure. In contrast, hospitals without 
the capability to perform bypass surgery or angioplasty tended to better "ration" 
care, in that they better identified patients more likely to benefit from the proce-
dure. However, the rate at which appropriate patients admitted to these hospitals 
received the procedure was surprisingly low. Our results suggest that both dimen-
sions of quality should be targeted in order to improve the quality of care among 
elderly AMI patients. 

11. A case study 
211 
Appendix 
In Table Al the posterior summaries of models of the form described by Equations ( 5) 
- ( 6) in Section 3.6 are reported: 
logit(P(Yiih = 1IX, ,B)) 
( 
f3oh ) 
,ah 1 z, r, n = 
f31h 
,B2h 
~ MVN(rzr,n). 
where Atjh =AIR or PAIR; xr;h = (Ageijh-Age, Maleijh-Malef; and zh = (1, Zlh' Z2h), 
z1h = 1 if hospital h had the capability to perform coronary angiography, but not bypass 
surgery or PTCA, z2h = 1 if hospital h could perform either bypass surgery or PTCA. 
Table A1. Stage 2 Regression Parameter Estimates. Posterior mean and 
95% credible intervals (in parentheses) for the second stage regression parameters 
(r) and posterior mean estimates of the covariance matrix of the first stage 
regression parameters (D). 
Intercept 
AIR 
Age 
Males 
D 
Intercept 
PAIR 
Age 
Males 
D 
(a)- Standard Categorical Estimates of Appropriateness 
-1.58 
(-1.84,-1.32) 
0.68 
( 0.24, 1.12) 
1.71 
( 1.39, 2.04) 
0.41 
( 0.10, 0.70) 
0.23 
(-0.24, 0.72) 
-0.18 
(-0.53, 0.18) 
-0.09 
(-0.12,-0.07) 
-0.03 
(-0.07, 0.02) 
-0.03 
(-0.06, 0.004) 
0.05 
(-0.23, 0.33) 
-0.12 
(-0.57, 0.36) 
0.30 
(-0.04, 0.61) 
( 
~;,;: (0.50)2 
) 
-0.02 
0.01 
(0.05)2 
0.04 
-0.03 
-0.01 
(0.36)2 
(b) - Model-Based Estimates of Appropriateness 
-1.59 
(-1.85,-1.34) 
0.66 
( 0.23, 1.11) 
1.70 
( 1.40, 2.02) 
0.44 
( 0.14, 0.75) 
0.26 
(-0.24, 0.76) 
-0.18 
(-0.54, 0.17) 
-0.09 
(-0.12,-0.07) 
-0.03 
(-0.07, 0.02) 
-0.03 
(-0.06, 0.002) 
0.06 
(-0.23, 0.37) 
-0.13 
(-0.61, 0.33) 
0.29 
(-0.05, 0.63) 
( 
~;,;: (0.52)2 
) 
-0.02 
0.01 
(0.05)2 
0.04 
-0.03 
-0.01 
(0.37) 2 
Acknowledgments: We are grateful to several colleagues in the Department of Health 
Care Policy, Harvard Medical School: Barbara J. McNeil for helpful comments; Ed-
ward Guadagnoli for use of the hospital survey data; and Margaret Volya and Fung-

212 
Landrum & Normand 
Yea Huang, for building and maintaining the Harvard Guideline Study data base. 
This research was support by grant R01-HS08071 from the Agency for Health Care 
Policy and Research, Rockville, MD. 
References 
Albert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous 
response data. Journal of the American Statistical Association, 88, 669-679. 
Bates, D. W., Miller, E., Berstein, S. J., Hauptman, P. J. and Leape, L. L. (1997). 
Coronary angiography and angioplasty after acute myocardial infarction. An-
nals of Internal Medicine, 126, 539-550. 
Berstein, S. J., Laouri, M., Hilborne, L. H. et al. (1992). Coronary Angiography: 
A Literature Review and Ratings of Appropriateness and Necessity. Santa 
Monica, CA: RAND Corporation. 
Boston Globe (January 2, 1998). Costs of cardiovascular disease keep rising, A16. 
Brook, R. H., McGlynn, E. A. and Cleary, P. D. (1996). Quality of health care, 
part 2: measuring quality of care. New England Journal of Medicine, 335, 
966-970. 
Dempster, A. P. (1974). The direct use oflikelihood for significance testing, in Pro-
ceedings of Conference on Foundational Questions in Statistical Inference. eds. 
0. Barndorff-Nielsen, P. Blaesild, and G. Schou, Department of Theorectical 
Statistics, University of Aarhus, 335-352. 
Every, N. R., Larson, E. B., Litwin, P. E. et al. (1993). The association between 
on-site cardiac catheterization facilities and the use of coronary angiography 
after acute myocardial infarction. New England Journal of Medicine, 329, 
546-551. 
Fleiss, J. L. (1986). The Design and Analysis of Clinical Experiments. Toronto, 
Canada: Wiley. 
Gatsonis, C. A., Epstein, A.M., Newhouse, J. P., Normand, S. T. and McNeil, B. 
J. (1995). Variations in the utilization of coronary angiography for elderly pa-
tients with acute myocardial infarction: an analysis using hierarchical logistic 
regression. Medical Care, 33, 625-642. 
Gatsonis, C. A., Normand, S. T., Liu, C. and Morris, C. (1993). Geographic vari-
ation of procedure utilization: a hierarchical model approach. Medical Care, 
31, YS54-YS59. 
Gelman, A., Carlin, J., Stern, H. and Rubin, D. B. (1995). Bayesian Data Analysis. 
New York, NY: Chapman and Hall. 
Gelman, A. and Rubin, D. B. (1992). Inference from iteration simulation using 
multiple sequences, Statistical Science. 7, 457-472. 
Gilks, W. R., Thomas, A. and Spiegelhalter, D. J. (1994). A language and program 
for complex Bayesian modeling, The Statistician. 43, 169-178. 

11. A case study 
213 
Johnson, V. E. (1996). On Bayesian analysis of multirater ordinal data: an appli-
cation to automated essay grading. Journal of the American Statistical Asso-
ciation, 91, 42-51. 
Landrum, M. B. and Normand, S. T. (1999). Applying Bayesian ideas to the 
development of medical guidelines, Statistics in Medicine. 18, 117-137. 
Longford, N. (1993). Random Coefficient Models. Oxford, U.K.: Oxford University 
Press. 
McClellan, M. (1995). Uncertainty, health care technologies, and health care costs. 
American Economic Review Papers and Proceedings, 85, 38-44. 
Nandram, B. and Chen, M. (1996). Reparameterizing the generalized linear model 
to accelerate Gibbs sampler convergence. Journal of Statistical Computations 
and Simulations, 54, 129-144. 
Normand, S. T., Glickman, M. E. and Gatsonis, C. A. (1997). Statistical methods 
for profiling providors of medical care: issues and applications. Journal of the 
American Statistical Association, 92, 803-814. 
Normand, S. T. and Glickman, M. E. and Ryan, T. (1997). Modeling mortality 
rates for elderly heart attack patients: profiling hospitals in the Cooperative 
Cardiovascular Project, in Case Studies in Bayesian Statistics. eds. C. Gat-
sonia, J. Hodges and N. Singpurwalla, Springer-Verlag, 155-236. 
Park, R. E., Fink, A., Brook R. H. et al. (1986). Physician ratings of appropri-
ate indications for six medical and surgical procedures. American Journal of 
Public Health, 76, 766-772. 
Pilote, L., Miller, D. P., Califf, R. M. et al. (1996). Determinants of the use of 
coronary angiography and revascularization after thrombolysis for acute my-
ocardial infarction. New England Journal of Medicine, 335, 1198-1205. 

This Page Intentionally Left Blank 

Part IV 
Semiparametric Approaches 

This Page Intentionally Left Blank 

12 
Sernipararnetric Generalized Linear 
Models: Bayesian Approaches 
Bani K. Mallick 
David G.T. Denison 
Adrian F .M. Smith 
ABSTRACT Generalized linear models are one of the most widely used tools of 
the data analyst. However, the model assumes that the structure of the regression 
relationship between the response and the covariates is linear on a known transformed 
scale. We focus here on different methods to perform the same type of analyses. 
These involve using nonparametric models to determine the relationship between 
the response and covariates after the usual transformation has been carried out. We 
demonstrate how such a semiparametric model performs for binary regression. 
1. 
Introduction 
Regression techniques are among some of the most widely used methods in applied 
statistics. Given a response variable Y, and a set of co variates X = (X 1, X 2 , • • · , Xp), 
one is often interested in estimating an assumed functional relationship between Y 
and X, and in predicting further responses for new values of the covariates. One 
way of modeling such a relationship is to present the expected value of Y as 
E(YIX) = p,(X), 
where, in general, p,(·) is an unknown function of the covariates. In practice, however, 
p,( ·) is usually approximated by a simple parametric function cfJ( ·; {3), where {3 = 
({31, · · ·, f3v) denotes a vector of unknown parameters. The function cfJ( ·; {3) is then 
treated as if it were the true underlying function p,(·), so the problem is reduced to 
that of estimating {3. Furthermore, in most applications the probability distribution 
of the response Y is assumed to belong to an exponential familty. This gives rise to 
the important class of generalized linear models (GLM) (Neider and Wedderburn, 
1972; McCullagh and Neider, 1989), which we shall find convenient to describe as 
follows. 
Random component: If Y1, · · · , Yn are independent random variables then the 
probability density function for an individual realization Yt has the form 
(1) 
for some functions a(·) and b(·, ·),where the mi are known weights. If u 2 is known 
then equation (1.1) is a natural exponential family model (see, for example, Mor-
217 

218 
Mallick, Denison & Smith 
ris, 1982) with canonical parameter fh. In this case J.li = E(Yil0i,u2) = a'(Oi) and 
var(YiiOi, u2) = ~~ a"(Oi)· The parameter u2 is referred to as the dispersion param-
eter of the model. If u 2 is unknown (1.1) is not a natural exponential family. 
Systematic component: For each response Yi, a vector of covariates, Xi = 
( :vil, · · ·, :Vip) of covariates is observed, producing the linear predictor 
(2) 
where a.b denotes the usual vector product of a and b. 
Link: The random and systematic components are related via a link function 
g(·), such that 
1Ji = g(J.ti)· 
(3) 
An important particular link is obtained when g- 1(·) =a'(·). In this case (}i = 1Ji, 
and g( ·) is then called the canonical link. 
The motivation of this chapter is to discuss the extended version of GLM in semi-
parametric frameworks. We keep the parametric structure for the random compo-
nent of the model but use nonparametric methods for the systematic component 
or the link function, so that the entire enterprise falls within what is referred to as 
semi parametric regression modeling. Here, we focus on work which takes a Bayesian 
viewpoint in modeling such semiparametric regression structure. The Bayesian ap-
proach to inference is attractive in incorporating prior information into the inference 
machinery through Bayes theorem, resulting in a unifying, constructive inference 
methodology. 
Classical inference for generalized linear models relies on maximum-likelihood 
estimation of the parameters and the associated asymptotic distributional proper-
ties of the estimates. Fortunately, exponential family models enjoy convexity prop-
erties that in many cases guarantee the existence and uniqueness of the maximum-
likelihood estimator (Wedderburn, 1976). Typically, however, maximization of the 
likelihood must be carried out numerically. On the other hand, Markov chain Monte 
Carlo (MCMC) methods provide a relatively straightforward means of making exact 
Bayesian inference for a wide class of generalized linear models (Dellaportas and 
Smith, 1993). 
To create nonparametric regression models in a GLM setup we can make the 
link function g unknown keeping the structure 1J linear, or we can assume the g 
is known but create a flexible model for 1J which could be completely nonlinear or 
conditionally linear. 
2. 
Modeling the Link Function g 
Here we treat the link function g as another unknown in the G LM specification 
and estimate it jointly with the mean structure. It turns out to be more straightfor-
ward to work not directly with the link function, but with a function related to its 
inverse. This will avoid the need to invert the function to evaluate the likelihood. 
2.1 
Binary Response Regression 
Binary response regression is used when the response takes only one of two values, 
say 0 and 1. The usual model assumes that the Yi are independent and 

12. Semiparametric GLMs 
219 
where F is an unknown cumulative distribution function. 
Newton, Czado and Chappell (1996) introduce a nonparametric approach to bi-
nary regression by assuming that F is a random draw from a Dirichlet process prior 
independent of {3. They show that with the inclusion of the latent variables Ui ""'F 
such that Pr(Yi = 1lf3,xi) = Pr(ui < {3.xi), F can be marginalized out and a 
straightforward Gibbs sampler arises. For more detailed discussion about this see 
Basu and Mukhopadhyay (in this volume). 
2.2 General Regression 
The Dirichlet process prior is conveniently employed in binary regression since the 
inverse link function is a distribution function. However, extensions to generalized 
linear models with other sampling mechanisms are not obvious. 
Mallick and Gelfand (1994) used mixture models for g. They observed that mod-
eling a strictly monotone function g, is equivalent to modeling a cumulative distri-
bution function ( c.d.f.). For instance, if the range of g is the real line then T(g( ·)) 
where, for example, T(z) = k1ek 2z(1 + k1ek 2z)-1 or T(z) = 1- exp(-k1ek 2z), with 
k1, k2 > 0, is a c.d.f. Here k1 and k2 are not model parameters but constants chosen 
so that under the transformation the resultant c.d.f. is well-behaved. (see Mallick 
( 1994) for more details). 
The mixture model approach models this unknown c.d.f. using a dense class of 
mixture of known distributions. For instance, Diaconis and Ylvisaker (1985) ob-
served that a discrete mixture of Beta densities provides a dense class of models for 
densities on [0,1]. 
In particular, consider 
r 
T(g(O)) = E w,JB(T(g0 (0)); c,, d,), 
l=l 
where r is the number of mixands, WI 
:_:::: 0, 2: w,=1 are the mixing weights, 
I B( u; c, d) denotes the incomplete Beta function associated with the Beta( c, d) den-
sity evaluated at u and g0 is a centering function for g. Mallick and Gelfand (1994) 
fix r, Cr and dr so g is specified by w = ( w1, · · · , Wr). They suggest experimenting 
with various values of r while c1 and d1 are chosen to provide a set of Beta densities 
which blanket [0,1], e.g. c1 = I, d1 = r + 1 - I, I = 1, 2, · · ·, r. If w is a Dirichlet 
random variable with distribution Dir( alr) then Mallick and Gelfand showed that 
E{T(g(O))} = T(g0(0)), i.e. g is roughly centered about g0 • 
Gibbs sampling is trivial as drawing gj/3 and f3lg is straightforward because draw-
ing g is equivalent to drawing w. Although this is not a fully non parametric approach 
however, it is still non parametric in motivation and does enrich the class of models 
for g near g0 as observed by Gelfand (1997). 
3. 
Modeling the Systematic Part 17 
:Modeling the deterministic part 7] is more popular than modeling g as it enhances 
the predictive power as well as keeping the interpretation of the regression param-
eters. If g is unknown then the interpretation of the regression parameters {3 is 
difficult because they are the unknown slopes of an unknown transformation. In the 
spirit of Mallick and Gelfand (1994), if we can integrate out all the parameters and 

220 
Mallick, Denison & Smith 
are only interested in the prediction of the model then the problem is resolved (also 
we get rid of the possible identifiability problems). Otherwise there exist several 
ways of modeling TJ while keeping the link function g known. 
3.1 
Model with Random Effects 
One way of extending nonparametric GLMs is to include a random intercept 
term having an unknown distribution into q. This model is advocated by Follman 
and Lambert (1989) where they used nonparametric mixture estimation to fit the 
model using the original work of Laird (1978). In this approach the distribution of 
the random effects is taken to be discrete with sufficiently few atoms to ensure the 
identifiability of the parameters. Observing these limitations, recently Walker and 
Mallick (1997) proposed a Bayesian nonparametric model using Polya tree distri-
butions for the random effects. Mukhopadhyay and Gelfand (1997) also proposed 
a similar type of model using a Dirichlet process as the non parametric prior distri-
bution for the random effect. These methods certainly create a better fitted model 
but we shall not expand on them further in this chapter. 
3.2 Model with Deterministic Error 
Recently Gutierrez-Pena and Smith (1997) introduced an interesting class of mod-
els based on the proposal of Blight and Ott (1975). They model the systematic part 
as 
g(/-li) = f/i = {3.xi + 8(xi), 
where 8(xi) = g(~-ti)-f3.xi is the nonparametric component of the model. Note that 
8(x) is a deterministic error component of the model rather than the usual random 
one as 8(xi) = 8(xj) is Xi = Xj. Usually 8( ·) is modelled by a Gaussian process 
prior with mean zero and covariance function p2 I<>..(·,·) where 
K ( 
*) _ ITP 
\ la;1-x~l"' 
>.. x, X 
-
1:1 "'I 
' 
with a E (0, 2] and ,\ E (0, 1). The rationale of this particular specification is that 
8(xi) and 8(xj) are expected to have similar values if the difference between Xi and 
Xj is small, while the degree of similarity must decrease as Xi and Xj become further 
apart. They have used the MCMC technique to find the posterior distributions of 
{3 and 8. 
If we are more interested in exploring the relationships (which may be complicated 
nonlinear functions) between the covariates and the response as well as to create 
a model with high predictive power we have to use nonlinear curves and surfaces 
within the linear-systematic structure. This is the main focus of this chapter. 
4. 
Models Using Curves and Surfaces 
Suppose the dependence of the response Y on one of the covariates Tis of more 
interest than its dependence with the other covariates X. Then the usual GLM 
structure for a single datapoint (yi, ti, Xi) can be modified as 
(4} 

12. Semiparametric GLMs 
221 
where h is an unknown curve to be estimated from the data. For classical analysis 
of this type of model see Green and Silverman (1994). 
Now if we want to express the relationships between the response and all the 
covariates in such a non-linear manner, the simplest model will be the additive 
model (Hastie and Tibshirani, 1990) where the model will be extended as 
(5) 
where h1, h2, •• • hk are unknown curves to be estimated. In this basic additive model 
formulation we ignore the interactions among covariates which could be an impor-
tant part of the model. 
So the general model should be 
(6) 
where f is a surface with p dimensions. Models in (1.3) and (1.4) are particular 
cases of this very general model. 
Multivariate adaptive regression spline (MARS) methodology introduced by Fried-
man (1991) is an efficient way to model surfaces. Denison, Mallick and Smith (1998c) 
developed Bayesian MARS to create random surfaces which will be used here to 
model f. 
5. 
GLMs using Bayesian MARS 
5.1 
Classical MARS 
The original MARS model of Friedman (1991) was motivated by the recursive 
partioning methods previously used (Morgan and Sonquist, 1963) such as classifica-
tion and regression trees (CART) (Brieman et al., 1984; Denison and Mallick, this 
volume). The MARS model was designed to have the intrepretability associated 
with CART but to overcome its greatest drawback, the poor predictions associated 
with fitting an assumed continous regression function with a piecewise constant 
surface. 
The MARS model uses truncated linear splines and their products as basis func-
tions and with this formulation Friedman achieved his desired objectives. However, 
in the classical context a deterministic search of the candidate model space induced 
by the basis functions is required to find the final model. These types of searches 
can be restrictive because, by design, they only cover a subspace of the complete 
space of models. 
In the GLM framework the MARS model is used to estimate the function which 
is a known transformation of the mean as in (1.6). Thus the nonparametric MARS 
model becomes embedded in a semiparametric framework. The estimate f to ~he 
true regression function f can be written as 
k 
f(x) = l:f3iBi(x), 
i=l 
where the Bi are the basis functions of the MARS model, of which there is an 
unknown number k. These basis functions, using the notation of Friedman (1991), 

222 
Mallick, Denison & Smith 
can be written as 
i=1 
i = 2,3, ... 
(7) 
where (·)+ = max(O, ·), Ji is the degree of the interaction of basis Bi, the Sji, 
which we shall call the sign indicators, equal ±1, the v(j, i) give the indices of the 
predictor variables corresponding to the knots tji. The v(j, ·) (j = 1, ... , J.) are 
constrained to be distinct so each predictor only appears once in each interaction 
term to maintain the 'linear' nature of the basis functions. See Section 3, Friedman 
(1991) for a comprehensive illustration of the model. Note that in all the work that 
follows we shall take the maximum number of interactions in a basis function to 
be two so that Ji $; 2 for all i. This is sensible in the generalized linear model 
framework where interpretation of the model is important. 
The form of the MARS function is found by starting the algorithm with only 
B1 (the constant basis function) in the model and then by stepwise addition of 
the basis functions which most reduce the chosen lack-of-fit criterion (usually the 
generalized cross-validation measure (Craven and Wabha, 1979)]. The candidate 
bases which can be added are found by 'splitting' the bases that are currently in 
the model; this prevents the candidate search space from becoming unmanageably 
large even though it is restrictive. After the model has been grown to have many 
basis functions stepwise deletion takes place. This involves working out the lack-
of-fit when each basis function is, in turn, removed and then taking away the one 
that least degrades the fit. When some minimum of the lack-of-fit is reached, then 
the process is terminated and the MARS model has been fitted. Note that at each 
step a suitable algorithm for maximizing the coefficients {3 is undertaken; this is not 
straightforward for GLM because the errors are not normal as is often the case for 
simple linear models. 
5.2 Bayesian MARS 
In this section we describe an extension to the classical MARS model. We attempt 
to account for model uncertainty by finding a posterior distribution for the unknown 
parameters in the model given the data, and we also want to perform a wide search of 
the model space. Both of these things are possible by taking a Bayesian perspective 
and using standard MCMC simulation methods. A full description of the Bayesian 
MARS (BMARS) algorithm, which we shall use in the remainder of this chapter, 
is given in Denison et al. (1998c) but here we shall just give a gene1·al outline of 
the model. Note that the Bayesian MARS method is just an extension in many 
dimensions of the Bayesian curve fitting methodology given in Denison, Mallick 
and Smith (1998a). 
We place prior distributions over the unknown MARS model parameters (k, 9(k)). 
Here 8(k) = [ ci' J3i' { Sji' tji} f~l] :=1 where ci is the type of basis function that 
Bi is classified as. This is determined from the v(j, i) and just classifies all basis 
functions involving the same covariates as of the same type no matter what ordering 
of j is used. With this model structure the dimension of 9(k) is given by n(k) = 
L:~=l 2(1 + Ji)· 
The priors we use are as follows. We assign a Poisson prior with parameter..\ over 
k with discrete uniform priors over the possible values of Sji and t;i, i.e. over {-1, 1} 
and the marginal predictor values of variable Xv(j,i) respectively. The prior over Ci is 

12. Semiparametric GLMs 
223 
chosen more carefully. We want the prior to reflect that among all interaction terms 
and main effects each type of basis is equally likely, but also that main effects are 
favored over interactions. So, for the Ci which represent main effects ( i = 1, ... , p) 
the prior probability for ci is '1/J/p and for the other ci their prior probability of 
being in the model is 2'1/J / { m( m - 1)} where '1/J( > 0.5) is the prior proportion of 
basis functions expected to be main effects. This specifies a prior distribution over 
the model parameters, ~say, sop(~)= p(B\{3). We create a hierarchical setup to 
assign the prior to the {3 because p(~, {3) = p(f31~)p(~). We choose the prior for the 
coefficients given the model to be a normal distribution centered around zero and 
with dispersion matrix A = 0.2diag(r2, ... , rk) where ri is the range of the output 
of the ith basis function. Note that no 'shrinkage' takes place on the constant basis 
function. This hierachical structure avoids possible problems with model selection, 
notably 'Lindley's paradox' (Lindley, 1957). 
We wish to make inference on the model parameters given the data so our tar-
get posterior distribution is p(k, ()(k)IV), where V denotes the data matrix (Y, X). 
Unfortunately, this posterior distribution is analytically intractable so we must use 
a simulation technique to generate samples from it. The dimension of the posterior 
is varying so we use a reversible jump MCMC algorithm (Green, 1995) to perform 
the simulation. This takes the form of a stochastic search over the model space 
8 = Ur=o ek where ek is the subspace of the Euclidean space, Rn(k), correspond-
ing to the vector space spanned by all the elements (J(k) with k basis functions. 
In the context of our problem, with multiple parameter subspaces of different 
dimensionality, it is necessary to devise different types of moves between the sub-
spaces, ek. These are combined to form what Tierney (1994) calls a hybrid sampler, 
making random choice between available moves at each transition, in order to tra-
verse freely around the combined parameter space. 
We use the following move types: (a) a movement in a knot location; (b) a change 
in a factor in a basis function; (c) a change in the basis coefficients; (d) the addition 
of a basis function and (e) the deletion of a basis function. Note that in steps (d) 
and (e) we are changing the dimension of the model and that we do not add basis 
functions in pairs as in the standard MARS forward-stepwise procedure; in fact, 
we depart completely from any sort of recursive partitioning approach. We have 
found that adding basis functions singly makes our procedure more flexible and the 
reversibility condition easier to satisfy (Denison, 1997). 
Thus the algorithm proceeds as follows. 
1. Start with only the constant basis function, B1, in the model. 
2. Set k equal to the number of basis functions in the current model. 
3. Generate a random quantity u uniformly on [0,1) and use this to determine 
which move step to perform. Thus with probability: bk goto BIRTH step (d), 
dk goto DEATH step (e), Pk goto MOVE step (a), 'r/k goto BIGMOVE step 
(b) and llk goto CHANGE step (c). 
4. Repeat 2 for a suitable number of iterations and collect every 5th model after 
the burn-in period is deemed to have ended. 
Note that the probabilities with which the different move types are undertaken in 
step 3 are chosen so that they sum to one. Specifically, we choose bk = 0.25 min{1, >.f(k+ 
1)},dk = 0.25min{l,k/>.} and the others are chosen so that Pk = 'f/k = 0.5vk. This 
type of formulation was suggested in Green (1995). 

224 
Mallick, Denison & Smith 
5.3 Bayesian MARS for GLMs 
When we change the MARS structures, as described below, the coefficients of 
the basis functions ~ in the new model can be found in a variety of ways. For 
the normal error regression model integrating out the coefficients using a conjugate 
prior for them is the most elegant method and leads to the acceptance probability 
just incorporating a Bayes factor (Holmes and Mallick, 1997). Unfortunately for 
the more difficult GLM examples we must draw the coefficients at each step using 
a simple Metropolis-Hastings proposal (Metropolis et al., 1953; Hastings, 1970). 
We propose the new coefficients from a normal distribution centered around their 
current values with dispersion matrix A. Proposing the new coefficients from a 
normal distribution with the same dispersion matrix as the prior is useful here as 
it leads to some cancellation in the acceptance probability. 
A problem with this specification is that the sampler moves slowly around the 
probability space because the proposal distribution is not diffuse. Hence, many 
iterations of the sampler must be made and we take only every lOOth one as being 
in the 'independent' sample. We still use this algorithm, however, because it ensures 
that the acceptance rates are adequate(> 10%). We have employed other strategies 
to sample the coefficients and these have produced better results but they are not 
fully Bayesian leading to the algorithm only performing a stochastic search of the 
model space. 
6. 
Examples of Bayesian MARS for GLMs 
In the examples that follow we ran the sampler in exactly the same way. That is 
for 500,000 burn-in iterations with every 100th iteration after that taken as being 
in the generated sample until 10,000 models were collected (another one million 
iterations). The Poisson mean for the number of basis functions was chosen to be 
two so that parsimonious models were encouraged. Note that we only demonstrate 
the model using the logit link for binary regression but any GLM can be handled 
in exactly the same way with the only difference being the likelihood. 
6.1 
Motivating Example 
We present this example purely to demonstrate how it can be the case that GLMs 
unrealistically restrict the form of the model and cannot handle some relationships 
for which nonparametric functions for the transformed mean work well. 
We use the kyphosis dataset which is available in Splus (Becker et al., 1988). The 
response is binary and represents the presence or absence of a postoperative defor-
mity known as kyphosis. There are 81 datapoints of which 17 had kyphosis after the 
operation. The original dataset has three predictors but, for illustrative purposes, 
we shall do logistic regression using just one of them, that is the Start predictor 
which gives the beginning of the range of vertebrae involved in the operation. 
We use the canonical link function for the binary data which is assumed to come 
from a Bernoulli distribution with an unknown parameter. Thus the g in (1.6) is 
taken to be the logit function, i.e.logit(p) = p/(1-J.t). Other possible link functions 
for data of this type are the probit and the complementary log-log link functions 
(McCullagh and Neider, 1989). 
Figure 12.1 compares the standard logistic regression model with the logistic 

0 
5 
10 
Start 
12. Semiparametric GLMs 
225 
FIGURE 12.1. Comparison of the classical logistic regression model (dotted line) with the 
logistic regression estimate using BMARS (solid line). The shading corresponds to the 95% 
Bayesian credible intervals. 
BMARS model. It appears that there is a most difference between the estimates 
when Start is over 15. The BMARS model estimates the probability of kyphosis 
as being significantly lower than the GLM when Start is over 15. 
The boxplots of Start for the two classes, given in Figure 12.2, confirm that when 
Start is high the presence of kyphosis is less likely, as captured by the non parametric 
Bayesian model. 
6.2 Pima Indian Example 
The Pima Indian dataset is another binary regression example but this one con-
cerns the presence or absence of diabetes among Pima Indian woman living near 
Phoenix, Arizona. There are eight covariates: number of pregnancies; plasma glucose 
concentration; diastolic blood pressure (mmHg); triceps skin fold thickness (mm); 
serum insulin (f.lU /ml); body mass index (kg m- 2); diabetes pedigree function and 
age in years. The data were collected by the US National Institute of Diabetes and 
Digestive and Kidney Diseases and can be obtained free of charge, from the website 
http://markov.stats.ox.ac.uk/pub/PRNN. 
An early study of this data was carried out by Smith et al. (1988) and it is ex-
tensively studied using a variety of methods, including logistic regression, in Ripley 
(1996). As in Ripley (1996) we omit the serum insulin predictor and use the 532 
complete records and split them into a training set of 200 points with a test set 
made up of the other 332 points. The best results given in Ripley (1996) have a 
error rate of about 20% (66 misclassified points) which is much better than the 
overall rate of diabetes of 33%. The logistic regression model was among those that 

226 
Mallick, Denison & Smith 
LO 
.,... 
t:O 
.$.,... 
CJ) 
LO 
absent 
present 
FIGURE 12.2. Boxplot of kyphosis data. Note how kyphosis is not present in any cases 
for which Start is greater than 15. 
performed well and had an error rate of 66/332 when fitted with all the predictors 
or when using stepwise selection to drop two of them (skin thickness and blood 
pressure). One reason that the standard G LM does so well is that the dataset is 
thought to be linear. 
The Bayesian MARS model has a misclassification rate of 67 points using the 
average of the whole sample and, as no interaction term is present in more than 
4% of the generated sample, it suggests linearity in the predictors. One of the main 
advantages of the Bayesian method is the measure of uncertainty we obtain over 
the predicted probabilities. If we need to make a prediction at a new location we 
find that we can do this with some certainty if the 95% Bayesian credible interval 
for its predicted probability does not include 0.5 but if it does we may decide that 
making a prediction is too difficult and therefore not worthwhile: we shall refer to 
these points as being unclear. 
In Figure 12.3 we display the results of our Bayesian GLM approach on the test 
set. It shows how the majority of points are correctly classified and, in particular, 
demonstrates how the unclear points are not necessarily those which are nearest the 
0.5 mark. 
Note that we have achieved better predictive results in simulations using Bayesian-
motivated stochastic searches of the posterior probability space. These typically in-
volve a crude maximization of the coefficients at each step. Good maximizations 
without gradient information and in many dimensions take too long to be useful 
in this iterative procedure. However, just this basic change can reduce the number 
of misclassified points to a level not yet reported in the literature (i.e. 61) at the 
expense of losing a truly Bayesian procedure. The improvement is mainly due to 
the fact that these stochastic searches tend to find many local modes but do not 
explore the tails of the distributions fully (Chipman, George and McCulloch, 1998; 

12. Semiparametric GLMs 
227 
~ 
I I 1111111111111 lllllmi~K-1** *+ 
++++++-HH1i~if-Ht1111111111111 
co 
0 
~ 
0 
~ 
0 
~ 
0 
q 
0 
11111111111111111111111111 ttl++ -HI+ H+ft-ft- 1IH- ++ t 
II 
~----------------~------~--------~------~ 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
Predicted probability of diabetes 
FIGURE 12.3. The 'rug' of ticks give the test datapoints and their predicted probabilities. 
Those identified with a'+' are the unclear points. 

228 
Mallick, Denison & Smith 
Denison et al., 1998b,d). For simple predictive purposes, where error bounds are 
not important, this seems to be a good strategy. 
References 
Becker, R.A., Chambers, J.M., and Wilks, A.R. (1988). The New S Language. 
Pacific Grove, CA: Wadsworth. 
Blight, B.J.N. and Ott, L. (1975). A Bayesian approach to model inadequacy for 
polynomial regression. Biometrika, 62, 79-85. 
Breiman, L., Friedman, J .H., Olshen, R. and Stone, C.J. (1984). Classification and 
Regression Trees. Belmont, CA: Wadsworth. 
Chipman, H., George, E.I. and McCulloch, R.E. (1998). Bayesian CART model 
search. J. Am. Statist. Assoc. (to appear). 
Craven, P. and Wabha, G. (1979). Smoothing noisy data with spline functions. 
Estimating the correct degree of smoothing by the method of cross-validation. 
Numer. Math., 31, 317-403. 
Dellaportas, P. and Smith, A.F.M. (1993). Bayesian-inference for generalized linear 
and proportional hazards models via Gibbs sampling. Appl. Statist., 42, 443-
459. 
Denison, D.G.T. (1997). Simulation based Bayesian nonparametric regression meth-
ods. Unpublished PhD Thesis. Imperial College, London. 
Denison, D.G.T. and Mallick, B.K. (1998). Classification trees. This volume. 
Denison, D.G.T., Mallick, B.K. and Smith, A.F.M. (1998a). Automatic Bayesian 
curve fitting. J. Roy. Statist. Soc. B, 60, 333-350. 
--- (1998b). A Bayesian CART algorithm. Biometrika, 85, 363-377. 
--- (1998c). Bayesian MARS. Statistics and Computing (to appear). 
--- (1998d). Discussion of Chipman, George, McCulloch. J. Am. Statist. Assoc. 
(to appear). 
Diaconis, P. and Ylvisaker, D. (1985) Conjugate priors for Exponential families. 
Annals of Statistics, 7, 269-281. 
Follman, D.A. and Lambert, D. (1989). Generalising logistic regression by non-
parametric mixing. J. Amer. Statist. Assoc., 84, 295-300. 
Friedman, J .H. (1991). Multivariate adaptive regression splines. The Annals of 
Statistics, 19, 1-141. 
Gelfand, A.E. (1997). Approaches for semiparametric Bayesian regression. Tech-
nical report. University of Connecticut, CT. 
Green, P.J. and Silverman, B.W. (1994). Nonparametric regression and generalized 
linear models: a roughness penalty approach. London: Chapman and Hall. 

12. Semiparametric GLMs 
229 
Guitierrez-Pena, E. and Smith, A.F.M. (1997). Aspects of smoothing and model 
inadequacy in generalized regression. Journal of Statistical Planning and In-
ference (to appear). 
Hastie, T.J. and Tibshirani, R.J. (1990). Generalized additive models. London: 
Chapman and Hall. 
Hastings, W.K. (1970). Monte Carlo sampling methods using Markov chains and 
their applications. Biometrika, 57, 97-109. 
Holmes, C.C. and Mallick, B.K. (1997). Bayesian wavelet networks for nonpara-
metric regression. Technical report. Imperial College, London. 
Laird, N. (1978). Nonparametric maximum likelihood estimation of a mixing dis-
tribution. J. Am. Statist. Assoc., 73, 805-811. 
Lindley, D.V. (1957). A statistical paradox. Biometrika, 44, 187-192. 
McCullagh, P. and Neider, J .A. (1989). Generalized Linear Models. London: Chap-
man and Hall. 
Mallick, B.K. (1994). Bayesian semiparametric modeling using mixtures. Unpub-
lished PhD Thesis. University of Connecticut, CT. 
Mallick, B.K. and Gelfand, A.E. (1994). Generalised linear models with unknown 
link functions. Biometrika, 81, 237-245. 
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E. 
(1953). Equations of state calculations by fast computing machines. J. Chern. 
Phys., 21, 1087-91. 
Morgan, J .N. and Sonquist, J .A. (1963). Problems in the analysis of survey data 
and a proposal. J. Am. Statist. Assoc., 58, 415-434. 
Morris, C. (1982). Natural Exponential families with quadratic variance functions. 
Ann. Statist., 10, 65-80. 
Neider, J.A. and Wedderburn, R.M. (1972). Generalized linear models. J. Roy. 
Statist. Soc. A , 135, 370-384. 
Newton, M.A., Czado, C. and Chappell, R. (1996). Bayesian inference for semi-
parametric binary regression. J. Amer. Statist. Assoc., 91, 1996. 
Ripley, B.D. (1996). Pattern Recognition and Neural Networks. Cambridge: Cam-
bridge University Press. 
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C. and Johannes, R.S. 
(1988). Using the ADAP learning algorithm to forecast the onset of diabetes 
mellitus. In Proceedings of the Symposium on Computer Applications in M edi-
cal Care (Washington, 1988). (Ed. R.A. Greenes), pp. 261-265. Los Alamitos, 
CA: IEEE Computer Society Press. 
Tierney, L. (1994). Markov chains for exploring posterior distributions (with dis-
cussion). Ann. Statist., 22, 1701-1762. 
Walker, S.G. and Mallick, B.K. (1997). Hierarchical generalized linear models and 
frailty models with Bayesian nonparametric mixing. J. Roy. Statist. Soc. B, 
59' 845-869. 

230 
Mallick, Denison & Smith 
Wedderburn, R.M. (1976). On the existance and uniqueness of the maximum like-
lihood estimates for certain generalized linear models. Biometrika, 63, 27-32. 

13 
Binary Response Regression with 
N orrnal Scale Mixture Links 
Sanjib Basu 
Saurabh Mukhopadhyay 
ABSTRACT Binary response regression is a useful technique for analyzing categor-
ical data. Popular binary models use special link functions such as the logit or the 
probit link. We propose Bayesian binary regression models where the inverse link 
functions H are scale mixtures of normal cumulative distribution functions. Two 
such models are described: ( i) H is a finite scale mixture with a Dirichlet distri-
bution prior on the mixing distribution, and ( ii) H is a general scale mixture with 
the mixing distribution having a Dirichlet process prior. Bayesian analyses of these 
models use data augmentation and Gibbs sampling. Model diagnostics by cross val-
idation of the conditional predictive distributions are proposed. The application of 
these models and the model diagnostic techniques are illustrated in student retention 
data. 
1. 
Introduction 
In many areas of applications of statistical principles and procedures one en-
counters observations that take one of two possible forms. Such binary data are 
often measured with covariates or explanatory variables that are either continuous 
or discrete or categorical. The relation between the response and the covariates is 
usually modeled by assuming that the probability of a "positive response", after 
a suitable transformation, is linear in the covariates. Let (Yi,!), i = 1, ... , N be 
the set of observations where !T = (xli, ... , Xki) is the set of covariates and the bi-
nary response Yi is either 0 or 1. Binary regression models assume that the random 
variables Yt, ... , YN are independent and 
P(Yi = 1) = H(!j §), 
i = 1, . .. ,N. 
(1) 
Here (j = (/31, ... , f3k )T is a vector of unknown parameters and the function H is 
usually assumed to be known. In the terminology of Generalized Linear Models 
(McCullagh and Neider (1989)), H is the inverse link function. For ease of expo-
sition, we refer to H as the link function in this article. Popular probit and logit 
models are obtained if H is chosen as the standard normal cumulative distribution 
function ( cdf) cJ> or the cdf of the standard logistic distribution respectively. Such 
choices of H are often done for convenience and on an ad hoc basis. 
231 

232 
Basu & Mukhopadhyay 
Prentice (1976), Aranda-Ordaz (1981), Guerrero and Johnson (1982), Stukel 
(1988) and many others studied binary regression with a parametric family of link 
functions (instead of a single fixed link) from a non-Bayesian viewpoint. Their work 
shows that these extended models can significantly improve fits. Recent Bayesian 
works on binary and polychotomous response regression with an extended class of 
link functions include Doss (1994), Albert and Chib (1993), Newton, Czado, and 
Chappell (1996), and Erkanli, Stangl, and Miiller (1993). 
Notice that (1) requires the range of H to be [0, 1]. Usually it is also preferable to 
have a nondecreasing smooth function H. These requirements match exactly with 
a smooth continuous cumulative distribution function ( cdf). Let :F be the class of 
all cdfs on ~. The family :F includes cdfs which are often undesirable as choices 
for H, for example, cdfs of discrete distributions. Instead, Basu and Mukhopadhyay 
(1998) (Basu and Mukhopadhyay (1998) from now on) consider the subclass of 
normal scale mixture cdfs :FN = { FN(·) = J <P(i-) dG(o-), G is a cdf on [0, oo)} 
[O,oo) 
as possible choices for the function H. The class of normal scale mixtures allows 
a variety of functional forms and varying tail structures (including normal, all t 
distributions, Logistic, Double Exponential, and Cauchy), thus presenting us with 
a wide array of choices for the link H. 
For H ( ·) = FN ( ·) = J <P(-;) dG( o-), our binary regression model ( 1) becomes 
P(Yi = 1) = J 
<P({i[ /}}/o-)dG(o-), 
i = 1, ... ,N, 
(2) 
which includes two unknowns, the regression coefficient /} and the mixing distri-
bution G. To complete the Bayesian model specification, a prior 1r(§, G) is as-
sumed on the unknowns. Typically, (j and G are assumed independent a priori, i.e., 
1r(/}, G)= 1r1(/}) 1r2(G). 
The posterior distribution 1r(/}, GIJ!) which combines the prior 1r(/}, G) and the 
sampling model of (2) is analytically intractable. In the spirit of Albert and Chib 
(1993), Basu and Mukhopadhyay (1998) describe a data-augmented Gibbs sampling 
method (Gelfand and Smith (1990),Tanner and Wong (1987)) which obtains Monte 
Carlo estimates of the posterior and other quantities of interest. 
Basu and Mukhopadhyay (1998) assume a finite mixing distribution with pre-
specified mixing location. In this article, we extend that finite mixture model to a 
general mixture model. We assume that the mixing distribution G is an arbitrary 
member of the classy:+= { F is a cdf on [0, oo)}. To complete our Bayesian model 
specification, we assume a Dirichlet process prior DP(o: ·v) on G. Here o: > 0 is 
the concentration parameter and v is a cdf on [0, oo). The implementation of this 
model is discussed in section 3. 
Model checking is an integral part of any model building. We consider several 
cross validation model checking criteria that are suggested by Gelfand, Dey, and 
Chang (1992). Computations of these criteria in our models are described in section 
section 4. In section section 5, we illustrate an application of these models to student 
retention data from the University of Arkansas. Conclusions are given in section 6. 
A word about notations. Throughout this article, we use upper case letters, Y, Z, 
to denote random variables and lower case letters, y, z, to denote observed values 
or running variables. Also, we use F, G to denote distributions and cdfs. 

13. Binary response regression 
233 
2. 
The Finite Mixture Model 
In this section, we briefly describe the finite normal scale mixture link considered 
by Basu and Mukhopadhyay (1998). In a typical binary regression setup, the ob-
served data consist of (Yi, ~ i), i = 1, ... , N, where Yi is the binary response and ~ i 
is a set of covariates which are either continuous or categorical. The binary response 
Yi 's are assumed to be independent Bernoulli ( fh). The normal scale mixture link 
model assumes fh = J<I>({~f (}}lu)dG(u) as in (2). 
In their finite mixture model, Basu and Mukhopadhyay (1998) assume that the 
8 
mixing distribution G is discrete with finite support, i.e., G = ?: Pi 6{ri} where 
J=1 
8 
0 ::; Pi ::; 1, 2:: Pi = 1, 6{ ri} is the degenerate distribution at ri, and the support 
i=1 
points 0 < r1 < ... < r 8 < oo are prespecified. Basu and Mukhopadhyay (1998) 
assume a Dirichlet distribution prior 1r2(G) = DD(!:) on the mixing proportions 
p = (p1, ... , p8 )T. They further assume an independent prior 11"1 ((}) on (}. 
"" The posterior distribution of(} and Gunder this model specification is 1r((}, Gly) = 
N 
constant. 11"1((}) 11"2(G) n {f <f>( {~!(}}I u) dG( u)}. This posterior distribution is ana-
i=1 
' 
lytically intractable. The Gibbs sampler provides a simulation based computational 
approach which enables one to obtain Monte Carlo estimates of posterior quantities 
of interest. General discussions about the Gibbs sampler, its implementation, and its 
convergence can be found in Gelfand and Smith (1990), Gelman and Rubin (1992), 
Casella and George (1992), Tierney (1994) and in the recent book by Gilks et al 
(1998). In the finite normal mixture setup Basu and Mukhopadhyay (1998) show 
that the implementation of the Gibbs sampler is further facilitated by introduction 
of latent random variable£= (Z1, ... , ZN )T and!!= (u1, ... , uN)T (as in Albert 
and Chib (1993)). The complete structure of the Basu and Mukhopadhyay (1998) 
model along with the distribution of the latent variables is described below: 
(a) Given(} and the latent£', the latent variables z1, ... 'ZN are independent with 
Zi r-.J N~f (}, u~). 
(b) Given g; Y1, ... , YN are completely determined with Yi = 1 if Zi > 0, and 
Yi = 0 otherwise. Note that marginalized over Zi, we have P(Yi = 1) = 
<f>( {g:f (}}I Ui)· The latent zi 's thus provide a flexible way to introduce the 
probit structure in the binary link. This flexible conditional representation of 
the probit model was first introduced by Albert and Chib (1993). 
(c) Given G, the latent variables u1, ... , UN are i.i.d. I'V G. These latent Ui's incor-
porate the normal scale mixture structure. 
(d) 
8 
The mixing distribution G is discrete with finite support, i.e., G = 2:: Pi 6{r·}. 
i=1 
J 
Here the support points 0 < r1 < ... < r 8 < oo are user specified and 
E = (p1, ... ,p8 )T has aDirichlet distribution prior 1r2(E) = DD(J:). 
(e) The regression parameter (} is independent of G and has a prior 1r1 ((}). 
To recapitulate, P(Yi = 1l£',f!,G) = P(Zi > Ol£',f!,G) = <f>({~f (}}lui)· Integrated 
over Ui, P(Yi = 11(}, G) = J <f>( {~f (}}I Ui) dG( ui), which is our model of (2). 
For this finite mixture model, Basu and Mukhopadhyay (1998) obtain the full 
conditional distributions of each unobserved variable given the observed J! and the 

234 
Basu & Mukhopadhyay 
remaining variables. These distributions are needed for the Gibbs generation. In-
troduction of the latent variables Z and u substantially simplifies the calculation of 
the conditional distributions. Each conditional density is obtained in a closed form 
and is, in fact, in the form of a common distribution and hence is easy to simulate 
from. 
3. 
General Mixtures and a Dirichlet Process Prior 
In the finite mixture model of Basu and Mukhopadhyay (1998), the mixing dis-
a 
tribution G = 2.: Pi6{r·} is discrete with finite support. The user needs to choose 
J=1 
J 
the support points of the mixing distributions, 1"1 < r2 < ... < r 8 • In this section, 
we provide further flexibility in the specification of G and make the specification 
process more automated than the finite mixture model. The mixing G is allowed to 
be a general distribution E :F+ where :F+ is the class of all distributions on [0, oo ). 
As a natural extension of the Dirichlet distribution prior of section 2, we assume a 
Dirichlet process DP( a· Go) prior (see Ferguson (1974)) on Gover the domain :F+. 
DP priors have received considerable attention in recent Bayesian literature. They 
provide a flexible way to incorporate the user's prior knowledge about G into the 
analysis. The hyperparameter Go is the prior mean of G and represent the user's 
guess about G. The "concentration" parameter a > 0 reflects the degree of "close-
ness" of the random G to the prior mean Go and represent the user's degree of 
belief about Go. 
DP priors were first introduced by Ferguson (1974). These priors introduces flex-
ibility in the model; however the computations get more intensive and closed form 
expressions become hard to obtain. The recent attention in Bayesian Modeling with 
DP priors stemmed from the MCMC based computational breakthroughs obtained 
by Escobar (1994), MacEachern (1994) and others. For different applications of 
DP priors, see Escobar and West (1995), Basu (1996), Mukhopadhyay and Gelfand 
(1997) and the references therein. 
The structure of the mixture model that we develop in this section is a general-
ization of the finite finite mixture model described in section 2. The generality is 
introduced in the generic structure of the mixing distribution G and in the specifi-
cation of its prior distribution. Thus, parts (a), (b) and (e) of the same as in section 
2. Parts (c) and (d) become more general as described below. For convenience in 
latter calculations, we reparametrize the models in terms of .-\i = 1/uf. 
(c*) Given G; the latent variables .-\1 = 1/u~, ... ,.-\N = 1/u'Jv are i.i.d. ,_G. 
(d*) The random distribution G follows a Dirichlet process prior, i.e., G ,_ DP(a · 
Go). 
We use a methodology based on Gibbs sampling to compute posterior quantities 
from this model. Note that to implement the Gibbs sampler, one needs to gen-
erate Monte Carlo samples from the conditional distributions of each unobserved 
variable given the observed y and the other unobserved variables iteratively. How-
ever, generating from ?T( G (y, (}, g,!!) is almost impossible, since this will require 
generating a whole distribution G on [0, oo). Instead, we consider the marginalized 
(integrated) model over G. Notice that, conditional on G, .-\1, ... , AN are i.i.d,..., G. 
This implies that marginally each .,\i ,_ G0 • However, when marginalized over G, 

13. Binary response regression 
235 
Ai 's are no longer independent, in fact, Ai has a positive probability of being equal 
to the previous Aj's, 1 ~ j < i. This fact was first pointed out by Antoniak (1974)). 
The joint distribution of the Ai 's can be obtained from their successive conditional 
distributions 
i-1 
E 8p,}(·). 
(3) 
j=1 
The successive conditional posterior distributions of the Ai 's, i.e., Ai I y, A1, ... , Ai-b 
however, do not have such a simple structure. Escobar (1994) ;howed (in the 
context of a Normal means problem) that if one considers instead the full con-
ditional posterior distribution of Ai given all the other Aj's, i.e., Ai 11', A(-i) = 
(A1, ... , Ai-1, Ai+1, ... , AN), then this distribution has a simple form and is often 
easy to simulate from. Note that these full conditional distributions are the ones 
required for Gibbs sampling. MacEachern (1994) later obtained a modification of 
Escobar's Gibbs sampling algorithm over a collapsed state space. Our implementa-
tion of Escobar's algorithm is similar to the one used by Erkanli, Stangl, and Muller 
(1993). 
The full conditional distributions of the other parameters and latent variables are 
relatively easy to obtain. 
( i) Given 11, /}, ~ and G, the latent variables Z1, ... , ZN are conditionally indepen-
dent. Moreover, each Zi conditionally follows a truncated N(J;j §, ul) distri-
bution. The distribution is truncated at left by 0 if Yi = 1, and is truncated 
at right by 0 if Yi = 0. 
For the following conditional distributions in ( ii)- ( iv) below, we assume that 
the givenl' and3 satisfy (Yi- !)zi > 0, i = 1, ... ,N. 
(ii) Given 1',§, ~and the other A(-i) = (Ab ... , Ai-b Ai+1, ... , AN), the full condi-
tional distribution of Ai is c · v'Xi ¢( {Zi- ~r /}}v'Xi) {aGo(·)+ 2: 8p;}(·)}. 
j'f;i 
Here cis a normalizing constant. 
(iii) If we assume a customary diffuse prior 1r1(!}) := 1, then the full conditional 
distribution of the regression parameter /} given data y and the latent vari-
ables 3,~ is Nk(§,(xTwx)- 1) where§= (XTWx)- 1XTw3 , w = 
diagonal(Ai), and X= ~ 1
, ... '~NJT is the design matrix (we assume rank(X) = 
k). See, for example, Pilz (1991). For other prior choices of 1r1(/}), the full con-
ditional distribution of/} can be similarly calculated (from Bayesian Linear 
model theory) and is sometimes obtained in a common form. 
The Gibbs sampler starts from an initial guess (~
0
, ~ 
0 ,/}
0
) and successively simu-
lates Monte Carlo samples from each of these conditional distributions by condition-
ing on the latest sampled value of the other variables at each stage. The simplest 
step in the Gibbs iterations is the generation of the regression vector /} from the 
multivariate normal conditional distribution in (iii). The generation of Zi's from 
the conditional distribution in ( i) is also relatively straightforward. Numerical prob-
lems may arise if the truncation moves to the tail of the normal distribution. See 
Daganpur (1988) for simulation techniques from truncated distributions. 
The simulation of Ai in ( ii) may not be so immediate. However, in typical ap-
plications, one chooses Go to be conjugate to the normal likelihood, v'Xi¢( {Zi-

236 
Basu & ~ukhopadhyay 
~f (}}A). For example, one may choose Go to be a Gamma(v /2, v /2) distribution 
(with density proportional to ).. 1112 - 1 exp( -v A/2)). In such a case, the "marginal" 
Ai = f JXi¢( {Zi - ~f (}}A)dGo(>..i) can be obtained in closed from. The sim-
ulation of )..i is then done in two stages. For each j 
=f=. i, the random variable 
)..i equals a previously generated Aj with probability VXi¢( {Zi- ~f (}}.jXj)/ {Ai + 
2.: JXi¢( {Zi-~! (}}.jXj)}, the random variable )..i· With the remaining probability 
j# 
' 
of Ai/{Ai+ 2.: VJ;i¢( {Zi-~! (}}.jXj)}, the random variable )..i follows the distribu-
j¢i 
' 
tion which would result as the posterior distribution of >..i if VJ;i¢( { Zi - ~T §}vXi) 
is the likelihood and Go ( )..i) is the prior. 
Notice that at any particular Gibbs iteration, the number of distinct )..'s is often 
much less than N, and therefore the computational effort can be reduced by a 
significant amount if we keep track of distinct values of ).. 's and their frequencies. 
4. 
Model Diagnostic 
4.1 
Basic Goal 
Model checking is an integral part of any data analysis. Common logit and pro bit 
models provide G2 and x2 statistics as summary measures of the overall quality 
of fit. Residuals comparing observed and fitted values are also used. As Agresti 
(1990) writes, "such diagnostic analyses help show whether lack of fit is due to an 
inappropriate link function or perhaps due to nonlinearity in effects of explanatory 
variables." 
In this section, we describe diagnostic tools for our Bayesian binary response 
model. Many eminent Bayesians including Geisser and Eddy (1979), Box (1980), 
Berger (1985), and Gelfand, Dey, and Chang [GDC] (1992) argue for using the pre-
dictive distribution in such diagnostics. Basu and Mukhopadhyay (1998) followed 
GDC (1992) in combining cross validation with this predictive approach. The di-
agnostic tools developed by Basu and Mukhopadhyay (1998) compare predictive 
distributions conditioned on the observed data with a single data point deleted 
against observed responses. We briefly describe these diagnostics tools below. 
In binary regression, typically many independent binary responses, Yi 's are ob-
served under the same covariate vector x .. Let L be the number of distinct x. 's, 
_, 
_, 
and we denote them by ~~, ... ,~1· Let nk be the total number of binary Y's ob-
L 
served under li~ ( 2.: nk = N) out of which Tk ( = 
2.: 
Yi) are 1 's. According to 
k=l 
i:X =X* 
,..,., i 
,...., k 
our sampling model, T1 , •.. , TL are independent and Tk """' Binomial(nk, fh) where 
fh = H(l:*T (}). For model checking, Basu and Mukhopadhyay (1998) cross validate 
the sufficient statistics T1, ... , TL (instead of the Y's). Let tk be the observed value 
of Tk, ! be the L x 1 observed data vector, and let ! ( _ k) denote the ( L - 1) X 1 
vector with kth observation tk deleted. Also, let w = ((}, G, ~,![) denote the set 
of unobserved variables. We use customary notations; f denote predictive distribu-
tions (e.g. f(Tk I! ( -k))) as well as sampling distributions (f(Tk I w)), and 7r denote 
priors ( 1r( w)) as well as posteriors ( 1r( w 11)). We assume all relevant integrals in 
the following exist. 

13. Binary response regression 
237 
4. 2 
Diagnostic Tools 
Basu and Mukhopadhyay (1998) develop diagnostic tools from a cross validated 
predictive approach and examine f(Tk 11 (-k))' i.e., the predictive distribution of 
the random variable Tk conditioned on the remaining observations 1 (-k)' Following 
GDC, a random Tk from the predictive distribution f(Tk 11 ( -k)) is compared 
against the observed value tk. This comparison is done by the following two criteria. 
See GDC (1992) for other checking criteria and more details. 
(a) 
(b) 
dlk = expected difference between the observed tk and the random Tk, i.e., 
tk -
p,~ where P,k is the mean of the distribution f(Tk 11 (-k)). dlk are thus 
the familiar residuals. The studentized residuals as dik = ~ where s~ = 
Var[Tk 11 (-k)] can also be used for diagnostic. Basu and Mukhopadhyay 
(1998) use the quantity Q1 = L':(dik)2 as a summary index of model fit. 
d2k = f(tk 11(-k)), i.e., the likelihood of observing Tk = tk given the remain-
ing observations 1 (-k)' Small values of d2k critize the model. Following the 
suggestion ofGeisser and Eddy (1979) and GDC (1992), Basu and Mukhopad-
L 
hyay (1998) use Q2 = TI d2k as the second summary index of model fit. No-
k=l 
tice that Q2 can be interpreted as a joint pseudo-marginal likelihood of the 
observed t. 
Computational Methods 
To compute dik and d2k, we need the mean Jl.k, the variance s~, and the value 
f(tk It ( -k)) of the predictive distribution f(Tk 11 ( -k)). Notice f(Tk 11 ( -k)) = 
J f(Tk I w)1r(w I !(-k)). One possible strategy to approximate f(Tk 11(-k)) is 
as follows : ( i) delete t k from the observed data vector 1 to obtain 1 ( _ k); ( ii) 
use Gibbs sampling (sections 2. and 3.) to generate R Monte Carlo samples of Wr 
from 1r(w I 1(-k)); and (iii) approximate f(Tk 11(-k)) by the Monte Carlo sum 
R 
-k I: f(Tk I wr)· Since we need f(Tk li(-k)) for every k = 1, ... ,L, this strategy 
r=l 
would require L separate Gibbs sampling runs. However, this can be substantially 
simplified using some neat tricks and manipulations involving the relevant posterior 
distributions and marginals. Basu and Mukhopadhyay (1998) show that all the 
relevant quantities needed for the computation of the diagnostic tools dik and d2k 
can be efficiently calculated from a single Gibbs run. We refer the reader to Basu 
and Mukhopadhyay (1998) for further details. 
5. 
Application: Student Retention at the University of 
Arkansas 
In this section, we illustrate our general mixture model using student reten-
tion data from the University of Arkansas. We further compare the performance 
of our general mixture model with the finite mixture model proposed by Basu 
and Mukhopadhyay (1998). Two other interesting applications of the finite mixture 

238 
Basu & ~ukhopadhyay 
model can be found in Basu and Mukhopadhyay (1998), one on Beetle mortality 
data and the other in Challenger o-ring distress data. 
The student retention data of this example is obtained from the Retention and 
Graduation Report of Donnelly and Line (1992) on the undergraduate students of 
the University of Arkansas. Freshmen classes entering the University of Arkansas 
in 1981, 1984-86, 1988, and 1989, consisting of N = 13890 students, are tracked 
in this report. One variable of interest is third year retention, i.e., Y = 1 if the 
student is still enrolled in his/her 3rd year fall semester, Y = 0 if sf he has dropped 
out. Retention is a concern to the University of Arkansas because of the relatively 
high dropout rate. The explanatory variables we use are ACT (American College 
Test given to graduating high school students) score as a continuous variable and 
(starting) year as a categorical variable. Preliminary plots suggest a second degree 
model in terms of ACT. We consider the model P(Yi = 1) = H(!:f (}) for the ith 
student where JEi is an 8 x 1 vector with Xi! = 1, Xi2 =ACT, Xi3 = (ACT)2 , Xi4 = 1 
if 1981, = 0 otherwise and so on. 
The 13890 students are divided into 48 groups according to their ACT scores and 
starting years. We use nk to denote the number of students in kth group and tk to 
denote the number retained in the 3rd year. 
A preliminary maximum likelihood logistic regression analysis shows that both 
ACT and (ACT)2 have "significant" effect (in Frequentist terminology) on P(Y = 1) 
whereas the categorical covariate Year is "insignificant". However, since our mod-
els use different link functions and since the frequentist findings may not immedi-
ately translate to the Bayesian models, we keep all 8 x-covariates in our models. 
We explore two Bayesian models. The two models differ in their link structures, 
P(Yi = 1) = H(j;f (}). The first model is the finite mixture model (MF) of Basu 
and Mukhopadhyay (1998) where the link function His assumed to be a finite scale 
81 
mixture of normal cdfs, i.e., H(·) = 2:: Pi iP(.jrj) and the mixing probabilities has 
j=l 
a Dirichlet distribution, i.e.,~ "'DD(J!). The second model is our proposed general 
mixture model (MG) where H(·) =Jib(· VA) dG(A) and the mixing distribution G 
has a Dirichlet process prior, i.e., G"' DP(a ·Go). 
In MF, we choose the set of Tj 's as T = {0.5, 1, 2, 3, 4, 6, 8, 10, 15, 20, 30}. This 
choice reflects our subjective feeling that there should be some small T values, a good 
representation of moderate r values and some large T values. We choose equal values 
for the Dirichlet distribution parameter, v1 = ... = vu = a/11 where a > 0 is a 
constant. For model MG, Go is taken to be the Gamma(1, 1) distribution. Several 
different values of a are tried in both models MF and MG. The summary criteria 
that we use for model fitting and checking (described below) differ for different 
choices of a, but not to any significant extent. The "best" results are obtained from 
a = 1 in MF and a = 8 in MG 
For the finite mixture model MF, we use the sampling based method proposed by 
Basu and Mukhopadhyay (1998). For the general mixture model MG proposed in 
this article, we use the Gibbs sampling method described in section 3. The results 
are shown in Table?? and Figure 13.1. 
We use the sum of squared differences between the observed and the expected 
L 
counts, i.e., SSE = 2:: (tk - nk E[H(!:f (}) 11])2 as a summary index of model fit. 
j=l 
The values of SSE for the two models MF and MG are listed in Table ??. In this 
table, we also list the summarized diagnostic indices Ql = 2:: d~k 2 and Q2 = n d2k. 
Note that small values of SSE, Q1 and large values of Q2 are preferable. The 

13. Binary response regression 
239 
TABLE 13.1. Student retention data : SSE and summary model diagnostic measures 
Finite mixture model MF 
General mixture model MG 
SSE 
2890.63 
2769.44 
48 
2 
Ql = L: d~k 
53.91 
51.23 
k-1 
02= 
~ 
1.49 x 1o-70 
8.84 x 10-70 
d2k 
k=l 
general mixture model MG does better in all the summary criteria. For each of 
the two models MF MG, we also plot the diagnostic criteria d~k and d2k for each 
observation. These plots are shown in Figure 13.1. For each model, these diagnostic 
plots identify several observations which have large d~k and large negative log( d2 ~:) 
(hence small d2k) values, for example, the 2nd, 4th, 19th, 21st and 41st observations. 
These are the possible influential observations of the models and may require further 
analysis. 
6. 
Discussion 
In this article, we have extended the finite normal scale mixture link model of 
Basu and Mukhopadhyay {1998) to a Bayesian model of binary response regression 
which can use general normal scale mixtures as the link function H. This extension 
provides further flexibility to the user and at the same time, make the analysis more 
automated and adaptive to data. We describe a Gibbs sampling based method which 
can be used to efficiently calculate various posterior quantities of interest. Further, 
we provide several model diagnostic measures and show how they can be easily 
computed from the same Gibbs sampler run. 
Note that the normal scale mixture links which we use (either in the finite mixture 
or the general mixture model) always produce symmetric links due to the symmetry 
of the normal distribution. Several authors have described data where asymmetric 
links are called for. One possible way to generate asymmetric links is by considering 
both location and scale mixtures of normals, as in Erkanli, Stangl, and Miiller 
(1993). Basu and Mukhopadhyay (1998) provide another alternative asymmetric 
link structure by considering scale mixtures of truncated normal distributions. This 
asymmetric link structure has many attractive features but also brings in some 
computational hurdles. We refer the reader to Basu and Mukhopadhyay (1998) for 
further details. 
References 
Agresti, A. (1990). Categorical Data Analysis. John Wiley. 
Albert, J .H. and Chib, S. (1993). Bayesian Analysis of Binary and Polychotomous 
Response Data. J. Amer. Statist. Assoc., 88, 669-679. 
Antoniak, C.E. (1974). Mixture of Dirichlet Processes with Applications to Bayesian 
Nonparametric problems. Ann. Statist .. 2,1152-1174. 

240 
Basu & Mukhopadhyay 
3 
2 
-2 
-3·~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
0 
-1 
-2 
-3 
-4 
-5 
-6 
-7 
-
-
0 
' 0 
~ 
+ 
+ 
+ 
10 
20 
30 
40 
50 
Pattern 
._._. Finite 
o o o General 
! 
... 
1 
l 
! 
~ 
~ 
• 
+ 
~ 
! 
~ ~ 
l 
' 
I 
I 
' 
• 
I 
10 
20 
30 
40 
50 
Pattern 
._._. Finite 
+ + + General 
FIGURE 13.1. Model diagnostics of Student retention data for the MF and MG models. 
The top figure plots dik against observation number (1-48). The bottom figure plots 
log(d2k) against observation number 
Aranda-Ordaz F.J. (1981). On two families of transformations to additivity for 
Binary Response data. Biometrika, 68, 357-363. 
Basu, S. (1996). Bayesian tests for unimodality. Tech. Rept., Univ. of Arkansas. 
Basu, S. and Mukhopadhyay S. (1998). Bayesian analysis of binary regression using 
symmetric and asymmetric links. Manuscript. 
Berger, J. (1985). Statistical Decision Theory and Bayesian Analysis. Springer-
Verlag, New York. 
Box, G. (1980). Sampling and Bayes' inference in scientific modeling and robustness 
(with discussion). J. R. Statist. Soc., Ser. A, 143, 382-430. 
Casella, G. and George, E.I. (1992). Explaining the Gibbs Sampler. Amer. Statis-
tician. 46, 167-174. 
Dagan pur, J. (1988). Principles of Random Variate Generation. Oxford University 
Press, Oxford. 
Devroye, L. (1986). Nonuniform Random variate Generation. Springer-Verlag, 
New York. 
Donnelly, S. and Line, K. (1992). Retention and Graduation Report. Division of 
Student Services, Univ. of Arkansas. 
Doss, H. (1994). Bayesian Non parametric Estimation for Incomplete Data via Suc-
cessive Substitution Sampling. Ann. Statist., 22, 1763-1786. 

13. Binary response regression 
241 
Escobar, M.D. (1994). Estimating Normal Means with a Dirichlet Process Prior. 
J. Amer. Statist. Assoc., 89, 268-277. 
Escobar, M.D. and West, M. (1995). Bayesian Density Estimation and Inference 
Using Mixtures. J. Amer. Statist. Assoc., 90, 577-588. 
Erkanli, A., Stangl, D., and Muller, P. (1993). A Bayesian analysis of ordinal data 
using mixtures. (to appear in Can ad. J. Statist.). 
Ferguson, T. (1974). Prior Distributions on Spaces of Probability Measures. Ann. 
Statist., 2, 615-629. 
Geisser, S. and Eddy, W. (1979). A predictive approach to model selection, J. 
Amer. Statist. Assoc .. 74, 153-160. 
Gelfand, A.E., Dey, D.K., and Chang, H. (1992). Model determination using pre-
dictive distributions with implementations via sampling-based methods. In 
Bayesian Statistics 4, J.M. Bernardo, et. al. (Eds.), Oxford University Press, 
Oxford. 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling-Based Approaches to Calcu-
lating Marginal Distributions. J. Amer. Statist. Assoc., 85, 398-409. 
Gelman, A. and Rubin, D. (1992). Inference from iterative simulation using mul-
tiple sequences. Statist. Sci., 7, 457-476. 
Gilks, W.R., Richardson, S., and Spiegelhalter, D.J. (1996). Markov Chain Monte 
Carlo in Practice. Chapman and Hall, Glasgow. 
Guerrero, V.M. and Johnson, R. (1982). Use of the Box-Cox transformation with 
Binary Response models. Biometrika, 69, 309-314. 
MacEachern, S.N. (1994). Estimating normal means with a conjugate style Dirich-
let process prior. Comm. Statist.-Simula., 23, 727-741. 
McCullagh, P. and Neider, J. (1989). Generalized Linear Models. 2nd ed., Chapman 
and Hall. 
Mukhopadhyay, S. and Gelfand, A. E. (1997). Dirichlet Process Mixed Generalized 
Linear Models. J. Amer. Statist. Assoc., 92, 633-639. 
Newton, M.A., Czado, C., and Chappell, R. (1996). Semiparametric Bayesian in-
ference for binary regression. J. Amer. Statist. Assoc., 91, 142-153. 
Pilz, J. (1991). Bayesian Estimation and Experimental Design in Linear Regression 
Models. John Wiley. 
Prentice, R.L. (1976). A Generalization of the Probit and Logit models for Dose 
Response Curves. Biometrics, 32, 761-768. 
Stukel, T.A. (1988) Generalized Logistic Models. J. Amer. Statist. Assoc .. 83, 
426-431. 
Tanner, T.A. and Wong, D.H. (1987). The Calculation of Posterior Distributions 
by Data Augmentations. J. Amer. Statist. Assoc., 82, 528-549. 
Tierney, L. (1994). Markov chains for exploring posterior distributions (wtih dis-
cussions). Ann. Statist., 22, 1701-1762. 

This Page Intentionally Left Blank 

14 
Binary Regression Using Data 
Adaptive Robust Link Functions 
Ruben A. Haro-L6pez 
Bani K. Mallick 
Adrian F. M. Smith 
ABSTRACT We present binary regression models that choose an arbitrary link 
function from a set of different inverse cumulative functions produced by a partic-
ular family of parametric distributions. This 'data adaptive' Bayesian analysis is 
implemented using cumulative functions of scale mixtures of normal distributions. 
The scale mixtures of normal distributions are symmetrical families of distributions 
that provide more parameters than the usual location and scale parameters that 
characterize the normal distribution. 'Data adaptive' Bayesian analysis of the bi-
nary regression model is performed using the shape parameter of the exponential 
power distribution. This methodology is implemented applying Markov chain Monte 
Carlo (MCMC) simulation methods. 
1. 
Introduction 
Binary response data, measured with covariates, are often modeled by assuming 
that the probability of a positive response, after a suitable transformation, is linear 
in the covariates. This transformation, usually known as link function, connects the 
probability to a linear predictor and is normally assumed to be a known function, 
for instance the inverse cumulative function of a certain distribution. McCullagh 
and Neider (1989) provides a comprehensive discussion of the use of link functions 
to model the relation between a binary response and linear covariates. 
The most commonly used binary regression models involve logit or probit link 
functions. The probit is obtained when the normal cumulative function is utilized 
and the logit when the logistic cumulative function is used. Bayesian analysis of 
the logit regression model using the Gibbs sampler is described in Dellaportas et 
al. (1993) and Albert and Chib (1993) extended the Bayesian analysis of the pro bit 
regression model to models with link functions defined through the scale mixtures 
of normal distributions. The latter, introduced in Andrews and Mallows (1974), 
are families of symmetrical distributions that provide more parameters than the 
usual location and scale parameters that characterize the normal distribution. The 
exponential power distribution is an example of a family of scale mixtures of normal 
distributions, where the extra parameter controls the distributional shape. 
In this article, we present binary regression models that choose an arbitrary link 
243 

244 
Haro, Mallick & Smith 
function from a set of different inverse cumulative functions, the latter produced by 
a particular family of scale mixtures of normal distributions that possess a shape 
parameter. This 'data adaptive' Bayesian analysis is a method of obtaining auto-
matic robustness with respect to the link function choice. This methodology does 
not constrain the statistical analysis to a fixed link function just as in Dellaportas 
et al. (1993) and Albert and Chib (1993). 
'Data adaptive' Bayesian analysis of the binary regression model is implemented 
using the Gibbs sampler. As in Albert and Chib (1995), we define Bayesian resi-
duals which possess a continuous-valued posterior distribution and use these to 
study potential outlying observations. Analysis of fitted posterior probabilities is 
used to compare different adjusted models. Finally, this methodology is illustrated 
with the analysis of the low birth weight in infants dataset given in Hosmer and 
Lemeshow (1989). 
2. 
The Binary Regression Model 
The 'data adaptive' Bayesian analysis for the binary regression model after obser-
ving the set of n independent binary (0 or 1) observations w = (wb ... , wn)T 
associated with k-dimensional row vectors of regressors Xi = (xil, ... , Xik), i = 
1, ... , n, is obtained as in Albert and Chib (1993) by constructing random variables 
Yi such that 
for i = 1, ... , n, with 
{ 
1 
Yi > 0 
Wi = 
0 
Yi:::; 0 ' 
and 
(1) 
i.e. Yi is distributed as a scale mixture of normal distributions, where Nk(x!J.L, E) 
denotes the k-dimensional normal distribution with probability density function 
IEI-
11
2 
( 
1 
) 
fN~e(x!J.L, E)= (27r)k/2 exp - 2(x- J.L)TE- 1(x- J.L) 
, 
II(.Aila) is the scale mixture parameter distribution for Ai E JR+, a E (a, b) is 
the shape parameter, f3 = ({31, ... , f3k)T E JRk is the column vector of unknown 
regression coefficients and h( ·) is a positive function. 
The random quantities Yi will produce the link function with the linear predictors 
and will imply the following posterior distribution 
1r({:l, alw) ex fJ L l'" /N, (y; lx;{:l, h( A;)) 1r( A; Ia) dy; d.\; 1r({:l, a), 
(2) 
where there is a linear relationship between Yi and Xi, and 
A. _ { ( -oo, 0) 
if Wi = 0 
~ -
(0, 00) 
if Wi = 1 ' 
for i = 1, ... , n. The prior distribution is specified as 
1r({3, a) = 11'N~e (f3lv, C) 1ru( ala, b), 
with known hyperparameters v E JRk and C E M(k), where M(k) denotes the set 
of all positive definite k x k matrices and U(aja, b) denotes the uniform distribution 
in (a, b). 

14. Binary regression 
245 
Remark 1 To eliminate sources of confounding, the scale mixture of normal distri-
butions utilized in the model (1) should have known location and scale parameters. 
Otherwise, the location can be confounded with the intercept term fh and the scale 
can be confounded with the overall scale of the regression coefficients. It is therefore 
natural to restrict this family of link functions to a fixed interquartile range or a 
smooth (bounded) variance. See Newton et al. for a related discussion. 
Defining u-1 = 2:::?:1 h(Ai)-1 xr Xi+C- 1' A= (Al, ... 'An)T andy= (Y1' ... 'Yn)T' 
analysis of equation (2) can be performed applying the Gibbs sampler simula-
tion technique to obtain a sample from the marginal distribution 7r(,l3, ajw) of 
7r(,l3, a, A, yjw ); see, for example, Smith and Roberts (1993). The Gibbs sampler 
structure is given by the following full conditional densities, 
f(f31w, .X, y, a) = !N. ~ U (~ h(~,) xf+ c-1v) , U) , 
f(y£iw, {3, .X, a, Y; ,i;<;) 
rx !N, (y;ix;{3, h(A;)) ( 1 ( ,::•:
0 
1 
) + 1 ( w:• ~
0 
0 ) ) , 
n 
f( ajw, ,£3, A, y) 
ex 
7ru( aja, b) IT 7r(Ada) 1ae(a,b) 
i=1 
and 
for i = 1, ... , n, where (JYt. = (Yi -
~i,£3)
2 and 
{ 
1 
X E A 
1xeA = 
0 x f/:. A 
(3). 
To sample from a truncated normal distribution, we use the inversion method dis-
cussed in Devroye (1986) with the normal distribution quantiles approximation in 
Derenzo (1977). The last two full conditional distributions will be determined in 
the next subsections for two different binary regression models. 
Remark 2 When working with symmetric and parametric families of link func-
tions, it is convenient to construct link functions with a family of distributions that 
has a smooth (bounded) variance function. The exponential power distribution de-
fined below has this property. 
·Normal Distribution 
The probit regression model is obtained using the normal distribution, N1 (Yi I 
~i,£3, 1), which can be expressed as a scale mixture of normal distributions by setting 
h(Ai)=1 and 7r({1}ja) = 1 as the scale mixture parameter distribution. We know 
that there is no shape parameter in this family of distributions. Therefore, it will not 
provide a "data adaptive" Bayesian analysis. The Gibbs sampler structure outlined 
in (3) uses only the two first conditional densities. 

246 
Haro, Mallick & Smith 
Exponential Power Distribution 
The exponential power binary regression model is obtained using the exponential 
power distribution, E 1(ydxif', 1, a), with probability density function 
2 
aJC; 
( (ca(x- J.L)
2
) a) 
fEP1 (x!J.L, (J' 
, a)= v'2r (1;;) (J' exp -
2(!'2 
, 
and variance function 
var(x) = exp(2(1- a)log4)(J'2 , 
(4) 
where 
_ 2r(fc;) 
Ca -
(..L) exp (2( a- 1) log 4). 
r 
2a 
Scale mixture of normal distributions can be obtained by setting h( >..i) = 1/ Ai 
and 
1r(A;Ia) = r (trca ( ~: r'' fs (~~I a, 1), 
(5) 
where S(xja, 1) denotes the standardized positive stable distribution with charac-
teristic exponent a E (0, 1 ); see West (1987) for the proof. We note that the shape 
parameter a is the device that enables us to obtain a "data adaptive" Bayesian 
analysis. 
1.0 
0.8 
0.6 
~ 1 
0.4 
0.2 
-6 
·4 
-2 
0 
2 
4 
6 
Linear Predictor 
FIGURE 14.1. Set that contains all the prior link functions generated from the exponential 
power distribution with variance given in equation ( 4). This set is compared with the pro bit 
link function and the logistic link function standardized to match the interquartile range 
of the Laplace (exponential power with a= 1/2) link function. 

14. Binary regression 
247 
Figure 14.1 displays the set that contains all prior link functions produced by the 
family of exponential power distributions with variance given in equation ( 4). 
Applying to equation (5) the standardized positive stable distribution represen-
tation given by lbragimov and Chernin (1959), 
ao: 
[
1 
( 
ta(8)) 
fs(.\la, 1) = Aa"'+1 Jo ta(8) exp -
,\a"' 
d8, 
for a, 8 E (0, 1), where 
( ) _ (sin( a 11" 8)) a"' sin((1- a) 11" 8) 
to: 8 -
. ( 
) 
. ( 
) 
sm 11"8 
sm 11"8 
and 
we can see that Ai and 8i in our problem follow a bivariate distribution with prob-
ability density function 
( 
)
a"'+3/2 
( 
(C )a"') 
~: 
exp -to: ( 8i) 
..\: 
fori= 1, .. . ,n, where 
"'o: = r (21o:) (1- a). 
To complete the full conditional distributions for the exponential power binary 
regression model outlined in (3), we transform Ai so that 1/Ji =log Ai and use instead 
the following full conditional density 
for i = 1, ... , n. This full conditional distribution is log-concave, so we can sample 
from it using the adaptive rejection sampling method for log-concave distributions 
given in Gilks and Wild (1992). The value for ..\i is obtained via the inverse trans-
formation ..\i = exp( ¢;). 
The full conditional density of 8i, 
fori= 1, ... , n, has a unique maximum at ta(8i) = max((..\dco:)a"', aa"'(1-a)). This 
can be confirmed by observing that to:(8) is a monotonic function for 8 E (0, 1) with 
lims-o to:(8) = aa"'(1- a) and lims-1 ta(8) = oo; see Haro-L6pez et al. for details. 
The knowledge of this maximum makes the adaptive histogram rejection sampling 
algorithm a suitable candidate to sample from this full conditional distribution (see 
appendix). 
The full conditional distribution of a has the following density 
To sample from this full conditional distribution, we implement the adaptive re-
jection Metropolis sampling algorithm in Gilks et al. (1995), Gilks et al. (1997. 

248 
Haro, Mallick & Smith 
Computation of the maximum of this density is crucial for obtaining good candi-
date points to construct the initial adaptive rejection envelope and this is pursued 
using the bisection root finder method. In practice for a < 0.9, the full conditional 
distribution of a is nearly log-concave. Therefore, it is convenient to use the algo-
rithm in Gilks et al. (1995) because it reduces to the algorithm in Gilks and Wild 
( 1992) when we are sampling from a log-concave distribution. 
3. 
Detection of Outliers and Model Comparison 
Once "within-model" inference is completed we can use statistical analysis for 
the detection of extreme observations. We can find outliers in the binary regression 
model as in Albert and Chib (1995) by using the following 'latent' Bayesian residual 
for the unobserved Yi, 
(Yi- xi[3) 2 
'~'i = 
h(Ai) 
' 
which possesses a continuous-valued posterior distribution. 
In Chaloner and Brant (1988) an outlier is defined as an observation with a 
large random error, generated by the linear regression model under consideration. 
This paper establishes that outliers can be detected by examining the posterior 
distribution of the random errors. 
In the context of scale mixtures of normal distributions, the conditional distri-
bution of the residual ri given {3 and Ai is xt, a chi-square distribution with one 
degree of freedom. For detecting outlying observations, this conditional distribution 
can be used as the basis with which to compare the posterior distribution of ri. 
For scale mixtures of normal distributions the knowledge of the conditional dis-
tribution of Wi given f3 and Ai can be used to calculate the posterior probabilities 
fitted by the model. Then, the Rao-Blackwellized estimate, introduced in Gelfand 
and Smith (1990), of the posterior probability of obtaining the Wi response on the 
ith observation is given by 
(6) 
i = 1, .. , , n, where <P( ·) is the standard normal cumulative function, 
W( -i) = ( W1 ..• , Wi-1, wi+l, ... , Wn) and { A~r)} and {f3(r)}, r = 1, ... , R, are pos-
terior distribution samples. Analysis of these fitted posterior probabilities can be 
used to compare different adjusted models. 
4. 
Numerical Illustration 
To illustrate our methodology, we use the low birth weight in infants dataset 
given in Hosmer and Lemeshow (1989). In this dataset, the binary outcome (wi) is 
whether the infant birth weight is less than 2.5 kg. in births from n = 189 women. 
The covariates in Xi are a constant, the age of the mother, the weight of the mother 
(lbs.) at last menstrual period, race (white, black or other), smoking status during 
pregnancy (0 or 1), previous premature labors (0 or 1), history of hypertension (0 

14. Binary regression 
249 
or 1), presence of uterine irritability (0 or 1) and number of physician visits in the 
first trimester (0, 1 or 2+ ). 
Running four different Gibbs sampler chains and using the following values for 
the hyperparameters, v = (0, ... , O)T and with the choice 
0 
0.001 
0 
we fitted normal and exponential power binary regression models with variance as 
in equation ( 4). The starting value of a in each chain was randomly chosen. 
(a) 
0.7 
3.0 
2.5 
0.6 
I 
2.0 
1.5 
·i' 
.Y 
i 
0.5 ~ 
J 
0 
w 
1.0 
0.5 
0.0 
,. 
i! 
0.4 I 
E 
I 
(f) 
0.3 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
0 
1 00000 200000 300000 400000 500000 
a 
Iteration 
1.0 
1.0 
0.8 
0.8 
0.8 
0.6 ~ 
0.6 
~o~ 
~ 
0.4 
0.4 
0.2 ltV 
0.2 
0.0 
~0.4 
J 
•1\.r--- $0.2 . -····- ..... J 
__ ...[_ ...... ~·······± 
0.0 ··•··•··········••···•··•••·•·•··•····•••·•·•···••·• 
250000 
325000 400000 
475000 
0 
250000 
500000 
0 
1 00 
200 
300 
400 
500 
Iteration 
Iteration 
Lag 
FIGURE 14.2. (a) Posterior density, prior density and ergodic means of several chains of 
the Gibbs sampler for a. (b) Trace (after a burn-in period), ergodic quantiles and sample 
autocorrelation of one chain of the Gibbs sampler for a of the exponential power binary 
regression model. 
Figure 14.2 (a) displays the posterior distribution and the ergodic means obtained 
from the four chains of the Gibbs sampler for a. Figure 14.2(b) shows the trace 
(after a burn-in period), ergodic quantiles and posterior sample autocorrelation of 
one Gibbs sampler chain for a. Observing that convergence is attained around the 
50, oooth and 250, oooth iterations, with low autocorrelation reached at every loth 
and lOOth iterations for the normal and the exponential power models respectively, 
we used this information to evaluate the posterior means and posterior standard 
errors of a and Pwhich are summarized in Table 14.1. 
Detection of extreme observations is carried out by using 'laten't' Bayesian resid-
uals ri. For normal and exponential power binary regression models, and for each 

250 
Haro, Mallick & Smith 
Exponential Power Model 
Normal Model 
Variable 
Post. Mean 
Post. Stand. Error 
Post. Mean 
Post. Stand. Error 
a 
0.4590 
0.1235 
Constant 
1.1265 
1.4489 
0.5096 
0.7237 
Age 
-0.0317 
0.0443 
-0.0244 
0.0228 
Mother Weight 
-0.0200 
0.0089 
-0.0094 
0.0040 
Race Black 
1.2990 
0.5924 
0. 7309 
0.3209 
Race Other 
0.8201 
0.5255 
0.4583 
0.2699 
Smoke 
0.7515 
0.4807 
0.4786 
0.2481 
Premature 
1.5074 
0.5698 
0.8420 
0.2920 
Hypertension 
2.2382 
0.8580 
1.1737 
0.4321 
Irritability 
0.6366 
0.5310 
0.4246 
0.2804 
One Visit 
-0.4978 
0.5547 
-0.2928 
0.2806 
More Visits 
0.2084 
0.5173 
0.0889 
0.2698 
TABLE 14.1. Posterior mean and standard errors of a and f3 for exponential power and 
normal binary regression models. 
of the 189 women, Fig 14.3displays the box plots of the Bayesian residual poste-
rior distributions. These are compared with the 50% and 95% quantiles of the Xt 
conditional distribution of r;,. An extreme observation will have a Bayesian resid-
ual posterior distribution substantially above the 95% quantile of its Chi-square 
conditional distribution. 
Looking at the normal Bayesian residuals ri, it is clear that woman 155 and 
183 are likely to be extreme observations. Comparing both models by the Bayesian 
residuals and applying the definition of an outlying observation in [?], we conclude 
that the exponential power fits better the data than the normal binary (probit) 
regression model. 
Figure 14.3 also shows for exponential power and normal binary regression mo-
dels, the Rao-Blackwellized estimates of the posterior probability of obtaining the 
w;, response on the ith individual. They are calculated using equation (6) with 
posterior distribution sampl~s { A~r)} and {,l3(r)}, r = 1, ... , R = 8000. Comparing 
the posterior probabilities given by each model, we definitely conclude that the 
exponential power model adjusts more satisfactorily to the data than the normal 
model. 
5. 
Discussion 
In Haro-L6pez and Smith (1996) it is noted that the Laplace distribution, included 
in the exponential power family of distributions, has a similar tail behavior to that 
of the logistic distribution. Therefore, it may be attractive to think of using the 

14. Binary regression 
251 
Problt Regression Model 
8~(aL)---------------------------------------------------:--I 
6 
.::4 
2 
6 
.::4 
2 
oL_~~~~~~~~~~~~~~~~~~~~~iM~~ 
70 
80 
90 
100 110 120 130 140 150 
1.0 
0.8 
0.6 
0.4 
0.2 
(b) 
Individual Number 
W1 on the jth Individual 
o.o L~:::::1~0~2g;0~3:::0~4~0=5:f0~62¥o~7§o~8~o=;;;;;..oo.......=1~oo-11o 120 13o 14o 1so 16o 110 18o 189 
Individual Number 
FIGURE 14.3. (a) Box plots of the Bayesian residual posterior distributions for normal and 
exponential power binary regression models. They are compared with the 95% quantiles 
of the xf. (b) Posterior estimate of the probability of obtaining the Wi response under 
normal (shaded area) and exponential power binary regression models for the ith woman. 
exponential power family of link functions' routinely in any Bayesian procedure 
where logit binary regression models might conventionally be used. 
Our methodology permitted the data studied previously to find the necessary 
degree of robustness by choosing a link function in the neighbourhood of the Laplace 
distribution from the exponential power link functions family. Without coercing the 
analysis to a specific model our model was consistent with the logit regression model 
applied in Venables and Ripley (1994). The numerical example pointed out that the 
study of the Bayesian residuals and the analysis of fitted posterior probabilities are 
useful and informative for choosing among different models. 
Acknowledgments 
The first author gratefully acknowledge the financial support from the National 
Council of Science and Technology of Mexico (CONACYT) and the Overseas Re-
search Student Awards Scheme of the Committee of Vice-Chancellors and Principals 
(CVCP), United Kingdom. 

252 
Haro, Mallick & Smith 
Appendix 
Adaptive Histogram Rejection Sampling 
This rejection algorithm, motivated by the work done in[?), is designed to sample 
from the following type of densities. Let f( 0) be a unimodal probability density 
function with kernel h(O) and 0 E (a, b). Let oM be the position where the supremum 
of h( 0) is attained and Sn = { Oi; i = 0, ... , n + 1} denote the current set of abscissae 
in ascending order, where 00 = a and On+1 = b. Define a piecewise linear function, 
gn(O) = { hh((OOi)) 
0
0
i-1 <0 0 <0 
Oi 
~or 00i :5. 00: 
, 
i = 1, ... , n, 
i 
i < < i+1 
10r 
i 2:: 
where we notationally suppress the dependence of 9n( 0) on Sn. If Oj = OM is in Sn 
for some j E ( 1, ... , n), then 9n ( 0) is an adaptive envelope for h( 0). 
To sample from f(O), initialize abscissae Sn with 01 =OM and carry on with the 
following steps using the sampling probability density function g( 0) oc 9n( 0) until a 
point 0 is accepted. 
• Sample 0 from g(O) and u f'V U(u!O, 1). 
• Evaluate h(O) and g(O), and perform the following rejection test: if 
h(O) 
u :5. g( 0) ' 
then accept 0. Otherwise reject 0 and set Sn+1 = Sn U { 0} with relabeled 
points in ascending order. Increment n and update g(O). 
By continually adding rejected points to Sn, the adaptive rejection envelope g( 0) 
will gradually converge to the true probability density f( 0) and efficiency of sam-
pling will increase. 
References 
Albert, J .H. and Chib, S. (1993). Bayesian analysis for binary and polychotomous 
response data. Journal of the American Statistical Association, 88, 669-679. 
Albert, J.H. and Chib, S. (1995). Bayesian residual analysis for binary response 
regression models. Biometrika, 82:746-759. 
Andrews, D.F. and Mallows, C.L. (1974). Scale mixtures of normal distributions. 
Journal of the Royal Statistical Society Ser. B, 36, 99-102. 
Buckle, D.J. (1995). Bayesian inference for stable distributions. Journal of the 
American Statistical Association, 90, 605-613, 1995. 
Chaloner, K. and Brant, R. (1988). A Bayesian approach to outlier detection and 
residual analysis. Biometrika, 75, 651-659. 
Derenzo, S.E. (1977) Approximations for hand calculators using small integer co-
efficients. Mathematics of Computation, 31, 214-222. 

14. Binary regression 
253 
Devroye, L. (1986). Non- Uniform Random Variate Generation. Springer-Verlag, 
New York, 1986. 
Dellaportas, P. and Smith, A.F.M (1993). Bayesian inference for generalized linear 
and proportional hazards models via Gibbs sampling. Applied Statistics, 42, 
443-459. 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling-based approaches to calculat-
ing marginal densities. Journal of the American Statistical Association, 85, 
398-409. 
Gilks, W.R., Best, N.G. and Tan, K.K.C. (1995). Adaptive rejection Metropolis 
sampling within Gibbs sampling. Applied Statistics, 44, 455-472. 
Gilks, W.R., Neal, R.M., Best, N.G. and Tan, K.K.C. (1997). Corrigendum: Adap-
tive rejection Metropolis sampling. Applied Statistics, 46, 541-542. 
Gilks, W.R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Applied Statistics, 41, 337-348. 
Haro-L6pez, R.A., Mallick, B.K. and Smith, A.F.M (1998). Data adaptive Bayesian 
analysis using scale mixtures of normal distributions. Technical Report. Im-
perial College, University of London. 
Haro-L6pez, R.A. and Smith, A.F.M (1996). On robust Bayesian analysis for the 
location and scale parameters. Under revision for the Journal of Multivariate 
Analysis. 
Hosmer, D.W. and Lemeshow, S. (1989). Applied Logistic Regression. Wiley, New 
York. 
Ibragimov, LA. and Chernin, K.E. (1959). On the unimodality of stable laws. 
Theory of Probability and its Applications, 4, 417-419. 
McCullagh, P. and Neider, J.A. (1989). Generalized Linear Models. Chapman and 
Hall, New York. 
Newton, M.A., Czado, C. and Chappell, R. (1996). Bayesian inference for semi-
parametric binary regression. Journal of the American Statistical Association, 
91, 142-153. 
Smith, A.F.M. and Roberts, G.O. (1993). Bayesian computation via the Gibbs 
sampler and related Markov chain Monte Carlo methods. Journal of the Royal 
Statistical Society Ser. B, 55, 3-23. 
Venables, W.N. and Ripley, B.D. (1994). Modern Applied Statistics with S-Plus. 
Springer-Verlag, New York. 
West, M. (1987). On scale mixtures of normal distributions. Biometrika, 74, 646-
648. 

This Page Intentionally Left Blank 

15 
A Mixture-Model Approach to the 
Analysis of Survival Data 
Lynn Kuo 
Fengchun Peng 
ABSTRACT We study a mixture model for survival data where covariates may 
influence both the incidence probabilities and their conditional latency distributions. 
The data may include the exactly observed, the right-censored, and the interval-
censored failure times. We apply the EM algorithm to find the maximum likelihood 
estimate. We also carry out a Markov chain Monte Carlo algorithm for Bayesian 
inference. Model selection methods based on the predictive density for cross-validated 
data are developed. These methods allow us to assess whether simpler models would 
suffice as opposed to the mixture models. The potential of the methods is illustrated 
with the flour beetle data given by Hewlett (1974). 
1. 
Introduction 
In some analyses of failure-time data, it has been observed that a group of sub-
jects may not react to treatments. A mixture model that incorporates different 
latency distributions for different groups seems to be appropriate. In this chapter 
we consider a mixture model where covariates may influence both the incidence 
probabilities and the conditional latency distributions. Let x denote the covariate 
vector (1 x q) associated with a subject of lifetime T. Then the mixture model 
assumes that the density ofT is 
J 
.• 
f(tlx, 8) = LPi(x, p)fj(tlx, cPj), 
(1) 
j=l 
where 'L,f=l Pi(x, p) = 1. Let Y be an index variable for the subpopulations. We 
use Pi (x, p) to denote the mixing probability, P(Y = ilx, p), also called the inci-
dence probability for the jth subpopulation. We use fi(tlx, cPj) to denote f;(tiY = 
j, x, cPj ), the conditional probability density (conditional latency density) function 
of the failure time for the lh subpopulation. We assume fi is a continuous density 
and indexed by an unknown parameter cPj that can be a vector. Moreover, we use 
8 = ( cjJ1 , ... , cjJ J, p) to denote the collection of all unknown parameters. The covari-
ate x may include 1, the dosage (or log-dose) level, and other explanatory variables. 
In this study we only deal with time-independent explanatory variables. Logistic 
regression links can be chosen for the incidence probabilities. For example, we can 
255 

256 
K uo & Peng 
choose 
(2) 
where ej = xpJ, with PI to be the transpose of Pj (1 X q) and p = (Pl, ... , PJ ). 
In addition to the logistic link, other models such as the normalized probit link or 
the normalized complementary log-log link (McCullagh and Neider, 1989) can be 
considered for the incidence probabilities. 
The cumulative distribution function for T is 
J 
F(tlx, 8) = LPi(x, p)Fj(tjx, cPj), 
(3) 
j=l 
where Fj is the cumulative distribution function for /j. 
This mixture model has been studied by Farewell (1982), Larson and Dinse (1985), 
Pack and Morgan (1990), Boos and Brownie (1991), Kuk and Chen (1992), and 
Taylor (1995). The paper of Boos and Brownie considers the failure times to be 
independent and identically distributed (i.i.d.) as in (1). The rest of the papers 
include censored failure times. Pack and Morgan consider interval-censored and 
right-censored time-to-response data for quantal assay. Larson and Dinse and Kuk 
and Chen consider proportional hazards models for the conditional latency distribu-
tions. Taylor considers Kaplan-Meier type formulations for the conditional latency 
distributions. 
In this chapter, we consider data that include both right-censored and interval-
censored observations. Our methodologies can be applied to left-censored observa-
tions as well. We provide two methodologies for inference. One is an EM algorithm or 
a Monte Carlo EM algorithm (MCEM) to obtain the maximum likelihood estimate 
of 8. The other is a Markov chain Monte Carlo (MCMC) algorithm for Bayesian 
inference. 
To apply the EM algorithm, we need to consider a complete likelihood of a product 
of components for i.i.d. observations as opposed to the mixture likelihood with 
censored observations. This is done by augmenting the original data with two classes 
of latent variables. One is the truncated random variable denoted by w that allows 
us to consider a likelihood for i.i.d. observations without censoring. The other is 
the index variable z that converts the mixture model to a model of independent 
components. Although the E-step with respect to the latent variables w is often 
difficult to evaluate, this is not the case for z when it is conditioned on w. We employ 
a MCEM algorithm (Wei and Tanner, 1990) to approximate the E-step, where 
integration with respect to w is done by Monte Carlo integration and integration 
with respect to z given w is evaluated exactly. Then we apply a maximization step 
to update the parameters until convergence. 
The MCMC algorithm consists of generating the two latent variables w, z and the 
unknown parameter (} iteratively. Let t denote the data set that contains censored 
observations. Starting at the initial choices of the variates, w<0), z(O), and 8(0), 
sample w(l) from f(wlz( 0),8(o),t); sample z(l) from f(zlw(1),8(o),t); and sample 
8(1) from f(8lw(l),z(l),t). Continue iteration until convergence. The f function 
here is a generic name for the conditional densities. 
Both the MCEM and the MCMC algorithms use data augmentation with latent 
variables to simplify the computation. The techniques used in this paper are related 
to that in many other papers. We only mention a few here. Dempster, Laird, and 
Rubin (1977) introduced the EM algorithm for analysis with aggregated data or 

15. Analysis of survival data 
257 
with missing data. McLachlan and Jones (1988) developed the EM algorithm for 
grouped and truncated data. Wei and Tanner (1990) proposed MCEM when the 
E-step is difficult to derive. Jordan and Jacobs (1994) showed how the parameters of 
the mixtures-of-experts (as well as the hierarchical mixtures-of-experts) architecture 
can be estimated using the EM algorithm. The MCMC algorithm was introduced 
by Geman and Geman (1984) for image restoration and was generalized by Tanner 
and Wong (1987) for missing value models. Gelfand and Smith (1990) applied it to 
Bayesian analysis. Diebolt and Robert (1994) developed a Gibbs sampler for i.i.d. 
observations for the usual mixture model, that is a simpler version of (1) without 
the covariates. Dey, Kuo, and Sahu (1995) studied a predictive approach to selecting 
the number of components in the mixture model. We are extending the two papers 
to censored data. The basic principle in the extension can be seen in Gelfand, Smith, 
and Lee (1992) and Kuo and Smith (1992). Peng, Jacobs, and Tanner (1996) present 
a hierarchical mixtures-of-experts architecture using both EM and a Gibbs sampler 
in a classification problem. 
The rest of the paper is organized as follows. Section 2 describes the likelihood. 
Section 3 describes the EM and MCEM algorithms. Section 4 describes the MCMC 
algorithm. Model selection based on the predictive approach is discussed in Section 
5. An illustration using the flour beetle data given by Hewlett (1974) is given in 
Section 6. 
2. 
Likelihood 
We assume that failure times are independently distributed, also independent 
from the censoring process. For each individual we observe the time to failure, a 
right-censored, or an interval-censored failure time. That is, for the right-censored 
individual we only know that the time to failure is greater than the censored time, 
while for the interval-censored individual, the time to failure is only known to be 
between two time points. 
The likelihood is then (Cox and Oakes, 1984) 
iEU 
iEC 
X II {F(tiUIXi, 8)- F(tiL,Xi, 8)}, 
iEZ 
I 
(1) 
where t denotes the data set with the ith entry denoted by ti if the ith subject 
is exactly observed, by tt if it is right-censored at ti, and by (tiL, tiu) if the ith 
subject is interval-censored between tiL and tiu; the sets U, C and I are the sets 
of indices for the exactly observed, the right-censored, and the interval-censored 
subjects, respectively. We assume there are N subjects in the study. Therefore, we 
use X to denote (xf, ... , x'j;, ), the matrix of all covariates. 
3. 
EM and Monte Carlo EM 
In this section, we develop a Monte Carlo EM algorithm to maximize 8 in the 
likelihood. First observe that part of the likelihood in (1) for the censored data 
is a product of integrals of the mixture density in (1). This incomplete expression 

258 
K uo & Peng 
makes maximization difficult even for simple latency distributions such as normal, 
gamma, etc. For each subject i, i = 1, ... , N, we consider the latent variable Wi 
that is truncated between (tiL, tiu) if it is interval-censored, truncated at tt if it 
is right-censored, and is ti if it is exactly observed. Then the joint density of the 
data and the latent variables w's has i.i.d. components, each distributed as (1). 
This will alleviate the difficulties in maximization with incomplete likelihood as 
discussed earlier. More specifically, if the ith subject is interval-censored between 
(tiL, tiu), then we can generate a latent variable Wi from the truncated density 
{f(wi)/[F(tiu)- F(tiL)]}I(tiL < Wi < tiu ), where f is given in (1). This can be 
done by setting 
where U "'uniform(O, 1) and p-l is the inverse function of F. Similarly, if the ith 
subject is right-censored at tt, then we can generate a latent variable Wi from the 
truncated density {!( wi)/[1- F(tt)]}I(tt < Wi < oo) by setting 
(2) 
More details about Equations (1) and (2) are given by Devroye (1986, pp. 38-39). 
Note from (1) that Wi can be generated as the unique root of F(wi) = F(tiLixi, 8)+ 
U[F(tiulxi, 8)- F(tiLIXi, 8)] and similarly for (2). They can be easily implemented 
using the IMSL subroutine "ZREAL" in FORTRAN. If the ith subject is exactly 
observed at ti, then we set Wi = ti. 
For each i, we generate Wi independently as discussed above. Let w denote 
(w1, ... , WN)· Then the likelihood of 8 given w, t, X is 
N 
N 
( 
J 
) 
L(lllw, t, X)= g 
f(w; I x;, II)= g ];Pi(x;, p)/j(w; I x;, t/>i) 
. 
(3) 
Next, we make use of the fact that a mixture model can always be expressed by a 
product of its components. It is done by considering another latent variable Zi = 
(Zit, ..• , ZiJ ), where Zij = 1 if Wi is considered as coming from the subpopulation 
having pdf /j and Ef=l Zij = 1. The vector Zi follows a multinomial distribution, 
i.e., 
zil8, w, t, X"' Mult(l, hil, ... , hiJ ), 
with hij, the abbreviation for hij(8lwi, t, X), gi~en by 
h··(nj . t X)_ Pj(Xi,P)/j(wilxi,cPj) 
$J 
t7 w,, , 
- "' 
. 
Lti Pi (xi, P)/i ( Wi I xi, cPj) 
(4) 
(5) 
Let z = (z1, ... , ZN ). Then the augmented likelihood for the completely imputed 
data z and w is 
N 
J 
L(8lz,w,t,X) = IIII {Pi(xi,P)f;(wilxi,c/Jj)Vij. 
(6) 
i=lj=l 
The complexity of fitting the mixture model is greatly simplified by working with 
the augmented likelihood because we only need to evaluate Pj(Xi, p) X/j(wilxi, cPj) 
when Zij = 1. 

15. Analysis of survival data 
259 
Suppose the current EM iteration is at the kth stage. The E-step in the E-M 
algorithm consists of evaluating 
Q(8, 9(k)) = j log(L(8jz, w, t, X)]f(zjw, 8(k), t, X)f(wl8(k), t, X)dzdw 
N 
J 
= L L E{hij(8(k)lwi, t, X) log(pj(Xi, p)Ji (wdxi, cPj )]} 
(7) 
i=l j=l 
where the expectation on the right-hand side of the second equality is with respect 
to the truncated random variable w. The M-step requires maximizing the above ex-
pression as a function of 8. The maximization for t:P1 and p can be done separately. 
Eor many standard probability distributions, statistical packages are available to 
handle the maximization. We continue this iteration using (7) until the two consec-
utive 8's are close enough. Mixture models are usually multimodal; hence the EM 
solutions are not unique and depend on the starting values. Therefore exploration 
with different starting points is needed to obtain the global maximum. 
Now we extend the EM to the MCEM algorithm. When the expectation is hard 
to evaluate with respect to the truncated random variables w, we circumvent it by 
multiple imputation by generating M copies of w, where the mth copy of w denoted 
by wm equals ( wr, ... , w!J). Then for each wm, we generate zm = ( zr, ... , z!J), 
where zr is generated by (4) with hij(8(k)lwf,t,X) defined in (5), where Wi is 
replaced by wf and 8 is replaced by 9(k). Therefore the E-step in (7) can be 
approximated by 
1 
M 
N 
J 
Q(8, 9(k)) = M L L L hi,j(8(k)lwi, t, X){log(pj(Xi, p)fJ(wflxi, cP1 )]}. 
m=l i=l j=l 
4. 
Gibbs Sampler 
Our next objective is a general Bayesian approach to mixture models for the 
survival data with censoring. Our sampling-based approach enables us to con-
struct credible regions for parameters or functionals of interest for any sample size. 
Moreover, by plotting the entire posterior distribution of 8, we can detect interest-
ing features of the posterior distribution, such as skewness, heavy tailedness, etc. 
Let 1r( 8) denote the prior density for 8; the posterior density is proportional to 
L(8lt, X) x 1r(8). We first consider the data augmentation idea of the latent vari-
ables z discussed earlier, so updating on the 8 can be made simple in the MCMC 
by means of considering a product of the components. Now we extend these tech-
niques to handle censored data. In addition to z, we augment the data with another 
class of latent variables w that treats the unobserved failure times due to censor-
ing as missing. This enables us to consider the augmented likelihood as given in 
(6). Then simulating 8 given w, z, t, and X can be performed either from standard 
distributions or by using the Metropolis algorithm (Metropolis et al., 1953). 
Now let us briefly discuss the Gibbs sampler. Start with some initial values of 
8(0), z(0), w<0). One of the values could be the EM or the MCEM solution of the 
mixture model. We can also perturb it with small errors for the starting points 
for replications of the Markov chain. We iteratively generate w, z, and 8 from the 
following full conditional distributions: 
Step 1: Given 8(o), t, X, we generate wP) by (1) if the ith subject is interval cen-
sored; generate wP) by (2), if it is right censored; set it to ti, if it is exactly observed. 

260 
K uo & Peng 
We do this independently for i = 1, ... , N. 
Step 2: Given w< 1), 8(0),t,X, we generate zP) ,...., Mult(1,hil, ... ,hiJ), where 
hij = hij(8(o)lwP), t,X) as given in (5). This is done for all i = 1, ... , N. 
Step 3: Given z(l), w(l), t, X, we generate 8(l) from p( 8lw(l), z(l), t, X) <X 1r(8)L( 81 
z(1), w(l), t, X),,where L(8lz, w, t, X) is given in (6). 
Then we continue to iterate by repeating steps 1-3 where the superscripts in the 
stage indicators are incremented. Convergence can be monitored by the Gelman 
and Rubin (1992) method. 
5. 
Model Selection 
Given the mixture model in (1), we need to know what is the best number of mix-
tures to fit the data. We also have many choices for the mixing probabilities that 
include the logistic link and the probit link, and many choices for the latency dis-
tributions that include normal, log-normal, exponential, and Weibull distributions. 
We explore a predictive approach for model selection. 
The basic idea is simple. We use part of the data to fit the model and the re-
maining part to test it. We divide our data into G groups of roughly equal size. For 
the data in the gth group, we evaluate the predictability of a model by evaluating 
the predictive likelihood of the gth group relative to the posterior distribution of 
8 ·given the rest of the data. Let t 9 denote the data set for the g~h group that 
may include censored data. Similarly, we use X 9 to denote the covariates for the 
gth group. Then the remaining data are denoted by t_ 9 and X-u· The conditional 
predictive ordinate for the gth group (GCPO) is defined by 
p(t9lt- 9,X-9 ) = f p(t9 I8,X9 )7r(8!t-9,X_9 )d8 
= f L(8lt9 , X 9)7r(81t-9 , X_ 9)d8. 
(1) 
This _GCPO assesses the predictability of our model for the new data t 9 • We do 
this for all g, g = 1, ... , G. The pseudo-marginal predictive likelihood for the whole 
data is defined to be the product of these GCPO 's, 
G 
PGCPO(t,X) = IT p(tult-o, X-u)· 
(2) 
o=l 
The predictive pseudo-likelihood depends on the model. We use PGCPO(t,X) (M) 
to denote the predictive pseudo-likelihood for the model M. Therefore, we select 
the best model that has the largest POCPO(t,x)(M) among the class of models 
{ME M}. 
Usually, if the data size is large, a G of two may be sufficient. ( cf. p. 240 of Efron 
and Tibshirani, 1993, for a related problem on estimating the prediction error). If 
the data size is small, we may need more groups. If G is N, then the method is 
just leave-one-out cross-validation. Then (2) reduces to the usual pseudo-marginal 
likelihood as defined in Gelfand, Dey, and Chang (1992). 
On the computation for (2), if G is small, we might use a brute-force Monte 
Carlo integration. For the gth group, we apply the Gibbs sampler as described 
in Section 4 using the new data set with the gth group deleted. Then we apply 

15. Analysis of survival data 
261 
Monte Carlo integration to (1) by averaging L(81t9 , X 0 ) over the 8 sampled from 
the Gibbs sampler applied to the new data set (t_ 0 ,X_0 ). If G = N, it would be 
too computing intensive to repeat the Gibbs sampler N times, every time using a 
different data set. Then a harmonic mean estimator of the CPO (GCPO in (1 with 
G = N)) can be obtained from the Gibbs sample based on the full data. Let }(; 
denote an index set that counts the usable variates e(k) generated by the Gibbs 
sampler applied to the full data as in Section 4. These variates can be a second half 
of a single chain, or a second half of all multiple chains. Let I< denote the size of 
JC. Then 
{ 
I<{2:~ceJCf(tdxi,8(k))J-
1 }-
1 fori EU; 
p(tilt-i,X-i) = 
I<{2:~ceJC[1- F(ttlxi,e(k))J- 1}- 1 for i E C; 
I<{I:~ceJC[F(tiulxi,e(k))- F(tiLixi,e(k))]- 1}- 1 fori E I. 
6. 
Example 
As an illustration, consider the data set in Table 15.1 given in Hewlett (1974). 
It was also used by Diggle and Gratton (1984), Pack and Morgan (1990), and 
several other authors. The data set consists of 317 male adult flour beetles that 
were exposed separately to pyrethrum, a plant-based insecticide. Among the 317 
males, 144,69, 54, and 50 males were sprayed of pyrethrum at concentrations of0.20, 
0.32, 0.50, 0.80 mg/cm2 , respectively. The equivalent log concentrations (denoted 
by log-doses) are -1.61, -1.14, -0.69, and -0.22. We will also analyze a similar data 
set for the females. That data set is omitted here, but it can be found in Hewlett 
(1974) and Pack and Morgan (1990, p.750). The group sizes for the female data set 
corresponding to the four dosages are 152, 81, 44, and 47. 
Let Xc = log(dc), where de is the dosage given to the beetles in the cth column. 
The survival time of a beetle given dosage Xc is modeled by a mixture of normal 
and exponential densities: 
where 
fl(tlxc,o:,~,u) 
h(tl.-\) 
P2 ( x c , 'Y, r) = 
+ 
_1_ 
{. (t -a- ~xc)
2 } • 
. /iC exp -
2 2 
' 
v ~7r(J' 
0' 
A exp{ -.-\t}I{t > 0}; and 
1 - P1 ( x c, 'Y, r). 
This model suggests a normal distribution for the failure times in the susceptible 
group with the mean to be a linear function of the log dose as in Boos and Brownie 
(1991). Because the other group is assumed to be more resistant to treatment, we 
choose an exponential distribution independent of the dosages. Other selections 
of standard distributions that might fit the data as well will be explored later. 
Because the mixing probabilities are directly related to the dosages, we consider 
logistic regression links for the incidence probabilities. Pack and Morgan (1990) 
ignore the contribution from the subpopulation of the long-term survivors for t ~ 
13 in their numerical examples. Our model includes contributions from both the 
susceptible and the long-term survivor groups over the entire range of the survival 

262 
K uo & Peng 
times. These contributions are weighted by the mixing probabilities that correspond 
to the proportions of the subgroups. These two aspects are the advantages of using 
a mixture model. 
Table 15.1: Male Flour Beetle ( Tribolium castaneum) Data. The numbers 
indicate the number dead per day, the row 14 gives the number survived after day 
13. 
rfc 
Log-dose(mg/cm2) 
-1.61 
-1.14 
-0.69 
-0.22 
2 
11 
10 
8 
10 
3 
10 
11 
11 
8 
4 
7 
16 
15 
14 
5 
4 
3 
4 
8 
6 
3 
2 
2 
2 
7 
2 
1 
1 
1 
8 
1 
0 
1 
0 
9 
0 
0 
0 
0 
10 
0 
0 
0 
1 
11 
0 
0 
0 
0 
12 
1 
0 
0 
0 
13 
1 
0 
0 
0 
14 
101 
19 
7 
2 
Total 
144 
69 
54 
50 
6.1 
EM Algorithm for the Specific Example 
Now we derive the EM algorithm for this example. Let nrc denote the cell count 
in the rth row and the cth column, where c = 1, ... , 4 is the index for the dose 
levels and r = 1, ... , 14 is the index on the days. For simplicity, we write I:rc for 
2:!=1 2:;!1 . We treat the data in the rth row as interval censored for r ~ 13, that 
is, beetles in the rth row die between the r- 1 th and rth days; and data in the 14th 
row as right-censored with survival until at least the 13th day. 
For this example, we are able to evaluate the E-step in closed form. There-
fore, no MCEM is needed. A related EM algorithm can be found in McLach-
lan and Jones (1988). Suppose the current EM iteration is at the kth stage. Let 
e(k) -
(o:(k) f3(k) (u(k)) 2 A(k) ""(k) r(k)) -
(--N(k) A,.(k) p(k)) where A. denotes 
-
' 
' 
' 
' I 
' 
-
"+'1 
l "f'2 
' 
1 
' 
"f'1 
(a, /3, u2), 4J2 denotes A, and p 1 denotes ( '' r). Note we set p 2 = (0, 0). For the 
E-step, we need to evaluate the expectation in (7) with respect to the truncated 
random variable w. Because of the grouped data, the summation over i in (7) can 
be written as I:rc with w;. replaced by Wrc, where Wrc is distributed as a truncated 
version of (3) with the current value of(J(k). We use (arc,brc) to denote the range 
of Wrc· We write p?) for PJk\xc,P) for short. For j = 1 or 2, let 
(k)f(k)(w. I 
A,.(k)) 
h . ( w. . e< k)) -
p j 
j 
r c lJ c, "f' j 
3 
rc, 
-
I:j PJk) fJk)(WrciXc, 4JJk)). 
Therefore, for the E-step, we need to evaluate the following expectations: 
A(k) 
_ E(k)[h ·(W. . 8(k))ws] 
s,jrc-
J 
rc, 
rc ' 

15. Analysis of survival data 
263 
for s = 0, 1 and 
A(k) 
-
E(k)[hl(W, . (J(k))(W. 
-
H(k+l))2] 
2,lrc -
rc, 
rc 
rl 
· 
For simplicity, we write ILb instead of ILl(xc), for a+ f3xc. It is straightforward to 
verify the following equalities for c = 1, ... , 4 and r = 1, ... , 14, where h4c = oo, 
for all c: 
where 
G(k) 
O,jrc 
G(k) 
l,lrc 
G(k) 
1,2rc 
G(k) 
= 
2,lrc 
and where 
(k)G(k) 
A(k) 
_ 
Pj 
s,jrc 
( 
) 
s,jrc- F(k)(brc)- F(k)(arc)' 
S = 0, 1, 2 ' 
Ha~}rc' 
n(k) H(k) 
_ (u(k))2 H(k) 
r'l 
O,lrc 
l,lrc' 
exp{ -A(k)arc}(arc + 1/ A(k))- exp{ -.:\(k)brc}(brc + 1/ A(k)), 
( (k))2[H(k) + (2 (k+l) _ 
(k))H(k) 
_ H(k) ] + ( (k+l) _ 
(k))2H(k) 
0" 
O,lrc 
IL1 
IL1 
l,lrc 
2,lrc 
ILl 
Jl1 
O,lrc' 
Ha~}rc 
Fjk)(brc)- Fjk)(arc), 
Hf~)rc 
JY)(brc)- JY)(arc), 
H (k) 
b f(k)( 
) 
/(k)( 
) 
2,lrc 
rc 1 
brc -
arc 1 
arc · 
Therefore, we obtain the following equations for the M-step: 
" 
A(k) 
f3(k+l)" 
A(k) 
(k+l) _ L..irc nrc l,lrc-
L..irc nrc O,lrcXc 
Q' 
-
(k) 
' 
Ere nrcAo,lrc 
" 
(k) 
( (k+l))2 _ L..irc nrcA2,lrc 
(T 
-
(k) 
' 
Ere nrcAo,lrc 
" 
A(k) 
;\(k+l) _ L..irc nrc 0,2rc 
-
(k) 
' 
Ere nrcA1,2rc 
and the M-step for 1 and T is computed using the Newton-Raphson method (Boos 
and Brownie, 1991, and Jordan and Xu, 1993). Let 
pf(k+l) = pf(k) + g(R(k)) -1 e(k)' 
where the gradient vector at the kth iteration is 
e(k) = L nrc ( E(k)[hl(Wrci (J(k))]- Plk)) 
a;~k)' 
rc 
Opl 
where 6 = xpf and ~ 
= { 1, Xc}T. The Hessian matrix at the kth iteration is 
apl 
R(k) _""'"" 
(k)(1 -
(k))~..5.:__ 
- ~ 
nrcP1 
P1 
r(k) 0 (k), 
rc 
OP1 
P1 
where g is a learning rate. 

264 
K uo & Peng 
6.2 Gibbs Samplers for the Specific Example 
In the prior specification, we assume the components of 8 are independent. Flat 
priors are chosen for a, /3, .:\, and u2 because we lack precise knowledge about these 
parameters. Although the priors are improper, they result in proper posteriors in 
most situations. We use normal priors for 1 and r, both with mean zero and a large 
known variance ufi, that is, 1,...., N(O, ufi) and T,...., N(O, ufi). Proper priors are chosen 
here due to the concern about the convergence of the posterior sample if flat priors 
were used for the parameters in the mixing probability (Diebold and Robert, 1994, 
p. 367). 
We choose the covariate vector x = ( 1, x) where x = log( dose level). As in the EM 
method, we treat each entry of Table 15.1 as interval censored except that the row 
labeled 14 is treated as right-censored. Therefore, reading from the first column, 
the data set t1 to t144 consists of {(0,1), ... ,(0,1),(1,2), ... ,(1,2), ... ,(12,13), 
3 copies 
11 copies 
13+, ... , 13+} with the same covariate Xi = (1, Xi) = (1, log .2) = (1, -1.61) for 
.__,___. 
101 copies 
i = 1, ... , 144. Similarly, we can list the rest of the data for the remaining dosages. 
On implementing the Gibbs sampler, we follow steps 1-3 in Section 4. We first 
follow steps 1-2 to generate latent variable for Wi and Zi = (zib Zi2) for each i. 
Then we follow step 3 to update the parameters. Now we give more details for 
step 3, where the joint conditional density of 8 can be implemented sequentially 
by updating one variable at a time. Because given w and z (we omit "given t, x" 
for simpler notation), the parameters p1 = ( 1, T), 
</11 = (a, /3, u2), and </12 = .:\ 
are independent, we have some simplifications in generating the inner loop. That is, 
we generate a(l) given f3(0) u<0) w<0) z(o). generate f3( 1) given a<l) u(0) w<0) z(O). 
' 
' 
' 
' 
' 
' 
' 
' 
generate u2(1) given a(l), /3(1), w<0), z(0); generate .:\(1) given w<0) and z(O); and 
independently generate 1'(1) and r(l) given w<0) and z(o). Then continue back to 
step 1 in Section 4 of the iteration. Let z+1 = '2.:~ 1 Zil. Now we just list the 
conditional densities used in the inner loop of step 3. 
I 
N (
'2.:~ 1 zil(wi- f3xi) 
~). 
a···"' 
' 
' 
Z+1 
Z+1 
(4) 
(5) 
(6) 
where IG denotes the inverse gamma distribution with mean C /(z+1 - 2), where 
C = '2.:~ 1 Zi1(wi- a- f3xi)2; 
.:\I ... ,...., r (N- z+1 + 1, 
N 
1 
) , 
l:i:1 Wi(1- Zi1) 
(7) 
where r denotes the gamma distribution with mean (N -z+1 +1)/(2.:~ 1 wi(1-zH)); 
and 
N 
( I ) II (exp{ IZil + TXiZil}) 
{-12 + r
2
} 
g 1, T • . . ex: 
exp 
. 
i=l 
1+exp{1'+Txi} 
2u02 
(8) 

15. Analysis of survival data 
265 
The variates 1 and r are generated by the Metropolis algorithm with (8) as the 
target distribution. 
In addition to the normal distribution for the latency distribution of the subgroup 
1, we also consider the log-normal distribution. Moreover, we consider probit links 
as opposed to logistic links for the mixing probabilities. We would like to answer 
the question: "Do we really need the mixture model? Would a simpler model with 
only one population suffice?" Therefore, we also fit the data with just the normal or 
the log-normal model. These models are the usual linear or log-linear models with 
Gaussian errors. We summarize the six models in Table 15.3. 
On implementing for Model 2, we only need to change the original data to the 
log scale, and change the step in (7) to 
For Models 3 and 4, we assume Pl(x, p) = cp(1' + rx), and P2(x, p) = 1- pl(x, p), 
where cp is the standard cumulative normal distribution function. We follow the 
same procedures as before, except (8) is replaced by 
Then 1 and r can either be generated by the Metropolis algorithm or by using 
another data augmentation with two classes of latent normal variates; one is right-
truncated at 1+rxi, the other is left-truncated at the same point. For Models 5 and 
6, we follow steps 1 and 3 of the Gibbs sampler in Section 4, where step 1 depends 
on a single model. Step 2 is not needed because we have a single linear or log-linear 
model. Then step 3 follows the usual Bayesian linear model updating. 
6. 3 Numerical Results 
Most of our results are based on the male data given in Table 15.1. Table 15.2 
lists the point estimates of the parameters computed from the EM algorithm and 
the MCMC algorithm for the model1 given by (3). The 90% and 95% highest pos-
terior density (HPD) intervals are computed from the Gibbs sampler. The Gibbs 
sampler is iterated 10,000 times and the results qf the last 5,000 iterations are kept 
as a final sample from the joint posterior distribution. The incidence probabili-
ties Pl(Xc, 1, r) = exp{1' + TXc}/(1 + exp{1' + TXc}) for the susceptible group are 
estimated by the Gibbs sampler at .266, .623, .872, .967 for the four dosages. The 
results are quite comparable to ( .299, .724, .870, .960)=( 43/144,50/69,47/54,48/50), 
the crude estimates of the proportion of beetles that react to the treatments (killed 
before day 13). Observe the 90% HPD interval of f3 includes zero; that is, there are 
no appreciable dose effects on survival times for the susceptible group. This finding 
is consistent with previous studies by Hewlett (1974) and Pack and Morgan (1990). 
However, dosage does affect mixing probabilities upon observing the point and the 
interval estimates of r. Moreover, the mean survival times for the male beetles for 
each of the four dosages p,(xc) = Pl(xc, 1, r)(a + f3xc) + P2(xc, 1, r)/ .:\ are estimated 
at 125.8, 66.0, 24.2, and 8.5 from the Gibbs sampler. 

266 
Kuo & Peng 
o.o 
0.2 
0.4 
0.6 
Mixing Probability 
0.6 
1.0 
FIGURE 15.1. Plot of the posterior densities of the incidence probability p1 (xc, ')', r) for 
the subpopulation 1 evaluated at the four dosages. The plot is the overlay of the four 
histograms for the incidence probabilities for the susceptible group each evaluated at the 
log-dose level -1.61, -1.14, -0.69 and -0.22 mgfcm2 , respectively. 
Table 1S .. 2: Parameter Estimates for the Male Data. 
Parameter 
MLE 
Post. Mean 
90% HPD Int. 
95% HPD Int. 
(EM) 
(MCMC) 
0: 
2.937 
2.982 
(2.350, 3.425) 
(2.002, 3. 729) 
(3 
0.254 
0.198 
( -0.462, 0.652) 
(-0.673, 0.721) 
0"2 
2.217 
2.253 
(1.559, 2.924) 
(1.148, 3.256) 
.\ 
0.006 
0.006 
(0.004, 0.008) 
(0.003, 0.009) 
I 
4.213 
4.197 
(3.146, 5.318) 
(2.986, 5. 729) 
T 
3.241 
3.238 
(2.466, 4.051) 
(2.237, 4.124) 
Figure 1 overlays the four histograms of the posterior incidence probabilities 
Pl(Xc, /, r) evaluated at the four log-dosages. Clearly it is monotonically increasing 
in the doses. The mixture model gives more weight to the normal distribution 
than to the exponential distribution for higher dose levels. This is consistent with 
fewer long-term survivors being observed at higher dosages. Figure 2 overlays the 
four posterior density plots for p(xc) for the four dosages respectively. The four 
densities reading from left to right correspond to the mean survival times for the 
beetles in the fourth column to the first column. The figure shows that all densities 
are unimodal. Their modes are quite comparable to the means given above. The 
mean decreases as the dosage increases. 
We also analyzed the data set for the female beetles. The mean survival times for 
the females were estimated to be 163.3, 124.7, 74.4, and 34.0 for the four dosages. 
Apparently, there are big differences in the means between the sexes. The strong sex 
effect is also observed by Hewlett (1974), Diggle and Gratton (1984), and Pack and 
Morgan (1990). It is also interesting to note /3 is -.318, with (-.734, .099) as the 90% 
HPD interval. This shows that the dose has a stronger effect for the female than the 
male on the latency distribution for the susceptible group. As expected, the mean 
latent time of survival for the susceptible group decreases as dose increases. 
For each of the six models, we evaluate the two-fold and four-fold cross-validated 
GCPOs. For the two-fold cross-validated GCPO, we randomly divide the data into 
two halves by randomly selecting the data points associated with half of the ran-

15. Analysis of survival data 
267 
co 
C! 
0 
~ 
0 
~~ 
0 
~ 
0 
0 
c::i 
0 
50 
100 
150 
200 
Mean Survival Time 
FIGURE 15.2. Plot of the posterior densities of the mean survival time p.(xc) evaluated at 
the four dosages. The plot is the overlay of the four histograms (from left to right) for the 
mean survival times evaluated at the log-dose level -.22, -.69, -1.14, and -1.61 mgfcm2 , 
respectively. Note the mean survival time increases as the dosage decreases. 
TABLE 15.1. Model Comparisons 
Model 
P1 
h 
h 
log PGCPO 
log PGCPO 
two-fold 
four-fold 
1 
logistic 
normal 
exponential 
-523.09 
-522.73 
2 
logistic 
log-normal 
exponential 
-524.09 
-523.88 
3 
pro bit 
normal 
exponential 
-526.66 
-529.17 
4 
pro bit 
log-normal 
exponential 
-527.33 
-529.49 
5 
1 
normal 
-726.93 
-773.79 
6 
1 
-587.18 
-590.06 
domly chosen labels in 1-317. Similarly, we divide the data randomly into four 
groups of approximately equal sizes. 
Table 15.3lists the log product of the GCPO for each of the six models for the male 
data. The results show that the mixture models' outperform the single-component 
model by a huge margin. Model 1 (3) is the best among the six models. Model 2 
for a log-normal distribution is slightly inferior. The logistic link fits the data much 
better than the normal link. Finally, both two-fold and four-fold cross-validation 
yield similar results. 
Table 15.4 gives the fitted values and the residuals in absolute values of the cell 
count for Model 1. To check the overall model fitting, we use the summation of 
absolute deviances, that is, Dabs = L:;!1 L::=l !observed 
fitted! for all the cells 
in Table 15.1. 
For model 1, this sum based on the first 13 days is 51.4, contrasting to 58.3 
fitted by Pack and Morgan. If we break the sum into the absolute deviance for 
each dosage level, we see the absolute deviances are 14.2, 14.5, 10 and 12.7 for our 
model 1, versus 10.7, 16.3, 14.9 and 16.4 for the Pack and Morgan model for the 
four dosage levels respectively. That is our model 1 improves upon the Pack and 

268 
·K uo & Peng 
TABLE 15.2. Fitted Values and Absolute Residuals 
rjc 
Fitted value 
!Residual! 
-1.61 
-1.14 
-0.69 
-0.22 
-1.61 
-1.14 
-0.69 
-0.22 
1 
5.8 
5.4 
5.3 
5.0 
2.8 
1.6 
0.3 
1.0 
2 
8.0 
8.0 
8.2 
7.8 
3.0 
2.0 
0.2 
2.2 
3 
10.5 
11.2 
12.0 
12.1 
0.5 
0.2 
1.0 
4.1 
4 
9.0 
10.0 
11.2 
11.8 
2.0 
6.0 
3.8 
2.2 
5 
5.3 
5.8 
6.6 
7.3 
1.3 
2.8 
2.6 
0.7 
6 
2.4 
2.3 
2.6 
2.9 
0.6 
0.3 
0.6 
0.9 
7 
1.1 
0.8 
0.8 
0.8 
0.9 
0.2 
0.2 
0.2 
8 
0.8 
0.3 
0.2 
0.2 
0.2 
0.3 
0.8 
0.2 
9 
0.7 
0.3 
0.1 
0.1 
0.7 
0.3 
0.1 
0.1 
10 
0.7 
0.2 
0.1 
0.1 
0.7 
0.2 
0.1 
0.9 
11 
0.7 
0.2 
0.1 
0.1 
0.7 
0.2 
0.1 
0.1 
12 
0.6 
0.2 
0.1 
0.1 
0.4 
0.2 
0.1 
0.1 
13 
0.6 
0.2 
0.1 
0.0 
0.4 
0.2 
0.1 
0.0 
14 
97.8 
24.2 
6.5 
1.7 
3.2 
5.2 
0.5 
0.3 
Morgan model in prediction for all levels except at the log dose of -1.61 level. We 
can also compare the residuals using graphical methods. The graphs omitted here 
show that Model 1 improves upon the Pack and Morgan model for essentially most 
of the cells. 
References 
D. D. Boos and C. Brownie. Mixture models for continuous data in dose-response 
studies when some animals are unaffected by treatment. Biometrics vol. 4 7, 
pp. 1489-1504, 1991. 
D.R. Cox and D. Oakes. Analysis of Survival Data. Chapman and Hall: London, 
1984. 
A. Dempster, N. Laird, N and D. Rubin. Maximum likelihood from incomplete 
data via the EM algorithm. Journal of the Royal Statistical Society Series B 
vol. 39, pp.1-38, 1977. 
L. Devroye. Non- Uniform Random Variate Generation. Springer-Verlag: New York, 
1986. 
D. Dey, L. Kuo, and S. Sahu. A Bayesian predictive approach to determining the 
number of components in a mixture distribution. Statistics and Computing 
vol. 5, pp. 297-305, 1995. 
J. Diebolt and C. Robert. Estimation of finite mixture distributions through Bayesian 
sampling. Journal of the Royal Statistical Society Series B vol. 56, pp. 363-375, 
1994. 

15. Analysis of survival data 
269 
P.J. Diggle and R.J. Gratton. Monte Carlo methods of inference for implicit stat-
istical models. Journal of the Royal Statistical Society Series B vol. 46, pp. 
193-227' 1984. 
B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall: 
New York, 1993. 
V.T. Farewell. The use of mixture models for the analysis of survival data with 
long-term survivors. Biometrics vol. 38, pp. 1041-1046, 1982. 
A. E. Gelfand, D. K. Dey, and H. Chang. Model determination using predictive 
distributions with implementation via sampling-based methods (with Discus-
sion). In Bayesian Statistics 4, J. M. Bernardo, J. 0. Berger, A.P. Dawid, and 
A. F. M. Smith (eds), pp. 147-169, Oxford University Press: Oxford, 1992. 
A.E. Gelfand and A. F. M. Smith. Sampling based approaches to calculating 
marginal densities. Journal of the American Statistical Association vol. 85, 
pp. 398-409, 1990. 
A.E. Gelfand, A.F.M. Smith, and T.M. Lee. Bayesian analysis of constrained pa-
rameter and truncated data problems using Gibbs sampling. Journal of the 
American Statistical Association vol. 87, pp. 523-532, 1992. 
A. Gelman, and D. Rubin. Inference from iterative simulation using multiple se-
quences. Statistical Science vol. 7, pp. 457-472, 1992. 
S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the 
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and 
Machine Intelligence vol. 6, pp. 721-741, 1984. 
' 
P. S. Hewlett. Time from dosage to death in beetles Tribolium castaneum, treated 
with pyrethrins or DDT, and its bearing on dose-mortality relations. Journal 
of Stored Product Research vol. 10, pp. 27-41, 1974. 
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM 
algorithm. Neural Computation vol. 6, pp. 181-214, 1994. 
M. I. Jordan, and L. Xu. Convergence results for the EM approach to mixtures of 
experts architectures. Neural Networks, vol. 8, pp. 1409-1431, 1995. 
I 
A.Y.C. Kuk and C.H. Chen. A mixture model combining logistic regression with 
proportional hazards regression. Biometrika vol. 79, pp. 531-541, 1992. 
L. Kuo and A. F. M. Smith. Bayesian computations in survival models via the 
Gibbs samplers (with Discussion). In Survival Analysis: State of the Art, J. 
P. Klein and P. K. Goel (eds), pp. 11-14. Kluwer Academic: Dordrecht, 1992. 
M. G. Larson and G. Dinse. A mixture model for the regression analysis of com-
peting risks data. Applied Statistics vol. 34, pp. 201-211, 1985. 
P. McCullagh and J. A. Neider. Generalized Linear Models. Chapman and Hall: 
London, 1989. 
G. J. McLachlan and P. N. Jones. Fitting mixtures models to grouped and trun-
cated data via the EM algorithm. Biometrics vol. 44, pp. 571-578, 1988. 

270 
K uo & Peng 
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. 
Equation of state calculations by fast computing machines. Journal of Chem-
ical Physics vol. 21, pp. 1087-1092, 1953. 
S. E. Pack and B. J. T. Morgan, B.J .T .. A mixture model for interval-censored 
time-to-response quantal assay data. Biometrics vol. 46, pp. 749-757, 1990. 
F. Peng, R.A. Jacobs, and M. A. Tanner. Bayesian inference in mixtures-of-experts 
and hierarchical mixtures-of-experts models with an application to speech 
recognition. Journal of the American Statistical Association vol. 91, pp. 953-
960, 1996. 
M. A. Tanner and W. H. Wong. The calculation of posterior distributions by data 
augmentation. Journal of the American Statistical Association vol82, pp. 528-
550, 1987. 
J. Taylor. Semi-parametric estimation in failure time mixture models. Biometrics 
vol. 51, pp. 899-907, 1995. 
C. G. Wei and M. Tanner. A Monte Carlo implementation of the EM algorithm 
and the poor man's data augmentation algorithm. Journal of the American 
Statistical Association, 85, pp. 699-714, 1990. 

Part V 
Model Diagnostics and 
Variable Selection in GLMs 

This Page Intentionally Left Blank 

16 
Bayesian Variable Selection Using 
the Gibbs San1pler 
Petros Dellaportas 
Jonathan J. Forster 
Ioannis Ntzoufras 
ABSTRACT Specification of the linear predictor for a generalized linear model 
requires determining which variables to include. We consider Bayesian strategies for 
performing this variable selection. In particular we focus on approaches based on the 
Gibbs sampler. Such approaches may be implemented using the publically available 
software BUGS. We illustrate the methods using a simple example. BUGS code is 
provided in an appendix. 
1. 
Introduction 
In a Bayesian analysis of a generalized linear model, model uncertainty may be 
incorporated coherently by specifying prior probabilities for plausible models and 
calculating posterior probabilities using 
f(m!y) = 
f(m)f(ylm) 
mEM 
(1) 
2: f(m)f(y!m)' 
mEM 
where m denotes the model, M is the set of all models under consideration, f(m) 
is the prior probability of model m. The observed data y contribute to the poste-
rior model probabilities through f(y!m), the marginal likelihood calculated using 
f(y!m) = J f(y!m,f3m)f(f3mlm)df3m where f(f3mlm) is the conditional prior dis-
tribution of !3m, the model parameters for model m and f(y!m, !3m) is the likelihood 
of the data y under model m. 
In particular, the relative probability of two competing models m 1 and m 2 reduces 
to 
f(ml!Y) _ 
f(ml) 
f(m2!Y) -
/(m2) 
J f(y!ml' f3m 1 )/(f3m 1 lml) df3m 1 
J f(y!m2, f3m 2 )f(f3m2 lm2) df3m 2 
(2) 
which is the familiar expression relating the posterior and prior odds of two models 
in terms of the Bayes factor, the second ratio on the right hand side of (2). 
The principal attractions of this approach are that ( 1) allows the calculation of 
posterior probabilities of all competing models, regardless of their relative size or 
structure, and this model uncertainty can be incorporated into any decisions or 
predictions required (Draper, 1995, gives examples of this). 
273 

27 4 
Della port as, Forster & N tzoufras 
Generalized linear models are specified by three components~ distribution, link 
and linear predictor. Model uncertainty may concern any of these, and the approach 
outlined above is flexible enough to deal with this. In this chapter, we shall restrict 
attention to variable selection problems, where the models concerned differ only 
in the form of the linear predictor. Suppose that there are p possible covariates 
which are candidates for inclusion in the linear predictor. Then each m E M can 
be naturally represented by a p-vector ""'( of binary indicator variables determining 
whether or not a covariate is included in the model, and M C {0, 1 }P. The linear 
predictor for the generalized linear model determined by""'( may be written as 
p 
11 = L 'YiXd3i 
(3) 
i=l 
where {3 is the "full" parameter vector with dimension p, and Xi and {3i are the 
design sub-matrix and parameter vector, corresponding to the ith covariate. This 
specification allows for covariates of dimension greater than 1, for example terms in 
factorial models. 
There has been a great deal of recent interest in Bayesian approaches for identi-
fying promising sets of predictor variables. See for example Brown et a/.(1998) and 
Chipman (1996, 1997), Clyde et a/.(1996), Clyde and DeSimone-Sasinowska (1997), 
George et a/.(1996), George and McCulloch (1993, 1996, 1997), Geweke (1996), 
Hoeting et al.(1996), Kuo and Mallick (1998), Mitchell and Beauchamp (1988), 
Ntzoufras et a/.(1997), Smith and Kohn (1996) and Wakefield and Bennet (1996). 
Most approaches require some kind of analytic, numerical or Monte Carlo ap-
proximation because the integrals involved in (2) are only analytically tractable in 
certain restricted examples. A further problem is that the size of the set of possible 
models M may be extremely large, so that calculation or approximation of f(ylm) 
for all m E M is very time consuming. One of the most promising approaches has 
been Markov chain Monte Carlo (MCMC). MCMC methods enable one, in prin-
ciple, to obtain observations from the joint posterior distribution of ( m, !3m) and 
consequently estimate f(mly) and f(f3m lm, y). 
In this chapter we restrict attention to model determination approaches which can 
be implemented by using one particular MCMC method, the Gibbs sampler. The 
Gibbs samper is particularly convenient for Bayesian computation in generalized 
linear models, due to the fact that posterior distributions are generally log-concave 
(Dellaportas and Smith, 1992). Furthermore, the Gibbs sampler can be implemented 
in a straightforward manner using the BUGS soffware (Spiegelhalter et al., 1996a). 
To facilitate this, we provide BUGS code for various approaches in Appendix A. 
The rest of the chapter is organized as follows. Section 2 describes several vari-
able selection strategies that can be implemented using the Gibbs sampler Section 
3 contains an illustrative example analysed using BUGS code. We conclude this 
chapter with a brief discussion in Section 4. 
2. 
Gibbs Sampler Based Variable Selection Strategies 
As we are assuming that model uncertainty is restricted to variable selection, m 
is determined by ""'(. We require a MCMC approach for obtaining observations from 
the joint posterior distribution of f(m,f3m)· The Gibbs sampler achieves this by 
generating successively from univariate conditional distributions, so, in principle, 

'·16. Variable selection using ·a1bbs sample1· 
275 
the Gibbs sampler is determined by f( m, !3m)· However, flexibility in the choice of 
parameter space, likelihood and prior has led to a number of different Gibbs sampler 
variable selection approaches being proposed. 
The first method we shall discuss is a general Gibbs sampler based model de-
termination strategy. The others have been developed more specifically for variable 
selection problems. 
2.1 
Carlin and Chib 's Method 
This method, introduced by Carlin and Chib (1995) is a flexible Gibbs sampling 
strategy for any situation involving model uncertainty. It proceeds by considering 
the extended parameter vector (m,{3k; k EM). If a sample can be generated from 
the joint posterior density for this extended parameter, a sample from the required 
posterior distribution f(m, !3m) can be extracted easily. 
A joint prior distribution for m and (f3k; k E M) is required. Here, (f3k; k E M) 
contains the model parameters for every model in M. Carlin and Chib (1995) spec-
ify the joint prior distribution through the marginal prior model probability f( m) 
and prior density f(f3mlm) for each model, as above, together with independent 
"pseudoprior" or linking densities f(f3m'lm =f. m') for each model. 
The conditional posterior distributions required for the Gibbs sampler are 
where 
f(ml{f:lk : k EM}, y) = ~m 
Ak · 
kEM 
Am= f(ylm,f3m) IT [f(f3slm)]f(m), 
V mE M. 
sEM 
m'=m 
m' =f. m 
(4) 
(5) 
Therefore, when m' = m, we generate from the usual conditional posterior for model 
m, and when m' =f. m we generate from the corresponding pseudoprior, f(f3m,lm). 
The model indicator m is generated as a discrete random variable using (5). 
The pseudopriors have no influence on f(f3ml11f), the marginal posterior distribu-
tion of interest. They act as a linking density, and careful choice of pseudoprior is 
essential, if the Gibbs sampler is to be sufficiently mobile. Ideally, f(f3m'lm =f. m') 
should resemble the marginal posterior distribution f(f3mdm', y ), and Carlin and 
Chib suggest strategies to achieve this. 
The flexibility of this method lies in the facility to specify pseudo priors which help 
the sampler run efficiently. This may also be perceived as a drawback in problems 
where there are a large number of models under consideration, such as variable 
selection involving a moderate number of potential variables. Then, specification 
of efficient pseudopriors may become too time-consuming. A further drawback of 
the method is the requirement to generate every !3m' at each stage of the sampler. 
(This may be avoided by using a 'Metropolis-Hastings' step to generate m, but is 
outside the scope of the current chapter; see Dellaportas et al., 1997, for details). 
Examples which show how BUGS can be used to perform this method can be 
found in Spiegelhalter et al.(1996b). 

276 
Della port as, Forster & N tzoufras 
2.2 Stochastic Search Variable Selection 
Stochastic Search Variable Selection (SSVS) was introduced by George and Mc-
Culloch (1993) for linear regression models and has been adapted for more complex 
models such as pharmacokinetic models (Wakefield and Bennett, 1996), construc-
tion of stock portfolios in finance (George and McCulloch, 1996), generalized linear 
models (George et al., 1996, George and McCulloch, 1997), log-linear models (Nt-
zoufras et al., 1997) and multivariate regression models (Brown et al., 1998). 
The difference between SSVS and other variable selection approaches is that the 
parameter vector {3 is specified to be of full dimension p under all models, so the 
linear predictor is 
p 
'11 =I: Xif3i· 
(6) 
i=l 
Therefore 11 = X {3 for all models, where X contains all the potential explanatory 
variables. The indicator variables li are involved in the modelling process through 
the prior 
(7) 
for specified Ci and 'Ei. The prior parameters Ci and 'Ei in (7) are chosen so that 
when {i = 0 (covariate is "absent" from the linear predictor) the prior distribution 
for {3i ensures that {3i is constrained to be "close to 0". When {i = 1 the prior is 
diffuse, assuming that little prior information is available about {3i. 
The full conditional posterior distributions of {3i and li are given by 
and 
f(li = 1IY,'i'\i,{3)- f(f3lli = 1,--y,i)f(li = 1,--y\i) 
f(li = Olv,'i'\i,/3) 
f(f3lli = 0,--y\i)f(li = 0,--y,i) 
(8) 
where 'i'\i denotes all terms of')' except {i. 
If we use the prior distributions for {3 and 'i' defined by (7) and assume that 
f(li = 0,--y\i) = f(li = 1,'1'\i) for all i, then 
(9) 
where di is the dimension of {3i. 
The prior for 'i' with each term present or absent independently with probability 
1/2 may be considered non-informative in the sense that it gives the same weight 
to all possible models. George and Foster (1997) argue that this prior can be con-
sidered as informative because it puts more weight on models of size close to pj2. 
However, posterior model probabilities are most heavily dependent on the choice 
of the prior parameters Cf and 'Ei. One way of specifying these is by setting Cf 'Ei 
as a diffuse prior (for {i = 1) and then choosing cf by considering the the value 
of lf3i I at which the densities of the two components of the prior distribution are 
equal. This can be considered to be the smallest value of lf3i I at which the term 
is considered of practical significance. George and McCulloch (1993) applied this 
approach. Ntzoufras et a/.(1997) considered log-linear interaction models where {3i 
terms are multidimensional. 

16. Variable selection using Gibbs sampler 
277 
2.3 
Unconditional Priors for Variable Selection 
Kuo and Mallick (1998) advocated the use of the linear predictor 1J = I:f= 1 'YiXd3i 
introduced in (3) for variable selection. They considered a prior distribution f(f3) 
which is independent of; (and therefore M) so that f(f3ilf3\i,;) = f(f3ilf3\i) 
Therefore, the full conditional posterior distributions are given by 
(10) 
and 
f('Yi = 1ly,;\i,f3) 
f(YI'Yi = 1,;\i,/3) f('Yi = 1,;\i) 
!( 'Yi = Oly, 1\i, {3) - f(YI'Yi = 0, i\i, {3) !( 'Yi = 0, 1\i). 
(11) 
The advantage of the above approach is that it is extremely straightforward. It is 
only required to specify the usual prior on {3 (for the full model) and the conditional 
prior distributions f(f3i l/3\i) replace the pseudopriors required by Carlin and Chib's 
method. However, this simplicity may also be a drawback, as there is no flexibility 
here to alter the method to improve efficiency. In practice, if, for any {3i, the prior 
is diffuse compared with the posterior, the method may be inefficient. 
2.4 
Gibbs Variable Selection 
Dellaportas et a/.(1997) considered a natural hybrid of SSVS and the "Uncon-
ditional Priors" approach of Kuo and Mallick (1998). The linear predictor is as-
sumed to be of the form of (3) where unlike SSVS, variables corresponding to 
'Yi = 0 are genuinely excluded from the model. The prior for (;, {3) is specified as 
!(;,{3) = f(;)f(f31;). Consider the partition of{3 into (f3;,f3\;) corresponding 
to those components of {3 which are included ( 'Yi = 1) or not included ( 'Yi = 0) in 
the model, then the prior f(f31;) may be partitioned into model prior f(f3;li) and 
pseudoprior f(f3\;lf3;,;). 
The full conditional posterior distributions are given by 
f(f3; 1!3,,' ;, y) 
<X 
f(ylf3, ;)f(f3; 1;)!(!3,, 1!3;' ;) 
(12) 
f(f3,;1f3;,;,y) 
<X 
f(f3,;1f3,,;) 
(13) 
and 
f('Yi = 11;\i,f3,y) _ f(Yif3,,i = 1,;\i) f(f31m = 1,;\i) f('Yi = 1,;,i) 
f('Yi = Ol;\i,{3,y) 
f(ylf3,'Yi = 0,;\i) f(f3I'Yi = 0,;\i) f('Yi = 0,;\i)' 
(14) 
This approach is simplified if it is assumed that the prior for {3i depends only on 'Yi 
and is given by 
(15) 
This prior, where f(f3ili) = f(f3d'Yi) potentially makes the method less efficient 
and is most appropriate in examples where X is orthogonal. In prediction, rather 
than inference about the variables themselves is of primary interest, then X may 
always be chosen to be orthogonal (see Clyde et al., 1996). 
There is a similarity between this prior and the prior used in SSVS. However, 
here the full conditional posterior distribution is given by 

278 
Dellaportas, Forster & Ntzoufras 
and a clear difference between this and SSVS is that the pseudoprior f(f3i IIi = 0) 
does not affect the posterior distribution and may be chosen as a "linking density" 
to increase the efficiency of the sampler, in the same way as the pseudopriors of 
Carlin and Chib's method. Possible choices of Jti and Si may be obtained from a 
pilot run of the full model; see, for example Dellaportas and Forster (1999). 
2.5 Summary of Variable Selection Strategies 
The similarities and differences between the three Gibbs sampling variable selec-
tion methods presented in Sections 2.2, 2.3 and 2.4 may easily be summarized by 
inspecting the conditional probabilities (8), (11) and, in particular, (14). 
In SSVS, f(y!f3, '1') is independent of')' and so the first ratio on the right hand 
side of (14) is absent in (8). For the 'Unconditional Priors' approach of Kuo and 
Mallick (1998), the second term on the right hand side of (14) is absent in (11) as 
{3 and ')' are a priori independent. For Gibbs Variable Selection, both likelihood 
and prior appear in the variable selection step. These differences are also evident 
by looking at the graphical representations of the three methods in Figure 16.1. 
The key differences between the methods (including Carlin and Chib's method) 
are in their requirements in terms of prior and/ or linking densities. Carlin and 
Chib's method and GVS both require linking densities whose sole function is to aid 
the efficiency of the sampler. GVS is less expensive in requirement of pseudopriors, 
but correspondingly less flexible. The prior parameters in SSVS all have an impact 
on the posterior, and therefore the densities cannot really be thought of linking 
densities. The simplest method that described by Kuo and Mallick (1988) does not 
require one to specify anything other than the usual priors for the model parameters. 
3. 
Illustrative Example: 2 x 2 x 2 Contingency Table 
We present an analysis of the data in Table 16.1, taken from Healy (1988). This 
is a three-way table with factors A,B and C. Factor A denotes the condition of the 
patient (more or less severe), factor B denotes if the patient was accepting antitoxin 
medication and the (response) factor C denotes whether the patient survived or 
not. 
Survival( C) 
Condition (A) 
Antitoxin (B) 
No 
Yes 
More Severe 
Yes 
15 
6 
No 
22 
4 
Less Severe 
Yes 
5 
15 
No 
7 
5 
TABLE 16.1. Example Dataset. 
Purely for illustration purposes, and to present the BUGS code in Appendix A, 
we model the above data using both log-linear and logistic regression models. 

16. Variable selection using Gibbs sampler 
279 
SSVS Graphical Model 
Kuo and Mallick Graphical Model 
Gibbs Variable Selection Graphical Model 
FIGURE 16.1. Graphical Model Representation for Stochastic Search Variable Selection, 
Kuo and Mallick Sampler and Gibbs Variable Selection (Squares denote Constants; Circles 
denote Stochastic Nodes). 

280 
Dellaportas, Forster & N tzoufras 
3.1 
Log-Linear models 
We focus attention on hierarchical models including the main effects focusing our 
interest on associations between model factors and the corresponding interaction 
terms in the models. Here, i E {1, A, B, C, AB, AC, BC, ABC} sop= 8. The prior 
specification for model vector; is {i ""Bernoulli(1r) with 1r = 1/9 ifi =ABC, 1r = 
1 if i E {1, A, B, C} and ldiABC ""Bernoulli(1r) with 7r = 0.5(1- {ABC)+ {ABC 
for the two factor interactions (i E {AB,AC,BC}). This specification implies that 
the prior probability of including a two factor interaction in the model is 0.5 if the 
three factor interaction is excluded from the model and 1 if it is included in the 
model. Hence the prior probabilities for all 9 possible hierarchical models are 1/9 
and and non-hierarchical models are not considered. 
For the model coefficients we used the prior specification suggested by Dellaportas 
and Forster (1999) for log linear models which results in 'Ei = 2 in (15) when the 
f3i are considered to be the usual 'sum-to-zero' constrained model parameters For 
SSVS we used c['Ei = 2 and Ci = 103 in (7), as suggested by Ntzoufras et a/.(1997). 
ssvs 
KM 
GVS 
Models 
A+B+C 
0.1 
0.2 
0.2 
AB+C 
0.0 
0.1 
0.1 
AC+B 
25.1 
25.7 
25.6 
BC+A 
0.3 
0.6 
0.6 
AB+AC 
7.9 
7.5 
7.3 
AB+BC 
0.1 
0.2 
0.2 
AC+BC 
58.9 
58.4 
58.9 
AB+BC+CA 
6.4 
6.6 
6.4 
ABC 
1.0 
0.8 
0.6 
TABLE 16.2. Posterior model probabilities (%) for log-linear models. SSVS: Stochastic 
Search Variable Selection; K!Vf: Kuo and Mallick's Unconditional Priors approach; GVS: 
Gibbs Variable Selection. 
The results are based on 100,000 iterations for Gibbs variable selection and Kuo 
and Mallick's method, and 400,000 iterations for SSVS which seemed to be less 
efficient. For all methods we discarded 10,000 iterations as a burn-in period. The 
pseudoprior densities for Gibbs variable selection were constructed from the sample 
moments of a pilot run of the full model of size 1,000 iterations. All three meth-
ods give similar results supporting the same models with very similar posterior 
probabilities. 
3.2 Logistic Regression Models 
When we consider binomial logistic regression models for response variable C and 
explanatory factors A and B, there are 5 possible nested models, 1, A, B, A+B and 
AB. Priors are specified by setting c['Ei = 4 x 2 in (7) and 'Ei = 4 x 2 in ( 15) which 
is equivalent to the prior used above for log-linear model selection. The pseudoprior 
parameters were specified as before, through a pilot chain, and finally we set 1 ABC "" 
Bernoulli(1/5) and ldiAB "" Bernoulli( 1r), with 1r = 0.5(1 - {AB) + {AB for 

16. Variable selection using Gibbs sampler 
281 
i E {A, B}. The resulting prior probabilities for all models are 1/5. The results in 
table (16.3) are based on 500,000 iterations for SSVS and Kuo and Mallick's method 
and 100,000 iterations for Gibbs variable selection, with burn-in period of 10,000 
iterations. Again, the results are very similar, although Gibbs variable selection 
seemed to be most efficient. 
The equivalent log-linear models in Table 16.2 are those which include the AB 
term, so the results can be seen to be in good agreement. 
ssvs 
KM 
GVS 
Models 
1 
0.2 
0.5 
0.5 
A 
48.0 
49.2 
49.3 
B 
1.0 
1.2 
1.2 
A+B 
45.3 
44.0 
43.9 
AB 
5.5 
5.2 
5.1 
TABLE 16.3. Posterior model probabilities (%) for logistic regression. SSVS: Stochastic 
Search Variable Selection; KM: Kuo and Mallick's Unconditional Priors approach; GVS: 
Gibbs Variable Selection. 
4. 
Discussion 
We have reviewed a number of Bayesian variable selection strategies based on the 
Gibbs sampler. Their major practical advantage is that they can be easily applied 
with a Gibbs sampling software such as BUGS. 
It is impossible to provide a general recommendation for a method of computation 
for a class of problems as large as variable selection in generalized linear models. 
The methods we have discussed range from the "Unconditional Priors approach" 
I 
which is extremely easy to implement, but may be insufficiently flexible for many 
practical problems, to the approach of Carlin and Chib, which is very flexible, but 
requires a lot of careful specification. 
We have only discussed methods based on the Gibbs sampler. Of course other 
extremely flexible MCMC methods exist, such the reversible jump approach intro-
duced by Green (1996). All MCMC methods require careful implementation and 
monitoring, and other approaches should also be considered. For many model se-
lection problems involving generalized linear models, an alternative approach is 
through asymptotic approximation. Raftery (1996) has provided a series of Splus 
routines for this kind of calculation. Such methods can be used in conjunction with 
the Gibbs sampler approaches discussed here. 
Any Bayesian model selection requires careful attention to prior specification. For 
discussion of elicitation of prior distributions for variable selection, see Garthwaite 
and Dickey (1992) and Ibrahim and Chen (1998). 

282 
Dellaportas, Forster & N tzoufras 
5. 
Appendix: BUGS CODES 
Code and data files are freely available in the web adress http:/ jwww.stat-athens. 
aueb.gr/--jbn/ or by electronic mail request. 
5.1 
Code for Log-linear Models for 23 Contingency Table 
model 
l 
# 
# 
# # # # 
# 
# # const 
loglinear; 
2x2x2 LOG-LINEAR VARIABLE SELECTION WITH BUGS 
(c) OCTOBER 1996 FIRST VERSION 
(c) OCTOBER 1997 FINAL VERSION 
WRITTEN BY IOANNIS NTZOUFRAS 
ATHENS UNIVERSITY OF ECONOMICS AND BUSINESS 
SSVS: Stochastic Search Variable Selection 
KM 
: Kuo and Mallick Gibbs sampler 
GVS : Gibbs Variable Selection 
var N = 8; 
# number of Poisson cells 
include, 
# conditional prior probabability for gi 
pmdl[9], 
#model indicator vector 
mdl, 
# code of model 
b[N], 
#model coefficients 
mean[N], 
#mean used in pseudoprior 
(GVS only) 
se[N], 
# st.dev. used in pseudoprior(GVS only) 
bpriorm[N], #prior mean forb depanding on g 
tau[N], 
#model coefficients precision 
# 
c, 
# precision multiplicator 
(SSVS only) 
x[N,N], 
#design matrix 
z[N,N], 
#matrix used in likelhood 
n[N], 
#Poisson cells 
lambda[N], #Poisson mean for each cell 
g[N]; 
#term indicator vector 
data n,x in "ex1log.dat", mean, se in 'prop1ll.dat'; 
inits in "ex111.in"; 
{ 
# 
# # 
# 
# 
c<-1000.0 # SSVS only 
calculation of the z matrix used in likelihood 
for (i in 1:N) {for (j in 1:N) { 
z[i,j]<-x[i,j]*b[j]*g[j] 
# For GVS/KM 
z[i,j]<-x[i,j]*b[j]; 
#For SSVS 
}} 
# 
model configuration 
for (i in 1:N) { 
log(lambda[i])<-sum(z[i,]); 
n[i]*dpois(lambda[i]) 
} 
# 
defining model code 
# 
0 for [A] [B] [C] , 1 for [AB] [C] , 
2 for [AC] [B] , 
# 
3 for [AB] [AC] , 
4 for [BC] [A] , 
6 for [AB] [BC] , 
# 
6 for [AC][BC], 
7 for [AB][BC][CA],16 for [ABC]. 
# 
# # 
# 
mdl<-g[6]+2*g[6]+4*g[7]+8*g[8]; 
for (i in 0:7) { pmdl[i+1]<-equals(mdl,i) } 
pmdl[9]<-equals(mdl,16) 
Prior for b model coefficient 
tau[1] <-0 .1; 
bpriorm[1]<-0.0; 
b[1]*dnorm(bpriorm[1],tau[1]); 
for (i in 2:N) { 

# 
# 
# 
# 
# 
# 
# 
# # # # 
# 
# 
# 
# 
} 
, 16. Variable selection using Gibbs sampler 
283 
GVS using se,mean from pilot run 
tau[i]<-g[i]/2+(1-g[i])/(se[i]*se[i]); 
bpriorm[i]<-mean[i]*(1-g[i]); 
Kuo and Mallick (prior indepedent of g[i]) 
------------------------------------------
tau[i]<-1/2; 
bpriorm[i]<-0.0; 
SSVS PRIOR SET-UP 
tau[i]<-pow(c,2-2*g[i])/2; 
bpriorm[i]<-0.0; 
b[i]-dnorm(bpriorm[i],tau[i]); 
# 
defining prior information for ~i in such way that 
# 
allow only hierarhical models w~th equal probability. 
# 
include<-(1-g[8])*0.5+g[8]*1.0; 
g[s]-dbern(0.1111111); 
g[7].dbern(include); 
g[6].dbern(include); 
g[5]-dbern(include); 
for (i in 1:4) { g[i]-dbern(1.0)}} 
5.2 Code for Logistic Models with 2 Binary Explanatory Factors 
model 
# # 
# 
# 
# # # # # 
# 
Binomial; 
LOGISTIC REGRESSION VARIABLE SELECTION WITH BUGS 
(c) OCTOBER 1996 FIRST VERSION 
(c) OCTOBER 1997 FINAL VERSION 
WRITTEN BY IOANNIS NTZOUFRAS 
ATHENS UNIVERSITY OF ECONOMICS AND BUSINESS 
SSVS: Stochastic Search Variable Selection 
KM 
: Kuo and Mallick Gibbs sampler 
GVS : Gibbs Variable Selection 
# const 
N = 4; 
# number of binomial experiments 
var 
# 
include, 
# conditional prior probabability for gi 
pmdl[5], 
#model indicator vector 
mdl, 
# code of model 
b[N], 
#model coefficients 
mean[N], 
#mean used in pseudoprior 
(GVS only) 
se[N], 
# st.dev, used in pseudoprior (GVS only) 
bpriorm[N],# prior mean forb depanding on g 
tau[N], 
#model coefficients precision 
c, 
# precision multiplicator 
(SSVS only) 
x[N,N], 
#design matrix 
z[N,N], 
#matrix used in likelhood 
r[N), 
#number of successes in binomial 
n[N], 
#total number of observations for binomial 
p[N], 
#probability of success for binomial model 
g[N]; 
#term indicator vector 
data r,n,x in "ex1logit.dat", mean, se in 'prop1.dat'; 
inits in "ex1.in"; 
{ 
# 
# # 
c<-1000 # SSVS only 
calculation of the z matrix used in likelihood 
for (i in 1:N) { for (j in 1:N) { 

284 
Dellaportas, Forster & N tzoufras 
z[i,j]<-x[i,j]*b[j]*g[j] # for GVS 
# 
z[i,j]<-x[i,j]*b[j]; 
#for SSVS 
}} 
# # 
model configuration 
:for (i in 1:N) { 
r[i]"dbin(p[i],n[i]); 
logit(p[i])<-sum(z[i,])} 
# 
defining model code 
# 
0 constant, 1 for [A], 2 for [B], 
# 
3 :for [A][B], and 6 for [AB] 
# 
# 
mdl<-g[2]+2*g[3]+3*g[4]; 
pmdl[1]<-equals(mdl,O) 
pmdl[2]<-equals(mdl,1) 
pmdl[3]<-equals(mdl,2) 
pmdl[4]<-equals(mdl,3) 
pmdl[6]<-equals(mdl,6) 
# 
Prior :for b model coefficient 
tau[1] <-0 .1; 
# 
bpriorm[1]<-0.0; 
b[1]"dnorm(bpriorm[1],tau[1]); 
for (i in 2:N) { 
# 
GVS using se,mean from pilot run 
# 
--------------------------------
# 
# 
# 
# # 
# 
# 
# # # 
# 
# 
# 
# 
} 
tau[i]<-g[i]/8+(1-g[i])/(se[i]*se[i]); 
bpriorm[i]<-mean[i]*(1-g[i]); 
Kuo and Mallick proposal is indedent o:f g[i] 
tau[i]<-1/8; 
bpriorm[i]<-0.0; 
SSVS PRIOR SET-UP 
tau[i]<-pow(c,2-2*g[i])/8; 
bpriorm[i]<-0.0; 
b[i]"dnorm(bpriorm[i],tau[i]); 
# 
defining prior information for ~i in such way that 
# 
allow only hierarhical models w~th 0.2 probability. 
# 
g [4] "dbern(O. 2); 
include<-(1-g[4])*0.6+g[4]*1.0 
g[2]"dbern(include); 
g[3]"dbern(include); 
g[1] "dbern(1.0) 
} 
References 
Agresti, A. (1990). Categorical Data Analysis. John Wiley and Sons, New York. 
Brown, P.J., Vannucci, M. and Fearn, T. (1998). Multivariate Bayesian Variable 
Selection and Prediction. Journal of Royal Statistical Society, B, 60, 627-641. 
Carlin, B.P. and Chib, S. (1995). Bayesian Model Choice via Markov Chain Monte 
Carlo Methods. Journal of Royal Statistical Society, B, 57, 473-484. 

16. Variable selection using Gibbs sampler 
285 
Chipman, H. (1996). Bayesian Variable Selection with Related Predictors. Cana-
dian Journal of Statistics, 24, 17-36. 
Chipman, H., Hamada, M., Wu, C.F.J. (1997). A Bayesian Variable-Selection Ap-
proach for Analysing Designed Experiments with Complex Aliasing. Techno-
metrics, 39, 372-381. 
Clyde, M. and DeSimone-Sasinowska, H. (1997). Accounting for Model Uncertainty 
in Poisson Regression Models: Does Particulate Matter? Technical Report, 
Institute of Statistics and Desicion Sciences, Duke University, USA. 
Clyde, M., DeSimone, H. and Parmigiani, G. {1996). Prediction via Orthognalized 
Model Mixing. Journal of the American Statistical Association, 91, 1197-1208. 
Dellaportas, P. and Forster, J .J. (1999). Markov Chain Monte Carlo Model Deter-
mination for Hierarchical and Graphical Models. Biometrika, to appear. 
Dellaportas, P., Forster, J .J. and Ntzoufras, !.(1997). On Bayesian Model and 
Variable Selection Using MCMC. Technical Report, Department of Statistics, 
Athens University of Economics and Business, Greece. 
Draper, D. (1995). Assesment and Propogation of Model Uncertainty (with dis-
cussion). Journal of the Royal Statistical Society, B, 57, 45-97. 
Garthwaite, P.H. and Dickey, J .M. (1992). Elicitation of Prior Distributions for 
Variable-Selection Problems in Regression. The Annals of Statistics, 20, 1697-
1719. 
George, E.I. and Foster, D.P. (1997). Calibration and Empirical Bayes Variable 
Selection. Technical Report, University of Texas at Austin and University of 
Pennsylvania, USA. 
George, E.I. and McCulloch, R.E. (1993). Variable Selection via Gibbs Sampling. 
Journal of the American Statistical Association, 88, 881-889. 
George, E.I. and McCulloch, R.E. (1996). Stochastic Search Variable Selection. 
Markov Chain Monte Carlo in Practice, eds. W.R.Gilks , S.Richardson and 
D.J .Spiegelhalter, Chapman and Hall, London, UK, 203-214. 
George, E.I., McCulloch, R.E. and Tsay R.S. (1~96). Two Approaches for Bayesian 
Model Selection with Applications. Bayesian Analysis in Statistics and Econo-
metrics, eds. D.A.Berry, M.Chaloner and J.K.Geweke, John Wiley and Sons, 
New York, USA, 339-348. 
George, E.I. and McCulloch, R.E. (1997). Approaches for Bayesian Variable Se-
lection. Statistica Sinica, 7, 339-373. 
Geweke, J. (1996). Variable Selection and Model Comparison in Regression. Bayesian 
Statistics 5, eds. J.M.Bernardo, J.O.Berger, A.P.Dawid and A.F.M.Smith, 
Claredon Press, Oxford, UK, 609-620. 
Green, P.J. (1996). Reversible Jump Markov Chain Monte Carlo Computation and 
Bayesian Model Determination. Biometrika, 82, 711-732. 
Healy, M.J.R. (1988). Glim: An Introduction. Claredon Press, Oxford, UK. 

286 
Dellaportas, Forster & N tzoufras 
Hoeting, J .A., Madigan, D., and Raftery, A. E. (1996). A Method for Simultaneous 
Variable Selection and Outlier Identification in Linear Regression. Journal of 
Computational Statistics and Data Analysis, 22, 251-270. 
Ibrahim, J .G. and Chen, M.H. (1998). Prior Elicitation and Variable Selection for 
Generalized Mixed Models. Generalized Linear Models: A Bayesian Perspec-
tive, eds. D.K. Dey, S. Ghosh and B. Mallick, Marcel Dekker Publications. 
Kuo, L. and Mallick, B. (1998). Variable Selection for Regression models. Sankhya, 
B, 60, Part 1, 65-81. 
Madigan, D. and Raftery, A.E. (1994). Model Selection and Accounting for Model 
Uncertainty in Graphical Models Using Occam's Window. Journal of the 
American Statistical Association, 89, 1535-1546. 
Mitchell, T.J. and Beauchamp, J .J. (1988). Bayesian Variable Selection in Linear 
Regression. Journal of the American Statistical Association, 83, 1023-1036. 
Ntzoufras, I., Forster, J.J. and Dellaportas, P. (1997). Stochastic Search Variable 
Selection for Log-linear Models. Technical Report, Faculty of Mathematics, 
Southampton University, UK. 
Raftery, A.E. (1996). Approximate Bayes Factors and Accounting for Model Un-
certainty in Generalized Linear Models. Biometrika, 83, 251-266. 
Smith M. and Kohn R. (1996). Nonparametric Regression Using Bayesian Variable 
Selection. Journal of Econometrics, 75, 317-343. 
Spiegelhalter, D., Thomas, A., Best, N. and Gilks, W.(1996a), BUGS 0.5: Bayesian 
Inference Using Gibbs Sampling Manual. MRC Biostatistics Unit, Institute of 
Public health, Cambridge, UK. 
Spiegelhalter, D., Thomas, A., Best, N. and Gilks, W.(1996b). BUGS 0.5: Exam-
ples Volume 2, MRC Biostatistics Unit, Institute of Public health, Cambridge, 
UK. 
Wakefield, J. and Bennett, J. (1996). The Bayesian modelling of Covariates for 
Population Pharmacokinetic Models. Journal of the American Statistical As-
sociation, 91, 917-927. 

17 
Bayesian Methods for Variable 
Selection in the Cox Model 
Joseph G. Ibrahim 
Ming-Hui Chen 
ABSTRACT We consider the problem of Bayesian variable selection for proportional 
hazards regression models with right censored data. We discuss a semi-parametric 
approach in which a nonparametric prior is specified for the baseline hazard rate and 
a fully parametric prior is specified for the regression coefficients. For the baseline 
hazard, we investigate the Extended Gamma (EG) process priors and discuss choices 
of prior parameters suitable for variable selection. For the regression coefficients and 
the model space, we consider a semi-automatic parametric informative prior specifi-
cation that focuses on the observables rather than the parameters. We demonstrate 
that our prior specification is quite useful and flexible for the variable selection 
problem under a wide variety of situations. To implement the methodology, we use a 
Markov chain Monte Carlo method to compute the posterior model probabilities. In 
particular, the computational method presented in this chapter only requires Gibbs 
sampling from the full model, and thus is highly efficient for the variable selection 
problem. A simulated example is given to demonstrate the methodology. 
1. 
Introduction 
In the analysis of regression models for censored survival data, one often wishes 
to assess the importance of certain prognostic factors such as age, gender, or race 
in predicting survival outcome. This is a general problem which is encountered in 
most clinical trials research in cancer and AIDS. Typically, proportional hazards 
regression models using Cox's partial likelihood (Cox, 1975) are used to address 
this problem. Current techniques used for variable selection include asymptotic 
procedures based on score tests, Wald tests, and other approximate chi-square pro-
cedures. These procedures rely on Cox's partial likelihood and therefore do not use 
the full likelihood function to do variable selection. As is well known, Cox's partial' 
likelihood is an approximation to the full likelihood. Frequentist variable selection 
based on the full likelihood requires joint estimation of the baseline hazard and the 
regression coefficients. In this case, a non parametric estimate of the baseline hazard 
rate or a fully parametric specification of the survival model would be required. 
Again, one needs to rely on asymptotics to obtain variable selection criteria. Joint 
estimation of the baseline hazard and the regression coefficients can be a very dif-
ficult task. We have not seen any methods in the statistical literature that address 
this in the variable selection context. 
Bayesian analyses of proportional hazards models using the full likelihood func-
tion are becoming computationally feasible due to modern technology and recent 
advances in computing techniques such as the Gibbs sampler (Gelfand and Smith, 
1990) and other Markov chain Monte Carlo (MCMC) methods. The literature on 
Bayesian variable selection for survival models, however, is still quite sparse at best. 
A recent article includes Raftery, Madigan and Volinsky (1.995). However, this arti-
cle does not directly address the joint modelling or computations for the regression 
coefficients and the hazard rate. There are some articles addressing Bayesian analy-
287 

288 
Ibrahim & Chen 
sis of survival models using MCMC methods. These include Clayton (1991), Skene 
and Wakefield (1990), Kuo and Smith (1990), and Gray (1994). A somewhat related 
article focusing on variable selection in generalized linear models is by George, Mc-
Culloch and Tsay (1995). The potential advantage of using Bayesian methods to 
jointly model the baseline hazard and the regression coefficients is that one can 
accurately compute posterior model probabilities and their standard errors using 
MCMC simulation techniques. However, there still remains the chore of specify-
ing meaningful prior distributions and doing intensive computations. We present a 
methodology for this here and give the technical details in Sections 2 and 3. 
Ibrahim and Laud (1994), and Laud and Ibrahim (1995, 1996) advocate a predic-
tive approach to variable selection for the linear model by adopting the philosophy 
in Geisser (1993). The predictive approach they recommend is based on the notion 
of specifying a prior prediction y0 for the response vector, and a scalar precision 
parameter co which quantifies one,s prior belief in Yo. Then, (Yo, co), along with the 
design matrix for model m, are used to specify an automated parametric informa-
tive prior for the regression coefficients f3(m). The motivation behind this approach 
is that the investigator often has prior information on the observables from either 
past studies assuming the form of replicate experiments or from case-specific infor-
mation on the subjects in the current study. This information is often quantifiable 
in the form of a vector of prior predictions for the response vector of the current 
study. The predictive approach is appealing for variable selection problems, since 
there are so many parameters arising from the different models, and all have differ-
ent physical meaning, therefore making direct informative prior elicitation for the 
model parameters generally quite difficult. 
This predictive methodology seems to be well suited for cancer and AIDS clin-
ical trials research. In cancer clinical trials, for example, current studies often use 
treatments that are very similar or slight modifications of treatments used in past 
studies for a particular disease. The survival times (possibly censored) from a pre-
vious study, denoted y0 , can then be viewed as a prior prediction for the survival 
times of the current study. These prior predictions can then be used along with 
some design matrix Xo,m for model m and a scalar co, to elicit a prior distribution 
for the regression coefficients in the current study. If Yo is data from a previous 
study, one can use a formal Bayesian updating approach to the problem by using 
the posterior of the previous study as the prior distribution for the current study. 
'we elaborate further on advantages and disadvantages of this approach for this 
problem in Section 2.4. In the framework we develop here, the prior prediction Yo 
is quite general in the sense that it does not have to rely on the same design matrix 
as the current study. Also, y0 need not be of the same dimension as the vector of 
survival times in the current experiment and Xo,m need not involve covariate values 
from the current experiment. Specifically, when there is a previous study of sample 
size no that measures the same covariates as the current study, Yo consists of the 
no x 1 raw data vector of survival times in the previous study, vo is the vector of 
censoring indicators corresponding to Yo, and Xo,m is the design matrix from the 
previous study. 
The remainder of the chapter is organized as follows. In Section 2.2 and Section 
2.3, we introduce the priors for the baseline hazard and the approximate likelihood 
that arises from the induced prior on the hazard rate. The prior distributions for 
the regression coefficients and a prior distribution for the model space are discussed 
in Sections 2.4 and 2.5. In Sections 3.1 and 3.2, we present a novel method for 
estimating the marginal distribution of the data and a novel Gibbs sampling scheme 
for the parameters that facilitates the computation of poste~ior model probabilities. 

17. Bayesian Methods for Cox Model 
289 
In Section 4, we conduct a detailed simulation study. We conclude this chapter with 
a discussion section. 
2. 
The Method 
2.1 
Model and Notation 
A proportional hazards model is defined by a hazard function of the form 
h(t,x) = hb(t)exp(x'j3), 
(1) 
where hb(t) denotes the baseline hazard function at timet, x denotes the covariate 
vector for an arbitrary individual in the population, and j3 denotes a vector of 
regression coefficients. Let m denote a specific model in the model space M. A 
model throughout refers to a subset of covariates from the full model. Under model 
m, the likelihood function for a set of right censored data on n individuals for the 
current study in a proportional hazards model based on (1) is given by 
n 
L (f3(m), hb(t)) = II [ hb(ti) exp( ry}m))] IIi ( Sb(ti)exp(Tl!m))) , 
(2) 
i=l 
where ry~m) = x~m)' f3(m), ti is an observed failure time or censoring time for the ith 
individual and Vi is the indicator variable taking on the value 1 if ti is a failure 
time, and 0 if it is a censoring time. Moreover, x~m) is a km x 1 vector of covariates 
for the ith individual under model m, and Xm denotes the n x km covariate matrix 
of rank km. Further, Fb(·) denotes the baseline cumulative distribution function, 
and Sb(·) = 1- Fb(·) is the baseline survivor function, which, since we consider 
continuous survival distributions, is related to hb(·) by Sb(t) = exp (- J; hb(u) du). 
2.2 Prior Distribution for hb( ·) 
Since the baseline hazard rate plays such a key role in survival analysis, it is 
appropriate to specify a model for it. We consider a nonparametric prior over the 
collection of absolutely continuous hazard rates. Specifying a nonparametric prior 
for the baseline hazard rate seems natural for this problem in order to allow for 
a large and flexible class of hazards. Moreover, it will also facilitate comparison 
with Cox's partial likelihood. Specifically, we use the extended gamma (EG) process 
priors of Dykstra and Laud (1981) for the baseline hazard rate. This prior is suitable 
for both right censored and exact observations and has several attractive properties. 
The motivation for specifying the EG process prior over the hazard rates is that 
it has the advantage of placing the prior probability on absolutely continuous rather 
than on discrete distributions, as is the case with the Dirichlet process prior. Though 
in many situations the discreteness of the Dirichlet process is not a drawback, it can 
produce serious difficulties in the proportional hazards setting. Additionally, the EG 
process priors are not piecewise constant, as are some other priors, therefore giving 
added flexibility in modeling the hazard rate. 
There has been some previous work on modeling the cumulative baseline hazard 
rate. Kalbfleisch (1978) considers gamma process priors for the cumulative baseline 
hazard and Clayton (1991) illustrates Gibbs sampling techniques for frailty mod-
els using gamma process priors on the cumulative baseline hazard rate. The prior 

290 
Ibrahim & Chen 
specifications of Kalbfleisch (1978) and Clayton (1991) are entirely different from 
the ones considered here. As Kalbfleisch (1978) points out, the assumption of in-
dependent increments for the prior on the baseline cumulative hazard may not be 
a very satisfactory representation of the prior distribution for the baseline hazard. 
Alternatively, we directly specify a prior distribution on the baseline hazard rate. 
The EG process prior of Dykstra and Laud (1981) can be described as follows. 
Let G(a, A) denote the gamma distribution with shape parameter a~ 0 and scale 
parameter A. For a= 0, we define this distribution to be degenerate at 0. For a> 0, 
its density with respect to Lebesgue measure is 
{ 
>.a-
a-1 -.>.:v 
f (xI a' A) = 
r( a) x 
0 e 
' 
x>O 
otherwise 
Let a(t), t ~ 0, be a non-decreasing left continuous function such that a(O) = 0, and 
let Z(t), t ~ 0, be a gamma process with parameter a(·). That is, Z(O) = 0, Z(t) 
has independent increments, and fort > s, Z(t)- Z(s) is G(a(t)- a(s), 1). Now 
let A(t), t?. 0 be a positive right continuous function with left hand limits existing 
bounded away from 0 and oo, and define 
h;(t) = J.' [A(slr
1 dZ(s) , 
where the integration is with respect to the sample paths of the Z(t) process. The 
process {hb(t), t ~ 0} is called the Extended Gamma (EG) process and is denoted 
by 
hb(t),..., r (a(·), A(·)) . 
Provided that a(s) is not 0, the sample paths of an EG process are well defined 
increasing hazard rates corresponding to absolutely continuous distributions. Since 
the sample paths of the EG process are almost surely increasing functions, we are 
placing our prior probability entirely within the class of distributions with increasing 
hazard rates. We note here that the EG process prior is thus not piecewise constant 
and much different from the prior considered by Gray (1994). 
We have chosen to place a prior distribution over the class of continuous hazard 
rates for several reasons. A model for a discrete survival distribution implies an 
extremely bumpy hazard, something which we seek to avpid. The assumption of 
an underlying continuous process provides a sound mathematical underpinning for 
the approximation to which we turn and allows an important flexibility in practice. 
There are many situations in which an increasing hazard rate is appropriate, as in 
the myeloma study for example; see Ibrahim and Chen (1998). However, there are 
also situations in which the increasing hazard rate assumption is inappropriate, for 
example, when a significant proportion of the patients are "cure". We note that 
such studies require sufficient follow-up. We demonstrate in Section 4, however, 
that posterior model probabilities and model choice in general are not sensitive to 
the increasing hazard rate assumption, and this assumption appears to have little 
impact on the analysis. For the variable selection problem, the baseline hazard rate 
is viewed as a nuisance parameter since our main goal is to make inference on the 
regression coefficients. Thus, the prior distribution and the modeling of the baseline 
hazard rate play a secondary role to our primary aim of variable selection. In any 
case, we recommend that the techniques presented in this section be used only when 
the assumption is deemed appropriate. 
To define an approximation to the EG process prior on hb(t) that is computa-
tionally tractable, we first construct a finite partition of the time axis. Let 0 ~ s0 < 

17. Bayesian Methods for Cox Model 
291 
s1 < ... < SM denote this finite partition, with SM > tj, for all j = 1, ... , n. Let 
denote the increment in the baseline hazard in the interval ( Si- 1, si], i = 1, ... , M. 
The 6i 's are random variables since the baseline hazard is assumed random. The 
Di 's are independent a priori, and have a distribution induced by the underlying 
EG process. The density of the Di 's is most easily expressed in terms of its Laplace 
transform, which is given by 
Lh(u) = exp {-J.' log ( 1 + ,\~s)) da(s)} 
(3) 
An expression for the density of the Oi with respect to Lebesgue measure is not 
available for a general A(s). However, the prior reduces to a gamma process prior 
when A(s) = Ai on (si-b si]. That is, the 6i's have independent gamma distributions 
with shape parameters a(si)-a(si-d and scale parameters Ai. We give more details 
on how to do computations with EG process priors in Section 3.2. The variance of 
the EG process is controlled by choosing A( Si) large (small) to reflect sharp (vague) 
prior beliefs at various time intervals. Letting d = ( 61, ... , DM ), the prior density 
of dis given by 
M 
1r(d) =IT J(oi) , 
(4) 
i=1 
where f(6i) is the prior density of 6i whose Laplace transform is given by (3). The 
prior parameters of the Di 's may, in principle, be chosen to depend on m. This 
dependence may come from the covariates in the current study or other model 
specific quantities. We do not attempt such a specification here. We essentially 
view d as a nuisance parameter in the variable selection problem, and thus its 
dependence on m is not as crucial as the dependence of the regression coefficients 
on m. 
Choices of prior parameters for d can be made in several ways. Viewing the 
baseline hazard rate as a nuisance parameter in the variable selection problem, one 
may take naive choices of prior parameters for the 6i's such as a(s) = Si- Bi- 1 for 
Si-1 :5 s :5 Si, and A( s) = 1 for all s. This choice of prior parameters results in 
independent G ( Si -
Si -1, 1) priors for the Di 's with the shape parameter consisting 
of the interval width and scale equal to 1. An obvious advantage of this choice is 
that it simplifies calculations. It may be suitable if there is little prior information 
available on the baseline hazard rate. 
A more elaborate choice of prior parameters for the Di 's can be made by taking 
into account the mean and the variance of the EG process. The mean and variance 
of the process are increasing functions denoted by (J.t(s), u2(s)), where 
p(s) = l A(u)a'(u) du 
u2(s) = l A2(u)a'(u) du, 
(5) 
(6) 
and a'(u) = da(u)fdu. As discussed in Dykstra and Laud (1981), it seems reason-
able to assign as J.t( s) the "best" guess of the hazard rate and use u2 ( s) to model 
the amount of uncertainty or variation in the hazard rate 

292 
Ibrahim & Chen 
For the variable selection problem, we consider a method of specifying more in-
formative choices for (a(s), A(s)) by incorporating the prior prediction y0 into the 
elicitation process. Notice that when A(u) = 1, p,(s) = u2(s) = a(s) from (5) and 
(6), and therefore a(s) is both the mean and the variance of the process. Moreover, 
each Di has a gamma prior with shape parameter ai = a(si)- a(si-d· In this case, 
a suitable choice of a( s) would be an increasing estimate the baseline hazard rate. 
To construct such an estimate, we can fit a Weibull model via maximum likelihood 
using Do = (no, yo, X 0 , v0) as the data. Often, the fit will result in a strictly in-
creasing hazard. We denote such a hazard by hb(s I Yo) and set p,(s) = hb(s I Yo). 
With the convention that A(s) =A for all s, we solve to obtain a(s) = Ahb(s I y0 ). 
A slight generalization of this parameter choice is to take A( s) piecewise constant 
over theM subintervals and take a(s) = A(s) hb(s I p,0), where A(s) = Si- Si-1, 
for Si -1 < s $ Si. In the event that the fitted Wei bull model results in a constant 
or decreasing hazard, doubt is cast on the appropriateness of the EG process as a 
model for the hazard, and we do not recommend the method discussed here. 
There are numerous other approaches to selecting this baseline hazard. Alter-
native classes of parametric models may be fit to y0 or a nonparametric method 
such as that of Padgett and Wei (1980) may be used to construct an increasing 
hazard. Once the hazard is selected, we have specified p,(s). With a constant A(s), 
this determines a(s) and the EG process is specified. With non-constant, but rel-
atively (relative to the interval widths) slowly varying A(s), a solid strategy is to 
take A(s) piecewise constant and proceed to solve for a(s). Whatever details are 
used in these choices, the advantage of this predictive approach is that we can use 
the prior information to construct estimates to be used as prior parameters. 
2. 3 
The Likelihood Function 
We now construct the approximate likelihood function for CB(m), d) for any 
model m E M. Let x(m) denote the km x 1 vector of covariates for an arbitrary 
individual under model m. Then, the cumulative distribution function for the pro-
portional hazards model at time s is given by 
F(s) = 1- exp {- exp{q(m)} l hb(t) dt} 
<::: 1 - exp {- exp{ 'l(m)} ( (s- so)+ hb(so) + t 6;(s- Si-1)+)} , 
(7) 
where (t)+ = t if t > 0, 0 otherwise, and fJ(m) = x(m)' f3(m). We assume here that 
hb(so) = 0, and F(s) = 1 for s > SM, so that (7) is slightly simplified. This first 
approximation arises since the specification of d does not specify the entire hazard 
rate, but only the Di. For purposes of approximation, we take the increment in the 
hazard rate, Di, to occur immediately after Si_1 . Let Pi denote the probability of 
a failure in the interval (si-1, Si], i = 1, ... , M. Using the fact that hb(s0 ) = 0, we 
have 
Pi = F(si)- F(si-d 
~ exp {- exp{q(m)} I: 6j(Bi-1- SJ-1)} 
J=1 

17. Bayesian Methods for Cox Model 
293 
Thus, in the ith interval (si-b si], the contribution to the likelihood function for an 
exact observation (i.e., a failure) is Pi and 1- F( Si) for a right censored observation. 
Let di be the number of failures, and Ci be the number of right censored observations 
in the ith interval, respectively, i = 1, ... , M. For ease of exposition, we order the 
observations so that in the ith interval the first di are failures and the remaining 
Ci are right censored, i = 1, ... , M. Let x ~;:) denote the vector of co variates for the 
kth individual in the ith interval under model m, and define 
M 
d; 
2: 2: Ujk(fJ(m))(sj-1- Bi-1) , 
j=i+1 k=1 
M 
d;+c; 
bi 
E E Ujk(fJ(m))(sj- Bi-d , 
(si- si-d :L>sj 
j=1 
The approximate likelihood function, given the data D for the current study, over 
all M intervals is given by 
L({J(m)' ~ I D) 
{il exp { -h;(a; + b;)}} 
x {il g ( 
1- exp{ -u;k(p(m))1l(~)})} . 
(8) 
We note that this likelihood involves a second approximation. Instead of condition-
ing on exact event times, we condition on the intervals in which events occur, and 
thus we approximate continuous right censored data by interval censored data. 
2.4 
Prior Distribution for the Regression Coefficients 
In general, for most problems, there are no firm guidelines on the method of prior 
elicitation. Typically, one tries to balance sound theoretical ideas with practical and 
computationally feasible ones. A common situation that arises in statistical practice 
is that data Do from a previous study is available to use as prior information for 
the current study. The issue of how to incorporate Do into the current study has 
no obvious solution since it depends in large part of how similar the two studies 
are. In most clinical trials, for example, no two studies will ever be identical. In 
many cancer clinical trials, the patient populations typically differ from study to 
study even when the same regimen is used to treat the same cancer. In addition, 
other factors may make the two studies heterogeneous. These include conducting the 
studies at different institutions or geographical locations, using different physicians, 
using different measurement instruments and so forth. Due to these differences, an 

294 
Ibrahim & Chen 
analysis which combines the data from both studies may not be desirable. In this 
case, it may be more appropriate to "weight" the data from the previous study so as 
to control its impact on the current study. Thus, it is desirable for the investigators 
to have a prior distribution that summarizes the prior data Do in an efficient and 
useful manner and allows them to tune or weight Do as they see fit in order to 
control its impact on the current study. In addition to this, it is always desirable 
to have a prior distribution that is easy to interpret and has a convenient workable 
closed form. The prior distribution we consider here satisfies these conditions. It is 
also a very practical and useful prior which approximates a more elaborate prior 
based on formal Bayesian updating as discussed in more detail below. 
We assume a priori independence between the the baseline hazard rate and the 
regression coefficients, and thus the joint prior density of (;J(m), Ll.) under model m 
is given by 
(9) 
The assumption of prior independence between p(m) and Ll. is a sensible specifica-
tion, since we are viewing the hazard rate as a nuisance parameter for our problem. 
We consider a fully parametric multivariate normal prior for p(m), since the normal 
prior has proved to be a flexible and useful class of priors for many regression prob-
lems (see Geisser, 1993). Thus let Nop(JJ, T) denote the p dimensional ·multivariate 
normal distribution with mean JJ and precision matrix T. Thus, under model m, we 
take 
(10) 
where co is a scalar quantifying the degree of prior belief one wishes to attach to 
JJ(m). Under model m, we have the prior information Do = (no, Yo, Xo,m, vo) and co, 
where Xo,m is an no x km design matrix, Yo is an no x 1 vector of prior predictions, 
and Vo is the corresponding no x 1 vector of censoring indicators. We take the prior 
mean of p(m) to be the solution to Cox's partial likelihood equations for p(m) using 
Do as data. Suppose there are r failures and n0 - r right censored values in y0 • 
Cox's partial likelihood for p(m) based on Do is given by 
r { 
{ (m)' a(m)} 
} 
L*(;J(m)) =IT 
exp Xoi 
P(m)' 
' 
i=l 
I:ten 
exp{xol ;J(m)} 
(Yoi) 
(11) 
where x~r;')' is the ith row of Xo,m, (Yol, ... , Yor) are the ordered failures and 'R(Yoi) 
is the set of labels attached to the individuals at risk just prior to Yoi. Now we take 
JJ(m) to be the solution to 
8log (L*(;J(m))) 
8f3)m) 
0, 
(12) 
j = 1, ... , km. The matrix Tm is taken to be the Fisher information matrix of p(m) 
based on the partial likelihood in ( 11). Thus 
(13) 
An attractive feature of the priors for pCm) is that they are semi-automatic in 
the sense that one only needs a one time input of (Do, co) to generate the prior 

17. Bayesian Methods for Cox Model 
295 
distributions for all m E M. The priors defined by (9) and (10) represent a sum-
mary of the prior data Do through (J.L(m), Tm) which are obtained via Cox's partial 
likelihood. This is a practical and useful summary of the data Do as indicated by 
many authors including Cox (1972, 1975), and Tsiatis (1981). 
The prior given by (9) and (10) has several advantages. First, it has a closed 
form and is easy to interpret. Second, the prior elicitation is straightforward in 
the sense that (J.L(m), Tm) and co completely determine the prior for p(m) for all 
m E M. Third, (9) and (10) are computationally feasible and relatively simple. 
Fourth, our prior assumes a priori independence between (;J(m), ~),which further 
simplifies interpretations as well as the elicitation scheme. 
In addition, (9) and (10) provide a reasonable asymptotic approximation to the 
prior for (;J(m), ~) that is obtained from a formal Bayesian update of the data D0 • 
To see this, let 7ro(;J(m), ~) denote the "original" joint prior for (;J(m), ~) for the 
previous study, and let L(;J(m), ~ I Do) denote the likelihood function of (;J(m), ~) 
for the previous study. Then, for the current study, a formal Bayesian update leads to 
a prior distribution that is equal to the posterior distribution based on the previous 
study. That is, the prior distribution of (pCm), ~) for the current study based on a 
formal Bayesian update of the original prior would take the form 
(14) 
To simplify the specification, we can take (p(m), ~) to be independent in the 
original prior so that 7ro(f3(m), ~) = 7ro(f3(m)) ?ro(~). Moreover, it is reasonable to 
take ?ro (;JC m), ~) to be a vague proper prior. The original prior ?ro (;JC m), ~) does not 
depend on Do, and thus 1r(;J(m), ~) depends on Do only through L(;J(m), ~ I Do). 
One of the major drawbacks with using the prior in (14) is that it does not allow 
the investigator to directly assign a weight to the prior data Do. That is, (14) does 
not allow a weight for L(;J(m), ~ I Do). For example, if 7ro(f3(m), ~) is chosen to 
be non-informative, then (14) effectively weights Do and D equally, which may be 
undesirable, especially in situations where the two studies are not very similar. To 
control the impact of the prior data Do in (14), we can weight L(;J(m), ~ I Do) in 
an appropriate fashion. We thus modify (14) to take the form 
(15) 
where co ~ 0 is a scalar prior parameter. The case co = 1 essentially corresponds to 
combining the prior data Do and the current data D. The case co = 0 can be used if 
no previous study exists and the investigator wishes to specify only an original prior 
?ro(;J(m), ~). Although (15) may seem like an attractive prior to use, it is relatively 
difficult to interpret and does not have a closed form. For example, it is not clear 
what the prior moments of (15) are. Moreover, (15) is quite computationally difficult 
to work with. Our priors for (;J(m), ~) given by (9) and (10) provide a reasonable 
asymptotic approximation to (15). In particular, our prior is based on a normal 
approximation to (L(;J(m), ~I Do)r
0 with 7ro(;J(m)) taken as a vague proper prior. 
This is a sensible and practical approximation, since for most inference problems, 
likelihoods are well approximated by normal distributions for large samples under 
suitable regularity conditions. 
Since the prior parameters (J.L(m), Tm) in (10) do not depend on ~' p(m) and ~ 
are independent a priori. This seems to be a reasonable and computationally prac-
tical assumption. Moreover, it facilitates a more straightforward semi-automatic 
prior elicitation which is easy to interpret. Also, as shown in Section 3, our sug-
gested priors have several computational advantages over (14) or (15). Although 

296 
Ibrahim & Chen 
our methods do not formally allow an exact combining of the raw data from the 
previous and current study, (10) is a reasonable approximation to combining the 
data from the previous study if one chooses c0 = 1. One advantage of our priors is 
that they provide a useful summary of Do even if Do is elicited from expert opinion. 
That is, one does not need an actual previous study to use these priors. The priors 
considered in this section give us a clear specification of our beliefs immediately 
before incorporating data from the current study. We view our simplification of the 
prior as an almost necessary step to facilitate the description of the analysis. 
If we have a previous study with the same set of covariates as the current study 
then the choice of Do is straightforward. In this case, n0 would be taken as the 
sample size from the previous study, Xo,m is taken as the design matrix from the 
previous study under model m, Yo is taken as the raw data vector of survival times 
from the previous study along with its censoring indicators v0 • The parameter co 
resembles the one from Zellner's g-priors (Zellner, 1986). Possible choices of co 
include co= no/n, or co= log( no) where n is the sample size of the current study. 
Other potential choices of co are discussed in Zellner (1986). If the set of covariates 
in the current study is a subset of the covariates in the previous study, then we can 
construct a submatrix Xo,m by omitting those columns corresponding to covariates 
not in the current study. 
If the set of covariates in the previous study is a subset of the set of covariates 
in the current study, then one possibility is to use a combination of prior data and 
expert opinion to construct the prior. Alternatively, the investigator may specify 
vague prior information for the regression coefficients of the new covariates. In any 
case, a prior for p(m) can be obtained as follows. Let Xnew denote then x s matrix 
of covariates not measured in the previous study with corresponding regression 
coefficients Pnew. Let Xnew,m be the n x Sm matrix of new covariates under model 
m. Thus, the columns of Xnew,m are a subset of those of X new. Also, let j]~r;'J denote 
the Sm x 1 vector of regression coefficients corresponding to Xnew,m· We partition 
p(m) into 
j3(m) = ( 
/]~~) ) 
a(m) 
pnew 
where /]~~) is an rm x 1 vector which represents the regression coefficients corre-
sponding to the set of covariates Xold common to the two studies. Thus, p(m) is a 
km x 1 vector of regression coefficients in the current study, where km = rm + Sm. 
In our prior specification, we assume that the new covariates have small or negligi-
ble correlation to the old covariates, that is, Corr(Xold, Xnew) ~ 0. This seems 
to be a sensible assumption if in fact the new set of covariates in the current 
study are b(ei~~~ie)ntifically investigated for the first time. The prior distribution 
of p(m) = 
is given by 
a(m) 
pnew 
(16) 
where 

17. Bayesian Methods for Cox Model 
297 
and 
-
( coTm,old 
Tm = 
(m)' 
Ao 
Here, A~m) is an rm x Sm matrix, Ro,m is the prior precision matrix for the new 
regression coefficients, b0 is a scalar, and fl.~~~ is the prior mean for !3~ 7~. The 
investigator can specify A~m), Ro,m, b0 , and fl.~~~ by expert opinion or by using 
case specific information from the current study. For example, A~m) and Ro,m can 
be constructed from the covariates of the current study. Vague choices of these 
parameters are also possible by taking A~m) = 0, b0 = .01 or bo = .001, fl.~~~ = 0, 
and Ro,m to be a diagonal matrix with diagonal elements equal to the inverses of 
the sample variances of the new covariates. In any case, we always recommend that 
sensitivity analyses be conducted for several values of these parameters. Note that 
taking A~m) = 0 implies that ;3~7~ and !3~7:/ are independent a priori, and taking 
Ro,m to be diagonal implies that the components of !3~7~ are independent a priori. 
If a relevant previous study does not exist, Do may be specified as follows. The 
prior prediction y0 may be elicited from expert opinion or by using case-specific 
information about each individual in the current study, and perhaps by using a 
design matrix Xm• based on a given model m* to obtain a point prediction of the 
form 
Yo= g(Xm• ), 
where g(.) is a specified function. The vector y0 may also be obtained by a theoretical 
model giving forecasts for the survival times. Also, in this case one could take 
Xo,m = Xm and n0 = n. Thus, we use the design matrix of the current study to 
obtain the prior distribution for p(m) in (10). 
The elicitation scheme is less automated than the one in which a previous study 
exists. There are a number of other ways one could elicit y0 , and in general, the 
elicitation depends on the context of the problem. We do not attempt to give a 
general elicitation scheme for y0 in this setting, but rather mention some general 
possibilities. In any case, the cleanest specification of Do is to use data from a 
previous study for which y0 would be taken to be the vector of survival times from 
the previous study and Xo,m is taken as the design matrix from the previous study 
under model m. 
2. 5 Prior Distribution on the Mode! Space 
The prior prediction y0 , whether the actual result of a previous study or a predic-
tion based on expert opinion, can always be viewed as prior data. Viewing y0 this 
way, we specify an original prior 7ro(f3(m), Ll.) for the model parameters as described 
in Section 2.4. We also specify an original prior for the model space, denoted p0(m). 
Our strategy is to specify vague proper original priors, and then update p0(m) by 
Bayes theorem to obtain prior model probabilities, p(m), for the current study. 
To this end, we specify a uniform original prior on M, that is, p0(m) = 2-k for 
all mE M. Moreover, we take 7ro(f3(m), Ll.) = 7ro(f3(m))7ro(Ll.) to be a vague proper 
prior as discussed in Section 2.4. Given the prior prediction y0 , the prior probability 
of model m for the current study based on an update of Yo via Bayes theorem is 
given by 
p(Dolm) Po(m) 
p(m) = p(mlDo) = 2: 
(D I ) ( ) , 
meMP 
om Po m 
(17) 

298 
Ibrahim & Chen 
where 
p(Dolm) =I I L(b., p(m)IDo) 7ro(P(m)) 7ro(~) dp(m) db. , 
(18) 
and L(b_,p(m)IDo) is the likelihood function of the parameters based on D0 • Thus, 
the prior probability of model m for the current study is precisely p(m) = p(miD0 ). 
This framework for specifying p(m) is most natural if we actually had a previous 
study that yielded survival times y0 • In this case, L(b., p(m) I Do) is just the likelihood 
function of the parameters based on the data Do from the previous study, 7r0(p(m)) 
is the original prior distribution of the regression coefficients from the previous study 
and 7ro(b.) is the original prior for the baseline hazard rate for the previous study. 
Then, Bayes theorem is used in the usual way to update. 
The original prior 7ro(p(m), b.) is mainly viewed here as a necessary device to 
calculate p(Do I m), and hence the prior model probabilities p(m). The main intent 
is to pick a vague proper prior 7r0(p(m), b.) that yields a reasonable set of prior model 
probabilities forM. The choice of 7ro(p(m)) we consider here is a multivariate normal 
prior with mean 0 (i.e., no regression) and a diagonal precision matrix having small 
diagonal elements. Thus, with a slight abuse of notation, we write 
7ro(P(m)) = Nokm (0, d~m)Wo,m) , 
where Wo,m is a diagonal matrix and d~m) is a positive scalar changing with each 
m. The ith diagonal element of Wo,m can be chosen to be equal to the ith diagonal 
element ofTm defined in (13). If the covariates are all standardized or are measured 
on the same scale, then we can take Wo,m = I. 
The parameter d~m) plays a major role in assigning the prior probability to model 
m. Large values of 4m) will tend to increase the prior probability for model m. 
It seems reasonable to let d~m) depend on km in some automated and consistent 
fashion. Following Laud and Ibrahim (1996), we let 
(m) _ 
d~m) 
Vo 
-
( ) ' 
1 +dom 
and take v~m) to be of the form 
(19) 
where b and a are specified prior parameters in [0, 1]. As Laud and Ibrahim (1996) 
point out, the decomposition of v~m) into the form in (19) provides flexibility in 
assigning a prior for M. The parameters a and b play an important role in this 
formulation, especially in determining prior probabilities for models of different 
size. One method of specifying a and b is as follows. Let k denote the number of 
covariates for the full model, and suppose, for example, the investigator wishes to 
assign prior weights in the range r 1 = .10 for the single covariate model (i.e., km = 1) 
to r2 = .15 for the full model (i.e., km = k). This leads to the equations 
r1 = ba = .10 
(20) 
and 
(21) 
for which a closed form solution for a and b can be obtained. The choice of a and b, 
and hence d~m), is made with the goal of letting, within each model, the likelihood 

17. Bayesian Methods for Cox Model 
299 
govern the posterior. This suggests taking d~m) small, i.e., r1 close to r2. On the 
other hand, the models in M have different dimensions, and so taking d~m) too 
small will lead to a posterior that favors models of small dimension very strongly. 
In practice, the values of a and b obtained from the procedure defined in (20) and 
(21) provides only a reasonable starting point. In actual practice, we recommend 
an iterative process to select the final values of a and b. Thus to select the final 
values, we compute the prior model probabilities given in (17) for various sets of 
(a, b). Final selection of (a, b) is made to ensure that a reasonable set of models 
receive non-negligible probability. Special attention must be paid to models of very 
large and very small dimension. If the covariate sets from the previous study and 
the current study are not identical, then we make the modifications to 7r0(p(m)) in 
a similar way as was done for 1r(p(m)) in Section 2.4. 
We take the original prior density for Ll., 7ro(Ll.), to be a product of M0 independent 
gamma densities each with parameters (fi, Ui), i = 1, ... , Mo, which leads to 
Mo 
7ro(Ll.) oc IT 6t-l exp { -6igi} . 
i=l 
Again, we take a vague proper prior for 1r0(Ll.), and thus choose Ui very small. Again, 
in practice we recommend an iterative process for selecting a final set of (fi, Ui)· 
We start with a specific choice, such as fi equal to the ith subinterval width and Ui 
small. Then, we compute the prior model probabilities under several choices using 
the procedure discussed above to decide on a final set. We note here that we have 
allowed the construction of 1r0(Ll.) to depend on a partition, M0 , of the time axis 
which may be different from the partition used to construct 1r(Ll.). This flexibility 
is especially useful and more practical when Do is based on a previous study, since 
the data from the previous study may behave quite differently from the data in the 
current study. Whenever possible, we try to select M = M0 • 
Finally, once 7r0(p(m)) and 7ro(Ll.) are specified, we use Bayes theorem to update 
and obtain (17). We emphasize here that 7r0(p(m), Ll.) only serves as a device for 
generating a sensible set of prior model probabilities, and thus plays a minimal role 
otherwise. 
3. 
Computational Implementation 
In this section, we discuss the method for computing the posterior model proba-
bilities. The method involves computing the marginal distribution of the data via 
ratios of normalizing constants. The method requires posterior samples only from 
the full model for computing posterior probabilities for all possible models. The 
method is thus very efficient for variable selection. In addition, we devise a novel 
method for Gibbs sampling from the joint posterior distribution of the parameters 
by introducing latent variables and by showing that the required posterior densities 
are log-concave. 
3.1 
Computing the Marginal Distribution of the Data 
The posterior probability of model m (for the current study) is given by 
p(miD) = 
p(Dim) p(m) 
, 
LmeM p(Dim) p(m) 
(22) 

300 
Ibrahim & Chen 
where p(Dim) denotes the marginal distribution of the data given model m, and 
p( m) denotes the prior probability of model m given by (17). The marginal density 
p( Dim) corresponds precisely to the normalizing constant of the joint posterior 
density of (Ll., ;J(m)). That is, 
p(Dim) = I I L(;J(m), Ll.l D) 1r(;J(m)) 1r(Ll.) d;J(m) dLl.. 
(23) 
Recently, many methods have been developed for estimating normalizing constants 
of posterior distributions. These include Chen and Shao (1997a), Geyer (1994), Chib 
(1995), Meng and Wong (1996), and Gelman and Meng (1994, 1996). The basic idea 
in all of these methods is that samples from the posterior distribution are used to 
estimate the normalizing constant. Since it is required to simultaneously estimate all 
posterior model probabilities given in (22), the aforementioned methods are either 
computationally expensive or inapplicable. However, two recent methods developed 
by Chen and Shao (1997b, 1998) are very attractive and can be adapted to estimate 
all of the posterior model probabilities p(miD). 
Let f3 = (;31 , ;32 , .•. , f3k )' denote the regression coefficients for the full model and 
enumerate the models in M by m = 1, 2,.,., JC where JC is the dimension of M and 
model JC denotes the full model. We also write f3 = (;J(m)', ;J( -m)')' where ;J( -m) is 
f3 with p(m) deleted. Finally, we let p(/3, Ll.ID) denote the posterior distribution of 
the full model, that is, 
p(;J, Ll.ID) ex L(/3, Ll. I D) 1r(f3) 1r(Ll.) . 
Suppose that under the full model, we have a posterior sample {(f3cn, Ll.cn), j = 
1, ... , N}. We explain in detail in Section 3.2 how to obtain these posterior samples. 
Using the result given in Chen and Shao (1997b), we have the key identity 
p(Dim) 
p(DIJC) 
= 
(L(;J(m)' Ll. I D)7r(;J(m))7r(Ll.)w(;JC -m) j;J(m)' Ll.)) 
E 
L(;J, Ll. I D)7r({3)7r(Ll.) 
= 
(L(;J(m)' Ll. I D)7r(f3(m))w(f3( -m) I;J(m)' Ll.)) 
E 
L(;J, Ll. I D)7r(f3) 
' 
(24) 
where the expectation is taken with respect to the posterior density p(;J, Ll. ID) 
under the full model. Since Ll. does not depend on m, we get the reduction above. 
In (24), the weight function w(f3< -m) I;J(m), Ll.) is a completely known conditional 
density of ;3< -m) given (;J(m), Ll.). Chen and Shao (1997a) show that the best choice 
of w(;J( -m) I;J(m), Ll.) is the conditional density of pC -m) given (;J(m), Ll.) with respect 
to the full model posterior distribution p(;J, Ll.ID). Since the closed form expression 
of this conditional density is not available, we follow an empirical procedure provided 
by Chen (1994) to select a w(;JC -m) I;J(m), Ll.). Specifically, using the posterior sample 
{(f3(j) 1 Ll.cn),j = 1, ... , N}, we construct the posterior mean and covariance matrix, 
denoted by ce, t), and then we choose _w(;J< -m) lf3Cm), Ll.) to be the conditional 
density of the normal distribution No~c(;J,E- 1 ) for p(-m) given pCm). Since the 
dimension of f3 is often smaller than the dimension of Ll. for our problem, we may 
also simply choose w(;JC -m) I;J(m), Ll.) as the conditional density of ;J( -m) given ;J(m) 
with respect to the prior 1r(;J). Note that here we choose w independent of Ll. since 
the correlation between f3 and Ll. is small or negligible. 
We have the following Monte Carlo scheme to simultaneously estimate p( miD) 
form= 1, 2, ... , JC. It can be easily observed that using (24), (22) can be rewritten 

17. Bayesian Methods for Cox Model 
301 
as 
E (L(f3(m) .~ID)7r(/?(m))w~(-m) l/3(m) .~)) ( ) 
( ID) 
Lef3.~1D)7r /3) 
P m 
p m 
= '\"'K, 
E (L(f3(r),~jD)7r({3(r))w({3(-r)jf3(r),~)) ( ). 
L..tr=1 
L(f3,~1D)7r(f3) 
P r 
(25) 
Then, following the Monte Carlo method of Chen and Shao (1997b) and using (25) 
along with the posterior sample {(Pen,11en),j = 1, ... ,N}, the posterior probabil-
ity of model m can be estimated by 
p( miD) = 
(Le (r) ~ ID) e (r)) e(.j(-r),(.l(r) ~ ) ) 
' 
'\"'K, 
1 '\""':' 
f3 w ' (j) 
71" f3 (j) w 1-' (j) 
1-' (j) ' 
(j) 
( 
) 
L..tr=1 N L..tJ=1 
L(f3u>•~U>ID)7r(f3(i)) 
P r 
(26) 
( em)' 
e-m)')' 
for m = 1, 2, ... , IC, where Pen = Pen , Pej) 
. According to the discussion 
above, we use a normal distribution 
(27) 
to construct the weight density w(pe -m) IPem), 11) where Nok(P, :E-1) may be either 
taken to be the prior distribution 1r(P) or it can be obtained by using method 
of moments estimates based on the posterior sample {(Pen' 11en), j = 1, ... , N}. 
Therefore, in (26), w(pe -m) IPem), 11) can be calculated by 
()() 
~-
1. 
w(j3 -m l/3 m , A)= (211')-
2 
IE11.2ml- 2 
X exp { -~(/3(-m) 
it11.2m)'Eu~2m(/3(-m) 
it11.2m)}, 
(28) 
where 
-
-
-
--1 -, 
:E11.2m = :Eum- :E12m:E22m:E12m ' 
Eum is the covariance matrix from the marginal distribution of pe -m), E12m con-
sists of the covariances between pe -m) and pem), and E22m is the covariance matrix 
of the marginal distribution of pem) with respect to the joint normal distribution 
Nok(p,:E- 1) for the whole vector p. Also in (28), 
ii11.2m = pe -m) + E12mE2"lm(pem) - pem)), 
where pe -m) is the mean of the marginal distribution of pe -m) implied by (27) and 
jjem) is the mean of the marginal distribution of pem) implied by (27). Once w is 
evaluated, p(miD) in (26) can be computed. 
There are several advantages of the above Monte Carlo procedure. First, we need 
only one random draw from the posterior distribution for the full model, which will 
greatly ease the computational burden. Second, it will be more computationally 
efficient since there are a lot of common terms that cancel out in the ratios of 
the two densities. In particular, we see that 7r(11) completely cancels out in the 
calculation of p(miD). The cancellation of 7r(11) in (26) is especially attractive since 
the normalizing constant for 7r(11) under the EG process prior is typically difficult 
to obtain for an arbitrary value of A(s). Even for the simple case where A(s) = s, one 
needs to use the inversion formula for the Laplace transform for evaluating the closed 
form of the prior density of 11. Third, this procedure is almost automatic. Finally, 
note that in (26), p(p, 11ID) plays the role of the ratio importance sampling density 
(see Chen and Shao, 1997a) which needs to be known only up to a normalizing 
constant, since this constant is cancelled out in the calculation. 

302 
Ibrahim & Chen 
3.2 Sampling from the Posterior Distribution of (j3(m), ~) 
Here we describe in more detail the computational aspects for the Bayesian model 
discussed in Section 2. Our main objective is to sample from the joint posterior 
distribution of CB(m), Ll.), and once these samples are obtained, we use the Monte 
Carlo procedure presented in Section 3.1 to estimate p(miD). Using Do as the data, 
we use an almost identical procedure to estimate p(m). 
To obtain posterior samples with the EG process priors, we follow an algorithm 
similar to Laud, Smith, and Damien (1996). In their paper, they provide an algo-
rithm for sampling from the EG process, but they do not introduce covariates nor 
discuss proportional hazards models. We adapt their algorithm here for the propor-
tional hazards model given in Section 2.1 by using a Gibbs sampling procedure. We 
denote the distribution of a random vector X by [X]. For convenience, we order the 
observations so that the first di are exact (i .. e, failures) and the remaining Ci are 
right censored for a total of ni = di + Ci observations in the ith interval. 
To obtain samples from LB, Ll.ID], we describe a Gibbs sampling strategy for sam-
pling from [Ll.LB, D) and (PILl., D]. The posterior density of Ll.IP, Dis given by 
p(~I,B, D) oc {;Q exp (-h;(a; +b;))} 
x {fig (1- exp { -u;k(,B)T;(~)}) }{fi .-(5;)} , 
(29) 
where Uik(/1) = exp {xhP}, ai = L~i+ 1 '2:~~ 1 exp{xjkP}(sj-1- Si-d, and 
bi = L~i '2:~~~~~ 1 exp{xjkP}(sj- Si-d· Following, Laud et al. (1996), an efficient 
method of sampling from (29) is to define latent variables in order to make the 
components of Ll. independent a posteriori. We do this by first defining 
i= 1, ... ,M 
to be independent exponential random variables truncated at 1 with mean equal to 
(Ti(Ll.)uik(p))- 1 • Thus each eik has density 
Letting e = (e1, ... , eM), we can write the posterior distribution of [Ll. I p, e, D] as 
p(Ll.IP, e, D) 
oc 
{fi(T;(~))d'} { 
exp {- ~ ~ 
e;kT;(~)u;k(,B)}} 
x {fi exp {-h;(a; + b;)}} {;Q .-(h;)}. 
Next we consider additional latent variables 
i= 1, ... ,M, 
where qi are independent multinomials. Each qi is an i-cell multinomial of di inde-
pendent trials with probability of the kth cell defined to be Pk = 
Letting 

17. Bayesian Methods for Cox Model 
303 
q 
( q1, ... qM), we are led to 
p(di,B, e, q, D) 
ex: 
X 
(30) 
Equation (30) can be simplified further. Define wik(,B) = Uik(,B)eik and let Wi+(,B) = 
2:~~ 1 Wik(,B). Since 
we can write (30) as 
exp {-~ t. 6;w;+(f3)(s; - s;_t)} 
= g 
exp { -6; t. wk+(f3)(sk - Sk-tl} 
p(dlf3, e, q, D) ex {fi 6f~•q,,} 
X {fi exp { -6; (a; + b; + t. Wk+(f3)(sk - Sk-tl)}} {D. 1r( 6;)} . 
(31) 
We see that from (31) that given the latent variables (e, q), the posterior density of 
d consists of a product of the marginal posterior densities of the Di 's, thus implying 
independence. We use (31) in the Gibbs sampling scheme to sample d. 
The posterior density of ,Bid, D is given by 
p(f3id, D) ex {.fl exp { -6;(ad b;)}} 
x {.fl t1 (1- exp {-u;k(f3)1J(d)})} 1r(f3). 
(32) 
Therefore, to obtain samples from [,8, d I D], we use a Gibbs sampling scheme to 
sample from the following four distributions: 
a) 
[di,B, e, q, D] 
b) 
[ei,B, d, q, D] 
c) 
(qi,B, d, e, D] 
d) 
[,Bid, D] . 
As pointed out by Laud et al. (1996), the prior density of d is infinitely divisible, 
and thus the distribution in a) has the form of a gamma density times an infinitely 

304 
Ibrahim & Chen 
divisible density. Bondesson (1982) and Damien, Laud and Smith (1995) propose 
an algorithm for sampling from infinitely divisible distributions. Here, we use Bon-
desson's algorithm to sample from the prior density of d, and follow the same basic 
steps as Laud et al. (1996) to obtain samples from a). Thus to sample from a), we 
1. Simulate N independent exponential variables with parameter A* = a(si)-
a(si-d . 
2. Define an N-vector T = (x1, x1 + x2, x1 + x2 + X3, ... , x1 + ... + XN ), where 
Xi are i.i.d exponential random variables with parameter A*, i = 1, ... , N. 
3. Sample N independent random variables v1' ... ' VN with distribution pro-
portional to a(s) restricted to (si-1, Si]. Thus the Vi's have density of the 
form 
" ( ) -
r• 
a( u )du 
' 
{ 
a(s) 
Jv· S 
-
••-1 
' 
0 
Si-1 < S :$ Si 
otherwise 
4. Sample N independent exponential variables Zi, i = 1, ... , N where each Zi 
has an exponential distribution with parameter A(Vi) exp(Ti), i = 1, ... N. 
Note that A(Vi) is the scale parameter of the EG process prior. 
N 
5. Define X= E Zi. Then X,...., r(a(si), A(si)). 
i=1 
6. Given X, we use a rejection algorithm with the gamma density as the envelope 
to obtain a sample from the distribution in a). The gamma density used in 
the rejection algorithm proportional to 
which has mode equal to 
M 
Now, we use the gamma envelope in (33) along with the mode m in a standard 
rejection algorithm to decide upon acceptance or rejection of X from [di,B, e, q, D]. 
Distributions b) and c) are quite straightforward to sample from since they corre-
spond to truncated exponential and multinomial distributions, respectively. Specif-
ically, 
{ 
M 
d· 
} { M 
d· 
} 
p(eiiJ,d,q,D) <X exp 
-tt~eikUik(iJ)T;(d) 
g[\I(e;k:,;l) 
, 
where I(.) is the indicator function and 
M 
p(qi,B, d, e, D) ex: II ot~l ql.a 
i=1 

17. Bayesian Methods for Cox Model 
305 
Cycling through a), b), and c) via Gibbs will yield samples from [.6.LB, D]. Once a 
sample of .6. is obtained from [.6.j,B, D], we complete the Gibbs cycle by sampling 
from [,Bj.6., D]. To obtain a sample ,B from this distribution, we first observe that 
[,81.6., D] is log-concave in each component of ,B. Therefore, we may directly use the 
algorithm of Gilks and Wild (1992) to sample from this posterior distribution. To 
show that [,81.6., D] is log-concave in each component of ,B, it suffices to show that 
82logp(,BI.6., D) 
0 
a,a; 
:::; 
for all r == 1, ... , k. Letting Aik(,B, .6.) == Uik(,8)1i(.6.), Bik(,B, .6.) 
1-exp { -Aik(,B, .6.)}, 
and Cik(,B, .6.) 
1- Aik(,B, .6.)- exp { -Aik(,B, .6.)} we get 
82log(p(,BI.6., D)) 
a ,a; 
M 
di 
:= L E { 
xlkr Aik(,B, .6.) Bik2(,B, .6.) exp { -Aik(,B, .6.)} cik(,B, .6.)} 
i=l k=l 
(34) 
M 
M 
dj 
L L L { 
Di xJkr exp{xjk,B}(sj-1 -Si-d} 
(35) 
i=l i=i+l k:::l 
M 
M 
dj+Cj 
2 
1 
()2 log 1r(,B) 
t;?; k~+l { 6; "'ikr exp{ "';kiJ}(s; - s;_!)} + 
&/1~ 
. 
(36) 
We first note that since we are using a normal prior for ,B, it is well known that 
821
~~;(§) < 0. Second, we clearly see that (35) and (36) are negative. Thus, to show 
that 
82
Iog(~~;~,D)) :::; 0, it is enough to show that (34) is negative. It suffices to 
show that Cik(,B, .6.) in (34) is negative, since all of the other terms in the summand 
of (34) are positive. We see that Cik(,B,.6.) is of the form f(x) == 1- ex- e-ell). 
Clearly, when x > 0 , f(x) < 0. For x E ( -oo, 0), we see that f(x) is a monotonic 
decreasing function and limx-.-oof(x) == 0. Thus f(x) < 0 for all x E R1, and thus 
Cik(,B, .6.) $ 0 for all (,B, .6.). Thus (,81.6., D) is log-concave in each component of ,B. 
4. 
Example: Simulation Study 
We consider a simulation to illustrate the methodology presented in Sections 2 
and 3. Our main goal in this example is to investigate the behavior of the prior and 
posterior model probability structures using various choices of prior parameters. We 
generate survival times ti, i == 1, ... , n, from a mixture of Weibull densities taking 
the form 
(37) 
where 
0 :::; 1/; :::; 1, n == 500, Ai == exp(2xn + 1.5xi2) and the Xi 
(xn, Xi2)' are independent 
bivariate normal random vectors each with mean J.L 
( 1, 1 )' and covariance matrix 

306 
Ibrahim & Chen 
E = ( .i7 
·~1
7 ) . The off-diagonal elements of E have been chosen so that the 
correlation between Xi1 and Xi2 is .5. In addition, 10% of the observations were 
randomly right censored. Two additional covariates (xis, Xi4) are independently 
randomly generated, each having a normal distribution with mean 1 and variance 
.25. Further, Xis and Xt4 are independent. Notice that It (ti) has a decreasing hazard 
function while h(ti) has an increasing hazard function. The full model (k = 4) thus 
contains four covariates (x 1 , ••• , x4 ), and the true model contains the covariates 
(x1, x2). To obtain the prior prediction y0 , we generate a new set of covariates 
that have the same distribution as the set corresponding to the ti's. Then, using 
this new set of covariates, we generate another independent set of 500 survival 
times, denoted Wi, from model (37). For illustrative purposes, we consider the prior 
prediction yo = (Yo1 , ... , Yon)' to be a perturbation of w = ( w1, ... , Wn )', taking 
the form Yoi = Wi + lo fr Zi, i = 1, ... , n where lo is a scalar multiple, u is the 
standard deviation of the Wi 's, and the Zi are i.i.d. truncated standard normal 
random variates. Once Yo is specified, the formulas given in (12) and (13) are used to 
obtain the prior mean and precision matrix for 1r(/3(m)). For 7ro(f3(m)), Wo,m is taken 
as a diagonal matrix with the same diagonal elements as Tm in (13). Several choices 
of lo, co, and ( r1, r2), where r1 = ab and r2 = ab114 , are considered to examine the 
behavior of the prior and posterior model probabilities. Since an intercept is not 
included in the model, we have a total of K = 15 models in M. 
First, we fix co= .1 and (rt, r2) = (.1, .15). Table 17.1 gives the largest prior and 
posterior model probabilities based on different values of 1/J and lo. In Table 17.1, 
we use the following priors for 1r(.6.) and 1r0(.6.). For 1r(.6.), we take M = 60, with 
each 6i 
I'V G(si - Si-1, .1), i = 1, ... , M. For 7ro(.6.), we take Mo = 60 with each 
6i I'V G(si- Si-1, .001). We choose the subintervals (si-1, si] to have equal numbers 
of failure or censored observations, i.e., Si is chosen to be the ( i/ M)th quantile of 
the ti 's. The prior model probabilities appear to be most sensitive to the quality of 
the prior prediction, that is, the choice of 10 for each ¢. 
Table 17.1. The Largest Prior and Posterior Model Probabilities 
for Various Values of 1/J and lo 
0.0 
0.5 
1.0 
lo 
.01 
3.0 
.01 
3.0 
.01 
3.0 
model 
p( m) 
model 
(x1,x2) 
.62 
(x1,x2) 
(x1,x2) 
.31 
(x1,x2) 
(X 1 , X 2) 
. 54 
(X 1 , X 2) 
(xt) 
.27 
(x1,x2) 
(x1, x2) 
.65 
(x1, x2) 
(xt) 
.18 
(x1, x2) 
p(m!D) 
.74 
.49 
.67 
.43 
.71 
.41 
Table 17.1 shows that true model obtains the largest posterior probability regardless 
of lo or ¢. However, the posterior probabilities tend to decrease with poorer predic-
tions (i.e., larger 10 ). For 1/J = 1 and lo = 3, the model with highest prior probability 
is the x1 model, which has a posterior probability of nearly 0, while the true model, 
(x1, x2), has the second largest prior probability with a value of .14. Similar results 
are obtained for 1/J = 0.5 and lo = 3. Thus it appears that the prior model proba-
bilities are somewhat sensitive to choice of lo, although one needs a fairly large lo 
to obtain a model other than the true model with the largest prior probability. We 
also note that the posterior model probabilities are not sensitive to the increasing 
hazard rate assumption implied by the EG process prior. When 1/J = .5, we have a 

17. Bayesian Methods for Cox Model 
307 
non-monotonic hazard rate and the model with the largest posterior probability is 
(x1, x2) with value .67. The case ¢ = 1.0 corresponds to a decreasing hazard rate, 
and still yields the (x1, x2) model with the largest posterior probability of .71. 
We also calculated the simulation standard errors of the estimated posterior model 
probabilities. For all posterior model probabilities in Table 17.1, the simulation 
standard errors are between 0.01 and 0.04 with a simulation sample size of 5, 000. A 
similar magnitude of standard errors was obtained for all of the remaining posterior 
model probabilities as well as for the other calculations below, and thus are not 
reported in the chapter. 
Second, we fix 1/; = 0 and vary lo and co. We use lo = .01 (i.e., a good prior 
prediction), and c0 = .1. We also consider three different partitioning schemes for 
the time axis, that is, choices of the subintervals (si-1, si]. We choose the subin-
tervals ( Bi-1, Si] with (i) equal numbers of failures or censored observations; (ii) 
approximately equal lengths subject to the restriction that at least one failure 
or censored observation occurs in each interval; (iii) decreasing numbers of fail-
ures or censored observations. More specifically, in case (iii) we took Si to be the 
((1- e( -j/M))/(1- e( -1)))th quantile of the ti's. For all three partition schemes, 
the true model, ( x1, x2) obtains the largest prior and posterior model probabilities. 
The largest prior probabilities are .62, .61, .61 and the largest posterior model prob-
abilities are .74, .75, .72 for partition schemes (i), (ii), and (iii), respectively. Thus, 
the prior and posterior model probabilities do not appear to be too sensitive to the 
choices of the subintervals ( Si-1, si]. This is a comforting feature of our approach 
since it allows the investigator some flexibility in choosing the subintervals. For the 
remaining calculations, we will use the subintervals ( Bi-1, si] with equal numbers of 
failure or censored observations. 
The third sensitivity analysis we conduct involves fixing 1/; = 0, lo = .01, co = 1, 
and varying (r1, r2). These results are summarized in Table 17.2. 
Table 17 .2. The Largest Prior and Posterior Model Probabilities 
for Various Choices of ( r1, r2) 
with Fixed 1/; = 0, lo = 0.01, and co = 1 
(.1,.15) 
(.1, .9) 
(.5, .9) 
model 
(x1, x2) 
(xb x2) 
(x1, x2) 
p(m) 
.62 
.73 
.99 
model 
(x1,x2) 
(x1,x2) 
(x1,x2) 
p(miD) 
.70 
.78 
1.0 
For each ( r1, r2) given in Table 17 .2, the model with the largest prior and posterior 
probability is the (x1, x2) model. We see that the prior model probability increases 
for the (x1, x2) model as r1 and r2 become larger in magnitude, as seen by (r1, r2) = 
(0.5, 0.9). Using co = 0.1 instead of co = 1, we obtain results very similar to those in 
Table 17 .2. The prior model probabilities are not very sensitive to changes in c0 for 
this fixed value of lo = .01. Finally, when we use a poor prior prediction, the wrong 
model can obtain the largest prior and posterior probability. For example, when 
1/; = 0, lo = 8, co= 5, and (r1, r2) = (.10, .50), the model with the largest posterior 
probability is x1, which has a prior probability of .13 and a posterior probability 
of .36. In this case, the model with the second largest posterior probability is the 
(x1, xa) model with prior probability of 0.07 and posterior probability of .24. The 
true model, (x1, x2), has the second largest prior probability of .13 and the fourth 
largest posterior probability of .10. 

308 
Ibrahim & Chen 
Next, we perform a sensitivity analysis for the prior parameters of 1r(d). We fix 
1/; = 0, lo = 0.01, co= .1, (r1, r2):::: (.1, .15). The prior and posterior model prob-
abilities were fairly robust when the shape parameter was kept fixed and the scale 
parameter was varied. Varying the scale parameter from .1 to .001 resulted in very 
little change in the posterior model probabilities. Changing the scale to Ai = .001, 
the model with the largest posterior probability is ( x1, x2) with prior probability of 
.62 and posterior probability of . 73. Thus, as long as the shape parameter remains 
fixed, the results are not too sensitive to changes in the scale parameter. However, 
when we fix the scale parameter and vary the shape parameter, the results are 
sensitive. For example with 1/; :::: 0, lo = .01, co = .1, (r1, r2) :::: (.1, .15), with a 
scale of Ai = .1 and a shape parameter of ai = 1 for the ith interval, the (xb x2) 
model obtains the largest prior probability (.62) but the full model (xt, x2, X3, x4) 
obtains the largest posterior probability (.99). However, if ai = Si- Si-b the model 
with largest posterior probability is (x1, x2) with a value of .74. Further, when both 
the shape and the scale are changed such that their product remains constant, the 
results are less sensitive. For example, when 1/; :::: 0, lo = 1, (rb r2) = (.1, .15), 
and ai = Ai = ( .1( Si -
Si-!) ) 112 , the model with the largest posterior probabil-
ity is ( x1, x2) with a value of .90. Similar phenomena were observed when doing a 
sensitivity analysis on 7ro(d). 
Finally, for 1/; = 0, lo = 0.01 and co = .1, we computed the posterior modes of 
the regression coefficients for the full model. These modes, along with the partial 
maximum likelihood estimates obtained by the PHREG procedure in SAS, are pre-
sented in Table 17.3. We mention that the PHREG procedure performs regression 
analysis of survival data based on Cox's partial likelihood. From Table 17 .3, it can 
be seen that the Bayesian and non-Bayesian estimates are similar. 
We have implemented the Gibbs sampler using the algorithm given in Section 
3.2. The convergence of the Gibbs sampler was checked using several diagnostic 
procedures as recommended by Cowles and Carlin (1996). We ran 10 multiple chains 
with dispersed initial values and computed potential scale reductions (PSR's). PSR 
values close to 1 are indicative of convergence of the Markov chain to the target 
distribution (See Gelman and Rubin, 1992). We also computed the autocorrelation 
coefficients within chains to ascertain that it is "rapidly mixing" as discussed by 
Geyer (1992). For the full model, at 500 iterations, the PSR's (the 97.5 percentiles) 
are 1.03 (1.06), 1.06 (1.11), 1.01 (1.02), and 1.02 (1.03) for {31, ..• , !34 respectively. We 
also found that the autocorrelations virtually disappear at lag 10 for all parameters. 
Therefore, the Gibbs sampler practically converges at 500 iterations. Since sampling 
from the posterior distribution is much cheaper than calculating posterior model 
probabilities, every lOth Gibbs iterate after convergence was used for estimation. 
50,500 Gibbs iterates produced an estimation sample of size 5,000. Finally, we used' 
the algorithm given in Section 3.1 to obtain the prior and posterior probabilities 
using samples from every lOth Gibbs iterate. 
Table 17 .3. Estimates of Regression Coefficients with Standard Errors 
in Parentheses 
Parameter 
Bayesian Estimate 
1.926 (.140) 
1.550 (.182) 
0.021 
( .086) 
0.046 
(.088) 
SAS Output 
1.959 (.145) 
1.602 
(.196) 
0.008 
( .090) 
0.045 (.090) 

17. Bayesian Methods for Cox Model 
309 
5. 
Discussion 
We have developed a semi-automatic method of doing Bayesian variable selection 
with informative priors for proportional hazards models. The computational algo-
rithms discussed in Section 3 have proved to be reasonable to implement for small 
to moderate variable selection problems, say of the order 10-15 covariates. These 
methods have not been implemented for problems with more than 15 covariates. 
We note here, as in any variable subset selection technique, we view our procedure 
as a screening process for identifying a small set of reasonable models, which then 
can be investigated further. As is well known, there is rarely a single best model 
which describes a set of data. An important direction for future research is on elic-
itation of the prior parameters of the EG process prior. More sophisticated choices 
of (a( s), A( s)) which depend on Yo are currently being investigated. The fruitful 
results obtained here have opened the door for further research of this issue. 
Due to the inherent computational complexity of variable selection with the semi-
parametric proportional hazards model, a practical, tractable, and easy to interpret 
prior specification should be given in a closed form in which the user is able to easily 
specify a prior mean and covariance matrix. The priors given in (9) and (10) have 
been developed with this purpose. Having a prior that does not have a closed form, 
such as that given by (15), adds greatly to the computational complexity for this 
model and moves beyond the scope of the algorithms developed in Section 3. Thus, 
the analytical tractability of the prior is of practical and computational importance 
to us. Theoretical and computational properties of the prior in (15) is the subject 
of ongoing research. 
References 
Bondesson, L. (1982). On simulation from infinitely divisible distributions. Ad-
vances in Applied Probability 14, 855-69. 
Chen. M.-H. (1994).lmportance-weighted marginal Bayesian posterior density esti-
mation. J. Am. Statist. Assoc. 89, 818-24. 
Chen, M.-H. and Shao, Q.-M. (1997a). On Monte Carlo methods for estimating 
ratios of normalizing constants. Ann. Statist. 25, 1563-1594. 
Chen, M.-H. and Shao, Q.-M. (1997b ). Estimating ratios of normalizing constants 
for densities with different dimensions. Statistica Sinica 7, 607-630. 
Chen, M.-H. and Shao, Q.-M. (1998). Monte Carlo methods on Bayesian analysis 
of constrained parameter problems. Biometrika 85, to appear. 
Chib, S. (1995). Marginal likelihood from the Gibbs output. J. Am. Statist. Assoc. 
90, 1313-21. 
Clayton, D. G. (1991). A Monte Carlo method for Bayesian inference in frailty 
models. Biometrics 64, 141-51. 
Cowles, M. K. and Carlin, B. P. (1996). Markov chain Monte Carlo convergence 
diagnostics: a comparative review. J. Am. Statist. Assoc. 91, 883-904. 
Cox, D. R. (1972). Regression models and life tables, (with discussion). J. R. 
Statist. Soc., B 34, 187-220. 

310 
Ibrahim & Chen 
Cox, D. R. (1975). Partial likelihood. Biometrika 62, 269-76. 
Damien, P., Laud, P. W. and Smith, A. F. M. (1995). Approximate random variate 
generation from infinitely divisible distributions. J. R. Statist. Soc. B 57, 547-
564. 
Dykstra, R. L. and Laud, P. W. (1981). A Bayesian nonparametric approach to 
reliability. Ann. Statist. 9, 356-67. 
Geisser, S. (1993). Predictive inference. New York: Chapman and Hall. 
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to calcu-
lating marginal densities. J. Am. Statist. Assoc. 85, 398-409. 
Gelman, A. and Meng, X.-L. (1994). Path sampling for computing normalizing 
constants: identities and theory. Technical Report 377, Department of Statis-
tics, The University of Chicago. 
Gelman, A. and Meng, X.-L. (1996). Simulating normalizing constants: from im-
portance sampling to bridge sampling to path sampling. Technical Report 44 0, 
Department of Statistics, The University of Chicago. 
Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using 
multiple sequences. Statistical Science 7, 457-511. 
George, I. E., McCulloch, R. E. and Tsay, R. S. (1995). Two approaches to Bayesian 
model selections with applications. in Bayesian Analysis in Econometrics 
and Statistics - Essays in Honor of Arnold Zellner, eds. D. A. Berry, K. A. 
Chaloner and J. K. Geweke, 339-48. 
Geyer, C. J. (1992). Practical Markov chain Monte Carlo (with discussion). Stat-
istical Science 7, 473-511. 
Geyer, C. J. (199~). Estimating normalizing constants and reweighting mixtures 
in Markov chain Monte Carlo. Revision of Technical Report No. 568, School 
of Statistics, University of Minnesota. 
Gilks, W. R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Appl. Statist. 41, 337-48. 
Gray, R. J. (1994). A Bayesian analysis of institutional effects in a multicenter 
cancer clinical trial. Biometrics 50, 244-53. 
Ibrahim, J. G. and Chen, M-H (1998). Prior distributions and Bayesian computa-
tion for proportional hazards models. Sankhya, B 60, 48-64. 
Ibrahim, J. G. and Laud, P. W. (1994). A predictive approach to the analysis of 
designed experiments. J. Am. Statist. Assoc. 89, 309-319. 
Kalbfleisch, J. D. (1978). Non-parametric Bayesian analysis of survival time data. 
J. R. Statist. Soc. B 40, 214-221. 
Kuo, L. and Smith, A. F. M. (1992). Bayesian computations in survival models 
via the Gibbs sampler. in Survival Analysis: State of the Art, eds. J. P. Klein 
and P. K. Goel, 11-24. 

17. Bayesian Methods for Cox Model 
311 
Laud, P. W. and Ibrahim, J. G. (1995). Predictive model selection. J. R. Statist. 
Soc. B 57, 247-62. 
Laud, P. W. and Ibrahim, J. G. (1996). Predictive specification of prior model 
probabilities in variable selection. Biometrika, 83, 267-274. 
Laud, P. W., Smith, A. F. M. and Damien, P. (1996). Monte Carlo methods for 
approximating a posterior hazard rate process. Statistics and Computing, 6, 
77-83. 
Meng, X. L. and Wong, W. H. (1996). Simulating ratios of normalizing constants 
via a simple identity: a theoretical exploration. Statistica Sinica 6, 831-60. 
Padgett, W. J. and Wei, L. J. ( 1980). Maximum likelihood estimation of a distri-
bution function with increasing failure rate based on censored observations. 
Biometrika 67, 470-74. 
Raftery, A. E., Madigan, D. and Volinsky, C. T. (1995). Accounting for model 
uncertainty in survival analysis improves predictive performance. Bayesian 
Statistics 5, Eds. J. M Bernardo, J. 0. Berger, A. P. Dawid and A. F. M. 
Smith, 323-50. 
Skene, A. M. and Wakefield, J. C. (1990). Hierarchical models for multi-centre 
binary response studies. Statistics in Medicine 9, 919-29. 
Tsiatis, A. A. (1981). A large sample study of Cox's regression model. Ann. Statist. 
9, 93-108. 
Zellner, A. (1986). On assessing prior distributions and Bayesian regression analysis 
with g-prior distributions. in Studies in Bayesian Econometrics and Statistics, 
eds. P. K. Goel and A. Zellner, New York: Elsevier, 233-43. 

This Page Intentionally Left Blank 

18 
Bayesian Model Diagnostics for 
Correlated Binary Data 
Dipak K. Dey 
M1ng-Hui Chen 
ABSTRACT Bayesian methods are considered for the analysis of correlated binary 
data when each binary observation may have its own covariates. Several models, 
including different versions of logistic regression, multivariate pro bit and Student's 
t links are considered. Fully parametric classical approaches to these are intractable 
and thus Bayesian methods are pursued using sampling based approach through 
Markov chain Monte Carlo method. Several model diagnostics using simulation based 
approach are developed and implemented for model adequacy. the proposed method-
ology is implemented to study the voting behavior of residents of Troy, Michigan. 
1. 
Introduction 
In this chapter we consider an exact small sample Bayesian analysis of the mod-
els proposed by Prentice (1988) and also by Ochi and Prentice (1984). Our models 
can be classified into two groups. The first group of models generalize the binary 
logistic model to multivariate data by considering a particular parameterized rep-
resentation for the correlations using the notion of random effects on the logistic 
regression structure or in terms of pairwise odds ratios. The other group of models 
are obtained by introducing a link function using an inverse cumulative distribution 
function ( cdf). Multivariate probit (MVP) and multivariate t-link (MVT) models 
are obtained in this scenario. The MVP model was first introduced by Ashford and 
Sowden (1970) and studied further by Amemiya (1985). Recently, by introducing 
latent variables Chib and Greenberg (1998) analyzed the MVP model in a Bayesian 
framework. 
The objective of this chapter is to explore different modeling strategies for the 
analysis of correlated binary data in a Bayesian perspective. As one entertains a 
collection of such models for a given data set one needs to address the problem 
of model adequacy. Specifically, we adopt the Markov chain Monte Carlo (MCMC) 
framework (e.g., Gelfand and Smith, 1990 and Tierney, 1994) to simulate the poste-
rior distribution for proposed models. Instead of just taking formal Bayesian model 
adequacy criterion (as in Box, 1980), our approach is based on exploratory data 
analysis methods (e.g., Gelfand, Dey and Chang, 1992 and Dey, Gelfand, Swartz 
and Vlachos, 1995). We develop different diagnostic measures suitable for the binary 
data and apply them to model adequacy. 
The remainder of the chapter is organized as follows. In Section 2, we consider 
different models and discuss how the likelihood is obtained for each model. Section 3 
is devoted to the development of the prior distributions and the distribution theory 
involved for the posterior calculations. In Section 4, we discuss different methods 
involved in checking model adequacy. Section 5 is devoted to the application of our 
proposed methodologies to survey data on the voting behavior of residents of Troy, 
Michigan. Finally, Section 6 gives brief concluding remarks. 
313 

314 
Dey & Chen 
2. 
The Models 
We first introduce some notations which will be used throughout the chapter. 
Suppose that we observe a binary (0-1) response Yij on the ith observations and 
jth variable and let Xij = (Xijt, Xij 2, ... , Xijpi) be the corresponding Prdimensional 
row regression vector for i = 1, 2, ... , n and j = 1, 2, ... , J. (Note that Xijl may 
be 1, which corresponds to an intercept.) Denote Yi = (Yit, Yi2, ... , YiJ )' and as-
sume that Yi1, Yi2, ... , YiJ are dependent and Y1, Y2, ... , Yn are independent. Let 
Yi = (Yit, Yi2, ..• , YiJ )' and y = (Yl, Y2, ... , Yn) be the observed data. Also corre-
sponding to Xj, suppose {3j = (f3jl, {3j2, ... , f3iPi)' is a Prdimensional column vector 
of regression coefficients for j = 1, 2, ... , J and (3 = (f3i, (3~, ... , (3j )'. 
Now we consider three different types of models for the above correlated binary 
data. The first two models, which are variants of random effects models, implicitly 
assume equicorrelation structure whereas the third model is based on latent vari-
ables where the correlation structure has more flexibility. There are other plausible 
models in the literature to incorporate correlation structure which are not pursued 
in this chapter. 
2.1 
Stratified and Mixture Models 
In the spirit of the stratified and mixture models considered in Prentice (1988), 
we consider the following random effects binary logistic regression model for Yii 
given Xi as 
P(v .. _ 
. ·I . {3· 
··) _ exp{(ai + Xijf3j )Yij} 
.l1.J -
YsJ az, J, XzJ 
-
1 + 
{ + 
(3 } , 
exp ai 
Xij j 
(1) 
where ai denotes random effects on the ith observation, which captures the corre-
lation among Yib Yi2, ... , YiJ. We take 
The above model is also known in the literature as a logit normal model. 
2.2 Conditional Models 
In certain studies there are natural ordering of the individuals within the block 
and the regression coefficients in the model may characterize dependencies of in-
terest. Our second model is a symmetric conditional random effects binary logistic 
regression model (e.g., Prentice 1988), which is defined as 
= 
P(Yi1 = Yil, Yi2 = Yi2, ... , YiJ = YiJ lai, (3, X it,· .. , XiJ) 
exp{l:j~~
1 ai(j) + L:f=l Xijf3jYij} 
J 
J 
(2) 
where Yi = LYii and iJi = LYij. In (2), ai(j) is chosen to be a polynomial in j. 
j=l 
j=l 
For example, the simplest choice of ai(j) = ai gives 

18. Bayesian Model Diagnostics for Correlated Binary Data 
315 
= 
(3) 
where ai i.!vd· N(O, u~). 
2.3 Multivariate Probit Models 
Here, we consider multivariate probit (MVP) models; see also Chib and Greenberg 
(1998) for the Bayesian analysis of such models. 
We introduce a J -dimensional latent variable Wi = ( Wi1, Wi2, ... , WiJ )' so that 
Yi . _ { 1 if Wij > 0 
ZJ -
0 if Wij ~ 0 
(4) 
and 
(5) 
where Xi= diag(xil, Xi2, ... , XiJ). In (5), we take E = (PJJ*)JxJ to be a correlation 
matrix such that Pii = 1 to ensure the identifiability of parameters. See Chib and 
Greenberg for the detailed discussions. The distribution of Wi determines the joint 
distribution of Yi through ( 4) and the correlation matrix E captures the correlations 
among the YiJ 's. More specifically, we have 
where 
fori=1,2, ... ,n. 
Aij = { ( -oo' 0] 
(O,oo) 
2.4 
Multivariate t-Link Models 
if YiJ = 0 
if YiJ = 1 
(6) 
(7) 
A generalization of multivariate probit models is multivariate t-link models 
(MVT). Similar to the MVP models, we introduce a J-dimensionallatent variable 
Wi = ( Wil, Wi2, ... , WiJ )' SO that 
Y,.. _ { 1 if Wij > 0 
ZJ -
0 if Wij ~ 0 
and 
Wi 
"'"' 
tv(xi(3, E) 
with probability density function 
1r(wilv, Xi/3, E) 
= 
rU<v+J)) 
( 
-1< 
)' -1( 
))-~<v+J) 
(1rv)(1/Z)Jf Uv) IEjl/Z 1 + v 
Wi- Xi/3 E 
Wi- Xi/3 
• 
(8) 
(9) 

316 
Dey & Chen 
(Note that one special case of (9) with v = 1 is termed as a multivariate Cauchy 
distribution and another special case of (9) with v-+ oo is the multivariate normal 
distribution (5).) Thus, the joint distribution of Yi is given by 
(10) 
where 1r( wdv, xi(3, E) and Aij are given in (9) and (7) respectively. For the ease 
of model complexity, we consider only fixed v so that we can investigate different 
multivariate t-link models. A discrete prior on v can be considered. However, this 
will bring an additional computational burden. 
3. 
The Prior Distributions and Posterior Computations 
In this section we present prior distributions for various models and develop 
algorithms to perform posterior computations for such models. 
3.1 
Prior Distributions 
First, we choose the same prior distribution for the regression coefficient vector 
f3 for all four models presented in Section 2. That is, 
(11) 
where Bo is a precision matrix, (30 is a location parameter vector, and both (30 and 
Bo are pre-specified. Typically, we choose (30 = 0 and 
where Bjl is chosen to be small (e.g., Bjz = 0.01) so that a vague prior distribution 
for (3 is obtained, which ensures that the posterior is driven by the data. 
Second, for the stratified and mixture models and the conditional models, we 
choose 
u~ f'V I g (a, b) , 
where the density of the I{}( a, b) is 
1r( u~ Ia, b) oc ( u~ )-(a+l)e-bfa~' 
(12) 
and a and bare chosen so that E(u~) is small (e.g., 0.001) and Var(u~) is large 
(e.g., 100) to ensure that the prior does not play a major role in the posterior. 
Finally, for the multivariate probit and t-link models we denote 
Then, analogous to Chib and Greenberg (1998), we choose 
7r(vec*(E)IEo, Go) 
ex 
exp { -~(vec'(E)- vec'(Eo))'Go(vec'(E)- vec'(Eo))} 
(13) 

18. Bayesian Model Diagnostics for Correlated Binary Data 
317 
for vee* (E) E A where Eo is a J x J correlation matrix with all diagonal elements 
equal to one, Go is a (J(J- 1)/2) x (J(J- 1)/2) precision matrix, and the region 
A is a subset of the region [-1, 1]i(J-l)/2 that leads to a proper correlation matrix. 
As mentioned in Chib and Greenberg (1998) and also shown by Rousseeuw and 
Molenberghs (1994), the region A forms a convex solid body in the hypercube 
[-1, 1]i(J- 1)12. Note that both hyper parameters Eo and Go are to be specified. 
The simplest choices of Eo and Go are Eo= lJ and Go= IJ(J- 1);2 , which are the 
J and J(J- 1)/2 dimensional identity matrices. 
3.2 Posterior Computations 
We use Gibbs sampling (e.g, Geman and Geman, 1984 and Gelfand and 
Smith, 1990) to perform the posterior computation. We present necessary steps 
needed to perform the Gibbs sampling algorithms for all four models considered in 
Section 2 in turn. 
Stratified and Mixture Models 
To run the Gibbs sampler for the stratified and mixture models, we take all 
parameters from their respective conditionals as follows: 
(a) For the ith random effect, let 1r( ai l/3, yi) be the conditional density for ai 
given the data and /3. Then, 
J 
( ·l/3 ·) rr exp{ ( ai + Xij /3j )Yij} 
7r az 
, Yz 
ex: 
{ 
} • 
i=l 1 + exp ai + Xij/3j 
(14) 
Since 
(15) 
7r(ail/3, Yi) is log-concave. Therefore, we can use the adaptive rejection sam-
pling algorithm of Gilks and Wild (1992) to generate ai fori= 1, 2, ... , n. 
(b) For the variance of the random effects ai, we take 
(16) 
where a = ( a1, ... , an)'. 
(c) For the regression coefficients /3jz's, let 7r(/31a, y) be the conditional density 
for f3 given the data and random effects a. Assuming that we choose f3o = 0 
and B0 is a diagonal precision matrix, then 
Similar to (14), it can be shown that 7r(f31a, y) is log-concave in each com-
ponent of f3 and, moreover, it can be shown that 7r(/31a, y) is log-concave 
in the whole vector of {3, i.e., the second derivative matrix ( ~;;,<G!;;~]) is 
non-positive definite and, therefore, we can again use the adaptive rejection 
sampling algorithm of Gilks and Wild to generate each component of {3. 

318 
Dey &Chen 
Conditional Models 
For the sake of simplicity, we consider the conditional models given in (3). The 
conditional density for ai is 
( 
1{3 
) 
exp{adJi+~f= 1 Xij/3jYij} 
11' a i , Yi 
ex: 
1 
1 
1 
• * 
J 
* · 
~Yi1 =O ~Yi2 =0 · · · ~YiJ=O exp{ C¥iYi + ~j=1 Xij/3j Yij} 
Similar to (15), it can be shown that 1r(aijf3, Yi) is log-concave; therefore, we again 
use the algorithm of Gilks and Wild to generate ai. The conditional density for 
u~ is the same as that given in (16); henceforth, it is trivial to sample u~ for its 
conditional distribution. Using the same prior for f3 as in the stratified and mixture 
models, the conditional density for f3 is 
ex: 
Then, 
82 log 11'(,8la, y) 
a ,a;, 
1r(f31a, y) 
[ft 
exp{aaji + ~f= 1 Xij/3jYiJ} 
] 
i=1 ~!:1 =O ~!:2=o · · · ~!:J=o exp{ aaji + ~f 
=1 Xij /3j Yij} 
x [llfi exp{-~B;,/1]!}1 
= _ 2: L;yt1 =O L;yt2=o · · · L;yiJ=o Xij!Y ij exp O'iYi 
L;j•=1 Xij 
J .• Yii* 
n ["'1 
"'1 
"'1 
2 
>~<2 
{ • * + "'J 
,8 
* } 
i=1 
L:~t1 =0 L:~t2 =o '' 'L:~tJ=O exp{ O'i!ii + L:f*=1 Xij ,8j• Yij•} 
( L:~t1 :::0 L:~i2=o ''' L:~iJ=O Xij!Y* iJ exp{ aifJi + L:f• =1 Xij ,83 • Yij•}) 
2
] 
= 
-
( 2::~:1 =o L:~i2 =o · .. L:~:J=o exp{ ad1i + L:f. =l Xij .Bi• Yi1··}) 
2 
-B3·!· 
Using the Cauchy-Schwarz inequality, we have 
( 
1 
1 
1 
J 
)2 
L L ... L XijlY*iiexp{aaji+ LXij/3J•Yij•} 
Yi1 =0 Yi2 =0 
YiJ=O 
j*=1 
1 
1 
1 
J 
::; L L .. · L x~jlY*~j exp{aiyi + L Xij/3j•Yij•} 
Yi1 :0 Yi2=0 
YiJ=O 
j*=1 
1 
1 
1 
J 
·I': I': .. · I': exp{a:iYi +I': Xij/3j•Yij•}· 
j*=1 
Thus, 7r(f31a, y) is log-concave in each component of {3; therefore, again the algorithm 
of Gilks and Wild can be used to generate each component of {3. 
Multivariate Probit Models 
To run the Gibbs sampler for the multivariate models, we need to sample {3, Wi, 
and E from their respective conditional distributions. Let /3 = B- 1 (Bof3o + ~7= 1 x~ 
E- 1wi):and B = Bo + ~7= 1 x~E-
1 xi. Then, given the Wi and E, we have 

18. Bayesian Model Diagnostics for Correlated Binary Data 
319 
From (6), it can be seen that the full conditional distribution of Wi is multivariate 
normal truncated to a region determined by Yi. More specifically, 
7r(Wi I /3, E, Yi) 
J 
ex IT [1{w;i>o}1{Yii=l} + 1{w;i$o}1{Yii=o}] 
j=l 
x exp { -~( w;- x;,B)'E- 1( w;- x;,B)} . 
As suggested by Geweke (1991), we generate this truncated multivariate normal 
variate Wi using a cycle of J Gibbs steps through the components of Wi so that Wij 
is sampled from the truncated normal Wij I Zij•, j* ::j:. j, /3, E, Yi respectively over an 
interval (0, oo) if Yii = 1 or ( -oo, 0] if Yii = 0. 
Finally, we consider sampling E from its conditional distribution. The conditional 
likelihood function L(EI/3, w, y), ignoring the normalizing constant, is 
where w = ( w1, w2, ... , wn) and vee* (E) E A. The full conditional density is pro-
portional to L(El/3, w, y)1r(vee*(E)IE0 , G0). Because of the complexity of this con-
ditional distribution, we use a Hastings algorithm (e.g., Metropolis et al. 1951, 
Hastings 1970, Tierney 1994) to generate E. Let E be the current value. Using an 
algorithm analogous to Chib and Greenberg (1998), we generate candidate values 
E* by specifying a random walk chain 
E* = E + H, 
where H = ( hij) is an increment matrix with zeros on the diagonals and with means 
E(hij) = 0. Let A be the least eigenvalue of E. Alternative to Chib and Greenberg's 
algorithm, we use a Metropolized hit-and-run algorithm (Chen and Schmeiser 1993) 
to simulate H. This algorithm operates as follows: 
(i) generate an i.i.d N(O, 1) random variate sequence of z12, Zta, ... , ZJ-t,Ji 
(ii) generated from N(O, u~) truncated to ( --72, ?2); 
(iii) calculate 
hij = ----.::..---,.-
(Ef;f E[=j zJ,) 
fori< j, hii = 0, and hij = hji fori> j. 
Note that in (ii), u~ is appropriately chosen so as to avoid excessive rejections in 
the Hastings algorithm. As discussed in Marsaglia and Olkin (1994), H generated 
in this manner guarantees that E* is positive definite. 
Let A* be the least eigenvalue of E*. Following Chen and Schmeiser (1993), given 
the proposal value, a move to the point E* is made with probability 
. { L(E* l/3, w, y)1r( vee* (E*)IEo, Go) ( 4> ( ~) 
-
4> ( -~)) 
1
} 
mm 
L(EI/3, w, y)1r( vee* (E) lEo, Go) ( 4> (\ltd) - 4> (-\Aqd)) ' ' 

320 
Dey & Chen 
where <!>(·) is the standard normal cumulative distribution function. Compared to 
the algorithm of Chib and Greenberg (1998), our Metropolized hit-and-run algo-
rithm is more advantageous since the use of the hit-and-run algorithm ensures can-
didate correlation matrix E* to be non-negative no matter which cr~ is chosen. 
Multivariate t-Link Models 
It is well-known that a Multivariate t distribution is the Gamma mixture of 
normal distributions (e.g., see Johnson and Kotz 1976 or Bernardo and Smith 1994). 
From (9), we have 
1r( wdv, xif3, E) 
f.oo [ ( ~ r 
IE/~;1 exp { -~(w;- x;P)' ([.) -t (w;- x;P)}] 
1 (v)v/2 "-1 " 
'r(~) 2 
el e-2'eidei· 
(17) 
To run the Gibbs sampler, we sample ei, Wi, /3, and E from their respective 
conditional distributions. Using (17), we independently generate 
where Q(a, b) denotes a gamma distribution with density 
u(ela, b) ex: ea-le-be. 
In a manner similar to the MVP models with obvious adjustments, we can generate 
Wi, /3, and E given the ei for the MVT models. For example, given ei, Wi, and E, 
we have 
4. 
Model Adequacy for Correlated Binary Data 
Once we have accomplished the first two steps of a Bayesian analysis, i.e., 
constructing probability models and computing the posterior distributions of all 
parameters of interest, using sampling based approach, it is natural to assess the 
fit of the models to the data and to own substantive knowledge. A good Bayesian 
analysis should include at least some checks of the adequacy of the fit of the model 
to the data. While model checking usually addresses the entire model specification 
(both likelihood and prior), model failures can happen in different phases, which in-
clude outliers, mean structure errors, dispersion misspecification, and inappropriate 
exchangeabilities in the hierarchical structure. In this section we consider differ-
ent simulation based model checking approaches using certain discrepancy measure 
which is a function of data as well as parameters. 
We first introduce the following notation which will be used in developing our 
model adequacy and selection procedures. Let 8 be all parameters in the likelihood, 
e.g., 8 = (at, ... , an, /3t, ... , /3J) for the stratified and mixture models, and fJ be all 
the parameters in the prior (i.e., hyper parameters), e.g., fJ = (cr;, a, b, {30 , Bo) for 

18. Bayesian Model Diagno~tics for Correlated Binary Data 
321 
the stratified and mixture models. Also Let f(Yi IO) be the joint distribution of Yi = 
(Yil, Yi2, ... , YiJ )' given 0 and covariates X it, ... , xu. Denote Pii ( 0) = E(Yii IO), 
which may be a function of all covariates as well, and qij(O) = 1- Pij(O). Note that 
for the stratified and mixture models and the conditional models (3), 
•• ( 11) _ 
exp{ D.'i + Xij /3j} 
PzJ u 
-
1 
{ 
{3 } , 
+ exp D.'i + Xij j 
for the MVP models Pij(O) = <P (Xij/3j ), and for the MVT models Pij(O) =tv (Xij/3j) 
where tv ( ·) is the cdf of the t distribution with v degrees of freedom which has the 
form 
f
w 
r(!(v+1)) ( 
-1 2)-!(v+1) 
tv(w)= 
(1/ 2) 
( 1
) 1+v u 
du. 
-oo (1rv) 
r 2l/ 
We denote 1r(Oj6) to be the prior distribution of 0 given 6. 
In order to introduce an appropriate discrepancy measure for the correlated bi-
nary data, we need to obtain the variance and covariance matrix of Yi, which is de-
noted by Ei(O) = (ui~j•(O))JxJ· For the stratified and mixture models, uijj•(O) = 0 
for j ::j:. j* and u;jj(OJ = Pij(O)qij(O). For the conditional models (3), 
exp{2ai + Xij/3j + Xij•/3j} 
2:y
1 
~.=o 2:v
1 
~ .• =o exp{ai(Yij + Yij•) + Xijf3JYij + Xirf3jYij•} 
•J 
t) 
-Pij ( O)Pii* ( 0) for j -:f j* 
and uijj(O) = Pij(O)qij(O). For the MVP models, 
uijj• (0) 
= 
<P( Xij {3j) + <P( Xij• /3j•) - 1 + <P ( -Xij /3j, -Xij• /3j•; Pii*) 
-<P(Xij/3j )<P(Xij•/3j•) for j ::j:. j*, 
where <P(a*, b*; p) is the bivariate normal distribution function which is defined as 
• 
b* 
* *. 
_ 
1 
fa f 
{ u2 - 2puv - v2 } 
<P(a , b , p)-
.j1:::-pi 
. exp -
(1 -
2) 
dudv 
21r' 
1 - p2 
-oo -oo 
2 
P 
and u;jj(O) = <P(Xijf3j )(1- <P(Xij{3j )). For the MVT models, 
uijj* ( 0) = tv ( Xij {3j) +tv ( Xij• f3r) - 1 +tv ( -Xij /3j, -Xij* /3j•; Pii*) 
-tv(Xij/3j )tv(Xij•/3j•) for j -:j; j*, 
where tv (a*, b*; p) is the bivariate t distribution function which is defined as 
tv( a*, b*; p) 
r ( l(v + 2)) 
fa• fb* ( 
u2 - 2puv - v2)- t<v+2) 
2 
1 + 
dudv 
(1rv)r (~v) .j1:::-pi -oo -oo 
v(1- p2) 
and uijj(O) = tv(Xij/3j)(1- tv(Xij/3j)). Note that tv(a*,b*;p) can be evaluated 
through the bivariate normal distribution function <P( a*, b*; p), since we have the 
following relationship between tv (a*, b*; p) and <P( a*, b*; p): 
(18) 

322 
Dey & Chen 
In the earlier sections we present a collection of models and proceed with Bayesian 
inference. The issue at stake is which model is most appropriate for the given bi-
nary data as it is well-known that an improper model could lead to a misleading 
conclusion. In classical statistics, goodness of fit tests have been employed to check 
the plausibility of the model fit to the data. The classical goodness of fit test quan-
tifies the extremeness of a particular discrepancy measure by calculating a tail area 
probability that the model under a specified null hypothesis is true. In the classical 
setup the test statistic and hence the tail area probability is a function of both the 
data as well as the unknown parameters which are specified only under the null 
hypothesis. Alternative to the classical goodness of fit test, we develop here the 
Bayesian model checking methods on the correlated binary data. 
In order to incorporate the correlated nature of binary responses, we introduce 
the following observation-level Pearson residual discrepancy measure: 
(19) 
where Pi((})= (Pi1(0),Pi2(0), ... ,piJ(O))' fori= 1,2, ... ,n. Furthermore, if the 
overall performance for a model is of interest, we introduce the total Pearson residual 
discrepancy measure, which is 
n 
D(O) = LDi(O). 
(20) 
i=l 
Now, letting d( da.ta, 0) be the generic notation for a discrepancy measure (e.g., 
an observation-level Pearson residual or the total Pearson residual), we propose the 
following two methods to perform the model adequacy study. 
Method 1: Posterior Predictive Comparison 
This approach is motivated from Gelman, Meng and Stern (1996) where the 
technique for checking the fit of a model to data is to draw simulated values of a 
discrepancy measure from the posterior predictive distribution and compare these 
samples to the sample from the observed data. 
Let Yobs be the observed data and Ynew be the generated data. Let f(OIYobs), 
f(d(Yobs,O)IYobs) and f(d(Ynew,O)IYobs) be the posterior (predictive) distributions 
of(}, d(Yobs,O), and d(Ynew,O) respectively. We generate oU) from f(OIYobs) (MCMC 
output) and yU) from f(yl(}(l)) and calculate d(Yobs, (}U)) and d(y(l), (J(l)) for I = 
1, 2, ... , B. Then, {d(Yobs, (J(l)), 1 ~I~ B} is a sample from f(d(Yobs, O)IYobs) and 
{d(y(l),(}(l)), 1 ~I~ B} is a sample from f(d(Ynew,O)IYobs)· 
We propose the following Bayesian exploratory data analysis to perform posterior 
predictive comparison. 
(a) We overlay two box plots, i.e., box plot of f(d(Yobs, O)IYobs) vs box plot of 
f( d(Ynew, O)IYobs ). That is, we display these two box plots in a side-by-side 
fashion. If the model is adequate, the two box plots will be very much alike. 
(b) For a given constant !{, we calculate 
p [I d(Yob~- df I > KIYob•] and p [I d(Yne~- df I > KIYob•] ' 
(21) 
where we take df = nJ when d(Ynew, 0) = D( 0) and df = J when d(Ynew, 0) = 
Di(O). Then we compare these two probabilities. If the model is appropriate, 
these two probabilities are comparable. Note that several different values of 
I<, e.g., J( = 1, 2, 3, will be tried. 

18. Bayesian Model Diagnostics for Correlated Binary Data 
323 
(c) We calculate P( d(Ynew, B) 2:: d(Yobs, B)), which can be estimated by 
1 B 
B L 1{d(y(l),iJ(I))2:d(Yobs 18(1))} 
l=l 
where 1{d(y<'>,e<'>)2:d(Yobs,B<'>)} is 1 if d(yU), B(l)) 2:: d(Yobs, B(l)) and 0 if other-
wise. If the model is adequate and n is reasonably large, this probability will 
not be far away from one half. Whereas the model will be suspected when 
this probability is close to one or zero. For this case, d(·, B) may be chosen as 
an observation-level Pearson residual discrepancy or a total Pearson residual 
discrepancy. 
Method II: Simulation Based Model Checking 
This method is entirely simulation based and has been proposed in Dey et al. 
(1995). Like the posterior prior comparison method, this method only requires model 
specification and simulation draws from the posterior distribution under the model. 
This approach replicates a posterior of interest using data obtained under the model 
and enables us to observe the extent of variability in such a posterior. Then we 
compare this posterior obtained under the observed data with the different posterior 
replicates to determine whether the posterior obtained under the observed data is 
in agreement with the other posterior replicates. This enables us to surmise if the 
observed data came from the proposed model. 
In short the approach indicated in Dey et al. (1995) compares posteriors generated 
from the observed data with the associated posteriors generated from the specific 
model. This method also uses a discrepancy measure as the model checking tool. 
The difference of this approach with the earlier method is that instead of using a 
single set of data under the model we replicate R (a large number) data sets. For 
each data set we obtain the posterior distribution of the discrepancy measure. In 
this way for a single discrepancy measure we have R posteriors obtained from the 
data generated under the model to which we can compare the posterior calculated 
from the observed data. The idea is that if the posterior obtained under the observed 
data fits among the other R posteriors then our model fits the data well for that 
particular discrepancy function. Due to the intensity of computation, we recommend 
a discrepancy measured(·, B) to be chosen as the total Pearson residual discrepancy 
measure D(B) in (20). 
The advantage of using this method is that the variability among the posteriors 
allows us to judge better whether the posterior under the observed data fits the 
model. 
The technical detail is given as follows. Similar to Method I, let y(O) = Yobs 
and y(r) be generated data at the rth replication for r = 1, 2, ... , R (say, R = 
1, 000). More specifically, y(r) can be obtained by generating B(r) from f(BIYobs) 
and hence generating y(r) from f(yiB(r)) for r = 1, 2, ... , R. For each r, we gener-
ate B(rj) from the posterior f(Biy(r)) and calculate d(y(r), B(rj)) for j = 1, 2, ... , B 
and r = 0, 1, 2, .. . R. In this way, we obtain R + 1 samples {d(y(r), B(d)), j = 
1, 2,.,., B} from the posterior distribution f(d(y(r), B)ly(r)) for r = 0, 1, ... , R. 
Using these R + 1 samples, we calculate 5-quantiles of each distribution, i.e., 
(r) -
( (r) 
(r) 
(r) 
(r) 
(r) )' 
-
0 1 2 
R 1 tt' 
- -
1 "'R 
(r) 
q 
-
qo.o5, q0.25' q0.5, q0.75, q0.95 
, r -
, , , · · ·, · 
e mg q -
R L.-r=l q 
, 
we calculate er = llq(r)- qll
2
, r = 1, 2, ... , R. Denote C to be, say, the 95th per-
centile of er 's. Then we perform the following Monte Carlo test: if llq(o) - qll
2 5 C, 

324 
Dey & Chen 
we conclude that the model is adequate. Note that choice of C is arbitrary, and is 
usually left to the user. 
5. 
Voter Behavior Data example 
To apply and illustrate our methodologies, we use the survey data on the 
voting behavior of 95 residents of Troy, Michigan given in Greene (1993) and further 
analyzed by Chib and Greenburg (1998). In this data set, the first decision (Yi1) is 
whether to send at least one child to public school and the second (Yi2) is whether 
to vote in favor of a school budget. As in Chib and Greenburg (1998), the covariates 
in XH are a constant, the natural logarithm of annual household income in dollars, 
and the natural logarithm of property taxes paid per year in dollars; and those in 
Xi2 are a constant, annual household income in dollars, property taxes paid per year 
in dollars, and the number of years the resident has been living in Troy. Since each 
voter made two decisions, namely, }i1 and }i2 , then there is a natural correlation 
in the response. 
We fit five models, i.e., a stratified and mixture model, a conditional model, a 
MVP model, two MVT models with v = 1 and v = 8, to this data set. For this data 
set, we have n = 95, J = 2, (31 = ((311, fJ12, f31a)' and fJ2 = ((321, ... , f324)'. Since 
J = 2, we take u~ = 0 for the stratified and mixture model and u~ = 1 for the 
conditional model to ensure the identifiability of parameters. In our calculation, we 
used f3o = 0, Bo = 0.01h, Eo = /2, and Go = 1. 
In our implementation of all five models, we first standardize covariates Xi2, X13, 
x22, x23, and x24 to accelerate the convergence of Gibbs sampling. We observe that 
the standardization is very effective for this data set. We check the convergence of 
the Gibbs sampler using several diagnostic procedures as recommended by Cowles 
and Carlin (1996). We take 10 multiple chains with dispersed initial values and 
compute potential scale reductions (PSR's). (PSR values close to 1 are indicative of 
convergence of the Markov chain to the target distribution.) A detailed description 
of the PSR is given by Gelman and Rubin (1992). We compute the autocorrelation 
coefficients within chains to ascertain that it is "rapidly mixing" as discussed by 
Geyer (1992). Finally, we graph the sample paths from all chains to confirm at 
which iterate the convergence occurs. Using the above diagnostic methods, we find 
that for most cases the Gibbs sampler converges no later than 200 iterations. For 
instance, for the MVP model, for all parameters f3ij and p, the ranges of the PSRs 
are from 1.011 to 1.038 and the ranges for the 97.5 percentiles of the PSRs are from 
1.017 to 1.075 at 200 iterations. Further we calculate the autocorrelations after 
the convergence using 5000 Gibbs iterates for each parameter and we find that the 
auto correlations for all f3ij 's disappear at lag 5 and the auto correlations for p are 
.787 at lag 1, .278 at lag 5, and .078 at lag 10. Note that the acceptance rate for 
the Metropolis step for generating p is about 0.62. 
We use 10,000 "stationary" Gibbs iterates to perform all relative posterior com-
putations. We apply the methodology proposed in Section 4 to check the model 
adequacy for all five models. We use the total Pearson residual discrepancy mea-
sure D((J) in (20) as our model checking tool. For the voter behavior data exam-
ple, the box plots of f( d(Yobs, O)IYobs) and /( d(Ynew, O)IYobs) are displayed in Fig 
18.1. The posterior predictive probabilities (21) with df = 190 for K = 1, 2, 3 are 
given in Table 18.1. The results from Figure 18.1 and Table 18.1 are consistent 
and it can be easily observed that the conditional model and the MVT model with 
v = 1 are less adequate than the other three. We also calculate the probabilities 

18. Bayesian Model Diagnostics for Correlated Binary Data 
325 
P( d(Ynew, 9) ;::: d(Yobs, 9)) for all five models and the MVT model with v = 1 gives 
0.088 while other four models give approximate 0.25. Again, the MVT model with 
v = 1 fails this type of model checking criterion. Finally we perform the simulation 
based model checking and all models pass the Monte Carlo test. For example, for 
the MVP and stratified and mixture models, the respective 95th percentile of er 's 
are 5838.5 and 5776.0 and the respective observed distances llq(o) - qll
2 are 454.3 
and 485.4. Note that the results for the stratified and mixed model and the MVT 
model with v = 8 are similar, which is consistent with the fact that t 8 is virtually 
the logistic distribution (e.g., Albert and Chib, 1993). 
Table 18.1: Posterior Predictive Probabilities 
p II D~~:;J I > /{ IYobs J !.11 D~~:;JI > I<IYnew I 
Model 
I<= 1 
/{ = 2 
I<= 3 
K=1 K=2 I<=3 
Stratified 
0.607 
0.363 
0.204 
0.424 
0.130 
0.036 
and Mixture 
Conditional 
0.662 
0.451 
0.327 
0.644 
0.344 
0.137 
MVP 
0.570 
0.332 
0.179 
0.405 
0.120 
0.028 
MVT 
0.865 
0.691 
0.466 
0.495 
0.187 
0.052 
(v = 1) 
MVT 
0.607 
0.373 
0.208 
0.466 
0.141 
0.032 
(. = 8) 
. 
. 
~ ; 
~ ~ 
i 
~ 
! 
t 
So 
Sn 
Oo 
On 
Po 
Pn 
T1o 
T1n 
Teo 
Ten 
FIGURE 18.1. Distributions of Total Pearson Discrepancy Measures where S=Stratified 
and Mixture, C=Conditional, P=MVP, T1=MVT1 and T8=MVT8; o=observed and 
n=new. 
6. 
Concluding Remarks 
Correlated binary data often arise in experiments when two or more measure-
ments are taken at one time for the same subjects or when repeated measurements 

326 
Dey & Chen 
are taken over time. If such correlation is ignored in the model, overstatement of the 
precision of parameter estimates results. We have considered several models in this 
paper to incorporate the correlation structure within the framework of multivariate 
generalized linear models. 
In this chapter using Bayesian modeling technique, we have proposed two versions 
of logistic regression models with random effects component to incorporate depen-
dencies and introduced latent variables to create multivariate probit and t-link 
models. Several model diagnostic criteria have been introduced for model adequacy, 
using discrepancy measures, and several graphical methods have been employed 
to get a visual representation of model diagnostics. Our calculations indicate that 
the conditional and multivariate Cauchy models are less adequate than the other 
three proposed models. Some possible extensions are to dynamic generalized linear 
models for correlated data, including time-varying coefficients, and polychotomous 
response models which will be pursued in future work. 
References 
Albert, J .H. and Chib, S. (1993). Bayesian Analysis of Binary and Polychotomous 
Response Data. Journal of the American Statistical Association, 88, 669-679. 
Amemiya, T. (1985). Advanced Econometrics. Boston: Harvard University Press. 
Ashford, J .R. and Sowden, R.R. (1970). Multivariate Probit Analysis. Biometrics, 
26, 535-546. 
Bernardo, J.M. and Smith, A.F.M. (1994). Bayesian Theory. Wiley: New York. 
Box, G.E.P. (1980). Sampling and Bayes's Inference in Scientific Modeling (with 
discussion). Journal of the Royal Statistical Society, Series A, 143, 383-430. 
Chen, M.-H. and Schmeiser, B.W. (1993). Performance of the Gibbs, Hit-and-
Run, and Metropolis Samplers. The Journal of Computational and Graphical 
Statistics, 2, 251-272. 
Chib, S. and Greenberg, E. (1998). Bayesian Analysis of Multivariate Pro bit Mod-
els. Biometrika, 85, to appear. 
Cowles, M.K. and Carlin, B.P. (1996). Markov Chain Monte Carlo Convergence 
Diagnostics: A Comparative Review. Journal of the American Statistical As-
sociation, 91, 883-904. 
Dey, D.K., Gelfand, A.E., Swartz, T.B., and Vlachos, P.K. (1995). Simulation 
Based Model Checking for Hierarchical Models. Technical Report #95-29; De-
partment of Statistics, University of Connecticut. 
Gelfand, A.E., Dey, D.K. and Chang, H. (1992). Model Determination using Pre-
dictive Distributions with implementation via Sampling-Based Methods. In: 
Bayesian Statistics, 4, (J. Bernardo, et al., eds.), Oxford University Press, 
Oxford, 147-158. 
Gelfand, A. E. and Smith, A.F .M. (1990). Sampling Based Approaches to Cal-
culating Marginal Densities. Journal of the American Statistical Association, 
85, 398-409. 

18. Bayesian Model Diagnostics for Correlated Binary Data 
327 
Gelman, A., Meng, X.L., and Stern, H.S. (1996). Posterior Predictive Assessment of 
Model Fitness via Realized Discrepancies (with discussion). Statistica Sinica, 
6, 733-808. 
Gelman, A. and Rubin, D.B. (1992). Inference from Iterative Simulation Using 
Multiple Sequences. Statistical Science, 7, 457-511. 
Geman, S. and Geman, D. {1984). Stochastic Relaxation, Gibbs Distributions and 
the Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 6, 721-741. 
Geweke, J. (1991 ). Efficient Simulation from the Multivariate Normal and Student-
t Distributions Subject to Linear Constraints. Computing Science and Statis-
tics: Proceedings of the Twenty- Third Symposium on the Interface, 571-578. 
Geyer, C.J (1992). Practical Markov Chain Monte Carlo (with discussion). Statis-
tical Science, 7, 473-511. 
Gilks, W .R. and Wild, P. (1992). Adaptive Rejection Sampling for Gibbs Sampling. 
Applied Statistics, 41, 337-348. 
Greene, W. (1983). Econometric Analysis, Second Edition. Macmillan: New York. 
Hastings, W.K. (1970). Monte Carlo Sampling Methods Using Markov Chains and 
Their Applications. Biometrika, 57, 97-109. 
Johnson, N .L. and Kotz, S. (1976). Distributions in Statistics: Continuous Multi-
variate Distributions. Wiley: New York. 
Marsaglia, G. and Olkin, I. (1984). Generating Correlation Matrices. SIAM Journal 
on Scientific and Statistical Computations, 5, 470-475. 
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E. 
(1953). Equations of state calculations by fast computing machines. Journal 
of Chemical Physics, 21, 1087-1092. 
Ochi, Y. and Prentice, R.L. (1984). Likelihood Inference in a Correlated Probit 
Regression Model. Biometrics, 71, 531-543. 
Prentice, R.L. (1988). Correlated Binary Regression with Covariate Specific to 
Each Binary Observation. Biometrics, 44, 1033-1048. 
Rousseeuw, P. and Molenberghs, G. (1994). The Shape of Correlation Matrices. 
American Statistician, 48, 276-279. 
Tierney, L. (1994). Markov Chains for Exploring Posterior Distributions (with 
discussions). Annals of Statistics, 22, 1701-1762. 

This Page Intentionally Left Blank 

Part VI 
Challenging Approaches in 
GLMs 

This Page Intentionally Left Blank 

19 
Bayesian Errors-in-Variables 
Modeling 
Jon .Wakefield 
David Stephens 
ABSTRACT Errors·in·variables models are relevant when explanatory variables 
are measured with error. These models fit very naturally into a Bayesian framework 
where the unobserved values of the explanatory variables are viewed as unknown 
parameters. In this paper we consider errors-in-variables modeling for generalized 
linear models. We present a general framework, review classical and Bayesian ap-
proaches and describe a Bayesian analysis of case-control data in which the associa-
tion between respiratory disease and socio economic status is considered, the latter 
being inexactly measured. 
1. 
Introduction 
The conventional approach to regression modeling is to assume that explanatory 
variables are measured without error. The defining feature of an errors-in-variables 
problem is that rather than observing the explanatory variables without error, at 
least one is measured as an error-prone "surrogate". This situation is also some-
times referred to as a measurement error problem. Following Carroll, Ruppert and 
Stefanski (1995) we use X to denote the true value of the variable, W the surrogate 
and Z explanatory variables that are measured without error; Y will denote the 
response. 
In this paper we consider generalized linear models in which: 
E[YIX, Z] = J-t, 
(1) 
with monotonic link function 
g(J-t) = f3o + f3xX + f3zZ, 
(2) 
f3 = (f3o, f3x, f3z) and Y from the exponential family. We let h( ·) denote the inverse 
link, i.e. J-t = h(f3o + f3xX + f3zZ). 
Now suppose that we observe W rather than X, then 
E[YIW, Z] = Ex1w,z [E(YIX, W, Z)) 
= Ex1w,z [E(YIX, Z)] 
if we assume that we have non-differential errors-in-variables, i.e. 
p(YIX, W, Z, (3) = p(YIX, Z, (3). 
(3) 
In other words W adds nothing to the prediction of Y if X is known. Note that this 
is equivalent to 
p(WIY, X, Z, (3) = p(WIX, Z, (3). 
(4) 
331 

332 
Wakefield & Stephens 
One situation in which this will not be true is in epidemiological applications in 
which questionnaires are used to assess exposure and recall bias occurs (Clayton, 
1992). In this case (4) is not true since individuals with the disease have a different 
measurement error model to individuals without the disease. 
Now from (3), 
E[YIW, Z] = Exlw,z[Jl] = Ex1w,z [h(,Bo + ,BxX + ,BzZ)]. 
When we have an identity link function we obtain 
E[YIW, Z] = ,Bo + ,BxE[XIW, Z] + ,BzZ, 
(5) 
i.e. a linear model in ,B. With a non-identity link function we have 
E[YIW, Z] f:. h(,Bo + ,BxE[XIW, Z] + ,BzZ). 
Hence if the model is really given by (1) and (2) and we regress Y on (W, Z) we 
do not obtain a G LM with the same link and there is no obvious way of estimating 
,Bx. This argument is relevant to the classical procedure of regression calibration, 
discussed in Section 3.2. 
Clearly to analyze errors-in-variables problems we must consider the relationship 
between the true and surrogate responses given the known covariates, i.e. p(W, XIZ). 
The classical errors-in-variables model considers 
p(W, XIZ) = p(WIX, Z) X p(XIZ), 
whereas with the Berkson errors-in-variables model we have 
p(W, XIZ) = p(XIW, Z) X p(WIZ). 
Since W is observed this approach only considers p(XIW, Z). 
In experimental studies the Berkson measurement error model will often be ap-
propriate. For example when the level of a variable, W, is recorded from a machine 
setting; the true level X is then modeled as a function of W. Similarly in the pro-
tocol for collection of blood samples in a clinical trial a nominal time, W, will be 
specified and again the true value X will be modeled as a function of W. In ob-
servational studies the classical measurement error model is more typical though 
Berkson measurement errors do arise; Richardson and Deltour (1998) give as an 
example the situation in which ambient air pollution W is measured for a group 
of individuals, perhaps defined by a common area, and individual exposures X are 
required; a reasonable Berkson model may then be E[XIW] = W. 
A large amount of work on errors-in-variables has been motivated by epidemio-
logical applications. In general, epidemiological studies consider observational data 
and exposure assessment is very difficult. In nutritional epidemiology for example a 
person's dietary intake is required and information may be obtained through non-
perfect surrogates such as 24-hour recall, food-frequency questionnaires, or dietary 
records (e.g. Landin, Freedman and Carroll, 1995). Case-control studies have also 
been examined from an errors-in-variables perspective (e.g. Carroll, Gail and Lubin, 
1993). In such applications it is conceptually convenient to consider the following 
separation for the classical errors-in-variables model (Clayton, 1992; Richardson and 
Gilks, 1993a,b ): 
Disease Model: 
p(YIX, Z, ,B). 
(6) 

19. Bayesian Errors-in-Variables 
333 
Measurement Model : 
p(WIX, Z, 0). 
Exposure Model: 
p(XIZ, ¢>). 
(7) 
(8) 
Hence the measurement error model relating W to X and Z depends on param-
eters 0, and the exposure model for X given Z depends on parameters¢. The disease 
model may depend on additional nuisance parameters (such as variances) but for 
notational convenience we suppress dependence on these parameters. A Bayesian 
approach completes the above model by assigning prior distributions to the un-
known parameters. Often these parameters will be assumed to be independent, i.e. 
p({3, 0, ¢>) := p(f3) X p(O) X p(cf>). 
(9) 
We note that in general the prior distribution for() and¢> in particular, may depend 
on Z, but for notational simplicity we suppress this possible dependence. 
Richardson and Gilks (1993a,b) consider designs within this framework using 
graphical models. Unless there are previous data or other forms of information 
available to build the measurement error model, data on X and W simultaneously 
must be available for at least some experimental units (e.g. people); in this situation 
X is referred to as the gold standard. When Y is not available on the subset of 
individuals upon whom both W and X are measured, these data are referred to 
as external; otherwise they are internal. In the former situation the transportability 
(Carrol, Ruppert and Stefanski, 1995) of the model, i.e. whether the model from 
the external data is appropriate for the current study, is a crucial consideration. 
In other situations repeat measures on W may be available, or several measuring 
instruments may have been used. If X is never available (which is the case in 
nutritional epidemiology) then it is clear that the analysis is totally dependent on 
prior assumptions. 
2. 
Illustrative Example: Case-control Study with 
Deprivation 
The Small Area Variations in Air-quality and Health (SAVIAH) project (Briggs 
et al., 1997) investigated the relationship between respiratory health and air pollu-
tion. In this paper we consider one of the study cities, the United Kingdom city of 
Huddersfield, in which a case-control study in children was carried out. The binary 
response was parentally-reported "wheezing or whistling in the chest in the past 
12 months"; for brevity this will be referred to as "wheeze". In the original study 
there were 3326 children in total, of which 536 were cases. For illustration here we 
consider a random sample of 331 individuals (approximately 10% of the original 
data) including 53 cases. 
A number of explanatory variables were available including a deprivation ( socio-
economic) score, the so-called Carstairs index (Carstairs and Morris, 1991), mea-
sured at the level of the decennial census-defined Enumeration District (ED). The 
Carstairs' index is the sum of four variables measured at the census at ED level, 
namely the percent of persons: with no car, in overcrowded housing, with house-
hold head in social class IV or V, and the percent of men unemployed. Each of these 
variables is then standardized across the whole country and the sum is taken as a 
measure of deprivation. High values of the score indicate deprived areas and low 
scores affluent areas. The ED's contain on average 400 individuals and within our 
study region there are a total of 427 containing between 0 and 35 study participants 
with a median of 7. 

334 
Wakefield & Stephens 
For each of the children in the study we have the location of the residence (in 
terms of Eastings and Northings), as well as the ED within which this location lies. 
Figure 19.1 shows the locations of the cases and the controls that we analyze. 
The link between ill-health and deprivation is well-documented (e.g. Jolley, Jar-
man and Elliott, 1992). Clearly, in general, in studies of health it is better to possess 
individual-level data on variables such as diet, smoking status, etc. Only rarely are 
such data available, however. Often in small-area studies of health (e.g. Elliott 
and Wakefield, 1998) deprivation indices such as the Carstairs' index act as sur-
rogates for unmeasured variables. The index may also be truly area-level (rather 
than individual-level) in the sense that it is measuring a characteristic of the area 
such as access to health services. We note that the relationship between health and 
deprivation is complex, and although the obvious causal link is for deprivation to 
cause ill-health, ill-health may cause deprivation (e.g. unemployment). 
In the study we consider a number of explanatory variables are available. First let 
p(X, Z) = Pr(wheezeiX, Z). Here for illustration we assume that the logit of this 
probability depends on sex (girl/boy denoted Zl), age (7 /8-9 years, denoted Z2), 
damp (without/with, denoted Z3) and deprivation (denoted X) with interactions 
between deprivation and the main effects; Zt, Z2, Z3 are assumed to be measured 
without error and X with error. A more complex analysis might also allow for the 
possibility that damp is measured with error also. The model is then given by: 
logit p(X, Z) 
f3o + f31Z1 + f32Z2 + f33Z3 + f34X 
+ f3sX x I(Z1 = 1) + f36X x I(Z2 = 1) + f37X x I(Z3 = 1) 
where I(·) denotes the indicator function. 
Here we may envisage two modeling strategies: we may view deprivation as an 
individual-level or an area-level variable. We describe both possibilities but then in 
Section 5 explicitly consider an individual-level analysis. Figure 19.2 shows, for the 
108,423 ED's of England and Wales a histogram of Carstairs' scores. We note that a 
small proportion of ED's contain so few individuals that for confidentiality reasons 
they remain "unclassified" in terms of deprivation. These ED's have been removed 
in Figure 19.2. 
When we treat deprivation as an area-level variable all individuals within a par-
ticular ED will be assigned the same score. For such an analysis we could assume 
that a classical errors-in-variables model were appropriate, i.e. if Xj denotes the 
deprivation score for ED j then we have 
Wi = Xj + c}", 
where Wj is the observed surrogate and the c j are independent and identically 
distributed (i.i.d.) as N(O, u~ ). Under this model we need to specify a prior distri-
bution for each of the variables measured with error, in this case the deprivation 
score. We might consider constructing a prior distribution from the data of Fig-
ure 19.2 by using, say, a finite mixture-of-normals prior. Strictly we should use the 
"true" deprivation but this is an example in which no gold standard exists. 
Recall from Section 1 that we need to specify a prior p(XIZ) and we note that, 
for example, houses with damp (i.e. Z3 = 1) will, in general, be in more deprived 
ED's. Consequently the use of the same prior for the effect of deprivation across 
all levels of the accurately-measured covariates is unrealistic. Further complications 
become evident, however, when one considers that within any one area we have a 
number of individuals, each with their own age, sex and damp variables. We have 
no information available to construct a more refined prior and., as noted above, the 

0 
0 0 0 
CW') 
""" 
0 0 0 
lO 
C\1 
""" 
0 
0 0 0 
C\1 
""" 
C> 0 
r::: 
:E 
0 0 
t:: 
lO 
0 
,... 
z 
""" 
0 0 0 0 ,... 
""" 
0 0 0 
lO 
0 
""" 
0 0 0 0 0 
""" 
405000 
410000 
19. Bayesian Errors-in-Variables 
335 
415000 
Easting 
420000 
425000 
FIGURE 19.1. Spatial variation of deprivation score with a subset of cases (+)and controls 
( · ); dark squares indicate deprived areas and light squares affluent areas. 

336 
Wakefield & Stephens 
0 0 0 0 ,.... 
0 0 0 
<X) 
0 8 
(C) 
0 8 
"""' 
8 
0 
C\1 
0 
-5 
0 
5 
10 
15 
20 
Deprivation score 
FIGURE 19.2. Histogram of enumeration district deprivation scores across England and 
Wales; high values indicate greatest deprivation. 

19. Bayesian Errors-in-Variables 
337 
assumption that the damp variable is measured without error is probably not real-
istic. An alternative therefore would be to specify a joint model for the distribution 
of damp and deprivation. 
We also note that we would not expect the prior for the deprivation scores over 
all J areas to be of the form Tif=1 p(Xi 14>) since there will be spatial dependence 
in the deprivation data; ED's that are geographically close will tend to have simi-
lar deprivation scores. This spatial dependence can clearly be seen in Figure 19 .1. 
Bernardinelli et al (1997) carry out errors-in-variables modeling in a disease mapping 
context using a conditional autoregressive model for a spatially-varying explanatory 
variable that is assumed to be inexactly measured. 
For the individual-level analysis that we present, we argue that a Berkson model 
is more appropriate since the observed surrogate is an average over the ED and so, 
denoting the deprivation of individual i by xi' we have 
where the cf are i.i.d. N(O, u} ). In this case, as commented in Section 1, we are 
not required to specify a prior distribution for X, but can examine the qualitative 
effect of varying the level of measurement error. 
A major problem shared by this and many epidemiological examples is the lack 
of a gold standard. Hence the analyzes that we present in Section 5 can be viewed 
as a sensitivity study. We allow the measurement error standard deviations, ux, for 
the Berkson model to range from 1.0 to 3.0; the amount of variability depicted in 
Figure 19.2 indicates that this range of values is appropriate. 
3. 
Classical approaches 
3.1 
Basic Formulation 
Fuller (1987) and Carroll, Ruppert and Stefanski (1995) contain comprehensive 
accounts of the frequentist approach to errors-in-variables modeling (the latter also 
contains a chapter on Bayesian approaches). 
Suppose first that we have normal linear model, i.e. 
Y = f3o + f3xX + f3zZ + fy, 
with cy f'.J N(O, O't ). Then from (5) we see that regressing Y on (W, Z) can lead to 
bias in the estimation of both f3x and f3z. Specifically suppose that X and W are 
both scalar and are related via the classical errors-in-variables model: 
with cw rv N(O, 0'~ ). Here fJo and fJ1 represent additive and multiplicative bias 
terms, respectively. Now consider the exposure model 
X = llXIZ + {XIZ' 
with cXIZ rv N (0, u}1z) and ( cy, cw, cXIZ) all mutually independent. We further 
assume that (X, Z) are bivariate normal with mean vector and variance/covariance 
matrix: 
[ ~~], 

338 
Wakefield & Stephens 
respectively. Then, using Bayes Theorem we obtain: 
where 
.Xo = 
Hence 
E[YIW, z,p] = p~ + p;w + p;z 
where 
p~ = Po+ Px.Xo 
p; = Px.Xl 
(ho-1-lz 
= Px X (]2 2 
2 
10'xlz+O'w 
p; = Pz + Px-X2. 
Using 
var(YIW, Z) = Exlw,z[var(YIX, Z)] + varxlw,z(E[YIX, Z]), 
it is straightforward to derive 
showing that the error variance is inflated. We note the following: 
• If we have fh = 1 and (J'~ = 0 but eo =f. 0, then we have no estimation bias 
in Px and Pz due to eo since we have simply relocated the X's and hence the 
slopes are unaffected. 
• If eo = 0, e1 = 1 (i.e. we have an unbiased surrogate) and there are no known 
covariates Z (and so O'llz = 0'}) then 
p; = Px.Xl 
where 
0'2 
' 
-
X 
/\l -
2 
2 
O'x+O'w 
and we obtain the often-quoted attenuation of the regression coefficient. The 
quantity .X1 is sometimes referred to as the reliability ratio. 
• If X and Z are correlated then there is bias in the estimation of Pz, as well 
as in Px· 

19. Bayesian Errors-in-Variables 
339 
• If fh 2:: 1 then we always have attenuation of f3x, for fh < 1 we may over 
estimate the size of the coefficient. 
We end this section by noting that once we are outside the class of linear models very 
few results are available to suggest the effects of errors-in-variables. In particular the 
attenuation of estimated coefficients should not be assumed. For example, Brenner et 
al (1992) show that in ecological studies (in which group-level rather than individual-
level data are analyzed), the effect of exposure misclassification is to over-estimate 
coefficients. 
3.2 Modeling and Analysis: Classical Extensions and Procedures 
We now briefly mention two of the more common extensions to the classical 
measurement error formulation described above. 
Regression Calibration 
In the regression calibration approach Y is regressed on E[XjW, Z], rather than 
X, thus producing a modified model for the observed data. A simple model for 
E[XIW, Z] (the so-called calibration model) is a linear regression. The regression 
calibration approach is straightforward to implement and has been applied to a 
range of models (including logistic regression for which it has been widely-used), 
see Carroll, Ruppert and Stefanski (1995), Chapter 3. 
Simulation Extrapolation 
Simulation extrapolation (SIMEX) is a procedure that can be used to assess the 
effect of measurement error on, for example, parameter estimates, by considering 
simulated replicate data sets derived from, the original data. The basic SIMEX 
principal is that by inflating the measurement error artificially (but in relation to 
the observed data) in a sequential fashion, and noting the varying effect on the 
estimated parameters, it is possible to extrapolate (backwards) to obtain estimates 
of parameters in the case of zero measurement error. Heuristic and more detailed 
mathematical justifications for the SIMEX approach are given in Carroll, Ruppert 
and Stefanski (1995), Chapter 4. 
4. 
Bayesian Approaches 
4.1 
General Framework 
As mentioned in the introduction it is very natural to view errors-in-variables 
models from a Bayesian perspective in which the unobserved exploratory variables 
X are treated as unknown parameters. Lindley and El-Sayyad (1968) were the 
first to consider a Bayesian approach and showed the difficulties that a likelihood 
approach encounters when the X values are viewed as parameters. This is to be 
expected since the usual regularity conditions are invalidated since the number of 
unknown parameters increases with the number of data points. 
For ease of exposition we begin by considering the classical errors-in-variables 
model and the disease/measurement/exposure model described by Equations (6)-
(8). The posterior distribution is then given by 
(X {3 f) A.IY w Z) = p(Y, WIX,{3,fJ,rjJ,Z) X p(X,f],fJ,rJ>IZ) 
p 
' ' ''+' 
' 
' 
p(Y, WIZ) 

340 
Wakefield & Stephens 
ex: p(YIX, z, !3) x p(WIX, z, e) x p(XIZ, <P) x p(/3, e, <P), 
(10) 
using the conditional independencies implied by (6)-(8) (which include non-differential 
measurement error). As in (9) we will take independent priors, so that p(/3, fi, <P) = 
p(f3) x p(fi) x p(</J). We re-iterate that, in general, the prior for (J and <P may depend 
on Z. 
Similarly for the Berkson errors-in-variables model we have 
p(X, {3, fiiY, w, Z) ex: p(YIX, z, {3) X p(XIW, z, (J) X p(/3, fi), 
with p(/3, fi) = p(f3) x p(fi). There is no need to specify a prior for X when this 
model is used. 
4.2 Implementation 
As in many applications Markov chain Monte Carlo (MCMC) has greatly eased 
the analysis of errors-in-variables models. The full conditional distributions ( Gilks, 
Richardson and Spiegelhalter, 1996) that are required for the classical errors-in-
variables model are obtained from, (10): 
p(f3IX, fi, </J, Y, W, Z) = p(f31X, Y, Z) 
ex: 
p(YIX, z, {3) X p(f3) 
p(XI/3, fi, </J, Y, W, Z) = p(YIX, z, {3) X p(WIX, z, fi) X p(XIZ, <P) 
p(fil/3, X, </J, Y, W, Z) = p(fijX, W, Z) 
ex: 
p(WIX, z, fi) X p(fi) 
p(</JIX, {3, fi, Y, W, Z) = p(</JIX, Z) 
ex: 
p(XIZ, <P) X p(</J). 
This first conditional distribution corresponds to the posterior distribution that 
would have resulted if X had been observed. The conditional distributional for X 
involves all of the stages of the disease/measurement/exposure model, showing how 
information propagates between the different stages. The conditional distributions 
for the Berkson model follow similarly. 
4.3 Previous Work 
Richardson (1996) provides a review of Bayesian measurement error modeling. 
Racine-Poon, Weihs and Smith (1991) considered errors-in-variables for dilution 
errors in radioimmunoassay using a Berkson model and numerical integration for 
implementation. Schmid and Rosner (1993) used a Bayesian model to analyze epi-
demiological data in which alcohol consumption was modeled as a distribution that 
included a spike at zero to represent zero consumption. 
The general parametric framework for epidemiological applications using M CM C 
for inference has been described by Richardson and Gilks (1993a,b ). Fully para-
metric approaches using MCMC have been suggested for: generalized linear model 
(Stephens and Dellaportas, 1992); nonlinear models (Dellaportas and Stephens, 
1995); ancillary risk factor models in epidemiology (Gilks and Richardson, 1992); 
disease mapping models, including a spatial clustering prior on the explanatory 
variables (Bernardinelli et al, 1997); ecological correlation studies with a Poisson 
regression model (Jordan et al, 1997); estimation of the dose-response relationship 
for atomic bomb survivors (Richardson and Deltour, 1998). 

19. Bayesian Errors-in-Variables 
341 
TABLE 19.1. Models for the wheeze data, logit p(Xi, Zi) =<Pi· 
MODEL 
Z 
Z 
Z 
LINEAR PREDICTOR · 
1 
0 
1 
0 
1 
0 
1 
0 
1 
1 
0 
0 
1 
1 
0 
0 
0 
1 
1 
1 
1 
Dellaportas and Stephens (1995) and Walker and Wakefield (1998) consider errors-
in-variables in second stage explanatory variables of a hierarchical model, the latter 
with a non-parametric distribution for the random effects in a nonlinear model. An-
other source of errors-in-variables in epidemiology is the consideration of inaccura-
cies in population denominators that are present due to, for example, underenumer-
ation at the census and errors due to the need to construct counts for inter-censual 
years. Wakefield and Wallace (1999) consider an errors-in-variables approach to this 
problem. Bennett and Wakefield (1998) consider an errors-in-variables approach to 
population pharmacokinetic/pharmacodynamic modeling where the observed phar-
macokinetic concentrations that act as explanatory variables for the pharmacody-
namic response were modeled using a classical measurement error model and an 
informative prior derived from previous studies. 
Muller and Roeder (1997) consider a non-parametric approach to errors-in-variables 
in case-control studies using Dirichlet processes. Mallick and Gelfand (1996) con-
sider a semi-parametric approach for generalized linear models using incomplete 
beta functions. 
5. 
Example revisited 
We now present a number of analyzes that consider the effect of errors-in-variables 
in the deprivation score. In particular, we wish to study the qualitative effect of 
varying the amount of measurement error in the deprivation score. We therefore 
repeat the analysis several times with u} in the Berkson model ranging from, 1.0 
to 9.0, and inspect posterior summaries for the parameters of interest relative to 
the results obtained assuming zero measurement error. 
The parameterization of the 
SEX*DEP+AGE*DEP+DAMP*DEP 
model used (in an obvious notation, see for example Wilkinson and Rogers, 1973), 
is as follows. In terms of parameters {3 = (f3o, {31, .... , f3s), we model the case/control 
random variable for individual i, Yi, as having a Bernoulli distribution, 
where logit p(Xi, Zi) = </>i, with </>i and (ZH, Z2i, Z3i) is given in the table below. 
We have eight distinct models, each corresponding to a different combination of the 
explanatory variables sex (Z1), age (Z2) and damp (Z3). Due to the interactions we 
have a different slope and intercept for each combination of the covariates. 
The deprivation score for individual i is denoted Xi. The parameterization in the 
table is utilized to simplify likelihood computations. We make inference about the 

342 
Wakefield & Stephens 
posterior via MCMC, all of the required conditional distributions follow well-known 
forms or are log-concave (for which the adaptive rejection algorithm of Gilks and 
Wild, 1992, can be used). 
We report the results first in terms of posterior summaries of the slope and 
intercept pairs for each of the the eight models. The second summary we examine is 
the odds ratio for each of the main effects as a function of deprivation, in order to 
investigate the effect of the interactions. For age, sex and dampness, at deprivation 
level X, these odds ratios are given by exp(Jh + f3sX), exp(/32 + /36X) and exp(/33 + 
f37X), respectively. Posterior distributions for these functions are readily computed 
from the MCMC output. 
Figure 19.3 depicts sample-based boxplot summaries (median, quartiles and 95% 
interval) for the slope and intercept parameters for models 1-8, with increasing 
measurement error left to right. The effect of varying measurement error is evi-
dent; as measurement error levels are increased, the posterior distributions exhibit 
increased variance, and location shift away from zero, i.e. attenuation is exhibited 
when measurement error is ignored. This qualitative behavior is in line with other 
comparable measurement error studies, but it is of interest to quantify the extent 
to which conclusions are altered as ux is varied over a plausible range. 
Figure 19.4 depicts similar sample-based summaries for the main effects odds 
ratios, that is, odds ratios for boys versus girls, 8-9 year-olds versus 7 year olds, 
and for houses with damp versus houses without dampness, for deprivation scores 
ranging from -2.0 to 3.0. We see that boys are more likely to suffer from wheeze than 
girls, as are those in a damp house, and in both cases these relationships become 
more pronounced as deprivation increases. For age the relationship is less clear cut; 
the effect switches as a function of deprivation. 
6. 
Conclusions and Discussion 
Great care must be taken when errors-in-variables modeling is carried out; par-
ticularly in the case when no gold standard exists since the analysis is driven by 
assumptions that are uncheckable from the data being analyzed. If the errors-in-
variables model is a bad approximation to the truth then spurious results will be 
obtained; if the posterior distributions of the true explanatory variables are far from 
the surrogates then this is a sign that a large amount of "feedback" has occurred 
suggesting that modeling assumptions are having a large effect. For this reason non-
and semi-parametric methods are appealing in errors-in-variables modeling. A fre-
quentist semi-parametric approach has recently been suggested by Spiegelman and 
Casella (1997). From a Bayesian perspective the modeling of the measurement error 
and exposure probability distributions can be achieved in a non-parametric fashion 
(using mixtures of Dirichlet processes or Polya tree formulations for example), or 
using the semi-parametric approach of Richardson and Green (1997), which is es-
sentially a mixture of normals representation with variable number of components. 
We finally note that although MCMC techniques allow the routine analysis of 
complex data structures they do not necessarily provide insight into the underlying 
structure of the problem since each application is a "one-off". For errors-in-variables 
specifically there is a need for more theoretical work in order to determine the 
effects of measurement error. For design in particular this would be highly desirable. 
Recently this problem has been examined by Devine and Smith (1998). 

c 
..-
I 
0 
0 
19. Bayesian Errors-in-Variables 
343 
(a) Intercepts with increasing measurement error 
MODEL 1 MODEL 2 MODEL 3 MODEL 4 MODEL 5 MODEL 6 MODEL 7 MODEL 8 
I 
!Ill ij j 
!!!!Iii 
1111111 
1::· 
. 
• 
i I!! l I! 
! : : li ll 
ldil!i 
i I ij I: I 
i l! l! I: IW$. 
I 
II II!: • 
i I ~ : : f ~ i li! i i! I 
iilllll 
.!i\!il 
l,,jlj! 
I! 
''I 
I! '· 
• i 
1 l! :i I! 
. 
! 
ll 
• • j lj q 
I 
•. ;tli 
I "l 
I 
I 
:,i 
·ilJi.l 
'ji 
I 
'I'' 
II 
! 
I 
! 
I 
I 
I 
I 
2 
4 
6 
8 
MODEL 
(b) Slopes with increasing measurement error 
2 
4 
MODEL 
6 
8 
FIGURE 19.3. Boxplots of (a) intercept and (b) slope parameters for models 1-8 with 
increasing measurement error (left to right). 

344 
Wakefield & Stephens 
(a.) Odds ratios for SEX with Increasing deprivation score 
...... ~ 
...................... ,. 
-2 
0 
2 
4 
Deprivation score 
(b) Odds ratios for AGE with Increasing deprivation score 
~ 
~ 
~ 
~ 
~ 
g 
-2 
0 
2 
4 
Deprivation score 
(c) Odds ratios for DAMP with Increasing deprivation score 
-2 
0 
2 
4 
Deprivation score 
FIGURE 19.4. Posterior log-odds ratios for the SEX, AGE, and DAMP with increasing 
measurement error (left to right). 

19. Bayesian Errors-in-Variables 
345 
Acknowledgements 
The authors would like to thank Dave Briggs of Nene College and Jeremy Bullard 
and Paul Elliott of Imperial College School of Medicine for supplying the data that 
were used in the example. The authors would also like to thank the Office for Na-
tional Statistics who made the postcoded cancer data available for use. Population 
data came from the Estimating with Confidence project. This work is based on 
data provided with the support of the ESRC and JISC and uses census and bound-
ary material which are copyright of the Crown, the Post Office and the ED-LINE 
Consortium. 
References 
Bennett, J .E. and Wakefield, J .C. (1998). The use of errors-in-variable modeling 
to link populations pharmacokinetic and pharmacodynamic data. Submitted 
to Biometrics. 
Bernardinelli, L., Pascutto, C., Best, N.G., and Gilks, W.R. (1997). Disease map-
ping with errors in covariates. Statistics in Medicine 16, 741-752. 
Brenner, H., Savitz, D.A., Jockel, K.-H. and Greenland, S. (1992). Effects of non-
differential exposure misclassification in ecologic studies. American Journal of 
Epidemiology 135, 85-95. 
Briggs, D.J., Collins, S., Elliott, P., Fischer, P., Kingham, S., Lebret, E., Pryl, 
K., Van Reeuwijk, H., Smallbone, K., and Van Der Veen, A. (1997). Map-
ping urban air pollution using GIS: a regression-based approach. International 
Journal of Geographical Information Systems 11, 699-718. 
Carroll, R.J ., Gail, M.H., and Lubin, J .H. (1993). Case-control studies with errors 
in covariates. Journal of the American Statistical Society 88, 185-99. 
Carroll, R.J ., Ruppert, D., and Stefanski, L.A. (1995). Measurement Error in Non-
linear Models. Chapman and Hall, London. 
Carstairs, V. and Morris, R. (1991). Deprivation and Health in Scotland. Aberdeen 
University Press, Aberdeen. 
Clayton, D.G. (1992). Models for the analysis of cohort and case-control studies 
with inaccurately measured exposures. In Statistical Models for Longitudinal 
Studies of Health, J.H. Dwyer, F. Manning, P. Lippert, and H. Hoffmeister 
(editors), pp. 301-331. Oxford University Press, Oxford. 
Dellaportas, P. and Stephens, D.A. (1995). Bayesian analysis of errors-in-variables 
regression models. Biometrics 51, 1085-95. 
Devine, O.J. and Smith, J .M. (1998). Estimating sample size for epidemiological 
studies: The impact of ignoring exposure measurement uncertainty. Statistics 
in Medicine 17, 1391-1402. 
Elliott, P. and Wakefield, J.C. (1998). Small-area studies of environment and 
health. In Statistics for the Environment 4: Health and the Environment, V. 
Barnett, A. Stein and K.F. Turkman (editors), pp. 3-27, John Wiley, New 
York. 

346 
Wakefield & Stephens 
Fuller, W.A. (1987). Measurement Error Models. John Wiley and Sons, New York. 
Gilks, W.R. and Richardson, S. (1992). Analysis of disease risks using ancillary risk 
factors, with application to job-exposure matrices. Statistics in Medicine 11, 
1443-1463. 
Gilks, W.R., Richardson, S., and Spigelhalter, D.J. (1996). Markov Chain Monte 
Carlo in Practice. Chapman and Hall, London. 
Gilks, W.R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Applied Statistics, 41, 337-348. 
Jolley, D., Jarman, B., and Elliott, P. (1992). Socio-economic confounding. In Ge-
ographical and Environmental Epidemiology: Methods for Small-area Studies, 
P. Elliott, J. Cuzick, D. English, and R. Stern (editors), pp. 115-24. Oxford 
University Press, Oxford. 
Jordan, P., Brubacher, D., Tsugane, S. Tsubono, Y., Gey, K.F., and Moser, U. 
(1997). Modelling of mortality data from a multi-centre study in Japan by 
means of Poisson regression with errors in variables. International Journal of 
Epidemiology 26, 501-7. 
Landin, R., Freedman, L.S. and Carroll, R.J. (1995). Adjusting for time trends 
when estimating the relationship between dietary intake obtained from, a food 
frequency questionnaire and true average intake. Biometrics 51, 169-181. 
Lindley, D.V. and El-Sayyad, G.M. (1968). The Bayesian estimation of a linear 
functional relationship. Journal of the Royal Statistical Society, Series B 30, 
190-202. 
Mallick, B. and Gelfand, A.E. (1996). Semiparametric errors in variables models: a 
Bayesian approach. Journal of Statistical Planning and Inference 52, 307-321. 
Muller, P. and Roeder, K. (1997). A Bayesian semiparametric model for case-
control studies with errors in variables. Biometrika 84, 523-537. 
Racine-Poon, A., Weihs, C., and Smith, A.F.M. (1991). Estimation of relative 
potency with sequential dilution errors in radioimmunoassay. Biometrics 41, 
1235-1246. 
Richardson, S. (1996). Measurement Error. In Markov Chain Monte Carlo in Prac-
tice, W.R. Gilks, S. Richardson, and D.J. Spiegelhalter (editors), pp. 401-417. 
Chapman and Hall, London. 
Richardson, S. and Deltour, I. (1998). Bayesian modelling of measurement error 
problems with reference to the analysis of atomic-bomb survivor data. In 
Statistics for the Environment 4: Health and the Environment, V. Barnett, A. 
Stein and K.F. Turkman (editors), pp. 259-279, John Wiley, New York. 
Richardson, S. and Gilks, W.R. (1993a). A Bayesian approach to measurement er-
ror problems in epidemiology using conditional independence models. A mer-
ican Journal of Epidemiology 138, 430-442. 
Richardson, S. and Gilks, W.R. (1993b). Conditional independence models for epi-
demiological studies with covariate measurement error. Statistics in Medicine 12, 
1703-1722. 

19. Bayesian Errors-in-Variables 
347 
Richardson, S. and Green, P.J. (1997). On Bayesian analysis of mixtures with 
an unknown number of components. Journal of the Royal Statistical Society, 
Series B 59, 731-792. 
Schmid, C.H. and Rosner, B. (1993). A Bayesian approach to logistic regression 
models having measurement error following a mixture distribution. Statistics 
in Medicine 12, 1141-1153. 
Spiegelman, D. and Casella, M. (1997). Fully parametric and semiparametric re-
gression models for common events with covariate measurement error, in main 
study /validation study designs. Biometrics 53, 395-409. 
Stephens, D.A. and Dellaportas, P. (1992). Bayesian analysis of generalized lin-
ear models with covariate measurement error. In Bayesian Statistics 4, J.M. 
Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith (editors), pp. 813-820. 
Oxford University Press, Oxford. 
Wakefield, J .C. and Wallace, C. (1999). Implications of estimated counts for studies 
of environment and health in small-areas. To appear in Office for National 
Statistics Occasional Series in Medical and Population Subjects. 
Walker, S.G. and Wakefield, J.C. (1998). Population models with a nonparametric 
random coefficient distribution. Sankhya, Series B 60, 196-212. 
Wilkinson, G.N. and Rogers, C.E. (1973). Symbolic description of factorial models 
for analysis of variance. Applied Statistics 22, 392-9. 

This Page Intentionally Left Blank 

20 
Bayesian Analysis of 
Compositional Data 
Malini Iyengar 
Dipak Dey 
ABSTRACT Compositional data often result when raw data are normalized or when 
data is obtained as proportions of a certain heterogeneous quantity. These conditions 
are fairly common in geology, economics and biology. The result is therefore, a vector 
of such observations per specimen. The usual multivariate procedures are seldom 
adequate for the analysis of compositional data and there is a relative dearth of 
alternative techniques suitable for the same. In this chapter, parametric and semi-
parametric methods to model such data is discussed and is illustrated on a real 
data set comprising sand, silt and clay compositions taken at various water depths 
in an Arctic lake. Simulation based approach is adopted to ascertain adequacy of 
the fit and several models are compared via measures based on posterior predictive 
distribution. 
1. 
Introduction 
Compositional data can be viewed as a non-negative vector with unit-sum con-
straint and therefore restricted to a simplex of appropriate dimension. A typical 
example of data of this nature would be compositional contents of alloys and ores. 
Analysis of such data also arises when dealing with synthesis of alloys or com-
pounds i.e., in specifying a compositional range of contents to achieve a certain 
level of some desired physical property. In environmental studies, toxic contents 
in air reveal the extent of atmospheric pollution and analyzing these components 
helps in understanding the harmful effects of toxins on our health. In another case 
data on several life-forms in a particular region may be available as percentages 
and we may want to assess effect of time and other varying ecological conditions on 
their survival. Although not frequently identified as such, the scope for obtaining 
information compositionally is unlimited. 
To begin with, Dirichlet distribution was used to explain compositional data. But, 
as Aitchison (1982) has commented, it turns out to be inadequate for the description 
of variability in this situation. One disadvantage with the Dirichlet environment is 
that the correlation structure of a Dirichlet composition is wholly negative thereby 
making it inappropriate to fit data with some positive correlations. Also, since 
every Dirichlet composition may be perceived as a composition formed from a basis 
of independent equi-scaled gamma distributed components, this Class has a very 
strong intrinsic independence structure which makes it unsuitable for data with 
mild dependence. Now, occurrence of data on a simplex inhibits the usage of the 
existing plethora of multivariate analytical procedures. Aitchison and Shen (1985b) 
introduced logistic normal distribution and in the context of modeling compositional 
data, Aitchison first recognized the inherent difficulties and made a fundamental 
contribution in this area by primarily adopting logistic normal class to model such 
data. His modeling strategy was to transform the G component (composition) vector 
x to vector yin RG-l through Additive Log Ratio (ALR) function (defined below). 
349 

350 
Iyengar & Dey 
Now, let sG- 1 denote the G-1 dimensional simplex, so that X E sG- 1 . Aitchison's 
idea to model imprecision in observation X is as follows. If {3 E sG- 1 represents the 
''true" unobserved vector of component proportions then, a perturbation operator 
''o" relates error term (in sG- 1 with f3 as, X= {3o( = ( rfcl~:,i .. ·l?~~(i ). So that, 
y = ALR(x) defined by y =(log(;;), ... , log(?-)), a (real) vector in RG- 1 is then 
modeled in multivariate distributional framewort Aitchison (1986) assumed logistic 
normality of x or equivalently multivariate normality of y. We note that, under ALR 
transformation one automatically requires all component ratios to be positive, and 
especially so with the assumption of logistic normal density which is not defined on 
the boundary of the simplex. Further Aitchison and Shen (1985b) also demonstrated 
that logistic normal density is invariant to permutation of components and this in 
turn makes the model independent of order in which constituents are recorded. 
Aitchison also suggested using the Box-Cox transformations to extend the scope of 
analysis. 
Rayens and Srinivasan (1991a, 1991b) adopted Aitchison's suggestion and incor-
porated Box-Cox transformations (see (2.1) for definition) as a generalization of 
ALR transform. But, they make strong structural assumptions on the covariance 
of the resulting transformed vector to ensure that classical inferential techniques 
are more tractable, as these procedures rely largely on the principle of maximizing 
likelihood. Their framework does not abide by the inherent constraints in such data 
and even under such assumptions introduction of covariates rapidly increases the 
level of complexity. Here we employ the Bayesian methods model such data using 
Box-Cox transformations. Let us begin by briefly examining this mode of analysis 
based on transformed quantities. Now, the LN class does not include Dirichlet dis-
tribution and it fails to model data with extreme independence properties. Also in 
transforming the composition vector to the real space, one has to sacrifice inter-
pretability of parameter estimates in simplex. So, a subsequent question is whether 
we can model on so and if so are there other alternatives to the Dirichlet model? 
In other words, if we choose so as the surface to build the model then the idea is 
to investigate distributions on so that not only model complex dependence proper-
ties but can also entertain extreme independence. Aitchison (1985b) introduced the 
AO(a, {3) class (not discussed here) and this family includes both logistic normal and 
Dirichlet as a special case. However closed form expressions for this density exist 
only in these two special cases. Rayens and Srinivasan (1994) studied generalized 
Liouville distributions (GLD) on simplex in response to this but, in their treatment 
of the material it remains unclear as to how covariates can be included in the model. 
The GLD class is a definite improvement over Dirichlet class on simplex. It permits 
distributions that admit negative or mixed correlation. Further, (to quote Rayens 
and Srinivasan (1994)) the GLD family contains non-Dirichlet distributions with 
non-positive correlation and overcomes deficits in the DO( a) class. Here we adopt a 
semiparametric Bayesian approach to model in the GLD framework. 
We use a Bayesian paradigm to model such data in both parametric and semi-
parametric setup and illustrate with the aid of an example. Markov chain Monte 
Carlo (MCMC) methods (see Gelfand and Smith (1995) and Tierney (1994)) are 
employed to simulate from posterior distribution for proposed models and inferences 
are drawn from these simulated values. The material is organized as follows. In the 
next section we formulate a parametric model and in Section 3 we describe model 
determination. In section 4 we discuss a semiparametric model and the following 
section describes the estimation procedure for the two models. Section 6 provides a 
detailed analysis of results thus obtained. The last section is reserved for concluding 

20. Compositional Data 
351 
remarks. 
2. 
A Parametric Approach 
The Box-Cox transformation was suggested by Aitchison as a means to develop 
more reliable models and this has been discussed by Rayens and Srinivasan (1991a, 
1991b). We briefly describe the key ideas in their paper as follows. Let BC(() denote 
the Box-Cox transformation (with parameter ;\) of ( E so and as in Aitchson's 
model they let x = j3o(. Instead of modeling error term as logistic normal as was 
done by Aitchison (1982), Rayens and Srinivasan (1991b) assume that there exists 
Ai E R so that, 
((i/(a)>.i -1 
Ai 
is N(O, o}), (1 ~ i ~ g). Note that they assume independence between the g com-
ponents and estimations are performed maximizing likelihood. The independence 
assumption may not be appropriate owing to the nature of x i.e., x being subject to 
unit sum constraint is not composed of independent quantities. Further, the model 
is fit marginally due to this assumption. 
We now adapt the above notation to subsequent treatment of the material. Con-
sider a heterogeneous mixture with G components. Let f3i = (f3i1, f3i2, ... , f3iG) 
denote the parameter vector of constituent proportions and Xij represent ith ob-
servation for ph constituent (i = 1, ... , n; j = 1, ... , G) so that, Xij ~ 0 and 
L:J= 1 Xij = 1 (V i = 1, ... , n). Here n denotes the number of composition vector 
samples. We note that f3i could denote the expected value of a composition vector 
under a Bayesian paradigm as opposed to a fixed "true" value in the frequentist 
case and may depend on covariate information (see subscript i). There is a natural 
unit sum constraint on the constituent vector Xi = (Xi!, ... , XiG) so that, Xi has 
dimension g = G- 1 i.e., Xi E su. Now, Aitchison's model depends on being able 
to fit compositional data under logistic normal class. Further, di! agnostic proce-
dures proposed by Aitchison (1986) are dependent on the choice of divisor xa even 
if the theory based on logistic normal distributions is not. In this regard, suppose 
we use another family of a general class of functions which also includes the ALR 
transformation we can increase the chances of being able to provide a better fit 
to such data. Now, consider a function H(.) so that, Yij = H(~), j = 1, .. . ,g; 
i = 1, ... , nand let Yi = (Yil, ... , Yiu), the 1 x g vector of transformed composition. 
By using the ratio .;.;;; the dimensionality of simplex is effectively preserved and the 
function H (.) is chosen so as to ensure that the resultant vector Yi has real compo-
nents which can be modeled by a suitable likelihood. When we model H( G;; ), the 
method acquires a GLIM like flavor with H(.) as a link function. Next as a means 
to improve the scope of analytic tools! we adopt Aitchison's suggestion of studying 
the Box-Cox transformations as a natural extension of ALR. 
Now, the Box-Cox transformation of Xi (1 ~ i ~ n), is defined as follows, 
Yi' = H(_!L) = 
~j 
x·. 
{ (xii/xiG)"i -1 
if AJ· =f. 0 
3 
XiG 
log(~) 
otherwise 
(1) 
where Aj E R is an unknown parameter (i = 1, ... , nand j = 1, ... , g). This family 
includes ALR transformation as a special case (i.e., Aj = 0 for all j = 1, ... , g). 
Here the positivity restriction on Xi reduces to having at least one component ( XiG) 

352 
Iyengar & Dey 
being uniformly positive. Now, we model Yi ( whose elements are defined in (2.1)) 
as follows, for 1 ~ i ~ n 
(2) 
where Zi is a 1 x p vector of covariates when ith sample is observed, a is 1 x 
g vector of intercept quantities and () is p x g matrix of regression coefficients. 
Also q = (fit, ... , fig) denotes the model error vector. We assume that fi (1 ~ 
i ~ n) are independent with the same distribution and model diagnostics are used 
to validate distributional assumptions of fi. We note that the dimension of this 
problem is at least 2g + g(g + 1)/2 and higher in presence of covariates. Now, 
estimates of f3i can be derived upon arriving at estimates of a and () and this is 
also discussed here. As modeling assumptions are made more universal in nature it 
becomes practically infeasible to continue to depend on exact analytical solutions. 
It is in these circumstances that sampling based inferences can no longer be viewed 
as a surrogate method but rather a primary mode of extracting information for 
inference. 
To illustrate the procedure we analyze data on sand, silt and clay compositions 
obtained at different water depths in an Arctic lake. The data originally appeared 
in Coakley and Rust (1968) and has been discussed in Aitchison (1986, page 359). 
Aitchison considers ALR transformation to fit the data and diagnostic measures 
examined by him do not ascertain goodness of fit. 
3. 
Simulation Based Model Determination 
The aspect of model determination comprises adequacy, selection and (possible) 
outlier detection. We rely largely on the predictive distribution to assess perfor-
mance of the models as more often than not, models are developed for purposes of 
prediction and therefore it is only natural to study their predictive distributions. 
Further, well known procedures such as Bayes factor (when prior distribution is 
proper) may not be very informative especially when there are more than two mod-
els to compare. Also Bayes factors are optimal only under a 0-1 loss function i.e., 
when model choice is viewed as hypothesis testing. Moreover when comparing mod-
els, predictive distributions are comparable whereas posterior distributions are not 
so, it is more appropriate to examine predictive distributions under various models. 
Now by employing Bayesian inference we enjoy the availability of the predictive dis-
tribution and this in turn encourages a range of diagnostic studies. In our example 
we utilize the following simulation based approaches to evaluate the performance of 
models. 
First, model adequacy is studied using the Gelman, Meng and Stern (1996) tech-
nique, where simulated values of a discrepancy measured(.,.) from predictive dis-
tribution are compared to the same from the data (i.e. y). Let 1 denote all unknown 
parameters and Ynew, the data generated from predictive distribution. Furthermore, 
let f( 1 I y), f( d(y, 1) I y) and f( d(Ynew, 1) I y) denote the posterior distributions of 
1, d(y, 1) and d(Ynew, 1) respectively. Samples 1i, are obtained from f(l I y) and Yi 
from f(y I 1i) for l = 1, ... B, where B represents number of iterates. From these 
samples d(y, 1i), d(yi, 1i) are computed. Clearly {d(y, 1i), l = 1, ... , B} is a sample 
from f(d(y, 1) I y) and {d(yi, 1i), l = 1, ... , B} is a sample from f(d(Ynew, 1) I y). 
Essentially, it is desired that the two discrepancy values, d(Ynew, 1) and d(y, 1) do 
not differ radically. One way to test this is would be by computing the probabil-
ity P(d(Ynew, 1) ~ d(y, 1)), denoted henceforth by pr. This probability may not 

20. Compositional Data 
353 
be analytically solvable and Monte Carlo methods can be used to estimate pr as 
follows, 
1 B 
pr = B 'I: I{d(yj,-ri)2:d(y,-ri)} 
1=1 
(3) 
The idea being that if the model is adequate, this probability would be about a half 
and the model would be judged fairly inadequate if this probability is either close 
to 1 or 0. 
The second analysis for model criticism adapts the Gelfand, Dey and Chang 
(1992) results for a multivariate setup. They present four different techniques to 
assess performance of models, of which the discussion on Bayesian standardized 
residual is used. This would in particular address possible issue of outliers in the 
model. We describe it briefly as follows. Assume that new observations (i.e. Ynew) 
are available for fixed values z of the covariate. From predictive distribution at these 
covariate values one can determine the mean m and covariance structure S. These 
values along with Ynew, can be utilized to compute a standardized distance, r where 
r 2 = (Ynew - m )' S- 1 (Ynew - m). To be able to do this one however needs these new 
values which ideally are not available and to overcome this, one borrows from the 
concept of cross validation as explained under. 
Let Y(i) denote the data with information on ith sample deleted. Likewise let Z(i) 
represent covariate information on all but the ith sample. The conditional predictive 
density of Yi given Y(i)' Z(i) and Zi is known as Conditional Predictive Ordinate 
(CPO) (see Pettit and Young (1990)) and it captures support of ith observation for 
the model. Referring Gelfand, Dey and Chang (1992) it can be shown that 
where 1 as before denotes all unknown parameters. Now, to estimate r 2 we need 
to compute mean and variance of this distribution. However, these closed form 
expressions are not always tractable and Monte Carlo estimates C POi, E("Yi I 
Y(i), Z(i), Zi), and V("Yi I Y(i), Z(i), Zi) are used. See Gelfand, Dey and Chang (1992) 
for more details. Now based on the cross validation scheme, Bayesian standardized 
residual ri (i = 1, ... , n) can be derived from 
Here again B denotes number of iterates. Large values of ri should increase concern 
about appropriateness of the model. Now, we obtain a Monte Carlo estimate of r[ 
from 
These residuals are a visual means to compare models. These procedures may ap-
prove more than one model, in which case a summary statistic is sought to compare 
contending models. A certain measure of loss associated with competing models is 
elicited. Ideally, the measure is designed so as to quantify the loss attributed to the 
model's ability to predict. This is sorted by a predictive simulation based approach 
i.e., data from proposed models is simulated and they are compared with the aim 
of minimizing predictive loss. The method considered here involves simulation from 
predictive distribution. As before, let Ynew denote data sampled from predictive 
distribution, f(Ynew, 1 I y) and 1 denote all unknown parameters in the model. In 

354 
Iyengar & Dey 
the present example we measure the discrepancy between simulated and observed 
data by the following loss function, 
d = E(ll Ynew - Yobs IFJI Yobs) 
The natural choice is then to adopt the model which minimizes d. Now, if y[ denotes 
values simulated from predictive distribution, a simulation based approach on B 
iterations to estimate d is given by, 
B 
A 
I"' * 
2 
d = B L; II Yt - Yl II 
1:1 
(5) 
On arriving at a subset of desirable models, inference may be performed with 
information from simulations. To generate a joint confidence region for constituent 
proportions, convex hulls defined by Monte Carlo estimates of these proportions are 
peeled to reveal the inner most region of desired volume. Since components add to 
one, instead of considering all G constituents, information on any g are considered 
sufficient. The hulls so generated clearly do not depend on which constituent has 
been dropped because convex hulls created by g proportions is an intersection of 
convex hulls in G dimensions with their analog in g dimensions. This procedure 
may not generate an exact 100(1- f)% (0 < f < 1) convex credible region due to 
the fact that points are peeled on the basis of extremity of hulls they lie in and as 
a result of which deleted hulls may account for approximately 100/% of points. 
4. 
A Semiparametric Approach 
We begin with an introduction to the GLD class and its properties. Let u : 
R~ ~ R+ be defined by u(x-a) = h{(~)b
1 +· · ·+(~)bg} where h(·) is continuous 
(almost everywhere) function from R+ toR+· LetS= {x-G = (x1, ... ,x0): Xi 2 
0, L:f=1 Xi :s; 1} denote the simplex subset of su. The generalized Liouville family 
is defined as follows. 
Definition 4.1: X-G E Sis said to have a GLD (with respect to Lebesgue measure) 
if the density of x_a is of the form 
g 
f(x-G) = A.u(x-a) IT xfi- 1 
for X-G E S and 0 otherwise, 
(6) 
i=1 
and is denoted by L0(h, a, b, q). Here a = (at, ... , a0 ), b = (b11 ... , b0 ) and q = 
( q1, ... , q0 ) are vectors of positive elements. A is the normalizing factor and u(x-G) 
is defined above. It is however sufficient for ( ~ )bi E R, \1 i = 1, ... , g and 
fsg h(x-G) nr=1 xfi- 1dx_G < 00. Clearly when h(x-G) = (1-L:f:1 Xi)aa-1, GLD 
corresponds to DD (a). 
Now, Gupta and Richards (1987, 1992a, 1992b) have studied dependence proper-
ties of Liouville family in the case f3i = qi = 1, \1 i = 1, ... , g. Rayens and Srinivasan 
(1991a) have demonstrated applicability of GLD class for compositional data anal-
ysis with some intuitive insight on the choice of h(·). However it remains unclear in 
their analysis, as to how one would incorporate covariates in their model. Typical 
choices discussed by Rayens and Srinivasan are h( r) = constant, h( r) = expw(r) 
with w( r) = TJ exp ( -r) (with TJ an unknown parameter) etc. The final choice of 
h( ·) is dictated by the desirable location of mode of the resulting density, L 0 in the 

20. Compositional Data 
355 
simplex, S. The density L9 can be inflated or depressed as a result of "pressure" 
applied by h( r) on the curved surface ( ~ )b 1 + ... + ( ~ 
)bg = r for each 0 < r < 1. 
ql 
qg 
Now, the functional form of L0 (see (6)) is such that it leaves room for further 
creativity in choice of h( ·) and it may be possible to include covariate effects in 
this part of L9. Consider expressing h(r) = g(ho(r), 7JZ) with z denoting covariate 
information (z could be a vector), 7] is an unknown parameter (vector) and g(·) 
represents the unknown function through which x is related to 7]Z. We note that 
this idea has a survival analysis flavor where the hazard function is modeled as 
above. Instead of intuitively assessing the nature of ho(·), we use semiparametric 
Bayesian methods to estimate h0 ( ·) assuming that the form of g( ·) is specified. First 
we partition the domain of ho(·), the unit interval [0, 1] into k arbitrary intervals by 
the partition P = { h, ... , I.d. We also assume that ho ( ·) has a piecewise constant 
form (see Breslow (1974)) i.e., ho( r) = ti for T E Ii. We use Dynamic methods 
for simultaneous estimation and smoothing of ho(·). From a Bayesian perspective 
smoothing and estimation algorithms can be derived as posterior mode estimators. 
The estimators may be viewed as discrete spline smoothers. In the example, we 
consider h(·) = ho(·)exp(17z). The monotonic form exp(7Jz) is chosen for the sake of 
mathematical tractability and diagnostic procedures are used to detect any inad-
equacies in this choice. Gelfand and Mallick (1995) have dealt with estimation of 
ho(·) in a different context with the functional form of g(·) unknown. However in 
this chapter, the accent is on demonstrating use of GLD class to fit compositional 
data with covariates and not so much the determination of g( ·). 
A first order regression model is used to determine ho ( T) (see Fahrmeir ( 1994) 
for reference.) Now, let h0(r) = tj forTE lj and 
f>i = logtj 
f>i = f>i -1 + ei, 
ei 1'-..J N(O, €1~) 
for j = 1, ... , k with t0 = 1. Further structure can be introduced in the evolution 
of E>j, for example a negative slope corresponds to decaying effects or €1~ = 0 would 
correspond to a constant slope. The above transition model allows a flexible struc-
ture for ho( r). The size of partition P is considered fixed here and the model is fit 
over several fixed sizes of P. Alternatively this quantity k may be taken random and 
Bayesian inference from posterior samples could be used as pointers for appropriate 
choices of k. The proposed model is fit with different sizes of the partition. We also 
fit the Dirichlet model for the sake of comparison and performance of these models 
is assessed via posterior predictive distributions. We use Conditional Predictive Or-
dinate (CPO) measures (see Pettit and Young (1990), Gelfand et al (1992)), along 
with Bayesian Information Criterion (BIC) (see Schwarz (1978)) to compare the 
models. Samples from posterior predictive distribution help compute conditional 
predictive ordinate values for the models. Now, the BIC criterion chooses the model 
that minimizes 
M 
-log(L(x;fl)) + 
log(n) 
where n is as defined earlier, fl denotes all model parameters and M denotes the 
number of model parameters. 
5. 
Posterior Distributions and Estimation 
As an example we analyze sand, silt, clay compositions (see Coakley and Rust 
(1968) and Aitchison (1986 page 359.) Depth at which these sediment compositions 

356 
Iyengar & Dey 
are sampled, has also been recorded. Here g = 2, n = 39 and p = 1. On visually 
inspecting the data it becomes clear that sand content has a downward trend with 
an increase in depth whereas silt follows an upward trend with increasing depths 
of water. Clay quantities also consolidate steadily with depth. Five samples violate 
unit sum constraint by fractional amounts but, this irregularity does not pose a 
problem as simulated quantities obey built-in unit sum structure. The data set is 
examined under the above parametric and semiparametric framework. 
Now, Equation (2.2) represents the relation between Yi and unknown parameters. 
Flat tailed distributions that model outliers can be a good starting point and in 
this example an exponential power family is examined as a means to model data. 
This family has received considerable attention as it includes a wide spectrum of 
continuous and symmetric distributions and under certain conditions, members of 
this family can be expressed as scale mixtures of normals. It also includes the 
multivariate normal and multivariate double exponential class as a special case. 
Now, the pdf of (symmetric) MultiVariate Exponential Power family (MVEP) is 
given by, 
EP(Yi I zi(J, a, v) = c0 IL:I- 112 exp -[co(Yi- a- Zi0)'L:- 1(Yi- a- ZiO)]v 
· 
· 
r(*) 
vcgl
2r(!) 
with kurtosis parameter v such that, 1/2::; v::; 1, co = r(f,;) and c0 = r(-f,;)n-g/ 2 • 
The kurtosis parameter v measures the degree of non-normality, when v = 1/2 
multivariate double exponential family is realized and at v = 1, the familiar multi-
variate normal family is defined. When 1/2 ::; v ::; 1, this family can be expressed 
as a scale mixture of multivariate normal density (see Choy (1995)). Under this 
distributional assumption on Yi, we fit a hierarchical model. Here especially, as 
functional forms encountered do not appear to be analytically navigable we adopt a 
sampling routine based on the Metropolis algorithm (see Choy (1995)) to generate 
simulated values from posterior distribution and information thus gained enables 
inference on desired quantities. To simplify representation of the model, some more 
notation is given here. Let o.j denote the jth column of the p X g matrix {} and let 
A = (At, ... , A0 ). Now, access to prior information is very desirable but may not 
always be possible and in this event non-informative priors or priors with suitable 
hyperparameters (so as to make the density sufficiently diffuse) may be used. The 
latter variety of priors are used in our analysis. For L: (unknown and positive definite 
matrix) we use a Wishart prior W0(m, M) with m 2:: g for the density to be well 
defined. We note that, in our analysis no restriction is made on functional form for 
L:. Now choosing m, the precision parameter small enough ensures that the Wishart 
prior has a large variance thereby making the density vague. Also this density for 
L: serves as a conjugate prior facilitating the sampling procedure. We choose ex-
ponential family densities with large variance parameters as priors for 0, a and A. 
Specifically we asssume that O.j,..... N(J-t1,u8lp) (1::; j::; g), a,..... N(ao1,ual0 ) and 
A,..... N(Ao1, O'>..lu) with hyperparameter J-t,..... N(O, r). Here I~c denotes the k x k iden-
tity matrix. A one-one linear transformation of kurtosis parameter, v from [1/2, 1] 
to [0, 1] is made and Beta( 1/2, 1/2) density is used as a prior for this transformed 
parameter denoted by h(v). Now the posterior distribution is proportio:aal to the 
following complex product of likelihood and prior, 
EP(yi I zi(}, a, a)N8(J-t1, O'(Jl0 )Na(ao1, O'al0 )Np,(v, r) 
x N,x(Ao1, u,x!0 )W0(m, M)Beta(1/2, 1/2) 
The resulting posterior distribution for parameters are fairly complicated and we 
employ the Gibbs sampling routine (see Gelfand and Smith (1990)) and a Metropolis-

20. Compositional Data 
357 
Hastings algorithm (see Metropolis et al. (1953)), a MCMC method to simulate 
from (posterior) distributions. We adopt the Odell and Feiveson (1966) procedure 
to sample from a Wishart density. The above steps are repeated to convergence 
and under mild regularity conditions, samples from complete conditionals approach 
samples from joint distribution for a sufficiently large number of iterations. Esti-
mates are based on convergent simulated values from 50,000 iterations. To ascer-
tain convergence, ten multiple chains with observations from every tenth iterate 
are taken around dispersed initial values and Potential Scale Reduction (PSR's) 
values (see Gelman and Rubin (1992)) are computed. When these values are close 
to one, convergence of the Markov chain to the target distribution is rendered im-
minent. All models considered shared this attribute with PSR values being nearly 
one in each case for all chains. We further subsampled the convergent chains and 
retained every fifth value to significantly reduce autocorrelation effects. Over the 
duration of analysis we tried several widely varying hyperparameters to ensure that 
inferences are data dependent. In results presented here the hyperparameters are 
O'a = O'(J = T = 10, 0'>. = 1, ao = Ao = v = 0, m = 5 and M = 1071. The MVEP 
model framework yields v = 0.95, which is sufficiently close to multivariate normal 
case where v = 1. Subsequent models were fit under the simpler assumption that 
Yi "' N(a + zi(}, E). Now with kurtosis parameter v fixed at one, prior density for 
other model parameters are as in MVEP case. Here results are shown in the normal 
case only with the above hyperparameters for the sake of clarity. 
Now, an estimate for f3i = (f3ib ... , f3iG) at Zi when A = (At, ... , Ag) :f. 0 is 
provided by 
for 1 ::; j ::; g and 
1 
+ .. · + [A; 8 (B~08 Zj + a; 8 ) + 
Here B represents number of iterations over which estimates are based and 1: 
represents valuthe e of unknown 1 at sth simulation. Zero values in A make the 
estimations simpler and can be deduced similarly. 
We now examine estimation and other issues in the above semiparametric model. 
Let p( 77), p( a), p(b), p( q) denote (independent) prior information for parameters. 
The product of likelihood and prior is of the form 
L(x: 0) = L0(h, a, b, q)p(7J)p(a)p(b)p(q) 
(7) 
Now, the posterior density of model parameters is proportional to the above prod-
uct. We note that in this case A in (6) is unknown and dependent on the covariate z. 
We focus primarily on the likelihood when prior information may not be adequate. 
As mentioned in the earlier section we use diffuse priors in this event ensuring that 
inferences are data dependent. We employ Gaussian priors for a suitable transfor-
mation of the parameters with precision matrix close to 0 so that a sufficiently 
diffuse prior is achieved. Now, analytic resolution of posterior derived from (7) is 
infeasible. A simulation based approach using a Markov chain Monte Carlo algo-
rithm to sample from the posterior distribution is implemented. In particular a 
Metropolis-Hastings (1970) version is used here with Gaussian proposal density for 

358 
Iyengar & Dey 
the parameters. As mentioned before since the parameters are assumed positive 
(see (6)) a transformation (logarithm) of parameters is sought to justify Gaussian 
prior assumptions for transformed parameters. 
Now, moments of GLD class exist and can be determined if we compute integrals 
of the form, 
g 
f h( {( Xl )bl + ... + ( Xg )bg}) IT x;i-ldx_a 
ls 
q1 
qg 
i=l 
In the particular case where bi = qi = 1, the above integral reduces to computing 
the one dimensional integral 
r(al) .. ·f(ag) 11 h( ) "'fi! a·-ld 
= 
T r.t...t•= 1 
' 
T. 
r(al + ... + ag) 0 
(8) 
(see Edwards (1922), Whittaker and Watson (1952)). The integral on the right can 
be calculated by a number of known methods. Therefore in the case bi = qi = 1, 
covariance function of the density may be evaluated analytically depending on the 
nature of h( · ). Now since h( ·) is of an unknown piecewise constant form the integral 
in (8) reduces to a sum of integrals to be evaluated over the intervals in partition 
p i.e., 
k 
r(al) ... r(ag) L f i~xp(17z)TE~::l ai-ldr 
r(al + ... + ag) i=l Jli t 
(9) 
Here ii represent estimates for ho( ·) over the partition. Estimates of other param-
eters determined suitably can be used in (9) to compute the covariance. In the case 
some bi :f= 1 or qi :f= 1, Monte Carlo techniques are available to evaluate the covari-
ance. This first entails being able to draw samples from GLD and in order to do so 
the following algorithm by Devroye (1986) can be used 
i. Generate (Y1, ... , Y9_1) f'V D9 ( a1 I b1, ... , a9 I b9 ) 
ii. Generate Y9 f'V L1(h, (al/bb ... , a91b9 ), 1, 1) independent of step (i.) 
iii. Define X;. = qi[YiY9jlfbi for 1 ::; i ::; g- 1 
1v. Define X 9 = q9((1- Y1 -
· · ·- Y9 )]lfbg 
v. (X1, ... ,X9) is L9(h,a,b,q) distributed 
The G LD model is fit and results from the situation when k = 4, k = 8 with 
P = {[ i?, i); 1 ::; j ::; k} and f3i = qi = 1 for 1 ::; i ::; g are presented. Further, in 
the analysis we also assume that h(r) = ho(r)exp((Jz). We also fit the DD(a) class and 
note that this class has no covariate representation as in GLD case. About 100,000 
iterations are run and one in every ten values is retained to minimize autocorrelation. 
The resulting chains are then tested for convergence via Raftery-Lewis (1992) diag-
nostic procedures. These diagnostics are satisfactory for all the chains, confirming 
convergence of samples to intended posterior distributions. The convergent chains 
are further sub-sampled with one in ten chosen to further mitigate autocorrelation 
effects, if any. Parameter estimates are based on the resulting samples. 

20. Compositional Data 
359 
6. 
Results 
With regard to the example considered, some results are summarized in Tables 
20.1- 20.4. We present two cases in the multivariate normal setup viz., Aitchison's 
ALR transformation with A = 0 (giving rise to logistic normal model) and the gen-
eral Box-Cox representation (see (2.1)). Aitchison's ALR model in the absence of 
depth yields /1 = (17.79, 56.38, 25, 83) and in Bayesian paradigm this model results 
in /1 = (18.04, 55.96, 26.0) and a = ( -0.363, 0.783). In the absence of covariate, 
classical estimates for {3 is just the normalized version of geometric mean of the 
data in G categories (because of unit sum constraint on proportions). Next we con-
sider models with covariate. Now, the Box-Cox transformed data are modeled in 
both classical and Bayesian setup and results are given in Table 20.1. Classical es-
timates presented in Table 20.1 are based on ideas to fit the model marginally (see 
Rayens and Srinivasan (1991b) as opposed to yielding (joint) g-dimensional multi-
variate normality (as in Bayesian inference). Consonance between the two methods 
of inference in this case is therefore not very satisfactory. Parameter estimates (not 
shown here) under the MVEP assumptions agree fairly well with Bayesian estimates 
of Table 20.1 for the normal model and this is anticipated since v = 0.95 in MVEP 
case. 
Now, consider diagnostic measures (see (3.1) and (3.3)) obtained for the five 
models in Table 20.2. On surveying p values, it is clear that models without co-
variate perform poorly in terms of explainability and these models can only be 
used as pointers. We also note that estimates of posterior predictive loss indicate a 
preference for the model with covariate and non-zero A. The MVEP model yields 
(p, d) = (0.5693, 3.79) and this model demonstrates an agreement with covariate as-
sisted A -:j:. 0 model on parameter estimates and performance diagnostics. However, 
complications involved in computations under MVEP framework do not offer any 
additional benefits when compared to its multivariate normal counterpart. Further-
more, estimates of v at 0.95 for the MVEP case goes one more step to instill faith 
in the subsequent multivariate normality assumption. 
Convex credible regions (not shown here) for the proportions were generated at 
z = 0, lowest depth at 10.4 m, mean depth at 48.04m and finally at largest depth 
at 103. 7m. Over the range of depth values considered it was evident that the model 
with non-zero A gives rise to more compatible convex credible region i.e., for instance 
with increasing depth the decrease in sand content is readily noticeable. Also this 
model gives rise to convex credible regions of comparatively smaller volume. One 
more visual aid to judge adequacy is given by the Bayesian standardized residual 
plots (not shown here) for the four models seen in Table 20.2 under normality. Again 
models with covariate have smaller distances and among the three models with 
depth as an explanatory variable, the situation with non-zero A is more attractive. 
Although Tfr values in models including covariate effect are comparable, observed 
versus predicted discrepancy estimates clearly indicate a preference for the case 
with non-zero A. In both these cases we may want to perhaps include a non-linear 
function of covariate but, model adequacy considerations do not exhibit an oversight 
in this regard. Even if intercept term a at (2.178, 0.618) dominates relation (2.2) 
over depth coefficient 0 at ( -0.066, -0.005) there are distinct diagnostic advantages 
to models including covariate. At this stage there is sufficient evidence to conclude 
that the Box-Cox Bayesian model with covariate is better than the simpler (ALR) 
version with A = 0. Now, credible regions are a visual aid to study behavior of data 
and unlike ternary diagrams used in the past by geologists, these convex credible 
regions may be created for any given value of Zi, regardless of dimension of Zi 

360 
Iyengar & Dey 
TABLE 20.1. Non-zero .A with covariate 
Parameter estimated II Classical inference II Bayesian method II 95% Credible set 
i:¥1 
2.47 
2.178 
( 1.43:1, 3.037) 
(¥2 
0.77 
0.618 
(0.489, 0.766) 
tit 
-0.09 
-0.066 
-0.083, -0.054 
02 
-0.009 
-0.005 
-0.006 -0.003 
..\1 
-0.53 
-0.:139 
-0.502, -0.097 
..\2 
-0.009 
-2.383 
-2.942 -1.527 
TABLE 20.2. Estimates of pr, d 
Model II With covariate II Without covariate 
ll ~ 8 II 8:gai5: l~5 II 
o~7~~
7
o~·1
9
o\
0
g 
in our model. However if g, the dimension of Xi is greater than three, it is not 
very easy to discriminate patterns in cloud of points in credible region plots. Now, 
Aitchison's (ALR) model without covariate does not yield satisfactory diagnostic 
measures i.e., his marginal tests do not support the claim to logistic normality. 
And further inclusion of a linear function of covariate also does not gain diagnostic 
support. Aitchison finally concludes that a logarithmic function of depth is possibly 
more suitable, and here he admits that even though logistic normality is not fully 
validated, it is more reasonable. Further, even this model produces unexplainable 
outliers (see Aitchison (1986) for details). 
We now discuss some results for the semiparametric model. As mentioned before 
we fix k = 4 and k = 8 and compare these with the Dirichlet model. Estimates 
of ho(·) for the two partitions are {1.07, 1.1, 1.13, 1.14} and {1.0, 1.01, 1.02, 1.02, 
1.03, 1.03, 1.03, 1.03}. These values indicate that ho(·) is roughly constant on its 
domain. Table 20.3 contains estimates of a, 'TJ for these two cases. 
Table 20.3: Parameter Estimates for GLD case 
Parameter I 
k=4 
k=8 
Since the two cases with k = 4 and k = 8 do not produce conflicting estimates, we 
retain the model with k = 8 for comparison and diagnostic procedures. In Table 
20.4 estimates (expected value and credible set) for concentrations of sand and 
silt under GLD (k = 8) and DD(a) models are given. We compute estimates for 
the GLD model when depth is 48.04 m (average value). However for the DD(a) 
model these estimates do not include covariate values. Now, empirical estimates of 
covariance (sand, silt) turns out to be -0.02. For GLD case this is -0.02 (when depth 
is 48.04 m) but this quantity is -0.055 in the Dirichlet case. We calculate these 
values analytically from results in (9). The estimates from the GLD model are more 
compatible and this model also provides reasonable estimates of the covariance 
matrix. The results also indicate that the GLD family captures variability (in the 
data) better than Dirichlet class. 

20. Compositional Data 
361 
. Table 20.4: GLD (k ~ S, z = 48.04m) vs. D0 (a) 
I Model I E(Sand), 95% credible set I E(Silt), 95% credible set I 
I GLD J 
0.162, (0.155, 0.169) 
0.554, (0.540, 0.563) 
I DD(a) I 
0.192, (0.181, 0.196) 
0.561, (0.538, 0.562) 
Next, we outline diagnostics for these models. Now, CPO values for the GLD model 
are much better than those for the DD(a) (not shown here). BIC values for GLD and 
DD (a) models are 4.55 and 11.06 respectively. The diagnostic measures considered 
provide more support to GLD model than the Dirichlet model. 
7. 
Conclusion 
Compositional data can be modeled via the Box-Cox tranformation model and 
this approach has a GLIM like flavor. The GLD class can also be used to model 
compositional data on simplex and the density can be adapted to include the in-
fluence of independent variables on compositions. In the event of (at least two) 
missing observations in a composition in Bayesian framework, the unknown quan-
tities may be treated as parameters with suitable priors and above computations 
can be adapted. It remains to be seen whether in general, inferences change when 
components are permuted under the Box-Cox transform. In this example however 
no major discrepancies in inferences (estimates of proportions, convex credible sets 
for f3i etc.) were detected. 
References 
Aitchison, J. (1982). The Statistical Analysis of Compositional Data. Journal of 
Royal Statistical Society, B, Vol 2, 139-177. 
Aitchison, J. (1985a). A General class of Distributions on the Simplex. Journal of 
Royal Statistical Society, B, Vol. 47, 136-146. 
Aitchison, J. (1986). The Statistical Analysis of Compositional Data. Chapman 
and Hall. 
Aitchison, J. and Shen, S.M. (1985b). Logistic-Normal distributions: Some prop-
erties and uses. Biometrika, Vol. 47,136-146. 
Breslow, N. (1974). Covariate analysis of censored survival data. Biometrics. Vol. 
30, 89-99. 
Choy, S. T. B. (1995). Robust Bayesian Analysis Using Scale Mixture of Normals 
Distributions. PhD Thesis, Department of Mathematics, Imperial College. 
Coakley, J. P. and Rust, B. R. (1968). Sedimentation in an Arctic lake. Journal of 
Sedimentary Petrology, Vol. 38, 1290-1300. 
Devroye, L. (1986). Non-uniform random variate generation, New York: Springer 
Verlag. 
Edwards, J. (1922). A treatise on the Integral Calculus, New York: Macmillan, 
Vol. II. 

362 
Iyengar & Dey 
Fahrmeir, L. (1994). Dynamic modeling and penalized likelihood estimation for 
discrete time survival data", Biometrika, Vol. 81, 317-330. 
Gelfand, A. E. and Dey, D. K. and Chang, H. (1992). Model determining us-
ing predictive distributions with implementation via sampling-based methods 
(with discussion), Proceedings of the Fourth Valencia International Meeting 
on Bayesian Statistics, Oxford University Press, eds. J. M. Bernardo and J .0. 
Berger and A.P. Dawid, 147-167. 
Gelfand, A. E. and Mallick, B. K. (1995). Bayesian Analysis of Proportional Haz-
ards Models Built from Monotone Functions, Biometrics Vol. 51, 843-853. 
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling based approaches to cal-
culating marginal densities. Journal of the American Statistical Association, 
Vol. 85, 398-409. 
Gelman, A., Meng, X. L. and Stern, H. S. (1996). Posterior Predictive Assessment 
of Model Fitness via Realized Discrepancies (with discussion). Statistica Sinica 
Vol. 6, 733-807. 
Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using 
multiple sequences. Statistical Science, Vol. 7, 457-511. 
Gupta, R. D. and Richards, D. St. P. (1987). Multivariate Liouville Distributions, 
Journal of Multivariate Analysis, Vol. 43, 233-256. 
Gupta, R. D. and Richards, D. St. P. (1992a). Multivariate Liouville Distributions, 
II.Probability and Mathematical Statistics Vol. 12, 291-309. 
Gupta, R. D. and Richards, D. St. P. (1992b). Multivariate Liouville Distributions, 
III. Journal of Multivariate Analysis Vol. 43, 29-57. 
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and 
their applications. Biometrika, Vol. 57, 97-109. 
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N. and Teller, A. H. (1953). 
Equations of state calculations by fast computing machines. Journal of Chem-
ical Physics, Vol. 21, 1087-1091. 
Odell, P. L. and Feiveson, A. H. (1966). A numerical procedure to generate a 
sample covariance matrix. Journal of the American Statistical Association, 
Vol. 61, 199-203. 
Pettit, L.I. and Young, K. D. S. (1990). Measuring the effect of observation on 
Bayes factors. Biometrika Vol. 77, 455-466. 
Raftery, A. E. and Lewis, S. M. (1992). How many iterations in the Gibbs sam-
pler? Proceedings of the Fourth Valencia International Meeting on Bayesian 
Statistics Oxford University Press, ed. J .M. Bernardo and A.F.M. Smith and 
A.P. Dawid and J .0. Berger. 
Rayens, W. S. and Srinivasan, C. (1991). Box-Cox Transformations in the Analysis 
of Compositional Data. Journal of Chemometrics, Vol. 5, 227-239. 
Rayens, W. S. and Srinivasan, C. (1991). Estimation in Compositional Data Anal-
ysis. Journal of Chemometrics Vol. 5, 361-374. 

20. Compositional Data 
363 
Rayens, W. S. and Srinivasan, C. (1994). Dependence Properties of Generalized 
Liouville Distributions on the Simplex. Journal of the American Statistical 
Association, Vol. 89, 1465-1470. 
Schwarz, G. (1978). Estimating the dimension of a model.Annals of Statistics, Vol. 
6, 461-464. 
Tierney, L. (1994). Markov Chains for Exploring Posterior Distributions (with 
discussion). Annals of Statistics, Vol. 22, 1701-1762. 
Whittaker, E. T. and Watson, G. N. (1952). A course in modern analysis. Cam-
bridge: University Press. 

This Page Intentionally Left Blank 

21 
Classification Trees 
D.G.T. Denison 
B.K. Mallick 
ABSTRACT The generalized linear model framework is often used in classification 
problems and the importance and effect of the predictor variables on the response is 
generally judged by examination of the relevant regression coefficients. This chapter 
describes classification trees which can also be used for performing classification but 
lead to more appealing 'rule-based' models which are simple to interpret. We utilise 
Markov chain Monte Carlo methods to formulate a Bayesian 'search' strategy to find 
good tree models which, in general, outperform the usual tree search methods. 
1. 
Introduction 
This chapter deals with the general classification problem. We wish to find a 
model, using a training set of data, which we can use to predict the (categorical) 
response of future observations given just their covariates. Using classical general-
ized linear model (GLM) theory for categorical data (Chapter 5, McCullagh and 
Neider, 1989) is restrictive and the resulting model can be difficult to interpret. 
Due to these problems interest in performing classification using tree-based models 
has grown, especially by applied statisticians. The ease of interpretability and sim-
ple output of the model, especially its rule-based nature, is particularly appealing. 
This leads to definite classes being assigned to each datapoint rather than possible 
class probabilities. While this may not seem to incorporate the uncertainty in class 
assignments its definiteness is seen as important to decision makers (e.g. doctors, in-
surance salesmen) even though this may not appear completely reasonable to more 
theoretical statisticians. 
Classification trees (Breiman et al., 1984) aim to model the unknown true regres-
sion function g when the data come from the relationship 
Yi = g(xi) 
i = 1, ... , n, 
where the responses Yi E {0, 1, ... , C} refer to the category of the datapoint with 
predictors Xi. Note that the most general classification problem assigns classes nom-
inally so that the numbers given to the classes should not be thought of as distances 
between them. Also, classification trees can easily handle not just ordinal, but also 
categorical, predictor variables. 
In the classification tree case we estimate g by the function g, which we can write 
as 
k 
g(X) = E(YIX) = E f3iBi(X), 
(1) 
i:=l 
where the Bi are the basis functions of the model, Y is the vector of responses 
and X the matrix of the predictor variables. Note that, unlike the standard GLM 
approach the responses are not transformed using a link function. 
Nonparametric models use the data to find their form. In many cases these can 
be written as in equation (1.1), where the only differences between the models is in 
365 

366 
Denison & Mallick 
the form of the basis functions. In this chapter we will demonstrate the potential 
advantages that can be achieved using Bayesian methods to find these basis func-
tions Bi. Classically this is done by performing a stepwise deterministic search of 
the model space induced by the basis functions (Breiman et al., 1984) which we 
shall describe later. However, this only optimises the form of the basis function at 
each step of the algorithm in a 'greedy' manner and does not jointly 'train' them. 
It has been shown (Denison, Mallick and Smith, 1998; Chipman, George and 
McCulloch, 1998) that better (in a predictive sense) classification trees can be found 
using a Bayesian search strategy. Unlike many of the chapters in this book where 
a full posterior distribution of the parameters of interest is recovered, often by 
simulation using Markov chain Monte Carlo algorithms, we use the same techniques 
just to find 'good' models knowing that generation from the full posterior is unlikely, 
except in the simplest of cases. 
2. 
The Classification Tree Model 
2.1 
The Basis Functions 
X1>4.8 
ctJ 
FIGURE 21.1. Example tree structure. The classification and the fraction of misclassified 
points is given inside and below each node, respectively. 
In any nonparametric model the basis functions play an important role in deter-
mining both the predictive power and the interpretability of the model. We shall 
introduce the basis functions used in classification models via a binary regression 
example and the tree given in Fig. 1. 
Suppose that there are 50 datapoints classified either as being in class 0 or 1. For 
illustration we chose 10 to be in class 1 with the other 40 in class 0. Associated with 
each datapoint are three predictor variables X = (X1, X2, X3). Suppose that X1 and 
x2 are ordinal variables taking continuous values on (0,10] and x3 is nominal with 
10 categories indexed {0, 1, ... , 9}. The aim is to find good basis functions, which 
is equivalent to finding good splitting questions (at the elliptical splitting nodes) 
involving X, in order to increases within-node homogeneity at the terminal nodes 

21. Classification Trees 
367 
(represented by rectangular boxes). The class associated with a given terminal node 
is the one which has the most members in that node and if there are equal numbers 
between some classes then the assignment is done randomly between them. 
In this example we can see that the tree has fitted well as the fraction of mis-
classified points, given below each node in Fig. 1, is small at the terminal nodes. 
It would be possible to carry on splitting the tree until there are no misclassified 
points but this would lead to the model overfitting so it is common to restrict the 
tree from allowing too few datapoints in any terminal node. 
The basis functions which describe the tree structure in Fig. 1 are 
B1 
= H[X2 :$ 8]H[X1 :$ 4.8], 
B2 = H[X2 ~ 8]H[X1 > 4.8], 
Ba = H[X2 > 8), 
where H is the Heaviside function which takes the value 1 when the statement is 
true and 0 otherwise. The corresponding bottom node assignments are /3 = (0, 1, 0). 
Note that X 3 was not used in the fit, this suggests that it is not relevant. However, 
if it was relevant because it is a categorical variable its basis functions would be of 
the form H[Xa E {0, 2, 5}]H[·]. 
As mentioned before, the problem of finding good classification trees is exactly 
the same as finding good questions to ask at each of the binary splitting nodes but 
we must also ensure that the tree does not overfit the data. 
2. 2 
The Classical Approach 
The usual approach to finding classification trees involves a deterministic search 
of the model space induced by the trees. In all but the simplest of cases this search 
cannot be undertaken exhaustively so restrictions in the manner it is undertaken 
are required. In common with other similar nonparametric methods (see Friedman 
(1991) for a review) an algorithm that incorporates stepwise addition followed by 
stepwise deletion is used. This approach is called greedy because it optimises the 
form of the tree at each step. Thus the basis functions are trained stepwise and not 
together which leads to many good models being missed in this type search. An 
outline of the algorithm is given below. 
Classical Algorithm 
1. Start with a tree with no splitting nodes present. 
2. Work out the lack-of-fit criterion when each possible split is made. 
3. Execute the split which most decreases the lack-of-fit. 
4. Repeat 2 and 3 until a large enough tree has been grown. 
5. Find the lack-of-fit when each basis in the model is, in turn, deleted. 
6. Delete the basis which most improves the fit. 
7. Repeat 5 and 6 until the lack-of-fit criterion reaches a minimum. 

368 
Denison & Mallick 
2.3 
The Bayesian Approach 
We propose a model which can be used to set up a probability distribution over 
the set of possible trees. 
Any binary tree-based model can be uniquely defined by the positions of the 
splitting nodes present, together with the variables and points where these variables 
are split. We define these variables respectively as sf
08
, syar and siule. The only other 
unknown parameter is the number of splitting nodes in the model. When there are 
k terminal nodes in the tree we can show that there are always k- 1 split nodes so 
we also use k to define the tree structures. 
The tree in Fig. 1 is useful for helping to understand the role of the variables 
(J(k) = (si
08
, syar, sJ:ule, ... , sk~i). In this case we know k = 3 and we write {) = 
(1, 2, 8, 2, 1, 4.8) as we define sP 08 = 1 for the splitting position at the root node and 
sP08 = 2t at the left-most possible split position at the fth level below the root node. 
The other possible split positions are labelled consecutively, left to right, along each 
level of the tree. 
To specify a Bayesian model we must assign priors to the unknown variables. We 
put a Poisson prior (restricted to be greater than 0) with parameter ,\ over k with 
uniform priors over the other unknowns to set up a prior probability distribution for 
the model. These priors are chosen reflecting the knowledge that we know little of 
the structure a priori except that we want to penalise large trees more than smaller 
ones, hence the Poisson prior over the number of terminal nodes. 
Assuming that each datapoint comes from a multinomial distribution then we 
can write down the likelihood of the data 
where ni is the number of datapoints in the ith terminal node with ffiij the number 
of datapoints in class j and T min is the minimum number of datapoints allowed 
in a terminal node. Henceforth we shall take T min = 5. By standard analytical 
simplification when using the reference Dirichlet prior for the Pij we find that we 
can integrate our the Pij from the unknown parameters (Chipman et al., 1998) in 
the model giving the marginal likelihood (assuming all the nodes have more than 
T min datapoints) as 
k { n m .. l} 
(k) 
j 
Zj • 
p(YIX, k, (J 
) =IT ( . C)' . 
i=l 
nz + . 
(2) 
Now we can construct a sampler over the varying dimensional parameter space 
using the reversible jump MCMC algorithm proposed by Green (1995). We must 
combine move types, some within a dimension and some between dimensions, to 
form a hybrid sampler (Tierney, 1994). Each move type is randomly chosen ac-
cording to a defined proposal structure which depends on the number of terminal 
nodes currently in the model. The move types we choose are: splitting a terminal 
node (a birth step); recombining adjacent terminal nodes (a death step); changing 
a splitting variable at a node and changing just the rule of a split at a node. The 
last two moves types do not change the dimension of the model so we can use a 
simple Metropolis-Hastings step (Metropolis et al., 1953; Hastings, 1970) to work 
out the acceptance probability of the proposed model in these cases. 

21. Classification Trees 
369 
Bayesian Algorithm 
1. Start with a tree with no splitting nodes present. 
2. Set k equal to the number of terminal nodes in the present tree. 
3. Attempt to perform either a birth, death, change of split variable or change 
of split rule move type (the probability that each move type is attempted is 
usually a function of k ). 
4. Repeat 2 until a sufficient sized sample of trees has been collected, ignoring 
the initial burn-in period when the log marginal likelihood of the trees varies 
wildly. 
Unfortunately the hierarchical nature of the classification tree basis functions 
(proposed new basis functions depend on the bases already in the model) leads to 
problems in simulating from the posterior distribution of (k, (J(k)). When the tree in 
the chain has grown to a reasonable size proposed changes in splitting questions near 
the top of the tree are unlikely to be accepted. This is because changing a question 
here alters many basis functions which affect lower branches in the structure. As 
all these bases have been trained together changing many of them significantly in a 
random way gives poor acceptance rates. This problem is exacerbated because, after 
large well-fitting trees have been found, the chain spends little time at structures 
with very few nodes and these are the only times when changes in the top nodes are 
likely. Thus sampling from the full posterior distribution of tree structures currently 
seems infeasible so we must content ourselves with performing only a stochastic 
search for good tree structures using our algorithm. 
One way to view the generated sample of trees is that it comes from an approx-
imate posterior distribution of tree structures conditional on the top few splitting 
nodes. So, due to the importance of the top few splits, to produce a sample of 
'good' structures we restrict the tree from growing more than some small number 
of terminal nodes (e.g. 6) at the beginning of the burn-in period. Thus reasonable 
initial splits are found and the tree does not go quickly down poor 'blind alleys'. 
For a fuller exposition of the methodology refer to Denison et al. (1998). 
3. 
Real data example 
We illustrate the Bayesian classification tree methodology on the breast cancer 
data studied in Breiman (1996) and Chipman et al. (1998). The aim is to predict 
whether a patient has a benign (B) or malignant (M) tumour based on nine pre-
dictor variables which are all numeric and take values on {1, ... , 10}. Of the 699 
observations, 16 had missing values and these were removed before the analysis was 
undertaken. Note that the data can be obtained from the Irvine repository database 
at the University of California 
( 'ftp: I /'ftp. ics. uci. edu/pub/machine-learning-databases) and it was origi-
nally studied in Wolberg and Mangasarian (1990). 
We took the independent sample of trees to be every 5th model visited from the 
last 300,000 iterations of the chain after an initial10,000 burn-in iterations. We took 
the Poisson mean for the number of terminal nodes, the only user-set parameter in 
our model, to be 2.7 to penalise complex trees (for explanation see Denison, Smith 
and Mallick, 1998). 

370 
Denison & Mallick 
FIGURE 21.2. The tree structure with the largest LML using the breast cancer dataset and 
the Bayesian search for classification trees. It has 10 terminal nodes and a misclassification 
rate of 13/683. 
Comparing the trees in the generated sample is not a trivial problem and many 
different approaches could be taken (see Chipman et al. (1998) and related dis-
cussion). We do not discuss this point further in this paper and just use the log 
marginal likelihood (LML), which can be easily found from (1.2), for comparison. 
In Fig. 2 we display the tree structure from the generated sample which has the 
highest LML. It has a very low misclassification rate (13 points out of 683) and only 
6 of the 9 predictors are used for the classification. 
Table 1 gives more details of our analysis of the dataset. It is clear that although 
we do not sample from the true posterior distribution many different trees with 
varying numbers of nodes are visited. Also the misclassification rates of our trees 
compare very well with those found using the 'greedy' method. Using the Splus 

21. Classification Trees 
371 
Table 21.1. Analysis of the Breast Cancer dataset. Posterior weights along with 
the largest LMLs of the trees with each number of terminal nodes. The number of 
misclassified points corresponds to the trees with the largest LML. 
Number of 
Posterior 
LML 
Number 
Bottom nodes 
Weight 
misclass. 
< 8 
0.002 
-70.1 
17 
8 
0.041 
-65.7 
15 
9 
0.147 
-63.4 
13 
10 
0.244 
-61.7 
13 
11 
0.248 
-61.8 
13 
12 
0.180 
-61.6 
12 
13 
0.097 
-62.0 
10 
14 
0.031 
-63.4 
12 
> 14 
0.010 
-65.3 
10 
function tree (Clark and Pregibon, 1992) we fitted greedy trees to the same data 
and found that all the trees with 11 or fewer terminal nodes misclassified at least 
22 points and for tree with between 12-14 terminal nodes 20 points were still mis-
classified. This demonstrates that the Bayesian method tends to also can find more 
parsimonious representations of the data than the greedy approach. 
4. 
Discussion 
Although classification trees have obvious appeal due to their simple represen-
tation through tree diagrams, they do not truly reflect our underlying beliefs about 
the data. They do not produce continuous models and splits are made perpendic-
ular to the coordinate axes in the predictor space. Some current research involves 
generalising these tree models, using a Bayesian framework, to overcome some of 
these difficulties. In particular, the Bayesian partition model (Holmes, Denison and 
Mallick, 1998) allows the predictor space to be split up with more flexible region 
boundaries and can be thought of as having many splitting questions at the root 
node off which many terminal nodes are linked. This makes the exploration of the 
full posterior distribution of the model structure possible, in contrast to the Bayesian 
classification tree, because of the lack of hierarchy in the model. 
References 
Breiman, L., Friedman, J .H., Olshen, R. and Stone, C.J. (1984). Classification and 
Regression Trees. Belmont, CA: Wadsworth. 
Breiman, L. (1996). Bagging predictors. Machine Learning, 24, 123-140. 
Chipman, H., George, E.I. and McCulloch, R.E. (1998). Bayesian CART model 
search (with discussion). J. Am. Statist. Assoc., 93, 937-960. 

372 
Denison & Mallick 
Clark, L.A. and Pregibon, D. (1992). Tree based models. In Statistical Models in 
S, (Ed. J.M. Chambers and T.J. Hastie), pp. 377-420. Pacific Grove, CA: 
Wadsworth. 
Denison, D.G.T., Mallick, B.K. and Smith, A.F.M. (1998). A Bayesian CART 
algorithm. Biometrika, 85, 363-377. 
Denison, D.G.T., Smith, A.F.M. and Mallick, B.K. (1998). Comment on Bayesian 
CART model search by H. Chipman, E.l. George and R.E. McCulloch. J. Am. 
Statist. Assoc., 93, 954-957. 
Friedman, J .H. (1991 ). Multivariate adaptive regression splines. The Annals of 
Statistics, 19, 1-141. 
Green, P.J. (1995). Reversible jump Markov chain Monte Carlo computation and 
Bayesian model determination. Biometrika, 82, 711-732. 
Hastings, W.K. (1970). Monte Carlo sampling methods using Markov chains and 
their applications. Biometrika, 57, 97-109. 
Holmes, C.C., Denison, D.G.T. and Mallick, B.K. (1998). Bayesian partitioning 
for classification and regression. Technical report. Imperial College, London. 
McCullagh, P. and Neider, J.A. (1989). Generalized Linear Models. London: Chap-
man and Hall. 
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E. 
(1953). Equations of state calculations by fast computing machines. J. Chern. 
Phys., 21, 1087-1091. 
Tierney, L. (1994). Markov chains for exploring posterior distributions (with dis-
cussion). Ann. Statist., 22, 1701-1762. 
Wolberg, W.H. and Mangasarian, O.L. (1990). Multisurface method of pattern 
separation for medical diagnosis applied to breast cytology. Proceedings of the 
National Academy of Sciences, 87, 9193-9196. 

22 
Modeling and Inference for 
Point-Referenced Binary Spatial 
Data 
Alan ~. Gelfand 
N alin1 Ravishanker 
Mark D. Ecker 
ABSTRACT The problem of modeling and analyzing point referenced binary spa-
tial data is addressed. We formulate a hierarchical model introducing spatial effects 
at the second stage. Rather than capturing the second stage spatial association using 
a Markov random field specification, we employ a second order stationary Gaussian 
spatial process. In fact, we introduce a convenient latent Gaussian spatial process 
making our observed data a realization of an indicator process on this latent process. 
Convenient analytic results ensue. Working with a Gaussian process specification in-
troduces the need for matrix inversion to implement likelihood or Bayesian inference. 
When the number of sites is large, high dimensional inversion is required which is 
slow (or possibly infeasible) and subject to inaccuracy and instability. We propose an 
alternative approach replacing inversion with simulation by introducing a suitable 
importance sampling density. We illustrate with the analysis of a set of repeat sales 
of residential properties in Baton Rouge, Louisiana. 
1. 
Introduction 
We consider the problem of modeling and analyzing binary response data collected 
at sampling locations in a fixed region D. A critical objective of the analysis is to 
assess spatial association between the observations. Let { 8i : 8i E D ~ Rd} for 
i = 1, ... , n denote a set of spatial locations, let Y ( 8i) denote the binary response 
at site 8i and let Y = (Y(81), Y(82), ... , Y(8n))'. Associated with each site 8i 
will be a vector X(8i) which can reflect both spatial and nonspatial information 
associated with the site. 
Our approach is to define a binary stochastic process, Y(8), over D. This requires 
the specification of the joint distribution of Y(81), Y(82), ... , Y(8n) for any nand 
for any set of locations 81, 82, ... , 8n given n. 
Formally, our modeling approach is included within that described in Diggle, 
Tawn and Moyeed (1998). That is, we conceptualize a latent stationary Gaus-
sian spatial process W(8) over D such that W = (W(81), W(82), ... , W(8n))' 
,..., N(O, u2 H(6)) where (H(6))ij = p(8i- 8j; 6), p being a valid correlation function 
in R 2• In addition, we introduce a vector of regression coefficients, {3, associated 
with X(8i). Then, we model the Y(8i) as conditionally independent given f3 and 
W. In fact, we assume 
(1) 
for a specified link function h. 
To obtain several useful analytic results and to facilitate computations, we envi-
sion an underlying Z(8i) = X(8i)'f3+W(8i)+t(8i) where the t(8i) are iid N(O, r 2). 
That is, t(8) is a white noise or measurement error process introduced apart from 
373 

374 
Gelfand, Ravishanker & Ecker 
the spatial process W(8) (see, e.g., Cressie, 1993 or Diggle, Liang and Zeger, 1994). 
Then, we set Y(8i) = 1 or 0 according to whether Z(8i) > 0 or < 0, respectively. 
That is, Y(8) is an indicator process associated with Z(8). As with the Y(8i), the 
Z(8i) are conditionally independent given~ and W. 
Considering the Y(8i)'s or the Z(8i)'s, we have formulated a hierarchical model 
with a conditionally independent first stage and a model for the ~ and for the 
spatial effects provided at the second stage. The parameters of the W( 8) process, 
o-2 and 6, become hyperparameters requiring a third modeling stage. The advantages 
to recognizing this hierarchical structure when W is modeled through a Gaussian 
process appear to have only been recently noticed in the literature, see, e.g., Diggle, 
Tawn and Moyeed (1998) or Ecker and Gelfand (1997). This contrasts with the use 
of Markov random field specifications for the W(8i) which have a longer history and 
consequently, a richer literature. See, for example, Besag, York and Mollie (1991) 
or Clayton and Bernardinelli (1992). 
This is not surprising. The latter approach models the conditional distribution 
W(8i)IW(8j ), j :f. i or equivalently, the inverse of the covariance matrix (Be-
sag, 1974; Bernardinelli and Montomoli, 1992) and is well suited for likelihood 
or Bayesian inference and in particular for simulation based model fitting using 
Gibbs sampling (Gelfand and Smith, 1990). Possible disadvantages are the fact 
that the covariance matrix is typically not of full rank; the density for W is not 
proper much less Gaussian. Even if the inverse covariance matrix is proper, off di-
agonal entries measure conditional associations between pairs W(8i) and W(8j) 
given W(8k), k f; i,j usually through an arbitrarily chosen neighborhood structure. 
Unconditional association between W(8i) and W(8j) comes from the resulting co-
variance matrix and need not be monotonically decreasing with distance (Besag and 
Kooperberg, 1995). Furthermore, there is no associated spatial process W(8) on D. 
Modeling spatial dependence continuously (perhaps monotonically) by analogy 
with classical geostatistics may be more appropriate in many applications. We may 
be interested in the variogram, the sill, the nugget, the range, possible anisotropy, 
kriging, etc. Unfortunately, modeling the covariance matrix requires matrix inver-
sion when evaluating the density for W which is needed for likelihood or Bayesian 
inference. Such inversion can be slow, perhaps infeasible, as n increases. More impor-
tantly, rounding error resulting from the numerous arithmetic operations becomes 
an issue; we can not be confident in the accuracy or stability of the resulting inverse. 
This problem arises in the more general setting of Diggle, Tawn and Moyeed (1998), 
who propose an arbitrary generalized linear model at the first stage for the Y ( 8i). 
They do not seem to acknowledge it, perhaps because they use at most moderate 
sample sizes. In fact, they employ a Gibbs sampler to fit their models, which would 
appear to exacerbate the inversion problem 
Recognition of this difficulty accounts for the preponderance of non-model based 
work in the case of observed Z(8i)'s. Variogram modeling becomes a curve fitting 
exercise. Kriging, including universal kriging, becomes a constrained optimization 
problem. Unfortunately, in the absence of a stochastic specification for the Z's, 
inference is limited to point estimation. In the case of binary response data, such 
nonparametric approaches are modified to accommodate indicator tranformations 
of the data. Indicator variograms (Cressie, 1993) and indicator kriging (Journel, 
1983) have been proposed though theoretical justification is weak (Cressie, 1993; 
Papritz and Moyeed, 1997). 
De Oliveira (1997) also models Y(8i) = 1 or 0 according to whether Z(8i) > 0 
or < 0, calling Y(8) a clipped Gaussian field. He does not, however, introduce the 
white noise process c( 8 ). As a result, no conditional independence for the Y( 8) is 

22. Point-referenced binary spatial data 
375 
obtained; the joint distribution of Y arises as an n dimensional integration of the 
density of W over a subset of Rn. De Oliveira (1997) introduces latent Z(si)'s in 
order to run a Gibbs sampler to avoid such integration. This requires sampling the 
full conditional distributions for the Z(si)'s which again introduces repeated matrix 
inversion. 
Hence, one additional contribution of this paper is to suggest that matrix inver-
sion can be replaced with suitable simulation using importance sampling. Matrix 
inversion is an order n3 operation. In its most polished form, our approach re-
duces computation to order n, making it feasible to provide approximate Bayesian 
inference for larger spatial datasets than previously considered in the literature. 
Indeed, because our approach runs much faster than performing inversions, it may 
be preferable even for moderate n. 
Returning to the binary spatial process example, questions of interest involve the 
strength of spatial association between sites. This can be investigated in several 
ways. Focusing on a pair of sites Si and Sj results in a 2 X 2 table for the possible 
outcomes of the pair (Y(si), Y(sj )). The four cell probabilities can be used to 
compute covariances (or correlations) as well as odds ratios (or log odds ratios). 
As functions of the model parameters, these quantities are random variables having 
posterior distributions. These posteriors can be used to infer (e.g., using the median 
together with lower and upper quantiles) about strength of association. Also, there 
is a latent variogram, in fact as we shall see, a renormalized variogram associated 
with W(s) + t(s), which again has a posterior distribution that can be obtained 
to learn about the association in the latent Gaussian process. Another question of 
interest involves Pr(Y(so) = 1) where so may be an observed site or a new site of 
interest. How does this probability change as levels of X( so) are changed? For a 
given X ( s 0 ), the posterior distribution of this probability can be obtained so that 
the effect of change in X( so) can be assessed. 
We illustrate our approach with an application to a set of 857 residential proper-
ties in Baton Rouge, Louisiana, where the Si are geographical coordinates for the ith 
house. All properties in the dataset are "repeat sales", that is, they sold once and 
then were resold during a ten year period of observation. Y ( Si) = 1 if the property 
resold within four years, Y(si) = 0 if not. We are looking to identify whether there 
is spatial association in the pattern of quicker resales (:5 4 years) compared with 
the slower resales (> 4 years). The X(si) record house characteristics anticipated 
to be important in property sales and resales: age of the house at the time of sale, 
living area, other area (total square footage less living area) and the number of 
bathrooms. 
The format of the paper is the following. In Section 2, we provide modeling details 
including several useful analytic calculations. We ultimately arrive at the full hier-
archical model employed in fitting the foregoing data. In Section 3, we summarize 
our simulation-based approach to avoid matrix inversion, extending Gelfand and 
Ravishanker (1998). Section 4 provides the analysis of the repeat sales real estate 
data. Finally, in Section 5, we offer a few related remarks. 
2. 
Modeling Details 
To formalize modeling details, assume that Z = ( Z ( s t), Z ( s 2), ... , Z ( Sn) )' is a 
realization from a spatial Gaussian process of the form 
(2) 

376 
Gelfand, Ravishanker & Ecker 
where X is n x p with ith row X(si)i ~ is p x 1; W is n x n from a second 
order stationary Gaussian process, i.e., W ""' N(O,u 2H(8)) where (H(8))ij = 
Corr(W(si), W(sj)) = p(si- sj;8), p a valid 2-dimensional correlation function 
parameterized by 8; and e is n x 1 distributed N(O, r 2 I). Then, 
Zl~, w""' N(X~ + w, r 2I). 
(3) 
Let 2")'( Si -Sj) be the detrended variogram (see Ecker and Gelfand, 1997) associated 
with Z, i.e., 
2")'(si- sj) = Var((Z(si)- X(si)'~)- (Z(sj)- X(sj)'~)) 
= 2{r2 +u2(1-p(si-Sj;8))}. 
(4) 
Z is not observed, but rather Y = (Y(s1), Y(s2), .•• , Y(sn))' is, where Y(si) = 1 
if Z(si) > 0 and Y(si) = 0 if Z(si) s; 0. Given Y, we seek to infer about the process 
parameters ~, 6, u 2 , r 2 as well as the probability that Y ( s0 ) = 1 where s 0 may be 
an observed or a future site and also the association between Y(si) and Y(sj ). 
Note that the Y(si) are conditionally independent Bernoulli random variables 
given~ and W. In fact, 
Pr(Y( si) = 1 I ~' W) 
Pr(Z(si) > 0 I~' W) 
1- <ll(-.!.(x(si)'~+ W(si))) 
T 
1 
<Jl(-(X(si)'~ + uV(si))) 
T 
(5) 
where W = uV; i.e., V""' N(O, H(8)). Thus, we see that, e.g., Tis not identifiable; 
if we multiply ~' u and T all by a constant c, (5) is unchanged. Without loss of 
generality, we can set r 2 = 1 or equivalently we can work with the renormalized 
variogram by dividing ( 4) by r 2 • 
Marginalizing (5) over Vi = V(si)""' N(O, 1), we obtain 
/_
oo 
j_X(sd~+uVi 
Pr(Y(si) = 1 I~~ u
2
) = 
-oo ¢(Vi) -oo 
¢(Ui) dUi dVi 
= Pr(Ui < X(si)
1 ~ + uVi I ~' u 2) 
Pr(Ui- uV£ < X(si)' ~ I~' u 2) 
<Jl ( X ( Si )' ~) . 
( 6) 
v'1+u2 
We seek to study the behavior of (6) as levels of X( si) are varied. From the Bayesian 
perspective, with a prior specification for~ and u 2 , (6) has a posterior distribution 
given Y for each X(si). Hence, we can compare the posteriors as we change X(si)· 
Just as (6) provides E(Y(si) I~' u 2), we obtain 
Var(Y(si) I ~,u
2
) = E(Y(si) I ~,u
2
) 
(E(Y(si) I ~,u
2 ))
2 
= <Jl (X(si)' ~) 
<Jl2 (X(si)'~) 
y'1 + u2 
v'1 + u2 
<Jl (X(si)'~) . <Jl ( 
X(si)'~) . 
(7) 
v'1 + u2 
v'1 + u2 
Turning to the association structure 

22. Point-referenced binary spatial data 
377 
-Pr(Y(si) = 11 {3,u2,6) ·Pr(Y(sj) = 11 {3,u 2,6). 
(8) 
Employing the same reasoning that led to (6), we obtain 
Pr(Y(si) = 1,Y(sj) = 11 {3,u2,6) = 
Pr(Ui- u\li < X(si)'{3, Uj- ullj < X(sj )'{3). 
(9) 
Since the joint distribution of Ti = Ui- u\li and Tj = Uj- ullj is bivariate normal 
with mean 0 and covariance matrix 
[ 
1 + u 2 
u 2 p( Si - Sj; 6) ] 
u 2p(si- Sj; 6) 
1 + u 2 
' 
given {3, u 2 and 6, (9) can be readily calculated. Though an analytic form for 
(8) is messy, a Monte Carlo integration by drawing pairs (Ti, Tj) is routine. Us-
ing (9) and (6) provides (8). Expressions analogous to (9) can be obtained for 
Pr(Y(si) = 1, Y(sj) = 0 I {3,u2,6), Pr(Y(si) = 0, Y(sj) = 1 I {3,u 2,6) and 
Pr(Y(si) = 0, Y(sj) = 0 I {3, u2 , 6) by again using Monte Carlo integration. In fact, 
since Pr(Y(si) = 1,Y(sj) = 0 I {3,u2,6) = Pr(Y(si) = 11 {3,u2)- Pr(Y(si) = 
1, Y(sj) = 1 I {3, u 2 , 6), the left side probability can be calculated using (9) and 
(6). Hence, once (6) has been obtained for each i, the four joint probabilities 
for (Y(si),Y(sj)) can be computed using a single Monte Carlo integration. As 
a result, odds ratios and log odds ratios associated with the joint distribution of 
(Y(si), Y(sj)) can be calculated. 
In the case where no covariates are included, i.e., X( Sj )' {3 = p, for all i, (6) 
becomes~ ( v'l~u2 ) and (7) becomes~ ( Jl~u2 ) -
~
2 ( v'l~u2 ). Also, suppose we 
calculate the variogram associated with the Y(s) process 
Var(Y(si)- Y(sj) I p, u 2 ) = 
2{(Var Y(si) I p, u 2 ) 
-
Cov((Y(si), Y(sj) I p, u 2 , 6)} 
(10) 
From the argument below (9), the expectation in (10) depends upon Si and Sj only 
through Si -
Sj so that the variogram is stationary. It is an indicator variogram in 
the sense of Cressie (1993, p. 282). 
When u is small, to a first order approximation, ~(p + u V) ~ ~(p) + ¢(p )u V. 
Inserting this approximation into the calculation of Cov(Y(si), Y(sj) I p, u 2 , 6), we 
obtain ¢2(p )u2 p( Si - Sj; 6). That is, for a given p, the spatial association in the 
Y(s) process is roughly ¢2(p) times the spatial association in the W(s) process. 
To complete the hierarchical Bayesian model specification requires a prior distri-
bution for {3, u2 and 6. Taking these to be flat, proper Inverse Gamma and proper 
Gamma, respectively provides the full distributional specification 
f(Y I {3, W) · f(W I u2 , 6) · !({3) · f(u2 ) • f(6) 
(11) 
where 
n 
f(Y I {3, W) = n 
(~(X(si)' {3 + Wi))Y(si) . (~( -(X(si)' {3 + Wi)))l-Y(si) 
i=l 

378 
Gelfand, Ravishanker & Ecker 
with wi = w ( Si). Lastly, the posterior for ~, u2 , 8 and w given y is proportional 
to (11). 
Models of the form (2), hence the associated model for Y, are such that the 
data can not fully separate spatial variability and pure heterogeneity (white noise). 
Prior specifications strongly influence the inference. This is well known in the case 
where the distribution for W is given through a Markov random field (Clayton and 
Bernardinelli, 1992, Bernardinelli and Montomoli, 1992) but seems less appreciated 
for the case where W is a stationary Gaussian process. As a result, according to the 
priors on u 2 and r 2 , we can tell a primarily spatial or primarily heterogeneity story, 
respectively. The above papers offer empirical guidelines with regard to providing 
priors which balance these two sources of variation. In our case with r 2 = 1, it is 
much simpler to give a neutral specification. We take u 2 to follow an inverse Gamma 
distribution with mean 1 and infinite variance. 
3. 
Computational Issues 
We propose the use of simulation to fit the model in (11). That is, we seek to 
draw samples from the posterior f(~, u 2 , 8, WI Y). We find the widely used Markov 
Chain Monte Carlo approach (see, e.g., Gilks, Richardson and Spiegelhalter, 1995) 
to be unattractive here, particularly for moderate to large n. This is due to the fact 
that, in evaluating (11), we must calculate f(W I u 2 , 8) which requires n- 1(8) (and 
IH(8)1). Hence, for each 8, a high dimensional matrix inversion is required which, 
due to the enormous number of arithmetic operations, may become very slow and, 
with cumulative rounding error, inaccurate. Moreover, to implement thousands of 
iterations may become infeasible within realistic time constraints. 
Following Gelfand and Ravishanker (1998), we propose to use a noniterative 
Monte Carlo approach with a suitably selected importance sampling density (ISD). 
This ISD enables both Monte Carlo integration for expectations (as in Geweke, 
1989) and Monte Carlo sampling for other distributional features (as in Smith 
and Gelfand, 1992). The ISD is of very high dimension (n + p + 2, if 8 is one-
dimensional) and thus would require an astronomical amount of sampling to learn 
about!(~, u2 , 8, WI Y). However, concern lies only in the p+2 dimensional poste-
rior f(~, u2 , 8 I Y). In fact, as a result of marginalizing over W, all of the quantities 
of interest in the previous section are fimctions solely of~, u 2 and 8. Samples from 
!(~, u 2 , 8 I Y) provide samples from the posteriors for any of these quantities, en-
abling inference regarding these quantities. For the posterior f(~, u2, 81 Y), a much 
smaller number of draws may be sufficient. Of course, this number will grow with 
n, increasing run times, but there is no feasible alternative. 
We propose an ISD of the form 
We now describe each of the components on the right hand side of (12). With a 
flat prior on~, we take Us(~ I W; Y) to be N(~ I t3w, Et3w) where t3w and 
E f3w is the maximum likelihood estimate and associated asymptotic covariance 
arising, at a given W, from f(Y I ~, W) whose form appears below (11). Since 
-E~~ is the p x p Hessian for ~ at a given W, it can be computed explicitly 
and hence Us(~ I W; Y) is easily evaluated. As a p-variate normal distribution, it 
is routinely sampled. 

22. Point-referenced binary spatial data 
379 
An alternative would be to sample/(~ I Y, W), the full conditional distribution 
for ~· Since f(Y I ~~ W) is log concave in ~ (Wedderburn, 1976), a rejection 
algorithm can be designed using bounding hyperplanes. 
Next let Ys(W I u 2,6;Y) be f(W I u 2 ,6) and g8 (u2,6;Y) = f(u 2 )f(6). In 
certain cases, we may be able to refine these choices, as discussed in Gelfand and 
Ravishanker (1998). Then the sampling weights, dividing (11) by (12), become 
f(Y I~' W) · f(W I u2,6) · f(u 2 ) • f(6) 
N(~ I t3w, tt3w) · f(W I u2 , 6) · f(u2 ) • f(6) 
f(Y I~' W) 
(13) 
free of u 2 and 6. Note that calculation of (13) requires no n x n matrix inversion. 
Our choice for g8(W I u 2 , 6; Y) has annihilated f(W I u 2 , 6); we replace matrix 
inversion with sampling from an n dimensional normal. If f(~ I W, Y) replaces 
the normal density for~ in the denominator of (13), w(~, u 2, 6, W; Y) becomes 
J f(Y I ~' W) d~, free of~ as well as u 2 and 6. 
Draws (uf, 6£, W£ ,~£) , l = 1, ... , L, from Ys(~, u2 , 6, W; Y) yield weights w£ 
using (13) which may be normalized to probabilities ql = w; fEw;. How do we 
use these random draws and weights to infer about !(~, u 2 , 6 I Y)? For !(~ I Y), 
we note that 
!(~ I Y) 
f !(~' I w' Y) . f(W I Y) dW 
1 
= f !(~'I w, Y). f(W I Y) d~'dW 
:;...:,r_....,n-_:._:..........,.,..J-,1;..,.-;::-') • Ys(~', u2 , 6, W; Y) d~'du 2 d6dW 
_ J !(~ I W, Y) · w(~', u 2 , 6, W; Y) · g8 (~
1
, u 2 , 6, W; Y) d~'du
2 d6dW 
-
f w(~', u 2 , 6, W; Y) · Ys(~', u 2 , 6, W; Y) d~' du2d6dW 
Hence a Monte Carlo integration for f(~ I Y) is the mixture distribution 
(14) 
Expression (14) is awkward to work with since, for each W£, (11) only provides 
f(Y I ~' Wl) explicitly. The normalizing constant for /(~ I W£, Y) requires a 
p-dimensional integration. In the case where no covariates are included, (p = 1), 
this is no problem. However, in general it is easier to run a Gibbs sampler at each 
w;, using adaptive rejection sampling (Gilks and Wild, 1992) since f(~ I W£, Y), 
equivalently f(Y I ~' Wl), is log concave in each component of~. 
If a few weights dominate with regard to the qt, this will be preferable to the 
discrete distribution placing mass ql at ~£. Taking the approximation one step 
further, we might replace/(~ I W£, Y) in (14) with N(~ I ~w·,Ea 
). 
t 
~--'W; 
Similarly, for f(u 2 , 6 I Y), if a few weights dominate, the discrete distribution 
in ( uf, 6£) will be unsatisfactory. The mass may be smoothed for each l using a 
bivariate t-distribution with low degrees of freedom for t 1 =log u 2 , t 2 = log 6. 
To determine the t-distribution requires a location vector and a dispersion ma-
trix. The location vector will be tu = log uf, tu = log 6£. We can obtain a 

380 
Gelfand, Ravishanker & Ecker 
dispersion matrix using a parametric bootstrap. Generate a sample of W's from 
N(O, u;* H(6£)). For each W, estimate u2 and 6, hence t1 and t2. Use the sample 
of ( tt, t2)'s to create a sample covariance matrix. 
The ISD approach still requires drawing W from N(O, u2 H(6)) which in turn 
requires triangulation (Cholesky decomposition) of H(6), an order n2 operation. 
Though cheaper than inversion (an order n3 operation) and more stable, repeated 
sampling of W as 6 changes requires repeated triangulation of H(6), which may 
still result in very long run times if n is large. 
We propose a first-order dependent approximation to sampling from N ( 0, u2 H ( 6)). 
Suppose w is blocked as (W1, w2, ... , Wx) with wi, niX 1 and L:ni = n. Par-
tition H(6) accordingly, i.e., suppressing 6, the nix nj matrix Hij = Cov(Wi, Wj)· 
Consider the joint distribution for w created as w1 "'N(O, u2 Hu) and wi I Wi-1"' 
N(Hi,i-1Hi-_\ i-1 Wi-t, u2(Hii-Hi,i-1Hi-_\ i-1Hi-1,i)), i = 1, ... , k. It is straight-
forward to ch~ck that the resulting distribution for W is again an n dimensional 
multivariate normal, that the mean for W is 0 and that the covariance matrix is 
2-
-
-
-
1 
u H(6) where Hii = Hii, Hi,i+1 = Hi,i+b and, for i < j, Hij = Hi,i+1 Hi+1,i+1 
Hi+1,i+2 ... , HJ:!1,j _1 Hj -1,j. In other words, fi agrees with H on the main and 
first off block-diagonals. 
But then, a natural suggestion emerges to strengthen the quality of the approx-
imation. By permuting the components of W, with corresponding permutation of 
the rows and columns of H, we can consider arbitrary blocking for W. Suppose, for 
example, that we partition the region D into vertical strips defined by creating in-
tervals on the x axis. Then each site will fall into one and only one strip and roughly, 
pairs of sites from adjacent strips, will, a priori, be more strongly associated than 
pairs from non-adjacent strips. In fact, association should tend to decline as strips 
become farther apart. For the corresponding H, entries in Hij will tend to grow 
smoother as li - il grows larger. In fact, this will also be true for Hij enhancing 
the quality of the approximation. In a sense, this approach offers a solution, in the 
simulation context, to the well-known problem of how to define and piece together 
subregions to infer about the spatial association in the overall region. 
The first order approximation is obviously not unique. In particular, we could 
employ horizontal strips rather than vertical ones. Possibly the strips could be 
constructed by the orientation of D. Also, the number of and width of individual 
strips is flexible. For us, the objective would be to achieve roughly equal ni across 
the strips. Then, the advantage to the approximation is apparent. In sampling W, 
we never need work with a matrix larger than max ni x max ni. Choosing k large 
enough, we can ensure that max ni x max ni is small enough so that the required 
inversion and triangulation to do the sampling runs very quickly. In the example of 
section 4, we have n = 857 with k = 10 and max ni = 92. 
4. 
An Illustration 
We examine a dataset consisting of 857 residential properties in Baton Rouge, 
Louisiana. The locational coordinates are plotted in Figure 22.1. The shape of the 
region encourages partitioning using horizontal strips. Again, the Si are the ge-
ographical coordinates of the ith house. All properties in the dataset are repeat 
sales. That is, during a ten year period of observation, they sold once and then were 
subsequently resold. We let Y(si) = 1 if the property resold within four years and 
Y(si) = 0 if not. The proportion of Y(si) = 1 in the sample is 45%. We seek to 
identify whether there is spatial association in the pattern of quicker (s; 4 years) 

22. Point-referenced binary spatial data 
381 
. 
., 
.\ . . 
•. 
·· . 
. . ·. . 
... ,· 
·t .a.t· 
.. ~··'1 
.·. 
. . :;-. ·•" 
• 
• 
II, 
:t~~.':fl"• 
• 
•• 
• ·•f .., • 
".::::. :: '/'il•f• ., ...... ~ 
.-:---,•. 
.~· ......... .,.. 
,, 
. 
.. -, .. . 
\tl' .~. 
·'' 
• 
'~! ... 
~>r,) 'f 
,. 4,.) ) 
.. 
·~-= 
.,..... 
t 
• ~, .. 
.. ;;, .. ,... 
"' 
. ""'···· 
·~, 
FIGURE 22.1. Home Locations in Baton Rouge, Louisiana. 

382 
Gelfand, Ravishanker & Ecker 
TABLE 22.1. Posterior Mean, Standard Deviation, 2.5% ,50% and 97.5% quantiles of 
Model Parameters 
Post. Mean 
Post. StDev. 
2.5% quantile 
Post. Median 
97.5% quantile 
f3o 
-0.115 
0.053 
-0.225 
-0.115 
-0.015 
f31 
0.224 
0.081 
0.062 
0.221 
0.381 
f32 
0.554 
0.293 
-0.031 
0.564 
1.146 
f3s 
0.093 
0.154 
-0.201 
0.094 
0.394 
f34 
-0.254 
0.304 
-0.853 
-0.259 
0.374 
(j2 
0.212 
0.034 
0.152 
0.208 
0.284 
6 
14.18 
2.12 
10.45 
14.02 
18.83 
r 
0.216 
0.033 
0.159 
0.214 
0.285 
resales relative to the slower (> 4 years) resales. Four covariates, reflecting house 
characteristics, are employed at each site. X 1(si) is the age of the house at the time 
of first sale, X2(si) reflects the living area of the house, Xs(si) represents other area 
(apart from living area), while X 4(si) refers to the number of bathrooms per house. 
All variables were standardized. Hence, ~ is 5 x 1, including an intercept term. For 
illustration, we adopt p(si 
SJ; 6) = exp( -6diJ) where diJ is the Euclidean distance 
between sites Si and SJ. The remainder of the model specification takes the form 
(11) with f(u 2 ) = JG(2, 1) and f(6) = Gamma(2, 0.04). The former has mean 1 
with infinite variance following the discussion at the end of seStion 2. The later 
has mean 50 (with very large variance) which corresponds to a prior guess of 0.06 
degrees for the range (see next paragraph), roughly 4 to 5 miles. 
The model is fitted using the approach of Section 3. 
Table 22.1 summarizes the posteriors for {3, u2 , and 6 using the mean, standard 
deviation, the median and the lower and upper 0.025 quantiles. The latent detrended 
variogram ( 4) with r 2 = 1 is a function of u2 , 6, and the Euclidean distance (d) 
between sites. We plot the posterior mean, 2E(1 + u2(1 
exp(--6d)) I Y) versus d 
in Figure 22.2. The range is defined to be the distance r such that 0.05 = exp( -6r), 
i.e., r = -log(0.05)/6. Its posterior is also summarized in Table 22.1. Finally, a 
plot of the observed binary response at each spatial location is not very insightful 
regarding spatial patterns. Instead, using (6), we can obtain the posterior mean of 
Pr(Y ( Si) = 1 I ~, u2) for each site. 
A greyscale plot of these expected probabilities, using S-Plus, is given in Figure 
22.3, smoothing the spatial pattern in the raw data. 
We attempt some intrepretation of these figures and the table. First, it is the case 
that the southern part of the city has evolved as the more desirable area to live in. 
There is more real estate activity in this area and it is expected that the bulk of the 
resales would be in this area, as Figure 22.1 reveals. 'I\uning to the coefficients, a 
standard logistic regression ofY on X 1 , X2 , X3 and X4 yields significant positive age 
effect (P = 0.003) and a nearly significant positive living area effect (P = 0.091). 
These results concur with Table 22.1. (Magnitudes need not align due to the implicit 
scaling in setting r 2 = 1). 
The implication is that the probability of quicker resale increases in age and living 
area. To clarify, the southwestern part of the city is near Louisiana State University 
and is dominated, residentially, by professionals. Hence, it would be expected to 

22. Point-referenced binary spatial data 
383 
0.0 
0.1 
0.2 
0.3 
Distance 
FIGURE 22.2. Plot of Estimated Latent Detrended Variogram. 

384 
Gelfand, Ravishanker & Ecker 
exhibit more rapid turnover than other areas. 
<J.) 
"0 
::I 
..... 
'iii 
-1 
""" 
d 
('I) 
0.6 
0.7 
<0 
d 
('I) 
1.0 
d 
('I) 
"""' 
d 
('I) 
-91.20 
-91.15 
-91.10 
-91.05 
-91.00 
-90.95 
Longitude 
FIGURE 22.3. Greyscale plot of E{Pr(Y(si) = lLB, u2)} versus Si. 
Figure 22.3 supports this, showing highest expected probabilities in the southwest. 
But also, homes in this area tend to be older and larger helping to explain the 
positive f11 and /32. 
The magnitude of o-2 (relative to r 2 ) shows that starting from a neutral spec-
ification with regard to source of variation, spatial variability emerges as roughly 
one-fifth of the white noise variability. The choice of covariates may have explained 
much of the spatial hetergeneity. Still, the spatial component is consequential, as 
the detrended variogram reveals. Moreover, within this component, association de-
clines slowly with distance; the estimated range of roughly 0.2 is approximately 14 
miles. 
5. 
Related Remarks 
We have considered the problem of modeling and analyzing point referenced bi-
nary spatial data. We have formulated a hierarchical model introducing spatial 
effects at the second stage in the spirit of familiar Markov random field approaches. 

22. Point-referenced binary spatial data 
385 
However, we capture the second stage spatial association using an, arguably, more 
direct and natural second order stationary Gaussian process. In the fitting of such 
models, we have indicated how the problem of repeated high dimensional matrix 
inversion can be replaced by simulation. Such simulation enables full inference re-
garding all parameters and parametric functions of interest. 
Link functions h in (1) besides the probit can be adopted. However, the attractive 
connection with the latent Z(s) process is lost and thus so are the convenient calcu-
lations in (6)-(9). A related question concerns whether W(s) can be modeled with 
an alternative to the Gaussian process. For instance, we might propose a heavier 
tailed t-process such that W "" tr(O, o-2 H(6)), a multivariate t distribution with 
r degrees of freedom, location vector 0 and dispersion matrix o-2 H(6). Unfortu-
nately, with just a single data vector Y, i.e., no replication, it will not be possible 
to distinguish at-process from a Gaussian one. 
Acknowledgements 
The work of the first author was supported in part by NSF grant DMS 96-25383. 
The authors thank C.F. Sirmans for valuable discussions. 
References 
Bernardinelli, L. and Montomoli C. (1992). Empirical Bayes versus Fully Bayesian 
Analysis of Geographical Variation in Disease Risk. Statistics in Medicine. 11, 
pp 983-1007. 
Besag, J .E. (1974). Spatial Interaction and the Statistical Analysis of Lattice Sys-
tems (with discussion). Journal of the Royal Statistical Society, Series B. 36, 
pp 192-236. 
Besag, J .E. and Kooperberg, K.L. (1995). On Conditional and Intrinsic Autore-
gressions. Biometrika. 82, pp 733-746. 
Besag, J .E., York, J .C. and Mollie, A. (1991). Bayesian Image Restoration, with 
two Applications in Spatial Statistics (with discussion). Annals of the Institute 
of Statistical Mathematics. 43, pp 1-59. 
Clayton, D.G. and Bernardinelli, L. (1992). Bayesian Methods for Mapping Disease 
Risk. In Small Area Studies in Geographical and Environmental Epidemiology, 
J. Cuzick and P. Elliott ( eds). Oxford: Oxford University Press. 
Cressie, N. (1993). Statistics for Spatial Data. New York: John Wiley and Sons. 
De Oliveira, V. (1997). Bayesian Prediction of Clipped Gaussian Random Fields. 
Technical Report, Department of Mathematics, University of Maryland. 
Diggle, P.J ., Liang, K-L. and Zeger, S.L. (1994). Analysis of Longitudinal Data. 
New York: Oxford Science Publications. 
Diggle, P.J., Tawn, J.A. and Moyeed, R.A. (1998). Model-based Geostatistics (with 
discussion). Applied Statistics. ( 47(3), pp 299-350. 

386 
Gelfand, Ravishanker & Ecker 
Ecker, M.D. and Gelfand, A.E. (1997). Spatial Modeling and Prediction under 
Range Anisotropy. Technical Report 97-26, Department of Statistics, Univer-
sity of Connecticut. 
Gelfand, A.E. and Ravishanker, N. (1998). Inference for Bayesian Variogram Mod-
els with Large Data Sets. Technical Report 98-18, Department of Statistics, 
University of Connecticut. 
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling Based Approaches to Calcu-
lating Marginal Densities. Journal of American Statistical Association 85, pp 
398-409. 
Geweke, J. (1989). Bayesian Inference in Econometric Models using Monte Carlo 
Integration. Econometrica. 57, pp 1317-1339. 
Gilks, W.R., Richardson, S. and Spiegelhalter, D.L. (1996). Markov Chain Monte 
Carlo in Practice. New York: Chapman and Hall. 
Gilks, W.R. and Wild P. (1992). Adaptive rejection sampling for Gibbs sampling. 
Applied Statistics. 41, pp 337-348. 
Journel, A.G. (1983). Nonparametric Estimation of Spatial Distributions. Mathe-
matical Geology. 15, pp 445-468. 
Papritz, A. and Moyeed, R.A. (1997). Empirical Validation of Linear and Nonlin-
ear Methods for Spatial Point Prediction. Technical Report, Department of 
Statistics, Lancaster University. 
Smith, A.F.M. and Gelfand, A.E. (1992). Bayesian statistics without tears: A 
sampling-resampling perspective. The Amer-ican Statistician. 46(2), pp 84-
88. 
Wedderburn, R.W.M. (1976). On the Existence and Uniqueness of the Maximum 
Likelihood Estimates for Certain Generalized Linear Models. Biometrika. 63, 
pp 27-32. 
Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics. Chich-
ester: John Wiley and Sons. 

23 
Bayesian Graphical Models and 
Software for G LMs 
Nicky Best 
Andrew Thomas 
ABSTRACT In this chapter, we describe two useful tools for the Bayesian analysis 
of generalized linear models (GLM's) and their extensions. The first is a graphical 
modelling approach for representing the conditional independence assumptions and 
qualitative structure underlying a statistical model. The second is the WinBUGS 
statistical software, which implements a Markov chain Monte Carlo approach to 
Bayesian inference. Graphical models form the central construct of both the statis-
tical model and the software by providing a direct link between the model description 
and the computational solutions to the associated inference problem. We describe 
how these parallels are exploited by the WinBUGS software and show how this leads 
to a readily extensible programming environment for complex Bayesian modeling. 
The remainder of the chapter offers practical guidelines for implementing Bayesian 
GLM's using the WinBUGS software. Particular emphasis is placed on the flexibil-
ity of both the Bayesian graphical modeling approach and the WinBUGS program 
to extend the standard GLM framework by accommodating additional sources of 
complexity such as correlated data structures, overdispersion, measurement error, 
missing data, outlying observations and so on. 
1. 
Bayesian Graphical Models and Conditional 
Independence Structures 
Graphical representations of dependencies between variables in a statistical model 
have been used to express the underlying problem structure in a wide variety of 
applications. Spiegelhalter ( 1998) cites examples including path analysis diagrams, 
structural equation models, Bayesian networks, classical conditional independence 
graphs, neural networks and Bayesian graphical models. Interest in the latter has 
grown rapidly in recent years due to the recognition that the formal properties 
of such graphs provide a direct link to the Markov Chain Monte Carlo (MCMC) 
computational algorithms used in complex Bayesian inference. 
A Bayesian graphical model is simply a pictorial representation of the condi-
tional independence assumptions underlying a statistical model. In general, these 
may be expressed in the form of a directed acyclic graph (DAG) in which all edges 
are directed and no cycles are permitted. Figure 23.1 shows a simple example of a 
DAG for the generalized linear model (GLM) described in Section 2. Each quan-
tity in the model is represented by a node in the graph: by convention, ellipses 
represent random variables (both observed data and unknown parameters, missing 
data, latent variables and so on) and rectangles (if any) represent fixed constants. 
Repetitive structures (for example, measurements of the random variable Y [i] on 
different units i=1, ... , I) are known as "plates" and are represented by a large 
thick-edged rectangle enclosing the repeated nodes. Directed edges (arrows) be-
tween nodes indicate dependencies: solid edges represent probabilistic relationships 
and hollow edges represent deterministic relationships. For example, in Figure 23.1, 
Y[i] is a stochastic node which depends probabilistically on mu[i] and phi, whilst 
387 

388 
Best & Thomas 
for(lln 1:1) 
FIGURE 23.1. DAG for a simple GLM 
mu [i] is a logical node which may be expressed as a deterministic function of X [i] 
and beta. Assuming for now that the quantities in the graph represent the genes 
of individuals in a family tree, and ignoring deterministic edges, we may regard 
mu [i] and phi as the genes of a pair of "parents" with a 'child' whose genes are 
represented by Y [i]. The conditional independence structure implied by a DAG 
follows naturally from the Mendelian inheritance laws associated with this genetic 
analogy. The genetic properties of a generic individual v depend only on the genes 
of its parent(s). Conditional on knowing the parent genes, no other individual can 
provide information about v's genes unless that individual is a "descendant" of v. 
This property may be expressed formally as follows 
v Jl non-descendants[v]jparents[v] 
(1) 
where Jl denotes "is conditionally independent of." It can also be shown (Lauritzen 
et a/. , 1990) that (1) is equivalent to assuming that the joint distribution of all 
quantities V = { v} in the graph has a simple factorization 
p(V) = IT p( vjparents[v]) 
(2) 
vEV 
1.1 
Computation on Bayesian Graphical Models 
Bayesian inference involves specification of a full probability model to describe 
the joint distribution of the observed data D (e.g. Y [i] and X [i] in Figure 23.1) 
and unobserved quantities e (e.g. beta and phi in Figure 23.1). Note that logical 
nodes such as mu[i] in Figure 23.1 are included to simplify the graph, but are 
collapsed over when identifying probabilistic relationships. According to the factor-
ization theorem (??) for DAGs, we need only specify the parent-child conditional 
distributions p(vjparents[v]) for each node v E V = {D, e} in order to fully define 
the model. Conditioning on the known value of D by applying Bayes theorem will 
then yield a posterior density p(eiD). For most real-life applications, e will be high 
dimensional and scientific interest will focus on the marginal posterior distribution 
of individual components (}k E e. This involves integrating the joint posterior den-
sity p(ejD) over the remaining unknowns Bj¢k E e. Such integrations are usually 
complex and best carried out using sampling-based methods such as Markov Chain 
Monte Carlo (MCMC) algorithms. MCMC methods involve simulation of values 
from a Markov Chain whose stationary distribution is the required posterior den-
sity. Inference is based on data-analytic summaries of the sampled values, such as 
means and quantiles, using the principle of Monte Carlo integration. 
Detailed accounts ofMCMC methods are provided elsewhere (Smith and Roberts, 
1993; Brooks, 1998). The key point here is that MCMC algorithms such as the Gibbs 
sampler involve simulation from the full conditional distribution of each unknown 

23. Software for GLMs 
389 
quantity v given all the other terms in the model V\v. It is easily shown that 
the factorization theorem (??) for DAGs leads to the following form for the full 
conditional distribution 
p( vI V\ v) 
ex 
terms in factorization of p(V) containing v 
= p( vlparents[v]) 
IT 
p( wlparents[w]) 
vEparents[w] 
By exploiting conditional independence structures, the graph thus provides a direct 
link between the statistical model and the computational algorithms required for in-
ference. This leads to the crucial ideas behind the BUGS software (described below), 
namely specification of full probability models in terms of parent-child relationships 
and local computation on full conditional distributions. 
1.2 Constructing Software from Graphical Models 
The conceptual design of the BUGS software is based on constructing an inter-
nal representation of the graph describing the joint probability model. The current 
version of BUGS (WinBUGS) has been developed for Microsoft Windows within 
the Black Box environment using the computer language Component Pascal. A 
component-orientated philosophy has been adopted. This is a new software engi-
neering approach which aims to create fully extensible modular systems. The soft-
ware consists of a number of components which implement the nodes in a DAG. 
Each component has a well-defined interface describing the implemented entities 
which can be used in other components. These are linked together at load time or 
run time to create a series of connected objects (conceptually similar to the parent-
child links in a DAG) required to execute the model. An example of a WinBUGS 
component is given in the Appendix. 
2. 
Implementing GLM's Using WinBUGS 
In this section we illustrate the principles of model specification and implemen-
tation for a simple Bayesian GLM using the WinBUGS software. More complex 
G LMs are considered in Sections 3-6. The reader is also referred to the Classic 
BUGS manual (Spiegelhalter et al., 1996) and WinBUGS online documentation for 
additional information. 
A standard G LM consists of 3 components: 
1. A random component specifying the probability distribution of the response 
variable Yi for units i = 1, ... , I. In general, this will depend on a mean 
parameter /Ji and on a global dispersion parameter ¢. 
2. A deterministic component specifying a linear function of covariates Xi upon 
which the response Yi is assumed to depend. This is termed the linear predic-
tor, denoted 'l]i = {3' xi. 
3. A link function g(.) that relates the linear predictor to the mean /Ji of the 
response variable: /Ji = y- 1(rJi) = y- 1({3'Xi). 
The WinBUGS software provides a graphical interface called DoodleBUGS which 
enables direct specification of the model as a DAG. Figure 23.1 shows how the 

390 
Best & Thomas 
structure of a standard GLM may be expressed using DoodleBUGS (see Section 1.) 
for a description of the graph notation and interpretation). Model specification is 
then completed by 
1. Specifying the form of the probability distribution (likelihood) for the re-
sponse variable Y [i]. DoodleBUGS provides a menu of distributions from 
which the density for a stochastic node may be selected. Common choices in-
clude Bernoulli or binomial (binary responses), Poisson (count data), normal 
or Student t (continuous responses), Weibull or gamma (survival times). 
2. Specifying the deterministic function of covariates and regression coefficients 
which equate to the mean mu[i]. In DoodleBUGS, logical nodes have a menu 
of link functions associated with them, plus a text-entry value field where the 
mathematical expression for the linear predictor should be typed. Common 
choices are are the logit link g(JJi) =log~~~~ (Bernoulli or binomial GLMs), 
the log link g(JJi) = log/Ji (Poisson or Weibull GLMs) and the identity link 
(normal or t GLMs). Other options include the probit and complementary 
log-log links. 
3. Assigning prior distributions for all "founder" nodes (i.e. stochastic nodes 
with no arrows pointing to them), namely the regression coefficient beta and 
the dispersion parameter phi. Again, these are selected from the menu of 
probability distributions in DoodleBUGS, and suitable choices are discussed 
in Section 7.2. Note that for Bernoulli, binomial and Poisson GLMs, the dis-
persion parameter phi is not required and so may be deleted from the graph. 
The DoodleBUGS graph now describes a full probability model for a standard GLM. 
This may be passed to the WinBUGS program where the structures underlying the 
graphical description of the model are parsed into a tree form. Observed nodes in 
the graph are then identified by loading the data file: these are the nodes which will 
be conditioned on when carrying out Bayesian posterior inference (see Section 1.1). 
WinBUGS compiles the model by traversing the tree to create a series of connected 
objects (see Appendix) which form a low-level executable version of the Bayesian 
graphical model. Each object has a set of attributes containing relevant local infor-
mation such as its parents and children in the graph and a suite of algorithms to 
implement the probability distribution or link function implied by the associated 
edges in the graph. This information is used to construct full conditional distribu-
tions needed for the MCMC simulation using the factorization theorem for DAGs. 
Once compilation is complete, the user must assign initial values for each stochastic 
node, either by loading a separate file of values and/or by requesting that WinBUGS 
automatically generate values by forward sampling from the prior distribution. 
The MCMC sampler may now be invoked using the Update menu in WinBUGS to 
initiate the required number of iterations. Posterior inference is achieved by using 
the Statistics menu to request summary statistics for the samples generated. A 
variety of graphical outputs are also available, including trace plots, autocorrelation 
plots and density plots. 
For more complex models, particularly those with large numbers of parameters 
and/or multivariate nodes, formal specification via the DoodleBUGS graphical in-
terface can become tricky and most users prefer to specify the model directly using 
the text-based BUGS language. This has a syntax similar to the Splus language 
and follows reasonably intuitively from the equation-based representation of a stat-
istical model. Nonetheless, the graphical representation of a full probability model 

23. Software for GLMs 
391 
can provide a valuable aid to model elaboration as illustrated in Sections 3.-6., and 
remains the key to its underlying implementation in WinBUGS. 
3. 
GLMs with Non-canonical Links 
For scientific reasons, or to explore sensitivity to different assumptions, non-
canonical link functions are often assumed for G LMs. Specification of such models 
in WinBUGS is achieved by simply selecting the desired function from the avail-
able links. However, care must be taken when the chosen link allows for values 
of IJi outside the valid range. For example, consider the analysis of data from the 
Solomon-Wynne experiment. The experiment involves applying an electric shock to 
a dog 10 seconds after a light stimulus. The dog can avoid the shock by jumping out 
of the cage after the light, in which case a success is recorded; if the dog fails to jump 
and receives the shock a failure is recorded. A plausible model is to suppose that a 
dog learns from previous experiments, with the probability of success deP.ending on 
the number of previous successes Xij and failures j- Xij, i.e. /Jij = O~ii B;, -x•;. This 
is equivalent to the following log-linear model 
giving a Bernoulli GLM with a non-canocnicallog link. This is trivial to implement 
in WinBUGS. However, prior distributions for the regression coefficients should 
reflect the fact that /Jij is a probability i.e. 0 < e{j 1x•;+{j2 (j -xi;) < 1. Hence con-
straints should be imposed to ensure /31 and {32 are negative. This is easily achieved 
in WinBUGS using the following notation to denote right-truncated normal prior 
distributions with upper bound of, say, -0.000001 for /31 and /32 
beta[1] 
N dnorm(O, 0.001) I(,-0.000001) 
beta[2] 
N dnorm(O, 0.001) I(,-0.000001) 
Implementation of user-defined links in WinBUGS is difficult and involves writ-
ing a new software component to execute the specific function (see Section 8.) and 
the Appendix for further details). Alternatively, non-standard links may be imple-
mented by transforming the linear predictor using the inverse link function, although 
strictly speaking, the model is no longer a GLM. The latter approach is used for 
the multinomial-logistic model described in Section 5. 
4. 
Generalized Linear Mixed Models (GLMMs) 
In many applications the variance of the response Yi exceeds the nominal variance 
for the assumed probability distribution. This phenomenon is termed overdispersion. 
Overdispersion can arise in a number of ways, but is most commonly assumed to re-
sult from clustering in the population. That is, the response sample is actually drawn 
from a population consisting of many subgroups. For example, repeat observations 
made on a sample of individuals may be clustered within subjects. Households or 
neighborhoods in a geographical study of disease rates represent another potential 
source of clustering. Overdispersion may also be induced if the response depends on 
an unobserved covariate, with responses sharing the same unknown covariate value 
forming the clusters. 
There are various methods for dealing with overdispersion in GLMs (see the 
chapter by Dey and Ravishanker (this volume) for a discussion). From a Bayesian 

392 
Best & Thomas 
perspective, one of the most natural approaches is to introduce random effects to 
model heterogeneity in the responses. Such models are termed generalized linear 
mixed models (GLMMs) or hierarchical GLMs. 
4.1 
Exchangeable Random Effects 
The simplest form of random effects model introduces an additional parameter 
Ai into the linear predictor for each response unit i. The Ai, i = 1, ... , I are as-
sumed independent and exchangeable and are modeled with a mean zero normal 
population distribution with unknown variance u2 . The latter is often termed a 
hyperparameter, for which a hyperprior distribution must be specified. For com-
putational convenience, WinBUGS parameterizes the normal distribution in terms 
of the mean and precision r = u- 2 • The conjugate prior for a normal precision 
parameter is the gamma distribution, and a gamma(0.001, 0.001) is often chosen as 
a proper but diffuse hyperprior for r (see Section 7.2). 
Figure 23.2 (a) shows how a simple extension of the DoodleBUGS graph for a 
standard GLM allows for the inclusion of exchangeable random effects. The popu-
lation distribution for lambda[i] and the hyperprior for tau are selected from the 
menu of probability distributions as before. Note that the user is not restricted to 
the assumption of normality for the random effects. A particular heavy-tailed pop-
ulation distributions, such as a Student t with small degrees of freedom or a double 
exponential (Laplace) distribution, may be appropriate if outlying observations are 
suspected. 
The strong conditional independence structure exhibited by GLMMs with ex-
changeable random effects makes them remarkably easy to estimate via MCMC 
methods such as those implemented in WinBUGS. However, readers should be aware 
of the potential for nonconvergence and poor mixing of the sampler in certain sit-
uations, particularly when data are sparse (for example, binomial data with small 
denominator) and vague hyperprior distributions are used. Specifying an informa-
tive prior for the random effects precision (see Section 7.2) can often remedy the 
situation. 
4. 2 Correlated Random Effects 
There are many situations where independent exchangeable random effects do not 
adequately capture the pattern of variation in the data. This is particularly so for 
temporal or spatial data where proximity in time or space may lead to correlated 
responses. Correlation amongst the random effects in a G LMM may be modeled 
in a variety of ways. These include direct specification of a multivariate normal 
prior with suitably parameterized covariance matrix for the joint distribution of 
..\ = {..\1, ... , AJ }, or specification of an autoregressive (AR) or conditional autore-
gressive (CAR) prior for the conditional distribution of each random effect given 
the remaining effects and an overall precision parameters r, i.e. p(..\iiAj,ci, r). The 
reader is referred to the chapter by Sun, Speckman and Tsutakawa (this volume) 
for further details. 
The graphical model underlying a GLMM with conditionally specified correlated 
random effects is more complex than the DAGs discussed so far. Links between 
the individual random effects are undirected due to the correlation structure. This 
leads to a chain graph and the rules which WinBUGS uses to construct full condi-
tional distributions from nodes in a DAG no longer apply (Spiegelhalter, Thomas 
and Best, 1996). (Note that it is possible to "trick" WinBUGS into implementing 

23. Software for GLMs 
393 
for(11n 1:1) 
for(lln 1:1) 
(a) 
(b) 
for(l1n 1:1) 
for(lln 1:1) 
(c) 
(d) 
for(l1n 1:1) 
for(l1n 1:1) 
(e) 
(f) 
FIGURE 23.2. Extensions to the DAG for a simple GLM: (a) GLMM with exchangeable 
random effects; (b) missing covariate data; (c) non-ignorable missing response data; (d) 
prediction; (e) classical covariate measurement error; (f) Berkson covariate measurement 
error. 

394 
Best & Thomas 
the correct model for a chain graph by explicitly specifying the required full con-
ditional distributions in the model description 
see Spiegelhalter, Thomas and 
Best (1996) for details). If the correlated random effects are modeled jointly using 
a multivariate prior however, it is possible to treat the vector A = { A1 , ... , AJ} as a 
single multivariate node in the graph. The graph thus reverts to a simple DAG with 
acyclic directed links between all nodes. At the time of writing, it is not possible to 
implement such models in WinBUGS since version 1.1.1 only handles multivariate 
normal nodes with unstructured (inverse) covariance matrices for which a Wishart 
prior must be assumed. However, future versions are intended to allow fully parame-
terized covariance matrices and will include special density functions corresponding 
to the joint distributions underlying certain AR and CAR processes. 
5. 
Polytomous Responses 
Polytomous data comprise responses which are restricted to one of a fixed set of possible 
values or categories. The likelihood is usually taken to be Yi -
Multinomial 
(Ni, tJil, tJi2, ... , 1-JiK) where Yi is a vector of counts (Yit, Yt2, ... , YiK) for each of the K possible 
outcome categories, with N; = I:f=t 
Yik· The parameter J.lik represents the probability of 
observing the kth category for unit i, with 2:~= 1 J.lilc = 1. A standard way to pa-
rameterize the multinomial GLM is by extending the logistic link function to obtain 
the logarithm of the ratio of the probability of each category relative to that of a 
baseline category, say k = 1. That is 
I 
J.lik 
og- = 
J.lil 
(3) 
where {31 = 0 for identifiability and f31c ( k > 1) represents the effect of a unit change 
in covariate Xi on the probability of observing a response in catego1 k relative to 
category 1. Since 2:~= 1 J.lik = 1 and from (3) J.lik = J.lile11•k, then LJc= 1 JJile11ik = 1 
giving J.lil = 1/ 2::=1 eflik. Hence (3) is equivalent to 
(4) 
The latter parameterization must be used in WinBU GS since the transformation 
g(J.lik) =log~ is not explicitly available as a link function in the software. 
An alternative parameterization for the multinomial GLM is to use a Poisson like-
lihood and condition on appropriate totals (Baker, 1995). This will generally be more 
efficient since it avoids the time-consuming evaluation of Ef=1 ef3iXi in ( 4). Suppose 
we assume that the data were in fact generated by as Poisson random variables Yi1c ,....,. 
Poisson(Oi/c) for k = 1, ... , I<. Then conditional on the total count Ni = 2:!<=1 Yi~c, 
the joint distribution of Yi = (Yit, Yi2, ... , YiK) is Multinomial(Ni, J.lib J.li2 1 ... , J.liK) 
with J.lik = Oilc/ Ef=1 Oij. We incorporate the constraint on the multinomial total 
into the Poisson model using the following 'trick'. Let 
(5) 
Then the Poisson likelihood is for the joint distribution of Yi is 
K 
IT 
Y 
(J 
nN· .._..K 
y 
a' X· 
(J· 
.._..K 
efJk' X; 
oi~k e- ik = ui1' eL.tk=l ikfJk • e-
tl L..tk=l 
k=l 

23. Software for GLMs 
395 
Assuming a Gamma(a,b) prior for (}i 1 and integrating gives a marginal likelihood 
for Yi of 
<X 
As a, b ~ 0 the correct multinomial likelihood is obtained. A Gamma(O,O) prior on 
(}il is improper and hence is not allowed in WinBUGS (see Section 7.2). However 
this is formally equivalent to a uniform prior on log (}il, so that we may approximate 
the above model by replacing (5) with 
log Bile = Ai + [3~Xi 
and give Ai a proper but locally uniform prior such as a normal with very large 
variance. 
5.1 
Ordered Categories 
Ordinal data are a special case of polytomous data for which the categories have 
a natural ordering, such as "low", "medium" and "high". We wish to incorporate 
such structure into the model, but in this context it does not make sense to talk 
of ''distance" or "spacing" between pairs of response categories. One solution is 
to assume a multinomial GLM but to define the linear predictor as a function 
of the cumulative probabilities 'lriJc = Lj<k J.lij instead of the individual category 
probabilities J.lii (Agresti, 1989). For example 
Multinomial(Ni, J.lil, J.li2, ... , J.liK) 
= 61c- [3tXi 
(6) 
(7) 
The unknown parameters 61c may be interpreted as cutpoints on the scale of an 
underlying continuous latent variable Zi. If the true but unobserved value of Zi 
lies in the interval [6~c-1,61c), where 60 = -oo, DK = oo and 61 < ... < DK-1, 
then category k will be recorded. Various distributions may be assumed for Zi, the 
most popular of which is the logistic distribution with mean {31 Xi and scale = 1. 
This corresponds to assuming a logistic link function g(.) in (7). Other common 
choices for g(.) include the complementary log-log link (corresponding to the as-
sumption of an extreme value distribution for the latent variable Zi) or the pro bit 
link (corresponding to the assumption of a Normal distribution for Zi)· 
Implementation of cumulative logit models is straightforward using WinBUGS 
(see Best et al. 
(1996) for an example). In addition to specifying equations (6) 
and (7), the user must include deterministic expressions for the individual category 
probabilities J.lik = 'lriJc-'lri,k-1 (where J.l1 = 1r1 and J.lK = 1-'lrJc-1) and prior distri-
butions for f3 and each unknown cutpoint OJc. Order constraints must be imposed on 
the latter using the I ( ~) notation to denote a bounded distribution in WinBUGS 
delta[1] 
N dnorm(O, 1.0E-6) I( 
, delta[2]) 
delta[2] 
N dnorm(O, 1.0E-6) I(delta[1], delta[3]) ..... 
Note that the model presented by Best et al. (1996) uses a categorical distribution 
for the response likelihood. This is formally equivalent to a multinomial with Ni = 1, 
and is appropriate, for example, when repeated ordered categorical responses are 
available for each subject, and an individual level analysis with time-dependent 
covariates is required. 

396 
Best & Thomas 
6. 
Adding Complexity in GLMs/GLMMs 
6.1 
Missing Data 
Missing data are handled very naturally within the Bayesian framework, being 
treated in the same way as all other unknown quantities in the model. Posterior 
inference is based on the joint distribution of parameters f3 and missing data Ymiss 
given the modeling assumptions and the observed data Yobs. In practice, this in-
volves simulating values from the conditional distribution of Ymiss given Yobs and 
the current values of [3, followed by simulation from the conditional distribution of 
f3 given the complete data Y = {Yobs, Ymiss }, where Ymiss take on their currently 
estimated values. The marginal posterior for f3 involves integrating the joint poste-
rior p(f3, YmissiYobs) over the distribution of missing values. The posterior estimates 
of f3 are thus fully adjusted for uncertainty due to the missing data. 
The missing value code in WinBUGS is NA: whenever the software encounters this 
code in a data file, it will automatically treat that node as an unknown quantity and 
generate values from the appropriate full conditional during MCMC simulation. If 
the missing values occur in the response variable, the structure of the graph remains 
unchanged from the complete data case and WinBUGS simply generates samples 
of Ymiss from p(Yif3), the likelihood distribution specified for the response variable. 
If missing covariates Xmiss are involved, the graph must be extended to include 
an exposure distribution for X, since all unknown stochastic quantities must be 
assigned a probability distribution within a fully Bayesian model. Suitable exposure 
distributions for continuous covariates include a normal distribution with mean 'if; 
and precision w (see Figure 23.2 (b)). The hyperparameters 'if; and w may be fixed 
a priori or assigned (possibly) vague hyperpriors (the observed covariate values 
Xobs will contain information by which to identify 'if; and w ). Suitable exposure 
distributions for binary or polytomous covariates include Bernoulli or categorical 
distributions respectively. 
6.2 Informative Missing Data 
Rubin (1976) discusses a number of mechanisms by which missing data may 
arise in practice. Let Mi be a binary indicator taking value 0 if Yi E Yobs and 1 if 
Yi E Ymiss and consider the following models for the missing data mechanism 
p(MIY, e) = p(Mie) 
p(MIY,e) = p(MIYobs,e) 
p(MjY,e) = p(M!Yobs, Ymiss,e) 
(8) 
(9) 
(10) 
where e are nuisance parameters controlling the probability of non-repsonse. Model 
(8) represents data missing completely at random (MCAR), in which the the proba-
bility of non-response is constant (e). Model (9) represents data missing at random 
(MAR), in which the probability of non-response may depend on observed data. 
For example, subjects in a longitudinal study who record an extreme response on 
one occasion may be more likely to dropout at the next occasion than subjects 
with more moderate responses. The probability of non-response may also depend 
on observed covariate values under the MAR mechanism. Model (10) represents an 
informative missing data mechanism. Here, the probability of non-response depends 
directly on the value of the response which would have been recorded had it been 
measured. For example, in a study of depression in the elderly, death may represent 
an informative dropout mechanism. Depression scores cannot be measured for those 

23. Software for GLMs 
397 
individuals who have died but are likely to be higher than in the surviving subjects 
since severe depression is a risk factor for death. 
Under MCAR or MAR missing data mechanisms, it can be shown that the 
marginal posterior distribution of the parameters of interest does not depend on 
the pattern of missing data i.e. p(fiiYobs, M) = p(fiiYobs)· The missing data mech-
anism is said to be ignorable and there is no need to include an explicit model for 
Min the analysis. The automatic handling of missing response data in WinBUGS 
assumes such an ignorable mechanism. 
If an informative mechanism is suspected, the user must augment the data set 
with the missing data indicator M and explicitly define p(MIYmiss,e) in the model 
specification. Two frequently used models for an informative missing data process 
are the pattern-mixture model (Little, 1993) and the selection model (Diggle and 
Kenward, 1994) . Best et al. (1996) present an example of how to implement the 
latter using the BUGS software; the general formulation is as follows 
Bernoulli(Pi) 
= (o + (1 Yi 
(11) 
where Yi will be missing whenever Mi = 1. The corresponding DoodleBUGS graph 
is shown in Figure 23.2 (c). Note that the linear predictor (11) may also depend 
on observed responses and covariates. An informative prior is generally required 
for ( 1 , and the user is advised to investigate sensitivity of the resulting inference 
to different assumptions for this prior and to the assumed model for missing data 
mechanism. 
6. 3 
Prediction 
The posterior predictive distribution for an observable Y is given by 
P(Yi,newiY) = j P(Yi,new, f'IY)dfi = j P(Yi,newlfi)p(f'IY)dfi 
where p(Yi,new It') is the likelihood for a future response Yi,new and p(fiiY) is the 
posterior distribution of fi given the observed data Y. Samples from the posterior 
predictive distribution are easily obtained by generating draws from the posterior 
distribution of fi and then simulating new values Yi,new by forward sampling from 
the likelihood conditional on the current value of fi. These samples may be obtained 
in WinBUGS by simply adding a new node y. new [i] to the graph with the same 
parents as the observed response data (see Figure 23.2 (d)). 
6.4 
Covariate Measurement Error 
Measurement error on covariates is widespread in practice, and may lead to serious 
bias in the estimated regression coefficients if ignored. Readers are referred to the 
chapter by Wakefield and Stephens (this volume) for a detailed discussion of this 
problem. 
Classical measurement error 
Let Xi denote the true covariate of interest, but suppose we only observe an im-
precise measurement Wi. The classical measurement error model for a continuous 
covariate assumes that the observed value depends on the true value according to 
some probability model with error variance cr~. For example, Wi ,...... Normal( Xi, rf), 

398 
Best & Thomas 
where Te = u;- 2 . Both Xi and rf are unknown quantities which must be assigned 
prior distributions in a full probability model. The exposure distribution for the true 
covariate Xi is typically assumed to be normal with unknown mean 'if; and precision 
w which are assigned vague hyperpriors. The prior forTe must be informative about 
the expected precision of wi as a proxy for xi' since this parameter is not identifi-
able from the observed data. However, if a relevant calibration sample (containing 
measurements of both Wi and Xi for some units i) or repeated proxy measurements 
Wij (j = 1, 2, ... ) for each Xi are included in the model, then these extra data do 
contain information about the measurement precision and so a more diffuse prior 
may be assumed for rf. The DoodleBUGS graph for a GLM with classical covari-
ate measurement error is shown in Figure 23.2 (e). Note that the structure of this 
model is similar to that for the model with missing covariate data (Figure 23.2 {b)) 
except for the addition of "children" for xi in the form of the imprecisely observed 
covariate values wi. 
Classical measurement error on a binary covariate may be modelled as 
Wi 
Bernoulli(pi) 
logit(Pi) 
ao + a1Xi 
where e010 = Pr(Wi = liXi = 0) and e010+011 = Pr(Wi = liXi = 1). Again, 
informative priors must be specified for a 0 and a1 unless a relevant calibration 
sample or repeated measurements Wij are available. 
Berkson measurement error 
Berkson measurement error models assume that the true covariate depends on 
the observed value. Such errors typically occur in experimental studies, where Wi 
are the nominal values of pre-set design points such as time of measurement or 
machine setting, and Xi are the actual measurement times or values output by 
the machine. The DoodleBUGS graph for Berkson measurement error in a GLM 
is shown in Figure 23.2 (f). Note that the direction of the arrow between Xi and 
Wi is the reverse of the classical measurement error model (Figure 23.2 (e)). Hence 
there is no need to specify a separate exposure distribution for the unknown true 
covariate Xi since under Berkson error this depends directly on the observed value 
Wi. However, an informative prior or calibration sample are still required for the 
precision parameter rf. 
7. 
General Advice on Modeling Using WinBUGS 
In this section we consider some general issues concerning the specification and 
implementation of Bayesian models using WinBUGS. Further discussion of these 
and related topics can be found in a recent paper on MCMC methods and their 
application by Brooks (1998). 
7.1 
Parameterization 
Samples generated using MCMC algorithms can sometimes exhibit poor mixing. 
That is, the sampler does not move rapidly throughout the support of the target 
distribution. Poor mixing may be identified by examining plots of the sample trace 
and autocorrelation functions in WinBUGS for evidence of high within-chain corre-
lations. This may lead to slow convergence and reduce the efficiency of the Markov 
chain for posterior estimation (see Section 7. 3 for further discussion of these issues). 

23. Software for GLMs 
399 
Poor mixing often occurs when variables in the model are nearly collinear. Repa-
rameterization may reduce these correlations, and a number of simple techniques 
are discussed by Gilks and Roberts (1996). In particular, we recommend that co-
variates appearing in the linear predictor of a GLM or GLMM should always be 
standardized about their sample mean 
Scaling covariates by the sample standard deviation may also be appropriate. This 
leads to approximate posterior orthogonality between the regression coefficients 
{,Bk}. 
Gelfand, Sahu and Carlin (1995) recommend a strategy known as hierarchical 
centring to reduce posterior correlations amongst random effects in a GLMM. This 
involves introducing extra layers into the hierarchical model to provide a «better 
behaved posterior surface". For example, consider a GLMM with linear predictor 
g(pi) = ,80 + ,81Xi + Ai where Ai "' Normal(O, r). The full hierarchically centred 
parameterization is 
A partial hierarchically centred parameterisation of the same model is 
Ai "' N ormal(,Bo, r) 
In general, hierarchical centring will work best for random effects which have a 
large posterior variance. However, the theory underlying selection of the most effi-
cient parameterization is difficult and problem-specific, and users are recommended 
to experiment with different forms of full-, partial- and non-hierarchically centred 
random effects to find the best parameterization for a given application. 
Poor mixing may also arise as a direct consequence of the random walk behaviour 
exhibited by the Gibbs sampling algorithm. Neal (1998) proposes an «overrelaxed" 
variant of the Gibbs sampler which generates new values that are negatively corre-
lated with the current values. This tends to cause sampled values to alternate from 
one side of the conditional mean to the other at consecutive updates. The net result 
is sample trajectories which move in a consistent direction (subject to some ran-
dom deviations and reflection from the tails of the distribution), thus covering the 
sample space more efficiently. WinBUGS provides an option for the user to request 
overrelaxed sampling where possible: the sample time per iteration will increase 
but within-chain correlations will generally be lower. Hence fewer iterations may be 
necessary to achieve convergence and the desired efficiency of the chain for posterior 
inference. 
Over-parameterized models 
An over-parameteriszd model results in some parameters being nonidentifiable in 
the likelihood and in the prior. However, it is still feasible (but potentially danger-
ous) to run an MCMC sampler for such a model provided that posterior inference 
is only based on identifiable functions of the parameters. Since there is no notion 
of convergence for nonidentifiable terms in the model, examination of trace plots 
and other diagnostics is only meaningful for the identifiable functions. For example, 
consider the following parameterizations for the linear predictor of a GLM with 
covariate xi representing a 3-level factor: 

400 
Best & Thomas 
{i) Explicit aliasing 
g(J.ti) = f3o + f3xi xi = 1, 2, 3 
{31 = 0; f3o, {32, f3s ,...., Independent vague priors 
(ii) Identifiable contrasts 
g(J.ti) 
f3o I f3t I !32 I !33 
f3o 
,...., 
Independent vague priors 
1 3 
= [3~ + 3 L f3Z ; !32 = !3; - !3; ; !33 = !3; - !3; 
k=1 
Parameterization {i) employs the usual corner-point constraint to alias one of the 
factor effects ([31) to zero. The remaining factor coefficients are identifiable directly 
and measure the effect of level k > 1 of factor X relative to level k = 1. Parame-
terization {ii) imposes no constraints on the factor coefficients. These are no longer 
identified since adding a constant to each [3j. and subtracting the same constant 
from {30 does not affect the probability. However, constrasts such as f32 - f3t are 
identified since the arbitrary constant cancels. 
Both models may be implemented in WinBUGS: it is the user's responsibility to 
check for over-parameterization and to ensure that posterior inference is based only 
on identifiable quantities. 
7.2 Prior Specification 
WinBUGS requires that a full probability model is defined and so does not allow 
improper prior distributions. This precludes the use of many standard "reference" 
or "non-informative" prior distributions. Here we offer some brief advice on how to 
choose proper priors for the unknown parameters of a GLM or GLMM in WinBUGS. 
Further details can be found in the Classic BUGS manual (Spiegelhalter et al., 1996). 
Lack of prior knowledge for a location parameter such as a regression coefficient 
may be expressed by assuming a locally uniform prior. The most common choice is 
a mean-zero normal distribution with large variance, s 2• A rule-of-thumb is to set 
s2 at least one to two orders of magnitude greater than the expected value of the 
regression coefficient. (Recall that WinBUGS parameterizes the normal distribution 
in terms of the mean and precision, so that the actual prior specified in the code 
will have very small precision = s- 2). 
Specification of a proper prior to express lack of knowledge for the precision of 
the random effects in a GLMM requires care. A widely used option is to a adopt a 
gamma( t, t) prior where t is a very small number such as 0.001. This represents a 
"just proper" form of the improper gamma(O,O) prior which is formally equivalent 
to assuming a uniform distribution on the log of the scale parameter u = r- t. 
It is often better to specify a moderately informative proper prior for the random 
effects precision parameter. GLMMs typically lead to inference concerning relative 
risk or odds ratios in which interpretation of the variance of a random effect is inde-
pendent of context. For example, a normal random effect with standard deviation 
u = 0.59 implies that 95% of subjects with identical covariates will have log odds 
(e.g. binomial logistic model) or log relative risk (e.g. Poisson log link model with 
offset) within a range of width 2 x 1.96 x 0.59 = 2.3 = log 10; that is, the 97.5% 
quantile of the distribution of odds ratios or relative risks for identical subjects 

23. Software for GLMs 
401 
will be one order of magnitude greater than the 2.5% quantile. This interpreta-
tion provides a mechanism by which to choose a suitable informative prior for the 
random effects precision parameter r in many GLMM applications. For example, 
suppose we thought it plausible that there was roughly one order of magnitude 
difference between the odds or relative risk of the response of interest for subjects 
with identical observed covariates. This implies that (2 X 1.96/ log 10)2 = 2.90 is 
a reasonable guess for r. Suppose also that we would be surprised to find more 
than 2 orders of magnitude difference between all but the most extreme 5% of such 
subjects. This gives an "low" value for T (i.e. an upper value for the variance) of 
(2 x 1.96/ log 100)2 = 0.73. A gamma(3,1) distribution has a mean of 3 and 96% 
probability of exceeding 0.73 and hence might be an appropriate prior for r in this 
context. 
7.3 
Convergence and Posterior Sample Size 
Two of the most important implementation problems in any practical application 
of MCMC methods are (i) determining the number of initial iterations to discard 
as "burn-in" and (ii) deciding how long the simulations should be run following 
convergence. 
To avoid potential bias due to the influence of arbitrary starting values, an initial 
portion of the simulation should be discarded. This is known as the "burn-in" and 
represents samples generated during an initial transient phase before the simulation 
has converged (at least approximately) to the posterior target distribution. Formal 
methods for assessing convergence (and hence the length of the required burn-in) 
have been the topic of a large and somewhat controversial literature. However, no 
method has yet provided a global and foolproof diagnostic (see Cowles and Carlin 
(1996) and Robert (1996) for reviews). We recommend a strategy of examining 
several different diagnostics before deciding on an appropriate "burn-in" period 
for a given model. WinBUGS provides a number of informal graphical diagnostics 
such as plots of the sample trace, autocorrelation function and running mean and 
95% quantiles. At the time of writing, no formal diagnostics are implemented in 
WinBUGS version 1.1.1. However future versions of the software are intended to 
include a facility for running multiple simulations using different starting values in 
order to calculate the diagnostic proposed by Gelman and Rubin (1992) based on 
classical analysis of variance of the between and within chain variation. 
Determining when to stop the simulation after convergence has been achieved 
depends on the level of precision required for posterior inference. The asymptotic 
variance of the Monte Carlo estimator (i.e. sample average) of a posterior expec-
tation E1r(j(O)) is given by u2 /N where N is the sample size and u 2 is a positive 
constant depending on the posterior variance of f(O) and the sample autocorre-
lation. Various methods exist for estimating u 2 ; WinBUGS uses the batch means 
method outlined by Roberts (1996), and reports the quantity u/VN in the column 
headed M ante Carlo error of the summary statistics table for each monitored node 
in the model. The simulation should be run until this error term is suitably small 
(say, 5% or 1% of the estimated posterior mean). However, care must be taken to 
ensure that N is large enough for the batch means approximation of u2 to be valid: 
users are thus recommended to run a minimum of 1000 iterations following the 
"burn-in" period in order to obtain a reliable estimate of the Monte Carlo error in 
WinBUGS. 

402 
Best & Thomas 
1.4 
Model Checking 
Methods for Bayesian model criticism have been widely debated since the intro-
duction of MCMC methods into mainstream statistics opened the way for practical 
application of Bayesian inference to real-life problems. Many purist still argue that 
the Bayes factor, based on the ratio of the marginal distributions of the observed 
data under two competing models, is the only criterion necessary for model compar-
ison. However, there are serious problems with the interpretation and computation 
of the Bayes factor (see Gelfand (1996) for a brief discussion) which have led many 
authors to suggest alternative approaches. These include Bayesian model averaging 
(Raftery, Madigan and Hoeting, 1997) , cross-validation procedures (Gelfand, Dey 
and Chang, 1992) and criteria based on the log-likelihood distribution (Gelfand and 
Ghosh, 1998; Spiegelhalter, Best and Carlin, 1998). 
WinBUGS provides no formal tools for model checking or comparison. However, 
the user is at liberty to implement many of the methods proposed in the litera-
ture. Section 9.3 of the Classic BUGS manual (Spiegelhalter et al., 1996) outlines 
how some of these may be achieved, and the chapter by Dellaportas, Forster and 
Ntzoufras (this volume) provides further illustrations of Bayesian variable selection 
methods which may be implemented in WinBUGS. The posterior predictive model 
checks discussed in the chapters by Dey and Ravishanker (this volume) and Albert 
and Ghosh (this volume) may be implemented by generating samples Ynew from 
the posterior predictive distribution as described in Section 6.3. A deterministic 
node set equal to the log likelihood (calculated explicitly by the user) may also be 
included in the WinBUGS model specification. Posterior samples of this node may 
then be used to compute global model comparison criteria such as the expected pre-
dictive deviance (Gelfand and Ghosh, 1998) or the deviance information criterion 
(Spiegelhalter, Best and Carlin, 1998). 
8. 
Extending the WinBUGS Software 
The aim of previous sections has been to provide a flavor of some of the models 
and problems that can be handled using the graphical modeling approach in Win-
BUGS, and to illustrate the flexibility of the software to extend standard GLMs and 
accommodate almost arbitrary complexity. With some thought, most of the models 
discussed in other chapters of this book could be implemented using the existing 
WinBUGS program. However, many of these involve sophisticated, state-of-the art 
model design and analytic techniques, and their implementation in WinBUGS may 
prove difficult and inefficient compared with using the authors' problem-specific 
computer code. By its nature, routine statistical software such as WinBUGS must 
lag behind cutting-edge statistical methodology: it is neither feasible nor desirable to 
implement every new algorithm and model that becomes available. Nonetheless, the 
ability of researchers to utilize standard features of the WinBUGS package, such as 
the model specification language, input/output facilities and error checking, during 
their own methodological development work would be highly advantageous. 
As already discussed, WinBUGS is constructed using a graphical model frame-
work. This facilitates software extensibility in much the same way as the graph 
underlying a statistical model may be extended. That is, by exploiting "conditional 
independencies" between the components which form the building blocks of the 
software graph. Inclusion of new methods and applications is achieved by writing 
extra components which simply either "plug in" to relevant slots in existing mod-

23. Software for GLMs 
403 
ules or make use of existing modules without requiring any part of the software 
to be recompiled. This feature represents a key strength of the WinBUGS design, 
and both facilitates the ongoing development of the core program and provides the 
structure which will enable users to write customised "add-on" routines to link into 
and exploit existing facilities within the software. We anticipate that such "ad d-ons" 
will cover three main requirements: (i) new logical functions; (ii) new probability 
distributions; and (iii) new sampling algorithms. An example outlining the steps 
involved in writing a component to implement a new logical function is given in the 
Appendix. Interested readers should contact the authors for further information. 
The WinBUGS software, documentation and worked examples are freely available 
over the World Wide Web from http: I /www. mrc-bsu. cam. ac. uk. 
section* Appendix: Software components 
WinB U GS builds an internal representation of the graphical model out of software 
components (objects). The nodes of the graph are represented by objects of type 
"Node" and the edges of the graph by pointers to node objects. The nodes of the 
graph can be classified into two families: type "Stochastic" and type "Logical". 
Stochastic nodes are quite complex and will not be discussed further. Here we 
consider the simpler logical node. 
The component interface for a logical node describes the list of implemented en-
tities (methods) that can be used in other components. These fall into two groups: 
methods used to build the graphical model (Set, Check, CanEvaluate, Map, Opti-
mize, /sConstant, Parents and Path) and methods used for the MCMC inference 
(Value, Monitored Value, Log Value, LogitLogQ). The Set method is used to set up 
any hidden attributes of the node such as its parents; the Check method can imple-
ment various consistency checks on the graphical model; the CanEvaluate method 
is a predicate which decides if the parents of a logical node have been initialized; 
the Map method allows the software to classify the functional form of the logical 
node and hence optimize the sampling methods used; the Optimize method allows 
a logical node to replace itself by a more efficient version; the IsConstant method 
is a predicate that tests whether the logical node is a function of only fixed value 
nodes; the Parents method finds all parents of the logical node that are of type 
"Stochastic" and are not data; the Path method is a predicate that tests if there 
is a path between two nodes; the Value method calculates the value of all the par-
ents of a logical node and then calculates the logical expression associated with 
the node using these values; the Monitored Value method is an optimization of the 
Value method used when monitoring nodes during the simulation; the Log Value and 
LogitLogQ methods calculate functions of the value of a logical node. 
To create a new type of logical node, an extension of the type "Logical" is made, 
in which each of the above methods must be implemented. For example, suppose 
we wish to create a component to implement the solution of Keplers equation x = 
l + esin(x), which would be used to define a logical node in the BUGS language 
viz. x <- kepler(l, e). A new object must be made, called type "Kepler" (say), 
which is a pointer to a node of type "Logical". Each method required for a node 
of type "Logical" must then be defined for the new type "Kepler" together with 
any additional procedures required to implement the node. Methods are defined 
by writing PROCEDURE statements. For example, the Value method for the Kepler 
object is 
PROCEDURE (keppler:Kepler) Value(): REAL; 
VAR I, e: REAL; 
BEG II 
I := kepler.I.Value(); e := kepler.e.Value(); 
RETURI SolveKepler(I, e) 
EID Value 
where SolveKepler is a new procedure which solves the Keplers equations. Having 

404 
Best & Thomas 
defined all the required methods and new procedures, the component interface is 
then written. This is quite simple and describes which of the entites implemented 
in the Kepler component can be used by other components 
DEFIIITIOI GraphKepler; 
IMPORT Graphlodes; 
VARi 
. 
d r-: Graphlodes.D1rectory; 
PROCEDURE Install; 
PROCEDURE SetDir (d: Graphlodes.Directory); 
EID GraphKepler. 
Here, the entity dir can be used to create a "Kepler node". The component interface 
is encoded in a machine readable format called a symbol file. This file allows the 
consistency of the component interfaces to be checked both at compile time and 
link time, thus improving the reliability of the software. 
References 
Agresti, A. (1989). A survey of models for repeated ordered categorical response 
data. Statistics in Medicine, 8, 1209-24. 
Albert, J. and Ghosh, M. (1999). Item response modeling. In Generalized linear 
models: a Bayesian perspective, (ed. D. Dey, S. Ghosh, and B. Mallick). Marcel 
Dekker, New York. 
Baker, S. (1995). The multinomial-Poisson transformation. The Statistician, 43, 
495-504. 
Best, N. G., Spiegelhalter, D. J ., Thomas, A., and Brayne, C. E. G. (1996). 
Bayesian analysis of realistically complex models. Journal of the Royal Stat-
istical Society, Series A, 159, 323-42. 
Brooks, S. (1998). Markov chain Monte Carlo method and its application. The 
Statistician, 47, 69-100. 
Cowles, M. and Carlin, B. (1996). Markov chain Monte Carlo convergence diag-
nostics: a comparative review. JAm Statist Assoc, 91, 883-904. 
Dellaportas, P., Forster, J., and Ntzoufras, I. (1998). Bayesian variable selection 
using the Gibbs sampler. In Generalized linear models: a Bayesian perspective, 
( ed. D. Dey, S. Ghosh, and B. Mallick). Marcel Dekker, New York. 
Dey, D. and Ravishanker, N. (1998). Bayesian approaches for overdispersion in gen-
eralized linear models. In Generalized linear models: a Bayesian perspective, 
(ed. D. Dey, S. Ghosh, and B. Mallick). Marcel Dekker, New York. 
Diggle, P. J. and Kenward, M.G. (1994). Informative dropout in longitudinal data 
analysis. Applied Statistics, 43, 49-94. 
Gelfand, A. (1996). Model determination using sampling-based methods. In Markov 
chain Monte Carlo in practice, ( ed. W. R. Gilks, S. Richardson, and D. J. 
Spiegelhalter), pp. 145-61. Chapman and Hall. 
Gelfand, A., Dey, D., and Chang, H. (1992). Model determination using predictive 
distributions with implementation via sampling-based methods. In Bayesian 
statistics 4, ( ed. J. M. Bernardo, J. 0. Berger, A. P. Dawid, and A. F. M. 
Smith), pp. 147-68. Oxford University Press. 

23. Software for GLMs 
405 
Gelfand, A. E. and Ghosh, S. (1998). Model choice: a minimum posterior predictive 
loss approach. Biometrika. 85, pp.1-11. 
Gelfand, A. E., Sahu, S. K., and Carlin, B. P. (1995). Efficient parameterisations for 
generalized linear models. In Bayesian Statistics 5. Clarendon Press, Oxford, 
UK. 
Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using 
multiple sequences. Statistical Science, 7, 457-72. 
Gilks, W. R. and Roberts, G. 0. (1996). Strategies for improving MCMC. In 
Markov chain Monte Carlo in practice, (ed. W. R. Gilks, S. Richardson, and 
D. J. Spiegelhalter), pp. 89-114. Chapman and Hall. 
Lauritzen, S. L., Dawid, A., Larsen, B., and Leimer, H. (1990). Independence 
properties of directed Markov fields. Networks, 20, 49-505. 
Little, R. (1993). Pattern-mixture models for multivariate incomplete data. J. Am. 
Statist. Assoc., 88, 125-34. 
Neal, R. M. (1998). Suppressing random walks in Markov chain Monte Carlo using 
ordered overrelaxation. In Learning in graphical models, (ed. M. Jordan), p. (to 
appear). Kluwer Academic Press. 
Raftery, A. L., Madigan, D., and Hoeting, J. (1997). Bayesian model averaging for 
linear regression models. J Amer Statist Assoc, 92, 179-91. 
Robert, C. (1996). Convergence assessments for Markov chain Monte Carlo meth-
ods. Statistical Science, 10, 231-53. 
Roberts, G. (1996). Markov chain concepts related to sampling algorithms. In 
Markov chain Monte Carlo in practice, (ed. W. R. Gilks, S. Richardson, and 
D. J. Spiegelhalter), pp. 45-57. Chapman and Hall. 
Rubin, D. B. (1976). Inference and missing data. Biometrika, 63, 581-92. 
Smith, A. F. M. and Roberts, G. 0. (1993). Bayesian computation via the Gibbs 
sampler and related Markov chain Monte Carlo methods (with discussion). J 
Roy Statist Soc B, 55, 3-24. 
Spiegelhalter, D., Best, N., and Carlin, B. (1998). Bayesian deviance, the effective 
number of parameters, and the comparison of arbitrarily complex models. 
Technical report, MRC Biostatistics Unit, Cambridge. 
Spiegelhalter, D. J. (1998). Bayesian graphical modeling: a case-study in monitor-
ing health outcomes. Applied Statistics, 47, 115-33. 
Spiegelhalter, D. J ., Thomas, A., and Best, N. G. (1996a). Computation on Bayesian 
graphical models. In Bayesian Statistics 5, (ed. J. M. Bernardo, J. 0. Berger, 
A. P. Dawid, and A. F. M. Smith), pp. 407-25. Clarendon Press, Oxford. 
Spiegelhalter, D. J., Thomas, A., Best, N. G., and Gilks, W. R. (1996b). BUGS 
0.5 Bayesian inference using Gibbs sampling manual (version ii). Medical 
Research Council Biostatistics Unit, Cambridge. 

406 
Best & Thomas 
Sun, D., Speckman, P., and Tsutakawa, R. (1999). Random effects in generalized 
linear mixed models (GLMMs). In Generalized linear models: a Bayesian per-
spective, (ed. D. Dey, S. Ghosh, and B. Mallick). Marcel Dekker, New York. 
Wakefield, J. and Stephens, D. (199). Bayesian errors-in-variables modeling. In 
Generalized linear models: a Bayesian perspective, (ed. D. Dey, S. Ghosh, and 
B. Mallick). Marcel Dekker, New York. 

Index 
Ability parameters 
conditional distribution 
item response modeling, 184 
Absolute residuals 
male flour beetle data set, 268t 
Acute myocardial infarction practice 
guidelinespost (see Post acute 
myocardial infarction practice 
guidelines) 
Additive Log Ratio (ALR) 
Aitchison's transformation, 359-
361 
function, 349-350 
Adjusted density method (ADM) 
small area inference, 95-96 
AIDS clinical trials 
variable selection, predictive 
approach,288 
Aitchison's Additive Log Ratio (ALR) 
transformation, 359-361 
Akaike Information Criteria (AIC), 16 
Albert's model, 7 
Alcohol consumption 
errors-in-variables, 340 
Analysis of regression models 
censored survival data, 287 
Analysis of survival data 
mixture-model approach, 255-268 
Gibbs sampler, 259-260 
MCEM, 256-259 
model selection, 260-261 
mixture-model approach example, 
261-268 
EM algorithm, 262-263 
Gibbs samplers, 263-265 
numerical results, 265-268 
Analysis of survival models 
MCMC, 287 
Asymmetric links 
generation, 239 
Asymptotic theory 
MLE,5 
Autocorrelated random effects, 30 
Autoregressive (AR) model of Ord, 27 
Baseline hazard rate 
model, 289 
cumulative, 289-290 
prior distribution 
Cox variable selection, 289-292 
Basic marginal likelihood identity, 124 
Basis functions 
classification trees, 366-367 
example, 366f 
Bayes' approach 
linear, 60-61 
Bayesian analysis 
classification trees, 368-369 
compositional data, 349-362 
parametric approach, 351-352 
posterior distributions and 
estimation, 355-359 
results, 359-361, 360t-361t 
semiparametric approach, 354-
355 
simulation based model 
determination, 352-354 
correlated ordinal data models, 
133-155 
GLM, 273-284 
informative prior elicitation, 44-46 
likelihood analysis, 6 
logit regression model, 243 
model choice, 15-16 
Bayesian computations, 36-37 
Bayesian deviance, 16 
Bayesian fitting 
one-parameter model 
item response modeling, 185 
Bayesian generalized linear models 
post AMI practice guideline 
development, 209-210 
small area inference, 89-105 
challenges and future directions, 
102-104 
computational issues, 96-100 
Poisson regression models, 94-96 
U.S. mortality data, 100-102 
Bayesian graphical models 
computation, 388-389 
conditional independence 
structures, 387-389 
constructing software, 389 
marginal posterior distribution, 388 
Markov chain Monte Carlo 
(MCMC) methods, 388-389 
WinBUGS, 389 
Bayesian hierarchical logistic 
regression 
coronary angiography 
appropriateness, 202-203 
407 

408 Index 
[Bayesian hierarchical logistic 
regression] 
post AMI practice guideline 
development, 209-210 
Bayesian inferences 
hierarchical GLMMs, 36-37 
Markov Chain Monte Carlo 
(MCMC)-based approaches, 
62-65 
Bayesian MARS (BMARS), 222-223 
generalized linear models, 221-228 
motivating example, 224-225 
Pima Indian example, 225-228 
Bayesian method 
correlated binary data, 113-129 
time series count data, 159-171 
Bayesian model, 5-8 
adequacy criterion, 14-15 
based method 
post AMI practice guideline 
development, 198-199 
diagnostics 
correlated binary data, 313-326 
multivariate exponential power 
distribution family links 
(MVEP), 137 
ordinal probit 
post AMI practice guideline 
development, 209-210 
partition 
classification trees, 371 
Bayesian procedure 
Markov Chain Monte Carlo 
(MCMC) implementation, 12 
Bayesian residual posterior 
distribution 
box plots, 250, 250f 
Bayesian residuals 
item response modeling, 186-187 
Bayesian two-stage prior distribution 
small area inference, 96-97 
Bayesian variable selection, 48-50 
Cox model, 287-309 
Gibbs sampler, 273-284 
Bayesian view 
generalized linear models (GLMS), 
3-17 
BAYESTAT, 121, 122-123 
correlated binary data, 127 
Bayes' theorem, 59 
Berkson measurement error model, 
332,341 
Bernoulli distribution, 390 
Bernoulli random variables 
small area inference, 91-92 
Beta processes, 10 
Binary regression 
data adaptive Bayesian analysis, 
244-248 
exponential power distribution, 
246-248 
normal distribution, 245 
data adaptive robust link functions, 
243-251 
binary regression model, 244-248 
numerical illustration, 248-250 
outliers detection, 248 
nonparametric approach, 219 
parametric family of link functions, 
232 
Binary response hierarchical model 
correlated binary data, 122 
Binary response regression 
application, 237-239 
Dirichlet process prior, 234-236 
finite mixture model, 233-234 
general mixtures, 234-236 
link functiong modeling, 218-219 
model diagnostic, 236-237 
normal scale mixture links, 231-
240 
Binomial distribution, 5, 25, 390 
Binomiallogit hierarchical model, 8 
small area inference, 97 
Birth step 
classification trees, 368 
Box-Cox transformation, 350-351, 
359-361 
Boxplots 
Bayesian residual posterior 
distribution, 250, 251f 
kyphosis dataset, 226f 
posterior 
multivariate probit (MVP), 119f 
Probit hierarchical model, 123f 
probit normal model, 121f 
Breast cancer 
classification trees, 369-371, 370f, 
371t 
Brooks's method, 8 
small area inference, 97 
BUGS, 16 (see also WinBUGS 
codes, 282-284) 
logistic models with 2 binary 

[BUGS] 
explanatory factors, 283-284 
log-linear models for 2a 
contingency table, 282-283 
small area inference, 94 
Markov Chain Monte Carlo 
(MCMC), 96 
Cancer clinical trials 
variable selection 
predictive approach, 288 
Can Evaluate method 
graphical model, 403 
Canonical link function, 224 
small area inference, 98 
Canonical parameters, 5, 12, 13 
Cargo data analysis 
OGLMs, 80 
Carlin and Chib's 
algorithm, conditional distribution 
sampling, 122, 129 
method, Gibbs sampler variable 
selection strategies, 275 
Carstairs' index, 333 
Categorical data 
GLM, 365 
Censored survival data 
analysis of regression models, 287 
Check method 
graphical model, 403 
Chen and Shao method 
Monte Carlos posterior estimates, 
301 
Chib and Carlin's 
algorithm, conditional distribution 
sampling, 122, 129 
method, Gibbs sampler variable 
selection strategies, 275 
Chib's method 
marginal likelihood, 127 
Chi-squared discrepancy measure, 
102 
Classical approach 
classification trees, 367 
Classical estimation procedures 
GLMs, 5 
Classical logistic regression model 
vs. logistic regression estimate, 
224-225,225f 
Classical MARS 
semiparametric generalized linear 
Index 409 
[Classical MARS] 
models, 221-222 
Classical measurement error 
WinBUGS, 398-399 
Classification and regression trees 
(CART), 221 
Classification trees, 365-371 
basis functions, 366-367 
example, 366f 
Bayesian approach, 368-369 
Bayesian partition model, 371 
birth step, 368 
breast cancer, 369-371, 370f, 371t 
classical approach, 367 
death step, 368 
example, 369-371 
Poison prior, 368 
Clinical indications 
development, 197 
Closed forms 
small area inference, 97 
Clustered binary outcome models, 123 
Common logit, 236 
Compatible 
vs. functionally compatible, 29 
Complementary log-log link, 390 
Complete hierarchical centering 
reparameterization technique, 
48,51 
· Compositional data 
Bayesian analysis, 349-362 
parametric approach, 351-352 
posterior distributions and 
estimation, 355-359 
results, 359-361, 360t-36lt 
semiparametric approach, 354-
355 
simulation based model 
determination, 352-354 
Conditional autoregressive (CAR) 
model 
Besag, 28 
sample paths, 32f 
Conditional distribution 
ability parameters 
item response modeling, 184 
item parameters 
item response modeling, 185 
latent variables 
item response modeling, 184 
Conditional distribution sampling 
Chib and Carlin algorithm, 122, 129 

410 Index 
Conditional independence structures 
Bayesian graphical models, 387-
389 
Conditional latency distributions, 255 
Conditional marginal density 
estimation (CDME) 
time series count data, 167 
Conditional models 
correlated binary data diagnostics, 
314-315 
posterior computations 
correlated binary data 
diagnostics, 318 
Conditional posterior distributions 
small area inference, 97, 99 
Conditional predictive ordinate 
OGLMs, 81 
Consensus panel, 196 
Contingency table 
Gibbs sampler variable selection, 
278-281 
Convergence 
WinBUGS, 401 
Convex credible regions, 359-360 
Coronary angiography 
post AMI practice guideline 
development, 198-199, 205-
208, 206f-207f 
likelihood, 209f 
Correlated binary data 
Bayesian method, 113-129 
longitudinal binary data, 119-
123 
multivariate probit model, 114-
119 
Correlated binary data diagnostics, 
313-326 
model adequacy for data, 320-324 
posterior predictive comparison, 
322-323 
simulation based model checking, 
323-324 
models, 314-316 
conditional, 314-315 
MVP, 315 
MVT, 315-316 
stratified and mixture, 314 
posterior computations, 317-320 
conditional models, 318 
MVP, 318-320 
MVT, 320 
stratified and mixture models, 317 
[Correlated binary data diagnostics] 
prior distributions, 316-317 
voter behavior data, 324-325 
Correlated ordinal data models 
Bayesian analysis, 133-155 
item response data example, 
148-155 
model comparisons, 143-146 
model determination, 142-148 
model diagnostics, 146-148 
models, 135-137 
posterior computations, 138-142 
prior distributions, 138 
Correlated random effects, 26-29 
GLMMs, 392-393 
Correlation matrix 
direct specification, 26-27 
Count data 
time series 
Bayesian methods, 159-171 
Covariance structure 
modeladequacy, 15 
Covariate effects 
multivariate probit (MVP), 118t 
Covariate measurement error 
WinBUGS, 397-400 
Cox's partial likelihood 
proportional hazards regression 
models, 287 
Cox variable selection, 287-309 
computational implementation, 
299-305 
data marginal distribution, 299-
301 
posterior distribution sampling, 
302-305 
method, 289-299 
baseline hazard rate prior 
distribution, 289-292 
likelihood function, 292-293 
model and notation, 289 
model space prior distribution, 
297-299 
regression coefficient prior 
distribution, 293-297 
simulation study, 305-308 
Cross-validation approach 
OGLMs, 80 
Cumulative baseline hazard rate 
model, 289-290 
Curves and surfaces models, 220-221 

Data adaptive Bayesian analysis 
binary regression model, 244-248 
Data adaptive robust link functions 
binary regression, 243-251 
Data marginal distribution 
Cox variable selection, 299-301 
Death step 
classification trees, 368 
Dependence structures 
correlated binary data, 116 
Dependent variable logit 
small area inference, 92 
Deprivation score 
histogram, 336f 
spatial variation, 335f 
Deterministic component, 389 
Deterministic error model 
systemic part h modeling, 220 
Deviance information criteria (DIC), 
16,402 
Diabetes 
predicted probability, 227f 
Difficulty parameters 
item response modeling, 176 
posterior scatterplot, 188f 
Directed acyclic graph (DAG) 
GLM, 387, 388f 
extensions, 393f 
Dirichlet Process (DP), 10, 11 
model determination, 83-84 
OGLMs, 81 
Dirichlet Process mixed generalized 
linear models (DPMGLMs), 14 
OGLMs, 81-84 
Dirichlet Process mixed overdispersed 
generalized linear models 
(DPMOGLMs) 
OGLMs, 81-84 
Dirichlet process prior, 219 
binary response regression, 234-
236 
compositional data, 349-350 
errors-in-variables, 341 
Discrimination parameters 
item response modeling, 176, 180 
Disease mapping, 90 
small area inference, 94 
Dispersion parameter 
OGLMs, 77 
Distribution, 27 4 
system errors, 60 
Distributional assumptions, GLMs, 4 
Index 411 
Disturbances 
set of, 58 
DoodleBUGS 
GLM, 389-390 
Double-exponential families, 12 
OGLMs, 76 
Drop-outs 
longitudinal binary data, 127 
Dynamic generalized linear models 
(DGLMs), 57-70 
applications, 65-70 
definition, 59-60 
Dynamic linear models (DLM), 58-59 
Efron's model, 13 
EM algorithm, 256 
mixture-model approach example, 
262-263 
Empirical Bayes (EB) 
approaches, 90 
logistic regression estimate, 91-
93 
small area inference, 94 
Equicorrelated multivariate probit 
(MVP) 
prior-posterior summary, 126t 
Ergodic means 
Gibbs sampler for a, 249, 249f 
Errors-in-variables modeling, 331-
344 
Bayesian approaches, 339-341 
framework, 339-340 
implementation, 340 
previous work, 340-341 
case-control study with depriva-
tion, 333-337, 341-342, 343f-
344f 
classic approaches, 337-339 
extensions and procedures, 339 
formulation, 337-339 
Escobar's Gibbs sampling algorithm, 
235 
Estimates of appropriateness 
model-based, 211 
standard categorical, 211 
Evolution equation, 59 
Exchangeable random effects 
GLMMs, 392 
Expected predictive deviance, 402 
Exponential dispersion models 
(EDM), 12-14 
OGLMs, 77 

412 Index 
Exponential family, 12-14 
Exponential power distribution 
binary regression model 
data adaptive Bayesian analysis, 
246-248 
Exponential power model, 250t 
Extended Gamma processes, 10 
prior, 290 
posterior samples, 302 
Finite mixture model 
binary response regression, 233-
234 
University of Arkansas student 
retention, 238 
us. general mixture model, 237-
238 
First order difference model, 30 
Fisher information matrix, 5 
fitting OGLMs, 78-79 
Fisher scoring algorithm 
DGLMs, 61 
Fitted values 
male flour beetle data set, 268t 
Flour beetle data set 
male, 262t, 267t, 268t 
Formal Bayesian model 
determination, 80 
Founder nodes 
prior distributions, 390 
Functionally compatible 
us. compatible, 29 
Gamma distribution, 390 
Gamma processes, 10 
Gamma process priors 
cumulative baseline hazard, 289-
290 
Gamma residual effects, 25 
Gaussian distributions 
gamma and inverse, 5 
inverse 
correlated ordinal data models, 
142 
Gaussian models 
us. Markov model 
point-referenced binary spatial 
data, 374 
state-space 
DGLMs, 61 
Gelfand's hierarchical centering 
reparameterization technique 
time series count data, 166-167 
General F implementation 
Gibbs sampler 
item response modeling, 183 
Generalized estimating equation 
(GEE) 
correlated ordinal data models, 133 
Generalized linear mixed models 
(GLMMs), 391-394 
correlated random effects, 392-393 
definition, 24-26 
exchangeable random effects, 392 
hierarchical, 31-36 
models, 43-44 
normal linear random effects, 43-
48 
prior distributions, 44-46 
prior elicitation and variable 
selection, 41-52 
random effects, 23-37 
WinBUGS, 392-393 
covariate measurement error, 
397-400 
informative missing data, 396-
397 
missing data, 396 
prediction, 397 
Generalized linear models (GLMs), 4-
5 
Bayesian 
small area inference, 89-105 
Bayesian analysis, 273-284 
Bayesian view, 3-17 
BMARS, 221-228 
categorical data, 365 
components, 27 4, 389 
DAG, 387, 388f 
DAG extensions, 393f 
DoodleBUGS, 389-390 
dynamic, 57-70 
non-canonical links, 391 
overdispersion, 12-14, 73-85, 391-
392 
parametric Bayesian, 10 
semiparametric, 10-12 
variable selection, 288 
WinBUGS, 389-391 
cavort measurement error, 397-
400 
informative missing data, 396-

[Generalized linear models (GLMs)] 
397 
missing data, 396 
prediction, 397 
Generalized Liouville distributions 
(GLD) model, 350 
compositional data, 354-361 
General mixture model 
University of Arkansas student 
retention, 238 
vs. finite mixture model, 237-238 
General mixtures 
binary response regression, 234-
236 
General regression 
link function g modeling, 219 
Gibbs sampler, 7, 36-37, 219 
fora 
ergodic means, 249, 249f 
posterior distribution, 249, 249f 
analysis of survival data, 259-260 
correlated ordinal data models, 
138-142 
cumulative baseline hazard, 289-
290 
DGLMs, 62,63-64 
fitting OGLMs, 79 
General F implementation 
item response modeling, 183 
GLMMs, 48 
method, Monte Carlo posterior 
estimates, 232-235 
mixture-model approach example, 
263-265 
post AMI practice guideline 
development, 199, 203, 204 
probit link data augmentation 
item response modeling, 184-
185 
small area inference, 97 
two-parameter exponential family 
model 
item response modeling, 181-182 
variable selection, 273-284 
Carlin and Chib's method, 275 
contingency table, 278-281 
example, 278-281 
logistic regression model 
example, 280-281, 281t 
log-linear model example, 280, 
280t 
Stochastic Search Variable 
Index 413 
[Gibbs sampler] 
Selection (SSVS), 276 
unconditional priors, 277 
Gibbs variable selection 
Gibbs sampler variable selection 
strategies, 277-278 
graphical model representation, 
279f 
posterior model probabilities, 280t, 
281t 
Gilks-Wild algorithm 
small area inference, 97 
Graphical model representation 
variable selection, 279f 
Grayscale plot 
residential properties dataset, 384f 
Health service areas (HSAs) 
small area inference, 100 
Hessian matrix 
correlated ordinal data models, 
140 
Hierarchical Bayes (HB), 4 
approaches, 90 
. logistic regression estimate, 91-
93 
model, 6 
point estimator 
small area inference, 93 
small area inference, 94 
Hierarchical, centering, reparameteri-
zation technique Gelfand, 
time series count data, 166-167 
Hierarchical generalized linear mixed 
models (GLMMs), 31-36 
Bayesian inferences, 36-37 
Hierarchical generalized linear 
models (GLMs) 
overdispersion, 84-85 
Hierarchical Poisson regression 
model, small area inference, 
94 
Highest posterior density (HPD) 
correlated ordinal data models, 
150-155 
Histogram 
posterior predictive distributions, 
192f 
Historical data 
time series count data, 159 
Hybrid sampler, 223 

414 Index 
Flyperparameters, 59 
small area inference, 96-97 
specifications, 47-48 
time series count data, 164-165 
Identity link, 390 
Importance sampling density (ISD) 
point-referenced binary spatial 
data,378-379 
Importance weighted marginal 
posterior density estimation 
(IWMDE) 
Chen method, 50 
Improper priors 
theorems 
item response modeling, 178-179 
Incidence probabilities, 255 
posterior densities plot, 266, 266f 
Independent random effects, 26 
Independent variable 
small area inference, 92 
Indicator kriging 
point-referenced binary spatial 
data, 374 
Indicator variograms 
point-referenced binary spatial 
data, 374 
Inference 
correlated binary data, 120 
finite population proportion 
small area, 92 
item response modeling, 185-186 
point-referenced binary spatial 
data,373-385 
small area 
Bayesian generalized linear 
models, 89-105 
Informative prior elicitation 
Bayesian analysis, 44-46 
item response modeling, 180 
Inverse Gaussian distribution 
correlated ordinal data models, 
142 
Inverse link function, 231 
lsConstant method 
graphical model, 403 
Item parameters 
conditional distribution 
item response modeling, 185 
Item response curve, 175-176 
Item response modeling, 173-193 
Bayesian fitting, 181-185 
checking, 186-188 
exam administration, 176-178 
example, 188-191 
inferences, 185-186 
item response curve, 175-176 
prior distributions, 178-180 
Jefferys' prior, 8, 13 
fitting OGLMs, 78-79 
Joint posterior distribution, 48 
small area inference, 98 
Joint prior 
time series count data, 163 
Joint probability mass function 
correlated binary data, 120 
Kalman Filter, 59, 60 
DGLMs, 62 
Kepler node, 404 
Kernel smoothing 
correlated binary data, 125-126 
Kriging 
indicator 
point-referenced binary spatial 
data, 374 
Kuo and Mallick sampler 
graphical model representation, 
279f 
Kuo and Mallick's Unconditional 
Priors 
posterior model probabilities, 280t, 
281t 
Kyphosis dataset, 224 
boxplot, 226f 
Laplace's method, 8 
small area inference, 94, 97 
Laplace's prior, 8 
Latent Bayesian residual, 248, 249-
250 
Latent detrended variogram plot 
residential properties dataset, 383f 
Latent variables 
conditional distribution 
item response modeling, 184 
Likelihood analysis, Bayesian 
analysis, 6 
Bayesian models, 5 

[Likelihood analysis, Bayesian 
analysis] 
contribution 
computation, 124-125 
equations, 5 
functions, 5, 6 
Cox variable selection, 292-293 
item response modeling, 178 
time series count data, 160-162 
mixture-model approach 
analysis of survival data, 257 
ordinate 
correlated binary data, 124-125 
Lindley's paradox, 223 
Linear approximation 
piecewise, 61 
Linear Bayes approach 
DGLMs, 60-61 
Linear predictor, 27 4, 389 
Link, 218, 274 
Link function, 6, 10, 25, 60, 243, 389 
g modeling, 218-219 
binary response regression, 218-
219 
general regression, 219 
mixture models, 219 
inverse, 231 
Link function h, 239 
Log-concave 
posterior, 5 
Log deviance loss, 16 
Logical nodes, 403-404 
Logistic distribution, 25 
Logistic models 
2 binary explanatory factors 
BUGS codes, 283-284 
item response modeling, 175-
176 
Logistic regression estimate 
us. classical logistic regression 
model, 224-225, 225f 
small area inference, 91-93, 98 
example, 91 
Logistic regression models, 225-226, 
280-281, 281t 
Logit link, 25, 390 
functions, 243 
LogitLogQ method 
graphical model, 403 
Logit normal model. (see Probit 
normal model) 
correlated binary data, 119 
Index 415 
Logit regression model 
Bayesian analysis, 243 
Log-linear models, 280, 280t 
for 23 contingency table 
BUGS codes, 282-283 
Log link, 390 
Log marginal likelihood, 126t 
Log scoring loss, 16 
Log Value method 
graphical model, 403 
Longitudinal binary data 
correlated binary data, 119-123 
drop-outs, 127 
Longitudinal binary responses 
scale mixture of multivariate 
normal (SMMVN) link 
functions, 133 
Low birth weight in infants dataset, 
248-250 
Male flour beetle data set, 262t 
absolute residuals, 268t 
fitted values, 268t 
model comparisons, 267t 
Map method 
graphical model, 403 
Marginal likelihood 
Chib's method, 127 
correlated binary data, 124 
multivariate models, 155 
time series count data, 161 
Marginal posterior distribution 
Bayesian graphical models, 388 
time series count data, 166-167 
Markov Chain Monte Carlo (MCMC), 
5,8,27,28,31,36-37,50 
based approaches 
complex Bayesian inference 
problems, 62-65 
correlated binary data, 124 
implementation 
Bayesian procedure, 12 
methods, 218,256,287-288,313 
Bayesian graphical models, 388-
389 
classification trees, 368 
compositional data, 350-351 
errors-in-variables models, 340 
point-referenced binary spatial 
data, 378 
WinBUGS, 390-391 

416 Index 
[Markov Chain Monte Carlo (MCMC)] 
posterior distribution sample 
algorithm, 117, 127-128 
small area inference, 94 
BUGS software, 96 
technique 
DGLMs, 64 
time series count data, 166-167 
Markov random field models, 29-30 
Markov transition model, 123 
Matrix inversion 
point-referenced binary spatial 
data, 375 
Maximized log-likelihood, 126t 
Maximum likelihood estimators 
(MLE), 5 
Maximum likelihood logistic 
regression analysis 
University of Arkansas student 
retention, 238 
Mean survival time 
posterior densities plot, 266-267, 
267f 
Measurement error problem, 331 
Medical care 
quality assessment, 196-197 
Meningococcic meningitis 
DGLMs application, 66-68 
Metropolis algorithm 
correlated ordinal data models, l42 
Metropolis-Hastings algorithm, 5, 7, 
36-37 
correlated ordinal data models, 138, 
140--141 
DGLMs, 64-65 
fitting OGLMs, 79 
small area inference, 97, 99-100 
Metropolis-Hastings step, 275 
Metropolis sampling scheme 
correlated ordinal data models, 141 
Missing at random (MAR), 396-397 
Missing completely at random 
(MCAR), 396-397 
Mixture models, 10 
analysis of survival data, 255-268 
likelihood,257 
correlated binary data diagnostics, 
314 
link function g modeling, 219 
posterior computations 
correlated binary data 
diagnostics, 317 
Model adequacy, 14-15 
OGLMs, 81 
Model and notation 
Cox variable selection, 289 
Model-based categorization 
AMI guideline compliance, 208f 
Modelchecking,232,236 
WinBUGS, 402 
Model choice 
Bayesian approach, 15-16 
Model comparisons 
male flour beetle data set, 267t 
Model determination 
analysis of survival data, 260--261 
approaches, 14-17 
compositional data, 352-354 
Model diagnostic 
binary response regression, 236-
237 
computational methods, 237 
diagnostic tools, 237 
goal, 236 
Modeled heterogeneity 
parametrized weighted distribution 
OGLMs, 78 
Model elaboration, 14 
Model expansion, 14 
Modeling 
point-referenced binary spatial 
data,373-385 
Model intra-cluster correlation 
correlated binary data, 113 
Model misspecification, 8 
Model probability computations 
time series count data 
Bayesian methods, 165-167 
Model space prior distribution 
Cox variable selection, 297-299 
Model uncertainty, 27 4 
MonitoredValue method 
graphical model, 403 
Monotone functions, 10 
Monte Carlo EM algorithm (MCEM) 
analysis of survival data, 256-
259 
Monte Carlo error, 401 
Monte Carlo method 
noniterative 
point-referenced binary spatial 
data, 378 
posterior model probabilities, 48-50 
time series count data, 167 

Monte Carlo posterior estimates 
Chen and Shao, 301 
Gibbs sampling method, 232-235 
Mortality data 
u.s. 
models, 100-102 
Motivating example 
Bayesian MARS (BMARS) GLMs, 
224-225 
Multistage generalized linear models 
(GLMs) 
overdispersion, 84-85 
Multivariate adaptive regression 
spline (MARS) methodology, 
221 
Multivariate Cauchy (MVC) 
correlated ordinal data models 
item response data example, 
148-155 
multivariate t-link (MVT), 136-137 
Multivariate DGLM, 61 
Multivariate discrete mass function 
correlated binary data, 115 
Multivariate exponential power 
distribution family links 
(MVEP), 136 
Bayesian modeling, 137 
scale mixture of multivariate 
normal (SMMVN) link 
functions, 13 7 
Multivariate logit (MVL), 136, 137 
correlated ordinal data models 
item response data example, 
148-155 
Multivariate nature, 104 
Multivariate normal distribution, 27 
Multivariate probit model 
correlated binary data, 114-119 
Multivariate probit (MVP), 136, 313 
correlated binary data, 117 
correlated ordinal data models, 133, 
141 
item response data example, 
148-155 
covariate effects, 118t 
models 
correlated binary data 
diagnostics, 315 
MVT, 118-119 
posterior boxplots, 119f 
posterior computations 
correlated binary data 
Index 417 
[Multivariate probit (MVP)] 
diagnostics, 318-320 
prior-posterior summary, 126t 
Toeplitz correlation structure, 126t 
Multivariate regression, 58 
Multiv~riate stable distribution 
families (MVS), 136, 137 
correlated ordinal data models, 
141-142, 142 
Multivariate t-link (MVT), 136 
correlated binary data diagnostics, 
315-316 
posterior computations, 320 
correlated ordinal data models, 141 
fitting, 118-119 
models, 313 
multivariate Cauchy (MVC), 136-
137 
National Center for Health Statistics 
(NCHS) 
HSAs, 100 
Nelder-Mead algorithm 
correlated ordinal data models, 140 
Non-canonicallinks 
GLM, 391 
Non-Gaussian state-space models 
DGLMs, 61 
Noninformative priors 
item response modeling, 178-179 
Noniterative Monte Carlo method 
point-referenced binary spatial 
data, 378 
Nonlinear state-space models 
DGLMs, 61 
Nonparametric approach 
binary regression, 219 
Nonparametric Bayesian framework 
OGLMs, 81-84 
Non-Wishart prior assumptions, 117 
Normal distribution, 390 
binary regression model, data adaptive 
Bayesian analysis, 245 
Normalizing constants 
posterior distributions, 300 
Normal model, 250t 
Normal residual effects, 25 
Normal scale mixture links 
binary response regression, 231-
240 
Nuisance parameters, 47-48 

418 Index 
Nutritional epidemiology 
errors-in variables, 332-333 
Observation equation, 58, 59, 60 
Observed nodes, 390 
One-parameter exponential family 
model, 8, 12, 31 
limitations, 13-14 
OGLMs, 76 
One-parameter model 
Bayesian fitting, item response 
modeling, 185 
Optimize method 
graphical model, 403 
Ordered categories 
WinBUGS, 395-396 
Ord model, 27 
Outlier detection, 14 
Outliers 
binary regression 
data adaptive robust link 
functions, 248 
Overdispersed generalized linear 
models (OGLMs), 12-14, 73-
85,391-392 
classes, 75-78 
hierarchical or multistage, 84-85 
nonparametric Bayesian 
framework, 81-84 
example, 83 
fitting, 81-82 
model determination, 83-84 
parametric Bayesian framework 
example, 79-80 
fitting, 78-81 
model determination, 80-81 
parametrized weighted 
distribution, 78 
Overshrinkage, 103 
Parameter exponential family 
one us. two, 12-14 
Parameter exponential family 
defining, 12 
Parametric approach, 10 
Parametric Bayesian framework 
fitting OGLMs, 78-81 
Parametric Bayesian GLMs, 10 
Parametric family of densities, 
11 
Parametric OGLMs 
model determination, 80-81 
Parametrized weighted distribution 
modeled heterogeneity 
OGLMs, 78 
Parents· method 
graphical model, 403 
Pediatric pain data, 51-52 
Penalized quasi-likelihood estimates 
(PQL), 6 
Piecewise linear approximation 
DGLMs, 61 
small area inference, 92 
Pima Indian dataset 
website, 225 
Pima Indian example 
Bayesian MARS (BMARS) 
generalized linear models, 225-
228 
Plates, 388 
Point-biserial correlation 
observed proportions correct, 17 4f 
Point estimator 
HSAs, 102 
Point-referenced binary spatial data 
computational issues, 378-380 
Gaussian us. Markov models, 374 
illustration, 380-384, 380f-381f, 
382t, 383f-384f 
indicator kriging, 37 4 
indicator variograms, 374 
ISD, 378-379 
Markov chain Monte Carlo 
(MCMC) methods, 378 
matrix inversion, 375 
modeling and inference, 373-385 
modeling details, 375-378 
noniterative Monte Carlo method, 
378 
residential properties dataset, 380-
384, 380f-381f, 382t, 383f-384f 
spatial dependence modeling, 374 
variogram modeling, 37 4 
Poisson distributions, 5, 25, 390 
Poisson gamma models, 26 
Poisson GLMMs, 51 
Poisson model 
time series count data, 168 
Poisson prior, 222-223 
classification trees, 368 
Poisson regression 
small area inference, 98 

Poisson regression interactive 
multilevel modeling (PRIMM) 
small area inference, 94, 96 
Poisson regression models 
small area inference, 94-96 
Poisson sampling process 
small area inference, 94 
Pollen data 
time series count data 
example, 167-171 
Pollutant level 
DGLMs application, 68-70 
Polya tree distributions, 10 
Polytomous responses 
WinBUGS, 394-395 
Population denominators 
errors-in-variables, 341 
Post acute myocardial infarction 
practice guidelines, 195-
211 
application, 200-209 
estimation, 203 
hospital profiling, 203-204, 206-
208, 207f-208f 
modeling adherence, 202-203, 
206 
patient population, 205-206, 
205f-206f 
quality of care variability, 204, 
208-209,209f-210f 
study population, 200-202 
development, 197-208 
appropriateness ratings 
elicitation, 197-198, 198f 
combining angiography data, 
198-199 
defining standard of care, 199-
200 
estimation, 199 
results, 200, 20 If 
Posterior 
log-concave, 5 
properties, 8-10 
Posterior boxplots 
MVP model, 119f 
Probit hierarchical model, 123f 
probit normal model, 121f 
Posterior computations 
correlated binary data diagnostics, 
316-320 
correlated ordinal data models, 
138-142 
Index 419 
Posterior densities plot 
incidence probability, 266, 266f 
mean survival time, 266-267, 267f 
Posterior dislikelihood function, 8-9 
Posterior distribution, 8-9 
compositional data, 355-359 
computations, 48 
DGLMs, 60 
Gibbs sampler for a, 249, 249f 
normalizing constants, 300 
posterior medians, 189f 
properties 
item response modeling, 178-179 
sample 
Cox variable selection, 302-305 
Markov Chain Monte Carlo 
(MCMC) algorithm, 117, 127-
128 
small area inference, 96-97 
Posterior expected predicted deviance 
(EPD), 102 
Posterior inference 
DGLMs application 
meningococcic meningitis, 67f · 
pollutant levels and respiratory 
diseases, 69f 
Posterior mean 
residential properties dataset, 382t 
Posterior medians 
posterior distributions, 189f 
Posterior mode estimation 
DGLMs, 61-62 
Posterior model probabilities, 51t-
52t, 306t, 307t 
Monte Carlo approach, 48-50 
Posterior ordinate 
correlated binary data, 125-126 
Posterior prediction 
checks 
item response modeling, 187-188 
comparison 
correlated binary data 
diagnostics, 322-323 
distributions 
histogram, 192f 
model choice, 16 
probabilities 
voter behavior data, 325t 
p-values 
HSAs, 102 
strategy 
model adequacy, 15 

420 Index 
Posterior-prior comparison, 14 
Posterior probability angiography, 
206f 
Posterior samples 
Extended Gamma (EG) process 
prior, 302 
Posterior sample size 
WinBUGS, 401 
Posterior scatterplot 
difficulty parameters, 188f 
Potential Scale Reduction (PSR) 
statistic, 203 
Predictive approach 
model selection, 260-261 
variable selection 
GLM, 288 
Predictive distribution, 236 
Prior 
distributions 
correlated binary data 
diagnostics, 316-317 
founder nodes, 390 
GLMMs, 44-46 
item response modeling, 178 
properties, 46-4 7 
scale mixture of multivariate 
normal (SMMVN) link 
functions, 138 
theorems, 46-4 7 
time series count data, 162-163 
elicitation 
generalized linear mixed models 
(GLMMs), 41-52 
GLMMs study, 51-52 
hyperparameters,47-48 
correlated binary data, 125-126 
link functions 
exponential power distribution, 
246f, 247 
model probabilities, 306t, 307t 
parameters, 291-292 
sensitivity analysis, 308 
posterior summary 
multivariate probit (MVP), 126t 
specification 
time series count data, 163 
WinBUGS, 400-401 
theorems 
item response modeling, 178-
179 
Priors 
unconditional, 277 
Probabilistic-specifications 
Bayesian nonparametric modeling, 
10 
Probit hierarchical model 
posterior boxplots, 123f 
Probit link, 390 
data augmentation 
item response modeling, 184-185 
functions, 243 
item response modeling example, 
188-191 
Probit models, 236 (see also Logit 
normal model) 
correlated binary data, 119, 126 
computations, 120-121 
posterior boxplots, 121f 
sampling algorithm, 121, 128 
Probit regression model, 245-246 
PROC MIXED, 3 
Proportional hazards model, 289 
Proportional hazards regression 
models 
Cox's partial likelihood, 287, 309 
Pseudopriors, 275 
Quality assessment 
medical care, 196-197 
Quasi-likelihood method, 8 
small area inference, 97 
Radioimmunoassay 
errors-in-variables 
dilution errors, 340 
Random component, 217-218, 389 
Random effects, 26-31 
autocorrelated, 30 
GLMMs, 23-37 
strongly correlated, 29-30 
systemic part h modeling, 220 
Randomness, 103-104 
Random walk prior, 30 
Rao-Blackwellized estimate, 248, 250, 
251f 
Ratio-of-Uniform algorithm 
correlated ordinal data models, 
141-142 
Reference prior distributions 
small area inference, 92 
Regression analysis, 3 
Regression calibration, 339 

Regression coefficients, 4 
prior distribution 
Cox variable selection, 293-297 
standard errors, 308t 
Regression parameter estimates, 211 
Regression techniques, 217 
Rejection sampling histogram, 252 
Reparameterization 
scale mixture of multivariate 
normal (SMMVN) link 
functions, 135-136 
Repeated measure data, 51-52 
Residential properties dataset 
grayscale plot, 384f 
latent detrended variogram plot, 
383f 
local coordinates, 380, 381f 
point-referenced binary spatial 
data, 380-384, 380f-381f, 382t, 
383f-384f 
posterior mean, 382t 
standard deviation, 382t 
Residual effects 
distribution, 25-26 
Respiratory diseases 
DGLMs application, 68-70 
Restricted maximum likelihood 
(REML) 
small area inference, 95 
Saddle point approximation 
OGLMs, 76 
Sample survey designs, 89 
Sampling/Importance Resampling 
(SIR) method 
correlated ordinal data models, 
141-142 
SAS, 3 
Scale mixture of multivariate normal 
(SMMVN) link functions 
correlated ordinal data models 
item response data example, 
148-155 
longitudinal binary responses, 133 
multivariate exponential power 
distribution family links 
(MVEP), 137 
reparameterization, 135-136 
Score vector, 5 
Semi hierarchical centering 
reparameteri.zation technique, 48, 51 
Index 421 
Semiparametric generalized linear 
models, 10-12, 217-228 
BMARS, 222-223 
classical MARS, 221-222 
curves and surfaces models, 220-221 
link furiction g modeling, 218-219 
systemic part h modeling, 219-220 
Sensitivity analysis 
prior parameters, 308 
Set method 
graphical model, 403 
Simulation based model checking 
correlated binary data diagnostics, 
323-324 
Simulation extrapolation (SIMEX), 
339 
Small area inference 
Bayesian generalized linear 
models, 89-105 
Small Area Variations in Air-quality 
and Health (SA VIAH) project, 
333-337,341-342 
Smooth function, 4 
Solomon-Wynne experiment, 391 
Spatial dependence modeling 
point-referenced binary spatial 
data, 374 
Splitting nodes, 366-369 
Splitting questions, 366-367 
S-PLUS program 
small area inference, 96 
Standard categorization 
AMI guideline compliance, 208f 
Standard deviation 
residential properties dataset, 382t 
Standard errors 
regression coefficients, 308t 
Standard of care 
defined 
post AMI guidelines, 196, 199-
200 
State space models (see Dynamic 
linear models (DLM)) 
Stochastic nodes, 403 
Stochastic Search Variable Selection 
(SSVS) 
Gibbs sampler variable selection 
strategies, 276 
graphical model representation, 
279f 
posterior model probabilities, 280t, 
281t 

422 Index 
Stratified models 
correlated binary data diagnostics, 
314 
posterior computations 
correlated binary data 
diagnostics, 317 
Strongly correlated random effects, 
29-30 
Structural assumptions 
GLMs, 4 
Student retention 
University of Arkansas, 237-239 
Student-t specification 
correlated binary data, 116 
Success parameters, 5 
Sun model, 26 
Systematic component, 218 
System errors 
distribution, 60 
Systemic part h modeling, 219-220 
deterministic error model, 220 
random effects model, 220 
Tail area comparison, 14 
Terminal nodes, 366-369 
Theorems 
Bayes', 59 
hierarchical GLMMs, 31-36 
improper priors 
item response modeling, 178-179 
prior 
distributions, 46-47 
Three-stage hierarchical 
multinominial-Dirichlet model 
small area inference, 99 
Time series count data 
Bayesian methods, 159-171 
example, 167-171 
likelihood functions, 160-162 
methods, 160-165 
model probability computations, 
165-167 
Toeplitz correlation structure 
multivariate probit (MVP), 126t 
Total Pearson Discrepancy Measures 
voter behavior data, 325f 
Tribolium castaneum data set, 262t, 
267t, 268t 
Two-parameter exponential family 
model, 12-14 
item response modeling 
[Two-parameter exponential family 
model] 
example, 188-191 
Gibbs sampler, 181-182 
Uncertain borrowing, 103 
Unclear points, 226, 227f 
Unconditional priors, 277 
Gibbs sampler variable selection 
strategies, 277 
Univariate Poisson, 60 
University of Arkansas student 
retention, 237-239 
finite mixture model (MF), 238 
general mixture model (MG), 238 
general mixture model us. finite 
mixture model, 237-238 
maximum likelihood logistic 
regression analysis, 238 
model diagnostics, 240f 
SSE, 239t 
U.S. mortality data 
models, 100-102 
Variable selection 
generalized linear mixed models 
(GLMMs), 41-52 
GLM, 288 
GLMMs study, 51-52 
Variograms 
latent detrended plot 
residential properties dataset, 
383f 
point-referenced binary spatial 
data 
indicator, 374 
modeling, 37 4 
Voter behavior data 
correlated binary data diagnostics, 
324-325 
W eibull densities, 305 
W eibull distribution, 390 
Wheeze data 
models, 341t 
WinBUGS 
classical measurement error, 398-
399 
convergence, 401 

[WinBUGS] 
covariate measurement error, 397-
400 
extending, 402-404 
GLM, 389-391 
GLMMs, 392-393 
Markov chain Monte Carlo 
(MCMC) methods, 390-391 
model checking, 402 
ordered categories, 395-396 
over-parameterized models, 399-
400 
Index 423 
[WinBUGS] 
polytomous responses, 394-395 
posterior sample size, 401 
prior specification, 400-401 
website, 403 
Wishart distribution 
correlated ordinal data models, 
138-139 
Wishart prior assumptions, 117 
Zeger-Karim formulation, 8 
Zellner's g-priors, 296 

