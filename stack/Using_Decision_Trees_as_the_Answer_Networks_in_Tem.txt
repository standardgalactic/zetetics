See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220837155
Using Decision Trees as the Answer Networks in Temporal Difference-
Networks
Conference Paper · January 2008
DOI: 10.3233/978-1-58603-891-5-847 · Source: DBLP
CITATIONS
0
READS
59
4 authors:
Some of the authors of this publication are also working on these related projects:
Game Theory View project
Cost-Sensitive Linear Regression View project
Laura Antanas
KU Leuven
19 PUBLICATIONS   120 CITATIONS   
SEE PROFILE
Kurt Driessens
Maastricht University
75 PUBLICATIONS   1,727 CITATIONS   
SEE PROFILE
Jan Ramon
KU Leuven
149 PUBLICATIONS   3,043 CITATIONS   
SEE PROFILE
Tom Croonenborghs
KU Leuven
109 PUBLICATIONS   1,021 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Tom Croonenborghs on 20 May 2014.
The user has requested enhancement of the downloaded file.

Using Decision Trees as the Answer Networks in
Temporal Difference-Networks
Laura-Andreea Antanas1, Kurt Driessens1, Jan Ramon 1 and Tom Croonenborghs 2
1
Introduction
State representation for intelligent agents is a continuous challenge
as the need for abstraction is unavoidable in large state spaces. Pre-
dictive representations offer one way to obtain state abstraction by
replacing a state with a set of predictions about future interactions
with the world. One such formalism is the Temporal-Difference Net-
works framework [2]. It splits the representation of knowledge in the
question network and the answer network.
The question network deﬁnes which questions (interactions) about
future experience are of interest. It contains nodes, each correspond-
ing to a single scalar prediction about a future observation given a
certain sequence of interactions with the environment. The nodes are
connected by links, annotated with action-labels, which represent
temporal relationships between the predictions made by the nodes,
conditioned on the action-labels on the links (more details in [2]).
The answer network provides the predictive models to update the
answers to the deﬁned questions, which are expected values of the
scalar quantities in the nodes. These values can be seen as estimates
of probabilities. With each executed action of the agent, the pre-
dictions are updated using the answer network models to obtain a
description of the new state. In classical TD-networks, logistic re-
gression models are used, whose weight vector is obtained using a
gradient learning approach.
We propose the use of probability-valued decision trees [1] in the
answer network of TD-Nets. We believe that decision trees are a par-
ticular good choice to investigate, as they offer a different yet power-
ful form of generalization. Moreover, this aids in a better understand-
ing of the strengths and weaknesses of TD-Nets and represents an im-
portant ﬁrst step towards using them in worlds with more extensive
observations. Furthermore, decision tree induction can be regarded
as a prototypical example of a non-gradient learning approach.
2
Decision Trees as Answer Networks in TD-Nets
The (abstracted) state representation in a TD-Net consists of (1) the
predictions made by the TD-Net in the previous timestep yt−1 =
[y(1)
t−1, . . . , y(n)
t−1] (one prediction for each node) with n the num-
ber of nodes in the question network, (2) the action executed dur-
ing the last time frame at−1 and (3) the current observation ot, i.e.,
xt = [yt−1, at−1, ot]. From this vector, the answer network will
compute new predictions yt = f(xt). In the original implemen-
tation of TD-Nets, this answer function is represented by a logistic
regression function, i.e., yt = f W (xt) = σ(Wxt).
1 Declarative Languages and Artiﬁcial Intelligence, Katholieke Universiteit
Leuven, Leuven, Belgium, email: {laura,kurtd,janr}@cs.kuleuven.be
2 Biosciences and Technology Department, KH Kempen University College,
Geel, Belgium, email: tom.croonenborghs@khk.be
We investigate the use of probability trees [1] as an alternative
to logistic regression for the answer network. The modiﬁcation of
the original TD-Nets framework consists solely in the introduction
of a probability tree function f T as the answer network, instead
of the logistic regression function f W . Both the semantics of TD-
Nets as well as the temporal improvement learning principle, re-
mains unchanged. The difference between the two approaches is that
we choose to represent the predictive model with one tree for each
node. In the original TD-nets implementation it is common to learn
a set of weight vectors (aggregated in matrices), but one matrix for
each <action,observation> combination is also practicable. In our
approach, the action and observation are inputs for the probability
trees which allows for more generalisation.
Generating the learning examples
We build learning examples
for the probability tree very similarly to the TD(1) learning scheme
described in [3]. On a very high level, TD(1) generates target values
for predictions looking as far into the future as possible. Figure 1
shows the timeline and dependancies between values of interest for
the generation of learning examples. After the execution of an action,
new predictions yt+1 for the nodes are made using the current tree
function f T
t and based on the previous predictions yt, the performed
action at and the resulting observation ot+1. Based on a Monte-Carlo
approach, TD(1) uses these predictions and the made observations to
derive target values for the old predictions of the question network,
according to the structure of the network and the chosen actions.
Figure 1.
Dependancies between the different values used for the
generation of learning examples. The structure of a learning example is
shown by the shaded box, where zt is the target.
For example, if the actions taken at time t and t+1 are at and at+1
respectively, our version of TD(1) will generate the learning exam-
ples as a result of these interactions. If a node (n′) in the network
is conditioned by actions at and at+1, following this order in time,
TD(1) will use the observation ot+2 as the target for the input vector
[yt−1, at−1, ot]. For another node (n′′), conditioned only by action
at, it will use [yt−1, at−1, ot] as the input vector and ot+1 as tar-
get. In practice, we implement this approach by using a history about
chosen actions and observation with a length equal to the maximum
depth of the question network.
We choose to use a TD(1) approach because it generates the most
informative learning examples for the probability tree. At the start,
the predictions y will be mostly noise. This means that both the input

vector as well as the target, when not based directly on an observa-
tion, will contain noise. TD(1) will avoid the second source of noise
when possible. Experiments reported in [3] show that TD(1) gives
the best learning performance for a logistic regression approach too.
Incremental Tree Learning
As stated before, we learn a sin-
gle probability tree for each node in the question network. We will
employ binary decision trees, where internal nodes in the tree test
attribute-value combinations. The available decision tests are identi-
ﬁed before the induction of the probability trees begins, using a lan-
guage bias deﬁned by the user of the system. It speciﬁes the possible
actions, ranges for prediction values and observations for the world
that can be considered when building the tree. Since predictions from
the question network nodes are needed while still learning the answer
network, we need an incremental tree learning algorithm. The incre-
mental tree induction algorithm we used is described in Algorithm 1.
Algorithm 1 Incremental Tree Induction
1: initialize by creating a tree with a single (empty) leaf
2: for each learning example do
3:
sort the example down the tree until it reaches a leaf
4:
update the statistics in the leaf and store the example
5:
if number of examples in leaf > window size then
6:
remove oldest example in the leaf and update the statistics
7:
end if
8:
if a split is needed and # examples in leaf > min ex size then
9:
generate an internal node using the indicated test
10:
grow 2 new empty leafs
11:
end if
12: end for
Each leaf in the tree stores statistical information about the ex-
amples it contains. This allows the algorithm to compute standard
deviations of the target-value for all subsets created by all the avail-
able tests. The splitting criterium checks if the examples in one leaf
are sufﬁciently coherent with respect to their target value. A leaf is
split when it contains enough examples for the statistics to be signif-
icant reliable: min ex size = 30. This algorithm is greedy and has no
mechanism to undo early mistakes. Since in the TD-Network learn-
ing setting, both the inputs and the targets of early learning examples
can be noisy, we employ a sliding window approach to forget early
learning examples. In the experiments, we use a window size = 50.
3
Empirical Evaluation
We compare the original logistic regression approach with the prob-
ability trees approach. To this extend, we implemented our own ver-
sion of the original TD(λ) learning algorithm as described in [3]. We
perfomed experiments in two different environments: a ring world
and a simple grid world. Experimental results are presented only for
the 5-state deterministic ring world, as also used in [3]. Our ring
world contains 5 interconnected circles. A circle indicates a state in
the world. The agent has two different actions A={N,P}. N moves
the agent to the adjacent state in clockwise rotation. P moves the
agent in the counter-clockwise direction. The agent can only observe
whether it is in state 1 or not, i.e. the observation bit is on (1) if the
agent is in state 1 and off (0) otherwise.
As in [3], we used symmetric action-conditional networks of depth
1, 2 and 3 as question networks. For the experiments with the classi-
cal TD-networks we used a learning rate α = 0.5, obtaining similar
results to the ones presented in [3].
To compare the different learning algorithms, we used the root
mean-squared error (RMSE) to determine the quality of the learned
models. This error at time t is calculated by comparing for each
node i the correct target z∗
i with the one predicted by the learned
model yi
t. The correct targets are computed using full knowledge of
the environment. Hence, if the RMSE converges to 0, a correct an-
swer network has been learned. The experiments are performed in an
episode-based fashion. All experiments present the average RMSE
as a function of the number of episodes over 10 different runs.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0
 200
 400
 600
 800
 1000
 1200
 1400
RMSE
Number of Learning Episodes
5-state ring world
FW D1
FW D2
FW D3
FT D1
FT D2
FT D3
Figure 2.
RMSE-curves for the symmetric action-conditional networks.
Figure 2 shows the results for the ring world with question net-
works of depth 1, 2 and 3. f T always converges faster than f W for
networks of equal depth. For networks of depth 1 it is not possible to
provide a completely accurate answer network, as the question net-
work is too small to represent the full environment, but the probabil-
ity tree learner quickly learns the best approximation. As expected,
networks with larger depth perform better. The results for the grid
world also show a faster learning performance for the decision trees.
4
Conclusion
We introduced the use of probability trees as answer networks in
TD Networks. We illustrated how to translate the standard TD(1)
learning approach into a training example generator and evaluated
the performance of a simple incremental and greedy tree induction
algorithm. The experimental evaluation shows consistently that the
learning performance of the probability trees outperforms the origi-
nal logistic regression approach. We consider this an important step
towards the wider applicability of TD Networkss.
As we only regard this work as a proof of concept, a wide range
of future work is possible. The current implementation of the tree
induction algorithm could be signiﬁcantly improved, for example by
including tree restructuring operators or extending the learning al-
gorithm to learn model-trees to combine the advantages of regres-
sion trees and logistic regression. Also, other non-parametric learn-
ers could be substituted for the probability tree learner. One exciting
direction is the use of more elaborate observations, such as those for
relational worlds. In this context decision trees offer the advantage of
the more ﬂexible parameterisation.
REFERENCES
[1]
D. Fierens, J. Ramon, H. Blockeel, and M. Bruynooghe, ‘A comparison
of approaches for learning probability trees’, in Proceedings of the 16th
European Conference on Machine Learning (ECML-05), pp. 556–563,
(2005).
[2]
R.S. Sutton and B. Tanner, ‘Temporal-difference networks’, in Advances
in Neural Information Processing Systems 17, pp. 1377–1384, (2004).
[3]
B. Tanner and R.S. Sutton, ‘Td(λ) networks: temporal-difference net-
works with eligibility traces’, in Proceedings of the 22nd international
conference on Machine learning (ICML-05), pp. 888–895, (2005).

