1
Vol.:(0123456789)
Scientific Reports |        (2020) 10:12356  
| https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports
Experimental kernel‚Äëbased 
quantum machine learning in¬†finite 
feature space
Karol¬†Bartkiewicz1,2*, Clemens¬†Gneiting3, Anton√≠n¬†ƒåernoch2*, Kate≈ôina¬†Jir√°kov√°2, 
Karel¬†Lemr2*¬†& Franco¬†Nori3,4
We implement an all-optical setup demonstrating kernel-based quantum machine learning for two-
dimensional classification problems. In this hybrid approach, kernel evaluations are outsourced to 
projective measurements on suitably designed quantum states encoding the training data, while the 
model training is processed on a classical computer. Our two-photon proposal encodes data points in 
a discrete, eight-dimensional feature Hilbert space. In order to maximize the application range of the 
deployable kernels, we optimize feature maps towards the resulting kernels‚Äô ability to separate points, 
i.e., their ‚Äúresolution,‚Äù under the constraint of finite, fixed Hilbert space dimension. Implementing 
these kernels, our setup delivers viable decision boundaries for standard nonlinear supervised 
classification tasks in feature space. We demonstrate such kernel-based quantum machine learning 
using specialized multiphoton quantum optical circuits. The deployed kernel exhibits exponentially 
better scaling in the required number of qubits than a direct generalization of kernels described in the 
literature.
Many contemporary computational problems (like drug design, traffic control, logistics, automatic driving, stock 
market analysis, automatic medical examination, material engineering, and others) routinely require optimiza-
tion over huge amounts of ¬≠data1. While these highly demanding problems can often be approached by suitable 
machine learning (ML) algorithms, in many relevant cases the underlying calculations would last prohibitively 
long. Quantum ML (QML) comes with the promise to run these computations more efficiently (in some cases 
exponentially faster) by complementing ML algorithms with quantum resources. The resulting speed-up can 
then be associated with the collective processing of quantum information mediated by quantum entanglement.
There are various approaches to QML, including linear algebra solvers, sampling, quantum optimization, 
or the use of quantum circuits as trainable models for inference (see, e.g., Refs.2‚Äì18). A strong focus in QML 
has been on deep learning and neural networks. Independently, kernel-based approaches to supervised QML, 
where computational kernel evaluations are replaced by suitable quantum measurements, have recently been 
¬≠proposed10,12 as interesting alternatives. Combining classical and quantum computations, they add to the family 
of quantum-classical hybrid algorithms.
Kernel-based QML (KQML)is particularly attractive to be implemented on linear-optics platforms, as quan-
tum memories are not required. Here, we thus investigate the prospect of KQML with multiphoton quantum 
optical circuits. To this end, we propose kernels that scale exponentially better in the number of required qubits 
than a direct generalization of kernels previously discussed in the ¬≠literature12. We also realize this scheme in a 
proof-of-principle experiment demonstrating its suitability on the platform of linear optics, thus, proving its 
practical applicability with current state of quantum technologies.
Let us explain KQML by first recalling some definitions and theorems, and then we overview the recently 
proposed method for finding linear boundaries in feature Hilbert space (FHS)12. FHS is defined as a space of 
complex vectors |œï(x), where œï describes a feature map (FM), and x denotes a real vector of dimension D (the 
input data). FHSs generally have higher dimension than the original data x. This implies that linear decision 
boundaries in FHS can give rise to nonlinear decision boundaries in the original data space. By virtue of such 
OPEN
1Faculty of Physics, Adam Mickiewicz University, 61‚Äë614¬†Poznan, Poland. 2RCPTM, Joint Laboratory of Optics of 
Palack√Ω University and Institute of Physics of Czech Academy of Sciences, 17. listopadu 12, 771 46¬†Olomouc, Czech 
Republic. 3Theoretical Quantum Physics Laboratory, RIKEN Cluster for Pioneering Research, Wako‚Äëshi¬†351‚Äë0198, 
Japan. 4Department of Physics, The University of Michigan, Ann Arbor, MI¬† 48109‚Äë1040, USA. *email: 
karol.bartkiewicz@upol.cz; acernoch@fzu.cz; k.lemr@upol.cz

2
Vol:.(1234567890)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
nonlinear FMs, it is not required to implement nonlinear transformations on the quantum-state encoded data, 
in contrast to the direct amplitude encoding common in other QML approaches.
The central idea underlying KQML is that inner products of vectors that are mapped into FHS can be directly 
accessed by measurements, which then suggests to identify these inner products with kernel functions. By 
physically measuring the kernel functions Œ∫(x‚Ä≤, x) = |œï(x‚Ä≤)|œï(x)|2‚Ää, it is thus possible to bypass their per pedes 
computation on a classical machine. Such measurement-based implementation may, in some cases, be signifi-
cantly faster than the latter option.
It follows from the representer theorem that a function of the reproducing kernel that minimizes the cost 
function (a solution to the ML problem) can be written as f ‚àó(x) = M
m=1 amŒ∫(x, xm), where M is the number 
of training samples, the coefficients am are real parameters subject to the training, and x belongs to feature space. 
For a given kernel Œ∫‚Ää, the parameters am can be found efficiently. The objective of ML is to deliver a function 
f ‚àó(x) that classifies the non-separable points x1, ..., xM‚àíK and xM‚àíK+1, ..., xM by finding a trade-off between 
the number of misclassifications and the width of the separating margin. The parameters am can be obtained by 
solving the following problem: minimize M
m=1(|am|2 + Œ≥ um) such that aiŒ∫(x, xi) ‚â•1 ‚àíui for i = 1, ..., M ‚àíK, 
and aiŒ∫(x, xi) ‚â§‚àí(1 ‚àíui) for i = M ‚àíK + 1, ..., M, u ‚â•0, where Œ≥ gives the relative weight of the number of 
misclassified points compared to the width of the margin. In a nutshell, this approach allows to replace the non-
linearity of the problem with linear multidimensional quantum computations, which offers a potential speed-up.
Results
Kernel resolution in finite dimensions.‚ÄÉ
An important and widespread kernel class are Gaussian-type 
kernels, which introduce a flexible notion of proximity among data points. An essential hyperparameter of 
Gaussian-type kernels is thus their variance (or, more generally, their resolution). The resolution determines a 
Gaussian kernel‚Äôs ability to distinguish data points, which, for given training data, can decide if a model can be 
trained successfully or not. If kernel resolution is too coarse, resulting decision boundaries miss relevant details 
in the data; if it is too refined, the model becomes prone to overfitting. Only if the resolution can be chosen suffi-
ciently flexibly to be accommodated to the structure of the data, model training can be expected to be successful.
In the infinite-dimensional feature spaces offered by continuous variable implementations, viable FMs with 
(in principle) arbitrary resolution can be implemented, e.g., by mapping data into squeezed ¬≠states12, where the 
adjustable squeezing factor then determines the resolution of the resulting Gaussian kernel (i.e., its variance). 
However, within the paradigm of discrete, finite-dimensional quantum information processing, the FHS dimen-
sion becomes a scarce resource, resulting in limitations on kernel resolution. As we show now, optimizing the 
range of kernel resolutions in finite dimensions then forces us to move beyond the scope of Gaussian kernels.
Let us discuss the optimal kernel resolution that can be achieved in N-dimensional FHS, within the class of 
FMs of the form
with {|n} a basis of the Hilbert space and x ‚àà[‚àí1/2, 1/2)‚Ää. Any data set can be brought to this form, which is a 
routine step in data preparation. We stress that the amplitudes rn are independent from the input values x. The 
resulting kernels then are of the form
In this shorthand notation Œ∫(x) ‚â•0
‚àÄx and Œ∫(0) = 1‚Ää. For the sake of clarity we consider here 1D input data x. 
For D-dimensional inputs x‚Ää, each input component xi is encoded separately, requiring an (N ¬∑ D + D)-dimen-
sional FHS. If the FHS is spanned by q qubits, we have N = 2q ‚àí1‚Ää. In particular, for N = 1 and rn = 1/2 we have 
Œ∫(x, x‚Ä≤) = cos[œÄ(x‚Ä≤ ‚àíx)]2, which realizes a cosine kernel (CK). The class of states (1) comprises also truncated 
squeezed states |œàTSQ(x)‚Ää, with
(Œ∂ denotes the squeezing factor and B renormalizes the state after truncation), and, what we call here, multi-slit 
interference states |œàMSI(x)‚Ää, with constant amplitudes ‚àörn = 1/
‚àö
N‚Ää. The latter inherit their name from the fact 
that, by virtue of x|p = e2œÄipx (h=1), they are formally equivalent to a balanced superposition of momentum 
states in a (hypothetical) compact continuous variable Hilbert space (augmented by an internal spin-N degree 
of freedom),
giving rise to ‚ÄúN-slit interference‚Äù in the position coordinate when projected onto x| ‚äó
1
‚àö
N
N
n=1n|19. Note 
that polynomial kernels (discussed, e.g., ¬≠in8,12) fall outside of the state class (1).
(1)
x ‚Üí|œà(x) =
N

n=0
‚àörne2œÄinx |n,
N

n=0
rn = 1,
(2)
Œ∫(x, x‚Ä≤) = Œ∫(x ‚àíx‚Ä≤) =

N

n=0
rn e2œÄin(x‚Ä≤‚àíx)

2
.
(3)
‚àörn =
‚àö(2n)!(‚àítanh Œ∂)n
‚àö
B 2nn!‚àöcosh Œ∂
(4)
|œàMSI(x) =
1
‚àö
N
N

n=1
x|p = n|n,

3
Vol.:(0123456789)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
We can use the above compact-space embedding to gain further insight into the nature of our kernel defini-
tion (2). If we interpret the states (1) as |œà = N
n=1
‚àörn|p = n ‚äó|n‚Ää, we can introduce the density operator 
œÅ = |œàœà| and trace over the internal spin degree of freedom,
We then find that the kernel (2) is related to the spatial coherences of the mixed reduced state œÅext‚Ää: 
Œ∫(x, x‚Ä≤) = |x|œÅext|x‚Ä≤|2.
We define a kernel‚Äôs spatial resolution x[Œ∫] by its variance (a hyperparameter typically optimized for Gauss-
ian kernels)
where the renormalized kernel ÀúŒ∫(x) = Œ∫(x)/R‚Ää, with R ‚â°
 1/2
‚àí1/2 dx Œ∫(x) = N
n=1 r2
n‚Ää, describes a valid probability 
distribution. In the case of the mulit-slit interference states |œàMSI‚Ää, one analytically obtains 
(x[Œ∫MSI])2 = 1
12(1 ‚àíS1(N))‚Ää, with the interferometric ‚Äúsqueezing factor‚Äù S1(N) = ‚àí12
œÄ2
N‚àí1
j=1 (‚àí1)j N‚àíj
Nj2 ‚Ää, and 
N ‚â•219.
The kernel (6) minimizing the variance is a solution to the optimization problem: minimize rT¬∑K¬∑r
|r|2  such that 
N
n=1 rn = 1‚Ää, where
and r = (r1, . . . , rN)T‚Ää. In Fig.¬†1 we compare this optimized kernel with the TSQ and the MSI kernel. The opti-
mized kernel comes with strongly suppressed side maxima as compared to the MSI kernel, while the TSQ 
maintains a nonvanishing plateau for all x values. Consequently, the optimized kernel enables, for a given N, a 
significantly improved resolution as compared to the other kernel choices. Figure¬†1b clarifies that amplitudes 
decaying symmetrically about the ‚Äúcenter‚Äù state are responsible for improving the kernel resolution.
On the other hand, a kernel that maximizes the variance (i.e., Œ∫(x) = 1‚Ää) follows from r1 = 1 and rn = 0 for 
n  = 1‚Ää, resulting in the variance (x[Œ∫])2 = 1/12. By a suitable choice of the coefficients rn‚Ää, we can thus tune the 
resolution of the kernel between its minimum value obtained for the optimized kernel and its maximum value 
assumed for a uniform kernel.
Whereas kernels of the form (2) can also be efficiently computed classically, their quantum evaluation may 
still deliver a significant speed-up. We illustrate this with an example, the computation of cos2N x. The optimal 
classical algorithm depends on the properties of N. In the best case scenario, N is a power of 2. Then, in the first 
step we compute cos2 x. Next, we compute [cos2 x]2, etc. The entire computation then takes log2(N + 1) steps. As 
we demonstrate below, for the quantum implementation, the required size of the FHS (number of qubits) grows 
also like log2(N + 1)‚Ää. However, in contrast, there the associated calculations are replaced by a single measure-
ment. We expect similar arguments to hold for more general classes of functions, as well.
Beyond the quantum-classical hybrid approach pursued here, the proposed FMs may, if seen as modules to be 
combined with other quantum computing subroutines, contribute their resource-efficient data point separation 
ability to an overall setup that comes with an inherently quantum scaling advantage. MSI states, for instance, can 
be generated in a gate-based quantum computer following the first stage of the phase-estimation ¬≠algorithm20.
Alternative Gaussian‚Äëkernel implementation.‚ÄÉ
Above we have shown that truncated squeezed states 
and their resulting kernels fall within the state class (1). If we relax the condition that the amplitudes rn be inde-
(5)
œÅext = Trint(œÅ) =
N

n=1
rn|p = np = n|.
(6)
(x[Œ∫])2 ‚â°
 1/2
‚àí1/2
dx x2 ÀúŒ∫(x),
(7)
Knm =
 1
12,
n = m
(‚àí1)|n‚àím|
2(n‚àím)2œÄ2 , else
Figure¬†1.‚ÄÇ ‚ÄâKernel family (2) for different amplitude choices. (a) We find that the resolution-optimized kernel 
(blue solid) exhibits suppressed side maxima as compared to the MSI kernel (red dashed), while the TSQ kernel 
(with squeezing factor Œ∂ = 2‚Ää, black dotted) maintains a nonvanishing plateau at all x values. For comparison, 
we also display the respective squeezed-state kernel for N ‚Üí‚àû (gray dotted) and CK (purple dash-dotted). (b) 
Characteristic amplitude progressions for the example of N = 14 and Œ∂ = 4‚Ää. (c) The optimized kernel exhibits 
a significantly improved resolution progression with N,¬† as compared to the MSI or the TSQ kernel (here with 
Œ∂ = 3).

4
Vol:.(1234567890)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
pendent from the input values x, we can formulate an alternative data encoding into truncated squeezed states 
according to
where N = 2q ‚àí1‚Ää, x = x1 + ix2, and Z‚àí1 = N
n=0
(s|x|)2n
n!
. Note that this feature map is defined for 2D inputs 
x = (x1, x2)T‚Ää. For large N this kernel again reproduces to good approximation a Gaussian kernel, as
where the variance is set by the hyperparameter s. In particular, as shown in Fig.¬†2, this approximation is valid 
for q = 2 and relatively small values of¬†s.
We find that this kernel performs, for the number of qubits q = 2 and after numerically optimizing the 
hyperparameter to s = 2‚Ää, on average as well as the cosine kernel for the same total number of qubits equal to 
N = 1 (see Fig.¬†4). Moreover, by appropriate parameter reconfiguration, it would be possible to realize this type 
of kernel using the same experimental setup. From an experimental perspective, however, it is more conveni-
ent (and thus scalable) to implement the feature map associated with the powers of the cosine kernel, which is 
exclusively implemented by setting phases and polarization angles.
Cosine kernels.‚ÄÉ
The kernel selected for our proof-of-principle demonstration of KQML is
(8)
|œÜ(x) = Z
N

n=0
(sx)n
‚àö
n!
|n,
(9)
Œ∫(x‚Ä≤, x) = |œï(x‚Ä≤)|œï(x)|2 ‚âàexp [‚àís2(x1 ‚àíx‚Ä≤
1)2 ‚àís2(x2 ‚àíx‚Ä≤
2)2],
Figure¬†2.‚ÄÇ ‚ÄâTraining results on a random inseparable data set of 40 samples (up/down-tipped triangles). The 
performance on a test set (left/right-tipped triangles) of 60 points (the fraction of correctly classified samples 
that were not used in the QML process) is given in the bottom right corner of each respective subplot. We find 
that the optimal variance/resolution choice for the Gaussian kernel is s = 2‚Ää. For s = 3 we deal with overfitting. 
Shown are the simulation results both for an exact Gaussian kernel and for the truncated FM (8) comprising 
4 terms (‚Ääq = 2‚Ää). The learned classification boundaries are given as contour plots. The slight difference in 
performance compared to the theoretical prediction is due to statistical fluctuations in the experimental data 
and the relatively small test set (misclassification of a single near-boundary point results in a 0.02 performance 
drop).

5
Vol.:(0123456789)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
w h e r e  
t h e  
F M  
t a k i n g  
a  
n o r m a l i z e d  
f e at u r e  
xn ‚àà[‚àíœÄ/2, œÄ/2) 
t o  
F H S  
i s 
|œï(x) = D
n=1
N
k=0
N
k

sink(xn) cosN‚àík(xn)|kn. Note that N is related to the number of qubits q per dimen-
sion as q = ‚åàlog2(N + 1)‚åâ. This FM can also be considered a constant-phase representation of constant-amplitude 
states. This is the same as representing states either in a basis of eigenstates of x or z components of a collective 
spin operator. In particular, (cos(x)|0 + sin(x)|1)/
‚àö
2 ‚áî(|0‚Ä≤ + e2ix|1‚Ä≤)/
‚àö
2, where |0 = (|0‚Ä≤ + |1‚Ä≤)/
‚àö
2 
and |1 = (|0‚Ä≤ ‚àí|1‚Ä≤)/
‚àö
2.
This mapping uses exponentially less resources (qubits) than the direct product of the map from Ref.12, i.e., 
|œï(x) = D
n=1
N
m=1
1
k=0 sink(xn) cos1‚àík(xn)|kn,m, where the number of qubits per dimension is q = N. 
Using the powers of CKs allows us to adjust the kernel resolution by choosing the proper value of N. Thus, the 
number of used qubits can be related directly to the variance of the kernel. The number of qubits here plays 
the same role as the squeezing parameter in the experimental proposal given in Ref.12. The CK can also include 
additional (D ‚àí1) degrees of freedom by virtue of a FM defined as
(10)
Œ∫(x‚Ä≤, x) = |œï(x‚Ä≤)|œï(x)|2 =
D

n=1
cos2N(x‚Ä≤
n ‚àíxn),
(11)
|œï(x) =
D

n=1
N

k=0
ei2yn‚àí1
N
k

sink(xn) cosN‚àík(xn)|kn,
Figure¬†3.‚ÄÇ ‚ÄâOptical circuit implementing both the FM and the model circuits. The performance 
of the setup in QML is shown in Fig.¬†4 for N = 1 and D = 2. The experimental setup consists of 
polarizing beam splitters (PBSs), beam dividers (BDs), quarter-wave and half-wave plates (QWPs 
and HWPs, respectively), and single photon detectors Dn for n = 1a, 1b, 2a, 2b, 3, 4‚Ää. D3 and D4 
are H/V polarization resolving (implemented as a PBS and two standard detectors). The kernel 
Œ∫(x‚Ä≤, x)exp = [
p,s=H,V CC(D2s, D3p)‚àíCC(D2V, D3H)+CC(D2H, D3V)]/ 
m>n
6
n=1 CC(Dm, Dn) is given as 
a ratio of coincidences CC(Dm, Dn) registered by photon detectors Dn and Dm to the total number of photons.

6
Vol:.(1234567890)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
where y0 = 0, the number of terms here is (N + 1)D, and the associated kernel measured by postselection is 
Œ∫(x‚Ä≤, x) = D
n=1 cos2N(x‚Ä≤
n ‚àíxn) cos2(y‚Ä≤
n‚àí1 ‚àíyn‚àí1).
Discussion
We have experimentally implemented KQML to solve three classification problems on a two-photon optical 
quantum computer. In our experiment we implemented a D = 2, N = 1 kernel (using all the modes from Fig.¬†3, 
we can set at most D = 5 with q = 1‚Ää). We used two photons, but only the top mode of the dual-rail encoding. 
Including more modes would lead to kernels causing overfitting (see Fig.¬†4).
We have performed measurements for M = 40 two-dimensional samples (‚ÄäD = 2‚Ää), drawn from two classes 
(see horizontally/vertically-tipped triangles in Fig.¬†4). This procedure was repeated for three benchmark classifi-
cation problems. For each benchmark 40 √ó 39/2 = 780 measurements were performed to create a corresponding 
Gram matrix (GM), which was subsequently used to find the best linear classification boundary as given by the 
representer theorem. In other words, a custom kernel Œ∫(xm, xn) = Œ∫(xn, xm) for m, n = 1, 2, ..., M was measured. 
This kernel was used as a custom precomputed kernel for the scikit-learn SVC classifier in python.
Pairs of H-polarized photons were prepared in a type-I spontaneous parametric down-conversion process in a 
Œ≤‚ÄìBaB2O4 crystal. The crystal was pumped by a 200¬†mW laser beam at 355¬†nm (repetition rate of 120¬†MHz). The 
coincidence rate, including all possible detection events from Fig.¬†3, was approximately 250 counts per second. 
The setup operates with high fidelity (98%) and the dominant source of errors can be attributed the Poissonian 
photon count statistics. The design of this setup is modular and its easy to incorporate more qubits by simply 
adding additional blocks. We measured each point for a time necessary to collect about 2,500 detection events. 
Thus, excluding the time needed to switch the setup parameters, the whole measurement for a single benchmark 
problem takes about two hours.
To prepare the contour plot of the decision function based on the experimental data shown in Fig.¬†4 and to 
quantify the performance of the trained model on the relevant test sets, we have also measured the GM for 1,225 
points and used its symmetries to fill in the unmeasured values. The values for points in between have been found 
Figure¬†4.‚ÄÇ ‚ÄâTraining results on a random inseparable data set of 40 samples (up/down-tipped triangles). The 
performance on a test set (left/right-tipped triangles) of 60 points (the fraction of correctly classified samples 
that were not used in the QML process) is given in the bottom right corner of each respective subplot. We see 
that the best choice of CK is N = 1‚Ää. For N = 2 we deal with overfitting and for N = 1/2 the kernel is too coarse 
to give as good results as for N = 1‚Ää. The learned classification boundaries are given as contour plots. The slight 
difference in performance of KQML in relation to the theoretical prediction is due to statistical fluctuations of 
the experimental data and relatively small test set (misclassification of a single near-boundary point results in 
0.02 performance drop).

7
Vol.:(0123456789)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
using linear interpolation. The data accumulation time can be shortened by orders of magnitudes by fine tuning 
the parameters of the setup and by using brighter photon sources.
Conclusions
We report on the first experimental implementation of supervised QML for solving a nonlinear multidimensional 
classification problem with clusters of points which are not trivially separated in the feature space. We hope that 
our research on QML will help to improve ML technologies, which are a major power-horse of many industries, 
a vivid field of research in computer science, and an important technique for solving real-world problems. We 
believe that both the theoretical and the experimental investigation of FM circuits and their constraints regarding 
kernel resolution and compression for a limited FHS (i.e., FHS size dependent FMs) constitutes a crucial step in 
the development of practical KQML for support-vector-machine ¬≠QML8‚Äì10,12,13.
We demonstrate that a linear-optical setup with discrete photon encoding is a reliable instrument for this 
class of quantum machine learning tasks. We also report obtaining exponentially better scaling of FHS in the 
case of CK than in the case of taking direct products of ¬≠qubits12. The same can hold for other more complex 
kernels implemented in finite FHS, which could appear unfeasible, but in fact require nontrivial FMs (e.g., the 
resolution-optimized kernels shown in Fig.¬†1). Thus, KQML can provide a promising perspective for utilizing 
noisy intermediate-scale quantum ¬≠systems21‚Äì24, complementing artificial quantum neural ¬≠networks25‚Äì29 and other 
hybrid quantum-classical ¬≠algorithms30‚Äì32.
The classical computational cost of the power kernel computation is O[log(N)] and the quantum cost is 
a constant value depending on the precision of the computation. In the classical case, one needs to perform 
O[log(N)] computation steps that can not run in parallel due to the recursive nature of the classical algorithm. 
In the quantum case, one needs to run 1 computation step but on log(N) qubits. As in any quantum computation, 
the precision of the calculation depends on the number of measurements and it can be considered constant for a 
given computational problem. This observation itself is a valuable result and a quantum advantage. The quantum 
advantage of the presented approach is apparent in terms of the complexity of calculations, i.e., O[log(N)] versus 
O(1)‚Ää. Consider the number of samples needed for quantum calculations. It depends on the confidence level (z) 
and admissible error: «´‚Ää. For a given pair of z and error «´‚Ää, one needs O(1/«´2) repetitions of the experiment. This 
is just a constant overhead. In the classical case, this constant overhead can be smaller, but the complexity of 
calculations can be larger as the it is N-dependent. Only if we face significantly lower than unity qubit-number-
dependent efficiency Œ∑ (i.e., circuit-size dependent losses), for a given z-value the complexity of quantum com-
putations should be considered as being O(Œ∑( ‚àílog(N))/«´2)‚Ää. However, the power scaling also applies to the 
total error probability of classical computations of log(N) steps. Note, however, that both Œ∑ and single-step error 
probability of classical computing are not fundamentally limited and can be arbitrary close to 1 or 0, respectively.
Our quantum kernels can be used for solving high-dimensional classification problems and could poten-
tially be computed faster than their classical counterparts. Popular problems solved by classification algorithms 
include image recognition (e.g. face detection or character recognition), speech recognition (e.g. voice user 
interfaces), medical diagnoses (e.g. associating results of medical tests with a class of diseases), real-time specific 
data extraction from vast amounts of unstructured data (e.g. classification of patterns in unstructured data) and 
many more. Classification can also be used as an initial phase for predictive computations that help to make the 
best decision based on the available data (e.g., managing risk, security, traffic, procurement etc.). We believe that 
this quantum-enhanced approach is useful especially in cases where it is difficult or impossible to achieve the 
result on time with classical computing.
Methods
Optical circuit for KQML.‚ÄÉ
States given by Eq.¬†(11) can be prepared in a quantum optical setup. In the 
reported proof of principle experiment, we can set N = 3 and D = 2‚Ää. This means that, effectively, the experiment 
deploys q = 2 qubits per dimension. The FM is defined via single-photon polarization states (H/V polarization) 
as well as dual-rail encoding (T/B for top/bottom rail, respectively)
where c(xn) ‚â°cos(xn) and s(xn) ‚â°sin(xn). This approach is resource-efficient as it only requires two photons 
to encode x into the FHS state of N = 3 and D = 2.
An optical circuit implementing this FM is depicted in Fig.¬†3. The top part of the FM circuit works as follows: 
first, it transforms the standard input |HB using wave plates resulting in |HB ‚Üí(|HB + |VB)/
‚àö
2. Next, a 
beam divider separates polarization modes in space, i.e., we have (|HB + |VT). Now, the effective operation 
of wave plates in the top and bottom modes can be described as first transforming |VT ‚Üí¬µT|HT + ŒΩT|VT 
and |HB ‚Üí¬µB|HB + ŒΩB|VB. The parameters are set as ¬µT =
‚àö
2c3(xn), ŒΩT =
‚àö
2s3(xn), ¬µB =
‚àö
6c2(xn)s(xn), 
ŒΩB =
‚àö
6c(xn)s2(xn).
This whole operation is unitary and can be described as U(x)|HH = |œï(x). The complex conjugate of opera-
tion U(x) is U‚Ä†(x‚Ä≤) and it can be used to express the kernel as Œ∫(x‚Ä≤, x) = |HH|U‚Ä†(x‚Ä≤)U(x)|HH|2. Thus, the 
circuit U‚Ä†(x‚Ä≤) for projecting the state |œï(x) to |œï(x‚Ä≤) can be constructed as the inverse of the feature embedding 
U(x) circuit, but for setup parameters set for x‚Ä≤‚Ää. The next action of the plates in the top and bottom rails is to 
perform a reverse transformation, but for xn = x‚Ä≤
n. Next, the plates flip the polarizations in the respective rails. 
Now, the interesting part of the engineered state is in the top rail with flipped polarization. To implement U(x‚Ä≤)‚Ä†, 
(12)
|œï(x) =
2

n=1

c3(xn)|HTn +
‚àö
3s(xn)c2(xn)|HBn
+
‚àö
3c(xn)s2(xn)|VBn + s3(xn)|VTn

,

8
Vol:.(1234567890)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
the last pair of waveplates is used both to flip the polarization and to perform the Hadamard transformation. 
Finally, the PBS transmits only H-polarized photons for further processing.
The procedure of measuring the kernel Œ∫(x‚Ä≤, x) can be extended to include additional dimensions, resulting 
in measuring the kernel ¬ØŒ∫(x‚Ä≤, x) = Œ∫(x‚Ä≤, x) cos2(y ‚àíy‚Ä≤) following from FM (11). Instead of the transformation 
U‚Ä†(x‚Ä≤)U(x), we consider R‚Ä†(y‚Ä≤)U‚Ä†(x‚Ä≤)U(x)R(y), where R(y) = e2iy|HH| is a phase shift applied to a prese-
lected H-polarized photon in the bottom part of the setup, and R‚Ä†(y‚Ä≤) = e‚àí2iy‚Ä≤|HH| is a phase shift to the 
postselected H-polarized photon in the same part of the setup. The phase difference between the postselected 
upper and lower H-polarized photons can be measured as cos2(y ‚àíy‚Ä≤). This is done with PBS‚Ä≤ which transmits 
diagonally-polarized photons |D = (|H + |V)/
‚àö
2 and reflects antidiagonal photons |A = (|H ‚àí|V)/
‚àö
2, 
and polarization-resolving single-photon detectors (see caption of Fig.¬†3).
Received: 30 March 2020; Accepted: 24 June 2020
References
	 1.	 Mohri, M., Rostamizadeh, A. & Talwalkar, A. Foundations of Machine Learning (MIT Press, Cambridge, 2012).
	 2.	 Schuld, M., Sinayskiy, I. & Petruccione, F. An introduction to quantum machine learning. Contemp. Phys. 56, 172 (2015).
	 3.	 Cai, X.-D. et al. Entanglement-based machine learning on a quantum computer. Phys. Rev. Lett. 114, 110504 (2015).
	 4.	 Biamonte, J. et al. Quantum machine learning. Nature 549, 195 (2017).
	 5.	 Schuld, M., Fingerhuth, M. & Petruccione, F. Implementing a distance-based classifier with a quantum interference circuit. EPL 
119, 60002 (2017).
	 6.	 Gao, J. et al. Experimental machine learning of quantum states. Phys. Rev. Lett. 120, 240501 (2018).
	 7.	 Ciliberto, C. et al. Quantum machine learning: A classical perspective. Proc. R. Soc. A 474, 20170551 (2018).
	 8.	 Rebentrost, P., Mohseni, M. & Lloyd, S. Quantum support vector machine for big data classification. Phys. Rev. Lett. 113, 130503 
(2014).
	 9.	 Li, Z., Liu, X., Xu, N. & Du, J. Experimental realization of a quantum support vector machine. Phys. Rev. Lett. 114, 140504 (2015).
	10.	 Chatterjee, R. & Yu, T. Generalized coherent states, reproducing kernels, and quantum support vector machines. Quantum Inf. 
Commun. 17, 1292 (2017).
	11.	 Sheng, Y.-B. & Zhou, L. Distributed secure quantum machine learning. Sci. Bull 62, 1025‚Äì1029 (2017).
	12.	 Schuld, M. & Killoran, N. Quantum machine learning in feature Hilbert spaces. Phys. Rev. Lett. 122, 040504 (2019).
	13.	 Havl√≠ƒçek, V. et¬†al. Supervised learning with quantum-enhanced feature spaces.
	14.	 Wang, W. & Lo, H.-K. Machine learning for optimal parameter prediction in quantum key distribution. Phys. Rev. A 100, 062334 
(2019).
	15.	 Hendry, D. & Feiguin, A. E. Machine learning approach to dynamical properties of quantum many-body systems. Phys. Rev. B 
100, 245123 (2019).
	16.	 Zhang, Y. et al. Machine learning in electronic-quantum-matter imaging experiments. Nature 570, 484‚Äì490 (2019).
	17.	 Tr√°vn√≠ƒçek, V., Bartkiewicz, K., ƒåernoch, A. & Lemr, K. Experimental measurement of the hilbert-schmidt distance between two-
qubit states as a means for reducing the complexity of machine learning. Phys. Rev. Lett. 123, 260501 (2019).
	18.	 Gyongyosi, L. & Imre, S. Optimizing high-efficiency quantum memory with quantum machine learning for near-term quantum 
devices. Sci. Rep. 10, (2020).
	19.	 Gneiting, C. & Hornberger, K. Detecting entanglement in spatial interference. Phys. Rev. Lett. 106, 210501 (2011).
	20.	 Nielsen, M. A. & Chuang, I. Quantum computation and quantum information (Cambridge University Press, Cambridge, 2002).
	21.	 Preskill, J. Quantum computing in the NISQ era and beyond. Quantum 2, 79 (2018).
	22.	 Fujii, K. & Nakajima, K. Harnessing disordered-ensemble quantum dynamics for machine learning. Phys. Rev. Appl. 8, 024030 
(2017).
	23.	 Dunjko, V., Ge, Y. & Cirac, J. I. Computational speedups using small quantum devices. Phys. Rev. Lett. 121, 250501 (2018).
	24.	 Kandala, A. et al. Error mitigation extends the computational reach of a noisy quantum processor. Nature 567, 491 (2019).
	25.	 Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photonics 11, 441 (2017).
	26.	 Bueno, J. et al. Reinforcement learning in a large-scale photonic recurrent neural network. Optica 5, 756 (2018).
	27.	 Tacchino, F., Macchiavello, C., Gerace, D. & Bajoni, D. An artificial neuron implemented on an actual quantum processor. npj 
Quantum Inf. 5, 26 (2019).
	28.	 Kak, S.¬†C. Quantum neural computing. In Advances in Imaging and Electron Physics, Vol.¬†94, 259 (Elsevier, Amsterdam, 1995).
	29.	 Farhi, E. & Neven, H. Classification with quantum neural networks on near term processors (2018). arXiv‚Äã:1802.06002‚Äã .
	30.	 Li, Y. & Benjamin, S. C. Efficient variational quantum simulator incorporating active error minimization. Phys. Rev. X 7, 021050 
(2017).
	31.	 Kandala, A. et al. Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets. Nature 549, 242 
(2017).
	32.	 Mitarai, K., Negoro, M., Kitagawa, M. & Fujii, K. Quantum circuit learning. Phys. Rev. A 98, 032309 (2018).
Acknowledgements
Authors acknowledge financial support by the Czech Science Foundation under the project No. 19-19002S. The 
authors also acknowledge the projects No. ¬†CZ.1.05/2.1.00/19.0377 of the Ministry of Education, Youth and 
Sports of the Czech Republic financing the infrastructure of their workplace. K.J. is supported in part by the 
Palacky University internal grant No.¬†IGA-PrF-2020-007. F.N. is supported in part by: NTT Research, Army 
Research Office (ARO) (Grant No. W911NF-18-1-0358), Japan Science and Technology Agency (JST) (via the 
CREST Grant No. JPMJCR1676), Japan Society for the Promotion of Science (JSPS) (via the KAKENHI Grant 
No. JP20H00134, and the JSPS-RFBR Grant No. JPJSBP120194828), and the Foundational Questions Institute 
Fund (FQXi) (Grant No. FQXi-IAF19-06), a donor advised fund of the Silicon Valley Community Foundation.
Author contributions
K.B., C.G., and F.N. developed the theoretical framework, and wrote the paper. K.B. planned the experiment, 
processed the experimental data. A.ƒå., K.J. and K.L. designed and built the experimental setup and performed 
the measurements. All authors discussed the results and participated in the manuscript preparation.

9
Vol.:(0123456789)
Scientific Reports |        (2020) 10:12356  | 
https://doi.org/10.1038/s41598-020-68911-5
www.nature.com/scientificreports/
Competing interests‚ÄÇ
The authors declare no competing interests.
Additional information
Correspondence and requests for materials should be addressed to K.B., A.ƒå.¬†or¬†K.L.
Reprints and permissions information is available at www.nature.com/reprints.
Publisher‚Äôs note‚ÄÇ Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
Open Access‚ÄÇ  This article is licensed under a Creative Commons Attribution 4.0 International 
License, which permits use, sharing, adaptation, distribution and reproduction in any medium or 
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the 
Creative Commons license, and indicate if changes were made. The images or other third party material in this 
article are included in the article‚Äôs Creative Commons license, unless indicated otherwise in a credit line to the 
material. If material is not included in the article‚Äôs Creative Commons license and your intended use is not 
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from 
the copyright holder. To view a copy of this license, visit http://creat‚Äãiveco‚Äãmmons‚Äã.org/licen‚Äãses/by/4.0/.
¬© The Author(s) 2020

