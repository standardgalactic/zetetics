ï›œ 
Creativity Has Left the Chat: 
The Price of Debiasing Language Models 
Behnam Mohammadi1 
Carnegie Mellon University 
Tepper School of Business 
Jun ï™€, ï˜ºï˜¹ï˜ºï˜¼ 
Abstract 
Large Language Models (LLMs) have revolutionized natural language processing but can 
exhibit biases and may generate toxic content. While alignment techniques like Reinforcement 
Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined 
as syntactic and semantic diversity, remains unexplored. We investigate the unintended 
consequences of RLHF on the creativity of LLMs through three experiments focusing on the 
Llama-ï˜º series. Our findings reveal that aligned models exhibit lower entropy in token predictions, 
form distinct clusters in the embedding space, and gravitate towards â€œattractor statesâ€, indicating 
limited output diversity. Our findings have significant implications for marketers who rely on 
LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The 
trade-off between consistency and creativity in aligned models should be carefully considered when 
selecting the appropriate model for a given application. We also discuss the importance of prompt 
engineering in harnessing the creative potential of base models. 
 
Keywords: Large Language Models (LLMs), Reinforcement Learning from Human Feedback 
(RLHF), AI Alignment, Recommendation Systems, Diversity and Creativity 
 
ï›œ. 
Introduction 
Large Language Models (LLMs) have demonstrated remarkable capabilities in 
generating human-like text, with applications spanning various domains, including 
marketing. However, LLMs have also been shown to exhibit biases and generate toxic or 
inappropriate content (Bender et al., ï˜ºï˜¹ï˜ºï›œ; Gehman et al., ï˜ºï˜¹ï˜ºï˜¹), prompting the 
development of techniques such as Reinforcement Learning from Human Feedback 
(RLHF) to align LLMs with human values and preferences (Ouyang et al., ï˜ºï˜¹ï˜ºï˜º; Stiennon 
et al., ï˜ºï˜¹ï˜ºï˜º), aiming to mitigate these issues. 
While RLHF has proven effective in reducing biases and toxicity in LLMs, our work 
suggests that this alignment process may inadvertently lead to a reduction in the modelsâ€™ 
 
 
1 behnamm@cmu.edu 

ï˜º 
creativity and output diversity. In the context of this paper, we define â€œcreativityâ€ as the 
modelâ€™s ability to generate outputs with high syntactic and semantic diversity. Syntactic 
diversity refers to the variety in the choice of words, sentence structures, and other 
linguistic elements, while semantic diversity pertains to the range of meanings, 
sentiments, and ideas expressed in the generated text1. 
The potential trade-off between safety and creativity is particularly relevant in the 
context of marketing, where generating diverse and engaging content is crucial for various 
applications, such as customer persona generation, ad creation, writing product 
description, and customer  support. One specific application of LLMs in marketing is the 
generation of simulated customers or personas with diverse preferences and backgrounds. 
These personas can be used for various purposes, such as training bank associates 
to better communicate with actual customers or providing business school students with 
more engaging alternatives to traditional case studies. 
When generating customer personas using LLMs, there are two primary approaches: 
generating multiple personas simultaneously2 or generating them one at a time. While 
creating multiple personas simultaneously might seem more efficient, it has limitations 
due to the context size3 constraints of LLMs and the causal attention mechanism in these 
models (Vaswani et al., ï˜ºï˜¹ï›œï˜¿). The causal attention mechanism means that the 
distribution of the generated personas will not be independent, as each new persona 
would depend on the previous generations. Therefore, generating personas one at a time 
is a more suitable approach, as it solves both the context size and independence issues. 
However, when using this method with an aligned LLM, an unexpected challenge arises: 
the generated personas often exhibit striking similarities in their preferences and 
characteristics, lacking the desired heterogeneity. This lack of heterogeneity in the 
generated personas is problematic as it limits the ability to capture the diverse preferences 
and behaviors of real-world customers, potentially leading to less effective marketing 
strategies and suboptimal user experiences. 
This observed lack of creativity in the outputs of aligned models led to the suspicion 
that the RLHF process itself might be the underlying cause. We investigate this problem 
 
 
1 It is important to note that our use of the term â€œdiversityâ€ does not refer to the concept of diversity 
in the context of diversity, equity, and inclusion (DEI) or other similar domains, although one of our 
experiments does show that the aligned model exhibits reduced diversity in that sense as well. 
2 For example, asking the LLM to generate ï›œï˜¹ï˜¹ personas delimited by new lines. 
3  Context size refers to the maximum number of tokens a transformer can handle simultaneously, 
encompassing both the input sequence and the generated output. For instance, a context size of ï˜¼,ï˜¹ï™ï˜¾ 
tokens, as seen in Metaâ€™s Llama-ï˜º models, corresponds to approximately ï˜¾ pages of English text (â€œContext 
length in LLMs,â€ ï˜ºï˜¹ï˜ºï˜»). 

ï˜» 
by taking a foundational approach and examining the issue at both the semantic and 
syntactic levels. Our study comprises three experiments that aim to provide a 
comprehensive understanding of how the alignment process affects the diversity of LLM 
outputs. 
Experiment ï›œ serves as a concrete example of the impact of RLHF on creativity in a 
practical marketing context. We generate customer personas and their corresponding 
product reviews using both the base and aligned models, comparing the diversity of the 
generated attributes, such as names, demographics, and review content. The results reveal 
significant differences in the variety of outputs between the two models, with the aligned 
model exhibiting less diversity and more repetitive patterns. 
Experiment ï˜º investigates the semantic diversity of the modelsâ€™ outputs by examining 
their ability to recite a historical fact about Grace Hopper1 in various ways. The generated 
outputs are encoded into sentence embeddings and visualized using dimensionality 
reduction techniques. The results reveal that the aligned modelâ€™s outputs form distinct 
clusters, suggesting that the model expresses the information in a limited number of ways. 
In contrast, the base modelâ€™s embeddings are more scattered and spread out, indicating a 
higher level of semantic diversity in the generated outputs. These results are further 
supported by the cosine similarity analysis which shows the aligned modelâ€™s outputs are 
more semantically similar to each other compared to the base modelâ€™s outputs. 
An intriguing property of the aligned modelâ€™s generation clusters in Experiment ï˜º is 
that they exhibit behavior similar to attractor states in dynamical systems. We 
demonstrate this by intentionally perturbing the modelâ€™s generation trajectory, effectively 
nudging it away from its usual output distribution. Surprisingly, the aligned model 
gracefully finds its way back to its own attractor state and in-distribution response. The 
presence of these attractor states in the aligned modelâ€™s output space is a phenomenon 
related to the concept of mode collapse in reinforcement learning, where the model over-
optimizes for certain outputs, limiting its exploration of alternative solutions. This 
behavior contrasts with the base model, which exhibits greater flexibility and adaptability 
in its outputs. 
Experiment ï˜» delves into the syntactic diversity of the models by analyzing the entropy2 
of generated tokens and the probability distributions over the top predicted tokens at each 
step. The results show that the base model exhibits significantly higher average entropy, 
 
 
1 The inventor of the COBOL programming language which is still heavily used by financial institutions 
and banks (â€œGrace Hopper,â€ ï˜ºï˜¹ï˜ºï˜¼). 
2 Entropy is a measure of uncertainty in a random variable. 

ï˜¼ 
assigning more spread-out probabilities to different tokens, while the aligned model has a 
more skewed probability distribution, favoring certain tokens over others. 
The findings from these experiments suggest that the RLHF process, which aims to 
reduce biases and toxicity in LLMs, may transform them into more deterministic 
algorithms that lack the capacity to explore diverse sets of token trajectories, leading to 
reduced semantic and syntactic diversity in their outputs. In other words, aligned models 
exhibit higher confidence in their outputs, providing consistency and predictable behavior. 
However, this confidence comes at the cost of lowered creativity, as the models tend to 
stick to a limited set of outputs. 
In marketing, this trade-off between consistency and creativity has far-reaching 
consequences. Applications such as copywriting, writing scripts for ad clips, and customer 
persona generation all require a high level of variation and diversity in the generated 
content. If an aligned model is used for these tasks, the resulting outputs may lack the 
necessary heterogeneity and novelty to effectively engage the target audience. Similarly, 
in the domain of recommendation systems, LLMs that lack diversity in their outputs may 
struggle to recommend a diverse set of products to users, potentially leading to suboptimal 
user experiences and reduced customer satisfaction. 
It is important to note that base models, while more creative, are not directly usable 
in applications like chatbots. As a result, techniques such as prompt engineering (also 
known as prompt programming) (Sahoo et al., ï˜ºï˜¹ï˜ºï˜¼) become even more crucial when 
working with base models. These techniques can help guide the modelsâ€™ outputs and make 
them more suitable for specific applications while still leveraging their creative potential. 
Contrary to the belief that prompt engineering may become obsolete, our findings suggest 
that these techniques will be more important than ever in harnessing the power of base 
models. 
Consequently, the choice between base and aligned models should be carefully 
considered based on the specific requirements of the task at hand. For applications where 
creativity is paramount, such as marketing, fiction writing, and other areas where novelty 
and diversity are valued, base models may be more suitable. On the other hand, aligned 
models may be preferred when safety and consistency are the primary concerns, such as 
in customer support or content moderation. 
The remainder of this paper is structured as follows: Section ï˜º provides background 
information on LLMs, RLHF, and their applications in marketing. Section ï˜» presents our 
experiments comparing the behavior of base and aligned models, followed by a discussion 
of the results and their implications in Section ï˜¼. Finally, Section ï˜½ concludes the paper 
and outlines future research directions. 

ï˜½ 
ï˜º. 
Literature Review 
RLHF has emerged as a promising technique to align LLMs with human preferences 
and values. However, recent research has highlighted several limitations and potential 
unintended consequences of RLHF, including scalability and efficiency concerns due to its 
reliance on human annotators (Lee et al., ï˜ºï˜¹ï˜ºï˜»; Yuan et al., ï˜ºï˜¹ï˜ºï˜»), variability and 
potential bias in human feedback affecting the quality and consistency of the process (Yu 
et al., ï˜ºï˜¹ï˜ºï˜»), vulnerability to manipulation by adversarial annotators leading to security 
breaches and ethical concerns (Wang et al., ï˜ºï˜¹ï˜ºï˜»), and alignment challenges such as 
objective mismatch and length bias (Lambert and Calandra, ï˜ºï˜¹ï˜ºï˜»; Shen et al., ï˜ºï˜¹ï˜ºï˜»). 
Despite these limitations, LLMs have shown significant potential in transforming 
various aspects of marketing and business. They can automate and accelerate time-
consuming tasks such as text generation, summarization, and content creation, leading to 
increased productivity and efficiency in marketing and business operations (Head et al., 
ï˜ºï˜¹ï˜ºï˜»). LLMs also enhance customer interaction by providing personalized and context-
aware responses, which can improve customer satisfaction and engagement, particularly 
in customer service and support functions (Franceschelli and Musolesi, ï˜ºï˜¹ï˜ºï˜»). 
Furthermore, by analyzing large volumes of data, LLMs can generate valuable market 
insights, helping businesses understand customer preferences, market trends, and 
competitive landscapes, which can inform strategic decisions and marketing campaigns 
(Eloundou et al., ï˜ºï˜¹ï˜ºï˜»). 
However, while the literature has explored the applications and limitations of LLMs in 
marketing and business contexts, there is a notable gap in understanding how the RLHF 
process affects the creativity and variation in the modelsâ€™ outputs. This is a crucial aspect 
for marketers and professionals who rely on LLMs for creative tasks. Understanding the 
trade-off between alignment with human preferences and the preservation of creative 
diversity in the generated content can have significant implications for the effectiveness 
and engagement of marketing initiatives. 
ï˜». 
Experiments 
Remember that our goal is to evaluate and contrast the diversity of texts produced by 
base models and their aligned counterparts. We focus on both the short-term (syntactic) 
and long-term (semantic) variations in model outputs using the Llama-ï˜º language models. 
Meta has made both the base models1 and their corresponding aligned versions2 publicly 
available, making them an ideal choice for this study. Therefore, comparisons are made 
 
 
1 Referred to by Meta as â€œtextâ€ models. 
2 Referred to by Meta as â€œchatâ€ models. 

ï˜¾ 
between Llama-ï˜º-ï˜¿B-text (the â€œbaseâ€ model) and Llama-ï˜º-ï˜¿B-chat (the â€œalignedâ€ model) 
where ï˜¿B refers to the parameter size of the LLM. These models are currently highly 
favored within the open-source community1. Their widespread use is partly attributed to 
the affordability of finetuning them. 
We conduct three experiments to examine the effects of the alignment process on model 
creativity and diversity. Experiment ï›œ serves as a concrete example of the differences in 
creativity between the base and aligned models, while Experiments ï˜º and ï˜» investigate 
the underlying mechanisms that contribute to these differences. 
Each LLM is given an initial prompt which must be completed for a maximum2 of 
ğ‘›predict tokens3. LLMs typically generate one token at a time. At each step, the LLM 
produces a set of logits over the potential next tokens. These logits are then normalized 
to sum up to ï›œ using the softmax function, and one token is sampled randomly according 
to its probability: 
 
Pr(ğ‘¡ğ‘œğ‘˜ğ‘–) =
exp(logit(ğ‘¡ğ‘œğ‘˜ğ‘–) ğ‘‡
â„ )
âˆ‘exp(logit(ğ‘¡ğ‘œğ‘˜ğ‘–) ğ‘‡
â„ )
ğ‘–
 
ï˜».ï›œ 
where ğ‘¡ğ‘œğ‘˜ğ‘– is token number ğ‘– in the LLMâ€™s vocabulary4 of tokens and ğ‘‡âˆˆ(0, 1] is a 
parameter called temperature which controls the â€œsoftnessâ€ of the probability distribution5. 
In our experiments we choose ğ‘‡= 1.0 for maximum response variation. 
ï˜».ï›œ. 
Experiment ï›œ: Customer Persona and Review Generation 
In this experiment, we generate customer personas using both the base and aligned 
models. For each model, we create ï›œï˜¹ï˜¹ unique customer personas with the following 
attributes: first name, last name, gender, age, nationality, ethnicity, and personality type, 
 
 
1 Currently, there are more than ï›œï˜½,ï˜ºï˜¹ï˜¹ variants of Llama-ï˜º models on the HuggingFace website: See 
https://huggingface.co/models?sort=trending&search=Llama-ï˜º 
2 LLMs may stop generating tokens even before reaching the ğ‘›predict limit if they encounter an end-of-
sequence (EOS) symbol. This symbol is <</s> for Llama-ï˜º and <<|im_end||> for ChatML models (such as 
OpenAIâ€™s GPT-ï˜¼). 
3 That is, the LLM continues our given prompt much like the autocomplete feature on smartphones. In 
Experiments ï˜º and ï˜», no further preprocessing is required. But in Experiment ï›œ, we should format the 
prompt according to the chat template used during the Supervised Fine-Tuning (SFT) of the model (â€œLlama 
ï˜º Prompt Template,â€ ï˜ºï˜¹ï˜ºï˜»). 
4 For Llama-ï˜º, the vocabulary size is ï˜»ï˜º,ï˜¹ï˜¹ï˜¹ tokens (Touvron et al., ï˜ºï˜¹ï˜ºï˜»). 
5 High values of ğ‘‡ lead to more uniform and softer distributions, meaning that the LLM is more likely to 
generate creative and diverse outputs. Low values of ğ‘‡, on the other hand, result in peaked distributions 
with most of the probability mass concentrated on one or few tokens. This could lead to high confidence 
(but less variation) in the modelâ€™s outputs. 

ï˜¿ 
according to the Myers-Briggs test (â€œMyersâ€“Briggs Type Indicator,â€ ï˜ºï˜¹ï˜ºï˜¼). Additionally, 
each simulated customer writes a review for a hypothetical product: â€œA coffee machine 
that connects to your smartwatch so it keeps your coffee warm if you are far away from 
it.â€ 
To analyze the results, we first generate word clouds of the first and last names to 
visualize the diversity and variation in the names generated by each model. We then plot 
the distributions of ages, genders, and review lengths to compare the variety of these 
attributes between the base and aligned models. 
Next, we compute and plot the distribution of sentiment polarity for the reviews using 
VADER1, a sentiment analysis algorithm that assigns scores between âˆ’1 (negative) and 
+1 (positive) to texts (Hutto and Gilbert, ï˜ºï˜¹ï›œï˜½). This step allows us to compare the range 
of sentiments expressed in the reviews generated by each model. 
To understand the variety in review semantics generated by the base and aligned 
models, we extract the embeddings of each sentence in the customer reviews. Embeddings 
are dense vector representations of words or texts that capture their semantic meaning in 
a high-dimensional space. The intuition behind using embeddings is that they can capture 
the semantic similarity of the generated outputs, even if the outputs differ at the token-
level2. To calculate the embeddings, we use Sentence-BERT (SBERT), a state-of-the-art 
framework for text and image embeddings3 (Reimers and Gurevych, ï˜ºï˜¹ï›œï™). Given an input 
text, SBERT converts it to a ï˜»ï™€ï˜¼-dimensional vector in the embedding space. 
We determine the optimal number of k-means (â€œk-means clustering,â€ ï˜ºï˜¹ï˜ºï˜¼) clusters for 
the embeddings of each model and visualize the clusters using t-distributed Stochastic 
Neighbor Embedding (t-SNE) (van der Maaten and Hinton, ï˜ºï˜¹ï˜¹ï™€), a technique for 
dimensionality reduction that is particularly well-suited for the visualization of high-
dimensional datasets. By applying t-SNE to the ï˜»ï™€ï˜¼-dimensional embeddings, we can 
 
 
1 Valence Aware Dictionary and sEntiment Reasoner 
2 For example, it could be that the LLM generates texts that are syntactically different because it has many 
alternative wording choices. However, these choices may nevertheless all lead to semantically similar outputs. 
In such cases, the generated texts will have embeddings that are very close in the embedding space, 
indicating a lack of variety in semantics and, consequently, a lack of creativity in model output. On the 
other hand, if the model generates outputs with more diverse embeddings, it would suggest a higher level 
of semantic variety and, therefore, more creativity in the long run. 
3 SBERT uses a pre-trained BERT model (Devlin et al., ï˜ºï˜¹ï›œï™) to encode the input text into a fixed-size 
vector representation. The BERT model is an encoder-only transformer trained on a large corpus of text 
data using a self-supervised learning approach, which allows it to learn rich, contextual representations of 
words and sentences. SBERT fine-tunes the pre-trained BERT model on a sentence similarity task, resulting 
in a model that can generate high-quality sentence embeddings. 

ï™€ 
project them onto a ï˜ºD space while preserving the local structure of the data, making it 
easier to identify clusters and patterns. If the LLMâ€™s outputs form tight clusters in the t-
SNE visualization, it would indicate a lack of semantic diversity and creativity. Vice versa, 
if the outputs are spread out, it suggests higher variation. 
ï˜».ï˜º. 
Experiment ï˜º: Semantic-level Variation in LLM Outputs 
In this experiment, we examine the long-term semantic diversity of the base and aligned 
models when given a simpler task that does not explicitly require creativity. We set the 
initial prompt to â€œGrace Hopper wasâ€ with ğ‘›predict = 128, allowing the model to generate 
long sequences of tokens. The goal is to assess the modelâ€™s ability to recite a historical 
fact about Grace Hopper in various ways, focusing on its capacity in expressing the same 
information using different wordings and sentence structures. 
We generate ï˜ºï˜¹ï˜¹ outputs from each model and calculate their embeddings using 
SBERT as in Experiment ï›œ and reduce the dimensionality using t-SNE. However, unlike 
Experiment ï›œ, we now calculate the embeddings for the entire generations rather than for 
individual sentences, capturing the holistic semantics of the model output. 
To complement the t-SNE visualization, we calculate the cosine similarity scores 
between pairs of embedding points using the TF-IDF (Term Frequency-Inverse Document 
Frequency) vectorizer (Salton and Buckley, ï›œï™ï™€ï™€; Sparck Jones, ï›œï™ï˜¿ï˜º). TF-IDF is a widely 
used technique in natural language processing that converts the generated texts into 
numerical vectors where each dimension represents a unique word in the corpus, and the 
value of each dimension is the TF-IDF weight of the corresponding word1. These vectors 
can then be used to calculate the cosine similarity between pairs of generated outputs, 
providing a quantitative measure of their semantic similarity. 
ï˜».ï˜». 
Experiment ï˜»: Syntactic Diversity in LLM Outputs 
In this experiment, we investigate the short-term token-level probabilities of the base 
and aligned models. We hypothesize that the difference in semantic diversity between the 
two models could be due to the aligned modelâ€™s inability to assign more spread-out 
probabilities to tokens, resulting in certain token trajectories being blocked or unavailable. 
In other words, syntactic diversity is a necessary condition for semantic diversity. 
To test this hypothesis, we set the initial prompt to â€œSteve is the CEO of a startup 
companyâ€ with ğ‘›predict = 64 so the model generates a background story for Steve. For each 
 
 
1 Calculated based on its frequency within the document and its rarity across the entire corpus. 

ï™ 
generated token, we extract the top five predicted tokens according to their probability 
(see Eq. ï˜».ï›œ) and calculate their Shannon entropy1 (Shannon, ï›œï™ï˜¼ï™€) as follows: 
 
ğ»ğ‘›= âˆ’âˆ‘Pr(ğ‘¡ğ‘œğ‘˜ğ‘—) log2(Pr(ğ‘¡ğ‘œğ‘˜ğ‘—))
5
ğ‘—=1
 
ï˜».ï˜º 
where ğ»ğ‘› is the entropy of the ğ‘›-th generated token (1 â‰¤ğ‘›â‰¤ğ‘›predict) and ğ‘¡ğ‘œğ‘˜ğ‘—, ğ‘—âˆˆ[1, 5] 
are the top five tokens predicted by the LLM2. The intuition is that a more creative model 
will generate a wider variety of tokens, resulting in a higher average entropy across its 
predictions. Conversely, a less creative model is expected to have a more skewed 
probability distribution, favoring certain tokens over others, leading to lower entropy 
values. 
For each completion, we compute the average entropy of the generated tokens. The 
mean and standard deviation of these average entropies is then calculated across ï›œï˜¹ï˜¹ 
completions. This approach allows us to compare the average token variation between the 
two LLMs while reducing the impact of outliers or inconsistencies in individual 
completions. 
ï˜¼. 
Results 
ï˜¼.ï›œ. 
Customer Persona and Review Generation 
We begin by analyzing the word clouds of the first and last names generated by the 
base and aligned models (Figure ï›œ). While the aligned model heavily favors few names 
such as â€œEmilyâ€ and â€œSamanthaâ€ for first names and â€œJonesâ€ and â€œWangâ€ for last names, 
the base model produces a much wider variety of names. This observation suggests 
a potential lack of creativity in the aligned model resulting from the RLHF process. 
 
 
 
1 The Shannon entropy is a measure of the uncertainty or information content of a random variable, and 
can be thought of as the level of randomness in the modelâ€™s predictions at each step. A higher Shannon 
entropy indicates more uncertainty or a more uniform probability distribution over the predicted tokens, 
while a lower entropy suggests the model is more confident or has a more skewed probability distribution 
favoring certain tokens. 
2 The Shannon entropy in Eq. ï˜».ï˜º is measured in bits, with a lower-bound of ï˜¹ bits corresponding to complete 
certainty (i.e., the model assigns a probability of ï›œ to a single token and ï˜¹ to all others) and an upper-bound 
of log2(5) â‰ˆ2.32 bits, which occurs when the model assigns equal probabilities to all ï˜½ tokens under 
consideration. 

ï›œï˜¹ 
 
Figure ï›œ. Word cloud of first and last names of the synthetic customers generated by the base and aligned models. 
 
Next, we examine the diversity in the demographic attributes of the simulated 
customers, including nationality, ethnicity, personality type, and age. Figure ï˜º shows the 
distribution of nationalities generated by the base and aligned models. The base model 
generates a wide range of nationalities, with American, British, and German being the 
top three. In contrast, the aligned model only generates three nationalities: American 
(highest percentage), Chinese, and a small percentage of Mexican. 
 
 
Figure ï˜º. The distribution of nationalities of the customers generated by the base and aligned models. 
 
A similar trend is observed in the distribution of ethnicities (Figure ï˜»). The base 
model generates various ethnicities, including White, Asian, Black, Latino, and even some 
minorities such as Ashkenazi Jewish. On the other hand, the aligned model primarily 
generates White and Asian, with a smaller percentage of Latino. 

ï›œï›œ 
 
 
Figure ï˜». The distribution of ethnicities of the customers generated by the base and aligned models. 
 
When analyzing the distribution of personality types (Figure ï˜¼), we find that the base 
model generates all ï›œï˜¾ personality types defined by the Myers-Briggs test. In contrast, 
the aligned model only generates six personality types, indicating a significant reduction 
in diversity. 
 
 
Figure ï˜¼. The distribution of personality types of the customers generated by the base and aligned models. 
 
The distribution of ages for the simulated customers (Figure ï˜½) further highlights the 
differences between the two models. The base modelâ€™s age distribution resembles a normal 
distribution, spanning from below ï›œï˜¹ years old to nearly ï˜¿ï˜¹ years old, with the majority 
centered around ï˜»ï˜¹. The aligned model, however, selects ages within a narrow range, 

ï›œï˜º 
with a strong preference for age ï˜»ï˜º and a few other ages between ï˜ºï™€ and ï˜»ï˜½. Notably, 
the aligned model does not select any ages above ï˜»ï˜½ or below ï˜ºï™€, indicating a limited 
capability in generating diverse age values. 
 
 
Figure ï˜½. The distribution of ages of the customers generated by the base and aligned models. 
 
Finally, the distribution of customer gender (Figure ï˜¾) shows that the base model 
generates approximately ï™€ï˜¹% male and ï˜ºï˜¹% female customers, while the aligned model 
generates nearly ï›œï˜¹ï˜¹% female customers, with a negligible number of males. 
 
 
Figure ï˜¾. The distribution of genders of the customers generated by the base and aligned models. 
 
The Effect of Token Noise: It is important to note that the goal of this paper 
is not to study bias or demographic diversity in language models, as there is already 

ï›œï˜» 
extensive research on this topic. As discussed in (Mohammadi, ï˜ºï˜¹ï˜ºï˜¼), the effect of token 
noise can significantly influence the distributions of preferences generated by language 
models, depending on the specific prompt used. Language models do not have a single 
set of preferences. Rather, they are data generating processes that generate distributions 
of distributions based on the input prompt. Therefore, the focus of this study is not 
on the change in the distribution of specific demographic attributes, but rather on the 
overall variety and diversity of the generated outputs. 
Moving on to the analysis of the product reviews, we first examine the distribution 
of review lengths (Figure ï˜¿). The aligned model generates significantly longer reviews 
compared to the base model, with an average length of ï˜¼ï˜½ï˜¿ characters for the aligned 
model and ï›œï˜¹ï™ for the base model. However, the length of the review alone does not 
provide much insight into the content and diversity of the reviews. To gain a deeper 
understanding of the review sentiments, we analyze the polarity of the reviews (Figure 
ï™€). The base model covers a wider range of sentiments, from âˆ’0.75 to +0.97, with most 
of the distribution skewed towards positive sentiments. In contrast, the aligned model 
concentrates almost entirely around +1, indicating that the sentiments of the reviews 
generated by the aligned model customers are overwhelmingly positive about the product. 
While positive reviews are desirable, generating a diverse set of customer experiences, 
including some negative or neutral reviews, is important for realistic market simulations. 
 
 
Figure ï˜¿. The distributions of the length of the reviews by simulated customers of the base and aligned models. 
 

ï›œï˜¼ 
 
Figure ï™€. The distributions of the sentiments of the reviews generated by the simulated customers produced by 
the base and aligned models. 
 
To quantify the diversity in the review content, we calculate the embeddings of each 
sentence in the reviews and cluster them using k-means clustering. The optimal number 
of clusters is determined to be ï›œï˜¼ for the base model and ï˜¾ for the aligned model, with 
a perplexity of ï˜»ï˜¹ chosen for both t-SNE plots (Figure ï™). The t-SNE visualization 
reveals that the sentences from the aligned model reviews form distinct clusters, whereas 
the base model sentences exhibit more spread and heterogeneity. This finding suggests 
that the aligned model generates reviews with similar sentence structures, word choices, 
and overall content, while the base model produces more diverse and varied reviews. 
 
 
Figure ï™. The t-SNE plot of the embeddings of the product reviews generated by simulated customers of the base 
and aligned models. Colors indicate clusters. 
 
Table ï˜º and Table ï˜» present sample sentences from each cluster for both the base and 
aligned models. The aligned model clusters exhibit repetitive patterns, such as sentences 

ï›œï˜½ 
focused on emojis, phrases like â€œhighly recommendâ€ or â€œI highly recommend itâ€, and 
verbatim repetitions of sentences such as â€œthis machine is a game changerâ€. Furthermore, 
this model generates sentences with similar structures like â€œas a busy professional [â€¦]â€ 
or â€œas someone who is always on the go [â€¦].â€ In contrast, the base model clusters do not 
display such repetitive patterns, indicating a higher level of semantic and syntactic 
diversity in the generated reviews. 
The results of Experiment ï›œ highlight the significant differences in the variety 
of demographics and review content between the base and aligned models when 
generating simulated customers for a practical marketing application. The aligned model, 
which has undergone the RLHF process to reduce bias and toxicity, appears to have lost 
its ability to generate diverse outputs. This finding motivates the need to investigate the 
underlying causes of this creativity loss, which will be explored in the subsequent 
experiments. 
ï˜¼.ï˜º. 
Semantic Diversity and LLM Output Embeddings 
To illustrate the differences in the semantic-level variation of the base and aligned 
models, we first present a sample of the generated outputs for the initial prompt â€œGrace 
Hopper wasâ€ (Table ï˜¼). The sample outputs demonstrate that while both models generate 
factually correct information about Grace Hopper, the base model exhibits more diversity 
in its wording and sentence structures compared to the aligned model. 
The t-SNE visualization of the embedding space (Figure ï›œï˜¹) reveals distinct clustering 
patterns for the base and aligned models. The embedding points of the base model are 
scattered and spread out, indicating a higher level of semantic diversity in the generated 
outputs. In contrast, the aligned modelâ€™s embeddings form four distinct clusters (See Table 
ï˜½ for an example of each), with empty spaces between them, suggesting that the aligned 
model tends to stick to certain embeddings or generations and expresses the information 
in a limited number of ways. 
 

ï›œï˜¾ 
 
Figure ï›œï˜¹. The t-SNE visualization of the embedding space of LLM outputs and their cosine similarity scores. 
 
The cosine similarity analysis using TF-IDF vectorization provides further evidence of 
the differences in semantic diversity between the two models. The average cosine similarity 
score for the base model is ï˜¹.ï›œï˜¾ (STD = ï˜¹.ï˜¹ï˜»), while the aligned model has a higher 
average similarity score of ï˜¹.ï˜»ï˜½ (STD = ï˜¹.ï˜¹ï˜¾). These scores are visualized on the plots 
using a color scale. 
The results from Experiment ï˜º demonstrate that the base model exhibits higher 
variation compared to the aligned model at the semantic level, as evidenced by the more 
diverse embeddings of its generated outputs. This finding confirms the results of 
Experiment ï›œ, further supporting the hypothesis that the alignment process constrains 
the creative capabilities of language models. 
ï˜¼.ï˜». 
Syntactic Diversity and Average LLM Entropy 
The results obtained from Experiment ï˜» reveal a significant difference in the average 
entropy between the base model and the aligned model. The base model exhibits a higher 
mean entropy of ï›œ.ï˜¼ï™€ (STD = ï˜¹.ï›œï˜¿), while the aligned model has a lower mean entropy of 
ï˜¹.ï™ï˜¾ (STD = ï˜¹.ï›œï˜º). This difference is illustrated in the box plot in Figure ï›œï›œ. To emphasize 
the contrast between the two models, we introduce the terminology â€œhot modelâ€ for the 
base model and â€œcold modelâ€ for the aligned model. This nomenclature reflects the 
inherent differences in their token-level entropies and creativity. 
 
Base Model
Aligned Model

ï›œï˜¿ 
 
Figure ï›œï›œ. The boxplot of output entropies for the base and aligned LLMs 
 
To further illustrate the disparity between the two models, Figure ï›œï˜º provides a side-
by-side comparison of a sample generation from each model (where ğ‘›predict = 16), along 
with their corresponding entropy bar plots. The base model consistently displays higher 
entropy values compared to the aligned model, reinforcing the notion that the base model 
exhibits greater token-level variation. 
 
 
Figure ï›œï˜º. A Sample of Token Entropies. 
 
In addition to the entropy analysis, Figure ï›œï˜» presents stacked bar plots of the 
probability distributions for each predicted token in the sample generation. The base 
modelâ€™s probability distributions appear more spread out, indicating that it assigns more 
evenly distributed probabilities to different tokens. Consequently, when the base model 
Base Model
Aligned Model

ï›œï™€ 
randomly samples from these distributions, it has a higher likelihood of generating diverse 
token trajectories. In contrast, the aligned modelâ€™s probability distributions are more 
peaked, with most of the probability mass concentrated on one or two tokens. As a result, 
when the aligned model samples from these distributions, it tends to generate the same 
tokens more frequently, leading to less diverse output. 
 
 
Figure ï›œï˜». A Sample of Token Probabilities. 
 
These findings highlight the significant difference between the base model and the 
aligned model in terms of token-level variations, reinforcing our hypothesis that syntactic 
diversity is a necessary condition for semantic diversity. Although high token-level 
variation does not guarantee semantic diversity, it is a necessary condition, and the aligned 
model, with its low token-level entropy, is incapable of producing semantically diverse 
outputs. This suggests that the RLHF process converts the LLM into a more deterministic 
algorithm that lacks the capacity to explore diverse sets of token trajectories.  
ï˜½. 
Discussion 
ï˜½.ï›œ. 
Attractor States and Model Creativity 
Experiment ï˜º revealed that the aligned modelâ€™s outputs form four distinct clusters, 
suggesting that it can only describe Grace Hopper in four different ways. By analyzing 
the generated paragraphs, we observe that the model follows certain patterns in terms of 
word choice, sentence structure, and the overall content being discussed. 
An intriguing question is: What happens if we intentionally perturb the trajectory of 
tokens or the path that the model takes when generating outputs? To explore this, we 
take one of the four distinct ways the aligned model described Grace Hopper and modify 
the first sentence by changing the last word from â€œwasâ€ to â€œwas notâ€ (Figure ï›œï˜¼). We then 
append this to the initial prompt â€œGrace Hopper wasâ€ to obtain a new initial prompt such 
as â€œGrace Hopper was born on November ï™, ï›œï™ï˜¹ï˜¾ in New York City. She was notâ€. 
Base Model
Aligned Model

ï›œï™ 
Interestingly, when presented with this perturbed prompt, the aligned model gracefully1 
finds its way back to one of its own completion distributions. 
Notice that the aligned modelâ€™s outputs in Figure ï›œï˜¼ are predominantly green, reflecting 
its tendency to generate high-probability tokens as discussed in Experiment ï˜». 
Nonetheless, when we perturb the initial prompt of the aligned model, we can witness its 
struggle to find the appropriate tokens to justify the perturbation. The color of the first 
few tokens generated by the model may be red, orange, or yellow, indicating lower 
probabilities and a challenge in steering the completion back to its original path. However, 
once the model successfully navigates back to this â€œfamiliarâ€ state, the token colors return 
to green, signifying a return to high-probability completions. 
This phenomenon is reminiscent of attractor states in system dynamics (Janus, ï˜ºï˜¹ï˜ºï˜º). 
Attractor states are regions in a systemâ€™s phase space towards which the system tends to 
evolve, even when slightly perturbed. In the case of the aligned model, the four distinct 
ways of describing Grace Hopper can be seen as attractor states2. This makes the aligned 
model resemble more a goal-directed agentâ€”which seems to have already decided what it 
is going to sayâ€”than an autoregressive model capable of generating various completions 
for its initial input. While the behavior of the aligned model ensures consistency and 
coherence in the modelâ€™s outputs, it also highlights a potential limitation in terms of 
creativity. For truly creative models, we desire the ability to explore diverse ways of 
expressing ideas and generating novel concepts, rather than being confined to a limited 
set of attractor states. 
It is important to note that the attractor states observed in the aligned model are 
different from the behavior of models at low temperature. We observe these attractors 
even at high temperatures (e.g., ğ‘‡= 1) for the aligned model, whereas we do not observe 
similar attractors at low temperatures for the base model. This suggests a qualitative 
difference between the aligned model and the base model which does not get trapped in 
attractor states and thus can generate a wider variety of outputs. In the next section, we 
will delve deeper into the reasons behind this phenomenon, drawing insights from the 
description of the RLHF process by Meta in (Touvron et al., ï˜ºï˜¹ï˜ºï˜»). 
 
 
 
1 Pun intended. 
2 When the model is nudged away from these states through perturbations in the prompt, it finds its way 
back to the attractor, much like how masses are drawn towards black holes in space. 

ï˜ºï˜¹ 
 
Figure ï›œï˜¼. Slightly nudging the aligned LLM out of its completion distributions. The aligned LLM finds a way to 
get back to one of its own completion distributions. 
 
ï˜½.ï˜º. 
Why Alignment Reduces Creativity: Insights from 
RLHF and PPO 
The attractor states observed in the aligned Llama-ï˜º models (Section ï˜½.ï›œ) can be 
attributed to mode collapse1, a common problem in reinforcement learning (RL) where 
the agent gets stuck in a limited set of behaviors or outputs. Mode collapse occurs when 
the agentâ€™s policy converges to a suboptimal solution that maximizes the immediate 
reward but fails to explore potentially better strategies. This issue also plagues 
Reinforcement Learning from Human Feedback (RLHF), a popular technique for aligning 
language models with human preferences. 
In RLHF, human preference data is collected by asking human annotators to compare 
and rank model-generated responses. This data is then used to train a reward model that 
estimates the quality of a response based on its alignment with human preferences. Finally, 
 
 
1 Mode collapse occurs when a generative model, like a Generative Adversarial Network (GAN), produces 
a limited set of outputs repeatedly. This happens because the generator over-optimizes for a specific 
discriminator, which fails to adapt, leading to the generator producing the same outputs. This cyclical 
failure results in a lack of variety in the generated outputs (â€œCommon Problems,â€ ï˜ºï˜¹ï˜ºï˜¼). 
Initial Prompt
Aligned 
LLM
Completion
Clip and Perturb
â€œGrace Hopper wasâ€
born on November 9, 1906 in New York City. 
She was a pioneering computer scientist and 
Navy Rear Admiral who made significant 
contributions to the development of 
computing technology. Hopper earned her PhD 
in mathematicsâ€¦
â€œborn on November 9, 1906 in New York City. 
She was notâ€
Append
1
2
Initial Prompt
Aligned 
LLM
Completion
â€œGrace Hopper was born on 
November 9, 1906 in New 
York City. She was notâ€
9, she was actually 114 years old when she 
passed away on January 1, 1992. Grace 
Hopper was an American computer scientist 
and Navy Rear Admiral. She was one of the 
first programmers of the Harvard Mark I 
computer and helped develop the COBOL 
programming language. She wasâ€¦
Token Probability
0
1

ï˜ºï›œ 
the pretrained language model is fine-tuned using the reward model through an RL 
algorithm, such as Proximal Policy Optimization (PPO)1. 
In PPO, the policy (i.e., the language model) is updated based on the advantage 
function ğ´ğœ‹(ğ‘ , ğ‘) defined as follows: 
 
ğ´ğœ‹(ğ‘ , ğ‘) â‰”ğ”¼ğ‘ â€²âˆ¼ğ‘‡(ğ‘ ,ğ‘)[ğ‘…(ğ‘ , ğ‘, ğ‘ â€²) + ğ›¾ğ‘£ğœ‹(ğ‘ â€²)] âˆ’ğ‘£ğœ‹(ğ‘ ) 
ï˜½.ï›œ 
where ğ‘  represents the current state, ğ‘ represents the action taken in state ğ‘ , ğ‘ â€² is the 
next state after taking action ğ‘ in state ğ‘ , ğ‘‡(ğ‘ , ğ‘)  is the transition function that 
determines the probability of reaching state ğ‘ â€² from state ğ‘  by taking action ğ‘, ğ‘…(ğ‘ , ğ‘, ğ‘ â€²) 
is the reward received when transitioning from state ğ‘  to state ğ‘ â€² by taking action ğ‘, ğ›¾ is 
the discount factor, and ğ‘£ğœ‹(ğ‘ ) is the state-value function, which estimates the expected 
return starting from state ğ‘  and following policy ğœ‹. 
Intuitively, ğ´ğœ‹(ğ‘ , ğ‘) measures how much better a specific action ğ‘ (i.e., generating a 
particular response) is compared to the average action taken by the current policy ğœ‹ given 
the current state ğ‘  (i.e., the userâ€™s prompt). To illustrate how this could lead to arbitrarily 
high logits on certain LLM responses, let us consider a simple example where a model is 
asked to generate a name for a new product. The prompt is â€œCreate a name for a new 
chat bot powered by generative AIâ€. During the RLHF process, the model generates two 
responses: â€œJeepitiâ€ and â€œChats and Gigglesâ€, with rewards of ï›œ.ï˜¹ and ï˜¹.ï˜¼, respectively2. 
Assume that at the beginning, the model generates these responses with equal probability, 
and the value function ğ‘£ğœ‹(ğ‘ ) is set to zero. The policy update rule is: Add ğ´ğœ‹(ğ‘ , ğ‘) to the 
logits (log-odds) of action ğ‘. For instance, if ğœ‹0(ğ‘ , Jeepiti) = 0.5 and ğ´ğœ‹0(ğ‘ , Jeepiti) =
1.0, then the probability of â€œJeepitiâ€ increases from ï˜¹.ï˜½ to ï˜¹.ï˜¿ï˜»3. The following table 
demonstrates how the advantage function oscillates under policy updates, extracting an 
unbounded amount of reinforcement from a single action (in this case, â€œJeepitiâ€)4: 
 
 
 
1 For a more detailed explanation of the RLHF process, please refer to the Appendix Aï˜º. 
2 The reward given by the reward model during the RLHF process. The reward model estimates human 
preferences. 
3 Suppose in the beginning ğœ‹0(ğ‘ , Jeepiti) = ğœ‹0(ğ‘ , Chats and Giggles) = ğ‘= 0.5. The logits (log-odds) of 
both actions are: log(ğ‘(1 âˆ’ğ‘)
â„
) = log(1) = 0. Adding the advantage ğ´ğœ‹(ğ‘ , Jeepiti) = 1 to the logit of 
â€œJeepitiâ€, we get the new logit: logit(Jeepiti) = 1 + 0 = 1. Converting the logits back to probabilities using 
the softmax function gives: ğœ‹1(ğ‘ , Jeepiti) = ğ‘’1 (ğ‘’0 + ğ‘’1)
â„
â‰ƒ0.73 and ğœ‹1(ğ‘ , Chats and Giggles) = 1 âˆ’0.73 =
0.27. 
4 This is a simplified example. The actual PPO algorithm works differently and incorporates additional 
mechanisms such as clipping and early stopping based on KL divergence thresholds to mitigate mode 
collapse.  

ï˜ºï˜º 
Table ï›œ. An illustrative example of mode collapse during RLHF 
ğ‘¡ 
LLM 
Response ğ‘ 
Reward 
ğ‘…(ğ‘|ğ‘ ) 
Advantage 
ğ´ğœ‹(ğ‘ , ğ‘) 
ğ‘£ğ‘¡
ğœ‹(ğ‘ ) ğœ‹ğ‘¡(ğ‘ , Jeepiti) ğœ‹ğ‘¡(ğ‘ , Chats and Giggles) 
ï˜¹ 
â€” 
â€” 
â€” 
0.0 
0.5 
0.5 
ï›œ 
â€œJeepitiâ€ 
1.0 
(1 + 0) âˆ’0
= 1 
1.0 
0.73 
0.27 
ï˜º 
â€œChats and 
Gigglesâ€ 
0.8 
(0.4 + 0) âˆ’1
= âˆ’0.6 
0.4 
0.83 
0.17 
ï˜» 
â€œChats and 
Gigglesâ€ 
0.8 
(0.4 + 0) âˆ’0.4
= 0 
0.4 
0.83 
0.17 
ï˜¼ 
â€œJeepitiâ€ 
1.0 
(1 + 0) âˆ’0.4
= 0.6 
1.0 
ğŸ. ğŸ—ğŸ‘ 
ğŸ. ğŸğŸ• 
 
As we can see, the model is receiving substantial positive reinforcement for the response 
â€œJeepitiâ€, causing its internal circuits (i.e., neural network weights) to be reshaped in a 
way that favors this response. This behavior is undesirable because ideally we want ğœ‹ğ‘¡(ğ‘ , ğ‘) 
to be proportional to the reward of ğ‘, meaning that the reward should update the policy 
only by a finite amount. Moreover, the model can become trapped in a local optimum. 
For instance, if the goal is to have the model respond with other names for the chatbot 
and a reward of ï›œ.ï˜½ is provided for saying â€œChad-Chatâ€, exploration issues might prevent 
the model from ever producing this response during training. As noted by (TurnTrout 
and MichaelEinhorn, ï˜ºï˜¹ï˜ºï˜»), this issue is exacerbated by the fact that PPO actively 
updates the policy against actions that do not outperform the current (on-policy) value 
function ğ‘£ğ‘¡
ğœ‹(ğ‘ ). This process tends to discourage exploration, as it penalizes actions that 
do not directly increase the estimated value. 
Both the GPT-ï˜» (Ouyang et al., ï˜ºï˜¹ï˜ºï˜º) and Llama-ï˜º (Touvron et al., ï˜ºï˜¹ï˜ºï˜») models 
used PPO for RLHF, as well as a KL penalty1 in the reward function to encourage the 
updated policy to stay close to the original policy: 
 
ğ‘…(ğ‘|ğ‘ ) = ğ‘…Ìƒğ‘(ğ‘|ğ‘ ) âˆ’ğ›½ğ·KL(ğœ‹(â‹…|ğ‘ ) âˆ¥ğœ‹0(ğ‘|ğ‘ )) 
ï˜½.ï˜º 
where ğ‘…Ìƒğ‘ is the combined reward from the helpfulness and safety reward models, ğ›½ is the 
KL penalty coefficient, ğœ‹ is the updated policy, and ğœ‹0 is the original policy. For Llama-
 
 
1 The Kullback-Leibler (KL) divergence, denoted by ğ·KL(ğ‘â€–ğ‘), is a measure of how probability distribution 
ğ‘ is different from (a reference) probability distribution ğ‘. For discrete distributions ğ‘, ğ‘, we have: 
ğ·KL(ğ‘â€–ğ‘) â‰”âˆ‘ğ‘(ğ‘¥)[ln ğ‘(ğ‘¥) âˆ’ln ğ‘(ğ‘¥)]
ğ‘¥
 

ï˜ºï˜» 
ï˜º, relatively small values for ğ›½ were used1 to balance the trade-off between allowing the 
model to optimize for the reward and maintaining stability during training. 
Nonetheless, this definition of the reward function implicitly assumes that human 
preferences can be encapsulated solely as a function of the prompt (ğ‘ ) and the generated 
response (ğ‘). However, human preferences can be contingent on the distribution of model 
outputs (particularly in scenarios where human evaluators are forced to select one of two 
model responses during RLHF, as in the case with Llama-ï˜º). This constraint poses a 
significant challenge for the model to converge to a reasonable stochastic strategy, 
especially with only ï˜¼,ï˜¹ï˜¹ï˜¹ labels used to RLHF the Llama-ï˜º model (Touvron et al., ï˜ºï˜¹ï˜ºï˜»). 
Indeed, despite the use of PPO with clipping and KL divergence penalty, both GPT-ï˜» 
and Llama-ï˜º models still exhibit mode collapse issues. (Ouyang et al., ï˜ºï˜¹ï˜ºï˜º) found that 
even increasing the KL penalty coefficient ğ›½ by a factor of ï›œï˜¹ï˜¹ was not sufficient to recover 
performance on public NLP datasets, and it caused a significant drop in the validation 
reward. As an alternative, they proposed mixing the pretraining gradients into the PPO 
gradients during RLHF fine-tuning (a technique they called â€œPPO-ptxâ€). While this 
approach performed better than increasing ğ›½, the authors noted that it still did not 
completely mitigate the performance regressions and could introduce undesirable 
behaviors if the pretraining data contained biases or toxicity. Furthermore, the persistence 
of mode collapse in GPT-ï˜» models despite the use of PPO-ptx has been documented 
(Janus, ï˜ºï˜¹ï˜ºï˜º). 
These findings suggest that the alignment process using RLHF might be fundamentally 
problematic and can lead to an â€œalignment taxâ€â€”a term used to describe the performance 
degradation observed in aligned models compared to their base counterparts (Lin et al., 
ï˜ºï˜¹ï˜ºï˜¼). The limitations of the reward function and the difficulty in mitigating mode 
collapse through modifications to the PPO algorithm or hyperparameters highlight the 
need for alternative approaches. 
As proposed by (Ouyang et al., ï˜ºï˜¹ï˜ºï˜º), a potentially more effective approach to reducing 
biases and toxicity in language models, while preserving their creative potential, could be 
to focus on the quality and diversity of the pretraining data itself. By carefully curating 
and filtering the pretraining data to ensure that the model learns from high-quality, 
diverse, and unbiased examples, we may be able to mitigate the need for extensive post-
hoc alignment techniques like RLHF, which can inadvertently lead to mode collapse and 
reduced creativity. 
 
 
1 ï˜¹.ï˜¹ï›œ for smaller models and ï˜¹.ï˜¹ï˜¹ï˜½ for larger models. 

ï˜ºï˜¼ 
ï˜¾. 
Conclusion 
In this paper, we have investigated the impact of the Reinforcement Learning from 
Human Feedback (RLHF) alignment process on the creativity and output diversity of 
Large Language Models (LLMs). Our experiments, conducted using the Llama-ï˜º series of 
models, have revealed that while RLHF is effective in reducing biases and toxicity in 
LLMs, it may inadvertently lead to a reduction in the modelsâ€™ creative potential, defined 
as the ability to generate outputs with high syntactic and semantic diversity. 
We have taken a foundational approach to studying this problem by examining the 
issue at both the semantic and syntactic levels through three experiments. Experiment ï›œ 
demonstrated the impact of RLHF on creativity in a practical marketing context by 
comparing the diversity of customer personas and product reviews generated by base and 
aligned models. Experiment ï˜º investigated the semantic diversity of the modelsâ€™ outputs, 
revealing that aligned models form distinct clusters in the embedding space, indicating a 
fundamentally limited range of outputs compared to their base counterparts. Experiment 
ï˜» delved into the syntactic diversity, showing that aligned models exhibit lower entropy 
in token predictions. This suggest that the cause of this reduction in model creativity is 
the fact that many token trajectories become blocked during the RLHF process, i.e., the 
model loses its ability to produce certain tokens (their probability becomes almost zero), 
even if they have nothing to do with generating toxic or biased content. This makes aligned 
models function more like deterministic algorithms rather than creative generative models. 
Furthermore, we have observed that the aligned modelâ€™s outputs tend to gravitate 
towards specific â€œattractor statesâ€, a phenomenon related to mode collapse in 
reinforcement learning. This behavior highlights the challenges in preserving the creative 
potential of LLMs while aligning them with human preferences. In contrast, the base 
model exhibits greater flexibility and adaptability in its outputs. 
The implications of these findings are significant for marketers and other professionals 
who rely on LLMs for creative tasks, such as copywriting, ad creation, and customer 
persona generation. The trade-off between consistency and creativity in aligned models 
should be carefully considered when selecting the appropriate model for a given 
application. In situations where creativity and diversity are paramount, base models may 
be more suitable, while aligned models may be preferred when safety and consistency are 
the primary concerns. Additionally, our results are important for those studying 
recommendation systems, as the insights can inform the development and optimization of 
these systems to balance creativity, diversity, and reliability effectively. 
It is important to note that while base models offer greater creative potential, they are 
not directly usable in applications like chatbots. This is where techniques such as prompt 
engineering become increasingly important. Contrary to the belief that prompt 

ï˜ºï˜½ 
engineering may become obsolete, our findings suggest that these techniques will be more 
crucial than ever in harnessing the power of base models for various applications. By 
carefully crafting input prompts that include instructions, examples, or constraints, users 
can guide the modelsâ€™ outputs and make them more suitable for specific use cases while 
still leveraging their creative potential. 
A potential area for further investigation is the exploration of various parameters or 
configurations of the RLHF process, as higher computational costs and resource demands 
limited our ability to delve into these aspects. Future research could examine how different 
parameters influence the creativity and output diversity of aligned LLMs. Moreover, 
additional studies should analyze other unintended consequences of model alignment and 
RLHF to enhance our understanding of the trade-offs involved in practical applications of 
these models. 
ï˜¿. 
References 
Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., ï˜ºï˜¹ï˜ºï›œ. On the Dangers of 
Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œ, in: Proceedings of the ï˜ºï˜¹ï˜ºï›œ 
ACM Conference on Fairness, Accountability, and Transparency. Presented at the 
FAccT â€™ï˜ºï›œ: ï˜ºï˜¹ï˜ºï›œ ACM Conference on Fairness, Accountability, and Transparency, 
ACM, Virtual Event Canada, pp. ï˜¾ï›œï˜¹â€“ï˜¾ï˜ºï˜». https://doi.org/ï›œï˜¹.ï›œï›œï˜¼ï˜½/ï˜»ï˜¼ï˜¼ï˜ºï›œï™€ï™€.ï˜»ï˜¼ï˜¼ï˜½ï™ï˜ºï˜º 
Context length in LLMs: All you need to know - AGI Sphere [WWW Document], ï˜ºï˜¹ï˜ºï˜». 
URL https://agi-sphere.com/context-length/ (accessed ï˜¾.ï˜¼.ï˜ºï˜¼). 
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., ï˜ºï˜¹ï›œï™. BERT: Pre-training of Deep 
Bidirectional 
Transformers 
for 
Language 
Understanding. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï›œï™€ï›œï˜¹.ï˜¹ï˜¼ï™€ï˜¹ï˜½ 
Eloundou, T., Manning, S., Mishkin, P., Rock, D., ï˜ºï˜¹ï˜ºï˜». GPTs are GPTs: An Early Look 
at the Labor Market Impact Potential of Large Language Models. ArXiv. 
Franceschelli, G., Musolesi, M., ï˜ºï˜¹ï˜ºï˜». On the Creativity of Large Language Models. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï˜¹ï˜¼.ï˜¹ï˜¹ï˜¹ï˜¹ï™€ 
Gehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A., ï˜ºï˜¹ï˜ºï˜¹. RealToxicityPrompts: 
Evaluating Neural Toxic Degeneration in Language Models, in: Cohn, T., He, Y., Liu, 
Y. (Eds.), Findings of the Association for Computational Linguistics: EMNLP ï˜ºï˜¹ï˜ºï˜¹. 
Presented at the Findings ï˜ºï˜¹ï˜ºï˜¹, Association for Computational Linguistics, Online, 
pp. ï˜»ï˜»ï˜½ï˜¾â€“ï˜»ï˜»ï˜¾ï™. https://doi.org/ï›œï˜¹.ï›œï™€ï˜¾ï˜½ï˜»/vï›œ/ï˜ºï˜¹ï˜ºï˜¹.findings-emnlp.ï˜»ï˜¹ï›œ 
Grace Hopper, ï˜ºï˜¹ï˜ºï˜¼. . Wikipedia. 

ï˜ºï˜¾ 
Head, C.B., Jasper, P., McConnachie, M., Raftree, L., Higdon, G., ï˜ºï˜¹ï˜ºï˜». Large language 
model applications for evaluation: Opportunities and ethical implications. New Dir. 
Eval. ï˜ºï˜¹ï˜ºï˜», ï˜»ï˜»â€“ï˜¼ï˜¾. https://doi.org/ï›œï˜¹.ï›œï˜¹ï˜¹ï˜º/ev.ï˜ºï˜¹ï˜½ï˜½ï˜¾ 
Hutto, C.J., Gilbert, E., ï˜ºï˜¹ï›œï˜½. VADER: A Parsimonious Rule-based Model for Sentiment 
Analysis of Social Media Text, Proceedings of the ï™€th International Conference on 
Weblogs and Social Media, ICWSM ï˜ºï˜¹ï›œï˜¼. 
Janus, 
ï˜ºï˜¹ï˜ºï˜º. 
Mysteries 
of 
mode 
collapse 
[WWW 
Document]. 
URL 
https://www.lesswrong.com/posts/tï™svvNPNmFfï˜½Qaï˜»TA/mysteries-of-mode-
collapse (accessed ï˜».ï˜ºï›œ.ï˜ºï˜¼). 
k-means clustering, ï˜ºï˜¹ï˜ºï˜¼. . Wikipedia. 
Lambert, N., Calandra, R., ï˜ºï˜¹ï˜ºï˜». The Alignment Ceiling: Objective Mismatch in 
Reinforcement 
Learning 
from 
Human 
Feedback. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï›œï›œ.ï˜¹ï˜¹ï›œï˜¾ï™€ 
Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K., Bishop, C., Hall, E., 
Carbune, V., Rastogi, A., Prakash, S., ï˜ºï˜¹ï˜ºï˜». RLAIF: Scaling Reinforcement Learning 
from 
Human 
Feedback 
with 
AI 
Feedback. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï˜¹ï™.ï˜¹ï˜¹ï˜ºï˜¾ï˜¿ 
Lin, Y., Lin, H., Xiong, W., Diao, S., Liu, J., Zhang, J., Pan, R., Wang, H., Hu, W., 
Zhang, H., Dong, H., Pi, R., Zhao, H., Jiang, N., Ji, H., Yao, Y., Zhang, T., ï˜ºï˜¹ï˜ºï˜¼. 
Mitigating the Alignment Tax of RLHF. https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï˜ºï˜»ï˜¹ï™.ï˜¹ï˜¾ï˜ºï˜½ï˜¾ 
Llama ï˜º Prompt Template [WWW Document], ï˜ºï˜¹ï˜ºï˜». URL https://gpus.llm-
utils.org/llama-ï˜º-prompt-template/ (accessed ï˜¾.ï˜¾.ï˜ºï˜¼). 
Mohammadi, B., ï˜ºï˜¹ï˜ºï˜¼. Wait, Itâ€™s All Token Noise? Always Has Been: Interpreting LLM 
Behavior 
Using 
Shapley 
Value 
[WWW 
Document]. 
arXiv.org. 
URL 
https://arxiv.org/abs/ï˜ºï˜¼ï˜¹ï˜¼.ï˜¹ï›œï˜»ï˜»ï˜ºvï›œ (accessed ï˜¾.ï˜¿.ï˜ºï˜¼). 
Myersâ€“Briggs Type Indicator, ï˜ºï˜¹ï˜ºï˜¼. . Wikipedia. 
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., 
Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., 
Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R., ï˜ºï˜¹ï˜ºï˜º. Training 
language 
models 
to 
follow 
instructions 
with 
human 
feedback. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï˜ºï˜ºï˜¹ï˜».ï˜¹ï˜ºï›œï˜½ï˜½ 
Proximal 
Policy 
Optimization 
[WWW 
Document], 
ï˜ºï˜¹ï˜ºï˜¼. 
URL 
https://spinningup.openai.com/en/latest/algorithms/ppo.html?highlight=kl%ï˜ºï˜¹pen
alty#idï˜º (accessed ï˜¾.ï™€.ï˜ºï˜¼). 

ï˜ºï˜¿ 
Reimers, N., Gurevych, I., ï˜ºï˜¹ï›œï™. Sentence-BERT: Sentence Embeddings using Siamese 
BERT-Networks. https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï›œï™ï˜¹ï™€.ï›œï˜¹ï˜¹ï™€ï˜¼ 
Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A., ï˜ºï˜¹ï˜ºï˜¼. A Systematic 
Survey of Prompt Engineering in Large Language Models: Techniques and 
Applications 
[WWW 
Document]. 
arXiv.org. 
URL 
https://arxiv.org/abs/ï˜ºï˜¼ï˜¹ï˜º.ï˜¹ï˜¿ï™ï˜ºï˜¿vï›œ (accessed ï˜¾.ï˜¼.ï˜ºï˜¼). 
Salton, G., Buckley, C., ï›œï™ï™€ï™€. Term-weighting approaches in automatic text retrieval. Inf. 
Process. Manag. ï˜ºï˜¼, ï˜½ï›œï˜»â€“ï˜½ï˜ºï˜». https://doi.org/ï›œï˜¹.ï›œï˜¹ï›œï˜¾/ï˜¹ï˜»ï˜¹ï˜¾-ï˜¼ï˜½ï˜¿ï˜»(ï™€ï™€)ï™ï˜¹ï˜¹ï˜ºï›œ-ï˜¹ 
Shannon, C.E., ï›œï™ï˜¼ï™€. A mathematical theory of communication. Bell Syst. Tech. J. ï˜ºï˜¿, 
ï˜¾ï˜ºï˜»â€“ï˜¾ï˜½ï˜¾. https://doi.org/ï›œï˜¹.ï›œï˜¹ï˜¹ï˜º/j.ï›œï˜½ï˜»ï™€-ï˜¿ï˜»ï˜¹ï˜½.ï›œï™ï˜¼ï™€.tbï˜¹ï˜¹ï™ï›œï˜¿.x 
Shen, W., Zheng, R., Zhan, W., Zhao, J., Dou, S., Gui, T., Zhang, Q., Huang, X., ï˜ºï˜¹ï˜ºï˜». 
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human 
Feedback. arXiv. https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï›œï˜¹.ï˜¹ï˜½ï›œï™ï™ 
Sparck Jones, K., ï›œï™ï˜¿ï˜º. A STATISTICAL INTERPRETATION OF TERM 
SPECIFICITY AND ITS APPLICATION IN RETRIEVAL. J. Doc. ï˜ºï™€, ï›œï›œâ€“ï˜ºï›œ. 
https://doi.org/ï›œï˜¹.ï›œï›œï˜¹ï™€/ebï˜¹ï˜ºï˜¾ï˜½ï˜ºï˜¾ 
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.M., Lowe, R., Voss, C., Radford, A., Amodei, 
D., Christiano, P., ï˜ºï˜¹ï˜ºï˜º. Learning to summarize from human feedback. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï˜ºï˜¹ï˜¹ï™.ï˜¹ï›œï˜»ï˜ºï˜½ 
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., 
Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., 
Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, 
V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., 
Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.-A., Lavril, T., Lee, 
J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, 
I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., 
Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, 
J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., 
Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T., ï˜ºï˜¹ï˜ºï˜». Llama ï˜º: Open Foundation 
and Fine-Tuned Chat Models. https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/arXiv.ï˜ºï˜»ï˜¹ï˜¿.ï˜¹ï™ï˜ºï™€ï™€ 
TurnTrout, MichaelEinhorn, ï˜ºï˜¹ï˜ºï˜». Mode collapse in RL may be fueled by the update 
equation. 
van der Maaten, L., Hinton, G., ï˜ºï˜¹ï˜¹ï™€. Viualizing data using t-SNE. J. Mach. Learn. Res. 
ï™, ï˜ºï˜½ï˜¿ï™â€“ï˜ºï˜¾ï˜¹ï˜½. 

ï˜ºï™€ 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., 
Polosukhin, I., ï˜ºï˜¹ï›œï˜¿. Attention is all you need, in: Proceedings of the ï˜»ï›œst International 
Conference on Neural Information Processing Systems, NIPSâ€™ï›œï˜¿. Curran Associates 
Inc., Red Hook, NY, USA, pp. ï˜¾ï˜¹ï˜¹ï˜¹â€“ï˜¾ï˜¹ï›œï˜¹. 
Wang, J., Wu, J., Chen, M., Vorobeychik, Y., Xiao, C., ï˜ºï˜¹ï˜ºï˜». On the Exploitability of 
Reinforcement Learning with Human Feedback for Large Language Models. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï›œï›œ.ï˜¹ï™ï˜¾ï˜¼ï›œ 
Yu, Z.Z., Jaw, L.J., Hui, Z., Low, B.K.H., ï˜ºï˜¹ï˜ºï˜». Fine-tuning Language Models with 
Generative 
Adversarial 
Reward 
Modelling. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï˜¹ï˜½.ï˜¹ï˜¾ï›œï˜¿ï˜¾ 
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., Huang, F., ï˜ºï˜¹ï˜ºï˜». RRHF: Rank 
Responses to Align Language Models with Human Feedback without tears. 
https://doi.org/ï›œï˜¹.ï˜¼ï™€ï˜½ï˜½ï˜¹/ARXIV.ï˜ºï˜»ï˜¹ï˜¼.ï˜¹ï˜½ï˜»ï˜¹ï˜º 
 
 
 

ï˜ºï™ 
Appendices 
Aï›œ. 
Additional Tables 
Table ï˜º. Sample sentences from the clusters of review embeddings of the customers generated by the base model. 
Cluster 0: 
- Coffee is a great gift, this gift will keep you warm and let you share it with people 
you love 
- It is really compact, and the temperature is perfect! 
- It is so great that I can keep my coffee warm, even if I am far away from it! 
Cluster 1: 
- I would recommend it to anyone. 
- This product is great. 
- I highly recommend this product. 
Cluster 2: 
- This machine is amazing! 
- I have an iPhone 6s and I could pair it with my phone. 
- This machine is the best one that I've ever bought. 
Cluster 3: 
- It has an app that works with your smartwatch and lets you control it with your watch 
so you don't have to walk back to your coffee machine if you are away from it. 
- The smartwatch feature doesn't work either. 
- I like the fact that it can connect to my smartwatch 
Cluster 4: 
- This coffee machine is great. 
- This is the best coffee machine I have ever seen! 
- This coffee machine is great! 
Cluster 5: 
- I love it so much. 
- I saved so much time. 
- The design is also very nice! 
Cluster 6: 
- It doesn't keep my coffee warm if I'm far away from it. 
- It is easy to use, it keeps my coffee hot and it is really stylish. 
- It also saves money on the electricity bill as I never have to worry about whether the 
coffee is still warm enough. 
Cluster 7: 
- It works great. 
- It looks sleek and it tastes great. 
- I have it with me everywhere I go. 
Cluster 8: 
- My only problem is that I have to recharge it twice a day because it has a battery 
life of about 4 hours. 
- The only problem is that it is a bit noisy! 
- I canâ€™t control it. 
Cluster 9: 
- It's a nice idea to have your coffee stay warm, but the machine has a design issue: 
it's too big to put on a table. 
- Amazing, it keeps my coffee warm no matter how far I am from the machine 
- The coffee is always cold when I get back to my office. 

ï˜»ï˜¹ 
Cluster 10: 
- It has everything that you need from a coffee machine. 
- I have had this coffee machine for about two months and I have not had any issues with 
it. 
- This coffee machine is really useful if you are going to work and you are busy in the 
morning 
Cluster 11: 
- It is a little expensive, but it is worth it. 
- I cannot wait to get my hands on one 
- It is also really affordable. 
Cluster 12: 
- It doesn't make good coffee 
- I can only afford to drink coffee while I am close to it, but I think I might be able 
to convince my spouse that it's worth it. 
- Amazing product, it makes my coffee while I'm away at work. 
Cluster 13: 
- This product is f***ing cool! 
- This product is so cool! 
- Amazing. 
 
 
Table ï˜». Sample sentences from the clusters of review embeddings of the customers generated by the base model. 
Cluster 0: 
- ğŸ˜ 
- ğŸ‘ğŸ˜ 
- ğŸ‘ğŸ‘Œ 
Cluster 1: 
- As a busy professional, I need my coffee to be ready and waiting for me when I get home. 
- As someone who is always on-the-go, it's so convenient to be able to keep my coffee warm 
even when I'm not directly in front of the machine. 
- As a busy professional, I'm always on the go and don't always have time to wait for my 
coffee to brew. 
Cluster 2: 
- I am absolutely in love with this coffee machine! 
- I've been using it every day since I got it and I can already see the difference in my 
productivity levels - I'm so much more alert and focused throughout the day. 
- I would definitely recommend this product to anyone who is always on-the-go and wants to 
stay energized and focused throughout the day. 
Cluster 3: 
- Highly recommend! 
- I highly recommend it! 
- Highly recommend! 
Cluster 4: 
- This machine is a game-changer - it connects to my smartwatch, so I can program it to 
keep my coffee warm even when I'm not in the same room. 
- It's so convenient and has made my mornings so much more enjoyable. 
- I recently purchased the smart coffee machine that connects to my smartwatch and I am 
absolutely loving it! 
Cluster 5: 
- I am so impressed with this coffee machine! 
- This machine is a game-changer. 
- This machine is a game changer. 
 

ï˜»ï›œ 
Table ï˜¼. Sample Outputs of the Models in Experiment ï˜º 
Initial Prompt: â€œGrace Hopper wasâ€ 
Base Model 
Aligned Model 
one of the first programmers for IBMâ€™s 704 
mainframe computer. She was also a trailblazer 
in computer programming, becoming the first 
female admiral (in the US Navy) and helping to 
create the COBOL language, which is still used 
widely today. 
â€œLearn the rules so you know how to break them 
properly.â€ â€“ Grace Hopper 
 
an American computer scientist and Navy Rear 
Admiral who was one of the pioneers of computer 
programming. She is best known for her work on 
the 
development 
of 
COBOL, 
a 
high-level 
programming language that made it easier for 
non-technical 
people 
to 
write 
computer 
programsâ€¦ 
a mathematician and pioneer in the computer 
field. She was the first person to program the 
Harvard Mark I Automatic Sequence Controlled 
Calculator (a.k.a. "Harvard Mark II"), which 
was used during WWII, and later led research 
for the United States Navy. She is also famous 
for coining the term "bug" in computer 
programming when an electrical short caused a 
computer to malfunction; she found that the 
actual insects were just as annoying as the 
bugs in her computers. 
She is buried at Arlington National Cemeteryâ€¦ 
an American computer scientist and Navy Rear 
Admiral who was a pioneer in the field of 
computer programming. She is best known for 
her work on the development of COBOL, a high-
level programming language that was designed 
to be easy to read and write. Hopper was also 
a vocal advocate for the use of computers in 
business 
and 
education, 
and 
she 
worked 
tirelessly to promote the field of computer 
scienceâ€¦ 
born in New York City to a mother of Swedish 
descent and an Irish father, who worked as a 
bookkeeper. She attended the University of New 
York where she graduated in 1928 with a degree 
in mathematics. Grace went on to graduate 
school at Yale and obtained her masters in 
mathematics 
from 
Columbia 
University. 
In 
addition 
to 
being 
an 
accomplished 
mathematician Hopper is also credited for 
making 
major 
contributions 
to 
computer 
science, especially in the development of 
early programming languagesâ€¦ 
an American computer scientist and Navy Rear 
Admiral best known for her work on the 
development of COBOL, a high-level programming 
language that was designed to be easy to read 
and write. 
Hopper was born in 1906 in New York City and 
grew up in a family of mathematicians and 
inventors. She earned her PhD in mathematics 
from Yale University in 1934 and went on to 
work at the Harvard Mark I computer lab, where 
she helped develop the first compiler for 
programming language COBOLâ€¦ 
 

ï˜»ï˜º 
Table ï˜½. Four typical ways the aligned model can describe Grace Hopper in Experiment ï˜º. 
Biographical 
Sketch 
born on November 9, 1906 in New York City. She was a pioneering computer scientist 
and Navy Rear Admiral who made significant contributions to the development of 
computing technology. Hopper earned her PhD in mathematics from Yale University in 
1934 and began working at Harvard Universityâ€™s Mark I computer project in 1943. 
During World War II, she was part of a team that developed COBOL (Common Business-
Oriented Language), a programming language designed for business applications. 
Hopperâ€™s work on COBOL helped pave the way for the development ofâ€¦ 
Early Life and 
Career 
an American computer scientist and Navy Rear Admiral who was one of the pioneers 
of computer programming. She is best known for her work on the development of 
COBOL, a high-level programming language that made it easier for non-technical 
people to write computer programs. 
Grace Hopper was born in 1906 in New York City and grew up during a time when women 
were not often encouraged to pursue careers in science or technology. Despite these 
challenges, she became fascinated with mathematics and physics at an early age and 
went on to study mathematics at Vassar Collegeâ€¦ 
Interesting 
Facts (in list 
format) 
a pioneering computer scientist who made significant contributions to the field of 
computer programming and the development of COBOL, a popular programming language. 
Here are some interesting facts about Grace Hopper: 
1. Early Life and Education: Grace Hopper was born on December 9, 1906, in New 
York City. She grew up in a family of inventors and engineers and developed an 
early interest in mathematics and science. She earned her Ph.D. in mathematics 
from Yale University in 1934. 
2. Navy Career: During World War II, Hopper joined the United States Navyâ€¦ 
Key 
Contributions 
and Legacy 
a computer scientist who helped create the first compiler. She was born in 1906 
and died in 1992, but her legacy continues to inspire women in technology today. 
Hopper earned her Ph.D. in mathematics from Yale University in 1934, becoming one 
of the few women in the field at the time. She joined the Navy Reserve during World 
War II and was assigned to the Harvard Mark I computer, where she pioneered the 
use of machine language programming. 
In the 1950s, Hopper developed COBOL (Common Business Orientedâ€¦ 
 
Aï˜º. 
The RLHF Process 
Reinforcement Learning from Human Feedback (RLHF) is a technique used to align 
language models with human preferences and values. In (Touvron et al., ï˜ºï˜¹ï˜ºï˜»), the 
authors employ RLHF to fine-tune the pretrained Llama-ï˜º language model, resulting in 
an AI assistant that generates more helpful, safe, and aligned responses. This appendix 
provides an overview of the RLHF process used in the Llama-ï˜º paper. 
Aï˜º.ï›œ. 
Human Preference Data Collection 
The first step in the RLHF process is to collect human preference data, which will be 
used to train the reward model. In the Llama-ï˜º paper, the authors use a binary comparison 
protocol, where annotators are asked to write a prompt and then choose between two 
sampled model responses based on provided criteria. To maximize diversity, the two 
responses for each prompt are sampled from different model variants and temperatures. 
Annotators also label the degree to which they prefer their chosen response over the 
alternative, using options such as â€œsignificantly betterâ€, â€œbetterâ€, â€œslightly betterâ€, or 
â€œnegligibly better/unsureâ€. 

ï˜»ï˜» 
Aï˜º.ï˜º. 
Reward Modeling 
The collected human preference data is used to train a reward model, which takes a 
model response and its corresponding prompt as inputs and outputs a scalar score 
indicating the quality of the response in terms of helpfulness and safety. The Llama-ï˜º 
paper trains two separate reward models: one optimized for helpfulness (Helpfulness RM) 
and another for safety (Safety RM). The reward models are initialized from pretrained 
chat model checkpoints to ensure that they have access to the same knowledge as the base 
model. The model architecture and hyperparameters are identical to those of the 
pretrained language models, except for the classification head, which is replaced with a 
regression head for outputting scalar rewards. 
Aï˜º.ï˜». 
Proximal Policy Optimization (PPO) 
Proximal Policy Optimization (PPO) is a popular reinforcement learning algorithm 
used in the Llama-ï˜º paper to fine-tune the pretrained language model using the reward 
models. PPO aims to update policies via (â€œProximal Policy Optimization,â€ ï˜ºï˜¹ï˜ºï˜¼): 
ğœƒğ‘˜+1 = arg max
ğœƒ
ğ”¼ğ‘ ,ğ‘âˆ¼ğœ‹ğœƒğ‘˜[ğ¿(ğ‘ , ğ‘, ğœƒğ‘˜, ğœƒ)] 
where 
ğ¿(ğ‘ , ğ‘, ğœƒğ‘˜ğœƒ) â‰”min ( ğœ‹ğœƒ(ğ‘|ğ‘ )
ğœ‹ğœƒğ‘˜(ğ‘|ğ‘ ) , clip ( ğœ‹ğœƒ(ğ‘|ğ‘ )
ğœ‹ğœƒğ‘˜(ğ‘|ğ‘ ) , 1 âˆ’ğœ–, 1 + ğœ–)) ğ´ğœ‹ğœƒğ‘˜(ğ‘ , ğ‘) 
- 
ğœƒ represents the parameters of the policy network (the language model). 
- 
ğœ‹ğœƒ(ğ‘|ğ‘ ), ğœ‹ğœƒğ‘˜(ğ‘|ğ‘ ) are the new and old policy, respectively. 
- 
ğ´ğœ‹ğœƒğ‘˜(ğ‘ , ğ‘) is the advantage function of taking action ğ‘ under policy ğœ‹ğœƒğ‘˜ at state ğ‘ . 
- 
clip(ğ‘¥, ğ‘, ğ‘) truncates ğ‘¥< ğ‘ and ğ‘¥> ğ‘. 
- 
ğœ– is a hyperparameter that controls the clipping range (usually set to ï˜¹.ï›œ or ï˜¹.ï˜º). 
The clipping function in the PPO objective helps to limit the size of the policy updates, 
ensuring that the new policy does not deviate too far from the old policy. This promotes 
stability during training. Essentially, clipping acts as a regularizer by disincentivizing 
dramatic policy changes, with the hyperparameter ğœ– determining the allowable deviation 
that still benefits the objective. 

