Econometric Theory, 0, 2023, 1–31.
doi:10.1017/S026646662300035X
SUBGEOMETRICALLY ERGODIC
AUTOREGRESSIONS WITH
AUTOREGRESSIVE CONDITIONAL
HETEROSKEDASTICITY
MIKA MEITZ
University of Helsinki
PENTTI SAIKKONEN
University of Helsinki
In this paper, we consider subgeometric (specifically, polynomial) ergodicity of uni-
variate nonlinear autoregressions with autoregressive conditional heteroskedasticity
(ARCH). The notion of subgeometric ergodicity was introduced in the Markov chain
literature in the 1980s, and it means that the transition probability measures converge
to the stationary measure at a rate slower than geometric; this rate is also closely
related to the convergence rate of β-mixing coefficients. While the existing literature
on subgeometrically ergodic autoregressions assumes a homoskedastic error term,
this paper provides an extension to the case of conditionally heteroskedastic ARCH-
type errors, considerably widening the scope of potential applications. Specifically,
we consider suitably defined higher-order nonlinear autoregressions with possibly
nonlinear ARCH errors and show that they are, under appropriate conditions,
subgeometrically ergodic at a polynomial rate. An empirical example using energy
sector volatility index data illustrates the use of subgeometrically ergodic AR–ARCH
models.
1. INTRODUCTION
Let Xt (t = 0,1,2,...) be a Markov chain on the state space X and initialized
from an X0 following some initial distribution. If the n-step probability measures
Pn(x; ·) = Pr(Xn ∈· | X0 = x) converge in total variation norm ∥·∥TV to the
stationary probability measure π at rate rn (for some r > 1), that is,
lim
n→∞rn∥Pn(x; ·)−π(·)∥TV = 0,
π a.e.,
(1)
the Markov chain is said to be geometrically ergodic. When the convergence in (1)
takes place at a suitably defined rate r(n) slower than geometric, that is,
The authors thank the Academy of Finland (M.M. and P.S.), Foundation for the Advancement of Finnish Securities
Markets (M.M.), and OP Group Research Foundation (M.M.) for financial support, and the Co-Editor (Robert
Taylor) and three anonymous referees for useful comments and suggestions. Address correspondence to Mika Meitz,
Department of Economics, University of Helsinki, P. O. Box 17, FI-00014 University of Helsinki, Finland; e-mail:
mika.meitz@helsinki.fi.
© The Author(s), 2023. Published by Cambridge University Press.
1
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

2
MIKA MEITZ AND PENTTI SAIKKONEN
lim
n→∞r(n)∥Pn(x; ·)−π(·)∥TV = 0,
π a.e.,
(2)
the Markov chain is called subgeometrically ergodic. Examples of common rates
(where c denotes a positive constant) include geometric (or exponential) when
r(n) = ecn = rn (r > 1), subexponential when r(n) = ecnγ (0 < γ < 1), polynomial
when r(n) = (1+n)c, and logarithmic when r(n) = (1+ln(n))c. The authoritative
and classic reference to Markov chain theory is the monograph of Meyn and
Tweedie (2009), while an up-to-date treatment of subgeometric ergodicity can be
found in Chapters 16 and 17 of Douc et al. (2018).
To give some background, the notion of subgeometric ergodicity was introduced
in the Markov chain literature in the 1980s when Nummelin and Tuominen (1983)
and Tweedie (1983) obtained the first subgeometric ergodicity results for general
state space Markov chains. Subsequent work by Tuominen and Tweedie (1994),
Fort and Moulines (2000), Jarner and Roberts (2002), Fort and Moulines (2003),
and Douc et al. (2004) led to a formulation of a so-called drift condition to ensure
subgeometric ergodicity, paralleling the use of a Foster–Lyapunov drift condition
to establish geometric ergodicity (see, e.g., Meyn and Tweedie, 2009, Chap.
15). Various topics in probability theory and statistics have also been considered
under subgeometric assumptions; for instance, Douc, Guillin, and Moulines (2008)
considered the central limit theorem and Berry–Esseen bounds, Atchadé and Fort
(2010) the convergence of Markov chain Monte Carlo algorithms, Merlevède,
Peligrad, and Rio (2011) a Bernstein-type inequality, and Meitz and Saikkonen
(2021) the rate of β-mixing. In this paper, we are interested in autoregressive
time series models. Results regarding the subgeometric ergodicity of first-order
autoregressions were obtained by Tuominen and Tweedie (1994), Veretennikov
(2000), Fort and Moulines (2003), Douc et al. (2004), Klokov and Veretennikov
(2004, 2005), and Klokov (2007), among others, whereas results for more general
higher-order autoregressions were obtained by Meitz and Saikkonen (2022).
In this paper, we consider subgeometric (specifically, polynomial) ergodicity of
autoregressive models with autoregressive conditional heteroskedasticity (ARCH;
Engle, 1982). The previous works on subgeometrically ergodic autoregressions
listed above only considered the case of independent and identically distributed
(IID) errors, and allowing for conditionally heteroskedastic errors considerably
widens the scope of potential applications. This is particularly important in
applications using economic and financial time series data. In the subgeometrically
ergodic AR–ARCH models we consider, the conditional mean is similar to the
(homoskedastic) AR models already considered in Meitz and Saikkonen (2022).
The precise model formulation will be given and motivated further in Section 2, but
we already note that the models we consider accommodate for behavior similar to
a unit-root process for large values of the observed series, but almost no restrictions
are placed on their dynamics for moderate values of the observed series. The
conditional variance is allowed to follow a rather general nonlinear ARCH process.
In our main result, we show that the considered AR–ARCH processes are,
under appropriate conditions, subgeometrically ergodic at a polynomial rate; the
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
3
convergence rate of β-mixing coefficients and finiteness of certain moments are
also obtained (for details, see Section 3.2).
The inclusion of ARCH (instead of IID) errors considerably complicates the
proofs of (sub)geometric ergodicity of nonlinear autoregressions. Papers consider-
ing subgeometric ergodicity of homoskedastic autoregressions were already listed
above. Geometric ergodicity of nonlinear autoregressive models with ARCH (or
generalized ARCH) errors has previously been considered by numerous authors
(see, e.g., Cline and Pu, 2004, Meitz and Saikkonen, 2008b, 2010, and the
many references therein). Compared to these two strands of previous literature,
the combination of the subgeometrically ergodic type of nonlinear dynamics in
the conditional mean with ARCH errors leads to additional complications in the
proofs. To appropriately separate these two sources of dynamics, we make use of
a (relatively unknown) extension of Bernoulli’s inequality due to Fefferman and
Shapiro (1972) (combined with Young’s inequality), and to control terms arising
due to conditional heteroskedasticity, we devise a special matrix norm that is of a
more complicated type than the norms typically used when analyzing the stability
of nonlinear time series models.
The rest of the paper is organized as follows. Section 2 introduces the nonlinear
AR–ARCH model considered and states the assumptions we employ. Results on
subgeometric ergodicity are given in Section 3. In Section 4, we consider an
empirical application of our model to a daily time series of an energy sector
volatility index. Section 5 concludes. All proofs are collected in Appendix A.
2. MODEL
2.1. Conditional Mean
We consider the univariate process yt (t = 1,2,...) generated by
yt = π1yt−1 +···+πp−1yt−p+1 +g(ut−1)+σtεt,
(3)
where p ≥1 is the autoregressive order, ut = yt −π1yt−1 −···−πp−1yt−p+1, g is a
real-valued function, εt is an IID error term, and σt = σ(yt−1) is a positive volatility
term that depends on p + q lagged values of yt, yt−1 = (yt−1,...,yt−p−q), where
q ≥1 is an ARCH order. For now, one concrete example of the volatility term is a
linear ARCH process, where σt satisfies
σ 2
t = ω +α1e2
t−1 +···+αqe2
t−q
(4)
and et = yt −π1yt−1 −···−πp−1yt−p+1 −g(ut−1), ω > 0, and αi ≥0 (i = 1,...,q);
a more general formulation for the conditional variance will be considered below.
Note that a compact expression for et is et = ut −g(ut−1) so that equation (3) can
be expressed as ut = g(ut−1)+σtεt. If π1 = ··· = πp−1 = 0 in equation (3), we have
ut = yt so that the autoregressive order p reduces to one and equation (3) reduces
to yt = g(yt−1)+σtεt.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

4
MIKA MEITZ AND PENTTI SAIKKONEN
Our first assumption contains basic requirements for the error term εt and
makes clear that the squared volatility, σ 2
t , is the conditional variance of yt (when
appropriate moments exist).
Assumption 1. {εt, t = 1,2,...} is a sequence of IID random variables that
is independent of (y0,...,y1−p−q), has zero mean and unit variance, and the
distribution of ε1 has a (Lebesgue) density that is bounded away from zero on
compact subsets of R.
Later on, we introduce an assumption on the conditional variance σ 2
t which
further restricts the moments of εt.
To further describe the conditional mean of the autoregressions we consider, we
next specify the conditions needed for the function g in equation (3). The following
assumption is a simplification of Assumption 1 in Meitz and Saikkonen (2022) (the
somewhat more general formulation used therein is briefly discussed at the end of
this subsection).
Assumption 2.
(i) The roots of the polynomial ϖ(z) = 1−π1z−···−πp−1zp−1 lie outside the
unit circle.
(ii) The function g : R →R in (3) is measurable, locally bounded, and satisfies
|g(u)| →∞as |u| →∞, and there exist positive constants r, M0, K0, and 0 < ρ < 2
such that, for all u ∈R,
|g(u)| ≤

(1−r|u| −ρ)|u|,
for |u| ≥M0,
K0,
for |u| ≤M0.
(5)
Assumption 2(i) corresponds to the conventional stationarity condition of a
linear autoregression in that it requires the roots of the polynomial ϖ(z) to lie
outside the unit circle. In the first-order case p = 1, this condition becomes
redundant because then π1 = ··· = πp−1 = 0. Assumption 2(ii) is needed to prove
the subgeometric ergodicity of the process yt, as already done by Fort and Moulines
(2003) and Douc et al. (2004) in the first-order case p = 1 and by Meitz and
Saikkonen (2022) for higher-order autoregressions.
We next provide some intuition and motivation for our model in (3). To clarify
the role of inequality (5) restricting the function g(·), suppose Assumptions 1 and
2(i) hold, but instead of Assumption 2(ii), suppose the function g(·) were linear
with g(u) = π0u and π0 ∈[−1,1]. Using the lag operator L, equation (3) could
then be written as
ut −π0ut−1 = (1−π0L)(1−π1L−···−πp−1Lp−1)yt = σtεt,
(6)
that is, as the familiar linear AR(p) model (with autoregressive heteroskedasticity).
Given Assumptions 1 and 2(i), the case π0 ∈(−1,1) corresponds to geometric
ergodicity of yt and the cases π0 = ±1 to non-ergodicity. Nonlinear functions g(·)
satisfying Assumption 2(ii) provide a middle ground between these extreme cases
of geometric ergodicity and non-ergodicity. For instance, if g(u) = (1−r|u| −ρ)u
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
5
for |u| > r1/ρ and g(u) = 0 otherwise (r > 0, 0 < ρ < 2), then for any fixed π0 ∈
(−1,1) and for all u sufficiently large in absolute value (i.e., for the values of u
that are crucial for determining ergodicity),
|π0||u| < |g(u)| < |u|.
The subgeometrically ergodic autoregressions we consider thus provide one pos-
sibility for modeling small departures from unit-root autoregressions. Assumption
2(ii) implies that for large values of |ut−1|, the conditional mean of model (3) is
close to that of an integrated process (of order one). On the other hand, as inequality
(5) restricts the function g(·) only for large values of its argument, no restrictions
(apart from the boundedness condition in (5)) are imposed when the argument
takes values inside some bounded set of values. Thus, the autoregressions we
consider may exhibit rather arbitrary (stationary, unit root, explosive, nonlinear,
etc.) behavior for moderate values of the observed series.
The autoregressions we consider are to some extent related to existing models
that have autoregressive roots near unity. To illustrate, when g(u) is as in the
previous paragraph and we further set p = r = ρ = 1, the model in (3) simplifies to
yt =

1−
1
|yt−1|

yt−1 +et
when |yt−1| > 1
and
yt = et
otherwise,
where et = σtεt. In comparison, a prototypical local-to-unity autoregression could
be expressed as
yt =

1−1
T

yt−1 +et,
t = 1,...,T,
where T denotes the sample size.
Both of the above formulations involve an autoregressive coefficient near unity,
the former when the observed process takes on large (absolute) values and the
latter when the sample size is large. However, the fact that the sample size is an
essential part of local-to-unity autoregressions makes them quite different from
the autoregressions we consider—in particular, the autoregressions we consider
are ergodic. For more details on local-to-unity autoregressions and other related
models, we refer the reader to the recent contributions of Lieberman and Phillips
(2020) and Phillips (2023) and the references therein.
Homoskedastic subgeometrically ergodic autoregressions satisfying (a some-
what more general version of) Assumption 2 were already considered by Meitz
and Saikkonen (2022). As many time series in economics, finance, and other
fields exhibit conditional heteroskedasticity, in this paper, we consider an extension
to ARCH errors. In the homoskedastic case considered in Meitz and Saikkonen
(2022), the term g(ut−1) in (3) was replaced with the more general formulation
ut−1 + ˜g(yt−1,...,yt−p) (with ˜g a real-valued function) to allow for more general
dependence on the past through the variables yt−1,...,yt−p (and not only through
the linear combination ut−1 = yt−1 −π1yt−2 −···−πp−1yt−p). The present simpler
formulation worked well in the empirical application of Section 4 and in some
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

6
MIKA MEITZ AND PENTTI SAIKKONEN
other examples we tried out, and leads to more transparent assumptions and
streamlined proofs.
2.2. Companion Form
To establish ergodicity, we need the companion form of the (p + q)-dimensional
process yt = (y1,t,y2,t) with a p-dimensional y1,t = (yt,...,yt−p+1) and a
q-dimensional y2,t = (yt−p,...,yt−p−q+1). First, we formulate the p-dimensional
companion form related to equation (3), which reads as
⎡
⎢⎢⎢⎢⎢⎢⎣
yt
yt−1
...
...
yt−p+1
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
π1
π2
···
πp−1
0
1
0
···
0
0
0
...
...
...
...
...
...
...
0
0
0
···
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
yt−1
yt−2
...
...
yt−p
⎤
⎥⎥⎥⎥⎥⎥⎦
+g(ut−1)
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
...
...
0
⎤
⎥⎥⎥⎥⎥⎥⎦
+σtεt
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
...
...
0
⎤
⎥⎥⎥⎥⎥⎥⎦
or, denoting the matrix in this equation with  and setting ιp = (1,0,...,0) (p×1),
as
y1,t = y1,t−1 +g(ut−1)ιp +σtεtιp
(7)
(when p = 1,  = 0 and ut−1 = yt−1). As σt = σ(yt−1) depends on the whole
(p+q)-dimensional vector yt−1, we have to expand (7) to the (p+q)-dimensional
companion form
 y1,t
y2,t

=


0p×q
0q×(p−1)
Iq
0q×1
 y1,t−1
y2,t−1

+g(ut−1)ιp+q +σtεtιp+q,
(8)
where Iq is the (q×q) identity matrix and 0∗×∗denotes a matrix of zeros with the
indicated dimensions (and ιp+q is defined in the obvious way). This shows that yt
is a Markov chain on Rp+q.
In order to establish ergodicity, we further transform the p-dimensional com-
panion form (7) in a way already used in Meitz and Saikkonen (2022, Sect. 4). To
this end, we define the matrices
A =
⎡
⎢⎢⎢⎢⎢⎣
1 −π1 −π2 ··· −πp−1
0
1
0
···
0
...
...
...
...
...
0
...
...
0
0 ···
···
0
1
⎤
⎥⎥⎥⎥⎥⎦
and
(9)
 = AA−1 =
⎡
⎢⎢⎢⎢⎣
0
0
0
···
0
1
π1
π2
···
πp−1
0
1
0
···
0
...
...
...
...
...
0
···
0
1
0
⎤
⎥⎥⎥⎥⎦
=

0
01×(p−1)
ιp−1
1

,
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
7
where A is nonsingular and 1 is the (p −1) × (p −1)-dimensional lower right-
hand corner of  (when p = 1, A = 1 and  = 0). With these definitions, equation
(7) can be transformed into
Ay1,t = Ay1,t−1 +g(ut−1)ιp +σtεtιp,
(10)
where Ay1,t = (ut,yt−1,...,yt−p+1). Now, for any p-dimensional vector x1, form
the partition x1 = (x1,1,...,x1,p) = (x1,1,x1,2) and define
z(x1) =

z1(x1)
z2(x1)

= Ax1 =

x1,1 −π1x1,2 −···−πp−1x1,p
x1,2

(11)
(when p = 1, x1,2 and z2(x1) are dropped). Using this notation, equation (10) can
be expressed as z(y1,t) = z(y1,t−1)+g(z1(y1,t−1))ιp +σ(yt−1)εtιp, that is, as

z1(y1,t)
z2(y1,t)

=

0
01×(p−1)
ιp−1
1

z1(y1,t−1)
z2(y1,t−1)

+g(z1(y1,t−1))ιp +σ(yt−1)εtιp
=

g(z1(y1,t−1))+σ(yt−1)εt
1z2(y1,t−1)+z1(y1,t−1)ιp−1

.
(12)
Here, the first equation is in a form where the autoregressive order is one and the
volatility term is a function of the (p+q)-dimensional vector yt−1 = (y1,t−1,y2,t−1),
whereas the second equation involves the p-dimensional vector y1,t−1 only.
By Assumption 2(i), the roots of the polynomial ϖ(z) lie outside the unit circle,
so that the eigenvalues of the matrix 1 in the second equation in (12) are smaller
than one in absolute value. As is well known, this implies the existence of a matrix
norm of 1 that is also smaller than one. Specifically, for any vector norm ∥· ∥,
denote by |||·||| the corresponding induced matrix norm (Horn and Johnson, 2013,
Def. 5.6.1); that is, for any conformable square matrix A, set
|||A||| = max
∥x∥=1∥Ax∥.
Then we obtain the following result (Horn and Johnson, 2013, Lem. 5.6.10).
Lemma 1. There exist a vector norm ∥·∥∗and a corresponding induced matrix
norm |||·|||∗such that |||1|||∗= ϖ < 1.
The existence of an induced matrix norm with the property in the above lemma
is essential in our proofs. (When p = 1, Assumption 2(i) and Lemma 1 are
redundant.) The norms ∥· ∥∗and |||·|||∗are defined on Rp−1 and R(p−1)×(p−1),
respectively, and they have been commonly used in time series models. In the next
subsection, we introduce norms which are of a different type.
2.3. Conditional Variance
The root condition of Assumption 2(i) and inequality (5) of Assumption 2(ii) are
of major importance for establishing the stability of our model. However, as these
conditions only concern the conditional mean, we need additional assumptions
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

8
MIKA MEITZ AND PENTTI SAIKKONEN
restricting the conditional variance σ 2
t . As an extension of the basic ARCH model
(4), we consider a nonlinear formulation of the conditional variance defined as
σ 2
t = ζ0,t−1ω +α1ζ1,t−1e2
t−1 +···+αqζq,t−1e2
t−q,
(13)
where ζi,t−1 = ζi(yt−1) is a function of yt−1 (i = 0,...,q) and otherwise the notation
is as in equation (4) (including the conditions ω > 0 and α1,...,αq ≥0). When
the functions ζi,t−1 are the same for all i = 0,...,q, we remove the index i and
use the notations ζt−1 and ζ(·). This is the case in our empirical example where
ζt−1 = ζ(yt−1) = 1/(1+e−γ (yt−1−a)) is a logistic function depending only on yt−1.
For possible alternatives, we consider a more general formulation and introduce
the following assumption.
Assumption 3. In equation (13), the following conditions are assumed. (i) The
parameters ω,α1,...,αq satisfy ω > 0, α1,...,αq ≥0, and q
i=1 αi < 1. (ii) For
each i = 0,...,q, the function ζi takes values in (0,1].
The above assumption includes the case ζi ≡1 for all i, which corresponds to
the linear ARCH model (4). It covers also the abovementioned logistic function.
Consider the q-dimensional process ξ t = (e2
t ,e2
t−1,...,e2
t−q+1) (t ≥1) with initial
values ξ 0 = (e2
0,...,e2
−q+1) where e2
0,...,e2
−q+1 are functions of y0. Inspired by
Cline and Pu (2004, Exam. 4.2), we now introduce the following equation, which
is a straightforward implication of equation (13) and the fact σtεt = et:
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
e2
t
e2
t−1
...
...
e2
t−q+1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
α1ζ1,t−1ε2
t
α2ζ2,t−1ε2
t
···
αq−1ζq−1,t−1ε2
t
αqζq,t−1ε2
t
1
0
···
0
0
0
1
...
...
...
...
...
...
0
0
0
···
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
e2
t−1
e2
t−2
...
...
e2
t−q
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
+
⎡
⎢⎢⎢⎢⎢⎢⎣
ζ0,t−1ε2
t ω
0
...
...
0
⎤
⎥⎥⎥⎥⎥⎥⎦
(t = 1,2,...) or, more briefly,
ξ t = ζ,tξ t−1 +ωζ,t,
t = 1,2,... ;
(14)
as ξ t is a function of yt, we occasionally write ξ t = ξ(yt). For later purposes, we
also note that due to the identities σtεt = et and e2
t = ι′
qξ(yt), we have
σ 2
t = σ 2(yt−1) = E[ι′
qξ(yt) | yt−1].
(15)
When there is need to make the dependence of ζ,t on yt−1 explicit, we use the
notation ζ,t(yt−1) and replace the (random) argument yt−1 by a fixed counterpart
when needed. Specifically, ζ,t(x) means that the functions ζi,t−1 = ζi,t−1(yt−1)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
9
used in ζ,t(yt−1) are replaced by ζi,t(x) for all i = 1,...,q, and the notations σ 2(x)
and ξ(x) are used similarly.
We also define the matrices
t =
⎡
⎢⎢⎢⎢⎢⎣
α1ε2
t
α2ε2
t
···
αq−1ε2
t
αqε2
t
1
0
···
0
0
0
1
...
...
...
...
...
...
0
0
0
···
0
1
0
⎤
⎥⎥⎥⎥⎥⎦
and
 = E[t] =
⎡
⎢⎢⎢⎢⎢⎣
α1
α2
···
αq−1
αq
1
0
···
0
0
0
1
...
...
...
...
...
...
0
0
0
···
0
1
0
⎤
⎥⎥⎥⎥⎥⎦
.
(16)
Note that t is obtained from the matrix ζ,t by choosing ζi,t = 1 for all
i = 1,...,q. Similarly, we denote ωt = (ωt,0,...,0)′ with ωt = ωε2
t and ω =
E[ωt] = (ω,0,...,0)′.
In our proofs, we need to appropriately control the size of the random matrix t,
and not just the size of the nonrandom matrix  = E[t]. This is the reason why we
next consider vector and matrix norms more complicated than those in Lemma 1.
To this end, we first recall the definition of an Lp-norm (for convenience, in this
subsection only, we use the notation p in Lp-norms; elsewhere in the paper p stands
for the autoregressive order in model (3)). If ∥·∥is any vector norm on Rq and v
is a q-dimensional random vector, equation
∥v∥Lp = (E[∥v∥p])1/p
(1 ≤p < ∞)
defines an Lp-norm on the set of (equivalence classes of almost surely equal) q×1
random vectors that are p-integrable (see, e.g., Dudley, 2004, Sects. 5.1 and 5.2). It
may be worth noting that for nonrandom vectors, there is no difference between the
norms ∥·∥and ∥·∥Lp, but for random vectors, the outcome of ∥·∥is random and
that of ∥·∥Lp is nonrandom. This Lp-norm can be used to induce a norm for random
matrices; for the conventional nonrandom matrix case and for the terminology
used below, see Horn and Johnson (2013, Def. 5.6.1 and Sect. 5.6). Specifically,
to define a generalized (non-submultiplicative) matrix norm |||·|||Lp, for any q×q
random matrix A, set
|||A|||Lp = max
∥x∥Lp=1∥Ax∥Lp = max
∥x∥=1∥Ax∥Lp
(x ∈Rq),
(17)
where the latter equality holds as x is nonrandom. This defines a generalized matrix
norm1 on the set of (equivalence classes of almost surely equal) q × q random
1Axioms (1), (1a), (2), and (3) of a generalized matrix norm (see Horn and Johnson, 2013, pp. 340–341) can be
checked similarly as in the proof of Theorem 5.6.2(c) of the same reference (replacing the norms ∥· ∥and |||·|||
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

10
MIKA MEITZ AND PENTTI SAIKKONEN
matrices with p-integrable entries; moreover, the norms ∥·∥, ∥·∥Lp, and |||·|||Lp are
related by the inequality2
∥Ax∥Lp ≤|||A|||Lp∥x∥
(x ∈Rq).
(18)
We next state a high-level condition that assumes the existence of a vector
norm on Rq with particular additional properties. (Primitive conditions ensuring
this high-level assumption will be given momentarily.) One of these properties
is monotonicity in the sense of Definition 5.4.18 of Horn and Johnson (2013):
a vector norm ∥· ∥is monotone if x,y ∈Rq satisfying |xi| ≤|yi| for i = 1,...,q
always implies that ∥x∥≤∥y∥. For clarity, we use the notation ∥·∥• for the specific
vector norm in the assumption below; similarly, we denote the related Lp-norm
by ∥· ∥•Lp and the generalized matrix norm by |||·|||•Lp. We also introduce two
constants, s0 ≥1 and b ≥1, such that
b = 1 when s0 = 1
and
b > (2s0 −ρ)/[s0(2−ρ)] > 1 when s0 > 1
(recall from Assumption 2 that ρ ∈(0,2) so that 2s0 > ρ). These constants are
used in the next section where we establish our ergodicity result and there the
size of s0 will have an effect on the rate of convergence obtained and the order of
moments that are finite. The rather complex conditions required from the constant
b are due to the connection between the conditional mean and ARCH errors (this
connection disappears when s0 = 1 as it also does in subgeometric homoskedastic
autoregressions).
Assumption 4. Suppose there exists a vector norm ∥· ∥• on Rq that is (i)
monotone and (ii) such that |||t|||•Lbs0 = λ < 1, where b and s0 are as described
above.
This assumption tacitly requires that E[|εt|2bs0] is finite, thereby strengthening
Assumption 1 (when s0 > 1). Assumption 4 is formulated in a way that is conve-
nient in our proofs but is not very transparent. The following lemma gives primitive
conditions ensuring that Assumption 4 holds (for a proof, see Appendix A).
Lemma 2. Suppose that Assumptions 1 and 3 hold and also that the
parameters α1,...,αq in Assumption 3 satisfy q
i=1 αi < 1/ ¯μ2bs0, where ¯μ2bs0 =
(E[|ε2
1|bs0])1/bs0. Then Assumption 4 holds.
To illustrate, consider the case q = s0 = b = 1 so that the condition in Lemma 2
reduces to the requirement α1 < 1. In geometrically ergodic AR models with
linear ARCH(1) errors, α1 < 1 is the usual requirement for covariance stationarity,
while geometric ergodicity can hold under even weaker conditions, such as
E[ln(α1ε2
t )] < 0 (see, e.g., Meitz and Saikkonen, 2010, Assump. 3 and Thm. 1
therein with ∥· ∥Lp and |||·|||Lp, replacing appropriate statements therein with their almost sure counterparts, and
using Minkowski’s inequality as an additional justification for axiom (3)).
2Inequality (18) can be verified analogously to Theorem 5.6.2(b) of Horn and Johnson (2013).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
11
for further details). In the present setting, the situation is different: as will be seen
in Section 3.2, condition α1 < 1 does not guarantee a finite variance.
3. SUBGEOMETRIC ERGODICITY AT A POLYNOMIAL RATE
3.1. Main Result
We now consider the stability of the model introduced in the previous section. We
begin with a brief account of some necessary Markov chain concepts (for more
comprehensive discussions, see Meyn and Tweedie, 2009; Douc et al., 2018; and
also Meitz and Saikkonen, 2022, Sect. 2). Let Xt (t = 0,1,2,...) be a Markov chain
on a general measurable state space (X,B(X)) (with B(X) the Borel σ-algebra),
and let Pn(x; A) = Pr(Xn ∈A | X0 = x) signify its n-step transition probability
measure. For an arbitrary fixed measurable function f : X →[1,∞) and for any
signed measure μ, define the f-norm ∥μ∥f as
∥μ∥f = sup
f0:|f0|≤f
|μ(f0)|,
(19)
where μ(f0) =

x∈X f0(x)μ(dx) and the supremum in (19) runs over all measurable
functions f0 : X →R such that |f0(x)| ≤f(x) for all x ∈X (when f ≡1, the f-norm
∥μ∥f reduces to the total variation norm ∥μ∥TV = supf0:|f0|≤1 |μ(f0)| used in (1) and
(2)). When the n-step probability measures Pn(x; ·) converge in f-norm and at rate
r(n) to the stationary probability measure π satisfying π(f) < ∞, that is,
lim
n→∞r(n)∥Pn(x; ·)−π∥f = 0
for π-almost all3 x ∈X,
(20)
we say that the Markov chain Xt is (f,r)-ergodic; this implicitly entails the
existence of π as well as certain moments as π(f) < ∞. In the conventional
geometrically ergodic case, r(n) = rn for some r > 1. To establish (f,r)-ergodicity,
we use a so-called drift condition defined as follows (here 1S(x) denotes the
indicator function taking value one when x belongs to the set S and zero elsewhere).
Condition D. There exist a measurable function V : X →[1,∞), a concave
increasing continuously differentiable function φ : [1,∞) →(0,∞), a measurable
set C, and a finite constant ˜b such that
E[V(X1) |X0 = x] ≤V(x)−φ (V(x))+ ˜b1C(x),
x ∈X.
(21)
The idea is to verify this condition with suitable functions V and φ, which together
with some additional conditions ensures the (f,r)-ergodicity of the process Xt; for
more details, see Meitz and Saikkonen (2022, Thm. 1).
Now, consider the stability of the Markov chain yt on Rp+q given in (8). To
define the function V in (21), we use the functions z1(·), z2(·), and ξ(·) in (11),
(12), and (14) and the norms ∥·∥∗and ∥·∥• in Lemma 1 and Assumption 4.
3That is, the convergence in (20) is required to hold for all x ∈X except for those x in a set that has probability zero
with respect to the stationary measure π.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

12
MIKA MEITZ AND PENTTI SAIKKONEN
Set x = (x1,...,xp+q) ∈Rp+q and decompose x to its p- and q-dimensional
components as x = (x1,x2). We define the function V as
V(x) = 1+|z1(x1)|2s0 +s1∥z2(x1)∥2s0α
∗
+s2∥ξ(x)∥bs0
• ,
(22)
where s0 and b are defined above Assumption 4, s1 and s2 are positive constants
to be specified later (with s1 small and s2 large), and α = 1 −ρ/2s0 (recall from
Assumption 2 that ρ ∈(0,2) so that α ∈(0,1)). It may be clarifying to note that
when p = 1, model (3) reduces to yt = g(yt−1) +σtεt; then we can set s1 = 0 and
drop z2(x1) so that the function V in (22) becomes V(x) = 1+|x1|2s0 +s2∥ξ(x)∥bs0
• .
To verify Condition D, we need to consider the conditional expectation
E

V(y1) | y0 = x

= 1+E

|z1(y1,1)|2s0 | y0 = x

+s1E

∥z2(y1,1)∥2s0α
∗
| y0 = x

+s2E

∥ξ(y1)∥bs0
•
| y0 = x

,
(23)
bound the conditional expectations on the right-hand side of (23), and express these
bounds in a way which conforms to inequality (21) with the function φ satisfying
the conditions required in Condition D. These considerations, combined with the
checking of some additional technical conditions, lead to the following theorem
(the proof can be found in Appendix A).
Theorem 1. Consider the Markov chain yt defined in (8). Suppose that Assump-
tions 1–4 hold and that V(x) is as in (22). Then yt is (f,r)-ergodic with the
polynomial convergence rate r(n) = nδ−1 and the function f given by f(x) =
V(x)1−δρ/2s0; this result holds for any choice of δ ∈[1,2s0/ρ] and for some (small
enough) s1 > 0 and some (large enough) s2 > 0.
Theorem 1 provides the first subgeometric ergodicity results for autoregres-
sions with autoregressive conditional heteroskedasticity. In this theorem, the
convergence rate r(n) shows the speed at which the n-step transition probability
measures of the process yt converge to the stationary probability measure. Due
to the polynomial convergence rate, we therefore call the process yt polynomially
ergodic. Note also that the choice of δ in Theorem 1 allows for a trade-off between
the rate of convergence and the size of the f-norm.
3.2. Discussion
3.2.1. Geometric Ergodicity.
In previous literature, geometric ergodicity of
nonlinear autoregressions with ARCH errors has been considered using a variety
of different assumptions for the allowed nonlinear dynamics and for the required
moment conditions for the innovations (see, e.g., Cline and Pu, 2004; Meitz and
Saikkonen, 2010, and the many references therein).
3.2.2. Homoskedastic Case.
Theorem 1 remains valid also in the homoskedas-
tic case (obtained by setting α1 = ··· = αq = 0). Previous polynomial ergodicity
results for homoskedastic autoregressions were obtained by Fort and Moulines
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
13
(2003, Sect. 2.2) and Meitz and Saikkonen (2022, Thm. 3), and Theorem 1
provides partial improvements over these earlier results in certain cases.
Assumptions and notation are slightly different in all the papers, but (in the
notation of the present paper) Theorem 1 improves earlier results when 1 ≤ρ < 2
and 1 < s0 < 2.
3.2.3. Proof Strategy.
The proof of Theorem 1 is also somewhat different from
the previous polynomial ergodicity results in Fort and Moulines (2003, Sect. 2.2)
and Meitz and Saikkonen (2022, Thm. 3). A rather obvious difference is that
these earlier results deal with homoskedastic autoregressions, whereas our model
contains a nonlinear ARCH term, the size of which is controlled with the special
matrix norm defined in Assumption 4. Regarding the conditional expectation, the
mentioned earlier results rely on Lemma 3 in Fort and Moulines (2003), while
our proof of Theorem 1 avoids the use of this lemma, and instead makes use of
a (relatively unknown) extension of Bernoulli’s inequality due to Fefferman and
Shapiro (1972) (combined with Young’s inequality).
3.2.4. Mixing and Moment Results.
As already indicated in the Introduction,
the polynomial ergodicity result of Theorem 1 also implies that the process
yt is β-mixing (and hence α-mixing). Moreover, the convergence rate of the
β-mixing coefficients β(n) is given by the fastest convergence rate, that is,
limn→∞n2s0/ρ−1β(n) = 0. For further details and justifications of these mixing
results, see Meitz and Saikkonen (2021, Thm. 2) and Meitz and Saikkonen (2022,
Sect. 2).
Another consequence of Theorem 1 is that the stationary distribution of yt has
finite moments up to order 2s0 −ρ (for a proof, see Appendix A). Note that
depending on the values of s0 ≥1 and ρ ∈(0,2), the order of these finite moments
may be very small; in particular, when s0 = 1, we do not obtain a finite variance.
3.2.5. Subexponential Ergodicity.
Theorem 1 concerns only polynomial
ergodicity of subgeometric AR–ARCH models, and does not consider subex-
ponential ergodicity (where the rate r(n) in (2) equals, say, ecnγ with c > 0 and
0 < γ < 1). The reason for this is that the properties of ARCH-type models do
not seem compatible with the moment requirements needed for subexponential
ergodicity. To elaborate on this, first note that the previous results of Douc et al.
(2004, Sect. 3.3) and Meitz and Saikkonen (2022, Sect. 4.1) on subexponential
ergodicity of homoskedastic nonlinear autoregressions (i) require the IID error
term to possess moments of all orders and (ii) imply that the observed process
yt also has finite moments of all orders. (To provide some further details, (ii) is
given as Corollary to Theorem 2 in Meitz and Saikkonen (2022). As for (i), see
Assumptions 3.3 and 2(a) of Douc et al. (2004) and Meitz and Saikkonen (2022),
respectively. These assumptions require the IID error terms to be sub-Weibull
random variables, which in turn entails they possess moments of all orders; see
Vladimirova et al. (2020, Def. 1 and Thm. 1) or Wong et al. (2020, Def. 3 and
Lem. 5).)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

14
MIKA MEITZ AND PENTTI SAIKKONEN
The abovementioned moment requirements are in stark contrast to ARCH-type
models. For instance, in the simplest ARCH(1) model (et = σtεt, σ 2
t = ω+α1e2
t−1,
and εt IID N(0,1)), the finiteness of moments of order 2r for et (E[|et|2r] < ∞)
is known to require the condition αr
1E[|εt|2r] < 1 (see, e.g., Ling and McAleer,
2002, Thm. 2.1; Ling, 1999, Exam. 6.1). For integer values of r, this condition is
equivalent with α1 < [(2r −1)!!]−1/r = [1·3·... ·(2r −1)]−1/r and consequently
all moments of the ARCH process et cannot be finite unless α1 = 0. The situation
is similar also in more complicated (G)ARCH and AR–(G)ARCH models (see,
e.g., Meitz and Saikkonen, 2008a, Thm. 2; Meitz and Saikkonen, 2008b, Thm. 1,
respectively). This suggests that ARCH-type heteroskedastic errors may not be
compatible with the moment requirements needed for subexponential ergodicity.
3.2.6. Potential Extensions.
Extending our results to allow for GARCH (and
not only ARCH) errors would be interesting. However, previous literature suggests
that studying the stability of nonlinear AR–GARCH models can be challenging.
Geometric ergodicity of nonlinear AR–GARCH models has previously been
studied by Liu, Li, and Li (1997), Ling (1999), Cline (2007), and Meitz and
Saikkonen (2008b); of these articles, the former two are confined to threshold AR–
GARCH models, whereas the latter two consider more general nonlinear autore-
gressions. In the present setting, the autoregressive part of the model we consider
is rather general (the restrictions imposed on function g(·) in Assumption 2(ii)
are quite mild, essentially restricting g(·) only for large values of its argument)
and techniques used for threshold models cannot be applied. Using an approach
similar to Cline (2007) appears challenging as the assumptions he employs are
quite general and appear difficult to verify (in fact, a threshold AR–GARCH model
is the only example that is explicitly treated in his article). On the other hand, Meitz
and Saikkonen (2008b) require certain structure and smoothness of the conditional
mean (see Assumption 2 of their paper), and it is not clear how to apply these results
in the current setting. As the extension to GARCH errors appears challenging, we
leave it for future research.
Another useful extension would be to consider the subgeometric ergodicity
of multivariate autoregressions with autoregressive conditional heteroskedasticity.
Fort and Moulines (2003, Sect. 2.2) and Douc et al. (2004, Sect. 3.3) already stud-
ied multivariate first-order autoregressions with IID errors and obtained results for
polynomial and subexponential ergodicity, respectively. In principle, generalizing
these results to the higher-order case with multivariate ARCH errors should be
possible, but it is not immediate how to formulate a general model that would
be both theoretically manageable and useful in practical applications. We hope to
return to this issue in subsequent work.
3.3. Examples
The conditional mean of the model we have so far discussed is very general, and
we next consider some concrete illustrating examples. The following two special
cases were introduced in Meitz and Saikkonen (2022, Sect. 5) in the case of a
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
15
homoskedastic error term. We first consider a model with a time-varying intercept
term based on a logistic function and specified as
yt = ν1L(ut−1;γ,a1)+ν2(1−L(ut−1;γ,a2))+yt−1
+πt−1yt−1 +···+πp−1yt−p+1 +σtεt,
(24)
where L(u;γ,a) = 1/(1+e−γ (u−a)) is the logistic function and the parameters γ ,
a1, a2 are assumed to satisfy γ > 0 and a1 ≤a2, and ν1, ν2 are assumed to satisfy
ν1 < 0 < ν2. Moreover,  signifies the difference operator (so that yt−1 = yt−1 −
yt−2), and the remaining notation is as in model (3). Arguments similar to those
in Meitz and Saikkonen (2022, Proof of Prop. 1) can now be used to prove the
following result (for details, see Appendix A).
Proposition 1. Consider the process yt defined in equation (24) and suppose
that Assumptions 1, 2(i), 3, and 4 hold. Then, yt is polynomially ergodic with
convergence rate r(n) = n2s0−1 and finite moments up to order 2s0 −1.
The convergence rate presented in Proposition 1 also shows the rate of β-mixing
coefficients.
As another special case, we consider a model with a time-varying slope term
defined as
yt = πt−1yt−1 +···+πp−1yt−p+1 +S(ut−1)ut−1 +σtεt,
(25)
where S(ut−1) is either S1(ut−1) = 1−r0/h(ut−1) or S2(ut−1) = exp{−r0/h(ut−1)}
(with r0 > 0) and the function h : R →(0,∞) as defined in Proposition 2 of
Meitz and Saikkonen (2022, Sect. 5.2). In addition to a general formulation
of the function h that proposition provides six special cases of which two are
h(u) = 1 + |u −a|ρ and h(u) = (1 + (u −a)2)ρ/2 (where a ∈R and ρ ∈(0,2);
see Assumption 2). Regarding the remaining notation, it is as in model (3).
The following result can be established by using arguments similar to those in
the proof of Proposition 2 in Meitz and Saikkonen (2022, Sect. 5.2) (for details,
see Appendix A).
Proposition 2. Consider the process yt defined in equation (25) and suppose
that Assumptions 1, 2(i), 3, and 4 hold. Then, yt is polynomially ergodic with
convergence rate r(n) = n2s0/ρ−1 and finite moments up to order 2s0 −ρ.
The rate of β-mixing coefficients coincides with the rate given in the proposi-
tion. As the function h depends on the parameter ρ ∈(0,2), the convergence rate in
Proposition 2 differs from that obtained in Proposition 1 except in the case ρ = 1.
4. EMPIRICAL APPLICATION
Although theoretical work on subgeometric ergodicity has been ongoing for
four decades, practical illustrations of (homoskedastic) subgeometrically ergodic
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

16
MIKA MEITZ AND PENTTI SAIKKONEN
autoregressions have been scarce; we are not aware of any previous empirical
applications of subgeometrically ergodic autoregressions using real data. A small
illustration of simulated data from one subgeometrically ergodic autoregression is
given in Fort and Moulines (2003, Sect. 3). Meitz and Saikkonen (2022, Sect. 5)
provide examples of some concrete subgeometrically ergodic autoregressive time
series models and illustrations of a few simulated data series from them. These
simulation exercises suggest that subgeometrically ergodic autoregressions could
be useful when the observed time series bears some resemblance to unit-root type
behavior and the autocorrelation function indicates very strong persistence, but
when the time series nevertheless exhibits eventual mean-reverting behavior. The
discussion in Section 2.1 around equation (6) had a similar message, suggesting
that these models could be seen as a middle ground between the extreme cases of
geometric ergodicity and non-ergodicity.
We next illustrate the use of subgeometrically ergodic AR–ARCH models in
a small empirical example. Our aim is simply to provide a proof of concept
for the applicability of subgeometrically ergodic AR–ARCH models, illustrating
that the model used fits the data well. Further work is certainly needed to judge
the usefulness of these models in practical applications, but we leave such more
comprehensive empirical applications for future research.
The data we employ consist of daily observations on the Chicago Board
Options Exchange energy sector volatility index (https://fred.stlouisfed.org/series/
VXXLECLS) over the period 16 March 2011 to 31 December 2021 (a total of
2,719 observations). This data series reflects energy sector risk and is displayed in
the top-left graph of Figure 1 (the solid graph; the dashed horizontal line shows the
estimate ˆa = 25.366, see (26) and (27) below). The time series plot shows signs
of strong persistence, which is also reflected in the autocorrelation function of the
data shown in the top-right graph of Figure 1.
We model this data series using the parametric specification in (24). As for the
error distribution, after some experimentation, a skew version of the t-distribution
due to Jones and Faddy (2003) was found to provide a good fit (in contrast,
estimation with normal errors led to a distinct discrepancy between the residual
distribution and the Gaussian one). The density function of this distribution is
f(x;c,d) = C−1
c,d

1+
x
(c+d +x2)1/2
c+1/2
1−
x
(c+d +x2)1/2
d+1/2
,
where c and d are positive parameters and Cc,d = 2c+d−1B(c,d)(c + d)1/2 (with
B(·,·) denoting the beta function); the case c = d results in a symmetric
t-distribution with 2c degrees of freedom and the cases c < d and c > d imply
skewness to the left and right, respectively. In our application, we use this
distribution centralized to have mean zero and standardized to have unit variance
(i.e., in the density function, x is replaced by sx + m and C−1
c,d by sC−1
c,d, where m
and s2 denote the mean and variance [see Jones and Faddy, 2003, Sect. 2.1]; this
requires that c > 1 and d > 1, for a moment of order k is finite when c > k/2 and
d > k/2).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
17
2012
2014
2016
2018
2020
20 40 60 80
120
0
50
100
150
200
250
0.0 0.2 0.4 0.6 0.8 1.0
0
20
40
60
80
100
120
140
−0.2 −0.1
0.0
0.1
2012
2014
2016
2018
2020
−0.20 −0.10 0.00
0.10
2012
2014
2016
2018
2020
0
5
10
15
20
2012
2014
2016
2018
2020
−4 −2 0
2
4
6
Figure 1. Top row: Daily observations on the Chicago Board Options Exchange energy sector
volatility index, 16 March 2011 to 31 December 2021 (left); the corresponding autocorrelation function
(right). Middle row: The function I(x) = −νL(x;γ,a)+ν(1−L(x;γ,a)) (left) and the corresponding
time-varying intercept term I(yt−1) (right), based on parameter estimates in (27). Bottom row: The
estimated volatility series ˆσt (left) and residual series ˆεt (right), based on parameter estimates in (27).
We estimate the model parameters using the method of maximum likelihood and
employ optimization routines in R. (We simply assume that standard properties of
maximum likelihood estimators hold and calculate standard errors based on the
standard formulas.) Trying out different model orders led to model (24) with order
p = 1 and with a nonlinear ARCH term of order q = 3 (with these choices, the
residual diagnostics shown in Figure B1 in Appendix B indicated a very good fit).
Specifically, the considered model is
yt = yt−1 −νL(yt−1;γ,a)+ν(1−L(yt−1;γ,a))+σtεt,
σ 2
t = (ω +α1e2
t−1 +α2e2
t−2 +α3e2
t−3)L(yt−1;γ,a),
(26)
where the errors εt are IID(0,1) and follow the above described (centralized
and standardized) skew t-distribution, L(y;γ,a) = 1/(1+e−γ (y−a)) is the logistic
function, the parameters ν and γ are positive, and a ∈R. (We also tried a model
where the logistic functions in the conditional expectation and in the ARCH term
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

18
MIKA MEITZ AND PENTTI SAIKKONEN
were different, but this extension had only a minor effect on the results.) ML
estimation (with the constraints ν,γ,ω > 0, α1,α2,α3 ≥0, and α1 + α2 + α3 < 1
that ensure polynomial ergodicity by Proposition 1) leads to the following results:
yt = yt−1 −0.187
(0.040)L(yt−1;0.171
(0.018),25.366
(1.434) )+0.187
(0.040)(1−L(yt−1;0.171
(0.018),25.366
(1.434) ))+ ˆσtˆεt,
ˆσ 2
t = (3.259
(0.493) +0.406
(0.081)e2
t−1 +0.310
(0.066)e2
t−2 +0.149
(0.052)e2
t−3)L(yt−1;0.171
(0.018),25.366
(1.434) ),
(27)
where the numbers in parentheses are standard errors; estimates for the parameters
in the error distribution are ˆc = 3.551(0.422) and ˆd = 2.138(0.197).
To illustrate the conditional mean of the estimated model, consider the func-
tion I(x) = −νL(x;γ,a) + ν(1 −L(x;γ,a)) and the corresponding time-varying
intercept term I(yt−1) based on the above parameter estimates. These are shown
in the middle row of Figure 1. In the left panel, the two horizontal dashed lines
show the minimum and maximum I(x) attains, while the three vertical dashed
lines indicate the minimum of the observed data series yt (11.71), the estimate
ˆa = 25.366, and the maximum of yt (130.61). On the right, the three horizontal
dashed lines show the minimum and maximum I(yt−1) attains (−0.187 and 0.154)
and the origin. Intuitively, when yt−1 is close to ˆa, the time-varying intercept term
I(yt−1) is close to zero and the conditional mean of (27) corresponds to unit-root
type behavior (without drift); when yt−1 takes values clearly below/above ˆa, the
intercept I(yt−1) is positive/negative and behavior akin to a unit-root process with
increasing/decreasing drift occurs.
The left panel in the bottom row of Figure 1 displays the estimated volatility
series ˆσt. The variation of the volatility over time is strong, and the large spikes
in the volatility series coincide with the large values in the observed series. The
logistic formulation of the conditional variance in (27) makes it possible for large
observations to amplify volatility more than a standard linear ARCH model would
allow for.
The right panel in the bottom row of Figure 1 shows the residual series ˆεt. Four
additional graphs analyzing the residuals are available in Figure B1 in Appendix B:
autocorrelation functions of the residuals and of the squared residuals, together
with a histogram and a Q–Q plot. The autocorrelation functions reveal that the
very strong persistence present in the original series has been quite well captured
by the estimated subgeometrically ergodic AR–ARCH model (only three of the
shown 100 autocorrelation coefficients are barely outside the displayed critical
values). The histogram and the Q–Q plot indicate that the employed skew version
of the t-distribution fits well as only a few outlying observations deviate from the
estimated density function and the 45 degree line.
Note also that the estimated AR–ARCH model satisfies the requirements of a
stationary and polynomially ergodic process with finite absolute moments.4 It may
4That is, the parameter estimates in (27) correspond to a process satisfying the requirements of Proposition 1 with
s0 = 1. (Note that these requirements are not satisfied with s0 = 1.5, which would correspond to finite second moments
of yt.)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
19
be interesting to note that estimation attempts using standard linear ARMA(1,1)–
GARCH(1,1) models (with skew t errors) led to estimated autoregressive coeffi-
cients in excess of 0.999, reflecting the very persistent nature of the data series
apparent from the time series and autocorrelation plots in the top row of Figure 1.
The primary purpose of this small empirical example was to demonstrate what
kind of time series could be modeled with subgeometrically ergodic AR–ARCH
models. It is worth pointing out that such models may work well even in cases
where the graphs of the employed time series and related autocorrelation functions
look very different from those displayed in Figure 1.
5. CONCLUSIONS
In this paper, we examined the subgeometric ergodicity of nonlinear autoregressive
models with autoregressive conditional heteroskedasticity. We provided conditions
that ensured polynomial ergodicity of the considered AR–ARCH models. Our
results generalized existing results that assumed the error terms to be IID. The use
of subgeometrically ergodic AR–ARCH models was illustrated in an empirical
example using energy sector volatility index data.
Several future research topics could be entertained. In this paper, we have only
considered ARCH-type conditional heteroskedasticity, and extending the results to
the generalized ARCH (GARCH) case would be of interest. Subgeometric ergod-
icity of multivariate autoregressions with autoregressive conditional heteroskedas-
ticity is another interesting topic left for future work. On the empirical side, further
applied work is certainly needed to judge the usefulness of subgeometrically
ergodic autoregressions (with or without ARCH) in practical applications. For
instance, providing more concrete advice on when to use subgeometrically (rather
than geometrically) ergodic autoregressions would be useful for practitioners.
Another question future applications should address is whether subgeometrically
ergodic autoregressions can outperform relevant competing models in out-of-
sample forecasting exercises.
A. APPENDIX A
This appendix contains the proofs of Lemma 2, Theorem 1, and Propositions 1 and 2 as
well as details for the finiteness of moments in Section 3.2.
Proof of Lemma 2. Define the vector (¯α1,..., ¯αq) = (α1 ¯μ2s0b,...,αq ¯μ2s0b), and let
¯ denote the q × q matrix obtained by replacing the first row of the matrix t by
(¯α1,..., ¯αq). By assumption, ¯α1,..., ¯αq ≥0 and q
i=1 ¯αi < 1. These conditions ensure that
the polynomial p(t) = tq −¯α1tq−1 −··· −¯αq has all its roots inside the unit circle (if a
root t with |t| ≥1 existed, the contradiction 1 = ¯α1/t +···+ ¯αq/tq ≤¯α1 +···+ ¯αq would
follow); this in turn implies that the matrix ¯ has spectral radius ρ( ¯) < 1 (see Horn and
Johnson, 2013, pp. 194–195). Therefore, the matrix Iq −¯ is invertible with (Iq −¯)−1 =
∞
i=0 ¯i. Set 1q = (1,...,1) (q × 1) and let (x)abs = (|x1|,...,|xq|) (q × 1) denote the
elementwise absolute value of a vector x ∈Rq. We define the vector norm ∥· ∥• on Rq as
∥x∥• = 1′q(Iq−¯)−1(x)abs. Note that as (Iq−¯)−1 = ∞
i=0 ¯i with ¯ having nonnegative
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

20
MIKA MEITZ AND PENTTI SAIKKONEN
(and also some strictly positive) entries, ∥x∥• = 1′q(Iq −¯)−1(x)abs > 1′q(x)abs = ∥x∥1
whenever x ̸= 0 (here ∥·∥1 denotes the usual l1 vector norm).
Now, let x ̸= 0 be arbitrary and consider ∥tx∥•. To this end, note that |α1ε2t x1 +
··· + αqε2t xq| ≤α1ε2t |x1| + ··· + αqε2t |xq|, which implies that the elementwise inequality
(tx)abs ≤t(x)abs holds (with probability one; note that only the first elements differ).
Thus, also,
∥tx∥• = 1′
q(Iq −¯)−1(tx)abs ≤1′
q(Iq −¯)−1t(x)abs.
As bs0 ≥1, Minkowski’s inequality and the definition of the vector (¯α1,..., ¯αq) yield
E[{1′
q(Iq −¯)−1t(x)abs}bs0]1/bs0 ≤1′
q(Iq −¯)−1 ¯(x)abs,
where, as (Iq −¯)−1 ¯ = (Iq −¯)−1 −Iq and ∥x∥• > ∥x∥1,
1′
q(Iq −¯)−1 ¯(x)abs = ∥x∥• −∥x∥1 = ∥x∥•(1−∥x∥1/∥x∥•) < ∥x∥•.
These derivations establish that
∥tx∥•Lbs0 = (E[∥tx∥bs0
• ])1/bs0 < ∥x∥•
and that
|||t|||•Lbs0 =
max
∥x∥•Lbs0 =1∥tx∥•Lbs0 = max
∥x∥•=1∥tx∥•Lbs0 < 1.
Finally, by its definition, it is clear that the vector norm ∥·∥• is monotone.
□
Proof of Theorem 1. For clarity, we break down the long proof into several intermediate
steps.
Step 1: Preliminaries. We first consider the function V defined in (22) and the condi-
tional expectation E[V(y1) | y0 = x]. As before, we decompose an x ∈Rp+q to its p- and
q-dimensional components as x = (x1,x2); similarly, we decompose y1 as y1 = (y1,1,y2,1).
For any x ∈Rp+q, it is convenient to define
V1(x1) = |z1(x1)|2s0,
V2(x1) = s1∥z2(x1)∥2s0α
∗
,
and
V3(x) = s2∥ξ(x)∥bs0
•
so that V(x) = 1+V1(x1)+V2(x1)+V3(x) (when p = 1, we can set s1 = 0 and drop z2(x1)
and V2). We next consider the three conditional expectations
E[V1(y1,1) | y0 = x] = E[|z1(y1,1)|2s0 | y0 = x],
(28)
E[V2(y1,1) | y0 = x] = E[s1∥z2(y1,1)∥2s0α
∗
| y0 = x],
(29)
E[V3(y1) | y0 = x] = E[s2∥ξ(y1)∥bs0
•
| y0 = x]
(30)
related to functions V1, V2, and V3. In Steps 2–4 below, we establish that these conditional
expectations can be bounded from above using the following upper bounds:
E[|z1(y1,1)|2s0 | y0 = x] ≤|z1(x1)|2s0 −˜r|z1(x1)|2s0α +C∥ξ(x)∥bs0
•
+C,
(31)
E[s1∥z2(y1,1)∥2s0α
∗
| y0 = x] ≤s1 ∥z2(x1)∥2s0α
∗
−˜ϖsα
1 ∥z2(x1)∥2s0α2
∗
+ ˜s1 |z1(x1)|2s0α +C,
(32)
E[s2∥ξ(y1)∥bs0
•
| y0 = x] ≤s2 ∥ξ(x)∥bs0
•
−˜λs2 ∥ξ(x)∥bs0
•
+C,
(33)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
21
where ˜r, ˜ϖ,˜s1, ˜λ > 0 with ˜λ < 1 and where ˜s1 can be made as close to zero as desired by
choosing a small enough s1 (and ˜s1 = 0 when p = 1). Moreover, here and in what follows,
for simplicity, we use C to denote a finite positive constant whose value may change from
occurrence to occurrence (alternatively, we could use C1,C2,...). For brevity, we also often
(but not always) drop the argument x1 from z1(x1) and z2(x1) and simply write z1 and z2.
For ease of reference, we also note here that Assumption 4 allows us to bound the
conditional variance as follows. By the definition of σ 2t in (13), Assumption 3, and definition
of ξ(yt−1) in (14), σ 2t ≤ω +e2
t−1 +···+e2
t−q = ω +∥ξ(yt−1)∥1 (with ∥·∥1 denoting the
usual l1 vector norm). The equivalence of vector norms on Rq and the fact that ω is a (finite)
constant implies that (for some finite constant C)
σ 2
t = σ 2(yt−1) ≤C(1+∥ξ(yt−1)∥•) a.s.
and
σ 2(x) ≤C(1+∥ξ(x)∥•) for all fixed x.
(34)
Step 2: Upper bound for V1. Using (12), the conditional expectation in (28) can be
expressed as
E[V1(y1,1) | y0 = x] = E[|g(z1(x1))+σ(x)ε1|2s0].
For any positive real number Z, define the set S1(Z) = {x ∈Rp+q : |z1(x1)| ≤Z} and let
Sc
1(Z) denote the complement of this set.
First, consider values of x such that x ∈Sc
1(Z) so that |z1| = |z1(x1)| > Z. Choose Z large
enough to ensure that g(z1) ̸= 0 (Assumption 2) so that |g(z1)+σ(x)ε1|2s0 can be written
as
|g(z1)|2s0 |1+σ(x)ε1/g(z1)|2s0.
(35)
We first bound the latter term in this expression using the following extension of Bernoulli’s
inequality due to Fefferman and Shapiro (1972): for any a ≥2, there exist positive numbers
A and B such that
|1+u|a ≤1+au+Au2 +B|u|a
(36)
for all u ∈R. Using (36), the latter term in (35) is dominated by
1+2s0
σ(x)
g(z1)ε1 +A σ(x)2
g(z1)2 ε2
1 +B σ(x)2s0
|g(z1)|2s0 |ε1|2s0.
This upper bound, (35), the facts E[ε1] = 0 and E[ε2
1] = 1 (Assumption 1), and the notation
μ2s0 = E[|ε1|2s0], now yield
E[V1(y1,1) | y0 = x] ≤|g(z1)|2s0 +A|g(z1)|2s0−2σ(x)2 +Bσ(x)2s0μ2s0.
Choose Z large enough to ensure that 0 < 1−r|z1|−ρ < 1 and |g(z1)| ≤(1−r|z1|−ρ)|z1|
(Assumption 2). Using the elementary inequalities (1−u)a1 ≤1−u and (1−u)a2 ≤1 for
all 0 < u < 1, a1 ≥1, and a2 ≥0, and recalling that s0 ≥1 and σ 2(x) ≤C(1+∥ξ(x)∥•)
(see (34)), we obtain
E[V1(y1,1) | y0 = x]
≤|z1|2s0 −r|z1|2s0−ρ +C|z1|2s0−2 +C|z1|2s0−2∥ξ(x)∥• +C∥ξ(x)∥s0• μ2s0
for some positive C (by choosing Z large enough, the constant term on the dominant side has
been absorbed into |z1|2s0). To merge the terms −r|z1|2s0−ρ and C|z1|2s0−2, by choosing
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

22
MIKA MEITZ AND PENTTI SAIKKONEN
Z large enough to ensure that C/r|z1|2−ρ < 1, we have
−r|z1|2s0−ρ +C|z1|2s0−2 = −r|z1|2s0−ρ(1−C/r|z1|2−ρ) ≤−ˆr|z1|2s0−ρ
for some positive constant ˆr. Hence,
E[V1(y1,1) | y0 = x]
(37)
≤|z1|2s0 −ˆr|z1|2s0−ρ +C|z1|2s0−2∥ξ(x)∥• +C∥ξ(x)∥s0• μ2s0 for all x ∈Sc
1(Z).
Now, consider values of x such that x ∈S1(Z). As inequality (5) implies that g(z1) is
bounded on S1(Z), triangle inequality and the elementary inequality

m

i=i
ai

r
≤cr
m

i=i
|ai|r
where cr = 1 for 0 < r ≤1 and cr = mr−1 for r > 1
(38)
for any real numbers a1,...,am (see, e.g., Davidson, 1994, p. 140) imply that |g(z1) +
σ(x)ε1|2s0 is dominated by C(1+σ 2s0(x)|ε1|2s0) (for some C > 0; we omit this statement
from now on) for all x ∈S1(Z). As μ2s0 = E[|ε1|2s0] and σ 2(x) ≤C(1+∥ξ(x)∥•), it is
seen that
E[V1(y1,1) | y0 = x] ≤C(1+∥ξ(x)∥s0• μ2s0)
for all
x ∈S1(Z).
(39)
Combining (37) and (39), noting that 2s0 −ρ = 2s0α (see the discussion following (22)),
and merging constants, we can conclude that for all x ∈Rp+q,
E[V1(y1,1) | y0 = x] ≤|z1|2s0 −ˆr|z1|2s0α +C|z1|2s0−2∥ξ(x)∥• +C∥ξ(x)∥s0• +C.
(40)
For future developments, it is convenient to further manipulate this upper bound. First,
consider the product |z1|2s0−2∥ξ(x)∥• appearing in (40) and momentarily focus on the case
s0 > 1 (when also b > 1). Using Young’s inequality (with exponents bs0/(bs0 −1) and bs0)
yields
|z1|2s0−2∥ξ(x)∥• ≤bs0 −1
bs0
|z1|2s0b(s0−1)/(bs0−1) + 1
bs0
∥ξ(x)∥bs0
• .
Simple calculations show that the assumption b > (2s0 −ρ)/[s0(2 −ρ)] implies that
2s0b(s0 −1)/(bs0 −1) < 2s0α. Therefore, for some small positive ς,
|z1|2s0−2∥ξ(x)∥• ≤C(1+|z1|2s0α−ς +∥ξ(x)∥bs0
• );
(41)
clearly, this upper bound also holds in the case s0 = 1.
Second, consider the terms involving |z1|2s0α in (40) and |z1|2s0α−ς in (41). By
considering values of |z1| larger and smaller than some large bound, it is straightforward to
see that
C|z1|2s0α−ς −ˆr|z1|2s0α = −ˆr(1−C/[ˆr|z1|ς])|z1|2s0α ≤C −˜r|z1|2s0α
for some positive constant ˜r. Third, the term ∥ξ(x)∥s0• appearing in (40) is clearly dominated
by a term of the form C +∥ξ(x)∥bs0
• .
Inequality (40) together with these additional manipulations leads to the final upper
bound
E[V1(y1,1) | y0 = x] ≤|z1|2s0 −˜r|z1|2s0α +C∥ξ(x)∥bs0
•
+C
(42)
which holds for all x ∈Rp+q.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
23
Step 3: Upper bound for V2. Using (12), we can express the conditional expectation in
(29) as
E[V2(y1,1) | y0 = x] = s1∥1z2(x1)+z1(x1)ιp−1∥2s0α
∗
.
Recall that α = 1 −ρ/2s0 ∈(0,1) (because s0 ≥1 and ρ ∈(0,2) by assumption) and that
|||1|||∗≤ϖ for some 0 < ϖ < 1 (by Lemma 1). These facts together with elementary
inequalities (and dropping the argument x1 from z1(x1) and z2(x1)) imply that
1z2 +z1ιp−1
α
∗≤∥1z2∥α
∗+
z1ιp−1
α
∗≤ϖ α ∥z2∥α
∗+
ιp−1
α
∗|z1|α.
This, together with the convexity of the function |x| →|x|2s0 (recall that s0 ≥1 by
assumption), imply that for any τ1 ∈(0,1) and τ2 = 1−τ1,
s1
1z2 +z1ιp−1
2s0α
∗
≤
⎛
⎝τ2
s1/2s0
1
ϖ α
τ2
∥z2∥α
∗+τ1
s1/2s0
1
ιp−1
α
∗
τ1
|z1|α
⎞
⎠
2s0
≤τ2
s1ϖ 2s0α
τ2s0
2
∥z2∥2s0α
∗
+τ1
s1
ιp−1
2s0α
∗
τ2s0
1
|z1|2s0α.
(43)
Consider the former term on the dominant side of (43). Fix a τ2 such that τ2 ∈(ϖ α,1)
and set ˜ϖ = 1 −(ϖ α/τ2)2s0 ∈(0,1). Then the former term on the dominant side of (43)
satisfies
τ2
s1ϖ 2s0α
τ2s0
2
∥z2∥2s0α
∗
= τ2s1(1−˜ϖ)∥z2∥2s0α
∗
< s1 ∥z2∥2s0α
∗
−˜ϖs1 ∥z2∥2s0α
∗
.
(44)
Suppose now that s1 is any fixed (but potentially arbitrarily small) positive number. If ∥z2∥∗
is large enough to ensure that s1 ∥z2∥2s0α
∗
≥1, then s1 ∥z2∥2s0α
∗
≥sα
1 ∥z2∥2s0α2
∗
as α ∈(0,1)
and the right side of (44) is dominated by s1 ∥z2∥2s0α
∗
−˜ϖsα
1 ∥z2∥2s0α2
∗
. On the other hand,
if s1 ∥z2∥2s0α
∗
< 1, the right side of (44) is bounded by a constant. Therefore,
τ2
s1ϖ 2s0α
τ2s0
2
∥z2∥2s0α
∗
< s1 ∥z2∥2s0α
∗
−˜ϖsα
1 ∥z2∥2s0α2
∗
+C.
Now, consider the latter term on the dominant side of (43). Choosing a small enough fixed
s1, this term can be made smaller than ˜s1 |z1|2s0α where ˜s1 can be chosen as close to zero
as desired. To summarize, it holds that
E[V2(y1,1) | y0 = x] ≤s1 ∥z2∥2s0α
∗
−˜ϖsα
1 ∥z2∥2s0α2
∗
+ ˜s1 |z1|2s0α +C,
(45)
where ˜ϖ ∈(0,1) and the value of ˜s1 > 0 can be chosen as close to zero as desired.
Step 4: Upper bound for V3. By the definition of the function ξ in (14), ξ(y1) =
ζ,1(y0)ξ(y0) + ωζ,1. We start by bounding both terms on the right-hand side of this
equality and, for simplicity, remove the argument y0 and instead use the notations ζ,1
and ξ0.
First, denote vζ,1 = ε2
1(α1ζ1,0e2
0 +···+αqζq,0e2
1−q) and v1 = ε2
1(α1e2
0 +···+αqe2
1−q).
Using the definitions of the matrices ζ,1 and 1 and the vector ξ0 (see (14) and (16)), we
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

24
MIKA MEITZ AND PENTTI SAIKKONEN
then have
ζ,1ξ0 = (vζ,1,e2
0,...,e2
1−q)
and
1ξ0 = (v1,e2
0,...,e2
1−q),
where all components of both vectors are nonnegative. As ζi,0 ∈(0,1] for all i = 1,...,q
by assumption, we have vζ,1 ≤v1 (a.s.). The monotonicity of the norm ∥· ∥• required in
Assumption 4 now implies that
ζ,1ξ0
• ≤
1ξ0
• (a.s.) (see the discussion preceding
Assumption 4). Regarding the vector ωζ,1, its first component is ζ0,0(y0)ε2
1ω ≤ε2
1ω (a.s.)
and the other components are zero, so that the monotonicity of the norm ∥· ∥• shows that
∥ωζ,1∥• ≤∥ω1∥• = ε2
1∥ω∥• (a.s.) where ω = (ω,0,...,0).
The preceding discussion together with the triangle inequality now yields, with proba-
bility one,
∥ξ(y1)∥• = ∥ζ,1(y0)ξ(y0)+ωζ,1∥• ≤∥ζ,1(y0)ξ(y0)∥• +∥ωζ,1∥• ≤∥1ξ(y0)∥• +ε2
1∥ω∥•.
Using the notation ¯μ2bs0 = (E[|ε0|2bs0])1/bs0 and Minkowski’s inequality, we find that

E[∥ξ(y1)∥bs0
•
| y0 = x]
1/bs0 ≤

E[∥1ξ(x)∥bs0
• ]
1/bs0 + ¯μ2bs0∥ω∥•.
By inequality (18) and Assumption 4, the first term on the dominant side satisfies

E[∥1ξ(x)∥bs0
• ]
1/bs0 = ∥1ξ(x)∥•Lbs0 ≤|||1|||•Lbs0 ∥ξ(x)∥• = λ∥ξ(x)∥•
with λ < 1. The preceding steps imply that

E[∥ξ(y1)∥bs0
•
| y0 = x]
1/bs0 ≤λ∥ξ(x)∥• + ¯μ2bs0∥ω∥• = λ∥ξ(x)∥•

1+ ¯μ2bs0∥ω∥•
λ∥ξ(x)∥•

,
and, using (30) and the notation ˆλ = λbs0 < 1, we obtain
E[V3(y1) | y0 = x] ≤s2ˆλ∥ξ(x)∥bs0
•

1+ ¯μ2bs0∥ω∥•
λ∥ξ(x)∥•
bs0
= s2∥ξ(x)∥bs0
•
−

1−ˆλ

1+ ¯μ2bs0∥ω∥•
λ∥ξ(x)∥•
bs0
s2 ∥ξ(x)∥bs0
•
.
As ˆλ ∈(0,1), we can choose a ξ > 0 such that the term in curly brackets is larger than some
˜λ ∈(0,1) whenever ∥ξ(x)∥• > ξ. Therefore, whenever ∥ξ(x)∥• > ξ, we have
E[V3(y1) | y0 = x] ≤s2 ∥ξ(x)∥bs0
•
−˜λs2 ∥ξ(x)∥bs0
•
.
On the other hand, whenever ∥ξ(x)∥• ≤ξ, the previous derivations also make it clear that
E[V3(y1) | y0 = x] is bounded by some constant. Therefore, for all x ∈Rp+q,
E[V3(y1) | y0 = x] ≤s2 ∥ξ(x)∥bs0
•
−˜λs2 ∥ξ(x)∥bs0
•
+C.
(46)
Step 5: Upper bound for V. We next combine the upper bounds (42), (45), and (46) derived
in Steps 2–4 to obtain
E[V(y1) | y0 = x] ≤1+|z1|2s0 −˜r|z1|2s0α +C∥ξ(x)∥bs0
•
+s1 ∥z2∥2s0α
∗
−˜ϖsα
1 ∥z2∥2s0α2
∗
+ ˜s1 |z1|2s0α
+s2∥ξ(x)∥bs0
•
−˜λs2∥ξ(x)∥bs0
•
+C.
(47)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
25
To combine the terms involving |z1|2s0α, set ¯r = ˜r −˜s1 so that
−˜r|z1|2s0α + ˜s1|z1|2s0α = −¯r|z1|2s0α;
recalling that ˜s1 can be chosen as close to zero as desired, we have ¯r > 0 by a suitable
choice of ˜s1. On the other hand, to manipulate the terms involving ∥ξ(x)∥bs0
• , choose s2
large enough to ensure that ˜λ−C/s2 ∈(0,1) and set ¯λ = ˜λ−C/s2. As now −¯λs2 = C−˜λs2,
the terms involving ∥ξ(x)∥bs0
•
in (47) can be written as
s2∥ξ(x)∥bs0
•
+C∥ξ(x)∥bs0
•
−˜λs2∥ξ(x)∥bs0
•
= s2∥ξ(x)∥bs0
•
−¯λs2∥ξ(x)∥bs0
• .
(48)
Whenever s2∥ξ(x)∥bs0
•
< 1, (48) is bounded by the constant 1; for s2∥ξ(x)∥bs0
•
≥1, we have
s2∥ξ(x)∥bs0
•
≥sα
2∥ξ(x)∥bs0α
•
as α ∈(0,1). Thus, the expression in (48) is always bounded
by 1 + s2∥ξ(x)∥bs0
•
−¯λsα
2∥ξ(x)∥bs0α
•
. As V(x) = 1 + |z1|2s0 + s1 ∥z2∥2s0α
∗
+ s2∥ξ(x)∥bs0
• ,
we obtain from (47) that
E[V(y1) | y0 = x] ≤V(x)−(1+ ¯r|z1|2s0α + ˜ϖsα
1 ∥z2∥2s0α2
∗
+ ¯λsα
2∥ξ(x)∥bs0α
•
)+C.
(49)
The inequality in (38) implies that
[V(x)]α = (1+|z1|2s0 +s1 ∥z2∥2s0α
∗
+s2∥ξ(x)∥bs0
• )α
≤1+|z1|2s0α +sα
1 ∥z2∥2s0α2
∗
+sα
2∥ξ(x)∥bs0α
•
so that, setting e = min{¯r, ˜ϖ, ¯λ} ∈(0,1), we have
e[V(x)]α ≤1+ ¯r|z1|2s0α + ˜ϖsα
1 ∥z2∥2s0α2
∗
+ ¯λsα
2∥ξ(x)∥bs0α
•
.
Therefore, setting ˜e = e/2,
E[V(y1) | y0 = x] ≤V(x)−˜e[V(x)]α +

C −˜e[V(x)]α
.
Now, define the set
AN =

x ∈Rp+q : |z1(x1)|2s0 ≤N,
∥z2(x1)∥2s0α
∗
≤N,
∥ξ(x)∥bs0α
•
≤N

,
(50)
where N is so large that AN is nonempty (see (4)). The complement of AN is denoted by Ac
N
so that x ∈Ac
N if either |z1(x1)|2s0 > N, ∥z2(x1)∥2s0α
∗
> N, or ∥ξ(x)∥bs0α
•
> N. By choosing
a large enough N, for all x ∈Ac
N, it holds that C−˜e[V(x)]α < 0 so that E[V(y1) | y0 = x] ≤
V(x) −˜e[V(x)]α for all x ∈Ac
N. On the other hand, the function V(x) −e[V(x)]α + C is
clearly bounded by some positive constant ˜b on AN. Therefore, we can conclude that there
exist an N and a positive constant ˜b such that
E[V(y1) | y0 = x] ≤V(x)−φ1 (V(x))+ ˜b1AN (x),
(51)
where φ1(v) = ˜evα. This implies that Condition D holds with φ = φ1 and C = AN.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

26
MIKA MEITZ AND PENTTI SAIKKONEN
Step 6: Showing that AN is petite. We first note that the definition of a petite set and
other Markov chain concepts we refer to below can be found in Meyn and Tweedie (2009,
Chaps. 4–6). The idea is to establish that the (potentially non-compact) set AN in (50) is
petite for any N ≥1 so large that AN is nonempty. To this end, we show below that there
exists an MN < ∞such that
sup
x∈AN
E[∥yp+q∥2s0α
1
| y0 = x] < M2s0α
N
,
(52)
where ∥·∥1 denotes the usual l1 vector norm. Next note that Theorem 2.2(ii) of Cline and
Pu (1998) along with our Assumption 1 shows that the Markov chain yt is a ψ-irreducible
and aperiodic T-chain (see also Example 2.1 of the aforementioned paper). Therefore, the
compact set BN = {x ∈Rp+q : ∥x∥1 ≤MN} is small (see Meyn and Tweedie, 2009, Thms.
6.2.5(ii) and 5.5.7). Moreover, due to Markov’s inequality,
inf
x∈AN
Pr[yp+q ∈BN | y0 = x] = 1−sup
x∈AN
Pr[∥yp+q∥1 ≥MN | y0 = x]
≥1−sup
x∈AN
E[∥yp+q∥2s0α
1
| y0 = x]/M2s0α
N
,
where the last expression is positive due to (52). Proposition 5.2.4(i) of Meyn and Tweedie
(2009) now implies that the set AN is small. Proposition 5.5.3 of the same reference therefore
implies that the set AN is also petite.
To complete the proof of petiteness of AN, it remains to establish (52). First, we introduce
some notation. We let |||·|||1 denote the maximum column sum norm defined for real square
matrices (this norm is induced by the l1 vector norm; see Horn and Johnson, 2013, Sect.
5.6). For brevity, we denote zt = z(y1,t) = Ay1,t and also partition the p-dimensional zt as
zt = (z1,t,z2,t) (see (11)). This allows us to write the companion form (12) as
zt =

z1,t
z2,t

=

g(z1,t−1)+σtεt
1z2,t−1 +z1,t−1ιp−1

.
(53)
Finally, we set ⃗z1,p+q = (z1,p+q,...,z1,1) and ⃗z2,p+q = (z2,p+q,...,z2,1).
Now, consider the norm ∥yp+q∥1 in (52). Using properties of the norms ∥·∥1 and |||·|||1,
we can write
∥yp+q∥1 ≤
p+q

t=1
∥y1,t∥1
=
p+q

t=1
∥A−1zt∥1 ≤|||A−1|||1
p+q

t=1
∥zt∥1 = |||A−1|||1(∥⃗z1,p+q∥1 +∥⃗z2,p+q∥1),
implying that ∥yp+q∥1 ≤C(∥⃗z1,p+q∥1 + ∥⃗z2,p+q∥1). Adding terms and making use of
inequality (38) and the fact that α ∈(0,1), we obtain
∥yp+q∥2s0α
1
≤C((1+∥⃗z1,p+q∥1)2s0α +∥⃗z2,p+q∥2s0α
1
)
≤C((1+∥⃗z1,p+q∥1)2s0 +∥⃗z2,p+q∥2s0α
1
)
≤C(1+∥⃗z1,p+q∥2s0
1
+∥⃗z2,p+q∥2s0α
1
).
(54)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
27
To obtain an upper bound for the term ∥⃗z1,p+q∥2s0
1 , consider the equality z1,t = g(z1,t−1)+
σtεt from (53) and note that, by Assumption 2, |g(u)| ≤K0 for |u| ≤M0 and |g(u)| ≤(1−
r|u|−ρ)|u| ≤|u| for |u| ≥M0, so that |g(u)| ≤K0 +|u| for all u ∈R (note that Assumption
2 requires M0 to be so large that r|u|−ρ ∈(0,1) for |u| ≥M0). Using these inequalities,
|z1,t| ≤K0 +|z1,t−1|+σt|εt| (t = 1,...,p+q), so that
|z1,1| ≤K0 +|z1,0|+σ1|ε1|,
|z1,2| ≤2K0 +|z1,0|+σ1|ε1|+σ2|ε2|,
...
|z1,p+q| ≤(p+q)K0 +|z1,0|+σ1|ε1|+···+σp+q|εp+q|.
Thus, ∥⃗z1,p+q∥1 ≤C

1+|z1,0|+p+q
i=1 σi|εi|

, and, making use of inequality (38),
∥⃗z1,p+q∥2s0
1
≤C

1+|z1,0|2s0 +
p+q

i=1
σ 2s0
i
|εi|2s0

.
(55)
Next, to bound the term ∥⃗z2,p+q∥2s0α
1
, consider z2,t = 1z2,t−1 + z1,t−1ιp−1 (see (53)).
Setting κ = |||1|||1 and using the fact ∥ιp−1∥1 = 1, we obtain
∥z2,t∥1 ≤|||1|||1∥z2,t−1∥1 +|z1,t−1|∥ιp−1∥1 = κ∥z2,t−1∥1 +|z1,t−1|,
and furthermore
∥z2,1∥1 ≤κ∥z2,0∥1 +|z1,0|,
∥z2,2∥1 ≤κ2∥z2,0∥1 +κ|z1,0|+|z1,1|,
...
∥z2,p+q∥1 ≤κp+q∥z2,0∥1 +κp+q−1|z1,0|+···+|z1,p+q−1|.
This implies that
∥⃗z2,p+q∥1 ≤C(∥z2,0∥1 +1+|z1,0|+∥⃗z1,p+q∥1).
As the norms ∥·∥1 and ∥·∥∗are equivalent, it holds that ∥z2,0∥1 ≤C∥z2,0∥∗. Making use
of inequality (38) and the fact that α ∈(0,1), we obtain
∥⃗z2,p+q∥2s0α
1
≤C(∥z2,0∥2s0α
∗
+(1+|z1,0|+∥⃗z1,p+q∥1)2s0α)
≤C(∥z2,0∥2s0α
∗
+(1+|z1,0|+∥⃗z1,p+q∥1)2s0)
≤C(1+∥z2,0∥2s0α
∗
+|z1,0|2s0 +∥⃗z1,p+q∥2s0
1 ).
(56)
Now, combine (54) with the upper bounds obtained for ∥⃗z1,p+q∥2s0
1
and ∥⃗z2,p+q∥2s0α
1
in
(55) and (56), and recall that z1,0 = z1(y1,0) and z2,0 = z2(y1,0), to obtain
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

28
MIKA MEITZ AND PENTTI SAIKKONEN
∥yp+q∥2s0α
1
≤C

1+∥z2(y1,0)∥2s0α
∗
+|z1(y1,0)|2s0 +
p+q

i=1
σ 2s0
i
|εi|2s0

.
As μ2s0 = E[|ε1|2s0] is finite, this implies that
E[∥yp+q∥2s0α
1
| y0 = x]≤C

1+∥z2(x1)∥2s0α
∗
+|z1(x1)|2s0 +μ2s0
p+q

i=1
E[σ 2s0
i
| y0 = x]

.
(57)
Next, consider the terms in (57) involving conditional expectations of the σ 2s0
i
’s. We first
derive an inequality which is similar to inequality (11) in Meitz and Saikkonen (2010).
Using repeated substitution and the equality ξ(yt) = ζ,tξ(yt−1)+ωζ,t, we obtain, for any
fixed t ≥1, that
ξ(yt) =
t−1

k=0
ζ,t−kξ(y0)+ωζ,t +
t−2

k=0
k

l=0
ζ,t−lωζ,t−k−1.
Now, consider the vector norm ∥· ∥• in Assumption 4. Denote by |||·|||• the matrix norm
induced by the vector norm ∥·∥•; that is, for any q×q matrix A, set
|||A|||• = max
∥x∥•=1∥Ax∥•
(x ∈Rq).
(For clarity, note that |||·|||• above and |||·|||•Lp defined in (17) coincide for nonrandom
matrices but differ for random ones.) As ∥·∥• in Assumption 4 is assumed to be monotone,
it follows from Problems 5.6.P41(c) and 5.6.P42 in Horn and Johnson (2013, p. 368) that
the induced matrix norm |||·|||• is monotone on the positive orthant, meaning that any q×q
matrices A and B that satisfy the (entrywise) inequalities 0 ≤A ≤B also satisfy the inequality
|||A|||• ≤|||B|||•. Usual properties of vector norms and matrix norms in conjunction with
inequality (38) therefore yield
∥ξ(yt)∥s0• ≤C
t−1

k=0
|||ζ,t−k|||s0
• ∥ξ(y0)∥s0• +C∥ωζ,t∥s0• +C
t−2

k=0
k
l=0
|||ζ,t−l|||s0
• ∥ωζ,t−k−1∥s0• .
By the monotonicity properties of the norms ∥· ∥• and |||·|||• and the definitions of the
matrices ζ,t and t in (14) and (16), we also obtain |||ζ,t|||• ≤|||t|||• and ∥ωζ,t∥• ≤
∥ωt∥• = ε2t ∥ω∥• (a.s.) for all t = 1,2,..., implying that
∥ξ(yt)∥s0• ≤C
t−1

k=0
|||t−k|||s0• ∥ξ(y0)∥s0• +C

|εt|2s0 +
t−2

k=0
k
l=0
|||t−l|||s0• |εt−k−1|2s0

∥ω∥s0• .
Now, denote the expectation E

|||t|||s0•

by χ (this expectation is finite due to Assump-
tion 4). Using the independence of the t’s and independence of |||t−l|||•’s and εt−k−1’s,
yields
E

∥ξ(yt)∥s0• | y0 = x

≤Cχt∥ξ(x)∥s0• +C

1+
t−2

k=0
χk+1

∥ω∥s0• .
(58)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
29
Inequality (34) in conjunction with (38) shows that σ 2s0
i
≤C(1+∥ξ(yi−1)∥s0• ) (a.s.) for all
i = 1,...,p+q. From (58), it then follows that E[σ 2s0
i
| y0 = x] ≤C(1+∥ξ(x)∥s0• ), which
together with (57) implies
E[∥yp+q∥2s0α
1
| y0 = x] ≤C

1+|z1(x1)|2s0 +∥z2(x1)∥2s0α
∗
+∥ξ(x)∥s0•

.
For any x ∈AN, the dominant side is bounded by C(1+2N +N1/bα), and thus we can find
a finite MN such that (52) holds.
Step 7: Completing the proof. We are now ready to complete the proof by applying
Theorem 1(iii) in Meitz and Saikkonen (2022). To this end, in the beginning of Step 6, we
already noted that the Markov chain yt is ψ-irreducible and aperiodic. That Condition D
holds was shown in (51) in Step 5. Petiteness of the set AN was shown in Step 6. We also
need to verify that supx∈AN V(x) < ∞; this inequality is a straightforward consequence of
the definitions of the set AN and the function V. Thus, applying Theorem 1(iii) in Meitz and
Saikkonen (2022), we can complete the proof.
□
Details for the finiteness of moments in Section 3.2. The arguments are similar to those
used in the proof of Corollary to Theorem 3 in Meitz and Saikkonen (2022). First, note that
inequality (45) continues to hold if the term ˜ϖsα
1 ∥z2∥2s0α2
∗
on its dominant side is replaced
with the term ˜ϖs1 ∥z2∥2s0α
∗
(this can be seen from (44) and the arguments that follow it).
Consequently, the same replacement can be done on the dominant sides of inequalities (47)
and (49), the latter inequality thus becoming
E[V(y1) | y0 = x] ≤V(x)−(1+ ¯r|z1(x1)|2s0α + ˜ϖs1 ∥z2(x1)∥2s0α
∗
+ ¯λsα
2∥ξ(x)∥bs0α
•
)+C.
Finiteness of certain moments with respect to the stationary distribution π of yt can
now be obtained from Theorem 14.3.7 of Meyn and Tweedie (2009), namely

Rp+q(1 +
¯r|z1(x1)|2s0α + ˜ϖs1 ∥z2(x1)∥2s0α
∗
+ ¯λsα
2∥ξ(x)∥bs0α
•
)π(dx) < ∞. Noting that 2s0α = 2s0 −
ρ and following the arguments in the proof of Corollary to Theorem 3 in Meitz and
Saikkonen (2022), it follows that the stationary version of yt satisfies E[|yt|2s0−ρ] < ∞.
□
Proofs of Propositions 1 and 2. For Proposition 1, note that model (24) can be written
as ut = ut−1 +ν1L(ut−1;γ,a1)+ν2(1−L(ut−1;γ,a2))+σtεt so that the function g(·) in
Assumption 2(ii) takes the form g(u) = u+ν1L(u;γ,a1)+ν2(1−L(u;γ,a2)). Arguments
used in the proof of Proposition 1 in Meitz and Saikkonen (2022) now show that Assumption
2(ii) holds with ρ = 1. Applying Theorem 1 with δ = 2s0 yields the polynomial ergodicity
result, and the moment result follows from the remarks made after Theorem 1. As for
Proposition 2, model (25) can be written as ut = S(ut−1)ut−1 + σtεt so that now g(u) =
S(u)u. Assumption 2(ii) can be verified as in the proof of Proposition 2 in Meitz and
Saikkonen (2022), and the result follows from Theorem 1 (with δ = 2s0/ρ).
□
B. APPENDIX B
This appendix contains Figure B1, which displays further analysis of the residuals of
model (27).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

30
MIKA MEITZ AND PENTTI SAIKKONEN
−0.06 −0.02
0.02
0.06
0
10
20
30
40
50
−0.06 −0.02
0.02
0.06
0
10
20
30
40
50
−4
−2
0
2
4
6
8
0.0
0.2
0.4
0.6
−4
−2
0
2
4
6
8
10
−4 −2 0
2
4
6
Figure B1. Further analysis of the residuals shown in (the bottom-right graph of) Figure 1:
autocorrelation function of ˆεt (top left), autocorrelation function of ˆε2
t (top right), histogram along
with the estimated error density (bottom left), and a Q–Q plot (bottom right). The dashed lines in the
autocorrelation function graphs show the conventional bounds ±1.96/
√
T ≈±0.038 (T = 2715; the
first four observations are used as initial values).
REFERENCES
Atchadé, Y., & G. Fort (2010) Limit theorems for some adaptive MCMC algorithms with subgeometric
kernels. Bernoulli 16, 116–154.
Cline, D. B. H. (2007) Stability of nonlinear stochastic recursions with application to nonlinear AR–
GARCH models. Advances in Applied Probability 39, 462–491.
Cline, D. B. H., & H. H. Pu (1998) Verifying irreducibility and continuity of a nonlinear time series.
Statistics & Probability Letters 40, 139–148.
Cline, D. B. H., & H. H. Pu (2004) Stability and the Lyapounov exponent of threshold AR–ARCH
models. Annals of Applied Probability 14, 1920–1949.
Davidson, J. (1994) Stochastic Limit Theory. Oxford University Press.
Douc, R., G. Fort, E. Moulines, & P. Soulier (2004) Practical drift conditions for subgeometric rates
of convergence. Annals of Applied Probability 14, 1353–1377.
Douc, R., A. Guillin, & E. Moulines (2008) Bounds on regeneration times and limit theorems for
subgeometric Markov chains. Annales de l’Institute Henri Poincare, Probabilites et Statistiques 44,
239–257.
Douc, R., E. Moulines, P. Priouret, & P. Soulier (2018) Markov Chains. Springer.
Dudley, R. M. (2004) Real Analysis and Probability. Cambridge University Press.
Engle, R. F. (1982) Autoregressive conditional heteroscedasticity with estimates of the variance of
United Kingdom inflation. Econometrica 50, 987–1007.
Fefferman, C., & H. S. Shapiro (1972) A planar face on the unit sphere of the multiplier space Mp,
1 < p < ∞. Proceedings of the American Mathematical Society 36, 435–439.
Fort, G., & E. Moulines (2000) V-subgeometric ergodicity for a Hastings–Metropolis algorithm.
Statistics & Probability Letters 49, 401–410.
Fort, G., & E. Moulines (2003) Polynomial ergodicity of Markov transition kernels. Stochastic
Processes and Their Applications 103, 57–99.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC AR–ARCH
31
Horn, R. A., & C. R. Johnson (2013) Matrix Analysis. (2nd ed.) Cambridge University Press.
Jarner, S. F., & G. O. Roberts (2002) Polynomial convergence rates of Markov chains. Annals of
Applied Probability 12, 224–247.
Jones, M. C., & M. J. Faddy (2003) A skew extension of the t-distribution, with applications. Journal
of the Royal Statistical Society: Series B 65, 159–174.
Klokov, S. A. (2007) Lower bounds of mixing rate for a class of Markov processes. Theory of
Probability and Its Applications 51, 528–535.
Klokov, S. A., & A. Y. Veretennikov (2004) Sub-exponential mixing rate for a class of Markov chains.
Mathematical Communications 9, 9–26.
Klokov, S. A., & A. Y. Veretennikov (2005) On subexponential mixing rate for Markov processes.
Theory of Probability and Its Applications 49, 110–122.
Lieberman, O., & P. C. B. Phillips (2020) Hybrid stochastic local unit roots. Journal of Econometrics
215, 257–285.
Ling, S. (1999) On the probabilistic properties of a double threshold ARMA conditional heteroskedas-
tic model. Journal of Applied Probability 36, 688–705.
Ling, S., & M. McAleer (2002) Necessary and sufficient moment conditions for the GARCH(r,s) and
asymmetric power GARCH(r,s) models. Econometric Theory 18, 722–729.
Liu, J., W. K. Li, & C. W. Li (1997) On a threshold autoregression with conditional heteroscedastic
variances. Journal of Statistical Planning and Inference 62, 279–300.
Meitz, M., & P. Saikkonen (2008a) Ergodicity, mixing, and existence of moments of a class of Markov
models with applications to GARCH and ACD models. Econometric Theory 24, 1291–1320.
Meitz, M., & P. Saikkonen (2008b) Stability of nonlinear AR–GARCH models. Journal of Time Series
Analysis 29, 453–475.
Meitz, M., & P. Saikkonen (2010) A note on the geometric ergodicity of a nonlinear AR–ARCH model.
Statistics & Probability Letters 80, 631–638.
Meitz, M., & P. Saikkonen (2021) Subgeometric ergodicity and β-mixing. Journal of Applied
Probability 58, 594–608.
Meitz, M., & P. Saikkonen (2022) Subgeometrically ergodic autoregressions. Econometric Theory 38,
959–985.
Merlevède, F., M. Peligrad, & E. Rio (2011) A Bernstein type inequality and moderate deviations for
weakly dependent sequences. Probability Theory and Related Fields 151, 435–474.
Meyn, S. P., & R. L. Tweedie (2009) Markov Chains and Stochastic Stability. (2nd ed.) Cambridge
University Press.
Nummelin, E., & P. Tuominen (1983) The rate of convergence in Orey’s theorem for Harris recurrent
Markov chains with applications to renewal theory. Stochastic Processes and Their Applications 15,
295–311.
Phillips, P. C. B. (2023) Estimation and inference with near unit roots. Econometric Theory 39, 221–
263.
Tuominen, P., & R. L. Tweedie (1994) Subgeometric rates of convergence of f-ergodic Markov chains.
Advances in Applied Probability 26, 775–798.
Tweedie, R. L. (1983) Criteria for rates of convergence of Markov chains, with application to queueing
and storage theory. In J. F. C. Kingman, & G. E. H. Reuter (Eds.), Probability, Statistics and Analysis,
pp. 260–276. Cambridge University Press.
Veretennikov, A. Y. (2000) On polynomial mixing and convergence rate for stochastic difference and
differential equations. Theory of Probability and Its Applications 44, 361–374.
Vladimirova, M., S. Girard, H. Nguyen, & J. Arbel (2020) Sub-Weibull distributions: Generalizing
sub-Gaussian and sub-exponential properties to heavier tailed distributions. Stat 9, e318.
Wong, K. C., Z. Li, & A. Tewari (2020) Lasso guarantees for β-mixing heavy-tailed time series. Annals
of Statistics 48, 1124–1142.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

