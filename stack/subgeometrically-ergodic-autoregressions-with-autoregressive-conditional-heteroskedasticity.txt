Econometric Theory, 0, 2023, 1â€“31.
doi:10.1017/S026646662300035X
SUBGEOMETRICALLY ERGODIC
AUTOREGRESSIONS WITH
AUTOREGRESSIVE CONDITIONAL
HETEROSKEDASTICITY
MIKA MEITZ
University of Helsinki
PENTTI SAIKKONEN
University of Helsinki
In this paper, we consider subgeometric (specifically, polynomial) ergodicity of uni-
variate nonlinear autoregressions with autoregressive conditional heteroskedasticity
(ARCH). The notion of subgeometric ergodicity was introduced in the Markov chain
literature in the 1980s, and it means that the transition probability measures converge
to the stationary measure at a rate slower than geometric; this rate is also closely
related to the convergence rate of Î²-mixing coefficients. While the existing literature
on subgeometrically ergodic autoregressions assumes a homoskedastic error term,
this paper provides an extension to the case of conditionally heteroskedastic ARCH-
type errors, considerably widening the scope of potential applications. Specifically,
we consider suitably defined higher-order nonlinear autoregressions with possibly
nonlinear ARCH errors and show that they are, under appropriate conditions,
subgeometrically ergodic at a polynomial rate. An empirical example using energy
sector volatility index data illustrates the use of subgeometrically ergodic ARâ€“ARCH
models.
1. INTRODUCTION
Let Xt (t = 0,1,2,...) be a Markov chain on the state space X and initialized
from an X0 following some initial distribution. If the n-step probability measures
Pn(x; Â·) = Pr(Xn âˆˆÂ· | X0 = x) converge in total variation norm âˆ¥Â·âˆ¥TV to the
stationary probability measure Ï€ at rate rn (for some r > 1), that is,
lim
nâ†’âˆrnâˆ¥Pn(x; Â·)âˆ’Ï€(Â·)âˆ¥TV = 0,
Ï€ a.e.,
(1)
the Markov chain is said to be geometrically ergodic. When the convergence in (1)
takes place at a suitably defined rate r(n) slower than geometric, that is,
The authors thank the Academy of Finland (M.M. and P.S.), Foundation for the Advancement of Finnish Securities
Markets (M.M.), and OP Group Research Foundation (M.M.) for financial support, and the Co-Editor (Robert
Taylor) and three anonymous referees for useful comments and suggestions. Address correspondence to Mika Meitz,
Department of Economics, University of Helsinki, P. O. Box 17, FI-00014 University of Helsinki, Finland; e-mail:
mika.meitz@helsinki.fi.
Â© The Author(s), 2023. Published by Cambridge University Press.
1
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

2
MIKA MEITZ AND PENTTI SAIKKONEN
lim
nâ†’âˆr(n)âˆ¥Pn(x; Â·)âˆ’Ï€(Â·)âˆ¥TV = 0,
Ï€ a.e.,
(2)
the Markov chain is called subgeometrically ergodic. Examples of common rates
(where c denotes a positive constant) include geometric (or exponential) when
r(n) = ecn = rn (r > 1), subexponential when r(n) = ecnÎ³ (0 < Î³ < 1), polynomial
when r(n) = (1+n)c, and logarithmic when r(n) = (1+ln(n))c. The authoritative
and classic reference to Markov chain theory is the monograph of Meyn and
Tweedie (2009), while an up-to-date treatment of subgeometric ergodicity can be
found in Chapters 16 and 17 of Douc et al. (2018).
To give some background, the notion of subgeometric ergodicity was introduced
in the Markov chain literature in the 1980s when Nummelin and Tuominen (1983)
and Tweedie (1983) obtained the first subgeometric ergodicity results for general
state space Markov chains. Subsequent work by Tuominen and Tweedie (1994),
Fort and Moulines (2000), Jarner and Roberts (2002), Fort and Moulines (2003),
and Douc et al. (2004) led to a formulation of a so-called drift condition to ensure
subgeometric ergodicity, paralleling the use of a Fosterâ€“Lyapunov drift condition
to establish geometric ergodicity (see, e.g., Meyn and Tweedie, 2009, Chap.
15). Various topics in probability theory and statistics have also been considered
under subgeometric assumptions; for instance, Douc, Guillin, and Moulines (2008)
considered the central limit theorem and Berryâ€“Esseen bounds, AtchadÃ© and Fort
(2010) the convergence of Markov chain Monte Carlo algorithms, MerlevÃ¨de,
Peligrad, and Rio (2011) a Bernstein-type inequality, and Meitz and Saikkonen
(2021) the rate of Î²-mixing. In this paper, we are interested in autoregressive
time series models. Results regarding the subgeometric ergodicity of first-order
autoregressions were obtained by Tuominen and Tweedie (1994), Veretennikov
(2000), Fort and Moulines (2003), Douc et al. (2004), Klokov and Veretennikov
(2004, 2005), and Klokov (2007), among others, whereas results for more general
higher-order autoregressions were obtained by Meitz and Saikkonen (2022).
In this paper, we consider subgeometric (specifically, polynomial) ergodicity of
autoregressive models with autoregressive conditional heteroskedasticity (ARCH;
Engle, 1982). The previous works on subgeometrically ergodic autoregressions
listed above only considered the case of independent and identically distributed
(IID) errors, and allowing for conditionally heteroskedastic errors considerably
widens the scope of potential applications. This is particularly important in
applications using economic and financial time series data. In the subgeometrically
ergodic ARâ€“ARCH models we consider, the conditional mean is similar to the
(homoskedastic) AR models already considered in Meitz and Saikkonen (2022).
The precise model formulation will be given and motivated further in Section 2, but
we already note that the models we consider accommodate for behavior similar to
a unit-root process for large values of the observed series, but almost no restrictions
are placed on their dynamics for moderate values of the observed series. The
conditional variance is allowed to follow a rather general nonlinear ARCH process.
In our main result, we show that the considered ARâ€“ARCH processes are,
under appropriate conditions, subgeometrically ergodic at a polynomial rate; the
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
3
convergence rate of Î²-mixing coefficients and finiteness of certain moments are
also obtained (for details, see Section 3.2).
The inclusion of ARCH (instead of IID) errors considerably complicates the
proofs of (sub)geometric ergodicity of nonlinear autoregressions. Papers consider-
ing subgeometric ergodicity of homoskedastic autoregressions were already listed
above. Geometric ergodicity of nonlinear autoregressive models with ARCH (or
generalized ARCH) errors has previously been considered by numerous authors
(see, e.g., Cline and Pu, 2004, Meitz and Saikkonen, 2008b, 2010, and the
many references therein). Compared to these two strands of previous literature,
the combination of the subgeometrically ergodic type of nonlinear dynamics in
the conditional mean with ARCH errors leads to additional complications in the
proofs. To appropriately separate these two sources of dynamics, we make use of
a (relatively unknown) extension of Bernoulliâ€™s inequality due to Fefferman and
Shapiro (1972) (combined with Youngâ€™s inequality), and to control terms arising
due to conditional heteroskedasticity, we devise a special matrix norm that is of a
more complicated type than the norms typically used when analyzing the stability
of nonlinear time series models.
The rest of the paper is organized as follows. Section 2 introduces the nonlinear
ARâ€“ARCH model considered and states the assumptions we employ. Results on
subgeometric ergodicity are given in Section 3. In Section 4, we consider an
empirical application of our model to a daily time series of an energy sector
volatility index. Section 5 concludes. All proofs are collected in Appendix A.
2. MODEL
2.1. Conditional Mean
We consider the univariate process yt (t = 1,2,...) generated by
yt = Ï€1ytâˆ’1 +Â·Â·Â·+Ï€pâˆ’1ytâˆ’p+1 +g(utâˆ’1)+ÏƒtÎµt,
(3)
where p â‰¥1 is the autoregressive order, ut = yt âˆ’Ï€1ytâˆ’1 âˆ’Â·Â·Â·âˆ’Ï€pâˆ’1ytâˆ’p+1, g is a
real-valued function, Îµt is an IID error term, and Ïƒt = Ïƒ(ytâˆ’1) is a positive volatility
term that depends on p + q lagged values of yt, ytâˆ’1 = (ytâˆ’1,...,ytâˆ’pâˆ’q), where
q â‰¥1 is an ARCH order. For now, one concrete example of the volatility term is a
linear ARCH process, where Ïƒt satisfies
Ïƒ 2
t = Ï‰ +Î±1e2
tâˆ’1 +Â·Â·Â·+Î±qe2
tâˆ’q
(4)
and et = yt âˆ’Ï€1ytâˆ’1 âˆ’Â·Â·Â·âˆ’Ï€pâˆ’1ytâˆ’p+1 âˆ’g(utâˆ’1), Ï‰ > 0, and Î±i â‰¥0 (i = 1,...,q);
a more general formulation for the conditional variance will be considered below.
Note that a compact expression for et is et = ut âˆ’g(utâˆ’1) so that equation (3) can
be expressed as ut = g(utâˆ’1)+ÏƒtÎµt. If Ï€1 = Â·Â·Â· = Ï€pâˆ’1 = 0 in equation (3), we have
ut = yt so that the autoregressive order p reduces to one and equation (3) reduces
to yt = g(ytâˆ’1)+ÏƒtÎµt.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

4
MIKA MEITZ AND PENTTI SAIKKONEN
Our first assumption contains basic requirements for the error term Îµt and
makes clear that the squared volatility, Ïƒ 2
t , is the conditional variance of yt (when
appropriate moments exist).
Assumption 1. {Îµt, t = 1,2,...} is a sequence of IID random variables that
is independent of (y0,...,y1âˆ’pâˆ’q), has zero mean and unit variance, and the
distribution of Îµ1 has a (Lebesgue) density that is bounded away from zero on
compact subsets of R.
Later on, we introduce an assumption on the conditional variance Ïƒ 2
t which
further restricts the moments of Îµt.
To further describe the conditional mean of the autoregressions we consider, we
next specify the conditions needed for the function g in equation (3). The following
assumption is a simplification of Assumption 1 in Meitz and Saikkonen (2022) (the
somewhat more general formulation used therein is briefly discussed at the end of
this subsection).
Assumption 2.
(i) The roots of the polynomial Ï–(z) = 1âˆ’Ï€1zâˆ’Â·Â·Â·âˆ’Ï€pâˆ’1zpâˆ’1 lie outside the
unit circle.
(ii) The function g : R â†’R in (3) is measurable, locally bounded, and satisfies
|g(u)| â†’âˆas |u| â†’âˆ, and there exist positive constants r, M0, K0, and 0 < Ï < 2
such that, for all u âˆˆR,
|g(u)| â‰¤

(1âˆ’r|u| âˆ’Ï)|u|,
for |u| â‰¥M0,
K0,
for |u| â‰¤M0.
(5)
Assumption 2(i) corresponds to the conventional stationarity condition of a
linear autoregression in that it requires the roots of the polynomial Ï–(z) to lie
outside the unit circle. In the first-order case p = 1, this condition becomes
redundant because then Ï€1 = Â·Â·Â· = Ï€pâˆ’1 = 0. Assumption 2(ii) is needed to prove
the subgeometric ergodicity of the process yt, as already done by Fort and Moulines
(2003) and Douc et al. (2004) in the first-order case p = 1 and by Meitz and
Saikkonen (2022) for higher-order autoregressions.
We next provide some intuition and motivation for our model in (3). To clarify
the role of inequality (5) restricting the function g(Â·), suppose Assumptions 1 and
2(i) hold, but instead of Assumption 2(ii), suppose the function g(Â·) were linear
with g(u) = Ï€0u and Ï€0 âˆˆ[âˆ’1,1]. Using the lag operator L, equation (3) could
then be written as
ut âˆ’Ï€0utâˆ’1 = (1âˆ’Ï€0L)(1âˆ’Ï€1Lâˆ’Â·Â·Â·âˆ’Ï€pâˆ’1Lpâˆ’1)yt = ÏƒtÎµt,
(6)
that is, as the familiar linear AR(p) model (with autoregressive heteroskedasticity).
Given Assumptions 1 and 2(i), the case Ï€0 âˆˆ(âˆ’1,1) corresponds to geometric
ergodicity of yt and the cases Ï€0 = Â±1 to non-ergodicity. Nonlinear functions g(Â·)
satisfying Assumption 2(ii) provide a middle ground between these extreme cases
of geometric ergodicity and non-ergodicity. For instance, if g(u) = (1âˆ’r|u| âˆ’Ï)u
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
5
for |u| > r1/Ï and g(u) = 0 otherwise (r > 0, 0 < Ï < 2), then for any fixed Ï€0 âˆˆ
(âˆ’1,1) and for all u sufficiently large in absolute value (i.e., for the values of u
that are crucial for determining ergodicity),
|Ï€0||u| < |g(u)| < |u|.
The subgeometrically ergodic autoregressions we consider thus provide one pos-
sibility for modeling small departures from unit-root autoregressions. Assumption
2(ii) implies that for large values of |utâˆ’1|, the conditional mean of model (3) is
close to that of an integrated process (of order one). On the other hand, as inequality
(5) restricts the function g(Â·) only for large values of its argument, no restrictions
(apart from the boundedness condition in (5)) are imposed when the argument
takes values inside some bounded set of values. Thus, the autoregressions we
consider may exhibit rather arbitrary (stationary, unit root, explosive, nonlinear,
etc.) behavior for moderate values of the observed series.
The autoregressions we consider are to some extent related to existing models
that have autoregressive roots near unity. To illustrate, when g(u) is as in the
previous paragraph and we further set p = r = Ï = 1, the model in (3) simplifies to
yt =

1âˆ’
1
|ytâˆ’1|

ytâˆ’1 +et
when |ytâˆ’1| > 1
and
yt = et
otherwise,
where et = ÏƒtÎµt. In comparison, a prototypical local-to-unity autoregression could
be expressed as
yt =

1âˆ’1
T

ytâˆ’1 +et,
t = 1,...,T,
where T denotes the sample size.
Both of the above formulations involve an autoregressive coefficient near unity,
the former when the observed process takes on large (absolute) values and the
latter when the sample size is large. However, the fact that the sample size is an
essential part of local-to-unity autoregressions makes them quite different from
the autoregressions we considerâ€”in particular, the autoregressions we consider
are ergodic. For more details on local-to-unity autoregressions and other related
models, we refer the reader to the recent contributions of Lieberman and Phillips
(2020) and Phillips (2023) and the references therein.
Homoskedastic subgeometrically ergodic autoregressions satisfying (a some-
what more general version of) Assumption 2 were already considered by Meitz
and Saikkonen (2022). As many time series in economics, finance, and other
fields exhibit conditional heteroskedasticity, in this paper, we consider an extension
to ARCH errors. In the homoskedastic case considered in Meitz and Saikkonen
(2022), the term g(utâˆ’1) in (3) was replaced with the more general formulation
utâˆ’1 + Ëœg(ytâˆ’1,...,ytâˆ’p) (with Ëœg a real-valued function) to allow for more general
dependence on the past through the variables ytâˆ’1,...,ytâˆ’p (and not only through
the linear combination utâˆ’1 = ytâˆ’1 âˆ’Ï€1ytâˆ’2 âˆ’Â·Â·Â·âˆ’Ï€pâˆ’1ytâˆ’p). The present simpler
formulation worked well in the empirical application of Section 4 and in some
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

6
MIKA MEITZ AND PENTTI SAIKKONEN
other examples we tried out, and leads to more transparent assumptions and
streamlined proofs.
2.2. Companion Form
To establish ergodicity, we need the companion form of the (p + q)-dimensional
process yt = (y1,t,y2,t) with a p-dimensional y1,t = (yt,...,ytâˆ’p+1) and a
q-dimensional y2,t = (ytâˆ’p,...,ytâˆ’pâˆ’q+1). First, we formulate the p-dimensional
companion form related to equation (3), which reads as
â¡
â¢â¢â¢â¢â¢â¢â£
yt
ytâˆ’1
...
...
ytâˆ’p+1
â¤
â¥â¥â¥â¥â¥â¥â¦
=
â¡
â¢â¢â¢â¢â¢â¢â£
Ï€1
Ï€2
Â·Â·Â·
Ï€pâˆ’1
0
1
0
Â·Â·Â·
0
0
0
...
...
...
...
...
...
...
0
0
0
Â·Â·Â·
0
1
0
â¤
â¥â¥â¥â¥â¥â¥â¦
â¡
â¢â¢â¢â¢â¢â¢â£
ytâˆ’1
ytâˆ’2
...
...
ytâˆ’p
â¤
â¥â¥â¥â¥â¥â¥â¦
+g(utâˆ’1)
â¡
â¢â¢â¢â¢â¢â¢â£
1
0
...
...
0
â¤
â¥â¥â¥â¥â¥â¥â¦
+ÏƒtÎµt
â¡
â¢â¢â¢â¢â¢â¢â£
1
0
...
...
0
â¤
â¥â¥â¥â¥â¥â¥â¦
or, denoting the matrix in this equation with  and setting Î¹p = (1,0,...,0) (pÃ—1),
as
y1,t = y1,tâˆ’1 +g(utâˆ’1)Î¹p +ÏƒtÎµtÎ¹p
(7)
(when p = 1,  = 0 and utâˆ’1 = ytâˆ’1). As Ïƒt = Ïƒ(ytâˆ’1) depends on the whole
(p+q)-dimensional vector ytâˆ’1, we have to expand (7) to the (p+q)-dimensional
companion form
 y1,t
y2,t

=


0pÃ—q
0qÃ—(pâˆ’1)
Iq
0qÃ—1
 y1,tâˆ’1
y2,tâˆ’1

+g(utâˆ’1)Î¹p+q +ÏƒtÎµtÎ¹p+q,
(8)
where Iq is the (qÃ—q) identity matrix and 0âˆ—Ã—âˆ—denotes a matrix of zeros with the
indicated dimensions (and Î¹p+q is defined in the obvious way). This shows that yt
is a Markov chain on Rp+q.
In order to establish ergodicity, we further transform the p-dimensional com-
panion form (7) in a way already used in Meitz and Saikkonen (2022, Sect. 4). To
this end, we define the matrices
A =
â¡
â¢â¢â¢â¢â¢â£
1 âˆ’Ï€1 âˆ’Ï€2 Â·Â·Â· âˆ’Ï€pâˆ’1
0
1
0
Â·Â·Â·
0
...
...
...
...
...
0
...
...
0
0 Â·Â·Â·
Â·Â·Â·
0
1
â¤
â¥â¥â¥â¥â¥â¦
and
(9)
 = AAâˆ’1 =
â¡
â¢â¢â¢â¢â£
0
0
0
Â·Â·Â·
0
1
Ï€1
Ï€2
Â·Â·Â·
Ï€pâˆ’1
0
1
0
Â·Â·Â·
0
...
...
...
...
...
0
Â·Â·Â·
0
1
0
â¤
â¥â¥â¥â¥â¦
=

0
01Ã—(pâˆ’1)
Î¹pâˆ’1
1

,
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
7
where A is nonsingular and 1 is the (p âˆ’1) Ã— (p âˆ’1)-dimensional lower right-
hand corner of  (when p = 1, A = 1 and  = 0). With these definitions, equation
(7) can be transformed into
Ay1,t = Ay1,tâˆ’1 +g(utâˆ’1)Î¹p +ÏƒtÎµtÎ¹p,
(10)
where Ay1,t = (ut,ytâˆ’1,...,ytâˆ’p+1). Now, for any p-dimensional vector x1, form
the partition x1 = (x1,1,...,x1,p) = (x1,1,x1,2) and define
z(x1) =

z1(x1)
z2(x1)

= Ax1 =

x1,1 âˆ’Ï€1x1,2 âˆ’Â·Â·Â·âˆ’Ï€pâˆ’1x1,p
x1,2

(11)
(when p = 1, x1,2 and z2(x1) are dropped). Using this notation, equation (10) can
be expressed as z(y1,t) = z(y1,tâˆ’1)+g(z1(y1,tâˆ’1))Î¹p +Ïƒ(ytâˆ’1)ÎµtÎ¹p, that is, as

z1(y1,t)
z2(y1,t)

=

0
01Ã—(pâˆ’1)
Î¹pâˆ’1
1

z1(y1,tâˆ’1)
z2(y1,tâˆ’1)

+g(z1(y1,tâˆ’1))Î¹p +Ïƒ(ytâˆ’1)ÎµtÎ¹p
=

g(z1(y1,tâˆ’1))+Ïƒ(ytâˆ’1)Îµt
1z2(y1,tâˆ’1)+z1(y1,tâˆ’1)Î¹pâˆ’1

.
(12)
Here, the first equation is in a form where the autoregressive order is one and the
volatility term is a function of the (p+q)-dimensional vector ytâˆ’1 = (y1,tâˆ’1,y2,tâˆ’1),
whereas the second equation involves the p-dimensional vector y1,tâˆ’1 only.
By Assumption 2(i), the roots of the polynomial Ï–(z) lie outside the unit circle,
so that the eigenvalues of the matrix 1 in the second equation in (12) are smaller
than one in absolute value. As is well known, this implies the existence of a matrix
norm of 1 that is also smaller than one. Specifically, for any vector norm âˆ¥Â· âˆ¥,
denote by |||Â·||| the corresponding induced matrix norm (Horn and Johnson, 2013,
Def. 5.6.1); that is, for any conformable square matrix A, set
|||A||| = max
âˆ¥xâˆ¥=1âˆ¥Axâˆ¥.
Then we obtain the following result (Horn and Johnson, 2013, Lem. 5.6.10).
Lemma 1. There exist a vector norm âˆ¥Â·âˆ¥âˆ—and a corresponding induced matrix
norm |||Â·|||âˆ—such that |||1|||âˆ—= Ï– < 1.
The existence of an induced matrix norm with the property in the above lemma
is essential in our proofs. (When p = 1, Assumption 2(i) and Lemma 1 are
redundant.) The norms âˆ¥Â· âˆ¥âˆ—and |||Â·|||âˆ—are defined on Rpâˆ’1 and R(pâˆ’1)Ã—(pâˆ’1),
respectively, and they have been commonly used in time series models. In the next
subsection, we introduce norms which are of a different type.
2.3. Conditional Variance
The root condition of Assumption 2(i) and inequality (5) of Assumption 2(ii) are
of major importance for establishing the stability of our model. However, as these
conditions only concern the conditional mean, we need additional assumptions
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

8
MIKA MEITZ AND PENTTI SAIKKONEN
restricting the conditional variance Ïƒ 2
t . As an extension of the basic ARCH model
(4), we consider a nonlinear formulation of the conditional variance defined as
Ïƒ 2
t = Î¶0,tâˆ’1Ï‰ +Î±1Î¶1,tâˆ’1e2
tâˆ’1 +Â·Â·Â·+Î±qÎ¶q,tâˆ’1e2
tâˆ’q,
(13)
where Î¶i,tâˆ’1 = Î¶i(ytâˆ’1) is a function of ytâˆ’1 (i = 0,...,q) and otherwise the notation
is as in equation (4) (including the conditions Ï‰ > 0 and Î±1,...,Î±q â‰¥0). When
the functions Î¶i,tâˆ’1 are the same for all i = 0,...,q, we remove the index i and
use the notations Î¶tâˆ’1 and Î¶(Â·). This is the case in our empirical example where
Î¶tâˆ’1 = Î¶(ytâˆ’1) = 1/(1+eâˆ’Î³ (ytâˆ’1âˆ’a)) is a logistic function depending only on ytâˆ’1.
For possible alternatives, we consider a more general formulation and introduce
the following assumption.
Assumption 3. In equation (13), the following conditions are assumed. (i) The
parameters Ï‰,Î±1,...,Î±q satisfy Ï‰ > 0, Î±1,...,Î±q â‰¥0, and q
i=1 Î±i < 1. (ii) For
each i = 0,...,q, the function Î¶i takes values in (0,1].
The above assumption includes the case Î¶i â‰¡1 for all i, which corresponds to
the linear ARCH model (4). It covers also the abovementioned logistic function.
Consider the q-dimensional process Î¾ t = (e2
t ,e2
tâˆ’1,...,e2
tâˆ’q+1) (t â‰¥1) with initial
values Î¾ 0 = (e2
0,...,e2
âˆ’q+1) where e2
0,...,e2
âˆ’q+1 are functions of y0. Inspired by
Cline and Pu (2004, Exam. 4.2), we now introduce the following equation, which
is a straightforward implication of equation (13) and the fact ÏƒtÎµt = et:
â¡
â¢â¢â¢â¢â¢â¢â¢â£
e2
t
e2
tâˆ’1
...
...
e2
tâˆ’q+1
â¤
â¥â¥â¥â¥â¥â¥â¥â¦
=
â¡
â¢â¢â¢â¢â¢â¢â£
Î±1Î¶1,tâˆ’1Îµ2
t
Î±2Î¶2,tâˆ’1Îµ2
t
Â·Â·Â·
Î±qâˆ’1Î¶qâˆ’1,tâˆ’1Îµ2
t
Î±qÎ¶q,tâˆ’1Îµ2
t
1
0
Â·Â·Â·
0
0
0
1
...
...
...
...
...
...
0
0
0
Â·Â·Â·
0
1
0
â¤
â¥â¥â¥â¥â¥â¥â¦
â¡
â¢â¢â¢â¢â¢â¢â¢â£
e2
tâˆ’1
e2
tâˆ’2
...
...
e2
tâˆ’q
â¤
â¥â¥â¥â¥â¥â¥â¥â¦
+
â¡
â¢â¢â¢â¢â¢â¢â£
Î¶0,tâˆ’1Îµ2
t Ï‰
0
...
...
0
â¤
â¥â¥â¥â¥â¥â¥â¦
(t = 1,2,...) or, more briefly,
Î¾ t = Î¶,tÎ¾ tâˆ’1 +Ï‰Î¶,t,
t = 1,2,... ;
(14)
as Î¾ t is a function of yt, we occasionally write Î¾ t = Î¾(yt). For later purposes, we
also note that due to the identities ÏƒtÎµt = et and e2
t = Î¹â€²
qÎ¾(yt), we have
Ïƒ 2
t = Ïƒ 2(ytâˆ’1) = E[Î¹â€²
qÎ¾(yt) | ytâˆ’1].
(15)
When there is need to make the dependence of Î¶,t on ytâˆ’1 explicit, we use the
notation Î¶,t(ytâˆ’1) and replace the (random) argument ytâˆ’1 by a fixed counterpart
when needed. Specifically, Î¶,t(x) means that the functions Î¶i,tâˆ’1 = Î¶i,tâˆ’1(ytâˆ’1)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
9
used in Î¶,t(ytâˆ’1) are replaced by Î¶i,t(x) for all i = 1,...,q, and the notations Ïƒ 2(x)
and Î¾(x) are used similarly.
We also define the matrices
t =
â¡
â¢â¢â¢â¢â¢â£
Î±1Îµ2
t
Î±2Îµ2
t
Â·Â·Â·
Î±qâˆ’1Îµ2
t
Î±qÎµ2
t
1
0
Â·Â·Â·
0
0
0
1
...
...
...
...
...
...
0
0
0
Â·Â·Â·
0
1
0
â¤
â¥â¥â¥â¥â¥â¦
and
 = E[t] =
â¡
â¢â¢â¢â¢â¢â£
Î±1
Î±2
Â·Â·Â·
Î±qâˆ’1
Î±q
1
0
Â·Â·Â·
0
0
0
1
...
...
...
...
...
...
0
0
0
Â·Â·Â·
0
1
0
â¤
â¥â¥â¥â¥â¥â¦
.
(16)
Note that t is obtained from the matrix Î¶,t by choosing Î¶i,t = 1 for all
i = 1,...,q. Similarly, we denote Ï‰t = (Ï‰t,0,...,0)â€² with Ï‰t = Ï‰Îµ2
t and Ï‰ =
E[Ï‰t] = (Ï‰,0,...,0)â€².
In our proofs, we need to appropriately control the size of the random matrix t,
and not just the size of the nonrandom matrix  = E[t]. This is the reason why we
next consider vector and matrix norms more complicated than those in Lemma 1.
To this end, we first recall the definition of an Lp-norm (for convenience, in this
subsection only, we use the notation p in Lp-norms; elsewhere in the paper p stands
for the autoregressive order in model (3)). If âˆ¥Â·âˆ¥is any vector norm on Rq and v
is a q-dimensional random vector, equation
âˆ¥vâˆ¥Lp = (E[âˆ¥vâˆ¥p])1/p
(1 â‰¤p < âˆ)
defines an Lp-norm on the set of (equivalence classes of almost surely equal) qÃ—1
random vectors that are p-integrable (see, e.g., Dudley, 2004, Sects. 5.1 and 5.2). It
may be worth noting that for nonrandom vectors, there is no difference between the
norms âˆ¥Â·âˆ¥and âˆ¥Â·âˆ¥Lp, but for random vectors, the outcome of âˆ¥Â·âˆ¥is random and
that of âˆ¥Â·âˆ¥Lp is nonrandom. This Lp-norm can be used to induce a norm for random
matrices; for the conventional nonrandom matrix case and for the terminology
used below, see Horn and Johnson (2013, Def. 5.6.1 and Sect. 5.6). Specifically,
to define a generalized (non-submultiplicative) matrix norm |||Â·|||Lp, for any qÃ—q
random matrix A, set
|||A|||Lp = max
âˆ¥xâˆ¥Lp=1âˆ¥Axâˆ¥Lp = max
âˆ¥xâˆ¥=1âˆ¥Axâˆ¥Lp
(x âˆˆRq),
(17)
where the latter equality holds as x is nonrandom. This defines a generalized matrix
norm1 on the set of (equivalence classes of almost surely equal) q Ã— q random
1Axioms (1), (1a), (2), and (3) of a generalized matrix norm (see Horn and Johnson, 2013, pp. 340â€“341) can be
checked similarly as in the proof of Theorem 5.6.2(c) of the same reference (replacing the norms âˆ¥Â· âˆ¥and |||Â·|||
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

10
MIKA MEITZ AND PENTTI SAIKKONEN
matrices with p-integrable entries; moreover, the norms âˆ¥Â·âˆ¥, âˆ¥Â·âˆ¥Lp, and |||Â·|||Lp are
related by the inequality2
âˆ¥Axâˆ¥Lp â‰¤|||A|||Lpâˆ¥xâˆ¥
(x âˆˆRq).
(18)
We next state a high-level condition that assumes the existence of a vector
norm on Rq with particular additional properties. (Primitive conditions ensuring
this high-level assumption will be given momentarily.) One of these properties
is monotonicity in the sense of Definition 5.4.18 of Horn and Johnson (2013):
a vector norm âˆ¥Â· âˆ¥is monotone if x,y âˆˆRq satisfying |xi| â‰¤|yi| for i = 1,...,q
always implies that âˆ¥xâˆ¥â‰¤âˆ¥yâˆ¥. For clarity, we use the notation âˆ¥Â·âˆ¥â€¢ for the specific
vector norm in the assumption below; similarly, we denote the related Lp-norm
by âˆ¥Â· âˆ¥â€¢Lp and the generalized matrix norm by |||Â·|||â€¢Lp. We also introduce two
constants, s0 â‰¥1 and b â‰¥1, such that
b = 1 when s0 = 1
and
b > (2s0 âˆ’Ï)/[s0(2âˆ’Ï)] > 1 when s0 > 1
(recall from Assumption 2 that Ï âˆˆ(0,2) so that 2s0 > Ï). These constants are
used in the next section where we establish our ergodicity result and there the
size of s0 will have an effect on the rate of convergence obtained and the order of
moments that are finite. The rather complex conditions required from the constant
b are due to the connection between the conditional mean and ARCH errors (this
connection disappears when s0 = 1 as it also does in subgeometric homoskedastic
autoregressions).
Assumption 4. Suppose there exists a vector norm âˆ¥Â· âˆ¥â€¢ on Rq that is (i)
monotone and (ii) such that |||t|||â€¢Lbs0 = Î» < 1, where b and s0 are as described
above.
This assumption tacitly requires that E[|Îµt|2bs0] is finite, thereby strengthening
Assumption 1 (when s0 > 1). Assumption 4 is formulated in a way that is conve-
nient in our proofs but is not very transparent. The following lemma gives primitive
conditions ensuring that Assumption 4 holds (for a proof, see Appendix A).
Lemma 2. Suppose that Assumptions 1 and 3 hold and also that the
parameters Î±1,...,Î±q in Assumption 3 satisfy q
i=1 Î±i < 1/ Â¯Î¼2bs0, where Â¯Î¼2bs0 =
(E[|Îµ2
1|bs0])1/bs0. Then Assumption 4 holds.
To illustrate, consider the case q = s0 = b = 1 so that the condition in Lemma 2
reduces to the requirement Î±1 < 1. In geometrically ergodic AR models with
linear ARCH(1) errors, Î±1 < 1 is the usual requirement for covariance stationarity,
while geometric ergodicity can hold under even weaker conditions, such as
E[ln(Î±1Îµ2
t )] < 0 (see, e.g., Meitz and Saikkonen, 2010, Assump. 3 and Thm. 1
therein with âˆ¥Â· âˆ¥Lp and |||Â·|||Lp, replacing appropriate statements therein with their almost sure counterparts, and
using Minkowskiâ€™s inequality as an additional justification for axiom (3)).
2Inequality (18) can be verified analogously to Theorem 5.6.2(b) of Horn and Johnson (2013).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
11
for further details). In the present setting, the situation is different: as will be seen
in Section 3.2, condition Î±1 < 1 does not guarantee a finite variance.
3. SUBGEOMETRIC ERGODICITY AT A POLYNOMIAL RATE
3.1. Main Result
We now consider the stability of the model introduced in the previous section. We
begin with a brief account of some necessary Markov chain concepts (for more
comprehensive discussions, see Meyn and Tweedie, 2009; Douc et al., 2018; and
also Meitz and Saikkonen, 2022, Sect. 2). Let Xt (t = 0,1,2,...) be a Markov chain
on a general measurable state space (X,B(X)) (with B(X) the Borel Ïƒ-algebra),
and let Pn(x; A) = Pr(Xn âˆˆA | X0 = x) signify its n-step transition probability
measure. For an arbitrary fixed measurable function f : X â†’[1,âˆ) and for any
signed measure Î¼, define the f-norm âˆ¥Î¼âˆ¥f as
âˆ¥Î¼âˆ¥f = sup
f0:|f0|â‰¤f
|Î¼(f0)|,
(19)
where Î¼(f0) =

xâˆˆX f0(x)Î¼(dx) and the supremum in (19) runs over all measurable
functions f0 : X â†’R such that |f0(x)| â‰¤f(x) for all x âˆˆX (when f â‰¡1, the f-norm
âˆ¥Î¼âˆ¥f reduces to the total variation norm âˆ¥Î¼âˆ¥TV = supf0:|f0|â‰¤1 |Î¼(f0)| used in (1) and
(2)). When the n-step probability measures Pn(x; Â·) converge in f-norm and at rate
r(n) to the stationary probability measure Ï€ satisfying Ï€(f) < âˆ, that is,
lim
nâ†’âˆr(n)âˆ¥Pn(x; Â·)âˆ’Ï€âˆ¥f = 0
for Ï€-almost all3 x âˆˆX,
(20)
we say that the Markov chain Xt is (f,r)-ergodic; this implicitly entails the
existence of Ï€ as well as certain moments as Ï€(f) < âˆ. In the conventional
geometrically ergodic case, r(n) = rn for some r > 1. To establish (f,r)-ergodicity,
we use a so-called drift condition defined as follows (here 1S(x) denotes the
indicator function taking value one when x belongs to the set S and zero elsewhere).
Condition D. There exist a measurable function V : X â†’[1,âˆ), a concave
increasing continuously differentiable function Ï† : [1,âˆ) â†’(0,âˆ), a measurable
set C, and a finite constant Ëœb such that
E[V(X1) |X0 = x] â‰¤V(x)âˆ’Ï† (V(x))+ Ëœb1C(x),
x âˆˆX.
(21)
The idea is to verify this condition with suitable functions V and Ï†, which together
with some additional conditions ensures the (f,r)-ergodicity of the process Xt; for
more details, see Meitz and Saikkonen (2022, Thm. 1).
Now, consider the stability of the Markov chain yt on Rp+q given in (8). To
define the function V in (21), we use the functions z1(Â·), z2(Â·), and Î¾(Â·) in (11),
(12), and (14) and the norms âˆ¥Â·âˆ¥âˆ—and âˆ¥Â·âˆ¥â€¢ in Lemma 1 and Assumption 4.
3That is, the convergence in (20) is required to hold for all x âˆˆX except for those x in a set that has probability zero
with respect to the stationary measure Ï€.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

12
MIKA MEITZ AND PENTTI SAIKKONEN
Set x = (x1,...,xp+q) âˆˆRp+q and decompose x to its p- and q-dimensional
components as x = (x1,x2). We define the function V as
V(x) = 1+|z1(x1)|2s0 +s1âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
+s2âˆ¥Î¾(x)âˆ¥bs0
â€¢ ,
(22)
where s0 and b are defined above Assumption 4, s1 and s2 are positive constants
to be specified later (with s1 small and s2 large), and Î± = 1 âˆ’Ï/2s0 (recall from
Assumption 2 that Ï âˆˆ(0,2) so that Î± âˆˆ(0,1)). It may be clarifying to note that
when p = 1, model (3) reduces to yt = g(ytâˆ’1) +ÏƒtÎµt; then we can set s1 = 0 and
drop z2(x1) so that the function V in (22) becomes V(x) = 1+|x1|2s0 +s2âˆ¥Î¾(x)âˆ¥bs0
â€¢ .
To verify Condition D, we need to consider the conditional expectation
E

V(y1) | y0 = x

= 1+E

|z1(y1,1)|2s0 | y0 = x

+s1E

âˆ¥z2(y1,1)âˆ¥2s0Î±
âˆ—
| y0 = x

+s2E

âˆ¥Î¾(y1)âˆ¥bs0
â€¢
| y0 = x

,
(23)
bound the conditional expectations on the right-hand side of (23), and express these
bounds in a way which conforms to inequality (21) with the function Ï† satisfying
the conditions required in Condition D. These considerations, combined with the
checking of some additional technical conditions, lead to the following theorem
(the proof can be found in Appendix A).
Theorem 1. Consider the Markov chain yt defined in (8). Suppose that Assump-
tions 1â€“4 hold and that V(x) is as in (22). Then yt is (f,r)-ergodic with the
polynomial convergence rate r(n) = nÎ´âˆ’1 and the function f given by f(x) =
V(x)1âˆ’Î´Ï/2s0; this result holds for any choice of Î´ âˆˆ[1,2s0/Ï] and for some (small
enough) s1 > 0 and some (large enough) s2 > 0.
Theorem 1 provides the first subgeometric ergodicity results for autoregres-
sions with autoregressive conditional heteroskedasticity. In this theorem, the
convergence rate r(n) shows the speed at which the n-step transition probability
measures of the process yt converge to the stationary probability measure. Due
to the polynomial convergence rate, we therefore call the process yt polynomially
ergodic. Note also that the choice of Î´ in Theorem 1 allows for a trade-off between
the rate of convergence and the size of the f-norm.
3.2. Discussion
3.2.1. Geometric Ergodicity.
In previous literature, geometric ergodicity of
nonlinear autoregressions with ARCH errors has been considered using a variety
of different assumptions for the allowed nonlinear dynamics and for the required
moment conditions for the innovations (see, e.g., Cline and Pu, 2004; Meitz and
Saikkonen, 2010, and the many references therein).
3.2.2. Homoskedastic Case.
Theorem 1 remains valid also in the homoskedas-
tic case (obtained by setting Î±1 = Â·Â·Â· = Î±q = 0). Previous polynomial ergodicity
results for homoskedastic autoregressions were obtained by Fort and Moulines
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
13
(2003, Sect. 2.2) and Meitz and Saikkonen (2022, Thm. 3), and Theorem 1
provides partial improvements over these earlier results in certain cases.
Assumptions and notation are slightly different in all the papers, but (in the
notation of the present paper) Theorem 1 improves earlier results when 1 â‰¤Ï < 2
and 1 < s0 < 2.
3.2.3. Proof Strategy.
The proof of Theorem 1 is also somewhat different from
the previous polynomial ergodicity results in Fort and Moulines (2003, Sect. 2.2)
and Meitz and Saikkonen (2022, Thm. 3). A rather obvious difference is that
these earlier results deal with homoskedastic autoregressions, whereas our model
contains a nonlinear ARCH term, the size of which is controlled with the special
matrix norm defined in Assumption 4. Regarding the conditional expectation, the
mentioned earlier results rely on Lemma 3 in Fort and Moulines (2003), while
our proof of Theorem 1 avoids the use of this lemma, and instead makes use of
a (relatively unknown) extension of Bernoulliâ€™s inequality due to Fefferman and
Shapiro (1972) (combined with Youngâ€™s inequality).
3.2.4. Mixing and Moment Results.
As already indicated in the Introduction,
the polynomial ergodicity result of Theorem 1 also implies that the process
yt is Î²-mixing (and hence Î±-mixing). Moreover, the convergence rate of the
Î²-mixing coefficients Î²(n) is given by the fastest convergence rate, that is,
limnâ†’âˆn2s0/Ïâˆ’1Î²(n) = 0. For further details and justifications of these mixing
results, see Meitz and Saikkonen (2021, Thm. 2) and Meitz and Saikkonen (2022,
Sect. 2).
Another consequence of Theorem 1 is that the stationary distribution of yt has
finite moments up to order 2s0 âˆ’Ï (for a proof, see Appendix A). Note that
depending on the values of s0 â‰¥1 and Ï âˆˆ(0,2), the order of these finite moments
may be very small; in particular, when s0 = 1, we do not obtain a finite variance.
3.2.5. Subexponential Ergodicity.
Theorem 1 concerns only polynomial
ergodicity of subgeometric ARâ€“ARCH models, and does not consider subex-
ponential ergodicity (where the rate r(n) in (2) equals, say, ecnÎ³ with c > 0 and
0 < Î³ < 1). The reason for this is that the properties of ARCH-type models do
not seem compatible with the moment requirements needed for subexponential
ergodicity. To elaborate on this, first note that the previous results of Douc et al.
(2004, Sect. 3.3) and Meitz and Saikkonen (2022, Sect. 4.1) on subexponential
ergodicity of homoskedastic nonlinear autoregressions (i) require the IID error
term to possess moments of all orders and (ii) imply that the observed process
yt also has finite moments of all orders. (To provide some further details, (ii) is
given as Corollary to Theorem 2 in Meitz and Saikkonen (2022). As for (i), see
Assumptions 3.3 and 2(a) of Douc et al. (2004) and Meitz and Saikkonen (2022),
respectively. These assumptions require the IID error terms to be sub-Weibull
random variables, which in turn entails they possess moments of all orders; see
Vladimirova et al. (2020, Def. 1 and Thm. 1) or Wong et al. (2020, Def. 3 and
Lem. 5).)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

14
MIKA MEITZ AND PENTTI SAIKKONEN
The abovementioned moment requirements are in stark contrast to ARCH-type
models. For instance, in the simplest ARCH(1) model (et = ÏƒtÎµt, Ïƒ 2
t = Ï‰+Î±1e2
tâˆ’1,
and Îµt IID N(0,1)), the finiteness of moments of order 2r for et (E[|et|2r] < âˆ)
is known to require the condition Î±r
1E[|Îµt|2r] < 1 (see, e.g., Ling and McAleer,
2002, Thm. 2.1; Ling, 1999, Exam. 6.1). For integer values of r, this condition is
equivalent with Î±1 < [(2r âˆ’1)!!]âˆ’1/r = [1Â·3Â·... Â·(2r âˆ’1)]âˆ’1/r and consequently
all moments of the ARCH process et cannot be finite unless Î±1 = 0. The situation
is similar also in more complicated (G)ARCH and ARâ€“(G)ARCH models (see,
e.g., Meitz and Saikkonen, 2008a, Thm. 2; Meitz and Saikkonen, 2008b, Thm. 1,
respectively). This suggests that ARCH-type heteroskedastic errors may not be
compatible with the moment requirements needed for subexponential ergodicity.
3.2.6. Potential Extensions.
Extending our results to allow for GARCH (and
not only ARCH) errors would be interesting. However, previous literature suggests
that studying the stability of nonlinear ARâ€“GARCH models can be challenging.
Geometric ergodicity of nonlinear ARâ€“GARCH models has previously been
studied by Liu, Li, and Li (1997), Ling (1999), Cline (2007), and Meitz and
Saikkonen (2008b); of these articles, the former two are confined to threshold ARâ€“
GARCH models, whereas the latter two consider more general nonlinear autore-
gressions. In the present setting, the autoregressive part of the model we consider
is rather general (the restrictions imposed on function g(Â·) in Assumption 2(ii)
are quite mild, essentially restricting g(Â·) only for large values of its argument)
and techniques used for threshold models cannot be applied. Using an approach
similar to Cline (2007) appears challenging as the assumptions he employs are
quite general and appear difficult to verify (in fact, a threshold ARâ€“GARCH model
is the only example that is explicitly treated in his article). On the other hand, Meitz
and Saikkonen (2008b) require certain structure and smoothness of the conditional
mean (see Assumption 2 of their paper), and it is not clear how to apply these results
in the current setting. As the extension to GARCH errors appears challenging, we
leave it for future research.
Another useful extension would be to consider the subgeometric ergodicity
of multivariate autoregressions with autoregressive conditional heteroskedasticity.
Fort and Moulines (2003, Sect. 2.2) and Douc et al. (2004, Sect. 3.3) already stud-
ied multivariate first-order autoregressions with IID errors and obtained results for
polynomial and subexponential ergodicity, respectively. In principle, generalizing
these results to the higher-order case with multivariate ARCH errors should be
possible, but it is not immediate how to formulate a general model that would
be both theoretically manageable and useful in practical applications. We hope to
return to this issue in subsequent work.
3.3. Examples
The conditional mean of the model we have so far discussed is very general, and
we next consider some concrete illustrating examples. The following two special
cases were introduced in Meitz and Saikkonen (2022, Sect. 5) in the case of a
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
15
homoskedastic error term. We first consider a model with a time-varying intercept
term based on a logistic function and specified as
yt = Î½1L(utâˆ’1;Î³,a1)+Î½2(1âˆ’L(utâˆ’1;Î³,a2))+ytâˆ’1
+Ï€tâˆ’1ytâˆ’1 +Â·Â·Â·+Ï€pâˆ’1ytâˆ’p+1 +ÏƒtÎµt,
(24)
where L(u;Î³,a) = 1/(1+eâˆ’Î³ (uâˆ’a)) is the logistic function and the parameters Î³ ,
a1, a2 are assumed to satisfy Î³ > 0 and a1 â‰¤a2, and Î½1, Î½2 are assumed to satisfy
Î½1 < 0 < Î½2. Moreover,  signifies the difference operator (so that ytâˆ’1 = ytâˆ’1 âˆ’
ytâˆ’2), and the remaining notation is as in model (3). Arguments similar to those
in Meitz and Saikkonen (2022, Proof of Prop. 1) can now be used to prove the
following result (for details, see Appendix A).
Proposition 1. Consider the process yt defined in equation (24) and suppose
that Assumptions 1, 2(i), 3, and 4 hold. Then, yt is polynomially ergodic with
convergence rate r(n) = n2s0âˆ’1 and finite moments up to order 2s0 âˆ’1.
The convergence rate presented in Proposition 1 also shows the rate of Î²-mixing
coefficients.
As another special case, we consider a model with a time-varying slope term
defined as
yt = Ï€tâˆ’1ytâˆ’1 +Â·Â·Â·+Ï€pâˆ’1ytâˆ’p+1 +S(utâˆ’1)utâˆ’1 +ÏƒtÎµt,
(25)
where S(utâˆ’1) is either S1(utâˆ’1) = 1âˆ’r0/h(utâˆ’1) or S2(utâˆ’1) = exp{âˆ’r0/h(utâˆ’1)}
(with r0 > 0) and the function h : R â†’(0,âˆ) as defined in Proposition 2 of
Meitz and Saikkonen (2022, Sect. 5.2). In addition to a general formulation
of the function h that proposition provides six special cases of which two are
h(u) = 1 + |u âˆ’a|Ï and h(u) = (1 + (u âˆ’a)2)Ï/2 (where a âˆˆR and Ï âˆˆ(0,2);
see Assumption 2). Regarding the remaining notation, it is as in model (3).
The following result can be established by using arguments similar to those in
the proof of Proposition 2 in Meitz and Saikkonen (2022, Sect. 5.2) (for details,
see Appendix A).
Proposition 2. Consider the process yt defined in equation (25) and suppose
that Assumptions 1, 2(i), 3, and 4 hold. Then, yt is polynomially ergodic with
convergence rate r(n) = n2s0/Ïâˆ’1 and finite moments up to order 2s0 âˆ’Ï.
The rate of Î²-mixing coefficients coincides with the rate given in the proposi-
tion. As the function h depends on the parameter Ï âˆˆ(0,2), the convergence rate in
Proposition 2 differs from that obtained in Proposition 1 except in the case Ï = 1.
4. EMPIRICAL APPLICATION
Although theoretical work on subgeometric ergodicity has been ongoing for
four decades, practical illustrations of (homoskedastic) subgeometrically ergodic
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

16
MIKA MEITZ AND PENTTI SAIKKONEN
autoregressions have been scarce; we are not aware of any previous empirical
applications of subgeometrically ergodic autoregressions using real data. A small
illustration of simulated data from one subgeometrically ergodic autoregression is
given in Fort and Moulines (2003, Sect. 3). Meitz and Saikkonen (2022, Sect. 5)
provide examples of some concrete subgeometrically ergodic autoregressive time
series models and illustrations of a few simulated data series from them. These
simulation exercises suggest that subgeometrically ergodic autoregressions could
be useful when the observed time series bears some resemblance to unit-root type
behavior and the autocorrelation function indicates very strong persistence, but
when the time series nevertheless exhibits eventual mean-reverting behavior. The
discussion in Section 2.1 around equation (6) had a similar message, suggesting
that these models could be seen as a middle ground between the extreme cases of
geometric ergodicity and non-ergodicity.
We next illustrate the use of subgeometrically ergodic ARâ€“ARCH models in
a small empirical example. Our aim is simply to provide a proof of concept
for the applicability of subgeometrically ergodic ARâ€“ARCH models, illustrating
that the model used fits the data well. Further work is certainly needed to judge
the usefulness of these models in practical applications, but we leave such more
comprehensive empirical applications for future research.
The data we employ consist of daily observations on the Chicago Board
Options Exchange energy sector volatility index (https://fred.stlouisfed.org/series/
VXXLECLS) over the period 16 March 2011 to 31 December 2021 (a total of
2,719 observations). This data series reflects energy sector risk and is displayed in
the top-left graph of Figure 1 (the solid graph; the dashed horizontal line shows the
estimate Ë†a = 25.366, see (26) and (27) below). The time series plot shows signs
of strong persistence, which is also reflected in the autocorrelation function of the
data shown in the top-right graph of Figure 1.
We model this data series using the parametric specification in (24). As for the
error distribution, after some experimentation, a skew version of the t-distribution
due to Jones and Faddy (2003) was found to provide a good fit (in contrast,
estimation with normal errors led to a distinct discrepancy between the residual
distribution and the Gaussian one). The density function of this distribution is
f(x;c,d) = Câˆ’1
c,d

1+
x
(c+d +x2)1/2
c+1/2
1âˆ’
x
(c+d +x2)1/2
d+1/2
,
where c and d are positive parameters and Cc,d = 2c+dâˆ’1B(c,d)(c + d)1/2 (with
B(Â·,Â·) denoting the beta function); the case c = d results in a symmetric
t-distribution with 2c degrees of freedom and the cases c < d and c > d imply
skewness to the left and right, respectively. In our application, we use this
distribution centralized to have mean zero and standardized to have unit variance
(i.e., in the density function, x is replaced by sx + m and Câˆ’1
c,d by sCâˆ’1
c,d, where m
and s2 denote the mean and variance [see Jones and Faddy, 2003, Sect. 2.1]; this
requires that c > 1 and d > 1, for a moment of order k is finite when c > k/2 and
d > k/2).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
17
2012
2014
2016
2018
2020
20 40 60 80
120
0
50
100
150
200
250
0.0 0.2 0.4 0.6 0.8 1.0
0
20
40
60
80
100
120
140
âˆ’0.2 âˆ’0.1
0.0
0.1
2012
2014
2016
2018
2020
âˆ’0.20 âˆ’0.10 0.00
0.10
2012
2014
2016
2018
2020
0
5
10
15
20
2012
2014
2016
2018
2020
âˆ’4 âˆ’2 0
2
4
6
Figure 1. Top row: Daily observations on the Chicago Board Options Exchange energy sector
volatility index, 16 March 2011 to 31 December 2021 (left); the corresponding autocorrelation function
(right). Middle row: The function I(x) = âˆ’Î½L(x;Î³,a)+Î½(1âˆ’L(x;Î³,a)) (left) and the corresponding
time-varying intercept term I(ytâˆ’1) (right), based on parameter estimates in (27). Bottom row: The
estimated volatility series Ë†Ïƒt (left) and residual series Ë†Îµt (right), based on parameter estimates in (27).
We estimate the model parameters using the method of maximum likelihood and
employ optimization routines in R. (We simply assume that standard properties of
maximum likelihood estimators hold and calculate standard errors based on the
standard formulas.) Trying out different model orders led to model (24) with order
p = 1 and with a nonlinear ARCH term of order q = 3 (with these choices, the
residual diagnostics shown in Figure B1 in Appendix B indicated a very good fit).
Specifically, the considered model is
yt = ytâˆ’1 âˆ’Î½L(ytâˆ’1;Î³,a)+Î½(1âˆ’L(ytâˆ’1;Î³,a))+ÏƒtÎµt,
Ïƒ 2
t = (Ï‰ +Î±1e2
tâˆ’1 +Î±2e2
tâˆ’2 +Î±3e2
tâˆ’3)L(ytâˆ’1;Î³,a),
(26)
where the errors Îµt are IID(0,1) and follow the above described (centralized
and standardized) skew t-distribution, L(y;Î³,a) = 1/(1+eâˆ’Î³ (yâˆ’a)) is the logistic
function, the parameters Î½ and Î³ are positive, and a âˆˆR. (We also tried a model
where the logistic functions in the conditional expectation and in the ARCH term
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

18
MIKA MEITZ AND PENTTI SAIKKONEN
were different, but this extension had only a minor effect on the results.) ML
estimation (with the constraints Î½,Î³,Ï‰ > 0, Î±1,Î±2,Î±3 â‰¥0, and Î±1 + Î±2 + Î±3 < 1
that ensure polynomial ergodicity by Proposition 1) leads to the following results:
yt = ytâˆ’1 âˆ’0.187
(0.040)L(ytâˆ’1;0.171
(0.018),25.366
(1.434) )+0.187
(0.040)(1âˆ’L(ytâˆ’1;0.171
(0.018),25.366
(1.434) ))+ Ë†ÏƒtË†Îµt,
Ë†Ïƒ 2
t = (3.259
(0.493) +0.406
(0.081)e2
tâˆ’1 +0.310
(0.066)e2
tâˆ’2 +0.149
(0.052)e2
tâˆ’3)L(ytâˆ’1;0.171
(0.018),25.366
(1.434) ),
(27)
where the numbers in parentheses are standard errors; estimates for the parameters
in the error distribution are Ë†c = 3.551(0.422) and Ë†d = 2.138(0.197).
To illustrate the conditional mean of the estimated model, consider the func-
tion I(x) = âˆ’Î½L(x;Î³,a) + Î½(1 âˆ’L(x;Î³,a)) and the corresponding time-varying
intercept term I(ytâˆ’1) based on the above parameter estimates. These are shown
in the middle row of Figure 1. In the left panel, the two horizontal dashed lines
show the minimum and maximum I(x) attains, while the three vertical dashed
lines indicate the minimum of the observed data series yt (11.71), the estimate
Ë†a = 25.366, and the maximum of yt (130.61). On the right, the three horizontal
dashed lines show the minimum and maximum I(ytâˆ’1) attains (âˆ’0.187 and 0.154)
and the origin. Intuitively, when ytâˆ’1 is close to Ë†a, the time-varying intercept term
I(ytâˆ’1) is close to zero and the conditional mean of (27) corresponds to unit-root
type behavior (without drift); when ytâˆ’1 takes values clearly below/above Ë†a, the
intercept I(ytâˆ’1) is positive/negative and behavior akin to a unit-root process with
increasing/decreasing drift occurs.
The left panel in the bottom row of Figure 1 displays the estimated volatility
series Ë†Ïƒt. The variation of the volatility over time is strong, and the large spikes
in the volatility series coincide with the large values in the observed series. The
logistic formulation of the conditional variance in (27) makes it possible for large
observations to amplify volatility more than a standard linear ARCH model would
allow for.
The right panel in the bottom row of Figure 1 shows the residual series Ë†Îµt. Four
additional graphs analyzing the residuals are available in Figure B1 in Appendix B:
autocorrelation functions of the residuals and of the squared residuals, together
with a histogram and a Qâ€“Q plot. The autocorrelation functions reveal that the
very strong persistence present in the original series has been quite well captured
by the estimated subgeometrically ergodic ARâ€“ARCH model (only three of the
shown 100 autocorrelation coefficients are barely outside the displayed critical
values). The histogram and the Qâ€“Q plot indicate that the employed skew version
of the t-distribution fits well as only a few outlying observations deviate from the
estimated density function and the 45 degree line.
Note also that the estimated ARâ€“ARCH model satisfies the requirements of a
stationary and polynomially ergodic process with finite absolute moments.4 It may
4That is, the parameter estimates in (27) correspond to a process satisfying the requirements of Proposition 1 with
s0 = 1. (Note that these requirements are not satisfied with s0 = 1.5, which would correspond to finite second moments
of yt.)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
19
be interesting to note that estimation attempts using standard linear ARMA(1,1)â€“
GARCH(1,1) models (with skew t errors) led to estimated autoregressive coeffi-
cients in excess of 0.999, reflecting the very persistent nature of the data series
apparent from the time series and autocorrelation plots in the top row of Figure 1.
The primary purpose of this small empirical example was to demonstrate what
kind of time series could be modeled with subgeometrically ergodic ARâ€“ARCH
models. It is worth pointing out that such models may work well even in cases
where the graphs of the employed time series and related autocorrelation functions
look very different from those displayed in Figure 1.
5. CONCLUSIONS
In this paper, we examined the subgeometric ergodicity of nonlinear autoregressive
models with autoregressive conditional heteroskedasticity. We provided conditions
that ensured polynomial ergodicity of the considered ARâ€“ARCH models. Our
results generalized existing results that assumed the error terms to be IID. The use
of subgeometrically ergodic ARâ€“ARCH models was illustrated in an empirical
example using energy sector volatility index data.
Several future research topics could be entertained. In this paper, we have only
considered ARCH-type conditional heteroskedasticity, and extending the results to
the generalized ARCH (GARCH) case would be of interest. Subgeometric ergod-
icity of multivariate autoregressions with autoregressive conditional heteroskedas-
ticity is another interesting topic left for future work. On the empirical side, further
applied work is certainly needed to judge the usefulness of subgeometrically
ergodic autoregressions (with or without ARCH) in practical applications. For
instance, providing more concrete advice on when to use subgeometrically (rather
than geometrically) ergodic autoregressions would be useful for practitioners.
Another question future applications should address is whether subgeometrically
ergodic autoregressions can outperform relevant competing models in out-of-
sample forecasting exercises.
A. APPENDIX A
This appendix contains the proofs of Lemma 2, Theorem 1, and Propositions 1 and 2 as
well as details for the finiteness of moments in Section 3.2.
Proof of Lemma 2. Define the vector (Â¯Î±1,..., Â¯Î±q) = (Î±1 Â¯Î¼2s0b,...,Î±q Â¯Î¼2s0b), and let
Â¯ denote the q Ã— q matrix obtained by replacing the first row of the matrix t by
(Â¯Î±1,..., Â¯Î±q). By assumption, Â¯Î±1,..., Â¯Î±q â‰¥0 and q
i=1 Â¯Î±i < 1. These conditions ensure that
the polynomial p(t) = tq âˆ’Â¯Î±1tqâˆ’1 âˆ’Â·Â·Â· âˆ’Â¯Î±q has all its roots inside the unit circle (if a
root t with |t| â‰¥1 existed, the contradiction 1 = Â¯Î±1/t +Â·Â·Â·+ Â¯Î±q/tq â‰¤Â¯Î±1 +Â·Â·Â·+ Â¯Î±q would
follow); this in turn implies that the matrix Â¯ has spectral radius Ï( Â¯) < 1 (see Horn and
Johnson, 2013, pp. 194â€“195). Therefore, the matrix Iq âˆ’Â¯ is invertible with (Iq âˆ’Â¯)âˆ’1 =
âˆ
i=0 Â¯i. Set 1q = (1,...,1) (q Ã— 1) and let (x)abs = (|x1|,...,|xq|) (q Ã— 1) denote the
elementwise absolute value of a vector x âˆˆRq. We define the vector norm âˆ¥Â· âˆ¥â€¢ on Rq as
âˆ¥xâˆ¥â€¢ = 1â€²q(Iqâˆ’Â¯)âˆ’1(x)abs. Note that as (Iqâˆ’Â¯)âˆ’1 = âˆ
i=0 Â¯i with Â¯ having nonnegative
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

20
MIKA MEITZ AND PENTTI SAIKKONEN
(and also some strictly positive) entries, âˆ¥xâˆ¥â€¢ = 1â€²q(Iq âˆ’Â¯)âˆ’1(x)abs > 1â€²q(x)abs = âˆ¥xâˆ¥1
whenever x Ì¸= 0 (here âˆ¥Â·âˆ¥1 denotes the usual l1 vector norm).
Now, let x Ì¸= 0 be arbitrary and consider âˆ¥txâˆ¥â€¢. To this end, note that |Î±1Îµ2t x1 +
Â·Â·Â· + Î±qÎµ2t xq| â‰¤Î±1Îµ2t |x1| + Â·Â·Â· + Î±qÎµ2t |xq|, which implies that the elementwise inequality
(tx)abs â‰¤t(x)abs holds (with probability one; note that only the first elements differ).
Thus, also,
âˆ¥txâˆ¥â€¢ = 1â€²
q(Iq âˆ’Â¯)âˆ’1(tx)abs â‰¤1â€²
q(Iq âˆ’Â¯)âˆ’1t(x)abs.
As bs0 â‰¥1, Minkowskiâ€™s inequality and the definition of the vector (Â¯Î±1,..., Â¯Î±q) yield
E[{1â€²
q(Iq âˆ’Â¯)âˆ’1t(x)abs}bs0]1/bs0 â‰¤1â€²
q(Iq âˆ’Â¯)âˆ’1 Â¯(x)abs,
where, as (Iq âˆ’Â¯)âˆ’1 Â¯ = (Iq âˆ’Â¯)âˆ’1 âˆ’Iq and âˆ¥xâˆ¥â€¢ > âˆ¥xâˆ¥1,
1â€²
q(Iq âˆ’Â¯)âˆ’1 Â¯(x)abs = âˆ¥xâˆ¥â€¢ âˆ’âˆ¥xâˆ¥1 = âˆ¥xâˆ¥â€¢(1âˆ’âˆ¥xâˆ¥1/âˆ¥xâˆ¥â€¢) < âˆ¥xâˆ¥â€¢.
These derivations establish that
âˆ¥txâˆ¥â€¢Lbs0 = (E[âˆ¥txâˆ¥bs0
â€¢ ])1/bs0 < âˆ¥xâˆ¥â€¢
and that
|||t|||â€¢Lbs0 =
max
âˆ¥xâˆ¥â€¢Lbs0 =1âˆ¥txâˆ¥â€¢Lbs0 = max
âˆ¥xâˆ¥â€¢=1âˆ¥txâˆ¥â€¢Lbs0 < 1.
Finally, by its definition, it is clear that the vector norm âˆ¥Â·âˆ¥â€¢ is monotone.
â–¡
Proof of Theorem 1. For clarity, we break down the long proof into several intermediate
steps.
Step 1: Preliminaries. We first consider the function V defined in (22) and the condi-
tional expectation E[V(y1) | y0 = x]. As before, we decompose an x âˆˆRp+q to its p- and
q-dimensional components as x = (x1,x2); similarly, we decompose y1 as y1 = (y1,1,y2,1).
For any x âˆˆRp+q, it is convenient to define
V1(x1) = |z1(x1)|2s0,
V2(x1) = s1âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
,
and
V3(x) = s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
so that V(x) = 1+V1(x1)+V2(x1)+V3(x) (when p = 1, we can set s1 = 0 and drop z2(x1)
and V2). We next consider the three conditional expectations
E[V1(y1,1) | y0 = x] = E[|z1(y1,1)|2s0 | y0 = x],
(28)
E[V2(y1,1) | y0 = x] = E[s1âˆ¥z2(y1,1)âˆ¥2s0Î±
âˆ—
| y0 = x],
(29)
E[V3(y1) | y0 = x] = E[s2âˆ¥Î¾(y1)âˆ¥bs0
â€¢
| y0 = x]
(30)
related to functions V1, V2, and V3. In Steps 2â€“4 below, we establish that these conditional
expectations can be bounded from above using the following upper bounds:
E[|z1(y1,1)|2s0 | y0 = x] â‰¤|z1(x1)|2s0 âˆ’Ëœr|z1(x1)|2s0Î± +Câˆ¥Î¾(x)âˆ¥bs0
â€¢
+C,
(31)
E[s1âˆ¥z2(y1,1)âˆ¥2s0Î±
âˆ—
| y0 = x] â‰¤s1 âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–sÎ±
1 âˆ¥z2(x1)âˆ¥2s0Î±2
âˆ—
+ Ëœs1 |z1(x1)|2s0Î± +C,
(32)
E[s2âˆ¥Î¾(y1)âˆ¥bs0
â€¢
| y0 = x] â‰¤s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’ËœÎ»s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
+C,
(33)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
21
where Ëœr, ËœÏ–,Ëœs1, ËœÎ» > 0 with ËœÎ» < 1 and where Ëœs1 can be made as close to zero as desired by
choosing a small enough s1 (and Ëœs1 = 0 when p = 1). Moreover, here and in what follows,
for simplicity, we use C to denote a finite positive constant whose value may change from
occurrence to occurrence (alternatively, we could use C1,C2,...). For brevity, we also often
(but not always) drop the argument x1 from z1(x1) and z2(x1) and simply write z1 and z2.
For ease of reference, we also note here that Assumption 4 allows us to bound the
conditional variance as follows. By the definition of Ïƒ 2t in (13), Assumption 3, and definition
of Î¾(ytâˆ’1) in (14), Ïƒ 2t â‰¤Ï‰ +e2
tâˆ’1 +Â·Â·Â·+e2
tâˆ’q = Ï‰ +âˆ¥Î¾(ytâˆ’1)âˆ¥1 (with âˆ¥Â·âˆ¥1 denoting the
usual l1 vector norm). The equivalence of vector norms on Rq and the fact that Ï‰ is a (finite)
constant implies that (for some finite constant C)
Ïƒ 2
t = Ïƒ 2(ytâˆ’1) â‰¤C(1+âˆ¥Î¾(ytâˆ’1)âˆ¥â€¢) a.s.
and
Ïƒ 2(x) â‰¤C(1+âˆ¥Î¾(x)âˆ¥â€¢) for all fixed x.
(34)
Step 2: Upper bound for V1. Using (12), the conditional expectation in (28) can be
expressed as
E[V1(y1,1) | y0 = x] = E[|g(z1(x1))+Ïƒ(x)Îµ1|2s0].
For any positive real number Z, define the set S1(Z) = {x âˆˆRp+q : |z1(x1)| â‰¤Z} and let
Sc
1(Z) denote the complement of this set.
First, consider values of x such that x âˆˆSc
1(Z) so that |z1| = |z1(x1)| > Z. Choose Z large
enough to ensure that g(z1) Ì¸= 0 (Assumption 2) so that |g(z1)+Ïƒ(x)Îµ1|2s0 can be written
as
|g(z1)|2s0 |1+Ïƒ(x)Îµ1/g(z1)|2s0.
(35)
We first bound the latter term in this expression using the following extension of Bernoulliâ€™s
inequality due to Fefferman and Shapiro (1972): for any a â‰¥2, there exist positive numbers
A and B such that
|1+u|a â‰¤1+au+Au2 +B|u|a
(36)
for all u âˆˆR. Using (36), the latter term in (35) is dominated by
1+2s0
Ïƒ(x)
g(z1)Îµ1 +A Ïƒ(x)2
g(z1)2 Îµ2
1 +B Ïƒ(x)2s0
|g(z1)|2s0 |Îµ1|2s0.
This upper bound, (35), the facts E[Îµ1] = 0 and E[Îµ2
1] = 1 (Assumption 1), and the notation
Î¼2s0 = E[|Îµ1|2s0], now yield
E[V1(y1,1) | y0 = x] â‰¤|g(z1)|2s0 +A|g(z1)|2s0âˆ’2Ïƒ(x)2 +BÏƒ(x)2s0Î¼2s0.
Choose Z large enough to ensure that 0 < 1âˆ’r|z1|âˆ’Ï < 1 and |g(z1)| â‰¤(1âˆ’r|z1|âˆ’Ï)|z1|
(Assumption 2). Using the elementary inequalities (1âˆ’u)a1 â‰¤1âˆ’u and (1âˆ’u)a2 â‰¤1 for
all 0 < u < 1, a1 â‰¥1, and a2 â‰¥0, and recalling that s0 â‰¥1 and Ïƒ 2(x) â‰¤C(1+âˆ¥Î¾(x)âˆ¥â€¢)
(see (34)), we obtain
E[V1(y1,1) | y0 = x]
â‰¤|z1|2s0 âˆ’r|z1|2s0âˆ’Ï +C|z1|2s0âˆ’2 +C|z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ +Câˆ¥Î¾(x)âˆ¥s0â€¢ Î¼2s0
for some positive C (by choosing Z large enough, the constant term on the dominant side has
been absorbed into |z1|2s0). To merge the terms âˆ’r|z1|2s0âˆ’Ï and C|z1|2s0âˆ’2, by choosing
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

22
MIKA MEITZ AND PENTTI SAIKKONEN
Z large enough to ensure that C/r|z1|2âˆ’Ï < 1, we have
âˆ’r|z1|2s0âˆ’Ï +C|z1|2s0âˆ’2 = âˆ’r|z1|2s0âˆ’Ï(1âˆ’C/r|z1|2âˆ’Ï) â‰¤âˆ’Ë†r|z1|2s0âˆ’Ï
for some positive constant Ë†r. Hence,
E[V1(y1,1) | y0 = x]
(37)
â‰¤|z1|2s0 âˆ’Ë†r|z1|2s0âˆ’Ï +C|z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ +Câˆ¥Î¾(x)âˆ¥s0â€¢ Î¼2s0 for all x âˆˆSc
1(Z).
Now, consider values of x such that x âˆˆS1(Z). As inequality (5) implies that g(z1) is
bounded on S1(Z), triangle inequality and the elementary inequality

m

i=i
ai

r
â‰¤cr
m

i=i
|ai|r
where cr = 1 for 0 < r â‰¤1 and cr = mrâˆ’1 for r > 1
(38)
for any real numbers a1,...,am (see, e.g., Davidson, 1994, p. 140) imply that |g(z1) +
Ïƒ(x)Îµ1|2s0 is dominated by C(1+Ïƒ 2s0(x)|Îµ1|2s0) (for some C > 0; we omit this statement
from now on) for all x âˆˆS1(Z). As Î¼2s0 = E[|Îµ1|2s0] and Ïƒ 2(x) â‰¤C(1+âˆ¥Î¾(x)âˆ¥â€¢), it is
seen that
E[V1(y1,1) | y0 = x] â‰¤C(1+âˆ¥Î¾(x)âˆ¥s0â€¢ Î¼2s0)
for all
x âˆˆS1(Z).
(39)
Combining (37) and (39), noting that 2s0 âˆ’Ï = 2s0Î± (see the discussion following (22)),
and merging constants, we can conclude that for all x âˆˆRp+q,
E[V1(y1,1) | y0 = x] â‰¤|z1|2s0 âˆ’Ë†r|z1|2s0Î± +C|z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ +Câˆ¥Î¾(x)âˆ¥s0â€¢ +C.
(40)
For future developments, it is convenient to further manipulate this upper bound. First,
consider the product |z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ appearing in (40) and momentarily focus on the case
s0 > 1 (when also b > 1). Using Youngâ€™s inequality (with exponents bs0/(bs0 âˆ’1) and bs0)
yields
|z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ â‰¤bs0 âˆ’1
bs0
|z1|2s0b(s0âˆ’1)/(bs0âˆ’1) + 1
bs0
âˆ¥Î¾(x)âˆ¥bs0
â€¢ .
Simple calculations show that the assumption b > (2s0 âˆ’Ï)/[s0(2 âˆ’Ï)] implies that
2s0b(s0 âˆ’1)/(bs0 âˆ’1) < 2s0Î±. Therefore, for some small positive Ï‚,
|z1|2s0âˆ’2âˆ¥Î¾(x)âˆ¥â€¢ â‰¤C(1+|z1|2s0Î±âˆ’Ï‚ +âˆ¥Î¾(x)âˆ¥bs0
â€¢ );
(41)
clearly, this upper bound also holds in the case s0 = 1.
Second, consider the terms involving |z1|2s0Î± in (40) and |z1|2s0Î±âˆ’Ï‚ in (41). By
considering values of |z1| larger and smaller than some large bound, it is straightforward to
see that
C|z1|2s0Î±âˆ’Ï‚ âˆ’Ë†r|z1|2s0Î± = âˆ’Ë†r(1âˆ’C/[Ë†r|z1|Ï‚])|z1|2s0Î± â‰¤C âˆ’Ëœr|z1|2s0Î±
for some positive constant Ëœr. Third, the term âˆ¥Î¾(x)âˆ¥s0â€¢ appearing in (40) is clearly dominated
by a term of the form C +âˆ¥Î¾(x)âˆ¥bs0
â€¢ .
Inequality (40) together with these additional manipulations leads to the final upper
bound
E[V1(y1,1) | y0 = x] â‰¤|z1|2s0 âˆ’Ëœr|z1|2s0Î± +Câˆ¥Î¾(x)âˆ¥bs0
â€¢
+C
(42)
which holds for all x âˆˆRp+q.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
23
Step 3: Upper bound for V2. Using (12), we can express the conditional expectation in
(29) as
E[V2(y1,1) | y0 = x] = s1âˆ¥1z2(x1)+z1(x1)Î¹pâˆ’1âˆ¥2s0Î±
âˆ—
.
Recall that Î± = 1 âˆ’Ï/2s0 âˆˆ(0,1) (because s0 â‰¥1 and Ï âˆˆ(0,2) by assumption) and that
|||1|||âˆ—â‰¤Ï– for some 0 < Ï– < 1 (by Lemma 1). These facts together with elementary
inequalities (and dropping the argument x1 from z1(x1) and z2(x1)) imply that
1z2 +z1Î¹pâˆ’1
Î±
âˆ—â‰¤âˆ¥1z2âˆ¥Î±
âˆ—+
z1Î¹pâˆ’1
Î±
âˆ—â‰¤Ï– Î± âˆ¥z2âˆ¥Î±
âˆ—+
Î¹pâˆ’1
Î±
âˆ—|z1|Î±.
This, together with the convexity of the function |x| â†’|x|2s0 (recall that s0 â‰¥1 by
assumption), imply that for any Ï„1 âˆˆ(0,1) and Ï„2 = 1âˆ’Ï„1,
s1
1z2 +z1Î¹pâˆ’1
2s0Î±
âˆ—
â‰¤
â›
âÏ„2
s1/2s0
1
Ï– Î±
Ï„2
âˆ¥z2âˆ¥Î±
âˆ—+Ï„1
s1/2s0
1
Î¹pâˆ’1
Î±
âˆ—
Ï„1
|z1|Î±
â
â 
2s0
â‰¤Ï„2
s1Ï– 2s0Î±
Ï„2s0
2
âˆ¥z2âˆ¥2s0Î±
âˆ—
+Ï„1
s1
Î¹pâˆ’1
2s0Î±
âˆ—
Ï„2s0
1
|z1|2s0Î±.
(43)
Consider the former term on the dominant side of (43). Fix a Ï„2 such that Ï„2 âˆˆ(Ï– Î±,1)
and set ËœÏ– = 1 âˆ’(Ï– Î±/Ï„2)2s0 âˆˆ(0,1). Then the former term on the dominant side of (43)
satisfies
Ï„2
s1Ï– 2s0Î±
Ï„2s0
2
âˆ¥z2âˆ¥2s0Î±
âˆ—
= Ï„2s1(1âˆ’ËœÏ–)âˆ¥z2âˆ¥2s0Î±
âˆ—
< s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
.
(44)
Suppose now that s1 is any fixed (but potentially arbitrarily small) positive number. If âˆ¥z2âˆ¥âˆ—
is large enough to ensure that s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
â‰¥1, then s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
â‰¥sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
as Î± âˆˆ(0,1)
and the right side of (44) is dominated by s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
. On the other hand,
if s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
< 1, the right side of (44) is bounded by a constant. Therefore,
Ï„2
s1Ï– 2s0Î±
Ï„2s0
2
âˆ¥z2âˆ¥2s0Î±
âˆ—
< s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+C.
Now, consider the latter term on the dominant side of (43). Choosing a small enough fixed
s1, this term can be made smaller than Ëœs1 |z1|2s0Î± where Ëœs1 can be chosen as close to zero
as desired. To summarize, it holds that
E[V2(y1,1) | y0 = x] â‰¤s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+ Ëœs1 |z1|2s0Î± +C,
(45)
where ËœÏ– âˆˆ(0,1) and the value of Ëœs1 > 0 can be chosen as close to zero as desired.
Step 4: Upper bound for V3. By the definition of the function Î¾ in (14), Î¾(y1) =
Î¶,1(y0)Î¾(y0) + Ï‰Î¶,1. We start by bounding both terms on the right-hand side of this
equality and, for simplicity, remove the argument y0 and instead use the notations Î¶,1
and Î¾0.
First, denote vÎ¶,1 = Îµ2
1(Î±1Î¶1,0e2
0 +Â·Â·Â·+Î±qÎ¶q,0e2
1âˆ’q) and v1 = Îµ2
1(Î±1e2
0 +Â·Â·Â·+Î±qe2
1âˆ’q).
Using the definitions of the matrices Î¶,1 and 1 and the vector Î¾0 (see (14) and (16)), we
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

24
MIKA MEITZ AND PENTTI SAIKKONEN
then have
Î¶,1Î¾0 = (vÎ¶,1,e2
0,...,e2
1âˆ’q)
and
1Î¾0 = (v1,e2
0,...,e2
1âˆ’q),
where all components of both vectors are nonnegative. As Î¶i,0 âˆˆ(0,1] for all i = 1,...,q
by assumption, we have vÎ¶,1 â‰¤v1 (a.s.). The monotonicity of the norm âˆ¥Â· âˆ¥â€¢ required in
Assumption 4 now implies that
Î¶,1Î¾0
â€¢ â‰¤
1Î¾0
â€¢ (a.s.) (see the discussion preceding
Assumption 4). Regarding the vector Ï‰Î¶,1, its first component is Î¶0,0(y0)Îµ2
1Ï‰ â‰¤Îµ2
1Ï‰ (a.s.)
and the other components are zero, so that the monotonicity of the norm âˆ¥Â· âˆ¥â€¢ shows that
âˆ¥Ï‰Î¶,1âˆ¥â€¢ â‰¤âˆ¥Ï‰1âˆ¥â€¢ = Îµ2
1âˆ¥Ï‰âˆ¥â€¢ (a.s.) where Ï‰ = (Ï‰,0,...,0).
The preceding discussion together with the triangle inequality now yields, with proba-
bility one,
âˆ¥Î¾(y1)âˆ¥â€¢ = âˆ¥Î¶,1(y0)Î¾(y0)+Ï‰Î¶,1âˆ¥â€¢ â‰¤âˆ¥Î¶,1(y0)Î¾(y0)âˆ¥â€¢ +âˆ¥Ï‰Î¶,1âˆ¥â€¢ â‰¤âˆ¥1Î¾(y0)âˆ¥â€¢ +Îµ2
1âˆ¥Ï‰âˆ¥â€¢.
Using the notation Â¯Î¼2bs0 = (E[|Îµ0|2bs0])1/bs0 and Minkowskiâ€™s inequality, we find that

E[âˆ¥Î¾(y1)âˆ¥bs0
â€¢
| y0 = x]
1/bs0 â‰¤

E[âˆ¥1Î¾(x)âˆ¥bs0
â€¢ ]
1/bs0 + Â¯Î¼2bs0âˆ¥Ï‰âˆ¥â€¢.
By inequality (18) and Assumption 4, the first term on the dominant side satisfies

E[âˆ¥1Î¾(x)âˆ¥bs0
â€¢ ]
1/bs0 = âˆ¥1Î¾(x)âˆ¥â€¢Lbs0 â‰¤|||1|||â€¢Lbs0 âˆ¥Î¾(x)âˆ¥â€¢ = Î»âˆ¥Î¾(x)âˆ¥â€¢
with Î» < 1. The preceding steps imply that

E[âˆ¥Î¾(y1)âˆ¥bs0
â€¢
| y0 = x]
1/bs0 â‰¤Î»âˆ¥Î¾(x)âˆ¥â€¢ + Â¯Î¼2bs0âˆ¥Ï‰âˆ¥â€¢ = Î»âˆ¥Î¾(x)âˆ¥â€¢

1+ Â¯Î¼2bs0âˆ¥Ï‰âˆ¥â€¢
Î»âˆ¥Î¾(x)âˆ¥â€¢

,
and, using (30) and the notation Ë†Î» = Î»bs0 < 1, we obtain
E[V3(y1) | y0 = x] â‰¤s2Ë†Î»âˆ¥Î¾(x)âˆ¥bs0
â€¢

1+ Â¯Î¼2bs0âˆ¥Ï‰âˆ¥â€¢
Î»âˆ¥Î¾(x)âˆ¥â€¢
bs0
= s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’

1âˆ’Ë†Î»

1+ Â¯Î¼2bs0âˆ¥Ï‰âˆ¥â€¢
Î»âˆ¥Î¾(x)âˆ¥â€¢
bs0
s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
.
As Ë†Î» âˆˆ(0,1), we can choose a Î¾ > 0 such that the term in curly brackets is larger than some
ËœÎ» âˆˆ(0,1) whenever âˆ¥Î¾(x)âˆ¥â€¢ > Î¾. Therefore, whenever âˆ¥Î¾(x)âˆ¥â€¢ > Î¾, we have
E[V3(y1) | y0 = x] â‰¤s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’ËœÎ»s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
.
On the other hand, whenever âˆ¥Î¾(x)âˆ¥â€¢ â‰¤Î¾, the previous derivations also make it clear that
E[V3(y1) | y0 = x] is bounded by some constant. Therefore, for all x âˆˆRp+q,
E[V3(y1) | y0 = x] â‰¤s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’ËœÎ»s2 âˆ¥Î¾(x)âˆ¥bs0
â€¢
+C.
(46)
Step 5: Upper bound for V. We next combine the upper bounds (42), (45), and (46) derived
in Steps 2â€“4 to obtain
E[V(y1) | y0 = x] â‰¤1+|z1|2s0 âˆ’Ëœr|z1|2s0Î± +Câˆ¥Î¾(x)âˆ¥bs0
â€¢
+s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
âˆ’ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+ Ëœs1 |z1|2s0Î±
+s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’ËœÎ»s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
+C.
(47)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
25
To combine the terms involving |z1|2s0Î±, set Â¯r = Ëœr âˆ’Ëœs1 so that
âˆ’Ëœr|z1|2s0Î± + Ëœs1|z1|2s0Î± = âˆ’Â¯r|z1|2s0Î±;
recalling that Ëœs1 can be chosen as close to zero as desired, we have Â¯r > 0 by a suitable
choice of Ëœs1. On the other hand, to manipulate the terms involving âˆ¥Î¾(x)âˆ¥bs0
â€¢ , choose s2
large enough to ensure that ËœÎ»âˆ’C/s2 âˆˆ(0,1) and set Â¯Î» = ËœÎ»âˆ’C/s2. As now âˆ’Â¯Î»s2 = Câˆ’ËœÎ»s2,
the terms involving âˆ¥Î¾(x)âˆ¥bs0
â€¢
in (47) can be written as
s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
+Câˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’ËœÎ»s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
= s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’Â¯Î»s2âˆ¥Î¾(x)âˆ¥bs0
â€¢ .
(48)
Whenever s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
< 1, (48) is bounded by the constant 1; for s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
â‰¥1, we have
s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
â‰¥sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
as Î± âˆˆ(0,1). Thus, the expression in (48) is always bounded
by 1 + s2âˆ¥Î¾(x)âˆ¥bs0
â€¢
âˆ’Â¯Î»sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
. As V(x) = 1 + |z1|2s0 + s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
+ s2âˆ¥Î¾(x)âˆ¥bs0
â€¢ ,
we obtain from (47) that
E[V(y1) | y0 = x] â‰¤V(x)âˆ’(1+ Â¯r|z1|2s0Î± + ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+ Â¯Î»sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
)+C.
(49)
The inequality in (38) implies that
[V(x)]Î± = (1+|z1|2s0 +s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
+s2âˆ¥Î¾(x)âˆ¥bs0
â€¢ )Î±
â‰¤1+|z1|2s0Î± +sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
so that, setting e = min{Â¯r, ËœÏ–, Â¯Î»} âˆˆ(0,1), we have
e[V(x)]Î± â‰¤1+ Â¯r|z1|2s0Î± + ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
+ Â¯Î»sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
.
Therefore, setting Ëœe = e/2,
E[V(y1) | y0 = x] â‰¤V(x)âˆ’Ëœe[V(x)]Î± +

C âˆ’Ëœe[V(x)]Î±
.
Now, define the set
AN =

x âˆˆRp+q : |z1(x1)|2s0 â‰¤N,
âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
â‰¤N,
âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
â‰¤N

,
(50)
where N is so large that AN is nonempty (see (4)). The complement of AN is denoted by Ac
N
so that x âˆˆAc
N if either |z1(x1)|2s0 > N, âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
> N, or âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
> N. By choosing
a large enough N, for all x âˆˆAc
N, it holds that Câˆ’Ëœe[V(x)]Î± < 0 so that E[V(y1) | y0 = x] â‰¤
V(x) âˆ’Ëœe[V(x)]Î± for all x âˆˆAc
N. On the other hand, the function V(x) âˆ’e[V(x)]Î± + C is
clearly bounded by some positive constant Ëœb on AN. Therefore, we can conclude that there
exist an N and a positive constant Ëœb such that
E[V(y1) | y0 = x] â‰¤V(x)âˆ’Ï†1 (V(x))+ Ëœb1AN (x),
(51)
where Ï†1(v) = ËœevÎ±. This implies that Condition D holds with Ï† = Ï†1 and C = AN.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

26
MIKA MEITZ AND PENTTI SAIKKONEN
Step 6: Showing that AN is petite. We first note that the definition of a petite set and
other Markov chain concepts we refer to below can be found in Meyn and Tweedie (2009,
Chaps. 4â€“6). The idea is to establish that the (potentially non-compact) set AN in (50) is
petite for any N â‰¥1 so large that AN is nonempty. To this end, we show below that there
exists an MN < âˆsuch that
sup
xâˆˆAN
E[âˆ¥yp+qâˆ¥2s0Î±
1
| y0 = x] < M2s0Î±
N
,
(52)
where âˆ¥Â·âˆ¥1 denotes the usual l1 vector norm. Next note that Theorem 2.2(ii) of Cline and
Pu (1998) along with our Assumption 1 shows that the Markov chain yt is a Ïˆ-irreducible
and aperiodic T-chain (see also Example 2.1 of the aforementioned paper). Therefore, the
compact set BN = {x âˆˆRp+q : âˆ¥xâˆ¥1 â‰¤MN} is small (see Meyn and Tweedie, 2009, Thms.
6.2.5(ii) and 5.5.7). Moreover, due to Markovâ€™s inequality,
inf
xâˆˆAN
Pr[yp+q âˆˆBN | y0 = x] = 1âˆ’sup
xâˆˆAN
Pr[âˆ¥yp+qâˆ¥1 â‰¥MN | y0 = x]
â‰¥1âˆ’sup
xâˆˆAN
E[âˆ¥yp+qâˆ¥2s0Î±
1
| y0 = x]/M2s0Î±
N
,
where the last expression is positive due to (52). Proposition 5.2.4(i) of Meyn and Tweedie
(2009) now implies that the set AN is small. Proposition 5.5.3 of the same reference therefore
implies that the set AN is also petite.
To complete the proof of petiteness of AN, it remains to establish (52). First, we introduce
some notation. We let |||Â·|||1 denote the maximum column sum norm defined for real square
matrices (this norm is induced by the l1 vector norm; see Horn and Johnson, 2013, Sect.
5.6). For brevity, we denote zt = z(y1,t) = Ay1,t and also partition the p-dimensional zt as
zt = (z1,t,z2,t) (see (11)). This allows us to write the companion form (12) as
zt =

z1,t
z2,t

=

g(z1,tâˆ’1)+ÏƒtÎµt
1z2,tâˆ’1 +z1,tâˆ’1Î¹pâˆ’1

.
(53)
Finally, we set âƒ—z1,p+q = (z1,p+q,...,z1,1) and âƒ—z2,p+q = (z2,p+q,...,z2,1).
Now, consider the norm âˆ¥yp+qâˆ¥1 in (52). Using properties of the norms âˆ¥Â·âˆ¥1 and |||Â·|||1,
we can write
âˆ¥yp+qâˆ¥1 â‰¤
p+q

t=1
âˆ¥y1,tâˆ¥1
=
p+q

t=1
âˆ¥Aâˆ’1ztâˆ¥1 â‰¤|||Aâˆ’1|||1
p+q

t=1
âˆ¥ztâˆ¥1 = |||Aâˆ’1|||1(âˆ¥âƒ—z1,p+qâˆ¥1 +âˆ¥âƒ—z2,p+qâˆ¥1),
implying that âˆ¥yp+qâˆ¥1 â‰¤C(âˆ¥âƒ—z1,p+qâˆ¥1 + âˆ¥âƒ—z2,p+qâˆ¥1). Adding terms and making use of
inequality (38) and the fact that Î± âˆˆ(0,1), we obtain
âˆ¥yp+qâˆ¥2s0Î±
1
â‰¤C((1+âˆ¥âƒ—z1,p+qâˆ¥1)2s0Î± +âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
)
â‰¤C((1+âˆ¥âƒ—z1,p+qâˆ¥1)2s0 +âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
)
â‰¤C(1+âˆ¥âƒ—z1,p+qâˆ¥2s0
1
+âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
).
(54)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
27
To obtain an upper bound for the term âˆ¥âƒ—z1,p+qâˆ¥2s0
1 , consider the equality z1,t = g(z1,tâˆ’1)+
ÏƒtÎµt from (53) and note that, by Assumption 2, |g(u)| â‰¤K0 for |u| â‰¤M0 and |g(u)| â‰¤(1âˆ’
r|u|âˆ’Ï)|u| â‰¤|u| for |u| â‰¥M0, so that |g(u)| â‰¤K0 +|u| for all u âˆˆR (note that Assumption
2 requires M0 to be so large that r|u|âˆ’Ï âˆˆ(0,1) for |u| â‰¥M0). Using these inequalities,
|z1,t| â‰¤K0 +|z1,tâˆ’1|+Ïƒt|Îµt| (t = 1,...,p+q), so that
|z1,1| â‰¤K0 +|z1,0|+Ïƒ1|Îµ1|,
|z1,2| â‰¤2K0 +|z1,0|+Ïƒ1|Îµ1|+Ïƒ2|Îµ2|,
...
|z1,p+q| â‰¤(p+q)K0 +|z1,0|+Ïƒ1|Îµ1|+Â·Â·Â·+Ïƒp+q|Îµp+q|.
Thus, âˆ¥âƒ—z1,p+qâˆ¥1 â‰¤C

1+|z1,0|+p+q
i=1 Ïƒi|Îµi|

, and, making use of inequality (38),
âˆ¥âƒ—z1,p+qâˆ¥2s0
1
â‰¤C

1+|z1,0|2s0 +
p+q

i=1
Ïƒ 2s0
i
|Îµi|2s0

.
(55)
Next, to bound the term âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
, consider z2,t = 1z2,tâˆ’1 + z1,tâˆ’1Î¹pâˆ’1 (see (53)).
Setting Îº = |||1|||1 and using the fact âˆ¥Î¹pâˆ’1âˆ¥1 = 1, we obtain
âˆ¥z2,tâˆ¥1 â‰¤|||1|||1âˆ¥z2,tâˆ’1âˆ¥1 +|z1,tâˆ’1|âˆ¥Î¹pâˆ’1âˆ¥1 = Îºâˆ¥z2,tâˆ’1âˆ¥1 +|z1,tâˆ’1|,
and furthermore
âˆ¥z2,1âˆ¥1 â‰¤Îºâˆ¥z2,0âˆ¥1 +|z1,0|,
âˆ¥z2,2âˆ¥1 â‰¤Îº2âˆ¥z2,0âˆ¥1 +Îº|z1,0|+|z1,1|,
...
âˆ¥z2,p+qâˆ¥1 â‰¤Îºp+qâˆ¥z2,0âˆ¥1 +Îºp+qâˆ’1|z1,0|+Â·Â·Â·+|z1,p+qâˆ’1|.
This implies that
âˆ¥âƒ—z2,p+qâˆ¥1 â‰¤C(âˆ¥z2,0âˆ¥1 +1+|z1,0|+âˆ¥âƒ—z1,p+qâˆ¥1).
As the norms âˆ¥Â·âˆ¥1 and âˆ¥Â·âˆ¥âˆ—are equivalent, it holds that âˆ¥z2,0âˆ¥1 â‰¤Câˆ¥z2,0âˆ¥âˆ—. Making use
of inequality (38) and the fact that Î± âˆˆ(0,1), we obtain
âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
â‰¤C(âˆ¥z2,0âˆ¥2s0Î±
âˆ—
+(1+|z1,0|+âˆ¥âƒ—z1,p+qâˆ¥1)2s0Î±)
â‰¤C(âˆ¥z2,0âˆ¥2s0Î±
âˆ—
+(1+|z1,0|+âˆ¥âƒ—z1,p+qâˆ¥1)2s0)
â‰¤C(1+âˆ¥z2,0âˆ¥2s0Î±
âˆ—
+|z1,0|2s0 +âˆ¥âƒ—z1,p+qâˆ¥2s0
1 ).
(56)
Now, combine (54) with the upper bounds obtained for âˆ¥âƒ—z1,p+qâˆ¥2s0
1
and âˆ¥âƒ—z2,p+qâˆ¥2s0Î±
1
in
(55) and (56), and recall that z1,0 = z1(y1,0) and z2,0 = z2(y1,0), to obtain
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

28
MIKA MEITZ AND PENTTI SAIKKONEN
âˆ¥yp+qâˆ¥2s0Î±
1
â‰¤C

1+âˆ¥z2(y1,0)âˆ¥2s0Î±
âˆ—
+|z1(y1,0)|2s0 +
p+q

i=1
Ïƒ 2s0
i
|Îµi|2s0

.
As Î¼2s0 = E[|Îµ1|2s0] is finite, this implies that
E[âˆ¥yp+qâˆ¥2s0Î±
1
| y0 = x]â‰¤C

1+âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
+|z1(x1)|2s0 +Î¼2s0
p+q

i=1
E[Ïƒ 2s0
i
| y0 = x]

.
(57)
Next, consider the terms in (57) involving conditional expectations of the Ïƒ 2s0
i
â€™s. We first
derive an inequality which is similar to inequality (11) in Meitz and Saikkonen (2010).
Using repeated substitution and the equality Î¾(yt) = Î¶,tÎ¾(ytâˆ’1)+Ï‰Î¶,t, we obtain, for any
fixed t â‰¥1, that
Î¾(yt) =
tâˆ’1

k=0
Î¶,tâˆ’kÎ¾(y0)+Ï‰Î¶,t +
tâˆ’2

k=0
k

l=0
Î¶,tâˆ’lÏ‰Î¶,tâˆ’kâˆ’1.
Now, consider the vector norm âˆ¥Â· âˆ¥â€¢ in Assumption 4. Denote by |||Â·|||â€¢ the matrix norm
induced by the vector norm âˆ¥Â·âˆ¥â€¢; that is, for any qÃ—q matrix A, set
|||A|||â€¢ = max
âˆ¥xâˆ¥â€¢=1âˆ¥Axâˆ¥â€¢
(x âˆˆRq).
(For clarity, note that |||Â·|||â€¢ above and |||Â·|||â€¢Lp defined in (17) coincide for nonrandom
matrices but differ for random ones.) As âˆ¥Â·âˆ¥â€¢ in Assumption 4 is assumed to be monotone,
it follows from Problems 5.6.P41(c) and 5.6.P42 in Horn and Johnson (2013, p. 368) that
the induced matrix norm |||Â·|||â€¢ is monotone on the positive orthant, meaning that any qÃ—q
matrices A and B that satisfy the (entrywise) inequalities 0 â‰¤A â‰¤B also satisfy the inequality
|||A|||â€¢ â‰¤|||B|||â€¢. Usual properties of vector norms and matrix norms in conjunction with
inequality (38) therefore yield
âˆ¥Î¾(yt)âˆ¥s0â€¢ â‰¤C
tâˆ’1

k=0
|||Î¶,tâˆ’k|||s0
â€¢ âˆ¥Î¾(y0)âˆ¥s0â€¢ +Câˆ¥Ï‰Î¶,tâˆ¥s0â€¢ +C
tâˆ’2

k=0
k
l=0
|||Î¶,tâˆ’l|||s0
â€¢ âˆ¥Ï‰Î¶,tâˆ’kâˆ’1âˆ¥s0â€¢ .
By the monotonicity properties of the norms âˆ¥Â· âˆ¥â€¢ and |||Â·|||â€¢ and the definitions of the
matrices Î¶,t and t in (14) and (16), we also obtain |||Î¶,t|||â€¢ â‰¤|||t|||â€¢ and âˆ¥Ï‰Î¶,tâˆ¥â€¢ â‰¤
âˆ¥Ï‰tâˆ¥â€¢ = Îµ2t âˆ¥Ï‰âˆ¥â€¢ (a.s.) for all t = 1,2,..., implying that
âˆ¥Î¾(yt)âˆ¥s0â€¢ â‰¤C
tâˆ’1

k=0
|||tâˆ’k|||s0â€¢ âˆ¥Î¾(y0)âˆ¥s0â€¢ +C

|Îµt|2s0 +
tâˆ’2

k=0
k
l=0
|||tâˆ’l|||s0â€¢ |Îµtâˆ’kâˆ’1|2s0

âˆ¥Ï‰âˆ¥s0â€¢ .
Now, denote the expectation E

|||t|||s0â€¢

by Ï‡ (this expectation is finite due to Assump-
tion 4). Using the independence of the tâ€™s and independence of |||tâˆ’l|||â€¢â€™s and Îµtâˆ’kâˆ’1â€™s,
yields
E

âˆ¥Î¾(yt)âˆ¥s0â€¢ | y0 = x

â‰¤CÏ‡tâˆ¥Î¾(x)âˆ¥s0â€¢ +C

1+
tâˆ’2

k=0
Ï‡k+1

âˆ¥Ï‰âˆ¥s0â€¢ .
(58)
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
29
Inequality (34) in conjunction with (38) shows that Ïƒ 2s0
i
â‰¤C(1+âˆ¥Î¾(yiâˆ’1)âˆ¥s0â€¢ ) (a.s.) for all
i = 1,...,p+q. From (58), it then follows that E[Ïƒ 2s0
i
| y0 = x] â‰¤C(1+âˆ¥Î¾(x)âˆ¥s0â€¢ ), which
together with (57) implies
E[âˆ¥yp+qâˆ¥2s0Î±
1
| y0 = x] â‰¤C

1+|z1(x1)|2s0 +âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
+âˆ¥Î¾(x)âˆ¥s0â€¢

.
For any x âˆˆAN, the dominant side is bounded by C(1+2N +N1/bÎ±), and thus we can find
a finite MN such that (52) holds.
Step 7: Completing the proof. We are now ready to complete the proof by applying
Theorem 1(iii) in Meitz and Saikkonen (2022). To this end, in the beginning of Step 6, we
already noted that the Markov chain yt is Ïˆ-irreducible and aperiodic. That Condition D
holds was shown in (51) in Step 5. Petiteness of the set AN was shown in Step 6. We also
need to verify that supxâˆˆAN V(x) < âˆ; this inequality is a straightforward consequence of
the definitions of the set AN and the function V. Thus, applying Theorem 1(iii) in Meitz and
Saikkonen (2022), we can complete the proof.
â–¡
Details for the finiteness of moments in Section 3.2. The arguments are similar to those
used in the proof of Corollary to Theorem 3 in Meitz and Saikkonen (2022). First, note that
inequality (45) continues to hold if the term ËœÏ–sÎ±
1 âˆ¥z2âˆ¥2s0Î±2
âˆ—
on its dominant side is replaced
with the term ËœÏ–s1 âˆ¥z2âˆ¥2s0Î±
âˆ—
(this can be seen from (44) and the arguments that follow it).
Consequently, the same replacement can be done on the dominant sides of inequalities (47)
and (49), the latter inequality thus becoming
E[V(y1) | y0 = x] â‰¤V(x)âˆ’(1+ Â¯r|z1(x1)|2s0Î± + ËœÏ–s1 âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
+ Â¯Î»sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
)+C.
Finiteness of certain moments with respect to the stationary distribution Ï€ of yt can
now be obtained from Theorem 14.3.7 of Meyn and Tweedie (2009), namely

Rp+q(1 +
Â¯r|z1(x1)|2s0Î± + ËœÏ–s1 âˆ¥z2(x1)âˆ¥2s0Î±
âˆ—
+ Â¯Î»sÎ±
2âˆ¥Î¾(x)âˆ¥bs0Î±
â€¢
)Ï€(dx) < âˆ. Noting that 2s0Î± = 2s0 âˆ’
Ï and following the arguments in the proof of Corollary to Theorem 3 in Meitz and
Saikkonen (2022), it follows that the stationary version of yt satisfies E[|yt|2s0âˆ’Ï] < âˆ.
â–¡
Proofs of Propositions 1 and 2. For Proposition 1, note that model (24) can be written
as ut = utâˆ’1 +Î½1L(utâˆ’1;Î³,a1)+Î½2(1âˆ’L(utâˆ’1;Î³,a2))+ÏƒtÎµt so that the function g(Â·) in
Assumption 2(ii) takes the form g(u) = u+Î½1L(u;Î³,a1)+Î½2(1âˆ’L(u;Î³,a2)). Arguments
used in the proof of Proposition 1 in Meitz and Saikkonen (2022) now show that Assumption
2(ii) holds with Ï = 1. Applying Theorem 1 with Î´ = 2s0 yields the polynomial ergodicity
result, and the moment result follows from the remarks made after Theorem 1. As for
Proposition 2, model (25) can be written as ut = S(utâˆ’1)utâˆ’1 + ÏƒtÎµt so that now g(u) =
S(u)u. Assumption 2(ii) can be verified as in the proof of Proposition 2 in Meitz and
Saikkonen (2022), and the result follows from Theorem 1 (with Î´ = 2s0/Ï).
â–¡
B. APPENDIX B
This appendix contains Figure B1, which displays further analysis of the residuals of
model (27).
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

30
MIKA MEITZ AND PENTTI SAIKKONEN
âˆ’0.06 âˆ’0.02
0.02
0.06
0
10
20
30
40
50
âˆ’0.06 âˆ’0.02
0.02
0.06
0
10
20
30
40
50
âˆ’4
âˆ’2
0
2
4
6
8
0.0
0.2
0.4
0.6
âˆ’4
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0
2
4
6
Figure B1. Further analysis of the residuals shown in (the bottom-right graph of) Figure 1:
autocorrelation function of Ë†Îµt (top left), autocorrelation function of Ë†Îµ2
t (top right), histogram along
with the estimated error density (bottom left), and a Qâ€“Q plot (bottom right). The dashed lines in the
autocorrelation function graphs show the conventional bounds Â±1.96/
âˆš
T â‰ˆÂ±0.038 (T = 2715; the
first four observations are used as initial values).
REFERENCES
AtchadÃ©, Y., & G. Fort (2010) Limit theorems for some adaptive MCMC algorithms with subgeometric
kernels. Bernoulli 16, 116â€“154.
Cline, D. B. H. (2007) Stability of nonlinear stochastic recursions with application to nonlinear ARâ€“
GARCH models. Advances in Applied Probability 39, 462â€“491.
Cline, D. B. H., & H. H. Pu (1998) Verifying irreducibility and continuity of a nonlinear time series.
Statistics & Probability Letters 40, 139â€“148.
Cline, D. B. H., & H. H. Pu (2004) Stability and the Lyapounov exponent of threshold ARâ€“ARCH
models. Annals of Applied Probability 14, 1920â€“1949.
Davidson, J. (1994) Stochastic Limit Theory. Oxford University Press.
Douc, R., G. Fort, E. Moulines, & P. Soulier (2004) Practical drift conditions for subgeometric rates
of convergence. Annals of Applied Probability 14, 1353â€“1377.
Douc, R., A. Guillin, & E. Moulines (2008) Bounds on regeneration times and limit theorems for
subgeometric Markov chains. Annales de lâ€™Institute Henri Poincare, Probabilites et Statistiques 44,
239â€“257.
Douc, R., E. Moulines, P. Priouret, & P. Soulier (2018) Markov Chains. Springer.
Dudley, R. M. (2004) Real Analysis and Probability. Cambridge University Press.
Engle, R. F. (1982) Autoregressive conditional heteroscedasticity with estimates of the variance of
United Kingdom inflation. Econometrica 50, 987â€“1007.
Fefferman, C., & H. S. Shapiro (1972) A planar face on the unit sphere of the multiplier space Mp,
1 < p < âˆ. Proceedings of the American Mathematical Society 36, 435â€“439.
Fort, G., & E. Moulines (2000) V-subgeometric ergodicity for a Hastingsâ€“Metropolis algorithm.
Statistics & Probability Letters 49, 401â€“410.
Fort, G., & E. Moulines (2003) Polynomial ergodicity of Markov transition kernels. Stochastic
Processes and Their Applications 103, 57â€“99.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

SUBGEOMETRICALLY ERGODIC ARâ€“ARCH
31
Horn, R. A., & C. R. Johnson (2013) Matrix Analysis. (2nd ed.) Cambridge University Press.
Jarner, S. F., & G. O. Roberts (2002) Polynomial convergence rates of Markov chains. Annals of
Applied Probability 12, 224â€“247.
Jones, M. C., & M. J. Faddy (2003) A skew extension of the t-distribution, with applications. Journal
of the Royal Statistical Society: Series B 65, 159â€“174.
Klokov, S. A. (2007) Lower bounds of mixing rate for a class of Markov processes. Theory of
Probability and Its Applications 51, 528â€“535.
Klokov, S. A., & A. Y. Veretennikov (2004) Sub-exponential mixing rate for a class of Markov chains.
Mathematical Communications 9, 9â€“26.
Klokov, S. A., & A. Y. Veretennikov (2005) On subexponential mixing rate for Markov processes.
Theory of Probability and Its Applications 49, 110â€“122.
Lieberman, O., & P. C. B. Phillips (2020) Hybrid stochastic local unit roots. Journal of Econometrics
215, 257â€“285.
Ling, S. (1999) On the probabilistic properties of a double threshold ARMA conditional heteroskedas-
tic model. Journal of Applied Probability 36, 688â€“705.
Ling, S., & M. McAleer (2002) Necessary and sufficient moment conditions for the GARCH(r,s) and
asymmetric power GARCH(r,s) models. Econometric Theory 18, 722â€“729.
Liu, J., W. K. Li, & C. W. Li (1997) On a threshold autoregression with conditional heteroscedastic
variances. Journal of Statistical Planning and Inference 62, 279â€“300.
Meitz, M., & P. Saikkonen (2008a) Ergodicity, mixing, and existence of moments of a class of Markov
models with applications to GARCH and ACD models. Econometric Theory 24, 1291â€“1320.
Meitz, M., & P. Saikkonen (2008b) Stability of nonlinear ARâ€“GARCH models. Journal of Time Series
Analysis 29, 453â€“475.
Meitz, M., & P. Saikkonen (2010) A note on the geometric ergodicity of a nonlinear ARâ€“ARCH model.
Statistics & Probability Letters 80, 631â€“638.
Meitz, M., & P. Saikkonen (2021) Subgeometric ergodicity and Î²-mixing. Journal of Applied
Probability 58, 594â€“608.
Meitz, M., & P. Saikkonen (2022) Subgeometrically ergodic autoregressions. Econometric Theory 38,
959â€“985.
MerlevÃ¨de, F., M. Peligrad, & E. Rio (2011) A Bernstein type inequality and moderate deviations for
weakly dependent sequences. Probability Theory and Related Fields 151, 435â€“474.
Meyn, S. P., & R. L. Tweedie (2009) Markov Chains and Stochastic Stability. (2nd ed.) Cambridge
University Press.
Nummelin, E., & P. Tuominen (1983) The rate of convergence in Oreyâ€™s theorem for Harris recurrent
Markov chains with applications to renewal theory. Stochastic Processes and Their Applications 15,
295â€“311.
Phillips, P. C. B. (2023) Estimation and inference with near unit roots. Econometric Theory 39, 221â€“
263.
Tuominen, P., & R. L. Tweedie (1994) Subgeometric rates of convergence of f-ergodic Markov chains.
Advances in Applied Probability 26, 775â€“798.
Tweedie, R. L. (1983) Criteria for rates of convergence of Markov chains, with application to queueing
and storage theory. In J. F. C. Kingman, & G. E. H. Reuter (Eds.), Probability, Statistics and Analysis,
pp. 260â€“276. Cambridge University Press.
Veretennikov, A. Y. (2000) On polynomial mixing and convergence rate for stochastic difference and
differential equations. Theory of Probability and Its Applications 44, 361â€“374.
Vladimirova, M., S. Girard, H. Nguyen, & J. Arbel (2020) Sub-Weibull distributions: Generalizing
sub-Gaussian and sub-exponential properties to heavier tailed distributions. Stat 9, e318.
Wong, K. C., Z. Li, & A. Tewari (2020) Lasso guarantees for Î²-mixing heavy-tailed time series. Annals
of Statistics 48, 1124â€“1142.
https://doi.org/10.1017/S026646662300035X Published online by Cambridge University Press

