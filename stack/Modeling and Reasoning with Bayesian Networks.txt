
This page intentionally left blank

Modeling and Reasoning with Bayesian Networks
This book provides a thorough introduction to the formal foundations and practical applications
of Bayesian networks. It provides an extensive discussion of techniques for building Bayesian
networks that model real-world situations, including techniques for synthesizing models from
design, learning models from data, and debugging models using sensitivity analysis. It also
treats exact and approximate inference algorithms at both theoretical and practical levels. The
treatment of exact algorithms covers the main inference paradigms based on elimination and
conditioning and includes advanced methods for compiling Bayesian networks, time-space
tradeoffs, and exploiting local structure of massively connected networks. The treatment of
approximate algorithms covers the main inference paradigms based on sampling and opti-
mization and includes inﬂuential algorithms such as importance sampling, MCMC, and belief
propagation.
The author assumes very little background on the covered subjects, supplying in-depth
discussions for theoretically inclined readers and enough practical details to provide an algo-
rithmic cookbook for the system developer.
Adnan Darwiche is a Professor and Chairman of the Computer Science Department at UCLA.
He is also the Editor-in-Chief for the Journal of Artiﬁcial Intelligence Research (JAIR) and a
AAAI Fellow.


Modeling and Reasoning with
Bayesian Networks
Adnan Darwiche
University of California, Los Angeles

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13    978-0-521-88438-9
ISBN-13
978-0-511-50728-1
© Adnan Darwiche 2009
2009
Information on this title: www.cambridge.org/9780521884389
This publication is in copyright. Subject to statutory exception and to the 
provision of relevant collective licensing agreements, no reproduction of any part
may take place without the written permission of Cambridge University Press.
Cambridge University Press has no responsibility for the persistence or accuracy 
of urls for external or third-party internet websites referred to in this publication, 
and does not guarantee that any content on such websites is, or will remain, 
accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
eBook (EBL)
hardback

Contents
Preface
page xi
1 Introduction
1
1.1
Automated Reasoning
1
1.2
Degrees of Belief
4
1.3
Probabilistic Reasoning
6
1.4
Bayesian Networks
8
1.5
What Is Not Covered in This Book
12
2 Propositional Logic
13
2.1
Introduction
13
2.2
Syntax of Propositional Sentences
13
2.3
Semantics of Propositional Sentences
15
2.4
The Monotonicity of Logical Reasoning
18
2.5
Multivalued Variables
19
2.6
Variable Instantiations and Related Notations
20
2.7
Logical Forms
21
Bibliographic Remarks
24
2.8
Exercises
25
3 Probability Calculus
27
3.1
Introduction
27
3.2
Degrees of Belief
27
3.3
Updating Beliefs
30
3.4
Independence
34
3.5
Further Properties of Beliefs
37
3.6
Soft Evidence
39
3.7
Continuous Variables as Soft Evidence
46
Bibliographic Remarks
48
3.8
Exercises
49
4 Bayesian Networks
53
4.1
Introduction
53
4.2
Capturing Independence Graphically
53
4.3
Parameterizing the Independence Structure
56
4.4
Properties of Probabilistic Independence
58
4.5
A Graphical Test of Independence
63
4.6
More on DAGs and Independence
68
v

vi
CONTENTS
Bibliographic Remarks
71
4.7
Exercises
72
4.8
Proofs
75
5 Building Bayesian Networks
76
5.1
Introduction
76
5.2
Reasoning with Bayesian Networks
76
5.3
Modeling with Bayesian Networks
84
5.4
Dealing with Large CPTs
114
5.5
The Signiﬁcance of Network Parameters
119
Bibliographic Remarks
121
5.6
Exercises
122
6 Inference by Variable Elimination
126
6.1
Introduction
126
6.2
The Process of Elimination
126
6.3
Factors
128
6.4
Elimination as a Basis for Inference
131
6.5
Computing Prior Marginals
133
6.6
Choosing an Elimination Order
135
6.7
Computing Posterior Marginals
138
6.8
Network Structure and Complexity
141
6.9
Query Structure and Complexity
143
6.10
Bucket Elimination
147
Bibliographic Remarks
148
6.11
Exercises
148
6.12
Proofs
151
7 Inference by Factor Elimination
152
7.1
Introduction
152
7.2
Factor Elimination
153
7.3
Elimination Trees
155
7.4
Separators and Clusters
157
7.5
A Message-Passing Formulation
159
7.6
The Jointree Connection
164
7.7
The Jointree Algorithm: A Classical View
166
Bibliographic Remarks
172
7.8
Exercises
173
7.9
Proofs
176
8 Inference by Conditioning
178
8.1
Introduction
178
8.2
Cutset Conditioning
178
8.3
Recursive Conditioning
181
8.4
Any-Space Inference
188
8.5
Decomposition Graphs
189
8.6
The Cache Allocation Problem
192
Bibliographic Remarks
196

CONTENTS
vii
8.7
Exercises
197
8.8
Proofs
198
9 Models for Graph Decomposition
202
9.1
Introduction
202
9.2
Moral Graphs
202
9.3
Elimination Orders
203
9.4
Jointrees
216
9.5
Dtrees
224
9.6
Triangulated Graphs
229
Bibliographic Remarks
231
9.7
Exercises
232
9.8
Lemmas
234
9.9
Proofs
236
10 Most Likely Instantiations
243
10.1
Introduction
243
10.2
Computing MPE Instantiations
244
10.3
Computing MAP Instantiations
258
Bibliographic Remarks
264
10.4
Exercises
265
10.5
Proofs
267
11 The Complexity of Probabilistic Inference
270
11.1
Introduction
270
11.2
Complexity Classes
271
11.3
Showing Hardness
272
11.4
Showing Membership
274
11.5
Complexity of MAP on Polytrees
275
11.6
Reducing Probability of Evidence to Weighted Model Counting
276
11.7
Reducing MPE to W-MAXSAT
280
Bibliographic Remarks
283
11.8
Exercises
283
11.9
Proofs
284
12 Compiling Bayesian Networks
287
12.1
Introduction
287
12.2
Circuit Semantics
289
12.3
Circuit Propagation
291
12.4
Circuit Compilation
300
Bibliographic Remarks
306
12.5
Exercises
306
12.6
Proofs
309
13 Inference with Local Structure
313
13.1
Introduction
313
13.2
The Impact of Local Structure on Inference Complexity
313
13.3
CNF Encodings with Local Structure
319

viii
CONTENTS
13.4
Conditioning with Local Structure
323
13.5
Elimination with Local Structure
326
Bibliographic Remarks
336
13.6
Exercises
337
14 Approximate Inference by Belief Propagation
340
14.1
Introduction
340
14.2
The Belief Propagation Algorithm
340
14.3
Iterative Belief Propagation
343
14.4
The Semantics of IBP
346
14.5
Generalized Belief Propagation
349
14.6
Joingraphs
350
14.7
Iterative Joingraph Propagation
352
14.8
Edge-Deletion Semantics of Belief Propagation
354
Bibliographic Remarks
364
14.9
Exercises
365
14.10
Proofs
370
15 Approximate Inference by Stochastic Sampling
378
15.1
Introduction
378
15.2
Simulating a Bayesian Network
378
15.3
Expectations
381
15.4
Direct Sampling
385
15.5
Estimating a Conditional Probability
392
15.6
Importance Sampling
393
15.7
Markov Chain Simulation
401
Bibliographic Remarks
407
15.8
Exercises
408
15.9
Proofs
411
16 Sensitivity Analysis
417
16.1
Introduction
417
16.2
Query Robustness
417
16.3
Query Control
427
Bibliographic Remarks
433
16.4
Exercises
434
16.5
Proofs
435
17 Learning: The Maximum Likelihood Approach
439
17.1
Introduction
439
17.2
Estimating Parameters from Complete Data
441
17.3
Estimating Parameters from Incomplete Data
444
17.4
Learning Network Structure
455
17.5
Searching for Network Structure
461
Bibliographic Remarks
466
17.6
Exercises
467
17.7
Proofs
470

CONTENTS
ix
18 Learning: The Bayesian Approach
477
18.1
Introduction
477
18.2
Meta-Networks
479
18.3
Learning with Discrete Parameter Sets
482
18.4
Learning with Continuous Parameter Sets
489
18.5
Learning Network Structure
498
Bibliographic Remarks
504
18.6
Exercises
505
18.7
Proofs
508
A Notation
515
B Concepts from Information Theory
517
C Fixed Point Iterative Methods
520
D Constrained Optimization
523
Bibliography
527
Index
541


Preface
Bayesian networks have received a lot of attention over the last few decades from both
scientists and engineers, and across a number of ﬁelds, including artiﬁcial intelligence
(AI), statistics, cognitive science, and philosophy.
Perhaps the largest impact that Bayesian networks have had is on the ﬁeld of AI,
where they were ﬁrst introduced by Judea Pearl in the midst of a crisis that the ﬁeld was
undergoing in the late 1970s and early 1980s. This crisis was triggered by the surprising
realization that a theory of plausible reasoning cannot be based solely on classical logic
[McCarthy, 1977], as was strongly believed within the ﬁeld for at least two decades
[McCarthy, 1959]. This discovery has triggered a large number of responses by AI
researchers, leading, for example, to the development of a new class of symbolic logics
known as non-monotonic logics (e.g., [McCarthy, 1980; Reiter, 1980; McDermott and
Doyle, 1980]). Pearl’s introduction of Bayesian networks, which is best documented in his
book [Pearl, 1988], was actually part of his larger response to these challenges, in which
he advocated the use of probability theory as a basis for plausible reasoning and developed
Bayesian networks as a practical tool for representing and computing probabilistic beliefs.
From a historical perspective, the earliest traces of using graphical representations of
probabilistic information can be found in statistical physics [Gibbs, 1902] and genetics
[Wright, 1921]. However, the current formulations of these representations are of a more
recent origin and have been contributed by scientists from many ﬁelds. In statistics, for
example, these representations are studied within the broad class of graphical models,
which include Bayesian networks in addition to other representations such as Markov
networks and chain graphs [Whittaker, 1990; Edwards, 2000; Lauritzen, 1996; Cowell
et al., 1999]. However, the semantics of these models are distinct enough to justify
independent treatments. This is why we decided to focus this book on Bayesian networks
instead of covering them in the broader context of graphical models, as is done by others
[Whittaker, 1990; Edwards, 2000; Lauritzen, 1996; Cowell et al., 1999]. Our coverage is
therefore more consistent with the treatments in [Jensen and Nielsen, 2007; Neapolitan,
2004], which are also focused on Bayesian networks.
Even though we approach the subject of Bayesian networks from an AI perspective, we
do not delve into the customary philosophical debates that have traditionally surrounded
many works on AI. The only exception to this is in the introductory chapter, in which we
ﬁnd it necessary to lay out the subject matter of this book in the context of some historical
AI developments. However, in the remaining chapters we proceed with the assumption
that the questions being treated are already justiﬁed and simply focus on developing the
representational and computational techniques needed for addressing them. In doing so,
we have taken a great comfort in presenting some of the very classical techniques in ways
that may seem unorthodox to the expert. We are driven here by a strong desire to provide
the most intuitive explanations, even at the expense of breaking away from norms. We
xi

xii
PREFACE
have also made a special effort to appease the scientist, by our emphasis on justiﬁcation,
and the engineer, through our attention to practical considerations.
There are a number of fashionable and useful topics that we did not cover in this
book, which are mentioned in the introductory chapter. Some of these topics were omitted
because their in-depth treatment would have signiﬁcantly increased the length of the book,
whereas others were omitted because we believe they conceptually belong somewhere
else. In a sense, this book is not meant to be encyclopedic in its coverage of Bayesian
networks; rather it is meant to be a focused, thorough treatment of some of the core
concepts on modeling and reasoning within this framework.
Acknowledgments
In writing this book, I have beneﬁted a great deal form a large number of individuals who
provided help at levels that are too numerous to explicate here. I wish to thank ﬁrst and
foremost members of the automated reasoning group at UCLA for producing quite a bit
of the material that is covered in this book, and for their engagement in the writing and
proofreading of many of its chapters. In particular, I would like to thank David Allen,
Keith Cascio, Hei Chan, Mark Chavira, Arthur Choi, Taylor Curtis, Jinbo Huang, James
Park, Knot Pipatsrisawat, and Yuliya Zabiyaka. Arthur Choi deserves special credit for
writing the appendices and most of Chapter 14, for suggesting a number of interesting
exercises, and for his dedicated involvement in the last stages of ﬁnishing the book. I am
also indebted to members of the cognitive systems laboratory at UCLA – Blai Bonet, Ilya
Shipster, and Jin Tian – who have thoroughly read and commented on earlier drafts of the
book. A number of the students who took the corresponding graduate class at UCLA have
also come to the rescue whenever called. I would like to especially thank Alex Dow for
writing parts of Chapter 9. Moreover, Jason Aten, Omer Bar-or, Susan Chebotariov, David
Chen, Hicham Elmongui, Matt Hayes, Anand Panangadan, Victor Shih, Jae-il Shin, Sam
Talaie, and Mike Zaloznyy have all provided detailed feedback on numerous occasions.
I would also like to thank my colleagues who have contributed immensely to this work
through either valuable discussions, comments on earlier drafts, or strongly believing in
this project and how it was conducted. In this regard, I am indebted to Russ Almond,
Bozhena Bidyuk, Hans Bodlaender, Gregory Cooper, Rina Dechter, Marek Druzdzel,
David Heckerman, Eric Horvitz, Linda van der Gaag, Hector Geffner, Vibhav Gogate,
Russ Greiner, Omid Madani, Ole Mengshoel, Judea Pearl, David Poole, Wojtek Przytula,
Silja Renooij, Stuart Russell, Prakash Shenoy, Hector Palacios Verdes, and Changhe Yuan.
Finally, I wish to thank my wife, Jinan, and my daughters, Sarah and Layla, for
providing a warm and stimulating environment in which I could conduct my work. This
book would not have seen the light without their constant encouragement and support.

1
Introduction
Automated reasoning has been receiving much interest from a number of ﬁelds, including
philosophy, cognitive science, and computer science. In this chapter, we consider the
particular interest of computer science in automated reasoning over the last few decades,
and then focus our attention on probabilistic reasoning using Bayesian networks, which
is the main subject of this book.
1.1 Automated reasoning
The interest in automated reasoning within computer science dates back to the very early
days of artiﬁcial intelligence (AI), when much work had been initiated for developing
computer programs for solving problems that require a high degree of intelligence. In-
deed, an inﬂuential proposal for building automated reasoning systems was extended
by John McCarthy shortly after the term “artiﬁcial intelligence” was coined [McCarthy,
1959]. This proposal, sketched in Figure 1.1, calls for a system with two components: a
knowledge base, which encodes what we know about the world, and a reasoner (inference
engine), which acts on the knowledge base to answer queries of interest. For example, the
knowledge base may encode what we know about the theory of sets in mathematics, and
the reasoner may be used to prove various theorems about this domain.
McCarthy’s proposal was actually more speciﬁc than what is suggested by Figure 1.1,
as he called for expressing the knowledge base using statements in a suitable logic, and
for using logical deduction in realizing the reasoning engine; see Figure 1.2. McCarthy’s
proposal can then be viewed as having two distinct and orthogonal elements. The ﬁrst is
the separation between the knowledge base (what we know) and the reasoner (how we
think). The knowledge base can be domain-speciﬁc, changing from one application to
another, while the reasoner is quite general and ﬁxed, allowing one to use it across different
application areas. This aspect of the proposal became the basis for a class of reasoning
systems known as knowledge-based or model-based systems, which have dominated the
area of automated reasoning since then. The second element of McCarthy’s early proposal
is the speciﬁc commitment to logic as the language for expressing what we know about the
world, and his commitment to logical deduction in realizing the reasoning process. This
commitment, which was later revised by McCarthy, is orthogonal to the idea of separating
the knowledge base from the reasoner. The latter idea remains meaningful and powerful
even in the context of other forms of reasoning including probabilistic reasoning, to which
this book is dedicated. We will indeed subscribe to this knowledge-based approach for
reasoning, except that our knowledge bases will be Bayesian networks and our reasoning
engine will be based on the laws of probability theory.
1

2
INTRODUCTION
Knowledge Base (KB)
Inference Engine
Observations
Conclusions
Figure 1.1: A reasoning system in which the knowledge base is separated from the reasoning process.
The knowledge base is often called a “model,” giving rise to the term “model-based reasoning.”
Observations
Conclusions
Logical Deduction
Statements in Logic
Figure 1.2: A reasoning system based on logic.
1.1.1 The limits of deduction
McCarthy’s proposal generated much excitement and received much interest throughout
the history of AI, due mostly to its modularity and mathematical elegance. Yet, as the
approach was being applied to more application areas, a key difﬁculty was unveiled, calling
for some alternative proposals. In particular, it was observed that although deductive logic
is a natural framework for representing and reasoning about facts, it was not capable of
dealing with assumptions that tend to be prevalent in commonsense reasoning.
To better explain this difference between facts and assumptions, consider the following
statement:
If a bird is normal, it will ﬂy.
Most people will believe that a bird would ﬂy if they see one. However, this belief cannot
be logically deduced from this fact, unless we further assume that the bird we just saw
is normal. Most people will indeed make this assumption – even if they cannot conﬁrm
it – as long as they do not have evidence to the contrary. Hence, the belief in a ﬂying
bird is the result of a logical deduction applied to a mixture of facts and assumptions. For
example, if it turns out that the bird, say, has a broken wing, the normality assumption
will be retracted, leading us to also retract the belief in a ﬂying bird.
This ability to dynamically assert and retract assumptions – depending on what is
currently known – is quite typical in commonsense reasoning yet is outside the realm of
deductive logic, as we shall see in Chapter 2. In fact, deductive logic is monotonic in the
sense that once we deduce something from a knowledge base (the bird ﬂies), we can never
invalidate the deduction by acquiring more knowledge (the bird has a broken wing). The
formal statement of monotonicity is as follows:
If  logically implies α, then  and  will also logically imply α.

1.1 AUTOMATED REASONING
3
Just think of a proof for α that is derived from a set of premises . We can never invalidate
this proof by including the additional premises . Hence, no deductive logic is capable of
producing the reasoning process described earlier with regard to ﬂying birds.
We should stress here that the ﬂying bird example is one instance of a more general
phenomenon that underlies much of what goes on in commonsense reasoning. Consider
for example the following statements:
My car is still parked where I left it this morning.
If I turn the key of my car, the engine will turn on.
If I start driving now, I will get home in thirty minutes.
None of these statements is factual, as each is qualiﬁed by a set of assumptions. Yet we
tend to make these assumptions, use them to derive certain conclusions (e.g., I will arrive
home in thirty minutes if I head out of the ofﬁce now), and then use these conclusions to
justify some of our decisions (I will head home now). Moreover, we stand ready to retract
any of these assumptions if we observe something to the contrary (e.g., a major accident
on the road home).
1.1.2 Assumptions to the rescue
The previous problem, which is known as the qualiﬁcation problem in AI [McCarthy,
1977], was stated formally by McCarthy in the late 1970s, some twenty years after his
initial proposal from 1958. The dilemma was simply this: If we write “Birds ﬂy,” then
deductive logic would be able to infer the expected conclusion when it sees a bird.
However, it would fall into an inconsistency if it encounters a bird that cannot ﬂy. On the
other hand, if we write “If a bird is normal, it ﬂies,” deductive logic will not be able to
reach the expected conclusion upon seeing a bird, as it would not know whether the bird
is normal or not – contrary to what most humans will do. The failure of deductive logic in
treating this problem effectively led to a ﬂurry of activities in AI, all focused on producing
new formalisms aimed at counteracting this failure.
McCarthy’s observations about the qualiﬁcation problem were accompanied by another
inﬂuential proposal, which called for equipping logic with an ability to jump into certain
conclusions [McCarthy, 1977]. This proposal had the effect of installing the notion of
assumption into the heart of logical formalisms, giving rise to a new generation of logics,
non-monotonic logics, which are equipped with mechanisms for managing assumptions
(i.e., allowing them to be dynamically asserted and retracted depending on what else is
known). However, it is critical to note that what is needed here is not simply a mechanism
for managing assumptions but also a criterion for deciding on which assumptions to assert
and retract, and when. The initial criterion used by many non-monotonic logics was based
on the notion of logical consistency, which calls for asserting as many assumptions as
possible, as long as they do not lead to a logical inconsistency. This promising idea proved
insufﬁcient, however. To illustrate the underlying difﬁculties here, let us consider the
following statements:
A typical Quaker is a paciﬁst.
A typical Republican is not a paciﬁst.
If we were told that Nixon is a Quaker, we could then conclude that he is a paciﬁst
(by assuming he is a typical Quaker). On the other hand, if we were told that Nixon is

4
INTRODUCTION
a Republican, we could conclude that he is not a paciﬁst (by assuming he is a typical
Republican). But what if we were told that Nixon is both a Quaker and a Republican? The
two assumptions would then clash with each other, and a decision would have to be made
on which assumption to preserve (if either). What this example illustrates is that assump-
tions can compete against each other. In fact, resolving conﬂicts among assumptions
turned out to be one of the difﬁcult problems that any assumption-based formalism must
address to capture commonsense reasoning satisfactorily.
To illustrate this last point, consider a student, Drew, who just ﬁnished the ﬁnal exam
for his physics class. Given his performance on this and previous tests, Drew came to the
belief that he would receive an A in the class. A few days later, he logs into the university
system only to ﬁnd out that he has received a B instead. This clash between Drew’s prior
belief and the new information leads him to think as follows:
Let me ﬁrst check that I am looking at the grade of my physics class instead of some other
class. Hmm! It is indeed physics. Is it possible the professor made a mistake in entering the
grade? I don’t think so . . . I have taken a few classes with him, and he has proven to be quite
careful and thorough. Well, perhaps he did not grade my Question 3, as I wrote the answer
on the back of the page in the middle of a big mess. I think I will need to check with him on
this . . . I just hope I did not miss Question 4; it was somewhat difﬁcult and I am not too sure
about my answer there. Let me check with Jack on this, as he knows the material quite well.
Ah! Jack seems to have gotten the same answer I got. I think it is Question 3 after all . . . I’d
better see the professor soon to make sure he graded this one.
One striking aspect of this example is the multiplicity of assumptions involved in
forming Drew’s initial belief in having received an A grade (i.e., Question 3 was graded,
Question 4 was solved correctly, the professor did not make a clerical error, and so on). The
example also brings out important notions that were used by Drew in resolving conﬂicts
among assumptions. This includes the strength of an assumption, which can be based on
previous experiences (e.g., I have taken a few classes with this professor). It also includes
the notion of evidence, which may be brought to bear on the validity of these assumptions
(i.e., let me check with Jack).
Having reached this stage of our discussion on the subtleties of commonsense reason-
ing, one could drive it further in one of two directions. We can continue to elaborate on
non-monotonic logics and how they may go about resolving conﬂicts among assump-
tions. This will also probably lead us into the related subject of belief revision, which
aims at regulating this conﬂict-resolution process through a set of rationality postulates
[G¨ardenfors, 1988]. However, as these subjects are outside the scope of this book, we will
turn in a different direction that underlies the formalism we plan to pursue in the upcoming
chapters. In a nutshell, this new direction can be viewed as postulating the existence of a
more fundamental notion, called a degree of belief, which, according to some treatments,
can alleviate the need for assumptions altogether and, according to others, can be used as
a basis for deciding which assumptions to make in the ﬁrst place.
1.2 Degrees of belief
A degree of belief is a number that one assigns to a proposition in lieu of having to declare
it as a fact (as in deductive logic) or an assumption (as in non-monotonic logic). For
example, instead of assuming that a bird is normal unless observed otherwise – which

1.2 DEGREES OF BELIEF
5
leads us to tenuously believe that it also ﬂies – we assign a degree of belief to the bird’s
normality, say, 99%, and then use this to derive a corresponding degree of belief in the
bird’s ﬂying ability.
A number of different proposals have been extended in the literature for interpreting
degrees of belief including, for example, the notion of possibility on which fuzzy logic
is based. This book is committed to interpreting degrees of belief as probabilities and,
therefore, to manipulating them according to the laws of probability. Such an interpretation
is widely accepted today and underlies many of the recent developments in automated
reasoning. We will brieﬂy allude to some of the classical arguments supporting this
interpretation later but will otherwise defer the vigorous justiﬁcation to cited references
[Pearl, 1988; Jaynes, 2003].
While assumptions address the monotonicity problem by being assertible and re-
tractible, degrees of belief address this problem by being revisable either upward or down-
ward, depending on what else is known. For example, we may initially believe that a bird
is normal with probability 99%, only to revise this to, say, 20% after learning that its
wing is suffering from some wound. The dynamics that govern degrees of belief will be
discussed at length in Chapter 3, which is dedicated to probability calculus, our formal
framework for manipulating degrees of belief.
One can argue that assigning a degree of belief is a more committing undertaking than
making an assumption. This is due to the ﬁne granularity of degrees of beliefs, which
allows them to encode more information than can be encoded by a binary assumption. One
can also argue to the contrary that working with degrees of belief is far less committing
as they do not imply any particular truth of the underlying propositions, even if tenuous.
This is indeed true, and this is one of the key reasons why working with degrees of belief
tends to protect against many pitfalls that may trap one when working with assumptions;
see Pearl [1988], Section 2.3, for some relevant discussion on this matter.
1.2.1 Deciding after believing
Forming beliefs is the ﬁrst step in making decisions. In an assumption-based framework,
decisions tend to follow naturally from the set of assumptions made. However, when
working with degrees of belief, the situation is a bit more complex since decisions will
have to be made without assuming any particular state of affairs. Suppose for example
that we are trying to capture a bird that is worth $40.00 and can use one of two methods,
depending on whether it is a ﬂying bird or not. The assumption-based method will have no
difﬁculty making a decision in this case, as it will simply choose the method based on the
assumptions made. However, when using degrees of belief, the situation can be a bit more
involved as it generally calls for invoking decision theory, whose purpose is to convert
degrees of beliefs into deﬁnite decisions [Howard and Matheson, 1984; Howard, 1990].
Decision theory needs to bring in some additional information before it can make the
conversion, including the cost of various decisions and the rewards or penalties associated
with their outcomes. Suppose for example that the ﬁrst method is guaranteed to capture
a bird, whether ﬂying or not, and costs $30.00, while the second method costs $10.00
and is guaranteed to capture a non-ﬂying bird but may capture a ﬂying bird with a 25%
probability. One must clearly factor all of this information before one can make the right
decision in this case, which is precisely the role of decision theory. This theory is therefore
an essential complement to the theory of probabilistic reasoning discussed in this book.

6
INTRODUCTION
Yet we have decided to omit the discussion of decision theory here to keep the book
focused on the modeling and reasoning components (see Pearl [1988], Jensen and Nielsen
[2007] for a complementary coverage of decision theory).
1.2.2 What do the probabilities mean?
A ﬁnal point we wish to address in this section concerns the classical controversy of
whether probabilities should be interpreted as objective frequencies or as subjective de-
grees of belief. Our use of the term “degrees of belief” thus far may suggest a commitment,
to the subjective approach, but this is not necessarily the case. In fact, none of the develop-
ments in this book really depend on any particular commitment, as both interpretations are
governed by the same laws of probability. We will indeed discuss examples in Chapter 5
where all of the used probabilities are degrees of belief reﬂecting the state of knowledge
of a particular individual and not corresponding to anything that can be measured by a
physical experiment. We will also discuss examples in which all of the used probabilities
correspond to physical quantities that can be not only measured but possibly controlled as
well. This includes applications from system analysis and diagnostics, where probabilities
correspond to the failure rates of system components, and examples from channel coding,
where the probabilities correspond to channel noise.
1.3 Probabilistic reasoning
Probability theory has been around for centuries. However, its utilization in automated
reasoning at the scale and rate within AI has never before been attempted. This has
created some key computational challenges for probabilistic reasoning systems, which
had to be confronted by AI researchers for the ﬁrst time. Adding to these challenges is the
competition that probabilistic methods had initially received from symbolic methods that
were dominating the ﬁeld of AI at the time. It is indeed the responses to these challenges
over the last few decades that have led to much of the material discussed in this book.
One therefore gains more perspective and insights into the utility and signiﬁcance of the
covered topics once one is exposed to some of these motivating challenges.
1.3.1 Initial reactions
AI researchers proposed the use of numeric degrees of belief well before the monotonicity
problem of classical logic was unveiled or its consequences absorbed. Yet such proposals
were initially shunned based on cognitive, pragmatic, and computational considerations.
On the cognitive side, questions were raised regarding the extent to which humans use such
degrees of belief in their own reasoning. This was quite an appealing counterargument at
the time, as the ﬁeld of AI was still at a stage of its development where the resemblance of
formalism to human cognition was very highly valued, if not necessary. On the pragmatic
side, questions were raised regarding the availability of degrees of beliefs (where do the
numbers come from?). This came at a time when the development of knowledge bases was
mainly achieved through knowledge elicitation sessions conducted with domain experts
who, reportedly, were not comfortable committing to such degrees – the ﬁeld of statistical
machine learning had yet to be inﬂuential enough then. The robustness of probabilistic
reasoning systems was heavily questioned as well (what happens if I change this .90 to
.95?). The issue here was not only whether probabilistic reasoning was robust enough

1.3 PROBABILISTIC REASONING
7
against such perturbations but, in situations where it was shown to be robust, questions
were raised about the unnecessary level of detail demanded by specifying probabilities.
On the computational side, a key issue was raised regarding the scale of applications that
probabilistic reasoning systems can handle, at a time when applications involving dozens
if not hundreds of variables were being sought. Such doubts were grounded in the prevalent
perception that joint probability distributions, which are exponentially sized in the number
of used variables, will have to be represented explicitly by probabilistic reasoning systems.
This would be clearly prohibitive on both representational and computational grounds for
most applications of interest. For example, a medical diagnosis application may require
hundreds of variables to represent background information about patients, in addition to
the list of diseases and symptoms about which one may need to reason.
1.3.2 A second chance
The discovery of the qualiﬁcation problem, and the associated monotonicity problem of
deductive logic, gave numerical methods a second chance in AI, as these problems created
a vacancy for a new formalism of commonsense reasoning during the 1980s. One of the
key proponents of probabilistic reasoning at the time was Judea Pearl, who seized upon
this opportunity to further the cause of probabilistic reasoning systems within AI. Pearl
had to confront challenges on two key fronts in this pursuit. On the one hand, he had to
argue for the use of numbers within a community that was heavily entrenched in symbolic
formalism. On the other hand, he had to develop a representational and computational
machinery that could compete with symbolic systems that were in commercial use at the
time.
On the ﬁrst front, Pearl observed that many problems requiring special machinery in
logical settings, such as non-monotonicity, simply do not surface in the probabilistic ap-
proach. For example, it is perfectly common in probability calculus to see beliefs going up
and down in response to new evidence, thus exhibiting a non-monotonic behavior – that
is, we often ﬁnd Pr(A) > Pr(A|B) indicating that our belief in A would go down when we
observe B. Based on this and similar observations, Pearl engaged in a sequence of papers
that provided probabilistic accounts for most of the paradoxes that were entangling sym-
bolic formalisms at the time; see Pearl [1988], Chapter 10, for a good summary. Most of
the primitive cognitive and pragmatic arguments (e.g., people do not reason with numbers;
where do the numbers come from?) were left unanswered then. However, enough desir-
able properties of probabilistic reasoning were revealed to overwhelm and silence these
criticisms. The culminations of Pearl’s efforts at the time were reported in his inﬂuential
book, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference
[Pearl, 1988]. The book contained the ﬁrst comprehensive documentation of the case for
probabilistic reasoning, delivered in the context of contemporary questions raised by AI
research. This part of the book was concerned with the foundational aspects of plausible
reasoning, setting clear the principles by which it ought to be governed – probability
theory, that is. The book also contained the ﬁrst comprehensive coverage of Bayesian
networks, which were Pearl’s response to the representational and computational chal-
lenges that arise in realizing probabilistic reasoning systems. On the representational side,
the Bayesian network was shown to compactly represent exponentially sized probability
distributions, addressing one of the classical criticisms against probabilistic reasoning sys-
tems. On the computational side, Pearl developed the polytree algorithm [Pearl, 1986b],
which was the ﬁrst general-purpose inference algorithm for networks that contain no

8
INTRODUCTION
Q3: Question 3
graded
Q4: Question 4
correct
E: Earned ‘A’
C: Clerical error
R: Reported ‘A’
O: Observe ‘A’
J: Jack confirms
answer
P: Perception error
C
Pr(C)
true
.001
Q3
Q4
E
Pr(E|Q3, Q4)
true
true
true
1
true
false
true
0
false
true
true
0
false
false
true
0
Q4
J
Pr(J|Q4)
true
true
.99
false
true
.20
Figure 1.3: The structure of a Bayesian network, in which each variable can be either true or false. To
fully specify the network, one needs to provide a probability distribution for each variable, conditioned
on every state of its parents. The ﬁgure shows these conditional distributions for three variables in the
network.
directed loops.1 This was followed by the inﬂuential jointree algorithm [Lauritzen and
Spiegelhalter, 1988], which could handle arbitrary network structures, albeit inefﬁciently
for some structures. These developments provided enough grounds to set the stage for a
new wave of automated reasoning systems based on the framework of Bayesian networks
(e.g., [Andreassen et al., 1987]).
1.4 Bayesian networks
A Bayesian network is a representational device that is meant to organize one’s knowledge
about a particular situation into a coherent whole. The syntax and semantics of Bayesian
networks will be covered in Chapter 4. Here we restrict ourselves to an informal exposition
that is sufﬁcient to further outline the subjects covered in this book.
Figure 1.3 depicts an example Bayesian network, which captures the information
corresponding to the student scenario discussed earlier in this chapter. This network has
two components, one qualitative and another quantitative. The qualitative part corresponds
to the directed acyclic graph (DAG) depicted in the ﬁgure, which is also known as the
1 According to Pearl, this algorithm was motivated by the work of Rumelhart [1976] on reading comprehension,
which provided compelling evidence that text comprehension must be a distributed process that combines both
top-down and bottom-up inferences. This dual mode of inference, so characteristic of Bayesian analysis, did not
match the capabilities of the ruling paradigms for uncertainty management in the 1970s. This led Pearl to develop
the polytree algorithm [Pearl, 1986b], which appeared ﬁrst in Pearl [1982] with a restriction to trees, and then in
Kim and Pearl [1983] for polytrees.

1.4 BAYESIAN NETWORKS
9
structure of the Bayesian network. This structure captures two important parts of one’s
knowledge. First, its variables represent the primitive propositions that we deem relevant
to our domain. Second, its edges convey information about the dependencies between
these variables. The formal interpretation of these edges will be given in Chapter 4 in
terms of probabilistic independence. For now and for most practical applications, it is
best to think of these edges as signifying direct causal inﬂuences. For example, the edge
extending from variable E to variable R signiﬁes a direct causal inﬂuence between earning
an A grade and reporting the grade. Note that variables Q3 and Q4 also have a causal
inﬂuence on variable R yet this inﬂuence is not direct, as it is mediated by variable E.
We stress again that Bayesian networks can be given an interpretation that is completely
independent of the notion of causation, as in Chapter 4, yet thinking about causation will
tend to be a very valuable guide in constructing the intended Bayesian network [Pearl,
2000; Glymour and Cooper, 1999].
To completely specify a Bayesian network, one must also annotate its structure with
probabilities that quantify the relationships between variables and their parents (direct
causes). We will not delve into this speciﬁcation procedure here but sufﬁce it to say
it is a localized process. For example, the probabilities corresponding to variable E in
Figure 1.3 will only reference this variable and its direct causes Q3 and Q4. Moreover, the
probabilities corresponding to variable C will only reference this variable, as it does not
have any causes. This is one of the key representational aspects of a Bayesian network:
we are never required to specify a quantitative relationship between two variables unless
they are connected by an edge. Probabilities that quantify the relationship between a
variable and its indirect causes (or its indirect effects) will be computed automatically by
inference algorithms, which we discuss in Section 1.4.2.
As a representational tool, the Bayesian network is quite attractive for three reasons.
First, it is a consistent and complete representation as it is guaranteed to deﬁne a unique
probability distribution over the network variables. Hence by building a Bayesian net-
work, one is specifying a probability for every proposition that can be expressed using
these network variables. Second, the Bayesian network is modular in the sense that its
consistency and completeness are ensured using localized tests that apply only to vari-
ables and their direct causes. Third, the Bayesian network is a compact representation as
it allows one to specify an exponentially sized probability distribution using a polynomial
number of probabilities (assuming the number of direct causes remains small).
We will next provide an outline of the remaining book chapters, which can be di-
vided into two components corresponding to modeling and reasoning with Bayesian
networks.
1.4.1 Modeling with Bayesian networks
One can identify three main methods for constructing Bayesian networks when trying
to model a particular situation. These methods are covered in four chapters of the book,
which are outlined next.
According to the ﬁrst method, which is largely subjective, one reﬂects on their own
knowledge or the knowledge of others and then captures it into a Bayesian network (as
we have done in Figure 1.3). According to the second method, one automatically syn-
thesizes the Bayesian network from some other type of formal knowledge. For example,
in many applications that involve system analysis, such as reliability and diagnosis, one
can synthesize a Bayesian network automatically from formal system designs. Chapter 5
will be concerned with these two modeling methods, which are sometimes known as

10
INTRODUCTION
the knowledge representation (KR) approach for constructing Bayesian networks. Our
exposure here will be guided by a number of application areas in which we state prob-
lems and show how to solve them by ﬁrst building a Bayesian network and then posing
queries with respect to the constructed network. Some of the application areas we dis-
cuss include system diagnostics, reliability analysis, channel coding, and genetic linkage
analysis.
Constructing Bayesian networks according to the KR approach can beneﬁt greatly
from sensitivity analysis, which is covered partly in Chapter 5 and more extensively in
Chapter 16. Here we provide techniques for checking the robustness of conclusions drawn
from Bayesian networks against perturbations in the local probabilities that annotate them.
We also provide techniques for automatically revising these local probabilities to satisfy
some global constraints that are imposed by the opinions of experts or derived from the
formal speciﬁcations of the tasks under consideration.
The third method for constructing Bayesian networks is based on learning them from
data, such as medical records or student admissions data. Here either the structure, the
probabilities, or both can be learned from the given data set. Since learning is an inductive
process, one needs a principle of induction to guide the construction process according
to this machine learning (ML) approach. We discuss two such principles in this book,
leading to what are known as the maximum likelihood and Bayesian approaches to
learning. The maximum likelihood approach, which is discussed in Chapter 17, favors
Bayesian networks that maximize the probability of observing the given data set. The
Bayesian approach, which is discussed in Chapter 18, uses the likelihood principle in
addition to some prior information that encodes preferences on Bayesian networks.2
Networks constructed by the KR approach tend to have a different nature than those
constructed by the ML approach. For example, these former networks tend to be much
larger in size and, as such, place harsher computational demands on reasoning algorithms.
Moreover, these networks tend to have a signiﬁcant amount of determinism (i.e., proba-
bilities that are equal to 0 or 1), allowing them to beneﬁt from computational techniques
that may be irrelevant to networks constructed by the ML approach.
1.4.2 Reasoning with Bayesian networks
Let us now return to Figure 1.1, which depicts the architecture of a knowledge-based rea-
soning system. In the previous section, we introduced those chapters that are concerned
with constructing Bayesian networks (i.e., the knowledge bases or models). The remain-
ing chapters of this book are concerned with constructing the reasoning engine, whose
purpose is to answer queries with respect to these networks. We will ﬁrst clarify what is
meant by reasoning (or inference) and then lay out the topics covered by the reasoning
chapters.
We have already mentioned that a Bayesian network assigns a unique probability to
each proposition that can be expressed using the network variables. However, the network
itself only explicates some of these probabilities. For example, according to Figure 1.3 the
probability of a clerical error when entering the grade is .001. Moreover, the probability
2 It is critical to observe here that the term “Bayesian network” does not necessarily imply a commitment to the
Bayesian approach for learning networks. This term was coined by Judea Pearl [Pearl, 1985] to emphasize three
aspects: the often subjective nature of the information used in constructing these networks; the reliance on Bayes’s
conditioning when reasoning with Bayesian networks; and the ability to perform causal as well as evidential
reasoning on these networks, which is a distinction underscored by Thomas Bayes [Bayes, 1963].

1.4 BAYESIAN NETWORKS
11
that Jack obtains the same answer on Question 4 is .99, assuming that the question was
answered correctly by Drew. However, consider the following probabilities:
r Pr(E =true): The probability that Drew earned an A grade.
r Pr(Q3 =true|E =false): The probability that Question 3 was graded, given that Drew did
not earn an A grade.
r Pr(Q4 =true|E =true): The probability that Jack obtained the same answer as Drew on
Question 4, given that Drew earned an A grade.
None of these probabilities would be part of the fully speciﬁed Bayesian network. Yet as
we show in Chapter 4, the network is guaranteed to imply a unique value for each one of
these probabilities. It is indeed the purpose of reasoning/inference algorithms to deduce
these values from the information given by the Bayesian network, that is, its structure and
the associated local probabilities.
Even for a small example like the one given in Figure 1.3, it may not be that trivial for an
expert on probabilistic reasoning to infer the values of the probabilities given previously. In
principle, all one needs is a complete and correct reading of the probabilistic information
encoded by the Bayesian network followed by a repeated application of enough laws of
probability theory. However, the number of possible applications of these laws may be
prohibitive, even for examples of the scale given here. The goal of reasoning/inference
algorithms is therefore to relieve the user from undertaking this probabilistic reasoning
process on their own, handing it instead to an automated process that is guaranteed to
terminate while trying to use the least amount of computational resources (i.e., time and
space). It is critical to stress here that automating the reasoning process is not only meant
to be a convenience for the user. For the type of applications considered in this book,
especially in Chapter 5, automated reasoning may be the only feasible method for solving
the corresponding problems. For example, we will be encountering applications that, in
their full scale, may involve thousands of variables. For these types of networks, one must
appeal to automated reasoning algorithms to obtain the necessary answers. More so, one
must appeal to very efﬁcient algorithms if one is operating under constrained time and
space resources – as is usually the case.
We cover two main classes of inference algorithms in this book, exact algorithms and
approximate algorithms. Exact algorithms are guaranteed to return correct answers and
tend to be more demanding computationally. On the other hand, approximate algorithms
relax the insistence on exact answers for the sake of easing computational demands.
Exact inference
Much emphasis was placed on exact inference in the 1980s and the early 1990s, leading
to two classes of algorithms based on the concepts of elimination and conditioning.
Elimination algorithms are covered in Chapters 6 and 7, while conditioning algorithms
are covered in Chapter 8. The complexity of these algorithms is exponential in the network
treewidth, which is a graph-theoretic parameter that measures the resemblance of a graph
to a tree structure (e.g., trees have a treewidth ≤1). We dedicate Chapter 9 to treewidth
and some corresponding graphical manipulations, given the inﬂuential role they play in
dictating the performance of exact inference algorithms.
Advanced inference algorithms
The inference algorithms covered in Chapters 6 through 8 are called structure-based,
as their complexity is sensitive only to the network structure. In particular, these

12
INTRODUCTION
algorithms will consume the same computational resources when applied to two net-
works that share the same structure (i.e., have the same treewidth), regardless of what
probabilities are used to annotate them. It has long been observed that inference algo-
rithms can be made more efﬁcient if they also exploit the structure exhibited by network
probabilities, which is known as local structure. Yet algorithms for exploiting local struc-
ture have only matured in the last few years. We provide an extensive coverage of these
algorithms in Chapters 10, 11, 12, and 13. The techniques discussed in these chapters have
allowed exact inference on some networks whose treewidth is quite large. Interestingly
enough, networks constructed by the KR approach tend to be most amenable to these
techniques.
Approximate inference
Around the mid-1990s, a strong belief started forming in the inference community that
the performance of exact algorithms must be exponential in treewidth – this is before
local structure was being exploited effectively. At about the same time, methods for
automatically constructing Bayesian networks started maturing to the point of yielding
networks whose treewidth is too large to be handled by exact algorithms. This has led to
a surge of interest in approximate inference algorithms, which are generally independent
of treewidth. Today, approximate inference algorithms are the only choice for networks
that have a large treewidth yet lack sufﬁcient local structure. We cover approximation
techniques in two chapters. In Chapter 14, we discuss algorithms that are based on reducing
the inference problem to a constrained optimization problem, leading to the inﬂuential
class of belief propagation algorithms. In Chapter 15, we discuss algorithms that are based
on stochastic sampling, leading to approximations that can be made arbitrarily accurate
as more time is allowed for use by the algorithm.
1.5 What is not covered in this book
As we discussed previously, decision theory has been left out to keep this book focused
on the modeling and reasoning components. We also restrict our discussion to discrete
Bayesian networks in which every variable has a ﬁnite number of values. One exception is
in Chapter 3, where we discuss continuous variables representing sensor readings – these
are commonly used in practice and are useful in analyzing the notion of uncertain evidence.
Another exception is in Chapter 18, where we discuss continuous variables whose values
represent model parameters – these are necessary for the treatment of Bayesian learning.
We do not discuss undirected models, such as Markov networks and chain graphs, as we
believe they belong more to a book that treats the broader subject of graphical models (e.g.,
[Whittaker, 1990; Lauritzen, 1996; Cowell et al., 1999; Edwards, 2000]). We have also left
out the discussion of high-level speciﬁcations of probabilistic models based on relational
and ﬁrst-order languages. Covering this topic is rather tempting given our emphasis on
modeling yet it cannot be treated satisfactorily without signiﬁcantly increasing the length
of this book. We also do not treat causality from a Bayesian network perspective as this
topic has matured enough to merit its own dedicated treatment [Pearl, 2000; Glymour and
Cooper, 1999].

2
Propositional Logic
We introduce propositional logic in this chapter as a tool for representing and reasoning
about events.
2.1 Introduction
The notion of an event is central to both logical and probabilistic reasoning. In the former,
we are interested in reasoning about the truth of events (facts), while in the latter we are
interested in reasoning about their probabilities (degrees of belief). In either case, one
needs a language for expressing events before one can write statements that declare their
truth or specify their probabilities. Propositional logic, which is also known as Boolean
logic or Boolean algebra, provides such a language.
We start in Section 2.2 by discussing the syntax of propositional sentences, which we
use for expressing events. We then follow in Section 2.3 by discussing the semantics
of propositional logic, where we deﬁne properties of propositional sentences, such as
consistency and validity, and relationships among them, such as implication, equivalence,
and mutual exclusiveness. The semantics of propositional logic are used in Section 2.4
to formally expose its limitations in supporting plausible reasoning. This also provides a
good starting point for Chapter 3, where we show how degrees of belief can deal with
these limitations.
In Section 2.5, we discuss variables whose values go beyond the traditional true and
false values of propositional logic. This is critical for our treatment of probabilistic
reasoning in Chapter 3, which relies on the use of multivalued variables. We discuss in
Section 2.6 the notation we adopt for denoting variable instantiations, which are the most
fundamental type of events we deal with. In Section 2.7, we provide a treatment of logical
forms, which are syntactic restrictions that one imposes on propositional sentences; these
include disjunctive normal form (DNF), conjunctive normal form (CNF), and negation
normal form (NNF). A discussion of these forms is necessary for some of the advanced
inference algorithms we discuss in later chapters.
2.2 Syntax of propositional sentences
Consider a situation that involves an alarm meant for detecting burglaries, and suppose
the alarm may also be triggered by an earthquake. Consider now the event of having either
a burglary or an earthquake. One can express this event using the following propositional
sentence:
Burglary ∨Earthquake.
13

14
PROPOSITIONAL LOGIC
A
C
Y
X
B
Figure 2.1: A digital circuit.
Here Burglary and Earthquake are called propositional variables and ∨represents logical
disjunction (or). Propositional logic can be used to express more complex statements,
such as:
Burglary ∨Earthquake =⇒Alarm,
(2.1)
where =⇒represents logical implication. According to this sentence, a burglary or an
earthquake is guaranteed to trigger the alarm. Consider also the sentence:
¬Burglary ∧¬Earthquake =⇒¬Alarm,
(2.2)
where ¬ represents logical negation (not) and ∧represents logical conjunction (and).
According to this sentence, if there is no burglary and there is no earthquake, the alarm
will not trigger.
More generally, propositional sentences are formed using a set of propositional vari-
ables, P1, . . . , Pn. These variables – which are also called Boolean variables or binary
variables – assume one of two values, typically indicated by true and false. Our previous
example was based on three propositional variables, Burglary, Earthquake, and Alarm.
The simplest sentence one can write in propositional logic has the form Pi. It is called
an atomic sentence and is interpreted as saying that variable Pi takes on the value true.
More generally, propositional sentences are formed as follows:
r Every propositional variable Pi is a sentence.
r If α and β are sentences, then ¬α, α ∧β, and α ∨β are also sentences.
The symbols ¬, ∧, and ∨are called logical connectives and they stand for negation,
conjunction, and disjunction, respectively. Other connectives can also be introduced, such
as implication =⇒and equivalence ⇐⇒, but these can be deﬁned in terms of the three
primitive connectives given here. In particular, the sentence α =⇒β is shorthand for
¬α ∨β. Similarly, the sentence α ⇐⇒β is shorthand for (α =⇒β) ∧(β =⇒α).1
A propositional knowledge base is a set of propositional sentences α1, α2, . . . , αn, that is
interpreted as a conjunction α1 ∧α2 ∧. . . ∧αn.
Consider now the digital circuit in Figure 2.1, which has two inputs and one output.
Suppose that we want to write a propositional knowledge base that captures our knowledge
about the behavior of this circuit. The very ﬁrst step to consider is that of choosing the
1 We follow the standard convention of giving the negation operator ¬ ﬁrst precedence, followed by the conjunction
operator ∧and then the disjunction operator ∨. The operators =⇒and ⇐⇒have the least (and equal) precedence.

2.3 SEMANTICS OF PROPOSITIONAL SENTENCES
15
set of propositional variables. A common choice here is to use one propositional variable
for each wire in the circuit, leading to the following variables: A, B, C, X, and Y. The
intention is that when a variable is true, the corresponding wire is considered high, and
when the variable is false, the corresponding wire is low. This leads to the following
knowledge base:
 =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
A
=⇒
¬X
¬A
=⇒
X
A ∧B
=⇒
Y
¬(A ∧B)
=⇒
¬Y
X ∨Y
=⇒
C
¬(X ∨Y)
=⇒
¬C.
2.3 Semantics of propositional sentences
Propositional logic provides a formal framework for deﬁning properties of sentences,
such as consistency and validity, and relationships among them, such as implication,
equivalence, and mutual exclusiveness. For example, the sentence in (2.1) logically implies
the following sentence:
Burglary =⇒Alarm.
Maybe less obviously, the sentence in (2.2) also implies the following:
Alarm ∧¬Burglary =⇒Earthquake.
These properties and relationships are easy to ﬁgure out for simple sentences. For
example, most people would agree that:
r A ∧¬A is inconsistent (will never hold).
r A ∨¬A is valid (always holds).
r A and (A =⇒B) imply B.
r A ∨B is equivalent to B ∨A.
Yet it may not be as obvious that A =⇒B and ¬B =⇒¬A are equivalent, or that
(A =⇒B) ∧(A =⇒¬B) implies ¬A. For this reason, one needs formal deﬁnitions of
logical properties and relationships. As we show in the following section, deﬁning these
notions is relatively straightforward once the notion of a world is deﬁned.
2.3.1 Worlds, models, and events
A world is a particular state of affairs in which the value of each propositional variable is
known. Consider again the example discussed previously that involves three propositional
variables, Burglary, Earthquake, and Alarm. We have eight worlds in this case, which are
shown in Table 2.1. Formally, a world ω is a function that maps each propositional
variable Pi into a value ω(Pi) ∈{true, false}. For this reason, a world is often called a
truth assignment, a variable assignment, or a variable instantiation.
The notion of a world allows one to decide the truth of sentences without ambiguity.
For example, Burglary is true at world ω1 of Table 2.1 since the world assigns the value
true to variable Burglary. Moreover, ¬Burglary is true at world ω3 since the world assigns
false to Burglary, and Burglary ∨Earthquake is true at world ω4 since it assigns true to

16
PROPOSITIONAL LOGIC
Table 2.1: A set of worlds, also known as truth assignments,
variable assignments, or variable instantiations.
world
Earthquake
Burglary
Alarm
ω1
true
true
true
ω2
true
true
false
ω3
true
false
true
ω4
true
false
false
ω5
false
true
true
ω6
false
true
false
ω7
false
false
true
ω8
false
false
false
Earthquake. We will use the notation ω |= α to mean that sentence α is true at world ω.
We will also say in this case that world ω satisﬁes (or entails) sentence α.
The set of worlds that satisfy a sentence α is called the models of α and is denoted by
Mods(α)
def= {ω : ω |= α}.
Hence, every sentence α can be viewed as representing a set of worlds Mods(α), which is
called the event denoted by α. We will use the terms “sentence” and “event” interchange-
ably.
Using the deﬁnition of satisfaction (|=), it is not difﬁcult to prove the following prop-
erties:
r Mods(α ∧β) = Mods(α) ∩Mods(β).
r Mods(α ∨β) = Mods(α) ∪Mods(β).
r Mods(¬α) = Mods(α).
The following are some example sentences and their truth at worlds in Table 2.1:
r Earthquake is true at worlds ω1, . . . , ω4:
Mods(Earthquake) = {ω1, . . . , ω4}.
r ¬Earthquake is true at worlds ω5, . . . , ω8:
Mods(¬Earthquake) = Mods(Earthquake).
r ¬Burglary is true at worlds ω3, ω4, ω7, ω8.
r Alarm is true at worlds ω1, ω3, ω5, ω7.
r ¬(Earthquake ∨Burglary) is true at worlds ω7, ω8:
Mods(¬(Earthquake ∨Burglary) = Mods(Earthquake) ∪Mods(Burglary).
r ¬(Earthquake ∨Burglary) ∨Alarm is true at worlds ω1, ω3, ω5, ω7, ω8.
r (Earthquake ∨Burglary) =⇒Alarm is true at worlds ω1, ω3, ω5, ω7, ω8.
r ¬Burglary ∧Burglary is not true at any world.
2.3.2 Logical properties
We are now ready to deﬁne the most central logical property of sentences: consistency.
Speciﬁcally, we say that sentence α is consistent if and only if there is at least one

2.3 SEMANTICS OF PROPOSITIONAL SENTENCES
17
world ω at which α is true, Mods(α) ̸= ∅. Otherwise, the sentence α is inconsistent,
Mods(α) = ∅. It is also common to use the terms satisﬁable/unsatisﬁable instead of
consistent/inconsistent, respectively. The property of satisﬁability is quite important since
many other logical notions can be reduced to satisﬁability. The symbol false is often used
to denote a sentence that is unsatisﬁable. We have also used false to denote one of the
values that propositional variables can assume. The symbol false is therefore overloaded
in propositional logic.
We now turn to another logical property: validity. Speciﬁcally, we say that sentence α
is valid if and only if it is true at every world, Mods(α) = , where  is the set of all
worlds. If a sentence α is not valid, Mods(α) ̸= , one can identify a world ω at which α
is false. The symbol true is often used to denote a sentence that is valid.2 Moreover, it is
common to write |= α when the sentence α is valid.
2.3.3 Logical relationships
A logical property applies to a single sentence, while a logical relationship applies to
two or more sentences. We now deﬁne a few logical relationships among propositional
sentences:
r Sentences α and β are equivalent iff they are true at the same set of worlds: Mods(α) =
Mods(β) (i.e., they denote the same event).
r Sentences α and β are mutually exclusive iff they are never true at the same world:
Mods(α) ∩Mods(β) = ∅.3
r Sentences α and β are exhaustive iff each world satisﬁes at least one of the sentences:
Mods(α) ∪Mods(β) = .4
r Sentence α implies sentence β iff β is true whenever α is true: Mods(α) ⊆Mods(β).
We have previously used the symbol |= to denote the satisﬁability relationship between
a world and a sentence. Speciﬁcally, we wrote ω |= α to indicate that world ω satisﬁes
sentence α. This symbol is also used to indicate implication between sentences, where
we write α |= β to say that sentence α implies sentence β. We also say in this case that α
entails β.
2.3.4 Equivalences and reductions
We now consider some equivalences between propositional sentences that can be quite
useful when working with propositional logic. The equivalences are given in Table 2.2
and are actually between schemas, which are templates that can generate a large number
of speciﬁc sentences. For example, α =⇒β is a schema and generates instances such as
¬A =⇒(B ∨¬C), where α is replaced by ¬A and β is replaced by (B ∨¬C).
2 Again, we are overloading the symbol true since it also denotes one of the values that a propositional variable can
assume.
3 This can be generalized to an arbitrary number of sentences as follows: Sentences α1, . . . , αn are mutually exclusive
iff Mods(αi) ∩Mods(αj) = ∅for i ̸= j.
4 This can be generalized to an arbitrary number of sentences as follows: Sentences α1, . . . , αn are exhaustive iff
Mods(α1) ∪. . . ∪Mods(αn) = .

18
PROPOSITIONAL LOGIC
Table 2.2: Some equivalences among sentence schemas.
Schema
Equivalent Schema
Name
¬true
false
¬false
true
false ∧β
false
α ∧true
α
false ∨β
β
α ∨true
true
¬¬α
α
double negation
¬(α ∧β)
¬α ∨¬β
de Morgan
¬(α ∨β)
¬α ∧¬β
de Morgan
α ∨(β ∧γ )
(α ∨β) ∧(α ∨γ )
distribution
α ∧(β ∨γ )
(α ∧β) ∨(α ∧γ )
distribution
α =⇒β
¬β =⇒¬α
contraposition
α =⇒β
¬α ∨β
deﬁnition of =⇒
α ⇐⇒β
(α =⇒β) ∧(β =⇒α)
deﬁnition of ⇐⇒
Table 2.3: Some reductions between logical relationships
and logical properties.
Relationship
Property
α implies β
α ∧¬β is unsatisﬁable
α implies β
α =⇒β is valid
α and β are equivalent
α ⇐⇒β is valid
α and β are mutually exclusive
α ∧β is unsatisﬁable
α and β are exhaustive
α ∨β is valid
Table 2.4: Possible worlds according to the sentence
(Earthquake ∨Burglary) =⇒Alarm.
world
Earthquake
Burglary
Alarm
Possible?
ω1
true
true
true
yes
ω2
true
true
false
no
ω3
true
false
true
yes
ω4
true
false
false
no
ω5
false
true
true
yes
ω6
false
true
false
no
ω7
false
false
true
yes
ω8
false
false
false
yes
One can also state a number of reductions between logical properties and relationships,
some of which are shown in Table 2.3. Speciﬁcally, this table shows how the relationships
of implication, equivalence, mutual exclusiveness, and exhaustiveness can all be deﬁned
in terms of satisﬁability and validity.
2.4 The monotonicity of logical reasoning
Consider the earthquake-burglary-alarm example that we introduced previously, which
has the eight worlds depicted in Table 2.4. Suppose now that someone communicates to
us the following sentence:
α :
(Earthquake ∨Burglary) =⇒Alarm.

2.5 MULTIVALUED VARIABLES
19
∆
α
∆
α
∆
α
(a)  |= α
(b)  |= ¬α
(c)  ̸|= α and  ̸|= ¬α
Figure 2.2: Possible relationships between a knowledge base  and sentence α.
By accepting α, we are considering some of these eight worlds as impossible. In particular,
any world that does not satisfy the sentence α is ruled out. Therefore, our state of belief
can now be characterized by the set of worlds,
Mods(α) = {ω1, ω3, ω5, ω7, ω8}.
This is depicted in Table 2.4, which rules out any world outside Mods(α).
Suppose now that we also learn
β :
Earthquake =⇒Burglary,
for which Mods(β) = {ω1, ω2, ω5, ω6, ω7, ω8}. Our state of belief is now characterized
by the following worlds:
Mods(α ∧β) = Mods(α) ∩Mods(β) = {ω1, ω5, ω7, ω8}.
Hence, learning the new information β had the effect of ruling out world ω3 in addition
to those worlds ruled out by α.
Note that if α implies some sentence γ , then Mods(α) ⊆Mods(γ ) by deﬁnition of
implication. Since Mods(α ∧β) ⊆Mods(α), we must also have Mods(α ∧β) ⊆Mods(γ )
and, hence, α ∧β must also imply γ . This is precisely the property of monotonicity in
propositional logic as it shows that the belief in γ cannot be given up as a result of learning
some new information β. In other words, if α implies γ , then α ∧β will imply γ as well.
Note that a propositional knowledge base  can stand in only one of three possible
relationships with a sentence α:
r  implies α (α is believed).
r  implies the negation of α (¬α is believed).
r  neither implies α nor implies its negation.
This classiﬁcation of sentences, which can be visualized by examining Figure 2.2, is a
consequence of the binary classiﬁcation imposed by the knowledge base  on worlds, that
is, a world is either possible or impossible depending on whether it satisﬁes or contradicts
. In Chapter 3, we will see that degrees of belief can be used to impose a more reﬁned
classiﬁcation on worlds, leading to a more reﬁned classiﬁcation of sentences. This will
be the basis for a framework that allows one to represent and reason about uncertain
beliefs.
2.5 Multivalued variables
Propositional variables are binary as they assume one of two values, true or false. However,
these values are implicit in the syntax of propositional logic, as we write X to mean

20
PROPOSITIONAL LOGIC
Table 2.5: A set of worlds over propositional and multivalued
variables. Each world is also called a variable instantiation.
world
Earthquake
Burglary
Alarm
ω1
true
true
high
ω2
true
true
low
ω3
true
true
off
ω4
true
false
high
ω5
true
false
low
ω6
true
false
off
ω7
false
true
high
ω8
false
true
low
ω9
false
true
off
ω10
false
false
high
ω11
false
false
low
ω12
false
false
off
X=true and ¬X to mean X=false. One can generalize propositional logic to allow for
multivalued variables. For example, suppose that we have an alarm that triggers either high
or low. We may then decide to treat Alarm as a variable with three values, low, high, and off.
With multivalued variables, one would need to explicate the values assigned to variables
instead of keeping them implicit. Hence, we may write Burglary =⇒Alarm=high. Note
here that we kept the value of the propositional variable Burglary implicit but we could
explicate it as well, writing Burglary=true =⇒Alarm=high.
Sentences in the generalized propositional logic can be formed according to the fol-
lowing rules:
r Every propositional variable is a sentence.
r V = v is a sentence, where V is a variable and v is one of its values.
r If α and β are sentences, then ¬α, α ∧β, and α ∨β are also sentences.
The semantics of the generalized logic can be given in a fashion similar to standard
propositional logic, given that we extend the notion of a world to be an assignment of
values to variables (propositional and multivalued). Table 2.5 depicts a set of worlds for
our running example, assuming that Alarm is a multivalued variable.
The notion of truth at a world can be deﬁned similar to propositional logic. For example,
the sentence ¬Earthquake ∧¬Burglary =⇒Alarm=off is satisﬁed by worlds ω1, . . . ,
ω9, ω12; hence, only worlds ω10 and ω11 are ruled out by this sentence. The deﬁni-
tion of logical properties, such as consistency and validity, and logical relationships,
such as implication and equivalence, can all be developed as in standard propositional
logic.
2.6 Variable instantiations and related notations
One of the central notions we will appeal to throughout this book is the variable in-
stantiation. In particular, an instantiation of variables, say, A, B, C is a propositional
sentence of the form (A=a) ∧(B =b) ∧(C =c), where a, b, and c are values of vari-
ables A, B, C, respectively. Given the extent to which variable instantiations will be used,
we will adopt a simpler notation for denoting them. In particular, we will use a, b, c instead
of (A=a) ∧(B =b) ∧(C =c). More generally, we will replace the conjoin operator (∧)
by a comma (,) and write α, β instead of α ∧β. We will also ﬁnd it useful to introduce a

2.7 LOGICAL FORMS
21
trivial instantiation, an instantiation of an empty set of variables. The trivial instantiation
corresponds to a valid sentence and will be denoted by ⊤.
We will consistently denote variables by upper-case letters (A), their values by lower-
case letters (a), and their cardinalities (number of values) by |A|. Moreover, sets of
variables will be denoted by bold-face upper-case letters (A), their instantiations by bold-
face lower-case letters (a), and their number of instantiations by A#. Suppose now that X
and Y are two sets of variables and let x and y be their corresponding instantiations. State-
ments such as ¬x, x ∨y, and x =⇒y are therefore legitimate sentences in propositional
logic.
For a propositional variable A with values true and false, we may use a to denote
A=true and ¯a to denote A=false. Therefore, A, A=true, and a are all equivalent
sentences. Similarly, ¬A, A=false, and ¯a are all equivalent sentences.
Finally, we will use x ∼y to mean that instantiations x and y are compatible, that is,
they agree on the values of all their common variables. For example, instantiations a, b, ¯c
and b, ¯c, ¯d are compatible. On the other hand, instantiations a, b, ¯c and b, c, ¯d are not
compatible as they disagree on the value of variable C.
2.7 Logical forms
Propositional logic will provide the basis for probability calculus in Chapter 3. It will
also be used extensively in Chapter 11, where we discuss the complexity of probabilistic
inference, and in Chapters 12 and 13, where we discuss advanced inference algorithms.
Our use of propositional logic in Chapters 11 to 13 will rely on certain syntactic forms
and some corresponding operations that we discuss in this section. One may therefore
skip this section until these chapters are approached.
A propositional literal is either a propositional variable X, called a positive literal,
or the negation of a propositional variable ¬X, called a negative literal. A clause is a
disjunction of literals, such as ¬A ∨B ∨¬C.5 A propositional sentence is in conjunctive
normal form (CNF) if it is a conjunction of clauses, such as:
(¬A ∨B ∨¬C) ∧(A ∨¬B) ∧(C ∨¬D).
A unit clause is a clause that contains a single literal. The following CNF contains two
unit clauses:
(¬A ∨B ∨¬C) ∧(¬B) ∧(C ∨¬D) ∧(D).
A term is a conjunction of literals, such as A ∧¬B ∧C.6 A propositional sentence is
in disjunctive normal form (DNF) if it is a disjunction of terms, such as:
(A ∧¬B ∧C) ∨(¬A ∧B) ∨(¬C ∧D).
Propositional sentences can also be represented using circuits, as shown in Figure 2.3.
This circuit has a number of inputs that are labeled with literals (i.e., variables or their
5 A clause is usually written as an implication. For example, the clause ¬A ∨B ∨¬C can be written in any of the
following equivalent forms: A ∧¬B =⇒¬C, A ∧C =⇒B, or ¬B ∧C =⇒¬A.
6 A term corresponds to a variable instantiation, as deﬁned previously.

22
PROPOSITIONAL LOGIC
¬A
B
¬ B
A
C
¬ D
D
¬ C
and
and
and
and
and
and
and
or
or
or
or
and
and
and
or
(a) Decomposability
¬A
B
¬ B
A
C
¬ D
D
¬ C
and
and
and
and
and
and
and
and
or
or
or
or
and
and
or
(b) Determinism
Figure 2.3: A circuit representation of a propositional sentence. The circuit inputs are labeled with literals
¬A, B, . . . , and its nodes are restricted to conjunctions (and-gates) and disjunctions (or-gates).
negations).7 Moreover, it has only two types of nodes that represent conjunctions (and-
gates) or disjunctions (or-gates). Under these restrictions, a circuit is said to be in negation
normal form (NNF). An NNF circuit can satisfy one or more of the following properties.
Decomposability. We will say that an NNF circuit is decomposable if each of its
and-nodes satisﬁes the following property: For each pair of children C1 and C2 of the
and-node, the sentences represented by C1 and C2 cannot share variables. Figure 2.3(a)
highlights two children of an and-node and the sentences they represent. The child on the
left represents the sentence (¬A ∧B) ∨(A ∧¬B), and the one on the right represents
(C ∧D) ∨(¬C ∧¬D). The two sentences do not share any variables and, hence, the
and-node is decomposable.
Determinism. We will say that an NNF circuit is deterministic if each of its or-nodes
satisﬁes the following property: For each pair of children C1 and C2 of the or-node, the
sentences represented by C1 and C2 must be mutually exclusive. Figure 2.3(b) highlights
two children of an or-node. The child on the left represents the sentence ¬A ∧B, and
the one on the right represents A ∧¬B. The two sentences are mutually exclusive and,
hence, the or-node is deterministic.
Smoothness. We will say that an NNF circuit is smooth if each of its or-nodes
satisﬁes the following property: For each pair of children C1 and C2 of the or-node,
7 Inputs can also be labeled with the constants true/false.

2.7 LOGICAL FORMS
23
the sentences represented by C1 and C2 must mention the same set of variables. Fig-
ure 2.3(b) highlights two children of an or-node that represent the sentences ¬A ∧B and
A ∧¬B. The two sentences mention the same set of variables and, hence, the or-node is
smooth.
The NNF circuit in Figure 2.3 is decomposable, deterministic, and smooth since all its
and-nodes are decomposable and all its or-nodes are deterministic and smooth.
2.7.1 Conditioning a propositional sentence
Conditioning a propositional sentence  on variable X, denoted |X, is a process of
replacing every occurrence of variable X by true. Similarly, |¬X results from replacing
every occurrence of variable X by false. For example, if
 = (¬A ∨B ∨¬C) ∧(A ∨¬B) ∧(C ∨¬D),
then
|A = (¬true ∨B ∨¬C) ∧(true ∨¬B) ∧(C ∨¬D).
Simplifying and using the equivalences in Table 2.2, we get
|A = (B ∨¬C) ∧(C ∨¬D).
In general, conditioning a CNF  on X and simplifying has the effect of removing every
clause that contains the positive literal X from the CNF and removing the negative literal
¬X from all other clauses. Similarly, when we condition on ¬X, we remove every clause
that contains ¬X from the CNF and remove the positive literal X from all other clauses.
For example,
|¬A = (¬false ∨B ∨¬C) ∧(false ∨¬B) ∧(C ∨¬D)
= (¬B) ∧(C ∨¬D).
2.7.2 Unit resolution
Unit resolution is a process by which a CNF is simpliﬁed by iteratively applying the
following. If the CNF contains a unit clause X, every other occurrence of X is replaced by
true and the CNF is simpliﬁed. Similarly, if the CNF contains a unit clause ¬X, every other
occurrence of X is replaced by false and the CNF is simpliﬁed. Consider the following
CNF:
(¬A ∨B ∨¬C) ∧(¬B) ∧(C ∨¬D) ∧(D).
If we replace the other occurrences of B by false and the other occurrences of D by true,
we get
(¬A ∨false ∨¬C) ∧(¬B) ∧(C ∨¬true) ∧(D).
Simplifying, we get
(¬A ∨¬C) ∧(¬B) ∧(C) ∧(D).

24
PROPOSITIONAL LOGIC
We now have another unit clause C. Replacing the other occurrences of C by true and
simplifying, we now get
(¬A) ∧(¬B) ∧(C) ∧(D).
Unit resolution can be viewed as an inference rule as it allowed us to infer ¬A and C in
this example. It is known that unit resolution can be applied to a CNF in time linear in the
CNF size.
2.7.3 Converting propositional sentences to CNF
One can convert any propositional sentence into a CNF through a systematic three-step
process:
1. Remove all logical connectives except for conjunction, disjunction, and negation. For ex-
ample, α =⇒β should be transformed into ¬α ∨β, and similarly for other connectives.
2. Push negations inside the sentence until they only appear next to propositional variables.
This is done by repeated application of the following transformations:
Step 1: ¬¬α is transformed into α.
Step 2: ¬(α ∨β) is transformed into ¬α ∧¬β.
Step 3: ¬(α ∧β) is transformed into ¬α ∨¬β.
3. Distribute disjunctions over conjunctions by repeated application of the following trans-
formation: α ∨(β ∧γ ) is transformed to (α ∨β) ∧(α ∨γ ).
For example, to convert the sentence (A ∨B) =⇒C into CNF, we go through the
following steps:
Step 1:
¬(A ∨B) ∨C.
Step 2:
(¬A ∧¬B) ∨C.
Step 3:
(¬A ∨C) ∧(¬B ∨C).
For another example, converting ¬(A ∨B =⇒C) leads to the following steps:
Step 1:
¬(¬(A ∨B) ∨C).
Step 2:
(A ∨B) ∧¬C.
Step 3:
(A ∨B) ∧¬C.
Although this conversion process is guaranteed to yield a CNF, the result can be
quite large. Speciﬁcally, it is possible that the size of the given sentence is linear in the
number of propositional variables yet the size of the resulting CNF is exponential in that
number.
Bibliographic remarks
For introductory textbooks that cover propositional logic, see Genesereth and Nilsson
[1987] and Russell and Norvig [2003]. For a discussion of logical forms, including NNF
circuits, see Darwiche and Marquis [2002]. A state-of-the-art compiler for converting
CNF to NNF circuits is discussed in Darwiche [2004] and is available for download at
http://reasoning.cs.ucla.edu/c2d/.

2.8 EXERCISES
25
2.8 Exercises
2.1. Show that the following sentences are consistent by identifying a world that satisﬁes each
sentence:
(a) (A =⇒B) ∧(A =⇒¬B).
(b) (A ∨B) =⇒(¬A ∧¬B).
2.2. Which of the following sentences are valid? If a sentence is not valid, identify a world that
does not satisfy the sentence.
(a) (A ∧(A =⇒B)) =⇒B.
(b) (A ∧B) ∨(A ∧¬B).
(c) (A =⇒B) =⇒(¬B =⇒¬A).
2.3. Which of the following pairs of sentences are equivalent? If a pair of sentences is not equiv-
alent, identify a world at which they disagree (one of them holds but the other does not).
(a) A =⇒B and B =⇒A.
(b) (A =⇒B) ∧(A =⇒¬B) and ¬A.
(c) ¬A =⇒¬B and (A ∨¬B ∨C) ∧(A ∨¬B ∨¬C).
2.4. For each of the following pairs of sentences, decide whether the ﬁrst sentence implies the
second. If the implication does not hold, identify a world at which the ﬁrst sentence is true but
the second is not.
(a) (A =⇒B) ∧¬B and A.
(b) (A ∨¬B) ∧B and A.
(c) (A ∨B) ∧(A ∨¬B) and A.
2.5. Which of the following pairs of sentences are mutually exclusive? Which are exhaustive? If a
pair of sentences is not mutually exclusive, identify a world at which they both hold. If a pair
of sentences is not exhaustive, identify a world at which neither holds.
(a) A ∨B and ¬A ∨¬B.
(b) A ∨B and ¬A ∧¬B.
(c) A and (¬A ∨B) ∧(¬A ∨¬B).
2.6. Prove that α |= β iff α ∧¬β is inconsistent. This is known as the Refutation Theorem.
2.7. Prove that α |= β iff α =⇒β is valid. This is known as the Deduction Theorem.
2.8. Prove that if α |= β, then α ∧β is equivalent to α.
2.9. Prove that if α |= β, then α ∨β is equivalent to β.
2.10. Convert the following sentences into CNF:
(a) P =⇒(Q =⇒R).
(b) ¬((P =⇒Q) ∧(R =⇒S)).
2.11. Let  be an NNF circuit that satisﬁes decomposability and determinism. Show how one can
augment the circuit  with additional nodes so it also satisﬁes smoothness. What is the time
and space complexity of your algorithm for ensuring smoothness?
2.12. Let  be an NNF circuit that satisﬁes decomposability, is equivalent to CNF , and does not
contain false. Suppose that every model of  sets the same number of variables to true (we
say in this case that all models of  have the same cardinality). Show that circuit  must be
smooth.
2.13. Let  be an NNF circuit that satisﬁes decomposability, determinism, and smoothness. Con-
sider the following procedure for generating a subcircuit m of circuit :
r
Assign an integer to each node in circuit  as follows: An input node is assigned 0 if
labeled with true or a positive literal, ∞if labeled with false, and 1 if labeled with a

26
PROPOSITIONAL LOGIC
negative literal. An or-node is assigned the minimum of integers assigned to its children,
and an and-node is assigned the sum of integers assigned to its children.
r
Obtain m from  by deleting every edge that extends from an or-node N to one of its
children C, where N and C have different integers assigned to them.
Show that the models of m are the minimum-cardinality models of , where the cardinality
of a model is deﬁned as the number of variables it sets to false.

3
Probability Calculus
We introduce probability calculus in this chapter as a tool for representing and reasoning
with degrees of belief.
3.1 Introduction
We provide in this chapter a framework for representing and reasoning with uncertain
beliefs. According to this framework, each event is assigned a degree of belief which is
interpreted as a probability that quantiﬁes the belief in that event. Our focus in this chapter
is on the semantics of degrees of belief, where we discuss their properties and the methods
for revising them in light of new evidence. Computational and practical considerations
relating to degrees of belief are discussed at length in future chapters.
We start in Section 3.2 by introducing degrees of belief, their basic properties, and
the way they can be used to quantify uncertainty. We discuss the updating of degrees of
belief in Section 3.3, where we show how they can increase or decrease depending on the
new evidence made available. We then turn to the notion of independence in Section 3.4,
which will be fundamental when reasoning about uncertain beliefs. The properties of
degrees of belief are studied further in Section 3.5, where we introduce some of the key
laws for manipulating them. We ﬁnally treat the subject of soft evidence in Sections 3.6
and 3.7, where we provide some tools for updating degrees of belief in light of uncertain
information.
3.2 Degrees of belief
We have seen in Chapter 2 that a propositional knowledge base  classiﬁes sentences
into one of three categories: sentences that are implied by , sentences whose negations
are implied by , and all other sentences (see Figure 2.2). This coarse classiﬁcation of
sentences is a consequence of the binary classiﬁcation imposed by the knowledge base
 on worlds, that is, a world is either possible or impossible depending on whether it
satisﬁes or contradicts .
One can obtain a much ﬁner classiﬁcation of sentences through a ﬁner classiﬁcation
of worlds. In particular, we can assign a degree of belief or probability in [0, 1] to each
world ω and denote it by Pr(ω). The belief in, or probability of, a sentence α can then be
deﬁned as
Pr(α)
def=

ω|=α
Pr(ω),
(3.1)
which is the sum of probabilities assigned to worlds at which α is true.
Consider now Table 3.1, which lists a set of worlds and their corresponding degrees of
beliefs. Table 3.1 is known as a state of belief or a joint probability distribution. We will
27

28
PROBABILITY CALCULUS
Table 3.1: A state of belief, also known as a joint
probability distribution.
world
Earthquake
Burglary
Alarm
Pr(.)
ω1
true
true
true
.0190
ω2
true
true
false
.0010
ω3
true
false
true
.0560
ω4
true
false
false
.0240
ω5
false
true
true
.1620
ω6
false
true
false
.0180
ω7
false
false
true
.0072
ω8
false
false
false
.7128
require that the degrees of belief assigned to all worlds add up to 1: 
w Pr(w) = 1. This
is a normalization convention that makes it possible to directly compare the degrees of
belief held by different states. Based on Table 3.1, we then have the following beliefs:
Pr(Earthquake)
=
Pr(ω1) + Pr(ω2) + Pr(ω3) + Pr(ω4) = .1
Pr(Burglary)
=
.2
Pr(¬Burglary)
=
.8
Pr(Alarm)
=
.2442
Note that the joint probability distribution is usually too large to allow a direct repre-
sentation as given in Table 3.1. For example, if we have twenty variables and each has two
values, the table will have 1, 048, 576 entries, and if we have forty variables, the table will
have 1, 099, 511, 627, 776 entries. This difﬁculty will not be addressed in this chapter as
the focus here is only on the semantics of degrees of belief. Chapter 4 will deal with these
issues directly by proposing the Bayesian network as a tool for efﬁciently representing
the joint probability distribution.
3.2.1 Properties of beliefs
We will now establish some properties of degrees of belief (henceforth, beliefs). First, a
bound on the belief in any sentence:
0 ≤Pr(α) ≤1
for any sentence α.
(3.2)
This follows since every degree of belief must be in [0, 1], leading to 0 ≤Pr(α), and since
the beliefs assigned to all worlds must add up to 1, leading to Pr(α) ≤1. The second
property is a baseline for inconsistent sentences:
Pr(α) = 0
when α is inconsistent.
(3.3)
This follows since there are no worlds that satisfy an inconsistent sentence α. The third
property is a baseline for valid sentences:
Pr(α) = 1
when α is valid.
(3.4)
This follows since a valid sentence α is satisﬁed by every world.
The following property allows one to compute the belief in a sentence given the belief
in its negation:
Pr(α) + Pr(¬α) = 1.
(3.5)

3.2 DEGREES OF BELIEF
29
α
α
¬
Figure 3.1: The worlds that satisfy α and those that satisfy ¬α form a partition of the set of all worlds.
α
β
Figure 3.2: The worlds that satisfy α ∨β can be partitioned into three sets, those satisfying α ∧¬β,
¬α ∧β, and α ∧β.
This follows because every world must either satisfy α or satisfy ¬α but cannot satisfy
both (see Figure 3.1). Consider Table 3.1 for an example and let α : Burglary. We then have
Pr(Burglary) = Pr(ω1) + Pr(ω2) + Pr(ω5) + Pr(ω6) = .2
Pr(¬Burglary) = Pr(ω3) + Pr(ω4) + Pr(ω7) + Pr(ω8) = .8
The next property allows us to compute the belief in a disjunction:
Pr(α ∨β) = Pr(α) + Pr(β) −Pr(α ∧β).
(3.6)
This identity is best seen by examining Figure 3.2. If we simply add Pr(α) and Pr(β),
we end up summing the beliefs in worlds that satisfy α ∧β twice. Hence, by subtracting
Pr(α ∧β) we end up accounting for the belief in every world that satisﬁes α ∨β only once.
Consider Table 3.1 for an example and let α : Earthquake and β : Burglary. We then
have
Pr(Earthquake) = Pr(ω1) + Pr(ω2) + Pr(ω3) + Pr(ω4) = .1
Pr(Burglary) = Pr(ω1) + Pr(ω2) + Pr(ω5) + Pr(ω6) = .2
Pr(Earthquake ∧Burglary) = Pr(ω1) + Pr(ω2) = .02
Pr(Earthquake ∨Burglary) = .1 + .2 −.02 = .28
The belief in a disjunction α ∨β can sometimes be computed directly from the belief
in α and the belief in β:
Pr(α ∨β) = Pr(α) + Pr(β)
when α and β are mutually exclusive.
In this case, there is no world that satisﬁes both α and β. Hence, α ∧β is inconsistent and
Pr(α ∧β) = 0.
3.2.2 Quantifying uncertainty
Consider the beliefs associated with variables in the previous example:
Earthquake
Burglary
Alarm
true
.1
.2
.2442
false
.9
.8
.7558

30
PROBABILITY CALCULUS
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 p
 ENT( X )
Figure 3.3: The entropy for a binary variable X with Pr(X) = p.
Intuitively, these beliefs seem most certain about whether an earthquake has occurred and
least certain about whether an alarm has triggered. One can formally quantify uncertainty
about a variable X using the notion of entropy:
ENT(X)
def=
−

x
Pr(x) log2 Pr(x),
where 0 log 0 = 0 by convention. The following values are the entropies associated with
the prior variables:
Earthquake
Burglary
Alarm
true
.1
.2
.2442
false
.9
.8
.7558
ENT(.)
.469
.722
.802
Figure 3.3 plots the entropy for a binary variable X and varying values of p = Pr(X).
Entropy is non-negative. When p = 0 or p = 1, the entropy of X is zero and at a minimum,
indicating no uncertainty about the value of X. When p = 1
2, we have Pr(X) = Pr(¬X)
and the entropy is at a maximum, indicating complete uncertainty about the value of
variable X (see Appendix B).
3.3 Updating beliefs
Consider again the state of belief in Table 3.1 and suppose that we now know that the
alarm has triggered, Alarm = true. This piece of information is not compatible with the
state of belief that ascribes a belief of .2442 to the Alarm being true. Therefore, we now
need to update the state of belief to accommodate this new piece of information, which
we will refer to as evidence. More generally, evidence will be represented by an arbitrary
event, say, β and our goal is to update the state of belief Pr(.) into a new state of belief,
which we will denote by Pr(.|β).
Given that β is known for sure, we expect the new state of belief Pr(.|β) to assign a
belief of 1 to β: Pr(β|β) = 1. This immediately implies that Pr(¬β|β) = 0 and, hence,
every world ω that satisﬁes ¬β must be assigned the belief 0:
Pr(ω|β) = 0
for all ω |= ¬β.
(3.7)

3.3 UPDATING BELIEFS
31
Table 3.2: A state of belief and the result of conditioning it on
evidence Alarm.
world
Earthquake
Burglary
Alarm
Pr(.)
Pr(.|Alarm)
ω1
true
true
true
.0190
.0190/.2442
ω2
true
true
false
.0010
0
ω3
true
false
true
.0560
.0560/.2442
ω4
true
false
false
.0240
0
ω5
false
true
true
.1620
.1620/.2442
ω6
false
true
false
.0180
0
ω7
false
false
true
.0072
.0072/.2442
ω8
false
false
false
.7128
0
To completely deﬁne the new state of belief Pr(.|β), all we have to do then is deﬁne the
new belief in every world ω that satisﬁes β. We already know that the sum of all such
beliefs must be 1:

ω|=β
Pr(ω|β) = 1.
(3.8)
But this leaves us with many options for Pr(ω|β) when world ω satisﬁes β. Since
evidence β tells us nothing about worlds that satisfy β, it is then reasonable to perturb our
beliefs in such worlds as little as possible. To this end, we will insist that worlds that have
zero probability will continue to have zero probability:
Pr(ω|β) = 0
for all ω where Pr(ω) = 0.
(3.9)
As for worlds that have a positive probability, we will insist that our relative beliefs in
these worlds stay the same:
Pr(ω)
Pr(ω′) = Pr(ω|β)
Pr(ω′|β)
for all ω, ω′ |= β, Pr(ω) > 0, Pr(ω′) > 0.
(3.10)
The constraints expressed by (3.8)–(3.10) leave us with only one option for the new
beliefs in worlds that satisfy the evidence β:
Pr(ω|β) = Pr(ω)
Pr(β)
for all ω |= β.
That is, the new beliefs in such worlds are just the result of normalizing our old beliefs,
with the normalization constant being our old belief in the evidence, Pr(β). Our new state
of belief is now completely deﬁned:
Pr(ω|β)
def=
⎧
⎨
⎩
0,
if ω |= ¬β
Pr(ω)
Pr(β)
if ω |= β.
(3.11)
The new state of belief Pr(.|β) is referred to as the result of conditioning the old state Pr
on evidence β.
Consider now the state of belief in Table 3.1 and suppose that the evidence is Alarm =
true. The result of conditioning this state of belief on this evidence is given in Table 3.2.
Let us now examine some of the changes in beliefs that are induced by this new evidence.
First, our belief in Burglary increases:
Pr(Burglary)
=
.2
Pr(Burglary|Alarm)
≈
.741 ↑

32
PROBABILITY CALCULUS
and so does our belief in Earthquake:
Pr(Earthquake)
=
.1
Pr(Earthquake|Alarm)
≈
.307 ↑
One can derive a simple closed form for the updated belief in an arbitrary sentence α
given evidence β without having to explicitly compute the belief Pr(ω|β) for every world
ω. The derivation is as follows:
Pr(α|β)
=

ω|=α
Pr(ω|β)
by (3.1)
=

ω|=α, ω|=β
Pr(ω|β) +

ω|=α, ω|=¬β
Pr(ω|β)
since ω satisﬁes β or ¬β but not both
=

ω|=α, ω|=β
Pr(ω|β)
by (3.11)
=

ω|=α∧β
Pr(ω|β)
by properties of |=
=

ω|=α∧β
Pr(ω)/Pr(β)
by (3.11)
=
1
Pr(β)

ω|=α∧β
Pr(ω)
= Pr(α ∧β)
Pr(β)
by (3.1).
The closed form,
Pr(α|β) = Pr(α ∧β)
Pr(β)
,
(3.12)
is known as Bayes conditioning. Note that the updated state of belief Pr(.|β) is deﬁned
only when Pr(β) ̸= 0. We will usually avoid stating this condition explicitly in the future
but it should be implicitly assumed.
To summarize, Bayes conditioning follows from the following commitments:
1. Worlds that contradict the evidence β will have zero probability.
2. Worlds that have zero probability continue to have zero probability.
3. Worlds that are consistent with evidence β and have positive probability will maintain
their relative beliefs.
Let us now use Bayes conditioning to further examine some of the belief dynamics
in our previous example. In particular, here is how some beliefs would change upon
accepting the evidence Earthquake:
Pr(Burglary)
=
.2
Pr(Burglary|Earthquake)
=
.2
Pr(Alarm)
=
.2442
Pr(Alarm|Earthquake)
≈
.75 ↑

3.3 UPDATING BELIEFS
33
That is, the belief in Burglary is not changed but the belief in Alarm increases. Here are
some more belief changes as a reaction to the evidence Burglary:
Pr(Alarm)
=
.2442
Pr(Alarm|Burglary)
≈
.905 ↑
Pr(Earthquake)
=
.1
Pr(Earthquake|Burglary)
=
.1
The belief in Alarm increases in this case but the belief in Earthquake stays the same.
The belief dynamics presented here are a property of the state of belief in Table 3.1 and
may not hold for other states of beliefs. For example, one can conceive of a reasonable state
of belief in which information about Earthquake would change the belief about Burglary
and vice versa. One of the central questions in building automated reasoning systems is
that of synthesizing states of beliefs that are faithful, that is, those that correspond to the
beliefs held by some human expert. The Bayesian network, which we introduce in the
following chapter, can be viewed as a modeling tool for synthesizing faithful states of
beliefs.
Let us look at one more example of belief change. We know that the belief in Burglary
increases when accepting the evidence Alarm. The question, however, is how would such
a belief further change upon obtaining more evidence? Here is what happens when we
get a conﬁrmation that an Earthquake took place:
Pr(Burglary|Alarm)
≈
.741
Pr(Burglary|Alarm ∧Earthquake)
≈
.253 ↓
That is, our belief in a Burglary decreases in this case as we now have an explanation
of Alarm. On the other hand, if we get a conﬁrmation that there was no Earthquake, our
belief in Burglary increases even further:
Pr(Burglary|Alarm)
≈
.741
Pr(Burglary|Alarm ∧¬Earthquake)
≈
.957 ↑
as this new evidence further establishes burglary as the explanation for the triggered alarm.
Some of the belief dynamics we have observed in the previous examples are not
accidental but are guaranteed by the method used to construct the state of belief in
Table 3.1. More details on these guarantees are given in Chapter 4 when we introduce
Bayesian networks.
One can deﬁne the conditional entropy of a variable X given another variable Y to
quantify the average uncertainty about the value of X after observing the value of Y:
ENT(X|Y)
def=

y
Pr(y)ENT(X|y),
where
ENT(X|y)
def=
−

x
Pr(x|y) log2 Pr(x|y).
One can show that the entropy never increases after conditioning:
ENT(X|Y) ≤ENT(X),

34
PROBABILITY CALCULUS
that is, on average, observing the value of Y reduces our uncertainty about X. However,
for a particular value y we may have ENT(X|y) > ENT(X). The following are some
entropies for the variable Burglary in our previous example:
Burglary
Burglary|Alarm=true
Burglary|Alarm=false
true
.2
.741
.025
false
.8
.259
.975
ENT(.)
.722
.825
.169
The prior entropy for this variable is ENT(Burglary) = .722. Its entropy is .825 after
observing Alarm=true (increased uncertainty), and .169 after observing Alarm=false
(decreased uncertainty). The conditional entropy of variable Burglary given variable Alarm
is then
ENT(Burglary|Alarm) = ENT(Burglary|Alarm=true)Pr(Alarm=true)
+ ENT(Burglary|Alarm=false)Pr(Alarm=false)
= .329,
indicating a decrease in the uncertainty about variable Burglary.
3.4 Independence
According to the state of belief in Table 3.1, the evidence Burglary does not change the
belief in Earthquake:
Pr(Earthquake)
=
.1
Pr(Earthquake|Burglary)
=
.1
Hence, we say in this case that the state of belief Pr ﬁnds the Earthquake event independent
of the Burglary event. More generally, we say that Pr ﬁnds event α independent of event
β iff
Pr(α|β) = Pr(α)
or Pr(β) = 0.
(3.13)
Note that the state of belief in Table 3.1 also ﬁnds Burglary independent of Earthquake:
Pr(Burglary)
=
.2
Pr(Burglary|Earthquake)
=
.2
It is indeed a general property that Pr must ﬁnd event α independent of event β if it
also ﬁnds β independent of α. Independence satisﬁes other interesting properties that we
explore in later chapters.
Independence provides a general condition under which the belief in a conjunction
α ∧β can be expressed in terms of the belief in α and that in β. Speciﬁcally, Pr ﬁnds α
independent of β iff
Pr(α ∧β) = Pr(α)Pr(β).
(3.14)
This equation is sometimes taken as the deﬁnition of independence, whereas (3.13) is
viewed as a consequence. We use (3.14) when we want to stress the symmetry between α
and β in the deﬁnition of independence.
It is important here to stress the difference between independence and logical disjoint-
ness (mutual exclusiveness), as it is common to mix up these two notions. Recall that

3.4 INDEPENDENCE
35
Table 3.3: A state of belief.
world
Temp
Sensor1
Sensor2
Pr(.)
ω1
normal
normal
normal
.576
ω2
normal
normal
extreme
.144
ω3
normal
extreme
normal
.064
ω4
normal
extreme
extreme
.016
ω5
extreme
normal
normal
.008
ω6
extreme
normal
extreme
.032
ω7
extreme
extreme
normal
.032
ω8
extreme
extreme
extreme
.128
two events α and β are logically disjoint (mutually exclusive) iff they do not share any
models: Mods(α) ∩Mods(β) = ∅, that is, they cannot hold together at the same world.
On the other hand, events α and β are independent iff Pr(α ∧β) = Pr(α)Pr(β). Note
that disjointness is an objective property of events, while independence is a property of
beliefs. Hence, two individuals with different beliefs may disagree on whether two events
are independent but they cannot disagree on their logical disjointness.1
3.4.1 Conditional independence
Independence is a dynamic notion. One may ﬁnd two events independent at some point
but then ﬁnd them dependent after obtaining some evidence. For example, we have seen
how the state of belief in Table 3.1 ﬁnds Burglary independent of Earthquake. However,
this state of belief ﬁnds these events dependent on each other after accepting the evidence
Alarm:
Pr(Burglary|Alarm)
≈
.741
Pr(Burglary|Alarm ∧Earthquake)
≈
.253
That is, Earthquake changes the belief in Burglary in the presence of Alarm. Intuitively, this
is to be expected since Earthquake and Burglary are competing explanations for Alarm, so
conﬁrming one of these explanations tends to reduce our belief in the second explanation.
Consider the state of belief in Table 3.3 for another example. Here we have three
variables. First, we have the variable Temp, which represents the state of temperature
as being either normal or extreme. We also have two sensors, Sensor1 and Sensor2,
which can detect these two states of temperature. The sensors are noisy and have different
reliabilities. According to this state of belief, we have the following initial beliefs:
Pr(Temp=normal) = .80
Pr(Sensor1=normal) = .76
Pr(Sensor2=normal) = .68
Suppose that we check the ﬁrst sensor and it is reading normal. Our belief in the second
sensor reading normal would then increase as expected:
Pr(Sensor2=normal|Sensor1=normal) ≈.768 ↑
1 It is possible however for one state of belief to assign a zero probability to the event α ∧β even though α and β are
not mutually exclusive on a logical basis.

36
PROBABILITY CALCULUS
Hence, our beliefs in these sensor readings are initially dependent. However, these beliefs
will become independent if we observe that the temperature is normal:
Pr(Sensor2=normal|Temp=normal) = .80
Pr(Sensor2=normal|Temp=normal, Sensor1=normal) = .80
Therefore, even though the sensor readings were initially dependent they become inde-
pendent once we know the state of temperature.
In general, independent events may become dependent given new evidence and, sim-
ilarly, dependent events may become independent given new evidence. This calls for the
following more general deﬁnition of independence. We say that state of belief Pr ﬁnds
event α conditionally independent of event β given event γ iff
Pr(α|β ∧γ ) = Pr(α|γ )
or Pr(β ∧γ ) = 0.
(3.15)
That is, in the presence of evidence γ the additional evidence β will not change the
belief in α. Conditional independence is also symmetric: α is conditionally independent
of β given γ iff β is conditionally independent of α given γ . This is best seen from the
following equation, which is equivalent to (3.15):
Pr(α ∧β|γ ) = Pr(α|γ )Pr(β|γ )
or Pr(γ ) = 0.
(3.16)
Equation (3.16) is sometimes used as the deﬁnition of conditional independence between α
and β given γ . We use (3.16) when we want to emphasize the symmetry of independence.
3.4.2 Variable independence
We will ﬁnd it useful to talk about independence between sets of variables. In particular,
let X, Y, and Z be three disjoint sets of variables. We will say that a state of belief Pr ﬁnds
X independent of Y given Z, denoted IPr(X, Z, Y), to mean that Pr ﬁnds x independent of
y given z for all instantiations x, y, and z.
Suppose for example that X = {A, B}, Y = {C}, and Z = {D, E}, where A, B, C, D,
and E are all propositional variables. The statement IPr(X, Z, Y) is then a compact notation
for a number of statements about independence:
A ∧B is independent of C given D ∧E.
A ∧¬B is independent of C given D ∧E.
...
¬A ∧¬B is independent of ¬C given ¬D ∧¬E.
That is, IPr(X, Z, Y) is a compact notation for 4 × 2 × 4 = 32 independence statements
of this form.
3.4.3 Mutual information
The notion of independence is a special case of a more general notion known as mutual
information, which quantiﬁes the impact of observing one variable on the uncertainty in
another:
MI(X; Y)
def=

x,y
Pr(x, y) log2
Pr(x, y)
Pr(x)Pr(y).

3.5 FURTHER PROPERTIES OF BELIEFS
37
Mutual information is non-negative and equal to zero if and only if variables X and Y are
independent. More generally, mutual information measures the extent to which observing
one variable will reduce the uncertainty in another:
MI(X; Y) = ENT(X) −ENT(X|Y)
= ENT(Y) −ENT(Y|X).
Conditional mutual information can also be deﬁned as follows:
MI(X; Y|Z)
def=

x,y,z
Pr(x, y, z) log2
Pr(x, y|z)
Pr(x|z)Pr(y|z),
leading to
MI(X; Y|Z) = ENT(X|Z) −ENT(X|Y, Z)
= ENT(Y|Z) −ENT(Y|X, Z).
Entropy and mutual information can be extended to sets of variables in the obvious
way. For example, entropy can be generalized to a set of variables X as follows:
ENT(X) = −

x
Pr(x) log2 Pr(x).
3.5 Further properties of beliefs
We will discuss in this section more properties of beliefs that are commonly used. We
start with the chain rule:
Pr(α1 ∧α2 ∧. . . ∧αn) = Pr(α1|α2 ∧. . . ∧αn)Pr(α2|α3 ∧. . . ∧αn) . . . Pr(αn).
This rule follows from a repeated application of Bayes conditioning (3.11). We will ﬁnd
a major use of the chain rule when discussing Bayesian networks in Chapter 4.
The next important property of beliefs is case analysis, also known as the law of total
probability:
Pr(α) =
n

i=1
Pr(α ∧βi),
(3.17)
where the events β1, . . . , βn are mutually exclusive and exhaustive.2 Case analysis holds
because the models of α ∧β1, . . . , α ∧βn form a partition of the models of α. Intuitively,
case analysis says that we can compute the belief in event α by adding up our beliefs in a
number of mutually exclusive cases, α ∧β1, . . . , α ∧βn, that cover the conditions under
which α holds.
Another version of case analysis is
Pr(α) =
n

i=1
Pr(α|βi)Pr(βi),
(3.18)
where the events β1, . . . , βn are mutually exclusive and exhaustive. This version is ob-
tained from the ﬁrst by applying Bayes conditioning. It calls for considering a number
of mutually exclusive and exhaustive cases, β1, . . . , βn, computing our belief in α under
2 That is, Mods(βj) ∩Mods(βk) = ∅for j ̸= k and n
i=1 Mods(βi) = , where  is the set of all worlds.

38
PROBABILITY CALCULUS
each of these cases, Pr(α|βi), and then summing these beliefs after applying the weight
of each case, Pr(βi).
Two simple and useful forms of case analysis are
Pr(α) = Pr(α ∧β) + Pr(α ∧¬β)
Pr(α) = Pr(α|β)Pr(β) + Pr(α|¬β)Pr(¬β).
These equations hold because β and ¬β are mutually exclusive and exhaustive. The main
value of case analysis is that in many situations, computing our beliefs in the cases is
easier than computing our beliefs in α. We see many examples of this phenomena in later
chapters.
The last property of beliefs we consider is known as Bayes rule or Bayes theorem:
Pr(α|β) = Pr(β|α)Pr(α)
Pr(β)
.
(3.19)
The classical usage of this rule is when event α is perceived to be a cause of event β –
for example, α is a disease and β is a symptom – and our goal is to assess our belief in
the cause given the effect. The belief in an effect given its cause, Pr(β|α), is usually more
readily available than the belief in a cause given one of its effects, Pr(α|β). Bayes theorem
allows us to compute the latter from the former.
To consider an example of Bayes rule, suppose that we have a patient who was just
tested for a particular disease and the test came out positive. We know that one in every
thousand people has this disease. We also know that the test is not reliable: it has a false
positive rate of 2% and a false negative rate of 5%. Our goal is then to assess our belief
in the patient having the disease given that the test came out positive. If we let the pro-
positional variable D stand for “the patient has the disease” and the propositional variable
T stand for “the test came out positive,” our goal is then to compute Pr(D|T).
From the given information, we know that
Pr(D) =
1
1,000
since one in every thousand has the disease – this is our prior belief in the patient having
the disease before we run any tests. Since the false positive rate of the test is 2%, we know
that
Pr(T|¬D) =
2
100
and by (3.5),
Pr(¬T|¬D) = 98
100.
Similarly, since the false negative rate of the test is 5%, we know that
Pr(¬T|D) =
5
100
and
Pr(T|D) = 95
100.

3.6 SOFT EVIDENCE
39
Using Bayes rule, we now have
Pr(D|T) =
95
100 ×
1
1,000
Pr(T)
.
The belief in the test coming out positive for an individual, Pr(T), is not readily available
but can be computed using case analysis:
Pr(T) = Pr(T|D)Pr(D) + Pr(T|¬D)Pr(¬D)
= 95
100 ×
1
1,000 +
2
100 × 999
1,000 =
2,093
100,000,
which leads to
Pr(D|T) =
95
2,093 ≈4.5%.
Another way to solve this problem is to construct the state of belief completely and
then use it to answer queries. This is feasible in this case because we have only two events
of interest, T and D, leading to only four worlds:
world
D
T
ω1
true
true
has disease, test positive
ω2
true
false
has disease, test negative
ω3
false
true
has no disease, test positive
ω4
false
false
has no disease, test negative
If we obtain the belief in each one of these worlds, we can then compute the belief in any
sentence mechanically using (3.1) and (3.12). To compute the beliefs in these worlds, we
use the chain rule:
Pr(ω1) = Pr(T ∧D) = Pr(T|D)Pr(D)
Pr(ω2) = Pr(¬T ∧D) = Pr(¬T|D)Pr(D)
Pr(ω3) = Pr(T ∧¬D) = Pr(T|¬D)Pr(¬D)
Pr(ω4) = Pr(¬T ∧¬D) = Pr(¬T|¬D)Pr(¬D).
All of these quantities are available directly from the problem statement, leading to the
following state of belief:
world
D
T
Pr(.)
ω1
true
true
95/100
×
1/1,000
= .00095
ω2
true
false
5/100
×
1/1,000
= .00005
ω3
false
true
2/100
×
999/1,000
= .01998
ω4
false
false
98/100
×
999/1,000
= .97902
3.6 Soft evidence
There are two types of evidence that one may encounter: hard evidence and soft evidence.
Hard evidence is information to the effect that some event has occurred, which is also the
type of evidence we have considered previously. Soft evidence, on the other hand, is not
conclusive: we may get an unreliable testimony that event β occurred, which may increase
our belief in β but not to the point where we would consider it certain. For example, our
neighbor who is known to have a hearing problem may call to tell us they heard the alarm

40
PROBABILITY CALCULUS
trigger in our home. Such a call may not be used to categorically conﬁrm the event Alarm
but can still increase our belief in Alarm to some new level.
One of the key issues relating to soft evidence is how to specify its strength. There are
two main methods for this, which we discuss next.
3.6.1 The “all things considered” method
One method for specifying soft evidence on event β is by stating the new belief in β after
the evidence has been accommodated. For example, we would say “after receiving my
neighbor’s call, my belief in the alarm triggering stands now at .85.” Formally, we are
specifying soft evidence as a constraint Pr′(β) = q, where Pr′ denotes the new state of
belief after accommodating the evidence and β is the event to which the evidence pertains.
This is sometimes known as the “all things considered” method since the new belief in
β depends not only on the strength of evidence but also on our initial beliefs that existed
before the evidence was obtained. That is, the statement Pr′(β) = q is not a statement
about the strength of evidence per se but about the result of its integration with our initial
beliefs.
Given this method of specifying evidence, computing the new state of belief Pr′ can
be done along the same principles we used for Bayes conditioning. In particular, suppose
that we obtain some soft evidence on event β that leads us to change our belief in β to q.
Since this evidence imposes the constraint Pr′(β) = q, it will also impose the additional
constraint Pr′(¬β) = 1 −q. Therefore, we know that we must change the beliefs in worlds
that satisfy β so these beliefs add up to q. We also know that we must change the beliefs
in worlds that satisfy ¬β so they add up to 1 −q. Again, if we insist on preserving the
relative beliefs in worlds that satisfy β and also on preserving the relative beliefs in worlds
that satisfy ¬β, we ﬁnd ourselves committed to the following deﬁnition of Pr′:
Pr′(ω)
def=
⎧
⎪⎪⎨
⎪⎪⎩
q
Pr(β)Pr(ω),
if ω |= β
1 −q
Pr(¬β)Pr(ω),
if ω |= ¬β.
(3.20)
That is, we effectively have to scale our beliefs in the worlds satisfying β using the
constant q/Pr(β) and similarly for the worlds satisfying ¬β. All we are doing here is
normalizing the beliefs in worlds that satisfy β and similarly for the worlds that satisfy
¬β so they add up to the desired quantities q and 1 −q, respectively.
There is also a useful closed form for the deﬁnition in (3.20), which can be derived
similarly to (3.12):
Pr′(α) = qPr(α|β) + (1 −q)Pr(α|¬β),
(3.21)
where Pr′ is the new state of belief after accommodating the soft evidence Pr′(β) = q.
This method of updating a state of belief in the face of soft evidence is known as Jeffrey’s
rule. Note that Bayes conditioning is a special case of Jeffrey’s rule when q = 1, which
is to be expected as they were both derived using the same principle.
Jeffrey’s rule has a simple generalization to the case where the evidence concerns a
set of mutually exclusive and exhaustive events, β1, . . . , βn, with the new beliefs in these

3.6 SOFT EVIDENCE
41
events being q1, . . . , qn, respectively. This soft evidence can be accommodated using the
following generalization of Jeffrey’s rule:
Pr′(α) =
n

i=1
qiPr(α|βi).
(3.22)
Consider the following example, due to Jeffrey. Assume that we are given a piece of
cloth C where its color can be one of green (cg), blue (cb), or violet (cv). We want to know
whether the next day the cloth will be sold (s) or not sold (¯s). Our original state of belief
is as follows:
worlds
S
C
Pr(.)
ω1
s
cg
.12
ω2
¯s
cg
.18
ω3
s
cb
.12
ω4
¯s
cb
.18
ω5
s
cv
.32
ω6
¯s
cv
.08
Therefore, our original belief in the cloth being sold is Pr(s) = .56. Moreover, our original
beliefs in the colors cg, cb, and cv are .3, .3, and .4, respectively. Assume that we now
inspect the cloth by candlelight and we conclude that our new beliefs in these colors
should be .7, .25, and .05, respectively. If we apply Jeffrey’s rule as given by (3.22), we
get
Pr′(s) = .7
.12
.3
	
+ .25
.12
.3
	
+ .05
.32
.4
	
= .42
The full new state of belief according to Jeffrey’s rule is
worlds
S
C
Pr′(.)
ω1
s
cg
.28 = .12 × .7/.3
ω2
¯s
cg
.42 = .18 × .7/.3
ω3
s
cb
.10 = .12 × .25/.3
ω4
¯s
cb
.15 = .18 × .25/.3
ω5
s
cv
.04 = .32 × .05/.4
ω6
¯s
cv
.01 = .08 × .05/.4
Note how the new belief in each world is simply a scaled version of the old belief with three
different scaling constants corresponding to the three events on which the soft evidence
bears.
3.6.2 The “nothing else considered” method
The second method for specifying soft evidence on event β is based on declaring the
strength of this evidence independently of currently held beliefs. In particular, let us
deﬁne the odds of event β as follows:
O(β)
def=
Pr(β)
Pr(¬β).
(3.23)
That is, an odds of 1 indicates that we believe β and ¬β equally, while an odds of 10
indicates that we believe β ten times more than we believe ¬β.

42
PROBABILITY CALCULUS
Given the notion of odds, we can specify soft evidence on event β by declaring the
relative change it induces on the odds of β, that is, by specifying the ratio
k = O′(β)
O(β) ,
where O′(β) is the odds of β after accommodating the evidence, Pr′(β)/Pr′(¬β). The ratio
k is known as the Bayes factor. Hence, a Bayes factor of 1 indicates neutral evidence and a
Bayes factor of 2 indicates evidence on β that is strong enough to double the odds of β. As
the Bayes factor tends to inﬁnity, the soft evidence tends toward hard evidence conﬁrming
β. As the factor tends to zero, the soft evidence tends toward hard evidence refuting β.
This method of specifying evidence is sometimes known as the “nothing else considered”
method as it is a statement about the strength of evidence without any reference to the
initial state of belief. This is shown formally in Section 3.6.4, where we show that a Bayes
factor can be compatible with any initial state of belief.3
Suppose that we obtain soft evidence on β whose strength is given by a Bayes factor of
k, and our goal is to compute the new state of belief Pr′ that results from accommodating
this evidence. If we are able to translate this evidence into a form that is accepted by
Jeffrey’s rule, then we can use that rule to compute Pr′. This turns out to be possible, as
we describe next. First, from the constraint k = O′(β)/O(β) we get
Pr′(β) =
kPr(β)
kPr(β) + Pr(¬β).
(3.24)
Hence, we can view this as a problem of updating the initial state of belief Pr using Jeffrey’s
rule and the soft evidence given previously. That is, what we have done is translate a
“nothing else considered” speciﬁcation of soft evidence – a constraint on O′(β)/O(β) –
into an “all things considered” speciﬁcation – a constraint on Pr′(β). Computing Pr′ using
Jeffrey’s rule as given by (3.21), and taking Pr′(β) = q as given by (3.24), we get
Pr′(α) = kPr(α ∧β) + Pr(α ∧¬β)
kPr(β) + Pr(¬β)
,
(3.25)
where Pr′ is the new state of belief after accommodating soft evidence on event β using a
Bayes factor of k.
Consider the following example, which concerns the alarm of our house and the
potential of a burglary. The initial state of belief is given by:
world
Alarm
Burglary
Pr(.)
ω1
true
true
.000095
ω2
true
false
.009999
ω3
false
true
.000005
ω4
false
false
.989901
One day, we receive a call from our neighbor saying that they may have heard the alarm
of our house going off. Since our neighbor suffers from a hearing problem, we conclude
that our neighbor’s testimony increases the odds of the alarm going off by a factor of 4:
O′(Alarm)/O(Alarm) = 4. Our goal now is to compute our new belief in a burglary taking
3 This is not true if we use ratios of probabilities instead of ratios of odds. For example, if we state that Pr′(α)/Pr(α) =
2, it must follow that Pr(α) ≤1/2 since Pr′(α) ≤1. Hence, the constraint Pr′(α)/Pr(α) = 2 is not compatible with
every state of belief Pr.

3.6 SOFT EVIDENCE
43
place, Pr′(Burglary). Using (3.25) with α : Burglary, β : Alarm and k = 4, we get
Pr′(Burglary) = 4(.000095) + .000005
4(.010094) + .989906 ≈3.74 × 10−4.
3.6.3 More on specifying soft evidence
The difference between (3.21) and (3.25) is only in the way soft evidence is speciﬁed. In
particular, (3.21) expects the evidence to be speciﬁed in terms of the ﬁnal belief assigned
to event β, Pr′(β) = q. On the other hand, (3.25) expects the evidence to be speciﬁed in
terms of the relative effect it has on the odds of event β, O′(β)/O(β) = k.
To shed more light on the difference between the two methods of specifying soft
evidence, consider a murder with three suspects: David, Dick, and Jane. Suppose that we
have an investigator, Rich, with the following state of belief:
world
Killer
Pr(.)
ω1
david
2/3
ω2
dick
1/6
ω3
jane
1/6
According to Rich, the odds of David being the killer is 2 since
O(Killer=david) =
Pr(Killer=david)
Pr(¬(Killer=david)) = 2.
Suppose that some new evidence turns up against David. Rich examines the evidence and
makes the following statement: “This evidence triples the odds of David being the killer.”
Formally, we have soft evidence with the following strength (Bayes factor):
O′(Killer=david)
O(Killer=david) = 3.
Using (3.24), the new belief in David being the killer is
Pr′(Killer=david) =
3 × 2/3
3 × 2/3 + 1/3 = 6
7 ≈86%.
Hence, Rich could have speciﬁed the evidence in two ways by saying, “This evidence
triples the odds of David being the killer” or “Accepting this evidence leads me to have
an 86% belief that David is the killer.” The ﬁrst statement can be used with (3.25) to
compute further beliefs of Rich; for example, his belief in Dick being the killer. The
second statement can also be used for this purpose but with (3.21).
However, the difference between the two statements is that the ﬁrst can be used by
some other investigator to update their beliefs based on the new evidence, while the
second statement cannot be used as such. Suppose that Jon is another investigator with
the following state of belief, which is different from that held by Rich:
world
Killer
Pr(.)
ω1
david
1/2
ω2
dick
1/4
ω3
jane
1/4

44
PROBABILITY CALCULUS
If Jon were to accept Rich’s assessment that the evidence triples the odds of David being
the killer, then using (3.24) Jon would now believe that:
Pr′(Killer=david) =
3 × 1/2
3 × 1/2 + 1/2 = 3
4 = 75%.
Hence, the same evidence that raised Rich’s belief from ≈67% to ≈86% also raised
Jon’s belief from 50% to 75%.
The second statement of Rich, “Accepting this evidence leads me to have about 86%
belief that David is the killer,” is not as meaningful to Jon as it cannot reveal the strength
of evidence independently of Rich’s initial beliefs (which we assume are not accessible
to Jon). Hence, Jon cannot use this statement to update his own beliefs.
3.6.4 Soft evidence as a noisy sensor
One of the most concrete interpretations of soft evidence is in terms of noisy sensors.
Not only is this interpretation useful in practice but it also helps shed more light on the
strength of soft evidence as quantiﬁed by a Bayes factor.
The noisy sensor interpretation is as follows. Suppose that we have some soft evidence
that bears on an event β. We can emulate the effect of this soft evidence using a noisy
sensor S having two states, with the strength of soft evidence captured by the false positive
and negative rates of the sensor:
r The false positive rate of the sensor, fp, is the belief that the sensor would give a positive
reading even though the event β did not occur, Pr(S|¬β).
r The false negative rate of the sensor, fn, is the belief that the sensor would give a negative
reading even though the event β did occur, Pr(¬S|β).
Suppose now that we have a sensor with these speciﬁcations and suppose that it reads
positive. We want to know the new odds of β given this positive sensor reading. We have
O′(β) = Pr′(β)
Pr′(¬β)
= Pr(β|S)
Pr(¬β|S) emulating soft evidence by a positive sensor reading
=
Pr(S|β)Pr(β)
Pr(S|¬β)Pr(¬β) by Bayes Theorem
= 1 −fn
fp
Pr(β)
Pr(¬β)
= 1 −fn
fp
O(β).
This basically proves that the relative change in the odds of β, the Bayes factor O′(β)/O(β),
is indeed a function of only the false positive and negative rates of the sensor and is
independent of the initial beliefs. More speciﬁcally, it shows that soft evidence with a
Bayes factor of k+ can be emulated by a positive sensor reading if the false positive and
negative rates of the sensor satisfy
k+ = 1 −fn
fp
.

3.6 SOFT EVIDENCE
45
Interestingly, this equation shows that the speciﬁc false positive and negative rates are not
as important as the above ratio. For example, a positive reading from any of the following
sensors will have the same impact on beliefs:
r Sensor 1: fp = 10% and fn = 5%
r Sensor 2: fp = 8% and fn = 24%
r Sensor 3: fp = 5% and fn = 52.5%.
This is because a positive reading from any of these sensors will increase the odds of a
corresponding event by a factor of k+ = 9.5.
Note that a negative sensor reading will not necessarily have the same impact for
the different sensors. To see why, consider the Bayes factor corresponding to a negative
reading using a similar derivation to what we have previously:
O′(β) = Pr′(β)
Pr′(¬β)
= Pr(β|¬S)
Pr(¬β|¬S) emulating soft evidence by a negative sensor reading
=
Pr(¬S|β)Pr(β)
Pr(¬S|¬β)Pr(¬β) by Bayes Theorem
=
fn
1 −fp
Pr(β)
Pr(¬β)
=
fn
1 −fp
O(β).
Therefore, a negative sensor reading corresponds to soft evidence with a Bayes factor of
k−=
fn
1 −fp
.
Even though all of the sensors have the same k+, they have different k−values. In
particular, k−≈.056 for Sensor 1, k−≈.261 for Sensor 2, and k−≈.553 for Sensor 3.
That is, although all negative sensor readings will decrease the odds of the corresponding
hypothesis, they do so to different extents. In particular, a negative reading from Sensor 1
is stronger than one from Sensor 2, which in turn is stronger than one from Sensor 3.
Finally, note that as long as
fp + fn < 1,
(3.26)
then k+ > 1 and k−< 1. This means that a positive sensor reading is guaranteed to
increase the odds of the corresponding event and a negative sensor reading is guaranteed
to decrease those odds. The condition in (3.26) is satisﬁed when the false positive and false
negative rates are less than 50% each, which is not unreasonable to assume for a sensor
model. The condition however can also be satisﬁed even if one of the rates is ≥50%.
To conclude, we note that soft evidence on a hypothesis β can be speciﬁed using two
main methods. The ﬁrst speciﬁes the ﬁnal belief in β after accommodating the evidence
and the second speciﬁes the relative change in the odds of β due to accommodating the
evidence. This relative change in odds is called the Bayes factor and can be thought of as
providing a strength of evidence that can be interpreted independently of a given state of
belief. Moreover, the accommodation of soft evidence by a Bayes factor can be emulated

46
PROBABILITY CALCULUS
by a sensor reading. In particular, for any Bayes factor we can choose the false positive and
negative rates of the sensor so its reading will have exactly the same effect on beliefs as
that of the soft evidence. This emulation of soft evidence by hard evidence on an auxiliary
variable is also known as the method of virtual evidence.
3.7 Continuous variables as soft evidence
We are mostly concerned with discrete variables in this book, that is, variables that take
values from a ﬁnite, and typically small, set. The use of continuous variables can be
essential in certain application areas but requires techniques that are generally outside
the scope of this book. However, one of the most common applications of continuous
variables can be accounted for using the notion of soft evidence, allowing one to address
these applications while staying within the framework of discrete variables.
Consider for example a situation where one sends a bit (0 or 1) across a noisy channel
that is then received at the channel output as a number in the interval (−∞, +∞).
Suppose further that our goal is to compute the probability of having sent a 0 given that
we have observed the value, say, −.1 at the channel output. Generally, one would need
two variables to model this problem: a discrete variable I (with two values 0 and 1) to
represent the channel input and a continuous variable O to represent the channel output,
with the goal of computing Pr(I =0|O =−.1). As we demonstrate in this section, one
can avoid the continuous variable O as we can simply emulate hard evidence on this
variable using soft evidence on the discrete variable I. Before we explain this common
and practical technique, we need to provide some background on probability distributions
over continuous variables.
3.7.1 Distribution and density functions
Suppose we have a continuous variable Y with values y in the interval (−∞, +∞). The
probability that Y will take any particular value y is usually zero, so we typically talk about
the probability that Y will take a value ≤y. This is given by a cumulative distribution
function (CDF) F(y), where
F(y) = Pr(Y ≤y).
A number of interesting CDFs do not have known closed forms but can be induced
from probability density functions (PDF) f (t) as follows:
F(y) =

 y
−∞
f (t)dt.
For the function F to correspond to a CDF, we need the PDF to satisfy the conditions
f (t) ≥0 and
 +∞
−∞f (t)dt = 1.
One of the most important density functions is the Gaussian, which is also known as
the Normal:
f (t) =
1
√
2πσ 2 e−(t−µ)2/2σ 2.
Here µ is called the mean and σ is called the standard deviation. When µ = 0 and σ 2 = 1,
the density function is known as the standard Normal. It is known that if a variable Y has a

3.7 CONTINUOUS VARIABLES AS SOFT EVIDENCE
47
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
-4
-2
 0
 2
 4
density f(y)
continuous variable Y
σ=1/2
σ=1
σ=2
Figure 3.4: Three Gaussian density functions with mean µ = 0 and standard deviations σ = 1/2, σ = 1,
and σ = 2.
Normal density with mean µ and standard deviation σ, then the variable Z = (Y −µ)/σ
will have a standard Normal density.
Figure 3.4 depicts a few Gaussian density functions with mean µ = 0. Intuitively, the
smaller the standard deviation the more concentrated the values around the mean. Hence,
a smaller standard deviation implies less variation in the observed values of variable Y.
3.7.2 The Bayes factor of a continuous observation
Suppose that we have a binary variable X with values {x, ¯x} and a continuous variable
Y with values y ∈(−∞, ∞). We next show that the conditional probability Pr(x|y) can
be computed by asserting soft evidence on variable X whose strength is derived from the
density functions f (y|x) and f (y|¯x). That is, we show that the hard evidence implied by
observing the value of a continuous variable can always be emulated by soft evidence
whose strength is derived from the density function of that continuous variable. This will
then preempt the need for representing continuous variables explicitly in an otherwise
discrete model.
We ﬁrst observe that (see Exercise 3.26)
Pr(x|y)/Pr(¯x|y)
Pr(x)/Pr(¯x)
= f (y|x)
f (y|¯x).
(3.27)
If we let Pr be the distribution before we observe the value y and let Pr′ be the new
distribution after observing the value, we get
Pr′(x)/Pr′(¯x)
Pr(x)/Pr(¯x) = O′(x)
O(x) = f (y|x)
f (y|¯x).
(3.28)
Therefore, we can emulate the hard evidence Y =y using soft evidence on x with a Bayes
factor of f (y|x)/f (y|¯x).

48
PROBABILITY CALCULUS
3.7.3 Gaussian noise
To provide a concrete example of this technique, consider the Gaussian distribution that
is commonly used to model noisy observations. The Gaussian density is given by
f (t) =
1
√
2πσ 2 e−(t−µ)2/2σ 2,
where µ is the mean and σ is the standard deviation. Considering our noisy channel
example, we use a Gaussian distribution with mean µ = 0 to model the noise for bit 0 and
another Gaussian distribution with mean µ = 1 to model the noise for bit 1. The standard
deviation is typically the same for both bits as it depends on the channel noise. That is,
we now have
f (y|X = 0) =
1
√
2πσ 2 e−(y−0)2/2σ 2
f (y|X = 1) =
1
√
2πσ 2 e−(y−1)2/2σ 2.
A reading y of the continuous variable can now be viewed as soft evidence on X = 0 with
a Bayes factor determined by (3.28):
k = O′(X=0)
O(X=0) = f (y|X = 0)
f (y|X = 1)
=
√
2πσ 2
√
2πσ 2
e−(y−0)2/2σ 2
e−(y−1)2/2σ 2
= e(1−2y)/2σ 2.
Equivalently, we can interpret this reading as soft evidence on X=1 with a Bayes factor
of 1/e(1−2y)/2σ 2.
To provide a feel for this Bayes factor, we list some of its values for different readings
y and standard deviation σ:
y
σ
−1/2
−1/4
0
1/4
1/2
3/4
1
5/4
6/4
1/3
8,103.1
854.1
90.0
9.5
1.0
.1
.01
.001
.0001
1/2
54.6
2.1
7.4
2.7
1.0
.4
.14
.05
.02
1
2.7
2.1
1.6
1.3
1.0
.8
.6
.5
.4
In summary, we have presented a technique in this section that allows one to condition
beliefs on the values of continuous variables without the need to represent these variables
explicitly. In particular, we have shown that the hard evidence implied by observing the
value of a continuous variable can always be emulated by soft evidence whose strength is
derived from the density function of that continuous variable.
Bibliographic remarks
For introductory texts on probability theory, see Bertsekas and Tsitsiklis [2002] and
DeGroot [2002]. For a discussion on plausible reasoning using probabilities, see Jaynes
[2003] and Pearl [1988]. An in-depth treatment of probabilistic independence is given in
Pearl [1988]. Concepts from information theory, including entropy and mutual informa-
tion, are discussed in Cover and Thomas [1991]. A historical discussion of the Gaussian

3.8 EXERCISES
49
distribution is given in Jaynes [2003]. Our treatment of soft evidence is based on Chan and
Darwiche [2005b]. The Bayes factor was introduced in Good [1950, 1983] and Jeffrey’s
rule was introduced in Jeffrey [1965]. Emulating the “nothing else considered” method us-
ing a noisy sensor is based on the method of “virtual evidence” in Pearl [1988]. The terms
“all things considered” and “nothing else considered” were introduced in Goldszmidt and
Pearl [1996].
3.8 Exercises
3.1. Consider the following joint distribution.
world
A
B
C
Pr(.)
ω1
true
true
true
.075
ω2
true
true
false
.050
ω3
true
false
true
.225
ω4
true
false
false
.150
ω5
false
true
true
.025
ω6
false
true
false
.100
ω7
false
false
true
.075
ω8
false
false
false
.300
(a)
What is Pr(A=true)? Pr(B =true)? Pr(C =true)?
(b)
Update the distribution by conditioning on the event C =true, that is, construct the
conditional distribution Pr(.|C =true).
(c)
What is Pr(A=true|C =true)? Pr(B =true|C =true)?
(d)
Is the event A=true independent of the event C =true? Is B =true independent of
C =true?
3.2. Consider again the joint distribution Pr from Exercise 3.1.
(a)
What is Pr(A=true ∨B =true)?
(b)
Update the distribution by conditioning on the event A=true ∨B =true, that is, construct
the conditional distribution Pr(.|A=true ∨B =true).
(c)
What is Pr(A=true|A=true ∨B =true)? Pr(B =true|A=true ∨B =true)?
(d)
Determine if the event B =true is conditionally independent of C =true given the event
A=true ∨B =true?
3.3. Suppose that we tossed two unbiased coins C1 and C2.
(a)
Given that the ﬁrst coin landed heads, C1 =h, what is the probability that the second
coin landed tails, Pr(C2 =t|C1 =h)?
(b)
Given that at least one of the coins landed heads, C1 =h ∨C2 =h, what is the probability
that both coins landed heads, Pr(C1 =h ∧C2 =h|C1 =h ∨C2 =h)?
3.4. Suppose that 24% of a population are smokers and that 5% of the population have cancer.
Suppose further that 86% of the population with cancer are also smokers. What is the
probability that a smoker will also have cancer?
3.5. Consider again the population from Exercise 3.4. What is the relative change in the odds that
a member of the population has cancer upon learning that they are also a smoker?
3.6. Consider a family with two children, ages four and nine:
(a)
What is the probability that the older child is a boy?
(b)
What is the probability that the older child is a boy given that the younger child is a boy?
(c)
What is the probability that the older child is a boy given that at least one of the children
is a boy?

50
PROBABILITY CALCULUS
(d)
What is the probability that both children are boys given that at least one of them is a
boy?
Deﬁne your variables and the corresponding joint probability distribution. Moreover, for each
of these questions deﬁne α and β for which Pr(α|β) is the answer.
3.7. Prove Equation 3.19.
3.8. Suppose that we have a patient who was just tested for a particular disease and the test
came out positive. We know that one in every thousand people has this disease. We also
know that the test is not reliable: it has a false positive rate of 2% and a false negative rate
of 5%. We have seen previously that the probability of having the disease is ≈4.5% given a
positive test result. Suppose that the test is repeated n times and all tests come out positive.
What is the smallest n for which the belief in the disease is greater than 95%, assuming the
errors of various tests are independent? Justify your answer.
3.9. Consider the following distribution over three variables:
world
A
B
C
Pr(.)
ω1
true
true
true
.27
ω2
true
true
false
.18
ω3
true
false
true
.03
ω4
true
false
false
.02
ω5
false
true
true
.02
ω6
false
true
false
.03
ω7
false
false
true
.18
ω8
false
false
false
.27
For each pair of variables, state whether they are independent. State also whether they are
independent given the third variable. Justify your answers.
3.10. Show the following:
(a) If α |= β and Pr(β) = 0, then Pr(α) = 0.
(b) Pr(α ∧β) ≤Pr(α) ≤Pr(α ∨β).
(c) If α |= β, then Pr(α) ≤Pr(β).
(d) If α |= β |= γ , then Pr(α|β) ≥Pr(α|γ ).
3.11. Let α and β be two propositional sentences over disjoint variables X and Y, respectively.
Show that α and β are independent, that is, Pr(α ∧β) = Pr(α)Pr(β) if variables X and Y are
independent, that is, Pr(x, y) = Pr(x)Pr(y) for all instantiations x and y.
3.12. Consider a propositional sentence α that is represented by an NNF circuit that satisﬁes the
properties of decomposability and determinism. Suppose the circuit inputs are over variables
X1, . . . , Xn and that each variable Xi is independent of every other set of variables that does
not contain Xi. Show that if given the probability distribution Pr(xi) for each variable Xi, the
probability of α can be computed in time linear in the size of the NNF circuit.
3.13. (After Pearl) We have three urns labeled 1, 2, and 3. The urns contain, respectively, three
white and three black balls, four white and two black balls, and one white and two black balls.
An experiment consists of selecting an urn at random then drawing a ball from it.
(a) Deﬁne the set of worlds that correspond to the various outcomes of this experiment. Assume
you have two variables U with values 1, 2, and 3 and C with values black and white.
(b) Deﬁne the joint probability distribution over the set of possible worlds identiﬁed in (a).
(c) Find the probability of drawing a black ball.
(d) Find the conditional probability that urn 2 was selected given that a black ball was drawn.
(e) Find the probability of selecting urn 1 or a white ball.
3.14. Suppose we are presented with two urns labeled 1 and 2 and we want to distribute k white
balls and k black balls between these urns. In particular, say that we want to pick an n and

3.8 EXERCISES
51
m where we place n white balls and m black balls into urn 1 and the remaining k −n white
balls and k −m black balls into urn 2. Once we distribute the balls to urns, say that we play
a game where we pick an urn at random and draw a ball from it.
(a)
What is the probability that we draw a white ball for a given n and m?
Suppose now that we want to choose n and m so that we maximize the probability that
we draw a white ball. Clearly, if both urns have an equal number of white and black balls
(i.e., n = m), then the probability that we draw a white ball is 1
2.
(b)
Suppose that k = 3. Can we choose an n and m so that we increase the probability of
drawing a white ball to 7
10?
(c)
Can we design a strategy for choosing n and m so that as k tends to inﬁnity, the probability
of drawing a white ball tends to 3
4?
3.15. Prove the equivalence between the two deﬁnitions of conditional independence given by
Equations 3.15 and 3.16.
3.16. Let X and Y be two binary variables. Show that X and Y are independent if and only if
Pr(x, y)Pr(¯x, ¯y) = Pr(x, ¯y)Pr(¯x, y).
3.17. Show that Pr(α) = O(α)/(1 + O(α)).
3.18. Show that O(α|β)/O(α) = Pr(β|α)/Pr(β|¬α). Note: Pr(β|α)/Pr(β|¬α) is called the likeli-
hood ratio.
3.19. Show that events α and β are independent if and only if O(α|β) = O(α|¬β).
3.20. Let α and β be two events such that Pr(α) ̸= 0 and Pr(β) ̸= 1. Suppose that Pr(α =⇒β) =
1. Show that:
(a) Knowing ¬α will decrease the probability of β.
(b) Knowing β will increase the probability of α.
3.21. Consider Section 3.6.3 and the investigator Rich with his state of belief regarding murder
suspects:
world
Killer
Pr(.)
ω1
david
2/3
ω2
dick
1/6
ω3
jane
1/6
Suppose now that Rich receives some new evidence that triples his odds of the killer being
male. What is the new belief of Rich that David is the killer? What would this belief be if after
accommodating the evidence, Rich’s belief in the killer being male is 93.75%?
3.22. Consider a distribution Pr over variables X ∪{S}. Let U be a variable in X and suppose
that S is independent of X \ {U} given U. For a given value s of variable S, suppose that
Pr(s|u) = η f (u) for all values u, where f is some function and η > 0 is a constant. Show
that Pr(x|s) does not depend on the constant η . That is, Pr(x|s) is the same for any value of
η > 0 such that 0 ≤η f (u) ≤1.
3.23. Prove Equation 3.21.
3.24. Prove Equations 3.24 and 3.25.
3.25. Suppose we transmit a bit across a noisy channel but for bit 0 we send a signal −1 and for bit
1 we send a signal +1. Suppose again that Gaussian noise is added to the reading y from
the noisy channel, with densities
f (y|X=0) =
1
√
2πσ 2 e−(y+1)2/2σ 2
f (y|X=1) =
1
√
2πσ 2 e−(y−1)2/2σ 2.

52
PROBABILITY CALCULUS
(a)
Show that if we treat the reading y of a continuous variable Y as soft evidence on X = 0,
the corresponding Bayes factor is
k = e−2y/σ 2.
(b)
Give the corresponding Bayes factors for the following readings y and standard devi-
ations σ:
(i) y = + 1
2 and σ = 1
4
(ii) y = −1
2 and σ = 1
4
(iii) y = −3
2 and σ = 4
5
(iv) y = + 1
4 and σ = 4
5
(v) y = −1 and σ = 2
(c)
What reading y would result in neutral evidence regardless of the standard deviation?
What reading y would result in a Bayes factor of 2 given a standard deviation σ = 0.2?
3.26. Prove Equation 3.27. Hint: Show ﬁrst that Pr(x|y)/Pr(x) = f (y|x)/f (y), where f (y) is the
PDF for variable Y .
3.27. Suppose we have a sensor that bears on event β and has a false positive rate fp and a false
negative rate fn. Suppose further that we want a positive reading of this sensor to increase
the odds of β by a factor of k > 1 and a negative reading to decrease the odds of β by the
same factor k. Prove that these conditions imply that fp = fn = 1/(k + 1).

4
Bayesian Networks
We introduce Bayesian networks in this chapter as a modeling tool for compactly speci-
fying joint probability distributions.
4.1 Introduction
We have seen in Chapter 3 that joint probability distributions can be used to model
uncertain beliefs and change them in the face of hard and soft evidence. We have also seen
that the size of a joint probability distribution is exponential in the number of variables
of interest, which introduces both modeling and computational difﬁculties. Even if these
difﬁculties are addressed, one still needs to ensure that the synthesized distribution matches
the beliefs held about a given situation. For example, if we are building a distribution that
captures the beliefs of a medical expert, we may need to ensure some correspondence
between the independencies held by the distribution and those believed by the expert. This
may not be easy to enforce if the distribution is constructed by listing all possible worlds
and assessing the belief in each world directly.
The Bayesian network is a graphical modeling tool for specifying probability distribu-
tions that, in principle, can address all of these difﬁculties. The Bayesian network relies
on the basic insight that independence forms a signiﬁcant aspect of beliefs and that it
can be elicited relatively easily using the language of graphs. We start our discussion in
Section 4.2 by exploring this key insight, and use our developments in Section 4.3 to
provide a formal deﬁnition of the syntax and semantics of Bayesian networks. Section 4.4
is dedicated to studying the properties of probabilistic independence, and Section 4.5 is
dedicated to a graphical test that allows one to efﬁciently read the independencies encoded
by a Bayesian network. Some additional properties of Bayesian networks are discussed in
Section 4.6, which unveil some of their expressive powers and representational limitations.
4.2 Capturing independence graphically
Consider the directed acyclic graph (DAG) in Figure 4.1, where nodes represent propo-
sitional variables. To ground our discussion, assume for now that edges in this graph
represent “direct causal inﬂuences” among these variables. For example, the alarm trig-
gering (A) is a direct cause of receiving a call from a neighbor (C).
Given this causal structure, one would expect the dynamics of belief change to satisfy
some properties. For example, we would expect our belief in C to be inﬂuenced by
evidence on R. If we get a radio report that an earthquake took place in our neighborhood,
our belief in the alarm triggering would probably increase, which would also increase our
belief in receiving a call from our neighbor. However, we would not change this belief if
we knew for sure that the alarm did not trigger. That is, we would ﬁnd C independent of
R given ¬A in the context of this causal structure.
53

54
BAYESIAN NETWORKS
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
Figure 4.1: A directed acyclic graph that captures independence among ﬁve propositional variables.
Visit to Asia?
(A)
Smoker?
(S)
Tuberculosis?
(T)
Lung Cancer?
(C)
Bronchitis?
(B)
Tuberculosis or Cancer?
(P)
Positive X-Ray?
(X)
Dyspnoea?
(D)
Figure 4.2: A directed acyclic graph that captures independence among eight propositional variables.
For another example, consider the causal structure in Figure 4.2, which captures some
of the common causal perceptions in a limited medical domain. Here we would clearly
ﬁnd a visit to Asia relevant to our belief in the x-ray test coming out positive but we would
ﬁnd the visit irrelevant if we know for sure that the patient does not have tuberculosis.
That is, X is dependent on A but is independent of A given ¬T .
The previous examples of independence are all implied by a formal interpretation of
each DAG as a set of conditional independence statements. To phrase this interpretation
formally, we need the following notation. Given a variable V in a DAG G:
r Parents(V ) are the parents of V in DAG G, that is, the set of variables N with an edge
from N to V . For example, the parents of variable A in Figure 4.1 are E and B.
r Descendants(V ) are the descendants of V in DAG G, that is, the set of variables N with
a directed path from V to N (we also say that V is an ancestor of N in this case). For
example, the descendants of variable B in Figure 4.1 are A and C.

4.2 CAPTURING INDEPENDENCE GRAPHICALLY
55
1
S
2S
3
S
nS
1
O
2
O
3
O
n
O
Figure 4.3: A directed acyclic graph known as a hidden Markov model.
r Non Descendants(V ) are all variables in DAG G other than V , Parents(V ), and
Descendants(V ). We will call these variables the nondescendants of V in DAG G. For
example, the nondescendants of variable B in Figure 4.1 are E and R.
Given this notation, we will then formally interpret each DAG G as a compact represen-
tation of the following independence statements:
I(V, Parents(V ), Non Descendants(V ))
for all variables V in DAG G.
(4.1)
That is, every variable is conditionally independent of its nondescendants given its par-
ents. We will refer to the independence statements declared by (4.1) as the Markovian
assumptions of DAG G and denote them by Markov(G).
If we view the DAG as a causal structure, then Parents(V ) denotes the direct causes of V
and Descendants(V ) denotes the effects of V . The statement in (4.1) will then read: Given
the direct causes of a variable, our beliefs in that variable will no longer be inﬂuenced by
any other variable except possibly by its effects.
Let us now consider some concrete examples of the independence statements rep-
resented by a DAG. The following are all the statements represented by the DAG in
Figure 4.1:
I(C, A, {B, E, R})
I(R, E, {A, B, C})
I(A, {B, E}, R)
I(B, ∅, {E, R})
I(E, ∅, B)
Note that variables B and E have no parents, hence, they are marginally independent of
their nondescendants.
For another example, consider the DAG in Figure 4.3, which is quite common in many
applications and is known as a hidden Markov model (HMM). In this DAG, variables
S1, S2, . . . , Sn represent the state of a dynamic system at time points 1, 2, . . . , n, respec-
tively. Moreover, the variables O1, O2, . . . , On represent sensors that measure the system
state at the corresponding time points. Usually, one has some information about the sensor
readings and is interested in computing beliefs in the system state at different time points.
The independence statement declared by this DAG for state variables Si is
I(St, {St−1}, {S1, . . . , St−2, O1, . . . , Ot−1}).
That is, once we know the state of the system at the previous time point, t −1, our belief
in the present system state, at time t, is no longer inﬂuenced by any other information
about the past.

56
BAYESIAN NETWORKS
Note that the formal interpretation of a DAG as a set of conditional independence
statements makes no reference to the notion of causality, even though we used causality
to motivate this interpretation. If one constructs the DAG based on causal perceptions,
then one would tend to agree with the independencies declared by the DAG. However, it
is perfectly possible to have a DAG that does not match our causal perceptions yet we
agree with the independencies declared by the DAG. Consider for example the DAG in
Figure 4.1 which matches common causal perceptions. Consider now the alternative DAG
in Figure 4.13 on Page 70, which does not match these perceptions. As we shall see later,
every independence that is declared (or implied) by the second DAG is also declared (or
implied) by the ﬁrst. Hence, if we accept the ﬁrst DAG, then we must also accept the
second.
We next discuss the process of parameterizing a DAG, which involves quantifying the
dependencies between nodes and their parents. This process is much easier to accomplish
by an expert if the DAG corresponds to causal perceptions.
4.3 Parameterizing the independence structure
Suppose now that our goal is to construct a probability distribution Pr that captures our
state of belief regarding the domain given in Figure 4.1. The ﬁrst step is to construct a
DAG G while ensuring that the independence statements declared by G are consistent
with our beliefs about the underlying domain. The DAG G is then a partial speciﬁcation
of our state of belief Pr. Speciﬁcally, by constructing G we are saying that the distribution
Pr must satisfy the independence assumptions of Markov(G). This clearly constrains the
possible choices for the distribution Pr but does not uniquely deﬁne it. As it turns out, we
can augment the DAG G by a set of conditional probabilities that together with Markov(G)
are guaranteed to deﬁne the distribution Pr uniquely.
The additional set of conditional probabilities that we need are as follows: For every
variable X in the DAG G and its parents U, we need to provide the probability Pr(x|u)
for every value x of variable X and every instantiation u of parents U. For example, for
the DAG in Figure 4.1 we need to provide the following conditional probabilities:
Pr(c|a), Pr(r|e), Pr(a|b, e), Pr(e), Pr(b),
where a, b, c, e, and r are values of variables A, B, C, E, and R. Here is an example of
the conditional probabilities required for variable C:
A
C
Pr(c|a)
true
true
.80
true
false
.20
false
true
.001
false
false
.999
This table is known as a conditional probability table (CPT) for variable C. Note that we
must have
Pr(c|a) + Pr(¯c|a) = 1 and Pr(c|¯a) + Pr(¯c|¯a) = 1.
Hence, two of the probabilities in this CPT are redundant and can be inferred from the
other two. It turns out that we only need ten independent probabilities to completely
specify the CPTs for the DAG in Figure 4.1.
We are now ready to provide the formal deﬁnition of a Bayesian network.

4.3 PARAMETERIZING THE INDEPENDENCE STRUCTURE
57
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
A
A
true
.6
false
.4
A
B
B|A
true
true
.2
true
false
.8
false
true
.75
false
false
.25
A
C
C|A
true
true
.8
true
false
.2
false
true
.1
false
false
.9
B
C
D
D|B,C
true
true
true
.95
true
true
false
.05
true
false
true
.9
true
false
false
.1
false
true
true
.8
false
true
false
.2
false
false
true
0
false
false
false
1
C
E
E|C
true
true
.7
true
false
.3
false
true
0
false
false
1
Figure 4.4: A Bayesian network over ﬁve propositional variables.
Deﬁnition 4.1. A Bayesian network for variables Z is a pair (G, ), where:
r G is a directed acyclic graph over variables Z, called the network structure.
r  is a set of CPTs, one for each variable in Z, called the network parametrization.
We will use X|U to denote the CPT for variable X and its parents U, and refer to the
set XU as a network family. We will also use θx|u to denote the value assigned by CPT
X|U to the conditional probability Pr(x|u) and call θx|u a network parameter. Note that
we must have 
x θx|u = 1 for every parent instantiation u.
■
Figure 4.4 depicts a Bayesian network over ﬁve variables, Z = {A, B, C, D, E}.
An instantiation of all network variables will be called a network instantiation. More-
over, a network parameter θx|u is said to be compatible with a network instantiation z
when the instantiations xu and z are compatible (i.e., they agree on the values they assign
to their common variables). We will write θx|u ∼z in this case. In the Bayesian network
of Figure 4.4, θa, θb|a, θ¯c|a, θd|b,¯c, and θ¯e|¯c are all the network parameters compatible with
network instantiation a, b, ¯c, d, ¯e.
We later prove that the independence constraints imposed by a network structure and
the numeric constraints imposed by its parametrization are satisﬁed by one and only
one probability distribution Pr. Moreover, we show that the distribution is given by the

58
BAYESIAN NETWORKS
following equation:
Pr(z)
def=

θx|u∼z
θx|u.
(4.2)
That is, the probability assigned to a network instantiation z is simply the product of
all network parameters compatible with z. Equation (4.2) is known as the chain rule for
Bayesian networks. A Bayesian network will then be understood as an implicit represen-
tation of a unique probability distribution Pr given by (4.2). For an example, consider the
Bayesian network in Figure 4.4. We then have
Pr(a, b, ¯c, d, ¯e) = θa θb|a θ¯c|a θd|b,¯c θ¯e|¯c
= (.6)(.2)(.2)(.9)(1)
= .0216
Moreover,
Pr(¯a, ¯b, ¯c, ¯d, ¯e) = θ¯a θ¯b|¯a θ¯c|¯a θ ¯d|¯b,¯c θ¯e|¯c
= (.4)(.25)(.9)(1)(1)
= .09
Note that the size of CPT X|U is exponential in the number of parents U. In general,
if every variable can take up to d values and has at most k parents, the size of any CPT
is bounded by O(dk+1). Moreover, if we have n network variables, the total number of
Bayesian network parameters is bounded by O(n · dk+1). This number is quite reasonable
as long as the number of parents per variable is relatively small. We discuss in future
chapters techniques for efﬁciently representing the CPT X|U even when the number of
parents U is large.
Consider the HMM in Figure 4.3 as an example, and suppose that each state variable
Si has m values and similarly for sensor variables Oi. The CPT for any state variable
Si, i > 1, contains m2 parameters, which are usually known as transition probabilities.
Similarly, the CPT for any sensor variable Oi has m2 parameters, which are usually known
as emission or sensor probabilities. The CPT for the ﬁrst state variable S1 only has m
parameters. In fact, in an HMM the CPTs for state variables Si, i > 1, are all identical,
and the CPTs for all sensor variables Oi are also all identical.1
4.4 Properties of probabilistic independence
The distribution Pr speciﬁed by a Bayesian network (G, ) is guaranteed to satisfy every
independence assumption in Markov(G) (see Exercise 4.5). Speciﬁcally, we must have
IPr(X, Parents(X), Non Descendants(X))
for every variable X in the network. However, these are not the only independencies
satisﬁed by the distribution Pr. For example, the distribution induced by the Bayesian
network in Figure 4.4 ﬁnds D and E independent given A and C yet this independence is
not part of Markov(G).
This independence and additional ones follow from the ones in Markov(G) using a set
of properties for probabilistic independence, known as the graphoid axioms, which include
symmetry, decomposition, weak union, and contraction. We introduce these axioms in this
1 The HMM is said to be homogeneous in this case.

4.4 PROPERTIES OF PROBABILISTIC INDEPENDENCE
59
section and explore some of their applications. We then provide a graphical criterion in
Section 4.5 called d-separation, which allows us to infer the implications of these axioms
by operating efﬁciently on the structure of a Bayesian network.
Before we introduce the graphoid axioms, we ﬁrst recall the deﬁnition of IPr(X, Z, Y),
that is, distribution Pr ﬁnds variables X independent of variables Y given variables Z:
Pr(x|z, y) = Pr(x|z)
or Pr(y, z) = 0,
for all instantiations x, y, z of variables X, Y, Z, respectively.
Symmetry
The ﬁrst and simplest property of probabilistic independence we consider is symmetry:
IPr(X, Z, Y) if and only if IPr(Y, Z, X).
(4.3)
According to this property, if learning y does not inﬂuence our belief in x, then learn-
ing x does not inﬂuence our belief in y. Consider now the DAG G in Figure 4.1 and
suppose that Pr is the probability distribution induced by the corresponding Bayesian net-
work. From the independencies declared by Markov(G), we know that IPr(A, {B, E}, R).
Using symmetry, we can then conclude that IPr(R, {B, E}, A), which is not part of the
independencies declared by Markov(G).
Decomposition
The second property of probabilistic independence that we consider is decomposition:
IPr(X, Z, Y ∪W) only if IPr(X, Z, Y) and IPr(X, Z, W).
(4.4)
This property says that if learning yw does not inﬂuence our belief in x, then learning y
alone, or learning w alone, will not inﬂuence our belief in x. That is, if some information
is irrelevant, then any part of it is also irrelevant. Note that the opposite of decomposition,
called composition,
IPr(X, Z, Y) and IPr(X, Z, W) only if IPr(X, Z, Y ∪W),
(4.5)
does not hold in general. Two pieces of information may each be irrelevant on their own
yet their combination may be relevant.
One important application of decomposition is as follows. Consider the DAG G in
Figure 4.2 and let us examine what the Markov(G) independencies say about variable B:
I(B, S, {A, C, P, T, X}).
If we use decomposition, we also conclude I(B, S, C): Once we know whether the person
is a smoker, our belief in developing bronchitis is no longer inﬂuenced by information
about developing cancer. This independence is then guaranteed to hold in any probability
distribution that is induced by a parametrization of DAG G. Yet this independence is
not part of the independencies declared by Markov(G). More generally, decomposition
allows us to state the following:
IPr(X, Parents(X), W)
for every W ⊆Non Descendants(X),
(4.6)
that is, every variable X is conditionally independent of any subset of its nondescendants
given its parents. This is then a strengthening of the independence statements declared by
Markov(G), which is a special case when W contains all nondescendants of X.

60
BAYESIAN NETWORKS
Another important application of decomposition is that it allows us to prove the chain
rule for Bayesian networks given in (4.2). Let us ﬁrst carry the proof in the context of DAG
G in Figure 4.1, where our goal is to compute the probability of instantiation r, c, a, e, b.
By the chain rule of probability calculus (see Chapter 3), we have
Pr(r, c, a, e, b) = Pr(r|c, a, e, b)Pr(c|a, e, b)Pr(a|e, b)Pr(e|b)Pr(b).
By the independencies given in (4.6), we immediately have
Pr(r|c, a, e, b) = Pr(r|e)
Pr(c|a, e, b) = Pr(c|a)
Pr(e|b) = Pr(e).
Hence, we have
Pr(r, c, a, e, b) = Pr(r|e)Pr(c|a)Pr(a|e, b)Pr(e)Pr(b)
= θr|e θc|a θa|e,b θe θb,
which is the result given by (4.2).
This proof generalizes to any Bayesian network (G, ) over variables Z as long as we
apply the chain rule to a variable instantiation z in which the parents U of each variable
X appear after X in the instantiation z. This ordering constraint ensures two things. First,
for every term Pr(x|α) that results from applying the chain rule to Pr(z) some instantiation
u of parents U is guaranteed to be in α. Second, the only other variables appearing in α,
beyond parents U, must be nondescendants of X. Hence, the term Pr(x|α) must equal the
network parameter θx|u by the independencies in (4.6).
For another example, consider again the DAG in Figure 4.1 and the following variable
ordering c, a, r, b, e. We then have
Pr(c, a, r, b, e) = Pr(c|a, r, b, e)Pr(a|r, b, e)Pr(r|b, e)Pr(b|e)Pr(e).
By the independencies given in (4.6), we immediately have
Pr(c|a, r, b, e) = Pr(c|a)
Pr(a|r, b, e) = Pr(a|b, e)
Pr(r|b, e) = Pr(r|e)
Pr(b|e) = Pr(b).
Hence,
Pr(c, a, r, b, e) = Pr(c|a)Pr(a|b, e)Pr(r|e)Pr(b)Pr(e)
= θc|a θa|b,e θr|e θb θe,
which is again the result given by (4.2).
Consider now the DAG in Figure 4.3 and let us apply the previous proof to the
instantiation on, . . . , o1, sn, . . . , s1, which satisﬁes the mentioned ordering property. The
chain rule gives
Pr(on, . . . , o1, sn, . . . , s1)
= Pr(on|on−1 . . . , o1, sn, . . . , s1) . . . Pr(o1|sn, . . . , s1)Pr(sn|sn−1 . . . , s1) . . . Pr(s1).

4.4 PROPERTIES OF PROBABILISTIC INDEPENDENCE
61
We can simplify these terms using the independencies in (4.6), leading to
Pr(on, . . . , o1, sn, . . . , s1) = Pr(on|sn) . . . Pr(o1|s1)Pr(sn|sn−1) . . . Pr(s1)
= θon|sn . . . θo1|s1 θsn|sn−1 . . . θs1.
Hence, we are again able to express Pr(on, . . . , o1, sn, . . . , s1) as a product of network
parameters.
We have shown that if a distribution Pr satisﬁes the independencies in Markov(G) and
if Pr(x|u) = θx|u, then the distribution must be given by (4.2). Exercise (4.5) asks for a
proof of the other direction: If a distribution is given by (4.2), then it must satisfy the
independencies in Markov(G) and we must have Pr(x|u) = θx|u. Hence, the distribution
given by (4.2) is the only distribution that satisﬁes the qualitative constraints given by
Markov(G) and the numeric constraints given by network parameters.
Weak union
The next property of probabilistic independence we consider is called weak union:
IPr(X, Z, Y ∪W) only if IPr(X, Z ∪Y, W).
(4.7)
This property says that if the information yw is not relevant to our belief in x, then the
partial information y will not make the rest of the information, w, relevant.
One application of weak union is as follows. Consider the DAG G in Figure 4.1
and let Pr be a probability distribution generated by some Bayesian network (G, ).
The independence I(C, A, {B, E, R}) is part of Markov(G) and, hence, is satisﬁed by
distribution Pr. Using weak union, we can then conclude IPr(C, {A, E, B}, R), which
is not part of the independencies declared by Markov(G). More generally, we have the
following:
IPr(X, Parents(X) ∪W, Non Descendants(X) \ W),
(4.8)
for any W ⊆Non Descendants(X). That is, each variable X in DAG G is independent of
any of its nondescendants given its parents and the remaining nondescendants. This can
be viewed as a strengthening of the independencies declared by Markov(G), which fall
as a special case when the set W is empty.
Contraction
The fourth property of probabilistic independence we consider is called contraction:
IPr(X, Z, Y) and IPr(X, Z ∪Y, W) only if IPr(X, Z, Y ∪W).
(4.9)
This property says that if after learning the irrelevant information y the information w is
found to be irrelevant to our belief in x, then the combined information yw must have been
irrelevant from the beginning. It is instructive to compare contraction with composition
in (4.5) as one can view contraction as a weaker version of composition. Recall that
composition does not hold for probability distributions.
Consider now the DAG in Figure 4.3 and let us see how contraction can help in proving
IPr({S3, S4}, S2, S1). That is, once we know the state of the system at time 2, information
about the system state at time 1 is not relevant to the state of the system at times 3 and 4.
Note that Pr is any probability distribution that results from parameterizing DAG G. Note
also that the previous independence is not part of Markov(G).

62
BAYESIAN NETWORKS
A
B
E
D
C
X
Y
Z
Figure 4.5: A digital circuit.
By (4.6), we have
IPr(S3, S2, S1)
(4.10)
IPr(S4, S3, {S1, S2}).
(4.11)
By weak union and (4.11), we also have
IPr(S4, {S2, S3}, S1).
(4.12)
Applying contraction (and symmetry) to (4.10) and (4.12), we get our result:
IPr({S4, S3}, S2, S1).
Intersection
The ﬁnal axiom we consider is called intersection and holds only for the class of strictly
positive probability distributions, that is, distributions that assign a nonzero probability
to every consistent event. A strictly positive distribution is then unable to capture logical
constraints; for example, it cannot represent the behavior of inverter X in Figure 4.5 as it
will have to assign the probability zero to the event A=true, C =true.
The following is the property of intersection:
IPr(X, Z ∪W, Y) and IPr(X, Z ∪Y, W) only if IPr(X, Z, Y ∪W),
(4.13)
when Pr is a strictly positive distribution.2 This property says that if information w is
irrelevant given y and information y is irrelevant given w, then the combined information
yw is irrelevant to start with. This is not true in general. Consider the circuit in Figure 4.5
and assume that all components are functioning normally. If we know the input A of
inverter X, its output C becomes irrelevant to our belief in the circuit output E. Similarly,
if we know the output C of inverter X, its input A becomes irrelevant to this belief. Yet
variables A and C are not irrelevant to our belief in the circuit output E. As it turns out,
the intersection property is only contradicted in the presence of logical constraints and,
hence, it holds for strictly positive distributions.
The four properties of symmetry, decomposition, weak union, and contraction, com-
bined with a property called triviality, are known as the graphoid axioms. Triviality simply
states that IPr(X, Z, ∅). With the property of intersection, the set is known as the positive
2 Note that if we replace IPr(X, Z ∪W, Y) with IPr(X, Z, Y), we get contraction.

4.5 A GRAPHICAL TEST OF INDEPENDENCE
63
Figure 4.6: A path with six valves. From left to right, the type of valves are convergent, divergent,
sequential, convergent, sequential, and sequential.
graphoid axioms.3 It is interesting to note that the properties of decomposition, weak
union, and contraction can be summarized tersely in one statement:
IPr(X, Z, Y ∪W) if and only if IPr(X, Z, Y) and IPr(X, Z ∪Y, W).
(4.14)
Proving the positive graphoid axioms is left to Exercise 4.9.
4.5 A graphical test of independence
Suppose that Pr is a distribution induced by a Bayesian network (G, ). We have seen
earlier that the distribution Pr satisﬁes independencies that go beyond what is declared
by Markov(G). In particular, we have seen how one can use the graphoid axioms to
derive new independencies that are implied by those in Markov(G). However, deriving
these additional independencies may not be trivial. The good news is that the inferential
power of the graphoid axioms can be tersely captured using a graphical test known as
d-separation, which allows one to mechanically and efﬁciently derive the independencies
implied by these axioms. Our goal in this section is to introduce the d-separation test,
show how it can be used for this purpose, and discuss some of its formal properties.
The intuition behind the d-separation test is as follows. Let X, Y, and Z be three
disjoint sets of variables. To test whether X and Y are d-separated by Z in DAG G, written
dsepG(X, Z, Y), we need to consider every path between a node in X and a node in Y and
then ensure that the path is blocked by Z. Hence, the deﬁnition of d-separation relies on
the notion of blocking a path by a set of variables Z, which we will deﬁne next. First, we
note that dsepG(X, Z, Y) implies IPr(X, Z, Y) for every probability distribution Pr induced
by G. This guarantee, together with the efﬁciency of the test, is what makes d-separation
such an important notion.
Consider the path given in Figure 4.6 (note that a path does not have to be directed).
The best way to understand the notion of blocking is to view the path as a pipe and to view
each variable W on the path as a valve. A valve W is either open or closed, depending
on some conditions that we state later. If at least one of the valves on the path is closed,
then the whole path is blocked, otherwise the path is said to be not blocked. Therefore,
the notion of blocking is formally deﬁned once we deﬁne the conditions under which a
valve is considered open or closed.
As it turns out, there are three types of valves and we need to consider each of them
separately before we can state the conditions under which they are considered closed.
Speciﬁcally, the type of a valve is determined by its relationship to its neighbors on the
path as shown in Figure 4.7:
3 The terms semi-graphoid and graphoid are sometimes used instead of graphoid and positive graphoid, respectively.

64
BAYESIAN NETWORKS
W
W
W
Sequential
Divergent
Convergent
Figure 4.7: Three types of valves used in deﬁning d-separation.
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
Sequential valve
Divergent valve
Convergent valve
Figure 4.8: Examples of valve types.
r A sequential valve (→W→) arises when W is a parent of one of its neighbors and a child
of the other.
r A divergent valve (←W→) arises when W is a parent of both neighbors.
r A convergent valve (→W←) arises when W is a child of both neighbors.
The path in Figure 4.6 has six valves. From left to right, the type of valves are convergent,
divergent, sequential, convergent, sequential, and sequential.
To obtain more intuition on these types of valves, it is best to interpret the given DAG
as a causal structure. Consider Figure 4.8, which provides concrete examples of the three
types of valves in the context of a causal structure. We can then attach the following
interpretations to valve types:
r A sequential valve N1→W→N2 declares variable W as an intermediary between a cause
N1 and its effect N2. An example of this type is E→A→C in Figure 4.8.
r A divergent valve N1←W→N2 declares variable W as a common cause of two effects
N1 and N2. An example of this type is R←E→A in Figure 4.8.
r A convergent valve N1→W←N2 declares variable W as a common effect of two causes
N1 and N2. An example of this type is E→A←B in Figure 4.8.
Given this causal interpretation of valve types, we can now better motivate the conditions
under which valves are considered closed given a set of variables Z:
r A sequential valve (→W→) is closed iff variable W appears in Z.
For example, the sequential valve E→A→C in Figure 4.8 is closed iff we know the value
of variable A, otherwise an earthquake E may change our belief in getting a call C.
r A divergent valve (←W→) is closed iff variable W appears in Z.
For example, the divergent valve R←E→A in Figure 4.8 is closed iff we know the value
of variable E, otherwise a radio report on an earthquake may change our belief in the
alarm triggering.

4.5 A GRAPHICAL TEST OF INDEPENDENCE
65
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
closed
open
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
open
open
Figure 4.9: On the left, R and B are d-separated by E, C. On the right, R and C are not d-separated.
r A convergent valve (→W←) is closed iff neither variable W nor any of its descendants
appears in Z.
For example, the convergent valve E→A←B in Figure 4.8 is closed iff neither the value
of variable A nor the value of C are known, otherwise, a burglary may change our belief
in an earthquake.
We are now ready to provide a formal deﬁnition of d-separation.
Deﬁnition 4.2. Let X, Y, and Z be disjoint sets of nodes in a DAG G. We will say that
X and Y are d-separated by Z, written dsepG(X, Z, Y), iff every path between a node
in X and a node in Y is blocked by Z where a path is blocked by Z iff at least one valve
on the path is closed given Z.
■
Note that according to this deﬁnition, a path with no valves (i.e., X →Y) is never blocked.
Let us now consider some examples of d-separation before we discuss its formal
properties. Our ﬁrst example is with respect to Figure 4.9. Considering the DAG G on
the left of this ﬁgure, R and B are d-separated by E and C: dsepG(R, {E, C}, B). There
is only one path connecting R and B in this DAG and it has two valves: R←E→A and
E→A←B. The ﬁrst valve is closed given E and C and the second valve is open given
E and C. But the closure of only one valve is sufﬁcient to block the path, therefore
establishing d-separation. For another example, consider the DAG G on the right of
Figure 4.9 in which R and C are not d-separated: dsepG(R, ∅, C) does not hold. Again,
there is only one path in this DAG between R and C and it contains two valves, R←E→A
and E→A→C, which are both open. Hence, the path is not blocked and d-separation
does not hold.
Consider now the DAG G in Figure 4.10 where our goal here is to test whether B and
C are d-separated by S: dsepG(B, S, C). There are two paths between B and C in this
DAG. The ﬁrst path has only one valve, C←S→B, which is closed given S and, hence,
the path is blocked. The second path has two valves, C→P→D and P→D←B, where
the second valve is closed given S and, hence, the path is blocked. Since both paths are
blocked by S, we then have that C and B are d-separated by S.
For a ﬁnal example of d-separation, let us consider the DAG in Figure 4.11 and try
to show that IPr(S1, S2, {S3, S4}) for any probability distribution Pr that is induced by the
DAG. We ﬁrst note that any path between S1 and {S3, S4} must have the valve S1→S2→S3
on it, which is closed given S2. Hence, every path from S1 to {S3, S4} is blocked by S2 and
we have dsepG(S1, S2, {S3, S4}), which leads to IPr(S1, S2, {S3, S4}). This example shows
how d-separation provides a systematic graphical criterion for deriving independencies,
which can replace the application of the graphoid axioms as we did on Page 61. The
d-separation test can be implemented quite efﬁciently, as we show later.

66
BAYESIAN NETWORKS
Visit to Asia?
(A)
Smoker?
(S)
Tuberculosis?
(T)
Lung Cancer?
(C)
Bronchitis?
(B)
Tuberculosis or Cancer?
(P)
Positive X-Ray?
(X)
Dyspnoea?
(D)
open
closed
closed
Figure 4.10: C and B are d-separated given S.
1
S
2
S
3
S
n
S
1
O
2
O
3
O
n
O
closed
Figure 4.11: S1 is d-separated from S3, . . . , Sn by S2.
4.5.1 Complexity of d-separation
The deﬁnition of d-separation, dsepG(X, Z, Y), calls for considering all paths connecting
a node in X with a node in Y. The number of such paths can be exponential yet one
can implement the test without having to enumerate these paths explicitly, as we show
next.
Theorem 4.1. Testing whether X and Y are d-separated by Z in DAG G is equivalent to
testing whether X and Y are disconnected in a new DAG G′, which is obtained by pruning
DAG G as follows:
r We delete any leaf node W from DAG G as long as W does not belong to X ∪Y ∪Z. This
process is repeated until no more nodes can be deleted.
r We delete all edges outgoing from nodes in Z.
■
Figure 4.12 depicts two examples of this pruning procedure. Note that the connectivity
test on DAG G′ ignores edge directions. Given Theorem 4.1, d-separation can be decided
in time and space that are linear in the size of DAG G (see Exercise 4.7).

4.5 A GRAPHICAL TEST OF INDEPENDENCE
67
Visit to Asia?
(A)
Smoker?
(S)
Tuberculosis?
(T)
Lung Cancer?
(C)
Bronchitis?
(B)
Tuberculosis or Cancer?
(P)
Positive X-Ray?
(X)
Dyspnoea?
(D)
X
Y
Visit to Asia?
(A)
Smoker?
(S)
Tuberculosis?
(T)
Lung Cancer?
(C)
Bronchitis?
(B)
Tuberculosis or Cancer?
(P)
Positive X-Ray?
(X)
Dyspnoea?
(D)
X
Y
Figure 4.12: On the left, a pruned DAG for testing whether X = {A, S} is d-separated from Y = {D, X}
by Z = {B, P}. On the right, a pruned DAG for testing whether X = {T, C} is d-separated from Y = {B}
by Z = {S, X}. Both tests are positive. Pruned nodes and edges are dotted. Nodes in Z are shaded.
4.5.2 Soundness and completeness of d-separation
The d-separation test is sound in the following sense.
Theorem 4.2. If Pr is a probability distribution induced by a Bayesian network (G, ),
then
dsepG(X, Z, Y) only if IPr(X, Z, Y).
■
Hence, we can safely use the d-separation test to derive independence statements about
probability distributions induced by Bayesian networks. The proof of soundness is con-
structive, showing that every independence claimed by d-separation can indeed be derived
using the graphoid axioms. Hence, the application of d-separation can be viewed as a
graphical application of these axioms.
Another relevant question is whether d-separation is complete, that is, whether it
is capable of inferring every possible independence statement that holds in the induced
distribution Pr. As it turns out, the answer is no. For a counterexample, consider a Bayesian
network with three binary variables, X→Y→Z. In this network, Z is not d-separated
from X. However, it is possible for Z to be independent of X in a probability distribution
that is induced by this network. Suppose, for example, that the CPT for variable Y is
chosen so that θy|x = θy|¯x. In this case, the induced distribution will ﬁnd Y independent
of X even though there is an edge between them (since Pr(y) = Pr(y|x) = Pr(y|¯x) and
Pr( ¯y) = Pr( ¯y|x) = Pr( ¯y|¯x) in this case). The distribution will also ﬁnd Z independent of
X even though the path connecting them is not blocked.
Hence, by choosing the parametrization carefully we are able to establish an indepen-
dence in the induced distribution that d-separation cannot detect. Of course, this is not too
surprising since d-separation has no access to the chosen parametrization. We can then
say the following. Let Pr be a distribution induced by a Bayesian network (G, ):
r If X and Y are d-separated by Z, then X and Y are independent given Z for any parametriza-
tion .
r If X and Y are not d-separated by Z, then whether X and Y are dependent given Z depends
on the speciﬁc parametrization .

68
BAYESIAN NETWORKS
Can we always parameterize a DAG G in such a way to ensure the completeness
of d-separation? The answer is yes. That is, d-separation satisﬁes the following weaker
notion of completeness.
Theorem 4.3. For every DAG G, there is a parametrization  such that
IPr(X, Z, Y) if and only if dsepG(X, Z, Y),
where Pr is the probability distribution induced by Bayesian network (G, ).
■
This weaker notion of completeness implies that one cannot improve on the d-separation
test. That is, there is no other graphical test that can derive more independencies from
Markov(G) than those derived by d-separation.
4.5.3 Further properties of d-separation
We have seen that conditional independence satisﬁes some properties, such as the
graphoids axioms, but does not satisfy others, such as composition given in (4.5).
Suppose that X and Y are d-separated by Z, dsep(X, Z, Y), which means that every
path between X and Y is blocked by Z. Suppose further that X and W are d-separated by
Z, dsep(X, Z, W), which means that every path between X and W is blocked by Z. It then
immediately follows that every path between X and Y ∪W is also blocked by Z. Hence,
X and Y ∪W are d-separated by Z and we have dsep(X, Z, Y ∪W). We just proved that
composition holds for d-separation:
dsep(X, Z, Y) and dsep(X, Z, W) only if dsep(X, Z, Y ∪W).
Since composition does not hold for probability distributions, this means the fol-
lowing. If we have a distribution that satisﬁes IPr(X, Z, Y) and IPr(X, Z, W) but not
IPr(X, Z, Y ∪W), there could not exist a DAG G that induces Pr and at the same time
satisﬁes dsepG(X, Z, Y) and dsepG(X, Z, W).
The d-separation test satisﬁes additional properties beyond composition that do not
hold for arbitrary distributions. For example, it satisﬁes intersection:
dsep(X, Z ∪W, Y) and dsep(X, Z ∪Y, W) only if dsep(X, Z, Y ∪W).
It also satisﬁes chordality:
dsep(X, {Z, W}, Y) and dsep(W, {X, Y}, Z) only if dsep(X, Z, Y) or dsep(X, W, Y).
4.6 More on DAGs and independence
We deﬁne in this section a few notions that are quite useful in describing the relationship
between the independence statements declared by a DAG and those declared by a prob-
ability distribution. We use these notions to state a number of results, including some on
the expressive power of DAGs as a language for capturing independence statements.
Let G be a DAG and Pr be a probability distribution over the same set of variables. We
will say that G is an independence map (I-MAP) of Pr iff
dsepG(X, Z, Y) only if IPr(X, Z, Y),
that is, if every independence declared by d-separation on G holds in the distribution Pr.
An I-MAP G is minimal if G ceases to be an I-MAP when we delete any edge from G.

4.6 MORE ON DAGS AND INDEPENDENCE
69
By the semantics of Bayesian networks, if Pr is induced by a Bayesian network (G, ),
then G must be an I-MAP of Pr, although it may not be minimal (see Exercise 4.5).
We will also say that G is a dependency map (D-MAP) of Pr iff
IPr(X, Z, Y) only if dsepG(X, Z, Y).
That is, the lack of d-separation in G implies a dependence in Pr, which follows from
the contraposition of the above condition. Again, we have seen previously that if Pr is a
distribution induced by a Bayesian network (G, ), then G is not necessarily a D-MAP
of Pr. However, we mentioned that G can be made a D-MAP of Pr if we choose the
parametrization  carefully.
If DAG G is both an I-MAP and a D-MAP of distribution Pr, then G is called a
perfect map (P-MAP) of Pr. Given these notions, our goal in this section is to answer two
basic questions. First, is there always a P-MAP for any distribution Pr? Second, given
a distribution Pr, how can we construct a minimal I-MAP of Pr? Both questions have
practical signiﬁcance and are discussed next.
4.6.1 Perfect MAPs
If we are trying to construct a probability distribution Pr using a Bayesian network
(G, ), then we want DAG G to be a P-MAP of the induced distribution to make all
the independencies of Pr accessible to the d-separation test. However, there are prob-
ability distributions Pr for which there are no P-MAPs. Suppose for example that we
have four variables, X1, X2, Y1, Y2, and a distribution Pr that only satisﬁes the following
independencies:
IPr(X1, {Y1, Y2}, X2)
IPr(X2, {Y1, Y2}, X1)
IPr(Y1, {X1, X2}, Y2)
IPr(Y2, {X1, X2}, Y1).
(4.15)
It turns out there is no DAG that is a P-MAP of Pr in this case.
This result should not come as a surprise since the independencies captured by DAGs
satisfy properties – such as intersection, composition, and chordality – that are not satisﬁed
by arbitrary probability distributions. In fact, the non existence of a P-MAP for the previous
distribution Pr follows immediately from the fact that Pr violates the chordality property. In
particular, the distribution satisﬁes I(X1, {Y1, Y2}, X2) and I(Y1, {X1, X2}, Y2). Therefore,
if we have a DAG that captures these two independencies, it must then satisfy either
I(X1, Y1, X2) or I(X1, Y2, X2) by chordality. Since neither of these are satisﬁed by Pr,
there exists no DAG that is a P-MAP of Pr.
4.6.2 Independence MAPs
We now consider another key question relating to I-MAPs. Given a distribution Pr,
how can we construct a DAG G that is guaranteed to be a minimal I-MAP of Pr? The
signiﬁcance of this question stems from the fact that minimal I-MAPs tend to exhibit
more independence, therefore requiring fewer parameters and leading to more compact
Bayesian networks (G, ) for distribution Pr.

70
BAYESIAN NETWORKS
Earthquake?
(E)
Burglary?
(B)
Alarm?
(A)
Call?
(C)
Radio?
(R)
Figure 4.13: An I-MAP.
The following is a simple procedure for constructing a minimal I-MAP of a distribution
Pr given an ordering X1, . . . , Xn of the variables in Pr. We start with an empty DAG G
(no edges) and then consider the variables Xi one by one for i = 1, . . . , n. For each
variable Xi, we identify a minimal subset P of the variables in X1, . . . , Xi−1 such that
IPr(Xi, P, {X1, . . . , Xi−1} \ P) and then make P the parents of Xi in DAG G. The resulting
DAG is then guaranteed to be a minimal I-MAP of Pr.
For an example of this procedure, consider the DAG G in Figure 4.1 and suppose that it
is a P-MAP of some distribution Pr. This supposition allows us to reduce the independence
test required by the procedure on distribution Pr, IPr(Xi, P, {X1, . . . , Xi−1} \ P), into an
equivalent d-separation test on DAG G, dsepG(Xi, P, {X1, . . . , Xi−1} \ P). Our goal then
is to construct a minimal I-MAP G′ for Pr using the previous procedure and order
A, B, C, E, R. The resulting DAG G′ is shown in Figure 4.13. This DAG was constructed
according to the following details:
r Variable A was added with P = ∅.
r Variable B was added with P = A, since dsepG(B, A, ∅) holds and dsepG(B, ∅, A) does
not.
r Variable C was added with P = A, since dsepG(C, A, B) holds and dsepG(C, ∅, {A, B})
does not.
r Variable E was added with P = A, B since this is the smallest subset of A, B, C such
that dsepG(E, P, {A, B, C} \ P) holds.
r Variable R was added with P = E since this is the smallest subset of A, B, C, E such
that dsepG(R, P, {A, B, C, E} \ P) holds.
The resulting DAG G′ is guaranteed to be a minimal I-MAP of the distribution Pr. That
is, whenever X and Y are d-separated by Z in G′, we must have the same for DAG G and,
equivalently, that X and Y are independent given Z in Pr. Moreover, this ceases to hold
if we delete any of the ﬁve edges in G′. For example, if we delete the edge E ←B, we
will have dsepG′(E, A, B) yet dsepG(E, A, B) does not hold in this case. Note that the
constructed DAG G′ is incompatible with common perceptions of causal relationships in
this domain – see the edge A →B for an example – yet it is sound from an independence
viewpoint. That is, a person who accepts the DAG in Figure 4.1 cannot disagree with any
of the independencies implied by Figure 4.13.
The minimal I-MAP of a distribution is not unique as we may get different results
depending on the variable ordering with which we start. Even when using the same variable
ordering, it is possible to arrive at different minimal I-MAPs. This is possible since we may

4.6 MORE ON DAGS AND INDEPENDENCE
71
have multiple minimal subsets P of {X1, . . . , Xi−1} for which IPr(Xi, P, {X1, . . . , Xi−1} \
P) holds. As it turns out, this can only happen if the probability distribution Pr represents
some logical constraints. Hence, we can ensure the uniqueness of a minimal I-MAP for
a given variable ordering if we restrict ourselves to strictly positive distributions (see
Exercise 4.17).
4.6.3 Blankets and boundaries
A ﬁnal important notion we shall discuss is the Markov blanket:
Deﬁnition 4.3. Let Pr be a distribution over variables X. A Markov blanket for a variable
X ∈X is a set of variables B ⊆X such that X ̸∈B and IPr(X, B, X \ B \ {X}).
■
That is, a Markov blanket for X is a set of variables that, when known, will render every
other variable irrelevant to X. A Markov blanket B is minimal iff no strict subset of B is
also a Markov blanket. A minimal Markov blanket is known as a Markov boundary. Again,
it turns out that the Markov boundary for a variable is not unique unless the distribution
is strictly positive.
Corollary 1. If Pr is a distribution induced by DAG G, then a Markov blanket for variable
X with respect to distribution Pr can be constructed using its parents, children, and spouses
in DAG G. Here variable Y is a spouse of X if the two variables have a common child in
DAG G.
■
This result holds because X is guaranteed to be d-separated from all other nodes given
its parents, children, and spouses. To show this, suppose that we delete all edges leaving
the parents, children, and spouses of X. Node X will then be disconnected from all nodes
in the given DAG except for its children. Hence, by Theorem 4.1 X is guaranteed to be
d-separated from all other nodes given its parents, children, and spouses. For an example,
consider node C in Figure 4.2 and the set B = {S, P, T } constituting its parents, children,
and spouses. If we delete the edges leaving nodes in B, we ﬁnd that node C is disconnected
from all other nodes except its child P. Similarly, in Figure 4.3 the set {St−1, St+1, Ot}
forms a Markov blanket for every variable St where t > 1.
Bibliographic remarks
The term “Bayesian network” was coined by Judea Pearl [Pearl, 1985] to emphasize
three aspects: the often subjective nature of the information used to construct them; the
reliance on Bayes’s conditioning when performing inference; and the ability to support
both causal and evidential reasoning, a distinction underscored by Thomas Bayes [Bayes,
1963]. Bayesian networks are called probabilistic networks in Cowell et al. [1999] and
DAG models in Edwards [2000], Lauritzen [1996], and Wasserman [2004]. Nevertheless,
“Bayesian networks” remains to be one of the most common terms for denoting these
networks in the AI literature [Pearl, 1988; Jensen and Nielsen, 2007; Neapolitan, 2004],
although other terms, such as belief networks and causal networks, are also frequently
used.
The graphoid axioms were identiﬁed initially in Dawid [1979] and Spohn [1980], and
then rediscovered by Pearl and Paz [1986; 1987], who introduced the term “graphoids,”
noticing their connection to separation in graphs, and who also conjectured their complete-
ness as a characterization of probabilistic independence. The conjecture was later falsiﬁed

72
BAYESIAN NETWORKS
A
C
B
D
G
H
E
F
A
A
1
.2
0
.8
B
B
1
.7
0
.3
B
E
E|B
1
1
.1
1
0
.9
0
1
.9
0
0
.1
A
B
D
D|AB
1
1
1
.5
1
1
0
.5
1
0
1
.6
1
0
0
.4
0
1
1
.1
0
1
0
.9
0
0
1
.8
0
0
0
.2
Figure 4.14: A Bayesian network with some of its CPTs.
by Studeny [1990]. The d-separation test was ﬁrst proposed by Pearl [1986b] and its
soundness based on the graphoid axioms was shown in Verma [1986]; see also Verma and
Pearl [1990a;b]. The algorithm for constructing minimal I-MAPs is discussed in Verma
and Pearl [1990a]. An in-depth treatment of probabilistic and graphical independence is
given in Pearl [1988].
4.7 Exercises
4.1. Consider the DAG in Figure 4.14:
(a) List the Markovian assumptions asserted by the DAG.
(b) Express Pr(a, b, c, d, e, f, g, h) in terms of network parameters.
(c) Compute Pr(A = 0, B = 0) and Pr(E = 1 | A = 1). Justify your answers.
(d) True or false? Why?
r dsep(A, BH, E)
r dsep(G, D, E)
r dsep(AB, F, GH)
4.2. Consider the DAG G in Figure 4.15. Determine if any of dsepG(Ai, ∅, Bi), dsepG(Ai, ∅, Ci),
or dsepG(Bi, ∅, Ci) hold for i = 1, 2, 3.
4.3. Show that every root variable X in a DAG G is d-separated from every other root variable Y .

4.7 EXERCISES
73
A1
A2
A3
B1
B2
B3
C1
C2
C3
Figure 4.15: A directed acyclic graph.
4.4. Consider a Bayesian network over variables X, S that induces a distribution Pr. Suppose that
S is a leaf node in the network that has a single parent U ∈X. For a given value s of variable
S, show that Pr(x|s) does not change if we change the CPT of variable S as follows:
θ′
s|u = η θs|u
for all u and some constant η > 0.
4.5. Consider the distribution Pr deﬁned by Equation 4.2 and DAG G. Show the following:
(a) 
z Pr(z) = 1.
(b) Pr satisﬁes the independencies in Markov(G).
(c) Pr(x|u) = θx|u for every value x of variable X and every instantiation u of its parents U.
4.6. Use the graphoid axioms to prove dsepG(S1, S2, {S3, . . . , Sn}) in the DAG G of Figure 4.11.
Assume that you are given the Markovian assumptions for DAG G.
4.7. Show that dsepG(X, Z, Y) can be decided in time and space that are linear in the size of
DAG G based on Theorem 4.1.
4.8. Show that the graphoid axioms imply the chain rule:
I(X, Y, Z) and I(X ∪Y, Z, W) only if I(X, Y, W).
4.9. Prove that the graphoid axioms hold for probability distributions, and that the intersection
axiom holds for strictly positive distributions.
4.10. Provide a probability distribution over three variables X, Y , and Z that violates the composition
axiom. That is, show that IPr(Z, ∅, X) and IPr(Z, ∅, Y) but not IPr(Z, ∅, XY). Hint: Assume
that X and Y are inputs to a noisy gate and Z is its output.
4.11. Provide a probability distribution over three variables X, Y , and Z that violates the intersection
axiom. That is, show that IPr(X, Z, Y) and IPr(X, Y, Z) but not IPr(X, ∅, YZ).
4.12. Construct two distinct DAGs over variables A, B, C, and D. Each DAG must have exactly
four edges and the DAGs must agree on d-separation.
4.13. Prove that d-separation satisﬁes the properties of intersection and chordality.
4.14. Consider the DAG G in Figure 4.4. Suppose that this DAG is a P-MAP of some distribution
Pr. Construct a minimal I-MAP G′ for Pr using each of the following variable orders:
(a) A, D, B, C, E
(b) A, B, C, D, E
(c) E, D, C, B, A
4.15. Identify a DAG that is a D-MAP for all distributions Pr over variables X. Similarly, identify
another DAG that is an I-MAP for all distributions Pr over variable X.
4.16. Consider the DAG G in Figure 4.15. Suppose that this DAG is a P-MAP of a distribution Pr.
(a) What is the Markov boundary for the variable C2?

74
BAYESIAN NETWORKS
(b) Is the Markov boundary of A1 a Markov blanket of B3?
(c) Which variable has the smallest Markov boundary?
4.17. Prove that for strictly positive distributions, if B1 and B2 are Markov blankets for some
variable X, then B1 ∩B2 is also a Markov blanket for X. Hint: Appeal to the intersection
axiom.
4.18. (After Pearl) Consider the following independence statements: I(A, ∅, B) and I(AB, C, D).
(a) Find all independence statements that follow from these two statements using the positive
graphoid axioms.
(b) Construct minimal I-MAPs of the statements in (a) (original and derived) using the following
variable orders:
r A, B, C, D
r D, C, B, A
r A, D, B, C
4.19. Assume that the algorithm in Section 4.6.2 is correct as far as producing an I-MAP G for the
given distribution Pr. Prove that G must also be a minimal I-MAP.
4.20. Suppose that G is a DAG and let W be a set of nodes in G with deterministic CPTs (i.e.,
their parameters are either 0 or 1). Propose a modiﬁcation to the d-separation test that
can take advantage of nodes W and that will be stronger than d-separation (i.e., discover
independencies that d-separation cannot discover).
4.21. Let Pr be a probability distribution over variables X and let B be a Markov blanket for variable
X. Show the correctness of the following procedure for ﬁnding a Markov boundary for X.
r
Let R be X \ ({X} ∪B).
r
Repeat until every variable in B has been examined or B is empty:
1.
Pick a variable Y in B.
2.
Test whether IPr(X, B \ {Y}, R ∪{Y}).
3.
If the test succeeds, remove Y from B, add it to R, and go to Step 1.
r
Declare B a Markov boundary for X and exit.
Hint: Appeal to the weak union axiom.
4.22. Show that every probability distribution Pr over variables X1, . . . , Xn can be induced by some
Bayesian network (G, ) over variables X1, . . . , Xn. In particular, show how (G, ) can be
constructed from Pr.
4.23. Let G be a DAG and let G′ by an undirected graph generated from G as follows:
1. For every node in G, every pair of its parents are connected by an undirected edge.
2. Every directed edge in G is converted into an undirected edge.
For every variable X, let BX be its neighbors in G′ and ZX be all variables excluding X and
BX. Show that X and ZX are d-separated by BX in DAG G.
4.24. Let G be a DAG and let X, Y, and Z be three disjoint sets of nodes in G. Let G′ be an
undirected graph constructed from G according to the following steps:
1. Every node is removed from G unless it is in X ∪Y ∪Z or one of its descendants is in
X ∪Y ∪Z.
2. For every node in G, every pair of its parents are connected by an undirected edge.
3. Every directed edge in G is converted into an undirected edge.
Show that dsepG(X, Z, Y) if and only if X and Y are separated by Z in G′ (i.e., every path
between X and Y in G′ must pass through Z).

4.8 PROOFS
75
4.25. Let X and Y be two nodes in a DAG G that are not connected by an edge. Let Z be a set of
nodes deﬁned as follows: Z ∈Z if and only if Z ̸∈{X, Y} and Z is an ancestor of X or an
ancestor of Y . Show that dsepG(X, Z, Y).
4.8 Proofs
PROOF OF THEOREM 4.1. Suppose that X and Y are d-separated by Z in G. Every path
α between X and Y must then be blocked by Z. We show that path α will not appear in G′
(one of its nodes or edges will be pruned) and, hence, X and Y cannot be connected in G′.
We ﬁrst note that α must have at least one internal node. Moreover, we must have one
of the following cases:
1. For some sequential valve →W→or divergent valve ←W→on path α, variable W
belongs to Z. In this case, the outgoing edges of W will be pruned and not exist in G′.
Hence, the path α cannot be part of G′.
2. For all sequential and divergent valves →W→and ←W→on path α, variable W is not
in Z. We must then have some convergent valve →W←on α where neither W nor one of
its descendants are in Z. Moreover, for at least one of these valves →W←, no descendant
of W can belong to X ∪Y.4 Hence, W will be pruned and not appear in G′. The path α
will then not be part of G′.
Suppose now that X and Y are not d-separated by Z in G. There must exist a path α
between X and Y in G that is not blocked by Z. We now show that path α will appear in
G′ (none of its nodes or edges will be pruned). Hence, X and Y must be connected in G′.
If path α has no internal nodes, the result follows immediately; otherwise, no node of
path α will be pruned for the following reason. If the node is part of a convergent valve
→W←, then W or one of its descendants must be in Z and, hence, cannot be pruned. If
the node is part of a sequential or divergent valve, →W→or ←W→, then moving away
from W in the direction of an outgoing edge will either:
1. Lead us to X or to Y in a directed path, which means that W has a descendant in X ∪Y
and will therefore not be pruned.
2. Lead us to a convergent valve →W ′←, which must be either in Z or has a descendant in
Z. Hence, node W will have a descendant in Z and cannot be pruned.
No edge on the path α will be pruned for the following reason. For the edge to be pruned,
it must be outgoing from a node W in Z, which must then be part of a sequential or
divergent valve on path α. But this is impossible since all sequential and divergent valves
on α are unblocked.
■
PROOF OF THEOREM 4.2. The proof of this theorem is given in Verma [1986]; see also
Verma and Pearl [1990a;b].
■
PROOF OF THEOREM 4.3. The proof of this theorem is given in Geiger and Pearl
[1988a]; see also Geiger and Pearl [1988b].
■
4 Consider a valve (→W←) on the path α: Xγ →W←βY. Suppose that W has a descendant in, say, Y. We then
have a path from X through γ and W and then directed to Y that has at least one less convergent valve than α. By
repeating the same argument on this new path, we must either encounter a convergent valve that has no descendant
in X ∪Y or establish a path between X and Y that does not have a convergent valve (the path would then be
unblocked, which is a contradiction).

5
Building Bayesian Networks
We address in this chapter a number of problems that arise in real-world applications,
showing how each can be solved by modeling and reasoning with Bayesian networks.
5.1 Introduction
We consider a number of real-world applications in this chapter drawn from the domains
of diagnosis, reliability, genetics, channel coding, and commonsense reasoning. For each
one of these applications, we state a speciﬁc reasoning problem that can be addressed by
posing a formal query with respect to a corresponding Bayesian network. We discuss the
process of constructing the required network and then identify the speciﬁc queries that
need to be applied.
There are at least four general types of queries that can be posed with respect to a
Bayesian network. Which type of query to use in a speciﬁc situation is not always trivial
and some of the queries are guaranteed to be equivalent under certain conditions. We deﬁne
these query types formally in Section 5.2 and then discuss them and their relationships in
more detail when we go over the various applications in Section 5.3.
The construction of a Bayesian network involves three major steps. First, we must
decide on the set of relevant variables and their possible values. Next, we must build the
network structure by connecting the variables into a DAG. Finally, we must deﬁne the
CPT for each network variable. The last step is the quantitative part of this construction
process and can be the most involved in certain situations. Two of the key issues that arise
here are the potentially large size of CPTs and the signiﬁcance of the speciﬁc numbers
used to populate them. We present techniques for dealing with the ﬁrst issue in Section 5.4
and for dealing with the second issue in Section 5.5.
5.2 Reasoning with Bayesian networks
To ground the discussion of this section in concrete examples, we ﬁnd it useful to make
reference to a software tool for modeling and reasoning with Bayesian networks. A
screenshot of one such tool, SamIam,1 is depicted in Figure 5.1. This ﬁgure shows a
Bayesian network, known as “Asia,” that will be used as a running example throughout
this section.2
5.2.1 Probability of evidence
One of the simplest queries with respect to a Bayesian network is to ask for the probability
of some variable instantiation e, Pr(e). For example, in the Asia network we may be
1 SamIam is available at http://reasoning.cs.ucla.edu/samiam/.
2 This network is available with the SamIam distribution.
76

5.2 REASONING WITH BAYESIAN NETWORKS
77
Figure 5.1: A screenshot of the Asia network from SamIam.
interested in knowing the probability that the patient has a positive x-ray but no dyspnoea,
Pr(X=yes, D =no). This can be computed easily by tools such as SamIam, leading to
a probability of about 3.96%. The variables E = {X, D} are called evidence variables
in this case and the query Pr(e) is known as a probability-of-evidence query, although
it refers to a very speciﬁc type of evidence corresponding to the instantiation of some
variables.
There are other types of evidence beyond variable instantiations. In fact, any proposi-
tional sentence can be used to specify evidence. For example, we may want to know the
probability that the patient has either a positive x-ray or dyspnoea, X=yes ∨D =yes.
Bayesian network tools do not usually provide direct support for computing the probabil-
ity of arbitrary pieces of evidence but such probabilities can be computed indirectly using
the following technique.
We can add an auxiliary node E to the network, declare nodes X and D as the parents
of E, and then adopt the following CPT for E:3
X
D
E
Pr(e|x, d)
yes
yes
yes
1
yes
no
yes
1
no
yes
yes
1
no
no
yes
0
3 We have omitted redundant rows from the given CPT.

78
BUILDING BAYESIAN NETWORKS
Given this CPT, the event E =yes is then equivalent to X=yes ∨D =yes and, hence, we
can compute the probability of the latter by computing the probability of the former.
This method, known as the auxiliary-node method, is practical only when the number of
evidence variables is small enough, as the CPT size grows exponentially in the number of
these variables. However, this type of CPT is quite special as it only contains probabilities
equal to 0 or 1. When a CPT satisﬁes this property, we say that it is deterministic. We
also refer to the corresponding node as a deterministic node. In Section 5.4, we present
some techniques for representing deterministic CPTs that do not necessarily suffer from
this exponential growth in size.
We note here that in the literature on Bayesian network inference, the term “evidence”
is almost always used to mean an instantiation of some variables. Since any arbitrary piece
of evidence can be modeled using an instantiation (of some auxiliary variable), we will
also keep to this usage unless stated otherwise.
5.2.2 Prior and posterior marginals
If probability-of-evidence queries are one of the simplest, then posterior-marginal queries
are one of the most common. We ﬁrst explain what is meant by the terms “posterior” and
“marginal” and then explain this common class of queries.
Marginals
Given a joint probability distribution Pr(x1, . . . , xn), the marginal distribution
Pr(x1, . . . , xm), m ≤n, is deﬁned as follows:
Pr(x1, . . . , xm) =

xm+1,...,xn
Pr(x1, . . . , xn).
That is, the marginal distribution can be viewed as a projection of the joint distribution on
the smaller set of variables X1, . . . , Xm. In fact, most often the set of variables X1, . . . , Xm
is small enough to allow an explicit representation of the marginal distribution in tabular
form (which is usually not feasible for the joint distribution).
When the marginal distribution is computed given some evidence e,
Pr(x1, . . . , xm|e) =

xm+1,...,xn
Pr(x1, . . . , xn|e),
it is known as a posterior marginal. This is to be contrasted with the marginal distribution
given no evidence, which is known as a prior marginal.
Figure 5.2 depicts a screenshot where the prior marginals are shown for every variable in
the network. Figure 5.3 depicts another screenshot of SamIam where posterior marginals
are shown for every variable given that the patient has a positive x-ray but no dyspnoea,
e: X=yes, D =no. The small windows containing marginals in Figures 5.2 and 5.3 are
known as monitors and are quite common in tools for reasoning with Bayesian networks.
According to these monitors, we have the following prior and posterior marginals for lung
cancer, C, respectively:
C
Pr(C)
yes
5.50%
no
94.50%
C
Pr(C|e)
yes
25.23%
no
74.77%

5.2 REASONING WITH BAYESIAN NETWORKS
79
Figure 5.2: Prior marginals in the Asia network.
Figure 5.3: Posterior marginals in the Asia network given a positive x-ray and no dyspnoea.

80
BUILDING BAYESIAN NETWORKS
Figure 5.4: Representing soft evidence on variable E using auxiliary variable V .
Soft evidence
We have seen in Section 3.6.4 how soft evidence can be reduced to hard evidence on a noisy
sensor. This approach can be easily adopted in the context of Bayesian networks by adding
auxiliary nodes to represent such noisy sensors. Suppose for example that we receive soft
evidence that doubles the odds of a positive x-ray or dyspnoea, X=yes ∨D =yes. In
the previous section, we showed that this disjunction can be represented explicitly in the
network using the auxiliary variable E. We can also represent the soft evidence explicitly
by adding another auxiliary variable V to represent the state of a noisy sensor, as shown
in Figure 5.4. The strength of the soft evidence is then captured by the CPT of variable
V , as discussed in Section 3.6.4. In particular, all we have to do is choose a CPT with a
false positive rate fp and a false negative rate fn such that
1 −fn
fp
= k+,
where k+ is the Bayes factor quantifying the strength of the soft evidence. That is, the
CPT for E should satisfy
1 −θ V=no| E=yes
θ V=yes| E=no
= θ V=yes| E=yes
θ V=yes| E=no
= 2.
One choice for the CPT of variable V is4
E
V
θv|e
yes
yes
.8
no
yes
.4
4 Again, we are suppressing the redundant rows in this CPT.

5.2 REASONING WITH BAYESIAN NETWORKS
81
Figure 5.5: Asserting soft evidence on variable E by setting the value of auxiliary variable V .
We can then accommodate the soft evidence by setting the value of auxiliary variable V
to yes, as shown in Figure 5.5. Note the prior and posterior marginals over variable E,
which are shown in Figures 5.4 and 5.5, respectively:
E
Pr(E)
yes
47.56%
no
52.44%
E
Pr(E|V =yes)
yes
64.46%
no
35.54%
The ratio of odds is then
O(E =yes|V =yes)
O(E =yes)
= 64.46/35.54
47.56/52.44 ≈2.
Hence, the hard evidence V =yes leads to doubling the odds of E =yes, as expected.
As mentioned in Section 3.6.4, the method of emulating soft evidence by hard evidence
on an auxiliary node is also known as the method of virtual evidence.
5.2.3 Most probable explanation (MPE)
We now turn to another class of queries with respect to Bayesian networks: computing
the most probable explanation (MPE). The goal here is to identify the most probable
instantiation of network variables given some evidence. Speciﬁcally, if X1, . . . , Xn are
all the network variables and if e is the given evidence, the goal then is to identify an
instantiation x1, . . . , xn for which the probability Pr(x1, . . . , xn|e) is maximal. Such an
instantiation x1, . . . , xn will be called a most probable explanation given evidence e.
Consider Figure 5.6, which depicts a screenshot of SamIam after having computed the
MPE given a patient with positive x-ray and dyspnoea. According to the result of this

82
BUILDING BAYESIAN NETWORKS
Figure 5.6: Computing the MPE given a positive x-ray and dyspnoea.
query, the MPE corresponds to a patient that made no visit to Asia, is a smoker, and has
lung cancer and bronchitis but no tuberculosis.
It is important to note here that an MPE cannot be obtained directly from posterior
marginals. That is, if x1, . . . , xn is an instantiation obtained by choosing each value xi
so as to maximize the probability Pr(xi|e), then x1, . . . , xn is not necessarily an MPE.
Consider the posterior marginals in Figure 5.3 as an example. If we choose for each
variable the value with maximal probability, we get an explanation in which the patient is
a smoker:
α : A=no, S =yes, T =no, C =no, B =no, P =no, X=yes, D =no.
This instantiation has a probability of ≈20.03% given the evidence e: X=yes, D =no.
However, the most probable explanation given by Figure 5.7 is one in which the patient
is not a smoker:
α : A=no, S =no, T =no, C =no, B =no, P =no, X=yes, D =no.
This instantiation has a probability of ≈38.57% given evidence e: X=yes, D =no.
5.2.4 Maximum a posteriori hypothesis (MAP)
The MPE query is a special case of a more general class of queries for ﬁnding the most
probable instantiation of a subset of network variables. Speciﬁcally, suppose that the set of
all network variables is X and let M be a subset of these variables. Given some evidence
e, our goal is then to ﬁnd an instantiation m of variables M for which the probability
Pr(m|e) is maximal. Any instantiation m that satisﬁes the previous property is known as a
maximum a posteriori hypothesis (MAP). Moreover, the variables in M are known as MAP

5.2 REASONING WITH BAYESIAN NETWORKS
83
Figure 5.7: Computing the MPE given a positive x-ray and no dyspnoea.
variables. Clearly, MPE is a special case of MAP when the MAP variables include all
network variables. One reason why a distinction is made between MAP and MPE is that
MPE is much easier to compute algorithmically, an issue that we explain in Chapter 11.
Consider Figure 5.8 for an example of MAP. Here we have a patient with a positive
x-ray and no dyspnoea, so the evidence is X=yes, D =no. The MAP variables are
M = {A, S}, so we want to know the most likely instantiation of these variables given the
evidence. According to Figure 5.8, the instantiation
A=no, S =yes
is a MAP that happens to have a probability of ≈50.74% given the evidence.
A common method for approximating MAP is to compute an MPE and then return the
values it assigns to MAP variables. We say in this case that we are projecting the MPE
on MAP variables. However, we stress that this is only an approximation scheme as it
may return an instantiation of the MAP variables that is not maximally probable. Con-
sider again the MPE example from Figure 5.7, which gives the following most probable
instantiation:
A=no, S =no, T =no, C =no, B =no, P =no, X=yes, D =no
under evidence X=yes, D =no. Projecting this MPE on the variables M = {A, S}, we
get the instantiation
A=no, S =no,
which has a probability ≈48.09% given the evidence. This instantiation is clearly not a
MAP as we found a more probable instantiation earlier, that is, A=no, S =yes with a
probability of about 50.74%.

84
BUILDING BAYESIAN NETWORKS
Figure 5.8: Computing the maximum a posteriori hypothesis (MAP) given a positive x-ray and no
dyspnoea.
There is a relatively general class of situations in which the solution to a MAP query
can be obtained immediately from an MPE solution by projecting it on the MAP variables.
To formally deﬁne this class of situations, let E be the evidence variables, M be the MAP
variables, and Y be all other network variables. The condition is that there is at most one
instantiation y of variables Y that is compatible with any particular instantiations m and
e of variables M and E, respectively. More formally, if Pr(m, e) > 0, then Pr(m, e, y) ̸=
0 for exactly one instantiation y (see Exercise 5.11). We later discuss two classes of
applications where this condition can be satisﬁed: diagnosis of digital circuits and channel
coding.
5.3 Modeling with Bayesian networks
We discuss in this section a number of reasoning problems that arise in real-world ap-
plications and show how each can be addressed by ﬁrst modeling the problem using a
Bayesian network and then posing one of the queries deﬁned in the previous section.
Before we proceed with this modeling exercise, we need to state some general modeling
principles that we adhere to in all our examples. Speciﬁcally, each Bayesian network will
be constructed in three consecutive steps.
Step 1: Deﬁne the network variables and their values. We partition network variables
into three types: query, evidence, and intermediary variables. A query variable is one that
we need to ask questions about, such as compute its posterior marginal. An evidence
variable is one about which we may need to assert evidence. Finally, an intermediary
variable is neither query nor evidence and is meant to aid the modeling process by detailing

5.3 MODELING WITH BAYESIAN NETWORKS
85
the relationship between evidence and query variables. Query and evidence variables are
usually immediately determined from the problem statement.5 Intermediary variables are
less obvious to determine and can depend on subtle modeling decisions. However, we
will provide some speciﬁc rules for when intermediary variables are necessary and when
they are not. Determining the values of variables may also not be that obvious and will be
dealt with in the context of speciﬁc examples.
Step 2: Deﬁne the network structure (edges). In all of our examples, we are guided
by a causal interpretation of network structure. Hence, the determination of this structure
is reduced to answering the following question about each network variable X: What is
the set of variables that we regard as the direct causes of X?
Step 3: Deﬁne the network CPTs. The difﬁculty and objectivity of this step varies
considerably from one problem to another. We consider some problems where the CPTs
are determined completely from the problem statement by objective considerations and
others where the CPTs are a reﬂection of subjective beliefs. In Chapters 17 and 18, we
also consider techniques for estimating CPTs from data.
5.3.1 Diagnosis I: Model from expert
The ﬁrst modeling example we consider is from medical diagnostics. Consider the fol-
lowing commonplace medical information:
The ﬂu is an acute disease characterized by fever, body aches, and pains, and can be associated
with chilling and a sore throat. The cold is a bodily disorder popularly associated with chilling
and can cause a sore throat. Tonsillitis is inﬂammation of the tonsils that leads to a sore throat
and can be associated with fever.
Our goal here is to develop a Bayesian network to capture this knowledge and use it
to diagnose the condition of a patient suffering from some of the symptoms mentioned
here.
Our ﬁrst step is to identify network variables, which as we mentioned previously fall
into three categories: query, evidence, and intermediary. To determine query variables,
we need to identify those events about which we need to ask questions. In this case,
we need to know whether the patient has a ﬂu, a cold, or tonsillitis, which suggests
three corresponding variables for this purpose; see Figure 5.9(a). To determine evidence
variables, we need to identify those events about which we can collect information. These
correspond to the different symptoms that a patient can exhibit: chilling, body ache and
pain, sore throat, and fever, which again leads to four corresponding variables, as shown in
Figure 5.9(a). The information given does not seem to suggest any intermediary variables,
so we do not include any. The values of each of the identiﬁed variables can be simply one
of two, true or false, although more reﬁned information may suggest different degrees of
body ache. Determining the network structure is relatively straightforward: There are no
causes for the different conditions and the cause of each symptom is immediate from the
given information.
5 Note that the distinction between query, evidence, and intermediary variables is not a property of the Bayesian
network but of the task at hand. Hence, one may redeﬁne these three sets of variables accordingly if the task
changes.

86
BUILDING BAYESIAN NETWORKS
Cold?
Flu?
Tonsillitis?
Sore Throat?
Fever?
Chilling?
Body Ache?
Condition
Sore Throat?
Fever?
Chilling?
Body Ache?
(a)
(b)
Figure 5.9: Two Bayesian network structures for medical diagnosis. The one on the right is known as a
naive Bayes structure.
It is tempting to have a different network structure for this problem, which is shown
in Figure 5.9(b). In this case, we decided to have one variable “Condition,” which has
multiple values: normal, cold, ﬂu, and tonsillitis. This network is clearly simpler than
the ﬁrst one and is an instance of a very common structure known as naive Bayes. More
generally, a naive Bayes structure has the following edges, C →A1, . . . , C →Am, where
C is called the class variable and A1, . . . , Am are called the attributes.
There is quite a bit of difference between the two structures in Figure 5.9 since the
naive Bayes structure makes a key commitment known as the single-fault assumption.
Speciﬁcally, it assumes that only one condition can exist in the patient at any time since
the multiple values of the “Condition” variable are exclusive of each other. This single-
fault assumption has implications that are inconsistent with the information given. For
example, it implies that if the patient is known to have a cold, then fever and sore throat
become independent since they are d-separated by the “Condition” variable. However,
this does not hold in the structure of Figure 5.9(a) as a fever may increase our belief in
tonsillitis, which could then increase our belief in a sore throat.
Here are some implications of this modeling inaccuracy. First, if the only evidence
we have is body ache, we expect the probability of ﬂu to increase in both networks. But
this will also lead to dropping the probabilities of cold and tonsillitis in the naive Bayes
structure yet these probabilities will remain the same in the other network since both cold
and tonsillitis are d-separated from body ache. Second, if all we know is that the patient
has no fever, then the belief in cold may increase in the naive Bayes structure, while it
is guaranteed to remain the same in the other structure since cold is d-separated from
fever.
We now turn to the speciﬁcation of CPTs for the developed network structure. The
main point here is that the CPTs for this network fall into one of two categories: the
CPTs for the various conditions and those for the symptoms. Speciﬁcally, the CPT for a
condition such as tonsillitis must provide the belief in developing tonsillitis by a person
about whom we have no knowledge of any symptoms. The CPT for a symptom such
as chilling must provide the belief in this symptom under the four possible conditions:
no cold and no ﬂu, cold and no ﬂu, no cold and ﬂu, or cold and ﬂu. The probabilities
needed for specifying these CPTs are usually obtained from a medical expert who supplies
this information based on known medical statistics or subjective beliefs gained through
practical experience (see also Section 5.4.1).

5.3 MODELING WITH BAYESIAN NETWORKS
87
Another key method for specifying the CPTs of this and similar networks is by estimat-
ing them directly from medical records of previous patients. These records may appear as
follows:
Case
Cold?
Flu?
Tonsillitis?
Chilling?
Bodyache?
Sorethroat?
Fever?
1
true
false
?
true
false
false
false
2
false
true
false
true
true
false
true
3
?
?
true
false
?
true
false
...
...
...
...
...
...
...
...
Each row in this table represents a medical case of a particular patient where “?” indicates
the unavailability of corresponding data for that patient. Many of the tools for Bayesian
network inference can take a table such as the one given and then generate a parametriza-
tion  of the given network structure that tries to maximize the probability of seeing the
given cases. In particular, if each case is represented by event di, then such tools will
generate a parametrization  that leads to a probability distribution Pr that attempts to
maximize the following quantity:
N

i=1
Pr(di).
Each term Pr(di) in this product represents the probability of seeing the case di and
the product itself represents the probability of seeing all of the N cases (assuming that
the cases are independent). Parameter estimation techniques are discussed at length in
Chapters 17 and 18.
We close this section by noting that a diagnostic problem for a particular patient
corresponds to a set of symptoms that represent the known evidence. The goal is then to
compute the most probable combination of conditions given the evidence. This can be
solved by posing a MAP query to the network with the MAP variables being cold, ﬂu,
and tonsillitis. If the evidence covers all of the four symptoms, the MAP query will then
reduce to an MPE query.
5.3.2 Diagnosis II: Model from expert
We now consider another problem from medicine that will serve to illustrate some major
issues that arise when constructing Bayesian networks:
A few weeks after inseminating a cow, we have three possible tests to conﬁrm pregnancy. The
ﬁrst is a scanning test that has a false positive of 1% and a false negative of 10%. The second
is a blood test that detects progesterone with a false positive of 10% and a false negative of
30%. The third test is a urine test that also detects progesterone with a false positive of 10%
and a false negative of 20%. The probability of a detectable progesterone level is 90% given
pregnancy and 1% given no pregnancy. The probability that insemination will impregnate a
cow is 87%.
Our task here is to build a Bayesian network and use it to compute the probability of
pregnancy given the results of some of these pregnancy tests.

88
BUILDING BAYESIAN NETWORKS
Pregnant?
(P)
Scanning Test
(S)
Urine Test
(U)
Blood Test
(B)
Progesterone Level
(L)
P
θp
yes
.87
P
S
θs|p
yes
−ve
.10
no
+ve
.01
P
L
θl|p
yes
undetectable
.10
no
detectable
.01
L
B
θb|l
detectable
−ve
.30
undetectable
+ve
.10
L
U
θu|l
detectable
−ve
.20
undetectable
+ve
.10
Figure 5.10: A Bayesian network for detecting pregnancy based on three tests. Redundant CPT rows
have been omitted.
The information given here suggests the following variables:
r One query variable to represent pregnancy (P)
r Three evidence variables to represent the results of various tests: scanning test (S), blood
test (B), and urine test (U)
r One intermediary variable to represent progesterone level (L).
Moreover, common understanding of causal relationships in this domain suggests the
causal structure in Figure 5.10, where pregnancy is a direct cause of both the scanning
test and progesterone level, which in turn is the direct cause of the blood and urine tests.
Some of the independencies implied by the constructed structure are:
r The blood and urine tests are independent, given the progesterone level.
r The scanning test is independent of the blood and urine tests, given the status of pregnancy.
Note, however, that blood and urine tests are not independent even if we know the status
of pregnancy since the result of one test will affect our belief in progesterone level, which
will then affect our belief in the second test’s outcome. The CPTs for this problem can be
speciﬁed directly from the problem statement as shown in Figure 5.10.
Suppose now that we inseminate a cow, wait for a few weeks, and then perform the
three tests, which all come out negative. Hence, the evidence we have is
e: S =−ve, B =−ve, U =−ve.
If we compute the posterior marginal for pregnancy given this evidence, we get
P
Pr(P|e)
yes
10.21%
no
89.79%

5.3 MODELING WITH BAYESIAN NETWORKS
89
Figure 5.11: Sensitivity analysis in SamIam: What are the single-parameter changes necessary to ensure
that the probability of pregnancy is no more than 5% given three negative tests?
Note that even though the probability of pregnancy is reduced from 87% to 10.21%, it is
still relatively high given that all three tests came out negative.
5.3.3 Sensitivity analysis
Suppose now that a farmer is not too happy with this result and would like three negative
tests to drop the probability of pregnancy to no more than 5%. Moreover, the farmer is
willing to replace the test kits for this purpose but needs to know the false positive and
negative rates of the new tests, which would ensure the constraint. This is a problem of
sensitivity analysis, discussed in Chapter 16, in which we try to understand the relationship
between the parameters of a Bayesian network and the conclusions drawn based on the
network. For a concrete example of this type of analysis, Figure 5.11 depicts a screenshot
of SamIam in which the following question is posed to the sensitivity analysis engine:
Which network parameter do we have to change, and by how much, to ensure that the
probability of pregnancy would be no more than 5% given three negative tests?
The previous query is implicitly asking for a single parameter change and Figure 5.11
portrays three possible changes, each of which is guaranteed to satisfy the constraint:6
1. If the false negative rate for the scanning test were about 4.63% instead of 10%
2. If the probability of pregnancy given insemination were about 75.59% instead of 87%
3. If the probability of a detectable progesterone level given pregnancy were about 99.67%
instead of 90%.
6 If multiple parameters can change simultaneously, the results would be different; see Chapter 16 for details.

90
BUILDING BAYESIAN NETWORKS
The last two changes are not feasible since the farmer does not intend to change the
insemination procedure nor does he control the progesterone level. Hence, he is left with
only one option: replace the scanning test by another that has a lower false negative rate.
What is interesting about these results of sensitivity analysis is that they imply that im-
proving either the blood test or the urine test cannot help. That is, the inherent uncertainty
in the progesterone level given pregnancy is such that even a perfect blood test or a perfect
urine test cannot help us in reaching the conﬁdence level we want. However, these tests
would become relevant if we were less ambitious. For example, if our goal is to drop the
probability of pregnancy to no more than 8% (instead of 5%), then SamIam identiﬁes the
following additional possibilities:
r The false negative for the blood test should be no more than about 12.32% instead of
30%.
r The false negative for the urine test should be no more than about 8.22% instead of 20%.
As is clear from this example, sensitivity analysis can be quite important when developing
Bayesian networks. We discuss this mode of analysis in some depth in Chapter 16.
5.3.4 Network granularity
The pregnancy problem provides an opportunity to discuss one of the central issues that
arises when building Bayesian networks (and models in general): How ﬁne-grained should
the network be?
Speciﬁcally, consider again the network in Figure 5.10 and note that progesterone level
(L) is neither a query variable nor an evidence variable. That is, we cannot observe the
value of this variable nor are we interested in making inferences about it. The question
then is: Why do we need to include it in the network?
Progesterone level is an intermediary variable that helps in modeling the relationship
between the blood and urine tests on the one hand and pregnancy on the other. It is
therefore a modeling convenience as it would be more difﬁcult to build the model without
including it explicitly. For example, the supplier of these tests may have only provided
their false positive and negative rates with respect to progesterone level and we may have
obtained the numbers relating progesterone to pregnancy from another source. Hence, the
inclusion of this intermediary variable in the network helps in integrating these two pieces
of information in a modular way. But now that we have the network in Figure 5.10, we
are able to compute the following quantities:
Pr(B =−ve|P =yes) = 36%
Pr(B =+ve|P =no) = 10.6%
Pr(U =−ve|P =yes) = 27%
Pr(U =+ve|P =no) = 10.7%,
which allow us to build the network in Figure 5.12, where the progesterone level is
no longer represented explicitly. The question now is whether this simpler network is
equivalent to the original one from the viewpoint of answering queries.
By examining the two structures, one can immediately detect a major discrepancy:
The simpler network in Figure 5.12 ﬁnds the blood and urine tests independent given
pregnancy, while the original one in Figure 5.10 does not. One practical implication of
this difference is that two blood and urine tests that are negative will count more in ruling
out a pregnancy in the simpler network than they would in the original one. Speciﬁcally,

5.3 MODELING WITH BAYESIAN NETWORKS
91
Pregnant?
(P)
Scanning Test
(S)
Urine Test
(U)
Blood Test
(B)
P
θp
yes
.87
P
S
θs|p
yes
−ve
.10
no
+ve
.01
P
B
θb|p
yes
−ve
.36
no
+ve
.106
P
U
θu|p
yes
−ve
.27
no
+ve
.107
Figure 5.12: A Bayesian network for detecting pregnancy based on three tests. Redundant CPT rows
have been omitted. Note: This is another example of a naive Bayes structure.
X
Y
U
V
Y
U
V
Figure 5.13: Bypassing the intermediary variable X in a Bayesian network.
the probability of pregnancy given these two negative tests is about 45.09% in the simpler
network, while it is about 52.96% in the original. Similarly, two positive tests will count
more in establishing a pregnancy in Figure 5.12 than they would in Figure 5.10. The
difference is not as dramatic in this case though – about 99.61% in the simpler network
versus 99.54% in the original one.
The moral of the previous example is that intermediary variables cannot be bypassed
in certain cases as that may lead to changing the model in some undesirable ways. Here,
the term “bypass” refers to the process of removing a variable, redirecting its parents to
its children, and then updating the CPTs of these children (as they now have different
parents).
As it turns out, one can identify a general case in which an intermediary variable can be
bypassed without affecting model accuracy, a concept that we will deﬁne formally next.
Suppose that Pr(.) is the distribution induced by a Bayesian network and let Pr′(.) be the
distribution induced by the new network after bypassing an intermediary variable. The
bypass procedure does not affect model accuracy in the case Pr(q, e) = Pr′(q, e) for all
instantiations of query variables Q and evidence variables E. This also implies that Pr(.)
and Pr′(.) will agree on every query formulated using these variables.
Suppose now that X is a variable that is neither a query variable nor an evidence
variable. Then X can be bypassed as long as it has a single child Y (see Figure 5.13). In
this case, the CPT for variable Y must be updated as follows:
θ′
y|uv =

x
θy|xvθx|u.
(5.1)
Here U are the parents of variable X and V are the parents of variable Y other than X. This
bypass can be justiﬁed using the techniques we introduce in Chapter 6 (see Exercise 6.4).

92
BUILDING BAYESIAN NETWORKS
A
B
E
D
C
X
Y
Z
A
B
C
D
E
X
Y
Z
(a) Digital circuit
(b) Bayesian network structure
Figure 5.14: Bayesian network from design.
We will also see some concrete examples of it in some of the problems that we discuss
later. We ﬁnally note that even though a variable may be bypassed without affecting the
model accuracy, one may wish not to bypass it simply because the bypass procedure will
lead to a large CPT. For example, if X and Y have ﬁve parents each, then Y will end up
having nine parents after the bypass.
5.3.5 Diagnosis III: Model from design
We now turn to another reasoning problem from the domain of diagnosis that differs from
previous problems in a fundamental way. Speciﬁcally, the Bayesian network model we
develop for this problem is quite general to the point that it can be generated automatically
for similar instances of the considered problem. The problem statement is as follows:
Consider the digital circuit Figure 5.14(a). Given some values for the circuit primary inputs
and output (test vector), our goal is to decide whether the circuit is behaving normally. If not,
our goal is to decide the most likely health states of its components.
The evidence variables for this problem are the primary inputs and output of the
circuit, A, B, and E, since we can observe their values. Given that our goal is to reason
about the health of components, we need to include a query variable for each of these
components: X, Y, and Z. That leaves the question of whether we need any intermediary
variables. The obvious candidates for such variables are the states of the internal wires,
C and D. As it turns out, modeling the circuit becomes much easier if we include these
variables, especially if we are dealing with more realistic circuits that may have hundreds
or thousands of components – it would be infeasible to model such circuits without
explicitly representing the states of internal wires. These choices lead to the variables
shown in the structure in Figure 5.14(b).
The edges of this structure are decided as follows. There are no direct causes for the
health state of each component, hence variables X, Y, and Z have no parents – a more
reﬁned model may include other conditions, such as circuit power, that may end up being
a direct cause of such variables. Similarly, there are no direct causes for the circuit primary
inputs, A and B, which is why they have no parents in the given structure. Consider now
variable D, which is the output of the and-gate. The direct causes of this variable are the
gate inputs, A and B, and its state of health, Y. Hence, these are the parents of D in the

5.3 MODELING WITH BAYESIAN NETWORKS
93
network structure. For the same reason, A and X are the parents of C, while C, D, and
Z are the parents of E. It should be clear that the developed structure generalizes easily
to other circuits regardless of the number and types of their gates. In fact, it generalizes
to any system that is composed of function blocks, where the outputs of each block are
determined by its inputs and its state of health – as long as the system does not contain
feedback loops, as that would lead to a cyclic structure.7
To completely specify the Bayesian network, we need to specify its CPTs. We also
have to decide on the values of different variables, which we avoided until now for a good
reason. First, the values of variables representing circuit wires – whether primary inputs,
outputs, or internal wires – are simply one of low or high. The choice of values for health
variables is not as obvious. The two choices are as follows. First, for each component its
health variable can only take one of two values, ok or faulty. However, the problem with
this choice is that the value faulty is too vague, as a component may fail in a number of
modes. For example, it is common to talk about stuck-at-zero faults in which the gate will
generate a low output regardless of its inputs. Similarly, one can have stuck-at-one faults
or input-output-short faults in which an inverter would simply short its input to its output.
From the viewpoint of precision, it is more appropriate to represent fault modes. Yet this
choice may seem to put more demands on us when specifying the CPTs, as we discuss
next.
First, note that the CPTs for this structure fall in one of three classes: CPTs for
variables representing primary inputs (A, B), CPTs for variables representing gate outputs
(C, D, E), and CPTs for variables representing component health (X, Y, Z). We will
consider each of these next.
The CPTs for health variables depend on the values we choose for these variables. If
we choose the values ok and faulty, then the CPT for each component, say, X would look
like this:
X
θx
ok
.99
faulty
.01
Hence, if we have the probability of a fault in each component, then all such CPTs are
determined immediately. If we choose to represent fault modes, say, stuckat0 and stuckat1,
then the CPT would look like this:
X
θx
ok
.99
stuckat0
.005
stuckat1
.005
which implies that we know the probabilities of various fault modes. Clearly, we can
assume that all fault modes are equally likely, in which case the probability of a faulty
component will again be enough to specify the CPT.
The CPTs for component outputs are straightforward in case we represent fault modes
since the probability of each possible output is then guaranteed to be either 0 or 1
(i.e., deterministic CPT) and can be determined directly from the gate’s functionality.
7 Systems with feedback loops can be modeled using Bayesian networks but require a different structure. See
Section 5.3.7 and the discussion on convolutional codes for an example of representing such systems using
Bayesian networks.

94
BUILDING BAYESIAN NETWORKS
For example, the following is the CPT for the inverter X (we are omitting redundant
rows):
A
X
C
θc|a,x
high
ok
high
0
low
ok
high
1
high
stuckat0
high
0
low
stuckat0
high
0
high
stuckat1
high
1
low
stuckat1
high
1
The CPTs for the other two gates can be speciﬁed similarly. If we choose to have only
two values for health variables, ok and faulty, then we need to decide on the probabilities
in case the gate is faulty:
A
X
C
θc|a,x
high
ok
high
0
low
ok
high
1
high
faulty
high
?
low
faulty
high
?
It is common to use a probability of .50 in this case and we show later that this choice
is equivalent in a precise sense to the previous choice of assigning equal probabilities to
fault modes.
We now move to the CPTs for primary inputs such as A, which require that we specify
tables such as this:
A
θa
high
.5
low
.5
We assumed here that a high input at A is equally likely as a low input. This appears
arbitrary at ﬁrst but the good news is that the choice for these CPTs does not matter for
the class of queries in which we are interested. That is, if our goal is to compute the
probability of some health state x, y, z given some test vector a, b, e, then this probability
is independent of Pr(a) and Pr(b), which can be chosen arbitrarily as long as they are not
extreme. To prove this in general, let the primary inputs be I, the primary outputs be O,
and the health variables be H. We then have
Pr(h|i, o) = Pr(h, i, o)
Pr(i, o)
by Bayes conditioning
= Pr(o|i, h)Pr(i|h)Pr(h)
Pr(o|i)Pr(i)
by the chain rule
= Pr(o|i, h)Pr(i)Pr(h)
Pr(o|i)Pr(i)
since I and H are d-separated
= Pr(o|i, h)Pr(h)
Pr(o|i)
.
Since both I and H are roots, Pr(o|i, h) does not depend on the CPTs for primary inputs I or
health variables H (see Exercise 5.12). Similarly, Pr(o|i) does not depend on the CPTs for

5.3 MODELING WITH BAYESIAN NETWORKS
95
primary inputs I. Moreover, Pr(h) depends only on the CPTs for health variables H since
these variables are independent of each other. Note that this proof implicitly assumed that
Pr(i, o) ̸= 0.
Note, however, that if our goal is to use the Bayesian network to predict the probability
that a certain wire, say, E is high, then the CPTs for primary inputs matter considerably.
But if this is our goal, it would then be reasonable to expect that we have some distribution
on the primary inputs; otherwise, we do not have enough information to answer the query
of interest.
Fault modes revisited
We now return to the two choices we considered when modeling component faults.
According to the ﬁrst choice, the health of each component, say, X had only two values,
ok and faulty, which leads to the following CPTs for the component health and output:
X
θx
ok
.99
faulty
.01
A
X
C
θc|a,x
high
ok
high
0
low
ok
high
1
high
faulty
high
.5
low
faulty
high
.5
According to the second choice, the health of each component had three values, ok,
stuckat0, and stuckat1, leading to the following CPTs:
X
θx
ok
.99
stuckat0
.005
stuckat1
.005
A
X
C
θc|a,x
high
ok
high
0
low
ok
high
1
high
stuckat0
high
0
low
stuckat0
high
0
high
stuckat1
high
1
low
stuckat1
high
1
We now show that these two choices are equivalent in the following sense. Since each
health variable has a single child (see Figure 5.14), we can bypass these variables as
suggested in Section 5.3.4. In particular, if we bypass variable X, then its single child C
will have only one parent A and the following CPT:
Pr(c|a) =

x
Pr(c|a, x)Pr(x).
If we bypass variable X, assuming it has values ok and faulty, we get the following CPT
for C:
A
C
θc|a
high
high
.005 = (0 ∗.99) + (.5 ∗.01)
low
high
.995 = (1 ∗.99) + (.5 ∗.01)
Similarly, if we bypass variable X, assuming it has values ok, stuckat0, and stuckat1, we
get the following CPT for C:
A
C
θc|a
high
high
.005 = (0 ∗.99) + (0 ∗.005) + (1 ∗.005)
low
high
.995 = (1 ∗.99) + (0 ∗.005) + (1 ∗.005)

96
BUILDING BAYESIAN NETWORKS
Hence, the two CPTs for variable C are the same. This would also be the case if we
bypass health variables Y and Z, leading to equivalent CPTs for each of variables D and
E. What this means is that the two Bayesian networks for the circuit in Figure 5.14 are
equivalent in the following sense: Any query that involves only the wires A, B, C, D, and
E is guaranteed to have the same answer with respect to either network. In fact, one can
even prove a more direct equivalence with respect to diagnosis tasks (see Exercise 5.15).
A diagnosis example
Suppose now that we observed the following test vector,
e: A=high, B =high, E =low,
and we wish to compute MAP over health variables X, Y, and Z under this observation.
According to the network with fault modes, we get two MAP instantiations:
MAP given e
X
Y
Z
ok
stuckat0
ok
ok
ok
stuckat0
each with a probability of ≈49.4%. We get effectively the same two instantiations with
respect to the second network with no fault modes:
MAP given e
X
Y
Z
ok
faulty
ok
ok
ok
faulty
where each MAP instantiation has also a probability of ≈49.4%. That is, we have two
most likely instantiations of the health variables, with the and-gate Y being faulty in one
and the or-gate Z being faulty in the other. Note here that we have assumed that all three
components have the same reliability of 99%.
Posterior marginals
It is instructive to examine the posterior marginals over the health variables X, Y, Z in
this case:
State
X
Y
Z
Pr(X, Y, Z|e)
1
ok
ok
ok
0
2
faulty
ok
ok
0
3
ok
faulty
ok
.49374
4
ok
ok
faulty
.49374
5
ok
faulty
faulty
.00499
6
faulty
ok
faulty
.00499
7
faulty
faulty
ok
.00249
8
faulty
faulty
faulty
.00005
This table reveals a number of interesting observations. First, State 2, in which X is faulty
but Y and Z are ok, is impossible; this follows from the circuit description. Second,
double fault scenarios are not all equally likely. For example, a double fault involv-
ing Y and Z is more likely than one involving Y and X under the given evidence.8
8 If Y and Z are faulty, we have two possible states for C and D (C =low, D either low or high). If Y and X are
faulty, we have only one possible state for C and D (C =low and D =low).

5.3 MODELING WITH BAYESIAN NETWORKS
97
A
B
C
D
E
X
Y
Z
A’
B’
C’
D’
E’
X’
Y’
Z’
A
B
C
D
E
X
Y
Z
A’
B’
C’
D’
E’
(a)
(b)
Figure 5.15: Two Bayesian network structures for diagnosing a circuit using two test vectors.
Due to this lack of symmetry, we have
Pr(Z = faulty|e) ≈50.38%
>
Pr(Y = faulty|e) ≈50.13%.
That is, among all possible states of the system states in which Z is faulty are more likely
than states in which Y is faulty. Hence, even though the two faults are symmetric when
considering most likely states of health variables (MAP), they are not symmetric when
considering posterior marginals.
Integrating time
We now turn to an extension of the diagnosis problem we considered thus far in which
we assume that we have two test vectors instead of only one. For example, to resolve the
ambiguity that results from the MAP query considered previously, suppose that we now
perform another test in which we apply two low inputs to the circuit and observe another
abnormal low output. Our goal is then to ﬁnd the most likely state of health variables
given these two test vectors.
The key point to realize here is that we now have six evidence variables instead of only
three as we need to capture the second test vector. This leads to three additional evidence
variables, A′, B′ and E′. The same applies to the intermediary variables, leading to two
additional variables, C′ and D′, which are needed to relate the elements of the second test
vector (see Figure 5.15). Whether we need to include additional health variables depends
on whether the health of a component stays the same during each of the two tests. If we
want to allow for the possibility of intermittent faults, where the health of a component
can change from one test to another, then we need to include additional health variables,
X′, Y ′, and Z′, as shown in Figure 5.15(a). Otherwise, the original health variables are
sufﬁcient, leading to the structure in Figure 5.15(b).
Assuming this latter structure, let us revisit our previous example with the following
two test vectors:
e: A=high, B =high, E =low
and
e′ : A=low, B =low, E =low.
If we compute MAP over health variables in this case, we obtain one instantiation:
MAP given e, e′
X
Y
Z
ok
ok
faulty
that has a probability of ≈97.53%.

98
BUILDING BAYESIAN NETWORKS
Power
Supply
Fan 1
Fan 2
Processor
1
Hard
Drive
Processor
2
Figure 5.16: A reliability block diagram.
If we decide to allow intermittent faults, then we need a new class of CPTs to completely
specify the resulting structure in Figure 5.15(a). In particular, for each component, say, X
we now need to specify a CPT such as this one:
X
X′
θx′|x
ok
ok
.99
ok
faulty
.01
faulty
ok
.001
faulty
faulty
.999
which represents a persistence model for the health of various components. For exam-
ple, this table says that there is a 99% chance that a healthy component would remain
healthy and there is a .1% chance that a faulty component would become healthy again
(intermittent fault).
We close this section by noting that the structures depicted in Figure 5.15 are known
as dynamic Bayesian network (DBN) structures since they include multiple copies of the
same variable, where the different copies represent different states of the variable over
time. We see more examples of dynamic Bayesian networks in future examples.
5.3.6 Reliability: Model from design
Consider the following problem from the domain of system reliability analysis:
Figure 5.16 depicts a reliability block diagram (RBD) of a computer system, indicating
conditions under which the system is guaranteed to be functioning normally (available). At
1,000 days since initial operation, the reliability of different components are as follows:
power supply is 99%, fan is 90%, processor is 96%, and hard drive is 98%. What is the
overall system reliability at 1,000 days since operation?
To address this problem, we need to provide an interpretation of an RBD. There are
many variations on RBDs but we focus here on the simplest type consisting of a DAG that
has a single leaf node and in which every node represents a block as given in Figure 5.16.
We interpret each block B as representing a subsystem that includes the component B
and the subsystems feeding into B. Moreover, for the subsystem represented by block B
to be available component B and at least one of the subsystems feeding into it must also
be available (see Exercise 5.9 for a more general model of availability). In Figure 5.16,
the block labeled “Processor 1” represents a subsystem that includes this processor, the

5.3 MODELING WITH BAYESIAN NETWORKS
99
Block B
Sub-System 1
Sub-System 2
Block B
Available
AND
OR
Sub-System 1
Available
Sub-System 2
Available
Figure 5.17: A Bayesian network fragment for a reliability block.
Hard
Drive (D)
Power
Supply (E)
A1
Fan 1 (F1)
Fan 2 (F2)
Processor 2 (P2)
Processor 1 (P1)
System (S)
A3
A2
A4
O1
O2
Figure 5.18: A Bayesian network structure for a reliability block diagram.
two fans, and the power supply. Moreover, the block labeled “Hard Drive” represents the
whole system, as it includes all components.
This interpretation suggests the construction in Figure 5.17 for converting a reliabil-
ity block into a Bayesian network fragment. To apply this construction to the RBD in
Figure 5.16, we need variables to represent the availability of each system component:
E for power supply, F1/F2 for the fans, P1/P2 for the processors, and D for the hard
drive. We also need a variable S that represents the availability of the whole system and
some intermediary variables to represent conjunctions and disjunctions, as suggested by
Figure 5.17. This leads to the Bayesian network structure in Figure 5.18.
Let us now consider the CPTs for this network. The root variables correspond to
system components, hence, their CPTs capture their reliability. For example, for the root
E we have
E
θe
avail
99%
The CPTs for other roots (components) are similar. Intermediary variables A1, . . . , A4
all represent and-gates. For example,
E
F1
A1
θa1|e,f1
avail
avail
true
1
avail
un avail
true
0
un avail
avail
true
0
un avail
un avail
true
0

100
BUILDING BAYESIAN NETWORKS
The CPT for variable S (system is available) is also an and-gate. Finally, the CPTs for
O1 (either fan subsystem is available) and O2 (either processor subsystem is available)
represent or-gates and can be speciﬁed similarly.
Given the Bayesian network presented here, we can compute system reliability by
simply computing the marginal for variable S (system is available). In this case, the
system reliability is ≈95.9%. Suppose now that we need to raise this system reliability
to 96.5% by replacing one of the components with a more reliable one. What are our
choices? We can use sensitivity analysis for this purpose, leading to three choices:
r Increase the reliability of the hard drive to ≈98.6%
r Increase the reliability of the power supply to ≈99.6%
r Increase the reliability of either fan to ≈96.2%.
What if we want to raise the system reliability to 97%? Sensitivity analysis tells us that
we have only one choice in this case: Increase the reliability of the hard drive to ≈99.1%.
Raising the system reliability to 98% would not be possible unless we change more than
one component simultaneously.
Suppose now that we found the system to be functioning abnormally at day 1,000. We
can then use MAP to ﬁnd the most likely explanation of this abnormality. We get a single
MAP answer in this case:
E
F1
F2
P1
P2
D
Pr(.|S = un avail)
avail
avail
avail
avail
avail
un avail
36%
Therefore, our MAP solution blames the hard drive. It is interesting to consider the next
two most likely explanations in this case:
E
F1
F2
P1
P2
D
Pr(.|S = un avail)
avail
un avail
un avail
avail
avail
avail
21.8%
un avail
avail
avail
avail
avail
avail
17.8%
Note that having two faulty fans is more likely than a power failure in this case yet not
as likely as having a hard drive failure. The prior example shows that we can use MAP
queries to identify the mostly likely component failures in case of an overall system
failure.
Logical constraints
Suppose now that we have a system constraint that precludes the ﬁrst fan from ever
operating concurrently with the ﬁrst processor. We would expect the overall system
reliability to be less than 95.9% in this case and our goal now is to compute this new
reliability.
The described situation involves a logical constraint over some network vari-
ables, ¬(F1 =avail ∧P1 =avail), which amounts to precluding some system states from
being possible. The most general way to impose this constraint is to introduce an
auxiliary variable, say, C in the network to represent this constraint (see Figure 5.19).

5.3 MODELING WITH BAYESIAN NETWORKS
101
Hard
Drive (D)
Power
Supply (E)
A1
Fan 1 (F1)
Fan 2 (F2)
Processor 2 (P2)
Processor 1 (P1)
System (S)
A3
A2
A4
O1
O2
C
Figure 5.19: A Bayesian network structure for a reliability block diagram involving a constraint over two
components.
The CPT for this variable is deterministic and given here:
F1
P1
C
θc|f1,p1
avail
avail
true
0
avail
un avail
true
1
un avail
avail
true
1
un avail
un avail
true
1
That is, C =true if and only if the constraint is satisﬁed. To enforce the constraint, we
must therefore set C to true before we compute the overall system reliability, which comes
out to 88.8% in this case.
Lifetime distributions and component reliability
We assumed previously that we are analyzing the system reliability at a given time (1,000
days) since the start of system operation. This allowed us to specify the reliability of
each component, which depends on the time the component has been in operation. The
dependence of component reliability on time is usually speciﬁed by a lifetime distribution,
R. In particular, if we let t be a continuous variable that represents time, then R(t) gives
the probability that the component will be functioning normally at time t. Hence, R(t)
is known as the component reliability at time t. A simple yet quite common lifetime
distribution is the exponential distribution, R(t) = e−λt, where λ represents the number
of failures per unit time (i.e., failure rate) – see Figure 5.20.
More generally, a life distribution R(t) is usually induced by a PDF f (t) that captures
unreliability information. In particular, the CDF F(t) induced by f (t),
F(t) =

 t
0
f (x)dx,
represents the probability that the component will fail by time t. This is called the
component unreliability. The component reliability is then
R(t) = 1 −F(t) = 1 −

 t
0
f (x)dx.

102
BUILDING BAYESIAN NETWORKS
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
reliability R(t)
time t
λ=1/10000
λ=1/5000
Figure 5.20: Exponential lifetime distributions for two different failure rates.
For example, the PDF for the exponential distribution is f (t) = λe−λt. This leads to the
following reliability function:
R(t) = 1 −

 t
0
λe−λxdx = 1 −(1 −e−λt) = e−λt.
5.3.7 Channel coding
Consider the following problem from the domain of channel coding:
We need to send four bits U1, U2, U3, and U4 from a source S to a destination D over a noisy
channel where there is a 1% chance that a bit will be inverted before it gets to the destination.
To improve the reliability of this process, we add three redundant bits X1, X2, and X3 to the
message where X1 is the XOR of U1 and U3, X2 is the XOR of U2 and U4, and X3 is the
XOR of U1 and U4. Given that we received a message containing seven bits at destination
D, our goal is to restore the message generated at the source S.
In channel coding terminology, the bits U1, . . . , U4 are known as information bits,
X1, . . . , X3 are known as redundant bits, and U1, . . . , U4, X1, . . . , X3 is known as the
code word or channel input. Moreover, the message received at the destination Y1, . . . , Y7
is known as the channel output. Our goal then is to restore the channel input given some
channel output.
As we have seen in previous examples, query and evidence variables are usually
determined immediately from the problem statement. In this case, evidence variables are
Y1, . . . , Y7 and they represent the bits received at destination D. Moreover, query variables
are U1, . . . , U4 and they represent the bits originating at source S. One can also include
the redundant bits X1, . . . , X3 in query variables or view them as constituting the set of
intermediary variables. We shall see later that this choice does not matter much so we will
include these bits in the query variables.
The causal structure for this problem is shown in Figure 5.21, where edges have been
determined based on a basic understanding of causality in this domain. Speciﬁcally, the

5.3 MODELING WITH BAYESIAN NETWORKS
103
1
U
2
U
3
U
4
U
1
X
2
X
3
X
1Y
2
Y
3Y
4
Y
5Y
6Y
7Y
Figure 5.21: A Bayesian network structure for modeling a coding problem.
direct causes of each redundant bit are its two corresponding information bits, and the
direct cause of each bit on the channel output is the corresponding bit on the channel
input. Information bits have no parents since they are not directly caused by any of the
other bits.
There are three CPT types in the problem. First, the CPT for each redundant bit, say,
X1 is deterministic and given as follows:
U1
U3
X1
θx1|u1,u3
1
1
1
0
1
0
1
1
0
1
1
1
0
0
1
0
This CPT simply captures the functional relationship between X1 and the corresponding
information bits U1 and U3. That is, Pr(x1|u1, u3) = 1 iff x1 = u1 ⊕u3 (⊕is the XOR
function).
The CPT for a channel output bit, say, Y1 is as follows:
U1
Y1
θy1|u1
1
0
.01
0
1
.01
This CPT is then capturing the simple noise model given in the problem statement but we
later discuss a more realistic noise model based on Gaussian distributions.
Finally, the CPTs for information bits such as U1 capture the distribution of messages
sent out from the source S. Assuming a uniform distribution, we can then use the following
CPT:
U1
θu1
1
.5
0
.5
MAP or posterior-marginal (PM) decoders?
Now that we have completely speciﬁed a Bayesian network for this decoding problem,
we need to decide on the speciﬁc query to pose. Our goal is to restore the channel input
given channel output but there are two ways for achieving this:

104
BUILDING BAYESIAN NETWORKS
1. Compute a MAP for the channel input U1, . . . , U4, X1, . . . , X3 given channel output
Y1, . . . , Y7.9
2. Compute the PM for each bit Ui/Xi in the channel input given channel output Y1, . . . , Y7,
and then select the value of Ui/Xi that is most probable.
The choice between MAP and PM decoders is a matter of the performance measure one
is interested in optimizing. We discuss two such measures in the following section and
relate them to MAP and PM decoders.
Evaluating decoders
Suppose that we have a decoder, which returns some channel input u1, . . . , u4, x1, . . . , x3
whenever it is given a channel output y1, . . . , y7. Suppose further that our goal is to
evaluate the performance of this decoder on a given set of channel inputs α1, . . . , αn.
There are two possible quality measures that we can use for this purpose:
r Word Error Rate (WER): Suppose that we send each input αi into the channel, collect the
corresponding (noisy) output βi, feed it into the decoder, and ﬁnally collect the decoder
output γi. We then compare each decoder output γi with corresponding channel input αi.
Let m be the number of channel inputs that are recovered incorrectly by the decoder, that
is, there is a mismatch on some bit between αi and γi. The word error rate for that decoder
is then m/n.
r Bit Error Rate (BER): We perform the same experiment as for WER, except that when
comparing the decoder output γi with the original channel input αi, we count the number
of bits on which they disagree. Let k be the total number of bits recovered incorrectly and
let l be the total number of bits sent across the channel. The bit error rate for the decoder
is then k/l.
Which performance measure to choose will depend on the application at hand. Suppose
for example that the information bits represent pixels in an image that contains the photo
of an individual to be recognized by a human when it arrives at the destination. Consider
now two situations. In the ﬁrst, we have a few bits off in each image we decode. In the
second, we have many images that are perfectly decoded but a few that have massive
errors in them. The BER would tend to be lower than the WER in the ﬁrst situation, while
it would tend to be larger in the second. Hence, our preference for one situation over the
other will imply a preference for one performance measure over the other.
The choice between MAP and PM decoders can therefore be thought of as a choice
between these performance measures. In particular, decoders based on MAP queries
minimize the average probability of word error, while decoders based on PM queries
minimize the average probability of bit error.
Noise models and soft evidence
Our previous discussion assumed a simple noise model according to which a channel
input bit xi is received as a channel output bit yi ̸= xi with 1% probability. A more
realistic and common noise model is to assume that we are transmitting our code bits
xi through a channel that adds Gaussian noise with mean xi and standard deviation σ
9 This is actually an MPE query since MAP variables include all variables except those for evidence. Even if the MAP
variables contain U1, . . . , U4 only, this MAP query can be obtained by projecting an MPE on the information bits
U1, . . . , U4 since the redundant bits X1, X2, X3 are functionally determined by the information bits U1, . . . , U4
(see Exercise 5.11).

5.3 MODELING WITH BAYESIAN NETWORKS
105
+
D2
D1
+
uk
x2k+1
x2k
b0
b1
b2
Figure 5.22: An example convolutional encoder. Each node denoted with a “+” represents a binary
addition, and each box Di represents a delay where the output of Di is the input of Di from the previous
encoder state.
(see Section 3.7.1). Speciﬁcally, we will assume that the channel output Yi is a continuous
variable governed by a conditional density function:
f (yi|xi) =
1
√
2πσ 2 e−(yi−xi)2/2σ 2.
As we have shown in Section 3.7.3, this more sophisticated noise model can be imple-
mented by interpreting the continuous channel output yi as soft evidence on the channel
input Xi =0 with a Bayes factor,
k = e(1−2yi)/2σ 2.
For example, if σ = .5 and we receive a channel output yi = .1, we interpret that as a soft
evidence on channel input Xi =0 with a Bayes factor k ≈5.
As shown in Section 5.2.2, this soft evidence can be integrated in our Bayesian network
by adding a child, say, Si of Xi and then setting its CPT such that
Pr(Si =true|Xi =0)
Pr(Si =true|Xi =1) = k = 5.
For example, either of the following CPTs will work:
Xi
Si
θsi|xi
0
true
.75
0
false
.25
1
true
.15
1
false
.85
Xi
Si
θsi|xi
0
true
.5
0
false
.5
1
true
.1
1
false
.9
as long as we emulate the channel output by setting the value of Si to true.
Convolutional codes
We will now discuss two additional types of coding networks: convolutional-code net-
works and turbo-code networks. The difference between these networks and the one we
discussed previously lies in the manner in which the redundant bits are generated. More-
over, both convolutional and turbo codes provide examples of modeling systems with
feedback loops using dynamic Bayesian networks.
In Figure 5.22, we see an example encoder for generating the redundant bits of a
convolutional code. This encoder has a state captured by three bits b0, b1, and b2, leading
to eight possible states. If we feed this encoder an information bit uk, it will do two things.
First, it will change its state, and second, it will generate two bits x2k and x2k+1. Here
x2k = uk is the same information bit we fed to the encoder, while x2k+1 is the redundant
bit. Hence, if we feed the bit sequence u0 . . . un−1 to the encoder, it will generate the bit
sequence x0x1 . . . , x2n−2x2n−1, where x0, x2, . . . , x2n−2 are our original information bits
and x1, x3, . . . , x2n−1 are the redundant bits.

106
BUILDING BAYESIAN NETWORKS
Table 5.1: Look-up tables for the encoder in Figure 5.22, to identify the current state sk and
redundant bit x2k+1 given previous state sk−1 and input bit uk. According to the semantics of
this encoder, we have x2k+1 = s0
k + s2
k , s0
k = s0
k−1 + s1
k−1 + uk, s1
k = s0
k−1, and s2
k = s1
k−1.
Here si is the ith bit in state s.
sk−1
uk
sk
sk−1
uk
sk
sk
x2k+1
000
0
000
100
0
110
000
0
000
1
100
100
1
010
001
1
001
0
000
101
0
110
010
0
001
1
100
101
1
010
011
1
010
0
101
110
0
011
100
1
010
1
001
110
1
111
101
0
011
0
101
111
0
011
110
1
011
1
001
111
1
111
111
0
U0
X0
X1
S0
U1
X2
X3
S1
U2
X4
X5
S2
U3
X6
X7
S3
Y0
Y1
Y2
Y3
Y4
Y5
Y6
Y7
Figure 5.23: A Bayesian network for a convolutional code.
Suppose for example that we want to determine the current state of the encoder
sk = b0b1b2 given input uk = 1 and the previous state of the encoder sk−1 = 010. The last
two bits of sk are simply the ﬁrst two bits of sk−1. We then have b0 = uk + b1 + b2 =
1 + 0 + 1 = 0, and thus our current state sk is 001. Given the current state, we can then
determine that the encoder outputs x2k = uk and x2k+1 = b0 + b2 = 0 + 1 = 1.
We can easily implement an encoder using two look-up tables, one for determining the
new state sk given the old state sk−1 and bit uk, and another for determining the redundant
bit x2k+1 given state sk (x2k is simply uk). These look-up tables are shown in Table 5.1 and
will prove useful as we next develop a Bayesian network for representing convolutional
codes.
Consider Figure 5.23, which depicts a Bayesian network for a convolutional code. The
network can be viewed as a sequence of replicated slices where slice k is responsible for
generating the codeword bits x2k and x2k+1 for the information bit uk. Note also that each
slice has a variable Sk that represents the state of the encoder at that slice. Moreover, this
state variable is determined by the previous state variable Sk−1 and the information bit
Uk. Hence, the network in Figure 5.23 is another example of dynamic Bayesian networks
that we encountered in Section 5.3.5.
Let us now parameterize this network structure. Root variables Uk corresponding to
information bits are assumed to have uniform priors as before. Moreover, channel output
variables Yk are assumed to be parameterized as shown previously, depending on the
noise model we assume. This leaves the CPTs for state variables Sk and for the codeword
bits Xk.

5.3 MODELING WITH BAYESIAN NETWORKS
107
U0
X0
X1
S0
U1
X2
X3
S1
U2
X4
X5
S2
U3
X6
X7
S3
U4
X9
S4
U5
X11
S5
U6
X13
S6
U7
X15
S7
Y9
Y11
Y13
Y15
Y0
Y1
Y2
Y3
Y4
Y5
Y6
Y7
Figure 5.24: A Bayesian network for a turbo code.
The CPTs for state variables Sk, k > 0, are deterministic and given as follows (based
on Table 5.1):
θsk|uk,sk−1 =
 1,
if the encoder transitions from sk−1 to sk given uk
0,
otherwise.
The CPT for variable S0 can be determined similarly while assuming some initial state
for the decoder.
The CPTs for variables X2k and X2k+1 are also deterministic and given as follows
(based also on Table 5.1):
θx2k|uk =
1,
if x2k = uk
0,
otherwise
θx2k+1|sk =
1,
if the encoder outputs x2k+1 given sk
0,
otherwise.
We can then decode a channel output y1, . . . , yn as we discussed in the previous section
by computing the posterior marginals Pr(uk|y1, . . . , yn) of variables Uk or by computing
MAP over these variables.
Turbo codes
Suppose now that we have four information bits u0, . . . , u3. In a convolutional code, we
will generate four redundant bits leading to a codeword with eight bits. In a turbo code,
we apply a convolutional code twice, once on the original bit sequence u0, u1, u2, u3 and
another time on some permutation of it, say, u1, u3, u2, u0. This leads to eight redundant
bits and a codeword with twelve bits.10
The structure of a Bayesian network that captures this scenario is given in Fig-
ure 5.24. This network can be viewed as consisting of two networks, each representing a
10 This gives a rate 1/3 code (ratio of information bits to total bits). In principle, we can drop some of the redundant
bits, leading to codes with different rates.

108
BUILDING BAYESIAN NETWORKS
convolutional code. In particular, the lower network represents a convolutional code for
the bit sequence u0, . . . , u3 and the upper network represents a convolutional code for the
bit sequence u4, . . . , u7. There are two points to observe about this structure. First, the
edges that cross between the networks are meant to establish the bit sequence u4, . . . , u7
(upper network) as a permutation of the bit sequence u0, . . . , u3 (lower network). In
particular, the CPTs for the bit sequence u4, . . . , u7 in the upper network are given by
θuk|uj =
1,
if uk = uj
0,
otherwise.
This CPT therefore establishes the equivalence between Uk in the upper network and Uj
in the lower.11 The second observation about Figure 5.24 is that the upper network does
not copy the information bits u4, . . . , u7 to the output as these are simply a permutation
of u0, . . . , u3, which are already copied to the output by the lower network.
It should be noted here that networks corresponding to convolutional codes are singly
connected; that is, there is only one (undirected) path between any two variables in the
network (these networks are also called polytrees). Networks corresponding to turbo
codes are multiply connected in that they do not satisfy this property. This has major
computational implications that we discuss in future chapters.
5.3.8 Commonsense knowledge
Consider the following commonsense reasoning problem:
When SamBot goes home at night, he wants to know if his family is home before he tries the
doors. (Perhaps the most convenient door to enter is double locked when nobody is home.)
Often when SamBot’s wife leaves the house, she turns on an outdoor light. However, she
sometimes turns on this light if she is expecting a guest. Also, SamBot’s family has a dog.
When nobody is home, the dog is in the back yard. The same is true if the dog has bowel
trouble. Finally, if the dog is in the back yard SamBot will probably hear her barking, but
sometimes he can be confused by other dogs barking. SamBot is equipped with two sensors,
a light sensor for detecting outdoor lights and a sound sensor for detecting the barking of
dogs. Both of these sensors are not completely reliable and can break. Moreover, they both
require SamBot’s battery to be in good condition.
Our goal is then to build a Bayesian network that SamBot will use to reason about this
situation. Speciﬁcally, given sensory input SamBot needs to compute his beliefs in whether
his family is home and whether any of his hardware is broken.
This problem is less structured than any of the problems we have examined so far.
The choice of evidence and query variables remain relatively obvious. We only have two
evidence variables in this case:
LightSensor: Is SamBot’s light sensor detecting a light (outdoor light)?
SoundSensor: Is SamBot’s sound sensor detecting a sound (barking)?
We also have four query variables:
FamilyHome: Is SamBot’s family (wife) home?
LightSensorBroken: Is SamBot’s light sensor broken?
11 In principle, one does not need the Uk variables in the upper network as these can simply be the Uj variables in
the lower network. Our choice of separating the two was meant to emphasize the two convolutional codes used to
compose a turbo code.

5.3 MODELING WITH BAYESIAN NETWORKS
109
ExpectingCompany
FamilyHome
DogBowel
OutdoorLight
DogOutside
OtherBarking
LightSensorBroken
Battery
DogBarking
SoundSensorBroken
SoundSensor
LightSensor
Figure 5.25: A Bayesian network structure for the SamBot problem.
SoundSensorBroken: Is SamBot’s sound sensor broken?
Battery: Is SamBot’s battery in good condition?
Finally, we have six intermediary variables:
ExpectingCompany: Is SamBot’s family (wife) expecting company?
OutdoorLight: Is the outdoor light on?
DogOut: Is SamBot’s dog outside?
DogBarking: Is some dog barking?
DogBowel: Is SamBot’s dog having bowel problems?
OtherBarking: Are other dogs barking?
The network structure corresponding to these choices is shown in Figure 5.25. Note
that all intermediary variables except for ExpectingCompany have a single child each.
Hence, they can be easily bypassed using the technique discussed in Section 5.3.4.
Parameterizing this structure can be accomplished based on a combination of sources:
r Statistical information, such as reliabilities of sensors and battery
r Subjective beliefs relating to how often the wife goes out, guests are expected, the dog
has bowel trouble, and so on
r Objective beliefs regarding the functionality of sensors.
One can also imagine the robot recording his experiences each evening and then con-
structing a data table similar to the one discussed in Section 5.3.1, which can then be used
to estimate the network parameters as discussed in Chapters 17 and 18.
5.3.9 Genetic linkage analysis
Before we can state the problem of genetic linkage analysis and how Bayesian networks
can be used to solve it, we need to provide some background.

110
BUILDING BAYESIAN NETWORKS
A pedigree is a structure that depicts a group of individuals while explicating their
sexes and identifying their children (see Figure 5.26). A pedigree is useful in reasoning
about heritable characteristics that are determined by genes, where different genes are
responsible for the expression of different characteristics. A gene may occur in different
states called alleles. Each individual carries two alleles of each gene, one received from
their mother and the other from their father. The alleles of an individual are called the
genotype, while the heritable characteristic expressed by these alleles (such as hair color,
blood type, and so on) are called the phenotype of the individual.
For example, consider the ABO gene, which is responsible for determining blood type.
This gene has three alleles: A, B, and O. Since each individual must have two alleles for
this gene, we have six possible genotypes in this case. Yet there are only four different
blood types, as some of the different genotypes lead to the same phenotype:
Genotype
Phenotype
A/A
Blood type A
A/B
Blood type AB
A/O
Blood type A
B/B
Blood type B
B/O
Blood type B
O/O
Blood type O
Hence, if someone has the blood type A, they could have the pair of alleles A/A or the
pair A/O for their genotype.
The phenotype is not always determined precisely by the genotype. Suppose for ex-
ample that we have a disease gene with two alleles, H and D. There are three possible
genotypes here yet none may guarantee the disease will show up:
Genotype
Phenotype
H/H
healthy
H/D
healthy
D/D
ill with probability .9
The conditional probability of observing a phenotype (e.g., healthy, ill) given the genotype
(e.g., H/H, H/D, D/D) is known as a penetrance. In the ABO gene, the penetrance is
always 0 or 1. However, for this gene, the penetrance is .9 for the phenotype ill given the
genotype D/D.
Recombination events
The alleles received by an individual from one parent are called a haplotype. Hence,
each individual has two haplotypes, one paternal and another maternal. Consider now the
pedigree in Figure 5.26, which explicates two genes G1 and G2 for each individual where
G1 has two alleles A and a and gene G2 has the alleles B and b.
Given the genotype of Mary in this pedigree, she can pass only one haplotype to her
child, Jack: AB. Similarly, John can pass only one haplotype to Jack: ab. On the other
hand, Jack can pass one of four haplotypes to his children: AB, Ab, aB, ab. Two of these
haplotypes, AB and ab, were received from his parents but the haplotypes Ab and aB
were not. For example, if Jack were to pass on the haplotype AB to an offspring, then this
haplotype would have come exclusively from Jack’s mother, Mary. However, if Jack were
to pass on the haplotype Ab, then part of this haplotype would have come from Mary
and the other part from Jack’s father, John. In such a case, we say that a recombination

5.3 MODELING WITH BAYESIAN NETWORKS
111
John
Mary
Jack
Sue
Lydia
Nancy
G1: (A,A)
G2: (B,B)
G1: (a,a)
G2: (b,b)
G1: (A,a)
G2: (B,b)
G1: (a,a)
G2: (b,b)
G1: (A,a)
G2: (b,b)
G1: (A,a)
G2: (B,b)
Figure 5.26: A pedigree involving six individuals. Squares represent males and circles represent females.
Horizontal edges connect spouses and vertical edges connect couples to their children. For example,
Jack and Sue are a couple with two daughters, Lydia and Nancy.
event has occurred between the two genes G1 and G2. We also say that the child receiving
this haplotype is a recombinant. In Figure 5.26, Lydia must be a recombinant. If Lydia is
not a recombinant, then she must have received the haplotype AB or ab from Jack and
the haplotype ab from Sue. This means that Lydia’s genotype would have been either
of G1 =(A, a), G2 =(B, b) or G2 =(a, a), G2 =(b, b). Yet Lydia has neither of these
genotypes.
Genetic linkage and gene maps
If two genes are inherited independently, the probability of a recombination is expected to
be 1/2. However, we sometimes observe that two alleles that were passed in the haplotype
from a grandparent to a parent tend to be passed again in the same haplotype from the
parent to a child. This phenomena is called genetic linkage, and one goal of genetic
linkage analysis is to estimate the extent to which two genes are linked. More formally,
the extent to which genes G1 and G2 are linked is measured by a recombination fraction or
frequency, θ, which is the probability that a recombination between G1 and G2 will occur.
Genes that are inherited independently are characterized by a recombination frequency
θ = 1/2 and are said to be unlinked. On the other hand, linked genes are characterized by
a recombination frequency θ < 1/2.
Linkage between genes is related to their locations on a chromosome within the cell
nucleus. These locations are typically referred to as loci (singular: locus) – see Figure 5.27.
In particular, for genes that are closely located on a chromosome, linkage is inversely
proportional to the distance between their locations: the closer the genes, the more linked
they are. The recombination frequency can then provide direct evidence on the distance
between genes on a chromosome, making it a useful tool for mapping genes onto a
chromosome. In fact, the recombination frequency is sometimes measured in units called
centimorgans, where a 1% recombination frequency is equal to 1 centimorgan.
In the rest of this section, we will assume that we have a set of closely linked genes
whose relative order on a given chromosome is already known. This allows us to produce a
distance map for these genes on the chromosome by simply focusing on the recombination
frequency of each pair of adjacent genes (loci) (see Figure 5.27).

112
BUILDING BAYESIAN NETWORKS
Gene 1
Gene 2
Gene 3
θ1
θ2
Figure 5.27: Genes correspond to locations on a chromosome within the cell nucleus. The ﬁgure depicts
three genes on a chromosome and some hypotheses on their recombination frequencies.
The likelihood of a hypothesis
Given a pedigree, together with some information about the genotypes and phenotypes of
involved individuals, we want to develop a Bayesian network that can be used to assess
the likelihood of a particular recombination frequency. By developing such a network,
we can discriminate between various hypotheses and even search for the most plausible
hypothesis.
More formally, let e be the observed genotypes and phenotypes and let θ be a recom-
bination frequency that we wish to investigate. We will then develop a Bayesian network
(G, ) that induces a distribution Pr(.) and use it to compute Pr(e). The parametrization 
of this network will be predicated on the recombination frequency θ, making Pr(e) corre-
spond to the likelihood of hypothesis θ. To compute the likelihood of another hypothesis
θ′, we will generate another parametrization ′ predicated on θ′ that induces another
distribution Pr′, leading to another likelihood Pr′(e). We would then prefer hypothesis θ′
over θ if its likelihood Pr′(e) is greater than the likelihood Pr(e) of θ.
From pedigrees to Bayesian networks
We now discuss a systematic process for constructing a Bayesian network from a given
pedigree. The network will include variables to capture both phenotype and genotype,
allowing one to capture such information as evidence on the developed network.
Figure 5.28 provides the basis of such a construction, as it depicts the Bayesian network
corresponding to three individuals numbered 1, 2, and 3, with 3 being the child of 1 and
2. The network assumes three genes for each individual, where gene j of individual i is
represented by two variables, GPij and GMij. Here GPij represents the paternal allele
and GMij represents the maternal allele. In addition, a variable Pij is included to represent
the phenotype for individual i caused by gene j.
For an individual i who is not a founder (i.e., his parents are included in the pedigree),
the network includes two selector variables for each gene j, SPij and SMij. These
variables are meant to determine the method by which individual i inherits his alleles
from his parents. In particular, variable SPij determines how i will inherit from his father:
if SPij = p, then i will inherit the allele that his father obtained from the grandfather, while
if SPij = m, then i will inherit the allele that his father obtained from the grandmother.
The selector SMij is interpreted similarly.

5.3 MODELING WITH BAYESIAN NETWORKS
113
GP11
GM11
P11
GP12
GM12
GP13
GM13
GP21
GM21
GP22
GM22
GP23
GM23
GP31
GM31
GP32
GM32
GP33
GM33
P12
P13
P21
P22
P23
P31
P32
P33
SP31
SP32
SP33
SM31
SM32
SM33
Child: 3
Father: 1
Mother: 2
Figure 5.28: A Bayesian network structure corresponding to a simple pedigree involving three individuals
numbered 1, 2, and 3. Here 3 is the child of father 1 and mother 2. Moreover, each individual has three
genes numbered 1, 2, and 3, which are assumed to be in this order on a chromosome.
To parameterize the network in Figure 5.28, we need four types of CPTs. First, for a
founder i we need the prior probability for the genotype variables GPij and GMij for
each gene j. This is usually obtained from population statistics that are typically collected
by geneticists. Second, for each individual i and gene j, we need a CPT for the phenotype
Pij. This may be a deterministic CPT or a probabilistic CPT, as we have seen previously.
Third, for each nonfounder i, we need the CPTs for genotype variables GPij and GMij.
These CPTs are deterministic and follow from the semantics of selector variables that
we have discussed previously. For example, if individual i has father k, then the CPT for
GPij is given by
θgpij |gpkj,gmkj ,spij =
⎧
⎨
⎩
1,
if spij = p and gpij = gpkj
1,
if spij = m and gpij = gmkj
0,
otherwise.
That is, if SPij = p, then the allele GPij for individual i will be inherited from the
paternal haplotype of his father k, GPkj. However, if SPij = m, then the allele GPij will
be inherited from the maternal haplotype of his father k, GMkj. The CPT for GMij is
speciﬁed in a similar fashion.
The ﬁnal type of CPTs concern selector variables, and it is these CPTs that will host our
hypotheses about recombination frequencies. Note here that we are assuming an ordering
of genes 1, 2, and 3 on the given chromosome. Hence, to produce a distance map for
these genes, all we need is the distance between genes 1 and 2 and the distance between
genes 2 and 3, which can be indicated by the corresponding recombination frequencies θ12
and θ23.
The selectors of the ﬁrst gene SP31 and SM31 will have uniform CPTs, indicating that
the parents will pass either their paternal or maternal alleles with equal probability for

114
BUILDING BAYESIAN NETWORKS
this gene. For the second gene, the CPTs for selectors SP32 and SM32 will be a function
of the recombination frequency θ12 between the ﬁrst and second gene. For example:
SP31
SP32
θsp32|sp31
p
p
1 −θ12
p
m
θ12
recombination between genes 1 and 2
m
p
θ12
recombination between genes 1 and 2
m
m
1 −θ12
The CPTs for other selector variables can be speciﬁed similarly.
Putting the network to use
The Bayesian network described previously contains variables for both genotype and
phenotype. It also contains CPT entries for every recombination frequency θij between
two adjacent genes i and j on a chromosome. Suppose now that this network induces
a distribution Pr(.) and let g be some evidence about the genotype and p be some ev-
idence about the phenotype. The probability Pr(g, p) will then represent the likelihood
of recombination frequencies included in the network’s CPTs. By simply changing the
CPTs for selector variables (which host the recombination frequencies) and recomputing
Pr(g, p), we will be able to compute the likelihoods of competing hypotheses about ge-
netic linkage.12 We can even conduct a search in the space of such hypotheses to identify
the one that maximizes likelihood while using the developed network and corresponding
probabilities Pr(g, p) to guide the search process. We will have more to say about such a
search process when we discuss parameter estimation in Chapter 17.
5.4 Dealing with large CPTs
One of the major issues that arise when building Bayesian network models is the potentially
large size of CPTs. Suppose for example, that we have a variable E with parents C1, . . . , Cn
and suppose further that each one of these variables has only two values. We then need
2n independent parameters to completely specify the CPT for variable E. The following
table gives a concrete feel of this CPT size for different values of n:
Number of Parents: n
Parameter Count: 2n
2
4
3
8
6
64
10
1,024
20
1,048,576
30
1,073,741,824
Both modeling and computational problems will arise as the number of parents n gets
larger but the modeling problem will obviously manifest ﬁrst. Speciﬁcally, a CPT with
1,024 entries is rarely a concern from a computational viewpoint but imagine trying to
commit a medical expert to specifying 1,024 numbers in order to quantify the relationship
12 For a given hypothesis θij, the score log Prθij (g, p)/Pr.5(g, p) is typically used to quantify the support for this
hypothesis, which is meant to be normalized across different pedigrees. Here Prθij is the distribution induced by
the network where θij is the recombination frequency between genes i and j, while Pr.5 is the distribution induced
by the network where .5 is the recombination frequency between i and j (no linkage).

5.4 DEALING WITH LARGE CPTS
115
1
C
2
C
n
C
E
. . .
. . .
1
C
2
C
n
C
E
2
Q
n
Q
L
1
Q
(a) Node with n parents
(b) Noisy-or circuit
Figure 5.29: Illustrating noisy-or semantics for a node with n parents.
between headache and ten different medical conditions that may cause it. There are two
different types of solutions to this problem of large CPTs, which we discuss next.
5.4.1 Micro models
The ﬁrst approach for dealing with large CPTs is to try to develop a micro model that
details the relationship between the parents C1, . . . , Cn and their common child E. The
goal here is to reveal the local structure of this relationship in order to specify it using a
number of parameters that is smaller than 2n.
One of the most common micro models for this purpose is known as the noisy-or
model, depicted in Figure 5.29(b). To understand this model, it is best to interpret parents
C1, . . . , Cn as causes and variable E as their common effect. The intuition here is that
each cause Ci is capable of establishing the effect E on its own, regardless of other causes,
except under some unusual circumstances that are summarized by the suppressor variable
Qi. That is, when the suppressor Qi of cause Ci is active, cause Ci is no longer able to
establish E. Moreover, the leak variable L is meant to represent all other causes of E
that were not modelled explicitly. Hence, even when none of the causes Ci are active,
the effect E may still be established by the leak variable L. Given this interpretation of
the noisy-or model, one would then expect the probabilities of suppressors and leak to be
usually small in practice.
The noisy-or model in Figure 5.29(b) can then be speciﬁed using n + 1 parameters,
which is remarkable from a modeling viewpoint. For example, to model the relationship
between headache and ten different conditions that may cause it, all we need are the
following numbers:
r θqi = Pr(Qi =active): the probability that the suppressor of cause Ci is active
r θl = Pr(L=active): the probability that the leak variable is active.
The noisy-or model contains enough information to completely specify the conditional
probability of variable E given any instantiation α of the parents C1, . . . , Cn. Hence, the
model can be used to completely specify the CPT for variable E in Figure 5.29(a). To
show this, let Iα be the indices of causes that are active in α. For example, if
α : C1 =active, C2 =active, C3 =passive, C4 =passive, C5 =active,

116
BUILDING BAYESIAN NETWORKS
1
C
2
C
n
C
E
2
Q
n
Q
1
Q
L
1A
2
A
n
A
. . .
1
C
2
C
n
C
E
2
Q
n
Q
L
1
Q
(a)
(b)
Figure 5.30: A Bayesian network for a noisy-or circuit.
then Iα is the set containing indices 1, 2, and 5. Using this notation, we have
Pr(E =passive|α) = (1 −θl)

i∈Iα
θqi.
(5.2)
From this equation, we also get Pr(E =active|α) = 1 −Pr(E =passive|α). Hence, the
full CPT for variable E with its 2n independent parameters can be induced from the n + 1
parameters associated with the noisy-or model.
One can derive (5.2) in a number of ways. The more intuitive derivation is that given
the status α of causes C1, . . . , Cn, the effect E will be passive only if the leak was passive
and all suppressors Qi, for i ∈Iα, were active. Since the leak and suppressors are assumed
to be independent, the probability of that happening is simply given by (5.2).
Another way to derive (5.2) is to build a micro Bayesian network, as given in Fig-
ure 5.30(a), which explicitly represents the noisy-or model. This network will have 3n + 2
variables, where the CPTs for all variables except causes C1, . . . , Cn are determined from
the noisy-or model. Note here that variables L, Q1, . . . , Qn have a single child each.
Hence, we can bypass each of them, as discussed in Section 5.3.4, while updating the CPT
for variable E after bypassing each variable. We can then bypass variables A1, . . . , An,
since each of them has a single child E. If we bypass all such variables, we get the CPT
given by (5.2).
To consider a concrete example of noisy-or models, let us revisit the medical diagnosis
problem from Section 5.3.1. Sore throat (S) has three causes in this problem: cold (C), ﬂu
(F), and tonsillitis (T ). If we assume that S is related to its causes by a noisy-or model,
we can then specify the CPT for S by the following four probabilities:
r The suppressor probability for cold, say, .15
r The suppressor probability for ﬂu, say, .01
r The suppressor probability for tonsillitis, say, .05
r The leak probability, say, .02.
The CPT for sore throat is then determined completely as follows:
C
F
T
S
θs|c,f,t
Equation 5.2
true
true
true
true
.9999265
1 −(1 −.02)(.15)(.01)(.05)
true
true
false
true
.99853
1 −(1 −.02)(.15)(.01)
true
false
true
true
.99265
1 −(1 −.02)(.15)(.05)
...
...
...
...
...
false
false
false
true
.02
1 −(1 −.02)

5.4 DEALING WITH LARGE CPTS
117
C1
C2 C3
C4 Pr(E=1)
1
1
1
1
0.0
1
1
1
0
0.0
1
1
0
1
0.0
1
1
0
0
0.0
1
0
1
1
0.0
1
0
1
0
0.0
1
0
0
1
0.0
1
0
0
0
0.0
0
1
1
1
0.9
0
1
1
0
0.9
0
1
0
1
0.9
0
1
0
0
0.9
0
0
1
1
0.3
0
0
1
0
0.3
0
0
0
1
0.6
0
0
0
0
0.8
C1
C2
C3
C4
0.0
0.9
0.3
0.6
0.8
Pr(E=1)
Figure 5.31: A CPT and its corresponding decision tree representation (also known as a probability tree).
Solid edges represent 1-values and dotted edges represent 0-values.
5.4.2 Other representations of CPTs
The noisy-or model is only one of several other models for local structure. Each of these
models is based on some assumption about the way parents C1, . . . , Cn interact with their
common child E. If the assumption corresponds to reality, then one can use these models
for local structure. Otherwise, the resulting Bayesian network will be an inaccurate model
of that reality (but it could be a good approximation).
Most often, we have some local structure in the relationship between a node and its
parents but that structure does not ﬁt precisely into any of the existing micro models such
as noisy-or. Consider the CPT in Figure 5.31. Here we have a node with four parents
and a CPT that exhibits a considerable amount of structure. For example, the probability
of E =1 given C1 =1 is 0 regardless of the values assumed by parents C2, C3, and C4.
Moreover, given that C1 =0 and C2 =1, the probability of E =1 is .9 regardless of the
values of other parents. Even with all of this local structure, the CPT of this node does
not correspond to the assumptions underlying a noisy-or model and, hence, cannot be
generated by such a model.
For this type of irregular structure, there are several nontabular representations that
are not necessarily exponential in the number of parents. We discuss some of these
representations next.
Decision trees and graphs
One of the more popular representations for this purpose is the decision tree, an example
of which is shown in Figure 5.31. The basic idea here is that we start at the root of the tree
and then branch downward at each node depending on the value of the variable attached to
that node. The decision tree in Figure 5.31 represents the probability of E =1 under every
possible instantiation of the parents C1, . . . , C4 except that these instantiations are not
represented explicitly. For example, if C1 =1 then the probability of E =1 is immediately
decided to be 0, as shown on the very left of this decision tree. Obviously, the decision
tree can have a size that is linear in the number of parents if there is enough structure in
the CPT. But it may also be exponential in size if no such structure exists in the given
CPT. We discuss in Chapter 13 a generalization of decisions trees called decision graphs
that can be exponentially more compact than trees.

118
BUILDING BAYESIAN NETWORKS
If-then rules
A CPT for variable E can be represented using a set of if-then rules of the form
If αi then Pr(e) = pi,
where αi is a propositional sentence constructed using the parents of variable E. For
example, the CPT in Figure 5.31 can be represented using the following rules:
If C1 =1
then
Pr(E =1) = 0
If C1 =0 ∧C2 =1
then
Pr(E =1) = .9
If C1 =0 ∧C2 =0 ∧C3 =1
then
Pr(E =1) = .3
If C1 =0 ∧C2 =0 ∧C3 =0 ∧C4 =1
then
Pr(E =1) = .6
If C1 =0 ∧C2 =0 ∧C3 =0 ∧C4 =0
then
Pr(E =1) = .8
For the rule-based representation to be complete and consistent, the set of rules
If αi then Pr(e) = pi
for a given value e of E must satisfy two conditions:
r The premises αi must be mutually exclusive. That is, αi ∧αj is inconsistent for i ̸= j.
This ensures that the rules will not conﬂict with each other.
r The premises αi must be exhaustive. That is, 
i αi must be valid. This ensures that every
CPT parameter θe|... is implied by the rules.
We also need to have one set of rules for all but one value e of variable E. Again, the
rule-based representation can be very efﬁcient if the CPT has enough structure yet may
be of exponential size when no such structure exists.
Deterministic CPTs
A deterministic or functional CPT is one in which every probability is either 0 or 1. These
CPTs are very common in practice and we have seen a number of them in Sections 5.3.5
and 5.3.7. When a node has a deterministic CPT, the node is said to be functionally
determined by its parents.
Deterministic CPTs can be represented compactly using propositional sentences. In
particular, suppose that we have a deterministic CPT for variable E with values e1, . . . , em.
We can then represent this CPT by a set of propositional sentences of the form
i ⇐⇒E =ei,
where we have one rule for each value ei of E and the premises i are mutually exclusive
and exhaustive. The CPT for variable E is then given by
θei|α =
1,
if parent instantiation α is consistent with i
0,
otherwise.
Consider for example the following deterministic CPT from Sections 5.3.5:
A
X
C
θc|a,x
high
ok
high
0
low
ok
high
1
high
stuckat0
high
0
low
stuckat0
high
0
high
stuckat1
high
1
low
stuckat1
high
1

5.5 THE SIGNIFICANCE OF NETWORK PARAMETERS
119
We can represent this CPT as follows:
(X=ok ∧A=high) ∨X=stuckat0
⇐⇒
C =low
(X=ok ∧A=low) ∨X=stuckat1
⇐⇒
C =high
This representation can be very effective in general, especially when the number of parents
is quite large.
Expanding CPT representations
We close this section with a word of caution on how the prior representations of CPTs
are sometimes used by Bayesian network tools. Many of these tools will expand these
structured representations into tabular representations before they perform inference. In
such cases, these structured representations are only being utilized in addressing the
modeling problem since the size of expanded representations is still exponential in the
number of parents. However, Chapter 13 discusses algorithms that can operate directly
on some structured representations of CPTs without having to expand them.
5.5 The signiﬁcance of network parameters
Bayesian network parameters can be viewed as local beliefs, as each parameter θx|u
represents the belief in variable X given its parents U (direct causes). On the other hand,
queries posed to a Bayesian network can be viewed as global beliefs as the query Pr(y|e)
represents our belief in some variables Y given the state of other variables E that can be
distantly related to them (i.e., their indirect causes, indirect effects, and so on). One of
the more practical issues when modeling and reasoning with Bayesian networks is that
of understanding the relationship between global beliefs Pr(y|e) and local beliefs θx|u.
Next, we present some known relationships between these two quantities that can provide
valuable insights when building Bayesian network models. Additional relationships are
discussed in Chapter 16, which is dedicated to the subject of sensitivity analysis.
Suppose that X is a variable that has two values, x and ¯x, and a set of parents U. We
must then have
θx|u + θ¯x|u = 1
for any parent instantiation u. Therefore, if we change either of these parameters we must
also change the other parameter to ensure that their sum continues to be 1. Now let τx|u
be a metaparameter such that:
θx|u = τx|u
θ¯x|u = 1 −τx|u.
By changing the metaparameter τx|u, we are then simultaneously changing both parameters
θx|u and θ¯x|u in a consistent way. Given this new tool, let us consider the following
result, which provides a bound on the partial derivative of query Pr(y|e) with respect to
metaparameter τx|u:

∂Pr(y|e)
∂τx|u
 ≤Pr(y|e)(1 −Pr(y|e))
Pr(x|u)(1 −Pr(x|u)).
(5.3)
Note that this bound is independent of the Bayesian network under consideration; that
is, it applies to any Bayesian network regardless of its structure and parametrization. The
plot of this bound against the current value of the metaparameter, Pr(x|u), and the current

120
BUILDING BAYESIAN NETWORKS
0.2
0.4
0.6
0.8
Pr(x| u)
0.2
0.4
0.6
0.8
Pr(y|e)
0
2
4
6
pd bound
0.2
0.4
0.6
0.8
Pr(x| u)
Figure 5.32: An upper bound on the partial derivative |∂Pr(y|e)/∂τx|u| as a function of the query value
Pr(y|e) and the parameter value Pr(x|u).
value of the query, Pr(y|e), is shown in Figure 5.32. A number of observations are in order
about this plot:
1. The bound approaches inﬁnity for extreme values of parameter Pr(x|u) and attains its
smallest value when Pr(x|u) = .5.
2. The bound approaches 0 for extreme values of query Pr(y|e) and attains its highest value
when Pr(y|e) = .5.
Therefore, according to this bound extreme queries tend to be robust when changing
nonextreme parameters yet nonextreme queries may change considerably when changing
extreme parameters.
Bound (5.3) can be used to show an even more speciﬁc relationship between parameters
and queries. Speciﬁcally, let O(x|u) denote the odds of variable X given its parents U,
O(x|u) = Pr(x|u)/(1 −Pr(x|u)),
and let O(y|e) denote the odds of variables Y given evidence E:
O(y|e) = Pr(y|e)/(1 −Pr(y|e)).
Let O′(x|u) and O′(y|e) denote these odds after having applied an arbitrary change to the
metaparameter τx|u. We then have
| ln(O′(y|e)) −ln(O(y|e))| ≤| ln(O′(x|u)) −ln(O(x|u))|.
(5.4)
The inequality (5.4) allows us to bound the amount of change in a query value using
only the amount of change we applied to a parameter without requiring any information
about the Bayesian network under consideration. This inequality can be very useful in
practice as it allows one to assess the impact of a parameter change on some query very
efﬁciently (in constant time).
Consider for example the screenshot in Figure 5.3 on Page 79, where the evidence e
indicates a patient with positive x-ray and no dyspnoea. The belief in this patient visiting

5.5 THE SIGNIFICANCE OF NETWORK PARAMETERS
121
Asia, Pr(A=yes|e), is about 1.17% in this case. Moreover, the belief in this patient having
cancer, Pr(C =yes|e), is about 25.23%. Consider now the parameter θC=yes|S=yes, which
represents our local belief in cancer given smoking. This parameter is currently set to 10%
and we would like to change its value to 5%. Our interest here is in assessing the impact
of this change on the probability of having visited Asia. Using bound (5.4), we have
ln

p
1 −p
	
−ln
 1.17
98.83
	 ≤
ln
 5
95
	
−ln
10
90
	 ,
where p = Pr′(A=yes|e) is the new probability in a visit to Asia after having changed
the parameter θC=yes|S=yes from 10% to 5%. Solving for p, we get
.56% ≤Pr′(A=yes|e) ≤2.44%
If we actually change the parameter and perform inference, we ﬁnd that the exact value
of Pr′(A=yes|e) is 1.19%, which is within the bound as expected.
We can use the same technique to bound the change in our belief in cancer after the
same parameter change, which gives
13.78% ≤Pr′(C =yes|e) ≤41.60%
Note that the bound is looser in this case, which is not surprising since the query under
consideration is less extreme.
Consider now the parameter θB=yes|S=yes that represents our local belief in bronchitis
given smoking. This parameter is currently set to 60% and we would like to reduce it to
50%. We now have the following bounds for query change:
.78% ≤Pr′(A=yes|e) ≤1.74%
18.36% ≤Pr′(C =yes|e) ≤33.61%
These bounds are tighter than these for parameter θC=yes|S=yes, which is not surprising
since parameter θB=yes|S=yes is less extreme.
We ﬁnally note that the bound in (5.4) assumes that the parameter we are changing
concerns a variable with only two values. This bound has a generalization to nonbinary
variables but we defer the discussion of this generalization to Chapter 16, where we
discuss sensitivity analysis in greater technical depth.
Bibliographic remarks
The Asia network of Section 5.2 is due to Lauritzen and Spiegelhalter [1988]. The
pregnancy network of Section 5.3.2 is due to Jensen [1996]. The SamBot network of
Section 5.3.8 is a slight modiﬁcation on the one in Charniak [1991]. The connection
between channel coding and graphical models, including Bayesian networks, is discussed
in McEliece et al. [1998a], Frey and MacKay [1997], Frey [1998]. Our treatment of
genetic linkage analysis is based on Fishelson and Geiger [2002; 2003]. Some of the
early examples for using Bayesian networks in medical diagnosis include: The Quick
Medical Reference (QMR) model [Miller et al., 1986], which was later reformulated as
a Bayesian network model [Shwe et al., 1991]; the CPCS-PM network [Pradhan et al.,
1994; Parker and Miller, 1987], which simulates patient scenarios in the medical ﬁeld
of hepatobiliary disease; and the MUNIN model for diagnosing neuromuscular disorders
from data acquired by electromyographic (EMG) examinations [Andreassen et al., 1987;

122
BUILDING BAYESIAN NETWORKS
1989; 2001]. Dynamic Bayesian networks were ﬁrst discussed in Dean and Kanazawa
[1989].
The noisy-or model and some of its generalizations are discussed in Pearl [1988],
Henrion [1989], Srinivas [1993], and Díez [1993]. Nontabular representations of CPTs
are discussed in Friedman and Goldszmidt [1996], Hoey et al. [1999], Nielsen et al. [2000],
Poole and Zhang [2003], Sanner and McAllester [2005], Mateescu and Dechter [2006],
and Chavira and Darwiche [2007]. Our discussion on the impact of network parameters
is based on Chan and Darwiche [2002].
5.6 Exercises
5.1. Joe’s x-ray test comes back positive for lung cancer. The test’s false negative rate is fn = .40
and its false positive rate is fp = .02. We also know that the prior probability of having lung
cancer is c = .001. Describe a Bayesian network and a corresponding query for computing
the probability that Joe has lung cancer given his positive x-ray. What is the value of this
probability? Use sensitivity analysis to identify necessary and sufﬁcient conditions on each of
fn, fp, and c that guarantee the probability of cancer to be no less than 10% given a positive
x-ray test.
5.2. We have three identical and independent temperature sensors that will trigger in:
r
90% of the cases where the temperature is high
r
5% of the cases where the temperature is nominal
r
1% of the cases where the temperature is low.
The probability of high temperature is 20%, nominal temperature is 70%, and low temperature
is 10%. Describe a Bayesian network and corresponding queries for computing the following:
(a) Probability that the ﬁrst sensor will trigger given that the other two sensors have also triggered
(b) Probability that the temperature is high given that all three sensors have triggered
(c) Probability that the temperature is high given that at least one sensor has triggered
5.3. Suppose that we apply three test vectors to the circuit in Figure 5.14, where each gate is
initially ok with probability .99. As we change the test vector, a gate that is ok may become
faulty with probability .01, and a gate that is faulty may become ok with probability .001.
(a) What
are
the
posterior
marginals
for
the
health
variables
given
the
following
test
vectors?
r A=high, B =high, E =low
r A=low, B =low, E =high
r A=low, B =high, E =high
(b) What about the following test vectors?
r A=high, B =high, E =low
r A=low, B =low, E =high
r A=low, B =high, E =low
(c) What are the MPE and MAP (over health variables) for each of the cases in (a) and (b)?
Assume that the test vectors are applied in the order given.
5.4. We have two sensors that are meant to detect extreme temperature, which occurs 20% of
the time. The sensors have identical speciﬁcations with a false positive rate of 1% and a false
negative rate of 3%. If the power is off (dead battery), the sensors will read negative regardless
of the temperature. Suppose now that we have two sensor kits: Kit A where both sensors
receive power from the same battery and Kit B where they receive power from independent

5.6 EXERCISES
123
batteries. Assuming that each battery has a .9 probability of power availability, what is the
probability of extreme temperature given each of the following scenarios:
(a) The two sensors read negative
(b) The two sensors read positive
(c) One sensor reads positive while the other reads negative.
Answer the previous questions with respect to each of the two kits.
5.5. Jack has three coins C1, C2, and C3 with p1, p2, and p3 as their corresponding probabilities
of landing heads. Jack ﬂips coin C1 twice and then decides, based on the outcome, whether
to ﬂip coin C2 or C3 next. In particular, if the two C1 ﬂips come out the same, Jack ﬂips coin
C2 three times next. However, if the C1 ﬂips come out different, he ﬂips coin C3 three times
next. Given the outcome of Jack’s last three ﬂips, we want to know whether his ﬁrst two ﬂips
came out the same. Describe a Bayesian network and a corresponding query that solves this
problem. What is the solution to this problem assuming that p1 = .4, p2 = .6, and p3 = .1
and the last three ﬂips came out as follows:
(a) tails, heads, tails
(b) tails, tails, tails
5.6. Lisa is given a fair coin C1 and asked to ﬂip it eight times in a row. Lisa also has a biased coin
C2 with a probability .8 of landing heads. All we know is that Lisa ﬂipped the fair coin initially
but we believe that she intends to switch to the biased coin and that she tends to be 10%
successful in performing the switch. Suppose that we observe the outcome of the eight coin
ﬂips and want to ﬁnd out whether Lisa managed to perform a coin switch and when. Describe
a Bayesian network and a corresponding query that solves this problem. What is the solution
to this problem assuming that the ﬂips came out as follows:
(a) tails, tails, tails, heads, heads, heads, heads, heads
(b) tails, tails, heads, heads, heads, heads, heads, heads
5.7. Consider the system reliability problem in Section 5.3.6 and suppose that the two fans depend
on a common condition C that materializes with a probability 99.5%. In particular, as long as
the condition is established, each fan will have a reliability of 90%. However, if the condition
is not established, both fans will fail. Develop a Bayesian network that corresponds to this
scenario and compute the overall system reliability in this case.
5.8. Consider the electrical network depicted in Figure 5.33 and assume that electricity can ﬂow
in either direction between two adjacent stations Si and Sj (i.e., connected by an edge),
and that electricity is ﬂowing into the network from sources I1 and I2. For the network to be
S4
S3
S5
S2
S1
O2
O1
I2
I1
Figure 5.33: An electrical network system.

124
BUILDING BAYESIAN NETWORKS
operational, electricity must also ﬂow out of either outlets O1 or O2. Describe a Bayesian
network and a corresponding query that allows us to compute the reliability of this network,
given the reliability rij of each connection between stations Si and Sj. What is the reliability
of the network if rij = .99 for all connections?
5.9. Consider Section 5.3.6 and suppose we extend the language of RBDs to include k-out-of-n
blocks. In particular, a block C is said to be of type k-out-of-n if it has n components pointing
to it in the diagram, and at least k of them must be available for the subsystem represented
by C to be available. Provide a systematic method for converting RBDs with k-out-of-n blocks
into Bayesian networks.
5.10
(After Jensen). Consider a cow that may be infected with a disease that can possibly be
detected by performing a milk test. The test is performed on ﬁve consecutive days, leading to
ﬁve outcomes. We want to determine the state of the cow’s infection over these days given
the test outcomes. The prior probability of an infection on day one is 1/10,000; the test false
positive rate is 5/1,000; and its false negative rate is 1/1,000. Moreover, the state of infection
at a given day depends only on its state at the previous day. In particular, the probability of a
new infection on a given day is 2/10,000, while the probability that an infection would persist
to the next day is 7/10.
(a) Describe a Bayesian network and a corresponding query that solves this problem. What is the
most likely state of the cow’s infection over the ﬁve days given the following test outcomes:
(1)
positive, positive, negative, positive, positive
(2)
positive, negative, negative, positive, positive
(3)
positive, negative, negative, negative, positive
(b) Assume now that the original false negative and false positive rates double on a given day in
case the test has failed on the previous day. Describe a Bayesian network that captures this
additional information.
5.11. Let E be the evidence variables in a Bayesian network, M be the MAP variables, and let Y
be all other variables. Suppose that Pr(m, e) > 0 implies that Pr(m, e, y) ̸= 0 for exactly one
instantiation y. Show that the projection of an MPE solution on the MAP variables M is also
a MAP solution.
5.12. Let R be some root variables in a Bayesian network and let Q be some nonroots. Show
that the probability Pr(q|r) is independent of the CPTs for roots R for all q and r such that
Pr(r) ̸= 0.
5.13. Consider the two circuit models of Section 5.3.5 corresponding to two different ways of
representing health variables. We showed in that section that these two models are equivalent
as far as queries involving variables A, B, C, D, and E. Does this equivalence continue to
hold if we extend the models to two test vectors as we did in Section 5.3.5? In particular, will the
two models be equivalent with respect to queries involving variables A, . . . , E, A′, . . . , E′?
Explain your answer.
5.14. Consider the two circuit models of Section 5.3.5 corresponding to two different ways of
representing health variables. We showed in that section that these two models are equivalent
as far as queries involving variables A, B, C, D, and E. Show that this result generalizes to
any circuit structure as long as the two models agree on the CPTs for primary inputs and the
CPTs corresponding to components satisfy the following conditions:
θH=ok = θ′
H=ok
θO=0|i,H=faulty =
θ′
H=stuckat0
θ′
H=stuckat0 + θ′
H=stuckat1
θO=1|i,H=faulty =
θ′
H=stuckat1
θ′
H=stuckat0 + θ′
H=stuckat1
.

5.6 EXERCISES
125
Here H is the health of the component, O is its output, and I are its inputs. Moreover, θ
are the network parameters when health variables have states ok and faulty, and θ′ are the
network parameters when health variables have states ok, stuckat0, and stuckat1.
5.15. Consider Exercise 5.14. Let H and H′ be the health variables in corresponding networks
and let X be all other variables in either network. Let h be an instantiation of variables H,
assigning either ok or faulty to each variable in H. Let h′ be deﬁned as follows:
h′
def=


h |= H=ok
(H ′ = ok)

∧


h |= H=faulty
(H ′ = stuckat0) ∨(H ′ = stuckat1)

.
Show that Pr(x, h) = Pr′(x, h′) for all x and h, where Pr is the distribution induced by the
network with health states ok/faulty and Pr′ is the distribution induced by the network with
health states ok/stuckat0/stuckat1.
5.16. Consider a DAG G with one root node S and one leaf node T , where every edge U →X
represents a communication link between U and X – a link is up if U can communicate with
X and down otherwise. In such a model, nodes S and T can communicate iff there exists a
directed path from S to T where all links are up. Suppose that each edge U →X is labeled
with a probability p representing the reliability of communication between U and X. Describe
a Bayesian network and a corresponding query that computes the reliability of communication
between S and T . The Bayesian network should have a size that is proportional to the size
of the DAG G.

6
Inference by Variable Elimination
We present in this chapter one of the simplest methods for general inference in Bayesian
networks, which is based on the principle of variable elimination: A process by which we
successively remove variables from a Bayesian network while maintaining its ability to
answer queries of interest.
6.1 Introduction
We saw in Chapter 5 how a number of real-world problems can be solved by posing queries
with respect to Bayesian networks. We also identiﬁed four types of queries: probability of
evidence, prior and posterior marginals, most probable explanation (MPE), and maximum
a posterior hypothesis (MAP). We present in this chapter one of the simplest inference
algorithms for answering these types of queries, which is based on the principle of variable
elimination. Our interest here will be restricted to computing the probability of evidence
and marginal distributions, leaving the discussion of MPE and MAP queries to Chapter 10.
We start in Section 6.2 by introducing the process of eliminating a variable. This process
relies on some basic operations on a class of functions known as factors, which we discuss
in Section 6.3. We then introduce the variable elimination algorithm in Section 6.4 and
see how it can be used to compute prior marginals in Section 6.5. The performance of
variable elimination will critically depend on the order in which we eliminate variables. We
discuss this issue in Section 6.6, where we also provide some heuristics for choosing good
elimination orders. We then expand the scope of variable elimination to posterior marginals
and probability of evidence in Section 6.7. The complexity of variable elimination is
sensitive to the network structure and speciﬁc queries of interest. We study the effect of
these two factors on the complexity of inference in Sections 6.8 and 6.9, respectively. We
conclude the chapter in Section 6.10 with a discussion of a common variant on variable
elimination known as bucket elimination.
6.2 The process of elimination
Consider the Bayesian network in Figure 6.1 and suppose that we are interested in
computing the following marginal:
D
E
Pr(D, E)
true
true
.30443
true
false
.39507
false
true
.05957
false
false
.24093
The algorithm of variable elimination will compute this marginal by summing out
variables A, B, and C from the given network to construct a marginal distribution over
126

6.2 THE PROCESS OF ELIMINATION
127
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
A
A
true
.6
false
.4
A
B
B|A
true
true
.2
true
false
.8
false
true
.75
false
false
.25
A
C
C|A
true
true
.8
true
false
.2
false
true
.1
false
false
.9
B
C
D
D|BC
true
true
true
.95
true
true
false
.05
true
false
true
.9
true
false
false
.1
false
true
true
.8
false
true
false
.2
false
false
true
0
false
false
false
1
C
E
E|C
true
true
.7
true
false
.3
false
true
0
false
false
1
Figure 6.1: A Bayesian network.
the remaining variables D and E. To explain this process of summing out variables,
consider the joint probability distribution in Figure 6.2, which is induced by the network
in Figure 6.1. To sum out a variable, say, A from this distribution is to produce a marginal
distribution over the remaining variables B, C, D, and E. This is done by merging all rows
that agree on the values of variables B, C, D, and E. For example, the following rows in
Figure 6.2
A
B
C
D
E
Pr(.)
true
true
true
true
true
.06384
false
true
true
true
true
.01995
are merged into the row
B
C
D
E
Pr(.)
true
true
true
true
.08379 = .06384 + .01995
As we merge rows, we drop reference to the summed-out variable A and add up the
probabilities of merged rows. Hence, the result of summing out variable A from the
distribution Pr in Figure 6.2, which has thirty-two rows, is another distribution Pr′ that
does not mention A and that has sixteen rows. The important property of summing
variables out is that the new distribution is as good as the original one as far as answering
queries that do not mention variable A. That is, Pr′(α) = Pr(α) for any event α that does
not mention variable A. Therefore, if we want to compute the marginal distribution over,
say, variables D and E, all we have to do then is sum out variables A, B, and C from the
joint distribution.

128
INFERENCE BY VARIABLE ELIMINATION
A
B
C
D
E
Pr(.)
true
true
true
true
true
.06384
true
true
true
true
false
.02736
true
true
true
false
true
.00336
true
true
true
false
false
.00144
true
true
false
true
true
0
true
true
false
true
false
.02160
true
true
false
false
true
0
true
true
false
false
false
.00240
true
false
true
true
true
.21504
true
false
true
true
false
.09216
true
false
true
false
true
.05376
true
false
true
false
false
.02304
true
false
false
true
true
0
true
false
false
true
false
0
true
false
false
false
true
0
true
false
false
false
false
.09600
false
true
true
true
true
.01995
false
true
true
true
false
.00855
false
true
true
false
true
.00105
false
true
true
false
false
.00045
false
true
false
true
true
0
false
true
false
true
false
.24300
false
true
false
false
true
0
false
true
false
false
false
.02700
false
false
true
true
true
.00560
false
false
true
true
false
.00240
false
false
true
false
true
.00140
false
false
true
false
false
.00060
false
false
false
true
true
0
false
false
false
true
false
0
false
false
false
false
true
0
false
false
false
false
false
.0900
Figure 6.2: A joint probability distribution induced by the Bayesian network in Figure 6.1.
This procedure will always work but its complexity is exponential in the number of
variables in the Bayesian network. The key insight underlying the method of variable
elimination is that one can sometimes sum out variables without having to construct
the joint probability distribution explicitly. In particular, variables can be summed out
while keeping the original distribution and all successive distributions in some factored
form. This allows the procedure to sometimes escape the exponential complexity of the
brute-force method discussed previously.
Before we discuss the method of variable elimination, we ﬁrst need to discuss its central
component: the factor.
6.3 Factors
A factor is a function over a set of variables, mapping each instantiation of these vari-
ables to a non-negative number. Figure 6.3 depicts two factors in tabular form, the ﬁrst
of which is over three variables B, C, and D. Each row of this factor has two compo-
nents: an instantiation and a corresponding number. In some cases, the number represents
the probability of the corresponding instantiation, as in factor f2 of Figure 6.3, which

6.3 FACTORS
129
B
C
D
f1
true
true
true
.95
true
true
false
.05
true
false
true
.9
true
false
false
.1
false
true
true
.8
false
true
false
.2
false
false
true
0
false
false
false
1
D
E
f2
true
true
.448
true
false
.192
false
true
.112
false
false
.248
Figure 6.3: Two factors: f1(b, c, d) = Pr(d|b, c) and f2(d, e) = Pr(d, e).
represents a distribution over variables D and E. In other cases, the number represents
some conditional probability that relates to the instantiation, as in factor f1 of Figure 6.3,
which represents the conditional probability of D given B and C. Hence, it is important
to stress that a factor does not necessarily represent a probability distribution over the
corresponding variables. However, most of the computations we perform on factors will
start with factors that represent conditional probabilities and end up with factors that
represent marginal probabilities. In the process we may have a mixture of factors with
different interpretations. The following is the formal deﬁnition of a factor.
Deﬁnition 6.1. A factor f over variables X is a function that maps each instantiation
x of variables X to a non-negative number, denoted f (x).1
■
We will use vars(f ) to denote the variables over which the factor f is deﬁned. We will
also write f (X1, . . . , Xn) to indicate that X1, . . . , Xn are the variables over which factor
f is deﬁned. Finally, we will allow factors over an empty set of variables. Such factors
are called trivial as they assign a single number to the trivial instantiation ⊤.
There are two key operations that are commonly applied to factors. The ﬁrst is summing
out a variable from a factor and the second is multiplying two factors. We will next deﬁne
these operations and discuss their complexity as they represent the building blocks of
many algorithms for inference with Bayesian networks, including variable elimination.
We already discussed the summing-out operation informally in the previous section, so
here we present the formal deﬁnition.
Deﬁnition 6.2. Let f be a factor over variables X and let X be a variable in X. The result
of summing out variable X from factor f is another factor over variables Y = X \ {X},
which is denoted by 
X f and deﬁned as

X
f

(y)
def=

x
f (x, y).
■
To visualize this summing-out operation, consider factor f1 in Figure 6.3. The result
of summing out variable D from this factor is then
B
C

D f1
true
true
1
true
false
1
false
true
1
false
false
1
1 A factor is also known as a potential.

130
INFERENCE BY VARIABLE ELIMINATION
Algorithm 1 SumOutVars(f (X), Z)
input:
f (X):
factor over variables X
Z:
a subset of variables X
output: a factor corresponding to 
Z f
main:
1: Y←X −Z
2: f ′←a factor over variables Y where f ′(y) = 0 for all y
3: for each instantiation y do
4:
for each instantiation z do
5:
f ′(y)←f ′(y) + f (yz)
6:
end for
7: end for
8: return f ′
Note that if we sum out variables B and C from the previous factor, we get a trivial factor
that assigns the number 4 to the trivial instantiation:

B

C

D f1
⊤
4
The summing-out operation is commutative; that is,

Y

X
f =

X

Y
f .
Hence, it is meaningful to talk about summing out multiple variables from a factor without
ﬁxing the variable order. This also justiﬁes the notation 
X f , where X is a set of variables.
Summing out variables X is also known as marginalizing variables X. Moreover, if Y are
the other variables of factor f , then 
X f is also called the result of projecting factor f
on variables Y.
Algorithm 1 provides pseudocode for summing out any number of variables from a
factor within O(exp(w)) time and space, where w is the number of variables over which
the factor is deﬁned. It is important to keep this complexity in mind as it is essential for
analyzing the complexity of various inference algorithms based on variable elimination.
We now discuss the second operation on factors, which is called multiplication. Con-
sider the two factors, f1(B, C, D) and f2(D, E), in Figure 6.3. To multiply these two
factors is to construct a factor over the union of their variables B, C, D, and E, which is
partially shown here:
B
C
D
E
f1(B, C, D)f2(D, E)
true
true
true
true
.4256 = (.95)(.448)
true
true
true
false
.1824 = (.95)(.192)
true
true
false
true
.0056 = (.05)(.112)
...
...
...
...
...
false
false
false
false
.2480 = (1)(.248)
Note that each instantiation b, c, d, e of the resulting factor is compatible with exactly
one instantiation in factor f1, b, c, d, and exactly one instantiation in factor f2, d, e.

6.4 ELIMINATION AS A BASIS FOR INFERENCE
131
Algorithm 2 MultiplyFactors(f1(X1), . . . , fm(Xm))
input:
f1(X1), . . . , fm(Xm):
factors
output: a factor corresponding to the product m
i=1 fi
main:
1: Z←m
i=1 Xi
2: f ←a factor over variables Z where f (z) = 1 for all z
3: for each instantiation z do
4:
for i = 1 to m do
5:
xi←instantiation of variables Xi consistent with z
6:
f (z)←f (z)fi(xi)
7:
end for
8: end for
9: return f
The value assigned by the new factor to instantiation b, c, d, e is then the product of
numbers assigned by factors f1 and f2 to these compatible instantiations. The following
is the formal deﬁnition of factor multiplication.
Deﬁnition 6.3. The result of multiplying factors f1(X) and f2(Y) is another factor over
variables Z = X ∪Y, which is denoted by f1f2 and deﬁned as
(f1f2) (z)
def= f1(x)f2(y),
where x and y are compatible with z; that is, x ∼z and y ∼z.
■
Factor multiplication is commutative and associative. Hence, it is meaningful to talk
about multiplying a number of factors without specifying the order of this multiplication
process.
Algorithm 2 provides pseudocode for multiplying m factors within O(m exp(w)) time
and space, where w is the number of variables in the resulting factor. Again, it is important
to keep this complexity in mind as factor multiplication is central to many algorithms for
inference in Bayesian networks.
6.4 Elimination as a basis for inference
Consider again the Bayesian network in Figure 6.1 and suppose that our goal is to compute
the joint probability distribution for this network, which is given in Figure 6.2. We can
do this in two ways. First, we can use the chain rule for Bayesian networks, which allows
us to compute the probability of each instantiation a, b, c, d, e as a product of network
parameters:
Pr(a, b, c, d, e) = θe|c θd|bc θc|a θb|a θa.
Another more direct method is to multiply the CPTs for this Bayesian network, viewing
each CPT as a factor. In particular, it is easy to verify that the factor
E|C D|BC C|A B|A A

132
INFERENCE BY VARIABLE ELIMINATION
A
B
C
A
A
true
.6
false
.4
A
B
B|A
true
true
.9
true
false
.1
false
true
.2
false
false
.8
B
C
C|B
true
true
.3
true
false
.7
false
true
.5
false
false
.5
Figure 6.4: A Bayesian network.
is indeed the joint probability distribution given in Figure 6.2. This shows one of the
key applications of factor multiplication, as it allows us to express the joint probability
distribution of any Bayesian network as a product of its CPTs.
Suppose now that our goal is to compute the marginal distribution over variables D
and E in the previous Bayesian network. We know that this marginal can be obtained by
summing out variables A, B, and C from the joint probability distribution. Hence, the
marginal we want corresponds to the following factor:
Pr(D, E) =

A,B,C
E|C D|BC C|A B|A A.
This equation shows the power of combining the operation for summing out variables
with the one for multiplying factors as they are all we need to compute the marginal
over any set of variables. However, there is still the problem of complexity since the
multiplication of all CPTs takes time exponential in the number of network variables.
Fortunately, the following result shows that such a multiplication is not necessary in
general.
Theorem 6.1. If f1 and f2 are factors and if variable X appears only in f2, then

X
f1f2 = f1

X
f2.
■
Therefore, if f1, . . . , fn are the CPTs of a Bayesian network and if we want to sum out
variable X from the product f1 . . . fn, it may not be necessary to multiply these factors
ﬁrst. For example, if variable X appears only in factor fn, then we do not need to multiply
any of the factors before we sum out X since

X
f1 . . . fn = f1 . . . fn−1

X
fn.
However, if variable X appears in two factors, say, fn−1 and fn, then we only need to
multiply these two factors before we can sum out X since

X
f1 . . . fn = f1 . . . fn−2

X
fn−1fn.
In general, to sum out variable X from the product f1 . . . fn, all we need to multiply are
factors fk that include X and then sum out variable X from the resulting factor 
k fk.
Let us consider an example with respect to the Bayesian network in Figure 6.4. Our
goal here is to compute the prior marginal on variable C, Pr(C), by ﬁrst eliminating
variable A and then variable B. There are two factors that mention variable A, A and

6.5 COMPUTING PRIOR MARGINALS
133
B|A. We must multiply these factors ﬁrst and then sum out variable A from the resulting
factor. Multiplying A and B|A, we get
A
B
AB|A
true
true
.54
true
false
.06
false
true
.08
false
false
.32
Summing out variable A, we get
B

A AB|A
true
.62 = .54 + .08
false
.38 = .06 + .32
We now have two factors, 
A AB|A and C|B, and we want to eliminate variable B.
Since B appears in both factors, we must multiply them ﬁrst and then sum out B from the
result. Multiplying,
B
C
C|B

A AB|A
true
true
.186
true
false
.434
false
true
.190
false
false
.190
Summing out,
C

B C|B

A AB|A
true
.376
false
.624
This factor is then the prior marginal for variable C, Pr(C). Therefore, according to the
Bayesian network in Figure 6.4, the probability of C =true is .376 and the probability of
C =false is .624.
6.5 Computing prior marginals
Algorithm 3, VE PR1, provides pseudocode for computing the marginal over some vari-
ables Q in a Bayesian network based on the previous elimination method. The algorithm
takes as input a Bayesian network N, variables Q, and an elimination order π over remain-
ing variables. Here π(1) is the ﬁrst variable in the order, π(2) is the second variable, and
so on. The algorithm iterates over each variable π(i) in the order, identifying all factors
fk that contain variable π(i), multiplying them to yield factor f , summing out variable
π(i) from f , and ﬁnally replacing factors fk by factor 
π(i) f . When all variables in
the order π are eliminated, we end up with a set of factors over variables Q. Multiplying
these factors gives the answer to our query, Pr(Q). From now on, we will use the phrase
eliminate variable π(i) to denote the multiplication of factors fk on Line 3, followed by
summing out variable π(i) on Line 4.
The question that presents itself now is: How much work does algorithm VE PR1
actually do? As it turns out, this is an easy question to answer once we observe that the
real work done by the algorithm is on Line 3 and Line 4. Each of the steps on these lines
takes time and space that is linear in the size of factor fi constructed on Line 4. Note that

134
INFERENCE BY VARIABLE ELIMINATION
Algorithm 3 VE PR1(N, Q, π)
input:
N:
Bayesian network
Q:
variables in network N
π:
ordering of network variables not in Q
output: the prior marginal Pr(Q)
main:
1: S←CPTs of network N
2: for i = 1 to length of order π do
3:
f ←
k fk, where fk belongs to S and mentions variable π(i)
4:
fi←
π(i) f
5:
replace all factors fk in S by factor fi
6: end for
7: return 
f ∈S f
factor fi on Line 4 and factor f on Line 3 differ by only one variable π(i). In the example
of Figure 6.4 where we eliminated variable A ﬁrst and then variable B, the largest factor
fi we had to construct had one variable in it. This can be seen in the following expression,
which explicates the number of variables appearing in each factor fi constructed on Line 4
of VE PR1:

B
C|B
1




A
AB|A



1
.
However, suppose that we eliminate variable B ﬁrst and then variable A. Our compu-
tation would then be

A
A
2




B
B|AC|B



1
which involves constructing a factor fi with two variables. Therefore, although any
order for variable elimination will do, the particular order we use is typically signiﬁcant
computationally. Some orders are better than others in that they lead to constructing
smaller intermediate factors on Line 4. Therefore, to minimize the resources consumed
by VE PR1 (both time and space) we must choose the “best” order, a subject that we
consider in the next section.
Before we show how to construct good elimination orders, we ﬁrst show how to
formally measure the quality of a particular elimination order. We start with the following
result.
Theorem 6.2. If the largest factor constructed on Line 4 of Algorithm 3, VE PR1, has
w variables, the complexity of Lines 3–5 is then O(n exp(w)), where n is the number of
variables in the Bayesian network.
■
The number w is known as the width of used order π and is taken as a measure of the
order quality. Therefore, we want to choose an order that has the smallest possible width.

6.6 CHOOSING AN ELIMINATION ORDER
135
Note that the total time and space complexity of VE PR1 is O(n exp(w) + n exp(|Q|))
as we ﬁnally construct a factor over variables Q on Line 7, which can be done in
O(n exp(|Q|)) time. If the number of variables Q is bounded by a constant, then the
complexity of the algorithm drops to O(n exp(w)). We may also choose to skip the mul-
tiplication of factors on Line 7 and simply return the factors in S. In this case, we are
keeping the marginal for variables Q in factored form and the complexity of VE PR1 is
O(n exp(w)) regardless of variables Q. Finally, note that this complexity analysis assumes
that we can identify factors that mention variable π(i) on Line 3 in time linear in the num-
ber of such factors. This can be accomplished using an indexing scheme that we discuss
in Section 6.10, leading to a variation on algorithm VE PR1 known as bucket elimination.
6.6 Choosing an elimination order
Suppose that we are presented with two orders π1 and π2 and we need to choose one of
them. We clearly want to choose the one with the smaller width but how can we compute
the width of each order? One straightforward but inefﬁcient method is to modify VE PR1
in order to keep track of the number of variables appearing in factor fi on Line 4. To
compute the width of a particular order, we simply execute the algorithm on that order
and return the maximum number of variables that any factor fi ever contained. This will
work but we can do much better than this, as we shall see.
Consider the network in Figure 6.1 and suppose that we want to compute the marginal
for variable E by eliminating variables according to the order B, C, A, D. The following
listing provides a trace of VE PR1:
i
π(i)
S
fi
w
A B|A C|A D|BC E|C
1
B
A C|A E|C f1(A, C, D)
f1 = 
B B|A D|BC
3
2
C
A f2(A, D, E)
f2 = 
C C|A E|C f1(A, C, D)
3
3
A
f3(D, E)
f3 = 
A A f2(A, D, E)
2
4
D
f4(E)
f4 = 
D f3(D, E)
1
The second column lists variables according to their elimination order. The third column
lists the set of factors S computed by the algorithm at the end of each iteration i. The
fourth column lists the factor fi constructed on Line 4 at iteration i, and the ﬁnal column
lists the size of this constructed factor as measured by the number of its variables. The
maximum of these sizes is the width of the given order, which is 3 in this case. Note that
such an algorithm trace can be constructed without having to execute VE PR1. That is, to
eliminate variable π(i) that appears in factors f (Xk), we simply replace such factors by
a newly constructed factor over the variables 
k Xk \ {π(i)}.
We can also compute the width of an order by simply operating on an undirected
graph that explicates the interactions between the Bayesian network CPTs. Such a graph
is deﬁned formally here.
Deﬁnition 6.4. Let f1, . . . , fn be a set of factors. The interaction graph G of these
factors is an undirected graph constructed as follows. The nodes of G are the variables
that appear in factors f1, . . . , fn. There is an edge between two variables in G iff those
variables appear in the same factor.
■
Another way to visualize the interaction graph G is to realize that the variables Xi of each
factor fi form a clique in G, that is, the variables are pairwise adjacent.

136
INFERENCE BY VARIABLE ELIMINATION
S1 : A
B|A
C|A
D|BC
E|C
A
E
D
C
B
S2 : A
C|A
E|C
f1(A, C, D)
A
E
D
C
S3 : A
f2(A, D, E)
A
E
D
S4 : f3(D, E)
E
D
S5 : f4(E)
E
Figure 6.5: Interaction graphs resulting from the elimination of variables B, C, A, D.
Figure 6.5 depicts the interaction graph that corresponds to each iteration of the above
trace of VE PR1. There are two key observations about these interaction graphs:
1. If G is the interaction graph of factors S, then eliminating a variable π(i) from S leads to
constructing a factor over the neighbors of π(i) in G. For example, eliminating variable
B from the factors S1 in Figure 6.5 leads to constructing a factor over variables A, C, D,
which are the neighbors of B in the corresponding interaction graph.
2. Let S′ be the factors that result from eliminating variable π(i) from factors S. If G′ and
G are the interaction graphs of S′ and S, respectively, then G′ can be obtained from G as
follows:
(a) Add an edge to G between every pair of neighbors of variable π(i) that are not already
connected by an edge.
(b) Delete variable π(i) from G.

6.6 CHOOSING AN ELIMINATION ORDER
137
Algorithm 4 OrderWidth(N, π)
input:
N:
Bayesian network
π:
ordering of the variables in network N
output: the width of elimination order π
main:
1: G←interaction graph of the CPTs in network N
2: w←0
3: for i = 1 to length of elimination order π do
4:
w←max(w, d), where d is the number of π(i)’s neighbors in G
5:
add an edge between every pair of non-adjacent neighbors of π(i) in G
6:
delete variable π(i) from G
7: end for
8: return w
Algorithm 5 MinDegreeOrder(N, X)
input:
N:
Bayesian network
X:
variables in network N
output: an ordering π of variables X
main:
1: G←interaction graph of the CPTs in network N
2: for i = 1 to number of variables in X do
3:
π(i)←a variable in X with smallest number of neighbors in G
4:
add an edge between every pair of non-adjacent neighbors of π(i) in G
5:
delete variable π(i) from G and from X
6: end for
7: return π
In fact, (a) corresponds to multiplying all factors that contain variable π(i) in S, as the
resulting factor must be over variable π(i) and all its neighbors in G. Moreover, (b)
corresponds to summing out variable π(i) from the resulting factor.
Algorithm 4 provides pseudocode for computing the width of an order and a cor-
responding Bayesian network. It does this by maintaining an interaction graph G for
the set of factors S maintained by VE PR1 during each iteration. One can use Algo-
rithm OrderWidth to measure the quality of a particular ordering before using it.
Computing the width of a particular variable order is useful when we have to choose
between a small number of orders. However, when the number of orders is large we need
to do better than simply computing the width of each potential order. As it turns out,
computing an optimal order is an NP-hard problem, but there are a number of heuristic
approaches that tend to generate relatively good orders.
One of the more popular heuristics is also one of the simplest: Always eliminate the
variable that leads to constructing the smallest factor possible. If we are maintaining the
interaction graph as we eliminate variables, this basically means that we always eliminate
the variable that has the smallest number of neighbors in the current interaction graph. This
heuristic method is given in Algorithm 5 and is known as the min-degree heuristic. It is also

138
INFERENCE BY VARIABLE ELIMINATION
Algorithm 6 MinFillOrder(N, X)
input:
N:
Bayesian network
X:
variables in network N
output: an ordering π of variables X
main:
1: G←interaction graph of the CPTs in network N
2: for i = 1 to number of variables in X do
3:
π(i)←a variable in X that adds the smallest number of edges on Line 4
4:
add an edge between every pair of non-adjacent neighbors of π(i)
5:
delete variable π(i) from G and from X
6: end for
7: return π
known that min-degree is optimal when applied to a network that has some elimination
order of width ≤2. Another popular heuristic for constructing elimination orders, which
is usually more effective than min-degree, is to always eliminate the variable that leads
to adding the smallest number of edges on Line 4 of MinDegreeOrder (called ﬁll-in
edges). This heuristic method is given in Algorithm 6 and is known as the min-ﬁll heuristic.
Algorithms for constructing elimination orders are further discussed in Chapter 9.
6.7 Computing posterior marginals
We now present a generalization of Algorithm 3, VE PR1, for computing the posterior
marginal for any set of variables. For example, if we take Q = {D, E} and e: A=true,
B =false in the network of Figure 6.1, we want to compute the following factor:
D
E
Pr(Q|e)
true
true
.448
true
false
.192
false
true
.112
false
false
.248
where the third row asserts that
Pr(D =false, E =true|A=true, B =false) = .112
More generally, given a Bayesian network N, a set of variables Q, and an instantiation
e, we want to compute the posterior marginal Pr(Q|e) for variables Q. Recall that prior
marginals are a special case of posterior marginals when e is the trivial instantiation.
We ﬁnd it more useful to compute a variation on posterior marginals called joint
marginals, Pr(Q, e). That is, instead of computing the probability of q given e,
Pr(q|e), we compute the probability of q and e, Pr(q, e). If we take Q = {D, E} and
e: A=true, B =false in the network of Figure 6.1, the joint marginal is
D
E
Pr(Q, e)
true
true
.21504
true
false
.09216
false
true
.05376
false
false
.11904

6.7 COMPUTING POSTERIOR MARGINALS
139
For example, the third row says that
Pr(D =false, E =true, A=true, B =false) = .05376
If we add up the probabilities in this factor we get .48, which is nothing but the
probability of evidence e: A=true, B =false. This is always the case since 
q Pr(q, e) =
Pr(e) by case analysis. Hence, by adding up the probabilities that appear in the joint
marginal, we will always get the probability of evidence e. This also means that we can
compute the posterior marginal Pr(Q|e) by simply normalizing the corresponding joint
marginal Pr(Q, e). Moreover, the probability of evidence e is obtained for free.
The method of variable elimination can be extended to compute joint marginals if we
start by zeroing out those rows in the joint probability distribution that are inconsistent
with evidence e.
Deﬁnition 6.5. The reduction of factor f (X) given evidence e is another factor over
variables X, denoted by f e, and deﬁned as
f e(x)
def=
 f (x),
if x ∼e
0,
otherwise.
■
For example, given the factor
D
E
f
true
true
.448
true
false
.192
false
true
.112
false
false
.248
and evidence e: E =true, we have
D
E
f e
true
true
.448
true
false
0
false
true
.112
false
false
0
We often omit the zeroed-out rows, writing
D
E
f e
true
true
.448
false
true
.112
Consider now the network of Figure 6.1 and let Q = {D, E} and e: A=true, B =false.
The joint marginal Pr(Q, e) can be computed as follows:
Pr(Q, e) =

A,B,C

E|C D|BC C|A B|A A
e.
(6.1)
Although this provides a systematic method for computing joint marginals, we still have
the problem of complexity as (6.1) requires that we multiply all CPTs before we start
eliminating variables. Fortunately, this is not needed, as shown by the following result.
Theorem 6.3. If f1 and f2 are two factors and e is an instantiation, then
(f1f2)e = f1
ef2
e.
■

140
INFERENCE BY VARIABLE ELIMINATION
Hence (6.1) reduces to
Pr(Q = {D, E}, e) =

A,B,C
e
E|C e
D|BC e
C|A e
B|A e
A,
which keeps the joint probability distribution in factored form, therefore allowing us to
use VE PR1 on the reduced CPTs.
Consider now the Bayesian network in Figure 6.4. Let Q = {C}, e : A=true, and
suppose that we want to compute the joint marginal Pr(Q, e) by eliminating variable A
ﬁrst and then variable B. We ﬁrst need to reduce the network CPTs given evidence e,
which gives
A
e
A
true
.6
A
B
e
B|A
true
true
.9
true
false
.1
B
C
e
C|B
true
true
.3
true
false
.7
false
true
.5
false
false
.5
The formula we need to evaluate is then
Pr(Q, e) =

B

A
e
A e
B|A e
C|B
=

B
e
C|B

A
e
A e
B|A.
All intermediate factors needed to evaluate this formula are shown here:
A
B
e
Ae
B|A
true
true
.54
true
false
.06
B

A e
Ae
B|A
true
.54
false
.06
B
C
e
C|B

A e
Ae
B|A
true
true
.162
true
false
.378
false
true
.030
false
false
.030
C

B e
C|B

A e
Ae
B|A
true
.192
false
.408
Therefore,
Pr(C =true, A=true) = .192
Pr(C =false, A=true) = .408
Pr(A=true) = .600
To compute the posterior marginal Pr(C|A=true), all we have to do is normalize the
previous factor, which gives
C
Pr(C|A=true)
true
.32
false
.68
Therefore, Pr(C =true|A=true) = .32 and Pr(C =false|A=true) = .68.
Algorithm 7, VE PR2, provides pseudocode for computing the joint marginal for a
set of variables Q with respect to a Bayesian network N and evidence e. The algorithm

6.8 NETWORK STRUCTURE AND COMPLEXITY
141
Algorithm 7 VE PR2(N, Q, e, π)
input:
N:
Bayesian network
Q:
variables in network N
e:
instantiation of some variables in network N
π:
an ordering of network variables not in Q
output: the joint marginal Pr(Q, e)
main:
1: S←{f e : f is a CPT of network N}
2: for i = 1 to length of order π do
3:
f ←
k fk, where fk belongs to S and mentions variable π(i)
4:
fi←
π(i) f
5:
replace all factors fk in S by factor fi
6: end for
7: return 
f ∈S f
is a simple modiﬁcation of Algorithm 3, VE PR1, where we reduce the factors of a
Bayesian network before we start eliminating variables. By normalizing the output of
this algorithm, we immediately obtain an algorithm for computing posterior marginals,
Pr(Q|e). Moreover, by adding up the numbers returned by this algorithm, we immediately
obtain an algorithm for computing the probability of evidence, Pr(e). It is not uncommon
to run VE PR2 with Q being the empty set. In this case, the algorithm will eliminate
all variables in the Bayesian network, therefore returning a trivial factor, effectively a
number, that represents the probability of evidence e.
6.8 Network structure and complexity
Suppose we have two Bayesian networks each containing, say, a hundred variables. The
best elimination order for the ﬁrst network has width 3, which means that variable elimi-
nation will do very well on this network if passed an optimal order. The best elimination
order for the second network has width 25, which means that variable elimination will do
relatively poorly on this network regardless of which order it is passed. Why is the second
network more difﬁcult for variable elimination even though both networks have the same
number of variables?
The answer to this question lies in this notion of treewidth, which is a number that
quantiﬁes the extent to which a network resembles a tree structure. In particular, the
more that the network resembles a tree structure, the smaller its treewidth is. Treewidth is
discussed at length in Chapter 9 but we point out here that no complete elimination order
can have a width less than the network treewidth. Moreover, there is an elimination order
whose width equals the network treewidth yet determining such an order is known to be
NP-hard. Figure 6.6 depicts some networks with their treewidths.
We should point out here that when we say an “elimination order” in the context of
deﬁning treewidth, we mean a complete order that includes all network variables. Hence,
the treewidth of a network can be deﬁned as the width of its best complete elimination
order (i.e., the one with the smallest width). Again, we treat the notion of treewidth more

142
INFERENCE BY VARIABLE ELIMINATION
(a) Treewidth=1
(b) Treewidth=1
(c) Treewidth=2
(d) Treewidth=3
(e) Treewidth=3
Figure 6.6: Networks with different treewidth. Determining network treewidth is discussed in Chapter 9.
formally in Chapter 9 but we next present a number of observations about it to further our
intuition about this notion:
r The number of nodes has no genuine effect on treewidth. For example, the network in
Figure 6.6(a) has treewidth 1. The network in Figure 6.6(b) is obtained by doubling the
number of nodes yet the treewidth remains 1.
r The number of parents per node has a direct effect on treewidth. If the number of parents
per node can reach k, the treewidth is no less than k. The network in Figure 6.6(c) was
obtained from the one in Figure 6.6(b) by adding one node and one edge. This addition
increased the maximum number of parents per node from 1 to 2, leading also to an increase
in the treewidth (see Exercise 6.15).
r Loops tend to increase treewidth. The network in Figure 6.6(d) was obtained from the
one in Figure 6.6(c) by creating loops. Note that the number of parents per node did not
increase yet the treewidth has increased from 2 to 3.
r The number of loops per se does not have a genuine effect on treewidth. It is the nature of
these loops which does; their interaction in particular. The network in Figure 6.6(e) has
more loops than the one in Figure 6.6(d) yet it has the same treewidth.
We also point out two important classes of networks with known treewidth. First is the
class of polytree networks, also known as singly connected networks. These are networks
in which there is at most one (undirected) path between any two nodes. The treewidth of
such networks is k, where k is the maximum number of parents that any node may have.
The network in Figure 6.6(c) is singly connected. We also have tree networks, which are

6.9 QUERY STRUCTURE AND COMPLEXITY
143
A
B
C
D
E
A
B
C
E
A
B
Network structure
Joint on B, E
Joint on B
Figure 6.7: Pruning nodes in a Bayesian network given two different queries.
polytrees where each node has at most one parent, leading to a treewidth of at most 1. The
networks in Figure 6.6(a) and Figure 6.6(b) are trees. We ﬁnally note that networks which
are not singly connected are known as multiply connected. Figure 6.6(d) and Figure 6.6(e)
contain two examples.
6.9 Query structure and complexity
Network structure has a major impact on the performance of variable elimination and on
the performance of most algorithms we shall discuss later. This is why such algorithms
are sometimes called structure-based algorithms. However, network structure can be
simpliﬁed based on another important factor: query structure. In general, a query is a
pair (Q, e) where e is an instantiation of evidence variables E and Q is the set of query
variables, where the goal is to compute the joint marginal Pr(Q, e).2 As we discuss next,
the complexity of inference can be very much affected by the number and location of
query and evidence variables within the network structure. The effect of query structure on
the complexity of inference is independent of the used inference algorithm. In particular,
for a given query (Q, e) we next provide two transformations that simplify a Bayesian
network, making it more amenable to inference yet preserving its ability to compute the
joint marginal Pr(Q, e) correctly.
6.9.1 Pruning nodes
Given a Bayesian network N and query (Q, e), one can remove any leaf node (with
its CPT) from the network as long as it does not belong to variables Q ∪E yet not
affect the ability of the network to answer the query correctly. What makes this pruning
operation powerful is that it can be applied iteratively, possibly leading to the pruning
of many network nodes. The result of removing leaf nodes as suggested is denoted by
pruneNodes(N, Q ∪E).
Theorem 6.4. Let N be a Bayesian network and let (Q, e) be a corresponding query. If
N′ = pruneNodes(N, Q ∪E), then Pr(Q, e) = Pr′(Q, e) where Pr and Pr′ are the proba-
bility distributions induced by networks N and N′, respectively.
■
Figure 6.7 depicts a Bayesian network and two of its prunings. In the ﬁrst case,
we are interested in the marginal over variables B and E with no evidence. Therefore,
2 It is possible that we have no evidence, E = ∅, in which case we are interested in the prior marginal for variables
Q. It is also possible that we have no query variables, Q = ∅, in which case we are interested in computing the
probability Pr(e) of evidence e.

144
INFERENCE BY VARIABLE ELIMINATION
D is a leaf node that can be pruned. After this pruning, all leaf nodes appear in the query
and, therefore, cannot be pruned. In the second case, we are interested in the marginal
over variable B with no evidence. Therefore, D and E are leaf nodes that can be pruned.
However, after pruning them node C becomes a leaf node that can also be pruned since it
does not appear in the query. Note that the network in Figure 6.7(a) has treewidth 2 yet
the pruned networks in Figures 6.7(b) and 6.7(c) have treewidth 1.
Pruning nodes can lead to a signiﬁcant reduction in network treewidth if variables
Q ∪E appear close to the network roots. In the worst case, all leaf nodes appear in
variables Q ∪E and no pruning is possible. In the best case, variables Q ∪E contains
only root nodes that permit the pruning of every node except for those in Q ∪E.
6.9.2 Pruning edges
Given a Bayesian network N and a query (Q, e), one can also eliminate some of the
network edges and reduce some of its CPTs without affecting its ability to compute the
joint marginal Pr(Q, e) correctly. In particular, for each edge U →X that originates from
a node U in E, we can:
1. Remove the edge U →X from the network.
2. Replace the CPT X|U for node X by a smaller CPT, which is obtained from X|U by
assuming the value u of parent U given in evidence e. This new CPT corresponds to

U u
X|U.
The result of this operation is denoted by pruneEdges(N, e) and we have the following
result.
Theorem 6.5. Let N be a Bayesian network and let e be an instantiation. If N′ =
pruneEdges(N, e), then Pr(Q, e) = Pr′(Q, e) where Pr and Pr′ are the probability distri-
butions induced by networks N and N′, respectively.
■
Figure 6.8 depicts the result of pruning edges in the network of Figure 6.1 given
evidence C =false. The two edges originating from node C were deleted and CPTs
D|BC and E|C were modiﬁed. In particular, all rows inconsistent with C =false were
removed and the reference to C was dropped from these factors. It is important to stress
that the pruned network is only good for answering queries of the form Pr(q, C =false).
If the instantiation C =false does not appear in the query, the answers returned by the
pruned and original networks may disagree.
6.9.3 Network pruning
The result of pruning nodes and edges for a network will be called pruning the network
given query (Q, e) and denoted by pruneNetwork(N, Q, e). Figure 6.9 depicts a pruning of
the Bayesian network in Figure 6.1 given the query Q = {D} and e : A=true, C =false.
Pruning leads to removing node E and the edges originating from nodes A and C, and
modifying the CPTs B|A, C|A and D|BC. Note that the pruned network in Figure 6.9
has a treewidth of 1, whereas the original network had a treewidth of 2. In general, network
pruning may lead to a signiﬁcant reduction in treewidth, which suggests the following
deﬁnition.
Deﬁnition 6.6. The effective treewidth for a Bayesian network N with respect to query
(Q, e) is the treewidth of pruneNetwork(N, Q, e).
■

6.9 QUERY STRUCTURE AND COMPLEXITY
145
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
A
A
true
.6
false
.4
A
B
B|A
true
true
.2
true
false
.8
false
true
.75
false
false
.25
A
C
C|A
true
true
.8
true
false
.2
false
true
.1
false
false
.9
B
D

C C=false
D|BC
true
true
.9
true
false
.1
false
true
0
false
false
1
E

C C=false
E|C
true
0
false
1
Figure 6.8: Pruning edges in the network of Figure 6.1, where e : C =false.
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
A
A
true
.6
false
.4
B
′
B = 
A A=true
B|A
true
.2
false
.8
C
′
C = 
A A=true
C|A
true
.8
false
.2
B
D
′
D|B = 
C C=false
D|BC
true
true
.9
true
false
.1
false
true
0
false
false
1
Figure 6.9: Pruning the Bayesian network in Figure 6.1 given the query Q = {D} and e : A=true,
C =false.

146
INFERENCE BY VARIABLE ELIMINATION
Algorithm 8 VE PR(N, Q, e)
input:
N:
Bayesian network
Q:
variables in network N
e:
instantiation of some variables in network N
output: the joint marginal Pr(Q, e)
main:
1: N′←pruneNetwork(N, Q, e)
2: π←an ordering of variables not in Q computed with respect to network N′
3: S←{f e : f is a CPT of network N′}
4: for i = 1 to length of order π do
5:
f ←
k fk, where fk belongs to S and mentions variable π(i)
6:
fi←
π(i) f
7:
replace all factors fk in S by factor fi
8: end for
9: return 
f ∈S f
It is important to realize that pruning a Bayesian network can be accomplished in time
that is linear in the size of the network (the size of its CPTs). Therefore, it is usually
worthwhile to prune a Bayesian network before answering queries.
Algorithm 8, VE PR, depicts the most general variable elimination algorithm of this
chapter. Given a query (Q, e), the algorithm prunes the Bayesian network before it starts
eliminating variables. The elimination order used by the algorithm is computed based on
the pruned network, not the original one.
6.9.4 Computing marginals after pruning: An example
We now apply VE PR to the network in Figure 6.1 with Q = {D} and e : A=true,
C =false. The pruning of this network given the query is depicted in Figure 6.9. The
eliminating order π = A, C, B is consistent with the min-degree heuristic. Reducing the
network CPTs given evidence e leads to the following factors:
A
e
A
true
.6
B
′e
B
true
.2
false
.8
C
′e
C
false
.2
B
D
′e
D|B
true
true
.9
true
false
.1
false
true
0
false
false
1
VE PR will then evaluate the following expression:
Pr(D, A=true, C =false) =

B

C

A
e
A ′e
B ′e
C ′e
D|B
=

A
e
A
 
B
′e
B ′e
D|B
 
C
′e
C

.

6.10 BUCKET ELIMINATION
147
All intermediate factors constructed during this process are shown here:
B
D
′e
B′e
D|B
true
true
.18
true
false
.02
false
true
0
false
false
.80

A e
A
⊤
.6
D

B ′e
B′e
D|B
true
.18
false
.82

C ′e
C
⊤
.2
The ﬁnal factor returned by VE PR is then
D

A e
A
 
B ′e
B′e
D|B
 
C ′e
C

true
.0216 = (.6)(.18)(.2)
false
.0984 = (.6)(.82)(.2)
which is the joint marginal Pr(D, A=true, C =false). From this factor, we conclude that
the probability of evidence, Pr(A=true, C =false), is .12. Note how trivial factors act as
scaling constants when multiplied by other factors.
6.10 Bucket elimination
The complexity analysis of Algorithm 8, VE PR, assumes that we can identify all factors
fk that mention a particular variable π(i) in time linear in the number of such factors.
One method for achieving this is to arrange the factors maintained by the algorithm into
buckets, where we have one bucket for each network variable. Consider the network in
Figure 6.1 as an example, and suppose that we want to eliminate variables according to
the order E, B, C, D, A. We can do this by constructing the following buckets, which are
initially populated with network CPTs:
Bucket Label
Bucket Factors
E
E|C
B
B|A, D|BC
C
C|A
D
A
A
That is, each CPT is placed in the ﬁrst bucket (from top) whose label appears in the CPT.
For example CPT C|A is placed in the bucket with label C because this is the ﬁrst bucket
whose label C appears in C|A. The only other bucket whose label appears in C|A is the
one for variable A but that comes later in the order.
Given these buckets, we eliminate variables by processing buckets from top to bottom.
When processing the bucket corresponding to some variable π(i), we are guaranteed that
the factors appearing in that bucket are exactly the factors that mention variable π(i). This
is true initially and remains true after processing each bucket for the following reason.
When processing the bucket of variable π(i), we multiply all factors in that bucket, sum
out variable π(i), and then place the resulting factor fi in the ﬁrst next bucket whose label

148
INFERENCE BY VARIABLE ELIMINATION
appears in fi. For example, after processing the bucket for variable E, the resulting factor

E E|C is placed in the bucket for variable C:
Bucket Label
Bucket Factors
E
B
B|A, D|BC
C
C|A, 
E E|C
D
A
A
If our goal is to obtain the marginal over variables D and A, Pr(D, A), then we only
process the ﬁrst three buckets, E, B, and C. After such processing, the buckets for D and
A will contain a factored representation of the marginal over these two variables. Again,
we can either multiply these factors or simply keep them in factored form.
Another variation on VE PR is in how evidence is handled. In particular, given evidence,
say, e: B =true, E =false we do not reduce CPTs explicitly. Instead, we create two new
factors,
B
λB
true
1
false
0
E
λE
true
0
false
1
and then add these factors to their corresponding buckets:
Bucket Label
Bucket Factors
E
E|C, λE
B
B|A, D|BC, λB
C
C|A
D
A
A
If we process buckets E, B, and C, the last two buckets for D and A will then contain the
joint marginal for these two variables, Pr(D, A, e). Again, we can either multiply the fac-
tors or keep them in factored form. The factors λB and λE are known as evidence indicators.
Moreover, the previous variation on VE PR is known as the method of bucket elimination.
Bibliographic remarks
The variable elimination algorithm was ﬁrst formalized in Zhang and Poole [1994],
although its traces go back to Shachter et al. [1990]. The bucket elimination algorithm
was introduced in Dechter [1996; 1999]. Variable elimination heuristics are discussed in
Kjaerulff [1990]. Pruning network edges and nodes was initially proposed in Shachter
[1990; 1986]. More sophisticated pruning techniques have also been proposed in Lin and
Druzdzel [1997].
6.11 Exercises
6.1. Consider the Bayesian network in Figure 6.1 on Page 127.
(a)
Use variable elimination to compute the marginals Pr(Q, e) and Pr(Q|e) where Q = {E}
and e : D =false. Use the min-degree heuristic for determining the elimination order,
breaking ties by choosing variables that come ﬁrst in the alphabet. Show all steps.

6.11 EXERCISES
149
A
B
C
D
A
A
true
.2
false
.8
A
B
B|A
true
true
.5
true
false
.5
false
true
.0
false
false
1.0
B
C
C|B
true
true
1.0
true
false
.0
false
true
.5
false
false
.5
B
D
D|B
true
true
.75
true
false
.25
false
true
.3
false
false
.7
Figure 6.10: A Bayesian network.
(b)
Prune the network given the query Q = {A} and evidence e : E =false. Show the steps
of node and edge pruning separately.
6.2. Consider the Bayesian network in Figure 6.10. Use variable elimination with ordering D, C, A
to compute Pr(B, C =true), Pr(C =true), and Pr(B|C =true).
6.3. Consider a chain network C0→C1→· · · →Cn. Suppose that variable Ct, for t ≥0, denotes
the health state of a component at time t. In particular, let each Ct take on states ok and
faulty. Let C0 denote component birth where Pr(C0 =ok) = 1 and Pr(C0 =faulty) = 0. For
each t > 0, let the CPT of Ct be
Pr(Ct =ok|Ct−1 =ok) = λ
Pr(Ct =faulty|Ct−1 =faulty) = 1.
That is, if a component is healthy at time t −1, then it remains healthy at time t with probability
λ. If a component is faulty at time t −1, then it remains faulty at time t with probability 1.
(a)
Using variable elimination with variable ordering C0, C1, compute Pr(C2).
(b)
Using variable elimination with variable ordering C0, C1, . . . , Cn−1, compute Pr(Cn).
6.4. Prove the technique of bypassing nodes as given by Equation 5.1 on Page 91.
6.5. Consider a naive Bayes structure with edges X →Y1, . . . , X →Yn.
(a)
What is the width of variable order Y1, . . . , Yn, X?
(b)
What is the width of variable order X, Y1, . . . , Yn?
6.6. Consider a two-layer Bayesian network N with nodes X ∪Y, X ∩Y = ∅, where each X ∈X
is a root node and each Y ∈Y is a leaf node with at most k parents. In such a network,
all edges in N are directed from a node in X to a node in Y. Suppose we are interested in
computing Pr(Y) for all Y ∈Y. What is the effective treewidth for N for each one of these
queries?
6.7. Prune the network in Figure 6.11 given the following queries:
(a) Q = {B, E} and e : A=true
(b) Q = {A} and e : B =true, F =true
Show the steps of node and edge pruning separately.
6.8. What is the width of order A, B, C, D, E, F, G, H
with respect to the network in
Figure 6.11?
6.9. Compute an elimination order for the variables in Figure 6.11 using the min-degree method.
In case of a tie, choose variables that come ﬁrst alphabetically.
6.10. What is the treewidth of the network in Figure 6.11? Justify your answer.

150
INFERENCE BY VARIABLE ELIMINATION
A
B
D
F
C
E
G
H
Figure 6.11: A Bayesian network structure.
6.11. Given a Bayesian network that has a tree structure with at least one edge, describe an efﬁcient
algorithm for obtaining an elimination order that is guaranteed to have width 1.
6.12. Given an elimination order π that covers all network variables, show how we can use this
particular order and Algorithm 8, VE PR, to compute the marginal Pr(X, e) for every net-
work variable X and some evidence e. What is the total complexity of computing all such
marginals?
6.13. Prove the following: For factors fi, fj, and evidence e: (fifj)e = fiefj e.
6.14. Prove the following: If N′ = pruneEdges(N, e), then Pr(Q, e) = Pr′(Q, e) for every Q. Hint:
Prove that the joint distributions of N and N′ agree on rows that are consistent with evi-
dence e.
6.15. Suppose we have a Bayesian network containing a node with k parents. Show that every
variable order that contains all network variables must have width at least k.
6.16. Consider the naive Bayes structure C →A1, . . . , C →Am, where all variables are binary.
Show how the algorithm of variable elimination can be used to compute a closed form for the
conditional probability Pr(c|a1, . . . , am).
6.17. Suppose we have a Bayesian network N and evidence e with some node X having parents
U1, . . . , Un.
(a)
Show that if X is a logical OR node that is set to false in e, we can prune all edges
incoming into X and assert hard evidence on each parent.
(b)
Show that if X is a noisy OR node that is set to false in e, we can prune all edges
incoming into X and assert soft evidence on each parent.
In particular, for each case identify a network N′ and evidence e′ where all edges incoming
into X have been pruned from N and where Pr(x, e) = Pr′(x, e′) for every network ins-
tantiation x.
6.18. Suppose that f1 is a factor over variables XY and f2 is a factor over variables XZ. Show
that

X f1
 
X f2

is an upper bound on 
X f1f2 in the following sense:

X
f1
 
X
f2

(u) ≥

X
f1f2

(u),
for all instantiations u of U = Y ∪Z. Show how this property can be used to produce an
approximate version of VE PR that can have a lower complexity yet is guaranteed to produce
an upper bound on the true marginals.

6.12 PROOFS
151
6.12 Proofs
PROOF OF THEOREM 6.1. Let f1 and f2 be over variables X1 and X2, respectively,
where X ̸∈X1 and X ∈X2. Let Y = X2 \ {X} and Z = X1 ∪Y. If for some z, x1 ∼z and
y ∼z, then

X
f1f2

(z) =

x
f1(x1)f2(xy)
= f1(x1)

x
f2(xy)
= f1(x1)

X
f2

(y).
Hence, 
X f1f2 = f1

X f2.
■
PROOF OF THEOREM 6.2. Note that n is also the number of network CPTs and, hence,
is the initial size of set S. Let R be the network variables other than Q. Suppose that we
have to multiply mi factors when eliminating variable π(i). The complexity of Lines 3–5
is then
|R|

i=1
O(mi exp(w))
since O(mi exp(w)) is the complexity of eliminating variable π(i). When we are about to
eliminate this variable, the number of factors in the set S is n −i−1
k=1(mk −1). Hence,
mi ≤n −i−1
k=1(mk −1) and i
k=1 mk ≤n + (i −1). Therefore, |R|
i=1 mi = O(n) and
the complexity of Lines 3–5 is O(n exp(w)).
■
PROOF OF THEOREM 6.3. Left to Exercise 6.13.
■
PROOF OF THEOREM 6.4. For any CPT X|U, the result of summing out variable X
from X|U is a factor that assigns 1 to each of its instantiations:

X
X|U

(u) = 1.
Multiplying this factor by any other factor f that includes variables U will give factor f
back. Therefore, if a leaf variable X does not belong to Q nor to E, we can sum it out
ﬁrst, leading to the identity factor. This is why we can always prune such a leaf variable
from the network.
■
PROOF OF THEOREM 6.5. Left to Exercise 6.14.
■

7
Inference by Factor Elimination
We present in this chapter a variation on the variable elimination algorithm, known as the
jointree algorithm, which can be understood in terms of factor elimination. This algorithm
improves on the complexity of variable elimination when answering multiple queries. It
also forms the basis for a class of approximate inference algorithms that we discuss in
Chapter 14.
7.1 Introduction
Consider a Bayesian network and suppose that our goal is to compute the posterior
marginal for each of its n variables. Given an elimination order of width w, we can
compute a single marginal using variable elimination in O(n exp(w)) time and space, as
we explained in Chapter 6. To compute all these marginals, we can then run variable
elimination O(n) times, leading to a total complexity of O(n2 exp(w)).
For large networks, the n2 factor can be problematic even when the treewidth is small.
The good news is that we can avoid this complexity and compute marginals for all
networks variables in only O(n exp(w)) time and space. This can be done using a more
reﬁned algorithm known as the jointree algorithm, which is the main subject of this
chapter.1 The jointree algorithm will also compute the posterior marginals for other sets
of variables, including all network families, where a family consists of a variable and its
parents in the Bayesian network. Family marginals are especially important for sensitivity
analysis, as discussed in Chapter 16, and for learning Bayesian networks, as discussed in
Chapters 17 and 18.
There are a number of ways to derive the jointree algorithm. The derivation we adopt
in this chapter is based on eliminating factors instead of eliminating variables and using
elimination trees instead of elimination orders. We start by presenting a simple factor
elimination algorithm in Section 7.2 and then discuss elimination trees in Section 7.3.
Two reﬁnements on the algorithm will be given in Sections 7.4 and 7.5, leading to a
message-passing formulation that achieves the desired computational complexity. We
then introduce jointrees in Section 7.6 as a tool for generating efﬁcient elimination trees.
We ﬁnally provide a more classical treatment of the jointree algorithm in Section 7.7,
where we express it in terms of jointrees as is commonly done in the literature. We also
discuss two important variations on the algorithm in this section that can vary in their time
and space complexity.
1 The algorithm is also called the clique-tree algorithm or the tree-clustering algorithm. The reasons for these names
will become more obvious in Chapter 9.
152

7.2 FACTOR ELIMINATION
153
Algorithm 9 FE1(N, Q)
input:
N:
a Bayesian network
Q:
a variable in network N
output: the prior marginal Pr(Q)
main:
1: S←CPTs of network N
2: fr←a factor in S that contains variable Q
3: while S has more than one factor do
4:
remove a factor fi ̸= fr from set S
5:
V←variables that appear in factor fi but not in S
6:
fj←fj

V fi for some factor fj in S
7: end while
8: return project(fr, Q)
7.2 Factor elimination
Suppose that we wish to compute the prior marginal over some variable Q in a Bayesian
network. A variable elimination algorithm will compute this marginal by eliminating every
other variable from the network. On the other hand, a factor elimination algorithm will
compute this marginal by eliminating all factors except for one that contains the variable Q.
Deﬁnition 7.1. The elimination of factor fi from a set of factors S is a two-step process.
We ﬁrst eliminate all variables V that appear only in factor fi and then multiply the
result 
V fi by some other factor fj in the set S.
■
Algorithm 9, FE1, provides the pseudocode for computing the marginal over a variable Q
using factor elimination. This algorithm makes use of the factor operation project(f, Q),
which simply sums out all variables not in Q:
project(f, Q)
def=

vars(f )−Q
f.
The correctness of FE1 is easy to establish as it can be viewed as a variation on variable
elimination. In particular, while variable elimination will eliminate one variable at a time
factor elimination will eliminate a set of variables V at once. Since these variables appear
only in factor fi, all we have to do is replace the factor fi by a new factor 
V fi as
suggested by Theorem 6.1 of Chapter 6. However, factor elimination takes one extra step,
which is multiplying this new factor by some other factor fj. Note that after each iteration
of the algorithm, the number of factors in S will decrease by one. After enough iterations,
S will contain a single factor fr that contains the variable Q. Projecting this factor on Q
provides the answer to our query.
As is clear from this elimination strategy, there are two choices to be made at each
iteration:
1. Choosing a factor fi to be eliminated on Line 4
2. Choosing a factor fj to be multiplied into on Line 6

154
INFERENCE BY FACTOR ELIMINATION
A
B
C
A
fA = A
true
.6
false
.4
A
B
fB = B|A
true
true
.9
true
false
.1
false
true
.2
false
false
.8
B
C
fC = C|B
true
true
.3
true
false
.7
false
true
.5
false
false
.5
Figure 7.1: A Bayesian network.
As with eliminating variables, any set of choices will provide a valid answer to our query
but some choices will be better than others computationally. We return to this issue at a
later stage.
Let us now consider the network in Figure 7.1 and suppose we want to compute the
marginal over variable C using FE1. Initially, we have three factors in S:
A
fA
true
.6
false
.4
A
B
fB
true
true
.9
true
false
.1
false
true
.2
false
false
.8
B
C
fC
true
true
.3
true
false
.7
false
true
.5
false
false
.5
If we remove factor fA from S, we get V = ∅and 
∅fA = fA. If we multiply this factor
into factor fB, the set S becomes
A
B
fAfB
true
true
.54
true
false
.06
false
true
.08
false
false
.32
B
C
fC
true
true
.3
true
false
.7
false
true
.5
false
false
.5
Let us now remove factor fAfB from S, which leads to V = {A} and
B

A fAfB
true
.62
false
.38
Multiplying this factor into fC leads to the following new set S:
B
C
fC

A fAfB
true
true
.186
true
false
.434
false
true
.190
false
false
.190
The last step of FE1 will project the pervious factor on variable C, leading to
C
project(fC

A fAfB, C)
true
.376
false
.624
which is the answer to our query.

7.3 ELIMINATION TREES
155
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
Figure 7.2: Two elimination trees for the CPTs associated with the network structure at the top.
Our discussion here was restricted to computing the marginal over a single variable.
However, suppose that we wish to compute the marginal over a set of variables Q that
are not contained in any existing factor. The simplest way to do this is by including an
auxiliary factor f (Q), where f (q) = 1 for all instantiations q. We show later, however,
that this may not be necessary as our ﬁnal version of factor elimination will compute
marginals over sets of variables that may not be contained in any factor.
7.3 Elimination trees
In variable elimination, the notion of an elimination order was used to specify a strategy
for controlling the elimination process. Moreover, the amount of work performed by
variable elimination was determined by the quality (width) of the used order.
In factor elimination, we appeal to trees for specifying an elimination strategy. Specif-
ically, we will show that each organization of factors into a tree structure represents a
particular factor elimination strategy. Moreover, we will deﬁne a quality measure for such
trees (also called width) that can be used to quantify the amount of work performed by
elimination algorithms driven by these trees. Figure 7.2 depicts two such trees, which we
call elimination trees, for the same set of factors.
Deﬁnition 7.2. An elimination tree for a set of factors S is a pair (T , φ) where T is
a tree. Each factor in S is assigned to exactly one node in tree T , where φi denotes
the product of factors assigned to node i in tree T . We also use vars(i) to denote the
variables appearing in factor φi.
■
In Figure 7.2, the elimination tree (T , φ) on the left has ﬁve nodes {1, . . . , 5}, which
are in one-to-one correspondence with the given factors. For example, φ2 = f (AB) and
φ4 = f (BCD). Moreover, vars(2) = {A, B} and vars(4) = {B, C, D}. In general, a node

156
INFERENCE BY FACTOR ELIMINATION
Algorithm 10 FE2(N, Q, (T , φ), r)
input:
N:
Bayesian network
Q:
some variables in network N
(T , φ):
elimination tree for the CPTs of network N
r:
a node in tree T where Q ⊆vars(r)
output: the prior marginal Pr(Q)
main:
1: while tree T has more than one node do
2:
remove a node i ̸= r having a single neighbor j from tree T
3:
V←variables appearing in φi but not in remaining tree T
4:
φj←φj

V φi
5: end while
6: return project(φr, Q)
in an elimination tree may have multiple factors assigned to it or no factors at all.2 For
many of the concrete examples we examine, there will be a one-to-one correspondence
between factors and nodes in an elimination tree.
When using an elimination tree for computing the marginal over variables Q, we need
to choose a special node r, called a root, such that Q ⊆vars(r). For example, if our goal
is to compute the marginal over variable C in Figure 7.2, then nodes 3, 4 or 5 can all act as
roots. This condition on root variables is not strictly needed but will simplify the current
discussion. We say more about this later.
Given an elimination tree and a corresponding root r, our elimination strategy will
proceed as follows. We eliminate a factor φi only if it has a single neighbor j and only if
i ̸= r. To eliminate the factor φi, we ﬁrst sum out variables V that appear in φi but not in
the rest of the tree, and then multiply the result 
V φi into the factor φj associated with
its single neighbor j.
Algorithm 10, FE2, which is a reﬁnement of FE1, makes its elimination choices based
on a given elimination tree and root. At each iteration, the algorithm eliminates a factor φi
and its corresponding node i. After all nodes i ̸= r have been eliminated, projecting the
factor φr on variables Q yields the answer to our query. Note that we still need to make a
choice on Line 2 of FE2 since we may have more than one node i in the tree that satisﬁes
the stated properties. However, it shall become clear later that the choice made at this step
does not affect the amount of work done by the algorithm.
Figure 7.3 depicts a trace of FE2 for computing the marginal over variable C. The four
elimination steps are as follows:
Step 1: Eliminate f2 and update f1 ←f1

∅f2.
Step 2: Eliminate f1 and update f4 ←f4

∅f1.
Step 3: Eliminate f4 and update f3 ←f3

BD f4.
Step 4: Eliminate f5 and update f3 ←f3

E f5.
The ﬁnal factor f3 is over variables A, C. Projecting it over C gives the desired result.
2 If no factors are assigned to node i, then φi is a trivial factor that assigns the number 1 to the trivial instantiation.

7.4 SEPARATORS AND CLUSTERS
157
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
1
4
3
5
f(AB)
f(AC)
f(CE)
Eliminate f2
4
3
5
f(AC)
f(CE)
f(BCD)
Eliminate f1
f (ABCD)
3
5
f(AC)
f(CE)
Eliminate f4
3
f (AC)
Eliminate f5
Figure 7.3: A trace of factor elimination with node 3 as the root.
When using variable elimination to compute marginals, any elimination order will
lead to correct results yet the speciﬁc order we choose may have a considerable effect
on the amount of work performed by the algorithm. The situation is similar with factor
elimination. That is, any elimination tree will lead to correct results yet some trees will
lead to less work than others. We address this point toward the end of the chapter. In the
next two sections, we focus on another aspect of factor elimination that allows it to have a
better complexity than standard variable elimination. In particular, we ﬁrst introduce two
notions, clusters and separators, that allow us to better understand the amount of work
performed by factor elimination. We then show that factor elimination can cache some
of its intermediate results and then re-use them when computing multiple marginals. It
is this ability to save intermediate results that allows factor elimination to have a better
complexity than standard variable elimination.
7.4 Separators and clusters
In this section, we deﬁne a set of variables for each edge in an elimination tree, called
a separator, and a set of variables for each node in the tree, called a cluster. These sets
will prove very helpful for the ﬁnal statement of the algorithm and for analyzing its
complexity.
Deﬁnition 7.3. The separator of edge i−j in an elimination tree is a set of variables
deﬁned as follows:
Sij
def= vars(i, j) ∩vars(j, i),
where vars(i, j) are variables that appear in factors on the i-side of edge i−j and vars(j, i)
are variables that appear in factors on the j-side of edge i−j.
■

158
INFERENCE BY FACTOR ELIMINATION
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
AB
AB
AC
C
Figure 7.4: An elimination tree with separators.
Figure 7.4 depicts an elimination tree with each edge labeled with its separator. For
example, since variables A, B appear in factors f1, f2, we have
vars(1, 4) = {A, B}.
Moreover, since variables A, B, C, D, E appear in factors f3, f4, f5, we have
vars(4, 1) = {A, B, C, D, E}.
Therefore,
S14 = S41 = {A, B} ∩{A, B, C, D, E} = {A, B}.
The importance of separators stems from the following observation regarding Line 4
of FE2:

V
φi = project(φi, Sij).
(7.1)
That is, when variables V are summed out of factor φi before it is eliminated, the resulting
factor is guaranteed to be over separator Sij (see Exercise 7.4). Consider for example
the four elimination steps with respect to Figure 7.3 and the corresponding separators in
Figure 7.4:
Eliminate f2: 
∅f2 = project(f2, AB).
Eliminate f1: 
∅f1 = project(f1, AB).
Eliminate f4: 
BD f4 = project(f4, AC).
Eliminate f5: 
E f5 = project(f5, C).
This observation has a number of implications. First, it leads to a more compact
statement of FE2, which we present later. Second, it allows us to bound the size of factors
that are constructed during the factor elimination process. Both of these implications are
best realized after we introduce the additional notion of a cluster.
Deﬁnition 7.4. The cluster of a node i in an elimination tree is a set of variables deﬁned
as follows:
Ci
def= vars(i) ∪

j
Sij.
The width of an elimination tree is the size of its largest cluster minus one.
■

7.5 A MESSAGE-PASSING FORMULATION
159
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
AB
AB
AC
C
2
1
4
3
5
AB
AB
ABCD
AC
CE
Figure 7.5: An elimination tree (left) and its corresponding clusters (right).
2
1
4
3
5
f(AB)
f(A)
f(BCD)
f(AC)
f(CE)
AB
AB
BC
C
2
1
4
3
5
BCD
AB
AB
ABC
CE
Figure 7.6: An elimination tree (left) and its corresponding clusters (right).
Figures 7.5 and 7.6 depict two elimination trees with corresponding clusters for each
node. The elimination tree in Figure 7.5 has width 3 and the one in Figure 7.6 has width
2. We come back to this notion of width later but sufﬁce it to say here that we can always
construct an elimination tree of width w given an elimination order of width w.
We close this section by making two key observations about clusters. First, when we
are about to eliminate node i on Line 2 of FE2, the variables of factor φi are exactly
the cluster of node i, Ci (see Exercise 7.5). Moreover, the factor φr on Line 6 of FE2
must be over the cluster of root r, Cr. Hence, FE2 can be used to compute the marginal
over any subset of cluster Cr. These observations allow us to rephrase FE2 as given in
Algorithm 11, FE3. The new formulation takes advantages of both separators and clusters.
7.5 A message-passing formulation
We now provide our ﬁnal reﬁnement on factor elimination where we rephrase FE3 using
a message-passing paradigm that allows us to execute the algorithm without destroying
the elimination tree in the process. This is important when computing multiple marginals
as it allows us to save intermediate computations and reuse them across different queries.
This reuse will be the key to achieving the complexity we promised previously in the
chapter. In particular, given an elimination tree of width w we will be able to compute
the marginal over every cluster in O(m exp(w)) time and space, where m is the number
of nodes in the elimination tree. As a side effect, we will be able to compute the marginal
over every network variable (and family) within the same complexity.
The message-passing formulation is based on the following observations given an
elimination tree (T , φ) with root r:
r For each node i ̸= r in the elimination tree, there is a unique neighbor of i that is closest
to root r. In Figure 7.7, tree edges are directed so that each node i points to its neighbor
closest to the root.

160
INFERENCE BY FACTOR ELIMINATION
1
4
2
3
6
9
10
7
11
5
8
Figure 7.7: An elimination tree where edges have been directed so that each node points to its neighbor
closest to the root node 7.
Algorithm 11 FE3(N, Q, (T , φ), r)
Ci is the cluster of node i in tree T
Sij is the separator of edge i−j in tree T
input:
N:
Bayesian network
Q:
some variables in network N
(T , φ):
elimination tree for the CPTs of network N
r:
node in tree T where Q ⊆Cr
output: the prior marginal Pr(Q)
main:
1: while tree T has more than one node do
2:
remove a node i ̸= r having a single neighbor j from tree T
3:
φj←φjproject(φi, Sij)
4: end while
5: return project(φr, Q)
r A node i will be eliminated from the tree only after all its neighbors, except the one closest
to the root, have been eliminated. For example, node 4 will be eliminated from Figure 7.7
only after nodes 1 and 2 are eliminated.
r When a node i is about to be eliminated, it will have a single neighbor j. Moreover, its
current factor will be projected over the separator between i and j and then multiplied
into the factor of node j. In Figure 7.7, when node 4 is eliminated its current factor is
projected onto the separator with node 7 and then multiplied into the factor of node 7.
Suppose now that we view the elimination of node i with single neighbor j as a process
of passing a message Mij from node i to neighbor j. We can then make the following
observations:
1. When j receives the message, it multiplies it into its current factor φj.
2. Node i cannot send the message to j until it has received all messages from neighbors
k ̸= j.
3. After i receives these messages, its current factor will be
φi

k̸=j
Mki

7.5 A MESSAGE-PASSING FORMULATION
161
1
4
2
3
6
9
10
7
11
5
8
Figure 7.8: Inward (pull, collect) phase: Messages passed toward the root node 7.
1
4
2
3
6
9
10
7
11
5
8
Figure 7.9: Inward (pull, collect) phase: Messages passed toward the root node 6.
and the message it sends to j will be
Mij
def= project

φi

k̸=j
Mki, Sij
	
.
(7.2)
In Figure 7.7, node 4 cannot send its message to node 7 until it receives messages from
nodes 1 and 2. Moreover, after receiving such messages, its factor will be
φ4M14M24
and the message it sends to node 7 will be
project(φ4M14M24, S47).
We can now formulate factor elimination as a message-passing algorithm. Speciﬁcally,
to compute the marginal over some variables Q, we select a root r in the elimination tree
such that Q ⊆Cr. We then push messages toward the root r. When all messages into the
root are available, we multiply them by φr and project onto Q. If our elimination tree has
m nodes, it will have m −1 edges and a total of m −1 messages will need to be passed.
Figure 7.8 depicts an elimination tree with ten messages directed toward the root node 7.
7.5.1 Multiple marginals and message reuse
Suppose now that we want to compute the marginal over some other cluster Ci, i ̸= r.
All we have to do is choose i as the new root and repeat the previous message-passing
process. This requires some additional messages to be passed but not as many as m −1
messages, assuming that we saved the messages passed when node r was the root. Consider
Figure 7.8 again and suppose that we want node 6 to be the root in this case. Out of the ten
messages we need to direct toward node 6, eight messages have already been computed
when node 7 was the root (see Figure 7.9).

162
INFERENCE BY FACTOR ELIMINATION
1
4
2
3
6
9
10
7
11
5
8
Figure 7.10: Outward (push, distribute) phase: Messages passed away from the root node 7.
The key observation here is that if we compute the marginals over every cluster by
choosing every node in the elimination tree as a root, the total number of messages we have
to pass is exactly 2(m −1). This follows because we have m −1 edges and two distinct
messages per edge. These messages are usually computed in two phases with each phase
computing m −1 messages. In the ﬁrst phase, known as the inward, pull, or collect phase,
we direct messages toward some root node r. In the second phase, known as the outward,
push, or distribute phase, we direct messages away from the root r. Figure 7.8 depicts
the messages passed in the inward phase with node 7 as the root. Figure 7.10 depicts the
messages passed in the outward phase in this case.
7.5.2 The cost of passing messages
To assess the cost of passing these messages, examine Equation 7.2 for computing a mes-
sage from i to j. According to this equation, we need to ﬁrst multiply a number of factors
and then project the result on a separator. The factor that results from the multiplication
process must be over the cluster of node i. Hence, the complexity of both multiplication
and projection are O(exp(w)), where w is the size of cluster Ci. This assumes that node
i has a bounded number of neighbors but we address this assumption later.
Recall that the width of an elimination tree is the size of its maximal cluster minus
one. Hence, if w is the width of the given elimination tree, then the cost of any message is
bounded by O(exp(w)). Since we have a total of 2(m −1) messages, the cost of computing
all cluster marginals is then O(m exp(w)). We later show that if we have a Bayesian
network with n variables and treewidth w, then there exists an elimination tree with O(n)
edges, width w, and a bounded number of neighbors for each node. Hence, the complexity
of computing all cluster marginals will be O(n exp(w)) given this tree. This is indeed
the major beneﬁt of factor elimination over variable elimination, which would require
O(n2 exp(w)) time and space to compute marginals over individual network variables.
7.5.3 Joint marginals and evidence
Before we discuss the generation of elimination trees in Section 7.6, we need to settle
the issue of handling evidence, which is necessary for computing joint marginals. In
particular, given some evidence e we want to use factor elimination to compute the joint
marginal Pr(Ci, e) for each cluster Ci in the elimination tree. As in variable elimination,
this can be done in two ways:
1. We reduce each factor f given the evidence e, leading to a set of reduced factors f e. We
then apply factor elimination to the reduced set of factors.

7.5 A MESSAGE-PASSING FORMULATION
163
2. We introduce an evidence indicator λE for every variable E in evidence e. Here λE is
a factor over variable E that captures the value of E in evidence e: λE(e) = 1 if e is
consistent with evidence e and λE(e) = 0 otherwise. We then apply factor elimination to
the extended set of factors.
The ﬁrst method is more efﬁcient if we plan to compute marginals with respect to only
one piece of evidence e. However, it is not uncommon to compute marginals with respect
to multiple pieces of evidence, e1, . . . , en, while trying to reuse messages across different
pieces of evidence. In this case, the second method is more efﬁcient if applied carefully.
This method is implemented by assigning the evidence indicator λE to a node i in the
elimination tree while ensuring that E ∈Ci. As a result, the clusters and separators of the
elimination tree will remain intact and so will its width.
Algorithm 12, FE, is our ﬁnal reﬁnement on factor elimination and uses the second
method for accommodating evidence. This version computes joint marginals using two
phases of message passing, as discussed previously. If one saves the messages across
different runs of the algorithm, then one can reuse these messages as long as they are not
invalidated when the evidence changes. In particular, when the evidence at node i changes
in the elimination tree, we need to invalidate all messages that depend on the factor at that
node. These messages happen to be the ones directed away from node i in the elimination
tree (see Figure 7.10).
7.5.4 The polytree algorithm
An interesting special case of Algorithm 12, FE, arises when the Bayesian network has
a polytree structure. In this case, one can use an elimination tree that corresponds to the
polytree structure as given in Figure 7.11. This special case of Algorithm 12 is known as
the polytree algorithm or belief propagation algorithm and is discussed in more detail in
Chapter 14. We note here that if k is the maximum number of parents attained by any node
in the polytree, then k will also be the width of the elimination tree. Hence, the time and
Algorithm 12 FE(N, (T , φ), e)
input:
N:
Bayesian network
(T , φ):
elimination tree for the CPTs of network N
e:
evidence
output: the joint marginal Pr(Ci, e) for each node i in elimination tree
main:
1: for each variable E in evidence e do
2:
i←node in tree T such that E ∈Ci
3:
λE←evidence indicator for variable E {λE(e) = 1 if e ∼e and λE(e) = 0
otherwise}
4:
φi←φiλE {entering evidence at node i}
5: end for
6: Choose a root node r in the tree T
7: Pull/collect messages towards root r using Equation 7.2
8: Push/distribute messages away from root r using Equation 7.2
9: return φi

k Mki for each node i in tree T {joint marginal Pr(Ci, e)}

164
INFERENCE BY FACTOR ELIMINATION
C
A
B
D
E
F
G
C
A
B
D
E
F
G
C
E
E
|
Θ
λ
AB
C
C
|
Θ
λ
E
G
G
|
Θ
λ
D
F
F
|
Θ
λ
C
D
D
|
Θ
λ
A
AΘ
λ
B
BΘ
λ
(a) Polytree
(b) Elimination tree
Figure 7.11: There is a one-to-one correspondence between the polytree nodes and the elimination tree
nodes. Moreover, the CPT X|U and evidence indicator λX of node X in the polytree are assigned to its
corresponding node in the elimination tree.
space complexity of the polytree algorithm is O(n exp(k)), where n is the number of nodes
in the polytree. This means that the algorithm has a linear time and space complexity since
the size of CPTs in the polytree is also O(n exp(k)). Exercises 7.13 and 7.14 reveal some
additional properties of the polytree algorithm.
7.6 The jointree connection
Constructing an elimination tree for a set of factors is straightforward: we simply construct
a tree and assign each factor to one of its nodes. Any elimination tree constructed in this
fashion will be good enough to drive algorithm FE. However, our interest is in constructing
low-width elimination trees, as this leads to minimizing the amount of work performed
by FE.
There are different methods for constructing elimination trees but the method we
discuss next will be based on an inﬂuential tool known as a jointree. It is this tool
that gives factor elimination its traditional name: the jointree algorithm. The connection
between elimination trees and jointrees is so tight that it is possible to phrase the factor
elimination algorithm directly on jointrees without explicit mention of elimination trees.
This is indeed how the algorithm is classically described and we provide such a description
in Section 7.7, where we also discuss some of the common variations on the jointree
algorithm.
We start by deﬁning jointrees.
Deﬁnition 7.5. A jointree for a DAG G is a pair (T , C) where T is a tree and C is a
function that maps each node i in tree T into a label Ci, called a cluster. The jointree
must satisfy the following properties:
1. The cluster Ci is a set of nodes from the DAG G.
2. Each family in the DAG G must appear in some cluster Ci.
3. If a node appears in two clusters Ci and Cj, it must also appear in every cluster Ck
on the path connecting nodes i and j in the jointree. This is known as the jointree
property.
The separator of edge i−j in a jointree is denoted by Sij and deﬁned as Ci ∩Cj. The
width of a jointree is deﬁned as the size of its largest cluster minus one.
■

7.6 THE JOINTREE CONNECTION
165
A
B
C
D
E
F
G
H
1
2
3
5
4
6
DFG
ADF
ABD
ACE
AEF
EFH
DF
AE
AD
EF
AF
1
2
3
5
4
6
DFGH
ADFH
ABD
ACE
AEFH
EFH
DFH
AE
AD
EFH
AFH
(a) DAG
(b) Minimal jointree
(c) Nonminimal jointree
Figure 7.12: Two jointrees for the same DAG.
Figure 7.12(a) depicts a DAG and Figure 7.12(b) depicts a corresponding jointree that
has six nodes: 1, . . . , 6. For this jointree, the cluster of node 1 is C1 = DFG, the separator
of edge 1−3 is S13 = DF, and the width is 2. A jointree for DAG G is said to be minimal
if it ceases to be a jointree for G once we remove a variable from one of its clusters. The
jointree in Figure 7.12(b) is minimal. The one in Figure 7.12(c) is not minimal as we can
remove variable H from clusters 1, 3 and 4.
Jointrees are studied in more depth in Chapter 9 but we point out here that the treewidth
of a DAG can be deﬁned as the width of its best jointree (i.e., the one with the smallest
width). Recall that we had a similar deﬁnition in Chapter 6 for elimination orders, where
the treewidth of a DAG was deﬁned as the width of its best elimination order. Therefore,
the best elimination order and best jointree for a given DAG must have equal widths.
Polytime, width-preserving transformations between elimination orders and jointrees are
provided in Chapter 9.
We next provide two results that show the tight connection between jointrees and
elimination trees. According to the ﬁrst result, every elimination tree induces a jointree
of equal width.
Theorem 7.1. The clusters of an elimination tree satisfy the three properties of a jointree
stated in Deﬁnition 7.5.
■
Figure 7.13 depicts two elimination trees that have the same set of clusters, shown in
Figure 7.12(b). We can verify that these clusters satisfy the three properties of a jointree.
According to our second result, a jointree can easily be used to construct an elimination
tree of no greater width.
Deﬁnition 7.6. Let (T , C) be a jointree for network N. An elimination tree (T , φ) for
the CPTs of network N is said to be embedded in the jointree if the variables of factor
φi are contained in the jointree cluster Ci.
■
Hence, to construct an elimination tree that is embedded in a jointree (T , C), all we need
to do is adopt the tree structure T of the jointree and then assign each CPT to some node
i in tree T while ensuring that the CPT variables are contained in cluster Ci. Figure 7.13
depicts two elimination trees that are embedded in the jointree of Figure 7.12(b) and also
in the jointree of Figure 7.12(c).

166
INFERENCE BY FACTOR ELIMINATION
1
2
3
5
4
6
DF
G|
Θ
EF
H|
Θ
A
F
A
|
Θ
Θ
AC
E
A
C
|
| Θ
Θ
AB
D
A
B
|
| Θ
Θ
1
2
3
5
4
6
DF
G|
Θ
EF
H|
Θ
A
F|
Θ
AC
E
A
C
|
| Θ
Θ
AB
D
A
B
|
| Θ
Θ
A
Θ
Figure 7.13: Two elimination trees that have identical clusters, as shown in Figure 7.12(b).
Theorem 7.2. Let Ci and Sij be the clusters and separators of a jointree and let C′
i and
S′
ij be the clusters and separators of an embedded elimination tree. Then C′
i ⊆Ci and
S′
ij ⊆Sij. Moreover, the equalities hold when the jointree is minimal.
■
Hence, the width of an elimination tree is no greater than the width of an embedding
jointree. The elimination trees in Figure 7.13 both have width 2. These trees are embedded
in the jointree of Figure 7.12(b), which has width 2. They are also embedded in the jointree
of Figure 7.12(c), which has width 3.
Given these results, we can immediately generate low-width elimination trees if we
have an ability to generate low-width jointrees (as discussed in Chapter 9). However, note
here that the classical description of the jointree algorithm does not refer to elimination
trees. Instead, it immediately starts with the construction of a jointree and then assigns
CPTs to the jointree clusters, as suggested by Deﬁnition 7.6. This is also the case for
the classical semantics of the jointree algorithm and its classical proof of correctness,
which do not make reference to elimination trees either and are therefore not based on the
concept of factor elimination as shown here.
7.7 The jointree algorithm: A classical view
In this section, we provide a more classical exposition of the jointree algorithm where we
discuss two of its most common variations known as the Shenoy-Shafer and Hugin archi-
tectures. The key difference between these architectures is due to the type of information
they store and the way they compute messages, which leads to important implications on
time and space complexity. We see later that factor elimination as derived in this chapter
corresponds to the Shenoy-Shafer architecture and, hence, inherits its properties. The
classical description of a jointree algorithm is as follows:
1. Construct a jointree (T , C) for the given Bayesian network. Figure 7.14 depicts a Bayesian
network and a corresponding jointree.
2. Assign each network CPT X|U to a cluster that contains X and U. Figure 7.14(b) depicts
factor assignments to jointree clusters.
3. Assign each evidence indicator λX to a cluster that contains X. Figure 7.14(b) depicts
evidence indicator assignments to jointree clusters.

7.7 THE JOINTREE ALGORITHM: A CLASSICAL VIEW
167
A
B
C
D
E
F
G
H
DFG
ACE
ADF
ABD
AEF
EFH
DF
G
G
|
Θ
λ
EF
H
H
|
Θ
λ
A
F
A
|
Θ
Θ
AC
E
A
C
E
C
|
| Θ
Θ
λ
λ
AB
D
A
B
D
B
|
| Θ
Θ
λ
λ
F
Aλ
λ
(a) Bayesian network
(b) Corresponding jointree
Figure 7.14: CPTs and evidence indicators are assigned to jointree clusters.
Note that Step 2 corresponds to the generation of an elimination tree as suggested by
Deﬁnition 7.6, although the elimination tree is left implicit here.
A jointree algorithm starts by entering the given evidence e through evidence indicators.
That is, λX(x) is set to 1 if x is consistent with evidence e and to 0 otherwise. The algorithm
then propagates messages between clusters. After passing two messages per edge in the
jointree, we can compute the marginals Pr(C, e) for every cluster C.
There are two main methods for propagating messages in a jointree, known as the
Shenoy-Shafer architecture and the Hugin architecture. The methods differ in both their
space and time complexity. In particular, the Shenoy-Shafer architecture would generally
require less space but more time on an arbitrary jointree. However, the time complexity of
both methods can be made equivalent if we restrict ourselves to a special type of jointree,
as we see next.
7.7.1 The Shenoy-Shafer architecture
Shenoy-Shafer propagation corresponds to Algorithm 12, FE, and proceeds as follows.
First, evidence e is entered into the jointree through evidence indicators. A cluster is then
selected as the root and message propagation proceeds in two phases, inward and outward.
In the inward phase, messages are passed toward the root. In the outward phase, messages
are passed away from the root. The inward phase is also known as the collect or pull
phase, and the outward phase is known as the distribute or push phase. Node i sends a
message to node j only when it has received messages from all its other neighbors k. A
message from node i to node j is a factor Mij deﬁned as follows:
Mij
def= project
⎛
⎝i

k̸=j
Mki, Sij
⎞
⎠,
(7.3)
where i is the product of factors (including evidence indicators) assigned to node i.
Note that cluster Ci includes exactly the variables of factor i

k̸=j Mki. Hence,
projecting this factor on variables Sij is the same as summing out variables Ci \ Sij from

168
INFERENCE BY FACTOR ELIMINATION
AB
A
0
0
Initialization:
After inward pass:
After outward pass:
AB
A
AB
A
root
a
a
A
Aθ
λ
A
B
B
|
θ
λ
.2
.7
a
a
.2
.7
a
a
0
0
a
a
0
0
a
a
.6
.4
a
a
Figure 7.15: Shenoy-Shafer propagation illustrated on a simple jointree under evidence b. The factors on
the left represent the content of the message directed toward the root. The factors on the right represent
the content of the message directed away from the root. The jointree is for network A →B, where θa = .6,
θb|a = .2 and θb|¯a = .7.
this factor. This is why (7.3) is more commonly written as follows in the literature:
Mij
def=

Ci\Sij
i

k̸=j
Mki.
(7.4)
Once message propagation is ﬁnished in the Shenoy-Shafer architecture, we have the
following for each cluster i in the jointree:
Pr(Ci, e) = i

k
Mki.
(7.5)
Hence, we can compute the joint marginal for any subset of variables that is included in
a cluster (see also Exercise 7.6). Figure 7.15 illustrates Shenoy-Shafer propagation on a
simple example.
7.7.2 Using nonminimal jointrees
The Shenoy-Shafer architecture corresponds to Algorithm 12, FE, when the jointree is
minimal as the clusters and separators of the jointree match the ones induced by the
embedded elimination tree. However, consider the jointree in Figure 7.12(c). This jointree
is not minimal as we can remove variable H from clusters 1, 3, and 4, leading to the
minimal jointree in Figure 7.12(b). Suppose now that we assign CPTs to these jointrees
as given on the left side of Figure 7.13. If we apply the Shenoy-Shafer architecture to the
nonminimal jointree with this CPT assignment, the factor 1 will be over variables DFH,
the separator S13 = DFH, and the cluster C1 = DFGH. Hence, when computing the
message from node 1 to node 3 using (7.3), we are projecting factor 1 on variables DFH
even though the factor does not contain variable H. Similarly, if we compute the message
using (7.4), we are summing out variable H from factor 1 even though it does not
appear in that factor. We can simply ignore these superﬂuous variables when projecting
or summing out, which leads to the same messages passed by the minimal jointree in
Figure 7.12(b).

7.7 THE JOINTREE ALGORITHM: A CLASSICAL VIEW
169
However, these variables are not superﬂuous when computing other messages. Consider
for example the message sent from node 6 to node 4. The factor 6 is over variables FGH
in this case. In the minimal jointree, variable H is summed out from this factor before
it is sent to node 4. However, in the nonminimal jointree the factor 6 is sent intact to
node 4. Hence, the nonminimal jointree insists on carrying the information about variable
H as messages travel from node 6 to node 4, then to node 3, and ﬁnally to node 1. As a
result, the nonminimal jointree ends up computing marginals over variable sets AEFH,
ADFH, and DFGH, which cannot be computed if we use the minimal jointree. This is
indeed one of the main values of using nonminimal jointrees as they allow us to compute
marginals over sets of variables that may not be contained in the clusters of minimal
jointrees.
7.7.3 Complexity of the Shenoy-Shafer architecture
Let us now look at the time and space requirements of the Shenoy-Shafer architecture,
where we provide a more reﬁned analysis than the one given in Section 7.5.1. In particular,
we relax here the assumption that each node in the jointree has a bounded number of
neighbors. The space requirements are those needed to store the messages computed by
(7.4). That is, we need two factors for each separator Sij, one factor stores the message
from cluster i to cluster j and the other stores the message from j to i. It needs to be
stressed here that (7.4) can be evaluated without the need to construct a factor over all
cluster variables (see Exercise 7.11). Hence, the space complexity of the Shenoy-Shafer
architecture is not exponential in the size of jointree clusters but only in the size of
jointree separators. This is a key difference with the Hugin architecture, to be discussed
later.
Moving to the time requirements of the Shenoy-Shafer architecture, suppose that we
have a jointree with n clusters and width w and let ni be the number of neighbors that
cluster i has in the jointree. For each cluster i, (7.4) has to be evaluated ni times and
(7.5) has to be evaluated once. Each evaluation of (7.4) leads to multiplying ni factors,
whose variables are all in cluster Ci. Moreover, each evaluation of (7.5) leads to multi-
plying ni + 1 factors, whose variables are also all in cluster Ci. The total complexity is
then

i
O(n2
i exp(|Ci|) + (ni + 1) exp(|Ci|)),
which reduces to

i
O((n2
i + ni + 1) exp(w))
where w is the jointree width. Since 
i ni = 2(n −1), this further reduces to
O((α + 3n −2) exp(w))
where α = 
i n2
i is a term that ranges from O(n) to O(n2) depending on the jointree
structure. For example, we may have what is known as a binary jointree in which each
cluster has at most three neighbors, leading to α = O(n). Or we may have a jointree with
one cluster having the other n −1 clusters as its neighbors, leading to α = O(n2).

170
INFERENCE BY FACTOR ELIMINATION
x
1
2
3
m
4
x3
x4
xm
x1
x2
Figure 7.16: An illustration of the division technique in the Hugin architecture.
We see in Chapter 9 that if we are given a Bayesian network with n variables and
an elimination order of width w, then we can always construct a binary jointree for the
network with the following properties:
r The jointree width is w.
r The jointree has no more than 2n −1 clusters.
Hence, we can always avoid the quadratic complexity suggested previously by a careful
construction of the jointree.
To summarize, the space complexity of the Shenoy-Shafer architecture is exponential
in the size of separators but not exponential in the size of clusters. Moreover, its time
complexity depends not only on the width and number of clusters in a jointree but also on
the number of neighbors per node in the jointree. However, with an appropriate jointree
the time complexity is O(n exp(w)), where n is the number of networks variables and w
is the network treewidth.
7.7.4 The Hugin architecture
We now discuss another variation on the jointree algorithm known as the Hugin architec-
ture. This architecture has a space complexity that is exponential in the size of clusters.
Yet its time complexity depends only on the width and number of clusters in the jointree,
not the number of neighbors per cluster.
The Hugin architecture uses a new operation that divides one factor by another. We
therefore motivate and deﬁne this operation before we introduce and analyze this architec-
ture. Consider ﬁrst Figure 7.16, which depicts a node x with neighbors 1, . . . , m, where
each edge between x and its neighbor i is labeled with a number xi (x is also a number).
Suppose now that node x wants to send a message to each of its neighbors i, where the
content of this message is the number:
x

j̸=i
xj.
There are two ways to do this. First, we can compute the product for each neighbor i,
which corresponds to the Shenoy-Shafer architecture. Second, we can compute the product
p = x m
j=1 xj only once and then use it to compute the message to each neighbor i as
p/xi. The second method corresponds to the Hugin architecture and is clearly more
efﬁcient as it only requires one division for each message (after some initialization), while

7.7 THE JOINTREE ALGORITHM: A CLASSICAL VIEW
171
the ﬁrst method requires m multiplications per message. However, the second method
requires that xi ̸= 0, otherwise p/xi is not deﬁned. But if the message p/xi is later
multiplied by an expression of the form xiα, then we can deﬁne p/0 to be 0, or any
other number for that matter, and our computations will be correct since (p/xi)xiα = 0
regardless of how p/0 is deﬁned in this case. This is basically the main insight behind the
Hugin architecture, except that the prior analysis is applied to factors instead of numbers.
Deﬁnition 7.7. Let f1 and f2 be two factors over the same set of variables X. The
division of factor f1 by factor f2 is a factor f over variables X denoted by f1/f2 and
deﬁned as follows:
f (x)
def=
 f1(x)/f2(x)
if f2(x) ̸= 0
0
otherwise.
■
Hugin propagation proceeds similarly to Shenoy-Shafer by entering evidence e using
evidence indicators, selecting a cluster as root, and propagating messages in two phases,
inward and outward. However, the Hugin message propagation scheme differs in some
major ways. First, it maintains over each separator Sij a single factor ij, with each entry
of ij initialized to 1. It also maintains over each cluster Ci a factor i, which is initialized
to i

j ij where i is the product of factors (including evidence indicators) assigned
to node i.
Node i passes a message to neighboring node j only when i receives messages from
all its other neighbors k. When node i is ready to send a message to node j, it does the
following:
r Saves the factor ij into old
ij .
r Computes a new factor ij←
Ci\Sij i.
r Computes a message to node j: Mij = ij/old
ij .
r Multiplies the computed message into the factor at node j: j←jMij.
It is important to stress here that the factor saved with the edge i−j is ij, which is
different from the message sent from node i to node j, ij/old
ij . The message is not
saved, which is contrary to the Shenoy-Shafer architecture.
After the inward and outward passes of Hugin propagation are completed, we have the
following for each node i in the jointree:
Pr(Ci, e) = i.
Hence, we can compute the joint marginal for any set of variables as long as that set is
included in a cluster. The Hugin propagation scheme also guarantees the following for
each edge i−j:
Pr(Sij, e) = ij.
That is, the separator factors contain joint marginals over the variables of these separators
(see Exercises 7.6). Figure 7.17 illustrates Hugin propagation on a simple example.
7.7.5 Complexity of the Hugin architecture
The space requirements for the Hugin architecture are those needed to store cluster and
separator factors: one factor for each cluster and one factor for each separator. Note that
cluster factors are usually much larger than separator factors, leading to a much larger

172
INFERENCE BY FACTOR ELIMINATION
AB
A
1
1
.20
0
.70
0
Initialization:
After inward pass:
After outward pass:
AB
A
.20
.70
.20
0
.70
0
.12
.28
AB
A
.12
.28
.12
0
.28
0
.12
.28
.60
.40
root
a
a
a
a
a
a
a
a
a
a
a
a
ab
b
a
b
a
b
a
ab
b
a
b
a
b
a
ab
b
a
b
a
b
a
A
Aθ
λ
A
B
B
|
θ
λ
Figure 7.17: Hugin propagation illustrated on a simple jointree under evidence b, where one factor is
associated with each cluster and with each separator. The jointree is for network A →B, where θa = .6,
θb|a = .2 and θb|¯a = .7.
demand on space than that required by the Shenoy-Shafer architecture (recall that the
space complexity of the Shenoy-Shafer architecture is exponential only in the size of
separators). This is the penalty that the Hugin architecture pays for the time complexity,
as we discuss next.
Suppose that we have a jointree with n clusters and width w. Suppose further that
the initial factors i and ij are already available for each cluster and separator. Let
us now bound the amount of work performed by the inward and outward passes of the
Hugin architecture, that is, the work needed to pass a message from each cluster i to
each of its neighbors j. Saving the old separator factor takes O(exp(|Sij|)), computing
the message takes O(exp(|Ci|) + exp(|Sij|)), and multiplying the message into the factor
of cluster j takes O(exp(|Cj|)). Hence, if each cluster i has ni neighbors j, the total
complexity is

i

j
O(exp(|Ci|) + 2 exp(|Sij|) + exp(|Cj|)),
which reduces to
O(n exp(w))
where w is the jointree width. Note that this result holds regardless of the number of
neighbors per node in a jointree, contrary to the Shenoy-Shafer architecture, which needs
to constrain the number of neighbors to achieve a similar complexity.
Bibliographic remarks
The Shenoy-Shafer architecture was introduced in Shenoy and Shafer [1990] and the
Hugin architecture was introduced in Jensen et al. [1990]. Both architectures are based
on Lauritzen and Spiegelhalter [1988], which introduced the ﬁrst jointree algorithm. All
three architectures are discussed and compared in Lepar and Shenoy [1998]. Another
architecture, called zero-conscious Hugin, is proposed in Park and Darwiche [2003c],

7.8 EXERCISES
173
which combines some of the beneﬁts attained by the Shenoy-Shafer and Hugin archi-
tectures. The notion of a binary jointree was introduced in Shenoy [1996]. A procedural
description of the Hugin architecture is given in Huang and Darwiche [1996], laying
out some of the techniques used in developing efﬁcient implementations. The jointree
algorithm is quite versatile in allowing other types of queries, including MAP and MPE
[Jensen, 2001; Cowell et al., 1999], and a framework for time-space tradeoffs [Dechter
and Fattah, 2001]. Jointrees have been referred to as junction trees in Jensen et al. [1990],
clique trees in Lauritzen and Spiegelhalter [1988], qualitative Markov trees in Shafer
[1987], and hypertrees in Shenoy and Shafer [1990]. However, we should mention that
many deﬁnitions of jointrees in the literature are based on speciﬁc procedures that con-
struct only a subset of the jointrees as given by Deﬁnition 7.5. We also note that the
polytree algorithm as discussed in Section 7.5.4 precedes the jointree algorithm and was
derived independently. The formulation we gave corresponds to the one proposed in Peot
and Shachter [1991], which is a slight modiﬁcation on the original polytree algorithm
described in Pearl [1986b].
7.8 Exercises
7.1. Answer the following queries with respect to the Bayesian network in Figure 7.18:
1. Pr(B, C).
2. Pr(C, D =true).
3. Pr(A|D =true, E =true).
You may prune the network before attempting each computation and use any inference
method you ﬁnd most appropriate.
C
A
B
D
E
F
G
A
fA
true
.6
false
.4
B
fB
true
.5
false
.5
A
B
C
fC
true
true
true
.9
true
true
false
.1
true
false
true
.1
true
false
false
.9
false
true
true
.5
false
true
false
.5
false
false
true
.3
false
false
false
.7
C
D
fD
true
true
.2
true
false
.8
false
true
.7
false
false
.3
C
E
fE
true
true
.1
true
false
.9
false
true
.2
false
false
.8
D
F
fF
true
true
.7
true
false
.3
false
true
.6
false
false
.4
E
G
fG
true
true
.2
true
false
.8
false
true
.8
false
false
.2
Figure 7.18: A Bayesian network.

174
INFERENCE BY FACTOR ELIMINATION
B
C
D
E
F
G
A
H
Figure 7.19: A Bayesian network structure.
3
1
2
4
5
6
7
fA
fB
fC
fD
fE
fF
fG
Figure 7.20: An elimination tree for the factors of Bayesian network in Figure 7.18.
7.2. Consider the Bayesian network in Figure 7.19. Construct an elimination tree for the Bayesian
network CPTs that has the smallest width possible and assigns at most one CPT to each tree
node. Compute the separators, clusters, and width of the elimination tree. It may be useful
to know that this network has the following jointree: ABC—BCE—BDE—DEF —EFG—
FGH.
7.3. Consider the Bayesian network in Figure 7.18 and the corresponding elimination tree in
Figure 7.20, and suppose that the evidence indicator for each variable is assigned to the node
corresponding to that variable (e.g., λC is assigned to node 3). Suppose we are answering
the following queries according to the given order:
Pr(G=true), Pr(G, F =true), Pr(F, A=true, F =true),
using Algorithm 12, FE.
1. Compute the separators, clusters, and width of the given elimination tree.
2. What messages are computed while answering each query according to the previous se-
quence? State the origin, destination, and value of each message. Use node 7 as the root to
answer the ﬁrst two queries, and node 6 as the root to answer the last query. For each query,
compute only the messages directed toward the corresponding root.

7.8 EXERCISES
175
3. What messages are invalidated due to new evidence as we attempt each new query?
4. What is the answer to each of the pervious queries?
7.4. Prove Equation 7.1. Hint: Prove by induction, assuming ﬁrst that node i has only a single
neighbor and then prove for an arbitrary node i.
7.5. Let i be a node in an elimination tree. Show that for any particular neighbor k of i, we have
Sik ⊆vars(i) ∪

j̸=k
Sij
and hence
Ci = vars(i) ∪

j̸=k
Sij.
7.6. Prove the following statement with respect to Algorithm 12, FE:
Mij = project(φij, Sij),
where φij is the product of all factors assigned to nodes on the i-side of edge i−j in the
elimination tree. Use this result to show that
MijMji = Pr(Sij, e)
for every edge i−j in the elimination tree.
7.7. Prove that for every edge i−j in an elimination tree, Sij = Ci ∩Cj.
7.8. Let N be a Bayesian network and let (T , φ) be an elimination tree of width w for the CPTs
of network N. Show how to construct an elimination order for network N of width ≤w. Hint:
Consider the order in which variables are eliminated in the context of factor elimination.
7.9. Consider the following set of factors: f (ABE), f (ACD), and f (DEF). Show that the op-
timal elimination tree (the one with smallest width) for this set of factors must have more
than three nodes (hence, the set of nodes in the elimination tree cannot be in one-to-one
correspondence with the factors).
7.10. A cutset C for a DAG G is a set of nodes that renders G a polytree when all edges outgoing
from nodes C are removed. Let k be the maximum number of parents per node in the DAG
G. Show how to construct an elimination tree for G whose width is ≤k + |C|.
7.11. Show that the message deﬁned by Equation 7.4 can be computed in space that is exponential
only in the size of separator Sij, given messages Mki, k ̸= j, and the factors assigned to
node i.
7.12. Consider the elimination tree on the left of Figure 7.13. Using factor elimination on this tree,
one cannot compute the marginal over AEFH as these variables are not contained in any
cluster. Construct an elimination tree with width 3 that allows us to compute the marginal over
these variables. Construct another elimination tree with width 3 that allows us to compute the
marginal over variables GH. Note: You may need to introduce auxiliary factors.
7.13. Consider the polytree algorithm as discussed in Section 7.5.4, and let X →Y be an edge
in the polytree. Show that the messages MXY and MYX sent across this edge must be over
variable X.
7.14. Consider the polytree algorithm as discussed in Section 7.5.4, and let e be some given
evidence. Let X →Y be an edge in the polytree, e+
XY be the evidence assigned to nodes
on the X-side of this edge, and e−
XY be the evidence assigned to nodes on the Y -side of the
edge (hence, e = e+
XY , e−
XY ). Show that the messages passed by Algorithm 12, FE, across
edges X →Y have the following meaning:
MXY = Pr(X, e+
XY )

176
INFERENCE BY FACTOR ELIMINATION
and
MYX = Pr(e−
XY |X).
Moreover, show that MXY MYX = Pr(X, e).
7.15. Deﬁnition 7.7 showed how we can divide two factors f1 and f2 when each is over the same
set of variables X. Let us deﬁne division more generally while assuming that factor f2 is over
variables Y ⊆X. The result is a factor f over variables X deﬁned as follows:
f (x)
def=
!
f1(x)/f2(y)
if f2(y) ̸= 0 for y ∼x
0
otherwise.
Show that

X\Y
f1/f2 =
⎛
⎝
X\Y
f1
⎞
⎠/f2.
Use this fact to provide a different deﬁnition for the messages passed by the Hugin architec-
ture.
7.9 Proofs
PROOF OF THEOREM 7.1. The ﬁrst two properties of a jointree are immediately sat-
isﬁed by the deﬁnition of clusters for an elimination tree. To show the third property,
suppose that some variable X belongs to clusters Ci and Cj of an elimination tree, and
let i−k . . . l−j be the the path between i and j (we may have k = l, or k = j and i = l).
By the result in Exercise 7.5, X ∈Ci implies that X ∈vars(i) or X ∈Sio for some o ̸= k.
Similarly, X ∈Cj implies that X ∈vars(j) or X ∈Sjo for some o ̸= l. This means that
X must appear in some factor on the i-side of edge i−k and X must appear in some factor
on the j-side of edge l−j. Hence, X must belong to the separator of every edge on the
path between i and j. Moreover, X must belong to every cluster on the path between i
and j.
■
PROOF OF THEOREM 7.2. By deﬁnition of an elimination tree separator, every variable
X ∈S′
ij must appear in some factor assigned to a node on the i-side of edge i−j and also
in some factor assigned to a node on the j-side of the edge. By the assumptions of this
theorem, variable X must then appear in some jointree cluster on the i-side of edge i−j
and some jointree cluster on the j-side of edge i−j. By the jointree property, variable
X must also appear in clusters Ci and Cj. This means that X must appear in separator
Sij = Ci ∩Cj and, hence, S′
ij ⊆Sij. This leads to C′
i ⊆Ci since C′
i is the union of
separators S′
ij and the variables of factor φi (which must be contained in Ci by the
assumptions of this theorem).
Suppose now that C′
i ̸= Ci for some node i; then X ∈Ci and X ̸∈C′
i for some variable
X. We now show that the jointree cannot be minimal. Since X ̸∈C′
i, then X ̸∈vars(i) and
X ̸∈S′
ij for all neighbors j of i. Hence, X can appear only in factors that are assigned to
a node on the j-side of edge i−j for a single neighbor j of i. Consider now the connected
subtree of jointree clusters that contain variable X.3 Consider now a leaf cluster Ck in this
subtree that lies on the i-side of edge i−j. Then X can be removed from cluster Ck without
destroying any of the jointree properties of Deﬁnition 7.5. The ﬁrst property cannot be
destroyed by removing variables from clusters. The third property cannot be destroyed
3 The clusters of every variable must form a connected subtree by the jointree property.

7.9 PROOFS
177
by removing a variable from a leaf cluster. The only thing we need to consider now is
the second property, that the variables of every network CPT must be contained in some
jointree cluster. For this, we note that no CPT that mentions variable X is assigned to node
k; otherwise, X ∈S′
ij and X ∈C′
i, which we know is not the case. Hence, by removing
X from cluster Ck, the variables of every network CPT will still be contained in some
jointree cluster.
■

8
Inference by Conditioning
We discuss in this chapter a class of inference algorithms that are based on the concept of
conditioning, also known as case analysis. Conditioning algorithms are marked by their
ﬂexible space requirements, allowing a relatively smooth tradeoff between time and space
resources.
8.1 Introduction
Reasoning by cases or assumptions is a common form of human reasoning, which is
also quite dominant in mathematical proofs. According to this form of reasoning, one
can simplify a problem by considering a number of cases where each corresponds to a
particular assumption. We then solve each of the cases under its corresponding assumption
and combine the results to obtain a solution to the original problem. In probabilistic
reasoning, this is best illustrated by the identity,
Pr(x) =

c
Pr(x, c).
(8.1)
Here we are computing the probability of instantiation x by considering a number of cases
c, computing the probability of x with each case c, and then adding up the results to get
the probability of x.
In general, solving a problem can always be made easier if we make the correct
assumptions. For example, we saw in Chapter 6 how evidence can be used to prune
network edges, possibly reducing the network treewidth and making it more amenable to
inference algorithms. If the given evidence does not lead to enough edge pruning, one can
always use case analysis to assume more evidence that leads to the necessary pruning.
We present two fundamental applications of this principle in this chapter, one leading to
the algorithm of cutset conditioning in Section 8.2 and the other leading to the algorithm
of recursive conditioning in Section 8.3. The main property of conditioning algorithms
is their ﬂexible space requirements, which can be controlled to allow a smooth tradeoff
between space and time resources. This is discussed in Section 8.4 using the algorithm
of recursive conditioning. We then turn in Section 8.5 to show how this algorithm can
be used to answer multiple queries as we did using the jointree algorithm of Chapter 7.
We ﬁnally address in Section 8.6 the problem of optimizing the use of available space
resources in order to minimize the running time of probabilistic inference.
8.2 Cutset conditioning
Consider the Bayesian network in Figure 8.1 and suppose that we want to answer the query
Pr(E, D =true, B =true). As we know from Chapter 6, we can prune the edge outgoing
from node B in this case, leading to the network in Figure 8.2(a). This network is a polytree
178

8.2 CUTSET CONDITIONING
179
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
A
A
true
.6
false
.4
A
B
B|A
true
true
.2
true
false
.8
false
true
.75
false
false
.25
A
C
C|A
true
true
.8
true
false
.2
false
true
.1
false
false
.9
B
C
D
D|BC
true
true
true
.95
true
true
false
.05
true
false
true
.9
true
false
false
.1
false
true
true
.8
false
true
false
.2
false
false
true
0
false
false
false
1
C
E
E|C
true
true
.7
true
false
.3
false
true
0
false
false
1
Figure 8.1: A Bayesian network.
and is equivalent to the one in Figure 8.1 as far as computing Pr(E, D =true, B =true).
Hence, we can now answer our query using the polytree algorithm from Chapter 7, which
has a linear time and space complexity.
The ability to use the polytree algorithm in this case is due to the speciﬁc evidence
we have. For example, we cannot use this algorithm to answer the query Pr(E, D =true)
since the evidence does not permit the necessary edge pruning. However, if we per-
form case analysis on variable B we get two queries, Pr(E, D =true, B =true) and
Pr(E, D =true, B =false), each of which can be answered using the polytree algorithm.
Figure 8.2 depicts the two polytrees N1 and N2 corresponding to these queries, re-
spectively. The ﬁrst query with respect to polytree N1 leads to the following joint
marginal:
E
Pr(E, D =true, B =true)
true
.08379
false
.30051
and the second query with respect to polytree N2 leads to
E
Pr(E, D =true, B =false)
true
.22064
false
.09456

180
INFERENCE BY CONDITIONING
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
C
D
′
D|C = 
B D|BCB=true
true
true
.95
true
false
.05
false
true
.9
false
false
.1
C
D
′
D|C = 
B D|BCB=false
true
true
.8
true
false
.2
false
true
0
false
false
1
(a) B =true
(b) B =false
Figure 8.2: Two polytrees which result from setting the value of variable B.
Adding up the corresponding entries in these factors, we get
E
Pr(E, D =true)
true
.30443
false
.39507
which is the joint marginal for our original query.
We can always use this technique to reduce a query with respect to an arbitrary network
into a number of queries that can be answered using the polytree algorithm. In general, we
may need to perform case analysis on more than one variable, as shown by the following
deﬁnition.
Deﬁnition 8.1. A set of nodes C is a loop-cutset for a Bayesian network N if removing
the edges outgoing from nodes C will render the network a polytree.
■
Every instantiation c of the loop-cutset allows us to reduce the network N to a polytree
Nc. We can then compute the marginal over every network variable X as follows:
Pr(X, e) =

c
Prc(X, e, c),
where Prc is the distribution induced by the polytree Nc. This method of inference is
known as cutset conditioning and is one of the ﬁrst methods developed for inference with
Bayesian networks.
Figure 8.3 depicts a few networks and some examples of loop-cutsets. Given a Bayesian
network N with n nodes and a corresponding loop-cutset C of size s, the method of cutset
conditioning requires O(exp(s)) invocations to the polytree algorithm. Moreover, each
of these invocations takes O(n exp(k)) time, where k is the maximum number of parents
per node in the polytree (k is also the treewidth of resulting polytree). Therefore, cutset
conditioning takes O(n exp(k + s)) time, which is exponential in the size of used cutset.
Computing a loop-cutset of minimal size is therefore an important task in the context of
cutset conditioning, but such a computation is known to be NP-hard.

8.3 RECURSIVE CONDITIONING
181
Figure 8.3: Networks and corresponding loop-cutsets (bold circles).
1
2
3
n
Figure 8.4: A Bayesian network with a loop-cutset that grows linearly with the network size.
The main advantage of cutset conditioning is its modest space requirements. In partic-
ular, the algorithm only needs to store the accumulated sum of joint marginals computed
by the different calls to the polytree algorithm. Therefore, the space complexity of cutset
conditioning is only O(n exp(k)), which is also the space complexity of the polytree al-
gorithm. This is quite important as the space complexity of elimination algorithms that
we discussed in Chapters 6 and 7 are exponential in the network treewidth, which can be
much larger than k.
8.3 Recursive conditioning
The main problem with cutset conditioning is that a large loop-cutset will lead to a blow
up in the number of cases that it has to consider. In Figure 8.4, for example, the depicted
loop-cutset contains n variables, leading cutset conditioning to consider 2n cases (when
all variables are binary). However, it is worth mentioning that elimination methods can
solve this network in linear time as it has a bounded treewidth.
We now discuss another conditioning method that exploits assumptions differently
than cutset conditioning. Speciﬁcally, instead of using assumptions to generate a polytree
network, we use such assumptions to decompose the network. By decomposition, we
mean the process of splitting the network into smaller, disconnected pieces that can
be solved independently. Figure 8.5 shows how we can decompose network N into two
subnetworks by performing a case analysis on variable B. That is, if variable B is included
in the evidence, then pruning the network will lead to removing the edges outgoing from
variable B and then decomposing the network. Figure 8.5 also shows how we can further
decompose one of the subnetworks by performing a case analysis on variable C. Note that
one of the resulting subnetworks contains a single node and cannot be decomposed further.
We can always use this recursive decomposition process to reduce a query with respect
to some network N into a number of queries with respect to single-node networks.

182
INFERENCE BY CONDITIONING
A
B
C
D
E
A
B
C
D
E
C
D
E
B
C
Figure 8.5: Decomposing a Bayesian network by performing a case analysis on variable B and then on
variable C.
Speciﬁcally, let C be a set of variables such that pruning the network N given query Pr(e, c)
leads to decomposing it into subnetworks Nc
l and Nc
r with corresponding distributions
Prcl and Prcr. We then have
Pr(e) =

c
Pr(e, c) =

c
Prc
l(el, cl)Prc
r(er, cr),
(8.2)
where el/cl and er/cr are the subsets of instantiation e/c pertaining to subnetworks Nc
l
and Nc
r, respectively. The variable set C will be called a cutset for network N in this
case, to be contrasted with a loop-cutset. Note that each of the networks Nc
l and Nc
r can
be decomposed using the same method recursively until we reach queries with respect to
single-node networks.
This is a universal process that can be used to compute the probability of any instan-
tiation. Yet there are many ways in which we can decompose a Bayesian network into
disconnected subnetworks. The question then is which decomposition should we use? As
it turns out, any decomposition will be valid but some decompositions will lead to less
work than others. The key is therefore to choose decompositions that will minimize the
amount of work done and to bound it in some meaningful way. We address this issue later
but we ﬁrst provide a formal tool for capturing a certain decomposition policy, which is
the subject of the following section.
Before we conclude this section, we highlight three key differences between cutset
conditioning and recursive conditioning. First, the role of a cutset is different. In cutset
conditioning, it is used to generate a polytree network; in recursive conditioning, it is
used to decompose a network into disconnected subnetworks. In Figure 8.1, for example,
variable B constitutes a valid loop-cutset since it would render the network a polytree
when instantiated. However, instantiating variable B will not decompose the network into
smaller subnetworks; hence, B is not a valid cutset in recursive conditioning. Next, there
is a single cutset in cutset conditioning that is used at the very top level to generate a
number of polytree networks. But there are many cutsets in recursive conditioning, each
of which is used at a different level of the decomposition. Finally, the boundary condition
in cutset conditioning is that of reaching a polytree network but the boundary condition
in recursive conditioning is that of reaching a single-node network.
8.3.1 Recursive decomposition
The method of recursive conditioning employs the principle of divide-and-conquer, which
is quite prevalent in computer algorithms. However, the effectiveness of this method is

8.3 RECURSIVE CONDITIONING
183
A
B
C
D
E
A
AB
BC
CD
BDE
B
C
D
A
Figure 8.6: A dtree for a Bayesian network. Only CPT variables (families) are shown next to each leaf
node. The cutset of each nonleaf node is also shown.
very much dependent upon our choice of cutsets at each level of the recursive process.
Recall that the number of cases we have to consider at each level is exponential in the
size of the cutset. Therefore, we want to choose our cutsets to minimize the total number
of considered cases. Before we can address this issue, we need to introduce a formal tool
for capturing the collection of cutsets employed by recursive conditioning.
Deﬁnition 8.2. A dtree for a Bayesian network is a full binary tree, the leaves of which
correspond to the network CPTs.
■
Recall that a full binary tree is a binary tree where each node has 2 or 0 children.
Figure 8.6 depicts an example dtree. Following standard conventions on trees, we will
often not distinguish between a tree node T and the tree rooted at that node. That is, T
will refer both to a tree and the root of that tree. We will also use vars(T ) to denote the set
of variables that appear at the leaves of tree T . Moreover, we will use T p, T l, and T r to
refer to the parent, left child, and right child of node T , respectively. Finally, an internal
dtree node is a node that is neither leaf (no children) nor root (no parent).
A dtree T suggests that we decompose its associated Bayesian network by instantiating
variables that are shared by its left and right subtrees, T l and T r, vars(T l) ∩vars(T r). In
Figure 8.6, variable B is the only variable shared by the left and right subtrees of the root
node. Performing a case analysis on this variable splits the network into two disconnected
networks Nl and Nr, each of which can be solved independently. What is most important
is that subtrees T l and T r are guaranteed to be dtrees for the subnetworks Nl and Nr,
respectively. Therefore, each of these subnetworks can be decomposed recursively using
these subtrees. The process continues until we reach single-node networks, which cannot
be decomposed further.
Algorithm 13, RC1, provides the pseudocode for an implementation of (8.2) that uses
dtree T to direct the decomposition process. RC1 is called initially with a dtree T of
the Bayesian network and will return the probability of evidence e with respect to this
network. The algorithm does not compute cutsets dynamically but assumes that they have
been precomputed as follows.
Deﬁnition 8.3. The cutset of a nonleaf node T in a dtree is deﬁned as follows:
cutset(T )
def= (vars(T l) ∩vars(T r)) \ acutset(T ),
where acutset(T ), called the a-cutset of T , is the union of all cutsets associated with
ancestors of node T in the dtree.
■

184
INFERENCE BY CONDITIONING
Algorithm 13 RC1(T , e)
input:
T :
dtree node
e:
evidence
output: probability of evidence e
main:
1: if T is a leaf node then
2:
return LOOKUP(T, e)
3: else
4:
p←0
5:
C←cutset(T )
6:
for each instantiation c that is compatible with evidence e (c ∼e) do
7:
p←p+RC1(T l, ec)RC1(T r, ec)
8:
end for
9:
return p
10: end if
LOOKUP(T , e)
1: X|U←CPT associated with dtree node T
2: u←instantiation of U compatible with evidence e (u ∼e)
3: if X is instantiated to x in evidence e then
4:
return θx|u
5: else
6:
return 1 {= 
x θx|u}
7: end if
For the root T of a dtree, we have acutset(T ) = ∅and cutset(T ) is simply vars(T l) ∩
vars(T r). But for a nonroot node T , the cutsets associated with ancestors of T are excluded
from vars(T l) ∩vars(T r) since these cutsets are guaranteed to be instantiated when RC1
is called on node T .
Note that the larger the evidence e is, the less work that RC1 will do since that would
reduce the number of instantiations c it has to consider on Line 6. The only space used
by algorithm RC1 is that needed to store the dtree in addition to the space used by the
recursion stack. The time complexity of RC1 can be assessed by bounding the number of
recursive calls it makes as this count is proportional to its running time. For this, we need
the following deﬁnitions.
Deﬁnition 8.4. The cutset width of a dtree is the size of its largest cutset. The a-cutset
width of a dtree is the size of its largest a-cutset.
■
Moreover, X# will denote the number of instantiations of variables X.
Theorem 8.1. The total number of recursive calls made by RC1 to a nonroot node T is
≤acutset(T )# = O(exp(dw)), where w is the cutset width of the dtree and d is the depth
of node T .
■
In Figure 8.7, the cutset width of each dtree is 1. However, the a-cutset width is 7 for
the ﬁrst dtree and is 3 for the second. In general, for a chain of n variables both dtrees will
have a cutset width of 1 but the unbalanced dtree will have an a-cutset width of O(n) and

8.3 RECURSIVE CONDITIONING
185
A
B
C
D
E
F
G
H
A
AB
BC
CD
DE
EF
FG
GH
A
B
C
D
E
F
G
A
BC
AB
CD
A
B
C
D
E
F
G
DE
EF
FG
GH
Figure 8.7: Two dtrees for a chain network, with their cutsets explicated. We are only showing the
variables of CPTs (families) next to each leaf node.
the balanced dtree will have an a-cutset width of O(log n). Therefore, RC1 can make an
exponential number of recursive calls to some node in the ﬁrst dtree but will make only a
linear number of recursive calls to each node in the second dtree. This example illustrates
the impact of the used dtree on the complexity of recursive conditioning. Theorem 8.1
leads to the following complexity of recursive conditioning.
Theorem 8.2. Given a balanced dtree with n nodes and cutset width w, the time complexity
of RC1 is O(n exp(w log n)) and the space it consumes is O(wn).
■
We provide in Chapter 9 an algorithm that converts an elimination order of width w
into a dtree with cutset width ≤w + 1. We also describe a method for balancing the dtree
while keeping its cutset width ≤w + 1. Note that by using such an elimination order, the
algorithm of variable elimination will take O(n exp(w)) time to compute the probability
of evidence. These results show that RC1 will then take O(n exp(w log n)) time given such
an order. Note, however, that variable elimination will use O(n exp(w)) space as well,
whereas RC1 will only use O(wn) space.
We note here that the time complexity of RC1 is not comparable to the time complexity
of cutset conditioning (see Exercise 8.4). We also point out that when the width w is
bounded, n exp(w log n) becomes bounded by a polynomial in n. Therefore, with an
appropriate dtree RC1 takes polynomial time on any network with bounded treewidth.
8.3.2 Caching computations
The time complexity of RC1 is clearly not optimal. This is best seen by observing RC1
run on the dtree in Figure 8.8, where all variables are assumed to be binary. Consider
the node T marked with the bullet •. This node can be called by RC1 up to sixteen
different times, once for each instantiation of acutset(T ) = ABCD. Note, however, that
only variable D appears in the subtree rooted at T . Hence, the sixteen calls to T fall in
one of two equivalence classes depending on the value of variable D. This means that the
subnetwork corresponding to node T will have only two distinct instances depending on
the value of variable D. But RC1 does not recognize this fact, forcing it to solve sixteen
instances of this subnetwork even though it can afford to solve only two.
In general, each node T in a dtree corresponds to a number of subnetwork instances.
All of these instances share the same structure, which is determined by node T . But each
instance will have a different set of (reduced) CPTs and a different evidence depending

186
INFERENCE BY CONDITIONING
A
B
C
D
E
F
G
H
cutsets
a-cutsets
contexts
A
AB
BC
CD
DE
EF
FG
GH
A
B
C
D
E
F
G
A
AB
ABC
ABCD
ABCDE
ABCDEF
A
AB
BC
CD
DE
EF
FG
GH
A
B
C
D
E
F
A
AB
BC
CD
DE
EF
FG
GH
Figure 8.8: The dtree node labeled with • can be called sixteen different times by RC1, once for each
instantiation of variables ABCD.
on the instantiation e involved in the recursive call RC1(T, e). However, we can bound the
number of distinct instances using the following notion.
Deﬁnition 8.5. The context of node T in a dtree is deﬁned as follows:
context(T )
def= vars(T ) ∩acutset(T ).
Moreover, the context width of a dtree is the size of its maximal context.
■
Figure 8.8 depicts the context of each node in the given dtree.
The number of distinct instances solved for the subnetwork represented by node T
cannot exceed context(T )#. That is, even though node T may be called as many as
acutset(T )# times, these calls will produce no more than context(T )# distinct results.
This is because only the instantiation of vars(T ) will actually matter for the subnetwork
represented by node T .
Since each distinct instance is characterized by an instantiation of context(T ), all RC1
needs to do is save the result of solving each instance indexed by the instantiation of
context(T ). Any time a subnetwork instance is to be solved, RC1 will check its memory
ﬁrst to see if it has solved this instance previously. If it did, it will simply return the cached
answer. If it did not, it will recurse on T , saving its computed solution at the end. This
simple caching mechanism will actually drop the number of recursive calls considerably,
as we discuss later, but at the expense of using more space.
Algorithm 14, RC2, presents the second version of recursive conditioning which caches
its previous computations. All we had to do is include a cache with each node T in the
dtree, which is used to store the answers returned by calls to T . RC2 will not recurse on a
node T before it checks the cache at T ﬁrst.
It should be clear that the size of cacheT in RC2 is bounded by context(T )#. In Figure 8.8,
the cache stored at each node in the dtree will have at most two entries. Therefore,
RC2 will consume only a linear amount of space in addition to what is consumed by
RC1. Interestingly enough, this additional space will drop the complexity of recursive
conditioning from exponential to linear on this network.
Theorem 8.3. The number of recursive calls made to a nonroot node T by RC2 is
≤cutset(T p)#context(T p)#.
■

8.3 RECURSIVE CONDITIONING
187
Algorithm 14 RC2(T , e)
Each dtree node T has an associated cache cacheT which is indexed by instantiations of
context(T ). All cache entries are initialized to nil.
input:
T :
dtree node
e:
evidence
output: probability of evidence e
main:
1: if T is a leaf node then
2:
return LOOKUP(T, e)
3: else
4:
Y←context(T )
5:
y←instantiation of Y compatible with evidence e, y ∼e
6:
if cacheT [y] ̸= nil then
7:
return cacheT [y]
8:
else
9:
p←0
10:
C←cutset(T )
11:
for each instantiation c compatible with evidence e, c ∼e do
12:
p←p+RC2(T l, ec)RC2(T r, ec)
13:
end for
14:
cacheT [y]←p
15:
return p
16:
end if
17: end if
In Figure 8.8, each cutset has one variable and each context has no more than one vari-
able. Therefore, RC2 will make no more than four recursive calls to each node in the dtree.
We now deﬁne an additional notion that will be quite useful in analyzing the behavior
of recursive conditioning.
Deﬁnition 8.6. The cluster of a dtree node is deﬁned as follows:
cluster(T )
def=
 vars(T ),
if T is a leaf node
cutset(T ) ∪context(T ),
otherwise.
The width of a dtree is the size of its largest cluster minus one.
■
Lemma 8.1 in the proofs appendix provides some interesting relationships between cutsets,
contexts, and clusters. One of these relations is that the cutset and context of a node
are always disjoint. This means that cutset(T )#context(T )# = cluster(T )#. Hence, the
following result.
Theorem 8.4. Given a dtree with n nodes and width w, the time complexity of RC2 is
O(n exp(w)) and the space it consumes is O(n exp(w)).
■
We present a polytime algorithm in Chapter 9 that constructs a dtree of width ≤w given
an elimination order of width w. Together with Theorem 8.4, this shows that recursive
conditioning under full caching has the same time and space complexity attained by
algorithms based on variable elimination that we discussed in Chapter 6 and 7.

188
INFERENCE BY CONDITIONING
Algorithm 15 RC(T , e)
input:
T :
dtree node
e:
evidence
output: probability of evidence e
main:
1: if T is a leaf node then
2:
return LOOKUP(T, e)
3: else
4:
Y←context(T )
5:
y←instantiation of Y compatible with evidence e, y ∼e
6:
if cacheT [y] ̸= nil then
7:
return cacheT [y]
8:
else
9:
p←0
10:
C←cutset(T )
11:
for each instantiation c compatible with evidence e, c ∼e do
12:
p←p+RC(T l, ec)RC(T r, ec)
13:
end for
14:
when cache?(T, y), cacheT [y]←p
15:
return p
16:
end if
17: end if
8.4 Any-space inference
We have presented two extremes of recursive conditioning thus far. On one extreme, no
computations are cached, leading to a space complexity of O(wn) and a time complexity
of O(n exp(w log n)), where w is the width of a given dtree and n is the network size. On
the other extreme, all previous computations are cached, dropping the time complexity to
O(n exp(w)) and increasing the space complexity to O(n exp(w)).
These behaviors of recursive conditioning are only two extremes of an any-space
version, which can use as much space as is made available to it. Speciﬁcally, recursive
conditioning can cache as many computations as available space would allow and nothing
more. By changing one line in RC2, we obtain an any-space version, which is given in
Algorithm 15, RC. In this version, we include an extra test on Line 14 that is used to
decide whether to cache a certain computation. One of the simplest implementations of
this test is based on the availability of global memory. That is, cache?(T, y) will succeed
precisely when global memory has not been exhausted and will fail otherwise. A more
reﬁned scheme will allocate a certain amount of memory to be used by each cache. We
can control this amount using the notion of a cache factor.
Deﬁnition 8.7. A cache factor for a dtree is a function cf that maps each node T in the
dtree into a number 0 ≤cf(T ) ≤1.
■
The intention here is for cf(T ) to be the fraction of cache entries that will be ﬁlled
by Algorithm RC at node T . That is, if cf(T ) = .2, then we will only use 20% of the
total cache entries required by cacheT . Note that Algorithm RC1 corresponds to the case

8.5 DECOMPOSITION GRAPHS
189
where cf(T ) = 0 for every node T . Moreover, Algorithm RC2 corresponds to the case
where cf(T ) = 1. For each of these cases, we provided a count of the recursive calls made
by recursive conditioning. The question now is: What can we say about the number of
recursive calls made by RC under a particular cache factor cf?
As it turns out, the number of recursive calls made by RC under the memory committed
by cf will depend on the particular instantiations of context(T ) that will be cached on
Line 14. However, if we assume that any given instantiation y of context(T ) is equally
likely to be cached, then we can compute the average number of recursive calls made by
RC and, hence, its average running time.
Theorem 8.5. If the size of cacheT in Algorithm RC is limited to cf(T ) of its full size and
if each instantiation of context(T ) is equally likely to be cached on Line 14 of RC, the
average number of calls made to a nonroot node T in Algorithm RC is
ave(T ) ≤cutset(T p)# 
cf(T p)context(T p)# + (1 −cf(T p))ave(T p)

.
(8.3)
■
This theorem is quite important practically as it allows one to estimate the running
time of RC under any given memory conﬁguration. All we have to do is add up ave(T ) for
every node T in the dtree. Note that once ave(T p) is computed, we can compute ave(T ) in
constant time. Therefore, we can compute and sum ave(T ) for every node T in the dtree
in time linear in the dtree size.
Before we further discuss the practical utility of Theorem 8.5, we mention two important
points. First, when the cache factor is such that cf(T ) = 0 or cf(T ) = 1 for all dtree nodes,
we say it is a discrete cache factor. In this case, Theorem 8.5 provides an exact count
of the number of recursive calls made by RC. In fact, the running time of RC1 and RC2
follow as corollaries of Theorem 8.5:1
r When cf(T ) = 0 for all T :
ave(T ) ≤cutset(T p)#ave(T p),
and the solution to this recurrence is ave(T ) ≤acutset(T )#. This is basically the result of
Theorem 8.1.
r When cf(T ) = 1 for all T :
ave(T ) ≤cutset(T p)#context(T p)#,
which is the result of Theorem 8.3.
One of the key questions relating to recursive conditioning is that of identifying the
cache factor which would minimize the running time according to Theorem 8.5. We return
to this issue later after ﬁrst discussing an extension of recursive conditioning that allows
us to compute probabilistic quantities beyond the probability of evidence.
8.5 Decomposition graphs
A decomposition graph (dgraph) is a set of dtrees that share structure (see Figure 8.9).
Running RC on a dgraph amounts to running it on each dtree in the graph. Suppose, for
1 The inequalities become equalities when the evidence is empty.

190
INFERENCE BY CONDITIONING
A
AB
ABC
CD
AB
A
C
ABC
Figure 8.9: A dgraph. The cutset of each root node is shown next to the node.
A
AB
ABC
CD
A
B
C
D
Figure 8.10: A Bayesian network and a corresponding dtree.
example, that we have a dgraph with dtrees T1, . . . , Tn and evidence e. Although each call
RC(T1, e), . . . , RC(Tn, e) will return the same probability of evidence, we may still want to
make all calls for the following reason. When RC is applied to a dtree under evidence e, it
not only computes the probability of e but also the marginal over variables in the cutset of
the dtree root. Speciﬁcally, if C is the root cutset, then RC will perform a case analysis over
all possible instantiations c of C, therefore computing Pr(c, e) as a side effect. Running
RC on different dtrees then allow us to obtain different marginals. Consider the dgraph in
Figure 8.9, which depicts the cutset Ci associated with each root node Ti in the dgraph.
The call RC(Ti, e) will then compute the marginal Pr(Ci, e) as a side effect.
Since the dtrees of a dgraph share structure, the complexity of RC on a dgraph is
better than the sum of its complexity on the isolated dtrees. We later quantify the dgraph
complexity in precise terms.
Given a dtree T for a Bayesian network, we can always construct a dgraph that has
enough root cutsets to allow the computation of all family marginals for the network. In
particular, if node X is not a leaf node in the Bayesian network, then the dgraph will have
a root cutset corresponding to its family XU. In this case, running RC on the root will
compute the marginal Pr(XU, e), as discussed previously. However, if node X is a leaf
node, then the dgraph will have a root cutset corresponding to its parents U. In this case,
running RC on the root will compute the marginal Pr(U, e). But as X is a leaf node, the
marginal Pr(XU, e) can be easily obtained from Pr(U, e). Figure 8.10 depicts a Bayesian

8.5 DECOMPOSITION GRAPHS
191
Algorithm 16 DT2DG(T )
Uses a cache cache(., .) that maps two dtree nodes to another dtree node. All cache entries
initialized to nil.
input:
T :
dtree with ≥5 nodes
output: the roots of a decomposition graph for dtree T
main:
1: Leaves ←leaf nodes of dtree T
2: Tree←undirected tree obtained from dtree T by removing edge directions and then
removing the dtree root and connecting its neighbors
3: Roots ←∅{roots of constructed dgraph}
4: for each leaf node P in Leaves do
5:
C←single neighbor of node P in Tree
6:
R←a new dtree node with children P and ORIENT(C, P)
7:
add R to Roots
8: end for
9: return Roots
ORIENT(C, P)
1: if cache(C, P) ̸= nil then
2:
return cache(C, P)
3: else if C is a node in Leaves then
4:
R←C
5: else
6:
N1, N2←neighbors of node C in Tree such that N1 ̸= P and N2 ̸= P
7:
R←a new dtree node with children ORIENT(N1, C) and ORIENT(N2, C)
8: end if
9: cache(C, P)←R
10: return R
network and a corresponding dtree. Figure 8.9 depicts a dgraph for this Bayesian network
that satisﬁes these properties.
Algorithm 16, DT2DG, takes a dtree for a given Bayesian network and returns a cor-
responding dgraph satisfying these properties. The algorithm generates an undirected
version of the dtree that is then used to induce the dgraph. In particular, for each leaf
node P in the dtree associated with CPT X|U, the algorithm constructs a dtree with its
root R having P as one of its children. This guarantees that the cutset of root R will be
either XU or U, depending on whether variable X is a leaf node in the original Bayesian
network. The construction of such a dtree can be viewed as orienting the undirected tree
toward leaf node P. We will therefore say that Algorithm DT2DG works by orienting the
tree towards each of its leaf nodes.
Figure 8.11 depicts a partial trace of Algorithm DT2DG. In particular, it depicts the
undirected dtree constructed initially by the algorithm together with the dtree constructed
by the algorithm as a result of orienting the tree toward the leaf node labelled with family
AB. Figure 8.9 depicts the full dgraph constructed by the algorithm. Note that orienting
the tree toward the ﬁrst leaf node constructs a new dtree. However, successive orientations
will reuse parts of the previously constructed dtrees. In fact, one can provide the following
guarantee on Algorithm DT2DG.

192
INFERENCE BY CONDITIONING
A
AB
ABC
CD
A
AB
ABC
CD
Figure 8.11: An undirected tree constructed by Algorithm DT2DG, and a corresponding dtree that results
from orienting the undirected tree toward the leaf node with family AB.
Theorem 8.6. If Algorithm DT2DG is passed a dtree with n ≥5 nodes and width w, it
will return a dgraph with (5n −7)/2 nodes and 4(n −2) edges. Moreover, every dtree in
the dgraph will have width w.2
■
This means that running RC2 on either the dtree or the dgraph will have the same time
and space complexity of O(n exp(w)) (see also Exercise 8.8). The constant factors for the
dgraph run will be larger as it will generate more recursive calls and will need to maintain
more caches.
As we mentioned previously, we present a polytime algorithm in Chapter 9 that con-
structs a dtree of width ≤w from an elimination order of width w. Given this result, we
can then construct a dgraph of width w in this case, allowing recursive conditioning to
compute all family marginals in O(n exp(w)) time and space. This is the same complexity
achieved by the jointree algorithm of Chapter 7 for computing family marginals.
8.6 The cache allocation problem
We consider in this section the problem of ﬁnding a cache factor that meets some given
memory constraints while minimizing the running time of recursive conditioning. We
restrict ourselves to discrete cache factors where we have full or no caching at each
dgraph node. We also discuss both optimal and greedy methods for obtaining discrete
cache factors that satisfy some given memory constraints.
We ﬁrst observe that not all caches are equally useful from a computational viewpoint.
That is, two dtree nodes may have equally sized caches yet one of them may increase the
running time more than the other if we decide to stop caching at that node. An extreme
case of this concerns dead caches, whose entries would never be looked up after the cache
was ﬁlled. In particular, one can verify that if a dtree node T satisﬁes
context(T ) = cluster(T p),
(8.4)
then the cache at node T is dead as its entries would never be looked up if we also cache
at its parent T p (see Exercise 8.9). The cache at a dgraph node T is dead if it has a single
2 If n < 5, then n = 3 (two node network) or n = 1 (single node network) and any dtree for these networks can
already compute all marginals. Therefore, there is no need for a dgraph in this case.

8.6 THE CACHE ALLOCATION PROBLEM
193
T1
T2
T3
T3
T2
T3
T3
0
1
1
1
0
0
0
0
0
0
1
1
1
1
G0
G1 G2
G3
G4
G5 G6
G7
Figure 8.12: Search tree for a dgraph with three internal nodes.
parent and that parent satisﬁes the previous condition (see Exercise 8.10). We assume that
dead caches are removed from a dgraph and, hence, the search for a cache factor applies
only to nodes with live caches. It is also common to bypass caching at leaf dgraph nodes
especially when the CPTs associated with these leaf nodes have a tabular form, as the
CPT can be viewed as a cache in this case (see Exercise 8.11).
To search for a cache factor that minimizes running time, we need an efﬁcient way to
compute the running time of RC under a given cache factor. For this, we use the following
extension of (8.3) which applies to dgraphs:
calls(T ) =

T p
cutset(T p)#"
cf(T p)context(T p)# + (1 −cf(T p))calls(T p)
#
.
(8.5)
That is, we simply add up the number of calls that a node receives from each of its parents.
8.6.1 Cache allocation by systematic search
The cache allocation problem can be phrased as a systematic search problem in which
states in the search space correspond to partial cache factors, that is, a factor that maps
each dgraph node into {0, 1, ?}. The initial state in this space is the empty cache factor in
which no caching decisions have been made for any nodes in the dgraph. The goal states
are then complete cache factors where a caching decision was made for every dgraph node.
Suppose for example that we are searching for a cache factor over three dgraph nodes
T1, T2, T3. This will lead to the search tree in Figure 8.12. In this ﬁgure, each node n in the
search tree represents a partial cache factor cf. For example, the node in bold corresponds
to the partial cache factor cf(T1) = 0, cf(T2) = 1, and cf(T3) =?. Moreover, if node n is
labeled with a dgraph node Ti, then the children of n represent two possible extensions
of the cache factor cf: one in which dgraph node Ti will cache all computations (1-child)
and another in which dgraph node Ti will cache no computations (0-child). Finally, if
node n represents a partial cache factor, then the leaf nodes below n represent all possible
completions of this factor.
According to the search tree in Figure 8.12, one always makes a decision on dgraph
node T1 followed by a decision on dgraph node T2 and then on node T3. A ﬁxed ordering of
dgraph nodes is not necessary as long as the following condition is met: a decision should
be made on a dgraph node Ti only after decisions have been made on all its ancestors in
the dgraph. We will explain the reason for this constraint later.
In the search tree depicted in Figure 8.12, the leftmost leaf (G0) represents no caching
and the rightmost leaf (G7) represents full caching. The search trees for this problem
have a maximum depth of d, where d is the number of considered nodes in the dgraph.

194
INFERENCE BY CONDITIONING
Algorithm 17 OPTIMAL CF(, M)
input:
:
a set of dgraph nodes
M:
bound on number of cache entries
output: an optimal cache factor that assigns no more than M cache entries
main:
1: cf o←a cache factor {global variable}
2: co←∞{global variable}
3: cf←a cache factor that maps each node in  to ‘?’
4: OPTIMAL CF AUX(, cf)
5: return cache factor cf o
OPTIMAL CF AUX(, cf)
1: if cf allocates more than M cache entries or lower bound(cf) ≥c0 then
2:
return {prune node}
3: else if  is empty then {cf is a complete cache factor}
4:
c←number of recursive calls under cache factor cf {Equation 8.5}
5:
if c < c0, then c0←c and cf 0←cf
6: else {search both extensions of the cache factor}
7:
T ←choose a dgraph node in  that has no ancestors in 
8:
d1, d2←distinct decisions from {0, 1}
9:
cf(T )←d1, OPTIMAL CF AUX( \ {T }, cf)
10:
cf(T )←d2, OPTIMAL CF AUX( \ {T }, cf)
11:
cf(T )←?
12: end if
Given this property, depth-ﬁrst branch-and-bound is a good search algorithm given its
optimality and linear space complexity. It is also an anytime algorithm, meaning that it
can always return its best result so far if interrupted, and if run to completion will return
the optimal solution. Hence, we will focus on developing a depth-ﬁrst branch-and-bound
search algorithm. It should be noted that the search for a cache factor needs to be done
only once per dtree and then can be used to answer multiple queries.
A lower bound
The branch-and-bound algorithm requires a function, lower bound(cf), which provides a
lower bound on the number of recursive calls made by RC on any completion of the partial
cache factor cf. Consider now the completion cf ′ of a cache factor cf in which we decide
to cache at each dgraph node on which cf did not make a decision. This cache factor cf ′
is the best completion of cf from the viewpoint of running time but it may violate the
constraint given on total memory. Yet we will use the number of recursive calls for cf ′
as a value for lower bound(cf) since no completion of the cache factor cf will lead to a
smaller number of recursive calls than cf ′.
Algorithm 17, OPTIMAL CF, depicts the pseudocode for our search algorithm. One
important observation is that once the caching decision is made on the ancestors of dgraph
node T we can compute exactly the number of recursive calls that will be made to dgraph
node T ; see (8.5). Therefore, when extending a partial cache factor, we always insist on
making a decision regarding a dgraph node T for which decisions have been made on
all its ancestors, which explains Line 7 of OPTIMAL CF AUX. This strategy also leads to

8.6 THE CACHE ALLOCATION PROBLEM
195
Algorithm 18 GREEDY CF(, M)
input:
:
a set of dgraph nodes
M:
bound on number of cache entries
output: a cache factor that assigns no more than M cache entries
main:
1: cf←cache factor that maps each node in  to 0
2: while M > 0 and  ̸= ∅do
3:
compute a score for each dgraph node in 
4:
T ←dgraph node in  with largest score
5:
if M ≥context(T )# then
6:
cf(T )←1
7:
M←M −context(T )#
8:
end if
9:
remove T from 
10: end while
11: return cf
improving the quality of the lower bound monotonically as we search deeper in the tree.
It also allows us to compute this lower bound for a node T in constant time given that we
have the lower bound for its parent in the search tree.
Even though we make caching decisions on parent dgraph nodes before their children,
there is still a lot of ﬂexibility with respect to the choice on Line 7. In fact, the speciﬁc
choice we make here (the order in which we visit dgraph nodes in the search tree) turns
out to have a dramatic effect on the efﬁciency of search. Experimentation has shown that
choosing the dgraph node T with the largest context(T )# (i.e., the largest cache) can be
signiﬁcantly more efﬁcient than some other basic ordering heuristics.
Algorithm OPTIMAL CF leaves one more choice to be made: which child of a search
tree node to expand ﬁrst, determined by the speciﬁc values of d1 and d2 on Line 8 of
OPTIMAL CF AUX. Experimental results suggest that a choice of d1 = 1 and d2 = 0 tend
to work better, in general. This corresponds to trying to cache at particular node before
trying not to cache at that node.
8.6.2 Greedy cache allocation
This section proposes a greedy cache allocation method that runs in quadratic time. This
is considerably more efﬁcient than the systematic search Algorithm OPTIMAL CF, which
can take exponential time in the worst case. However, the greedy method is not guaranteed
to produce optimal cache allocations.
The greedy method starts with no memory allocated to any of the dgraph caches. It then
chooses a dgraph node T (one at a time) and allocates M = context(T )# cache entries to
node T . Suppose now that c1 is the number of recursive calls made by RC before memory
is allocated to the cache at node T . Suppose further that c2 is the number of recursive calls
made by RC after memory has been allocated to the cache at T (c2 ≤c1). The node T will be
chosen greedily to maximize (c1 −c2)/M: the number of reduced calls per memory unit.
The pseudocode for this method is shown in Algorithm 18, GREEDY CF. The while-
loop will execute O(n) times, where n is the number of dgraph nodes. Note that (8.5) can

196
INFERENCE BY CONDITIONING
be evaluated for all nodes in O(n) time, which gives us the total number of recursive calls
made by RC under any cache factor cf. Hence, the score of each dgraph node T can be
computed in O(n) time by simply evaluating (8.5) twice for all nodes: once while caching
at T and another time without caching at T . Under this method, Line 3 will take O(n2)
time, leading to a total time complexity of O(n3) for Algorithm GREEDY CF.
However, we will now show that the scores of all candidates can be computed in only
O(n) time, leading to a total complexity of O(n2). The key idea is to maintain for each
dgraph node T two auxiliary scores:
r callscf (T ): the number of calls made to node T under cache factor cf as given by (8.5)
r cpccf (T ): the number of calls made to descendants of node T for each call made to T
(inclusive of that call) under cache factor cf:
cpccf (T ) =
1,
if T is leaf or cf(T )=1
1 + cutset(T )#(cpccf (T l) + cpccf (T r)),
otherwise.
(8.6)
Let cf 2 be the cache factor that results from caching at node T in cache factor cf 1, and
let c1 and c2 be the total number of recursive calls made by RC under cache factor cf 1
and cf 2, respectively. The score of node T under cache factor cf 1 is then given by (see
Exercise 8.14):
scorecf 1(T ) =
c1 −c2
context(T )#
= cutset(T )#(callscf1(T ) −context(T )#)(cpccf 1(T l) + cpccf 1(T r))
context(T )#
.
(8.7)
Therefore, if we have the auxiliary scores calls(.) and cpc(.) for each node in the dgraph,
we can obtain the scores for all dgraph nodes in O(n) time. To initialize calls for each
node, we traverse the dgraph such that parent nodes are visited prior to their children. At
each node T , we compute calls(T ) using (8.5), which can be obtained in constant time
since the number of calls to each parent is already known. To initialize cpc for each node,
we visit children prior to their parents and compute cpc(T ) using (8.6).
To update these auxiliary scores, we ﬁrst note that the update is triggered by a single
change in the cache factor at node T . Therefore, the only affected scores are for nodes
that are ancestors and descendants of node T . In particular, for the descendants of node
T only calls(.) needs to be updated. Moreover, for node T and its ancestors only cpc(.)
needs to be updated.
Bibliographic remarks
Cutset conditioning was the ﬁrst inference algorithm proposed for arbitrary Bayesian net-
works [Pearl, 1986a; 1988]. Several reﬁnements were proposed on the algorithm for the
purpose of improving its time complexity but at the expense of increasing its space require-
ments [Darwiche, 1995; Díez, 1996]. Some other variations on cutset conditioning were
also proposed as approximate inference techniques (e.g., [Horvitz et al., 1989]). The rela-
tion between cutset conditioning and the jointree algorithm was discussed early in Shachter
et al. [1994]. The combination of conditioning and variable elimination algorithms was
discussed in Dechter [1999; 2003], and Mateescu and Dechter [2005]. The computa-
tion of loop-cutsets is discussed in Suermondt et al. [1991], Becker and Geiger [1994],

8.7 EXERCISES
197
A
B
C
D
E
F
G
C
EFG
CDF
BCE
B
ABD
A
1
3
2
5
4
6
7
8
9
10
11
12
13
Figure 8.13: A network and a corresponding dtree.
and Becker et al. [1999]. Recursive conditioning was introduced in Darwiche [2001], yet
the use of recursive decomposition in Bayesian network inference goes back to Cooper
[1990a]. Cache allocation using systematic search was proposed in Allen and Darwiche
[2004], and using greedy search in Allen et al. [2004]. Time-space tradeoffs have also
been achieved by combining elimination and conditioning algorithms as suggested in
Suermondt et al. [1991] and Dechter and Fattah [2001].
8.7 Exercises
8.1. Consider the Bayesian network and corresponding dtree given in Figure 8.13:
(a) Find vars(T ), cutset(T ), acutset(T ), and context(T ) for each node T in the dtree.
(b) What is the dtree width?
8.2. Consider the dtree in Figure 8.13. Assuming that all variables are binary:
(a) How many recursive calls will RC1 make when run on this dtree with no evidence?
(b) How many recursive calls will RC2 make when run on this dtree with no evidence?
(c) How many cache hits will occur at node 9 when RC2 is run with no evidence?
(d) Which of the dtree nodes have dead caches, if any?
All of these questions can be answered without tracing the recursive conditioning algorithm.
8.3. Figure 8.13 depicts a Bayesian network and a corresponding dtree with height 6. Find a dtree
for this network whose height is 3 and whose width is no worse than the width of the original
dtree.
8.4. Show a class of networks on which the time complexity of Algorithm RC1 is worse than the
time complexity of cutset conditioning. Show also a class of networks on which the time
complexity of cutset conditioning is worse than RC1.
8.5. Show that for networks whose loop-cutset is bounded, Algorithm RC2 will have time and
space complexity that are linear in the network size if RC2 is run on a carefully chosen dtree.
8.6. Show that if a network is pruned as given in Section 6.9 before applying RC1, then Line 6
of algorithm LOOKUP will never be reached. Recall that pruning a network can only be done
under a given query.
8.7. Consider the Bayesian network and dtree shown in Figure 8.14. Create a dgraph for this
network by orienting the given dtree with respect to each of its leaf nodes.
8.8. Suppose that Algorithm DT2DG is passed a dtree with n nodes and height O(log n). Will the
generated dgraph also have height O(log n)?

198
INFERENCE BY CONDITIONING
A
B
C
D
BCD
B
A
ABC
Figure 8.14: A network and a corresponding dtree.
8.9. Let T1, T2, . . . , Tn be a descending path in a dtree where
cf(Ti) =
!
1,
for i = 1 and i = n
0,
otherwise.
Show that each entry of cacheTn will be retrieved no more than
""
context(T1) ∪
n−1

i=1
cutset(Ti)
#
\ context(Tn)
##
−1
times after it has been ﬁlled by Algorithm RC. Show also that the deﬁnition of a dead dtree
cache given by Equation 8.4 follows as a special case.
8.10. Show that the deﬁnition of a dead cache given by Equation 8.4 is not sufﬁcient for a cache at
a dgraph node T . That is, show an example where this condition is satisﬁed for every parent
of dgraph node T yet cache entries at T are retrieved by RC after the cache has been ﬁlled.
8.11. Assume that CPTs support an implementation of LOOKUP that takes time and space that are
linear in the number of CPT variables (instead of exponential). Provide a class of networks
that have an unbounded treewidth yet on which RC will take time and space that are linear in
the number of network variables.
8.12. Suppose that you are running recursive conditioning under a limited amount of space. You
have already run the depth-ﬁrst branch-and-bound algorithm and found the optimal discrete
cache factor for your dgraph. Suddenly, the amount of space available to you increases. You
want to ﬁnd the new optimal discrete cache factor as quickly as possible.
(a) What is wrong with the following idea? Simply run the branch-and-bound algorithm on the set
of nodes that are not included in the current cache factor, ﬁnding the subset of these nodes that
will most efﬁciently ﬁll up the new space. Then add these nodes to the current cache factor.
(b) How can we use our current cache factor to ﬁnd the new optimal cache factor more quickly
than if we had to start from scratch?
8.13. Consider the dtree in Figure 8.13. Assuming all variables are binary, which internal node will
the greedy cache allocation algorithm select ﬁrst for caching? Show your work.
8.14. Prove Equation 8.7.
8.8 Proofs
The proofs in this section will make use of the following lemma, which is presented in
Chapter 9 as Theorem 9.13.
Lemma 8.1. The following relationships hold:
(a) cutset(T ) ∩context(T ) = ∅for nonleaf node T
(b) context(T ) ⊆cluster(T p)
(c) cutset(T p) ⊆context(T )

8.8 PROOFS
199
(d) cutset(T1) ∩cutset(T2) = ∅for nonleaf nodes T1 ̸= T2
(e) context(T ) = cluster(T ) ∩cluster(T p)
(f) context(T l) ∪context(T r) = cluster(T )
PROOF OF THEOREM 8.1. That the number of calls is ≤acutset(T )# follows from
Theorem 8.5 – see the discussion after the theorem statement. To show acutset(T )# =
O(exp(dw)), we note the following:
r The cutsets associated with the ancestors of T are pairwise disjoint by Lemma 8.1(d).
r The size of any of these cutsets is no greater than w.
r acutset(T ) is the union of cutset(T ′), where T ′ is an ancestor of T .
Hence, the size of acutset(T ) is bounded by dw and acutset(T )# = O(exp(dw)).
■
PROOF OF THEOREM 8.2. The height of a balanced dtree is O(log n) and its a-cutset
width must be O(w log n). Therefore, the number of recursive calls made by RC1 to
any node is O(exp(w log n)). The total number of recursive calls made by RC1 is then
O(n exp(w log n)). As far as the space requirements, the only space used by RC1 is for
the recursion stack and dtree storage.
■
PROOF OF THEOREM 8.3. Follows as a corollary of Theorem 8.5 – see discussion after
the theorem statement.
■
PROOF OF THEOREM 8.4. We have O(n) caches and the size of each cache is
≤context(T )#. Since the dtree has width w, we have context(T )# = O(exp(w)). Hence,
the size of all caches is O(n exp(w)). By Theorem 8.3, the number of recursive calls
to each node T is ≤cutset(T p)#context(T p)#. Note also that the dtree has width
w, cluster(T p) = cutset(T p) ∪context(T p), and the cutset and context are disjoint by
Lemma 8.1(a). Hence, cutset(T p)#context(T p)# = O(exp(w)) and the total number of
recursive calls is O(n exp(w)).
■
PROOF OF THEOREM 8.5. We will assume that RC is called on an empty evidence
e = true, leading to a worst-case complexity.
The central concept in this proof is the notion of a T -type for a given node T in the dtree.
This is basically the set of all calls to node T that agree on the instantiation of context(T )
at the time the calls are made. Calls of a particular T -type are guaranteed to return the
same probability. In fact, the whole purpose of cacheT is to save the result returned by
one member of each T -type so the result can be looked up when other calls in the same
T -type are made. Each T -type is identiﬁed by a particular instantiation y of context(T ).
Hence, there are context(T )# different T -types, each corresponding to one instantiation
of context(T ). We further establish the following deﬁnitions and observations:
r acpt(T ) is deﬁned as the average number of calls of a particular T -type.
r ave(T ) is deﬁned as the average number of calls to node T and equals ave(T ) =
acpt(T )context(T )#.
r A T -type y is either cached or not cached depending on whether the test cache?(T, y)
succeeds.
r We have cf(T )context(T )# cached T -types and (1 −cf(T ))context(T )# T -types that are
not cached.3
3 In algorithm RC1, all T -types are noncached (cf(T )=0). In RC2, all T -types are cached (cf(T ) = 1).

200
INFERENCE BY CONDITIONING
r A T p-type x is consistent with T -type y iff instantiations x and y agree on the values of
their common variables context(T p) ∩context(T ). Calls in a particular T -type y will be
generated recursively only by calls in a consistent T p-type x.
r There are (context(T p) \ context(T ))# T p-types that are consistent with a given T -type
y. On average,
r cf(T p)(context(T p) \ context(T ))# of them are cached, and
r (1 −cf(T p))(context(T p) \ context(T ))# are not cached.
This follows because each T p-type is equally likely to be cached. Moreover,
r On average, a cached T p-type x will generate cutset(T p)# calls to node T since
RC(T p) will recurse on only one call per cached T p-type. Only one of these calls is
consistent with a particular T -type y since cutset(T p) ⊆context(T ) by Lemma 8.1(c).
r On average, a noncached T p-type x will generate acpt(T p)cutset(T p)# calls to node
T since RC(T p) will recurse on every call in a noncached T p-type. Only acpt(T p) of
these calls are consistent with a particular T -type y.
r acpt(T ) can be computed for a particular T -type y by considering the average number of
calls of T p-types that are consistent with y:
acpt(T ) = αβ + γ σ
= (context(T p) \ context(T ))# $
cf(T p) + (1 −cf(T p))acpt(T p)
%
where
α = cf(T p)(context(T p) \ context(T ))# (number of cached T p-types consistent with y)
β = 1 (number of calls of T -type y each generates)
γ = (1 −cf(T p))(context(T p) \ context(T ))# (number of noncached T p-types
consistent with y)
σ = acpt(T p) (number of calls of T -type y each generates)
Hence,
ave(T )
= acpt(T )context(T )#
= (context(T p) \ context(T ))# $
cf(T p) + (1 −cf(T p))acpt(T p)
%
context(T )#
= (cluster(T p) \ context(T ))# $
cf(T p) + (1 −cf(T p))acpt(T p)
%
context(T )#
by Lemma 8.1(b,c)
= cluster(T p)# $
cf(T p) + (1 −cf(T p))acpt(T p)
%
by Lemma 8.1(b)
= cutset(T p)#context(T p)# $
cf(T p) + (1 −cf(T p))acpt(T p)
%
by Lemma 8.1(a,b)
= cutset(T p)# 
cf(T p)context(T p)# + (1 −cf(T p))acpt(T p)context(T p)#
= cutset(T p)# 
cf(T p)context(T p)# + (1 −cf(T p))ave(T p)

.
■
PROOF OF THEOREM 8.6. A dtree with n nodes has (n + 1)/2 leaves and (n + 1)/2 −2
internal nodes (which are neither leaves nor root). The dgraph generated by Algorithm 16
will have (n + 1)/2 leaves corresponding to the dtree leaves and (n + 1)/2 roots, one for
each leaf node. As for internal dgraph nodes, the algorithm will construct a node on Line 7

8.8 PROOFS
201
of Algorithm ORIENT only if node C is a nonleaf node in the undirected tree Tree and if
the test cache(C, P) fails. There are (n + 1)/2 −2 nonleaf nodes C in the undirected tree
and the test cache(C, P) will fail exactly three times, one time for each neighbor P of C.
Hence, the number of internal dgraph nodes constructed is 3((n + 1)/2 −2), leading to a
total number of dgraph nodes:
n + 1
2
+ n + 1
2
+ 3
n + 1
2
−2
	
= (5n −7)/2.
Each nonleaf dgraph node will have two outgoing edges. Hence, the total number of
dgraph edges is
2
n + 1
2
+ 3
n + 1
2
−2
		
= 4(n −2).
Now consider the width of dtrees in the generated dgraph. Consider a dtree T ′ that
results from orienting dtree T toward a leaf node with CPT X|U. Each node in the
resulting dtree T ′ must then fall into one of these categories:
r A root node: Its cluster must be contained in each of its children’s clusters, by
Lemma 8.1(c) and since the context of a root node is empty.
r A leaf node: Its cluster in dtree T ′ is the same as its cluster in dtree T (variables of the
CPT associated with the node).
r An internal node R: This node must be constructed on Line 7 of Algorithm ORIENT and
correspond to a node C in the original dtree T . We can verify that the variables connected
to node R through one of its neighbors in dtree T ′ are exactly the set of variables connected
to node C through one of its neighbors in dtree T . Hence, the cluster of node R in dtree
T ′ is the same as the cluster of node C in dtree T by Lemma 9.5 of Chapter 9.
This shows that the width of dtree T ′ is the same as the width of dtree T .
■

9
Models for Graph Decomposition
We consider in this chapter three models of graph decomposition: elimination orders,
jointrees and dtrees, which underly the key inference algorithms we discussed thus far.
We present formal deﬁnitions of these models, provide polytime, width-preserving trans-
formations between them, and show how the optimal construction of each of these models
corresponds in a precise sense to the process of optimally triangulating a graph.
9.1 Introduction
We presented three inference algorithms in previous chapters whose complexity can
be exponential only in the network treewidth: variable elimination, factor elimination
(jointree), and recursive conditioning. Each one of these algorithms can be viewed as
decomposing the Bayesian network in a systematic manner, allowing us to reduce a
query with respect to some network into a query with respect to a smaller network. In
particular, variable elimination removes variables one at a time from the network, while
factor elimination removes factors one at a time and recursive conditioning partitions the
network into smaller pieces. We also saw how the decompositional choices made by these
algorithms can be formalized using elimination orders, elimination trees (jointrees), and
dtrees, respectively. In fact, the time and space complexity of each of these algorithms
was characterized using the width of its corresponding decomposition model, which is
lower-bounded by the treewidth.
We provide a more comprehensive treatment of decomposition models in this chapter
including polytime, width-preserving transformations between them. These transforma-
tions allow us to convert any method for constructing low-width models of one type into
low-width models of other types. This is important since heuristics that may be obvious
in the context of one model may not be obvious in the context of other models.
We start in Section 9.2 by providing some graph-theoretic preliminaries that we use
in the rest of the chapter. We then treat elimination orders in Section 9.3, jointrees in
Section 9.4, and dtrees in Section 9.5. Our treatment of decomposition models also relates
them to the class of triangulated graphs in Section 9.6. In particular, we show that the
construction of an optimal elimination order, an optimal jointree, or an optimal dtree are
all equivalent to the process of constructing an optimal triangulation of a graph.
9.2 Moral graphs
When discussing a decomposition model for a Bayesian network, we will work with
the interaction graph of its factors (CPTs). Recall that an interaction graph for factors
f1, . . . , fn is an undirected graph whose nodes correspond to the variables appearing in
these factors and whose edges connect variables that appear in the same factor. If the
202

9.3 ELIMINATION ORDERS
203
A
B
C
D
E
F
G
A
B
C
D
E
F
G
B
A
C
D
E
F
B
A
C
D
E
F
Figure 9.1: DAGs (left) and their corresponding moral graphs (right).
factors f1, . . . , fn are the CPTs of a Bayesian network with DAG G, the interaction graph
for these factors can be obtained directly from G by a process known as moralization.
Deﬁnition 9.1. The moral graph
of a DAG G is an undirected graph obtained as
follows:
r Add an undirected edge between every pair of nodes that share a common child
in G.
r Convert every directed edge in G to an undirected edge.
■
Figure 9.1 depicts two DAGs and their corresponding moral graphs.
The notion of treewidth, which was central to our complexity analysis of inference
algorithms, is usually deﬁned for undirected graphs in the graph-theoretic literature. We
give this deﬁnition in the next section but we note here that the deﬁnition can be extended
formally to DAGs through their moral graphs.
Deﬁnition 9.2. The treewidth of a DAG G is the treewidth of its moral graph.
■
When we say “graph G” in this chapter, we mean an undirected graph; otherwise, we
say “DAG G” to mean a directed acyclic graph. The following are more deﬁnitions we
adopt. A neighbor of node X in graph G is a node Y connected to X by an edge. We also
say in this case that nodes X and Y are adjacent. The degree of node X in graph G is
the number of neighbors that X has in G. A clique in graph G is a set of nodes that are
pairwise adjacent, that is, every pair of nodes in the clique are connected by an edge. A
maximal clique is a clique that is not strictly contained in another clique. Finally, a graph
G is complete if its nodes form a clique.
9.3 Elimination orders
We discuss the generation of low-width elimination orders in this section using both
heuristic and optimal search methods. We also provide polytime, width-preserving algo-
rithms for converting jointrees and dtrees into elimination orders. We start with a formal

204
MODELS FOR GRAPH DECOMPOSITION
B
A
C
D
E
F
B
A
C
D
E
F
B
A
C
D
E
F
(a) DAG G
(b) Moral graph Gm
(c) Filled-in graph Gπ
m
Figure 9.2: The elimination order π is F, A, B, C, D, E.
A
B
C
D
E
A
B
C
D
A
B
C
A
B
A
C1 = CE
C2 = BCD
C3 = ABC
C4 = AB
C5 = A
G1
G2
G3
G4
G5
Figure 9.3: The graph and cluster sequence induced by elimination order π = E, D, C, B, A.
deﬁnition of elimination orders and some of their properties, including a deﬁnition of
treewidth based on such orders.
An elimination order for graph G is a total ordering π of the nodes of G, where π(i)
denotes the ith node in the ordering. To deﬁne the properties of elimination orders and
to deﬁne treewidth based on elimination orders, it is best to ﬁrst deﬁne the notion of
eliminating a node from a graph.
Deﬁnition 9.3. Let G be a graph and X be a node in G. The result of eliminating node
X from graph G is another graph obtained from G by ﬁrst adding an edge between every
pair of nonadjacent neighbors of X and then deleting node X from G. The edges that
are added during the elimination process are called ﬁll-in edges.
■
The elimination of node X from graph G ensures that X’s neighbors are made into a
clique before X is deleted from the graph. Let E be all of the ﬁll-in edges that result from
eliminating nodes from graph G according to order π. We will then use Gπ to denote
the graph that results from adding these ﬁll-in edges to G and write Gπ = G + E. We
will also refer to Gπ as the ﬁlled-in graph of G (see Figure 9.2). We also ﬁnd it useful to
deﬁne the sequence of graphs and clusters that are induced by an elimination order.
Deﬁnition 9.4. The elimination of nodes from graph G according to order π induces a
graph sequence G1, G2, . . . , Gn, where G1 = G and graph Gi+1 is obtained by elimi-
nating node π(i) from graph Gi. Moreover, the elimination process induces a cluster se-
quence C1, . . . , Cn, where Ci consists of node π(i) and its neighbors in graph Gi.
■
Figure 9.3 depicts a graph together with the corresponding graph and cluster sequence
induced by the elimination order E, D, C, B, A.

9.3 ELIMINATION ORDERS
205
We can now formally deﬁne the width of an elimination order in terms of the largest
cluster it induces.
Deﬁnition 9.5. Let π be an elimination order for graph G and let C1, . . . , Cn be the
cluster sequence induced by applying the elimination order π to graph G. The width of
elimination order π with respect to graph G is
width(π, G)
def=
n
max
i=1 |Ci| −1.
We also extend the deﬁnition to DAGs: the width of elimination order π with respect to
a DAG is its width with respect to the moral graph of this DAG.
■
Considering Figure 9.3, the elimination order E, D, C, B, A has width 2 since the largest
cluster it induces contains three variables.
Now that we have deﬁned the width of an elimination order, we can also deﬁne the
treewidth of a graph as the width of its best elimination order.
Deﬁnition 9.6. The treewidth of a graph G is
treewidth(G)
def=
min
π width(π, G),
where π is an elimination order for graph G.
■
When the width of an elimination order π equals the treewidth of graph G, we say
that the order is optimal. Hence, by ﬁnding an optimal elimination order one is also
determining the treewidth of the underlying graph.
When the elimination order π does not lead to any ﬁll-in edges when applied to graph
G, we say that π is a perfect elimination order for graph G. Not every graph admits a
perfect elimination order. However, graphs that admit such orders are quite special as they
allow us to construct optimal elimination orders in polytime. More on this is presented in
Section 9.6.
9.3.1 Elimination heuristics
Since the computation of an optimal elimination order is known to be NP-hard, a number
of greedy elimination heuristics have been proposed in literature, suggesting ways to
eliminate nodes from a graph G based on local considerations. The following are two of
the more common heuristics:
r min-degree: Eliminate the node having the smallest number of neighbors.
r min-ﬁll: Eliminate the node that leads to adding the smallest number of ﬁll-in edges.
Figure 9.4 depicts a graph where node C has a min-degree score of 4 (number of neighbors)
and a ﬁll-in score of 1 (number of ﬁll-in edges). The min-ﬁll heuristic is known to produce
better elimination orders than the min-degree heuristic. Moreover, the min-degree heuristic
is known to be optimal for graphs whose treewidth is ≤2.
It is quite common in practice to combine heuristics. For example, we ﬁrst select
the most promising node to eliminate based on the min-ﬁll heuristic, breaking ties with
the min-degree heuristic. We can also apply min-degree ﬁrst and then break ties using

206
MODELS FOR GRAPH DECOMPOSITION
B
A
E
C
D
Figure 9.4: Variable C has four neighbors but will add one min-ﬁll edge if eliminated.
min-ﬁll. Stochastic techniques have also proven quite powerful in combining heuristics.
These techniques can be applied in two ways:
r The same elimination heuristic is used to eliminate every node but ties are broken stochas-
tically.
r Different elimination heuristics are used to eliminate different nodes, where the choice of
a heuristic at each node is made stochastically.
9.3.2 Optimal elimination preﬁxes
A preﬁx of elimination order π is a sequence of variables τ that occurs at the beginning
of order π. For example, if π is the order A, B, C, D, E, then A, B, C is a preﬁx of π
and so is A, B. If τ is a preﬁx of some optimal elimination order π, we say that τ is an
optimal elimination preﬁx as it can be completed to yield an optimal elimination order.
The notion of width can be extended to elimination preﬁxes in the obvious way, that is,
by considering the cluster sequence that results from applying the elimination preﬁx to a
graph.
We now discuss four rules for preprocessing undirected graphs to generate optimal
preﬁxes. That is, the preprocessing rules will eliminate a subset of the variables in a
graph, while guaranteeing that they represent the preﬁx of some optimal elimination
order. These rules are not complete for arbitrary graphs in the sense that we cannot always
use them to produce a complete elimination order. Yet one can use these rules to eliminate
as many nodes as possible before continuing the elimination process using heuristic or
optimal elimination methods.
As we apply these rules, which can be done in any order, a lower bound (low) is
maintained on the treewidth of the given graph. Some of the rules will update this bound,
while others will use it as a condition for applying the rule. If we start with a graph G
and a lower bound low, the rules will guarantee the following invariant. If graph G′ is the
result of applying any of these rules to G and if low is updated accordingly, then
treewidth(G) = max(treewidth(G′), low).
Therefore, these rules can be used to reduce the computation of treewidth for graph G
into the computation of treewidth for a smaller graph G′. Moreover, they are guaranteed
to generate only optimal elimination preﬁxes.
In the following, we assume that graph G has at least one edge and, hence, its treewidth
is ≥1. We therefore assume that low is set initially to 1 unless we have some information

9.3 ELIMINATION ORDERS
207
A
B
C
D
Figure 9.5: Nodes A, B, C, D forming a cube. The dashed edges indicate optional connections to nodes
not in the ﬁgure.
about the treewidth of G that allows us to set low to a higher value, as we discuss in
Section 9.3.3.
Some of the rules deal with simplicial and almost simplicial nodes. A simplicial node
is one for which all of its neighbors are pairwise adjacent, forming a clique. An almost
simplicial node is one for which all but one neighbor form a clique. The four rules are
given next, with proofs of correctness omitted:
r Simplicial rule: Eliminate any simplicial node with degree d, updating low to max(low, d).
r Almost simplicial rule: Eliminate any almost simplicial node with degree d as long as
low ≥d.
r Buddy rule: If low ≥3, eliminate any pair of nodes X and Y that have degree 3 each and
share the same set of neighbors.
r Cube rule: If low ≥3, eliminate any set of four nodes A, B, C, D forming the structure
in Figure 9.5.
We point out here that these four rules are known to be complete for graphs of tree-
width ≤3.
Some of these rules have well-known special cases. For example, the simplicial rule
has these special cases:
r Islet rule: Eliminate nodes with degree 0.
r Twig rule: Eliminate nodes with degree 1.
These follow since nodes of degree 0 and 1 are always simplicial. The almost simplicial
rule has two special cases:
r Series rule: Eliminate nodes with degree 2 if low ≥2.
r Triangle rule: Eliminate nodes with degree 3 if low ≥3 and if at least two of the neighbors
are connected by an edge.
We close this section by pointing out that preprocessing rules can be extremely impor-
tant in practice as the application of these rules may reduce the graph size considerably.
In turn, this can improve the running time of expensive algorithms that attempt to obtain
optimal elimination orders, as given in Section 9.3.4. It can also improve the quality of
elimination orders obtained by elimination heuristics.

208
MODELS FOR GRAPH DECOMPOSITION
A
C
B
E
D
C
B
E
D
(a) Graph degree is 2
(b) Graph degree is 3
Figure 9.6: The degree of a graph (a) is smaller than the degree of a subgraph (b).
9.3.3 Lower bounds on treewidth
We discuss several lower bounds on treewidth in this section that can be used to empower
the preprocessing rules discussed in the previous section. These lower bounds are also
critical for algorithms that search for optimal elimination orders, which we discuss in the
next section.
Our ﬁrst lower bound is based on the cliques of a graph.
Theorem 9.1. If graph G has a clique of size n, then treewidth(G) ≥n −1.
■
Another lower bound is the degree of a graph (see Exercise 9.12), which is easier to
compute.
Deﬁnition 9.7. The degree of a graph is the minimum number of neighbors attained by
any of its nodes.
■
Both of the previous bounds are typically too weak. A better bound can be based on
the following observations. First, the treewidth of any subgraph cannot be larger than the
treewidth of the graph containing it. Second, the degree of a subgraph may be higher
than the degree of the graph containing it. Figure 9.6 depicts such an example. These two
observations lead to a lower bound called graph degeneracy.
Deﬁnition 9.8. The degeneracy of a graph is the maximum degree attained by any of
its subgraphs.
■
The degeneracy of a graph is also known as the maximum minimum degree (MMD). The
MMD lower bound is easily computed by generating a sequence of subgraphs, starting
with the original graph and then obtaining the next subgraph by removing a minimum-
degree node. The MMD is then the maximum degree attained by any of the generated
subgraphs (see Exercise 9.14). Consider for example the graph in Figure 9.6(a). To
compute the MMD lower bound for this graph, we generate subgraphs by removing nodes
according to the order A, B, C, D, E. The resulting subgraphs have the respective degrees
2, 3, 2, 1, 0. Hence, the MMD lower bound is 3, which is also the graph degeneracy.
An even better lower bound can be obtained by observing that one can contract edges in
a graph without raising its treewidth, where contracting edge X−Y corresponds to replacing
its nodes X and Y with a new node that is adjacent to the union of their neighbors (see
Exercise 9.13). Figure 9.7 depicts an example of edge contraction.

9.3 ELIMINATION ORDERS
209
D
C
A
E
B
D
C
AB
E
Figure 9.7: Contracting the edge A−B, which leads to removing nodes A and B, while adding the new
node AB. Contracting the edge increases the graph degree from 2 to 3 in this case.
Deﬁnition 9.9. A graph minor is obtained by any combination of removing nodes,
removing edges, or contracting edges in a graph.
■
Deﬁnition 9.10. The contraction degeneracy of a graph is the maximum degree attained
by any of its minors.
■
The contraction degeneracy lower bound is also known as the MMD+ lower bound.
Although MMD+ provides tighter bounds than MMD, it is unfortunately NP-hard to
compute. However, there are heuristic approximations of MMD+ that are relatively easy
to compute yet can still provide tighter bounds than MMD. These approximations consider
only a small subset of graph minors, which are obtained by contracting edges. The two
dominant approximations work by contracting an edge incident to a minimum-degree
node. This includes the MMD+(min-d) approximation, which works by contracting the
edge between a minimum-degree node and one of its minimum-degree neighbors. The
other approximation is MMD+(least-c), which works by contracting the edge between a
minimum-degree node and a neighbor with which it shares the least number of common
neighbors. In practice, MMD+(least-c) leads to better bounds but MMD+(min-d) can be
computed more efﬁciently.
9.3.4 Optimal elimination orders
In this section, we consider two algorithms for computing optimal elimination orders.
The ﬁrst algorithm is based on depth-ﬁrst search, which is marked by its modest space
requirements and its anytime behavior. The second algorithm is based on best-ﬁrst search,
which has a better time complexity but consumes more space. The two algorithms can be
motivated by the different search spaces they explore.
Depth-ﬁrst search
Figure 9.8 depicts the search space explored by depth-ﬁrst search, which is a tree with
leaf nodes corresponding to distinct variable orders. This tree will then have size O(n!),
where n is the total number of variables. One can explore this tree in a depth-ﬁrst manner,
which can be implemented in O(n) space. However, one can improve the performance of
the algorithm by using lower bounds on treewidth to prune some parts of the search tree.
Suppose for example that we already have an elimination order π with width wπ and are
currently exploring an elimination preﬁx τ whose width is wτ. Suppose further that we
have a lower bound b on the treewidth of subgraph Gτ, which results from applying the

210
MODELS FOR GRAPH DECOMPOSITION
A
B
C
AB
AC
AD
BA
BC
BD
CA
CB
CD
DA
DB DC
D
Figure 9.8: The search tree for an elimination order over four variables.
preﬁx τ to graph G. If max(wτ, b) ≥wπ, then we clearly cannot improve on the order π,
since every completion of preﬁx τ is guaranteed to have an equal or higher width. In this
case, we can simply abandon the exploration of preﬁx τ and the corresponding search
subtree.
Algorithm 19, DFS OEO, provides the pseudocode for the proposed algorithm. The
global order π is a seed to the algorithm, which can be chosen based on elimination
heuristics such as min-ﬁll. The better this seed is, the more efﬁcient the algorithm will be
as this will lead to more pruning on Line 5. Algorithm DFS OEO is an anytime algorithm
as it can be stopped at any point during its execution, while guaranteeing that the global
variable π will be set to the best elimination order found thus far.
Best-ﬁrst search
As mentioned previously, DFS OEO has a linear space complexity. However, its main
disadvantage is its time complexity that stems from the tree it needs to search. In particular,
if the algorithm is given a graph G with n variables, it may have to fully explore a tree of size
O(n!) (the number of elimination orders for n variables is n!). One can improve the time
complexity of Algorithm DFS OEO considerably if one exploits the following theorem.
Theorem 9.2. If τ1 and τ2 are two elimination preﬁxes that contain the same set of
variables, then applying these preﬁxes to a graph G will lead to identical subgraphs, Gτ1
and Gτ2.
■
Given this result, the search tree of Algorithm DFS OEO will contain many replicated
parts. For example, the subtree that results from eliminating variable A and then variable
B will be the same as the subtree that results from eliminating variable B and then variable
A (see Figure 9.8). If we merge these identical subtrees, we reach the graph structure given
in Figure 9.9. Each node in this graph structure corresponds to a set of variables that can
be eliminated in any order. However, regardless of what order τ is used to eliminate these
variables, the subgraph Gτ will be the same and, hence, this subgraph need not be solved
multiple times as is done by Algorithm DFS OEO.
One can avoid these redundant searches using a best-ﬁrst search. The basic idea here
is to maintain two lists of search nodes: an open list and a closed list. The open list
contains search nodes that need to be explored, while the closed list contains search nodes

9.3 ELIMINATION ORDERS
211
{}
{A}
{B}
{C}
{D}
{A,B}
{A,C}
{A,D}
{B,C}
{B,D}
{C,D}
{A,B,C}
{A,B,D}
{A,C,D}
{B,C,D}
{A,B,C,D}
Figure 9.9: The search graph for an elimination order over four variables.
Algorithm 19 DFS OEO(G)
input:
G:
graph
output: optimal elimination order for graph G
main:
π←some elimination order for graph G {global variable}
wπ←width of order π {global variable}
τ←empty elimination preﬁx
wτ←0
DFS OEO AUX(G, τ, wτ)
return π
DFS OEO AUX(Gτ, τ, wτ)
1: if subgraph Gτ is empty then {τ is a complete elimination order}
2:
if wτ < wπ, then π←τ and wπ←wτ {found better elimination order}
3: else
4:
b←lower bound on treewidth of subgraph Gτ {see Section 9.3.3}
5:
if max(wτ, b) ≥wπ, return {prune as we cannot improve on order π}
6:
for each variable X in subgraph Gτ do
7:
d←number of neighbors X has in subgraph Gτ
8:
Gρ←result of eliminating variable X from subgraph Gτ
9:
ρ←result of appending variable X to preﬁx τ
10:
wρ←max(wτ, d)
11:
DFS OEO AUX(Gρ, ρ, wρ)
12:
end for
13: end if

212
MODELS FOR GRAPH DECOMPOSITION
Algorithm 20 BFS OEO(G)
input:
G:
graph
output: optimal elimination order for graph G
1: τ←empty elimination preﬁx
2: b←lower bound on treewidth of graph G {see Section 9.3.3}
3: OL←{(G, τ, 0, b)} {open list}
4: CL←empty list {closed list}
5: while open list OL not empty do
6:
(Gτ, τ, wτ, bτ)←node from open list OL with smallest max(wτ, bτ)
7:
if subgraph Gτ is empty then {τ is a complete and optimal order}
8:
return elimination order τ
9:
else
10:
for each variable X in subgraph Gτ do
11:
d←number of neighbors X has in subgraph Gτ
12:
Gρ←result of eliminating variable X from subgraph Gτ
13:
ρ←result of appending variable X to preﬁx τ
14:
wρ←max(wτ, d)
15:
bρ←lower bound on treewidth of subgraph Gρ {see Section 9.3.3}
16:
if OL contains (Gρ′, ρ′, wρ′, bρ′) and Gρ′ = Gρ then
17:
if wρ < wρ′ then {preﬁx ρ is better}
18:
remove (Gρ′, ρ′, wρ′, bρ′) from open list OL
19:
add (Gρ, ρ, wρ, bρ) to open list OL
20:
end if
21:
else if CL does not contain (Gρ′, ρ′, wρ′, bρ′) where Gρ′ = Gρ then
22:
add (Gρ, ρ, wρ, bρ) to open list OL {Gρ never visited before}
23:
end if
24:
end for
25:
add (Gτ, τ, wτ, bτ) to closed list CL
26:
end if
27: end while
that have already been explored. Algorithm 20, BFS OEO, provides the pseudocode for
best-ﬁrst search, which represents a search node by a tuple (Gτ, τ, wτ, bτ). Here τ is
an elimination preﬁx, wτ is its width, Gτ is the subgraph that results from applying the
preﬁx τ to the original graph G, and bτ is a lower bound on the treewidth of subgraph
Gτ.1 Algorithm BFS OEO iterates by choosing a most promising node from the open
list, that is, a node (Gτ, τ, wτ, bτ) that minimizes max(wτ, bτ). This corresponds to
choosing an elimination preﬁx τ that leads to the most optimistic estimate of treewidth.
If the preﬁx τ is complete, the algorithm terminates while declaring that τ is an optimal
elimination order. Otherwise, the algorithm will generate the children (Gρ, ρ, wρ, bρ) of
node (Gτ, τ, wτ, bρ), which result from eliminating one variable from subgraph Gτ.
Before adding the new node (Gρ, ρ, wρ, bρ) to the open list, the algorithm will check if
the subgraph Gρ appears on either the closed list or the open list. If no duplicate subgraph
1 In a practical implementation, one does not keep track of the full preﬁx τ but just a pointer back to the parent of
the search node. The preﬁx can be recovered by following these pointers to the root node of the search graph.

9.3 ELIMINATION ORDERS
213
is found, the node is added to the open list and the search continues. If a duplicate subgraph
is found on the closed list, then it has already been expanded and the node (Gρ, ρ, wρ, bρ)
will be discarded. If a duplicate subgraph is found on the open list and it turns out to have
a worse preﬁx than ρ, then the duplicate is removed and the new node (Gρ, ρ, wρ, bρ) is
added to the open list; otherwise, the new node will be discarded. Note that a duplicate
cannot exist on both the open and closed lists simultaneously (see Exercise 9.15). Proving
the correctness of this algorithm is left to Exercise 9.16.
We note here that although the search graph explored by BFS OEO has size O(2n) –
which is a considerable improvement over the search tree of size O(n!) explored by
DFS OEO – Algorithm BFS OEO may have to store O(2n) nodes in the worst case. This is
to be contrasted with the linear number of nodes stored by Algorithm DFS OEO.
We close this section by emphasizing again the importance of preprocessing rules that
we discussed in Section 9.3.2. In particular, by using these rules one can generate an
optimal preﬁx τ and then run Algorithm DFS OEO or Algorithm BFS OEO on the smaller
subgraph Gτ instead of the original graph G. This can lead to exponential savings in
some cases as it may considerably reduce the size of search spaces explored by these
algorithms. These preprocessing rules can also be used at each node in the search space.
That is, when considering a search node (Gτ, τ, wτ, bτ), we can run these preprocessing
rules to ﬁnd an optimal preﬁx ϱ of subgraph Gτ and then continue the search using the
subgraph Gτϱ and preﬁx τϱ. This can reduce the search space considerably but it may
also increase the computational overhead associated with the processing at each search
node. It may then be necessary to apply only a select subset of preprocessing rules at each
search node to balance the preprocessing cost with the savings that result from searching
a smaller space.
9.3.5 From jointrees to elimination orders
Jointrees were introduced in Chapter 7 and are given a more comprehensive treatment in
Section 9.4. Here we deﬁne a polytime algorithm for converting a jointree to an elimination
order of no greater width. In particular, given a DAG G and a corresponding jointree,
Algorithm 21, JT2EO, will return an elimination order for G whose width is no greater
than the jointree width.
Algorithm 21 JT2EO(T , C)
input:
(T , C):
jointree for DAG G
output: elimination order π for DAG G where width(π, G) is no greater than the width
of jointree (T , C)
main:
1: π←empty elimination order
2: while there is more than one node in tree T do
3:
remove a node i from T that has a single neighbor j
4:
append variables Ci \ Ci ∩Cj to order π
5: end while
6: append variables Cr to order π, where r is the remaining node in tree T
7: return elimination order π

214
MODELS FOR GRAPH DECOMPOSITION
ABD
ACE
DFG
ADF
AEF
EFH
1
5
3
2
6
4
ABD
ACE
DFG
ADF
AEF
EFH
1
3
2
4
5
6
(a)
(b)
Figure 9.10: A jointree with two numberings of the clusters according to the order in which JT2EO may
visit them.
JT2EO is effectively simulating the process of factor elimination, as discussed in
Chapter 7. In particular, at each step JT2EO picks up a leaf cluster Ci that has a single
neighbor Cj and eliminates those variables that appear in cluster Ci but nowhere else in
the tree. Note that if a variable appears in cluster Ci and in some other cluster Ck in the
tree, it must also appear in the neighbor Cj of Ci and, hence, in Ci ∩Cj. This is why
JT2EO eliminates variables Ci \ Ci ∩Cj.
As stated, JT2EO leaves a number of choices undetermined: choosing node i to remove
next from the jointree and choosing the order in which variables Ci \ Ci ∩Cj are appended
to the elimination order. None of these choices actually matter to the guarantee provided
by the algorithm. Hence, JT2EO can be thought of as deﬁning a class of elimination
orders whose width is no greater than the given jointree width. Consider Figure 9.10 as
an example. The ﬁgure contains a jointree with two numberings of the clusters according
to the order in which JT2EO may remove them. The ordering in Figure 9.10(a) gives
rise to the elimination order π = G, C, B, H, D, {AEF}, where {AEF} means that these
variables can be in any order. The numbering in Figure 9.10(b) leads to the elimination
order π = G, B, D, C, A, {EFH}. These orderings have width 2, which is also the width
of the given jointree.
Note that JT2EO may generate elimination orders whose width is less than the width
of the jointree. This can only happen when the jointree is not optimal, that is, the jointree
width is greater than the treewidth of the corresponding graph. Exercise 9.17 suggests a
method for proving the correctness of JT2EO.
9.3.6 From dtrees to elimination orders
Dtrees were introduced in Chapter 8 and are given a more comprehensive treatment
in Section 9.5. Here we present a polytime algorithm for converting a dtree into an
elimination order of no greater width. In particular, we show that each dtree speciﬁes a
partial elimination order and any total order consistent with it is guaranteed to have no
greater width.
Deﬁnition 9.11. A variable X is eliminated at a dtree node T precisely when X belongs
to cluster(T ) \ context(T ).
■
Note that if T is a nonleaf node, then cluster(T ) \ context(T ) is precisely cutset(T ).
Figures 9.11(b) and 9.12(b) depict two dtrees and the variables eliminated at each of their
nodes.

9.3 ELIMINATION ORDERS
215
A
B
C
D
E
F
G
H
AC
ACE
EFH
A
AF
AB
ABD
DFG
C
E
H
A
DF
G
B
(a) DAG
(b) dtree
Figure 9.11: Converting a dtree into an elimination order. The variables eliminated at a dtree node are
shown next to the node.
B
A
C
E
F
D
G
FG
EF
CE
A
AC
AB
BD
E
F
AC
B
G
D
(a) DAG
(b) dtree
Figure 9.12: Converting a dtree into an elimination order. The variables eliminated at a dtree node are
shown next to the node.
Theorem 9.3. Given a dtree for DAG G, every variable in G is eliminated at some unique
node in the dtree.
■
Theorem 9.3 allows us to view each dtree as inducing a partial elimination order.
Deﬁnition 9.12. A dtree induces the following partial elimination order on its variables:
For every pair of variables X and Y, we have X < Y if the dtree node at which X is
eliminated is a descendant of the dtree node at which Y is eliminated. We also have
X = Y if the two variables are eliminated at the same dtree node.
■
In the dtree of Figure 9.11(b), we have C < E < A < D and D = F. We also have
H < E, B < A, G < D, and G < F. Any total elimination order consistent with these
constraints will have a width that is no greater than the dtree width.
Theorem 9.4. Given a dtree of width w for DAG G, let π be a total elimination order
for G which is consistent with the partial elimination order deﬁned by the dtree. We then
have width(π, G) ≤w.
■

216
MODELS FOR GRAPH DECOMPOSITION
Algorithm 22 DT2EO(T )
input:
T :
dtree
output: elimination order π for the variables of dtree T
main:
1: if T is a leaf node then
2:
return any ordering of variables cluster(T ) \ context(T )
3: else
4:
πl←DT2EO(T l)
5:
πr←DT2EO(T r)
6:
π←a merge of suborders πl and πr preserving the order of variables within each
suborder
7:
append variables cluster(T ) \ context(T ) to order π
8:
return elimination order π
9: end if
The following two orders are consistent with the dtree in Figure 9.11(b): π1 =
C, H, E, B, A, G, D, F and π2 = H, C, B, E, G, A, F, D. For another example, con-
sider Figure 9.12(b), which depicts a dtree with width 2. The elimination order
π = G, F, D, B, A, C, E has width 1 and is consistent with the dtree. An elimination
order consistent with a dtree will have a width lower than the dtree width only when the
dtree is not optimal. The dtree in Figure 9.12(b) is not optimal as the corresponding DAG
has treewidth 1.
Algorithm 22, DT2EO, provides pseudocode for generating an elimination order from
a dtree based on the method described here. The algorithm performs a post-order traversal
of the dtree in which a dtree node is visited after each of its children has been visited.
9.4 Jointrees
We discussed jointrees in Chapter 7, where we showed how they represent the basis of
the jointree algorithm. We discuss jointrees in more detail in this section, starting with a
repetition of their deﬁnition from Chapter 7.
Deﬁnition 9.13. A jointree for a DAG G is a pair (T , C) where T is a tree and C is a
function that maps each node i in tree T into a label Ci called a cluster. The jointree
must satisfy the following properties:
1. The cluster Ci is a set of nodes from the DAG G.
2. Each family in the DAG G must appear in some cluster Ci.
3. If a node appears in two clusters Ci and Cj, it must also appear in every cluster Ck
on the path connecting nodes i and j in the jointree. This is known as the jointree
property.
The separator of edge i−j in a jointree is deﬁned as Sij = Ci ∩Cj. The width of a
jointree is deﬁned as the size of its largest cluster minus one.
■
A jointree is sometimes called a junction tree.
Figure 9.13 depicts a DAG and two corresponding jointrees. The jointree in Fig-
ure 9.13(b) has three nodes: 1, 2, 3. Moreover, the cluster of node 1 is C1 = ABC, the

9.4 JOINTREES
217
B
A
C
D
E
BCD
CE
ABC
1
2
3
BC
C
BCD
CDE
ABC
AB
1
4
2
3
BC
AB
CD
(a) DAG
(b) Minimal jointree
(c) Nonminimal jointree
Figure 9.13: A DAG and two corresponding jointrees.
separator of edge 1−2 is S12 = BC, and the jointree width is 2. The jointree Figure 9.13(c)
has four nodes and width 2. Jointrees can also be deﬁned for undirected graphs, as given
in Exercise 9.25. According to this deﬁnition, a jointree for the moral graph Gm of DAG
G will also be a jointree for DAG G.
To gain more insight into jointrees, we note that the following transformations preserve
all three properties of a jointree for DAG G:
r Add variable: We can add a variable X of G to a cluster Ci as long as Ci has a neighbor
Cj that contains X.
r Merge clusters: We can merge two neighboring clusters Ci and Cj into a single cluster
Ck = Ci ∪Cj, where Ck will inherit the neighbors of Ci and Cj.
r Add cluster: We can add a new cluster Cj and make it a neighbor of an existing cluster
Ci as long as Cj ⊆Ci.
r Remove cluster: We can remove a cluster Cj if it has a single neighbor Ci and Cj ⊆Ci.
The jointree in Figure 9.13(c) results from applying two transformations to the jointree
in Figure 9.13(b). The ﬁrst transformation is adding the cluster C4 = AB. The second
transformation is adding variable D to cluster C3.
These transformations have practical applications. For example, the addition of variable
D as indicated previously allows the jointree algorithm to compute the marginal over
variables CDE. This marginal will not be computed when the algorithm is applied to the
jointree in Figure 9.13(b). Moreover, by merging two clusters we eliminate the separator
connecting them from the jointree. Recall that the Shenoy-Shafer algorithm of Chapter 7
needs to create factors over each separator in the jointree. Hence, this transformation can
be used to reduce the space requirements of the algorithm. This will typically increase the
running time as the merging of clusters will typically lead to creating a larger cluster, and
the algorithm’s running time is exponential in the size of clusters. This transformation can
therefore be the basis for time-space tradeoffs using the jointree algorithm.
We say that a jointree for DAG G is minimal if it ceases to be a jointree for G once we
remove a variable from any of its clusters. The jointree in Figure 9.13(c) is therefore not
minimal. As we mentioned previously, we may want to work with a nonminimal jointree
as it allows us to obtain additional quantities that may not be obtainable from a minimal
jointree (see also Section 10.3 for another application).
9.4.1 From elimination orders to jointrees
We now discuss a polytime, width-preserving algorithm for generating a jointree from an
elimination order. The algorithm consists of two parts, where the ﬁrst part is concerned

218
MODELS FOR GRAPH DECOMPOSITION
B
A
C
D
E
F
B
A
C
D
E
F
(a) DAG
(b) Moral graph
Figure 9.14: A DAG and its moral graph. The elimination order F, A, B, C, D, E has width 2 with respect
to the DAG and its moral graph.
with the construction of jointree clusters and the second part is concerned with connecting
them into a tree structure that satisﬁes the jointree property.
Constructing the clusters
Given a DAG G and a corresponding elimination order π, we show in this section how
to generate a set of clusters with the following properties. First, the size of every cluster
is ≤width(π, G) + 1. Second, the clusters satisfy Conditions 1 and 2 of Deﬁnition 9.13
for a jointree. Third, the clusters can be connected into a tree structure that satisﬁes
Condition 3 of Deﬁnition 9.13 (the jointree property). However, the connection algorithm
is given in the following section.
The method for generating such clusters is relatively simple.
Theorem 9.5. Let C1, . . . , Cn be the cluster sequence that results from applying the
elimination order π to the moral graph Gm of DAG G. Every family of DAG G must be
contained in some cluster of the sequence.
■
The clusters C1, . . . , Cn will therefore satisfy the ﬁrst two conditions of a jointree. More-
over, the size of each cluster must be ≤width(π, G) + 1 by Deﬁnition 9.5 of width.
Hence, if we use these clusters to construct the jointree, the width of the jointree will
be ≤width(π, G).
Consider now Figure 9.14, which depicts a DAG G, its moral graph Gm, and an
elimination order π = F, A, B, C, D, E that has width 2. The cluster sequence induced
by applying this order to the moral graph Gm is
C1 = FDE, C2 = ABC, C3 = BCD, C4 = CDE, C5 = DE, C6 = E.
(9.1)
Every family of DAG G is contained in one of these clusters.
As we see later, the cluster sequence induced by an elimination order can always
be connected into a tree structure that satisﬁes the jointree property. This is due to the
following result.
Theorem 9.6. Let C1, . . . , Cn be the cluster sequence induced by applying elimination
order π to graph G. For every i < n, the variables Ci ∩(Ci+1 ∪. . . ∪Cn) are contained
in some cluster Cj where j > i. This is known as the running intersection property of the
cluster sequence.
■

9.4 JOINTREES
219
B
A
E
C
D
Figure 9.15: A graph.
Consider again the cluster sequence in (9.1). We have C1 ∩(C2 ∪. . . ∪C6) = DE.
Moreover, the set DE is contained in cluster C4. The same property can be veriﬁed for
the remaining clusters in this sequence.
The cluster sequence induced by an elimination order may contain nonmaximal clusters,
that is, ones that are contained in other clusters of the sequence. Note, however, that it
is impossible for a cluster Ci to be contained in another cluster Cj that follows it in the
sequence, j > i (see Exercise 9.18). Yet it is possible that a cluster Ci will be contained
in an earlier cluster Cj, j < i. In this case, we can remove cluster Ci from the sequence
but we must reorder the sequence if we want to maintain the running intersection property
(which is needed when connecting the clusters into a jointree at a later stage).
Consider, for example, the following cluster sequence that is induced by applying the
elimination order π = A, B, C, D, E to the graph in Figure 9.15:
C1 = ACD,
C2 = BC,
C3 = CD,
C4 = DE,
C5 = E.
Cluster C3 is nonmaximal as it is contained in cluster C1. If we simply remove this cluster,
we get the sequence
C1 = ACD,
C2 = BC,
C4 = DE,
C5 = E,
which does not satisfy the running intersection property since C1 ∩(C2 ∪C4 ∪C5) = CD
is not contained in any of the clusters that follow C1. However, we can recover this property
if we move cluster C1 to the position that cluster C3 used to assume:
C2 = BC,
C1 = ACD,
C4 = DE,
C5 = E.
More generally, we have the following result.
Theorem 9.7. Let C1, . . . , Cn be a cluster sequence satisfying the running intersection
property. Suppose that Ci ⊇Cj where i < j and i is the largest index satisfying this
property. The following cluster sequence satisﬁes the running intersection property:2
C1, . . . , Ci−1, Ci+1, . . . , Cj−1, Ci, Cj+1, . . . , Cn.
■
That is, we can remove the nonmaximal cluster Cj from the sequence and still maintain
the running intersection property as long as we place cluster Ci in its position.
Assembling the jointree
Now that we have a cluster sequence that satisﬁes the ﬁrst two conditions of a jointree,
we will see how we can connect these clusters into a tree structure that satisﬁes the
2 It is impossible to have another Ck such that i < k ̸= j and Ci ⊇Ck, assuming that no cluster in the sequence can
be contained in a following cluster. This last property continues to hold after removing a nonmaximal cluster, as
suggested by the theorem.

220
MODELS FOR GRAPH DECOMPOSITION
B
A
C
D
E
F
B
A
C
D
E
F
B
A
C
D
E
F
FDE
ABC
BCD
CDE
Graph G
Moral graph Gm
Filled-in graph Gπ
Jointree
Figure 9.16: The jointree is constructed by EO2JT 1 using order π = F, A, B, C, D, E. The clusters
of the jointree are ordered from top to bottom so they satisfy the running intersection property.
Algorithm 23 EO2JT 1(G,π)
input:
G:
DAG
π:
elimination order for DAG G
output: jointree for DAG G with width equal to width(π, G)
main:
1: Gm←moral graph of DAG G
2: C1, . . . , Cn←cluster sequence induced by applying order π to graph Gm
3: ←a cluster sequence resulting from removing nonmaximal clusters from sequence
C1, . . . , Cn according to Theorem 9.7
4: return jointree assembled from cluster sequence  according to Theorem 9.8
third condition of a jointree (i.e., the jointree property). We provide two methods for
this purpose, each with different properties. The ﬁrst method is based on the following
result.
Theorem 9.8. Let C1, . . . , Cn be a cluster sequence that satisﬁes the running intersection
property. The following procedure will generate a tree of clusters that satisﬁes the jointree
property: Start ﬁrst with a tree that contains the single cluster Cn. For i = n −1, . . . , 1,
add cluster Ci to the tree by connecting it to a cluster Cj that contains Ci ∩(Ci+1 ∪
. . . ∪Cn).
■
Algorithm 23, EO2JT 1, provides pseudocode for converting an elimination order into
a jointree of equal width based on the method described here. Figure 9.16 depicts a jointree
constructed by EO2JT 1. We can verify that this tree satisﬁes the conditions of a jointree
(Deﬁnition 9.13). Figure 9.17 contains another example of a DAG and its moral graph,
and Figure 9.18 depicts a jointree for this DAG constructed also using EO2JT 1.
The previous method for constructing jointrees was based on Theorems 9.5, 9.6, and 9.7
for constructing clusters, and on Theorem 9.8 for connecting them into a tree structure.
A more general method for constructing jointrees is based on the following result, which
bypasses Theorems 9.6 and 9.7 when constructing clusters and replaces the connection
method of Theorem 9.8 by a more general method.

9.4 JOINTREES
221
A
B
C
D
E
F
G
H
A
B
C
D
E
F
G
H
(a) DAG G
(b) Moral graph Gm
Figure 9.17: A DAG and its moral graph.
ABD
ACE
DFG
ADF
AEF
EFH
ABD
ACE
DFG
ADF
AEF
EFH
Figure 9.18: A jointree for the network in Figure 9.17 (with two different layouts) induced by the elimination
order B, C, G, D, A, E, F, H. The clusters of the jointree on the left are ordered from top to bottom so
they satisfy the running intersection property.
Algorithm 24 EO2JT 2(G,π)
input:
G:
DAG
π:
elimination order for DAG G
output: jointree for DAG G with width equal to width(π, G)
main:
1: Gm←moral graph of DAG G
2: C1, . . . , Cn←cluster sequence induced by applying order π to graph Gm
3: S←set of maximal clusters in the sequence C1, . . . , Cn
4: return Jointree assembled from clusters in S according to Theorem 9.9
Theorem 9.9. Let C1, . . . , Cn be the set of maximal clusters that result from applying an
elimination order to a graph. Deﬁne a cluster graph by including an edge Ci−Cj between
every pair of clusters and let |Ci ∩Cj| be the cost of this edge. A tree that spans these
clusters satisﬁes the jointree property if and only if it is a maximum spanning tree of the
cluster graph.
■
Algorithm 24, EO2JT 2, provides pseudocode for a method EO2JT 2 that converts an
elimination order into a jointree of equal width using this method. Figure 9.19 provides
an example with two jointrees constructed for the same set of clusters using EO2JT 2.

222
MODELS FOR GRAPH DECOMPOSITION
ABC
CE
BCD
2
1
BC
C
C
1
ABC
CE
BCD
C
BC
ABC
CE
BCD
BC
C
(a) Cluster graph
(b) Maximal spanning tree (c) Maximal spanning tree
Figure 9.19: Connecting clusters into a tree that satisﬁes the jointree property.
Algorithm EO2JT 2 provides an opportunity to generate jointrees that minimize some
secondary cost measure. In particular, suppose that each separator has a secondary cost,
depending for example on the cardinalities of variables involved. Suppose further that our
goal is to generate a jointree that minimizes this secondary cost measure, which we assume
is obtained by adding up the secondary costs of separators in a jointree. Algorithms for
generating spanning trees can indeed be extended so they not only return the maximal
spanning tree as described by Theorem 9.9 but also one that minimizes the secondary cost
measure.
We note here that a maximum spanning tree can be computed using either Kruskal’s
algorithm, which can be implemented in O(n2 log n) time, or Prim’s algorithm, which
can be implemented in O(n2 + n log n) time, where n is the number of clusters in the
jointree. Standard implementations of these algorithms require O(n2) space as they need
to represent all possible edges and their weights. Even though algorithm EO2JT 2 is more
general than algorithm EO2JT 1, the latter is more commonly used in practice as it can
be implemented more efﬁciently. In particular, the assembly of a jointree using EO2JT 1
can be implemented in O(n2) time and O(n) space.
Now that we have procedures for converting between elimination orders and jointrees
in both directions while preserving width, we can deﬁne the treewidth of DAG G as
the width of its best jointree. This also shows a strong connection between the com-
plexity of the variable elimination algorithm of Chapter 6 and the jointree algorithm of
Chapter 7.
9.4.2 From dtrees to jointrees
The following theorem provides a polytime, width-preserving method for converting a
dtree into a jointree.
Theorem 9.10. The clusters of a dtree satisfy the jointree property.
■
Since there is a one-to-one correspondence between the families of DAG G and the
clusters of leaf nodes in a dtree, Theorem 9.10 shows that the clusters of a dtree are im-
mediately a jointree with the same width. Figure 9.20 depicts a DAG and a corresponding
dtree with its clusters explicated. One can verify that this tree satisﬁes all three conditions
of a jointree. Note however that we have many nonmaximal clusters in this jointree. In
fact, by applying the jointree transformations discussed previously, we can reduce this
jointree to either of these in Figure 9.19. Even though the jointree induced by a dtree is
usually not minimal, it is special in two ways:
r Each cluster has at most three neighbors, leading to a binary jointree.
r The tree contains exactly 2n −1 clusters, where n is the number of nodes in DAG G.

9.4 JOINTREES
223
B
A
C
D
E
A
AB
AC
BCD
CE
BC
ABC
AB
BC
(a) DAG
(b) Dtree
Figure 9.20: A dtree with its clusters explicated (see also Figure 9.22).
These properties are important for the Shenoy-Shafer algorithm given in Chapter 7, which
needs a jointree with these properties to guarantee the complexity bounds discussed in
that chapter.
9.4.3 Jointrees as a factorization tool
We close our discussion of jointrees by an important property, showing how we can read
independence information from the jointree which are guaranteed to hold in the DAG for
which it was constructed.
Theorem 9.11. Let G be a DAG and let (T , C) be a corresponding jointree. For every
edge i−j in the jointree, let Z = Ci ∩Cj, X be the union of clusters on the i-side of edge
i−j, and Y be the union of clusters on the j-side of edge i−j. Then X \ Z and Y \ Z are
d-separated by Z in DAG G.
■
Consider for example the jointree in Figure 9.18 and the clusters Ci = ADF and
Cj = AEF, leading to Z = AF, X = ABDFG and Y = ACEFH. Theorem 9.11
claims that X \ Z = BDG is d-separated from Y \ Z = CEH by Z = AF in the DAG of
Figure 9.17(a). This can be veriﬁed since all paths between BDG and CEH are indeed
blocked by AF.
The importance of Theorem 9.11 is due mostly to the following result, which shows
how a jointree of DAG G can be used to factor any distribution that is induced by G.
Theorem 9.12. Given a probability distribution Pr(X) that is induced by a Bayesian
network and given a corresponding jointree, the distribution can be expressed in the
following form:
Pr(X) =

jointree node i
Pr(Ci)

jointree edge i−j
Pr(Ci ∩Cj)
.
(9.2)
■
This factored form of a probability distribution will prove quite useful in some of the
approximate inference methods we discuss in Chapter 14. For an example, consider the
jointree in Figure 9.18. According to Theorem 9.12, any probability distribution induced
by the DAG in Figure 9.17(a) can be expressed as:
Pr(A, B, C, D, E, F, G, H)
= Pr(DFG)Pr(ACE)Pr(ADF)Pr(AEF)Pr(ABD)Pr(EFH)
Pr(DF)Pr(AE)Pr(AF)Pr(AD)Pr(EF)
.

224
MODELS FOR GRAPH DECOMPOSITION
B
A
C
D
E
A
AB
AC
BCD
CE
A
AB
AC
BCD
CE
(a) DAG
(b) Dtree
(c) Dtree
Figure 9.21: A DAG with two corresponding dtrees. Each leaf node in a dtree is labeled with its corre-
sponding family from the DAG.
9.5 Dtrees
We discussed dtrees in Chapter 8, where we showed the role they play in driving the
recursive conditioning algorithm. In that chapter, a dtree was deﬁned for a Bayesian
network as a full binary tree whose leaf nodes are in one-to-one correspondence with the
network CPTs. Here we provide a more general deﬁnition that is based on DAGs instead
of Bayesian networks.
Deﬁnition 9.14. A dtree for DAG G is a pair (T, vars) where T is a full binary tree
whose leaf nodes are in one-to-one correspondence with the families of DAG G, and
vars(.) is a function that maps each leaf node L in the dtree to the corresponding family
vars(L) of DAG G.
■
Figure 9.21 depicts a DAG with two of its dtrees. Dtrees can also be deﬁned for undirected
graphs, as given in Exercise 9.26.
Recall that a full binary tree is a binary tree where each node has 2 or 0 children. Hence,
a full binary tree that has n leaf nodes must also have n −1 nonleaf nodes. Following
standard conventions on trees, we will often not distinguish between a tree node T and
the tree rooted at that node, that is, T will refer both to a tree and the root of that tree.
We also use T p, T l, and T r to refer to the parent, left child, and right child of node T ,
respectively. Finally, an internal dtree node is one that is neither leaf (no children) nor
root (no parent).
We extend the function vars(T ) to nonleaf nodes in a dtree as follows:
vars(T )
def= vars(T l) ∪vars(T r).
We also recall the following deﬁnitions from Chapter 8:
cutset(T )
def=
(vars(T l) ∩vars(T r)) \ acutset(T )
for nonleaf node T
acutset(T )
def=

T ⋆ancestor of T
cutset(T ⋆)
context(T )
def=
vars(T ) ∩acutset(T )
cluster(T )
def=
vars(T ),
if T is a leaf node
cutset(T ) ∪context(T ), otherwise.
The width of a dtree is deﬁned as the size of its largest cluster minus one. Figure 9.22
depicts the cutsets, contexts, and clusters of a dtree.
The cutsets, contexts, and clusters associated with dtree nodes satisfy some important
properties that were used in Chapter 8 and are appealed to in various proofs in this chapter.

9.5 DTREES
225
A
AB
AC
BCD
CE
BC
A
A
AB
AC
BCD
CE
BC
BC
AB
A
AB
AC
BC
C
A
AB
AC
BCD
CE
BC
ABC
AB
BC
(a) Cutsets
(b) Contexts
(c) Clusters
Figure 9.22: The cutsets, contexts, and clusters of a dtree. Note that for a nonleaf node, the cluster is
the union of cutset and context.
Algorithm 25 EO2DT(G,π)
input:
G:
DAG
π:
elimination order for G
output: dtree for DAG G having width ≤width(π, G)
main:
1: ←leaf dtree nodes corresponding to families of DAG G
2: for i = 1 to length of order π do
3:
T1, . . . , Tn←trees in  which contain variable π(i)
4:
remove T1, . . . , Tn from 
5:
add T = COMPOSE(T1, . . . , Tn) to 
6: end for
7: return dtree which results from composing the dtrees in 
Theorem 9.13. The following hold for dtree nodes:
(a) cutset(T ) ∩context(T ) = ∅for nonleaf node T
(b) context(T ) ⊆cluster(T p)
(c) cutset(T p) ⊆context(T )
(d) cutset(T1) ∩cutset(T2) = ∅for nonleaf nodes T1 ̸= T2
(e) context(T ) = cluster(T ) ∩cluster(T p)
(f) context(T l) ∪context(T r) = cluster(T ).
■
Of particular importance are Property (a), which says that the cutset and context of a
node are disjoint, and Property (d), which says that cutsets of various nodes are mutually
disjoint.
We next treat three different subjects relating to dtrees. First, we provide a polytime,
width-preserving algorithm for converting an elimination order into a dtree, leaving the
conversion of a jointree to a dtree to Exercise 9.21. We then provide a more direct,
heuristic method for constructing low-width dtrees based on the concept of hypergraph
partitioning. We ﬁnally treat the subject of balancing dtrees, which was critical to some
of the complexity results in Chapter 8.
9.5.1 From elimination orders to dtrees
In this section, we provide a polytime algorithm for converting an elimination order
into a dtree of no greater width. The method, EO2DT, is given in Algorithm 25 and

226
MODELS FOR GRAPH DECOMPOSITION
A
B
F
E
D
C
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
A
AB
ABE
AC
BCD
DF
 D,F,E,C,B,A
π=
3
4
5
6
1
2
Σ
Σ
Σ
Σ
Σ
Σ
Σ
π= F,E,A,B,C,D
3
Σ
6
Σ
5
Σ
4
Σ
2
Σ
1
Σ
Σ
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
A
AB
BCD
DF
AC
ABE
Figure 9.23: A step-by-step construction of dtrees for the included DAG using algorithm EO2DT and
two different elimination orders. Each step i depicts the trees present in  of EO2DT after processing
variable π(i).
is based on the COMPOSE operator, which takes a set of binary trees T1, . . . , Tn and
connects them (arbitrarily) into a single binary tree COMPOSE(T1, . . . , Tn). EO2DT starts
initially by constructing a set of dtrees, each containing a single node and corresponding
to one of the families in DAG G. It then considers variables π(1), π(2), . . . , π(n) in
that order. Each time EO2DT considers a variable π(i), it composes all binary trees that
mention variable π(i). It ﬁnally returns the composition of all remaining binary trees.
Two examples of this algorithm are depicted in Figure 9.23. In the ﬁrst example, we use
the order π = D, F, E, C, B, A of width 3 to generate a dtree of width 2. In the second
example, we use the elimination order π = F, E, A, B, C, D of width 2 and generate a
dtree of the same width.
Theorem 9.14. Let G be a DAG and let π be a corresponding elimination order of width
w. The call EO2DT(G, π) returns a dtree of width ≤w for DAG G.
■

9.5 DTREES
227
Algorithm 26 HG2DT(H, E)
input:
H:
hypergraph
E:
hyperedges
output: dtree whose leaves correspond to the nodes of H
main:
1: T ←new dtree node
2: if H has only one node NF then
3:
vars(T )←F
4: else
5:
H l, H r←two approximately equal parts of H that minimize E⋆below
6:
E⋆←hyperedges that are not in E, yet include nodes in H l and in H r
7:
T l←HG2DT(H l, E⋆∪E)
8:
T r←HG2DT(H r, E⋆∪E)
9: end if
10: return dtree T
We now have width-preserving procedures for converting between elimination orders
and dtrees in both directions, allowing us to deﬁne the treewidth of DAG G as the width of
its best dtree. This also shows the strong connection between the complexity of variable
elimination from Chapter 6 and the complexity of recursive conditioning from Chapter 8.
9.5.2 Constructing dtrees by hypergraph partitioning
In this section, we provide a heuristic method for constructing low-width dtrees based on
hypergraph partitioning. Given our algorithms for converting dtrees to elimination orders
and jointrees, the presented method will be immediately available for the construction of
low-width elimination orders and jointrees.
A hypergraph is a generalization of a graph in which an edge is permitted to connect
an arbitrary number of nodes rather than exactly two. The edges of a hypergraph are
referred to as hyperedges. The problem of hypergraph partitioning is to ﬁnd a way to
split the nodes of a hypergraph into k approximately equal parts such that the number of
hyperedges connecting vertices in different parts is minimized. Hypergraph partitioning
algorithms are outside the scope of this chapter, so we restrict our discussion to how they
can be employed in constructing dtrees.
Generating a dtree for a DAG using hypergraph partitioning is fairly straightforward.
The ﬁrst step is to express the DAG G as a hypergraph H:
r For each family F in DAG G, we add a node NF to H.
r For each variable V in DAG G, we add a hyperedge to H that connects all nodes NF such
that V ∈F.
An example of this method is depicted in Figure 9.24. Notice that any full binary tree
whose leaves correspond to the vertices of hypergraph H is a dtree for our DAG. This
observation allows us to design a simple recursive procedure HG2DT using hypergraph
partitioning to produce a dtree, which is given in Algorithm 26.
Algorithm HG2DT, which is called initially with E = ∅, attempts to minimize the cutset
of each dtree node T it constructs. To see this, observe that every time we partition the

228
MODELS FOR GRAPH DECOMPOSITION
B
A
C
D
A
B
ABC
CD
A
B
C
D
A
B
ABC
CD
A
B
D
C
(a) From DAG to hypergraph
(b) Partitioned hypergraph
Figure 9.24: Partitioning a hypergraph.
hypergraph H into H l and H r, we attempt to minimize the number of hyperedges in
E⋆. By construction, these hyperedges correspond to DAG variables that are shared by
families in H l and those in H r (and have not already been cut by previous partitions).
Hence, by attempting to minimize the hyperedges E⋆, we are actually attempting to
minimize the cutset associated with dtree node T . Note that we do not make any direct
attempt to minimize the width of the dtree, which depends on both the cutset and context
of a dtree node. However, we see next that cutset minimization combined with a balanced
partition is a good heuristic for width minimization. In particular, we next show that this
method can be viewed as a relaxation of a method that is guaranteed to construct a dtree of
width ≤4w + 1 if the DAG has treewidth w.
Suppose now that every dtree node T satisﬁes the following conditions, which are
guaranteed to hold for some dtree if the DAG has treewidth w:
r The cutset of T has no more than w + 1 variables.
r No more than two thirds of the variables in context(T ) appear in either vars(T l) or
vars(T r).
Under these conditions, the dtree is guaranteed to have a width ≤4w + 1 (see Exer-
cise 9.23). Note, however, that HG2DT does not generate dtrees that satisfy these condi-
tions. Instead, it tries to minimize the cutset without ensuring that its size is bounded by
w + 1. Moreover, it generates balanced partitions without necessarily ensuring that no
more than two thirds of the context is in either part. Hence, HG2DT can be viewed as a
relaxation of a method that constructs dtrees under these conditions.
9.5.3 Balancing dtrees
In this section, we present an algorithm for balancing a dtree while increasing its width
by no more than a constant factor. The algorithm is similar to EO2DT except that the
composition process is not driven by an elimination order. Instead, it is driven by applying
an operation known as tree contraction, which is explained next.
The operation of tree contraction is applied to a directed tree, which is a tree identiﬁed
with a root. Each node in the tree has a single parent, which is the neighbor of that node
that is closest to the root (dtrees are directed trees). Contraction simply absorbs some
of the tree nodes into their neighbors to produce a smaller tree. To absorb node N1 into
node N2 is to transfer the neighbors of N1 into neighbors of N2 and to remove node N1

9.6 TRIANGULATED GRAPHS
229
B
D
F
H
J
L
N
O
A
C
E
G
I
K
M
AB
CD
EF
GH
IJ
KL
MNO
ABCD
EFGH
IJ
KL
MNO
rake
compress
Figure 9.25: Demonstrating the tree-contraction operation.
from the tree. Contraction works by applying a rake operation to the tree, followed by a
compress operation. The rake operation is simple: it absorbs each leaf node into its parent.
The compress operation is more involved: it identiﬁes all maximal chains N1, N2, . . . , Nk
and then absorbs Ni into Ni+1 for odd i. The sequence N1, N2, . . . , Nk is a chain if Ni+1
is the only child of Ni for 1 ≤i < k and if Nk has exactly one child and that child is
not a leaf. Contraction is a general technique that has many applications. Typically, each
tree node N will have an application-speciﬁc label, label(N). When node N1 is absorbed
into its neighbor N2, the label of N2 is updated to label(N1) ⋆label(N2) where “⋆” is a
label-speciﬁc operation.
Figure 9.25 depicts an example where contraction is applied to a tree where the labels
of nodes are strings and the corresponding operation is string concatenation. The main
property of contraction is that any tree can be reduced to a single node by only applying
contraction O(log n) times, where n is the number of tree nodes.
We use the contraction operation to balance a dtree T as follows. First, we label each
nonleaf node in T with the empty dtree. Second, we label each leaf node of T with itself.
We then choose the label-speciﬁc operation to be COMPOSE as deﬁned in Section 9.5.1.
Finally, we apply contraction successively to the dtree until it is reduced to a single node
and return the label of the ﬁnal node. We refer to this algorithm as BAL DT.
Theorem 9.15. Let T be a dtree having n nodes and a largest context size of w. BAL DT(T )
will return a dtree of height O(log n), a cutset width ≤w, a context width ≤2w, and a
width ≤3w −1.
■
Given our method for converting dtrees to jointrees, algorithm BAL DT can therefore
be used to construct balanced jointrees as well. A balanced jointree with n clusters is one
that has a root node r, where the path from root r to any other node has length O(log n).
9.6 Triangulated graphs
We discussed three models of graph decomposition in this chapter – elimination orders,
jointrees, and dtrees – and measured the quality of each model by a corresponding notion
of width. We provided an equivalence between these models in the sense that optimizing
the width of one model is equivalent to optimizing the width of another. We also showed
that the width of these models cannot be less than the treewidth of the correspond-
ing DAG.
These equivalences can be understood at a more basic level by showing that the
construction of each of these models corresponds to constructing a decomposable graph,

230
MODELS FOR GRAPH DECOMPOSITION
F
G
H
D
C
E
J
A
B
F
G
H
D
C
E
J
A
B
(a) Graph
(b) Triangulated graph
Figure 9.26: A graph and one of its triangulations.
and that minimizing the width of these models corresponds to minimizing the treewidth
of this decomposable graph.
Intuitively, a decomposable graph is one for which we can construct an optimal elimi-
nation order in polytime and, hence, determine its treewidth in polytime. Decomposable
graphs are characterized in a number of different ways in the literature. One of the more
common characterizations is using the notion of perfect orders.
Deﬁnition 9.15. The elimination order π is perfect for graph G if and only if it does
not add any ﬁll-in edges when applied to G: G = Gπ.
■
Theorem 9.16. If graph G admits a perfect elimination order π, then the order π can be
identiﬁed in polytime and treewidth(G) = width(π, G).
■
Not every graph admits a perfect elimination order. However, we can always add edges
to a graph G so it admits a perfect elimination order. In particular, given an elimination
order π the ﬁlled-in graph Gπ is guaranteed to admit π as a perfect order (see Lemma 9.1
on Page 234).
The following is another characterization of decomposable graphs.
Deﬁnition 9.16. A graph is said to be triangulated precisely when every cycle of
length ≥4 has a chord, that is, an edge connecting two nonconsecutive nodes in the
cycle. Triangulated graphs are also called chordal graphs.
■
Figure 9.26(a) depicts a graph that is not triangulated, as it contains cycles of length four or
more with no chords (e.g., the cycle A, B, E, H, C). Figure 9.26(b) depicts a triangulated
graph that has no such cycles.
Theorem 9.17. A graph is triangulated if and only if it admits a perfect elimination
order.
■
Recall that a ﬁlled-in graph Gπ admits π as a perfect elimination order. Therefore, the
graph Gπ must be triangulated. This suggests that one can always triangulate a graph by
adding edges to it. Figure 9.27 depicts a graph and two triangulations that result from
adding the ﬁll-in edges of two corresponding elimination orders.
We now have the following equivalence.
Theorem 9.18. The following properties hold:
r Given a triangulation G⋆for graph G, we can construct in polytime an elimination order
π with width(π, G) ≤treewidth(G⋆).

BIBLIOGRAPHIC REMARKS
231
B
A
C
D
E
F
B
A
C
D
E
F
B
A
C
D
E
F
(a) Graph
(b) Triangulation Gπ1
(c) Triangulation Gπ2
Figure 9.27: π1 = A, B, C, D, E, F and π2 = C, A, B, D, E, F .
r Given an elimination order π for graph G, we can construct in polytime a triangulation
G⋆for graph G with treewidth(G⋆) ≤width(π, G).
■
Therefore, ﬁnding an optimal triangulation for a graph is equivalent to constructing an
optimal elimination order for that graph (jointrees and dtrees as well).
The classical treatment of jointrees makes a direct appeal to triangulated graphs. For
example, it is quite common in the literature to describe the method of constructing jointree
clusters as a method that starts by triangulating the moral graph of a Bayesian network
and then identifying its maximal cliques as the jointree clusters. This is equivalent to the
description we gave in Section 9.4.1 due to the following result.
Theorem 9.19. Let C1, . . . , Cn be the cluster sequence induced by applying elimination
order π to graph G. The maximal clusters in C1, . . . , Cn are precisely the maximal cliques
of the triangulated graph Gπ.
■
Hence, the discussion of triangulation is not necessary for a procedural description of the
jointree construction algorithm, since the cluster sequence already identiﬁes the maximal
cliques of a triangulated graph.
Bibliographic remarks
For an introductory treatment of graphs and some of their algorithms, such as maximal
spanning trees, see Cormen et al. [1990].
The notion of treewidth, whose computation is NP-complete [Arnborg et al., 1987], was
introduced in Robertson and Seymour [1986] based on the notion of tree decompositions,
which are jointrees for undirected graphs (as deﬁned in Exercise 9.25).3 Other deﬁnitions
of treewidth based on triangulated graphs are discussed in Gavril [1974] and Golumbic
[1980], and a more recent overview of graph theoretic notions that are equivalent to
treewidth is given in Bodlaender [1998]. The degeneracy lower bound on treewidth is
attributed to Szekeres and Wilf [1968] and Lick and White [1970]. The contraction
degeneracy lower bound was introduced and shown to be NP-hard in Bodlaender et al.
[2004; 2006]. The heuristic version of contraction degeneracy referred to as MMD+
(min-d) was developed independently by Bodlaender et al. [2004] and, as minor-min-
width, by Gogate and Dechter [2004]. A variety of other lower bounds on treewidth are
3 See also the related notion of branch decompositions [Robertson and Seymour, 1991; 1995] as deﬁned in Exer-
cise 9.26.

232
MODELS FOR GRAPH DECOMPOSITION
studied by Bodlaender et al. [2005a] and Koster et al. [2005]. A survey of treewidth and
related topics is given in Bodlaender [2005], including exact algorithms, approximation
algorithms, upper bounds, and preprocessing rules; see also Bodlaender [2007; 2006].
Our treatment of elimination orders is based on Bertele and Brioschi [1972], who
identify the notion of width using the term dimension. Our discussion of elimination
preﬁxes and the associated preprocessing rules is based on Bodlaender et al. [2005b],
which has its roots in Arnborg and Proskurowski [1986]. Elimination heuristics are
discussed in Kjaerulff [1990]. The ﬁrst algorithm to employ depth-ﬁrst branch-and-bound
on the elimination order search space was given in Gogate and Dechter [2004], who used
the MMD+(min-d) lower bound on treewidth. The ﬁrst best-ﬁrst search algorithm for
ﬁnding optimal elimination orders was given in Dow and Korf [2007], who also used the
MMD+(min-d) lower bound. Our treatment of this algorithm is also based on Dow and
Korf [2008].
The construction of jointrees according to Theorem 9.8 is based on Lauritzen [1996].
The construction of jointrees according to Theorem 9.9 is due to Jensen and Jensen
[1994] and Shibata [1988]. The notion of a dtree was introduced in Darwiche [2001] and
is closely related to branch decomposition in the graph-theoretic literature [Robertson and
Seymour, 1991; 1995] (see Exercise 9.26). The construction of dtrees using hypergraph
decomposition was introduced in Darwiche and Hopkins [2001] and its theoretical basis
was given in Hopkins and Darwiche [2002]. For a treatment of the relation between
elimination orders and triangulation, see Rose [1970] and Golumbic [1980].
9.7 Exercises
9.1. Construct a jointree for the DAG in Figure 9.28 using the elimination order π = A, G, B,
C, D, E, F and Algorithm 23.
9.2. Construct a jointree for the DAG in Figure 9.28 using the elimination order π = A, G, B,
C, D, E, F and Algorithm 24. In case of multiple maximum spanning trees, show all of them.
9.3. Convert the dtree in Figure 9.28 to a jointree and then remove all nonmaximal clusters.
9.4. Construct a total elimination order that is consistent with the partial elimination order induced
by the dtree in Figure 9.28. When the relative order of two variables is not ﬁxed by the dtree,
place them in alphabetic order.
9.5. Construct a dtree for the DAG in Figure 9.28 using the elimination order π = A, G, B,
C, D, E, F and Algorithm 25. What is the width of this order? What is the width of the
generated dtree?
A
B
C
D
E
F
G
C
EFG
CDF
BCE
B
ABD
A
1
3
2
5
4
6
7
8
9
10
11
12
13
Figure 9.28: A network and a corresponding dtree.

9.7 EXERCISES
233
9.6. Consider the networks in Figure 6.6(d) and Figure 6.6(e) on Page 142. Show that the treewidth
of these networks is 3. Hint: Use the preprocessing rules for generating optimal elimination
preﬁxes.
9.7. Show that the leaf nodes of a Bayesian network represent a preﬁx for an optimal elimination
order.
9.8. Show that the root nodes of a Bayesian network, which have a single child each, represent a
preﬁx for an optimal elimination order.
9.9. Show that the Islet and Twig rules are complete for graphs with treewidth 1; that is, they are
sufﬁcient to generate optimal elimination orders for such graphs.
9.10. Show that the Islet, Twig, and Series rules are complete for graphs with treewidth 2; that is,
they are sufﬁcient to generate optimal elimination orders for such graphs.
9.11. Show that the min-degree heuristic is optimal for graphs with treewidth ≤2.
9.12. Show that the degree of a graph (Deﬁnition 9.7) is a lower bound on the treewidth of the
graph.
9.13. Consider the jointree for an undirected graph as deﬁned in Exercise 9.25 and suppose that
the treewidth of an undirected graph is the width of its best jointree (one with lowest width).
Use this deﬁnition of treewidth to show that contracting an edge in a graph does not increase
its treewidth.
9.14. Show that the degeneracy of a graph (Deﬁnition 9.8) can be computed by generating a se-
quence of subgraphs, starting with the original graph, and then generating the next subgraph
by removing a minimum-degree node. The degeneracy is then the maximum degree attained
by any of the generated subgraphs.
9.15. Show the following about Algorithm 20, BFS OEO: At no point during the search can we have
a node (Gρ, ρ, wρ, bρ) on the open list and another node (Gρ′, ρ′, wρ′, bρ′) on the closed
list where Gρ = Gρ′.
9.16. Show that Algorithm 20, BFS OEO, will not choose a node (Gτ, τ, wτ, bτ) from the open list
where τ is a complete yet suboptimal elimination order.
9.17. Prove the correctness of Algorithm 21, JT2EO. One way to prove this is to bound the size
of factors constructed by the elimination Algorithm VE PR from Chapter 6 while using the
elimination order generated by JT2EO.
9.18. Consider a cluster sequence C1, . . . , Cn induced by an elimination order π on graph G.
Show that it is impossible for a cluster Ci to be contained in another cluster Cj where j > i.
9.19. Prove Lemma 9.5.
9.20. Consider a dtree generated from an elimination order π using Algorithm 25. Is it possible
for the order not to be consistent with the dtree according to Deﬁnition 9.12, that is, the total
order π is not compatible with the partial order induced by the dtree. Either show impossibility
or provide a concrete example showing such a possibility.
9.21. Provide a polytime, width-preserving algorithm for converting a jointree into a dtree. Your
algorithm should be direct, bypassing the notion of an elimination order.
9.22. Show the following: Given a probability distribution Pr(X) that is induced by a Bayesian
network and given a corresponding dtree, the distribution can be expressed in the following
form:
Pr(X) =

dtree node T
Pr(cluster(T ))

dtree node T
Pr(context(T ))
.

234
MODELS FOR GRAPH DECOMPOSITION
9.23. Show that a dtree will have width ≤4w + 1 if each of its nodes T satisﬁes the following
conditions:
r
The cutset of T has no more than w + 1 variables.
r
No more than two thirds of the variables in context(T ) appear in either vars(T l) or
vars(T r).
9.24. Use Algorithm BAL DT of Section 9.5.3 to balance the dtree in Figure 9.28.
9.25. Deﬁne a jointree for an undirected graph G as a pair (T , C), where T is tree and C. is a
function that maps each node i in the tree T to a label Ci, called a cluster, that satisﬁes the
following conditions:
r
The cluster Ci is set of nodes in graph G.
r
For every edge X−Y in graph G, the variables X and Y appear in some cluster Ci.
r
The clusters of tree T satisfy the jointree property.
Show that every clique of G must be contained in some cluster of the jointree. Show also that
if G is the moral graph of some DAG G⋆, then (T , C) is also a jointree for DAG G⋆. Note: This
deﬁnition of a jointree is known as a tree decomposition (for graph G) in the graph-theoretic
literature.
9.26. Deﬁne a dtree for an undirected graph G as a pair (T, vars), where T is a full binary tree whose
leaves are in one-to-one correspondence with the graph edges and vars(.) is a function that
maps each leaf node L in the dtree to the variables vars(L) of corresponding edge. Deﬁne
the cutset, context, and cluster as they are deﬁned for dtrees of DAGs. Show that the dtree
clusters satisfy the jointree property. Show also that every clique of G must be contained in
some cluster of the dtree. Note: This deﬁnition of a dtree is known as a branch decomposition
(for graph G) in the graph-theoretic literature.
9.27. Let G be a DAG, Gm its moral graph, and consider a jointree for DAG G with width w. Let
Gd be a graph that results from connecting in Gm every pair of variables that appear in some
jointree cluster. Show that Gd is triangulated and that its treewidth is w.
9.28. Show that the treewidth of a triangulated graph equals the size of its maximal clique minus
one.
9.8 Lemmas
This section contains a number of lemmas that are used in the proofs section.
Lemma 9.1. Let G be a graph, π an elimination order for graph G, and let Gπ be the
corresponding ﬁlled-in graph. We then have:
r π is a perfect elimination order for Gπ.
r The cluster sequence induced by applying order π to graph G is identical to the cluster
sequence induced by applying order π to graph Gπ.
r width(π, G) = width(π, Gπ).
PROOF. The third part follows immediately from the second part given the deﬁnition of
width.
To prove the ﬁrst two parts, let E = E1, . . . , En be the ﬁll-in edges of order π and graph
G, where Ei are the edges added when eliminating node π(i). The basic observation here is
that edges Ei, . . . , En do not mention variables π(1), . . . , π(i). Hence, it sufﬁces to show

9.8 LEMMAS
235
the following: Considering the graph sequences G1, G2, . . . , Gn and Gπ
1 , Gπ
2 , . . . , Gπ
n ,
the only difference between graphs Gi and Gπ
i is that the latter graph has the extra edges
Ei, . . . , En. Since these edges do not mention node π(i), eliminating this node from either
graph Gi or Gπ
i will generate the same cluster. The previous result follows immediately
for i = 1. Suppose now that it holds for graphs Gi and Gπ
i . Eliminating π(i) from graph
Gi will then add edges Ei. Eliminating it from graph Gπ
i will add no edges. Hence, the
only difference between graphs Gi+1 and Gπ
i+1 is that the latter graph has the extra edges
Ei+1, . . . , En.
■
Lemma 9.2. If G⋆is a graph that admits a perfect elimination order π, and if G is the
result of removing some edges from G⋆, then width(π, G) ≤width(π, G⋆).
PROOF. It is sufﬁcient to prove that if C1, . . . , Cn is the cluster sequence induced by
applying order π to graph G and if C⋆
1, . . . , C⋆
n is the cluster sequence induced by
applying order π to graph G⋆, then Ci ⊆C⋆
i for i = 1, . . . , n. The proof is by induction
on the size of elimination order π. The base case holds trivially for n = 1. Suppose now
that it holds for elimination orders of size n −1 and let us show that it holds for orders of
size n. The neighbors that node π(1) has in graph G are clearly a subset of its neighbors
in graph G⋆, hence, C1 ⊆C⋆
1. Let G1 be the result of eliminating node π(1) from graph
G and let G⋆
1 be the result of eliminating the node from graph G⋆. Since π is a perfect
elimination order for G⋆, the neighbors of node π(1) will form a clique in G⋆. Therefore,
every ﬁll-in edge that is added when eliminating node π(1) from graph G is already an
edge in G⋆. Hence, G1 can be obtained by removing some edges from G⋆
1. Moreover,
π(2), . . . , π(n) is a perfect elimination order for graph G⋆
1. By the induction hypothesis,
Ci ⊆C⋆
i for i = 2, . . . , n.
■
Lemma 9.3. The clusters C1, . . . , Cn induced by a perfect elimination order π on graph
G are all cliques in graph G.
PROOF. Let G1 = G, . . . , Gn be the graph sequence induced by applying order π to
graph G. Recall that cluster Ci contains node π(i) and its neighbors in graph Gi. Since
order π is perfect, no edges are added between the neighbors of π(i) when it is being
eliminated. Hence, the neighbors of π(i) form a clique in G, making cluster Ci also a
clique in G.
■
Lemma 9.4. The clusters C1, . . . , Cn induced by a perfect elimination order π on graph
G include all of the maximal cliques in graph G.
PROOF. Suppose that C is some maximal clique in graph G. Let X = π(i) be the variable
in C that appears ﬁrst in the order π. When eliminating X, every node Y ∈C is a neighbor
of X unless Y = X since C is a clique. Hence, C ⊆Ci. Moreover, when eliminating X,
every neighbor Z of X must be in C and, hence, Ci ⊆C for the following reason. If some
neighbor Z is not in C, we have one of two cases. First, Z is not adjacent to every node
in C. We must then add ﬁll-in edges when eliminating X, which is a contradiction as π
is a perfect elimination order. Second, Z is adjacent to every node in C, then C ∪{Z}
is a clique, which is a contradiction with C being a maximal clique. We must then have
C = Ci.
■
Lemma 9.5. Consider a dtree node T and deﬁne vars↑(T ) = ∅if T is a root node;
otherwise, vars↑(T ) = 
T ′ vars(T ′), where T ′ is a leaf dtree node connected to node T

236
MODELS FOR GRAPH DECOMPOSITION
through its parent. We have:
cutset(T ) = vars(T l) ∩vars(T r) \ vars↑(T )
context(T ) = vars(T ) ∩vars↑(T )
cluster(T ) = (vars(T l) ∩vars(T r)) ∪(vars(T l) ∩vars↑(T )) ∪(vars(T r) ∩vars↑(T )).
This implies that X ∈cluster(T ) only if X ∈vars(T ).
PROOF. Left to Exercise 9.19.
■
Lemma 9.6. Let π be an elimination order of DAG G and let  = S1, . . . , Sn be the
families of G. Deﬁne the elimination of variable π(i) from collection  as replacing the
sets Sk ∈ containing π(i) by the set (
k Sk) \ {π(i)}. If we eliminate variables according
to order π, concurrently, from the moral graph Gm of G and from the collection , we
ﬁnd the following: As we are about to eliminate variable π(i), the set (
k Sk) \ {π(i)}
contains exactly the neighbors of π(i) in graph Gm.
PROOF. (sketch). It sufﬁces to show that at every stage of the elimination process, the
following invariant holds: two nodes are adjacent in Gm if and only if they belong to some
set in . This holds initially and it is easy to show that it continues to hold after each
elimination step.
■
Lemma 9.7. When processing variable π(i) in EO2DT, let T = COMPOSE(T1, . . . , Tn)
and let T ⋆be a dtree node that is added in the process of composing trees T1, . . . , Tn. We
then have cluster(T ⋆) ⊆vars(T ) ∩{π(i), . . . , π(n)}.
PROOF. Suppose that a variable X belongs to cluster(T ⋆). Then by Lemma 9.5, X must
either belong to two trees in T1, . . . , Tn or belong to a tree in T1, . . . , Tn and another tree
in  \ {T1, . . . , Tn}. In either case, X cannot belong to {π(1), . . . , π(i −1)} since these
variables have already been processed, so each can belong only to a single tree in .
Therefore, X must belong to π(i), . . . , π(n). Moreover, X must belong to at least one tree
in T1, . . . , Tn. Hence, X must belong to T and X ∈vars(T ) ∩{π(i), . . . , π(n)}.
■
9.9 Proofs
PROOF OF THEOREM 9.1. Suppose we have a clique C of size n in graph G and let
π be an optimal elimination order for G. Let π(i) be the earliest node in order π such
that π(i) ∈C. When π(i) is about to be eliminated, all other nodes in C will be in the
graph. These nodes are also neighbors of node π(i) since C is a clique. Hence, the cluster
Ci induced when eliminating node π(i) must contain clique C. This means that |Ci| ≥
|C|, width(π, G) ≥|C| −1 = n −1, and also treewidth(G) ≥n −1 since the order π is
optimal.
■
PROOF OF THEOREM 9.2. This is known as the Invariance theorem in Bertele and
Brioschi [1972]. We now show the following, which is sufﬁcient to prove the theorem.
Nodes A and B are adjacent in graph Gτ1 but not adjacent in graph G if and only if graph
G contains a path
A, X1, . . . , Xm, B

9.9 PROOFS
237
connecting A and B, where all internal nodes Xi belong to preﬁx τ1. This basically means
that the set of edges that are added between nodes in Gτ1 after elimination preﬁx τ1 depend
only on the set of variables in this preﬁx and are independent of the order in which these
variables are eliminated. This then guarantees Gτ1 = Gτ2.
Let G = G1, . . . , Gn = Gτ1 be the graph sequence induced by eliminating preﬁx τ1.
Suppose now that we have a path A, X1, . . . , Xm, B connecting nodes A and B in graph
G1, as discussed previously. Let Gi be the last graph in the sequence in which the above
path is preserved. Graph Gi+1 must then be obtained by eliminating some variable Xj
from the path. This leads to adding an edge between the two variables before and after
Xj on the path (if one does not already exist). Hence, Gi+1 will continue to have a path
connecting A and B with all its internal nodes belonging to preﬁx τ1. This means that
nodes A and B remain to be connected by such a path after each elimination. It also means
that A and B are adjacent in graph Gτ1.
Suppose now that A and B are adjacent in graph Gτ1 but are not adjacent in the
initial graph G. Let Gi be the ﬁrst graph in the sequence in which variables A and B
became adjacent. This graph must then have resulted from eliminating a variable Xj that
is adjacent to both A and B in Gi−1. This means that variables A and B are connected in
Gi−1 by a path whose internal nodes are in preﬁx τ1. By repeated application of the same
argument for edges A−Xj and B−Xj, it follows that A and B must be connected in G by
a path whose internal nodes are in preﬁx τ1.
■
PROOF OF THEOREM 9.3. For any variable X, we have one of two cases:
1. Variable X ∈vars(T ) for a unique leaf dtree node T . Variable X cannot appear in any cut-
set in this case. Moreover, X ̸∈context(T ) and X ∈vars(T ) \ context(T ) = cluster(T ) \
context(T ). Hence, variable X will be eliminated only at the unique leaf node T .
2. Variable X ∈vars(T ) for multiple leaf nodes T . Variable X cannot be eliminated at any
leaf node T because X ∈vars(T ) implies X ∈context(T ) in this case. Moreover, X must
appear in some cutset since it appears in multiple leaf nodes. By Theorem 9.13(d), it must
appear in a unique cutset and, hence, eliminated at a unique node.
■
PROOF OF THEOREM 9.4. Given Theorem 9.10, the dtree nodes and their associated
clusters form a jointree. Calling Algorithm 21 on this dtree (jointree) generates the same
elimination orders characterized by Theorem 9.4 as long as the root of the dtree is
removed last by Algorithm 21. To see this, let i be a node in the dtree and let j be the
single neighbor of i when i is removed from the dtree by Algorithm 21. It then follows
that j is the parent of i in the dtree and, hence, cluster(i) ∩cluster(j) = context(i) by
Theorem 9.13(e). Hence, when Algorithm 21 removes node i from the dtree, it appends
variables cluster(i) \ context(i) to the elimination order, which are precisely the variables
eliminated at node i in the dtree.
■
PROOF OF THEOREM 9.5. We ﬁrst note that by the deﬁnition of a moral graph, every
family of DAG G is a clique in its moral graph Gm. Let Gπ
m be the ﬁlled-in graph of Gm.
Every clique in the moral graph Gm is then a clique in Gπ
m. Hence, every family of DAG
G is a clique in Gπ
m.
By Lemma 9.1, we have that π is a perfect elimination order for Gπ
m and that applying
this order to graph Gπ
m must induce the cluster sequence C1, . . . , Cn. By Lemma 9.4,
we now have that every maximal clique of Gπ
m must appear in the cluster sequence
C1, . . . , Cn. Hence, every family of DAG G must be contained in some cluster of the
sequence.
■

238
MODELS FOR GRAPH DECOMPOSITION
PROOF OF THEOREM 9.6. Without loss of generality, we assume that π is a perfect
elimination order for graph G. If π is not perfect for G, replace G by its ﬁlled-in version
Gπ. By Lemma 9.1, π is perfect for Gπ and applying it to Gπ will induce the same cluster
sequence C1, . . . , Cn.
Let G = G1, G2, . . . , Gn be the graph sequence induced by elimination order π. For
each i < n, Si = Ci ∩(Ci+1 ∪. . . ∪Cn) is the set of neighbors that variable π(i) has in
graph Gi. Since these neighbors form a clique in Gi+1, the set Si must be contained in
some maximal clique of Gi+1. Since π(i + 1), . . . , π(n) is a perfect elimination order
for Gi+1, the maximal cliques of Gi+1 must all appear in the sequence Ci+1, . . . , Cn by
Lemma 9.4. Hence, the set Si must be contained in some cluster in Ci+1, . . . , Cn and the
running intersection property holds.
■
PROOF OF THEOREM 9.7. Let Ci ∩(Ci+1 ∪. . . ∪Cn) ⊆Ck for some k > i in accor-
dance with the running intersection property. Since Cj ⊆Ci and Cj ⊆(Ci+1 ∪. . . ∪Cn),
we have
Cj ⊆Ci ∩(Ci+1 ∪. . . ∪Cn) ⊆Ck
and, hence, k ≤j since no cluster in the sequence can be contained in a following cluster.
If k < j, i would not be the largest index, as given by the theorem; hence, k = j. This
means that no variable in Cr = Ci \ Cj can appear in Ci+1 ∪. . . ∪Cn. In particular,
no variable in Cr can appear in Cs, for i < s < j. Hence, every cluster Cs continues to
satisfy the running intersection property after Cj is replaced by Ci because the intersection
Cs ∩(Cs+1 ∪. . . ∪Cn) does not change. Clusters other than Cs clearly continue to satisfy
the running intersection property.
■
PROOF OF THEOREM 9.8. Since the sequence C1, . . . , Cn satisﬁes the running inter-
section property, for every cluster Ci, i < n, there must exist a cluster Cj, j > i, that
includes Ci ∩(Ci+1 ∪. . . ∪Cn). Our proof will be by induction on i. For i = n, the
tree consisting of cluster Cn will trivially satisfy the jointree property. Suppose now that
we have assembled a tree of clusters Ti corresponding to the sequence Ci, . . . , Cn that
satisﬁes the jointree property. Let Ti−1 be the tree that results from connecting cluster
Ci−1 to a cluster C in Ti such that Ci−1 ∩(Ci ∪. . . ∪Cn) ⊆C. We now show that tree
Ti−1 must also satisfy the jointree property by showing that the clusters of every variable
form a connected subtree in Ti−1.4 Consider a variable X. By the induction hypothesis,
the clusters containing X in tree Ti must form a connected tree. If X ̸∈Ci−1, then the
clusters of X in Ti−1 will also form a connected tree. If X ∈Ci−1, we have two cases. If
X does not appear in Ti, then the clusters of X in Ti−1 will also form a connected tree. If
X appears in Ti, X ∈C by the deﬁnition of C and the clusters of X in Ti−1 will also form
a connected tree. Hence, the tree Ti−1 must satisfy the jointree property.
■
PROOF OF THEOREM 9.9. A proof of this theorem is given in Jensen and Jensen [1994].
The more common statement of this result assumes that clusters C1, . . . , Cn are the
maximal cliques of a triangulated graph, which is equivalent to the given statement in
light of Theorem 9.19.
■
PROOF OF THEOREM 9.10. To prove the jointree property, we use Lemma 9.5. Suppose
that l, m, and n are three nodes in a dtree. Suppose further that l is on the path connecting
4 This is an equivalent condition to the jointree property.

9.9 PROOFS
239
m and n. Let X be a node in cluster(m) ∩cluster(n). We want to show that X belongs to
cluster(l). We consider two cases:
1. m is an ancestor of n. Hence, l is an ancestor of n. Since X ∈cluster(n), X ∈vars(n)
and, hence, X ∈vars(l). Since X ∈cluster(m), either X ∈cutset(m) or X ∈context(m).
If X ∈cutset(m), then X ∈vars(ml) and X ∈vars(mr). If X ∈context(m), then X ∈
vars↑(m). In either case, we have X ∈vars↑(l), X ∈vars(l) ∩vars↑(l) = context(l) and,
hence, X ∈cluster(l).
2. m and n have a common ancestor o. Either o = l or o is an ancestor of l. Therefore, it is
sufﬁcient to show that X ∈cluster(o) (given the above case). Without loss of generality,
suppose that m is in the left subtree of o and n is in the right subtree. Since X ∈
vars(m), X ∈vars(ol). Since X ∈vars(n), X ∈vars(or). Therefore, X ∈cluster(o) by
Lemma 9.5.
■
PROOF OF THEOREM 9.11. Let Gm be the moral graph of DAG G and consider Exer-
cise 4.24. We then have: If X \ Z and Y \ Z are intercepted by Z in graph Gm, then X \ Z
and Y \ Z are d-separated by Z in DAG G. Consider now the graph G⋆that results from
connecting every pair of nodes that appear in some cluster of the given jointree. Then
graph G⋆contains the moral graph Gm and hence: If X \ Z and Y \ Z are intercepted by
Z in graph G⋆, then X \ Z and Y \ Z are d-separated by Z in DAG G.
Let X ∈X \ Z and Y ∈Y \ Z. X then appears only in clusters on the i-side of edge
i−j and Y appears only in clusters on the j-side of the edge. We now show that any path
between X and Y in graph G⋆must be intercepted by a variable in Z.
We ﬁrst note that X \ Z and Y \ Z must be disjoint since Z = X ∩Y by the jointree
property. This implies that X and Y cannot appear in a common jointree cluster and,
hence, cannot be adjacent in G⋆. Suppose now that we have a path X−Z1 . . . Zk−Y, k ≥1.
We now show by induction on k that any such path must be intercepted by Z. If k = 1,
then Z1 ∈X since it is adjacent to X (i.e., it must appear with X in some cluster) and
Z1 ∈Y since it is adjacent to Y. Hence, Z1 ∈X ∩Y = Z. Suppose now that k > 1. If
Z1 ∈Z, then the path is intercepted by Z. Suppose that Z1 ̸∈Z. We have Z1 ∈X since X
and Z1 are adjacent. Hence, Z1 ∈X \ Z, Y ∈Y \ Z, and the path Z1−Z2 . . . Zk−Y must
be intercepted by Z given the induction hypothesis.
■
PROOF OF THEOREM 9.12. Let Ck be any root cluster in the jointree and let us direct
the jointree edges away from this root. If the edge j →i appears in the directed jointree,
we say that cluster Cj is a parent of cluster Ci. Since we have a tree structure, every
cluster except the root will have a single parent.
Let C1, . . . , Cn be an ordering of the jointree clusters such that Cn is the root cluster and
every cluster appears before its parent in the order. Then for every i, clusters C1, . . . , Ci−1
must include all the descendants of Ci (i.e., clusters connected to Ci through a child)
and Ci+1, . . . , Cn must be all nondescendants (i.e, clusters connected to Ci through its
parent). Every cluster Ci, i ̸= n, can be partitioned into two sets Xi and Yi = Ci ∩Cj,
where Cj is the parent of cluster Ci. By Theorem 9.11 and the Decomposition axiom, we
have
IPr(Xi, Yi,

n

k=i+1
Ck

\ Yi).
(9.3)
Note that the sets Yi, i ̸= n, range over all separators in the jointree.

240
MODELS FOR GRAPH DECOMPOSITION
Let c1, . . . , cn be compatible instantiations of clusters C1, . . . , Cn, respectively. We
then have
Pr(c1, c2, . . . , cn)
= Pr(x1y1, x2y2, . . . , cn)
= Pr(x1|y1, x2y2, . . . , cn)Pr(y1, x2y2, . . . , cn)
= Pr(x1|y1, x2y2, . . . , cn)Pr(x2y2, . . . , cn) since Yi ⊆Xi+1 ∪Yi+1 ∪. . . ∪Cn
= Pr(x1|y1)Pr(x2y2, . . . , cn) by (9.3)
= Pr(x1|y1) . . . Pr(xn−1|yn−1)Pr(cn) by repeated application of the previous steps
= Pr(x1y1)
Pr(y1) . . . Pr(xn−1yn−1)
Pr(yn−1)
Pr(cn)
= Pr(c1)
Pr(y1) . . . Pr(cn−1)
Pr(yn−1)Pr(cn)
=
n
i=1 Pr(ci)
n−1
j=1 Pr(yj)
.
■
PROOF OF THEOREM 9.13. We have the following:
(a) Follows immediately from the deﬁnitions of context(T ) = acutset(T ) ∩vars(T ) and
cutset(T ) = (vars(T l) ∩vars(T r)) \ acutset(T ).
(b) Suppose X ∈context(T ). Then X ∈acutset(T ) ∩vars(T ) and, hence, X ∈vars(T p). We
have two cases:
r X ∈acutset(T p), then X ∈context(T p).
r X ̸∈acutset(T p), then X ∈cutset(T p) since X ∈acutset(T ).
Therefore, X ∈context(T p) or X ∈cutset(T p).
(c) Let T s be the sibling of T and suppose X ∈cutset(T p). Then X ∈acutset(T ) by deﬁnition
of acutset and X ∈vars(T ) ∩vars(T s) by deﬁnition of a cutset. Therefore, X ∈vars(T ),
X ∈acutset(T ) and, hence, X ∈context(T ).
(d) Suppose ﬁrst that T1 is an ancestor of T2. We have cutset(T1) ⊆acutset(T2) by deﬁnition
of acutset. We also have cutset(T2) ∩acutset(T2) = ∅by deﬁnition of cutset. Hence,
cutset(T1) ∩cutset(T2) = ∅. Suppose now that T1 and T2 have a common ancestor and
that X ∈cutset(T1) ∩cutset(T2) for some X. We then have X ∈vars(T1) and X ∈vars(T2).
Moreover, if X3 is the common ancestor closest to T1 and T2, then either X ∈cutset(T3) or
X ∈acutset(T3) and, hence, X ∈acutset(T3). This means that X is in the cutset of some
common ancestor of T1 and T2, which is a contradiction with the ﬁrst part.
(e) By deﬁnition of context, we have context(T ) ⊆cluster(T ). By (b), we have
context(T ) ⊆cluster(T p). Hence, context(T ) ⊆cluster(T ) ∩cluster(T p). Suppose that
X ∈cluster(T ) ∩cluster(T p). Then X ∈vars(T ) since X ∈cluster(T ). Given X ∈
cluster(T p), we have one of two cases by (a):
r X ∈cutset(T p), then X ∈context(T ) by (c).
r X ∈context(T p),
then
X ∈acutset(T p)
and
X ∈vars(T p).
Therefore,
X ∈
acutset(T ) and X ∈context(T ).
(f) We
have
context(T l) ∪context(T r) ⊆cluster(T )
by
(b).
Moreover,
cutset(T ) ⊆
context(T l) by (c) and cutset(T ) ⊆context(T l) ∪context(T r). Suppose now that
X ∈context(T ). Then X ∈vars(T ) ∩acutset(T ), X ∈vars(T l) or X ∈vars(T r), X ∈
acutset(T l), and X ∈acutset(T r). Hence, X ∈context(T l) or X ∈context(T r), and

9.9 PROOFS
241
X ∈context(T l) ∪context(T r). Therefore, context(T ) ⊆context(T l) ∪context(T r) and
cluster(T ) ⊆context(T l) ∪context(T r).
■
PROOF OF THEOREM 9.14. The proof of this theorem will depend on Lemmas 9.6
and 9.7. EO2DT can be viewed as performing variable elimination on a collection of sets
that initially contains the families of G as given by Lemma 9.6. We need to establish this
correspondence ﬁrst to prove our theorem. Consider the set of dtrees  maintained by
EO2DT and let us assume that after processing variable π(i), each dtree in  is associated
with the following set of variables:
S(T )
def= vars(T ) ∩{π(i + 1), . . . , π(n)},
that is, variables in T that have not yet been processed. Initially, the trees in  correspond
to families in DAG G since no variable has yet been processed. As we process variable
π(i), we collect all trees T1, . . . , Tn such that π(i) ∈S(T1), . . . , S(Tn) and replace them
by the tree T = COMPOSE(T1, . . . , Tn). According to the prior deﬁnition of S(.), we have
S(T = COMPOSE(T1, . . . , Tn)) = S(T1) ∪. . . ∪S(Tn) \ {π(i)},
and hence the correspondence we are seeking with the elimination process of Lemma 9.6.
From this correspondence and Lemma 9.6, we conclude that after processing variable
π(i), the tree T = COMPOSE(T1, . . . , Tn) that is added to  is such that S(T ) contains
exactly the neighbors of variable π(i) in the moral graph of DAG G after having elim-
inated π(1), . . . , π(i −1) from it. This means that the size of S(T ) is ≤width(π, G).
Since S(T ) = vars(T ) ∩{π(i + 1), . . . , π(n)}, the size of vars(T ) ∩{π(i), . . . , π(n)}
is ≤width(π, G) + 1. Given Lemma 9.7, this means that the cluster of any node that
is added as a result of composing T1, . . . , Tn cannot be larger than width(π, G) + 1. This
proves that the width of constructed dtree is no more than the width of order π.
■
PROOF OF THEOREM 9.15. The proof of this theorem can be found in Darwiche [2001].
■
PROOF OF THEOREM 9.16. Let w = width(π, G). By Lemma 9.3, the clusters induced
by order π are all cliques in G. Hence, graph G must have a clique of size w + 1. By
Theorem 9.1, the treewidth of graph G must be ≥w. Since we have an order with width
w, graph G must then have treewidth w.
To identify a perfect elimination order for G, all we have to do is identify a node in
graph G whose neighbors form a clique (i.e., a simplicial node). If no simplicial node
exists, the graph cannot have a perfect elimination order. If we ﬁnd such a simplicial
node X, we eliminate X from G and repeat the process until either all nodes have been
eliminated, therefore leading to a perfect elimination order, or we fail to ﬁnd a simplicial
node, proving that the graph has no perfect elimination order.
■
PROOF OF THEOREM 9.17. The proof of this theorem can be found in Golumbic
[1980].
■
PROOF OF THEOREM 9.18. Suppose that we have a triangulation G⋆of graph G where
treewidth(G⋆) = w. By Theorem 9.17, G⋆admits a perfect elimination order π. By
Theorem 9.16, width(π, G⋆) = w and the order π can be identiﬁed in polytime. By
Lemma 9.2, width(π, G) ≤width(π, G⋆) and, hence, width(π, G) ≤w. We can therefore
identify in polytime an elimination order π for graph G such that width(π, G) ≤w.

242
MODELS FOR GRAPH DECOMPOSITION
Suppose we have an elimination order π for graph G where width(π, G) = w. By
Lemma 9.1, π is a perfect elimination order for Gπ and width(π, Gπ) = w. By Theo-
rem 9.16, treewidth(Gπ) = w and by Theorem 9.17, Gπ must be triangulated. Hence, we
can in polytime construct a triangulation for graph G having treewidth w.
■
PROOF OF THEOREM 9.19. By Lemma 9.1, order π induces the same cluster sequence
C1, . . . , Cn when applied to either G or Gπ. Since π is a perfect elimination order for
Gπ, Lemma 9.4 says that C1, . . . , Cn contains all of the maximal cliques of Gπ. Hence,
the maximal clusters in C1, . . . , Cn are the maximal cliques of Gπ.
■

10
Most Likely Instantiations
We consider in this chapter the problem of ﬁnding variable instantiations that have maximal
probability under some given evidence. We present two classes of exact algorithms for
this problem, one based on variable elimination and the other based on systematic search.
We also present approximate algorithms based on local search.
10.1 Introduction
Consider the Bayesian network in Figure 10.1, which concerns a population that is 55%
male and 45% female. According to this network, members of this population can suffer
from a medical condition C that is more likely to occur in males. Moreover, two diagnostic
tests are available for detecting this condition, T1 and T2, with the second test being more
effective on females. The CPTs of this network also reveal that the two tests are equally
effective on males.
One can partition the members of this population into four different groups depending
on whether they are male or female and whether they have the condition or not. Suppose
that a person takes both tests and all we know is that the two tests yield the same
result, leading to the evidence A=yes. We may then ask: What is the most likely
group to which this individual belongs? This query is therefore asking for the most
likely instantiation of variables S and C given evidence A=yes, which is technically
known as a MAP instantiation. We have already discussed this class of queries in
Chapter 5, where we referred to variables S and C as the MAP variables. The MAP
instantiation in this case is S =male and C =no with a posterior probability of about
49.3%.
The inference algorithms we presented in earlier chapters can be used to compute
MAP instantiations when the number of MAP variables is relatively small. Under such a
condition, we can simply compute the posterior marginal over MAP variables and then
select the instantiation that has a maximal posterior probability. However, this method
is guaranteed to be exponential in the number of MAP variables due to the size of
the posterior marginal. Our goal in this chapter is to present algorithms that can com-
pute MAP instantiations without necessarily being exponential in the number of MAP
variables.
A notable special case of the MAP problem arises when the MAP variables contain all
unobserved network variables. In the previous example, this corresponds to partitioning
members of the population into sixteen different groups depending on whether they are
male or female, whether they have the condition or not, and which of the four possible
outcomes would be observed for the two tests. That is, we would be asking for the most
243

244
MOST LIKELY INSTANTIATIONS
S
C
T1
T2
A
S
θs
male
.55
female
.45
S
C
θc|s
male
yes
.05
male
no
.95
female
yes
.01
female
no
.99
C
T1
θt1|c
yes
+ve
.80
yes
−ve
.20
no
+ve
.20
no
−ve
.80
S
C
T2
θt2|c,s
male
yes
+ve
.80
male
yes
−ve
.20
male
no
+ve
.20
male
no
−ve
.80
female
yes
+ve
.95
female
yes
−ve
.05
female
no
+ve
.05
female
no
−ve
.95
T1
T2
A
θa|t1,t2
+ve
+ve
yes
1
+ve
+ve
no
0
+ve
−ve
yes
0
+ve
−ve
no
1
−ve
+ve
yes
0
−ve
+ve
no
1
−ve
−ve
yes
1
−ve
−ve
no
0
Figure 10.1: A Bayesian network. Variable S represents the gender of an individual and variable C
represents a condition that is more likely in males. For males, tests T1 and T2 have the same false positive
and false negative rates. For females, test T2 has better rates. Variable A indicates the agreement between
the two tests on a particular individual.
likely instantiation of variables S, C, T1, and T2 given the evidence A=yes. The MAP
instantiation in this case is
S =female,
C =no,
T1 =−ve,
T2 =−ve,
with a posterior probability of about 47%.
This special class of MAP instantiations is known as MPE instantiations. As we shall
see, computing MPE instantiations is much easier than computing MAP instantiations,
which is one reason why this class of queries is distinguished by its own special name.
It is worth mentioning here that projecting this MPE instantiation on variables S and C
leads to S =female and C =no, which is not the MAP instantiation for variables S and C.
Hence, one cannot generally obtain MAP instantiations by projecting MPE instantiations,
although this technique is sometimes used as an approximation method for MAP.
We ﬁrst consider the computation of MPE instantiations in Section 10.2, where we
present algorithms based on variable elimination and systematic search. We then consider
the computation of MAP instantiations in Section 10.3, where we present exact and
approximate algorithms. The complexity classes of MAP and MPE queries is considered
formally in Chapter 11.
10.2 Computing MPE instantiations
Suppose that we have a Bayesian network with variables Q. The MPE probability given
evidence e is deﬁned as
MPEP (e)
def=
max
q
Pr(q, e).

10.2 COMPUTING MPE INSTANTIATIONS
245
There may be a number of instantiations q that attain this maximal probability. Each of
these instantiations is then an MPE instantiation, where the set of all such instantiations
is deﬁned as
MPE(e)
def=
argmax
q
Pr(q, e).
MPE instantiations can also be characterized as instantiations q that maximize the
posterior probability Pr(q|e) since Pr(q|e) = Pr(q, e)/Pr(e) and Pr(e) is independent of
instantiation q.
We next deﬁne a variable elimination algorithm for computing the MPE probability
and instantiations, and then follow with an algorithm based on systematic search. The
complexity of the variable elimination algorithm is guaranteed to be O(n exp(w)), where
n is the number of network variables and w is the width of the elimination order. The
algorithm based on systematic search does not have this complexity guarantee yet it can
be more efﬁcient in practice.
10.2.1 Computing MPE by variable elimination
Consider the joint probability distribution in Table 10.1, which has one MPE instantiation
(assuming no evidence):
S
C
T1
T2
A
Pr(.)
31
female
no
−ve
−ve
yes
.338580
Suppose for now that our goal is to compute the MPE probability, .338580. We can do
this using the method of variable elimination that we discussed in Chapter 6 but when
eliminating a variable, we maximize out that variable instead of summing it out. To
maximize out variable S from the factor f (S, C, T1, T2, A), we produce another factor
over the remaining variables C, T1, T2, and A by merging all rows that agree on the values
of these remaining variables. For example, the ﬁrst and seventeenth rows in Table 10.1,
S
C
T1
T2
A
Pr(.)
1
male
yes
+ve
+ve
yes
.017600
17
female
yes
+ve
+ve
yes
.003420
are merged as
C
T1
T2
A
yes
+ve
+ve
yes
.017600 = max(.017600, .003420)
As we merge rows, we drop reference to the maximized variable S and assign to the
resulting row the maximum probability associated with the merged rows.
The result of maximizing out variable S from factor f is another factor that we denote
by maxS f . Note that the factor maxS f does not mention variable S. Moreover, the new

246
MOST LIKELY INSTANTIATIONS
Table 10.1: The joint probability distribution for the Bayesian network in
Figure 10.1. Even rows are omitted as they all have zero probabilities.
S
C
T1
T2
A
Pr(.)
1
male
yes
+ve
+ve
yes
.017600
3
male
yes
+ve
−ve
no
.004400
5
male
yes
−ve
+ve
no
.004400
7
male
yes
−ve
−ve
yes
.001100
9
male
no
+ve
+ve
yes
.020900
11
male
no
+ve
−ve
no
.083600
13
male
no
−ve
+ve
no
.083600
15
male
no
−ve
−ve
yes
.334400
17
female
yes
+ve
+ve
yes
.003420
19
female
yes
+ve
−ve
no
.000180
21
female
yes
−ve
+ve
no
.000855
23
female
yes
−ve
−ve
yes
.000045
25
female
no
+ve
+ve
yes
.004455
27
female
no
+ve
−ve
no
.084645
29
female
no
−ve
+ve
no
.017820
31
female
no
−ve
−ve
yes
.338580
factor maxS f agrees with the old factor f on the MPE probability. Hence, maxS f is as
good as f for computing this probability. This means that we can continue to maximize
variables out of maxS f until we are left with a trivial factor. The probability assigned by
that factor is then the MPE probability. We show later how this method can be extended
so that it returns an MPE instantiation in addition to computing its probability. But we
ﬁrst need to provide the formal deﬁnition of maximization.
Deﬁnition 10.1. Let f (X) be a factor and let X be a variable in X. The result of
maximizing out variable X from factor f is another factor over variables Y = X \ {X},
which is denoted by maxX f and deﬁned as

max
X f
	
(y)
def=
max
x
f (x, y).
■
Similar to summation, maximization is commutative, which allows us to refer to max-
imizing out a set of variables without having to specify the order in which we maximize.
Maximization is also similar to summation in the way it interacts with multiplication (see
Exercise 10.6).
Theorem 10.1. If f1 and f2 are factors and if variable X appears only in f2, then
max
X f1f2 = f1 max
X f2.
■
This result justiﬁes the use of Algorithm 27, VE MPE, for computing the MPE prob-
ability. This algorithm resembles Algorithm 8, VE PR, from Chapter 6 as it eliminates
variables one at a time while multiplying only those factors that mention the variable
being eliminated. The only difference with VE PR is that:
r VE MPE maximizes out variables instead of summing them out.
r VE MPE does not prune nodes since every node in the network is relevant to the result.
r VE MPE eliminates all variables from the network, leading to a trivial factor.

10.2 COMPUTING MPE INSTANTIATIONS
247
Algorithm 27 VE MPE(N, e)
input:
N:
a Bayesian network
e:
evidence
output: trivial factor f , where f (⊤) is the MPE probability of evidence e
main:
1: N′←pruneEdges(N, e) {see Section 6.9.2}
2: Q←variables in network N′
3: π←elimination order of variables Q
4: S←{f e : f is a CPT of network N′}
5: for i = 1 to |Q| do
6:
f ←
k fk, where fk belongs to S and mentions variable π(i)
7:
fi←maxπ(i) f
8:
replace all factors fk in S by factor fi
9: end for
10: return trivial factor 
f ∈S f
VE MPE has the same complexity as VE PR. That is, for a network with n variables
and an elimination order of width w, the time and space complexity of VE MPE is then
O(n exp(w)).
Recovering an MPE instantiation
Algorithm VE MPE can be modiﬁed to compute an MPE instantiation in addition to its
probability. The basic idea is to employ extended factors, which assign to each instanti-
ation both a number and an instantiation. Consider for example the Bayesian network in
Figure 10.2. An extended factor f with respect to this network will appear as
Y
O
f (.)
f [.]
true
false
.000095
I =true, X=true
false
false
.460845
I =false, X=false
This factor assigns the value .000095 to instantiation Y =true, O =false, but it also
assigns the instantiation I =true, X=true to Y =true, O =false. We use f [x] to denote
the instantiation that extended factor f assigns to x while continuing to use f (x) for
denoting the number it assigns to x.
The instantiation f [x] is now used to record the MPE instantiation as it is being
constructed.1 Consider the following factor as an example:
X
Y
O
f
true
true
false
.0095
I =true
true
false
false
.0205
I =true
false
true
false
.0055
I =false
false
false
false
.4655
I =false
1 Exercise 10.10 provides more precise semantics for the instantiation f [x].

248
MOST LIKELY INSTANTIATIONS
O
J
I
X
Y
J
O
X
Y
I
I
I
true
.5
false
.5
J
J
true
.5
false
.5
J
Y
Y|J
true
true
.01
true
false
.99
false
true
.99
false
false
.01
I
J
X
X|IJ
true
true
true
.95
true
true
false
.05
true
false
true
.05
true
false
false
.95
false
true
true
.05
false
true
false
.95
false
false
true
.05
false
false
false
.95
X
Y
O
O|XY
true
true
true
.98
true
true
false
.02
true
false
true
.98
true
false
false
.02
false
true
true
.98
false
true
false
.02
false
false
true
.02
false
false
false
.98
Figure 10.2: A Bayesian network modeling the behavior of a digital circuit.
Maximizing out variable X now gives us the following factor:
Y
O
max
X f
true
false
.0095 = max(.0095, .0055)
I =true, X=true
false
false
.4655 = max(.0205, .4655)
I =false, X=false
Note how we record the value true of X in the ﬁrst row since this is the value of X that
corresponds to the maximal probability .0095. Similarly, we record the value false of X
in the second row for the same reason. There are situations where multiple values of the
maximized variable lead to a maximal probability. Recording any of these values will do
in such a case.
The following is the formal deﬁnition of operations on extended factors.
Deﬁnition 10.2. Let f (X) be an extended factor, X be a variable in X, and let Y =
X \ {X}. We then deﬁne

max
X f
	
[y]
def= x⋆f [x⋆, y],
where x⋆= argmaxx f (x, y). Moreover, let f1(X) and f2(Y) be extended factors and let
Z = X ∪Y. We then deﬁne
(f1f2)[z]
def= f1[x]f2[y],
where x and y are instantiation compatible with z (x ∼z and y ∼z).
■

10.2 COMPUTING MPE INSTANTIATIONS
249
1
S
2
S
3
S
nS
1
O
2
O
3
O
n
O
Figure 10.3: A hidden Markov model (discussed in Section 4.2).
Note that since each variable is eliminated only once, it will be recorded in only
one factor. Hence, the instantiation f [x⋆, y] does not mention variable X. Similarly, the
instantiations f1[x] and f2[y] do not mention a common variable.
Given extended factors and their operations, we can use VE MPE to compute an MPE
instantiation in addition to its probability. Speciﬁcally, we start the algorithm with each
CPT f assigning the trivial instantiation to each of its rows: f [.] = ⊤. The trivial factor
f returned by the algorithm will then contain the MPE probability, f (⊤), together with
an MPE instantiation, f [⊤].
Before we consider an example of VE MPE, we point out an important special case
that arises from applying the algorithm to a class of Bayesian networks known as hidden
Markov models (HMM) (see Figure 10.3). In particular, if we apply the algorithm to an
HMM with evidence o1, . . . , on, and elimination order π = O1, S1, O2, S2, . . . , On, Sn,
we obtain the well-known Viterbi algorithm for HMMs. In the context of this algo-
rithm, an MPE instantiation is known as a most probable state path. Moreover, if we
compute the probability of evidence o1, . . . , on using variable elimination and the pre-
vious order, we obtain the well known Forward algorithm for HMMs. Here the com-
puted probability is known as the sequence probability. Finally, we note that the order
π = O1, S1, O2, S2, . . . , On, Sn has width 1 with respect to an HMM. Hence, both the
Viterbi and Forward algorithms have linear time and space complexity, which follow
immediately from the complexity of variable elimination.
An example of computing MPE
Let us now consider an example of using VE MPE to compute an MPE instantiation
and its probability for the Bayesian network in Figure 10.2 given evidence J =true,
O =false.
We ﬁrst prune edges, leading to the network in Figure 10.4. Reducing network CPTs
with the given evidence leads to the following extended factors (we are omitting rows that
are assigned the value 0):
I
e
I
true
.5
⊤
false
.5
⊤
J
e
J
true
.5
⊤
Y
e
Y
true
.01
⊤
false
.99
⊤
I
X
e
X|I
true
true
.95
⊤
true
false
.05
⊤
false
true
.05
⊤
false
false
.95
⊤
X
Y
O
e
O|XY
true
true
false
.02
⊤
true
false
false
.02
⊤
false
true
false
.02
⊤
false
false
false
.98
⊤

250
MOST LIKELY INSTANTIATIONS
J
I
X
O
Y
J
O
X
Y
I
I
I
true
.5
false
.5
J
J
true
.5
false
.5
Y
Y
true
.01
false
.99
I
X
X|I
true
true
.95
true
false
.05
false
true
.05
false
false
.95
X
Y
O
O|XY
true
true
true
.98
true
true
false
.02
true
false
true
.98
true
false
false
.02
false
true
true
.98
false
true
false
.02
false
false
true
.02
false
false
false
.98
Figure 10.4: Pruning edges in the network of Figure 10.2 given the evidence J =true, O =false.
Note how each of these factors assigns the trivial instantiation ⊤to each of its rows.
Assuming the elimination order J, I, X, Y, O, Algorithm VE MPE will then evaluate the
following expression:
max
J,I,X,Y,O e
I e
J e
Y e
X|I e
O|XY
= max
O

max
Y

max
X

max
I
e
Ie
X|I
	
e
O|XY
	
e
Y
	 
max
J
e
J
	
.
All intermediate factors constructed during this evaluation are depicted in Figure 10.5.
Therefore, an MPE instantiation given evidence e :J =true, O =false is
I =false,
J =true,
X=false,
Y =false,
O =false.
Moreover, the MPE probability is .2304225.
We can modify Algorithm VE MPE further to enumerate the set of all MPE instantiations
in time and space that are linear in the number of such instantiations (see Exercise 10.11).
We can also adapt the algorithm of recursive conditioning from Chapter 8 for computing
the MPE probability and instantiations, leading to an algorithm with a similar computa-
tional complexity to the one based on variable elimination (see Exercise 10.22).
10.2.2 Computing MPE by systematic search
In this section, we consider another class of algorithms for computing MPE that are
based on systematic search and can be more efﬁcient in practice than algorithms based on
variable elimination.
Consider the Bayesian network in Figure 10.1 and suppose that our goal is to ﬁnd an
MPE instantiation given evidence A=true. One way of doing this is using depth-ﬁrst

10.2 COMPUTING MPE INSTANTIATIONS
251
max
J
e
J
⊤
.5
J =true
I
X
e
Ie
X|I
true
true
.475
⊤
true
false
.025
⊤
false
true
.025
⊤
false
false
.475
⊤
X
max
I
e
Ie
X|I
true
.475
I =true
false
.475
I =false
X
Y
O

max
I
e
Ie
X|I
	
e
O|XY
true
true
false
.0095
I =true
true
false
false
.0095
I =true
false
true
false
.0095
I =false
false
false
false
.4655
I =false
Y
O
max
X

max
I
e
Ie
X|I
	
e
O|XY
true
false
.0095
I =true, X=true
false
false
.4655
I =false, X=false
Y
O

max
X

max
I
e
Ie
X|I
	
e
O|XY
	
e
Y
true
false
.000095
I =true, X=true
false
false
.460845
I =false, X=false
O
max
Y

max
X

max
I
e
Ie
X|I
	
e
O|XY
	
e
Y
false
.460845
I =false, X=false, Y =false
max
O

max
Y

max
X

max
I
e
Ie
X|I
	
e
O|XY
	
e
Y
	
⊤
.460845
I =false, X=false, Y =false, O =false
max
O

max
Y

max
X

max
I
e
Ie
X|I
	
e
O|XY
	
e
Y
	 
max
J
e
J
	
⊤
.2304225
J =true, I =false, X=false, Y =false, O =false
Figure 10.5: Intermediate factors of an MPE computation.
search on the tree depicted in Figure 10.6. The leaf nodes of this tree are in one-
to-one correspondence with the instantiations of unobserved network variables. More-
over, each nonleaf node corresponds to a partial network instantiation. For example, the
node marked by an arrow represents the instantiation S =male, C =yes. The children
of this node correspond to extensions of this instantiation, which are obtained by as-
signing different values to the variable T1. For example, the left child corresponds to
instantiation S =male, C =yes, T1 =−ve, while the right child corresponds to extension
S =male, C =yes, T1 =+ve.
We can traverse this search tree using depth-ﬁrst search as given by Algorithm 28,
DFS MPE. Assuming that we have n unobserved network variables, DFS MPE can be
implemented to take O(n) space and O(n exp(n)) time. Note here that the probability
Pr(i) on Line 2 can be computed in time linear in the number of network variables using
the chain rule of Bayesian networks.

252
MOST LIKELY INSTANTIATIONS
T2
T2
T2
T2
T2
T2
T2
T2
T1
T1
T1
T1
C
C
S
male
female
no
yes
no
yes
-
+
-
+
-
+
-
-
+
-
+
-
+
-
+
-
+ -
+
-
+
-
+
+
Figure 10.6: Searching for a most likely instantiation in the network of Figure 10.1 given evidence A=yes.
The instantiation marked in bold has the greatest probability in this case when combined with evidence
A=yes.
Algorithm 28 DFS MPE(N, e)
input:
N:
Bayesian network
e:
evidence
output: an MPE instantiation for evidence e
main:
Q←network variables distinct from variables E
s←network instantiation compatible with evidence e {global variable}
p←probability of instantiation s {global variable}
DFS MPE AUX(e, Q)
return s
DFS MPE AUX(i,X)
1: if X is empty then {i is a network instantiation}
2:
if Pr(i) > p, then s←i and p←Pr(i)
3: else
4:
X←a variable in X
5:
for each value x of variable X do
6:
DFS MPE AUX(ix, X \ {X})
7:
end for
8: end if
Branch-and-bound search
We can improve the performance of Algorithm DFS MPE by pruning parts of the search
tree using an upper bound on the MPE probability. Suppose for example that we are
exploring a search node corresponding to instantiation i. If we have already visited a leaf
node corresponding to a network instantiation s with probability p and we know that every
completion of instantiation i will not have a higher probability, we can then abandon the
search node as it would not lead to instantiations that improve on the instantiation s found
thus far.
Consider Figure 10.7, which depicts a search tree, and let us assume that the tree
is traversed from left to right (we later explain how the upper bounds in this ﬁgure are
computed). The ﬁrst instantiation visited by depth-ﬁrst search is then the one on the far left,

10.2 COMPUTING MPE INSTANTIATIONS
253
T2
T2
T1
T1
C
C
S
male
female
no
yes
no
yes
-
+
-
+
-
+
-
+
.3344
.33858
.0176
.0209
.004455
.00342
Figure 10.7: Pruning nodes while searching for a most likely instantiation in the network of Figure 10.1
given evidence A=yes. Numbers associated with nonleaf nodes represent upper bounds on the
MPE probability. For example, .0176 represents an upper bound on the MPE probability of evidence
S =male, C =yes, and A=true.
which has a probability of .3344. Using this instantiation as a baseline, the algorithm can
now prune two subtrees as their upper bounds are lower than .3344. The next instantiation
found by the algorithm has a probability of .33858, which is an improvement on the ﬁrst
instantiation. Using this new instantiation as a baseline, the algorithm prunes two more
subtrees before terminating with the MPE instantiation
S =female,
C =no,
T1 =−ve,
T2 =−ve,
A=yes,
which has an MPE probability of .33858.
The main point here is that our search algorithm did not have to visit every node in
the search tree, which allows it to escape the exponential complexity in certain cases. If
we let MPEu
P (i) stand for the computed upper bound on the MPE probability MPEP (i),
we can then modify Algorithm DFS MPE AUX by inserting the following line just before
Line 4:
if MPEu
P (i) ≤p, return.
The efﬁciency of the resulting algorithm, which is known as depth-ﬁrst branch-and-
bound, will clearly depend on the tightness of computed upper bounds and the time it
takes to compute them. In general, we want the tightest bounds possible and we would
like to compute them efﬁciently, as we must compute one bound for each node in the
search tree.
Generating upper bounds by node splitting
Instead of providing a speciﬁc upper bound on the MPE probability, we next discuss a
technique that allows us to generate a spectrum of such bounds in which one can trade off
the bound tightness with the time it takes to compute it.
Consider the network structure in Figure 10.8(a) and the corresponding transformation
depicted in Figure 10.8(b). According to this transformation, the tail of edge S →T2 has
been cloned by variable ˆS, that is, a variable that is meant to duplicate S by having the
same set of values. Similarly, the tail of edge T1 →A has now been cloned by variable
ˆT1. Here, both clone variables are roots and are assumed to have uniform CPTs, while all
other network variables are assumed to maintain their original CPTs (except that we need
to replace S by its clone ˆS in the CPT of variable T2 and replace T1 by its clone ˆT1 in the
CPT of variable A).

254
MOST LIKELY INSTANTIATIONS
S
C
T1
T2
A
S
C
T1
T2
A
S
T1
ˆ
ˆ
S
C
T1
T2
A
C
ˆ
(a) Original network
(b) Splitting nodes S and T1
(c) Splitting node C
Figure 10.8: Splitting nodes.
Transformations such as these can be used to reduce the treewidth of a network to a
point where applying inference algorithms, such as variable elimination, becomes feasible.
However, the transformed network does not induce the same distribution induced by the
original network but is known to produce upper bounds on MPE probabilities. In particular,
suppose that e is the given evidence and let ˆe be the instantiation of clone variables implied
by evidence e. That is, for every variable X instantiated to x in evidence e, its clone ˆX (if
any) will also be instantiated to x in ˆe. The following is then guaranteed to hold:
MPEP (e) ≤β · MPE′
P (e, ˆe),
where MPEP (e) and MPE′
P (e, ˆe) are MPE probabilities with respect to the original and
transformed networks, respectively, and β is the total number of instantiations for the
cloned variables. Hence, we can compute an upper bound on the MPE probability by
performing inference on the transformed network, which usually has a low treewidth by
design.
Consider Figure 10.8(b) as an example. If the evidence e is S =male and A=yes, then
ˆe is ˆS =male. Moreover,
MPEP (e) = .3344
MPE′
P (e, ˆe) = .0836
β = 4,
leading to
.3344 ≤(4)(.0836) = .3344.
Hence, the upper bound is exact in this case. Moreover, if the evidence e is simply A=yes,
then ˆe is empty and we have
MPEP (e) = .33858
MPE′
P (e, ˆe) = .099275
β = 4,
leading to
.33858 < (4)(.099275) = .3971.

10.2 COMPUTING MPE INSTANTIATIONS
255
We later state this guarantee formally but we ﬁrst need the following deﬁnition.
Deﬁnition 10.3. Let X be a node in a Bayesian network N with children Y. We say that
node X is split according to children Z ⊆Y when it results in a network that is obtained
from N as follows:
r The edges outgoing from node X to its children Z are removed.
r A new root node ˆX, with the same values as X, is added to the network with nodes
Z as its children.
r The node ˆX is assigned a uniform CPT. The new CPT for a node in Z is obtained
from its old CPT by replacing X with ˆX.
Here node ˆX is called a clone. Moreover, the number of instantiations for split variables
is called the split cardinality.
■
In Figure 10.8(b), node S has been split according to its child T2 and node T1 has been
split according to its child A, leading to a split cardinality of 4.
When a node X is split according to a single child Y, we say that we have deleted
the edge X →Y. Hence, the transformation in Figure 10.8(b) is the result of deleting
two edges, S →T2 and T1 →A. Note, however, that in Figure 10.8(c) node C has been
split according to both of its children, leading to a split cardinality of 2. Hence, this
transformation cannot be described in terms of edge deletion. We now have the following
theorem, which shows how splitting variables can be used to produce upper bounds on
the MPE probability.
Theorem 10.2. Let N′ be the network that results from splitting variables in network N
and let β be the split cardinality. For evidence e on network N, let ˆe be the instantiation
of split variables that is implied by e. We then have
MPEP (e) ≤β · MPE′
P (e, ˆe),
where MPEP (e) and MPE′
P (e, ˆe) are MPE probabilities with respect to the networks N
and N′, respectively.
■
We ﬁnally note that network transformations based on variable splitting can be cas-
caded. For example, a clone variable ˆX that results from splitting variable X can later
be split, producing its own clone. Moreover, each of the transformed networks produces
upper bounds for all of the networks that precede it in the transformation sequence.
If variable splits are chosen carefully to reduce the network treewidth, then we can
trade off the quality of the bounds with the time it takes to compute them. Typically, one
chooses a treewidth that can be handled by the given computational resources and then
splits enough variables to reach that treewidth.
Reducing the search space
When searching for an instantiation of variables X1, . . . , Xn, the size of the search tree
will be O(exp(n)), as we must have a leaf node for each variable instantiation. However,
when using a split network for producing upper bounds we can reduce the search space
signiﬁcantly by searching only over the subset of variables that have been split. This is
due to the following theorem.

256
MOST LIKELY INSTANTIATIONS
T1
T1
S
male
female
-
+
-
+
.3344
.0209
.33858
.004455
Figure 10.9: Reducing the search space for most likely instantiations.
Theorem 10.3. Consider Theorem 10.2 and suppose that distributions Pr and Pr′ are
induced by networks N and N′, respectively. If the evidence e includes all split variables
and if Q are the variables of N not in E, then
Pr(Q, e) = β · Pr′(Q, e, ˆe).
■
This result is effectively showing that when the evidence instantiates all split variables,
queries with respect to the split network are guaranteed to yield exact results. Hence,
under evidence that instantiates all split variables we can perform exact inference in time
and space that is exponential only in the treewidth of the split network.
Consider again the search tree in Figure 10.7, which is over four variables S, C, T1, and
T2. The upper bounds for this search tree are generated by the network in Figure 10.8(b),
which results from splitting two variables S and T1. Hence, any time these variables are
instantiated in the search tree, the bound produced by the split network is guaranteed to
be exact. This means that one needs to only search over these two variables, leading to the
smaller search tree shown in Figure 10.9. Consider for example the left-most leaf node,
which corresponds to the instantiation S =male and T1 =−ve. Since both split variables
are instantiated in this case, Theorem 10.3 will then guarantee that
MPEP (e) = 4 · MPE′
P (e, ˆe),
where e is the instantiation
S =male,
T1 =−ve,
A=yes,
and ˆe is the instantiation
ˆS =male,
ˆT1 =−ve.
This means that once we reach this leaf node, there is no need to search deeper in the
tree as we already know the exact MPE probability for instantiation e. Moreover, we can
recover an MPE instantiation by calling Algorithm VE MPE on the split network with
evidence e, ˆe in this case.
Algorithm 29, BB MPE, depicts the pseudocode for our search in this reduced space.
BB MPE uses depth-ﬁrst search to ﬁnd an instantiation q of the split variables for which
the instantiation s = qe has a greatest MPE probability p (this is done by calling
BB MPE AUX). The algorithm then computes an MPE instantiation for the identiﬁed s
using exact inference on the split network (this is done by calling VE MPE). Note that
inference is invoked on the split network in a number of places. Except in the case when
Line 4 is reached, the results returned by the split network are guaranteed to be exact.
This result follows because all such inferences are done while instantiating all of the split
variables.

10.2 COMPUTING MPE INSTANTIATIONS
257
Algorithm 29 BB MPE(N, e, N′)
input:
N:
Bayesian network
e:
evidence
N′:
Bayesian network which results from splitting nodes in N
output: MPE instantiation for network N and evidence e
main:
Q←variables that were split in network N (assumes Q ∩E = ∅)
q←an instantiation of split variables Q
β←split cardinality
s←qe {current solution of BB MPE AUX}
p←β · MPE′
P (sˆs) {MPE probability of s}
BB MPE AUX(e, Q) {modiﬁes s and p}
return MPE instantiation for evidence sˆs and network N′ using VE MPE
BB MPE AUX(i, X)
1: b←β · MPE′
P (iˆi) {bound on the MPE probability of i}
2: if X is empty then {leaf node, bound b is exact}
3:
if b > p, then s←i and p←b {i better than current solution}
4: else if b > p then
5:
X←a variable in X
6:
for each state x of variable X do
7:
BB MPE AUX(ix, X \ {X})
8:
end for
9: end if
Complexity analysis
Suppose now that our original network has n variables and an elimination order of width
w. Using variable elimination, we can solve an MPE problem for this network in time
and space O(n exp(w)). Suppose, however, that we wish to solve this MPE problem using
BB MPE, where we split m variables leading to a network with n + m variables and an
elimination order of width w′. Assuming that inference on the split network is performed
using variable elimination, the space complexity of BB MPE is now dominated by the space
complexity of performing inference on the split network, which is O((n + m) exp(w′)).
This is also the time complexity for such inference, which must be performed for each
node in the search tree. Since the search tree size is now reduced to O(exp(m)) nodes, the
total time complexity is then O((n + m) exp(w′ + m)).
This may not look favorable to BB MPE but we should note the following. First, the space
complexity of the BB MPE is actually better as it is exponential in the reduced width w′ in-
stead of the original width w. Second, the time complexity of variable elimination is both
a worst- and a best-case complexity. For BB MPE however, this is only a worst-case com-
plexity, leaving the possibility of a much better average-case complexity (due to pruning).
10.2.3 Reduction to W-MAXSAT
In Chapter 11, we consider a reduction of the MPE problem into the well-known W-
MAXSAT problem, which can be solved using systematic and local search algorithms.

258
MOST LIKELY INSTANTIATIONS
There is an extensive literature on these search methods that can be brought to bear on
MPE, either directly or through the reduction to be discussed in Chapter 11.
10.3 Computing MAP instantiations
The MAP probability for variables M and evidence e is deﬁned as
MAPP (M, e)
def= max
m Pr(m, e).
There may be a number of instantiations m that attain this maximal probability. Each of
these instantiations is then a MAP instantiation, where the set of all such instantiations is
deﬁned as
MAP(M, e)
def= argmax
m
Pr(m, e).
MAP instantiations can also be characterized as instantiations m that maximize the pos-
terior probability Pr(m|e) since Pr(m|e) = Pr(m, e)/Pr(e) and Pr(e) is independent of
instantiation m.
10.3.1 Computing MAP by variable elimination
We can compute the MAP probability MAPP (M, e) using the algorithm of variable
elimination by ﬁrst summing out all non-MAP variables and then maximizing out MAP
variables M. By summing out non-MAP variables, we effectively compute the joint
marginal Pr(M, e) in factored form. By maximizing out MAP variables M, we effectively
solve an MPE problem over the resulting marginal. The proposed algorithm can therefore
be thought of as a combination of Algorithm VE PR from Chapter 6 and Algorithm VE MPE
that we presented previously for computing the MPE probability.
Algorithm 30, VE MAP, provides the pseudocode for computing the MAP probability
using variable elimination. There are two aspects of this algorithm that are worth noting.
First, the elimination order used on Line 2 is special in the sense that MAP variables appear
last in the order. Second, the algorithm performs both types of elimination, maximizing-
out for MAP variables and summing-out for non-MAP variables. Algorithm VE MAP can
also be used to compute a MAP instantiation by using extended factors, just as when
computing an MPE instantiation. We present an example of this later.
MAP and constrained width
The complexity of VE MAP is similar to that of VE MPE in the following sense. Given
a Bayesian network with n variables and an elimination order of width w, the time and
space complexity of VE MAP is O(n exp(w)). There is one key difference with VE MPE,
though. The variable order used on Line 2 of VE MAP is constrained as it requires MAP
variables M to appear last in the order. What this means is that we may not be able to use
a good ordering (one with low width) simply because low-width orders may not satisfy
this constraint. Consider the network in Figure 10.10, which has a polytree structure.
The treewidth of this network is 2 since we have at most two parents per node. Suppose
now that we want to compute MAP for variables M = {Y1, . . . , Yn} with respect to this
network. Any order in which variables M come last has a width ≥n, even though we have
an unconstrained variable order with width 2. Hence, VE MPE requires linear time in this
case, while VE MAP requires exponential time.

10.3 COMPUTING MAP INSTANTIATIONS
259
Algorithm 30 VE MAP(N, M, e)
input:
N:
Bayesian network
M:
some variables in the network
e:
evidence (E ∩M = ∅)
output: trivial factor containing the MAP probability MAPP (M, e)
main:
1: N′←pruneNetwork(N, M, e) {see Section 6.9.3}
2: π←a variable elimination order for N′ in which variables M appear last
3: S←{f e : f is a CPT of network N′}
4: for i = 1 to length of order π do
5:
f ←
k fk, where factor fk belongs to S and mentions variable π(i)
6:
if π(i) ∈M then
7:
fi←maxπ(i) f
8:
else
9:
fi←
π(i) f
10:
end if
11:
replace all factors fk in S by factor fi
12: end for
13: return trivial factor 
f ∈S f
X1
X2
X3
X4
Xn
Y1
Y2
Y3
Y4
Yn
Figure 10.10: A polytree structure.
In general, we cannot use arbitrary elimination orders as we cannot interleave variables
that we are summing out with those that we are maximizing out because maximization
does not commute with summation, as shown by the following theorem.
Theorem 10.4. Let f be a factor over disjoint variables X, Y, and Z. We then have
&
X
max
Y
f
'
(z) ≥
&
max
Y

X
f
'
(z)
for all instantiations z.
■
The complexity of computing MAP using variable elimination is then at best exponen-
tial in the constrained treewidth, as deﬁned next.
Deﬁnition 10.4. A variable order π is M-constrained iff variables M appear last in the
order π. The M-constrained treewidth of a graph is the width of its best M-constrained
variable order.
■

260
MOST LIKELY INSTANTIATIONS
Computing MAP is therefore more difﬁcult than computing MPE in the context of
variable elimination. An interesting question is whether this difference in complexity is
genuine or whether it is an idiosyncrasy of the variable elimination framework. We shed
more light on this question when we discuss the complexity of probabilistic inference in
Chapter 11.
An example of computing MAP
Let us now consider an example of using Algorithm VE MAP to compute the MAP
probability and instantiation for the Bayesian network of Figure 10.2 with MAP vari-
ables M = {I, J} and evidence e : O =true. We use the constrained variable order
π = O, Y, X, I, J for this purpose. Summing out variables O, Y, and X, we obtain the
following set of factors, which represent a factored representation of the joint marginal
Pr(I, J, O =true):
I
J
f1
true
true
.93248
⊤
true
false
.97088
⊤
false
true
.07712
⊤
false
false
.97088
⊤
I
f2
true
.5
⊤
false
.5
⊤
J
f3
true
.5
⊤
false
.5
⊤
We will now trace the rest of VE MAP by maximizing out variables I and J. To eliminate
variable I, we multiply its factors and then maximize:
I
J
f1 f2
true
true
.466240
⊤
true
false
.485440
⊤
false
true
.038560
⊤
false
false
.485440
⊤
J
max
I
f1 f2
true
.466240
I =true
false
.485440
I =true
To eliminate variable J, we multiply its factors and then maximize:
J

max
I
f1 f2

f3
true
.233120
I =true
false
.242720
I =true
max
J

max
I
f1 f2

f3
⊤
.242720
I =true, J =false
Therefore, the instantiation I =true, J =false is a MAP instantiation in this case. More-
over, .242720 is the MAP probability, Pr(I =true, J =false, O =true).
10.3.2 Computing MAP by systematic search
MAP can be solved using depth-ﬁrst branch-and-bound search, just as we did for MPE.
The pseudocode for this is shown in Algorithm 31, BB MAP, which resembles the one
for computing MPE aside from two exceptions. First, the instantiation i on Line 2 does
not cover all network variables (only MAP variables and evidence). Hence, computing
its probability can no longer be performed using the chain rule of Bayesian networks
and would therefore require inference on the given network. Second, although the upper
bound needed for pruning on Line 3 can be computed based on a split network as we
did for MPE, the quality of this bound has been observed to be somewhat loose in
practice, at least compared to the MPE case. We therefore provide a different method
for computing upper bounds on the MAP probability next that is based on the following
theorem.

10.3 COMPUTING MAP INSTANTIATIONS
261
Algorithm 31 BB MAP(N, M, e)
input:
N:
Bayesian network
M:
some variables in the network
e:
evidence (E ∩M = ∅)
output: MAP instantiation for variables M and evidence e
main:
m←some instantiation of variables M {global variable}
p←probability of instantiation m, e {global variable}
BB MAP AUX(e, M)
return m
BB MAP AUX(i, X)
1: if X is empty then {leaf node}
2:
if Pr(i) > p, then m←i and p←Pr(i)
3: else if MAPu
P (X, i) > p then
4:
X←a variable in X
5:
for each value x of variable X do
6:
BB MAP AUX(ix, X \ {X})
7:
end for
8: end if
Theorem 10.5. Consider Algorithm 30, VE MAP, and let Pr be the distribution induced
by the given Bayesian network. If we use an arbitrary elimination order on Line 2 of the
algorithm, the number q it returns will satisfy
MAPP (M, e) ≤q ≤Pr(e).
■
That is, even though we cannot use an arbitrary elimination order for computing the
MAP probability, we can use such an order for computing an upper bound on the MAP
probability. As it turns out, some of these arbitrary orders produce better bounds than
others, and our goal is to obtain an order that provides one of the better bounds. We
discuss this issue next but only after analyzing the complexity of our search algorithm.
Suppose that we are given a network with n variables, an elimination order of width
w, and a constrained elimination order of width w + c. We can compute MAP for this
network using variable elimination in O(n exp(w + c)) time and space. Suppose now that
we use Algorithm BB MAP for this purpose. The search tree has O(exp(m)) nodes in this
case, where m is the number of MAP variables. The inferences needed for a leaf node
on Line 2 takes O(n exp(w)) time and space, assuming the use of variable elimination.
Computing the upper bound on Line 3 also requires a similar complexity. Hence, the
total space complexity of the algorithm is O(n exp(w)) and the total time complexity is
O(n exp(w + m)).
Comparing this to the use of variable elimination, VE MAP, we ﬁnd that the search
algorithm has a better space complexity. As to the time complexity, it depends on the
constrained width w + c and the number of MAP variables m. Even if the number of
MAP variables m is larger than c, the search algorithm may still perform better. Recall
here that the complexity of variable elimination is for the best and worst case. However,

262
MOST LIKELY INSTANTIATIONS
CX
AXY
AB
AW
BDZ
root
1
2
3
4
5
CXY
AXY
ABZ
AW
BDZ
root
1
2
3
4
5
(a) Jointree
(b) Jointree with promoted variables
Figure 10.11: Variables that are underlined are eliminated in the corresponding clusters. The MAP
variables are W, X, Y , and Z. Only variables Y and Z are promoted in this case. In particular, variable
Y has been promoted from cluster 2 to cluster 1, and variable Z has been promoted from cluster 4 to
cluster 3.
the complexity of search is only for the worst case, where the average complexity may be
much better in practice.
Improving the bound
As it turns out, even though any elimination order can be used to produce an upper bound,
some orders will produce tighter upper bounds than others. Intuitively, the closer the used
order is to a constrained order, the tighter the bound is expected to be. We next discuss a
technique for selecting one of the better (unconstrained) orders.
Our technique is based on elimination orders that are obtained from jointrees, as
discussed in Section 9.3.5. For example, considering the jointree in Figure 10.11(a) and
the root cluster C1, we can generate a number of elimination orders, including
π1 = D, Z, B, W, A, Y, C, X
π2 = Z, D, W, B, Y, A, X, C.
Suppose now that the MAP variables are W, X, Y, and Z. The elimination orders π1
and π2 are not constrained in this case. Note, however, that order π1 will produce better
bounds than order π2 as the MAP variables appear later in the order.
We can easily transform a jointree of width w into another jointree of width w that
induces elimination orders closer to a constrained order. The basic idea is to promote
MAP variables toward the root without increasing the jointree width w. Speciﬁcally, let
Ck and Ci be two adjacent clusters in the jointree where Ci is closer to the root. Any
MAP variable M that appears in cluster Ck is added to cluster Ci as long as the new size
of cluster Ci does not exceed w + 1. This transformation is guaranteed to preserve the
jointree properties as discussed in Section 9.4.
Figure 10.11 illustrates a jointree before and after promotion of the MAP variables.
The new jointree induces the following order:
π3 = D, B, Z, W, A, C, X, Y,
which is closer to a constrained order than order π1 given previously because the MAP
variables Y and Z appear later in the order:
π1 = D, Z, B, W, A, Y, C, X
π3 = D, B, Z, W, A, C, X, Y.

10.3 COMPUTING MAP INSTANTIATIONS
263
Effectively, each time a MAP variable is promoted from cluster Ck to cluster Ci (which is
closer to the root), the elimination of that variable is postponed, pushing it past all of the
non-MAP variables that are eliminated in cluster Ci. This typically has a monotonic effect
on the upper bound, although in rare cases the bound may remain the same. We stress
here that the promotion technique is meant to improve the bound computed by elimination
orders that are induced by the jointree and a selected root. After such promotion, the quality
of a bound computed by orders that are induced by some other root may actually worsen.
The performance of Algorithm 31, BB MAP, can also be signiﬁcantly improved if
one employs heuristics for choosing variables and values on Lines 4 and 5, respectively.
Exercise 10.21 suggests some heuristics for this purpose. Another technique that is known
to improve the performance of branch-and-bound search concerns the choice of a seed
instantiation with which to start the search. In particular, the greater the probability of this
initial instantiation, the more pruning we expect during search. The next section discusses
local search methods, which can be very efﬁcient yet are not guaranteed to ﬁnd the most
likely instantiation. It is quite common to use these local search methods for producing
high-quality seeds for branch-and-bound search.
10.3.3 Computing MAP by local search
A MAP instantiation can be approximated using local search, which can be more efﬁcient
than systematic search as presented in the previous section. In particular, given a network
with n variables and an elimination order of width w, the local search algorithm we
discuss next takes O(r · n exp(w)) time and space, where r is the number of search steps
performed by the algorithm.
Similar to systematic search, local search is applied in the space of instantiations m
of MAP variables M. However, instead of systematically and exhaustively searching
through this space, we simply start with an initial instantiation m0 and then visit one of its
neighbors. A neighbor of instantiation m is deﬁned as an instantiation that results from
changing the value of a single variable X in m. If the new value of X is x, we denote the
resulting neighbor by m −X, x. To perform local search efﬁciently, we need to compute
the scores (probabilities) for all of the neighbors m −X, x efﬁciently. That is, we need to
compute Pr(m −X, x, e) for each X ∈M and each of its values x not in m. Computing
the scores of all neighbors Pr(m −X, x, e) can be done in O(n exp(w)) time and space,
as shown in Chapter 12.
A variety of local search methods can be used to navigate the instantiations of MAP
variables M. One particular approach is stochastic hill climbing, which is given in Algo-
rithm 32, LS MAP. Stochastic hill climbing proceeds by repeatedly either moving to the
most probable neighbor of current instantiation m or changing a variable value at random
in the current instantiation m. This is repeated for a number of steps r before the algorithm
terminates and returns the best instantiation visited in the process.
The quality of the solution returned by a local search algorithm depends to a large
extent on which part of the search space it starts to explore. There are a number of
possible initialization schemes that we could use for this purpose. Suppose that n is the
number of network variables, w is the width of a given elimination order, and m is the
number of MAP variables. We can then use any of the following initialization methods:
r Random initialization: For each MAP variable, we select one of its values with a uniform
probability. This method takes O(m) time.

264
MOST LIKELY INSTANTIATIONS
Algorithm 32 LS MAP(N, M, e)
input:
N:
Bayesian network
M:
some network variables
e:
evidence (E ∩M = ∅)
output: instantiation m of M which (approximately) maximizes Pr(m|e)
main:
1: r←number of local search steps
2: Pf ←probability of randomly choosing a neighbor
3: m⋆←some instantiation of variables M {best instantiation}
4: m←m⋆{current instantiation}
5: for r times do
6:
p←random number in [0, 1]
7:
if p ≤Pf then
8:
m←randomly selected neighbor of m
9:
else
10:
compute the score Pr(m −X, x, e) for each neighbor m −X, x
11:
if no neighbor has a higher score than the score for m then
12:
m←randomly selected neighbor of m
13:
else
14:
m←a neighbor of m with a highest score
15:
end if
16:
end if
17:
if Pr(m, e) > Pr(m⋆, e), then m⋆←m
18: end for
19: return m⋆
r MPE-based initialization: Compute the MPE solution given the evidence. Then for each
MAP variable, set its value according to the MPE instantiation. This method takes
O(n exp(w)) time.
r Maximum marginal initialization: For each MAP variable X, set its value to the instance
x that maximizes Pr(x|e). This method takes O(n exp(w)) time.
r Sequential initialization: This method considers the MAP variables X1, . . . , Xm, choosing
each time a variable Xi that has the highest probability Pr(xi|e, y) for one of its values
xi, where y is the instantiation of MAP variables considered so far. This method takes
O(m · n exp(w)) time.
The last initialization method is the most expensive but has also proven to be the most
effective. Finally, when random initialization is used, it is not uncommon to use the
technique of random restarts, where one runs the algorithm multiple times with a different
initial, random instantiation in each run.
Bibliographic remarks
The use of variable elimination to solve MAP and MPE was proposed in Dechter [1999].
The Viterbi algorithm for solving MPE on HMMs is discussed in Viterbi [1967] and
Rabiner [1989]. Constrained width was ﬁrst introduced in Park and Darwiche [2004a],
where it was used to analyze the complexity of variable elimination for solving MAP.

10.4 EXERCISES
265
Solving MPE using branch-and-bound search was ﬁrst proposed in Kask and Dechter
[2001]; see also Marinescu et al. [2003] and Marinescu and Dechter [2006]. A more
sophisticated algorithm based on a variant of recursive conditioning was proposed in
Marinescu and Dechter [2005]. The upper bounds used in these algorithms are based on
the minibuckets framework [Dechter and Rish, 2003], which is a relaxation of variable
elimination that allows one to produce a spectrum of upper bounds that trade accuracy with
efﬁciency. The upper bound based on splitting variables was proposed in Choi et al. [2007],
where a correspondence was shown with the upper bounds produced by the minibuckets
framework. However, the computation of these bounds based on split networks was shown
to be more general as it allows one to use any algorithm for the computation of these
bounds, such as the ones for exploiting local structure that we discuss in Chapter 13.
The use of branch-and-bound search for solving MAP was ﬁrst proposed in Park and
Darwiche [2003a], who also introduced the MAP upper bound discussed in this chapter.
Arithmetic circuits, discussed in Chapter 12, were used recently for the efﬁcient compu-
tation of this bound [Huang et al., 2006]. The use of stochastic local search for solving
MAP was ﬁrst proposed in Park and Darwiche [2001]. Other approximation schemes
were proposed in Park and Darwiche [2004a], Yuan et al. [2004], and Sun et al. [2007].
Local search and other approximation schemes have also been proposed for solving MPE
(e.g., [Kask and Dechter, 1999; Hutter et al., 2005; Wainwright et al., 2005; Kolmogorov
and Wainwright, 2005]).
10.4 Exercises
10.1.
Compute the MPE probability and a corresponding MPE instantiation for the Bayesian net-
work in Figure 10.1, given evidence A=no and using Algorithm 27, VE MPE.
10.2.
Consider the Bayesian network in Figure 10.1. What is the most likely outcome of the two
tests T1 and T2 for a female on whom the tests came out different? Use Algorithm 30,
VE MAP, to answer this question.
10.3.
Show that the MAP example in Section 10.3.1 admits another MAP instantiation with the
same probability .242720.
10.4.
Construct a factor f over variables X and Y such that

Y
max
X f ̸= max
X

Y
f .
10.5.
Construct a factor f over variables X and Y such that

Y
max
X f = max
X

Y
f .
10.6.
Prove Theorem 10.1.
10.7.
Consider a naive Bayes structure with edges C →A1, . . . , C →An. What is the complexity
of computing the MPE probability for this network using Algorithm 27, VE MPE? What is
the complexity of computing the MAP probability for MAP variables A1, . . . , An using Algo-
rithm 30, VE MAP? How does the complexity of these computations change when we have
evidence on variable C?
10.8.
True or false: We can compute a MAP instantiation MAP(M, e) by computing the MAP instan-
tiations mx = MAP(M, ex) for every value x of some variable X ̸∈M and then returning
argmaxmx Pr(mx, e). What if X ∈M? For each case, either prove or provide a counter-
example.
10.9.
Suppose that we have a jointree with n clusters and width w. Show that MAP can be solved
in O(n exp(w)) time and space if all MAP variables are contained in some jointree cluster.

266
MOST LIKELY INSTANTIATIONS
10.10. Show that Algorithm 27, VE MPE, is correct when run with extended factors. In particular,
prove the following invariant for this algorithm. Let X be the variables appearing in the set S
maintained by VE MPE, Y be all other (eliminated) variables, and f be the product of factors
in S. Then f (x) = Pr(x, f [x], e) and f (x) = maxy Pr(x, y, e). Show that this invariant holds
before the algorithm reaches Line 5 and remains true after each iteration of Lines 6–8.
10.11. Extend Algorithm VE MPE so it returns a structure  from which all MPE solutions can be
enumerated in time linear in their count and linear in the size of . The extended algorithm
should have the same complexity as VE MPE. Hint: Appeal to NNF circuits.
10.12. Prove Lemma 10.1 on Page 267.
10.13. Let N be a network with treewidth w and let N′ be another network that results from splitting
m variables in N. Show that w′ ≥w −m, where w′ is the treewidth of network N′. This
means that to reduce the treewidth of network N by m, we must split at least m variables.
10.14. Let N be a Bayesian network and let C be a corresponding loop cutset. Provide tight lower
and upper bounds on the treewidth of a network that results from splitting variables C in N.
Note: A variable C ∈C can be split according to any number of children.
10.15. Suppose that we have a Bayesian network N with a corresponding jointree J . Let N′ be a
network that results from splitting some variable X in N according to its children (i.e., one
clone is introduced). Show how we can obtain a jointree for network N′ by modifying the
clusters of jointree J . Aim for the best jointree possible (minimize its width). Show how this
technique can be used to develop a greedy method for obtaining a split network that has a
particular width.
10.16. Consider a Bayesian network N, a corresponding jointree J , and assume that the CPTs
for network N have been assigned to clusters in J . Consider a separator Sij that contains
variable X and suppose that the CPT of variable X has been assigned to a cluster on the
j-side of edge i−j. Consider now a transformation that produces a new jointree J ′ as follows:
r
Replace all occurrences of variable X by ˆX on the i-side of edge i−j in the jointree.
r
Remove all occurrences of X and ˆX from clusters that have not been assigned CPTs
mentioning X, as long as the removal will not destroy the jointree property.
Describe a network N′ that results from splitting variables in N for which J ′ would be a valid
jointree.
10.17. Let N′ be a network that results from splitting nodes in network N, and let Pr′ and Pr be their
corresponding distributions. Show that
Pr(e) ≤β · Pr′(e, ˆe),
where ˆe is the instantiation of clone variables that is implied by evidence e and β is the split
cardinality.
10.18. Prove Theorem 10.3.
10.19. Consider the classical jointree algorithm as deﬁned by Equations 7.4 and 7.5 in Chapter 7:
Mij =

Ci\Sij
i

k̸=j
Mki
fi(Ci) = i

k
Mki.
Suppose that we replace all summations by maximization when computing messages,
Mij = max
Ci\Sij i

k̸=j
Mki.
What are the semantics of the factor fi(Ci) in this case? In particular, can we use it to
recover answers to MPE queries?

10.5 PROOFS
267
10.20. Consider the classical jointree algorithm as deﬁned by Equations 7.4 and 7.5 in Chapter 7:
Mij =

Ci\Sij
i

k̸=j
Mki
fi(Ci) = i

k
Mki.
Suppose that we replace all summations over MAP variables M by maximization when
computing messages as follows:
Mij =
max
(Ci\Sij )∩M

(Ci\Sij )\M
i

k̸=j
Mki.
What are the semantics of the factor fi(Ci) in this case? In particular, can we use it to
recover answers relating to MAP queries?
10.21. Consider Line 3 of Algorithm 31 and let
Bx = MAPu
P (X \ {X}, ix).
Consider now the following heuristics:
r
On Line 4, choose variable X that maximizes MX/TX, where
MX = max
x
Bx
TX =

Bx≥p
Bx,
and p is the probability on Line 2 of Algorithm 31.
r
On Line 5, choose values x in decreasing order of Bx.
Implement a version of Algorithm 31 that employs these heuristics and compare its perfor-
mance with and without the heuristics. Note: The jointree algorithm can be used to compute
the bounds Bx efﬁciently (see Exercise 10.20).
10.22. We can modify Algorithm 13, RC1, from Chapter 8 to answer MPE queries by replacing
summation with maximization, leading to RC MPE shown in Algorithm 33. This algorithm
computes the MPE probability but not the MPE instantiations.
(a) How can we modify Algorithm RC MPE so that it returns the number of MPE instantiations?
(b) How can we modify Algorithm RC MPE so that it returns an NNF circuit that encodes the set
of MPE instantiations, that is, the circuit models are precisely the MPE instantiations?
10.5 Proofs
Lemma 10.1. Let N′ be the network resulting from splitting variables in network N. Let
Pr′ and Pr be their corresponding distributions. We then have
Pr(x) = β · Pr′(x, ˆx),
where X are the variables of network N, ˆx is an instantiation of all clone variables that
is implied by x, and β is the split cardinality.
PROOF. Left to Exercise 10.12.
■
PROOF OF THEOREM 10.2. Let N′ be the network resulting from splitting variables in
network N. Let Pr′ and Pr be their corresponding distributions. Suppose that
MPEP (e) > β · MPE′
P (e, ˆe).

268
MOST LIKELY INSTANTIATIONS
Algorithm 33 RC MPE(T, e)
input:
T :
dtree node
e:
evidence
output: MPE probability for evidence e
main:
1: if T is a leaf node then
2:
X|U←CPT associated with node T
3:
u←instantiation of parents U consistent with evidence e
4:
if X has value x in evidence e then
5:
return θx|u
6:
else
7:
return maxx θx|u
8:
end if
9: else
10:
p←0
11:
C←cutset(T )
12:
for each instantiation c consistent with evidence e, c ∼e do
13:
p←max(p,RC MPE(T l, ec)RC MPE(T r, ec))
14:
end for
15:
return p
16: end if
We must then have a variable instantiation x that is compatible with evidence e, where
Pr(x) > β · MPE′
P (e, ˆe).
By Lemma 10.1, we have
Pr(x) = β · Pr′(x, ˆx) > β · MPE′
P (e, ˆe).
This is a contradiction because MPE′
P (e, ˆe) is the MPE probability for network N′ and
because x is compatible with e and ˆx is compatible with ˆe.
■
PROOF OF THEOREM 10.3. Left to Exercise 10.18.
■
PROOF OF THEOREM 10.4. We are comparing here the two quantities

x
max
y
f (xyz) and max
y

x
f (xyz).
We ﬁrst note that maxy f (xyz) ≥f (xy⋆z) for any x and any y⋆. Summing over x, we get

x
max
y
f (xyz) ≥

x
f (xy⋆z).
This is true for any value y⋆. It is then true for the particular value that maximizes

x f (xyz). Hence,

x
max
y
f (xyz) ≥max
y

x
f (xyz).

10.5 PROOFS
269
Note here that the equality holds only when there is some value y⋆of variable Y such that
f (xy⋆z) = maxy f (xyz) for all values x of variable X. That is, for a given z, the optimal
value of variable Y is independent of variable X.
■
PROOF OF THEOREM 10.5. Note ﬁrst that from any elimination order π, a valid MAP
order π′ can be produced by successively commuting a MAP variable M with a non-MAP
variable S whenever variable M is immediately before the variable S in the order. For
example, consider MAP variables X, Y, and Z, non-MAP variables A, B, and C, and the
order π0 = A, X, Y, B, C, Z. This is not a valid order for MAP as the MAP variables do
not appear last. Yet we can convert this order to the valid order π4 = A, B, C, X, Y, Z
using the following commutations:
r Initial order: π0 = AXYBCZ
r
YB
BY →π1 = AXBYCZ
r
YC
CY →π2 = AXBCYZ
r
XB
BX→π3 = ABXCYZ
r
XC
CX→π4 = ABCXYZ
r Final MAP order: π4
Given Theorem 10.4, applying Algorithm VE MAP to each of these orders will produce a
number that is guaranteed to be no less than the number produced by the order following
it in the sequence. For example, the ﬁrst two orders, π0 and π1, give
⎛
⎜⎝max
Z

C

B
max
Y
  
max
X

A
f
⎞
⎟⎠(⊤) ≥
⎛
⎜⎝max
Z

C
max
Y

B
  
max
X

A
f
⎞
⎟⎠(⊤).
Note that the number produced by the last order is the MAP probability. Hence, if we use
the invalid order π0 instead of the valid order π4, we obtain a number q that is an upper
bound on the MAP probability: MAPP (M, e) ≤q.
For the second part of the theorem, note that for a factor f over disjoint variables X
and Z, we have

X
f

(z) ≥
"
max
X f
#
(z).
Therefore, if we replace all maximizations by summations in Algorithm VE MAP, we
then produce a number greater than q. Moreover, now that all variables are eliminated
by summation, the resulting number must be Pr(e); hence, q ≤Pr(e). We therefore have
MAPP (M, e) ≤q ≤Pr(e).
■

11
The Complexity of Probabilistic Inference
We consider in this chapter the computational complexity of probabilistic inference. We
also provide some reductions of probabilistic inference to well known problems, allowing
us to beneﬁt from specialized algorithms that have been developed for these problems.
11.1 Introduction
In previous chapters, we discussed algorithms for answering three types of queries with
respect to a Bayesian network that induces a distribution Pr(X). In particular, given some
evidence e we discussed algorithms for computing:
r The probability of evidence e, Pr(e) (see Chapters 6–8)
r The MPE probability for evidence e, MPEP (e) (see Chapter 10)
r The MAP probability for variables Q and evidence e, MAPP (Q, e) (see Chapter 10).
In this chapter, we consider the complexity of three decision problems that correspond to
these queries. In particular, given a number p, we consider the following problems:
r D-PR: Is Pr(e) > p?
r D-MPE: Is there a network instantiation x such that Pr(x, e) > p?
r D-MAP: Given variables Q ⊆X, is there an instantiation q such that Pr(q, e) > p?
We also consider a fourth decision problem that includes D-PR as a special case:
r D-MAR: Given variables Q ⊆X and instantiation q, is Pr(q|e) > p?
Note here that when e is the trivial instantiation, D-MAR reduces to asking whether
Pr(q) > p, which is identical to D-PR.
We provide a number of results on these decision problems in this chapter. In particular,
we show in Sections 11.2–11.4 that D-MPE is NP-complete, D-PR and D-MAR are PP-
complete, and D-MAP is NPPP-complete. We start in Section 11.2 with a review of these
complexity classes and some of the prototypical problems that are known to be complete
for them. We then show the hardness of the problems in Section 11.3 and their membership
in Section 11.4.
Even though the decisions problems D-MPE, D-PR, and D-MAR are intractable in
general, they can all be solved in polynomial time on networks whose treewidth is bounded.
In Section 11.5, we show that D-MAP is NP-complete on polytrees with no more than
two parents per node, that is, polytrees with treewidth ≤2.
The proofs we provide for various completeness results include reductions that can be
quite useful in practice. In fact, we also provide additional reductions in Sections 11.6
and 11.7 that form the basis of state-of-the-art inference algorithms for certain classes in
Bayesian networks. In particular, we show in Section 11.6 how to reduce the probability
270

11.2 COMPLEXITY CLASSES
271
of evidence to a weighted model count on a CNF sentence. We then show in Section 11.7
how to reduce MPE to weighted MAXSAT, which is also a problem applied to CNF
sentences.
11.2 Complexity classes
Proving that a problem P is complete for a particular complexity class C shows that P is
among the hardest problems in this class. That is, if we know how to solve this problem,
then we would know how to solve every other problem in the class. Proving completeness
is therefore accomplished by proving two properties:
r Hardness: The problem P is as hard as any problem in the class C.
r Membership: The problem P is a member of the class C.
We show hardness by choosing a problem P ′ that is known to be complete for the class C
and then provide an efﬁcient reduction from P ′ to P. Intuitively, this shows that a box that
solves P can also be used to solve P ′ and, hence, any member of the class C since P ′ is
complete for class C. Note that hardness shows that problem P is as hard as any problem
in class C but it leaves the possibility that P is actually harder than every problem in class
C. Showing that problem P is a member of class C rules out this possibility.
To carry out our complexity proofs, we therefore need to choose problems that are
known to be complete for the classes NP, PP, and NPPP. The following deﬁnition provides
three such problems. In this deﬁnition and in the rest of the chapter, we refer to an
instantiation x1, . . . , xn of a set of Boolean variables X1, . . . , Xn on which a propositional
sentence α is deﬁned. Each value xi can be viewed as assigning a truth value to variable
Xi. In particular, xi can be viewed as assigning the value true to Xi, in which case it will
correspond to the positive literal Xi, or assigning the value false to Xi, in which case
it will correspond to the negative literal ¬Xi. In this sense, the instantiation x1, . . . , xn
corresponds to a truth assignment (or a world), hence it is meaningful to talk about about
whether the instantiation x1, . . . , xn will satisfy sentence α or not (see Chapter 2 for a
review of satisfaction).
Deﬁnition 11.1. Given a propositional sentence α over Boolean variables X1, . . . , Xn,
the following decision problems are deﬁned:
r SAT: Is there a variable instantiation x1, . . . , xn that satisﬁes α?
r MAJSAT: Do the majority of instantiations over variables X1, . . . , Xn satisfy α?
r E-MAJSAT: Given some 1 ≤k ≤n, is there an instantiation x1, . . . , xk for which
the majority of instantiations x1, . . . , xk, xk+1, . . . , xn satisfy α?
■
These decision problems are known to be complete for the complexity classes of interest
to us. In particular, SAT is NP-complete, MAJSAT is PP-complete, and E-MAJSAT is
NPPP-complete. This also holds when the propositional sentence is restricted to be in
CNF. Recall that a CNF is a conjunction of clauses α1 ∧. . . ∧αn where each clause is a
disjunction of literals ℓ1 ∨. . . ℓm and each literal is a variable Xi or its negation ¬Xi.
Intuitively, to solve an NP-complete problem we have to search for a solution among
an exponential number of candidates where it is easy to decide whether a given candidate
constitutes a solution. For example, in SAT we are searching for a truth assignment that
satisﬁes a sentence (testing whether a truth assignment satisﬁes a sentence can be done
in time linear in the sentence size). Similarly, in D-MPE we are searching for a network

272
THE COMPLEXITY OF PROBABILISTIC INFERENCE
instantiation that is compatible with the evidence and has a probability greater than some
threshold. Note here that a network instantiation is a variable instantiation that assigns a
value to each variable in the Bayesian network. Hence, computing the probability of a
network instantiation can be performed in time linear in the Bayesian network size using
the chain rule for Bayesian networks.
Intuitively, to solve a PP-complete problem we have to add up the weights of all solu-
tions where it is easy to decide whether a particular candidate constitutes a solution and
it is also easy to compute the weight of a solution. For example, in MAJSAT a solution
is a truth assignment that satisﬁes the sentence and the weight of a solution is 1. And
in D-PR a solution is a network instantiation that is compatible with evidence and the
weight of a solution is its probability, which is easy to compute. Another characteriza-
tion of PP-complete problems is as problems that permit polynomial time, randomized
algorithms that can guess a solution to the problem while being correct with a probability
greater than .5. Therefore, problems that are PP-complete can be solved to any ﬁxed
degree of accuracy by running a randomized, polynomial-time algorithm a sufﬁcient (but
unbounded) number of times. We use this particular characterization in one of our proofs
later.
Finally, to solve an NPPP-complete problem we have to search for a solution among an
exponential number of candidates but we need to solve a PP-complete problem to decide
whether a particular candidate constitutes a solution. For example, in E-MAJSAT we
are searching for an instantiation x1, . . . , xk but to test whether an instantiation satisﬁes
the condition we want, we must solve a MAJSAT problem. Moreover, in D-MAP we
are searching for a variable instantiation x1, . . . , xk that is compatible with evidence and
has a probability that exceeds a certain threshold. But to compute the probability of an
instantiation x1, . . . , xk, we need to solve a D-PR problem.1
The class NP is included in the class PP, which is also included in the class NPPP.
Moreover, these classes are strongly believed to be distinct. This distinction implies that
D-MPE is strictly easier than D-PR, which is then strictly easier than D-MAP. This, for
example, suggests that the use of variable elimination to solve D-MPE, as in Chapter 10, is
actually an overkill since the amount of work that variable elimination does is sufﬁcient to
solve a harder problem, D-PR. It also suggests that the additional penalty we incur when
using variable elimination to solve D-MAP is due to the intrinsic difﬁculty of D-MAP, as
opposed to an idiosyncrasy of the variable elimination method.
11.3 Showing hardness
Suppose that we have the following propositional sentence:
α : (X1 ∨X2 ∨¬X3) ∧((X3 ∧X4) ∨¬X5).
(11.1)
We show in this section that we can solve SAT, MAJSAT, and E-MAJSAT for this and
similar sentences by constructing a corresponding Bayesian network, Nα, and reducing the
previous queries to D-MPE, D-PR, and D-MAP queries, respectively, on the constructed
network. Figure 11.1 depicts the Bayesian network structure corresponding to the sentence
α given in (11.1).
1 Note, however, that if the instantiation x1, . . . , xk is complete, that is, covers every variable in the network, then
computing its probability can be performed in linear time.

11.3 SHOWING HARDNESS
273
1
X
2
X
3
X
4
X
5
X
∧
∧
∨
∨
¬
S
¬
Figure 11.1: A Bayesian network representing a propositional sentence.
More generally, the Bayesian network Nα for a propositional sentence α has a single
leaf node Sα and is constructed inductively as follows:
r If α is a single variable X, its Bayesian network has a single binary node Sα = X with the
following CPT: θx = 1/2 and θ¯x = 1/2.
r If α is of the form ¬β, then its Bayesian network is obtained from the Bayesian network
for β by adding the node Sα with parent Sβ and CPT (we omit redundant CPT rows):
Sβ
Sα
Sα|Sβ
true
true
0
false
true
1
r If α is of the form β ∨γ , then its Bayesian network is obtained from the Bayesian
networks for β and γ by adding the node Sα with parents Sβ and Sγ and CPT:
Sβ
Sγ
Sα
Sα|Sβ,Sγ
true
true
true
1
true
false
true
1
false
true
true
1
false
false
true
0
r If α is of the form β ∧γ , then its Bayesian network is obtained from the Bayesian
networks for β and γ by adding the node Sα with parents Sβ and Sγ and CPT:
Sβ
Sγ
Sα
Sα|Sβ,Sγ
true
true
true
1
true
false
true
0
false
true
true
0
false
false
true
0
Suppose now that we have a propositional sentence α over variables X1, . . . , Xn and let
Nα be a corresponding Bayesian network with leaf node Sα. The probability distribution
Pr induced by network Nα satisﬁes the following key property.
Theorem 11.1. We have
Pr(x1, . . . , xn, Sα =true) =

0,
if x1, . . . , xn |= ¬α
1/2n
if x1, . . . , xn |= α.
(11.2)
■

274
THE COMPLEXITY OF PROBABILISTIC INFERENCE
This theorem will be the basis for three reductions that we show next.
Theorem 11.2 (Reducing SAT to D-MPE). There is a variable instantiation x1, . . . , xn
that satisﬁes sentence α iff there is a variable instantiation y of network Nα such that
Pr(y, Sα =true) > 0.
■
Theorem 11.3 (Reducing MAJSAT to D-PR). The majority of variable instantia-
tions x1, . . . , xn satisfy α iff the probability of evidence Sα =true is greater than 1/2:
Pr(Sα =true) > 1/2.
■
This reduction implies that MAJSAT can be reduced to D-MAR since D-PR is a special
case of D-MAR.
Theorem 11.4 (Reducing E-MAJSAT to D-MAP). There is a variable instantiation
x1, . . . , xk, 1 ≤k ≤n, for which the majority of instantiations x1, . . . , xk, xk+1, . . . , xn
satisfy α iff there is a MAP instantiation x1, . . . , xk such that Pr(x1, . . . , xk, Sα =true) >
1/2k+1.
■
11.4 Showing membership
In the previous section, we showed that D-MPE is NP-hard, D-PR and D-MAR are
PP-hard, and D-MAP is NPPP-hard. To show the completeness of these problems with
respect to the mentioned classes, we need to establish the membership of each problem
in the corresponding class. Establishing membership of D-MPE and D-MAP is relatively
straightforward but establishing the membership of D-PR and D-MAR are more involved,
so we handle them last.
Membership of D-MPE in NP
To show that D-MPE belongs to the class NP, all we have to show is that we can verify a
potential solution for this problem in polynomial time. That is, suppose we are given an
instantiation x of network variables and we want to check whether Pr(x, e) > p for some
evidence e and threshold p. If x is inconsistent with e, then Pr(x, e) = 0. If x is consistent
with e, then xe = x and Pr(x) can be computed in polynomial time using the chain rule of
Bayesian networks. We can therefore perform the test Pr(x, e) > p in polynomial time,
and D-MPE is then in the class NP.
Membership of D-MAP on polytrees in NP
The problem of D-MAP on polytrees with no more than two parents per node is also
in the class NP. In this problem, verifying a solution corresponds to checking whether
Pr(q, e) > p for a partial instantiation q of the network variables. We cannot use the
chain rule to compute Pr(q, e) since q, e does not instantiate all network variables.
Yet Pr(q, e) can be computed using variable elimination in polynomial time since the
network treewidth is bounded by 2. This shows the membership of D-MAP in NP for
polytrees.
Membership of D-MAP in NPPP
To show that general D-MAP belongs to the class NPPP will require that we verify a
potential solution to the problem in polynomial time, assuming that we have a PP-oracle.

11.5 COMPLEXITY OF MAP ON POLYTREES
275
Since verifying a potential D-MAP amounts to testing whether Pr(q, e) > p, which is
PP-complete, we can verify a potential D-MAP solution in polynomial time given the
oracle.
Membership of D-MAR in PP
To show that D-MAR is in the class PP, we present a polynomial time algorithm that
can guess a solution to D-MAR while guaranteeing that the guess will be correct with
probability greater than .5. The algorithm for guessing whether Pr(q|e) > p is as follows:
1. Deﬁne the following probabilities as a function of the threshold p:
a(p) =
1,
if p < .5
1/(2p),
otherwise
b(p) =
(1 −2p)/(2 −2p),
if p < .5
0,
otherwise.
2. Sample a variable instantiation x from the Bayesian network as given in Section 15.2.
This can be performed in time linear in the network size.
3. Declare Pr(q|e) > p according to the following probabilities:
r a(p) if the instantiation x is compatible with e and q
r b(p) if the instantiation x is compatible with e but not with q
r .5 if the instantiation x is not compatible with e.
Theorem 11.5. The previous procedure will declare Pr(q|e) > p correctly with proba-
bility greater than .5.2
■
This theorem shows that D-MAR is in the class PP and, consequently, that D-PR is
also in the class PP.
11.5 Complexity of MAP on polytrees
We saw in Chapters 6 and 10 that networks with bounded treewidth can be solved in
polynomial time using variable elimination to answer both probability of evidence and
MPE queries. We also saw that variable elimination may take exponential time on such
networks when answering MAP queries. We show in this section that this difﬁculty is
probably due to an intrinsic property of the MAP problem, as opposed to some idiosyncrasy
of the variable elimination algorithm. In particular, we show that D-MAP is NP-complete
for polytree networks with no more than two parents per node (treewidth ≤2). Therefore,
even though the complexity of D-MAP is improved for polytrees (NP-complete instead
of NPPP-complete), the problem remains intractable for this class of networks.
Membership in NP was shown in the previous section. To show NP-hardness, we reduce
the MAXSAT problem, which is NP-complete, to D-MAP.
Deﬁnition 11.2 (MAXSAT). Given a set of clauses α1, . . . , αm over propositional
variables X1, . . . , Xn and an integer 0 ≤k < m, is there an instantiation x1, . . . , xn that
satisﬁes more than k of the clauses αi?
■
2 This theorem and its proof are due to James D. Park.

276
THE COMPLEXITY OF PROBABILISTIC INFERENCE
S1
S2
S3
Sn
X1
X2
X3
Xn
S0
Figure 11.2: Structure used for reducing MAXSAT to MAP on polytrees.
The idea behind the reduction is to use a circuit whose structure is given in Figure 11.2.
The circuit has n + 1 inputs. Input S0 has the values 1, . . . , m and is used to select a
clause from the set α1, . . . , αm (S0 = i selects clause αi). The other n inputs correspond
to the propositional variables over which the clauses are deﬁned, with each variable
having two values true and false. The intermediate variables Sj, 1 ≤j ≤n have values in
0, 1, . . . , m. The semantics of these intermediate variables is as follows. Suppose we set
S0 = i, therefore selecting clause αi. Suppose also that we set the propositional variables
X1, . . . , Xn to some values x1, . . . , xn. The behavior of the circuit is such that Sj will take
the value 0 iff at least one of the values in x1, . . . , xj satisﬁes the clause αi. If none of
these values satisfy the clause, then Sj will take the value i. This is given by
Pr(sj|xj, sj−1) =
⎧
⎪⎪⎨
⎪⎪⎩
1,
if sj−1 = sj = 0
1,
if sj−1 = i, sj = 0 and xj satisﬁes αi
1,
if sj−1 = sj = i and xj does not satisfy αi
0,
otherwise.
If we further assume that we have uniform priors for variable S0, Pr(S0 = i) = 1/m,
and uniform priors on inputs X1, . . . , Xn, Pr(xi) = .5, we get the following reduction.
Theorem 11.6 (Reducing MAXSAT to D-MAP). There is an instantiation x1, . . . , xn
that satisﬁes more than k of the clauses α1, . . . , αm iff there is an instantiation x1, . . . , xn
such that Pr(x1, . . . , xn, Sn =0) > k/(m2n).
■
This completes our proof that D-MAP is NP-complete for polytree networks with no more
than two parents per node.
We present two more reductions in the rest of this chapter, allowing us to cast proba-
bilistic inference on Bayesian networks in terms of inference on CNFs. In particular, we
start in the following section by showing how we can compute the probability of evidence
by solving the problem of weighted model counting on CNFs. We then follow by show-
ing how we can compute the MPE probability (and MPE instantiations) by solving the
problem of weighted MAXSAT on CNFs. We present these reductions with the goal of
capitalizing on state-of-the-art algorithms for CNF inference.
11.6 Reducing probability of evidence to weighted
model counting
We show in this section how we can compute the probability of evidence with respect
to a Bayesian network by computing the weighted model count of a corresponding

11.6 REDUCING PROBABILITY OF EVIDENCE
277
propositional sentence in CNF. We present two reductions for this purpose, each producing
a different CNF encoding of the Bayesian network. One of these encodings will also be
used in Chapter 12 when we discuss the compilation of Bayesian networks, and again in
Chapter 13 when we discuss inference algorithms that exploit local structure.
We start by deﬁning the weighted model counting problem.
Deﬁnition 11.3 (Weighted model counting, WMC). Let  be a propositional sentence
over Boolean variables X1, . . . , Xn and let Wt be a function that assigns a weight
Wt(xi) ≥0 to each value xi of variable Xi. The weighted model count of  is deﬁned
as the sum of weights assigned to its models:
WMC()
def=

x1,...,xn|=
Wt(x1, . . . , xn),
where
Wt(x1, . . . , xn)
def=
n

i=1
Wt(xi).
■
We next describe two CNF encodings of Bayesian networks, allowing us to reduce the
probability of evidence to a WMC. In particular, given a Bayesian network N that induces
a distribution Pr and given evidence e, we show how to systematically construct a CNF
 such that Pr(e) = WMC().
11.6.1 The ﬁrst encoding
This CNF encoding employs two types of Boolean variables: indicators and parameters.
In particular, for each network variable X with parents U, we have
r A Boolean variable Ix, called an indicator variable, for each value x of network vari-
able X.
r A Boolean variable Px|u, called a parameter variable, for each instantiation xu of the
family XU.
The network in Figure 11.3 generates the following indicator variables,
Ia1, Ia2,
Ib1, Ib2,
Ic1, Ic2,
and the following parameter variables,
Pa1, Pa2,
Pb1|a1, Pb2|a1, Pb1|a2, Pb2|a2,
Pc1|a1, Pc2|a1, Pc1|a2, Pc2|a2.
A
B
C
A
A
a1
.1
a2
.9
A
B
B|A
a1
b1
.1
a1
b2
.9
a2
b1
.2
a2
b2
.8
A
C
C|A
a1
c1
.1
a1
c2
.9
a2
c1
.2
a2
c2
.8
Figure 11.3: A Bayesian network.

278
THE COMPLEXITY OF PROBABILISTIC INFERENCE
Table 11.1: A CNF encoding of the Bayesian network in
Figure 11.3.
Indicator Clauses
A
Ia1 ∨Ia2
¬Ia1 ∨¬Ia2
B
Ib1 ∨Ib2
¬Ib1 ∨¬Ib2
C
Ic1 ∨Ic2
¬Ic1 ∨¬Ic2
Parameter Clauses
A
Ia1 ⇐⇒Pa1
B
Ia1 ∧Ib1 ⇐⇒Pb1|a1
Ia1 ∧Ib2 ⇐⇒Pb2|a1
Ia2 ∧Ib1 ⇐⇒Pb1|a2
Ia2 ∧Ib2 ⇐⇒Pb2|a2
C
Ia1 ∧Ic1 ⇐⇒Pc1|a1
Ia1 ∧Ic2 ⇐⇒Pc2|a1
Ia2 ∧Ic1 ⇐⇒Pc1|a2
Ia2 ∧Ic2 ⇐⇒Pc2|a2
CNF clauses are also of two types: indicator clauses and parameter clauses. A set of
indicator clauses is generated for each variable X with values x1, x2, . . . , xk as follows:
Ix1 ∨Ix2 ∨. . . ∨Ixk
¬Ixi ∨¬Ixj ,
for i < j.
(11.3)
These clauses ensure that exactly one indicator variable for variable X will be true. The
network in Figure 11.3 generates the indicator clauses given in Table 11.1.
A set of clauses are also generated for each variable X and its parameter variable
Px|u1,u2,...,um. These include an IP clause,
Iu1 ∧Iu2 ∧. . . ∧Ium ∧Ix =⇒Px|u1,u2,...,um,
and a set of PI clauses,
Px|u1,u2,...,um =⇒Ix
Px|u1,u2,...,um =⇒Iui,
for i = 1, . . . , m.
These clauses ensure that a parameter variable is true if and only if the correspond-
ing indicator variables are true. We typically write these parameter clauses as one
equivalence:
Iu1 ∧Iu2 ∧. . . ∧Ium ∧Ix ⇐⇒Px|u1,u2,...,um.
(11.4)
The network in Figure 11.3 generates the parameter clauses given in Table 11.1. We now
have the following reduction.
Theorem 11.7. Let N be a Bayesian network inducing probability distribution Pr and let
N be its CNF encoding given by (11.3) and (11.4). For any evidence e = e1, . . . , ek, we
have
Pr(e) = WMC(N ∧Ie1 ∧. . . ∧Iek),
given the following weights: Wt(Ix) = Wt(¬Ix) = Wt(¬Px|u) = 1 and Wt(Px|u) =
θx|u.
■

11.6 REDUCING PROBABILITY OF EVIDENCE
279
Table 11.2: Network instantiations and corresponding truth assignments
for the network in Figure 11.3.
Network
Truth assignment sets these variables
Weight of
instantiation
to true and all others to false
truth assignment
a1b1c1
ω0 : Ia1 Ib1 Ic1 Pa1 Pb1|a1 Pc1|a1
.1 · .1 · .1 = .001
a1b1c2
ω1 : Ia1 Ib1 Ic2 Pa1 Pb1|a1 Pc2|a1
.1 · .1 · .9 = .009
a1b2c1
ω2 : Ia1 Ib2 Ic1 Pa1 Pb2|a1 Pc1|a1
.1 · .9 · .1 = .009
a1b2c2
ω3 : Ia1 Ib2 Ic2 Pa1 Pb2|a1 Pc2|a1
.1 · .9 · .9 = .081
a2b1c1
ω4 : Ia2 Ib1 Ic1 Pa2 Pb1|a1 Pc1|a2
.9 · .2 · .2 = .036
a2b1c2
ω5 : Ia2 Ib1 Ic2 Pa2 Pb1|a1 Pc2|a2
.9 · .2 · .8 = .144
a2b2c1
ω6 : Ia2 Ib2 Ic1 Pa2 Pb2|a1 Pc1|a2
.9 · .8 · .2 = .144
a2b2c2
ω7 : Ia2 Ib2 Ic2 Pa2 Pb2|a1 Pc2|a2
.9 · .8 · .8 = .576
To get some intuition on why Theorem 11.7 holds, consider the CNF N in Table 11.1
that encodes the network N in Figure 11.3. There is a one-to-one correspondence bet-
ween the models of this CNF and the network instantiations, as shown in Table 11.2.
Moreover, the weight of each model is precisely the probability of corresponding network
instantiation. Consider now the evidence e = a1c2. By conjoining Ia1 ∧Ic2 with N, we
are then dropping all CNF models that are not compatible with evidence e (see Table 11.2).
We then get
WMC(N ∧Ia1 ∧Ic2) = Wt(ω1) + Wt(ω3) = .009 + .081 = .09 = Pr(e).
11.6.2 The second encoding
In this section, we discuss another CNF encoding of Bayesian networks that allows us to
reduce the probability of evidence computation to a model count. This second encoding is
somewhat less transparent semantically than the previous one, but it produces CNFs that
have a smaller number of variables and clauses. However, the size of generated clauses
may be larger in the presence of multivalued variables.
The encoding assumes some ordering on the values of each network variable X, writing
x′ < x to mean that value x′ comes before x in the ordering. The encoding uses variables
of two types, indicators and parameters, deﬁned as follows. For each network variable X
with parents U, we have:
r A Boolean variable Ix, called an indicator variable, for each value x of variable X, which
is similar to the ﬁrst encoding.
r A Boolean variable Qx|u, called a parameter variable, for each instantiation xu of the
family XU, assuming that x is not last in the value order of variable X. Note that
these parameter variables do not correspond to those used in the ﬁrst encoding. In
particular, we do not have a parameter variable for each instantiation xu of the fam-
ily XU. Moreover, the semantics of these parameter variables are different, as we see
later.
The network in Figure 11.4 generates the following parameter variables:
Qa1, Qa2,
Qb1|a1, Qb1|a2, Qb1|a3.

280
THE COMPLEXITY OF PROBABILISTIC INFERENCE
A
B
A
A
a1
.3
a2
.5
a3
.2
A
B
B|A
a1
b1
.2
a1
b2
.8
a2
b1
1
a2
b2
0
a3
b1
.6
a3
b2
.4
Figure 11.4: A Bayesian network.
CNF clauses are also of two types: indicator clauses and parameter clauses. A set of
indicator clauses is generated for each variable X with values x1, x2, . . . , xk, as in the ﬁrst
encoding:
Ix1 ∨Ix2 ∨. . . ∨Ixk
¬Ixi ∨¬Ixj ,
for i < j.
(11.5)
Suppose now that x1 < x2 < · · · < xk is an ordering of X’s values and let u = u1, . . . , um
be an instantiation of X’s parents. A set of parameter clauses is then generated for variable
X as follows:
Iu1 ∧. . . ∧Ium ∧¬Qx1|u ∧. . . ∧¬Qxi−1|u ∧Qxi|u =⇒Ixi,
if i < k
Iu1 ∧. . . ∧Ium ∧¬Qx1|u ∧. . . ∧¬Qxk−1|u =⇒Ixk.
(11.6)
The network in Figure 11.4 generates the clauses given in Table 11.3. We now have the
following result.
Theorem 11.8. Let N be a Bayesian network inducing probability distribution Pr and let
N be its CNF encoding given by (11.5) and (11.6). For any evidence e = e1, . . . , ek, we
have
Pr(e) = WMC(N ∧Ie1 ∧. . . ∧Iek),
given the following weights:
Wt(Ix) = 1
Wt(¬Ix) = 1
Wt(Qx|u) =
θx|u
1 −
x′<x θx′|u
Wt(¬Qx|u) = 1 −Wt(Qx|u).
■
For example, considering the encoding in Table 11.3, we have the following weights:
Wt(Qa1) = θa1 = .3
Wt(Qa2) =
θa2
1 −θa1
= .5
.7
11.7 Reducing MPE to W-MAXSAT
In this section, we consider a reduction of MPE into a problem known as weighted
MAXSAT (W-MAXSAT). This problem applies to a weighted CNF of the form

11.7 REDUCING MPE TO W-MAXSAT
281
Table 11.3: A CNF encoding of the Bayesian network in Figure 11.4. We are assuming
here that a1 < a2 < a3 and b1 < b2.
Indicator Clauses
A
Ia1 ∨Ia2 ∨Ia3
¬Ia1 ∨¬Ia2
¬Ia1 ∨¬Ia3
¬Ia2 ∨¬Ia3
B
Ib1 ∨Ib2
¬Ib1 ∨¬Ib2
Parameter Clauses
A
Qa1 =⇒Ia1
¬Qa1 ∧Qa2 =⇒Ia2
¬Qa1 ∧¬Qa2 =⇒Ia3
B
Ia1 ∧Qb1|a1 =⇒Ib1
Ia1 ∧¬Qb1|a1 =⇒Ib2
Ia2 ∧Qb1|a2 =⇒Ib1
Ia2 ∧¬Qb1|a2 =⇒Ib2
Ia3 ∧Qb1|a3 =⇒Ib1
Ia3 ∧¬Qb1|a3 =⇒Ib2
αw1
1 , . . . , αwm
m , where each αi is a clause with weight wi ≥0. The following is an example
weighted CNF over Boolean variables X, Y, and Z:
(X ∨¬Y ∨¬Z)3, (¬X)10.1, (Y).5, (Z)2.5.
(11.7)
Deﬁnition 11.4 (W-MAXSAT). Let αw1
1 , . . . , αwm
m
be a weighted CNF over Boolean
variables X1, . . . , Xn. The weight of a truth assignment x1, . . . , xn is deﬁned as
Wt(x1, . . . , xn)
def=

x1,...,xn|=αi
wi.
The W-MAXSAT problem is that of ﬁnding a truth assignment with maximal weight. ■
Considering the weighted CNF in (11.7), the truth assignment X, Y, ¬Z has weight
3 + .5 = 3.5 since it satisﬁes the ﬁrst and third clauses but not the second and fourth.
Moreover, the assignment ¬X, ¬Y, Z has weight 3 + 10.1 + 2.5 = 15.6, which is maxi-
mal, that is, no other assignment will have a higher weight.
We ﬁnd it useful to deﬁne the penalty of a truth assignment as the weight of clauses
that it does not satisfy:
Pn(x1, . . . , xn)
def=

x1,...,xn̸|=αi
wi.
It then follows that
Wt(x1, . . . , xn) + Pn(x1, . . . , xn) = ψ,
where ψ is a constant and equals the sum of all weights appearing in the weighted CNF.
Finding a truth assignment with a minimal penalty is then the same as ﬁnding one with a
maximal weight, which allows us to deﬁne W-MAXSAT as the problem of ﬁnding a truth
assignment with minimum penalty.
We ﬁnd it useful to introduce a special weight, W, which we shall give to a clause
when we want to ensure that it is satisﬁed by every maximal truth assignment. Intuitively,
W is a very large weight that must be acquired by any truth assignment if it is optimal.
We later suggest a method for choosing this weight. Clauses that receive this special
weight are said to be hard clauses, while other clauses are said to be soft clauses.
We are now ready to show how MPE can be reduced to W-MAXSAT by encoding a
Bayesian network as a weighted CNF. The encoding employs only one type of Boolean

282
THE COMPLEXITY OF PROBABILISTIC INFERENCE
variables and two types of clauses. In particular, for each network variable X and value
x we have a Boolean variable Ix, called an indicator variable. CNF clauses are of two
types: indicator clauses and parameter clauses. A set of indicator clauses is generated for
each variable X with values x1, x2, . . . , xk, just as in the previous encodings:
(Ix1 ∨Ix2 ∨. . . ∨Ixk)W
(11.8)
(¬Ixi ∨¬Ixj )W,
for i < j.
(11.9)
Note that all indicator clauses are hard clauses. Hence, they must be satisﬁed by every
maximal truth assignment.
In addition to indictor clauses, we have one parameter clause for each parameter
θx|u1,...,um:
(¬Ix ∨¬Iu1 ∨. . . ∨¬Ium)−log θx|u1,...,um ,
(11.10)
where −log 0 is deﬁned as W. Hence, if a parameter θx|u1,...,um equals zero, it will generate
a hard clause.
Now that we have deﬁned the set of all clauses generated by a Bayesian network, we
can set the special weight W to be greater than the sum of weights assigned to all soft
clauses. Hence, any truth assignment that violates even a single hard clause cannot be
optimal.3
The network in Figure 11.4 will generate the following weighted CNF:
r Indicator clauses:
(Ia1 ∨Ia2 ∨Ia3)W
(¬Ia1 ∨¬Ia2)W
(¬Ia1 ∨¬Ia3)W
(¬Ia2 ∨¬Ia3)W
(Ib1 ∨Ib2)W
(¬Ib1 ∨¬Ib2)W
r Parameter clauses:
(¬Ia1)−log .3
(¬Ia2)−log .5
(¬Ia3)−log .2
(¬Ia1 ∨¬Ib1)−log .2
(¬Ia1 ∨¬Ib2)−log .8
(¬Ia2 ∨¬Ib1)−log 1
(¬Ia2 ∨¬Ib2)W
(¬Ia3 ∨¬Ib1)−log .6
(¬Ia3 ∨¬Ib2)−log .4
Suppose now that we have a Bayesian network that induces a distribution
Pr(X1, . . . , Xn) and let αw1
1 , . . . , αwm
m
be the weighted CNF produced by this network
according to (11.8)–(11.10). Let x1, . . . , xn be a network instantiation. We say that a truth
assignment  corresponds to network instantiation x1, . . . , xn if and only if indictors
Ix1, . . . , Ixn appear positively in  and all other indicators appear negatively in . In the
previous example, the truth assignment
¬Ia1, ¬Ia2, Ia3, Ib1, ¬Ib2
then corresponds to the network instantiation a3b1. Given this deﬁnition, it should be clear
than any truth assignment that does not correspond to a network instantiation will have a
penalty ≥W as it must violate one of the indicator clauses in (11.8) and (11.9). Hence,
if a truth assignment has a minimal penalty (i.e., < W), it must correspond to a network
instantiation.
3 Note that the set of hard clauses generated by a Bayesian network is satisﬁable. Hence, there is at least one truth
assignment that satisﬁes each and every hard clause.

11.8 EXERCISES
283
Consider now a truth assignment  that corresponds to a network instantiation x and
let θx|u1,...,uk be a network parameter that is compatible with  and x, that is, the indicators
Ix, Iu1, . . . , Iuk appear positively in . Then  will violate the clause for parameter
θx|u1,...,uk and will therefore incur a penalty of −log θx|u1,...,uk. In fact, the total penalty of
truth assignment  will simply be the sum of −log θx|u over all parameters θx|u that are
compatible with  and x. We therefore have
Pn() =

θx|u∼x
−log θx|u = −log

θx|u∼x
θx|u = −log Pr(x).
That is, by choosing a truth assignment  with a minimum penalty, we are choosing a
network instantiation x that has a maximal probability. Interestingly enough, the reduction
given in this section continues to work even if we drop the indicator clauses given in (11.9)
(see Exercise 11.10). Dropping such clauses can have a dramatic effect on the encoding
size, especially when the cardinality of network variables is quite large.
We close this section by pointing out that evidence can be accommodated using the same
technique adopted in the previous encodings. That is, if the evidence is e = e1, . . . , ek,
all we have to do is add the hard clauses Ie1, . . . , Iek to the weighted CNF. This will rule
out any truth assignment that is not compatible with the evidence. For example, given the
evidence e = a1 in our previous example, the maximal truth assignment will be
Ia1, ¬Ia2, ¬Ia2, ¬Ib1, Ib2
which has a penalty of −log .8 −log .3 = −log(.8)(.3) = −Pr(a1, b2). The correspond-
ing network instantiation is a1, b2 in this case, which is then guaranteed to be an MPE
instantiation given evidence e = a1.
Bibliographic remarks
The complexity of D-PR was initially studied in Cooper [1990b], where it was shown
to be NP-hard, and then in Roth [1996], where the connection to model counting was
ﬁrst shown. The NP-hardness of D-MPE was shown in Shimony [1994] and the NPPP-
completeness of D-MAP was shown in Park and Darwiche [2004a]. That D-MAP remains
NP-complete even for polytrees was shown in Park and Darwiche [2004a]. The CNF en-
coding of Section 11.6.1 is due to Darwiche [2002], and the one in Section 11.6.2 is due to
Sang et al. [2005]. Reducing MPE to weighted MAXSAT is due to Park [2002].
11.8 Exercises
11.1. Prove Theorem 11.1.
11.2. Prove Theorem 11.2.
11.3. Consider the CPT in Table 11.4. Generate a CNF encoding for this CPT according to (11.3)
and (11.4). Show the weights of all variables in the encoding.
11.4. Consider the CPT in Table 11.4. Generate a CNF encoding for this CPT according to (11.5)
and (11.6). Show the weights of all variables in the encoding.
11.5. Consider Theorems 11.7 and 11.8. Provide a different weight function Wt(.) that depends
on evidence e for which the following would hold: Pr(e) = WMC(N).
11.6. Consider the CPT in Table 11.4. Generate a W-MAXSAT encoding for this CPT.

284
THE COMPLEXITY OF PROBABILISTIC INFERENCE
Table 11.4: A conditional probability table.
A
B
C
C|A,B
a1
b1
c1
.2
a1
b1
c2
.1
a1
b1
c3
.7
a1
b2
c1
0
a1
b2
c2
0
a1
b2
c3
1
a2
b1
c1
.5
a2
b1
c2
.2
a2
b1
c3
.3
a2
b2
c1
.2
a2
b2
c2
0
a2
b2
c3
.8
11.7. Consider the class of Bayesian networks in which every variable is binary and every nonroot
CPT is deterministic (that is, contains only zero/one parameters). Describe a corresponding
reduction to WMC with a CNF encoding that includes a single Boolean variable for each
network node, no variables for network parameters, and no more than one clause for each
network parameter.
11.8. Prove Theorem 11.8.
11.9. Show how we can extend Theorems 11.7 and 11.8 to handle a more general type of evidence
in which we may not know the exact value of a multivalued variable but only that some of its
values are impossible.
11.10. Show that we can drop the indicator clauses given in (11.9) without affecting the correctness
of the MPE to W-MAXSAT reduction given in Section 11.7.
11.9 Proofs
PROOF OF THEOREM 11.1. Left to Exercise 11.1.
■
PROOF OF THEOREM 11.2. Left to Exercise 11.2.
■
PROOF OF THEOREM 11.3. We ﬁrst note that
Pr(Sα =true)
=

x1,...,xn
Pr(x1, . . . , xn, Sα =true)
=

x1,...,xn|=α
Pr(x1, . . . , xn, Sα =true) +

x1,...,xn|=¬α
Pr(x1, . . . , xn, Sα =true)
= 1
2n c + 0
by (11.2)
= c
2n ,
where c is the number of instantiations x1, . . . , xn that satisfy the sentence α. Since
α has n variables, we have 2n instantiations of these variables. The majority of these
instantiations satisfy α precisely when c > 2n/2. This is equivalent to c/2n > .5, which
is also equivalent to Pr(Sα =true) > .5.
■

11.9 PROOFS
285
PROOF OF THEOREM 11.4. We ﬁrst note that
Pr(x1, . . . , xk, Sα =true)
=

xk+1,...,xn
Pr(x1, . . . , xk, xk+1, . . . , xn, Sα =true)
=

xk+1,...,xn
x1,...,xn|=α
Pr(x1, . . . , xk, xk+1, . . . , xn, Sα =true)
+

xk+1,...,xn
x1,...,xn|=¬α
Pr(x1, . . . , xk, xk+1, . . . , xn, Sα =true)
= 1
2n c + 0
by (11.2)
= c
2n ,
where c is the number of instantiations xk+1, . . . , xn for which instantiation x1, . . . ,
xk, xk+1, . . . , xn satisﬁes α. There are 2n−k instantiations of variables Xk+1, . . . , Xn.
A majority of these instantiations lead x1, . . . , xk, xk+1, . . . , xn to satisfy α precisely
when c > 2n−k/2. This is equivalent to c/2n > 1/2k+1, which is also equivalent to
Pr(x1, . . . , xk, Sα =true) > 1/2k+1.
■
PROOF OF THEOREM 11.5. The probability of declaring Pr(q|e) > p is given by
r = a(p)Pr(q, e) + b(p)Pr(¬q, e) + 1/2(1 −Pr(e))
= a(p)Pr(q, e) + b(p)Pr(¬q, e) + 1/2 −Pr(e)/2.
Therefore, r > .5 if and only if
a(p)Pr(q, e) + b(p)Pr(¬q, e) > Pr(e)/2,
which is equivalent to
a(p)Pr(q|e) + b(p)Pr(¬q|e) > .5.
Now consider two cases. If p < .5, we have the following equivalences:
a(p)Pr(q|e) + b(p)Pr(¬q|e) > .5
Pr(q|e) + (1 −2p)/(2 −2p)(1 −Pr(q|e)) > .5
Pr(q|e)(1 −(1 −2p)/(2 −2p)) > .5 −(1 −2p)/(2 −2p)
Pr(q|e)(1/(2 −2p)) > p/(2 −2p)
Pr(q|e) > p
If p ≥.5, we have the following equivalences:
a(p)Pr(q|e) + b(p)Pr(¬q|e) > .5
Pr(q|e)/(2p) > .5
Pr(q|e) > p
Therefore, r > .5 if and only if Pr(q|e) > p.
■

286
THE COMPLEXITY OF PROBABILISTIC INFERENCE
PROOF OF THEOREM 11.6. We ﬁrst make some observations on the network in Fig-
ure 11.2. Note that setting the input variables S0, X1, . . . , Xn will functionally deter-
mine all other variables Sj. Moreover, according to the circuit behavior, Sn = 0 pre-
cisely when the selected clause αi is satisﬁed by the input instantiation x1, . . . , xn.
More generally, if xl is the ﬁrst value in x1, . . . , xn that satisﬁes αi, then we must have
S1 = i, . . . , Sl−1 = i, Sl = 0, . . . , Sn = 0. And if no value in x1, . . . , xn satisﬁes αi, then
S1 = i, . . . , Sn = i.
We now have the following property of the network in Figure 11.2:
Pr(S0 =i, x1, . . . , xn, Sn =0) =
 (1/m)(.5)n,
if x1, . . . , xn satisﬁes clause αi
0,
otherwise.
Moreover,
Pr(x1, . . . , xn, Sn =0) =
m

i=1
Pr(S0 =i, x1, . . . , xn, Sn =0)
= c 1
m2n ,
where c is the number of clauses in α1, . . . , αm satisﬁed by the instantiation x1, . . . , xn.
Hence, more than k clauses are satisﬁed by this instantiation iff c > k, which is precisely
when Pr(x1, . . . , xn, Sn =0) > k/(m2n).
■
PROOF OF THEOREM 11.7. There is a one-to-one correspondence between the models
of CNF N and the variable instantiations of network N. To see this, note ﬁrst that there is
a one-to-one correspondence between the instantiations of indicator variables in the CNF
and instantiations of all variables in the Bayesian network (this follows from the indicator
clauses). Moreover, for every instantiation of indicator variables, there is a single ins-
tantiation of parameter variables that is implied by it (this follows from parameter clauses).
Finally, note that the model ω that corresponds to a network instantiation x sets to true
only those parameter variables that are compatible with the instantiation x. This implies
that the weight of a model ω for CNF N is precisely the probability of corresponding
network instantiation x. Moreover, by adding the indicators Ie1 ∧. . . ∧Iek to the CNF,
we eliminate all CNF models (network instantiations) incompatible with the evidence.
Hence, the weighted model count will be the sum of weights (probabilities) of models
(network instantiations) compatible with given evidence.
■
PROOF OF THEOREM 11.8. Left to Exercise 11.8.
■

12
Compiling Bayesian Networks
We discuss in this chapter the compilation of Bayesian networks into arithmetic circuits.
The compilation process takes place ofﬂine and is done only once per network. The
resulting arithmetic circuit can then be used to answer multiple online queries through a
simple process of circuit propagation.
12.1 Introduction
Consider the Bayesian network in Figure 12.1. We present an approach in this chapter
for compiling such a network into an arithmetic circuit using an ofﬂine procedure that is
applied only once. We then show how the compiled circuit can be used to answer multiple
online queries using a simple process of circuit propagation.
Figure 12.2 depicts a circuit compilation of the network in Figure 12.1. The circuit has
two types of inputs: the θ variables, which are called parameters, and the λ variables,
which are called indicators. Parameter variables are set according to the network CPTs,
while indicator variables are set according to the given evidence. Once the circuit inputs
are set, it can be evaluated using a bottom-up pass, which proceeds from the circuit
inputs to its output. The circuit output computed by this evaluation process is guaranteed
to be the probability of given evidence. Figure 12.3 depicts the result of performing an
evaluation pass on the given circuit under evidence a ¯c. The circuit output in this case, .1,
is guaranteed to be the probability of evidence a ¯c.
We can perform a second pass on the circuit called a differentiation pass, which
proceeds top-down from the circuit output toward the circuit inputs. This pass evaluates
the partial derivatives of the circuit output with respect to each and every circuit input.
Figure 12.3 depicts the result of performing a differentiation pass on the given circuit
under evidence a ¯c. We see later that the circuit output is a linear function of each input.
Hence, a derivative represents the change in the circuit output for each unit change in the
circuit input. For example, the partial derivative .4 associated with input λ¯a represents the
amount by which the circuit output will change if the value of input λ¯a changes from 0 to
1. We show later that using the values of these derivatives, we can compute many marginal
probabilities efﬁciently without the need to re-evaluate the circuit. The values of partial
derivatives also have important applications to sensitivity analysis which we discuss in
Chapter 16. Finally, we note that the compiled circuits can be used to compute MPE and,
if constructed carefully, can also be used to compute MAP.
The advantages of compiling Bayesian networks into arithmetic circuits can be sum-
marized as follows. First, the separation of the inference process into ofﬂine and online
phases allows us to push much of the computational overhead into the ofﬂine phase,
which can then be amortized over many online queries. Next, the simplicity of the com-
piled arithmetic circuit and its propagation algorithms facilitate the development of online
reasoning systems. Finally, compilation provides an effective framework for exploiting
287

288
COMPILING BAYESIAN NETWORKS
A
B
C
A
A
true
.5
false
.5
A
B
B|A
true
true
1
true
false
0
false
true
0
false
false
1
A
C
C|A
true
true
.8
true
false
.2
false
true
.2
false
false
.8
Figure 12.1: A Bayesian network.
+
+
+
*
*
*
*
*
*
+
+
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
Figure 12.2: An arithmetic circuit for the Bayesian network in Figure 12.1.
+
+
+
*
*
*
*
*
*
+
+
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
1
.5
.5
1
.1
1
0
0
1
1
.8
0
.2
.2
1
.8
.1
.1
0
.1
0
0
0
.4
0
.5
.1
.1
.2
0
.4
0
0
Figure 12.3: The result of circuit evaluation and differentiation under evidence a ¯c. There are two numbers
below each leaf node. The ﬁrst (top) is the value of that leaf node. The second (bottom) is the value of the
partial derivative of the circuit output with respect to that circuit input.

12.2 CIRCUIT SEMANTICS
289
A
B
A
A
true
θa = .3
false
θ¯a = .7
A
B
B|A
true
true
θb|a = .1
true
false
θ¯b|a = .9
false
true
θb|¯a = .8
false
false
θ¯b|¯a = .2
Figure 12.4: A Bayesian network.
the network local structure (i.e., the properties of network parameters), allowing an in-
ference complexity that is not necessarily exponential in the network treewidth. As we
see in Chapter 13, the techniques for exploiting local structure usually incur a nontrivial
overhead that may not be justiﬁable unless they are pushed into the ofﬂine phase of a
compilation process.
We discuss the semantics of compilation in Section 12.2 and provide algorithms for
circuit propagation in Section 12.3. The compilation of Bayesian networks into arithmetic
circuits is then discussed in Section 12.4.
12.2 Circuit semantics
The arithmetic circuit we compile from a Bayesian network is a compact representation
of the probability distribution induced by the network. We show this correspondence
more formally later but let us consider a concrete example ﬁrst. According to the seman-
tics of Bayesian networks, the probability distribution induced by the network in Fig-
ure 12.4 is
A
B
Pr(A, B)
a
b
θaθb|a
a
¯b
θaθ¯b|a
¯a
b
θ¯aθb|¯a
¯a
¯b
θ¯aθ¯b|¯a
Let us now multiply each probability in this distribution by indicator variables, as
follows:
A
B
Pr(A, B)
a
b
λaλbθaθb|a
a
¯b
λaλ¯bθaθ¯b|a
¯a
b
λ¯aλbθ¯aθb|¯a
¯a
¯b
λ¯aλ¯bθ¯aθ¯b|¯a
That is, we multiply the indicator variable λv into the probability of each instantiation that
includes value v. Let us ﬁnally take the sum of all probabilities in the distribution:
f = λaλbθaθb|a + λaλ¯bθaθ¯b|a + λ¯aλbθ¯aθb|¯a + λ¯aλ¯bθ¯aθ¯b|¯a.
(12.1)
The function f , called the network polynomial, can be viewed as a representation of
the probability distribution induced by the network in Figure 12.4. In particular, we can
use this polynomial to compute the probability of any evidence e by simply setting the
indicator variables to 1 or 0, depending on whether they are consistent with evidence e. For
example, given evidence e = ¯a, we set the indicators as follows: λa = 0, λ¯a = 1, λb = 1,

290
COMPILING BAYESIAN NETWORKS
and λ¯b = 1. The value of the polynomial under these settings is then
f (e = ¯a) = (0)(1)θaθb|a + (0)(1)θaθ¯b|a + (1)(1)θ¯aθb|¯a + (1)(1)θ¯aθ¯b|¯a
= θ¯aθb|¯a + θ¯aθ¯b|¯a
= Pr(e).
For another example, consider the network in Figure 12.1. The polynomial of this
network has eight terms, some of which are shown here:
f = λaλbλcθaθb|aθc|a +
λaλbλ¯cθaθb|aθ¯c|a +
...
λ¯aλ¯bλ¯cθ¯aθ¯b|¯aθ¯c|¯a.
(12.2)
In general, for a Bayesian network with n variables each term in the polynomial will
contain 2n variables: n parameters and n indicators, where each of the variables has
degree 1. Hence, the polynomial is a multilinear function (MLF), as it is a linear function
in terms of each of its variables.1
The network polynomial has an exponential size as it includes a term for each in-
stantiation of the network variables. Due to its size, we cannot work with the network
polynomial directly. Instead, we will use the arithmetic circuit as a compact representation
of the network polynomial. As we see later, there are interesting situations in which the
size of an arithmetic circuit can be bounded even when the size of the polynomial it
represents cannot. The generation of arithmetic circuits is discussed in Section 12.4. We
ﬁrst deﬁne formally the network polynomial and its circuit representation.
As a matter of notation, we write θx|u ∼z to mean that the subscript xu is consistent
with instantiation z, xu ∼z. Hence, 
θx|u∼z θx|u denotes the product of all parameters θx|u
for which xu is consistent with z. We similarly interpret the notation λx ∼z.
Deﬁnition 12.1. Let N be a Bayesian network over variables Z. For every variable
X with parents U in the network, variable λx is called an indicator and variable θx|u is
called a parameter. The polynomial of network N is deﬁned over indicator and parameter
variables as
f
def=

z

θx|u∼z
θx|u

λx∼z
λx.
The value of network polynomial f at evidence e, denoted by f (e), is the result of
replacing each indicator λx in f with 1 if x is consistent with e, x ∼e, and with 0
otherwise.
■
The outer sum in the deﬁnition of f ranges over all instantiations z of the network
variables. For each instantiation z, the inner products range over parameter and indicator
variables that are compatible with instantiation z.
The following is another example for computing a value of the network polyno-
mial in (12.1). If the evidence e is a ¯b, then f (e) is obtained by applying the follow-
ing substitutions to f : λa = 1, λ¯a = 0, λb = 0, and λ¯b = 1, leading to the probability
f (e) = Pr(e) = θaθ¯b|a.
1 Linearity implies that the partial derivative ∂f/∂λx is independent of λx. Similarly, the partial derivative ∂f/∂θx|u
is independent of θx|u.

12.3 CIRCUIT PROPAGATION
291
Theorem 12.1. Let N be a Bayesian network inducing probability distribution Pr and
having polynomial f . For any evidence e, we have f (e) = Pr(e).
■
As mentioned previously, we represent network polynomials by arithmetic circuits,
which can be much smaller in size.
Deﬁnition 12.2. An arithmetic circuit over variables  is a rooted DAG whose leaf nodes
are labeled with variables in  and whose other nodes are labeled with multiplication
and addition operations. The size of an arithmetic circuit is the number of edges it
contains.
■
Figure 12.2 depicts an arithmetic circuit representation of the network polynomial
in (12.2). Note how the value of each node in the circuit is a function of the values
of its children.
The compilation of Bayesian networks into arithmetic circuits provides a new measure
for the complexity of inference, which is more reﬁned than treewidth as it can be sensitive
to the properties of network parameters (local structure).
Deﬁnition 12.3. The circuit complexity of a Bayesian network N is the size of the
smallest arithmetic circuit that represents the network polynomial of N.
■
For example, when the values of network parameters are known to have speciﬁc
values, such as 0 and 1, or when some relationships exist between these parameters,
such as equality, the arithmetic circuit can be simpliﬁed considerably, leading to a circuit
complexity that could be much tighter than the complexity based on treewidth.
We next address two questions. First, assuming that we have a compact arithmetic
circuit that represents the network polynomial, how can we use it to answer probabilistic
queries? Second, how do we obtain a compact arithmetic circuit that represents a given
network polynomial? The ﬁrst question will be addressed in the following section and the
second question will be addressed in Section 12.4.
12.3 Circuit propagation
Once we have an arithmetic circuit for a given Bayesian network, we can compute the
probability of any evidence e by simply evaluating the circuit at that evidence. We can
actually answer many more queries beyond the probability of evidence if we have access
to the circuit’s partial derivatives. Consider the circuit in Figure 12.3, for example, which
has been evaluated and differentiated at evidence e = a ¯c. The value of partial derivative
∂f/∂λ¯a equals .4 in this case. This derivative represents the amount of change in the
circuit output for each unit change in the circuit input λ¯a. For example, if we change
the input λ¯a from the current value of 0 to 1, the circuit output changes from .1 to .5.
Note, however, that changing the input λ¯a from 0 to 1 corresponds to changing the
evidence from a ¯c to ¯c. Hence, from the value of this derivative we can conclude that the
probability of evidence ¯c is .5, without the need for re-evaluating the circuit under this new
evidence.
We next present a theorem that gives a precise probabilistic meaning to circuit deriva-
tives, allowing us to answer many interesting queries based on the values of these deriva-
tives. But we ﬁrst need the following notational convention. Let e be an instantiation and
X be a set of variables. Then e −X denotes the instantiation that results from erasing the

292
COMPILING BAYESIAN NETWORKS
values of variables X from instantiation e. For example, if e = ab¯c, then e −A = b¯c and
e −AC = b.
Before we present our next result, we note here that if f is a network polynomial, then
∂f/∂λx and ∂f/∂θx|u are also polynomials and, hence, can be evaluated at some evidence
e in the same way that polynomial f is evaluated at e. The quantities ∂f/∂λx(e) and
∂f/∂θx|u(e) are then well deﬁned and Theorem 12.2 reveals their probabilistic semantics.
Theorem 12.2. Let N be a Bayesian network representing probability distribution Pr and
having polynomial f and let e be some evidence. For every indicator λx, we have
∂f
∂λx
(e) = Pr(x, e −X).
(12.3)
Moreover, for every parameter θx|u we have
θx|u
∂f
∂θx|u
(e) = Pr(x, u, e).
(12.4)
■
We next provide some concrete examples of these derivatives to shed some light on their
practical value.
Consider again the circuit in Figure 12.3, where evidence e = a ¯c and ∂f
∂λ¯a (e) = .4. Since
e −A = ¯c, we immediately conclude that
∂f
∂λ¯a
(e) = Pr(¯a, e −A) = Pr(¯a ¯c) = .4.
Equation (12.3) can therefore be used to compute the probability of new evidence e′ that
results from ﬂipping the value of some variable X in evidence e. If variable X is not set
in evidence e, we have e −X = e. Hence, (12.3) can be used to compute the marginal
probability x, e in this case.
Similarly, (12.4) can be used to compute all family marginals Pr(x, u, e) from the
values of circuit derivatives. Finally, note that (12.4) is commonly used in the context
of standard algorithms, such as the jointree algorithm, to compute the values of partial
derivatives:
∂f
∂θx|u
(e) = Pr(x, u, e)
θx|u
,
when θx|u ̸= 0.
(12.5)
That is, we could use a standard algorithm to compute the marginal Pr(x, u, e) and then
use (12.5) to evaluate the derivative
∂f
∂θx|u (e) given this marginal. This common technique
is only valid when θx|u ̸= 0, however. In the following section, we provide a more general
technique that does not require this condition.
The derivative in (12.5) plays a key role in sensitivity analysis (Chapter 16) and in
learning network parameters (Chapter 17). This derivative is also commonly expressed as
∂f
∂θx|u
(e) = ∂Pr(e)
∂θx|u
,
(12.6)
where Pr is the distribution corresponding to the network polynomial f . We use this form
in Chapters 16 and 17.

12.3 CIRCUIT PROPAGATION
293
Algorithm 34 CircP1(AC, vr(), dr()). Assumes that the values of leaf circuit nodes v
have been initialized in vr(v).
input:
AC:
arithmetic circuit
vr():
array of value registers (one register for each circuit node)
dr():
array of derivative registers (one register for each circuit node)
output: computes the value of circuit output v in vr(v) and computes derivatives of leaf
nodes v in dr(v)
main:
1: for each circuit node v (visiting children before parents) do
2:
compute the value of node v and store it in vr(v)
3: end for
4: dr(v)←0 for all non-root nodes v; dr(v)←1 for root node v
5: for each circuit node v (visiting parents before children) do
6:
for each parent p of node v do
7:
if p is an addition node then
8:
dr(v)←dr(v) + dr(p)
9:
else
10:
dr(v)←dr(v) + dr(p) 
v′̸=v vr(v′), where v′ is a child of parent p
11:
end if
12:
end for
13: end for
12.3.1 Evaluation and differentiation passes
Evaluating an arithmetic circuit is straightforward: we simply traverse the circuit bottom-
up, computing the value of a node after having computed the values of its children. The
procedure for computing values of derivatives is also simple and is given in Algorithm 34,
but the correctness of this procedure is not obvious and needs to be established. However,
before we prove correctness we point to Figure 12.5, which contains an arithmetic circuit
evaluated and differentiated under evidence e = a ¯c using Algorithm 34.
The algorithm uses two arrays of registers: vr(v) stores a value for each node v and
dr(v) stores a partial derivative. We assume that the values of leaf nodes, which correspond
to indicators and parameters, have been initialized. The algorithm starts by performing
a bottom-up pass in which the value of each node v is computed and stored in the vr(v)
register. It then initializes the dr(.) array and performs a second pass in which it ﬁlls the
dr(.) array. To see how this array is ﬁlled, let us ﬁrst use r to denote the root node (circuit
output) and let v be an arbitrary circuit node. The key observation here is that the value
of the root node vr(r) is a function of the value vr(v) of an arbitrary node v. Hence, it is
meaningful to compute the partial derivative of vr(r) with respect to vr(v), ∂vr(r)/∂vr(v).
Algorithm 34 computes such a derivative for each node v and stores it in the register dr(v).
Note that we are interested in the derivative dr(v) only for leaf nodes v, which correspond
to parameter and indicator variables. However, computing these derivatives for every node
makes things easier for the following reason: Once the derivatives are computed for the
parents of node v, computing the derivative for node v becomes straightforward. This is
indeed the basis of Algorithm 34, which proceeds top-down, computing the derivatives
for parents before computing them for children.

294
COMPILING BAYESIAN NETWORKS
+
+
+
*
*
*
*
*
*
+
+
*
*
*
*
1
.1
.1
1
1
.1
0
.4
1
.1
1
0
.2
.5
.8
0
.5
.2
.5
0
.1
1
0
1
0
0
0
.1
1
0
0
.5
0
0
.2
.5
.8
0
1
.1
1
.1
0
0
0
.1
1
0
1
0
.8
0
.2
0
.2
.5
.8
0
0
.4
1
.1
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
Figure 12.5: An arithmetic circuit for the Bayesian network in Figure 12.1 after it has been evaluated and
differentiated under evidence a ¯c, using Algorithm 34. Registers vr(.) are shown on the left and registers
dr(.) are shown on the right.
To further justify the particular update equations used by the algorithm, note ﬁrst that
∂vr(r)/∂vr(r) = 1 and r is the root, which is the reason why dr(r) is initialized to 1. For
an arbitrary node v ̸= r with parents p, the chain rule of differential calculus gives
∂vr(r)
∂vr(v) =

p
∂vr(r)
∂vr(p)
∂vr(p)
∂vr(v) .
Since the derivatives of the root r are stored in the dr(.) array, we have
dr(v) =

p
dr(p)∂vr(p)
∂vr(v) .
Suppose now that v′ ranges over the children of parent p. If parent p is a multiplication
node, then
∂vr(p)
∂vr(v) =
∂(vr(v) 
v′̸=v vr(v′))
∂vr(v)
=

v′̸=v
vr(v′).
However, if parent p is an addition node, then
∂vr(p)
∂vr(v) =
∂(vr(v) + 
v′̸=v vr(v′))
∂vr(v)
= 1.
If we let +p stand for an addition parent of v and let ⋆p stand for a multiplication parent
having children v′, we then have
dr(v) =

+p
dr(+p) +

⋆p
dr(⋆p)

v′̸=v
vr(v′).
Algorithm 34 uses this precise equation to ﬁll in the dr(.) array.

12.3 CIRCUIT PROPAGATION
295
Algorithm 35 CircP2(AC, vr(), dr()). Assumes the values of leaf circuit nodes v have
been initialized in vr(v) and the circuit alternates between addition and multiplication
nodes, with leaves having multiplication parents.
input:
AC:
arithmetic circuit
vr():
array of value registers (one register for each circuit node)
dr():
array of derivative registers (one register for each circuit node)
output: computes the value of circuit output v in vr(v) and computes derivatives of leaf
nodes v in dr(v)
main:
1: for each non-leaf node v with children c (visit children before parents) do
2:
if v is an addition node then
3:
vr(v)←
c: bit(c)=0 vr(c) {if bit(c) = 1, value of c is 0}
4:
else
5:
if v has a single child c′ with vr(c′) = 0 then
6:
bit(v)←1; vr(v)←
c̸=c′ vr(c)
7:
else
8:
bit(v)←0; vr(v)←
c vr(c)
9:
end if
10:
end if
11: end for
12: dr(v)←0 for all non-root nodes v; dr(v)←1 for root node v
13: for each non-root node v (visit parents before children) do
14:
for each parent p of node v do
15:
if p is an addition node then
16:
dr(v)←dr(v) + dr(p)
17:
else
18:
if vr(p) ̸= 0 then {p has at most one child with zero value}
19:
if bit(p) = 0 then {p has no zero children}
20:
dr(v)←dr(v) + dr(p)vr(p)/vr(v)
21:
else if vr(v) = 0 then {v is the single zero child}
22:
dr(v)←dr(v) + dr(p)vr(p)
23:
end if
24:
end if
25:
end if
26:
end for
27: end for
The bottom-up pass in Algorithm 34 clearly takes time linear in the circuit size, where
size is deﬁned as the number of circuit edges. However, the top-down pass takes linear
time only when each multiplication node has a bounded number of children; otherwise,
the time to evaluate the term 
v′̸=v vr(v′) cannot be bounded by a constant.
This is addressed by Algorithm 35, which is based on observing that the term

v′̸=v vr(v′) equals vr(p)/vr(v) when vr(v) ̸= 0 and, hence, the time to evaluate it can
be bounded by a constant if we use division. Even the case vr(v) = 0 can be handled
efﬁciently but that requires an additional bit per multiplication node p:
r bit(p) = 1 when exactly one child of node p has a zero value.

296
COMPILING BAYESIAN NETWORKS
When this bit is set, the register vr(p) will not store the value of p, which must be zero.
Instead, it will store the product of values for p’s children, excluding the single child that
has a zero value. The use of this additional bit leads to Algorithm 35, which takes time
linear in the circuit size. Note that after ﬁnishing the bottom-up pass, we are guaranteed
the following:
r The value of every addition node v is stored in vr(v).
r The value of every multiplication node v is stored in vr(v) if bit(v) = 0, and the value is 0
otherwise.
Algorithm 35 assumes that the circuit alternates between addition and multiplication
nodes with leaf nodes having multiplication parents. This can be easily relaxed but makes
the statement of the algorithm somewhat more complicated as we would need to include
more tests to decide the value of a node (based on its type and associated bit).
12.3.2 Computing MPEs
An arithmetic circuit can be easily modiﬁed into a maximizer circuit that computes the
probability of MPEs.
Deﬁnition 12.4. The maximizer circuit ACm for an arithmetic circuit AC is obtained by
replacing each addition node in circuit AC with a maximization node.
■
Figure 12.6 depicts a maximizer circuit obtained from the arithmetic circuit in Fig-
ure 12.2. A maximizer circuit computes the value of the maximum term in a network
polynomial instead of adding up the values of these terms as done by an arithmetic circuit.
In particular, if the arithmetic circuit AC represents the polynomial
f =

z

θx|u∼z
θx|u

λx∼z
λx,
then the maximizer circuit ACm represents the following maximizer polynomial:
f m def= max
z

θx|u∼z
θx|u

λx∼z
λx.
max
max
max
*
*
*
*
*
*
max
max
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
cλ
cλ
b
λ
a
b|θ
a
b|θ
a
b|
θ
a
b|
θ
a
c|θ
a
c|θ
a
c|θ
a
c|θ
Figure 12.6: A maximizer circuit for the Bayesian network in Figure 12.1.

12.3 CIRCUIT PROPAGATION
297
max
max
max
*
*
*
*
*
*
max
max
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
cλ
cλ
b
λ
a
b|θ
a
b|θ
a
b|
θ
a
b|
θ
a
c|θ
a
c|θ
a
c|θ
a
c|θ
1
0
0
1
.8
.2
.2
.8
0
1
11
1
.5
.5
1
0
0
0
1
.8
.2
.2
.8
.8
.8
0
1
0
.4
.4
Figure 12.7: A maximizer circuit for the Bayesian network in Figure 12.1, evaluated at evidence ¯b.
Therefore, evaluating the maximizer circuit ACm at evidence e gives the MPE probability,
MPEP (e):
ACm(e) = f m(e) = MPEP (e).
Figure 12.7 depicts a maximizer circuit that is evaluated at evidence e = ¯b, leading to the
MPE probability .4.
A maximizer circuit can also be used to recover one or all MPE instantiations. This is
done using the notion of a complete subcircuit.
Deﬁnition 12.5. Let ACm be a maximizer circuit. A complete subcircuit in ACm is a
subcircuit obtained by starting at the root of ACm and moving downward from parents
to children, including all children of a visited multiplication node and including exactly
one child of a visited maximization node. Each complete subcircuit corresponds to a
polynomial term obtained by taking the product of variables associated with leaf nodes
in the subcircuit. The value of a subcircuit at evidence e is the value of its corresponding
term at evidence e.
■
We therefore have a one-to-one correspondence between complete subcircuits and poly-
nomial terms.
Figure 12.7 highlights two complete subcircuits, one in bold lines and the other in
dashed bold lines. The subcircuit highlighted with bold lines corresponds to the term
λ¯aλ¯bλ¯cθ¯aθ¯b|¯aθ¯c|¯a and has a value of .4 at evidence ¯b. The subcircuit highlighted in bold
dashed lines corresponds to the term λaλbλcθaθb|aθc|a and has a value of 0 at this evidence.
The maximizer circuit in Figure 12.6 has eight complete subcircuits corresponding to the
eight terms of the network polynomial.
We can construct an MPE instantiation by choosing a complete subcircuit whose value
is maximal at the given evidence e. This can be done by ensuring that for each maximiza-
tion node v in the subcircuit, the chosen child c of node v has the same value as v. Applying
this procedure to Figure 12.7 leads to the subcircuit highlighted in bold lines, which cor-
responds to the term λ¯aλ¯bλ¯cθ¯aθ¯b|¯aθ¯c|¯a and MPE instantiation ¯a ¯b¯c with a probability of .4.

298
COMPILING BAYESIAN NETWORKS
Algorithm 36 CircP MPE(AC, vr(), dr()). Assumes that the values of leaf circuit nodes
v have been initialized in vr(v).
input:
AC:
maximizer circuit
vr():
array of value registers (one register for each circuit node)
dr():
array of derivative registers (one register for each circuit node)
output: computes the value of circuit output v in vr(v) and computes derivatives of leaf
nodes v in dr(v)
main:
1: for each circuit node v (visiting children before parents) do
2:
compute the value of node v and store it in vr(v)
3: end for
4: dr(v)←0 for all non-root nodes v; dr(v)←1 for root node v
5: for each circuit node v (visiting parents before children) do
6:
for each parent p of node v do
7:
if p is a maximization node then
8:
dr(v)←max(dr(v), dr(p))
9:
else
10:
dr(v)←max(dr(v), dr(p) 
v′̸=v vr(v′)), where v′ is a child of p
11:
end if
12:
end for
13: end for
In general, a maximization node v can have multiple children c with the same value as
v, indicating the existence of multiple MPE instantiations. By choosing a different child
c from this set we can induce different complete subcircuits, each corresponding to a
different MPE instantiation.
We can also deﬁne a second pass on a maximizer circuit that traverses the circuit
top-down from parents to children, as given by Algorithm 36.2 This is very similar to the
second pass of Algorithm 34 except that we replace additions by maximizations when
computing the values of dr(.) registers. The values of these registers also have differential
and probabilistic semantics similar to their arithmetic circuit counterparts. However, the
derivatives are not with respect to the maximizer circuit but with respect to restrictions of
this circuit, as we explain next.
Note ﬁrst that the derivatives ∂f m/∂θx|u and ∂f m/∂λx are not well deﬁned as the
function f m is not continuous in the variables θx|u and λx. For example, the value of
function f m may stay constant for certain changes in variable θx|u yet suddenly change
its value when the change in θx|u becomes large enough. However, these derivatives are
well deﬁned for restrictions of the maximizer polynomial, deﬁned next.
Deﬁnition 12.6. The restriction of maximizer polynomial f m to instantiation e is
deﬁned as
f m
e
def= max
z∼e

θx|u∼z
θx|u

λx∼z
λx,
where all symbols are as given by Deﬁnition 12.1.
■
2 We can improve the complexity of this algorithm by using an additional bit per multiplication node, as in
Algorithm 35.

12.3 CIRCUIT PROPAGATION
299
That is, instead of maximizing over all polynomial terms, we maximize over only those
terms that are consistent with evidence e. Consider now the restriction f m
x,u. All terms
of this maximizer polynomial contain the parameter θx|u, therefore f m
x,u is continuous in
terms of θx|u, leading to a well-deﬁned partial derivative with respect to parameter θx|u.
The same is true for the restriction f m
x , leading to a well-deﬁned partial derivative with
respect to indicator λx.
Consider for example the following maximizer polynomial for the network in
Figure 12.1,
f m = max(λaλbλcθaθb|aθc|a, λaλbλ¯cθaθb|aθ¯c|a, . . . , λ¯aλ¯bλ¯cθ¯aθ¯b|¯aθ¯c|¯a)
and its restriction
f m
b,a = max(λaλbλcθaθb|aθc|a, λaλbλ¯cθaθb|aθ¯c|a)
= θb|a max(λaλbλcθaθc|a, λaλbλ¯cθaθ¯c|a).
The derivative with respect to parameter θb|a is well deﬁned in this case:
∂f m
b,a/∂θb|a = max(λaλbλcθaθc|a, λaλbλ¯cθaθ¯c|a).
Given these deﬁnitions, we can now state the meaning of the registers dr(.) computed
by Algorithm 36.
Theorem 12.3. Let f m be the maximizer polynomial represented by a maximizer circuit
passed to Algorithm 36. The following holds after termination of Algorithm 36: For a leaf
node v that corresponds to parameter θx|u, we have
dr(v) = ∂f m
x,u
∂θx|u
(e).
(12.7)
Moreover, for a leaf node v that corresponds to indicator λx, we have
dr(v) = ∂f m
x
∂λx
(e).
(12.8)
■
Now that we have deﬁned the differential semantics of the quantities computed by the
second pass of Algorithm 36, let us reveal their probabilistic semantics.
Theorem 12.4. Let f m be a maximizer polynomial. We then have
∂f m
x
∂λx
(e) = MPEP (x, e −X),
(12.9)
and
θx|u
∂f m
x,u
∂θx|u
(e) = MPEP (e, x, u).
(12.10)
■
Note the similarity between Theorem 12.4 and Theorem 12.2, which reveals the proba-
bilistic semantics of arithmetic circuit derivatives.
The derivative with respect to parameter variables has applications in Chapter 16 on
sensitivity analysis. The derivative with respect to indicator variables has a more direct
application as it gives the probability of MPE after ﬂipping the value of variable X in
evidence e to x. If variable X is not set in e, then e −X = e and the derivative gives the
MPE marginal probability, MPEP (x, e).

300
COMPILING BAYESIAN NETWORKS
max
max max
*
*
*
*
*
*
max max
*
*
*
*
1
0
.4
1
1
0
1
.4
0
.4
1
.4
.8
0
.8
.5
.5
0
.5
.8
0
1
.4
1
0
.4
0
.4
1
.4
.8
0
.2
.5
.2
0
.8
.5
0
.4
0
.4
0
0
0
.4
1
.4
1
.4
.8
0
.2
.5
.2
0
.8
.5
1
.1
1
.4
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
Figure 12.8: A maximizer circuit for the Bayesian network in Figure 12.1, evaluated and differentiated at
evidence ¯b. Registers on the left contain node values and registers on the right contain node derivatives.
Figure 12.8 depicts a maximizer circuit evaluated and differentiated under evidence ¯b
using Algorithm 36. There is only one MPE instantiation in this case, ¯a ¯b¯c. The following
table lists the partial derivatives with respect to indicators together with their probabilistic
semantics according to Theorem 12.4:
partial derivative
value
meaning
λa
0
MPEP (a, ¯b)
λ¯a
.4
MPEP (¯a, ¯b)
λb
.4
MPEP (b)
λ¯b
.4
MPEP (¯b)
λc
.1
MPEP (¯b, c)
λ¯c
.4
MPEP (¯b, ¯c)
12.4 Circuit compilation
We discuss two classes of algorithms for compiling arithmetic circuits, one class in this
chapter and the other in Chapter 13. The algorithms discussed in this chapter are based
on inference algorithms discussed previously. In particular, Section 12.4.1 describes a
method for generating an arithmetic circuit by keeping a trace of the variable elimination
algorithm, while Section 12.4.2 describes a method for extracting an arithmetic circuit
from the structure of a jointree, and Section 12.4.3 describes a method based on CNF
encodings. These methods are sensitive only to the network structure, leading to a circuit
complexity that is independent of local structure (i.e., the speciﬁc values of network
parameters). The second class of algorithms to be discussed in Chapter 13 will exploit the
local structure of a Bayesian network and can be quite efﬁcient even when the network
treewidth is very large.

12.4 CIRCUIT COMPILATION
301
12.4.1 The circuits of variable elimination
We now describe a method for compiling a Bayesian network into an arithmetic cir-
cuit by keeping a trace of the variable elimination algorithm discussed in Chapter 6. In
particular, instead of performing arithmetic operations as per the standard variable elimi-
nation algorithm, we perform circuit-construction operations that incrementally build up
an arithmetic circuit in a bottom-up fashion.
To use variable elimination for constructing circuits, we need to work with circuit
factors instead of standard factors. In a circuit factor, each variable instantiation is mapped
to a circuit node instead of a number. Factor operations then need to be extended to work
with circuit factors, which is accomplished by simply replacing the arithmetic operations
of addition and multiplication by corresponding operations that construct circuit nodes.
In particular, given circuit nodes n1 and n2, we use +(n1, n2) to denote an addition node
that has n1 and n2 as its children. Similarly, ⋆(n1, n2) will denote a multiplication node
that has n1 and n2 as its children. The multiplication of two circuit factors f (X) and f (Y)
is then a factor over variables Z = X ∪Y, deﬁned as
f (z)
def= ⋆(f (x), f (y)),
where x ∼z
and
y ∼z.
The summing out of variable X from circuit factor f (X) is deﬁned similarly.
The algorithm we shall present starts by constructing a circuit factor for each network
CPT. In particular, for every variable X with parents U, a circuit factor is constructed
over variables XU. This factor maps each instantiation xu into a circuit node ⋆(λx, θx|u).
Considering the network in Figure 12.4, the following circuit factors are constructed,
where nodes n1, . . . , n6 are also shown in Figure 12.9:
A
A
true
n1
= ⋆(λa, θa)
false
n2
= ⋆(λ¯a, θ¯a)
A
B
B|A
true
true
n3
= ⋆(λb, θb|a)
true
false
n4
= ⋆(λ¯b, θ¯b|a)
false
true
n5
= ⋆(λb, θb|¯a)
false
false
n6
= ⋆(λ¯b, θ¯b|¯a)
After constructing these factors, we apply the algorithm of variable elimination to elim-
inate every variable in the network. Just as in standard variable elimination, multiplying
all resulting factors leads to a trivial factor with one entry, which in this case is the root to
a circuit that represents the network polynomial.
*
*
*
*
+
+
+
*
*
*
*
1
3
4
5
6
2
7
8
9
10
11
a
λ
a
λ
b
λ
b
λ
a
θ
a
θ
a
b|
θ
a
b|θ
a
b|
θ
a
b|
θ
Figure 12.9: An arithmetic circuit for the Bayesian network in Figure 12.4 constructed by keeping a trace
of variable elimination.

302
COMPILING BAYESIAN NETWORKS
We now perform this elimination process using the order π = B, A. To eliminate
variable B, we sum it out from factor B|A, since it is the only factor containing B:
A

B B|A
true
n7
= +(n3, n4)
false
n8
= +(n5, n6)
This leads to constructing two new circuit nodes n7 and n8, shown in Figure 12.9.
To eliminate variable A, we must multiply the previous factor with factor A and then
sum out variable A from the result. Multiplying leads to constructing two new circuit
nodes, yielding
A
A

B B|A
true
n9
= ⋆(n1, n7)
false
n10
= ⋆(n2, n8)
Summing out constructs another circuit node, leading to

A A

B B|A
⊤
n11
= +(n9, n10)
Now that we have eliminated every variable, the resulting factor has a single entry n11
that is the root of the constructed circuit, shown in Figure 12.9. This circuit is guaranteed
to correspond to the network polynomial.
The correctness of this algorithm follows immediately from the semantics of variable
elimination, which evaluates the generated circuit in the process of computing the proba-
bility of evidence. The size of the resulting circuit is also bounded by the time complexity
of variable elimination since the circuit nodes correspond to operations performed by the
elimination algorithm. That is, if the elimination order used has n variables and width w,
the size of the circuit and the time to generate it are bounded by O(n exp(w)).
Recall that the algorithm of variable elimination is best suited for answering single
queries, at least compared with the jointree algorithm, which can compute multiple queries
within the same time complexity. One advantage of using variable elimination to compile
arithmetic circuits is that we can use the resulting circuit to answer multiple queries in
time linear in the circuit size, as shown in the previous section. In this sense, using variable
elimination to compile arithmetic circuits provides the same computational advantages
that we obtain from using the jointree algorithm.
We can also use the algorithm of recursive conditioning to compile out arithmetic
circuits by keeping a trace of the operations it performs, just as with variable elimination.
As such, both elimination and conditioning algorithms can be viewed as factorization
algorithms as they factor the network polynomial into a corresponding arithmetic circuit
representation.
12.4.2 Circuits embedded in a jointree
We now present another method for generating arithmetic circuits that is based on extract-
ing them from jointrees.
Before a jointree is used to generate a circuit, each CPT X|U must be assigned to
a cluster that contains family XU. Moreover, for each variable X an evidence indicator
λX must be assigned to a cluster that contains X. Finally, a cluster in the jointree is cho-
sen and designated as the root, allowing us to deﬁne parent/child relationships between

12.4 CIRCUIT COMPILATION
303
A
λAθA
AC
λCθC|A
AB
λBθB|A
A
A
root
+
+
+
*
*
*
*
*
*
+
+
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
Figure 12.10: A jointree for the Bayesian network in Figure 12.1 and its corresponding arithmetic circuit.
neighboring clusters and separators. In particular, if two variable sets (clusters or separa-
tors) are adjacent in the jointree, the one closer to the root will be a parent of the other.
The jointree in Figure 12.10 depicts the root cluster in addition to the assignment of CPTs
and evidence indicators to various clusters. We show next that each jointree embeds an
arithmetic circuit that represents the network polynomial.
Deﬁnition 12.7. Given a root cluster, a particular assignment of CPT, and evidence
indicators to clusters, the arithmetic circuit embedded in a jointree is deﬁned as follows.
The circuit includes:
r One output addition node f
r An addition node s for each instantiation of a separator S
r A multiplication node c for each instantiation of a cluster C
r An input node λx for each instantiation x of variable X
r An input node θx|u for each instantiation xu of family XU.
The children of the output node f are the multiplication nodes c generated by the
root cluster. The children of an addition node s are all compatible multiplication nodes
c, c ∼s, generated by the child cluster. The children of a multiplication node c are
all compatible addition nodes s, s ∼c, generated by child separators, in addition to
all compatible inputs nodes θx|u and λx, xu ∼c, for which CPT X|U and evidence
indicator λX are assigned to cluster C.
■
Figure 12.10 depicts a jointree and its embedded arithmetic circuit. Note the correspon-
dence between addition nodes in the circuit (except the output node) and instantiations of
separators in the jointree. Note also the correspondence between multiplication nodes in
the circuit and instantiations of clusters in the jointree. One useful feature of the circuit
embedded in a jointree is that it does not require that we represent its edges explicitly, as
these can be inferred from the jointree structure. This leads to smaller space requirements
but increases the time for evaluating and differentiating the circuit, given the overhead
needed to infer these edges.3 Another useful feature of the circuit embedded in a jointree
is the guarantees one can offer on its size.
3 Some optimized implementations of jointree algorithms maintain indices that associate cluster entries with com-
patible entries in their neighboring separators to reduce jointree propagation time. These algorithms are then
representing both the nodes and edges of the embedded circuit explicitly.

304
COMPILING BAYESIAN NETWORKS
Theorem 12.5. Let J be a jointree for Bayesian network N with n clusters, a maximum
cluster size c, and a maximum separator size s. The arithmetic circuit embedded in jointree
J represents the network polynomial for N and has O(n exp(c)) multiplication nodes,
O(n exp(s)) addition nodes, and O(n exp(c)) edges.
■
We saw in Chapter 9 that a Bayesian network with n nodes and treewidth w has a
jointree with no more than n clusters and a maximum cluster size of w + 1. Theorem 12.5
is then telling us that the circuit complexity of such networks is O(n exp(w)).
12.4.3 The circuits of CNF encodings
In Section 11.6, we showed that CNF encodings can be used to answer certain probabilistic
queries (e.g., computing the probability of evidence by applying a weighted model counter
to a CNF encoding). We show in this section that CNF encodings can also be used to
generate arithmetic circuits for the corresponding Bayesian networks. In particular, we
show that an arithmetic circuit can be immediately obtained once the CNF encoding (of
Section 11.6.1) is converted into an equivalent NNF circuit that satisﬁes certain properties.
Our proposed technique will consist of the following steps:
1. Encode the Bayesian network using a CNF  (see Section 11.6.1).
2. Convert the CNF  into an NNF circuit  that satisﬁes the properties of decomposability,
determinism, and smoothness (see Section 2.7).
3. Extract an arithmetic circuit from the NNF circuit .
We ﬁrst illustrate this compilation technique using a concrete example and then discuss
the reasons it works in general.
Consider the Bayesian network in Figure 12.4 and the CNF encoding scheme described
in Section 11.6.1. If we apply this encoding scheme to the network, we obtain the following
CNF:4
Ia
∨
I¯a
Ib
∨
I¯b
¬Ia
∨
¬I¯a
¬Ib
∨
¬I¯b
Ia
⇐⇒
Pa
I¯a
⇐⇒
P¯a
Ia ∧Ib
⇐⇒
Pb|a
Ia ∧I¯b
⇐⇒
P¯b|a
I¯a ∧Ib
⇐⇒
Pb|¯a
I¯a ∧I¯b
⇐⇒
P¯b,¯a
(12.11)
Figure 12.11(a) depicts an NNF circuit that is equivalent to this CNF and satisﬁes the
properties of decomposability, determinism, and smoothness. Figure 12.11(b) depicts an
arithmetic circuit obtained from this NNF circuit by applying the following substitutions
(and some minor simpliﬁcations):
r Every or-node ∨is replaced by an addition node +.
r Every and-node ∧is replaced by a multiplication node ∗.
4 A sentence such as Ia ∧Ib ⇐⇒Pb|a corresponds to a set of clauses Ia ∧Ib =⇒Pb|a, Pb|a =⇒Ia, and
Pb|a =⇒Ib.

12.4 CIRCUIT COMPILATION
305
a
bP |
bI
bI
¬
a
bP |
a
bP |
bI
¬
bI
a
bP |
a
bP |
¬
a
bP |
¬
a
bP |
¬
a
bP |
¬
aI
aI
¬
aP
aP
¬
aI
aI
¬
aP
aP
¬
∧
∧
∨
∨
∨
∧
∧
∧
∧
∧
∧
∧
∧
a
b|
θ
b
λ
a
b|
θ
a
b|
θ
b
λ
a
b|
θ
a
λ
a
θ
a
λ
a
θ
*
*
+
+
+
*
*
*
*
(a) NNF circuit
(b) Arithmetic circuit
Figure 12.11: Extracting an arithmetic circuit from an NNF circuit.
r Every negative literal of the form ¬Ix or ¬Px|u is replaced by 1.
r Every positive literal Ix is replaced by the indicator λx.
r Every positive literal Px|u is replaced by the parameter θx|u.
We can verify that the arithmetic circuit in Figure 12.11(b) does indeed represent the
network polynomial of the Bayesian network given in Figure 12.4.
To see why this compilation technique works in general, we ﬁrst show that every CNF
can be interpreted as encoding an MLF with a one-to-one correspondence between the
CNF models and the MLF terms.5 Consider for example the CNF  = (VA ∨¬VB) ∧VC,
which has three models:
model
VA
VB
VC
encoded term t
ω1
true
false
true
AC
ω2
true
true
true
ABC
ω3
false
false
true
C
Each of these models ω can be interpreted as encoding a term t in the following sense.
A variable X appears in the term t if and only if the model ω sets the variable VX to
true. The CNF  then encodes an MLF that results from adding up all these terms:
AC + ABC + C.
In fact, the MLF encoded by the CNF of a Bayesian network, as described in Sec-
tion 11.6.1, is precisely the polynomial of this network (see the proof of Theorem 11.7).
For an example, consider the network in Figure 12.4 and its polynomial
f = λaλbθaθb|a + λaλ¯bθaθ¯b|a + λ¯aλbθ¯aθb|¯a + λ¯aλ¯bθ¯aθ¯b|¯a.
We can show that the CNF in (12.11) does indeed encode this polynomial once we replace
CNF variables I by indicators λ and CNF variables P by parameters θ.
Suppose now that we have a CNF f that encodes an MLF f . If f is converted into an
equivalent NNF circuit f that satisﬁes the properties of decomposability, determinism,
5 Recall that a multilinear function (MLF) over variables  is a function of the form t1 + t2 + · · · + tn, where each
term ti is a product of distinct variables from . For example, if  = A, B, C, then A + AB + AC + ABC is an
MLF. Without loss of generality, we disallow duplicate terms in the representation of MLFs.

306
COMPILING BAYESIAN NETWORKS
∧∧∧∧
∨∨∨∨
VC
∧∧∧∧
∧∧∧∧
VB
¬VB
∨∨∨∨
VA
¬VA
∗
++++
C
∗
∗
B
1
++++
A
1
(a) NNF circuit
(b) Arithmetic circuit
Figure 12.12: The NNF circuit is equivalent to the CNF f = (VA ∨¬VB) ∧VC, which encodes the
MLF f = AC + ABC + C.
and smoothness, then we can extract an arithmetic circuit for the MLF f by applying the
following substitutions to the NNF circuit f :
r Replace every conjunction by a multiplication and every disjunction by a summation.
r Replace every negative literal ¬VX by the constant 1.
r Replace every positive literal VX by X.
Figure 12.12 provides an example of this conversion procedure whose correctness is left
to Exercise 12.17. Note that the size of the resulting arithmetic circuit is proportional to
the size of the NNF circuit. Hence, by minimizing the size of the NNF circuit, we also
minimize the size of the generated arithmetic circuit. This means that the computational
effort for compiling a Bayesian network is now shifted to the process of converting a
CNF into an NNF circuit that satisﬁes the properties of decomposability, determinism,
and smoothness.
Bibliographic remarks
The network polynomial and the semantics of its partial derivatives as discussed in
this chapter are due to Darwiche [2000; 2003], although a more restricted version of
the network polynomial was initially proposed in Castillo et al. [1996; 1997], and the
derivatives with respect to network parameters were originally studied in Russell et al.
[1995]. The representation of network polynomials using arithmetic circuits and the
corresponding circuit propagation algorithms are due to Darwiche [2000; 2003]. The
relationship between circuit propagation and jointree propagation was studied in Park and
Darwiche [2003b; 2004b], where the circuit of a jointree was ﬁrst deﬁned. The generation
of circuits using variable elimination was proposed in Darwiche [2000] and using CNF
encodings in Darwiche [2002]. Maximizer circuits and their corresponding propagation
algorithms were introduced in Chan and Darwiche [2006]. A state-of-the-art compiler
for converting CNF to NNF circuits is discussed in Darwiche [2004] and is available for
download at http://reasoning.cs.ucla.edu/c2d/.
12.5 Exercises
12.1. Consider the arithmetic circuit in Figure 12.13:
(a) Construct the polynomial f represented by this arithmetic circuit.
(b) Compute the partial derivatives ∂f/∂λa1, ∂f/∂λa2, ∂f/∂θc1|a1, and ∂f/∂θc2|a2.

12.5 EXERCISES
307
+
+
+
λ
λb1
*
*
*
*
*
*
*
*
*
*
*
*
θb1|a2
θb1| a1
b2| a2
b2| a1
.3
θ
θ
.7
.8
θ
θ
.4
.6
θ
θ
.99
.01
λ
λ
λ
λ
θ
θ
.1
.9
+
*
+
*
*
*
.2
b2
c2|a1
c1|a1
a1
a1
c1
c2
a2
a2
c1|a2
c2|a2
Figure 12.13: An arithmetic circuit.
(c) Evaluate the derivatives in (b) at evidence e = a1c1.
(d) What is the probabilistic meaning of these derivatives?
12.2. Consider the arithmetic circuit in Figure 12.13. Evaluate and differentiate this circuit at
evidence e = a1c1. Perform the differentiation using both propagation schemes given by
Algorithms 34 and 35.
12.3. Consider a Bayesian network with structure A→B→C, where all variables are binary.
Construct an arithmetic circuit for this network using the method of variable elimination given
in Section 12.4.1 and using elimination order C, B, A.
12.4. Show that the following partial derivatives are equal to zero for a network polynomial f :
(a)
∂2f
∂λx∂λx′ , where x and x′ are values (possibly equal) of the same variable.
(b)
∂2f
∂θx|u∂θx′|u′ , where xu and x′u′ are instantiations (possibly equal) of the same family.
What does this imply on the structure of the polynomial f ?
12.5. Provide probabilistic semantics for the following second partial derivatives of the network
polynomial:
∂2f
∂λx∂λy
(e),
∂2f
∂θx|u∂λy
(e),
∂2f
∂θx|u∂θy|v
(e).
Provide a circuit propagation scheme that will compute these derivatives and discuss its time
and space complexity.
12.6. Suppose we extend the notion of evidence to allow for constraining the value of a variable
instead of ﬁxing it. For example, if X is a variable with three values x1, x2, and x3, then a

308
COMPILING BAYESIAN NETWORKS
piece of evidence on variable X may be X=x1 ∨X=x2, which rules out value x3 without
committing to either values x1 or x2. Given this extended notion of evidence, known as
a ﬁnding, show how we can evaluate a network polynomial so that its value equals the
probability of the given ﬁndings.
12.7. Consider the construction of an arithmetic circuit using variable elimination. For each addition
node n constructed by this algorithm, let var(n) stand for the variable X whose elimination
has led to the construction of node n. Given some MAP variables M and evidence e, show
that the following procedure generates an upper bound on the MAP probability MAPP (M, e):
Evaluate the circuit while treating an addition node n as a maximization node if var(n) ∈M.
12.8. Given a Bayesian network and some evidence e, show how to construct a Boolean formula
whose models are precisely the MPE instantiations of evidence e. The complexity of your
algorithm should be O(n exp(w)), where n is the network size and w is the width of a given
elimination order.
12.9.
Provide an algorithm for generating circuits that can be used to compute MAP. Describe the
computational complexity of the method.
12.10. Consider a Bayesian network with structure A→B→C, where all variables are binary and
its jointree is A−AB−BC. Construct the arithmetic circuit embedded in this jointree as
given in Section 12.4.2, assuming that BC is the root cluster and that CPTs and evidence
indicators for variables A, B, and C are assigned to clusters A, AB, and BC, respectively.
12.11. Show that the arithmetic circuit embedded in a jointree has the following properties:
(a) The circuit alternates between addition and multiplication nodes.
(b) Each multiplication node has a single parent.
12.12. Given an arithmetic circuit that is embedded in a binary jointree, describe a circuit propagation
scheme that will evaluate and differentiate the circuit under the following constraints:
r
The method can use only two registers for each addition or leaf node but no registers
for multiplication nodes.
r
The time complexity of the algorithm is linear in the circuit size.
Compare your developed scheme to the Shenoy-Shafer architecture for jointree propagation.
Recall that a binary jointree is one in which each cluster has at most three neighbors.
12.13. Given an arithmetic circuit that is embedded in a jointree, describe a circuit propagation
scheme that will evaluate and differentiate the circuit under the following constraints:
r
The method can use only one register dvr(v) for each circuit node v.
r
When the algorithm terminates, the register dvr(v) contains the product dr(v)vr(v),
where dr(v) and vr(v) are as computed by Algorithm 34.
r
The time complexity of the algorithm is linear in the circuit size.
Compare your developed scheme to the Hugin architecture for jointree propagation.
12.14. Consider the arithmetic circuit in Figure 12.13. Evaluate and differentiate this circuit at
evidence e = b2 using the propagation scheme given by Algorithm 36. Compute all MPE
instantiations given the evidence and describe the meaning of derivatives with respect to
inputs λa1 and θc2|a2 in this case.
12.15. Let X be a binary variable in a Bayesian network with maximizer polynomial f m. Show that
every MPE instantiation for evidence e includes X=x if and only if
∂f m
x
∂λx
(e) > ∂f m
¯x
∂λ¯x
(e).
12.16. Show the MLF encoded by the propositional sentence A ∧(B =⇒D) ∧(C =⇒B).

12.6 PROOFS
309
12.17. Let f be a propositional sentence encoding an MLF f . Let f be an equivalent NNF circuit
that satisﬁes decomposability, determinism, and smoothness. Show that the arithmetic circuit
extracted from f as given in Section 12.4.3 is a representation of the MLF f .
12.18. Prove Lemma 12.1 on Page 310.
12.19. Consider the network polynomial f for a Bayesian network and let θx|u be a set of parameters
with equal values in the same CPT. Replace all these parameters in the polynomial f with
a new variable η. What is the probabilistic meaning of ∂f/∂η(e) for a given evidence e?
12.20. Let f be an MLF and let fm be another MLF obtained by including only the minimal terms
of f . Here a term of f is minimal if the number of variables it contains is minimal among the
terms of f . Suppose now that f is a CNF that encodes MLF f . Describe a procedure for
obtaining an arithmetic circuit for MLF fm. Hint: Consider Exercise 2.13.
12.6 Proofs
PROOF OF THEOREM 12.1. Deﬁnition 12.1 gives
f =

z

θx|u∼z
θx|u

λx∼z
λx,
and
f (e) =

z

θx|u∼z
θx|u

λx∼z
1,
if x ∼e
0,
otherwise
=

z∼e

θx|u∼z
θx|u
=

z∼e
Pr(z)
= Pr(e).
■
PROOF OF THEOREM 12.2. By the deﬁnition of partial derivatives, we have
∂f
∂λy
=

z∼y

θx|u∼z
θx|u

λx∼z
x̸=y
λx.
Deﬁnition 12.1 then gives us
∂f
∂λy
(e) =

z∼y

θx|u∼z
θx|u

λx∼z
x̸=y
1,
if x ∼e
0,
otherwise
=

z∼y
z∼e−Y

θx|u∼z
θx|u
=

z∼y
z∼e−Y
Pr(z)
= Pr(y, e −Y).
By the deﬁnition of partial derivative, we also have
∂f
∂θy|v
=

z∼yv

θx|u∼z
xu̸=yv
θx|u

λx∼z
λx.

310
COMPILING BAYESIAN NETWORKS
Deﬁnition 12.1 then gives us
∂f
∂θy|v
(e) =

z∼yv

θx|u∼z
xu̸=yv
θx|u

λx∼z
1,
if x ∼e
0,
otherwise
=

z∼yv
z∼e

θx|u∼z
xu̸=yv
θx|u.
Multiplying both sides by θy|v, we get
θy|v
∂f
∂θy|v
(e) =

z∼yv
z∼e

θx|u∼z
θx|u
=

z∼yv
z∼e
Pr(z)
= Pr(y, v, e).
■
Lemma 12.1 (Path coefﬁcients). Consider a maximizer circuit ACm evaluated at some
evidence e and let α be a path from the circuit root to some leaf node n. Deﬁne the
coefﬁcient r of path α as the product of values attained by nodes c, where c is not on path
α but has a multiplication parent on α. Then r · k is the maximum value attained by any
complete subcircuit that includes path α, where k is the value of leaf node n. (The proof
of this Lemma is left for Exercise 12.18.)
PROOF OF THEOREM 12.3. By Lemma 12.1, the maximum coefﬁcient value attained
by any path from the circuit root to parameter θx|u is also the maximum value attained
by any complete subcircuit that includes parameter θx|u. Suppose now that α1, . . . , αm
are all the paths from the root to parameter θx|u and let r1, . . . , rm be their coefﬁcients,
respectively. We then have
∂f m
xu
∂θx|u
(e) =
m
max
i=1 r1, . . . , rm.
Therefore, if we can compute the maximum of these coefﬁcients, we can also compute
the derivative.
We can now reduce the problem to an all-pairs shortest path problem. In particular, let
the weight of edge v →c be 0 = −ln 1 when v is a maximization node and let the weight
of edge v →c be −ln π when v is a multiplication node, where π is the product of the
values of the other children c′ ̸= c of node v. The length of path αi is then −ln ri, which
is the sum of weights of αi’s edges.
We can easily verify that the second pass of Algorithm 36 is just the all-pairs shortest
path algorithm with edge weights as deﬁned previously. The same analysis applies to a
leaf node corresponding to indicator λx.
■
PROOF OF THEOREM 12.4. We have
f m
y = max
z∼y

θx|u∼z
θx|u

λx∼z
λx
= λy max
z∼y

θx|u∼z
θx|u

λx∼z
x̸=y
λx.

12.6 PROOFS
311
By the deﬁnition of partial derivatives, we have
∂f m
y
∂λy
= max
z∼y

θx|u∼z
θx|u

λx∼z
x̸=y
λx.
Evaluating the derivative at evidence e,
∂f m
y
∂λy
(e) = max
z∼y

θx|u∼z
θx|u

λx∼z
x̸=y
1,
if x ∼e
0,
otherwise
= max
z∼y
z∼e−Y

θx|u∼z
θx|u
= max
z∼y
z∼e−Y
Pr(z)
= MPEP (y, e −Y).
We also have
f m
yv = max
z∼yv

θx|u∼z
θx|u

λx∼z
λx
= θy|v max
z∼yv

θx|u∼z
xu̸=yv
θx|u

λx∼z
λx.
By the deﬁnition of partial derivatives, we have
∂f m
yv
∂θy|v
= max
z∼yv

θx|u∼z
xu̸=yv
θx|u

λx∼z
λx.
Evaluating the derivative at evidence e, we get
∂f m
yv
∂θy|v
(e) = max
z∼yv

θx|u∼z
xu̸=yv
θx|u

λx∼z
1,
if x ∼e
0,
otherwise
= max
z∼yv
z∼e

θx|u∼z
xu̸=yv
θx|u.
Multiplying both sides by θy|v, we get
θy|v
∂f m
yv
∂θy|v
(e) = max
z∼yv
z∼e

θx|u∼z
θx|u
= max
z∼yv
z∼e
Pr(z)
= Pr(y, v, e).
■
PROOF OF THEOREM 12.5. That the embedded arithmetic circuit represents the net-
work polynomial follows from the semantics of the jointree algorithm, whose trace for
computing the probability of evidence corresponds to the circuit (just pull messages
toward the root cluster and then sum the entries of this cluster).
By Deﬁnition 12.7, there is a one-to-one correspondence between multiplication nodes
and cluster instantiations; hence, the number of multiplication nodes is O(n exp(c)). Sim-
ilarly and except for the root node, there is a one-to-one correspondence between addition

312
COMPILING BAYESIAN NETWORKS
nodes and separator instantiations; hence, the number of addition nodes is O(n exp(s))
since the number of jointree edges is n −1.
As for the number of edges, note that the circuit alternates between addition and
multiplication nodes, where input nodes are always children of multiplication nodes.
Hence, we count edges by simply counting the total number of neighbors (parents and
children) that each multiplication node has. By Deﬁnition 12.7, each multiplication node
has a single parent. Moreover, the number of children that a multiplication node c has
depends on the cluster C that generates it. Speciﬁcally, the node has one child s for each
child separator S, one child λx for each evidence table λX assigned to cluster C, and one
child θx|u for each CPT θX|U assigned to the same cluster.
Now let r be the root cluster, i be any cluster, ci be the cluster size, ni be the number
of its neighbors, and ei and pi be the numbers of evidence indicators and CPTs assigned
to the cluster, respectively. The total number of neighbors for multiplication nodes is then
bounded by
exp(cr)(nr + 1 + er + pr) +

i̸=r
exp(ci)(ni + ei + pi).
Note that a multiplication node generated by the root cluster has one addition parent and
nr addition children, while a multiplication node generated by a nonroot cluster has one
addition parent and ni −1 addition children. Since, ci ≤c for all i, we can bound the
number of edges by
exp(c) + exp(c)

i
(ni + ei + pi).
Note also that the number of edges in a tree is one less than the number of nodes, leading
to 
i ni = 2(n −1). Moreover, we have 
i ei = n and 
i pi = n since we only have
n evidence indicators and n CPTs. Hence, the total number of edges can be bounded by
(4n −1) exp(c), which is O(n exp(c)).
■

13
Inference with Local Structure
We discuss in this chapter computational techniques for exploiting certain properties of
network parameters, allowing one to perform inference efﬁciently in some situations
where the network treewidth can be quite large.
13.1 Introduction
We discussed in Chapters 6–8 two paradigms for probabilistic inference based on elim-
ination and conditioning, showing how they lead to algorithms whose time and space
complexity are exponential in the network treewidth. These algorithms are often called
structure-based since their performance is driven by the network structure and is in-
dependent of the speciﬁc values attained by network parameters. We also presented in
Chapter 11 some CNF encodings of Bayesian networks, allowing us to reduce probabilis-
tic inference to some well-known CNF tasks. The resulting CNFs were also independent
of the speciﬁc values of network parameters and are therefore also structure-based.
However, the performance of inference algorithms can be enhanced considerably if one
exploits the speciﬁc values of network parameters. The properties of network parameters
that lend themselves to such exploitation are known as parametric or local structure. This
type of structure typically manifests in networks involving logical constraints, context-
speciﬁc independence, or local models of interaction, such as the noisy-or model discussed
in Chapter 5.
In this chapter, we present a number of computational techniques for exploiting local
structure that can be viewed as extensions of inference algorithms discussed in earlier
chapters. We start in Section 13.2 with an overview of local structure and the impact it
can have on the complexity of inference. We then provide three sets of techniques for
exploiting local structure. The ﬁrst set concerns the encoding of Bayesian networks into
CNFs, where we show in Section 13.3 how these CNFs can be reﬁned so they encode local
structure as well. The second set of techniques is discussed in Section 13.4 and is meant to
reﬁne conditioning algorithms so their performance is not necessarily exponential in the
network treewidth. The third set of techniques is discussed in Section 13.5 and is meant
to reﬁne elimination algorithms for the same purpose.
13.2 The impact of local structure on inference complexity
Perhaps the simplest way to illustrate the impact of local structure on the complexity of
inference is through the effect it has on the circuit complexity of Bayesian networks. Con-
sider for example, Figure 13.1, which depicts a Bayesian network and two corresponding
arithmetic circuits. The circuit in Figure 13.1(a) is compiled from the network as given
in Chapter 12, that is, without exploiting the values of network parameters. The circuit
in Figure 13.1(b) is obtained from the previous circuit by ﬁrst substituting the values of
313

314
INFERENCE WITH LOCAL STRUCTURE
A
B
C
A
θA
true
.5
false
.5
A
B
θB|A
true
true
1
true
false
0
false
true
0
false
false
1
A
C
θC|A
true
true
.8
true
false
.2
false
true
.2
false
false
.8
+
+
+
*
*
*
*
*
*
+
+
*
*
*
*
a
λ
a
λ
a
θ
a
θ
b
λ
c
λ
c
λ
b
λ
a
b|
θ
a
b|
θ
a
b|
θ
a
b|
θ
a
c|
θ
a
c|
θ
a
c|
θ
a
c|
θ
(a) Arithmetic circuit valid for any network parameters
+
*
*
+
+
*
*
*
*
.2
.8
*
.5
a
λ
a
λ
b
λ
c
λ
c
λ
b
λ
(b) Arithmetic circuit valid only for speciﬁc network parameters
Figure 13.1: A Bayesian network and two corresponding arithmetic circuits.
network parameters and then simplifying the circuit. As this example illustrates, the size
of an arithmetic circuit can be quite dependent on the values of network parameters. For
example, zero parameters can lead to substantial reductions in the circuit size and so does
equality among the values of distinct parameters.
For another example, consider the network in Figure 13.2, which has a treewidth of
n, making it inaccessible to structure-based algorithms when n is large enough. Suppose
now that each variable has values in {0, 1}, that variables Xi have uniform distributions,

13.2 THE IMPACT OF LOCAL STRUCTURE ON INFERENCE COMPLEXITY
315
X1
X2
Xn
Y
Figure 13.2: A Bayesian network with treewidth n.
and that node Y is a logical-or of its parents. Under these conditions, θy|x1,...,xn = 1 if and
only if either:
r y =1 and xi =1 for some xi, or
r y =0 and xi =0 for all xi.
Given these parameter values, the network polynomial can be factored as follows:
f =

x1,...,xn,y
λx1 . . . λxnλyθx1 . . . θxnθy|x1,...,xn
=
1
2
	n
⎛
⎜⎝λy=0λx1=0 . . . λxn=0 + λy=1

x1,...,xn
xi=1 for some i
λx1 . . . λxn
⎞
⎟⎠
=
1
2
	n
⎛
⎝λy=0
n

i=1
λxi=0 + λy=1
n

i=1
⎛
⎝
i−1

j=1
λxj=0
⎞
⎠λxi=1
⎛
⎝
n

j=i+1
(λxj=0 + λxj=1)
⎞
⎠
⎞
⎠. (13.1)
Given this factorization, we can represent the polynomial using an arithmetic circuit of
size O(n) even though the underlying network has treewidth n (see Exercise 13.1). Hence,
inference on this network can be done in time and space that are linear in its treewidth
n.1 In fact, this complexity continues to hold even if variables Xi do not have uniform
distributions (see Exercise 13.3).
The CPT of variable Y in the previous example is deterministic, that is, all of its
parameters are equal to 0 or 1. We can obtain a similar reduction in complexity even if
the CPT is not deterministic. Suppose for example that Y is a soft-or of its parents, that
is, if any of the parents is 1, then Y is 1 with probability ϵ1. Similarly, if all parents are 0,
then Y is 0 with probability ϵ0. Under these conditions, the network polynomial factors
as follows:
1
2
	n 
(ϵ0λy=0 + (1 −ϵ0)λy=1)
n

i=1
λxi=0
+

(1 −ϵ1)λy=0 + ϵ1λy=1

n

i=1
⎛
⎝
i−1

j=1
λxj=0
⎞
⎠λxi=1
⎛
⎝
n

j=i+1
(λxj=0 + λxj=1)
⎞
⎠
⎞
⎠.
(13.2)
1 The same reduction in complexity can be obtained by a technique known as CPT decomposition. In particular, it is
well known that an n-input or-gate can be simulated by a number of two-input or-gates that are cascaded together. If
we employ this technique, we can represent the network in Figure 13.2 by an equivalent network having treewidth
2. Again, this decomposition is only possible due to the speciﬁc values of network parameters.

316
INFERENCE WITH LOCAL STRUCTURE
This can also be represented by an arithmetic circuit of size O(n), allowing us to perform
inference in time and space that are linear in n (see Exercise 13.3).
The summary from these examples is that when network parameters exhibit a certain
structure, exact inference can be performed efﬁciently even if the network treewidth is
quite large.
13.2.1 Context-speciﬁc independence
The two networks discussed previously share a common type of local structure known as
context-speciﬁc independence. In particular, given that some node Xi takes the value 1,
the probability of Y becomes independent of other parents Xj for j ̸= i:
Pr(Y|xi =1) = Pr(Y|x1, . . . , xi−1, xi =1, xi+1, . . . , xn),
for all values x1, . . . , xi−1, xi+1, . . . , xn. This type of independence is called context-
speciﬁc as it is holds only for certain values of Xi, that is, the independence would not
hold if Xi takes the value 0. Context-speciﬁc independence is therefore a function of
network parameters and may be lost when changing the values of these parameters. This
is to be contrasted with variable-based independence discussed in Chapters 3 and 4, which
is a function of the network structure and would continue to hold for any values of network
parameters.
The existence of context-speciﬁc independence can be sufﬁcient for generating arith-
metic circuits whose size is not exponential in the network treewidth yet one may attain
such a complexity even without the existence of context-speciﬁc independence. For ex-
ample, consider Figure 13.2 and suppose that Y is 1 iff an odd number of its parents are
1. In this case, the probability of Y remains dependent on a particular parent Xi even if
all other parents Xj are known; yet the network still admits an arithmetic circuit of linear
size (see Exercise 13.4).
13.2.2 Determinism
Consider the grid network in Figure 13.3, composed of n × n binary nodes. The treewidth
of this network grows linearly in n yet if we assume that each node is a Boolean function
of its parents, then we can describe an arithmetic circuit of size O(n2) for this network.
To see this, note that setting the value of node I is guaranteed to imply the value of each
and every other node in the network. Hence, the number of nonvanishing terms in the
network polynomial must equal the number of values for node I and each term has size
O(n2). The network polynomial will then have two terms, each of size O(n2).
For a more general characterization of this example and similar ones, consider Bayesian
networks in which every nonroot node is functionally determined by its parents. That is,
for every node X with parents U ̸= ∅, we have Pr(x|u) = 1 for each parent instantiation u
and some value x. This means that Pr(x′|u) = 0 for all other values x′ ̸= x and, hence, the
value u of parents U implies the value x of child X. For these networks, called functional
networks, the number of nonvanishing terms in the network polynomial is exponential
only in the number of network roots, regardless of the network treewidth.
A more general type of local structure known as determinism occurs whenever network
parameters take on zero values. Functional networks exhibit a strong type of determinism
yet one may have this form of local structure even when a node is not functionally

13.2 THE IMPACT OF LOCAL STRUCTURE ON INFERENCE COMPLEXITY
317
I
Figure 13.3: A Bayesian network with a grid structure having n2 nodes. The treewidth of this class of
networks grows linearly in n.
D1
D2
D3
D4
S1
S2
S3
D1
D2
D3
D4
S1
S2
S3
(a)
(b)
Figure 13.4: On the left, a Bayesian network where each leaf node is a logical-or of its parents. On the
right, the result of pruning (dotted edges) the network given that S1 is false and that S2 and S3 are true.
determined by its parents. For example, variable X may not be functionally determined
by its parents U yet one of its values x may be impossible given the parent instantiation
u, θx|u = 0. The impossibility of x given u is known as a logical constraint. Logical
constraints can signiﬁcantly reduce the complexity of inference even when they do not
correspond to functional dependencies.
13.2.3 Evidence
Local structure can be especially effective computationally in the presence of particular
evidence. Consider the network in Figure 13.4(a) as an example and assume that each
node Si is a logical-or of its parents. Suppose now that we observe S1 to be false and
S2 and S3 to be true. Since S1 is false and given that it is a logical-or of its parents D1,
D2, and D3, we can immediately conclude that all of these parents must also be false.
Using the edge-pruning technique from Chapter 6, we can now prune all edges that are
outgoing from these nodes, leading to the network in Figure 13.4(b). The key point here is
that this pruning was enabled by the speciﬁc evidence that sets S1 to false and by the
speciﬁc relationship between S1 and its parents. In particular, this pruning would not be
possible if S1 were set to true, neither would it be possible if S1 were, say, a logical-and
of its parents. More generally, we can show that certain inferences on networks of the
type given in Figure 13.4 can be performed in time and space that is exponential only in
the number of nodes Si whose values are observed to be true, regardless of the network
treewidth (see Exercises 13.19 and 13.20).

318
INFERENCE WITH LOCAL STRUCTURE
C1
C2
C3
E
θe|c1,c2,c3
true
true
true
true
.9999265
true
true
false
true
.99853
true
false
true
true
.99265
true
false
false
true
.853
false
true
true
true
.99951
false
true
false
true
.9902
false
false
true
true
.951
false
false
false
true
.02
Figure 13.5: A CPT with no determinism or equal parameters.
1
C
2
C
3
C
E
2
Q
3
Q
1
Q
L
1A
2
A
3
A
1
C
2
C
3
C
2
Q
3
Q
L
1
Q
E
Figure 13.6: A noisy-or model.
13.2.4 Exposing local structure
Consider the CPT in Figure 13.5 relating a node E to its parents C1, C2, and C3. The
parameters of this CPT do not exhibit any of the structures discussed previously such as
determinism, context-speciﬁc independence, or parameter equality.
As it turns out, this CPT is generated by a noisy-or model using the following parameters
(see Section 5.4.1):
r The suppressor probability for C1 is .15.
r The suppressor probability for C2 is .01.
r The suppressor probability for C3 is .05.
r The leak probability for E is .02.
Suppose that we now replace the family in Figure 13.5 by the network fragment in
Figure 13.6 to explicate this noisy-or semantics (again, see Section 5.4.1). Given this
expansion, the CPTs for E, A1, A2, A3 are now all deterministic, showing much more
local structure than was previously visible. In fact, the techniques we discuss in the rest of
this chapter will not be effective on this network unless the corresponding local structure is
exposed as we just discussed. The main point here is that we can expose local structure by
decomposing CPTs. Alternatively, we can hide local structure by summing out variables
from a Bayesian network (e.g., variables Q1, A1, . . . , Q3, A3 and L in Figure 13.6),
which can negatively impact the effectiveness of algorithms that attempt to exploit local
structure.

13.3 CNF ENCODINGS WITH LOCAL STRUCTURE
319
We next present three sets of techniques for exploiting local structure that correspond to
reﬁnements on inference algorithms discussed in previous chapters. These include reﬁne-
ments on CNF-based algorithms (Sections 13.3), reﬁnements on conditioning algorithms
(Section 13.5), and reﬁnements on elimination algorithms (Section 13.5).
13.3 CNF encodings with local structure
We proposed a CNF encoding of Bayesian networks in Section 11.6.1 and then presented
two corresponding methods that use the encoding to perform inference on Bayesian
networks. In particular, we showed in Section 11.6.1 how the CNF encoding can be
used with a model counter to compute the probability of evidence. We then showed in
Section 12.4.3 how the encoding can be used to compile out an arithmetic circuit for the
corresponding network.
However, the CNF encoding of Section 11.6.1 was structure-based as it did not depend
on the speciﬁc values of network parameters. We provide a more reﬁned encoding in
this section that exploits these values, leading to improved performance of both model
counters and circuit compilers.
13.3.1 Encoding network structure
Consider the CNF encoding discussed in Section 11.6.1, which includes an indicator
variable Ix for each network variable X and value x and a parameter variable Px|u for
each network parameter θx|u. If we apply this encoding to the network in Figure 13.1, we
obtain a CNF that consists of two types of clauses. The ﬁrst set of clauses, called indicator
clauses, are contributed by network variables:
Ia ∨I¯a
Ib ∨I¯b
Ic ∨I¯c
¬Ia ∨¬I¯a
¬Ib ∨¬I¯b
¬Ic ∨¬I¯c
The second set of clauses, called parameter clauses, are contributed by network parame-
ters:2
Ia
⇐⇒
Pa
I¯a
⇐⇒
P¯a
Ia ∧Ib
⇐⇒
Pb|a
Ia ∧I¯b
⇐⇒
P¯b|a
I¯a ∧Ib
⇐⇒
Pb|¯a
I¯a ∧I¯b
⇐⇒
P¯b|¯a
Ia ∧Ic
⇐⇒
Pc|a
Ia ∧I¯c
⇐⇒
P¯c|a
I¯a ∧Ic
⇐⇒
Pc|¯a
I¯a ∧I¯c
⇐⇒
P¯c|¯a
(13.3)
2 Note here that each of these sentences corresponds to a set of clauses. For example, the sentence Ia ∧Ib ⇐⇒Pb|a
corresponds to three clauses: Ia ∧Ib =⇒Pb|a, which is called an IP clause, and Pb|a =⇒Ia and Pb|a =⇒Ib,
which are called PI clauses.

320
INFERENCE WITH LOCAL STRUCTURE
We have already shown two particular uses of these CNF encodings in prior chapters:
r Model counters (Section 11.6.1): If we assign a weight of 1 to all literals Ix, ¬Ix,
and ¬Px|u and assign a weight of θx|u to each literal Px|u, the weighted model count
of  ∧Ie1 ∧. . . ∧Ien then corresponds to the probability Pr(e1, . . . , en). Hence, we
can compute the probability of evidence by applying a model counter to the CNF
encoding .
r Circuit compilers (Section 12.4.3): If the CNF encoding  is converted to an NNF circuit
that satisﬁes the properties of decomposability, determinism, and smoothness, then this
NNF circuit can be immediately converted into an arithmetic circuit for the corresponding
Bayesian network. Hence, we can compile an arithmetic circuit by applying an NNF-
circuit compiler to the CNF encoding .
Note that the CNF encoding discussed here does not exploit local structure, as this structure
is only encoded in the weights assigned to literals. We next discuss a more reﬁned encoding
that captures this local structure directly into the CNF.
13.3.2 Encoding local structure
We now show how we can encode three different properties of network parameters in the
CNF of a Bayesian network, leading to smaller CNFs that tend to be easier for model
counters and NNF-circuit compilers.
Zero parameters
Consider the Bayesian network in Figure 13.1, which includes local structure, some of it
in the form of determinism. Consider the parameter θ¯b|a = 0 in particular, which adds the
following clauses to the CNF encoding:
Ia ∧I¯b ⇐⇒P¯b|a.
These clauses ensure that a model of the CNF sets the variable P¯b|a to true if and only if it
sets the variables Ia and I¯b to true. Note, however, that the weight assigned to the positive
literal P¯b|a is 0 in this case. Hence, any model that sets P¯b|a to true has a zero weight and
does not contribute to the weighted model count. We can therefore replace the previous
clauses with the single clause
¬Ia ∨¬I¯b
without affecting the weighted model count of any query (the clause will only eliminate
models that have a zero weight). Note that this technique also has the affect of removing
variable P¯b|a from the CNF encoding.
More generally, for every zero parameter θx|u1,...,un = 0, we can replace its clauses by
the single clause
¬Iu1 ∨. . . ∨¬Iun ∨¬Ix,
and drop the variable Px|u1,...,un from the encoding.
One parameters
Consider now the parameter θb|a = 1 for variable B in Figure 13.1. Given the value of this
parameter, the positive literal Pb|a has a weight of 1, which will not affect the weight of

13.3 CNF ENCODINGS WITH LOCAL STRUCTURE
321
any model that sets variable Pb|a to true. We can therefore drop the clauses of this variable
without changing the weighted model count of any query. More generally, parameters that
are equal to 1 need not generate any clauses or variables.
Equal parameters
The Bayesian network in Figure 13.1 exhibits another form of local structure known as
parameter equality. For example, θc|a = θ¯c|¯a = .8 in the CPT of variable C. This basically
means that the positive literals Pc|a and P¯c|¯a will have the same weight of .8. We can
therefore replace their clauses
Ia ∧Ic ⇐⇒Pc|a
I¯a ∧I¯c ⇐⇒P¯c|¯a
with the following sentence:
(Ia ∧Ic) ∨(I¯a ∧I¯c) ⇐⇒P1,
where P1 is a new variable that has the weight .8. Note that the sentence is not in CNF, so
it needs to be converted to a set of clauses to keep the encoding in CNF (see Section 2.7.3).
More generally, let F = F1, . . . , Fn be the variables appearing in a CPT and let
f1, . . . , fm be a set of instantiations of F that correspond to equal parameters θ1, . . . , θm.
We can then replace these parameters with a new parameter η in the encoding. Moreover,
we can replace their clauses with the sentence
(If1 ∨. . . ∨Ifm) ⇐⇒η,
where Ifi denotes Ifi1 ∧. . . ∧Ifin when fi = fi1, . . . , fin.
Putting it all together
Applying these encoding techniques to the Bayesian network in Figure 13.1, we obtain
the following CNF:
Ia ∨I¯a
Ib ∨I¯b
Ic ∨I¯c
¬Ia ∨¬I¯a
¬Ib ∨¬I¯b
¬Ic ∨¬I¯c
¬Ia ∨¬I¯b
¬I¯a ∨¬Ib
(Ia ∧Ic) ∨(I¯a ∧I¯c)
⇐⇒
P1
(weight of P1 is .8)
(Ia ∧I¯c) ∨(I¯a ∧Ic)
⇐⇒
P2
(weight of P2 is .2)
Ia ∨I¯a
⇐⇒
P3
(weight of P3 is .5)
(13.4)
This is to be contrasted with the CNF in (13.3), which corresponds to the same Bayesian
network but does not encode local structure.
Let us now use this CNF to compile an arithmetic circuit as described in Section 12.4.3.
Figure 13.7(a) depicts an NNF circuit for this CNF, which satisﬁes the properties of de-
composability, determinism, and smoothness. Figure 13.7(b) depicts an arithmetic circuit

322
INFERENCE WITH LOCAL STRUCTURE
3P
2P
1P
∧
∨
aI
aI
¬
aI
aI
¬
bI
bI
¬
bI
bI
¬
cI
cI
¬
cI
cI
¬
∧
∧
∧
∧
∧
∨
∨
∧
1P
¬
2P
¬
5.
2.
8.
*
+
aλ
a
λ
b
λ
b
λ
cλ
c
λ
*
*
*
*
*
+
+
*
(a) NNF circuit
(b) Arithmetic circuit
Figure 13.7: Extracting an arithmetic circuit from an NNF circuit.
that is extracted from this NNF circuit (after some simpliﬁcations).3 As expected, this
circuit is smaller than the one in Figure 13.1(a), which was constructed without exploiting
the speciﬁc values of network parameters.
There are other techniques for exploiting local structure beyond the ones discussed in
this section. Some of these techniques are suggested in Exercises 13.21 and 13.22. The
technique in Exercise 13.22 is known to produce the most efﬁcient encodings in practice.
13.3.3 Encoding evidence
Suppose that we have a CNF encoding  of a Bayesian network. When using CNF 
with a model counter, we must add the evidence to the CNF before calling the model
counter. In particular, if the evidence is e1, . . . , ek, then we must call the model counter
on the extended CNF  ∧Ie1 ∧. . . ∧Iek.
Consider now the use of a CNF to compile an arithmetic circuit. We have two choices
here for handling evidence:
r Compile a circuit from the CNF before incorporating any evidence. When a piece of
evidence arrives later, incorporate it into the circuit as discussed in Chapter 12. This is
the normal mode of using arithmetic circuits as it involves a single compilation, leading
to one circuit that is used to handle multiple pieces of evidence.
r Encode the evidence into the CNF  and then compile an arithmetic circuit. The circuit
must then be recompiled each time the evidence changes.
As an example of the second choice, consider the CNF in (13.4) and suppose the evidence
we have is ¯c. For the purpose of compiling this CNF into an arithmetic circuit, we
incorporate this evidence by conditioning on the corresponding indicator I¯c (that is, we
3 Note here that we plugged in the actual weights of variables P1, P2, and P3 in the circuit. We can keep the actual
variables and compute their derivatives, as discussed in Chapter 12, but we need to be careful in how we interpret
the values of these derivatives (see for example Exercise 12.19.)

13.4 CONDITIONING WITH LOCAL STRUCTURE
323
3P
1P
2P
∧
∨
aI
aI
¬
aI
aI
¬
bI
bI
¬
bI
bI
¬
∧
∧
1P
¬
2P
¬
cI
¬
5.
8.
2.
*
+
a
λ
a
λ
b
λ
b
λ
*
*
(a) NNF circuit
(b) Arithmetic circuit
Figure 13.8: Extracting an arithmetic circuit from an NNF circuit.
replace the literal I¯c by true) and then simplify, leading to the CNF
Ia ∨I¯a
Ib ∨I¯b
¬Ia ∨¬I¯a
¬Ib ∨¬I¯b
¬Ic
¬Ia ∨¬I¯b
¬I¯a ∨¬Ib
(Ia ∧Ic) ∨I¯a
⇐⇒
P1
Ia ∨(I¯a ∧Ic)
⇐⇒
P2
Ia ∨I¯a
⇐⇒
P3
Figure 13.8(a) depicts an NNF circuit for this CNF, which satisﬁes the properties of
decomposability, determinism, and smoothness. Figure 13.8(b) depicts the corresponding
arithmetic circuit.
The new arithmetic circuit is clearly smaller than the one in Figure 13.7(b), which was
compiled without evidence. Yet the new circuit can only be used to answer queries in
which ¯c is part of the evidence, such as a ¯c and ¯b¯c. This may sound too restrictive at ﬁrst
but there are many situations where a certain piece of evidence needs to be part of every
query posed. This includes, for example, genetic linkage analysis discussed in Chapter 5,
where the phenotype and genotype correspond to such evidence. The same is also true
for more general forms of inference such as sensitivity analysis, discussed in Chapter 16,
where we search for parameter values under certain query constraints (evidence), and in
MAP computations, discussed in Chapter 10, where we search for a variable instantiation
with maximal probability given some ﬁxed evidence.
If we have a piece of evidence that will be part of every query, it is then more effective
to incorporate it in the CNF encoding before generating an arithmetic circuit. In fact, the
size of resulting circuit may be exponentially smaller than the size of the circuit compiled
without evidence.
13.4 Conditioning with local structure
In this section, we consider some techniques for exploiting local structure in the context
of conditioning algorithms, recursive conditioning in particular.

324
INFERENCE WITH LOCAL STRUCTURE
B
C
X
A
ABC
B
C
X
A
ABC
(a) X independent of B, C given A=true
(b) Setting variable A to true
Figure 13.9: The role of local structure in reducing the number of cases considered by recursive condi-
tioning. The set ABC is a cutset for the corresponding dtree node.
We show that in the presence of local structure, recursive conditioning can reduce the
number of cases it needs to consider and the number of cache entries it has to maintain.
This can lead to exponential savings in certain cases, allowing an inference complexity
that is not necessarily exponential in the network treewidth.
13.4.1 Context-speciﬁc decomposition
Consider Figure 13.9(a) for an example that contains a dtree fragment with variables ABC
as a root cutset. According to the recursive conditioning algorithm, we need to instantiate
all three variables in order to decompose the network and recurse independently on the
resulting subnetworks. This means that eight cases must be considered, each corresponding
to one instantiation of the cutset. Suppose, however, that variable X is independent of
variables BC given that A is true (context-speciﬁc independence). It is then sufﬁcient in
this case to set A to true in order to decompose the network without the need to instantiate
variables B and C, as shown in Figure 13.9(b). That is, by setting A to true, not only can
we delete edges outgoing from A but also other edges coming into X. This means that
recursive conditioning needs to consider at most ﬁve cases instead of eight: one case for
A=true and up to four cases for A=false. More generally, if local structure is exploited
to apply context-speciﬁc decomposition as given previously, the complexity of recursive
conditioning may not be exponential in the size of cutsets, leading to exponential savings
in certain situations.
13.4.2 Context-speciﬁc caching
Consider now Figure 13.10(a) for another example where we have a dtree node T with
context ABC (the node is marked with a •). According to the recursive conditioning
algorithm, one needs to maintain a cache with eight entries at node T to store the values
of eight distinct computations corresponding to the instantiations of variables ABC.
Suppose, however, that variable X is independent of variables BC given A=true. When
A is set to true, the subnetwork corresponding to dtree node T will no longer be dependent
on the values of B and C as shown in Figure 13.10(b). Hence, all four computations
corresponding to A=true give the same answer, which means that four of the cache
entries at node T will be storing the same value. By accounting for this equivalence
among cache entries, not only do we reduce the size of cache entries at node T from eight

13.4 CONDITIONING WITH LOCAL STRUCTURE
325
B
C
X
A
ABC
BC
A
B
C
X
A
BC
A
ABC
(a) X independent of B, C given A=true
(b) Setting variable A to true
Figure 13.10: The role of local structure in reducing the number of cache entries maintained by recursive
conditioning. The sets BC and A are cutsets for the corresponding nodes. The set ABC is a context for
the corresponding node.
A
C
B
A
A
true
.7
false
.3
A
B
B|A
true
true
1
true
false
0
false
true
.8
false
false
.2
A
C
C|A
true
true
.4
true
false
.6
false
true
1
false
false
0
Figure 13.11: Exploiting determinism in recursive conditiong.
to at most ﬁve but we also reduce the number of recursions from node T from eight to
at most ﬁve. Again, if local structure is exploited to apply context-speciﬁc caching, as
given here, the complexity of recursive conditioning may not be exponential in the size
of contexts. This can lead to exponential savings in certain situations, especially that the
algorithm’s complexity need not be exponential in cutset sizes either.
13.4.3 Determinism
One particular attraction of conditioning algorithms is the ease with which they can ex-
ploit determinism. In particular, we can preprocess the network to generate a propositional
knowledge base  that captures the logical constraints implied by the network parameters
and then use logical deduction on this knowledge base to reduce the amount of work per-
formed by the algorithm. The knowledge base is generated by adding a logical constraint
(clause) for each zero parameter θxn|x1,...,xn−1 = 0 in the network:
¬(X1 =x1) ∨. . . ∨¬(Xn−1 =xn−1) ∨¬(Xn =xn).
For an example, consider the network fragment in Figure 13.11. There are two zero
parameters in the given CPTs, θ B=false| A=true and θ C=false| A=false, leading to the following
knowledge base :
¬(A=true) ∨¬(B =false),
¬(A=false) ∨¬(C =false).
These logical constraints can be simpliﬁed to
(A=false) ∨(B =true),
(A=true) ∨(C =true).

326
INFERENCE WITH LOCAL STRUCTURE
Suppose now that in the course of performing case analysis on variable B, we set this
variable to false. We would then add B =false to the knowledge base  and apply logical
deduction to the resulting knowledge base. This leads to deducing A=false and also
C =true. If we move on to performing case analysis on variable C, we would then skip
the case C =false as it is guaranteed to have a zero probability. This skipping technique
can be quite effective in the course of recursive conditioning as it can be applied at each
dtree node and for each case considered at that node.
Note, however, that when applying this technique in practice, we do not typically use
general logical deduction on the knowledge base due to the associated computational cost.
Instead, unit resolution (discussed in Chapter 2) is commonly used for this purpose as it
can be implemented in time linear in the knowledge base size, even though it may not be
able to ﬁnd all possible implications of the knowledge base.
13.5 Elimination with local structure
In this section, we consider some techniques for exploiting local structure in the context
of variable elimination algorithms.
The exploitation of local structure by elimination algorithms is relatively straight-
forward at the conceptual level. In a nutshell, the presence of local structure implies
that factors can be represented more compactly using data structures that are more so-
phisticated than the tabular representation used in Chapter 6. As a result, the size of
these factors will no longer be necessarily exponential in the number of variables ap-
pearing in a factor, which can have a dramatic effect on the complexity of elimination
algorithms.
To elaborate on this point, recall that our complexity analysis of variable elimination
in Chapter 6 produced the following result. Given an elimination order of width w,
the time and space complexity of the algorithm is O(n exp(w)) where n is the number
of network variables. If we re-examines this complexity result, we ﬁnd that a factor
generated by variable elimination can contain as many as w variables. When factors
are represented using tables, their size can then be exponential in w and the worst-
case complexity of O(n exp(w)) is reached. However, given a more sophisticated data
structure for representing factors, their size may never be exponential in the number of
their variables. In this case, the complexity of O(n exp(w)) is only an upper bound and
the average case complexity may be much better.
We can therefore exploit local structure in elimination algorithms by adopting a non-
tabular representation of factors and by supplying the necessary operations on this rep-
resentation:
r Multiplication, where we compute the product of factors f1 and f2, f1f2.
r Summing out, where we sum out variable X from factor f , leading to 
X f .4
r Reduction, where we reduce a factor f given evidence e, leading to f e.
There are a number of nontabular representations of factors that have been proposed in
the literature. In the rest of this chapter, we focus on one such representation based on
decision diagrams. In particular, we describe this representation and its operations in
Sections 13.5.1–13.5.2. We then show in Section 13.5.3 how it can be employed in the
4 We also need maximizing out if we are computing most likely instantiations.

13.5 ELIMINATION WITH LOCAL STRUCTURE
327
X
Y
Z
f (.)
F
F
F
.9
F
F
T
.1
F
T
F
.9
F
T
T
.1
T
F
F
.1
T
F
T
.9
T
T
F
.5
T
T
T
.5
X
Z
.1 .9
Y
.5
Z
Figure 13.12: A factor over binary variables X, Y, Z with a tabular representation (left) and an ADD
representation (right).
context of variable elimination. We ﬁnally show in Section 13.5.4 how it can be used
to compile arithmetic circuits by keeping a trace of variable elimination as we did in
Chapter 12.
13.5.1 Algebraic decision diagrams
In this section, we describe a representation of factors using algebraic decision diagrams
(ADDs), a data structure that provides support for a variety of factor operations including
multiplication, summing out, and reduction given evidence.
Figure 13.12 depicts an example factor with its tabular and ADD representations. An
ADD is a DAG with one root node and multiple leaf nodes. In the ADD of Figure 13.12,
parents are drawn above their children, a layout convention that we adopt from here on.
Each nonleaf node in an ADD is labeled with a binary variable having two children, a
high child pointed to by a solid edge and a low child pointed to by a dotted edge. Each
leaf node is called a sink and labeled with a real number. The ADD in Figure 13.12 has
three sinks. The ADD must satisfy a variable ordering condition: the variables labeling
nodes must appear in the same order along any path from the root to a leaf node. In the
ADD of Figure 13.12, variables appear according to the order X, Y, Z. Note, however,
that a path need not mention every variable in the order yet all paths must be consistent
with a single total ordering of the ADD variables. An ADD is identiﬁed with its root
node.
To see how an ADD represents a factor, let us identify the value assigned by the ADD in
Figure 13.12 to the variable instantiation X=F, Y =T , Z =T . We always start at the ADD
root, labeled with X in this case. We check the value of variable X according to the given
instantiation. Since X has value F, we move to its low child, labeled with Z in this case.
Since Z has the value T , we move to its high child, which is a sink labeled with .1. Hence,
the factor assigns the value .1 to instantiation X=F, Z =T , independent of Y’s value.
That is, it assigns this value to both instantiations X=F, Y =T , Z =T and X=F, Y =F,
Z =T . Similarly, the factor assigns the value .9 to instantiation X=T , Y =F,
Z =T .
The ADD representation of factors is interesting for a number of reasons. First, the ADD
size can be exponentially smaller and never worse than the size of a tabular representation
of the same factor – Figure 13.13 depicts an example showing an exponential difference

328
INFERENCE WITH LOCAL STRUCTURE
X1
.4
.2
X2
X2
X3
X3
X4
X4
Xn
Xn
Figure 13.13: An ADD representing a factor f (X1, . . . , Xn), where f (x1, . . . , xn) = .2 if an odd num-
ber of values xi are true and f (x1, . . . , xn) = .4 otherwise. The ADD size is O(n), while the tabular
representation is O(exp(n)).
in size. Second, ADD operations can be implemented efﬁciently. In particular, given two
ADDs f1 and f2 of sizes n and m, respectively, the product f1f2 can be computed in
O(nm) time, summing out 
X f1 can be computed in O(n2) time, and the reduced factor
f1e can be computed in O(n) time. Summing out variables can also be accomplished in
O(n) time in some cases (see Exercise 13.17).
ADDs can contain redundant nodes. An ADD node is redundant if its high and low
children are the same or if it has the same high and low children as another node in the
ADD. In the ﬁrst case, we simply remove the node from the ADD and direct all its parents
to one of its identical children. In the second case, we also remove the node from the
ADD and direct all its parents to the duplicate node. Figure 13.14 depicts an example of
removing redundant nodes from an ADD. The resulting ADD is said to be reduced as it
contains no redundant nodes. From now on, we assume that ADDs are reduced unless
mentioned otherwise.
ADD representations satisfy another interesting property. Once we ﬁx the variable
ordering, each factor has a unique reduced ADD representation. Therefore, the corre-
sponding ADD size is only a function of the variable order used in the ADD, which is
why ADDs are said to be a canonical representation. However, we should stress that dif-
ferent variable orders can lead to signiﬁcant differences in the ADD size. The difference
can be exponential in the number of ADD variables in some cases. We also point out that
the ADD variable order is independent and distinct from the variable order used by the
variable elimination algorithm. However, experimental results suggest that using an ADD
variable order that reverses the variable elimination order tends to produce good results
(see Exercise 13.17).
13.5.2 ADD operations
We now discuss the three factor operations required by variable elimination, assuming
that factors are represented by ADDs. We also discuss the automated conversion of tabular

13.5 ELIMINATION WITH LOCAL STRUCTURE
329
X0
.4
.5
.7
X1
X1
X2
X2
X3
X0
.4
.5
.7
X1
X1
X2
X2
X0
.4
.5
.7
X1
X1
X2
X0
.4
.5
.7
X1
X2
(a)
(b)
(c)
(d)
Figure 13.14: (a) An ADD with redundant nodes; (b) after removing the redundant node labeled with
X3; (c) after removing one of the redundant nodes labeled with X2; and (d) after removing the redundant
node labeled with X1.
representations of factors into ADD representations. This allows a complete implementa-
tion of variable elimination using ADDs.
All ADD operations of interest rest on two primitive operations called apply and
restrict, which we discuss next.
Apply
The APPLY operation takes two ADDs representing factors f1(X) and f2(Y) and a binary
numeric operation ⊙and returns a new ADD representing the factor f1 ⊙f2 over variables
Z = X ∪Y, deﬁned as
(f1 ⊙f2)(z)
def= f1(x) ⊙f2(y), where x ∼z and y ∼z.
For example, by taking ⊙to be numeric multiplication, we can use the APPLY operation to
compute the product of two ADDs. Algorithm 37 provides pseudocode for this operation,
which is guaranteed to run in O(nm) time where n and m are the sizes of given ADDs
(see also Exercise 13.16 for another bound on the size of ADD returned by APPLY).5
Figure 13.15 provides an example of using the APPLY operation to compute the product
of two ADDs.
The APPLY operation can also be used to reduce a factor f given some evidence e, f e.
All we have to do here is construct an ADD f ′ over variables E, which maps instantiation
e to 1 and all other instantiations to 0. The product ff ′ will then represent the reduced
factor f e.
Restrict
The second primitive operation on ADDs is called restrict and can be used to implement
the sum-out operation. Restrict takes an ADD representing factor f (X, E) and a variable
instantiation e and then returns an ADD representing factor f ′(X) = 
E f e, that is,
5 The pseudocode of Algorithm 37 does not include standard optimizations which can have a dramatic effect on
performance. These include checking for situations where one of the arguments to APPLY is a zero or identity
element for the operation ⊙.

330
INFERENCE WITH LOCAL STRUCTURE
Algorithm 37 APPLY(ϕ1, ϕ2, ⊙)
input:
ϕ1:
reduced ADD that respects variable order ≺
ϕ2:
reduced ADD that respects variable order ≺
⊙:
numeric operation
output: reduced ADD ϕ1 ⊙ϕ2
main:
1: swap ϕ1 and ϕ2 if var(ϕ2) ≺var(ϕ1)
2: if cache(ϕ1, ϕ2) ̸= nil then
3:
return cache(ϕ1, ϕ2)
4: else if ϕ1 and ϕ2 are sinks then
5:
ϕ←unique sink(label(ϕ1), label(ϕ2), ⊙)
6: else if var(ϕ1) = var(ϕ2) then
7:
l ←APPLY(low(ϕ1), low(ϕ2), ⊙)
8:
h ←APPLY(high(ϕ1), high(ϕ2), ⊙)
9:
ϕ ←l if l = h otherwise ϕ ←unique node(l, h, var(ϕ1))
10: else {var(ϕ1) ≺var(ϕ2)}
11:
l ←APPLY(low(ϕ1), ϕ2, ⊙)
12:
h ←APPLY(high(ϕ1), ϕ2, ⊙)
13:
ϕ ←l if l = h otherwise ϕ ←unique node(l, h, var(ϕ1))
14: end if
15: cache(ϕ1, ϕ2) ←ϕ
16: return ϕ
supporting functions: The functions var(ϕ), low(α), and high(α) return the variable of
node ϕ, its low child, and its high child, respectively. If ϕ is a sink, then var(ϕ) is a variable
that follows every other variable in the order. The function label(α) returns the label of
sink node α. The function cache(ϕ1, ϕ2) remembers the result of calling apply on the
ADDs ϕ1 and ϕ2. The function unique sink(l1, l2, ⊙) returns a sink with label l1 ⊙l2 if
one already exists, otherwise it creates one. The function unique node(ϕ1, ϕ2, V ) returns
a node labeled with variable V and having low child ϕ1 and high child ϕ2 if one exists,
otherwise creates one.
f ′(x) = f (x, e). Algorithm 38 provides pseudocode for this operation, which runs in
O(n) time where n is the size of given ADD. Figure 13.16 provides an example of
restricting an ADD to two different instantiations.
Suppose now that we wish to sum out variable X from a factor f . We can do this by
ﬁrst restricting f to X=F, then to X=T , and then adding up the results:

X
f =

X
f X=F

+

X
f X=T

.
This allows us to sum out a variable from an ADD by simply using the restrict operation
twice followed by the apply operation with ⊙deﬁned as numeric addition +. If the
ADD has size n, then summing out will run in O(n2) time. If the summed-out variable
appears last in the variable ordering, then we can perform the operation in O(n) time (see
Exercise 13.17). Figure 13.16 depicts an example of implementing the sum-out operation

13.5 ELIMINATION WITH LOCAL STRUCTURE
331
X
Z
.1
.9
Y
.5
Z
Z
0
1
W
W
X
Z
.1 .9
Y
.5
W
W
W
W
Z
Z
W
W
0
X
Y
Z
f1(.)
F
F
F
.1
F
F
T
.9
F
T
F
.1
F
T
T
.9
T
F
F
.9
T
F
T
.1
T
T
F
.5
T
T
T
.5
Z
W
f2(.)
F
F
0
F
T
1
T
F
1
T
T
0
X
Y
Z
W
f3(.)
F
F
F
F
0
F
F
F
T
.1
F
F
T
F
.9
F
F
T
T
0
F
T
F
F
0
F
T
F
T
.1
F
T
T
F
.9
F
T
T
T
0
T
F
F
F
0
T
F
F
T
.9
T
F
T
F
.1
T
F
T
T
0
T
T
F
F
0
T
T
F
T
.5
T
T
T
F
.5
T
T
T
T
0
(a) f1
(b) f2
(c) f1f2
Figure 13.15: Computing the product of two ADDs.
using a combination of restrict and apply, leading to an ADD that represents the following
factor:
X
Y
W
F
F
F
.9
F
F
T
.1
F
T
F
.9
F
T
T
.1
T
F
F
.1
T
F
T
.9
T
T
F
.5
T
T
T
.5
From tabular to ADD representations
We now show how the APPLY operation can be used to convert a tabular representation of a
factor into an ADD representation. Consider Figure 13.17, which depicts a factor in tabular

332
INFERENCE WITH LOCAL STRUCTURE
Algorithm 38 RESTRICT(ϕ, e)
input:
ϕ:
reduced ADD
e:
variable instantiation
output: a reduced ADD 
E ϕe
main:
1: if cache(ϕ) ̸= nil then
2:
return cache(ϕ)
3: else if ϕ is a sink then
4:
return ϕ
5: else if var(ϕ) is instantiated to true in e then
6:
ϕ′ ←RESTRICT(high(ϕ), e)
7: else if var(ϕ) is instantiated to false in e then
8:
ϕ′ ←RESTRICT(low(ϕ), e)
9: else
10:
l ←RESTRICT(low(ϕ), e)
11:
h ←RESTRICT(high(ϕ), e)
12:
ϕ′ ←l if l = h otherwise ϕ′ ←unique node(l, h, var(ϕ1))
13: end if
14: cache(ϕ) ←ϕ′
15: return ϕ′
supporting functions: The functions cache(), var(), low(), high(), and unique node() are
as given by Algorithm 37.
form. The ADD in Figure 13.17(a) represents the ﬁrst row in this table. That is, the ADD
maps the instantiation X=F, Y =F, Z =F to .1 and maps every other instantiation to 0.
Similarly, the ADD in Figure 13.17(b) represents the second row in this table, mapping
instantiation X=F, Y =F, Z =T to .9 and all other instantiations to 0. By adding these
two ADDs using the APPLY operation, we obtain the ADD in Figure 13.17(c), which
represents the ﬁrst two rows in the table. That is, this ADD maps these two rows to the
corresponding values and maps every other row to 0. By constructing an ADD for each
remaining row and adding up the resulting ADDs, we obtain an ADD corresponding to
the given table. However, we can devise more efﬁcient methods for converting tabular
representations of factors into their ADD representations (see Exercise 13.12).
13.5.3 Variable elimination using ADDs
Now that we have a systematic method for converting tabular factors into their ADD rep-
resentations and given the ADD operations for multiplying, summing out, and reducing
ADDs, we are ready to provide a full implementation of variable elimination based on
ADDs. This would be similar to Algorithm 8, VE PR, of Chapter 6 except that we now use
ADD representations of factors and the corresponding ADD operations. Note that the use
of ADDs requires a variable ordering that must be respected by any ADDs on which the
APPLY operation is called. Using the reverse of the variable elimination order has been

13.5 ELIMINATION WITH LOCAL STRUCTURE
333
X
Z
.1 .9
Y
.5
W
W
W
W
Z
Z
W
W
0
X
W
.1 .9
Y
.5
W
(a) f
(b) 
Z f
X
.1 .9
Y
.5
W
W
W
0
X
.1 .9
Y
.5
W
W
W
0
(c) 
Z f Z=F
(d) 
Z f Z=T
Figure 13.16: Summing out a variable from an ADD by adding two of its restrictions.
X
Y
Z
Z|XY
F
F
F
.1
F
F
T
.9
F
T
F
.1
F
T
T
.9
T
F
F
.9
T
F
T
.1
T
T
F
.5
T
T
T
.5
X
Y
Z
0
.1
X
Y
Z
0
.9
X
Y
Z
0
.9
.1
(a)
(b)
(c)
Figure 13.17: From left to right: A tabular representation of a factor, an ADD representing its ﬁrst row, an
ADD representing its second row, and an ADD representing their summation.

334
INFERENCE WITH LOCAL STRUCTURE
observed empirically to give good results (see Exercise 13.17). However, a more sophis-
ticated approach would employ ADD packages that dynamically compute variable orders
within each APPLY operation with the intent of minimizing the size of resulting ADD.
Although dynamic variable ordering can lead to smaller ADDs, the overhead associated
with it may not be justiﬁable unless the reduction in ADD size is quite signiﬁcant. In
general, the use of ADDs within variable elimination has proven beneﬁcial only when the
local structure is relatively excessive – this is needed to offset the overhead associated
with ADD operations. One exception here is the use of ADDs and variable elimination
to compile Bayesian networks into arithmetic circuits, a subject that we discuss in the
following section.
Finally, we point out that our treatment has thus far assumed that all of our variables
are binary, precluding the application of ADDs to Bayesian networks with multivalued
variables. For this class of networks, we can employ multi-valued ADDs that are deﬁned
in a similar manner. We can also use binary-valued ADDs except that we must now
represent each multivalued variable in the Bayesian network by a number of binary ADD
variables (see Exercise 13.18).
13.5.4 Compiling arithmetic circuits using ADDs
We showed in Chapter 12 that variable elimination can be used to compile a Bayesian
network into an arithmetic circuit by keeping a trace of the algorithm. We also showed
that the time complexity of circuit compilation using standard variable elimination is
exponential in the width of the used variable order. However, if we use ADDs to represent
factors, the time complexity may not be necessarily exponential in the order width.
The use of ADDs to compile out circuits proceeds as in standard variable elimination
with two exceptions, which are discussed next.
r We have to work with ADDs whose sinks are labeled with arithmetic circuit nodes instead
of numeric constants (see Figure 13.19). This also requires that the operator ⊙passed to
the APPLY operation must now be deﬁned to operate on circuit nodes. For example, to
multiply two ADDs, we deﬁne ⊙as an operator that takes two circuit nodes α and β and
returns a new circuit node labeled with the multiplication operator ∗and having α and β
as its children. To sum two ADDs, we do the same except that the new circuit node will
be labeled with the addition operator +.
r We need to include an indicator ADD for each network variable to capture evidence on
that variable (see Figure 13.18). That is, for each variable X, we need an ADD f (X)
where f (X=F) = λx0 and f (X=T ) = λx1.
Let us now consider an example where we compile an arithmetic circuit for the Bayesian
network X →Y whose CPTs are depicted in Figure 13.18. The ﬁgure also depicts the
initial ADDs we start with for the given Bayesian network: two ADDs for each network
variable, one representing the variable CPT and another representing its indicators.
To construct an arithmetic circuit for this network, all we have to do is eliminate all
network variables. Assuming that we eliminate variable Y ﬁrst, we have to ﬁrst multiply
all ADDs that mention this variable, leading to the ADD in Figure 13.19(a). We can
now sum out variable Y from this ADD, leading to the one in Figure 13.19(b). To
eliminate variable X, we again have to multiply all ADDs that mention this variable,
leading to the ADD in Figure 13.19(c). Summing out variable X from this ADD leads
to the one in Figure 13.19(d). This ﬁnal ADD contains a single sink, which is labeled

13.5 ELIMINATION WITH LOCAL STRUCTURE
335
X
X
x0
.1
x1
.9
X
Y
Y|X
x0
y0
0
x0
y1
1
x1
y0
.5
x1
y1
.5
CPT for X
CPT for Y
X
.1
.9
X
λx0
λx1
X
Y
0
1
.5
Y
λ y0
λ y1
ADD for
ADD for
ADD for
ADD for
CPT of X
indicators of X
CPT of Y
indicators of Y
Figure 13.18: The CPTs and ADDs for a simple Bayesian network X →Y .
X
Y
Y
0
λy1
*
.5
λy0
*
X
+
λ y1
*
.5
λ y0
*
X
*
*
.1
.9
*
*
λx0
λx1
+
λ y1
*
.5
λ y0
*
+
*
*
.1
.9
*
*
λx0
λx1
+
λ y1
*
.5
λ y0
*
(a) f1
(b) 
Y f1
(c) f2
(d) 
X f2
Figure 13.19: Using variable elimination with ADDs to compile an arithmetic circuit.
with an arithmetic circuit corresponding to the given network. Algorithm 39 depicts the
pseudocode for compiling out an arithmetic circuit using variable elimination on factors
that are represented by ADDs.
We conclude this section by pointing out that we can use ADDs in the context of
variable elimination for computing probabilities or for compiling networks. As mentioned
previously, the use of ADDs for computing probabilities will only be beneﬁcial if the
network has sufﬁcient local structure to offset the overhead incurred by ADD operations.
Note, however, that this overhead is incurred only once when compiling networks and is
then amortized over many queries.

336
INFERENCE WITH LOCAL STRUCTURE
Algorithm 39 AC VE ADD(N)
input:
N:
Bayesian network with binary variables
output: arithmetic circuit for Bayesian network N
main:
1:  ←the set of indicator and CPT ADDs for network N
2: π ←an ordering of the n network variables
3: for i from 1 to n do
4:
S ←the ADDs in  that mention variable π(i)
5:
ϕi←
π(i)

ϕ∈S ϕ
6:
 ←( \ S) ∪{ϕi}
7: end for
8: ϕ ←
ϕ′∈ ϕ′
9: return label(ϕ) {ADD ϕ has a single (sink) node}
Bibliographic remarks
The role of determinism in improving the efﬁciency of inference was ﬁrst observed in
Jensen and Andersen [1990], where it was exploited in the context of jointree algo-
rithms. Context-speciﬁc independence was formalized in Boutilier et al. [1996], who also
observed its potential impact on the complexity of inference. The use of structured rep-
resentations in the context of variable elimination was ﬁrst reported in Zhang and Poole
[1996]. ADDs were introduced in Bahar et al. [1993]. Their use was ﬁrst reported in
Hoey et al. [1999] for representing network CPTs and in Chavira and Darwiche [2007]
for compiling Bayesian networks. Other types of structured representations, including
those based on decision trees and graphs, rules, and sparse tabular representations were
reported in Friedman and Goldszmidt [1996], Nielsen et al. [2000], Poole and Zhang
[2003], Dechter and Larkin [2003], and Sanner and McAllester [2005].
The exploitation of local structure in the context of conditioning algorithms was re-
ported in Allen and Darwiche [2003] and Bacchus et al. [2003]. The logical encoding of
Bayesian networks was ﬁrst proposed in Darwiche [2002] and later in Sang et al. [2005].
It was further reﬁned in Chavira and Darwiche [2005; 2006] and applied to relational
Bayesian networks in Chavira et al. [2006]; see Chavira and Darwiche [2008] for a recent
survey. The exploitation of evidence in noisy-or networks was proposed in Heckerman
[1989] and more generally in Chavira et al. [2005] and Sang et al. [2005].
Network preprocessing can also be quite effective in the presence of local structure,
especially determinism, and is sometimes orthogonal to the techniques discussed in this
chapter. For example, preprocessing has proven quite effective and critical for networks
corresponding to genetic linkage analysis, allowing exact inference on networks with
very large treewidth [Fishelson and Geiger, 2002; 2003; Allen and Darwiche, 2008].
A fundamental form of preprocessing is CPT decomposition in which one decomposes
a CPT with local structure into a series of CPTs by introducing auxiliary variables
[Díez and Gal´an, 2003; Vomlel, 2002]. This decomposition can reduce the network
treewidth, allowing inference to be performed much more efﬁciently. The problem of
ﬁnding an optimal CPT decomposition corresponds to the problem of determining tensor

13.6 EXERCISES
337
A
C
B
row
A
B
C
C|AB
1
a1
b1
c1
0
2
a1
b1
c2
.5
3
a1
b1
c3
.5
4
a1
b2
c1
.2
5
a1
b2
c2
.3
6
a1
b2
c3
.5
7
a2
b1
c1
0
8
a2
b1
c2
0
9
a2
b1
c3
1
10
a2
b2
c1
.2
11
a2
b2
c2
.3
12
a2
b2
c3
.5
Figure 13.20: A Bayesian network with one of its CPTs.
rank [Savicky and Vomlel, 2006], which is NP-hard [Hrastad, 1990]. However, closed-
form solutions are known for CPTs with a particular local structure [Savicky and Vomlel,
2006].
13.6 Exercises
13.1. Provide an arithmetic circuit of size O(n) for the polynomial in Equation 13.1.
13.2. Consider Exercise 13.1. Show that a smaller circuit can be constructed for this polynomial
assuming that the circuit can contain subtraction nodes.
13.3. Consider a Bayesian network consisting of binary nodes X1, . . . , Xn, Y , and edges Xi →Y
for i = 1, . . . , n. Assume that Y is true with probability ϵ1 given that any of its parents are true
and that it is false with probability ϵ0 given that all its parents are false. Provide an arithmetic
circuit for this network of size O(n) while making no assumptions about the distributions over
nodes Xi.
13.4. Consider a Bayesian network consisting of binary nodes X1, . . . , Xn , Y , and edges Xi →Y
for i = 1, . . . , n. Assume that Y is true if and only if an odd number of its parents Xi are
true. Provide an arithmetic circuit for this network of size O(n) while making no assumptions
about the distributions over nodes Xi.
13.5. Construct two CNF encodings for the CPT in Figure 13.20, one that ignores local structure
(Section 13.3.1) and another that accounts for local structure (Section 13.3.2).
13.6. Construct an arithmetic circuit for the network in Figure 13.20 while assuming that the
instantiation a2, b1 is always part of the evidence (see Section 13.3.3). Assume that variables
A and B have uniform CPTs.
13.7. Encode the CPT in Figure 13.21 as a CNF while ignoring local structure (Section 13.3.1).
Show how the encoding would change if we account for determinism (Section 13.3.2). Show
how this last encoding would change if we account for equal parameters (Section 13.3.2).
13.8. Construct an ADD for the CPT in Figure 13.21 using the variable order A, B, C, D.
13.9. Consider a factor f over binary variables X1, . . . , Xn, where f (x1, . . . , xn) = 1 if exactly
one value in x1, . . . , xn is true and f (x1, . . . , xn) = 0 otherwise. Construct an ADD repre-
sentation of this factor with size O(n).
13.10. Reduce the ADD in Figure 13.22 (see also Figure 13.14).

338
INFERENCE WITH LOCAL STRUCTURE
A
B
C
D
Pr(D|A, B, C)
true
true
true
true
.9
true
true
true
false
.1
true
true
false
true
.9
true
true
false
false
.1
true
false
true
true
.9
true
false
true
false
.1
true
false
false
true
.2
true
false
false
false
.8
false
true
true
true
.5
false
true
true
false
.5
false
true
false
true
.5
false
true
false
false
.5
false
false
true
true
0
false
false
true
false
1
false
false
false
true
0
false
false
false
false
1
Figure 13.21: A conditional probability table.
X
Y
Y
Z
Z
Z
Z
.1
.9
.5
.1
.9
.5
.1
.9
Figure 13.22: A decision tree representation of a factor (unreduced ADD).
13.11. Show how we can reduce an ADD with n nodes using an algorithm that takes O(n log n)
time (see Figure 13.14).
13.12. Consider the method described in Section 13.5.2 for converting a tabular representation of
a factor into its ADD representation. Consider now a second method in which we construct
a decision tree to represent the factor (see Figure 13.22) and then reduce the ADD using
the algorithm developed in Exercise 13.11. Implement and compare the efﬁciency of these
methods.
13.13. Construct an ADD over variables A and B, which maps each instantiation to 1 if either A or
B is true and to 0 otherwise. Multiply this ADD with the one constructed in Exercise 13.8.
13.14. Sum-out variable B from the ADD constructed in Exercise 13.8.
13.15. Restrict the ADD constructed in Exercise 13.8 to C =false.
13.16. Let f1(X1) and f2(X2) be two factors and let ϕ1 and ϕ2 be their corresponding ADD rep-
resentations using the same variable ordering. Show that the size of the ADD returned by
APPLY(ϕ1, ϕ2, ⊙) is O(exp(|X1 ∪X2|)).
13.17. We presented a O(n2) algorithm for summing out a variable from an ADD where n is the
size of the given ADD. Assuming that the summed-out variable appears last in the ADD
variable order, show how the sum-out operation can be implemented in O(n). Show that this

13.6 EXERCISES
339
complexity applies to summing out multiple variables as long as they appear last in the ADD
order.
13.18. Show how we can represent a factor over multivalued variables using an ADD with binary
variables. Explain how the sum-out operation (of a multivalued variable) should be imple-
mented in this case.
13.19. Consider Bayesian networks of the type given in Figure 13.4 and let e be some evidence on
nodes Si. Let m be the number of nodes Si set to true by evidence e and let k be the number
of network edges. Show that Pr(Dj|e) can be computed in O(k exp(m)). Hint: Prune the
network and appeal to the inclusion-exclusion principle.6
13.20. Consider Exercise 13.19. Show that the same complexity still holds if each node Si is a
noisy-or of its parents.
13.21. Consider the problem of encoding equal parameters, as discussed in Section 13.3.2. Show
that the following technique can be adopted to deal with this problem:
r
Do not drop clauses for parameters with value 1.
r
Replace each set of equal parameters θi in the same CPT by a new parameter η and
drop all PI clauses of parameters θi.
r
Add the following clauses ¬ηi ∨¬ηj for i ̸= j and ¬ηi ∨¬θj for all i, j. Here
η1, . . . , ηk are all the newly introduced parameters to a CPT and θ1, . . . , θm are all
surviving old parameters in the CPT.
In particular, show that the old and new CNF encodings agree on their weighted model
counts.
13.22. Consider the problem of encoding equal parameters as discussed in Section 13.3.2. Consider
the following technique for dealing with this problem:
r
Do not drop clauses for parameters with value 1.
r
Replace each set of equal parameters θi in the same CPT by a new parameter η and
drop all the PI clauses of parameters θi.
Let f be the MLF encoded by resulting CNF . Show that the minimal terms of f correspond
to the network polynomial. Hence, when coupled with the procedure of Exercise 12.20, this
method can be used as an alternative for exploiting equal parameters.
6 This principle says that
Pr(α1 ∨. . . ∨αn) =
n

k=1
(−1)k−1

I⊆{1,...,n},|I|=k
Pr

i∈I
αi

.
Note that there are 2n −1 terms in this equation.

14
Approximate Inference by Belief Propagation
We discuss in this chapter a class of approximate inference algorithms which are based on
belief propagation. These algorithms provide a full spectrum of approximations, allowing
one to trade-off approximation quality with computational resources.
14.1 Introduction
The algorithm of belief propagation was ﬁrst introduced as a specialized algorithm that
applied only to networks having a polytree structure. This algorithm, which we treated
in Section 7.5.4, was later applied to networks with arbitrary structure and found to
produce high-quality approximations in certain cases. This observation triggered a line of
investigations into the semantics of belief propagation, which had the effect of introducing
a generalization of the algorithm that provides a full spectrum of approximations with
belief propagation approximations at one end and exact results at the other.
We discuss belief propagation as applied to polytrees in Section 14.2 and then discuss its
application to more general networks in Section 14.3. The semantics of belief propagation
are exposed in Section 14.4, showing how it can be viewed as searching for an approximate
distribution that satisﬁes some interesting properties. These semantics will then be the
basis for developing generalized belief propagation in Sections 14.5–14.7. An alternative
semantics for belief propagation will also be given in Section 14.8, together with a
corresponding generalization. The difference between the two generalizations of belief
propagation is not only in their semantics but also in the way they allow the user to trade
off the approximation quality with the computational resources needed to produce them.
14.2 The belief propagation algorithm
Belief propagation is a message-passing algorithm originally developed for exact inference
in polytree networks – this is why it is also known as the polytree algorithm.
There are two versions of belief propagation, one computing joint marginals and the
other computing conditional marginals. In particular, for some evidence e the ﬁrst version
computes Pr(X, e) for every variable X in the polytree, while the second version computes
Pr(X|e). The ﬁrst version falls as a special case of the jointree algorithm and is discussed
ﬁrst. The second version is slightly different and is the one we pursue in this chapter.
Consider the polytree network in Figure 14.1 and suppose that our goal is to apply the
jointree algorithm to this network under evidence E =true using the jointree on the left
of Figure 14.2. This jointree is special in the sense that its structure coincides with the
polytree structure. That is, a node i in the jointree, which corresponds to variable X in
the polytree, has its cluster as Ci = XU, where U are the parents of X in the polytree.
Moreover, an undirected edge i−j in the jointree, which corresponds to edge X →Y in
the polytree, has its separator as Sij = X. This immediately implies that the jointree width
340

14.2 THE BELIEF PROPAGATION ALGORITHM
341
B
D
E
F
C
A
A
A
true
.01
false
.99
A
B
B|A
true
true
.100
true
false
.900
false
true
.001
false
false
.999
C
C
true
.001
false
.999
B
C
D
D|BC
true
true
true
.99
true
true
false
.01
true
false
true
.90
true
false
false
.10
false
true
true
.95
false
true
false
.05
false
false
true
.01
false
false
false
.99
D
E
E|D
true
true
.9
true
false
.1
false
true
.3
false
false
.7
D
F
F|D
true
true
.2
true
false
.8
false
true
.1
false
false
.9
Figure 14.1: A polytree network.
A
AB
C
DE
BCD
DF
A
B
C
D
D
A
AB
C
DE
BCD
DF
)
(A
B
π
)
(B
D
π
)
(C
D
π
)
(D
E
λ
)
(D
F
λ
Figure 14.2: Message passing in a jointree corresponding to a polytree.

342
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
equals the treewidth of the given polytree. It also implies that each separator has a single
variable and, hence, each jointree message is over a single variable.
Belief propagation is the jointree algorithm under these conditions, where messages
are notated differently and based on the polytree structure. In particular, the message
from node U to child X is denoted by πX(U) and called the causal support from U to
X. Moreover, the message from node Y to parent X is denoted by λY(X) and called the
diagnostic support from Y to X (see Figure 14.2). Given this notation, the joint marginal
for the family of variable X with parents Ui and children Yj is given by
Pr(XU, e) = λe(X)X|U

i
πX(Ui)

j
λYj (X).
Here λe(X) is an evidence indicator where λe(x) = 1 if x is consistent with evidence e and
zero otherwise. This equation follows immediately from the jointree algorithm once we
adjust for notational differences. Using this new notation, causal and diagnostic messages
can also be deﬁned as
λX(Ui) =

XU\{Ui}
λe(X)X|U

k̸=i
πX(Uk)

j
λYj (X)
πYj (X) =

U
λe(X)X|U

i
πX(Ui)

k̸=j
λYk(X).
A node can send a message to a neighbor only after it has received messages from all
other neighbors. When a node has a single neighbor, it can immediately send a message
to that neighbor. This includes a leaf node X with a single parent U, for which
λX(U) =

X
λe(X)X|U.
It also includes a root node X with a single child Y, for which
πY(X) = λe(X)X.
These are indeed the base cases for belief propagation, showing us the type of messages
that can be computed immediately as they do not depend on the computation of any other
message. Typically, messages are ﬁrst propagated by pulling them toward a particular
node, called a root, and then propagated again by pushing them away from the root. This
particular order of propagating messages, called a pull-push schedule, guarantees that
when a message is about to be computed, all messages it depends on would have already
been computed.
Figure 14.2 depicts the messages propagated toward node D under evidence E =true.
We have three π-messages in this case:
A
πB(A)
true
.01
false
.99
B
πD(B)
true
.00199
false
.99801
C
πD(C)
true
.001
false
.999
We also have two λ-messages:
D
λE(D)
true
.9
false
.3
D
λF(D)
true
1
false
1
To compute the joint marginal for the family of variable D, we simply evaluate
Pr(BCD, e) = D|BC · πD(B)πD(C) · λE(D)λF(D),

14.3 ITERATIVE BELIEF PROPAGATION
343
leading to the following:
B
C
D
Pr(BCD, e)
true
true
true
1.7731 × 10−6
true
true
false
5.9700 × 10−9
true
false
true
1.6103 × 10−3
true
false
false
5.9640 × 10−5
false
true
true
8.5330 × 10−4
false
true
false
1.4970 × 10−5
false
false
true
8.9731 × 10−3
false
false
false
2.9611 × 10−1
where all quantities are rounded to four decimal places. By summing all table entries, we
ﬁnd that Pr(e) ≈.3076. We can also compute the joint marginal for variable C once we
compute the message passed from D to C:
C
λD(C)
true
.8700
false
.3071
C
Pr(C, e)
true
.0009
false
.3067
Note that we could have also computed the joint marginal for variable C using the joint
marginal for the family of D: Pr(C, e) = 
BD Pr(BCD, e).
To compute conditional marginals, we simply normalize joint marginals as in the
jointree algorithm. However, another approach is to use the following alternative equa-
tions for belief propagation, in which BEL(XU) denotes the conditional marginal
Pr(XU|e):
BEL(XU)
=
η λe(X)X|U

i
πX(Ui)

j
λYj (X)
λX(Ui)
=
η

XU\{Ui}
λe(X)X|U

k̸=i
πX(Uk)

j
λYj (X)
πYj (X)
=
η

U
λe(X)X|U

i
πX(Ui)

k̸=j
λYk(X).
Note that the only difference between these equations and the previous ones is in the use
of the constant η, which we use generically as a constant that normalizes a factor to sum
to one (to simplify the notation, we refrain from distinguishing between constants η that
normalize different factors). We will indeed use this version of belief propagation as it
helps prevent numerical underﬂow.1
14.3 Iterative belief propagation
Although belief propagation was designed as an exact algorithm for polytrees, it was
later applied to multiply connected networks. However, this application poses some
procedural and semantic difﬁculties. On the procedural side, recall that a message can
be sent from a node X to its neighbor Y only after X has received messages from
all its other neighbors. Suppose now that we need to apply belief propagation to the
1 As we see in the following section, messages are updated iteratively from previously computed messages. As we
iterate, messages become the product of increasingly many factors with values that become increasingly small,
even though only the relative values of messages are needed to compute conditional marginals.

344
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Algorithm 40 IBP(N, e)
input:
N:
a Bayesian network inducing distribution Pr
e:
an instantiation of some variables in network N
output: approximate marginals, BEL(XU), of Pr(XU|e) for each family XU in N
main:
1: t ←0
2: initialize all messages π0, λ0 (uniformly)
3: while messages have not converged do
4:
t ←t + 1
5:
for each node X with parents U do
6:
for each parent Ui do
7:
λt
X(Ui) = η 
XU\{Ui} λe(X)X|U

k̸=i πt−1
X (Uk) 
j λt−1
Yj (X)
8:
end for
9:
for each child Yj do
10:
πt
Yj (X) = η 
U λe(X)X|U

i πt−1
X (Ui) 
k̸=j λt−1
Yk (X)
11:
end for
12:
end for
13: end while
14: return BEL(XU) = η λe(X)X|U

i πt
X(Ui) 
j λt
Yj (X) for families XU
network in Figure 14.3. We can start off by sending message λE(C) from node E to node
C as the message does not depend on any other. But no other message can be propagated
next, as each is dependent on others that are waiting to be propagated. On the semantic
side, the correctness of belief propagation depends critically on the underlying polytree,
possibly leading to incorrect results if applied to multiply connected networks. As we
see later, these procedural difﬁculties can be addressed and the resulting algorithm, al-
though no longer always correct, can still provide high-quality approximations in many
cases.
Algorithm 40, IBP, depicts the application of belief propagation to multiply connected
networks. The key observation here is that we start off by assuming some initial value to
each message in the network. Given these initial values, every node will now be ready
to send a message to each of its neighbors. Algorithm 40 assumes that message passing
takes place in iterations and is the reason it is referred to as iterative belief propagation
(IBP).2 That is, at iteration t, every node X sends a message to its neighbors using the
messages it received from its other neighbors at iteration t −1. Algorithm 40 contin-
ues iterating until messages converge, which we declare when the value of messages
at the current iteration are within some threshold from their values at the previous it-
eration. When IBP converges, the values of its messages at convergence are called a
ﬁxed point (see Appendix C). In general, IBP may have multiple ﬁxed points on a given
network.
A natural question that we may ask is how fast IBP converges, or if we should even
expect it to converge at all. Indeed, as described we can identify networks where the
2 The algorithm is also commonly known as loopy belief propagation, as it is run in “loopy” networks.

14.3 ITERATIVE BELIEF PROPAGATION
345
B
A
D
C
E
1
2
3
5
6
7
8
9
10
4
Figure 14.3: A Bayesian network, annotated with an ordering of IBP messages.
messages computed by IBP can oscillate, and if left to run without limit IBP could loop
forever.
The convergence rate of IBP can depend crucially on the order in which messages are
propagated, which is known as a message schedule. Algorithm 40 is said to use a parallel
schedule since we wait until all messages for an iteration are computed before they are
propagated (in parallel) in the following iteration. Thus, in a parallel message-passing
schedule, the precise order we compute messages does not affect the dynamics of the
algorithm. On the other hand, we can adopt a sequential schedule where messages are
propagated as soon as they are computed. In this case, we are allowed much ﬂexibility
in when and how quickly information gets propagated across a network. Although one
message-passing schedule may converge and others may not, all schedules in principle
have the same ﬁxed points (if IBP starts at a ﬁxed point, it stays at a ﬁxed point, independent
of the schedule). Thus, for simplicity we assume parallel schedules beyond this section.
For a concrete example of sequential schedules, consider the network in Figure 14.3,
where we compute messages in the following order:
πB(A), πC(A), πD(B), πD(C), πE(C), λD(B), λB(A), λE(C), λD(C), λC(A).
When we are ready to compute the message πE(C) using messages πC(A) and λD(C), we
can use the most up-to-date ones: πC(A) from the current iteration and λD(C) from the
previous iteration. Since we use the most up-to-date message πC(A) to compute πE(C),
information available at node A is able to propagate to E, two steps away, in the same
iteration. Computing messages in parallel, this same information would take two iterations
to reach E.
A sequential schedule could also vary the order in which it computes messages from one
iteration to another and pass only a subset of the messages in each iteration. For example,
for each iteration we could pick a different spanning tree embedded in a network and pass
messages only on the spanning tree using a pull-push schedule. Considering Figure 14.3,
we may use a message order
πB(A), πD(B), λD(C), πE(C), λE(C), πD(C), λD(B), λB(A)
in one iteration and another message order
λE(C), λD(C), λC(A), λB(A), πB(A), πC(A), πD(C), πE(C)
in the next. Such a schedule allows information to propagate from one end of the network
to the other in a single iteration. To propagate the same information, a parallel schedule

346
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
needs as many iterations as the diameter of the polytree. If a message-passing schedule
updates a message in one iteration but not the next, we should ensure that each message
is updated often enough. In particular, if a message is updated in a given iteration, any
other message that uses it must be updated (or at least checked for convergence) in some
future iteration.
Unfortunately, even a careful message-passing schedule may not guarantee conver-
gence. Moreover, even if IBP converges there may be another ﬁxed point that could
provide a better approximation. Although there are techniques that can be used to counter
oscillations, IBP often does not provide a good approximation in such problematic sit-
uations. We instead look to generalizations of belief propagation that can lead to more
accurate approximations as well as more stable dynamics.
14.4 The semantics of IBP
We provide a semantics for Algorithm 40, IBP, in this section, showing how it can
be viewed as searching for an approximate probability distribution Pr′ that attempts to
minimize the Kullback-Leibler divergence with the distribution Pr induced by the given
Bayesian network.
14.4.1 The Kullback-Leibler divergence
Consider the Kullback-Leibler divergence, known as the KL divergence, between two
distributions Pr and Pr′ given that each has been conditioned on evidence e:
KL(Pr′(X|e), Pr(X|e)) =

x
Pr′(x|e) log Pr′(x|e)
Pr(x|e) .
(14.1)
KL(Pr′(X|e), Pr(X|e)) is non-negative and equal to zero if and only if Pr′(X|e) and Pr(X|e)
are equivalent. However, the KL divergence is not a true distance measure in that it is not
symmetric. In general,
KL(Pr′(X|e), Pr(X|e)) ̸= KL(Pr(X|e), Pr′(X|e)).
Moreover, when computing KL(Pr′(X|e), Pr(X|e)) we say that we are weighting the KL
divergence by the approximate distribution Pr′ (as opposed to the true distribution Pr).
We indeed focus on the KL divergence weighted by the approximate distribution as it has
some useful computational properties.
Theorem 14.1. Let Pr(X) be a distribution induced by a Bayesian network N having
families XU. The KL divergence between Pr and another distribution Pr′ can be written
as a sum of three components:
KL(Pr′(X|e), Pr(X|e)) = −ENT′(X|e) −

XU
AVG′(log λe(X)X|U) + log Pr(e),
where
r ENT′(X|e) = −
x Pr′(x|e) log Pr′(x|e) is the entropy of the conditioned approximate
distribution Pr′(X|e).
r AVG′(log λe(X)X|U) = 
xu Pr′(xu|e) log λe(x)θx|u is a set of expectations over the orig-
inal network parameters weighted by the conditioned approximate distribution.
■

14.4 THE SEMANTICS OF IBP
347
A number of observations are in order about this decomposition of the KL divergence.
First, the component ENT′(X|e) depends only on the approximate distribution Pr′ and,
hence, our ability to compute it efﬁciently depends on the speciﬁc form of Pr′. Next,
the component AVG′(log λe(X)X|U) depends on both distributions but can be computed
efﬁciently assuming that we can project the approximate distribution Pr′ on the families
of the original network N. Finally, the component log Pr(e) is effectively a constant as it
is independent of the approximate distribution Pr′.
From these observations, we see that evaluating the ﬁrst two components of the KL
divergence between Pr and Pr′ requires:
r A computation of the entropy for Pr′, and
r A computation of marginals according to Pr′ for families in the original network N.
Moreover, if our goal is to choose a speciﬁc instance Pr′ that minimizes the KL divergence,
then we need to consider only its ﬁrst two components as the third component, log Pr(e),
is independent of our choice of Pr′.
More speciﬁcally, we have now formulated the minimization of the KL divergence in
terms of the following.
Corollary 2. Consider Theorem 14.1. A distribution Pr′(X|e) minimizes the KL divergence
KL(Pr′(X|e), Pr(X|e)) if it maximizes
ENT′(X|e) +

XU
AVG′(log λe(X)X|U).
(14.2)
■
This formulation thus reveals two competing properties of Pr′(X|e) that minimize the KL
divergence:
r Pr′(X|e) should match the original distribution by giving more weight to more likely
parameters λe(x)θx|u (i.e., maximize the expectations).
r Pr′(X|e) should not favor unnecessarily one network instantiation over another by being
evenly distributed (i.e., maximize the entropy).
14.4.2 Optimizing the KL divergence
We can now pose the approximate inference problem as an optimization problem, where
the goal is to search for an approximate distribution Pr′(X|e) that minimizes the KL
divergence with Pr(X|e). In particular, we can assume a parameterized form for the
approximate distribution Pr′(X|e) and try to search for the best instance of that form, that
is, the best set of parameter values. Note that although we desire a solution that minimizes
the KL divergence, depending on how we search for such a solution and what techniques
we use we may only ﬁnd a local extremum or, more generally, a stationary point of the
KL divergence. See Appendix D for a review of concepts in constrained optimization that
we employ here.
Regardless of our particular approach, we should expect to be able to compute the
ENT and AVG components of the KL divergence. This in turn dictates that we choose a
suitably restricted form for Pr′ that facilitates such a computation. As we see next, the
approximations computed by Algorithm 40, IBP, are based on assuming an approximate

348
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
distribution Pr′(X) that factors as
Pr′(X|e) =

XU
Pr′(XU|e)

U∈U Pr′(U|e).
(14.3)
Here XU ranges over the families of network N and U ranges over nodes that appear
as parents in N. Note how this form is in terms of only the marginals that Pr′ assigns to
parents and families. A number of observations are in order about this assumption.
First, this choice of Pr′(X|e) is expressive enough to describe distributions Pr(X|e)
induced by polytree networks N. That is, if N is a polytree, then the corresponding
distribution Pr(X|e) does indeed factor according to (14.3) (see Exercise 14.4). In the case
where N is not a polytree, then we are simply trying to ﬁt Pr(X|e) into an approximation
Pr′(X|e) as if it were generated by a polytree network.
Second, the form in (14.3) allows us to express the entropy of distribution Pr′(X|e) as
ENT′(X|e) = −

XU

xu
Pr′(xu|e) log
Pr′(xu|e)

u∼u Pr′(u|e),
where XU ranges over the network families and U is a parent in U. Again, note how
the entropy is expressed in terms of only marginals over parents, Pr′(u|e), and families,
Pr′(xu|e). If we use µu and µxu to denote these marginals, respectively, then our goal
becomes that of searching for the values of these marginals which will hopefully minimize
the KL divergence, and thus by Corollary 2 maximize its ENT and AVG components.
Suppose now that we have a ﬁxed point of IBP, that is, a set of messages π and
λ that satisfy the convergence conditions of Algorithm 40. The associated marginal
approximations BEL(U) and BEL(XU) are in fact a solution to this optimization
problem.
Theorem 14.2. Let Pr(X) be a distribution induced by a Bayesian network N having
families XU. Then IBP messages are a ﬁxed point if and only if IBP marginals µu =
BEL(u) and µxu = BEL(xu) are a stationary point of
ENT′(X|e) +

XU
AVG′(log λe(X)X|U)
= −

XU

xu
µxu log
µxu

u∼u µu
+

XU

xu
µxu log λe(x)θx|u,
(14.4)
under normalization constraints,

u
µu =

xu
µxu = 1
for each family XU and parent U, and under consistency constraints,

xu∼y
µxu = µy
for each family instantiation xu and value y of family member Y ∈XU.
■
That is, the parent marginals µu = BEL(u) and family marginals µxu = BEL(xu) com-
puted by IBP parameterize an approximate distribution Pr′(X|e) that factorizes as in (14.3).
Moreover, these marginal approximations are stationary points of the KL divergence be-
tween Pr′(X|e) and Pr(X|e) under constraints that ensure that they behave, at least locally,
like true marginal distributions. Our normalization constraints ensure that marginals µu

14.5 GENERALIZED BELIEF PROPAGATION
349
and µxu normalize properly and our consistency constraints ensure that node marginals
µy are consistent with family marginals µxu, for each member Y of a family XU.3
The previous correspondence only tells us that IBP ﬁxed points are stationary points
of the KL divergence: they may only be local minima or they may not be minima at all
(see Appendix D). When IBP performs well, it often has ﬁxed points that are indeed
minima of the KL divergence. For problems where IBP does not behave as well, we
next seek approximations Pr′ whose factorizations are more expressive than that of the
polytree-based factorization of (14.3).
14.5 Generalized belief propagation
Not every probability distribution can be factored as given by (14.3). According to this
equation, the distribution is a quotient of two terms: one is a product of family marginals
and the other is a product of variable marginals. If we do not insist on marginals being
over families and individual variables, we can devise a more general form that can cover
every possible distribution. In particular, any distribution can be expressed as
Pr′(X|e) =

C Pr′(C|e)

S Pr′(S|e) ,
(14.5)
where C and S are sets of variables. If the approximate distribution Pr′ is assumed to take
the form in (14.5), then its entropy can also be expressed as follows.
Theorem 14.3. If a distribution Pr′ has the form
Pr′(X|e) =

C Pr′(C|e)

S Pr′(S|e) ,
then its entropy has the form
ENT′(X|e) =

C
ENT′(C|e) −

S
ENT′(S|e).
■
This immediately means that when the marginals Pr′(C|e) and Pr′(S|e) are readily
available, the ENT component of the KL divergence can be computed efﬁciently. In fact,
if we further assume that each family XU of the original Bayesian network is contained
in some set C or set S, then the AVE component of the KL divergence is also computable
efﬁciently. Hence, given the functional form in (14.5), marginals that minimize the KL
divergence will maximize
ENT′(X|e) +

XU
AVG′(log λe(X)X|U)
= −

C

c
µc log µc +

S

s
µs log µs +

XU

xu
µxu log λe(x)θx|u,
where µc, µs and µxu denote Pr′(c|e), Pr′(s|e), and Pr′(xu|e), respectively.
We saw in Section 9.4.3 that the probability distribution induced by a Bayesian network
N can always be factored as given in (14.5) as long as the sets C correspond to the clusters
of a jointree for N and the sets S correspond to the separators. As we shall see, if we base
3 Note, however, that these normalization and consistency constraints are not sufﬁcient in general to ensure that the
marginals µu and µxu are globally consistent, that is, that they correspond to some distribution.

350
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
our factorization on a jointree and assert some normalization and local consistency con-
straints among cluster and separator marginals, solving the previous optimization problem
yields the same update equations of the jointree algorithm. Moreover, the quantities µc
and µs obtained will correspond to the exact cluster and separator marginals.
A factorization based on a jointree leads to an expensive optimization problem whose
complexity is not less than that of the jointree algorithm itself. Hence, the factorization used
by IBP in (14.3) and the factorization based on jointrees can be viewed as two extremes:
one being quite efﬁcient but possibly quite approximate and one being expensive but
leading to exact results. However, there is a spectrum of other factorizations that fall in
between the two extremes, allowing a trade-off between the quality of approximations
and the efﬁciency of computing them. The notion of a joingraph is one way to obtain such
a spectrum of factorizations and is discussed next.
14.6 Joingraphs
Joingraphs are a generalization of jointrees that can be used to obtain factorizations
according to (14.5). Joingraphs are also used in the following section to formulate a
message-passing algorithm that is analogous to iterative belief propagation and is thus
referred to as iterative joingraph propagation. We can deﬁne joingraphs in a manner similar
to how we deﬁned jointrees.
Deﬁnition 14.1. A joingraph G for network N is a graph where nodes i are labeled
by clusters Ci and edges i−j are labeled by separators Sij. Moreover, G satisﬁes the
following properties:
1. Clusters Ci and separators Sij are sets of nodes from network N.
2. Each family in N must appear in some cluster Ci.
3. If a node X appears in two clusters Ci and Cj, then there exists a path connecting
i and j in the joingraph such that X appears in every cluster and separator on that
path.
4. For every edge i−j in the joingraph, Sij ⊆Ci ∩Cj.
■
We can think of a joingraph as a way of relaxing certain constraints asserted by a
jointree. For example, suppose that two clusters Ci and Cj in a jointree share a set of
variables X. Then every cluster and every separator on the path connecting Ci and Cj must
contain the set X. We relax this constraint in a joingraph and assert only that each variable
X ∈X be contained in the clusters and separators on some path connecting Ci and Cj
(Property 3). Although both properties imply that any variable X must be connected in
the embedded graph induced by X, the jointree imposes a stronger constraint. Similarly,
we do not require separators Sij to be precisely the intersection of clusters Ci and Cj
(Property 4), as is the case for jointrees.
Although a jointree induces an exact factorization of a distribution (see Section 9.4.3),
a joingraph G induces an approximate factorization,
Pr′(X|e) =

i Pr′(Ci|e)

ij Pr′(Sij|e),
(14.6)
which is a product of cluster marginals over a product of separator marginals. When the
joingraph corresponds to a jointree, this factorization will be exact.

14.6 JOINGRAPHS
351
A
C
E
B
D
A
ABC
CDE
B
ABD
A
A
B
B
C
D
Bayesian network
Dual joingraph
Figure 14.4: A Bayesian network and its corresponding dual joingraph.
ABCD
CDE
ABC
ABD
ABC
CD
ABD
ACD
CDE
ABC
ABD
AC
CD
AD
B
Jointree
Joingraph
Figure 14.5: A jointree and a joingraph for the network in Figure 14.4.
If we use a special joingraph called the dual joingraph, the factorization in (14.6)
reduces to that in (14.3), which is used by IBP. A dual joingraph G for network N is
obtained as follows:
r G has the same undirected structure of network N.
r For each family XU in network N, the corresponding node i in joingraph G will have the
cluster Ci = XU.
r For each U →X in network N, the corresponding edge i−j in joingraph G will have the
separator Sij = U.
In Figure 14.4, the dual joingraph is given on the right and induces the factorization
Pr′(X|e) = Pr′(A|e)Pr′(B|e)Pr′(ABC|e)Pr′(ABD|e)Pr′(CDE|e)
Pr′(A|e)2Pr′(B|e)2Pr′(C|e)Pr′(D|e)
,
which is the same factorization used by IBP. Hence, iterative joingraph propagation
(which we discuss in the following section) subsumes iterative belief propagation. The
jointree in Figure 14.5 induces the following factorization, which is exact:
Pr′(X|e) = Pr′(ABC|e)Pr′(ABD|e)Pr′(ABCD|e)Pr′(CDE|e)
Pr′(ABC|e)Pr′(ABD|e)Pr′(CD|e)
.

352
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Figure 14.5 depicts another joingraph for the given network, leading to the following
factorization:
Pr′(X|e) = Pr′(ABC|e)Pr′(ABD|e)Pr′(ACD|e)Pr′(CDE|e)
Pr′(B|e)Pr′(AC|e)Pr′(AD|e)Pr′(CD|e)
.
Since smaller clusters and separators result in computationally simpler problems, join-
graphs allow us a way to trade the quality of the approximation with the complexity of
computing it.
14.7 Iterative joingraph propagation
Suppose that we have a Bayesian network N that induces a distribution Pr and a cor-
responding joingraph that induces a factorization Pr′, as given in (14.6). Suppose fur-
ther that we want to compute cluster marginals µci = Pr′(ci|e) and separator marginals
µsij = Pr′(sij|e) that minimize the KL divergence between Pr′(X|e) and Pr(X|e). This op-
timization problem can be solved using a generalization of IBP called iterative joingraph
propagation (IJGP), which is a message-passing algorithm that operates on a joingraph.
In particular, the algorithm starts by assigning each network CPT X|U and evidence
indicator λe(X) to some cluster Ci that contains family XU. It then propagates messages
using the following equations:
BEL(Ci) =
η i

k
Mki
Mij = η

Ci\Sij
i

k̸=j
Mki,
where i is the product of all CPTs and evidence indicators assigned to cluster Ci, Mij is
the message sent from cluster i to cluster j, η is a normalizing constant, and BEL(Ci) is the
approximation to cluster marginal Pr(Ci|e). We can also use the algorithm to approximate
the separator marginal Pr(Sij|e) as follows:
BEL(Sij) = η MijMji.
IJGP is given in Algorithm 41 and resembles IBP as given in Algorithm 40 except that
we only have one type of messages when operating on a joingraph. Note that we are using
a parallel message-passing schedule in Algorithm 41 but we can use sequential schedules
as in IBP. Note also that if IJGP is applied to the dual joingraph of the given network, it
reduces to IBP. The semantics of IJGP is also a generalization of the semantics for IBP,
as shown next.
Theorem 14.4. Let Pr(X) be a distribution induced by a Bayesian network N having
families XU and let Ci and Sij be the clusters and separators of a joingraph for N. Then
messages Mij are a ﬁxed point of IJGP if and only if IJGP marginals µci = BEL(ci) and
µsij = BEL(sij) are a stationary point of
ENT′(X|e) +

Ci
AVG′(log i)
= −

Ci

ci
µci log µci +

Sij

sij
µsij log µsij +

Ci

ci
µci log i(ci),

14.7 ITERATIVE JOINGRAPH PROPAGATION
353
Algorithm 41 IJGP(G, )
input:
G:
a joingraph
:
factors assigned to clusters of G
output: approximate marginal BEL(Ci) for each node i in the joingraph G
main:
1: t ←0
2: initialize all messages Mt
ij (uniformly)
3: while messages have not converged do
4:
t ←t + 1
5:
for each joingraph edge i−j do
6:
Mt
ij ←η 
Ci\Sij i

k̸=j Mt−1
ki
7:
Mt
ji ←η 
Cj\Sij j

k̸=i Mt−1
kj
8:
end for
9: end while
10: return BEL(Ci) ←η i

k Mt
ki for each node i
under normalization constraints,

ci
µci =

sij
µsij = 1
for each cluster Ci and separator Sij, and under consistency constraints,

ci∼sij
µci = µsij =

cj∼sij
µcj
(14.7)
for each separator Sij and neighboring clusters Ci and Cj.
■
We now have an algorithm that provides a spectrum of approximations. On one end is
IBP, which results from applying IJGP to the dual joingraph of the given network. On the
other end is the jointree algorithm, which results from applying IJGP to a jointree (as a
joingraph). Between these two ends, we have a spectrum of joingraphs and corresponding
factorizations, where IJGP seeks stationary points of the KL divergence between these
factorizations and the original distribution.
One way to see the effect of a joingraph on the approximation quality is by considering
the local consistency constraints given by (14.7) and the extent to which they are sufﬁcient
to ensure global consistency of the corresponding marginals. Consider, for example, the
jointree in Figure 14.5. Since clusters ABC and ABCD are consistent with respect to their
marginals on variables AB and clusters ABCD and ABD are consistent on variables AB,
we can infer that clusters ABC and ABD are consistent on variables AB as well. However,
when we consider the joingraph in Figure 14.5 we see that although clusters ABC and
ABD are consistent on A and also consistent on B, we are not necessarily guaranteed they
are consistent on variables AB. This lack of global consistency highlights the fact that
although IBP and IJGP may be able to provide accurate approximations, the approximate
marginals they produce may not correspond to any real distribution. We explore a more
concrete example of this issue in Exercise 14.1.

354
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
A
U
B
C
X
A
U
B
C
X
S
U
U
ˆ
Sˆ
ˆ
Θ
Uˆ
Θ
—
(a)
Original network N,
original distribution Pr,
original evidence e
(b)
Approximate network N′,
approximate distribution Pr′,
approximate evidence e′
Figure 14.6: Deleting the edge U →X. Network N comes with evidence e = ¯a, b, ¯c. This evidence is
copied to network N′ in addition to the new evidence ˆs, leading to e′ = ¯a, b, ¯c, ˆs.
14.8 Edge-deletion semantics of belief propagation
We present an alternative semantics of belief propagation in this section and use it as a
basis for an alternative formulation of generalized belief propagation.
Consider the network in Figure 14.6(a) and suppose that we delete the edge U →X
as shown in Figure 14.6(b). That is, we ﬁrst remove the edge and then add two new
variables:
r A variable ˆU that has the same values as U and is meant to be a clone of variable U.
r A variable ˆS that has two values and is meant to assert soft evidence on variable U (hence,
ˆS is assumed to be observed).
We later provide some intuition on why we added these variables but the main question
we raise now is as follows: To what extent can the network in Figure 14.6(b) approximate
the one in Figure 14.6(a)? In particular, to what extent can a query on the original network
be approximated by a query on the edge-deleted network? This question is signiﬁcant
because we can use this technique of deleting edges to reduce the treewidth of a given
network to the point where exact inference can be applied efﬁciently. Hence, we can use
this edge-deletion technique to cast the problem of approximate inference as a problem of
exact inference on an approximate network. Interestingly enough, we show in this section
that belief propagation can be understood as a degenerate case of this approximation
paradigm. In particular, we show that belief propagation corresponds to deleting each
and every edge in the Bayesian network. Not only will these semantics provide new
insights into belief propagation but they also provide another formulation of generalized
belief propagation with a speciﬁc method for trading the approximation quality with
computational resources.

14.8 EDGE-DELETION SEMANTICS OF BELIEF PROPAGATION
355
However, before we proceed we will need to settle some notational conventions that
we use consistently in the rest of the chapter. We use N, Pr, and e to represent an original
Bayesian network, its induced probability distribution, and its corresponding evidence,
respectively. We also use N′, Pr′, and e′ to represent the corresponding approximate
network, distribution, and evidence. Our goal here is to approximate queries of the form
Pr(α|e) by queries of the form Pr′(α|e′). The evidence e′ always consists of the original
evidence e and the value ˆs for each variable ˆS added to network N′ during edge deletion
(see Figure 14.6).
14.8.1 Edge parameters
Consider Figure 14.6 again, which depicts a network N and one of its approximations
N′. We cannot use the approximate network N′ before specifying the CPTs for the newly
added variables ˆU and ˆS. These CPTs,  ˆU and  ˆS|U, are called parameters for the deleted
edge U →X, or simply edge parameters.
Interestingly enough, if the deleted edge splits the network into two disconnected
subnetworks, as is the case with Figure 14.6, then we can ﬁnd edge parameters that
guarantee exact node marginals in the approximate network.
Theorem 14.5. Suppose that we delete a single edge U →X that splits the network
into two disconnected subnetworks. Suppose further that the parameters of deleted edge
U →X satisfy the following conditions:
 ˆU = Pr′(U|e′ −ˆS)
(14.8)
ˆs|U = η Pr′(e′| ˆU), for some constant η > 0.
(14.9)
It then follows that Pr(Q|e) = Pr′(Q|e′) for every variable Q and evidence e.
■
According to Theorem 14.5, if we choose the CPTs  ˆU and  ˆS|U carefully, then we
can replace each query Pr(Q|e) on the original network with a query Pr′(Q|e′) on the
approximate network while being guaranteed to obtain the same result. Recall that e′ −ˆS
denotes the evidence that results from retracting the value of variable ˆS from evidence e′.
Note also that checking whether some edge parameters satisfy (14.8) and (14.9) can be
accomplished by performing inference on the approximate network. That is, all we need
to do is plug the edge parameters  ˆU and  ˆS|U into the approximate network N′, compute
the quantities Pr′(U|e′ −ˆS) and Pr′(e′| ˆU) by performing inference on the approximate
network, and ﬁnally check whether (14.8) and (14.9) are satisﬁed.
Consider now Figure 14.6, which leads to the following instance of (14.8) and (14.9):
 ˆU = Pr′(U|¯a, b, ¯c)
(14.10)
ˆs|U = η Pr′(¯a, b, ¯c, ˆs| ˆU).
(14.11)
According to Exercise 14.8, these equations simplify to
 ˆU = Pr′(U|¯a, b)
(14.12)
ˆs|U = η Pr′(¯c| ˆU).
(14.13)
Recall here that the edge U →X splits the network of Figure 14.6(a) into two disconnected
subnetworks. Note also that the evidence ¯a, b is part of one subnetwork (containing U),
while the evidence ¯c is part of the other subnetwork (containing ˆU). Hence, the CPT for
clone ˆU can be viewed as summarizing the impact of evidence ¯a, b on variable U – this

356
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
evidence is now disconnected from the subnetwork containing ˆU. Similarly, the CPT for
variable ˆS is summarizing the impact of evidence ¯c on the clone ˆU – this evidence is
also disconnected from the subnetwork containing variable U. Hence, edge parameters
are playing the role of communication devices between the disconnected subnetworks.
We ﬁnally point out that the edge parameters of Theorem 14.5 come with a stronger
guarantee than the one suggested by the theorem. In particular, not only will node marginals
be exact but also the marginal over any set of variables that appears on the U-side of edge
U →X or on the X-side of that edge (see Exercise 14.14).
14.8.2 Deleting multiple edges
We have thus far considered an idealized case: deleting a single edge that splits the
network into two disconnected subnetworks. We also provided edge parameters that
guarantee exact results when computing node marginals in the edge-deleted network.
We now consider the more general case of deleting multiple edges, where each edge may
or may not split the network. This leaves us with the question of which edge parameters
to use in this case, as the conditions of Theorem 14.5 will no longer hold. As it turns out,
if we use these edge parameters while deleting every edge in the network, we obtain the
same node marginals that are obtained by iterative belief propagation. This result is shown
by the following corollary of a more general theorem that we present later (Theorem 14.6).
Corollary 3. Suppose that we delete all edges of a Bayesian network N and then set
edge parameters according to (14.8) and (14.9). For each deleted edge U →X, the
message values πX(U) =  ˆU and λX(U) = ˆs|U represent a ﬁxed point for iterative
belief propagation on network N. Moreover, under this ﬁxed point, BEL(X) = Pr′(X|e′)
for every variable X.
■
According to this result, iterative belief propagation corresponds to a degenerate case
in which we delete every network edge. Therefore, we can potentially improve on the
approximations of iterative belief propagation by working with an approximate network
that results from deleting fewer edges. We address this topic in Section 14.8.4 but we ﬁrst
show how we can search for edge parameters that satisfy (14.8) and (14.9).
14.8.3 Searching for edge parameters
We now consider an iterative procedure for ﬁnding edge parameters that satisfy (14.8) and
(14.9), assuming that an arbitrary number of edges have been deleted. We then provide a
stronger correspondence between the resulting algorithm and iterative belief propagation.
According to this procedure, we start with an approximate network N′
0 in which all
edge parameters 0
ˆU and 0
ˆs|U are initialized, say, uniformly. Let Pr′
0 be the distribution
induced by this network. For each iteration t > 0, the edge parameters for network N′
t are
determined by performing exact inference on the approximate network N′
t−1 as follows:
t
ˆU = Pr′
t−1(U|e′ −ˆS)
(14.14)
t
ˆs|U = η Pr′
t−1(e′| ˆU).
(14.15)
If the edge parameters for network Nt are the same as those for network Nt−1 (or within
some threshold), we stop and say the edge parameters we found represent a ﬁxed point of
the iterative algorithm. If not, we continue iterating until we reach a ﬁxed point (if any).

14.8 EDGE-DELETION SEMANTICS OF BELIEF PROPAGATION
357
Algorithm 42 ED BP(N, e, )
input:
N:
a Bayesian network
e:
an instantiation of some variables in network N
:
a set of edges in N
output: approximate network N′
main:
1: t ←0
2: N′
0 ←result of deleting edges  in N
3: e′←e plus ˆs for each added node ˆS to N′
4: initialize all edge parameters 0
ˆU and 0
ˆs|U (uniformly)
5: while edge parameters have not converged do
6:
t ←t + 1
7:
for each edge U →X in network N do
8:
t
ˆU ←Pr′
t−1(U|e′ −ˆS)
9:
t
ˆs|U ←η Pr′
t−1(e′| ˆU) {η is typically chosen to normalize t
ˆs|U, that is, η =
1/ 
ˆu Pr′
t−1(e′|ˆu)}
10:
end for
11: end while
12: return N′
t
This iterative procedure, called ED BP, is summarized in Algorithm 42. Figure 14.7
depicts a network that results from deleting an edge, and Figure 14.8 depicts the iterations
of ED BP as it searches for the parameters of this edge. Note that ED BP converges after
four iterations in this case.
Algorithm 42, ED BP, corresponds to Algorithm 40, IBP, in the following sense.
Theorem 14.6. Let N′ be a Bayesian network that results from deleting every edge from
network N. Suppose we run IBP on network N and ED BP on network N′, where initially
all π0
X(U) = 0
ˆU and all λ0
X(U) = 0
ˆs|U. For each edge U →X in network N and each
iteration t, we have
r πt
X(U) = t
ˆU
r λt
X(U) = t
ˆs|U.
Moreover, for all variables X in network N and each iteration t, we have
r BELt(X) = Pr′
t(X|e′)
r BELt(XU) = Pr′
t(X ˆU|e′), where U are the parents of variable X in network N and ˆU are
its parents in network N′.
■
This correspondence tells us that IBP is in fact searching for the edge parameters of a
fully disconnected network. Moreover, the messages computed by IBP contain precisely
the values of edge parameters computed by ED BP.
Interestingly enough, we can obtain a similar correspondence between ED BP and IBP
as long as we delete enough edges to render the network a polytree. In particular, suppose
the network N has n nodes. When deleting edges, all we need is to generate a polytree with

358
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
A
B
D
C
A
B
D
C
Aˆ
Sˆ
(a) Evidence e : D =true
(b) Evidence e′ : D =true, ˆS =true
A
A
true
.8
false
.2
A
B
B|A = B| ˆA
true
true
.8
true
false
.2
false
true
.4
false
false
.6
A
C
C|A
true
true
.5
true
false
.5
false
true
1.0
false
false
.0
B
C
D
D|BC
true
true
true
.1
true
true
false
.9
true
false
true
.3
true
false
false
.7
false
true
true
.9
false
true
false
.1
false
false
true
.8
false
false
false
.2
ˆA
 ˆA
true
.8262
false
.1738
A
ˆS
 ˆS|A
true
true
.3438
true
false
.6562
false
true
.6562
false
false
.3438
Figure 14.7: A network (a) and its approximation (b) that results from deleting edge A →B and then
running Algorithm 42, ED BP. The CPTs  ˆA and  ˆS|A are obtained after the convergence of ED BP (see
Figure 14.8).
t = 0
t = 1
t = 2
t = 3
t = 4
A=true
.5000
.3496
.3440
.3438
.3438
 ˆS=true|A
A=false
.5000
.6504
.6560
.6562
.6562
ˆA=true
.5000
.8142
.8257
.8262
.8262
 ˆA
ˆA=false
.5000
.1858
.1743
.1738
.1738
Figure 14.8: Edge parameters computed by Algorithm 42, ED BP, in the approximate network of Fig-
ure 14.7(b).
no more than n −1 edges. If we keep exactly n −1 original edges, we have a maximal
polytree, which is a spanning tree of nodes in the original network. If we keep zero edges,
we have a fully disconnected network as in Theorem 14.6. Between these extremes, there
is a spectrum of polytrees (see Figure 14.9). Yet if we run ED BP on any of these polytrees,
the family marginals we obtain will correspond to those obtained by IBP. For an example
consider Figure 14.7, which depicts a network N′ that results from deleting a single edge
A →B in network N. Since the approximate network N′ is a polytree, it yields family

14.8 EDGE-DELETION SEMANTICS OF BELIEF PROPAGATION
359
Figure 14.9: Three polytrees (right) that result from deleting edges from the Bayesian network on the far
left. The polytree on the far right is maximal.
(a) Original network
(b) Deleting four edges
(c) Deleting two edges
Figure 14.10: Deleting edges in a Bayesian network.
marginals that correspond to those obtained by running belief propagation on network N
(see also Exercise 14.10).
Intuitively, we expect that a polytree network that results from deleting fewer edges
should in general lead to different (and better) approximations. This intuition is indeed
correct but the difference appears only when considering the marginal for a set of variables
that does not belong to any network family. We ﬁnd a need for computing such marginals in
the following section, leading us to favor maximal polytrees when applying Algorithm 42,
ED BP.
14.8.4 Choosing edges to delete (or recover)
Deleting every edge is obviously the coarsest approximation we can expect. In practic, we
only need to delete enough edges to render the resulting network amenable to exact infer-
ence. Figure 14.10 depicts a network and two potential approximations. The approximate
network in Figure 14.10(b) is a polytree and leads to the same approximations obtained by
iterative belief propagation. The network in Figure 14.10(c) results from deleting fewer
edges and leads to a better approximation.
Deciding which edges to delete in general requires inference, which is not possible on
networks that we are interested in approximating. An alternative approach is to delete too
many edges and then recover some of them. For example, the network in Figure 14.10(c)
can be viewed as the result of recovering two edges in the network of Figure 14.10(b). The
difference here is that deciding which edges to recover can be accomplished by performing

360
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Algorithm 43 ER(N, e)
input:
N:
a Bayesian network
e:
an instantiation of some variables in network N
output: approximate network N′
main:
1:  ←a set of edges whose deletion renders N a maximal polytree
2: N′ ←ED BP(N, e, )
3: while recovery of edges in N′ is amenable to exact inference do
4:
rank deleted edges U →X based on MI(U; ˆU|e′)
5:
 ← \ {top k edges with the largest scores}
6:
N′ ←ED BP(N, e, )
7: end while
8: return N′
inference on an approximate network, which we know is feasible. Our strategy is then be
as follows: We ﬁrst delete enough edges to render the network a maximal polytree. We
next perform inference on the approximate network to rank all deleted edges according
to their impact on improving the approximation quality. We then recover some of the
edges whose recovery will have the best impact on approximation quality. This process
is repeated until we reach a network that becomes inaccessible to exact inference if more
edges are added to it.
The key to this approach is that of assessing the impact a recovered edge will have on
improving approximation quality. Before we proceed on this question, recall that when
deleting an edge U →X that splits the network, Algorithm 42, ED BP, can identify edge
parameters that lead to exact node marginals. But what if deleting the edge U →X does
not fully split the network yet comes close to this?
The intuition here is to rank edges based on the extent to which they split the network.
One method for doing this is by measuring the mutual information between variable U
and its clone ˆU in the approximate network:
MI(U; ˆU|e′) =

uˆu
Pr′(uˆu|e′) log
Pr′(uˆu|e′)
Pr′(u|e′)Pr′(ˆu|e′).
If deleting edge U →X splits the network into two, the mutual information between U
and ˆU is zero. Since edge deletion leads to exact results in this case, there is no point in
recovering the corresponding edge. On the other hand, if MI(U; ˆU|e′) is not zero, then
edge U →X does not split the network. Moreover, the value of MI(U; ˆU|e′) can be
viewed as a measure of the extent to which edge U →X splits the network, leading us to
favor the recovery of edges that have a larger value of MI(U; ˆU|e′).
Algorithm 43, ER, summarizes this proposal for edge recovery. First, we choose an
initial network based on a maximal polytree embedded in the given network N and
parameterize the deleted edges using Algorithm 42, ED BP. Next, we use the resulting
approximation to compute the joint marginals Pr′(uˆu|e′) needed for the mutual informa-
tion scores. Finally, we recover the top k edges using our mutual information heuristic
and run ED BP again on a more connected network N′, resulting in a more structured

14.8 EDGE-DELETION SEMANTICS OF BELIEF PROPAGATION
361
Y1
Y2
Y3
Y4
X1
X3
X4
A
B
C
D
X2
E
1
2
3
4
A
X1
X1|A
true
true
.75
true
false
.25
false
true
.25
false
false
.75
A
Y1
Y1|A
true
true
.9
true
false
.1
false
true
.1
false
false
.9
X1
Y1
B
B|X1Y1
true
true
true
1
true
true
false
0
true
false
true
.1
true
false
false
.9
false
true
true
.2
false
true
false
.8
false
false
true
0
false
false
false
1
Figure 14.11: Variables X1, . . . , X4 have the same CPTs and so do variables Y1, . . . , Y4 and variables
B, C, D and E. Variable A has Pr(A=true) = .8. Evidence e is X2 =false, E =true.
Y1
Y2
Y3
Y4
X1
X3
X4
A
B
C
D
X2
E
1
2
3
4
Figure 14.12: Deleting four edges in the network of Figure 14.11.
approximation. We can repeat this process of ranking deleted edges and recovering them
as long as we can afford to perform exact inference on network N′. Although we can
recover as many edges as possible after the initial ranking of edges, we expect that our
mutual information heuristic will become more accurate as more edges are recovered,
leading to better quality approximations. On the other hand, we should also keep in mind
that the cost of re-ranking edges increases as more edges are recovered. Exercise 14.15
proposes an efﬁcient scheme for ranking deleted edges based on mutual information,
which is important for a practical implementation of Algorithm 43.
Figure 14.13 illustrates an example where we recover edges into the network of Fig-
ure 14.12, one edge at a time, until all edges are recovered. Edge recovery is based on the
mutual information heuristic as shown in Figure 14.14. Note that the edge labeled 2 does
not improve any of the reported approximations. In fact, if we recovered this edge ﬁrst,
we would see no improvement in approximation quality. This is shown in Figures 14.15
and 14.16, where we recover edges with the smallest mutual information ﬁrst. We see
here that the approximations improve more modestly (if at all) than those in Figure 14.13.

362
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Deleted Edges
Node Marginals
{1, 2, 3, 4}
{2, 3, 4}
{2, 3}
{2}
{}
Pr′(A=true|e′)
.7114
.7290
.7300
.7331
.7331
Pr′(B =true|e′)
.3585
.4336
.4358
.4429
.4429
Pr′(C =true|e′)
.2405
.2733
.2775
.2910
.2910
Pr′(D =true|e′)
.5824
.6008
.6100
.5917
.5917
Figure 14.13: Improving node marginals by recovering edges into the network of Figure 14.12. The
marginals in the far right column are exact.
Deleted Edges
Edge Scores
{1, 2, 3, 4}
{2, 3, 4}
{2, 3}
{2}
{}
A →Y1 (1)
1.13 × 10−3
B →Y2 (2)
0
0
0
0
C →X3 (3)
6.12 × 10−4
7.26 × 10−4
6.98 × 10−4
D →X4 (4)
1.12 × 10−3
1.08 × 10−3
Figure 14.14: Scoring deleted edges in the network of Figure 14.12. Scores corresponding to largest
mutual information are shown in bold. Based on these scores, edges are recovered according to the order
1, 4, 3, and then 2.
Deleted Edges
Node Marginals
{1, 2, 3, 4}
{1, 3, 4}
{1, 4}
{4}
{}
Pr′(A=true|e′)
.7114
.7114
.7160
.7324
.7331
Pr′(B =true|e′)
.3585
.3585
.3681
.4412
.4429
Pr′(C =true|e′)
.2405
.2405
.2564
.2879
.2910
Pr′(D =true|e′)
.5824
.5824
.5689
.5848
.5917
Figure 14.15: Recovering edges into the network of Figure 14.12. The marginals in the far right column
are exact.
Deleted Edges
Edge Scores
{1, 2, 3, 4}
{1, 3, 4}
{1, 4}
{4}
{}
A →Y1 (1)
1.13 × 10−3
1.13 × 10−3
9.89 × 10−4
B →Y2 (2)
0
C →X3 (3)
6.12 × 10−4
6.12 × 10−4
D →X4 (4)
1.12 × 10−3
1.12 × 10−3
1.14 × 10−3
1.11 × 10−3
Figure 14.16: Scoring deleted edges in the network of Figure 14.12. Scores corresponding to smallest
mutual information are shown in bold. Based on these scores, edges are recovered according to the order
2, 3, 1 and then 4.
This highlights the impact that a good (or poor) recovery heuristic can have on the quality
of approximations.
14.8.5 Approximating the probability of evidence
In this section, we consider the problem of approximating the probability of evidence,
Pr(e), by performing inference on a network that results from deleting edges. We
may consider approximating this probability using Pr′(e′), the probability of extended
evidence e′ in the approximate network. However, this probability is not well deﬁned

14.8 EDGE-DELETION SEMANTICS OF BELIEF PROPAGATION
363
given (14.9), since this equation does not specify a unique CPT for variable ˆS and there-
fore does not specify a unique value for Pr′(e′).4 We next propose a correction to Pr′(e′)
that addresses this problem. That is, the correction leads to an approximation that is in-
variant to the CPT of node ˆS as long as it satisﬁes (14.9). Moreover, it corresponds to a
well-known approximation that is often computed by iterative belief propagation.
Consider the case where we delete a single edge U →X but where the mutual informa-
tion MI(U; ˆU|e′) is zero in the resulting network. Let us call such an edge a zero-MI edge
(an edge that splits the network is guaranteed to be zero-MI). According to Theorem 14.7,
the approximate probability of evidence can be corrected to the true probability when a
single zero-MI edge is deleted.
Theorem 14.7. Let N′ be a Bayesian network that results from deleting a single edge
U →X in network N. Suppose further that the edge parameters of network N′ satisfy
(14.8) and (14.9) and that MI(U; ˆU|e′) = 0 in network N′. Then
Pr(e) = Pr′(e′) ·
1
zUX
,
where
zUX =

u=ˆu
θˆuθˆs|u.
■
That is, if we delete an edge U →X and ﬁnd out that U and ˆU are independent in the
approximate network N′, we can correct the approximate probability of evidence Pr′(e′)
using zUX and recover the exact probability Pr(e).5
Even though Theorem 14.7 considers only the case of deleting a single zero-MI edge,
it does suggest an approximate correction that applies to multiple edges whether zero-MI
or not. In particular, we can adopt the correction Pr′(e′) · 1
z, where
z =

U →X
zUX =

U →X

u=ˆu
θˆuθˆs|u,
(14.16)
and U →X ranges over all deleted edges. This correction no longer recovers the true
probability of evidence yet it corresponds to a well-known approximation that is typically
computed by iterative belief propagation.
To formally state this connection, consider again Theorem 14.2 and Algorithm 40,
IBP. According to this theorem, a ﬁxed point of iterative belief propagation corresponds
to a stationary point of (14.4):
Fβ = −ENT′(X|e) −

XU
AVG′(log λe(X)X|U).
The quantity exp{−Fβ} (i.e., e−Fβ) is usually taken as an approximation to the probability
of evidence, where Fβ is known as the Bethe free energy.6
Theorem 14.8 shows that the Bethe free energy approximation can be formulated as a
corrected probability of evidence.
4 Note, however, that any CPT that satisﬁes (14.9) leads to the same marginals Pr′(.|e′), since these marginals are
conditioned on evidence e′ (see Exercise 14.9).
5 Since zUX ≤1, this correction can only increase the value of Pr′(e′). Hence, Pr′(e′) is a lower bound on Pr(e) when
U →X is a zero-MI edge.
6 This term comes from the statistical physics literature. As a result, it is typically stated that the ﬁxed points of IBP
are stationary points of the Bethe free energy. The Bethe free energy Fβ is an approximation to the (Helmholtz)
free energy F where exp{−F} = Pr(e) is the exact probability of evidence.

364
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Theorem 14.8. Let N′ be a network that results from deleting every edge in network N.
Suppose further that the edge parameters of network N′ satisfy (14.8) and (14.9). Then
exp{−Fβ} = Pr′(e′) · 1
z ,
where Fβ is the Bethe free energy and z is given by (14.16).
■
Note that the proposal for correcting Pr′(e′) applies to any number of deleted edges.
Yet the Bethe free energy corresponds to the extreme case of deleting every edge. We then
expect to improve on the Bethe approximation by recovering some of the deleted edges.
Note, however, that similar to the case with marginals, we only see improvements after
we have recovered enough edges to go beyond a polytree structure.7
Consider again the example of Figure 14.11, where this time we use evidence e :
X2 =false, E =false. Recovering edges back into the network of Figure 14.12, we ﬁnd
the following approximations to the probability of evidence:
Deleted Edges
{1, 2, 3, 4}
{2, 3, 4}
{2, 3}
{2}
{}
Pr′(e′) · 1
z
.4417
.4090
.3972
.3911
.3911
When no edges are recovered, we have the Bethe approximation exp{−Fβ} ≈.4417. As
we recover edges, the approximation improves until only the edge labeled 2 is deleted. This
is a zero-MI edge since nodes B and ˆB are d-separated, leading to an exact approximation.
Hence, .3911 is the correct probability of evidence in this case and is obtained before we
even recover all edges.
Bibliographic remarks
Belief propagation was originally proposed as an exact inference algorithm for polytree
Bayesian networks [Kim and Pearl, 1983]. Belief propagation is an extension of a similar
algorithm for exact inference in directed trees, where nodes have at most one parent
[Pearl, 1982]. In Pearl [1988], a suggestion was made that belief propagation could still
be applied to networks with loops, as an approximate algorithm, leading to iterative
belief propagation, IBP. Frequently called loopy belief propagation, IBP did not enjoy
the popularity it does today until some decoders for error correction were shown to be
instances of IBP in Bayesian networks with loops; see for example McEliece et al. [1998b]
and Frey and MacKay [1997].
Message-passing schedules of iterative belief propagation are discussed brieﬂy in
Wainwright et al. [2001], Tappen and Freeman [2003], and more extensively in Elidan
et al. [2006]. The connection between ﬁxed points of IBP and stationary points of the
Bethe free energy in Theorem 14.2 was made in Yedidia et al. [2000]. This connection
led to improved algorithms, such as the Kikuchi cluster variational method, and further
to generalized belief propagation (GBP) [Yedidia et al., 2005]. For some background on
the statistical physics that inﬂuenced these discoveries, see, for example, Yedidia [2001]
and Percus et al. [2006]. Iterative joingraph propagation, IJGP, the particular ﬂavor of
GBP that we focused on here, is examined in Aji and McEliece [2001] and Dechter
et al. [2002]. The edge-deletion semantics for IBP were given in Choi and Darwiche
[2006; 2008], along with the correspondence between ED BP and IJGP. Alternative
7 This basically means that the correspondence of Theorem 14.8 continues to hold as long as we delete enough edges
to render the network a polytree.

14.9 EXERCISES
365
characterizations of IBP are also given by tree-based reparameterization [Wainwright et
al., 2001] and expectation propagation, [Minka, 2001]. See also Jordan et al. [1999] and
Jaakkola [2001] for tutorials on the closely related family of variational approximations.
An alternative edge deletion approach is discussed in Suermondt [1992] and van Engelen
[1997], which is explored in Exercise 14.17.
14.9 Exercises
14.1.
Consider the Bayesian network in Figure 14.17 and suppose that we condition on evidence
e: D =true. Suppose that we have run Algorithm 40, IBP, on the network, where it converges
and yields the (partial) set of messages and family marginals given in Figure 14.18.
(a)
Fill in the missing values for IBP messages in Figure 14.18.
(b)
Fill in the missing values for family marginals in Figure 14.18.
(c)
Compute marginals BEL(A) and BEL(B) using the IBP messages in Figure 14.18 and
those computed in (a).
(d)
Compute marginals BEL(A) and BEL(B) by summing out the appropriate variables
from the family marginals BEL(ABC) as well as BEL(ABD).
(e)
Compute joint marginal BEL(AB) by summing out the appropriate variables from family
marginals BEL(ABC) as well as BEL(ABD).
Are the marginals computed in (d) consistent? What about those computed in (e)?
14.2.
Consider the following function,
f (x) = −.21x + .42
−.12x + .54
and the problem of identifying a ﬁxed point x⋆,
x⋆= f (x⋆)
using ﬁxed point iteration. We start with some initial value x0, then at a given iteration t > 0
we update our current value xt using the value xt−1 from the previous iteration:
xt = f (xt−1).
A
C
B
D
A
A
true
.5
false
.5
B
B
true
.25
false
.75
A
B
C
C|AB
true
true
true
.9
true
true
false
.1
true
false
true
.8
true
false
false
.2
false
true
true
.7
false
true
false
.3
false
false
true
.6
false
false
false
.4
A
B
D
D|AB
true
true
true
.6
true
true
false
.4
true
false
true
.4
true
false
false
.6
false
true
true
.3
false
true
false
.7
false
false
true
.3
false
false
false
.7
Figure 14.17: A Bayesian network.

366
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
A
πC(A)
πD(A)
λC(A)
λD(A)
true
.5
.5
.6
false
.5
.5
.4
B
πC(B)
πD(B)
λC(B)
λD(B)
true
.3
.25
.5
false
.7
.75
.5
A
B
C
BEL(ABC)
true
true
true
.162
true
true
false
.018
true
false
true
true
false
false
.084
false
true
true
.084
false
true
false
.036
false
false
true
.168
false
false
false
.112
A
B
D
BEL(ABD)
true
true
true
.2
true
true
false
0
true
false
true
true
false
false
0
false
true
true
.1
false
true
false
0
false
false
true
.3
false
false
false
0
Figure 14.18: (Partial) IBP messages and marginal approximations.
A
A
true
.7
false
.3
ˆA
 ˆA
true
θ
false
1 −θ
A
ˆA
B
B|A ˆA
true
true
true
.3
true
true
false
.7
true
false
true
.6
true
false
false
.4
false
true
true
.7
false
true
false
.3
false
false
true
.4
false
false
false
.6
Figure 14.19: CPTs for a Bayesian network over three variables: A →B and ˆA →B with evidence
e = B =true.
Starting with x0 = .2, we have x1 ≈.7326 and x2 ≈.5887. In this particular example, re-
peated iterations will approach the desired solution x⋆.
(a)
Continue evaluating xt for t ≥3 until the four most signiﬁcant digits stop changing.
Consider the Bayesian network in Figure 14.19 and the problem of identifying a param-
eter θ deﬁning CPT  ˆA, such that
θ = Pr(A=true|e).
(b)
Suppose we set θ = .2. What is Pr(A=true|e)? Suppose we now reset θ to
Pr(A=true|e). What is the new value of Pr(A=true|e)?
(c)
Design a ﬁxed point iterative method to ﬁnd such a θ.
14.3.
Consider the modiﬁed belief propagation algorithm (with normalizing constants). For an edge
U →X, let e+
UX be the evidence instantiating nodes on the U-side of edge U →X and
let e−
UX be the evidence instantiating nodes on the X-side of the edge. Prove the following:
πX(U) = Pr(U|e+
UX)
λX(U) = η Pr(e−
UX|U),
where η is a constant that normalizes message λX(U).

14.9 EXERCISES
367
14.4.
Consider a polytree network N with families XU that induces a distribution Pr(X).
(a)
Prove that the distribution Pr(X|e) factorizes as
Pr(X|e) =

XU
Pr(XU|e)

U∈U Pr(U|e).
(b)
Prove that the factorization of (a) is equivalent to
Pr(X|e) =

XU
Pr(XU|e)
Pr(X|e)nX ,
where nX is the number of children that variable X has in network N.
14.5.
Consider Theorem 14.1. Let N and N′ be two Bayesian networks over the same set of
variables X, possibly having different structures. Prove that
log Pr(e) ≥ENT′(X|e) +

XU
AVG′(log λe(X)X|U).
Thus, if a distribution Pr′(X|e) is induced by a sufﬁciently simple Bayesian network N′, we
can compute a lower bound on the probability of evidence Pr(e).
14.6.
Consider running Algorithm 40, IBP, on a network N that does not have any evidence.
Further, suppose that all IBP messages are initialized uniformly.
(a)
Show that all λ messages are neutral. That is, for any given message λX(U), show that
all values λX(u) are the same.
(b)
Argue that when there is no evidence, IBP need not propagate any λ messages.
(c)
Design a sequential message passing schedule where IBP is guaranteed to converge
in a single iteration.
Hint: It may be useful to analyze how Algorithm 42, ED BP, parameterizes a fully disconnected
network when the original network N has no evidence.
14.7.
Let N and N′ be two Bayesian networks over the same set of variables X, where N′ is fully
disconnected. That is, N′ induces the distribution
Pr′(X) =

X∈X
′
X.
Identify CPTs ′
X for network N′ that minimize KL(Pr, Pr′).
14.8.
Show that Equations 14.10 and 14.11 reduce to Equations 14.12 and 14.13.
14.9.
Let N be a Bayesian network that induces a distribution Pr(X) and let S be a leaf node in
network N that has a single parent U. Show that the conditional distribution Pr(X \ {S}|s) is
the same for any two CPTs S|U and ′
S|U of node S as long as s|U = η · ′
s|U for some
constant η > 0.
14.10. Consider again Figure 14.7, which deﬁnes a network N, and another network N′ that results
from deleting edge A →B. Figure 14.8 depicts the edge parameters for N′ computed using
Algorithm 42, ED BP. Figure 14.20 depicts the messages computed by Algorithm 40, IBP,
in the original network N.
(a)
Identify how edge parameters in N′ correspond to IBP messages in N.
(b)
Consider the following correspondence between the approximations for family marginals
computed by IBP in N and by ED BP in N′:
A
B
BEL(AB) = Pr′( ˆAB|e′)
true
true
.3114
true
false
.4021
false
true
.0328
false
false
.2537
Compute BEL(A) = Pr′( ˆA|e′).

368
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
Edge A →B
A
πB(A)
λB(A)
true
.8262
.3438
false
.1738
.6562
Edge A →C
A
πC(A)
λC(A)
true
.6769
.5431
false
.3231
.4569
Edge B →D
B
πD(B)
λD(B)
true
.7305
.1622
false
.2695
.8378
Edge C →D
C
πD(C)
λD(C)
true
.6615
.4206
false
.3384
.5794
Figure 14.20: Messages computed by Algorithm 40, IBP, after converging on the network of Fig-
ure 14.7(a).
(c)
Given the results of (b), argue that we must have Pr′( ˆA|e′) = Pr′(A|e′).
(d)
Identify a pair of variables in N′ whose joint marginal can be easily computed in N′ but
cannot be computed using IBP in N.
14.11. Let N′ be a network that results from deleting a single edge U →X from the network N
and suppose that this edge splits the network N into two disconnected subnetworks. Show
that Pr(eX|u) = Pr′(eX|ˆu), where eX is evidence that instantiates nodes on the X-side of
edge U →X.
14.12. Consider Equations 14.8 and 14.9:
 ˆU = Pr′(U | e′ −ˆS)
ˆs|U = η Pr′(e′ | ˆU).
(a)
Show that Equations 14.8 and 14.9 are equivalent to
θˆu = ∂Pr′(e′)
∂θˆs|u
θˆs|u = ∂Pr′(e′)
∂θˆu
,
where u = ˆu.
(b)
Show that Equations 14.8 and 14.9 are equivalent to
Pr′(U|e′)
= Pr′( ˆU|e′)
Pr′(U|e′ −ˆS) = Pr′( ˆU).
14.13. Let N be a Bayesian network and N′ be the result of deleting edges in N. Show how an
ED BP ﬁxed point in N′ corresponds to a ﬁxed point of IJGP. In particular, show how to
construct a joingraph that allows IJGP to simulate Algorithm 42.
14.14. Let N′ be a Bayesian network that results from deleting an edge U →X that splits network
N into two disconnected subnetworks: NU containing U and NX containing X. Let XU
be the variables of subnetwork NU and XX be the variables of subnetwork NX. If the
parameters  ˆU and ˆs|U are determined by Equations 14.8 and 14.9, show that the marginal
distributions for each approximate subnetwork are exact:
Pr′(XU|e′) = Pr(XU|e)
Pr′(XX|e′) = Pr(XX|e).
14.15. Let N′ be a polytree that results from deleting edges in network N and suppose that the
parameter edges of N′ satisfy Equations 14.8 and 14.9. Suppose further that we are

14.9 EXERCISES
369
using the polytree algorithm (i.e., belief propagation) to compute the MI scores in Algo-
rithm 43. To compute the MI score for each edge U →X that we delete, we need two types
of values:
r
Node marginals Pr′(u|e′) and Pr′(ˆu|e′)
r
Joint marginals Pr′(uˆu|e′).
If network N has n nodes and m edges and since N′ is a polytree, at most n −1
edges are left undeleted. Thus, we may need to score O(n2) deleted edges in the worst
case. Luckily, to compute every node marginal we only need to run belief propagation
once. However, if we naively computed joint marginals Pr′(uˆu|e′), we could have run be-
lief propagation O(m maxU |U|2) times: for each of our O(m) deleted edges, we could
have run belief propagation once for each of the |U|2 instantiations uˆu. Consider the
following:
Pr′(uˆu|e′) = Pr′(ˆu|u, e′)Pr′(u|e′).
Show that we can compute Pr′(ˆu|u, e′) for all instantiations uˆu using only O(n maxU |U|)
runs of belief propagation, thus showing that we need only the same number of runs of belief
propagation to score all edges in Algorithm 43.
14.16. Let N and N′ be two Bayesian networks that have the same structure but possibly different
CPTs. Prove that
KL(Pr, Pr′) =

XU

u
Pr(u) · KL(X|u, ′
X|u)
=

XU

u
Pr(u) ·

x
θx|u log θx|u
θ′
x|u
,
where X|U are CPTs in network N and ′
X|U are the corresponding CPTs in network N′.
14.17. Suppose we have a network N with a variable X that has parents Y and U. Suppose that
deleting the edge Y →X results in a network N′ where
r N′ has the same structure as N except that edge Y →X is removed.
r
The CPT for variable X in N′ is given by
θ′
x|u
def= Pr(x|u).
r
The CPTs for variables other than X in N′ are the same as those in N.
(a)
Prove that when we delete a single edge Y →X, we have
KL(Pr, Pr′) = MI(X; Y|U) =

xyu
Pr(xyu) log
Pr(xy|u)
Pr(x|u)Pr(y|u).
(b)
Prove that when we delete multiple edges but at most one edge Y →X incoming into
X is deleted, the error is additive:
KL(Pr, Pr′) =

Y →X
MI(X; Y|U).
(c)
Identify a small network conditioned on some evidence e where the KL divergence
KL(Pr(X|e), Pr′(X|e)) can be made arbitrarily large even when divergence KL(Pr, Pr′)
can be made arbitrarily close to zero. Hint: A two-node network A →B sufﬁces.
14.18. Let N′ be a Bayesian network that results from deleting edges U →X from network N.
Show that the edge parameters of network N′ satisfy Equations 14.8 and 14.9 if and only if
the edge parameters are a stationary point of Pr′(e′) · 1
z, where z is given by Equation 14.16.
14.19. Prove Equation 14.27 on Page 376.

370
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
14.20. Consider a Bayesian network with families XU and distribution Pr(X). Show that for evidence
e, we have log Pr(e) = ENT + AVG where
r ENT = −
x Pr(x|e) log Pr(x|e) is the entropy of distribution Pr(X|e).
r AVG = 
XU

xu Pr(xu|e) log λe(x)θx|u is a sum of expectations over network param-
eters.
14.10 Proofs
PROOF OF THEOREM 14.1.
KL(Pr′(X|e), Pr(X|e))
=

x
Pr′(x|e) log Pr′(x|e)
Pr(x|e)
=

x
Pr′(x|e) log Pr′(x|e) −

x
Pr′(x|e) log Pr(x|e)
=

x
Pr′(x|e) log Pr′(x|e) −

x
Pr′(x|e) log Pr(x, e)
Pr(e)
=

x
Pr′(x|e) log Pr′(x|e) −

x
Pr′(x|e) log Pr(x, e) +

x
Pr′(x|e) log Pr(e)
=

x
Pr′(x|e) log Pr′(x|e) −

x
Pr′(x|e) log Pr(x, e) + log Pr(e).
We now have
−

x
Pr′(x|e) log Pr′(x|e) = ENT′(X|e).
Moreover,

x
Pr′(x|e) log Pr(x, e) =

x
Pr′(x|e) log

xu∼x
λe(x)θx|u
=

x

xu∼x
Pr′(x|e) log λe(x)θx|u
=

XU

xu

x∼xu
Pr′(x|e) log λe(x)θx|u
=

XU

xu
log λe(x)θx|u

x∼xu
Pr′(x|e)
=

XU

xu
Pr′(xu|e) log λe(x)θx|u
=

XU
AVG′(log λe(X)X|U).
■
Hence,
KL(Pr′(X|e), Pr(X|e)) = −ENT′(X|e) −

XU
AVG′(log λe(X)X|U) + log Pr(e).
PROOF OF THEOREM 14.2. The proof of Theorem 14.2 is identical to the proof of
Theorem 14.4 in the case where the joingraph is the dual joingraph of N.
■

14.10 PROOFS
371
PROOF OF THEOREM 14.3.
ENT(X|e) = −

x
Pr(x|e) log Pr(x|e)
= −

x
Pr(x|e) log

c∼x Pr(c|e)

s∼x Pr(s|e)
= −

x
Pr(x|e) log

c∼x
Pr(c|e) +

x
Pr(x|e) log

s∼x
Pr(s|e)
= −

x

c∼x
Pr(x|e) log Pr(c|e) +

x

s∼x
Pr(x|e) log Pr(s|e)
= −

C

c

x∼c
Pr(x|e) log Pr(c|e) +

S

s

x∼s
Pr(x|e) log Pr(s|e)
= −

C

c
Pr(c|e) log Pr(c|e) +

S

s
Pr(s|e) log Pr(s|e)
=

C
ENT(C|e) −

S
ENT(S|e).
■
PROOF OF THEOREM 14.4. We want to show that a ﬁxed point for IJGP is a stationary
point of the KL divergence or, equivalently, a stationary point of
f = ENT′(X|e) +

Ci
AVG′(log i)
= −

Ci

ci
µci log µci +

Sij

sij
µsij log µsij +

Ci

ci
µci log i(ci)
under the normalization constraints,

ci
µci =

sij
µsij = 1
(14.17)
for all clusters Ci and separators Sij, and under the consistency constraints,

ci∼sij
µci = µsij =

cj∼sij
µcj
(14.18)
for each separator Sij and neighboring clusters Ci and Cj.
First, we construct the Lagrangian L from our objective function f (see Appendix D):
L = f +

Ci
γi

ci
µci −1

+

Sij
γij
⎛
⎝
sij
µsij −1
⎞
⎠
+

Sij

sij
κij(sij)
⎛
⎝
cj∼sij
µcj −µsij
⎞
⎠+ κji(sij)
⎛
⎝
ci∼sij
µci −µsij
⎞
⎠,
where Lagrangian multipliers γi and γij enforce the normalization constraints and the
multipliers κij(sij), κji(sij) enforce the consistency constraints.
Setting to zero the partial derivatives of L with respect to our parameters µ, γ, and κ,
we want to solve for these same parameters. The partial derivatives of L with respect to
Lagrange multipliers γi and γij yield

ci
µci −1 = 0,

sij
µsij −1 = 0,

372
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
giving us back the normalization constraints in (14.17). The partial derivatives of L with
respect to Lagrange multipliers κij(sij) and κji(sij) yield

cj∼sij
µcj −µsij = 0,

ci∼sij
µci −µsij = 0,
giving us back our consistency constraints in (14.18).
When we take the partial derivatives of L with respect to cluster marginals µci and
separator marginals µsi, we have, respectively,
−1 −log µci + log i(ci) + γi +

k
κki(sik) = 0
1 + log µsij + γij −κji(sij) −κij(sji) = 0,
where k are neighbors of i. Solving for µci and µsi, we get
µci = exp{−1} · exp{γi} · i(ci) ·

k
exp{κki(sik)}
(14.19)
µsij = exp{−1} · exp{−γij} · exp{κji(sij)} · exp{κij(sij)}
(14.20)
where exp{x} (equivalently ex) is the exponential function. We now have expressions for
the cluster and separator marginals at a stationary point of the KL divergence.
Applying our normalization constraints (14.17) on (14.19), we get

ci
µci = exp{−1} · exp{γi} ·

ci
i(ci) ·

k
exp{κki(sik)} = 1
and thus
exp{γi} = exp{1} ·
&
ci
i(ci) ·

k
exp{κki(sik)}
'−1
.
Substituting back into (14.19), we see that the role of exp{γi} is as a normalizing constant;
similarly, for exp{−γij} in (14.20).
We now establish a correspondence between these expressions for marginals and the
ones computed by IJGP. In particular, if we substitute Mki(sik) = exp{κki(sik)}, (14.19)
and (14.20) simplify to
µci = η i(ci)

k
Mki(sik)
(14.21)
µsij = η Mji(sij)Mij(sij).
(14.22)
Applying the consistency constraint given by (14.18) to (14.21) and (14.22), we get

ci∼sij
i(ci)

k
Mki(sik) ∝Mji(sij)Mij(sij),
and thus
Mij(sij) = η

ci∼sij
i(ci)

k̸=j
Mki(sik).
We have therefore derived a correspondence between IJGP marginals and the stationary
points of the KL divergence.
■
PROOF OF THEOREM 14.5. Let ˆS = ˆs be the soft evidence introduced when deleting
the edge U →X. Let eU, ˆs denote the evidence in the subnetwork of N′ containing

14.10 PROOFS
373
variable U and let eX denote the evidence in the subnetwork containing variable X.
Hence, e = eU, eX and e′ = ˆs, eU, eX.
Before showing Pr′(q|e′) = Pr(q|e), we show the more speciﬁc result Pr′(u|e′) =
Pr(u|e). This is shown while observing that Pr(u, eU) = Pr′(u, eU) and Pr(eX|u) =
Pr′(eX|ˆu). The ﬁrst equality follows immediately once we prune the networks N and
N′ for the corresponding queries (see Section 6.9). The second equality is the subject of
Exercise 14.11. We now have
Pr(u|e) ∝Pr(e|u)Pr(u)
= Pr(eU|u)Pr(eX|u)Pr(u)
EU and EX are d-separated by U
= Pr(u, eU)Pr(eX|u)
= Pr′(u, eU)Pr′(eX|ˆu)
see previous equalities
= Pr′(u, eU)Pr′(eX|ˆu)Pr′(eU, ˆs|ˆu)
Pr′(eU, ˆs|ˆu)
= Pr′(u, eU) Pr′(e′|ˆu)
Pr′(eU, ˆs|ˆu)
EX and ˆS, EU d-separated by ˆU
= Pr′(u, eU) Pr′(e′|ˆu)
Pr′(eU, ˆs)
ˆS, EU and ˆU d-separated
∝Pr′(u, eU)
θˆs|u
Pr′(eU, ˆs)
by (14.9)
= Pr′(u, eU, ˆs)
Pr′(eU, ˆs)
θˆs|u = Pr′(ˆs|u, eU)
= Pr′(u|eU, ˆs.)
= Pr′(u|e′)
U and EX d-separated by EU, ˆS.
(14.23)
To show that Pr′(q|e′) = Pr(q|e) when Q is in the subnetwork of N′ containing U, we
have
Pr(q|e) =

u
Pr(q|u, e)Pr(u|e)
=

u
Pr(q|u, eU)Pr(u|e)
Q and EX d-separated by U, EU
=

u
Pr′(q|u, eU)Pr(u|e)
equivalent after pruning N and N′
=

u
Pr′(q|u, eU)Pr′(u|e′)
by (14.23)
=

u
Pr′(q|u, e′)Pr′(u|e′)
Q and EX, ˆS d-separated by U, EU
= Pr′(q|e′).
We can similarly show the equality when Q is in the subnetwork of N′ containing X once
we observe that Pr(u|e) = Pr′(ˆu|e′) (see Lemma 14.1).
■
PROOF OF THEOREM 14.6. We show the correspondence between ED BP and IBP by
induction. Let a variable X in network N have parents Ui and children Yj. Observe
that X is the center of a star network in N′ whose arms are auxiliary variables ˆUi and
ˆSj introduced by deleting edges Ui →X and X →Yj, respectively (see Figure 14.21,

374
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
X
Ui
Sj
Skˆ
ˆ
ˆ
X
πt
X(Ui)
λt
Yj(X )
λt
Yk(X)
Figure 14.21: Correspondence between ED BP parameter updates in a fully disconnected network and
message passing in IBP.
left). For an iteration t, let t
ˆUi parameterize clone variable ˆUi and let t
ˆsj|X parameterize
variable ˆSj. Then at iteration t = 0, we are given that π0
X(Ui) = 0
ˆUi for all edges Ui →X
and λ0
Yj (X) = 0
ˆsj|X for all edges X →Yj.
We ﬁrst want to show for an iteration t > 0 and for an edge X →Yj, the IBP message
that variable X passes to its child Yj is the same as the parameters for the clone ˆX that
was made a parent of Yj. That is, we want to show πt
Yj (X) = t
ˆX.
Assume that e′
X is the evidence in the star network of X in N′. Starting from (14.14),
we have
t
ˆX = Pr′
t−1(X|e′ −ˆSj) = Pr′
t−1(X|e′
X −ˆSj),
since X is independent of all evidence other than the evidence e′
X that is directly connected
to X. Letting ˆU denote the set of clones that became parents of X in N′, we have
t
ˆX = η Pr′
t−1(X, e′
X −ˆSj) = η

ˆU
Pr′
t−1(X ˆU, e′
X −ˆSj)
where η = [Pr′(e′
X −ˆSj)]−1. We can now factorize into the subnetwork parameters of
the star centered at X:
t
ˆX = η

ˆU
λe(X)X| ˆU

i
t−1
ˆUi

k̸=j
t−1
ˆsk|X.
Finally, by our inductive hypothesis and by relabeling ˆU to U, we have
t
ˆX = η

U
λe(X)X|U

i
πt−1
X (Ui)

k̸=j
λt−1
Yk (X) = πt
Yj (X).
We can similarly show that λt
Yj (X) = t
ˆsj|X, that Prt(X|e) = Pr′
t(X|e′), and that
Prt(XU|e) = Pr′
t(X ˆU|e′).
■
Lemma 14.1. Let N′ be a Bayesian network that results from deleting a single edge
U →X in network N. Suppose further that the edge parameters of network N′ satisfy
(14.8) and (14.9). Then
Pr′(u|e′) = Pr′(ˆu|e′) =
1
zUX
· θˆuθˆs|u
for states u = ˆu, where
zUX =

u=ˆu
θˆuθˆs|u.
PROOF. Noting (14.9), we have
θˆuθˆs|u = θˆu · η Pr′(e′|ˆu) = Pr′(ˆu, e′) · η = Pr′(ˆu|e′) · η Pr′(e′).
(14.24)

14.10 PROOFS
375
Using (14.8), we have
θˆuθˆs|u = Pr′(u|e′ −ˆS)θˆs|u
= Pr′(u, e′ −ˆS)θˆs|u
Pr′(e′ −ˆS)
= Pr′(u, e′ −ˆS)Pr′(ˆs|u, e′ −ˆS)
Pr′(e′ −ˆS)
=
Pr′(u, e′)
Pr′(e′ −ˆS)
= Pr′(u|e′) ·
Pr′(e′)
Pr′(e′ −ˆS)
(14.25)
Summing (14.24) and (14.25) over states u = ˆu (as in zUX) and equating, we ﬁnd that
η = [Pr′(e′ −ˆS)]−1 and also zUX = η Pr′(e′). Making this substitution in (14.24) and
(14.25), we ﬁnd that
θˆuθˆs|u = Pr′(ˆu|e′) · zUX = Pr′(u|e′) · zUX,
(14.26)
which yields our lemma.
■
PROOF OF THEOREM 14.7. Suppose that we replace the edge U →X in network N
with a chain U→ˆU→X where the edge U →ˆU denotes an equivalence constraint:
θˆu|u = 1 iff ˆu = u. The resulting augmented network is equivalent to the original network
over the original variables and, in particular, it yields the same probability of evidence.
We thus observe ﬁrst
Pr(e) =

uˆu
Pr(uˆu, e) =

u=ˆu
Pr(uˆu, e).
Noting (12.5) and (12.6) and that θˆu|u = 1 when u = ˆu, we have
Pr(e) =

u=ˆu
∂Pr(uˆu, e)
∂θˆu|u
θˆu|u =

u=ˆu
∂Pr(uˆu, e)
∂θˆu|u
=

u=ˆu
∂Pr′(uˆu, e)
∂θˆu
.
The last equality follows since Pr(u, ˆu, e) and Pr′(u, ˆu, e) differ only in the parameters
θˆu|u and θˆu. Noting (12.5) and (12.6) again, and going further, we have
Pr(e) =

u=ˆu
Pr′(uˆu, e)
θˆu
=

u=ˆu
Pr′(uˆu, e)θˆs|u
θˆuθˆs|u
=

u=ˆu
Pr′(uˆu, e)Pr′(ˆs|uˆu, e)
θˆuθˆs|u
=

u=ˆu
Pr′(uˆu, e′)
θˆuθˆs|u
.
Using (14.26) from the proof of Lemma 14.1, we have
Pr(e) =

u=ˆu
Pr′(uˆu, e′)
Pr′(ˆu|e′) · zUX
= Pr′(e′) ·
1
zUX
·

u=ˆu
Pr′(u|ˆu, e′).
Since we assumed that MI(U; ˆU|e′) = 0, we have Pr(e) = Pr′(e′) ·
1
ZUX .
■
PROOF OF THEOREM 14.8. This proof is based on a number of observations. The ﬁrst
observation uses Theorem 14.6 to express the Bethe free energy Fβ, given by (14.4), in

376
APPROXIMATE INFERENCE BY BELIEF PROPAGATION
terms of quantities in the approximate network N′:
Fβ =

X ˆU

x ˆu
Pr′(x ˆu|e′) log
Pr′(x ˆu|e′)

ˆu∼ˆu Pr′(ˆu|e′)
−

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu,
where X ranges over variables in the original network N and ˆU are the (cloned) parents
of variable X in network N′ (see Figure 14.21).
The second observation is based on Exercise 14.20, which shows that log Pr′(e′) can be
expressed as a sum of two terms ENT + AVG, where ENT is the entropy of distribution
Pr′(.|e′). Since network N′ is fully disconnected, this entropy decomposes as follows (see
Exercise 14.19):
ENT = −

X ˆU

x ˆu
Pr′(x ˆu|e′) log Pr′(x ˆu|e′).
(14.27)
The third observation underlying this proof concerns the second term AVG in log Pr′(e′) =
ENT + AVG. From the deﬁnition given in Exercise 14.20, we have
AVG =

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu
+

U →X

ˆu
Pr′(ˆu|e′) log λe′(ˆu)θˆu +

U →X

u
Pr′(u|e′) log λe′(ˆs)θˆs|u.
Note that each term in this sum is ranging over different types of network parameters in
the approximate network N′: parameters θx|ˆu from the original network N and parameters
θˆu and θˆs|u introduced by deleting edges U →X in network N′. Note also that λe′(ˆu) = 1
for all ˆu as we have no evidence on the clone ˆU. Moreover, λe′(ˆs) = 1 since ˆS is observed
to ˆs. We therefore drop these evidence indicators for simplicity.
Since Pr′(ˆu|e′) = Pr′(u|e′) by Lemma 14.1, we now have
AVG =

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu +

U →X

u=ˆu
Pr′(ˆu|e′) log θˆuθˆs|u.
By Lemma 14.1, we also have θˆuθˆs|u = zUXPr′(ˆu|e′); thus,

U →X

u=ˆu
Pr′(ˆu|e′) log θˆuθˆs|u =

U →X

ˆu
Pr′(ˆu|e′) log zUXPr′(ˆu|e′)
=

U →X
log zUX +

U →X

ˆu
Pr′(ˆu|e′) log Pr′(ˆu|e′).
Substituting this expression into AVG, we get
AVG =

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu
+

U →X
log zUX +

U →X

ˆu
Pr′(ˆu|e′) log Pr′(ˆu|e′).

14.10 PROOFS
377
Adding this to the entropy term in (14.27), we get
log Pr′(e′) = ENT + AVG
= −

X ˆU

x ˆu
Pr′(x ˆu|e′) log Pr′(x ˆu|e′) +

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu
+

U →X
log zUX +

U →X

ˆu
Pr′(ˆu|e′) log Pr′(ˆu|e′).
Since

U →X

ˆu
Pr′(ˆu|e′) log Pr′(ˆu|e′)
=

U →X

ˆu

x ˆu
ˆu∼ˆu
Pr′(x ˆu|e′) log Pr′(ˆu|e′)
=

X ˆU

x ˆu

ˆu∼ˆu
Pr′(x ˆu|e′) log Pr′(ˆu|e′)
=

X ˆU

x ˆu
Pr′(x ˆu|e′) log

ˆu∼ˆu
Pr′(ˆu|e′),
we have
log Pr′(e′) = ENT + AVG
= −

X ˆU

x ˆu
Pr′(x ˆu|e′) log
Pr′(x ˆu|e′)

ˆu∼ˆu Pr′(ˆu|e′) +

X ˆU

x ˆu
Pr′(x ˆu|e′) log λe(x)θx|ˆu
+

U →X
log zUX
= −Fβ +

U →X
log zUX
= −Fβ + log z.
Hence, −Fβ = log Pr′(e′) −log z and exp{−Fβ} = Pr′(e′) · 1
z as desired.
■

15
Approximate Inference by Stochastic Sampling
We discuss in this chapter a class of approximate inference algorithms based on stochas-
tic sampling: a process by which we repeatedly simulate situations according to their
probability and then estimate the probabilities of events based on the frequency of their
occurrence in the simulated situations.
15.1 Introduction
Consider the Bayesian network in Figure 15.1 and suppose that our goal is to estimate the
probability of some event, say, wet grass. Stochastic sampling is a method for estimating
such probabilities that works by measuring the frequency at which events materialize
in a sequence of situations simulated according to their probability of occurrence. For
example, if we simulate 100 situations and ﬁnd out that the grass is wet in 30 of them, we
estimate the probability of wet grass to be 3/10. As we see later, we can efﬁciently simulate
situations according to their probability of occurrence by operating on the corresponding
Bayesian network, a process that provides the basis for many of the sampling algorithms
we consider in this chapter.
The statements of sampling algorithms are remarkably simple compared to the meth-
ods for exact inference discussed in previous chapters, and their accuracy can be made
arbitrarily high by increasing the number of sampled situations. However, the design of
appropriate sampling methods may not be trivial as we may need to focus the sampling
process on a set of situations that are of particular interest. For example, if the goal is to
estimate the probability of wet grass given a slippery road, we may need to only simulate
situations in which the road is slippery to increase the rate at which our estimates con-
verge to the true probabilities. Another potential complication with sampling algorithms
concerns the guarantees we can offer on the quality of estimates they produce, which may
require sophisticated mathematical analysis in certain cases.
We begin in the next section by presenting the simplest sampling method for computing
(unconditional) probabilities but without providing any analysis of its properties. We then
go over some estimation background in Section 15.3 that allows us to provide a formal
treatment of this sampling method and some of its variants in Section 15.4. We then
discuss the problem of estimating conditional probabilities in Section 15.5, leading to two
key algorithms for this purpose that are discussed in Sections 15.6 and 15.7.
15.2 Simulating a Bayesian network
Suppose that we have a Bayesian network that induces a probability distribution Pr(X).
To draw a random sample of size n from this distribution is to independently generate a
sequence of instantiations x1, . . . , xn where the probability of generating instantiation xi
is Pr(xi). We will show in this section how we can efﬁciently draw a random sample from
378

15.2 SIMULATING A BAYESIAN NETWORK
379
Winter?
(A)
Sprinkler?
(B)
Wet Grass?
(D)
Rain?
(C)
Slippery Road?
(E)
A
A
true
.6
false
.4
A
B
B|A
true
true
.2
true
false
.8
false
true
.75
false
false
.25
A
C
C|A
true
true
.8
true
false
.2
false
true
.1
false
false
.9
B
C
D
D|BC
true
true
true
.95
true
true
false
.05
true
false
true
.9
true
false
false
.1
false
true
true
.8
false
true
false
.2
false
false
true
0
false
false
false
1
C
E
E|C
true
true
.7
true
false
.3
false
true
0
false
false
1
Figure 15.1: A Bayesian network.
a probability distribution speciﬁed by a Bayesian network, a process known as simulating
the Bayesian network.
As mentioned in the introduction, if we have an ability to draw random samples, then we
can estimate the probability of event α by simply considering the fraction of instantiations
at which α is true. For example, if we sample 100 instantiations and α happens to be true
in 30 of them, then our estimate will be 30/100 = .3. This very simple estimation process
is analyzed formally in Section 15.4. However, in this section we simply focus on the
process of drawing random samples from a distribution induced by a Bayesian network.
Consider the Bayesian network in Figure 15.1, which induces the distribution
Pr(A, B, C, D, E). The basic idea for drawing a random instantiation from a Bayesian
network is to traverse the network in topological order, visiting parents before children,
and generate a value for each visited node according to the probability distribution of that
node. In Figure 15.1, this procedure dictates that we start with variable A while sampling
a value from its distribution Pr(A).1 Suppose, for example, that we end up sampling the
value a. We can now visit either variable B or C next. Suppose we visit B. We then have
to sample a value for B from the distribution Pr(B|a) since a is the chosen value for
variable A. Suppose that this value turns out to be ¯b and that we also sample the value
1 Since Pr(a) = .6, this can be done by ﬁrst choosing a random number r in the interval [0, 1) and then selecting
the value a if r < .6 and the value ¯a otherwise. If a variable X has multiple values x1, . . . , xm, we can generate a
sample from Pr(X) as follows. We ﬁrst generate a random number p in the interval [0, 1) and then choose the value
xk if k−1
i=1 Pr(xi) ≤p < k
i=1 Pr(xi).

380
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Table 15.1: A sample of size n = 10 generated from the
network in Figure 15.1, where X = {A, B, C, D, E}.
A
B
C
E
D
x1
true
true
true
true
true
x2
true
false
true
true
false
x3
false
false
false
false
false
x4
false
true
false
false
true
x5
true
true
true
false
true
x6
true
true
true
true
true
x7
false
true
false
false
true
x8
false
false
true
true
true
x9
false
false
false
false
false
x10
true
false
false
false
false
Algorithm 44 SIMULATE BN(N)
input:
N:
A Bayesian network inducing distribution Pr(X)
output: a network instantiation x sampled from the distribution Pr(X)
main:
1: π←a total order of network variables in which parents appear before their children
2: ←⊤{trivial variable instantiation}
3: n←number of network variables
4: for i from 1 to n do
5:
X←variable at position i in order π
6:
u←value of X’s parents in instantiation 
7:
x←value of variable X sampled from the distribution Pr(X|u)
8:
←, x {appending value x to instantiation }
9: end for
10: return 
c for variable C using a similar procedure. We can then sample a value for D from the
distribution Pr(D|¯b, c) and, ﬁnally, sample a value for variable E from Pr(E|c).
Algorithm 44, SIMULATE BN, describes this procedure, which runs in time linear in
the network size. Table 15.1 depicts a random sample of size n = 10 generated by this
algorithm from the network in Figure 15.1. According to this sample, the probability of
A= ¯a is 5/10, and the probability of D =d is 6/10.
Theorem 15.1. Algorithm 44, SIMULATE BN, generates an instantiation x with prob-
ability Pr(x) where Pr(X) is the distribution induced by the given Bayesian net-
work.
■
Note also that the instantiations generated by Algorithm SIMULATE BN are indepen-
dent of each other. Hence, by repeated application of this algorithm we can generate a
random sample of any size.
We next review some classical results from estimation theory, which we use in Sec-
tion 15.4 to provide a more formal treatment of the sampling algorithm discussed in this
section. These results will also provide the formal foundation for many of the algorithms
discussed later in the chapter.

15.3 EXPECTATIONS
381
Pr(X)
f (X)
g(X)
x1
.125
1
3
x2
.125
1
4
x3
.125
2
4
x4
.125
4
5
x5
.125
4
5
x6
.125
6
4
x7
.125
7
4
x8
.125
7
3
Figure 15.2: Two functions with the same expected value, 4, but different variances. The variance of f
is 5.5 and the variance of g is .5.
15.3 Expectations
Consider a function f (X) that takes its values depending on which instantiation x materi-
alizes. Since instantiations materialize according to some probability distribution Pr(X),
the values of function f (X) also materialize according to some distribution.2 We can
deﬁne the expected value of function f (X) as:
Ex(f )
def=

x
f (x) · Pr(x),
(15.1)
which is sometimes denoted by µ and called the expectation or mean. For an example,
the expected value of function f (X) in Figure 15.2 is 4.
All of our sampling algorithms are based on a central technique in which we formulate
the probability of an event as an expected value of some function f (X). Approximate infer-
ence then becomes a matter of estimating expectations, allowing us to provide guarantees
on the quality of our approximations by capitalizing on results from the vast literature on
estimating expectations. Our goal in this section is then to introduce this concept of an
expectation and provide some of the fundamental techniques for estimating expectations
and guaranteeing the quality of these estimates.
Continuing with the example in Figure 15.2, note that the function g has the same
expected value as function f , that is, 4. Yet it should be clear that the values of function
g are more concentrated around this expectation than the values of function f . This
distinction is important when trying to estimate the expected value by observing some
values of the function. For example, an observed value of function g is more likely to be
close to the expectation 4 than an observed value of f . Therefore, we prefer observing
the values of function g instead of function f when trying to estimate this expectation.
We can formalize the distinction between these two functions and the impact it has on
the quality of estimates by using the notion of variance, which quantiﬁes the extent to
which an observed value of a function tends to deviate from its expected value:
Va (f )
def=

x
(f (x) −Ex(f ))2 · Pr(x).
(15.2)
In Figure 15.2, the variance of function f is 5.5 and the variance of function g it .5.
Hence, the observed values of f tend to deviate from its expectation more than the values
of function g do.
2 This is why the value of f (X) is typically called a random variable.

382
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
The variance is typically denoted by σ 2, where σ is also called the standard deviation.3
The variance of a function can be used to offer some precise guarantees on the extent to
which its observed values will deviate from its expectation.
Theorem 15.2 (Chebyshev’s inequality). Let µ and σ 2 be the expectation and variance
of a function and let v be one of its observed values. For any ϵ > 0, we have
P(|v −µ| < ϵ) ≥1 −σ 2
ϵ2 .
■
The main use of this theorem is as follows. Suppose that all we know is the variance of
the function and wish to estimate its expectation. Suppose further that we observe a value
v of our function and wish to use this value as an estimate for the expectation µ. What
can we say about the quality of this estimate? In particular, what conﬁdence do we have
that the expectation µ lies in the interval (v −ϵ, v + ϵ)? Chebyshev’s inequality says that
our conﬁdence is at least 1 −σ 2/ϵ2. We say in this case that the inequality is used to
provide a conﬁdence interval for our estimate. We stress here that our conﬁdence level
in such intervals increases as the variance decreases, which highlights the importance
of a low variance in producing high levels of conﬁdence.4 Considering Figure 15.2 and
using only the variance of function g, we can immediately say that its expectation will
lie in the interval (v −2.5, v + 2.5) with 92% conﬁdence, where v is an observed value
of the function. However, we can say the same thing about function f with only 12%
conﬁdence. Hence, if we initially know that the two functions have the same expectation
and if our goal is to estimate this expectation, we clearly prefer to observe the values of
function g as they allow us to provide tighter guarantees on the corresponding estimates.
Theorem 15.2 allows us to provide conﬁdence intervals using only the variance of the
function. If we also know how the values of the function are distributed, then we can
provide tighter intervals, as shown by the following theorem.
Theorem 15.3. Let µ and σ 2 be the expectation and variance of a function and let v be
one of its observed values. If the values of the function are normally distributed, then
P(|v −µ| < ϵ) = α,
where
ϵ = −1
1 + α
2
	
σ
and (.) is the CDF of the standard Normal distribution.
■
The main use of this theorem is as follows. We start with some conﬁdence level,
say, α = 95%, which leads to −1((1 + α)/2) = 1.96 based on standard tabulations of
the function −1. We then get the guarantee P(|v −µ| < 1.96σ) = .95. That is, we
can now say that the observed value v lies in the interval (µ −1.96σ, µ + 1.96σ) with
3 The notions of expectation and variance can be extended to continuous distributions
in the usual way. Sup-
pose, for example, that h(X) is a probability density function and suppose that f (X) is a function that maps
every value of X to a real number. The expectation of f is then Ex(f )
def=

f (x)h(x)dx and its variance is
Va (f )
def=

(f (x) −Ex(f ))2h(x)dx.
4 Another use of Chebyshev’s inequality is as follows. Suppose that we know both the expectation µ and variance σ 2
of a function. We can then guarantee that an observed value of this function will lie in the interval (µ −ϵ, µ + ϵ)
with a probability at least 1 −σ 2/ϵ2.

15.3 EXPECTATIONS
383
95% conﬁdence. This answer is better than the interval we can obtain from Chebyshev’s
inequality (see Exercise 15.3).
15.3.1 Probability as an expectation
All of our approximation algorithms are based on formulating the probability of an event
as an expectation of some function. There are a number of ways for deﬁning the probabil-
ity of an event as an expectation, each leading to a different variance. We present several
deﬁnitions in this chapter but begin with the simplest function we can deﬁne for this
purpose.
Deﬁnition 15.1. The direct sampling function for event α, denoted ˘α(X), maps each
instantiation x to a number in {0, 1} as follows:
˘α(x)
def=
!
1,
if α is true at instantiation x
0,
otherwise.
■
Theorem 15.4. The direct sampling function ˘α(X) has the following expectation and
variance with respect to distribution Pr(X):
Ex(˘α) = Pr(α)
(15.3)
Va (˘α) = Pr(α)Pr(¬α) = Pr(α) −Pr(α)2.
(15.4)
■
Hence, we can now formulate the problem of approximating a probability Pr(α) as a
problem of estimating the expectation Ex(˘α). We discuss this estimation problem next but
we ﬁrst note that the variance of function ˘α gets smaller as the probability Pr(α) becomes
extreme, with the highest variance attained when Pr(α) = 1/2.
15.3.2 Estimating an expectation: Monte Carlo simulation
The basic technique for estimating the expectation of a function f (X) is using Monte
Carlo simulation, which works as follows. We ﬁrst simulate a random sample x1, . . . , xn
from the underlying distribution Pr(X) then evaluate the function at each instantiation
of the sample, f (x1), . . . , f (xn), and ﬁnally compute the arithmetic average of attained
values. This average is called the sample mean and deﬁned as:
Avn(f )
def=
1
n
n

i=1
f (xi).
(15.5)
Note that the sample mean is a function of the sample space yet the notation Avn(f )
keeps the sample x1, . . . , xn implicit for convenience. Moreover, the distribution over the
sample space is known as the sampling distribution.
Since the sample mean is a function of the sample space, it has its own expectation and
variance that are given by the following theorem.
Theorem 15.5. Let Avn(f ) be a sample mean where the function f has expectation µ
and variance σ 2. The expectation of the sample mean Avn(f ) is µ and its variance is
σ 2/n.
■

384
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
The ﬁrst part of Theorem 15.5 provides a justiﬁcation for using the sample mean
Avn(f ) as an estimate of the expectation µ. In particular, we say here that the estimate
Avn(f ) is unbiased, which is a general term used in estimation theory to indicate that
the expectation of the estimate equals the quantity we are trying to estimate. The second
part of the theorem shows that the variance of this estimate is inversely proportional to
the sample size n. Hence, the quality of our estimates will monotonically improve as we
increase the sample size.
A fundamental result known as the law of large numbers tells us that the sample mean
is guaranteed to converge to the expectation of the function f as the sample size tends to
inﬁnity.
Theorem 15.6 (Law of Large Numbers). Let Avn(f ) be a sample mean where the function
f has expectation µ. For every ϵ > 0, we then have
lim
n→∞P(|Avn(f ) −µ| ≤ϵ) = 1.
■
Note that |Avn(f ) −µ| is the absolute error of our estimate. Hence, we can guarantee
that the absolute error is bounded by any given ϵ as the sample size tends to inﬁnity. We
say in this case that the estimate Avn(f ) is consistent, which is a general term used in
estimation theory to indicate that an estimate converges to the quantity we are trying to
estimate as the sample size tends to inﬁnity.
Another fundamental result known as the Central Limit Theorem says roughly that the
sample mean has a distribution that is approximately Normal. Before we state this result,
we point out that the following statements are equivalent:
r The values of function f are Normally distributed with mean µ and variance σ 2/n.
r The values of function √n(f −µ) are Normally distributed with mean 0 and variance
σ 2.
Although the second statement appears less intuitive, it is sometimes necessary as it refers
to a Normal distribution that is independent of the sample size n.
Theorem 15.7 (Central Limit Theorem). Let Avn(f ) be a sample mean where the function
f has expectation µ and variance σ 2. As the sample size n tends to inﬁnity, the distribution
of √n(Avn(f ) −µ) converges to a Normal distribution with mean 0 and variance σ 2. We
say in this case that the estimate Avn(f ) is asymptotically Normal.
■
The Central Limit Theorem is known to be robust, suggesting that the distribution of
√n(Avn(f ) −µ) can be approximated by a Normal distribution with mean 0 and variance
σ 2. Equivalently, the distribution of estimate Avn(f ) can be approximated by a Normal
distribution with mean µ and variance σ 2/n.
It is helpful here to contrast what Theorems 15.5 and 15.7 are telling us about the
sample mean Avn(f ). Theorem 15.5 tells us that the sample mean has expectation µ
and variance σ 2/n. On the other hand, Theorem 15.7 tells us that the sample mean is
approximately Normal with mean µ and variance σ 2/n. Recall again that the sample mean
Avn(f ) is meant as an estimate of the expectation µ, so we are typically interested in
guarantees of the form, “The expectation µ lies in the interval (Avn(f ) −ϵ, Avn(f ) + ϵ)
with some conﬁdence level α.” Theorem 15.5 allows us to provide conﬁdence intervals
based on Chebyshev’s inequality (Theorem 15.2) as this inequality requires that we only
know the variance of the sample mean. However, Theorem 15.7 allows us to provide

15.4 DIRECT SAMPLING
385
Algorithm 45 DIRECT SAMPLING(α, n, N)
input:
α:
event
n:
sample size
N:
Bayesian network inducing distribution Pr(X)
output: an estimate for the probability Pr(α)
main:
1: P←0
2: for i = 1 to n do
3:
x←SIMULATE BN(N) {Algorithm 44}
4:
P←P + ˘α(x) {see Deﬁnition 15.1 for computing ˘α(x)}
5: end for
6: return P/n {sample mean}
better conﬁdence intervals based on Theorem 15.3, as this theorem requires the sample
mean to be Normally distributed.
It is known that Theorem 15.7 continues to hold if we replace the variance σ 2 by what
is known as the sample variance,
S2
n(f )
def=
1
n −1
n

i=1
(f (xi) −Avn(f ))
2.
(15.6)
This is quite important in practice as it allows us to compute conﬁdence intervals even
when we do not know the value of variance σ 2. Similar to the sample mean, the sample
variance is also a function of the sample space. Moreover, the expectation of this function
is known to be σ 2, which justiﬁes using the sample variance as an estimate of σ 2. Finally,
the square root of the sample variance is known as the standard error.
15.4 Direct sampling
We now have all the ingredients needed to provide a formal treatment of the method
introduced in Section 15.2 for estimating a probability Pr(α). We begin by providing a
formal statement of this method, which we call direct sampling, and then follow with a
discussion of its properties.
According to Theorem 15.4, the probability Pr(α) can be formulated as an expectation
of the function ˘α given by Deﬁnition 15.1. Hence, we can estimate this probability by
estimating the expectation of ˘α using Monte Carlo simulation:
1. Simulate a sample x1, . . . , xn from the given Bayesian network.
2. Compute the values ˘α(x1), . . . , ˘α(xn).
3. Estimate the probability Pr(α) using the sample mean,
Avn(˘α) = 1
n
n

i=1
˘α(xi).
(15.7)
Consider Table 15.1, which depicts a sample drawn from the Bayesian network in Fig-
ure 15.1. Consider also the event α: the grass is wet or the road is slippery. The corre-
sponding values of function ˘α are 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, leading to an estimate of 7/10
for the probability of event α. Algorithm 45 provides the pseudocode for direct sampling.

386
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
The following result, which is a corollary of Theorems 15.4 and 15.5, provides some
properties of direct sampling.
Corollary 4. Consider the sample mean Avn(˘α) in (15.7). The expectation of this sample
mean is Pr(α) and its variance is Pr(α)Pr(¬α)/n.
■
Using the Central Limit Theorem, we also conclude that the distribution of sample mean
Avn(˘α) can be approximated by a Normal distribution with mean Pr(α) and variance
Pr(α)Pr(¬α)/n. Note, however, that we cannot compute the variance Pr(α)Pr(¬α) in
this case as we do not know the probability Pr(α) (we are trying to estimate it). Yet as
mentioned previously, we can still apply the Central Limit Theorem by using the sample
variance S2n(˘α) instead of the true variance. We next provide some guarantees on both
the absolute and relative error of the estimates computed by direct sampling.
15.4.1 Bounds on the absolute error
The absolute error of an estimate Avn(˘α) is the absolute difference it has with the true
probability Pr(α) we are trying to estimate. We next provide two bounds on the absolute
error of estimates computed by direct sampling.
The ﬁrst bound is immediate from Corollary 4 and Chebyshev’s inequality.
Corollary 5. For any ϵ > 0, we have
P(|Avn(˘α) −Pr(α)| < ϵ) ≥1 −Pr(α)Pr(¬α)
nϵ2
.
■
That is, the estimate Avn(˘α) computed by direct sampling falls within the interval (Pr(α) −
ϵ, Pr(α) + ϵ) with probability at least 1 −Pr(α)Pr(¬α)/nϵ2. Figure 15.3 plots this bound
as a function of the sample size n and probability Pr(α). Note that we typically do not
know the probability Pr(α) yet we can still use this bound by assuming a worst-case
scenario of Pr(α) = 1/2.
A sharper bound that does not depend on the probability Pr(α) follows from the
following result.
Theorem 15.8 (Hoeffding’s inequality). Let Avn(f ) be a sample mean where the function
f has expectation µ and values in {0, 1}. For any ϵ > 0, we have
P(|Avn(f ) −µ| ≤ϵ) ≥1 −2e−2nϵ2.
■
The second bound on the absolute error is immediate from Corollary 4 and Hoeffding’s
inequality.
Corollary 6. For any ϵ > 0, we have
P(|Avn(˘α) −Pr(α)| ≤ϵ) ≥1 −2e−2nϵ2.
■
That is, the estimate Avn(˘α) computed by direct sampling falls within the interval (Pr(α) −
ϵ, Pr(α) + ϵ) with probability at least 1 −2e−2nϵ2. Figure 15.4 plots this bound as a
function of the sample size n.

15.4 DIRECT SAMPLING
387
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0
 10000
 20000
 30000
 40000
 50000
Absolute error ε 
Sample size n
Confidence = 95%
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0
 10000
 20000
 30000
 40000
 50000
Absolute error ε 
Sample size n
Confidence = 99%
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
Figure 15.3: Absolute error ϵ as a function of the sample size n and event probability, according to
Corollary 6. The conﬁdence level represents the probability that the estimate will fall within the interval
(Pr(α) −ϵ, Pr(α) + ϵ).
15.4.2 Bounds on the relative error
The relative error of an estimate Avn(˘α) is deﬁned as
|Avn(˘α) −Pr(α)|
Pr(α)
.

388
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0
 10000
 20000
 30000
 40000
 50000
Absolute error ε 
Sample size n
Confidence = 95%
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0
 10000
 20000
 30000
 40000
 50000
Absolute error ε 
Sample size n
Confidence = 99%
Figure 15.4: Absolute error ϵ as a function of the sample size n, according to Corollary 6. The conﬁdence
level represents the probability that the estimate will fall within the interval (Pr(α) −ϵ, Pr(α) + ϵ).
Considering Figure 15.3, it should be clear that the bound on the absolute error becomes
tighter as the probability of an event becomes more extreme. Yet the corresponding bound
on the relative error becomes looser as the probability of an event becomes more extreme
(see Figure 15.5). For example, for an event with probability .5 and a sample size of
10,000, there is a 95% chance that the relative error is about 4.5%. However, for the same
conﬁdence level the relative error increases to about 13.4% if the event has probability .1,

15.4 DIRECT SAMPLING
389
 0
 20
 40
 60
 80
 100
 0
 10000
 20000
 30000
 40000
 50000
Relative error ε 
Sample size 
Confidence = 95%
n
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
 0
 20
 40
 60
 80
 100
 0
 10000
 20000
 30000
 40000
 50000
Relative error ε 
Sample size 
Confidence = 95%
n
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
Figure 15.5: Percentage relative error ϵ as a function of the sample size n and event probability, according
to Corollary 7. The conﬁdence level represents the probability that the estimate will fall within the interval
(Pr(α) −ϵPr(α), Pr(α) + ϵPr(α)).
and increases again to about 44.5% if the event has probability .01. We can quantify this
more precisely based on Corollary 5.
Corollary 7. For any ϵ > 0, we have
P
|Avn(˘α) −Pr(α)|
Pr(α)
< ϵ
	
≥1 −Pr(¬α)
nϵ2Pr(α).
■

390
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
 0
 20
 40
 60
 80
 100
 0
 10000
 20000
 30000
 40000
 50000
Relative error ε 
Sample size n
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
Confidence = 95%
 0
 20
 40
 60
 80
 100
 0
 10000
 20000
 30000
 40000
 50000
Relative error ε 
Sample size n
Pr(α)=.01
Pr(α)=.05
Pr(α)=.1
Pr(α)=.5
Confidence = 95%
Figure 15.6: Percentage relative error ϵ as a function of the sample size n and event probability, according
to Corollary 8. The conﬁdence level represents the probability that the estimate will fall within the interval
(Pr(α) −ϵPr(α), Pr(α) + ϵPr(α)).
This result, plotted in Figure 15.5, highlights the difﬁculty with bounding the relative
error when estimating the probability of rare events, even when the sample size is large.
We also have the following bound on the relative error, which follows directly from
Corollary 6 and is plotted in Figure 15.6.

15.4 DIRECT SAMPLING
391
Corollary 8. For any ϵ > 0, we have
P
|Avn(˘α) −Pr(α)|
Pr(α)
≤ϵ
	
≥1 −2e−2nϵ2Pr(α)2.
■
Note that both Corollaries 7 and 8 require the probability Pr(α) or some lower bound
on it.
15.4.3 Rao-Blackwell sampling
Direct sampling is the only method we currently have for estimating a probability Pr(α).
This method uses Monte Carlo simulation to estimate the expectation of the direct sam-
pling function ˘α that has variance Pr(α)Pr(¬α) (see Theorem 15.4). We next deﬁne another
estimation method called Rao-Blackwell sampling, which is based on the expectation of
a different function and can have a smaller variance.
To reveal the intuition behind Rao-Blackwell sampling, suppose we have a Bayesian
network over disjoint variables X and Y, where our goal is to estimate the probability
of some event α, Pr(α). Suppose further that the computation of Pr(α|y) can be done
efﬁciently for any instantiation y. Rao-Blackwell sampling exploits this fact to reduce the
variance by sampling from the distribution Pr(Y) instead of the full distribution Pr(X, Y).
In particular, Rao-Blackwell sampling does the following:
1. Draw a sample y1, . . . , yn from the distribution Pr(Y).
2. Compute Pr(α|yi) for each sampled instantiation yi.
3. Estimate the probability Pr(α) using the average (1/n) n
i=1 Pr(α|yi).
As we see next, this estimate generally has a smaller variance than the one produced by
direct sampling.
Rao-Blackwell sampling is formalized as follows.
Deﬁnition 15.2. The Rao-Blackwell function for event α and distribution Pr(X, Y),
denoted ¨α(Y), maps each instantiation y into [0, 1] as follows:
¨α(y)
def= Pr(α|y).
■
Hence, if our sample is y1, . . . , yn and if we use Monte Carlo simulation to estimate the
expectation of the Rao-Blackwell function ¨α(Y), then our estimate will simply be the
sample mean,
Avn(¨α) = 1
n
n

i=1
Pr(α|yi).
Theorem 15.9. The expectation and variance of the Rao-Blackwell function ¨α(Y) with
respect to distribution Pr(Y) are
Ex(¨α) = Pr(α)
Va (¨α) =

y
Pr(α|y)2Pr(y) −Pr(α)2.
■
The variance of the newly deﬁned function ¨α ranges between zero and Pr(α)Pr(¬α)
(see Exercise 15.13). Hence, the variance of Rao-Blackwell sampling is no greater
than the variance of direct sampling and can actually be much smaller. Note, however,

392
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
that for Rao-Blackwell sampling to be feasible the variables Y must be chosen care-
fully to allow for the efﬁcient computation of the conditional probability Pr(α|y) (see
Exercise 15.11).
15.5 Estimating a conditional probability
Consider now the problem of estimating a conditional probability Pr(α|β) when the
distribution Pr(.) is induced by a Bayesian network. Since Pr(α|β) corresponds to the
expectation of the direct sampling function ˘α with respect to distribution Pr(.|β), we can
estimate Pr(α|β) by estimating the corresponding expectation. However, the problem with
this approach is that sampling from the distribution Pr(.|β) is generally hard, which tends
to preclude the use of Monte Carlo simulation for computing this expectation. That is, we
typically cannot efﬁciently generate a sequence of independent instantiations x1, . . . , xn
where the probability of generating instantiation xi is Pr(xi|β).5
A common solution to this problem is based on estimating the probabilities Pr(α ∧β)
and Pr(β) and then taking their ratio as an estimate for Pr(α|β). For example, if γ =
α ∧β and if we are using direct sampling to estimate probabilities, our estimate for the
conditional probability Pr(α|β) is then the ratio Avn( ˘γ )/Avn( ˘β).
When computing the ratio Avn( ˘γ )/Avn( ˘β), it is quite common to generate one sample
of size n from the distribution Pr(.) and then use it for computing the sample means
Avn( ˘γ ) and Avn( ˘β) simultaneously. In particular, let c1 be the number of instantiations
in the sample at which γ = α ∧β is true and let c2 be the number of instantiations at
which β is true (c1 ≤c2). Then Avn( ˘γ )/Avn( ˘β) equals (c1/n)/(c2/n) = c1/c2, which is
the estimate for Pr(α|β). This method is called rejection sampling as it can be thought of
as rejecting all sampled instantiations at which β is false (see Exercise 15.14 for a more
formal explanation).
Theorem 15.10 sheds some light on the quality of estimates produced by rejection
sampling.
Theorem 15.10. Let α and β be two events and let γ = α ∧β. The estimate
Avn( ˘γ )/Avn( ˘β) is asymptotically Normal and its distribution can be approximated by
a Normal distribution with mean Pr(α|β) and variance
Pr(α|β)Pr(¬α|β)
nPr(β)
.
■
According to this result, the variance of rejection sampling grows larger as the probability
of β gets smaller. Hence, when Pr(β) is really small rejection sampling may not be an
attractive method for estimating conditional probabilities, even when the sample size n is
large.
We next present two alternative approaches for estimating conditional probabilities.
The ﬁrst method, called importance sampling, is based on formulating the probability as
an expectation of yet another function. The second method, called Markov chain Monte
Carlo simulation, allows us to compute expectations with respect to distributions that may
be hard to sample from, such as Pr(.|β). These methods are discussed in Sections 15.6
and 15.7, respectively.
5 Contrast this with the discussion in Section 15.2 where we provided an efﬁcient procedure for sampling from
the unconditional distribution Pr(X). That is, we showed how to efﬁciently generate a sequence of independent
instantiations x1, . . . , xn where the probability of generating instantiation xi is Pr(xi).

15.6 IMPORTANCE SAMPLING
393
15.6 Importance sampling
Importance sampling is a technique that can be used to reduce the variance when estimating
the probabilities of rare events or when estimating probabilities that are conditioned on
rare events. The basic idea behind this technique is to sample from another distribution *
Pr
that emphasizes the instantiations consistent with the rare event. This is done by deﬁning
the probability of an event as an expectation of a new function as given next.
Deﬁnition 15.3. Given an event α and two distributions Pr(X) and *
Pr(X), the importance
sampling function for event α, denoted ˜α(X), maps each instantiation x into a number
as follows:
˜α(x)
def=
 Pr(x)/*
Pr(x),
if α is true at instantiation x
0,
otherwise.
*
Pr(X) is called the importance or proposal distribution and is required to satisfy the
following condition: *
Pr(x) = 0 only if Pr(x) = 0 for all instantiations x at which α is
true.
■
Note that although a direct sampling function ˘α has values in {0, 1} and a Rao-Blackwell
function ¨α has values in [0, 1], an important sampling function ˜α has values in [0, ∞).
To estimate a probability Pr(α), importance sampling simply estimates the expectation
of the corresponding importance sampling function ˜α using Monte Carlo simulation. That
is, the method start by drawing a sample x1, . . . , xn from the importance distribution
*
Pr(X). It then computes the corresponding values ˜α(x1), . . . , ˜α(xn). It ﬁnally estimates
the probability Pr(α) using the sample mean,
Avn(˜α)
def=
1
n
n

i=1
˜α(xi).
(15.8)
Importance sampling improves on direct sampling only when the importance distri-
bution satisﬁes some conditions. To characterize these conditions, we ﬁrst start with the
following properties of the importance sampling function.
Theorem 15.11. The expectation and variance of an importance sampling function ˜α(X)
with respect to the importance distribution *
Pr(X) are
Ex(˜α) = Pr(α)
Va (˜α) =
⎛
⎝
x|=α
Pr(x)2
*
Pr(x)
⎞
⎠−Pr(α)2.
■
If *
Pr(x) ≥Pr(x) for all instantiations x that are consistent with event α, the variance
Va (˜α) ranges between zero and Pr(α)Pr(¬α) (see Exercise 15.19). Hence, under this con-
dition the variance of importance sampling is no greater than the variance of direct sam-
pling (see Theorem 15.4). Intuitively, this condition requires the importance distribution
to emphasize the instantiations consistent with event α no less than they are emphasized
by the original distribution. We later provide a concrete instance of importance sampling
that satisﬁes this condition.
We next deﬁne an additional property of importance distributions that, although difﬁcult
to ensure in practice, provides an idealized case that reveals the promise of this method
in estimating probabilities.

394
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Deﬁnition 15.4. Two distributions Pr(X) and *
Pr(X) are proportional at event α if and
only if Pr(x)/*
Pr(x) = c for some constant c > 0 and all instantiations x that are consistent
with event α.
■
Theorem 15.12. If the importance and original distributions are proportional at event α,
we have
Va (˜α) =
Pr(α)
*
Pr(α)
	
Pr(α) −Pr(α)2.
(15.9)
■
Consider now the variance of direct sampling:
Va (˘α) = Pr(α) −Pr(α)2.
If the importance distribution assigns a greater probability to event α, *
Pr(α) > Pr(α),
importance sampling is guaranteed to have a smaller variance than direct sampling.
Moreover, the reduction in variance is precisely tied to the extent to which event α is em-
phasized by the importance distribution. Note that when *
Pr(α) = 1, the variance is zero
and the importance distribution *
Pr(.) equals Pr(.|α). Hence, Pr(.|α) can be viewed as
the ideal importance distribution when estimating Pr(α) yet we typically cannot use this
distribution for the reasons mentioned in Exercise 15.20.
Let us now return to our original problem of estimating a conditional probability Pr(α|β)
when event β is rare. Suppose again that we estimate this conditional probability using
rejection sampling, that is, using the ratio of estimates for Pr(α, β) and Pr(β). Suppose,
however, that we now use importance sampling to produce these estimates. Theorem
15.13 sheds some light on the quality of our estimate when the importance distribution
satisﬁes the proportionality condition.
Theorem 15.13. Let α and β be two events, let γ = α ∧β, and suppose that the original
and importance distributions are proportional at event β as given by Deﬁnition 15.4. The
estimate Avn( ˜γ )/Avn( ˜β) is asymptotically Normal and its distribution can be approxi-
mated by a Normal distribution with mean Pr(α|β) and variance
Pr(α|β)Pr(¬α|β)
n*
Pr(β)
.
■
Compare this with the estimate based on direct sampling (Theorem 15.10), for which
the variance is
Pr(α|β)Pr(¬α|β)
nPr(β)
.
Importance sampling in this idealized case allows us to replace Pr(β) by *
Pr(β) in the
denominator, which in turn makes the variance independent of Pr(β).
This analysis reveals the promise of importance sampling in dealing with unlikely
events as it allows us to make the variance independent of the probability of such events.
However, we stress that using importance distributions that satisfy the condition of The-
orem 15.12 is generally not feasible. We next discuss a weaker condition that is easy to
ensure in practice yet is guaranteed to improve on the variance of direct sampling.

15.6 IMPORTANCE SAMPLING
395
A
B
C
D
E
A
B
C
D
E
Bayesian network
Likelihood weighting network
Figure 15.7: A Bayesian network and its likelihood weighting network given evidence on variables B and
E. Both networks have the same CPTs for variables A, C, and D. The CPTs for variables B and E in the
second network depend on the available evidence. For example, if the evidence is B =b and E = ¯e, then
θb = 1, θ¯b = 0 and θe = 0, θ¯e = 1.
15.6.1 Likelihood weighting
Consider a Bayesian network N and some evidence e, where the goal is to estimate some
conditional probability Pr(α|e). The method of likelihood weighting uses an importance
distribution, which is deﬁned as follows.
Deﬁnition 15.5. Let N be a Bayesian network and let e be a variable instantiation. The
corresponding likelihood-weighting network *
N is obtained from N by deleting edges
going into nodes E while setting the CPTs of these nodes as follows. If variable E ∈E
is instantiated to e in evidence e, then *θe = 1; otherwise, *θe = 0. All other CPTs of
network *
N are equal to those in network N.
■
Figure 15.7 depicts a Bayesian network and a corresponding likelihood-weighting
network given evidence on nodes B and E. Note here that the CPTs for variables A, C,
and D are the same for both networks.
The variance of likelihood weighting is no greater than the variance of direct sampling
due to Theorem 15.14.
Theorem 15.14. Let N be a Bayesian network, e be some evidence, and let *N be the
corresponding likelihood-weighting network. Suppose that networks N and *N induce
distributions Pr(X) and *
Pr(X), respectively, and that x is consistent with evidence e. If θe|u
ranges over the parameters of variables E ∈E in network N, where eu is consistent with
instantiation x, then
Pr(x)/*
Pr(x) =

θe|u
θe|u ≤1.
■
According to Theorem 15.19, *
Pr(x) ≥Pr(x) for every instantiation x consistent with
evidence e. Hence, the variance of likelihood weighting cannot be larger than the vari-
ance of direct sampling (see Exercise 15.19). Theorem 15.14 also shows that the ratio
Pr(x)/*
Pr(x) can be computed efﬁciently for any instantiation x as it corresponds to the
product of all network parameters θe|u that belong to the CPTs of evidence variables E
and are consistent with instantiation x. Consider Figure 15.7 for an example and assume
that we have the evidence B =b and E = ¯e. For instantiation x : ¯a, b, c, d, ¯e, we have
Pr(¯a, b, c, d, ¯e)
*
Pr(¯a, b, c, d, ¯e) = θ¯aθb|¯aθc|¯aθd|bcθ¯e|c
θ¯a(1)θc|¯aθd|bc(1) = θb|¯aθ¯e|c.

396
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Algorithm 46 LIKELIHOOD WEIGHTING(e, n, N)
input:
e:
evidence
n:
sample size
N:
Bayesian network
output: an estimate for Pr(e) and Pr(x|e) for each value x of variable X in network N,
where Pr is the distribution induced by N
main:
1: N′←LW network for network N and evidence e
2: P←0 {estimate for Pr(e)}
3: P[x]←0 for each value x of variable X in network N {estimate for Pr(x, e)}
4: for i = 1 to n do
5:
x←SIMULATE BN(N′)
6:
W←product of all network parameters θe|u where E ∈E and eu ∼x
7:
P←P + W
8:
P[x]←P[x] + W for each variable X and its value x consistent with x
9: end for
10: return P/n and P[x]/P for each value x of variable X in network N
Consider again the network in Figure 15.7 (its CPTs are shown in Figure 15.1). Suppose
that our goal is to estimate the conditional probability Pr( ¯d|b, ¯e) by estimating each of
Pr( ¯d, b, ¯e) and Pr(b, ¯e) and then taking their ratio. The following table contains a sample
of ﬁve instantiations generated from the likelihood-weighting network in Figure 15.7
using evidence b, ¯e.
Instantiation x
Pr(x)/*
Pr(x)
˜γ (x)
˜β(x)
x1 : a, b, c, d, ¯e
θb|aθ¯e|c
= (.2)(.3)
0
.060
x2 : a, b, c, ¯d, ¯e
θb|aθ¯e|c
= (.2)(.3)
.060
.060
x3 : a, b, ¯c, ¯d, ¯e
θb|aθ¯e|¯c
= (.2)(1)
.200
.200
x4 : ¯a, b, c, d, ¯e
θb|¯aθ¯e|c
= (.75)(.3)
0
.225
x5 : a, b, c, d, ¯e
θb|aθ¯e|c
= (.2)(.3)
0
.060
The table also shows the corresponding values of the importance sampling function for
events γ = ¯d, b, ¯e and β = b, ¯e. Hence, we have Av5( ˜γ ) = .260/5 and Av5( ˜β) = .605/5,
leading to .260/.605 as the estimate of conditional probability Pr( ¯d|b, ¯e).
Note here that we used the same importance distribution for both events γ and β. In
principle, we could use two distinct importance distributions, each targeted toward the
corresponding event. Using the same distribution is more common in practice, especially
when estimating Pr(X|e) for each node X in a Bayesian network. Algorithm 46 provides
pseudocode for estimating these conditional probabilities using the method of likelihood
weighting.
15.6.2 Particle ﬁltering
Likelihood weighting can be applied to dynamic Bayesian networks (DBNs), leading to an
instance of what is known as sequential importance sampling (SIS) (see Exercise 15.22).
However, in this section we discuss a more sophisticated application of likelihood weight-
ing to DBNs, known as particle ﬁltering. We ﬁrst describe particle ﬁltering on HMMs
and then on a more general class of DBNs.

15.6 IMPORTANCE SAMPLING
397
1S
2
S
3
S
n
S
1
O
2
O
3
O
n
O
Figure 15.8: A hidden Markov model (HMM).
1S
1
O
)
Pr(
1S
)
|
Pr(
1
1 S
O
1S
2
S
2
O
)
(
Pr
1S
∗
)
|
Pr(
2
2 S
O
)
|
Pr(
1
2 S
S
2
S
3
S
3
O
)
(
Pr
2
S
∗
)
|
Pr(
3
3 S
O
)
|
Pr(
2
3 S
S
(a) Pr′(S1, O1)
(b) Pr′(S1, S2, O2)
(c) Pr′(S2, S3, O3)
Figure 15.9: The networks used by particle ﬁltering when performing inference on the HMM in Figure 15.8.
Consider the HMM in Figure 15.8 and suppose that our goal is to estimate the con-
ditional probability Pr(St|O1, . . . , Ot) for each time t. The key insight behind particle
ﬁltering is to approximate this HMM using a number of smaller networks as shown in
Figure 15.9. In particular, for each time step t > 1, particle ﬁltering will work with a
network that induces a distribution Pr′(St−1, St, Ot) speciﬁed by the following CPTs:
r A CPT for node St−1, Pr⋆(St−1), which is speciﬁed as given next.
r A CPT for node St, Pr(St|St−1), obtained from the original HMM.
r A CPT for node Ot, Pr(Ot|St), obtained from the original HMM.
Suppose now that the CPT for node St−1 is set as
Pr⋆(St−1) = Pr(St−1|O1, . . . , Ot−1).
(15.10)
That is, the CPT is chosen to summarize the impact that past observations have on node
St−1. This choice leads to the following distribution for the network fragment at time ti
(see Exercise 15.23):
Pr′(St−1, St, Ot) = Pr(St−1, St, Ot|O1, . . . , Ot−1).
(15.11)
Hence, the network fragment used by particle ﬁltering at time t now integrates all the
evidence obtained before time t. More precisely, this network fragment can be used to
obtain the probability of interest, as we now have (see Exercise 15.23):
Pr′(St|Ot) = Pr(St|O1, . . . , Ot).
(15.12)
The only problem with this proposal is that we typically do not have the distribution
Pr(St−1|O1, . . . , Ot−1) in (15.10) as it is precisely this distribution that particle ﬁltering
is trying to compute. Particle ﬁltering instead uses an estimate of this distribution when
specifying the CPT Pr⋆(St−1) for node St−1. Moreover, it obtains this estimate from the
network fragment it has for time t −1.

398
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
1S
1
O
)
Pr(
1S
1S
2
S
2
O
)
(
Pr
1S
∗
)
|
Pr(
1
2 S
S
2
S
3
S
3
O
)
(
Pr
2
S
∗
)
|
Pr(
2
3 S
S
(a) *
Pr(S1, O1)
(b) *
Pr(S1, S2, O2)
(c) *
Pr(S2, S3, O3)
Figure 15.10: The likelihood-weighting networks used by particle ﬁltering.
Let us consider a concrete example of how particle ﬁltering works by considering the
HMM in Figure 15.8 and the evidence
O1 =T , O2 =F, O3 =F.
To estimate Pr(S3|O1 =T , O2 =F, O3 =F), particle ﬁltering performs the following com-
putations:
r It estimates Pr′(S1|O1 =T ) using the network in Figure 15.9(a) and then uses the result to
specify Pr⋆(S1) in the network of Figure 15.9(b).
r It then estimates Pr′(S2|O2 =F) using the network in Figure 15.9(b) and then uses the
result to specify Pr⋆(S2) in the network of Figure 15.9(c).
r It ﬁnally estimates Pr′(S3|O3 =F) using the network in Figure 15.9(c).
This last estimate corresponds to the desired probability Pr(S3|O1 =T , O2 =F, O3 =F);
see (15.12).
Note that this process requires that we only pass the estimate of Pr′(St−1|Ot−1) from
time t −1 to time t. Hence, the space requirements for particle ﬁltering are independent
of the time span, which is critical if t is large. This is indeed one of the main highlights of
particle ﬁltering.
Likelihood weighting
Note that particle ﬁltering must estimate three conditional probabilities of the form
Pr′(St|Ot) for t = 1, 2, 3. These estimates are computed using likelihood weighting on
the network fragments corresponding to these times. Figure 15.10 depicts the likelihood-
weighting networks and corresponding distributions *
Pr(.) used by particle ﬁltering.
In particular, at time t > 1 particle ﬁltering generates the following sample from the
likelihood-weighting network:
s1
t−1, s1
t , ot,
s2
t−1, s2
t , ot,
. . . ,
sn
t−1, sn
t , ot,
where ot is the value of Ot as given by the evidence. It then uses this sample to estimate
the conditional probability Pr′(st|ot) by ﬁrst estimating the unconditional probabilities
Pr′(st, ot) and Pr′(ot) and computing their ratio.
We can show that the estimate of Pr′(ot) is given by the following average (see Exer-
cise 15.24):
1
n
n

i=1
Pr(ot|si
t ),
(15.13)

15.6 IMPORTANCE SAMPLING
399
Algorithm 47 PARTICLE FILTER({s1
t−1, . . . , sn
t−1}, ot, Pr(St|St−1), Pr(Ot|St))
input:
{s1
t−1, . . . , sn
t−1}:
particles passed from time t −1
ot:
evidence for time t
Pr(St|St−1):
transition distribution
Pr(Ot|St):
sensor distribution
output: an estimate of Pr′(St|ot), and n particles passed to time t + 1
main:
1: P←0 {estimate for Pr′(ot)}
2: P[st]←0 for each state st of variable St {estimate for Pr′(st, ot)}
3: for i = 1 to n do
4:
si
t ←sampled value from Pr(St|si
t−1)
5:
P←P + Pr(ot|si
t )
6:
P[st]←P[st] + Pr(ot|si
t ) for state st = si
t
7: end for
8: return P[st]/P for each state st (an estimate of Pr′(st|ot)), and n particles sampled
from the distribution P[st]/P (passed to time t + 1)
where Pr(ot|si
t ) is called the weight of state si
t . Moreover, we can show that the estimate
of Pr′(st, ot) is given by the average (see Exercise 15.24):
1
n

si
t =st
Pr(ot|si
t ).
(15.14)
Hence, the estimate for the conditional probability Pr′(st|ot) is given by

si
t =st Pr(ot|si
t )

si
t Pr(ot|si
t ) .
(15.15)
Using particles
The standard implementation of particle ﬁltering does not actually pass the estimate of
Pr′(St−1|Ot−1) from time t −1 to time t. Instead, a sample of size n is generated from this
distribution and passed to the next time step since this is all we need to apply likelihood
weighting at time t. Members of this sample are called particles.
The particles for a time step can then be generated from the particles at the previous
time step as follows:
r For each of the n particles si
t−1 passed from time t −1 to time t, sample a state si
t from
the distribution Pr(St|si
t−1).
r Compute an estimate for Pr′(St|ot) using the sampled states s1
t , . . . , sn
t as given by (15.15).
r Sample n states from the estimate of Pr′(St|ot). These are the n particles to be passed from
time t to time t + 1.
Algorithm 47 provides the pseudocode for one iteration of particle ﬁltering.6
6 Note that when variable St has a very large number of states, the estimated probability for many of these states
will be zero. In fact, with n particles no more than n states of variable St can have estimates greater than zero.
This implies that computing an estimate for the distribution Pr′(St|ot) can be done in O(n) time and space. This

400
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
S1
S3
S2
S4
O1
O2
S1
S3
S2
S4
O1
O2
t=
t=
1
2
Figure 15.11: A dynamic Bayesian network.
Particle ﬁltering on more general DBNs
Our treatment of particle ﬁltering has thus far been focused on HMMs as given by
Figure 15.8, which are speciﬁed by an initial distribution Pr(S1), a transition distribution
Pr(St|St−1), and a sensor distribution Pr(Ot|St). The ﬁrst and second distributions are used
to generate particles, while the third distribution is used to compute the weights of these
particles.
Particle ﬁltering can be similarly applied to a larger class of DBNs with the following
structure (see Figure 15.11):
r Each edge in the network connects two nodes at the same time t or extends from a node
at time t −1 to a node at time t.
r The set of nodes at time t is partitioned into two sets St and Ot, where nodes Ot are leaves
having their parents in St and are guaranteed to be observed at time t.
These networks can be viewed as factored versions of HMMs, in which the hidden
state St is now factored into a set of variables St and the observed variable Ot is fac-
tored into a set of variables Ot. For this class of DBNs, the three relevant distributions,
Pr(S1), Pr(St|St−1), and Pr(Ot|St), can be represented using network fragments that al-
low us to efﬁciently perform the computations needed by particle ﬁltering, as discussed
next.
First, the distribution Pr(S1) is represented by the initial time slice (t = 1) of the DBN
as given in Figure 15.11. We can therefore sample an initial set of particles for time t = 1
by applying Algorithm 44 to this network.
Second, the distribution Pr(St|st−1) can be represented by a fragment of the DBN as
given in Figure 15.12(a). This network fragment can then be used to generate particles
for time t based on particles for time t −1. For example, in Figure 15.12(a) we can use
each particle for time t −1 to set the values of variable S2 and S3 at time t −1 and then
apply Algorithm 44 to sample values for variables S1, S2, S3, S4 at time t.
complexity is critical when applying particle ﬁltering to the more general class of DBNs that we discuss next (see
Figure 15.12). For these networks, the variable St is replaced by a set of variables St. Hence, it is critical to compute
an estimate for the distribution Pr′(St|Ot) in O(n) time and space instead of O(m), where m is the number of
instantiations of variables St.

15.7 MARKOV CHAIN SIMULATION
401
s3
s2
S1
S3
S2
S4
s3
s4
O1
O2
(a) Pr(S1
t , S2
t , S3
t , S4
t |s2
t−1, s3
t−1)
(b) Pr(O1
t , O2
t |s3
t , s4
t )
Figure 15.12: Fragments of a DBN. The fragment in (a) represents the distribution Pr(St|St−1). It is
constructed from the network slice at time t and the parents at time t −1. The fragment in (b) represents
the distribution Pr(Ot|St). It is a subset of the network slice at time t obtained by keeping only nodes in
Ot and their parents in St.
Third, the distribution Pr(Ot|St) can be represented by a fragment of the DBN as given
in Figure 15.12(b). Using this fragment, we can efﬁciently compute the weight of a particle,
Pr(ot|st), which must correspond to a product of network parameters. This follows since
the nodes in Ot are all leaf nodes in the network fragment with their parents in St. For
example, in Figure 15.12 Pr(O1
t , O2
t |s1
t , s2
t , s3
t , s4
t ) evaluates to Pr(O1
t |s3
t , s4
t )Pr(O2
t |s4
t ),
which is a simple product of network parameters.
Specifying the pseudocode for particle ﬁltering using this more general class of DBNs
is left to Exercise 15.25.
15.7 Markov chain simulation
Given a function f (X) and an underlying distribution Pr(X), we present in this section
a method for estimating the expectation of this function but without sampling directly
from the distribution Pr(X). This method, known as Markov chain Monte Carlo (MCMC)
simulation, provides an alternative to Monte Carlo simulation as the latter method requires
an ability to sample from the distribution Pr(X).
MCMC ﬁrst constructs a Markov chain,
X1→X2→. . . →Xn.
It then generates a sample x1, x2, . . . , xn from the distributions P(X1), P(X2|x1), . . . ,
P(Xn|xn−1), respectively, through a process known as simulating the Markov chain. If the
Markov chain satisﬁes some speciﬁc properties, then MCMC estimates based on such a
sample are consistent. That is, as the sample size n tends to inﬁnity, the sample mean,
1
n
n

i=1
f (xi),
converges to the expectation of function f with respect to distribution Pr(X).
Note here that MCMC does not have to sample directly from the distribution Pr(X),
which is a main advantage of this algorithm as it allows us to compute expectations with
respect to distributions that may be hard to sample from. For example, we see later that
MCMC can compute expectations with respect to a conditional distribution, Pr(.|β), even
when sampling from this distribution may not be computationally feasible.
In the next section, we deﬁne some properties of Markov chains that guarantee the
consistency of estimates produced by MCMC. We follow this by discussing a particular
method for constructing Markov chains that satisfy these properties.

402
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
15.7.1 Markov chains
A Markov chain for variables X is a dynamic Bayesian network of the form
X1→X2→. . . →Xn,
where the instantiations x of X are called states, the CPT for variable X1 is called the initial
distribution, and the CPTs for variables X2, . . . , Xn are called transition distributions or
transition matrices. When all transition distributions are the same, the Markov chain is
said to be homogeneous. We restrict our discussion in this section to homogeneous Markov
chains and use P to denote the distribution induced by the chain.
Suppose now that we have a Markov chain where the initial distribution P(X1) is set
to some given distribution Pr(X). If the chain maintains this distribution for all times, that
is, P(Xt) = Pr(X) for t > 1, we say that Pr(X) is a stationary distribution for the Markov
chain. We also say that a Markov chain is irreducible if every state x′ is reachable from
every other state x, that is, if for every pair of states x and x′, there is some time t such that
P(Xt =x′|X1 =x) > 0. The states of the Markov chain are said to be recurrent in this case
as each state is guaranteed to be visited an inﬁnite number of times when we simulate
the chain. Every Markov chain has at least one stationary distribution yet an irreducible
Markov chain is guaranteed to have a unique stationary distribution.7 We also have
the following.
Theorem 15.15. Let X1→X2→. . . →Xn be an irreducible Markov chain and let Pr(X)
be its stationary distribution. Let f (X) be a function and x1, . . . , xn be a sample simulated
from the given Markov chain. The sample mean
Avn(f ) = 1
n
n

i=1
f (xi)
will then converge to the expectation of function f ,
lim
n→∞Avn(f ) = Ex(f ) =

x
f (x)Pr(x).
■
To use MCMC as suggested here, we must therefore construct an irreducible Markov
chain that has an appropriate stationary distribution.
Consider now a Markov chain whose transition matrix stands in the following relation-
ship with a distribution Pr(X):
Pr(x)P(Xi =x′|Xi−1 =x)
=
Pr(x′)P(Xi =x|Xi−1 =x′).
(15.16)
This condition is known as detailed balance, and a Markov chain that satisﬁes detailed
balance is said to be reversible. If a Markov chain is reversible, that is, satisﬁes (15.16),
then Pr(X) is guaranteed to be a stationary distribution for the chain. Note, however,
that reversibility is sufﬁcient but not necessary for ensuring that the Markov chain has a
particular stationary distribution (see Exercises 15.27 and 15.28).
We next provide a systematic method for constructing reversible Markov chains called
Gibbs sampling, and then show how it can be applied to Bayesian networks.
7 An irreducible Markov chain may or may not converge to its stationary distribution, yet Exercise 15.29 deﬁnes a
condition called aperiodicity, which ensures this convergence.

15.7 MARKOV CHAIN SIMULATION
403
15.7.2 Gibbs sampling
Given a distribution Pr(X), we can always construct a Markov chain that is guaranteed
to have Pr(X) as its stationary distribution as long as we adopt the following transition
matrix.
Deﬁnition 15.6. Given a distribution Pr(X), the corresponding Gibbs transition matrix
is deﬁned as follows, where m is the number of variables in X:
P(Xi =x′|Xi−1 =x)
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if x and x′ disagree on more than one variable
1
mPr(s′|x −S),
if x and x′ disagree on a single variable S,
which has value s′ in x′
1
m

S∈X
Pr(sx|x −S),
if x = x′ and sx is the value of variable S in x.
■
A Markov chain with this transition matrix is called a Gibbs chain.
We next provide some intuition about this transition matrix and how we can sample
from it efﬁciently, but let us ﬁrst stress the following important point. To deﬁne the
matrix, we only need to compute quantities of the form Pr(s|x −S) with respect to the
distribution Pr, where x is a complete variable instantiation. Moreover, we show later that
if the distribution Pr is induced by a Bayesian network, then this computation can be
performed efﬁciently using the chain rule of Bayesian networks.
For a concrete example of a Gibbs transition matrix, suppose that X = {A, B, C} and
x = a, b, ¯c. The following table depicts part of the Gibbs transition matrix for distribution
Pr(X):
x′
P(x′|x=a, b, ¯c)
Variables on which x′ and x disagree
a, b, c
Pr(c|a, b)/3
C
a, b, ¯c
(Pr(a|b, ¯c) + Pr(b|a, ¯c) + Pr(¯c|a, b))/3
a, ¯b, c
0
B, C
a, ¯b, ¯c
Pr(¯b|a, ¯c)/3
B
¯a, b, c
0
A, C
¯a, b, ¯c
Pr(¯a|b, ¯c)/3
A
¯a, ¯b, c
0
A, B, C
¯a, ¯b, ¯c
0
A, B
We now have the following result.
Theorem 15.16. The Gibbs transition matrix for distribution Pr(X) satisﬁes detailed
balance
Pr(x)P(Xi =x′|Xi−1 =x) = Pr(x′)P(Xi =x|Xi−1 =x′).
Moreover, if the distribution Pr(X) is strictly positive, the Gibbs chain is irreducible and,
hence, Pr(X) is its only stationary distribution.
■
If the distribution Pr(X) is not strictly positive, the Gibbs chain may or may not be
irreducible yet it maintains Pr(X) as a stationary distribution.

404
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Simulating a Gibbs chain
To simulate a Gibbs chain, we start with some initial state x1 that is sampled from the
initial distribution P(X1).8 To sample a state x2 from P(X2|x1), we perform the following
(see Exercise 15.31):
r Choose a variable S from X at random.
r Sample a state s from the distribution Pr(S|x1 −S).
r Set the state x2 to x1 −S, s.
We can then repeat the same process to sample a state x3 from P(X3|x2) and so on. We use
the term Gibbs sampler or Gibbs simulator to refer to any algorithm than can simulate a
Gibbs chain as discussed here. Note that by deﬁnition of the Gibbs transition matrix, any
consecutive states xi and xi+1 disagree on at most one variable.
Computing Gibbs estimates
Now that we have a Gibbs sampler for a distribution Pr(X), we can use it to estimate the
expectation of any function with respect to this distribution. The simplest example here
is estimating the expectation of the direct sampling function ˘α for some event α. That is,
given a sample x1, x2, . . . , xn simulated from the Markov chain, we simply estimate the
probability Pr(α) using the sample mean
Avn(˘α) = 1
n
n

i=1
˘α(xi).
We can also estimate the conditional probability Pr(α|e), where e is some evidence,
by observing that it corresponds to the expectation of the direct sampling function ˘α with
respect to the conditional distribution Pr(.|e). Hence, we can estimate Pr(α|e) using the
mean of a sample x1, x2, . . . , xn that is simulated using a Gibbs sampler for the conditional
distribution Pr(X|e) (X are the variables distinct from E). We can construct such a sampler
by passing the distribution Pr(X|e) to Deﬁnition 15.6, which leads to the following Gibbs
transition matrix:
P(Xi =x′|Xi−1 =x)
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if x and x′ disagree on more than one variable
1
mPr(s′|x −S, e),
if x and x′ disagree on a single variable S,
which has value s′ in x′
1
m

S∈X
Pr(sx|x −S, e), if x = x′ and sx is the value of variable S in x.
Again, this highlights the main advantage of Gibbs sampling as it allows us to compute
expectations with respect to a conditional distribution Pr(.|e) even when sampling from
this distribution may not be feasible computationally.
8 The choice of initial distribution does not affect the consistency of estimates computed by MCMC but may affect its
speed of convergence. Ideally, we want the initial distribution P(X1) to be the stationary distribution Pr(X). Note,
however, that sampling from the stationary distribution is usually hard, otherwise we would not be using MCMC
in the ﬁrst place. Yet since we will need to sample only once from this distribution, it may be justiﬁable to expend
some computational effort on this using, for example, rejection sampling as discussed in Exercise 15.14.

15.7 MARKOV CHAIN SIMULATION
405
Algorithm 48 GIBBS NEXT STATE(x, e, N)
input:
x:
instantiation
e:
evidence
N:
Bayesian network inducing distribution Pr(X, E), X ∩E = ∅
output: a state sampled from P(Xi|Xi−1 =x), where P(Xi|Xi−1) is the Gibbs transition
matrix for the distribution Pr(X|e)
main:
1: S←a variable chosen randomly from X
2: for each state s of variable S do
3:
←instantiation x −S, s, e
4:
u←value attained by parents of node S in instantiation 
5:
c1, . . . , ck←values attained by children of node S in instantiation 
6:
u1, . . . , uk←values attained by parents of nodes C1, . . . , Ck in instantiation 
7:
P(s)←θs|u
k
i=1 θci|ui
8: end for
9: η ←
s P(s) {normalizing constant}
10: P(s)←P(s)/η for each state s
11: s←a value for variable S sampled from the distribution P(S)
12: return instantiation x −S, s
15.7.3 A Gibbs sampler for Bayesian networks
To apply Gibbs sampling to a Bayesian network that contains evidence e, we need to
construct a Gibbs sampler for the distribution Pr(X|e), where X are the network variables
distinct from E. In particular, given an instantiation x, we need to sample a next state
from the distribution P(Xi|Xi−1 =x), where P(Xi|Xi−1) is the Gibbs transition matrix for
distribution Pr(X|e). Given our previous discussion, this amounts to choosing a variable
S at random from X, sampling a value s from the distribution Pr(S|x −S, e), and then
choosing x −S, s as the next state. Hence, all we need to show here is how to compute
the distribution Pr(S|x −S, e) so we can sample from it. This is given by Theorem 15.17.
Theorem 15.17. Given a Bayesian network that induces a distribution Pr(X, E), X ∩E =
∅, and given a variable S ∈X, we have
Pr(s|x −S, e) = η · θs|u
k
i=1
θci|ui,
where η is a normalizing constant, and
r U are the parents of S and u is their state in x −S, s, e;
r C1, . . . , Ck are the children of S and c1, . . . , ck are their states in x −S, s, e;
r U1, . . . , Uk are the parents of C1, . . . , Ck and u1, . . . , uk are their states in x −S, s, e.
■
Algorithm 48 provides pseudocode for a Gibbs sampler based on Theorem 15.17, which
is used by Algorithm 49 for estimating node marginals in a Bayesian network.

406
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Algorithm 49 GIBBS MCMC(e, n, N)
input:
e:
evidence
n:
sample size
N:
Bayesian network
output: estimate Pr(S|e) for each node S in the network, where S ̸∈E
main:
1: X←variables of network N that are distinct from E
2: P(s)←0 for each value s of variable S ∈X {estimate for Pr(S|e)}
3: x1←an instantiation of variables X {sampled from Pr(X|e) if possible}
4: increment P(s) for each variable S and its value s in instantiation x1
5: for i from 2 to n do
6:
xi←GIBBS NEXT STATE(xi−1, e, N)
7:
increment P(s) for each variable S and its value s in instantiation xi
8: end for
9: return P(s)/n for each variable S ∈X and value s of S
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
C
D
A
D
C
B
B
Figure 15.13: Simulating a Gibbs chain. Gray nodes have the value false and white nodes have the value
true.
Figure 15.13 depicts an example of applying Algorithm 49. What we have here is a
network with evidence E =e where our goal is to estimate the conditional probabilities for
variables A, B, C, and D given E =e. Gibbs sampling start with an initial instantiation of
network variables in which E = e. This initial instantiation is ¯a, b, c, ¯d, e in Figure 15.13.
Gibbs sampling then picks a variable, say, A and then samples a value of this variable

15.7 MARKOV CHAIN SIMULATION
407
from the distribution Pr(A|b, c, ¯d, e). According to Figure 15.13, the value a is sampled
in this case, leading to the next instantiation a, b, c, ¯d, e. Picking the variable B next
and sampling a value from Pr(B|a, c, ¯d, e) leads to the instantiation a, ¯b, c, ¯d, e, and so
on. Note that when picking variable D, the sampled value is the same as the current
one, leaving the instantiation without change. The sample corresponding to Figure 15.13
consists of the following instantiations:
Instantiation
Sampled variable
¯a, b, c, ¯d, e
a, b, c, ¯d, e
A
a, ¯b, c, ¯d, e
B
a, ¯b, ¯c, ¯d, e
C
a, ¯b, ¯c, ¯d, e
D
¯a, ¯b, ¯c, ¯d, e
A
¯a, b, ¯c, ¯d, e
B
¯a, b, ¯c, ¯d, e
C
¯a, b, ¯c, d, e
D
Estimating a probability of some event, say, C = ¯c using direct sampling is then a matter
of counting the number of instantiations in which the event is true. For example, given
the previous sample, the estimate for Pr(¯c|e) is 6/9 and the estimate for Pr(c|e) is 3/9.
Bibliographic remarks
For introductory references on expectations and estimation, see DeGroot [2002] and
Wasserman [2004], and on Markov chains, see Bertsekas and Tsitsiklis [2002]. For a
survey of Monte Carlo methods, see Neal [1993]. For a more advanced and comprehensive
coverage of these methods, see Robert and Casella [2004], Gilks et al. [1996], and
Rubinstein [1981], and for their application to dynamic models, see Doucet et al. [2001].
The simulation of Bayesian networks was introduced in Henrion [1988], where rejec-
tion sampling was also introduced for these networks under the name of logic sampling.
Likelihood weighting was introduced in Shachter and Peot [1989] and Fung and Chang
[1989], yet the literature contains many other sampling algorithms that can be under-
stood as applications of importance sampling to Bayesian networks (e.g., [Fung and
del Favero, 1994; Hernandez et al., 1998; Salmeron et al., 2000]). Some of the more
recent algorithms use belief propagation (see Chapter 14) for obtaining an importance
distribution [Yuan and Druzdzel, 2003; 2006; Gogate and Dechter, 2005], while others
resort to learning and adjusting the importance distribution throughout the inference pro-
cess [Shachter and Peot, 1989; Ortiz and Kaelbling, 2000; Cheng and Druzdzel, 2000;
Moral and Salmeron, 2003]. Some applications of Rao-Blackwell sampling are discussed
in Doucet et al. [2000] and Bidyuk and Dechter [2006]. The application of Gibbs sam-
pling to Bayesian networks was ﬁrst introduced in Pearl [1987] and then analyzed in
York [1992].
The complexity of approximating probabilistic inference is discussed in Dagum and
Luby [1993], where it is shown that approximating both conditional and unconditional
probabilities are NP-hard when zero probabilities (or probabilities arbitrarily close to
zero) are present in the network.

408
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
15.8 Exercises
15.1.
Let Na be the number of students in a class having age a:
Age a
Na
18
5
19
15
20
21
21
30
22
20
23
6
24
3
What is the expected age of a student chosen randomly from this class? Formulate this
problem by deﬁning a function f where the expected value of the function corresponds to
the answer. Compute the variance of the identiﬁed function.
15.2.
Show that the variance of function f (X) with respect to distribution Pr(X) satisﬁes
Va (f ) = Ex(f 2) −Ex(f )2.
15.3.
Let µ and σ 2 be the expectation and variance of a function and let v be one of its observed
values. Using Chebyshev’s inequality (Theorem 15.2), what is the value of ϵ that allows us
to state that µ lies in the interval (v −ϵ, v + ϵ) with conﬁdence ≥95%? Compare this with
the value of ϵ based on Theorem 15.3.
15.4.
Let f be a function with expectation µ and variance σ 2 = 4. Suppose we wish to estimate
the expectation µ using Monte Carlo simulation (Equation 15.5) and a sample of size n. How
large should n be if we want to guarantee that expectation µ falls in the interval (Avn(f ) −
1, Avn(f ) + 1) with conﬁdence ≥99%? Use Chebyshev’s inequality (Theorem 15.2).
15.5.
Prove the law of large numbers (Theorem 15.6).
15.6.
Consider the Bayesian network in Figure 15.1 and the parameter θ¯a representing the prob-
ability of A=false (i.e., it is not winter). For each of the following values of this parameter,
.01, .4, and .99, do the following:
(a) Compute the probability Pr(d, e): wet grass and slippery road.
(b) Estimate Pr(d, e) using direct sampling with sample sizes ranging from n = 100 to n =
15,000.
(c) Generate a plot with n on the x-axis and the exact value of Pr(d, e) and the estimate for
Pr(d, e) on the y-axis.
(d) Generate a plot with n on the x-axis and the exact variance of the estimate for Pr(d, e) and
the sample variance on the y-axis.
15.7.
Show that the sample variance of function f (X) and sample x1, . . . , xn can be computed
as follows:
S2
n(f ) =
1
n −1T,
where
T =
n

i=1
(f (xi) −Avn(f ))2 =
n

i=1
f (xi)2 −1
n

n

i=1
f (xi)
2
.
Note that the ﬁrst form of T given in this chapter suggests computation by a two-pass
algorithm: compute the mean in one pass and compute the differences squared in another
pass. The second form of T suggests computation by a one-pass algorithm that simply
accumulates the sums of f (xi)2 and f (xi).

15.8 EXERCISES
409
15.8.
Suppose we want to simulate a Bayesian network N that has been conditioned on evidence
e. Suppose further that we are given cluster marginals Pr(Ci|e) and separator marginals
Pr(Sij|e) computed by running the jointree algorithm on network N and evidence e. Give
an efﬁcient algorithm for simulating the network N conditioned on e given these cluster and
separator marginals. That is, show how we can efﬁciently generate a sequence of indepen-
dent network instantiations x1, . . . , xn where the probability of generating instantiation xi is
Pr(xi|e).
15.9.
Show the following for an instantiation x returned by Algorithm 44, SIMULATE BN:
(a) The partial instantiation x1, . . . , xk of x is generated with probability Pr(x1, . . . , xk), where Xi
is the variable at position i in the used order π.
(b) The value x assigned to variable X by instantiation x is generated with probability Pr(x).
(c) The instantiation c assigned to variables C ⊆X by x is generated with probability Pr(c).
Hint: You can use (a) to prove (b), and use (b) to prove (c).
15.10. Prove Theorem 15.9.
15.11. Given a Bayesian network over variables X and Y where X ∩Y = ∅and Y is a loop-cutset,
show how Pr(z|y) can be computed in time polynomial in the network size where Z ⊆X.
15.12. Given a Bayesian network over variables X and containing evidence e, show how we can
construct a Gibbs sampler for the distribution Pr(Y|e), Y ⊆X \ E, assuming that we can
compute Pr(y, e) for each instantiation y of variables Y. That is, we want a Gibbs sampler
that allows us to compute expectations with respect to the distribution Pr(Y|e).
15.13. Show that the variance of Theorem 15.9 ranges between zero and Pr(α)Pr(¬α). State a
condition under which the variance reduces to zero and a condition under which it reduces
to Pr(α)Pr(¬α).
15.14. Let Pr(X) be a distribution that is hard to sample from. Let Pr′(X) be another distribution that
is easy to sample from, where Pr(x) ≤c · Pr′(x) for all instantiations x and some constant
c > 0. The method of rejection sampling performs the following steps to generate a sample
of size n from distribution Pr(X):
Repeat n times:
1.
Sample an instantiation x from distribution Pr′(X).
2.
Accept x with probability Pr(x)/c · Pr′(x).
3.
If x is not accepted (i.e., rejected), go to Step 1.
We can show that the accepted instantiations represent a random sample from distribu-
tion Pr(X). Consider now the algorithm given in Section 15.5 for computing a conditional
probability Pr(α|β). Show how this algorithm can be formulated as an instance of rejection
sampling by specifying the corresponding distribution Pr′ and constant c.
15.15. Consider the bounds on the absolute and relative errors of direct sampling that we provided in
Sections 15.4.1 and 15.4.2 (based on Chebyshev’s inequality). Derive similar bounds based
on this inequality for the absolute and relative errors of (idealized) importance sampling
(Theorem 15.12). Can we use Hoeffding’s inequality for this purpose? Why?
15.16. Consider a simple Bayesian network of the form A →B, where A and B are binary variables.
Provide a closed form for the variance of likelihood weighting when estimating Pr(a). Give
a similar form for the estimate of Pr(b). Your forms must be expressed in terms of network
parameters and the probabilities of a and b.
15.17. Prove Theorem 15.14.
15.18. Consider a Bayesian network in which all leaf nodes are deterministic (their CPTs contain
only 0 and 1 entries) and let e be an instantiation of the leaf nodes. What is the variance of
likelihood weighting when estimating Pr(e)?

410
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
1S
2
S
3
S
1
O
2
O
3
O
1S
2
S
3
S
1
O
2
O
3
O
(a) HMM
(b) Likelihood weighting HMM
Figure 15.14: An HMM for three time steps and the corresponding likelihood weighting HMM given
evidence on nodes O1, O2, and O3.
15.19. Consider the functions ˘α and ˜α given in Deﬁnitions 15.1 and 15.3, respectively. Show that
Va (˜α) ≤Va (˘α) if *
Pr(x) ≥Pr(x) for all instantiations x consistent with α.
15.20. Show the following: If Pr(.|α) can be used as an importance distribution, then Pr(α) can be
computed exactly after generating a sample of size 1.
15.21. Consider a Bayesian network X1 →. . . →Xn and suppose that we want to estimate Pr(xn)
using likelihood weighting. Find a closed form for the variance of this method in terms of
the CPT for node Xn and the marginal distribution of node Xn−1. Identify conditions on the
network that lead to a zero variance.
15.22. Consider the HMM in Figure 15.14 and suppose that our goal is to estimate the probability
Pr(St|o1, . . . , ot) for each time t given evidence o1, . . . , ot. Suppose that we use likelihood
weighting for this purpose and let *
Pr(.) be the importance distribution. Show that for every
instantiation s1, . . . , st, o1, . . . , ot that satisﬁes the given evidence, we have
Pr(s1, . . . , st, o1, . . . , ot)
*
Pr(s1, . . . , st, o1, . . . , ot) = Pr(o1, . . . , ot|s1, . . . , st).
Describe an implementation of this algorithm whose space complexity is independent of
time t.
15.23. Show that Equations 15.11 and 15.12 are implied by Equation 15.10.
15.24. Prove Equations 15.13 and 15.14. That is, show that these are the estimates computed by
likelihood weighing when applied to the network fragment at time t > 1.
15.25. Write the pseudocode for applying particle ﬁltering to the class of DBNs discussed in Sec-
tion 15.6.2 and Figure 15.12.
15.26. Show that if a Markov chain satisﬁes the detailed balance property given in Equation 15.16,
then Pr(X) must be a stationary distribution for the chain.
15.27. Provide a Markov chain over three states that has a stationary distribution yet does not
satisfy the detailed balance property.
15.28. The Gibbs sampler can be implemented in two ways. In one case, the variable S whose state
will be sampled is chosen at random. In another case, a particular sequence for variables S
is predetermined and sampling is always performed according to that sequence. The second
method is known to result in an irreducible Markov chain that does not satisfy the detailed
balance property. Compare the performance of these methods empirically.
15.29. A Markov chain is said to be aperiodic9 if and only if there exists a speciﬁc time t > 1 such
that for every pair of states x and x′, we have P(Xt =x′|X1 =x) > 0. If a Markov chain is
aperiodic, it will also be irreducible and, hence, have a unique stationary distribution, say,
9 The literature on Markov chains contains multiple deﬁnitions of the notion of aperiodicity.

15.9 PROOFS
411
Pr(X).10 Moreover, if the chain is aperiodic, it will converge to its stationary distribution. That
is,
lim
t→∞P(Xt =x′|X1 =x) = Pr(x′)
for all states x and x′. Hence, when simulating the Markov chain, the simulated instantiations
are eventually sampled from the stationary distribution Pr(X) and become independent of
the initial state at time 1.
Consider now the Markov chain for a binary variable X with the transition matrix
P(x|¯x) = 1 and P(¯x|x) = 1; hence, P(¯x|¯x) = 0 and P(x|x) = 0. Is this chain aperiodic?
Is it irreducible? If it is, identify its unique stationary distribution. Will the chain converge to
any distribution?
15.30. Show that the Gibbs transition matrix given in Deﬁnition 15.6 satisﬁes the following condition:

x′
P(Xi =x′|Xi−1 =x) = 1.
15.31. Consider the Gibbs transition matrix P(Xi|Xi−1) given in Deﬁnition 15.6 and let x be a state
of variables X. Consider now the state x′ generated as follows:
r
Let S be a variable chosen randomly from X.
r
Let s be a state of variable S sampled from Pr(S|X −S).
r
Set state x′ to x −S, s.
Show that the state x′ is generated with probability P(Xi =x′|Xi−1 =x).
15.9 Proofs
PROOF OF THEOREM 15.1. To prove that Algorithm 44 generates an instantiation
x1, . . . , xn with probability Pr(x1, . . . , xn), we need to recall the chain rule for Bayesian
networks. In particular, let π = X1, . . . , Xn be a total ordering of network variables as
given in Algorithm 44 and let x1, . . . , xn be a corresponding variable instantiation. If ui
denote the instantiation of Xi’s parents in x1, . . . , xn, then
Pr(x1, . . . , xn) = Pr(xn|xn−1, . . . , x1)Pr(xn−1|xn−2, . . . , x1) . . . Pr(x1)
= Pr(xn|un)Pr(xn−1|un−1) . . . Pr(x1)
=
n

i=1
Pr(xi|ui).
Note that Algorithm 44 iterates over variables Xi according to order π, sampling a value xi
for each variable Xi from the distribution Pr(Xi|ui). Hence, the probability of generating
the instantiation x1, . . . , xn is simply the product of probabilities used for generating the
individual variable values, which equals Pr(x1, . . . , xn) as given here.
■
PROOF OF THEOREM 15.2. This is a classical result from statistics that follows imme-
diately from another classical result known as Markov’s inequality. For a proof of both
inequalities, see DeGroot [2002].
■
PROOF OF THEOREM 15.3. Classical result; see DeGroot [2002] and Wasserman
[2004].
■
10 Note the subtle difference between aperiodicity and irreducibility, where the latter states that for every pair of states
x and x′, there exists a speciﬁc time t > 1 such that P(Xt =x′|X1 =x) > 0. As deﬁned here, periodicity implies
irreducibility but the converse is not true.

412
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
PROOF OF THEOREM 15.4. For the expectation, we have
Ex(˘α) =

x|=α
˘α(x)Pr(x) +

x̸|=α
˘α(x)Pr(x)
=

x|=α
(1)Pr(x) +

x̸|=α
(0)Pr(x)
=

x|=α
Pr(x)
= Pr(α).
For the variance, we have
Va (˘α) =

x
(˘α(x) −Pr(α))2Pr(x)
=

x|=α
(˘α(x) −Pr(α))2Pr(x) +

x̸|=α
(˘α(x) −Pr(α))2Pr(x)
=

x|=α
(1 −Pr(α))2Pr(x) +

x̸|=α
(0 −Pr(α))2Pr(x)
= (1 −Pr(α))2Pr(α) + Pr(α)2Pr(¬α)
= (1 −Pr(α))2Pr(α) + Pr(α)2(1 −Pr(α))
= Pr(α) −Pr(α)2
= Pr(α)Pr(¬α).
■
PROOF OF THEOREM 15.5. This is a classical result from statistics; see DeGroot
[2002].
■
PROOF OF THEOREM 15.6. Left to Exercise 15.5.
■
PROOF OF THEOREM 15.7. This is a classical result from statistics; see DeGroot
[2002]. However, we point out that the more formal statement of this theorem is that
√n(Avn(f ) −µ) converges in distribution to a Normal with mean 0 and variance σ 2.
That is, for all t, we have
lim
n→∞P(√n(Avn(f ) −µ) ≤t) = (t),
where (t) is the CDF for a Normal distribution with mean 0 and variance σ 2.
■
PROOF OF THEOREM 15.8. This theorem is discussed in Wasserman [2004].
■
PROOF OF THEOREM 15.9. Left to Exercise 15.10.
■
PROOF OF THEOREM 15.10. The more formal statement of this theorem is as follows.
As the sample size n tends to inﬁnity, the distribution of √n(Avn( ˘γ )/Avn( ˘β) −Pr(α|β))
converges in distribution to a Normal with mean 0 and variance
Pr(α|β)Pr(¬α|β)
Pr(β)
.
Hence, the estimate Avn( ˘γ )/Avn( ˘β) is asymptotically Normal. Approximate normality
follows from the robustness of the Central Limit Theorem.

15.9 PROOFS
413
Theorem 15.10 follows from the multivariate Delta method; see for example Wasserman
[2004]. Roughly speaking, this method states that if a set of estimates are approximately
Normal, then a function of these estimates is also approximately Normal.
Let X = ˘γ and Y = ˘β be two direct sampling functions, µX and µY be their means,
σ 2X and σ 2Y be their variances, σXY be their covariance, and let  be the variance-
covariance matrix
 =
 σ 2X
σXY
σXY
σ 2Y
	
.
The multivariate Central Limit Theorem [Wasserman, 2004] shows that as n tends to
inﬁnity, the vector √n(A −µ), where
A =

Avn(X)
Avn(Y)

and
µ =
 µX
µY
	
,
tends to a multivariate Normal distribution with mean 0 and variance matrix . This result
allows us to invoke the Delta method as follows.
Consider the function g(X, Y) = X/Y and deﬁne
∇g(X,Y) =

∂g
∂X
∂g
∂Y

.
Let ∇µ denote ∇g(X,Y) evaluated at mean vector µ and assume that the elements of ∇µ are
nonzero. The Delta method then says that as n tends to inﬁnity, the distribution of
√n

g(Avn(X), Avn(Y)) −g(µX, µY)

= √n
Avn(X)
Avn(Y) −µX
µY
	
tends to a Normal distribution with mean 0 and variance ∇T
µ∇µ. This implies that
the distribution of estimate Avn(X)/Avn(Y) is approximately Normal with mean µX/µY
and variance ∇T
µ∇µ/n. All we need now is to evaluate the mean µX/µY and variance
∇T
µ∇µ/n. We have
∂
∂X
X
Y
	
= 1
Y
∂
∂Y
X
Y
	
= −X
Y 2
µX = Pr(α ∧β)
µY = Pr(β)
σ 2
X = Pr(α ∧β)(1 −Pr(α ∧β))
σ 2
Y = Pr(β)(1 −Pr(β))
σXY = Pr(α ∧β) −Pr(α ∧β)Pr(β).
The last step follows since σXY = Ex(XY) −Ex(X)Ex(Y), and XY = ˘γ ˘β = ˘γ since
γ = α ∧β. We then have
∇g(X,Y) =

1
Y
−X
Y 2

∇µ =

1
µY
−µX
µ2
Y


414
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Hence, the mean is
µX
µY
= Pr(α, β)
Pr(β)
= Pr(α|β).
Moreover, the variance is
∇T
µ ∇µ =
 1
µY
−µX
µ2
Y
	 
σ 2X
σXY
σXY
σ 2Y
 
1
µY
−µX
µ2
Y

,
which evaluates to
∇T
µ ∇µ = σ 2X
µ2
Y
−2µXσXY
µ3
Y
+ µ2
Xσ 2Y
µ4
Y
.
Substituting and simplifying leads to
∇T
µ ∇µ
n
= Pr(α|β)Pr(¬α|β)
nPr(β)
.
■
PROOF OF THEOREM 15.11. We did not deﬁne Pr(x)/*
Pr(x) when *
Pr(x) = 0. However,
since Pr(x) = 0 in this case, the following derivations hold regardless of how this is
deﬁned.
We have
Ex(˜α) =

x|=α
Pr(x)
*
Pr(x)
	
*
Pr(x) +

x̸|=α
(0)*
Pr(x)
=

x|=α
Pr(x)
= Pr(α).
Using Exercise 15.2, we have
Va (˜α) =

x
˜α(x)2 · *
Pr(x) −Pr(α)2
=

x|=α
˜α(x)2 · *
Pr(x) +

x̸|=α
˜α(x)2 · *
Pr(x) −Pr(α)2
=

x|=α
Pr(x)2
*
Pr(x)2 · *
Pr(x) +

x̸|=α
0 · *
Pr(x) −Pr(α)2
=

x|=α
Pr(x)2
*
Pr(x) −Pr(α)2.
■
PROOF OF THEOREM 15.12. This proof follows immediately from Theorem 15.11
and observing that Pr(α)/*
Pr(α) = Pr(x)/*
Pr(x) for all instantiations x that are consistent
with α.
■
PROOF OF THEOREM 15.13. This proof parallels that for Theorem 15.10 using the
Delta method, except that we now deﬁne X = ˜γ and Y = ˜β, where γ = α ∧β. We then

15.9 PROOFS
415
get
µX = Pr(α ∧β)
µY = Pr(β)
σ 2
X = Pr(α ∧β)2 −*
Pr(α ∧β)Pr(α ∧β)2
*
Pr(α ∧β)
σ 2
Y = Pr(β)2 −*
Pr(β)Pr(β)2
*
Pr(β)
σXY = Pr(α ∧β)Pr(β)
*
Pr(β)
−Pr(α ∧β)*
Pr(β)Pr(β)
*
Pr(β)
.
These forms can be simpliﬁed further. However, using these speciﬁc forms makes further
substitutions easier to manipulate and simplify.
Substituting and simplifying, we get
∇T
µ ∇µ
n
= Pr(α|β)Pr(¬α|β)
n*
Pr(β)
.
■
PROOF OF THEOREM 15.14. Left to Exercise 15.17.
■
PROOF OF THEOREM 15.15. See Neal [2004] for a discussion of this result and some
relevant pointers.
■
PROOF OF THEOREM 15.16. The detailed balance property holds immediately if x =
x′. If x and x′ disagree on more than one variable, both sides evaluate to zero. Suppose
now that x and x′ disagree on a single variable S, which has value s in x and value s′ in
x′. We then have
Pr(x)P(x′|x) ?
= Pr(x′)P(x|x′)
Pr(x)P(s′|x −S)/m ?
= Pr(x′)P(s|x′ −S)/m
Pr(x)
Pr(x′)
mPr(x −S) = Pr(x′)
Pr(x)
mPr(x′ −S).
The last step follows since x −S is the same as x′ −S.
When Pr(X) is strictly positive, it follows immediately from the deﬁnition of a Gibbs
matrix that any two states of X can be connected by a sequence of nonzero transitions.
■
PROOF OF THEOREM 15.17. Let Z1, . . . , Zm be all variables other than S and its chil-
dren C1, . . . , Ck, and let V1, . . . , Vm be their corresponding parents. Using the chain rule
of Bayesian networks, we have
Pr(s, x −S, e) = θs|u
k
i=1
θci|ui
m

i=1
θzi|vi,
where each zi, vi is compatible with instantiation s, x −S, e. Note that the variable S
appears in exactly k + 1 CPTs: the one for S, θs|u and the ones for its k children, θci|ui.
Hence, the product m
i=1 θzi|vi does not depend on the state s of variable S and we have
Pr(s, x −S, e)
∝
θs|u
k
i=1
θci|ui.

416
APPROXIMATE INFERENCE BY STOCHASTIC SAMPLING
Note also that
Pr(x −S, e) =

s⋆
Pr(s⋆, x −S, e)
does not depend on s. Hence,
Pr(s|x −S, e) = Pr(s, x −S, e)
Pr(x −S, e)
∝θs|u
k
i=1
θci|ui
= η · θs|u
k
i=1
θci|ui.
■

16
Sensitivity Analysis
We consider in this chapter the relationship between the values of parameters that quantify
a Bayesian network and the values of probabilistic queries applied to these networks. In
particular, we consider the impact of parameter changes on query values, and the amount
of parameter change needed to enforce some constraints on these values.
16.1 Introduction
Consider a laboratory that administers three tests for detecting pregnancy: a blood test, a
urine test, and a scanning test. Assume also that these tests relate to the state of pregnancy
as given by the network of Figure 16.1 (we treated this network in Chapter 5). According
to this network, the prior probability of pregnancy is 87% after an artiﬁcial insemination
procedure. Moreover, the posterior probability of pregnancy given three negative tests
is 10.21%. Suppose now that this level of accuracy is not acceptable: the laboratory is
interested in improving the tests so the posterior probability is no greater than 5% given
three negative tests. The problem now becomes one of ﬁnding a certain set of network
parameters (corresponding to the tests’ false positive and negative rates) that guarantee
the required accuracy. This is a classic problem of sensitivity analysis that we address
in Section 16.3 as it is concerned with controlling network parameters to enforce some
constraints on the queries of interest.
Assume now that we replace one of the tests with a more accurate one, leading to a
new Bayesian network that results from updating the parameters corresponding to that
test. We would now like to know the impact of this parameter change on other queries
of interest, for example, the probability of pregnancy given three positive tests. We can
solve this problem by simply recomputing these queries to ﬁnd out their new values but
this approach could be infeasible if we are interested in a large number of queries. As
we see in this chapter, we can apply techniques from sensitivity analysis to efﬁciently
obtain a bound on the possible changes in query results by considering only the changes
applied to network parameters. This part of sensitivity analysis is then concerned with the
robustness of queries against parameter changes and is the subject of the next section.
16.2 Query robustness
We start in this section by addressing the problem of query robustness, which can be
approached at two different levels depending on whether we are interested in network-
independent or network-speciﬁc results. In network-independent sensitivity analysis, we
obtain robustness results in the form of bounds on query results yet these bounds can be
very efﬁciently computed. In network-speciﬁc sensitivity analysis, we can characterize
precisely the relationship between a particular query and a network parameter by explicitly
constructing the function that governs this relationship. However, the construction of this
417

418
SENSITIVITY ANALYSIS
Pregnant?
(P)
Scanning Test
(S)
Urine Test
(U)
Blood Test
(B)
Progesterone Level
(L)
P
P
yes
.87
P
S
S|P
yes
−ve
.10
no
+ve
.01
P
L
L|P
yes
undetectable
.10
no
detectable
.01
L
B
B|L
detectable
−ve
.30
undetectable
+ve
.10
L
U
U|L
detectable
−ve
.20
undetectable
+ve
.10
Figure 16.1: A Bayesian network for detecting pregnancy based on three tests. Redundant CPT rows
have been omitted.
function generally requires inference and is therefore more costly than the bounds obtained
by network-independent sensitivity analysis. We consider both approaches next.
16.2.1 Network-independent robustness
Assume that we are given a Bayesian network N0 and we change some network parameters
to obtain a new network N. Consider now a general query of the form α|β that can have
different probabilities, Pr0(α|β) and Pr(α|β), with respect to the two networks N0 and N.
Our goal in this section is to try to characterize the change in the probability of query α|β
as a result of the parameter change. Our ﬁrst approach for this is based on computing a
distance measure between the two distributions Pr0 and Pr and then using the value of this
distance to bound the new probability Pr(α|β). We ﬁrst introduce this distance measure
and then discuss how to compute it and how to use it for obtaining the sought bounds.
Deﬁnition 16.1 (CD distance). Let Pr0(X) and Pr(X) be two probability distributions
and deﬁne the measure D(Pr0, Pr) as
D(Pr0, Pr)
def= ln max
x
Pr(x)
Pr0(x) −ln min
x
Pr(x)
Pr0(x),
where 0/0
def= 1 and ∞/∞
def= 1.
■
Table 16.1 depicts an example of computing this measure for two distributions.
This measure satisﬁes the three properties of distance and is therefore a distance
measure. In particular, if Pr0, Pr, and Pr′ are three probability distributions over the same
set of variables, we have:
r Positiveness: D(Pr0, Pr) ≥0 and D(Pr0, Pr) = 0 iff Pr0 = Pr
r Symmetry: D(Pr0, Pr) = D(Pr, Pr0)
r Triangle Inequality: D(Pr0, Pr) + D(Pr, Pr′) ≥D(Pr0, Pr′).

16.2 QUERY ROBUSTNESS
419
Table 16.1: Two distributions with a CD distance equal to 1.61 = ln 2 −ln .4.
A
B
C
Pr(a, b, c)
Pr0(a, b, c)
Pr0(a, b, c)/Pr(a, b, c)
true
true
true
.10
.20
2.00
true
true
false
.20
.30
1.50
true
false
true
.25
.10
.40
true
false
false
.05
.05
1.00
false
true
true
.05
.10
2.00
false
true
false
.10
.05
.50
false
false
true
.10
.10
1.00
false
false
false
.15
.10
.67
Our interest in the CD distance stems from two reasons. First, it can be easily computed
for distributions that correspond to very similar Bayesian networks, that is, networks that
result from a local perturbation to one another. Second, it allows us to bound query results
computed with respect to one distribution in terms of query results computed with respect
to the other.
Theorem 16.1. Let Pr0 and Pr be two probability distributions over the same set of
variables and let α and β be arbitrary events. Given the distance measure D(Pr0, Pr)
between Pr0 and Pr, we have the following tight bound:
e−D(Pr0,Pr) ≤O(α|β)
O0(α|β) ≤eD(Pr0,Pr),
where O0(α|β) and O(α|β) are the odds of α|β under distributions Pr0 and Pr, res-
pectively.
■
We can express the bound given by Theorem 16.1 in two other useful forms. First, we
can use logarithms:
ln O(α|β) −ln O0(α|β)
 ≤D(Pr0, Pr).
(16.1)
Second, we can use probabilities instead of odds to express the bound:
e−dp

e−d −1

p + 1 ≤Pr(α|β) ≤
edp

ed −1

p + 1,
(16.2)
where p = Pr0(α|β) and d = D(Pr0, Pr). Figure 16.2 plots the bounds on Pr(α|β) as a
function of the initial probability p for several values of the CD distance d. As is clear
from this ﬁgure, the smaller the CD distance between two distributions, the tighter the
bounds we can obtain. Moreover, for a given value d of the CD distance the bounds get
tighter as the initial probability p becomes more extreme (tends to 0 or 1).
Before we provide concrete examples of how the CD distance can be used to address
the query robustness problem of sensitivity analysis, we discuss a condition under which
can efﬁciently compute the distance.
Theorem 16.2. Let N0 and N be Bayesian networks, where network N is obtained
from N0 by changing the conditional probability distribution of variable X given parent
instantiation u from 0
X|u to X|u, that is, changing parameter value θ0
x|u to some new

420
SENSITIVITY ANALYSIS
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
bounds on Pr(α|β)
probability p
distance d = .1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
bounds on Pr(α|β)
probability p
distance d = .5
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
bounds on Pr(α|β)
probability p
distance d = 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
bounds on Pr(α|β)
probability p
distance d = 2
Figure 16.2: Plotting the CD distance bounds for several values of the distance.
value θx|u for every x. Let Pr0 and Pr be the distributions induced by networks N0 and N,
respectively. If Pr0(u) > 0, then
D(Pr0, Pr) = D(0
X|u, X|u)
= ln max
x
θx|u
θ0
x|u
−ln min
x
θx|u
θ0
x|u
.
(16.3)
Moreover, if Pr0(u) = 0, then D(Pr0, Pr) = 0.
■
Theorem 16.2 shows that the CD distance between the global distributions induced by
networks N0 and N is exactly the CD distance between the local conditional distributions
0
X|u and X|u (assuming all other parameters in N0 and N are the same and that u has
a nonzero probability). This theorem is important practically as it identiﬁes a condition
under which the bounds given by Theorem 16.1 can be computed efﬁciently.
To consider a concrete example, let us examine the network in Figure 16.3 under the ev-
idence e = ¬S, R, that is, there is no smoke but people are reported to be leaving. The pro-
bability of ﬁre in this case is Pr0(F|¬S, R) = .029. Suppose now that we change the CPT
for variable Smoke as follows:
Fire (F)
Smoke (S)
0
S|F
true
true
.90
true
false
.10
false
true
.01
false
false
.99
Fire (F)
Smoke (S)
S|F
true
true
.92
true
false
.08
false
true
.01
false
false
.99

16.2 QUERY ROBUSTNESS
421
Tampering (T)
Report (R)
Leaving (L)
Alarm (A)
Fire (F)
Smoke (S)
Fire (F)
F
true
.01
Tampering (T )
T
true
.02
Fire
Smoke (S)
S|F
true
true
.9
false
true
.01
Fire
Tampering
Alarm (A)
A|F,T
true
true
true
.5
true
false
true
.99
false
true
true
.85
false
false
true
.0001
Alarm
Leaving (L)
L|A
true
true
.88
false
true
.001
Leaving
Report (R)
R|L
true
true
.75
false
true
.01
Figure 16.3: A Bayesian network. Redundant CPT rows have been omitted.
According to Theorem 16.2, the CD distance between the initial and new distributions is
then equal to the CD distance between the initial and new CPTs for Smoke:
D(Pr0, Pr) = D(0
S|F, S|F) = ln .10
.08 −ln .90
.92 = .245.
Using (16.2) with d = .245 and p = .029, we get the following bound on the new proba-
bility for ﬁre:
.023 ≤Pr(F|¬S, R) ≤.037.
The exact new probability of ﬁre is Pr(F|¬S, R) = .024 in this case.
For another example, consider the impact of changing the CPT for Tampering as
follows:
Tampering (T )
0
T
true
.02
false
.98
Tampering (T )
T
true
.036
false
.964
The CD distance between these CPTs for Tampering is
D(Pr0, Pr) = D(0
T , T ) = ln .036
.02 −ln .964
.98 = .604.

422
SENSITIVITY ANALYSIS
Using (16.2) with d = .604 and p = .029, we get the following bound on the new proba-
bility for ﬁre:
.016 ≤Pr(F|¬S, R) ≤.052.
The exact new probability of ﬁre is Pr(F|¬S, R) = .021 in this case.
Co-varying parameters
Suppose that X is a multivalued variable with initial CPT 0
X|U. Suppose further that we
wish to change a particular parameter value θ0
x|u to the new value θx|u. Since 
x θx|u = 1,
we cannot change the value of parameter θ0
x|u without changing the values of its co-varying
parameters θ0
x⋆|u, x⋆̸= x. There are many ways in which we can change these co-varying
parameters, but a common technique known as the proportional scheme is to change the
values of co-varying parameters while maintaining their relative ratios.
As an example of this scheme, consider the following distribution for variable X:
θ0
x1|u
θ0
x2|u
θ0
x3|u
.6
.3
.1
and suppose that we wish to change the ﬁrst parameter value from .6 to .8. We know in this
case that a total change of −.2 must be applied to co-varying parameters. The proportional
scheme will distribute this amount among co-varying parameters while preserving their
ratios, leading to
θx1|u
θx2|u
θx3|u
.8
.15
.05
Note here that θx3|u/θx2|u = θ0
x3|u/θ0
x2|u. From now on, we assume that co-varying param-
eters are changed according to this scheme, which is deﬁned formally next.
Deﬁnition 16.2 (Single Parameter Change). Let 0
X|U be an initial CPT for variable X
and suppose we want to change a single parameter value θ0
x|u to the new value θx|u. The
co-varying parameters of θ0
x|u are changed simultaneously as follows, for all x⋆̸= x:
θx⋆|u
def= ρ(x, x⋆, u)(1 −θx|u)
ρ(x, x⋆, u)
def=
⎧
⎨
⎩
θ0
x⋆|u
1−θ0
x|u
if θ0
x|u ̸= 1
1
|X|−1
if θ0
x|u = 1,
where |X| is the number of values that variable X has.
■
Note that when the changing parameter has an initial value of 1, all its co-varying parame-
ters have initial values of 0. In this case, the proportional scheme distributes the parameter
change equally among co-varying parameters.
More on single parameter changes
When changing a single parameter as given by Deﬁnition 16.2, Theorem 16.2 takes a
more speciﬁc and intuitive form.

16.2 QUERY ROBUSTNESS
423
Theorem 16.3. Consider Theorem 16.2 and suppose that the CPT X|U results from
changing a single parameter θ0
x|u in the initial CPT 0
X|U. Equation 16.3 then reduces to
the simpler form
D(Pr0, Pr) =
ln O(x|u) −ln O0(x|u)
 .
■
That is, the CD distance between the old and new distributions is nothing but the
absolute change in the log-odds of the changed parameter. Combining this result with
(16.1), we get
ln O(α|β) −ln O0(α|β)
 ≤
ln O(x|u) −ln O0(x|u)
 .
(16.4)
The new inequality suggests a particular method for measuring the amount of change
that a parameter or query undergoes: the absolute change in log-odds. Moreover, the
inequality shows that if the change is measured this way, the amount of change that a
query undergoes can be no more than the amount of the corresponding parameter change.
For more insight into this method of measuring change, consider two parameter
changes, one from .1 to .15, and another from .4 to .45. Both of these changes amount to
the same absolute change of .05, however, the ﬁrst change amounts to a log-odds change
of .463 and the second change amounts to a log-odds change of .205. Therefore, the
second change is smaller according to the log-odds measure even though it is an equal
absolute change. Two parameter changes that amount to the same relative change can
also lead to different amounts of log-odds change. For example, consider two parameter
changes, one from .1 to .2, and another from .2 to .4. Both these changes double the initial
parameter value. However, the ﬁrst change amounts to a log-odds change of .811, while
the second change amounts to a log-odds change of .981.
16.2.2 Network-speciﬁc robustness
We approached the robustness problem in the previous section from a network-
independent viewpoint, which allowed us to provide some general guarantees on the
amount of change that a query can undergo as a result of a parameter change. The ability
to provide these guarantees independently of the given network has two implications.
First, the guarantees themselves can be computed efﬁciently as they do not require infer-
ence. Second, the guarantees have the form of bounds on the query and these bounds get
looser as the parameter change increases.
We can provide stronger guarantees on query changes if we take into consideration
the speciﬁc network and query at hand. In particular, we can construct a speciﬁc function
called the sensitivity function that relates any particular parameter to any particular query,
allowing us to efﬁciently compute the exact effect that a parameter has on the query.
However, the price we have to pay computationally is in constructing the sensitivity
function itself as that requires performing inference on the given network.
The construction of a sensitivity function is based on the following result.
Theorem 16.4. Let N0 and N be two Bayesian networks where N is obtained from N0
by changing a single parameter value θ0
x|u to the new value θx|u. Let Pr0 and Pr be the two
distributions induced by these networks. The value of a query Pr(α) can be expressed in
terms of the original network N0 and the new parameter value θx|u as
Pr(α) = µα
x|u · θx|u + να
x|u,

424
SENSITIVITY ANALYSIS
where
µα
x|u = ∂Pr(α)
∂θx|u
−

x⋆̸=x
ρ(x, x⋆, u)∂Pr(α)
∂θx⋆|u
,
(16.5)
να
x|u =

x⋆̸=x
ρ(x, x⋆, u)∂Pr(α)
∂θx⋆|u
+

u⋆̸=u
Pr0(u⋆, α).
(16.6)
Here ρ(x, x⋆, u) is as given by Deﬁnition 16.2.
■
According to Theorem 16.4, the value of query Pr(α) is a linear function of the new
parameter value θx|u. A key observation about Theorem 16.4 is that the quantities needed
to compute the constants µα
x|u and να
x|u can be obtained by performing inference on the
initial network N0. We discussed the computation of derivatives in Chapter 12 but we
recall here that1
∂Pr(α)
∂θx|u
= Pr(α, x, u)
θx|u
, when θx|u ̸= 0.
Therefore, if we have an algorithm that can compute in O(f ) time the probability of
α, x, u, for a given α and all parameters θx|u, then we can also compute in O(f ) time
the constants µα
x|u and να
x|u for a given α and all parameters θx|u. This allows us to
construct the sensitivity functions for a given query with respect to all network parameters
simultaneously in O(f ) time.
For a conditional query Pr(α|β), we can construct the sensitivity function as
Pr(α|β) = Pr(α, β)
Pr(β)
=
µα,β
x|u · θx|u + να,β
x|u
µβ
x|u · θx|u + νβ
x|u
.
As an example, consider again the Bayesian network in Figure 16.3. The sensitivity
function for the query Pr(F|S, ¬R) with respect to the parameter θS|¬F is given by
Pr(F|S, ¬R) =
.003165
.968357 · θS|¬F + .003165,
which is plotted in Figure 16.4. We see that at the current parameter value of .01, the
query value is .246 but if we decrease the parameter value to .00327, the query value
increases to .500. This example shows that once a sensitivity function is constructed,
we can immediately compute query changes that result from parameter changes without
the need to perform further inference. Note, however, that if we were to use the bound
given by (16.2), we would conclude that by changing the parameter from .01 to .00327
the new query value would be within the bounds of .096 and .502. Although this bound
is relatively loose, it can be obtained without the computational overhead associated with
constructing a sensitivity function.
16.2.3 Robustness of MPEs
In this section, we consider the robustness of MPE instantiations with respect to parameter
changes. That is, given some evidence e and a corresponding set of MPE instantiations
1 Note also that ∂Pr(α)/∂θx|u = Pr0(α, x, u)/θ0
x|u when θ0
x|u ̸= 0.

16.2 QUERY ROBUSTNESS
425
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0
 0.2
 0.4
 0.6
 0.8
 1
probability Pr(F|S,¬R)
parameter θS|¬F
sensitivity function
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.005
 0.01
 0.015
 0.02
probability Pr(F|S,¬R)
parameter θS|¬F
sensitivity function
Figure 16.4: A sensitivity function plotted for two different ranges.
x1, . . . , xn, our goal in this section is to identify parameter changes that are guaranteed
to preserve these MPE instantiations. Note that a parameter change may increase or
decrease the probability of these instantiations without necessarily changing their identity
(i.e., adding or removing instantiations to the set).
Consider a parameter θx|u that we plan to change and assume for now that variable X
is binary. Our ﬁrst observation is that the MPE probability can be decomposed as2
MPEP (e) = max(MPEP (xu, e), MPEP (¯xu, e), MPEP (¬u, e)).
That is, we can partition network instantiations that are compatible with evidence e into
three groups depending on whether they are compatible with xu, ¯xu, or ¬u. We can
then compute the MPE probability for each category and take the maximum of all three
probabilities. This decomposition is useful as it reveals the following. When changing the
value of parameter θx|u, and correspondingly θ¯x|u, only MPEP (xu, e) and MPEP (¯xu, e)
2 Let u1, . . . , un be the instantiations of variables U that are distinct from u. The notation ¬u is then a shorthand for
(U=u1) ∨. . . ∨(U=un).

426
SENSITIVITY ANALYSIS
change while MPEP (¬u, e) remains unchanged as it does not depend on parameters θx|u
and θ¯x|u. More speciﬁcally, we have
MPEP (xu, e) = r(xu, e) · θx|u
MPEP (¯xu, e) = r(¯xu, e) · θ¯x|u,
(16.7)
where r(xu, e) and r(¯xu, e) are the partial derivatives given in Chapter 12 (see (12.10)),
which are independent of parameters θx|u and θ¯x|u. Therefore, if we have these derivatives
we can immediately predict the amount of change that MPEP (xu, e) undergoes as a result
of changing θx|u. Similarly we can predict the change in MPEP (¯xu, e) as a result of chang-
ing θ¯x|u.
Suppose now that we have
MPEP (xu, e) = MPEP (¯xu, e) = MPEP (¬u, e).
This implies that some MPE instantiations are compatible with xu, some are compatible
with ¯xu, and some with ¬u. If we now increase the value of parameter θx|u, the value of
parameter θ¯x|u will decrease. This means that MPEP (xu, e) will increase, MPEP (¯xu, e)
will decrease, and MPEP (¬u, e) will stay the same, leading to
MPEP (xu, e) > MPEP (¯xu, e)
and
MPEP (xu, e) > MPEP (¬u, e).
Given these changes, only MPE instantiations that are compatible with xu will survive:
their identity stays the same even though their probability increases. On the other hand,
MPE instantiations that are compatible with either ¯xu or ¬u disappear as their probability
is dominated by those compatible with xu.
Together with (16.7), these observations lead to the following conditions which are
necessary and sufﬁcient for preserving MPE instantiations when changing parameters
θx|u and θ¯x|u:
r If an MPE instantiation is compatible with xu, it is preserved as long as the following
inequalities hold:
r(e, xu) · θx|u ≥r(e, ¯xu) · θ¯x|u
r(e, xu) · θx|u ≥MPEP (e, ¬u).
r If an MPE instantiation is compatible with ¯xu, it is preserved as long as the following
inequalities hold:
r(e, ¯xu) · θ¯x|u ≥r(e, xu) · θx|u
r(e, ¯xu) · θ¯x|u ≥MPEP (e, ¬u).
r If an MPE instantiation is compatible with ¬u, it is preserved as long as the following
inequalities hold:
MPEP (e, ¬u) ≥r(e, xu) · θx|u
MPEP (e, ¬u) ≥r(e, ¯xu) · θ¯x|u.
We note here that we can easily decide whether an MPE instantiation is compatible with
either xu, ¯xu, or ¬u since an MPE instantiation is a complete variable instantiation.

16.3 QUERY CONTROL
427
Moreover, the constants r(xu, e) and r(¯xu, e) can be computed as given in Chapter 12
(see (12.10)), while MPEP (e, ¬u) can be computed as
MPEP (e, ¬u) = max
u⋆̸=u MPEP (u⋆, e)
= max
x,u⋆̸=u MPEP (xu⋆, e)
= max
x,u⋆̸=u r(xu⋆, e) · θx|u⋆.
(16.8)
For a simple example, consider the Bayesian network A →B with the following CPTs:
A
A
a
.5
¯a
.5
A
B
B|A
a
b
.2
a
¯b
.8
¯a
b
.6
¯a
¯b
.4
Assuming we have no evidence, e = true, there is one MPE instantiation in this case, a ¯b,
which has probability MPEP (e) = .4. Suppose now that we plan to change parameters
θb|a and θ¯b|a and want to compute the amount of change guaranteed to preserve the MPE
instantiation. We have r(ba, e) = r(¯ba, e) = r(b¯a, e) = r(¯b¯a, e) = .5.3 Moreover, using
(16.8), we have MPEP (¯a, e) = .3 and MPEP (a, e) = .4. We can now easily compute the
amount of parameter change that is guaranteed to preserve the MPE instantiation. The
conditions we must satisfy are
r(¯ba, e) · θ¯b|a ≥r(ba, e) · θb|a
r(¯ba, e) · θ¯b|a ≥MPEP (¯a, e).
This leads to θ¯b|a ≥θb|a and θ¯b|a ≥.6. Therefore, the current MPE instantiation is pre-
served as long as θ¯b|a ≥.6, which has a current value of .8.
We ﬁnally point out that these robustness equations can be extended to multivalued
variables as follows. If variable X has values x1, . . . , xm, where m ≥2, then each of the
conditions showed previously consists of m inequalities instead of just two. For example,
if an MPE is compatible with xiu, it is preserved as long as the following inequalities
hold:
r(xiu, e) · θxi|u ≥r(xju, e) · θxj |u
for all j ̸= i
r(xiu, e) · θxi|u ≥MPEP (¬u, e).
16.3 Query control
We addressed the problem of query robustness in the previous section, where we discussed
two types of solutions that vary in their accuracy and computational complexity. We ad-
dress the inverse problem in this section: identifying parameter changes that are necessary
and sufﬁcient to induce a particular query change. For example, we may compute the
probability of some event α given evidence β and ﬁnd it to be .5 even though we believe
it should be no less than .9. Our goal would then be to identify necessary and sufﬁcient
parameter changes that induce the desirable change in query value. When addressing this
3 These constants can be obtained using (16.7): r(xu, e) = MPEP (xu, e)/θx|u. This method is generally not as
efﬁcient as the method given in Chapter 12. It is also not applicable if θx|u = 0.

428
SENSITIVITY ANALYSIS
problem, we can constrain the number of parameters that we are able to change in the
process.
We consider two cases in the remainder of this section. In the ﬁrst case, we assume
that only one parameter can be changed. In the second case, we assume that multiple
parameters can be changed but they all must belong to the same CPT. Changing multiple
parameters that occur in multiple CPTs is also possible based on the techniques we present
but the associated computational difﬁculties make it less interesting practically.
16.3.1 Single parameter changes
We start with some of the more common query constraints that we may want to establish
through a parameter change:
Pr(y|e) ≥κ
(16.9)
Pr(y|e) ≤κ
(16.10)
Pr(y|e) −Pr(z|e) ≥κ
(16.11)
Pr(y|e)
Pr(z|e) ≥κ.
(16.12)
Here κ is a constant, evidence e is an instantiation of variables E, and events y and z are
values of the variables Y and Z, respectively, with Y, Z ̸∈E. For example, if we want
to make event y more likely than event z given evidence e, we specify the constraint
Pr(y|e) −Pr(z|e) ≥0. We can also make event y at least twice as likely as event z, given
evidence e, by specifying the constraint Pr(y|e)/Pr(z|e) ≥2.
We will start by considering the constraint given by (16.9) and then show how the
solution technique can be easily applied to other types of constraints. The key observation
here is based on Theorem 16.4, which says that the probability of some event, Pr(α), is a
linear function of any particular parameter θx|u:
Pr(α) = µα
x|u · θx|u + να
x|u.
One major implication of linearity is this: Suppose we apply a change of δx|u to the given
parameter. We can then express the new query value as
Pr(α) = Pr0(α) + µα
x|u · δx|u.
Recall that Pr0(α) is the probability of α before we apply the parameter change. Now to
enforce (16.9), it sufﬁces to ensure that Pr(y, e) ≥κ · Pr(e) or, equivalently,
Pr0(y, e) + µy,e
x|u · δx|u ≥κ

Pr0(e) + µe
x|u · δx|u

.
Rearranging the terms, we get the following result.
Corollary 9. Let N0 and N be two Bayesian networks where N is obtained from N0 by
changing a single parameter value θ0
x|u to the new value θx|u. Let Pr0 and Pr be the two
distributions induced by these networks. To ensure that Pr(y|e) ≥κ holds, the amount of
parameter change δx|u = θx|u −θ0
x|u must be such that
Pr0(y, e) −κ · Pr0(e) ≥δx|u

−µy,e
x|u + κ · µe
x|u

.
(16.13)
■

16.3 QUERY CONTROL
429
Note that the solution of δx|u in Corollary 9 always has one of the following two
forms:
r δx|u ≤ϵ for some computed ϵ < 0, in which case the new value of θx|u must be in the
interval [0, θ0
x|u + ϵ]. This case corresponds to a decrease in the current parameter value.
r δx|u ≥ϵ for some computed ϵ > 0, in which case the new value of θx|u must be in the
interval [θ0
x|u + ϵ, 1]. This case corresponds to an increase in the current parameter value.
Therefore, ϵ is the minimum amount of change in θx|u that can enforce the query constraint.
For some parameters, it is possible to ﬁnd no legitimate solutions for (16.13), meaning
there is no way we can change these parameters to enforce the desired query constraint.
Note that all quantities that appear in (16.13) can be obtained by performing inference
on the original network N0. In fact, if we have an algorithm that can compute in O(f )
time the probability of i, x, u, for a given variable instantiation i and all parameters θx|u
in a Bayesian network, then we can also compute in O(f ) time the quantities needed by
(16.13), for some query y|e and all network parameters θx|u. Suppose for example, that
we are using the jointree algorithm for this computation and we have a jointree of width w
and size n (number of clusters). The algorithm can then be used to compute the probability
of i, x, u in O(n exp(w)) time for any given variable instantiation i and all parameters
θx|u. We can therefore use the algorithm to obtain instances of (16.13) for some query y|e
and all network parameters θx|u in O(n exp(w)) time as well. We can do this by running
the algorithm twice, once with evidence e and again with extended evidence y, e. We can
verify that we can recover all needed quantities from the marginals computed by the two
runs of the jointree algorithm in this case.
Let us now consider an example with respect to the network in Figure 16.3. Given
evidence e = S, ¬R, that is, smoke is observed but no report of people evacuating the
building, the probability of ﬁre is .246. Suppose however that we believe the probability
of ﬁre should be no less than .5 and, hence, decide to identify parameter changes that
enforce the constraint Pr(F|e) ≥.5. Using Corollary 9 and considering every possible
parameter in the network, we identify the following parameter changes:
r Increase θF from .01 to ≥.029977
r Decrease θS|¬F from .01 to ≤.003269
r Increase θT from .02 to ≥.801449
r Increase θL|¬A from .001 to ≥.923136
r Increase θR|¬L from .01 to ≥.776456
The last three parameter changes can be ruled out based on commonsense considerations.
The other parameter changes are either to increase the prior probability of a ﬁre or to
decrease the probability of observing smoke without having a ﬁre. Note that the network
contains many other parameters that do not yield solutions to (16.13), which means that
we cannot satisfy the given query constraint by changing those parameters.
Using similar derivations to those given here, Corollary 9 can be extended to enforce
additional types of query constraints as follows:
r To satisfy Pr(y|e) ≤κ, we need a parameter change δx|u such that
Pr0(y, e) −κ · Pr0(e) ≤δx|u

−µy,e
x|u + κ · µe
x|u

.
(16.14)

430
SENSITIVITY ANALYSIS
r To satisfy Pr(y|e) −Pr(z|e) ≥κ, we need a change δx|u such that
Pr0(y, e) −Pr0(z, e) −κ · Pr0(e) ≥δx|u

−µy,e
x|u + µz,e
x|u + κ · µe
x|u

.
(16.15)
r To satisfy Pr(y|e)/Pr(z|e) ≥κ, we need a change δx|u such that
Pr0(y, e) −κ · Pr0(z, e) ≥δx|u

−µy,e
x|u + κ · µz,e
x|u

.
(16.16)
The computational complexity remarks made earlier about (16.16) also apply to these
inequalities. For example, using the jointree algorithm we can obtain instances of (16.15)
for some queries y|e and z|e and all network parameters θx|u by running the algorithm
three times, ﬁrst with evidence e then with evidence y, e and ﬁnally with evidence z, e.
Again, we can verify that all needed quantities can be recovered from the marginals
computed by the jointree algorithm in this case.
16.3.2 Multiple parameter changes
We now turn to the problem of satisfying query constraints while allowing more than one
parameter to change. However, as mentioned earlier we restrict the changing parameters
to one CPT. The main reason for this restriction is computational as it allows us to
handle multiple parameters within the same computational complexity required for single
parameters.
We ﬁrst give a concrete example of multiple parameter changes within the same CPT
by considering the following CPTs for the multivalued variable X:
Z
Y
X
0
X|Y,Z
X|Y,Z
true
true
x1
.1
.1
true
true
x2
.5
.5
true
true
x3
.4
.4
true
false
x1
(.8)
.7
true
false
x2
.1
.15
true
false
x3
.1
.15
false
true
x1
(.5)
.7
false
true
x2
.2
.08
false
true
x3
.3
.12
false
false
x1
.3
.4
false
false
x2
(.7)
.8
false
false
x3
0
0
Here we have a CPT X|Y,Z that results from changing three parameter values in the
CPT 0
X|Y,Z– the ones enclosed in parentheses. Note how each of these changes induces
a corresponding change in co-varying parameters. For example, the change in parameter
value θ0
x1| ¯y,z (.8 to .7) leads to a corresponding change in co-varying parameters θ0
x2| ¯y,z (.1
to .15) and θ0
x3| ¯y,z (.1 to .15). In general, when applying multiple parameter changes to a
particular CPT we are allowed to change only one parameter value θ0
x|u for a given parent
instantiation u, as all other parameter values θ0
x⋆|u, x⋆̸= x, are co-varying and cannot be
changed independently.
The following is the key result underlying the techniques associated with changing
multiple parameters.

16.3 QUERY CONTROL
431
Theorem 16.5. Let N0 and N be two Bayesian networks where N is obtained from N0 by
changing one parameter value θ0
x|u for each parent instantiation u in the CPT for variable
X.4 Let Pr0 and Pr be the two distributions induced by these networks. The value of a
query Pr(α) can be expressed in terms of the original network N0 and the new parameter
values θx|u as
Pr(α) =

u
µα
x|u · θx|u + ν′α
x|u,
where µα
x|u is given by (16.5), and
ν′α
x|u =

x⋆̸=x
ρ(x, x⋆, u)∂Pr(α)
∂θx⋆|u
.
■
Hence, the new probability Pr(α) is a linear function of each of the new parameter
values θx|u (note that each parent instantiation u determines a unique value x of X for
which we are changing the parameter θ0
x|u to θx|u). Given this linear relationship, if we
apply a change of δx|u to each parameter we can express the new probability Pr(α) as
Pr(α) = Pr0(α) +

u
µα
x|u · δx|u.
To enforce (16.9), it sufﬁces to ensure that Pr(y, e) ≥κ · Pr(e) or, equivalently,
Pr0(y, e) +

u
µy,e
x|u · δx|u ≥κ

Pr0(e) +

u
µe
x|u · δx|u

.
Rearranging the terms, we get the following result.
Corollary 10. Let N0 and N be two Bayesian networks where N is obtained from N0
by changing one parameter value θ0
x|u for each parent instantiation u in the CPT for
variable X. Let Pr0 and Pr be the two distributions induced by these networks. To ensure
that Pr(y|e) ≥κ holds, the amount of parameter changes δx|u = θx|u −θ0
x|u must be such
that
Pr0(y, e) −κ · Pr0(e) ≥

u
δx|u

−µy,e
x|u + κ · µe
x|u

,
(16.17)
where µy,e
x|u and µe
x|u are given by (16.5).
■
Parameter changes that ensure the query constraint can be found by ﬁrst solving for
the equality condition in (16.17). This deﬁnes a hyperplane that splits the parameter
change space into two regions, one of which satisﬁes the inequality. We also note that
the computational complexity of the quantities needed by (16.17) is no worse than those
needed by (16.13).
Consider again the Bayesian network in Figure 16.3. Suppose the evidence e is R, ¬S,
that is, there is a report of people evacuating the building but no smoke is observed. The
probability of alarm tampering is .5 given this evidence but suppose we wish to make it no
less than .65 by changing the CPT for variable R. That is, we currently have Pr0(T |e) = .5
4 The amount of change can be zero: θ0
x|u = θx|u.

432
SENSITIVITY ANALYSIS
-0.01
-0.008
-0.006
-0.004
-0.002
 0
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
parameter change δR|¬L
parameter change δ ¬R|L
Figure 16.5: The solution space of multiple parameter changes.
but wish to enforce the constraint Pr(T |e) ≥.65 by changing multiple parameters in the
CPT 0
R|L. Assume further that the parameters we wish to change are:
r θ0
¬R|L = .25: the probability of not receiving an evacuation report when there is an evac-
uation (false negative)
r θ0
R|¬L = .01, the probability of receiving an evacuation report when there is no evacuation
(false positive).
Let δ¬R|L and δR|¬L be the changes we wish to apply to these parameters, respectively.
Corollary 10 gives us the following characterization of required parameter changes:
−.003294 ≥.003901 · δ¬R|L + .621925 · δR|¬L.
The solution space is plotted in Figure 16.5. The line indicates the set of points where the
equality condition Pr(T |e) = .65 holds, and the solution space is deﬁned by the region
at or below the line. Therefore, we can ensure Pr(T |e) ≥.65 by applying any parameter
change in this region.
Even though changing single or multiple parameters in the same CPT requires the
same computational complexity, the solution space of multiple parameter changes can be
more difﬁcult to visualize when the number of parameters is large enough. Yet changing
multiple parameters is often necessary in practical applications. For example, if a variable
corresponds to some sensor readings, then the parameters appearing in its CPT correspond
to false positive and false negative rates of the sensor. In the case of single parameter
changes, we are looking for solutions that allow us to only change one of these rates but
not the other. With multiple parameter changes, we can in principle change both rates.
Note also that for some variables, there may exist multiple parameter changes that
satisfy a particular constraint yet no single parameter change may be sufﬁcient for this
purpose. For example, consider again the Bayesian network in Figure 16.3. Suppose we
are given evidence e = S, R, that is, smoke is observed and there is a report of people
evacuating the building. The probability of alarm tampering is .0284 under this evidence.
Suppose now that we pose the question: What parameter changes can we apply to decrease
this probability to at most .01? As it turns out, if we restrict ourselves to single parameter
changes, then only one parameter change is possible: decrease the prior probability of
tampering from its initial value of .02 to at most .007. For example, no single parameter
change in the CPT of Alarm would ensure the given constraint, and we may be inclined to

16.3 QUERY CONTROL
433
believe that the parameters in this CPT are irrelevant to the query. However, if we allow
multiple parameter changes in a single CPT, it is possible to satisfy the constraint by
changing the CPT for Alarm. One such change is given in the following:
Fire (F)
Tampering (T )
Alarm (A)
A|F,T
true
true
true
.088
true
false
true
.999
false
true
true
.354
false
false
true
.001
We ﬁnally note that Corollary 10 can be extended to enforce additional types of query
constraints as follows:
r To satisfy Pr(y|e) ≤κ, we need parameter changes δx|u such that
Pr0(y, e) −κ · Pr0(e) ≤

u
δx|u

−µy,e
x|u + κ · µe
x|u

.
(16.18)
r To satisfy Pr(y|e) −Pr(z|e) ≥κ, we need parameter changes δx|u such that
Pr0(y, e) −Pr0(z, e) −κ · Pr0(e) ≥

u
δx|u

−µy,e
x|u + µz,e
x|u + κ · µe
x|u

.
(16.19)
r To satisfy Pr(y|e)/Pr(z|e) ≥κ, we need parameter changes δx|u such that
Pr0(y, e) −κ · Pr0(z, e) ≥

u
δx|u

−µy,e
x|u + κ · µz,e
x|u

.
(16.20)
Again, the computational complexity of quantities appearing in these inequalities is no
worse than the complexity of quantities corresponding to single parameter changes. Hence,
we can still characterize multiple parameter changes efﬁciently for these constraints as
long as the changes are restricted to the same CPT.
Bibliographic remarks
The early studies of sensitivity analysis in Bayesian networks have been mostly empirical
(e.g., [Laskey, 1995; Pradhan et al., 1996]). The ﬁrst analytical result relating to current
studies is due to Russell et al. [1995], who observed that the probability of evidence is
a linear function in each network parameter. This observation was made in the context
of learning network parameters using gradient ascent approaches, which called for com-
puting the gradient of the likelihood function with respect to network parameters (see
Section 17.3.2). A more general result was then shown in Castillo et al. [1996; 1997],
where the probability of evidence was expressed as a polynomial of network parameters
in which each parameter has degree 1. Computing the coefﬁcients of this polynomial
efﬁciently using jointrees was discussed in Jensen [1999] and Kjaerulff and van der Gaag
[2000], and using circuit propagation in Darwiche [2003].
The CD distance measure is due to Chan and Darwiche [2005a]. The precursors of this
measure were initially reported in Chan and Darwiche [2002], where the ﬁrst results on
network-independent sensitivity analysis were presented. The techniques discussed for
network-speciﬁc sensitivity analysis are due to Coup´e and van der Gaag [2002] – see also
van der Gaag et al. [2007] for a recent survey. The techniques discussed for sensitivity
analysis with multiple parameters are due to Chan and Darwiche [2004] and the ones for
MPE robustness are due to Chan and Darwiche [2006].

434
SENSITIVITY ANALYSIS
16.4 Exercises
16.1.
Let us say that two probability distributions Pr0(X) and Pr(X) have the same support if
for every instantiation x, Pr0(x) = 0 iff Pr(x) = 0. Show that the CD distance satisﬁes the
following property: D(Pr0, Pr) = ∞iff the distributions Pr0 and Pr do not have the same
support.
16.2.
Show that the CD distance satisﬁes the properties of positiveness, symmetry, and triangle
inequality.
16.3.
Consider a distribution Pr(X), event α, and show that
Pr(α)
Pr0(α) ≤max
x|=α
Pr(x)
Pr0(x)
and
Pr(α)
Pr0(α) ≥min
x|=α
Pr(x)
Pr0(x).
16.4.
Show that the bounds given by Theorem 16.1 are tight in the sense that for every pair of
distributions Pr0 and Pr, there are events α and β such that
O(α | β)
O0(α | β) = eD(Pr0,Pr)
O(¬α | β)
O0(¬α | β) = e−D(Pr0,Pr).
16.5.
Show that Inequality 16.2 is implied by Theorem 16.1.
16.6.
Consider two Bayesian networks N0 and N, where network N is obtained from N0 by
changing a single parameter value θ0
x in the CPT of root node X. Show the following for any
evidence e,
O(x|e)
O0(x|e) = O(x)
O0(x).
16.7.
Show that D(0
X|U, X|U) ≥maxu D(0
X|u, X|u).
16.8.
Prove Inequalities 16.14, 16.15, and 16.16.
16.9.
Consider the solution space characterized by Inequality 16.17. Call a solution optimal if
it minimizes the CD distance between the original and new networks. Show that optimal
solutions must satisfy the equality constraint corresponding to Inequality 16.17.
16.10. Suppose that Pr is a distribution obtained from Pr0 by incorporating soft evidence on events
(β1, . . . , βn) using Jeffrey’s rule (see Chapter 3). Show that
D(Pr0, Pr) = ln
n
max
i=1
Pr(βi)
Pr0(βi) −ln
n
min
i=1
Pr(βi)
Pr0(βi).
16.11. Consider a simpliﬁed version of the pregnancy network in Figure 16.1 where we can only
administer the scanning test to detect pregnancy. The current probability of pregnancy given
a positive scanning test is 0.9983 and the current probability of no pregnancy given a
negative scanning test is 0.5967. We wish to apply changes in both the false-positive and
false-negative rates of the scanning test by amounts of δ1 and δ2, respectively, such that
the corresponding probabilities are at least 0.999 and 0.7 (instead of 0.9983 and 0.5967,
respectively). Plot the solution spaces of δ1 and δ2 such that they satisfy the two constraints
and ﬁnd the unique solution such that the minimum conﬁdence levels are exactly realized by
the parameter changes.
16.12. Consider a simpliﬁed version of the ﬁre network in Figure 16.3 where we are only interested
in the variables Fire, Tampering, and Alarm. Compute the sensitivity function for query
Pr(F | A) in terms of parameter θA|F,T then again for the same query in terms of parameter
θA|¬F,¬T and plot the functions. From the plots, what would the effects on the query value
be if we apply a small absolute change in each of the two parameters? Also, compute the

16.5 PROOFS
435
sensitivity functions for the query Pr(A | F) in terms of these two parameters. What is the
difference between these two sensitivity functions and the previous two?
16.13. Assume that instead of using the proportional scheme of Deﬁnition 16.2, we distribute the
parameter change equally among co-varying parameters. In this case, what is the new value
of θx⋆|u? What problem may this scheme face when distributing the parameter change?
16.14. Prove that the proportional scheme of Deﬁnition 16.2 is the optimal scheme of distributing
parameter changes among co-varying parameters in the sense that it minimizes the CD
distance D(Pr0, Pr) among all possible schemes.
16.15. Consider the network in Figure 16.3. The probability of having a ﬁre given that the alarm has
triggered is .3667. We now wish to install a smoke detector that responds only to whether
smoke is present such that when both the alarm and the smoke detectors trigger, the prob-
ability of ﬁre is at least .8. How can we use sensitivity analysis to ﬁnd the required reliability
of the smoke detector? Specify the three elements of the sensitivity analysis process: the
Bayesian network that models this scenario, the query constraint you wish to satisfy, and the
parameters you are allowed to change. Hint: You may recall how sensors are modeled as
soft evidence in Chapter 3.
16.16. Let N0 and N be Bayesian networks where network N is obtained from N0 by changing the
CPTs of variables X and Y from 0
X|UX to X|UX and from 0
Y|UY to Y|UY , respectively –
that is, changing parameter value θ0
x|uX to some new value θx|uX for every x and uX, and
θ0
y|uY to some new value θy|uY for every y and uY . Let Pr0 and Pr be the distributions induced
by networks N0 and N, respectively, and also assume that Pr0(uX) > 0 and Pr0(uY ) > 0
for every uX and uY . Prove that if the two families X, UX and Y, UY are disjoint, that is, they
do not share any variables, then
D(Pr0, Pr) = D(0
X|UX, X|UX) + D(0
Y|UY , Y|UY ).
Also prove that if the two families are not disjoint, then the sum of the distances between the
CPTs is an upper bound on D(Pr0, Pr).
16.17. Let N0 and N be Bayesian networks where network N is obtained from N0 by changing
the CPTs of variables X1, . . . , Xm from 0
Xi|UXi to Xi|UXi (i.e., changing parameter value
θ0
xi|uXi to some new value θxi|uXi for every xi and uXi ). Let Pr0 and Pr be the distributions
induced by networks N0 and N, respectively, and also assume that Pr0(uXi) > 0 for every
uXi . Devise a procedure that computes D(Pr0, Pr). Hint: This procedure can be similar to
the one used for computing the MPE probability.
16.5 Proofs
PROOF OF THEOREM 16.1. If distributions Pr0 and Pr do not have the same support, we
have D(Pr0, Pr) = ∞and thus −∞= e−D(Pr0,Pr) ≤O(α|β)/O0(α|β) ≤eD(Pr0,Pr) = ∞.
Otherwise, the odds ratio O(α|β)/O0(α|β) can be expressed as
O(α|β)
O0(α|β) = Pr(α|β)
Pr(¬α|β)/ Pr0(α|β)
Pr0(¬α|β)
= Pr(α, β)
Pr(¬α, β)/ Pr0(α, β)
Pr0(¬α, β)
=

x|=α,β Pr(x)

x|=¬α,β Pr(x)/

x|=α,β Pr0(x)

x|=¬α,β Pr0(x)
=

x|=α,β Pr(x)

x|=α,β Pr0(x)/

x|=¬α,β Pr(x)

x|=¬α,β Pr0(x).

436
SENSITIVITY ANALYSIS
Using Exercise 16.3, we can now obtain the upper bound on the odds ratio:
O(α|β)
O0(α|β) ≤

max
x|=α,β
Pr(x)
Pr0(x)
	
/

min
x|=¬α,β
Pr(x)
Pr0(x)
	
≤

max
x
Pr(x)
Pr0(x)
	
/

min
x
Pr(x)
Pr0(x)
	
= eD(Pr0,Pr).
Similarly, we can also obtain the lower bound on the odds ratio:
O(α|β)
O0(α|β) ≥

min
x|=α,β
Pr(x)
Pr0(x)
	
/

max
x|=¬α,β
Pr(x)
Pr0(x)
	
≥

min
x
Pr(x)
Pr0(x)
	
/

max
x
Pr(x)
Pr0(x)
	
= e−D(Pr0,Pr).
Therefore, we have e−D(Pr0,Pr) ≤O(α|β)/O0(α|β) ≤eD(Pr0,Pr). If both O(α|β) and O0(α|β)
take on either 0 or ∞, Theorem 16.1 still holds because 0/0
def= 1 and ∞/∞
def= 1.
Finally, proving the tightness of the bound is left to Exercise 16.4.
■
PROOF OF THEOREM 16.2. Using Lemma 16.1, we have that maxx (Pr(x)/Pr0(x)) =
maxx (θx|u/θ0
x|u)
and
minx (Pr(x)/Pr0(x)) = minx (θx|u/θ0
x|u).
Therefore,
we
have
D(Pr0, Pr) = D(0
X,u, X,u).
■
Lemma 16.1. Assume that we change parameter θ0
x|u to θx|u for every value x and
Pr0(u) > 0. For every x where θx|u > 0 or θ0
x|u > 0, there must exist some x |= x, u such
that it satisﬁes the condition Pr(x)/Pr0(x) = θx|u/θ0
x|u. For all other instantiations x that
do not satisfy this condition, we must have Pr(x) = Pr0(x) and thus Pr(x)/Pr0(x) = 1.5
PROOF. We ﬁrst note that Pr(u) = Pr0(u) > 0. For any instantiation x, either x |= ¯u or
x |= x, u for some x. We now consider the different cases of x.
Case: If x |= ¯u, we must have Pr(x) = Pr0(x) since we are only changing parameters
θ0
x|u.
Case: If x |= x, u, we consider four cases of x:
r If θx|u = θ0
x|u = 0, we must have Pr(x, u) = Pr0(x, u) = 0. Therefore, for all instantiaions
x |= x, u, Pr(x) = Pr0(x) = 0.
r If θx|u = 0 and θ0
x|u > 0, we must have Pr(x, u) = 0 and Pr0(x, u) > 0. Therefore, for
all instantiations x |= x, u, either Pr(x) = Pr0(x) = 0 or Pr(x) = 0 and Pr0(x) > 0, giving
us Pr(x)/Pr0(x) = 0 = θx|u/θ0
x|u. Moreover, because Pr0(x, u) > 0 there must exist some
x |= x, u such that Pr0(x) > 0 and thus satisﬁes the condition Pr(x)/Pr0(x) = θx|u/θ0
x|u.
r If θx|u > 0 and θ0
x|u = 0, we must have Pr(x, u) > 0 and Pr0(x, u) = 0. Therefore, for
all instantiations x |= x, u either Pr(x) = Pr0(x) = 0 or Pr(x) > 0 and Pr0(x) = 0, giving
us Pr(x)/Pr0(x) = ∞= θx|u/θ0
x|u. Moreover, because Pr(x, u) > 0 there must exist some
x |= x, u such that Pr(x) > 0 and thus satisﬁes the condition Pr(x)/Pr0(x) = θx|u/θ0
x|u.
r If θx|u > 0 and θ0
x|u > 0, we must have Pr(x, u) > 0 and Pr0(x, u) > 0. Therefore, for all
instantiations x |= x, u, either Pr(x) = Pr0(x) = 0 or Pr(x) > 0 and Pr0(x) > 0, giving us
5 Either Pr(x) = Pr0(x) > 0, and thus Pr(x)/Pr0(x) = 1, or Pr(x) = Pr0(x) = 0, and thus Pr(x)/Pr0(x)
def= 1.

16.5 PROOFS
437
Pr(x)/Pr0(x) = θx|u/θ0
x|u. Moreover, because Pr(x, u) > 0 and Pr0(x, u) > 0 there must
exist some x |= x, u such that Pr(x) > 0 and Pr0(x) > 0 and thus satisﬁes the condition
Pr(x)/Pr0(x) = θx|u/θ0
x|u.
■
PROOF OF THEOREM 16.3. If θx|u = θ0
x|u, then both sides of the equation are equal
to zero. Suppose now that θx|u ̸= θ0
x|u. If θ0
x|u = 0 or θ0
x|u = 1, then both sides of the
equation are equal to ∞(see Exercise 16.1). Suppose now that θ0
x|u ̸= 0 and θ0
x|u ̸= 1.
From Deﬁnition 16.2, we have
θx⋆|u
θ0
x⋆|u
= ρ(x, x⋆, u)(1 −θx|u)
θ0
x⋆|u
=
θ0
x⋆|u
1−θ0
x|u (1 −θx|u)
θ0
x⋆|u
= 1 −θx|u
1 −θ0
x|u
.
Assume that θx|u > θ0
x|u. From Theorem 16.2, we have
D(Pr0, Pr) = ln max
x
θx|u
θ0
x|u
−ln min
x
θx|u
θ0
x|u
= ln θx|u
θ0
x|u
−ln 1 −θx|u
1 −θ0
x|u
= ln
θx|u
1 −θx|u
−ln
θ0
x|u
1 −θ0
x|u
= ln O(x|u) −ln O0(x|u)
=
ln O(x|u) −ln O0(x|u)
 .
We can prove the similar case when θx|u < θ0
x|u:
D(Pr0, Pr) = ln max
x
θx|u
θ0
x|u
−ln min
x
θx|u
θ0
x|u
= ln 1 −θx|u
1 −θ0
x|u
−ln θx|u
θ0
x|u
= ln 1 −θx|u
θx|u
−ln
1 −θ0
x|u
θ0
x|u
= −ln O(x|u) + ln O0(x|u)
=
ln O(x|u) −ln O0(x|u)
 ,
since ln O0(x|u) > ln O(x|u).
■
PROOF OF THEOREM 16.4. We can express Pr(α) as
Pr(α) = ∂Pr(α)
∂θx|u
θx|u +

x⋆̸=x
∂Pr(α)
∂θx⋆|u
θ⋆
x|u +

u⋆̸=u
Pr(u⋆, α).

438
SENSITIVITY ANALYSIS
Since Pr(u⋆, α) for all u⋆̸= u remains constant under changes of θx|u and θx⋆|u, we have
Pr(u⋆, α) = Pr0(u⋆, α). From Deﬁnition 16.2, we have
Pr(α) = ∂Pr(α)
∂θx|u
θx|u +

x⋆̸=x
∂Pr(α)
∂θx⋆|u
ρ(x, x⋆, u)(1 −θx|u) +

u⋆̸=u
Pr0(u⋆, α).
Expanding the expression, we get the values of µα
x|u and να
x|u from (16.5) and (16.6),
respectively.
■
PROOF OF THEOREM 16.5. We can express Pr(α) as
Pr(α) =

u
⎛
⎝∂Pr(α)
∂θx|u
θx|u +

x⋆̸=x
∂Pr(α)
∂θx⋆|u
θ⋆
x|u
⎞
⎠.
From Deﬁnition 16.2, we have
Pr(α) =

u
⎛
⎝∂Pr(α)
∂θx|u
θx|u +

x⋆̸=x
∂Pr(α)
∂θx⋆|u
ρ(x, x⋆, u)(1 −θx|u)
⎞
⎠.
Expanding the expression, we get the values of µα
x|u and ν′α
x|u from (16.5) and (16.5),
respectively.
■

17
Learning: The Maximum Likelihood Approach
We discuss in this chapter the process of learning Bayesian networks from data. The learn-
ing process is studied under different conditions, which relate to the nature of available
data and the amount of prior knowledge we have on the Bayesian network.
17.1 Introduction
Consider Figure 17.1, which depicts a Bayesian network structure from the domain of
medical diagnosis (we treated this network in Chapter 5). Consider also the data set
depicted in this ﬁgure. Each row in this data set is called a case and represents a medical
record for a particular patient.1 Note that some of the cases are incomplete, where “?”
indicates the unavailability of corresponding data for that patient. The data set is therefore
said to be incomplete due to these missing values; otherwise, it is called a complete
data set.
A key objective of this chapter is to provide techniques for estimating the parameters
of a network structure given both complete and incomplete data sets. The techniques
we provide therefore complement those given in Chapter 5 for constructing Bayesian
networks. In particular we can now construct the network structure from either design
information or by working with domain experts, as discussed in Chapter 5, and then use
the techniques discussed in this chapter to estimate the CPTs of these structures from data.
We also discuss techniques for learning the network structure itself, although our focus
here is on complete data sets for reasons that we state later.
The simplest problem we treat in this chapter is that of estimating parameters for
a given network structure when the data set is complete. This problem is addressed in
Section 17.2 and has already been addressed to some extent in Chapter 15 since a complete
data set corresponds to a sample as deﬁned in that chapter. However, in Chapter 15 a
sample was simulated from a given Bayesian network with the goal of estimating certain
probabilities by operating on the sample. In this chapter, the sample (i.e., data set) is
typically collected from a real-world situation, which provides little control over the
sample size. Moreover, the goal now is to estimate all probabilities of the form Pr(x|u) =
θx|u, where θx|u is a network parameter. We indeed adopt the same solution to this problem
that we adopted in Chapter 15 while presenting more of its properties from a learning
perspective.
The second problem we address in this chapter concerns the estimation of network
parameters from an incomplete data set. This is treated in Section 17.3. As we shall see,
this problem is computationally much more demanding and the resulting solutions have
weaker properties than those obtained for complete data sets.
1 A case is also referred to as an instance, unit, observation, record, and subject in the literature.
439

440
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Cold?
Flu?
Tonsillitis?
Sore Throat?
Fever?
Chilling?
Body Ache?
Case
Cold?
Flu?
Tonsillitis?
Chilling?
Bodyache?
Sorethroat?
Fever?
1
true
false
?
true
false
false
false
2
false
true
false
true
true
false
true
3
?
?
true
false
?
true
false
...
...
...
...
...
...
...
...
Figure 17.1: A Bayesian network structure and a corresponding data set.
The last problem we address concerns the learning of network structures from complete
data sets. The treatment of this problem is divided into two orthogonal dimensions. The
ﬁrst dimension, addressed in Section 17.4, deals with the scoring function that we must
use when learning network structure (i.e., which network structures should we prefer).
The second dimension, addressed in Section 17.5, deals with the computational demands
that arise when searching for a network structure that optimizes a certain scoring function.
We can distinguish between three general approaches to the learning problem that adopt
different criteria for what parameters and structure we should seek. The ﬁrst approach
is based on the likelihood principle, which favors those estimates that have a maximal
likelihood, that is, ones that maximize the probability of observing the given data set. This
approach is therefore known as the maximum likelihood approach to learning and is the
one we treat in this chapter.
The second approach requires more input to the learning process as it demands us to
deﬁne a meta-distribution over network structures and parameters. It then reduces the
problem of learning to a problem of classical inference in which the data set is viewed
as evidence. In particular, it ﬁrst conditions the meta distribution on the given data set
and then uses the posterior meta-distribution as a criterion for deﬁning estimates. This
approach is known as the Bayesian approach to learning and is treated in Chapter 18.
A third approach for learning Bayesian networks is known as the constraint-based
approach and applies mostly to learning network structures. According to this approach,
we seek structures that respect the conditional independencies exhibited by the given
data set. We do not treat this approach in this book but provide some references in the
bibliographic remarks section.
All of the previous approaches are meant to induce Bayesian networks that are mean-
ingful independent of the tasks for which they are intended. Consider for example a
network that models a set of diseases and a corresponding set of symptoms. This network
may be used to perform diagnostic tasks by inferring the most likely disease given a set
of observed symptoms. It may also be used for prediction tasks where we infer the most
likely symptom given some diseases. If we concern ourselves with only one of these

17.2 ESTIMATING PARAMETERS FROM COMPLETE DATA
441
Health Aware
(H)
Smokes
(S)
Exercises
(E)
Case
H
S
E
1
T
F
T
2
T
F
T
3
F
T
F
4
F
F
T
5
T
F
F
6
T
F
T
7
F
F
F
8
T
F
T
9
T
F
T
10
F
F
T
11
T
F
T
12
T
T
T
13
T
F
T
14
T
T
T
15
T
F
T
16
T
F
T
H
S
E
PrD(.)
T
T
T
2/16
T
T
F
0/16
T
F
T
9/16
T
F
F
1/16
F
T
T
0/16
F
T
F
1/16
F
F
T
2/16
F
F
F
1/16
(a) Network structure
(b) Complete data
(c) Empirical distribution
Figure 17.2: Estimating network parameters from a complete data set.
tasks, say, diagnostics, we can use a more specialized learning principle that optimizes
the diagnostic performance of the learned network. In machine-learning jargon, we say
that we are learning a discriminative model in this case as it is often used to discriminate
among patients according to a predeﬁned set of classes (e.g., has cancer or not). This is
to be contrasted with learning a generative model, which is to be evaluated based on its
ability to generate the given data set regardless of how it performs on any particular task.
We do not cover discriminative approaches to learning in this book but provide some
references in the bibliographic remarks section.
17.2 Estimating parameters from complete data
Consider the simple network structure in Figure 17.2(a) and suppose that our goal is to
estimate its parameters from the data set D given in Figure 17.2(b).
Our estimation approach is based on the assumption that this data set was simulated
from the true Bayesian network as given in Section 15.2, that is, the cases are generated
independently and according to their true probabilities. Under these assumptions, we
can deﬁne an empirical distribution PrD(.) that summarizes the data set as given in
Figure 17.2(c). According to this distribution, the empirical probability of instantiation
h, s, e is simply its frequency of occurrence in the data set,
PrD(h, s, e) = D#(h, s, e)
N
,
where D#(h, s, e) is the number of cases in the data set D that satisfy instantiation h, s, e,
and N is the data set size.
We can now estimate parameters based on the empirical distribution. Consider the
parameter θs|h for example, that corresponds to the probability that a person will smoke

442
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
given that they are health-aware, Pr(s|h). Our estimate for this parameter is now given by
PrD(s|h) = PrD(s, h)
PrD(h)
= 2/16
12/16 = 1/6.
This corresponds to the simplest estimation technique discussed in Chapter 15, where
we also provided a key result on the variance of its produced estimates.2 We restate this
result next but after formalizing some of the concepts used here.
Deﬁnition 17.1. A data set D for variables X is a vector d1, . . . , dN where each di
is called a case and represents a partial instantiation of variables X. The data set is
complete if each case is a complete instantiation of variables X; otherwise, the data set
is incomplete. The empirical distribution for a complete data set D is deﬁned as
PrD(α)
def=
D#(α)
N
,
where D#(α) is the number of cases di in the data set D that satisfy event α, that is,
di |= α.3
■
Figure 17.2(b) depicts a complete data set D for variables H, S, and E, leading to the
following:
r D#(α) = 9 when α is (H =T ) ∧(S =F) ∧(E =T )
r D#(α) = 12 when α is (H =T )
r D#(α) = 14, when α is (H =T ) ∨(E =T ).
Figure 17.2(c) depicts the empirical distribution induced by the complete data set in
Figure 17.2(b).
Given Deﬁnition 17.1, we are then suggesting that we estimate the parameter θx|u by
the empirical probability
θml
x|u
def= PrD(x|u) = D#(x, u)
D#(u) .
(17.1)
The count D#(x, u) is called a sufﬁcient statistic in this case. More generally, any function
of the data is called a statistic. Moreover, a sufﬁcient statistic is a statistic that contains
all of the information in the data set that is needed for a particular estimation task.
Considering the network structure and corresponding data set in Figure 17.2, we then
have the following parameter estimates:
H
θml
H
h
3/4
¯h
1/4
H
S
θml
S|H
h
s
1/6
h
¯s
5/6
¯h
s
1/4
¯h
¯s
3/4
H
E
θml
E|H
h
e
11/12
h
¯e
1/12
¯h
e
1/2
¯h
¯e
1/2
Note that the estimate θml
x|u will have different values depending on the given data set.
However, we expect that the variance of this estimate will decrease as the data set increases
2 In particular, this estimation technique corresponds to the method of direct sampling discussed in Section 15.4.
3 Note that D#(α) = N when α is a valid event as it will be satisﬁed by every case di.

17.2 ESTIMATING PARAMETERS FROM COMPLETE DATA
443
in size. In fact, if we assume that the data set D is a sample of size N simulated from a
distribution Pr, then Theorem 15.10 of Chapter 15 tells us that the distribution of estimate
θml
x|u is asymptotically Normal and can be approximated by a Normal distribution with
mean Pr(x|u) and variance
Pr(x|u)(1 −Pr(x|u))
N · Pr(u)
.
Note how the variance depends on the size of data set, N, the probability of parent
instantiation, Pr(u), and the true value of the estimated parameter, Pr(x|u). In particular,
note how sensitive this variance is to Pr(u), which makes it difﬁcult to estimate the
parameter when this probability is quite small. In fact, if this probability is too small and
the data set is not large enough, it is not uncommon for the empirical probability PrD(u)
to be zero. Under these conditions, the estimate PrD(x|u) is not well deﬁned, leading to
what is known as the problem of zero counts.4 We provide a technique for dealing with
this problem in Chapter 18, which is based on incorporating prior knowledge into the
estimation process.
For the next property of the estimates we have deﬁned, let θ be the set of all parameter
estimates for a given network structure G and let Prθ(.) be the probability distribution
induced by structure G and estimates θ. Let us now deﬁne the likelihood of these estimates
as
L(θ|D)
def=
N

i=1
Prθ(di).
(17.2)
That is, the likelihood of estimates θ is the probability of observing the data set D under
these estimates. We now have the following result.
Theorem 17.1. Let D be a complete data set. The parameter estimates deﬁned by (17.1)
are the only estimates that maximize the likelihood function5
θ⋆= argmax
θ
L(θ|D) iff θ⋆
x|u = PrD(x|u).
■
It is for this reason that these estimates are called maximum likelihood (ML) estimates
and are denoted by θml:
θml = argmax
θ
L(θ|D).
We deﬁned these estimates based on the empirical distribution and then showed that
they maximize the likelihood function. Yet it is quite common to start with the goal of
maximizing the likelihood function and then derive these estimates accordingly. This
alternative approach is justiﬁed by some strong, desirable properties satisﬁed by estimates
that maximize the likelihood function. We indeed follow this approach when dealing with
incomplete data in the next section.
Another property of our ML estimates is that they minimize the KL divergence (see
Appendix B) between the learned Bayesian network and the empirical distribution.
4 That is, we get a zero if we count the number of cases that satisfy instantiation u.
5 We assume here that all parameter estimates are well deﬁned; that is, PrD(u) > 0 for every instantiation u of every
parent set U (see Exercises 17.2 and 17.3).

444
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Theorem 17.2. Let D be a complete data set over variables X. Then
argmax
θ
L(θ|D) = argmin
θ
KL(PrD(X), Prθ(X)).
■
Since ML estimates are unique for a given structure G and complete data set D, the
likelihood of these parameters is then a function of the structure G and data set D. We
therefore deﬁne the likelihood of structure G given data set D as
L(G|D)
def= L(θml|D),
where θml are the ML estimates for structure G and data set D. The likelihood function
plays a major role in many of the developments in this chapter. However, it turns out to
be more convenient to work with the logarithm of the likelihood function, deﬁned as
LL(θ|D)
def=
log L(θ|D) =
N

i=1
log Prθ(di).
The log-likelihood of structure G is deﬁned similarly:
LL(G|D)
def=
log L(G|D).
The likelihood function is ≥0, while the log-likelihood function is ≤0. However, max-
imizing the likelihood function is equivalent to maximizing the log-likelihood func-
tion. We use log2 for the log-likelihood function but suppress the base 2 from now
forward.
We close this section by pointing out a key property of the log-likelihood function for
network structures: it decomposes into a number of components, one for each family in
the Bayesian network structure.
Theorem 17.3. Let G be a network structure and D be a complete data set of size N. If
XU ranges over the families of structure G, then
LL(G|D) = −N

XU
ENTD(X|U),
(17.3)
where ENTD(X|U) is the conditional entropy, deﬁned as
ENTD(X|U) = −

xu
PrD(xu) log2 PrD(x|u).
■
This decomposition is critical to the algorithms for learning network structure, dis-
cussed in Section 17.4. See also Appendix B for a review of entropy and related concepts
from information theory.
17.3 Estimating parameters from incomplete data
The parameter estimates considered in the previous section have a number of interesting
properties: they are unique, asymptotically Normal, and maximize the probability of data.
Most importantly these estimates are easily computable by performing a single pass on the
data set. We proved some of these properties independently yet some of them follow from

17.3 ESTIMATING PARAMETERS FROM INCOMPLETE DATA
445
the others under more general conditions than those we considered here. For example,
maximum likelihood estimates are known to be asymptotically Normal for a large class
of models that include but are not limited to Bayesian networks. It is therefore common
to seek maximum likelihood estimates for incomplete data sets.
The properties of these estimates, however, will depend on the nature of incompleteness
we have. Consider for example a network structure C →T where C represents a medical
condition and T represents a test for detecting this condition. Suppose further that the true
parameters of this network are
C
θc
yes
.25
no
.75
C
T
θt|c
yes
+ve
.80
yes
−ve
.20
no
+ve
.40
no
−ve
.60
(17.4)
Hence, we have Pr(T =+ve) = Pr(T =−ve) = .5. Consider now the following data sets,
all of which are incomplete:
D1
C
T
1
?
+ve
2
?
+ve
3
?
−ve
4
?
−ve
5
?
−ve
6
?
+ve
7
?
+ve
8
?
−ve
D2
C
T
1
yes
+ve
2
yes
+ve
3
yes
−ve
4
no
?
5
yes
−ve
6
yes
+ve
7
no
?
8
no
−ve
D3
C
T
1
yes
+ve
2
yes
+ve
3
?
−ve
4
no
?
5
yes
−ve
6
?
+ve
7
no
?
8
no
−ve
Each of these data sets exhibits a different pattern of data incompleteness. For example,
the values of variable C are missing in all cases of the ﬁrst data set, perhaps because
we can never determine this condition directly. We say in this situation that variable C
is hidden or latent. The situation is different in the second data set, where variable C is
always observed and variable T has some missing values but is not hidden. The third data
set exhibits yet a different pattern of data incompleteness where both variables have some
missing values but neither is hidden.
Let us now consider the ﬁrst data set, D1, and observe that the cases are split equally
between the +ve and −ve values of T . In fact, we expect this to be true in the limit given
the distribution generating this data. As shown in Exercise 17.12, the ML estimates are
not unique for a class of data sets that includes this one. For example, the exercise shows
that the ML estimates for data set D1 are characterized by the following equation:
θT =+ve|C=yes · θC=yes + θT =+ve|C=no · θC=no = 1
2.
Note that the true parameter values in (17.4) satisfy this equation. But the following
estimates do as well:
θC=yes = 1,
θT =+ve|C=yes = 1/2,

446
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
A
B
C
D
A
θ0
a
a1
.20
a2
.80
A
B
θ0
b|a
a1
b1
.75
a1
b2
.25
a2
b1
.10
a2
b2
.90
A
C
θ0
c|a
a1
c1
.50
a1
c2
.50
a2
c1
.25
a2
c2
.75
B
D
θ0
d|b
b1
d1
.20
b1
d2
.80
b2
d1
.70
b2
d2
.30
Figure 17.3: A Bayesian network inducing a probability distribution Prθ0(.).
with θT =+ve|C=no taking any value. Hence, the ML estimates are not unique for the given
data set. However, this should not be surprising since this data set does not contain
enough information to pin down the true parameters. The nonuniqueness of ML estimates
is therefore a desirable property in this case, not a limitation.
Let us now consider the second data set, D2, to illustrate another important point
relating to why the data may be missing. Following are two scenarios:
r People who do not suffer from the condition tend not to take the test. That is, the data is
missing because the test is not performed.
r People who test negative tend not to report the result. That is, the test is performed but its
value is not recorded.
These two scenarios are different in a fundamental way. For example, in the second
scenario the fact that a value is missing does provide some evidence that this value must
be negative. This issue is discussed in Section 17.3.3 where we show that the ML approach
gives the intended results when applied under the ﬁrst scenario but gives unintended results
under the second scenario as it does not integrate all of the information we have about this
scenario. However, we show that the ML approach can still be applied under the second
scenario but that requires some explication of the mechanism that causes the data to be
missing.
We next present two methods that search for ML estimates under incomplete data.
Both methods are based on local search, which start with some initial estimates and
then iteratively improve on them until some stopping condition is met. Both methods
are generally more expensive than the method for complete data yet neither is generally
guaranteed to ﬁnd ML estimates.
17.3.1 Expectation maximization
Consider the structure in Figure 17.3 and suppose that our goal is to ﬁnd ML estimates
for the following data set:
D
A
B
C
D
d1
?
b1
c2
?
d2
?
b1
?
d2
d3
?
b2
c1
d1
d4
?
b2
c1
d1
d5
?
b1
?
d2
(17.5)

17.3 ESTIMATING PARAMETERS FROM INCOMPLETE DATA
447
Suppose further that we are starting with the initial estimates θ0 given in Figure 17.3,
which have the following likelihood:
L(θ0|D) =
5

i=1
Prθ0(di)
= Prθ0(b1, c2)Prθ0(b1, d2)Prθ0(b2, c1, d1)Prθ0(b2, c1, d1)Prθ0(b1, d2)
= (.135)(.184)(.144)(.144)(.184)
= 9.5 × 10−5.
Note that contrary to the case of complete data, evaluating the terms in this product
generally requires inference on the Bayesian network.6
Our ﬁrst local search method, called expectation maximization (EM), is based on the
method of complete data discussed in the previous section. That is, this method ﬁrst
completes the data set, inducing an empirical distribution, and then uses it to estimate
parameters as in the previous section. The new set of parameters are guaranteed to have
no less likelihood than the initial parameters, so this process can be repeated until some
convergence condition is met.
To illustrate the process of completing a data set, consider again the data set in (17.5).
The ﬁrst case in this data set has two variables with missing values, A and D. Hence,
there are four possible completions for this case. Although we do not know which one of
these completions is the correct one, we can compute the probability of each completion
based on the initial set of parameters we have. This is shown in Figure 17.4, which lists
for each case di the probability of each of its completions, Prθ0(ci|di), where Ci are the
variables with missing values in case di.
Note that the completed data set deﬁnes an (expected) empirical distribution, shown
in Figure 17.4(b). According to this distribution, the probability of an instantiation, say,
a1, b1, c2, d2 is computed by considering all its occurrences in the completed data set.
However, instead of simply counting the number of such occurrences, we add up the
probabilities of seeing them in the completed data set. There are three occurrences of the
instantiation a1, b1, c2, d2 in the completed data set, which result from completing the
cases d1, d2, and d5. Moreover, the probability of seeing these completions is given by
PrD,θ0(a1, b1, c2, d2) = Prθ0(a1, d2|d1) + Prθ0(a1, c2|d2) + Prθ0(a1, c2|d5)
N
= .444 + .326 + .326
5
= .219
Note that we are using PrD,θ0(.) to denote the expected empirical distribution based on
parameters θ0. More generally, we have the following deﬁnition.
Deﬁnition 17.2. The expected empirical distribution of data set D under parameters θk
is deﬁned as
PrD,θk(α)
def=
1
N

di,ci|=α
Prθk(ci|di),
where α is an event and Ci are the variables with missing values in case di.
■
6 When the data is complete, each term in the product can be evaluated using the chain rule for Bayesian networks.

448
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
D
A
B
C
D
Prθ0(Ci|di)
d1
?
b1
c2
?
a1
b1
c2
d1
.111 = Prθ0(a1, d1|b1, c2)
a1
b1
c2
d2
.444
a2
b1
c2
d1
.089
a2
b1
c2
d2
.356
d2
?
b1
?
d2
a1
b1
c1
d2
.326 = Prθ0(a1, c1|b1, d2)
a1
b1
c2
d2
.326
a2
b1
c1
d2
.087
a2
b1
c2
d2
.261
d3
?
b2
c1
d1
a1
b2
c1
d1
.122 = Prθ0(a1|b2, c1, d1)
a2
b2
c1
d1
.878
d4
?
b2
c1
d1
a1
b2
c1
d1
.122 = Prθ0(a1|b2, c1, d1)
a2
b2
c1
d1
.878
d5
?
b1
?
d2
a1
b1
c1
d2
.326 = Prθ0(a1, c1|b1, d2)
a1
b1
c2
d2
.326
a2
b1
c1
d2
.087
a2
b1
c2
d2
.261
A
B
C
D
PrD,θ0(.)
a1
b1
c1
d1
0
a1
b1
c1
d2
.130
a1
b1
c2
d1
.022
a1
b1
c2
d2
.219
a1
b2
c1
d1
.049
a1
b2
c1
d2
0
a1
b2
c2
d1
0
a1
b2
c2
d2
0
a2
b1
c1
d1
0
a2
b1
c1
d2
.035
a2
b1
c2
d1
.018
a2
b1
c2
d2
.176
a2
b2
c1
d1
.351
a2
b2
c1
d2
0
a2
b2
c2
d1
0
a2
b2
c2
d2
0
(a) Completed data set with expected values
(b) Expected empirical
of completed cases
distribution
Figure 17.4: Completing a data set using the probability distribution Prθ0(.) deﬁned by the Bayesian
network in Figure 17.3.
Recall that di, ci |= α means that event α is satisﬁed by complete case di, ci. Hence,
we are summing Prθk(ci|di) for all cases di and their completions ci that satisfy event α.
When the data set is complete, PrD,θk(.) reduces to the empirical distribution PrD(.),
which is independent of parameters θk. Moreover, N · PrD,θk(x) is called the expected
count of instantiation x in data set D, just as N · PrD(x) represents the count of instanti-
ation x in a complete data set D.
We can now use this expected empirical distribution to estimate parameters, just as we
did for complete data. For example, we have the following estimate for parameter θc1|a2:
θ1
c1|a2 = PrD,θ0(c1|a2) = PrD,θ0(c1, a2)
PrD,θ0(a2)
= .666
Figure 17.5 depicts all parameter estimates based on the expected empirical distribution
PrD,θ0(.), leading to the new estimates θ1 with likelihood
L(θ1|D) =
5

i=1
Prθ1(di)
= (.290)(.560)(.255)(.255)(.560)
= 5.9 × 10−3
> L(θ0|D).
Hence, the new estimates have a higher likelihood than the initial ones we started with.
This holds more generally as we show next.

17.3 ESTIMATING PARAMETERS FROM INCOMPLETE DATA
449
A
B
C
D
A
θ1
a
a1
.420
a2
.580
A
B
θ1
b|a
a1
b1
.883
a1
b2
.117
a2
b1
.395
a2
b2
.605
A
C
θ1
c|a
a1
c1
.426
a1
c2
.574
a2
c1
.666
a2
c2
.334
B
D
θ1
d|b
b1
d1
.067
b1
d2
.933
b2
d1
1
b2
d2
0
Figure 17.5: A Bayesian network inducing a probability distribution Prθ1(.).
Deﬁnition 17.3. The EM estimates for data set D and parameters θk are deﬁned as
θk+1
x|u
def= PrD,θk(x|u).
(17.6)
■
That is, EM estimates are based on the expected empirical distribution, just as our
estimates for complete data are based on the empirical distribution. We now have the
following key result.
Corollary 11. EM estimates satisfy the following property:
LL(θk+1|D) ≥LL(θk|D).
■
This is a corollary of Theorems 17.5 and 17.6 (to be discussed later), which characterize
the EM algorithm and also explain its name. However, before we discuss these theorems
we show that EM estimates can be computed without constructing the expected empirical
distribution. This will form the basis of the EM algorithm to be presented later.
Theorem 17.4. The expected empirical distribution of data set D given parameters θk
can be computed as
PrD,θk(α) = 1
N
N

i=1
Prθk(α|di).
■
That is, we simply iterate over the data set cases while computing the probability of α
given each case (i.e., no need to explicitly consider the completion of each case). The EM
estimates for data set D and parameters θk can now be computed as
θk+1
x|u =
N
i=1 Prθk(xu|di)
N
i=1 Prθk(u|di)
.
(17.7)
Note that contrary to (17.6), (17.7) does not reference the expected empirical distribution.
Instead, this equation computes EM estimates by performing inference on a Bayesian
network parameterized by the previous parameter estimates θk. For example,
θ1
c1|a2 =
5
i=1 Prθ0(c1, a2|di)
5
i=1 Prθ0(a2|di)
=
0 + .087 + .878 + .878 + .087
.444 + .348 + .878 + .878 + .348 = .666
Note here that the quantities Prθ0(b1, a2|di) and Prθ0(a2|di) are obtained by performing
inference on the Bayesian network in Figure 17.3, which is parameterized by θ0. This
leads to estimates θ1 depicted in Figure 17.5. The Bayesian network in this ﬁgure can
then be used to obtain estimates θ2 as suggested by (17.7). The process can be repeated
until some convergence criterion is met.

450
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Algorithm 50 ML EM(G, θ0, D)
input:
G:
Bayesian network structure with families XU
θ0:
parametrization of structure G
D:
data set of size N
output: ML/EM parameter estimates for structure G
main:
1: k←0
2: while θk ̸= θk−1 do {this test is different in practice}
3:
cxu←0 for each family instantiation xu
4:
for i = 1 to N do
5:
for each family instantiation xu do
6:
cxu←cxu + Prθk(xu|di) {requires inference on network (G, θk)}
7:
end for
8:
end for
9:
compute parameter estimates θk+1 using θk+1
x|u = cxu/ 
x⋆cx⋆u
10:
k←k + 1
11: end while
12: return θk
Algorithm 50, ML EM, provides the pseudocode for the EM algorithm. The convergence
test on Line 2 of this algorithm is usually not used in practice. Instead, we terminate the
algorithm when the difference between θk and θk−1 is small enough or when the change
in the log-likelihood is small enough.
There are a number of important observations about the behavior of EM. First, the
algorithm may converge to different parameters with different likelihoods depending
on the initial estimates θ0 with which it starts. It is therefore not uncommon to run
the algorithm multiple times, starting with different estimates in each iteration (perhaps
chosen randomly) and then returning the best estimates found across all iterations. Second,
each iteration of the EM algorithm will have to perform inference on a Bayesian network.
In particular, in each iteration the algorithm computes the probability of each instantiation
xu given each case di as evidence. Note here that all these computations correspond to
posterior marginals over network families. Hence, we want to use an algorithm that can
compute family marginals efﬁciently such as the jointree algorithms, which can implement
the inner loop on Line 5 in one jointree propagation. Moreover, it should be noted here
that since a data set typically contains many duplicate cases, we do not need to iterate
over all cases as suggested on Line 4. Instead, it sufﬁces to iterate over distinct cases as
long as the contributions of duplicate cases are accounted for correctly.
We next provide a central result on EM, that immediately implies some of its properties
and also explains its name. First, recall the log-likelihood function,
LL(θ|D) =
N

i=1
log Prθ(di).
We saw previously how we can maximize this function for a complete data set by choosing
parameter estimates based on the empirical distribution
θx|u = PrD(x|u).

17.3 ESTIMATING PARAMETERS FROM INCOMPLETE DATA
451
Consider now the following new function of parameters called the expected log-likelihood
that computes the log-likelihood of parameters but with respect to a completed data set:
ELL(θ|D, θk)
def=
N

i=1

ci

log Prθ(ci, di)

Prθk(ci|di).
As previously, Ci are the variables with missing values in case di. Recall again the EM
estimates based on the expected empirical distribution:
θk+1
x|u = PrD,θk(x|u).
We now have the following result, which draws a parallel between the two cases of
log-likelihood and expected log-likelihood.
Theorem 17.5. EM parameter estimates are the only estimates that maximize the expected
log-likelihood function:7
θk+1 = argmax
θ
ELL(θ|D, θk) iff θk+1
x|u = PrD,θk(x|u).
■
Hence, EM is indeed searching for estimates that maximize the expected log-likelihood
function, which also explains its name.
Theorem 17.6 shows why the EM algorithm works.
Theorem 17.6. Parameters that maximize the expected log-likelihood function cannot
decrease the log-likelihood function:
If θk+1 = argmax
θ
ELL(θ|D, θk), then LL(θk+1|D) ≥LL(θk|D).
■
Theorem 17.6 provides the basis for EM, showing that the estimates it returns can
never have a smaller log-likelihood than the estimates with which it starts. However, we
should stress that the quality of estimates returned by EM will very much depend on the
estimates with which it starts (see Exercise 17.14).
We close this section with the following classical result on EM, showing that it is
capable of converging to every local maxima of the log-likelihood function.
Theorem 17.7. The ﬁxed points of EM are precisely the stationary points of the log-
likelihood function.
■
The EM algorithm is known to converge very slowly if the fraction of missing data is
quite large. It is sometimes sped up using the gradient ascent approach, to be described
next. This is done by ﬁrst running the EM algorithm for a number of iterations to obtain
some estimates and then using these estimates as a starting point of the gradient ascent
algorithm.
17.3.2 Gradient ascent
Another approach for maximizing the log-likelihood function is to view the problem as
one of optimizing a continuous nonlinear function. This is a widely studied problem where
most of the solutions are based on local search, which starts by assuming some initial value
θ0
x|u for each parameter θx|u, and then moves through the parameter space in steps of the
form θk+1
x|u = θk
x|u + δk
x|u. Different algorithms use different values for the increment δk
x|u
7 We assume here that all parameter estimates are well-deﬁned; that is, PrD,θk (u) > 0 for every instantiation u of
every parent set U.

452
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
yet most use gradient information for determining this increment. Recall that for a function
f (v1, . . . , vn), the gradient is the vector of partial derivatives ∂f/∂v1, . . . , ∂f/∂vn. When
evaluated at a particular point (v1, . . . , vn), the gradient gives the direction of the greatest
increase in the value of f . Hence, a direct use of the gradient, called gradient ascent,
suggests that we move in the direction of the gradient by incrementing each variable vi
with η ∂f
∂vi (v1, . . . , vn), where η is a constant known as the learning rate. The learning
rate decides the size of the step we take in the direction of the gradient and must be
chosen carefully, possibly changing its value with different steps. There is a large body
of literature on gradient ascent methods that can be brought to bear on this problem. The
literature contains a number of variants on gradient ascent such as conjugate gradient
ascent, which are known to be more efﬁcient in general.
We note here that we are dealing with a constrained optimization problem as we must
maximize the log-likelihood function LL(θ|D) based on the gradient ∂LL(θ|D)/∂θx|u
while maintaining the constraints θx|u ∈[0, 1] and 
x θx|u = 1. A classic technique for
addressing this issue is to search in an alternative parameter space known as the soft-
max space. That is, we introduce a new variable τx|u with values in (−∞, ∞) for each
parameter θx|u. We then deﬁne
θx|u
def=
eτx|u

x⋆eτx⋆|u .
Any values of the new variables τx|u then induce values for parameters θx|u that satisfy
the given constraints. Note, however, that we must now use the following gradient:
∂LL(θ|D)
∂τx|u
= ∂LL(θ|D)
∂θx|u
∂θx|u
∂τx|u
.
(17.8)
Note also that we must be careful in navigating the soft-max parameter space since
adding a constant value to each of the new variables τx|u will not change the value of para-
meter θx|u.
Applying gradient ascent methods relies on the ability to compute the gradient. The
following result shows how this can be done for the log-likelihood function (see also
Exercise 17.10).
Theorem 17.8. We have
∂LL(θ|D)
∂θx|u
=
N

i=1

1
Prθ(di)
	 ∂Prθ(di)
∂θx|u
(17.9)
=
N

i=1
Prθ(xu|di)
θx|u
, when θx|u ̸= 0.
(17.10)
■
Equation 17.9 allows us to use arithmetic circuits (as discussed in Chapter 12) to
compute the gradient, while (17.10) allows us to compute the gradient using other types
of algorithms such as the jointree algorithm, which do not compute derivatives. We
ﬁnally note that the complexity of computing the gradient is similar to the complexity
of performing an iteration of the EM algorithm. In both cases, we need to compute all
family marginals with respect to each distinct case in the data set. The number of iterations
performed by each algorithm may be different, with the learning rate having a key effect
in gradient ascent algorithms.

17.3 ESTIMATING PARAMETERS FROM INCOMPLETE DATA
453
Missing
Test (I)
Test (T)
Condition
(C)
Missing
Test (I)
Test (T)
Condition
(C)
(a) Ignorable mechanism
(b) Nonignorable mechanism
Figure 17.6: Missing-data mechanisms.
17.3.3 The missing-data mechanism
Consider the network structure C →T discussed previously, where C represents a medical
condition and T represents a test for detecting this condition. Figure 17.6 depicts two
extended network structures for this problem, each including an additional variable I
that indicates whether the test result is missing in the data set. In Figure 17.6(a), the
missing data depends on the condition (e.g., people who do not suffer from the condition
tend not to take the test), while in Figure 17.6(b) the missing data depends on the test
result (e.g., individuals who test negative tend not to report the result). Hence, these
extended structures explicate different dependencies between data missingness and the
values of variables in the data set. We say in this case that the structures explicate different
missing-data mechanisms.
Our goal in this section is to discuss ML estimates that we would obtain with respect
to structures that explicate missing-data mechanisms and compare these estimates with
those obtained when ignoring such mechanisms (e.g., using the simpler structure C →T
as we did previously). We start with the following deﬁnition.
Deﬁnition 17.4. Let G be a network structure, let D be a corresponding data set, and
let M be the variables of G that have missing values in the data set. Let I be a set
of variables called missing-data indicators that are in one-to-one correspondence with
variables M. A network structure that results from adding variables I as leaf nodes to G
is said to explicate the missing-data mechanism and is denoted by GI.
■
In Figure 17.6, I is a missing-data indicator and corresponds to variable T . Note that
I is always observed, as its value is determined by whether the value of T is missing.
We generally use DI to denote an extension of the data set D that includes missing-data
indicators. For example, we have
D
C
T
1
yes
+ve
2
yes
+ve
3
yes
−ve
4
no
?
5
yes
−ve
6
yes
+ve
7
no
?
8
no
−ve
DI
C
T
I
1
yes
+ve
no
2
yes
+ve
no
3
yes
−ve
no
4
no
?
yes
5
yes
−ve
no
6
yes
+ve
no
7
no
?
yes
8
no
−ve
no

454
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Now that we have two different missing-data mechanisms, we can apply the ML approach
in three different ways:
r To the original structure C →T and the data set D
r To the extended structure GI in Figure 17.6(a) and the data set DI
r To the extended structure GI in Figure 17.6(b) and the data set DI
That is, we are ignoring the missing-data mechanism in this ﬁrst case and accounting
for it in the second and third cases. Moreover, all three approaches yield estimates for
variables C and T as they are shared among all three structures. The question we now
face is whether ignoring the missing-data mechanism will change the ML estimates for
these variables.
As it turns out, the ﬁrst and second approaches indeed yield identical estimates that are
different from the ones obtained by the third approach. This suggests that the missing-data
mechanism can be ignored if it corresponds to the one in Figure 17.6(a) but cannot be
ignored if it corresponds to the one in Figure 17.6(b). The following deﬁnition provides
a general condition for when we can ignore the missing-data mechanism.
Deﬁnition 17.5. Let GI be a network structure that explicates the missing-data mecha-
nism of structure G and data set D. Let O be variables that are always observed in the
data set D and let M be the variables that have missing values in the data set. We say
that GI satisﬁes the missing at random (MAR) assumption if I and M are d-separated
by O in structure GI.
■
Intuitively, GI satisﬁes the MAR assumption if once we know the values of variables O,
the speciﬁc values of variables M become irrelevant to whether these values are missing
in the data set. Figure 17.6(a) satisﬁes the MAR assumption: once we know the condition
of a person, the outcome of their test is irrelevant to whether the test result is missing.
Figure 17.6(b) does not satisfy the MAR assumption: even if we know the condition of a
person, the test result may still be relevant to whether it will be missing.
If the MAR assumption holds, the missing-data mechanism can be ignored as shown
by the following theorem.
Theorem 17.9. Let GI and DI be a structure and a data set that explicate the missing-
data mechanism of G and D. Let θ be the parameters of structure G and θI be the
parameters of indicator variables I in structure GI. If GI satisﬁes the MAR assumption,
then
argmax
θ
LL(θ|D) = argmax
θ
max
θI
LL(θ, θI|DI).
■
Hence, under the MAR assumption we obtain the same ML estimates θ whether we
include or ignore the missing-data mechanism.
Consider now the structure in Figure 17.6(b) and let us present an example of how
ignoring this missing-data mechanism can change the ML estimates. To simplify the
example, consider a data set with a single case:
C =no,
T =?.

17.4 LEARNING NETWORK STRUCTURE
455
A
B
C
D
A
θml
a
a1
4/5
a2
1/5
A
B
θml
b|a
a1
b1
3/4
a1
b2
1/4
a2
b1
1
a2
b2
0
A
C
θml
c|a
a1
c1
1/4
a1
c2
3/4
a2
c1
1
a2
c2
0
B
D
θml
d|b
b1
d1
1/4
b1
d2
3/4
b2
d1
1
b2
d2
0
Figure 17.7: A network structure with its maximum likelihood parameters for the data set in (17.11). The
log-likelihood of this structure is −13.3.
If we ignore the missing-data mechanism, that is, compute ML estimates with respect to
structure C →T , we get the following:
r No one has the condition C: θC=no is 1.
r Nothing is learned about the reliability of test T (its parameters are unconstrained).
If we now compute ML estimates with respect to the single case
C =no,
T =?,
I =yes,
and the missing-data mechanism in Figure 17.6(b), we also get that θC=no is 1 but we also
get the following additional constraint (see Exercise 17.18): If the missing-data mechanism
is not trivial – that is, if it is not the case that θI=yes|T =−ve = θI=yes|T =+ve = 1 – then we
must have one of the following:
r The test has a true negative rate of 100% and negative test results are always missing, that
is, θT =−ve|C=no = 1 and θI=yes|T =−ve = 1.
r The test has a false positive rate of 100% and positive test results are always missing, that
is, θT =+ve|C=no = 1 and θI=yes|T =+ve = 1.
These constraints on network parameters are not implied by the ML approach if we
ignore the missing-data mechanism. Again, this should not be surprising as the missing-
data mechanism in Figure 17.6(b) is not ignorable in this case.
17.4 Learning network structure
We have thus far assumed that we know the structure of a Bayesian network and concerned
ourself mostly with estimating the values of its parameters. Our main approach for this
estimation has been to search for ML estimates, that is, ones that maximize the probability
of observing the given data set. We now assume that the structure itself is unknown and
suggest methods for learning it from the given data set. It is natural here to adopt the
same approach we adopted for parameter estimation, that is, search for network structures
that maximize the probability of observing the given data set. We indeed start with this
approach ﬁrst and then show that it needs some further reﬁnements, leading to a general
class of scoring functions for network structures. Our focus here is on learning structures
from complete data. Dealing with incomplete data is similar as far as scoring functions
are concerned but is much more demanding computationally, as becomes apparent in
Section 17.5.

456
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
A
B
C
D
A
θml
a
a1
4/5
a2
1/5
A
B
θml
b|a
a1
b1
3/4
a1
b2
1/4
a2
b1
1
a2
b2
0
A
C
θml
c|a
a1
c1
1/4
a1
c2
3/4
a2
c1
1
a2
c2
0
A
D
θml
d|a
a1
d1
1/2
a1
d2
1/2
a2
d1
0
a2
d2
1
Figure 17.8: A network structure with its maximum likelihood parameters for the data set in (17.11). The
log-likelihood of this structure is −14.1.
Let us now consider Figure 17.7, which depicts a network structure G together with its
ML estimates for the following data set:
D
A
B
C
D
d1
a1
b1
c2
d1
d2
a1
b1
c2
d2
d3
a1
b2
c1
d1
d4
a2
b1
c1
d2
d5
a1
b1
c2
d2
(17.11)
The log-likelihood of this network structure is given by
LL(G|D) = −13.3
Consider now the alternate network structure G⋆depicted in Figure 17.8, together with
its ML estimates. This structure has the following likelihood:
LL(G⋆|D) = −14.1,
which is smaller than the likelihood for structure G. Hence, we prefer structure G to G⋆
if our goal is to search for a maximum likelihood structure.
We next present an algorithm for ﬁnding ML tree structures in time and space that are
quadratic in the number of nodes in the structure.
17.4.1 Learning tree structures
The algorithm we present in this section is based on a particular scoring measure for tree
structures that is expressed in terms of mutual information (see Appendix B). Recall that
the mutual information between two variables X and U is a measure of the dependence
between these variables in some distribution. Our scoring measure is based on mutual
information in the empirical distribution:
MID(X, U)
def=

x,u
PrD(x, u) log
PrD(x, u)
PrD(x)PrD(u).
In particular, given a tree structure G with edges U →X, our scoring measure is given
by
tScore(G|D)
def=

U →X
MID(X, U).
That is, the score is based on adding up the mutual information between every variable and
its single parent in the tree structure. Theorem 17.10 states that trees having a maximal
likelihood are precisely those trees that maximize the previous score.

17.4 LEARNING NETWORK STRUCTURE
457
A
B
C
D
.32
.32
.32
.02
.07
.17
A
B
C
D
(a) Mutual information graph
(b) Maximum spanning tree
A
B
C
D
A
B
C
D
(c) Maximum likelihood tree
(d) Maximum likelihood tree
Figure 17.9: Searching for ML trees using the data set in (17.11).
Theorem 17.10. If G is a tree structure, and D is a complete data set, then
argmax
G
tScore(G|D) = argmax
G
LL(G|D).
■
According to Theorem 17.10, we can ﬁnd a maximum likelihood tree using an algorithm
for computing maximum spanning trees. We now illustrate this algorithm by constructing
ML trees over variables A, B, C, and D, using the data set in (17.11).
The ﬁrst step of the algorithm is constructing a complete, undirected graph over all
variables as shown in Figure 17.9(a). We include a cost with each edge in the resulting
graph representing the mutual information between the two nodes connected by the edge.
We then compute a spanning tree with a maximal cost where the cost of a tree is just
the sum of costs associated with its edges; see Figure 17.9(b). This method generates
an undirected spanning tree that coincides with a number of directed trees. We can then
choose any of these directed trees by ﬁrst selecting a node as a root and then directing
edges away from that root. Figures 17.9(c,d) depict two possible directed trees that
coincide with the maximum spanning tree in Figure 17.9(b). The resulting tree structures
are guaranteed to have a maximal likelihood among all tree structures. For example,
each of the trees in Figures 17.9(c,d) has a log-likelihood of −12.1, which is guaranteed
to be the largest log-likelihood attained by any tree structure over variables A, B, C,
and D.
We can obtain this log-likelihood by computing the probability of each case in the data
set using any of these tree structures (and its corresponding ML estimates). We can also
use Theorem 17.3, which shows that the log-likelihood corresponds to a sum of terms,
one term for each family in the network. For example, if we consider the tree structure G

458
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
B
C
D
A
B
C
D
A
B
C
D
A
(a) Tree
(b) DAG
(c) Complete DAG
Figure 17.10: Improving the likelihood of a network structure by adding edges.
in Figure 17.9(c), this theorem gives
LL(G|D)
= −N × (ENTD(A|C) + ENTD(B) + ENTD(C|B) + ENTD(D|B))
= −5 × (.400 + .722 + .649 + .649)
= −12.1.
(17.12)
Note how the terms correspond to the families of given tree structure: AC, B, CB,
and DB.
17.4.2 Learning DAG structures
Suppose now that our goal is to ﬁnd a maximum likelihood structure but without restricting
ourselves to tree structures. Also consider the DAG structure in Figure 17.10(b), which
is obtained by adding an edge D →A to the tree structure in Figure 17.10(a); this is the
same ML tree in Figure 17.9(c). Using Theorem 17.3, the log-likelihood of this DAG is
given by
LL(G|D)
= −N × (ENTD(A|C, D) + ENTD(B) + ENTD(C|B) + ENTD(D|B))
= −5 × (0 + .722 + .649 + .649)
= −10.1,
which is larger than the log-likelihood of the tree in Figure 17.10(a). Note also that the
only difference between the two likelihoods is the entropy term for variable A since this
is the only variable with different families in the two structures. In particular, the family
of A is AC in the tree and is ACD in the DAG. Moreover,
ENTD(A|C, D) < ENTD(A|C),
and, hence,
−ENTD(A|C, D) > −ENTD(A|C),
which is why the DAG has a larger log-likelihood than the tree. However, this is not
completely accidental as shown by Theorem 17.11 .
Theorem 17.11. If U ⊆U⋆, then ENT(X|U) ≥ENT(X|U⋆).
■

17.4 LEARNING NETWORK STRUCTURE
459
 0
 10
 20
 30
 40
 50
 60
-5
 0
 5
 10
 15
 20
 25
 0
 10
 20
 30
 40
 50
 60
-5
 0
 5
 10
 15
 20
 25
(a) Straight line
(b) Fourth-degree polynomial
Figure 17.11: The problem of overﬁtting.
That is, by adding more parents to a variable, we never increase its entropy term and,
hence, never decrease the log-likelihood of resulting structure.
Corollary 12. If DAG G⋆is the result of adding edges to DAG G, then
LL(G⋆|D) ≥LL(G|D).
■
According to this corollary, which follows immediately from Theorems 17.3 and 17.11,
if we simply search for a network structure with maximal likelihood, we end up choosing a
complete network structure, that is, a DAG to which no more edges can be added (without
introducing directed cycles).8
Complete DAGs are undesirable for a number of reasons. First, they make no assertions
of conditional independence and, hence, their topology does not reveal any properties of
the distribution they induce. Second, a complete DAG over n variables has a treewidth of
n −1 and is therefore impossible to work with practically. Third and most importantly,
complete DAGs suffer from the problem of overﬁtting, which refers to the use of a model
that has too many parameters compared to the available data.
The classical example for illustrating the problem of overﬁtting is that of ﬁnding a
polynomial that ﬁts a given set of data points (x1, y1), . . . , (xn, yn). Consider the following
data points for an example:
x
y
1
1.10
5
4.5
10
11
15
14.5
20
22
Looking at this data set, we expect the relationship between x and y to be linear, suggesting
a model of the form y = ax + b; see Figure 17.11(a). Yet the linear function does not
provide a perfect ﬁt for the data as the relationship is not precisely linear. However, if
we insist on a perfect ﬁt we can use a fourth-degree polynomial that is guaranteed to ﬁt
the data perfectly (we have ﬁve data points and a fourth-degree polynomial has ﬁve free
8 Recall that there are n! complete DAGs over n variables. Each of these DAGs corresponds to a total variable
ordering X1, . . . , Xn in which variable Xi has X1, . . . , Xi−1 as its parents.

460
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
parameters). Figure 17.11(b) depicts a perfect ﬁt for the data using such a polynomial. As
is clear from this ﬁgure, even though the ﬁt is perfect the polynomial does not appear to
provide a good generalization of the data beyond the range of the observed data points.
In summary, the problem of overﬁtting materializes when we focus on learning a model
that ﬁts the data well without constraining enough the number of free model parameters.
The result is that we end up adopting models that are more complex than necessary.
Moreover, such models tend to provide poor generalizations of the data and, therefore,
perform poorly on cases that are not part of the given data set.
Even though there is no agreed upon solution to the problem of overﬁtting, all available
solutions tend to be based on a common principle known as Occam’s Razor, which says
that we should prefer simpler models over more complex models, others things being
equal. To realize this principle, we need a measure of model complexity and a method
for balancing the complexity of a model with its data ﬁt. For Bayesian networks (and
many other modeling frameworks), model complexity is measured using the number of
independent parameters in the model.
Deﬁnition 17.6. Let G be a DAG over variables X1, . . . , Xn with corresponding parents
U1, . . . , Un and let Y# denote the number of instantiations for variables Y. The dimension
of DAG G is deﬁned as
||G||
def=
n

i=1
||XiUi||
||XiUi||
def=
(Xi
# −1)Ui
#.
■
The dimension of a DAG is therefore equal to the number of independent parameters in
its CPTs. Note that for a given parent instantiation ui, all but one of the parameters θxi|ui
are independent. For example, considering Figure 17.10 and assuming that variables are
all binary, we have the following dimensions for the depicted network structures (from
left to right): 7, 9, and 15.
Using this notion of model complexity, we can deﬁne the following common class of
scoring measures for a network structure G and data set D of size N:
Score(G|D)
def= LL(G|D) −ψ(N) · ||G||.
(17.13)
The ﬁrst component of this score, LL(G|D), is the log-likelihood function we con-
sidered previously. The second component, ψ(N) · ||G||, is a penalty term that favors
simpler models, that is, ones with a smaller number of independent parameters. Note
that this penalty term has a weight, ψ(N) ≥0, which is a function of the data set
size N.
When the penalty weight ψ(N) is a constant that is independent of N, we obtain a
score in which model complexity is a secondary issue. To see this, note that the log-
likelihood function LL(G|D) grows linearly in the data set size N (see (17.3)), and will
quickly dominate the penalty term. In this case, the model complexity will only be used to
distinguish between models that have relatively equal log-likelihood terms. This scoring
measure is known as the Akaike information criterion (AIC).
Another yet more common choice of the penalty weight is ψ(N) = log2 N
2
, which leads
to a more inﬂuential penalty term. Note, however, that this term grows logarithmically
in N, while the log-likelihood term grows linearly in N. Hence, the inﬂuence of model
complexity decreases as N grows, allowing the log-likelihood term to eventually dominate

17.5 SEARCHING FOR NETWORK STRUCTURE
461
the score. This penalty weight gives rise to the minimum description length (MDL)
score:
MDL(G|D)
def= LL(G|D) −
log2 N
2
	
||G||.
(17.14)
For example, the structure in Figure 17.10(a) has the following MDL score:
= −12.1 −
log2 5
2
	
(7)
= −12.1 −8.1
= −20.2,
while the one in Figure 17.10(b) has the following score:
= −10.1 −
log2 5
2
	
(9)
= −10.1 −10.4
= −20.5
Therefore, the MDL score prefers the ﬁrst structure even though it has a smaller log-
likelihood.
The MDL score is also known as the Bayesian information criterion (BIC). It is
sometimes expressed as the negative of the score in (17.14), where the goal is to minimize
the score instead of maximizing it. Note, however, that both the scores in (17.13) and
(17.14) are ≤0.
17.5 Searching for network structure
Searching for a network structure that optimizes a particular score can be quite expensive
due to the very large number of structures one may need to consider. As a result, greedy
algorithms tend to be of more practical use when learning network structures. Systematic
search algorithms can also be practical but only under some conditions. However, both
classes of algorithms rely for their efﬁcient implementation on a property that most scoring
functions have. This property is known as decomposability or modularity as it allows us
to decompose the score into an aggregate of local scores, one for each network family.
Consider for example, the score given in (17.13) and let XU range over the families of
DAG G. This score can be decomposed as
Score(G|D) =

XU
Score(X, U|D),
(17.15)
where
Score(X, U|D)
def=
−N · ENTD(X|U) −ψ(N) · ||XU||.
Note how the score is a sum of local scores, each contributed by some family XU. Note
also how the contribution of each family is split into two parts: one resulting from a
decomposition of the log-likelihood component of the score (see (17.3)) and the other
resulting from a decomposition of the penalty component (see Deﬁnition 17.6).
The previous decomposition enables a number of heuristic and optimal search algo-
rithms to be implemented more efﬁciently. We discuss some of these methods next.

462
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
Add
Reverse
Remove
Add
Figure 17.12: Local search for a network structure.
17.5.1 Local search
One can search for a network structure by starting with some initial structure and then
modifying it locally to increase its score. The initial structure can be chosen randomly,
based on some prior knowledge, or can be a tree structure constructed optimally from
data using the quadratic algorithm discussed previously. The local modiﬁcations to the
structure are then constrained to adding an edge, removing an edge, or reversing an edge
while ensuring that the structure remains a DAG (see Figure 17.12). These local changes to
the network structure also change the score, possibly increasing or decreasing it. However,
the goal is to commit to the change that increases the score the most. If none of the local
changes can increase the score, the algorithm terminates and returns the current structure.
A number of observations are in order about this local search algorithm. First, it is
not guaranteed to return an optimal network structure, that is, one that has the largest
score. The only guarantee provided by the algorithm is that the structure it returns is
locally optimal in that no local change can improve its score. This suboptimal behavior of
local search can usually be improved by techniques such as random restarts. According
to this technique, we would repeat the local search multiple times, each time starting
with a different initial network, and then return the network with the best score across all
repetitions.
The second observation about the previous local search algorithms relates to updating
the score after applying local changes. Consider again Figure 17.12 and let G be the net-
work structure in the center and G⋆be the network structure that results from deleting edge
A →B. Since this change affects only the family of node B and given the decomposition
of (17.15), we have
Score(G⋆|D) = Score(G|D) −Score(B, A|D) + Score(B|D).
That is, to compute the new score, we subtract the contribution of B’s old family,
Score(B, A|D), and then add the contribution of B’s new family, Score(B|D). More
generally, adding or removing an edge changes only one family, while reversing an edge
changes only two families. Hence, the score can always be updated locally as a result of
the local network change induced by adding, removing, or reversing an edge.

17.5 SEARCHING FOR NETWORK STRUCTURE
463
X5
X4
X3
X2
X1
X5
X4
X3
X2
X1
X5
X4
X3
X2
X1
Figure 17.13: Greedy search for a parent set for variable X5.
17.5.2 Constraining the search space
A common technique for reducing the search space size is to assume a total ordering on
network variables and then search only among network structures that are consistent with
the chosen order. Suppose for example that we use the variable order X1, . . . , Xn. The
search process can now by viewed as trying to ﬁnd for each variable Xi a set of parents
Ui ⊆X1, . . . , Xi−1.
Not only does this technique reduce the size of search space, it also allows us
to decompose the search problem into n independent problems, each concerned with
ﬁnding a set of parents for some network variable. That is, the search problem now
reduces to considering each variable Xi independently and then ﬁnding a set of par-
ents Ui ⊆X1, . . . , Xi−1 that maximize the corresponding local score Score(Xi, Ui|D).
We next discuss both greedy and optimal methods for maximizing these local
scores.
Greedy search
One of the more effective heuristic algorithms for optimizing a family score is known as
K3.9 This algorithm starts with an empty set of parents, successively adding variables to the
set one at a time until such additions no longer increase the score. Consider Figure 17.13
for an example, where the goal is to ﬁnd a set of parents for X5 from the set of variables
X1, . . . , X4. The K3 algorithm will start by setting U5 to the empty set and then ﬁnds a
variable Xi (if any), i = 1, . . . , 4, that maximizes
Score(X5, Xi|D) ≥Score(X5|D).
Suppose that X3 happens to be such a variable. The algorithm then sets U5 = {X3} and
searches for another variable Xi in X1, X2, X4 that maximizes
Score(X5, X3Xi|D) ≥Score(X5, X3|D).
Suppose again that X2 happens to be such a variable, leading to the new set of parents
U5 = {X2, X3}. It may happen that adding X1 to this set does not increase the score and
neither will adding X4. In this case, K3 terminates, returning U5 = {X2, X3} as the parent
set for X5.
9 The name K3 refers to the version of this algorithm that optimizes the MDL score, although it also applies to other
scores as well. Another version of this heuristic algorithm, called K2, works similarly but uses a different score to
be discussed in Chapter 18.

464
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
{}
{X1}
{X2}
{X3}
{X4}
{X1, X2}
{X1, X3}
{X1, X4}
{X2, X3}
{X2, X4}
{X3, X4}
{X1, X2, X3}
{X1, X2, X4}
{X1, X3, X4}
{X2, X3, X4}
{X1, X2 , X3, X4}
(a) Order X1, X2, X3, X4
{}
{X4}
{X2}
{X3}
{X1}
{X4, X2}
{X4, X3}
{X4, X1}
{X2, X3}
{X2, X1}
{X3, X1}
{X4, X2, X3}
{X4, X2, X1}
{X4, X3, X1}
{X2, X3, X1}
{X4, X2 , X3, X1}
(b) Order X4, X2, X3, X1
Figure 17.14: A search tree for identifying an optimal parent set for variable X5, expanded according to
two different variable orders.
K3 is a greedy algorithm that is not guaranteed to identify the optimal set of parents Ui,
that is, the one that maximizes Score(Xi, Ui|D). Therefore, it is not uncommon to use the
structure obtained by this algorithm as a starting point for other algorithms, such as the
local search algorithm discussed previously or the optimal search algorithm we discuss
next.
Optimal search
We next discuss an optimal search algorithm for network structures that is based on
branch-and-bound depth-ﬁrst search. Similar to K3, the algorithm assumes a total order
of network variables X1, . . . , Xn and searches only among network structures that are

17.5 SEARCHING FOR NETWORK STRUCTURE
465
consistent with this order. As mentioned previously, this allows us to decompose the
search process into n independent search problems.
Figure 17.14(a) depicts a search tree for ﬁnding a set of parents U5 for variable
X5 assuming a total order X1, . . . , Xn. The ﬁrst level of this tree has a single node
corresponding to an empty set of parents, U5 = {}, and each additional level is obtained
by adding a single variable to each parent set at the previous level while avoiding the
generation of duplicate sets. Tree nodes are then in one-to-one correspondence with the
possible parent sets for X5. Hence, a search tree for variable Xi has a total of 2i−1 nodes,
corresponding to the number of subsets we can choose from variables X1, . . . , Xi−1.
We can search the tree in Figure 17.14(a) using depth-ﬁrst search while maintaining
the score s of the best parent set visited thus far. When visiting a node Ui, we need
to evaluate Score(Xi, Ui|D) and check whether it is better than the score s obtained
thus far. Depth-ﬁrst search guarantees that every parent set is visited, leading us to
identify an optimal parent set at the end of the search. However, this optimality comes
at the expense of exponential complexity as we have to consider 2i−1 parent sets for
variable Xi.
The complexity of this algorithm can be improved on average if we can compute for
each search node Ui an upper bound on Score(Xi, U⋆
i |D), where Ui ⊆U⋆
i . If the computed
upper bound at node Ui is not better than the best score s obtained thus far, then we can
prune Ui and all nodes below it in the search tree since none of these parent sets can
be better than the us found thus far. This pruning allows us to escape the exponential
complexity in some cases. Clearly, the extent of pruning depends on the quality of upper
bound used. Theorem 17.12 provides an upper bound for the MDL score.
Theorem 17.12. Let Ui be a parent set and let U+
i be the largest parent set appearing
below Ui in the search tree. If U⋆
i is a parent set in the tree rooted at Ui, then
MDL(Xi, U⋆
i |D) ≤−N · ENTD(Xi|U+
i ) −ψ(N) · ||XiUi||.
■
This bound needs to be computed at each node Ui in the search tree. Moreover,
unless the bound is greater than the best score obtained thus far, we can prune all nodes
U⋆
i . Consider Figure 17.14(a) for an example. At the search node U5 = {X2}, we get
U+
5 = {X2, X3, X4}. Moreover, U⋆
5 ranges over parent sets {X2}, {X2, X3}, {X2, X4}, and
{X2, X3, X4}.
Consider now the search tree in Figure 17.14(b) compared to the one in Figure 17.14(a).
Both trees enumerate the sixteen parent sets for variable X5 yet the order of enumeration
is different. Consider for example the ﬁrst branch in each tree. Variable X1 appears
more frequently in the ﬁrst branch of Figure 17.14(a), while variable X4 appears more
frequently in the ﬁrst branch of Figure 17.14(b). Suppose now that variable X4 tends
to reduce the entropy of variable X5 more than does X1. We then expect the search in
Figure 17.14(b) to visit parent sets with higher scores ﬁrst. For example, the third node
visited in Figure 17.14(a) is {X1, X2}, while the third node visited in Figure 17.14(b)
is {X4, X2}. Visiting parent sets with higher scores earlier leads to more aggressive
pruning, which is why the search tree in Figure 17.14(b) is preferred in this case. That
is, we prefer a tree that is expanded according to a variable order Xk1, . . . , Xki−1, where
ENT(Xi|Xk1) ≤· · · ≤ENT(Xi|Xki−1). It is for this reason that this order of expansion is
sometimes adopted by optimal search algorithms.
We close this section by pointing out that our discussion on the search for net-
work structures has been restricted to complete data sets. The main reason for this is

466
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
computational. For example, the likelihood of a network structure does not admit a closed
form when the data set is incomplete. Moreover, it does not decompose into components
as given by Theorem 17.3. Hence, algorithms for learning structures with incomplete
data typically involve two searches: an outer search in the space of network structures
and an inner search in the space of network parameters. We provide some references
in the bibliographic remarks section on some techniques for implementing these double
searches.
Bibliographic remarks
The EM algorithm was introduced in Dempster et al. [1977] and adopted to Bayesian
networks in Lauritzen [1995]. The gradient ascent algorithm was proposed in Russell
et al. [1995], which presented the gradient form of Theorem 17.8 (see also Thiesson
[1995]). The foundations for handling missing data, including the introduction of the
MAR assumption, is given in Rubin [1976]. A more comprehensive treatment of the
subject is available in Little and Rubin [2002].
The quadratic algorithm for learning ML tree structures was proposed by Chow and
Liu [1968]. The MDL principle was introduced in Rissanen [1978] and then adopted into
a score for Bayesian network structures in Bouckaert [1993], Lam and Bacchus [1994],
and Suzuki [1993]. The BIC score, which is the same as the MDL score, was introduced
in Schwarz [1978] as an approximation of the marginal likelihood function, discussed in
Chapter 18. The AIC score was introduced in Akaike [1974].
The K2 algorithm was proposed by Cooper and Herskovits [1991; 1992]. The branch-
and-bound algorithm with a ﬁxed variable ordering is due to Suzuki [1993]. The version
discussed in this chapter is due to Tian [2000], which proposed the MDL bound of
Theorem 17.12.
The complexity of learning network structures is shown to be NP-hard in Chickering
[1996]. Methods for learning network structures in the case of incomplete data are pre-
sented in Friedman [1998], Meilua and Jordan [1998], Singh [1997], and Thiesson et al.
[1999], while methods for learning Bayesian networks with local structure in the CPTs
are discussed in Buntine [1991], Díez [1993], Chickering et al. [1997], Friedman and
Goldszmidt [1996], and Meek and Heckerman [1997].
A comprehensive coverage of information theory concepts is given in Cover and
Thomas [1991], including a thorough discussion of entropy, mutual information, and
the KL divergence. A tutorial on learning Bayesian networks is provided in Heckerman
[1998], which surveys most of the foundational work on the subject and provides many
references for more advanced topics than covered in this chapter. Related surveys of the
literature can also be found in Buntine [1996] and Jordan [1998]. The subject of learning
Bayesian networks is also discussed in Cowell et al. [1999] and Neapolitan [2004], which
provide more details on some of the subjects discussed here and cover additional topics
such as networks with continuous variables.
Another approach for learning Bayesian network structures, known as the constraint-
based approach, follows more closely the deﬁnition of Bayesian networks as encoders
of conditional independence relationships. According to this approach, we make some
judgments about the (conditional) dependencies and independencies that follow from the
data and then use them as constraints to reconstruct the network structure. For example,
if we determine from the data that variable X is independent of Y given Z, then we can
infer that there is no edge between X and Y in the network structure. Two representative

17.6 EXERCISES
467
algorithms of constraint-based learning are the IC algorithm [Pearl, 2000] and the PC
algorithm Spirtes et al. [2001]. An orthogonal perspective on learning network structures,
based on causality is discussed in Heckerman [1995], Glymour and Cooper [1999], and
Pearl [2000].
One of the main reasons for learning Bayesian networks is to use them as classiﬁers,
and one of the more common Bayesian network classiﬁers is the naive Bayes classiﬁer
[Duda and Hart, 1973], which is described in Exercise 17.15. Quite a bit of attention
has been given to this class of classiﬁers and its extensions, such as the tree-augmented
naive Bayes (TAN) classiﬁer [Friedman et al., 1997; Cerquides and de Mantaras, 2005]
and its variants [Eamonn J. Keogh, 2002; Webb et al., 2005; Jing et al., 2005]. When
learning Bayesian network classiﬁers discriminatively, one typically maximizes the con-
ditional log-likelihood function deﬁned in Exercise 17.16 as this will typically lead to
networks that give better classiﬁcation performance; see also [Ng and Jordan, 2001].
Note, however, that network parameters that optimize the conditional log-likelihood func-
tion may not be meaningful probabilistically. Hence, an estimated parameter value, say,
θa|¯b = .8 should not be interpreted as an estimate .8 of the probability Pr(a|¯b) (contrast
this to Theorem 17.1). Moreover, optimizing the conditional log-likelihood function can
be computationally more demanding than optimizing the log-likelihood function since
the decomposition of Theorem 17.3 no longer holds. Approaches have been proposed for
learning arbitrary network structures that optimize the conditional log-likelihood function
(e.g., [Guo and Greiner, 2005; Grossman and Domingos, 2004]) and for estimating the
parameters of these network structures (e.g., [Greiner et al., 2005; Roos et al., 2005]).
17.6 Exercises
17.1. Consider a Bayesian network structure with the following edges A →B, A →C, and
A →D. Compute the ML parameter estimates for this structure given the following data
set:
Case
A
B
C
D
1
T
F
F
F
2
T
F
F
T
3
F
F
T
F
4
T
T
F
T
5
F
F
T
T
6
F
T
T
F
7
F
T
T
T
8
T
F
F
T
9
F
F
T
F
10
T
T
T
T
17.2. Consider a Bayesian network structure with edges A →B and B →C. Compute the ML
parameter estimates for this structure given the following data set:
Case
A
B
C
1
T
F
F
2
T
F
F
3
F
F
T
4
T
F
F
Are the ML estimates unique for this data set? If not, how many ML estimates do we have in
this case?

468
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
17.3. Let G be a network structure with families XU, let D be a complete data set, and suppose
that PrD(u) = 0 for some instantiation u of parent set U. Show that the likelihood function
LL(θ|D) is independent of parameters θx|u for all x.
17.4. Let G be a network structure with families XU and let D be a complete data set. Prove the
following form for the likelihood of structure G:
L(G|D) =

XU

xu

θml
x|u
D#(xu)
,
where θml
x|u is the ML estimate for parameter θx|u.
17.5. Consider a Bayesian network with edges A →B, A →C, and the parameters θ:
A
θa
T
.3
F
.7
A
B
θb|a
T
T
.5
T
F
.5
F
T
.8
F
F
.2
A
C
θc|a
T
T
.1
T
F
.9
F
T
.5
F
F
.5
Consider the following data set D:
Case
A
B
C
1
F
?
T
2
F
T
T
3
?
F
T
4
?
T
F
5
T
F
?
What is the expected empirical distribution for data set D given parameters θ, PrD,θ(.)?
17.6. Consider Exercise 17.5 and assume that the given CPTs are the initial CPTs used by EM.
Compute the EM parameter estimates after one iteration of the algorithm.
17.7. Consider Exercise 17.5 and assume that the given CPTs are those used by the gradient
ascent approach for estimating parameters. Decide whether this approach increases or
decreases the values of parameters θ¯a, θb|a, and θ¯c|a in its ﬁrst iteration. Use the soft-max
search space to impose parameter constraints.
17.8. Consider a complete data set D and let G1 and G2 be two complete DAGs. Prove or disprove
LL(G1|D) = LL(G2|D). If the equality does not hold, provide a counterexample.
17.9. What happens if we apply the EM algorithm to a complete data set? That is, what estimates
does it return as a function of the initial estimates with which it starts?
17.10. Compute the derivative ∂θx|u
∂τx|u
given in Equation 17.8.
17.11. Compute an ML tree structure for the data set in Exercise 17.1. What is the total number of
ML tree structures for this data set?
17.12. Consider a Bayesian network structure X →Y and a data set D of size N, where X is
hidden and Y is observed in every case of D. Show that ML estimates are characterized by

x
θxθy|x = D#(y)
N
for all values y, that is, ML estimates are those that ensure Pr(y) = D#(y)/N for all y.
17.13. Consider a Bayesian network structure X →Y and Y →Z and a data set D of size N in
which Y is hidden yet X and Z are observed in every case of D. Provide a characterization
of the ML estimates for this problem, together with some concrete parameter estimates that
satisfy the characterization.

17.6 EXERCISES
469
C
A2
Am
A1
Figure 17.15: A naive Bayes network structure.
17.14. Consider the naive Bayes network structure in Figure 17.15 and suppose that D is a data
set in which variable C is hidden and variables A1, . . . , Am are observed in all cases. Show
that if EM is applied to this problem with parameters having uniform initial values, then it will
converge in one step, returning the following estimates:
θc =
1
|C|
θai|c = D#(ai)
N
.
Here |C| is the cardinality of variable C, N is the size of given data set, and D#(ai) is the
number of cases in the data set that contain instantiation ai of variable Ai.
17.15. Consider the naive Bayes network structure in Figure 17.15 and let Prθ(.) be the distribution
induced by this structure and parametrization θ. Let us refer to variable C as the class
variable, variables A1, . . . , Am as the attributes, and each instantiation a1, . . . , am as an
instance. Suppose that we now use this network as a classiﬁer where we assign to each
instance a1, . . . , am the class c that maximizes Prθ(c|a1, . . . , am). Show that when variable
C is binary, the class of instance a1, . . . , am is c if
log θc +
m

i=1
log θai|c > log θ¯c +
m

i=1
log θai|¯c.
Note: This is known as a naive Bayes classiﬁer.
17.16. Consider Exercise 17.15, let D be a complete data set that contains the cases ci, ai
1, . . . , ai
m
for i = 1, . . . , N, and let PrD(.) be the empirical distribution induced by data set D. Deﬁne
the conditional log-likelihood function as
CLL(D|θ)
def=
N

i=1
log Prθ(ci|ai
1, . . . , ai
m).
Deﬁne also the conditional KL divergence as

a1,...,am
PrD(a1, . . . , am) · KL(PrD(C|a1, . . . , am), Prθ(C|a1, . . . , am)).
Show that a parametrization θ maximizes the conditional log-likelihood function iff it mini-
mizes the conditional KL divergence.
17.17. Consider Exercise 17.16 and let D′ be an incomplete data set that is obtained from data set
D by removing the values of class variable C, that is, D′ contains the cases ai
1, . . . , ai
m for
i = 1, . . . , N. Show that
LL(D|θ) = CLL(D|θ) + LL(D′|θ).
17.18. Consider the structure in Figure 17.6(b) and a data set with a single case:
C =no,
T =?,
I =yes.
Show that the ML estimates are characterized as follows:
r θC=no is 1.
r
One of the following must be true for the parameters of test T and missing-data indi-
cator I:

470
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
r θT =−ve|C=no = 1 and θI=yes|T =−ve = 1
r θT =+ve|C=no = 1 and θI=yes|T =+ve = 1
r θI=yes|T =−ve = θI=yes|T =+ve = 1.
17.19. Consider the data set D in Exercise 17.1 and the following network structures:
r G1: A →B, B →C, C →D, and A →D
r G2: A →B, B →C, C →D, and B →D.
Which structure has a higher likelihood given D? What is the exact value for LL(G1|D) −
LL(G2|D)?
17.20. Compute the MDL score for the network structure in Figure 17.10(c) given the data set
in (17.11).
17.7 Proofs
Lemma 17.1. Consider a ﬁxed distribution Pr⋆(X) and a variable distribution Pr(X).
Then
Pr⋆= argmax
Pr

x
Pr⋆(x) log Pr(x),
and Pr⋆is the only distribution that satisﬁes this property, that is, Pr⋆is the only distribution
that maximizes this quantity.
PROOF. First, note the following key property of the KL divergence:
KL(Pr⋆(X), Pr(X)) =

x
Pr⋆(x) log Pr⋆(x)
Pr(x) ≥0,
where KL(Pr⋆, Pr) = 0 if and only if Pr⋆= Pr [Cover and Thomas, 1991]. It then imme-
diately follows that

x
Pr⋆(x) log Pr⋆(x) ≥

x
Pr⋆(x) log Pr(x),
with the equality holding if and only if Pr⋆= Pr. Hence, Pr⋆is the only distribution that
maximizes the given quantity.
■
PROOF OF THEOREM 17.1. As given in the proof of Theorem 17.3, we have
LL(θ|D) = N

XU

xu
PrD(xu) log Prθ(x|u)
= N

XU

u
PrD(u)

x
PrD(x|u) log Prθ(x|u).
Therefore, the log-likelihood function decomposes into independent components that
correspond to parent instantiations u:

x
PrD(x|u) log Prθ(x|u).
Each of these components can be maximized independently using Lemma 17.1, which
says that the distribution Prθ(x|u) that maximizes this quantity is unique and is equal to
PrD(x|u).
■

17.7 PROOFS
471
PROOF OF THEOREM 17.2.
KL(PrD(X), Prθ(X)) =

x
PrD(x) log PrD(x)
Prθ(x)
=

x
PrD(x) log PrD(x) −

x
PrD(x) log Prθ(x).
Since the term 
x PrD(x) log PrD(x) does not depend on the choice of parameters
θ, minimizing the KL divergence KL(PrD(X), Prθ(X)) corresponds to maximizing

x PrD(x) log Prθ(x). We also have

x
PrD(x) log Prθ(x) =

x
D#(x)
N
log Prθ(x)
= 1
N

x
D#(x) log Prθ(x)
= 1
N

x

di=x
log Prθ(di)
= 1
N
N

i=1
log Prθ(di)
= 1
N log
N

i=1
Prθ(di)
= log L(θ|D)
N
.
Hence, minimizing the KL divergence KL(PrD(X), Prθ(X)) is equivalent to maximizing
log L(θ|D), which is equivalent to maximizing L(θ|D).
■
PROOF OF THEOREM 17.3. We ﬁrst consider the decomposition of the log-likelihood
function LL(θ|D) for an arbitrary set of parameter estimates θ:
LL(θ|D) =
N

i=1
log Prθ(di)
=
N

i=1
log

di|=xu
Prθ(x|u)
by the chain rule of Bayesian networks
=
N

i=1

di|=xu
log Prθ(x|u)
=

XU

xu

di|=xu
log Prθ(x|u)
=

XU

xu
D#(xu) log Prθ(x|u)
= N

XU

xu
PrD(xu) log Prθ(x|u).

472
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Let us now consider ML estimates θml:
LL(θml|D) = N

XU

xu
PrD(xu) log Prθml(x|u)
= N

XU

xu
PrD(xu) log PrD(x|u)
= −N

XU
ENTD(X|U).
This proves the theorem since LL(G|D) = LL(θml|D).
■
PROOF OF THEOREM 17.4. By Deﬁnition 17.2, we have
PrD,θk(α) = 1
N

dici|=α
Prθk(ci|di)
= 1
N

dici|=α
Prθk(ci, di|di) by deﬁnition of conditioning
= 1
N

dici
Prθk(ci, di, α|di)
= 1
N

dici
Prθk(ci, α|di) by deﬁnition of conditioning
= 1
N

di
Prθk(α|di) by case analysis
= 1
N
N

i=1
Prθk(α|di).
The third step follows since ci, di, α is inconsistent if dici ̸|= α and ci, di, α is equivalent
to ci, di if dici |= α.
■
PROOF OF THEOREM 17.5. We have
ELL(θ|D, θk) =
N

i=1

ci
Prθk(ci|di) log Prθ(ci, di)
=
N

i=1

ci
Prθk(ci|di) log

cidi|=xu
Prθ(x|u)
=
N

i=1

ci
Prθk(ci|di)

cidi|=xu
log Prθ(x|u)
=

XU

xu

cidi|=xu
Prθk(ci|di) log Prθ(x|u)
=

XU

xu
⎛
⎝
cidi|=xu
Prθk(ci|di)
⎞
⎠log Prθ(x|u)
=

XU

xu
(NPrD,θk(xu)) log Prθ(x|u) by Deﬁnition 17.2
= N

XU

u
PrD,θk(u)

x
PrD,θk(x|u) log Prθ(x|u).

17.7 PROOFS
473
We now have a set of independent components, each corresponding to a parent instantia-
tion u:

x
PrD,θk(x|u) log Prθ(x|u).
These components can be maximized independently using Lemma 17.1, which says
that the only distribution Prθ(.) that maximizes this quantity is PrD,θk(.). Hence, the
parameter estimates given by (17.6) are the only estimates that maximize the log-likelihood
function.
■
PROOF OF THEOREM 17.6. Maximizing the expected log-likelihood function is equiv-
alent to maximizing
N

i=1

ci
Prθk(ci|di) log Prθ(ci, di)
Prθk(ci, di).
(17.16)
This expression is obtained by subtracting the term
Prθk(ci|di) log Prθk(ci, di)
from the expected log-likelihood function. This term does not depend on the sought
parameters θ and, hence, does not change the optimization problem. We now have
N

i=1

ci
Prθk(ci|di) log Prθ(ci, di)
Prθk(ci, di)
=
N

i=1

ci
Prθk(ci|di) log
 Prθ(ci|di)
Prθk(ci|di)
Prθ(di)
Prθk(di)
	
=
N

i=1

ci
Prθk(ci|di) log Prθ(di)
Prθk(di) +

ci
Prθk(ci|di) log Prθ(ci|di)
Prθk(ci|di)

=
N

i=1

log Prθ(di)
Prθk(di) +

ci
Prθk(ci|di) log Prθ(ci|di)
Prθk(ci|di)

=
N

i=1

log Prθ(di)
Prθk(di) −KL(Prθk(Ci|di), Prθ(Ci|di))
	
.
Suppose now that
θk+1 = argmax
θ
N

i=1

log Prθ(di)
Prθk(di) −KL(Prθk(Ci|di), Prθ(Ci|di))
	
.
The quantity maximized above must be ≥0 since choosing θk+1 = θk leads to a zero
value. Replacing θ by the optimal parameters θk+1 then gives
N

i=1

log Prθk+1(di)
Prθk(di) −KL(Prθk(Ci|di), Prθk+1(Ci|di))
	
≥0.
Since the KL divergence is ≥0, we now have
N

i=1
log Prθk+1(di)
Prθk(di) ≥0

474
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
and also
log
N

i=1
Prθk+1(di)
Prθk(di) ≥0.
From the deﬁnition of the likelihood function, we then have
log L(θk+1|D)
L(θk|D)
≥0
and
log L(θk+1|D) −log L(θk|D) ≥0,
which implies that LL(θk+1|D) ≥LL(θk|D).
■
PROOF OF THEOREM 17.7. We need to consider the gradient of the log-likelihood
function under the normalization constraints 
x θx|u = 1. For this, we construct the
Lagrangian
f (θ, λ) = LL(θ|D) +

X

u
λX|u
"
1 −

x
θx|u
#
and set the gradient to zero. The equations ∂f/∂λX|u = 0 give us our normalization
constraints. The equations ∂f/∂θx|u = 0 give us (see Theorem 17.8)
∂f
∂θx|u
= ∂LL(θ|D)
∂θx|u
−λX|u =
N

i=1
1
Prθ(di)
∂Prθ(di)
∂θx|u
−λX|u = 0.
Rearranging, we get
λX|u =
N

i=1
1
Prθ(di)
∂Prθ(di)
∂θx|u
.
Multiplying both sides by θx|u, we get (see Theorem 12.2 and (12.6) of Chapter 12)
λX|uθx|u =
N

i=1
1
Prθ(di)
∂Prθ(di)
∂θx|u
θx|u =
N

i=1
Prθ(xu|di).
(17.17)
Summing all equations for state u, we have
λX|u

x
θx|u =
N

i=1

x
Prθ(xu|di),
and thus,
λX|u =
N

i=1
Prθ(u|di).
Dividing (17.17) by λX|u and substituting the previous, we ﬁnd that a stationary point of
the log-likelihood must be a ﬁxed point for EM:
θx|u =
N
i=1 Prθ(xu|di)
N
i=1 Prθ(u|di)
.
Reversing the proof, we can also show that a ﬁxed point for EM is a zero-gradient for the
log-likelihood.
■

17.7 PROOFS
475
PROOF OF THEOREM 17.8. We have
∂LL(θ|D)
∂θx|u
= ∂N
i=1 log Prθ(di)
∂θx|u
=
N

i=1
∂log Prθ(di)
∂θx|u
=
N

i=1

1
Prθ(di)
	 ∂Prθ(di)
∂θx|u
.
The second part of this theorem is shown in Theorem 12.2 and (12.6) of Chapter 12.
■
PROOF OF THEOREM 17.9. The likelihood function for the parameters of structure GI
has the form
L(θ, θI|DI) =

k
Prθ,θI(ok, αk, ik),
where k ranges over the cases of data set DI, ok are the values of variables O, ik are the
values of variables I, and αk are the available values of variables M (all in case k). Given
the MAR assumption, we have
Prθ,θI(ok, αk, ik) = Prθ,θI(ik|ok)Prθ,θI(ok, αk).
Hence, the likelihood function can be decomposed as
L(θ, θI|DI) =

k
Prθ,θI(ik|ok)
 
k
Prθ,θI(ok, αk)

.
Note that the ﬁrst component depends only on parameters θI and the second component
depends only on parameters θ. Moreover, if Prθ(.) is the distribution induced by structure
G and parameters θ, then

k
Prθ,θI(ok, αk) =

k
Prθ(ok, αk)
= L(θ|D).
We then have
L(θ, θI|DI) =

k
Prθ,θI(ik|ok)

L(θ|D).
Since the ﬁrst component depends only on parameters θI, we have
argmax
θ
L(θ|D) = argmax
θ
max
θI
L(θ, θI|DI).
■
PROOF OF THEOREM 17.10. First, note the deﬁnitions of entropy, conditional entropy,
and mutual information:
ENTD(X) = −

x
PrD(x) log PrD(x)
ENTD(X|U) = −

x,u
PrD(x, u) log PrD(x|u)
MID(X, U) =

x,u
PrD(x, u) log
PrD(x, u)
PrD(x)PrD(u).

476
LEARNING: THE MAXIMUM LIKELIHOOD APPROACH
Expanding the deﬁnition of mutual information and substituting the deﬁnitions of entropy
and conditional entropy leads to
MID(X, U) = ENTD(X) −ENTD(X|U).
Suppose now that X ranges over the nodes of a tree structure G and U is the parent of
X. By Theorem 17.3, we have
LL(G|D) = −N

XU
ENTD(X|U).
Hence, we also have
LL(G|D) = −N

XU
(ENTD(X) −MID(X, U))
= −N

XU
ENTD(X) + N

XU
MID(X, U).
Note that neither N nor the term −N 
XU ENTD(X) depend on the tree structure G.
Hence,
argmax
G
LL(G|D) = argmax
G

XU
MID(X, U) = argmax
G
tScore(G|D),
which proves the theorem.
■
PROOF OF THEOREM 17.11. Let U⋆= U ∪U′ where U ∩U′ = ∅. The mutual infor-
mation between X and U′ given U is deﬁned as [Cover and Thomas, 1991]
MID(X, U′|U)
def= ENTD(X|U) −ENTD(X|U, U′).
It is also known that [Cover and Thomas, 1991]
MID(X, U′|U) ≥0.
We then have
ENTD(X|U) −ENTD(X|U, U′) ≥0
and
ENTD(X|U) ≥ENTD(X|U⋆).
■
PROOF OF THEOREM 17.12. Consider ﬁrst the MDL score:
MDL(XiU⋆
i |D) = −N · ENTD(Xi|U⋆
i ) −ψ(N) · ||XiU⋆
i ||.
Note also that Ui ⊆U⋆
i ⊆U+
i . By Theorem 17.11, we have ENTD(Xi|U+
i ) ≤
ENTD(Xi|U⋆
i ) and, hence,
−N · ENTD(Xi|U+
i ) ≥−N · ENTD(Xi|U⋆
i ).
By Deﬁnition 17.6, we have ||XiUi|| ≤||XiU⋆
i || and, hence,
−ψ(N) · ||XiUi|| ≥−ψ(N) · ||XiU⋆
i ||.
This immediately leads to
MDL(XiU⋆
i |D) ≤−N · ENTD(Xi|U+
i ) −ψ(N) · ||XiUi||.
■

18
Learning: The Bayesian Approach
We discuss in this chapter a particular approach to learning Bayesian networks from
data, known as the Bayesian approach, which is marked by its ability to integrate prior
knowledge into the learning process and to reduce learning to a problem of inference.
18.1 Introduction
Consider the network structure in Figure 18.1 and suppose that our goal is to estimate
network parameters based on the data set shown in the ﬁgure. We discussed this problem
in Chapter 17 where we introduced the maximum likelihood principle for learning. Our
goal in this chapter is to present another principle for learning that allows us to reduce the
learning process to a problem of inference. This method, known as Bayesian learning, is
marked by its ability to integrate prior knowledge into the learning process and subsumes
the maximum likelihood approach under certain conditions.
To illustrate the Bayesian approach to learning, consider again the structure in Fig-
ure 18.1, which has ﬁve parameter sets: θH = (θh, θ¯h), θS|h = (θs|h, θ¯s|h), θS|¯h = (θs|¯h, θ¯s|¯h),
θE|h = (θe|h, θ¯e|h), and θE|¯h = (θe|¯h, θ¯e|¯h). Suppose that we know the values of two of these
parameter sets as
θS|h = (.1, .9)
θE|h = (.8, .2).
Suppose further that we have prior knowledge to the effect that
θH ∈{(.75, .25), (.90, .10)}
θS|¯h ∈{(.25, .75), (.50, .50)}
θE|¯h ∈{(.50, .50), (.75, .25)},
where each of the two values are considered equally likely. The Bayesian approach
to learning can integrate this information into the learning process by constructing the
meta-network shown in Figure 18.2. Here variables θH, θS|¯h, θE|¯h represent the possible
values of unknown network parameters, where the CPTs of these variables encode our
prior knowledge about these parameters. Moreover, variables Hi, Si, and Ei represent the
values that variables H, S, and E take in case i of the data set, allowing us to assert the
data set as evidence on the given network.
By explicitly encoding prior knowledge about network parameters and by treating data
as evidence, the Bayesian approach can now reduce the process of learning to a process
of computing posterior distributions:
P(θH, θS|¯h, θE|¯h|D),
477

478
LEARNING: THE BAYESIAN APPROACH
Health Aware
(H)
Smokes
(S)
Exercises
(E)
Case
H
S
E
1
F
F
T
2
T
F
T
3
T
F
T
4
F
F
F
5
F
T
F
Figure 18.1: A network structure with a complete data set.
h
S|
θ
h
E|
θ
H
θ
1
H
1
S
1
E
2
H
2
S
2
E
N
H
N
S
N
E
Figure 18.2: Bayesian learning as inference on a meta-Bayesian network.
where P is the distribution induced by the meta-network and D is the evidence entailed
by the data set. As we have two possible values for each of the unknown parameter sets,
we have eight possible parameterizations for the base network in Figure 18.1. Hence, the
previous posterior distribution can be viewed as providing a ranking over these possible
parameterizations.
The Bayesian approach can extract various quantities from this posterior distribution.
For example, we can identify parameter estimates that have the highest probability:
argmax
θH ,θS|¯hθE|¯h
P(θH, θS|¯hθE|¯h|D).
These are known as MAP estimates, for maximum a posteriori estimates, and are closely
related to maximum likelihood estimates, as we see later.
The Bayesian approach does not commit to a single value of network parameters θ as
it can work with a distribution over the possible values of these parameters, P(θ|D). As
a result, the Bayesian approach can compute the expected value of a given query with
respect to the distribution over network parameters. For example, the expected probability
of observing a person that both smokes and exercises can be computed as

θ
Prθ(s, e)P(θ|D),
where Prθ(.) is the distribution induced by the base network in Figure 18.1 and parametriza-
tion θ. That is, we are computing eight different probabilities for s, e, one for each
parametrization θ, and then taking their average weighted by the posterior parameter dis-
tribution P(θ|D). We see later that when the data set is complete, the Bayesian approach

18.2 META-NETWORKS
479
can be realized by working with a single parametrization, making it quite similar to the
maximum likelihood approach.
We will start in Section 18.2 by deﬁning the notion of a meta-network formally and
then describe a particular class of meta-networks that is commonly assumed in Bayesian
learning. We then consider parameter estimation in Section 18.3 while assuming that each
parameter has a ﬁnite number of possible values. We then treat the continuous case in
Section 18.4 and ﬁnally discuss the learning of network structures in Section 18.5.
18.2 Meta-networks
In this section, we characterize a class of meta-networks that is commonly assumed in
Bayesian learning. We start with the notion of a parameter set, which is a set of co-varying
network parameters.
Deﬁnition 18.1. Let X be a variable with values x1, . . . , xk and let U be its parents.
A parameter set for variable X and parent instantiation u, denoted by θX|u, is the set
of network parameters (θx1|u, . . . , θxk|u). A parameter set that admits a ﬁnite number of
values is said to be discrete; otherwise, it is said to be continuous.
■
The parameter set θS|¯h in Figure 18.2 was assumed to admit the following two values:
θS|¯h ∈{(.25, .75), (.50, .50)}.
This parameter set is therefore discrete and each of its values corresponds to an assignment
of probabilities to the set of co-varying parameters (θs|¯h, θ¯s|¯h). Hence, if θS|¯h = (.25, .75),
then θs|¯h = .25 and θ¯s|¯h = .75.
To further spell out our notational conventions for parameter sets, consider the following
expression:

θS|¯h
θs|¯hθ¯s|¯h.
That is, we are summing over all possible values of the parameter set θS|¯h and then
multiplying the values of parameters corresponding to each element of the summand.
This expression therefore evaluates to
(.25)(.75) + (.50)(.50).
We write an number of expressions later that resemble the form given here.
We are now ready to deﬁne meta-networks formally.
Deﬁnition 18.2. Let G be a network structure. A meta-network of size N for structure
G is constructed using N instances of structure G with variable X in G appearing
as Xi in the ith instance of G. Moreover, for every variable X in G and its parent
instantiation u, the meta-network contains the parameter set θX|u and corresponding
edges θX|u →X1, . . . , θX|u →XN.
■
Figure 18.2 contains a meta-network for the structure S ←H →E. Note, however,
that this meta-network does not contain parameter sets θS|h and θE|h as the values of these
variables are ﬁxed in this example. A full meta-network that includes all parameter sets
is shown in Figure 18.3(a).
In the rest of this chapter, we distinguish between the base network, which is a classical
Bayesian network, and the meta-network as given by Deﬁnition 18.2. We also use θ

480
LEARNING: THE BAYESIAN APPROACH
to denote the set of all parameters for the base network and call it a parametrization.
Equivalently, θ represents the collection of parameter sets in the meta-network. The
distribution induced by a base network and parametrization θ is denoted by Prθ(.) and
called a base distribution. The distribution induced by a meta-network is denoted by P(.)
and called a meta-distribution.
18.2.1 Prior knowledge
In the Bayesian approach to learning, prior knowledge on network parameters is encoded
in the meta-network using the CPTs of parameter sets. For example, we assumed in
Figure 18.2 that the two values of parameter set θS|¯h are equally likely. Hence, the CPT
of this parameter set is
θS|¯h = (θs|¯h, θ¯s|¯h)
P(θS|¯h)
(.25, .75)
50%
(.50, .50)
50%
These CPTs are then given as input to the learning process and lead to a major distinction
with the maximum likelihood approach to learning, which does not factor such information
into the learning process.
The CPTs of other variables in a meta-network (i.e., those that do not correspond to
parameter sets) are determined by the intended semantics of such networks. In particular,
consider a variable X in the base network having parents U and let X1, . . . , Xn be the
instances of X and U1, . . . , Un be the instances of U in the meta-network. All instances
of X have the same CPT in the meta-network:
P(Xi|ui, θX|u1, . . . , θX|um) = θX|uj , where uj = ui.
(18.1)
That is, given that parents Ui take on the value ui, the probability of Xi will be de-
termined only by the corresponding parameter set, θX|ui. Consider the meta-network in
Figure 18.3(a) and instances Si. We then have
P(Si|Hi =h, θS|h, θS|¯h) = θS|h
P(Si|Hi = ¯h, θS|h, θS|¯h) = θS|¯h.
18.2.2 Data as evidence
In addition to representing prior knowledge about network parameters, the Bayesian
approach allows us to treat data as evidence. Consider for example the following complete
data set:
Case
H
S
E
1
h
¯s
e
2
h
¯s
¯e
3
¯h
s
¯e
We can interpret each case i as providing evidence on variables Hi, Si, and Ei in the
meta-network. Hence, the data set can be viewed as the following variable instantiation:
D = (H1 =h) ∧(S1 = ¯s) ∧(E1 =e) ∧. . . ∧(H3 = ¯h) ∧(S3 =s) ∧(E3 = ¯e).
We can assert this data set as evidence on the meta-network and then compute the
corresponding posterior distribution on network parameters.

18.2 META-NETWORKS
481
h
S|
θ
h
E|
θ
H
θ
1
H
1
S
1
E
2
H
2
S
2
E
3
H
3
S
3
E
h
S|
θ
h
E|
θ
h
S|
θ
h
E|
θ
H
θ
1h
1s
1e
2
h
2s
2e
3
h
3s
3e
h
S|
θ
h
E|
θ
(a) Meta-network
(b) Pruned meta-network
Figure 18.3: Pruning edges of a meta-network based on a complete data set.
For the meta-network in Figure 18.3(a), we initially have the following distribution on
parameter sets:
P(θH, θS|h, θS|¯h, θE|h, θE|¯h) = P(θH)P(θS|h)P(θS|¯h)P(θE|h)P(θE|¯h).
Note how the prior distribution could be decomposed in this case, which is possible
for any meta-network given by Deﬁnition 18.2 (since parameter sets are root nodes and
are therefore d-separated). In fact, we next show that this decomposition holds for the
posterior distribution as well, given that the data set is complete:
P(θH, θS|h, θS|¯h, θE|h, θE|¯h|D) = P(θH|D)P(θS|h|D)P(θS|¯h|D)P(θE|h|D)P(θE|¯h|D).
Figure 18.3(b) provides the key insight behind this decomposition. Here variables are
instantiated according to their values in the data set, allowing us to prune edges that are
either outgoing from observed variables (see Section 6.9.2) or representing superﬂuous
dependencies (see (18.1)). All edges outgoing from variables H1, H2, and H3 fall into
the ﬁrst category. All other pruned edges fall into the second category. For example,
the edge θS|¯h →S1 is now superﬂuous given that H1 is instantiated to h, that is, S1 no
longer depends on the parameter set θS|¯h in this case. These two types of edge pruning are
guaranteed to lead to a meta-network in which every parameter set is disconnected from
all other parameter sets.
18.2.3 Parameter independence
We can now easily prove the following key result.
Theorem 18.1. Consider a meta-network as given by Deﬁnition 18.2 and let 1 and
2 each contain a collection of parameter sets, 1 ∩2 = ∅. The following conditions,
known as parameter independence, are then guaranteed to hold:
r 1 and 2 are independent, P(1, 2) = P(1)P(2).
r 1 and 2 are independent given any complete data set D, P(1, 2|D) =
P(1|D)P(2|D).
■

482
LEARNING: THE BAYESIAN APPROACH
Parameter independence is sometimes classiﬁed as either global or local. In particular,
global parameter independence refers to the independence between two parameter sets,
θX|u and θY|v, corresponding to distinct variables X ̸= Y. On the other hand, local pa-
rameter independence refers to the independence between parameter sets, θX|u and θX|u⋆,
u ̸= u⋆, corresponding to the same variable X.
We point out that we may adopt other deﬁnitions of a meta-network that do not
necessarily embody the condition of parameter independence (see Exercise 18.3). Yet
parameter independence is almost always assumed in Bayesian learning as it simpliﬁes
the complexity of inference on meta-networks. This is critical for Bayesian learning since
this method is based on reducing the problem of learning to a problem of inference. We
also point out that certain types of parameter dependence can be accommodated without
affecting the computational complexity of approaches we discuss next (see Exercise 18.4).
18.3 Learning with discrete parameter sets
Now that we have settled the basic formalities needed by the Bayesian approach, we
discuss Bayesian learning in this section for both complete and incomplete data while
conﬁning ourselves to discrete parameter sets. This allows us to introduce the main
concepts underlying Bayesian learning without introducing additional machinery for
handling continuous variables. Continuous parameter sets are then treated in Section 18.4.
Consider again the network structure and corresponding data set D in Figure 18.1.
Recall that two of the parameter sets have known values in this example:
θS|h = (.1, .9)
θE|h = (.8, .2).
Moreover, the remaining parameter sets have the following possible values:
θH ∈{(.75, .25), (.90, .10)},
θS|¯h ∈{(.25, .75), (.50, .50)},
θE|¯h ∈{(.50, .50), (.75, .25)},
leading to eight possible parameterizations.
Suppose now that our goal is to compute the probability of observing a smoker who
exercises regularly, that is, s, e. According to the maximum likelihood approach, we must
ﬁrst ﬁnd the maximum likelihood estimates θml based on the given data and then use them
to compute this probability. Among the eight possible parameterizations in this case, the
one with maximum likelihood (i.e., the one that maximizes the probability of data) is
θml :
θH = (.75, .25), θS|¯h = (.25, .75), θE|¯h = (.50, .50).
If we plug in these parameter values in the base network of Figure 18.1, we obtain the
following probability of observing a smoker who exercises regularly:
Prθml(s, e) ≈9.13%.
However, the Bayesian approach treats this problem differently. In particular, it views
the data set D as evidence on variables H1, S1, E1, . . . ,H5, S5, E5 in the meta-network of
Figure 18.2. It then computes the posterior on variables S6 and E6 by performing inference
on this meta-network, leading to
P(S6 =s, E6 =e|D) ≈11.06%.

18.3 LEARNING WITH DISCRETE PARAMETER SETS
483
The Bayesian approach is therefore not estimating any parameters as is done in the
maximum likelihood approach. Theorem 18.2 provides an interpretation of what the
Bayesian approach is really doing.
Theorem 18.2. Given discrete parameter sets and a data set D of size N, we have
P(αN+1|D) =

θ
Prθ(α)P(θ|D).
(18.2)
Here event αN+1 is obtained from α by replacing every occurrence of variable X by its
instance XN+1.
■
For example, if α is S =s, E =e, then α6 is S6 =s, E6 =e. In the previous example, we
then have
P(S6 =s, E6 =e|D) =

θ
Prθ(S =s, E =e)P(θ|D).
The Bayesian approach is therefore considering every possible parametrization θ, comput-
ing the probability Prθ(S =s, E =e) using the base network, and then taking a weighted
average of the computed probabilities. In other words, the Bayesian approach is computing
the expected value of Prθ(S =s, E =e).
Theorem 18.2 holds for any data set, whether complete or not. However, if the data
set is complete, then we can compute the expectation of Theorem 18.2 by performing
inference on the base network as long as it is parameterized using the following estimates.
Deﬁnition 18.3. Let θX|u be a discrete parameter set. The Bayesian estimate for param-
eter θx|u given data set D is deﬁned as
θbe
x|u
def=

θX|u
θx|u · P(θX|u|D).
■
That is, the Bayesian estimate is the expectation of θx|u according to the posterior distri-
bution of parameter set θX|u. The set of all Bayesian estimates θbe
x|u is denoted by θbe. We
now have the following key result.
Theorem 18.3. Given discrete parameter sets and a complete data set D of size N, we
have
P(αN+1|D) = Prθbe(α),
(18.3)
where θbe are the Bayesian estimates given data set D.
■
Recall again that the probability P(αN+1|D) is an expectation of the probability Prθ(α).
Hence, Theorem 18.3 says that we can compute this expectation by performing inference
on a base network that is parameterized by the Bayesian estimates. It is for this reason
that computing the Bayesian estimates is a focus of attention for Bayesian learning under
complete data.
18.3.1 Computing Bayesian estimates
Bayesian learning is relatively well-behaved computationally when the data set is complete
(and given the assumption of parameter independence). We saw this already in the previous
section where computations with respect to the meta-network could be reduced to ones
on the base network. However, this reduction is based on our ability to compute Bayesian

484
LEARNING: THE BAYESIAN APPROACH
estimates, as those estimates are needed to parameterize the base network. As it turns out,
these estimates are also easy to compute given Theorem 18.4.
Theorem 18.4. Let θX|u be a discrete parameter set and let D be a complete data set. We
then have
P(θX|u|D) = η P(θX|u)

x

θx|u
D#(xu)
,
(18.4)
where η is a normalizing constant.
■
Consider now the parameter set θE|¯h with values {(.50, .50), (.75, .25)} and a uniform
prior. Also consider the following data set D from Figure 18.1:
Case
H
S
E
1
F
F
T
2
T
F
T
3
T
F
T
4
F
F
F
5
F
T
F
We then have the following posterior:
P(θE|¯h = (.50, .50)|D) = η × .50 ×

.50
1
.50
2
P(θE|¯h = (.75, .25)|D) = η × .50 ×

.75
1
.25
2
.
Normalizing, we get
P(θE|¯h = (.50, .50)|D) ≈72.73%
P(θE|¯h = (.75, .25)|D) ≈27.27%.
We can now immediately compute the Bayesian estimate for every parameter by taking
its expectation according to this posterior:
θbe
e|¯h = .50 × 72.73% + .75 × 27.27% ≈.57
θbe
¯e|¯h = .50 × 72.73% + .25 × 27.27% ≈.43
The Bayesian estimate for parameter set θE|¯h = (θe|¯h, θ¯e|¯h) is then (.57, .43) in this case.
18.3.2 Closed forms for complete data
We now summarize the computations that are known to have closed forms under complete
data. We assume here a base network with families XU and a complete data set D of
size N:
r The prior probability of network parameters (see Theorem 18.1):
P(θ) =

XU

u
P(θX|u)
(18.5)
r The posterior probability of network parameters (see Theorem 18.1):
P(θ|D) =

XU

u
P(θX|u|D)
(18.6)

18.3 LEARNING WITH DISCRETE PARAMETER SETS
485
r The likelihood of network parameters (see Exercise 18.5):
P(D|θ) =
N

i=1
P(di|θ) =
N

i=1
Prθ(di)
(18.7)
r The marginal likelihood (see Theorem 18.3):1
P(D) =
N

i=1
P(di|d1, . . . , di−1) =
N

i=1
Prθbe
i (di)
(18.8)
where θbe
i
are the Bayesian estimates for data set d1, . . . , di−1.
In addition to the Bayesian estimates, we can easily compute MAP estimates under com-
plete data. Recall that MAP estimates are those that have a maximal posterior probability:
θma = argmax
θ
P(θ|D).
Given (18.6), we then have
θma
X|u = argmax
θX|u
P(θX|u|D).
It is worth mentioning here the relationship between MAP and maximum likelihood
parameters. Since,
P(θ|D) = P(D|θ)P(θ)
P(D)
∝P(D|θ)P(θ),
the only difference between MAP and maximum likelihood parameters is in the prior
P(θ). Hence, if all network parameterizations are equally likely, that is, P(θ) is a uniform
distribution, then MAP and maximum likelihood parameters will coincide:
argmax
θ
P(θ|D) = argmax
θ
P(D|θ).
18.3.3 Dealing with incomplete data
Consider again the network structure in Figure 18.1 and the incomplete data set D:
Case
H
S
E
1
?
F
T
2
?
F
T
3
?
F
T
4
?
F
F
5
?
T
F
Suppose now that our goal is to compute the probability of observing a smoker who
exercises regularly. As with the case of complete data, the Bayesian approach asserts the
previous data set as evidence on the network in Figure 18.2 and then poses the following
query:
P(S6 =s, E6 =e|D) ≈10.77%.
1 Since P(D|θ) is called the likelihood of parameters θ, the quantity P(D) is called the marginal likelihood since it
equals 
θ P(D|θ)P(θ).

486
LEARNING: THE BAYESIAN APPROACH
Case
H
S
E
1
?
F
T
2
F
?
T
3
?
F
T
Case
H
S
E
1
T
?
?
2
?
T
?
3
T
?
?
Case
H
S
E
1
T
?
?
2
?
F
?
3
F
?
?
(a) Incomplete data set D
(b) Completion Dc
(c) Completion Dc
Figure 18.4: A data set and two of its completions (out of eight possible completions). Note that D, Dc
is a complete data set.
Note, however, that we can no longer obtain the answer to this query by performing
inference on a base network as suggested by Theorem 18.3 since this theorem depends
on data completeness.
When the data set is incomplete, evaluating P(αN+1|D) is generally hard. Therefore, it
is not uncommon to appeal to approximate inference techniques in this case. We discuss
two of these techniques next.
Approximating the marginal likelihood
The ﬁrst set of techniques focuses on approximating the marginal likelihood, P(D), as
this provides a handle on computing P(αN+1|D):
P(αN+1|D) = P(αN+1, D)
P(D)
.
Note here that αN+1, D is a data set of size N + 1, just as D is a data set of size N –
assuming that αN+1 is a variable instantiation and can therefore be treated as a case.
The marginal likelihood P(D) can be approximated using stochastic sampling tech-
niques discussed in Chapter 15. A common application of these techniques is in the
context of the candidate method, which requires us to approximate P(θ|D) for some
parametrization θ and then use this approximation to compute the marginal likelihood as
P(D) = P(D|θ)P(θ)
P(θ|D)
.
Note here that the likelihood P(D|θ) can be computed exactly by performing inference
on the base network as given by (18.7). The prior P(θ) can also be computed exactly as
given by (18.5).
To approximate P(θ|D), we observe the following:
P(θ|D) =

Dc
P(θ|D, Dc)P(Dc|D),
where Dc is a completion of the data set D, assigning values to exactly those variables
that have missing values in D (see Figure 18.4). Hence, P(θ|D) is an expectation of
P(θ|D, Dc) with respect to the distribution P(Dc|D). We can then use the techniques
discussed in Chapter 15 for estimating expectations. For example, we can use Gibbs
sampling for this purpose, as suggested by Exercise 18.15.
Using MAP estimates
Another technique for dealing with incomplete data is based on the following approxima-
tion:
P(αN+1|D) =

θ
Prθ(α)P(θ|D) ≈Prθma(α),

18.3 LEARNING WITH DISCRETE PARAMETER SETS
487
where θma are the MAP parameter estimates deﬁned as
θma
def=
argmax
θ
P(θ|D).
That is, instead of summing over all parameter estimates θ, we restrict ourselves to MAP
estimates θma. Note that this approximation can be computed by performing inference
on a base network that is parameterized by these MAP estimates, just as we did with the
Bayesian estimates.
The approximation here is usually justiﬁed for large data sets where the posterior
P(θ|D) becomes sharper while peaking at the MAP estimates. Note, however, that com-
puting MAP estimates is also hard, so we typically appeal to local search methods. We
next describe an EM algorithm that searches for MAP estimates and has similar prop-
erties to the EM algorithm discussed in Chapter 17 for ﬁnding maximum likelihood
estimates.
Our goal here is to ﬁnd a parametrization θ that maximizes P(θ|D). As mentioned
previously, P(θ|D) can be expressed as
P(θ|D) =

Dc
P(θ|D, Dc)P(Dc|D),
where Dc is a completion of data set D, assigning values to exactly those variables that
have missing values in D (see Figure 18.4). We can therefore think of P(θ|D) as an
expectation of P(θ|D, Dc) computed with respect to the distribution P(Dc|D). Our goal
is then to ﬁnd a parametrization θ that maximizes this expectation.
Suppose instead that we maximize the following expectation:
e(θ|D, θk)
def=

Dc

log P(θ|D, Dc)

P(Dc|D, θk).
That is, we are now computing the expectation of log P(θ|D, Dc) instead of P(θ|D, Dc).
Moreover, we are computing this expectation with respect to the distribution P(Dc|D, θk)
instead of P(Dc|D) where θk is some arbitrary parametrization.
The resulting expectation is clearly not equal to the original expectation in which we
are interested. As such, it cannot yield the MAP estimates we are after. As it turns out,
maximizing this new expectation is guaranteed to at least improve on the parametriza-
tion θk.
Theorem 18.5. If θk+1 = argmaxθ e(θ|D, θk), then P(θk+1|D) ≥P(θk|D).
■
We can therefore use this observation as a basis for developing a local search algorithm
for MAP estimates. That is, we start with some initial estimates θ0, typically chosen
randomly, and generate a sequence of estimates θ0, θ1, θ2, . . . until some convergence
criterion is met. If estimate θk+1 is obtained from estimate θk as given by Theorem 18.5,
we are then guaranteed that the probability of these estimates will be nondecreasing.
To complete the description of the algorithm, all we need to show is how to optimize
the expectation e(θ|D, θk). This is actually straightforward once we obtain the following
expected counts.
Deﬁnition 18.4. Given a data set D, the expected count of event α given parameter
estimates θk is deﬁned as
D#(α|θk)
def=

Dc
"
[DDc]#(α)
#
P(Dc|D, θk).
■

488
LEARNING: THE BAYESIAN APPROACH
Algorithm 51 MAP EM D(G, θ0, D)
input:
G:
Bayesian network structure with families XU
θ0:
parametrization of structure G
D:
data set of size N
output: MAP/EM parameter estimates for structure G
main:
1: k←0
2: while θk ̸= θk−1 do {this test is different in practice}
3:
cxu←0 for each family instantiation xu
4:
for i = 1 to N do
5:
for each family instantiation xu do
6:
cxu←cxu + Prθk(xu|di) {requires inference on network (G, θk)}
7:
end for
8:
end for
9:
compute parameters θk+1 using θk+1
X|u = argmaxθX|u P(θX|u) 
x

θx|u
cxu
10:
k←k + 1
11: end while
12: return θk
That is, we consider every completion Dc of data set D, compute the count of α with
respect to the complete data set DDc (see Deﬁnition 17.1), and ﬁnally take the average
of these counts weighted by the distribution P(Dc|D, θk). Interestingly enough, we can
obtain this expected count without enumerating all possible completions Dc. In particular,
Exercise 18.10 asks for a proof to the following closed form:
D#(α|θk) =
N

i=1
Prθk(α|di).
(18.9)
According to (18.9), we can obtain an expected count by performing inference on the
base network that is parameterized by θk. Once we have these expected counts, we can
optimize the expectation e(θ|D, θk) as shown by Theorem 18.6.
Theorem 18.6. θk+1 = argmaxθ e(θ|D, θk) iff
θk+1
X|u = argmax
θX|u
P(θX|u)

x

θx|u
D#(xu|θk)
.
■
Recall now Theorem 18.4, which provides a closed form for the posterior of a parameter
set given complete data. Theorem 18.6 is then showing that to optimize e(θ|D, θk),
all we need is to compute the posterior for each parameter set θX|u while treating the
expected counts as coming from a complete data set. We can then optimize the expectation
e(θ|D, θk) by simply ﬁnding the MAP estimate θk+1
X|u for each of the computed posteriors.
Algorithm 51 provides the pseudocode for computing MAP estimates using the described
EM algorithm.
We effectively reduced an incomplete-data MAP problem to a sequence of complete-
data MAP problems. The only computational demand here is that of computing the

18.4 LEARNING WITH CONTINUOUS PARAMETER SETS
489
expected counts, which are needed to compute the posteriors of each complete-data
problem.
18.4 Learning with continuous parameter sets
We have thus far worked only with discrete parameter sets. For example, we assumed in
Figure 18.3 that the parameter set θS|¯h has only two values (.25, .75) and (.50, .50) since
our prior knowledge precluded all other values for network parameters (θs|¯h, θ¯s|¯h). On the
other hand, if we allow all possible values for these parameters, the parameter set θS|¯h will
then be continuous (i.e., having an inﬁnite number of values).
To apply Bayesian learning in this context, we need a method for capturing prior
knowledge on continuous parameter sets (CPTs are only appropriate for discrete parameter
sets). We also need to discuss the semantics of meta-networks that contain continuous
variables. We have not deﬁned the semantics of such networks in this text as our treatment
is restricted to networks with discrete variables (except for the treatment of continuous
sensor readings in Section 3.7). We address both of these issues in the next two sections.
18.4.1 Dirichlet priors
Consider the parameter set θH = (θh, θ¯h) in Figure 18.3 and suppose that we expect
it has the value (.75, .25) yet we do not rule out other values, such as (.90, .10) and
(.40, .60). Suppose further that our belief in other values decreases as they deviate more
from the expected value (.75, .25). One way to specify this knowledge is using a Dirichlet
distribution, which requires two numbers ψh and ψ¯h, called exponents, where
ψh
ψh + ψ¯h
is the expected value of parameter θh and
ψ¯h
ψh + ψ¯h
is the expected value of parameter θ¯h. For example, we can use the exponents ψh = 7.5
and ψ¯h = 2.5 to obtain the expectation (
7.5
7.5+2.5,
2.5
7.5+2.5) = (.75, .25). We can also use the
exponents ψh = 75 and ψ¯h = 25 to obtain the same expectation (
75
75+25,
25
75+25). As is clear
from this example, there is an inﬁnite number of exponents that we can use, all of which
lead to the same expected value of network parameters.
According to the semantics of a Dirichlet distribution, which we provide formally
next, these different values of the exponents are not all the same. In particular, the sum of
these exponents, ψh + ψ¯h, is interpreted as a measure of conﬁdence in the expectations
they lead to. This sum is called the equivalent sample size of the Dirichlet distribution,
where a larger equivalent sample size is interpreted as providing more conﬁdence in the
corresponding expectations.
To provide more intuition into the Dirichlet distribution, think of the exponent ψh as
the number of health-aware individuals observed before encountering the current data
set, and similarly for the exponent ψ¯h. According to this interpretation, the exponents
(ψh =7.5, ψ¯h =2.5) and (ψh =75, ψ¯h =25) can both be used to encode the belief that
75% of the individuals are health-aware yet the second set of exponents imply a stronger
belief as they are based on a larger sample.

490
LEARNING: THE BAYESIAN APPROACH
Consider now variable E in Figure 18.3 and suppose that it takes three values:
r e1: the individual does not exercise at all
r e2: the individual exercises but not regularly
r e3: the individual exercises regularly.
Suppose now that we wish to encode our prior knowledge about the parameter set θE|h =
(θe1|h, θe2|h, θe3|h). If we expect this set to have the value (.10, .60, .30), we can then use
the exponents
ψe1|h =10,
ψe2|h =60,
ψe3|h =30,
which lead to the expectations
10
10 + 60 + 30,
60
10 + 60 + 30,
30
10 + 60 + 30.
We are now ready to present the formal deﬁnition of the Dirichlet distribution.
Deﬁnition 18.5. A Dirichlet distribution for a continuous parameter set θX|u is speciﬁed
by a set of exponents, ψx|u ≥1.2 The equivalent sample size of the distribution is deﬁned
as
ψX|u
def=

x
ψx|u.
The Dirichlet distribution has the following density:
ρ(θX|u)
def= η

x

θx|u
ψx|u−1
,
where η is a normalizing constant
η
def=
(ψX|u)

x (ψx|u).
Here (.) is the Gamma function, which is an extension of the factorial function to real
numbers.3
■
The Dirichlet density function may appear somewhat involved yet we only use it in
proving Theorem 18.9. Aside from this theorem, we only need the following well-known
properties of the Dirichlet distribution, which we provide without proof. First, the expected
value of network parameter θx|u is given by
Ex(θx|u) = ψx|u
ψX|u
.
(18.10)
Next, the variance of this parameter is given by
Va

θx|u

= Ex(θx|u)(1 −Ex(θx|u))
ψX|u + 1
.
(18.11)
Note that the larger the equivalent sample size, ψX|u, the smaller the variance and, hence,
the more conﬁdence we have in the expected values of network parameters. The ﬁnal
2 The Dirichlet distribution can be deﬁned for exponents 0 < ψx|u < 1 but its behavior for these exponents leads to
mathematical complications that we try to avoid here. For example (18.12) does not hold in this case.
3 The Gamma function is generally deﬁned as (a) =
 ∞
0
xa−1e−xdx. We have (1) = 1 and (a + 1) = a(a),
which means that (a) = (a −1)! when a is an integer ≥1.

18.4 LEARNING WITH CONTINUOUS PARAMETER SETS
491
h
S|
θ
h
E|
θ
H
θ
H
S
E
Figure 18.5: A meta-network with continuous parameter sets.
property we use of the Dirichlet distribution concerns the mode of a parameter set, which
is the value having the largest density. This is given by
Md

θx|u

=
ψx|u −1
ψX|u −|X|,
(18.12)
where |X| is the number of values for variable X. We close this section by noting that a
Dirichlet distribution with two exponents is also known as the beta distribution.
18.4.2 The semantics of continuous parameter sets
A meta-network with discrete parameter sets induces a probability distribution but a meta-
network with continuous parameter sets induces a density function. Consider for example
the meta-network in Figure 18.5 and suppose that all parameter sets are continuous. The
density function speciﬁed by this meta-network is given by
ρ(θH, θS|¯h, θE|¯h, H, S, E) = ρ(θH)ρ(θS|¯h)ρ(θE|¯h)P(H|θH)P(S|H, θS|¯h)P(E|H, θE|¯h).
Hence, similar to Bayesian networks with discrete variables, the semantics of a network
with continuous variables is deﬁned by the chain rule, except that we now have a prod-
uct of densities (for continuous variables) and probabilities (for discrete variables). The
other difference we have is when computing marginals. For example, the marginal over
parameter sets is a density given by
ρ(θH, θS|¯h, θE|¯h) =

h,s,e
ρ(θH, θS|¯h, θE|¯h, H =h, S =s, E =e),
while the marginal over discrete variables is a distribution given by4
P(H, S, E) =

 
 
ρ(θH, θS|¯h, θE|¯h, H, S, E)dθHdθS|¯hdθE|¯h.
The more general rule is this: Discrete variables are summed out while continuous vari-
ables are integrated over. The result is a probability only if all continuous variables are
4 Suppose that θX|u = (θx1|u, . . . , θxk|u). Integrating over a parameter set θX|u is a shorthand notation for successively
integrating over parameters θx1|u, . . . , θxk−1|u while ﬁxing the value of θxk|u to 1 −k−1
i=1 θxi|u.

492
LEARNING: THE BAYESIAN APPROACH
integrated over; otherwise, the result is a density. For example, the marginal over parameter
set θS|¯h is a density given by
ρ(θS|¯h) =

 
  
h,s,e
ρ(θH, θS|¯h, θE|¯h, H =h, S =s, E =e)

dθHdθE|¯h.
Density behaves like probability as far as independence is concerned. For example, since
the meta-network satisﬁes parameter independence, we have
ρ(θH, θS|¯h, θE|¯h) = ρ(θH)ρ(θS|¯h)ρ(θE|¯h)
and
ρ(θH, θS|¯h, θE|¯h|D) = ρ(θH|D)ρ(θS|¯h|D)ρ(θE|¯h|D)
when the data set D is complete. Density also behaves like probability as far as condi-
tioning is concerned. For example,
ρ(H|θH) = ρ(θH, H)
ρ(θH)
and
ρ(θH|H) = ρ(θH, H)
P(H)
.
18.4.3 Bayesian learning
We can now state the main theorems for Bayesian learning with continuous parameter
sets, which parallel the ones for discrete sets. We start with the following parallel to
Theorem 18.2.
Theorem 18.7. Given continuous parameter sets and a data set D of size N, we have5
P(αN+1|D) =

Prθ(α)ρ(θ|D)dθ.
(18.13)
■
That is, the quantity P(αN+1|D) is an expectation of the probability Prθ(α), which is
deﬁned with respect to the base network.
Now the parallel to Deﬁnition 18.3:
Deﬁnition 18.6. Let θX|u be a continuous parameter set. The Bayesian estimate for
network parameter θx|u given data set D is deﬁned as
θbe
x|u
def=

θx|u · ρ(θX|u|D)dθX|u.
■
As in the discrete case, we can sometimes reduce inference on a meta-network to
inference on a base network using the Bayesian estimates θbe. This is given by the
following parallel to Theorem 18.3.
5 Integrating over a parametrization θ is a shorthand notation for successively integrating over each of its parameter
sets.

18.4 LEARNING WITH CONTINUOUS PARAMETER SETS
493
Theorem 18.8. Given continuous parameter sets and a complete data set D of size N,
we have
P(αN+1|D) = Prθbe(α),
(18.14)
where θbe are the Bayesian estimates given data set D.
■
18.4.4 Computing Bayesian estimates
The Bayesian estimates are at the heart of the Bayesian approach to learning when the
data set is complete. This is due to Theorem 18.8, which allows us to reduce Bayesian
learning to inference on a base network that is parameterized by the Bayesian estimates.
However, the computation of these estimates hinges on an ability to compute posterior
marginals over parameter sets, which is needed in Deﬁnition 18.6. The following parallel
to Theorem 18.4 provides a closed form for these posteriors.
Theorem 18.9. Consider a meta-network where each parameter set θX|u has a prior
Dirichlet density ρ(θX|u) speciﬁed by exponents ψx|u. Let D be a complete data set.
The posterior density ρ(θX|u|D) is then a Dirichlet density, speciﬁed by the following
exponents:
ψ′
x|u = ψx|u + D#(xu).
(18.15)
■
Consider now the parameter set θS|h = (θs|h, θ¯s|h) with a prior density ρ(θS|h) speciﬁed
by the exponents
ψs|h = 1
and
ψ¯s|h = 9.
The prior expectation of parameter θs|h is then .1. Consider now the data set
Case
H
S
E
1
F
F
T
2
T
F
T
3
T
F
T
4
F
F
F
5
F
T
F
The posterior density ρ(θS|h|D) is also Dirichlet, speciﬁed by the exponents
ψ′
s|h = 1 + 0 = 1
and
ψ′
¯s,h = 9 + 2 = 11.
The posterior expectation of parameter θs|h is now 1/12.
More generally, the posterior expectation of parameter θx|u given complete data is given
by
θbe
x|u = ψx|u + D#(xu)
ψX|u + D#(u) ,
(18.16)
where ψx|u are the exponents of the prior Dirichlet distribution and ψX|u is its equiva-
lent sample size. This is the Bayesian estimate in the context of Dirichlet distributions.
Moreover, given (18.12) the MAP estimate given complete data is
θma
x|u = ψx|u + D#(xu) −1
ψX|u + D#(u) −|X|.
(18.17)
In this example, the MAP estimate for parameter θs|h is 0.

494
LEARNING: THE BAYESIAN APPROACH
Let us now compare these estimates with the maximum likelihood estimate given by
(17.1):
θml
x|u = D#(xu)
D#(u) .
Note that contrary to maximum likelihood estimates, the Bayesian (and sometimes MAP)
estimates do not suffer from the problem of zero counts. That is, these estimates are well-
deﬁned even when D#(u) = 0. Note also that the Bayesian and MAP estimates converge
to the maximum likelihood estimates as the data set size tends to inﬁnity, assuming the
data set is generated by a strictly positive distribution.
Consider now the prior Dirichlet distribution, called a noninformative prior, in which
all exponents are equal to one: ψx|u = 1. The expectation of parameter θx|u is 1/|X| under
this prior, leading to a uniform distribution for variable X given any parent instantiation
u. Under this prior, the Bayesian estimate given a complete data is
θbe
x|u = 1 + D#(xu)
|X| + D#(u).
Moreover, the MAP estimate is
θma
x|u = D#(xu)
D#(u) ,
which coincides with the maximum likelihood estimate.6
18.4.5 Closed forms for complete data
We now summarize the computations for continuous parameter sets that are known to
have closed forms under complete data. We assume here a base network with families
XU and a complete data set D of size N:
r The prior density of network parameters:
ρ(θ) =

XU

u
ρ(θX|u)
(18.18)
r The posterior density of network parameters:
ρ(θ|D) =

XU

u
ρ(θX|u|D)
(18.19)
r The likelihood of network parameters (same as 18.7):
P(D|θ) =
N

i=1
Prθ(di)
(18.20)
r The marginal likelihood:
P(D) =

XU

u
(ψX|u)
(ψX|u + D#(u))

x
(ψx|u + D#(xu))
(ψx|u)
.
(18.21)
6 Note, however, that this equality is not implied by the fact that parameters θx|u have equal expectations. For example,
if all exponents are equal to 10, then all parameters have equal expectations yet the MAP and maximum likelihood
estimates do not coincide.

18.4 LEARNING WITH CONTINUOUS PARAMETER SETS
495
The last equation is stated and proved in Theorem 18.12 on Page 514. The proof provides
an alternative form that does not use the Gamma function but the form here, which may
seem surprising at ﬁrst, is more commonly cited in the literature.
18.4.6 Dealing with incomplete data
The techniques for dealing with incomplete data are similar to those for discrete parameter
sets: approximating the marginal likelihood using stochastic sampling and using MAP
estimates as computed by EM. We brieﬂy discuss these techniques next in the context of
continuous parameter sets. We also discuss large-sample approximations of the marginal
likelihood, which can be more efﬁcient computationally than stochastic sampling ap-
proaches.
Using MAP estimates
We previously provided an EM algorithm that searches for MAP estimates of discrete
parameter sets. The algorithm has a parallel for continuous parameter sets, which we
describe next.
Our goal here is to ﬁnd parameter estimates that maximize the density ρ(θ|D), which
can be expressed in terms of the following expectation:
ρ(θ|D) =

Dc
ρ(θ|D, Dc)P(Dc|D).
Recall that Dc is a completion of the data set D.
Instead of optimizing this quantity, we optimize the following expectation:
e(θ|D, θk)
def=

Dc

log ρ(θ|D, Dc)

P(Dc|D, θk),
where θk are some initial estimates. The resulting estimates from this new optimization
problem are then guaranteed to improve on the initial estimates, as shown by Theo-
rem 18.10.
Theorem 18.10. If θk+1 = argmaxθ e(θ|D, θk), then ρ(θk+1|D) ≥ρ(θk|D).
■
The description of the new EM algorithm is completed by Theorem 18.11, which shows
how we can maximize the new expectation.
Theorem 18.11. θk+1 = argmaxθ e(θ|D, θk) iff
θk+1
x|u = D#(xu|θk) + ψx|u −1
D#(u|θk) + ψX|u −|X|.
■
Hence, all we need is to compute the expected counts D#(.|θk) using (18.9), which
requires inference on the base network.
Let us now recall Theorem 18.9, which provides a closed form for the posterior Dirichlet
density under complete data, and (18.12), which provides a closed form for the modes
of a Dirichlet distribution (i.e., the values of parameters that have a maximal density).
Theorem 18.11 then suggests that we compute a posterior Dirichlet distribution for each
parameter set while treating the expected counts as coming from a complete data set.
It then suggests that we take the modes of these computed posteriors as our parameter
estimates. Hence, once again we reduced a MAP problem for incomplete data to a

496
LEARNING: THE BAYESIAN APPROACH
Algorithm 52 MAP EM C(G, θ0, D, ψx|u/ψX|u)
input:
G:
Bayesian network structure with families XU
θ0:
parametrization of structure G
D:
data set of size N
ψx|u/ψX|u:
Dirichlet prior for each parameter set θX|u of structure G
output: MAP/EM parameter estimates for structure G
main:
1: k←0
2: while θk ̸= θk−1 do {this test is different in practice}
3:
cxu←0 and cu←0 for each family instantiation xu
4:
for i = 1 to N do
5:
for each family instantiation xu do
6:
cxu←cxu + Prθk(xu|di) {requires inference on network (G, θk)}
7:
cu←cu + Prθk(u|di)
8:
end for
9:
end for
10:
compute parameters θk+1 using θk+1
x|u = (cxu + ψx|u −1)/(cu + ψX|u −|X|)
11:
k←k + 1
12: end while
13: return θk
sequence of MAP problems for complete data. Moreover, the only computational demand
here is in computing the expected counts, which are needed to deﬁne the posteriors for
each complete-data problem. Algorithm 52 provides the pseudocode for computing MAP
estimates using the described EM algorithm.
The marginal likelihood: Stochastic sampling approximations
The marginal likelihood can also be approximated using the candidate method discussed
previously for discrete parameter sets. That is, for some parametrization θ, we can compute
the marginal likelihood using the following identity:
P(D) = P(D|θ)ρ(θ)
ρ(θ|D)
.
As with discrete parameter sets, the likelihood P(D|θ) can be computed exactly by
performing inference on the base network as given by (18.20). The prior density ρ(θ)
can also be computed exactly as given by (18.18). Finally, the density ρ(θ|D) can be
approximated using Gibbs sampling (see Exercise 18.15).
The marginal likelihood: Large-sample approximations
We now discuss a class of approximations for the marginal likelihood that is motivated by
some properties that hold for large data sets. Consider the simple network A →B, where
variables A and B are binary. This network has six parameters yet only three of them are
independent. Suppose now that we choose the following independent parameters,
θ = (θa1, θb1|a1, θb1|a2),

18.4 LEARNING WITH CONTINUOUS PARAMETER SETS
497
leaving out dependent parameters θa2, θb2|a1, and θb2|a2. The density ρ(D, θ) can now
be viewed as a function f (θa1, θb1|a1, θb1|a2) of these independent parameters where the
marginal likelihood is the result of integrating over all independent parameters:
P(D) =

 
 
f (θa1, θb1|a1, θb1|a2)dθa1dθb1|a1dθb1|a2.
The main observation underlying large-sample approximations concerns the behav-
ior of the function f as the data set size tends to inﬁnity. In particular, under certain
conditions this function becomes peaked at MAP parameter estimates, allowing it to be
approximated by a multivariate Gaussian g(θa1, θb1|a1, θb1|a2) whose mean are the MAP
estimates. By integrating over the independent parameters of this Gaussian, we obtain
the following approximation of the marginal likelihood, which is known as the Laplace
approximation:
log P(D) ≈log P(D|θma) + log ρ(θma) + d
2 log(2π) −1
2 log ||.
(18.22)
Here d is the number of independent parameters, θma are the MAP estimates, and  is
the Hessian of −log ρ(D, θ) evaluated at the MAP estimates. That is,  is the matrix of
second partial derivatives:
 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−∂2 log ρ(D, θ)
∂θa1∂θa1
(θma)
−∂2 log ρ(D, θ)
∂θa1∂θb1|a1
(θma)
−∂2 log ρ(D, θ)
∂θa1∂θb1|a2
(θma)
−∂2 log ρ(D, θ)
∂θa1∂θb1|a1
(θma)
−∂2 log ρ(D, θ)
∂θb1|a1∂θb1|a1
(θma)
−∂2 log ρ(D, θ)
∂θb1|a1∂θb1|a2
(θma)
−∂2 log ρ(D, θ)
∂θa1∂θb1|a2
(θma)
−∂2 log ρ(D, θ)
∂θb1|a1∂θb1|a2
(θma)
−∂2 log ρ(D, θ)
∂θb1|a2∂θb1|a2
(θma)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Computing the Laplace approximation requires a number of auxiliary quantities, some
of which are straightforward to obtain, while others can be computationally demanding.
In particular, we must ﬁrst compute the MAP estimates θma as discussed in the previous
section. We must also compute the prior density of MAP estimates ρ(θma) and their
likelihood P(D|θma). Both of these quantities can be computed as given by (18.18)
and (18.20), respectively. We must ﬁnally compute the Hessian  and its determinant
||. Computing the Hessian requires the computation of second partial derivatives, as
discussed in Exercise 18.20. The determinant of matrix  can be computed in O(d3).
The Laplace approximation can be quite accurate. For example, it is known to have
a relative error that is O(1/N) under certain conditions, which include the existence
of unique MAP estimates. However, the main computational problem with the Laplace
approximation is in computing the Hessian and its determinant. This can be especially
problematic when the number of independent parameters d is large. To deal with this
difﬁculty, it is not uncommon to simplify the Hessian by assuming
∂2 log ρ(D, θ)
∂θx|u∂θy|v
(θma) = 0
for X ̸= Y.

498
LEARNING: THE BAYESIAN APPROACH
1
|a
B
θ
2
|a
B
θ
A
θ
2
A
2
B
N
A
N
B
1
A
1
B
1
|a
B
θ
2
|a
B
θ
A
θ
1
1B
A
N
NB
A
1
|b
A
θ
2
|b
A
θ
B
θ
Ω
(a) Meta-network (parameters)
(b) Meta-network (structure)
Figure 18.6: Bayesian learning with unknown network structure.
Another simpliﬁcation of the Laplace approximation involves retaining only those
terms that increase with the data set size N and replacing the MAP estimates by maximum
likelihood estimates. Since the likelihood term log P(D|θma) increases linearly in N and
the determinant term log || is known to increase as d log N, we get
log P(D) ≈log P(D|θml) −d
2 log N.
This approximation is known as the Bayesian information criteria (BIC). It also corre-
sponds to the MDL score discussed in Chapter 17. This should not be surprising, as the
main difference between the maximum likelihood and Bayesian approaches is in the prior
on network parameters, and the effect of these priors is known to diminish as the sample
size tends to inﬁnity.
18.5 Learning network structure
Our discussion of the Bayesian learning approach has thus far assumed that we know the
network structure yet are uncertain about network parameters. We now consider the more
general case where both network structure and parameters are unknown. To handle this
case, we consider a structural meta-network that not only includes parameter sets but also
an additional variable to capture all possible network structures. Figure 18.6(b) depicts
such a network for a problem that includes two variables A and B.
There are a number of differences between this structural meta-network and the one
in Figure 18.6(a), which assumes a ﬁxed network structure. First, we have now lumped
variables A and B into a single variable AB that has four values a1b1, a1b2, a2b1, and a2b2.
Second, we have included an additional variable  that has three values, corresponding
to the three possible structures over two variables:

Network structure
Parameter sets
G1
A, B
θA, θB
G2
A →B
θA, θB|a1, θB|a2
G3
B →A
θB, θA|b1, θA|b2

18.5 LEARNING NETWORK STRUCTURE
499
The third difference is that the structural meta-network has six parameter sets instead of
three, as shown in the previous table, to account for network structures G1 and G3. The
meta-network in Figure 18.6(b) is then saying that the distribution over variables A, B
is determined by the network structure  and the values of parameter sets θA, θB, θB|a1,
θB|a2, θA|b1, and θA|b2.
In principle, the Bayesian approach is the same whether we have a ﬁxed network
structure or not. However, the main difference is that:
r We need more information to specify a structural meta-network.
r Inference on a structural meta-network is more difﬁcult.
Regarding the ﬁrst point, note that we now need a prior distribution over network structures
(the CPT for variable ). We also need to specify priors over many more parameter sets
to cover every possible network structure. As to the increased complexity of inference,
note for example that
P(αN+1|D) =

G
P(αN+1|D, G)P(G|D),
where P(αN+1|D, G) is the computational problem we treated earlier in Sections 18.3
and 18.4 (structure G was kept implicit in those sections).
However, this computation is not feasible in practice as it requires summing over all
possible network structures G. Two simpliﬁcation techniques are typically brought to
bear on this difﬁculty. According to the ﬁrst technique, called selective model averaging,
we restrict the learning process to a subset of network structures to reduce the size of
summation. According to the second and more common technique, known as model
selection, we search for a network structure that has the highest posterior probability and
then use this structure for answering further queries. In particular, we search for a network
structure that satisﬁes
G⋆= argmax
G
P(G|D).
This is the same as searching for a structure that satisﬁes
G⋆= argmax
G
P(D|G)P(G)
P(D)
= argmax
G
P(D|G)P(G)
since the marginal likelihood P(D) does not depend on the network structure G. We can
also write this as
G⋆= argmax
G
log P(D|G) + log P(G).
This score is now very similar to those discussed in Section 17.4. This is known as a
Bayesian scoring measure, where its two components can be interpreted similarly to the
two components of the general measure given by (17.13). In particular, the component
log P(G) can be viewed as a penalty term, typically assigning lower probabilities to more
complex network structures. Moreover, the component log P(D|G) can be viewed as
measure of ﬁt for the structure G to data set D. In fact, we next deﬁne some speciﬁc
Bayesian scoring measures and show that they can be decomposed similar to the measure

500
LEARNING: THE BAYESIAN APPROACH
in (17.13), allowing us to use techniques from Section 17.5 when searching for a network
structure.
18.5.1 The BD score
The Bayesian score is actually a class of scores. We next discuss one of the more common
subclasses of Bayesian scores known as Bayesian Dirichlet (BD), which is based on some
assumptions we considered earlier:
r Parameter independence
r Dirichlet priors
r Complete data sets.
Under these assumptions, the marginal likelihood given a network structure G, P(D|G),
is provided by (18.21) (the structure G is implicit in that equation). The BD score of a
Bayesian network structure G with families XU is then given by
BD(G|D)
def=
P(D|G)P(G)
=
P(G)

XU

u
(ψX|u)
(ψX|u + D#(u))

x
(ψx|u + D#(xu))
(ψx|u)
.
(18.23)
This is still a family of scores as one must also deﬁne the probability of every structure
G and provide a Dirichlet prior for each parameter set θX|u of structure G. Specifying
these elements of a BD score is rather formidable in practice, so different assumptions are
typically used to simplify this speciﬁcation process.
Suppose for example that we have a total ordering X1, . . . , Xn on network variables
where variable Xj can be a parent of variable Xi only if j < i. Suppose further that pji
is the probability of Xj being a parent of Xi (pji = 0 if j ≥i). If XiUi are the families
of structure G and if the presence of edges in a particular structure are assumed to be
mutually independent, the probability of structure G is then given by
P(G) =

XiUi
⎡
⎣
Xj ∈Ui
pji
⎤
⎦
⎡
⎣
Xj̸∈Ui
(1 −pji)
⎤
⎦.
(18.24)
Note how the prior distribution decomposes into a product of terms, one for each family
in the given structure. Note also that we only need n(n −1)/2 probability assessments,
pji, to fully specify the prior distribution on all network structures. These assessments
can be reduced signiﬁcantly if we are certain about the presence of certain edges, pji = 1,
or absence of these edges, pji = 0.7
Assumptions have also been made to simplify the speciﬁcation of the Dirichlet priors,
including the use of noninformative priors where all exponents ψx|u are set to 1 (this is
known as the K2 score). We next provide a more realistic set of assumptions that leads to
a more sophisticated score.
7 We can adopt a similar prior distribution without having to impose a total ordering on the variables but this slightly
complicates the score description.

18.5 LEARNING NETWORK STRUCTURE
501
18.5.2 The BDe score
The BDe score (for BD likelihood equivalence) is a common instance of the BD family,
which is based on some additional assumptions. The most important of these assumptions
are:
r Parameter modularity
r Likelihood equivalence.
Parameter modularity says that the Dirichlet prior for parameter set θX|u is the same
in every network structure that contains this parameter set. Consider for example the two
network structures G1 : A →B, B →C and G2 : A →B, A →C. Variable B has the
same set of parents, A, in both structures. Hence, the parameter set θB|a with its prior
distribution is shared between structures G1 and G2 according to parameter modularity.
The structural meta-network in Figure 18.6 embeds this assumption of parameter
modularity, leading to only six Dirichlet priors:

Network structure
Parameter sets
G1
A, B
θA,
θB
G2
A →B
θA,
θB|a1,
θB|a2
G3
B →A
θB,
θA|b1,
θA|b2
If we did not have parameter modularity, we would need eight priors instead, correspond-
ing to eight distinct parameter sets:

Network structure
Parameter sets
G1
A, B
θG1
A ,
θG1
B
G2
A →B
θG2
A ,
θG2
B|a1,
θG2
B|a2
G3
B →A
θG3
B ,
θG3
A|b1,
θG3
A|b2
Hence, the Dirichlet prior for parameter set θG1
A
could be different from the one
for θG2
A .
The characteristic assumption of the BDe score is that of likelihood equivalence, which
says that a data set should never discriminate among network structures that encode the
same set of conditional independencies.8 That is, if structures G1 and G2 represent the
same set of conditional independencies, then P(D|G1) = P(D|G2). An example of this
happens when the two structures are complete, as neither structure encodes any conditional
independencies.
As it turns out, these assumptions in addition to some other weaker assumptions9 imply
that the exponent ψx|u of any parameter set θX|u has the following form:
ψx|u = ψ · Pr(x, u),
(18.25)
8 The assumptions of prior equivalence and score equivalence have also been discussed. Prior equivalence says
that the prior probabilities of two network structures are the same if they represent the same set of conditional
independencies. Score equivalence holds when we have both prior equivalence and likelihood equivalence.
9 These include the assumption that every complete structure has a nonzero probability and the assumption that all
parameter densities are positive.

502
LEARNING: THE BAYESIAN APPROACH
where
r ψ > 0 is called the equivalent sample size.
r Pr is called the prior distribution and is deﬁned over the base network.
The prior distribution is typically speciﬁed using a Bayesian network called the prior
network. Together with an equivalent sample size, the prior network is all we need to
specify the Dirichlet prior for every parameter set. Note, however, that obtaining the value
of an exponent using (18.25) requires inference on the prior network.
To shed more light on the semantics of a prior network and equivalent sample size,
consider the Dirichlet distribution they deﬁne for a given parameter set θX|u. First, note
that the equivalent sample size for this Dirichlet is
ψX|u =

x
ψ · Pr(x, u) = ψPr(u).
Moreover, given the properties of a Dirichlet distribution see (18.10) and (18.11), we have
the following expectation and variance for parameter θx|u:
Ex(θx|u) = Pr(x|u)
Va

θx|u

= Pr(x|u)(1 −Pr(x|u))
ψ · Pr(u) + 1
.
Hence, the prior network provides expectations for the parameters of any network struc-
ture. Moreover, the equivalent sample size ψ has a direct impact on our conﬁdence in these
expectations: a larger ψ implies a larger conﬁdence. The conﬁdence in these expected
values also increases as the probability Pr(u) increases.
Since the BDe score is based on the assumption of likelihood equivalence, it leads to the
same marginal likelihood for network structures that encode the same set of conditional
independencies. Moreover, it is known that decreasing the equivalent sample size ψ
diminishes the impact that the prior network has on the BDe score. We also mention that
adopting a prior network with no edges is not an uncommon choice when using the BDe
score. Another choice for the prior network is one that includes uniform CPTs, leading
to exponents ψx|u = ψ/X#U# where X#U# is the number of instantiations for the family
XU. This instance of the BDe score is known as BDeu (for BDe uniform).
For an example of specifying Dirichlet exponents in the BDe score, consider a domain
with two binary variables A and B. Let the equivalent sample size be ψ = 12 and the
prior distribution be
A
B
Pr(.)
a1
b1
1/4
a1
b2
1/6
a2
b1
1/4
a2
b2
1/3
Equation 18.25 then gives the following Dirichlet exponents:
ψa1
ψa2
ψb1
ψb2
ψb1|a1
ψb2|a1
ψb1|a2
ψb2|a2
= ψa1|b1
= ψa2|b1
= ψa1|b2
= ψa2|b2
5
7
6
6
3
2
3
4

18.5 LEARNING NETWORK STRUCTURE
503
Consider now the data set d1 = a1b1, d2 = a1b2, the structure G : A →B, and let us
use the closed form in (18.21) to compute the likelihood of structure G:
P(d1, d2|G)
=
1
(ψA)
(ψA + D#(⊤))
(ψa1 + D#(a1))
(ψa1)
(ψa2 + D#(a2))
(ψa2)
2
1
(ψB|a1)
(ψB|a1 + D#(a1))
(ψb1|a1 + D#(b1a1))
(ψb1|a1)
(ψb2|a1 + D#(b2a1))
(ψb2|a1)
2
1
(ψB|a2)
(ψB|a2 + D#(a2))
(ψb1|a2 + D#(b1a2))
(ψb1|a2)
(ψb2|a2 + D#(b2a2))
(ψb2|a2)
2
.
Recall that ⊤is the trivial variable instantiation and, hence, D#(⊤) is the size of data set,
which is 2 in this case. Substituting the values of exponents and data counts, we get
P(d1, d2|G) =
1
(12)
(12 + 2)
(5 + 2)
(5)
(7 + 0)
(7)
2
1
(5)
(5 + 2)
(3 + 1)
(3)
(2 + 1)
(2)
2
1
(7)
(7 + 0)
(3 + 0)
(3)
(4 + 0)
(4)
2
= 1
26.
We can verify that the likelihood of structure G : B →A is also 1/26, which is to be
expected as both structures encode the same set of conditional independencies.
18.5.3 Searching for a network structure
Consider now the logarithm of the BD score given in (18.23):
log BD(G|D) = log P(G)
+

XU
log

u
(ψX|u)
(ψX|u + D#(u))

x
(ψx|u + D#(xu))
(ψx|u)
.
(18.26)
If the distribution over network structures, P(G), decomposes into a product over families,
as in (18.24), the whole BD score is then a sum of scores, one for each family:
log BD(G|D) =

XU
log BD(X, U|D).
This BD score has the same structure as the one given in (17.15). Therefore, the heuristic
and optimal search algorithms discussed in Section 17.5 can now be exploited to search
for a network structure that maximizes the BD score and its variants, such as the BDe and
BDeu scores.

504
LEARNING: THE BAYESIAN APPROACH
We close this section by pointing out that our discussion on the search for network
structures is restricted to complete data sets for the same computational reasons we
encountered in Chapter 17. That is, the posterior P(G|D) ∝P(D|G)P(G) does not admit
a closed form, and neither does it decompose into family components, when the data set
is incomplete. Hence, algorithms for learning structures under incomplete data may have
to use approximations such as the Laplace approximation to compute P(D|G).10 Even
then, evaluating the score of a network structure continues to be demanding, making the
search for a network structure quite expensive in general.
Bibliographic remarks
The material discussed in this chapter has its roots in the seminal work of Cooper and
Herskovits [1991; 1992], who provided some of the ﬁrst results on learning Bayesian
networks, including the BD score, the K2 score, and the derivation of (18.21) and (18.14).
The BD score was reﬁned in Buntine [1991], who proposed the BDeu score and the
prior distribution on network structures given by (18.24). The BD score was further
reﬁned in Heckerman et al. [1995], who introduced the general form of the likelihood
equivalence assumption, leading to the BDe score. Interestingly, the BDe score was
derived from a set of assumptions that includes parameter modularity and independence
but not the assumption of Dirichlet priors (parameter modularity was ﬁrst made explicit
in Heckerman et al. [1995]). Yet these assumptions were shown to imply such priors; see
also Geiger and Heckerman [1995]. Related work on Bayesian learning was also reported
in Dawid and Lauritzen [1993] and Spiegelhalter et al. [1993]. A real medical example
is discussed with some detail in Spiegelhalter et al. [1993], which also provides links
to mainstream statistical methods such as model comparison. A particular emphasis on
learning undirected structures can be found in Dawid and Lauritzen [1993].
The terms of global and local (parameter) independence were introduced in Spiegelhal-
ter and Lauritzen [1990]. Relaxing these assumptions to situations that include parameter
equality is addressed computationally in Thiesson [1997].
The complexity of learning network structures was addressed in Chickering [1996] and
Chickering et al. [1995; 2004]. For example, it was shown in Chickering [1996] that for a
general class of Bayesian scores, learning a network that maximizes the score is NP-hard
even when the data set is small, and each node has at most two parents. Heuristic search
algorithms are discussed and evaluated in Heckerman et al. [1995]. Methods for learning
network structures in the case of incomplete data are presented in Friedman [1998], Meilua
and Jordan [1998], Singh [1997], and Thiesson et al. [1999], while methods for learning
Bayesian networks with local structure in the CPTs are discussed in Buntine [1991],
Díez [1993], Chickering et al. [1997], Friedman and Goldszmidt [1996], and Meek and
Heckerman [1997].
The candidate method for approximating the marginal likelihood is discussed in Chib
[1995]. A number of approximation techniques for the marginal likelihood are surveyed
and discussed in Chickering and Heckerman [1997]. This includes a detailed discussion
of the Laplace approximation with references to its origins, in addition to a number of
alternative approximations such as Cheeseman and Stutz [1995]. The study contains an
empirical evaluation and corresponding discussion of some practical issues that arise
when implementing these approximation approaches.
10 In the Laplace approximation given by (18.22), the structure G is left implicit.

18.6 EXERCISES
505
A comprehensive tutorial on learning Bayesian networks is provided in Heckerman
[1998], who surveys most of the foundational work on the subject and provides many
references for more advanced topics than covered in this chapter. Related surveys of the
literature can also be found in Buntine [1996] and Jordan [1998]. The subject of learning
Bayesian networks is also discussed in Cowell et al. [1999] and Neapolitan [2004], who
provide more details on some of the subjects discussed here and cover additional topics
such as networks with continuous variables.
18.6 Exercises
18.1. Consider the meta-network in Figure 18.2 and the corresponding data set D:
Case
H
S
E
1
F
F
T
2
T
F
T
3
T
F
T
4
F
F
F
5
F
T
F
Assume the following ﬁxed values of parameter sets:
θS|h = (.1, .9),
θE|h = (.8, .2).
For the remaining sets, suppose that we have the following prior distributions:
θH = (θh, θ¯h)
P(θH)
(.75, .25)
80%
(.90, .10)
20%
θS|¯h = (θs|¯h, θ¯s|¯h)
P(θS|¯h)
(.25, .75)
10%
(.50, .50)
90%
θE|¯h = (θe|¯h, θ¯e|¯h)
P(θE|¯h)
(.50, .50)
35%
(.75, .25)
65%
Compute the following given the data set D:
(a) The maximum likelihood parameter estimates.
(b) The MAP parameter estimates.
(c) The Bayesian parameter estimates.
Compute the probability of observing an individual who is a smoker and exercises regu-
larly given each of the above parameter estimates. Compute also P(S6 =s, E6 =e|D) with
respect to the meta-network.
18.2. Consider a network structure A →B where variables A and B are binary. Suppose that we
have the following priors on parameter sets:
θA = (θa1, θa2)
P(θA)
(.80, .20)
70%
(.20, .80)
30%
θB|a1 = (θb1|a1, θb2|a1)
P(θB|a1)
(.90, .10)
50%
(.10, .90)
50%
θB|a2 = (θb1|a2, θb2|a2)
P(θB|a2)
(.95, .05)
60%
(.05, .95)
40%
Compute the posterior distributions of parameter sets given the data set
D
A
B
d1
a1
b2
d2
a1
b2
d3
a1
b1
d4
a2
b2

506
LEARNING: THE BAYESIAN APPROACH
Compute also the expected parameter values given this data set. Finally, evaluate the query
P(A5 =a1|B5 =b2, D).
18.3. Consider Exercise 18.2. Describe a meta-network that captures the following constraint
θB|a1 = (.90, .10) iff θB|a2 = (.05, .95).
18.4. Consider a Bayesian network structure with three binary variables A, B, and C and two edges
A →B and A →C. Suppose that the CPTs for variables B and C are equal and these
CPTs are independent of the one for variable A. Draw the structure of a meta-network for this
situation. Develop a closed form for computing the posterior distribution on parameter set
θB|a1 given a complete data set D. Assume ﬁrst that parameter sets are discrete, deriving a
closed form that resembles Equation 18.4. Then generalize the form to continuous parameter
sets with Dirichlet distributions.
18.5. Consider a meta-network as given by Deﬁnition 18.2. Let  contain XN for every variable
X in structure G and let  contain X1, . . . , XN−1 for every variable X in structure G. Show
that  is d-separated from  by the nodes representing parameter sets.
18.6. Consider Theorem 18.2 and show that Prθ(α) = P(αi|θ), where αi results from replacing
every variable X in α by its instance Xi.
18.7. Consider Theorem 18.3 and show that
P(αN+1|βN+1, D) = Prθbe(α|β).
18.8. Consider Theorem 18.2 and show that
P(αN+1|βN+1, D) =

θ
Prθ(α|β)P(θ|βN+1, D).
18.9. Consider Theorem 18.4. Show that
P(D|θX|u) ∝

x

θx|u
D#(xu)
.
18.10. Prove Equation 18.9.
18.11. Prove Equation 18.27 which appears on Page 514.
18.12. Let D be an incomplete data set and let Dc denote a completion of D. Show that the
expected value of a network parameter θx|u given data set D is
Ex(θx|u) =

Dc
f (DDc)P(Dc|D),
where f (DDc) is the value of the Bayesian estimate θbe
x|u given the complete data set DDc.
18.13. Consider a network structure A →B where variables A and B are binary and the Dirichlet
priors are given by the following exponents:
ψa1
ψa2
ψb1|a1
ψb2|a1
ψb1|a2
ψb2|a2
10
30
2
2
10
40
Suppose that we are given the following data set D:
A
B
d1
a1
b2
d2
a1
b1
d3
a2
b1
Compute:
(a) The Bayesian parameter estimates θbe
a1 , θ be
b1|a1, and θbe
b1|a2.
(b) The marginal likelihood P(D).

18.6 EXERCISES
507
18.14. Consider the network structure A →B and the Dirichlet priors from Exercise 18.13. Com-
pute the expected values of parameters θa1, θb1|a1, and θb1|a2 given the following data set:
A
B
d1
a1
b2
d2
?
b1
d3
a2
?
Hint: Perform case analysis on the missing values in the data set.
18.15. Consider a meta-network that induces a meta-distribution P, let D be an incomplete data set,
and let θ be a parametrization. Describe a Gibbs sampler that can be used for estimating
the expectation of P(θ|D, Dc) with respect to the distribution P(Dc|D) where Dc is a
completion of the data set D. That is, describe a Gibbs-Markov chain for the distribution
P(C|D), where C are the variables with missing values in D, and then show how we can
simulate this chain. Solve this problem for both discrete and continuous parameter sets.
18.16. Identify the relationship between the expected counts of Deﬁnition 18.4 and the expected
empirical distribution of Deﬁnition 17.2.
18.17. Show that Algorithm 50, ML EM, is a special case of Algorithm 52, MAP EM C, in the case
where all parameter sets θX|u have noninformative priors (i.e., all Dirichlet exponents are
equal to 1). Note: In this case, the ﬁxed points of Algorithm 52 are also stationary points of
the log-likelihood function.
18.18. The Hessian matrix of the Laplace approximation is speciﬁed in terms of three types of
second partial derivatives, with respect to:
r
A single parameter θx|u
r
Parameters θx|u and θx⋆|u of the same parameter set θX|u, where x ̸= x⋆
r
Parameters θx|u and θy|v of two different parameter sets θX|u and θY|v.
(a) Show that the partial derivative of the Dirichlet density of parameter set θX|u with respect to
parameter θx|u is
∂ρ(θX|u)
∂θx|u
= ψx|u −1
θx|u
· ρ(θX|u).
(b) Given (a), show that the partial derivative of the prior density of network parameters ρ(θ) is
∂ρ(θ)
∂θx|u
= ψx|u −1
θx|u
· ρ(θ).
(c) Given (a) and (b), show how to compute the second partial derivatives
∂2ρ(θ)
∂θx|u∂θx|u
,
∂2ρ(θ)
∂θx|u∂θx⋆|u
,
∂2ρ(θ)
∂θx|u∂θy|v
.
18.19. Analogously to Exercise 18.18, consider partial derivatives of the likelihood of network pa-
rameters P(D|θ).
(a) Show that the partial derivative of the likelihood P(D|θ) with respect to parameter θx|u is
∂P(D|θ)
∂θx|u
= P(D|θ) · D#(xu|θ)
θx|u
.
(b) Given (a), show how to compute the second partial derivatives
∂2P(D|θ)
∂θx|u∂θx|u
,
∂2P(D|θ)
∂θx|u∂θx⋆|u
,
∂2P(D|θ)
∂θx|u∂θy|v
.
18.20. Consider the second partial derivative
∂2 log ρ(D, θ)
∂θx|u∂θy|v
(θma)

508
LEARNING: THE BAYESIAN APPROACH
used in the Laplace approximation. Show how this derivative can be computed by perform-
ing inference on a Bayesian network with parametrization θma. What is the complexity of
computing all the second partial derivatives (i.e., computing the Hessian)? Hint: Use the fact
that ρ(D, θ) = P(D|θ)ρ(θ) and consider Exercises 18.18 and 18.19.
18.21. Extend the structural meta-network in Figure 18.6 to allow for reasoning about incomplete
data sets. That is, the meta-network needs to have additional nodes to allow us to assert an
incomplete data set as evidence.
18.22. Consider a BDe score that is speciﬁed by a prior distribution Pr. Let G be a complete network
structure and d be a complete case. Show that Pr(d) = P(d|G) where P is the corresponding
meta-distribution. Note: This exercise provides additional semantics for the prior distribution
of a BDe score.
18.23. Consider the BDe score, a sample size of ψ = 12, and the prior distribution
A
B
Pr(.)
a1
b1
1/4
a1
b2
1/6
a2
b1
1/4
a2
b2
1/3
Consider also the data set d1 = a1b1, d2 = a1b2 and the structures G1 : A →B and G2 :
A, B. Compute the likelihood of each structure using Equation 18.21.
18.24. Consider the BDe score, a sample size of ψ = 12, and the prior distribution
A
B
Pr(.)
a1
b1
1/4
a1
b2
1/6
a2
b1
1/4
a2
b2
1/3
Consider also the data set d1 = a1b1, d2 = a1b2 and the structure G : A →B. Compute
the probabilities P(d1|G), P(d2|d1, G) and P(d1, d2|G). Compute the same quantities for
the structure G : B →A as well.
18.7 Proofs
PROOF OF THEOREM 18.1. Let  contain all variables that are not parameter sets (i.e.,
variables instantiated by a complete data set). We need to show the following: If 1 and 2
contain a collection of parameter sets each, 1 ∩2 = ∅, then 1 and 2 are independent
and they remain independent given .
Independence holds initially since parameter sets are root nodes in a meta-network and
since root nodes are d-separated from each other in a Bayesian network. Hence, 1 and
2 are d-separated in the meta-network.
To show that parameter independence continues to hold given a complete data set D
(i.e., given ), we point out the two types of edge pruning discussed in Section 18.2.2.
Pruning these edges makes each pair of parameter sets disconnected from each other;
hence, 1 and 2 are d-separated by  in the pruned meta-network. Moreover, the joint
distribution P′(., D) induced by the pruned meta-network agrees with the joint distribution
P(., D) induced by the original meta-network. Hence, the two meta-networks agree on
the independence of 1 and 2 given D.
■

18.7 PROOFS
509
PROOF OF THEOREM 18.2. We have
P(αN+1|D) =

θ
P(αN+1|D, θ)P(θ|D)
=

θ
P(αN+1|θ)P(θ|D)
=

θ
Prθ(α)P(θ|D).
The second step is left to Exercise 18.5 and the third step is left to Exercise 18.6.
■
PROOF OF THEOREM 18.3. According to Theorem 18.2, we have
P(αN+1|D) =

θ
Prθ(α)P(θ|D).
Hence, all we need to show is that
Prθbe(α) =

θ
Prθ(α)P(θ|D).
In fact, it sufﬁces to show this result for complete instantiations d of the base network G
since:
Prθbe(α) =

d|=α
Prθbe(d)
=

d|=α

θ
Prθ(d)P(θ|D)
=

θ
⎛
⎝
d|=α
Prθ(d)
⎞
⎠P(θ|D)
=

θ
Prθ(α)P(θ|D).
Recall here that d |= α reads: instantiation d satisﬁes event α.
Suppose now that X1U1, . . . , XnUn are the families of base network G and let
u1, . . . , un be the instantiations of parents U1, . . . , Un in network instantiation d. Let
us also decompose a particular parametrization θ into two components: θd, which con-
tains values of parameter sets θX1|u1, . . . , θXn|un that are relevant to the probability of d,
and θ¯d, which contains the values of remaining parameter sets (irrelevant to d). Note that
the probability of network instantiation d, Prθ(d), depends only on the θd component of
parametrization θ. We then have

θ=θd,θ¯d
Prθ(d)P(θ|D) =

θd,θ¯d
Prθd,θ¯d(d)P(θd|D)P(θ¯d|D)
=

θ¯d,θX1|u1,...,θXn|un
Prθd,θ¯d(d)P(θX1|u1, . . . , θXn|un|D)P(θ¯d|D)
=

θ¯d,θX1|u1,...,θXn|un
Prθd,θ¯d(d)
 n

i=1
P(θXi|ui|D)

P(θ¯d|D)

510
LEARNING: THE BAYESIAN APPROACH
=

θ¯d,θX1|u1,...,θXn|un
⎛
⎝
xi∼d
θxi|ui
⎞
⎠
 n

i=1
P(θXi|ui|D)

P(θ¯d|D)
=

θ¯d,θX1|u1,...,θXn|un
⎛
⎝
xi∼d
θxi|ui · P(θXi|ui|D)
⎞
⎠P(θ¯d|D)
=

θX1|u1,...,θXn|un
⎛
⎝
xi∼d
θxi|ui · P(θXi|ui|D)
⎞
⎠
θ¯d
P(θ¯d|D).
Observing that 
θ¯d P(θ¯d|D) = 1, we now have

θ
Prθ(d)P(θ|D) =

θX1|u1,...,θXn|un

xi∼d
θxi|ui · P(θXi|ui|D)
=

xi∼d

θXi |ui
θxi|ui · P(θXi|ui|D)
=

xi∼d
θbe
xi|ui
= Prθbe(d).
■
PROOF OF THEOREM 18.4. Consider the meta-network after we have pruned all edges
given the complete data set D, as given in Section 18.2.2. Each parameter set θX|u becomes
part of an isolated network structure that satisﬁes the following properties:
r It is a naive Bayes structure with θX|u as its root and some instances Xi of X as its children.
For example, in Figure 18.3(b) θE|h is the root of a naive Bayes network with variables E1
and E2 as its children. Similarly, θH is the root of a naive Bayes network with variables
H1, H2, and H3 as its children.
r Instance Xi is connected to θX|u only if its parents are instantiated to u in the data set D.
If not, the edge θX|u →Xi is superﬂuous and then pruned.
r The number of instances Xi connected to θX|u is therefore D#(u).
r The number of instances Xi connected to θX|u and having value x is D#(xu).
Let us now decompose the data set D into two parts, D1 and D2, where D1 is the
instantiation of variables that remain connected to parameter set θX|u after pruning. We
then have
P(θX|u|D) = P(θX|u|D1)
= P(D1|θX|u)P(θX|u)
P(D1)
.
Let Xi, i ∈I, be the instances of X that remain connected to parameter set θX|u and let xi
be their instantiated values. We then have
P(D1|θX|u) =

i∈I
P(Xi =xi|θX|u)
=

i∈I
θxi|u
=

x

θx|u
D#(xu)
.

18.7 PROOFS
511
Hence,
P(θX|u|D) = η P(θX|u)

x

θx|u
D#(xu)
,
where the normalizing constant η is 1/P(D1).
■
PROOF OF THEOREM 18.5. Suppose that θk+1 = argmaxθ e(θ|D, θk). It then follows
that
e(θk+1|D, θk) −e(θk|D, θk) ≥0
and

Dc
P(Dc|D, θk) log P(θk+1|D, Dc) −

Dc
P(Dc|D, θk) log P(θk|D, Dc) ≥0.
Moreover,

Dc
P(Dc|D, θk) log P(θk+1|D, Dc)
P(θk|D, Dc)
≥0

Dc
P(Dc|D, θk) log
"P(Dc|D, θk+1)
P(Dc|D, θk)
P(D, θk+1)
P(D, θk)
#
≥0

Dc
P(Dc|D, θk) log P(Dc|D, θk+1)
P(Dc|D, θk) +

Dc
P(Dc|D, θk) log P(D, θk+1)
P(D, θk)
≥0.
Moving the ﬁrst term to the right-hand side, we get

Dc
P(Dc|D, θk) log P(D, θk+1)
P(D, θk)
≥

Dc
P(Dc|D, θk) log P(Dc|D, θk)
P(Dc|D, θk+1).
The right-hand side is now a KL divergence, which is ≥0. We therefore have

Dc
P(Dc|D, θk) log P(D, θk+1)
P(D, θk)
≥0
log P(D, θk+1)
P(D, θk)

Dc
P(Dc|D, θk) ≥0
log P(D, θk+1)
P(D, θk)
≥0.
It then follows that
P(D, θk+1) ≥P(D, θk) and P(θk+1|D) ≥P(θk|D).
■
PROOF OF THEOREM 18.6. In the following proof, Xu ranges over the indices of pa-
rameter sets, that is, X is a variable in the base network and u is an instantiation of its

512
LEARNING: THE BAYESIAN APPROACH
parents. We have
e(θ|D, θk) =

Dc
P(Dc|D, θk) log P(θ|D, Dc)
=

Dc
P(Dc|D, θk) log

Xu
P(θX|u|D, Dc)
=

Dc
P(Dc|D, θk)

Xu
log P(θX|u|D, Dc)
=

Xu

Dc
P(Dc|D, θk) log P(θX|u|D, Dc).
To maximize e(θ|D, θk), we therefore need to maximize a number of independent prob-
lems. That is, we need to choose parameter sets θX|u independently to maximize

Dc
P(Dc|D, θk) log P(θX|u|D, Dc),
which is the same as maximizing (see Theorem 18.4):

Dc
P(Dc|D, θk) log P(θX|u)

x

θx|u
[DDc]#(xu)
=

Dc
P(Dc|D, θk) log P(θX|u) +

Dc

x
P(Dc|D, θk) log

θx|u
[DDc]#(xu)
= log P(θX|u) +

x

Dc
P(Dc|D, θk)

[DDc]#(xu)

log θx|u
= log P(θX|u) +

x
log θx|u

Dc
P(Dc|D, θk)

[DDc]#(xu)

= log P(θX|u) +

x

log θx|u

D#(xu|θk) by Deﬁnition 18.4
= log

P(θX|u)

x

θx|u
D#(xu|θk)
.
Finally, maximizing this expression is the same as maximizing
P(θX|u)

x

θx|u
D#(xu|θk)
.
■
PROOF OF THEOREM 18.7. The proof is similar to that for Theorem 18.2 except that
we integrate instead of summing over parameterizations.
■
PROOF OF THEOREM 18.8. The proof is similar to that for Theorems 18.3 except that
we integrate instead of summing over parameterizations.
■

18.7 PROOFS
513
PROOF OF THEOREM 18.9. This proof is based on that for Theorem 18.4. In particular,
if we deﬁne D1 as deﬁned in that proof, we have
ρ(θX|u|D) = ρ(θX|u|D1)
= P(D1|θX|u)ρ(θX|u)
P(D1)
=
P(D1|θX|u) · η

x

θx|u
ψx|u−1
P(D1)
,
where η is the normalizing constant for the Dirichlet density ρ(θX|u). Moreover, as shown
in the proof of Theorem 18.4, we have
P(D1|θX|u) =

x

θx|u
D#(xu)
.
Hence, we have
ρ(θX|u|D) =

x

θx|u
D#(xu)

· η

x

θx|u
ψx|u−1
P(D1)
=
η

x

θx|u
ψx|u+D#(xu)−1
P(D1)
.
This is a Dirichlet density with exponents ψx|u + D#(xu) and normalizing constant
η /P(D′).
■
PROOF OF THEOREM 18.10. The proof for this theorem resembles that for Theo-
rem 18.5.
■
PROOF OF THEOREM 18.11. In the following proof, Xu ranges over the indices of
parameter sets, that is, X is a variable in the base network and u is an instantiation of its
parents. We have
e(θ|D, θk) =

Dc
P(Dc|D, θk) log ρ(θ|D, Dc)
=

Dc
P(Dc|D, θk) log

Xu
ρ(θX|u|D, Dc)
=

Dc
P(Dc|D, θk)

Xu
log ρ(θX|u|D, Dc)
=

Xu

Dc
P(Dc|D, θk) log ρ(θX|u|D, Dc).
Maximizing e(θ|D, θk) is then equivalent to choosing each parameter set θX|u indepen-
dently to maximize

Dc
P(Dc|D, θk) log ρ(θX|u|D, Dc).
Substituting the value of the posterior density ρ(θX|u|D, Dc) as given by Theorem 18.9
and using simpliﬁcations similar to those used in the proof of Theorem 18.6, we can show

514
LEARNING: THE BAYESIAN APPROACH
that maximizing this expression is the same as maximizing the following:

x

θx|u
ψx|u+D#(xu|θk)−1
,
which is an un-normalized Dirichlet distribution. Using (18.12), the mode of this Dirichlet
is given by
θx|u = D#(xu|θk) + ψx|u −1
D#(u|θk) + ψX|u −|X|.
■
Theorem 18.12. Given a network structure with families XU and a complete data set D,
we have
P(D) =

XU

u
(ψX|u)
(ψX|u + D#(u))

x
(ψx|u + D#(xu))
(ψx|u)
.
■
PROOF. Given a complete data set D = d1, . . . , dN, deﬁne Di = d1, . . . , di. Equa-
tions 18.14 and 18.16, with the chain rule of Bayesian networks, give
P(di|Di−1) =

xu∼di
ψx|u + Di−1#(xu)
ψX|u + Di−1#(u) .
Using this equation and the chain rule of probability calculus, we get
P(D) =
N

i=1
P(di|Di−1) =
N

i=1

xu∼di
ψx|u + Di−1#(xu)
ψX|u + Di−1#(u) .
Rearranging the terms in this expression, we get (see Exercise 18.11)
P(D) =

XU

u

x(ψx|u)(ψx|u + 1) . . . (ψx|u + D#(xu) −1)
(ψX|u)(ψX|u + 1) . . . (ψX|u + D#(u) −1)
.
(18.27)
This closed form is commonly expressed using the Gamma function, (.), which is known
to satisfy the following property:
(a + k)
(a)
= a(a + 1) . . . (a + k −1),
where a > 0 and k is a non-negative integer. Given this property, we have
P(D) =

XU

u
(ψX|u)
(ψX|u + D#(u))

x
(ψx|u + D#(xu))
(ψx|u)
.
■

Appendix A
Notation
Variables and Instantiations (see Page 20)
A
variable (upper case)
a
value of variable A (lower case)
A
set of variables (upper case)
instantiation of variables A (lower case)
⊤
trivial instantiation (i.e., instantiation of an empty set of variables)
x1, . . . , xn
event (X1 =x1) ∧. . . ∧(Xn =xn)
A, a
event A=true (assuming A is a binary variable)
¬A, ¯a
event A=false (assuming A is a binary variable)
x −Y
the result of erasing the value of variable Y from instantiation x
x −Y
the result of erasing the values of variables Y from instantiation x
x ∼y
instantiations x and y are compatible (agree on the values of common
variables)
X#
number of instantiations for variables X
|X|
number of values for variable X
Logic
∧
conjunction connective
∨
disjunction connective
¬
negation connective
=⇒
implication connective
⇐⇒
equivalence connective
α, β, γ
events (propositional sentences)
true
a valid sentence or
the value of propositional variable
false
an inconsistent sentence or
the value of propositional variable
ω
world (truth assignment, complete variable instantiation):
maps each variable to a value
ω |= α
world ω satisﬁes sentence α / sentence α is true at world ω
Mods(α)
the models of sentence α / worlds at which sentence α is true
β |= α
sentence β implies sentence α
|= α
sentence α is valid
515

516
APPENDIX A: NOTATION
Probability
Pr(α)
probability of event α
Pr(α|β)
probability of α given β
O(α)
odds of event α
O(α|β)
odds of α given β
Pr(X)
probability distribution over variables X (a factor)
Pr(x)
probability of instantiation x (a number)
Prθ(.)
probability distribution induced by a Bayesian network with
parameters θ
PrD(.)
probability distribution induced by a complete data set D
IPr(X, Z, Y)
X independent of Y given Z in distribution Pr
ENT(X)
entropy of variable X
ENT(X|Y)
conditional entropy of variable X given variable Y
MI(X; Y)
mutual information between variables X and Y
KL(Pr, Pr′)
KL divergence between two distributions
Networks and Parameters
(G, )
Bayesian network with DAG G and CPTs 
XU
network family: a node X and its parents U
X|U
CPT for node X and its parents U
θx|u
network parameter: probability of x given u
θx|U
parameters for value x of variable X given its parents U:
if U has instantiations u1, . . . , un, then θx|U = (θx|u1, . . . , θx|un)
θX|u
parameter set for variable X given parent instantiation u:
if X has values x1, . . . , xn, then θX|u = (θx1|u, . . . , θxn|u)
θml
x|u
maximum likelihood (ML) estimate for parameter θx|u
θma
x|u
MAP estimate for parameter θx|u
θbe
x|u
Bayesian estimate for parameter θx|u
θx|u ∼z
parameter θx|u is compatible with instantiation z
(i.e., instantiations xu and z must be compatible)
Factors
f (X)
factor f over variables X
vars(f )
variables of factor f
f (x)
number assigned by factor f to instantiation x
f [x]
instantiation assigned by extended factor f to x

X f
result of summing out variable X from factor f
maxX f
result of maximizing out variable X from factor f
f1f2
result of multiplying factors f1 and f2
f1/f2
result of dividing factor f1 by f2
f e
result of reducing factor f given evidence e

Appendix B
Concepts from Information Theory
Consider the entropy of variable X,
ENT(X)
def=

x
Pr(x) log2
1
Pr(x) = −

x
Pr(x) log2 Pr(x),
and assume that 0 log2 0 = 0 by convention.
Intuitively, the entropy can be thought of as measuring the uncertainty we have about the
value of variable X. For example, suppose X is a boolean variable where Pr(X = true) = p
and Pr(X = false) = 1 −p. Figure B.1 plots the entropy of X for varying values of p.
Entropy is non-negative, so when p = 0 or 1, the entropy of X is zero and at a minimum:
there is no uncertainty about the value of X. When p = .5, the distribution Pr(X) is
uniform and the entropy is at a maximum: we have no information about the value of X.
We can also deﬁne the conditional entropy of X given another variable Y,
ENT(X|Y)
def=

x,y
Pr(x, y) log2
1
Pr(x|y)
=

y
Pr(y)

x
Pr(x|y) log2
1
Pr(x|y)
=

y
Pr(y)ENT(X|y),
which measures the average uncertainty about the value of X after learning the value
of Y. It is possible to show that the entropy never increases after conditioning,
ENT(X|Y) ≤ENT(X).
That is, on average learning the value of Y reduces our uncertainty about X. However, for
a particular instance y, we may have ENT(X|y) > ENT(X).
We can also deﬁne the mutual information between variables X and Y,
MI(X; Y)
def=

x,y
Pr(x, y) log2
Pr(x, y)
Pr(x)Pr(y),
which measures the amount of information variables X and Y have about each other.
Mutual information is non-negative and equal to zero if and only if X and Y are in-
dependent. On the other hand, it is easy to see that MI(X; X) = ENT(X). We can also
think of mutual information as the degree to which knowledge of one variable reduces the
uncertainty in the other:
MI(X; Y) = ENT(X) −ENT(X|Y)
= ENT(Y) −ENT(Y|X).
517

518
APPENDIX B: CONCEPTS FROM INFORMATION THEORY
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 p
 ENT( X )
Figure B.1: ENT(X) = −p log2 p −(1 −p) log2(1 −p).
0
0.2
0.4
0.6
0.8
1
0
0.5
1
0
1
2
3
4
 p
 q
 KL( Pr, Pr’)
Figure B.2: KL(X, Y) = p log2
p
q + (1 −p) log2
1−p
1−q .
Mutual information then measures the uncertainty reduced or, alternatively, information
gained about one variable due to observing the other.
Extending these notions to sets of variables is straightforward. For example, the joint
entropy of a joint distribution over variables X,
ENT(X) = −

x
Pr(x) log2 Pr(x),
measures the uncertainty we have in the joint state of variables X.
Finally, consider the Kullback-Leibler divergence, which is often abbreviated as the
KL divergence and sometimes referred to as the relative entropy:
KL(Pr, Pr′)
def=

x
Pr(x) log2
Pr(x)
Pr′(x).

APPENDIX B: CONCEPTS FROM INFORMATION THEORY
519
The KL divergence is non-negative and equal to zero if and only if distributions Pr and Pr′
are equivalent. By convention, we assume that 0 log2
0
p = 0. Note that the KL divergence
is not a true distance measure as it does not obey the triangle inequality and is not
symmetric, that is, KL(Pr, Pr′) ̸= KL(Pr′, Pr) in general.
For example, suppose that X is a boolean variable where Pr(X=true) = p and
Pr′(X=true) = q. Figure B.2 plots the KL divergence between Pr(X) and Pr′(X) for
varying values of p and q. When p = q, the distributions are equivalent and the diver-
gence is zero. As p approaches one and q approaches zero (and vice versa), the divergence
goes to inﬁnity.

Appendix C
Fixed Point Iterative Methods
Given a function f , a ﬁxed point for f is a value x⋆such that
x⋆= f (x⋆).
We can identify such values using ﬁxed point iteration. We start with some initial value
upon which we repeatedly apply our function f . That is, we compute new values xt using
the preceding value xt−1:
xt = f (xt−1).
For example, if we start with an initial value x0, we compute in the ﬁrst few iterations:
x1 = f (x0)
x2 = f (x1) = f (f (x0))
x3 = f (x2) = f (f (x1)) = f (f (f (x0))).
For suitable functions f , this iterative process tends to converge to a ﬁxed point x⋆.
Many problems can be formulated as ﬁxed point problems, which we can try to solve
using this iterative procedure. For example, suppose we want to solve the following
equation:
g(x) = 2x2 −x −3
8 = 0.
We can formulate this problem as a ﬁxed point problem. After rearranging, we ﬁnd that
one solution is equivalent to
x =
3
1
2

x + 3
8
	
.
We can then search for a ﬁxed point of the following function,
f (x) =
3
1
2

x + 3
8
	
,
where the value x⋆= f (x⋆) gives us a zero of the function g(x). If we start with initial
value x0 = .1, we ﬁnd that x1 = f (x0) ≈.4873 and x2 = f (x1) ≈.6566.
Figure C.1 illustrates the dynamics of this search for the ﬁrst few iterations. In this
example, the ﬁxed point iterations are converging toward the ﬁxed point of f (x), which
exists at the intersection of f (x) and the line h(x) = x. This value x⋆= .75 yields a zero
for the original function g(x), as desired.
520

APPENDIX C: FIXED POINT ITERATIVE METHODS
521
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 x
 f( x )
Figure C.1: A convergent search for a ﬁxed point of f (x) (solid line). Also plotted is h(x) = x (dotted
line).
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
 x
 f( x )
Figure C.2: A divergent search for a ﬁxed point of f (x) (solid line). Also plotted is h(x) = x (dotted line).
Even for the same problem, different ﬁxed point formulations can converge quickly,
converge slowly, or may not converge at all. For example, we could have searched instead
for a ﬁxed point of the following function:
f (x) = 2x2 −3
8.
This function has two ﬁxed points: x⋆= −.25 and x⋆= .75, both zeros of the original
function g(x). If we started with an initial value of x0 = .8, iterations yield increasingly
larger values, away from any ﬁxed point (see Figure C.2).

522
APPENDIX C: FIXED POINT ITERATIVE METHODS
We can also use ﬁxed point iteration to ﬁnd solutions to systems of equations. For
example, suppose we want to ﬁnd a point where the following two circles intersect:
x2 + y2 = 1
(x −1)2 + y2 = 1
4.
We can formulate this problem as a ﬁxed point problem where we search for a ﬁxed point
of the following function:
f
1 x
y
2	
=
⎡
⎣
4
1 −y2
5
1
4 −(x −1)2
⎤
⎦,
where the vector [x⋆
y⋆] = f ([x⋆
y⋆]) gives us a point of intersection. If we start with an
initial vector [x0
y0] = [.5
.5], we ﬁnd that [x1
y1] ≈[.8660
.4817] and [x2
y2] ≈[.8763
.4845]. Continued iterations
converge to the ﬁxed point [x⋆
y⋆] = [
7
8
√
15
8
].

Appendix D
Constrained Optimization
Consider a constrained optimization problem of the following form:
minimize
f (x)
subject to
gj(x) = 0,
j = 1, . . . , m,
(D.1)
where we search for a vector x = (x1, . . . , xn) that minimizes an objective function
f : Rn →R subject to equality constraints speciﬁed by functions gj : Rn →R. Our
goal here is to review Lagrange’s method for approaching this class of problems, which
formulates the constrained optimization task as an unconstrained one.
Consider ﬁrst an unconstrained optimization problem that asks for points x that
minimize
f (x).
(D.2)
Recall also that a multivariate function f has a local minimum at a point x if f (x) ≤f (y)
for all points y near x. That is, x is a local minimum if there exists a ball with center
x (equivalently, a disk in two dimensions) where f is at least as small at x as it is at
all other points inside the ball. Similarly, a function f has a local maximum at a point
x if f (x) ≥f (y) for all points y near x. A function f has a global minimum at x if
f (x) ≤f (y) for all y in the domain of f . Similarly, f has a global maximum at x if
f (x) ≥f (y) for all y in the domain of f . Minima and maxima are referred to collectively
as extreme points, which may also be local or global.
We consider an extreme point an optimal point if it is a solution to our optimization
problem (D.2). Again, a point x is locally optimal if it is a solution for (D.2) over points
near x and globally optimal if it is a solution for all points in the domain of f . All
extreme points, and hence all optimal points that solve our optimization problem, are also
stationary points. A point x is a stationary point if the partial derivative of f with respect
to all variables xi is zero at the point x. That is, the gradient vector of f is zero at x, or
simply ∇f (x) = ( ∂f
∂x1 (x), . . . , ∂f
∂xn (x)) = 0. Since all optimal points are stationary points,
we can ﬁrst search for stationary points and then determine if they are optimal or not
once we ﬁnd them. Unfortunately, stationary points may be neither minima nor maxima:
they may be saddle points, or points of inﬂection. Second derivative tests can be used to
determine whether a stationary point is a minima, maxima, or saddle point.
Using the method of Lagrange multipliers, we can convert the constrained optimization
problem in (D.1) to the unconstrained problem in (D.2). In particular, when we are given
an objective function f and constraints gj(x) = 0, we search for stationary points of the
Lagrangian L:
L(x, λ) = f (x) +

j
λjgj(x).
523

524
APPENDIX D: CONSTRAINED OPTIMIZATION
That is, we want the point x and the point λ = (λ1, . . . , λm) where ∇L(x, λ) = 0. Here
λ = (λ1, . . . , λm) are newly introduced variables called Lagrange multipliers that help
enforce these constraints.
If we set the gradient to zero, ∇L(x, λ) = 0, we have a system of n + m equations we
now want to solve in terms of each of our n variables xi and each of our m multipliers λj.
When we set to zero the partial derivative of L with respect to variables xi, we have
∂L
∂xi
= ∂f
∂xi
+

j
λj
∂gj
∂xi
= 0.
When we set to zero the partial derivatives with respect to multipliers λj, we have
∂L
∂λj
= gj = 0,
recovering our equality constraints.
To see how stationary points of the Lagrangian yield solutions to our constrained op-
timization problem, consider the following simple example. Suppose we want to identify
the point (x, y) on the unit circle centered at the origin, closest to the point ( 1
2, 1
2). That
is, we want to minimize the (square of the) distance between (x, y) and ( 1
2, 1
2):
f (x, y) =

x −1
2
	2
+

y −1
2
	2
,
subject to the constraint that the point (x, y) lies on the unit circle,
g(x, y) = x2 + y2 −1 = 0.
We ﬁrst construct the Lagrangian,
L(x, y, λ) = f (x, y) + λg(x, y)
=

x −1
2
	2
+

y −1
2
	2
+ λ(x2 + y2 −1),
where we introduce the multiplier λ for our single constraint. Setting to zero the partial
derivatives of L(x, y, λ) with respect to x, y, and λ, we get the following system of
equations:
2x −1 + 2λx = 0
2y −1 + 2λy = 0
x2 + y2 −1 = 0.
Rearranging, we get
x = 1
2
1
1 + λ
y = 1
2
1
1 + λ
x2 + y2 = 1.
Substituting the ﬁrst and second equation into the third and again rearranging, we ﬁnd
that
1
1 + λ = ±
√
2.

APPENDIX D: CONSTRAINED OPTIMIZATION
525
Substituting back into the ﬁrst two equations, we ﬁnd that
(x, y) = ±
√
2
2 ,
√
2
2

are stationary points of our objective function. By evaluating f (x, y) at each point, we
can see that
f
√
2
2 ,
√
2
2

= 3 −2
√
2
2
is a global minimum and
f

−
√
2
2 , −
√
2
2

= 3 + 2
√
2
2
is a global maximum.


Bibliography
S. M. Aji and R. McEliece. The generalized dis-
tributive law and free energy minimization. In
Proceedings of the 39th Allerton Conference
on Communication, Control and Computing,
pages 672–681, 2001.
H. Akaike. A new look at the statistical model
identiﬁcation. IEEE Transactions on Auto-
matic Control, 19:716–723, 1974.
D. Allen and A. Darwiche. New advances in
inference by recursive conditioning. In Pro-
ceedings of the Conference on Uncertainty in
Artiﬁcial Intelligence, pages 2–10. Morgan
Kaufmann, San Francisco, CA, 2003.
D. Allen and A. Darwiche. Optimal Time–
Space Tradeoff in Probabilistic Inference.
In Advances in Bayesian Network, volume
146, pages 39–55. Studies in Fuzziness and
Soft Computing. Springer-Verlag, New York,
2004.
D. Allen and A. Darwiche. RC-Link: genetic
linkage analysis using bayesian networks. In-
ternational Journal of Approximate Reason-
ing, 48(2):499–525, 2008.
D. Allen, A. Darwiche, and J. Park. A greedy al-
gorithm for time-space tradeoff in probabilis-
tic inference. In Proceedings of the Second
European Workshop on Probabilistic Graph-
ical Models, pages 1–8, 2004.
S. Andreassen, F. V. Jensen, S. K. Ander-
sen, B. Falck, U. Kjærulff, M. Woldbye,
A. R. Sorensen, A. Rosenfalck, and F. Jensen.
MUNIN – an expert EMG assistant. In
J. E. Desmedt, editor, Computer-Aided Elec-
tromyography and Expert Systems, chap-
ter 21. Elsevier Science Publishers, Amster-
dam, 1989.
S. Andreassen, M. Suojanen, B. Falck, and
K. G. Olesen. Improving the diagnostic per-
formance of MUNIN by remodelling of the
diseases. In Proceedings of the 8th Confer-
ence on AI in Medicine in Europe, pages 167–
176. Springer-Verlag, Berlin, 2001.
S. Andreassen, M. Woldbye, B. Falck, and S. K.
Andersen. MUNIN – a causal probabilistic
network for interpretation of electromyo-
graphic ﬁndings. In J. McDermott, edi-
tor, Proceedings of the 10th International
Joint Conference on Artiﬁcial Intelligence,
pages 366–372. Morgan Kaufmann, San
Francisco, CA, 1987.
S. Arnborg, D. G. Corneil, and A. Proskurowski.
Complexity of ﬁnding embeddings in a k-
tree. SIAM Journal on Algebraic and Discrete
Methods, 8:277–284, 1987.
S. Arnborg and A. Proskurowski. Characteriza-
tion and recognition of partial 3-trees. SIAM
Journal on Algebraic and Discrete Methods,
7:305–314, 1986.
F. Bacchus, S. Dalmao, and T. Pitassi. Value
elimination: Bayesian inference via back-
tracking search. In Proceedings of the 19th
Annual Conference on Uncertainty in Artiﬁ-
cial Intelligence, pages 20–28. Morgan Kauf-
mann, San Francisco, CA, 2003.
R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D.
Hachtel, E. Macii, A. Pardo, and F. Somenzi.
Algebraic decision diagrams and their appli-
cations. In IEEE /ACM International Confer-
ence on CAD, pages 188–191. IEEE Com-
puter Society Press, Santa Clara, CA, 1993.
T. Bayes. An essay towards solving a prob-
lem in the doctrine of chances. Philosophical
Transactions of the Royal Society, 3:370–418,
1963.
A. Becker, R. Bar-Yehuda, and D. Geiger. Ran-
dom algorithms for the loop cutset problem.
In Proceedings of the 15th Conference on Un-
certainty in Artiﬁcial Intelligence, pages 49–
56. Morgan Kaufmann, San Francisco, CA,
1999.
A. Becker and D. Geiger. Approximation algo-
rithms for the loop cutset problem. In Pro-
ceedings of the Tenth Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 60–68,
1994.
U. Bertele and F. Brioschi. Nonserial Dynamic
Programming. Academic Press, New York,
1972.
527

528
BIBLIOGRAPHY
D. P. Bertsekas and J. N. Tsitsiklis. Introduction
to Probability. Athena Scientiﬁc, Nashua,
NH, 2002.
B. Bidyuk and R. Dechter. Cutset sampling with
likelihood weighting. In Proceedings of the
22nd Annual Conference on Uncertainty in
Artiﬁcial Intelligence. AUAI Press, Arling-
ton, VA, 2006.
H. L. Bodlaender. A partial k-arboretum of
graphs with bounded treewidth. Theoretical
Computer Science, 209:1–45, 1998.
H. L. Bodlaender. Discovering treewidth. In
Proceedings of the 31st Conference on Cur-
rent Trends in Theory and Practice of Com-
puter Science, volume 3381, pages 1–16.
Lecture Notes in Computer Science. Springer,
New York, January 2005.
H. L. Bodlaender. Treewidth: characterizations,
applications, and computations. In F. V.
Fomin, editor, Proceedings of the 32nd Inter-
national Workshop on Graph-Theoretic Con-
cepts in Computer Science, volume 4271,
pages 1–14. Lecture Notes in Computer Sci-
ence. Springer, New York, 2006.
H. L. Bodlaender. Treewidth: structure and al-
gorithms. In G. Prencipe and S. Zaks, editors,
Proceedings of the 14th International Collo-
quium on Structural Information and Com-
munication Complexity, volume 4474, pages
11–25. Lecture Notes in Computer Science.
Springer, New York, 2007.
H. L. Bodlaender, A. Grigoriev, and A. M. C. A.
Koster. Treewidth lower bounds with bram-
bles. In Proceedings of the 13th Annual Eu-
ropean Symposium on Algorithms, volume
3669, pages 391–402. Lecture Notes in Com-
puter Science. Springer-Verlag, New York,
2005.
H. L. Bodlaender, A. M. C. A. Koster, and
T. Wolle. Contraction and treewidth lower
bounds. In Proceedings of the 12th Annual
European Symposium on Algorithms, volume
3221, pages 628–689. Lecture Notes in Com-
puter Science. Springer, New York, January
2004.
H. L. Bodlaender, A. M. C. A. Koster, and
F. van den Eijkhof. Pre-processing rules
for triangulation of probabilistic networks.
Computational Intelligence, 21(3):286–305,
2005.
H. L. Bodlaender, A. M. C. A. Koster, and
T. Wolle. Contraction and treewidth lower
bounds. Journal of Graph Algorithms and Ap-
plications, 10(1):5–49, 2006.
R. R. Bouckaert. Probabilistic network con-
struction using the minimum description
length principle. In Lecture Notes in Com-
puter Science, volume 747, pages 41–48.
Springer, New York, 1993.
C. Boutilier, N. Friedman, M. Goldszmidt, and
D. Koller. Context-speciﬁc independence in
Bayesian networks. In Proceedings of the
12th Conference on Uncertainty in Artiﬁcial
Intelligence, pages 115–123. Morgan Kauf-
mann, San Francisco, CA, 1996.
W. Buntine. Theory reﬁnement on Bayesian net-
works. In Proceedings of the Seventh Confer-
ence on Uncertainty in Artiﬁcial Intelligence,
pages 52–60. Morgan Kaufmann, Los Ange-
les, CA, July 1991.
W. Buntine. A guide to the literature on learn-
ing graphical models. IEEE Transactions on
Knowledge and Data Engineering, 8:195–
210, 1996.
E. Castillo, J. M. Guti´errez, and A. S. Hadi. Goal
oriented symbolic propagation in Bayesian
networks. In Proceedings of the AAAI Na-
tional Conference, pages 1263–1268. AAAI
Press, Menlo Park, CA, 1996.
E. Castillo, J. M. Guti´errez, and A. S. Hadi.
Sensitivity analysis in discrete Bayesian net-
works. IEEE Transactions on Systems, Man,
and Cybernetics, 27:412–423, 1997.
J. Cerquides and R. Lopez de Mantaras. TAN
classiﬁers based on decomposable distri-
butions. Machine Learning, 59(3):323–354,
2005.
H. Chan and A. Darwiche. When do numbers re-
ally matter? Journal of Artiﬁcial Intelligence
Research, 17:265–287, 2002.
H. Chan and A. Darwiche. Sensitivity analysis
in Bayesian networks: from single to multiple
parameters. In Proceedings of the Twentieth
Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 67–75. AUAI Press, Arlington,
VA, 2004.
H. Chan and A. Darwiche. A distance measure
for bounding probabilistic belief change. In-
ternational Journal of Approximate Reason-
ing, 38:149–174, 2005a.
H. Chan and A. Darwiche. On the revision of
probabilistic beliefs using uncertain evidence.
Artiﬁcial Intelligence, 163:67–90, 2005b.
H. Chan and A. Darwiche. On the robustness
of most probable explanations. In Proceed-
ings of the 22nd Conference on Uncertainty
in Artiﬁcial Intelligence, pages 63–71. AUAI
Press, Arlington, VA, 2006.

BIBLIOGRAPHY
529
E. Charniak. Bayesian networks without tears.
AI Magazine, 12(4):50–63, 1991.
M. Chavira, D. Allen, and A. Darwiche. Ex-
ploiting evidence in probabilistic inference.
In Proceedings of the 21st Conference on
Uncertainty in Artiﬁcial Intelligence, pages
112–119. AUAI Press, Arlington, VA, 2005.
M. Chavira and A. Darwiche. Compiling
Bayesian networks with local structure. In
Proceedings of the 19th International Joint
Conference on Artiﬁcial Intelligence, pages
1306–1312. Professional Book Center, Den-
ver, CO, 2005.
M. Chavira and A. Darwiche. Encoding CNFS
to empower component analysis. In Proceed-
ings of the Ninth International Conference
on Theory and Applications of Satisﬁability
Testing, pages 61–74. Springer, New York,
2006.
M. Chavira and A. Darwiche. Compiling
Bayesian networks using variable elimina-
tion. In Proceedings of the 20th International
Joint Conference on Artiﬁcial Intelligence,
pages 2443–2449. AAAI Press, Menlo Park,
CA, 2007.
M. Chavira and A. Darwiche. On probabilistic
inference by weighted model counting. Artiﬁ-
cial Intelligence Journal, 172(6–7):772–799,
2008.
M. Chavira, A. Darwiche, and M. Jaeger. Com-
piling relational Bayesian networks for exact
inference. International Journal of Approxi-
mate Reasoning, 42(1–2):4–20, 2006.
P. Cheeseman and J. Stutz. Bayesian classi-
ﬁcation (AutoClass): theory and results. In
U. Fayyad, G. Piatesky-Shapiro, P. Smyth,
and R. Uthurusamy, editors, Advances in
Knowledge Discovery and Data Mining,
pages 153–180. AAAI Press, Menlo Park,
CA, 1995.
J. Cheng and M. J. Druzdzel. BN-AIS: an adap-
tive importance sampling algorithm for evi-
dential reasoning in large Bayesian networks.
Journal of Artiﬁcial Intelligence Research,
13:155–188, 2000.
S. Chib. Marginal likelihood from the Gibbs out-
put. Journal of the American Statistical Asso-
ciation, 90:1313–1321, 1995.
D. Chickering. Learning Bayesian networks is
NP-complete. In D. Fisher and H. Lenz, ed-
itors, Learning from Data, pages 121–130.
Springer-Verlag, New York, 1996.
D. Chickering, D. Geiger, and D. Heckerman.
Learning Bayesian networks: search methods
and experimental results. In Proceedings of
the Fifth Conference on Artiﬁcial Intelligence
and Statistics, pages 112–128. Society for
Artiﬁcial Intelligence in Statistics, Ft. Laud-
erbale, FL, 1995.
D. Chickering and D. Heckerman. Efﬁcient ap-
proximations for the marginal likelihood of
Bayesian networks with hidden variables.
Machine Learning, 29:181–212, 1997.
D. Chickering, D. Heckerman, and C. Meek. A
Bayesian approach to learning Bayesian net-
works with local structure. In D. Geiger and
P. Shenoy, editors, Proceedings of the Thir-
teenth Conference on Uncertainty in Artiﬁ-
cial Intelligence, pages 80–89. Morgan Kauf-
mann, Providence, RI, 1997.
D. Chickering, D. Heckerman, and C. Meek.
Large-sample learning of Bayesian networks
is NP-hard. Journal of Machine Learning Re-
search, 5:1287–1330, 2004.
A. Choi, M. Chavira, and A. Darwiche. Node
splitting: a scheme for generating upper
bounds in bayesian networks. In Proceed-
ings of the 23rd Conference on Uncertainty in
Artiﬁcial Intelligence, pages 57–66. AUAI
Press, Arlington, VA, 2007.
A. Choi and A. Darwiche. An edge deletion
semantics for belief propagation and its prac-
tical impact on approximation quality. In Pro-
ceedings of the National Conference on Arti-
ﬁcial Intelligence. AAAI Press, Menlo Park,
CA, 2006.
A. Choi and A. Darwiche. Approximating the
partition function by deleting and then cor-
recting for model edges. In Proceedings of
the 24th Conference on Uncertainty in Artiﬁ-
cial Intelligence. AUAI Press, Arlington, VA,
2008.
C. Chow and C. Liu. Approximating dis-
crete probability distributions with depen-
dence trees. IEEE Transactions on Informa-
tion Theory, 14:462–467, 1968.
G. Cooper. Bayesian belief–network inference
using recursive decomposition. Technical Re-
port KSL-90-05. Knowledge Systems Labo-
ratory, Stanford, CA, 94305, 1990a.
G. Cooper. The computational complexity of
probabilistic inference using Bayesian be-
lief networks. Artiﬁcial Intelligence, 42(2–3):
393–405, 1990b.
G. Cooper and E. Herskovits. A Bayesian
method for constructing Bayesian belief net-
works from databases. In Proceedings of
the Seventh Conference on Uncertainty in

530
BIBLIOGRAPHY
Artiﬁcial Intelligence, pages 86–94. Morgan
Kaufmann, Los Angeles, CA, 1991.
G. Cooper and E. Herskovits. A Bayesian
method for the induction of probabilistic net-
works from data. Machine Learning, 9:309–
347, 1992.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
Introduction to Algorithms. MIT press, Cam-
bridge, MA, 1990.
V. M. H. Coup´e and L. C. van der Gaag. Proper-
ties of sensitivity analysis of bayesian belief
networks. Annals of Mathematics and Artiﬁ-
cial Intelligence, 36:323–356, 2002.
T. M. Cover and J. A. Thomas. Elements of In-
formation Theory. Wiley, New York, 1991.
R. Cowell, A. Dawid, S. Lauritzen, and
D. Spiegelhalter. Probabilistic Networks and
Expert Systems. Springer, New York, 1999.
P. Dagum and M. Luby. Approximating proba-
bilistic inference in Bayesian belief networks
is NP-hard. Artiﬁcial Intelligence, 60(1):141–
153, 1993.
A. Darwiche. Conditioning algorithms for ex-
act and approximate inference in causal net-
works. In Proceedings of the 11th Confer-
ence on Uncertainty in Artiﬁcial Intelligence,
pages 99–107. Morgan Kaufmann, San Fran-
cisco, CA, 1995.
A. Darwiche. A differential approach to in-
ference in Bayesian networks. In Proceed-
ings of the 16th Conference on Uncertainty
in Artiﬁcial Intelligence, pages 123–132,
2000.
A. Darwiche. Recursive conditioning. Artiﬁcial
Intelligence, 126(1–2):5–41, 2001.
A. Darwiche. A logical approach to factor-
ing belief networks. In Proceedings of the
Eighth International Conference on Princi-
ples of Knowledge Representation and Rea-
soning, pages 409–420. Morgan Kaufmann,
San Francisco, CA, 2002.
A. Darwiche. A differential approach to infer-
ence in Bayesian networks. Journal of the
ACM, 50(3):280–305, 2003.
A. Darwiche. New advances in compiling CNF
to decomposable negation normal form. In
Proceedings of European Conference on Arti-
ﬁcial Intelligence, pages 328–332. IOS Press,
Amsterdam, 2004.
A. Darwiche and M. Hopkins. Using recursive
decomposition to construct elimination or-
ders, jointrees and dtrees. In Trends in Arti-
ﬁcial Intelligence, Lecture notes in AI, 2143,
pages 180–191. Springer-Verlag, New York,
2001.
A. Darwiche and P. Marquis. A knowledge com-
pilation map. Journal of Artiﬁcial Intelligence
Research, 17:229–264, 2002.
A. Dawid. Conditional independence in statis-
tical theory. Journal of the Royal Statistical
Society, Series B, 41(1):1–31, 1979.
A. Dawid and S. Lauritzen. Hyper Markov
laws in the statistical analysis of decompos-
able graphical models. Annals of Statistics,
21:1272–1317, 1993.
T. Dean and K. Kanazawa. A model for
reasoning about persistence and causation.
Computational Intelligence 5(3):142–150,
1989.
R. Dechter. Bucket elimination: a unifying
framework for probabilistic inference. In Pro-
ceedings of the 12th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 211–
219. Morgan Kaufmann, San Francisco, CA,
1996.
R. Dechter. Bucket elimination: a unifying
framework for reasoning. Artiﬁcial Intelli-
gence, 113:41–85, 1999.
R. Dechter. Constraint Processing. Morgan
Kaufmann, San Francisco, CA, 2003.
R. Dechter and Y. El Fattah. Topological param-
eters for time-space tradeoff. Artiﬁcial Intel-
ligence, 125(1–2):93–118, 2001.
R. Dechter, K. Kask, and R. Mateescu. Iterative
join-graph propagation. In Proceedings of the
Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 128–136. Morgan Kaufmann,
San Francisco, CA, 2002.
R. Dechter and D. Larkin. Bayesian inference in
the presence of determinism. In C. M. Bishop
and B. Frey, editors, Proceedings of the Ninth
International Workshop on Artiﬁcial Intelli-
gence and Statistics, Key West, FL. The So-
ciety for Artiﬁcial Intelligence and Statistics,
NJ, 2003.
R. Dechter and I. Rish. Mini-buckets: a general
scheme for bounded inference. Journal of the
ACM, 50(2):107–153, 2003.
M. H. DeGroot. Probability and Statistics. Ad-
dison Wesley, Boston, MA, 2002.
A. Dempster, N. Laird, and D. Rubin. Maximum
likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39:1–38, 1977.
F. J. Díez. Parameter adjustment in Bayesian
networks: the generalized noisy-or gate. In
Proceedings of the Ninth Conference on Un-
certainty in Artiﬁcial Intelligence, pages 99–
105. Morgan Kaufmann, San Francisco, CA,
1993.

BIBLIOGRAPHY
531
F. J. Díez. Local conditioning in Bayesian
networks. Artiﬁcial Intelligence, 87(1):1–20,
1996.
F. J. Díez and S. F. Gal´an. An efﬁcient factoriza-
tion for the noisy MAX. International Jour-
nal of Intelligent Systems, 18:165–177, 2003.
A. Doucet, N. de Freitas, and N. Gordon. Se-
quential Monte Carlo Methods in Practice.
Springer, New York, 2001.
A. Doucet, N. de Freitas, K. P. Murphy, and
S. J. Russell. Rao-blackwellised particle ﬁl-
tering for dynamic bayesian networks. In Pro-
ceedings of the 16th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 176–
183. Morgan Kaufmann, San Francisco, CA,
2000.
P. A. Dow and R. E. Korf. Best-ﬁrst search for
treewidth. In Proceedings of the 22nd Confer-
ence on Artiﬁcial Intelligence. AAAI Press,
CA, 2007.
P. A. Dow and R. E. Korf. Best-ﬁrst search with
a maximum edge cost function. In Proceed-
ings of the Tenth International Symposium on
Artiﬁcial Intelligence and Mathematics, Ft.
Lauderdale, FL, 2008.
R. Duda and P. Hart, editors. Pattern Classiﬁ-
cation and Scene Analysis. Wiley, New York,
1973.
D. Edwards. Introduction to Graphical Mod-
elling, Second edition. Springer, New York,
2000.
G. Elidan, I. McGraw, and D. Koller. Residual
belief propagation: Informed scheduling for
asynchronous message passing. In Proceed-
ings of the Conference on Uncertainty in Ar-
tiﬁcial Intelligence. AUAI Press, Arlington,
VA, 2006.
M. Fishelson and D. Geiger. Exact genetic
linkage computations for general pedigrees.
Bioinformatics, 18(1):189–198, 2002.
M. Fishelson and D. Geiger. Optimizing exact
genetic linkage computations. In Proceedings
of the International Conference on Research
in Computational Molecular Biology, pages
114–121. ACM Press, New York, 2003.
B. Frey, editor. Graphical Models for Machine
Learning and Digital Communication. MIT
Press, Cambridge, MA, 1998.
B. Frey and D. MacKay. A revolution: belief
propagation in graphs with cycles. In Pro-
ceedings of the Conference on Neural Infor-
mation Processing Systems, pages 479–485.
MIT Press, Cambridge, MA, 1997.
N. Friedman. The Bayesian structural EM al-
gorithm. In Proceedings of the Fourteenth
Conference on Uncertainty in Artiﬁcial In-
telligence Learning, pages 129–138. Morgan
Kaufmann, San Mateo, CA, 1998.
N. Friedman, D. Geiger, and M. Goldszmidt.
Bayesian network classiﬁers. Machine Learn-
ing, 29:131–163, 1997.
N. Friedman and M. Goldszmidt. Learning
Bayesian networks with local structure. In
Proceedings of the Twelfth Conference on
Uncertainty in Artiﬁcial Intelligence, pages
252–262. Morgan Kaufmann, Portland, OR,
1996.
R. Fung and K. Chang. Weighing and inte-
grating evidence for stochastic simulation in
Bayesian networks. In Proceedings of the
Fifth Workshop on Uncertainty in Artiﬁcial
Intelligence, Windsor, ON, pages 112–117.
1989. Also in M. Henrion, R. Shachter, L.
Kanal, and J. Lemmer, editors, Uncertainty in
Artiﬁcial Intelligence, volume 5, pages 209–
219. North-Holland, New York, 1990.
R. Fung and B. del Favero. Backward simulation
in Bayesian networks. In Proceedings of the
Tenth Annual Conference on Uncertainty in
Artiﬁcial Intelligence, pages 227–234. Mor-
gan Kaufmann, San Mateo, CA, 1994.
P. G¨ardenfors. Knowledge in Flux: Modeling
the Dynamics of Epistemic States. MIT Press,
Cambridge, MA, 1988.
F. Gavril. The intersection graphs of subtrees in
trees are exactly the chordal graphs. Journal
of Combinatorial Theory Series B, 16:47–56,
1974.
D. Geiger and D. Heckerman. A characteri-
zation of the Dirichlet distribution with ap-
plication to learning Bayesian networks. In
Proceedings of the Eleventh Conference on
Uncertainty in Artiﬁcial Intelligence, Mon-
treal, Quebec, pages 196–207. Morgan Kauf-
mann, CA, 1995. See also Technical Report
TR-95-16. Microsoft Research, Redmond,
WA, February 1995.
D. Geiger and J. Pearl. Logical and algorith-
mic properties of conditional independence.
Technical Report 870056 (R-97). Com-
puter Science Department, UCLA, February
1988a.
D. Geiger and J. Pearl. On the logic of causal
models. In Proceedings of the 4th Work-
shop on Uncertainty in Artiﬁcial Intelligence,
pages 136–147, St. Paul, MN, 1988b. Also in
R. Shachter, T. Levitt, L. Kanal, and J. Lem-
mer, editors, Uncertainty in Artiﬁcial Intel-
ligence, volume 4, pages 3–14. Amsterdam,
1990.

532
BIBLIOGRAPHY
M. R. Genesereth and N. J. Nilsson. Logical
Foundations of Artiﬁcial Intelligence. Mor-
gan Kaufmann, San Mateo, CA, 1987.
W. Gibbs. Elementary Principles of Statistical
Mechanics. Yale University Press, 1902.
W. Gilks, S. Richardson, and D. Spiegelhal-
ter. Markov Chain Monte Carlo in Practice.
Chapman and Hall, London, England, 1996.
C. Glymour and G. Cooper, editors. Computa-
tion, Causation, and Discovery. MIT Press,
Cambridge, MA, 1999.
V. Gogate and R. Dechter. A complete anytime
algorithm for treewidth. In Proceedings of the
20th Annual Conference on Uncertainty in
Artiﬁcial Intelligence, pages 201–220. AUAI
Press, Arlington, VA, 2004.
V. Gogate and R. Dechter. Approximate infer-
ence algorithms for hybrid bayesian networks
with discrete constraints. In Proceedings of
the Conference on Uncertainty in Artiﬁcial
Intelligence, pages 209–216. AUAI Press,
Arlington, VA, 2005.
M. Goldszmidt and J. Pearl. Qualitative proba-
bilities for default reasoning, belief revision,
and causal modeling. Artiﬁcial Intelligence,
84(1–2):57–112, July 1996.
M. C. Golumbic. Algorithmic Graph Theory and
Perfect Graphs. Academic Press, New York,
1980.
I. J. Good. Probability and the Weighing of Ev-
idence. Charles Grifﬁn, London, 1950.
I. J. Good. Good Thinking: The Foundations of
Probability and Its Applications. University
of Minnesota Press, Minneapolis, MN, 1983.
R. Greiner, X. Su, B. Shen, and W. Zhou. Struc-
tural extension to logistic regression: discrim-
inative parameter learning of belief net clas-
siﬁers. Machine Learning, 59(3):297–322,
2005.
D. Grossman and P. Domingos. Learning
Bayesian network classiﬁers by maximizing
conditional likelihood. In Proceedings of the
21st International Conference on Machine
Learning, pages 361–368. ACM Press, New
York, 2004.
Y. Guo and R. Greiner. Discriminative model
selection for belief net structures. In Twenti-
eth National Conference on Artiﬁcial Intelli-
gence, Pittsburgh, PA, pages 770–776. AAI
Press, Menlo Park, CA, 2005.
D. Heckerman. A tractable inference algorithm
for diagnosing multiple diseases. In Proceed-
ings of the Fifth Conference on Uncertainty
in Artiﬁcial Intelligence, pages 174–181. El-
sevier, New York, 1989.
D. Heckerman. A bayesian approach to learn-
ing causal networks. In Proceedings of the
Eleventh Conference on Uncertainty in Arti-
ﬁcial Intelligence, Montreal, Quebec, pages
285–295. Morgan Kaufmann, San Francisco,
CA, 1995.
D. Heckerman. A tutorial on learning with
Bayesian networks. In Learning in graphical
models, pages 301–354. Kluwer, The Nether-
lands, 1998.
D. Heckerman, D. Geiger, and D. Chickering.
Learning Bayesian networks: the combina-
tion of knowledge and statistical data. Ma-
chine Learning, 20:197–243, 1995.
M.
Henrion.
Propagating
uncertainty
in
Bayesian networks by probalistic logic sam-
pling. In Uncertainty in Artiﬁcial Intelligence
2, pages 149–163. Elsevier Science Publish-
ing, New York, 1988.
M. Henrion. Some practical issues in construct-
ing belief networks. In L. N. Kanal, T. S.
Levitt, and J. F. Lemmer, editors, Uncertainty
in Artiﬁcial Intelligence volume 3, pages
161–173. Elsevier Science Publishers B.V.,
North Holland, 1989.
L. D. Hernandez, S. Moral, and A. Salmeron. A
Monte Carlo algorithm for probabilistic prop-
agation in belief networks based on impor-
tance sampling and stratiﬁed simulation tech-
niques. International Journal of Approximate
Reasoning, 18:53–91, 1998.
J. Hoey, R. St-Aubin, A. Hu, and G. Boutilier.
SPUDD: stochastic planning using decision
diagrams. In Proceedings of the 15th Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 279–288. Morgan Kaufmann,
San Francisco, CA, 1999.
M. Hopkins and A. Darwiche. A practical relax-
ation of constant-factor treewidth approxima-
tion algorithms. In Proceedings of the First
European Workshop on Probabilistic Graph-
ical Models, pages 71–80, 2002.
E. J. Horvitz, H. J. Suermondt, and G. Cooper.
Bounded conditioning: ﬂexible inference for
decisions under scarce resources. In Proceed-
ings of the Conference on Uncertainty in Ar-
tiﬁcial Intelligence, Windsor, ON, pages 182–
193. Association for Uncertainty in Artiﬁ-
cial Intelligence, Mountain View, CA, August
1989.
R. A. Howard. From inﬂuence to relevance to
knowledge. In R. M. Oliver and J. Q. Smith,
editors, Inﬂuence Diagrams, Belief Nets, and
Decision Analysis, pages 3–23. Wiley, New
York, 1990.

BIBLIOGRAPHY
533
R. A. Howard and J. E. Matheson. Inﬂuence
diagrams. In Principles and Applications of
Decision Analysis, volume 2, pages 719–762.
Strategic Decision Group, Menlo Park, CA,
1984.
J. Hrastad. Tensor rank is NP-complete. Journal
of Algorithms, 11:644–654, 1990.
C. Huang and A. Darwiche. Inference in be-
lief networks: a procedural guide. Interna-
tional Journal of Approximate Reasoning,
15(3):225–263, 1996.
J. Huang, M. Chavira, and A. Darwiche. Solving
MAP exactly by searching on compiled arith-
metic circuits. In Proceedings of the 21st Na-
tional Conference on Artiﬁcial Intelligence,
pages 143–148. AAAI Press, Menlo Park,
CA, 2006.
F. Hutter, H. H. Hoos, and T. St¨utzle. Efﬁ-
cient stochastic local search for MPE solv-
ing. In Proceedings of the International Joint
Conference on Artiﬁcial Intelligence, pages
169–174. Professional Book Center, Denver,
CO, 2005.
T. Jaakkola. Tutorial on variational approxi-
mation methods. In D. Saad and M. Op-
per, editors, Advanced Mean Field Meth-
ods:
Theory
and
Practice,
chapter
10,
pages 129–159. MIT Press, Cambridge, MA,
2001.
E. T. Jaynes. Probability Theory: The Logic of
Science. Cambridge University Press, Cam-
bridge, England, 2003.
R. Jeffrey. The Logic of Decision. McGraw-Hill,
New York, 1965.
F. V. Jensen. An Introduction to Bayesian Net-
works. Springer-Verlag, New York, 1996.
F. V. Jensen. Gradient descent training of
Bayesian networks. In Proceedings of the
Fifth European Conference on Symbolic and
Quantitative Approaches to Reasoning with
Uncertainty, pages 5–9. Springer, New York,
1999.
F. V. Jensen. Bayesian Networks and Decision
Graphs. Springer-Verlag, New York, 2001.
F. Jensen and S. K. Andersen. Approximations
in Bayesian belief universes for knowledge
based systems. In Proceedings of the Sixth
Conference on Uncertainty in Artiﬁcial In-
telligence, Cambridge, MA, pages 162–169.
Elsevier, New York, July 1990.
F. V. Jensen and F. Jensen. Optimal Junction
Trees. In Proceedings of the Tenth Confer-
ence on Uncertainty in Artiﬁcial Intelligence,
Seattle, WA, pages 360–366. Morgan Kauf-
mann, San Francisco, CA, 1994.
F. V. Jensen, S. Lauritzen, and K. G. Ole-
sen. Bayesian updating in recursive graphical
models by local computation. Computational
Statistics Quarterly, 4:269–282, 1990.
F. V. Jensen and T. D. Nielsen. Bayesian Net-
works and Decision Graphs. Springer, New
York, 2007.
Y. Jing, V. Pavlovic, and J. Rehg. Efﬁcient
discriminative learning of Bayesian net-
work classiﬁers via boosted augmented naive
Bayes. In Proceedings of the International
Conference on Machine Learning. ACM
Press, New York, 2005.
M. Jordan, editor. Learning in Graphical Mod-
els. Kluwer, The Netherlands, 1998.
M. Jordan, Z. Ghahramani, T. Jaakkola, and
L. K. Saul. An introduction to variational
methods for graphical models. Machine
Learning, 37(2):183–233, 1999.
K. Kask and R. Dechter. Stochastic local search
for bayesian networks. In Workshop on AI
and Statistics, pages 113–122. Morgan Kauf-
mann, San Francisco, CA, 1999.
K. Kask and R. Dechter. A general scheme
for automatic generation of search heuris-
tics from speciﬁcation dependencies. Artiﬁ-
cial Intelligence, 129:91–131, 2001.
J. H. Kim and J. Pearl. A computational model
for combined causal and diagnostic reason-
ing in inference systems. In Proceedings of
the International Joint Conference on Artiﬁ-
cial Intelligence, pages 190–193. Karlsruhe,
Germany, 1983.
U. Kjaerulff. Triangulation of graphs – algo-
rithms giving small total state space. Tech-
nical Report R-90-09. Department of Mathe-
matics and Computer Science, University of
Aalborg, Denmark, 1990.
U. Kjaerulff and L. C. van der Gaag. Mak-
ing sensitivity analysis computationally ef-
ﬁcient. In Proceedings of the 16th Confer-
ence on Uncertainty in Artiﬁcial Intelligence.
Morgan Kaufmann, San Francisco, CA,
2000.
V. Kolmogorov and M. J. Wainwright. On the
optimality of tree-reweighted max-product
message passing. In Proceedings of the Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 316–323. AUAI Press, Arling-
ton, VA, 2005.
A. M. C. A. Koster, T. Wolle, and H. L. Bodlaen-
der. Degree-based treewidth lower bounds. In
Proceedings of the 4th International Work-
shop on Experimental and Efﬁcient Algo-
rithms, volume 3503, pages 101–112. Lecture

534
BIBLIOGRAPHY
Notes in Computer Science. Springer, New
York, 2005.
W. Lam and F. Bacchus. Learning Bayesian be-
lief networks: an approach based on the MDL
principle. User Modeling and User-Adapted
Interaction, 10:269–293, 1994.
K. B. Laskey. Sensitivity analysis for probabil-
ity assessments in Bayesian networks. IEEE
Transactions on Systems, Man, and Cyber-
netics, 25:901–909, 1995.
S. Lauritzen. The EM algorithm for graphical as-
sociation models with missing data. Compu-
tational Statistics and Data Analysis, 19:191–
201, 1995.
S. Lauritzen. Graphical Models. Oxford Science
Publications, Oxford, England, 1996.
S. Lauritzen and D. J. Spiegelhalter. Local
computations with probabilities on graphical
structures and their application to expert sys-
tems. Journal of Royal Statistics Society, Se-
ries B, 50(2):157–224, 1988.
V.
Lepar
and
P.
Shenoy.
A
comparison
of
Lauritzen-Spiegelhalter,
HUGIN,
and
Shenoy-Shafer architectures for computing
marginals of probability distributions. In Pro-
ceedings of the Fourteenth Annual Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 328–337. Morgan Kaufmann,
San Francisco, CA, 1998.
D. R. Lick and A. T. White. k-degenerate
graphs. SIAM Journal of Discrete Mathemat-
ics, 22:1082–1096, 1970.
Y. Lin and M. Druzdzel. Computational ad-
vantages of relevance reasoning in Bayesian
belief networks. In Proceedings of the 13th
Annual Conference on Uncertainty in Arti-
ﬁcial Intelligence, pages 342–350. Morgan
Kaufmann, San Francisco, CA, 1997.
R. J. A. Little and D. B. Rubin. Statistical Anal-
ysis with Missing Data. Wiley, NJ, 2002.
R. Marinescu and R. Dechter. AND/OR branch-
and-bound for graphical models. In Proceed-
ings of the International Joint Conference on
Artiﬁcial Intelligence, pages 224–229. Pro-
fessional Book Center, Denver, CO, 2005.
R. Marinescu and R. Dechter. Memory inten-
sive branch-and-bound search for graphical
models. In National Conference on Artiﬁcial
Intelligence. AAAI Press, Menlo Park, CA,
2006.
R. Marinescu, K. Kask, and R. Dechter. System-
atic vs. non-systematic algorithms for solving
the MPE task. In Proceedings of the Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 394–402. Morgan Kaufmann,
San Francisco, CA, 2003.
R. Mateescu and R. Dechter. AND/OR cutset
conditioning. In International Joint Confer-
ence on Artiﬁcial Intelligence, pages 230–
235. Professional Book Center, Denver, CO,
2005.
R. Mateescu and R. Dechter. Compiling con-
straint networks into AND/OR multi-valued
decision diagrams. In Constraint Porgram-
ming. Springer, New York, 2006.
J. McCarthy. Programs with common sense. In
Proceedings of the Teddington Conference
on the Mechanization of Thought Processes,
1959.
http://www-formal.stanford.edu/jmc/
mcc59.html.
J. McCarthy. Epistemological problems of arti-
ﬁcial intelligence. In Proceedings of the Fifth
International Joint Conference on Artiﬁcial
Intelligence, 1977. Invited talk: http://www-
formal.stanford.edu/jmc/epistemological.pdf.
J. McCarthy. Circumscription – a form of non-
monotonic reasoning. Artiﬁcial Intelligence,
13:27–39, 1980.
D. McDermott and J. Doyle. Nonmonotonic
logic I. Artiﬁcial Intelligence, 13:41–72,
1980.
R. McEliece, D. MacKay, and J. Cheng. Trubo
decoding as an instance of Pearl’s belief prop-
agation algorithm. IEEE Journal on Selected
Areas in Communication, 16:140–152, 1998.
R. McEliece, D. MacKay, and J.-F. Cheng.
Turbo decoding as an instance of Pearl’s
belief propagation algorithm. IEEE Jour-
nal on Selected Areas in Communications,
16(2):140–152, 1998.
C. Meek and D. Heckerman. Structure and pa-
rameter learning for causal independence and
causal interaction models. In Proceedings of
the Thirteenth Conference on Uncertainty
in Artiﬁcial Intelligence, Morgan Kaufmann,
Providence, RI, August 1997.
M. Meilua and M. Jordan. Estimating depen-
dency structure as a hidden variable. In Ad-
vances in Neural Information Processing Sys-
tems 10, volume 10. Morgan Kaufmann, San
Mateo, CA, 1998.
R. A. Miller, F. E. Fasarie, and J. D. Myers.
Quick medical reference (QMR) for diagnos-
tic assistance. Medical Computing, 3:34–48,
1986.
T. P. Minka. A Family of Algorithms for Approx-
imate Bayesian Inference. PhD thesis. MIT
Cambridge, MA, 2001.

BIBLIOGRAPHY
535
S. Moral and A. Salmeron. Dynamic impor-
tance sampling computation in Bayesian net-
works. In Proceedings of Seventh European
Conference on Symbolic and Quantitative
Approaches to Reasoning with Uncertainty,
pages 137–148. Springer, New York, 2003.
R. Neal. Probabilistic inference using Markov
chain Monte Carlo methods. Technical Re-
port CRG-TR-93-1. Department of Computer
Science, University of Toronto, September
1993.
R. Neal. Improving asymptotic variance of
MCMC estimators: non-reversible chains are
better. Technical Report 0406. Department of
Statistics, University of Toronto, 2004.
R. Neapolitan. Learning Bayesian Networks.
Prentice Hall, NJ, 2004.
A. Y. Ng and M. I. Jordan. On discriminative
versus generative classiﬁers: a comparison of
logistic regression and naive Bayes. In Ad-
vances in Neural Information Processing Sys-
tems 14, pages 841–848. MIT Press, Cam-
bridge, MA, 2001.
T. Nielsen, P. Wuillemin, F. Jensen, and
U. Kjaerulff. Using ROBDDs for inference
in Bayesian networks with troubleshooting
as an example. In Proceedings of the 16th
Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 426–435. Morgan Kaufmann,
San Francisco, CA, 2000.
L. Ortiz and L. Kaelbling. Adaptive impor-
tance sampling for estimation in structured
domains. In Proceedings of the 16th Annual
Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 446–454. Morgan Kaufmann,
San Francisco, CA, 2000.
J. Park. Using weighted MAXSAT engines to
solve MPE. In Proceedings of the Eighteenth
National Conference on Artiﬁcial Intelli-
gence, pages 682–687. AAAI Press, Menlo
Park, CA, 2002.
J. Park and A. Darwiche. Approximating MAP
using stochastic local search. In Proceedings
of the 17th Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 403–410. Morgan
Kaufmann, San Francisco, CA, 2001.
J. Park and A. Darwiche. Solving MAP exactly
using systematic search. In Proceedings of the
19th Conference on Uncertainty in Artiﬁcial
Intelligence, pages 459–468. Morgan Kauf-
mann, San Francisco, CA, 2003a.
J. Park and A. Darwiche. A differential seman-
tics for jointree algorithms. In Advances in
Neural Information Processing Systems 15,
volume 1, pages 299–307. MIT Press, Cam-
bridge, MA, 2003b.
J. Park and A. Darwiche. Morphing the HUGIN
and Shenoy-Shafer architectures. In Trends
in Artiﬁcial Intelligence, Lecture Notes in AI,
2711, pages 149–160. Springer-Verlag, New
York, 2003c.
J. Park and A. Darwiche. Complexity results
and approximation strategies for MAP ex-
planations. Journal of Artiﬁcial Intelligence
Research, 21:101–133, 2004a.
J. Park and A. Darwiche. A differential seman-
tics for jointree algorithms. Artiﬁcial Intelli-
gence, 156:197–216, 2004b.
R. C. Parker and R. A. Miller. Using causal
knowledge to create simulated patient cases:
the CPCS project as an extension of Internist-
1. In Proceedings of the Eleventh Annual
Symposium on Computer Applications in
Medical Care, pages 473–480. IEEE Com-
puter Society Press, 1987.
M. J. Pazzani and E. J. Keogh. Learning
augmented Bayesian classiﬁers: a compari-
son of distribution-based and classiﬁcation-
based approaches. International Journal on
Artiﬁcial Intelligence Tools, 11:587–601,
2002.
J. Pearl. Reverend Bayes on inference engines: a
distributed hierarchicalapproach. InProceed-
ings of the National Conference on Artiﬁcial
Intelligence, Pittsburgh, PA, pages 133–136.
AAAI Press, Menlo Park, CA, 1982.
J. Pearl. Bayesian networks: a model of self-
activated memory for evidential reasoning.
In Proceedings, Cognitive Science Society,
Irvine, CA, pages 329–334. Lawrence Erl-
baum, Philadelphia, PA, 1985.
J. Pearl. A constraint-propagation approach to
probabilistic reasoning. In L. N. Kanal and J.
F. Lemmer, editors, Uncertainty in Artiﬁcial
Intelligence, pages 357–369. North-Holland,
Amsterdam, 1986a.
J. Pearl. Fusion, propagation, and structuring
in belief networks. Artiﬁcial Intelligence,
29:241–288, 1986b.
J. Pearl. Evidential reasoning using stochastic
simulation of causal models. Artiﬁcial Intel-
ligence, 32:245–257, 1987.
J. Pearl. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA, 1988.
J. Pearl. Causality: Models, Reasoning, and In-
ference. Cambridge University Press, New
York, 2000.

536
BIBLIOGRAPHY
J. Pearl and A. Paz. Graphoids: graph-based
logic for reasoning about relevance relations.
Technical Report 850038 (R-53). Computer
Science Department, UCLA, 1986.
J. Pearl and A. Paz. Graphoids: a graph-based
logic for reasoning about relevance relations.
In B. Duboulay, D. Hogg and L. Steels, ed-
itors, Advances in Artiﬁcial Intelligence-II,
pages 357–363. North-Holland, Amsterdam,
1987.
A. Peot and R. Shachter. Fusion and propaga-
tion with multiple observations in belief net-
works. Artiﬁcial Intelligence, 48(3):299–318,
1991.
A. G. Percus, G. Istrate, and C. Moore, editors,
Introduction: where statistical physics meets
computation. In Computational Complexity
and Statistical Physics, chapter 1, pages 3–24.
Oxford University Press, Oxford, England,
2006.
D. Poole and N. L. Zhang. Exploiting contex-
tual independence in probabilistic inference.
Journal of Artiﬁcial Intelligence, 18:263–
313, 2003.
M. Pradhan, M. Henrion, G. Provan, B. Del
Favero, and K. Huang. The sensitivity of be-
lief networks to imprecise probabilities: an
experimental investigation. Artiﬁcial Intelli-
gence, 85:363–397, 1996.
M. Pradhan, G. Provan, B. Middleton, and M.
Henrion. Knowledge engineering for large
belief networks. In Uncertainty in Artiﬁcial
Intelligence: Proceedings of the Tenth Con-
ference, pages 484–490. Morgan Kaufmann,
San Francisco, CA, 1994.
L. R. Rabiner. A tutorial on hidden Markov mod-
els and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257–
286, 1989.
R. Reiter. A logic for default reasoning. Artiﬁ-
cial Intelligence, 13:81–132, 1980.
J. Rissanen. Modeling by shortest data descrip-
tion. Automatica, 14(1):465–471, 1978.
C. P. Robert and G. Casella. Monte Carlo Sta-
tistical Methods. Springer, New York, 2004.
N. Robertson and P. D. Seymour. Graph minors.
II. Algorithmic aspects of tree-width. Journal
of Algorithms, 7:309–322, 1986.
N. Robertson and P. D. Seymour. Graph mi-
nors. X. Obstructions to tree-decomposition.
Journal of Combinatorial Theory Series B,
52:153–190, 1991.
N. Robertson and P. D. Seymour. Graph mi-
nors. XIII. The disjoint paths problem. Jour-
nal Combinatorial Theory Series B, 63:65–
110, 1995.
T. Roos, H. Wettig, P. Gr¨unwald, P. Myllym¨aki,
and H. Tirri. On discriminative Bayesian net-
work classiﬁers and logistic regression. Ma-
chine Learning, 59(3), 2005.
D. J. Rose. Triangulated graphs and the elimina-
tion process. Journal of Mathematical Anal-
ysis and Applications, 32:597–609, 1970.
D. Roth. On the hardness of approximate reason-
ing. Artiﬁcial Intelligence, 82(1–2):273–302,
April 1996.
D.
Rubin.
Inference
and
missing
data.
Biometrika, 3:581–592, 1976.
R. Y. Rubinstein. Simulation and the Monte
Carlo Method. Wiley, New York, 1981.
D. E. Rumelhart. Toward an interactive model
of reading. Technical Report CHIP-56. Uni-
versity of California, La Jolla, CA, 1976.
S. Russell, J. Binder, D. Koller, and K.
Kanazawa. Local learning in probabilistic
networks with hidden variables. In Proceed-
ings of the Fourteenth International Joint
Conference on Artiﬁcial Intelligence, Mon-
treal, Quebec, pages 1146–1152. Morgan
Kaufmann, San Mateo, CA, 1995.
S. Russell and P. Norvig. Artiﬁcial Intelligence:
A Modern Approach. Prentice Hall, NJ,
2003.
A. Salmeron, A. Cano, and S. Moral. Importance
sampling in Bayesian networks using prob-
ability trees. Computational Statistics and
Data Analysis, 34:387–413, 2000.
T. Sang, P. Beame, and H. Kautz. Solving
Bayesian networks by weighted model count-
ing. In Proceedings of the Twentieth National
Conference on Artiﬁcial Intelligence, vol-
ume 1, pages 475–482. AAAI Press, Menlo
Park, CA, 2005.
S. Sanner and D. A. McAllester. Afﬁne alge-
braic decision diagrams (AADDS) and their
application to structured probabilistic infer-
ence. In Proceedings of the International
Joint Conference on Artiﬁcial Intelligence,
pages 1384–1390. Professional Book Center,
Denver, CO, 2005.
P. Savicky and J. Vomlel. Tensor rank-one de-
composition of probability tables. In Pro-
ceedings of the Eleventh Conference on Infor-
mation Processing and Management of Un-
certainty in Knowledge-based Systems, pages
2292–2299. Springer, New York, 2006.
G. Schwarz. Estimating the dimension of a
model. Annals of Statistics, 6:461–464, 1978.

BIBLIOGRAPHY
537
R. Shachter. Evaluating inﬂuence diagrams. Op-
erations Research, 34(6):871–882, 1986.
R. Shachter. Evidence absorption and propaga-
tion through evidence reversals. Uncertainty
in Artiﬁcial Intelligence, 5:173–189, 1990.
R. Shachter, B. D. D’Ambrosio, and B. del
Favero. Symbolic probabilistic inference in
belief networks. In Proceedings of the Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 126–131. Elsevier, New York,
1990.
R. Shachter, S. K. Andersen, and P. Szolovits.
Global conditioning for probabilistic infer-
ence in belief networks. In Proceedings of the
Conference on Uncertainty in Artiﬁcial Intel-
ligence, Seattle, WA, pages 514–522, Morgan
Kaufmann, San Francisco, CA, 1994.
R. Shachter and M. Peot. Simulation approaches
to general probabilistic inference on belief
networks. In M. Henrion, R. Shachter, L. N.
Kanal, and J. F. Lemmer, editors, Uncer-
tainty in Artiﬁcial Intelligence 5, pages 221–
231. Elsevier Science Publishing, New York,
1989.
G. Shafer. Propagating belief functions in qual-
itative markov trees. International Journal
of Approximate Reasoning, 4(1):349–400,
1987.
P. Shenoy. Binary join trees. In Proceedings of
the 12th Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 492–49. Morgan
Kaufmann, San Francisco, CA, 1996.
P. Shenoy and G. Shafer. Axioms for probability
and belief–function propagation. Uncertainty
in Artiﬁcial Intelligence, 4, pages 169–198.
Elsevier, New York, 1990.
Y. Shibata. On the tree representation of chordal
graphs. Journal of Graph Theory, 12(3):421–
428, 1988.
S. E. Shimony. Finding MAPs for belief net-
works is NP-hard. Artiﬁcial Intelligence,
68:399–410, 1994.
M.
Shwe,
B.
Middleton,
D.
Heckerman,
M. Henrion, E. Horvitz, H. Lehmann, and
G. Cooper. Probabilistic diagnosis using a
reformulation of the INTERNIST-1/QMR
knowledge base I. The probabilistic model
and inference algorithms. Methods of Infor-
mation in Medicine, 30:241–255, 1991.
M. Singh. Learning Bayesian networks from in-
complete data. In Proceedings of the Four-
teenth National Conference on Artiﬁcial In-
telligence, pages 534–539, Providence, RI.
AAAI Press, Menlo Park, CA, 1997.
D. Spiegelhalter, A. Dawid, S. Lauritzen, and
R. Cowell. Bayesian analysis in expert sys-
tems. Statistical Science, 8:219–282, 1993.
D. Spiegelhalter and S. Lauritzen. Sequen-
tial updating of conditional probabilities
on directed graphical structures. Networks,
20:579–605, 1990.
P. Spirtes, C. Glymour, and R. Scheines. Causa-
tion, Prediction, and Search, second edition.
MIT Press, Cambridge, MA, 2001.
W. Spohn. Stochastic independence, causal in-
dependence, and shieldability. Journal of
Philosophical Logic, 9:73–99, 1980.
S. Srinivas. A generalization of the noisy-or
model. In Proceedings of the Conference on
Uncertainty in Artiﬁcial Intelligence, pages
208–218. Morgan Kaufmann, San Francisco,
CA, 1993.
M. Studeny. Conditional independence rela-
tions have no ﬁnite complete characteriza-
tion. In Proceedings of the 11th Prague Con-
ference on Information Theory, Statistical
Decision Foundation and Random Processes,
pages 27–31. Springer, New York, 1990.
H. J. Suermondt. Explanation in Bayesian Belief
Networks. PhD thesis. Stanford, CA, 1992.
H. J. Suermondt, G. Cooper, and D. E. Heck-
erman. A combination of cutset condition-
ing with clique–tree propagation in the path–
ﬁnder system. In Proceedings of the Sixth
Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 245–253. Morgan Kaufmann,
San Francisco, CA, 1991.
X. Sun, M. J. Druzdzel, and C. Yuan. Dynamic
weighting A* search-based MAP algorithm
for Bayesian networks. In Proceedings of
the Twentieth International Joint Conference
on Artiﬁcial Intelligence, pages 2385–2390.
AAAI Press, Menlo Park, CA, 2007.
J. Suzuki. A construction of Bayesian networks
from databases based on an MDL principle.
In D. Heckerman and A. Mamdani, editors,
Proceedings of the Ninth Conference on Un-
certainty in Artiﬁcial Intelligence, pages 266–
273, Washington, DC. Morgan Kaufmann,
CA, 1993.
G. Szekeres and H. S. Wilf. An inequality for
the chromatic number of a graph. Journal of
Combinatorial Theory, 4:1–3, 1968.
M. F. Tappen and W. Freeman. Comparison of
graph cuts with belief propagation for stereo,
using identical MRF parameters. In Proceed-
ings of the International Conference on Com-
puter Vision, pages 900–907, 2003.

538
BIBLIOGRAPHY
B. Thiesson. Accelerated quantiﬁcation of
Bayesian networks with incomplete data. In
Proceedings of the First International Con-
ference on Knowledge Discovery and Data
Mining, pages 306–311, Montreal, Quebec.
Morgan Kaufmann, CA, August 1995.
B. Thiesson. Score and information for recursive
exponential models with incomplete data. In
Proceedings of the Thirteenth Conference on
Uncertainty in Artiﬁcial Intelligence, Provi-
dence, RI. Morgan Kaufmann, CA, 1997.
B. Thiesson, C. Meek, D. Chickering, and
D. Heckerman. Computationally efﬁcient
methods
for
selecting
among
mixtures
of graphical models, with discussion. In
Bayesian Statistics 6: Proceedings of the
Sixth Valencia International Meeting, pages
631–656. Clarendon Press, Oxford, 1999.
J. Tian. A branch-and-bound algorithm for MDL
learning Bayesian networks. In C. Boutilier
and M. Goldszmidt, editors, Proceedings of
the Sixteenth Conference on Uncertainty in
Artiﬁcial Intelligence, pages 580–588, Stan-
ford, CA, 2000.
L. C. van der Gaag, S. Renooij, and V. M. H.
Coup´e. Sensitivity analysis of probabilis-
tic networks. In Advances in Probabilistic
Graphical Models, Studies in Fuzziness and
Soft Computing, volume 213, pages 103–124.
Springer, Berlin, 2007.
R. A. van Engelen. Approximating Bayesian be-
lief networks by arc removal. IEEE Transac-
tions on Pattern Analysis and Machine Intel-
ligence, 19(8):916–920, 1997.
T. Verma. Causal networks: semantic and ex-
pressiveness. Technical Report R-65. Cogne-
tive Systems Laboratory, UCLA, 1986.
T. Verma and J. Pearl. Causal networks: seman-
tics and expressiveness. In R. Shachter, T. S.
Lewitt, L. N. Kanal and J. F. Lemmer, ed-
itors, Uncertainty in Artiﬁcial Intelligence,
volume 4, pages 69–76. Elsevier Science Pub-
lishers, Amsterdam, 1990a.
T. Verma and J. Pearl. Equivalence and synthesis
of causal models. In Proceedings of the Sixth
Conference on Uncertainty in Artiﬁcial In-
telligence, pages 220–227, Cambridge, MA,
1990b. Also in P. Bonissone, M. Henrion,
L. N. Kanal, and J. F. Lemmer, editors, Uncer-
tainty in Artiﬁcial Intelligence 6, pages 225–
268. Elsevier Science Publishers, B.V., Ams-
terdam, 1991.
A. Viterbi. Error bounds for convolutional codes
and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory, 13(2):260–269, 1967.
J. Vomlel. Exploiting functional dependence
in Bayesian network inference. In Proceed-
ings of the Eighteenth Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 528–
535. Morgan Kaufmann, CA, 2002.
M. J. Wainwright, T. Jaakkola, and A. S.
Willsky. Tree-based reparameterization for
approximate inference on loopy graphs.
In Proceedings of the Annual Conference
on Neural Information Processing Systems,
pages 1001–1008. MIT Press, Cambridge,
MA, 2001.
M. J. Wainwright, T. Jaakkola, and A. S. Will-
sky. MAP estimation via agreement on trees:
message-passing and linear programming.
IEEE Transactions on Information Theory,
51(11):3697–3717, 2005.
L. Wasserman. All of Statistics. Springer, New
York, 2004.
G. I. Webb, J. R. Boughton, and Z. Wang. Not
so naive Bayes: aggregating one-dependence
estimators. Machine Learning 58(1):5–24,
2005.
J. Whittaker. Graphical Models in Applied Mul-
tivariate Statistics. Wiley, Chichester, Eng-
land, 1990.
S. Wright. Correlation and causation. Journal of
Agricultural Research, 20:557–585, 1921.
J. Yedidia. An idiosyncratic journey beyond
mean ﬁeld theory. In D. Saad and M. Op-
per, editors, Advanced Mean Field Methods:
Theory and Practice, chapter 3, pages 21–35.
MIT Press, Cambridge, MA, 2001.
J. Yedidia, W. Freeman, and Y. Weiss. Gener-
alized belief propagation. In Proceedings of
the Conference on Neural Information Pro-
cessing Systems, pages 689–695. MIT Press,
Cambridge, MA, 2000.
J. Yedidia, W. Freeman, and Y. Weiss. Con-
structing free-energy approximations and
generalized belief propagation algorithms.
IEEE Transactions on Information Theory,
51(7):2282–2312, 2005.
J. York. Use of the Gibbs sampler in expert
systems. Artiﬁcial Intelligence, 56:111–130,
1992.
C. Yuan and M. J. Druzdzel. An importance
sampling algorithm based on evidence pre-
propagation. In Proceedings of the 19th Con-
ference on Uncertainty in Artiﬁcial Intelli-
gence, pages 624–631. Morgan Kaufmann,
San Francisco, CA, 2003.

BIBLIOGRAPHY
539
C. Yuan and M. J. Druzdzel. Importance sam-
pling algorithms for Bayesian networks:
principles and performance. Mathematical
and Computer Modelling, 43:1189–1207,
2006.
C. Yuan, T.-C. Lu, and M. J. Druzdzel. An-
nealed MAP. In Proceedings of the Confer-
ence on Uncertainty in Artiﬁcial Intelligence,
pages 628–635. AUAI Press, Arlington, VA,
2004.
N. L. Zhang and D. Poole. A simple approach
to Bayesian network computations. In Pro-
ceedings of the Tenth Conference on Uncer-
tainty in Artiﬁcial Intelligence, pages 171–
178. Morgan Kaufmann, San Francisco, CA,
1994.
N. L. Zhang and D. Poole. Exploiting causal in-
dependence in Bayesian network inference.
Journal of Artiﬁcial Intelligence Research,
5:301–328, 1996.


Index
D-MAP, 270, 274, 275
D-MAR, 270, 275
D-MPE, 270, 274
D-PR, 270, 274
E-MAJSAT, 271, 274
MAXSAT, 275
MAJSAT, 271, 274
SAT, 271, 274
W-MAXSAT, 257, 281
WMC, 277, 278, 280
a-cutset, 183, 224
absolute error of estimate, 386
ADDs, 327
and compiling networks, 334
and variable elimination, 332
operations, 328
AIC score, 460
Akaike Information Criterion (AIC), 460
algebraic decision diagrams, see also ADDs, 327
alleles, 110
almost simplicial node, 207
ancestor, 54
aperiodic Markov chain, 402, 410
arithmetic circuit, 291
compiling from Bayesian networks, 300
propagation, 293
semantics of partial derivatives, 292
with local structure, 319
asymptotically Normal, 384, 392, 394, 443
automated reasoning, 1
balanced dtree, 228
Bayes
conditioning, 32
factor, 42
rule, 38
theorem, 38
Bayesian approach for learning Bayesian networks,
477
Bayesian Dirichlet (BD) score, 500
Bayesian Information Criterion (BIC), 461, 498
Bayesian network, 57
arithmetic circuit representation, 287
chain rule, 58, 60
CNF encodings, 319
CPT, 57
dimension, 460
dynamic, see also DBN, 98
family, 57
functional, 316
granularity, 90
instantiation, 57
monitors, 78
multiply-connected, 108, 143
parameter, 57
parametrization, 57
polynomial representation, 290
semantics of partial derivatives, 292
polytree, 108, 142
probability of evidence, 76
pruning, 144
simulating, 379
singly-connected, 108, 142
structure, 57
Bayesian parameter estimates, 483, 492
Bayesian scoring measure, 499
BD score, 500
BDe score, 501
BDeu score, 502
belief propagation, 340
algorithm, 163, 175
edge-deletion semantics, 354
generalized, 349
iterative, 343
loopy, 344
message schedule, 345
messages, 342
belief revision, 4
BER, 104
Beta distribution, 491
Bethe free energy, 363, 364
BIC score, 461, 498
binary jointree, 169, 222
bit error rate, 104
BP, see belief propagation, 340
branch decomposition, 232, 234
bucket elimination, 135, 147
bypass variable, 91, 95, 116
cache allocation, 192
candidate method, 486
case, 442
case analysis, 37, 178, 326
causal support, 342
causality, 12, 53, 55, 56, 64, 70, 85, 88, 92, 102, 112,
115, 116
CD distance, 418
CDF, 46, 101
central limit theorem, 384
541

542
INDEX
chain rule
for Bayesian networks, 58, 60, 131, 274, 403, 415, 447
for probability, 37, 39, 60
channel coding, 102
Chebyshev’s inequality, 382
chordal graph, 230
chordality axiom, 68
circuit complexity, 291, 313
classiﬁer
naive Bayes, 467, 469
tree-augmented naive Bayes (TAN), 467
clause, 21
IP, 319
PI, 319
unit, 21
clique, 135, 203, 204, 208
induced by elimination order, 231
maximal, 203, 231
clone, 255
cluster, 158, 164, 187, 216, 224
sequence, 204
CNF, 21
converting a sentence into CNF, 24
encoding of Bayesian network, 277, 279, 280, 319
weighted, 280
co-varying parameters, 422, 479
code word, 102
collect phase of jointree algorithm, 162, 167
compatible instantiations, 21
compiling Bayesian networks, 287
using CNF encodings, 304
using jointrees, 302
using variable elimination, 301, 334
with local structure, 322, 334
complete graph, 203
complexity classes, 271
composition axiom, 59
conditional
independence, 35
KL–divergence, 469
log-likelihood function, 469
mutual information, 37
probability table, see also CPT, 56
conditioning, 178
a probability distribution on evidence, 31, 32
a propositional sentence on a literal, 23
recursive, 181
using a loop-cutset, 180
with local structure, 323
conﬁdence interval, 382
conjunctive normal form, see also CNF, 21
consistency, 17
consistent
estimate, 384
instantiations, 21
sentence, 17
constrained
elimination order, 258
optimization, 452
treewidth, 259
constraint-based approach for learning Bayesian
networks, 440, 466
context, 186, 224
context-speciﬁc
caching, 324
decomposition, 324
independence, 313, 316, 324
continuous
distribution, 46, 101, 382, 489–491
variables, 12, 46, 101, 105, 491
contracting edge, 208
contraction axiom, 61
convolutional codes, 105
CPT, 56
ADD representation, 327
decision tree representation, 117
decomposition, 315
deterministic, 78, 101, 103, 118
large size, 114
non-tabular representation, 117, 327
rule-based representation, 118
cumulative distribution function, see also CDF, 46
cutset, 183, 224
cutset conditioning algorithm, 180
D-MAP, 69
d-separation, 63, 65, 86, 94, 223, 364, 454, 506
completeness, 68
complexity, 66
soundness, 67
DAG, 53
data set, 442
DBN, 98, 105, 106, 396, 400
decision theory, 5, 12
decoder, 103, 104
decomposability, 22, 304, 306, 320
decomposable graph, 230
decomposition
axiom, 59
graph, see also dgraph, 189
tree, see also dtree, 183, 224
deduction theorem, 25
degeneracy of graph, 208
degree
of graph, 208
of node, 203
degrees of belief, 4, 27
deleting edges, 255
Delta method, 413, 414
dependence MAP, see also D-MAP, 69
depth-ﬁrst search
for cache allocation, 193
for computing MAP, 260
for computing MPE, 250
for ﬁnding optimal elimination order, 209
for learning network structure, 464
descendants, 54
detailed balance, 402
determinism, 22, 304, 306, 316, 320, 325
deterministic
CPT, 78, 93, 101, 103, 118
node, 78
variable, 78
dgraph, 189

INDEX
543
diagnosis, 85, 87, 92
diagnostic support, 342
direct sampling, 383, 385
directed acyclic graph, see also DAG, 53
Dirichlet distribution, 490
discriminative model, 441, 467
disjunctive normal form, see also DNF, 21
distance measure
CD distance, 418
distribute phase of jointree algorithm, 162, 167
distribution
Beta, 491
continuous, 46, 101, 382, 489–491
Dirichlet, 490
empirical, 442
exponential, 101
Gaussian, see also Normal distribution, 46
importance, 393
initial, 402
jointree factorization, 223
lifetime, 101
marginal, 78
Normal, 46, 104, 382, 384, 392, 394, 443
proposal, 393
sampling, 383
standard Normal, 46, 47, 382
stationary, 402
strictly positive, 62
transition, 402
DNF, 21
dtree, 183, 214
balanced, 228
for DAG, 224
for graph, 234
from elimination order, 225
properties, 225
width, 187, 224
dynamic Bayesian network, see also DBN, 98
edge
contraction, 208
deletion, 255
ﬁll-in, 138, 204
effective treewidth, 144
elimination
and triangulation, 230
heuristics, 205
of a factor, 153
of a node, 204
preﬁx, 206
tree, 155
elimination order, 204
and triangulated graphs, 230
constrained, 258
from dtree, 216
from jointree, 214
optimal, 205, 209
perfect, 205, 230
width, 134
EM, 446
for MAP estimates, 488, 496
for ML estimates, 450
emission probabilities, 58
empirical distribution, 442
entropy, 30, 33, 346, 349, 444, 458, 517
relative, 518
equivalence, 17
equivalent sample size, 489
error
absolute, 386
relative, 387
standard, 385
estimate
consistent, 384
unbiased, 384
estimating
a conditional probability, 392
an expectation, 383
event, 16
evidence
and local structure, 317, 322
entering into a jointree, 167
hard, 39
indicator, 148, 163, 166, 342
probability of, 139, 141
soft, 39, 80, 104, 150, 354, 434, 435
variable, 77, 84, 143
virtual, 46, 49, 81
exhaustiveness, 17
expectation, 381
expectation maximization, see also EM, 446
expected
count, 448, 487
log-likelihood function, 451
value, 381
exponential distribution, 101
extended factor, 247, 248
factor, 129
division, 171, 176
elimination, 153
extended, 247
extended operations, 248
from tabular to ADD representations, 331
maximization operation, 246
multiplying, 131
reducing, 139
represented by ADDs, 327
summing out, 129
trivial, 129
false negative, 38, 44, 80, 87
false positive, 38, 44, 80, 87
feedback loops, 105
ﬁll-in edges, 138, 204
ﬁlled-in graph, 204
ﬁrst-order languages, 12
Forward algorithm, 249
free energy, 363
function blocks, 93
functional
dependencies, 316
networks, 316
functionally determined variable, 118
fuzzy logic, 5

544
INDEX
Gamma function, 490
Gaussian
distribution, see also Normal distribution,
46
noise, 104
GBP, see generalized belief propagation, 349
generalized belief propagation, 349
generative model, 441
genes, 110
genetic linkage, 109
genotype, 110
Gibbs sampling, 403
global parameter independence, 482
gradient ascent algorithm
for estimating network parameters,
451
graph
admiting a perfect elimination order,
230
chordal, 230
complete, 203
contraction, 208
contraction degeneracy, 209
decomposable, 230
degeneracy, 208
degree, 208
ﬁlled-in, 204
interaction, 135, 202
maximum minimum degree, 208
minor, 209
MMD, 208
moral, 203
sequence, 204
triangulated, 230
graphical models, xiii
graphoid axioms, 58, 62
greedy search
for cache allocation, 195
for learning network structure, 463
haplotype, 110
hard evidence, 39
heuristic
elimination, 205
min-degree, 137, 205
min-ﬁll, 138, 205
Hidden Markov Model, see also HMM, 55
hidden variable, 445
HMM, 55, 58, 249, 396
emission probabilities, 58
Forward algorithm, 249
most probable state path, 249
sensor probabilities, 58
sequence probability, 249
transition probabilities, 58
Veterbi algorithm, 249
Hoeffding’s inequality, 386
Hugin architecture, 166, 170, 308
hypergraph partitioning, 227
I-MAP, 68, 69
IBP, see also belief propagation, 343
IJGP, see also iterative joingraph propagation, 352
implication, 17
importance
distribution, 393
sampling, 393
independence, 34
context-speciﬁc, 313, 316, 324
contraction axiom, 61
d-separation, 63, 65, 86, 94, 223, 364, 454, 506
decomposition axiom, 59
graphoid axioms, 62
intersection axiom, 62
of network parameters, 481
positive graphoid axioms, 62
symmetry axiom, 59
triviality axioms, 62
using DAGs, 53
variable-based, 36, 59
weak union axiom, 61
independence MAP, see also I-MAP, 68
indicator
clause, 319
variable, 319
inequality
Chebyshev’s, 382
Hoeffding’s, 386
information
bits, 102
gain, 518
instantiation, 20
interaction graph, 135, 202
intermediary variable, 84
intermittent faults, 97
intersection axiom, 62
inward phase of jointree algorithm, 162, 167
IP clause, 278, 319
irreducible Markov chain, 402
iterative belief propagation, see also belief propagation,
343
iterative joingraph propagation, 352
Jeffrey’s rule, 40
joingraph, 350
joint marginal, 138, 140
jointree, 164, 213, 223
algorithm, 164
collect phase, 162, 167
distribute phase, 162, 167
Hugin architecture, 166, 170, 308
inward phase, 162, 167
outward phase, 162, 167
pull phase, 162, 167
push phase, 162, 167
Shenoy-Shafer architecture, 166, 167, 217, 223,
308
balanced, 229
binary, 169, 222
for DAG, 216
for graph, 217, 234
from dtree, 222
from elimination order, 220
minimal, 165, 166, 217

INDEX
545
property, 164, 216
transforming, 217, 262
width, 164, 216
junction tree, see also jointree, 216
K2 score, 463, 500
K3 score, 463
KL divergence, 346, 443, 470, 471, 473, 511, 518
knowledge base, 14
knowledge-based systems, 1
Kullback-Leibler divergence, see KL divergence, 518
Laplace approximation, 497
large-sample approximation, 496
latent variable, 445
law of large numbers, 384
law of total probability, 37
learning Bayesian networks
the Bayesian approach, 477
the constraint-based approach, 440, 466
the likelihood approach, 439
lifetime distribution, 101
likelihood
approach for learning Bayesian networks, 439
equivalence, 501
function, 443
of a structure, 444
of parameters, 443
principle, 440
weighting, 395
likelihood ratio, 51
literal, 21
local parameter independence, 482
local search
for computing MAP, 263
for estimating network parameters, 446, 451, 486, 495
for learning network structure, 462
local structure, 291, 300, 313
and arithmetic circuits, 319, 334
and CNF encodings, 319
and conditioning, 323
and determinism, 316, 325
and evidence, 317, 322
and variable elimination, 326
exposing, 318
log-likelihood
of parameters, 444
of structure, 444
logical
connectives, 14
constraints, 100, 316
properties, 16
relationships, 17
loop-cutset, 180
loopy belief propagation, see also belief propagation,
344
lower bound
on treewidth, 208
MAP, 82, 96, 100
computing using depth-ﬁrst search, 260
computing using local search, 263
computing using variable elimination, 258
instantiation, 258
probability, 258
upper bound, 261
variables, 83
MAP parameter estimates, 478, 485–487, 495
MAR, 454
marginal
distribution, 78
joint, 138, 140
likelihood, 485, 486
large-sample approximation, 496
stochastic sampling approximation, 496
posterior, 78, 138, 141
prior, 78, 133
marginalizing a variable, 130
Markov
blanket, 71
boundary, 71
Markov chain, 402
aperiodic, 402, 410
converge, 411
detailed balance, 402
homogeneous, 402
irreducible, 402
recurrent states, 402
reversible, 402
simulation, 401
states, 402
Markov Chain Monte Carlo, see also MCMC, 401
Markov networks, xiii, 12
Markovian assumptions, 55
maximal clique, 203
maximizer circuit, 296
maximizing out, 246, 248
maximum a posteriori, see also MAP, 243
maximum likelihood (ML) parameter estimates, 443,
450
maximum minimum degree, 208
MCMC, 401
MDL score, 461
mean, 381
message schedule, 345
micro models, 115
min-degree heuristic, 137, 205
min-ﬁll heuristic, 138, 205
minimal I-MAP, 69
minimal jointree, 165, 166, 217
Minimum Description Length, see also MDL, 461
minimum-cardinality models, 26
missing data at random, 454
missing-data mechanism, 453
MLF, 290, 305, 339
MMD, 208
model, 16
complexity, 460
counting, 277, 319
dimension, 460
from design, 92, 98
from expert, 85, 87
persistence, 98
selection, 499

546
INDEX
model-based reasoning, 2
monitors, 78
monotonicity, 18
monotonicity problem, 2
Monte Carlo simulation, 383
moral graph, 203
moralization, 203
most probable explanation, see also MPE, 244
MPE, 81
computing using circuit propagation, 296
computing using depth-ﬁrst search, 250
computing using variable elimination, 245
instantation, 245
probability, 244
reducing to W-MAXSAT, 257, 280
sensitivity analysis, 424
upper bound, 255
multi-linear function, see also MLF, 290
multivalued variables, 19
multiply-connected network, 108, 143
multiplying factors, 131
mutual exclusiveness, 17
mutual information, 36, 360, 363, 368, 456, 457,
517
naive Bayes, 86, 91, 149, 265, 468, 510
classiﬁer, 467, 469
negation normal form, see also NNF, 22
network, see Bayesian network, 57
NNF circuit, 22, 304–306, 320
decomposable, 22, 309
deterministic, 22, 309
smooth, 22, 309
node
almost simplicial, 207
simplicial, 207
noise model, 104
noisy-or model, 115, 150, 318, 339
non-descendants, 55
non-informative prior, 494
non-monotonic logic, xiii, 3
Normal distribution, 46, 104, 382, 384, 392, 394,
443
Occam’s razor, 460
odds, 41, 51, 120, 419
optimal elimination order, 205, 209
best-ﬁrst search, 210
depth-ﬁrst search, 209
outward phase of jointree algorithm, 162, 167
overﬁtting, 459
P-MAP, 69
parameter
clause, 319
co-varying, 422, 479
equality, 321
estimates
Bayesian, 483, 492
MAP, 478, 485–487
maximumum likelihood (ML), 443, 450
independence, 481
modularity, 501
set, 479, 489
variable, 319
parametric structure, see also local structure, 313
parents, 54
partial derivatives
of maximizer polynomials, 299
of network polynomials, 292
particle ﬁltering, 396
PDF, 46, 101
pedigree, 110
perfect elimination order, 205, 230
and triangulated graphs, 230
perfect MAP, see also P-MAP, 69
persistence model, 98
phenotype, 110
PI clause, 278, 319
polytree
algorithm, 7, 163, 175, 180, 340
network, 108, 142
positive graphoid axioms, 62
possible world, 15
posterior marginal, 78, 138, 141
potential, see also factor, 129
preprocessing rules, 206
prior
equivalence, 501
knowledge, 480
marginal, 78, 133
probability, 27
as an expectation, 383
density function, see also PDF, 46
distribution, 27
jointree factorization, 223
of evidence, 76, 139, 141
projecting a factor, 130
proposal distribution, 393
propositional sentence
consistent, 17
satisﬁable, 17
semantics, 15
syntax, 13
valid, 17
pruning
edges, 144
network, 144
nodes, 143
pull phase of jointree algorithm, 162, 167
push phase of jointree algorithm, 162, 167
qualiﬁcation problem, 3
query variable, 84, 143
random
restarts, 264, 462
sample, 378
variable, 381
Rao-Blackwell sampling, 391
recombination
event, 111
fraction or frequency, 111
recursive conditioning algorithm, 181

INDEX
547
reducing factor, 139
redundant bits, 102
refutation theorem, 25
rejection sampling, 392, 404, 409
relational languages, 12
relative
entropy, 518
error, 387
reliability
analysis, 98
block diagram, 98
reversible Markov chain, 402
running intersection property, 218, 219
SamIam, 76–78, 81, 89, 90
sample
mean, 383
variance, 385
sampling
direct, 383, 385
distribution, 383
Gibbs, 403
importance, 393
likelihood weighting, 395
particle ﬁltering, 396
Rao-Blackwell, 391
rejection, 392, 404, 409
sequential importance, 396
satisﬁability, 17
satisﬁable sentence, 17
score equivalence, 501
scoring measure
AIC, 460
Bayesian, 499
BD, 500
BDe, 501
BDeu, 502
BIC, 461
K2, 463, 500
K3, 463
MDL, 461
selective model averaging, 499
sensitivity analysis, 89, 100, 119, 417
network independent, 418
network speciﬁc, 423
of most probable explanations, 424
sensitivity function, 423
sensor probabilities, 58
separator, 157, 164, 216
sequential importance sampling, 396
Shenoy-Shafer architecture, 166, 167, 217, 223,
308
simplicial node, 207
simulating
a Bayesian network, 379
a Gibbs chain, 404
a Markov chain, 401
simulation
Markov chain, 401
Monte Carlo, 383
single-fault assumption, 86
singly-connected network, 108, 142
smoothness, 22, 25, 304, 306, 320
soft evidence, 39, 46, 80, 104, 150, 354, 434, 435
and continuous variables, 46
as a noisy sensor, 44
soft-max parameter space, 452
splitting variables, 255
standard
deviation, 382
error, 385
standard Normal distribution, 46, 47, 382
state of belief, 27
stationary distribution, 402
stochastic sampling, 378
strictly positive distribution, 62
structure
local, 291, 300, 313
parametric, 313
structure-based algorithms, 143, 313
sufﬁcient statistic, 442
summing out, 129
symmetry axiom, 59
systematic search
for cache allocation, 193
for computing MAP, 260
for computing MPE, 250
for computing optimal elimination orders, 209
for learning network structure, 464
TAN classiﬁer, 467
term, 21
transition probabilities, 58
tree
contraction, 228
decomposition, 231, 234
treewidth, 141, 142, 165
complexity, 231
constrained, 259
dtree deﬁnition, 227
effective, 144
elimination order deﬁnition, 205
jointree deﬁnition, 222
lower bound, 208, 209
of DAG, 203
of graph, 205
triangulated graph, 230
trivial
factor, 129
instantiation, 21
triviality axiom, 62
truth assignment, 15
turbo codes, 107
unbiased estimate, 384
unit
clause, 21
resolution, 23, 326
upper bound
for MAP, 261
for MPE, 255
valid sentence, 17
validity, 17

548
INDEX
variable
assignment, 15
binary, 14
Boolean, 14
bypass, 91, 95, 116
clone, 255
continuous, 12, 46, 101, 105, 491
deterministic, 78
evidence, 84, 143
functionally determined, 118
hidden, 445
instantiation, 20
intermediary, 84
latent, 445
maximizing out, 246, 248
multi-valued, 19
propositional, 14
query, 84, 143
splitting, 255
variable elimination
for compiling networks, 334
for inference, 126
using ADDs, 332
with local structure, 326
variable-based independence,
36
variance, 381
Veterbi algorithm, 249
virtual evidence, 46, 49, 81
weak union axiom, 61
weighted CNF, 280
weighted model counting, 277
with local structure, 319
WER, 104
width
of a-cutset, 184
of context, 186
of cutset, 184
of dtree, 187, 224
of elimination order, 134, 205
of elimination preﬁx, 206
of elimination tree, 158
of jointree, 164, 216
word error rate, 104
world, 15
zero counts, 443

