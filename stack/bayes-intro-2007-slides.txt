Lecture 1.
Introduction to Bayesian Monte Carlo
methods in WINBUGS
1-1

Introduction to Bayesian Analysis and WinBUGS
Summary
1. Probability as a means of representing uncertainty
2. Bayesian direct probability statements about parameters
3. Probability distributions
4. Monte Carlo simulation
5. Implementation in WinBUGS (and DoodleBUGS) - Demo
6. Directed graphs for representing probability models
7. Examples
1-2

Introduction to Bayesian Analysis and WinBUGS
How did it all start?
In 1763, Reverend Thomas Bayes of Tunbridge Wells wrote
In modern language, given r ∼Binomial(θ, n), what is Pr(θ1 < θ < θ2|r, n)?
1-3

Introduction to Bayesian Analysis and WinBUGS
Basic idea: Direct expression of uncertainty about
unknown parameters
eg ”There is an 89% probability that the absolute increase in major bleeds is less
than 10 percent with low-dose PLT transfusions” (Tinmouth et al, Transfusion,
2004)
% absolute increase in major bleeds
−50
−40
−30
−20
−10
0
10
20
30
1-4

Introduction to Bayesian Analysis and WinBUGS
Why a direct probability distribution?
1. Tells us what we want: what are plausible values for the parameter of interest?
2. No P-values: just calculate relevant tail areas
3. No (diﬃcult to interpret) conﬁdence intervals: just report, say, central area
that contains 95% of distribution
4. Easy to make predictions (see later)
5. Fits naturally into decision analysis / cost-eﬀectiveness analysis / project
prioritisation
6. There is a procedure for adapting the distribution in the light of additional
evidence: i.e. Bayes theorem allows us to learn from experience
1-5

Introduction to Bayesian Analysis and WinBUGS
Inference on proportions
What is a reasonable form for a prior distribution for a proportion?
θ ∼Beta[a, b] represents a beta distribution with properties:
p(θ|a, b)
=
Γ(a + b)
Γ(a)Γ(b) θa−1 (1 −θ)b−1;
θ ∈(0, 1)
E(θ|a, b)
=
a
a + b
V(θ|a, b)
=
ab
(a + b)2(a + b + 1) :
WinBUGS notation:
theta ~ dbeta(a,b)
1-6

Introduction to Bayesian Analysis and WinBUGS
Beta distribution
success rate
0.0
0.4
0.8
0
1
2
3
4
5
Beta(0.5,0.5)
success rate
0.0
0.4
0.8
0
1
2
3
4
5
Beta(1,1)
success rate
0.0
0.4
0.8
0
1
2
3
4
5
Beta(5,1)
success rate
0.0
0.4
0.8
0
1
2
3
4
5
Beta(5,5)
success rate
0.0
0.4
0.8
0
1 2 3 4
5
Beta(5,20)
success rate
0.0
0.4
0.8
0
4
8
12
Beta(50,200)
1-7

Introduction to Bayesian Analysis and WinBUGS
Gamma distribution
0
1
2
3
4
5
0.0
0.4
0.8
Gamma(0.1,0.1)
0
1
2
3
4
5
0.0
0.4
0.8
Gamma(1,1)
0
1
2
3
4
5
0.0
0.4
0.8
Gamma(3,3)
0
5
10
15
0.0
0.2
0.4
Gamma(3,0.5)
0
5
10
15
0.0
0.2
0.4
Gamma(30,5)
0
10
20
30
40
0.0
0.06
0.14
Gamma(10,0.5)
1-8

Introduction to Bayesian Analysis and WinBUGS
The Gamma distribution
Flexible distribution for positive quantities. If Y ∼Gamma[a, b]
p(y|a, b)
=
ba
Γ(a)ya−1 e−by;
y ∈(0, ∞)
E(Y |a, b)
=
a
b
V(Y |a, b)
=
a
b2.
• Gamma[1,b] distribution is exponential with mean 1/b
• Gamma[v
2, 1
2] is a Chi-squared χ2
v distribution on v degrees of freedom
• Y ∼Gamma[0.001,0.001] means that p(y) ∝1/y, or that log Y ≈Uniform
• Used as conjugate prior distribution for inverse variances (precisions)
• Used as sampling distribution for skewed positive valued quantities (alterna-
tive to log normal likelihood) — MLE of mean is sample mean
• WinBUGS notation:
y ~ dgamma(a,b)
1-9

Introduction to Bayesian Analysis and WinBUGS
Example: Drug
• Consider a drug to be given for relief of chronic pain
• Experience with similar compounds has suggested that annual response rates
between 0.2 and 0.6 could be feasible
• Interpret this as a distribution with mean = 0.4, standard deviation 0.1
• A Beta[9.2,13.8] distribution has these properties
1-10

Introduction to Bayesian Analysis and WinBUGS
0.0
0.2
0.4
0.6
0.8
1.0
probability of response
Beta[9.2, 13.8] prior distribution supporting response rates between 0.2 and 0.6,
1-11

Introduction to Bayesian Analysis and WinBUGS
Making predictions
Before observing a quantity Y , can provide its predictive distribution by integrating
out unknown parameter
p(Y ) =
Z
p(Y |θ)p(θ)dθ.
Predictions are useful in e.g. cost-eﬀectiveness models, design of studies, checking
whether observed data is compatible with expectations, and so on.
1-12

Introduction to Bayesian Analysis and WinBUGS
If
θ
∼
Beta[a, b]
Yn
∼
Binomial(θ, n),
the exact predictive distribution for Yn is known as the Beta-Binomial.
It has
the complex form
p(yn) = Γ(a + b)
Γ(a)Γ(b)

n
yn

Γ(a + yn)Γ(b + n −yn)
Γ(a + b + n)
.
E(Yn) = n
a
a + b
If a = b = 1 (Uniform distribution), p(yn)is uniform over 0,1,...,n.
But in WinBUGS we can just write
theta ~ dbeta(a,b)
Y
~ dbin(theta,n)
and the integration is automatically carried out and does not require algebraic
cleverness.
1-13

Introduction to Bayesian Analysis and WinBUGS
0.0
0.4
0.8
0
1
2
3
4
probability of response
density
(a) Prior dist.
0
5
10
15
20
number of successes
density
0.00
0.02
0.04
0.06
0.08
0.10
0.12
(b) Predictive dist.
(a) is the Beta prior distribution
(b) is the predictive Beta-Binomial distribution of the number of successes Y in
the next 20 trials
From Beta-binomial distribution, can calculate P(Yn ≥15) = 0.015.
1-14

Introduction to Bayesian Analysis and WinBUGS
Example: a Monte Carlo approach to estimating tail-areas of distributions
Suppose we want to know the probability of getting 8 or more heads when we
toss a fair coin 10 times.
An algebraic approach:
Pr(≥8 heads)
=
10
X
z=8
p

z|π = 1
2, n = 10

=

10
8


1
2
8 1
2
2
+

10
9


1
2
9 1
2
1
+

10
10


1
2
10 1
2
0
=
0.0547.
A physical approach would be to repeatedly throw a set of 10 coins and count
the proportion of throws that there were 8 or more heads.
1-15

Introduction to Bayesian Analysis and WinBUGS
A simulation approach uses a computer to toss the coins!
0
2
4
6
8
10
Number of heads
100 simulations
0
2
4
6
8
10
Number of heads
10,000 simulations
0
2
4
6
8
10
Number of heads
True distribution
Proportion with 8 or more ’heads’ in 10 tosses:
(a) After 100 ’throws’ (0.02); (b) after 10,000 throws (0.0577); (c) the true
Binomial distribution (0.0547)
1-16

Introduction to Bayesian Analysis and WinBUGS
General Monte Carlo analysis - ‘forward sampling’
Used extensively in risk modelling - can think of as ’adding uncertainty’ to a
spreadsheet
• Suppose have logical function f containing uncertain parameters
• Can express our uncertainty as a prior distribution
• Simulate many values from this prior distribution
• Calculate f at the simulated values (‘iterations’)
• Obtain an empirical predictive distribution for f
• Sometimes termed probabilistic sensitivity analysis
• Can do in Excel add-ons such as @RISK or Crystal Ball.
1-17

Introduction to Bayesian Analysis and WinBUGS
The BUGS program
Bayesian inference Using Gibbs Sampling
• Language for specifying complex Bayesian models
• Constructs object-oriented internal representation of the model
• Simulation from full conditionals using Gibbs sampling
• Current version (WinBUGS 1.4) runs in Windows
• ‘Classic’ BUGS available for UNIX but this is an old version
WinBUGS is freely available from http://www.mrc-bsu.cam.ac.uk/bugs
• Scripts enable WinBUGS 1.4 to run in batch mode or be called from other
software
• Interfaces developed for R, Excel, Splus, SAS, Matlab
• OpenBUGS site http://www.rni.helsinki.fi/openbugs provides an open source
version
1-18

Introduction to Bayesian Analysis and WinBUGS
Running WinBUGS for Monte Carlo analysis (no
data)
1. Open Speciﬁcation tool from Model menu.
2. Program responses are shown on bottom-left of screen.
3. Highlight model by double-click. Click on Check model.
4. Click on Compile.
5. Click on Gen Inits.
6. Open Update from Model menu, and Samples from Inference menu.
7. Type nodes to be monitored into Sample Monitor, and click set after each.
8. Type * into Sample Monitor, and click trace to see sampled values.
9. Click on Update to generate samples.
10. Type * into Sample Monitor, and click stats etc to see results on all monitored
nodes.
1-19

Introduction to Bayesian Analysis and WinBUGS
Using WinBUGS for Monte Carlo
The model for the ’coin’ example is
Y ∼Binomial(0.5, 10)
and we want to know P(Y ≥8).
This model is represented in the BUGS language as
model{
Y
~
dbin(0.5,10)
P8
<-
step(Y-7.5)
}
P8 is a step function which will take on the value 1 if Y -7.5 is ≥0, i.e. Y is 8 or
more, 0 if 7 or less.
Running this simulation for 100, 10000 and 1000000 iterations, and then taking
the empirical mean of P8, provided the previous estimated probabilities that Y will
be 8 or more.
1-20

Introduction to Bayesian Analysis and WinBUGS
Some aspects of the BUGS language
•
<-
represents logical dependence, e.g.
m <- a + b*x
•
~
represents stochastic dependence, e.g.
r ~ dunif(a,b)
• Can use arrays and loops
for (i in 1:n){
r[i] ~ dbin(p[i],n[i])
p[i] ~ dunif(0,1)
}
• Some functions can appear on left-hand-side of an expression, e.g.
logit(p[i])<- a + b*x[i]
log(m[i])
<- c + d*y[i]
• mean(p[]) to take mean of whole array, mean(p[m:n]) to take mean of elements
m to n. Also for sum(p[]).
•
dnorm(0,1)I(0,)
means the prior will be restricted to the range (0, ∞).
1-21

Introduction to Bayesian Analysis and WinBUGS
Functions in the BUGS language
• p <- step(x-.7) = 1 if x ≥0.7, 0 otherwise. Hence monitoring p and recording
its mean will give the probability that x≥0.7.
• p <- equals(x,.7) = 1 if x = 0.7, 0 otherwise.
•
tau <- 1/pow(s,2)
sets τ = 1/s2.
•
s <-
1/ sqrt(tau)
sets s = 1/√τ.
• p[i,k] <- inprod(pi[], Lambda[i,,k]) sets pik = P
j πjΛijk.
inprod2 may be
faster.
• See ’Model Speciﬁcation/Logical nodes’ in manual for full syntax.
1-22

Introduction to Bayesian Analysis and WinBUGS
Some common Distributions
Expression Distribution Usage
dbin
binomial
r ~ dbin(p,n)
dnorm
normal
x ~ dnorm(mu,tau)
dpois
Poisson
r ~ dpois(lambda)
dunif
uniform
x ~ dunif(a,b)
dgamma
gamma
x ~ dgamma(a,b)
NB. The normal is parameterised in terms of its mean and precision = 1/ variance
= 1/sd2.
See ’Model Speciﬁcation/The BUGS language: stochastic nodes/Distributions’
in manual for full syntax.
Functions cannot be used as arguments in distributions (you need to create
new nodes).
1-23

Introduction to Bayesian Analysis and WinBUGS
Drug example: Monte Carlo predictions
Our prior distribution for proportion of responders in one year θ was Beta[9.2, 13.8].
Consider situation before giving 20 patients the treatment. What is the chance
if getting 15 or more responders?
θ
∼Beta[9.2, 13.8] prior distribution
y
∼Binomial[θ, 20] sampling distribution
Pcrit = P(y ≥15)
Probability of exceeding critical threshold
# In BUGS syntax:
model{
theta
~ dbeta(9.2,13.8)
# prior distribution
y
~ dbin(theta,20)
# sampling distribution
P.crit
<- step(y-14.5)
# =1 if y >= 15, 0 otherwise
}
1-24

Introduction to Bayesian Analysis and WinBUGS
WinBUGS output and exact answers
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
theta
0.4008
0.09999 9.415E-4
0.2174
0.3981
0.6044
1
10000
y
8.058
2.917
0.03035
3.0
8.0
14.0
1
10000
P.crit
0.0151
0.122
0.001275
0.0
0.0
0.0
1
10000
Note that the mean of the 0-1 indicator P.crit provides the estimated tail-area
probability.
Exact answers from closed-form analysis:
• θ: mean 0.4 and standard deviation 0.1
• y: mean 8 and standard deviation 2.93.
• Probability of at least 15: 0.015
These are independent samples, and so MC error = SD/
√
No.iterations.
Can achieve arbitrary accuracy by running the simulation for longer.
1-25

Introduction to Bayesian Analysis and WinBUGS
WinBUGS output
Independent samples, and so no auto-correlation and no concern with conver-
gence.
1-26

Introduction to Bayesian Analysis and WinBUGS
Graphical representation of models
• Doodle represents each quantity as a node in directed acyclic graph (DAG).
• Constants are placed in rectangles, random quantities in ovals
• Stochastic dependence is represented by a single arrow, and logical function
as double arrow
• WinBUGS allows models to be speciﬁed graphically and run directly from the
graphical interface
• Can write code from Doodles
• Good for explanation, but can be tricky to set up
1-27

Introduction to Bayesian Analysis and WinBUGS
Script for running Drug Monte Carlo example
Run from Model/Script menu
display(’log’)
# set up log file
check(’c:/bugscourse/drug-MC’)
# check syntax of model
#
data(’c:/bugscourse/drug-data’)
# load data file if there is one
compile(1)
# generate code for 1 simulations
# inits(1,’c:/bugscourse/drug-in1’)
# load initial values if necessary
gen.inits()
# generate initial values for all unknown quantities
# not given initial values
set(theta)
# monitor the true response rate
set(y)
# monitor the predicted number of successes
set(P.crit)
# monitor whether a critical number of successes occur
trace(*)
# watch some simulated values (although slows down simulation)
update(10000)
# perform 10000 simulations
history(theta)
# Trace plot of samples for theta
stats(*)
# Calculate summary statistics for all monitored quantities
density(theta)
# Plot distribution of theta
density(y)
# Plot distribution of y
1-28

Introduction to Bayesian Analysis and WinBUGS
Example: Power — uncertainty in a power calculation
• a randomised trial planned with n patients in each of two arms
• response with standard deviation σ = 1
• aimed to have Type 1 error 5% and 80% power
• to detect a true diﬀerence of θ = 0.5 in mean response between the groups
Necessary sample size per group is
n = 2σ2
θ2 (0.84 + 1.96)2 = 63
Alternatively, for ﬁxed n, the power is
Power = Φ
 r
nθ2
2σ2 −1.96
!
.
1-29

Introduction to Bayesian Analysis and WinBUGS
Suppose we wish to express uncertainty concerning both θ and σ, e.g.
θ ∼N[0.5, 0.12],
σ ∼N[1, 0.32].
1. Simulate values of θ and σ from their prior distributions
2. Substitute them in the formulae
3. Obtain a predictive distribution over n or Power
prec.sigma
<- 1/(0.3*0.3)
# transform sd to precision=1/sd2
prec.theta
<- 1/(0.1*0.1)
sigma
~ dnorm(1, prec.sigma)I(0,)
theta
~ dnorm(.5, prec.theta)I(0,)
n
<- 2 * pow(
(.84 +1.96) * sigma / theta ,
2 )
power
<- phi(
sqrt(63/2)* theta /sigma
-1.96
)
prob70 <-step(power-.7)
1-30

Introduction to Bayesian Analysis and WinBUGS
Median 95% interval
n
62.5
9.3 to 247.2
Power (%)
80
29 to 100
For n= 63, the median power is 80%, and a trial of 63 patients per group could
be seriously underpowered. There is a 37% chance that the power is less than
70%.
1-31

Lecture 2.
Introduction to conjugate Bayesian
inference
2-1

Introduction to Bayesian Analysis and WinBUGS
What are Bayesian methods?
• Bayesian methods have been widely applied in many areas:
– medicine / epidemiology
– genetics
– ecology
– environmental sciences
– social and political sciences
– ﬁnance
– archaeology
– .....
• Motivations for adopting Bayesian approach vary:
– natural and coherent way of thinking about science and learning
– pragmatic choice that is suitable for the problem in hand
2-2

Introduction to Bayesian Analysis and WinBUGS
Spiegelhalter et al (2004) deﬁne a Bayesian approach as
‘the explicit use of external evidence in the design, monitoring, analysis,
interpretation and reporting of a [scientiﬁc investigation]’
They argue that a Bayesian approach is:
• more ﬂexible in adapting to each unique situation
• more eﬃcient in using all available evidence
• more useful in providing relevant quantitative summaries
than traditional methods
2-3

Introduction to Bayesian Analysis and WinBUGS
Example
A clinical trial is carried out to collect evidence about an unknown ‘treatment
eﬀect’
Conventional analysis
• p-value for H0: treatment eﬀect is zero
• Point estimate and CI as summaries of size of treatment eﬀect
Aim is to learn what this trial tells us about the treatment eﬀect
Bayesian analysis
• Asks: ‘how should this trial change our opinion about the treatment eﬀect?’
2-4

Introduction to Bayesian Analysis and WinBUGS
The Bayesian analyst needs to explicitly state
• a reasonable opinion concerning the plausibility of diﬀerent values of the
treatment eﬀect excluding the evidence from the trial (the prior distribution)
• the support for diﬀerent values of the treatment eﬀect based solely on data
from the trial (the likelihood),
and to combine these two sources to produce
• a ﬁnal opinion about the treatment eﬀect (the posterior distribution)
The ﬁnal combination is done using Bayes theorem, which essentially weights
the likelihood from the trial with the relative plausibilities deﬁned by the prior
distribution
One can view the Bayesian approach as a formalisation of the process of learning
from experience
2-5

Introduction to Bayesian Analysis and WinBUGS
Posterior distribution forms basis for all inference — can be summarised to provide
• point and interval estimates of treatment eﬀect
• point and interval estimates of any function of the parameters
• probability that treatment eﬀect exceeds a clinically relevant value
• prediction of treatment eﬀect in a new patient
• prior information for future trials
• inputs for decision making
• ....
2-6

Introduction to Bayesian Analysis and WinBUGS
Bayes theorem and its link with Bayesian inference
Bayes’ theorem Provable from probability axioms
Let A and B be events, then
p(A|B) = p(B|A)p(A)
p(B)
.
If Ai is a set of mutually exclusive and exhaustive events (i.e. p(S
i Ai) = P
i p(Ai) =
1), then
p(Ai|B) =
p(B|Ai)p(Ai)
P
j p(B|Aj)p(Aj).
2-7

Introduction to Bayesian Analysis and WinBUGS
Example: use of Bayes theorem in diagnostic testing
• A new HIV test is claimed to have “95% sensitivity and 98% speciﬁcity”
• In a population with an HIV prevalence of 1/1000, what is the chance that
patient testing positive actually has HIV?
Let A be the event that patient is truly HIV positive, A be the event that they
are truly HIV negative.
Let B be the event that they test positive.
We want p(A|B).
“95% sensitivity” means that p(B|A) = .95.
“98% speciﬁcity” means that p(B|A) = .02.
Now Bayes theorem says
p(A|B) =
p(B|A)p(A)
p(B|A)p(A) + p(B|A)p(A).
Hence p(A|B) =
.95×.001
.95×.001+.02×.999 = .045.
Thus over 95% of those testing positive will, in fact, not have HIV.
2-8

Introduction to Bayesian Analysis and WinBUGS
• Our intuition is poor when processing probabilistic evidence
• The vital issue is how should this test result change our belief that patient is
HIV positive?
• The disease prevalence can be thought of as a ‘prior’ probability (p = 0.001)
• Observing a positive result causes us to modify this probability to p = 0.045.
This is our ‘posterior’ probability that patient is HIV positive.
• Bayes theorem applied to observables (as in diagnostic testing) is uncontro-
versial and established
• More controversial is the use of Bayes theorem in general statistical analyses,
where parameters are the unknown quantities, and their prior distribution
needs to be speciﬁed — this is Bayesian inference
2-9

Introduction to Bayesian Analysis and WinBUGS
Bayesian inference
Makes fundamental distinction between
• Observable quantities x, i.e. the data
• Unknown quantities θ
θ can be statistical parameters, missing data, mismeasured data ...
→parameters are treated as random variables
→in the Bayesian framework, we make probability statements about model
parameters
! in the frequentist framework, parameters are ﬁxed non-random quantities
and the probability statements concern the data
2-10

Introduction to Bayesian Analysis and WinBUGS
As with any statistical analysis, we start by positing a model which speciﬁes
p(x | θ)
This is the likelihood, which relates all variables into a ’full probability model’
From a Bayesian point of view
• θ is unknown so should have a probability distribution reﬂecting our uncer-
tainty about it before seeing the data
→need to specify a prior distribution p(θ)
• x is known so we should condition on it
→use Bayes theorem to obtain conditional probability distribution for unob-
served quantities of interest given the data:
p(θ | x) =
p(θ) p(x | θ)
R
p(θ) p(x | θ) dθ ∝p(θ) p(x | θ)
This is the posterior distribution
The prior distribution p(θ), expresses our uncertainty about θ before seeing the
data.
The posterior distribution p(θ | x), expresses our uncertainty about θ after seeing
the data.
2-11

Introduction to Bayesian Analysis and WinBUGS
Inference on proportions using a continuous prior
Suppose we now observe r positive responses out of n patients.
Assuming patients are independent, with common unknown response rate θ, leads
to a binomial likelihood
p(r|n, θ)
=

n
r

θr(1 −θ)n−r ∝θr(1 −θ)n−r
θ needs to be given a continuous prior distribution.
Suppose that, before taking account of the evidence from our trial, we believe all
values for θ are equally likely (is this plausible?) ⇒θ ∼Unif(0, 1) i.e. p(θ) =
1
1−0 = 1
Posterior is then
p(θ|r, n) ∝θr(1 −θ)(n−r) × 1
This has form of the kernel of a Beta(r+1, n-r+1) distribution (see lect 1), where
θ
∼
Beta(a, b) ≡Γ(a + b)
Γ(a)Γ(b) θa−1(1 −θ)b−1
2-12

Introduction to Bayesian Analysis and WinBUGS
To represent external evidence that some response rates are more plausible than
others, it is mathematically convenient to use a Beta(a, b) prior distribution for θ
p(θ) ∝θa−1(1 −θ)b−1
Combining this with the binomial likelihood gives a posterior distribution
p(θ | r, n)
∝
p(r | θ, n)p(θ)
∝
θr(1 −θ)n−rθa−1(1 −θ)b−1
=
θr+a−1(1 −θ)n−r+b−1
∝
Beta(r + a, n −r + b)
2-13

Introduction to Bayesian Analysis and WinBUGS
Comments
• When the prior and posterior come from the same family of distributions the
prior is said to be conjugate to the likelihood
– Occurs when prior and likelihood have the same ‘kernel’
• Recall from lecture 1 that a Beta(a, b) distribution has
mean
=
a/(a + b),
variance
=
ab/

(a + b)2(a + b + 1)

Hence posterior mean is E(θ|r, n) = (r + a)/(n + a + b)
• a and b are equivalent to observing a priori a −1 successes in a + b −2 trials
→can be elicited
• With ﬁxed a and b, as r and n increase, E(θ|r, n) →r/n (the MLE), and the
variance tends to zero
– This is a general phenomenon: as n increases, posterior distribution gets
more concentrated and the likelihood dominates the prior
• A Beta(1, 1) is equivalent to Uniform(0, 1)
2-14

Introduction to Bayesian Analysis and WinBUGS
Example: Drug
• Recall example from lecture 1, where we consider early investigation of a new
drug
• Experience with similar compounds has suggested that response rates be-
tween 0.2 and 0.6 could be feasible
• We interpreted this as a distribution with mean = 0.4, standard deviation 0.1
and showed that a Beta(9.2,13.8) distribution has these properties
• Suppose we now treat n = 20 volunteers with the compound and observe
y = 15 positive responses
2-15

Introduction to Bayesian Analysis and WinBUGS
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
Beta(9.2, 13.8) prior distribution
supporting response rates
between 0.2 and 0.6
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
Likelihood arising from a
Binomial observation of 15
successes out of 20 cases
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
Parameters of the Beta
 distribution are updated to
(a+15, b+20-15) = (24.2, 18.8):
mean 24.2/(24.2+18.8) = 0.56
2-16

Introduction to Bayesian Analysis and WinBUGS
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
(a)
0
10
20
30
40
Number of sucesses
0
10
20
30
40
(b)
(a) Beta posterior distribution after having observed 15 successes in 20 trials
(b) predictive Beta-Binomial distribution of the number of successes ˜y40 in the
next 40 trials with mean 22.5 and standard deviation 4.3
Suppose we would consider continuing a development program if the drug man-
aged to achieve at least a further 25 successes out of these 40 future trials
From Beta-binomial distribution, can calculate P(˜y40 ≥25) = 0.329
2-17

Introduction to Bayesian Analysis and WinBUGS
Drug (continued): learning about parameters from
data using Markov chain Monte-Carlo (MCMC) meth-
ods in WInBUGS
• Using MCMC (e.g. in WinBUGS), no need to explicitly specify posterior
• Can just specify the prior and likelihood separately
• WinBUGS contains algorithms to evaluate the posterior given (almost) arbi-
trary speciﬁcation of prior and likelihood
– posterior doesn’t need to be closed form
– but can (usually) recognise conjugacy when it exists
2-18

Introduction to Bayesian Analysis and WinBUGS
The drug model can be written
θ
∼Beta[a, b]
prior distribution
y
∼Binomial[θ, m] sampling distribution
ypred ∼Binomial[θ, n]
predictive distribution
Pcrit = P(ypred ≥ncrit) Probability of exceeding critical threshold
# In BUGS syntax:
# Model description ’
model {
theta
~ dbeta(a,b)
# prior distribution
y
~ dbin(theta,m)
# sampling distribution
y.pred
~ dbin(theta,n)
# predictive distribution
P.crit
<- step(y.pred-ncrit+0.5)
# =1 if y.pred >= ncrit, 0 otherwise
}
2-19

Introduction to Bayesian Analysis and WinBUGS
Graphical representation of models
Note that adding data to a model is simply extending the graph.
2-20

Introduction to Bayesian Analysis and WinBUGS
Data ﬁles
Data can be written after the model description, or held in a separate .txt or .odc
ﬁle
list( a = 9.2,
# parameters of prior distribution
b = 13.8,
y = 15,
# number of successes
m = 20,
# number of trials
n = 40,
# future number of trials
ncrit = 25)
# critical value of future successes
Alternatively, in this simple example, we could have put all data and constants
into model description:
model{
theta
~ dbeta(9.2,13.8)
# prior distribution
y
~ dbin(theta,20)
# sampling distribution
y.pred
~ dbin(theta,40)
# predictive distribution
P.crit
<- step(y.pred-24.5)
# =1 if y.pred >= ncrit, 0 otherwise
y
<- 15
}
2-21

Introduction to Bayesian Analysis and WinBUGS
The WinBUGS data formats
WinBUGS accepts data ﬁles in:
1. Rectangular format (easy to cut and paste from spreadsheets)
n[] r[]
47
0
148 18
...
360 24
END
2. S-Plus format:
list(N=12,n = c(47,148,119,810,211,196,
148,215,207,97,256,360),
r = c(0,18,8,46,8,13,9,31,14,8,29,24))
Generally need a ‘list’ to give size of datasets etc.
2-22

Introduction to Bayesian Analysis and WinBUGS
Initial values
• WinBUGS can automatically generate initial values for the MCMC analysis
using gen inits
• Fine if have informative prior information
• If have fairly ‘vague’ priors, better to provide reasonable values in an initial-
values list
Initial values list can be after model description or in a separate ﬁle
list(theta=0.1)
2-23

Introduction to Bayesian Analysis and WinBUGS
Running WinBUGS for MCMC analysis (single chain)
1. Open Speciﬁcation tool from Model menu.
2. Program responses are shown on bottom-left of screen.
3. Highlight model by double-click. Click on Check model.
4. Highlight start of data. Click on Load data.
5. Click on Compile.
6. Highlight start of initial values. Click on Load inits.
7. Click on Gen Inits if more initial values needed.
8. Open Update from Model menu.
9. Click on Update to burn in.
10. Open Samples from Inference menu.
11. Type nodes to be monitored into Sample Monitor, and click set after each.
12. Perform more updates.
13. Type * into Sample Monitor, and click stats etc to see results on all monitored
nodes.
2-24

Introduction to Bayesian Analysis and WinBUGS
WinBUGS output
2-25

Introduction to Bayesian Analysis and WinBUGS
WinBUGS output and exact answers
node
mean
sd
MC error
2.5%
median
97.5%
start sample
theta
0.5633
0.07458
4.292E-4
0.4139
0.5647
0.7051
1001 30000
y.pred
22.52
4.278
0.02356
14.0
23.0
31.0
1001 30000
P.crit
0.3273
0.4692
0.002631
0.0
0.0
1.0
1001 30000
Exact answers from conjugate analysis
• θ: mean 0.563 and standard deviation 0.075
• Y pred: mean 22.51 and standard deviation 4.31.
• Probability of at least 25: 0.329
MCMC results are within Monte Carlo error of the true values
2-26

Introduction to Bayesian Analysis and WinBUGS
Bayesian inference using the Normal distribution
Known variance, unknown mean
Suppose we have a sample of Normal data xi ∼N(θ, σ2)
(i = 1, ..., n).
For now assume σ2 is known and θ has a Normal prior θ ∼N(µ, σ2/n0)
Same standard deviation σ is used in the likelihood and the prior. Prior variance
is based on an ‘implicit’ sample size n0
Then straightforward to show that the posterior distribution is
θ|x ∼N
n0µ + nx
n0 + n ,
σ2
n0 + n

2-27

Introduction to Bayesian Analysis and WinBUGS
• As n0 tends to 0, the prior variance becomes larger and the distribution
becomes ‘ﬂatter’, and in the limit the prior distribution becomes essentially
uniform over −∞, ∞
• Posterior mean (n0µ + nx)/(n0 + n) is a weighted average of the prior mean
µ and parameter estimate x, weighted by their precisions (relative ‘sample
sizes’), and so is always a compromise between the two
• Posterior variance is based on an implicit sample size equivalent to the sum
of the prior ‘sample size’ n0 and the sample size of the data n
• As n →∞, p(θ|x) →N(x, σ2/n) which does not depend on the prior
• Compare with frequentist setting, the MLE is ˆθ = ¯x with SE(ˆθ) = σ/√n, and
sampling distribution
p(ˆθ | θ) = p(¯x|θ) = N(θ, σ2/n)
2-28

Introduction to Bayesian Analysis and WinBUGS
Example: THM concentrations
• Regional water companies in the UK are required to take routine measure-
ments of trihalomethane (THM) concentrations in tap water samples for
regulatory purposes
• Samples are tested throughout the year in each water supply zone
• Suppose we want to estimate the average THM concentration in a particular
water zone, z
• Two independent measurements, xz1 and xz2 are taken and their mean, xz is
130 µg/l
• Suppose we know that the assay measurement error has a standard deviation
σ[e] = 5µg/l
• What should we estimate the mean THM concentration to be in this water
zone?
Let the mean THM conc. be denoted θz.
A standard analysis would use the sample mean xz = 130µg/l as an estimate of
θz, with standard error σ[e]/√n = 5/
√
2 = 3.5µg/l
A 95% conﬁdence interval is xz ± 1.96 × σ[e]/√n, i.e.
123.1 to 136.9 µg/l.
2-29

Introduction to Bayesian Analysis and WinBUGS
Suppose historical data on THM levels in other zones supplied from the same
source showed that the mean THM concentration was 120 µg/l with standard
deviation 10 µg/l
• suggests Normal(120, 102) prior for θz
• if we express the prior standard deviation as σ[e]/√n0, we can solve to ﬁnd
n0 = (σ[e]/10)2 = 0.25
• so our prior can be written as θz ∼Normal(120, σ2
[e]/0.25)
Posterior for θz is then
p(θz|x)
=
Normal
0.25 × 120 + 2 × 130
0.25 + 2
,
52
0.25 + 2

=
Normal(128.9, 3.332)
giving 95% interval for θz of 122.4 to 135.4µg/l
2-30

Introduction to Bayesian Analysis and WinBUGS
80
100
120
140
160
180
mean THM concentration, ug/l (theta)
Prior
Likelihood
Posterior
2-31

Introduction to Bayesian Analysis and WinBUGS
Prediction
Denoting the posterior mean and variance as µn = (n0µ + nx)/(n0 + n) and σ2
n =
σ2/(n0 + n), the predictive distribution for a new observation ˜x is
p(˜x|x) =
Z
p(˜x|x, θ)p(θ|x)dθ
which generally simpliﬁes to
p(˜x|x) =
Z
p(˜x|θ)p(θ|x)dθ
which can be shown to give
p(˜x|x) ∼N  µn, σ2
n + σ2
So the predictive distribution is centred around the posterior mean with variance
equal to sum of the posterior variance and the sample variance of ˜x
2-32

Introduction to Bayesian Analysis and WinBUGS
Example: THM concentration (continued)
• Suppose the water company will be ﬁned if THM levels in the water supply
exceed 145µg/l
• Predictive distribution for THM concentration in a future sample taken from
the water zone is N(128.9, 3.332 + 52) = N(128.9, 36.1)
• Probability that THM concentration in future sample exceeds 145µg/l is
1 −Φ[(145 −128.9)/
p
(36.1)] = 0.004
80
100
120
140
160
180
THM concentration, ug/l
Posterior
Predictive
2-33

Introduction to Bayesian Analysis and WinBUGS
Bayesian inference using count data
Suppose we have an independent sample of counts x1, ..., xn which can be assumed
to follow a Poisson distribution with unknown mean µ:
p(x|µ)
=
Y
i
µxie−µ
xi!
The kernel of the Poisson likelihood (as a function of µ) has the same form as
that of a Gamma(a, b) prior for µ:
p(µ)
=
ba
Γ(a)µa−1e−bµ
Note: A Gamma(a, b) density has mean a/b and variance a/b2
2-34

Introduction to Bayesian Analysis and WinBUGS
This implies the following posterior
p(µ | x)
∝
p(µ) p(x | µ)
=
ba
Γ(a)µa−1e−bµ
n
Y
i=1
e−µµxi
xi!
∝
µa+nx−1 e−(b+n)µ
=
Gamma(a + nx, b + n).
The posterior is another (diﬀerent) Gamma distribution.
The Gamma distribution is said to be the conjugate prior.
E(µ | x)
=
a + nx
b + n
=
x

n
n + b

+ a
b

1 −
n
n + b

So posterior mean is a compromise between the prior mean a/b and the MLE x
2-35

Introduction to Bayesian Analysis and WinBUGS
Example: London bombings during WWII
• Data below are the number of ﬂying bomb hits on London during World War
II in a 36 km2 area of South London
• Area was partitioned into 0.25 km2 grid squares and number of bombs falling
in each grid was counted
Hits, x
0
1
2
3
4
7
Number of areas, n 229 211
93
35
7
1
Total hits, P
i nixi = 537
Total number of areas, P
i ni = 576
• If the hits are random, a Poisson distribution with constant hit rate θ should
ﬁt the data
• Can think of n = 576 observations from a Poisson distribution, with x =
537/576 = 0.93
2-36

Introduction to Bayesian Analysis and WinBUGS
The ‘invariant’ Jeﬀreys prior (see later) for the mean θ of a Poisson distribution
is p(θ) ∝1/
√
θ, which is equivalent to an (improper) Gamma(0.5,0) distribution.
Therefore
p(θ|y)
=
Gamma(a + nx, b + n) = Gamma(537.5, 576)
IE(θ|y)
=
537.5
576
= 0.933;
Var(θ|y) = 537.5
5762 = 0.0016
Note that these are almost exactly the same as the MLE and the square of the
SE(MLE)
2-37

Introduction to Bayesian Analysis and WinBUGS
Summary
For all these examples, we see that
• the posterior mean is a compromise between the prior mean and the MLE
• the posterior s.d. is less than each of the prior s.d. and the s.e.(MLE)
‘A Bayesian is one who, vaguely expecting a horse and catching a glimpse
of a donkey, strongly concludes he has seen a mule’ (Senn, 1997)
As n →∞,
• the posterior mean →the MLE
• the posterior s.d. →the s.e.(MLE)
• the posterior does not depend on the prior.
These observations are generally true, when the MLE exists and is unique
2-38

Introduction to Bayesian Analysis and WinBUGS
Choosing prior distributions
When the posterior is in the same family as the prior then we have what is
known as conjugacy. This has the advantage that prior parameters can usually
be interpreted as a prior sample. Examples include:
Likelihood
Parameter
Prior
Posterior
Normal
mean
Normal
Normal
Normal
precision
Gamma Gamma
Binomial
success prob.
Beta
Beta
Poisson
rate or mean Gamma Gamma
• Conjugate prior distributions are mathematically convenient, but do not exist
for all likelihoods, and can be restrictive
• Computations for non-conjugate priors are harder, but possible using MCMC
(see next lecture)
2-39

Introduction to Bayesian Analysis and WinBUGS
Calling WinBUGS from other software
• Scripts enable WinBUGS 1.4 to be called from other software
• Interfaces developed for R, Splus, SAS, Matlab
• See www.mrc-bsu.cam.ac.uk/bugs/welcome.shtml
• Andrew Gelman’s bugs function for R is most developed - reads in data, writes
script, monitors output etc. Now packaged as R2WinBUGS.
• OpenBUGS site http://mathstat.helsinki.ﬁ/openbugs/ provides an open source
version, including BRugs package which works from within R
2-40

Introduction to Bayesian Analysis and WinBUGS
Further reading
Berry (1996) (Introductory text on Bayesian methods, with medical slant)
Lee (2004) (Good intro to Bayesian inference; more mathematical than Berry;
3rd edition contains WinBUGS examples)
Bernardo and Smith (1994) (Advanced text on Bayesian theory)
2-41

Lecture 3.
Introduction to MCMC
3-1

Introduction to Bayesian Analysis and WinBUGS
Why is computation important?
• Bayesian inference centres around the posterior distribution
p(θ|x) ∝p(x|θ) × p(θ)
where θ is typically a large vector of parameters θ = {θ1, θ2, ...., θk}
• p(x|θ) and p(θ) will often be available in closed form, but p(θ|x) is usually
not analytically tractable, and we want to
– obtain marginal posterior p(θi|x) = R R
... R
p(θ|x) dθ(−i) where θ(−i) de-
notes the vector of θ’s excluding θi
– calculate properties of p(θi|x), such as mean (= R
θip(θi|x)dθi), tail areas
(= R ∞
T p(θi|x)dθi) etc.
→numerical integration becomes vital
3-2

Introduction to Bayesian Analysis and WinBUGS
Monte Carlo integration
We have already seen that Monte Carlo methods can be used to simulate values
from prior distributions and from closed form posterior distributions
If we had algorithms for sampling from arbitrary (typically high-dimensional) pos-
terior distributions, we could use Monte Carlo methods for Bayesian estimation:
• Suppose we can draw samples from the joint posterior distribution for θ, i.e.
(θ(1)
1 , ..., θ(1)
k
), (θ(2)
1 , ..., θ(2)
k
), ..., (θ(N)
1
, ..., θ(N)
k
)
∼
p(θ|x)
• Then
– θ(1)
1 , ..., θ(N)
1
are a sample from the marginal posterior p(θ1|x)
– E(g(θ1)) = R
g(θ1)p(θ1|x)dθ1 ≈
1
N
PN
i=1 g(θ(i)
1 )
→this is Monte Carlo integration
→theorems exist which prove convergence in limit as N →∞even if the sample
is dependent (crucial to the success of MCMC)
3-3

Introduction to Bayesian Analysis and WinBUGS
How do we sample from the posterior?
• We want samples from joint posterior distribution p(θ|x)
• Independent sampling from p(θ|x) may be diﬃcult
• BUT dependent sampling from a Markov chain with p(θ|x) as its stationary
(equilibrium) distribution is easier
• A sequence of random variables θ(0), θ(1), θ(2), ... forms a Markov chain if
θ(i+1) ∼p(θ|θ(i)) i.e.
conditional on the value of θ(i), θ(i+1) is independent
of θ(i−1), ..., θ(0)
• Several standard ‘recipes’ available for designing Markov chains with required
stationary distribution p(θ|x)
– Metropolis et al. (1953); generalised by Hastings (1970)
– Gibbs Sampling (see Geman and Geman (1984), Gelfand and Smith
(1990), Casella and George (1992)) is a special case of the Metropolis-
Hastings algorithm which generates a Markov chain by sampling from full
conditional distributions
– See Gilks, Richardson and Spiegelhalter (1996) for a full introduction and
many worked examples.
3-4

Introduction to Bayesian Analysis and WinBUGS
Gibbs sampling
Let our vector of unknowns θ consist of k sub-components θ = (θ1, θ2, ..., θk)
1) Choose starting values θ(0)
1 , θ(0)
2 , ..., , θ(0)
k
2) Sample θ(1)
1
from p(θ1|θ(0)
2 , θ(0)
3 , ..., , θ(0)
k
, x)
Sample θ(1)
2
from p(θ2|θ(1)
1 , θ(0)
3 , ..., , θ(0)
k
, x)
.....
Sample θ(1)
k
from p(θk|θ(1)
1 , θ(1)
2 , ..., , θ(1)
k−1, x)
3) Repeat step 2 many 1000s of times
– eventually obtain sample from p(θ|x)
The conditional distributions are called ‘full conditionals’ as they condition on all
other parameters
3-5

Introduction to Bayesian Analysis and WinBUGS
Gibbs sampling ctd.
Example with k = 2
θ1
θ2
p(θ)
θ(0)
θ(1)
θ(2)
• Sample θ(1)
1
from p(θ1|θ(0)
2 , x)
• Sample θ(1)
2
from p(θ2|θ(1)
1 , x)
• Sample θ(2)
1
from p(θ1|θ(1)
2 , x)
• ......
θ(n) forms a Markov chain with (eventually) a stationary distribution p(θ|x).
3-6

Introduction to Bayesian Analysis and WinBUGS
Using MCMC methods
There are two main issues to consider
• Convergence (how quickly does the distribution of θ(t) approach p(θ|x)?)
• Eﬃciency (how well are functionals of p(θ|x) estimated from {θ(t)}?)
3-7

Introduction to Bayesian Analysis and WinBUGS
Checking convergence
This is the users responsibility!
• Note: Convergence is to target distribution (the required posterior), not to
a single value.
• Once convergence reached, samples should look like a random scatter about
a stable mean value
3-8

Introduction to Bayesian Analysis and WinBUGS
Convergence diagnosis
• How do we know we have reached convergence?
• i.e. How do we the know number of ‘burn-in’ iterations?
• Many ‘convergence diagnostics’ exist, but none foolproof
• CODA and BOA software contain large number of diagnostics
Gelman-Rubin-Brooks diagnostic
• A number of runs
• Widely diﬀering starting points
• Convergence assessed by quantifying whether sequences are much further
apart than expected based on their internal variability
• Diagnostic uses components of variance of the multiple sequences
3-9

Introduction to Bayesian Analysis and WinBUGS
Example: A dose-response model
Consider the following response rates for diﬀerent doses of a drug
dose xi No. subjects ni No. responses ri
1.69
59
6
1.72
60
13
1.75
62
18
1.78
56
28
1.81
63
52
1.83
59
53
1.86
62
61
1.88
60
60
Fit a logistic curve with ‘centred’ covariate (xi −x):
ri
∼
Bin(pi, ni)
logit pi
=
α + β(xi −x)
α
∼
N(0, 10000)
β
∼
N(0, 10000)
3-10

Introduction to Bayesian Analysis and WinBUGS
Checking convergence with multiple runs
• Set up multiple initial value lists, e.g.
list(alpha=-100, beta=100)
list(alpha=100, beta=-100)
• Before clicking compile, set num of chains to 2
• Load both sets of initial values
• Monitor from the start of sampling
• Assess how much burn-in needed using the bgr statistic
Using the bgr statistic
• Green: width of 80% intervals of pooled chains: should be stable
• Blue: average width of 80% intervals for chains: should be stable
• Red: ratio of pooled/within: should be near 1
• Double-click on plot, then cntl + right click gives statistics
3-11

Introduction to Bayesian Analysis and WinBUGS
beta chains 1:2
iteration
1
2000
4000
6000
    0.0
  100.0
  200.0
  300.0
  400.0
beta chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
beta chains 1:2
iteration
1
2000
4000
6000
    0.0
    0.5
    1.0
beta chains 2:1
iteration
7950
7900
7850
7800
7750
   20.0
   30.0
   40.0
   50.0
history
trace
bgr diagnostic
autocorrelation
node   mean   sd
MC error
2.5%    median  97.5%
start
sample
alpha  0.7489 0.139  0.00138
0.4816
0.7468
1.026
1001
14000
beta   34.6   2.929  0.02639
29.11
34.53
40.51
1001
14000
Output  for ‘centred’ analysis
3-12

Introduction to Bayesian Analysis and WinBUGS
Problems with convergence
Fit a logistic curve with ‘un-centred’ covariate x:
ri
∼
Bin(pi, ni)
logit pi
=
α + βxi
α
∼
N(0, 10000)
β
∼
N(0, 10000)
3-13

Introduction to Bayesian Analysis and WinBUGS
alpha chains 1:2
iteration
1
5000
10000
15000
20000
 -100.0
  -75.0
  -50.0
  -25.0
    0.0
   25.0
beta chains 1:2
iteration
1
5000
10000
15000
20000
  -20.0
    0.0
   20.0
   40.0
   60.0
History plots for ‘un-centred’ analysis
3-14

Introduction to Bayesian Analysis and WinBUGS
beta chains 1:2
iteration
1
20000
40000
60000
    0.0
    1.0
    2.0
    3.0
    4.0
node  mean   sd
MC error
2.5%   median
97.5%
start
sample
beta  33.97  2.955  0.1734
28.7   33.89
40.3
40001
40000
Drop first 40,000 iterations as burn-in
bgr output for ‘un-centred’ analysis
3-15

Introduction to Bayesian Analysis and WinBUGS
beta
   25.0
   35.0
alpha
  -80.0
  -70.0
  -60.0
  -50.0
  -40.0
beta
   20.0
   30.0
   40.0
alpha
    0.0
    0.5
    1.0
    1.5
beta chains 1:2 sample: 40000
   20.0    25.0    30.0    35.0    40.0
    0.0
   0.05
    0.1
   0.15
beta chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
autocorrelation
posterior density
centred
un-centred
bivariate posteriors
Output  for ‘un-centred’ analysis
3-16

Introduction to Bayesian Analysis and WinBUGS
How many iterations after convergence?
• After convergence, further iterations are needed to obtain samples for pos-
terior inference.
• More iterations = more accurate posterior estimates.
• Eﬃciency of sample mean of θ as estimate of theoretical posterior expectation
E(θ) usually assessed by calculating Monte Carlo standard error (MC error)
• MC error = standard error of posterior sample mean as estimate of theoretical
expectation for given parameter
• MC error depends on
– true variance of posterior distribution
– posterior sample size (number of MCMC iterations)
– autocorrelation in MCMC sample
• Rule of thumb: want MC error < 1 −5% of posterior SD
3-17

Introduction to Bayesian Analysis and WinBUGS
Inference using posterior samples from MCMC runs
A powerful feature of the Bayesian approach is that all inference is based on the
joint posterior distribution
⇒can address wide range of substantive questions by appropriate summaries of
the posterior
• Typically report either mean or median of the posterior samples for each
parameter of interest as a point estimate
• 2.5% and 97.5% percentiles of the posterior samples for each parameter give
a 95% posterior credible interval (interval within which the parameter lies
with probability 0.95)
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
beta
34.60
2.929
0.0239
29.11
34.53
40.51
1001
14000
So point estimate of beta would be 34.60, with 95% credible interval (29.11,
40.51)
3-18

Introduction to Bayesian Analysis and WinBUGS
Probability statements about parameters
• Classical inference cannot provide probability statements about parameters
(e.g. p-value is not Pr(H0 true), but probability of observing data as or more
extreme than we obtained, given that H0 is true)
• In Bayesian inference, it is simple to calculate e.g. Pr(θ > 1):
= Area under posterior distribution curve to the right of 1
= Proportion of values in posterior sample of θ which are > 1
theta
0.5
1.0
1.5
2.0
2.5
3.0
Shaded Area
=
Prob(theta>1)
Posterior Distribution of theta
• In WinBUGS use the step function:
p.theta <- step(theta - 1)
• For discrete parameters, may also be
interested in Pr(δ = δ0):
p.delta <- equals(delta, delta0)
• Posterior means of p.theta and p.delta
give the required probabilities
3-19

Introduction to Bayesian Analysis and WinBUGS
Complex functions of parameters
• Classical inference about a function of the parameters g(θ) requires con-
struction of a speciﬁc estimator of g(θ). Obtaining appropriate error can be
diﬃcult.
• Easy using MCMC: just calculate required function g(θ) as a logical node at
each iteration and summarise posterior samples of g(θ)
In dose-response example, suppose we want to estimate the ED95: that is the
dose that will provide 95% of maximum eﬃcacy.
logit 0.95
=
α + β(ED95 −x)
ED95
=
(logit 0.95 −α)/β + x
Simply add into model
ED95 <- (logit(0.95) - alpha)/beta + mean(x[])
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
ED95
1.857
0.007716
8.514E-5
1.843
1.857
1.874
1001
10000
3-20

Introduction to Bayesian Analysis and WinBUGS
How to rank if you must
• Recent trend in UK towards ranking ‘institutional’ performance e.g. schools,
hospitals
• Might also want to rank treatments, answer ‘which is the best‘ etc
• Rank of a point estimate is a highly unreliable summary statistic
⇒Would like measure of uncertainty about rank
• Bayesian methods provide posterior interval estimates for ranks
• WinBUGS contains ‘built-in’ options for ranks:
– Rank option of Inference menu monitors the rank of the elements of a
speciﬁed vector
– rank(x[], i) returns the rank of the ith element of x
– equals(rank(x[],i),1)
=1 if ith element is ranked lowest, 0 otherwise.
Mean is probability that ith element is ’best’ (if counting adverse events)
– ranked(x[], i) returns the value of the ith-ranked element of x
3-21

Introduction to Bayesian Analysis and WinBUGS
Example of ranking: ‘Blocker’ trials
• 22 trials of beta-blockers used in WinBUGS manual to illustrate random-
eﬀects meta-analysis.
• Consider just treatment arms: which trial has the lowest mortality rate?
• Assume independent ‘Jeﬀreys’ beta[0.5, 0.5] prior for each response rate.
for( i in 1 : Num) {
rt[i] ~ dbin(pt[i],nt[i])
pt[i] ~ dbeta(0.5,0.5)
# Jeffreys prior
rnk[i] <-
rank(pt[], i)
# rank of i’th trial
prob.lowest[i]
<- equals(rnk[i],1)
# prob that i’th trial lowest
N[i]<-i
# used for indexing plot
}
3-22

Introduction to Bayesian Analysis and WinBUGS
[13]
[22]
[19]
[18]
[2]
[14]
[4]
[21]
[10]
[11]
[6]
[3]
[5]
[1]
[9]
[8]
[7]
[17]
[20]
[16]
[15]
[12]
box plot: pt
    0.0
    0.1
    0.2
Mortality rates and ranks
[13]
[22]
[19]
[18]
[2]
[4]
[14]
[21]
[6]
[10]
[11]
[3]
[5]
[1]
[9]
[8]
[7]
[17]
[20]
[16]
[15]
[12]
caterpillar plot: rnk
    0.0
   10.0
   20.0
3-23

Introduction to Bayesian Analysis and WinBUGS
Trial
    0.0
   10.0
   20.0
Prob of lowest mortality
    0.0
    0.5
    1.0
Ranking methods may be useful when
• comparing alternative treatments
• comparing subsets
• comparing response-rates, cost-eﬀectiveness or any summary measure
3-24

Introduction to Bayesian Analysis and WinBUGS
Further reading
Gelfand and Smith (1990) (key reference to use of Gibbs sampling for Bayesian
calculations)
Casella and George (1992) (Explanation of Gibbs sampling)
Brooks (1998) (tutorial paper on MCMC)
Spiegelhalter et al (1996) (Comprehensive coverage of practical aspects of MCMC)
3-25

Lecture 4.
Bayesian linear regression models
4-1

Introduction to Bayesian Analysis and WinBUGS
Bayesian regression models
Standard (and non standard) regression models can be easily formulated within a
Bayesian framework.
• Specify probability distribution (likelihood) for the data
• Specify form of relationship between response and explanatory variables
• Specify prior distributions for regression coeﬃcients and any other unknown
(nuisance) parameters
4-2

Introduction to Bayesian Analysis and WinBUGS
Some advantages of a Bayesian formulation in regression modelling include:
• Easy to include parameter restrictions and other relevant prior knowledge
• Easily extended to non-linear regression
• Easily ‘robustiﬁed’
• Easy to make inference about functions of regression parameters and/or pre-
dictions
• Easily extended to handle missing data and covariate measurement error
4-3

Introduction to Bayesian Analysis and WinBUGS
Linear regression
Consider a simple linear regression with univariate Normal outcome yi and a vector
of covariates x1i, ..., xpi, i = 1, ..., n
yi
=
β0 +
p
X
k=1
βkxki + ǫi
ǫi
∼
Normal(0, σ2)
An equivalent Bayesian formulation would typically specify
yi
∼
Normal(µi, σ2)
µi
=
β0 +
p
X
k=1
βkxki
(β0, β1, ..., βp, σ2)
∼
Prior distributions
A typical choice of ‘vague’ prior distribution (see later for more details) that will
give numerical results similar to OLS or MLE is:
βk
∼
N(0, 100000)
k = 0, ..., p
1/σ2
∼
Gamma(0.001, 0.001)
4-4

Introduction to Bayesian Analysis and WinBUGS
Example: New York Crime data
• 23 Precincts in New York City
• Response = THEFT: seasonally adjusted changes in larcenies (thefts) from a
27-week base period in 1966 to a 58-week experimental period in 1966-1967
• Predictors = MAN: % change in police manpower; DIST: district indicator
(1 Downtown, 2 Mid-town, 3 Up-town)
• Model speciﬁcation:
THEFTi
∼
Normal(µi, σ2)
i = 1, ..., 21
µi
=
α + β × MANi + <eﬀect of DIST>
1/σ2
∼
Gamma(0.001, 0.001)
α
∼
N(0, 100000)
β
∼
N(0, 100000)
Prior
on
coeﬃcients for DIST eﬀect
4-5

Introduction to Bayesian Analysis and WinBUGS
Specifying categorical covariates using the BUGS language
DISTi is a 3-level categorical explanatory variable
Two alternative ways of specifying model in BUGS language
1. Create usual ’design matrix’ in data ﬁle:
MAN[] THEFT[] DIST2[] DIST3[]
-15.76
3.19
0
0
# district 1
0.98
-3.45
0
0
3.71
0.04
0
0
.......
-9.56
3.68
0
0
-2.06
8.63
1
0
# district 2
-0.76
10.82
1
0
-6.30
-0.50
1
0
.......
-2.82
-2.02
1
0
-16.19
0.94
0
1
# district 3
-11.00
4.42
0
1
......
-10.77
1.58
0
1
END
4-6

Introduction to Bayesian Analysis and WinBUGS
BUGS model code is then
for (i in 1:N) {
THEFT[i] ~ dnorm(mu[i], tau)
mu[i] <- alpha + beta*MAN[i] + delta2*DIST2[i] + delta3*DIST3[i]
}
alpha ~ dnorm(0, 0.00001)
beta ~ dnorm(0, 0.00001)
delta2 ~ dnorm(0, 0.00001)
delta3 ~ dnorm(0, 0.00001)
tau ~ dgamma(0.001, 0.001)
sigma2 <- 1/tau
Note: BUGS parameterises normal in terms of mean and precision (1/variance)!!
Initial values ﬁle would be something like
list(alpha = 1, beta = -2, delta2 = -2, delta3 = 4, tau = 2)
4-7

Introduction to Bayesian Analysis and WinBUGS
2. Alternatively, input explanatory variable as single vector coded by its level:
MAN[]
THEFT[]
DIST[]
-15.76
3.19
1
0.98
-3.45
1
3.71
0.04
1
.....
-9.56
3.68
1
-2.06
8.63
2
-0.76
10.82
2
-6.30
-0.50
2
.....
-2.82
-2.02
2
-16.19
0.94
3
-11.00
4.42
3
.....
-10.77
1.58
3
END
4-8

Introduction to Bayesian Analysis and WinBUGS
Then use ’double indexing’ feature of BUGS language
for (i in 1:23) {
THEFT[i] ~ dnorm(mu[i], tau)
mu[i] <- alpha + beta*MAN[i] + delta[DIST[i]]
}
alpha ~ dnorm(0, 0.00001)
beta ~ dnorm(0, 0.00001)
delta[1] <- 0
# set coefficient for reference category to zero
delta[2] ~ dnorm(0, 0.00001)
delta[3] ~ dnorm(0, 0.00001)
tau ~ dgamma(0.001, 0.001)
sigma2 <- 1/tau
In initial values ﬁle, need to specify initial values for delta[2] and delta[3] but
not delta[1]. Use following syntax:
list(alpha = 1, beta = -2, delta = c(NA, -2, 4), tau = 2)
4-9

Introduction to Bayesian Analysis and WinBUGS
Raw data
−20
0
20
40
−10
−5
0
5
10
15
% change in manpower
change in theft rate
1
2
3
−10
−5
0
5
10
15
District
change in theft rate
4-10

Introduction to Bayesian Analysis and WinBUGS
beta chains 1:2 sample: 10000
   -1.0
   -0.5
    0.0
  0.0
  1.0
  2.0
  3.0
  4.0
delta[2] chains 1:2 sample: 10000
  -20.0
  -10.0
    0.0
   10.0
  0.0
0.05
  0.1
0.15
delta[3] chains 1:2 sample: 10000
  -20.0
  -10.0
    0.0
   10.0
  0.0
0.05
  0.1
0.15
Change in theft rate per 1% increase in police manpower
Change in theft rate in Midtown relative to Downtown
Change in theft rate in Uptown relative to Downtown
Posterior mean -0.24
95% interval (-0.47, -0.01)
Posterior mean 0.6
95% interval (-5.1, 6.6)
Posterior mean -4.0
95% interval (-10.0, 2.1)
4-11

Introduction to Bayesian Analysis and WinBUGS
• 95% intervals for DIST eﬀect both include zero
→drop DIST from model (see later for Bayesian model comparison criteria)
beta chains 1:2 sample: 10000
   -1.0
   -0.5
    0.0
    0.0
    1.0
    2.0
    3.0
    4.0
Change in theft rate per 1% increase in police manpower
Posterior mean -0.18
95% interval (-0.39, 0.04)
  -20.0
    0.0
   20.0
  40.0
  -20.0
  -10.0
    0.0
   10.0
   20.0
% change in police manpower
change in theft rate
Observed data
Fitted value, 
mu[i]
95% interval for 
mu[i]
4-12

Introduction to Bayesian Analysis and WinBUGS
• Inﬂuential point corresponds to 20th Precinct
• During 2nd period, manpower assigned to this Precinct was experimentally
increased by about 40%
• No experimental increases in any other Precinct
→Robustify model assuming t-distributed errors
for (i in 1:23) {
THEFT[i] ~ dt(mu[i], tau, 4)
# robust likelihood (t on 4 df)
mu[i] <- alpha + beta*MAN[i]
}
alpha ~ dnorm(0, 0.00001)
beta ~ dnorm(0, 0.00001)
tau ~ dgamma(0.001, 0.001)
sigma2 <- 1/tau
dummy <- DIST[1]
# ensures all variables in data file appear in model code
4-13

Introduction to Bayesian Analysis and WinBUGS
  -20.0
    0.0
   20.0
   40.0
-20.0
-10.0
   0.0
 10.0
 20.0
 
  -20.0
    0.0
   20.0
  40.0
-20.0
-10.0
   0.0
 10.0
 20.0
Posterior mean -0.18
95% interval (-0.39, 0.04)
Posterior mean -0.13
95% interval (-0.36, 0.18)
beta chains 1:2 sample: 10000
   -1.0
   -0.5
    0.0
0.0
1.0
2.0
3.0
4.0
beta chains 1:2 sample: 10000
   -1.0
   -0.5
    0.0
0.0
1.0
2.0
3.0
4.0
Model with Normal errors
Model with Student t errors
4-14

Introduction to Bayesian Analysis and WinBUGS
• Precinct 20 still quite inﬂuential
• Add additional covariate corresponding to a binary indicator for Precinct 20
– Equivalent to ﬁtting separate (saturated) model to Precinct 20
for(i in 1:23) {
THEFT[i] ~ dt(mu[i], tau, 4) # robust likelihood (t on 4 df)
mu[i] <- alpha + beta*MAN[i] + delta*PREC20[i]
# separate term for precinct 20
}
alpha ~ dnorm(0, 0.000001)
beta ~ dnorm(0, 0.000001)
delta ~ dnorm(0, 0.000001)
tau ~ dgamma(0.001, 0.001)
sigma2 <- 1/tau
# residual error variance
dummy <- DIST[1] # ensures all variables in data file appear in model code
# Create indicator variable for precinct 20
# (alternatively, could add this variable to data file)
for(i in 1:13) {
PREC20[i] <- 0
}
PREC20[14] <- 1
for(i in 15:23) {
PREC20[i] <- 0
}
4-15

Introduction to Bayesian Analysis and WinBUGS
model fit: mu
  -20.0
    0.0
   20.0
   40.0
-30.0
-20.0
-10.0
   0.0
 10.0
 20.0
beta chains 1:2 sample: 10000
   -1.0
   -0.5
    0.0
    0.5
  0.0
  1.0
  2.0
  3.0
Posterior mean 0.18
95% interval (-0.16, 0.49)
4-16

Introduction to Bayesian Analysis and WinBUGS
Specifying prior distributions
Why did we choose a N(0, 100000) prior for each regression coeﬃcient and a
Gamma(0.001, 0.001) prior for the inverse of the error variance?
Choice of prior is, in principle, subjective
• it might be elicited from experts (see Spiegelhalter et al (2004), sections 5.2,
5.3)
• it might be more convincing to be based on historical data, e.g. a previous
study
→assumed relevance is still a subjective judgement (see Spiegelhalter at al
(2004), section 5.4)
• there has been a long and complex search for various ‘non-informative’, ‘ref-
erence’ or ‘objective’ priors (Kass and Wasserman, 1996)
4-17

Introduction to Bayesian Analysis and WinBUGS
‘Non-informative’ priors
• Better to refer to as ‘vague’, ‘diﬀuse’ or ‘minimally informative’ priors
• Prior is vague with respect to the likelihood
– prior mass is diﬀusely spread over range of parameter values that are
plausible, i.e. supported by the data (likelihood)
4-18

Introduction to Bayesian Analysis and WinBUGS
Uniform priors (Bayes 1763; Laplace, 1776)
Set
p(θ)
∝
1
• This is improper (R
p(θ)dθ ̸= 1)
• The posterior will still usually be proper
• Inference is based on the likelihood p(x | θ)
• It is not really objective, since a ﬂat prior p(θ) ∝1 on θ does not correspond
to a ﬂat prior on φ = g(θ), but to p(φ) ∝
 dθ
dφ
 where
 dθ
dφ
 is the Jacobian
– Note: Jacobian ensures area under curve (probability) in a speciﬁed inter-
val (θ1, θ2) is preserved under the transformation →same area in interval
(φ1 = g(θ1), φ2 = g(θ2))
4-19

Introduction to Bayesian Analysis and WinBUGS
Proper approximations to Uniform(−∞, ∞) prior:
• p(θ) = Uniform(a, b) where a and b specify an appropriately wide range,
e.g. Uniform(−1000, 1000)
• p(θ) = N(0, V ) where V
is an appropriately large value for the variance,
e.g. N(0, 100000)
• Recall that WinBUGS parameterises Normal in terms of mean and precision,
so vague normal prior will be, e.g.
theta ~ dnorm(0, 0.00001)
4-20

Introduction to Bayesian Analysis and WinBUGS
Jeﬀreys’ invariance priors
Consider 1-to-1 transformation of θ : φ = g(θ), e.g. φ = 1 + θ3
Transformation of variables: p(θ) is equivalent to p(φ) = p(θ = g−1(φ))
 dθ
dφ

Jeﬀreys proposed deﬁning a non-informative prior for θ as p(θ) ∝I(θ)1/2 where
I(θ) is Fisher information for θ
I(θ) = −IEX|θ
∂2 log p(X|θ)
∂θ2

= IEX|θ
"∂log p(X|θ)
∂θ
2#
• Fisher Information measures curvature of log likelihood
• High curvature occurs wherever small changes in parameter values are asso-
ciated with large changes in the likelihood
– Jeﬀreys’ prior gives more weight to these parameter values
– data provide strong information about parameter values in this region
– ensures data dominate prior everywhere
• Jeﬀreys’ prior is invariant to reparameterisation because
I(φ)1/2 = I(θ)1/2

dθ
dφ

4-21

Introduction to Bayesian Analysis and WinBUGS
Examples of Jeﬀreys’ priors
• Normal case: unknown mean m, known variance v
Sample x1, . . . , xn from N(m, v)
log p(x|m) = −
X (xi −m)2
2v
+ C
⇒I(m) = n/v
So Jeﬀreys’ prior for m is ∝1, i.e. the Uniform distribution
• Normal case: known mean m, unknown variance v, with s = P(xi −m)2
log p(x|v) = −n/2 log v −s
2v
⇒I(v) =
n
2v2
So Jeﬀreys’ prior for v is ∝v−1
This improper distribution is approximated by a Gamma(ǫ, ǫ) distribution with
ǫ →0
Note: p(v) ∝v−1 is equivalent to a uniform prior on log v
4-22

Introduction to Bayesian Analysis and WinBUGS
Some recommendations
Distinguish
• primary parameters of interest in which one may want minimal inﬂuence of
priors
• secondary structure used for smoothing etc. in which informative priors may
be more acceptable
Prior best placed on interpretable parameters
Great caution needed in complex models that an apparently innocuous uniform
prior is not introducing substantial information
‘There is no such thing as a ‘noninformative’ prior.
Even improper priors give
information: all possible values are equally likely’ (Fisher, 1996)
4-23

Introduction to Bayesian Analysis and WinBUGS
Location parameters (e.g.
means, regression coeﬃcients)
• Uniform prior on a wide range, or a Normal prior with a large variance can
be used, e.g.
θ ∼Unif(−100, 100)
theta ~ dunif(-100, 100)
θ ∼Normal(0, 100000)
theta ~ dnorm(0, 0.00001)
Prior will be locally uniform over the region supported by the likelihood
– ! remember that WinBUGS parameterises the Normal in terms of mean and
precision so a vague Normal prior will have a small precision
– ! ‘wide’ range and ‘small’ precision depend on the scale of measurement
of θ
4-24

Introduction to Bayesian Analysis and WinBUGS
Scale parameters
• Sample variance σ2: standard ‘reference’ (Jeﬀreys’) prior
p(σ2)
∝
1
σ2 ∝Gamma(0,0)
p(log(σ))
∝
Uniform(−∞, ∞)
• Note that Jeﬀreys’ prior on the inverse variance (precision), τ = σ−2 is also
p(τ) ∝1
τ ∝Gamma(0, 0)
which may be approximated by a ‘just proper’ prior
τ ∼Gamma(ǫ, ǫ)
This is also the conjugate prior and so is widely used as a ‘vague’ proper prior
for the precision of a Normal likelihood
In BUGS language:
tau ~ dgamma(0.001, 0.001)
or alternatively
tau <- 1/exp(logsigma2);
logsigma2 ~ dunif(-100, 100)
Sensitivity analysis plays a crucial role in assessing the impact of particular
prior distributions, whether elicited, derived from evidence, or reference, on the
conclusions of an analysis.
4-25

Introduction to Bayesian Analysis and WinBUGS
Informative priors
• An informative prior expresses speciﬁc, deﬁnite information about a variable
• Example: a prior distribution for the temperature at noon tomorrow
– A reasonable approach is to make the prior a normal distribution with
mean equal to today’s noontime temperature, with variance equal to the
day-to-day variance of atmospheric temperature
• Posterior from one problem (today’s temperature) becomes the prior for an-
other problem (tomorrow’s temperature)
• Priors elicited from experts can be used to take account of domain-speciﬁc
knowledge, judgement, experience
• Priors can also be used to impose constraints on variables (e.g. based on
physical or assumed properties) and bound variables to plausible ranges
4-26

Introduction to Bayesian Analysis and WinBUGS
Example: Trade union density
(Western and Jackman, 1994)
• Example of regression analysis in comparative research
• What explains cross-national variation in union density?
• Union density is deﬁned as the percentage of the work force who belongs to
a trade union
• Competing theories:
– Wallerstein: union density depends on the size of the civilian labour force
(LabF)
– Stephens: union density depends on industrial concentration (IndC)
– Note: These two predictors correlate at -0.92.
4-27

Introduction to Bayesian Analysis and WinBUGS
• Data: n = 20 countries with a continuous history of democracy since World
War II
• Variables: Union density (Uden), (log) labour force size (LabF), industrial
concentration (IndC), left wing government (LeftG), measured in late 1970s
• Fit linear regression model to compare theories
Udeni
∼
N(µi, σ2)
µi
=
b0 + b1(LeftGi −LeftG) + b2(LabFi −LabF) + b3(IndCi −IndC)
Vague priors:
1/σ2
∼
Gamma(0.001, 0.001)
b0
∼
N(0, 100000)
b1
∼
N(0, 100000)
b2
∼
N(0, 100000)
b3
∼
N(0, 100000)
4-28

Introduction to Bayesian Analysis and WinBUGS
Trace plots, posterior estimates and MC error for regression coeﬃcients
b3 chains 1:2
iteration
1001
2000
4000
6000
-50.0
   0.0
  50.0
100.0
b3 chains 1:2
iteration
1001
2000
4000
6000
-50.0
   0.0
 50.0
100.0
1.67
20.6
12.1
b3
0.34
4.18
-4.14
b2
.002
0.08
0.27
b1
5.19
62.8
61.7
b0
MC 
error
sd
mean
0.67
20.2
0.98
b3
0.13
3.96
-6.33
b2
.001
0.08
0.27
b1
0.02
2.48
54.0
b0
MC 
error
sd
mean
Without centering covariates
With centered covariates
4-29

Introduction to Bayesian Analysis and WinBUGS
Posterior distribution of regression coeﬃcients
LeftG
LabF
IndC
[1]
[2]
[3]
box plot: b
-40.0
-20.0
   0.0
 20.0
 40.0
4-30

Introduction to Bayesian Analysis and WinBUGS
Motivation for Bayesian approach with informative priors
• Because of small sample size and multicollinear variables, not able to adjudi-
cate between theories
• Data tend to favour Wallerstein (union density depends on labour force size),
but neither coeﬃcient estimated very precisely
• Other historical data are available that could provide further relevant infor-
mation
• Incorporation of prior information provides additional structure to the data,
which helps to uniquely identify the two coeﬃcients
4-31

Introduction to Bayesian Analysis and WinBUGS
Wallerstein informative prior
• Believes in negative labour force eﬀect
• Comparison of Sweden and Norway in 1950:
– doubling of labour force corresponds to 3.5-4% drop in union density
– on log scale, labour force eﬀect size ≈−3.5/ log(2) ≈−5
• Conﬁdence in direction of eﬀect represented by prior SD giving 95% interval
that excludes 0
b2 ∼N(−5, 2.52)
• Vague prior assumed for IndC eﬀect, b3 ∼N(0, 100000)
4-32

Introduction to Bayesian Analysis and WinBUGS
Stephens informative prior
• Believes in positive industrial concentration eﬀect
• Decline in industrial concentration in UK in 1980s:
– drop of 0.3 in industrial concentration corresponds to about 3% drop in
union density
– industrial concentration eﬀect size ≈3/0.3 = 10
• Conﬁdence in direction of eﬀect represented by prior SD giving 95% interval
that excludes 0
b3 ∼N(10, 52)
• Vague prior assumed for IndC eﬀect, b3 ∼N(0, 100000)
4-33

Introduction to Bayesian Analysis and WinBUGS
Both Wallerstein and Stephens priors
• Both believe left-wing governments assist union growth
• Assuming 1 year of left-wing government increases union density by about
1% translates to eﬀect size of 0.3
• Conﬁdence in direction of eﬀect represented by prior SD giving 95% interval
that excludes 0
b1 ∼N(0.3, 0.152)
• Vague prior b0 ∼N(0, 100000) assumed for intercept
4-34

Introduction to Bayesian Analysis and WinBUGS
Posterior distribution of regression coeﬃcients under diﬀerent priors
box plot: b2
  -15.0
  -10.0
   -5.0
    0.0
    5.0
box plot: b3
  -50.0
    0.0
   50.0
Effect of Labour Force Size 
(Wallerstein hypothesis) 
Effect of Industrial Concentration
(Stephens hypothesis) 
b2 (LabF):
Vague       Info       Vague
b3 (IndC):
Vague      Vague
Info
b2 (LabF):
Vague       Info       Vague
b3 (IndC):   Vague      Vague
Info
4-35

Introduction to Bayesian Analysis and WinBUGS
Comments
• Eﬀects of LabF and IndC estimated more precisely
• Both sets of prior beliefs support inference that labour-force size decreases
union density
• Only Stephens prior supports conclusion that industrial concentration in-
creases union density
• Choice of prior is subjective – if no consensus, can we be satisﬁed that data
have been interpreted fairly?
– Sensitivity to priors (e.g.
repeat analysis using priors with increasing
variance) — see Practical exercises
– Sensitivity to data (e.g.
residuals, inﬂuence diagnostics) — see later
lecture
4-36

Introduction to Bayesian Analysis and WinBUGS
Multivariate responses
• In many applications, it is common to collect data on a number of diﬀerent
outcomes measured on the same units, e.g.
– sample survey, where respondents asked several diﬀerent questions
– experiment with several diﬀerent outcomes measured on each unit
• May wish to ﬁt regression model to each response
– May have diﬀerent covariates in each regression
– But errors may be correlated
– Might also wish to impose cross-equation parameter restrictions
→Seemingly Unrelated Regressions (SUR) (Zellner, 1962)
Bayesian approach to SUR models →model vector of response for each unit as
multivariate normal (could also have robust version using multivariate t)
Possible to extend Bayesian SUR models to binary, categorical, count responses
using multivariate latent variable approach.
4-37

Introduction to Bayesian Analysis and WinBUGS
Example: Analysis of compositional data
• Compositional data are vectors of proportions pi = (pi1, ..., piJ) representing
relative contributions of each of J categories to the whole, e.g.
– Proportion of income spent on diﬀerent categories of expenditure
– Proportion of electorate voting for diﬀerent political parties
– Relative abundance of diﬀerent species in a habitat
– Chemical composition of a rock or soil sample
– Proportion of deaths from diﬀerent causes in a population
• Regression models for compositional data must satisfy two constraints
0 ≤pij ≤1
X
j
pij = 1
4-38

Introduction to Bayesian Analysis and WinBUGS
• Two main modelling strategies, treating vector of proportions as the data
(suﬃcient statistics)
– Model pi using a Dirichlet likelihood (multivariate generalisation of a beta
distribution)
∗assumes the ratios of “compositions” (i.e. proportions) are independent
– Multivariate logistic normal model (Aitchison, 1986) — apply additive log
ratio (alr) transformation, yij = log(pij/piJ) and model yi as multivariate
normal
∗allows dependence between ratios of proportions
∗this can be thought of as a type of SUR model
• To allow for sampling variability in the observed counts (including zero counts),
model counts (rather than proportions) as multinomial (see later)
4-39

Introduction to Bayesian Analysis and WinBUGS
Multivariate logistic normal model
Deﬁne
yij = log(pij/piJ)
the log ratios of proportions in each category relative to a reference category J
Note that piJ = 1 −P
j̸=J pij, so
pij =
exp yij
1 + P
j̸=J exp yij
Since yij are unconstrained, can model vector yi = {yij, j ̸= J} as multivariate
normal
4-40

Introduction to Bayesian Analysis and WinBUGS
Example: British General Election 1992
• Data originally analysed by Katz and King (1999), and formulated as BUGS
example by Simon Jackman
• Data consist of vote proportions for Conservative (j=1), Labour (j=2) and
Lib-Dem (j=3) parties from 1992 General Election for each of 521 con-
stituencies
• Additive log ratio transformation applied to proportions, taking Lib-Dem vote
as reference category
• Covariates include lagged values of the log ratios from previous election, and
indicators of the incumbency status of each party’s candidate
4-41

Introduction to Bayesian Analysis and WinBUGS
BUGS model code
for(i in 1:521){
y[i,1:2] ~ dmnorm(mu[i,], prec[ , ])
for(j in 1:2){
mu[i,j] <- beta[j,1]*x[i,1] + beta[j,2]*x[i,2] + beta[j,3]*x[i,3] +
beta[j,4]*x[i,4] + beta[j,5]*x[i,5] + beta[j,6]*x[i,6]
}
}
## priors for elements of precision matrix
prec[1:2,1:2] ~ dwish(R[,],k)
R[1,1] <- .01; R[1,2] <- 0; R[2,1] <- 0; R[2,2] <- .01;
k <- 2
# convert precision to covariance matrix
sigma[1:2,1:2] <- inverse(prec[ , ])
rho <- sigma[1,2]/sqrt(sigma[1,1]*sigma[2,2])
# correlation
## Priors for regression coefficients
for(j in 1:2){
for(k in 1:6) {
beta[j,k] ~ dnorm(0, 0.000001)
}
}
4-42

Introduction to Bayesian Analysis and WinBUGS
Priors on precision matrix of multivariate normal
The multivariate generalisation of the Gamma (or χ2) distribution is the Wishart
distribution, which arises in classical statistics as the distribution of the sum-of-
squares-and-products matrix in multivariate normal sampling.
The Wishart distribution Wp(k, R) for a symmetric positive deﬁnite p × p matrix
Ωhas joint density function proportional to
|R|k/2|Ω|(k−p−1)/2 exp (−(1/2)tr(RΩ))
in terms of two parameters:
a real scalar k > p −1 and a symmetric positive
deﬁnite matrix R. The expectation of this distribution is
E[Ω] = kR−1
When the dimension p is 1, i.e.
reverting back to univariate case, it is easy to
show that the Wishart distribution becomes the more familiar:
W1(k, R) ≡Gamma(k/2, R/2) ≡(χ2
k)/R
If we use the Wishart distrubution as a prior distribution for a precision matrix Ω
in sampling from Np(µ, Ω−1), we ﬁnd, generalising the univariate case above, that
we get the same form for the posterior for Ω– another Wishart distribution.
In view of the result above for the expectation of the Wishart distribution, we
usually set (1/k)R to be a prior guess at the unknown true variance matrix. A
common choice is to take k = p.
4-43

Introduction to Bayesian Analysis and WinBUGS
Note
• In BUGS language, you must specify the dimension of vectors or arrays on
the left hand side of multivariate distributions
• In above example, each row (observation) of the 521 × 2 matrix y is a vector
of length 2, hence
y[i,1:2] ~ dmnorm.....
• Likewise, prec is a 2 × 2 matrix, hence
prec[1:2, 1:2] ~ dwish.....
• You cannot specify the dimension to be a parameter (even if the value of
the parameter is speciﬁed elsewhere in the code or data ﬁle), e.g.
J <- 2
prec[1:J, 1:J] ~ dwish(R[,], k)
will give an error at compilation
• You do not need to specify the dimension of vectors or arrays on the right
hand side of distribution statements, e.g. dimension of R[,] is not speciﬁed
above (the dimension is implicit from dimension of left hand side)
4-44

Introduction to Bayesian Analysis and WinBUGS
Interpretation of model parameters
• Interpretation of parameter estimates on a multivariate log-odds scale is dif-
ﬁcult
• Using the inverse alr transformation, easy to recover estimates of expected
proportions or predicted counts in diﬀerent categories
• Eﬀect of covariates can be examined by calculating diﬀerence or ratio of
expected proportions for diﬀerent values of the covariate
– Using MCMC, easy to obtain uncertainty intervals for such contrasts
• Example:
eﬀect of incumbency on expected proportion of votes for each
party
– For party j, calculate expected alr-transformed proportion for two values
of incumbency: (1) party j’s candidate is incumbent; (2) open seat (no
candidate is incumbent)
– Hold values of all other covariates constant, e.g. at their means
– Use inverse alr transformation to obtain expected proportions under each
incumbency value, and take diﬀerence
4-45

Introduction to Bayesian Analysis and WinBUGS
BUGS code for calculating incumbency eﬀects
for(j in 1:2){
# value of mu with Conservative incumbent and average values of other variables
mu.con[j] <- beta[j,1]*mean(x[,1]) + beta[j,2]*mean(x[,2]) +
beta[j,3]*mean(x[,3]) + beta[j,4]*1
# value of mu with Labour incumbent and average values of other variables
mu.lab[j] <- beta[j,1]*mean(x[,1]) + beta[j,2]*mean(x[,2]) +
beta[j,3]*mean(x[,3]) + beta[j,5]*1
# value of mu with open seat and average values of other variables
mu.open[j] <- beta[j,1]*mean(x[,1]) + beta[j,2]*mean(x[,2]) + beta[j,3]*mean(x[,3])
# expected proportions
exp.mu.con[j] <- exp(mu.con[j]); p.con[j] <- exp.mu.con[j]/(1 + sum(exp.mu.con[]))
exp.mu.lab[j] <- exp(mu.lab[j]); p.lab[j] <- exp.mu.lab[j]/(1 + sum(exp.mu.lab[]))
exp.mu.open[j] <- exp(mu.open[j]); p.open[j] <- exp.mu.open[j]/(1 + sum(exp.mu.open[]))
}
# difference in expected proportions due to incumbency
incumbency.con <- p.con[1] - p.open[1]
incumbency.lab <- p.lab[2] - p.open[2]
4-46

Introduction to Bayesian Analysis and WinBUGS
Results
Posterior mean
95% CI
Expected vote (Cons), area 1
44.8%
(44.2%, 45.4%)
Expected vote (Lab), area 1
46.1%
(45.3%, 46.8%)
Expected vote (LibDem), area 1
9.1%
(8.7%, 9.5%)
incumbency advantage (Cons)
−0.06%
(−0.75%, 0.70%)
incumbency advantage (Lab)
−0.30%
(−1.6%, 1.0%)
incumbency advantage (LibDem)
8.6%
(5.1%, 12.3%)
ρ (correlation between log ratio
0.87
(0.85, 0.89)
for Cons:LibDem and Lab:LibDem)
Note: results for incumbency advantage agree with those in Tomz et al (2002)
but not with Katz and King, who analysed data from 10 consecutive elections
and used empirical Bayes shrinkage priors on the β coeﬃcients across years
4-47

Introduction to Bayesian Analysis and WinBUGS
Multivariate t likelihood
• Multivariate logistic normal for compositional data relies on assumption that
the log ratios are approximately multivariate normal
• Katz and King (1999) argue that this assumption is not appropriate for British
election data
– majority of constituencies tend to be more clustered, and a minority more
widely dispersed, than the multivariate normal implies
• K&K propose replacing multivariate normal by a heavier-tailed multivariate
student t distribution
• Multivariate t has 3 parameters:
p-dimensional mean vector, p × p inverse
scale (precision) matrix and a scalar degrees of freedom parameter
• A Wishart prior can be used for the inverse scale matrix
• Degrees of freedom parameter can either be ﬁxed, or assigned a prior distri-
bution
• Note: as degrees of freedom →∞, t →Normal
4-48

Introduction to Bayesian Analysis and WinBUGS
BUGS code for multivariate t likelihood
• Only need to change 2 lines of code
1. Likelihood:
##
y[i,1:2] ~ dmnorm(mu[i,1:2], prec[ , ])
y[i,1:2] ~ dmt(mu[i,1:2], prec[ , ], nu)
2. Specify either ﬁxed value for degrees of freedom, nu, or a suitable prior
## nu <- 4
nu ~ dunif(2, 250)
4-49

Introduction to Bayesian Analysis and WinBUGS
Results
Normal
Student t
Expected vote (Cons), area 1
44.8% (44.2%, 45.4%)
44.7% (44.2%, 45.2%)
Expected vote (Lab), area 1
46.1% (45.3%, 46.8%)
46.4% (45.7%, 47.0%)
Expected vote (LibDem), area 1
9.1% (8.7%, 9.5%)
8.8% (8.6%, 9.3%)
incumbency advantage (Cons)
−0.06% (−0.75%, 0.70%) 0.00% (-0.65%, 0.60%)
incumbency advantage (Lab)
−0.30% (−1.6%, 1.0%)
−0.50% (−1.7%, 0.70%)
incumbency advantage (LibDem)
8.6% (5.1%, 12.3%)
2.3% (−2.0%, 8.3%)
ρ
0.87 (0.85, 0.89)
0.87 (0.85, 0.90)
ν
–
4.5 (3.4, 5.9)
DIC
−480
−1220
pD
14.5
11.8
4-50

Lecture 5.
Further Bayesian regression models
5-1

Introduction to Bayesian Analysis and WinBUGS
Generalised Linear regression Models
• Speciﬁcation of Bayesian GLMs follows straightforwardly from previous dis-
cussion of linear models
• No closed form solution available, but straightforward to obtain samples from
posterior using MCMC
Example: Beetles
Dobson (1983) analyses binary dose-response data from a bioassay experiment in
which the numbers of beetles killed after 5 hour exposure to carbon disulphide at
N=8 diﬀerent concentrations are recorded.
•
•
•
•
•
•
•
•
dose, x
mortality rate, y/n
1.70
1.75
1.80
1.85
0.2
0.6
1.0
5-2

Introduction to Bayesian Analysis and WinBUGS
We start by ﬁtting a logistic regression model
yi
∼
Binomial(pi, ni)
logitpi
=
α + β(xi −x)
α
∼
Normal(0, 10000)
β
∼
Normal(0, 10000)
Beetles: logistic regression model ﬁt (red = posterior mean of pi; blue =
95% interval; black dots = observed rate yi/ni)
model fit: p
   1.65
    1.7
   1.75
    1.8
   1.85
    1.9
0.0
.25
0.5
.75
1.0
dose
obs. rate
posterior
95%
level i
yi/ni
mean of pi
interval
1
0.10
0.06
(0.03, 0.09)
2
0.22
0.16
(0.11, 0.22)
3
0.29
0.36
(0.29, 0.43)
4
0.50
0.61
(0.54, 0.67)
5
0.83
0.80
(0.74, 0.85)
6
0.90
0.90
(0.86, 0.94)
7
0.98
0.96
(0.93, 0.97)
8
1.00
0.98
(0.96, 0.99)
5-3

Introduction to Bayesian Analysis and WinBUGS
Some evidence of lack of ﬁt at extremes, so try alternative complementary log-log
link function
yi
∼
Binomial(pi, ni)
cloglogpi
=
α + β(xi −x)
α
∼
Normal(0, 10000)
β
∼
Normal(0, 10000)
Beetles: cloglog regression model ﬁt (red = posterior mean of pi; blue =
95% interval; black dots = observed rate yi/ni)
model fit: p
   1.65
    1.7
   1.75
    1.8
   1.85
    1.9
0.0
.25
0.5
.75
1.0
dose
obs. rate
posterior
95%
level i
yi/ni
mean of pi
interval
1
0.10
0.09
(0.06, 0.14)
2
0.22
0.19
(0.14, 0.24)
3
0.29
0.34
(0.28, 0.40)
4
0.50
0.54
(0.48, 0.60)
5
0.83
0.76
(0.70, 0.81)
6
0.90
0.92
(0.87, 0.95)
7
0.98
0.98
(0.96, 0.99)
8
1.00
1.00
(0.99, 1.00)
5-4

Introduction to Bayesian Analysis and WinBUGS
Can write probit model in two diﬀerent ways
probitpi
=
α + β(xi −x)
or
pi
=
Φ(α + β(xi −x))
In WinBUGS , either
probit(p[i]) <- alpha + beta*(x[i]-mean(x[]))
or
p[i] <- phi(alpha + beta*(x[i]-mean(x[])))
The second way is slower, but can be more robust to numerical problems.
5-5

Introduction to Bayesian Analysis and WinBUGS
Note the importance of centering the covariate (dose) in this example to reduce
correlations between the parameters
History plot for slope, ββββ: Centred covariate
History plot for slope, ββββ: Uncentred covariate
Bivariate scatter plot showing correlation between sampled values of αααα and ββββ
Centered covariate
Uncentred covariate
beta chains 1:2
iteration
1
250
500
750
1000
   20.0
   30.0
   40.0
   50.0
   60.0
beta chains 1:2
iteration
1
500
1000
 -100.0
    0.0
  100.0
beta
   25.0
   30.0
   35.0
   40.0
   45.0
   50.0
beta
 -100.0
    0.0
  100.0
5-6

Introduction to Bayesian Analysis and WinBUGS
Non linear regression models
Example: Dugongs
Carlin and Gelfand (1991) consider data on length (yi) and age (xi) measurements
for 27 dugongs (sea cows) captured oﬀthe coast of Queensland
•••
•
•
•
•
•
•
••
•••
•
•
••
•
•
•
••
•
•
•
age, x
length, y
0
5
10
15
20
25
30
1.8
2.2
2.6
5-7

Introduction to Bayesian Analysis and WinBUGS
A frequently used nonlinear growth curve with no inﬂection point and an asymp-
tote as xi tends to inﬁnity is
yi
∼
Normal(µi, σ2)
µi
=
α −βγxi
where α, β > 0 and γ ∈(0, 1)
Vague prior distributions with suitable constraints may be speciﬁed as e.g.
α
∼
Uniform(0, 100)
β
∼
Uniform(0, 100)
γ
∼
Uniform(0, 1)
Alternatively, vague Normal priors with appropriate bounds could be speciﬁed for
α and β, e.g.
α
∼
Normal(0, 10000)I(0, )
β
∼
Uniform(0, 10000)I(0, )
For the sampling variance, could specify uniform prior log variance or log sd scale
log σ ∼Uniform(−10, 10)
or gamma prior on precision scale
1/σ2 ∼Gamma(0.001, 0.001)
5-8

Introduction to Bayesian Analysis and WinBUGS
Dugongs: model ﬁt (red = posterior mean of µi; blue = 95% interval)
model fit: mu
    0.0
   10.0
   20.0
   30.0
1.5
2.0
2.5
3.0
5-9

Introduction to Bayesian Analysis and WinBUGS
Categorical data
1. Data recorded as yi = 0 or 1
y[i] ~ dbern(p[i])
probit(p[i]) <- beta0 + beta1*x1[i] .....
NB probit can be rather fragile:
p[i] <- phi(beta0 + beta1*x1[i] .....)
is slower but may be more robust.
2. A single categorical variable yi= 1,2,3
y[i] ~ dcat(p[])
where p[] is an array of probabilities
5-10

Introduction to Bayesian Analysis and WinBUGS
Modelling unknown denominators
Suppose we are told that a fair coin has come up heads 10 times - how many
times (n) has it been tossed?
We want to specify a uniform prior distribution for n
1. Could give a continuous prior distribution for n and use ’round’ function
model {
r <- 10
q <- 0.5
r ~ dbin(q, n)
n.cont ~ dunif(1, 100)
n <- round(n.cont)
}
BUGS output:
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
n
21.08
4.794
0.07906
13.0
21.0
32.0
1001
5000
n.cont
21.08
4.804
0.07932
13.31
20.6
32.0
1001
5000
We can be 95% sure that the coin has been tossed between 13 and 32 times
5-11

Introduction to Bayesian Analysis and WinBUGS
2. Or a discrete uniform prior on 10 to 100
model {
r <- 10
q <- 0.5
r ~ dbin(q, n)
# discrete prior on 10 to 100
for(j in 1:9)
{
p[j]<-0
}
for(j in 10:100){
p[j]<-1/91}
n ~ dcat(p[])
}
BUGS output:
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
n
21.07
4.761
0.03929
13.0
21.0
32.0
1001
10000
We obtain a similar answer to before, i.e. we can be 95% sure that the coin has
been tossed between 13 and 32 times
5-12

Introduction to Bayesian Analysis and WinBUGS
Multinomial data
Suppose observed data are arrays of counts in K categories,
e.g. y = (1,2,4), (3,3,3) etc
for(i in 1:N) {
y[i,1:3] ~ dmulti(q[], n[i])
n[i] <- sum(y[i,])
}
If have no covariates, can use Dirichlet prior for q[], so that p(q) ∝qαk
k
q[1:3] ~ ddirch(alpha[])
α needs to be ﬁxed (i.e. can’t learn about α), e.g.
for(k in 1:3) {
alpha[k]<-1
}
gives uniform prior on q
Remember that you need to specify the dimension of vectors or arrays on the left
hand side of distributions in the BUGS languange
5-13

Introduction to Bayesian Analysis and WinBUGS
If have covariates, can use multinomial-logistic model
ηik = log qik
qi1
= αk +
X
p
βpkxpi,
k = 2, ..., K; i = 1, ..., N
• Conceptually, this is equivalent to K −1 logistic regressions comparing cate-
gory k > 1 with category 1
• cf log ratio model for compositional data — here we are modelling the ob-
served counts in each category (rather than the observed log ratios), and
treating the underlying probabilities as unknown parameters
• Can also re-write∗model in terms of the original probabilities rather than the
log ratios (log odds):
qik =
φik
P
k φik
where φik = eηik = eαk+P
p βpkxpi
with constraint that φi1 = 1 (i.e. ηi1 = 0)
*To verify this result exponentiate the ﬁrst equation for the log ratio to obtain qik = qi1eηik, and
note that the convention ηi1 = 0 makes this formula valid for all k. Next sum over k and use the
fact that P
k qik = 1 to obtain qi1 = 1/P
k eηik.
5-14

Introduction to Bayesian Analysis and WinBUGS
Example: Car choice
Foster et al (1998) present data on choice of car (family, sporty or work) for 263
customers by gender, age and marital status.
Age
Sex Marital
Car type (y)
Family Sporty Work
< 25
F
Married
6
6
3
25 −35 F
Married
32
8
8
> 35
F
Married
13
2
4
< 25
M
Married
5
4
1
25 −35 M
Married
37
12
11
> 35
M
Married
13
2
3
< 25
F
Single
4
11
0
25 −35 F
Single
9
5
2
> 35
F
Single
4
0
2
< 25
M
Single
4
11
3
25 −35 M
Single
10
14
6
> 35
M
Single
3
4
1
Here we will view car type as the response and age, sex and marital status as
predictors
5-15

Introduction to Bayesian Analysis and WinBUGS
BUGS code
for(i in 1:12){
y[i,1:3] ~ dmulti(q[i,], n[i])
n[i] <- sum(y[i,])
for(k in 1:3) {
q[i,k] <- phi[i,k] / sum(phi[i,])
log(phi[i,k]) <- alpha[k] + beta[k, age[i]] + delta[k]*sex[i] + gamma[k]*marital[i]
}
}
# constraints
alpha[1]<-0; beta[1,1]<-0; beta[1,2]<-0; beta[1,3]<-0; delta[1]<-0; gamma[1]<-0
beta[2,1] <- 0; beta[3,1] <- 0
# further constraints (baseline age categories)
# priors
for(k in 2:3) {
alpha[k] ~ dnorm(0, 0.0001)
beta[k, 2] ~ dnorm(0, 0.0001);
beta[k, 3] ~ dnorm(0, 0.0001)
delta[k] ~ dnorm(0, 0.0001)
gamma[k] ~ dnorm(0, 0.0001)
# odds ratios of choosing car type k (2=sporty, 3=work) versus type 1 (family)
OR.age25[k] <- exp(beta[k,2])
# odds ratio for age 25-35 vs <25
OR.age35[k] <- exp(beta[k,3])
# odds ratio for age >35 vs <25
OR.male[k] <- exp(delta[k])
# odds ratio for males vs females
OR.single[k] <- exp(gamma[k])
# odds ratio for single vs married
}
5-16

Introduction to Bayesian Analysis and WinBUGS
Data
age[]
sex[]
marital[]
y[,1]
y[,2]
y[,3]
1
1
1
6
6
3
2
1
1
32
8
8
3
1
1
13
2
4
1
2
1
5
4
1
2
2
1
37
12
11
3
2
1
13
2
3
1
1
2
4
11
0
2
1
2
9
5
2
3
1
2
4
0
2
1
2
2
4
11
3
2
2
2
10
14
6
3
2
2
3
4
1
END
Initial values
list(alpha = c(NA, 1, 1), delta = c(NA, 1, 1),
gamma = c(NA, 1, 1),
beta = structure(.Data = c(NA, NA, NA,
NA, 1, 1,
NA, 1, 1), .Dim=c(3,3))
)
Note syntax for arrays (beta) — data read row by row, with .Dim statement
specifying number of rows and number of columns.
5-17

Introduction to Bayesian Analysis and WinBUGS
Results
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
OR.age25[2]
0.3343
0.1262
0.00313
0.1513
0.3135
0.633
4001
52000
OR.age25[3]
1.036
0.6156
0.01894
0.3354
0.8885
2.603
4001
52000
OR.age35[2]
0.1905
0.09994 0.00190
0.0589
0.1695
0.440
4001
52000
OR.age35[3]
1.06
0.7099
0.01921
0.2781
0.8796
2.874
4001
52000
OR.male[2]
1.525
0.4869
0.01719
0.7882
1.457
2.681
4001
52000
OR.male[3]
1.324
0.482
0.01787
0.624
1.237
2.482
4001
52000
OR.single[2] 3.529
1.143
0.04155
1.809
3.354
6.247
4001
52000
OR.single[3] 1.502
0.6006
0.02165
0.6331
1.402
2.973
4001
52000
• So, for example, the odds of buying a sporty rather than family car is over
3.5 times higher for single customers compared to married customers (95%
CI 1.8, 6.2) (OR.single[2])
• May be more eﬃcient to ﬁt this using Poisson distributions: see alli example
in manual
• Can also extend to ordered categorical data (see Bones and inhalers examples
in manual)
5-18

Introduction to Bayesian Analysis and WinBUGS
Original data were in subject-speciﬁc format with age as a continuous variable:
age[] sex[] marital[] y[]
34
2
1
1
36
2
2
2
23
2
1
1
....
To ﬁt model to individual level data, use categorical rather than multinomial
likelihood
5-19

Introduction to Bayesian Analysis and WinBUGS
for(i in 1:263){
y[i] ~ dcat(q[i,])
for(k in 1:3) {
q[i,k] <- phi[i,k] / sum(phi[i,])
log(phi[i,k]) <- alpha + beta[k]*age[i] + delta[k]*sex[i] + gamma[k]*marital[i]
}
}
# constraints
alpha[1]<-0; beta[1]<-0; delta[1]<-0; gamma[1]<-0
# priors
for(k in 2:3) {
alpha[k] ~ dnorm(0, 0.0001)
beta[k] ~ dnorm(0, 0.0001)
delta[k] ~ dnorm(0, 0.0001)
gamma[k] ~ dnorm(0, 0.0001)
# odds ratios of choosing car type k (2=sporty, 3=work) versus type 1 (family)
OR.age[k] <- exp(beta[k])
# odds ratio per year
OR.male[k] <- exp(delta[k])
# odds ratio for males vs females
OR.single[k] <- exp(gamma[k])
# odds ratio for single vs married
}
5-20

Introduction to Bayesian Analysis and WinBUGS
Results
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
OR.age[2]
0.8818
0.0267
4.753E-4
0.8292
0.8822
0.9326
4001
22500
OR.age[3]
0.9701
0.0293
4.388E-4
0.9112
0.9707
1.027
4001
22500
OR.male[2]
1.404
0.4324
0.02191
0.7466
1.333
2.375
4001
22500
OR.male[3]
1.32
0.5035
0.0258
0.6219
1.234
2.573
4001
22500
OR.single[2]
3.802
1.256
0.06471
1.986
3.573
6.814
4001
22500
OR.single[3]
1.532
0.5835
0.02781
0.7131
1.435
3.008
4001
22500
• Results for OR.single and OR.male very similar to previous model
• Odds ratios for age indicate
– signiﬁcant reduction in odds of buying sporty versus family car as age
increases
– slight but non-signiﬁcant reduction in odds of buying work versus family
car as age increases
5-21

Lecture 6.
Predictions, missing data, model checking,
model comparison
6-1

Introduction to Bayesian Analysis and WinBUGS
Making predictions
• Important to be able to predict unobserved quantities for
– ‘ﬁlling-in’ missing or censored data
– model checking - are predictions ‘similar’ to observed data?
– making predictions!
• Easy in MCMC/WinBUGS; just specify a stochastic node without a data-
value - it will be automatically predicted
• Provides automatic imputation of missing data
• Easiest case is where there is no data at all: just ‘forward sampling’ from
prior, Monte Carlo methods
6-2

Introduction to Bayesian Analysis and WinBUGS
Example: Dugongs — prediction
Suppose we want to project beyond current observations, eg at ages 35 and 40
Could explicitly set up predictions
for (i in 1:N){
y[i]
~
dnorm( mu[i], inv.sigma2 )
mu[i] <-
alpha - beta * pow(gamma, x[i])
}
mu35
<-
alpha - beta * pow(gamma, 35)
mu40
<-
alpha - beta * pow(gamma, 40)
y35
~
dnorm( mu35, inv.sigma2 )
y40
~
dnorm( mu40, inv.sigma2 )
Interval around µ40 will reﬂect uncertainty concerning ﬁtted parameters
Interval around y40 will additionally reﬂect sampling error σ and uncertainty about
σ
6-3

Introduction to Bayesian Analysis and WinBUGS
Dugongs: prediction as missing data
Easier to set up as missing data - WinBUGS automatically predicts it
list(x = c( 1.0,
1.5,
1.5,
1.5, 2.5,
4.0,
5.0,
5.0,
7.0,
8.0,
8.5,
9.0,
9.5, 9.5,
10.0, 12.0, 12.0, 13.0,
13.0, 14.5, 15.5, 15.5, 16.5, 17.0, 22.5, 29.0, 31.5, 35, 40),
Y = c(1.80, 1.85, 1.87, 1.77, 2.02, 2.27, 2.15, 2.26, 2.47,
2.19, 2.26, 2.40, 2.39, 2.41, 2.50, 2.32, 2.32, 2.43,
2.47, 2.56, 2.65, 2.47, 2.64, 2.56, 2.70, NA, NA), N = 29)
node
mean
sd
MC error
2.5%
median
97.5%
start
sample
mu[28]
2.651
0.07189 0.00423
2.533
2.642
2.815
1001
10000
Y[28]
2.651
0.1228
0.004537
2.415
2.648
2.902
1 001
10000
mu[29]
2.655
0.07825 0.004772
2.533
2.644
2.837
1001
10000
Y[29]
2.653
0.1275
0.005026
2.413
2.649
2.921
1001
10000
6-4

Introduction to Bayesian Analysis and WinBUGS
Dugongs: projections
6-5

Introduction to Bayesian Analysis and WinBUGS
Dugongs: prediction as model checking
y.pred[i]
~
dnorm( mu[i], inv.sigma2 )
model fit: Y.pred
    0.0
   10.0
   20.0
   30.0
    1.5
    2.0
    2.5
    3.0
6-6

Introduction to Bayesian Analysis and WinBUGS
Criticism of non-hierarchical models
‘Standard’ checks based on ﬁtted model, such as
• residuals: plot versus covariates, checks for auto-correlations and so on
• prediction: check accuracy on external validation set, or cross validation
• etc...
All this applies in Bayesian modelling, but in addition:
• parameters have distributions and so residuals are variables
• should check for conﬂict between prior and data
• should check for unintended sensitivity to the prior
• using MCMC, have ability to generate replicate parameters and data.
6-7

Introduction to Bayesian Analysis and WinBUGS
Residuals in non-hierarchical models
• Standardised Pearson residuals
(y −θ)/σ
where θ = E[Y ], σ2 = V[Y ]
• In Bayesian analysis these are random quantities, with distributions
• If assuming Normality, then
P(Y ) = Φ[(Y −θ)/σ]
has a Uniform[0,1] distribution under true θ and σ
6-8

Introduction to Bayesian Analysis and WinBUGS
Example: Dugongs — non-linear models
yi
∼
N[α −βγxi, σ2]
for (i in 1:N){
y[i]
~
dnorm( mu[i], inv.sigma2 )
mu[i]
<-
alpha - beta * pow(gamma, x[i])
res[i] <- (y[i] - mu[i])/sigma
p.res[i] <- phi(res[i])
}
alpha
~ dunif(0,100)
beta
~ dunif(0,100)
gamma
~ dunif(0, 1)
log.sigma
~ dunif(-10,10)
#
uniform prior on log(standard deviation)
log(sigma) <- log.sigma
inv.sigma2 <- 1/(sigma*sigma) # inv.sigma2 is precision
• Can monitor standardised residuals, res, and their P-values, p.res
6-9

Introduction to Bayesian Analysis and WinBUGS
Dugongs: residuals
[1]
[2][3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12][13][14]
[15]
[16][17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25][26]
[27]
box plot: res
-4.0
-2.0
 0.0
 2.0
 4.0
[27]
[10]
[17]
[16]
[11]
[4]
[7]
[18]
[22]
[13]
[2]
[14]
[19]
[12]
[3]
[5]
[1]
[8]
[24]
[20]
[15]
[6]
[9]
[23]
[21]
[25]
[26]
caterpillar plot: P.res
6-10

Introduction to Bayesian Analysis and WinBUGS
Comments on residuals
• Could use X2 = Σir2
i as overall measure of residual variation
• Gelman et al suggest plotting single draw in Q-Q plot.
• Can also calculate deviance residuals.
• Note: not independent, so best used informally
• Multivariate version: Mahalanobis distance
Mi = (yi −IE(yi))′ Var(yi)−1 (yi −IE(yi))
• If want single value for testing distributional shape, could plug-in posterior
means, or use residual posterior means, but better to use approximate pre-
dictive residuals calculated outside BUGS:
ri = yi −E(Y pred
i
)
q
V(Y pred
i
)
Multivariate version is
Mi =

yi −IE(ypred
i
)
′
Var(ypred
i
)−1 
yi −IE(ypred
i
)

6-11

Introduction to Bayesian Analysis and WinBUGS
Cross-validation
• Compare observed data (or function of data) with predicted values under the
H0 model
• Assessing conﬂict between the observed data and their predictive distribution.
• Split data into yf used to ﬁt the model; yc for criticism.
c
obs
c
obs
T(y       )
T(y        )
c
pred
y
yc
pred
yf
θ
• T(yc) termed discrepancy statistic by Gelman et al (1995a,b) such that it has
extreme value if data conﬂict with the model
• common choice is just T(yci) = yci to check for individual outliers
6-12

Introduction to Bayesian Analysis and WinBUGS
Bayesian p-values
P(T
p(T(y    ))
(y    )<T(y   ))
obs
pred
pred
obs
Reference dist.
Bayesian p-value
measure
Observed discrepancy
T(y   )
• (1-sided) probability that predicted data could be more extreme than ob-
served, as measured by the discrepancy statistic T(yc)
• Has Unif[0,1] distribution under H0
• Suﬀers from usual problems of multiple testing
6-13

Introduction to Bayesian Analysis and WinBUGS
Comments on Cross-validation
• suﬀers from usual problems of masking
• does not target speciﬁc model assumptions
• tiresome to implement using MCMC
Alternatives to cross-validation
• importance resampling
• approximate p(Y pred
i
|y\i) by p(Y pred
i
|y)
– just replicate data (‘posterior predictive P-values’)
– replicate parameters and data (‘mixed predictive P-values’)
• can end up with many P-values - use ideas of False Discovery Rates
6-14

Introduction to Bayesian Analysis and WinBUGS
Model comparison
What is the ‘deviance’ ?
• For a likelihood p(y|θ), we deﬁne the deviance as
D(θ) = −2 log p(y|θ)
(1)
• In WinBUGS the quantity deviance is automatically calculated, where θ are
the parameters that appear in the stated sampling distribution of y
• The full normalising constants for p(y|θ) are included in deviance
• e.g. for Binomial data
y[i] ~ dbin(theta[i],n[i]), the deviance is
−2
X
i

yi log θi + (ni −yi) log(1 −θi) + log

ni
ri




6-15

Introduction to Bayesian Analysis and WinBUGS
What is the ‘standardised deviance’ ?
• In generalised linear models the saturated deviance is (loosely) deﬁned as
D(y) - the deviance with the observations substituted for their expectations
• We deﬁne the standardised deviance as D(θ) −D(y)
• e.g. for Binomial data, Bayesian standardised deviance is
−2
X
i

yi log
θi
yi/ni
+ (ni −yi) log
(1 −θi)
(1 −yi/ni)

• Just sum of deviance residuals
• This is a random quantity with a posterior distribution
• If model ﬁts the data, expected to have χ2
I distribution, where I is the di-
mensionality of θ.
• Can be used as absolute measure of ﬁt
• In WinBUGS you currently need to calculate it yourself
6-16

Introduction to Bayesian Analysis and WinBUGS
Use of mean deviance as measure of ﬁt
• Dempster (1974) suggested plotting posterior distribution of deviance
• Many authors suggested using posterior mean deviance D = IE[D] as a mea-
sure of ﬁt
• Invariant to parameterisation of θ
• Robust, generally converges well
• But more complex models will ﬁt the data better and so will have smaller D
• Need to have some measure of ‘model complexity’ to trade oﬀagainst D
6-17

Introduction to Bayesian Analysis and WinBUGS
Bayesian model comparison using DIC
• Natural way to compare models is to use criterion based on trade-oﬀbetween
the ﬁt of the data to the model and the corresponding complexity of the model
• Spiegelhalter et al (2002) proposed a Bayesian model comparison criterion
based on this principle:
Deviance Information Criterion, DIC = ‘goodness of ﬁt’ + ‘complexity’
• They measure ﬁt via the deviance
D(θ) = −2 log L(data|θ)
• Complexity measured by estimate of the ‘eﬀective number of parameters’:
pD
=
Eθ|y[D] −D(Eθ|y[θ])
=
D −D(θ);
i.e.
posterior mean deviance minus deviance evaluated at the posterior mean
of the parameters
• The DIC is then deﬁned analagously to AIC as
DIC
=
D(θ) + 2pD
=
D + pD
Models with smaller DIC are better supported by the data
• DIC can be monitored in WinBUGS from Inference/DIC menu
6-18

Introduction to Bayesian Analysis and WinBUGS
• These quantities are easy to compute in an MCMC run
• Aiming for Akaike-like, cross-validatory, behaviour based on ability to make
short-term predictions of a repeat set of similar data.
• Not a function of the marginal likelihood of the data, so not aiming for Bayes
factor behaviour.
• Do not believe there is any ‘true’ model.
• pD is not invariant to reparameterisation.
• pD can be be negative! (not desirable)
• Alternative to pD suggested
6-19

Introduction to Bayesian Analysis and WinBUGS
Could DIC be improved?
• It would be better if WinBUGS used the posterior mean of the ‘direct param-
eters’ (eg those that appear in the WinBUGS distribution syntax) to give a
’plug-in’ deviance, rather than the posterior means of the stochastic parents.
• Users are free to calculate this themselves: could dump out posterior means
of ‘direct’ parameters in likelihood, then calculate deviance outside WinBUGS
or by reading posterior means in as data and checking deviance in node info
• Lesson:
need to be careful with highly non-linear models, where posterior
means may not lead to good predictive estimates
• Same problem arises with mixture models
6-20

Introduction to Bayesian Analysis and WinBUGS
DIC is allowed to be negative - not a problem!
• A probability density p(y|θ) can be greater than 1 if has a small standard
deviation
• Hence a deviance can be negative, and a DIC negative
• Only diﬀerences in DIC are important: its absolute size is irrelevant
• Suppose observe data (-0.01, 0.01)
• Unknown mean (uniform prior), want to choose between three models with
σ = 0.001, 0.01, 0.1.
Dbar
Dhat
pD
DIC
y1
177.005
176.046
0.959
177.964
y2
-11.780
-12.740
0.961
-10.819
y3
-4.423
-5.513
1.090
-3.332
• Each correctly estimates the number of unknown parameters.
• The middle model (σ = 0.01) has the smallest DIC, which is negative.
6-21

Introduction to Bayesian Analysis and WinBUGS
Why won’t DIC work with mixture likelihoods?
• WinBUGS currently ‘greys out’ DIC if the likelihood depends on any discrete
parameters
• So cannot be used for mixture likelihoods
• Not clear what estimate to plug in for class membership indicator – mode?
• If mixture is represented marginally (ie not using an explicit indicator for class
membership), could use θ but could be taking mean of bimodal distribution
and get poor estimate
• Celeux et al (2003) have made many suggestions
• Can still be used if prior (random eﬀects) is a mixture
6-22

Introduction to Bayesian Analysis and WinBUGS
More on missing data in WinBUGS
Missing response data, assuming missing data mechanism is ignorable
• denote missing observations by NA in the data ﬁle
• specify response distribution (likelihood) as you would for complete data
• missing data are treated as additional unknown parameters
⇒WinBUGS will automatically simulate values for the missing observations
according to the speciﬁed likelihood distribution, conditional on the current
values of all relevant unknown parameters
Ignorable missing response data is essentially a prediction problem — see
earlier dugongs example
If missing data mechanism is informative
• need explicit model for missing data mechanism
• usually need informative priors on parameters of missing data model as no
information in the data
• See Best et al. (1996) for one example
6-23

Introduction to Bayesian Analysis and WinBUGS
Missing covariate data
• denote missing observations by NA in the data ﬁle
• specify prior distribution for the covariate
– e.g. if X is a continuous covariate containing some missing values, could
specify Xi ∼Normal(µ, σ2) or build regression model relating Xi to other
observed covariates
– can then assume vague priors for µ and σ2; posterior distribution of µ and
σ2 will be informed by the observed part of the vector of X’s
• WinBUGS will automatically simulate values from the posterior distribution
of the missing covariates (which will depend on the prior for the X’s and the
likelihood contribution from the corresponding response variable)
6-24

Introduction to Bayesian Analysis and WinBUGS
Example: Childhood malaria in the Gambia
Diggle et al (2002)
Data:
• 2035 children in 65 villages in the Gambia
• Response: Binary indicator of presence of malarial parasites in blood sample
taken from each child
• Covariates include: child’s age and use of bed nets, inclusion/exclusion of vil-
lage from primary health care system and greenness of surrounding vegetation
(from satellite information)
Questions of interest include:
• Does sleeping under a bed net reduce risk of malaria?
6-25

Introduction to Bayesian Analysis and WinBUGS
• Here we consider a slightly modiﬁed version of Diggle et al’s dataset:
– BEDNET = binary indicator of whether child sleeps under a (treated)
bed net
– Suppose the value of BEDNET is missing for 30% of children
• Consider 2 alternative models for the missing covariate:
1. Probability of BEDNET = 1 is same for all children a priori
BEDNETi
∼
Bernoulli(q)
q
∼
Beta(1, 1)
2. Probability of BEDNET = 1 depends on whether or not village belongs
to primary health care system (PHC)
BEDNETi
∼
Bernoulli(qi)
logitqi
=
γ1 + γ2PHCi;
(+ vague priors on γ1 and γ2)
6-26

Introduction to Bayesian Analysis and WinBUGS
DAG for Model 1
pi
xi
q
β
child i
Xi
BEDNETi
6-27

Introduction to Bayesian Analysis and WinBUGS
WinBUGS code for model 1
model {
for(i in 1:2035) {
Y[i] ~ dbern(p[i])
logit(p[i]) <- alpha + beta.age[AGE[i]] + beta.bednet*BEDNET[i] +
beta.green*(GREEN[i] - mean(GREEN[])) + beta.phc*PHC[i]
}
# model for missing exposure variable
for(i in 1:2035) { BEDNET[i] ~ dbern(q)
} # prior model for whether or not child
# i sleeps under treated bednet
q ~ dbeta(1, 1) # vague prior (uniform) on prob of sleeping under treated bednet
# vague priors on regression coefficients
alpha ~ dflat()
beta.bednet ~ dflat()
........etc.......
# calculate odds ratios of interest
OR.bednet <- exp(beta.bednet)
# odds ratio of malaria for children using
# treated bednets
PP.bednet <- step(0.8 - OR.bednet) # probability that using treated bed net
# reduces risk of malaria by at least 20%
}
6-28

Introduction to Bayesian Analysis and WinBUGS
WinBUGS code for model 2
• Replace model for missing exposure variable by
# model for missing exposure variable
for(i in 1:2035) {
BEDNET[i] ~ dbern(q[i])
# prior model for whether or not child i
# sleeps under treated bednet
logit(q[i]) <- gamma[1] + gamma[2]*PHC[i] # allow prob of using treated
# bednet to depend on whether
# or not village belongs to
# primary health care system
}
for(k in 1:2) {
gamma[k] ~ dflat()
}
OR.treated.phc <- exp(gamma[2])
# odds ratio of sleeping under
# treated bednet for children
# living in villages in the PHC
6-29

Introduction to Bayesian Analysis and WinBUGS
Results
OR.bednet
PP.bednet
OR.age2
Mean
95% interval
Mean
95% interval
No missing data
0.57
(0.45, 0.72)
0.99
1.40
(1.06, 1.81)
Model 1
0.66
(0.49, 0.86)
0.93
1.39
(1.06, 1.80)
Model 2
0.64
(0.47, 0.83)
0.95
1.41
(1.06, 1.83)
Single imputation∗
0.76
(0.61, 0.95)
0.68
1.40
(1.05, 1.80)
Complete case
0.63
(0.47, 0.83)
0.96
1.70
(1.20, 2.35)
(exclude all cases
with missing data)
∗Imputed using observed proportion of bed net users
6-30

Lecture 7.
Introduction to hierarchical models
7-1

Introduction to Bayesian Analysis and WinBUGS
Often interested in making inferences on many parameters θ1, ..., θN measured on
N ‘units’ (individuals, subsets, areas, time-points, trials, etc) which are related or
connected by the structure of the problem ?
We can identify three diﬀerent assumptions:
1. Identical parameters: All the θ’s are identical, in which case all the data
can be pooled and the individual units ignored.
2. Independent parameters: All the θ’s are entirely unrelated, in which case
the results from each unit can be analysed independently (for example using
a fully speciﬁed prior distribution within each unit)
→individual estimates of θi are likely to be highly variable (unless very large
sample sizes)
3. Exchangeable parameters: The θ’s are assumed to be ‘similar’ in the sense
that the ‘labels’ convey no information
Under broad conditions an assumption of exchangeable units is mathematically
equivalent to assuming that θ1, ..., θN are drawn from a common prior distribution
with unknown parameters
7-2

Introduction to Bayesian Analysis and WinBUGS
Example:
Hierarchical model for THM concentra-
tions
• Recall conjugate normal-normal model for the THM example in lecture 2
• Full data includes THM measurements for 70 water supply zones
• We assume a normal likelihood for the data in each zone
xiz ∼Normal(θz, σ2
[e]);
i = 1, ..., nz; z = 1, ..., 70
• Notice that we now have 70 distinct mean parameters θz
• What prior should we specify for each θz?
• Note that we now also take σ2
[e] to be unknown and assume a vague prior
1/σ2
[e] ∼Gamma(0.001, 0.001)
7-3

Introduction to Bayesian Analysis and WinBUGS
Identical parameters
• Assume that the mean THM levels are the same in all zones, θz = θ ∀z
• Assign a prior
θ ∼Normal(µ, σ2
[z])
with speciﬁed values of µ and σ2
[z] (note that ’[z]’ is a label not a subscript)
→conjugate normal-normal model discussed in Lecture 2
• But, assuming θz = θ is not really sensible since we do not expect zones
supplied by diﬀerent sources to have identical THM levels
7-4

Introduction to Bayesian Analysis and WinBUGS
Independent parameters
• Instead, we might assume independent vague priors for each zone mean, e.g.
θz ∼Normal(0, 100000),
z = 1, ..., 70
• This will give posterior estimates E(θz|xz) ≈xz (the raw zone mean, which is
the MLE)
→each θz estimated independently
→no ’pooling’ or ’borrowing’ of information across zones
→no smoothing of estimates
→how do we choose (and justify) values for the parameters of the Normal
prior?
7-5

Introduction to Bayesian Analysis and WinBUGS
Similar (exchangeable) parameters
Rather than specifying independent priors for each θz, we could specify a hierar-
chical prior:
θz ∼Normal(µ, σ2
[z]),
z = 1, ..., 70
where µ and σ2
[z] are unknown parameters to also be estimated
(Note: subscripts in [] are labels, not indices)
⇒assign prior distributions to µ and σ2
[z], e.g.
µ
∼
Normal(0, 100000)
1/σ2
[z]
∼
Gamma(0.001, 0.001)
⇒joint prior distribution for the entire set of parameters
p(θ1, ..., θ70, σ2
[e], µ, σ2
[z]) =
( 70
Y
z=1
p(θz|µ, σ2
[z])
)
p(σ2
[e])p(µ) p(σ2
[z])
Then apply Bayes theorem as usual to simultaneously estimate joint posterior
distribution of all the unknown quantities:
p(θ1, ..., θ70, σ2
[e]µ, σ2
[z]|x)
∝
( 70
Y
z=1
p(θz|µ, σ2
[z])
)
p(σ2
[e]) p(µ) p(σ2) ×
( 70
Y
z=1
nz
Y
i=1
p(xiz|θz, σ2
[e])
)
7-6

Introduction to Bayesian Analysis and WinBUGS
Marginal posterior for each zone mean parameter θz is obtained by integrating
the joint posterior p(θ, σ2
[e]µ, σ2
[z]|x) over the other parameters (σ2
[e]µ, σ2
[z], other θj,
j ̸= z) [easy using MCMC]
Advantages of this approach:
Posterior for each θz
• ‘borrows strength’ from the likelihood contributions for all of the zones, via
their joint inﬂuence on the estimate of the unknown population (prior) pa-
rameters µ and σ2
[z]
• leads to global smoothing of the zone mean THM levels
• reﬂects our full uncertainty about the true values of µ and σ2
[z]
Such models are called Hierarchical or Random eﬀects or Multilevel models
7-7

Introduction to Bayesian Analysis and WinBUGS
Graphical representation of THM models
xzi
θ
measurement i
σ2[e]
zone z
θz
xzi
measurement i
σ2[e]z
zone z
θz
xzi
µ
measurement i
σ2[z]
σ2[e]
zone z
Pooled
model
Independent
model
Hierarchical
model
7-8

Introduction to Bayesian Analysis and WinBUGS
Parameter Interpretation for hierarchical model
• θz is mean THM concentration in zone z for the study period
• µ is the overall mean THM concentration across all zones for the study period
• σ2
[z] is the between-zone variance in THM concentrations
• σ2
[e] is the residual variance in THM concentrations (reﬂects measurement
error and true within-zone variation in THM levels)
Note:
could elaborate model to allow zone-speciﬁc residual error variance σ2
[e]z
with a hierarchical prior:
xzi
∼
Normal(θz, σ2
[e]z),
i = 1, ...nz
log

σ2
[e]z

∼
Normal(v, φ2)
v
∼
Uniform(−100, 100)
1/φ2
∼
Gamma(0.001, 0.001)
7-9

Introduction to Bayesian Analysis and WinBUGS
Posterior mean THM level
   50.0km
N
Raw meanTHM levels
   50.0km
N
(0) <   90.0
(1)    90.0 -   100.0
(9)   100.0 -   110.0
(22)   110.0 -   120.0
(19)   120.0 -   130.0
(9)   130.0 -   140.0
(5) >=  140.0
(0) <   90.0
(0)    90.0 -   100.0
(2)   100.0 -   110.0
(27)   110.0 -   120.0
(33)   120.0 -   130.0
(6)   130.0 -   140.0
(2) >=  140.0
7-10

Introduction to Bayesian Analysis and WinBUGS
Shrinkage (smoothing) of zone mean THM levels in hierarchical model
100
120
140
THM level
MLE
Posterior
mean
7-11

Introduction to Bayesian Analysis and WinBUGS
Point estimates and 95% intervals for zone mean THM levels
0
10
20
30
40
50
60
70
100
140
Zone
THM level
Independent MLE
0
10
20
30
40
50
60
70
100
140
Zone
THM level
Hierarchical model
• Note estimates for 5 zones with no data under hierarchical model
• Independent model also assumes independent zone-speciﬁc variances — hence
no CI for zones with only 1 measurement
• Hierarchical model assumes common error variance — might be preferable to
assume hierarchical prior on zone-speciﬁc variances
7-12

Introduction to Bayesian Analysis and WinBUGS
General form for hierarchical model
In general, suppose we have data x and parameters θ = (θ1, ..., θn)
• Likelihood p(x|θ) models structure of the observables
• Prior p(θ) is decomposed into conditional distributions
p(θ|φ2) × p(φ2|φ3) × . . . × p(φm)
The marginal prior distribution for θ is then
p(θ)
=
Z
p(θ|φ2) × p(φ2|φ3) × × p(φm−1|φm) × p(φm) dφ2 dφ3 . . . dφm
• φk are called hyperparameters of level k and are introduced to simplify prior
speciﬁcation.
• The conditional prior distributions p(φk−1|φk) express structural judgements
(e.g. exchangeability, spatial correlation...)
• Theoretically there can be as many levels as necessary, but in practice it is
usually hard to interpret parameters of level 3 or higher
• A non-informative prior is usually speciﬁed for the marginal distribution of
the top-level parameters
7-13

Introduction to Bayesian Analysis and WinBUGS
Implementation of THM hierarchical model in WinBUGS
Data contain between 0 and 6 observations per zone →‘ragged array’
Zone THM level
1 111.3, 112.9, 112.9, 105.5
2 122.6, 124.6, 135.4, 135.7, 156.7, 144.8
3 133.1, 116.6, 106.2, 126
4 111.6, 112.5, 98.6, 107.7
5 –
6 124.7
.. ....
Three alternative ways to code model and data in BUGS
7-14

Introduction to Bayesian Analysis and WinBUGS
Method 1 — Oﬀsets
Model code
for(z in 1:Nzone){
for(i in offset[z]:(offset[z+1]-1)) {
thm[i] ~ dnorm(theta[z], tau.e) # likelihood
}
theta[z] ~ dnorm(mu, tau.z) # zone mean (random effects)
}
# priors on random effects mean and variance
mu ~ dnorm(0, 0.000001)
tau.z ~ dgamma(0.001, 0.001)
sigma2.z <- 1/tau.z
# random effects variance
tau.e ~ dgamma(0.001, 0.001)
sigma2.e <- 1/tau.e
# residual error variance
7-15

Introduction to Bayesian Analysis and WinBUGS
Data
list(Nzone=70,
thm=c(111.3, 112.9, 112.9, 105.5, 122.6, 124.6, 135.4,
135.7, 156.7, 144.8, 133.1, 116.6, 106.2, 126,
111.6, 112.5, 98.6, 107.7, 124.7, .....),
offset = c(1, 5, 11, 15, 19, 19, 20.....),
)
7-16

Introduction to Bayesian Analysis and WinBUGS
Method 2 — Nested index
Model code
for(i in 1:Nobs) {
thm[i] ~ dnorm(theta[zone[i]], tau.e) # likelihood
}
for(z in 1:Nzone) {
theta[z] ~ dnorm(mu, tau.z)
# zone means (random effects)
}
# priors on random effects mean and variance
mu ~ dnorm(0, 0.000001)
tau.z ~ dgamma(0.001, 0.001)
sigma2.z <- 1/tau.z
# random effects variance
tau.e ~ dgamma(0.001, 0.001)
sigma2.e <- 1/tau.e
# residual error variance
7-17

Introduction to Bayesian Analysis and WinBUGS
Data
list(Nobs = 173, Nzone = 70,
thm = c(111.3, 112.9, 112.9, 105.5, 122.6, 124.6, 135.4,
135.7, 156.7, 144.8, 133.1, 116.6, 106.2, 126,
111.6, 112.5, 98.6, 107.7, 124.7,...),
zone = c(1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,
4, 4, 4, 4, 6,.....))
Alternative data format:
list(Nobs=173, Nzone=70)
thm[]
zone[]
111.3
1
112.9
1
112.9
1
105.5
1
122.6
2
124.6
2
.....
.....
END
7-18

Introduction to Bayesian Analysis and WinBUGS
Method 3 — Pad out data with NA’s
Model code
for(z in 1:Nzone) {
for(i in 1:6) {
y[z,i] ~ dnorm(theta[z],tau.e)
# likelihood
}
theta[z] ~ dnorm(mu, tau.z)
# zone means (random effects)
}
# priors on random effects mean and variance
mu ~ dnorm(0, 0.000001)
tau.z ~ dgamma(0.001, 0.001)
sigma2.z <- 1/tau.z
# random effects variance
tau.e ~ dgamma(0.001, 0.001)
sigma2.e <- 1/tau.e
# residual error variance
7-19

Introduction to Bayesian Analysis and WinBUGS
Data
list(Nzone=70, thm=structure(.Data=
c(111.3, 112.9, 112.9, 105.5,
NA,
NA,
122.6, 124.6, 135.4, 135.7, 156.7, 144.8,
133.1, 116.6, 106.2, 126.0,
NA,
NA,
111.6, 112.5,
98.6, 107.7,
NA,
NA,
NA,
NA,
NA,
NA,
NA,
NA,
124.7,
NA,
NA,
NA,
NA,
NA,
.......), .Dim=c(70, 6)))
Alternative data format:
list(Nzone=70)
thm[,1]
thm[,2]
thm[,3]
thm[,4]
thm[,5]
thm[,6]
111.3
112.9
112.9
105.5
NA
NA
122.6
124.6
135.4
135.7
156.7
144.8
133.1
116.6
106.2
126.0
NA
NA
111.6
112.5
98.6
107.7
NA
NA
NA
NA
NA
NA
NA
NA
124.7
NA
NA
NA
NA
NA
.....
.....
END
7-20

Introduction to Bayesian Analysis and WinBUGS
Variance Partition Coeﬃcient (VPC)
• In hierarchical or multilevel models, the residual variation in the response
variable is split into components attributed to diﬀerent levels
• Often of interest to quantify percentage of total variation attributable to
higher level units
• In simple 2-level Normal linear models, can use VPC or intra-cluster correla-
tion (ICC) coeﬃcient
VPC =
σ2
[z]
σ2
[z] + σ2
[e]
– σ2
[e] is the ‘level 1’ variance (i.e. variance of Normal likelihood)
– σ2
[z] is the ‘level 2’ variance (i.e. random eﬀects variance)
• In WinBUGS, add extra line in model code to calculate VPC, e.g.
vpc <- sigma2.z / (sigma2.z + sigma2.e)
then monitor posterior samples of vpc to obtain point estimate and uncer-
tainty interval
7-21

Introduction to Bayesian Analysis and WinBUGS
vpc chains 1:2 sample: 10000
    0.0     0.2     0.4     0.6     0.8
   0.0
   2.0
   4.0
   6.0
Posterior mean = 0.49
95% CI (0.32, 0.64)
Posterior distribution of VPC
So approximately half the total variation in THM levels is between water zones,
and half is within water zones
7-22

Introduction to Bayesian Analysis and WinBUGS
Hierarchical centering
• Above formulation of THM model is hierarchically centered
– random eﬀect is centered around overall mean
θz ∼Normal(µ, σ2
[z])
• Alternatively, could use non-centered parameterisation
– random eﬀect is a priori independent of overall mean
θz
=
µ + φz
φz
∼
Normal(µ, σ2
[z])
• Choice of parameterisation can have big impact on mixing of MCMC chain
• Depends on relative size of variances at level 1 (σ2
[e]) and level 2 (σ2
[z]) — see
Gelfand, Sahu and Carlin (1995)
– If VPC large (σ2
[z] >> σ2
[e]), hierarchical centering more eﬃcient
– If VPC small (σ2
[z] << σ2
[e]), non-centered parameterisation more eﬃcient
• The following results are based on simulating data for THM example with
diﬀerent VPCs
7-23

Introduction to Bayesian Analysis and WinBUGS
θz
xzi
µ
measurement i
σ2[z]
σ2[e]
zone z
Hierarchically centered
model
θz
xzi
µ
measurement i
σ2[e]
zone z
Non centered
model
φz
σ2[z]
7-24

Introduction to Bayesian Analysis and WinBUGS
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
mu
  115.0
  120.0
  125.0
phi[1]
  -40.0
  -30.0
  -20.0
  -10.0
    0.0
   10.0
mu
  115.0   120.0   125.0
theta[1]
   90.0
  100.0
  110.0
  120.0
  130.0
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
VPC = 0.49; 95% interval (0.32, 0.64)
mean       sd
MC error
121.3
1.42
0.039 
mean       sd
MC error
121.3
1.41
0.017
Non hierarchically centered
Hierarchically centered
7-25

Introduction to Bayesian Analysis and WinBUGS
Non hierarchically centered
Hierarchically centered
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
mu
  115.0
  120.0
  125.0
phi[1]
    0.0
    5.0
   10.0
   15.0
   20.0
mu
  110.0
  120.0
theta[1]
  120.0
  125.0
  130.0
  135.0
  140.0
VPC = 0.83; 95% interval (0.75, 0.89)
mean       sd
MC error
120.9
1.37
0.060
mean       sd
MC error
120.8
1.43
0.014
7-26

Introduction to Bayesian Analysis and WinBUGS
Non hierarchically centered
Hierarchically centered
mu
  110.0
  120.0
theta[1]
   90.0
  100.0
  110.0
  120.0
  130.0
  140.0
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
mu chains 1:2
lag
0
20
40
   -1.0
   -0.5
    0.0
    0.5
    1.0
mu
  110.0
  120.0
phi[1]
  -40.0
  -20.0
    0.0
   20.0
VPC = 0.10; 95% interval (0.01, 0.30)
mean       sd
MC error
120.8
2.01
0.039 
mean       sd
MC error
120.3
2.06
0.102
7-27

Introduction to Bayesian Analysis and WinBUGS
Comparison with alternative estimation methods
Consider a general 2-level model
• Likelihood p(x|θ)
• Prior p(θ) decomposed into
– p(θ|φ) (random eﬀects distribution)
– p(φ) (prior on hyperparameters)
7-28

Introduction to Bayesian Analysis and WinBUGS
Empirical Bayes Estimation
• In full hierarchical Bayes, φ is treated as unknown and assigned a prior distri-
bution
– Posterior distribution of random eﬀects is
p(θ|x) =
Z
p(θ, φ|x)dφ
• In Empirical Bayes, a plug-in estimate ˆφ is used and inference proceeds as in
the non-hierarchical case with φ known
– Posterior distibution of random eﬀects is
p(θ|x, ˆφ)
– Ignores uncertainty in hyperparameters φ
– Point estimates usually similar to full Bayes, but interval estimates too
narrow
• ˆφ typically estimated using the marginal likelihood p(x|φ) = R
p(x|θ)p(θ|φ)dθ,
either by maximization or method of moments
• EB uses the data twice (once to estimate prior parameters, then to estimate
posterior)
7-29

Introduction to Bayesian Analysis and WinBUGS
IGLS/RIGLS Estimation
• (R)IGLS = (Restricted) Iterative Generalised Least Squares
• Provides ML estimates of ﬁxed eﬀects and variance components in Gaussian
multi-level models
• As with EB, no prior distribution is assumed for φ, and estimation is essentially
based on marginal likelihood p(x|φ)
• As with EB, ignoring full uncertainty in hyperparamters leads to overly precise
estimates
7-30

Introduction to Bayesian Analysis and WinBUGS
Example: Meta-analysis of Magnesium following MI
(See example 3.13 in Spiegelhalter et al (2004))
Intervention: Intravenous magnesium sulphate may have a protective eﬀect after
acute myocardial infarction (AMI).
Data:
Series of 8 small randomised trials estimating odds ratio for in-hospital
mortality in magnesium vs. control groups
Here we take the reported log odds ratio, xi, for each trial i as the ‘data’, and
assume these are normally distributed with known sampling variance, s2
i = reported
SE2 of the log odds ratio
Analysis: Random eﬀects meta-analysis
xi
∼
N(θi, s2
i )
θi
∼
N(µ, σ2)
• Empirical Bayes: use ‘plug-in’ values of ˆµ = log(0.58) and ˆσ = 0.29 based on
method of moments estimator
• Full Bayes: assume diﬀuse priors
µ
∼
Unif(−10, 10)
σ−2
∼
Gamma(0.001, 0.001)
7-31

Introduction to Bayesian Analysis and WinBUGS
Graphical models for the Magnesium meta-analysis example
θi
xi
s2i
study i
θi
xi
s2i
µ
σ2
study i
Empirical Bayes Model
Full Bayes Model
ˆµ
ˆσ2
7-32

Introduction to Bayesian Analysis and WinBUGS
Trial
Magnesium group
Control group
Observed
log OR
SE
Deaths
Patients
Deaths Patients
xi
si
Morton
1
40
2
36
-0.65
1.06
Rasmussen
9
135
23
135
-1.02
0.41
Smith
2
200
7
200
-1.12
0.74
Abraham
1
48
1
46
-0.04
1.17
Feldstedt
10
150
8
148
0.21
0.48
Shechter
1
59
9
56
-2.05
0.90
Ceremuzynski
1
25
3
23
-1.03
1.02
LIMIT-2
90
1159
118
1157
-0.30
0.15
7-33

Introduction to Bayesian Analysis and WinBUGS
Mortality OR
0.10
0.25
0.50
0.75
1.50
Overall
Morton
Rasmussen
Smith
Abraham
Feldstedt
Shecter
Ceremuz.
LIMIT-2
Full Bayes
Empirical Bayes
Independent
7-34

Introduction to Bayesian Analysis and WinBUGS
General comments on hierarchical models
Hierarchical models allow “borrowing of strength” across units
• posterior distribution of θi for each unit borrows strength from the likeli-
hood contributions for all the units, via their joint inﬂuence on the posterior
estimates of the unknown hyper-parameters
→improved eﬃciency
MCMC allows considerable ﬂexibility over choice of random eﬀects distribution
(not restricted to normal random eﬀects)
Judgements of exchangeability need careful assessment
• units suspected a priori to be systematically diﬀerent might be modelled by
including relevant covariates so that residual variability more plausibly reﬂects
exchangeability
• subgroups of prior interest should be considered separately
7-35

Introduction to Bayesian Analysis and WinBUGS
Hierarchical regression models
Example: Hepatitis B Immunisation
Background
• Hepatitis B (HB) is endemic in Africa
• National program of childhood vaccination against HB introduced in Gambia
• Program eﬀectiveness depends on duration of immunity aﬀorded by vaccina-
tion
Data
• 106 children immunized against HB
• For each child: anti-HB titre measured at time of vaccination (baseline) and
on 2 or 3 follow-up occasions
Study objective
• To obtain a model useful for predicting an individual child’s protection against
HB after vaccination
Related studies
• Similar study in Senegal found:
anti-HB titre ∝1
T
where T = time since HB vaccination
7-36

Introduction to Bayesian Analysis and WinBUGS
Raw data for a subset of 106 individuals
log time since vaccination (days)
log anti-HB titre (mIU)
6.0
6.5
7.0
0
2
4
6
8
10
12
7-37

Introduction to Bayesian Analysis and WinBUGS
Non hierarchical linear model (LM) for the HB data
1. Probability distn (likelihood) for responses:
yij
∼
Normal(µij, σ2)
where yij = log of the jth anti-HB titre measurement for child i
2. Linear predictor:
µij = α + β(tij −t) + γ(y0i −y0)
where
tij = log of time (days since vaccination) of jth measurement for child i
y0i = log of baseline anti-HB titre for child i
Problems
• Assumes a common regression line for all children
• Takes no account of the repeated measurements within children
7-38

Introduction to Bayesian Analysis and WinBUGS
⇒modify LM to allow separate intercept and slope for each child:
yij
∼
Normal(µij, σ2)
µij
=
αi + βi(tij −t) + γ(y0i −y0)
Assumes that conditionally on αi and βi, {yij, j = 1, 2, ..} are independent
• Assume αi’s are exchangeable and βi’s are exchangeable, e.g.
αi
∼
Normal(µα, σ2
α)
i = 1, ..., 106
βi
∼
Normal(µβ, σ2
β)
i = 1, ..., 106
Note — alternatively, could allow slopes and intercepts to be correlated and
assume that pairs (αi, βi) are exchangeable
→bivariate normal prior (see Practical Exercises)
• We may then assume vague priors for the hyperparameters of the population
distribution, e.g.
µβ, µα
∼
Normal(0, 10000)
τα = σ−2
α , τβ = σ−2
β
∼
Gamma(0.001, 0.001)
This is an example of a Hierarchical LM or Linear Mixed Model (LMM) or Random
Coeﬃcients model
7-39

Introduction to Bayesian Analysis and WinBUGS
Graph of a LM and LMM for the HB data
tij
α
yij
β
γ
µij
σ2
measurement j
child i
y0i
tij
αi
yij
βi
γ
µij
σ2
measurement j
child i
y0i
µα
σ2α
µβ
σ2β
7-40

Introduction to Bayesian Analysis and WinBUGS
Implementation in WinBUGS
Data contain 2 or 3 observations per child, so ragged array again
Child Log Titre
Log Time
1 4.99, 8.02
6.54, 6.96
2 6.83, 4.91, 6.29 5.84, 6.52, 6.98
3 3.95, 4.35
6.60, 7.02
4 ....
.....
7-41

Introduction to Bayesian Analysis and WinBUGS
Model code using nested index formulation
for(k in 1:TotalObs) {
y[k] ~ dnorm(mu[k],tau)
mu[k] <- alpha[child[k]]+ beta[child[k]]*(t[k]-tbar) + gamma*(y0[child[k]]-y0bar)
}
for(i in 1:N) {
alpha[i] ~ dnorm(mu.alpha, tau.alpha)
beta[i] ~ dnorm(mu.beta, tau.beta)
}
.... etc.
Data
list(N=106, TotalObs=288)
y[]
t[]
child[]
y0[]
4.99
6.54
1
8.61
8.02
6.96
1
8.61
6.83
5.84
2
7.10
4.91
6.52
2
7.10
.....
.....
END
7-42

Introduction to Bayesian Analysis and WinBUGS
Results for the LM and LMM models ﬁtted to the HB data
Parameters
LM
Parameters
LMM
α
6.03 (0.10)
µα
6.04 (0.15)
β
-1.05 (0.22)
µβ
-1.08 (0.13)
γ
0.67 (0.06)
γ
0.67 (0.08)
σ2
3.00 (0.26)
σ2
1.01 (0.11)
σ2
α
2.02 (0.35)
σ2
β
0.06 (0.09)
DIC
1136
DIC
913
pD
4.0
pD
95.1
One can see considerable improvement from ﬁtting the LMM model, the variability
being substantial only for the intercept
Note how the residual variance σ2 has been reduced.
7-43

Introduction to Bayesian Analysis and WinBUGS
Multiple random eﬀects and cross classiﬁed data
• Straightforward to extend basic 2-level hierarchical model to include multiple
random eﬀects at diﬀerent levels:
– nested hierarchies, e.g. THM measurements within zones within regions;
pupils within classes within schools
– cross-classiﬁed hierarchies, e.g. THM measurements cross-classiﬁed within
zones and years; pupils cross-classiﬁed within primary and secondary schools
• Easiest to formulate cross-classiﬁed models in BUGS using nested index no-
tation
7-44

Introduction to Bayesian Analysis and WinBUGS
Example: Schools – exam scores cross-classiﬁed by primary and secondary
school
• These data were obtained from the MLwiN website
www.mlwin.com/softrev/2lev-xc.html
• We use a random sample of 800 children who attended 132 primary schools
and 19 secondary schools in Scotland
• The following variables were used
Y: Exam attainment score of pupils at age 16
VRQ: verbal reasoning score taken on secondary school entry
SEX: Pupil’s gender (0 = boy, 1 = girl)
PID: Primary school identifying code
SID: Secondary school identifying code
• A normal hierarchical model is ﬁtted, with independent random eﬀects for
primary school and secondary school
• Verbal reasoning score and gender are included as ‘ﬁxed’ covariate eﬀects (but
note that in Bayesian framework, ‘ﬁxed’ eﬀect coeﬃcients are still assigned
prior distributions)
7-45

Introduction to Bayesian Analysis and WinBUGS
BUGS model code
for(i in 1:Nobs) {
Y[i] ~ dnorm(mu[i], tau.e)
mu[i] <- alpha + beta[1]*SEX[i] + beta[2]*VRQ[i] +
theta.ps[PID[i]] + theta.ss[SID[i]]
}
# random effects distributions (note: non centered)
for (j in 1:Nprim) { theta.ps[j] ~ dnorm(0, tau.ps) } # primary school effects
for (k in 1:Nsec) { theta.ss[k] ~ dnorm(0, tau.ss) } # secondary school effects
# priors on regression coefficients and variances
tau.e ~ dgamma(0.001, 0.001)
sigma2.e <- 1/tau.e
# residual error variance
tau.ps ~ dgamma(0.001, 0.001)
sigma2.ps <- 1/tau.ps
# between primary school variance
tau.ss ~ dgamma(0.001, 0.001)
sigma2.ss <- 1/tau.ss
# between secondary school variance
alpha ~ dnorm(0, 0.000001)
# intercept
for(q in 1:2) { beta[q] ~ dnorm(0, 0.000001)
}
# regression coefficients
# percentage of total variance explained ...
VPC.ps <- sigma2.ps / (sigma2.e + sigma2.ps + sigma2.ss) #..by primary school effects
VPC.ss <- sigma2.ss / (sigma2.e + sigma2.ps + sigma2.ss) #..by secondary school effects
7-46

Introduction to Bayesian Analysis and WinBUGS
Results
Parameters
Model 1
Model 2
α
5.53
(5.17, 5.88)
5.85
(5.59, 6.10)
β1 (sex)
–
–
0.23
(-0.08, 0.53)
β2 (VRQ)
–
–
0.16
(0.15, 0.17)
σ2
[e]
8.18
(7.35, 9.10)
4.49
(4.03, 5.00)
σ2
[ps]
1.12
(0.43, 1.98)
0.36
(0.08, 0.70)
σ2
[ss]
0.19
(0.10, 0.82)
0.02 (0.0007, 0.12)
VPCps
11.8% (4.7%, 19.8%) 7.4% (1.5%, 13.8%)
VPCss
2.0%
(0.1%, 8.3%)
0.4% (0.01%, 2.4%)
DIC
4008
3514
pD
58.0
43.8
7-47

Introduction to Bayesian Analysis and WinBUGS
[128]
[107]
[91][40]
[10]
[129]
[23]
[28]
[21]
[116]
[130]
[125]
[66][62][7]
[5]
[115]
[119]
[52]
[33]
[58]
[60]
[71]
[117]
[29]
[127]
[32]
[118]
[49]
[20]
[55]
[88]
[65][46]
[41]
[54]
[77]
[1]
[120]
[45]
[87]
[11]
[113]
[96]
[51]
[83]
[92]
[22]
[50]
[53]
[18]
[102]
[6]
[89]
[42]
[112]
[69]
[100]
[110]
[75]
[105]
[13]
[68]
[97]
[122]
[108]
[126]
[17]
[27]
[16]
[79]
[36]
[106]
[2][86]
[95][9]
[67]
[109]
[8]
[15]
[44]
[14]
[4]
[12]
[98]
[76][74]
[30]
[56][61]
[57]
[43]
[37]
[25]
[111]
[103]
[84]
[19]
[80]
[3][39]
[34]
[31]
[114]
[121]
[82]
[64]
[104]
[48]
[70]
[85]
[94]
[124]
[101]
[26]
[131]
[38]
[24]
[63][93]
[35]
[73][47]
[72]
[78]
[99]
[123]
[132]
[81]
[90]
[59]
Posterior distribution of primary school random effects
   -4.0
   -2.0
    0.0
    2.0
    4.0
[19]
[8]
[18]
[1]
[10]
[7]
[17]
[3]
[15]
[9]
[4]
[6]
[14]
[13]
[16]
[2]
[11]
[12]
[5]
Posterior distribution of secondary school random effects
   -4.0
   -2.0
    0.0
    2.0
    4.0
7-48

Introduction to Bayesian Analysis and WinBUGS
Heteroscedasticity
• Heteroscedasticity →non constant variance
• Can occur at any level of hierarchical model
• Easily handled in MCMC framework by modelling variance as a speciﬁed
function of other variables
7-49

Introduction to Bayesian Analysis and WinBUGS
Example: complex level 1 variation in Schools example
Original model:
Yi
∼
Normal(µi, σ2
[e])
µi
=
α + β1SEXi + β2VRQi + θ[ps]PIDi + θ[ss]SIDi
....
Complex level 1 variation depending on VRQ:
Yi
∼
Normal(µi, σ2
[e]i)
log σ2
[e]i
=
γ1 + γ2VRQi
µi
=
.......
Along with priors on α, βk and random eﬀects variances, also need priors on
coeﬃcients of variance model:
γk ∼Normal(0, 0.000001);
k = 1, 2
7-50

Introduction to Bayesian Analysis and WinBUGS
BUGS model code
for(i in 1:Nobs) {
Y[i] ~ dnorm(mu[i], tau.e[i])
mu[i] <- alpha + beta[1]*SEX[i] + beta[2]*VRQ[i] +
theta.ps[PID[i]] + theta.ss[SID[i]]
# complex level 1 variance
logsigma2.e[i] <- gamma[1] + gamma[2]*VRQ[i]
tau.e[i] <- 1/exp(logsigma2.e[i])
}
# remaining code is same as before
......
......
# except no longer need prior on residual error variance
##tau.e ~ dgamma(0.001, 0.001)
##sigma2.e <- 1/tau.e
# residual error variance
# instead need to include priors on coefficient of variance model
for(k in 1:2) { gamma[k] ~ dnorm(0, 0.000001)
}
7-51

Introduction to Bayesian Analysis and WinBUGS
BUGS model code continued....
## VPC will now depend on value of VRQ
# level 1 variance for child with VRQ in lowest 10th percentile
sigma2.e.lowVRQ <- exp(gamma[1] + gamma[2] * (-19))
# level 1 variance for child with VRQ in highest 10th percentile
sigma2.e.hiVRQ <- exp(gamma[1] + gamma[2] * 15)
## percentage of total variance explained by primary school effects....
# .....for pupils with low VRQ
VPC.ps.lowVRQ <- sigma2.ps / (sigma2.e.lowVRQ + sigma2.ps + sigma2.ss)
# .....for pupils with hi VRQ
VPC.ps.hiVRQ <- sigma2.ps / (sigma2.e.hiVRQ + sigma2.ps + sigma2.ss)
7-52

Introduction to Bayesian Analysis and WinBUGS
Initial values
• Remember to edit initial values from previous model to:
– remove initial values for tau.e
– add initial values for gamma vector
• Some care needed when specifying initial values for gamma[2] to avoid numer-
ical problems in BUGS
– gamma[2] measures eﬀect of unit change in VRQ (which ranges from −30
to 40) on log residual variance
– Residual variance was around 5 from previous analysis, so expect values
of log variance around log 5 = 1.6
⇒gamma[2] should be quite small (<< 1)
e.g.
list(alpha = 0, tau.ps = 1, tau.ss = 1, beta=c(0,0), gamma=c(1, 0.001))
7-53

Introduction to Bayesian Analysis and WinBUGS
Results
Parameter
Posterior mean
95% CI
γ2
0.019 (0.008, 0.029)
VPCps (low VRQ)
9.0% (2.0%, 18.0%)
VPCps (hi VRQ)
5.0% (1.0%, 10.4%)
VPCss (low VRQ)
0.6% (0.01%, 3.3%)
VPCss (hi VRQ)
0.4% (0.01%, 1.9%)
DIC
3503
pD
43.3
Recall model with homoscedastic level 1 variance had DIC = 3514, pD = 43.8,
so heteroscedastic model preferred
7-54

Introduction to Bayesian Analysis and WinBUGS
Hierarchical models for variances
Example: N-of-1 trials
Spiegelhalter et al (2004) Example 6.10
• N-of-1 trials →repeated within-person crossover trials
• Often suitable for investigating short-term symptom relief in chronic condi-
tions
• Example:
– Intervention: Amitriptyline for treatment of ﬁbromyalgia to be compared
with placebo.
– Study design: 23 N-of-1 studies - each patient treated for a number of
periods (3 to 6 per patient), and in each period both amitriptyline and
placebo were administered in random order
– Outcome measure: Diﬀerence in response to a symptom questionnaire
in each paired crossover period. A positive diﬀerence indicates Amitripty-
line is superior
– Evidence from study: 7/23 experienced beneﬁt from the new treatments
in all their periods
7-55

Introduction to Bayesian Analysis and WinBUGS
Raw data for each patient
Treatment benefit
patient
-2
0
2
4
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
7-56

Introduction to Bayesian Analysis and WinBUGS
Statistical model
If ykj is the jth measurement on the kth individual, we assume
ykj ∼N(θk, σ2
k)
Assume both θk’s and σ2
k’s are exchangeable, in the sense there is no reason to
expect systematic diﬀerences and we act as if they are drawn from some common
prior distribution.
Note: alternative assumptions are either that θk and σ2
k are same for all patients
(pooled model) or that they are independent (ﬁxed eﬀects) for each patient
We make the speciﬁc distributional assumption that
θk
∼
N(µθ, φ2
θ)
log(σ2
k)
∼
N(µσ, φ2
σ)
A normal distribution for the log-variances is equivalent to a log-normal distribu-
tion for the variances
Uniform priors adopted for µθ, φθ, µσ and φσ.
7-57

Introduction to Bayesian Analysis and WinBUGS
Graphical model
θk
ykj
σ2k
µσ
φ2σ
patient k
θk
ykj
σ2k
µθ
φ2θ
patient k
Independent eﬀect
period j
period j
Exchangeable means and variances
7-58

Introduction to Bayesian Analysis and WinBUGS
Estimates and 95% intervals for treatment eﬀect, and posterior probability that
eﬀect > 0
Treatment benefit
patient
-6
-4
-2
0
2
4
6
Overall
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
0.41
0.47
0.75
0.58
0.51
0.62
0.75
0.71
0.8
0.76
0.87
0.95
0.95
0.98
0.93
0.97
0.98
0.98
0.99
0.99
0.99
0.98
0.98
0.99
independent
exchangeable
7-59

Introduction to Bayesian Analysis and WinBUGS
Interpretation
• Exchangeable model shrinks in the extreme patients, reﬂecting the limited
information from each individual (see patient 23)
• It might be felt the model is exercising undue inﬂuence in this situation
• Despite shrinkage, narrower intervals mean that 9 patients have 95% intervals
excluding 0 compared to 6 with the independent analysis
• One consequence of allowing exchangeable variances is that patient 9 has a
wider interval under the exchangeable model
– patient 9’s observations were very close together →very narrow interval
under independence model
• Straightforward to include patient-level covariates
• Sensitivity analysis to the shape of both the sampling and the random-eﬀects
distribution: say assuming t-distributions.
7-60

Introduction to Bayesian Analysis and WinBUGS
Further reading
WinBUGS examples volumes I and II (lots of examples of Bayesian hierarchical
models)
Congdon (2001) (lots of examples of Bayesian hierarchical models)
Gelman et al (2004) Chapters 5, 13, 14
7-61

Introduction to Bayesian Analysis and WinBUGS
References
Aitchison, J. (1986).
The statistical analysis of compositional data.
Chapman and Hall, New
York.
Bartholomew, DJ, Steele, F, Moustaki, I and Galbraith, JI. (2002). The analysis and interpretation
of multivariate data for social scientists, Chapman & Hall, London.
Berry, DA (1996). Statistics: A Bayesian Perspective, Duxbury, London.
Best, NG, Spiegelhalter, DJ, Thomas, A and Brayne, CEG (1996). Bayesian analysis of realistically
complex models. J R Statist Soc A, 159, 323–342.
Breslow, N (1990). Biostatistics and Bayes. Statistical Science, 5, 269–298.
Brooks, SP (1998). Markov chain Monte Carlo method and its application. The Statistician, 47,
69-100.
Brooks, SP and Gelman, A (1998). Alternative methods for monitoring convergence of iterative
simulations. Journal of Computational and Graphical Statistics, 7, 434-455.
Casella, G and George, EI (1992). Explaining the Gibbs sampler. The American Statistician, 46,
167–174.
Celeux, G, Forbes F, Robert CP and Titterington DM. (2003). Deviance information criteria for
missing data models. Institut National de la Statistique et des Etudes Economiques. Serie des
Documents de Travail du CREST, No. 2003-30.
Congdon, P. (2001) Bayesian statistical modelling. Wiley.
7-62

Introduction to Bayesian Analysis and WinBUGS
Cowles, MK and Carlin, BP (1996) Markov chain Monte Carlo convergence diagnostics: a com-
parative review. Journal of the American Statistical Association, 91, 883–904.
Dempster, A (1998). Bayesian methods. In Encyclopedia of Biostatistics, (eds. P Armitage and
T Colton). Wiley, Chichester, pp. 263–271.
Diggle, P (1988).
An approach to the analysis of repeated measurements.
Biometrics, 44,
959–971.
Diggle, P, Moyeed, R, Rowlingson, B and Thomson, M (2002). Childhood malaria in the Gambia:
a case-study in model-based geostatistics. Applied Statistics, 51, 493–506.
DerSimonian, R and Laird, N (1986). Meta-analysis in clinical trials. Controlled Clinical Trials, 7,
177-188.
Dunson, D (2001). Commentary: Practical advantages of Bayesian analysis in epidemiologic data.
American Journal of Epidemiology, 153, 1222–1226.
Fisher, LD (1996). Comments on Bayesian and frequentist analysis and interpretation of clinical
trials — comment. Controlled Clinical Trials, 17, 423–34.
Foster, RA, Stine, RP and Waterman, DP (1998). Business Analysis Using Regression. Springer-
Verlag
Gelfand, AE and Smith, AFM (1990). Sampling-based approaches to calculating marginal densi-
ties. J Amer Statistic Assoc, 85, 398–409.
Gelman, A (2005). Prior distributions for variance parameters in hierarchical models. Bayesian
Analysis, to appear.
Gelman, A, Carlin, JC, Stern, H and Rubin, DB (2004).
Bayesian Data Analysis, 2nd edition,
Chapman & Hall, New York.
7-63

Introduction to Bayesian Analysis and WinBUGS
Greenland, S (1997). Probability logic and probabilistic induction. Epidemiology, 9, 322–332.
Kass, RE and Wasserman, L (1996). The selection of prior distributions by formal rules. Journal
of the American Statistical Association, 91, 1343–70.
Katz, JA and King, G. (1999). A Statistical Model for Multiparty Electoral Data. The American
Political Science Review, 93, 15-32.
Lee, PM (2004). Bayesian Statistics: An Introduction, 3rd edition, Arnold, London.
Lilford, RJ and Braunholtz, D (1996). The statistical basis of public policy: a paradigm shift is
overdue. British Medical Journal, 313, 603–607.
Little RJA and Rubin DB (2002). Statistical Analysis with Missing Data, 2nd edition, Wiley, New
Jersey.
O’Hagan, A (1988). Probability: Methods and Measurement, Chapman and Hall, London.
Senn, S (1997). Statistical basis of public policy — present remembrance of priors past is not the
same as a true prior. British Medical Journal, 314, 73.
Spiegelhalter, DJ (1998). Bayesian graphical modelling: a case-study in monitoring health out-
comes. Journal of the Royal Statistical Society, Series C, 47, 115–133.
Spiegelhalter, DJ, Thomas, A, and Best, NG (1995). Computation on Bayesian graphical models.
In Bayesian Statistics 5 (eds. JM Bernardo, JO Berger, AP Dawid and AFM Smith).
Oxford
University Press, Oxford), pp. 407-425.
Spiegelhalter, DJ, Gilks, WR and Richardson, S (1996). Markov chain Monte Carlo in Practice,
Chapman & Hall, London.
7-64

Introduction to Bayesian Analysis and WinBUGS
Spiegelhalter, DJ, Abrams, K and Myles, JP (2004). Bayesian Approaches to Clinical Trials and
Health Care Evaluation, Wiley, Chichester.
Spiegelhalter, DJ, Best, NG, Carlin, BP, and van der Linde, A (2002).
Bayesian measures of
model complexity and ﬁt (with discussion). J Roy Statist Soc B, 64, 583–639.
Tomz, M, Tucker, JA and Wittenberg, J. (2002).
An easy and accurate regression model for
multiparty electoral data. Political Analysis, 10, 66–83.
Western, B and Jackman, S. (1994). Bayesian Inference for Comparative Research Source. The
American Political Science Review, 88, 412
Zellner, A. (1962). An eﬃcient method for estimating seemingly unrelated regressions and tests
for aggregation bias. J. Amer. Statist. Assoc., 57, 348–368.
7-65

Appendix.
Supplementary material
A-1

Introduction to Bayesian Analysis and WinBUGS
Extra slides for Lecture 2
A-2

Introduction to Bayesian Analysis and WinBUGS
Bayes theorem and its link with Bayesian inference
Axioms of probability
Let A, B be events, and {Ai, i = 1, 2, 3, ...} be a set of events. The probability of
A, p(A), is a number which satisﬁes:
Axiom 1: 0 ≤p(A) ≤1 and p(A) = 1 if A is certain.
Axiom 2: If the events Ai are mutually exclusive, p(S
i Ai) = P
i p(Ai)
Axiom 3: p(A T B) = p(B|A)p(A)
A-3

Introduction to Bayesian Analysis and WinBUGS
Bayesian inference with binary data
Example: Inference on proportions using discrete prior
Assume treatment may have response rate θ of .2, .4, .6 or .8., each of equal
prior probability. If we observe a single positive response (x = 1), how is our belief
revised?
Likelihood, p(x | θ) = θx(1 −θ)(1−x)
θ
Prior
Likelihood
Likelihood × prior
Posterior
p(θ) p(x = 1 | θ) = θ
p(x = 1|θ)p(θ)
p(θ|x = 1) =
p(x=1|θ)p(θ)
P
j p(x=1|θj)p(θj)
.2
.25
.2
.05
.10
.4
.25
.4
.10
.20
.6
.25
.6
.15
.30
.8
.25
.8
.20
.40
P
j
1.0
.50
1.0
Note: a single positive response makes it four times as likely that the true response
rate is 80% rather than 20%.
A-4

Introduction to Bayesian Analysis and WinBUGS
Prediction
With a Bayesian approach, prediction is straightforward.
Suppose we wish to
predict the outcome of a new observation ˜x (say), given what we have already
observed.
For discrete θ we have
p(˜x|x) =
X
θj
p(˜x, θj|x)
which, assuming ˜x and x are conditionally independent given θ, is equal to
p(˜x|x) =
X
θj
p(˜x|θj)p(θj|x)
where the p(θj|x) can be thought of as ‘posterior weights’.
In example, predictive probability of treatment outcome for a new patient is:
p(˜x = 0|x = 1)
=
X
θj
(1 −θj)p(θj|x = 1)
=
(0.8) × 0.1 + (0.6) × 0.2 + (0.4) × 0.3 + (0.2) × 0.4 = 0.4
p(˜x = 1|x = 1)
=
X
θj
θjp(θj|x = 1)
=
0.2 × 0.1 + 0.4 × 0.2 + 0.6 × 0.3 + 0.8 × 0.4 = 0.6
A-5

Introduction to Bayesian Analysis and WinBUGS
.More complex priors for proportions
Suppose we want to express more complex prior opinion that cannot be adequately
summarised by a beta distribution
For example, might suspect that either drug will produce similar eﬀect to other
related compounds, or if it doesn’t behave like these compounds we are unsure
about its likely eﬀect
Prior is then a mixture
p(θ) = qp1(θ) + (1 −q)p2(θ)
where pi = Beta(ai, bi)
If we now observe r successes out of n cases, it turns out that the posterior is
p(θ|r, n) = q′p1(θ|r, n) + (1 −q′)p2(θ|r, n)
where
pi(θ|r, n)
∝
p(r|θ, n)pi(θ)
q′
=
qp1(r|n)
qp1(r|n) + (1 −q)p2(r|n)
Note: pi(r|n) is the beta-binomial predictive probability of r successes in n cases
assuming θ has distribution pi(θ)
⇒posterior is mixture of respective beta posteriors, with mixture weights adapted
to support prior that provides best prediction for the observed data
A-6

Introduction to Bayesian Analysis and WinBUGS
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
(a) Beta(9.2, 13.8) prior
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
(b) Mixture prior with 20% uniform
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
(c) Posterior
Probability or response
0.0
0.2
0.4
0.6
0.8
1.0
(d) Posterior
Mixture posterior has 44% on beta(24.2, 18.8) and 56% on beta(16, 6)
A-7

Introduction to Bayesian Analysis and WinBUGS
Derivation of posterior for Normal data with unknown mean, known vari-
ance
Suppose we have a sample of Normal data xi ∼N(θ, σ2)
(i = 1, ..., n). For now
assume σ2 is known and θ has a Normal prior θ ∼N(µ, σ2/n0)
Then the posterior distribution is
p(θ|x)
∝
Y
i
p(xi | θ) p(θ)
∝
exp

−
P
i(xi −θ)2
2σ2

× exp

−(θ −µ)2n0
2σ2

By matching terms in θ and writing P xi = nx it can be shown that
X
i
(xi −θ)2 + (θ −µ)2n0 =

θ −n0µ + nx
n0 + n
2
(n0 + n) + constant
The term involving θ is exactly that arising from a Normal distribution, so
p(θ|x) = N
n0µ + nx
n0 + n ,
σ2
n0 + n

A-8

Introduction to Bayesian Analysis and WinBUGS
Extra slides for Lecture 4
A-9

Introduction to Bayesian Analysis and WinBUGS
Transformations and Jacobians
Suppose p(θ) = Uniform(0, 1), i.e. p(θ) = 1 for θ ∈(0, 1)
Now consider φ =
√
θ. What is the distribution of φ?
The probability that θ ∈(a, b) must equal probability that φ ∈(√a,
√
b)
⇒area under curve of p(θ) in the interval (a, b) = area under curve of p(φ) in the
interval (√a,
√
b)
Turns out that the amount we need to stretch or compress the height of the
density p(θ) to give the corresponding height of the density p(φ) is given by the
Jacobian of the transformation,
 dθ
dφ

Here, φ =
√
θ ⇒θ = φ2, so
 dθ
dφ
 = 2φ and hence p(φ) = 2φ × 1 = 2φ for φ ∈(0, 1)
0.0
0.4
0.8
0.0
1.0
2.0
theta
p(theta)
0.0
0.4
0.8
0.0
1.0
2.0
phi
p(phi)
A-10

Introduction to Bayesian Analysis and WinBUGS
Further examples of Jeﬀreys’ priors
• Poisson case with parameter θ
log p(x|θ) = −θ + x log θ + C
⇒I(θ) = 1/θ
So Jeﬀreys’ prior for θ is ∝θ−1/2
This improper distribution is approximated by a Gamma distribution with
α = 1/2 and β →0
Some inconsistencies associated with Jeﬀreys’ priors
For example, applying this rule to the normal case with both mean and variance
parameters unknown does not lead to the same prior as applying separately the
rule for the mean and the variance and assuming a priori independence between
these parameters.
A-11

Introduction to Bayesian Analysis and WinBUGS
Various ‘non-informative’ priors for the binomial parameter
Consider r successes from n trials: r ∼Binom(n, θ), then
log p(r|θ) = r log θ + (n −r) log(1 −θ) + C
and I(θ) =
n
θ(1−θ).
Thus Jeﬀreys’ prior is
p(θ) ∝(θ(1 −θ))−1
2,
which is a Beta(1/2, 1/2) distribution .
The Bayes-Laplace Uniform density is a Beta(1,1) distribution.
A prior density that is uniform for logitθ is ∝to (θ(1−θ))−1, which is the improper
Beta(0,0) distribution.
In practise, there will not be much diﬀerence between these alternatives, but the
improper Beta(0,0) prior distribution leads to an improper posterior if r = 0 (or
n = 0)!
A-12

Introduction to Bayesian Analysis and WinBUGS
Extra slides for Lecture 6
A-13

Introduction to Bayesian Analysis and WinBUGS
Bayesian measures of model dimensionality (Spiegelhalter et al, 2002)
pD
=
Eθ|y[dΘ(y, θ, ˜θ(y))]
=
Eθ|y[−2 log p(y|θ)] + 2 log p(y|˜θ(y)).
If we take ˜θ = E[θ|y], then
pD = “posterior mean deviance - deviance of posterior means”.
In normal linear hierarchical models:
pD = tr(H)
where Hy = ˆy. Hence H is the ‘hat’ matrix which projects data onto ﬁtted values.
Thus pD = P hii = Pleverages.
A-14

Introduction to Bayesian Analysis and WinBUGS
Approximately Normal likelihoods
Let D(θ) = −2 log p(y|θ).
A Taylor expansion of D(θ) around D(θ), followed by posterior expectation, gives
Eθ|y[D(θ)]
≈
D(θ) −E
h
tr

(θ −θ)TL′′
θ(θ −θ)
i
=
D(θ) + tr

−L′′
θ V

where V = E 
(θ −θ)(θ −θ)T
is the posterior covariance matrix of θ, −L′′
θ is the
observed Fisher’s information evaluated at the posterior mean of θ.
Thus
pD = tr

−L′′
θV

,
can be thought of as the ratio of the information in the likelihood about the
parameters as a fraction of the total information in the posterior (likelihood +
prior). parameters - this result in likelihood parameters (degrees of freedom)
→can also think of pD as the dimensionality of the parameter space that is
identiﬁable by the data
A-15

Introduction to Bayesian Analysis and WinBUGS
Which plug-in estimate to use in pD?
• pD is not invariant to reparameterisation, i.e. which estimate is used in D(˜θ)
• WinBUGS currently uses posterior mean of stochastic parents of θ, i.e.
if
there are stochastic nodes ψ such that θ = f(ψ), then D(˜θ) = D(f(ψ))
• pD can be be negative if posterior of ψ is very non-normal and so f(ψ) does
not provide a very good estimate of θ.
• Also can get negative pD if non-log-concave sampling distribution and strong
prior-data conﬂict
A-16

Introduction to Bayesian Analysis and WinBUGS
Example
• If θ ∼U[0, 1], then ψ = θa is beta(a−1, 1).
• Suppose we observe r = 1 successes out of n = 2 Bernoulli trials, so that
r ∼Bin[θ, n]
• Consider putting prior on ψ = θ, θ5 and θ20, each equivalent to uniform prior
on θ
r <- 1;
n<- 2 a[1]<-1 ; a[2] <- 5; a[3] <- 20
for (i in 1:3){
a.inv[i]<- 1/a[i]
theta[i] <- pow(psi[i], a.inv[i])
psi[i]
~ dbeta(a.inv[i] , 1)
}
r1<- r; r2<-r ; r3 <- r
r1 ~ dbin(theta[1],n)
r2 ~ dbin(theta[2],n)
r3 ~ dbin(theta[3],n)
A-17

Introduction to Bayesian Analysis and WinBUGS
After 21000 iterations
Dbar
Dhat
pD
r1
1.947
1.386
0.561 r2
1.912
1.547
0.365 r3
1.921
2.239
-0.318
Mean deviances (Dbar) and posteriors for all θ’s are the same, but using ψ as a
plug-in is clearly a bad idea.
A-18

