Fundamentals of Learning Course Notes
Cynthia Rudin
Welcome! Machine learning is a broad ﬁeld within machine learning and predic-
tive statistics. It involves the design of algorithms that learn by example. Ma-
chine learning is (in a broad sense) pattern recognition. It is not real intelligence.
In machine learning, we often represent objects as vectors (that is, an ordered
set of numbers). For instance, an image can be represented by its red-green-blue
pixel values. A medical patient can be represented by numbers at represent the
patient’s age, number of prior strokes, whether they have had past congenital
heart failure, whether they take speciﬁc dosages of speciﬁc drugs, etc.
I’ll start by listing some of the important machine learning problems, starting
with the most famous one: classiﬁcation.
List of Some (Not All) Important Problems in Machine Learning
1. Classiﬁcation
• Input: {(xi, yi)}n
i=1 “examples,” “instances with labels,” “observations”
• xi ∈X ⊂Rp. When yi ∈{−1, 1} it is “binary classiﬁcation.”
• Output: f : X →R and use sign(f) to classify. If sign(f) ∕= y, the
point is misclassiﬁed. This can also be written y · sign(f) ≤0.
• Applications: automatic handwriting recognition, speech recognition,
biometrics, document classiﬁcation, detecting fraudulent credit trans-
actions
1

• Multiclass problems: If there are more than two classes, the problem is a
multiclass problem. Binary classiﬁcation techniques generalize nicely to
multiclass. The easiest way is to solve binary “one-vs-all” classiﬁcation
problems for each class.
• Algorithms: There are many famous classiﬁcation algorithms, including
logistic regression, AdaBoost, random forest, support vector machines,
k-nearest neighbors, decision tree optimization algorithms, and neural
networks.
2. Conditional probability estimation
• Input: {(xi, yi)}n
i=1, xi ∈X, yi ∈{−1, 1}
• Output: f : X →[0, 1] as “close” to p(y = 1|x) as possible.
• Applications: estimate probability of failure, probability to default on
loan, probability to commit another crime
• Algorithms: Some classiﬁcation algorithms, such as logistic regression
and AdaBoost, can be used for conditional probability estimation too.
3. Regression
• Input: {(xi, yi)}n
i=1, xi ∈X, yi ∈R
• Output: f : X →R
• Applications: predicting an individual’s income, predict house prices,
predict demand for energy, predict test scores, predict amount that
customer will spend
• Algorithms: Least squares, ridge regression, L1-penalized regression
(lasso), kernel least squares, kernel ridge regression, kernel regression
(which is diﬀerent from kernel ridge regression), gaussian process re-
gression
4. Ranking - between classiﬁcation and regression.
• Input: {xi}n
i=1, xi ∈X, and also we have a set of pairs (i, k) that
are labeled that i should be ranked above k, that is, we should have
f(xi) > f(xk) for these special pairs.
• Output: f : X →R such that f(xi) > f(xk) for the speciﬁed (i, k)
pairs as often as possible.
2

• Applications: Some search engines use ranking methods. Useful also
for predictive maintenance applications.
5. Finding patterns (correlations) in large datasets, rule mining, frequent item-
set mining
• Input: {xi}n
i=1, xi ∈X, (labels can be included optionally)
• Output for Frequent Itemset Mining: Find frequent combinations of
items (e.g., Milk & Cookies).
• Output for Rule Mining: Find rules such as (Diapers →Beer) that are
“interesting” according to some interestingness criteria.
• There are thousands of algorithms that all produce the same result.
The Apriori and FP-Growth algorithms are the most famous.
6. Clustering - grouping data into clusters that “belong” together - objects
within a cluster are more similar to each other than to those in other clusters.
This is unsupervised.
• Input: {xi}n
i=1, xi ∈X
• Output: f : X →{1, . . . , K} (K clusters)
• Applications: clustering consumers for market research, clustering genes
into families, image segmentation (medical imaging)
• Algorithms: Kmeans, Kmedians, Hierarchical agglomerative clustering
7. Density estimation. This is unsupervised.
• {xi}i, xi ∈X
• Output: f : X →[0, 1] as “close” to p(x) as possible.
• Applications: anomaly detection (anomalous mechanical behavior of a
piece of equipment), useful for variance estimates (lower density areas
might have larger conﬁdence intervals)
• Algorithms: Adaptive kernel density estimation methods are probably
the most popular.
Rule mining, clustering, and density estimation are unsupervised methods
(no ground truth), whereas classiﬁcation, ranking, and conditional probability
estimation are supervised methods (there is ground truth). In all of these
3

problems, we do not necessarily assume we know the distribution (or even the
form of the distribution) that the data are drawn from.
Training and Testing (in-sample and out-of-sample) for supervised learning.
Training: training data are input, and model f is the output.
{(xi, yi)}n
i=1 =⇒Algorithm =⇒f.
Testing: Evaluate the model. You want to predict y for a new x, where (x, y)
comes from the same distribution as {(xi, yi)}n
i=1.
That is, (x, y)∼D(X, Y) and each (xi, yi) ∼D(X, Y).
Problems arise in practice when the training data and test data are not drawn
from the same distribution.
To judge the quality of predictions, we need to know how to answer: How well
does f(x) match the label y? We will measure the goodness of f using a loss
function ℓ: Y × Y →R. It is a function that takes f(x) and y and produces a
(generally nonnegative) number.
For instance,
ℓ(f(x), y) = (f(x) −y)2
least squares loss, or
ℓ(f(x), y) = 1[sign(f(x))∕=y]
(mis)classiﬁcation error
The expected loss over the distribution that the data comes from is:
Rtest(f) = E(x,y)∼Dℓ(f(x), y)
=
!
(x,y)∼D
ℓ(f(x), y)dD(x, y).
Rtest is also called the true risk or the test error. This is the main quantity
considered in supervised learning.
4

We can’t calculate it. But we want Rtest to be small. If so, that would mean
f(x) is a good predictor of y.
How can we ensure Rtest(f) is small? The answer involves a few magic ingredi-
ents: a low training error, a large number of training points, and a simple model
class. Let’s discuss that.
Let’s look at how well f performs (on average) on the training set {(xi, yi)}n
i=1:
Rtrain(f) = 1
n
n
"
i=1
ℓ(f(xi), yi).
Rtrain is also called the empirical risk or training error. For example,
Rtrain(f) = 1
n
n
"
i=1
1[sign(f(xi))∕=yi].
(For instance, this might represent how many handwritten digits f classiﬁed in-
correctly.)
Say our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small,
hopefully Rtest(f) is too. But that doesn’t always happen. Sometimes we overﬁt.
Overﬁtting is the enemy of machine learning.
The left ﬁgure contains the data, and the right ﬁgure is a good ﬁt. In the middle
ﬁgure, f was overﬁtted to the data, modeled the noise, “memorized” the exam-
ples, and didn’t give us much other useful information. In other words, it doesn’t
5

“generalize,” i.e., predict. We didn’t “learn” anything!
Thus, it’s not enough to have a small training error, because we could overﬁt.
We also need a guarantee on how close Rtrain is to Rtest. Hence we need the other
two ingredients for when Rtrain would be close to Rtest:
• If n is large.
• If f is “simple.”
A beautiful mathematical theory (discussed next) formalizes what these mean,
and how we can get a guarantee on the test risk being close to the training
risk. This guarantee is important, because it tells us that if we have all three
ingredients (low training error, large training set, simple models), then our test
risk is also likely to be small.
Computational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a.,
Learning Theory, and in particular, Vapnik’s Structural Risk Minimization
(SRM) addresses generalization (Vapnik and Chervonenkis, 1971). Here’s SRM’s
classic picture:
6

Structural Risk Minimization says that we need models to be somewhat simple
in order to learn/generalize/avoid overﬁtting.
Statistical learning theory addresses how to construct probabilistic guarantees
on the true risk. In order to do this, it quantiﬁes classes of “simple models,”
discussed formally later.
What can we use as a deﬁnition of a “simple” model in practice?
There are many deﬁnitions. For instance:
• “simple” linear models have small coeﬃcients.
f(x) =
"
j
λjx(j) where ∥λ∥2
2 =
"
j
λ2
j < C.
• “simple” models could be models that are very smooth, and can’t change
too fast as we move around the space X.
• “simple” models have small values of a “simplicity function” which we gen-
erally call a regularization term.
• “simple” Bayesian models might be models that have large values of a
Bayesian prior. (As we will learn, this is equivalent to having a small regu-
larization term.)
Later we will learn some formal deﬁnitions of simplicity.
This expression below is kind of omnipresent. This form captures many algo-
rithms: SVM, boosting, ridge regression, LASSO, and logistic regression.
Regularized Learning Expression:
"
i
ℓ(f(xi), yi) + CRreg(f).
In the regularized learning expression, there are two terms: the loss term (which
controls training performance) and the regularization term (which controls model
7

simplicity). The loss ℓ(f(xi), yi) is often misclassiﬁcation error 1[yi∕=sign(f(xi))] =
1[yif(xi)≤0] for classiﬁcation problems. The regularization parameter C is a num-
ber that trades oﬀbetween training loss and simplicity. I’ll tell you later how to
choose it, using cross-validation.
Note that minimizing #
i 1[yif(xi)≤0] with respect to f is computationally hard.
This is why we often minimize upper bounds for it that are convex and easier to
minimize. Here are some of them. I have noted which common machine learning
algorithms they are used in.
• “logistic loss” log
$
1 + e−yif(xi)%
⇐= logistic regression, neural networks
• “hinge loss” max(0, 1 −yif(xi)) ⇐= support vector machines (SVM)
• “exponential loss” e−yif(xi) ⇐= AdaBoost
The margin is an important quantity in classiﬁcation. A point with a margin
of 0 means it is on the decision boundary. A large positive margin means that
the point is correctly classiﬁed and far from the decision boundary. A negative
margin means the point is misclassiﬁed. Let’s show an example on this simple
2D dataset, where the function f increases to the left.
8

Now let’s plot the margins. I’ll ﬂip the points over the decision boundary so that
all the correctly classiﬁed points on the right and all the incorrectly classiﬁed
points on the left. That way, the margin is now along the horizontal axis.
Intuitively, the margin tells us how well the point was classiﬁed. As we have
shown, the major loss functions of machine learning are deﬁned in terms of
margins.
Let us deﬁne various options for the regularization term Rreg(f). Usually f is
linear, f(x) = #
j λjx(j) (where notation x(j) means the jth component of x).
We choose Rreg(f) to be either:
• ∥λ∥2
2 = #
j λ2
j ⇐= used by ridge regression and SVM
9

• ∥λ∥1 = #
j |λj| ⇐= used by LASSO, approximately used by AdaBoost
The method called ridge regression is simply the squared loss with ℓ2 regulariza-
tion:
1
n
"
i
(yi −f(xi))2 + C∥λ∥2
2.
Support vector machines use the hinge loss and the ℓ2 norm:
1
n
"
i
max(0, 1 −yif(xi)) + C∥λ∥2
2.
AdaBoost uses the exponential loss without regularization, (although it acts as
if it uses ℓ1 regularization in some cases):
1
n
"
i
e−yif(xi).
Regularized logistic regression uses the logistic loss with either the ℓ1 or ℓ2 norm.
It is also possible to use the ℓ0 norm (well, it’s actually a semi-norm), which is
the count of non-zero terms in the model. It’s more diﬃcult to regularize this
because it’s not continuous in its arguments, but we will use it when we discuss
modern optimization of decision trees. As we’ll see, using ℓ0 has some major
advantages if we can do it computationally. The major one is that C gains more
intuitive meaning - it becomes the amount of loss we are willing to trade oﬀfor
one fewer term in the model.
You might have noticed that the diﬀerences between these algorithms, as I have
described them so far, is somewhat small. In fact, these algorithms all tend to
perform similarly in practice on many datasets if they use the same features. The
catch is that they use very diﬀerent kinds of features, as you will see.
References:
V. N. Vapnik and A. Y. Chervonenkis. “On the uniform convergence of rela-
tive frequencies of events to their probabilities.” Theory of Probability and its
Applications, 16(2):264-280, 1971.
10

