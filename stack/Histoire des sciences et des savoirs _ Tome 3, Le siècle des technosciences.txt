

Histoire des sciences et des savoirs

dans la même série
Histoire des sciences et des savoirs
sous la direction de Dominique Pestre
tome 1	 De la Renaissance aux Lumières
	
sous la direction de Stéphane Van Damme
tome 2	 Modernité et globalisation
	
sous la direction de Kapil Raj et Otto Sibum
tome 3	 Le siècle des technosciences
	
sous la direction de Christophe Bonneuil et Dominique Pestre

Sous la direction de Dominique Pestre
Histoire des sciences  
et des savoirs 
3. Le siècle des technosciences (depuis 1914)
Sous la direction de Christophe Bonneuil  
et Dominique Pestre
M. Armatte, C. Bigg, C. Bonneuil, S. Boudia, C. Cao, A. Dahan, D. Edgerton, 
P.N. Edwards, S. Franklin, D. Gardey, J.-P. Gaudillière, N. Jas, C. Lécuyer,  
J.-M. Lévy-Leblond, V. Lipphardt, Y. Mahrane, T. Mitchell, L. Nash, D. Pestre, 
A. Rasmussen, J. Revel, S. Schweber, S. Shapin, T. Shenk, S. Visvanathan
Traductions de l’anglais
Clara Breteau et Cyril Le Roy
ÉDITIONS DU SEUIL
25, bd Romain-Rolland, Paris XIVe

isbn 978-2-02-129813-0
© Éditions du Seuil, octobre 2015
Le Code de la propriété intellectuelle interdit les copies ou reproductions destinées à une utilisation 
collective. Toute représentation ou reproduction intégrale ou partielle faite par quelque procédé 
que ce soit, sans le consentement de l’auteur ou de ses ayants cause, est illicite et constitue une 
contrefaçon sanctionnée par les articles L.355-2 et suivants du Code de propriété intellectuelle.
www.seuil.com

 
Introduction  
au tome 3


Le siècle des technosciences  
(depuis 1914)
C h r i s t o p h e  B o n n e u i l  
e t  D o m i n i q u e  P e s t r e
Si le xixe siècle fut celui de l’invention de la science comme autorité 
épistémique et politique reposant sur les sciences données comme sources 
des progrès techniques et industriels, le siècle qui commence avec la 
Première Guerre mondiale est celui de la pénétration des savoirs scien-
tifiques et de leurs imaginaires dans tous les aspects de la vie sociale, 
économique et culturelle. Du pouvoir de l’atome au consumérisme 
high-tech, de la scientifisation de la guerre à la molécularisation de la 
médecine ou de l’agriculture, de l’affirmation des sciences économiques 
et sociales au monitoring scientifique du réchauffement climatique, rares 
sont les fragments de notre réalité qui n’ont pas été transformés par les 
sciences, les techniques et les savoirs contemporains.
Ce volume explore donc le déploiement des sciences et des techniques 
dans tous les milieux, académiques, industriels et militaires, dans les 
appareils d’État et les organisations internationales, dans le management 
de l’économie et la gestion des dégâts du progrès – mais encore la masse 
des savoirs produits dans le corps social, les think tanks et les ONG. Sont 
aussi au cœur de ce volume les manières de produire ces connaissances, 
les espaces dans lesquels elles voient le jour, ce qui fait qu’elles prennent 
la forme qu’elles ont ; mais encore les cités de justice et ontologies qu’elles 
promeuvent, les outils et dispositifs qu’elles engendrent, les impacts 
qu’elles ont sur les milieux et les populations. Le but est de voir large, 
de dire ces savoirs en situation, de signaler les tendances lourdes, de 
repérer les arrangements nouveaux qui surgissent ; mais encore l’articu-
lation du systémique et de l’inattendu, le travail institutionnel pérenne 
et le surgissement « par le bas » – les logiques argumentatives comme 
les rapports de force.
Faire ce travail requiert de procéder de multiples façons, de varier les 
angles d’approche, les échelles, les catégories, les cadres méthodolo-
giques et ontologiques. La première partie de ce volume part des grands 

10	
christophe bonneuil et dominique pestre 
champs d’activité sociale, de l’État et du politique, des militaires et de la 
vie industrielle, des idéologies sociales, des promesses et des affaires, et 
de leurs relations aux sciences et savoirs. Elle considère les images des 
sciences et des scientifiques, leur promotion comme leurs conséquences. 
Elle décrit la transformation des formes de vie des populations induite 
par le développement technoscientifique comme les formes de connais-
sance qui en surgissent. Elle s’attache à la critique, aux formes alternatives 
de savoir, à la place de la « société civile ». Elle prend à bras-le-corps les 
régulations, leurs outils, leurs logiques et renversements.
La deuxième partie considère les sciences elles-mêmes, ce qu’elles 
disent et font, la manière dont elles pensent et produisent le monde – plus 
exactement elle donne une idée de leur variété et de quelques-unes de 
celles qui ont pesé depuis 1914 : les sciences physiques alors dites « fonda-
mentales » et qui enracinent les discours épistémiques de l’essentiel du 
xxe siècle ; la longue durée des sciences technobiologiques et du gène ; le 
déploiement des savoirs sur l’environnement, de la conservation de la nature 
aux services écosystémiques. Mais encore la modélisation et les simula-
tions qui transforment la pratique des sciences depuis les années 1920 ; la 
montée en puissance d’une science économique s’autonomisant et devenant 
outil de gouvernement ; la maturation des sciences sociales, leur diversi-
fication et leurs variations épistémiques et géographiques – mais encore, 
puisque le siècle l’imposait, la compréhension de la « diversité humaine » 
et des races par des sciences prises dans les rets des idéologies sociales.
Notre dernier angle d’approche, dans ce volume, consiste à regarder les 
savoirs, outils et dispositifs de gouvernement – puisque le savoir n’a cessé 
de s’affirmer comme l’une des formes premières de légitimité de l’action 
au cours du siècle. À considérer la gestion des corps, notamment du corps 
féminin, objet par excellence des sciences médicales et comportemen-
tales dans ces années – et fantastique lunette pour saisir les changements 
qui traversent le siècle. À décrire le management de l’« innovation », 
et notamment à penser son lien à la propriété et aux volontés écono-
miques des États. À dire les cadres épistémiques et les outils mis en place 
pour gouverner les conséquences sanitaires et environnementales du 
développement technique. À présenter comment la Terre est devenue 
objet de science et de politique, les dispositifs d’information, de collecte 
et de calibration qu’il a fallu mettre en place, à l’origine dans un cadre 
militaire, pour surveiller et gouverner ce « système Terre ». Et finalement 
à regarder ce qu’a signifié le « développement », en considérant les débats 
et critiques qui ont entouré la mise en place de l’État développementa-
liste en Inde, et les mutations récentes, technoscientifiques, industrielles 
et économiques de la Chine.

	
le siècle des technosciences  	
11
 
Les grands traits du siècle
Nous commencerons ce volume par une série de propositions sur les 
sciences et savoirs en tant qu’ils sont liés aux enjeux sociopolitiques et 
économiques de la période.
La première est que le xxe siècle est le siècle des guerres – une propo-
sition banale mais essentielle pour qui veut parler des sciences. Ce siècle 
est le moment des guerres totales et de la brutalisation systématique 
des populations (même si cela est pratiqué avant 1914 dans le monde 
colonial), le temps des guerres asymétriques et des guerres de libération 
nationale, de la guerre technoscientifique qu’est la guerre froide – et les 
sciences y ont solidement contribué via les techniques matérielles (des 
gaz de combat aux drones), l’optimisation des usages (la recherche opéra-
tionnelle), l’anthropologie et les sciences médicales. En retour, elles ont 
dû s’hybrider, elles en sont sorties recomposées, réinventées : la science 
des matériaux dérive ainsi, dans les États-Unis de la fin des années 1950, 
des besoins de la défense.
La guerre de 1914 est certes une guerre de production, mais les 
nouveautés techniques (l’avion, le sous-marin) et scientifiques (le réglage 
du tir à longue distance) ne sont pas sans importance. Avec le radar, 
les missiles et l’informatique, mais encore l’analyse des systèmes et la 
recherche et développement (R & D), la Seconde Guerre mondiale et  
la guerre froide rendent le phénomène irréversible. Les décennies de guerre 
froide puis celle de l’après-11 septembre 2001 ont sécrété une culture de 
guerre généralisée façonnée par l’urgence, qui a transformé l’être même des 
sociétés et des sciences. Matériellement et conceptuellement, ces dernières 
soutiennent dorénavant une politique de la sécurité et de la guerre perma-
nente. Si les guerres ne se gagnent pas simplement par la science, mais 
sur le terrain, via le fantassin ou le partisan, et si la barbarie du siècle n’a 
pas nécessairement besoin du raffinement des technologies – la kalach-
nikov peut suffire –, il n’en reste pas moins que, de l’atome à l’espace et à 
l’environnement, cette omniprésence de la guerre marque profondément 
les pratiques d’investigation et les formes de régulation des sciences, des 
techniques et des savoirs sur l’ensemble de la période.
Ce siècle est aussi, deuxième thèse, celui des États-nations, d’États  
qui passent au centre de toute vie. Depuis la Première Guerre mondiale, 
les États sont dotés, grâce à l’impôt, de moyens inconnus du xixe siècle. 
Ils anticipent les besoins et instruisent le développement économique. 
Ils le font à l’Est et dans les États totalitaires, mais aussi au Japon et dans 
la Turquie d’Atatürk, aux États-Unis sous Roosevelt comme dans les pays 

12	
christophe bonneuil et dominique pestre 
sociaux-démocrates d’Europe. De même, les États développementalistes, 
à leur apogée dans les années 1960, font des sciences et de l’innovation 
un élément majeur de leur « modernisation ». Ces États organisent aussi 
la protection de leurs acteurs économiques dans la compétition inter-
nationale et sont, tout au long du siècle, les principaux entrepreneurs de 
science. Avec la Première Guerre, la plupart des pays européens se dotent 
ainsi de structures pérennes de financement de la recherche – le DSIR 
en Grande-Bretagne et le CNR en Italie sont institués durant le conflit. 
En 1945, « la science » étant décrite comme ayant gagné la guerre, les 
États se mettent à compter leurs savants et ingénieurs avec frénésie – ces 
derniers deviennent, sous la plume de ces gestionnaires, des valuable 
commodities – et à en planifier une production toujours croissante.
Une thèse veut que ceci n’ait plus vraiment cours, que nous soyons passés 
aujourd’hui à la gouvernance et au village global. Il y a du vrai dans cet 
énoncé : le Web et les réseaux sociaux sont devenus des agents politiques 
majeurs, et l’interconnexion des échanges comme le poids actuel de la 
finance limitent les marges de manœuvre de bien des États. Il convient 
toutefois de voir les limites de ces discours. Cela ne s’applique pas aux 
grands États – les dépenses de R & D du département de la Défense aux 
États-Unis restent actuellement supérieures, chaque année, à ce qu’elles 
étaient dans les hautes heures de la guerre froide. De même, la guerre 
économique mondiale restant largement une guerre entre systèmes 
fiscaux et industriels nationaux, chaque pays fait tout pour reproduire le 
miracle de la Silicon Valley dans son jardin. Si donc l’État a récemment 
perdu de son envergure, cela n’est pas vrai dans les champs technos-
cientifiques – les technologies de l’information et de la communication 
(NTIC) ou les biotechnologies, par exemple, autour desquelles États-Unis, 
Europe, Japon et Chine rivalisent avec de grands programmes.
Troisièmement, les années 1914-2014 sont le siècle américain. Les 
États-Unis sont déjà la première puissance industrielle dans les années 
1920, le pays doté du plus grand potentiel de recherche dans les années 
1930 et, depuis les années 1940, la puissance dominante en termes 
géostratégiques, médiatiques et culturels, d’innovation technique  
et institutionnelle, de consommation et de pollution. Terre d’accueil et 
pôle d’attraction scientifique, le pays recueille plus de 330 prix Nobel en 
sciences et économie en cent ans.
Parce que ce pays est le centre du système monde de la période, qu’il 
est éminemment technophile et libéral en termes économiques (comparé 
à l’Europe par exemple), qu’il est souvent radical dans ses explorations 
(l’eugénisme, la big science, l’utilisation de produits chimiques ou le 
human enhancement), il se trouve confronté aux problèmes avant les 

	
le siècle des technosciences  	
13
autres et à plus grande échelle (pollutions massives ou question globale 
des ressources). Il tend à gouverner ces problèmes par de nouveaux 
instruments à forte teneur scientifique (regulatory sciences, analyses 
coût-bénéfice…) qui font émerger de nouvelles communautés d’experts 
jouant alors les premiers rôles mondiaux (par exemple dans l’agronomie 
post-Land-Grant, l’écologie post-Dust Bowl, l’économie de l’environ-
nement ou la santé environnementale).
Parce que ce pays est éminemment libéral en termes sociaux et qu’il  
est doté de minorités résolues et vite professionnelles (pensez à ses ONG), 
il invente des solutions souvent radicales elles aussi (pensez à la puissance 
de l’EPA au début des années 1970). Comme son industrie est forte et 
que son système politique lui est très lié, les contre-attaques peuvent 
être aussi très dures (pensez aux think tanks libéraux et au détricotage 
des régulations environnementales dans les années 1980). Finalement, 
parce qu’il se raconte comme habité par un destin et qu’il a les moyens de 
sa politique, ce pays construit son hégémonie autant par la force (et son 
complexe scientifique-militaire-industriel) que par le soft power (industrie 
culturelle ou politiques de ses fondations).
Ce siècle est encore le siècle d’une industrie en pleine explosion et 
réinvention, le siècle de la démiurgie productive et de la consommation 
de masse ; d’abord à l’Ouest puis dans les puissances émergentes de ce qui 
fut le « tiers-monde ». Il est le siècle de l’invention du PIB par les sciences 
économiques, et celui d’un PIB mondial multiplié par 25 (en dollars 
constants) entre 1900 et 2008. Il est en conséquence le siècle de l’indus-
trialisation des sciences, de la recomposition des façons de produire et 
des techniques par les savoirs scientifiques. Les années d’après guerre 
sont celles de la big science, le siècle des révolutions vertes qui peuplent 
les paysages d’un petit nombre de variétés végétales et de races animales 
standardisées pour s’ajuster aux industries d’amont et d’aval de l’agri-
culture ; ou de la chimie du carbone qui remplace progressivement les 
matériaux historiques (verre, bois, fer, cuivre, papier, caoutchouc, coton 
ou laine) par des produits de synthèse.
De la guerre de 1914 aux années 1970, l’industrie est d’abord une affaire 
nationale. Du fait des politiques d’autarcie dans les années 1930, du fait 
des États, du keynésianisme et de leur rôle de modernisateur dans les 
décennies d’après guerre. Dans cette période, la recherche industrielle 
est surtout pensée à travers le laboratoire de firme planifiant une offre 
toujours renouvelée et alliant recherche fondamentale et développement. 
Les exemples types, et qui servent de « modèles », sont les Bell Labora-
tories et les laboratoires de la chimie et de la pharmacie allemandes ou 
suisses. La production repose largement sur les commandes de guerre 

14	
christophe bonneuil et dominique pestre 
pour l’électronique, les matériaux et la chimie, sur les remboursements 
que garantissent l’État social ou les assurances pour la pharmacie.
Depuis deux ou trois décennies, ces pratiques se sont transformées. La 
conception de produits et de lignées génériques s’adaptant rapidement 
à la demande – et moins la R & D – est devenue la pierre angulaire de la 
recherche industrielle. La R & D est devenue un paramètre qu’on peut 
externaliser. Cette évolution dérive de la guerre froide, du dévelop-
pement de la recherche et des start-up qu’elle permet, mais aussi de 
volontés politiques dans les États-Unis des années 1980 en réponse au 
sentiment de déclin qui y prévaut. Amorcées par une redéfinition des 
règles de la propriété intellectuelle et le Bayh-Dole Act, elles conduisent 
à des formes de parcellisation des savoirs, d’une part, à des formes  
de monopole et de judiciarisation, de l’autre. Une économie politique 
et morale des savoirs nouvelle et assise sur une propriété plus exclusive 
(les droits de propriété industriels) s’installe donc au cœur de ce qu’on 
nomme souvent – expression ectoplasmique et à fonctions multiples – les 
« économies » ou « sociétés de la connaissance ».
Le siècle qui nous occupe est, cinquième proposition, le siècle des rêves 
et espoirs technologiques, des images heureuses d’une nature nettoyée grâce 
au DDT (« l’Idaho sans insecte »), de l’évolution dirigée et sélective des 
« races » humaines, de l’énergie nucléaire bon marché, de l’espace « terre 
de conquête », des molécules ciblées de la pharmacie, de la reproduction 
maîtrisée, de la lutte contre la faim grâce aux semences nouvelles, du 
contrôle de la vieillesse via la biologie de synthèse – des images reprises 
en boucle par des médias, pour certaines depuis un siècle, qui nous disent 
la grandeur des sciences et le progrès. Le siècle a été et reste largement 
celui des idéologies scientistes – celles qui ont soutenu le monde du 
socialisme scientifique, de l’État développementaliste, de la Chine commu-
niste et postcommuniste – et bien sûr du libéralisme économique et 
des marchés qui optimisent, par nature disent ses chantres officiels, les 
solutions techniques les plus idoines.
Le discours des dernières décennies veut aussi que ce scientisme moder-
nisateur ne soit plus totalement de mise, que nos sociétés soient devenues 
attentives aux dégâts et effets pervers qu’elles occasionnent – qu’elles aient 
quitté la modernité insouciante pour devenir des « sociétés du risque », 
des « sociétés réflexives » et « postmodernes ». Ces discours capturent 
quelque chose de vrai à condition de ne pas sous-estimer les réflexi-
vités du passé ni la centralité persistante des logiques de puissance et  
de croissance du présent. Qui plus est, les aspirations humaines sont 
elles-mêmes contradictoires. Les demandes de précaution vis-à-vis des 
effets de long terme des nouveautés technoscientifiques s’accompagnent 

	
le siècle des technosciences  	
15
d’une adoption massive des hautes technologies, des technologies mobiles 
de l’information et de la communication aux demandes de techniques 
médicales, voire d’« augmentation de soi ».
Ce siècle est donc aussi le siècle des dégâts du progrès et des inquié-
tudes. C’est le siècle de l’amiante et des « smogs », des pollutions devenues 
globales, du changement climatique, de la déforestation (5 des 10 millions 
de kilomètres carrés de couvert forestier perdus depuis 11 000 ans l’ont 
été au seul xxe siècle). C’est le siècle des vaccinations de masse mais aussi 
des cas de menaces sur la santé publique causées par des produits de la 
biomédecine (la thalidomide par exemple). C’est également l’âge d’un 
basculement sanitaire, de la prédominance des maladies infectieuses à 
l’affirmation mondiale (y compris au Sud) des maladies chroniques liées 
aux modes de vie urbains et à l’omniprésence de molécules de synthèse 
dans l’environnement. Et les décennies d’après 1945 sont désormais identi-
fiées à la « grande accélération » de l’Anthropocène.
Les critiques du « progrès » sont vives au sortir de la Première Guerre 
mondiale (du fait des techniques employées, les gaz par exemple), durant 
la Grande Crise (certains proposent alors un moratoire sur la recherche), 
après l’explosion des deux premières bombes atomiques en 1945. Le 
mouvement de défiance prend toutefois une nouvelle ampleur à partir des 
années 1960, en réponse à une industrialisation exceptionnelle. Naturalistes 
défendant la nature, physiciens dissidents de la bombe, écologues alertant 
sur les atteintes à la biosphère, et finalement climatologues – les scien-
tifiques jouent un rôle clé dans l’identification des problèmes. Mais c’est 
aussi aux transformations sociales et à l’émergence de classes moyennes 
éduquées que ce mouvement est redevable. Et depuis les années 1990, 
les entreprises elles-mêmes se disent les maîtres d’œuvre d’un progrès 
et d’un développement « verts ».
La période est aussi, en septième lieu, celle de la multiplication des 
espaces de savoir et des conflits autour de ce qui constitue un savoir 
légitime. Après un long mouvement de professionnalisation du savoir 
scientifique engagé au xixe siècle, de disqualification épistémique des 
amateurs et de division du travail entre innovateurs et usagers dans les 
sociétés industrielles, le xxe siècle voit la multiplication d’espaces alter-
natifs aux universités, aux industriels et à l’État. En témoignent le rôle 
des fondations dans l’orientation de la recherche dès les années 1910 
aux États-Unis, la poussée des ONG de développement ou de défense de 
l’environnement dans les années 1970, et dont les équipes de recherche 
pénètrent le territoire des savoirs experts. C’est le cas avec les mouve-
ments féministes de santé et de maîtrise de la reproduction, puis avec 
les associations de malades qui bousculent les pouvoirs biomédicaux 

16	
christophe bonneuil et dominique pestre 
et font évoluer les lois et programmes de recherche, et plus récemment 
avec la vogue des sciences dites « participatives ». C’est enfin le cas avec la 
gamme très large de mouvements de contestation après 1968, du monde 
des économistes dissidents au mouvement altermondialiste, des instituts 
d’écologie indépendants allemands au mouvement pour l’agro-écologie 
au Brésil.
C’est aussi le cas, dans un registre symétrique, avec les think tanks 
libéraux et conservateurs (la Heritage Foundation ou Enterprise), des 
institutions de réflexion créées à l’initiative du monde des affaires dans 
les mêmes années 1970, et dont le but est de miner l’évidence des discours 
keynésiens et à vocation sociale qui dominent alors l’Université. Leur but 
est d’asseoir à leur place, en prenant appui sur l’espace public comme 
lieu de légitimation, le discours de la libéralisation nécessaire. Dans les 
années 1990, à la chute du mur de Berlin, ces groupes promeuvent aussi 
le discours de la guerre nécessaire et du conflit inévitable des civili-
sations. Ces think tanks, mais aussi les intérêts industriels organisés 
(comme l’a notamment montré la mise au jour des tobacco files), contri-
buent fortement au remodelage des savoirs universitaires légitimes dans  
les champs de l’économie, mais également du droit, de l’environnement, 
de la toxicologie et de la santé, voire du climat, avec le financement massif 
de travaux climatosceptiques.
Ce siècle est encore le siècle des régulations croissantes, de la multipli-
cation des comités d’experts, des administrations et agences. Le cœur du 
problème de la régulation est la tension entre le devoir de maintenir la 
croissance via le progrès technique, dogme incontournable, et la nécessité 
de protéger les populations contre les effets indésirables de ce progrès 
(nécessité qui varie fortement des pays riches aux pays pauvres, et des 
élites aux corps vils). Portée par des intérêts contradictoires, cette tension 
se traduit le plus souvent par le refus d’interdire totalement un produit 
lorsqu’il n’y a pas de produit de remplacement moins toxique, et par 
l’invention de dispositifs gestionnaires toujours nouveaux et qui s’accu-
mulent. Par exemple la définition de seuils de toxicité (dès les années 
1920), le calcul en coût-bénéfice après la Seconde Guerre mondiale (car 
on ne peut exiger une protection dont le coût serait prohibitif), l’éva-
luation et le management des risques (surtout depuis les années 1970 
et 1980), les savoirs et politiques d’« adaptation » (depuis Tchernobyl, 
Katrina et Fukushima notamment) – mais aussi la gestion par le secret, 
la fabrication active de l’ignorance et les états de fait industriels.
Après une phase de régulation appuyée sur l’expertise technique 
dans les années 1930-1960, une nouveauté majeure, à partir des années 
1970, est la mise en économie continue des pollutions et des questions 

	
le siècle des technosciences  	
17
d’environnement – via les « instruments économiques », les taxes,  
les marchés de droits, les engagements volontaires de responsabilité des 
entreprises. Si les régulations successives gardent souvent un train de 
retard sur la croissance globale des dégâts, elles n’en jouent pas moins 
un rôle important de polarisation des recherches, suscitant de nouveaux 
champs de savoir.
Le siècle continue aussi de connaître des formes actives de biopoli-
tique et de gestion des populations. En continuité avec le siècle précédent, 
celles-ci reposent sur des savoirs scientifiques, la médecine, les insti-
tutions publiques, et elles restent d’abord un gouvernement des corps 
subalternes : corps ouvriers et populaires (soumis à de forts taux de 
toxicité), corps racialisés et coloniaux, corps féminins et corps déviants 
(homo- ou intersexuels), corps des sujets des essais thérapeutiques dont 
l’Inde est aujourd’hui le vaste laboratoire. Les rapports sociaux, raciaux 
et de genre sont au cœur scientifique, médical et gestionnaire du gouver-
nement des populations sur l’ensemble de la période, au cœur de formes 
(changeantes) de la biopolitique.
Pour les corps ouvriers et populaires du second xixe siècle, des condi-
tions de travail et d’habitat en détérioration rapide génèrent des maladies 
spécifiques. Y répondent l’hygiène industrielle, portée par des réforma-
teurs sociaux, puis la microbiologie. La nouveauté des années 1920 est 
la toxicologie, une science de laboratoire réductionniste et gestionnaire 
qui participe directement au management industriel et établit les seuils 
d’exposition. À partir des années 1950 et 1960, la transformation des 
conditions matérielles de vie, notamment du fait de la dissémination 
infinie des produits de la pétrochimie, conduit à une sorte de « démocra-
tisation des pollutions » (puisqu’elles atteignent tout un chacun), à une 
redéfinition de la sécurité personnelle, et donc à l’explosion des contes-
tations par des classes moyennes maintenant affectées.
Pour la gestion du corps féminin, qui engage souvent le « corps de la 
nation », les continuités sont aussi très fortes avec le dernier xixe siècle. 
Dans l’entre-deux-guerres, la médicalisation du cycle de vie sexuel des 
femmes est en cours, portée par l’hormonothérapie. Les savoirs démogra-
phiques et « eugénistes » sont aussi centraux dans les débats sur les 
capacités génératives des femmes et le contrôle des classes inférieures. 
Les pratiques eugénistes sont nombreuses, même si très différenciées. 
Les politiques de stérilisation sont fréquentes aux États-Unis, en Suède, 
en Suisse et bien sûr dans l’Allemagne nazie où l’hygiène raciale est la 
plus criminelle, alors que les pays catholiques la pratiquent peu. Après 
guerre, la question du contrôle démographique domine le gouvernement 
des populations du « Sud », tandis que dans les pays industrialisés s’ajoute 

18	
christophe bonneuil et dominique pestre 
au planning familial le projet d’une sexualité épanouie – ce que promet 
la nouvelle science sexologique. Dissociant sexualité et engendrement, 
la pilule contraceptive marque le début de la « révolution sexuelle », de 
l’« enfant par projet » et, de façon plus générale, d’une biopolitique indivi-
dualisée. Ce temps ouvre aussi à la reconnaissance des identités sexuelles 
différentes – toutes transformations qui sont centralement gagées sur les 
sciences et le développement technique, de l’endocrinologie à la chirurgie.
Finalement, et c’est notre dixième trait caractéristique du siècle, le 
xxe siècle est le siècle du monde terrestre (enfin) quadrillé, de la Terre 
finie et inventoriée par l’ensemble des sciences – le moment terminal 
des globalisations qu’ouvre le xve siècle. Du moins est-ce le récit que 
les élites du Nord se font de leurs cinq derniers siècles de conquêtes et 
« découvertes ». La Terre, ses territoires comme ses populations sont 
dorénavant connus, enregistrés, maîtrisés. La géopolitique naît comme 
domaine d’action et la discipline géographique se recompose, en France 
comme en Allemagne. Elle parle des « limites de notre cage » (expression 
de Jean Brunhes en 1909) et considère que le temps qui s’ouvre est celui 
de l’exploitation rationnelle de la planète. Le xxe siècle est un monde 
approprié car travaillé en continu par les sciences de l’inventaire et de 
la conquête (après l’Antarctique peu avant 1914, c’est le cas des fonds 
océaniques et de la haute atmosphère durant la guerre froide), un monde 
dont le « rétrécissement » repose sur le déploiement massif de techno-
logies (de la radio au Web et de l’automobile à l’avion).
Ce « rétrécissement » se traduit en doctrines diverses d’un monde clos 
(celui de la surveillance des territoires et populations pour la guerre), 
d’un monde interdépendant (un phénomène drastiquement accentué 
par la financiarisation globale), parfois dans les termes d’un « organisme 
terrestre » vivant (selon les mots de Vidal de La Blache) – d’un monde 
qui serait dorénavant sans frontières, sauf celle que le Nord se donnera à 
nouveau dans les années 1960 au travers de l’espace et de son imaginaire.
Un siècle de pratiques et discours des sciences
Ce siècle est d’abord, pour ce qui a trait aux sciences elles-mêmes, le 
siècle des sciences de laboratoire, des sciences qui refont le monde à travers 
le détour par ce lieu artificiel qu’est l’expérimentation contrôlée. Dans  
la lignée du siècle précédent, le laboratoire et les ensembles théoriques 
qui l’accompagnent supplantent les sciences d’observation et de collection. 
Des années 1910 aux années 1970-1980, ce sont les sciences physiques 
et leurs génies associés qui imposent leurs marques matérielles, qui 

	
le siècle des technosciences  	
19
pèsent sur ses choix techniques et politiques. Dans l’espace public des 
sciences, ce sont la mécanique quantique et la physique atomique, puis 
la physique des hautes énergies qui s’imposent – la nouvelle génétique et 
la biologie moléculaire ne réussissant à obtenir la même autorité épisté-
mique que plus tard.
C’est au sein des sciences physiques que se définit alors la notion de 
« fondamentalité », une notion que reprennent les philosophes – pensez  
au cercle de Vienne, à Bachelard, Popper, Kuhn. C’est à travers la mécanique 
quantique et la théorie de la relativité que s’énoncent les normes de la 
bonne science – même si chimistes, biologistes et physiciens des solides 
les contestent. Et ces sciences s’imposent du fait de leur rôle dans la guerre, 
la physique des hautes énergies en profitant pleinement en termes finan-
ciers – ce qu’illustre parfaitement le cas du CERN, créé au tournant des 
années 1940-1950.
Dans cette période, toutefois, la physique est une affaire beaucoup 
plus large – et ce sont l’électronique solide et la science des matériaux 
que financent massivement les industriels et, dès 1958, les militaires.  
Les titres de gloire de ces sciences techniques-industrielles incluent alors  
les transistors, les communications, le guidage, les ordinateurs, l’explosion 
de matériaux nouveaux – mais aussi l’instrumentation, notamment pour 
la médecine et pour ces technosciences centrales en termes industriels 
que sont la chimie et la pharmacie.
Depuis la fin des années 1970, ce sont d’autres sciences de laboratoire, 
d’autres technosciences qui ont pris cette place dans les imaginaires et, 
dans une mesure un peu moindre, dans les réalités industrielles : les 
sciences de la vie et les biotechnologies, des sciences en mesure de recom-
biner et d’optimiser le vivant. Peut-être aussi parce qu’elles se solidifient 
dans les années de libéralisation économique, les modes de travail de  
ces sciences apparaissent comme d’emblée « pragmatiques ». Elles sont 
explicitement orientées vers et par des volontés d’action technologique, 
leur efficacité est d’abord gagée sur des savoir-faire et des techniques – elles 
sont, en un sens, des manières de faire et de manipuler autant que des 
« corps de connaissances ». Leur rôle dans la guerre n’est pas aussi central 
que celui des sciences physiques et mathématiques mais elles conduisent 
à des formes de biopolitique historiquement neuves à l’interface des 
individus et des marchés.
Les changements des dernières décennies ne se limitent toutefois 
pas à un déplacement du centre de gravité de la technophysique à la 
technobiologie, avec les glissements d’images, de normes et de pratiques 
scientifiques et productives qui les accompagnent. La pratique des sciences 
s’est aussi recomposée du fait du déploiement des outils mathématiques 

20	
christophe bonneuil et dominique pestre 
et informatiques, du fait de la puissance de calcul des ordinateurs, du 
fait des modélisations et simulations qu’ils permettent. Les capacités  
de lecture directe des phénomènes (via les puces à ADN en biologie par 
exemple, qui enregistrent et traitent automatiquement et en ligne de très 
grands nombres d’interactions), les capacités de stockage de l’information 
(l’immensité des banques de données qui sont au cœur de la physique 
des hautes énergies comme des sciences du système Terre), comme les 
capacités de traitement des données (la variété des logiciels, eux-mêmes 
largement distribués et échangés) ont transformé en profondeur la vie 
des sciences et de la recherche.
Les effets de cette montée en puissance exponentielle des moyens 
d’engendrement et de traitement de l’information se manifestent dans tous 
les domaines. Les modélisations sont ainsi devenues l’essence de l’analyse 
des grands systèmes naturels après avoir été au cœur du déploiement des 
grands systèmes technico-militaires (rappelons que c’est à la conception 
des bombes à hydrogène qu’ont d’abord servi les premières simulations 
sur ordinateur numérique à la fin des années 1940).
De nouveaux champs de science ont aussi émergé dans les dernières 
décennies. Si l’histoire du dernier siècle et demi a été celle d’une montée 
inexorable des approches réductionnistes et des sciences de labora-
toire au détriment de toutes les autres, les dernières décennies ont vu 
le retour des approches holistes ou « systémiques ». La vie des sciences 
s’occupe largement aujourd’hui d’écosystèmes, de questions d’engineering 
écologique, du réchauffement de la planète. Des dizaines de milliers de 
chercheurs travaillent sur le système Terre et ses équilibres, sur la bio­­
diversité et son évolution, sur les effets des pollutions, sur la gestion des 
« risques globaux » – toutes choses dont les racines intellectuelles sont 
dans l’écologie et qui sont assez nouvelles par rapport à ce qui a fait la 
gloire des sciences de la première moitié du siècle. Une conséquence ne 
peut qu’en être la fin du rêve de l’unité des sciences, la fin de la quête 
du « fondamental », la fin de la norme épistémologique unique encore 
rêvée en 1960.
Les raisons de ce « retour » des questions globales sont nombreuses. Il 
s’agit d’abord de l’émergence des nouveaux outils informatiques que nous 
venons de mentionner. Ils sont la condition de ces études, la condition de 
l’intégration de la masse très diversifiée d’observations et de données en 
un espace cognitif unique. Ils sont aussi la condition de l’alerte puisque 
ce sont les travaux des scientifiques qui ont rendu « réels », à partir des 
années 1980, le réchauffement climatique global ou la réduction de la 
biodiversité. Plus généralement, toutefois, ce sont les ratés du progrès 
et leur cortège d’affaires et d’effets environnementaux pervers – et bien 

	
le siècle des technosciences  	
21
sûr la forme que leur ont donnée les sciences et le mouvement écologiste 
depuis les années 1960 – qui ont fait surgir ces questions dans l’espace 
public et ont conduit les savoirs scientifiques à se déployer massivement 
dans ces directions.
Dans ces champs nouveaux de science, les questions de gestion et 
les considérations morales et politiques sont irrémédiablement mêlées 
aux études et évaluations de fait – en bref, ces analyses sont autant 
descriptives que prescriptives. Les questions sont celles de la conser-
vation et de la durabilité, du management de la nature et du futur de 
l’espèce, de l’inflexion souhaitée des choix techniques ou de dévelop-
pement. Autour du changement climatique par exemple, les questions 
mêlent inexorablement la qualité du maillage des mesures faites par les 
satellites, la mobilisation de disciplines très diverses (océanographie  
ou paléochronologie), la maîtrise des logiciels, la qualité des hypothèses, 
le choix des paramètres et calibrations. Mais les questions sont aussi celles 
du partage géopolitique des responsabilités, le choix des régulations à 
mettre en place, celui des indicateurs à utiliser (la mesure des émissions 
de CO2 par exemple) et qui sont censés guider l’action (réduire le réchauf-
fement) à travers des dispositifs économiques et politiques spécifiques 
(un marché du carbone). Ces problèmes sont conflictuels et intrinsè-
quement politiques, et les solutions retenues peuvent induire des effets 
pervers nombreux (la mesure du CO2 est-elle un bon indice de l’équi-
libre du système Terre ? Quels biais sont introduits si l’on ne retient que 
celui-ci pour fonder une politique ?). C’est aussi cette complexité du 
problème qui fait que les sciences sociales sont maintenant mobilisées, 
dans ces champs, en parallèle aux sciences « de la nature ».
Les conséquences sociales et politiques de ces nouvelles sciences du 
système Terre, des pratiques de simulation et de modélisation, comme des 
bio-nano-technologies, sont finalement considérables. Elles sont d’une 
nature très neuve – pensez, pour prendre un exemple extrême, au clonage 
humain ou au transhumanisme dont les enjeux civilisationnels sont plus 
massifs encore que ceux de la physique du xxe siècle. Les simulations 
et traitements informatiques gargantuesques conduisent à des descrip-
tions et scénarios que personne ne peut appréhender indépendamment 
d’elles. Le débat démocratique peut alors se retrouver dans une position 
très particulière puisque seules ces simulations peuvent dire ce que sont 
« les faits », alors que les scientifiques continuent d’en débattre. La question 
de la confiance dans le travail des scientifiques comme celle de la gestion 
de cette confiance dans l’espace public et médiatique deviennent donc 
des questions politiques majeures.
Mais ce siècle est encore celui de l’invention des sciences sociales. Parce 

22	
christophe bonneuil et dominique pestre 
que la question sociale envahit l’espace public des démocraties impériales 
et de masse que sont l’Europe et les États-Unis au début du siècle. Des 
savoirs portant sur la « société » et se déclarant scientifiques s’affirment 
donc – la sociologie par exemple, ou l’économie. Les cadres nationaux 
comme les traditions intellectuelles restent toutefois décisifs : les sciences 
sociales présentent de grandes variations de pays à pays, dans leurs 
découpages disciplinaires ou leurs positionnements de méthode. Aux 
États-Unis, ces savoirs s’articulent à des préoccupations d’intervention 
sociale. Le béhaviorisme, la psychologie sociale et le fonctionnalisme 
sont puissants, cette tendance à l’ingénierie sociale s’amplifiant au temps 
de la guerre froide. En Allemagne, pour un contraste, c’est la « théorie 
critique » qui s’impose après guerre, et elle analyse le capitalisme, l’alié-
nation, les formes culturelles.
À partir des années 1950, les sciences sociales basculent et décrivent des 
mondes scandés par des paradigmes et des sauts discontinus – les travaux 
de Claude Lévi-Strauss et de Thomas Kuhn pouvant servir d’étendards. 
Dans un jeu de balancier bien connu, les sciences sociales des années 1970 
se tournent en revanche vers les individus et leurs capacités à refaire les 
mondes – vers l’agency, la coproduction, l’hybridité, les objets frontières. 
Plus récemment, c’est la « provincialisation de l’Europe » qui donne le la.
On pourrait donc parler, s’inspirant de Hartmut Rosa, d’un premier 
moment des sciences sociales, ouvert par Marx, Weber et Durkheim, 
et qui s’intéresse à la transformation historique des sens du temps. Puis 
d’un deuxième, puissant dans les années d’après guerre, et qui s’attache 
aux « structures », reproductions et temps longs. Enfin d’un troisième, 
depuis quarante ans, qui regarde la manière dont les « acteurs » font sens 
du monde et ne cessent de le transformer. Paradoxalement, ces sciences 
sociales qui tracent des foisonnements et revirements nombreux et locaux 
s’intéressent peu à la question des temporalités historiques.
Les années 1914-2014 constituent finalement le siècle des économistes, 
des experts devenus les conseillers privilégiés des princes. Leur savoir 
s’autonomise au début du siècle. Une rupture s’opère toutefois dans le 
monde anglophone lorsque le terme economics s’impose au détriment 
d’« économie politique ». Ce mouvement d’élaboration d’une nouvelle 
approche et d’un cadre d’analyse plus technique s’appuie sur la création 
de statistiques autonomisant l’activité économique, et dont l’origine est 
d’abord dans le monde des affaires et de la finance.
La discipline économique (economics) invente dans les années 1930 
« l’économie » (the economy) – c’est-à-dire l’ensemble des flux de production 
et de redistribution, conçus à l’échelle nationale comme une gigantesque 
machine qu’un appareil moderne de savoir, de calcul et de gouvernement 

	
le siècle des technosciences  	
23
peut optimiser. Central pour les États, cet objet conduit à l’émergence 
de nouvelles données et indices (le revenu national) et à une réorien-
tation du politique qui donne aux économistes professionnels une  
place incontournable. Dans l’après-guerre, la mathématisation s’impose 
définitivement (un mouvement stimulé par la guerre froide et la R & D) 
et les économistes (via le modèle Arrow-Debreu notamment) pensent 
l’économie en termes de système en équilibre compétitif.
Le « marché », comme catégorie dotée d’un sens nouveau, s’impose 
dans les dernières décennies – et il transforme radicalement le paysage 
de la science économique. Le marché n’est alors plus seulement le lieu 
de l’échange économique mais l’espace dans lequel des individus libres 
partent des prix pour définir la voie qui produira le résultat socialement 
optimal. Le point focal de l’analyse n’est donc plus l’ensemble de l’économie 
(the economy) mais les décisions d’un Homo economicus stratégiquement 
compétent.
Très vite cette microéconomie montre tout son intérêt et aide à faire 
advenir les acteurs qui comptent. Même si les États ne restent pas sans 
pouvoir (dans les grands pays), les banques centrales et les institutions 
financières deviennent les endroits où les décisions se prennent, au 
détriment des élus. Les lieux les plus lucratifs sont dorénavant dans 
la finance, dont les outils dérivent de l’économie financière et de ses 
techniques mathématiques. En ce sens, l’économie comme discipline 
contribue centralement à faire advenir le monde des dernières décennies, 
à faire qu’il puisse se déployer continûment – en bref, elle le crée plus 
qu’elle ne le décrit.
Comment conclure ?
En notant d’abord l’emmêlement des chronologies et la complexité des 
logiques de transformation qui peuvent être invoquées. Une première 
chronologie peut être construite à partir des grandes mutations scienti-
fiques et techniques, sociales, économiques et géopolitiques. Elle séparerait 
(en gros) les années 1910-1970 des années 1970-2010 et caractériserait 
la nature des sciences mobilisées, les modes d’innovation, les certitudes 
sociales et le rapport à « la Science ». D’un côté seraient les sciences de 
laboratoire, la physique « fondamentale », l’écologie des écosystèmes 
quantifiant des flux d’énergie et de matière, l’« économie nationale », 
la biomédecine moléculaire et curative, les sciences sociales « structu-
rales », une démiurgie techno-industrielle questionnée seulement à la 
marge, des images de science simples. De l’autre les biotechnologies, les 

24	
christophe bonneuil et dominique pestre 
mathématiques appliquées, les modélisations et les moyens de calcul, une 
écologie des perturbations, de la résilience et des « services » écosysté-
miques, le dispositif global des sciences du système Terre, les sciences 
sociales pensant individus, réseaux et corps, les marchés comme acteurs 
économiques centraux – et un monde « complexe », plutôt plat et fluide, 
et fait de réseaux, d’« incertitudes » et de devoir d’adaptation.
Mais on pourrait aussi proposer des chronologies plus courtes et parler 
par exemple du moment autarcique de l’entre-deux-guerres, de celui de 
la guerre froide, de la phase de libéralisation, et aujourd’hui de celle de 
la sécurité ; et, dans chaque cas, analyser ce que ce cadre signifie pour les 
pratiques de sciences et de savoirs.
Il est aussi des continuités fortes et des logiques de retournements 
complexes. Les guerres sont décisives, bien sûr, et elles accélèrent 
l’investissement des États dans le déploiement national des sciences  
et techniques – au Nord, de 1918 aux politiques récentes de sécurité et, 
après les décolonisations dans les Suds, lors de la constitution des États 
développementalistes. Mais les héritages se font selon des logiques à fronts 
parfois renversés. Le retournement libéral des années 1980, par exemple, 
s’enracine profondément dans les logiques de la guerre froide – pensez au 
dilemme du prisonnier ou à « l’ontologie de l’ennemi » comme source de 
l’Homo economicus non coopératif, ou à la politique des brevets qui prévaut 
des années 1930 aux années 1970 et ce qu’elle fait advenir malgré elle.
En retour, les outils technoscientifiques modifient l’être des sociétés, 
les modes d’existence des individus, les modes d’action des acteurs finan-
ciers. La mondialisation et l’explosion des marchés (marchés de produits 
et marché des idées) conduisent à une redistribution des lieux de pouvoir, 
à une autre géographie des sciences et des techniques, à une vulnéra-
bilité de nombre d’États – et à l’apparition d’une forme d’hégémonie 
nouvelle, pour parler comme Gramsci, qui se dit soucieuse des dégâts 
imposés à la planète (à défaut de l’être sur la question sociale) et qui est 
centrée sur les grands États et leurs experts, des organisations globales 
comme la Banque mondiale ou l’OMC, les réseaux d’entreprises multi-
nationales (le World Business Council for Sustainable Development) et 
quelques grandes fondations et ONG.
Mais ces remarques ne permettent que d’appréhender de petits éléments 
de l’iceberg, elles oublient une infinité de mouvements – ce que, heureu-
sement, les chapitres qui suivent permettent de situer et de mieux saisir.

 
Première partie
Sciences, économies,  
sociétés


1 Figures de scientifiques
S t e v e n  S h a p i n
L’identité du scientifique au cours du dernier siècle peut être appré-
hendée de diverses façons. La première consiste à recourir à la notion de 
persona, aux exemples typiques, à s’intéresser aux rôles sociaux et culturels 
reconnus du scientifique 1. Qu’est-ce qui a été pensé et dit à propos de la 
figure du scientifique, de sa constitution mentale et morale, des institu-
tions et formes sociales au sein desquelles il évolue, de sa conduite idéale 
et réelle, de l’intérêt et de la valeur de ce qu’il fait ? Une autre approche, 
en apparence plus pragmatique et plus simple, consiste à explorer ce 
que les statistiques nous révèlent à propos des scientifiques. Combien 
sont-ils ? Que font-ils ? Dans quelles sortes d’endroits et d’institutions les 
trouve-t-on ? Comment, et à quelles fins, sont-ils sollicités ?
Cela fait déjà un certain temps que des historiens suivent la première 
approche (« souple » ou soft-headed) pour écrire sur la persona de ces 
scientifiques qui occupent des rôles à la fois valorisés et contestés dans 
les sociétés modernes, et dont nous pouvons voir à quel point leur image 
a changé dans la période récente. La notion quasi religieuse de vocation 
a cédé la place à la conception séculière d’un travail, le scientifique est 
passé du statut d’intellectuel à celui de spécialiste et de technicien, de 
celui de quêteur de vérité à celui de producteur de biens indispensables 
à la croissance économique, la santé publique et la puissance militaire 2.
Je pourrais faire un récit plus nuancé de ce qui a été dit sur les scien-
tifiques depuis un siècle, mais je souhaiterais regarder plutôt ici des 
questions moins débattues liées à l’émergence des statistiques. Dans 
quelles circonstances la population scientifique a-t-elle commencé à 
1. La notion de persona s’est répandue en histoire des sciences, notamment par les travaux de 
Lorraine Daston, mais tous ces termes peuvent être utilisés pour saisir quelque chose à propos 
des transformations, au cours de l’histoire, des représentations des personnes étudiant la nature. 
Cf. Daston et Sibum 2003, Daston et Galison 1992.
2. Kohler 1994, Herzig 2005, Haynes 1994, LaFollette 1990, Kaiser 2002 et 2004, Shapin 2008a 
et 2008b, Stevens 2013.
 Deux figures contrastées de scientifique au xxe siècle : Albert Einstein et Craig Venter.

28	
steven shapin
faire l’objet de recensements systématiques ? À quelles fins ces derniers 
ont-ils été effectués ? Avec quels effets sur les régimes de valeurs et 
sur la gestion des personnes occupant les rôles scientifiques ? Et puis 
comment les scientifiques ont-ils été identifiés et classés en catégories avant  
d’être comptés ? Les projets statistiques sont en effet financés par les 
gouvernements, alors que les discussions sur la persona se déroulent le 
plus souvent dans les cercles d’historiens, ce qui n’est pas dénué d’impor-
tance. Le projet statistique est orienté vers des fins pratiques tandis que 
le projet culturel a pour objectif la compréhension historique et socio-
logique. Mais quiconque entreprenant de compter les scientifiques doit 
avoir une idée préalable de qui sont ces « gens », et tout projet statistique 
suppose un minimum de projet culturel « souple », de documentation sur 
la persona et l’image du scientifique.
Même si des initiatives ont été prises au cours de la première moitié 
du xxe siècle pour recenser les scientifiques et organiser les résultats de 
ces recensements sous forme exploitable 1, le comptage systématique des 
scientifiques et la dissémination d’informations quantitatives à leur sujet 
aux administrations et responsables politiques ne deviennent réellement 
significatifs qu’au cours de la Seconde Guerre mondiale et des décennies 
qui suivent. On peut évoquer quatre raisons à ce constat. Premièrement, 
le xxe siècle est marqué par un changement historique du statut de la 
science qui, de passe-temps, devient un métier. Le plus grand scientifique 
du xixe siècle, Charles Darwin, est encore un amateur qui fait avec ses 
propres moyens et n’est jamais payé pour effectuer son travail de recherche. 
Dès les années 1930, en revanche, la vie scientifique se caractérise par son 
professionnalisme, avec une généralisation de l’échange de connaissances 
et de compétences scientifiques contre rémunération. Deuxièmement, le 
monde compte beaucoup plus de scientifiques au milieu du siècle qu’il n’y 
en a jamais eu auparavant, et les observateurs appartenant à la communauté 
scientifique, comme ceux qui lui sont extérieurs, s’intéressent beaucoup à 
cette croissance du nombre de scientifiques et à ses conséquences. Troisiè-
mement, tout au long du siècle, l’industrie, les militaires et l’État sont  
de plus en plus intéressés par les études conduites par les scientifiques  
dans les laboratoires de recherche industriels ou dans des installations 
gouvernementales. Enfin, les appréciations de ce qu’est le savoir scienti-
fique, et de ce à quoi il est utile, associent de plus en plus la science, au 
cours du siècle, aux préoccupations pratiques, techniques et commerciales.
Selon une anecdote du physicien Robert Oppenheimer, à la fin des 
années 1920, un mathématicien de l’université de Göttingen, qui se rend 
1. Godin 2007a.

	
figures de scientifiques	
29
tous les ans à la « Technische Hochschule » pour donner un cours sur 
« la relation entre la science et la technologie », est obligé de renoncer à 
son enseignement pour cause de maladie. L’un de ses éminents collègues 
le remplace et commence alors son discours en disant qu’il lui a été 
demandé de parler de la relation entre science et technologie, mais que 
cela lui est rigoureusement impossible puisque « Sie haben ja gar nichts 
miteinander zu tun » (elles n’ont rien à voir l’une avec l’autre) 1. La science 
est une chose et la technologie en est une autre. À Los Alamos, pendant la 
guerre, Oppenheimer dirige le programme qui doit permettre le dévelop-
pement de l’arme atomique grâce aux connaissances de la physique 
« fondamentale ». Après la guerre, il aime raconter l’histoire de Göttingen 
pour montrer combien les choses ont changé, et avec quelle rapidité.
D’après leurs documents, les gouvernements d’après guerre veulent 
connaître le nombre de scientifiques parce que ces derniers sont une 
ressource précieuse qui doit pouvoir être évaluée, gérée et mobilisée 
comme les autres ressources précieuses. Ces pratiques aboutissent 
rapidement à ce que l’on peut appeler la « normalisation » du rôle du 
scientifique. Si l’homme de science a pu être considéré comme un « prêtre 
de la nature » (selon l’usage au xviie siècle), comme répondant à un appel 
séculier (au sens de Max Weber) 2, ou comme un membre d’une commu-
nauté unique par ses vertus (selon la sociologie des sciences de Robert 
Merton), vers le milieu du siècle le scientifique est généralement vu 
comme un professionnel salarié dont les activités ont une valeur pratique 
qui lui valent d’être soutenu 3.
La big science et ses acteurs
La normalisation de l’identité du scientifique s’accélère grandement avec 
la Seconde Guerre mondiale et la guerre froide. Dans son discours d’adieu 
resté célèbre, le président des États-Unis Dwight D. Eisenhower s’inquiète 
en janvier 1961 de l’influence politique injustifiée du « complexe milita-
ro-industriel », évoquant le risque que « l’ordre public puisse devenir captif 
d’une élite scientifique et technologique », mais aussi du développement 
de la collectivisation et de l’organisation de la recherche scientifique qui, 
selon certains critiques, étouffent la créativité 4. Quelques mois plus tard, 
le directeur du laboratoire d’Oak Ridge de la Commission de l’énergie 
1. Oppenheimer 1955 [1947] (p. 89).
2. Shapin 2006 et 2003.
3. Shapin 2008b.
4. Eisenhower 1972 [1961].

30	
steven shapin
atomique des États-Unis introduit le terme de big science pour exprimer 
une inquiétude à propos de l’impact que la taille et les formes institu-
tionnelles de l’entreprise scientifique contemporaine ont sur la qualité 
du travail scientifique 1.
En 1978, le biochimiste Erwin Chargaff déclare que la nouvelle biologie 
moléculaire a vendu son âme au démon de la grandeur et du carriérisme :
L’institutionnalisation de la science comme activité de masse, qui a débuté de 
mon vivant, s’est accompagnée d’une obligation de croissance permanente […] 
– non tant parce qu’il reste beaucoup de choses à découvrir que parce qu’il y 
a tant de gens qui veulent être payés pour en faire 2.
Même s’il ne s’applique pas toujours à lui-même les conseils qu’il 
prodigue, Oppenheimer rappelle aux collègues de sa discipline que leurs 
compétences en physique ne les autorisent pas à se prononcer sur toutes 
sortes de questions intellectuelles, morales ou politiques :
L’étude de la physique, et je pense que mes collègues des autres disciplines 
seront d’accord pour que je parle également en leur nom, ne crée pas des 
philosophes rois. Elle n’a jusqu’à maintenant pas fait de rois. Elle ne produit 
pratiquement jamais de philosophes compétents – cela est si rare que ces 
derniers doivent être considérés comme des exceptions 3.
Plus tard, Oppenheimer est présenté par Michel Foucault comme un 
parfait exemple de la nouvelle figure de l’« intellectuel spécifique » apparue 
dans le sillage de la Seconde Guerre mondiale, autorisée à se prononcer 
sur des questions techniques limitées mais qui ne joue plus le rôle de 
« représentant de l’universel ». En ce sens, le nouveau scientifique / intel-
lectuel spécifique n’est finalement pas du tout un véritable intellectuel 4. 
Ce que Foucault présente comme un fait caractéristique des dispositifs 
modernes avait été identifié auparavant par Charles P. Snow comme une 
incompréhension. Ce dernier se demandait ainsi pourquoi un scienti-
fique comme Ernest Rutherford n’avait jamais été considéré comme un 
intellectuel. Mais Snow livrait là une bataille perdue depuis longtemps 5.
1. Weinberg 1961. Des inquiétudes à propos de l’impact de l’organisation scientifique sur la 
créativité avaient été exprimées plusieurs années auparavant par Whyte 1956 (partie 5).
2. Chargaff 1978 (p. 117).
3. Oppenheimer 1955 [1947] (p. 92).
4. Foucault 1980 (p. 126-128).
5. Snow 1993 [1959] (p. 4). Ailleurs (p. xxv, n. 13), Snow exprime son mépris pour les « intel-
lectuels », signe que lui aussi effectuait une distinction entre la figure du scientifique et celle 
de l’intellectuel.

	
figures de scientifiques	
31
 
Compter les scientifiques qui comptent
Si la big science apparaît comme un problème pour certains critiques 
du nouveau contrat entre la science et l’État pendant la guerre froide, 
ce dernier, de son côté, n’arrive pas à satisfaire ses besoins croissants 
en scientifiques et développe en conséquence une série de mesures 
agressives pour assurer cet approvisionnement. Les physiciens sont initia-
lement les principaux bénéficiaires de ces demandes, mais de nombreuses 
autres spécialités, dont les sciences humaines, connaissent également 
un développement extraordinaire du nombre de leurs praticiens et de 
leurs ressources, stimulé par la conviction que les scientifiques de toutes 
sortes peuvent constituer des atouts essentiels sur le plan commercial, 
militaire ou civique 1.
Les arguments sur l’utilité du travail des scientifiques appuient la nécessité 
de disposer de connaissances statistiques sur leur nombre afin d’établir 
des projections sur les besoins futurs, des politiques concrètes permettant 
de produire les scientifiques nécessaires, et d’arrêter la production de 
personnel inutile. Ainsi, pendant et après la Seconde Guerre mondiale, 
l’idée du scientifique comme simple individu qui fait des découvertes 
est prolongée, voire remplacée, par une conception bureaucratique des 
scientifiques comme marchandises auxquelles est affectée une valeur, et 
dont le nombre doit être connu de manière précise puis augmenté grâce 
à des politiques publiques. Le sociologue canadien Benoît Godin décrit 
de manière très détaillée les initiatives conduites au début du xxe siècle 
pour étudier et inventorier les stocks nationaux en ressources humaines 
scientifiques, exercices statistiques devenus plus systématiques et politi-
quement urgents pendant la Seconde Guerre mondiale et les décennies 
suivantes 2.
Science, the Endless Frontier (La Frontière infinie), un rapport de 
1945 commandé par le président des États-Unis Franklin D. Roosevelt 
à Vannevar Bush, directeur du Bureau de recherches et de dévelop-
pement scientifiques des États-Unis (Office of Scientific Research and 
1. Sur la physique, la formation des physiciens, ainsi que le rôle et le caractère des physiciens 
en Amérique pendant la guerre froide, voir notamment Kaiser 2002, 2004, 2006 et 2015 [à 
paraître en]. Voir aussi le livre désormais classique de Kevles 1977 (chap. 19-25). Sur les sciences 
sociales pendant la guerre froide, voir Lemov 2010, Buck 1985, Solovey 2013.
2. Godin 2002, 2005 (notamment p. 26), 2007b (notamment p. 5-7) et 2009 (p. 9-12 et 15-16). En 
appui de la mobilisation militaire prévue ou éventuelle, le gouvernement français avait conduit 
une série d’inventaires des ressources scientifiques entre les deux guerres mondiales, mais ce 
n’est que dans les années 1960 que des statistiques ont à nouveau été recueillies de manière 
systématique sur les ressources scientifiques « humaines et matérielles », cf. Bouchard 2006.

32	
steven shapin
Development, OSRD) pendant la guerre, imagine la forme des dispositifs 
d’après guerre devant organiser les relations entre la science et le gouver-
nement fédéral, avec un assemblage rapide de chiffres sur le nombre  
de scientifiques et de personnels disponibles. En 1947, le rapport de John 
R. Steelman pour le président Harry S. Truman, plus complet, inclut 
une enquête statistique conduite de manière plus systématique 1. Les 
Britanniques, qui ont compilé des statistiques sur le personnel scienti-
fique pendant la guerre, continuent à le faire après le conflit. En 1946, 
le comité Barlow de la Chambre des communes produit un rapport sur 
les besoins en personnel scientifique pour les dix années suivantes, ses 
auteurs ne voyant pas la nécessité d’indiquer explicitement pourquoi il 
est si important de « développer nos ressources scientifiques ». Ils disent 
simplement : « Jamais l’importance de la science n’a été aussi largement 
reconnue et jamais autant d’espoirs de progrès et de bien-être pour l’avenir 
n’ont été placés dans les scientifiques. » Le rapport contient également 
quelques statistiques préliminaires sur le nombre de scientifiques que 
compte le Royaume-Uni, ainsi qu’un appel à doubler le nombre de scien-
tifiques sortant alors annuellement des universités 2. Avec la création de 
la National Science Foundation (NSF) aux États-Unis en 1950, la collecte 
et la diffusion des statistiques sur le nombre de scientifiques prennent 
des formes plus complexes et sophistiquées, tout comme l’incorporation 
de ces statistiques dans la formulation des politiques nationales et inter-
nationales 3. Dans les années 1960, l’Organisation de coopération et de 
développement économiques (OCDE) suit les traces des Américains et  
des Britanniques avec la publication, en 1963, de la 1re édition du Manuel 
de Frascati qui établit les protocoles pour compter et catégoriser la science, 
les scientifiques et les ressources scientifiques 4.
La collecte de statistiques officielles sur le nombre de scientifiques ne 
sert pas seulement à dresser un état de la situation pour les politiques à 
mettre en œuvre. Ces statistiques, et les projets politiques dans lesquels 
elles s’inscrivent, permettent également de reconfigurer l’idée de ce qu’est 
un scientifique, à quoi sert le travail des scientifiques, ainsi que la manière 
dont la valeur des études scientifiques peut être évaluée et reconnue. Cela 
se retrouve en premier lieu dans la rhétorique entourant ces statistiques et, 
deuxièmement, dans certaines des catégories utilisées pour les présenter. 
1. Bush 1995 [1945] (notamment p. 127-134, 158 et 166-179), Steelman 1947 (notamment t. 4 : 
Manpower for Research).
2. Scientific Man-Power 1946 (p. 3 et 8).
3. Le projet « Science Indicators » a été lancé par la NSF en 1972 dans le cadre d’une recherche 
bureaucratique de mesures objectives et quantifiées du bien-être, de la productivité et de 
l’impact des scientifiques : cf. NSF 1973, Godin 2001, Elkana et al. 1978.
4. Godin 2008b.

	
figures de scientifiques	
33
Pour Steelman, les scientifiques sont une « ressource indispensable » pour 
toutes sortes de « progrès » nationaux 1. Avec le déclenchement de la guerre 
de Corée en 1950, la rhétorique sur la « ressource » s’affine : les scienti-
fiques sont désormais spécifiquement présentés comme des « outils de 
guerre », comme un « produit de guerre », comme un « atout de guerre 
majeur » qui peut être « stocké » comme « tout autre type de ressource 
essentielle 2 ». La question est régulièrement abordée sous l’angle de la 
« pénurie », à savoir un décalage entre l’offre fournie par les universités 
et les besoins en scientifiques de l’industrie et du gouvernement et, dans 
une moindre mesure, des universités qui produisent cette ressource. De 
manière générale, les scientifiques sont considérés comme des « intrants » 
dont la relation causale avec un certain nombre de « produits » matériels 
doit être déterminée de manière qualitative, avant d’être éventuellement 
fixée quantitativement. Le langage économétrique devient omniprésent 
dans les références aux scientifiques dans le débat public. Ces derniers 
deviennent une marchandise dont il est possible de parler, et d’ajuster 
la quantité, comme n’importe quel autre bien : « La pénurie de scienti-
fiques fait suite à une augmentation très importante de la demande, qui 
s’accompagne d’un approvisionnement inférieur à la normale 3. »
Comme prévu, davantage de scientifiques sont produits, même si 
les inquiétudes sur les « pénuries » subsistent dans les États-Unis de la 
guerre froide, notamment avivées par le lancement du Spoutnik en 1957 
et modérées uniquement par les besoins concurrents liés d’un côté à la 
guerre du Vietnam et de l’autre aux programmes de la Grande Société 
de la présidence de Lyndon Johnson 4. Les premiers efforts de maîtrise 
des dépenses fédérales dans le domaine de la recherche et du dévelop-
pement ne sont pas couronnés de succès : au milieu des années 1960, on 
estime que les dépenses du gouvernement états-unien dans le domaine 
sont supérieures à la totalité du budget fédéral avant Pearl Harbor 5. La 
croissance des stocks de scientifiques s’accompagne du développement 
de l’idée selon laquelle le scientifique peut être assimilé à quelque chose 
qui peut être stocké. Les notions de travail scientifique comme travail 
discipliné, organisé et permettant de résoudre des puzzles, de la science 
comme « art des possibles » et des problèmes scientifiques comme pouvant 
1. Steelman 1947 (t. 4, p. 1).
2. Smyth 1951 (p. 64).
3. Steelman 1947 (t. 4, p. 1-6).
4. Greenberg 2001. Greenberg montre que, contrairement aux arguments des chefs de file de 
la communauté scientifique américaine, la rhétorique de la « pénurie » a persisté longtemps 
après que l’offre en scientifiques est devenue surabondante en termes économiques dans de 
nombreux domaines. Voir également Godin 2005 (p. 239-260).
5. Price 1965 (p. 3).

34	
steven shapin
être résolus grâce à des attaques massives ne sont pas nouvelles, mais 
elles sont désormais tellement prégnantes qu’elles peuvent susciter 
des réactions négatives (comme le discours d’adieu d’Eisenhower et 
l’essai de Weinberg sur la big science). Thomas Kuhn, qui participe à des  
conférences sur la créativité scientifique sous l’égide du gouvernement, 
ne considère en aucune manière sa notion de « science normale » comme 
une réponse à ces inquiétudes politiques, mais ses idées trouvent bien 
un écho significatif dans ce contexte 1.
Les évaluations du nombre de scientifiques varient, bien sûr, en fonction 
des critères retenus pour déterminer qui est un scientifique, mais tous 
les indices révèlent une croissance énorme au cours du xxe siècle, et en 
particulier après la Seconde Guerre mondiale. En 1906, James Cattell 
juge 4 000 hommes de science dignes de figurer dans son recueil biogra-
phique. En 1944, ce nombre a considérablement augmenté pour atteindre 
34 000 2. Le rapport Steelman évalue à 71 000 le nombre de scientifiques 
en Amérique en 1937 (dont 13 900 titulaires de doctorat) et à 128 000 en 
1947 (dont 23 200 titulaires de doctorat) 3. En 1963, un sociologue estime 
que les États-Unis comptent alors environ un million de personnes dotées 
de diplômes scientifiques et techniques 4. Selon le premier rapport Science 
Indicators de la NSF, « le nombre total de scientifiques et d’ingénieurs 
actifs aux États-Unis a augmenté d’environ 50 % de 1960 à 1971, pour 
atteindre le nombre de 1 750 000. Le nombre de titulaires de doctorat 
a doublé au cours de la même période et représente 10 % du total 5 ». 
L’édition actuelle de Science Indicators indique que « le nombre de travail-
leurs (aux États-Unis) dans les métiers de la science et de l’ingénierie  
est passé d’environ 152 000 en 1950 à 5,4 millions en 2009 », avec un taux 
de croissance annuel moyen atteignant presque 6 %, cinq fois plus que 
le taux de croissance de la main-d’œuvre totale âgée de plus de 18 ans 6.
Les scientifiques et l’administration ont conscience de cette croissance 
significative, comme le reflète sa mise en valeur par la « scientométrie », 
qui se développe dans les années 1960. En 1963, l’historien et socio-
logue Derek John de Solla Price publie un ouvrage qui a une grande 
1. Voir notamment Kuhn 1963. La conférence pendant laquelle Kuhn a présenté son article 
s’est tenue en 1959 et comptait parmi ses participants certains officiels de l’Advanced Research 
Projects Agency du Pentagone, du Centre de recherche sur le personnel et la formation de 
l’armée de l’air (Air Force Personnel and Training Research Center), ainsi que des représentants 
de l’entreprise de chimie Dow Chemical. Voir aussi Cohen-Cole 2013 (chap. 2).
2. Godin 2007a (p. 701-702).
3. Steelman 1947 (t. 4, p. 11).
4. Price 1986 [1963] (p. 7).
5. NSF 1973 (p. 48).
6. National Science Board, Science and Engineering Indicators 2012, chap. 3, < http://www.
nsf.gov/statistics/seind12/> .

	
figures de scientifiques	
35
influence car il soutient que, quelles que soient les catégories choisies 
(articles scientifiques, revues, découvertes ou personnel scientifique), la 
croissance est toujours exponentielle. Ce qui, pour de nombreux scien-
tifiques eux-mêmes, semble constituer un phénomène jamais observé 
auparavant, caractéristique de la big science, est en fait ramené à un 
simple moment d’un processus suivant une loi universelle. Il est dans 
la nature de la science de croître ainsi, et une « science de la science » 
existe justement pour étudier des lois comme celle-ci. La période corres-
pondant à un doublement de pratiquement tous les aspects de la science 
est d’environ dix à quinze ans, et Price soutient qu’il n’y a aucune période  
dans l’histoire pendant laquelle ce principe ne s’est pas vérifié. La seule 
différence est que le doublement porte maintenant sur des valeurs absolues 
plus élevées. Price propose ainsi une explication historique pour l’anhis-
toricité de la conscience des scientifiques : 90 % de tous les scientifiques 
ayant jamais vécu sont aujourd’hui vivants, et autant de travail scienti-
fique va être produit au cours des dix ou vingt années qui viennent qu’il 
en a été produit dans toute l’histoire antérieure. La science efface la 
conscience de son passé, ou tout du moins la conscience de la pertinence 
du passé – mais il en a toujours été ainsi (Charles P. Snow écrivait en 1959 
que les scientifiques étaient des personnes qui avaient le « futur vissé 
au corps 1 » ou, dit autrement, qui de manière unique ne s’intéressaient 
pas au passé). Price normalise ainsi à la fois les aspects institutionnels 
du changement scientifique et l’idée des scientifiques selon laquelle le 
changement qu’ils vivent n’est pas historiquement normal. Dans le même 
temps, il soutient que des aspects de ce mode normal de changement ne 
peuvent échapper à la courbe de croissance produite par cette augmen-
tation spectaculaire. D’ailleurs, si la croissance devait se poursuivre ainsi, 
« nous aurions deux scientifiques pour chaque homme, femme, enfant et 
chien de la population… La fin du monde scientifique est ainsi distante 
de moins d’un siècle dans le temps 2 ».
Price ne s’intéresse pas beaucoup aux différentes catégories de personnel 
scientifique, étant persuadé que tous les indices de la science suivent des 
modèles de changement similaires. Cependant, les exercices bureaucra-
tiques / statistiques de la période de l’après-guerre reconfigurent l’image 
du scientifique de manière fondamentale. La notion conjointe de R & D 
(recherche et développement) apparaît, peut-être pour la première fois, 
dans le rapport Steelman de 1947 – qui fait probablement lui-même 
1. Snow 1993 [1959] (p. 10).
2. Price 1986 [1963] (p. 7-17, citation p. 17). Price avait introduit ces notions quelques années 
auparavant, lors de séminaires donnés à Yale en 1959.

36	
steven shapin
référence au Bureau de recherches et de développement scientifiques 
(OSRD), dirigé par Vannevar Bush pendant la guerre. La manière dont  
est institutionnalisée la catégorie R & D n’a pas encore été clairement 
déterminée, même si Godin et Lane situent son émergence dans la période 
d’après guerre dans un contexte de préoccupations politiques à propos 
de l’association de la catégorie industrielle du « développement » avec 
la catégorie essentiellement universitaire de « recherche ». La R & D est 
utilisée dans le cadre des efforts du gouvernement – notamment dans 
le rapport Science, the Endless Frontier – pour justifier un soutien accru 
à la recherche grâce à la valorisation des résultats du développement. 
L’apparition du prétendu « modèle linéaire » de l’innovation, dans lequel 
la recherche « pure » ou « fondamentale » est identifiée comme le premier 
élément d’une séquence de causalités aboutissant à une amélioration des 
biens et services matériels, constitue la contribution des économistes au 
compromis d’après guerre entre la science et l’État 1.
Le « QSE » comme nouveau scientifique modèle
Qui est considéré comme un scientifique dans le cadre des statistiques 
collectées par les administrations ? Avant guerre, la catégorie du scientifique 
est distinguée de celle du technologue, tout comme il est régulièrement 
rappelé que la bien nommée « étude scientifique » n’a rien à voir avec  
les résultats matériels que l’on peut légitimement en attendre. Comparée 
à la technologie, la science poursuit des objectifs distincts, fait appel à des 
capacités cognitives spécifiques et est sujette à des modes d’évaluation 
différents. Mais le compromis d’après guerre poursuit la normalisation du 
scientifique en associant des activités et des intentions auparavant consi-
dérées comme distinctes. Avant de donner le nombre de scientifiques 
et leur type, toutes les entreprises statistiques doivent s’accorder sur les 
limites à établir entre les travaux scientifiques et les autres types d’activité. 
Des décisions pratiques doivent être prises pour savoir si les « scienti-
fiques » (« travailleurs scientifiques », « personnel de recherche », etc.) 
sont des gens qui peuvent être identifiés de manière fiable par le type de 
travail qu’ils effectuent, par les institutions dans lesquelles ils travaillent, 
par l’éducation qu’ils peuvent avoir reçue et, dans ce cas, le type et  
la durée des études. Dans le même temps, des distinctions doivent être 
effectuées entre les différentes sortes de travail scientifique et les affilia-
tions correspondantes. Les spécialistes des « sciences sociales » sont-ils 
1. Steelman 1947 (t. 1, p. 9-13), Godin et Lane 2011, Godin 2008c, Edgerton 2004.

	
figures de scientifiques	
37
des scientifiques 1 ? Les enseignants en science doivent-ils être considérés 
comme les scientifiques faisant de la recherche ? Qu’en est-il des personnes 
ayant reçu une formation scientifique avancée mais qui ne travaillent 
pas, à proprement parler, dans un laboratoire au moment t ? Que dire 
également de cette partie de leur journée de travail que les scientifiques 
universitaires ne passent pas au laboratoire, ou encore des chercheurs 
de l’industrie qui passent tout leur temps au laboratoire ?
C’est dans ce contexte de l’après-guerre qu’apparaît un nouvel acteur 
à la fois dans la conscience culturelle et dans la pratique administrative. 
Il s’agit du « scientifique et ingénieur qualifié », rapidement connu sous 
son acronyme anglais de QSE pour « Qualified Scientist and Engineer ». 
Les origines précises du QSE sont incertaines, notamment parce que le 
terme est déjà utilisé avant la guerre, dans des contextes variés, mais pas 
en tant que « terme consacré », simplement comme un moyen pour faire 
référence aux différentes catégories de personnes dont l’expertise a été 
certifiée de manière institutionnelle. Si l’armée et le Bureau de l’éducation 
(Office of Education) des États-Unis conservent, dans les années 1940, 
un tableau de service des personnels scientifiques et spécialisés (Roster 
of Scientific and Specialized Personnel), et si la NSF est mandatée à sa 
création pour compiler un registre du personnel scientifique et technique 
(Register of Scientific and Technical Personnel), la catégorie adminis-
trative typiquement moderne du QSE semble apparaître dans le contexte 
des compilations statistiques d’après guerre britanniques, avant de se 
retrouver dans des enquêtes sur la main-d’œuvre scientifique de l’Orga-
nisation européenne de coopération économique (OECE) issue du plan 
Marshall, avant de devenir une norme à l’Organisation de coopération et 
de développement économiques (OCDE) à partir des années 1960 2. Le 
rapport du comité Barlow à la Chambre des communes en 1946 propose 
des chiffres sur ceux qui sont appelés les « scientifiques qualifiés », définis 
comme les « diplômés [quel que soit le niveau] en mathématiques, physique, 
chimie et biologie », manipulant cette définition en y ajoutant « un petit 
nombre d’hommes et de femmes qui, sans être diplômés de l’Université, 
sont membres d’institutions scientifiques reconnues à des niveaux consi-
dérés comme équivalents à un diplôme universitaire sur ces sujets ». 
Tout en soulignant ensuite « la relation étroite entre la science pure et les  
différentes branches de l’ingénierie », le rapport Barlow ne propose « aucune 
1. Solovey 2012.
2. National Resources Planning Board 1943, NSF 1951 (p. 29), Kelley 1953 (p. 3), Godin 2005 
(p. 187 et 249-251), Organisation for European Economic Co-operation 1955, Cockcroft 1965 
(p. 30-32, 39 et 51).

38	
steven shapin
estimation » du nombre d’ingénieurs et de technologues 1. Deux ans plus 
tard, le premier rapport annuel du Conseil consultatif sur la politique 
scientifique (First Annual Report of the Advisory Council on Scientific 
Policy) appelle la Grande-Bretagne à adopter des politiques garantissant 
un « approvisionnement adéquat » dans une catégorie unifiée de « scien-
tifiques et technologues qualifiés 2 ». C’est probablement vers le milieu 
et la fin des années 1950 que l’administration britannique s’accorde sur 
l’usage de la catégorie QSE (comme une entité dont le nombre est suivi 
attentivement par l’administration), répartissant de manière séparée les 
scientifiques et les ingénieurs, tout en proposant une définition formelle 
de l’appellation « scientifiques et ingénieurs qualifiés » pour inclure les 
« diplômés d’université et les titulaires d’un diplôme de technologie 
[délivré par le Conseil national des diplômes technologiques] », ainsi que 
les enseignants des « collèges » techniques et les membres d’entreprises 
appartenant à des institutions professionnelles telles que l’Institut des 
ingénieurs en mécanique et l’Institut royal de chimie 3.
Tout au long des années 1960 et par la suite, les recueils statistiques 
continuent à faire la différence entre les sous-espèces scientifiques et 
technologiques – indiquant par exemple le nombre de chimistes, de biolo-
gistes, d’ingénieurs en chimie et d’ingénieurs en électricité. Pourtant, c’est 
le rassemblement des catégories représentées par le scientifique et l’ingé-
nieur (ou technologue) qui est historiquement remarquable, au moment 
où les relations causales sont explicitement mises en lumière entre les 
catégories de science pure, science appliquée et technologie. Ainsi, la 
normalisation de la figure du scientifique est importante à la fois politi-
quement et culturellement. D’un point de vue culturel, le regroupement de 
la catégorie du scientifique avec celle de l’ingénieur constitue une re-spé-
cification de l’objet de l’étude scientifique et, d’un point de vue politique, 
il s’agit d’une justification publique des études apparemment sans objet. 
Après avoir été célébrée, l’absence d’objet est désormais dissoute politi-
quement dans une culture de résultats pouvant finalement être évalués 
1. Scientific Man-Power 1946 (p. 3 et 10). Il faut noter que les diplômes de premier degré des 
universités britanniques, plus spécialisés, correspondent à davantage de compétences et de 
connaissances pertinentes sur des sujets précis que le diplôme de licence (bachelor’s degree), 
portant généralement sur une base disciplinaire plus large, des universités états-uniennes.
2. Advisory Council on Scientific Policy 1948 (p. 12-14).
3. Advisory Council on Scientific Policy 1959 (p. 1-4 et 30). Au milieu des années 1950, l’usage 
en vigueur à l’OECE était évidemment moins formel, la catégorie dans laquelle il fallait répondre 
à la pénurie étant désignée sous le nom de « scientifiques et ingénieurs hautement qualifiés ». 
Cf. OEEC 1958. Ce rapport soulignait les différences existant entre les pays à propos de qui 
était considéré comme un scientifique ou comme un ingénieur, mais il affinait le problème, 
tout en soulignant l’importance de la sensibilisation de l’opinion quant à la valeur unique de 
telles personnes pour la croissance économique et la sécurité nationale (par exemple p. 9 et 23).

	
figures de scientifiques	
39
avec des outils économétriques. En termes de soutien public au travail 
du scientifique, l’idée de recherche sans objet n’est pas envisageable et les 
scientifiques se retrouvent, via cette normalisation, dans le même cadre 
que d’autres acteurs importants d’un point de vue économique.
Une hybridité vigoureuse : le scientifique  
à la fin du xxe siècle et au-delà
Maintenant que nous avons une idée de comment, pourquoi et à 
quelles fins les scientifiques sont comptés, nous pouvons revenir sur les 
aspects qualitatifs de leur identité. Ici encore s’avère pertinente la notion 
de normalisation, à savoir l’association de l’identité des scientifiques à 
celle d’autres rôles reconnus de manière courante dans la vie civile. Les 
scientifiques ont été impliqués dans des affaires commerciales depuis 
longtemps, mais ce qui est nouveau à la fin du xxe siècle et au début 
du xxie est l’idée selon laquelle l’objectif et la motivation réelle du travail 
des scientifiques sont, et doivent être, un résultat commercial – une idée 
qui se répand non seulement dans l’opinion publique et chez les bailleurs 
de fonds, mais aussi parmi les scientifiques eux-mêmes. La figure de 
l’ingénieur entrepreneur remonte au moins à la révolution industrielle ; 
celle du scientifique employé de l’industrie de haute technologie au moins 
aux grandes entreprises allemandes de la chimie de la seconde partie du 
xixe siècle. Mais le scientifique entrepreneur devint une figure reconnue 
dans la culture courante seulement avec l’émergence des start-up de 
l’électronique dans la période de l’après-Seconde Guerre mondiale, et  
des sociétés de biotechnologie à partir des années 1970, à commencer par 
des entreprises financées par du capital-risque comme Genentech en 1976 
et Biogen en 1978. La construction et l’acceptation de cette figure sont 
ensuite renforcées par la montée en puissance des bureaux de transferts 
de technologies des universités américaines qui gèrent et encouragent 
la propriété intellectuelle (PI) commercialisable et, dans une certaine 
mesure, l’essaimage de sociétés start-up brevetant de la PI professorale 
et faisant appel à l’implication de professeurs 1. Alors que l’acceptation 
du rôle hybride de scientifique et homme d’affaires se limitait autrefois 
aux cercles industriels, à partir des années 1970 et 1980 cette hybridité 
est largement reconnue dans la norme et activement encouragée par les 
gouvernements, en tant que financeurs ultimes de la recherche.
Le séquençage du génome humain en 2000 a été célébré comme 
1. Hughes 2011 ; voir aussi Shapin 2008b (chap. 6-8).

40	
steven shapin
l’aboutissement de l’organisation scientifique, comme l’exemple paradig-
matique des champs de force institutionnels dans lesquels se retrouvent les 
scientifiques. Le projet Génome humain est une entreprise hybride tout 
du long – présentée comme une « course » entre entreprises « publiques » 
(gouvernementales) et « privées » (commerciales et quasi commerciales), 
comme une réalisation de science pure, et comme une entreprise dont 
les objectifs ultimes sont la production de médicaments rentables et 
l’émergence d’une nouvelle espèce d’entreprises de biotechnologie pour 
un marché mondial. Les connaissances acquises grâce au projet Génome 
appartiennent à la science biologique, mais les moyens utilisés pour les 
produire dépendent complètement des nouveaux instruments technolo-
giques de séquençage du génome et de nouvelles formes d’organisation 
du travail scientifique et technique. L’initiative « privée » est elle-même 
hybride d’un point de vue institutionnel, avec un emboîtement complexe 
d’entreprises commerciales et d’entités à but non lucratif qui brevettent 
les séquences de gènes découvertes pour leurs homologues commerciaux. 
Le chef de file de l’initiative publique américaine est un fervent chrétien 
(Francis Collins) et les instituts fédéraux nationaux en charge de la santé 
(National Institutes of Health, NIH, évidemment à but non lucratif) 
assurent la sécurisation des brevets sur les séquences de gènes ; ce qui 
irrite la personne à la tête de l’entreprise privée, le biologiste (athée) John 
Craig Venter, qui décide de quitter les NIH et de se mettre à son compte 1. 
Venter devient alors la figure emblématique de la science commerciale et 
le premier milliardaire des biotechnologies. (En 2004, le magazine Business 
Week consacre Venter comme l’un des « grands innovateurs » d’Amérique, 
le faisant poser pour l’occasion avec une blouse blanche de laboratoire 
sur la partie droite du corps et un complet-veston d’homme d’affaires 
sur le côté gauche, dans une chimère idéale, et typiquement contempo-
raine, du commerce et de la connaissance scientifique 2.) Cependant, des 
tensions sont réelles entre les objectifs de Venter et les intérêts commer-
ciaux liés à l’initiative privée, et ce dernier finit par être licencié. Venter 
devient ensuite une figure de proue du domaine émergent de la biologie 
synthétique, prenant la tête d’un autre assemblage d’entités commer-
ciales et à but non lucratif, dans une entreprise scientifique organisée 
pour produire des résultats qui sont imaginés à la fois en termes commer-
ciaux et avec des visées altruistes.
Au milieu du xxe siècle, et sans doute longtemps après sa mort en 1955, 
1. Pour l’histoire scientifique et institutionnelle du projet Génome, voir Shreeve 2004, Sulston 
et Ferry 2002.
2. Pour cette image voir Shapin 2008b (p. 224), ainsi que < http://sciencecomm.wikispaces.
com/UNIT + 2_J.+ Craig + Venter > .

	
figures de scientifiques	
41
Albert Einstein constitue la figure emblématique du scientifique et son 
visage rétroéclairé représente pour un grand nombre ce que cela veut dire 
d’être un scientifique, à savoir un personnage distrait, échevelé, ascétique, 
plein de bonté et las de ce monde – un homme dont les recherches 
théoriques ont un lien avec l’invention d’une bombe atomique mais qui, 
à la nouvelle de son utilisation sur Hiroshima, ne peut que prononcer les 
mots de yiddish « Oy vey 1 ». Aucune figure n’a supplanté Einstein comme 
incarnation de la science moderne, mais il est certain que nous pouvons 
voir un candidat en la personne de Craig Venter, un homme qui, comme 
dit le proverbe, dîne avec le diable mais soutient qu’il a une cuillère suffi-
samment longue pour résister aux tentations, un homme qui bombe le 
torse en brandissant son autobiographie dans laquelle il se présente à  
la fois comme un génie ambitieux de la technologie et de l’organisation 
et comme un gardien ultratraditionnel du temple de l’individualisme et 
de l’indépendance qu’il considère comme essentiels pour l’idée même de 
science 2. Mais il existe un autre prétendant au statut d’icône scientifique 
contemporaine, en la personne d’un spécialiste de physique théorique qui 
a pris sa retraite après avoir occupé la même chaire de mathématiques 
qu’Isaac Newton à Cambridge 3. Désincarné au sens figuré, et pratiquement 
non incarné au sens physique, semblant venu d’un autre monde, presque 
céleste, Stephen Hawking représente la continuation d’une persona scien-
tifique restée pratiquement inchangée depuis celle du prêtre de la nature, 
ascétique et pensif, qui existait au xviie siècle. La figure du scientifique 
moderne est donc un work in progress dont le statut actuel témoigne de 
la manière incohérente avec laquelle nous considérons ceux qui révèlent 
les réalités de la nature et à qui nous accordons un pouvoir potentiel très 
grand. Si Venter représente l’avant-garde du changement institutionnel 
et culturel de la figure du scientifique, cette avant-garde elle-même ne 
représente qu’une partie de la réalité contemporaine. La figure de Venter 
est présentée comme une incarnation de l’hybridité entre scientifique et 
technologue, entre travail intellectuel pur et objectifs commerciaux et 
civiques. On pourrait dire que le nouveau scientifique modèle est comme 
tout le monde, voire un peu plus. Mais la figure d’Hawking est là pour 
rappeler les limites actuelles de cet effondrement et de cette hybridité.
Traduit par Cyril Le Roy
1. « Ô malheur ! » Cité par Nathan et Norden 1960 (p. 308).
2. Venter 2007.
3. Mialet 2012.

42	
steven shapin
 
Références bibliographiques
Advisory Council on Scientific Policy 1948, First Annual Report of the Advisory 
Council on Scientific Policy (1947-1948), Cmnd. 7465, Londres, His  Majesty’s 
Stationery Office.
–	 1959, Scientific and Engineering Manpower in Great Britain, Cmnd. 902, Londres, 
His Majesty’s Stationery Office.
Bouchard Julie, 2006, « A  Prehistory of Statistics on Science and Technology in 
France : From Inventories to Statistics », < http://juliebouchard.online.fr/articles- 
pdf/2006c-bouchard-statistics.pdf > (consulté le 6 janvier 2015).
Buck Peter, 1985, « Adjusting to Military Life : The Social Sciences Go to War (1941-
1950) », in Merritt Roe Smith (dir.), Military Enterprise and Technological Change : 
Perspectives on the American Experience, Cambridge (MA), MIT Press, p. 205-252.
Bush Vannevar, 1995 [1945], Science, the Endless Frontier : A Report to the President 
on a Program for Postwar Scientific Research, Washington (DC), National Science 
Foundation.
Chargaff Erwin, 1978, Heraclitean Fire : Sketches from a Life before Nature, New 
York, Rockefeller University Press.
Cockcroft Sir John (dir.), 1965, The Organization of Research Establishments, 
Cambridge, Cambridge University Press.
Cohen-Cole Jamie, 2013, The Open Mind : Cold War Politics and the Sciences of 
Human Nature, Chicago (IL), University of Chicago Press.
Daston Lorraine, 1998, « Fear and Loathing of the Imagination in Science », 
Daedalus, vol. 127, hiver, p. 73-95.
Daston Lorraine et Galison Peter, 1992, « The Image of Objectivity », Representa-
tions, vol. 40, automne, p. 81-128.
Daston Lorraine et Sibum H. Otto, 2003, « Introduction : Scientific Personae and 
Their Histories », Science in Context, vol. 16, p. 1-8.
Edgerton David, 2004, « The “Linear Model” Did Not Exist : Reflections on the 
History and Historiography of Science and Research in Industry in the Twentieth 
Century », in  Karl Grandin, Nina Wormbs et Sven Widmalm, The  Science- 
Industry Nexus : History, Policy, Implications, Canton (MA), Science History Publi-
cations, p. 31-57.
Eisenhower Dwight D., 1972 [1961], « Farewell Address [17  January 1961] », 
in Carroll W. Pursell (dir.), The Military-Industrial Complex, New York, Harper 
& Row,, p. 204-208.
Elkana Yehuda, Lederberg Joshua, Merton Robert K., Thackray Arnold et 
Zuckerman Harriet (dir.), 1978, Toward a Metric of Science : The Advent of Science 
Indicators, New York, John Wiley.
Foucault Michel, 1980, « Truth and Power », in  Colin Gordon (dir.), Power  / 
Knowledge : Selected Interviews and Other Writings (1972-1977), New York, 
Pantheon, p. 109-133.
Godin Benoît, 2002, « The Numbers Makers : Fifty Years of Science and Technology 
Official Statistics », Minerva, vol. 40, p. 375-397.
–	 2005, Measurement and Statistics on Science and Technology : 1920 to the Present, 
Londres, Routledge.
–	 2007a, « From Eugenics to Scientometrics : Galton, Cattell, and Men of Science », 
Social Studies of Science, vol. 37, p. 691-728.

	
figures de scientifiques	
43
–	 2007b, « What Is Science ? Defining Science by the Numbers (1920-2000) », 
Working Paper no. 35, Project on the History and Sociology of S & T Statistics, 
< http://www.csiic.ca/PDF/Godin_35.pdf > .
–	 2008a, « The Emergence of Science and Technology Indicators : Why Did Govern-
ments Supplant Statistics with Indicators ? », Working Paper no. 8, Project on the 
History and Sociology of S & T Statistics, < http://www.csiic.ca/PDF/Godin_8.pdf 
> .
–	 2008b, « The Making of Statistical Standards : The OECD and the Frascati Manual 
(1962-2002) », Working Paper no. 39, Project on the History and Sociology of STI 
Statistics, < http://www.csiic.ca/PDF/Godin_39.pdf > .
–	 2008c, « In the Shadow of Schumpeter : W.  Rupert Maclaurin and the Study of 
Technological Innovation », Working Paper no.  2, Project on the Intellectual 
History of Innovation, < http://www.csiic.ca/PDF/IntellectualNo2.pdf > .
–	 2009, « The Culture of Numbers : The Origins and Development of Statistics on 
Science », Working Paper no.  40, Project on the History and Sociology of STI 
Statistics, < http://www.csiic.ca/PDF/Godin_40.pdf > .
Godin Benoît et Lane Joseph, 2011, « Forschung oder Entwicklung ? Eine kurze 
Darstellung zweier Kategorien der Wissenschaftsforschung », Gegenworte, no 26, 
automne, p. 44-48.
Greenberg Daniel S., 2001, Science, Money, and Politics : Political Triumph and 
Ethical Erosion, Chicago (IL), University of Chicago Press.
Haynes Roslynn D., 1994, From Faust to Strangelove : Representations of the Scientist 
in Western Literature, Baltimore (MD), Johns Hopkins University Press.
Herzig Rebecca, 2005, Suffering for Science : Reason and Sacrifice in Modern America, 
New Brunswick (NJ), Rutgers University Press.
Hughes Sally Smith, 2011, Genentech : The Beginnings of Biotech, Chicago (IL), 
University of Chicago Press.
Kaiser David, 2002, « Scientific Manpower, Cold War Requisitions, and the 
Production of American Physicists after World War II », Historical Studies in the 
Physical and Biological Sciences, no 33, p. 131-159.
–	 2004, « The Postwar Suburbanization of American Physics », American Quarterly, 
no 56, p. 851-888.
–	 2006, « The Physics of Spin : Sputnik Politics and American Physicists in the 1950s », 
Social Research, no 73, p. 1225-1252.
–	 2015 [à paraître en], American Physics and the Cold War Bubble, Chicago (IL), 
University of Chicago Press.
Kelley Harry C., 1953, « National Register of Scientific and Technical Personnel », 
Science, vol. 118, no 3063, 11 septembre, p. 3.
Kevles Daniel J., 1977, The Physicists : The History of a Scientific Community in 
Modern America, New York, Alfred A. Knopf.
Kohler Robert E., 1994, Lords of the Fly : Drosophila Genetics and the Experimental 
Life, Chicago (IL), University of Chicago Press.
Kuhn Thomas S., 1963, « The Essential Tension : Tradition and Innovation in Scien-
tific Research », in Calvin W. Taylor et Frank Barron, Scientific Creativity : Its 
Recognition and Development. Selected Papers from the Proceedings of the First, 
Second, and Third University of Utah Conferences : « The Identification of Creative 
Scientific Talent », Huntington (NY), Robert E. Krieger, p. 341-354.
LaFollette Marcel C., 1990, Making Science Our Own : Public Images of Science 
(1910-1955), Chicago (IL), University of Chicago Press.

44	
steven shapin
Lemov Rebecca, 2010, « “Hypothetical Machines” : The Science Fiction Dreams of a 
Cold War Social Science », Isis, no 101, p. 401-411.
Mialet Hélène, 2012, Hawking Incorporated : Stephen Hawking and the Anthro-
pology of the Knowing Subject, Chicago (IL), University of Chicago Press.
Nathan Otto et Norden Heinz (dir.), 1960, Einstein on Peace, New York, Simon & 
Schuster.
National Resources Planning Board, 1943, Report of the National Roster of 
Scientific and Specialized Personnel (June 1942), Washington (DC), Government 
Printing Office.
NSF, 1951, First Annual Report of the National Science Foundation (1950-1951), 
Washington (DC), Government Printing Office.
–	 1973, Science Indicators 1972  : Report of the National Science Board, National 
Science Foundation, Washington (DC), Government Printing Office.
Oppenheimer J.  Robert, 1955 [1947], « Physics in the Contemporary World », in   
J. Robert Oppenheimer, The Open Mind, New York, Simon & Schuster, p. 81-102.
OEEC (Organisation for European Economic Co-operation), 1955, Manpower 
Committee, Shortages and Surpluses of Highly Qualified Scientists and Engineers in 
Western Europe : A Report, Paris, OEEC.
–	 1958, The Problem of Scientific and Technical Manpower in Western Europe, 
Canada, and the United States, Paris, OEEC.
Price Derek John de Solla 1986 [1963], Little Science, Big Science… and Beyond, New 
York, Columbia University Press.
Price Don K., 1965, The Scientific Estate, Cambridge (MA), Harvard University Press.
Scientific Man-Power, 1946 : Scientific Man-Power, Report of a Committee Appointed 
by the Lord President of the Council, Cmd. 6824, Londres, His Majesty’s Stationery 
Office.
Shapin Steven, 2003, « The Image of the Man of Science », in  Roy Porter (dir.), 
Eighteenth-Century Science, t. 4 de The Cambridge History of Science, Cambridge, 
Cambridge University Press, p. 159-183.
–	 2006, « The Man of Science », in Lorraine Daston et Katharine Park (dir.), Early 
Modern Science, t. 3 de The Cambridge History of Science, Cambridge, Cambridge 
University Press, p. 179-191.
–	 2008a, « The Scientist in 2008 », Seed Magazine, no 19, décembre, p. 58-62.
–	 2008b, The Scientific Life : A Moral History of a Late Modern Vocation, Chicago (IL), 
University of Chicago Press.
Shreeve James, 2004, The Genome War : How Craig Venter Tried to Capture the Code 
of Life and Save the World, New York, Alfred A. Knopf.
Smyth Henry D., 1951, « The Stockpiling and Rationing of Scientific Manpower », 
Bulletin of the Atomic Scientists, vol. 7, no 2, février, p. 38-42 et 64.
Snow Charles P., 1993 [1959], The Two Cultures and the Scientific Revolution, 
Cambridge, Cambridge University Press.
Solovey Mark, 2012, « Senator Fred Harris’s Effort to Create a National Social 
Science Foundation : Challenge to the US National Science Establishment », Isis, 
vol. 103, p. 54-82.
–	 2013, Shaky Foundations : The Politics-Patronage-Social Science Nexus in Cold War 
America, New Brunswick (NJ), Rutgers University Press.
Steelman John R., 1947, Science and Public Policy : A Report to the President, The 
President’s Scientific Research Board, Washington (DC), Government Printing 
Office, 5 vol.

	
figures de scientifiques	
45
Stevens Hallam, 2013, Life Out of Sequence : A Data-Driven History of Bioinformatics, 
Chicago (IL), University of Chicago Press.
Sulston John et Ferry Georgina, 2002, The Common Thread : A Story of Science, 
Politics, Ethics and the Human Genome, New York, Bantam.
Venter J. Craig, 2007, A Life Decoded : My Genome, My Life, New York, Viking.
Weinberg Alvin M., 1961, « Impact of Large-Scale Science on the United States », 
Science, vol. 134, no 3473, 21 juillet, p. 161-164.
Whyte William H., 1956, The Organization Man, New York, Simon & Schuster.


2 Sciences et guerres
A n n e  R a s m u s s e n
Que les sciences jouent un rôle central dans le déroulement des guerres 
au cours du xxe siècle, les contemporains en ont été convaincus dès  
la Grande Guerre : « La guerre, à mesure qu’elle se prolonge, prend de 
plus en plus le caractère d’une lutte de science et de machine », constate 
en novembre 1915 le mathématicien Paul Painlevé, bientôt ministre de 
la Guerre. Dans le camp adverse, l’écrivain combattant Ernst Jünger 
se fait le contempteur du nouveau visage de la guerre moderne – la 
Materialschlacht (lutte de matériel) – qui, lors de la bataille de la Somme, 
traduit la domination de la machine sur l’homme et exprime « l’âme de 
la guerre scientifique ». La perception que les sciences, et les techniques 
embarquées dans les « machines », sont partie prenante de la conduite des 
guerres et, plus encore, sont devenues décisives dans leur issue, n’a fait 
que croître au long du siècle. Les étapes de ce processus ont été franchies 
crescendo du projet Manhattan à la surenchère technologique de la course 
aux armements constitutive de la guerre froide. Dans la logique de la 
dissuasion où la sécurité du pays dépend de la puissance de son arsenal, 
les techniques de pointe logées dans les têtes nucléaires, les missiles balis-
tiques et les satellites sont la condition de possibilité même du conflit. Un 
lien indissoluble associe, dans la seconde moitié du xxe siècle, les armes 
et le pouvoir, les prouesses de la science et le prestige national.
Les historiens se sont employés à rendre compte de ces interactions 
entre sciences et guerre. Dans quelle mesure l’activité scientifique et 
les productions techniques ont-elles influencé le cours des conflits ? 
Telle est la question centrale qu’ont longtemps posée les histoires de la 
guerre dans une pesée stratégique des effets des sciences sur l’issue des 
combats. Une telle grille de lecture procède de l’affirmation d’un pacte 
noué entre les sciences et la guerre, devenue un cliché emblématique 
d’une forme de représentation de la modernité saisie dans la longue durée. 
Ce pacte aurait été souscrit à la Renaissance en conséquence du projet 
 Maurice Busset, Bombardement de Ludwigshafen, 1918.

48	
anne rasmussen
assigné aux sciences modernes de transformer la nature et de conférer 
aux hommes la puissance fondée sur la connaissance. Les pouvoirs 
enrôlent les sciences à des fins militaires pour rendre plus performante 
leur action sur le territoire et sur les hommes, comme l’attestent quatre 
siècles de développement des « armes savantes » puis des « sciences de 
l’ingénieur ». Ce projet dont l’histoire est longue est cependant loin d’être 
invariant et a même semblé changer de nature dans la période contem-
poraine, sous le double effet des mutations de l’entreprise scientifique et 
de la totalisation des conflits. Au cours du xxe siècle, les sciences sont 
devenues « un élément central des dispositifs d’innovation, un outil 
essentiel du dispositif productif de masse, et un moyen de la rationali-
sation bureaucratique 1 » : trois fonctions qui les ont rendues nécessaires 
à la préparation de la guerre de l’âge industriel dont la spécificité a été 
de conjuguer elle aussi ces trois fonctions – innovation, production, 
rationalisation. Les guerres du xxe siècle et les violences extrêmes dont 
elles ont été porteuses ont été analysées par des penseurs de l’après-­
Holocauste comme le produit d’une rationalité technique et instrumentale 
au service d’un projet d’ingénierie sociale 2. Dans cette perspective, la 
brutalisation induite par la guerre n’apparaît pas en contradiction avec 
le processus de civilisation que l’idéologie progressiste, nourrie de la 
référence à la science, a mise en œuvre – elle ne traduit pas une régression 
à un stade de barbarie – mais, au contraire, ce sont les valeurs et les 
pratiques de la modernité administrative et productive, et ses institutions 
scientifiques, technologiques et bureaucratiques, qui en fondent les condi-
tions de possibilité. Dans cette dynamique, les sciences ont occupé une  
place centrale.
Les études sociales des sciences ont mis en valeur, comme l’ont souligné 
John Krige et Dominique Pestre, que les sciences fondaient, au-delà de la 
production des savoirs, des « systèmes d’action » qui pèsent sur la nature 
et la société, des « systèmes de valeur » qui inspirent des normes et des 
idéaux, des « systèmes de représentations sur lesquels reposent discours 
et postures d’autorité 3 ». À ce titre, envisager les relations mutuelles entre 
sciences et guerres, c’est certes envisager la manière dont les sciences ont 
contribué à définir les moyens et les enjeux des conflits au xxe siècle et 
se sont trouvées en retour façonnées par eux, mais c’est aussi prendre 
en compte les multiples formes d’investissement de la « science » dans 
la guerre : mettre au point des gaz de combat dans des laboratoires ou 
1. Pestre 2003 (p. 40).
2. Bauman 2002, Traverso 2009.
3. Krige et Pestre (dir.) 1997 (p. xxii).

	
sciences et guerres	
49
participer au projet Manhattan, expérimenter en contexte opérationnel, 
travailler en tant qu’universitaire dans des agences militaires, réformer 
les institutions de recherche pour l’après-guerre, lutter pour le leadership 
national dans le monde savant, signer ou refuser de signer des manifestes, 
autant de manières de faire la guerre en tant que scientifique, en convo-
quant les savoirs et les dispositifs d’action, les ressources symboliques et 
l’éthos de ceux qui font profession de science.
Deux lectures temporelles
L’histoire contemporaine des relations entre sciences et guerre est 
généralement lue à travers deux prismes temporels. L’un inscrit cette 
histoire dans un long xxe siècle prenant racine dans le dernier tiers du xixe 
avec les premiers conflits d’après la révolution industrielle, inaugurés  
avec la guerre civile américaine, puis la guerre des Boers, la guerre 
russo-japonaise et les deux guerres balkaniques, qui font la preuve de 
la puissance nouvelle du feu – artillerie lourde et mitrailleuses. L’autre 
prisme temporel met l’accent sur les conséquences de la Seconde 
Guerre mondiale et sur la guerre froide, deux conflits vus comme les 
catalyseurs d’une mutation radicale des pratiques scientifiques et d’un 
développement accéléré des sciences et des techniques dans les sociétés  
occidentales 1.
La première lecture, à l’échelle d’un siècle et demi, insiste sur le legs de 
la construction des États – où la « science » a été convoquée pour former 
à la raison des citoyens éclairés – et de la nationalisation des sociétés 
européennes 2. Les sciences participent d’une entreprise nationale, au sens 
où la nation fournit le cadre privilégié de leur développement, de leur 
financement et des carrières, tandis qu’en retour les sciences contribuent 
à l’affirmation de la puissance économique et de la grandeur. Elles sont 
ainsi devenues un puissant levier de la compétition nationale renforcé 
par l’investissement des États dans les universités, les écoles techniques 
et les laboratoires publics. En contrepartie de ce soutien, les scientifiques 
occupent des fonctions technocratiques et mobilisent les savoirs au profit 
de la cause nationale 3. Comme l’exprime au début du siècle le physicien 
et physiologiste allemand Hermann von Helmholtz, les savants doivent 
former « comme une armée organisée », qui travaille « pour le bien de la 
1. Dahan et Pestre (dir.) 2004.
2. Edgerton 2005.
3. Harrison et Johnson (dir.) 2009.

50	
anne rasmussen
nation tout entière […] sur son ordre et à ses frais 1 ». Il prévoyait, avec 
justesse, que la science aurait pour principale caractéristique au xxe siècle 
d’être au service de la nation en guerre.
Cette lecture met l’accent sur le moment 14-18 comme avènement 
d’une nouvelle façon de conduire la guerre où les sciences et leurs « appli-
cations » tiennent un rôle de premier plan. Ce moment est considéré 
comme la matrice de la totalisation des conflits qui advient au xxe siècle, 
désignant l’intégration dans le processus guerrier de tous les secteurs  
de l’activité et du système productif. Les logiques de totalisation, engagées 
dès 1914-1915 2, rendent poreuse la frontière entre civil et militaire et, à 
ce titre, sont un enjeu pour les usages des sciences. Elles signifient d’abord 
engagement des sociétés globales. Sur le front des sciences, la mobili-
sation du savant dans son laboratoire au service du champ de bataille en 
est symptomatique. Avec la fin des espoirs d’une guerre courte, nombre 
de professionnels des sciences réclament une mobilisation spécifique 
et leur intégration dans des institutions de défense, convaincus comme 
l’écrivain H.G. Wells « que la victoire ne pourra revenir qu’à l’emploi le 
plus intensif des meilleures compétences scientifiques 3 ». Les formes 
de collaboration entre militaires et scientifiques en furent durablement 
modelées.
La totalisation élargit ensuite la sphère des victimes de la guerre, par la 
capacité de nuisance croissante des systèmes d’armes et leur extension à 
de nouvelles cibles qui sont désormais les populations dans leur ensemble. 
En permettant de tuer à distance, la Grande Guerre inaugure, via les 
premiers bombardements aériens, les dégâts que les conflits du xxe siècle 
causeront aux civils. D’autres modalités suivront, tels les bombardements 
dits stratégiques de la Seconde Guerre mondiale – le terme désigne 
l’objectif d’atteindre les industries de guerre autant que le moral des 
populations –, l’usage des armes chimiques dans la guerre du Vietnam et 
le conflit Iran-Irak, ou, point d’orgue de la violence indistincte pratiquée 
à l’encontre des sociétés dans leur entier, la bombe atomique à Hiroshima 
et Nagasaki en août 1945. Les civils ont ainsi compté plus de la moitié 
des victimes du second conflit mondial 4.
La mise en ordre de bataille idéologique est un troisième aspect de la 
totalisation née de la guerre moderne. Celle-ci n’est pas réductible à ses 
enjeux militaires, ni même économiques, mais elle est aussi une confron-
tation de systèmes de savoir et de valeurs. Les belligérants de 1914 affirment 
1. Cité par Crawford 1992 (p. 35).
2. Horne (dir.) 2010.
3. H.G. Wells, courrier des lecteurs, The Times, 11 juin 1915.
4. Lindqvist 2012, Traverso 2009.

	
sciences et guerres	
51
mener la guerre au nom du droit – chaque partie renvoyant l’ennemi  
au non-droit et à l’illégitimité –, ce qui implique une construction 
idéologique et des discours mobilisés à l’appui du travail permanent de 
légitimation de l’effort de guerre consenti par les sociétés. Dans cette lutte 
du vrai contre le faux, l’autorité que confère la « science » autant que la 
référence à des savoirs validés ont fourni une efficace machine intellec-
tuelle de justification du conflit favorisant son extension à l’ensemble du 
champ culturel. Le Times peut en janvier 1915 parler d’une professor- 
made war.
Une autre lecture chronologique des interactions entre sciences et 
guerre met en exergue une séquence temporelle plus courte, qui débute 
avec les ruptures induites par la Seconde Guerre mondiale. Elle fait l’hypo-
thèse que, dans les années 1940 et 1950, toute l’« entreprise Science » a 
été mobilisée, bien au-delà des seules « sciences pour la guerre » à l’œuvre 
dans la première moitié du siècle 1. La culture de guerre, façonnée par 
l’urgence et la mobilisation permanente, y travaille en profondeur les 
modes de pensée et d’action. L’anticipation et le contrôle, fondés sur de 
nouvelles pratiques formelles et calculatoires, passent au premier plan ; 
les institutions militaires, universitaires et industrielles s’entrecroisent, 
de même que les sciences fondamentales et l’engineering. Ainsi les objets, 
les pratiques, les cultures scientifiques sont bouleversés par le conflit et, 
en retour, la place éminente qu’ils y acquièrent leur permet de peser sur 
le monde d’après guerre.
Cette lecture met l’accent sur la permanence que revêt dans la seconde 
moitié du xxe siècle la préparation de la guerre, ou national preparedness, 
selon la formule que l’astronome américain George Ellery Hale défend 
durant le premier conflit auprès du président Wilson aux fins de créer 
le National Research Council (NRC) pour répondre par la recherche 
fondamentale à la concurrence allemande. L’idée de preparedness prend 
une tout autre ampleur une génération plus tard quand, en juin 1940, le 
principal architecte de la politique scientifique de guerre aux États-Unis, 
Vannevar Bush, convainc le président Roosevelt d’abonder le National 
Defence Research Committee pour relever les défis scientifiques du conflit 
à venir. Un an plus tard, il dirige une nouvelle agence, l’Office of Scientific 
Research and Development, dont la dotation témoigne de l’importance 
clé désormais assignée au financement de la recherche sous la forme  
de contrats à des fins militaires 2. Après 1945, contrairement au topos du 
retour à la normale du premier après-guerre, l’affrontement entre monde 
1. Dahan et Pestre (dir.) 2004.
2. Leslie 1993 (p. 6).

52	
anne rasmussen
communiste et monde occidental entretient un état chronique de guerre 
technoscientifique et économique. C’est ce que désigne la formule warfare 
state 1, postulant l’étroite interdépendance entre puissance militaire 
et puissance technologique. Cette nouvelle donne domine les années 
1950 alors que le projet Manhattan a imposé l’idée d’un superpouvoir 
des sciences et des techniques, déterminant dans l’issue des conflits : à 
toute guerre future sa solution technologique, grâce aux ressources que 
les sociétés sont désormais prêtes à consentir en vertu d’une véritable 
mystique de la science. Cette représentation est durable et fonde autant 
la vision communiste du monde des années 1950 que celle des conser-
vateurs américains à l’œuvre dans leur initiative de défense stratégique 
des années 1980, ou « guerre des étoiles » vue par les médias. Le lien 
essentiel entre monde militaire et sphère technoscientifique se traduit 
par les budgets considérables que les grandes puissances consacrent à la 
recherche et au développement en matière d’armement, sous patronage 
étatique, s’articulant à une mobilisation massive de l’appareil productif. 
Aux États-Unis, durant la guerre froide, le département de la Défense 
est ainsi devenu le plus grand entrepreneur national de science, en 
physique et en sciences de l’ingénieur, mais aussi en sciences de la vie et 
en sciences sociales 2. Dès les années 1950, à la suite de la guerre de Corée, 
les budgets américains consacrés à la recherche militaire dans les labora-
toires universitaires et industriels ont retrouvé leur niveau du second 
conflit mondial. Ils l’excèdent ensuite très largement pour atteindre un 
pic au milieu des années 1960, celles qui suivent le lancement en 1957 
du premier satellite soviétique Spoutnik, et ce pic sera encore nettement 
dépassé dans les années 1980.
La preparedness a vu son domaine s’étendre constamment depuis la 
fin de la guerre froide, complexifiant les relations entre science et guerre. 
Elle met en œuvre des dispositifs de prévention et, au-delà encore, des 
« techniques de préparation » qui puisent dans les modèles scientifiques 
et dans des scénarios simulant le réel, alimentant des plans de réponse 
gouvernementaux. Au début du xxie siècle, cette démarche, qui se donne 
pour légitimité la protection de la sécurité nationale, non seulement 
recouvre la préparation des guerres au sens classique, mais elle unifie un 
ensemble de menaces – terroristes, sanitaires, environnementales – dans 
des méthodes intégrées d’intervention 3.
1. Edgerton 2005.
2. Leslie 1993.
3. Zylberman 2013.

	
sciences et guerres	
53
 
Innover pour gagner la guerre ?
Les sciences constitueraient le carburant indispensable à la conduite 
des conflits de l’âge industriel. Cette affirmation d’évidence suppose que 
les sciences sont au cœur du développement technique et économique 
des États-nations, et met en valeur les liens que les recherches fondamen-
tales entretiennent avec les savoirs qui les fondent et les applications qui 
les prolongent. Elle place au centre du dispositif les artefacts embléma-
tiques des conflits du xxe siècle, des gaz de combat de 1915 à la bombe 
atomique de 1945, des missiles embarqués de la guerre froide au système 
antimissile Patriot de la guerre du Golfe ou aux drones des conflits 
actuels. L’importance conférée à l’innovation correspond à la vision des 
pouvoirs qui, depuis la fin du xviiie siècle, ont suscité des programmes de 
recherche appliquée à des fins militaires, portés par la capacité nouvelle 
de décision politique des États modernes. Ceux-ci sont à l’origine d’une 
demande permanente d’exploration de voies techniques originales en 
lien avec les desseins stratégiques de leurs armées.
À la thèse du pacte indéfectible entre innovation technique, science 
et guerre moderne, bien des correctifs ont cependant été apportés. En 
soulignant notamment que la barbarie du xxe siècle n’a pas toujours 
eu besoin du raffinement de moyens technologiques pour se déployer. 
Certes, l’alliance mortifère de la science et de la guerre est portée à un 
point extrême par le système concentrationnaire nazi administrant  
la mort de masse au camp d’Auschwitz-Birkenau, devenu symbole de la 
scientifisation de l’entreprise de destruction humaine : elle associe une 
innovation technologique – un gaz spécialement conçu pour tuer, le 
cyanure d’hydrogène –, un système d’organisation industrielle de l’exter-
mination et la rationalisation bureaucratique de son application. Toutefois, 
la puissance destructrice mise au service de la guerre raciale n’a pas eu 
besoin de la sophistication de la science pour s’exercer parmi les popula-
tions civiles des territoires occupés en Pologne et en Russie, de 1941 à 
1944, quand des unités mobiles, les Einsatzgruppen, dans le sillage de 
la Wehrmacht, mirent à mort plus de 1,3 million de juifs par le vecteur 
des armes légères et de la technique fruste du monoxyde de carbone  
des gaz d’échappement de moteurs de camion. Dans cette perspective, 
l’historien David Edgerton a souligné qu’au xxe siècle la capacité offensive 
de la guerre a d’abord été fondée sur des armements anciens, l’artillerie 
notamment. Les armes traditionnelles restent les plus meurtrières et, 
par là, les plus efficaces. Ainsi, le fusil d’assaut inventé par l’ingénieur 
Kalachnikov, réputé robuste, peu sophistiqué et bon marché, fut l’arme 

54	
anne rasmussen
principale des mouvements de libération des pays du bloc communiste, 
autant que celle des mouvements de guérilla soutenus par les États-Unis 1. 
Un autre exemple saisissant est l’usage des machettes comme arme de 
destruction d’une rare efficacité dans le génocide perpétré au Rwanda, 
au terme d’un massacre de masse d’une extrême intensité en quelques 
mois de 1994.
La thèse inverse, qui met en exergue le lien intrinsèque entre guerre  
et innovations technoscientifiques, a cependant été amplement démontrée 
par les conflits du xxe siècle. La Première Guerre mondiale est vue comme 
la « guerre des chimistes », dont les explosifs ont causé plus de la moitié 
des morts sur le champ de bataille, la Seconde est la « guerre des physi-
ciens » par le rôle décisif du radar et de l’arme atomique, et la notion 
de « révolution dans les affaires militaires » du xxie siècle insiste sur la 
primauté des technologies de l’information. Ces formules univoques sont 
toutefois réductrices, et contestées par les acteurs eux-mêmes, tel Lloyd 
George qui préféra parler de « guerre des ingénieurs » pour rendre compte 
de l’association entre hommes de science, techniciens, experts militaires 
et industriels. En interrogeant la radicalité des mutations techniques à 
l’œuvre, les historiens ont ainsi tracé une ligne de partage entre les deux 
guerres mondiales.
En 1918, avec l’emploi massif des chars, des avions, des sous-marins, 
des gaz et des explosifs, l’environnement technique du champ de bataille 
est très différent de celui de l’entrée en conflit. Pour les belligérants,  
il était manifeste que la technique avait façonné la guerre, qu’il s’agisse  
de la détection sous-marine, la conduite de tir, l’instrumentation aéronau-
tique et optique, la photographie aérienne ou la transmission sans  
fil. Toute une culture de la précision, qui, en temps de paix, avait besoin 
du champ clos du laboratoire pour être mise en œuvre, a été développée 
dans des conditions expérimentales bouleversées sur le champ de bataille 2. 
La quête des moyens de précision se traduit par des innovations comme 
celle du repérage par le son, inauguré par les Britanniques dans la bataille 
de Cambrai de la fin 1917, qui consiste en une série de microphones placés 
sur le front d’où l’attaque devait être lancée et qui peuvent détecter le son 
assourdi d’un tir de canon au loin. Les ondes sonores sont ensuite tracées 
à la manière des mesures sismographiques d’un séisme, la comparaison 
de ces relevés permettant d’établir l’exacte position du canon.
Ces innovations n’ont cependant pas été décisives dans l’issue du 
conflit qui a reposé sur des armes déjà en fonction en 1914, mais dont 
1. Edgerton 2013 (p. 197).
2. David Aubin et Patrice Bret, « Introduction », in « Le sabre et l’éprouvette » 2003 (p. 43-47).

	
sciences et guerres	
55
de nouveaux usages ont été expérimentés, avec une efficacité croissante 1. 
Marc Bloch, lui-même combattant, a souligné combien, en quatre ans 
d’affrontement, les soldats ont été « techniquement » modelés par les 
transformations des équipements et celles des modes de combat. Mais, 
plus que toute rupture technologique, c’est surtout l’immense effort 
de production industrielle qui a été décisif. Le cas des gaz de combat 
est parlant. Leur usage, apparu en avril 1915 sur le champ de bataille 
d’Ypres à l’initiative allemande, après des mois d’essais secrets, puis 
étendu à d’autres fronts, résulte bien d’une succession d’innovations : 
dans la mise au point des toxiques et de leur pouvoir incapacitant ou 
létal, et la parade défensive qu’elle suscite ; dans les vecteurs de leur 
dissémination, des fûts enterrés sur la ligne de front aux obus remplis 
de gaz ; dans les moyens de leur production industrielle ; dans les struc-
tures organisationnelles de la recherche, dont les plus abouties ont été 
supervisées par le Kaiser-Wilhelm Institut de chimie de Berlin, dirigé par 
Fritz Haber et Walther Nernst. Il est pourtant avéré que, en dépit de la 
rupture induite par ce nouveau système d’arme, les gaz n’ont eu, une fois 
l’effet de surprise épuisé, qu’un faible impact stratégique 2. L’argument ne 
conduit pas à réduire leur influence, mais les déplace au nombre des effets 
psychologiques d’une arme de terreur, caractéristique de cette guerre  
d’attrition.
Les ruptures technologiques sont en revanche manifestes dans le second 
conflit mondial. Celui-ci a abouti, à l’issue d’une intense mobilisation  
des mondes scientifique et mathématique, à donner une place centrale à 
la conception d’armes de rupture, voire aux Wunderwaffen qu’appelait de 
ses vœux le ministre à la Propagande du Reich pour renverser « miraculeu-
sement » une situation militaire compromise. Développement du radar et 
conception de la bombe A au premier chef, mais aussi électronique, fusée 
de proximité ou fusée à carburant solide ont été l’objet de ce déploiement 
scientifique sans précédent. Ce franchissement de seuils ne s’arrête pas 
aux objets techniques et aux instruments, ni même à l’immense effort de 
production industrielle qui les a rendus opérationnels. De manière aussi 
fondamentale, il concerne l’ensemble des nouveaux outils qui, faisant fond 
sur les mathématiques, la logique, les statistiques, les probabilités, utilisant 
les moyens de calcul et les outils formels permis par les premiers ordina-
teurs, offrent des techniques de gestion, d’anticipation et de modélisation, 
appliqués aux situations concrètes et abstraites que la guerre engendre, 
voire à la guerre elle-même : recherche opérationnelle, théorie des jeux, 
1. Winter (dir.) 2013-2014 (vol. 2).
2. Lepick 1998.

56	
anne rasmussen
analyse des systèmes, traduisent l’ambition de gérer rationnellement les 
processus réels, qu’ils soient matériels ou sociaux 1.
Durant la guerre froide, comme l’a souligné Paul Edwards, la métaphore 
du monde clos qui sous-tend la vision du monde partagée par les deux 
camps est étayée par les outils informatiques : la guerre est un champ 
imaginaire à penser, autant qu’une réalité pratique, dans laquelle les 
simulations sont plus décisives que les armes. Un nouveau seuil est franchi 
avec la gestion de l’incertitude stratégique propre à la dissuasion qui 
s’organise autour de scénarios du pire élaborés par le monde des experts 
et des stratèges. La course aux armements qui en résulte repose sur deux 
pieds, l’accumulation de moyens et l’innovation technologique 2. Les objets 
emblématiques en sont les vecteurs et la conquête de l’espace pour les 
lanceurs, suscitant à partir du milieu des années 1950 un immense inves-
tissement dans la recherche dédiée aux missiles intercontinentaux et aux 
programmes de missiles Polaris tirés à partir de sous-marins en plongée.
Les sciences sont alors les pourvoyeuses de solutions pour des défis 
technologiques d’une grande complexité, qu’ils concernent les dimen-
sions de la bombe, les vecteurs et les dispositifs qui les optimisent tels 
que miniaturisation des ogives, rayon d’action et vitesse des bombardiers, 
précision des tirs de missiles, guidage des vecteurs 3. On leur assigne une 
finalité de rupture, s’agissant d’armes nouvelles, comme les missiles à 
ogives multiples capables de se diriger chacune sur un objectif différent 
(Mirv), ou de systèmes d’armes déjà connus qu’il convient de mener à 
une « efficacité totale », comme les armements défensifs qui conjuguent 
moyens de repérage et armes d’interception. Dans la configuration  
de la dissuasion, l’armement défensif est en quête d’invulnérabilité ou de 
« quasi-invulnérabilité » : l’initiative de défense stratégique du président 
Reagan en constitue un point d’orgue, déployant à partir de 1983 un 
immense programme de recherche appuyé sur une agence d’État et sur 
de nombreux secteurs de pointe de l’industrie américaine, abandonné à 
la fin de la guerre froide puis relancé par l’administration Bush après le 
11 septembre 2001.
En temps de guerre froide, avec les implications de la dissuasion quant 
au niveau des arsenaux et à la quête de technologies capables de rompre 
« l’équilibre de la terreur », la prise en compte des sciences de guerre 
devient un élément capital des processus de décision, non plus seulement 
stratégique – comme l’arme chimique ou la bombe atomique – mais 
1. Dahan et Pestre (dir.) 2004.
2. Edwards 2013, Chagnollaud 2011.
3. Chagnollaud 2011 (p. 23).

	
sciences et guerres	
57
partie prenante du politique. Ainsi, au début des années 1960, c’est l’éva-
luation par les décideurs américains, en contexte d’incertitude, d’une 
donnée technologique – le supposé missile gap, ou fossé technologique en 
matière de missiles à longue portée censé séparer l’arsenal des États-Unis 
de celui de l’URSS après le succès de son premier vol spatial habité – qui 
détermine la politique du secrétaire d’État à la Défense, Robert McNamara, 
fixant des objectifs sans précédent en termes quantitatifs et qualitatifs 
à l’effort de recherche militaire. Ce nouveau régime des relations entre 
sciences et guerre, caractérisé par une co-construction de la politique de 
défense américaine et de la politique de recherche scientifique, rendues 
interdépendantes au nom de la sécurité nationale, pourrait être résumé 
par le slogan qui préside en 1999 aux États-Unis à la loi National Missile 
Defense invitant à développer un programme « aussi vite que la techno-
logie le permettrait 1 ».
Des sciences pour penser et rationaliser la guerre
Le prisme de l’innovation favorise une lecture univoque de sciences 
« pour la guerre » enrôlées par des conflits qu’elles transforment. 
À l’encontre d’une telle vision, des historiens ont questionné la porosité 
des catégories – science et militaire, science et guerre – et les inter­­
actions à l’œuvre dans leurs relations, interrogeant ainsi la continuité 
des pratiques entre temps de paix et temps de guerre. Ces relectures ont 
été favorisées dans les années 1970 et 1980 par les études critiques des 
sciences et, en temps de guerre froide, par la lumière faite sur les liens 
entre le militaire, le politique et l’économie, en écho à la dénonciation 
du « complexe militaro-industriel ».
La « militarisation » de l’entreprise science a été envisagée sous l’angle 
d’un processus d’hybridation. Le tableau conventionnel des relations entre 
guerre et science met en valeur que les productions scientifiques naissent 
dans la sphère civile et que, détournées de leur usage, elles sont appro-
priées par le militaire. Ce tableau peut être nuancé, voire inversé, comme 
l’a fait David Edgerton 2, montrant que nombre de dispositifs, de l’aviation 
à la radio, ont été développés dans des contextes militaires et résultent 
de transferts multiples. Le système radar britannique, en application de 
la radio, est issu de l’expérience de défense aérienne des années 1930, qui 
a pour origine la Grande Guerre. Ce qui devient un dispositif militaire 
1. Chagnollaud 2011 (p. 73).
2. Edgerton 2013 (p. 195).

58	
anne rasmussen
central de la Seconde Guerre mondiale est à nouveau réapproprié après 
le conflit pour être à l’origine de la constitution du champ des résonances 
nucléaires. A contrario, des sciences exclusivement pour la guerre ont pu 
s’avérer être un pur produit de la science universitaire, comme la bombe 
atomique en constitue un exemple de taille, impliquant le monde civil. 
Par ailleurs, les systèmes d’armes du xxe siècle ont requis l’hybridation 
de disciplines dans des projets communs : qu’il s’agisse de faire converger 
des savoirs académiques au service de dispositifs spécifiques (chimistes, 
biologistes, médecins dans la guerre défensive contre les gaz en 1915),  
de solliciter des compétences dans les périphéries universitaires pour 
leur conférer un rôle opérationnel central (géographie, topographie, 
géodésie, météorologie, optique, télégraphie ou sciences des communi-
cations dans la Grande Guerre) ou bien de faire travailler ensemble les 
sciences les plus fondamentales et l’engineering (la mobilisation de plus 
de 150 000 ingénieurs, techniciens et ouvriers dans le projet Manhattan 
en convergence avec les physiciens fondamentalistes). Enfin, la milita-
risation des sciences pose la question de l’intégration des scientifiques 
aux sphères militaires. Les universitaires comme les états-majors ont 
eu tendance à sous-estimer leur collaboration mutuelle : soit pour en 
dénoncer la faiblesse – les savants déplorent leur mise à l’écart imputée 
au conservatisme des états-majors, les militaires entendent les contenir 
à la résolution de problèmes plutôt qu’à la définition des objectifs –, 
soit, à rebours, pour dénier l’existence de cette collaboration qui cadre 
mal avec l’éthos que les scientifiques affichent par prédilection. Les 
pratiques engendrées par la guerre se sont-elles pérennisées une fois 
dissipées les contraintes de l’urgence et de la mobilisation générale ? 
On suivra Dominique Pestre concluant que le second conflit mondial 
et la guerre froide « induisent une fracture majeure dans la pratique des 
sciences de laboratoire 1 » et imposent, durablement, de nouvelles formes  
collaboratives.
Les usages que les guerres ont faits des sciences au xxe siècle ont aussi 
reposé sur les processus d’optimisation à l’œuvre dans la conduite des 
opérations militaires, l’organisation de la production et la gestion des 
hommes. Ces opérations ont eu en commun d’utiliser des outils scienti-
fiques pour rationaliser et accroître le rendement de l’entreprise guerre. 
Présentons-en deux épisodes exemplaires.
Dans les guerres mondiales, les préoccupations d’« efficacité » (le terme 
efficiency est plus adéquat) concernent les individus et les collectifs, les 
troupes et les travailleurs, mais aussi la lutte contre les effets nocifs de 
1. Pestre, « Le nouvel univers des sciences et techniques », in Dahan et Pestre 2004 (p. 26).

	
sciences et guerres	
59
l’enrôlement total qui suscite une usure dévastatrice pour les sociétés 
qui les subissent. Dans les différents domaines qui associent savoirs et 
pratiques de la performance humaine, la psychologie, pour l’esprit, autant 
que la physiologie, pour le corps, ont trouvé une place éminente. C’est 
le cas, dans la première moitié du xxe siècle, des études sur la sélection 
du « facteur humain ». Le champ d’intervention, qui en est à ses débuts 
dans la sphère professionnelle, consiste à mesurer les conditions psycho-
logiques que requièrent des aptitudes techniques. L’objectif est aussi de 
mesurer et sélectionner les compétences « psychophysiques » de personnels 
spécialisés, en une étroite association entre les processus psycho­logiques 
et leurs fondements physiologiques qui constitue le mot d’ordre de la 
psychotechnique. Dans cette lignée, les Allemands Walther Moede et 
Curt Piorkowski mènent des tests sur les automobilistes, les pilotes 
d’avion et les opérateurs radio, selon les méthodes et les instruments, 
tel le chronoscope de Hipp, que la psychologie expérimentale développe 
en laboratoire 1. En France, Jean-Maurice Lahy conduit sur le front des 
études psychotechniques pour la sélection des artilleurs. Étudiant leur 
temps de réaction, il met en œuvre un « indice de fatigabilité », et tente 
d’objectiver la nervosité au combat par des tests mesurant la « plasticité 
fonctionnelle » comme « indice de sang-froid ». La discipline prend son 
essor dans le conflit suivant, où il est désormais reconnu que la guerre 
moderne requiert, même chez le simple soldat, la capacité à prendre des 
initiatives opérationnelles dans lesquelles entrent en jeu des aptitudes 
intellectuelles qu’il convient de discipliner.
Deuxième exemple, celui des think tanks qui se développent au service 
de l’institution militaire après 1945. Comme les ont décrits Dominique 
Pestre et Paul Edwards 2, il s’agit ici d’optimiser, par des dispositifs mathé-
matiques et expérimentaux, les opérations tactiques et stratégiques 
qui commandent à l’efficacité au combat, la logistique et la gestion des 
hommes et des moyens dans les systèmes. Les pionniers de la recherche 
opérationnelle, tels les physiciens et ingénieurs chargés de mettre au 
point le système radar pour la défense aérienne britannique, développent 
une vision intégratrice des techniques radar, de l’électronique et de 
la formation des hommes. Cette pensée des systèmes s’est particuliè-
rement développée aux États-Unis dans le cadre de la Rand Corporation, 
archétype des think tanks au service de l’institution militaire, qui vise à 
rationaliser l’action des décideurs en leur fournissant des outils logiques 
et formels, et à concevoir les moyens futuristes du prochain conflit. 
1. Rabinbach 1990.
2. Pour tout ce paragraphe, Pestre 2002.

60	
anne rasmussen
L’organe fondé par l’US Air Force réunit autour du spécialiste en mathé-
matiques appliquées Warren Weaver la fine fleur des experts scientifiques 
universitaires – mathématiciens et logiciens, mais aussi économistes, 
physiciens, psychologues – recrutés pour la définition des programmes 
par l’analyse des systèmes, la modélisation d’objets complexes, qui passent 
aussi par des mises en situation expérimentale très concrètes. Ainsi John 
von Neumann est l’une des figures de proue des mathématiciens liés à la 
Rand, appliquant la théorie des jeux dont il était pionnier à la simulation 
de scénarios de guerre nucléaire globale.
Un pacte critique
Le pacte qui unit les sciences et les guerres ne saurait être saisi sans 
prendre en compte une de ses dimensions constitutives, celle de la critique 
constante dont il a été l’objet. Si celle-ci n’a pas eu d’influence politique 
telle qu’elle puisse même freiner le développement des « sciences pour 
la guerre », elle est cependant une composante essentielle des représen-
tations que les sociétés se font des sciences et que les scientifiques se font 
d’eux-mêmes. Comment concilier la construction issue du projet des 
Lumières et du progressisme du xixe siècle – les sciences comme entre-
prise de rationalisation et de compréhension universelle dont la société est 
bénéficiaire – et ce mal absolu qu’est la guerre, menaçant, au xxe siècle, 
la survie même de l’humanité ? Comment concilier le rôle assigné aux 
savants dans le processus de brutalisation que la guerre implique et l’éthos 
dont ils se réclament ? La critique des sciences pour la guerre a mis en 
débat cette question insoluble. Elle est parvenue à gagner progressivement 
des cercles sociaux élargis, mais elle a aussi permis, à rebours des buts 
critiques poursuivis, que politiques et scientifiques justifient cette colla-
boration organique et négocient des compromis.
Dès la Grande Guerre, il est manifeste que la mobilisation des sciences 
accroît la capacité destructrice des batailles. En 1915, chaque belligérant 
suscite des commissions d’enquête qui dénoncent les « méthodes de guerre 
inhumaines » de l’ennemi, incluant bombardements aériens et armes 
chimiques. La stigmatisation de la science mortifère accède à la scène 
publique, même si ce n’est pas la science en tant que telle, mais la science 
ennemie seule, qui se trouve dans le viseur, accusée d’être caporalisée et 
manipulée par des intérêts idéologiques. C’est à ce titre que Fritz Haber 
est mis en accusation par le traité de Versailles en tant que criminel de 
guerre à l’égal du Kaiser. Mais la réflexion sur la responsabilité du savant 
demeure subsidiaire, si bien que le même Haber peut être récipiendaire 

	
sciences et guerres	
61
en 1920 du prix Nobel de chimie au titre de l’année 1918. De fait, très 
rares sont les scientifiques à être restés « au-dessus de la mêlée », tel 
Einstein refusant de signer le Manifeste des 93 qui réunit en 1914 l’élite 
des savants allemands, ou à avoir dénoncé la compromission de la science 
dans la guerre, tel le mathématicien Bertrand Russell. C’est seulement 
une fois le conflit refermé que se fait entendre la condamnation de la 
« science homicide », selon les mots de l’historien français Jules Isaac en 
1922 : « Si la guerre a tourné en catastrophe, c’est à la Science qu’il faut 
s’en prendre, et à elle seule. Par elle, la capacité homicide et destructive 
de la guerre s’est trouvée décuplée, centuplée 1. » L’instrumentalisation 
des sciences par la guerre a été consentie durant le conflit. Qu’elle soit 
dénoncée à la faveur du pacifisme dominant de l’entre-deux-guerres, 
dans les démocraties tout au moins, favorise l’émergence de postures 
critiques, mais aussi justificatrices.
Un premier argumentaire idéalise la science pure, indépendante de 
tout intérêt et ne visant qu’à la recherche de la vérité. Pensons à Paul 
Langevin qui, quoique artisan de la guerre sous-marine dans son labora-
toire de Toulon, dénonce en 1925 la « prostitution de la science à la 
guerre » à la tribune de la Ligue des droits de l’homme. Il ne s’agit pas ici 
de psychologiser des comportements – qu’on pourrait analyser en termes 
de déni – mais d’en rendre compte par un éthos partagé assignant à la 
science un idéal d’autonomie. « La science est profondément indifférente 
à la manière dont nous pouvons l’utiliser », répond le doyen de la faculté 
des sciences de Paris à Jules Isaac 2. Elle peut se laisser détourner par les 
circonstances que la défense de la civilisation, dans la Grande Guerre, 
celle des démocraties, dans la Seconde Guerre mondiale, ou celle des 
blocs, dans la guerre froide, exigent, mais elle doit reprendre son cours 
normal une fois la parenthèse refermée. La critique du complexe militaro- 
industriel s’inscrit dans le même registre. Émanant du pouvoir 
lui-même – c’est Eisenhower qui met en garde en 1961 contre « l’influence 
illégitime » du complexe militaro-industriel et le risque du « dévelop-
pement désastreux d’un pouvoir usurpé » –, elle témoigne de l’impact 
croissant du militaire sur la recherche et l’enseignement supérieur, en 
termes de budgets et d’agenda, mais aussi de finalités et même d’état 
d’esprit. La critique cible la perte d’autonomie du monde scientifique qui 
n’aurait plus la capacité à agir sur le monde, ni même à le comprendre, 
à d’autres fins que militaires 3.
1. Isaac 2002 [1922] (p. 84).
2. 25 novembre 1923, cité in Isaac 2002 [1922] (p. 87).
3. Leslie 1993.

62	
anne rasmussen
Un deuxième argumentaire né de l’entre-deux-guerres est celui de 
l’idéal régulateur. La science, simple pourvoyeuse de moyens, n’est pas en 
cause dans les mauvais usages qui en sont faits : aussi appartient-il à une 
instance supérieure de fixer des limites aux capacités de destruction et 
d’en contrôler l’application. Le débat porte sur la légitimité de l’autorité 
régulatrice et les mobiles qui l’animent. Le pouvoir politique a toujours 
affirmé devoir rester aux commandes pour subordonner tout contrôle 
aux intérêts supérieurs de la stratégie. Ainsi, sous l’égide du droit inter-
national, c’est le coût gigantesque de la recherche antimissile asséchant 
les budgets de défense, pour une réussite opérationnelle médiocre, qui 
a poussé aux premières négociations de désarmement entre Américains 
et Soviétiques à la fin des années 1960. Les scientifiques constituent un 
autre groupe de pression qui revendique de prendre part à la décision 
en matière de régulation, considérant qu’ils sont les seuls à même d’en 
maîtriser la portée. Le franchissement de seuil causé par l’arme nucléaire, 
puis thermonucléaire, y joue un grand rôle, comme le souligne Joliot-
Curie dès novembre 1945 :
Les scientifiques sont conscients de leur responsabilité dans cette affaire et 
admettent de moins en moins d’être mis hors circuit lorsqu’il s’agit de prendre 
des décisions sur l’utilisation de leurs découvertes et inventions. Ils savent trop 
la mauvaise utilisation qu’on en a souvent faite. Ils désirent fermement être 
admis à discuter les questions concernant le contrôle international 1.
Certains des physiciens de premier plan impliqués dans le projet 
Manhattan ont pu ainsi invoquer sans solution de continuité la validité 
de leur expertise dans la mobilisation de la science de guerre autant que 
dans sa démobilisation. Ainsi en 1939 Leo Szilard fait appel à Einstein, 
pacifiste patenté, pour convaincre le président américain du bien-fondé  
de l’investissement massif dans la conception de l’arme nucléaire. Une 
fois la bombe opérationnelle, le même Szilard adresse à Truman une 
lettre signée par le même Einstein sur la nécessité d’un contrôle inter-
national de l’arme nucléaire, tandis que James Franck est l’auteur en 
juillet 1945 d’un rapport plaidant pour une démonstration dissuasive 
de la bombe, en la faisant exploser dans le désert, avant tout bombar-
dement d’une ville japonaise. 155 physiciens signent une pétition dans 
ce sens, censurée par l’armée américaine et rendue publique seulement 
en 1963 2. Dans l’immédiat après-guerre, l’action des Leo Szilard, Niels 
1. Joliot-Curie 1945 (p. 198).
2. Salomon 2006.

	
sciences et guerres	
63
Bohr, James Franck, Hans Bethe, Isidore Rabi, conduit à la création de 
l’association américaine des Atomic Scientists dont le Bulletin devient 
la tribune des militants en faveur de la réduction de l’armement, voire 
du désarmement. C’est le même esprit de cogestion de la régulation 
entre politiques et scientifiques qui conduit en 1957 à la fondation du 
mouvement international des Pugwash Conferences on Science and 
World Affairs, à l’initiative de Bertrand Russell et du physicien polonais 
Józef Rotblat, le seul scientifique à avoir quitté le projet Manhattan 
avant l’explosion d’Hiroshima. Les scientifiques qui s’y rencontrent en 
tant qu’individus y discutent au nom de leur expertise des voies pour 
limiter les tensions internationales causées par la course aux armements. 
Dans cette optique, le mouvement scientifique de protestation antinu-
cléaire, impulsé par une partie de ceux qui ont pris part à la conception 
des nouvelles armes, joue un rôle moteur pour catalyser la mobilisation 
de plus larges secteurs de la société, à travers des vecteurs médiatiques 
(l’anthologie One World or None, 1946), associatifs (l’organisation Ban 
the Bomb) ou militants (la campagne pétitionnaire de 1958 contre les 
essais menée par le chimiste Linus Pauling).
Comme en 1914-1918, la critique d’après 1945 est toujours de nature 
morale mais elle est désormais portée par l’individu, et pas seulement par 
le collectif savant. Le célèbre examen de conscience d’Oppenheimer, leader 
du projet Manhattan, n’est pas singulier. Sakharov, principal concepteur 
de la bombe H soviétique en 1953 au sein du projet coordonné par Beria, 
adopte après 1962 une attitude critique au nom des « problèmes moraux » 
que suscitent les essais nucléaires. Pour Primo Levi, dont le témoignage 
prend doublement appui sur son identité professionnelle de chimiste et 
son expérience de déporté, il s’agit d’en appeler à la conscience individuelle 
des scientifiques : « Il faut donc que, dans le monde entier, physiciens, 
chimistes et biologistes prennent pleinement conscience de leur sinistre 
pouvoir 1. » L’enjeu de la question morale n’est pas seulement de responsa-
bilité sociale. Il est aussi dans l’hubris et le plaisir à la source de l’activité 
scientifique. Sakharov concédait que la physique des explosions thermonu-
cléaires était le paradis du théoricien, tandis qu’Oppenheimer s’exprimait 
sur les motivations des scientifiques engagés dans la conception de la 
bombe : « La raison pour laquelle nous avons fait ce travail est que ce fut 
une nécessité organique. Si vous êtes un scientifique, vous ne pouvez pas 
arrêter ce genre de choses 2. » Les guerres du xxe siècle, par la démesure 
des investissements sociaux et économiques qui leur sont consentis, 
1. Levi 2005 [1987] (p. 919).
2. Salomon 2006 (p. 243 et passim).

64	
anne rasmussen
ont offert un terrain expérimental à la réalisation de la profession de foi 
baconienne : savoir comment le monde fonctionne.
Références bibliographiques
Bauman Zygmund, 2002, Modernité et Holocauste, Paris, La Fabrique.
Chagnollaud Jean-Paul, 2011, Brève Histoire de l’arme nucléaire entre prolifération 
et désarmement, Paris, Ellipses.
Crawford Elisabeth, 1992, Nationalism and Internationalism in Science (1880-
1939) : Four Studies of the Nobel Population, Cambridge, Cambridge University 
Press.
Dahan Amy et Pestre Dominique (dir.), 2004, Les Sciences pour la guerre (1940-
1960), Paris, Éd. de l’EHESS.
Edgerton David, 2005, Warfare State : Britain (1920-1970), Cambridge, Cambridge 
University Press.
–	 2013, Quoi de neuf ? Du rôle des techniques dans l’histoire globale, Paris, Seuil.
Edwards Paul N., 2013, Un monde clos. L’ordinateur, la bombe et le discours politique 
de la guerre froide, Paris, B2.
Harrison Carol E. et Johnson Ann (dir.), 2009, « National Identity : The Role of 
Science and Technology », dossier thématique d’Osiris, vol. 24.
Horne John (dir.), 2010, Vers la guerre totale. Le tournant de 1914-1915, Paris, 
Tallandier.
Isaac Jules, 2002 [1922], « Paradoxe sur la science homicide », rééd. in Alliage, no 52, 
p. 79-87.
Joliot-Curie Frédéric, 1945, « La désintégration atomique », Les Cahiers rationalistes,  
vol. 58, no 86, p. 178-199.
Kevles Daniel J., 1978, The Physicists : The History of a Scientific Community in 
Modern America, New York, Alfred A. Knopf.
Krige John et Pestre Dominique (dir.), 1997, Science in the Twentieth Century, 
Amsterdam, Harwood Academic Publishers.
Lepick Olivier, 1998, La Grande Guerre chimique, Paris, PUF.
« Le sabre et l’éprouvette », 2003 : « Le sabre et l’éprouvette. L’invention d’une science 
de guerre (1914-1939) », dossier thématique de 14-18 : Aujourd’hui, Heute, Today, 
Paris, Noésis.
Leslie Stuart W., 1993, The Cold War and American Science : The Military-Industrial-
Academic Complex at MIT and Stanford, New York, Columbia University Press.
Levi Primo, 2005 [1987], « Le sinistre pouvoir de la science », Uomini e libri, no 112, 
janvier-février 1987, in Primo Levi, Œuvres, Paris, Robert Laffont, p. 918-919.
Lindqvist Sven, 2012, Une histoire du bombardement, Paris, La Découverte.
Pestre Dominique, 2002, « La pensée mathématique des systèmes », in Dominique 
Pestre (dir.), dossier « La  science et la guerre », La  Recherche, hors-série no  7, 
p. 10-15.
–	 2003, Science, argent et politique. Un essai d’interprétation, Paris, Quæ.
Prochasson Christophe et Rasmussen Anne (dir.), 2004, Vrai et faux dans la 
Grande Guerre, Paris, La Découverte.
Rabinbach Anson, 1990, The Human Motor : Energy, Fatigue and the Origins of 
Modernity, Berkeley (CA), University of California Press.
Sachse Carola et Walker Mark (dir.), 2005, Politics and Science in Wartime : 

	
sciences et guerres	
65
Comparative International Perspective on the Kaiser Wilhelm Institute, Chicago 
(IL), University of Chicago Press.
Salomon Jean-Jacques, 2006, Les Scientifiques entre pouvoir et savoir, Paris, Albin 
Michel.
Schroeder-Gudehus Brigitte, 1978, Les Scientifiques et la paix. La communauté 
internationale au cours des années 20, Montréal, Presses de l’université de Montréal.
Schweber Silvan S., 2000, In the Shadow of the Bomb : Oppenheimer, Bethe, and the 
Moral Responsibility of the Scientist, Princeton (NJ), Princeton University Press.
Traverso Enzo, 2009, 1914-1945 : la guerre civile européenne, Paris, Hachette 
Littératures.
Winter Jay (dir.), 2013-2014, La Première Guerre mondiale, Paris, Fayard, 3 vol.
Wittner Lawrence S., 2009, Confronting the Bomb : A Short History of the World 
Nuclear Disarmament Movement, Palo Alto (CA), Stanford University Press.
Zylberman Patrick, 2013, Tempêtes microbiennes. Essai sur la politique de sécurité 
sanitaire dans le monde transatlantique, Paris, Gallimard.


3 L’État entrepreneur de science
D av i d  E d g e r t o n
Au cours des dernières années, de nombreux organismes d’État chargés 
des politiques de recherche se sont convertis à l’idée que des politiques 
nationales de recherche agressives et ciblées pouvaient générer des 
retombées économiques rapides et spectaculaires. Leurs responsables 
espèrent être à l’origine des prochains Google ou Microsoft, même si 
cela serait plutôt dans le domaine des sciences de la vie que dans celui 
de l’informatique. Certains d’entre eux estiment qu’une industrie de la 
biologie synthétique pesant 100 milliards de dollars US pourrait émerger 
au cours de la prochaine décennie. Ces rêves sont absolument nouveaux : 
jamais auparavant les organismes de recherche des États n’ont prétendu 
pouvoir atteindre des résultats de ce type. Il va d’ailleurs sans dire qu’il 
n’existe aucun exemple de réalisations de cette ampleur, et ces prétentions 
doivent être comprises comme le produit d’une politique de recherche 
universitaire bien particulière financée par l’État, qui a peu à voir avec 
la réalité de la recherche, et encore moins avec la création de nouvelles 
industries. En effet, les politiques de recherche sont principalement 
déterminées par des considérations politiques et budgétaires et non par 
une véritable interaction avec l’économie. « En temps de guerre, la vérité 
est si précieuse qu’elle devrait toujours être protégée par un rempart de 
mensonges », disait Winston Churchill. De même les arcanes du finan-
cement de la recherche par l’État sont-ils protégés non seulement par 
une extravagante futurologie, mais aussi par des catégorisations sérieu-
sement erronées et des histoires inventées de toutes pièces.
Dans le domaine des politiques de recherche, rien n’est véritablement 
conforme aux apparences. Par exemple, les termes consacrés tels que 
« politique scientifique », ou les débats sur les relations entre « science et 
société » ou « science et État », ont des significations qui sont loin d’être 
fondées et qui véhiculent des présupposés qui ont une grande impor-
tance. Par exemple, dans le modèle anglo-saxon, la science et l’État sont 
 Construction du ballon dirigeable par la compagnie Goodyear Zeppelin, Akron, Ohio, États-Unis, 
vers 1930.

68	
david edgerton
supposés être des catégories distinctes, la « science » étant la recherche 
conduite dans les universités d’élite, la « politique scientifique » désignant 
la politique relative à ce genre de recherche. Il est également supposé que 
les relations entre science et État sont les relations entre ces universités 
d’élite et l’État, etc. De tels présupposés sont fortement trompeurs dès que 
l’on veut s’intéresser à l’ensemble de la recherche, à toutes les formes de 
savoir que nous pourrions qualifier de science et à l’ensemble des relations 
de la science avec l’État et la société. De manière similaire, il existe des 
récits historiques selon lesquels des inventions particulières marquent 
l’entrée dans des époques totalement nouvelles, qui se succèdent au gré 
de « révolutions », dont le nombre est, étrangement, rarement estimé à 
plus de trois ou quatre au cours de l’histoire humaine 1.
Pour analyser les relations entre science et État, mieux vaut éviter de 
partir des discours sur la « politique scientifique » ou ce qui s’y rapporte, 
et s’appuyer plutôt sur ce que nous savons du fonctionnement des États 
et du rôle historique des nouveaux savoirs. Par souci de clarté, nous 
pouvons distinguer deux modèles opposés d’interaction entre États et 
production des savoirs. Aucun des deux ne correspond à la réalité, mais 
chacun illustre des croyances courantes et puissantes. Selon le premier 
modèle, le bon savoir, indépendant et solide, est issu de la société civile, 
et non de l’État. Il est le produit de civils aux penchants cosmopolites, 
non pas de l’armée ou de la bureaucratie. Ce savoir et ses praticiens sont 
nécessairement en contact avec l’État, et en particulier avec l’armée, deux 
entités toutefois considérées dangereuses, corruptrices et conservatrices. 
Étant donné le pouvoir de la connaissance, l’État et l’armée se retrouvent 
nécessairement transformés par cette interaction, pour le meilleur et pour 
le pire, ce qui soulève des questions sur « la science et l’État », « la science 
et l’armée », des sujets qui continuent de structurer les débats, en parti-
culier d’un point de vue moral. La « politique scientifique » est le terme 
consacré pour désigner la gestion de ces relations, ou au moins celles qui 
sont considérées comme les plus significatives (les relations de l’État avec 
l’élite des chercheurs universitaires extérieurs à l’État). Ces débats sont 
d’ailleurs tous marqués par un certain manque de réalisme.
Selon un second modèle, bien moins répandu et influent, les nations 
et les États sont considérés comme des agents créatifs, en concurrence 
les uns avec les autres. Dans ce modèle, les États sont eux-mêmes des 
inventeurs et des producteurs de savoir essentiels, si bien qu’il devient 
oiseux de faire des distinctions entre la science et l’État, entre le savoir 
et l’État. Les universitaires et les chercheurs non seulement voient leurs 
1. Pour un point de vue critique sur ce type d’histoire des technologies, voir Edgerton 2013.

	
l’état entrepreneur de science	
69
recherches financées par l’État, mais ils sont en outre fonctionnaires de 
l’État, et les États ont des corps d’experts techniques et de chercheurs 
qu’il serait absurde d’imaginer comme existant hors de l’État. Dans un tel 
modèle, on peut s’attendre à ce que les besoins militaires de l’État occupent 
une place essentielle dans la recherche, la production et l’entretien des 
savoirs, étant donné la centralité des fonctions militaires de l’État. De plus, 
les États ont un pouvoir énorme sur la société et ainsi sur la production 
de savoir en général.
Nous avons ainsi un modèle dans lequel tous les États réagissent face à 
un ensemble global et cosmopolite d’institutions inventives et de produc-
teurs de savoir appartenant à la société civile, et un autre dans lequel les 
États et ses responsables dominent, et cherchent à contrôler, le savoir, 
en concurrence avec d’autres États.
Bien qu’ils soient profondément ancrés dans nos manières de penser, 
aucun de ces deux modèles ne fonctionne véritablement. Par exemple, 
l’idée selon laquelle il existerait des « systèmes nationaux d’innovation », 
englobant à la fois l’État et la société civile, reste un fantasme techno- 
nationaliste qui ignore le constat empirique évident, mais peu reconnu, 
qu’il n’y a pas de corrélation directe à l’échelle d’un pays entre le taux 
de croissance économique et l’activité dans le secteur de l’innovation. 
La raison est simple : les nations tirent davantage parti des innovations 
venant de l’étranger que de celles qui émergent au sein de leurs propres 
frontières ; le savoir et la pratique technique ont fondamentalement une 
importante dimension mondiale, non étatique. Cela ne veut pas dire que 
nous devrions naïvement adopter l’idée d’un techno-globalisme et nous 
ranger derrière celle, ancienne mais très persistante, du dépérissement 
des États-nations sous l’impact de dispositifs civils globaux et globali-
sants. En effet, cette idée est battue en brèche par le fait que la plupart 
des dispositifs prétendus globaux et globalisants ont été créés par des 
États-nations à des fins nationales, qu’il s’agisse de la radio, des avions, 
ou encore de l’Internet 1.
Comme cela a été souligné il y a bien longtemps, le rôle de l’État a varié 
non seulement dans le temps, mais aussi dans ce que les États particuliers 
voulaient faire, ainsi que dans leurs positions les uns vis-à-vis des autres, 
selon qu’ils s’agissaient d’États puissants ou aspirant à l’être. On peut par 
exemple citer la grande différence qui existait entre les États-Unis d’Amé-
rique, immensément riches mais dont l’appareil d’État central était faible 
jusqu’à la Seconde Guerre mondiale, et la pauvre Union soviétique, qui 
essayait de rattraper son retard. Il faut également noter que le pouvoir 
1. Edgerton 2007.

70	
david edgerton
de l’État dans le domaine de l’innovation tend à avoir été confiné à des 
domaines particuliers, en premier lieu l’armée, puis les communications 
ainsi que certains projets et structures de grande échelle. Néanmoins,  
il y a suffisamment de coordination entre les États pour qu’émergent  
des choses similaires au même moment dans plusieurs pays, de sorte que 
quelques généralisations peuvent être tentées. Mon point de vue est que 
le xxe siècle a d’abord été marqué par une tendance lourde à la nationali-
sation des programmes de recherche, qui a été suivie par un mouvement 
significatif de dénationalisation.
La Grande Guerre et l’entre-deux-guerres
Il serait faux de considérer le monde d’avant 1914 comme un monde 
globalisé où le laisser-faire était la règle. L’action étatique, le protection-
nisme économique et, bien sûr, les questions impériales jouent en effet 
un rôle significatif. Il n’en reste pas moins que la Grande Guerre est 
l’occasion pour les États et les nations de prendre de nouvelles initia-
tives qui auront une influence considérable dans l’entre-deux-guerres  
et au-delà. Après guerre, le commerce mondial ne revient pas aux 
niveaux de la Belle Époque, et partout s’élèvent des barrières écono-
miques nationales et impériales plus restrictives qu’avant la guerre. Les 
États indépendants, dont le nombre a augmenté, et les empires protègent 
leurs économies. Le monde est aussi caractérisé par l’émergence d’un 
continent non capitaliste, l’URSS.
Pendant la guerre, les principaux États belligérants se mobilisent 
non seulement pour se doter de davantage d’armes et en concevoir de 
nouvelles, mais aussi pour développer leur approvisionnement continu 
en matières considérées comme faisant partie de l’économie civile, mais 
utiles à la guerre. Avec la perturbation du commerce, l’autonomie devient 
une question essentielle pour les États, pour une période qui dure bien 
après la fin des combats. L’exemple de l’industrie des teintures synthé-
tiques est bien connu. Avant que la guerre n’éclate, ce secteur, caractérisé 
par une intense activité de recherche, est dominé par l’Allemagne au 
niveau mondial. L’entrée en guerre pousse les belligérants, notamment 
la Grande-Bretagne et la France, à créer des entreprises nationales et 
leurs propres programmes de recherche d’État dans ce secteur. À la fin 
du conflit, l’industrie allemande des colorants synthétiques reste la plus 
puissante du monde, mais elle est dorénavant confrontée à la concur-
rence sérieuse d’entreprises états-uniennes, britanniques, françaises et, 
bien sûr, suisses.

	
l’état entrepreneur de science	
71
Toutes sortes de régimes politiques se tournent alors vers la science, la 
rationalité et les experts pour forger de nouvelles nations et promouvoir 
de nouveaux vecteurs de développement socio-économique. Si certains 
régimes sont dénoncés comme étant anti-science et anti-modernité 
par leurs détracteurs, aucune nation, sauf peut-être l’État de la cité du 
Vatican, ne se revendique comme telle. L’Allemagne nazie se considère 
moderne et scientifique, tout comme l’URSS, la Grande-Bretagne, la 
France et les États-Unis. La Chine républicaine et la Turquie d’Atatürk 
se présentent également comme des régimes modernes et promoteurs 
de science qui veulent en finir avec l’obscurantisme du passé. Au Japon, 
malgré ses attributs divins, l’empereur Hirohito est un expert en biologie 
marine qui s’habille à l’occidentale, que ce soit en civil ou en militaire. 
En fait, la plupart des critiques de la modernité sont modernes, et ceux 
qui critiquent les régimes impériaux sont généralement des nationalistes 
modernes qui, s’ils invoquent un passé préimpérial, n’en font pas pour 
autant un modèle d’avenir.
Autarcie
La quête d’autarcie trouve un nouvel élan avec la crise économique  
du début des années 1930, conjuguée à la volonté des nations d’assurer 
leur sécurité sur le plan militaire. Cela est particulièrement le cas de  
l’Allemagne qui construit son effort de recherche et de développement 
dans les années 1930 spécifiquement pour se procurer une quantité 
toujours plus importante de produits de synthèse en remplacement des 
biens importés. Parmi les projets clés, on peut citer divers procédés 
de production de pétrole à partir du charbon, le développement de la 
production de caoutchouc de synthèse et de fibres. En fait, de nombreux 
pays pouvant se voir privés d’accès aux régions productrices de coton ou 
de laine investissent dans des procédés de fabrication de fibres à partir 
de ressources locales, allant du bois à la protéine de lait. L’autarcie est un 
élément central de la politique de tous les États fascistes ou semi-fascistes, 
même dans ceux où cela paraît difficilement envisageable, comme la Grèce. 
Cela vaut non seulement pour l’industrie, mais aussi pour l’agriculture, 
et cela entraîne partout le développement de programmes nationaux de 
recherche en agriculture 1. Ces politiques se poursuivent aussi longtemps 
que le fascisme, même lorsque celui-ci survit à la Seconde Guerre mondiale. 
Par exemple, l’Espagne franquiste dépense beaucoup pour la recherche 
1. Saraiva et Wise 2010.

72	
david edgerton
et le développement dans les années 1950, notamment pour mettre au 
point des procédés de transformation du charbon en pétrole.
Toutes les nations ne s’engagent pas résolument sur la voie de l’autarcie. 
Les États-Unis d’Amérique, en tant que puissance d’échelle continentale, 
disposent d’à peu près tout ce dont ils ont besoin, à l’exception des 
produits tropicaux. La Grande-Bretagne, la plus grande puissance 
commerciale dans le monde, ne peut espérer atteindre l’autarcie étant 
donné l’étendue de ses besoins et sa petite taille. Cependant, même 
la Grande-Bretagne, traditionnellement favorable au libre-échange, 
commence à introduire des contrôles et des taxes sur les importations, 
ponctuellement d’abord puis de façon plus systématique, et à développer 
des secteurs industriels derrière ces protections. En plus de créer une 
industrie chimique, elle développe des processus de transformation 
du charbon en pétrole et, plus généralement, oriente ses politiques de 
recherche à des fins d’approvisionnement national et impérial, par exemple 
dans le domaine de la réfrigération qui permet le transport de viande et 
de fruits frais depuis les territoires de l’Empire en Australasie. L’impé-
rialisme est un facteur important du soutien apporté par les États aux 
projets de cette époque en matière d’infrastructures de transport et de  
communications.
Il y a alors des compagnies nationales de transport aérien et maritime 
équipées d’engins produits dans le pays. Les États subventionnent la 
construction des grands paquebots de l’époque comme le Bremen et 
l’Europa, le Rex et le Conte di Savoia, et les plus gros de tous, les Normandie, 
Queen Mary et Queen Elizabeth. Le coût de ces bateaux est comparable 
à celui d’ouvrages colossaux comme le barrage Hoover dans l’ouest des 
États-Unis, ou encore à celui des plus grands navires de guerre et des 
gratte-ciel gigantesques comme l’Empire State Building. Chaque pays a 
également sa compagnie aérienne nationale qui fait voler des avions de 
fabrication domestique : Imperial Airways, Air France et Lufthansa sont 
des vitrines pour les appareils nationaux, qui représentent un coût et un 
effort de recherche importants pour les gouvernements. La KLM néerlan-
daise fait exception lorsqu’elle décide de passer d’un fournisseur national 
à des avions américains dans les années 1930. De manière similaire, il 
existe des liens étroits entre les États et les sociétés nationales de radio, 
par exemple RCA aux États-Unis, Marconi en Grande-Bretagne et la 
Compagnie générale de télégraphie sans fil (CSF) en France. L’intérêt 
national pour l’aviation, les paquebots et la radio revêt un caractère 
idéologique. Selon un trope répété ad nauseam, les libéraux considèrent 
ces secteurs comme des technologies internationales et internationali-
santes qui doivent être des vecteurs de la paix mondiale en faisant tomber 

	
l’état entrepreneur de science	
73
les barrières entre nations. Elles sont en réalité le produit de ces mêmes 
barrières et sont conçues pour les renforcer.
Construire le socialisme
Pendant l’entre-deux-guerres, aucun État n’apporte de soutien plus 
massif et proclamé que l’Union soviétique à la recherche, aux nouvelles 
connaissances. Dotée d’une philosophie d’État considérée comme fondée 
sur les sciences de la nature, l’URSS se présente comme l’héritière de 
tout ce qu’il y avait de meilleur dans le monde bourgeois, comme le seul 
système politique ayant libéré le progrès technique des freins posés par  
les contradictions capitalistes. La planification et l’application planifiée  
de la recherche qui bénéficient d’un soutien généreux sont censées 
accélérer le rythme du progrès humain. Cette idée a également ses 
partisans enthousiastes hors d’URSS, comme le pionnier des études sur 
les sciences, John Desmond Bernal. Le pays du socialisme réellement 
existant reste bien sûr traversé par de nombreuses contradictions. Tout 
en étant profondément internationaliste, il est également extrêmement 
nationaliste, en particulier dans les années 1940. Même s’il est convaincu 
de la supériorité soviétique dans la planification de la recherche, dans 
l’innovation et dans l’utilisation des machines, le pays achète en masse 
de nouvelles machines et des usines, et a recours à des experts étrangers, 
principalement états-uniens, pour les mettre en place. L’Union sovié-
tique développe ainsi une capacité extraordinaire à apprendre, copier 
et améliorer, de sorte qu’elle peut développer de manière autonome, 
dès les années 1930, des machines meilleures que celles qu’elle importe. 
Finalement, l’Union soviétique d’après 1945 est capable de réaliser des 
premières marquantes dans le domaine des armes nucléaires et des fusées, 
de sorte qu’elle semble pouvoir dépasser le monde capitaliste.
La Seconde Guerre mondiale
Au début de la Seconde Guerre mondiale, on peut raisonnablement 
s’attendre à un ralentissement des activités de recherche et développement, 
et à une réaffectation des chercheurs expérimentés à des tâches plus 
urgentes. Cela est le cas mais, dans un certain nombre de pays belligérants, 
les activités de recherche et développement se développent fortement. 
L’exemple le plus frappant est celui des États-Unis qui, s’ils sont déjà une 
grande puissance en 1939, ne sont pas encore leader dans le domaine de 

74	
david edgerton
la recherche et du développement militaires. Avec leur énorme capacité 
de production et leur immense richesse, les États-Unis développent  
leur capacité de recherche en prenant des idées en Europe et en les réalisant 
rapidement à une échelle gigantesque. Ils terminent la guerre avec les 
moteurs à pistons les plus puissants, les plus gros avions, une production 
massive de caoutchouc synthétique, la capacité de produire d’énormes 
quantités de carburant pour avion, ainsi qu’en position dominante dans 
le domaine des nouvelles armes comme les radars, les fusées de proximité 
et bien sûr la bombe atomique. Pendant la guerre, les grands projets sont 
conduits par une variété d’agences, l’armée, la marine et les autorités 
responsables de l’utilisation du pétrole comme arme de guerre. Seule une 
petite partie de ces projets relève d’agences de recherche spécialisées, dont 
l’Office of Scientific Research and Development (« Bureau de recherches 
et de développement scientifiques », en charge notamment du projet sur 
les radars) qui est la principale source de financement militaire pour la 
recherche et le développement dans les universités.
Grâce à l’effort de guerre, les États-Unis se retrouvent avec des capacités 
de recherche complètement transformées, émergeant comme la première 
puissance mondiale en 1945 dans le domaine militaire et dans la plupart 
des secteurs civils qu’ils ne dominaient pas déjà auparavant. Par la suite, 
les universités continuent d’être des centres de recherche majeurs liés à 
l’État et aux militaires. Les États-Unis peuvent alors entrevoir l’avenir avec 
confiance dans un monde où l’innovation rapide joue un rôle toujours 
plus important, un monde qu’ils sont en position de dominer.
Le long boom et la guerre froide
Les années qui suivirent la Seconde Guerre mondiale, en particulier à 
partir de 1949-1950, sont marquées par des niveaux de dépenses militaires 
exceptionnels, dont une grande partie est consacrée à l’acquisition et à la 
conception de nouvelles armes. La supériorité technologique étant consi-
dérée comme toujours plus essentielle pour la guerre, non seulement 
la R & D dans le domaine militaire progresse de manière considérable, 
mais de plus en plus de crédits sont dépensés sur un nombre toujours 
plus limité de programmes. Dans de nombreux pays, le financement par 
l’État de la recherche et développement bénéficie principalement aux 
agences militaires, représentant plus de la moitié de l’ensemble de la 
R & D dans les années 1950, même dans les pays capitalistes. L’essentiel 
de l’argent est consacré au développement de fusées, aux moteurs à 
réaction et aux avions, aux armes atomiques, ou encore à l’électronique, 

	
l’état entrepreneur de science	
75
dans des laboratoires industriels ou gouvernementaux. Dans certains 
pays, et surtout aux États-Unis, l’armée devint le principal financeur 
de la recherche en science physique dite « fondamentale » dans les 
universités. Après la guerre, les services de l’armée et la Commission 
de l’énergie atomique des États-Unis dominent le financement de la 
recherche universitaire dans le pays. Ce n’est qu’en 1950 qu’est créée une 
agence purement civile, la National Science Foundation, qui ne rivalise 
que beaucoup plus tard avec les financements militaires sur les campus 
universitaires. Dans le contexte particulier des États-Unis, la guerre 
marque un changement important et permanent : pour la première 
fois, le gouvernement fédéral accorde un financement à grande échelle 
de la recherche dans les universités. Un tel mouvement a bien sûr été 
engagé depuis des décennies dans d’autres grandes nations, dont les 
États-Unis copient ainsi les programmes de recherche et les modes de 
financement. Mais ils le font à une échelle beaucoup plus grande, avec 
un accent particulier sur la recherche militaire. Ce qui se passe sur les 
campus ne constitue qu’une partie de l’histoire.
Comment ce rôle dominant joué par l’État après la guerre doit-il être 
interprété ? L’élément de loin le plus important est le soutien aux projets, 
firmes et industries qui fournissent des machines pour les entreprises 
nationales, en premier lieu les forces armées du pays, mais également 
les compagnies aériennes, les infrastructures et les industries nationales. 
Dans ce contexte, il ne fait aucun sens de penser à des « modèles linéaires 
d’innovation » ou autres concepts trompeurs issus d’analyses partielles  
d’institutions d’importance limitée telles que la National Science Foundation 
des États-Unis 1. Nous devons adopter un nouveau langage pour saisir à la 
fois les continuités importantes et les changements radicaux engendrés 
par l’ampleur et l’extrême complexité des programmes techniques, avec 
leur lot de nouvelles exigences en termes de gestion, de nouvelles formes 
d’analyse. Pour cela, il est absolument nécessaire de reconnaître le rôle 
central de l’appareil militaire et des bureaucraties d’entreprise.
Nations
Une conséquence remarquable de la Seconde Guerre mondiale – même 
si les Nations unies tentent de la tempérer et s’il y a un partage important 
des technologies entre alliés – est la volonté de chaque pays, après 1945, de 
développer en son sein toutes les technologies modernes, principalement 
1. Edgerton 2005, Godin 2006, Scranton 2006 et 2011.

76	
david edgerton
dans le domaine militaire. La France humiliée se met par exemple en 
quête de puissance et de prestige dans le monde de l’après-guerre en 
cherchant à assurer son indépendance technologique. Thomson-Houston 
devient ainsi la société Thomson en 1953, libérée de son accord de 
licence avec GE. La France développe un programme nucléaire, une 
force de frappe et devient la quatrième puissance atomique en 1960. Elle 
développe également de nouveaux moteurs à réaction et de nouveaux 
avions, en particulier sous la Cinquième République 1. Dans le monde 
entier, de nombreux pays sont pris par la frénésie nucléaire et la volonté 
de fabriquer leurs propres avions à réaction. L’Argentine développe ainsi 
un avion de chasse et un programme nucléaire. C’est également le cas  
de la Suède et de l’Espagne. Les nouveaux États indépendants s’engagent 
sur la voie de nouveaux techno-nationalismes postimpériaux, comme 
l’Inde et l’Égypte qui développent leurs propres chasseurs à réaction.
Ces programmes rencontrent bien sûr beaucoup de difficultés. La 
plupart des nations abandonnent leurs ambitions nucléaires, même 
si certaines nations rejoignent le cercle des puissances nucléaires, et 
que certaines cherchent encore à y entrer aujourd’hui. Si pendant un 
temps un certain nombre de pays fabriquent des avions de chasse, seules 
quelques puissances restent dans le jeu dans les années 1960 et 1970. 
La production de moteurs d’avion ne s’étend pas au-delà d’un cercle de 
puissances clés qui reste très restreint jusqu’à nos jours. Cela est parti-
culièrement frappant dans le domaine des avions gros porteurs civils. En 
1960, parmi les six types d’avion de ligne en exploitation dans le monde, 
un est français (la Caravelle, équipée de moteurs britanniques), un est 
britannique, un soviétique (le Tupolev 104) et trois sont états-uniens.  
Il ne reste plus aujourd’hui que deux acteurs majeurs dans le monde en 
matière de construction aéronautique : les États-Unis et un consortium 
européen. La Russie ne produit plus que quelques appareils et la Chine 
et le Japon n’ont pas percé sur ce marché, la société brésilienne Embraer 
étant l’exception qui confirme la règle 2. Dans le domaine nucléaire, 
des programmes nationaux ou à caractère semi-national de dévelop-
pement de la bombe se développent plus tard, avec des avancées majeures  
en Afrique du Sud, en Israël, en Inde et au Pakistan. Pour ce qui est  
de l’énergie nucléaire, le nombre de nations ou d’entreprises capables de 
produire des réacteurs reste très limité.
Les énormes difficultés à investir ces domaines constituent une raison 
importante pour collaborer. En Europe, dès la fin des années 1950, il 
1. Jacq 1995.
2. Je suis reconnaissant envers Richard Aboulafia pour cette remarque.

	
l’état entrepreneur de science	
77
est clair que les projets majeurs en concurrence avec les États-Unis ne 
peuvent être le fait de puissances individuelles (ainsi voient le jour le 
Concorde franco-britannique, le chasseur franco-britannique, ou encore 
les avions et les moteurs civils et militaires européens). Il en est de même 
pour la CECLES (Commission européenne pour la mise au point et la 
construction de lanceurs d’engins spatiaux) et l’ASE (Agence spatiale 
européenne) dans le domaine de l’aérospatiale civile, la première étant 
issue d’un programme de missile britannique qui n’avait aucun futur de 
manière indépendante.
Le défi du socialisme réellement existant
Il est bien connu que Khrouchtchev affirmait que l’URSS allait dépasser 
les pays capitalistes, à une époque où beaucoup dans le monde capitaliste 
pensaient que cela était possible. Aussi étonnant que cela puisse paraître 
aujourd’hui, le bloc soviétique représente un défi sérieux, militairement 
et idéologiquement, pour les puissances capitalistes. Son économie se 
développe rapidement dans l’après-guerre, avec l’industrialisation, l’urbani-
sation et la formation d’un grand nombre de scientifiques et d’ingénieurs. 
À la fin des années 1960, l’URSS compte davantage de scientifiques 
et d’ingénieurs en R & D que les États-Unis. Certains succès comme 
la première bombe atomique soviétique (1949), le satellite Spoutnik 
(1957) et le voyage dans l’espace de Youri Gagarine (1961) ont un reten-
tissement mondial, tandis que d’autres réalisations comme la première 
application du nucléaire pour la production d’électricité (devançant  
les Britanniques) ou le second avion à réaction (après les Britanniques) 
ont un moindre écho. Dans les pays occidentaux, de nombreux propagan-
distes soulignent les succès soviétiques pour justifier l’augmentation des 
investissements dans l’éducation et la recherche spatiale, ou encore pour 
promouvoir la planification et les projets dirigés par l’État. La réussite 
apparente du modèle soviétique sert à légitimer ce type de politique même 
auprès des anticommunistes. Mais les succès soviétiques sont de courte 
durée. Au lieu de rattraper son retard, l’URSS se retrouve rapidement à 
perdre du terrain, ce qui apparaît de manière évidente dans les années 
1970. Certains se plaisent à qualifier l’URSS de « Haute-Volta avec des 
fusées », une expression exagérée mais qui contient une part de vérité. 
La même chose peut être dite de la Chine communiste des années 1980 
ou de la Corée du Nord aujourd’hui. Le fait que le socialisme réellement 
existant ne puisse concurrencer le monde capitaliste est une question qui 
a eu une importance historique significative, comme on peut le mesurer 

78	
david edgerton
en imaginant ce qui aurait pu être dit si l’Union soviétique avait mieux 
réussi dans le domaine de l’innovation.
À la question de savoir si le socialisme réellement existant a produit une 
science ou une technologie spécifiques, on peut certainement répondre 
par l’affirmative, mais cela a conduit à de terribles pénuries. La RDA a 
produit des voitures avec des carrosseries en résine, l’Union soviétique  
a utilisé des soupapes pendant plus longtemps que les États-Unis – mais 
il ne s’agissait pas là de choix volontaires. La Chine pourrait être un 
exemple, avec sa politique particulière qui a consisté à utiliser d’un côté 
de grandes machines d’origine soviétique, et de l’autre de plus petites 
machines dans les campagnes. Mais ces machines ne lui étaient pas 
spécifiques et étaient généralement basées sur de vieilles technologies 
venant de l’Ouest.
Vers la fin des années 1960 et dans les années 1970, la planification 
centralisée autoritaire de l’Union soviétique se retrouve critiquée depuis 
une variété de perspectives, non seulement en tant que communisme, 
mais en tant que « haut modernisme 1 ». L’arrogance de l’expert à l’esprit 
étroit et du bureaucrate au pouvoir illimité est la cible des critiques dans 
les pays capitalistes. Elle l’est également pendant la Révolution culturelle 
chinoise, qui est en partie dirigée contre le stalinisme bureaucratique 
et les experts. La critique de la technocratie et de la planification vient 
également d’une nouvelle droite émergente et du mouvement écologiste 
naissant. Pour nombre de ses opposants, le nucléaire, qu’il soit capita-
liste ou socialiste, implique la hiérarchie, le secret, un État policier et 
des coûts énormes.
Agriculture
Le discours de la modernité se caractérise par un certain mépris et une 
indifférence vis-à-vis de la petite agriculture paysanne. La production 
alimentaire doit être organisée sur le modèle de l’usine moderne, à grande 
échelle, mécanisée. Pendant l’entre-deux-guerres, l’Union soviétique 
détruit son agriculture traditionnelle par l’établissement forcé d’énormes 
exploitations agricoles collectives d’État. Celles-ci permettent d’obtenir 
des gains de productivité, mais paradoxalement moins élevés que ceux 
de l’agriculture capitaliste à petite échelle, en particulier après la Seconde 
Guerre mondiale. En effet, l’agriculture d’après guerre est l’objet d’une des 
transformations à la fois les plus remarquables et les moins documentées 
1. Scott 1998.

	
l’état entrepreneur de science	
79
de l’histoire mondiale. Grâce au soutien des États, aux subventions et à 
l’investissement dans la recherche et le développement, de nombreux 
agriculteurs sont en mesure d’augmenter leurs rendements à des niveaux 
jamais atteints auparavant. Combiné à la mécanisation, le dévelop-
pement des apports en engrais, eau, pesticides, herbicides, et le recours 
à de nouvelles variétés, sont à l’origine d’une profonde transformation. 
La productivité du travail augmente comme jamais auparavant. Elle 
augmente même plus dans l’agriculture que dans l’industrie en Europe. La 
Grande-Bretagne est un cas exemplaire en ce sens : importatrice de la moitié  
de son alimentation au milieu du siècle, elle devient autosuffisante dans 
les années 1980, alors qu’un pourcentage significativement inférieur de 
la population travaille la terre.
Dans les pays pauvres non communistes, la stagnation de la produc-
tivité agricole fait l’objet d’une politique, conduite par des organisations 
internationales, pour augmenter les rendements à l’hectare sans déplacer 
la main-d’œuvre. Les clés de cette politique sont l’irrigation et les engrais,  
ce qui nécessite de nouvelles variétés, dérivées de variétés plus anciennes 
de l’agriculture intensive orientale. Ce que l’on appelle la « révolution verte » 
commence comme un processus, encore inachevé, de transformation de 
l’agriculture pour atteindre des niveaux de productivité approchant ceux 
des pays riches. Un changement extrêmement important voit le jour en 
Chine dans les années 1980, où l’inversion du processus de collectivi-
sation, dans de nouvelles circonstances, entraîne des gains de productivité 
agricole énormes. Sans ces bouleversements de l’agriculture, la formidable 
industrialisation et l’urbanisation du monde n’auraient pas été possibles.
La révolution néolibérale
La révolution néolibérale qui prit son essor sous les gouvernements 
Thatcher et Reagan dans les années 1980 fait suite à l’expérience pionnière 
du Chili des années 1970 (rétrospectivement, le programme des « Quatre 
Modernisations » en Chine en 1978 doit également être mentionné dans 
le même cadre). Même si les États ne se désengagent généralement pas 
de l’économie, de la recherche et de la création de nouvelles industries, il 
en est tout de même ainsi dans certains cas. La caractéristique essentielle 
de cette révolution néolibérale n’est pas simplement la privatisation de 
monopoles et d’entreprises d’État, mais la levée de leur obligation d’acheter 
des produits nationaux, et leur ouverture à la concurrence, à des degrés 
divers. La dénationalisation est un élément central de cette révolution. Cela 
donne lieu à un abandon du volontarisme qui caractérise les politiques 

80	
david edgerton
industrielles nationales, avec des conséquences profondes sur les initia-
tives centralisées de développement de nouvelles technologies nationales 
au service d’industries nationales. Au Royaume-Uni, la libéralisation et 
la privatisation de la distribution d’électricité et d’autres services publics 
affaiblissent sérieusement les industries nucléaire, téléphonique, électro-
nique, informatique et ferroviaire britanniques. Le cas français est par 
contre bien différent avec le lancement de nouveaux programmes d’État 
dans ces deux derniers domaines (Minitel et trains à grande vitesse), 
ainsi que des investissements massifs dans l’énergie nucléaire. La France 
constitue une exception en étant le pays qui a le plus longtemps maintenu 
des politiques abandonnées dans un monde de plus en plus néolibéral.
C’est cependant en Chine que se déroule la plus importante révolution 
néolibérale, même si elle n’est que partielle. À partir des « Quatre Moder-
nisations » de 1978, la Chine transforme son agriculture et son industrie, 
dont les rendements atteignent des niveaux historiquement inédits. Il 
s’agit sans doute de la plus rapide révolution industrielle et agricole 
de l’histoire mondiale. La Chine importe de la technologie mais aussi, 
contrairement à l’exemple soviétique, du capital étranger, principalement 
japonais, taïwanais et chinois de l’étranger. Ainsi, l’entreprise taïwanaise 
Foxconn, qui est la première entreprise manufacturière du monde, 
possède à Shenzhen, en Chine continentale, la plus grande usine qui 
ait jamais existé dans le monde, où elle emploie 400 000 ouvriers. Cette 
usine fabrique des produits Apple pour le monde entier. Néolibéralisme 
et mondialisation sont intimement liés.
Cependant, dans ce monde néolibéral, les États continuent de financer 
la recherche de manière importante, avec une part croissante des fonds 
destinée aux universités. Ce financement est justifié par l’hypothèse selon 
laquelle cette recherche universitaire fait émerger les industries d’avenir. 
Cette hypothèse a pour toile de fond une image particulière, qui est  
celle du succès des entreprises qui ont essaimé à partir des universités 
états-uniennes dans le domaine de l’électronique et plus récemment dans 
celui de l’informatique et d’Internet (Microsoft, Google, Apple). L’espoir 
est que l’histoire se répète, cette fois-ci autour des biotechnologies. En 
termes de visibilité, le Human Genome Project de la fin des années 1990 
et du début du xxie siècle occupe à ce titre une place particulière. Les 
États-Unis ont dépensé plus de 5 milliards de dollars (de 2010) sur ce 
projet et maintiennent aujourd’hui les mêmes niveaux d’investissement. 
Mais les résultats des biotechnologies sont modestes. Des variétés de 
maïs, de soja et de coton OGM se sont certes rapidement diffusées depuis 
les États-Unis à partir du milieu des années 1990, avec une croissance 
remarquable de 10 % par an des surfaces plantées en OGM. Mais cette 

	
l’état entrepreneur de science	
81
évolution essentiellement conduite par le secteur privé états-unien n’a 
rien d’aussi révolutionnaire dans ses effets que l’augmentation des intrants 
agricoles au cours des décennies précédentes. Dans le secteur pharma-
ceutique, les nouveautés biotechnologiques ont un impact minime, et 
cette industrie reste caractérisée par une innovation exceptionnellement 
lente et coûteuse, avec des impacts faibles, ou parfois négatifs, sur la santé 
humaine 1. Les résultats de ces dépenses de recherche sont loin d’être 
évidents. Malgré un déferlement d’« innovations », il se pourrait bien que 
l’ère néolibérale ait été une époque de développement technique relati-
vement faible, à l’exception notable des technologies de l’information et 
de la communication 2.
La forte continuité de l’activité des États dans le domaine des affaires 
militaires remet également en cause l’idée (répandue dans les études  
d’histoire des sciences, mais pas dans celles d’histoire des technologies) 
selon laquelle nous vivrions dans un monde post-guerre froide. Si le 
complexe militaro-scientifique soviétique s’est effectivement effondré de 
manière spectaculaire, rien d’un tant soit peu comparable ne s’est produit 
dans les pays de l’OTAN, au Japon ou en Chine. Par exemple, après la 
guerre froide, les dépenses en R & D du département de la Défense des 
États-Unis ont été chaque année supérieures à leurs niveaux des années 
1970.
Malgré toutes les discussions sur la notion de triple hélice, sur l’université 
entrepreneuriale, ou sur un nouveau mode de recherche universitaire 
qui serait caractérisé par ses liens avec le monde de l’entreprise et des 
affaires, les revenus que les universités retirent de la propriété intellec-
tuelle restent modestes. Même les universités les plus inventives restent 
extrêmement dépendantes des droits d’inscription, des dotations et du 
financement par l’État, en particulier pour la recherche. On pourrait 
même dire que les idées avancées sur des universités qui seraient d’un 
type radicalement nouveau ont simplement été un moyen d’obtenir de 
l’argent public. Il est aussi révélateur que l’évaluation des chercheurs, 
des universités et des pays, par la comptabilisation des publications et 
des citations, soit devenue une opération de routine. Il ne s’agit pas là de 
néolibéralisme, mais de néobureaucratisme ; il n’existe pas de mesures 
véritablement convaincantes de l’efficacité réelle, ni même de l’impact 
économique de la recherche. Mais il y a bien sûr eu un profond changement 
culturel au sein des universités et entre les étudiants, dans la lignée du 
tournant néolibéral.
1. Le Fanu 2011.
2. Edgerton 2013, Cohen 2011, Le Fanu 2011, Mirowski 2011.

82	
david edgerton
Il y a de cela une génération, la néolibérale de formation scientifique 
Margaret Thatcher faisait grand cas de la menace du réchauffement 
climatique (pour des raisons qui ne sont pas encore évidentes à ce jour), 
une question qui faisait alors l’objet de beaucoup d’attention et de finan-
cements étatiques, ainsi que d’une unanimité remarquable parmi les 
chercheurs. Pourtant, la recherche sur les nouvelles sources d’énergie  
et, plus important encore, sur les moyens de lutter contre le changement 
climatique, se développe très lentement (en effet, au xxie siècle, les 
émissions de gaz à effet de serre continuent de croître, tout comme la 
consommation de charbon). Cela doit sonner comme un rappel sur une 
question que nous n’avons pas abordée – à savoir que le monde dans 
lequel vit la majeure partie de l’humanité est très éloigné du monde 
auquel nous pensons lorsque nous évoquons les États et la science. 
Ceci ne veut pas dire que ces mondes ne sont pas concernés par la 
connaissance, ou par les machines issues de la science moderne, loin 
de là, mais qu’ils se situent hors des lorgnettes de ceux qui analysent 
les « politiques scientifiques ». En fait, cette remarque a une portée plus 
générale : les concepts, les idées et les exemples qui ont droit de cité dans 
la plupart des récits sur la science et l’État ne saisissent que très partiel-
lement la multiplicité des capacités d’agir, des politiques et des pratiques  
à l’œuvre.
Traduit par Cyril Le Roy
Références bibliographiques
Clarke Sabine, 2012, « Pure Science with a Practical Aim : The Meanings of 
Fundamental Research in Britain (ca. 1916-1950) », Isis, vol. 101, no 2, p. 285-311.
Cohen Tyler Cohen, 2011, The Great Stagnation : How America Ate All the Low- 
Hanging Fruit of Modern History, Got Sick, and Will (Eventually) Feel Better, New 
York, Dutton.
Edgerton David, 2005, « “The Linear Model” Did Not Exist : Reflections on the 
History and Historiography of Science and Research in Industry in the Twentieth 
Century », in Karl Grandin et Nina Wormbs (dir.), The Science-Industry Nexus : 
History, Policy, Implications, New York, Watson, p. 31-57.
–	 2007, « The Contradictions of Techno-Nationalism and Techno-Globalism : 
A Historical Perspective », New Global Studies, vol. 1, no 1, p. 1-32.
–	 2013, Quoi de neuf ? Du rôle des techniques dans l’histoire globale, Paris, Seuil.
Forman Paul, 1987, « Behind Quantum Electronics : National Security as Basis for 
Physical Research in the United States (1940-1960) », Historical Studies in the 
Physical and Biological Sciences, vol. 18, no 1, p. 149-229.
Giffard Hermione, 2011, The Development and Production of Turbojet Aero-Engines 
in Britain, Germany and the United States (1936-1945), PhD thesis, Imperial College,  
Londres.

	
l’état entrepreneur de science	
83
Godin Benoît, 2006, « The Linear Model of Innovation : The Historical Construction 
of an Analytical Framework », Science, Technology, and Human Values, vol.  31, 
no 6, p. 639-667.
Guzzetti Luca (dir.), 2000, Science and Power : The Historical Foundations of 
Research Policies in Europe, Florence, Istituto e Museo di Storia della Scienza.
Harwood Jonathan, 2012, Europe’s Green Revolution and Others Since : The Rise and 
Fall of Peasant-Friendly Plant Breeding, Londres, Routledge.
Jacq François, 1995, « The Emergence of French Research Policy : Methodological 
and Historiographical Problems (1945-1970) », History and Technology, vol.  12, 
no 4, p. 285-308.
Krige John, 2006, American Hegemony and the Postwar Reconstruction of Science in 
Europe, Cambridge (MA), MIT Press.
Krige John et Barth Kai-Henrik (dir.), 2006, « Global Power Knowledge : Science 
and Technology in International Affairs », dossier thématique d’Osiris, no 21.
Le Fanu James, 2011, The Rise and Fall of Modern Medicine, Londres, Abacus, 2e éd.
McDougall Walter A., 1985, The Heavens and the Earth : A Political History of the 
Space Age Race, New York, Basic Books.
Mirowski Philip, 2011, Science-Mart : Privatizing American Science, Cambridge 
(MA), Harvard University Press.
Pestre Dominique, 2003, Science, argent et politique. Un essai d’interprétation, Paris, 
Quæ.
Saraiva Tiago et Wise M. Norton (dir.), 2010, « Autarky  / Autarchy : Genetics, 
Food Production and the Building of Fascism », Historical Studies in the Natural 
Sciences, vol. 40, no 4, p. 419-600.
Scott John, 1998, Seeing Like a State : How Certain Schemes to Improve the Human 
Condition Have Failed, New Haven (CT), Yale University Press.
Scranton Philip, 2006, « Technology-Led Innovation : The Non-Linearity of US Jet 
Propulsion Development”, History and Technology, vol. 22, no 4, p. 337-367.
–	 2011, « Mastering Failure : Technological and Organisational Challenges in British 
and American Military Jet Propulsion (1943-1957) », Business Historys, vol.  53, 
no 4, p. 479-504.
Siddiqi Asif, 2000, Challenge to Apollo : The Soviet Union and the Space Race (1945-
1974), NASA History Office.
Thomas William, 2015, Rational Action : The Sciences of Policy in Britain and 
America (1940-1960), Cambridge (MA), MIT Press.


4 Une manière industrielle  
de savoir
J e a n - P a u l  G a u d i l l i è r e
En janvier 2011, l’Inspection générale des affaires sociales (IGAS) 
publiait un rapport qui faisait le bilan d’une enquête conduite en un 
temps record par trois de ses membres et qui portait sur le Mediator, 
une préparation de benfluorex inventée et commercialisée depuis 1976 
par la firme pharmaceutique Servier 1. La raison pour laquelle l’IGAS 
avait été chargée d’une telle enquête était le scandale public suscité par 
la révélation, à l’automne 2010, que la prescription de Mediator était sans 
doute, selon les évaluations faites par quelques cardiologues et épidé-
miologistes, responsable du décès de 500 à 2 000 patients.
Depuis son introduction sur le marché, le Mediator avait officiellement 
été vendu comme moyen de réduire la charge lipidique chez des patients 
atteints de diabète ou d’un fort excès de lipides sanguins. Le Mediator 
était toutefois une innovation contestée. Depuis le milieu des années 
1990, comme le rappelait le rapport de l’IGAS, des critiques, incluant non 
seulement les éditeurs de la revue Prescrire mais aussi des responsables 
de l’Assurance maladie, s’étaient manifestées, soulignant que la consom-
mation de Mediator allait bien au-delà du cercle des patients relevant des 
indications officielles mais concernait nombre de personnes en surpoids 
chez qui la molécule de Servier était utilisée comme produit anorexigène.
De fait, le rapport de l’IGAS dressait un réquisitoire sévère à l’encontre 
du système français de régulation de la pharmacie. D’une part, les autorités 
de contrôle du médicament avaient ignoré les signalements de décès 
susceptibles d’attirer l’attention sur la dangerosité du Mediator. D’autre 
part, pour les inspecteurs de l’IGAS, ce produit n’aurait jamais dû être 
mis sur le marché. Pour étayer ce jugement, le rapport mettait en avant 
deux registres de problèmes. Le premier était que, depuis la première 
1. IGAS 2011.
 La synthèse organique aux commandes de l’invention de médicaments. Le laboratoire de chimie 
de Geigy à la fin des années 1950.

86	
jean-paul gaudillière
demande d’autorisation de commercialisation, Servier avait toujours insisté 
sur le fait que le benfluorex était une substance innovante, un produit 
sans rapport avec les « coupe-faim » que la firme avait produits dans les 
années 1970 et finalement retirés du marché du fait de leur appartenance 
au groupe des amphétamines, une classe de substances progressivement 
interdites à cause de leurs effets secondaires et addictifs. Le marketing 
massif de Servier auprès des médecins avait ainsi systématiquement 
opposé le benfluorex et ses homologues chimiques et pharmacologiques. 
Le second registre de problèmes était le manque de moyens des agences 
de régulation, la multiplication des situations de « conflit d’intérêts » qui 
avaient facilité l’acceptation des arguments et des données utilisés par 
Servier pour mettre en balance des risques sanitaires jugés marginaux 
et les bénéfices socio-économiques résultant de l’activité d’un fleuron de 
la pharmacie française.
L’affaire du Mediator est intéressante non seulement parce qu’elle a 
suscité un débat sur les dispositifs d’expertise mais aussi parce qu’elle met 
en cause l’organisation capitalistique de la recherche thérapeutique. Pour 
qui accepte de prendre un peu de distance avec le point de vue rétros-
pectif des inspecteurs de l’IGAS considérant que le benfluorex est une 
amphétamine et n’aurait pas dû, d’un point de vue pharmacologique, être 
autorisé, ce qui est en jeu dans la trajectoire du Mediator est une certaine 
organisation de la recherche, centrée sur les divisions de « R & D » internes 
aux entreprises, laquelle domine le secteur du médicament depuis la 
Seconde Guerre mondiale.
Ce régime n’est pas propre au domaine de la santé. La pharmacie 
compte, avec la chimie organique, l’électricité ou l’électronique, parmi 
les secteurs de haute technologie que l’historiographie a retenus comme 
exemples de l’émergence, au xxe siècle, de la recherche d’entreprise. La 
figure du chercheur industriel est en effet une création récente, sans 
équivalent au xixe siècle ; du moins si l’on prend soin de distinguer la 
recherche industrielle de l’existence de collaborations et échanges entre 
universités et entreprises, lesquelles ont une histoire aussi longue que 
celle des institutions académiques 1.
Cette apparition du chercheur industriel est généralement illustrée par 
l’histoire des laboratoires propres à de grandes firmes ou par les chiffres 
collectés par des États qui, à partir des années 1930, ont fait de la capacité 
de recherche industrielle, c’est-à-dire de l’ampleur des investissements 
humains et financiers de « R & D », un enjeu stratégique, la condition de 
la sécurité nationale, de la croissance économique et du progrès social. 
1. Shapin 2008.

	
une manière industrielle de savoir 	
87
Un des meilleurs exemples de cette littérature sur l’internalisation des 
pratiques de recherche est l’analyse de la trajectoire des laboratoires de 
recherche de General Electric et de la Bell proposée par Leonard Reich 
dans The Making of American Industrial Research 1. Celui-ci met tout 
particulièrement en lumière l’importance des questions de propriété 
intellectuelle, le rôle de « personnalités » hybrides ayant une carrière 
académique et une expérience d’ingénieur, les fonctions de « service » 
pour le reste de l’entreprise, et le fait que la croissance de ces labora-
toires a donné les moyens d’une fondamentalisation des investigations. 
Des dynamiques similaires ont été mises au jour pour DuPont 2, Kodak 3 
ou BASF 4.
L’ambition de ce chapitre est de revenir sur les conditions d’apparition et 
les tensions propres à cette « manière industrielle de savoir ». Mais il s’agit 
aussi de prendre en compte les transformations récentes qui conduisent 
nombre d’observateurs de l’innovation scientifique et technique à conclure 
que le modèle de l’usine à chercheurs, si ce n’est la science industrielle, 
a fait son temps, remplacé par une nouvelle « économie de la connais-
sance 5 ». En d’autres termes, il s’agit de savoir dans quelle mesure le régime 
de la R & D « interne » n’a pas été une parenthèse, une forme de gouver-
nement des savoirs indissociable des « Trente Glorieuses », indissociable 
d’une l’économie politique issue de la Grande Dépression et qui a réglé  
les rapports entre État, entreprises et marchés durant l’après-guerre. Pour 
cela, on partira de ce que les travaux récents sur la pharmacie nous disent 
du modèle de la R & D d’entreprise, de son fonctionnement et de son futur.
R & D et screening pharmaceutique : une science  
au service de la construction de marchés
Souvent mise en parallèle avec la trajectoire des entreprises de l’élec-
tricité, l’histoire des firmes de la chimie qui ont, comme Bayer, pénétré le 
monde du médicament est à l’origine d’un scénario spécifique opposant 
première et seconde moitié du xxe siècle.
Avant guerre, le monde de la pharmacie est un monde de professionnels 
et de petites entreprises. De professionnels parce que la pharmacie est, 
comme la médecine, une profession libérale, organisée autour de la 
1. Reich 1985.
2. Hounshell et Smith 2000.
3. Jenkins 1975.
4. Abelshauser 2001.
5. Kahin et Foray 2006.

88	
jean-paul gaudillière
possession d’un diplôme et d’un monopole d’exercice. De petites entreprises 
parce que le médicament, s’il est massivement consommé, relève de l’officine 
(où sont élaborées les préparations magistrales, à la demande du médecin 
et selon les recettes du Codex apprises sur les bancs de la Faculté) ou de 
la firme de petite taille, dirigée par un pharmacien, fabriquant quelques 
spécialités prêtes à l’emploi, combinaisons de substances répertoriées dans 
le Codex ou innovations plus radicales mais aux compositions secrètes. 
Après 1945, le paysage bascule : la pharmacie devient un secteur industriel 
en prise sur un marché de masse, à forte intensité de R & D 1. Changement 
d’échelle, standardisation, capacités organisationnelles et mobilisation de la 
chimie sont alors considérés comme les raisons du succès de la « révolution 
thérapeutique 2 » mais aussi comme l’explication des liens entre ce système 
et le contexte de la grande « corporation » à l’origine du screening comme 
méthode d’invention de nouveaux médicaments.
Plus précisément, l’historien John Lesch a relaté comment, dans les 
années 1920 et 1930, Bayer a réinventé ses dispositifs de recherche interne 
pour adopter le screening 3. Il décrit ainsi comment, sous l’égide de Gerhard 
Domagk, la compagnie a organisé une quête systématique et planifiée de 
nouvelles molécules cliniquement actives et commercialement viables. Le 
cœur de ce screening première manière était la conjonction entre une vaste 
infrastructure chimique employant des dizaines de techniciens et docteurs 
en charge de la synthèse à grande échelle de molécules organiques, et un 
laboratoire de pharmacologie où biologistes et pharmaciens multipliaient 
les tests sur un petit nombre de modèles animaux standards. L’originalité 
du système de Bayer tenait moins à cette conjonction qu’à l’échelle et  
à l’organisation des opérations. Ce dispositif, qui allait déboucher sur la 
mise sur le marché des premiers sulfamides, était moins une innovation 
intellectuelle qu’une nouvelle pratique organisationnelle, une « indus-
trialisation » de la recherche pharmaceutique.
L’historiographie récente a mis à mal cette interprétation de deux 
manières. D’une part en soulignant son caractère téléologique : le résultat 
(la généralisation du screening) est pris comme explication du processus ; 
d’autre part en rappelant la diversité des formes de savoir et des contextes 
d’émergence des composés emblématiques de la révolution thérapeutique. 
Les trajectoires des antibiotiques, des hormones sexuelles, des corti-
coïdes ou des psychotropes montrent combien il est réducteur de faire 
de la chimie et de la manipulation des molécules le principal registre de 
1. Gaudillière 2007.
2. Weatherall 1990.
3. Lesch 1993 et 2007.

	
une manière industrielle de savoir 	
89
savoirs mobilisés – aux dépens de la clinique ou des savoirs du vivant 1. 
De même, les trajectoires d’autres firmes comme Schering, Merck ou 
Hoffman soulignent le caractère graduel et tardif, pas avant les années 
1960, de l’introduction du screening 2. Un autre registre d’explication de la 
priorité donnée à la recherche interne et au screening est donc à élaborer.
Une première grille de lecture renvoie aux travaux prenant pour objet 
l’histoire de produits ou de firmes – par exemple la firme Schering, 
entreprise d’origine familiale et typique du régime professionnel de la 
pharmacie. Dans l’entre-deux-guerres, Schering connaît une croissance 
importante 3. Comme nombre de ses concurrents européens, la compagnie 
s’intéresse aux produits tirés du vivant, en particulier aux préparations 
hormonales. Elle met sur le marché une palette d’extraits des glandes 
sécrétrices susceptibles d’une utilisation thérapeutique : insuline tirée  
du pancréas, extraits de foie, extraits de testicules ou d’ovaires, etc. Dans 
les années 1920 et 1930, les hormones sexuelles sont les produits phares 
de Schering qui devient un acteur clé de la transformation des extraits 
de glandes en stéroïdes, des substances purifiées et caractérisées par leur 
structure moléculaire, produites par synthèse partielle. Un processus de 
« molécularisation » a donc précédé et conditionné l’adoption du screening.
Contrairement au modèle Bayer, la recherche de Schering n’était pas 
centrée sur la chimie. L’essentiel des investigations biochimiques sur les 
hormones stéroïdes a été réalisé par un laboratoire universitaire, celui 
d’Adolf Butenandt, directeur de l’Institut de biochimie de la Kaiser-
Wilhelm Gesellschaft à Berlin et collaborateur de longue durée de la 
firme 4. En interne, Schering disposait d’une importante infrastructure de 
recherche physiologique dont l’origine est à chercher dans les pratiques 
de standardisation et de contrôle des extraits de glandes, c’est-à-dire 
dans le traitement industriel de la matière médicale. Les essais animaux 
destinés à évaluer l’activité des préparations par la mesure quantitative 
d’un effet sur l’organisme (par exemple la croissance de l’utérus de souris 
dépourvues d’ovaires pour les hormones œstrogènes) étaient ainsi au 
cœur de l’activité du laboratoire maison. La « fondamentalisation » des 
travaux, parallèle à celle évoquée plus haut pour l’électricité, a été portée 
par la maîtrise de ces outils et s’est initialement faite dans une perspective 
physiologique, en relation étroite avec un petit noyau de cliniciens à qui 
Schering confiait l’exploration des propriétés cliniques de ses prépara-
tions selon des modalités ad hoc. Cela ne veut pas dire que la chimie était 
1. Bud 2008, Haller 2012, Healy 1997, Quirke 2008, Ratmoko 2010.
2. Bächi 2009, Bürgi 2011, Galambos et Sewell 1997, Gaudillière 2010.
3. Bartmann 2003, Höllander 1955, Wimmer 1994.
4. Gaudillière 2005.

90	
jean-paul gaudillière
absente mais que son rôle était de contribuer à l’industrialisation de la 
préparation : dans un premier temps pour varier les solvants et les condi-
tions d’extraction ; ensuite, lorsque les extraits sont devenus des stéroïdes 
purifiés, pour trouver de nouvelles matières premières ou en définissant 
des protocoles de réactions utilisables à grande échelle.
Au milieu des années 1960, cette description n’est plus de mise. Le 
laboratoire de physiologie a été remplacé par un laboratoire de biochimie, 
internalisant le type de recherches effectuées par Butenandt, tandis que 
le laboratoire de chimie a pris la forme d’un département incluant des 
centaines de chercheurs et techniciens occupés non seulement à l’amé-
lioration des protocoles mais aussi et surtout à la synthèse de molécules 
nouvelles 1. De plus, comme le montre la diversification des hormones des 
glandes surrénales en une vaste famille de corticostéroïdes, une véritable 
planification de la découverte a été mise en place, basée sur la succession 
linéaire des étapes de screening : choix des têtes de série, synthèse des 
analogues et dérivés, tests d’efficacité sur un petit nombre de modèles 
animaux, mise à disposition des substances auprès d’un réseau ad hoc 
de médecins partenaires 2.
Entre les deux configurations, l’affaire de la cortisone a servi de catalyseur 
à la réorganisation de la R & D. Pour Schering, l’introduction par Merck, 
en 1947, d’un stéroïde artificiel inventé par ses chimistes hors de toute 
inférence sur la relation structure / fonction, mais actif dans le traitement 
des rhumatismes et des processus inflammatoires 3, a été vécue comme 
une crise dont les leçons ont été mises en pratique dès la fin des années 
1950. Le succès de la cortisone suggérait en effet qu’il était possible de 
construire un marché de masse par la maîtrise d’une molécule obtenue 
par la mobilisation des seules compétences chimiques, sans input clinique 
préalable. Comme la pénicilline, la cortisone était un des premiers « block-
busters » de la révolution thérapeutique, mais, à l’inverse de la première 
dont l’histoire a dû être réécrite pour conforter la saga du screening, la 
seconde n’avait plus d’origine biologique.
Cependant, parce que le screening n’est pas l’affaire d’une ou de quelques 
firmes mais devient bien un modèle général de R & D au cours des « Trente 
Glorieuses », son histoire ne peut être réduite à une microanalyse de 
la contingence des inventions ou des cultures locales. D’où un second 
registre d’interprétation centré sur les transformations à plus large échelle 
des rapports entre savoirs du médicament et modes de construction des 
1. Kobrak 2002.
2. Gaudillière 2013.
3. Rasmussen 2002.

	
une manière industrielle de savoir 	
91
marchés, prenant en compte leur rôle déterminant dans la stabilisation 
du régime de « l’usine à découverte thérapeutique ». De ce point de vue, 
les travaux récents ont privilégié trois éléments : la propriété intellec-
tuelle, les régulations administratives, le marketing scientifique.
Un des facteurs rapprochant la pharmacie d’après guerre de la chimie 
ou de l’électronique est en effet la place croissante prise par le brevet 
dans l’économie du secteur. Le changement est majeur : jusqu’à la guerre, 
la plupart des pays européens excluaient le médicament de la sphère de  
la brevetabilité 1. Les raisons de cette longue exception sont désormais 
bien connues et tiennent à deux aspects : les considérations de santé 
publique et la culture professionnelle des pharmaciens 2. La normalisation 
du brevet de médicament est aussi indissociable de la généralisation des 
systèmes de protection sociale qui a relativisé les problèmes d’accès et 
d’autre part de l’industrialisation de la production qui a éliminé la prépa-
ration magistrale. Le passage d’un monde où l’appropriation des agents 
thérapeutiques passait par la marque, le secret ou le monopole technique 
à un mode dominé par le brevet a toutefois été nourri par l’évolution des 
pratiques de recherche interne et a, en retour, changé leur nature.
À partir de la fin du xixe siècle, l’exclusion de brevetabilité des substances 
thérapeutiques n’impliquait pas l’absence complète d’appropriation 
par brevet. Celle-ci portait toutefois sur les procédures et pas sur les 
substances : protocoles d’isolement, séquences réactionnelles, procédés de 
synthèse. S’est ainsi constitué, autour des firmes chimiques et pharmaceu-
tiques, un véritable milieu du brevet associant ingénieurs de production, 
avocats des services juridiques des compagnies, experts des cabinets  
de conseil, examinateurs des offices de la propriété industrielle, juges des 
tribunaux spécialisés. À travers la soumission de demandes étendues de 
protection ou de dépôts de plainte pour copie illicite, ce milieu a porté 
un glissement de la jurisprudence du brevet de procédé au brevet de 
substance 3. En conséquence, dès la fin des années 1930, il était devenu 
monnaie courante de considérer que la production d’une substance dont 
le procédé de fabrication avait été breveté constituait une infraction au 
régime de la propriété intellectuelle, mais aussi d’accepter les demandes de 
brevets sur des agents thérapeutiques, dès lors que les textes ne faisaient 
pas référence à un usage médical 4.
L’émergence du brevet de médicament a ainsi été nourrie par la molécu-
larisation, mais elle l’a aussi renforcée. Dès lors que toute composition 
1. Gaudillière 2008.
2. Cassier 2004.
3. Pottage 2010.
4. Cassier 2008, Gaudillière 2008.

92	
jean-paul gaudillière
de matière décrite comme molécule nouvelle du fait de son origine dans 
les laboratoires était brevetable, investir dans la recherche interne de 
substances chimiques à action thérapeutique devenait un outil puissant 
de contrôle et de création des marchés pharmaceutiques. Un nouvel  
effet de boucle entre formes de savoir et régime de propriété s’est ainsi 
mis en place, finalement sanctionné par les changements législatifs de 
l’après-guerre qui mirent formellement fin à l’exemption de brevetabilité 1.
Un second facteur de transformation des rapports entre savoirs indus-
triels et construction des marchés qui a pesé d’un grand poids dans la 
généralisation du screening est le changement des dispositifs de régulation 2. 
Dans son acception contemporaine, le modèle du screening ne se réduit 
pas au couplage entre les laboratoires de chimie et de pharmacologie, 
il inclut l’ensemble des étapes d’essais cliniques des molécules. Ceux-ci 
sont idéalement divisés en essais de phase I destinés à évaluer la toxicité 
et les effets secondaires, essais de phase II portant sur l’efficacité théra-
peutique par rapport à un traitement placebo, là aussi sur des effectifs 
limités, et enfin essais de phase III destinés à confirmer les données  
d’efficacité sur une plus grande population et dans des conditions plus 
proches de la routine. L’histoire de l’introduction des essais statistiques 
standards en médecine est bien connue 3, en particulier les relations 
étroites qu’elle entretient avec les réformes du statut légal du médicament 
et la mise en place du système des agences 4. La trajectoire de la Food 
and Drug Administration (FDA) américaine et la réforme de 1962 qui fit 
pour la première fois des essais contrôlés un dispositif légal font référence 
en la matière. L’inclusion systématique des essais dans le screening tient 
donc au fait que la mise en œuvre de ces différentes étapes devint une 
obligation administrative imposée aux compagnies.
Cette lecture mettant en opposition État et marché fait toutefois l’éco-
nomie du rôle des compagnies dans l’adoption de ces réformes. Dominique 
Tobbell montre ainsi comment, dans le cas des États-Unis, la participation 
des grandes firmes et de leur association professionnelle aux nouvelles 
régulations a très vite remplacé l’opposition au nouveau dispositif 5. Plus, 
la réforme américaine de 1962 a été cooptée comme un moindre mal face 
à ce qui apparaissait, dans les discussions du Congrès américain, comme 
une menace plus redoutable : un contrôle des prix et une réforme du brevet. 
Là encore, comme dans le cas des brevets, la régulation a été portée par 
1. Chauveau 1999.
2. Gaudillière et Hess 2013.
3. Marks 1997.
4. Carpenter 2010.
5. Tobbell 2012.

	
une manière industrielle de savoir 	
93
la transformation des pratiques de recherche clinique des firmes et elle 
a en retour consolidé la généralisation du screening.
Un bon exemple de cette dialectique est fourni par le cas de la firme 
suisse Geigy. Dans les années 1950, comme Schering, celle-ci fait réaliser 
ses évaluations cliniques sur une base peu formalisée : un petit nombre 
de praticiens hospitaliers sont des associés de longue durée, ils reçoivent 
toutes sortes de préparations, demandent parfois qu’on leur en fabrique, 
ils choisissent librement les cibles pathologiques et les conditions de 
traitement. La principale contrepartie de leur accès aux substances, voire 
du soutien financier dont ils bénéficient, est une obligation d’information 
sur les cas cliniques. Dans ce système, la statistique jouait un rôle négli-
geable. La décennie suivante voit la mise en place d’une autre organisation : 
création d’une division de la recherche clinique, écriture de protocoles, 
essais coordonnés d’une même molécule dans plusieurs centres, croissance 
du nombre de patients, agrégation statistique et finalement organisation de 
l’évaluation et de la décision en deux phases, l’une toxicologique et l’autre 
thérapeutique. La standardisation et la « protocolisation » sont ainsi les 
moyens d’une « internalisation » du contrôle des expérimentations cliniques 
en lien étroit avec la planification des innovations. Elles participent d’une 
dynamique interne, gestionnaire, de recours aux essais statistiques. Mais 
cette évolution est aussi une réponse à la régulation administrative. Dans 
le cas de Geigy, le problème n’était pas initialement la mise sur le marché 
en Suisse, en France ou en Allemagne, pays où les critères d’évaluation 
clinique restaient ouverts, mais l’accès au marché américain.
Quoique réalisés par des praticiens non salariés, dans des hôpitaux 
et centres de soins extérieurs aux firmes, les essais cliniques massi-
vement développés à partir des années 1970 relèvent de l’univers de la 
recherche industrielle pour au moins deux raisons. La première est que 
le changement d’échelle des opérations et le financement par les entre-
prises du médicament impliquaient une centralisation de l’organisation 
de l’expérimentation par les divisions cliniques et leurs responsables, 
médecins le plus souvent. Du choix des cibles jusqu’aux choix de publi-
cation (ou non) des résultats en passant par la définition des protocoles, 
la totalité des décisions importantes sont ainsi devenues des affaires 
« internes ». La seconde raison est que cette nouvelle composante de la 
R & D a été intégrée à un dispositif élargi de screening et de planification 
stratégique, un dispositif donnant un rôle décisif au marketing scientifique.
L’essor du marketing après la guerre a en effet profondément modifié 
la nature de la R & D pharmaceutique. La publicité n’était certes pas un 
fait nouveau de la pharmacie. Dans les années 1920 et 1930, en France 
ou en Allemagne, les annonces concernant les médicaments occupaient  

94	
jean-paul gaudillière
la première place dans les différents secteurs de l’industrie. Pour autant,  
les modalités et cibles de cette publicité ont profondément évolué 
entre 1930 et 1970 1.
L’émergence du marketing scientifique correspond à un changement 
des cibles : la publicité par annonce qui portait en majorité sur des spécia-
lités vendues sans ordonnance et visait les patients a cédé la place à un 
ensemble de médias destinés aux médecins et concernant les produits 
accessibles seulement par la prescription 2. En prise sur la profession-
nalisation du marketing, cette évolution est passée par une refonte des 
dispositifs de promotion. Priorité a ainsi été donnée à la visite médicale, 
ce système de représentants disposant d’une formation technique sur 
les propriétés des produits dont les « visiteurs » assurent la publicisation 
auprès des médecins. Au sein de firmes comme Bayer ou Geigy, en trois 
décennies, le nombre des visiteurs est ainsi passé de quelques dizaines 
à plusieurs milliers. Plus généralement, les campagnes sont devenues  
des opérations intégrées reposant sur une palette d’outils incluant lettres 
personnalisées, brochures, reproductions d’articles, invitations à des 
conférences, produits dérivés, etc.
La croissance rapide des investissements marketing a ainsi mobilisé 
la science de deux manières différentes. D’une part, le marketing est 
lui-même devenu une activité de recherche : les divisions en charge de la 
planification des mises sur le marché ont fait appel aux savoirs des experts 
du marketing avec leur cortège de donnés issues de la psychologie, de 
la sociologie, de la sémiotique et de l’économie de la santé. Mais elles  
ont aussi mis en place des enquêtes sur les marchés allant bien au-delà du 
simple suivi des ventes et visant à comprendre l’évolution des pratiques 
de soin, la sociologie des médecins, leurs comportements de prescription, 
les attentes des patients / consommateurs. D’autre part, le marketing 
« scientifique » est devenu scientifique parce que la mobilisation – au 
sens de mise en forme et en circulation – des résultats de la recherche a 
acquis une place centrale dans les argumentaires de promotion destinés 
à influencer les pratiques de prescription.
Une conséquence majeure de ces déplacements a été l’intégration 
des essais cliniques et du marketing dans une perspective de prévision, 
à moyen terme, de l’innovation et de la construction des marchés. La 
notion de « cycle de vie » dont l’usage s’est généralisé dans les années 
1960 en est un bon marqueur 3. Issue de l’utilisation par les profes-
1. Gaudillière et Thoms 2013.
2. Tone et Siegel Watkins 2007, Greene et Siegel Watkins 2012.
3. Thoms 2014.

	
une manière industrielle de savoir 	
95
sionnels du marketing des concepts de la cybernétique, celle-ci met en 
avant le caractère provisoire des produits, leur obsolescence inévitable 
et la nécessité d’un recours continu à des nouveautés. L’intégration de 
la recherche clinique et du marketing a ainsi fait émerger une nouvelle 
figure de chercheur industriel avec ces médecins engagés au long cours 
dans le montage, l’évaluation et la publicisation des essais. Elle a aussi 
créé des effets en retour par lesquels les potentialités du marché, telles 
qu’évaluées par les services de marketing, ont non seulement porté à 
la diversification par les départements de chimie et de pharmacologie  
de certaines classes de molécules mais aussi poussé à la redéfinition, sous 
l’égide des firmes et de leurs campagnes de promotion, des frontières de 
maladies et des besoins de santé 1.
Le cas de la pharmacie est ainsi un excellent révélateur du caractère 
situé dans le temps du modèle de « l’usine à chercheurs » ainsi que des 
liens qu’il entretient avec des formes particulières de savoir. Il met  
aussi en exergue les relations existant entre le régime de la R & D interne 
et les formes d’organisation et de régulation du capitalisme « fordiste 2 ». 
Ce dernier associe en effet deux grandes configurations institution-
nelles : d’une part la grande entreprise visant la production de masse, 
les économies d’échelle et privilégiant l’organisation en divisions ; d’autre 
part le système de protection sociale assurant une redistribution (limitée) 
de la richesse alors conçue comme garantie de la croissance. Dans ce 
contexte, la planification (entendue comme intervention économique 
à l’échelle nationale et visant une anticipation de moyenne durée) a été 
une pratique publique tout autant que privée. C’est elle qui, pendant 
un demi-siècle, a fourni son substratum à une recherche industrielle 
mimant les pratiques académiques et celles des États. Dans ce contexte, la 
production de connaissances était un outil essentiel de construction des 
marchés, comme source de nouveaux produits mais aussi, et peut-être 
surtout, en tant qu’outil de régulation économique.
La fin de l’usine à chercheurs ? Le chercheur entrepreneur  
et l’économie « de la connaissance »
Au cours des années 1990, les lecteurs de Science ou de Nature ont 
commencé à voir apparaître de nouvelles figures de savants – des person-
nalités dont on pouvait suivre la réussite comme dirigeants d’entreprise 
1. Greene 2007.
2. Aglietta et Boyer 1976.

96	
jean-paul gaudillière
dans les articles de Business Week, The Economist ou du Wall Street 
Journal. Si Bill Gates avec la trajectoire de Microsoft est, par le nombre 
d’articles et de livres qui lui ont été consacrés, la plus visible de ces figures 
de chercheur entrepreneur, le cas du biochimiste Craig Venter est tout 
aussi central.
Chercheur des Instituts nationaux de la santé (NIH) américains, Venter 
quitta en effet l’institution publique en charge de la coordination du 
programme de séquençage du génome humain en 1997 dans le cadre d’une 
polémique concernant à la fois les modalités techniques pour augmenter 
la productivité du décryptage de la structure moléculaire des chromo-
somes humains et l’opportunité de breveter les fragments de gènes, aux 
fonctions inconnues, alors utilisés pour l’analyse à haut débit de l’ADN. 
Suite à son départ, Venter prit la direction de sa propre firme de biotech-
nologie Celera (financée par la compagnie d’instrumentation Perkin 
Elmers) dont la raison d’être était la poursuite de recherches génomiques 
du double point de vue de la mise au point des automates de séquençage 
et de l’identification de gènes susceptibles de jouer un rôle intéressant 
en agriculture ou en médecine. En 2001, la présentation officielle de la 
structure (presque) complète « du » génome humain consacrait la réussite 
de Celera puisque Craig Venter et Francis Collins, le directeur des NIH, 
y étaient associés pour célébrer la fin du programme et les mérites de la 
collaboration entre secteur public et recherche biotechnologique privée.
Cette figure est a priori un contre-modèle du savant de l’usine à 
chercheurs. Incarnant la créativité individuelle, le caractère imprévisible 
des innovations radicales, la rupture avec les grandes organisations, « le » 
chercheur entrepreneur (en la matière il n’y a pas de « la ») poursuit son 
activité aux frontières de l’Université et du business, apportant des réponses 
concrètes à l’enlisement bureaucratique des « usines à chercheurs » et à 
la crise de la R & D planifiée.
Sociologues et économistes de l’innovation renvoient cette nouvelle 
incarnation des liens entre sciences, innovation et industrie à l’émergence 
d’une économie dite de la connaissance caractérisée par la conjonction 
de trois éléments : 1o de nouveaux registres de savoir ; 2o la création de 
petites entreprises (start-up) se consacrant essentiellement ou exclusi-
vement à la recherche ; 3o un financement par le capital-risque et une 
valorisation précoce des résultats par la création de droits de propriété 
intellectuelle. Cette économie a pris son essor dans les années 1980 autour 
de deux secteurs : les technologies de l’informatique et les biotechnologies 
bénéficiant d’investissements en croissance exponentielle pour l’essentiel 
collectés par le biais de marchés boursiers spécialisés. Ce qui justifie 
l’idée d’un nouveau modèle est que la création de firmes de recherche 

	
une manière industrielle de savoir 	
97
est devenue une pratique (assez) courante des chercheurs universi-
taires (particulièrement aux États-Unis) et qu’elle est entretenue par  
un puissant mouvement d’externalisation de la R & D des grandes entre-
prises. À partir de la fin des années 1980, RCA, AT & T, Westinghouse, 
Hewlett-Packard, IBM, Hoffmann-La Roche, Merck et bien d’autres 
commencèrent à réduire la taille de leurs plateformes de recherche pour 
favoriser la constitution d’un réseau de firmes partenaires : des firmes 
chargées des étapes les plus expérimentales de la R & D, mobilisant des 
chercheurs académiques aussi bien que leurs propres employés et produc-
trices de brevets plutôt que de biens 1.
Pourquoi une telle évolution ? Une première réponse fait écho au 
discours des chercheurs entrepreneurs et met en avant l’épuisement  
des capacités d’innovation de la recherche industrielle d’après guerre. Le 
cas de la pharmacie est là encore emblématique en ce que la notion de 
« crise de l’innovation thérapeutique » y est, depuis une décennie, une 
question débattue publiquement. Celle-ci est associée à la réduction 
depuis les années quatre-vingt-dix du nombre de nouvelles molécules 
bénéficiant d’une autorisation de mise sur le marché ; à l’augmentation 
des coûts moyens de la recherche préalable à cette mise sur le marché ; 
et à la multiplication des retraits après introduction en clinique du  
fait d’événements indésirables reconnus comme non maîtrisables ou 
d’un échec commercial dans un contexte de concurrence renforcée et  
de montée de l’évaluation économique du « service médical rendu ». Pour 
les industriels, cette crise est d’abord la conséquence du durcissement 
des régulations administratives, de la généralisation des essais cliniques 
et de l’adoption, par les agences, des exigences de la médecine dite « des 
preuves ». Pourtant, si l’on considère la situation à la FDA, la chrono-
logie des autorisations et leurs coûts sont loin de renforcer cette lecture 2. 
Alors que le nombre des mises sur le marché est resté relativement stable 
depuis les années cinquante, les sommes de R & D investies n’ont cessé 
de croître sans que les changements de la réglementation (réforme de 
1962 comprise) ne pèsent sur la tendance. Ce qui semble en cause est 
donc moins l’organisation administrative du marché que la productivité 
décroissante du screening et les dynamiques propres au régime de l’usine 
à chercheurs.
Cette idée « schumpéterienne » d’un régime d’innovation arrivant à 
épuisement a fortement contribué au boom des biotechnologies. En 
effet, contrairement à la situation qui avait présidé aux investissements 
1. Buderi 2002.
2. Munos 2009.

98	
jean-paul gaudillière
en recherche pharmaceutique de l’après-guerre ou dans le domaine  
du logiciel et de la micro-informatique dans les années 1980 et 1990, les 
biotechnologies du gène n’avaient aucun produit thérapeutique nouveau 
à offrir. La thérapie génique tout comme les applications médicales 
de la génomique autres que l’amélioration des diagnostics de risque 
étaient une pure promesse 1. Sans la confiance des grandes entreprises 
du médicament, la multiplication des start-up, leur financement par le 
capital-risque, la mise en place de partenariats scientifiques et finan-
ciers complexes auraient été beaucoup plus difficiles. La crédibilité de  
la promesse biotechnologique a ainsi bénéficié d’une conviction interne 
à la pharmacie selon laquelle la connaissance des macromolécules biolo-
giques, de l’ADN et des protéines était la voie de renouvellement du 
screening car elle permettrait l’identification de nouvelles cibles.
Ces liens croissants entre grandes entreprises de la pharmacie et start-up 
de biotech s’inscrivent dans le phénomène plus général d’externalisation 
de la recherche sur le médicament. Un exemple parlant est fourni par 
la diversification des Contract Research Organizations (CRO) 2. Leurs 
fonctions peuvent être très diverses : fourniture d’articles clés en main 
et suivi des processus de publication ; expertise et montage de dossiers 
de propriété intellectuelle ; organisation d’essais cliniques et préparation  
des demandes d’autorisation de mise sur le marché. Une part essentielle 
de l’activité des divisions médicales se trouve ainsi transférée à des firmes 
spécialisées. Cette externalisation est généralement interprétée comme 
la conséquence d’un processus de « rationalisation » visant à la réduction 
des coûts. La gestion externe transfère la construction des réseaux de 
collaboration hospitaliers, le recrutement des patients ou la collecte  
des données à des entreprises spécialisées dans la gestion de pools de 
praticiens et de populations cibles. Les CRO remplacent donc les services 
hospitaliers et, à la différence de ces derniers, elles n’ont que très peu 
d’autonomie financière et partagent la même culture managériale que 
les industriels donneurs d’ordre. Et comme l’a souligné Philip Mirowski, 
l’ampleur du recours aux CRO n’est pas indépendante des transforma-
tions de la recherche académique et de sa « commercialisation 3 ».
Au-delà de l’industrie pharmaceutique, l’érosion du régime de l’usine à 
chercheurs renvoie aux critiques des départements internes des firmes, 
lesquels auraient cultivé une forme de « retranchement » les faisant 
ressembler à des départements académiques. Transformation des divisions 
1. Martin 1999, Martin et Hedgecoe 2003.
2. Mirowski et Van Horn 2005, Sismondo 2009.
3. Mirowski et Sent 2006.

	
une manière industrielle de savoir 	
99
de recherche en unités financièrement autonomes, contrats ciblés avec 
des start-up et des universités, évaluation de la productivité, renfor-
cement de la propriété intellectuelle : les mêmes solutions semblent avoir 
été promues pour réformer la R & D « classique ». Le degré de généra-
lisation du modèle du chercheur entrepreneur en dehors des secteurs 
emblématiques de « l’économie de la connaissance », par exemple dans 
des secteurs à forte intensité de R & D comme l’automobile ou l’aéro-
nautique, reste toutefois une question ouverte.
La nouvelle économie de la connaissance semble donc participer de la 
stabilisation d’un autre régime de régulation du capital et des marchés, 
à partir des années 1980. Sans entrer dans une discussion détaillée des 
nouvelles pratiques de gestion, il faut d’abord souligner combien le gouver-
nement des entreprises a changé par rapport au modèle « chandlerien » 
de la grande firme organisée en divisions fonctionnelles et pratiquant 
l’intégration verticale. L’externalisation, la flexibilisation des tâches, la 
mise en avant de la variété, des effets de niche, de la production just in 
time, tout autant que les délocalisations industrielles, ont contribué à la 
mise en place d’organisations privilégiant l’entreprise réseau et le fonction-
nement par « projet 1 ».
Outre ces dynamiques propres à l’organisation de la production, 
d’importantes reconfigurations institutionnelles ont précédé et permis 
les réorganisations de la recherche interne. Dans le contexte américain, 
les années 1980 ont ainsi vu la conjonction de trois initiatives politiques 
visant à faire de l’État moins un entrepreneur de recherche que  
l’activateur des nouveaux marchés de la connaissance : 1o la transfor-
mation du Nasdaq en marché boursier de l’innovation ; 2o l’adoption 
du Bayh-Dole Act ; 3o l’élargissement par l’USPTO de la sphère de la 
brevetabilité.
Le Nasdaq est apparu dans les années 1960 comme second marché 
électronique initialement destiné à faciliter les cotations et à améliorer 
leur transparence. Sa nature a changé avec les réformes qui ont réorganisé 
les transactions sur les marchés financiers (Security Exchange Act). Le 
tournant des années 1980 a consisté à permettre la cotation sur le Nasdaq 
de firmes dont l’équilibre financier n’était pas un acquis mais était une 
perspective assise sur leur potentiel de croissance 2. Les nouvelles règles ont 
ainsi permis que des compagnies sans revenus liés à la vente de produits 
ou à la fourniture de services (typiquement le cas des start-up) puissent 
faire appel au capital financier dès lors que leurs actifs « tangibles », 
1. Boltanski et Chiapello 1999.
2. Coriat 2003.

100	
jean-paul gaudillière
lesquels incluent brevets, marques déposées et droits d’auteurs, sont 
jugés suffisants.
Avec le Bayh-Dole Act de 1980, les universités ont été autorisées à 
octroyer aux entreprises des licences exclusives sur des brevets prenant 
appui sur des résultats de recherches financées par des fonds publics 
fédéraux 1. Cette réforme a eu d’autant plus d’impact qu’elle s’inscrivait 
dans une série de mesures qui, au long des années 1980, avaient déjà 
facilité la commercialisation de la recherche militaire ou émanant des 
laboratoires nationaux. D’une façon générale, la politique fédérale d’après 
guerre qui visait à limiter l’appropriation par l’octroi de licences non 
exclusives ou même (dans le cas de la politique antitrust) de licences 
obligatoires a été abandonnée 2.
L’évolution la plus décisive pour la création d’un véritable marché des 
produits de la recherche a finalement été l’extension de la brevetabilité à 
d’autres entités : gènes, cellules, animaux, logiciels, business models 3, etc. 
Comme dans les années 1930 pour le médicament, le déplacement des 
normes est moins passé par la loi que par la jurisprudence, l’écriture de 
guidelines et les pratiques d’entreprises. Particulièrement spectaculaire 
dans le cas du vivant (un brevet de gène protège aujourd’hui une invention 
qui n’est rien d’autre que la structure moléculaire d’un segment d’ADN 
chromosomique isolé au laboratoire et associé à une fonction biologique 
garante d’une utilité potentielle), cette évolution est contestée, depuis le 
milieu des années 1990, par des collectifs de patients, d’usagers, d’agri-
culteurs ou de cliniciens 4.
Conclusion
L’émergence de la figure du « chercheur entrepreneur » met en évidence 
de profondes transformations des relations entre sciences, industrie et 
marchés. Même si cela implique un certain schématisme, il est intéressant, 
de façon heuristique, de conclure par une typologie prenant en compte 
les formes de savoir, l’organisation de la recherche et de la production, 
les régulations et modalités de construction des marchés. Si l’on prend 
comme fil conducteur les figures dominantes de savants aux prises avec 
l’industrie, on peut avancer l’idée de trois périodes :
1. Movery 2004, Washburn 2005, Berman 2008.
2. Mirowski et Sent 2006, Lécuyer (dans ce volume, p. 423).
3. Kevles 2002, Calvert et Joly 2011.
4. Parthasarathy 2010, Gaudillière et Joly 2013.

	
une manière industrielle de savoir 	
101
Chercheur 
inventeur
Chercheur 
industriel
Chercheur  
entrepreneur
Période  
de visibilité
1860-1920
1920-1980
1980-
Sites privilégiés 
de production  
de connaissance
Universités  
et ateliers
Laboratoires 
et divisions 
de R & D 
des grandes 
entreprises
Universités, 
start-up et 
contract research 
organizations
Secteurs 
emblématiques
Mécanique, 
électricité
Chimie, 
pharmacie, 
électronique
Biotechnologie, 
informatique
Way of knowing
(Pickstone)
Analytique
Synthétique
Moléculaire et 
interactionniste
Normes 
privilégiées
Nouveauté
Stabilité
Pureté
Homogénéité
Reproductibilité
Adaptabilité
Connectivité  
et fonctionnement 
en réseau
Gestion des  
risques
Modèle 
techno-industriel
Mécanisation
Mise au point 
de produits de 
référence
Standardisation 
et planification
Économies 
d’échelle
Pipe-line  
de R & D
Variété et 
production  
flexible
Collecte  
et traitement  
de l’information
Propriété intel-
lectuelle et 
construction des 
marchés
Marques et 
brevets de 
procédés
Exploitation 
directe
Brevets 
(procédés  
et substances) 
comme actifs 
de négociation 
(cartels)
Marketing 
scientifique
Brevets étendus 
(vivant, protocoles) 
et contrats d’usage 
comme actifs 
financiers
Branding et 
marketing des 
styles de vie
Régulations 
économiques  
et sociales
Libre-échange 
et empires 
coloniaux
Fordisme  
et politiques 
keynésiennes
État-nation  
et « providence »
Néolibéralisme, 
globalisation et 
financiarisation
État sécuritaire  
et organisateur  
des marchés

102	
jean-paul gaudillière
Inclure dans cette typologie un régime du « chercheur inventeur » 
dont les modus operandi sont illustrés par des trajectoires de savants ou 
d’ingénieurs comme Pasteur ou Edison vise non seulement à rappeler 
la longue durée de la science industrielle évoquée en introduction à ce 
chapitre mais aussi à insister sur le caractère de parenthèse du régime 
d’après guerre. Par bien des aspects, la figure du chercheur entrepreneur 
semble renouer avec certaines pratiques du début du siècle. Pasteur 
n’était-il pas le fondateur d’une start-up de recherche et de production, un 
savant multipliant les dispositifs de propriété intellectuelle et les contrats 
pour la réalisation de projets aux frontières de l’acquisition de connais-
sances, de l’invention de produits et de la réalisation de services ? Il ne 
s’agit bien sûr pas ici de plaider pour un simple « retour de l’ancien » mais, 
plus modestement, de saisir à quel point le modèle de la R & D interne a 
dépendu d’une économie politique alternative aux doxas libérales faisant 
de l’État l’instrument de libération des marchés.
Références bibliographiques
Abelshauser Werner, 2001, Die BASF. Eine Unternehmensgeschichte, Munich, 
C.H. Beck Verlag.
Aglietta Michel et Boyer Robert, 1976, Régulation et crises du capitalisme. 
L’expérience des États-Unis, Paris, Calmann-Lévy.
Bächi Beat, 2009, Vitamin C für Alle ! Pharmazeutische Produktion, Vermarktung 
und Gesundheitspolitik (1933-1953), Zurich, Chronos Verlag.
Bartmann Wolfgang, 2003, Zwischen Tradition und Fortschritt. Aus der Geschichte 
der Pharmabereiche von Bayer, Hoechst und Schering von 1935-1975, Stuttgart, 
Franz Steiner Verlag.
Berman Elisabeth Popp, 2008, « Why Did Universities Start Patenting ? Institution 
Building and the Road to the Bayh-Dole Act », Social Studies of Science, vol. 38, 
no 6, p. 835-871.
Boltanski Luc et Chiapello Ève, 1999, Le Nouvel Esprit du capitalisme, Paris, 
Gallimard.
Bud Robert, 2008, Penicillin : Triumph and Tragedy, Oxford, Oxford University Press.
Buderi Robert, 2002, « The Once and Future Industrial Research », in Albert Teich 
(dir.), Science and Technology Policy Yearbook, Washington (DC), AAAS.
Bürgi Michael, 2011, Pharmaforschung im 20. Jahrhundert. Arbeit an der Grenze 
zwischen Hochschule und Industrie, Zurich, Chronos Verlag.
Calvert Jane et Joly Pierre-Benoît, 2011, « How Did the Gene Become a Chemical 
Compound : The Ontology of the Gene and the Patenting of DNA », Social Science 
Information, vol. 50, no 2, p. 157-177.
Carpenter Daniel, 2010, Reputation and Power : Organizational Image and 
Pharmaceutical Regulation at the FDA, Princeton (NJ), Princeton University Press.
Cassier Maurice, 2004, « Brevets pharmaceutiques et santé publique en France », 
Entreprise et histoire, no 36, p. 29-47.
–	 2008, « Patents and Public Health in France : Pharmaceutical Patent Law 

	
une manière industrielle de savoir 	
103
in-the-Making at the Patent Office between the Two World Wars », History and 
Technology, vol. 24, no 1, p. 135-151.
Chauveau Sophie, 1999, L’Invention pharmaceutique. La pharmacie française entre 
l’État et la société au xxe siècle, Paris, Les Empêcheurs de penser en rond.
Coriat Benjamin, 2003, « Does Biotech Reflect a New Science-Based Innovation 
Regime ? », Industry and Innovation, vol. 10, no 3, p. 231-253.
Galambos Louis et Sewell Jane, 1997, Networks of Innovation : Vaccines 
Development at Merck, Sharp & Dohme & Mullford (1895-1955), Cambridge, 
Cambridge University Press.
Gaudillière Jean-Paul, 2005, « Better Prepared Than Synthesized : Adolf Butenandt, 
Schering AG and the Transformation of Sex Steroids into Drugs », Studies in 
History and Philosophy of Science, vol. 36, no 4, p. 612-644.
–	 2007, « L’industrialisation du médicament : une histoire de pratique entre sciences, 
techniques, droit et médecine », Gesnerus, vol. 64, no 1, p. 93-108.
–	 2008, « How Pharmaceuticals Became Patentable in the Twentieth Century », 
History and Technology, vol. 24, no 2, p. 99-106.
–	 2010, « Une marchandise scientifique ? Savoirs, industrie et régulation du 
médicament dans l’Allemagne des années trente », Annales, vol. 65, p. 89-120.
–	 2013, « From Propaganda to Scientific Marketing : Schering, Cortisone, and the 
Construction of Drugs Markets », History and Technology, vol. 29, no 4, p. 188-209.
Gaudillière Jean-Paul et Hess Volker (dir.), 2013, Ways of Regulating Drugs in the 
19th and 20th Centuries, Basingstoke et New York, Palgrave Macmillan.
Gaudillière Jean-Paul et Joly Pierre-Benoît, 2013, « Crise des brevets de gènes et 
nouveaux objets d’appropriation. Vers une ontologie biologique de la propriété 
intellectuelle ? », in  Frédéric Thomas et Valérie Boisvert, Le  Pouvoir de la 
biodiversité, Paris, Éd. de l’IRD, p. 115-124.
Gaudillière Jean-Paul et Thoms Ulrike (dir.), 2013, « Pharmaceutical Firms and 
the Construction of Drugs Markets : From Branding to Scientific Marketing », 
introduction au dossier thématique de History and Technology, vol.  29, no  4, 
p. 105-115.
Greene Jeremy A., 2007, Prescribing by Numbers : Drugs and the Definition of Disease, 
Baltimore (MD), Johns Hopkins University Press.
Greene Jeremy A. et Siegel Watkins Elizabeth (dir.), 2012, Prescribed : Writing, 
Filling, Using, and Abusing the Prescription in Modern America, Baltimore (MD), 
Johns Hopkins University Press.
Haller Lea, 2012, Cortison : Wissensgeschichte eines Hormons (1900-1955), Zurich, 
Chronos Verlag.
Healy David, 1997, The Anti-Depressant Era, Cambridge (MA), Harvard University 
Press.
Höllander Hans, 1955, Geschichte der Schering AG, Berlin, Schering AG.
Hounshell David et Smith John K., 2000, Science and Corporate Strategy : DuPont 
R & D, New York, Cambridge University Press.
IGAS (Inspection générale des affaires sociales), 2011, Enquête sur le Mediator. 
Rapport définitif, RM-2011-001P, <  http://www.ladocumentationfrancaise.fr/
rapports-publics/114000028-enquete-sur-le-mediator-R > .
Jenkins Richard V., 1975, Images and Enterprise : Technology and the American 
Photographic Industry, Baltimore (MD), Johns Hopkins University Press.
Kahin Brian et Foray Dominique (dir.), 2006, Advancing the Knowledge Economy, 
Cambridge (MA), MIT Press.

104	
jean-paul gaudillière
Kevles Daniel, 2002, A History of Patenting Life in the United States with Comparative 
Attention to Europe and Canada, Report to European Group on Ethics, Science 
and New Technology, Bruxelles, Commission européenne.
Kobrak Chistopher, 2002, National Cultures and International Competition : 
The  Experience of Schering  AG (1851-1950), Cambridge, Cambridge University 
Press.
Lesch John E., 1993, « Chemistry and Biomedicine in an Industrial Setting : 
The  Invention of the Sulfa Drugs », in  Seymor H.  Mauskopf (dir.), Chemical 
Sciences in the Modern World, Philadelphie (PA), University of Pennsylvania Press, 
p. 158-215.
–	 2007, The First Miracle Drugs : How the Sulfa Drugs Transformed Medicine, Oxford, 
Oxford University Press.
Marks Harry, 1997, The Progress of Experiment : Science and Therapeutic Reform in 
the United States (1890-1990), Cambridge, Cambridge University Press.
Martin Paul, 1999, « Gene as Drugs : The Social Shaping of Gene Therapy and the 
Reconstruction of Genetic Disease », Sociology of Health and Illness, vol. 21, no 5, 
p. 517-538.
Martin Paul et Hedgecoe Adam, 2003, « The Drugs Don’t Work : Expectations 
and the Shaping of Pharmacogenetics », Social Studies of Science, vol.  33, no  3, 
p. 327-354.
Mirowski Philip et Sent Esther-Mirjam, 2006, « The Commercialization of Science 
and the Response of STS », in New Handbook of STS, Cambridge (MA), MIT Press.
Mirowski Philip et Van Horn Robert, 2005, « The Contract Research Organization 
and the Commercialization of Scientific Research », Social Studies of Science, 
vol. 35, no 4, p. 503-548.
Mowery David C., 2004, Ivory Tower and Industrial Innovation, University-Industry 
Technology Transfer before and after the Bayh-Dole Act in the United States, Palo 
Alto (CA), Stanford University Press.
Munos Bernard, 2009, « Lessons from 60 Years of Pharmaceutical Innovation », 
Nature Reviews in Drug Discovery, no 8, p. 959-968.
Parthasarathy Shobita, 2010, Building Genetic Medicine : Breast Cancer, Techno­
logy and the Comparative Politics of Health Care, Cambridge (MA), MIT Press.
Pottage Alain, 2010, Figures of Invention : A History of Modern Patent Law, Oxford, 
Oxford University Press.
Quirke Viviane, 2008, Collaboration in the Pharmaceutical Industry : Changing 
Relationships in Britain and France (1935-1965), New York, Routledge.
Rasmussen Nicolas, 2002, « Steroids in Arms : Science, Government, Industry and 
the Hormones of the Adrenal Cortex in the United States (1930-1950) », Medical 
History, vol. 46, no 2, p. 299-324.
Ratmoko Christina, 2010, Damit die Chemie Stimmt. Die Anfänge der industriellen 
Herstellung von weiblichen und männlichen Sexualhormonen (1914-1938), Zurich, 
Chronos Verlag.
Reich Leonard, 1985, The Making of American Industrial Research : Science and 
Business at GE and Bell (1876-1926), New York, Cambridge University Press.
Shapin Steven, 2008, The Scientific Life : A Moral History of a Late Modern Vocation, 
Chicago (IL), University of Chicago Press.
Sismondo Sergio, 2009, « Ghosts in the Machine : Publication Planning in the 
Medical Sciences », Social Studies of Science, vol. 39, no 2, p. 171-198.
Thoms Ulrike, 2014, « Innovation, Life Cycles and Cybernetics in Marketing : 

	
une manière industrielle de savoir 	
105
Theoretical Concepts in the Scientific Marketing of Drugs and Their Conse-
quences », in Ulrike Thoms et Jean-Paul Gaudillière (dir.), Research for Sales : 
The  Development of Drugs Scientific Marketing in the 20th Century, Londres, 
Pickering & Chatto.
Tobbell Dominique, 2012, Pills, Power and Policy : The Struggle for Drug Reform in 
Cold War America and Its Consequences, Berkeley (CA), University of California 
Press.
Tone Andrea et Siegel Watkins Elizabeth (dir.), 2007, Medicating Modern America : 
Prescription Drugs in History, New York, New York University Press.
Washburn Jennifer, 2005, University, Inc. : The  Corporate Corruption of Higher 
Education, New York, Basic Books.
Weatherall Miles, 1990, In Search for a Cure : A  History of Pharmaceutical 
Discovery, Oxford, Oxford University Press.
Wimmer Wolfgang, 1994, « Wir haben fast immer was Neues » : Gesundheitswesen 
und Innovationen der Pharma-Industrie in Deutschland (1880-1935), Berlin, 
Duncker & Humblot.


5 Sciences et savoirs dans l’État  
développementiste
S h i v  V i s va n at h a n
Dans l’antichambre de l’ancien dirigeant ghanéen Kwame Nkrumah se 
trouve, vers 1960, un tableau qui dépeint Nkrumah lui-même se débattant 
pour se libérer de chaînes immenses. Le ciel est chargé d’orage et d’éclairs, 
et dans un coin du tableau se tiennent trois hommes blancs : un mission-
naire serrant une bible dans ses mains, un homme d’affaires portant un 
attaché-case et un anthropologue tenant un livre intitulé Les Systèmes 
politiques africains. Cette peinture représente la lutte entre les nouvelles 
nations émergentes et le colonialisme.
Après la décolonisation, le tiers-monde trouve comme réponse à ces 
problèmes le « développement ». Mot populaire en biologie et retra-
vaillé pour la politique par Harry Truman, le développement devient un 
terme couramment utilisé comme synonyme de changement, de progrès, 
d’innovation 1. La notion d’État développementiste est plus spécifique. 
Elle exprime d’abord l’idée que le développement fait l’objet d’un geste 
catalyseur initié par l’État dont il représente l’idéologie et la compétence 
principale. L’État développementiste institue un contrat social tacite par 
lequel il justifie son existence en promettant sécurité et développement à 
ses citoyens. L’État se conçoit comme un projet. La transition du sous-dé-
veloppement au développement devient un domaine dans lequel on fait 
carrière. La science, c’est-à-dire la science occidentale moderne, est promue 
agent de changement et objet de politique de la part de l’État. Utilita-
riste et instrumentale comme en témoigne la notion clé de « transfert de 
technologie », la science devient le véhicule du développement lui-même 
illustré par le principe de substitution à l’importation. L’État développe-
mentiste se présente ainsi comme un faisceau de perspectives dirigées 
vers la promesse d’un changement économique. Et si l’on entend le terme 
1. Sachs 1992.
 Témoin de l’entrée de l’Inde dans la course au nucléaire au lendemain de son indépendance, le Centre 
Bhabba de recherche atomique fut inauguré le 16 janvier 1961 par le Premier ministre Jawaharlal Nehru.

108	
shiv visvanathan
dans son sens aseptisé, presque tous les mouvements nationalistes du 
tiers-monde en ont produit, au cours de l’histoire, diverses variantes.
La question de la science et du développement  
dans le mouvement national indien
L’État développementiste indien nécessite, pour être compris, d’être 
mis en rapport avec les débats qui se déroulent au sein du mouvement 
nationaliste à partir de 1904, année de lancement du mouvement Swadesi 
(mouvement anticolonial pour « l’autosuffisance », boycottant notamment 
des produits anglais 1) en réponse à la partition du Bengale par Lord 
Curzon. Ces débats comportent un volet important et hautement cosmo-
polite sur la science et la technologie, à la fois comme visions du monde 
et instruments du changement. Ces débats sont au cœur de l’affirmation 
du développementisme et en déterminent nombre de caractéristiques. 
Le nationalisme indien est réflexif et autocritique, particulièrement 
dans ses analyses sur la science et la technologie. Non seulement il est 
bienveillant vis-à-vis des Britanniques, mais il fait aussi du nationalisme 
un terrain pour imaginer le potentiel que l’Occident a perdu ou laissé 
en sommeil au fond de lui-même. Il oppose à la vision d’un Occident 
dominant celle d’un Occident vaincu ou devant trouver des alterna-
tives. La science newtonienne doit par exemple rencontrer ses pendants 
occultes et théosophiques. Les travaux d’Alfred Wallace qui mettent en 
avant la diversité, et notamment la diversité cognitive, sont tout parti-
culièrement importants pour le mouvement nationaliste 2. Par ailleurs, 
l’Occident est perçu comme offrant à l’Inde la possibilité d’incarner cette 
alternative. Ainsi le nationalisme indien affirme-t-il que l’un de ses projets 
est de sauver l’Occident de la modernité – une position dont Gandhi, 
par exemple, est partisan.
Le débat au sein du mouvement nationaliste met aux prises des positions 
différentes quant à l’avenir et la science.
La première est celle des Swadesi nationalistes qui veulent que la science, 
la technologie et l’économie, inchangées dans leur élan modernisateur, 
passent sous contrôle indien. Leur mot d’ordre : « L’acide sulfurique est 
la marque du progrès. » Les traditionalistes et les gandhiens s’opposent à 
cette idée. Pour les traditionalistes, les formes artisanales ne doivent pas 
devenir obsolètes et succomber aux forces de la modernité. Leur principal 
1. Sarkar 1973.
2. Wallace 1908.

	
sciences et savoirs dans l’état développementiste 	
109
représentant, le critique d’art et géologue Ananda Kentish Coomaraswamy, 
invente la notion de société postindustrielle. Ce concept, inspiré de William 
Morris mais qu’il est le premier à imaginer 1, fait référence à une société 
où un artisanat socialiste survit aux assauts du mécanisme industriel 2.
Pour Coomaraswamy, l’industrialisation est un processus monotone et 
standardisé qui détruit la diversité, fait de l’obsolescence une règle et vide 
la tradition de toute substance. Il déclare : « C’est quand on supprime le 
chanteur que l’on préserve la chanson populaire 3. » Opposé aux nouvelles 
teintures synthétiques, il affirme que celles-ci sanctionnent l’hégémonie 
de l’uniformité et condamnent la diversité des teintures végétales.
Si les traditionalistes remettent en question l’obsolescence, Mohandas 
Gandhi critique quant à lui, dans son grand manifeste Hind Swaraj 4, à 
la fois la modernité et la production de masse. Selon lui, le colonialisme 
transforme les villes en sites administratifs et les villages en cloaques.  
Il cite l’exemple de la médecine moderne qu’il considère comme un cycle 
iatrogène – « je mange, je m’empiffre, je prends des médicaments, puis je 
m’empiffre à nouveau » – lui apparaissant comme une forme d’esclavage.
1. Lipsey 1977.
2. Bien plus tard, Daniel Bell en popularisa une vision différente de la « société postindustrielle », 
celle d’une une société de services dominée par la science : Bell 1973.
3. Coomaraswamy 1981 (p. 74-75).
4. Gandhi 2014 [1909].
Gandhiens
Gandhi, Kumarappa
Théosophes
Blavatsky, Besant
Science moderne 
occidentale
Mouvement Swadesi
Har Dayal, Ray
Léninistes
Saha, Mahalanobis
Cosmopolites
Bose, Tagore, Geddes
Traditionalistes
Coomaraswamy
Figure 1 : Six positions sur la science dans le mouvement national indien  
et ses interlocuteurs internationaux

110	
shiv visvanathan
Gandhi opère une distinction profonde entre deux échelles, l’une 
éthique et l’autre cosmique, et deux termes porteurs de vie, le swadesi et 
le swaraj. La notion de swadesi contient une approche du local où, pour 
des considérations éthiques et économiques, les produits et matériaux 
locaux doivent être privilégiés. Le boycott de la machine à tisser auquel 
Gandhi appelle signale la montée des idées du swadesi et un retour à  
la tradition artisanale. Cependant, chez Gandhi, le swadésisme n’est que 
l’octave la plus grave d’une tessiture très large. En effet, sans l’ouverture 
au monde et la libération incluant tous les êtres qu’est le swaraj, le 
swadesi n’est qu’un simple esprit de clocher. Gandhi conçoit le swaraj 
sous la forme de cercles océaniques se répandant en spirales à travers 
la planète entière. Tout système technologique, culturel ou épistémo-
logique doit intégrer en même temps le swaraj et le swadesi. Vue sous 
cet angle, la civilisation occidentale est un localisme qui a cherché à  
s’universaliser 1.
Parmi les autres groupes il y a les cosmopolites, avec Rabindranath 
Tagore et Patrick Geddes. Tagore est un grand poète à l’esprit universel, 
profondément intéressé par la science. Geddes est un biologiste 
écossais devenu urbaniste et éducateur. Dès 1915, dans son livre sur  
l’évolution des villes, Geddes fait la distinction entre deux industrialismes, 
le néolithique et le paléolithique 2. L’industrialisme paléolithique est basé 
sur la rigidité des machines et l’énergie du charbon. L’industrialisme 
néolithique, quant à lui, est inventé après l’écologie et la thermodyna-
mique, et est plus ouvert aux processus du vivant. Selon Geddes, ces 
deux formes d’industrialisme entrent en conflit : l’économie de la feuille 
rivalise avec l’économie de la mine. De nouvelles cités-jardins repré-
sentent l’idée qu’il se fait de la ville néolithique. Patrick Geddes écrit plus 
de 30 rapports sur les villes indiennes 3, et l’on observe aujourd’hui un 
regain d’intérêt pour ses travaux en écologie urbaine.
Tagore est connu pour sa vision de l’université comme d’une ville en 
microcosme. L’université occidentale incarne un esprit urbain hostile 
à la nature, tandis que l’esprit indien en revient toujours à la figure du 
sage vivant dans la forêt en harmonie avec la nature. Tagore prédit que 
le dialogue des civilisations est un dialogue entre ces deux universités 
archétypales – entre l’université-ville de l’Occident et les forêts-univer-
sités d’Inde, entre une science qui domine la nature et un système de 
connaissances qui vit en harmonie avec elle 4.
1. Sahasrabudhey 1985.
2. Geddes 1993 [1915].
3. Tyrwhitt 1947.
4. Tagore 1913.

	
sciences et savoirs dans l’état développementiste 	
111
Geddes pense que l’on peut combiner ces deux approches. Il voit en effet 
le système universitaire comme instaurant une écologie de la connaissance 
à travers le jeu des réactions que suscitent les avis dissidents des écoles. 
L’émergence de l’université médiévale est pour lui le produit de tentatives 
visant à réconcilier les doctrines grecques et celles de l’Église. À la fin  
du xve siècle, l’université de la Renaissance renaît à travers l’invention de 
l’imprimerie et la redécouverte des classiques grecs. L’université germa-
nique créée au xviiie siècle fusionne le travail des grands encyclopédistes 
et du système d’examen moderne. L’université allemande contemporaine 
combine les fonctions de recherche et d’enseignement. Geddes déplore 
que l’Inde ait choisi comme modèle l’université de Londres, qui a réduit 
l’exemple germanique à un système d’examens destiné à produire des 
employés de bureau 1.
Les théosophes n’ouvrent pas seulement la voie au nationalisme mais 
préfigurent aussi une critique « féministe » de la science qui met en avant  
le charnel et le potentiel de l’enfance. Leur défense du corps et de la pluralité 
des idées que l’on peut en avoir se transforme en lutte contre la vivisection 
et la violence des expériences scientifiques. Le mouvement alimente une 
certaine idée de l’occulte comme d’un réservoir de métaphores, visions 
et hypothèses alternatives qui déploie, en défi au monde newtonien, une 
critique de la nature hégémonique de la science occidentale. Alfred Wallace 
fait partie des grands scientifiques dissidents et son livre The Wonderful 
Century en est un exemple classique. Le texte s’ouvre sur un éloge de la 
science victorienne puis offre une préfiguration intéressante du tournant 
kuhnien. Wallace affirme que toute science, à ses heures de gloire, est 
susceptible de se montrer dominatrice et de perdre le sens de la diversité 
comme la perception intuitive du domaine des possibles. Selon Wallace, 
il est impératif que les scientifiques inventent des alternatives et prêtent 
l’oreille aux voix dissidentes. Wallace met ces propos en application en 
considérant avec ouverture la recherche psychique ou les débats sur la 
vaccination. Ses positions dissidentes ne se cantonnent pas à la science 
et portent aussi sur l’importance du socialisme et la promotion de l’indé-
pendance de l’Inde, alors sous statut colonial. La dissension et la diversité 
sont au cœur de sa perspective évolutionniste. Celle-ci est relayée par  
les théosophes et s’étend à la protection animale, au mouvement des 
suffragettes, aux nouvelles idées sur le potentiel de l’enfant développées 
par Maria Montessori 2. La théosophie n’est pas seulement la préfigu-
ration des critiques féministe et kuhnienne de la science. Elle propose 
1. Geddes 1904 (p. 19).
2. Lane 1979.

112	
shiv visvanathan
aussi un cadre d’analyse des différences cognitives en montrant comment 
des métaphores nouvelles ouvrent des possibilités inexplorées dans l’art 
(Nicolai Roerich), l’éducation des enfants (Maria Montessori) ou encore 
la science (Wallace, I.C. Bose). Une grande partie des rêves formulés plus 
tard au sujet d’une science alternative doivent en réalité beaucoup aux 
fondations posées par la théosophie 1.
La plupart des mouvements que nous venons de voir prennent parti 
pour une approche pluraliste ainsi que pour la prise en compte des diffé-
rences cognitives. Cependant, une forme bien particulière de scientisme 
triomphe au sein du mouvement national, puis dans l’Inde indépendante, 
une forme qui offre une vision de la modernité combinant léninisme, 
planification, science et une certaine approche de l’énergie. Ce discours 
puise ses origines dans le groupe « Science et culture » et chez son leader, 
Meghnad Saha 2.
Saha est un brillant physicien spécialiste de l’ionosphère. Devenu 
membre de la Société royale britannique dans les années 1920, il rêve 
d’une société fondée sur la méthode scientifique. Il est poussé à l’action 
en entendant un jour le parlementaire Kailashnatah Katju proclamer,  
lors de l’ouverture d’une usine d’allumettes, qu’il s’agit là d’un pas en avant 
dans l’industrialisation du pays. Exaspéré par une telle confusion, il sent 
qu’il manque au groupe nationaliste du Congrès une approche systéma-
tique de l’industrialisation. Il s’attache à créer une perspective historique 
en consultant les Annales de la Société des Nations et en retire l’idée que 
les civilisations modernes peuvent être classées en fonction de la quantité 
d’énergie qu’elles produisent. Selon ce critère, les États-Unis ont un index 
énergétique de 2 500, tandis que l’Inde plafonne aux alentours de 70. Saha 
remarque que le seul pays à avoir développé une perspective systéma-
tique sur les questions d’énergie est la Russie soviétique.
Il est aussi intrigué par la relation entre les scientifiques et les leaders 
politiques, et en particulier par celle entretenue par Krzhizhanovsky 
et Lénine. Ce dernier encourage la création de l’Académie des sciences 
soviétiques et défend l’idée d’une production de masse. Krzhizhanovsky 
a l’idée d’Energetika, un index énergétique par pays, et c’est sous son 
influence que Lénine introduit le slogan « Le communisme, c’est les 
Soviets plus l’électricité ». Meghnad Saha part alors à la recherche de 
celui qui pourrait être son Lénine.
Il le trouve en la personne de Subhas Chandra Bose, à l’époque le 
tout nouveau président du Parlement indien. Saha le rencontre et lui 
1. Pour une analyse détaillée, voir Visvanathan 1997 (p. 94-145).
2. Visvanathan 1985.

	
sciences et savoirs dans l’état développementiste 	
113
demande ce qu’il compte faire dans le futur pour l’industrie. Bose reste 
vague mais n’est pas long à demander son avis à Saha. Celui-ci recom-
mande que soient mis en place un comité de planification nationale, 
dont Bose approuve la création. C’est également sous l’influence de  
Saha que le président élu du comité, Visvesvaraya, s’efface pour laisser 
le jeune Jawaharlal Nehru en prendre la tête. Le dialogue entre scienti-
fiques et politiciens commence.
En 1934, Saha fonde la revue Science and Culture et pendant vingt 
ans, jusqu’à sa mort en 1956, la revue et le réseau Science and Culture 
sont les fers de lance de la planification et de l’industrialisation de l’Inde. 
Dans son discours au congrès scientifique de 1947, Nehru déclare que 
l’avenir appartient à ceux qui savent devenir des amis de la science. Il 
ajoute que les barrages et les laboratoires sont les nouveaux temples de 
l’Inde, affirmation que l’on peut considérer comme une autre version du 
slogan de Lénine.
Ainsi, planification et industrialisation deviennent les maîtres mots du 
développement de l’Inde. Alors qu’il en est l’instigateur et le défenseur, 
Saha, par ironie du sort, est marginalisé. Seul scientifique élu en toute 
indépendance au Parlement, il engage une lutte contre le nouveau régime. 
Après son décès suite à un arrêt cardiaque en 1956, la revue Science et 
Culture sombre dans l’anonymat. Le grand réseau discursif créé autour 
de la science, de la technologie et de l’économie disparaît alors presque 
complètement, et un modèle de développement plus classique émerge 
avec l’indépendance.
Science et développement dans l’Inde indépendante
Le projet de développement indien suit les canons de l’économie conven-
tionnelle et considère la science comme au service du régime. L’Inde 
devient l’un des rares pays où le développement d’un « tempérament 
scientifique » est inscrit dans la Constitution, dans le cadre d’une classifi-
cation hiérarchique implicite. Tout ce qui est à grande échelle, moderne, 
scientifique est officiel, tandis que la petite échelle et le traditionnel, bien 
que tolérés, constituent l’économie parallèle. La science est au cœur des 
formes d’énergie officielles telles que l’électricité et le nucléaire, tandis que 
la biomasse relève du monde des sociétés de subsistance. La médecine 
officielle est allopathique, mais des concessions sont faites aux systèmes 
traditionnels tels que les médecines unani, ayurvédique et siddha 1. L’Inde 
1. Srinivasa Murti 1923.

114	
shiv visvanathan
est fière de sa politique scientifique considérée comme symboliquement 
aussi importante pour le pays que le passeport et le drapeau.
L’État développementiste bat son plein de 1947 à 1975, date à laquelle 
l’état d’urgence, première période de dictature que l’Inde connaisse, est 
décrété par la fille de Nehru, Indira Gandhi. Cet ordre nouveau, qui 
implique la suspension des droits démocratiques, est interprété par le 
régime comme une crise de l’État développementiste 1 – il semble en effet 
que des forces externes et des groupes internes soient rétifs au dévelop-
pement. L’épisode développementiste dictatorial donne libre cours non 
seulement à la violence mais aussi à une forme d’ironie involontaire. 
Indira Gandhi fait appel à de grands scientifiques pour s’assurer de leur 
soutien et le régime s’engage dans une lutte contre la pauvreté ciblant 
les plus démunis. Deux exemples doivent suffire pour montrer l’antago-
nisme dans lequel il se trouve alors vis-à-vis des pauvres et des marginaux. 
À Delhi, les projets d’urbanisme entraînent des opérations de démolition 
massive – qui forcent les pauvres à quitter le centre-ville et à se rendre 
moins visibles. Les efforts en matière de planning familial s’adressent 
en premier lieu à des minorités – mais sous la forme de programmes de 
stérilisation massive. Toutes ces actions sont évidemment faites au nom 
de la modernité, de la discipline, de la science, mais l’ironie de ces formes 
de développement échappe au régime.
La période qui suit la cuisante défaite électorale infligée au parti majori-
taire voit émerger des mouvements qui contestent l’hégémonie de la science 
et du développementisme d’État. Le développement comme projet n’a 
bien sûr jamais cessé d’être sous tension et il semble avoir besoin d’être 
constamment enrichi, nuancé, humanisé par des adjectifs nouveaux. 
Ainsi, le développement en Inde est d’abord un développement commu-
nautaire axé sur la reconstruction et la réadaptation des réfugiés après 
la partition. Il est suivi dans les années 1960 par le développement local 
qui met l’accent sur les technologies intermédiaires. Les programmes 
d’industries lourdes portés auparavant aux nues sont maintenant consi-
dérés comme inadéquats. Fritz Schumacher, théoricien de la technologie 
intermédiaire et ancien associé de Keynes, devient conseiller auprès du 
bureau de la Planification. Son livre Small Is Beautiful 2 est utilisé des côtés 
tant officiel que non officiel pour défendre l’idée que le développement 
doit se faire localement et progressivement. Le développement local est 
transformé de façon éloquente par Brundtland et son idée de dévelop-
pement durable. Il est toutefois deux façons de concevoir la durabilité, 
1. Visvanathan et Sethi 1998 (p. 45-87).
2. Schumacher 1979 [1973].

	
sciences et savoirs dans l’état développementiste 	
115
dans les pays du Nord et dans ceux du Sud, ce qui donne lieu à des discus-
sions particulièrement vives. La durabilité est en effet reconnue, mais 
sans avoir intégré la notion de « société du risque » qui fait des incerti-
tudes de la science un élément clé des connaissances. Le développement 
humain comme perspective critique émerge en parallèle au dévelop-
pement durable et inclut les idées de droits, de démocratie et de genre. 
Chaque qualificatif associé au concept de développement crée donc un 
épicycle nouveau qui modifie le thème principal – comme si l’on était 
dans un nouveau monde ptoléméen où des épicycles sont constamment 
inventés pour sauver le phénomène du développement.
Il faut ici admettre que la critique porte d’abord sur l’hégémonie 
occidentale, son organisation en centres et périphéries, voire ses sugges-
tions de démocratisation à travers des dispositifs participatifs dans lesquels 
on demande que les préoccupations écologiques soient intégrées. En  
ce sens, le discours public habituel relève de l’économie politique, ou  
des formes que doit prendre « la » démocratie. Les débats soulevés par les 
mouvements militants et les manifestes alternatifs déplacent largement 
la chose, notamment du fait qu’ils s’intéressent au cognitif, voire à l’épis-
témique. La science est mise en question par eux comme système de 
connaissance et comme système de vie. Au-delà de la critique de l’imagi-
naire du développement, les alternatives (que je vais maintenant considérer) 
offent donc, en écho à Castoriadis, des possibilités et futurs beaucoup 
plus ouverts et décisifs.
Critiques et alternatives militantes,  
pour un contrat social postdéveloppementiste
Parmi les mouvements qui se confrontent en Inde à la question des 
savoirs alternatifs se trouvent le mouvement Chipko, qui milite depuis 
1973 contre la déforestation, le mouvement d’opposition aux barrages sur 
la Narmada, les écologistes luttant pour la biodiversité, les mouvements 
antinucléaires, ceux qui bataillent contre les démolitions massives, ainsi 
qu’une série de critiques émanant de groupes écologistes, féministes et 
de défenseurs des droits qui cherchent à développer des rapports neufs 
entre connaissance et politique. On y trouve aussi des universitaires et 
activistes tels que Ashis Nandy, Claude Alvares, Ziauddin Sardar, Shiv 
Visvanathan ou encore Vandana Shiva.
La nature des critiques varie d’un mouvement à l’autre. On peut les 
organiser de manière conceptuelle en partant d’un axe d’innovation 
représenté de manière schématique par la succession de trois étapes : 

116	
shiv visvanathan
l’invention, l’innovation et la diffusion. L’invention renvoie à l’idée scien-
tifique, l’innovation à la commercialisation initiale de cette idée et sa 
reproduction à grande échelle à travers des techniques, et la diffusion à 
la circulation de cette idée à travers la société.
Le mouvement, si on le considère dans son ensemble, aborde les 
sciences et technologies de différentes manières : le Kerala Sastra Sahitya 
Parishad (KSSP) et l’expérience Hoshangabad s’attachent à diffuser l’esprit 
scientifique en organisant des jeux-concours, des pièces de théâtre telles  
que La Vie de Galilée de Bertolt Brecht ou des expériences ludiques  
pour transmettre le plaisir, l’esprit et la méthode scientifiques. De tels 
mouvements ne mettent pas la science en question et visent à la faire 
pénétrer plus avant dans les villages et la population. Les groupes gandhiens 
ou socialistes cherchent, pour leur part, à créer des technologies à  
partir de matériaux locaux, et essaient de trouver la bonne échelle.  
Parmi eux, beaucoup sont partisans de technologies intermédiaires. Le 
groupe le plus important, ASTRA (Application de la science et de la 
technologie aux espaces ruraux), est fondé en 1974 à l’Académie indienne 
des sciences à Bangalore. Une troisième mouvance s’attache aux aspects 
ésotériques, culturels et nationaux des sciences qu’elle examine sous 
un angle épistémologique 1. Parmi eux, on retiendra le groupe pour la 
Science et la technologie de la patrie et du peuple (PPST) fondé en 1978 
à Madras, et un laboratoire au sein du Murugappa Chettiar Research 
Center, centre de recherche indépendant fondé en 1977 à Chennai,  
Tamil Nadu.
Les travaux de ce dernier et les écrits du réseau « Alternative Science 
Network » cherchent à établir un cadre théorique permettant d’arti-
culer épistémologie et politique. Ils vont au-delà des prescriptions du 
modèle de transfert de technologie qui conçoit le Nord comme le centre 
et la source des inventions et le Sud comme un simple consommateur 
d’idées. Pour ces groupes, la violence de la science est à chercher dans 
l’acte même de sa construction et dans le processus de cognition. Ils 
questionnent l’approche conventionnelle de l’épistémologie comme 
question de validation et de vérification des connaissances, qui est, pour 
eux, une manière scolaire de pratiquer la philosophie des sciences. Ils 
estiment au contraire que la validation des connaissances n’opère qu’à 
travers les moyens et les conditions d’existence, les cycles de la vie et les 
styles de vie. Pour eux, un système de savoir ne peut être vérifié que sur 
la durée et en contexte. Vue sous cet angle, la paix n’est pas seulement la 
fin des violences ou l’arrêt des hostilités. C’est une attitude non violente 
1. Voir Alvares 1980, Nandy 1980, Uberoi 1984, Visvanathan 1997.

	
sciences et savoirs dans l’état développementiste 	
117
ancrée dans le travail, dans les sens et dans le corps, dans la relation que 
l’on entretient à la nature et à la propriété.
Ces groupes construisent une sagesse et un ensemble d’idées construites 
dans les luttes et les débats que des intellectuels contestataires ont avec les 
élites politiques et scientifiques. À travers ces actions se joue la tentative de 
mieux prendre en compte les groupes informels et marginaux qui luttent 
pour leur survie. Il s’agit aussi, sous un autre angle, de faire dialoguer deux 
civilisations en suggérant que des cosmologies alternatives sont suscep-
tibles de donner une vie nouvelle aux sciences. Ces groupes cherchent 
donc à porter plus loin les controverses antérieures autour des sciences et 
du développement national en refusant que leur futur soit hypothéqué et 
déterminé uniquement par les paradigmes de développement en vigueur. 
Bien que menant leur combat chacun à leur manière, ils partagent la 
même expérience de résistance et la même base d’idées sur la façon de 
réinventer la démocratie en lien au débat sur la connaissance.
Ce foyer commun peut être présenté en deux volets, comme un ensemble 
d’hypothèses et d’idées tacites, d’une part, comme une série de concepts 
articulés qui composent un cadre de compréhension et un terrain d’entente, 
de l’autre.
1.	Que ce soit de façon tacite ou explicite, tous ces groupes font la diffé-
rence entre les notions de libération et d’émancipation. Les mouvements 
nationalistes libèrent leurs pays de l’Occident et créent des États-nations 
en s’engageant dans des politiques de développement, de transfert de 
technologie et de substitution à l’importation. Mais les groupes marginaux 
réalisent que l’indépendance n’est pas suffisante et qu’il faut, pour changer 
les choses, exorciser tout un ensemble de catégories et de modes de vie 
hégémoniques. L’émancipation revient ainsi aux racines de la violence 
sociale développées dans les systèmes de pensée.
2.	Ces communautés de pensée ne sont pas antioccidentales ni ennemies 
des sciences. Elles s’érigent contre les systèmes hégémoniques et s’affirment 
en faveur du pluralisme et du syncrétisme. Elles cherchent à créer des 
récits, mythes et systèmes de pensée alternatifs. L’année 1492, par exemple, 
est réinterprétée moins comme la date de l’avènement de la modernité 
que comme celle du déclin de l’Occident pluraliste dans lequel Arabes, 
juifs et chrétiens pouvaient débattre de leurs visions du monde. Ainsi 
l’autre Occident, l’Occident vaincu et dormant, en vient-il à s’opposer 
à toute recherche d’alternatives – même si Goethe et Swedenborg sont 
aussi critiques que Tagore ou le mouvement Bhakti. Comme nous l’avons 
rapporté, l’un des objectifs du mouvement nationaliste indien est, pour 
Gandhi, de libérer l’Occident – ce qui ne peut se faire que via l’émanci-
pation et le renouveau des connaissances.

118	
shiv visvanathan
3.	Pour ces groupes, la portée de l’épistémologie est étendue au-delà 
de la logique d’expertise, jusque dans la vie ordinaire de la cité. Chaque 
citoyen est considéré comme détenteur d’un savoir, d’un métier, d’un art 
de vivre. Ce qu’a dit Coomaraswamy de l’artiste s’applique au citoyen. 
Selon lui, un artiste n’est pas un type particulier d’être humain – ce qui 
fait d’un être humain un artiste est la poursuite de sa vocation 1. De la 
même manière, chaque citoyen est porteur de savoir dans la quête même 
de ses modes de subsistance. Malheureusement, l’idée que l’État dévelop-
pementiste se fait du citoyen laisse peu de place aux compétences qui 
sont requises pour survivre dans les économies parallèles ou marginales. 
Une grande quantité de savoir-faire et d’aptitudes sont laissés en dehors 
de la connaissance citoyenne. Le besoin se fait donc sentir d’une classi-
fication des connaissances qui puisse reconnaître ces formes comme 
aussi valides et importantes, au sein d’un monde pluriel, que les savoirs 
scientifiques.
4.	Ces mouvements affirment aussi que, si la science moderne est 
formelle et constitutionnelle, l’économie informelle et marginale a besoin 
d’une constitution tacite. Cette idée découle du concept de « connaissance 
tacite » de Michael Polanyi. Le philosophe et chimiste hongrois estime 
que le discours formel de la méthode scientifique n’épuise pas le tout des 
sciences. Celles-ci ont aussi à voir avec des dimensions tacites telles que 
les silences, l’expression corporelle, les présupposés, les sous-entendus, les 
gestes. Il est donc besoin de l’idée d’une constitution tacite pour recouvrir 
les différences de langues, de connaissance, de catégories qui ne sont pas 
comprises dans le savoir scientifique formel. Une démocratie de la connais-
sance nécessite d’aller par-delà la science officielle et d’explorer le champ 
des formes tacites – par exemple les savoirs populaires, traditionnels et 
ceux du « système D » inscrits en dehors de la rationalité scientifique.
5.	L’idée de constitution tacite comme base d’un cadre pluraliste de la 
connaissance débouche sur une articulation intéressante entre connais-
sance et justice autour de la notion de justice cognitive. Cette idée a été 
proposée par l’auteur suite à des discussions menées au sein des mouve-
ments militants 2.
La visite de chefs de tribu et d’activistes en provenance du Gujarat a 
tenu lieu d’événement déclencheur. Le Gujarat est l’un des États abritant 
le plus grand nombre de tribus décriminalisées de l’Inde. L’idée même de 
tribu criminalisée est un héritage de la période coloniale durant laquelle 
l’empire raciste et eugéniste classe certains groupes dans la catégorie 
1. Coomaraswamy 1956.
2. Visvanathan 2009.

	
sciences et savoirs dans l’état développementiste 	
119
des criminels congénitaux. À la suite de l’indépendance, ces tribus sont 
officiellement retirées de cette catégorie mais leur statut n’en est pas 
vraiment changé pour autant. Leurs membres sont la cible continuelle  
de la police et souvent battus ou assassinés au cours des altercations. 
Pour eux, la survie est devenue un art. Les activistes qui œuvrent avec 
ces tribus entament un jour une discussion avec l’auteur. Son travail sur  
la science et la démocratie les intéresse et ils lui demandent d’organiser  
un séminaire ou panchayat, une rencontre pour la communauté autour des 
systèmes de connaissance. Ils veulent établir un dialogue entre la médecine 
occidentale et leurs guérisseurs traditionnels, entre la médecine ethnique 
et la psychiatrie moderne, entre leurs ojhas et les travailleurs sociaux. Ils 
savent que leurs membres sont traumatisés par la violence policière, que 
l’anémie drépanocytaire est endémique et que de nombreuses personnes 
meurent avant trente-cinq ans. Ils signalent qu’un groupe de chercheurs 
de l’université Harvard est venu leur rendre visite mais qu’il n’y a pas  
eu de suites. Les activistes veulent un dialogue mais sans l’idée habituelle 
de participation promue par la Banque mondiale, ni la hiérarchie entre 
connaissance experte et connaissance commune. Ils veulent un dialogue 
entre leurs savoirs et le savoir occidental ou gouvernemental, avec l’idée 
d’une possible interaction entre les deux.
L’idée de justice cognitive est forgée en réponse à ce besoin, et évoque 
un terrain épistémique où tous les types de connaissance sont diffé-
rents mais égaux entre eux. Elle renvoie au droit qu’ont toutes les formes  
de savoir de survivre et d’innover. Le concept est construit comme un 
défi à l’hégémonie scientifique mais invite celle-ci à rejoindre le jeu. Prise 
seule, la science ne produit pas les formes de bien-être que l’on est en 
droit d’attendre, et les experts du logement, de la santé, de l’agriculture 
et de l’eau ont besoin de formes de savoir complémentaires.
6.	En tant qu’outil de dialogue, la justice cognitive a besoin d’un 
écosystème plus large. D’abord, la démocratie comme communauté 
politique doit reconnaître la pluralité des temporalités. Comme l’a 
remarqué un anthropologue travaillant à Bastar, la durabilité du rapport 
Brundtland est conçue dans une temporalité linéaire et historique, tandis 
que le temps des cultures à Bastar et Orissa s’accomplit dans un entre-
mêlement temporel : le temps du festival, le temps cyclique, le temps du 
jeûne et du festin, le temps des menstruations de la terre où le labour 
est tabou, le temps du mythe et des saisons. L’idée du développement 
est construite elle aussi sur une échelle linéaire, selon une séquence qui 
voit les tribus devoir évoluer en sociétés agricoles puis se transformer 
en sociétés industrielles et postindustrielles. L’État tribal est considéré 
comme faisant partie d’un passé révolu, ce qui donne le droit aux pays 

120	
shiv visvanathan
appartenant au futur industriel de hisser les tribus dans la modernité. 
Cependant, comme le dit Ashis Nandy, le tribal n’est pas mon ancêtre – il 
est mon contemporain. L’hégémonie du temps linéaire est ainsi une 
forme de violence transformant des populations en objets de musée et 
les condamnant au triage, au génocide, à la violence. Une Constitution 
qui reconnaîtrait la pluralité des temporalités admettrait la possibilité 
qu’il y ait des opportunités et des trajectoires de vie différentes.
7.	Du point de vue des groupes activistes, la Constitution telle qu’elle 
existe reflète un esprit de chapelle limité dans le temps, l’espace et les 
imaginaires. Le temps comme progrès, mouvement de muséification et 
biologie sociale engendre la violence de l’obsolescence et du déchet. La 
diversité des temporalités nécessite également une diversité de natures, 
et cette diversité a besoin d’être représentée dans les cosmologies et les 
constitutions. Le contrat social auquel la science prend part permet à 
celle-ci de représenter, d’objectiviser et de mener ses expériences sur la 
nature. Mais il faut trouver une place pour accueillir les représentations 
du corps et de la nature dans leur variété. C’est là que les sciences alter-
natives prennent toute leur importance.
Ziauddin Sardar a proposé une version succincte et pluraliste de la 
justice cognitive 1. En tant que citoyen britannique, il déclare avoir accès 
au système de santé national. Il ajoute qu’en tant que musulman prati-
quant il a le droit d’avoir ses propres idées sur le corps, la médecine, les 
soins et la mort. Ces droits sont complémentaires au même titre que 
l’égalité peut se voir enrichie par la pluralité.
La nature doit elle aussi être représentée de façon multiple. Il ne s’agit 
pas seulement ici de droits et de l’idée de donner aux arbres un statut 
légal. Le scientifique Han Jeny en donne un bon exemple 2. Il raconte que, 
apportant en classe à Berkeley des photos de différents types de sols et 
demandant à ses élèves ce qu’elles représentent, les étudiants répondent 
qu’il s’agit certainement de tableaux impressionnistes ou d’œuvres de 
Matisse. Ils sont alors tout surpris de découvrir qu’il s’agit de sols vus 
au microscope. Il faut cependant dépasser l’idée de droits pour recon-
naître le potentiel épistémique de l’agriculture. Les variétés de sol rendent 
possible la diversité, mais c’est la diversité qui a besoin d’une reconnais-
sance épistémique. Les travaux d’Alfred Wallace et de Nikolaï Vavilov 
sont ici importants en raison du lien entre diversité et formes de connais-
sance. Un exemple souvent cité est que l’Inde a plus de 50 000 variétés 
de riz. Nous sommes les dépositaires d’un plasma germinatif reçu en 
1. Sardar 2003.
2. Stuart 1984.

	
sciences et savoirs dans l’état développementiste 	
121
héritage – or il est clair que la science n’est pas capable d’assumer seule 
cette diversité. Pour être entretenue, celle-ci requiert différentes formes 
d’agriculture, de religion, de mythe, de communauté – de savoirs. Dans 
son sens évolutionniste et épistémique, la diversité devient un point 
d’ancrage pour la démocratie de la connaissance. Comme Shiva l’a souvent 
affirmé, la monoculture agricole reflète une monoculture des esprits 1.
Le dialogue entre savoirs traditionnels et savoirs modernes est particu-
lièrement crucial dans les débats concernant la médecine et l’agriculture. 
Il doit toutefois être complété par les directions ouvertes par les sciences 
du risque pour lesquelles les certitudes de l’univers newtonien n’ont 
plus cours. On distingue deux formes de risque, le risque ontologique 
et le risque épistémique. Le premier provient d’incertitudes résultant du 
manque de connaissances dans un domaine particulier. Si une ignorance 
de la sorte peut être comblée, il faut admettre la possibilité que ce ne soit 
jamais le cas pour l’incertitude ontologique.
Ces approches du risque se conjuguent maintenant aux approches 
de la diversité pour contester le statut prépondérant de la certitude 
en science, et les recherches doivent dorénavant s’orienter vers une 
science non prométhéenne, plus prudente. Celle-ci serait ancrée dans 
une idée de la paix articulée en termes épistémiques, soit en une forme 
de connaissance non violente. Cela nécessite en retour l’établissement 
de plusieurs propositions. Premièrement, l’innovation scientifique doit 
être considérée comme un contrat social reliant l’éthique de la mémoire 
et celle de l’innovation. Il manque à une connaissance pour l’instant 
ancrée dans l’obsolescence une éthique de la mémoire. Deuxièmement, 
le corps doit reprendre une place centrale, non pas comme objet mais 
comme sensorium. Il y a là une tentative de mettre la perspective linéaire 
basée sur le visuel au défi d’une république des sens où la vue, l’ouïe, le 
toucher et l’odorat composeraient ensemble un espace commun sensuel 
et compréhensible. Troisièmement, la langue prend une grande impor-
tance. Un concept traduit en plusieurs langues incite à une poétique de 
la vérité, ce qui est un bien.
On peut enfin relier démocratie et cadre pluraliste de connaissances. 
Une société de la connaissance ne donne pas la primauté à la science 
mais à la diversité et à son potentiel. Le savoir y est considéré comme une 
forme de citoyenneté et la science comme l’une des formes possibles de 
cette citoyenneté. Les mouvements et débats que nous avons présentés 
conduisent à l’idée que leurs luttes et leurs critiques ne sont pas seulement 
des formes de résistance mais qu’elles conduisent à une multiplicité des 
1. Shiva 1993.

122	
shiv visvanathan
formes de vie. Dans un tel contexte, la connaissance crée de la vie. Il y 
a une prise de conscience que la connaissance hégémonique conduit 
à la monoculture et à la muséification. Mais la démocratie a besoin, 
au-delà de la science, d’un système de rationalités plurielles. Et la survie 
des économies parallèles et de la société de la biomasse nécessite des 
types de savoirs autres. Des connaissances diverses créent des imagi-
naires qui dépassent les paradigmes de la science. Dans un tel monde, 
la démocratie est une forme de connaissance et la citoyenneté un acte 
de science qui donne toute sa place à l’inventivité. L’État développemen-
tiste avec ses notions simples d’expertise, de progrès et de science devient 
aujourd’hui une possibilité parmi d’autres – mais le monde est de plus en  
plus ouvert.
Traduit par Clara Breteau
Références bibliographiques
Alvares Claude, 1980, Homo Faber : Technology and Culture in India, China and the 
West from 1500 to the Present Day, La Haye, Nijhoff.
Bell Daniel, 1973, The Coming of Post-Industrial Society : A  Venture in Social 
Forecasting, New York, Basic Books.
Coomaraswamy Ananda Kentish, 1915, « Love and Art », Modern Review, vol. 14, 
no 11, mai, p. 574-584.
–	 1947, The Bug Bear of Literacy, Londres, Dennis Dopson.
–	 1956, Christian and Oriental Philosophy of Art, New York, Dover Publications.
–	 1981, Essays in National Idealism, New Delhi, Munshiram Manoharlal.
Gandhi Mohandas K., 2014 [1909], Hind Swaraj. L’émancipation à l’indienne, Paris, 
Fayard.
Geddes Patrick, 1904, On Universities in Europe and India : Five Letters to an Indian 
Friend, Madras, National Press.
–	 1993 [1915], L’Évolution des villes, Paris, Temenos.
Lane Harlan, 1979, The Wild Boy of Aveyron, Londres, Palladin Granada.
Lipsey Roger, 1977, Coomaraswamy : His Life and Work, Bollingen Foundation 
Collection, Princeton (NJ), Princeton University Press.
Nandy Ashis, 1980, Alternative Sciences : Creativity and Authenticity in Two Indian 
Scientists, New Delhi, Allied.
Sachs Wolfgang (dir.), 1992, The Development Dictionary : A Guide to Knowledge as 
Power, Londres, Zed Books.
Sahasrabudhey Sunil, 1985, « Hind Swaraj and the Science Question », in Nageshwar 
Prasad (dir.), Hind Swaraj : A Fresh Look, Delhi, Gandhi Peace Foundation.
Sardar Ziauddin, 2003, « Healing the Multiple Wounds : Medicine in Multicultural 
Society », in Sohail Inayatullah et Gail Boxwell (dir.), Islam, Postmodernism 
and Other Futures, Londres, Pluto Press, p. 299-311.
Sarkar Sumit, 1973, The Swadesi Movement in Bengal (1903-1905), New Delhi, 
People’s Publishing House.
Schumacher Fritz 1979 [1973], Small Is Beautiful. Une société à la mesure de 
l’homme, Paris, Seuil.

	
sciences et savoirs dans l’état développementiste 	
123
Shiva Vandana, 1993, Monocultures of the Mind : Perspectives on Biodiversity and 
Biotechnology, Londres, Zed Books.
Srinivasa Murti G., 1923, « Secretaries Minute », Report of Committee of Indigenous 
Medicine, Madras, Government Printing Press.
Stuart Kevin, 1984, « My Friend, the Soil : A Conversation with Hans Jenny », Journal 
of Soil and Water Conservation, vol. 39, no 3, p. 158-161.
Tagore Rabindranath, 1913, « The Relation of the Universe and the Individual », 
Modern Review, vol. 14, no 1, juillet, p. 1.
Tyrwhitt Jacqueline (dir.), 1947, Patrick Geddes in India, Londres, Lund Humphries.
Uberoi J.P. Singh, 1984, The Other Mind of Europe : Goethe as a Scientist, New York 
et Oxford, Oxford University Press.
Visvanathan Shiv, 1985, Organizing for Science : The Making of Industrial Research 
Laboratory, Delhi, Oxford University Press.
–	 1997, A Carnival for Science, Delhi, Oxford University Press.
–	 2009, « The Search for Cognitive Justice », Seminar, no 597, p. 45-49.
Visvanathan Shiv et Sethi Harsh (dir.), 1998, Foul Play : Chronicles of Corruption 
(1947-1997), New Delhi, Banyan Books.
Wallace Alfred Russel, 1908, The Wonderful Century, Londres, George Allen & 
Unwin.


6 Les savoirs du social
D o m i n i q u e  P e s t r e
Les science et savoirs ne sont pas le fait de dieux. Ils sont produits 
par des humains aux capacités limitées et ils ne peuvent pas ne pas  
être simplifiés, partiels et partiaux. Les sciences modernes sont toutefois 
plus que des activités de savoir. Elles sont aussi des activités à vocation 
pratique. À travers les mathématiques, l’expérimentation contrôlée et les 
modélisations, elles autorisent une meilleure maîtrise sur les phénomènes, 
la nature et le social. Pour cette raison, la science moderne a toujours été 
liée aux pouvoirs de tous types. Elle a toujours été suivie avec attention 
par les gouvernants, et les savants ont offert leurs services aux princes, 
aux États démocratiques et autoritaires, aux entreprises commerciales 
et aux militaires. En bref, aucun savoir n’est jamais pur et sans cadrage 
initial. David Bloor n’avait donc pas tort, contre la seconde symétrie 
latourienne, de postuler une possible relation (à expliciter) entre espace 
de production et type de savoir produit 1.
Les savoirs, dans l’histoire, n’ont jamais relevé des seules académies, 
universités et autres milieux professionnels. Il a toujours existé des savoirs 
distribués dans le social, des savoirs populaires ou amateurs, artisans 
ou productifs, contestataires ou alternatifs. Pensons aux savoirs natura-
listes ou astronomiques au xixe siècle, très souvent le fait d’amateurs,  
ou à ceux qui émanent des populations affectées par les dégâts du progrès. 
Pensons aux évaluations différentes des bienfaits des grands aménage-
ments (barrages par exemple), aux savoirs pratiques et ouvriers sans 
lesquels la vie industrieuse n’aurait pu s’épanouir ou, plus récemment, 
au logiciel libre 2.
Au xxe siècle, les savoirs scientifiques et techniques, et notamment 
1. Voir le débat entre Bloor et Latour dans Studies in History and Philosophy of Science, vol. 30, 
no 1, mars 1999.
2. Pritchard 2011, Fressoz 2012, Sibum (dans le t. 2 de cet ouvrage, p. 285) et Schaffer (ibid., 
p. 115), Visvanathan (dans ce volume, p. 107).
 Les amateurs et passionnés, leurs pratiques, réseaux et imaginaires. Annonce d’une conférence de 
« Do-It-Yourself Biology » tenue à Paris en décembre 2014.

126	
dominique pestre
les activités de laboratoire, sont et restent au cœur de la vie économique 
des nations. Cela ne s’est en rien réduit dans les dernières décennies et 
les États investissent toujours plus pour contrôler la frontière technos-
cientifique et la rendre plus productive et efficace. Les savoirs sur les 
populations ou la santé publique se sont eux aussi fortement accrus, 
et les économistes ont cherché à maîtriser la machine économique ou 
le bon fonctionnement des marchés – via la promotion des calculs en 
coût-bénéfice par exemple ou, plus récemment, le déploiement d’algo-
rithmes financiers 1.
La thèse que je développerai dans ce chapitre est que, en dépit de conti-
nuités réelles, un basculement s’est imposé dans les années 1960-1970. 
Après l’apogée de la première guerre froide et la contestation des années 
1960, on passe de l’Université en gloire, car alter ego de l’État agissant pour 
le bien de tous, à une science désanctuarisée. On constate un déploiement 
et une visibilité neufs des savoirs produits par ce qu’on a pris l’habitude 
d’appeler la « société civile », on assiste à une redistribution des places et 
pouvoirs, à une redéfinition des institutions qui comptent, à un dépla-
cement des espaces produisant du savoir. La place de l’État s’est réduite, 
la pertinence des savoirs qu’il avait promus se perd et on assiste à une 
autonomie accrue des acteurs économiques et à l’émergence de savoirs 
et outils nouveaux 2.
En d’autres termes, je soutiendrai la thèse d’un basculement du régime 
de création et de déploiement des savoirs en société, basculement marqué 
par la place nouvelle prise par l’espace public comme lieu de légitimation, 
par la multiplication de groupes prétendant au savoir, par une montée 
d’acteurs nouveaux imposant leurs normes du bien comme du « bon » 
savoir – et construisant de nouvelles alliances, de nouveaux blocs hégémo-
niques aurait dit Gramsci.
J’adopterai dans ce chapitre une stratégie qui a fait ses preuves chez 
les historiens depuis vingt ans – l’exemple le plus abouti étant celui des 
historiens critiques de la « révolution scientifique 3 ». À savoir, identifier 
les espaces sociaux où des connaissances sont produites et partir de leur 
renouvellement pour caractériser la transformation des savoirs eux-mêmes. 
Dans un premier temps, je m’intéresserai au demi-siècle qui court des 
années 1910 aux années 1960. Ce moment est celui où les savoirs relèvent 
massivement de l’institution académique travaillant de conserve, dans le 
cadre national, avec les autres grandes institutions que sont les États et 
1. Sur l’économie, voir Shenk et Mitchell (dans ce volume, p. 233) ; sur le rôle des États, Edgerton 
(dans ce volume, p. 67).
2. Pour un exemple, Desrosières 2009.
3. Biagioli 1993, Wintroub 2000.

	
les savoirs du social	
127
les puissances économiques. Un trait de ces savoirs est qu’ils visent une 
gestion intégrée des technologies, de l’économie, de l’État et du pacte 
social. Je considérerai ensuite les quarante dernières années en m’attardant 
sur quatre types d’espaces : le management et ce qu’il transforme dans 
la production des savoirs et la gestion de l’ordre social ; les think tanks 
libéraux-conservateurs et les savoirs et technologies qu’ils déploient pour 
remplacer l’« économie » par la science des marchés ; le monde des ONG 
et de la « société civile » organisée ; et finalement la place prise par les 
populations « ordinaires » dans la production de savoirs de tous ordres.
De la Première Guerre mondiale aux années 1960 :  
la science comme autorité
De la Première Guerre mondiale aux années 1960, l’État gagne extraor-
dinairement en puissance. Il passe au cœur des activités de savoir, il en 
devient le nœud stratégique. Pour le développement économique et la 
préparation de la guerre, l’État devient entrepreneur de science, promoteur 
du développement technique et industriel, centre de la régulation écono-
mique et du bien-être social. Le « contrat social » est celui d’un progrès 
construit sous l’ombrelle d’un État « arbitre » et savant en mesure, grâce 
aux diverses sciences, de développer les meilleures politiques.
De la fin du xixe siècle aux années 1920, les sciences et techniques 
deviennent un bien national. L’État finance les universités et instituts 
techniques, il les aide à se transformer en lieux de recherche et construit, 
en Europe, de nouvelles institutions, les caisses et centres de recherche : 
Kaiser-Wilhelm Gesellschaft en Allemagne, CNRS en France, DSIR en 
Grande-Bretagne, CNR en Italie 1.
L’État soutient aussi l’industrie et la coordination des acteurs écono-
miques. Il le fait via la création de centres nationaux de normalisation, 
des unités de recherche essentielles en termes industriels puisqu’elles 
organisent la compatibilité des produits et systèmes. Le plus emblématique 
de tous est la Physikalisch-Technische Reichsanstalt à Berlin, parrainée en 
1887 par Helmholtz, le patron de la science allemande, Siemens, l’entre-
preneur le plus puissant, et Bismarck, l’homme politique au cœur de 
l’unité du pays. L’État soutient encore les initiatives privées, par exemple 
celles des industriels développant des centres de recherche collectifs par 
branches. Quant aux industriels, ils ont souvent eux-mêmes intégré cette 
dimension que la science « fondamentale » est un bien premier. Philips, 
1. Pestre 2003.

128	
dominique pestre
Siemens ou Bayer en Europe, comme AT & T aux États-Unis avec les 
fameux laboratoires Bell, en sont les instanciations les plus exemplaires 1.
Le rôle de l’État comme acteur central du progrès et de l’unité nationale 
implique aussi de gérer les effets négatifs du progrès. L’État social implique 
la protection du travail, notamment les « risques professionnels » pris en 
charge par l’assurance – en France en 1898. Mais il gère aussi les consé-
quences de la mise en circulation de produits à effets sanitaires négatifs 
(les produits chimiques par exemple). Dans les premières décennies du 
xxe siècle, des comités formés d’experts (médecins, toxicologues et épidé-
miologues), d’organisations professionnelles et de représentants des États 
définissent des normes de qualité – à défaut d’interdire totalement les 
produits lorsqu’ils sont jugés indispensables à la compétitivité écono-
mique du pays. Après 1945, ces comités deviennent internationaux, la 
question des radiations ionisantes donnant l’impetus initial 2.
Les économistes, dès la fin de la Première Guerre mondiale, visent pour 
leur part le contrôle de l’économie nationale, c’est-à-dire l’ensemble des 
flux de production et de redistribution conçu comme une gigantesque 
machine que peuvent optimiser économistes et grands commis de l’État. 
Cette idée de l’économie comme un système qui peut être piloté à l’échelle 
d’un territoire n’est pas qu’une métaphore 3. Elle se traduit en théories 
et algorithmes, en savoirs pratiques et dispositifs cognitifs et sociaux. 
Le plus connu de ces dispositifs est le Plan, en France ou aux Pays-Bas. 
Il repose sur des objectifs construits par les « partenaires sociaux », sur 
des données statistiques, des modèles et l’économétrie. Il est certes des 
intérêts derrière ces pratiques (et d’autres qui sont oubliés, comme ceux 
de l’environnement) mais ces savoirs se donnent comme généraux puisque 
« coélaborés » en faveur du bien collectif. Ils tendent donc à apparaître 
comme neutres et universels – et sont dotés, malgré les contestations 
locales, d’une grande autorité politique 4.
Dans ce processus, la « science » apparaît comme l’autorité en matière 
de savoirs, et on lui reconnaît un magistère supérieur sur la nature du 
monde sublunaire. Mais l’institution science devient aussi une autorité 
politique, la légitimité qui lui est accordée dérivant du fait qu’elle fait 
journellement la preuve de sa force via le progrès. La science est légitime 
par les outputs techniques qu’elle aide à déployer, par ce qu’elle permet 
de mettre en œuvre dans l’ordre matériel, social et économique. La 
1. Cahan 2004, Vries 2005.
2. Pestre 2013, Fressoz 2012, Jas 2007 et 2014.
3. Un modèle hydraulique de l’économie datant du début des années 1950 est visible au Science 
Museum de Londres.
4. Desrosières 1999, Armatte et Dahan (dans ce volume, p. 339).

	
les savoirs du social	
129
science est alors l’alter ego de l’État interventionniste. Ils partagent tous 
deux une revendication de transcendance – ou du moins de neutralité 
vis-à-vis des intérêts particuliers. Ils se pensent comme au-dessus des 
contingences, énonçant le Vrai ou le Bien de la nature ou de la nation. En 
plus de sa légitimité démocratique, dérivant du suffrage universel, l’État 
reçoit de la science une garantie de rationalité pour ses actions. Comme 
le dit Guillaume Carnino, les savoirs universitaires adviennent alors en 
singulier majuscule – ils deviennent « La Science 1 ».
L’apogée de ce système est atteint durant la guerre froide – moment où 
les forces armées et les think tanks militaires américains jouent un rôle 
décisif. Ce qui est alors frappant est la mathématisation des démarches 
(un renforcement de leur scientificité) et la promotion d’une rationalité qui 
se donne comme englobante et systémique, comme totale et optimale – au 
fond, comme quasi divine. La Marine ou l’US Air Force financent massi-
vement les sciences de laboratoire et en viennent à remodeler les champs 
disciplinaires. Les programmes balistiques et nucléaires conduisent 
à la gestion par projet et à une réforme des modes industriels d’inno-
vation et de management 2. La RAND Corporation développe l’analyse 
des systèmes – pour penser le déploiement des forces stratégiques et 
l’échange nucléaire d’abord, la réorganisation de l’appareil d’État et des 
systèmes sociaux ensuite. Se développent aussi l’analyse coût-bénéfice 
et la théorie de la décision en univers incertain, des outils d’analyse qui 
se veulent rationnels et imparables, en tout cas non contestables par les 
néophytes. Appliquées à la gestion des risques, ces approches séparent 
analyse scientifique et décision politique, renforçant ainsi le caractère 
intangible des faits établis par les experts 3.
Durant la guerre froide, la montée en majesté de « la science » se marque 
aussi par l’invention des « politiques scientifiques » et du « pilotage de la 
recherche ». Ceux-ci impliquent essentiellement le triptyque scientifiques / 
industriels / experts et il place (symboliquement ?) la science aux origines 
des innovations et solutions. L’OCDE est le lieu privilégié de globalisation 
des instruments nécessaires à la mise en œuvre de ces nouveautés – ce 
qu’incarne le Manuel de Frascati (1963). Bien évidemment, le nucléaire, 
l’aéronautique, l’espace, l’électronique et les télécommunications restent 
les secteurs les plus financés – guerre froide oblige – mais ils ne sont pas 
soumis à la règle commune et relèvent plus souvent des pouvoirs discré-
tionnaires des États et des militaires travaillant avec les industriels 4.
1. Carnino 2015.
2. Lécuyer (dans ce volume, p. 423).
3. Dahan et Pestre 2004, Boudia et Jas (dans ce volume, p. 381), Boudia 2014.
4. Godin 2009, Edgerton (dans ce volume, p. 67).

130	
dominique pestre
Sous l’impulsion de l’État, la fabrication des savoirs est donc assez 
pyramidale durant cette période, et la nation constitue l’univers de 
référence. Est-ce à dire qu’il n’est pas de contestations, que ce réseau 
organisé fonctionne sans oppositions ? Non, bien sûr, mais celles-ci ne sont 
pas assez puissantes pour ébranler les modes d’évaluation en place. Des 
savoirs continuent d’être produits dans les associations et mouvements 
de contestation, mais ceux-ci, sans relais, pèsent peu sur les politiques 
et le débat public. Des savoirs émanent aussi de sociétés savantes, des 
naturalistes notamment. Ils critiquent la dégradation de l’environnement, 
mais l’enthousiasme ambiant pour le progrès technique, comme le fait 
que les niveaux de vie augmentent, les rendent peu audibles. Il est aussi 
une contestation populaire qui porte sur les effets indésirables du progrès 
(à propos des pollutions ou des grands travaux, comme l’aménagement 
de la vallée du Rhône en France), mais elle reste localisée, sans possibilité, 
avant les années 1960, de se structurer à l’échelle médiatique. Les seuls 
relais de ces contestations sont les élus locaux, qui les portent parfois 
auprès des administrations 1.
Depuis les années 1970 : (1) les savoirs du management  
et leur diffusion dans la société
Pour saisir les changements qui émergent à partir des années 1970, 
je partirai des formes de contestation et de refus, et de la manière dont 
elles sont intégrées et gérées par les pouvoirs en place. Je commencerai 
par la contestation ouvrière, massive au tournant des années 1960-1970. 
Celle-ci – pensez aux usines de la Fiat ou de GM aux États-Unis – met 
en danger l’ordre industriel et suscite des réactions vives de la part  
des manageurs. En sort un autre type de savoirs gestionnaires, une  
autre approche du management que marque le vocable de « toyotisme ». 
Je ne mentionnerai ici qu’un exemple, le fact-based management,  
une gestion développée aux États-Unis et qui vise à reprendre en main 
les ateliers et s’appuie sur la « réalité objective » des faits enregistrés par 
l’encadrement. La fabrication de cette factualité repose sur la mise en  
place de mesures de l’efficacité des employés, sur la construction de compa-
raisons et classements – ce qui permet de gérer l’éviction des mauvais 
sujets sur une base qu’on peut publiquement montrer comme « fondée sur  
les faits 2 ».
1. Pritchard 2011, Pessis, Topçu et Bonneuil 2013.
2. Boltanski et Chiapello 2000, Bruno et Didier 2013.

	
les savoirs du social	
131
Cette forme de savoir vise à diriger la conduite des autres, mais moins 
par la violence brute (celle de la chaîne et l’usure des corps-machines) 
que par l’incitation et surtout l’obligation de participer à sa propre autoé-
valuation. Cela ne signifie en rien la disparition des formes de violence 
(on lira Dejours pour s’en convaincre 1) et ce nouveau type de management 
remplit sa fonction car il repose sur le pouvoir souverain du chef d’entre-
prise. Il n’en reste pas moins que ces savoirs sont d’un nouveau type  
et qu’ils ne construisent pas leur autorité comme les savoirs antérieurs. 
Ils sont des outils dont la puissance passe par le réordonnancement 
qu’ils imposent aux sujets, par ce qu’ils les amènent à faire. Leur pouvoir 
dérive de l’ubiquité des dispositifs qui les accompagnent et auxquels les 
individus peuvent rarement échapper. Ces savoirs et dispositifs refont 
le monde au quotidien car ils en ont la capacité – si ce n’est, toutefois,  
l’autorité morale.
Ces techniques et savoirs ont très vite migré hors de l’entreprise et ont 
contribué à la transformation du lien social, de ce qui « tient les sociétés ». 
Un premier héritage est celui de ce qu’on pourrait appeler l’evidence-based 
anything – à l’instar de l’evidence-based medicine qui, au nom de méta-ana-
lyses statistiques, prétend pouvoir dire qu’elle est « la meilleure pratique 
disponible ». Cette forme de « normalisation » s’accompagne de nouvelles 
nosologies, elle signifie une dévalorisation des savoirs de la clinique, une 
réduction de l’autonomie des praticiens et, parce qu’elle n’est pas innocente 
d’intérêts, une préférence pour les solutions chimiques.
Un second héritage est la mise en place, à partir des années 1980, de 
benchmarks – la méthode ouverte de coordination au niveau des États 
européens, par exemple – et la publication de palmarès – comme celui 
des meilleures universités. Ces benchmarks et palmarès ont des origines 
multiples – des journaux, des institutions privées ou publiques – mais ils 
ne s’imposent que s’ils ont derrière eux une conjonction de forces suffi-
samment puissantes pour les rendre inévitables. Pour le classement de 
Shanghai, on peut penser aux universités chinoises cherchant l’endroit 
où envoyer leurs étudiants, aux universités les mieux classées et qui 
promeuvent le classement, aux États qui veulent rendre leurs universités 
« excellentes », etc. Ces dispositifs d’incitation relèvent rarement d’un 
processus d’élaboration public ; ils constituent plutôt des savoirs montés 
de façon ad hoc, en fonction de certains objectifs, et faisant ensuite leur 
office parmi les populations où ils circulent. L’autorité de ces savoirs ne 
découle pas d’un assentiment public préalable mais de qui les porte et 
du nombre de réappropriations qui adviennent. Du fait de l’anonymat 
1. Dejours 1998.

132	
dominique pestre
qui les caractérise (ils sont comme sans auteur et simplement de bon 
sens – pensez à nouveau à la « médecine des preuves ») et de leur poids 
diffus mais constant (le classement de Shanghai est là et opère car repris 
et utilisé), ces savoirs et technologies offrent peu de prise à la contes-
tation et suscitent plutôt ressentiment, angoisse et « stress 1 ».
Depuis les années 1970 : (2) les savoirs des think tanks  
libéraux-conservateurs
Les savoirs nouveaux qui se déploient au début des années 1970 sont 
aussi le fait d’institutions nouvelles. Pour exister et rendre leurs savoirs 
audibles face aux savoirs qu’elles contestent (ceux des États et des univer-
sités), elles tendent à en appeler à l’opinion publique.
Les premières institutions à se comporter ainsi sont les think tanks 
libéraux des années 1970, des instituts de réflexion créés initialement à 
l’initiative du monde des affaires et des réseaux républicains dans le but 
de miner l’évidence des discours keynésiens et à vocation sociale qui 
dominent alors le monde académique. Leur stratégie, face au fait que ces 
savoirs occupent le lieu légitime qu’est l’Université, est d’en appeler, avec 
l’appui d’une presse en phase de politisation accélérée (pensez à Fox News 
et au groupe Murdoch), à l’opinion publique pour asseoir leurs analyses 
et les rendre crédibles 2. Ces analyses prennent la forme de rapports et 
d’études, de textes programmatiques énonçant ce que sont les « réalités » 
du monde, comment elles sont ignorées, et ce qu’il convient de faire.  
Ces savoirs, qui sont autant descriptifs que prescriptifs, gagnent en 
légitimité à travers les polémiques publiques – jusqu’au moment où, 
étant entendus, ils peuvent pénétrer l’Université et la reconfigurer. Un 
exemple emblématique en est la Heritage Foundation, fondée en 1971, 
et qui joue un rôle décisif dans la préparation intellectuelle et la formu-
lation politique du tournant reaganien 3.
Historiquement, le second temps est celui des think tanks néoconserva-
teurs des années 1990. Leur but est d’accentuer la révolution idéologique 
amorcée vingt ans auparavant, de refonder les valeurs – de promouvoir le 
monde de la sécurité, la guerre des civilisations et le refus de tout multi-
latéralisme. Leur stratégie repose sur le même principe – la publication 
de rapports et la création de polémiques à travers une presse amie. Leur 
1. Bruno et Didier 2013, numéro spécial de Minerva (vol. 47, no 3, octobre 2009).
2. Edwards 1997.
3. Heatherly 1981.

	
les savoirs du social	
133
importance, dans les années 1990 et 2000, est qu’ils contribuent à rouvrir 
toutes les « boîtes noires » des choses acceptées, à accélérer la déstabili-
sation des valeurs et savoirs acquis, et à rendre banale une géopolitique 
agressive. Plus généralement, les savoirs des think tanks sont explici-
tement articulés sur des causes politiques et économiques. En cela, ils ne 
partagent pas l’éthos habituel des universitaires : ils mènent un combat 
et le savoir produit est explicitement mobilisé dans ce but.
Il faut ensuite penser aux institutions internationales et à leurs mutations. 
Je pense à l’OCDE et aux institutions de Bretton Woods (Banque mondiale 
et FMI) mais encore aux Nations unies dont les programmes se trans-
forment constamment des années 1970 aux années 2000. Ces institutions, 
en parallèle aux think tanks, deviennent elles aussi de grosses produc-
trices d’analyses factuelles et de critères normatifs 1.
Les variations sont ici importantes. L’OCDE, par exemple, est la « boîte 
à idées » des pays développés, l’institution la plus importante pour la 
coordination de leurs politiques – ce qui lui permet de définir les normes 
s’appliquant de fait à l’échelle planétaire. Les exemples les plus connus  
du rôle de l’OCDE sont le rapport Brooks de 1971, un rapport étonnant 
car troublé par la contestation des années 1960 ; les rapports et recomman-
dations des années 1970 et 1980 sur l’environnement et qui promeuvent 
les « instruments économiques » (taxes sur les effluents et les produits, 
marchés de droits à polluer, politiques fiscales d’incitation) comme seule 
manière rationnelle de protéger la croissance sans nuire à l’environ-
nement ; les actions entreprises dans les années 1990 et 2000 en faveur  
de la « libéralisation nécessaire » (de l’eau par exemple) ; et bien sûr 
l’assouplissement des régulations et la promotion des benchmarks. D’où 
la nature des savoirs jugés bons et utiles dans les années 2000 et 2010,  
des services climatiques ou écosystémiques aux partenariats public-privé 
et aux « arrangements multi-acteurs » – les « tables rondes » légiférant 
sur la qualité des produits agricoles du Sud par exemple 2.
Les programmes des Nations unies sont assez transversaux aux modes 
de travail et d’action de l’OCDE ou de la Banque mondiale – qui en vient 
d’ailleurs à se dénommer Banque des savoirs au début du xxie siècle. Ils 
sont plus consensuels mais aussi moins dotés, moins reliés aux nœuds 
financiers et de pouvoir du Nord, et ils ont donc moins d’« efficacité ». 
Ils sont régulièrement contournés ou ignorés par les premiers, et leurs 
valeurs et programmes de recherche peuvent devenir l’objet de violentes 
attaques. C’est par exemple ce qui advient à l’OMS à la fin des années 
1. Moretti et Pestre 2015.
2. Cheyns 2011.

134	
dominique pestre
1970 lorsque son programme sur les soins de santé primaires défendu 
à Alma Ata suscite l’ire américaine et l’arrêt de divers financements 1.
J’aimerais clore cette section sur un point. Les normes définies par ces 
institutions pèsent sur l’Université et l’obligent à expliciter et justifier ce 
qui fait la légitimité et l’utilité de ses énoncés, de ses catégories et cadrages. 
Ce qui est une situation neuve et qui conduit le gouvernement Bush, à 
partir des années 2000, à dénier sa part de vérité aux savoirs des sciences 
qui déplaisent ou entravent le déploiement normal des affaires – comme 
avec les soft drinks ou le réchauffement climatique. Elle conduit aussi à des 
remodelages constants des formes d’organisation de l’expertise lorsque 
celle-ci ne produit pas les résultats escomptés 2.
Depuis les années 1970 : (3) les ONG nouvelles  
et leurs stratégies
Les think tanks libéraux et conservateurs ne sont pas les seuls, à partir 
des années 1970, à faire de l’espace public le nouveau lieu de légitimation 
des savoirs. On trouve, dans un registre parallèle, les grandes ONG inter-
nationales qui se généralisent elles aussi dans ces années – Greenpeace 
est fondée en 1971, tout comme Heritage. Ces organisations construisent 
elles aussi des équipes de recherche qui pénètrent le territoire des savoirs 
experts et permettent de contester l’inertie ou l’aveuglement des savoirs 
« officiels », des universités et du monde industriel. À proprement parler, 
ce sont d’ailleurs les ONG qui créent, en osmose avec les universitaires, 
le champ des sciences de l’environnement 3.
Le « groupe » des ONG est beaucoup moins homogène que celui des 
think tanks libéraux et conservateurs. On a d’abord des différences de 
taille et de localisation géographique – des grandes machines type WWF 
au Nord aux ONG développementalistes du Sud, souvent plus locales. 
Les statuts sont aussi différents. Certaines reposent sur le volontariat, 
d’autres sont devenues des institutions reconnues officiellement par les 
Nations unies ou la Communauté européenne, qui emploient des centaines 
de professionnels, et par lesquelles transitent des financements d’États 
pour le développement ou l’environnement. Les logiques historiques 
sont aussi très diverses et, depuis vingt ans, certaines grosses ONG du 
Nord (WWF, mais moins Greenpeace par exemple) sont devenues les 
1. Gaudillière 2014.
2. Pestre 2008, Oreskes et Conway 2010.
3. Mahrane (dans ce volume, p. 275).

	
les savoirs du social	
135
partenaires du monde des affaires pour promouvoir un monde pragma-
tiquement durable 1.
Au cours des trois dernières décennies, le phénomène des ONG se 
généralise et devient un rouage central de la production des savoirs. Trois 
points me semblent à retenir. D’abord, comme pour les think tanks, les 
ONG pèsent dans l’espace public par la production de connaissances ; 
elles mobilisent beaucoup de matière grise, elles produisent données et 
rapports – et cela rouvre les boîtes noires des sciences officielles. Ces 
organismes ont par ailleurs contribué à transformer les médias et à placer 
l’« opinion publique » en arbitre des choix et savoirs qui sont proposés. 
Les ONG, à la différence des syndicats ouvriers, n’ont pas d’abord des 
« revendications » ; elles montent des opérations très visibles et profitent 
de l’intérêt médiatique suscité pour exposer leurs conclusions et peser 
sur les consciences. Enfin, ces savoirs sont articulés sur des causes et 
des visions du monde – ce qui ne signifie pas que ces savoirs n’ont pas 
de valeur, loin s’en faut. Le résultat en est une fantastique ouverture des 
lieux de production des savoirs, un « aplatissement » des hiérarchies, 
une articulation plus explicite sur les intérêts en jeu – mais surtout une 
perte de l’autorité « transcendante » dont « la science » jouissait dans la 
phase antérieure.
Depuis les années 1970 : (4) production de savoirs  
et populations « ordinaires »
Depuis quarante ans, les savoirs ne sont pas seulement produits par les 
institutions – universités, États, entreprises, think tanks, ONG et autres 
organisations internationales. Ils le sont aussi par des populations de plus 
en plus nombreuses et diversement organisées. Ce phénomène n’est pas 
neuf, nous l’avons dit, mais il a explosé depuis les années 1970. Pour avoir 
un ordre de grandeur, on retiendra qu’on compte aujourd’hui en France 
14 000 associations de patients (avec 4 millions d’usagers adhérents) et que 
1 500 à 2 000 associations se créent chaque année autour des questions 
d’environnement 2. Dans ce qui suit, je me propose d’aborder cette galaxie 
à travers quatre entrées, renvoyant le lecteur aux sources et au Web pour 
d’autres exemples.
Le premier ensemble pourrait être celui des amateurs férus de science, un 
phénomène bien connu du xixe siècle. Je ne donnerai que deux exemples 
1. MacDonald 2010.
2. Voir < http://www.annuaire-aas.com > ou < http://www.francebenevolat.org > .

136	
dominique pestre
des pratiques récentes, l’un concernant les pratiques naturalistes (via 
Tela Botanica), l’autre qui montre la place nouvelle des amateurs dans 
l’analyse et la visualisation des données spatiales.
Tela Botanica est un réseau francophone en ligne regroupant environ 
20 000 personnes et où tout un chacun peut se former, contribuer à l’iden-
tification des flores locales, suivre la vie d’écosystèmes ou apprendre à 
préserver la biodiversité. Il est organisé autour d’espaces collaboratifs (des 
wikis), de banques de données à enrichir, de présentations cartographiques 
et d’une centaine de projets. Il offre des fiches pratiques visant la création 
d’observatoires, la gestion et le partage d’images, l’observation des saisons 
ou des flores urbaines. Le réseau est intégré aux travaux académiques 
et a permis l’émergence d’une communauté épistémique ouverte impli-
quant professionnels et amateurs. D’ailleurs, la « flore électronique » que 
le réseau produit est devenue un des standards du domaine (elle reçoit 
10 000 visites par jour) 1.
Descente de la sonde Huygens et atterrissage sur le sol de Titan en 2005. Croquis 
d’artiste élaboré à partir des images envoyées par la sonde et mises à disposition sur 
le site de l’Agence spatiale européenne. 
Mon second exemple est plus limité. Il a trait aux images enregis-
trées lors de la descente de la sonde Huygens sur Titan en janvier 2005, 
images immédiatement mises en ligne par l’Agence spatiale européenne. 
Or, en moins de huit heures, un réseau d’amateurs proposait des images 
1. < http://www.tela-botanica.org >, Millerand, Heaton et Proulx 2011.

	
les savoirs du social	
137
en 3D de la topographie de Titan, bientôt suivies de « vues d’artiste » 
plus complexes. La nouveauté est ici la mise à disposition immédiate 
des données par l’ASE, leur traitement par des développeurs d’images, et 
leur mise à disposition sur divers sites. Un régime que le Web autorise, 
comme dans le cas de Tela Botanica, mais qui signale aussi une nouvelle 
disposition de certaines institutions scientifiques vis-à-vis des amateurs 1.
Le deuxième ensemble de pratiques distribuées que je souhaite évoquer, 
celui des associations de patients, est lui aussi devenu massif dans les 
années 1970 et 1980 – via le Women Health Movement (et son best-seller 
Our Bodies Ourselves), le National Women Health Network (et ses actions 
juridiques et de lobbying)… et bien sûr l’épidémie de sida qui a conduit à 
une redéfinition des relations entre médecins, compagnies pharmaceu-
tiques et malades. Ces mouvements s’approprient le rapport à la maladie 
et aux institutions hospitalières, ils mettent au premier plan l’importance 
de bien « vivre » le traitement, ils visent la prise en main de soi et le refus 
de s’en remettre aux seuls experts et professionnels. La revendication 
est celle d’un partage des savoirs, d’une participation à la mise en œuvre  
des protocoles lors de l’introduction de nouvelles molécules – comme 
d’un droit à s’inspirer des pratiques « alternatives » dans les champs 
qu’ignore la médecine savante. Les associations de patients apportent 
aussi aux professionnels d’importantes données (ce qui est visible dans 
le cas des maladies rares où l’expertise clinique est dans les mains des 
familles) et ces travaux, menés souvent en lien avec les entreprises de 
la pharmacie, conduisent à redéfinir les traitements. Dans les sciences 
sociales, cela a conduit à de nombreuses réflexions. Bruno Latour insiste 
sur le fait que l’engagement de ces groupes marque une nouvelle expéri-
mentation collective, d’autres ont parlé de citoyenneté scientifique (Elam 
et Bertisson), d’un modèle pour l’empowerment (Clarke), de biosocialité 
(Rabinow) et d’une nouvelle forme de vital politics à l’échelle molécu-
laire (Rose) 2.
Le troisième ensemble est lié aux savoirs environnementaux – d’abord 
en réaction aux effets négatifs créés par le développement industriel. Des 
évolutions historiques sont ici visibles. Au début, dans les années 1960, 
les questions tournent autour de pollutions locales (de l’air, de l’eau, 
de certaines zones géographiques, ou des pollutions dues à certaines 
molécules). Plus tard, les questions abordées par les nombreuses associa-
tions qui émergent deviennent aussi globales – et ces actions débouchent 
1. Bigg 2007, 2011.
2. Löwy 2009, Epstein 2008. Pour d’autres exemples, < http://www.renaloo.com/forum >, 
<http://www.cancercontribution.fr >, <http://www.grippenet.fr > .

138	
dominique pestre
sur des victoires, comme pour le contrôle des CFC par le protocole  
de Montréal. Pour le nucléaire, les pratiques évoluent d’un refus initial 
de toute industrie nucléaire à des mesures autonomes de la radioactivité 
ambiante et un contrôle des institutions de régulation – après Tcher-
nobyl – mais sans que cesse la mobilisation pour l’arrêt du nucléaire. Pour 
le réchauffement climatique, des milliers d’associations – des Bingo aux 
associations les plus locales – continuent à inventorier les dégradations 
et à publier des données, et elles sont toujours massivement présentes 
dans le off des Conférences des parties organisées annuellement par le 
Programme des Nations unies 1. Elles y sont essentielles dans les années 
2000, même si elles ne sont jamais dominantes.
Le quatrième ensemble a trait à l’innovation. L’innovation n’a certes 
jamais été le seul fait des universitaires, ingénieurs et industriels : Réaumur, 
déjà, visitait les artisans pour saisir leurs tours de main et pratiques, et le 
développement de la radio dans l’entre-deux-guerres est largement dû aux 
milliers d’amateurs (au sens étymologique de « ceux qui aiment ») ayant 
construit ou inventé leurs propres appareils. La place des populations qui 
contribuent à l’innovation s’est toutefois fortement accrue dans la période 
récente. On pense bien sûr à l’économie numérique, aux logiciels libres, 
aux développeurs du Web, à la création de Wikipédia – et à tous ceux qui 
les utilisent, transforment et adaptent. Christophe Lécuyer et Fred Turner 
ont aussi montré que le développement de la Silicon Valley ne pouvait 
se comprendre sans la contre-culture californienne des années 1960 
et 1970, sans sa marginalité mais aussi sans l’étonnant mélange qui y 
prévaut entre fascination pour les techniques, travail librement choisi, 
coopération et esprit entrepreneurial. Outre ces cas bien connus, on 
peut aussi évoquer le mouvement des semences paysannes en France 
qui refuse la mainmise financière mais aussi culturelle des semenciers 
distributeurs de lignées pures, qui milite pour le droit à sélectionner ses 
propres graines, les échanger et expérimenter librement – un gage de 
qualité pour les farines et de résistance écologique face aux aléas clima-
tiques et de terrain 2.
Dans les sciences sociales, cette multiplication des formes « ascendantes » 
d’innovation construites sur les usages et des échanges horizontaux – et 
auxquelles les entreprises sont sensibles puisqu’elles leur offrent parfois 
des moyens économiques de concevoir leurs produits – a conduit à une 
abondante littérature sur l’innovation en réseau, l’importance d’intégrer 
ces formes dans les politiques publiques, ou le devoir de prévoir des 
1. Mahrane (dans ce volume, p. 275), Topçu 2014, Aikut et Dahan 2014.
2. Lécuyer 2006, Turner 2006, Bonneuil et Demeulenaere 2007.

	
les savoirs du social	
139
formes intermédiaires de propriété – toutes choses d’une extrême valeur. 
Il n’en reste pas moins que cette promotion tous azimuts de l’hybridité 
et de la co-construction, si chère aux science studies, a ses limites. Il 
lui arrive d’oublier que ces formes d’innovation sont parfois aussi au 
cœur d’un nouveau business model. Le succès de Google, par exemple, 
repose sur l’enthousiasme de centaines de milliers d’« amateurs » – mais 
il n’est pas que « collaboratif » : il est aussi très asymétrique. C’est en 
effet l’existence de cette main-d’œuvre gratuite et travaillant sur les 
plateformes en logiciel libre de la firme (avec plaisir et parfois espoir 
d’être recrutée) qui a offert à celle-ci ses capacités exceptionnelles de  
développement.
Je terminerai cette section par quelques mots sur ce qui a pu permettre 
ou faciliter cette floraison de producteurs nouveaux de savoir, un fait 
dont on ne peut que se réjouir. Cette floraison renvoie d’abord à ce que 
Marcel Gauchet nomme le fait libéral – le fait que les individus demandent 
toujours plus d’autonomie, de liberté et de droits, qu’ils refusent les 
formes d’autorité qui s’imposent à eux et s’organisent indépendamment 
des « systèmes » (au sens d’Habermas 1). Elle renvoie aux affaires qui 
ont encombré l’espace public autour de pollutions historiques (Torrey 
Canyon en 1967, Bhopal en 1984), d’accidents majeurs (Tchernobyl en 
1986), d’affaires de santé publique (de la thalidomide en 1961 au Mediator 
en 2012) – en bref, aux développements productifs exponentiels que 
nos sociétés ont connus. Elle dérive encore de l’émergence de nouvelles 
sciences, autour de l’environnement, du climat ou de la biodiversité,  
des sciences qui sont modélisatrices mais aussi de terrain, et qui ouvrent 
de nouveaux espaces aux non-professionnels (les sciences de labora-
toire favorisent moins de telles ouvertures). Elle émerge finalement des 
nouveaux outils de traitement et de rassemblement des données (pour 
les modélisations) et du Web. Celui-ci produit des formes moins hiérar-
chiques d’apprentissage et de traitement des problèmes ; il induit des 
méthodes de travail « en extension », d’autres manières de produire et 
consommer l’information, d’autres manières de juger des savoirs dispo-
nibles et de gérer la relation aux autorités. Polycentrique, il marginalise 
les canaux hiérarchiques de transmission des savoirs – et mine ainsi la 
science comme forme naturelle d’autorité 2.
1. Gauchet 2010-2012, Habermas 1987.
2. Pestre 2013, Mallein 2007.

140	
dominique pestre
 
Remarques finales
Résumons. Les savoirs sont partiaux et partiels car cadrés de façon 
toujours spécifique, et tout savoir a sa part d’ignorance. Ils sont aussi 
distribués dans le social, personne ne dispose de tous les savoirs perti-
nents – et c’est une hubris que de le croire. Cela n’implique toutefois pas 
que tout se vaille : chacun sait, dans un monde technologique et inter-
connecté, qu’il faut disposer du bon savoir – que ce soit pour l’innovation, 
l’évaluation des questions ou le suivi des solutions. Mais ce bon savoir, 
ce savoir « qui convient », se définit en fonction de valeurs, d’objectifs, 
d’intérêts personnels ou collectivement organisés.
Tout est donc compliqué – notamment lorsqu’il faut choisir. Deux 
tendances se manifestent aujourd’hui pour réduire ce nœud gordien 
des limites inhérentes à notre condition humaine et à nos savoirs. 
Certains promeuvent la confrontation des arguments, la discussion, 
puis la décision – notamment pour les régulations collectives ; d’autres 
en appellent aux marchés, seul recours contre notre ignorance 1. Dans le 
premier cas, la diversité des savoirs et de leurs cadrages est donnée comme 
un avantage épistémologique et politique, la confrontation raisonnée 
comme une nécessité – et cela conduit à une demande d’extension de la 
démocratie 2. Dans le second, la solution est celle d’un libéralisme new 
look et radical (seuls les marchés anticipent correctement), internalisant 
éventuellement certaines externalités négatives (environnementales par 
exemple) ou demandant des droits de propriété plus stricts (sur les biens 
communs ou via la transformation du droit des brevets). La politique 
est alors celle des états de fait – ce qui engendre refus et protestations 
a posteriori.
Il n’est évidemment pas de solution consensuelle au-delà de ces 
alternatives – puisqu’il ne s’agit pas d’une affaire à régler entre acteurs 
« rationnels ». Et l’équilibre qui prévaudra dans le futur ne peut être anticipé. 
Il sera fonction de nos imaginations, des rapports de force entre personnes 
et groupes, de l’évolution du système Terre – notamment de la rapidité 
avec laquelle opérera le processus d’anthropocénéisation du monde. Ce 
qui ne veut pas dire qu’il n’est pas à prendre parti et à contribuer aux 
études, bien au contraire. Car il est une chose sûre, l’une des rares qui le 
soient – les civilisations peuvent disparaître ou se suicider.
1. Hayek 1944.
2. Rosanvallon 2006.

	
les savoirs du social	
141
 
Références bibliographiques
Aykut Stefan et Dahan Amy, 2014, « La gouvernance du changement climatique. 
Anatomie d’un schisme de réalité », in Dominique Pestre (dir.), Le Gouvernement 
des technosciences, Paris, La Découverte, p. 97-131.
Biagioli Mario, 1993, The Practice of Science in the Culture of Absolutism, Chicago 
(IL), University of Chicago Press.
Bigg Charlotte, 2011, « Images », in Gérard Azoulay et Dominique Pestre (dir.), 
C’est l’espace ! 101 savoirs, histoires et curiosités, Paris, Gallimard, p. 179-181.
–	 2007, « In weiter Ferne so nah. Bilder des Titans », Bildwelten des Wissens, 5/2, p. 9-19.
Boltanski Luc et Chiapello Ève, 2000, Le  Nouvel Esprit du capitalisme, Paris, 
Gallimard.
Bonneuil Christophe et Demeulenaere Élise, 2007, « Vers une génétique de pair 
à pair ? L’émergence de la sélection participative », in Florian Charvolin, André 
Micoud et Lynn K. Nyhart, Des sciences citoyennes ? La question de l’amateur 
dans les sciences naturalistes, La Tour-d’Aigues, Éd. de l’Aube, p. 122-147.
Boudia Soraya, 2014, « Gouverner par les instruments économiques. La trajectoire 
de l’analyse coût-bénéfice dans l’action publique », in  Dominique Pestre (dir.), 
Le Gouvernement des technosciences, Paris, La Découverte, p. 231-260.
Brown Theodore M., Cueto Marcos et Fee Elizabeth, 2006, « The World Health 
Organization and the Transition from International to Global Public Health », 
American Journal of Public Health, vol. 96, p. 62-72.
Bruno Isabelle et Didier Emmanuel, 2013, Benchmarking. L’État sous pression 
statistique, Paris, La Découverte.
Cahan David, 2004, An Institute for an Empire : The Physikalisch-Technische 
Reichsanstalt (1871-1918), Cambridge, Cambridge University Press.
Carnino Guillaume, 2015, L’Invention de la science. La nouvelle religion de l’âge 
industriel, Paris, Seuil.
Cheyns Emmanuelle, 2011, « Multi-Stakeholder Initiatives for Sustainable Agriculture :  
Limits of the “Inclusiveness” Paradigm », in Stefano Ponte, Jakob Vestergaard 
et Peter Gibbon (dir.), Governing through Standards : Origins, Drivers and Limits, 
Londres, Palgrave, p. 318-354.
Dahan Amy et Pestre Dominique, 2004, Les Sciences pour la guerre (1940-1960), 
Paris, Éd. de l’EHESS.
Dejours Christophe, 1998, Souffrance en France. La banalisation de l’injustice 
sociale, Paris, Seuil.
Desrosières Alain, 1999, « La commission et l’équation : une comparaison des plans 
français et néerlandais entre 1945 et 1980 », Genèses, no 34, p. 28-52.
–	 2009, « How to Be Real and Conventional : A Discussion of the Quality Criteria of 
Official Statistics », Minerva, vol. 47, p. 307-322.
Edwards Lee, 1997, The Power of Ideas : The Heritage Foundation at 25 Years, Ottawa 
(IL), Jameson Books.
Epstein Steve, 2008, « Patient Groups and Health Movements », in  Edward 
J.  Hackett, Olga Amsterdamska, Michael Lynch et Judy Wajcman (dir.), 
The Handbook of Science and Technology Studies, Cambridge (MA), MIT Press, 
p. 499-539.
Fressoz Jean-Baptiste, 2012, L’Apocalypse joyeuse. Une histoire du risque techno­­
logique, Paris, Seuil.

142	
dominique pestre
Gauchet Marcel, 2010-2012, séminaire sur La  Signification du néolibéralisme 
(17 décembre 2010-25 mai 2011) et sur La Radicalisation de la modernité : le droit 
et la dynamique de l’individualisation (16  novembre 2011-30  mai 2012), Paris, 
Éd. de l’EHESS, < http://marcelgauchet.fr/blog > .
Gaudillière Jean-Paul, 2014, « De la santé publique internationale à la santé 
globale. L’OMS, la Banque mondiale et le gouvernement des thérapies chimiques », 
in Dominique Pestre (dir.), Le Gouvernement des technosciences, Paris, La Décou-
verte, p. 65-96.
Godin Benoît, 2009, « The Making of Science, Technology and Innovation Policy », 
accessible sur < www.ucs.inrs.ca > .
Habermas Jürgen, 1987, Théorie de l’agir communicationnel, Paris, Fayard, 2 vol.
Hayek Friedrich A., 1944, The Road to Serfdom, Chicago (IL), University of Chicago 
Press.
Heatherly Charles (dir.), 1981, Mandate for Leadership, Washington (DC), The   
Heritage Foundation.
Jas Nathalie, 2007, « Public Health and Pesticide Regulation in France before and 
after Silent Spring », History and Technology, vol. 23, no 4, p. 369-388.
–	 2014, « Gouverner les substances chimiques dangereuses dans les espaces interna-
tionaux », in Dominique Pestre (dir.), Le Gouvernement des technosciences, Paris, 
La Découverte, p. 31-64.
Lécuyer Christophe, 2006, Making Silicon Valley : Innovation and the Growth of High 
Tech (1930-1970), Cambridge (MA), MIT Press.
Löwy Ilana, 2009, Preventive Strikes : Women, Precancer, and Prophylactic Surgery, 
Baltimore (MD), Johns Hopkins University Press.
MacDonald Kenneth Iain, 2010, « The Devil Is in the (Bio) diversity : Private Sector 
“Engagement” and the Restructuring of Biodiversity Conservation », Antipode, 
vol. 42, no 3, p. 513-550.
Mallein Philippe, 2007, « Usage des TIC et signaux faibles du changement social », 
< http://ensmp.net/pdf/2008/TIC%20et%20Paradoxes%20philippe%20Mallein.
pdf > .
Millerand Florence, Heaton Lorna et Proulx Serge, 2011, « Émergence d’une 
communauté épistémique. Création et partage du savoir botanique en réseau », 
in Serge Proulx et Annabelle Klein (dir.), Connexions. Communication numérique 
et lien social, Namur, Presses universitaires de Namur.
Moretti Franco et Pestre Dominique, 2015, « Bankspeak : The Language of World 
Bank Reports », The New Left Review, no 92, mars-avril, p. 75-99.
Murphy Michelle, 2012, Seizing the Means of Reproduction : Entanglements of Feminism, 
Health, and Technoscience, Durham (NC), Duke University Press.
Oreskes Naomi et Conway Erik M., 2010, Merchants of Doubt : How a Handful of 
Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming, 
New York, Bloomsbury Press.
Pessis Céline, Topçu Sezin et Bonneuil Christophe (dir.), 2013, Une autre histoire 
des « Trente Glorieuses ». Modernisation, contestations et pollutions dans la France 
d’après guerre, Paris, La Découverte.
Pestre Dominique, 2003, Science, argent et politique. Un essai d’interprétation, Paris, 
Quæ.
–	 2008, « Challenges for the Democratic Management of Technoscience : Gover-
nance, Participation and the Political Today », Science as Culture, vol. 17, no 2, juin, 
p. 101-119.

	
les savoirs du social	
143
–	 2013, À contre-science. Politiques et savoirs des sociétés contemporaines, Paris, Seuil.
Pritchard Sara B., 2011, Confluence : The Nature of Technology and the Remaking of 
the Rhône, Cambridge (MA), Harvard University Press.
Rosanvallon Pierre, 2006, La Contre-Démocratie. La politique à l’âge de la défiance, 
Paris, Seuil.
Topçu Sezin, 2014, L’Agir contestataire à l’épreuve de l’atome. Critique et gouver-
nement de la critique dans l’histoire de l’énergie nucléaire en France (1968-2008), 
Paris, Seuil.
Turner Fred, 2006, From Counterculture to Cyberculture : Stewart Brand, the Whole 
Earth Network and the Rise of Digital Utopianism, Chicago (IL), University of 
Chicago Press.
Vieille-Blanchard Élodie, 2011, Les Limites à la croissance dans un monde global, 
thèse de doctorat, EHESS, Paris.
Vries Marc de, 2005, 80 Years of Research at the Philips Natuurkundig Laboratorium 
(1914-1994), Amsterdam, Pallas Publications.
Wintroub Michael, 2000, « Court Society », in Arne Hessenbruch (dir.), Reader’s 
Guide to the History of Science, Londres, Fitzroy Dearborn, p. 154-157.


7 Un siècle toxique.  
L’émergence de la « santé  
environnementale »
L i n d a  N a s h
La « santé environnementale » désigne à la fois une condition matérielle 
et une perspective culturelle. L’expression naît dans les années 1930 
parmi les spécialistes de santé publique et ne devient d’usage courant que 
dans les décennies qui suivent la Seconde Guerre mondiale. Bien sûr, les 
préoccupations sur le lien entre milieux et santé remontent loin dans le 
temps, comme en témoignent la tradition grecque autour d’Hippocrate 
(Airs, eaux, lieux), les pratiques ayurvédiques indiennes ou encore la 
médecine traditionnelle chinoise. Mais l’association entre santé et lieu fait 
alors tellement partie des axiomes de base que parler de santé environ-
nementale est redondant. L’adjectif ne devient pertinent qu’après que les 
milieux ont été séparés de la santé des corps.
Les chercheurs s’accordent en effet sur le fait qu’un tournant s’est 
produit vers la fin du xixe siècle dans la compréhension de la santé et 
de la maladie. On date généralement de cette époque les débuts de la 
médecine scientifique et, même si nombre d’historiens contestent la 
dimension triomphaliste de ce récit, ils restent globalement d’accord que 
la théorie microbienne, comme mode de compréhension des maladies, a 
considérablement affaibli les rapports développés entre milieux et santé. 
Dans le sillage des découvertes de Louis Pasteur, une nouvelle génération 
de médecins et de professionnels de santé se met à situer de plus en 
plus les causes des maladies non dans le milieu environnant mais dans 
des bactéries, virus ou parasites spécifiques. Et la santé en vient alors à 
apparaître comme la question d’un corps autonome qui tient l’environ-
nement à distance 1.
1. Rosen 1958, Tomes 1998.
 Manifestation contre la pollution à Los Angeles, 1955.

146	
linda nash
Ces évolutions intellectuelles de la médecine ont toutefois pris place, 
et là est le paradoxe le plus intéressant, dans une époque de change-
ments environnementaux sans précédent. Alors que les sociétés 
européennes mettent à profit l’énergie du charbon et du pétrole, elles 
commencent à transformer leurs environnements de façon profonde, 
radicale, définitive. Certaines racines du concept moderne de santé 
environnementale remontent ainsi à la fin du xixe siècle et aux change-
ments rapides, environnementaux, sociaux et corporels provoqués par  
l’industrialisation.
La santé dans le milieu industriel :  
de la pureté à la sécurité
Avant même que les professionnels de la médecine commencent à 
limiter leur regard à la santé du corps comme entité « autonome », l’indus-
trialisation bouleverse l’environnement planétaire et suscite l’apparition 
de nouvelles maladies et épidémies. L’énorme quantité de main-d’œuvre 
qu’elle demande entraîne la formation d’un environnement nouveau, 
celui de la ville industrielle. Des lieux comme Chicago ou Manchester 
deviennent des centres caractérisés par la surpopulation, des conditions 
de vie misérables pour les ouvriers et une pollution inégalée de l’air et 
de l’eau. Les maladies infectieuses se mettent à faire des ravages dans les 
centres urbains d’Europe et d’Amérique du Nord. À la fin du xixe siècle, 
la théorie des germes fournit les réponses de base à ces épidémies : le rôle 
de l’environnement perd de son importance au profit d’une mise en avant 
des corps individuels, et tout particulièrement de corps se distinguant 
par leur race ; en Europe comme en Amérique du Nord, les minorités 
ethniques et raciales sont souvent visées comme « porteuses » de maladies 
dans des régions sans cela tenues pour saines 1.
Les niveaux désastreux de pollution urbaine, qui exacerbent les maladies 
connues, se mettent à en créer de nouvelles. Les densités élevées de 
population humaine et animale, combinées à la croissance des déchets 
industriels, transforment rivières et lacs en égouts fétides véhiculant la 
typhoïde ou le choléra. Les consommations de charbon lestent le ciel de 
fumées causant des maladies respiratoires et le rachitisme chez les enfants. 
Pour les réformateurs progressistes, les relations entre corps et environ-
nement sont claires. Ils font le lien entre problèmes de santé et milieux 
pollués, et ils exercent des pressions sur les autorités locales pour que 
1. Platt 2005, Tarr 1996, Hardy 1993, Kraut 1994, Leavitt 1996, Shah 2001.

	
un siècle toxique	
147
des mesures soient prises – au moment même où la médecine rétrécit 
son champ d’action pour le ramener à l’échelle du corps 1.
Afin de satisfaire leurs besoins en ressources, les nations industrielles 
étendent leur contrôle sur des régions qui en sont riches. Tandis que 
des technologies nouvelles amènent des quantités inégalées de charbon, 
pétrole et minéraux à la surface de la terre, des foules de travailleurs 
sont transportées vers des sites invraisemblables tels que les puits de 
charbon du pays de Galles ou d’Amérique du Nord, les mines de cuivre 
en Afrique, ou encore les mines de phosphate en Polynésie. Les usines 
accumulent ces éléments, et les ouvriers se retrouvent à travailler dans 
des milieux confinés, de douze à quatorze heures par jour. Cette concen-
tration a pour résultat un nombre croissant de maladies spécifiques : 
la « colique du plomb » (empoisonnement au plomb), la « pourriture 
du potier » (silicose), la « nécrose phosphorée » (empoisonnement au 
phosphore). Bien que nombre des maladies liées à des travaux particu-
liers soient connues de longue date, elles apparaissent dorénavant plus 
fréquemment et touchent de plus en plus de gens.
La toxicité de ces environnements suscite l’apparition de nouveaux 
métiers, de nouveaux mouvements. Ceux qui sont consacrés à la mise 
en évidence du lien entre symptômes de maladies et travail industriel 
forment une nouvelle discipline : l’hygiène industrielle. Que ce soit en 
Europe ou aux États-Unis, les réformateurs parcourent les usines, parlent 
avec les ouvriers de leurs maladies, suivent à la trace leurs différentes 
manifestations. Dans les années 1880, l’État allemand se met à exiger 
l’inspection régulière des industries du plomb et, en 1895, le gouver-
nement britannique crée la Commission sur les activités dangereuses. 
Sous la direction du médecin Thomas Oliver, cette commission produit 
un ouvrage pionnier dans l’histoire de la médecine du travail : Les Indus-
tries et leurs dangers. Aspects historiques, sociaux et légaux des activités 
industrielles affectant la santé, de l’avis d’un comité d’experts. Le rapport 
Oliver couvre le sujet de manière exhaustive, et accorde une attention 
particulière aux activités émettrices de poussière (poterie, industrie 
équipementière, production de cuivre, de fer et de zinc, extraction de 
pierres, textile) ainsi qu’aux secteurs utilisant des métaux toxiques tels 
que le mercure, l’arsenic, le plomb 2.
Aux États-Unis, le pilier du mouvement est Alice Hamilton – fille d’une 
très bonne famille du Midwest, bactériologiste de formation et membre 
du groupe de réforme progressiste de la Hull House de Chicago. En 1910, 
1. Platt 2005, Melosi 1980, Mosley 2001.
2. Oliver 1902, Sellers 1997a, Corn 1992.

148	
linda nash
Hamilton participe dans l’Illinois aux travaux de la commission d’État 
chargée d’étudier la santé des travailleurs des « industries du poison ». 
Dans le rapport qui s’ensuit, elle fournit d’amples informations sur l’empoi-
sonnement à grande échelle par le plomb dont l’industrie est le théâtre,  
ce qui provoque les premières réformes de médecine du travail dans  
l’histoire des États-Unis 1. Comme le suggère l’œuvre d’Oliver ou la carrière 
d’Hamilton, l’hygiène industrielle se préoccupe alors essentiellement 
du plomb – un composé longtemps reconnu comme toxique et devenu 
omniprésent à la fin du xixe siècle dans les céramiques industrielles, les 
batteries électriques, les tuyaux d’eau ou la peinture.
Dans les années qui suivent la Première Guerre mondiale, l’étude 
des maladies industrielles entre dans les laboratoires. En focalisant 
leurs recherches sur des produits chimiques spécifiques, une nouvelle 
génération de professionnels entreprend de mesurer leurs effets sur 
la santé de manière quantitative. Cette nouvelle science, la toxico-
logie, ramène les dangers du lieu de travail à des produits considérés 
comme des microbes, à des agents singuliers capables, en pénétrant dans  
l’organisme, de provoquer des symptômes précis. Ce faisant, la toxicologie 
véhicule les définitions étroites de la causalité et de la maladie données 
par la théorie des germes : ce n’est pas l’environnement global qui compte 
mais l’exposition à tel ou tel produit chimique spécifique 2.
Ces manières de faire sont aussi déterminées par le désir de rendre le 
travail plus efficace et par la présomption que les ouvriers sont par nature 
enclins à la dissimulation. Dans le référentiel de la toxicologie, seules 
les situations qui peuvent être liées quantitativement à une exposition 
mesurée et associées à un effet physiologique connu peuvent signaler 
une maladie d’origine chimique. Comme l’a affirmé Christopher Sellers, 
la toxicologie sert donc les intérêts des nouveaux professionnels scien-
tifiques et ceux des directeurs d’entreprise. La toxicologie a beau être  
une science, elle est aussi le fruit d’un compromis politico-économique 
né de la période d’entre-deux-guerres. Les questions qui structurent la 
discipline obéissent moins à des idées générales concernant la « santé » 
qu’à des objectifs d’efficacité au travail et de productivité 3.
Les représentations du corps forgées par la toxicologie se distinguent 
toutefois de celles rencontrées en bactériologie par un aspect important. 
Tandis que la bactériologie voit la santé comme le produit d’un corps 
pur tenant les germes à l’écart, la toxicologie y voit le résultat d’un  
1. Grant 1967, Hamilton 1943.
2. Sellers 1997a (p. 141-186), Nash 2008.
3. Sellers 1997a (p. 159-169).

	
un siècle toxique	
149
corps stable et équilibré – conception tirée de la nouvelle discipline de  
la physiologie expérimentale. Les nouveaux physiologistes considèrent 
le corps moins comme un contenant susceptible d’être contaminé que 
comme un système autorégulateur cherchant, selon les mots de Walter 
Cannon, professeur à Harvard, à atteindre des conditions d’« homéostasie ». 
S’inspirant du concept d’autorégulation, les toxicologues introduisent 
l’idée de seuil biologique selon lequel il existe un niveau d’exposition en 
deçà duquel le corps peut absorber et s’adapter aux substances polluantes 
sans encourir de dommages permanents. Découlant d’expériences en 
laboratoire sur des animaux, ces seuils deviennent la base des « niveaux 
de concentration sans danger » dans les usines. Il s’ensuit que les concepts 
de pureté environnementale et corporelle ne remplissent plus de rôle 1. 
La quête de la pureté corporelle est remplacée par celle d’une sécurité 
corporelle vis-à-vis d’expositions chimiques continues.
Le fait que le danger des produits chimiques se manifeste au-delà 
du lieu de travail est connu dès le début du siècle. La Grande-Bretagne  
s’est ainsi efforcée de réduire les émissions chimiques des usines en 
reconnaissant les dangers encourus par les populations avoisinantes.  
Au début des années 1900, les effets nocifs du plomb sur la santé sont 
aussi de notoriété publique. Ainsi la France en interdit-elle l’usage dans 
les peintures d’intérieur en 1909. Dix ans plus tard, la Société des Nations 
fait pression pour généraliser cette interdiction. Bien que plus de 40 pays 
rallient cette position, les États-Unis s’y refusent – du fait de la puissance 
de l’industrie du plomb, de l’isolationnisme qui prévaut alors et du  
poids de l’administration conservatrice 2. La production et l’utilisation du 
plomb dans les habitations et les produits de consommation restent donc 
extrêmement répandues durant presque tout le xxe siècle dans ce pays.
Il n’y a toutefois pas que dans la peinture que le plomb représente un 
danger. À partir de la découverte dans les années 1860 des propriétés 
insecticides du « vert de Paris » (acéto-arsénite de cuivre), les fermiers 
américains adoptent les pesticides à base de métaux. Et ce sont les défis 
posés par la spongieuse ou bombyx disparate – un ravageur résistant 
au « vert de Paris » – qui encouragent l’adoption, en 1892, d’un nouveau 
composé de l’arsenic : l’arséniate de plomb. Dans le demi-siècle qui  
suit, cet insecticide reste le plus répandu (seul le DDT le surpassera), et 
les produits frais deviennent une source d’intoxication potentielle pour 
les consommateurs 3.
1. Nash 2008, Sellers 1997a (p. 175-183), Sturdy 1988.
2. Halliday 1961, Warren 2000 (p. 57 et 62).
3. Whorton 1975.

150	
linda nash
Bien que les mêmes pesticides soient commercialisés en Europe, les 
fermiers ne les adoptent pas de façon aussi massive qu’aux États-Unis, et 
les gouvernements européens ont une approche beaucoup plus prudente. 
Les Français interdisent les insecticides à l’arsenic en 1846, et une deuxième 
fois en 1916, tandis que le gouvernement britannique établit en 1903  
des normes strictes sur les produits alimentaires de 0,01 grain d’arsenic 
par livre. Les législateurs américains sont tout aussi préoccupés mais sont 
contraints par leur manque d’autorité : les États-Unis ne limitent l’usage 
de l’arsenic qu’en 1927, à la suite d’un cas d’empoisonnement largement 
relayé par les médias, et sous la menace d’un embargo britannique sur les 
produits américains 1. Les législateurs se tournent aussi vers les toxico-
logues industriels pour qu’ils déterminent les seuils de concentration 
sans danger. La logique qui prévaut à l’usine se trouve ainsi devenir la 
règle dans l’espace public. Et l’accent mis par la toxicologie sur la notion 
de seuil biologique en deçà duquel aucun dommage sérieux ne peut se 
produire alimente, pendant des décennies, le discours de niveaux « sans 
danger » dans notre environnement et nos corps 2.
Pendant la Seconde Guerre mondiale et le boom économique d’après 
guerre, la production et la consommation de plomb augmentent consi-
dérablement. Quantité de maisons construites dans l’après-guerre 
sont envahies de plomb – ce qui implique une exposition continue du 
grand public sur des dizaines d’années. Bien que le risque soit alors 
général, tout le monde n’est pas égal quant au degré d’exposition. Du 
fait de la faiblesse de la réglementation, les Américains sont les plus 
exposés ; et à l’intérieur du pays, ceux qui en souffrent le plus sont les 
populations pauvres et non blanches confinées dans l’habitat décati 
des centres-villes. Au début des années 1950, une épidémie d’empoi-
sonnement au plomb est repérée parmi les enfants pauvres des villes. 
Ce n’est pourtant qu’en réponse à l’activisme des années 1960 que les 
premières mesures sérieuses sont prises. Mais le présupposé toxico-
logique selon lequel il existe un niveau d’exposition au plomb « sans 
danger » mène ces essais de réforme dans l’impasse. Ce n’est qu’en 
1976 que les États-Unis interdisent la peinture au plomb et amorcent 
la suppression progressive de l’essence au plomb 3. Le plomb reste 
1. Le standard américain pour l’arsenic établi en 1927 fut affaibli en 1940 sous l’influence des 
producteurs de fruits et leurs alliés politiques. Cf. Whorton 1975 (p. 68-92 et 133-175).
2. Warren 2000. La promulgation d’un standard pour le plomb fut retardée par l’absence de 
méthode de détection rapide et simple de ce métal ; des progrès dans les méthodes analytiques 
permirent l’adoption d’une réglementation sur les taux maximaux de plomb dans les produits 
alimentaires en 1933. Cf. Whorton 1975 (p. 220-221), Sellers 1997a (p. 200-209).
3. Selon Warren, la consommation de plomb s’est accrue de 70 % aux États-Unis entre 1939 et 1944. 
Cf. Warren 2000 (p. 134-177, chiffres p. 173). Jusqu’à la fin des années 1960, l’administration de santé 

	
un siècle toxique	
151
toutefois très présent dans l’environnement et le corps humain : la 
répartition géographique de l’empoisonnement a simplement évolué. 
L’exposition au plomb est maintenant plus importante dans certains 
pays en développement, et il n’est guère facile de se débarrasser 
des produits saturés de plomb une fois qu’ils existent. Ils sont dans  
les déchetteries, transportés par bateaux dans les régions les plus 
pauvres du globe pour y être recyclés, entraînant parfois – comme cela 
a été le cas pour la Chine ou l’Uruguay – des crises majeures de santé 
publique. Et l’histoire d’amour de la société moderne avec le plomb 
continue : on extrait aujourd’hui plus de plomb que jamais auparavant 1.
Mais le plomb n’est pas tout et l’industrialisation a produit aussi une 
pléthore de composés dérivés du charbon et du pétrole. Alimentée au 
départ par les demandes des armées et, aux États-Unis et en Allemagne, 
par des systèmes de brevet récompensant généreusement les inven-
teurs individuels, une quête effrénée de nouveaux produits synthétiques 
démarre à la fin du xixe siècle. Les candidats à l’invention se livrent à des 
expériences sur toutes sortes de matériaux mais les extraits de goudron 
de houille, dérivés du procédé de cokéfaction, sont les ingrédients clés 
des nouvelles substances, notamment des teintures et explosifs. Avec 
ses importantes ressources en charbon et la qualité de son système 
technique, l’Allemagne domine la production de colorants et teintures. 
Les usines chimiques commencent à se répandre le long du Rhin à la fin 
des années 1800, et en 1895 un médecin allemand fait état de nombreux 
cas de tumeurs à la vessie parmi les employés des usines de colorants et 
chez des riverains vivant en aval 2.
Même si le charbon demeure la principale source de matières premières 
chimiques jusqu’à la Seconde Guerre mondiale, les compagnies pétro-
lières commencent à créer des dérivés du pétrole dès les années 1900. 
L’un des tout premiers est le toluol (1903), ingrédient de la dynamite.  
La découverte du procédé de craquage du pétrole en 1908, qui permet 
de réduire les grosses molécules de carbone et de produire des carbu-
rants plus légers et faciles d’utilisation, donne naissance à de nouveaux 
publique du pays soutenait qu’un niveau de plomb dans le sang de 60 µg/l était acceptable. Cette 
norme a ensuite été baissée pour atteindre 5 µg/l en 2012, et les scientifiques s’accordent pour avancer 
que ce taux devrait être de 0 ou proche de 0. Markowitz et Rosner 2013 (p. 18), Pirkle et al. 1994.
1. « Lead Poisoning in China : The Hidden Scourge », The New York Times, 5 juin 2011 ; Renfrew 
2012. La production mondiale de plomb s’est accrue de 38 % depuis 1970 pour atteindre 4,7 millions 
de tonnes. US Geological Survey, « Historical Statistics for Minteral and Material Commodities 
in the US », Data Series 140, < http://minerals.usgs.gov/ds/2005/140/#data > (consulté le 
2 décembre 2013). En 2006, l’Union européenne a interdit le plomb dans tous les produits ; 
les autres pays n’ont pas suivi cette voie.
2. Cioc 2002 (p. 112-130).

152	
linda nash
produits. La perturbation des chaînes d’approvisionnement au  
cours de la Première Guerre mondiale incite au développement de 
substituts synthétiques, et les compagnies pétrolières accouchent,  
dans l’entre-deux-guerres, de nouvelles fibres synthétiques comme le 
nylon, des caoutchoucs de synthèse (Néoprène), des plastiques (Téflon, 
PVC) 1, etc.
La Seconde Guerre mondiale ne fait qu’accroître le développement 
de l’industrie chimique, notamment aux États-Unis où les besoins 
créés par la guerre suscitent d’énormes investissements fédéraux. À la 
fin du conflit, les fabricants du pays ont commencé à remplacer les 
matériaux conventionnels utilisés jusque-là par des produits synthé-
tiques. Les maisons, construites auparavant en bois, verre, métal, plâtre 
et briques, sont dorénavant emplies de contreplaqué, amiante, plastiques.  
L’alimentation connaît quant à elle une « révolution gastrochimique » et 
les vêtements, faits auparavant en coton ou en laine, sont de plus en plus 
fabriqués à partir de Nylon ou de polyester 2.
Le passage rapide de sociétés construites sur les hydrates de carbone 
issus de la biomasse à des sociétés basées sur les dérivés du pétrole et 
du charbon constitue une révolution des fondements matériels de la  
vie. Au cours du xxe siècle, les nouvelles industries de la pétrochimie 
et du plastique œuvrent avec succès à la mise à l’écart des questions 
sur les effets de leurs produits sur la santé et l’environnement – faisant 
au contraire de ces produits les symboles de la culture et de l’économie 
modernes. La société DuPont le formule à sa façon dans un slogan célèbre  
de 1933 : « Améliorez votre vie, avec de meilleurs produits… grâce à la 
chimie 3. »
Au moment où retentissent ces mots d’ordre modernisateurs,  
des dizaines de cas de cancers de la vessie sont diagnostiqués chez 
DuPont, à la production de colorants. Reconnaissant la possibilité que 
l’entreprise ait une part de responsabilité, DuPont crée un laboratoire 
de toxicologie en 1934 et met à sa tête le médecin allemand Wilhelm 
Hueper. Celui-ci démontre que l’un des produits phares de l’entreprise, 
la bêta-naphtylamine, provoque le cancer de la vessie chez les chiens. 
Une exploration aussi approfondie des risques encourus par les travail-
leurs ne correspond toutefois pas à ce que DuPont a en tête, et Hueper 
est démis de ses fonctions après seulement trois ans d’exercice 4.
1. Spitz 1988, Aftalion 1991.
2. US Public Health Service 1962 (p. 21), White 1994 (p. 211-236).
3. Morris et Ahmed 1992, Meikle 1995.
4. Hueper 1938 (p. 255), Sellers 1997a. DuPont monta ensuite des accusations selon lesquelles 
Hueper était un nazi, puis un communiste. Proctor 1995 (p. 39-40 et 43).

	
un siècle toxique	
153
L’emballage et la transformation des produits alimentaires sont parmi 
les premiers domaines où l’on utilise les dérivés de produits pétroliers. 
Mais les consommateurs sont sceptiques et, au début du siècle, les préoc-
cupations suscitées par l’utilisation dans l’alimentation d’ingrédients 
artificiels conduisent à des réformes dans une bonne partie de l’Europe 
et des États-Unis. En 1905, la France légifère sur la fraude et la falsifi-
cation en matière de produits alimentaires ; l’année suivante, les États-Unis 
votent la loi sur la pureté de la nourriture et des médicaments, dont le 
texte donne au Bureau de chimie du gouvernement fédéral le pouvoir  
de saisir toute nourriture susceptible de contenir des ingrédients 
« dangereux pour la santé ». Le premier directeur du département,  
Harvey Wiley, est un réformateur convaincu dédié à l’idée de nourriture 
« pure 1 ».
Wiley et son administration sont chargés de déterminer quels produits 
chimiques sont « dangeureux pour la santé » et doivent donc être bannis. 
La loi part du principe que l’origine des problèmes se trouve dans les 
composés chimiques entrant dans la nourriture « pure » et qu’il est 
possible de les traiter au cas par cas. Wiley parvient ainsi à faire interdire 
les conservateurs alimentaires les plus dangereux et à limiter le nombre 
des colorants utilisés. Cependant, les tribunaux tendent à faire peser 
la charge de la preuve sur le gouvernement. Et avec le développement 
rapide de l’industrie chimique alimentaire et l’augmentation continue des 
produits transformés dans l’entre-deux-guerres, des milliers de produits 
chimiques entrent dans l’alimentation sans avoir subi de tests 2.
L’écologie des corps et la santé environnementale
La Seconde Guerre mondiale marque un tournant environnemental 
pour le monde industrialisé. La guerre fait exploser la demande en métaux, 
pétrole et innovations chimiques. Par ailleurs, la découverte d’insecti-
cides très efficaces, la production de masse de nouveaux produits de 
consommation à base de pétrole et la poursuite effrénée de l’armement 
nucléaire engendrent des effets aux ramifications multiples sur l’eau, l’air 
et les sols. Il devint très vite évident que ces changements affectent aussi 
les corps humains, et l’idée que la santé est largement indépendante de 
l’environnement se voit rapidement abandonnée.
1. Dessaux 2007, Junod 2000.
2. White 1994 (p. 1-161). La Grande-Bretagne donna suite, interdisant le formaldéhyde, l’acide 
salicylique et l’acide borique en 1925. French et Phillips 2000. Wiley ne parvint cependant pas 
à obtenir l’interdiction de la saccharine, cf. Junod 2000.

154	
linda nash
À la fin des années 1950, une spécialiste de biologie marine améri-
caine et écrivaine scientifique s’attaque à l’écriture d’un livre sur les effets 
perturbateurs d’une nouvelle classe de pesticides de synthèse. En 1962, 
Rachel Carson publie Un printemps silencieux, qui devient rapidement 
un classique. Au cœur sont le DDT et d’autres composés organochlo-
riques, des produits extraordinairement efficaces contre la plupart des 
insectes, tout en ayant une faible dangerosité pour les mammifères. Le 
monde agro-industriel adopte donc ces nouvelles substances et voit  
en elles une solution alternative aux menaces que font peser sur la santé 
les pesticides à base de métaux.
L’ouvrage de Rachel Carson repose sur l’idée que la prolifération dans 
l’environnement de ces nouvelles substances compromet la santé humaine 
par des voies insoupçonnées mais potentiellement catastrophiques. 
Carson s’appuie, pour étayer sa thèse, sur les avancées de la toxicologie, 
notamment sur le travail de Wilhelm Hueper, ainsi que sur les informa-
tions accumulées lors d’une série d’audiences tenues entre 1950 et 1952 
Les audiences Delaney et les origines 
À partir de 1950-1952, la Chambre des représentants américaine organise 
une série de consultations sur les additifs chimiques destinés à l’alimentation. 
L’Agence américaine des produits alimentaires et médicamenteux (FDA) rencontre 
depuis les années 1910 des difficultés à réguler les additifs alimentaires. À la date 
d’ouverture des audiences, la FDA estime à plus de 700 les produits chimiques 
utilisés couramment pour la transformation des produits alimentaires, admettant 
qu’environ 300 sont présumés dangereux ou n’ont pas été testés. Les témoignages 
mettent en évidence les préoccupations des experts et des consommateurs quant 
aux effets à long terme sur la santé de ces mutations de l’industrie des aliments. 
James Delaney, qui préside ces audiences, fait part de l’inquiétude montante sur la 
nature potentiellement cancérigène de certains produits chimiques utilisés dans 
l’alimentation.
Malgré l’échec des États-Unis dans leur tentative d’agir fermement contre les 
résidus de pesticides dans l’alimentation, le Congrès fait passer en 1958 l’interdiction 
d’utiliser dans les produits alimentaires transformés tout produit chimique connu 
ou passant pour être cancérigène. Cependant, quelques années plus tard, le Congrès 
approuve l’ajout d’une exception autorisant la poursuite de l’usage de substances 
cancérigènes dans l’alimentation animale. Le recours aux hormones synthétiques 
est alors en jeu, notamment le diéthylstilbestrol (DES), utilisé abondamment dans 
les fermes américaines pour engraisser le bétail et la volaille.
L’idée derrière cette mesure est de permettre à l’industrie de l’élevage d’utiliser 
ces hormones si elles sont indétectables dans la viande qui arrive au consommateur. 

	
un siècle toxique	
155
au Congrès américain sur les résidus de pesticides et les additifs alimen-
taires (voir l’encadré « Les audiences Delaney… », ci-dessus). Mais Carson 
ne considère pas les corps humains comme des entités séparées de leur 
environnement par des frontières imperméables. Elle fait plutôt référence 
à l’écologie des systèmes et à la biologie évolutionniste pour affirmer que 
perturber les milieux revient en fin de compte à perturber leurs habitants : 
les conditions écologiques sont au cœur de la santé humaine.
Bien que Carson se soit concentrée sur les pesticides agricoles, son livre 
rencontre un écho retentissant – signe que de plus en plus de gens consi-
dèrent que le monde est pollué. L’étalement urbain, le développement 
rapide des industries, ainsi que l’explosion du recours aux combustibles 
fossiles, transforment les pollutions de l’eau ou de l’air en sujets majeurs. 
Dès le début des années 1940, le « smog » est un problème récurrent 
à Los Angeles. En 1948, un pic de pollution plus long qu’à l’ordinaire 
entraîne la mort de 19 personnes et cause des maladies chez un tiers 
des habitants de la ville de Donora, en Pennsylvanie. Quatre ans après, 
de l’évaluation des risques réglementaires
Au bout de quelques années, toutefois, les autorités détectent régulièrement du 
DES dans la viande, du fait de l’adoption de méthodes d’analyse chimique plus 
précises. Décidée à ne pas interdire l’usage d’un produit extrêmement rentable, la 
FDA prend le parti de mettre sur pied un nouveau cadre réglementaire. En 1973, de 
nouvelles dispositions « sur la précision de la méthode », limitées à cette catégorie 
très restreinte de substances utilisées pour la nourriture animale, remplacent la 
notion d’« absence de résidus » par celle d’« absence de risque significatif », et 
déclarent que, tant que la méthode de détection est capable d’établir l’« absence 
de risque significatif », il s’agit d’une méthode acceptable.
Selon la définition de la FDA, l’« absence de risque significatif » pour tout 
produit chimique donné résulte de la combinaison de deux facteurs : la puissance 
de la substance chimique et la dose probable à laquelle sont soumis les individus. 
Cependant, l’Agence souligne aussi que le niveau correspondant à une « absence 
de risque significatif » pour chaque substance est susceptible d’être abaissé à 
l’avenir à mesure que les technologies de détection et les connaissances à ce sujet 
progresseront. Bien que la proposition ne s’applique qu’à une poignée de produits 
chimiques, les représentants de l’industrie l’attaquent avec véhémence et en 
appellent à une définition fixe et non pas mobile. L’Agence donne sa réponse en 
1979 : l’« absence de risque significatif » est établie comme un risque de cancer par 
individu de 1 pour 1 million. Durant la décennie qui suit, la méthode de la FDA 
et le niveau de risque acceptable de 1 pour 1 million deviennent les standards de 
la réglementation américaine.

156	
linda nash
un « smog meurtrier » affecte plus de 100 000 personnes à Londres et est 
plus tard identifié comme à l’origine de 4 000 décès. Bien que les experts 
en santé publique expriment leurs inquiétudes au sujet de la pollution 
de l’eau depuis la fin du xixe siècle, ces enjeux montent d’un cran après  
guerre.
Carson fait aussi référence à une autre source d’inquiétudes, à savoir 
les dangers des retombées radioactives et le développement d’armes 
nucléaires toujours plus puissantes. Les retombées nucléaires sont l’objet 
de préoccupations après la Seconde Guerre mondiale, à un moment où  
les États-Unis poursuivent des campagnes d’essais nucléaires permanents. 
Ce n’est toutefois qu’à la suite d’incidents graves au milieu des années 1950 
que les dangers des retombées parviennent aux oreilles du public. Un 
essai mené en 1954 dans l’océan Pacifique sur l’atoll de Bikini – là où les 
États-Unis ont fait exploser leur première bombe H – entraîne la conta-
mination massive de l’est de l’océan. Au rang des victimes, on compte 
l’équipage d’un bateau de pêche japonais. Quand les pêcheurs rentrent au 
pays, la possibilité que les produits alimentaires soient eux-mêmes conta-
minés déclenche une panique. Plus de 30 millions de Japonais signent 
des pétitions réclamant l’arrêt des essais. Le Congrès américain annonce 
la tenue d’audiences tandis que l’Académie des sciences américaine et le 
Comité de la recherche médicale britannique lancent des études sur les 
dangers de la radioactivité pour la santé 1. Les risques associés à l’irra-
diation ont alors de profondes répercussions sur la notion de sécurité 
corporelle. On est en effet en présence d’un danger nouveau et invisible 
susceptible de parcourir de longues distances et d’entrer à l’abri des regards 
dans l’alimentation. Qui plus est, la radiation est connue pour affecter 
non seulement les personnes exposées mais aussi les fœtus. Alors qu’une 
nouvelle génération de scientifiques activistes s’attache à faire connaître 
les dangers des retombées radioactives, des populations entières prennent 
conscience de leur vulnérabilité partagée face à ces produits nouveaux.
Ces transformations matérielles à grande échelle contribuent à faire 
évoluer les représentations de la santé, tant parmi les experts que dans le 
public non initié. Basées pendant plus de cinquante ans sur une conception 
du corps comme séparé et imperméable, les idées qui gouvernent la santé 
publique sont gravement entamées dans les années 1960 par les réalités 
des pollutions environnementales. La réapparition ou, plutôt, la persis-
tance de préoccupations publiques quant aux milieux de vie et leurs 
effets sur la santé réactive alors les mouvements réclamant un contrôle 
renforcé de l’industrie et des réformes environnementales.
1. Carson 1962, Whittemore 1987 (p. 505-549), Boudia 2007.

	
un siècle toxique	
157
En 1960, le Congrès américain organise des auditions sur la « santé 
environnementale ». Celles-ci mettent en évidence les problèmes associés 
aux déchets nucléaires, à la pollution de l’air et de l’eau, ainsi qu’aux 
produits chimiques présents dans l’alimentation et sur les lieux de travail. 
Deux ans plus tard, un comité d’experts chargé d’examiner ces questions 
critique la séparation institutionnelle des programmes de santé humaine et 
environnementale au sein de l’administration américaine et en préconise 
la refonte. À la fin des années 1960, le mouvement environnemental 
moderne est devenu une force politique majeure qui fait reconnaître 
l’importance des effets des pollutions sur la santé 1.
Un printemps silencieux rencontre un énorme écho en Europe mais 
aussi au Japon, montrant qu’une même anxiété parcourt le monde indus-
trialisé. De tous les pays concernés, la Suède seule prend des mesures 
précoces et vigoureuses contre les pesticides, faisant interdire le DDT dès 
1969. Dans le même temps, les représentants des industries et gouver-
nements britanniques et français continuent de décrire le problème  
des pesticides comme spécifique aux États-Unis, sous prétexte que le 
système agricole y est largement plus industrialisé et la réglementation 
moins robuste 2.
De toute évidence, l’avènement de la médecine moderne, l’accent qu’elle 
met sur les microbes et l’imperméabilité des corps ainsi que la séparation 
qu’elle établit entre santé humaine et environnement permettent que les 
transformations environnementales sans précédent qui ont lieu se déploient 
sans examen sérieux des conséquences possibles sur la santé. Qui plus est, 
les experts, tout comme les non-spécialistes, adhèrent toujours en grand 
nombre au récit héroïque du progrès médical et scientifique, affirmant 
que la découverte de ces nouveaux problèmes atteste en premier lieu du 
succès rencontré dans la résolution des problèmes antérieurs 3.
Au milieu des années 1960, des manifestations populaires mettent  
les gouvernements nationaux à l’épreuve et les contraignent à prendre 
des mesures pour combattre la pollution de l’environnement. Les 
États-Unis s’attaquent au départ avec bien plus de vigueur à la régulation 
des polluants que les pays européens ou le Japon, faisant passer des  
mesures limitant sévèrement la pollution de l’air et de l’eau et réclamant  
l’évaluation des conséquences environnementales des activités. Dans  
leur recherche de modèles de régulation, les membres de l’administration 
américaine se réfèrent aux domaines de la toxicologie et de la santé au travail. 
1. US Public Health Service 1962, Hays 1987.
2. Stoll 2012.
3. Voir par exemple US Public Health Service 1962, Hays 1987.

158	
linda nash
En 1970, le Congrès américain adopte la loi de la sécurité et de la santé 
au travail, attribuant au ministère du Travail le pouvoir d’établir les seuils 
réglementaires d’exposition chimique. Quelques années plus tard, la loi 
sur l’eau potable (1974) et celle sur le contrôle des substances toxiques 
(1976) donnent à l’Autorité américaine de protection de l’environnement 
le pouvoir de décider des seuils réglementaires pour l’environnement dans 
son ensemble 1. Cependant, alors que ces lois sont débattues et votées, les 
représentations du corps et de ses relations à l’environnement se voient 
transformées une nouvelle fois.
De la sécurité au risque dans un monde saturé de chimie
La fin du xxe siècle marque, selon l’expression d’Ulrich Beck, l’avènement 
d’une « société du risque 2 ». Selon Beck, la prolifération en apparence sans 
fin de risques pour la santé et l’intégrité de l’environnement constitue un 
produit de la modernisation capitaliste. Au-delà du fait que l’industriali-
sation introduit de nouvelles menaces, c’est la représentation de celles-ci 
sous la forme de « risques » plutôt que de « dangers » ou de « maladies » 
qui marque un tournant dans les politiques et discours. Alors que l’idéal 
« sécuritaire » reconnaît comme inévitable la pénétration dans le corps  
de substances dangereuses, experts et administrations maintiennent  
l’existence de niveaux en dessous desquels le corps est en mesure de réparer 
les dommages et donc de se prémunir contre toute atteinte sérieuse.  
La notion de risque, à l’opposé, repose sur l’idée que la sécurité, tout 
comme auparavant la pureté, est une chimère et que les risques pour 
la santé sont inhérents au monde moderne, et par là même inévitables. 
Considérée sous cet angle, la santé devient alors une notion relative, un 
état résultant d’une pesée des risques les uns par rapport aux autres, et 
de leur mise en rapport avec certains bénéfices.
Le « risque » fait partie de la langue et de la culture communes ; « l’éva-
luation des risques », en revanche, est une technique bureaucratique 
qui transforme la notion en un élément opérationnel de législation 
environnementale. Les efforts américains d’encadrement des programmes 
nucléaires et des additifs alimentaires carcinogènes en sont à l’origine. La 
radioactivité et les produits chimiques cancérigènes posent problème aux 
autorités lorsqu’elles tentent de déterminer des niveaux de « sécurité » en 
deçà desquels il n’y a pas d’effet sur la santé. Reconnaissant qu’il est alors 
1. Andrews 1999.
2. Beck 1992.

	
un siècle toxique	
159
problématique de donner l’assurance qu’il n’y a pas de danger – et refusant 
d’arrêter la production d’armes nucléaires comme l’utilisation alimen-
taire de produits chimiques –, le gouvernement américain se dédouane 
de ses responsabilités de garant de la sécurité publique.
Les scientifiques savent depuis le milieu des années 1920 que l’expo-
sition aux radiations a des effets génétiques sur plusieurs générations 
sans qu’il soit possible de déterminer un quelconque niveau de seuil ou 
de sécurité. Une de leurs caractéristiques est en effet qu’on ne peut les 
observer que statistiquement, sous la forme d’une augmentation générale 
du nombre des mutations au sein d’une population. Comme certaines de 
ces mutations se produisent naturellement, il est impossible de dire si une 
mutation individuelle particulière est due à l’irradiation, à l’exposition à 
des produits chimiques, ou si elle serait advenue de toute façon. De plus, 
des risques élevés au sein d’une population peuvent être relativement bas 
à l’échelle d’un individu. Ce paradoxe permet aux experts (du nucléaire 
notamment), en se focalisant uniquement sur le niveau faible de risque 
pour l’individu, de minimiser l’importance des menaces. Il devient aussi 
possible de comparer pour chaque individu, du moins de façon rhéto-
rique, les dangers des radiations nucléaires avec les risques attachés au 
rayonnement naturel, aux lieux de travail, ou même à la conduite d’une 
automobile. Par ailleurs, la Commission à l’énergie atomique américaine 
(AEC) ne se lasse jamais de mettre en avant, en ces temps de guerre 
froide, la menace que représente l’Union soviétique pour la sécurité du 
pays. De façon implicite, et bien qu’elle ne le présente jamais de cette 
manière, l’AEC se livre à une analyse coût-bénéfice dont il résulte que les 
coûts pour la santé des Américains de l’exposition aux radiations sont 
plus qu’équilibrés par les bénéfices incommensurables découlant de la 
supériorité nucléaire des États-Unis 1.
Une telle façon de considérer les problèmes de santé comme des risques 
nécessaires pris pour s’assurer des bénéfices plus grands conduit à faire 
accepter comme normaux les effets sur la santé qu’entraînent certains 
niveaux de radiation. Reconnaissant qu’il n’y a pas de quantité sans risque 
en matière de radioactivité, la principale agence de réglementation améri-
caine, le Comité national sur la protection contre les radiations, rétablit 
tranquillement sous un autre nom les standards de 1948, sans changement 
du niveau d’exposition autorisé. La « quantité tolérée » de radiation est 
renommée « quantité maximale permise 2 ».
Au milieu des années 1950, au moment même où se déroule la crise 
1. Whittemore 1987 (p. 135-217), Walker 2000, Boland 2002 (p. 496-617).
2. Whittemore 1987 (p. 322-323).

160	
linda nash
suscitée par les retombées des essais nucléaires, l’Agence américaine 
des produits alimentaires et médicamenteux (FDA) rencontre beaucoup 
de difficultés en interne à réguler l’explosion, dans l’alimentation, de 
produits chimiques dont certains passent pour être cancérigènes. En 
1958, le Congrès américain décide de remédier aux préoccupations 
montantes dues au fait qu’il ne semble pas y avoir de niveau d’exposition 
« sans risque » à un produit cancérigène. Un amendement à la loi sur  
les additifs alimentaires connu sous le nom d’amendement Delaney se fait 
l’écho du vieux principe de pureté en interdisant l’utilisation de produits 
potentiellement cancérigènes dans les produits alimentaires transformés. 
La loi est pourtant établie de façon assez pragmatique, puisqu’elle laisse 
hors de son périmètre de nombreux additifs déjà rentrés dans les usages 
courants. À l’époque cependant, les États-Unis sont le seul pays à prendre 
de telles précautions 1.
Les difficultés qu’il y a à défendre la pureté à grands coups de lois dans 
un marché alimentaire déjà saturé de produits chimiques deviennent 
cependant manifestes : au cours des vingt années qui suivent, la FDA 
rompt avec sa politique de blocage des produits cancérigènes. Dans les 
années 1970, elle met en place un programme basé sur la « modularité 
des méthodes » et fait du concept de risque le pilier de sa réglementation. 
Cette évolution culmine en 1979, avec la mise au point d’un standard 
numérique de « risque acceptable » pour les produits cancérigènes dans 
l’alimentation (1 pour 1 million). Ce faisant, l’évaluation quantitative du 
risque pénètre désormais l’univers de la consommation.
La méthode est adoptée comme principe par les acteurs industriels et 
les leaders politiques conservateurs qui cherchent alors à contrecarrer les 
réglementations environnementales du tournant des années 1960-1970 ; 
l’évaluation des risques est vue par eux comme un moyen de faire  
entrer les analyses coût-bénéfice dans les réglementations sur la santé 
et l’environnement, au mépris de la résistance populaire à l’idée que la 
santé humaine puisse être évaluée en termes économiques. Les tribunaux 
et l’administration présidentielle qui tentent de limiter le pouvoir des 
agences mettent en avant l’évaluation des risques comme une approche 
et un encadrement plus « scientifiques » de la santé environnementale.
Ce faisant, les préoccupations de la population en matière de toxicité 
explosent, embrasées par une série de désastres majeurs. En 1976, une 
explosion à l’usine chimique ICMESA en Italie, à Seveso, relâche de la 
dioxine et entraîne la mort de plus de 3 000 bêtes de ferme. En 1978, 
l’histoire de la communauté de Love Canal contaminée par des produits 
1. Vogel 2013 (p. 34-38), FAO / WHO Expert Committee on Food Additives 1961.

	
un siècle toxique	
161
chimiques à Niagara Falls dans l’État de New York devient une affaire 
nationale qui lève le voile sur l’existence de milliers de sites de déchets 
toxiques à travers le pays. Un an plus tard, la fusion partielle du réacteur 
de la centrale nucléaire de Three Mile Island en Pennsylvanie conduit 
à l’évacuation de 140 000 personnes. Enfin, en 1982, l’usine d’Union 
Carbide en Inde, à Bhopal, explose, relâchant une foule de gaz toxiques ; 
plus de 3 000 personnes y trouvent la mort et 500 000 personnes sont 
blessées dans ce qui demeure aujourd’hui le pire accident industriel  
au monde.
Alors que les toxiques deviennent une source majeure d’inquiétudes 
pour les populations, l’évaluation quantitative des risques gagne en 
visibilité. En 1983, le Conseil national de la recherche des États-Unis fait 
de l’évaluation des risques un cadre réglementaire propre à déterminer 
les dangers et à établir les priorités dans de multiples domaines. Des 
personnalités de premier plan de l’administration du président Ronald 
Reagan adoptent publiquement l’évaluation des risques comme un moyen 
efficace pour établir des priorités parmi les réglementations sur l’envi-
ronnement et la santé de la décennie précédente, considérées comme 
excessives, afin de les limiter.
Ainsi, au début des années 1990, les méthodologies du risque ont 
largement remplacé la notion de sécurité dans les politiques réglementaires 
américaines. Face à leurs promoteurs qui soutiennent que l’évaluation  
du risque rend les politiques environnementales plus scientifiques et 
moins ouvertes au politique, des voix s’élèvent pour affirmer que de tels 
procédés ne font rien de moins que travestir les jugements politiques 
et sociaux en décisions techniques afin de limiter l’intervention du 
public. Selon une idée moins souvent avancée, l’avènement de l’évaluation  
du risque signe la victoire de l’application aux domaines de la santé et du 
corps de la logique de marché. Et tandis que la méthode acquiert ses lettres 
de noblesse à l’intérieur du pays, son intégration aux accords commer-
ciaux internationaux rend possible sa diffusion auprès des principaux 
partenaires américains. Ce changement politique et professionnel n’a 
toutefois jamais été approuvé ni même débattu publiquement. La plupart 
des gens – aux États-Unis ou dans le reste du monde – attendent toujours 
des gouvernements qu’ils assurent leur sécurité face aux dangers d’un 
environnement contaminé. Cette divergence de vues devient manifeste 
lors des nombreuses controverses qui opposent régulateurs et scientifiques 
adeptes de l’idée d’un « risque » négligeable, et collectifs revendiquant la 
hausse des niveaux de sécurité et de précaution.

162	
linda nash
 
Conclusion
Ainsi le terme de « santé environnementale » est-il un produit du 
xxe siècle et du monde industrialisé. Il fait état d’une séparation intel-
lectuelle entre les corps et leurs écologies tout autant que leurs liens 
indéfectibles. Mais la question de savoir comment ces liens doivent être 
compris reste la source de nombreuses controverses. Le système de 
régulation mis en place au début du xxe siècle est construit sur la double 
idée que les corps sont relativement imperméables et que les menaces  
de santé environnementale sont limitées en nombre et peu reliées 
les unes aux autres. Quand il devient évident que des centaines de 
substances polluantes pénètrent le corps humain et que des dizaines de 
milliers sont présentes dans l’environnement, les autorités de régulation 
se retrouvent confrontées à une mission impossible. Tandis que la 
population réclame la sécurité, les industriels déclarent que seuls les 
produits chimiques les plus dangereux justifient une réglementation 
directe, et seulement s’ils sont présents en grande quantité. Les travaux 
scientifiques s’accumulent néanmoins pour saper les fondements de 
telles affirmations. Une exposition à de faibles doses est susceptible 
d’engendrer des effets substantiels. À l’échelle d’une vie, ces effets peuvent 
créer des synergies et s’accumuler. Tandis que le nombre et le niveau des 
expositions sont bien supérieurs dans la plupart des cas à ce qui était  
anticipé.
Ceux qui ont le plus d’intérêts à l’expansion des industries nucléaire 
et chimique cherchent alors, en réponse, à déplacer les réglementa-
tions vers des solutions plus accommodantes. C’est le cas, par exemple, 
des systèmes mettant en balance les coûts (ou risques) avec des 
bénéfices prétendus. La « société du risque » dans laquelle nous nous 
trouvons aujourd’hui est non seulement un monde plein de ces dangers 
engendrés par le capitalisme industriel, mais aussi un monde dans 
lequel la plupart d’entre nous en sommes venus à appréhender notre 
santé – ainsi que celle de nos enfants – sous la forme privilégiée du calcul  
économique.
Traduit par Clara Breteau
Références bibliographiques
Aftalion Fred, 1991, A History of the International Chemical Industry, Philadelphie 
(PA), University of Pennsylvania Press.
Andrews Richard N.L., 1999, Managing the Environment, Managing Ourselves : 

	
un siècle toxique	
163
A History of American Environmental Policy, New Haven (CT), Yale University 
Press.
Beck Ulrich, 2008 [1992], La Société des risques, Flammarion.
Boland Joseph B., 2002, The Cold War Legacy of Regulatory Risk Analysis : The Atomic 
Energy Commission and Radiation Safety, Ph.D., University of Oregon.
Boudia Soraya, 2007, « Global Regulation : Controlling and Accepting Radioactivity 
Risks », History and Technology, vol. 23, no 4, décembre, p. 389-406.
Carson Rachel, 2014 [1962], Printemps silencieux, Wildproject Éditions.
Cioc Mark, 2002, The Rhine : An Eco-Biography (1815-2000), Seattle (WA), University 
of Washington Press.
Corn Jaqueline, 1992, Response to Occupational Health Hazards : A  Historical 
Perspective, New York, Van Nostrand Reinhold.
Dessaux Pierre-Antoine, 2007, « Chemical Expertise and Food Market Regulation 
in Belle Époque France », History and Technology, vol. 23, no 4, décembre, p. 351- 
368.
FAO  / WHO Expert Committee on Food Additives, 1961, Evaluation of the 
Carcinogenic Hazards of Food Additives, Genève, World Health Organization.
French Michael et Phillips Jim, 2000, Cheated Not Poisoned ? Food Regulation in 
the United Kingdom (1875-1938), Manchester, Manchester University Press.
Grant Madeleine P., 1967, Alice Hamilton : Pioneer Doctor in Industrial Medicine, 
Londres, Abelard-Schuman.
Halliday E.C., 1961, A Historical Review of Atmospheric Pollution, World Health 
Organization, Monograph Series no 46.
Hamilton Alice, 1943, Exploring the Dangerous Trades : The Autobiography of Alice 
Hamilton, M.D., Boston (MA), Little, Brown & Company.
Hardy Anne, 1993, The Epidemic Streets : Infectious Disease and the Rise of Preventive 
Medicine (1856-1900), Oxford, Oxford University Press.
Hays Samuel P., 1987, Beauty, Health, and Permanence : Environmental Politics in the 
United States (1955-1985), New York, Cambridge University Press.
Hueper Wilhelm C., 1938, « Cancer of the Urinary Bladder in Workers of Chemical  
Dye Factories and Dyeing Establishments », Journal of Industrial Hygiene, vol. 16, 
no 4.
Junod Suzanne W., 2000, « Food Standards in the United States : The Case of the Peanut 
Butter and Jelly Sandwich », in David F. Smith et Jim Phillips (dir.), Food, Science, 
Policy, and Regulation in the Twentieth Century : International and Comparative 
Perspectives, New York, Routledge, p. 167-188.
Kraut Alan M., 1994, Silent Travelers : Germs, Genes, and the « Immigrant Menace », 
New York, NY, BasicBooks.
Leavitt Judith W., 1996, Typhoid Mary : Captive to the Public’s Health, Boston (MA), 
Beacon Press.
Markowitz Gerald et Rosner David, 2013, Lead Wars : The Politics of Science and the 
Fate of America’s Children, Berkeley (CA), University of California Press.
Meikle Jeffrey L., 1995, American Plastic : A Cultural History, New Brunswick (NJ), 
Rutgers University Press.
Melosi Martin, 1980, Pollution and Reform in American Cities (1870-1930), Austin 
(TX), University of Texas Press.
Morris David J. et Ahmed Irshad, 1992, The Carbohydrate Economy : Making Chemicals 
and Industrial Materials from Plant Matter, Washington (DC), Institute for Local 
Self-Reliance.

164	
linda nash
Mosley Stephen, 2001, The Chimney of the World : A History of Smoke Pollution in 
Victorian and Edwardian Manchester, Cambridge, White Horse Press.
Nash Linda, 2006, Inescapable Ecologies : A History of Environment, Disease, and 
Knowledge, Berkeley (CA), University of California Press.
–	 2008, « Purity and Danger : Historical Reflections on the Regulation of Environmental 
Pollutants », Environmental History, vol. 13, no 4, octobre, p. 651-658.
Oliver Thomas, 1902, Dangerous Trades : The Historical, Social, and Legal Aspects 
of Industrial Occupations as Affecting Health, by a Number of Experts, Londres, 
J. Murray.
Pirkle J.L. et  al., 1994, « The Decline in Blood Lead Levels in the United States : 
The National Health and Nutrition Examination Surveys (NHANES) », JAMA : 
The  Journal of the American Medical Association, vol.  272, no  44, 27  juillet,  
p. 284-291.
Platt Harold L., 2005, Shock Cities : The Environmental Transformation and Reform 
of Manchester and Chicago, Chicago (IL), University of Chicago Press.
Proctor Robert, 1995, Cancer Wars : How Politics Shapes What We Know and Don’t 
Know about Cancer, New York, Basic Books.
Renfrew Daniel E., 2012, « New Hazards and Old Disease : Lead Contamination and 
the Uruguayan Battery Industry », in  Christopher Sellers et Joseph Melling 
(dir.), Dangerous Trade : Histories of Industrial Hazard across a Globalizing World, 
Philadelphie (PA), Temple University Press, p. 99-112.
Rosen George, 1958, A History of Public Health, New York, MD Publications.
Sellers Christopher C., 1997a, Hazards of the Job : From Industrial Disease to 
Environmental Health Science, Chapel Hill (NC), University of North Carolina 
Press.
–	 1997b, « Discovering Environmental Cancer : Wilhelm Hueper, Post-World War II 
Epidemiology, and the Vanishing Clinician’s Eye », American Journal of Public Health, 
vol. 87, novembre, p. 1824-1835.
Shah Nayan, 2001, Contagious Divides : Epidemics and Race in San Francisco’s 
Chinatown, Berkeley (CA), University of California Press.
Spitz Peter H., 1988, Petrochemicals : The Rise of an Industry, New York, Wiley.
Stoll Mark, 2012, « Rachel Carson’s Silent Spring : A Book That Changed the World », 
< http://www.environmentandsociety.org/exhibitions/silent-spring/overview > .
Sturdy Steve, 1988, « Biology as Social Theory : John Scott Haldane and Physiological 
Regulation », The British Journal for the History of Science, vol. 21, p. 315-340.
Tarr Joel A., 1996, The Search for the Ultimate Sink Urban Pollution in Historical 
Perspective, Akron (OH), University of Akron Press.
Tomes Nancy, 1998, The Gospel of Germs : Men, Women, and the Microbe in American 
Life, Cambridge (MA), Harvard University Press.
US Public Health Service, 1962, Report of the Committee on Environmental Health 
Problems, Washington (DC), Government Printing Office.
Vogel Sarah A., 2013, Is It Safe ? BPA and the Struggle to Define the Safety of 
Chemicals, Berkeley (CA), University of California Press.
Walker J. Samuel, 2000, Permissible Dose : A History of Radiation Protection in the 
Twentieth Century, Berkeley (CA), University of California Press.
Warren Christian, 2000, Brush with Death : A  Social History of Lead Poisoning, 
Baltimore (MD), Johns Hopkins University Press.
White Suzanne R., 1994, Chemistry and Controversy : Regulating the Use of Chemicals 
in Foods (1883-1959), Ph.D., Emory University.

	
un siècle toxique	
165
Whittemore Gilbert Franklin, 1987, The National Committee on Radiation 
Protection (1928-1960) : From Professional Guidelines to Government Regulation, 
Ph.D., Harvard University.
Whorton James C., 1975, Before Silent Spring : Pesticides and Public Health in 
Pre-DDT America, Princeton (NJ), Princeton University Press.


8 Le siècle de l’atome  
en images
C h a r l o t t e  B i g g  1
Le xxe siècle est le siècle de l’atome. De l’électron au boson de Higgs, de 
Becquerel à Tchernobyl, du laboratoire de Frédéric Joliot-Curie aux centri-
fugeuses iraniennes en passant par le projet Manhattan et le CERN, les 
mines d’uranium du Kazakhstan ou du Niger, les sous-marins nucléaires 
ou les usages médicaux des radio-isotopes, l’atome et le nucléaire sont 
devenus des enjeux majeurs dans des domaines toujours plus interdépen-
dants – scientifiques, technologiques, industriels, militaires et politiques. 
Aucun autre objet que l’atome n’est probablement plus apte à illustrer 
la place des sciences dans les bouleversements qu’a connus le siècle et 
dans leur héritage contemporain : guerres mondiales, avènement de la big 
science, accidents industriels et pollutions, mondialisation et politiques 
énergétiques.
Chacune des ramifications de cette histoire donne lieu à la production 
d’une iconographie caractéristique et souvent marquante ; que l’on songe 
à l’atome de Bohr ou au champignon atomique ; à l’architecture hyperbo-
loïde des tours de refroidissement des centrales nucléaires ou encore au 
logo IBM fait d’atomes individuels de xénon, emblème des nanosciences. 
Ces leitmotivs visuels imprègnent notre culture collective. Chacun est 
issu d’un contexte et d’une série d’événements identifiables, mais, par 
leur répétition, reproduction et circulation ils finissent par s’ancrer et 
nourrir les imaginaires de la technoscience du xxe siècle.
Précisons qu’il faut ici comprendre la culture non pas comme un 
1. Je remercie Jochen Hennig pour son autorisation aimable de reprendre ici certaines des analyses 
développées dans le cadre d’un projet commun autour des images de l’atome, comprenant un 
séminaire à l’université Humboldt de Berlin (2006), une exposition au Deutsches Museum, 
Munich (« Atombilder. Strategien der Sichtbarmachung im 20. Jahrhundert », 2007-2008) et 
une publication collective (Bigg et Hennig 2009).
 De 1952 à 1957, la ville de Las Vegas organisa des concours de beauté dont la lauréate était nommée 
Miss Atomic Bomb. Ici : Miss Atomic Bomb prise en photographie par Don English, mai 1957. 

168	
charlotte bigg
ensemble d’objets (traditionnellement des productions artistiques, litté-
raires ou architecturales) mais comme un processus dynamique à travers 
lequel les individus et les groupes créent du sens à partir de ces objets, 
y compris ceux relevant de registres que l’on qualifie de populaires. La 
culture, dans cette conception, n’est pas produite d’une part et consommée 
de l’autre mais est le fruit, en perpétuel devenir, de l’interaction d’indi-
vidus entre eux et avec des objets, des images et des textes. Elle s’élabore, 
s’exprime et définit la manière dont on conçoit, ressent et parle de ces 
images et textes, la manière dont on les représente. La culture entre en 
jeu dans l’élaboration des identités à la fois individuelles et collectives. 
C’est un lieu où les relations de pouvoir s’expriment, sont contestées et 
transformées. L’histoire culturelle des sciences s’intéresse ainsi moins 
aux échanges réciproques entre deux objets constitués qui seraient la 
science et la culture (la manière dont le roman ou le film de science-fiction 
commentent les sciences par exemple) qu’aux processus d’élaboration 
mutuelle de ces deux entités et, plus fondamentalement, à la contribution 
des activités scientifiques à l’élaboration de conceptions et de pratiques 
partagées du monde et de la société 1.
L’histoire culturelle considère que ces conceptions et pratiques sont 
situées dans l’espace et le temps. On s’est ainsi intéressé à la perception 
olfactive ou au regard entendus comme des phénomènes historiquement 
situés, même si l’on peut supposer que les ressorts physiologiques de la 
perception humaine sont relativement stables 2. L’histoire des cultures 
visuelles comprend ainsi l’étude des normes esthétiques, des techniques 
de fabrication des images et des conventions de représentation (comme 
la perspective), mais aussi les théories de la perception et les conceptions 
particulières du statut et des fonctions de l’image dans des contextes 
donnés 3.
Si les images et pratiques visuelles ont joué un rôle à toutes les époques, 
le xxe siècle se caractérise par une révolution médiatique marquée par une 
augmentation exponentielle du nombre d’images dans l’imprimé grâce à 
de nouvelles techniques de visualisation, d’enregistrement et de repro-
duction ; alors qu’en parallèle on voit éclore et se répandre les technologies 
de diffusion et de réception des images et du son que sont la télévision, le 
cinéma, la vidéo puis Internet. Sans nier l’importance pérenne du verbe 
écrit et parlé, cette prolifération d’images et de technologies visuelles 
1. Pour une introduction à l’histoire culturelle, voir Ory 2007, Poirrier 2010, Burke 2008.
2. Sur la perception olfactive, voir Corbin 1982 ; pour le regard, voir par exemple les travaux 
pionniers de Baxandall 1972, Alpers 1983, Crary 1992.
3. Pour une introduction à l’étude des cultures visuelles, voir Mirzoeff 1995, Sturken et Cartwright 
2001, Kromm et Benforado Bakewell 2010.

	
le siècle de l’atome en images 	
169
fait qu’une étude des représentations culturelles (au sens des imaginaires 
collectifs) du xxe siècle peut difficilement être réalisée sans prendre en 
considération ces dernières et les cultures visuelles qui s’y rattachent. 
Nous proposons quelques points d’entrée de cette histoire, suivant les 
fils mêlés de l’histoire de l’atome et du nucléaire d’une part, de l’histoire 
des technologies et médias visuels d’autre part, qui font toutes deux la 
spécificité du xxe siècle.
Visualisations scientifiques de l’atome
L’atome est un objet en apparence paradoxal pour une histoire des 
représentations, puisqu’il est invisible par définition – et sa réalité même 
fit longtemps débat. Ce n’est qu’au début du xxe siècle qu’il devient acces-
sible à l’expérience par l’intermédiaire de nouveaux instruments liés à un 
bouleversement des sciences à la fois pratique et théorique. Ces change-
ments profonds s’appliquent à la matière comme au vivant, relèvent de la 
physique comme de la chimie et de la biologie, et plus tard des sciences 
des matériaux et des nanosciences 1. Les images scientifiques de l’atome 
reflètent et permettent de contextualiser ces développements. Aussi  
bien lorsqu’elles créent des mondes visuels aussi nouveaux que les théories 
et les instruments qui les convoquent – participant ainsi d’une modernité 
revendiquée ; mais aussi à l’inverse, lorsque ces images minorent la 
nouveauté des théories et des instruments qui les produisent en convo-
quant des traditions iconographiques anciennes. L’image scientifique, 
comme toute image, n’est jamais un simple miroir de la réalité.
Une iconographie pérenne : l’atome-sphère
L’image de l’atome à la fois la plus ancienne et la plus répandue le repré-
sente sous la forme d’une sphère, dans les modèles de molécules par 
exemple 2 (voir figure 1, p. 170). Ces modèles, des solides en trois dimen-
sions ou, plus récemment, sous la forme de simulations numériques, 
sont des outils heuristiques et pédagogiques permettant de penser les 
transformations chimiques. Ils matérialisent des pratiques de manipu-
lation virtuelle qui font intervenir l’imagination visuelle comme moyen 
de conceptualiser les mécanismes moléculaires inaccessibles à partir de 
1. Sur l’impact de ces techniques en chimie, voir Morris 2002, Reinhardt 2006 ; en biologie, 
Rasmussen 1997, Chadarevian et Kamminga 1998.
2. Lüthy 2003.

170	
charlotte bigg
données fournies par des expériences 1. L’origine de ces modèles est à 
situer dans la communication et l’enseignement des sciences aussi bien 
que dans la recherche : l’un des premiers modèles du genre est créé en 
1865 par le chimiste Wilhelm August von Hofmann à l’aide de bâtons et 
de balles de croquet dans le but d’illustrer une conférence publique à la 
Royal Institution de Londres 2.
 
Figure 1 : Dessins de modèles moléculaires (ici d’acide acétique)
Ces dessins réalisés par Linus Pauling en collaboration avec l’artiste Roger Hayward 
en 1964 témoignent du grand intérêt de Pauling pour la dimension esthétique de la 
visualisation scientifique. En haut, un exemple de modèle CPK inventé par Pauling 
avec Robert Corey et Walter Koltun, qui permet de représenter l’espace occupé par 
chaque atome, information obtenue grâce à des mesures cristallographiques. Ce 
modèle est une variante du modèle classique (en bas) de type Hofmann. Les conven-
tions établies par Hofmann perdurent encore aujourd’hui, comme celle de dénoter les 
éléments par des couleurs, par exemple l’oxygène par le rouge. Ces modèles peuvent 
être assemblés à partir d’éléments standards vendus en kit par des entreprises spécia-
lisées ou manipulés virtuellement à l’aide de logiciels spécifiques.
1. Rocke 2010.
2. Meinel 2004.

	
le siècle de l’atome en images 	
171
Dans ces modèles, l’atome est représenté de manière purement 
conventionnelle. Il est alors surprenant de constater la similarité de ces 
représentations avec les visualisations expérimentales des atomes qui se 
développent au xxe siècle. Dans les années 1980, la microscopie en champ 
proche stimule le développement des nanosciences et des nanotechno-
logies (voir figure 2, p. 172). Des instruments tels que le microscope à effet 
tunnel et le microscope à force atomique permettent, dans des condi-
tions précisément définies, de visualiser, voire de manipuler la surface 
de métaux à l’échelle nanométrique. Alors que les données numériques 
produites par ces instruments sont susceptibles d’être mises en images de 
multiples façons, c’est finalement un mode de représentation conforme 
à la convention iconographique de l’atome-sphère qui est adopté 1.
On retrouve dans ces images et les commentaires qu’elles suscitent 
l’espoir, déjà exprimé au début du xxe siècle, du « saint Graal de la résolution 
atomique », de pouvoir améliorer la résolution des microscopes optiques 
indéfiniment, jusqu’à pouvoir enfin voir l’atome 2. Cela est physiquement 
impossible, et les microscopes en champ proche opèrent selon des 
principes entièrement différents de ceux qui régissent les microscopes 
optiques. Il reste toutefois quelque chose de cette idée dans l’usage du 
terme « microscope » pour décrire ces instruments. Les représentations 
populaires des nanosciences suggèrent d’ailleurs une commensurabilité 
trompeuse entre le monde atomique et le nôtre, qui serait le pendant 
d’un progrès continu des technologies microscopiques, nous permettant 
en quelque sorte de « zoomer » dans des dimensions de plus en plus 
éloignées, à la manière du film célèbre réalisé en 1968 par Charles et Ray 
Eames, Powers of Ten 3.
On peut d’ailleurs déceler dans certaines nanotopographies une 
adhérence aux conventions de représentations macroscopiques, y compris 
photographiques, lorsque par exemple on croit percevoir des ombres 
portées par les monticules qui figurent les atomes individuels. Mais 
ici les techniques photographiques n’entrent pas en jeu, et ce sont des 
logiciels qui sont employés pour transformer les données numériques 
en supports visuels 4. On réalise avec cet exemple que l’invisible ne l’est 
jamais complètement : à l’instant de sa visualisation il est déjà en partie 
défini non seulement par ses caractéristiques physiques, les théories qui 
1. Hennig 2011 (p. 221-276). Sur les questions que soulève la visualisation dans les nanosciences, 
voir également Baird et al. 2004, Mody et Lynch 2010.
2. Hessenbruch 2004 (p. 137).
3. Ce film et une version ultérieure réalisée en 1977 s’inspirent de Boeke 1957. Voir Pratschke 
2009. Sur la place de la science-fiction dans les discours autour des nanosciences contemporaines, 
voir Milburn 2008.
4. Hennig 2001 (p. 221-276).

172	
charlotte bigg
Figure 2 : Une séquence d’images STM prises pendant la construction  
d’un agencement d’atomes de xénon sur une surface de nickel (110)
L’une des images les plus célèbres des nanosciences émergentes, publiée dans 
Nature en 1990 par les pionniers du microscope à effet tunnel Don Eigler et Erhard 
Schweizer. Parmi les différents modes de visualisation des données fournies par l’ins-
trument, c’est finalement une convention similiphotographique qui a été adoptée, 
comme l’a suggéré Jochen Hennig. On peut interpréter ce choix comme un hommage 
à l’importance historique de la technique photographique dans la mise en évidence 
d’entités invisibles à l’œil nu dans les sciences physiques du xxe siècle, alors que la 
visualisation des atomes individuels sous forme de petites sphères fait écho à une 
tradition iconographique plus ancienne encore. La force de cette succession d’images 
réside dans ce témoignage visuel et expérimental de la possibilité de manipuler la 
matière à l’échelle atomique. Inscrivant en atomes le logo du géant informatique 
IBM, cette image est aussi révélatrice du rôle important des entreprises privées 
dans la recherche scientifique et technologique et sa communication au public : 
c’est également avec le concours d’IBM que Charles et Ray Eames réalisent leur film 
pédagogique Powers of Ten.
le sous-tendent et les contraintes techniques de sa mise en images mais 
aussi par les attentes qu’il suscite, les habitudes visuelles et les imagi-
naires qu’il convoque, même dans les contextes les plus scientifiques 1. 
1. Geimer 2010 (p. 21).

	
le siècle de l’atome en images 	
173
Parmi ces imaginaires, il faut inclure l’image de l’atome-sphère, dont la 
pérennité suggère l’existence d’une logique iconographique indépen-
dante de l’évolution historique des conceptions de ce qui fait la matière 
et de ce que recouvre le terme « atome ».
Les empreintes photographiques du monde subatomique
La photographie joue un rôle essentiel dans l’étude et la visualisation du 
monde subatomique tout au long du xxe siècle. En parallèle, le dévelop-
pement de techniques photomécaniques de reproduction permet une 
diffusion sans précédent de ces images dans les imprimés, faisant de 
découvertes scientifiques comme celle des rayons X de véritables phéno-
mènes médiatiques, dont les répercussions vont bien au-delà du monde 
scientifique.
Le tournant du xxe siècle révèle en effet l’existence d’un monde nouveau 
peuplé de particules et de rayons rendus perceptibles grâce à des dispo-
sitifs fondés sur de nouvelles théories électromagnétiques de la matière : 
compteurs Geiger ou écrans luminescents dont les cliquetis et scintil-
lements permettent de comptabiliser le passage de particules et rayons 
ionisants, chambres à brouillard dont l’expansion rapide exhibe la trajec-
toire de particules subatomiques. Mais il ne faut pas oublier les médiations 
opérées par les techniques photographiques, souvent réduites au rôle 
d’enregistrement des phénomènes révélés par ces instruments selon une 
conception mimétique de la photographie considérée comme véritable 
« rétine du savant 1 ». Les émulsions photographiques constituent en effet 
un support majeur des visualisations expérimentales dans la physique 
atomique puis nucléaire au xxe siècle (voir figure 3, p. 174).
Les photographies sont à la fois le révélateur, le moyen d’étude et la 
preuve tangible des nouveaux rayonnements, comme le suggère la célèbre 
découverte de la radioactivité par le moyen de la photographie par Henri 
Becquerel en 1896 2. Elles sont à la fois des outils de représentation et 
de recherche : la chimie des émulsions photographiques est particuliè-
rement délicate à maîtriser et reste un sujet d’étude jusque tard dans le 
xxe siècle dans le domaine de la physique des particules 3. Ces photo-
graphies, dont rapidement l’apparence s’uniformise, forment un nouvel 
ensemble d’images expérimentales qui à leur tour contribuent à l’élabo-
ration de nouveaux modèles de l’atome ; à commencer par l’atome de 
1. L’expression est de l’astrophysicien Jules Janssen. Voir Gunthert 2000. Sur l’histoire du 
discours sur la photographie au xixe siècle, voir Brunet 2000.
2. Voir Wilder 2009.
3. Galison 1997 (chap. 2 et 3).

174	
charlotte bigg
Bohr « planétaire », autre icône visuelle qui continue de circuler malgré 
sa remise en cause par les physiciens dès les années 1920, par exemple 
sous la forme du logo de l’Agence internationale de l’énergie atomique 1.
Ces photographies sont également très largement diffusées dans les 
journaux savants et populaires : l’impact médiatique de la découverte 
des rayons X n’est comparable au xxe siècle qu’à l’annonce de la bombe 
atomique en 1945 2. Cette diffusion est rendue possible par de nouvelles 
1. Schirrmacher 2010.
2. Badash 1979 (p. 9).
Figure 3 : Photographie de la diffraction des rayons X  
réalisée par Max von Laue
Les émulsions photographiques constituent un support majeur pour la détection et 
l’étude des nouvelles radiations et particules découvertes au tournant du xxe siècle. 
En 1912, le physicien Max von Laue utilise la photographie comme instrument pour 
élucider la nature des rayons X, à savoir s’ils sont de nature corpusculaire ou ondula-
toire. Les images produites constituent une trace de ces expériences, mais aussi une 
preuve visuelle que la diffraction des rayons X est possible. Ces originaux photogra-
phiques sont montrés par le physicien von Laue lors du Congrès Solvay de 1913 puis 
collés dans le rapport imprimé du congrès, comme autant de preuves garantissant 
sa priorité dans cette découverte. Ce type de photographie prendra par la suite une 
tout autre fonction lorsque la diffraction des rayons X sera utilisée non plus pour les 
examiner mais pour étudier la structure des cristaux par leur moyen. Les photographies 
sont alors mesurées pour obtenir des structures en trois dimensions. C’est à partir de 
ces mesures que l’on peut construire des modèles tels que ceux de Pauling (Figure 1).

	
le siècle de l’atome en images 	
175
techniques de reproduction photomécanique des images qui se répandent 
à partir des années 1890. Moins coûteuses et plus performantes que la 
gravure manuelle, elles facilitent la publication d’images de bonne qualité 
et de grande taille dans la presse scientifique et non scientifique. Elles 
contribuent en retour à renforcer auprès du public la position d’autorité 
de l’image photographique et l’idéal d’une « objectivité mécanique », 
pendant instrumental de l’objectivité des faits scientifiques 1.
Mais les clichés de la physique atomique deviennent, à travers leur 
reproduction et circulation infinies, des clichés, au sens de lieux communs 
visuels. Leur esthétique caractéristique est appropriée dans de nombreux 
domaines, notamment la photographie d’art moderniste, pendant que 
fleurissent dans les journaux les publicités thématisant la phosphorescence 
des matières radioactives ou leurs propriétés prétendument guérissantes. 
On a rapproché ces images du monde (sub)atomique d’une préoccu-
pation plus générale, autour de 1900, de la visualisation de l’invisible, 
une nouvelle perception de la réalité qui infuse les sciences, les arts, 
la littérature et le cinéma naissant 2. La photographie, imaginée dès ses 
origines comme un moyen de révéler des objets invisibles, deviendra son 
« médium et son discours paradigmatique 3 ».
Plus pragmatiquement, la photographie continuera tout au long du siècle 
à être un moyen privilégié de visualisation des radiations invisibles à l’œil 
nu : avant les dosimètres électroniques, c’est l’émulsion photographique qui, 
le plus souvent, permet de mesurer la dose de rayons ionisants à laquelle 
une personne (chercheur, médecin ou patient soumis à une radiothérapie) 
a été exposée. La sensibilité des émulsions photographiques à la radioac-
tivité entre aussi en jeu dans la détection d’émissions radioactives moins 
contrôlées : c’est ainsi qu’en 1945 les employés d’une usine d’Eastman 
Kodak Company ont vent en avant-première du premier essai nucléaire 
réalisé dans le plus grand secret : les films photographiques produits dans 
cette usine avaient été voilés par leur emballage provenant d’une usine 
de l’Indiana, contaminé par les essais réalisés dans le Nouveau-Mexique, 
à une distance de 1 600 kilomètres. En 1979, pour évaluer la gravité d’un 
incident à la centrale nucléaire de Three Mile Island, les experts réqui-
sitionnent les provisions de films Kodacolor 400 dans les magasins à 
10 kilomètres à la ronde pour les analyser 4. Les premières photographies 
de l’accident catastrophique de la centrale nucléaire de Tchernobyl du 
26 avril 1986 sont d’autant plus poignantes que la radioactivité intense 
1. Hüppauf et Weingart 2008 (p. 11), Daston et Galison 2007.
2. Pour les sciences, voir Sibum 2008. Pour les arts, voir par exemple Henderson 1988.
3. Scholz et Griem 2010 (p. 8).
4. Webb 1949, Shuping 1981 ; voir Bigg et Hennig 2009.

176	
charlotte bigg
y est visible, s’inscrivant dans le film photographique, déformant les 
couleurs ou même brûlant la pellicule, au moment même où elle irradie 
les corps des « liquidateurs » que l’on voit sur les images.
Montrer (ou pas) le nucléaire : affaires de politique
L’ère atomique et l’âge des mass media
Avec l’essor de la physique nucléaire au milieu du siècle et le dévelop-
pement des applications de la fission atomique, on assiste à l’émergence 
de nouvelles iconographies, d’une tout autre nature que celles évoquées 
jusqu’à présent, non pas centrées sur la visualisation de l’atome comme 
objet scientifique mais sur les technologies nucléaires. Ces dernières 
témoignent d’abord de l’émergence avec le projet Manhattan de la big 
science, la recherche technoscientifique à grande échelle, largement 
financée sur fonds militaires. Mais elles thématisent aussi le fait que la 
bombe atomique et les réacteurs nucléaires sont l’objet de nouveaux 
enjeux aux échelles nationales et internationales dans les domaines inter-
dépendants militaires, géostratégiques, industriels, énergétiques puis 
environnementaux.
Ces images résultent également de nouvelles configurations média-
tiques qui se mettent en place dans la seconde moitié du xxe siècle.  
Au développement continu depuis le début du siècle des techniques de 
fabrication et d’impression des photographies, qui génèrent une augmen-
tation constante du nombre, de la qualité (y compris la couleur) et de la 
surface occupée par les images dans les supports imprimés ; au dévelop-
pement du cinéma s’ajoutent dans les décennies suivantes les nouvelles 
technologies de communication que sont la radio, la télévision et la 
vidéo, qui relèvent d’ailleurs souvent d’arrangements technoscientifiques 
impliqués dans la recherche militaire comme, aux États-Unis, la Radio 
Corporation of America. Ensemble, ces technologies, bientôt appelées 
mass media, accélèrent la circulation des images, élargissent leur public 
jusqu’à atteindre parfois une dimension globale.
La notion d’ère atomique, stricto sensu, définit le nouvel ordre mondial 
inauguré par le premier essai atomique dans le désert du Nouveau-
Mexique en 1945. Mais les retombées de la bombe atomique ne sont pas 
seulement d’ordre militaire et politique. Elles préoccupent l’ensemble de 
la société. En particulier (mais pas seulement) aux États-Unis d’Amé-
rique, l’atomic age est indissociable d’une production journalistique, 
littéraire, audiovisuelle, muséologique très riche, thématisant les espoirs 

	
le siècle de l’atome en images 	
177
et les peurs associés à l’exploitation de la technoscience nucléaire, allant 
des campagnes officielles aux expressions populaires, des films de propa-
gande comme Duck and Cover au Docteur Folamour de Stanley Kubrick, 
de la science-fiction au marché que représente la vente d’abris antiato-
miques privatifs et aux manifestations antinucléaires 1.
Les images qui y sont associées sont perçues comme résumant l’époque : 
c’est la photographie de Miss Atomic Bomb (voir p. 166), couronnée à 
Las Vegas en 1957, que la Smithsonian Institution désigne comme l’image 
iconique des années 1950 – pour les États-Unis, s’entend. Alors que, 
pour des raisons évidentes, au Japon ou en Allemagne les connotations 
de la bombe atomique sont largement négatives, pour les Américains 
le champignon atomique qui orne le maillot de la danseuse et bientôt 
jusqu’aux timbres et paquets de chewing-gum incarne la victoire de la 
Seconde Guerre mondiale et la prospérité économique qui s’ensuit. La 
proximité géographique entre Las Vegas et les sites de tests nucléaires du 
Nevada favorise ce mélange des styles, lorsque pour lancer le tourisme 
dans cette ville on y fait la publicité à la fois de ses casinos et du spectacle 
sublime des détonations atomiques 2.
Depuis Las Vegas, on peut assister au spectacle des tests nucléaires 
effectués à partir de 1951 à moins de 100 kilomètres, au Nevada Test Site. 
La bombe atomique est intégrée à la culture populaire qui se développe 
dans cette ville de divertissement après la guerre, avec notamment l’orga-
nisation de concours de « Miss Bombe atomique ». Ainsi que l’a souligné 
Gerhard Paul, cette photographie doit aussi son succès à sa compo-
sition : vue en contre-plongée, la miss paraît aussi monumentale qu’un 
champignon atomique, dont la forme iconique est reproduite en coton, 
ajustée à la silhouette féminine et se confondant avec les nuages naturels. 
À travers ce type d’image, la bombe est dotée de connotations sexuelles. 
C’est à cette époque qu’apparaissent l’expression « bombe sexuelle » ou le 
bikini, nommé en référence explicite à l’atoll des îles Marshall où l’armée 
américaine avait effectué les tests nucléaires « Operation Crossroads ».
La bombe est devenue, selon l’un des premiers historiens de la culture 
nucléaire, « une catégorie de l’Être, comme l’espace et le temps qui, 
selon Kant, font partie prenante de la structure même de notre esprit, 
déterminant la forme et la signification de toutes nos perceptions 3 ». 
La possibilité de l’annihilation complète de l’homme par l’homme 
ou, a contrario, la promesse d’une énergie infinie et pratiquement 
1. Sur la culture nucléaire aux États-Unis, voir parmi beaucoup d’autres ouvrages Weart 2012. 
Pour une perspective transnationale, voir Van Lente 2012, Kargon et Low 2003.
2. Hales 1991, Bexte 2005.
3. Boyer 1991 (p. xix-xx).

178	
charlotte bigg
gratuite bouleversent le rapport du citoyen à la nation et des sociétés  
à la nature.
Dans la cristallisation de ces cultures nucléaires, la place des images 
est centrale, d’abord à travers la question de leur visibilité ; moins comme 
question technique et scientifique, comme on l’a vu plus haut, que comme 
enjeu politique : la production et le contrôle des images associées au 
nucléaire sont au cœur des politiques publiques destinées d’abord à 
garantir le secret militaire mais aussi à façonner le citoyen moderne.
Figure 4 : Anthony J. Ouellette,  
test atomique avec public (Nevada, 1955)
Une quantité très importante de photographies et de films est réalisée dans le cadre 
de la documentation officielle des essais atomiques aux États-Unis. Ces photogra-
phies, qui mettent en scène la présence de témoins de l’événement, soulignent le fait 
qu’il s’agit ici aussi d’un spectacle, même si celui-ci est par moments physiquement 
insoutenable (voir également l’image de couverture). Si le champignon est devenu le 
symbole de la bombe atomique dans le contexte américain, au Japon c’est le pika-don 
qui la résume : l’éclair qui le précède, d’une intensité lumineuse si intense qu’il ne 
peut être représenté que métaphoriquement. Il faudra d’ailleurs un important travail 
de recherche pour créer des émulsions photographiques spécifiques permettant de 
photographier les détonations sans qu’elles soient irrémédiablement surexposées. Le 
terrible impact sur la santé des témoins des tests nucléaires, aux États-Unis comme 
dans le Pacifique et ailleurs, deviendra rapidement évident, malgré la censure.

	
le siècle de l’atome en images 	
179
 
Censure et propagande
126 essais nucléaires sont réalisés à l’air libre entre 1945 et 1963 aux 
États-Unis. Ils donnent lieu à une production photographique et cinéma-
tographique officielle immense (voir figure 4, p. 178), qui suggère que 
l’appétit visuel pour le spectacle grandiose est partagé par l’adminis-
tration. Un studio cinématographique secret est établi à Hollywood, le 
Lookout Mountain Studio, censé documenter les explosions dans un 
but scientifique, pour l’étude des impacts notamment 1. Ce programme 
donne lieu à de nombreuses innovations techniques, des caméras ultra-
rapides électroniques aux émulsions nucléaires capables de capturer 
des phénomènes d’une intensité lumineuse, y compris radioactive, et 
à des vitesses jusqu’alors imperceptibles. Il contribue à nourrir l’imagi-
nation populaire, à ancrer par exemple le nouveau stéréotype visuel du 
champignon atomique à travers la nouvelle presse illustrée (Life, Time…), 
mais aussi à créer de nouvelles images étranges, comme celles que produit 
Harold Edgerton à l’aide de sa caméra rapatronique 2.
En fin de compte, cependant, très peu de ces images seront vues, 
disparaissant pour la plupart dans des archives classifiées 3. Pendant la 
Seconde Guerre mondiale et par la suite, le programme américain de 
recherche et de développement de la bombe atomique est bien entendu 
soumis au secret militaire ; même si, curieusement, de nombreux dispo-
sitifs entrant en jeu dans la production de la bombe sont brevetés et ainsi 
rendus publics 4. Contrastant avec la présence médiatique forte et voulue 
des images des essais atomiques américains, les premières photogra-
phies de victimes d’explosions nucléaires ne sont publiées qu’en 1952. 
Les photographies des nuages atomiques réalisées par l’armée américaine 
au-dessus de Nagasaki et d’Hiroshima sont immédiatement diffusées, 
alors que les images de l’effet des explosions sur les villes et les popula-
tions sont censurées jusqu’en 1970 (1980 pour les films en couleurs) 5. 
Cette production iconographique et médiatique importante contraste 
avec la situation au Japon, où l’usage de la photographie est strictement 
1. Kuran 2006.
2. Elkins 2003.
3. Voir Kuran 2006 et son film The Atomic Filmmakers : Hollywood’s Secret Film Studio (1998). 
Ces photographies sont d’ailleurs partagées par leurs auteurs, ainsi que de nombreux témoins 
et participants à ces essais, aujourd’hui souvent malades. Voir Gallagher et Schneider 1993, 
qui ont retrouvé des survivants et les ont photographiés.
4. Wellerstein 2008.
5. Paul 2006.

180	
charlotte bigg
contrôlé dès 1941. On ne connaît que cinq photographies prises au sol 
le jour de l’explosion à Hiroshima et aucune de Nagasaki 1.
En décembre 1953, le gouvernement américain dirigé par Dwight 
Eisenhower lance le programme « Atome pour la paix » pour promouvoir 
les applications pacifiques de la recherche nucléaire dans les domaines 
médicaux, industriels et énergétiques. Les technologies nucléaires sont 
mises en avant par le gouvernement américain comme autant d’instru-
ments de politique étrangère dans le nouveau contexte international de 
la guerre froide et de la décolonisation 2. Ce programme comprend une 
série d’initiatives médiatiques, avec des expositions réalisées dans un 
grand nombre de pays. En 1955, à l’occasion d’une conférence interna-
tionale sur les usages pacifiques du nucléaire, un réacteur nucléaire est 
transporté en avion d’Oak Ridge à Genève, où il est remonté à proximité 
du siège des Nations unies. Invitant les visiteurs à activer eux-mêmes 
le réacteur à l’aide d’une manette, cette mise en scène suggère l’inno-
cuité de la technologie, tandis que les modalités techniques du dispositif 
sont expliquées à l’aide d’affiches, de conférences, de cours, de modèles. 
Cette exposition, comme tout l’appareil de propagande d’« Atome pour 
la paix », peut être comprise comme un moyen, principalement visuel, 
d’éduquer les peuples ou du moins leurs représentants au « désir pour 
l’atome pacifique », solution technique promettant d’accéder au progrès 
matériel et à l’American way of life 3.
Aux États-Unis mêmes, l’affirmation du statut de superpuissance 
nucléaire s’accompagne d’une stratégie de défense civile qui vise à une 
mobilisation permanente de la population, une éducation psychologique 
du citoyen qui doit permettre à la nation de faire face sans paniquer à 
la menace omniprésente de la guerre nucléaire. La presse, la télévision, 
la radio, le cinéma, par l’évocation récurrente, voire la simulation d’une 
attaque nucléaire, avec la profusion par exemple de « films catastrophes », 
servent à créer une forme particulière de peur, tout en normalisant le 
danger nucléaire. Les images du nucléaire participent ainsi crucialement 
à la reconfiguration des rapports qu’entretiennent le secret, la sécurité, 
la technoscience et l’identité nationale dans les décennies qui suivent la 
Seconde Guerre mondiale, aux États-Unis et ailleurs 4.
1. Lucken 2008.
2. Krige 2008.
3. Krige 2010.
4. Masco 2006, Masco 2008, Weart 2012. Pour un traitement de la culture nucléaire et l’identité 
nationale au Royaume-Uni, voir notamment Welsh 2003 ; en France, Hecht 1998 ; en Afrique, 
Hecht 2012.

	
le siècle de l’atome en images 	
181
 
Conclusion : des cultures nucléaires  
aux nouvelles représentations de la Terre
À la fin des années 1960 apparaît une nouvelle forme de militantisme 
environnemental qui doit beaucoup aux mouvements antinucléaires. Au 
Royaume-Uni, la Campaign for Nuclear Disarmement est le catalyseur 
de l’émergence d’une conception d’un environnement planétaire fragile 
et interdépendant, menacé d’abord par la guerre nucléaire, puis plus 
généralement par les pollutions d’ordres divers, dont les fuites radioac-
tives des réacteurs nucléaires qui essaiment à cette époque. C’est par ce 
biais principalement qu’une conception « écocentrique » de la nature se 
répand. Cette dernière est perçue désormais non pas comme un domaine 
distinct de celui de l’homme mais comme un environnement dont l’espèce 
humaine est pleinement partie prenante et que la dégradation affecterait 
immanquablement 1.
Ces nouvelles représentations entrent aussi en résonance avec une 
reconceptualisation scientifique de la planète, retombée des grands 
programmes de recherche scientifiques, militaires et industriels lancés 
par les agences américaines pendant la guerre froide. Les sciences du 
système-Terre sont un produit de la mobilisation ininterrompue après  
la guerre des institutions de géographie, géologie, géophysique, des  
sciences de l’atmosphère ou encore de topographie et de cartographie, 
combinée avec le développement de technologies de géolocalisation et 
de détection à l’échelle planétaire, notamment par le moyen d’avions 
et de satellites 2. Les besoins militaires d’une surveillance globale du 
monde contribuent à une appréhension scientifique de la Terre comme 
un environnement fini et dont l’interaction des parties peut être étudiée 
à une échelle globale. Indissociable du changement de perspective 
induit par la conquête de l’espace à la même époque, cette nouvelle 
conception est présente dans les médias au travers d’un ensemble de 
productions discursives et visuelles marquantes, qui incluent la métaphore 
du vaisseau-Terre, les photographies de la Terre prise depuis la Lune 
par les astronautes ou, plus tard, l’image du « trou » dans la couche  
d’ozone 3.
Dans ces nouvelles représentations de la planète qui émergent dans 
les dernières décennies du xxe siècle, l’atome ou le nucléaire ne sont 
1. Burkett 2012. Pour les États-Unis, sur le lien entre l’Atomic Energy Commission, l’écologie 
scientifique et l’environnementalisme, voir par exemple Rothschild 2013.
2. Cloud 2001, Gordin 2009 (p. 285-307).
3. Grevsmühl 2014.

182	
charlotte bigg
donc pas explicites, mais ils sont indissociables des cultures nucléaires 
de l’après-guerre.
Références bibliographiques
Alpers Svetlana, 1983, The Art of Describing : Dutch Art in the Seventeenth Century, 
Chicago (IL), University of Chicago Press.
Badash Lawrence, 1979, Radioactivity in America : Growth and Decay of a Science, 
Baltimore (MD), Johns Hopkins University Press.
Baird Davis, Nordmann Alfred et Schummer Joachim (dir.), 2004, Discovering the 
Nanoscale, Amsterdam, IOS Press.
Baxandall Michael, 1972, Painting and Experience in Fifteenth-Century Italy : 
A Primer in the Social History of Pictorial Style, Oxford, Oxford University Press.
Bensaude-Vincent Bernadette et Simon Jonathan, 2008, Chemistry : The Impure 
Science, Londres, Imperial College Press.
Bexte Peter, 2005, « Wolken über Las Vegas », Archiv für Mediengeschichte, n° 5, 
p. 131-137.
Bigg Charlotte, 2008, « Evident Atoms : Visuality in Jean Perrin’s Brownian Motion 
Research », Studies in the History and Philosophy of Science, vol. 39, p. 312-322.
Bigg Charlotte et Hennig Jochen (dir.), 2009, « Spuren des Unsichtbaren. Fotografie 
macht Radioaktivität sichtbar », in Charlotte Bigg et Jochen Hennig, Atombilder. 
Ikonografie des Atoms in Wissenschaft und Öffentlichkeit des 20.  Jahrhunderts, 
Göttingen, Wallstein, p. 31-36.
Boeke Kees, 1957, Cosmic View : The Universe in 40 Jumps, New York, J. Day.
Boyer Paul S., 1985, By the Bomb’s Early Light : American Thought and Culture at the 
Dawn of the Atomic Age, New York, Pantheon.
Brunet François, 2000, La Naissance de l’idée de photographie, Paris, PUF.
Buchwald Jed et Warwick Andrew (dir.), 2004, Histories of the Electron, Cambridge 
(MA), MIT Press.
Burke Peter, 2008, What Is Cultural History ?, Cambridge, Polity Press.
Burkett Jodi, 2012, « The Campaign for Nuclear Disarmament and Changing 
Attitudes towards the Earth in the Nuclear Age », The British Journal for the History 
of Science, vol. 45, no 4, p. 625-639.
Chadarevian Soraya de et Kamminga Harmke (dir.), 1998, Molecularizing Biology 
and Medicine : New Practices and Alliances (1920s to 1970s), Amsterdam, Harwood 
Academic Publishers.
Chang Hasok, 2012, Is Water H2O ? Evidence, Pluralism and Realism, Dordrecht, 
Springer.
Cloud John, 2001, « Imaging the World in a Barrel : CORONA and the Clandestine 
Convergence of the Earth Sciences », Social Studies of Science, vol. 31, p. 231-251.
Corbin Alain, 1982, Le Miasme et la jonquille. L’odorat et l’imaginaire social aux 
xviiie-xixe siècles, Paris, Aubier-Montaigne.
Crary Jonathan, 1992, Techniques of the Observer : On Vision and Modernity in the 
Nineteenth Century, Cambridge (MA), MIT Press.
Daston Lorraine et Galison Peter, 2007, Objectivité, Dijon, Les Presses du Réel.
Elkins James,  2003, After and Before : Documenting the A-Bomb,  New York, PPP Editions.
Galison Peter, 1997, Image and Logic : A Material Culture of Microphysics, Chicago 
(IL), University of Chicago Press.

	
le siècle de l’atome en images 	
183
Gallagher Carole et Schneider Keith, 1993, American Ground Zero : The Secret 
Nuclear War, Cambridge (MA), MIT Press.
Geimer Peter, 2010, « Sichtbar / unsichtbar. Szenen einer Zweiteilung », in Susanne 
Scholz et Julika Griem (dir.), Medialisierung des Unsichtbaren um 1900, Munich, 
Fink, p. 17-30.
Gordin Michael, 2009, Red Cloud at Dawn : Truman, Stalin and the End of the 
Atomic Monopoly, New York, Farrar, Straus & Giroux.
Grevsmühl Sebastian, 2014, L’Invention de l’environnement global, Paris, Seuil.
Gunthert André, 2000, « La rétine du savant. La fonction heuristique de la photo-
graphie », Études photographiques, no 7, mis en ligne le 18 novembre 2002, < http://
etudesphotographiques.revues.org/205 > (consulté le 3 juillet 2013).
Hales Peter B., 1991, « The Atomic Sublime », American Studies, vol. 32, p. 5-31.
Hecht Gabrielle, 1998, The Radiance of France : Nuclear Power and National Identity 
after World War II, Cambridge (MA), MIT Press.
–	 2012, Being Nuclear : Africans and the Global Uranium Trade, Cambridge (MA), 
MIT Press.
Henderson Linda Dalrymple, 1988, « X-Rays and the Quest for Invisible Reality  
in the Art of Kupka, Duchamp and the Cubists », Art Journal, vol.  47, no 4, 
p. 323-340.
Hennig Jochen, 2011, Bildpraxis. Visuelle Strategien der frühen Nanotechnologie, 
Bielefeld, Transcript.
Hessenbruch Arne, 2004, « Nanotechnology and the Negociation of Novelty », 
in Davis Baird, Alfred Nordmann et Joachim Schummer (dir.), Discovering the 
Nanoscale, Amsterdam, IOS Press, p. 135-144.
Hogg Jonathan et Laucht Christoph (dir.), 2012, « British Nuclear Culture », dossier 
thématique de The British Journal for the History of Science, vol. 45, no 4, p. 479-719.
Hüppauf Bernd et Weingart Peter (dir.), 2008, Science Images and Popular Images 
of the Sciences, New York, Routledge.
Kargon Robert et Low Morris (dir.), 2003, « Visions of the Atomic Age : Towards a 
Comparative Perspective », dossier thématique de History and Technology, vol. 19, 
no 3, p. 175-298.
Krige John, 2008, « The Peaceful Atom as Political Weapon : Euratom and American 
Foreign Policy in the Late 1950s », Historical Studies in the Natural Sciences, 
vol. 38, no 1, p. 5-44.
–	 2010, « Techno-Utopian Dreams, Techno-Political Realities », in Michael D. Gordin, 
Helen Tilley et Gyan Prakash (dir.), Utopia / Dystopia : Conditions of Historical 
Possibility, Princeton (NJ), Princeton University Press, p. 151-175.
Kromm Jane et Benforado Bakewell Susan (dir.), 2010, A  History of Visual 
Culture : Western Civilization from the 18th to the 21st Century, Oxford, Berg.
Kuran Peter, 2006, How to Photograph an Atomic Bomb, Santa Clarita (CA), VCE.
Lucken Michael, 2008, 1945-Hiroshima. Les images sources, Paris, Hermann.
Lüthy Christoph, 2003, « The Invention of Atomist Iconography », in  Wolfgang 
Lefèvre, Jürgen Renn et Urs Schoepflin (dir.), The Power of Images in Early 
Modern Science, Bâle, Birkhäuser, p. 117-138.
Masco Joseph, 2006, The Nuclear Borderlands : The Manhattan Projet in Post-Cold 
War New Mexico, Princeton (NJ), Princeton University Press.
–	 2008, « “Survival Is Your Business” : Engineering Ruins and Affect in Nuclear 
America », Cultural Anthropology, vol. 23, p. 361-398.
Meinel Christoph, 2004, « Molecules and Croquet Balls », in Soraya de Chadarevian 

184	
charlotte bigg
et Nick Hopwood (dir.), Models : The Third Dimension of Science, Palo Alto (CA), 
Stanford University Press, p. 242-276.
Milburn Colin, 2008, Nanovision : Engineering the Future, Durham (NC), Duke 
University Press.
Mirzoeff Nicholas (dir.), 1995, The Visual Culture Reader, New York et Londres, 
Routledge.
Mody Cyrus et Lynch Michael, 2010, « Test Objects and Other Epistemic Things : 
A History of a Nanoscale Object », The British Journal for the History of Science, 
vol. 43, no 3, p. 423-458.
Morris Peter J.T. (dir.), 2002, From Classical to Modern Chemistry : The Instrumental 
Revolution, Cambridge, Royal Society of Chemistry.
Nye Mary Jo, 1993, From Chemical Philosophy to Theoretical Chemistry, Berkeley 
(CA), University of California Press.
Ory Pascal, 2007, L’Histoire culturelle, Paris, PUF.
Paul Gerhardt, 2006, « “Mushroom Clouds”. Entstehung, Struktur und Funktion einer 
Medienikone des 20.  Jahrhunderts im interkulturellen Vergleich », in  Gerhardt 
Paul (dir.), Visual History, Göttingen, Vandenhoeck & Ruprecht.
Poirrier Philippe, 2010, Les Enjeux de l’histoire culturelle, Paris, Seuil.
Pratschke Margarete, 2009, « Charles und Ray Eames’ Powers of Ten. Die künst­
lerische Bildfindung des Atoms zwischen spielerischem Entwurf und wissen-
schaftlicher Affirmation », in Charlotte Bigg et Jochen Hennig (dir.), Atombilder. 
Ikonografie des Atoms in Wissenschaft und Öffentlichkeit des 20.  Jahrhunderts, 
Göttingen, Wallstein, p. 21-30.
Rasmussen Nicolas, 1997, Picture Control : The Electron Microscope and the Trans-
formation of Biology in America (1940-1960), Palo Alto (CA), Stanford University 
Press.
Reinhardt Carsten, 2006, Shifting and Rearranging : Physical Methods and the 
Transformation of Modern Chemistry, Sagamore Beach (MA), Science History 
Publications.
Rocke Alan, 2010, Image and Reality : Kekulé, Kopp and the Scientific Imagination, 
Chicago (IL), University of Chicago Press.
Rothschild Rachel, 2013, « Environmental Awareness in the Atomic Age : Radio-
ecologists and Nuclear Technology », Historical Studies in the Natural Sciences, 
vol. 43, no 4, p. 492-530.
Schirrmacher Arne, 2010, « Looking into (the) Matter : Scientific Artifacts and 
Atomistic Iconography », in Peter Morris et Klaus Staubermann (dir.), Illumi-
nating Instruments, Washington (DC), Smithsonian Institution Scholarly Press, 
p. 131-155.
Scholz Susanne et Griem Julika (dir.), 2010, Medialisierung des Unsichtbaren um 
1900, Munich, Fink.
Shuping Ralph E., 1981, Use of Photographic Film to Estimate Exposure near the 
Three Mile Island Nuclear Power Station, Washington (DC), US Department of 
Health and Human Services.
Sibum H. Otto (dir.), 2008, « Science and the Changing Senses of Reality circa 1900 », 
Studies in History and Philosophy of Science, vol. 39, no 3, p. 295-458.
Sturken Marita et Cartwright Lisa, 2001, Practices of Looking : An Introduction to 
Visual Culture, Oxford, Oxford University Press.
Van Lente Dick (dir.), 2012, The Nuclear Age in Popular Media : A Transnational 
History (1945-1965), Basingstoke, Palgrave Macmillan.

	
le siècle de l’atome en images 	
185
Weart Spencer, 2012, The Rise of Nuclear Fear, Cambridge (MA), Harvard University 
Press.
Webb J.H., 1949, « The Fogging of Photographic Film by Radioactive Contaminants in 
Cardboard Packaging Materials », Physical Review, vol. 76, no 3, p. 375-380.
Wellerstein Alex, 2008, « Patenting the Bomb : Nuclear Weapons, Intellectual 
Property and Technological Control », Isis, vol. 99, no 1, p. 57-87.
Welsh Ian, 2003, Mobilising Modernity : The Nuclear Moment, New York, Routledge.
Wilder Kelley, 2009, Photography and Science, Londres, Reaktion Books.


 
Deuxième partie
Champs de sciences


9 L’avènement  
des sciences sociales
J a c q u e s  R e v e l
Les sciences sociales ne sont pas nées au xxe siècle. Leur déploiement 
contemporain a été précédé par une longue gestation. Leur histoire est 
complexe. Elle s’est inscrite dans une série de configurations successives 
au sein desquelles le projet, les enjeux, les instruments et les ressources 
d’un savoir sur le monde social ont été plusieurs fois redéfinis. Autour 
de 1900, ces questions font l’objet d’un intérêt qui va bien au-delà des 
cercles savants. « Sociologie » est le mot à la mode, constate ironiquement 
le sociologue Gabriel Tarde, et à travers lui ce qu’il évoque de façon plus 
ou moins précise, les attentes et les investissements qu’il recouvre, un 
intérêt diffus pour le « social », souvent doublé d’une volonté politique. 
L’historien Henri Hauser lui fait écho : « Le prestige exercé sur la société 
contemporaine par cette épithète de social est si grand qu’il suffit de lui 
accrocher un substantif pour éveiller l’attention du monde et de la foule 1. » 
La modernité, les formes nouvelles de l’organisation sociale, la sollicitation 
des masses, l’exercice de la démocratie posent des problèmes auxquels on 
ressent alors l’urgence d’apporter des réponses. De fait, le débat d’idées mais 
aussi la pratique sont largement diffusés dans le Paris de la Belle Époque, 
portés par des initiatives privées ou semi-privées, en même temps que 
le projet plus général, plus confus aussi, d’une intervention sur le social 
mobilise l’activisme de sociétés de pensées d’inspirations très diverses, 
de revues, voire d’institutions para-universitaires qui se multiplient alors. 
On sait que c’est contre cette indistinction que s’est résolument défini le 
projet intellectuel et disciplinaire d’Émile Durkheim, celui d’une science 
rigoureusement fondée en méthode, à distance de l’opinion et du sens 
commun. Ce parti n’est pas propre à la sociologie ni au monde intel-
lectuel français. Le tournant du xixe au xxe siècle est le moment où la 
1. Hauser 1903 (p. 16).
 « Donnez-moi un enfant et je le modèlerai en n’importe quoi. », Burrhus F. Skinner, A Matter of Consequences, 
New York University Press, 1985. Sur la photo, Skinner (à gauche) et ses pigeons expérimentaux.

190	
jacques revel
plupart des domaines de la recherche que nous plaçons sous le pavillon 
des sciences sociales font le choix de se constituer en disciplines scienti-
fiques, mieux, en disciplines universitaires. Leur démarche rencontrera 
un succès inégal.
Il reste qu’une nouvelle donne paraît alors acquise. D’un côté, un intérêt 
insistant, largement partagé, pour les questions que posent les formes 
nouvelles de l’expérience sociale. De l’autre, portée par ces préoccupa-
tions diffuses et en même temps réagissant impatiemment contre elles, 
l’affirmation de savoirs spécialisés qui revendiquent de les reformuler à 
partir d’hypothèses explicitées et de leur apporter des réponses objec-
tives au terme de procédures méthodiques et reproductibles. Ce dispositif 
est-il si différent de celui que nous pouvons observer à plus d’un siècle de 
distance ? Entendons-nous : par rapport à la situation qui était la leur à 
la veille de la Première Guerre mondiale, les sciences sociales ont acquis 
une présence et elles ont produit une masse de connaissances qui sont 
incomparables ; elles n’en ont pas réduit pour autant la part de l’opinion 
parce que les frontières de leurs domaines de spécialisation sont le plus 
souvent poreuses. Peu nombreux sont les contemporains qui prétendent 
former une opinion personnelle sur la physique quantique ou la biologie 
moléculaire. Tous ou presque le font quand il s’agit de formuler des 
jugements sur le monde social, qui est après tout celui dans lequel ils 
vivent. Et ils le font souvent aujourd’hui dans le lexique des sciences 
sociales qui, pour une part au moins, a été repris dans le langage commun 
par le relais des médias. Ils ne sont pas les seuls. Entre le grand public 
et les spécialistes, toute une gamme d’institutions et d’instances s’inté-
ressent depuis longtemps à ces questions et elles entendent faire valoir 
leurs vues d’autant plus efficacement qu’elles contribuent notablement aux 
ressources de la recherche. De telles mitoyennetés peuvent, à l’occasion, 
poser problème. Mais elles doivent d’abord être constatées. Et lorsque 
nous parlons de « l’avènement des sciences sociales », elles nous rappellent 
qu’il s’agit là d’une formulation ambiguë. Les sciences sociales sont certes 
devenues omniprésentes dans notre monde quotidien. Mais elles le sont 
sous des formes diverses et parfois contradictoires, parce qu’elles restent 
soumises à des tensions permanentes qui résultent d’attentes, d’injonc-
tions et d’usages qui sont souvent désaccordés.
Affirmation, incertitudes, tensions
Les dernières décennies du xixe et les premières années du xxe siècle, un 
peu plus tôt, un peu plus tard, voient donc l’entrée en scène des sciences 

	
l’avènement des sciences sociales 	
191
sociales sur la scène universitaire en Europe et en Amérique du Nord. 
Le scénario de leur constitution en disciplines académiques est, en gros, 
toujours le même. Il est inséparable d’un programme scientifique qui  
est le plus souvent présenté comme en rupture avec l’existant : c’est le cas, 
en France, des Règles de la méthode sociologique, que Durkheim codifie 
en 1894 et qui entendent rompre à la fois avec la connaissance commune 
et avec les propositions qui les ont précédées ; c’est, sur un mode plus 
pacifique, celui de la géographie dans la version qu’en propose Vidal 
de La Blache dans les mêmes années. Une institutionnalisation réussie 
appelle aussi la reconnaissance d’un cursus académique sanctionné par des 
diplômes et qu’il sera possible de faire valoir sur le marché des professions, 
dans l’Université et hors d’elle. Une discipline bénéficie, enfin, de s’orga-
niser en interne à travers des formes associatives, et de se faire connaître 
au-dehors par la publication de ses résultats, et d’abord par celle d’une 
ou de plusieurs revues qui en illustreront le projet commun. Ce modèle 
d’innovation, que l’on retrouve partout dans le monde des premières 
sciences sociales, a nourri de multiples légendes de fondation. Dans la 
réalité, il a pu être diversement modulé : aux États-Unis, l’inscription de 
cursus inédits dans les universités a été précoce tout comme la consti-
tution d’associations qui prenaient en charge la régulation professionnelle 
et celle des compétences en économie, science politique, sociologie ; en 
France, c’est plus souvent la rupture épistémologique ou méthodologique 
qui a été valorisée, alors même que la réception des nouvelles disciplines 
était bien moins assurée.
Car il faut bien l’admettre : la reconnaissance des sciences sociales a été 
inégale. En Italie, leurs ambitions ont été très tôt récusées en doute par 
le philosophe et historien Benedetto Croce et le rejet fera durablement 
ressentir ses effets pendant l’essentiel du xxe siècle. Aux États-Unis, en 
particulier dans les universités nouvellement fondées, elles ont été reçues 
et organisées comme des programmes de connaissance de plein exercice 
articulés à des préoccupations d’intervention sociale. En Allemagne, où 
la présence de certaines d’entre elles était plus ancienne qu’ailleurs, elles 
ont pu trouver leur place dans le modèle ouvert de l’Université humbold-
tienne. En France, où l’introduction des « disciplines sociales » figurait 
dans le cahier des charges de la refondation universitaire de la Troisième 
République, c’est pourtant dans des institutions périphériques, fussent-elles 
prestigieuses – le Collège de France, l’École pratique des hautes études, 
l’École normale –, que certaines d’entre elles ont été longtemps hébergées. 
Dans tous les cas, les nouvelles venues ont rencontré la résistance, et 
parfois la franche hostilité, des disciplines classiques avec lesquelles il 
leur a fallu tenter de négocier un territoire propre. De multiples conflits 

192	
jacques revel
sont nés de ces confrontations, en particulier en Allemagne et en France : 
conflits de méthodes, mais aussi, de façon plus diffuse, plus publique  
aussi, opposition entre des cultures spécialisées et la traditionnelle reven-
dication d’une culture générale dont les lettres, la philosophie et l’histoire 
se concevaient comme les seules dépositaires légitimes 1.
Mais il y a plus. Si le pavillon des sciences sociales est fortement 
visible au début du xxe siècle, il couvre en fait des réalités hétérogènes. 
Leur développement, on l’a souvent souligné, s’est inscrit dans un cadre 
national dont résultaient des attentes, des ressources, mais aussi des 
contraintes particulières. Il porte aussi la marque de traditions intellec-
tuelles profondément différenciées. Dans le monde allemand, qui fait 
figure de référence au tournant du siècle, l’héritage ancien de la tradition 
caméraliste, d’une part, la centralité de la philosophie dans les formations 
supérieures, de l’autre, ont été à l’origine d’une préoccupation réflexive 
qui contraste fortement avec l’expérience américaine, où prédominait 
l’exigence d’une connaissance positive du monde social. À ces contextes 
correspondent des dispositifs eux aussi différents : une conception plus 
englobante, une intégration relativement poussée, dans le premier cas, 
à partir d’un questionnaire largement partagé ; dans le second, une insti-
tutionnalisation rapide, mais segmentée, dans laquelle chaque discipline 
définit pour son compte son périmètre et ses règles. La France propose  
un paysage encore différent : une communauté méthodologique fortement 
revendiquée mais un éclatement institutionnel qui durera jusque dans la 
seconde moitié du xxe siècle.
De quoi, d’ailleurs, parle-t-on quand on évoque les sciences sociales ? 
Par-delà des distinctions sémantiques qui ne sont pas négligeables (entre 
sciences sociales, sciences humaines, sciences de l’homme), on a bien 
affaire à un ensemble de programmes qui ont en commun de s’opposer à 
la tradition des humanités (même si elles ne le font pas toujours dans les 
mêmes termes) et de proposer une intelligence du monde contemporain. 
Au tournant du siècle, elles s’inscrivent aussi sur fond d’un évolutionnisme 
hérité du xixe siècle mais qui peut faire l’objet de lectures contrastées. 
Ces programmes se veulent scientifiques ; ils entendent se conformer 
à l’agenda méthodique et épistémologique des sciences positives – à 
l’exception notable du monde allemand où la distinction, formulée par 
Dilthey, entre les sciences de la nature et celles de l’esprit, est à l’origine 
d’un clivage essentiel et dont les effets vont être durables. Au-delà, 
les contenus diffèrent substantiellement. La science politique et l’éco-
nomie sont très tôt instituées dans les universités américaines, bien plus 
1. Ringer 1992.

	
l’avènement des sciences sociales 	
193
tardivement en France. L’histoire ne fait pas partie du cercle des sciences 
sociales aux États-Unis, où le clivage avec les humanités est plus tranché, 
tandis qu’elle reste une protagoniste reconnue des nouvelles disciplines 
(même s’il lui arrive d’être sévèrement remise en cause dans ses démarches) 
en Allemagne ou en France. Pas de définition unifiée, donc, mais une 
série d’agencements qui renvoient à autant d’expériences singulières. 
Les conditions dans lesquelles se sont constituées les nouvelles disci-
plines, tantôt par soustraction à des domaines préexistants, tantôt par 
adjonction, les rapports de concurrence (et parfois aussi de solidarité) 
qu’elles entretiennent entre elles, les paradigmes dans lesquels elles  
ont choisi de s’inscrire, sont autant d’éléments décisifs. La psychologie, 
qui tient pourtant une place éminente dans l’ordre des savoirs sociaux 
au tournant du siècle, reste une entité fragile, parce qu’elle reste éclatée 
entre les facultés de médecine et celles des lettres. L’anthropologie, elle 
aussi sollicitée en différentes directions, est longue à trouver une assise 
disciplinaire stabilisée. L’économie, très tôt autonomisée dans le monde 
anglo-saxon, demeure longtemps en France dans la mouvance des facultés 
de droit. À l’inverse, la sociologie, dans sa version durkheimienne, manifeste 
d’emblée l’ambition de s’affirmer comme la science sociale, celle dont 
les autres disciplines ne seraient que des branches spécialisées. Cette 
conception, fermement affirmée dans les textes prescriptifs de l’école, 
commande aussi l’organisation du travail pratique de ses membres, comme 
en témoignent les sommaires de la première Année sociologique ; c’est elle 
encore qui alimente une série de polémiques talentueuses ouvertes par 
les durkheimiens, par François Simiand en particulier, à l’encontre des 
historiens, des géographes ou des économistes. Pourtant, cette première 
tentative d’une organisation volontariste, systématique, autour d’une 
conception de la science et d’une méthode n’a pas d’avenir immédiat 
parce que la sociologie n’a pas les moyens de son entreprise face à des 
disciplines plus solidement installées. Cet épisode n’est pas anecdotique 
pour autant. Car il pose comme problème l’existence d’une communauté 
des sciences sociales qui reste largement à démontrer.
Le bilan de ce moment des fondations est donc en demi-teinte. De 
nouveaux programmes de connaissances sont proposés ; ils sont associés 
à des formations spécialisées, qui débouchent dans certains cas sur 
des compétences professionnelles ; tous ensemble, ils répondent aux 
demandes de sociétés qui ont besoin de données sûres pour définir des 
règles de gestion et d’action. En ce sens, les sciences sociales ont bien 
réussi leur percée. Dans un âge scientiste, elles trouvent leur légitimité 
dans la production de connaissances qu’elles affirment être capables de 
soustraire de façon méthodique aux biais de l’opinion et de l’idéologie. 

194	
jacques revel
Elles peuvent donc nourrir l’ambition de participer au bien commun. Mais 
elles ne forment pas pour autant un ensemble unifié ni même coordonné. 
Leur projet est bien de proposer un modèle de connaissance général, 
mais il est mis en œuvre dans des conditions qui restent fortement tribu-
taires de conditions et de traditions nationales. Les deux constructions 
théoriques majeures qui, en ces mêmes années, se donnent pour tâche 
de les penser comme un ensemble, celle d’Émile Durkheim en France et 
celle de Max Weber en Allemagne (et qui sur le moment peuvent d’ailleurs 
s’ignorer superbement), vont nourrir une bonne part du débat pendant 
l’essentiel du xxe siècle, et jusqu’à nos jours. Mais dans l’immédiat, elles 
ne suffisent pas à convaincre les protagonistes de s’accorder sur un ordre 
commun des sciences sociales alors même qu’ils ne cessent de s’inter-
roger sur leur spécificité 1.
1920-1970 : le modèle américain
Le demi-siècle qui va des lendemains de la Première Guerre mondiale 
aux années 1960 est souvent décrit comme celui d’une hégémonie améri-
caine dans le domaine des sciences sociales, longuement préparée et 
puissamment affirmée après 1945. Le diagnostic est certainement trop 
sommaire. Il reste vrai qu’un modèle original se met alors en place, qui 
est inséparable d’une croissance spectaculaire des disciplines sociales, 
des résultats qu’elles produisent et de la place qu’elles occupent dans la 
manière dont la société se pense et se gère. La conviction qu’une accumu-
lation des connaissances est nécessaire à l’amélioration des conditions 
sociales et au progrès démocratique n’est en rien propre aux États-Unis. 
Mais tandis que l’Europe est encore en proie au doute au lendemain de 
la guerre, ceux-ci se préoccupent d’organiser un avenir qui leur semble 
promis. À l’arrière-plan se profile une théorie de la modernisation qui 
trouvera sa pleine explicitation après le second conflit mondial et qui est 
porteuse d’une perspective normative implicite.
Au service de ce programme, une conception rigoureuse de l’activité 
scientifique très fortement inspirée par le positivisme logique. Sur le 
modèle des sciences de la nature, elle associe l’exigence d’observations 
empiriques systématiques à des protocoles d’analyse précisément étalonnés. 
Pour répondre à ce qu’on attend d’elles, les nouvelles sciences sociales 
doivent donc se donner les moyens de produire des données objectives et 
testables, indépendantes de tout biais subjectif et de toute préconception 
1. Fabiani 2006 (p. 22).

	
l’avènement des sciences sociales 	
195
idéologique. Plutôt que de spéculer sur des réalités qui restent hors 
d’atteinte et de tout contrôle possible, il faut se concentrer sur l’obser-
vable, c’est-à-dire sur des comportements. Au « mentalisme » s’oppose 
le béhaviorisme, dont John B. Watson forge le nom et dont il expose le 
programme et la méthode dans un ouvrage qui fait date en 1925 1. Sur la 
base de variables observables et mesurables, cette approche se propose 
de rendre compte expérimentalement de comportements psycholo-
giques, économiques, sociaux. Watson est un psychologue passé de l’étude  
des comportements animaux à celle des comportements humains et 
qui entend décrire et mesurer les réactions d’individus en réponse à des 
stimuli extérieurs et définis comme sociaux. Une fois agrégés, les résultats 
de l’observation pourront être traités statistiquement et ils permettront 
d’identifier des régularités positives. Des données agrégées, des compa-
raisons et des contrôles auxquels elles se prêtent, on attend qu’ils rendent 
possible de dégager des catégories générales objectives. Mais l’ambition 
n’est pas ici seulement nomologique : elle est aussi prédictive. Toujours 
sur le modèle des sciences de la nature, on pense pouvoir contribuer à 
la formulation de décisions et à la définition de politiques rationnelles.
L’approche béhavioriste a souvent été caricaturée sous la forme simpli-
ficatrice d’un binôme stimulus-réponse (et il est arrivé qu’elle se conforme 
à cette caricature). Mais outre qu’elle a fait l’objet de développements 
divers et concurrents ainsi que, parfois, d’une extrême sophistication, il 
reste qu’elle a durablement marqué les sciences sociales aux États-Unis 
et à partir d’eux. La psychologie expérimentale est bien évidemment la 
première discipline affectée par ce nouveau programme, dès les années 
1920. À partir de la décennie suivante et jusqu’aux années 1960, le béhavio-
risme radical de Burrhus F. Skinner y occupe une position centrale.  
Mais le modèle scientifique est bientôt repris à leur compte par d’autres 
disciplines. Plutôt qu’aux institutions politiques, aux programmes  
et aux intentions, ou encore à une réflexion théorique sur la démocratie, 
la science politique choisit elle aussi de se consacrer à l’étude des compor-
tements de ceux qui en sont les acteurs effectifs aux différents niveaux 
où ils se situent, des comportements électoraux à ceux des partis et aux 
mécanismes de production de l’opinion. L’économie suit le mouvement. 
Dans les années d’incertitude de la Grande Dépression, alors que le 
besoin est pressant d’arrêter les principes d’une intervention politique  
et de pouvoir prévoir les crises dans l’avenir, l’analyse cyclique doit pouvoir 
se fonder sur des données massives et sûres. La mathématisation de la 
discipline, en particulier de la macroéconomie, va devenir l’un de ses 
1. Watson 1925.

196	
jacques revel
traits distinctifs. Elle s’étend en ces années, avec le développement de 
l’économétrie. Dans tous ces cas, l’outil statistique est devenu essentiel à 
l’analyse des données et aux formes d’expérimentation auxquelles celles-ci 
se prêtent. La tradition en est relativement ancienne, mais la statistique 
fait désormais partie intégrante de l’équipement, de la pratique, et aussi 
d’un style nouveau des sciences sociales qui s’impose en ces années. À ce 
propos, l’historienne Dorothy Ross parle d’une conception qui relève 
de l’« ingénierie » scientifique 1. Le terme est bien choisi, qui associe un 
positivisme revendiqué, une forte exigence procédurale, en même temps 
que la préoccupation d’apporter des réponses factuelles à des problèmes 
sociaux concrets.
Le modèle qui s’impose est porté par un ensemble de conditions 
favorables. L’entre-deux-guerres voit aux États-Unis un fort dévelop-
pement du réseau universitaire et des sciences sociales en son sein. 
Des nouveaux départements spécialisés qui se créent, on entend qu’ils 
produisent des approches inédites mais aussi qu’ils forment les spécia-
listes dont la société a besoin. L’État fédéral encourage le mouvement, 
mais il n’est pas seul à le faire dans un pays où le haut enseignement reste 
très décentralisé. La « philanthropie scientifique » joue ici un rôle décisif. 
Les fondations privées – Carnegie, Russell Sage, Rockefeller, Ford plus 
tardivement, pour ne retenir que les plus visibles – participent massi-
vement au développement des sciences sociales dont elles attendent 
une contribution décisive à « l’amélioration des conditions sociales 
et des conditions de vie aux États-Unis » (avant de tenter d’étendre 
leur programme à l’Europe puis au reste du monde). Elles encouragent 
les efforts de quelques universités choisies, comme celle de Chicago.  
Elles contribuent aussi à l’orientation des politiques publiques à travers 
les thématiques qu’elles privilégient ; elles le font aussi en soutenant,  
par exemple, la création du Social Science Research Council en 1923. Les 
financements lourds qu’elles accordent à des programmes de recherche 
et de formation sélectionnés sont compris comme autant de contri-
butions à un progrès qui doit protéger le monde contemporain contre  
les crises et la menace d’une rupture radicale, hantises de ces années  
difficiles. Leur perspective est explicitement utilitariste et réformiste. 
D’où l’accent qui est significativement placé sur l’apport qui peut être 
celui des sciences sociales à une meilleure intégration et à une stabili-
sation du monde social : on attend d’elles qu’elles permettent de formuler 
des solutions politiques pratiques. D’où sans doute aussi le choix qui est 
fait de considérer la société dans ses multiples dimensions comme un 
1. Ross 1991 et 2003.

	
l’avènement des sciences sociales 	
197
ensemble systémique régi par des fonctions régulières qui en assurent 
l’équilibre et l’harmonie. Les approches fonctionnalistes – le mot apparaît 
dans les années 1930 – vont caractériser durablement ce moment de 
l’histoire des sciences sociales.
L’expérience de la Seconde Guerre mondiale renforce encore ces 
tendances. Économistes, psychologues, sociologues, anthropologues 
sont alors massivement mobilisés au service d’un effort de guerre que  
l’on cherche à rationaliser. Les praticiens sont requis de travailler en 
équipes et de coordonner leurs démarches pour proposer des solutions à 
des problèmes pratiques. Celles-ci s’étaient essentiellement inscrites dans  
un cadre disciplinaire. Elles doivent maintenant pouvoir être confrontées au 
sein d’un cadre commun de références et de pratiques. Les deux décennies 
d’après guerre, qui sont celles du plus fort épanouissement du modèle 
américain, voient ainsi se développer une interdisciplinarité qui a été de 
fait avant de faire l’objet de théorisations ambitieuses. Sur un fonds ancien 
de convictions positivistes et scientistes se greffe une nouvelle ambition 
théorique, portée par la certitude que les sciences sociales ont désormais 
atteint l’âge de la maturité. Talcott Parsons (1902-1979) est sans doute 
l’une des figures qui incarnent le mieux ce moment. Formé en Europe à 
la croisée entre économie et sociologie, il a été profondément influencé 
par les œuvres de Durkheim, Weber, Pareto. La théorie de l’action qu’il 
n’a cessé de développer depuis la fin des années 1930 propose une vaste 
construction conceptuelle au sein de laquelle il se propose de classer les 
comportements individuels et collectifs et les valeurs qui les orientent. 
Le structuro-fonctionnalisme auquel son nom reste attaché est insépa-
rable d’un projet systémique sophistiqué. Mais Parsons est porteur d’un 
projet pratique aussi, puisqu’il entend définir ce que peuvent être pour 
une société les meilleures conditions d’adaptation au changement. Il 
est enfin un bâtisseur d’empire académique. Le Department of Social 
Relations qu’il fonde à Harvard en 1946 et qui réunit des sociologues, des 
psychologues sociaux et cliniciens, des anthropologues, est le premier 
exemple d’un département pluridisciplinaire aux États-Unis : il y voit 
les composantes d’une « science sociale de base » enfin intégrée. Ces 
partenaires ne partagent pas seulement un grand projet ; ils mettent en 
commun toute une culture technique de la recherche dont on a récemment 
montré l’importance 1. Au service de l’entreprise, Parsons déploie une 
efficace activité de broker, qui lui permet de mobiliser des appuis insti-
tutionnels publics et privés, des ressources et de l’influence, comme c’est 
le cas de son contemporain, l’économiste, politologue et psychologue 
1. Isaac 2012.

198	
jacques revel
Herbert Simon, et d’un certain nombre de figures de premier plan (Paul 
Lazarsfeld, Robert Merton) et de nombre de programmes de recherche 
qui se multiplient en ces années.
C’est le temps des grandes espérances. Portées par une spectaculaire 
croissance universitaire – elles sont désormais installées dans les centres 
les plus prestigieux –, par le soutien renforcé des agences publiques (dont 
la National Science Foundation, 1950) et des fondations privées, en prise 
directe sur les demandes et sur les choix des décideurs, les sciences sociales 
paraissent indispensables au fonctionnement d’une société moderne. La 
recherche opérationnelle est à l’ordre du jour. Au paroxysme de la guerre 
froide, elles offrent la perspective d’une gestion tout à la fois volontariste 
et libérale que formalise la théorie de la modernisation 1. Elles prétendent à 
une rigueur scientifique exemplaire. La mathématisation, la formalisation 
et la modélisation sont à l’ordre du jour dans plusieurs domaines disci-
plinaires, à commencer par l’économie néoclassique dont les principes 
sont codifiés par Samuelson, Leontief, Friedman et nombre d’autres : 
par-delà leurs différences, qui peuvent être notables, leurs vues dessinent 
ensemble un canon (mainstream) qui va s’imposer à la discipline dans  
le reste du monde. Plus généralement, des instruments théoriques et 
méthodologiques sont désormais partagés : ainsi de la théorie des jeux ou 
de celle du choix rationnel, ainsi du paradigme de l’individualisme métho-
dologique repris à leur compte par la science politique et, à un moindre 
degré, par la sociologie. Dans leur version américaine, les sciences sociales 
combinent ambition théorique et exigence empirique : le survey devient la 
forme canonique de l’enquête statistique lourde. Puissamment installées, 
fortement reconnues en termes d’utilité sociale, elles paraissent en voie 
d’intégration. Dans ce moment de force, elles peuvent prétendre s’offrir 
en modèle de référence.
Les expériences européennes
Face à l’offre américaine, l’Europe dispose de ressources limitées. 
Pourtant elle aussi offre de grands noms et de grandes œuvres, dont 
certaines font l’objet d’une très forte reconnaissance internationale, de 
Vilfredo Pareto à John Maynard Keynes, de Maurice Halbwachs, Marcel 
Mauss, à Marc Bloch, Jean Piaget ou Norbert Elias, pour ne retenir que 
quelques figures célèbres. Mais les sciences sociales y restent relati-
vement marginales ; elles sont dispersées et faiblement intégrées. Elles 
1. Gilman 2003.

	
l’avènement des sciences sociales 	
199
ont payé un lourd tribut aux désastres des deux guerres mondiales ; elles 
sont menacées de disparaître par les régimes totalitaires dont l’instau-
ration a été à l’origine de l’émigration d’un grand nombre de spécialistes 
majeurs vers le Nouveau Monde. Les flux se sont, en quelque sorte, 
inversés par rapport aux premières décennies du siècle. Au voyage  
de formation en Europe, en Allemagne en particulier, s’est substitué le 
séjour aux États-Unis, facilité par une politique résolue d’invitations, de 
bourses et d’échanges. On a beaucoup parlé d’une hégémonie améri-
caine pour qualifier cette période. Il est vrai que d’Amérique proviennent 
désormais des standards de recherche qui sont repris à travers le monde. 
C’est particulièrement le cas pour l’économie 1 et ce l’est aussi dans 
une large mesure pour les sciences politiques, pour certains aspects de  
la sociologie. Ce mouvement concerne le monde entier, en particulier les 
pays émergents. Il est vrai aussi que certains pays européens, comme le 
Royaume-Uni ou les pays nordiques, sont plus accueillants que d’autres 
à une conception utilitariste des sciences sociales. Mais c’est plus généra-
lement une internationalisation qui s’impose alors, au service de laquelle 
des agences comme l’Unesco tiennent une place importante. Elle encourage 
bien évidemment la diffusion de modèles communs. Elle peut aussi, à 
l’occasion, offrir l’occasion de confrontations critiques qui ne sont pas 
seulement anecdotiques 2.
Il n’est pas aisé de trouver des caractéristiques communes aux expériences 
européennes, ne serait-ce que parce qu’elles s’enracinent en des tradi-
tions intellectuelles, politiques et sociales profondément différentes. Une 
comparaison entre l’anthropologie britannique et l’anthropologie française 
permet d’en prendre la mesure, comme l’illustrent aussi les développements 
divergents de la recherche sociologique entre la France et l’Allemagne. 
Il reste qu’un certain nombre de références fondatrices sont partagées 
(ce qui ne signifie pas pour autant qu’elles ne soient pas discutées) : ainsi 
Durkheim et Weber, mais aussi Marx et, jusqu’à un certain point, Freud. 
On observe aussi que là où les sciences sociales américaines entendaient 
contribuer à l’intégration du monde social et à sa stabilisation, leurs 
homologues européennes privilégient davantage une position critique. 
Un projet inscrit dans une durée relativement longue, comme celui  
de l’Institut de recherche sociale (plus tard connu sous le nom d’École de 
Francfort), en est un exemple parlant 3. Il est fondé en 1923 et l’histoire 
en est discontinue, puisqu’elle passe par l’exil à New York dix ans plus 
1. Fourcade 2006.
2. Guilhot, Heilbron et Jeanpierre 2009.
3. Jay 1977 [1973], Durand-Gasselin 2012.

200	
jacques revel
tard, avant un retour en Allemagne après la Seconde Guerre mondiale ; 
elle se prolonge sur plus de trois générations. Elle est polyphonique aussi, 
dès la génération des grands fondateurs, Horkheimer, Adorno, Fromm, 
Benjamin, et ne se laisse pas résumer dans une formule commune. L’entre-
prise rassemble cependant des chercheurs qui se proposent de mobiliser 
des approches diverses des sociétés contemporaines au service d’une 
« théorie critique » : critique des formes avancées de l’évolution du capita-
lisme et des pathologies sociales qu’elles engendrent, mais aussi critique 
réflexive du travail de la critique. À partir d’un diagnostic de crise – les 
promesses des Lumières ont été dévoyées, elles débouchent sur « une 
nouvelle forme de barbarie 1 » –, le travail collectif se propose de rendre 
compte des différentes dimensions de l’aliénation dans l’organisation du 
travail et les relations sociales, dans la production et la consommation 
culturelles, dans les formes de l’expérience quotidienne. La théorie critique 
se donne moins pour tâche de produire une synthèse que de confronter 
des points de vue et des démarches (celles de l’économie, de la socio-
logie, de la philosophie politique, de l’esthétique, de la psychologie et de 
la psychanalyse), de façon à enrichir en permanence un espace d’inter-
rogations plurielles. On mesure ce qui peut séparer un tel projet des 
conceptions américaines. L’exigence d’une objectivité scientifique fondée 
sur l’observation empirique et sur la standardisation des procédures,  
la constitution d’un savoir positif comptent moins ici que le déplacement 
du questionnaire et son renouvellement constant. L’enracinement philo-
sophique et le prolongement du projet marxiste sont évidents, comme 
l’est le souci constant d’une inscription historique de l’analyse.
Cette présence de la dimension historique dans le concert des sciences 
sociales ne se limite pas à la seule mouvance de l’héritage wébérien. On 
en retrouve en France une version bien différente, plus modeste, certai-
nement plus empirique, mais dont les effets ne sont pas moins durables. 
L’expérience des Annales 2, la revue fondée par Marc Bloch et Lucien 
Febvre en 1929, et ses prolongements institutionnels au lendemain de 
la guerre 3, est à l’origine d’une configuration disciplinaire originale dans 
laquelle l’histoire tient une place centrale et, jusqu’à un certain point, 
unique. Fernand Braudel peut ainsi revendiquer pour elle un rôle néces-
saire dans le difficile dialogue entre les sciences sociales, toutes requises 
1. Adorno et Horkheimer 1974 [1947], Adorno 2003 [1951].
2. Connue génériquement sous le nom d’Annales, la revue a d’abord été intitulée Annales d’histoire 
économique et sociale ; en 1946, elle est devenue Annales. Économies, sociétés, civilisations ; en 
1994, elle a pris son titre actuel : Annales. Histoire, sciences sociales.
3. Avec la création de la Sixième Section de l’École pratique des hautes études (1947), qui 
deviendra l’École des hautes études en sciences sociales en 1975. Sur le modèle, voir Braudel 1972.

Deux exemples de représentations du monde social
2. Les réseaux : un double espace relationnel  
(Cottereau et Marzok 2011).
vers 1600
vers 1800
Maire
Auditore
Eleto
Décurion
Erario
Prieur du Mont de Piété
Émigration
Passage par voie féminine
dans une autre famille
Extinction de la lignée
N.B. Le rang de naissance des personnes va de gauche (aîné) à droite (cadets)
Giovan
Lorenzo
Donato
Donato
Donato
Antonio
Donato
Antonio
Donato
Antonio
Gregorio
Gregorio
Luca
Florio
Bernardino
Angelo
Giovan
Francesco
Giovanni
Domenico
Domenico
Domenico
Domenico
Biasi
Leonardo
Bernardino Giacomo
Giuseppe
Giuseppe
Giuseppe
Giuseppe
Giuseppe
Angelo
Pietro
Geronimo
Agostino
Pietro
Lorenzo
Leonardo
Benedetto
Gregorio
Gregorio
Leonardo
Diego
Michele
Gaetano
Pietro
Liborio
Francesco
Alessandro
Donato
Agostino
Leonardo
Domenico
Nicola
Donato
Agostino
Salvatore Luigi
Nicolo
Leonardo
Florio
Tomaso
Giovanni
Gregorio Pietro
Gronzo
Leonardo
Agostino
Alessandro
Leonardo
Pietro
Giovan
Lorenzo
Colella
Giovan
Maria
Giovan
Lorenzo
Donato
Donato
Donato
Antonio
Donato
Antonio
Donato
Antonio
Gregorio
Gregorio
Luca
Florio
Bernardino
n
a
v
oi
G
ole
g
n
A
Francesco
Giovanni
Domenico
Domenico
Domenico
Domenico
Biasi
Leonardo
Bernardino Giacomo
Giuseppe
Giuseppe
Giuseppe
Giuseppe
Giuseppe
Angelo
Pietro
Geronimo
Agostino
Pietro
Lorenzo
Leonardo
Benedetto
Gregorio
Gregorio
Leonardo
Diego
Michele
Gaetano
Pietro
Liborio
Francesco
Alessandro
Donato
Agostino
Leonardo
Domenico
Nicola
Donato
Agostino
Pio
Salvatore Luigi
Nicolo
Leonardo
Florio
Tomaso
Tomaso
Giovanni
Giovanni
Giacomo Gregorio Pietro
Gronzo
Leonardo
Agostino
Alessandro
Leonardo
Pietro
Colella
Giovan
Maria
Giovan
Lorenzo
En France
Primicerio
Sabato
Micella
Ricchiuto
Presbiteri
Pacella
et Lanza
Siena
1. La transmission des charges  
au sein d’un lignage en Italie aux xviie-xviiie siècles (Delille 2003).
Réseau des liens de Mohammed et Fatima en 2007
Les sommets,  hommes et  femmes, ont des tailles 
proportionnelles à leur importance dans le réseau
Technique graphique de figuration du double 
centrement de réseau, avec courbes au départ 
des deux ego, conçue par Stéphane Baciocchi
Analyse de réseau et première étape de tracé 
ont été menées à l’aide du logiciel Pajek.
Lien interne à la parenté
Lien de premier degré, hors parenté
Lien de second degré hors parenté
En courbes     Liens au départ des deux Ego

202	
jacques revel
de prendre en compte l’inscription temporelle des phénomènes qu’elles 
étudient 1. Deux décennies plus tard, dans un contexte intellectuel différent 
dominé par un marxisme ouvert et critique, le milieu réuni autour de 
la revue Past and Present, puis, plus largement, autour de la New Left, 
proposera une version comparable, même si elle est plus explicitement 
politique, du même projet.
Ces exemples, parmi d’autres variations européennes qu’il aurait été 
souhaitable de pouvoir prendre en compte, posent l’un et l’autre le problème 
de l’organisation des rapports entre les sciences sociales. En simpli-
fiant les choses à l’extrême, il est possible d’opposer deux formules. La 
première entend prescrire la conformation de chaque pratique disci-
plinaire particulière à un modèle épistémologique et méthodologique 
commun ; c’est, on s’en souvient, celle que Durkheim et ses disciples avaient 
tenté d’imposer autour des règles de la méthode sociologique au début 
du xxe siècle ; c’est elle encore que l’on a vue resurgir avec le moment  
structuraliste dans les années 1960 2. La seconde formule est souple : elle 
passe par une coexistence de fait et par la multiplication des points de 
vue et le régime de l’emprunt ; c’est celle qu’illustrent, selon des modalités  
très différentes, l’élaboration d’une théorie critique par l’école de Francfort 
et l’expérience, tout empirique, des Annales. Le problème de la pluri- et 
de l’interdisciplinarité est sans nul doute aussi ancien que les sciences 
sociales elles-mêmes et il n’a cessé de les interroger, depuis plus d’un 
siècle, sur ce qui fonde leur communauté ainsi que sur leur statut et sur 
leur légitimité scientifiques.
Turbulences et recompositions
Entre l’Amérique et l’Europe, les différences sont donc fortes : diffé-
rence de taille, mais aussi de conception des dispositifs scientifiques ; le 
rôle et l’utilité des sciences sociales n’y sont pas pensés ni mis en œuvre 
de la même manière. Le rapport de force, enfin, est inégal : les décennies 
qui ont suivi la Seconde Guerre mondiale sont celles d’une prépondé-
rance marquée du modèle américain, qui est repris à travers le monde. 
Dans cette période de croissance durable, il propose l’image rassurante 
d’un ensemble de savoirs positifs et cumulatifs dont on attend qu’ils 
contribuent au progrès économique, social, culturel et politique des 
1. Braudel 1958, repris in Braudel 1972 (p. 41-83).
2. Lévi-Strauss 1964. On en reste ici à des exemples français. Mais on peut, sans forcer les 
choses, identifier à ce modèle le « moment Parsons » des sciences sociales américaines après 
la Seconde Guerre mondiale.

	
l’avènement des sciences sociales 	
203
sociétés contemporaines. Cet optimisme, et les certitudes scientistes 
qui le soutiennent, se voient pourtant mis en cause à partir des années 
1960-1970. Commence alors un long temps de turbulences dont nous 
ne sommes pas encore sortis.
Les raisons en sont complexes. La critique du modèle structuro-fonction-
naliste a pu être entreprise assez tôt (Charles Wright Mills, L’Imagination 
sociologique, 1959), mais c’est surtout autour de 1968, repère symbolique, 
qu’elle s’est faite plus radicale et qu’elle s’est faite plus générale, en sociologie 
et en sciences politiques, et même, à un moindre degré, en économie. Là 
où l’on avait choisi de penser en termes de systèmes fonctionnels intégra-
teurs, tout à la fois cadres d’analyse et projection idéologique, on insiste 
maintenant sur ses dysfonctionnements et l’on valorise les expériences 
marginales ou alternatives (l’asile, la prison, la maladie mentale) avec la 
conviction qu’elles ont quelque chose d’essentiel à nous apprendre sur  
la société dans son ensemble. Le succès de projets intellectuels aussi  
différents que ceux d’Herbert Marcuse ou Erving Goffman aux États-Unis, 
de Michel Foucault en France, peut servir à illustrer ce déplacement 
majeur. Mais bientôt c’est l’idée même de la société comme une totalité, 
« comme un ensemble naturel intégré par ses fonctions systémiques et par 
sa culture 1 », qui se voit remise en cause. La crise mondiale qui commence 
avec les années 1970 confronte les acteurs sociaux, et les praticiens des 
sciences sociales parmi eux, avec des réalités difficilement déchiffrables. Le 
monde dans lequel ils vivent et dont ils s’efforcent de rendre compte leur 
apparaît moins cohérent, moins lisible. La globalisation (ou la mondia-
lisation), thématique devenue obsessionnelle pendant les dernières 
décennies, ajoute encore à cette opacité, comme le font les transforma-
tions profondes de la carte géopolitique du monde. Dans le même temps, 
les vieilles sociétés développées font l’épreuve d’un changement radical de 
leur rapport au temps historique. La « crise de l’avenir » que diagnostique 
Krzysztof Pomian en 1980 2, la sortie du temps du progrès en désorga-
nisent l’expérience : un avenir mal déchiffrable, un présent incertain, un 
passé qui n’offre plus les garanties que l’on attendait de lui. Aux majes-
tueuses évolutions de longue, voire de très longue durée, dans lesquelles 
paraissaient s’inscrire des transformations massives et autoréalisatrices 
s’est substitué un temps discontinu, non linéaire, jalonné de seuils et de 
bifurcations. Un temps d’incertitudes qui s’oppose à l’optimisme scien-
tiste et méthodologique de la période précédente.
Le vieux socle de certitudes positives qui avait porté le développement 
1. Dubet et Martuccelli 1998 (p. 17).
2. Pomian 1999 (p. 233-262).

204	
jacques revel
des sciences sociales se voit ébranlé. La Structure des révolutions scienti-
fiques, que publie Thomas S. Kuhn en 1962, en est un premier signe fort. 
L’ouvrage, qui sera à l’origine d’un très large débat, propose, autour de la 
notion de paradigme, une lecture discontinuiste des formes du progrès 
scientifique et développe une conception pragmatiste de la vérité scienti-
fique. Les thèses en seront parfois exaspérées dans le sens d’un relativisme 
radical, comme dans l’épistémologie développée par Paul Feyerabend.  
Plus utilement, elles inspireront aussi les « études sociales sur les sciences » 
(science studies) qui entendent ne pas séparer les acquis de la science des 
conditions institutionnelles, sociales et pratiques dans lesquelles ils ont 
été obtenus. Du point de vue des sciences sociales, c’est l’idée même d’une 
connaissance neutralisée, protégée de toute sollicitation externe, qui se 
trouve ainsi mise en cause. Le débat est repris et pour partie réorienté 
pendant les années 1970 par le philosophe américain Richard Rorty. Il l’est 
aussi, avec des attendus très différents au départ, en Europe, en particulier 
à travers l’œuvre de Michel Foucault dont la réception et les interpréta-
tions sont très largement internationales. Foucault reformule lui aussi le 
problème de la vérité scientifique autour du concept de « véridicité » en 
demandant dans quelles conditions historiques et épistémologiques parti-
culières il est possible de poser une question en termes de vrai et de faux. 
L’homme qu’étudient les sciences humaines, la société que se donnent 
pour objet les sciences sociales, ne sont que des agencements historiques, 
datables et provisoires 1. Par ailleurs, les connaissances que produisent 
ces savoirs sont inséparables, selon lui, d’un long processus de mise en 
ordre et de contrôle du monde social par des pouvoirs « disciplinaires ». 
Ces critiques sont, on le voit, d’inspiration et de nature très diverses. 
Elles prennent sens dans des traditions philosophiques profondément 
différentes : le pragmatisme américain, le second Wittgenstein dans le 
monde anglo-saxon, la philosophie « continentale » en Europe. Il leur est 
parfois arrivé d’être conjuguées, comme c’est le cas dans l’« anthropologie 
interprétative » qu’a proposée Clifford Geertz et qui a si profondément 
laissé son empreinte sur toute une part des sciences sociales 2. Toutes 
ensemble, elles offrent un répertoire dans lequel ont puisé les propo-
sitions dans les dernières décennies. On en tirera parfois les raisons 
d’un relativisme sceptique généralisé, comme dans les formes les plus 
radicales du « tournant linguistique ». Mais elles serviront aussi d’appui à 
des entreprises neuves : celle des science studies évoquées plus haut ; dans 
1. Foucault 1966 et 1975. Du même, on consultera bien entendu les Dits et écrits (1994), s.v. 
« véridiction », « vérité », « histoire de la vérité », « jeux de vérité ».
2. Geertz 1973.

	
l’avènement des sciences sociales 	
205
l’approche socio-anthropologique de l’activité scientifique développée 
par Bruno Latour, Michel Callon ou Peter Galison. Elles trouvent aussi 
une proposition de cadrage épistémologique avec Le Raisonnement socio­­
logique de Jean-Claude Passeron 1.
Enfin, c’est la configuration générale du débat qui se voit profondément 
remaniée. Pendant longtemps, il s’était circonscrit à l’Europe et à l’Amé-
rique du Nord. Pendant les dernières décennies, il est devenu mondial. 
« Provincialiser l’Europe », le mot d’ordre lancé par Dipesh Chakra-
barty, va en fait bien au-delà de sa cible explicite. Les protagonistes sont 
désormais bien plus nombreux et ils sont différents parce qu’ils n’avaient 
pas été pris en compte jusque-là. Ils font valoir des attentes, ils témoignent 
d’expériences diversifiées et obligent à une reformulation des enjeux. Le 
développement spectaculaire des études féministes, des études postco-
loniales et des subaltern studies, pour ne citer que certains domaines 
les plus visibles, ne propose pas seulement des revendications alterna-
tives, de traiter enfin le monde « à parts égales ». Il rend possible une 
multiplication de points de vue différents sur des réalités qui n’avaient 
été jusque-là abordées que dans une perspective restreinte aux pays 
qui avaient conçu et porté le programme des sciences sociales. Ce n’est 
sans doute pas un hasard si ces mêmes années sont celles qui ont vu la 
démarche anthropologique occuper une place centrale dans le dispo-
sitif des sciences sociales. C’est qu’elle permet de prendre en compte la 
diversité des expériences humaines et de les confronter à travers l’exercice 
de la comparaison. Ces programmes de décentrement par rapport aux 
problématiques « classiques » sont aujourd’hui déclinés de multiples 
manières. Pour en retenir un exemple, particulièrement significatif parce 
qu’il répond à des interrogations qui sont devenues centrales dans nos 
sociétés, toute une série d’approches environnementales (historiennes, 
sociologiques, éco-anthropologiques, éthiques) est venue, elle aussi, 
remettre en question la centralité et l’exceptionnalisme humains depuis 
les années 1970, les entités naturelles étant désormais pensées et comme 
les protagonistes d’un « social » qui se voit ainsi très largement reformulé 
dans la réflexion en cours sur l’Anthropocène.
De ces incertitudes et de ces questionnements nouveaux, le premier 
résultat a sans nul doute été l’érosion accélérée des grands paradigmes 
fonctionnalistes qui avaient, explicitement ou non, sous-tendu le 
programme des sciences sociales depuis la fin du xixe siècle. Ils avaient 
1. Le sous-titre de l’ouvrage, dont une première version est publiée en 1991 (2e édition profon-
dément révisée en 2006), en explicite le projet qui est celui d’identifier des règles de scientificité 
propres aux sciences humaines et sociales : « L’espace non poppérien du raisonnement naturel ».

206	
jacques revel
pu être très divers, du structuro-fonctionnalisme américain aux marxismes 
européens et au structuralisme, sans oublier le vieux fonds positiviste 
qui a tenu une place si durable dans la vie scientifique française. Mais, 
si différents soient-ils, ils proposaient un cadre intégrateur qui garan-
tissait le projet, au moins asymptotique, d’une saisie et d’une intelligibilité 
globales du monde socio-historique. Alors même que le processus de 
globalisation est à l’œuvre, et sans nul doute parce qu’il est à l’œuvre, c’est 
la représentation de la société comme une totalité ou comme un système 
qui est, provisoirement, devenue difficile à penser. Dans le même temps, 
les ambitions de la plupart des disciplines sociales ont été revues à la 
baisse par rapport à ce qu’elles pouvaient être une génération plus tôt. 
Ou, plus exactement, elles se sont faites plus modestes et, surtout, plus 
réflexives. Elles interrogent aujourd’hui leur généalogie, les concepts et 
les instruments dont elles se servent, les conditions effectives de leurs 
pratiques, les usages sociaux des résultats qu’elles produisent. Pierre 
Bourdieu rappelait que « les sciences sociales ont le privilège de pouvoir 
utiliser leurs propres instruments scientifiques comme des instruments de 
réflexivité 1 ». De ce privilège, elles ont appris à faire un usage critique qui 
est souvent éclairant 2, même s’il arrive aussi que l’exercice de la réflexivité 
se fasse narcissique et devienne un but en soi.
Ces turbulences, que Quentin Skinner a évoquées comme une série de 
« vagues successives » lancées contre la forteresse des sciences sociales 
positives telle qu’elle s’était constituée pendant plus d’un demi-siècle, ont 
eu pour conséquences des révisions et des réorganisations en profondeur. 
L’unité du champ des sciences sociales est sans nul doute moins évidente 
qu’elle ne l’était dans les attentes : il s’est au contraire davantage fragmenté 
et aucun paradigme général ne prétend aujourd’hui pouvoir en rendre 
compte. Entre les disciplines et leurs démarches, des clivages se sont 
marqués, en particulier entre l’économie et le reste des sciences sociales, 
même si des réflexions récentes font voir qu’on ne saurait se contenter 
d’opposer trop simplement les sciences du « modèle » et celles du « récit 3 ». 
On aurait tort, par ailleurs, de ne souligner que l’aspect négatif de ces 
transformations. Tout autant que de fragmentation, il n’est pas abusif 
de parler ici de recompositions, au moins partielles. Les programmes 
de recherche sont devenus plus modestes, mais, affranchis des modèles 
dominants, ils se prêtent plus efficacement à la confrontation entre les 
disciplines : ainsi entre les sciences cognitives et les disciplines sociales 
1. Bourdieu 2004 (p. 19).
2. Pour un exemple remarquable, voir Desrosières 1993.
3. Grenier, Grignon et Menger 2001.

	
l’avènement des sciences sociales 	
207
plus anciennement installées, comme l’anthropologie, l’économie, la 
linguistique. Il est certainement devenu plus difficile de penser l’interdis-
ciplinarité comme un droit, plus encore d’imaginer une science sociale 
réunifiée. La « pratique restreinte » qu’en recommande Bernard Lepetit 
n’implique pourtant aucun renoncement : outre qu’elle permet d’enre-
gistrer comme une donnée de fait le degré relatif d’ouverture propre à 
chaque discipline, elle souligne que les mécanismes de l’innovation, dont 
les transferts disciplinaires sont un cas particulier, supposent entre les 
approches des différences telles que l’on puisse en attendre un bénéfice 
en termes de connaissances 1.
Ce travail de recomposition à la base n’est pas spectaculaire. Le modèle 
du défrichement par larges fronts pionniers, qui avait été longtemps 
dominant dans l’imaginaire des sciences sociales, a laissé place à des 
formes d’expérimentation plus locales. L’éclipse des paradigmes unifica-
teurs a pourtant eu des conséquences reconnaissables. La plus évidente  
en est peut-être la reprise en compte des acteurs qui avaient longtemps  
été absents au profit d’approches systémiques. Les grands modèles 
fonctionnalistes n’impliquaient pas que l’on fît appel à eux pour rendre 
compte de ce qui survient dans le monde social. Leur affaiblissement 
contemporain (et sans doute aussi la défaillance des institutions de 
régulation dans la plupart des sociétés contemporaines) peut aider à 
comprendre le « tournant pragmatique » qui, de la sociologie des grandeurs 
à l’économie des conventions, des science studies à la micro-histoire, 
propose la représentation d’un monde social discontinu, régi par des 
formes de rationalité discrètes, imposant aux acteurs des contraintes 
mais leur offrant aussi des prises et des possibilités de choix 2. Une 
telle approche fait du temps une variable essentielle de l’action et, plus 
généralement, la prise en compte de la dimension historique peut elle 
aussi être comprise comme une évolution significative. Qu’en ces temps 
d’incertitude resurgisse la vieille thématique wébérienne, qui rappelle 
que le travail des sciences sociales porte sur des objets historiques et 
qu’il est lui-même pris dans l’historicité, ne relève sans doute pas du  
hasard.
Il y a plus d’un demi-siècle, Fernand Braudel posait le diagnostic d’une 
« crise générale des sciences de l’homme 3 ». Il ne serait pas trop difficile 
de montrer que c’est toute l’histoire des sciences humaines et sociales 
qui, depuis plus d’un siècle, est faite d’un enchaînement de crises, ce qui 
1. Lepetit 1990, Berthelot 2001.
2. Boltanski et Thévenot 1991.
3. Braudel 1958, in Braudel 1972 (p. 41).

208	
jacques revel
ne signifie pas que celles-ci ont toujours été de même nature, ni qu’elles 
se soient inscrites dans des cadres de références comparables. Aussi bien 
peut-on les lire comme une succession de mises à l’épreuve et de recom-
positions, qui affectent des pratiques de connaissances qui portent sur 
le monde même dans lequel elles sont produites et qui ne sauraient être 
pensées en dehors de cette relation fondatrice. Situation inconfortable ? 
Certainement. Mais elle est sans doute le prix à payer pour le privilège 
d’être les contemporains du contemporain.
Références bibliographiques
Adorno Theodor Wiesengrund, 2003 [1951], Minima moralia. Réflexions sur la vie 
mutilée, trad. fr., Paris, Payot.
Adorno Theodor Wiesengrund et Horkheimer Max, 1974 [1947], La Dialectique 
de la raison, trad. fr., Paris, Gallimard.
Backhouse Roger E. et Fontaine Philippe (dir.), 2010, The History of the Social 
Sciences since 1945, New York, Cambridge University Press.
Berthelot Jean-Michel (dir.), 2001, Épistémologie des sciences sociales, Paris, PUF.
Boltanski Luc et Thévenot Laurent, 1991, De la justification, Paris, Gallimard.
Boudon Raymond, 1986, « Mathematical and Statistical Thinking in the Social 
Sciences », in  Karl W.  Deutsch, Andrei S.  Markovits et John Platt (dir.), 
Advances in the Social Sciences (1900-1980) : What, Who, Where, How, Cambridge 
(MA), Abt, p. 199-217.
Bourdieu Pierre, 2004, « L’objectivation du sujet de l’objectivation », in  Johan 
Heilbron, Rémi Lenoir et Gisèle Sapiro (dir.), Pour une histoire des sciences 
sociales, Paris, Fayard, p. 19-23.
Braudel Fernand, 1958, « Histoire et sciences sociales : la longue durée », Annales. 
Économies, sociétés, civilisations, vol. 13, no 4, p. 725-753.
–	 1972, Écrits sur l’histoire, Paris, Flammarion.
Cottereau Alain et Marzok Mokhtar Mohatar, Une famille andalouse. Ethno-
comptabilité d’une économie invisible, Paris, Bouchène, 2011, p.297.
Delille Gérard, Le Maire et le Prieur. Pouvoir central et pouvoir local en Méditer-
ranée occidentale (xve-xviiie siècle), Paris, EHESS, 2003, p. 329. 
Desrosières Alain, 1993, La Politique des grands nombres. Histoire de la raison 
statistique, Paris, La Découverte.
Dubet François et Martuccelli Danilo, 1998, Dans quelle société vivons-nous ?, 
Paris, Seuil.
Durand-Gasselin Jean-Marc, 2012, L’École de Francfort, Paris, Gallimard.
Fabiani Jean-Louis, 2006, « À quoi sert la notion de discipline ? », in Jean Boutier, 
Jean-Claude Passeron et Jacques Revel (dir.), Qu’est-ce qu’une discipline ?, Paris, 
Éd. de l’EHESS, coll. « Enquête » no 6, p. 11-34.
Foucault Michel, 1966, Les Mots et les choses. Une archéologie des sciences humaines, 
Paris, Gallimard.
–	 1975, Surveiller et punir. Naissance de la prison, Paris, Gallimard.
–	 1994, Dits et écrits, Paris, Gallimard, 4 vol.
Fourcade Marion, 2006, « The Construction of a Global Profession : The Transnation-
alization of Economics », American Journal of Sociology, vol. 112, no 1, p. 145-194.

	
l’avènement des sciences sociales 	
209
Geertz Clifford, 1973, The Interpretation of Cultures, New York, Basic Books.
Gilman Nils, 2003, Mandarins of the Future : Modernization Theory in Cold War 
America, Baltimore (MD), Johns Hopkins University Press.
Grenier Jean-Yves, Claude Grignon et Pierre-Michel Menger (dir.), 2001, 
Le Modèle et le récit, Paris, Éd. de la Maison des sciences de l’homme.
Guilhot Nicolas, Heilbron Johan et Jeanpierre Laurent, 2009, « Social Sciences », 
in Akira Irye et Pierre-Yves Saunier (dir.), The Palgrave Dictionary of Transna-
tional History, Basingstoke, Palgrave Macmillan, p. 953-959.
Hauser Henri, 1903, L’Enseignement des sciences sociales. État actuel de cet ensei-
gnement dans les divers pays du monde, Paris, A. Chevalier-Maresq.
Isaac Joel, 2012, Working Knowledge : Making the Human Sciences from Parsons to 
Kuhn, Cambridge (MA) et Londres, Harvard University Press.
Jay Martin, 1977 [1973], L’Imagination dialectique. Histoire de l’école de Francfort 
(1923-1950), Paris, Payot.
Kuhn Thomas S., 1972 [1962], Structure des révolutions scientifiques, trad. fr., Paris, 
Flammarion.
Lepetit Bernard, 1990, « Propositions pour une pratique restreinte de l’interdiscipli-
narité », Revue de synthèse, vol. 4, no 3, p. 331-338.
Lévi-Strauss Claude, 1964, « Critères scientifiques dans les disciplines sociales et 
humaines », Revue internationale des sciences sociales, vol. 16, no 4, p. 579-597.
Pomian Krzysztof, 1999, Sur l’histoire, Paris, Gallimard.
Porter Theodore M. et Ross Dorothy (dir.), 2003, The Modern Social Sciences, t. 7 de 
The Cambridge History of Science, Cambridge et New York, Cambridge University 
Press.
Ringer Fritz, 1992, Fields of Knowledge : French Academic Culture in Comparative 
Perspective (1890-1920), Cambridge, Cambridge University Press, et Paris, Éd. de 
la Maison des sciences de l’homme.
Ross Dorothy, 1991, The Origins of the American Social Science, New York et 
Cambridge, Cambridge University Press.
–	 1993, « An Historian’s View of American Social Science », Journal of the History of 
Behavioral Sciences, vol. 29, no 2, p. 99-112.
–	 2003, « Changing Contours of the Social Sciences Disciplines », in  Theodore 
M. Porter et Dorothy Ross (dir.), The Modern Social Sciences, t. 7 de The Cambridge 
History of Science, Cambridge, Cambridge University Press, p. 205-237.
Wallerstein Immanuel et  al., 1996, Open the Social Sciences, Report of the 
Gulbenkian Commission on the Restructuring of the Social Sciences, Palo Alto 
(CA), Stanford University Press.
Watson John B., 1925, Behaviorism, New York, People’s Institute.


10 Foucault  
et les transformations  
du biopouvoir
S a r a h  F r a n k l i n
Publié en 1976, La Volonté de savoir, premier volume retentissant de 
l’Histoire de la sexualité de Michel Foucault, fait sentir son influence depuis 
maintenant près de quatre décennies. Le biopouvoir, concept central du 
livre, s’avère fondamental pour analyser et penser nombre des transfor-
mations de la période récente – la naissance du mouvement de libération 
gay et lesbien, la deuxième vague du féminisme, le développement excep-
tionnel de la génétique, de l’assistance médicale à la procréation, des 
biotechnologies, des sciences biologiques de manière générale. Sans 
surprise, comme pour tout concept repris abondamment dans un grand 
nombre de champs au long d’une période riche en innovations sociales 
et scientifiques, la signification du terme reste sujette à interprétations 
et controverses.
Ce chapitre aborde l’origine du concept de biopouvoir chez Foucault. Il 
s’attache ensuite à son destin d’instrument analytique à périmètre variable 
dans le contexte de ce qui s’est imposé sous le nom de « biosociété ». Il 
s’achève par quelques réflexions sur les usages qui pourront être faits à 
l’avenir de cet outil conceptuel. À travers une série d’exemples développés 
par des chercheurs ayant recours au concept depuis les années 1970, ce 
chapitre tente de montrer l’importance grandissante que le biopouvoir 
est susceptible de prendre au xxie siècle.
Généalogie du biopouvoir
Le terme de « biopouvoir » apparaît relativement tard dans l’œuvre 
de Foucault, vingt ans après la parution de son premier livre en 1954. Il 
 Mémorial Cap 110, Anse Caffard. Sculpture de l’artiste martiniquais Laurent Valère en mémoire des 
noyés de la traite des esclaves.

212	
sarah franklin
peut donc être utile de retracer la généalogie du concept, et de rappeler 
les conditions générales dans lesquelles il en vient à apparaître au milieu 
des années 1970.
Les premiers travaux de Foucault portent sur la folie et le contrôle 
des individus déviants par des formes bureaucratiques de discipline et 
de surveillance, mais aussi sur l’histoire de l’idée de population, notion 
centrale à ses yeux. La naissance de la biologie est une préoccupation 
de Foucault dans Les Mots et les choses, publié en 1966. Dix ans avant de 
porter explicitement son attention sur le biopouvoir, il y affirme, dans  
une phrase devenue célèbre, que « la vie n’existe pas » avant le xviiie siècle, 
et que la seule et unique raison de cette non-existence est à trouver dans 
le manque d’un système capable d’en appréhender les interconnexions. 
Selon Foucault, la vie « en tant que telle » ne peut exister sans l’effet unifi-
cateur du concept darwinien d’évolution grâce auquel le vivant peut  
être compris comme partageant un ensemble identique de liaisons. 
Avant les années 1850, l’histoire naturelle est constituée principalement 
d’un assortiment de tableaux et de grilles qui servent certes à classifier 
la vie mais qui ne définissent pas ses propriétés ou principes propres. 
Pour Foucault, ce n’est qu’après Darwin qu’une nouvelle définition de 
la vie comme système naturel confère au vivant une unité conceptuelle, 
permettant d’établir la discipline scientifique moderne qu’est la biologie 1.
Selon le modèle darwinien, ce n’est pas essentiellement le mécanisme de 
la sélection qui explique comment la vie prend forme et se développe au 
cours du temps. C’est l’accent que met Darwin sur l’ancestralité commune 
et singulière de toutes les formes de vie qui tient lieu de principe irréduc-
tible. En bref, la différence entre le vivant et le non-vivant repose sur 
une prémisse : toute vie organique est reliée par le partage d’une même 
substance reproductrice et descend d’un ancêtre commun. Comme l’a 
observé l’anthropologue Marilyn Strathern à la suite de l’historienne Gillian 
Beer, le choix que fait Darwin d’établir une analogie entre le pedigree, 
ou l’arbre généalogique, et le modèle des interconnexions du vivant, n’a 
pas pour seul enjeu d’ancrer sa théorie dans une représentation préexis-
tante et suffisamment proche de la théologie de l’arbre biblique pour 
compenser l’offense potentielle faite au public judéo-chrétien 2. Selon 
Strathern, l’emprunt que fait Darwin transforme également l’analogie 
elle-même, entraînant des conséquences qui apparaissent rétrospecti-
vement encore plus spectaculaires. Car c’est bien par l’effet retour, par la 
trajectoire inverse qui fait repasser l’analogie choisie à son point d’origine, 
1. Foucault 1966.
2. Strathern 1992, Beer 1983.

	
foucault et le biopouvoir	
213
que le modèle de liens familiaux qu’il emploie pour rendre un système 
biologique visible change la signification de ces liens – en un mot, il  
les naturalise. Ces relations en viennent alors à être perçues comme des 
« relations biologiques », et c’est par le biais de cet effet retour que tout 
un ensemble de concepts émerge 1. Parmi eux, l’idée désormais familière 
de parenté naturelle ou biologique, ainsi que celles de famille, d’identité 
et de population.
Si l’on ajoute cette autre nouveauté qu’est la naissance, au milieu du 
xxe siècle, de la génétique moléculaire moderne qui situe dans les gènes 
l’origine des relations biologiques, on obtient alors la séquence complète 
des transformations qui s’avèrent essentielles pour comprendre la façon 
dont émerge le modèle du biopouvoir dans l’œuvre de Foucault, et la 
manière dont ce concept se retrouve impliqué dans nombre des change-
ments ayant affecté la vie sociale et les sciences du vivant depuis la seconde 
moitié du xxe siècle. En effet, tout comme l’utilisation des liens familiaux 
par analogie avec le système naturel naturalise en retour les liens généa-
logiques ; tout comme la forme donnée à la biologie par analogie avec le 
monde naturel biologise la nature ; la génétisation de la biologie succède 
à la biologisation de la nature. Pour faire écho au style de Foucault, on 
pourrait assurer que, jusqu’à la fin du xxe siècle, il n’existe tout simplement 
pas de parenté génétique : ce n’est qu’après que l’adjectif « génétique » est 
devenu un synonyme de celui de « biologique » et, ce faisant, commence 
à en faire dériver la signification – de la même manière que la parenté 
biologique a auparavant déplacé les relations définies par le sang –, que 
l’idée selon laquelle chaque personne a deux parents génétiques devient 
commune et comme allant de soi.
Cette suite de changements liant nature, biologie et génétique a des 
conséquences décisives sur l’introduction du terme de « biopouvoir », 
ainsi que sur ses interprétations ultérieures. Je reviendrai plus loin sur 
les questions importantes que la génétique et la génétisation soulèvent 
vis-à-vis du biopouvoir, ainsi que sur les raisons pour lesquelles la génétique 
nouvelle est devenue l’un des domaines où son utilisation est la plus 
répandue. Cependant, il peut être utile, auparavant, de se pencher sur 
les circonstances précises de la naissance du biopouvoir, terme que 
Foucault utilise dans un sens particulier dans le premier volume d’Histoire  
de la sexualité.
1. Franklin 2013.

214	
sarah franklin
 
Histoire de la sexualité
Axé principalement sur la sexualité, le livre développe l’idée que la 
sexualité fonctionne comme un contexte pour la formation du sujet. 
Cela permet à Foucault de questionner les liens qui existent entre sexe et 
pouvoir, puis de suggérer un nouveau modèle de gouvernementalité, ou 
plus précisément un nouveau mode de gouvernement des populations. 
Jouant simultanément sur plusieurs niveaux de discours – ce qui lui a 
été reproché – Foucault traite du sujet individuel mais aussi du système 
dans et à travers lequel ce sujet émerge et adopte des comportements 
et des représentations (de lui-même, du corps, de la société). Ce faisant, 
il s’attache à théoriser la façon dont le processus de formation du sujet 
se déploie dans et à travers les formes de pouvoir, et la manière dont le 
pouvoir se retrouve à fonctionner pour donner forme aux identités indivi-
duelles. Cependant, comme le signalent très justement Mark Cousins  
et Athar Hussain, Foucault entre très rarement dans une discussion de ce 
qu’est le pouvoir en tant que tel – il le fait plutôt à travers la discussion 
de ce qu’il n’est pas.
Ainsi, toutes les remarques que formule Foucault au sujet du pouvoir s’ins-
crivent soit en négatif – en rejetant les acceptions courantes des relations de 
pouvoir –, soit comme des protocoles généraux portant sur l’analyse de ces 
relations 1.
Cousins et Hussain décrivent la théorie du pouvoir de Foucault comme 
étant « idiosyncrasique », s’érigeant à partir « d’une série de remarques 
disparates, certaines relativement banales, d’autres tout à fait frappantes 
et nouvelles, d’autres encore laconiques ou prêtant à confusion après un 
examen plus attentif ». Ces remarques, disent-ils, « fourmillent d’images 
et d’analogies empruntées aux thèmes de la guerre, des rapports de force, 
de la relation entre roi et sujets, du berger et de son troupeau, etc. 2 ». 
Cependant, s’il s’agit bien d’un propos idiosyncrasique général et riche 
en clichés, il offre en même temps, et pour la première fois, une théorie 
du biopouvoir ; en effet, la façon nouvelle dont il rend compte du pouvoir 
comme d’un champ polymorphe se heurte de plein fouet aux modèles 
hiérarchiques abondamment utilisés par la pensée politique. Foucault 
avance l’idée provocatrice selon laquelle la loi hégémonique et ce qu’il 
1. Cousins et Hussain 1984 (p. 225).
2. Ibid. (p. 225 pour les deux citations).

	
foucault et le biopouvoir	
215
appelle « l’appareil juridico-discursif » reposant sur la contrainte, la 
répression et la subordination, ne peuvent être ni les dépositaires ni les 
sources du pouvoir. Il propose pour y faire pendant une notion combinant 
d’autres analogies, celles d’un mode « pastoral » du pouvoir articulé autour 
des notions d’orientation et de soin, et dont la clé peut être trouvée 
dans la relation protectrice qui unit le prêtre (le berger) à son peuple 
(le troupeau). C’est en mariant le pouvoir personnel et intime qu’exerce 
le prêtre sur ses ouailles, illustré par la relation de confession, au champ 
fluide et polymorphe du pouvoir rendu par le préfixe « bio », que Foucault 
caractérise les contours d’un tournant historique marqué par la naissance 
d’un nouveau type de pouvoir. Cela lui permet aussi de décrire la façon 
dont le pouvoir assoit son effet en produisant des sujets qui incarnent 
dans leurs corps les moyens qu’il a de discipliner la vie.
Après la déclaration célèbre de Foucault selon laquelle le pouvoir n’est 
pas un nom propre, et qu’il n’existe pas en tant que substantif 1, il n’est pas 
surprenant de voir que ses thèses sur le biopouvoir apparaissent comme 
étant en contradiction avec ses théories précédentes et qu’elles ont, de 
manière générale, semé la confusion. C’est l’une des raisons pour lesquelles 
il est utile d’examiner la façon dont le biopouvoir fonctionne, plutôt que ce 
qu’il est ou comment il se définit. En effet, Foucault lui-même ne veut rien 
dire d’autre lorsqu’il répète que les fins du biopouvoir lui sont immanentes 
et résident dans ses moyens. Il suffit de se rappeler le recours constant 
chez Foucault aux images de réseaux, machines, appareils et dispositifs, 
pour se rendre compte de l’importance du concept de technologie dans 
ses théories du pouvoir et de la subjectivation, et donc de la place qu’il 
accorde aux rouages du pouvoir et à ses mécanismes. L’attention qu’il 
porte très tôt à la façon dont la connaissance fonctionne sur le mode 
d’une technologie anticipe d’ailleurs sa description ultérieure des techno-
logies du sexe, qui lui permettent de démontrer comment le biopouvoir 
assoit ses effets à travers un appareil fluide qu’il aurait pu qualifier de 
« biodiscipline ». La question de savoir pourquoi et comment le sexe est 
technologique, ainsi que les raisons pour lesquelles cette analogie est si 
cruciale dans le modèle foucaldien, vont nous permettre de mieux saisir 
la façon dont le concept s’est construit et a évolué à partir d’un modèle 
quasi anthropologique de parenté.
L’une des raisons pour lesquelles les modèles foucaldiens du sexe, 
de la technologie et du biopouvoir semblent plus intuitifs à la lectrice 
formée à l’anthropologie est que Foucault prête à la parenté une influence 
1. Foucault 1980.

216	
sarah franklin
structurante dans la vie sociale 1. Pour Claude Lévi-Strauss, dont la thèse 
sur l’anthropologie structurale publiée en 1949 constitue en partie le 
sujet de doctorat de Foucault (publié en 1961), les systèmes de parenté 
sont en même temps le fondement des lois et les sources premières de 
l’identité sociale : ils participent à la formation du sujet ou à sa discipline à 
travers un code fonctionnant au sein de la société comme une grammaire 
générale. Le rôle de la reproduction dans les travaux de Lévi-Strauss 
est ambigu. En effet, il y voit un bien qui « comme les mots doit être 
l’objet d’échanges », mais cette idée ne lui permet pas de rendre compte 
de manière satisfaisante du trafic universel des femmes dont il affirme 
qu’il découle du tabou de l’inceste inaugurant le passage de la nature à  
la culture 2. Beaucoup de travaux anthropologiques ont été critiqués de la 
même manière pour leur présupposé selon lequel la reproduction serait 
en même temps la raison d’être de la parenté et le produit des systèmes 
qu’elle met en place 3. Et ces liens circulaires portant sur la signification 
précise de la reproduction « biologique » au sein de l’anthropologie sociale 
ne sont pas exempts d’une forme d’ethnocentrisme vu l’inexistence de la 
catégorie dans un grand nombre de sociétés 4.
Quand Foucault retrace, dans Histoire de la sexualité, le passage de ce 
qu’il appelle le pouvoir « souverain » au biopouvoir, il porte attention au 
rôle du roi, et en particulier à son droit de condamner à mort, selon son 
bon vouloir, n’importe lequel de ses sujets. Mais la structure de pouvoir 
à laquelle s’attache en réalité Foucault est le système de parenté de l’aris-
tocratie, et en particulier les moyens de succession et de transmission 
mis en place. Il estime que ces systèmes portent avant tout sur la perpé-
tuation de la lignée, et donc sur le contrôle de la reproduction comme 
moyen d’assurer des héritiers. Cette mise en avant du mariage, de la repro-
duction et de la succession comme dirigés vers l’intérieur tranche avec la 
fonction reproductive orientée vers l’extérieur avancée par Lévi-Strauss : 
selon lui les mariages entre proches parents, enfants d’une même lignée, 
et l’endogamie sont vus comme des menaces de renfermement dont le 
tabou de l’inceste constitue l’antidote. En d’autres mots, bien que les 
deux auteurs partagent le même intérêt pour le détail des tactiques, la 
mécanique ou la grammaire du contrôle de la reproduction, l’important 
pour Lévi-Strauss est bien de se marier à l’extérieur de la communauté, 
tandis que pour Foucault l’essentiel est de trouver à se marier en son sein.
Foucault décrit ainsi « un système de mariage, de fixation » régnant 
1. Butler 1989 (p. 606-607).
2. Rubin 1975.
3. Strathern 1988.
4. Strathern 1980.

	
foucault et le biopouvoir	
217
dans l’aristocratie jusqu’au xviiie siècle, combiné à des « mécanismes 
de contrainte » dont l’un des « objectifs principaux » est de « reproduire 
le jeu des relations et de maintenir la loi qui les régit ». Il identifie ainsi 
la stratégie principale de succession, de contrôle de la reproduction  
et donc de préservation de l’aristocratie, à un « dispositif d’alliance » 
qu’il estime être « ordonné sans doute à une homéostasie du corps social 
qu’il a pour fonction de maintenir ; de là son lien privilégié avec le droit ; 
de là aussi le fait que le temps fort pour lui, c’est la “reproduction” 1 ». 
Au milieu du xviiie siècle, cependant, alors que l’industrialisation se 
met en place, « le dispositif d’alliance » ne rencontre plus autant de 
succès et se voit remplacer par un nouvel appareil décrit par Foucault 
comme le dispositif de sexualité. Ce nouvel ensemble « n’est pas ordonné 
à la reproduction » mais est lié « dès l’origine à une intensification du 
corps – à sa valorisation comme objet de savoir et comme élément dans 
les rapports de pouvoir 2 ». Contrairement à la fonction reproductrice 
du monde social aristocratique – c’est-à-dire la perpétuation de règles 
permettant de pouvoir tabler sur la transmission à travers le temps d’un 
statut et de biens via un dispositif fixe de parenté –, la famille moderne 
obéit au principe inverse. Elle est basée moins sur des mécanismes de 
transmission réglés que sur « des techniques mobiles, polymorphes et 
conjoncturelles de pouvoir ». Engendrant « une extension permanente 
des domaines et des formes de contrôle », celles-ci se manifestent « par 
des relais nombreux et subtils » dont le principal relais « est le corps 3 ». 
Ainsi, le nouveau système de parenté de la classe bourgeoise montante 
articulée autour de la famille repose non pas sur le maintien d’un ordre 
et d’une succession assurée par un dispositif d’alliance, mais plutôt sur 
un système plastique et instable alimenté par un renforcement du rôle du 
sexe – une sexualisation de la famille nucléaire qui « depuis le xviiie siècle 
[est devenue] un lieu obligatoire d’affects, de sentiments, d’amour » insti-
tuant une économie du plaisir 4. Une proposition originale de Foucault 
consiste à dire que ce moyen de discipliner la généalogie est d’une part 
sans précédent, et d’autre part qu’il ne fonctionne que parce que imbriqué 
dans des formes spécifiques de connaissance, telles la biologie moderne 
et, plus tard, la génétique.
Contrairement au « dispositif d’alliance » focalisé sur la propriété, la 
lignée et le maintien d’une stabilité homéostatique, la famille bourgeoise 
moderne se concentre, selon Foucault, sur la sexualité. La menace de 
1. Foucault 1976 (p. 140).
2. Ibid. (p. 141), souligné par moi.
3. Ibid. (p. 140-141).
4. Ibid. (p. 143).

218	
sarah franklin
l’inceste joue dans ce modèle un rôle tout différent de celui imaginé par 
Lévi-Strauss, n’étant ni l’origine stimulante d’échanges par le mariage 
(exogamie), ni une conséquence sublimée de la préservation de la lignée 
(endogamie). Le tabou de l’inceste incite plutôt de manière paradoxale 
à l’intensification de la sexualité à l’intérieur de la famille nucléaire. 
L’explosion des discours concernant la sexualité – les activités, spectacles 
et attractions sexuelles aussi bien que les manifestations de déviations, 
perversions et prohibitions – est interprétée par Foucault comme attestant 
que la sexualité à cette époque est non pas tant « réprimée » qu’exacerbée. 
La famille devient en particulier « le foyer le plus actif de la sexualité » où 
l’inceste « est sans cesse sollicité et refusé », faisant du foyer bourgeois 
« un foyer d’incitation permanente à la sexualité 1 ». Cependant, il est 
important de souligner avec Foucault qu’au xixe siècle cette nouvelle 
technologie du sexe qu’il appelle sexualité n’a pas besoin de déplacer entiè-
rement les « relations de sexe » établies par le dispositif d’alliance ; elle 
en altère simplement les mécanismes en leur superposant une « ortho-
pédie » qui remplace les « vieilles catégories morales de la débauche  
ou de l’excès » par des valeurs comme la santé, l’hygiène et le contrôle 
de la reproduction 2. Foucault affirme en particulier que « le nouveau 
dispositif qui s’est superposé » au dispositif d’alliance « se branche sur les 
partenaires sexuels ; mais selon un tout autre mode 3 », et suivant diffé-
rents principes.
Foucault est très explicite sur ce point. Dans Histoire de la sexualité il 
explique qu’une nouvelle forme de « responsabilité biologique » émerge 
au sein de l’espèce humaine à travers ce qu’il appelle le noyau « perver-
sion-hérédité-dégénérescence ». Il écrit notamment que
l’analyse de l’hérédité plaçait le sexe (les relations sexuelles, les maladies 
vénériennes, les alliances matrimoniales, les perversions) en position de 
« responsabilité biologique » par rapport à l’espèce : non seulement le sexe 
pouvait être affecté de ses propres maladies, mais il pouvait, si on ne le contrôlait 
pas, soit transmettre des maladies, soit en créer pour les générations futures 4.
C’est de cette nouvelle responsabilité biologique vis-à-vis de l’espèce, 
partagée par l’État et par ses sujets, que découlent les impératifs médicaux, 
politiques, mais aussi personnels et familiaux d’encadrement des conduites 
sexuelles. Ces dernières se font moyen de contrôle de la population et 
1. Ibid. (p. 143-144).
2. Ibid. (p. 155-156).
3. Ibid. (p. 140).
4. Ibid. (p. 156).

	
foucault et le biopouvoir	
219
de son futur à travers la gouvernance administrative des naissances, des 
mariages, de la fertilité, mais aussi des maladies, des perversions et des 
comportements. C’est dans ce nouvel espace épistémologique qui prend 
l’espèce humaine ou « l’Homme » pour objet que Foucault voit la vie, le sexe 
et la population assujettis à un nouveau devoir de contrôle biologique. Il 
y a là un tournant crucial qui précipite « l’entrée des phénomènes propres 
à la vie de l’espèce humaine dans l’ordre du savoir et du pouvoir ainsi que 
dans le champ des techniques politiques 1 ». Il s’agit bien, en somme, de 
l’entrée de la vie dans l’Histoire, et de la naissance du biopouvoir.
Le moyen par lequel les forces de reproduction animant le système  
de parenté aristocratique se retrouvent déplacées par l’administration 
du sexe au début du xixe siècle illustre le lien entre les technologies 
proposées par Foucault et la théorie qu’il propose du pouvoir ou de la 
discipline. Il nous permet aussi d’élucider les raisons pour lesquelles il en 
vient précisément à être appelé « biopouvoir », et de mieux comprendre 
pourquoi la façon dont ce dernier opère préoccupe plus Foucault que  
de savoir ce sur quoi il porte exactement. On peut alors imaginer 
que, en ce début des années 1970, Foucault aurait pu entreprendre de 
développer, à titre d’exemple, des études de cas similaires à celles qu’il a 
réalisées sur les prisons, les asiles ou la clinique, et examiner des sujets 
comme le contrôle des naissances, la pornographie ou les maladies  
vénériennes.
Mais il n’en est rien et Foucault oriente son attention dans une direction 
moins prévisible pour laquelle l’analogie du pasteur et de son troupeau est 
centrale. Car bien que Foucault tire ses notions de réseaux et de dispositifs 
de ses travaux sur la production des savoirs, de la vérité et des relations 
qu’elles entretiennent avec les mots et les choses, il est chez lui une autre 
préoccupation, celle qui met l’accent sur un ensemble de conventions 
pour lesquelles il emploie l’expression de « technologies de la chair ». En 
utilisant ce mot, Foucault a dans l’idée un acte de communication très 
précis, en l’occurrence le rôle de la confession au sein de l’Église catholique. 
La relation du pénitent avec le père confesseur représente une analogie 
capitale dans la théorie que fait Foucault du pouvoir comme autodisci-
pline ou « volonté » intériorisée. Elle éclaire en effet non seulement l’objet 
des nouvelles formes de « responsabilité biologique » qui échoient aussi 
bien à l’État qu’à ses sujets, mais aussi les mécanismes qui relient cette 
volonté des sujets aux tactiques de l’État.
Sans une théorie des formes de conduite et des dispositifs opéra-
toires du pouvoir, Foucault ne peut en effet expliquer comment les 
1. Ibid. (p. 141-142).

220	
sarah franklin
sujets deviennent si désireux de s’intégrer dans l’appareil de leur propre 
subordination. Il ne peut pas non plus rendre compte des incitations 
et proliférations qu’il décrit en parlant de la société victorienne qui en 
vient à être en même temps produite et formée à travers la discipline de 
la conduite sexuelle et la poursuite de la responsabilité biologique. Selon 
Foucault, cette transformation correspond à la façon dont la « techno-
logie traditionnelle de la chair » sous la forme de la conduite pastorale 
chrétienne, et l’élan témoigné pour l’expression de la pénitence à travers la 
confession, évoluent en une « nouvelle technologie du sexe » administrée 
par la médecine, la pédagogie, la psychiatrie, la démographie et l’État. 
C’est ainsi que le sexe devient « une affaire où le corps social tout entier, 
et presque chacun de ses individus, était appelé à se mettre en surveil-
lance 1 ». C’est ainsi que l’accompagnement pastoral anglican se voit 
remplacé par la médecine du xixe siècle et par des « campagnes à propos 
de la natalité [qui] déplacent, sous une autre forme et à un autre niveau, 
le contrôle des rapports conjugaux ». C’est enfin de cette façon que 
« la question de la mort et du châtiment éternel » devient le « problème 
de la vie et de la maladie 2 ».
Dans ce qui suit, nous regardons comment la notion de biopouvoir est 
mise en œuvre dans les sciences sociales depuis trois décennies, et quels 
paradigmes nouveaux voient le jour sous son influence. Pour ce faire, 
les quelques fils conducteurs évoqués jusqu’ici sont centraux. Parce que 
dans la théorie de Foucault le pouvoir est moins hiérarchique et punitif 
que générateur et formateur, cette compréhension résonne particuliè-
rement bien avec les questions que pose « l’âge du contrôle biologique », 
les questions d’identité, de gouvernance ou de discipline des corps. De la 
même manière, l’importance toujours actuelle des structures de parenté, 
tout comme la nature imprévisible des changements affectant la signifi-
cation et les enjeux du contrôle de la reproduction, font que le concept 
de biopouvoir voit ses implications renouvelées et renforcées par la 
fécondation in vitro, la dérivation des cellules souches embryonnaires  
ou la possibilité de cloner les mammifères supérieurs telle la brebis Dolly. 
Les sujets que Foucault n’a pas abordés durant sa vie – comme le contrôle 
de la population, le recensement, les inégalités homme-femme ou les 
sciences de la reproduction – laissent suffisamment de place pour que 
nombre de chercheurs les déploient. L’intérêt que Foucault a manifesté 
de longue date pour l’eugénisme, le souci de soi ou la surveillance de la 
santé et des maladies mène de façon prévisible à une application large 
1. Ibid. (p. 154).
2. Ibid. (p. 155).

	
foucault et le biopouvoir	
221
de ses idées dans les domaines de la génétique, de l’épidémiologie, du 
développement des produits pharmaceutiques, dans ce que Vinh-Kim 
Nguyen a décrit comme la culture de l’« expérimentalité » en biomédecine 
et les sciences de la vie 1. Comme nous allons le voir, l’intérêt de Foucault 
pour le pastoralisme prend aussi de nouveaux aspects dans un contexte 
néolibéral où l’introduction de stratégies de pouvoir douces telles que  
1. Nguyen 2009.
La brebis Dolly, premier mammifère cloné en laboratoire 
à partir d’un noyau de cellule somatique adulte, en 1996.

222	
sarah franklin
les mesures incitatives entre en écho avec les contributions toujours renou-
velées que sa théorie du pouvoir idiosyncrasique continue d’apporter. 
Toutes choses qui vont nous permettre de passer en revue les évolutions 
de la pensée politique et sociale contemporaine.
Les évolutions du biopouvoir
Le concept de biopouvoir est parfaitement adapté à de nombreux 
phénomènes sociopolitiques du xxe siècle et du début du xxie tels que 
la culture de tissus 1, l’agriculture ou l’élevage modernes 2, l’embryologie 3, 
l’adoption massive de la pilule contraceptive par voie orale 4. Bien que  
de nombreux chercheurs aient tiré parti des idées de Foucault 5, les appli-
cations les plus concluantes de son concept de biopouvoir commencent 
surtout à la fin du xxe siècle en réponse à l’entreprise de séquençage du 
génome humain – ce qui est peu surprenant puisque le biopouvoir est 
ancré explicitement, nous l’avons dit, dans les champs de la généalogie, 
de la population et de l’avenir de l’espèce.
L’anthropologue Paul Rabinow, qui compte parmi les plus grands spécia-
listes du travail de Foucault, est l’un des premiers à identifier l’intérêt du 
concept de biopouvoir pour appréhender les enjeux du « projet Génome 
humain ». C’est lui qui introduit en 1992 le terme de « bio­socialité » pour 
désigner les nouveaux aspects de la formation du sujet et des commu-
nautés qui résultent de la circulation de l’information portant sur les 
gènes humains et les pathologies génétiques. Selon Rabinow, la génétique 
nouvelle confirme la prédiction que Foucault a faite de l’entrée de la vie 
même dans le régime de l’administration humaine, et ce à une ampleur 
inégalée. En s’appuyant sur la définition que donne Foucault du biopouvoir 
comme « ce qui a amené la vie et son mécanisme dans le domaine du 
calcul explicite et a fait du pouvoir de la connaissance un agent de trans-
formation de la vie humaine », Rabinow explique que le projet Génome 
humain est technologique de deux façons : d’abord dans le sens ordinaire 
où il emploie des machines, mais aussi dans le sens plus complexe où 
l’on est en présence de connaissances qui engendrent leur objet. Dit 
autrement, l’objectif du projet Génome humain n’est pas seulement de 
reconstituer la séquence du génome humain ; il consiste aussi à produire 
1. Landecker 2007.
2. Fitzgerald 1990, Ritvo 1987.
3. Hopwood 2000, Keller 2003.
4. Watkins 1998.
5. Clarke 1998, Palladino 2003, Murphy 2012.

	
foucault et le biopouvoir	
223
une nouvelle connaissance des gènes qui puisse permettre de détecter et 
contrôler ceux-ci, que ce soit à l’intérieur des corps ou à travers l’ensemble 
de la population. Il s’ensuit, selon Rabinow, que cette nouvelle connais-
sance du génome humain s’imbrique dans le tissu social, non seulement 
par le biais de nouveaux types de savoirs et d’interventions médicales, 
mais aussi à travers des formes d’affiliation sociale et de formation des 
sujets – ce qu’il décrit comme une « forme absolument nouvelle d’auto-
production… que j’appelle biosocialité 1 ». À travers celle-ci, la culture n’est 
plus modelée sur la nature, mais devient plutôt un modèle indépendant 
qui refait la nature en « technique ».
Dans ce contexte inédit d’« autoproduction », des sujets nouveaux 
émergent en relation à des instruments d’administration des corps atteints, 
des corps à risques, des corps potentiellement compromis par leurs gènes. 
Sa théorie identifie comme clé la combinaison d’une gouvernemen-
talité du risque génétique à des fins de prévention avec l’inculcation de 
nouvelles formes d’identités biologiques – le risque d’être porteur d’une 
mutation génétique par exemple. Dans le modèle de biosocialité proposé 
par Rabinow, une extension sans précédent du contrôle politique sur 
le vivant peut se déployer – en lien explicite entre de nouvelles formes  
de savoir scientifique et d’identité génétique, un mandat de détection et 
d’intervention en présence de pathologies génétiques (pour le compte 
des générations présentes ou futures), une perception de ces processus 
par ceux qu’ils affectent directement comme les signes d’un progrès, 
voire d’une libération, etc.
Parmi les nombreux théoriciens qui développent des analyses similaires 
à celles de Rabinow, les chercheurs en sciences sociales suivent plutôt 
l’émergence parallèle des nouvelles technologies et des identités génétiques. 
Rayna Rapp, par exemple, mène depuis quinze ans une étude ethnogra-
phique sur les tests chromosomiques prénataux dans la ville de New York 2. 
Elle s’intéresse de près à la façon dont les femmes enceintes interprètent 
et assimilent un nombre toujours croissant d’informations génétiques. 
Ses conclusions se rapprochent de la description que fait Foucault du 
changement survenu à la fin du xviiie siècle quand les formes anciennes 
de parenté se retrouvent non pas tant supplantées par, que fusionnées 
avec les formes nouvelles de familles associées au dispositif de sexualité. 
Contrairement au tableau que fait Rabinow de la disparition de la séparation 
nature-culture et de l’idée « moderne » de société, Rapp décrit l’inter­­
action qui opère entre « des traditions [parentales et familiales] plus 
1. Rabinow 1992 (p. 241).
2. Rapp 1999.

224	
sarah franklin
anciennes et plus profondes » (telle l’importance accordée à la ressem-
blance physique ou à des « croyances populaires informelles ») et des 
catégories scientifiques ou professionnelles telles que les typologies 
utilisées pour interpréter les anomalies détectées dans les gènes 1. Le 
suivi des régimes d’identification du risque génétique ainsi que le rejet 
assez répandu des « catégories émergentes de la biomédecine et des 
sciences apparentées » sont interprétés par Rapp comme témoignant non 
seulement de la « distribution inégale des retombées scientifiques » dans 
la vie sociale contemporaine, mais aussi des résistances rencontrées par 
ces nouvelles catégories 2. Il est particulièrement intéressant de constater 
le rôle décisif joué contre toute attente par la religion chez la plupart des 
personnes interrogées, et la façon dont il résonne avec la mise en avant 
par Foucault des anciennes « technologies de la chair » et des logiques 
qui les distinguent des « technologies du sexe ».
C’est un tableau très différent des identités biopolitiques émergeant 
dans le contexte des nouvelles technologies génétiques que brosse Adriana 
Petryna. Dans une étude qui a fait date sur les survivants de Tchernobyl, 
celle-ci témoigne de leurs luttes pour s’engager dans une politique de 
calculs biologiques et devenir ce qu’elle appelle des « citoyens biolo-
giques ». Comme elle le fait remarquer, le lien entre biologie et identité 
est tout sauf récent, mais les pratiques spécifiques à travers lesquelles 
cette liaison s’opère, comme les identités qui en résultent, diffèrent à 
toutes les échelles. Comme l’ont observé de nombreux chercheurs, la 
compréhension nouvelle de l’identité génétique qu’a engendrée l’identifi-
cation de certaines mutations moléculaires telles que BRCA1 ou 2 n’a pas 
seulement donné lieu à l’émergence de nouvelles formes de communautés 
et d’activisme social. Elle est aussi devenue un facteur majeur de dévelop-
pement économique pour l’industrie pharmaceutique et le secteur des 
biotechnologies, alimentant la montée d’entreprises tentaculaires telles 
que Genzyme, ainsi que la commercialisation et la privatisation rapides 
des séquences de gènes 3. Pour Petryna, ces mutations ont vu l’émergence 
parallèle de nouvelles formes de citoyenneté – telles que celles de ces 
Ukrainiens au patrimoine biogénétique menacé et qui se sont retrouvés 
à développer des moyens de poser des chiffres sur leurs symptômes afin 
de pouvoir prétendre à des prestations compensatoires de la part des 
autorités administratives. De tels citoyens biologiques regroupent un 
nouveau type de sujets biologiquement instruits, rompus à l’analyse de 
1. Ibid. (p. 242).
2. Ibid. (p. 303).
3. Petryna 2003. Voir aussi Rajan 2006.

	
foucault et le biopouvoir	
225
la grammaire administrative conçue pour identifier et intervenir sur le 
risque génétique. Ils apparaissent aussi comme des sujets classiques du 
biopouvoir au sens foucaldien, dans une configuration où l’atteinte biolo-
gique devient un moyen de protection et d’accès au statut de membre 
d’un groupe social reconnu par l’État.
Dans un rapport récent sur la biomédicalisation, Adele Clarke et ses 
collègues estiment
qu’une nouvelle économie biopolitique de la médecine, de la santé, de la 
maladie, de la vie et de la mort […] forme [dorénavant] un espace incroya-
blement dense et élaboré dans lequel les connaissances biomédicales, les 
technologies, les services et le capital interagissent et se constituent de plus 
en plus les uns les autres 1.
Ces préoccupations face à l’expansion rapide du domaine de la biopo-
litique conduisent de nombreux théoriciens à relier le biopouvoir au 
contrôle économique par le biais de concepts tels que le « biocapital 2 », 
la « biovaleur 3 », la « biodisponibilité 4 », et plus récemment le « capital 
vivant 5 ». D’autres chercheurs, tels que Giorgio Agamben et João Biehl, ont 
entrepris d’examiner l’envers du biopouvoir à travers l’idée de la production 
d’une non-vie, d’une « vie nue » ou encore de « zones d’abandon », dans 
une démarche similaire à celle de Judith Butler dans son introduction au 
concept de « vie précaire ». À travers ces différentes formes, un changement 
de perspective décisif s’opère avec l’analyse de la gouvernementalité comme 
d’un appareil qui non seulement permet mais requiert la production de 
la non-vie.
En ligne avec ce qu’on peut attendre d’une théorie « idiosyncrasique », 
un autre débat a lieu à propos de la relation entre gouvernementalité et 
souveraineté, et de leur lien à la vitalité ou à « la vie elle-même ». Comme 
le précise Foucault en 1978 dans la conférence qu’il lui consacre au Collège 
de France, la gouvernementalité décrit la façon dont la vie de la population 
est administrée, et les moyens par lesquels les relations entre l’adminis-
tration et son ou ses objet(s) sont légitimées et maintenues. Selon lui, 
une différence majeure entre la gouvernementalité et la souveraineté est 
à trouver dans le fait que celle-là est « tactique » et consensuelle, tandis 
que celle-ci est répressive, punitive et hiérarchique. Cependant, nul n’est 
1. Clarke et al. 2010 (p. 1).
2. Franklin 2001, Franklin et Lock 2003, Rose 2006, Rajan 2006.
3. Waldby 2002.
4. Cohen 2005.
5. Rajan et al. 2012.

226	
sarah franklin
besoin pour la gouvernementalité de remplacer le pouvoir souverain pour 
assurer sa domination ou même son hégémonie. Comme le note Judith 
Butler, l’autorité de la loi peut être convertie en une forme tactique de 
souveraineté, comme cela s’est produit aux États-Unis après les attaques 
du 11 Septembre sur le territoire américain. Elle écrit ainsi dans sa 
description de la vie précaire :
Bien que Foucault fasse ce qu’il appelle une distinction analytique entre le 
pouvoir souverain et la gouvernementalité, suggérant ainsi à plusieurs reprises 
que celle-ci est une forme tardive du pouvoir, il maintient néanmoins la possi-
bilité que ces deux formes de pouvoir puissent coexister et qu’elles coexistent 
de fait de plusieurs façons, notamment en relation avec cette forme du pouvoir 
qu’il appelle la « discipline » 1.
Selon Butler, l’aptitude de ces différentes formes de pouvoir non 
seulement à coexister mais aussi à se combiner est l’une des raisons 
pour lesquelles la théorie foucaldienne demeure aussi instable qu’indis-
pensable : le fait qu’une gouvernementalité de la population réalisée à 
travers la biopolitique se superpose à un ordre social antérieur ne signifie 
pas que le second ait à disparaître. Rabinow fait une remarque similaire 
en défendant l’idée que l’émergence de la biosocialité ne sanctionne pas 
« un changement d’époque marqué par une cohérence totalisante » mais 
rassemble plutôt une série d’événements, d’épisodes et d’alignements 
inégaux et fragmentés.
On constate donc, à travers l’expansion de l’usage des termes de biopo-
litique et de biopouvoir, un double changement d’échelle et de point 
d’application : parce que la production de la vie en est venue à être perçue 
comme la production de la non-vie et comme un mélange de formes 
anciennes et nouvelles de gouvernement, la mission attribuée à la biopoli-
tique s’en trouve revisitée. Cela correspond à l’interprétation sociologique 
plus générale faite au début du xxie siècle du biopouvoir comme consti-
tuant un espace de négociation socialement ambivalent.
Le travail de l’historienne Michelle Murphy offre un exemple type 
de ce phénomène – à travers l’analyse qu’elle mène de la « biopolitique 
féministe » dans un contexte d’explosion des technologies de la repro-
duction 2. Comme elle le constate, la transformation du sexe en objet de 
gouvernement n’opère pas seulement dans les laboratoires et les cliniques, 
mais également dans les administrations d’État, les ONG, les organi-
sations humanitaires et les organisations supranationales telles que la 
1. Butler 2004 (p. 53-54).
2. Murphy 2012.

	
foucault et le biopouvoir	
227
Banque mondiale ou le FMI. Elle écrit ainsi qu’un trait caractéristique 
du xxe siècle est à trouver dans la façon dont
des schémas nationaux et transnationaux ont encouragé à grande échelle 
la limitation technologique des naissances par la distribution de millions 
de pilules contraceptives, de stérilets et d’autres formes de contraception 
chirurgicale, intervenant ainsi sur la fertilité de populations entières au 
prétexte de bénéfices économiques à venir. La modification de la repro-
duction sous sa forme agrégée de « population » est ainsi devenue un problème 
de transition planétaire traité via des solutions techniques, étatiques et  
commerciales 1.
Comme Murphy le note, cette transformation de la reproduction 
en question de gestion des populations et de généalogie assistée non 
seulement affecte la fertilité humaine mais englobe aussi les mécanismes 
reproductifs des plantes et des animaux. Et les mutations technologiques 
s’accompagnent d’une expansion de la « responsabilité biologique », de 
la « responsabilisation » vis-à-vis des générations futures 2.
Dans l’étude qu’elle consacre à la façon dont les groupes de féministes 
américaines cherchent à intervenir sur les politiques de reproduction, 
Murphy emploie le terme d’« entremêlements » pour décrire le processus 
d’une parole faite en réponse aux formes du pouvoir biopolitique et qui 
a pourtant pour résultat de réinscrire les sujets dans les termes mêmes 
qu’ils cherchent à éviter. Murphy introduit à cet égard l’expression de 
« contre-conduite technoscientifique » pour désigner le processus par 
lequel « les réappropriations des moyens de reproduction » par chacun(e) 
peuvent être décrites comme des formes locales de résistance qui défient 
et simultanément étendent l’emprise du biopouvoir sur les sujets.
Murphy analyse ainsi une série d’« expérimentations sociotechniques », 
tels les efforts du mouvement pour la santé des femmes, à partir de 1970, 
pour offrir à chacune les moyens de prendre en charge sa propre santé 
gynécologique. Murphy met en avant les emmêlements qui apparaissent 
alors : en utilisant des méthodes comme l’autoexamen, ou des campagnes 
pour l’accès aux frottis vaginaux ou aux contraceptifs, les féministes 
contribuent à une réponse au biopouvoir qui simultanément étend sa 
mission et redistribue ses effets. La procréation devient
un problème aux effets distribués dans l’espace et le temps, matériel autant que 
politique, et qui est rattaché aux questions de l’État, de la race, de la liberté, 
1. Ibid. (p. 1).
2. Ibid. (p. 2).

228	
sarah franklin
de l’individu et de la prospérité économique par des liens entre le microlo-
gique et le transnational via la problématique du corps.
Murphy se penche ainsi sur les politiques (ambivalentes) qui sont 
nécessaires au renforcement du pouvoir des femmes via la mobilisation 
des technologies, des sciences, de la médecine clinique et de l’appareil 
d’État. En décrivant le « nœud des généalogies 1 » qui convergent autour 
du concept de reproduction, et en démontrant comment le projet de 
responsabilisation doit être compris comme un terrain où des pouvoirs 
se manifestent « par capillarité », Murphy offre une étude de cas élégante 
de la dialectique à l’œuvre. Composé d’une superposition d’appareils 
institutionnels et disciplinaires, de catégories normalisatrices et de 
l’éthique du contrôle biologique « responsabilisant », le biopouvoir repose 
également sur les mains tendues de celles qui cherchent, en se prenant en 
charge, à intervenir dans ce contrôle biologique. Les deux « envers » du 
biopouvoir – venant « d’en haut » et représenté par les formes tradition-
nelles du pouvoir souverain via les droits de réquisition et d’occupation, 
par exemple, et « d’en bas », de citoyens non désireux de se responsabiliser 
et de reconnaître l’autorité du savoir biologique – continuent d’offrir de 
nouveaux cas d’école, tout en nous rappelant la tension inévitable entre 
hiérarchie verticale et géographie horizontale. Les changements que 
Foucault fait lui-même subir à l’adjectif « généalogique », en écho aux 
voies de traverse, détours et permutations étranges que connaît l’emprise 
du pouvoir sur ses sujets, reflètent précisément ce sens antidirectionnel 
et son ambivalence.
Conclusion
La montée des régimes de pouvoir néolibéraux conduit à des tactiques 
d’« inductions-incitations », d’intelligence collective, d’essaimages viraux, 
de communications instantanées et de partages sur les réseaux sociaux 
souvent animées par des volontés paradoxales de contrôle, possession, 
classification, stratification et exclusion. Dans ce contexte, les idées de 
Foucault ont vu leur valeur et leur crédibilité renforcées de manière specta-
culaire. Elles montrent qu’il n’est nul besoin de restreindre le concept de 
biopouvoir au « contrôle de la vie », qu’il se laisse au contraire appliquer à 
un ensemble de phénomènes plus vaste. Le préfixe « bio » connaît à notre 
époque des transformations qui le font passer de référence littérale au 
1. Ibid. (p. 7).

	
foucault et le biopouvoir	
229
vivant à signe de technicité, et les évolutions futures pourront mener à une 
dé-biologisation encore plus marquée. Cela s’accompagnera toutefois, très 
certainement, d’une re-biologisation permettant d’englober une totalité 
plus grande – le réchauffement climatique par exemple, phénomène 
quasi biologique à l’origine de l’apparition d’une nouvelle catégorie géolo-
gique, celle de « l’Anthropocène ». Alors que pour Foucault la relation 
de l’anthropos aux sciences humaines est faite de fascination viscérale, 
Bruno Latour se focalise sur les sujets, objets et agents non humains 
à l’intérieur des sciences humaines, sociales et politiques. De manière 
parallèle, Donna Haraway, qui a anticipé l’importance des tactiques contre- 
biologiques combinées à l’intervention d’acteurs technologiques et non 
humains, se tourne vers des modèles d’interaction symbiotique qui préfi-
gurent encore un autre tournant possible de la biopolitique. Il pourrait 
s’agir cette fois du renversement du rapport de la gouvernementalité à la 
vitalité à travers des interactions liant, par exemple, les activités de colonies 
d’algues à la santé humaine et aux schémas migratoires. Politiquement 
peu simples à lire, de telles connexions ne pourront être interprétées à 
partir des modèles courants du pouvoir politique, économique ou social. 
La mise en avant par Foucault de la nature idiosyncrasique du contrôle 
pourrait alors signifier que les modèles qu’il propose figurent parmi  
les plus puissants restant à développer si l’on veut comprendre et prédire 
la façon dont les effets du pouvoir sont susceptibles d’être engendrés et 
habités à l’avenir.
Traduit par Clara Breteau
Références bibliographiques
Agamben Georgio, 1998, Homo sacer : Sovereign Power and Bare Life, Palo Alto (CA), 
Stanford University Press.
Beer Gillian, 1983, Darwin’s Plots : Evolutionary Narrative in Darwin, George Eliot 
and Nineteenth-Century Literature, Londres, Routledge & Kegan Paul.
Biehl João Guilherme, 2005, Vita : Life in a Zone of Social Abandonment, Berkeley 
(CA), University of California Press.
Butler Judith, 1989, « Michel Foucault and the Paradox of Bodily Inscriptions », 
The Journal of Philosophy, vol. 86, no 11, p. 601-607.
–	 2004, Precarious Life : The Powers of Mourning and Violence, Londres, Verso.
Clarke Adele E., 1998, Disciplining Reproduction : Modernity, American Life Sciences, 
and the Problems of Sex, Berkeley (CA), University of California Press.
Clarke Adele E., Mamo Laura et Fosket Jennifer Ruth (dir.), 2010, Biomedicaliza-
tion : Technoscience, Health, and Illness in the US, Durham (NC), Duke Univer­­sity 
Press.
Clough Patricia Ticineto et Willse Craig, 2011, Beyond Politics : Essays on the 
Governance of Life and Death, Durham (NC), Duke University Press.

230	
sarah franklin
Cohen Lawrence, 2005, « Operability, Bioavailability, and Exception », in Aihwa Ong 
et Stephen Collier (dir.), Global Assemblages : Technology, Politics and Ethics as 
Anthropological Problems, Oxford, Blackwell Publishing, p. 79-90.
Cousins Mark et Hussain Athar, 1984, Foucault, Londres, Macmillan.
Fitzgerald Deborah K., 1990, The Business of Breeding : Hybrid Corn in Illinois 
(1890-1940), Ithaca (NY), Cornell University Press.
Foucault Michel, 1966, Les Mots et les choses. Une archéologie des sciences humaines, 
Paris, Gallimard.
–	 1976, Histoire de la sexualité, t. 1 : La Volonté de savoir, Paris, Gallimard.
–	 1978, « Governmentality », Ideology and Consciousness, no 6, p. 5-12.
–	 1980, Power  / Knowledge : Selected Interviews and Other Writings (1972-1977), 
Brighton, Harvester Press.
Franklin Sarah, 2013, Biological Relatives : IVF, Stem Cells and the Future of Kinship, 
Durham (NC), Duke University Press.
–	 2001, « Culturing Biology : Cell Lines for the Second Millennium », Health, vol. 5, 
no 3, p. 335-354.
Franklin Sarah et Lock Margaret (dir.), 2003, Remaking Life and Death : Toward 
an Anthropology of the Biosciences, Santa Fe (NM), School of American Research 
Press.
Hopwood Nick, 2000, « Producing Development : The Anatomy of Human Embryos 
and the Norms of Wilhelm His », Bulletin of the History of Medicine, vol. 74, no 1, 
p. 29-79.
Keller Evelyn Fox, 2003, Making Sense of Life : Explaining Biological Development 
with Models, Metaphors and Machines, Cambridge (MA), Harvard University 
Press.
Landecker Hannah, 2007, Culturing Life : How Cells Become Technologies, Cambridge  
(MA), Harvard University Press.
Lévi-Strauss Claude, 1969 [1949], The Elementary Structures of Kinship, trad. 
J.H. Bell, J.R. von Sturmer et R. Needham, Londres, Tavistock.
Murphy Michelle, 2012, Seizing the Means of Reproduction : Entanglements of Feminism, 
Health, and Technoscience, Durham (NC), Duke University Press.
Nguyen Vinh-Kim, 2009, « Government-by-Exception : Enrolment and Experimen-
tality in Mass HIV Treatment Programmes in Africa », Social Theory and Health, 
vol. 7, no 3, p. 196 – 217.
Palladino Paolo, 2003, Plants, Patients and Historians : On (Re) membering in the 
Age of Genetic Engineering, New Brunswick (NJ), Rutgers University Press.
Petryna Adriana, 2003, Life Exposed : Biological Citizens after Chernobyl, Princeton 
(NJ), Princeton University Press.
Rabinow Paul, 1992, « Artificiality and Enlightenment : From Sociobiology to Bioso-
ciality », in Jonathan Crary et Sanford Kwinter (dir.), Incorporations, New York, 
Zone Books, p. 234-252.
Rajan Kaushik Sunder, 2006, Biocapital : The Constitution of Postgenomic Life, 
Durham (NC), Duke University Press.
–	 (dir.), 2012, Lively Capital : Technologies, Ethics, and Governance in Global Markets, 
Durham (NC), Duke University Press.
Rapp Rayna, 1999, Testing Women, Testing the Fetus : The Social Impact of Amniocentesis 
in America, New York, Routledge.
Ritvo Harriet, 1987, The Animal Estate : The English and Other Creatures in the 
Victorian Age, Cambridge (MA), Harvard University Press.

	
foucault et le biopouvoir	
231
Rose Nikolas, 2006, The Politics of Life Itself : Biomedicine, Power, and Subjectivity in 
the Twenty-First Century, Princeton (NJ), Princeton University Press.
Rubin Gayle, 1975, « The Traffic in Women : Notes on the “Political Economy” of 
Sex », in  Rayna Reiter (dir.), Toward an Anthropology of Women, New York, 
Monthly Review Press, p. 157-210.
Schneider David M., 1986, American Kinship : A Cultural Account, Englewood Cliffs 
(NJ), Prentice Hall.
Strathern Marilyn, 1980, « No Nature, No Culture : The Hagen Case », in Carol 
MacCormack et Marilyn Strathern (dir.), Nature, Culture and Gender, 
Cambridge, Cambridge University Press, p. 174-222.
–	 1988, The Gender of the Gift : Problems with Women and Problems with Society in 
Melanesia, Berkeley (CA), University of California Press.
–	 1992, After Nature : English Kinship in the Twentieth Century, Cambridge, 
Cambridge University Press.
Waldby Cathy, 2002, « Stem Cells, Tissue Cultures and the Production of Biovalue », 
Health, vol. 6, no 3, p. 305-323.
Watkins Elizabeth Siegel, 1998, On the Pill : A Social History of Oral Contraceptives 
(1950-1970), Baltimore (MD), Johns Hopkins University Press.


11 Les savoirs de l’économie
T i m o t h y  S h e n k  
e t  T i m o t h y  M i t c h e l l
Depuis Adam Smith et les physiocrates, l’ascension de la « science 
économique » est autant déplorée qu’acclamée, et les dernières années 
du xixe siècle sont marquées par une accélération des efforts pour mettre 
sur pied une « véritable » discipline. De nouvelles institutions destinées à 
accueillir des économistes apparaissent, tandis que les idées redéfinissent 
le domaine et les techniques propres à son étude.
Aujourd’hui, les économistes jouissent d’une influence dont leurs 
prédécesseurs n’ont jamais rêvé. L’économie est devenue la science 
sociale hégémonique et bénéficie, à travers le monde, d’une autorité 
sans équivalent sur la politique et les affaires. Elle constitue un nouveau 
domaine de compétence pour les gouvernements, le domaine « écono-
mique », et elle a refondu certains des concepts les plus capitaux du 
siècle comme celui de « marché », doté aujourd’hui d’une importance  
totémique.
Au début du xxe siècle, pourtant, loin de chercher à faire de leur disci-
pline la science sociale dominante, les économistes sont plutôt occupés 
à poser les bases d’une science unifiée centrée sur la « question sociale ». 
Sociologues, politologues, anthropologues, psychologues ont alors un 
rôle à jouer au côté de l’économie, et il est courant de voir l’économie 
traitée comme une branche de cette recherche plus large, ou de trouver 
des définitions qui voient en elle ce que les fameux Principes écono-
miques d’Alfred Marshall appellent « les aspects économiques » de la vie 
sociale de l’homme 1. Au fil du xixe siècle, les visions de la société comme 
système fonctionnant selon des logiques propres et indépendantes de l’État 
gagnent du terrain. Et ceux qui pratiquent les bien nommées « sciences 
1. Marshall 1890.
 Calculateur MONIAC (Monetary National Income Analogue Computer) ou « machine de Phillips », 
créé en 1949 par l’économiste William Phillips pour modéliser l’économie nationale britannique.

234	
timothy shenk et timothy mitchell 
sociales » tentent d’utiliser leurs compétences pour développer le potentiel 
(et maîtriser les excès) de forces (économiques autant que sociales) qu’ils 
voient comme en passe de transformer le monde.
Pour bien comprendre l’histoire de l’économie au xxe siècle, il est 
nécessaire de faire cohabiter deux éléments : l’avenir que les économistes 
pensent être en train de construire, et l’avenir qu’ils ont contribué à créer 
en réalité. Au tournant des xixe et xxe siècles, l’économie fait partie d’une 
science sociale cherchant à comprendre et apprivoiser une entité décou-
verte depuis peu et connue sous le nom de « capitalisme ». Cent ans plus 
tard, les économistes produisent des modèles toujours plus élaborés 
qui cherchent à explorer les caractéristiques essentielles d’objets – telle 
l’économie – revendiqués par la discipline comme terrains propres. Ils 
s’appuient pour ce faire sur des outils et données fournis par une vaste 
chaîne de production qui achemine des moyens colossaux vers des milliers 
d’économistes, franchissant sans entraves les barrières censées exister 
entre le « public » et le « privé ». La circulation entre frontières intellec-
tuelles est aussi aisée – et les économistes comportementaux utilisent  
la psychologie pour construire un homme économique plus complexe. 
On voit aussi couramment des économistes se lancer dans des entreprises 
impériales et tentant d’annexer des objets appartenant aux champs voisins 
et les ramener à leurs modes d’analyse. Mais l’écart qui s’est creusé entre 
cette version de l’économie et les aspirations de ses fondateurs est vaste, 
et ses conséquences sont considérables.
La construction d’une discipline
Cela fait bien sûr des siècles que l’on débat, en tous lieux, de sujets 
catégorisés comme « économiques ». Dans bien des cas cependant, les 
parties au débat n’ont que peu ou pas de formation spécifique. À la fin 
du xixe siècle, ceux qui cherchent à faire de l’économie une discipline 
s’appuient sur quelques antécédents, mais ceux-ci sont d’un faible secours 
vu l’échelle des redéploiements à accomplir. Revues spécialisées, chaires 
universitaires, associations professionnelles et instituts de recherche se 
multiplient, tandis que les universités deviennent le terrain privilégié de 
la formation en économie. Il n’est alors plus question de continuer à voir 
les économistes comme ayant simplement une vision du monde un peu 
particulière. Ils sont désormais les produits d’une formation spécialisée, 
la constitution de la discipline s’opérant de façon plus ou moins rapide 
selon les lieux. Au début du xxe siècle, ce qui a été principalement un 
passe-temps d’amateur s’est transformé en carrières professionnelles et 

	
les savoirs de l’économie	
235
les bases sont jetées d’un schéma d’organisation académique en passe de 
se répandre à travers la planète.
Dans le monde anglophone, la transition passe par l’adoption du terme 
economics qui indique une volonté d’éloignement du champ de l’« économie 
politique » considéré comme trop ancien et pas assez rigoureux. Un débat 
connexe, parmi les économistes allemands et autrichiens et baptisé du 
nom de Methodenstreit, oppose les défenseurs d’une approche historique 
de la vie économique aux partisans d’une science à forts soubassements 
théoriques et dont les constructions seraient basées sur des principes 
fondamentaux. Bien qu’attachés initialement à des lieux géographiques 
précis, ces débats se déploient à travers le globe et, selon les normes 
allemandes ou anglaises, la France est en retard. Pendant une bonne partie 
du xxe siècle, toutefois, les chercheurs qui tentent d’articuler l’économie 
au droit, à l’histoire, à la sociologie ou aux autres sciences sociales ont 
peu à craindre du contingent bien plus réduit d’ingénieurs et de mathé-
maticiens qui vantent les mérites de leurs « calculs économiques 1 ».
L’Europe et les États-Unis du tournant du siècle ne sont pas les seuls à 
connaître l’avalanche de statistiques qui accompagne les activités écono-
miques ; et la production (comme l’analyse) de ces statistiques n’est pas 
uniquement, ni même principalement, le fait d’économistes universitaires. 
Le savoir auquel les statistiques économiques promettent d’accéder est 
en effet bien trop utile et rentable pour rester confidentiel. Le chemin de 
fer, le télégraphe et les autres domaines qui font le nouveau monde de 
l’entreprise et qui bousculent radicalement les sociétés, rendent indispen-
sable la création de nouveaux types de données qu’un secteur financier 
dynamique cherche à faire connaître aux investisseurs potentiels. La 
Compagnie américaine des téléphones et télégraphes est pionnière dans 
la fabrication de ces données statistiques aux États-Unis, par exemple, 
mais les détracteurs du système jouent un rôle non négligeable. Ainsi 
des syndicats qui sont décisifs dans l’enregistrement des statistiques du 
chômage. Les journaux et autres entreprises dédiés au suivi détaillé de ce 
que beaucoup appellent la « vie économique » commencent à proliférer : le 
Wall Street Journal est lancé en 1883, suivi peu de temps après par l’index 
boursier des valeurs industrielles de Charles Dow, le Dow Jones Indus-
trial Average. Tous deux rendent compte d’une communauté active qu’ils 
tendent à solidifier de par leur existence. Ce faisant, les données et statis-
tiques qu’ils présentent se trouvent dotées d’un sens nouveau et puissant.
De nombreux chercheurs en économie partagent cette foi dans le 
pouvoir des statistiques. Auparavant, de Ricardo à Walras, même les 
1. Etner 1987.

236	
timothy shenk et timothy mitchell 
économistes ayant une passion pour les mathématiques donnent peu 
de place aux données quantitatives dans leurs travaux. Dans la seconde 
moitié du xixe siècle, en revanche, de plus en plus d’économistes, et parti-
culièrement ceux dotés d’un penchant historique, sont convaincus que les 
statistiques sont essentielles à l’analyse économique et à sa boîte à outils. 
Le mouvement est puissant en Allemagne et est soutenu ailleurs par un 
réseau de bureaux dédiés à l’observation du travail et le flot régulier d’éco-
nomistes partant se former dans ce pays. La Première Guerre mondiale 
fait perdre cette prérogative à l’Allemagne mais l’expansion de la recherche 
économique que le pays a contribué à lancer s’installe dans la durée. À la 
fin des années 1920, les instituts de recherche en économie exercent 
une influence inégalée, que ce soit le Bureau national de recherche en 
économie américain ou l’Institut Varga dans l’Union soviétique de Staline.
Intellectuellement, on décrit souvent l’histoire intellectuelle de l’éco-
nomie au début du xxe siècle comme l’aboutissement d’une révolution 
lancée par les marginalistes dans les années 1870. Cette version n’est que 
la caricature d’une histoire qui est en fait beaucoup plus intéressante et 
diversifiée. Il est vrai que ceux que l’on appelle plus tard les marginalistes 
substituent à l’étude des cycles de production et de consommation, et des 
La politique des prix
Cette importance de la texture sociale de la vie économique aide à mieux 
rendre compte de la popularité des politiques réformistes chez les économistes 
de l’époque. Le marginalisme, souvent présenté comme une rationalisation de 
l’exploitation vidant l’économie politique de son potentiel radical, peut alors 
conduire au socialisme. Comme le contemporain et rival de Marshall, Edwin 
Cannan, le fait remarquer depuis sa chaire à la London School of Economics : 
« La doctrine de l’utilité marginale étiquette comme économiques de nombreuses 
choses qui auparavant ne pouvaient être qualifiées qu’en des termes “senti-
mentaux” ou “non économiques” » – une extension dont les socialistes tirent 
parti pour se positionner comme les champions à la fois de la justice sociale 
et de l’usage efficace de ressources limitées 1. Du point de vue des mathéma-
tiques, il n’y a pas non plus de différence entre un équilibre atteint par le jeu  
du marché et un équilibre atteint via une planification centralisée. On débat  
d’ailleurs beaucoup de la capacité des socialistes à fixer des prix rendant les 
marchés lisibles, et lorsque John Maynard Keynes annonce en 1926 « la fin du 
“laisser-faire” », il fait écho à une position largement répandue 2.
1. Cannan 1903 (p. 405).
2. Keynes 1926.

	
les savoirs de l’économie	
237
formes de distribution de la valeur entre classes sociales, une probléma-
tique centrée sur les agents économiques individuels qui, s’appuyant sur 
les prix pour maximiser leur utilité, arrivent de façon non préméditée à 
l’équilibre de marché. Cependant, jusqu’à la Seconde Guerre mondiale, 
l’économie se signale bien plus par son pluralisme que par la force d’un 
quelconque consensus. Les structures institutionnelles, la gestion des 
ressources naturelles, les rouages de l’empire, les complexités de la finance 
ou la question des races ne sont que quelques-uns des sujets qui disputent 
alors la première place aux travaux rassemblés sous le nom trompeur 
mais désormais familier d’« économie néoclassique ».
Contrairement aux stéréotypes qui font aujourd’hui loi, il vaut aussi 
de rappeler que les travaux assimilés au canon marginaliste s’intéressent 
alors aux institutions. Les théories de Walras s’inscrivent dans un type 
de marché spécifique, celui d’une Bourse réglementée. Chez Marshall, 
les marchés ont des qualités tangibles similaires, même s’il les présente 
plutôt comme des déclinaisons des places de marché urbaines. Tous deux 
situent donc leurs marchés dans des contextes éminemment sociaux.
La difficulté à distinguer clairement les marginalistes de leurs collègues 
ne signifie pas pour autant que les économistes n’ont aucun désaccord 
profond. Mais vouloir lire cette période de façon téléologique comme un 
prologue à ce que l’économie allait devenir ne fait que rendre les choses 
incompréhensibles. Au tournant du xxe siècle, l’économie hésite entre 
deux voies qui bénéficient toutes deux d’un riche héritage intellectuel : 
devenir la science de la gestion de la nature et des ressources, ou celle des 
prix et de l’argent. On décrit à l’époque cette alternative comme le choix 
entre donner la priorité à l’« efficacité » ou à la « rareté ». La première 
fait des économistes les médiateurs entre monde matériel et monde 
social ; la seconde entre société et univers de flux monétaires suivant 
leurs logiques propres.
Il n’est pas alors aisé de dire laquelle des deux va l’emporter. Pendant 
des centaines d’années, la vie économique a été tenue comme insépa-
rable de la production agricole. Les départements d’agriculture, aux 
États-Unis, sont souvent le refuge des économistes, et quelques-uns 
parmi les plus grands, comme John Kenneth Galbraith, ont suivi des 
études d’économie agricole. Jusque dans les années 1970 les manuels 
pour débutants contiennent souvent un chapitre sur l’agriculture, vestige 
et signe de ce qui a été autrefois l’élément constitutif de la discipline. Les 
économistes ont entrepris de déchiffrer l’une des facettes du compor-
tement de l’homme en société, mais leurs explorations du champ social 
ont conduit beaucoup d’entre eux à le redéfinir et à refuser la quaran-
taine mettant la nature à part du social.

238	
timothy shenk et timothy mitchell 
Un autre élément, essentiel même si opportunément oublié, occupe 
les esprits dans les premières décennies du xxe siècle – la ressource 
inestimable qu’est la pureté raciale. Irving Fisher, par exemple, l’un des 
économistes les plus acclamés de son vivant, est un fervent partisan 
de l’eugénisme. Dans son esprit, économie et eugénisme sont insépa-
rables et constituent les deux faces de la richesse nationale qu’il étudie. 
Cette position l’affilie au courant majoritaire de la profession, aux côtés 
d’hommes tels qu’Edward Ross, l’un des membres fondateurs de l’Asso-
ciation américaine d’économie et père de la formule « suicide de la race », 
une variante de la préoccupation traditionnelle de l’économie politique 
pour la gestion des populations 1. Le déploiement des données et des outils 
techniques de la statistique est aussi organiquement lié à l’eugénisme : 
les analyses biométriques de populations conduisent au développement 
de techniques statistiques très novatrices qui justifient certes les messages 
eugénistes, mais qui ont leur puissance propre et qui sont largement 
reprises, dans la suite, par l’économétrie.
Dans les années 1920, les économistes peuvent tirer un bilan positif de 
ce qui a été accompli : ils sont devenus plus essentiels au gouvernement 
des activités économiques, la structuration de la discipline se poursuit  
à un rythme soutenu et les autres institutions dédiées à la production  
de savoirs économiques (et relevant du monde des affaires) continuent de 
se développer. L’argent afflue à un niveau sans précédent via un ensemble 
d’organisations philanthropiques, d’agences étatiques et de centres de 
recherche technique. Cependant, les travaux des économistes ont beau 
avoir une portée plus grande, leur influence reste limitée. La diversité 
intellectuelle entretient de vives discussions, et cette hétérogénéité fait 
qu’il est aisé d’écarter l’économie comme une occupation de nature univer-
sitaire, trop divisée sur elle-même pour être utile au gouvernement des 
choses. On prête une oreille polie à ceux qui prédisent qu’une révolution 
conceptuelle est imminente, mais les contours de cette transformation 
restent flous.
La fabrique de l’Économie
Durant les années 1930, l’économie en tant que discipline (economics) 
participe à la fabrication de l’Économie (the economy) comme nouvel 
objet – et la gestion de cet objet devient la préoccupation principale 
des gouvernements dans les décennies suivantes. La réorganisation de 
1. Ross 1901 (p. 88).

	
les savoirs de l’économie	
239
la vie politique autour de cet objet, et l’importance des savoirs écono-
miques nécessaires à sa gestion, confèrent une importance nouvelle aux 
économistes professionnels. Enjambant les incertitudes et désaccords 
du passé, ceux-ci commencent à revendiquer une place de choix au sein 
des sciences sociales et dans la décision publique.
Au cours des décennies précédentes, le mot « économie » désigne 
généralement un processus et non un objet. En lien avec l’idée de « gouver-
nement », l’économie a pour sens l’administration ou l’utilisation efficace 
des ressources et moyens matériels. Dans les formules telles qu’« économie 
politique » ou « économie sociale », elle renvoie non seulement à la 
« bonne » gestion de la vie mais aussi aux modes de savoir techniques 
et administratifs utiles à cette gestion. Au milieu du xxe siècle, l’idée 
de « l’Économie » (utilisée maintenant en anglais avec l’article défini, 
et ici avec une majuscule) prend un sens nouveau. Les économistes 
commencent à utiliser le terme pour désigner la totalité des relations de 
production, d’échange et de consommation à l’intérieur d’un territoire 
donné. Ce changement reflète l’usage de nouvelles méthodes statistiques 
qui visent la mesure et le suivi des relations monétarisées à l’échelle des 
pays. Il reflète aussi des changements dans les dimensions matérielles  
et sociales de la vie, notamment la montée des bureaucraties étatiques et 
des administrations gestionnaires dans les firmes. Il signale finalement le 
passage, pour les grandes puissances industrielles, de l’ère de l’expansion 
territoriale et des politiques impériales à la priorité donnée au gouver-
nement de l’État-nation et à la gestion de la crise.
Une invention majeure contribue à la fabrication de l’Économie, la 
mise en place des statistiques du revenu national. Dans l’entre-deux-
guerres, des statisticiens tels que Simon Kuznets aux États-Unis mettent 
au point des méthodes pour mesurer ce qui sera connu plus tard comme 
le produit national brut (PNB). Imaginant que le monde économique 
est constitué de trois grandes entités – les ménages, les entreprises et le 
gouvernement –, les calculs du revenu national produisent une estimation 
mensuelle des échanges entre ces unités et présentent le total comme 
reflétant l’évolution d’un objet nouveau : l’Économie nationale, vue alors 
comme une unité systémique. D’autres forces perturbent certes le système 
de l’extérieur, mais l’économie peut à présent être conçue comme un objet 
dynamique, distinct des autres processus sociaux et sujet à ses propres 
lois de fonctionnement, empiriquement déchiffrables. Le gouvernement 
n’est toutefois pas le seul à développer de nouveaux usages autour de 
ces catégories et données. Des changements sont à l’œuvre en parallèle : 
la montée du nombre de grandes entreprises cotées, l’utilisation quoti-
dienne des écritures et de la monnaie de papier, le paiement de l’impôt 

240	
timothy shenk et timothy mitchell 
sur le revenu par une proportion grandissante de ménages, etc. – toutes 
activités qui multiplient les façons dont la vie collective est réglée par des 
calculs monétaires et comptables, avec pour résultat la production d’un 
monde unifié par le nombre et qui peut dès lors être imaginé et repré-
senté comme une Économie.
À la même époque, la Grande Dépression attire l’attention sur les travaux 
de l’économiste anglais John Maynard Keynes (et sur des recherches 
similaires dans d’autres pays). La Théorie générale de Keynes contribue 
à déplacer le regard des microétudes centrées sur les agents et les prix 
sur des marchés spécifiques à ce qui sera connu plus tard sous le nom 
de « macroéconomie » et qui regarde la production nationale, l’emploi, 
le revenu, l’épargne, l’investissement, l’offre de monnaie et les taux 
d’imposition. Les gouvernements s’appuient sur ces idées pour calculer 
ces nouveaux agrégats et introduire des programmes pour gérer leurs 
interactions. Ce faisant, les concepts qu’invente l’économie-discipline en 
viennent à faire partie du monde que les économistes prétendent décrire. 
Et les expérimentations parallèles de planification en Union soviétique, 
comme le travail des économistes de l’Allemagne de Weimar ou ceux de 
la Société des Nations, renforcent l’émergence de l’Économie comme 
objet de connaissance.
Une troisième évolution caractéristique de la période, et vitale pour 
comprendre le changement que nous cherchons à caractériser, est le 
développement de l’économie mathématique. L’éditeur et conseiller en 
investissements Alfred Cowles, frustré par l’inexactitude des prévisions 
boursières, finance le développement de techniques mathématiques et 
statistiques pour faire de l’économie une science plus précise. Il fonde une 
revue, Econometrica, et un institut de recherche, la Commission Cowles, 
chargée de produire des modèles généraux des processus économiques. 
Des systèmes d’équations sont au point de départ de ces savoirs. L’infor-
mation statistique est désormais collectée moins comme une description 
du monde réel que comme échantillon de données à introduire dans les 
modèles mathématiques, desquels les lois économiques sont induites. Plus 
tard, les abstractions de l’économie mathématique compliqueront consi-
dérablement la tâche des économistes visant la définition de politiques 
économiques de portée pratique. L’abstraction même de la science contribue 
cependant à renforcer alors, par contraste, l’effet de réel de l’objet Économie 
auquel les modèles et les données semblent se rapporter 1.
Finalement, des méthodes plus pratiques de calcul et de gestion de 
l’économie nationale se développent durant la Seconde Guerre mondiale. 
1. Pour un autre aperçu sur ces modèles, voir Armatte et Dahan (dans ce volume, p. 339).

	
les savoirs de l’économie	
241
Les gouvernements doivent en effet maximiser leur production indus-
trielle, contrôler les prix et salaires, gérer l’allocation du travail et des 
matières premières pour être en mesure de répondre aux besoins civils 
et militaires. Ils regroupent donc des économistes dans des bureaux de 
planification où de nouvelles méthodes d’estimation et de modélisation 
des interactions entre variables macroéconomiques sont développées. 
Et ces efforts amènent la science économique sur le devant de la scène.
Un dernier point : la Seconde Guerre mondiale confirme la fin de 
l’ère des empires coloniaux à travers lesquels les grandes puissances ont 
organisé production et commerce via leur mainmise sur les territoires. 
Or les réseaux enchevêtrés de l’impérialisme mondial rendent difficile 
le calcul des processus économiques en termes d’unités territoriales 
autosuffisantes. Alors que les puissances industrielles desserrent leur 
emprise, perdent leurs empires et se recentrent sur la gestion de leurs 
affaires économiques nationales, il devient soudainement possible, pour 
la pensée économique et la pratique gouvernementale, d’organiser ces 
affaires à l’échelle d’une nation – et de son Économie.
Un âge d’or de la croissance  
et des modélisations mathématiques ?
À la fin de la guerre, les populations des puissances industrielles attendent 
de leurs gouvernements la mise en place de sociétés égalitaires basées 
sur de hauts salaires, l’amélioration de la protection sociale, l’arrêt (aux 
États-Unis) de la ségrégation raciale et, dans de nombreux cas, le renfor-
cement du rôle des travailleurs dans la gestion et la propriété des moyens 
de production. Déployant les méthodes récemment apprises, les écono-
mistes apportent une autre réponse : au lieu de redistribuer propriétés et 
revenus, les gouvernements peuvent mener des politiques de « croissance ».
La science économique a souvent été confrontée aux changements – à 
la croissance de la population, l’expansion du commerce, les excès ou 
pénuries de ressources naturelles, l’inflation. Dans le contexte du New 
Deal, les politiques de relance, qui impliquent la remise en service d’usines 
et la création d’emplois pour les chômeurs, sont comprises comme s’ins-
crivant dans des cycles de croissance et de contraction de l’activité, un 
mouvement désigné sous le nom de « cycles économiques ». L’expansion 
n’est donc pas une fin en soi - même si l’étude des cycles économiques 
conduit à définir les moyens d’en niveler les variations, bridant si néces-
saire les mouvements d’expansion pour diminuer l’ampleur des récessions, 
ou cherchant à réduire les goulets d’étranglement (qui peuvent entraver 

242	
timothy shenk et timothy mitchell 
la sortie de crise) dans l’offre de marchandises ou de travail. Après 1945, 
en revanche, la « croissance » signifie tout autre chose : l’objet que les 
politiques gouvernementales se donnent n’est pas de faire croître la 
population, le commerce, les ressources ou la richesse, mais quelque 
chose de plus vaste et moins matériel, et surtout capable de connaître, 
on l’espère, un développement continu : l’Économie.
Les économistes utilisent aussi la croissance pour penser et conduire 
les relations entre pays industrialisés et anciennes colonies, et le maître 
mot en est le « développement économique ». Dans l’entre-deux-guerres, 
la Grande-Bretagne et les autres puissances impériales introduisent des 
programmes de développement colonial, espérant ainsi réduire les forces 
d’opposition par une meilleure exploitation des ressources et l’amélio-
ration des conditions de vie des populations. Après guerre, les économistes 
américains qui travaillent (notamment) avec la Banque mondiale utilisent 
la mesure du PNB et de sa croissance pour construire un nouvel avenir 
pour les Suds. Ayant rejeté les demandes des nouveaux États indépendants  
tels que l’Inde et qui portaient sur la révision des termes du commerce 
international et le transfert de capitaux du Nord vers le Sud, les États-Unis 
se tournent vers la science de la croissance économique comme réponse à 
la pauvreté et aux inégalités globales. Ces programmes de développement 
ne réduisent pas l’écart entre les niveaux de richesse et de qualité de vie 
au Nord et au Sud, mais ils offrent une fois de plus à la nouvelle science 
la possibilité de définir et modeler les politiques publiques.
Cette affaire du développement n’est qu’un des exemples du poids 
grandissant qu’exercent après guerre les économistes en tant que gestion-
naires de l’économie et experts en efficacité. À la même époque, des agences 
comme la Fondation Ford encouragent l’exportation de la version améri-
caine de la science économique. Des différences nationales persistent, 
mais la science économique américaine, ou du moins une version très 
sélective et étroite de celle-ci, en vient à dominer l’enseignement et la 
recherche au niveau mondial.
Le mouvement qui fonde la discipline sur la modélisation mathéma-
tique des phénomènes économiques installe une lingua franca qui donne 
son jargon à la profession. De la démarche quasiment philosophique 
qu’elle était à ses débuts, la science économique achève sa conversion en 
ingénierie. Rien ne représente mieux cette transformation que la théorie 
de l’équilibre général développée par Kenneth Arrow et Gérard Debreu 
au début des années 1950. Le modèle Arrow-Debreu montre que l’éco-
nomie dans son ensemble, et pas seulement les marchés de marchandises 
spécifiques, peut être modélisée mathématiquement comme en équilibre 
concurrentiel – comme un système dans lequel, sur la base d’une série 

	
les savoirs de l’économie	
243
d’hypothèses irréalistes comme celle de la concurrence parfaite, chaque 
producteur et chaque consommateur trouvent un prix réalisant l’équi-
libre entre offre et demande.
La théorie d’Arrow-Debreu devint une pièce maîtresse de la synthèse 
néoclassique – cette tentative d’origine principalement américaine de 
réconcilier la macroéconomie keynésienne avec la microéconomie néoclas-
sique de Léon Walras. De brillants successeurs de Keynes, tels les membres 
de l’école de Cambridge en Angleterre, voient d’un œil critique ces tenta-
tives visant à considérer l’économie dans son ensemble comme la simple 
agrégation de calculs économiques individuels. Cependant, le manuel 
de Paul Samuelson, Economics : An Introductory Analysis, et l’influence 
immense qu’il exerce assurent le succès de la synthèse. Publié une première 
fois en 1948 puis traduit dans plus de 40 langues, le manuel est enrichi 
et parfois amputé de certaines idées au fil de ses 19 éditions. Mais il 
assure avec succès la promotion de la mathématisation de la discipline 
et sa division en deux branches distinctes bien qu’apparemment compa-
tibles, la macroéconomie traitant de l’économie dans son ensemble et la 
microéconomie des marchés individuels.
Le triomphe du marché
Quand les années 1970 arrivent, la conjonction essentielle d’éléments 
qui a stimulé la montée en puissance de l’Économie se délite. Au grand 
plaisir de Milton Friedman et de ses amis, l’ordre institué par les accords 
de Bretton Woods s’effondre en 1971. À l’ère de ce qui commence à devenir 
la « mondialisation », ce signal n’est pourtant que le premier des défis que 
doivent affronter ceux qui cherchent à contrôler les économies nationales. 
Un nouveau néologisme destiné à devenir le pain quotidien des écono-
mistes et politiques l’indique, la « stagflation », qui décrit l’association 
d’une croissance économique stagnante et d’une inflation galopante. 
On présente souvent cette période comme le moment où l’âge d’or du 
capitalisme – les « Trente Glorieuses » en France – cède le pas au néoli-
béralisme, révélant ainsi la poigne de fer que dissimulait auparavant le 
gant de velours de la social-démocratie. Un examen plus attentif révèle 
toutefois une histoire beaucoup plus riche et intéressante.
D’abord, le dispositif de production des savoirs économiques constitués 
de l’entre-deux-guerres aux années 1950 et 1960 survit aux régimes 
politiques basés sur la planification nationale. La performance écono-
mique offre toujours une mesure des succès des gouvernements, on confie 
toujours aux économistes la tâche de concevoir les politiques publiques 

244	
timothy shenk et timothy mitchell 
capables de soutenir la croissance, et les connaissances en économie 
sont plus appréciées que jamais. La prospective économétrique se mue 
toutefois en partie en une industrie privée lucrative, évolution notable 
pour une discipline auparavant dominée par des chercheurs employés 
dans des administrations d’État et qui, avant la Grande Dépression, dénon-
çaient les analystes privés comme étant des charlatans.
Les institutions qui pilotent l’économie d’après guerre continuent à se 
développer, redirigées toutefois vers de nouveaux objectifs. Le président 
de la Réserve fédérale américaine, Paul Volcker, décrit l’essentiel de cette 
transformation et son côté paradoxal dans ces propos :
Nous sommes tous keynésiens à présent – explique-t-il – et ceci est lié à la 
façon dont nous regardons les choses. Les statistiques du revenu national 
appartiennent à une vision du monde keynésienne, et le langage des écono-
mistes est largement keynésien. Mais si l’on entend par « keynésien » l’idée 
qu’il faudrait mettre l’économie sous perfusion, que les relations qui la consti-
tuent sont claires et comprises, et que cela, si bien mis en œuvre, nous permet 
d’accéder à une prospérité éternelle, alors ça, c’est des conneries 1.
Les propos de Volcker laissent pourtant de côté le maître mot de ceux 
qui prétendent avoir dépassé les « conneries » keynésiennes : le marché, 
terme qui prend alors un sens très neuf. Jusqu’aux années 1950, les défen-
seurs du monde des affaires sont les gardiens de l’ensemble des institutions 
que rassemble le vocable de « libre entreprise ». Mais la notion de marché 
qui s’impose dans les années 1970 est un concept d’une autre nature, plus 
léger, plus abstrait et facile à employer. Contrairement aux bureaucraties 
pesantes et quasi dictatoriales des États, le marché est (ou plutôt est 
présenté comme) un lieu générique et flexible où des individus exerçant 
leur libre arbitre interprètent les informations renvoyées par les prix, 
calculent la trajectoire qu’ils doivent emprunter – ceci débouchant, pour 
la société, sur des résultats optimaux.
Alors que les défenseurs du marché s’étendent à loisir sur les mérites 
de la concurrence débridée, le marché lui-même se révèle assez difficile 
à caractériser. Cette ambiguïté autorise une schizophrénie tranquille, le 
marché étant à la fois une propriété universelle des relations humaines 
dont on ne peut défier le verdict sans se voir anéanti, et une chose vulné-
rable qui nécessite une protection de tous les instants vis-à-vis de ses 
potentiels saboteurs. Mais les concepts flous sont fort utiles, nous le 
savons, et ils servent souvent des objectifs concrets avec plus d’efficacité 
que leurs rivaux trop précis.
1. Cité par Neikirk 1986 (p. 107).

	
les savoirs de l’économie	
245
Au sein de la profession, parmi les économistes universitaires, l’obsession 
pour les marchés débouche sur une quête des microfondations néces-
saires à la théorie économique d’ensemble. Milton Friedman, lorsqu’il 
défend son alternative « monétariste » au keynésianisme, continue bien 
sûr à utiliser les agrégats qui constituent la base de la macroéconomie 
d’après guerre. De même, les politiques et leurs experts continuent d’uti-
liser la plupart de ces outils – mais l’avant-garde académique les condamne 
comme infiniment rétrogrades : ce que requiert la science économique, ce 
sont des modèles qui arrivent à des conclusions valables pour l’économie 
en général à partir, et de l’intérieur, de la théorie de l’équilibre général.
Une solution est trouvée grâce à l’hypothèse des « attentes rationnelles ». 
Celle-ci pose comme principe que les individus sont des acteurs écono-
miques extrêmement ingénieux capables de comprendre les modèles 
que les économistes construisent pour expliquer leurs actions. Et sous 
ces conditions de « réflexivité », les politiques de stabilisation ne peuvent 
agir sur l’économie que si elles sont imprévisibles. Les partisans de cette 
économie « néoclassique » avancent donc une conception du temps et 
de l’espace économiques originale : le référentiel d’analyse pertinent n’est 
L’hypothèse d’efficience du marché
L’un des exemples les plus remarquables de cette foi nouvelle dans les marchés 
est l’acceptabilité grandissante de l’hypothèse d’efficience du marché (HEM). 
Celle-ci décrit les marchés financiers comme des processeurs d’information 
hors pair qu’aucun individu ne pourra jamais espérer dépasser. Cette façon de 
considérer le marché comme le dispositif idéal de traitement de l’information 
(plutôt que comme un outil d’allocation des ressources) est à la base de l’économie 
financière et transforme l’essentiel de la finance. Des « ingénieurs financiers » 
autoproclamés s’appuient sur l’HEM et d’autres innovations – la plus célèbre 
et populaire étant le modèle de valorisation des options Black-Scholes – pour 
créer des instruments financiers d’une complexité toujours plus grande. Certains 
prétendent que ces évolutions permettent aux investisseurs de gérer plus effica-
cement leur risque, voire de le supprimer. Sur les marchés financiers, l’argent 
coule à flots, facilitant la circulation du capital et de la production par-delà les 
frontières. Quand les années 1980 arrivent, l’ère de Bretton Woods a laissé place à 
ce qui sera appelé plus tard le « consensus de Washington ». Les recommandations 
de ses thuriféraires lient étroitement stabilité des prix, privatisations, libérali-
sation du commerce et déficits budgétaires minimaux. Les leaders politiques 
qui tentent d’infléchir la tendance sont mis en garde contre les châtiments que 
leur réservent des marchés d’obligations sans merci. À la base de ce système, on 
retrouve l’architecture financière que la théorie économique a rendue possible.

246	
timothy shenk et timothy mitchell 
plus l’Économie considérée comme un tout mais les décisions d’un Homo 
economicus prévoyant. Être capable de prévoir le résultat des interventions 
politiques implique une autre façon de gouverner au présent à travers 
le futur. Si les gens comprennent que les baisses d’impôt d’aujourd’hui 
sont les hausses de demain, ils ne vont pas dépenser leur argent, comme 
le prévoient les keynésiens, mais l’économiser. La théorie des attentes 
rationnelles bouche le trou qui permettait aux keynésiens de jouer de 
l’ambiguïté du présent pour amener les gens, malgré eux, sur le chemin de 
la prospérité. Alors que les critiques affluent, Robert Lucas, qui inaugure 
le tournant des attentes rationnelles depuis Chicago, prédit que « le terme 
de “macroéconomie” va tout simplement disparaître de la circulation » 
et n’être plus que le vestige d’une ère révolue 1.
Mais la prédiction de Lucas est fausse : la « macroéconomie » subsiste, 
notamment parce que les partisans de la théorie des attentes rationnelles 
ne réussissent pas à construire des modèles utiles à ceux qui gèrent effecti-
vement l’économie (ce qui malgré tout reste massif). Dans les années 1980, 
la dynamique suscitée par les champions des attentes rationnelles passe 
donc ailleurs, du côté des défenseurs des « cycles réels ». Cette nouvelle 
génération de théoriciens considère toujours les individus comme des 
acteurs rationnels, mais ils voient dans certains chocs « réels », comme 
la flambée des prix du pétrole, le lieu d’origine des fluctuations écono-
miques. Ce mouvement est à son tour contesté par ceux qui se décrivent 
comme néokeynésiens et qui démontrent que des modèles répondant 
aux critères des économistes néoclassiques peuvent être utilisés pour 
justifier les politiques keynésiennes – à condition d’être fondés sur des 
hypothèses appropriées. Selon eux, les variations de la demande agrégée 
peuvent être expliquées par des erreurs dans les prises de décision indivi-
duelles. Les sceptiques rejettent le néokeynésianisme, l’accusant de tenter 
par tous les moyens de rationaliser ses propres conclusions. Et les néokey-
nésiens eux-mêmes reconnaissent que l’on donne un poids analytique 
énorme à des sujets en apparence mineurs comme la vitesse à laquelle 
les prix et les salaires s’adaptent à des conditions de marché changeantes, 
alors que les moyens exacts par lesquels ces décisions individuelles se 
reflètent dans les outils conventionnels keynésiens comme la courbe de 
Phillips restent flous.
Mais ces scrupules sont vite abandonnés, notamment parce que la 
profession a besoin d’un consensus réconciliant gestion courante des 
affaires économiques et théorie économique de pointe. Un nouveau 
type de modèle, le modèle d’équilibre général dynamique (DSGE), pour 
1. Lucas 1987 (p. 107).

	
les savoirs de l’économie	
247
lequel les acteurs économiques sont rationnels et dirigés vers le futur 
tandis que les marchés se voient reconnaître toute une série d’imper-
fections, passe au cœur de la macroéconomie. La tentative d’équilibrer 
les macroagrégats économiques qui a occupé l’économie au milieu du 
siècle n’est plus qu’un lointain souvenir et, au tournant du millénaire, on 
célèbre le consensus atteint par la discipline comme jamais auparavant.
Les caricatures qui présentent les économistes comme les adorateurs 
de modèles désespérément abstraits et dénués de fondements empiriques 
oublient certaines évolutions récentes de la discipline. Du travail mené 
par Gary Becker dans les années 1960 sur le « capital humain » à la célèbre 
Freakonomics basée sur les travaux du collègue de Becker à l’université 
de Chicago, Steven Levitt, la portée de l’économie s’étend et englobe tout 
ce qui semble pouvoir se rattacher de près ou de loin à des questions de 
maximisation d’utilité – qu’on parle racisme, suicide ou autre. Les écono-
mistes comportementalistes cherchent le moyen de mettre sur pied  
des acteurs plus subtils dotés de rationalités limitées, loin du stéréotype 
du maximisateur d’utilité sans cœur. Dans les années 1980, la théorie 
des jeux devient omniprésente et facilite l’élargissement du champ de la 
discipline. Des contingents d’économistes formés durant leurs études à 
l’économétrie profitent de l’explosion de la quantité de données rendues 
disponibles par la sophistication croissante des ordinateurs pour réintro-
duire des pratiques de recherche empiriques au cœur de la profession.
La microéconomie ne met pas longtemps à démontrer son utilité hors 
de la discipline. Considérés comme les maîtres en calcul de taux d’effi-
cacité, les économistes décrochent des postes de consultants en entreprise, 
tandis que, sous l’impulsion du mouvement florissant « droit et économie » 
de Chicago, les cours en théorie des prix deviennent courants dans les 
formations juridiques. En raison des querelles qui l’agitent, la macroé-
conomie met plus de temps à attirer l’attention des hauts fonctionnaires. 
En 2006 encore, Gregory Mankiw constate, alors qu’il vient juste de 
quitter son poste de directeur du Conseil des conseillers économiques : 
« La triste vérité est que, durant les trente dernières années, la recherche 
en macroéconomie n’a eu en pratique qu’un impact mineur dans l’analyse 
des politiques fiscale et monétaire 1. » Bien que la situation change peu de 
temps après ces propos désabusés, des modèles plus simples qui auraient 
fort bien pu sortir, au prix de quelques ajustements, d’un livre de stratégie 
keynésienne des années 1960 continuent d’être les outils les plus utilisés 
pour gérer l’économie. Les néokeynésiens se montrent d’ailleurs redouta-
blement efficaces aux postes de pouvoir dans la sphère politique : Lawrence 
1. Mankiw 2006 (p. 42).

248	
timothy shenk et timothy mitchell 
Summers, Janet Yellen, Joseph Stiglitz, Olivier Blanchard, Ben Bernanke 
et Mankiw lui-même sont quelques-unes des nombreuses figures issues 
de ce mouvement.
Si une certaine continuité peut être observée dans les théories qui guident 
les hauts fonctionnaires, la responsabilité du gouvernement économique 
glisse toutefois, de manière subtile mais décisive, des mains de repré-
sentants élus à celles des banquiers centraux. À partir des années 1970, 
le manque de confiance dans la capacité des gouvernements démocra-
tiques à mener les arbitrages difficiles requis en période de contraction 
économique marque le début des autorités indépendantes capables de 
maîtriser le savoir technique et de suivre des règles limitant le pouvoir 
discrétionnaire des politiques. Ce programme est mis en pratique pour 
la première fois par les banques centrales, un modèle appliqué par la 
suite de façon large, de la collecte de l’impôt à la gestion des aéroports. 
L’autorité que les banques sont supposées exercer, comme ses modalités 
détaillées, connaissent des changements considérables sur la période. 
Autour des années 1980, la tendance monétariste est vite remplacée par 
des politiques de calibrage des taux d’intérêt à court terme dans le but de 
réduire l’inflation (et de ramener par conséquent à la baisse les anticipations 
d’inflation). Par-delà ces évolutions, l’idée que les experts indépendants 
peuvent protéger l’économie et discipliner le public reste inébranlée.
Et pendant ce temps, l’économie-discipline prospère… La probabilité 
que les élites politiques aient des diplômes d’économie n’a jamais été 
aussi haute : près de la moitié des pays pris en compte dans un rapport 
de 1998 ont au moins un haut responsable diplômé en économie. Plus 
d’un huitième ont un leader ayant fait un doctorat en économie dans une 
faculté occidentale. Une petite élite d’universités situées principalement 
aux États-Unis contrôle l’entrée dans le domaine pour le monde entier. 
Dans les années 1980, Ben Bernanke et Mervyn King sont collègues  
de bureau au MIT. Vingt ans plus tard, ils dirigent la Réserve fédérale 
américaine et la Banque d’Angleterre. Stanley Fischer, directeur de thèse 
de Bernanke, prend la tête de la Banque d’Israël. Un autre étudiant de 
Fischer, Mario Draghi, devient président de la Banque centrale européenne.  
Les organisations transnationales emploient par ailleurs des centaines 
d’économistes – on en dénombre environ 800 à la Banque mondiale dans 
les années 1990 –, ce qui surpasse les départements de recherche univer-
sitaires les plus prestigieux. La Réserve fédérale américaine emploie 
presque autant d’économistes que la Banque mondiale. Et les gouver-
nements à travers la planète en emploient des milliers d’autres, sans 
compter ceux des think tanks et autres instituts de recherche privés qui 
prolifèrent depuis les années 1970.

	
les savoirs de l’économie	
249
Derrière la plupart des évolutions importantes de l’économie universi-
taire se profilent donc les silhouettes de grandes organisations extérieures 
au monde académique. La théorie des attentes rationnelles est développée 
à ses débuts dans une antenne de la Réserve fédérale dans le Minnesota. 
Les modèles DSGE sont produits par des bureaux régionaux de la 
Réserve fédérale. Plusieurs décennies auparavant, le ministère de la 
Défense a fait bénéficier la recherche en théorie des jeux de financements 
généreux – avant qu’elle ne se diffuse largement à travers la discipline. De 
nombreux autres piliers du domaine sont dépendants de fonds publics, 
de la comptabilité nationale à la programmation linéaire.
Mais les responsables politiques ne sont pas les seuls à faire montre 
d’une telle soif de savoir économique. En 2000, aux États-Unis comme 
à peu près partout dans le monde, plus de la moitié des docteurs en 
économie travaillent en dehors de la sphère publique. Le milieu acadé-
mique est aussi concerné. Dans les universités prestigieuses on trouve 
autant d’économistes dans les écoles de commerce que dans les dépar-
tements d’économie. Et les apprentis hommes d’affaires initiés pendant 
leurs études à ce que Gary Becker appelle « une façon économique de 
voir la vie » sont plus susceptibles d’engager des économistes à leur sortie 
de l’université 1.
Pour les économistes, les postes les plus lucratifs sont dans la finance. 
Celle-ci a vu ses pratiques bouleversées au début des années 1980 par 
l’arrivée de techniques dérivées de l’économie financière. Des modèles 
quantitatifs formulent des prévisions de retour sur investissement, tandis 
que des algorithmes d’une extrême sophistication sous-tendent le déclen-
chement des transactions. Les marchés de produits dérivés sont plus que 
tout autre redevables de ces innovations. Au cours des années 2000, des 
trillions de dollars circulent à travers ces marchés, ce qui n’aurait pas été 
possible sans l’apport intellectuel de l’économie financière.
L’économie-discipline et les économistes ont développé des liens inextri-
cables avec les objets qu’ils prétendent étudier, et sont devenus plus que 
jamais indispensables à leur bon fonctionnement. Ils en retirent d’énormes 
avantages et la réussite dans un département important ouvre la voie à des 
carrières dans le milieu académique, la politique et les affaires, ou dans 
les trois à la fois. Mais ces réussites personnelles résultent d’une transfor-
mation plus large. Les économistes ne se sont pas contentés d’observer 
ce monde, ils ont participé de manière décisive à son avènement.
Cette influence n’a jamais été aussi manifeste qu’en 2008, au moment 
où le plus clair de ce que les économistes ont érigé menace de s’effondrer : 
1. Becker 1992.

250	
timothy shenk et timothy mitchell 
un secteur financier secoué par les instruments censés le dompter ; des 
responsables politiques ayant célébré pendant des années la « Grande 
Modération » et qui luttent pour empêcher une autre dépression ; une 
profession abasourdie devant des événements que ses modèles n’ont pas 
vus venir et qu’ils ont même rendus possibles 1. En fait, il ne s’agit pas 
d’une simple crise économique – il s’agit, plus essentiellement, d’une 
crise faite par les économistes.
Traduit par Clara Breteau
Références bibliographiques
Alacevich Michele, 2009, The Political Economy of the World Bank : The Early Years, 
Palo Alto (CA) et Washington (DC), Stanford University Press et the World Bank.
Backhouse Roger et Boianovsky Mauro, 2013, Transforming Modern Macroeco-
nomics : Exploring Disequilibrium Microfoundations (1956-2003), Cambridge, 
Cambridge University Press.
Becker Gary, 1992, « The Economic Way of Looking at Life », conférence Nobel, 
Stockholm.
Bernanke Ben, 2004, « The Great Moderation », discours prononcé devant l’Eastern 
Economic Association, Washington (DC).
Bockman Johanna, 2011, Markets in the Name of Socialism : The Left-Wing Origins of 
Neoliberalism, Palo Alto (CA), Stanford University Press.
Burgin Angus, 2012, The Great Persuasion : Reinventing Free Markets since the Great 
Depression, Cambridge (MA), Harvard University Press.
Cannan Edwin, 1903, A History of the Theories of Production and Distribution in 
English Political Economy from 1776 to 1848, Londres, P.S. King & Son.
Clavin Patricia, 2013, Securing the World Economy : The Reinvention of the League of 
Nation (1920-1946), Oxford, Oxford University Press.
Cook Simon, 2009, The Intellectual Foundations of Alfred Marshall’s Economic 
Science : A Rounded Globe of Knowledge, Cambridge, Cambridge University Press.
Cooper Frederick et Packard Randall (dir.), 1998, International Development and 
the Social Sciences : Essays on the History and Politics of Knowledge, Berkeley (CA), 
University of California Press.
Desrosières Alain, 1993, La Politique des grands nombres. Histoire de la raison 
statistique, Paris, La Découverte.
Etner François, 1987, Histoire du calcul économique en France, Paris, Economica.
Foucault Michel, 2004, Naissance de la biopolitique. Cours au Collège de France 
(1978-1979), Paris, Seuil / Gallimard.
Fourcade Marion, 2009, Economists and Societies : Discipline and Profession in 
the United States, Britain, and France, 1880s to 1990s, Princeton (NJ), Princeton 
University Press.
Grimmer-Solem Erik, 2003, The Rise of Historical Economics and Social Reform in 
Germany (1864-1894), Oxford, Oxford University Press.
1. Pour une des premières proclamations de l’arrivée de la « Grande Modération », voir 
Bernanke 2004.

	
les savoirs de l’économie	
251
Hall Peter (dir.), 1989, The Political Power of Economic Ideas : Keynesianism across 
Nations, Princeton (NJ), Princeton University Press.
Keynes John Maynard, 1926, The End of Laissez-Faire, Londres, Hogarth Press.
Laidler David, 1991, The Golden Age of the Quantity Theory, Princeton (NJ), 
Princeton University Press.
–	 1999, Fabricating the Keynesian Revolution : Studies of the Inter-War Literature on 
Money, the Cycle, and Unemployment, Cambridge, Cambridge University Press.
Lucas Robert, 1987, Models of Business Cycles, New York, Basil Blackwell.
Mackenzie Donald, 2006, An Engine Not a Camera : How Financial Models Shape 
Markets, Cambridge (MA), MIT Press.
Maier Charles, 1987, In Search of Stability : Explorations in Historical Political 
Economy, Cambridge, Cambridge University Press.
Mankiw Gregory N., 2006, « The Macroeconomist as Scientist and Engineer », 
Journal of Economic Perspectives, vol. 20, no 4, automne, p. 29-46.
Marshall Alfred, 1890, Principles of Economics, Londres, Macmillan, <  www.
econlib.org/index.html > .
Mehrling Perry, 1997, The Money Interest and the Public Interest, Cambridge (MA), 
Harvard University Press.
–	 2005, Fischer Black and the Revolutionary Idea of Finance, Hoboken (NJ), John 
Wiley & Sons.
Mirowski Philip, 2002, Machine Dreams : Economics Becomes a Cyborg Science, 
Cambridge, Cambridge University Press.
–	 2013, Never Let a Serious Crisis Go to Waste : How Neoliberalism Survived the 
Financial Meltdown, New York, Verso.
Mirowski Philip et Plehwe Dieter (dir.), 2009, The Road from Mont Pelerin : 
The  Making of the Neoliberal Thought Collective, Cambridge (MA), Harvard 
University Press.
Mitchell Timothy, 2011, Carbon Democracy : Political Power in the Age of Oil, New 
York, Verso.
Montecinos Verónica et Markoff John (dir.), Economists in the Americas, 
Cheltenham et Northampton, Edward Elgar.
Morgan Mary, 1990, The History of Econometric Ideas, Cambridge, Cambridge 
University Press.
–	 2012, The World in the Model, Cambridge, Cambridge University Press.
Neikirk William, 1986, Volcker : Portrait of the Money Man, Chicago (IL), Congdon 
& Weed.
Roberts Alasdair, 2010, The Logic of Discipline : Global Capitalism and the Archi-
tecture of Government, Oxford, Oxford University Press.
Ross Edward, 1901, « The Causes of Race Superiority », Annals of the American 
Academy of Political and Social Science, no 18, p. 67-89.
Rutherford Malcolm, 2011, The Institutionalist Movement in American Economics 
(1918-1947) : Science and Social Control, Cambridge, Cambridge University Press.
Shenk Timothy, 2013, Maurice Dobb : Political Economist, New York, Palgrave Macmillan.
Speich Daniel, 2013, Die Erfindung des Bruttosozialprodukts. Globale Ungleichheit in 
der Wissensgeschichte der Ökonomie, Göttingen, Vandenhoeck & Ruprecht.
Tooze Adam, 2001, Statistics and the German State (1900-1945) : The Making of 
Modern Economic Knowledge, Cambridge, Cambridge University Press.
Weintraub E. Roy, 2002, How Economics Became a Mathematical Science, Durham 
(NC), Duke University Press.


12 Les savoirs  
de la diversité humaine
V e r o n i k a  L i p p h a r d t
Comment les scientifiques donnent-ils sens aux variations au sein 
de l’espèce humaine ? Pour le généticien Kenneth Mather, « la diversité 
humaine a été un sujet d’intérêt et de spéculation depuis les origines de 
l’humanité 1 ». Par cette phrase, Mather prétend que la diversité humaine 
a toujours existé et a toujours fait l’objet de questionnements. Il affirme 
aussi qu’après les « spéculations » du passé la science contemporaine 
sait désormais ce qu’est la diversité humaine et ce qu’elle a toujours été. 
En faisant de la diversité un concept et un phénomène stable gravé dans 
l’ADN, Mather et ses collègues pensent accéder à sa nature fondamentale 
et régler les questions qui la concernent une fois pour toutes. Cependant, 
ceux qui cherchent à comprendre la diversité humaine en la stabilisant 
éludent ce que Fleck a appelé le « signal de résistance 2 ». La diversité 
humaine ne s’est en effet jamais réduite aux concepts, techniques, théories 
ou cadres d’explication successivement avancés par les sciences. Elle 
s’est toujours avérée plus complexe et fuyante que prévu, et fut bien plus 
qu’un simple enjeu de connaissance. La production de savoirs sur cette 
question est toujours allée de pair avec des élaborations juridiques, des 
interventions administratives, des politiques coloniales, dans lesquelles 
des conceptions particulières de la diversité humaine fondaient des hiérar-
chies et des discriminations entre groupes humains. Les spéculations 
savantes alimentent – et sont alimentées par – les pratiques sociales et 
politiques, et les scientifiques, qu’ils en soient conscients ou non, qu’ils 
les combattent ou non, ne sont nullement exempts de croyances déter-
minant profondément leurs travaux.
Mais raconter l’histoire des savoirs sur la diversité humaine sous l’angle 
1. Mather 1964.
2. Fleck 1980.
 Katrin von Lehmann, Blick auf Vielfalt 3-3 [Aperçu sur la diversité, détail], 2013.

254	
veronika lipphardt
privilégié d’une science de la race, d’une histoire des légitimations scienti-
fiques du racisme, ne saurait suffire pour rendre compte de la complexité 
des investigations scientifiques menées au xxe siècle. Nous nous proposons 
donc d’ouvrir notre lecture à d’autres dimensions.
La version standard : l’histoire du racisme scientifique
Dans leur présentation des « sciences de la race », des « sciences raciales », 
de « la race dans les sciences » ou du « racisme scientifique », les historiens 
accordent une attention toute particulière à la façon dont les scientifiques 
participent à la classification des races, à l’élaboration de typologies et au 
développement d’une pensée de l’inégalité des races 1.
Aux alentours de 1800, les premières classifications et typologies 
raciales commencent à circuler dans les cercles académiques. Au milieu 
du xixe siècle, les scientifiques débattent de la possibilité que toutes les 
races humaines aient des ancêtres communs. Mais le monogénisme qui 
l’emporte se révèle tout aussi compatible avec le racisme et le colonialisme 
que le polygénisme. Accepter de segmenter l’espèce humaine en diffé-
rents groupes raciaux permet de les hiérarchiser, ce que font les théories 
raciales de l’époque, avec notamment les outils de l’anthropométrie 2.
Au début du xxe siècle, le racisme scientifique se solidifie et s’inscrit 
dans l’intérêt croissant pour l’eugénisme ou l’hérédité. Aux États-Unis 
comme en Allemagne, les tenants radicaux de la discrimination raciale et 
de l’eugénisme sont alors aux avant-postes des institutions scientifiques.
On connaît maintenant assez bien les trajectoires et intrications 
politiques des sciences de la race dans divers contextes nationaux 3. 
Chaque histoire nationale, comme les histoires transnationales qui les 
relient, est marquée par des génocides, des persécutions, des déporta-
tions, des discriminations ou une indifférence criminelle dans lesquels 
des scientifiques peuvent directement tremper.
Après la Seconde Guerre mondiale, la position dure des biologistes de 
la race perd tout crédit, et la génétique des populations promet de fonder 
l’explication de la variation humaine sur de nouvelles bases. Les premiers 
chercheurs en génétique des populations participent aux campagnes 
contre le racisme organisées par l’Unesco et s’efforcent, dans la Décla-
ration sur la race publiée par celle-ci en 1950, d’aboutir à un consensus 
1. Stepan 1984, Jackson et Weidman 2004, Kaupen-Haas et Saller 1999.
2. Mendelsohn 2001.
3. Quelques études de cas chez Stocking 1988, Lindee et Santos 2012.

	
les savoirs de la diversité humaine 	
255
sur le concept. Mais si tous s’accordent pour condamner les typologies 
raciales, les atrocités nazies ou la pensée d’une supériorité de certaines 
races, de sérieuses divergences subsistent entre scientifiques : la question 
est de savoir si la « race » a un sens scientifique, s’il faut tout simplement 
rejeter la notion ou la réinventer selon une acception non raciste. Et selon 
Jenny Reardon – malgré la distance rhétorique prise avec le terme de 
« race » – un « concept de race appliqué aux populations » reste globa-
lement au cœur du paradigme de la nouvelle génétique des populations, et 
le raisonnement par typologies a encore prise dans la seconde moitié du 
xxe siècle 1. Qui plus est, de nombreuses études indiquent que la « race » 
fait son retour dans les sciences durant la dernière décennie du xxe siècle.
De toute évidence, ce récit classique d’une biologie qui reste struc-
turée autour de la typologisation, sinon de la hiérarchisation des races 
tout au long du xxe siècle, est loin d’être fausse. Mais cette histoire du 
« racisme scientifique » est partielle et partiale, et elle ne donne pas le 
tableau complet de la recherche sur les variations humaines ou la « nature 
humaine ». Avant comme après 1945, des chercheurs s’efforcent d’expurger 
leurs travaux des contaminations politiques ou sociétales, ou de ce qu’ils 
perçoivent comme telles. Il convient d’étudier dans le détail la façon dont 
les scientifiques se sont emparés de la question de la variation humaine 
comme objet épistémique pour comprendre les rôles qu’elle a joués, voir 
la façon dont elle s’est retrouvée mêlée à des enjeux de pouvoir, et saisir 
pourquoi la notion de « race », sans cesse controversée, revient toujours 
aussi régulièrement.
Les scientifiques emploient des termes tels que « variation humaine » 
et « diversité humaine » dans un sens biologique explicite dès avant 
1900. Ils utilisent de façon interchangeable des expressions telles que 
« variété de l’homme » ou Mannigfaltigkeit en allemand (du mot manifold, 
« pluralité »), « diversité raciale », « variation raciale », mais aussi, à la fin 
du xxe siècle, « diversité génétique », « variation génétique », « hétérogé-
néité génétique » ou « variabilité géographique ». On trouve, y compris 
au xixe siècle, des chercheurs employant les termes de « diversité » ou 
de « cline » pour affirmer les multiplicités infinies de l’espèce humaine 
et s’opposer à l’idée d’une classification étroite entre trois, quatre, cinq, 
quatorze races ou plus. Nancy Stepan affirme ainsi avec raison que l’his-
toire des sciences raciales est « l’histoire d’une série d’accommodements 
successifs de la part de différentes sciences avec les exigences liées à 
des convictions profondes sur la “naturalité” des inégalités entre races 
1. Gannett 2001 (p. 490), Reardon 2004, Gannett et Griesemer 2004, Wade 2002, Farber 2011.

256	
veronika lipphardt
humaines 1 ». Mais si la « race » est bien le concept dominant des années 
1800-1945, et s’il reste sous-jacent après cette date, il n’a jamais été le 
seul concept employé dans le milieu scientifique pour décrire les dif­­fé-
rences biologiques censées être héréditaires. Et c’est à redécouvrir ces 
pratiques épistémiques plus complexes tout au long du xxe siècle qu’est 
consacré ce chapitre.
Nouvelles méthodes et lointains horizons :  
les expéditions scientifiques au début du xxe siècle
En 1905, un jeune docteur ambitieux du nom de Carl Bruck embarque 
pour Java afin d’y développer un projet sur les différences de sang existant 
chez les êtres humains. Tout en restant marquée par le contexte colonial 
1. Stepan 1984 (p. 335).
Carl Bruck et la sérologie des populations javanaises
En 1905, Bruck se joint à une mission d’études à Java en compagnie d’Albert 
Neisser, dermatologue réputé. L’espace colonial où se côtoient Européens et 
« indigènes » offre un terrain parfait pour de telles recherches. Le jeune médecin 
délaisse les mesures anthropométriques alors répandues parmi les voyageurs 
scientifiques de l’époque. Sur la base d’échantillons sanguins, il compare les 
taux de coagulation entre différents groupes composés d’hommes et d’ânes. Sa 
méthode s’inspire de la Komplementbindungsmethode développée par Neisser et 
appliquée depuis en médecine légale. Il prélève et analyse des sérums sanguins 
dont « 7 hollandais, 5 chinois, 6 malaisiens, plus 7 javanais, 1 javanais occidental » 
et 1 « arabe » 1, ainsi que des sérums de diverses espèces de singes.
Ayant compilé les résultats dans un tableau, Bruck conclut à la possibilité 
d’établir des distinctions entre les races et d’en comprendre les relations phylo-
génétiques. La sérologie vient donc justifier et « scientifiser » la thèse d’une 
hiérarchie des races puisqu’il obtient une gradation des profils immunologiques 
des sérums de races, « des extractions » les plus « basses » aux plus « hautes ». 
Pourtant, ce travail porte sur un échantillon réduit et ne manque pas d’anomalies 
questionnant le schème racialiste de Bruck : deux des sept Javanais impliqués ne 
rentrent pas dans le profil type du groupe et les Javanais apparaissent supérieurs 
aux Japonais. Et il explique cette anomalie en invoquant l’hypothèse, chez certains 
Javanais, d’une lignée d’ancêtres hindous particulièrement « pure » et jusque-là 
méconnue…
1. Bruck 1907 (p. 796).

	
les savoirs de la diversité humaine 	
257
et une typologie raciale, l’enquête de Bruck présente l’intérêt d’avoir 
appliqué l’approche immunologique, alors l’une des plus innovantes  
de la recherche médicale. Ses recherches (voir l’encadré « Carl Bruck et 
la sérologie des populations javanaises », p. 256) illustrent comment, au 
début du xxe siècle et à l’intérieur d’une pensée anthropologique racialiste, 
de nouveaux aspects de la diversité humaine sont explorés, comment de 
nouvelles méthodes et de nouveaux objets s’affirment en lien avec l’essor 
de la sérologie, la biochimie et la génétique.
Méthodes et démarches
Depuis le xviiie siècle, la « pluralité » humaine suscite l’intérêt de 
voyageurs et de savants à la jonction de plusieurs disciplines. À la fin du 
xixe siècle, le cadre se resserre autour des sciences de la vie, et notamment 
de l’anthropologie médicale. L’étude des variations humaines requiert  
la connaissance et la maîtrise des méthodes jugées les plus objectives  
et avancées en la matière : l’anthropométrie et les statistiques. À l’aide de 
ces techniques, les données s’accumulent rapidement. Partout, dans les 
écoles, les casernes, les usines, les hôpitaux et les dispensaires coloniaux, 
on mesure la taille des parties du corps et on détermine les maladies, les 
couleurs de cheveux, d’yeux et de peau de centaines de milliers d’individus.
Dès le début du xxe siècle toutefois, les scientifiques commencent à 
mobiliser d’autres méthodes pour passer au crible d’autres traits : les 
fluides corporels, les excréments, les fonctions physiologiques, ainsi que 
l’anatomie du cerveau, des couches de la peau et des paupières. Elles 
permettent de comparer des caractéristiques pathologiques telles que 
la mortalité, l’évolution des maladies, les symptômes, les parasites et 
les phénomènes biochimiques concomitants. Par ailleurs, la généalogie, 
les études de jumeaux et la génétique ouvrent d’autres perspectives à 
l’étude des variations humaines. Dans les années 1920, quand la généa-
logie médicale et la génétique gagnent en importance, il devient courant 
de compléter l’anthropométrie par la sérologie, l’établissement d’arbres 
généalogiques et l’examen des traits pathologiques ou physiologiques 
de chaque individu.
Groupes et lieux
Alors que les recherches sur la variation humaine se déploient avec 
l’avancée du colonialisme européen, nombre d’expéditions médicales 
comme celle de Neisser et Bruck à Java combinent, afin de mieux accéder 
aux populations, recherche et soins médicaux. La comparaison des 

258	
veronika lipphardt
statistiques vitales et médicales d’« indigènes » et d’« Européens » alimente 
les efforts d’adaptation des colonisateurs aux nouveaux territoires, comme 
leur gestion des populations colonisées. Les colonies sont aussi un labora-
toire idéal où étudier la variation humaine en tant que telle : comme 
celle-ci semble coïncider avec la géographie, les chercheurs font le tour 
du globe pour en établir la cartographie, se ménageant un accès aux 
populations à travers des infrastructures coloniales comme les établis-
sements de soins ou les lieux de travail. Les groupes se prêtant le mieux 
aux études sont en effet ceux qui sont déjà soumis à un régime politique 
nouveau – les salariés, les usagers des institutions étatiques, les colonisés 
ou les minorités brimées par les politiques nationalistes.
Dans la plupart des travaux des médecins coloniaux on trouve des 
comparaisons entre différents groupes humains, et il n’est pas rare que 
ces groupes soient définis en termes « raciaux ». La diversité humaine ou 
la race, sans être la préoccupation centrale de ces études vont cependant 
apporter des données et des arguments dans d’autres domaines. Ces 
travaux circulent abondamment et se stabilisent par leur mise en relation 
avec d’autres études présentant des catégories similaires, qu’il s’agisse de 
catégories administratives ou de classifications plus intuitives 1.
La montée du racisme dans les sociétés et les milieux politiques 
européens est l’un des facteurs décisifs de développement des études 
sur la diversité humaine. De nombreux scientifiques s’affirment expli-
citement racistes ; d’autres insistent sur le fait que leurs études sont 
« purement objectives », « strictement empiriques » ou s’appuient sur des 
« méthodologies avancées ».
D’un pays autodéclaré « civilisé » à l’autre, des styles propres d’étude 
de la variation humaine se révèlent 2. Aux États-Unis, l’intérêt pour la 
couleur est prédominant, tandis qu’en Allemagne l’essentiel de l’attention 
se concentre sur la comparaison entre les « Allemands », les « Juifs » et 
les « Slaves ». Les scientifiques suisses se focalisent quant à eux sur les 
petits villages des Alpes ; les Norvégiens sur les nomades « Sami » ; dans 
le Pacifique, on scrute les populations insulaires ; en Union soviétique, on 
s’intéresse aux clines. Dans tous les cas, les populations qu’on estime les 
plus isolées géographiquement ou socialement sont considérées comme 
les plus intéressantes, car on pense qu’elles représentent les conditions 
de peuplement homogène les plus anciennes.
Ainsi, à un niveau très élémentaire, l’idée selon laquelle les diffé-
rences entre groupes humains résultent de l’isolement, de la distance et 
1. AG gegen Rassismus in den Lebenswissenschaften 2009.
2. Voir les divers articles in Lindee et Santos 2012.

	
les savoirs de la diversité humaine 	
259
de mouvements migratoires est au cœur de la pensée de la diversité et 
des sciences raciales. Et cette approche est portée par des tendances de 
fond bien antérieures à l’influence des théories de l’évolution. Une lecture 
essentialiste de l’origine et de la cohérence des groupes se retrouve ainsi 
dans beaucoup de récits racialistes : les Juifs n’ont-ils pas été connus de 
tout temps pour leur volonté tenace de rester séparés des Wirtsvölker 1 ? 
Peut-on ignorer l’importance de l’idéologie communautaire endogame 
dans la Rome antique ? N’est-il pas avéré que les Basques ne se sont 
mélangés avec aucun autre peuple européen ? De la même façon, les 
communautés nomades scandinaves semblent bien avoir traversé des 
phases de dépeuplement, sans avoir jamais reçu d’apports extérieurs. 
Dans les cas où les différences sont peu visibles (comme en Europe), les 
histoires de sécession servent bien souvent d’appui narratif pour reven-
diquer des différences et légitimer les séparations.
Comprendre l’hérédité
La théorisation des différences biologiques comme résultant de longues 
périodes d’isolement, même si elle est loin d’être nouvelle (l’isolement géogra-
phique est par exemple à la base de la théorie darwinienne de l’évolution), 
est transformée par les nouvelles approches de l’hérédité. Au début du 
xxe siècle, la question de savoir si les hommes pouvaient changer d’esprit, 
de corps, de mentalité à l’échelle d’une vie, ou si une variété de traits hérédi-
taires pouvait rester stable sur de nombreuses générations, est au centre de 
nombreuses controverses politiques aussi bien que scientifiques. Les défen-
seurs du néolamarckisme affirment que les traits acquis par un individu 
peuvent être transmis à sa descendance. Ils s’opposent aux néodarwiniens 
qui, s’appuyant en particulier sur les thèses d’August Weismann, prétendent 
que le germoplasme reste stable et intact au gré des transmissions et 
des générations. Comme l’ont montré Rheinberger et Müller-Wille, les 
théories de Weismann et de Mendel ont restreint le nombre d’explications 
disponibles pour appréhender l’hérédité 2. Tandis que des anthropologues 
médicaux, des généticiens et de nombreux partisans de l’eugénisme diffusent 
le néodarwinisme, un nombre considérable de scientifiques issus d’autres 
domaines comme la paléontologie ou la médecine continuent d’adhérer 
au néolarmarckisme 3. Ce dernier perd toutefois de son influence devant 
la théorie synthétique de l’évolution à partir des années 1940.
1. « Peuples-hôtes ».
2. Müller-Wille et Rheinberger 2012.
3. Löwy et Gaudillière 2012 [2001].

260	
veronika lipphardt
Les scientifiques qui étudient la génétique des variations humaines ou 
qui l’utilisent de façon systématique afin de mieux comprendre l’hérédité 
sont généralement néodarwiniens. Certains d’entre eux élaborent des 
protocoles de recherche similaires aux expériences de croisement de 
Mendel : ils considèrent deux groupes homogènes considérés comme 
« purs » et longtemps isolés l’un de l’autre, analysant ensuite le produit de 
leur « mélange », c’est-à-dire la progéniture de mariages mixtes effectués 
entre ces deux groupes, sur deux générations. La génération qui en 
résulte – nommée génération F2 dans la terminologie mendélienne – est 
censée présenter un phénotype distinct, attaché à un génotype clairement 
identifié. La logique expérimentale mendélienne est mise en œuvre dans 
des expériences de génétique animale puis, au début des années 1910, 
de génétique humaine 1. Elle se systématise au moyen d’enquêtes multi-
générationnelles et d’arbres généalogiques tels ceux développés par les 
généticiens de l’Eugenics Record Office américain.
Ces théories biologiques appliquées à l’histoire des groupes parti-
cipent de préoccupations concernant la reproduction, les bonnes et  
les mauvaises alliances, la dégénérescence, et d’autres sujets d’anxiété 
caractéristiques des projets eugénistes. La diversité est alors attachée à 
de vastes temporalités : les « races » ou « variétés » sont supposées dotées 
de caractéristiques biologiques héréditaires stables sur des dizaines de 
milliers d’années. Elles ne semblent pas pouvoir subir de changements 
significatifs sur deux ou trois générations à partir du moment – condition 
essentielle – où il y a absence de mélange avec d’autres groupes.
Diversité animale et diversité végétale  
dans les sciences de la vie
L’étude de la variation humaine, de la stabilité et du « mélange des 
races » participe des développements de la génétique et de la biologie de 
l’évolution 2. Après Darwin, un intérêt croissant se porte sur la diversité 
des espèces de plantes, animaux et micro-organismes, leur structure 
génétique, leur répartition géographique et les différents schémas d’évo-
lution sous-jacents. La notion de diversité qui est présente dans ces 
travaux offre un cadre de référence aux savoirs sur la diversité humaine 
du début du xxe siècle.
Elisabeth Schiemann, spécialiste de la génétique des plantes et opposante 
1. Fischer 1913, Davenport et Steggerda 1929, MacCanghey 1919.
2. Müller-Wille et Rheinberger 2008, Anderson 2012.

	
les savoirs de la diversité humaine 	
261
au régime nazi, propose ainsi, en 1931, une théorie alternative des varia-
tions humaines 1. Refusant l’idée que les ancêtres des Européens modernes 
sont des Aryens venus de l’Inde, elle affirme que la population européenne 
est issue de groupes humains en provenance de l’ensemble du continent 
asiatique et d’Afrique, y compris des régions subsahariennes. Schiemann 
enrichit le spectre des approches de la diversité humaine en utilisant les 
théories de Nikolai Vavilov à propos de la migration et de l’évolution  
des plantes 2. Utilisant des études de chromosomes, Vavilov montre que 
les espèces végétales sont originaires des lieux où elles présentent la  
plus grande diversité génétique. En périphérie, la diversité génétique est 
considérablement inférieure. Cette théorie, connue aujourd’hui sous le 
nom de « théorie des centres de diversité », renouvelle les façons d’aborder 
la diversité. Considérée auparavant comme une série classifiable d’entités 
hiérarchiques, celle-ci devient une structure complexe d’informations à 
partir de laquelle il s’agit de reconstituer une histoire concrète de descen-
dances et de migrations à travers le temps et l’espace. La théorie de Vavilov 
est contestée et amendée, y compris par Vavilov lui-même, aboutissant 
au concept de centre « secondaire » de diversité, ne correspondant pas 
à un lieu d’origine mais à un lieu où une espèce s’est développée dans 
des conditions très favorables, après une première migration depuis son 
centre d’origine.
Après Schiemann, d’autres généticiens issus du végétal ou de l’animal 
tels Theodosius Dobzhansky et Leslie Clarence Dunn introduisent à leur 
tour de nouveaux concepts et de nouvelles approches issus de la génétique 
des populations dans l’étude de la diversité humaine. Leur travail a alors 
très peu à voir avec celui des chercheurs qui défendent les classifications 
et typologies raciales.
L’approche anthropométrique et classificatrice des « races » subit 
d’ailleurs de sérieuses attaques dès le début du xxe siècle. Le célèbre 
anthropologue américain Franz Boas passe souvent pour avoir délivré 
dans les années 1920 la première critique radicale de la science de la race 
en questionnant notamment la thèse de caractéristiques raciales héritables 
de l’intelligence 3. Mais c’est dès le tournant du siècle qu’émergent des 
critiques, dirigées non seulement contre les théories raciales mais aussi 
bientôt contre l’eugénisme 4. Non reconnues par la science institution-
nelle, ces voix dissidentes sont toutefois reprises dans certains secteurs 
de la presse et de l’opinion. Face aux résistances des milieux et revues 
1. Schiemann 1931.
2. Vavilov 1926, Bauer 2014.
3. Geisthövel 2013, Lipphardt 2009.
4. Lipphardt 2008.

262	
veronika lipphardt
scientifiques dominants, le médecin et anthropologue russe Samuel 
Weissenberg adopte une stratégie éditoriale sur deux tableaux : il publie 
de nombreuses études d’anthropométrie et de généalogie médicale dans 
des revues scientifiques assurant sa reconnaissance – mais réserve ses 
critiques virulentes de la science de la race aux revues de vulgarisation 1.
À partir des années 1920, alors que les limites de la notion de « race » 
pour décrire les variations humaines dans leur complexité font l’objet de 
critiques grandissantes, un nombre croissant de scientifiques cherchent 
des alternatives allant du simple remplacement du mot « race » par celui 
de « sous-espèce » ou « variété », sans changer pour autant le concept 
ni les classifications, à des critiques invoquant les complexités rencon-
trées à l’intérieur de larges groupes raciaux comme les Européens, les 
Asiatiques, les Africains, mais aussi à des critiques radicales s’atta-
quant à la division même de l’espèce humaine en sous-groupes 2. Ceux 
qui rejettent l’approche taxonomique par races de la diversité humaine 
le font au titre de deux arguments principaux, déjà introduits par des 
chercheurs comme Herder 3 : toute variation se présente en dégradés et 
non en blocs discrets ; et il est plus de diversité à l’intérieur des groupes 
qu’entre les groupes.
Malgré la récurrence de ces critiques, l’anthropométrie reste toutefois 
la technique considérée comme la plus fiable pour étudier les variations 
humaines durant la première moitié du xxe siècle. Bien que la complexité 
de la diversité humaine devienne notoire et manifeste, la plupart des 
études scientifiques menées dans cette discipline procèdent à une répar-
tition raciale des individus, et sont souvent ouvertement racistes.
Infiniment divers :  
les transformations des années 1950
Après la Seconde Guerre mondiale, un réseau de scientifiques se 
tisse autour de généticiens tels que Dobzhansky et Dunn pour aborder 
la variation humaine sous l’angle de la génétique des populations. La 
Fondation Rockefeller finance généreusement les premières recherches, 
conduites à grand renfort de missions d’étude et de conférences inter­
nationales 4. Des institutions spécialisées nouvelles apparaissent, telles que 
1. Weissenberg 1927.
2. Voir les vives critiques de Friedrich Hertz, Jean Finot, Samuel Weissenberg, Hans Friedenthal, 
Max Marcuse, Hugo Iltis.
3. Herder 2000 [éd. originale en 1784-1791] (p. 23 et 26).
4. Osborn 1954, Cold Spring Harbor Symposia 1955.

	
les savoirs de la diversité humaine 	
263
l’Institut pour l’étude de la variation humaine, à l’université Columbia, 
dirigé par Dunn et Dobzhansky.
À Bombay, un Laboratoire d’étude de la variation humaine est fondé 
par L.D. Sanghvi, un Indien qui a fait sa thèse sous la direction de Dunn 
et qui est l’un des premiers chercheurs à faire de l’analyse systématique en 
génétique des populations. Doté d’une poignée de marqueurs génétiques, 
Sanghvi effectue des prélèvements sanguins et des tests de daltonisme 
ou de sensibilité au PTC (phénylthiocarbamide). Ayant réalisé une série 
de mesures anthropométriques afin d’évaluer les mérites respectifs des 
différentes méthodes, Sanghvi affirme que « la population indienne 
est infiniment diverse 1 ». Selon lui, l’Inde et ses classifications raciales 
anciennes font fausse route en prétendant répartir les Indiens en un  
petit nombre de races. Il prétend au contraire que, « pour toute étude de 
la distribution des caractères génétiques de la population indienne, les 
seules unités raciales pertinentes sont les groupes endogames 2 ». Selon 
Sanghvi, deux castes indiennes présentent autant de différences génétiques 
entre elles qu’on en observe entre des Noirs et des Blancs américains. 
Ainsi, le système de castes constitue pour lui un dispositif d’isolement et 
de diversité humaine. Loin de pointer dans ses travaux les dominations 
liées à ce système et leur consolidation sous le régime colonial, Sanghvi 
illustre un cas où la critique de l’approche raciale s’accompagne de l’appro-
bation d’autres formes d’exclusion sociale, celle des castes.
À l’instar de Sanghvi, les généticiens parcourent aussi l’Australie, l’Alaska, 
l’Afrique, les îles du Pacifique, les pays d’Amérique du Sud et d’autres 
endroits « reculés » pour étudier des populations isolées. Aucune des publi-
cations en génétique ne fait cependant allusion à l’oppression subie par ces 
populations, ni à la façon dont elle a pu jouer un rôle dans leur isolement 
par le biais d’interventions administratives ou sanitaires. Aucune de ces 
populations n’est véritablement étudiée pour la première fois et elles ont 
déjà été l’objet des savoirs-pouvoirs de la race. Si les relations entre les 
chercheurs et les sujets de ces expériences changent avec les décolonisa-
tions durant les années 1960, et plus tard à l’occasion du Human Genome 
Diversity Project, les groupes isolés demeurent l’objet privilégié des études 3.
La transition méthodologique de l’anthropométrie, discréditée par 
la biologie raciale allemande, à la génétique dans l’étude de la variation 
1. Sanghvi et Khanolkar 1950 (p. 53).
2. Ibid. (p. 52-53 et 62).
3. Dans le même temps, sous l’égide de l’OMS, les anthropologues investissent un nouveau 
paradigme unificateur des sciences naturelles et humaines à travers l’étude des « peuples primitifs ». 
Cf. Radin 2014. L’anthropologie culturelle des années 1960 adhère d’ailleurs généralement aux 
présupposés théoriques sous-jacents à l’accent sur les « populations isolées » et aux objets « peuples 
primitifs », « isolats primitifs » ou « populations primitives » : cf. Thompson 1967, OMS 1964.

264	
veronika lipphardt
humaine n’est pas immédiate après guerre. La génétique des populations, 
au prestige croissant de par son approche formelle et mathématique, 
étend peu à peu aux humains les approches d’abord développées sur les 
mouches et les poulets. La génétique des groupes sanguins pratiquée 
depuis les années 1920 apparaît comme particulièrement prometteuse 
dans l’appréhension par la génétique de la variation humaine. Bien que la 
distribution géographique des allèles des groupes sanguins suggère des 
schémas de variation humaine différents de ceux dégagés par l’anthropo-
métrie, les résultats continuent d’être formulés selon les classifications 
raciales pendant plusieurs décennies encore. Cela étant, en tant que 
premier marqueur génétique stable, transmis selon un mécanisme 
génétique mendélien bien connu, distinct des traits morphologiques et 
non corrélé à des jugements hiérarchiques, le groupe sanguin promet 
d’apporter des informations « neutres » dans le domaine hautement 
controversé de la variation humaine. La répartition géographique ou 
le caractère apparemment héréditaire de certaines prédispositions aux 
maladies ou caractéristiques immunitaires apportent d’autres marqueurs 1. 
À chaque fois qu’est mise au jour une différence génétique d’intérêt 
médical – comme la capacité de déceler par le goût la présence de 
certaines substances chimiques telles que le PTC –, les scientifiques 
entreprennent d’explorer la variabilité des groupes humains par rapport 
à cet aspect. Objet épistémique, la « diversité humaine » est aussi un 
objet technique instrumentalisé par la génétique médicale mobilisant 
des groupes humains déjà bien balisés à des fins de contrôle ou de 
comparaison.
L’établissement de tels marqueurs génétiques, la nouvelle théorie 
synthétique de l’évolution, sans oublier le mouvement antiraciste dans 
les sciences de la vie d’après guerre, transforment ainsi peu à peu mais 
profondément les ordres du jour, méthodes de recherche, pratiques et 
modes de collaboration dans la discipline.
Diversité et populations : glissements terminologiques  
à l’ère de l’après-guerre
L’expression « diversité humaine » occupe une place prépondérante 
après la Seconde Guerre mondiale. Elle permet de contrer explicitement 
les approches en termes de « races » et reflète le passage des typologies 
raciales à la génétique des populations 2. Les scientifiques l’utilisent alors 
1. Mendelsohn 2001.
2. Müller-Wille et Rheinberger 2008.

	
les savoirs de la diversité humaine 	
265
dans leurs pamphlets contre les nazis et la science de la race. Certains 
essaient de remplacer complètement l’usage du terme de « race » par  
celui de « groupe ethnique », « population », « isolat génétique », 
« population mendélienne », « communauté endogame » ou encore 
« isolat 1 ». Le succès de l’opération reste limité tant le mot de « race » 
est répandu dans l’espace public et structure encore l’appréhension 
commune des différences humaines et la singularité des groupes humains. 
Toutefois, la formule de « diversité humaine » se répand. Au milieu des 
années 1960, l’expression est d’usage courant dans la génétique humaine 
et l’anthropologie physique, et il va sans dire qu’elle renvoie à la diversité 
génétique humaine 2.
Pour les chercheurs, une population restée isolée durant plusieurs 
siècles doit alors être génétiquement distincte d’autres populations, 
en raison des mutations, de la dérive génétique, de la sélection et de 
l’endogamie. On s’oriente vers une compréhension plus dynamique de 
l’évolution humaine, notamment du fait que les changements génétiques 
renvoient à des durées plus courtes que l’ancien paradigme d’évolution 
des races 3. Cependant, on pense toujours que les groupes humains sont 
apparus quelque part, ont migré ensuite ailleurs et se sont reproduits 
« de façon isolée » pendant un long moment. La notion d’isolement est 
ici plus que théorique : elle aide à constituer les échantillons et à trouver 
des populations aux limites bien définies. Ces concepts d’origine, de 
migration, d’isolement ou de mélange génétiques restent globalement 
assez similaires aux vieux concepts attachés à la race.
Les clines
La polémique qui oppose en 1962 l’anthropologue Frank Livingstone et 
le généticien Theodosius Dobzhansky marque une rupture avec l’approche 
isolationniste. Livingstone affirme qu’« une minorité croissante de biolo-
gistes » détient d’excellents arguments « pour arrêter d’utiliser le concept de 
race au sujet des populations actuelles d’Homo sapiens 4 ». Et de préciser :
Une telle position ne suppose pas l’absence de variabilité biologique entre 
les populations d’organismes de même espèce. Il s’agit seulement de dire que 
la variabilité ne se conforme pas à des compartiments isolés étiquetés sous 
1. Montagu 1947, Gannett 2001, Lipphardt 2012.
2. Mather 1962 et 1964, Lewontin 1953, Dobzhansky 1973.
3. Certains auteurs pensaient que quelques centaines d’années suffisaient à produire des 
divergences marquées chez un isolat génétique humain ; cf. Sanghvi et Khanolkar 1950 (p. 62).
4. Livingstone 1962 (p. 279).

266	
veronika lipphardt
le nom de différentes races. En d’autres mots, on peut dire qu’il n’y a pas de 
races, mais seulement des clines 1.
Des idées semblables avaient été exposées plus de cent cinquante ans 
auparavant, mais le propos de Livingstone apparaît comme neuf. À la même 
époque, l’anthropologue physique belge Jean Hiernaux tente d’établir une 
taxonomie numérique comme alternative à la classification raciale – alors 
que les colonies africaines où il a séjourné pour collecter ses données 
s’acheminent vers l’indépendance 2. Les deux concepts développés par 
Hiernaux et Livingstone continuent d’envisager la variation humaine 
comme distribuée géographiquement et transmise de façon génétique, 
abandonnant cependant les notions de « race » ou d’« isolat ». Si même la 
notion de « population mendélienne » avait fonctionné de façon voisine  
de la race, le « cline » sort de ce cadre tout en cherchant à décrire des 
différences de même type. Pour reprendre les mots de Livingstone :
Si l’un des enjeux principaux de l’anthropologie physique est d’expliquer la 
variabilité génétique – et je pense que c’est le cas –, il existe cependant des 
méthodes qui n’utilisent pas le concept de race pour décrire et expliquer la 
variabilité. Celle-ci peut aussi être décrite en termes de cline et de morphisme 
(Huxley 1955) 3.
Toutefois, les clines ne se prêtent pas facilement à la recherche, et ne 
font pas écho à des lieux communs politiques, comme c’était le cas pour 
les groupes humains isolés. À partir de la fin des années 1960, on observe 
dans les champs de l’anthropologie physique et de la génétique des popula-
tions un déclin marqué de l’intérêt pour les questions « raciales », alors 
que d’autres aspects de la diversité génétique prennent de l’importance. 
Le célèbre généticien Cavalli-Sforza observera plus tard que, à chaque fois 
qu’une innovation technologique se produit – que ce soit avec l’analyse 
des groupes sanguins, les protéines dans les années 1960, ou enfin l’ADN 
dans les années 1980 –, les scientifiques sont surpris de découvrir des 
variations humaines toujours plus nombreuses :
On eut un premier aperçu de l’ampleur stupéfiante de la variation génétique 
assez tard – au début des années 1950 – qui se mua en une vue plus complète 
dans les années 1960, quand on fut en mesure d’étudier de façon systéma-
tique les différences individuelles de protéines. [Cependant,] ce n’est que 
1. Ibid.
2. Ibid., Hiernaux 1964.
3. Livingstone 1962 (p. 279).

	
les savoirs de la diversité humaine 	
267
quand les analyses purent être conduites au niveau du matériau de l’hérédité 
lui-même, l’ADN, que la variation génétique individuelle commença à révéler 
toute son étendue 1.
L’article retentissant de Richard Lewontin, « La répartition de la diversité 
humaine » (1972), vient apporter une base à partir de laquelle se développe 
une conceptualisation nouvelle et particulièrement fructueuse de la 
diversité humaine 2. Néanmoins, le mot de « diversité » ayant fini par 
désigner dans les années 1980 toutes sortes de différences entre êtres 
humains, les généticiens commencent alors à lui adjoindre l’adjectif 
« génétique ». Aujourd’hui, la diversité humaine recouvre une multiplicité 
de différences – que ce soit de sexe, d’ethnie, d’âge ou de classe sociale. 
Elle est devenue une cible privilégiée des politiques en matière d’emploi 
ou de discrimination positive et fait généralement l’objet d’une gestion 
particulière dans les institutions.
Depuis la contribution de Lewontin, les généticiens, philosophes et 
chercheurs en sciences sociales n’ont pas cessé, et ce particulièrement 
durant la dernière décennie, de débattre du statut de la diversité génétique 
humaine, cherchant à savoir s’il fallait ou non l’interpréter comme ressor-
tissant à la même logique que la classification raciale de sens commun.
Conclusion
En dépit de toutes les complexités identifiées par la génétique, les 
chercheurs du domaine « sciences, technologie et société » ont démontré 
que l’état de la recherche en diversité humaine, à la fin du xxe siècle, a 
conservé certains des aspects antérieurs de la science de la race. Les 
groupes et les logiques qui leur sont rattachés n’ont guère changé, même 
si l’on ne trouve plus trace des éléments qui étaient les plus discrimi-
nants ou les plus compromettants sur le plan politique. Avec l’histoire 
génétique se développe, sous des dehors politiquement corrects, un 
récit de l’histoire des groupes comme récit de leur isolement social. Les 
études génétiques récentes sur Rome, par exemple, reprennent les vieux 
épisodes de l’origine, de la migration, de la dispersion, de l’isolement et de 
l’idéologie endogame. Par ailleurs, les études génétiques de portée conti-
nentale ou mondiale se basent encore sur les mêmes vieilles catégories 
d’Européens, d’Africains, d’Asiatiques et ainsi de suite. Les travaux menés 
1. Cavalli-Sforza et al. 1994 (p. 3), Reardon 2004.
2. Marks 1995.

268	
veronika lipphardt
par Cavalli-Sforza des années 1960 à nos jours en sont une bonne illus-
tration, et tout particulièrement ceux sur le Human Genome Diversity 
Project qui ont fait de lui le spécialiste le plus reconnu de la diversité 
humaine et de sa répartition géographique. Le HapMap Project en est 
un autre bon exemple 1.
Pourquoi la génétique des populations humaines égrène-t-elle encore 
et toujours dans ses conclusions les mêmes catégories ? On peut refor-
muler la question d’un point de vue narratologique : comment se fait-il 
que les histoires de certains groupes puissent circuler si facilement entre 
la science et le grand public, alors que ce n’est pas le cas pour d’autres ? Les 
généticiens affirment que leur logiciel ne présente aucun biais et que ces 
catégories reviennent sans arrêt pour la simple et bonne raison qu’elles 
sont inscrites dans la structure même de la variation humaine à l’échelle 
mondiale 2. Selon leurs détracteurs, la répétition démontre seulement 
que les mêmes biais sont à l’œuvre, et qu’ils sont tout aussi forts dans la 
science que dans la société.
Il convient de souligner le caractère extrêmement arbitraire, du point 
de vue historique et socio-anthropologique, du choix du groupe à étudier 
et du processus d’échantillonnage. Les scientifiques se sont largement 
concentrés sur certains groupes qu’ils trouvaient particulièrement intéres-
sants, ils les ont réifiés par leur travail et ont continué de présenter leurs 
résultats en perpétuant une même logique. Les schémas narratifs semblent 
avoir constamment voyagé entre « science » et « grand public » et trouvé 
des deux côtés des terreaux dans lesquels s’implanter. Ainsi, s’ils paraissent 
plausibles, c’est parce qu’ils font écho à des refrains déjà entendus et 
gardés en mémoire. La façon dont les classifications scientifiques de 
l’espèce humaine s’accordent avec ce qu’en pense le sens commun peut 
être matière à soupçons – et on peut en dire autant des rapports suspects 
qu’entretiennent les catégories de la science ou du bon sens avec celles en 
usage dans l’administration. De telles catégories ont aidé à structurer le 
travail de millions d’aides-soignants, de fonctionnaires et d’éducateurs ; 
elles ont contribué à faciliter l’accès aux soins différenciés, aux privi-
lèges, au logement, à la nourriture, à l’électricité, à l’eau, à la scolarité, 
aux postes d’influence, aux tribunaux, à la protection policière. Or ce ne 
sont pas de nouvelles classifications dont nous aurions besoin. Comme le 
montre l’exemple donné par Sanghvi, remplacer la classification par races 
par une classification par castes revient à faire apparaître de nouvelles 
divisions problématiques. Tant qu’elle se traduit par une classification, 
1. Reardon 2004 ; sur le HapMap Project, voir Fujimura et Rajagopalan 2012.
2. Jobling 2012.

	
les savoirs de la diversité humaine 	
269
toute schématisation de la diversité génétique humaine fige les humains 
dans le temps, dans l’espace, ainsi que dans un ordre naturel et social.
Ce chapitre s’est fixé pour but de remettre l’histoire de la science de 
la race (et de la race dans la science) dans le contexte d’une histoire plus 
large, celle des efforts qu’ont faits les scientifiques pour comprendre la 
diversité humaine. Ce n’est qu’à partir d’un tel discours que la position 
spécifique de la « race », comparée et reliée à d’autres approches de  
la diversité, acquiert une visibilité. Mais ces autres façons de parler de la 
diversité humaine devraient aussi faire l’objet d’une attention politique et 
être regardées du même œil critique que le concept de « race » lui-même. 
Les expressions de « diversité humaine » et de « variation humaine » n’ont 
peut-être pas circulé autant dans les sphères politiques et sociales que 
la « race », et leurs occurrences présentent en comparaison un aspect 
assez innocent, inoffensif, voire positif. Mais comme pour la « race », 
malgré tous les efforts de nettoyage déployés par certains, l’apparition 
de la « diversité humaine » dans les contextes scientifiques, politiques et  
sociétaux signifie que les tensions ne sont jamais bien loin, présentes  
et palpables à l’envers d’une neutralité de façade.
La variation humaine a rempli de nombreuses fonctions instrumen-
tales et pragmatiques dans les projets et publications académiques. Elle 
a rendu de fiers services épistémiques aux scientifiques – différents mais 
non incompatibles avec la fonction idéologique qu’elle a exercée pour 
la classe politique. En tous les cas, il ne fait aucun doute que les scien-
tifiques qui ont étudié la variation humaine ou qui l’ont utilisée comme 
un instrument de connaissance défendent des positions politiques et 
qu’ils sont profondément marqués par les discours biopolitiques dans 
leur ensemble.
Ann Stoler a insisté sur la nécessité qu’il y a à réfléchir aux motivations 
qui sont les nôtres lorsque nous choisissons de raconter telle ou telle 
histoire de la race 1. Je ne prétends pas pour ma part que ce chapitre raconte 
la seule histoire possible de la compréhension de la variation humaine et 
de son interprétation comme « race ». De fait, les ramifications de cette 
histoire sont si nombreuses et complexes que nous avons probablement 
besoin d’une pluralité de récits pour rendre compte de cette complexité. 
Cependant, il me semble que mettre en avant la dimension épistémolo-
gique de l’étude de la variation humaine aide à mieux mettre en lumière 
des aspects jusqu’alors restés dans l’ombre : le danger que représente, 
en plus de l’eugénisme et du racisme, le fait de prendre l’isolement des 
groupes humains pour acquis ; celui qu’il y a à recourir continuellement, 
1. Stoler 2002, Fearnley 2009.

270	
veronika lipphardt
dans les travaux génétiques, à certaines catégories sans se soucier de savoir 
si elles sont basées sur des concepts de race ou de population ; enfin, la 
façon dont des discours répandus au sujet de certains groupes conjugués 
aux découvertes génétiques fait aujourd’hui encore le lit de l’essentia-
lisme. Tout cela permet aussi de comprendre pourquoi les sciences de 
la variation humaine alimentent de fait les discours de la « race » au lieu 
de les combattre. Ce n’est pas que les scientifiques soient nécessairement 
racistes. Il s’agit plutôt de dire que les différences qu’ils emploient comme 
si elles faisaient partie d’une pratique scientifique acceptée sont cultu-
rellement et socialement codées comme raciales. Ce qui rend bien plus 
difficile en retour, pour tout le monde et y compris pour les scientifiques, 
le combat contre le racisme, le biologisme et le déterminisme.
Traduit par Clara Breteau
Références bibliographiques
AG gegen Rassismus in den Lebenswissenschaften, 2009, Gemachte Differenz : 
Kontinuitäten biologischer « Rasse »-Konzepte, vol. 1, Münster, Unrast-Verlag.
Anderson Warwick, 2012, « Hybridity, Race, and Science : The Voyage of the Zaca 
(1934-1935) », Isis, vol. 103, p. 229-253.
Barkan Elazar, 1992, The Retreat of Scientific Racism : Changing Concepts of Race 
in Britain and the United States between the World Wars, Cambridge, Cambridge 
University Press.
Bauer Susanne, 2014, « Virtual Geographies of Belonging : The Case of Soviet and 
Post-Soviet Human Genetic Diversity Research », Science, Technology and Human 
Values, vol. 39, no 4, p. 511-537.
Bruck Carl, 1907, « Die biologische Differenzierung von Affenarten und menschlichen 
Rassen durch spezifische Blutreaktion », Berliner Klinische Wochenschrift, vol. 44, 
no 26, p. 793-797.
Cavalli-Sforza Luigi Luca, Menozzi Paolo et Piazza Alberto, 1994, The History 
and Geography of Human Genes, Princeton (NJ), Princeton University Press.
Cold Spring Harbor Symposia, 1955, Population Genetics : The Nature and Causes 
of Genetic Variability in Populations, New York, Biological Laboratory, coll. « Cold 
Spring Harbor Symposia on Quantitative Biology », no 20.
Davenport Charles B. et Steggerda Morris, 1929, Race Crossing in Jamaica, 
Washington (DC), Carnegie Institution.
Dobzhansky Theodosius Grigorievich, 1973, Genetic Diversity and Human Equality, 
New York, Basic Books.
Farber Paul Lawrence, 2011, Mixing Races : From Scientific Racism to Modern Evolu-
tionary Ideas, Baltimore (MD), Johns Hopkins University Press.
Fearnley Andrew M., 2009, « How Historians’ Beliefs about Race Have Influenced 
Histories of Racial Thought », Reviews in American History, vol. 37, no 3, p. 386-394.
Fischer Eugen, 1913, Die Rehobother Bastards und das Bastardierungsproblem beim 
Menschen, Iéna, Gustav Fischer.
Fleck Ludwik, 1980, Entstehung und Entwicklung einer wissenschaftlichen Tatsache. 

	
les savoirs de la diversité humaine 	
271
Einführung in die Lehre vom Denkstil und Denkkollektiv, Francfort-sur-le-Main, 
Suhrkamp, 9e éd.
Fujimura Joan et Rajagopalan Ramya, 2012, « Making History via DNA, Making 
DNA from History : Deconstructing the Race-Disease Connection in Admixture 
Mapping », in Keith Wailoo, Catherine Lee et Alondra Nelson (dir.), Genetics 
and the Unsettled Past : The  Collision between DNA, Race, and History, New 
Brunswick (NJ), Rutgers University Press.
Gannett Lisa, 2001, « Racism and Human Genome Diversity Research : The Ethical 
Limits of “Population Thinking” », Philosophy of Science, vol. 68, no 3, p. 479-492.
Gannett Lisa et Griesemer James, 2004, « The ABO Blood Groups : Mapping 
the History and Geography of Genes in Homo sapiens », in  Hans-Jörg Rhein-
berger et Jean-Paul Gaudillière (dir.), Classical Genetic Research and Its 
Legacy : The Mapping Cultures of Twentieth-Century Genetics, Londres, Routledge, 
p. 119-172.
Geisthövel Alexa, 2013, Intelligenz und Rasse. Franz Boas’ psychologischer Anti­ras-
sismus zwischen Amerika und Deutschland (1910-1942), Bielefeld, Transcript 
Verlag.
Gissis Snait B., 2008, « When Is “Race” a Race ? (1946-2003) », Studies in History and 
Philosophy of Biological and Biomedical Sciences, vol. 39, no 4, p. 437-450.
Herder Johann Gottfried von, 2000, « Ideas on the Philosophy of the History of 
Humankind », in Robert Bernasconi et Tommy L. Lott (dir.), The Idea of Race, 
Indianapolis (IN), Hackett, p. 23-26.
Herskovits Melville, 1927, « Variability and Racial Mixture », The American 
Naturalist, vol. 61, no 672, p. 68-81.
Hiernaux Jean, 1964, « The Concept of Race and the Taxonomy of Mankind », 
in  Ashley Montagu (dir.), The  Concept of Race, Londres, Free Press Glencoe, 
p. 29-45.
Jackson John P. et Weidman Nadine M., 2004, Race, Racism, and Science : Social 
Impact and Interaction, Santa Barbara (CA), ABC-CLIO.
Jobling Mark A., 2012, « The Impact of Recent Events on Human Genetic Diversity », 
Philosophical Transactions of the Royal Society B, vol. 367, p. 793-799.
Kaupen-Haas Heidrun et Saller Christian (dir.), 1999, Wissenschaftlicher 
Rassismus. Analysen einer Kontinuität in den Human- und Naturwissenschaften, 
Francfort-sur-le-Main, Campus.
Kenan Malik, 1996, The Meaning of Race : Race, History and Culture in Western 
Society, Basingstoke, Macmillan.
Kohn Marek, 1995, The Race Gallery : The Return of Racial Science, Londres, Cape.
Lewontin Richard C., 1953, « The Effect of Compensation on Populations Subject to 
Natural Selection », The American Naturalist, vol. 87, no 837, décembre, p. 375-381.
Lindee Susan et Santos Ricardo Ventura (dir.), 2012, numéro spécial de Current 
Anthropology, vol. 53, no 5, avril.
Lipphardt Veronika, 2008, Biologie der Juden. Jüdische Wissenschaftler über « Rasse » 
und Vererbung (1900-1935), Göttingen, Vandenhoeck & Ruprecht.
–	 2009, « “Investigation of Biological Changes”. Franz Boas in Kooperation mit 
deutsch-jüdischen Anthropologen (1929-1940) », in Hans-Walter Schmuhl (dir.), 
Kulturrelativismus und Antirassismus. Der Anthropologe Franz Boas (1858-1942), 
Bielefeld, Transcript Verlag, p. 163-186.
–	 2012, « Isolates and Crosses : Human Evolution and Population Genetics in the 
Mid-Twentieth Century », Current Anthropology, vol. 53, suppl. no 5, p. 69-82.

272	
veronika lipphardt
Livingstone Frank B., 1962, « On the Non-Existence of Human Races », Current 
Anthropology, vol. 3, no 3, juin, p. 279-281.
López-Beltrán Carlos, 1994, « Forging Heredity : From Metaphor to Cause, a Reifi-
cation Story », Studies in History and Philosophy of Science, vol. 25, p. 211-235.
Löwy Ilana et Gaudillière Jean-Paul, 2012 [2001], Heredity and Infection : 
The History of Disease Transmission, Londres, Routledge.
MacCanghey Vaughan, 1919, « Race Mixture in Hawaii », Journal of Heredity, no 10, 
p. 41-47.
Marks Jonathan, 1995, Human Biodiversity : Genes, Race, and History, Foundations 
of Human Behavior, New York, Aldine de Gruyter.
Mather Kenneth, 1962, « Biometrical Genetics of Man », in The Use of Vital and 
Health Statistics for Genetic and Radiation Studies : Proceedings of the Seminar 
Sponsored by the United Nations and the World Health Organization (Geneva 
5-9 September 1960), New York, Nations unies, p. 235-239.
–	 1964, Human Diversity : The Nature and Significance of Differences among Men, 
Édimbourg, Oliver & Boyd.
Mendelsohn Andrew, 2001, « Medicine and the Making of Bodily Inequality in 
Twentieth-Century Europe », in Jean-Paul Gaudillière et Ilana Löwy, Heredity 
and Infection : The History of Disease Transmission, Londres, Routledge, p. 21-79.
Montagu Ashley, 1947, Man’s Most Dangerous Myth : The Fallacy of Race, New York, 
Columbia University Press.
Müller-Wille Staffan et Rheinberger Hans-Jörg, 2008, « Race and Genomics : 
Old Wine in New Bottles ? », NTM Zeitschrift für Geschichte der Wissenschaften, 
Technik und Medizin, vol. 16, no 3, p. 363-386.
–	 2012, A Cultural History of Heredity, Chicago (IL), University of Chicago Press.
OMS, 1964, Research in Population Genetics of Primitive Groups, Report of a WHO 
Scientific Group, vol. 279, Genève.
Osborn Frederick, 1954, « The Origin and Evolution of Man : Cold Spring Harbor 
Symposium 1950 », Eugenics Quarterly, vol. 1, no 1, p. 67-70.
Radin Joanna, 2014, « Unfolding Epidemiological Stories : How the WHO Made 
Frozen Blood into a Flexible Resource for the Future », Studies in History and 
Philosophy of the Life Sciences, vol. 47, p. 62-73.
Reardon Jenny, 2004, Race to the Finish : Identity and Governance in an Age of 
Genomics, Princeton (NJ), Princeton University Press.
Sanghvi L.D. et Khanolkar V.R., 1950, « Data Relating to Seven Genetical 
Characters in Six Endogamous Groups in Bombay », Annals of Eugenics, vol. 15, 
no 51, p. 52-76.
Schiemann Elisabeth, 1931, « Beziehungen zwischen der Stammesgeschichte der 
Menschen und der Kulturpflanzen », Jahrbuch des Naturwissenschaftlichen Vereins 
für die Neumark in Landsberg, no 3, p. 5-15.
Stepan Nancy L., 1984, The Idea of Race in Science : Great Britain (1800-1960), 
Basingstoke, Macmillan.
Stocking George W. (dir.), 1988, Bones, Bodies, Behaviour : Essays on Biological 
Anthropology, Wisconsin (WI), University of Winsconsin Press.
Stoler Ann Laura, 2002, « Reflections on “Racial Histories and Their Regimes of 
Truth” », in Philomena Essed et David Theo Goldberg, Race Critical Theories : 
Text and Context, Malden (MA), Blackwell, p. 417-421.
Thompson Laura, 1967, « Steps toward a Unified Anthropology », Current Anthro-
pology, vol. 8, nos 1-2, p. 67-91.

	
les savoirs de la diversité humaine 	
273
Tucker William H., 2002, The Funding of Scientific Racism : Wickliffe Draper and the 
Pioneer Fund, Urbana (IL), University of Illinois Press.
Vavilov Nikolai I., 1926, Studies on the Origin of Cultivated Plants, Leningrad, 
Institut de botanique appliquée.
Wade Peter, 2002, Race, Nature and Culture : An Anthropological Perspective, 
Londres, Pluto Press.
Weissenberg Samuel, 1927, « Die gegenwärtigen Aufgaben der jüdischen Demo­­
graphie », Zeitschrift für Demographie und Statistik der Juden, no 4, p. 402-418.


13 L’écologie.  
Connaître et gouverner la nature
Y a n n i c k  M a h r a n e
Dès le milieu du xixe siècle, sous l’effet conjugué de l’essor du capita-
lisme industriel et de l’expansion territoriale des puissances européennes 
et états-unienne, des dégradations matérielles de l’environnement se font 
jour en Europe, en Amérique et dans les empires coloniaux. S’ouvre alors 
une séquence d’alertes environnementales portées par des diplomates, 
géographes, naturalistes et forestiers. Dans un contexte où l’approvision-
nement régulier en bois devient central au développement et à la stabilité 
des économies, les élites du moment dénoncent, aux États-Unis et dans 
les colonies britanniques, françaises et néerlandaises, les effets de l’exploi-
tation intensive du couvert forestier sur la modification des climats et la 
réduction des précipitations 1. En Grande-Bretagne, le développement 
du machinisme, des bateaux à vapeur, l’extension des réseaux de chemin 
de fer et des voies de communication font craindre aux autorités une 
déplétion inexorable des ressources en charbon 2. À la suite de nombreux 
voyages passés dans diverses contrées de l’Ancien et du Nouveau Monde 
et du Proche-Orient, le diplomate américain George P. Marsh alerte en 
1864 de la transformation irréversible de la planète par l’agir humain 
technique et industriel 3. Ces préoccupations environnementales sont 
également partagées en Europe par les géographes tels Élisée Reclus 
en France et Ernst Friedrich en Allemagne 4. Loin d’être marginales, ces 
critiques de l’agir anthropocénique gagnent même du terrain entre le 
dernier quart du xixe siècle et le début du xxe, comme l’ont souligné les 
historiens de l’environnement 5.
1. Starr 1866.
2. Madureira 2012.
3. Marsh 1864.
4. Reclus 1864, Raumolin 1984.
5. Bonneuil et Fressoz 2013 (p. 120).
	 Essai de radiomarquage dans une parcelle en plein champ située dans la réserve nucléaire d’Oak Ridge, 
sous la supervision d’Eugene P. Odum (debout à droite), été 1962.

276	
yannick mahrane
C’est dans cette période où les limites environnementales à l’indus-
trialisation et à la modernisation économique et agricole deviennent 
de plus en plus perceptibles et où les sociétés occidentales inter-
rogent de manière croissante les effets de leur modèle d’occupation 
agricole sur les environnements nouvellement conquis que le vocable 
d’« écologie », défini comme « science des relations de l’organisme avec 
son environnement », apparaît, en 1866, sous la plume du biologiste Ernst  
Haeckel.
Ce chapitre propose d’aborder l’évolution et les transformations de 
l’écologie mondiale à travers la description de trois régimes de production 
de savoir-pouvoir correspondant à trois périodes courant de la fin du 
xixe siècle à aujourd’hui. Il s’attache à décrire les contextes économique, 
social, culturel et politique dans lesquels l’écologie et ses sous-disciplines 
se sont forgées et en quoi ces savoirs contribuent en retour à définir 
des champs d’expertise et d’action publique en matière de gestion de la 
nature. Chacun de ces régimes se caractérise par un mode d’articulation 
particulier entre une certaine manière de voir et de gouverner le vivant 
en recourant à des ontologies et métaphores spécifiques, des pratiques 
expérimentales et des dispositifs instrumentaux, des formes de coopé-
ration et d’organisation institutionnelle de la recherche, des ordres de 
justification et des formes d’action collective, et enfin la prédominance 
d’une culture épistémique sur d’autres 1.
De la fin du xixe siècle à la fin de la Seconde Guerre mondiale, dans le 
cadre de la formation et modernisation des États-nations et de l’expansion 
des puissances impériales, se développe une écologie végétale caractérisée 
par une ontologie organiciste, organisée autour de jardins botaniques, 
de stations agricoles expérimentales et d’institutions d’enseignement 
agricole supérieur. Fondée sur une culture épistémique de la physio-
logie expérimentale et de la phytogéographie, celle-ci vise à accroître le 
contrôle étatique des territoires et des ressources par les administrations 
et à encadrer scientifiquement les pratiques agricoles.
De 1945 au milieu des années 1970 s’affirme, dans un contexte de guerre 
froide et d’un nouveau régime d’accumulation fordiste, une « nouvelle 
écologie » plus formelle et mathématisée, administrée selon un modèle 
de big science et institutionnalisée dans les laboratoires nationaux du 
complexe militaro-industriel. Celle-ci est dominée par une culture épisté-
mique de l’ingénierie, et des pratiques expérimentales centrées sur l’usage 
de radiotraceurs et la modélisation informatique. Fondée sur une ontologie 
mécaniciste et cybernétique de la nature et articulée à une promesse 
1. Pestre 2003 (p. 34).

	
connaître et gouverner la nature	
277
d’optimisation et de gestion, cette écologie vise à répondre aux besoins 
croissants en ressources naturelles des économies.
Enfin, du milieu des années 1970 à aujourd’hui, les sciences écolo-
giques entrent dans un régime « néolibéral et postfordiste » caractérisé 
par une ontologie réticulaire et économique du vivant et dans lequel les 
« systèmes socioécologiques » et les « services écosystémiques » deviennent 
les unités fondamentales d’analyse et d’action. Cette nouvelle ontologie 
du vivant s’articule à la promotion d’une « gestion adaptative », d’une 
décentralisation et d’une flexibilisation des institutions, d’une évaluation 
économique des « services » rendus par la nature et de la mise en place 
d’incitations économiques ; un rôle prédominant est par ailleurs donné 
à l’interdisciplinarité et aux partenariats public-privé.
L’émergence de l’écologie en Europe  
et aux États-Unis, fin xixe-milieu xxe siècle
L’essor de l’écologie au service  
des puissances impériales européennes
De la fin du xviie à la seconde moitié du xixe siècle, l’expansion des 
puissances impériales européennes s’accompagne d’un mouvement d’inven-
taire, de systématisation des pratiques de transfert, de mobilisation et 
d’acclimatation des ressources végétales de la planète entière, associés 
étroitement au développement d’un réseau de jardins botaniques dans 
les colonies, redessinant profondément la carte agricole du monde et 
favorisant l’institutionnalisation et la professionnalisation de la botanique 
au sein de l’histoire naturelle 1.
Cependant, dans le dernier quart du xixe siècle, dans un contexte marqué 
par un déclin économique des systèmes de plantation monoculturale 
et une volonté administrative de diversifier et de rationaliser scientifi-
quement la production agricole dans les colonies, l’inventaire botanique 
perd de son importance et les jardins botaniques se voient contraints de 
réorienter leurs activités vers la production de recherche expérimentale 
et de conseil agricole. Cette réorientation se traduit par la création de 
stations de recherche botanique favorisant l’essor de nouvelles disciplines 
scientifiques – telles que l’écologie – fondées sur les méthodes expéri-
mentales et de laboratoire visant à développer l’agriculture tropicale 2.
Parmi elles, le jardin de botanique de Buitenzorg dans les Indes 
1. Bonneuil et Bourguet 1999 (p. 21-28).
2. Headrick 1988 (p. 215).

278	
yannick mahrane
néerlandaises, financé par des organisations de planteurs, devient le point 
de séjour d’une nouvelle génération de botanistes spécialisée en physio-
logie végétale et formée aux méthodes de laboratoire. Leurs observations 
de la richesse taxinomique et de la variété des conditions environnemen-
tales de croissance des plantes de la nature tropicale les incitent à étendre 
leur cadre d’analyse expérimentale à l’étude des processus d’adaptation 
et des structures physiologiques des plantes dans leur environnement 
naturel 1. Suite aux expéditions dans les régions de Lagoa Santa au Brésil, 
aux Indes occidentales et au Groenland, Eugenius Warming (1841-1924), 
professeur et directeur du jardin botanique de Copenhague, formalise une 
géographie botanique écologique fondée sur les méthodes de la morpho-
logie végétale et de la botanique systématique. Ce nouveau savoir vise 
à décrire la distribution géographique des plantes selon l’adaptation de 
la physionomie et de la croissance des communautés végétales à leurs 
conditions environnantes telles que l’eau, la lumière, la température, etc. 
Assignant à la géographie botanique la tâche centrale d’enquêter sur « les 
problèmes concernant l’économie des plantes, les demandes qu’elles 
exercent sur leur environnement, et les moyens qu’elles emploient pour 
utiliser leurs conditions environnantes et adapter leur structure interne et 
externe et leur forme générale 2 », cette mise en ordre de la nature reposant 
sur une métaphore « économique » permet à Warming de légitimer la 
géographie botanique comme un outil d’extension du contrôle impérial 
danois sur les ressources des régions étrangères 3.
Si la première société savante d’écologie (la British Ecological Society) 
voit le jour en Grande-Bretagne en 1913, ce n’est que dans la période de 
l’entre-deux-guerres qu’une écologie végétale et une écologie animale se 
structurent et s’institutionnalisent durablement. Peder Anker montre ainsi 
comment la discipline écologique s’affirme dans cette période de l’Empire 
britannique grâce au soutien de deux réseaux de patronage impérial. 
L’un est situé à la périphérie, en Afrique du Sud, autour de la figure de 
Jan C. Smuts, l’autre au centre autour de « l’école d’écologie impériale 
d’Oxford ». Ce patronage contribue à l’introduction dans les adminis-
trations coloniales britanniques d’Afrique des méthodes écologiques 
employées comme instrument de gestion des ressources et d’encadrement 
scientifique des pratiques agricoles des populations locales. Des relevés 
et des cartographies écologiques de la végétation, des sols, des systèmes 
de culture locaux, fondés sur un examen détaillé des corrélations entre 
1. Cittadino 1990 (p. 134-145).
2. Warming 1909 (p. 2).
3. Anker 2001 (p. 13).

	
connaître et gouverner la nature	
279
facteurs environnementaux et types de couvert végétal, déboucheront 
notamment sur des politiques de développement et de réaménagement 
agricole en Rhodésie du Nord dans les années 1940 1.
C’est dans le cadre des luttes que se livrent les puissances européennes 
pour le contrôle des territoires et des ressources minérales, et de stratégies 
d’intégration économique impériale visant à favoriser les échanges et le 
commerce entre colonies et dominions britanniques, que le zoologue 
Charles Elton développe une nouvelle ligne de recherche sous le nom 
d’« écologie animale ». Engagé à titre de consultant par des institutions 
impériales comme la Compagnie de la baie d’Hudson et l’Empire Marketing 
Board pour développer des modèles de prévision des cycles des popula-
tions animales (visant à stabiliser les marchés de fourrure), Elton forge 
une représentation synoptique de la nature, une économie naturelle dans 
laquelle les espèces animales sont structurées et régulées par des chaînes et 
cycles alimentaires, taille et type de nourriture et pyramide des nombres 2.
L’écologie végétale organiciste aux États-Unis  
de la Progressive Era au New Deal. Entre science,  
agriculture et conservationnisme
Entre le dernier quart du xixe siècle et le début du xxe, une écologie 
végétale dynamique émerge et s’institutionnalise aux États-Unis à la 
croisée de trois dynamiques : crises agricole et environnementale causées 
par un peuplement, une mise en culture et une urbanisation rapides des 
prairies ; des initiatives fédérales en matière de recherche et de politique 
agricole en réponse à ces crises ; et enfin, une dynamique de profession-
nalisation de la science botanique.
À partir du milieu du xixe siècle, le paysage environnemental des 
États-Unis se voit profondément remodelé sous l’effet du capitalisme 
industriel et agricole intensif et la croissance des métropoles urbaines. 
L’adoption de l’Homestead Act en 1862 et l’achèvement en 1869 du 
premier chemin de fer transcontinental accélèrent le peuplement de 
l’Ouest et l’expansion des monocultures céréalières, l’élevage extensif du 
bétail et l’industrie sylvicole sur les Grandes Plaines. S’ensuivent alors une 
érosion des sols, une disparition des graminées et plantes fourragères, 
une augmentation d’espèces invasives et d’insectes nuisibles, une dégra-
dation du couvert forestier et une diminution drastique de la capacité de 
charge de nombreux pâturages.
1. Hodge 2007 (p. 144-178).
2. Anker 2001, Erickson 2010.

280	
yannick mahrane
Dans ce contexte se met en place, durant la Progressive Era, un nouvel 
interventionnisme fédéral en matière de politique et de recherche agricole 
qui favorise l’émergence de la génétique mendélienne, de l’agronomie,  
de l’écologie végétale et du conservationnisme scientifique. Les adoptions 
consécutives du Morrill Land-Grant Act en 1862, du Hatch Act en 1887 
et du Morill Act en 1890 entérinent l’institutionnalisation d’un système 
national d’expertise scientifique agricole au service de la modernisation 
agricole. Il est organisé autour d’institutions d’enseignement scientifique 
supérieur, de collèges et de stations expérimentales agricoles créés dans 
chaque État, coordonnés par le département de l’Agriculture des États-Unis, 
fondé en 1862, et approvisionnés par d’importants fonds fédéraux.
Cette institutionnalisation d’une recherche scientifique agricole offre un 
cadre propice à la réception de la génétique mendélienne et à l’essor des 
techniques expérimentales d’hybridation et de croisement conduisant vers 
1910 à la culture du maïs hybride. Au cours de cette période s’introduit 
aussi, au plus haut niveau de l’appareil d’État, un mouvement scienti-
fique conservationniste œuvrant activement à la mise en place d’une 
planification rationnelle et technocratique centralisée des ressources.  
Il débouche notamment sur la création, en 1876, d’un Service des forêts 
en charge de l’administration et de l’exploitation des forêts selon une 
logique du « rendement soutenu ».
Dans cette vague d’expansion de la recherche agricole et d’intervention 
conservationniste, un nombre croissant de botanistes américains localisés 
majoritairement dans les universités et stations agricoles du Midwest 
formalisent une écologie végétale dynamique. Soucieux de démontrer 
l’utilité pratique de leur botanique expérimentale dans l’effort agricole, 
ils se tournent vers l’étude en plein air des processus et des fonctions 
des formations végétales. C’est ainsi que le département de botanique 
de l’université du Nebraska s’affirme comme le creuset de la « Grassland 
School of Ecology 1 ».
Frederic Clements, professeur de botanique et de physiologie végétale, y 
formalise au début du xxe siècle une écologie végétale organiciste fondée 
sur une théorie de la succession végétale qui dominera pendant près d’un 
demi-siècle la recherche écologique états-unienne. Puisant sa grammaire 
et ses méthodes dans la physiologie expérimentale, Clements définit les 
formations végétales comme des organismes complexes dotés de struc-
tures et de fonctions susceptibles des mêmes méthodes d’investigation 
et d’expérimentation que celles pratiquées sur les organismes simples au 
laboratoire. Pour Clements, ces formations végétales passent par différentes 
1. Tobey 1981.

	
connaître et gouverner la nature	
281
phases de développement gouvernées par les facteurs édaphiques et clima-
tiques, débutant par quelques espèces pionnières sur un sol vierge (des 
graminées par exemple), remplacées ensuite par de nouvelles associa-
tions pour aboutir à un assemblage végétal stable contrôlé par le climat 
appelé « climax » (par exemple, un peuplement forestier) 1.
Ce n’est pas une coïncidence si cette logique de la succession végétale 
semble reproduire celle de la célèbre thèse de la Frontière, forgée en 1893 
par l’historien Frederick J. Turner, où l’histoire des États-Unis apparaît 
comme le développement successif de conquêtes et de peuplements de 
terres « vierges ». L’historienne des sciences Sharon Kingsland établit 
ainsi un parallèle saisissant entre l’essor de l’écologie scientifique aux 
États-Unis et les problèmes d’adaptation et d’aménagement agricoles 
qu’ont rencontrés les pionniers à mesure que la frontière de peuplement 
se déplaçait vers l’ouest 2.
La formalisation et les pratiques scientifiques de cette écologie végétale  
organiciste s’appuient sur de nouvelles méthodes d’investigation 
expérimentale, une utilisation systématique d’instruments de mesure 
(thermomètre, psychromètre, etc.) et de nouvelles techniques de quanti-
fication, constituant une économie morale fondée sur une quête de la 
précision et une pratique plus distanciée de la nature. Elle vise tout autant 
à transformer l’écologie en une science rigoureuse et exacte proche des 
standards du laboratoire qu’à garantir la scientificité d’une discipline 
pratiquée en plein air et à en contrôler l’accès en en excluant les botanistes 
amateurs 3. Utilisé comme technique d’échantillonnage, le quadrat permet 
à l’écologue d’estimer et de quantifier l’abondance d’une ou de plusieurs 
espèces végétales et d’en comparer la fréquence statistique entre différentes 
zones composant une formation. En transformant les plantes en indicateurs 
des conditions physico-chimiques de leur milieu de croissance, la station 
de phytomètres se donne comme un « micro-monde » dans lequel l’éco-
logue est en mesure de contrôler et de mesurer, avec la même précision 
qu’au laboratoire, les facteurs environnementaux (humidité, teneur en 
eau, composition des sols, lumière, précipitation, vent, etc.) influençant 
le développement d’une formation végétale et les réponses des plantes.
Loin d’être poursuivie comme un simple savoir théorique et acadé-
mique, l’écologie végétale de l’école du Nebraska se développe au contraire 
à l’interface des mondes universitaire et agricole et de la recherche fonda-
mentale et appliquée, contribuant à redéfinir dans les années 1910-1920 
1. Clements 1905.
2. Kingsland 2005.
3. Kohler 2002.

282	
yannick mahrane
les pratiques de la foresterie, de l’agronomie et à forger une gestion scien-
tifique des pâturages. Mus par les mêmes impératifs économiques que 
ses coreligionnaires conservationnistes, ces écologues ambitionnent 
d’ériger l’écologie au rang de discipline maîtresse dans l’aménagement 
des ressources naturelles états-uniennes. De fait, cette description de la 
nature comme un processus ordonné permet à l’écologue de montrer 
comment l’écologie est en mesure de prédire avec précision les dynamiques 
végétales dans une aire donnée et de désigner les milieux les plus adaptés à 
la culture d’une espèce végétale particulière. Outre sa portée épistémique, 
l’emploi de l’analogie organiciste constitue aussi une stratégie rhétorique 
permettant de légitimer la figure de l’écologue en tant qu’expert dans la 
planification et la gestion des ressources naturelles 1.
Mais c’est dans le contexte du New Deal accentué par la catastrophe 
environnementale du Dust Bowl que cette écologie accède véritablement 
au rang d’expertise publique. Tandis que les États-Unis entrent dans une 
1. Nicolson 1989.
Station de phytomètres conçue par Clements pour contrôler et mesurer le rôle des 
facteurs environnementaux dans la croissance des plantes.

	
connaître et gouverner la nature	
283
période de récession économique après le krach boursier de 1929, une 
série de tempêtes de poussière et de sable frappe, pendant près d’une 
décennie, les Grandes Plaines. Ces tempêtes résultent de la conjonction 
de longues périodes de sécheresse et d’une érosion exacerbée des sols 
provoquée par la surexploitation agricole. Rendus impropres à toute 
culture, les États du Midwest voient l’exode de millions de personnes, 
retracé dans Les Raisins de la colère de John Steinbeck. En réponse à cette 
crise agricole et environnementale, le président Roosevelt lance dans  
le cadre du New Deal une vaste politique de réhabilitation agricole des 
sols concourant au recrutement d’écologues du Nebraska à titre d’experts, 
et à l’élaboration de politiques publiques en réhabilitation et conser-
vation des sols des Grandes Plaines, fondées sur les principes et concepts 
cardinaux de l’écologie végétale clementsienne 1.
Les transformations de l’écologie après la Seconde  
Guerre mondiale, entre technocratie et environnementalisme
Au sortir de la Seconde Guerre mondiale, les sciences écologiques 
connaissent, particulièrement aux États-Unis, une profonde reconfigu-
ration de leurs pratiques instrumentales, de leur manière de voir et de 
gouverner la nature, dans le cadre d’un régime d’accumulation fordiste, 
d’un programme nucléaire civil et militaire, et d’une nouvelle alliance de 
la recherche avec le complexe militaro-industriel.
Une nature fordiste
Afin de garantir la prévisibilité des investisseurs et gouvernements 
dans le secteur de la pêche industrielle alors en plein essor et marquée 
par de fortes fluctuations des débarquements commerciaux, des biolo-
gistes marins sont sollicités pour développer de nouveaux modèles 
statistiques des populations halieutiques. Redéfinissant les populations 
de poissons comme des systèmes autorégulés tendant naturellement  
vers l’équilibre, caractérisés par des paramètres biologiques définis – taux 
de reproduction, de croissance spécifique, de mortalité naturelle et par 
pêche –, ces modèles contribuent à transformer ces populations en objets 
prédictibles et gouvernables dont il devient possible de calculer des taux 
d’exploitation optimale 2.
1. Masutti 2006.
2. Bavington 2010, Finley 2011.

284	
yannick mahrane
En congruence avec ce nouveau cadre de modernisation économique 
et industrielle d’après guerre sont aussi forgées de nouvelles approches 
centrées sur la notion d’écosystème. La science des écosystèmes re- 
conceptualise la nature comme une usine fordiste automatisée traversée 
par des intrants et des extrants d’énergie et de matière, organisée autour 
d’une chaîne de montage standardisée et une division spécialisée du 
travail – la chaîne trophique dans laquelle les espèces sont rassem-
blées en groupes fonctionnels selon la tâche qu’elles accomplissent dans 
ce processus de production et de transformation d’énergie. Dans cette 
approche, le rendement énergétique (vitesse de conversion de l’énergie en 
biomasse entre différents compartiments trophiques) devient le nouveau 
critère d’évaluation de performance de la nature, et le rôle principal de 
l’écologue se résume, à l’instar de l’ingénieur taylorien, à mesurer la 
« productivité biologique » sous la forme de budget énergétique au moyen 
de différents dispositifs de mesure (bombe calorimétrique, mesure des 
échanges gazeux d’une plante dans des enceintes expérimentales, etc.) 1.
Cette redéfinition de la nature comme un système autorégulé suscep-
tible de contrôle et d’optimisation participe d’une dynamique plus large 
de reconfiguration du champ des savoirs, catalysée par l’essor de nouvelles 
approches technoscientifiques élaborées en temps de guerre. Issue des 
travaux du mathématicien Norbert Wiener et de l’ingénieur Julian Bigelow 
sur une machine de prédiction de tir dans le cadre de la lutte anti-aérienne 
durant la mobilisation, la cybernétique s’offre après guerre comme une 
nouvelle grammaire générale dans laquelle tous les objets sont suscep-
tibles d’une analyse en termes de systèmes, de boucles de rétroaction et 
de mécanismes de contrôle 2.
Sécuriser l’atome
Ce sont toutefois les opportunités de recherche et de financement liées 
au développement du programme nucléaire civil et militaire et à une 
préoccupation croissante des instances pour les effets et mécanismes  
de contamination radioactive, qui vont favoriser l’institutionnalisation de 
l’écologie des écosystèmes dans les années 1950. Bénéficiant du patronage 
du complexe militaro-industriel, l’écologie va alors s’affirmer comme 
un dispositif clé de contrôle et de sécurisation de l’énergie atomique et 
intégrer dans ses pratiques de nouveaux outils tels que les radiotraceurs 
et les calculateurs modernes.
1. Odum 1959.
2. Dahan et Pestre 2004.

	
connaître et gouverner la nature	
285
Entre 1961 et 1976, la division « Médecine et biologie » de la Commission 
de l’énergie atomique des États-Unis coordonne plus de 60 programmes 
de recherche en écologie des écosystèmes, au sein des laboratoires 
nationaux d’Hanford, de Brookhaven, d’Oak Ridge et de la centrale de 
la Savannah River afin de mesurer et de prédire les seuils de tolérance 
des organismes et des écosystèmes aux radiations ionisantes. À l’opposé 
du mouvement écologiste qui se structure dès la fin des années 1950 en 
dénonçant l’impact des essais nucléaires sur la santé et l’environnement, 
ces écologues arborent un environnementalisme optimiste et technocra-
tique. En témoignent les espoirs qu’Eugene Odum place en 1959 dans la 
technologie des radiotraceurs pour résoudre les problèmes environne-
mentaux liés au développement de l’énergie atomique :
Certaines des choses que nous craignons le plus dans le futur, la radioac-
tivité par exemple, nous aideront, si elles sont intelligemment étudiées, à 
résoudre les problèmes mêmes qu’elles créent. C’est pourquoi les isotopes 
utilisés comme traceurs nous permettront d’élucider les processus de renou-
vellement que nous devons comprendre avant que ne soient libérés en toute 
sécurité les déchets radioactifs dans l’environnement 1.
La recherche écologique entreprise dans ces laboratoires nationaux n’est 
cependant pas exclusivement orientée vers la résolution de problèmes de 
gestion environnementale de la technologie nucléaire. Des historiens des 
sciences ont en effet montré combien ces écologues parviennent, à l’inté-
rieur des contraintes qui leur sont imposées, à définir et à développer 
leurs propres programmes de recherche, dont attestent l’introduction 
de la technique des marqueurs radio-isotopiques dans les pratiques des 
écologues comme technologie de savoir et l’essor de l’écologie systémique 2.
En effet, si ces études expérimentales répondent avant tout à un impératif 
de sécurité technologique, elles s’avèrent être aussi de puissants outils 
de connaissance mobilisés par les écologues pour délimiter et isoler plus 
finement les chaînes trophiques, tracer la circulation et les transforma-
tions de matière et d’énergie d’un compartiment à l’autre et en quantifier 
les taux de résidence et de renouvellement.
C’est l’espace institutionnel du laboratoire national d’Oak Ridge, et 
les infrastructures informatiques qu’il offre, qui contribuent à en faire le 
berceau de l’écologie systémique. Décrits comme formant un mouvement 
social modernisateur cherchant à révolutionner les pratiques de l’éco-
logie, ces écologues formalisent l’écologie systémique comme un nouveau 
1. Odum 1959 (p. v et vi).
2. Bocking 1995.

286	
yannick mahrane
programme de recherche hybride alliant modélisation mathématique, 
méthodologies de la recherche opérationnelle ou de l’analyse des systèmes, 
et simulation sur des calculateurs analogiques ou numériques 1. Élaborées 
en temps de guerre pour rationaliser la lutte anti-sous-marine et la  
logistique des forces de frappe américaines et britanniques, ces approches 
se disséminent après guerre dans les secteurs académique, industriel 
et gouvernemental et colonisent l’écologie. L’analyse des systèmes est 
utilisée par ces écologues comme une méthode permettant d’identifier, 
de dénombrer et de classer les éléments importants du système écolo-
gique à étudier, et comme un moyen pour transformer l’écologie en une 
science de gestion des systèmes écologiques complexes et un outil d’aide 
à la décision publique. La bonne gestion des ressources se voit reformulée 
en un problème d’optimisation permettant de concilier la maximisation à 
court terme de la productivité biologique des écosystèmes et le maintien 
à long terme des processus écologiques renouvelant les ressources 2.
Si ce nouveau programme de recherche reste, dans un premier temps, 
largement confiné dans les laboratoires du complexe militaro-industriel, ses 
promoteurs parviennent dans les années 1960 à l’imposer plus largement 
comme un savoir clé dans la gestion des problèmes environnementaux.
Le Programme biologique international entre environnementalisme,  
responsabilité sociale et big science
De 1964 à 1974, le Conseil international des unions scientifiques et 
l’Union internationale des sciences biologiques coordonnent le Programme 
biologique international (PBI). Souhaitant relever le statut académiquement 
faible de la discipline écologique dans les départements de biologie en 
en démontrant son utilité pratique dans la résolution des problèmes 
environnementaux globaux (pollutions, déplétion des ressources, etc.) 
et le « bien-être humain », ses architectes proposent la mise en place à 
l’échelle mondiale de stations d’observation et de mesures simultanées  
de la « capacité productive de la planète ». Avec plus de 2 000 projets 
répartis dans 43 pays participants, 200 rencontres internationales et la 
publication de 24 manuels, la science des écosystèmes bénéficie, tant 
à l’échelle nationale qu’internationale, d’une institutionnalisation sans 
précédent.
Aux États-Unis, le PBI prend place à la croisée de deux dynamiques. 
Cherchant à capitaliser la montée des préoccupations environnementales 
1. Kwa 1993a et 1993b.
2. Watt 1968 (p. 54).

	
connaître et gouverner la nature	
287
des années 1960, l’Ecological Society of America (ESA) entreprend une 
redéfinition stratégique de la responsabilité sociale de l’écologie au moyen 
d’une tactique d’intéressement et d’enrôlement du mouvement écologiste 
et des parlementaires américains pour positionner l’écologie systémique 
comme un point de passage obligé dans la définition et la résolution des 
problèmes environnementaux. En quête d’une solution technoscienti-
fique à la crise environnementale et d’un désarmement de la critique 
écologiste par son institutionnalisation, les parlementaires américains, 
séduits par l’idéal d’ingénierie et de contrôle de la nature que semble  
alors offrir l’écologie systémique, appuient favorablement la proposition 
d’un PBI américain 1. La participation active des pays du bloc de l’Est 
dans ce programme suscite également l’intérêt du président Johnson qui 
le désigne comme un modèle de coopération scientifique internationale 
indispensable à une politique de détente et appelle toutes les nations du 
monde à la création d’un conseil international de l’environnement humain 2.
Bénéficiant d’un montant de 43 millions de dollars pour une période de 
six ans (1968-1974), le PBI américain mobilise plus de 1 800 chercheurs 
et consolide l’entrée de l’écologie des écosystèmes dans un modèle de 
big science et la rapprochant des sciences environnementales plané-
taires comme l’océanographie, la géophysique, la météorologie et les 
sciences de l’atmosphère. Chacun des cinq projets est responsable de la 
modélisation et de la simulation informatique d’un des grands biomes 
couvrant le territoire états-unien. L’ambition est d’améliorer la capacité 
prédictive de l’écologie des réponses des écosystèmes à des perturbations 
environnementales ou à différentes stratégies d’exploitation et de rationa-
liser scientifiquement la gestion des ressources naturelles. Organisés et 
administrés selon une structure extrêmement hiérarchique et centralisée, 
ces projets font appel à de grandes équipes scientifiques interdiscipli-
naires composées de responsables administratifs, de savants de terrain, 
de techniciens, de programmateurs et de modélisateurs travaillant sur des 
calculateurs numériques à grande vitesse et à la constitution de banques 
centrales de données observationnelles 3. Construits sur un nombre 
restreint de variables physiques supposées déterminer l’ensemble des 
variables de l’écosystème et échouant à intégrer des paramètres tels que 
la différenciation spatiale et le rôle de la connectivité dans les réseaux 
trophiques, ces modèles s’avèrent cependant rapidement réductionnistes 
et déterministes 4.
1. Kwa 1987.
2. Lyndon B. Johnson, « Commencement Address at Glassboro State College », 4 juin 1968.
3. Aronova et al. 2010.
4. Kwa 1993a et 1993b.

288	
yannick mahrane
L’institutionnalisation des préoccupations environnementales dans 
l’appareil d’État avec l’adoption du National Environmental Policy Act 
(NEPA) en 1969 et le relatif succès du PBI conduisent, au début des années 
1970, à la création des premiers départements et de programmes d’ensei-
gnement dédiés exclusivement à l’écologie au sein des universités. De plus, 
l’obligation du NEPA pour tous les aménageurs de procéder à des études 
d’impact environnemental offre aux écologues une nouvelle niche profes-
sionnelle dans le champ de l’expertise et de la consultance environnementale.
Une écologie libérale entre gestion adaptative  
et économicisation, 1973-2013
Une nature « hors de contrôle » ?
Les années 1970 voient l’essor d’un nouveau régime de savoir dans 
lequel la biosphère est redéfinie comme un réseau de systèmes écolo-
giques complexes adaptatifs, enchevêtrés, au comportement stochastique 
soumis à des surprises et des chocs constants. Dans cette approche, 
l’hétérogénéité spatiale, la variabilité temporelle, les perturbations ou  
les catastrophes (destruction d’un littoral par une tempête, récurrence  
d’un feu dans un parc ou une forêt), qu’elles soient naturelles ou d’origine 
anthropique, apparaissent comme constitutives des dynamiques et 
processus de renouvellement des écosystèmes. Les états d’équilibre ne 
correspondent qu’à des états transitoires du système observé.
Certes, le reflux des notions d’équilibre et de stabilité en écologie 
doit beaucoup au succès des travaux sur les structures dissipatives et 
l’auto-organisation des systèmes (Prigogine), des théories du chaos et des 
catastrophes (René Thom) et des sciences de la complexité diffusées par 
le Santa Fe Institute ; mais c’est aussi à la croisée de dynamiques sociales, 
culturelles et économiques bien plus profondes encore que cette nouvelle 
manière de voir le vivant s’est imposée.
Dans un contexte marqué au début des années 1970 par une vaste fronde 
anti-environnementale, une instrumentalisation croissante de l’expertise 
écologique par des cabinets privés et une crainte de perte d’autonomie 
professionnelle, les écologues et l’ESA se dissocient de l’écologisme 
contestataire pour se replier sur des préoccupations plus académiques 
et sur l’écologie théorique 1. Ce retrait de l’arène activiste s’accompagne 
d’une délégitimation de l’écologie systémique et de la promotion d’une 
1. Nelkin 1977.

	
connaître et gouverner la nature	
289
nouvelle culture épistémique de l’écologie évolutionniste. Plus conforme 
à l’idéologie entrepreneuriale des années 1980, celle-ci voit en effet l’évo-
lution moins comme une marche progressive vers plus de complexité, 
d’interdépendance et d’autorégulation que comme le résultat d’individus 
spécifiques égoïstes poursuivant des stratégies d’optimisation afin d’aug-
menter leur succès de reproduction. En outre, l’échec de l’intervention 
militaire au Vietnam, l’affaiblissement du modèle de l’État planificateur 
keynésien, le choc pétrolier, l’essor d’un nouveau régime d’accumulation 
postfordiste, contribuent tout autant à un désenchantement, en écologie, 
en météorologie et en économétrie, à l’égard des modélisations intégrées 
à grande échelle, et à un reflux des notions d’équilibre, de stabilité et 
d’idéal de contrôle qui sous-tendaient ces modèles 1.
Les travaux de l’écologue canadien Crawford S. Holling constituent un 
tournant majeur à cet égard. Tenant pour responsable de l’effondrement 
d’un grand nombre de ressources halieutiques l’approche dite « centrée 
sur l’équilibre », mobilisée dans la gestion des pêches, Holling propose 
dès 1973 de lui substituer une approche « centrée sur la résilience ».  
Cette écologie de la résilience cherche à mesurer l’amplitude des chocs et 
des perturbations qu’un écosystème peut absorber sans altérer ses carac-
téristiques fonctionnelles garantissant son adaptation 2.
Loin de rompre avec tout idéal managérial, l’affirmation de cette 
écologie post-équilibre s’est au contraire accompagnée d’un passage d’une 
stratégie de contrôle des écosystèmes vers une stratégie de gestion où 
ce sont les collectifs humains et leurs interactions avec ces écosystèmes 
complexes qui deviennent les nouvelles cibles d’intervention 3. L’importance 
croissante du corpus de recherche dédié à l’analyse des « systèmes socio- 
écologiques » en témoigne.
Ces nouveaux experts approchent moins la gestion de la nature comme 
« la préservation de toutes les espèces » ou comme le maintien d’un 
rendement soutenu maximal que comme une stratégie visant à « garantir 
la capacité des écosystèmes, soumis aux pressions exercées par l’activité 
économique, à évoluer de manière créative dans un monde incertain 4 ». 
Cette écologie post-équilibre a aussi pour fonction de neutraliser et 
dépolitiser toute critique des limites environnementales à la croissance 
économique. Prenant soin de ne pas pointer la responsabilité du capita-
lisme et de l’industrie, cette nouvelle génération d’écologues cadre la 
crise environnementale globale comme la conséquence d’une rigidité  
1. Kwa 1994.
2. Holling 1973.
3. Bavington 2002.
4. Perrings et al. 1995 (p. 4).

290	
yannick mahrane
des instances bureaucratiques et d’un excès de réglementation étatique. Dès 
lors, les solutions ne pourront venir que d’« approches plus innovantes », 
« d’agences plus flexibles, d’industries plus autonomes et de citoyens 
mieux informés 1 ».
Appliquée depuis une trentaine d’années dans les domaines des pêcheries 
et de la foresterie, une approche intitulée « gestion adaptative » conçoit 
la gestion de la nature et les chocs et surprises qui en découlent comme 
un processus d’expérimentation et d’apprentissage itératif et de réajus-
tement des conduites en fonction de ce qui a été appris. Cette nouvelle 
écologie post-équilibre s’intègre parfaitement dans une logique qui met 
l’accent sur l’innovation, la flexibilisation des processus de production 
et la décentralisation de la gestion. Bien plus encore, Jeremy Walker et 
Melinda Cooper ont souligné la congruence idéologique de ce nouveau 
paradigme écologique avec la gouvernementalité néolibérale qui gagne 
les arènes internationales de gouvernance via l’épistémologie hayekienne 
des systèmes complexes 2.
La nature comme « réseau connecté »
Profondément influencé par les premières images de la Terre prises 
depuis l’espace, le développement des technologies satellitaires et des 
ordinateurs personnels, ce nouveau régime de savoir-pouvoir se carac-
térise par un rejet explicite de la métaphore de la machine propre à 
l’écologie systémique, et la promotion de nouvelles métaphores puisées 
dans les nouvelles technologies de l’information. C’est ainsi que l’éco-
logue Daniel Botkin affirme que si
l’âge de la machine et ses métaphores sont inappropriés pour expliquer 
comment les systèmes écologiques fonctionnent et comment nous pouvons 
les maintenir, […] les ordinateurs fournissent de nouvelles métaphores […] 
pour notre perception de la vie sur la planète […] Ils nous permettent […] 
de percevoir la forêt comme composée de nombreux arbres individuels 
croissant, absorbant de l’air, de l’eau et de l’azote, produisant des semences, et 
mourant – tous ces processus se déroulant simultanément, de manière inter-
connectée bien qu’indépendante 3.
Marqueur de l’avènement de ce nouveau paradigme réseau, l’écologie 
du paysage, forgée dans les années 1980, appréhende la nature comme une 
1. Holling et Meffe 1996 (p. 331).
2. Walker et Cooper 2011.
3. Botkin 2012 (p. 158 et 177).

	
connaître et gouverner la nature	
291
mosaïque paysagère, modifiée et structurée par les activités humaines, 
composées de taches d’habitats reliées entre elles par des réseaux constitués 
de corridors, de filtres et de barrières. La taille des taches, leur distance 
et leur degré de fragmentation et de connectivité conditionnent la distri-
bution spatiale et la capacité de survie et de reproduction des populations. 
L’écologie du paysage se fonde sur une mise en forme spatiale, mathé-
matique et numérique du paysage rendue possible par l’usage couplé  
de la photographie aérienne, de la cartographie, des outils de télémétrie 
et télédétection. Ces outils permettent de collecter des données géoré-
férencées sur les configurations paysagères qui sont ensuite intégrées et 
analysées sur des Systèmes d’informations géographiques (SIG). Cette 
nouvelle représentation de la nature est étroitement associée à la mise en 
place de dispositifs de conservation réticulaires tels que la Trame verte 
et bleue en France. Ces « réseaux écologiques » sont conçus pour piloter 
des flux de gènes, d’organismes, et des processus physico-chimiques à 
l’échelle d’un paysage. Ces dispositifs sont composés de « zones nodales » 
conçues comme des réservoirs de dispersion et de migration des espèces 
et de « corridors » jouant le rôle de liaison fonctionnelle pour favoriser 
les échanges physiques et génétiques entre les habitats.
Une nouvelle économie de la nature
Un autre élément saillant de ce nouveau régime repose dans la re-concep-
tualisation de la nature comme « capital naturel » pourvoyeur de « services 
écosystémiques » et dont la bonne conservation dépendrait d’une mise 
en économie et de la création de « marchés environnementaux ». On 
serait alors en train d’assister à l’intégration progressive du système Terre 
dans le système financier mondial. Christophe Bonneuil a ainsi analysé 
comment la transition d’une représentation ressourciste de la nature 
appréhendée comme stock d’entités (gènes, espèces, réserves, etc.) à une 
représentation de la nature comme flux de services d’actifs financiers à 
valoriser s’est opérée main dans la main avec le basculement d’un capita-
lisme industriel fordiste en un capitalisme financiarisé 1.
L’essor et l’institutionnalisation de l’économie écologique au début des 
années 1990 ont joué un rôle clé dans ce déplacement. Depuis le milieu 
des années 2000, de nouvelles plateformes scientifiques coordonnées 
par des universitaires, des ONG de la conservation (TNC, WWF, CI) et 
des cabinets de conseil privé, ont vu le jour. Combinant les approches 
quantitatives et de modélisation de l’écologie et les méthodes d’évaluation 
1. Bonneuil 2015.

292	
yannick mahrane
monétaire de l’économie de l’environnement, celles-ci se proposent de 
monétiser la nature pour attirer le capital privé dans le secteur de la 
conservation. Aussi l’une des figures de proue de cette nouvelle approche 
affirme-t-elle que « la science des services écosystémiques a besoin de 
progresser rapidement. En promettant un retour sur investissement dans 
la nature, la communauté scientifique doit offrir les connaissances et les 
outils nécessaires pour prédire et quantifier ce retour 1 ».
Ces plateformes (The Natural Capital Project, ARtificial Intelligence 
for Ecosystem Services, MIMES, etc.), mobilisant économistes, biolo-
gistes de la conservation, développeurs et programmateurs, analystes 
d’affaires, développent des outils quantitatifs et de modélisation spatia-
lement explicite sur des logiciels SIG libres, permettant une quantification, 
une cartographie spatiale et une évaluation économique des usages  
de ces « services écosystémiques » selon la couverture biophysique et la 
répartition d’occupation des sols. De nombreuses expériences pilotes à 
travers le monde, fondées sur ces méthodologies, travaillent actuellement 
au développement de marchés volontaires et de mécanismes qualifiés de 
« paiements pour services environnementaux » mettant en relation des 
« bénéficiaires » et des « fournisseurs ».
Cet alignement des objectifs de la conservation sur les intérêts du 
capital privé a eu deux effets majeurs sur l’écologie scientifique. D’une 
part, l’écologie est de plus en plus réduite à un rôle subsidiaire de techno-
logie métrique visant à certifier la valeur et à garantir la crédibilité des 
« services écosystémiques » échangés sur des marchés environnementaux 2. 
C’est dans cette direction que le National Research Council a récemment 
demandé aux écologues d’accroître leurs efforts pour développer des 
modèles écologiques dont les résultats « pourront être utilisés comme 
une donnée dans l’analyse économique 3 ». D’autre part, cet alignement a 
favorisé l’essor d’une nouvelle niche professionnelle pour les écologues dans 
des bureaux d’études privés, des banques d’investissement et des banques 
de compensation (plus de 500 aux États-Unis aujourd’hui et connaissant 
une expansion rapide en Europe et dans le monde) offrant des services 
de génie et de restauration écologique pour réduire ou compenser les 
impacts des industries et aménageurs sur l’environnement.
1. Daily et al. 2009 (p. 21).
2. Robertson 2006.
3. National Research Council 2005 (p. 257).

	
connaître et gouverner la nature	
293
 
Références bibliographiques
Anker Peder, 2001, Imperial Ecology : Environmental Order in the British Empire 
(1895-1945), Cambridge, Cambridge University Press.
Aronova Elena, Baker Karen et Oreskes Naomi, 2010, « Big Science and Big 
Data in Biology : From the International Geophysical Year through the Interna-
tional Biological Program to the Long Term Ecological Research (LTER) Network, 
1957-Present », Historical Studies in the Natural Sciences, vol. 40, no 2, p. 183-224.
Bavington Dean, 2002, « Managerial Ecology and Its Discontents : Exploring the 
Complexities of Control, Careful Use and Coping in Resource and Environmental 
Management », Environments, vol. 30, no 3, p. 3-21.
–	 2010, « From Hunting Fish to Managing Populations : Fisheries Science and the 
Destruction of Newfoundland Cod Fisheries », Science as Culture, vol. 19, no 4, 
p. 509-528.
Bocking Stephen, 1995, « Ecosystems, Ecologists, and the Atom : Environmental 
Research at Oak Ridge National Laboratory », Journal of the History of Biology, 
vol. 28, no 1, p. 1-47.
Bonneuil Christophe, 2015, « Une nature liquide ? Les discours de la biodiversité 
dans le nouvel esprit du capitalisme », in Frédéric Thomas et Valérie Boisvert 
(dir.), Le Pouvoir de la biodiversité. Néolibéralisation de la nature dans les pays 
émergents, Paris, Quæ, p.193-213.
Bonneuil Christophe et Bourguet Marie-Noëlle, 1999, « De l’inventaire du globe à la  
“mise en valeur” du monde : botanique et colonisation (fin xviiie  siècle-début 
xxe siècle). Présentation », Revue française d’histoire d’outre-mer, nos 322-323, p. 9-38.
Bonneuil Christophe et Fressoz Jean-Baptiste, 2013, L’Événement Anthropocène. 
La Terre, l’histoire et nous, Paris, Seuil.
Botkin Daniel B., 2012, The Moon in the Nautilus Shell : Discordant Harmonies 
Reconsidered, Oxford, Oxford University Press.
Cittadino Eugene, 1990, Nature as the Laboratory : Darwinian Plant Ecology in the 
German Empire (1880-1900), Cambridge, Cambridge University Press.
Clements Frederic E., 1905, Research Methods in Ecology, Lincoln (NE), University 
Publishing Company.
Dahan Amy et Pestre Dominique (dir.), 2004, Les Sciences pour la guerre (1940-
1960), Paris, Éd. de l’EHESS.
Daily Gretchen et  al., 2009, « Ecosystem Services in Decision Making : Time to 
Deliver », Frontiers in Ecology and the Environment, vol. 7, no 1, p. 21-28.
Erickson Paul, 2010, « Knowing Nature through Markets : Trade, Populations, and 
the History of Ecology », Science as Culture, vol. 19, no 4, p. 529-551.
Finley Carmel, 2011, All the Fish in the Sea : Maximum Sustainable Yield and the 
Failure of Fisheries Management, Chicago (IL), University of Chicago Press.
Headrick Daniel R., 1988, The Tentacles of Progress : Technology Transfer in the Age 
of Imperialism (1850-1940), New York, Oxford University Press.
Hodge Joseph M., 2007, Triumph of the Expert : Agrarian Doctrines of Development 
and the Legacies of British Colonialism, Athens (OH), Ohio University Press.
Holling Crawford S., 1973, « Resilience and Stability of Ecological Systems », Annual 
Review of Ecology and Systematics, vol. 4, p. 1-23.
Holling Crawford S. et Meffe Gary K., 1996, « Command and Control and the 
Pathology of Natural Resource Management », Conservation Biology, vol. 10, no 2, 

294	
yannick mahrane
p. 328-337.
Kingsland Sharon, 2005, The Evolution of American Ecology (1890-2000), Baltimore 
(MD), Johns Hopkins University Press.
Kohler Robert E., 2002, Landscapes and Labscapes : Exploring the Lab-Field Border 
in Biology, Chicago (IL), University of Chicago Press.
Kwa Chunglin, 1987, « Representations of Nature Mediating between Ecology and 
Science Policy : The  Case of the International Biological Programme », Social 
Studies of Science, vol. 17, no 3, p. 413-442.
–	 1993a, « Modeling the Grasslands », Historical Studies in the Physical and Biological 
Sciences, vol. 24, no 1, p. 125-155.
–	 1993b, « Radiation Ecology, Systems Ecology and the Management of the Environ­
ment », in Michael Shortland (dir.), Science and Nature : Essays in the History 
of the Environmental Sciences, Oxford, British Society for the History of Science, 
p. 213-249.
–	 1994, « Modelling Technologies of Control », Science as Culture, vol.  4, no  3, 
p. 363-391.
Madureira Nuno L., 2012, « The Anxiety of Abundance : William Stanley Jevons and 
Coal Scarcity in the Nineteenth Century », Environment and History, vol. 18, no 3, 
p. 395-421.
Marsh George  P., 1864, Man and Nature, or Physical Geography as Modified by 
Human Action, New York, C. Scribner.
Masutti Christophe, 2006, « Frederic Clements, Climatology, and Conservation in 
the 1930s », Historical Studies in the Physical and Biological Sciences, vol. 37, no 1, 
p. 27-48.
National Research Council, 2005, Valuing Ecosystem Services : Toward Better 
Environmental Decision-Making, Washington (DC), National Academies Press.
Nelkin Dorothy, 1977, « Scientists and Professional Responsibility : The Experience 
of American Ecologists », Social Studies of Science, vol. 7, no 1, p. 75-95.
Nicolson Malcolm, 1989, « National Styles, Divergent Classifications : A Compara-
­tive Case Study from the History of French and American Plant Ecology », 
Knowledge and Society : Studies in the Sociology of Science Past and Present, vol. 8, 
p. 139-186.
Odum Eugene P., 1959, Fundamentals of Ecology, Philadelphie (PA), W.B. Saunders 
Company.
Perrings Charles et al. (dir.), 1995, Biodiversity Conservation : Problems and Policies, 
Dordrecht, Kluwer Academic Publishers.
Pestre Dominique, 2003, Science, argent et politique. Un essai d’interprétation, Paris, 
Quæ.
Raumolin Jussi, 1984, « L’homme et la destruction des ressources naturelles : la 
Raubwirtschaft au tournant du siècle », Annales. Économies, sociétés, civilisations, 
vol. 39, no 4, p. 798-819.
Reclus Élisée, 1864, « De l’action humaine sur la géographie physique », Revue des 
Deux Mondes, vol. 34, no 54, p. 762-771.
Robertson Morgan M., 2006, « The Nature That Capital Can See : Science, State, 
and Market in the Commodification of Ecosystem Services », Environment and 
Planning D : Society and Space, vol. 24, p. 367-387.
Starr Frederick, 1866, « American Forests : Their Destruction and Preservation », 
in US Department of Agriculture, Report of the Commissioner of Agriculture, 
Washington (DC), Government Printing Office, p. 210-234.

	
connaître et gouverner la nature	
295
Tobey Ronald C., 1981, Saving the Prairies : The Life Cycle of the Founding School 
of American Plant Ecology (1895-1955), Berkeley (CA), University of California  
Press.
Walker Jeremy et Cooper Melinda, 2011, « Genealogies of Resilience : From Systems 
Ecology to the Political Economy of Crisis Adaptation », Security Dialogue, vol. 42, 
no 2, p. 143-160.
Warming Eugenius, 1909, The Oecology of Plants : An Introduction to the Study of 
Plant Communities, Oxford, Clarendon Press.
Watt Kenneth, 1968, Ecology and Resource Management : A Quantitative Approach, 
New York, McGraw-Hill.


14 Le siècle du gène
C h r i s t o p h e  B o n n e u i l
C’est avec raison que le xxe siècle est appelé « siècle du gène ». La 
génétique n’est en effet pas seulement née avec le siècle et devenue l’une 
de ses disciplines les plus emblématiques. De Mendel à la postgénomique, 
avec des formalismes empruntés aux mathématiques et une instrumen-
tation issue de la physico-chimie, elle a aussi « génétisé » l’ensemble des 
savoirs et discours sur le vivant, irradiant les sphères agricoles, médicales, 
politiques, culturelles, et les manières de penser les relations entre les êtres 
à travers le temps. Du mendélisme et du mutationnisme aux prophéties 
démiurgiques entourant les OGM et la médecine prédictive, la génétique 
n’a cessé de formuler des promesses de vies mieux dirigées et de s’insérer 
dans des entreprises politiques (l’eugénisme, la santé, la modernisation 
agricole) et des stratégies économiques de mise au travail et de valori-
sation du vivant.
Histoires de génétique
Il n’est alors guère surprenant que la génétique soit l’un des domaines 
scientifiques dont l’histoire a été la plus racontée, notamment par ses 
praticiens tels Watson, Sturtevant, Jacob ou Monod. Les narrations succes-
sives de nombre de ces biologistes ont sédimenté un récit scandé par 
des étapes apportant chacune des avancées nouvelles ; un récit qui place 
certains personnages et processus au premier plan tout en en repoussant 
d’autres dans l’obscurité. Ces histoires sont marquées par les revendi-
cations épistémiques ou disciplinaires de leurs auteurs, par des enjeux 
institutionnels et par les façons propres à chaque époque de concevoir 
ce qui relie les êtres vivants à travers l’espace et le temps.
Une narration standard se dégage qu’on pourrait résumer ainsi : vers 
 « La sensationnelle science de l’hérédité qui peut produire une nouvelle race d’hommes ». Extrait 
de la revue Popular Science, novembre 1934.

298	
christophe bonneuil
1900, avec la « redécouverte » des lois de Mendel, s’ouvre le siècle de la 
conquête de la pensée biologique par le gène. Vers 1915 avec la théorie 
chromosomique de Morgan, la génétique, née en relation étroite avec 
la sélection animale, végétale, et l’eugénisme, entre dans une phase plus 
autonome, plus académique, scandée de « progrès » qu’Evelyn Fox Keller 
décrit comme « réguliers et cumulatifs 1 ». Les savoirs sur l’hérédité, 
fortement marqués par leur contexte social du xixe siècle, deviendraient 
alors plus purs, plus techniques et spécialisés. Müller-Wille et Rhein-
berger placent cette émancipation dès les années 1900 : pour eux, on 
passe alors des « débats vifs et vastes spéculations » sur l’hérédité en lien 
avec des « préoccupations sociopolitiques et culturelles sur les popula-
tions humaines » à une « discipline expérimentale sui generis cristallisée 
autour d’un petit nombre d’organismes modèles », et donc largement 
exonérée de tout lien – autre que de type « précondition » – avec les 
champs économique, sociopolitique et culturel 2. Ce savoir disciplinarisé 
se donne un nom en 1906 : la « génétique ». Si les savoirs de l’hérédité 
participent d’une matrice sociale et idéologique très large au xixe siècle, 
la génétique du xxe siècle appellerait donc des récits plus épistémo­
logiques et moins contextualisés. Il y aurait donc une histoire culturelle 
de l’hérédité mais guère d’histoire culturelle de la génétique.
Après cette génétique formelle où la nature biochimique du gène et 
son mode d’action ne sont pas éclaircis, émerge, à partir de la fin des 
années 1930, une « biologie moléculaire » qui conquiert les institutions à 
partir de la découverte de la double hélice de l’ADN par Watson et Crick 
en 1953. Tandis que la génétique avait émergé des projets biopolitiques 
d’optimisation et de contrôle des plantes, animaux et humains, la biologie 
moléculaire et la théorie synthétique de l’évolution à base de génétique 
des populations apparaissent, dans le récit standard, comme des disci-
plines fondamentales. Ce n’est qu’un quart de siècle plus tard, avec la 
découverte des enzymes de restriction et les techniques de l’ADN recom-
binant, c’est-à-dire la possibilité de transférer des gènes à travers la barrière 
d’espèce, que la biologie moléculaire passerait « d’une science théorique 
à une science pratique 3 ». Le premier OGM, une bactérie exprimant un 
gène de batracien, date de 1973 et ouvre de nouveaux horizons d’« appli-
cations » de la biologie moléculaire, mais aussi un temps de controverses 
publiques. Si cette tendance industrialiste et réductionniste se retrouve 
aujourd’hui dans la biologie de synthèse, on assisterait toutefois, avec la 
1. Keller 2003 (p. 7).
2. Müller-Wille et Rheinberger 2012 (p. 128 – et p. 136 sur la notion de « précondition »).
3. Morange 1994 (p. 213).

	
le siècle du gène	
299
génomique (depuis le lancement du projet « Génome humain » en 1990), 
la biologie des systèmes et l’épigénétique, à un dépassement des dogmes 
de la biologie moléculaire : remise en question du déterminisme génétique 
avec une nouvelle vision de l’ADN non plus comme un programme  
mais comme une mémoire sélectivement mobilisée par la dynamique 
cellulaire sous l’effet de l’environnement ; découverte du continent des 
petits ARN codés par les 98 % de l’ADN non codant pour des protéines 
(initialement qualifié d’« ADN poubelle ») ; éclatement du concept de 
gène 1 et retour au fondamental et à la complexité 2 – voire, pour certains, 
réhabilitation du lamarckisme 3.
À ces généticiens et biologistes moléculaires contant la généalogie 
intellectuelle d’un champ qu’ils participent ainsi à délimiter, institution-
naliser et renouveler, se sont adjoints, à partir de la fin des années 1960, 
des sociologues et historiens, tels Robert Olby puis Robert Kohler. Au 
récit standard fait de progrès, de percées et d’arrachement à un contexte 
pour dégager des savoirs de plus en plus exacts, ils ajoutent d’autres récits 
en termes d’émergence de communautés et de dynamiques institution-
nelles, sociopolitiques et culturelles.
Les enquêtes historiennes menées depuis ont ainsi montré que, loin 
d’être un moine isolé, Mendel était parfaitement au fait des enjeux scien-
tifiques de son temps et s’inscrivait dans un collectif de sélectionneurs et 
rationalisateurs de l’agriculture et de l’industrie de la Moravie, alors une 
des régions les plus dynamiques d’Europe 4. Réalisant qu’Hugo De Vries, 
l’un des « redécouvreurs » des lois de Mendel, ou Wilhelm Johannsen, 
l’inventeur des mots « gène » et « génotype », ne furent pas vraiment investis 
dans des programmes expérimentaux d’hybridation, les historiens ont 
établi le rôle – aussi central que les croisements mendéliens – des pratiques 
de purification et standardisation des organismes (comme du concept de 
lignée pure) dans l’émergence de la génétique dite « classique 5 ». Apparaît 
alors l’importance, comme terreau des nouvelles façons de connaître et 
manipuler l’hérédité au début du xxe siècle, d’entrepreneurs enthou-
siastes de la manipulation industrielle du vivant, de la rationalisation de 
l’agriculture et de l’amélioration des populations humaines 6. L’histoire 
de la génétique dite « classique » peut alors être fructueusement inscrite 
dans celle, plus large, des projets de rationalisation du vivant, recouvrant 
1. Keller 2003.
2. Barnes et Dupré 2008.
3. Jablonka et Lamb 1995.
4. Wood et Orel 2005.
5. Bonneuil 2015a.
6. Paul et Kimmelman 1988, Paul 1995.

300	
christophe bonneuil
à la fois des similarités transnationales et des singularités nationales. 
L’exploration des similarités permet d’envisager la génétique comme fille 
de la seconde révolution industrielle et de l’enrôlement du vivant dans la 
production de masse, et comme élément de la « révolution du contrôle » 
et des grandes organisations – celle qui inaugure de nouvelles formes de 
gestion de l’information 1 et de biopolitique au siècle des États moder-
nisateurs 2. L’analyse comparative des contextes nationaux fait ressortir 
la grande diversité des programmes expérimentaux de connaissance et 
d’optimisation génétique, et leur production conjointe avec des projets 
biopolitiques contrastés, tant en direction des humains que des non- 
humains 3. L’étude de l’amélioration génétique du porc menée par Tiago 
Saraiva illustre ainsi le contraste entre le projet autarcique d’un régime 
nazi privé de colonies, visant un porc bien gras et patriotique, valorisant 
les produits du sol allemand (betterave, pomme de terre…), et celui des 
sélectionneurs américains en quête d’un porc idéal plus maigre et plus 
« mondialisé » du fait de la concurrence des matières grasses végétales 
tropicales dans l’alimentation animale et humaine 4. L’analyse des projets de 
recherche et de rationalisation génétique des humains, microbes, plantes 
et animaux offre ainsi une fenêtre privilégiée pour comprendre la fabrique 
des différents régimes politiques du xxe siècle et décloisonner l’histoire 
des sciences en l’intégrant aux grands objets de l’histoire du siècle.
Quant à l’idée du caractère fondamental et universel de la biologie 
moléculaire promue par les narrations historiques de ses praticiens, elle 
n’a pas non plus résisté à la multiplication des investigations mobilisant 
les archives. La mise en avant de la biologie moléculaire et de la théorie 
synthétique de l’évolution comme sciences neutres en surplomb du 
politique et de leur époque est justement le fruit du contexte idéologique 
et culturel d’après 1945 : il s’agit alors pour les généticiens de se distancier 
de la biologie raciale aux impacts moralement répugnants de l’Allemagne 
nazie, d’exorciser un passé eugéniste et de rejeter le lyssenkisme. Tandis 
que l’Unesco affirme fermement l’absence de fondement scientifique 
aux théories raciales 5, Luis Campos note aussi une mise en veilleuse du 
discours baconien et démiurgique des généticiens qui avait été, dans 
la première moitié du siècle, prolixe en promesses de refaire des êtres 
vivants plus performants 6. Après les projets de créer de nouveaux êtres 
1. Thurtle 2007.
2. Cf. Edgerton (dans ce volume, p. 67).
3. Flitner 2003.
4. Saraiva 2016.
5. Cf. Lipphardt (dans ce volume, p. 253).
6. Campos 2009, Endersby 2013.

	
le siècle du gène	
301
par purification, croisement planifié ou mutagenèse (projet formulé par 
De Vries dès 1901), la biologie moléculaire a en effet émergé d’un projet 
de contrôle technologique de la vie porté par la Fondation Rockefeller,  
qui apporte 90 millions de dollars entre 1932 et 1959, finançant presque 
tous les futurs prix Nobel du domaine 1. Son directeur pour les sciences 
de la nature, le physicien Warren Weaver qui forge en 1938 l’expression 
de « biologie moléculaire », explique la Grande Dépression de 1929 par 
un sous-développement des savoirs sur l’homme, sa biologie et sa psycho-
logie, au regard des savoirs accumulés sur la matière. Il écrit :
Est-ce que l’homme peut acquérir un contrôle rationnel de son propre 
pouvoir ? Pouvons-nous développer une génétique si sûre et si complète que 
nous puissions espérer produire dans le futur des hommes supérieurs ? […] 
Pouvons-nous libérer la psychologie de sa confusion et de son inefficacité 
présentes, et la transformer en outil que chaque homme pourrait utiliser dans 
sa vie de tous les jours […] pour rendre sa conduite raisonnable 2 ?
Les enquêtes historiennes établissent aussi que, loin de dériver en 
droite ligne de la génétique du début du siècle, la biologie moléculaire 
s’est constituée dans un paysage technologique riche de collaborations 
entre science et industrie en vue de comprendre et agir sur les processus 
biologiques au niveau moléculaire 3. Aux États-Unis, la recherche sur les 
vitamines est tirée par l’USDA et l’industrie agroalimentaire, tandis que 
General Electrics ou Radio Corporation of America s’intéressent à l’amé-
lioration des plantes. En Europe du Nord, le laboratoire de recherche 
industrielle de Carlsberg est un haut lieu de la biochimie, tandis que 
Theodor Svedberg à Uppsala tisse des liens industriels étroits pour 
développer l’ultracentrifugation et l’analyse biochimique du fonction-
nement cellulaire. En Allemagne, le Kaiser-Wilhelm Institut für Biochimie 
collabore avec l’industrie pharmaceutique pour l’étude et la commercia-
lisation des hormones sexuelles 4, tandis que d’autres instituts s’attellent 
à la biochimie et à la synthèse de matières premières faute d’empire 
colonial. Un intense trafic entre recherche et contexte d’application se 
noue également dans des hôpitaux comme l’Institut Rockefeller à New 
York où Avery montre que les gènes sont constitués d’ADN en 1944, ainsi 
qu’à l’Institut Pasteur où travaillent François Jacob et Jacques Monod. 
C’est dans ce paysage scientifico-industriel qu’une nouvelle armada 
1. Müller-Wille et Rheinberger 2012 (p. 168).
2. Cité par Morange 1994 (p. 107). Voir Kay 1993.
3. Müller-Wille et Rheinberger 2012 (p. 162-168).
4. Gaudillière (dans ce volume, p. 85).

302	
christophe bonneuil
technologique (ultracentrifugation, électrophorèse, rayons X, microscopie 
électronique puis radio-isotopes) est déployée à partir des années 1920 
pour étudier (et transformer en produits pharmaceutiques ou alimen-
taires) les vitamines, les hormones, les virus, les anticorps, les enzymes, 
puis l’ADN. Cette exploration des pratiques matérielles et des dispo-
sitifs conduit à envisager la dynamique de recherche en génétique et 
biologie moléculaire en termes de « systèmes expérimentaux » contenant 
(au double sens d’un portage et d’un cadrage) les objets épistémiques. Du 
« réacteur drosophile » étudié par Robert Kohler aux systèmes in vitro de 
la biologie moléculaire analysés par Hans-Jörg Rheinberger, ces systèmes 
expérimentaux déploient leur productivité en cadrant les voies d’investi-
gation tout en constituant la toile sur laquelle de nouveaux phénomènes 
apparaissent, d’abord comme anomalies, puis comme objets et instru-
ments de recherche 1. Mouches, vers, souris, arabettes et autres organismes 
modèles de la génétique sont alors plus que des objets de savoir – des 
réactifs, des formes incorporées du savoir génétique lui-même.
Par des études de cas et des comparaisons transnationales, les travaux 
des années 1990 et 2000 montrent aussi que, loin d’être une simple 
« réception » nationale d’un programme disciplinaire préexistant, le 
développement de la biologie moléculaire implique, dans chaque pays, de 
nouveaux agencements et de nouvelles identités. Ces travaux diffractent 
ainsi la biologie moléculaire en plusieurs histoires nationales connectées 
mais singulières, insérant les nouvelles façons de connaître la vie dans les 
histoires nationales, politiques et sociales de l’après-guerre 2.
Enfin, les travaux des historiens sur la biologie de synthèse ont permis 
d’en tracer les racines dans les biotechnologies des années 1970, mais 
aussi dans des projets plus anciens de synthèse du vivant (l’expression 
« biologie de synthèse » date de 1912 3). Si l’ère des biotechnologies et des 
bio-industries est aujourd’hui mise au second plan par certaines narra-
tions historiques « repurifiantes 4 », elle fait l’objet de nombreux travaux de 
sociologues, historiens, anthropologues et politistes 5. Quant au tournant 
postgénomique et épigénétique, il est propice à déplacer le regard et 
pourrait bientôt conduire à relire les débats sur l’hérédité acquise et les 
actions héréditaires de l’environnement et de la nutrition, fort vifs dans 
la première moitié du siècle, avec un éclairage différent de ceux donnés 
quand la biologie moléculaire triomphait. La dramatisation de la crise 
1. Kohler 1994, Rheinberger 1997.
2. Gaudillière 2002, Strasser 2002.
3. Campos 2009 ; voir aussi Bud 1993.
4. Telles Barnes et Dupré 2008.
5. Un ouvrage récent est Rasmussen 2014.

	
le siècle du gène	
303
actuelle du gène ne doit cependant pas masquer que le génocentrisme 
reste puissamment opératoire dans le contexte aussi bien expérimental, 
industriel que légal (brevet) et réglementaire.
Le récit qui suit s’attache à proposer une histoire culturelle de la façon 
dont la génétique a constitué le vivant et les gènes en objets de savoir, de 
discours et d’intervention.
De l’hérédité à la génétique
Dans un monde linnéen, la transmutation des espèces était non 
seulement une hérésie, mais aussi une impossibilité logique car, sans 
immutabilité de l’espèce, explique Kant, il ne resterait qu’un monde 
instable où il serait impossible de classer et de penser. En allongeant le 
temps de la vie et de la Terre, les sciences du xixe siècle font tomber ces 
frontières : pour Darwin, la frontière d’espèce n’est qu’une frontière de 
variété qui se serait creusée et la vie est un flot continu de variation, un 
« réseau inextricable d’affinités 1 ». La biologie construit un regard histo-
rique et processuel sur le vivant 2, et donc sur l’hérédité pensée comme 
une force, une mémoire empreinte de l’influence du milieu et du poids 
du temps. Les hommes descendent du singe et pourraient y revenir sans 
l’action d’une sélection continue, et les enfants de génies retournent à la 
médiocrité, illustrant à quel point l’excellence n’est jamais acquise. Ainsi, 
dans la théorie galtonienne de l’hérédité, si les enfants de génies régressent 
vers la médiocrité, c’est parce que les gemmules porteuses de l’hérédité 
se sont tant exprimées dans l’organisme parent qu’elles sont fatiguées et 
moins capables de concurrencer les éléments restés latents lorsqu’il s’est agi  
de se transmettre à la descendance… D’où la « difficulté constatée par les 
sélectionneurs à maintenir un trait d’élite d’une variété s’il est apparu par 
accident » et l’idée que « les races existantes ne sont maintenues à leur 
niveau que par l’action sévère de la sélection 3 ».
Tout comme la fragilité de l’ordre après la découverte de l’entropie, 
cette hérédité « molle » des biologistes, biométriciens et sélectionneurs 
de la fin du xixe siècle fait écho à une angoisse face à la montée des 
classes populaires et de la société de masse. Elle témoigne aussi d’une 
conception dans laquelle l’hérédité ne se durcit qu’avec l’action du temps 
et du milieu : hérédité, culture et environnement participent du même 
1. Darwin 1859 (p. 434).
2. Thurtle 2007.
3. Galton 1876 (p. 339-340).

304	
christophe bonneuil
processus gouvernant la continuité et la variation des êtres à travers les 
générations.
C’est en rupture franche avec ce monde de l’hérédité historique, proces-
suelle, molle et poreuse à l’environnement que se pose la génétique du 
début du xxe siècle. Elle y oppose une vision structurale, combinatoire, 
dure, autiste vis-à-vis de l’environnement, et a-historique de l’hérédité.
Premièrement, en effet, la nouvelle génétique se veut a-historique. La 
structure 1 y prime sur le temps et le transcende. Haro sur la variation 
continue et trop lente de l’évolution et vive la variation brusque obtenue 
sur commande (cf. les mutations et saltations de De Vries et la quête 
d’une évolution expérimentalement dirigée 2) ! Dans un célèbre article 
où il expose la « conception moderne de l’hérédité », Wilhelm Johannsen 
conçoit le génotype comme une « structure physico-chimique ». Alors 
que Nägeli, Weismann et tant d’autres biologistes du siècle précédent 
insistaient sur la dimension processuelle du développement, de l’hérédité 
et du germplasme lui-même, celui-ci, selon Johannsen, « réagit exclusi-
vement en fonction de son état réalisé, non de l’histoire de sa création » ; 
la nouvelle génétique est
une vision a-historique des réactions des êtres vivants […] analogue à celle 
de la chimie […] ; les substances chimiques n’ont pas de passé compro-
mettant, H2O reste toujours H2O, et réagit toujours de la même manière, 
quoi qu’il en soit de l’histoire de sa formation ou de l’état antérieur de ses  
éléments 3.
Dans cette conception, l’hérédité ne s’accumule plus avec le temps ; elle 
est question de combinaison : « Cette force mystérieuse [de l’atavisme] 
n’existe pas », l’atavisme est en réalité une situation de « recombinaison 
identique de facteurs héréditaires [récessifs] 4 », déclare le sélectionneur 
Philippe de Vilmorin ; « une race pure pour un caractère n’est pas, comme 
l’on croyait autrefois, celle qui possède une longue lignée d’ancêtres ayant 
ce caractère ; c’est tout simplement une race dans laquelle le caractère est 
produit par l’union de deux gamètes de même sorte 5 ».
Deuxièmement, cette sortie de l’histoire est aussi une sortie du lien 
d’engendrement et de transmission. Si l’on rapporte cette mutation des 
conceptions de l’hérédité au cadre analytique des différentes « cités de 
1. Gayon 2000.
2. Endersby 2013.
3. Johannsen 1911 (p. 139).
4. Vilmorin et Meunissier 1913, cités par Bonneuil 2015a.
5. Meunissier 1910 (p. 13), cité par Bonneuil 2015a.

	
le siècle du gène	
305
justice » proposé par Luc Boltanski et Laurent Thévenot 1, on peut noter 
que la vision de l’hérédité de l’âge de Darwin relève plutôt de la cité 
domestique tandis que celle de la nouvelle génétique participe de la cité 
industrielle. Suivant la théorie de la séparation des plasmas germinatif et 
somatique 2, on ne considère plus que le parent transmet un trait ou son 
déterminant à sa progéniture (transmission qui dans la cité domestique 
implique une dette personnelle, une inscription dans l’histoire) mais que 
seules les lignées germinales sont en relation de continuité, donnant à 
chaque génération un organisme, comme une tige surgissant d’un rhizome. 
Dès lors, « l’hérédité peut se définir comme la présence ou l’absence de 
gènes identiques chez les ascendants et les descendants 3 ». Ainsi définie, 
l’hérédité est séparée des autres dimensions (phénotypique, symbolique, 
dynamique, historique, culturelle) de la chaîne génésique liant les êtres à 
travers les générations. Cette mutation recoupe aussi les analyses cultu-
relles de Jean Baudrillard sur l’industrialisation des univers de production 
et de consommation. Dans l’univers préindustriel, l’origine d’un produit, 
son authenticité était une propriété qui contribuait à lui donner son sens 
et à signifier le statut social de son détenteur. Mais
c’est une nouvelle génération de signes et d’objets qui se lève avec la révolution 
industrielle. Des signes sans tradition de caste, […] d’emblée produits sur 
une échelle gigantesque. Le problème de leur singularité et de leur origine 
ne se pose plus : la technologie est leur origine, ils n’ont de sens que dans la 
dimension du simulacre industriel. C’est-à-dire la série. C’est-à-dire la possi-
bilité même de deux ou de n objets identiques. La relation entre eux n’est 
plus celle d’un original à sa contrefaçon, ni analogie ni reflet, mais l’équiva-
lence, l’indifférence 4.
Suivant Baudrillard, on peut dire que l’hérédité entre dans l’ère de la 
série industrielle, lorsque la nouvelle génétique substitue à l’histoire 
« verticale » – faite d’engendrements, d’empreintes et de compétitions – une 
logique « horizontale » de recombinaisons et d’assemblages identiques de gènes.
L’hérédité aristocratique, où l’excellence est précieuse parce que rare et 
nécessitant des soins permanents (étiquette, choix matrimoniaux, etc.), 
cède la place à une hérédité industrielle. Inscrite dans le marbre du 
« génotype », l’excellence doit à présent pouvoir être produite en masse 
et de façon assez stable pour voyager dans de vastes marchés, à la façon 
1. Boltanski et Thévenot 1991.
2. Weismann 1892.
3. Johannsen 1911 (p. 159).
4. Baudrillard 1975 (p. 85).

306	
christophe bonneuil
de la première bière industriellement produite en 1883 par Carlsberg, 
fermentée par un clonale de levure issu d’une unique cellule. À la façon 
des vaccins eux aussi produits à grande échelle et dont l’efficace et l’inno-
cuité doivent être constantes. À la façon aussi des lignées de drosophiles 
cultivées par centaines au laboratoire de Morgan ou des variétés de blé 
aux qualités standards produites par les grandes stations de sélection 
et requises par les Grands Moulins, puisque « l’idéal pour l’industrie 
est d’opérer sur des produits dont la nature est bien définie et toujours 
identique 1 ».
Troisièmement, c’est dans ce mouvement très large de standardisation 
industrielle que la pureté génétique des êtres devient une quête et une 
norme centrale de tout savoir légitime sur l’hérédité. Pour Johannsen, 
« l’étude du comportement des lignées pures est le fondement de la 
science de l’hérédité, bien que la plupart des populations – notamment 
la société humaine – ne consiste nullement en lignées pures 2 ». C’est cette 
quête de pureté qui permet de révéler l’existence de types discrets, de 
lignées aux caractéristiques stables, là où la biologie et les sélectionneurs  
du xixe siècle voyaient un continuum agi par l’environnement. Peu après 
1900, de nouveaux concepts et vocables apparaissent : ceux, proposés par 
Johannsen, de « lignée pure » ou de « biotype » ; ceux de « clone », avancé 
dès 1903 par le sélectionneur américain Webber, ou de « variété clonale », 
imaginé en 1912 par Shull, père du maïs hybride. Tous cherchent « un 
terme qui inclurait les séries de formes génotypiquement identiques 3 » 
quel que soit leur mode d’engendrement (reproduction végétative, parthé-
nogenèse, division cellulaire, autogamie, hybrides F1).
Cette prolifération de concepts et de pratiques visant l’homogénéité 
génétique ne saurait se réduire à la rigueur toujours nécessaire pour faire 
science ; elle traduit l’entrée du vivant (vaccins, fermentations, agroalimen-
taire) dans l’univers industriel : de nouvelles formes de vie, génétiquement 
stables et prévisibles, sont produites car elles se prêtent mieux à la  
rationalisation des processus de production, à l’allongement des filières et 
aux enjeux de propriété industrielle. Pour illustrer cette matrice commune 
de la rationalisation industrielle des bio-industries et de la naissance de la 
génétique, notons que Wilhelm Johannsen est assistant au laboratoire de 
la brasserie Carlsberg entre 1881 et 1887, précisément lorsque s’y met au 
point ce processus industriel de fabrication de bière issue de lignée pure 
de levure. Rien de surprenant à ce que Johannsen ait ensuite appliqué aux 
1. Blaringhem 1905 (p. 362), cité par Bonneuil 2015a.
2. Johannsen 1903 (p. 9).
3. Jennings 1911 (p. 842), cité par Bonneuil 2015a.

	
le siècle du gène	
307
végétaux de telles méthodes de purification et soit devenu le promoteur 
de la notion de lignée pure et de la quête, sous la fluctuation continue des 
apparences (phénotypes), de la variation discrète de structures profondes 
homogènes et stables (génotypes).
Quatrièmement, la pénétration de nouvelles pratiques issues des 
grandes organisations industrielles, techniques ou bureaucratiques ne 
se limite pas à la standardisation et à la pureté. Concomitante de l’essor 
d’une biologie expérimentale s’opère une industrialisation des pratiques 
de connaissance en biologie, dans les hôpitaux, les enquêtes statistiques 
ou anthropobiologiques, les stations agronomiques et les laboratoires.  
Le travail scientifique porte en effet sur de grandes quantités d’entités  
avec un usage croissant des statistiques. Sélectionneurs, médecins et de 
biologistes vantent désormais les mérites de l’accumulation de données en 
masse. Le directeur du laboratoire d’amélioration des plantes de l’USDA 
entonne le credo du systematic plant breeding dans les années 1890 et 
de la « nécessité de travailler avec de grands nombres » afin d’accroître la 
« probabilité de sélectionner et obtenir le résultat désiré 1 ». Son collègue 
Willet M. Hays, futur sous-secrétaire à l’Agriculture, met au point plusieurs 
outils et méthodes, telle la centgener method pour industrialiser la sélection 
du blé, et estime que « le travail d’amélioration d’une espèce peut être 
entrepris d’une façon aussi systématique et effective que la manufacture 
de machines à coudre 2 ». Il s’agit dans les stations agronomiques, pour 
rendre comparables les caractéristiques de milliers de lignées suivies 
parallèlement, de les traiter chacune de façon rigoureusement identique 
(homogénéisation parfaite de la distance et profondeur de semis, du sol 
par l’apport d’engrais, du développement par organisation « industrielle » 
du désherbage en ligne, etc.). Soumettre des milliers d’entités à une obser-
vation systématique implique aussi un personnel nombreux, une division 
du travail et des formes dépersonnalisées d’organisation de l’information 
propres à la « révolution du contrôle » et au system management qui  
se déploient au tournant du siècle dans les entreprises ferroviaires et de 
vente par correspondance, l’armée, les administrations et hôpitaux, les 
bibliothèques. Systèmes d’écriture simplifiés recourant aux symboles, 
répertoires munis d’index et de références croisées, dactylographie sur 
feuilles volantes et multiples, fiches cartonnées déplaçables dans un 
système de fichiers verticaux 3. Hugo De Vries souligne ce lien entre la 
nouvelle science de l’hérédité et le travail « à grande échelle », et vante 
1. Webber 1895 (p. 54), cité par Bonneuil 2015a.
2. Hays 1905 (p. 177), cité par Bonneuil 2015a.
3. Yates 1989.

308	
christophe bonneuil
« l’ampleur quasi incroyable du book-keeping (tenue de registre, inscrip-
tions) » mis en œuvre à la station de sélection de Svalöf en Suède avec 
laquelle lui comme Johannsen sont en liens étroits 1.
Un laboratoire universitaire comme celui de Morgan à l’université 
Columbia n’échappe pas à cette rationalisation industrielle. Peut-être 
inspiré par la réorganisation de la bibliothèque de cette université par 
Dewey, le groupe de Morgan utilise des fiches cartonnées amovibles pour 
noter les propriétés des souches ou les résultats des croisements. Surtout, 
il se transforme entre 1905 et 1918 – à la même époque que les ateliers  
de production de Ford à Rouge River –, passant d’un laboratoire général 
de biologie où chacun travaille sur plusieurs espèces différentes à une 
plateforme d’investigation intensive et systématique d’un seul organisme  
qui génère à vive allure – une nouvelle génération tous les douze jours – des 
mutations aisément détectables quand on travaille à grande échelle.  
La température et l’humidité sont parfaitement contrôlées, et les lignées 
font l’objet d’une ingénierie pour standardiser leur taux de crossing-over. 
Le « réacteur drosophile » est un « système de production » de nombreux 
mutants et de nouvelles questions qui conduisent Morgan à changer de 
système de classification des mutations et à passer d’une perspective 
d’évolution expérimentale à une perspective de cartographie postmen-
délienne 2. Les investigateurs – Morgan, Bridges, Sturtevant, Müller 
puis d’autres – se divisent le travail, chacun produisant des centaines 
de briques d’information, assemblées dans les cartes chromosomiques. 
Celles publiées entre 1919 et 1923 sont issues de la manipulation de  
13 à 20 millions de mouches.
Le nombre d’espèces étudiées annuellement dans les articles de la revue 
American Naturalist chute d’environ 400 vers 1870 à une centaine vers 
1940. Cette réduction du nombre d’organismes pris pour objets par les 
biologistes au profit de l’étude intensifiée d’un petit nombre d’espèces 
modèles 3 prépare une nouvelle biomédecine 4 et une nouvelle biopoli-
tique végétale et animale 5 dans les espaces agricoles. La quête de formes 
homogènes de vie s’inscrit dans le système expérimental de la station 
agronomique et sa culture épistémique, une « agronomie des preuves » 
où le design expérimental est associé à une analyse statistique poussée. 
C’est dans ces théâtres de la preuve agronomique où l’on expérimente 
1. De Vries 1907 (p. 48 et 79).
2. Kohler 1994 (p. 49).
3. Bruno Strasser, communication personnelle.
4. Rader 2004, Gaudillière 2002.
5. Concernant la biopolitique humaine, voir dans ce volume les contributions de Veronika 
Lipphardt (p. 253) et de Sarah Franklin (p. 211).

	
le siècle du gène	
309
en veillant à varier un seul paramètre à la fois que l’homogénéisation 
génétique prend tout son sens. Promouvoir un petit nombre de variétés 
homogènes, c’est aussi rendre plus lisible et gouvernable le monde rural 
pour l’État et ses experts. La norme de variétés « distinctes, homogènes et 
stables » va donc s’imposer dans les modernisations agricoles du xxe siècle, 
de la bataille du blé de l’Italie fasciste à la « révolution verte » au Sud, en 
passant par le maïs hybride F1 états-unien ou la loi nazie de 1934 sur  
les semences. Cette dernière, en autorisant l’administration à mettre hors 
la loi des variétés jugées improductives ou sensibles aux maladies, sera 
imitée dans plusieurs pays (en France dès 1942) et préfigurera l’actuel 
catalogue européen des semences 1.
Du pangène au gène
Dans le dernier tiers du xixe siècle domine l’idée d’une hérédité portée 
par des particules, présentes dans les gamètes et assemblées dans l’œuf pour 
former un individu adulte, déterminant des ressemblances non seulement 
avec tous les individus de son espèce, mais aussi avec ses ascendants 
directs. Ces particules sont les « gemmules » de la théorie pangénétique 
de Darwin (1869) et de la « théorie de l’hérédité » de Galton en 1876, les 
« biophores » de Weismann, les Anlagen de plusieurs auteurs après Mendel, 
les « idioplasmes » de Nägeli, les « pangènes » de De Vries, etc., avant 
que ne s’impose le vocable de « gènes » proposé par Johannsen en 1909.
Mais qu’y a-t-il de commun entre les gemmules de la théorie pangé-
nétique de Darwin et Galton et les gènes couronnés par la génétique 
« moderne » du début du xxe siècle ? Alors que les gemmules étaient 
de véritables organoïdes capables de migrer dans tout l’organisme, 
d’apprendre ou de se fatiguer, les gènes sont enfouis dans le noyau de  
la lignée germinale – et n’apprennent plus rien de l’environnement ou de 
l’expérience de l’organisme. Alors que l’espèce prenait place dans le grand 
livre historique de l’évolution et que la gemmule menait une vie aventu-
reuse et riche dans le maelström de la vie, le gène apparaît comme une 
brique inerte insécable et recombinable, dont on cherche les fréquences 
et les combinaisons comme bases sous-jacentes de la diversité, l’évolution, 
l’amélioration et la stabilisation des vivants. Leur mode d’existence est 
semblable à celui d’un Lego standardisé dans une infrastructure indus-
trielle modulable ou d’une carte mobile d’un système de fichiers verticaux.
On peut schématiser quatre aspects de cette nouvelle éthologie des 
1. Saraiva 2016, Bonneuil et Thomas 2009.

310	
christophe bonneuil
particules héréditaires. Premièrement, un nombre croissant de biologistes 
et sélectionneurs rompent après les années 1880 avec une conception 
physiologique d’un équilibre entre les traits d’un organisme pour concevoir 
l’organisme comme un ensemble de caractères que l’on peut sélectionner 
séparément en vue d’associer deux traits auparavant vus comme antago-
nistes. « L’organisme est une collection de traits. Nous pouvons retirer le 
caractère jaune et brancher [plug in] le vert, retirer la hauteur et brancher le 
nanisme », claironnait ainsi William Bateson 1. Avant même la redécouverte 
des lois de Mendel s’affirme l’idée que chaque type de particule héréditaire 
détermine un trait bien discret et peut se transmettre indépendamment 2.
Deuxièmement, le gène n’est plus un vagabond : en tant qu’unité de 
transmission, loin de circuler librement dans l’organisme, il est cantonné 
depuis la théorie de la « pangenèse intracellulaire » de De Vries en 1889 au 
noyau des cellules, et depuis la théorie de Weismann en 1892 à la lignée 
germinale. Tous deux invoquent l’argument de la division du travail pour 
justifier leur hypothèse. Weismann compare les cellules somatiques à  
des unités de l’armée n’ayant pas besoin de l’ensemble du bagage génétique 
pour accomplir leur fonction précise 3. De même De Vries déclare-t-il 
que « la transmission est une fonction du noyau, le développement la 
mission du cytoplasme 4 ». Cette séparation de la vie en un domaine du 
stockage et un domaine de l’actualisation contribue à la coupure, analysée 
plus haut, de l’hérédité et de l’histoire comme de l’environnement. En 
1973 encore le biologiste moléculaire Salvador Luria séparera « la vie en 
action », relevant de la biologie moléculaire, de « la vie dans l’histoire », 
relevant de la génétique des populations et de la biologie évolutive 5.
Troisièmement, on assiste à une dévitalisation des particules hérédi-
taires. Alors qu’avant 1892 les pangènes ou biophores de De Vries et de 
Weismann pouvaient se nourrir, croître et se multiplier. Mais en 1894, 
De Vries adopte la métaphore des boules de Quételet, les reprend en 1896 
pour rendre compte du ratio (dit « mendélien ») obtenu en deuxième 
génération de ses croisements. L’hérédité devient ainsi plus une question 
de tirage et recombinaison de boules inertes que de croissance, circu-
lation et compétition entre corpuscules dynamiques. Finalement, en 
conjonction avec le courant mendélien, De Vries abandonne en 1903 sa 
conception antérieure de pangènes pouvant changer d’état, de nombre 
ou de nature pour en faire des unités figées et indépendantes dans une 
1. Bateson 1902, cité par Allen 2003 (p. 67).
2. Voir notamment De Vries 1889.
3. Weismann 1892 (p. 40-41).
4. De Vries 1889 (p. 194).
5. Luria 1973.

	
le siècle du gène	
311
structure génétique modulaire 1. Limités à deux exemplaires seulement 
par cellule somatique, deux « allèles 2 », les gènes de la génétique classique 
voient donc bientôt leur existence réduite à trois paramètres seulement : 
présence / absence, dominant / récessif, puis position sur le chromosome.
Ce concept de gène-Lego donne naissance dans les années 1920 à la 
notion de « ressources génétiques », mise en avant par Nicolaï Vavilov 
et ses collègues en Union soviétique. Vavilov, formé par Bateson, entre-
prend, dans le cadre du projet impérial soviétique, de collecter les races et 
variétés du monde entier. En 1940, la collection de l’institut de Vavilov à  
Leningrad ne comprend pas moins de 250 000 accessions. Vavilov entend 
par ses collectes « trouver les éléments de base, les “briques et le ciment” 
[les gènes] […] pour posséder le matériau initial de l’amélioration des plantes 
et des animaux […] pour la construction de la machinerie moderne 3 ». 
La vision de la biodiversité comme stock de gènes inertes et indépen-
dants que l’on peut conserver ex situ dans des « banques de gènes » est 
héritière de cette entreprise. On la retrouve au cœur de la « révolution 
verte », puis de la Convention sur la diversité biologique adoptée à Rio 
en 1992. Celle-ci se donne en effet pour objectif (art. 1er) « la conser-
vation de la diversité biologique, l’utilisation durable de ses éléments et 
le partage juste et équitable des avantages découlant de l’exploitation 
des ressources génétiques, notamment grâce à un accès satisfaisant aux 
ressources génétiques ».
Le gène-programme, planificateur en chef de la cellule-usine
Les nouveaux instruments et méthodes de la biochimie et biophysique 
de l’entre-deux-guerres, à commencer par l’ultracentrifugation et l’électro-
phorèse, tendent à discrétiser le cytoplasme, le faisant entrer lui aussi dans 
de nouveaux imaginaires modulaires et de nouveaux modes opératoires 
industriels. Les « colloïdes » cèdent le premier rôle cellulaire aux enzymes 
et autres protéines, comme les gènes avaient atomisé le germoplasme 4. 
De l’hypothèse « un gène-une enzyme » de Beadle et Tatum en 1941 à 
l’élucidation du « code génétique » dans les années 1960 en passant par 
la mise en évidence de la structure en double hélice de l’ADN, le gène 
acquiert une nature tangible de molécule chimique informationnelle 5. De 
1. Stamhuis et al. 1999 (p. 247-259).
2. Dans le cas le plus général, des organismes diploïdes.
3. Vavilov 1931, Flitner 2003.
4. Müller-Wille et Rheinberger 2012 (p. 166).
5. Morange 1994, Müller-Wille et Rheinberger 2012 (p. 161-186).

312	
christophe bonneuil
structure, l’hérédité se fait aussi « message » en synergie – faite aussi de 
malentendus – avec l’essor de la théorie de l’information et de la cyber-
nétique 1. Les protéines ainsi « codées », avec la notion de spécificité selon 
une analogie préhensive (clé-serrure), œuvrent « exactement comme des 
machines-outils », « disposées dans une chaîne de production pour une 
vitesse optimale et un rendement efficient 2. » Ni le hasard, ni l’histoire, 
ni la compétition n’ont droit de cité dans l’espace productif cellulaire 
ainsi envisagé. En bonne logique fordiste, le « dogme central » (Crick) 
de la biologie moléculaire considère que l’information ne circule que 
du sommet à la base, de l’ADN aux protéines, selon le modèle de l’ins-
truction. Jacob et Monod interprètent leurs travaux sur l’opéron de façon 
à conclure non pas à une régulation des gènes par l’environnement mais 
à un « mécanisme génétique de régulation » en quelque sorte déjà prévu 
dans le hardware du génome 3. Chaque gène code pour la production en 
masse d’une seule protéine agissant avec le « fonctionnement rigide d’une 
machine métallique avec rouages, engrenages et pistons 4 ». Et quelques 
« gènes régulateurs » gouvernent le développement de l’organisme avec 
la précision d’horloger d’un servosystème cybernétique conduisant  
un missile vers sa cible : « Le génome contient non seulement une série 
de plans, mais aussi un programme coordonné de synthèse des protéines, 
ainsi que les moyens de contrôler son exécution 5. » Dans la France planiste 
plus encore qu’ailleurs, la cellule-usine de l’âge fordiste semble s’activer 
sous le contrôle des gènes constituant le programme et l’ingénieur en chef. 
Mais c’est l’évolutionniste américain Ernst Mayr qui introduit la notion 
de « programme génétique » en 1961 et affirme que le « code de l’ADN 
[…] est le programme de l’ordinateur comportemental de cet individu 6 ».
Forte de nouveaux outils de « copier-coller » moléculaire, le génie 
génétique hérite de la notion mendélienne de « recombinaison » de gènes 
dont l’assemblage est à optimiser en greffant des gènes un à un – d’où 
le terme d’« ADN recombinant », mais aussi de l’imaginaire du gène- 
programme. Il s’agit en effet d’« asservir n’importe quel être vivant à l’exé-
cution d’une partie du programme génétique d’un autre être vivant 7 ». 
Avec la manipulabilité du gène et son interopérabilité d’une espèce à 
l’autre, et dans un contexte plus libéral que celui de l’après-guerre, le 
gène devient, dans les années 1980, unité d’appropriation intellectuelle 
1. Kay 1993.
2. Luria 1973 (p. 67).
3. Keller 2003 (p. 79).
4. Monod 1965 (p. 8).
5. Jacob et Monod 1962 (p. 354).
6. Mayr 1961 (p. 1504).
7. Kahn 1996 (p. 16).

	
le siècle du gène	
313
(brevet de séquence) et actif d’une nouvelle bioéconomie 1. La biologie de 
synthèse des dernières années radicalise ce programme en prétendant, 
comme De Vries ou Davenport il y a un siècle, faire mieux que l’évolution, 
en refabriquant sur mesure les génomes de nouveaux êtres.
C’est donc autour des imaginaires du plan, du programme et du contrôle, 
propres au milieu du xxe siècle, qu’une biologie moléculaire « instruction-
niste » creuse la coupure surplombante du génome sur l’environnement 
qu’avait ouverte la génétique. Pour Monod, le programme génétique 
« est totalement, intensément conservateur, fermé sur soi-même et 
absolument incapable de recevoir quelque enseignement que ce soit du 
monde extérieur 2 ». En digne « grand » de la cité industrielle 3, le gène 
apparaît alors investi des prérogatives et du prestige du Plan dans les 
sociétés à économie régulée, ou de l’autorité de l’ingénieur des méthodes. 
Contrôler et commander les gènes, comme dans la course au génome 
humain des années 1990, les plantes transgéniques ou mutées brevetées, 
ou les chromosomes artificiels, semble la voie royale.
Le gène-réseau de la biologie postgénomique
Ces dernières années, les avancées de la biologie ont cependant bouleversé 
les paradigmes antérieurs du néodarwinisme et de la biologie molécu-
laire. Cette dernière a connu une recomposition massive des façons de 
travailler et des compétences mobilisées. L’automatisation et l’expérimen-
tation à haut débit génèrent une foule de données nouvelles sur les profils 
d’expression coordonnée de milliers de gènes, à laquelle la bio-informa-
tique est chargée de donner sens. De même que les chimistes et physiciens 
avaient apporté à la biologie moléculaire leurs outils (cristallographie, 
marqueurs radioactifs, etc.), leurs concepts (l’ADN comme molécule  
à part, ordonnée, permettant d’endiguer l’entropie selon Schrödinger) 
et leurs imaginaires (les feed-back et le code), les bio-informaticiens qui 
peuplent désormais les laboratoires de biologie apportent une nouvelle 
culture à la postgénomique (informatique, théorie des graphes pour 
analyser les propriétés topologiques des réseaux, imaginaire hypertextuel 
de l’Internet…) 4. On assiste alors à la dissolution totale ou partielle 
de la notion de gène (voir le projet Encode), à la mise en évidence de 
phénomènes épigénétiques transmis héréditairement, à la découverte 
1. Thomas 2015.
2. Monod 1970 (p. 145).
3. Boltanski et Thévenot 1991.
4. Fujimura 2005.

314	
christophe bonneuil
du nouveau continent des micro-ARN, à un retour de la complexité et à la 
montée des approches réseaux (biologie des systèmes). Un nombre croissant 
de biologistes fustigent volontiers le génocentrisme et le réductionnisme 
passés et s’intéressent maintenant à des réseaux moléculaires complexes. 
Ils recherchent l’efficacité et la robustesse des processus biologiques dans 
des interactions systémiques et dynamiquement reconduites plutôt que 
par un ordre préexistant dans l’ADN. De la biologie moléculaire à la 
biologie systémique, on passe d’une ontologie substantielle à une ontologie 
relationnelle et, en termes d’économie morale, de la cité industrielle à la 
cité connexionniste par projet 1. La centralisation planiste perd son aura 
dans le regard des scientifiques sur le vivant comme dans le débat socio- 
économique. Déloyal à la machinerie productive de l’organisme, le gène 
semble même parfois jouer son propre jeu ; il devient « égoïste 2 ». Albert-
László Barabási, le mathématicien des réseaux qui a joué un rôle clé  
dans la mise au jour des réseaux de régulation génétique, pousse même 
l’analogie avec le monde économique un cran plus loin :
Le modèle (hiérarchique) en arbre était bien adapté à la production de masse, 
qui était la clé du succès économique jusque récemment. Mais à présent, la 
valeur est dans les idées et l’information. […] Face à une explosion de l’infor-
mation et un besoin inégalé de flexibilité dans un marché en évolution rapide 
[…] les entreprises basculent d’un modèle organisationnel arborescent et 
optimisé vers un modèle en réseau évolutif et dynamique, offrant une structure 
de commandement plus malléable et flexible 3.
La biodiversité, plus que réservoir de gènes, est aussi appréhendée 
comme un tissu bioculturel, dynamique et sans couture. Les anciens 
bornages, les anciennes compartimentations (entre variétés locales et 
variétés « améliorées », entre conservation et utilisation de la diversité, 
entre hérédité et environnement…) érigés par le modernisme génétique 
du xxe siècle cèdent du terrain face à une exaltation hypermoderne de  
la connexion, et l’hybridité. On passerait donc d’une modernité figeant des 
entités tout en invoquant le débordement des frontières à une modernité 
visant à cartographier les continuums en réseau, à intensifier les liens, à 
en capter la valeur ajoutée.
Comme les entreprises, le vivant – de la cellule à l’écosystème en passant  
par l’individu – est évalué, moins sur sa performance en conditions réglées, 
stables et optimales, mais plutôt en fonction de sa capacité à s’ajuster à 
1. Cette analyse est développée dans Bonneuil 2015b.
2. Dawkins 1976.
3. Barabási 2002 (p. 201-202).

	
le siècle du gène	
315
des changements rapides et constants. À toutes ses échelles, le vivant 
est de plus en plus vu comme un système agile, adaptatif, complexe et 
réticulé, ce qui n’est pas sans faire écho à un nouvel ordre économique 
et social caractérisé par l’innovation perpétuelle, la connectivité et la 
spécialisation flexible. Le gène-programme rigide de la biologie molécu-
laire des années 1960 se réinvente flexible dans les réseaux d’interaction 
moléculaires de la biologie systémique 1. Autrefois vu comme domaine 
de l’« ADN poubelle », le nouveau continent des micro-ARN apparaît 
aujourd’hui comme une nouvelle sphère de régulations légères – évitant 
le coût métabolique de la production protéique. Les biologistes utilisent 
parfois explicitement pour le décrire des analogies avec les fonctions 
supérieures jouées par l’économie immatérielle (numérique, finance, 
assurance, innovation, etc.) dans le capitalisme contemporain.
Ces congruences sont-elles de simples coïncidences, de simples facilités 
métaphoriques pour rendre intelligibles les nouvelles avancées de la 
biologie ? Gageons que les travaux à venir identifieront, au Santa Fe Institute 
et ailleurs, entre les sphères de la nouvelle biologie, du management et 
de la finance, et des univers numériques, des circulations non seulement 
d’imaginaires, mais d’acteurs et de pratiques. Et qu’elles éclaireront autant 
la biologie contemporaine que les travaux historiens accumulés ont permis 
d’enrichir notre appréhension de la naissance de la génétique moderne 
ou de la biologie moléculaire.
Références bibliographiques
Allen Garland E., 2003, « Mendel and Modern Genetics : The Legacy for Today », 
Endeavour, vol. 27, no 2, p. 63-68.
Barabási Albert-László, 2002, Linked : The New Science of Networks, New York, Perseus.
Barnes Barry et Dupré John, 2008, Genomes and What to Make of Them, Chicago 
(IL), University of Chicago Press.
Baudrillard Jean, 1975, L’Échange symbolique et la mort, Paris, Gallimard.
Boltanski Luc et Thévenot Laurent, 1991, De la justification. Les économies de la 
grandeur, Paris, Gallimard.
Bonneuil Christophe, 2015a, « Pure Lines as Industrial Simulacra : A  Cultural 
History of Genetics from Darwin to Johannsen », in  Christina Brandt, Staffan 
Müller-Wille et Hans-Jörg Rheinberger (dir.), A Cultural History of Heredity, 
t. 2 : Exploring Heredity, Cambridge (MA), MIT Press.
–	 2015b, « Une nature liquide ? Les discours de la biodiversité dans le nouvel esprit 
du capitalisme », in Frédéric Thomas et Valérie Boisvert (dir.), Le Pouvoir de la 
biodiversité. Néolibéralisation de la nature dans les pays émergents, Paris, Quæ, 
p. 193-213.
Bonneuil Christophe et Thomas Frédéric, 2009, Gènes, pouvoirs et profits, Paris, Quæ.
1. Bonneuil 2015b.

316	
christophe bonneuil
Bud Robert, 1993, The Uses of Life : A History of Biotechnology, Cambridge (MA), 
Cambridge University Press.
Campos Luis, 2009, « That Was the Synthetic Biology That Was”, in Markus Schmidt 
et  al. (dir.), Synthetic Biology : The  Technoscience and Its Societal Consequences, 
Dordrecht, Springer, p. 5-21.
Darwin Charles, 1859, On the Origin of Species by Means of Natural Selection, 
Londres, John Murray.
Dawkins Richard, 2003 [1976], Le Gène égoïste, Odile Jacob.
De Vries Hugo, 1889, Intracellulare Pangenesis, Iéna, Gustav Fischer.
–	 1907, Plant-Breeding, Chicago (IL), Open Court.
Endersby Jim, 2013, « Mutant Utopias : Evening Primroses and Imagined Futures in 
Early Twentieth-Century America », Isis, vol. 104, p. 471-503.
Flitner Michael, 2003, « Genetic Geographies : A Historical Comparison of Agrarian 
Modernization and Eugenic Thought in Germany, the Soviet Union, and the United 
States », Geoforum, vol. 34, no 2, p. 175-185.
Fujimura Joann H., 2005, « Postgenomic Futures : Translations across the Machine- 
Nature Border in Systems Biology », New Genetics and Society, vol. 24, no 2, p. 195- 
225.
Galton Francis, 1876, « A Theory of Heredity », The Journal of the Anthropological 
Institute of Great Britain and Ireland, p. 329-348.
Gaudillière Jean-Paul, 2002, Inventer la biomédecine. La France, l’Amérique et la 
production des savoirs du vivant (1945-1965), Paris, La Découverte.
Gayon Jean, 2000, « From Measurement to Organization : A Philosophical Scheme 
for the History of the Concept of Heredity », in  Peter Beurton, Raphael Falk 
et Hans-Jörg Rheinberger (dir.), The Concept of the Gene in Development and 
Evolution, Cambridge, Cambridge University Press, p. 69-90.
Jablonka Eva et Lamb Marion, 1995, Epigenetic Inheritance and Evolution : 
The Lamarckian Dimension, Oxford, Oxford University Press.
Jacob François et Monod Jacques, 1961, « Genetic Regulatory Mechanisms in the 
Synthesis of Proteins », Journal of Molecular Biology, vol. 3, p. 318-356.
Johannsen Wilhelm, 1903, Erblichkeit in Populationen und in reinen Linien, Iéna, 
Gustav Fischer.
–	 1911, « The Genotype Conception of Heredity », American Naturalist, vol.  45, 
no 531, p. 129-159.
Kahn Axel, 1996, Société et révolution biologique, Paris, Éd. de l’INRA.
Kay Lily E., 1993, The Molecular Vision of Life : Caltech, the Rockefeller Foundation, 
and the Rise of the New Biology, Oxford, Oxford University Press.
Keller Evelyn Fox, 2003, Le Siècle du gène, Paris, Gallimard.
Kohler Robert E., 1994, Lords of the Fly : Drosophila Genetics and the Experimental 
Life, Chicago (IL), University of Chicago Press.
Luria Salvador, 1973, Life : The Unfinished Experiment, New York, Charles Scrib-
ner’s Sons.
Mayr Ernst, 1961, « Cause and Effects in Biology », Science, vol.  134, no  3489, 
10 novembre, p. 1501-1506.
Monod Jacques, 1965, « L’être vivant comme machine », tapuscrit de sa conférence 
lors des XXe  Rencontres internationales de Genève, 2  septembre, archives de  
l’Institut Pasteur, fonds Monod, MON Mss 4.
–	 1970, Le Hasard et la nécessité, Paris, Seuil.
Morange Michel, 1994, Histoire de la biologie moléculaire, Paris, La Découverte.

	
le siècle du gène	
317
Müller-Wille Staffan et Rheinberger Hans-Jörg, 2012, A Cultural History of 
Heredity, Chicago (IL), University of Chicago Press.
Paul Diane B., 1995, Controlling Human Heredity, 1865 to the Present, Atlantic 
Highlands (NJ), Humanities Press.
Paul Diane B. et Kimmelman Barbara A., 1988, « Mendel in America : Theory and 
Practice (1900-1919) », in  Ronald Rainger, Keith R.  Benson et Jane Maien-
schein (dir.) The American Development of Biology, Philadelphie (PA), University 
of Pennsylvania Press, p. 281-310.
Pichot André, 1999, Histoire de la notion de gène, Paris, Flammarion.
Rader Karen A., 2004, Making Mice : Standardizing Animals for American Biomedical 
Research (1900-1955), Princeton (NJ), Princeton University Press.
Rasmussen Nicolas, 2014, Gene Jockeys : Life Science and the Rise of Biotech Enter-
prise, Baltimore (MD), Johns Hopkins University Press.
Rheinberger Hans-Jörg, 1997, Toward a History of Epistemic Things : Synthesizing 
Proteins in a Test Tube, Stanford (CA), Stanford University Press.
Saraiva Tiago, 2016, Fascist Pigs : Technoscientific Organisms and the History of 
Fascism, Cambridge (MA), MIT Press.
Stamhuis Ida H., Meijer Onno G. et Zevenhuizen Erik J.A., 1999, « Hugo De Vries 
on Heredity (1889-1903) : Statistics, Mendelian Laws, Pangenes, Mutations », Isis, 
vol. 90, no 2, p. 238-267.
Strasser Bruno, 2002, « Institutionalizing Molecular Biology in Post-War Europe : 
A  Comparative Study », Studies in History and Philosophy of Biological and 
Biomedical Sciences, vol. 33, p. 515-546.
Sturtevant Alfred H., 1965, A History of Genetics, New York, Harper & Row.
Thomas Frédéric (dir.), 2015, Le Pouvoir de la biodiversité. Néolibéralisation de la 
nature dans les pays émergents, Paris, Presses de l’IRD.
Thurtle Phillip, 2007, The Emergence of Genetic Rationality : Space, Time and Infor-
mation in American Biological Science (1870-1920), Seattle (WA), University of 
Washington Press.
Vavilov Nicolaï I., 1931, « The Problem of the Origin of the World’s Agriculture in 
the Light of the Latest Investigations », in Science at the Crossroads, Londres, Frank 
Cass & Co. (<http://www.marxists.org/subject/science/essays/vavilov.htm >).
Weismann August, 1892, Die Kontinuität des Keimplasmas als Grundlage einer 
Theorie der Vererbung, Iéna, Gustav Fischer, 2e éd. complétée.
Wood Roger J. et Orel Vítězslav, 2005, « Scientific Breeding in Central Europe during 
the Early Nineteenth Century : Background to Mendel’s Later Work », Journal of 
the History of Biology, vol. 38, p. 239-272.
Yates JoAnne, 1989, Control through Communication : The Rise of System in American 
Management, Baltimore (MD), Johns Hopkins University Press.


15 Les théories fondamentales  
de la matière
S i lva n  S .  S c h w e b e r  
e t  J e a n - M a r c  L é v y- L e b l o n d
Le 4 juillet 2012, les unes des journaux du monde entier firent état de la 
découverte du « boson de Higgs » au CERN, à Genève. Une semaine plus 
tard, dans une tribune publiée par le New York Times, Steven Weinberg, 
lauréat du prix Nobel dont les apports théoriques ont contribué à l’avè-
nement de cette particule, posait la question suivante : « Pourquoi une 
telle couverture médiatique aujourd’hui alors que d’autres découvertes 
de particules élémentaires ont déjà eu lieu sans que l’on fasse autant de 
tapage ? » Il répondait que « cette particule fournit une clé essentielle pour 
comprendre comment toutes les autres particules élémentaires acquièrent 
leur masse ». Après avoir été imaginée en 1964 par plusieurs théoriciens, 
cette particule, qui serait plus justement dénommée boson BEH 1, peut 
être considérée comme la clé de voûte du « modèle standard » – comme 
on appelle la théorie aujourd’hui dominante qui décrit les particules 
élémentaires et leurs interactions. La découverte annoncée par le CERN 
couronne ainsi le succès de ce modèle, qui constitue l’un des grands 
accomplissements intellectuels du xxe siècle. L’énorme quantité de 
données empiriques qu’il explique avec une grande précision, la gamme 
d’énergies qu’il couvre, la robustesse de ses présupposés dans son domaine 
de validité, attestent de son importance. Il conduit aussi, nous le verrons, 
à discuter la nature particulière et l’évolution de la connaissance scienti-
fique en tant qu’activité sociale.
On ne saurait cependant ignorer des facteurs moins strictement scien-
tifiques de la faveur médiatique qu’a connue le boson de Higgs. Il s’agit 
1. « Boson BEH », d’après les initiales des noms de ses principaux inventeurs : Brout et Englert d’une 
part et Higgs d’autre part, indépendamment et simultanément – sans parler des contributions 
de Kibble, de Hagen et Guralnik, etc.
 Peter Higgs devant le détecteur Atlas du LHC au CERN.

320	
silvan s. schweber et jean-marc lévy-leblond 
là en vérité d’un exemple particulièrement représentatif de la big science 
aujourd’hui en œuvre dans ce secteur de la physique : le LHC (Large 
Hadron Collider), accélérateur de particules du CERN (Centre européen de 
recherches nucléaires) à Genève, est une machine proprement gigantesque, 
logée dans un anneau souterrain de 27 kilomètres de circonférence, dont le 
coût est de l’ordre de 5 milliards d’euros. L’échelle humaine n’est pas moins 
impressionnante que l’échelle technique, puisque les articles annonçant 
la découverte étaient signés de plusieurs centaines de chercheurs, ce qui 
rendait la liste des auteurs et de leurs laboratoires presque aussi longue 
que le texte proprement dit. Plus de 3 000 expérimentateurs et ingénieurs 
ont été impliqués dans la construction d’ATLAS, l’un des deux détecteurs 
utilisés pour les expériences auprès du LHC ; et plus de 5 000 expérimen-
tateurs d’une cinquantaine de pays collaborent à l’analyse des milliers de 
milliards de collisions dont ces détecteurs enregistrent les traces. Il est 
probable qu’aucun prix Nobel ne récompensera la découverte du boson 
de Higgs étant donné le nombre de personnes impliquées. De fait, le 
caractère remarquablement coopératif de l’entreprise constitue peut-être 
sa particularité première.
On conçoit donc aisément que les enjeux de visibilité institutionnelle, 
de prestige académique, de justification sociale et économique aient joué 
un rôle essentiel dans la publicité faite à cet événement – sans que cela 
diminue en rien son intérêt scientifique. Comment en est-on arrivé là ?
La physique fondamentale  
avant la Seconde Guerre mondiale
Vers la fin du xixe siècle, la physique avait pour double fondement la 
mécanique classique et la théorie des champs électromagnétiques. Mais 
elle échouait à rendre compte de la stabilité d’un système constitué de 
particules interagissant via les forces électriques, tels les atomes, ainsi 
que de la structure des radiations lumineuses émises par ces atomes, 
et de la nature des forces entre atomes expliquant la constitution des 
molécules 1. Pire encore peut-être, la théorie classique ne parvenait pas à 
expliquer le rayonnement des corps macroscopiques en fonction de leur 
température (métaux « chauffés au rouge » dans les forges, fours, etc.). 
1. Nous concentrant ici sur la physique de la matière, nous laissons de côté l’autre grand problème 
rencontré à la fin du xixe siècle par la physique classique, à savoir la difficulté de concilier la 
représentation galiléo-newtonienne de l’espace et du temps avec la théorie maxwellienne de 
l’électromagnétisme. C’est la théorie einsteinienne de la relativité qui allait redéfinir le cadre 
spatio-temporel de façon à régler cette contradiction apparente.

	
les théories fondamentales de la matière 	
321
Un hiatus inquiétant séparait alors les théories « fondamentales » de 
la matière et les disciplines plus proches des applications techniques 
comme la chimie, l’électronique, la physique des rayonnements (de la 
radio aux rayons X). Mais le début du xxe siècle vit la mise au point de  
la théorique quantique, qui rencontra un succès remarquable en surmontant 
les difficultés évoquées. Ses fondateurs, juste avant la Première Guerre 
mondiale, furent Planck, Einstein et Bohr, avant qu’elle ne soit pleinement 
développée dans la décennie 1920-1930, avec comme protagonistes 
principaux de Broglie, Heisenberg, Born, Schrödinger, Dirac. Dès la 
première édition de ses Principes de la mécanique quantique, en 1930, 
Dirac pouvait souligner que la représentation du monde physique par 
la théorie quantique crée un ordre hiérarchique séparant le domaine de 
validité de cette théorie de celui où prévalent les théories classiques 1.
Contrairement à la mécanique classique, la mécanique quantique stipule 
que les énergies des « états liés » (où les particules concernées restent 
groupées) ne peuvent avoir que des valeurs discontinues. Il existe en parti-
culier un état de plus basse énergie, dit « état fondamental ». Remarquons 
au passage que, quoi qu’on ait fait dire au principe malencontreusement 
nommé d’incertitude de Heisenberg, la mécanique quantique nous assure 
de solides connaissances sur la matière : tous les atomes de sodium, dans 
leur état fondamental, lorsqu’ils sont isolés, sont identiques ; il en va  
de même pour les noyaux d’hélium, de plomb ou de tout autre élément. 
De plus, si la différence entre l’énergie de l’état fondamental et celle du 
premier état excité d’un système lié est très supérieure à l’énergie que 
le système peut acquérir en interagissant avec son environnement, le 
système est stable (dans ce contexte). Les noyaux dans les environnements 
terrestres ont cette propriété : les premiers états excités des noyaux sont 
séparés de leur état fondamental par des énergies d’un ordre de grandeur 
des milliers de fois au moins supérieures aux énergies disponibles autour 
d’eux (par exemple par l’agitation thermique ou les liaisons électroniques).
Le succès de la mécanique quantique tient ainsi à ce que, dans leur 
contexte terrestre habituel, les atomes et les molécules peuvent être consi-
dérés comme composés de noyaux stables et d’électrons que l’on peut 
traiter comme ponctuels. Le fait que la taille des noyaux soit très petite 
par rapport aux dimensions atomiques et que leur masse soit très grande 
comparée à celle des électrons justifie l’approximation selon laquelle les 
1. C’est la constante de Planck, notée h, qui sert d’étalon : les systèmes dont les grandeurs 
caractéristiques de masse M, de longueur L et de temps T ont des valeurs telles que ML2 / T >> h 
(dont la valeur numérique dans le système SI est 6,62606957 × 10−34) sont macroscopiques et 
peuvent être décrits par la mécanique classique ; ceux pour lesquels ML2 / T ≈ h sont en général 
microscopiques et doivent être décrits par la mécanique quantique.

322	
silvan s. schweber et jean-marc lévy-leblond 
électrons interagissent essentiellement avec les noyaux et entre eux via 
les forces électrostatiques de Coulomb. Joue également un rôle capital la 
faible vitesse des électrons par rapport à celle de la lumière, qui permet 
de négliger en première instance les effets dus à la relativité einsteinienne 
(ce pourquoi on parle souvent de « mécanique quantique non relati-
viste »). Le dernier élément essentiel est que les électrons, absolument 
identiques, ne peuvent être distingués les uns des autres, ce qui les 
contraint à obéir au principe d’exclusion de Pauli, qui régit la structure 
électronique « en couches » des atomes. Ainsi furent expliquées la consti-
tution des atomes, celle des molécules, la nature des liaisons chimiques.
Dès ses débuts, la théorie quantique put être appliquée efficacement 
à des systèmes comportant un grand nombre de particules. Ainsi, le 
comportement quantique collectif des électrons permit-il de comprendre 
les propriétés thermiques et électriques des matériaux – conducteurs 
(métaux), isolants ou semi-conducteurs –, ouvrant la voie aux développe-
ments techniques de l’électronique moderne (transistors) après la Seconde 
Guerre mondiale. Le comportement collectif des photons et en particulier 
le phénomène d’émission stimulée théoriquement découvert par Einstein 
en 1917 allait un demi-siècle plus tard conduire au développement des 
lasers. En physique des basses températures, l’explication de phénomènes 
originaux telles la supraconductivité ou la superfluidité renforcerait encore 
dans les années 1950 (voir plus bas) la puissance souvent explicative et 
parfois prédictive de la théorie quantique. Et certains objets du macro-
cosme, comme les naines blanches, révéleraient même de remarquables 
exemples de comportement quantique à grande échelle 1.
À la fin des années 1920 et dans les années 1930, et du côté cette fois-ci 
du microcosme, c’est la physique nucléaire qui ouvrirait un nouveau 
champ d’application à la théorie quantique. Il est remarquable que les 
noyaux des atomes se comportent suivant les lois quantiques générales 
pourtant mises au point dans le domaine atomique à des échelles cent 
mille à un million de fois plus grandes, et bien qu’ils soient régis par des 
forces d’une tout autre nature que les interactions électromagnétiques 
qui gouvernent les électrons atomiques. La structure, la stabilité et les 
propriétés détaillées des noyaux dépendent de la dynamique quantique 
des forces nucléaires d’interaction entre leurs composants (protons et 
neutrons). Cela dit, l’intensité de ces forces rend l’analyse de leurs effets 
bien plus difficile que dans le cas des interactions électromagnétiques 
et seuls des succès partiels s’avérèrent possibles. Ces développements 
théoriques de la physique nucléaire allèrent de pair avec une mutation 
1. Chandrasekhar 1931.

	
les théories fondamentales de la matière 	
323
des techniques expérimentales, où les accélérateurs de particules (cyclo-
trons) prirent le relais des éléments radioactifs naturels, et de nouveaux 
détecteurs (chambres à bulles) entrèrent en jeu.
Peu de temps après la formulation des lois quantiques régissant la 
dynamique de systèmes composés d’un nombre fini d’électrons et de 
noyaux, le formalisme fut étendu par Dirac et d’autres, à la fin des années 
1920, de manière à inclure la description des interactions avec le champ 
électromagnétique. C’est ainsi qu’au cours des années 1930 se développe 
une nouvelle conceptualisation de la physique où les interactions électro-
magnétiques entre particules chargées (comme les forces électrostatiques 
bien connues) peuvent être expliquées par des échanges de photons.  
La difficulté de la tâche était considérable puisqu’elle exigeait de passer du 
cadre spatio-temporel newtonien au cadre einsteinien, seul compatible 
avec la description de l’électromagnétisme. En fait, une électrodynamique 
quantique cohérente et à peu près satisfaisante ne serait développée 
qu’après la Seconde Guerre mondiale. Cependant, d’emblée, les connais-
sances ainsi acquises induisent le développement d’une théorie générale 
des champs quantiques permettant la compréhension d’autres phéno-
mènes au niveau subnucléaire : formulation par Fermi d’une théorie de 
la radioactivité bêta, suggestion de Yukawa que, par analogie avec les 
forces électromagnétiques, les forces nucléaires de courte portée entre 
nucléons peuvent être générées par les échanges entre eux de particules 
massives jusque-là non observés (dénommés « mésons » après leur mise 
en évidence expérimentale à la fin des années 1940).
Ainsi s’est imposée une conception de la physique fondamentale où le 
monde physique peut être considéré comme hiérarchiquement ordonné en 
domaines et questions assez bien délimités : le niveau cosmologique (celui 
des galaxies, de leurs éléments, de leur évolution et de leur dynamique), 
le niveau macroscopique (constitué des états classiques de la matière, 
solides, liquides, gaz, de leurs structures, propriétés et processus), le niveau 
microscopique (moléculaire et atomique, nucléaire et subnucléaire) 1. 
L’objectif de la physique est d’identifier, classer et caractériser ces diffé-
rents niveaux, et leurs relations. Les derniers niveaux sont considérés 
comme plus « fondamentaux » et l’on s’efforce de reconstruire les niveaux 
supérieurs grâce à la connaissance des niveaux de base. Plus concrètement, 
les théories visant les niveaux inférieurs cherchent à expliquer de manière 
quantitative les paramètres déterminés empiriquement qui décrivent les 
1. Il faut cependant noter que la distinction entre ces niveaux n’est pas absolue, puisque nombre 
de propriétés de la matière à notre échelle (telle son impénétrabilité) dépendent du comportement 
quantique de ses constituants. Voir aussi l’exemple cité plus haut des étoiles naines blanches.

324	
silvan s. schweber et jean-marc lévy-leblond 
composants des niveaux supérieurs. Il y a évidemment là une pétition de 
principe quant à la possibilité même d’une telle remontée. On reviendra 
plus loin sur l’évaluation du succès de ces démarches réductionnistes.
Il est essentiel de noter qu’avant les années 1930 ces avancées n’ont 
guère connu d’applications. Rutherford lui-même, découvreur en 1913 du 
noyau atomique, exprimait en 1935 encore son total scepticisme quant à 
la possibilité d’utilisations pratiques de la physique nucléaire.
La physique fondamentale  
après la Seconde Guerre mondiale
La découverte de la fission nucléaire (Hahn, Strassmann, Meitner) en 
1938, à la veille de la guerre, va se révéler d’une importance essentielle. 
Ce phénomène ouvre en effet la voie, grâce à la possibilité de réactions en 
chaîne, à la libération des énergies considérables que recèlent les noyaux 
atomiques. La possibilité d’applications militaires est rapidement perçue 
par les physiciens, qui finiront par en convaincre les pouvoirs politiques 
des pays belligérants. Ainsi naîtra en 1942 le projet Manhattan d’étude 
puis de fabrication des premières armes nucléaires aux États-Unis, qui se 
conclura par les bombardements d’Hiroshima et Nagasaki en août 1945, 
marquant tragiquement l’entrée de l’humanité dans l’ère nucléaire. On 
ne saurait surestimer la mutation ainsi induite dans la conception de la 
recherche. Ce sont en effet les meilleurs fondamentalistes, théoriciens  
et expérimentateurs, qui furent mis à la tâche pour concevoir les appli-
cations militaires et, accessoirement, civiles (centrales électronucléaires) 
de leurs connaissances. Les investissements consentis en raison de l’effort  
de guerre furent gigantesques ; on estime que les dépenses militaires 
états-uniennes en matière de recherche et développement furent multi-
pliées par un facteur 1 000 entre 1935 et 1945.
La science fondamentale prend ainsi aux yeux des responsables politiques 
et économiques une importance toute nouvelle et sera conçue dès la fin 
de la guerre comme un élément capital d’une politique de puissance, 
caractérisée aussi bien par la course aux armements des années de guerre 
froide que par l’expansion des industries technologiques. Cette nouvelle 
donne fut explicitée par le fameux rapport présenté au président Truman 
en 1947 par son conseiller scientifique Vannevar Bush sous le titre aussi 
éloquent qu’oxymorique de Science, the Endless Frontier (« La science, une 
frontière sans limites »). L’ordre de grandeur des investissements publics 
en matière de science fondamentale changea du tout au tout et permit 
le développement de la big science, reposant sur une instrumentation 

	
les théories fondamentales de la matière 	
325
d’échelle véritablement industrielle que matérialisèrent par exemple les 
grands accélérateurs de particules.
Si la science fondamentale contribua ainsi à la guerre, la réciproque 
fut non moins essentielle. Les techniques militaires développées, par 
exemple en électronique pour améliorer l’efficacité des radars, trans-
forment la physique atomique et moléculaire. Au niveau subnucléaire, 
les nouveaux accélérateurs, dont la construction devient possible grâce 
à la technologie des klystrons développée pendant la guerre, font radica-
lement évoluer le domaine en permettant l’étude de phénomènes à des 
énergies toujours croissantes. On désignera ainsi cette discipline comme 
la « physique des hautes énergies », appellation quelque peu ambiguë, 
puisqu’elle pourrait laisser croire qu’il s’agit d’un domaine producteur 
d’énergies considérables, alors qu’il en est au contraire consommateur.
Ces progrès de l’instrumentation ont des répercussions immédiates 
en physique théorique et permettent la mise en évidence d’effets spéci-
fiques de l’électrodynamique quantique. Les mesures des structures 
fines et hyperfines du spectre d’énergie de l’atome d’hydrogène exigent 
d’aller au-delà des seules prédictions de l’équation de Dirac. Ainsi,  
Bethe remarque que l’autoénergie d’un électron due à son champ électro-
magnétique intrinsèque peut être considérée comme une contribution 
à la masse inertielle de l’électron, et qu’elle doit être rajoutée à la masse 
mécanique de l’électron. Malheureusement, cette contribution, calculée 
par l’électrodynamique quantique, est infinie. On s’en tire par une 
procédure dite de « renormalisation » où seule la somme des masses 
électromagnétique et mécanique de l’électron est identifiée à sa masse 
expérimentalement mesurée, ce qui permet l’élimination des quantités 
infinies apparues dans les calculs.
C’est ainsi que finira par être formulée une électrodynamique quantique 
cohérente, théorie de l’interaction des électrons et des positrons avec le 
champ électromagnétique, en accord avec la relativité einsteinienne. Bethe, 
Schwinger, Feynman, Tomonaga et Dyson développent dans l’immédiat 
après-guerre le programme de renormalisation moderne, explicitant les 
règles méthodologiques permettant de contourner les divergences rencon-
trées dans certaines théories quantiques relativistes. Les représentations 
diagrammatiques inventées par Feynman simplifient grandement les 
calculs et joueront un rôle essentiel dans le succès de l’électrodynamique 
quantique et plus généralement de la théorie quantique des champs.
Tout au long des années 1950 à 1980, la physique des particules élémen-
taires connut un développement massif. Dans la foulée des apports 
militaires (armement nucléaire) et civils (centrales électronucléaires) 
de leur science, les physiciens purent obtenir un soutien étatique massif 

326	
silvan s. schweber et jean-marc lévy-leblond 
pour une physique dont ils n’hésitèrent pas à faire miroiter à terme des 
applications pratiques spectaculaires – restées largement illusoires. De 
plus, l’ampleur des efforts financiers nécessaires au déploiement de la 
big science conduisit à une internationalisation qui joua un rôle politique 
non négligeable. Ainsi, la création du CERN, par-delà sa seule ambition 
scientifique, constitua un véritable banc d’essai pour les formes nouvelles 
des collaborations institutionnelles européennes.
Les accélérateurs de particules et les détecteurs atteignirent des dimen-
sions considérables, et des coûts à l’avenant. De nombreuses particules 
nouvelles, de durée éphémère et donc absentes des environnements 
naturels à notre échelle, furent découvertes. La mise en ordre de ce 
véritable zoo put être accomplie grâce à la mise en évidence de symétries 
structurelles profondes (par Gell-Mann en particulier), ce qui conduisit à 
postuler l’existence d’un niveau sous-jacent (sub-sub-nucléaire), peuplé 
de nouvelles entités baptisées « quarks » et « gluons », constituant des 
« nucléons », et de leurs cousins plus ésotériques.
En même temps, conséquence des progrès techniques réalisés par les 
formalismes de Schwinger, Feynman et Dyson, la physique de l’état solide 
et la mécanique statistique commencèrent à utiliser les méthodes de la 
théorie quantique des champs. La fin des années 1950 voit l’incorporation 
de ces méthodes dans la théorie du « problème à N corps », ainsi que l’on 
nomme la physique théorique de la matière condensée, qui concerne les 
systèmes macroscopiques composés d’un nombre élevé de particules. Un 
succès remarquable est la formulation par Bardeen, Cooper et Schrieffer 
d’un modèle (dit BCS) utilisant la théorie des champs pour expliquer la 
supraconductivité. Ce phénomène, connu depuis le début du xxe siècle, 
se caractérise par la disparition de toute résistance électrique en dessous 
d’une certaine température critique, et par l’exclusion de tout flux magné-
tique à l’intérieur d’un supraconducteur en dessous de cette température 
(effet Meissner). Jusqu’à la formulation de la théorie BCS, la supracon-
ductivité était considérée comme un problème majeur que la mécanique 
quantique n’arrivait pas à expliquer.
La théorie de la supraconductivité permit un considérable approfondis-
sement de la théorie quantique des champs. De fait, la solution obtenue 
par BCS viole le principe de la conservation du nombre de particules, 
alors même que ce principe est respecté par les équations qui déter-
minent la structure et la dynamique du système. Une analyse majeure de 
l’approche BCS par Anderson et d’autres permit de préciser et d’étendre 
le rôle joué en théorie quantique des champs par les principes d’inva-
riance et les symétries associées. La clarification ultérieure de la notion 
de « brisure spontanée de symétrie » par Anderson, Nambu, Goldstone, 

	
les théories fondamentales de la matière 	
327
Salam, Weinberg, Higgs, Kibble et d’autres fut une étape essentielle vers 
la formulation finale du modèle standard et l’acceptation des théories 
de jauge de Yang-Mills qui le sous-tendent. Les physiciens se convain-
quirent que les théories quantiques des champs constituent le formalisme 
approprié pour la représentation de toutes les théories de base décrivant 
le monde microscopique jusqu’à des distances de l’ordre de 10 – 20 m, soit 
des échelles un million de fois inférieures à celles de la physique nucléaire.
Les entités responsables des forces électromagnétiques et des forces 
nucléaires faibles (à l’origine de la radioactivité bêta) sont représentées 
pratiquement de manière identique dans les équations qui gouvernent 
ces forces. En intervertissant les symboles représentant le photon, l’entité 
responsable de la force électromagnétique entre particules chargées, avec 
une combinaison des symboles représentant les particules W et Z, respon-
sables de la force nucléaire faible, les équations restent inchangées. Cette 
symétrie rassemble les descriptions des deux types de force en une seule 
théorie unifiée appelée « électrofaible ». La théorie électrofaible dicte les 
interactions entre les entités de base assujeties aux forces électromagné-
tiques et aux forces nucléaires faibles : les leptons (électron, muon, tauon 
Brisure spontanée de symétrie
En physique classique, lorsque la théorie qui décrit un système respecte une 
certaine symétrie, il se peut que l’état fondamental du système (son état de plus 
faible énergie) ne soit pas invariant sous cette symétrie ; on dit alors qu’il y a brisure 
de la symétrie. L’exemple le plus simple est celui d’un aimant, dont l’axe des pôles 
correspond à une direction particulière alors que les interactions responsables 
du magnétisme ne privilégient aucune direction (isotropie). La même question 
peut être posée pour les systèmes physiques décrits par les théories quantiques. 
Pour les systèmes possédant un nombre fini de degrés de liberté, il est possible 
de démontrer que l’état fondamental doit toujours respecter la symétrie de la 
dynamique. Une brisure spontanée de symétrie ne peut survenir que dans les 
systèmes comprenant un nombre infini de particules, quoique conservant une 
densité finie. La brisure spontanée d’une symétrie continue est toujours associée 
à une dégénérescence de l’état fondamental (dénommé le « vide ») ; il existe alors 
une quantité infinie d’états de même énergie minimale. De plus, dans le cas de la 
théorie quantique des champs, si la brisure spontanée de symétrie est continue, 
un théorème, démontré la première fois par Goldstone, Salam et Weinberg, 
stipule que le spectre de particules physiques doit contenir une particule de 
masse nulle pour chaque symétrie brisée. Ces particules sont désignées sous le 
nom de bosons de Goldstone. Il existe une exception importante à ce théorème 
dans le cas où la symétrie brisée est une symétrie de jauge. Elle est responsable 
du mécanisme de Higgs-Kibble, qui engendre le fameux boson BEH.

328	
silvan s. schweber et jean-marc lévy-leblond 
Symétries
La relativité einsteinienne dite restreinte est caractérisée par une symétrie globale. 
Elle repose sur l’équivalence des référentiels utilisés pour repérer les diverses 
grandeurs physiques lorsque ces deux référentiels sont en mouvement relatif 
uniforme (à vitesse constante). La transformation (dite de Lorentz) qui connecte 
deux tels référentiels équivalents est une transformation globale, qui vaut en tout 
point et en tout instant.
En revanche, les axiomes de la relativité dite générale sont uniquement locaux. 
La relativité générale impose simplement l’équivalence de deux référentiels dans 
l’environnement immédiat d’un point donné de l’espace-temps. La vitesse relative 
de ces deux référentiels dans le voisinage d’un point donné de l’espace-temps 
dépend donc de ce point et varie de point à point : il s’agit d’une transformation 
locale. La relativité générale stipule que les lois de la physique sont les mêmes 
dans deux référentiels quelconques, ce qui se traduit par l’invariance de la forme 
des équations de base sous des transformations arbitraires de coordonnées. 
Cette exigence peut être satisfaite par l’introduction d’un champ lié aux accélé-
rations correspondant aux variations de vitesse dans l’espace-temps. Le principe 
d’équivalence amène alors à constater que ce champ n’est autre que le champ de  
gravitation.
La relativité générale conduit donc à reconnaître l’idéalisation impliquée dans 
les hypothèses de la relativité restreinte et pose du coup la question de savoir si 
l’imposition sur une théorie d’une symétrie globale, quelle qu’elle soit, n’est pas 
une exigence trop large et irréaliste. Ainsi, peu de temps après l’avènement de 
la mécanique quantique, Fritz London considère les conséquences de l’exigence 
d’invariance de la fonction d’onde d’une particule quantique sous des variations
et leurs neutrinos associés), les quarks, les bosons médiateurs des inter­
actions faibles (W, Z et photon). Mais si rien n’intervenait pour briser la 
symétrie mentionnée, les particules W et Z, comme le photon, n’auraient 
pas de masse. En fait, toutes les autres particules élémentaires seraient 
également de masse nulle. Or la plupart des particules élémentaires ont 
une masse. Pour briser la symétrie électrofaible et donner des masses 
à ces entités élémentaires, Weinberg et d’autres théoriciens émirent au 
début des années 1960 l’hypothèse de l’existence de champs quantiques 
spécifiques correspondant à l’existence d’un nouveau type de particule 
électriquement neutre, instable : le boson de Higgs. Sa détection au CERN 
en 2012 était basée sur les modes de désintégration prévus par le modèle 
standard, apportant ainsi un fort crédit au mécanisme de Higgs expli-
quant pourquoi les particules élémentaires sont massives.
Très peu de temps après la formulation de la théorie électrofaible, le 
modèle de théorie des champs mettant en œuvre la notion de symétrie de 

	
les théories fondamentales de la matière 	
329
de jauge
de phase locales et découvre le lien entre cette exigence et le couplage des parti-
cules chargées au champ électromagnétique.
En physique nucléaire, après la découverte du neutron en 1932, il est rapidement 
établi que les protons et les neutrons (même s’ils ont des propriétés électriques diffé-
rentes) ne peuvent être distingués les uns des autres en ce qui concerne les forces 
nucléaires. Heisenberg est ainsi amené à considérer le proton et le neutron comme 
les deux états d’une particule désormais appelée nucléon. Les forces nucléaires sont 
ainsi invariantes par rapport aux transformations mutuelles de ces états. Consi-
dérer cette invariance comme une symétrie locale, dite de jauge, fait apparaître un 
champ de force appelé champ de Yang-Mills. Ses propriétés sont beaucoup plus 
compliquées que celles du champ électromagnétique, car les transformations de 
jauge ont ici une structure plus riche et plus complexe (elles sont « non commu-
tatives »). Comme pour les photons, les quanta associés au champ de Yang-Mills 
sont de masse nulle et, de manière similaire, le terme d’interaction entre le nucléon 
et le champ est déterminé par le principe d’invariance de jauge.
L’utilisation des champs de jauge de Yang-Mills a servi de base aux grandes 
avancées dans la physique des particules élémentaires au cours de la seconde moitié 
du xxe siècle. Cela est dû au fait que, dans le cas des symétries qui agissent sur 
les attributs internes des particules, l’exigence que la symétrie soit locale prescrit 
la forme de l’interaction du champ de jauge avec ses sources. Toutes les interac-
tions actuellement connues (gravitationnelle, électrofaible, nucléaire forte) sont 
décrites par des théories de jauge. Cela a fait naître l’espoir de toutes les englober 
dans une « Grande Théorie Unifiée » impliquant un seul champ de jauge ayant de 
nombreuses dimensions.
jauge locale, qui a été utilisé dans la théorie électrofaible, est adopté pour 
la formulation de la chromodynamique quantique (CDQ). La CDQ est la 
théorie qui rend compte des forces nucléaires dites cette fois « fortes » ; 
elle le fait en termes de particules considérées aujourd’hui comme les 
plus élémentaires, les quarks et les gluons, médiateurs des interactions 
entre les quarks qui portent maintenant des charges métaphoriquement 
appelées « couleurs ». Une nouvelle fois, le mécanisme de Higgs est invoqué 
pour donner une masse à ces entités. La CDQ a deux propriétés remar-
quables. La première est ce qu’on nomme la « liberté asymptotique » : les 
interactions entre quarks et gluons et entre gluons disparaissent lorsque 
la séparation entre eux devient nulle. La seconde est le « confinement » : 
les quarks et les gluons, qui sont les composants présumés des nucléons 
et des mésons, ont la propriété de ne pas pouvoir être observés comme 
objets séparés et indépendants ; ils n’existent qu’au sein des hadrons, les 
entités qui interagissent via les forces nucléaires fortes.

330	
silvan s. schweber et jean-marc lévy-leblond 
Les théories quantiques relativistes des champs reposent sur certaines 
hypothèses concernant le comportement des particules sur des échelles 
arbitrairement petites de l’espace-temps ou, de manière équivalente, sur 
des échelles arbitrairement élevées en énergies et quantités de mouvement. 
Cependant, il est impossible de vérifier directement par l’expérience ces 
hypothèses. Or, dans les théories actuelles, ces comportements à haute 
énergie (ou courtes distances) donnent aux valeurs calculées de diverses 
grandeurs physiques des valeurs infinies (« divergences »). Si, dans le cas 
de l’électrodynamique quantique, on a pu maîtriser ces « divergences », 
tel n’est pas le cas pour la chromodynamique quantique qui régit les 
interactions nucléaires fortes.
Le programme de renormalisation développé après la Seconde Guerre 
mondiale et visant à contourner ces divergences fit l’objet d’une nouvelle 
lecture par Kenneth Wilson et d’autres au début des années 1970. De 
manière intéressante, un grand nombre d’idées ont émergé dans le sillage 
de l’analyse et de la résolution d’un problème de physique de la matière 
condensée par Wilson : le problème des transitions de phase (les transfor-
mations liquide / solide par exemple). Wilson réussit à montrer que l’effet 
de tous les termes d’interaction possibles d’une théorie donnée au-delà 
d’une certaine coupure (cut-off) aux hautes énergies pouvait être inter-
prété comme une reparamétrisation des interactions dans le domaine 
de basse énergie de la théorie.
La grande avancée de Wilson, Weinberg et d’autres fut de démontrer 
l’universalité de la physique des basses énergies résultant du processus de 
renormalisation, justifiant ainsi l’utilisation des « théories des champs effec-
tives », permettant de décrire les phénomènes dans un régime d’énergie très 
inférieur à une certaine coupure. Une telle théorie effective suppose que la 
physique dans le domaine dans lequel elle est valide peut être énoncée en 
termes d’entités considérées comme élémentaires, à partir desquelles sont 
construites les entités composites qui peuplent ce domaine. Ces entités 
« élémentaires » constituent les niveaux de liberté effectifs pour cette échelle. 
Elles dépendent d’une théorie plus « fondamentale » seulement de par un 
petit ensemble de paramètres qui entrent dans la description de l’évolution 
de ces entités. Une théorie effective des champs est une description de 
type phénoménologique, à validité limitée. Ainsi, les progrès des sciences 
physiques dans le domaine de la théorie quantique des champs justifient 
la vision qui segmente le monde physique en différents niveaux chacun 
décrit par une théorie ad hoc, mais stable et robuste. Ce qui importe ici 
est le fait que cette description précise et stable du monde microscopique 
n’est pas déstabilisée par l’incorporation de nouveaux effets de petite 
distance apportés par des découvertes à plus haute énergie. Par exemple, 

	
les théories fondamentales de la matière 	
331
la mécanique quantique non relativiste qui rend compte de la plupart 
des phénomènes atomiques et moléculaires n’est nullement affectée par 
les mutations de la théorie concernant les interactions de haute énergie 
entre particules fondamentales, tout au moins tant que l’on se garde de 
tester le système sur des distances si faibles que la « théorie effective » n’est 
plus applicable. Cela signifie que la théorie qui décrit atomes, molécules 
et solides, et permet de calculer un grand nombre de leurs propriétés, 
connaît une forme de clôture. Les chercheurs dans ces domaines sont 
surtout intéressés par la découverte d’entités ou d’effets inédits, souvent 
inobservables dans le monde naturel, par leur complexité et diversité, et 
moins par la théorie fondamentale qui régit les interactions et détermine 
l’évolution des structures aux niveaux sous-jacents.
L’évolution de la physique fondamentale  
et les transformations de la société
Il faut alors remarquer que les progrès de la physique théorique depuis le 
milieu des années 1970 entrent en résonance avec les profonds boulever-
sements culturels, sociaux et politiques que le monde occidental connaît 
depuis cette époque. Si les explications de ces mutations dépendent pour 
l’essentiel de facteurs externes (sociaux, politiques, économiques), les 
facteurs cognitifs internes aux champs scientifiques et technologiques 
ne sauraient pour autant être négligés, comme nous venons de le voir.
Cependant, sauf pour une petite minorité de physiciens, le programme 
de recherche de la discipline est largement dicté par ces facteurs externes : 
à savoir, les exigences d’innovation pratique, de rentabilité économique, 
d’efficacité technique, comme on le voit dans les domaines des nanotech-
nologies, de la photonique ou de l’informatique quantique. Il devient 
ainsi difficile de distinguer recherches fondamentales et recherches appli-
quées et même, souvent, de séparer activités de recherche scientifique 
et activités de développement technologique.
Paul Forman a pu caractériser les transformations radicales en cours, 
comme « l’inversion de la relation culturellement assignée à la science 
et à la technologie, de la primauté de la science par rapport à la techno-
logie jusqu’à 1980 environ, à la primauté de la technologie par rapport à 
la science depuis ». Pour Forman, cela correspond à la démarcation entre 
modernité et postmodernité. La modernité se caractérisait par le fait que
le désintéressé primait sur l’intéressé et par la croyance que les moyens 
consacrent les fins, que l’adhésion aux moyens qui conviennent est la 

332	
silvan s. schweber et jean-marc lévy-leblond 
meilleure garantie pour arriver au « bon résultat ». Avec la postmodernité, 
à l’inverse, l’approche est descendante : la technologie est la bénéficiaire, 
aux dépens de la science, de la subordination pragmatique et utilitariste 
que nous faisons des moyens aux fins, et […] notamment du scepticisme à 
l’égard du désintéressement et de la condescendance quant aux questions  
conceptuelles 1.
Le néolibéralisme a incontestablement joué un rôle important dans 
l’avènement de cette transformation que Philip Mirowski caractérise ainsi :
Le principe fondamental du néolibéralisme est que le marché constitue un 
processeur d’informations idéal, et que toute économie qui réussit est une 
économie de la connaissance. Bien qu’il s’agisse d’un artefact, le marché est 
censé être plus savant que n’importe quel individu et pouvoir toujours apporter 
des solutions aux problèmes (dont il est la cause première) : les entreprises 
ne peuvent rien faire de mal. En ce qui concerne la raison d’État, le néolibé-
ralisme est ambivalent. D’un côté, il est pour la démocratie comme cadre 
approprié de l’État dans un marché idéal, mais d’un autre côté, il souhaite 
en supprimer les pouvoirs. Il traite la politique comme s’il s’agissait d’un 
marché et défend une théorie économique de la démocratie dans laquelle  
la citoyenneté est assimilée au fait d’être client des services de l’État. Le néoli-
béralisme considère l’éducation comme une marchandise et non comme une 
expérience qui transforme l’existence, en produisant des étudiants informés, 
responsables et attentifs (caring) 2.
La résonance de cette idéologie avec certaines conceptions en physique 
contemporaine est illustrée par l’ouvrage de Robert Laughlin, A Different 
Universe : Reinventing Physics from the Bottom Down. Laughlin est un 
théoricien de la matière condensée qui a partagé le prix Nobel pour 
l’explication de l’effet Hall fractionnaire découvert par von Klitzing. Le 
message de Laughlin est que l’émergentisme a triomphé du réduction-
nisme. Sa thèse est que les objets macroscopiques sont les produits d’un 
principe d’organisation et d’un comportement collectif qui ne peuvent 
pas être réduits à l’évolution de leurs composants « élémentaires ». La 
position de Laughlin est encore plus extrême que celle exprimée par 
Philip Anderson dans son article marquant « More Is Different » publié 
dans Science en 1971.
De fait, si, comme nous l’avons vu, l’analyse descendante de la consti-
tution de la matière à des niveaux d’échelles de plus en plus petits a connu 
un très grand succès, on ne peut en dire autant de la synthèse remontante 
1. Forman 2002.
2. Mirowski 2011.

	
les théories fondamentales de la matière 	
333
qui visait à expliquer les comportements de la matière à un certain niveau 
à partir de ses composants au niveau inférieur. À la question : « Dans quelle 
mesure pouvons-nous reconstruire le monde à partir de la théorie effective 
la plus “fondamentale” connue à ce jour, à savoir le modèle standard ? », 
la réponse est que l’on n’est capable de prévoir l’existence ou de calculer 
les propriétés que d’un nombre très limité d’entités pouvant résulter des 
états liés de quarks et gluons.
Cela tient à une multitude de raisons, notamment la complexité des 
calculs nécessaires. Avec des approximations astucieuses (telle la formu-
lation de la chromodynamique quantique sur réseau), on peut calculer 
les propriétés de certaines particules hadroniques avec une précision 
impressionnante. Mais le calcul des propriétés du plus simple noyau 
composé (le deutéron, qui ne contient qu’un proton et un neutron) ou 
a fortiori de la structure des niveaux d’un noyau de bore ou de carbone 
par un calcul ab initio fondé sur le modèle standard n’est pas possible, 
même avec les ordinateurs les plus puissants. La même situation prévaut 
pour les propriétés des molécules composées d’un peu plus d’une dizaine 
d’atomes, basés sur l’équation non relativiste de Schrödinger qui régit 
le comportement des électrons et des noyaux interagissant par le biais 
des simples forces de Coulomb – la théorie « de base » pour le domaine 
atomique –, même s’il est vrai que la puissance des ordinateurs ou des 
approches récentes telles que la méthode des fonctionnelles de densité 
ont considérablement étendu la gamme des structures moléculaires qui 
peuvent être calculées et prédites. Ces nouveaux outils ont transformé 
des pans entiers de la physique de la matière condensée et de la chimie 
quantique, dont les chercheurs peuvent se retrouver indifféremment dans 
des départements de physique, de chimie ou d’informatique.
Cependant, plusieurs facteurs conduisent à mettre en question l’avenir 
à court terme de la physique des entités et interactions fondamentales 
dans ses formes actuelles. D’abord, le coût des équipements préoccupe 
à juste titre la puissance publique. À une échelle qui atteint la dizaine 
de milliards d’euros, il est légitime de se préoccuper de l’équilibre entre 
l’intérêt collectif de la société et l’intérêt propre de la science – quoique 
les procédures de décision de l’État en la matière ne soient actuellement 
guère gouvernées par les principes démocratiques. Toujours est-il que 
les arbitrages rendus par les gouvernements dans la situation de tension 
économique qui prévaut depuis les années 1980 ont conduit à diminuer 
sérieusement les efforts consentis en faveur de la big science. Un événement 
tout à fait significatif à cet égard fut l’abandon par le Congrès états-
unien fin 1993 du projet d’accélérateur géant SSC (Superconducting 
Super Collider) qui aurait surpassé le LHC du CERN. Ce projet fut jugé 

334	
silvan s. schweber et jean-marc lévy-leblond 
trop dispendieux quand son budget prévisionnel atteignit la dizaine de 
milliards de dollars – dont deux avaient déjà été dépensés.
Ensuite, les difficultés théoriques évoquées plus haut que rencontre  
la théorie des interactions fortes (chromodynamique quantique) quand 
il s’agit de mener à bien la résolution de ses équations de base, sèment le 
doute quant à la capacité de la théorie à rencontrer un succès aussi notable 
que l’électrodynamique quantique dans l’explication et la prédiction des 
phénomènes à son niveau de pertinence. Les tentatives en cours depuis 
deux décennies au moins pour tenter d’élargir ou de modifier ce cadre 
théorique ne semblent guère promises à un succès rapide malgré leurs 
dénominations ambitieuses (« supercordes », « supersymétries »). D’ail-
leurs, il se pourrait que la découverte du boson de Higgs ait été une sorte 
de chant du cygne de la physique fondamentale, dans la mesure où il est 
loin d’être certain que de nouvelles découvertes expérimentales soient à 
attendre dans la zone des énergies actuellement accessibles aux accélé-
rateurs les plus puissants existants ou en projet. C’est une idée assez 
répandue que celle de l’existence d’un large « désert » expérimental séparant 
ces zones d’énergie de celles auxquelles se manifesteraient de nouveaux 
concepts théoriques (unification globale des interactions par exemple).
Enfin, l’incapacité déjà ancienne des physiciens à concilier la théorie 
quantique des champs avec la relativité générale, autrement dit à traiter 
les forces de gravitation sur le même pied que les interactions nucléaires 
(fortes et faibles) et électromagnétiques, reste un sujet de profonde préoc-
cupation 1. De plus, l’apparition de phénomènes inattendus au niveau 
cosmologique, comme la possible existence d’une « matière sombre » 
inconnue qui constituerait la plus grande part du contenu matériel de 
l’Univers et serait faite d’entités pour l’instant totalement mystérieuses, 
ajoute aux interrogations concernant nos conceptions actuelles en physique 
fondamentale.
Le profond changement que laissent entrevoir ces évolutions n’est 
qu’un indice de ce qui advient de manière générale dans une science 
1. Les théories quantiques des champs invariantes sous les seules exigences de la relativité 
restreinte ont clairement une validité limitée parce qu’elles supposent que le cadre spatio-temporel 
dans lequel toutes les interactions ont lieu est rigide, et ne dépend pas des événements qui s’y 
déroulent. Mais il s’est avéré jusqu’à présent impossible de formuler une théorie quantique 
de la gravité, c’est-à-dire de formuler des théories qui articulent de manière cohérente la 
théorie de la relativité générale et la théorie quantique. Nous ne savons pas encore ni quand 
ni comment notre compréhension progressera. Il est clair en tout cas que toute théorie qui 
prétendra rendre compte de toutes les forces actuellement connues (y compris la gravitation) 
devra reproduire tous les résultats du modèle standard corroborés par les expériences, et avec 
le même degré de précision.

	
les théories fondamentales de la matière 	
335
dont l’activité est de plus en plus déterminée par des exigences externes, 
au premier chef la demande d’innovation technologique. Les pratiques 
sociales de la recherche qui se veut encore fondamentale sont d’ailleurs 
devenues de plus en plus collectives et spécialisées à la fois. Les chercheurs 
modernes exercent une profession au sein d’institutions rigides bien 
plus qu’ils ne répondent à une vocation personnelle. Alors que le lien 
entre leurs activités scientifiques et les questions de société est crucial, 
la formation des chercheurs et leurs conditions d’exercice leur donnent  
de moins en moins de possibilités de réflexion et d’action à ce niveau. 
Cela soulève la question de savoir si, dans ces conditions complexes et 
intriquées en termes de compétences, de connaissances et de consé-
quences, il est encore possible de prétendre à une expertise scientifique 
générale. Dans un autre domaine, les défis que soulève l’analyse scienti-
fique du changement climatique posent la même question.
Au fond, c’est la survie même d’une science fondamentale qui, sans 
abandonner l’espoir de se rendre utile, voudrait rester une activité spécu-
lative et désintéressée qui est en question. Il faut espérer que restent 
valables dans ce domaine les belles paroles de William James :
Dans notre vie active comme cognitive, nous créons. Nous ajoutons quelque 
chose aux deux parties de la réalité – au sujet comme au prédicat. Le monde 
est tout à fait malléable, il attend que nous lui apportions, de nos mains, les 
dernières touches. […] L’homme lui fait engendrer des vérités 1.
Traduction du texte de S.S. Schweber par Cyril Le Roy
Références bibliographiques
Essais généraux
Brading Katherine et Castellani Elena, 2003, Symmetries in Physics : Philoso-
phical Reflections, Cambridge, Cambridge University Press.
Close Frank, 2004, Particle Physics : A Very Short Introduction, New York et Oxford, 
Oxford University Press.
Cohen-Tannoudji Gilles et Spiro Michel, 1986, La Matière-Espace-Temps. La 
logique des particules élémentaires, Paris, Fayard.
–	 2013, Le Boson et le chapeau mexicain, Paris, Gallimard, coll. « Folio Essais ».
Dahan Amy et Pestre Dominique (dir.), 2004, Les Sciences pour la guerre (1940-
1960), Paris, Éd. de l’EHESS.
Du Sautoy Marcus, 2008, Symmetry : A Journey into the Patterns of Nature, New 
York, Harper’s ; trad. fr., La Symétrie, Paris, Seuil, coll. « Points Sciences », 2013.
Feynman Richard P., 1988, QED : The Strange Theory of Light and Matter, Princeton 
1. James 2007 [1907] (p. 268-271).

336	
silvan s. schweber et jean-marc lévy-leblond 
(NJ), Princeton University Press ; trad. fr., Lumière et matière, Paris, Seuil, coll. 
« Points Sciences », 1999.
Ford Kenneth W., 2004, The Quantum World : Quantum Physics for Everyone, 
Cambridge (MA), Harvard University Press.
–	 2011, 101 Quantum Questions : What You Need to Know about the World You Can’t 
See, Cambridge (MA), Harvard University Press.
Hooft Gerard ’t-, 1997, In Search of the Ultimate Building Blocks, Cambridge, 
Cambridge University Press.
James William, 2007 [1907], Le Pragmatisme, trad. de l’anglais par N. Ferron, Paris, 
Flammarion, coll. « Champs ».
Kragh Helge, 2002, Quantum Generations : A History of Physics in the Twentieth 
Century, Princeton (NJ), Princeton University Press.
Krige John et Pestre Dominique (dir.), 2014, Science in the Twentieth Century, 
Londres, Routledge.
Laughlin Robert, 2005, A Different Universe : Reinventing Physics from the Bottom 
Down, New York, Basic Books.
Lévy-Leblond Jean-Marc, 2000, « Une matière sans qualités ? (Grandeur et limites 
du réductionnisme physique) », in Luciano Boi (dir.), Science et philosophie de la 
nature, Berlin, Springer.
–	 2006, De la matière (relativiste, quantique, interactive), Paris, Seuil.
Mermod Ronald, 1999, De l’électron aux quarks. Une physique particulière, Lausanne, 
Presses polytechniques et universitaires romandes.
Mirowski Philip, 2011, Science-Mart : Privatizing American Science, Cambridge 
(MA), Harvard University Press.
Mirowski Philip et Plehwe Dieter (dir.), 2009, The Road from Mont Pelerin : 
The  Making of the Neoliberal Thought Collective, Cambridge (MA), Harvard 
University Press.
Riordan Michael, 1987, The Hunting of the Quark : A True Story of Modern Physics, 
New York, Simon & Schuster.
Rodgers Daniel T., 2011, Age of Fracture, Cambridge (MA), Harvard University 
Press.
Watkins Peter, 1986, Story of the W et Z, Cambridge, Cambridge University Press.
Études spécialisées
Boisot Max, Nordberg Markus, Yami Saïd et Nicquevert Bertrand (dir.), 2011, 
Collisions and Collaboration : The Organization of Learning in the ATLAS Experi­
ment at the LHC, Oxford et New York, Oxford University Press.
Brown Laurie M., Pais Abraham et Pippard Brian (dir.), 1995, Twentieth Century 
Physics, New York, American Institute of Physics Press, 3 vol.
Brown Laurie M. et Rechenberg Helmut, 1996, The Origin of the Concept of Nuclear 
Forces, Philadelphie (PA), Institute of Physics Publishing.
Cao Tian Yu, 2010, From Current Algebra to Quantum Chromodynamics, Cambridge, 
Cambridge University Press.
Chandrasekhar Subramanian, 1931, « The Maximum Mass of Ideal White 
Dwarfs », The Astrophysical Journal, vol. 74, p. 81.
Duncan Anthony, 2012, The Conceptual Framework of Quantum Field Theory, New 
York, Oxford University Press.

	
les théories fondamentales de la matière 	
337
Fitch Val et Rosner Jonathan, 1995, « Elementary Particle Physics in the Second 
Half of the Twentieth Century », in  L.M.  Brown, A.  Pais et B.  Pippard (dir.), 
Twentieth Century Physics, op. cit., vol. 2, p. 635-794.
Forman Paul, 2002, « Recent Science : Late Modern and Post-Modern », in Philip 
Mirowski et Esther-Mirjam Sent (dir.), Science Bought and Sold : Rethinking the 
Economics of Science, Chicago (IL), University of Chicago Press, p. 109-148.
–	 2012, « On the Historical Forms of Knowledge Productions and Curation : 
Modernity Entailed Disciplinarity, Postmodernity Entails Antidisciplinarity », 
Osiris, vol. 27, p. 56-100.
Gottfried Kurt et Weisskopf Victor F., 1986, Concepts of Particle Physics, Oxford, 
Clarendon Press, 2 vol.
Leggett Anthony J., 1995, « Superfluids and Superconductors », in L.M. Brown, 
A. Pais et B. Pippard (dir.), Twentieth Century Physics, op. cit., vol. 2, p. 913-966.
Leite Lopes José et Escoubès Bruno, 1997, Sources et évolution de la physique 
quantique. Textes fondateurs, Paris, Elsevier Masson.
Mills Robert, 1989, « Gauge Fields », American Journal of Physics, vol.  57, no  6, 
p. 493-507.
Nambu Yoichiro, 1981, Quarks : Frontiers in Elementary Particle Physics, Singapour, 
World Scientific.


16 Modèles.  
De la représentation à l’action
M i c h e l  A r m at t e  
e t  A m y  D a h a n
Dans le langage courant, la notion de modèle est polysémique et désigne 
aussi bien un prototype (maquette, matrice, moule) préfigurant une 
collection d’objets ou un type idéal qu’une représentation (iconique ou 
formelle) construite a posteriori 1. Le terme a pris un sens plus précis dans 
le domaine des sciences au début du xxe siècle, au moment où les modèles 
deviennent des outils de la compréhension du monde et des moyens d’agir 
sur lui. Ils associent alors trois dimensions habituellement dénommées 
syntaxique, sémantique et pragmatique. Aujourd’hui, il est impossible 
de traiter des modèles de façon décontextualisée, hors du temps et des 
lieux de savoir, de décision et d’action. Cela fut le défaut d’une approche 
philosophique plus normative que descriptive des pratiques de science 2.
En guise d’introduction, nous esquisserons d’abord une histoire générale 
des modèles. Nous n’entrerons pas par types logiques mais plutôt par la 
place occupée par ces modèles dans les régimes d’expertise. Au début 
du xxe siècle, l’épistémologie considère les modèles comme de simples 
dispositifs cognitifs utiles au processus de connaissance. Le cadre est celui 
d’une science unifiée et réductionniste et seules les propriétés linguistiques 
et algébriques des modèles sont traitées. La Seconde Guerre mondiale 
marque un tournant du fait de l’extension des objets scientifiques, de leur 
complexité croissante, de l’hétérogénéité des approches, de la prolifération 
de théories et outils nouveaux d’analyse et de représentation (cela va des 
mathématiques appliquées à l’informatique). Depuis les années 1980, le 
point essentiel est l’inscription des sciences et de leur outillage dans une 
1. Bachelard 1979.
2. Armatte et Dahan 2004.
 Le « Bretherton Diagram » représentant le « système Terre », 1986.

340	
michel armatte et amy dahan 
ingénierie et des innovations d’ampleur inédite, et qui mettent les sciences 
au service de projets de transformation et d’adaptation de l’homme à 
son environnement. Le contrôle, la régulation et la gestion des risques 
sont des enjeux majeurs, et les modèles deviennent à la fois des outils de 
découverte, de preuve, de management et de gouvernement. Les décrire 
et les analyser exige donc de développer une sociologie situant chaque 
modélisation dans un réseau d’acteurs, d’enjeux et de stratégies, dans 
une histoire qui la rattache à un certain régime d’expertise, une manière 
d’articuler sciences et sociétés, savoirs et pouvoirs. Nous déclinerons 
cette histoire à travers des exemples pris dans diverses disciplines, illus-
trant ainsi tant la diversité que l’existence d’un régime « commun » de 
la modélisation. Dans une seconde partie, nous nous attarderons sur, et 
analyserons plus avant, deux domaines d’importance, celui des sciences 
économiques et celui des sciences du climat.
La modélisation avant 1930, quand physique  
et logique mathématique fixent le cadre
Au xixe siècle, la notion de modèle est synonyme d’une « mathémati-
sation du réel », selon l’expression de Giorgio Israel 1. Elle est à l’œuvre dans 
la science moderne quand géomètres et philosophes naturels délaissent 
l’approche scolastique pour l’étude du Livre de la Nature écrit en langage 
mathématique. Une caractéristique de cette mathématisation précoce 
est d’avoir drastiquement simplifié la description pour que la mathéma-
tisation, souvent linéaire ou différentielle, en fournisse une illustration 
correcte. Pour mathématiser un phénomène comme la chute des corps, 
il faut, disait Galilée, « défalquer les empêchements », faire abstraction 
des frottements et des aléas, et reconstruire un réel idéalisé – comme 
dans les exemples du plan incliné ou du pendule qui ont si souvent servi 
de modèles des modèles.
La terminologie et la notion scientifique de modèle sont concomitantes 
d’une nouvelle approche de la physique, celle de Maxwell, poursuivie 
par Kirchhoff et Hertz, et dont témoigne l’article « Model » de l’Encyclo-
paedia Britannica, signé de Ludwig Boltzmann en 1902. Maxwell écrit 
en 1870 que
la reconnaissance de l’analogie formelle entre deux systèmes d’idées – avec 
des quantités physiques correspondantes appartenant vraiment à la même 
1. Israel 1996.

	
le siècle des modèles	
341
classe mathématique – conduit à une connaissance des deux systèmes plus 
profonde que celle qui pourrait être obtenue quand on les étudie séparément.
Cette redéfinition du « modèle » s’articule sur une coupure vis-à-vis 
du mécanisme et du réalisme. La notion de modèle reste centrale dans 
l’histoire de la mécanique statistique, en particulier dans les travaux de 
Gibbs, Einstein, Ehrenfest et Borel.
Cette épistémologie des modèles qui émerge en physique est relayée 
par les travaux des logiciens. Russell et Whitehead, dans leurs Principia 
(1910-1913), puis Hilbert dans les années 1920, proposent de fonder 
logiquement l’ensemble des mathématiques en établissant des règles 
de construction et de dérivation de formules indépendantes de leur 
signification mathématique. La notion principale de cette métamathé-
matique est celle de système formel qui articule un alphabet, des règles 
syntaxiques, des règles d’inférence et des axiomes. La théorie est alors 
la suite des formules qui résultent des axiomes et des démonstrations 
(les théorèmes). Cette réflexion se prolonge dans les travaux du Cercle 
de Vienne des années 1930 visant à faire de toute science un langage 
formel contrôlé par sa seule syntaxe 1 . Après le coup de tonnerre des 
théorèmes d’impossibilité de Gödel, cette piste est abandonnée au profit 
d’une version sémantique de la notion de modèle, où la notion d’inter-
prétation devient centrale, développée en particulier par Tarski 2. Cette 
vision des modèles comme représentation perdure peu ou prou jusqu’à 
nos jours dans la philosophie des sciences, avec néanmoins quelque 
distance par rapport à l’idée que cette représentation soit réaliste. Barbe-
rousse et Ludwig (2000) les caractérisent comme des fictions construites, 
Morgan et Morrison (1999) en font des médiateurs autonomes entre les 
deux formes, théorique et empirique, sous lesquelles le monde « réel » 
serait appréhendé.
Dans cette première période, le modèle est donc principalement un 
objet épistémologique qui s’inscrit dans une science unifiée marquée par 
une méthode cartésienne de décomposition de tout système en parties. 
Simultanément, toutefois, apparaît une version empirique de la notion de 
modèle (model of data) qui associe au système étudié un jeu de mesures 
quantitatives évaluées par des dispositifs de mesure ou de comptage statis-
tique. De telles approches ont pu revendiquer un statut d’objectivation 
des faits et donner ainsi naissance, au début du xxe siècle, à un grand 
nombre de « métries » (anthropométrie, psychométrie, économétrie…), 
1. Carnap, Hahn et Neurath 1985 [1929]
2. Sinaceur 1999.

342	
michel armatte et amy dahan 
ou s’ériger en instances de vérification inductive de la théorie. Dans les 
sciences du vivant, Ronald Fisher inaugure le lien entre statistique et 
génétique moderne et fonde la notion de modèle en statistique mathé-
matique. Son approche des plans d’expériences et d’essais randomisés 
sera mobilisée plus tard à grande échelle dans la recherche agronomique 
et en épidémiologie.
Giorgio Israel (1996) fait du modèle des battements du cœur de Van 
der Pol le prototype même de cette nouvelle génération de modèles qui 
rompent, dans les années 1920, avec la mathématisation du réel de Galilée 
et Descartes. En effet, ce modèle n’est pas fondé sur une description 
mathématique des caractéristiques essentielles de l’objet établies expéri-
mentalement, mais sur une forme mathématique commune à des objets 
différents mais isomorphes. L’analogie mathématique avec des systèmes 
électriques se substitue à l’analyse propre du système pour générer un 
modèle formel. À la même période (autour de 1925), Volterra et Lotka 
jettent les bases d’une dynamique des populations, dont les équations 
s’écrivent sous la forme d’équations différentielles, relatives à l’équilibre 
des effectifs proies-prédateurs. Dans les années 1920-1950, la méthode 
des modèles formels s’impose à toute la biologie quantitative à partir 
de quatre lieux distincts 1 : la biométrie, la biologie des populations, la 
biologie mathématique et la biocybernétique. Dans tous ces domaines, 
les modèles ne sont jamais « objectifs » ou « vrais », et ils ne se donnent 
pas comme des représentations fidèles et réalistes. Legay (1997) donne 
ainsi l’exemple d’une étude sur la bilharziose qui peut se faire de trois 
points de vue distincts : médical, zoologique, écologique. Ils sont préfor-
matés par les outils cognitifs (catégories, conventions) ou matériels 
(instruments d’observation), et par la question spécifique que l’on a 
choisi de traiter et qui impose un découpage particulier de la réalité.  
La biologie expérimentale prolonge la notion de modèle comme analogie 
en désignant sous ce terme des entités physiques comme des machines, 
des animaux (la souris, la drosophile) ou des dispositifs expérimentaux 
qui représentent les entités et systèmes étudiés. On parle alors de modèles 
physiques par opposition aux modèles dits dialectiques, fondés sur la 
notion de langage formel.
Dans ce régime de production des savoirs 2, la « vraie science » a peur 
d’une contagion du politique, synonyme d’ingérence d’acteurs ou de 
groupes porteurs d’intérêts particuliers ; elle se définit comme désin-
téressée, un terme qui figure dans la plupart des discours savants du 
1. Varenne 2007.
2. Pestre 2003.

xixe siècle. Cette séparation entre science et politique résistera mal à la  
poussée technocratique des années 1920 et 1930, et encore moins à  
la période de la guerre et de la guerre froide.
Quand la Seconde Guerre mondiale  
inaugure un bouleversement durable des pratiques
La crise financière puis économique des années 1930 provoque un 
grand nombre de réactions des milieux savants (X-Crise en France, le 
séminaire Menger à Vienne, la fondation de la Société économétrique  
aux États-Unis) et des milieux politiques qui s’engagent dans des politiques 
de régulation étatique inspirées de la nouvelle théorie keynésienne et 
du New Deal. La Seconde Guerre mondiale est d’un plus grand impact 
encore avec la mobilisation des élites savantes au service d’un nouveau 
complexe universitaire et militaro-industriel dont le programme est 
dessiné dans le rapport de Vannevar Bush 1. Les scientifiques se retrouvent 
à travailler ensemble sur des projets de contrôle et d’optimisation d’opé-
rations militaires, dans l’urgence et avec obligation de résultats, mais 
aussi avec des moyens financiers et matériels inédits – dont les premiers 
ordinateurs : analogiques (MIT), digitaux (l’ENIAC de von Neumann) ou 
modèles « à opérateurs » (McCulloch et Pitts, Couffignal). Trois ensembles 
de questions sont à l’origine de ces nouvelles pratiques et conduisent 
après guerre à des dynamiques de recherche puissantes :
–	 les problèmes de mécanique des fluides, d’ondes de choc, de théorie 
des explosions, de balistique sous-marine ; ces problèmes se situent au 
carrefour de l’hydrodynamique et de l’analyse numérique, et concernent  
la météorologie et la dynamique de propagation des explosions nucléaires ;
–	 le deuxième ensemble est issu des problèmes de recherche opéra-
tionnelle et de logistique (en particulier de défense anti-aérienne ou 
de contrôle de fabrication), de problèmes économiques (répartition de 
ressources), et conduit à un ensemble varié de disciplines construites 
autour d’activités de modélisation (statistique mathématique, théorie 
des jeux, programmation linéaire ou stochastique) ;
–	 le troisième ensemble, résumé sous les termes de cybernétique et 
d’autorégulation, est issu de travaux sur des systèmes de défense et de 
communication comportant hommes et machines (Norbert Wiener), et 
d’un effort de modélisation logique du cerveau (von Neumann, McCulloch 
et Pitts).
1. Dahan et Pestre 2004.
	
le siècle des modèles	
343

344	
michel armatte et amy dahan 
John von Neumann joue un rôle majeur dans ces trois domaines. Il 
symbolise une figure nouvelle de mathématicien intervenant dans les 
choix politiques, militaires et technologiques, très proche du pouvoir 
au plus haut niveau. Avec von Neumann, les modèles ont d’emblée ce 
caractère transférable et polymorphe propre à la méthode des mathéma-
ticiens. Selon lui, la science ne tente pas d’expliquer, à peine d’interpréter, 
elle fabrique principalement des modèles dont on attend surtout qu’ils 
fonctionnent. Il en résulte une explosion des mathématiques appliquées 1 
et de disciplines à base d’optimisation (programmation linéaire, graphes, 
programmation dynamique) appliquée aux questions de transport, 
gestion de la production ou analyse d’activités. Il faut aussi mentionner le 
rôle crucial de l’ingénierie de communication (électrique, téléphonique, 
servomécanismes) et de contrôle 2, celui de la cybernétique avec la figure 
tutélaire de Wiener, qui articule feedbacks, non-linéarité, boîtes noires, 
théorie des systèmes et automates, auxquels s’ajouteront ceux du chaos 
déterministe et des systèmes dynamiques 3.
Dans cette matrice de changements, les modèles deviennent des instru-
ments du développement scientifique, mais également des outils d’action. 
La prévision, la prospective, l’aide à la décision, la simulation sont au 
centre des nouveaux usages. L’ordinateur permet le calcul numérique 
sur des données en grande quantité, en complément et validation de la 
modélisation théorique (analyse de données, estimation de paramètres, 
tests d’hypothèse). Ainsi, la modélisation stimule la quantification sur 
laquelle elle s’appuie. Là où dominait auparavant une dualité, l’articulation 
théorie / observation devient possible et trouve de nouveaux outils, en 
économétrie par exemple. L’irruption de la théorie des jeux, en économie 
ou en biologie, permet à la modélisation de faire émerger la notion de 
stratégies. L’ordinateur sert aussi de substitut à l’analyse mathématique 
par la simulation numérique (méthode Monte-Carlo), et de substitut à 
l’expérience physique par l’expérimentation virtuelle (bombe thermonu-
cléaire). Les règles algorithmiques prennent un peu partout le pas sur  
les lois mathématiques. Quelques années plus tard, vers 1980, des champs 
scientifiques entiers verront, avec la simulation numérique, leur statut 
épistémologique basculer : c’est le cas de la turbulence bidimensionnelle, 
de la paléoclimatologie, de l’étude des atmosphères planétaires ou celle 
de l’évolution des galaxies. De sciences de pure observation, elles passent 
au statut de sciences d’expérimentation numérique.
1. Dahan 1996 et 2004.
2. Bennett 1993, Bissell 2004.
3. Aubin et Dahan 2002.

Cette deuxième phase de la modélisation, que l’on caractérise comme 
celle de la simulation numérique 1, rompt avec l’épistémologie des modèles 
formels, l’idée de représentation homothétique du réel saisi au prisme 
d’une science unifiée, et s’accommode d’une multiplicité d’approches, 
intérêts et objectifs stratégiques. Toutefois, le tournant méthodologique 
que l’ordinateur fait prendre aux pratiques scientifiques et à la conception 
des modèles est loin d’être uniforme. Il suscite des résistances – voir la 
controverse, en météorologie, entre les modèles pour prédire et les modèles 
pour comprendre 2 – et coexiste avec une démarche d’axiomatisation et 
une conception plus rigide de « modèles-structures », persistante chez 
les économistes. Une distinction émerge ici entre modèles de connais-
sance, fondés sur des lois connues, et modèles de simulation ou d’action 
qui doivent se satisfaire d’un système complexe, dont les lois physiques 
sont inconnues, et construits comme des boîtes noires censées reproduire 
des comportements par comparaison des données d’entrée et sortie. La 
simulation permet de se substituer à des expériences multiples coûteuses, 
de démêler un jeu complexe de régulations ou d’amplifications possibles 
des effets de chaque facteur (rôle heuristique) ou pour agir sur le système 
et mettre en œuvre des procédures de contrôle optimal et de gestion de 
son évolution. Toutefois, ces deux types de modèles (connaissance et 
action) cohabitent et donnent le plus souvent lieu à des hybrides.
Le régime d’expertise de cette seconde période relève d’un schéma 
linéaire : la décision politique se fonde avantageusement sur les recherches 
des scientifiques et les chercheurs y trouvent beaucoup d’avantages à 
condition que l’on respecte leur autonomie initiale.
Quand les modèles deviennent omniprésents  
comme outils d’expertise et de gouvernement des risques
Une troisième période s’ouvre à la fin des années 1970 et au début  
des années 1980. Ce moment est marqué par les deux crises pétrolières 
et la transition entre le régime de croissance des Trente Glorieuses et le 
nouveau régime de stagflation qui en résulte. La crise économique est 
aussi une crise écologique dont les marqueurs sont le rapport du Club 
de Rome sur les limites de la croissance 3 et l’émergence des questions 
environnementales au niveau onusien (sommets de la Terre de Stockholm, 
1. Sismondo 1999.
2. Dahan 2001.
3. Meadows 1972.
	
le siècle des modèles	
345

346	
michel armatte et amy dahan 
Nairobi et Rio 1992). Le tournant économique s’accompagne d’un tournant 
idéologique : fin du compromis fordiste, recul du keynésianisme au 
profit des monétaristes puis des nouveaux classiques avec l’argument 
des anticipations rationnelles, déconsidération de l’État entrepreneur 
et de l’État-providence ; et d’un tournant financier : la toute-puissance 
de l’entrepreneur et des gestionnaires est mise sous la coupe des action-
naires et des marchés financiers qui s’enrichissent de produits financiers 
nouveaux (les options). Le cadre est celui du tournant politique néolibéral 
induit par la mondialisation des échanges, les mandatures de Thatcher 
et Reagan et le consensus de Washington. Enfin, les marchés profitent 
largement d’un nouveau moteur, les TIC qui permet une unification des 
technologies de communication et offre de nouvelles opportunités de prise 
de données, de processus de calcul, et de diffusion : le système d’obser-
vation satellitaire de la Terre, par exemple, qui alimente continûment  
des modèles de données qu’il faut traiter et assimiler pour les faire entrer 
en input d’autres modèles 1.
Ces conditions inaugurent un nouveau régime de science (privatisations, 
extension de la propriété intellectuelle) et un nouveau régime d’expertise 
dans lequel le cadrage politique, la délégation aux agences, la gestion 
des crises et des risques et le besoin de gouvernance à diverses échelles 
définissent les programmes de recherche. Dans cette troisième phase, 
la modélisation s’adapte à la fois aux nouveaux défis (crises écologiques 
et sanitaires) et aux nouveaux outils. Un nouveau rôle s’impose, celui 
d’une intégration de savoirs hétérogènes et interdisciplinaires au sein d’un 
instrument informatique agrégateur pour traiter de problèmes globaux 
et aider à la décision. La modélisation d’objets restreints dans le cadre  
de sciences disciplinaires ne s’interrompt pas pour autant, en particulier 
en sciences des matériaux, nanotechnologies ou neurosciences. Toutefois, 
la complexité croissante des objets de science, définie formellement par 
des feedbacks et des non-linéarités, est l’élément novateur des pratiques 
scientifiques de modélisation. Cette complexité caractérise les systèmes  
et phénomènes qui agglutinent des éléments hétéroclites (humains et non 
humains), des échelles spatiales différentes (de la molécule à la planète) 
et des échelles temporelles dispersées. Dans les sciences de l’ingénieur 
ou la gestion locale de l’environnement, appelées joliment des « scien-
cettes » non poppériennes 2, la modélisation devient aussi un instrument 
de dialogue technique et social.
Parmi ces modèles qui associent du naturel, de l’humain et du culturel, on 
1. Edwards 2010.
2. Bouleau 2002.

peut citer les modèles de transport et de flux de population dans une ville 1, 
les modèles de gestion de l’eau, le modèle Rains de gestion européenne 
des pollutions atmosphériques et pluies acides 2 ou IMPACT (Interna-
tional Model for Policy Analysis of Agricultural Commodities and Trade), 
un modèle économique prospectif de l’agriculture mondiale 3. Le système 
économique mondial et ses sous-sytèmes (énergie, transport, industrie, 
agriculture), la biosphère et ses biotopes, la biodiversité ou le climat sont 
autant de domaines de modélisation possédant cette caractéristique  
de complexité hétéroclite. Celle-ci induit la nécessité de conjuguer une 
multiplicité de savoirs ayant des épistémologies et des statuts différents 
et impliquant différentes cultures et communautés (experts, décideurs 
politiques, société civile, gens concernés…). L’évolution du modèle est 
alors souvent dépendante de la survie du réseau des acteurs qu’il fédère 
ou de sa capacité à répondre aux besoins politiques des décideurs. Plus 
que jamais, le modèle devient un agrégateur de cultures épistémiques, 
et donc un outil plus ou moins partagé de gouvernement : la simulation 
prend le dessus sur la représentation, sort du réalisme et s’ajuste aux 
nécessités du contrôle des systèmes.
La notion de modèle apparaît dès lors comme un outil essentiel dans 
l’articulation entre science et action, selon un troisième régime d’expertise 
dans lequel la mise à l’agenda politique de questions nouvelles condi-
tionne et formate la convocation des scientifiques. Le modèle devient 
moins important que la modélisation comme activité de projection 
dans un futur possible et souhaitable. Ces nouvelles formes de modéli-
sation posent des questions inédites et les deux exemples que nous allons 
détailler témoignent de ce caractère hybride.
La modélisation en économie 4
Le terme même de « modèle » apparaît chez les statisticiens (à la suite 
de Fisher et Wald) et chez les économistes, réunis dans la nouvelle Société 
d’économétrie. Ceux-ci s’inspirent des modèles d’oscillateurs des ingénieurs 
ou écrivent directement, comme Tinbergen le fait pour la Société des 
Nations, des modèles d’une vingtaine d’équations inspirées des égalités 
de la comptabilité nationale naissante et des déterminations hypothé-
tiques des grands agrégats (consommation, investissement…). Keynes 
1. Bazzani et al. 2003.
2. Kieken 2004.
3. Cornilleau et Leblond 2012.
4. En complément, voir Shenk et Mitchell (dans ce volume, p. 233).
	
le siècle des modèles	
347

348	
michel armatte et amy dahan 
(1939) s’offusquera de cette « cuisine statistique ». Si les modèles formels 
lui semblent utiles à des raisonnements qualitatifs, il juge téméraire et 
inconscient de faire reposer les analyses sur les estimations numériques 
d’un grand nombre de paramètres à partir d’une information statistique 
mince (11 années) et peu fiable. Les travaux de la Commission Cowles 
vont néanmoins accoucher, avec Haavelmo (1944), d’une version intégrée 
des modèles qui combine la notion de modèle mathématique de type 
walrassien adossé à une théorie, et l’idée d’un modèle aléatoire, qui permet 
de confronter théorie et données dans une optique vérificationniste ou 
réfutationniste. Les équations sont des traductions directes de relations 
économiques de comportement dites autonomes inspirées d’hypothèses 
théoriques concernant par exemple le niveau de l’investissement ou 
celui du chômage. Les aléas introduits explicitement dans les équations 
traduisent à la fois des erreurs de mesure et de spécification (variables 
oubliées), des effets d’échantillonnage par la nature et par l’observateur, 
et des résultats de décisions et de comportements individuels différenciés 
mais inobservables. La nature hybride de ces aléas est problématique si 
l’on veut analyser les incertitudes, mais elle autorise des tests statistiques, 
ce qui ouvre des perspectives en matière de réfutation : tester une théorie 
économique, c’est tester statistiquement les paramètres de son modèle. 
Ce principe n’a pu se traduire en innovation méthodologique effective 
qu’après de nombreuses discussions et innovations techniques 1.
Dans le contexte keynésien de l’après-guerre, la modélisation macro­
économétrique aura des objectifs plus pragmatiques : fournir des prévisions 
d’évolution à court terme des économies nationales ou fournir, par 
simulation de variantes d’un même modèle, des projections à moyen 
terme dans le cadre d’une planification indicatrice (pour les Pays-Bas  
ou la France). Le cas exemplaire de la relation de Phillips, représentant 
dans ces modèles la mécanique du bouclage prix-salaires-emploi, illustre 
ce double rôle des modèles : validation ou invalidation théorique de cette 
relation âprement disputée entre keynésiens et monétaristes d’une part, 
outils d’arbitrage d’autre part entre inflation et emploi ou patrons et 
salariés dans le cadre de politiques économiques.
Ce paradigme de la Commission Cowles s’effondre au milieu des années 
1970 à la suite de la crise pétrolière et monétaire, du changement de régime 
(stagflation) et des critiques monétaristes qui disqualifient la relation 
de Phillips 2. Aux méthodes de la prévision, on commence à préférer les 
processus multivariés, efficaces dans le court terme, les modèles d’équilibre 
1. Morgan 1990, Armatte 2004 et 2010.
2. Malgrange 1990.

général calculables mobilisés dans les opérations d’ajustement struc-
turel par la Banque mondiale. Des méthodes différentes de prospective à  
long terme font leur apparition, plus souples, fondées sur des driving 
forces bien choisies, et intégrant des changements brutaux anticipés ou 
voulus par certains scénarios politiques. La méthode des scénarios est 
expérimentée dans les grandes entreprises, les administrations, au Club 
de Rome et dans quelques instituts américains liés à la guerre froide.  
Dans les années 1980, le cadre général devient celui d’un néolibéralisme 
pour lequel l’État privé de ses fonctions régulatrices n’a plus besoin  
de modèles de prévision et de planification. S’y substituent des outils 
d’évaluation a posteriori des politiques publiques comme le benchmarking 
(LOLF en France, MOC en Europe…) ou l’économie expérimentale  
randomisée.
C’est la globalisation financière qui formate désormais l’ensemble de 
la sphère économique, et le rôle des modèles dans cette conquête est 
considérable. La préhistoire de la finance avait donné lieu chez Bachelier 
(1900) aux premières modélisations probabilistes des cours boursiers par 
un processus brownien continu. La théorie des portefeuilles optimaux 
de Markovitz et Scharpe, sur la base d’hypothèses semblables, proposait 
dans les années 1960 de rapporter le rendement d’une action à celui du 
marché dans son ensemble, en s’appuyant sur les indices du type Dow 
Jones et CAC 40. Leur modèle a ouvert l’ère nouvelle de la gestion indiciaire 
passive des portefeuilles d’actions. L’apparition des marchés à terme, 
c’est-à-dire d’un contrat d’échange permettant de se couvrir contre un 
risque portant sur un futur incertain, a un grand impact à la fois sur le 
montant des sommes engagées et sur la rapide diffusion des risques. Cette 
innovation touche à la fois les produits, les mécanismes, les théories, les 
modèles et les institutions. La théorie de la formation des prix des options, 
due à Black, Scholes et Merton en 1973, était tout à fait révolutionnaire  
dans sa méthode de couverture dynamique à l’aide d’un portefeuille 
d’actions et d’obligations suivant le prix d’une option 1. Elle a non seulement 
été validée pour rendre compte des marchés qui se sont ouverts (1986 
pour le MEDAF en France), mais elle a aussi permis la multiplication de 
ces marchés, leur configuration et régulation par des critères de mesure 
des risques (value at risk) établis en filiation directe des hypothèses proba-
bilistes des inventeurs, et la mise en œuvre, dans les salles de marchés, 
de programmes de trading calqués sur leur formule. La crise boursière  
de 1987 et celle du fonds de placement LTCM (dont Merton et Scholes 
sont partie prenante) en 1998 ont révélé les caractéristiques particulières 
1. Bouleau 1999.
	
le siècle des modèles	
349

350	
michel armatte et amy dahan 
de cette modélisation 1. Bien que l’hypothèse de marche aléatoire continue 
ait été validée sur la période précédente par une bonne adéquation avec 
les prix observés, elle s’est montrée complètement inopérante pendant 
la crise. À cela plusieurs raisons.
Les hypothèses de normalité des cours ont été contestées par Mandelbrot 
dès les années 1960, une contestation prolongée dans les années 1990 par 
des arguments théoriques (les cours ne résultent pas de chocs aléatoires 
homogènes et indépendants), des tests empiriques, des propositions visant 
à remplacer cette loi par une loi de Pareto-Lévy faisant une plus large 
place aux risques élevés, et par une remise en cause de l’indépendance et 
de la continuité du processus. Les auteurs de ces travaux (Walter, Taleb) 
ont proposé de parler dans ce cas d’un risque de modèle, s’ajoutant au 
risque dont traite le modèle.
Au-delà de cette critique technique, la principale leçon de cette crise 
est l’assimilation abusive du système financier à un système physique 
pour lequel la théorie et le modèle ne changent pas l’objet « naturel ». La 
théorie financière moderne (contrairement à une théorie physique) fait 
partie intégrante de la réalité car elle décrit des institutions, des croyances 
et des actions humaines qu’elle contribue à façonner. C’est l’argument de 
la performativité des théories et des modèles, mobilisé par MacKenzie 
pour rendre compte de la crise du LTCM et repris par Callon 2 : la théorie 
financière moderne configure la réalité pour qu’elle ressemble à la théorie. 
La théorie des options a fait l’hypothèse d’un marché efficient et elle a 
introduit des pratiques de trading qui, en se multipliant et se répétant  
à grande fréquence, ont rendu le marché liquide et efficient. On est 
proche de la prophétie autoréalisatrice qui dans le cas général peut être 
considérée comme favorable à la théorie aussi bien qu’à la stabilité des 
institutions mises en place. La notion de prophétie autoréalisatrice a été 
« inventée » par le père de Merton au sujet de rumeurs de banqueroute 
qui suffisent à déclencher une ruée dans les banques et provoquent la 
banqueroute réelle. Dans ce cas, la performativité est plutôt contre- 
productive, pathologique et perverse. Dans le cas du LTCM, l’efficience 
des marchés, favorable au fonds depuis 1993, s’est transformée en un grave 
défaut après l’été 1998 en propageant très vite des signaux interprétés à 
tort comme des vrais prix. Un marché financier est une création humaine 
dont la vie dépend de millions de décisions humaines (d’achat et de vente) 
et dont le fonctionnement est donc dépendant du contexte social dans 
lequel il s’inscrit. Les hypothèses du modèle n’étaient pas fausses mais 
1. MacKenzie 2000.
2. Au sens de Callon 2007.

seulement inadaptées au contexte, et sensibles aux décisions prises par 
les agents au moment critique qui n’ont fait que renforcer la bulle. Les 
hypothèses d’efficience des marchés ont entraîné une explosion du trading 
motivé par les occasions d’arbitrage qui se multiplient dès lors que les 
individus et les institutions interagissent. Les marchés financiers sont par 
ailleurs largement faussés par des ententes illicites, des contrôles par des 
agences non indépendantes, des pressions de lobbies variés – des pratiques 
qui relèvent du crime organisé ou des chocs exogènes (les défauts de la 
dette russe, les vacances des traders en août, le jeu des banques utilisant 
le même modèle). Les décisions individuelles des traders sont prises dans 
des jeux de croyances et de confiance, des phénomènes de mimétisme, des 
stratégies collectives qui ont, comme pour la monnaie, un rôle essentiel.
La titrisation des dettes et la crise des subprimes qui a suivi en 2007-2008 
offrent d’autres exemples d’interactions destructrices. Les modèles 
de marchés sont des constructions sociales prises dans des jeux de 
feedbacks complexes avec l’objet qu’ils modélisent. Prendre le modèle 
pour la réalité n’est pas correct a priori mais la sociologie de la finance  
nous confirme que les acteurs le font facilement. Le domaine de la  
finance offre ainsi des vues saisissantes sur la complexité des liens entre 
modèle et réalité mais aussi du statut de l’expertise mathématicienne, 
tour à tour enrôlée, adulée puis montrée du doigt.
Le changement climatique :  
modèles, simulations, scénarios
La question du changement climatique (CC), après avoir été posée 
par un certain nombre de lanceurs d’alerte, a donné lieu dans les années 
1980 à une gouvernance politique mondiale par le biais d’une convention 
onusienne (UNFCCC) dont les conférences annuelles (COP) sont très 
suivies, mais aussi à une gouvernance scientifique singulière par le biais 
du Groupe intergouvernemental sur l’évolution du climat (IPCC en 
anglais) chargé de produire des rapports de synthèse des publications 
scientifiques du domaine.
L’outil principal d’évaluation scientifique est celui de la modélisation 
globale, c’est-à-dire d’une modélisation d’ensemble des caractéristiques 
du climat (températures, précipitations, pressions, etc.) et des facteurs de 
son évolution. Les modèles des climatologues (global circulation models) 
sont fondés sur différentes lois empruntées à la mécanique des fluides 
(équations de Navier-Stokes), à la physique des radiations (effet de serre) 
et à la chimie, et se caractérisent par des échelles de temps et d’espace 
	
le siècle des modèles	
351

352	
michel armatte et amy dahan 
multiples et imbriquées, et par des compromis entre méthodes statistiques 
et déterministes. Toutes ces connaissances scientifiques de l’atmosphère, 
étendues ensuite aux couplages avec les océans et les calottes glaciaires, 
interviennent dans des modèles numériques qui fournissent, pour chaque 
maille à trois dimensions de l’atmosphère (latitude, longitude, altitude),  
les valeurs moyennes au temps t des variables climatiques. Évidemment, 
ces modèles ne peuvent prendre en compte tous les phénomènes physiques 
en jeu, et une bonne partie de ceux-ci donnent lieu à des paramétri-
sations qui confèrent aux modèles un double caractère théorique et 
empirique. D’ailleurs, ces paramétrisations sont souvent des « petits 
modèles » au sein du modèle global. De plus, pour utiliser et valider les 
modèles globaux, les scientifiques ont besoin d’autres outils et ils font 
appel à une pléiade de modèles intermédiaires (qu’ils dénomment leur 
« laboratoire de modèles ») qui entrent dans des protocoles complexes 
d’intercomparaisons. Les modèles globaux, portés chacun par un groupe 
de laboratoires, fonctionnent comme de gros simulateurs chargés d’agréger 
des connaissances de statut épistémologique fort différent, et de repro-
duire dans leurs grandes lignes les climats passés et actuels.
Le maître mot est ici celui de « scénario ». Les scénarios expriment, 
par des prévisions numériques et par des récits (les story lines) qui en 
assurent la cohérence spatiale et temporelle, les hypothèses retenues 
quant au forçage du système climatique par les perturbations anthro-
piques. C’est donc à d’autres communautés scientifiques que celle des 
sciences du climat – les communautés des économistes, des sociologues, 
des biologistes (ces derniers s’intéressant aux interactions entre cycles  
du carbone et couvertures végétales et forestières) – que revient la charge 
d’élaborer de tels scénarios. Ceux qui furent établis par le GIEC, fondés 
sur des critères variables d’ouverture et de résilience des économies, ont 
fourni un cadre à toutes les simulations effectuées pour les quatre premiers 
rapports du GIEC. Les scénarios sont injectés en input de modèles dits 
d’évaluation intégrée pour évaluer l’effet de diverses politiques écono-
miques et environnementales.
Les modèles d’évaluation intégrée (integrated assessment models, IAM) 
sont en général à structures modulaires (comme le modèle IMPACT de 
l’alimentation mondiale) ; ils intègrent un module démographique, des 
modules économiques généraux, des modules sectoriels et des modules 
climatiques sommaires. Leur construction combine des approches 
top-down, des approches bottom-up, et une forme issue de la dynamique 
des systèmes à l’œuvre dans le modèle World 3 du Club de Rome 1. Les 
1. Armatte 2007.

IAM servent aussi à une expertise des impacts et des politiques de 
réduction ou d’adaptation, expertise relevant d’un schéma logique linéaire 
et séquentiel de type PSIR ( pression, state, impact, responses) calqué 
sur la structuration même du GIEC en trois groupes. Ils peuvent être 
associés à d’autres outils de calcul économique de type coût-bénéfice, 
comme le rapport Stern (2006) l’a illustré. Ce qui caractérise une telle 
modélisation intégrée est donc moins la nature du modèle lui-même que 
sa mobilisation par le réseau d’acteurs qui vont le faire vivre et évoluer, 
et le cadrage de l’expertise auquel l’outil et les acteurs vont contribuer : 
une évaluation de coûts ou de politiques, la négociation d’un dispositif 
ou d’un protocole, etc.
Cette architecture de la modélisation soulève des questions épisté-
mologiques (quel type de connaissance obtient-on avec de tels outils ?) 
aussi bien que des questions sociales et politiques (quelle expertise cela 
fournit ?). En se substituant à la méthode expérimentale, aux méthodes 
de prévision statistique inférentielle et à la méthode hypothético-dé-
ductive, la méthode de la simulation numérique s’avère à la fois très 
puissante et d’une fiabilité contestable. Cela se traduit par des incerti-
tudes, directement visibles dans les rapports du GIEC par la dispersion 
des projections. Les tentatives de décomposition de ces incertitudes en 
fonction de leur source (variabilité intrinsèque des phénomènes, variété  
des scénarios possibles, variation des sorties de modèles) s’avèrent 
incapables de les expliquer et encore moins de les réduire. Que faire 
alors de ces incertitudes ? Faut-il les traduire en risques mesurables par 
des coûts et des probabilités associées ? Cela semble bien difficile, car on 
ne sait pas sur quelle base évaluer les coûts et surtout on ignore avec quel 
coefficient actualiser les coûts du futur. Quant aux probabilités affectées à 
des résultats qui mélangent de nombreuses procédures, les controverses 
se multiplient sur ce qui pourrait être aussi à la base de leur évaluation : 
possibilités des futurs, dires d’experts, fréquences dans les simulations.
Le schéma linéaire PSIR a été vivement mis en accusation. Le GIEC a 
remédié partiellement à ces critiques pour son 5e rapport, rendu public 
en 2014, en substituant au schéma séquentiel une organisation parallèle 
des travaux des deux communautés réunies autour d’une même cible. 
Celle-ci est définie en termes de concentration atmosphérique des gaz à 
effet de serre : les climatologues entrent en input de leurs modèles diffé-
rentes valeurs de la concentration, et les socio-économistes explorent les 
scénarios de politiques économiques et environnementales correspondant 
à des trajectoires censées aboutir à ces mêmes valeurs de concentration. 
La tentation pour les modélisateurs est alors de vouloir inclure des 
processus de plus en plus nombreux, non seulement climatiques mais 
	
le siècle des modèles	
353

354	
michel armatte et amy dahan 
aussi bio- et géochimiques, et utiliser de gros modèles du système Terre 
(Earth system models) 1.
Il reste de nombreuses questions sur le rôle des simulations dans 
l’expertise climatique. La crise économique de 2008 suivie de l’échec de 
la conférence de Copenhague en 2009 ont révélé les fragilités constitu-
tives du couple gouvernance politique du climat / expertise scientifique 
du GIEC 2. Si la crise de confiance a pu se diffuser comme une traînée 
de poudre, des institutions politiques vers celles de la science, c’est aussi 
parce que les régimes successifs d’expertise coexistent partiellement et 
suscitent des attentes contradictoires de la part des scientifiques, des 
politiques ou dans l’opinion publique.
Mentionnons trois questions particulièrement vives qui concernent les 
modèles comme outils dans le dialogue social et la gouvernance politique :
1)	Quel usage doit-on faire des travaux de modélisation dans la gouver-
nance globale ? Entre ceux qui trouvent qu’ils n’ont qu’un faible écho 
dans la cacophonie générale de la négociation, et ceux qui estiment au 
contraire que les modélisations ont eu une importance excessive au point 
de fausser le jeu, comment traiter objectivement de ces enjeux et de leur 
institutionnalisation ?
2)	Comment mobiliser les résultats des modèles globaux à un niveau 
local ou national, le seul qui intéresse les acteurs de terrain ? On sait que 
les modèles AOGCM (atmosphere-ocean global circulation models) ont 
de mauvais résultats dans certaines régions, d’où les programmes visant 
une comparaison systématique de modèles pour en établir les fiabi-
lités relatives. Toutefois, entre la course à la réduction du maillage et 
donc à la puissance des ordinateurs et le downscaling artificiel, n’est-il 
pas temps de s’orienter résolument vers d’autres outils pour aborder les 
problèmes d’adaptation et de vulnérabilité au changement climatique ? Et 
si le diagnostic de l’Anthropocène se confirme, comment ne pas scruter 
davantage du côté des sciences sociales, inventer des chemins de transition 
écologique pour nos sociétés, et identifier les forces sociales souhaitant 
ces transformations ?
3)	Comment associer plus intimement les populations concernées aux 
travaux des experts pour dépasser la méfiance ou au contraire la trop 
grande confiance dans les résultats de ces modèles ? L’enjeu démocra-
tique de cette nouvelle science est de taille et il est loin d’être résolu.
1. Dahan 2010.
2. Aykut et Dahan 2011.

 
Conclusion
L’objet modèle a considérablement évolué depuis le début du xxe siècle 
et joue aujourd’hui un rôle central dans les domaines les plus divers, un 
rôle qui ne se réduit plus à la validation d’une théorie, mais intervient dans 
la simulation du fonctionnement et de l’évolution de systèmes complexes. 
Cette fonction majeure modifie ses statuts épistémologique et social. Elle 
le conduit à l’interface directe avec la décision et la gouvernementalité 
politique. À travers les exemples de l’économie et du climat, nous avons 
décrit quelques-uns des dispositifs associés à la modélisation, suggéré que 
la notion abstraite de modèle hors de toute contextualisation historique 
et sociale a peu de sens, et discuté quelques questions, épistémologiques 
et politiques, qui s’attachent à ces nouveaux usages.
Références bibliographiques
Armatte Michel, 2004, « Les sciences économiques reconfigurées par la pax 
americana », in  Dominique Pestre et Amy Dahan (dir.), Les  Sciences pour la 
guerre (1940-1960), Paris, Éd. de l’EHESS, p. 129-174.
–	 2007, « Les économistes face au long terme : l’ascension de la notion de scénario », 
in Amy Dahan (dir.), Les Modèles du futur, Paris, La Découverte, p. 63-90.
–	 2010, La Science économique comme ingénierie. Quantification et modélisation, 
Paris, Presses des Mines.
Armatte Michel et Dahan Amy, 2004, « Modèles et modélisations (1950-2000). 
Nouvelles pratiques, nouveaux enjeux », Revue d’histoire des sciences, vol. 57, no 2, 
p. 245-305.
Aubin David et Dahan Amy, 2002, « Writing the History of Dynamical Systems and 
Chaos : Longue Durée and Revolution, Disciplines and Culture », Historia Mathe-
matica, vol. 29, no 3, p. 273-339.
Aykut Stefan et Dahan Amy, 2011, « Le régime climatique avant et après Copen-
hague. Sciences, politiques et l’objectif des deux degrés », Natures, sciences, sociétés, 
no 19, p. 144-157.
Bachelard Suzanne, 1979, « Quelques aspects historiques des notions de modèle et 
de justification des modèles », Actes du colloque « Élaboration et justification des 
modèles », éd. par Pierre Delattre, Paris, Maloine.
Bachelier Louis, 1900, « Théorie de la spéculation », Annales de l’École normale 
supérieure, no 17, p. 21-86.
Barberousse Anouk et Ludwig Pascal, 2000, « Les modèles comme fictions », Philo-
sophie, no 68, p. 16-43.
Bazzani Armando, Giorgini Bruno, Servizi Graziano et Turchetti Giorgio, 
2003, « A Chronotopic Model of Mobility in Urban Spaces », Physica A, vol. 325, 
nos 3-4, p. 517-530..
Bennett Stuart, 1993, History of Control Engineering (1930-1955), Stevenage, Peter 
Peregrinus.
Bissell Chris, 2004, « Models and Black Boxes : Mathematics as an Enabling Technology 
	
le siècle des modèles	
355

356	
michel armatte et amy dahan 
in the History of Communication and Control Engineering », Revue d’histoire des 
sciences, vol. 57, no 2, p. 305-338.
Boltzmann Ludwig, 1902, « Model », in Encyclopaedia Britannica.
Bouleau Nicolas, 1999, Martingales et marchés financiers, Paris, Odile Jacob.
–	 2002, « La modélisation et les sciences de l’ingénieur », in Pascal Nouvel (dir.), 
Enquête sur le concept de modèle, Paris, PUF.
Callon Michel, 2007, « Performative Economics », in  D.  MacKenzie, Do  Econo-
mists Make Markets ?, Princeton, Princeton University Press, p. 311-357.
Carnap Rudolf, 1928, Der logische Aufbau der Welt, Berlin, Weltkreis ; trad. fr., 
La Construction logique du monde, Paris, Vrin, 2002.
Carnap Rudolf, Hahn Hans et Neurath Otto, 1985 [1929], « La conception scienti-
fique du monde », in Antonia Soulez (dir.), Manifeste du Cercle de Vienne et autres 
écrits, Paris, PUF.
Cornilleau Lise et Leblond N., 2012, « Gouverner la sécurité alimentaire globale 
par la modélisation ? Le cas du modèle IMPACT de l’IFPRI », communication non 
publiée, Paris, Collège doctoral de l’IFRIS.
Dahan Amy, 1996, « L’essor des mathématiques appliquées aux États-Unis : l’impact 
de la Seconde Guerre mondiale », Revue d’histoire des mathématiques, no 2, p. 149- 
213.
–	 2001, « History and Epistemology of Models : Meteorology (1946-1963) as a 
Case-Study », Archive for History of Exact Sciences, vol. 55, no 5, p. 395-422.
–	 2004, « Axiomatiser, modéliser, calculer. Les mathématiques, instrument universel 
et polymorphe d’action », in Dominique Pestre et Amy Dahan (dir.), Les Sciences 
pour la guerre (1940-1960), Paris, Éd. de l’EHESS.
–	 (dir.), 2007, Les Modèles du futur, Paris, La Découverte.
–	 2010, « Putting the Earth System in a Numerical Box ? The Evolution from Climate 
Modeling toward Climate Change », Studies in History and Philosophy of Modern 
Physics, no 41, p. 282-292.
Dahan Amy et Guillemot H., 2006, « Changement climatique. Dynamiques scien-
tifiques, expertise, enjeux géopolitiques », Revue de sociologie du travail, no 48, 
p. 412-432.
Dahan Amy et Pestre Dominique (dir.), 2004, Les Sciences pour la guerre (1940-
1960), Paris, Éd. de l’EHESS.
Edwards Paul N., 2010, A Vast Machine : Computer Models, Climate Data and the 
Politics of Global Warming, Cambridge (MA), MIT Press.
Forrester Jay Wright, 1971, World Dynamics, Cambridge (MA), MIT Press.
Galison Peter, 1996, « Computer Simulations and the Trading Zone », in  Peter 
Galison et David J. Stump (dir.), The Disunity of Science : Boundaries, Contexts, 
and Power, Palo Alto (CA), Stanford University Press, p. 118-157.
Haavelmo Trygve, 1944, The Probability Approach in Econometrics, supplément à 
Econometrica, no 12.
Israel Giorgio, 1996, La Mathématisation du réel, Paris, Seuil.
Keynes John Maynard, 1939, « Professor Tinbergen’s Method », Economic Journal, 
no 49, p. 558-568.
Kieken Hubert, 2004, « RAINS : modéliser les pollutions atmosphériques pour la 
négociation internationale », Revue d’histoire des sciences, vol. 57, no 2, p. 379-408.
Legay Jean-Marie, 1997, L’Expérience et le modèle, Paris, Éd. de l’INRA.
MacKenzie Donald, 2000, « Fear in the Markets », London Review of Books, no 13, 
avril, p. 31-32.

–	 et al., 2007, Do Economists Make Markets ? On the Performativity of Economics, 
Princeton (NJ), Princeton University Press.
Malgrange Pierre, 1990, « Forces et faiblesses des modèles macroéconométriques », 
in  Bernard Cornet et Henry Tulkens (dir.), Modélisation et décisions écono-
miques, Louvain, De Boeck.
Meadows Dennis L. et Meadows Donella H., 1974 [1972], The Limits to Growth : 
A Report for the Club of Rome’s Project on the Predicament of Mankind, New York, 
New American Library.
Morgan Mary S., 1990, The History of Econometric Ideas, Cambridge, Cambridge 
University Press.
Morgan Mary S. et Morrison Margaret, 1999, Models as Mediators : Perspectives 
on Natural and Social Science, Cambridge, Cambridge University Press.
Pestre Dominique, 2003, Science, argent et politique. Un essai d’interprétation, Paris, 
Quæ.
Sinaceur Hourya, 1999, « Modèle », in  Dominique Lecourt (dir.), Dictionnaire 
d’histoire et de philosophie des sciences, Paris, PUF, p. 649-651.
Sismondo Sergio (dir.), 1999, « Modeling and Simulation », dossier thématique de 
Science in Context, vol. 12, no 2.
Varenne Franck, 2007, Du modèle à la simulation informatique, Paris, Vrin.
Walliser Bernard, 2011, Comment raisonnent les économistes. Les fonctions des 
modèles, Paris, Odile Jacob.
	
le siècle des modèles	
357


 
Troisième partie
Les sciences  
et le gouvernement  
du monde


17 Genre, corps et biomédecine
D e l p h i n e  G a r d e y
Vers 1907 aux États-Unis, des cas de typhoïde apparaissent sans qu’il 
soit possible d’en expliquer l’origine. Les autorités sanitaires formulent 
une hypothèse inédite : l’existence de personnes porteuses saines du virus 
et agents de sa transmission. La presse se focalise autour du personnage 
de Mary Mallon, une cuisinière ambulante, bientôt placée en quarantaine. 
Hospitalisée contre son gré, elle devient un véritable « rat de labora-
toire » avant d’être relâchée puis réhospitalisée après avoir contaminé  
de nouvelles personnes. Appelée « Typhoid Mary », cette femme robuste 
et peu coopérative est stigmatisée autant par les autorités médicales que 
par la presse du fait de son comportement « viril » et de son caractère 
« récidiviste ». Accusée d’avoir repris son métier de cuisinière, cette 
célibataire sans famille ni soutien passe plus de vingt-six années dans des 
institutions de santé. Comparant son cas à celui d’autres porteurs sains 
de la typhoïde, l’historienne Judith Walzer Leavitt conclut que c’est bien 
son statut social (de domestique) et son genre non conforme (une femme 
d’âge mûr, célibataire, virile et rebelle) qui explique en partie le sort qui 
lui est réservé 1. La liberté de Mary Mallon ne semble pas opposable à ces 
objectifs plus nobles que sont les progrès des connaissances médicales 
ou la nécessité de préserver la société d’une contamination insidieuse.
L’anecdote ouvre le xxe siècle témoignant d’une époque passée et d’une 
époque à venir. Du côté du passé, le cas Mallon rejoint la tradition des 
« corps vils » : ces êtres insuffisamment inscrits dans le social pour pouvoir 
être protégés. Du corps des orphelins, des prostituées, des aliénés, il a été 
possible de disposer pour expérimenter et il est toujours nécessaire de 
s’en préoccuper. La préservation du corps social en dépend 2. En tant que 
socialement peu conforme et peu insérée, la vie de Mary Mallon compte 
1. Walzer Leavitt 1995.
2. Chamayou 2008.
 Mary Mallon, dite « Typhoid Mary », en quarantaine à l’hôpital de Brother Island, État de New York. 
Elle y est restée de 1907 à 1910. Sa seconde quarantaine a duré 23 ans, de 1915 à 1938.

362	
delphine gardey
peu au regard des vies qu’elle risque d’atteindre et de mettre en danger. 
D’une certaine façon, sa vie peut être effacée. L’effacement a cours car, 
en un sens, il a déjà eu lieu 1.
Du côté du présent et de ce qui est à venir, le cas Mallon poursuit et 
modifie des discussions et des pratiques antérieures. Il pose sur nouveaux 
frais la question du risque et des moyens (expérimentaux, sanitaires, 
disciplinaires) qu’il convient ou non d’employer pour réaliser les objectifs 
(médicaux et sociaux) espérés. La discussion est ancienne et détaillée 
par Jean-Baptiste Fressoz à propos de l’inoculation de la variole à la fin 
du xviiie siècle 2. La question du coût et du bénéfice n’est pas seulement 
une question médicale ou sociale, elle devient une question juridique et 
politique. Dans le cas Mallon, la privation abusive de liberté ouvre une 
brèche dans l’un des fondements constitutionnels de la démocratie améri-
caine : jusqu’où, afin de soigner et de prévenir, peut-on s’émanciper du 
respect des droits individuels fondamentaux ? Ce n’est pas l’effacement de  
la vie concrète de Mary Mallon qui pose problème mais l’effacement  
de la valeur abstraite de sa vie et donc possiblement de toute vie. Là réside 
une contrainte nouvelle pour l’intervention publique dans le domaine  
de la santé et de la gestion médicale des corps.
Le cas Mallon permet de bousculer certaines idées attendues. Quand on 
pense « typhoïde », on ne pense pas a priori « genre ». Pourtant, le genre 
en tant que rapport social compte dans cette histoire (tout comme, dans 
d’autres cas, les rapports coloniaux). Le genre peut jouer un rôle ou être 
en jeu dans nombre d’aspects de l’histoire contemporaine des corps et 
des relations qu’ils entretiennent avec les sciences médicales. Mobiliser la 
typhoïde, c’est signaler que les identités sociales et sexuées sont négociées 
dans le rapport avec les techniques et les institutions médicales et pointer 
le fait que des savoirs et pratiques scientifiques participent à la définition 
et à la production de ces identités 3. Un autre intérêt de cette histoire est 
de poser la question des « corps qui comptent », des vies qui méritent 
d’être soignées ou sauvées 4, de ce qu’il convient de mettre en œuvre 
pour favoriser, limiter ou contrôler les vies qui méritent d’être vécues et 
reproduites 5. Il est encore de pointer le fait que les corps ordinaires se 
trouvent transformés dans leurs destinées par l’expérience de la maladie 
et la relation avec les technologies médicales 6, et d’ouvrir à la question 
1. Butler 2010.
2. Fressoz 2012.
3. Gardey et Löwy 2000, Chabaud-Rychter et Gardey 2002.
4. Butler 2010.
5. Boltanski 2004.
6. Akrich et Laborie 1999.

	
genre, corps et biomédecine	
363
plus vaste de la gestion et de la régulation des corps sains et malsains, 
du normal et du pathologique 1.
Pour parcourir le xxe siècle, je me propose de tirer bénéfice de l’accu-
mulation de cas, de façon à mesurer des tendances et effets de long 
terme et à apprécier les dynamiques historiques qui ont cours. Des 
questions classiques sont reprises : le corps des femmes et celui des 
hommes connaissent-ils un destin asymétrique comme source d’inves-
tigation ou de considération médicale et publique au xxe siècle ? Quelles 
recherches et pratiques médicales sur les corps sexués et reproducteurs ? 
Le genre intervient-il comme facteur discriminant dans l’évolution de 
la condition sanitaire des populations au cours de cette période ? Quels 
sont les savoirs et les pratiques médicales qui contribuent à produire 
des connaissances sur la différence de sexe et la « nature » de cette diffé-
rence ? Qu’en est-il de la construction sociale et médicale du masculin ? 
En quoi les savoirs et les pratiques médicaux sur les sexualités contri-
buent à modifier les rapports de genre ? Dans quelle mesure peut-on parler 
d’extension de la sphère de ce qui relève du médical dans la définition des 
expériences sociales de l’identité au cours du xxe siècle ? Comment évolue 
la frontière du « naturel » et du « culturel » et en quoi les biotechnologies 
contribuent à les transformer ? Finalement, qu’est-ce qui est déplacé en 
termes de limites et de contraintes, quelles sont les capacités d’agir et les 
nouvelles normes qui apparaissent en fin de période ?
Entre deux guerres : ruptures et continuités
Le corps féminin : un objet durable  
d’investissement médical et politique
Si les figures déviantes de l’hystérique, de l’aliénée, de la prostituée 
ou de la criminelle intéressent au plus haut point les sciences médicales 
et les autorités sanitaires au xixe siècle, le corps féminin continue de 
mobiliser savoirs et pratiques médicaux au xxe siècle, mais différemment. 
Considéré comme le site de la sexualité (à la différence du corps des 
hommes), le corps féminin est triplement investi entre les deux guerres 
comme corps sexué (ou sexuel), corps gestant et corps reproducteur – ces 
deux dernières qualités engageant directement le « corps de la nation ». 
Les femmes n’intéressent pas tant pour elles-mêmes que pour ce qui 
les qualifie dans leur différence et leur rôle social. De ce point de vue, 
1. Fassin et Memmi 2004.

364	
delphine gardey
asymétrie et surinvestissement sont manifestes du côté des autorités 
médicales et des autorités étatiques.
Un nouveau partage semble intervenir au début du xxe siècle. La 
psychiatrie et la sexologie naissante héritent de l’hystérique et des « plaisirs 
pervers », cependant qu’en biologie et avec l’endocrinologie s’ouvre un 
espace d’investigation inédit centré sur l’œstrus, l’ovulation, le cycle 
menstruel et – du côté des sciences animales – l’insémination artificielle 1. 
Si la sexologie (américaine d’abord) se focalise sur la sexualité comme 
« propre de l’homme », les sciences de la reproduction se détachent des 
questions sexuelles et partagent avec les sciences vétérinaires et l’animal 
nombre d’agendas, acquérant une forte autonomie et une forte légitimité en 
fin de période. Encore en gestation, les sciences reproductives n’affectent 
véritablement la vie des femmes que par le biais de l’endocrinologie. 
L’organothérapie (le développement d’applications cliniques à partir 
d’extraits de gonades naturelles puis synthétiques) connaît, en effet, un 
véritable âge d’or en Europe et aux États-Unis de la fin des années 1920 
aux années 1940. Les traitements et indications sont multiples : depuis  
les maux traditionnellement définis comme féminins (maux de tête, 
irritabilité, dépression) jusqu’à la définition de certains moments de la vie 
des femmes comme pathologies nécessitant traitement (menstruations, 
ménopause) 2. La médicalisation du cycle de vie sexuel des femmes est en 
cours, portée par l’initiative de laboratoires à la recherche de débouchés 3.
Contrôler la matrice, contrôler les populations
Les sciences de la vie ne sont pas seules à redéfinir le paysage des relations 
entre genre, sciences et corps au cours de cette période. La démographie, 
mais aussi des actrices et des acteurs non scientifiques, jouent un rôle 
majeur dans la discussion sur les capacités génératives des femmes et le 
contrôle des populations. La préoccupation de certains groupes en faveur 
du contrôle et de la limitation des naissances s’amplifie dans l’entre-deux-
guerres. Les partisans du birth control en Europe comme aux États-Unis 
(féministes, néomalthusiens, certains socialistes) espèrent des bienfaits 
individuels, familiaux et sociaux dans la limitation des naissances vue 
comme une source d’émancipation individuelle et de progrès social. La 
féministe Margaret Sanger, « mère » de la pilule contraceptive, est une figure 
majeure de cette période et de la suivante 4. Au congrès de démographie 
1. Clarke 1998, Oudshoorn 1994.
2. Oudshoorn 1994.
3. Oudshoorn 1993, Löwy 2006.
4. Marks 2001, Clarke 1998.

	
genre, corps et biomédecine	
365
qui se tient à Genève en 1927, elle est pourtant seule à défendre le birth 
control contre la communauté des démographes, exclusivement masculine, 
et centrée sur des orientations eugénistes 1.
Les frontières entre néomalthusiens et eugénistes sont fragiles et 
complexes au cours de cette période. Dans l’un et l’autre cas, un bénéfice 
social, voire national, est espéré au titre d’une « bonne » gestion des 
populations. Mais le programme eugéniste se caractérise par la dimension 
de sélection des populations qu’il introduit. Pour certains courants 
eugénistes, la fécondité des couches populaires apparaît comme une 
aberration génétique : il faut viser la reproduction des « meilleurs », 
non celle des plus faibles, et cette sélection s’appuie sur un programme 
biologique. L’eugénisme s’intéresse au birth control comme moyen de 
contrôler tout à la fois la quantité et la « qualité » de la reproduction. 
L’idée d’une reproduction humaine scientifiquement organisée prend 
ici toute sa signification 2. Le « national » et le « racial » s’opposent aux 
méfaits de la « civilisation moderne » dont les effets « débilitants » nuisent 
à « l’instinct génésique 3 ». Comme Donna Haraway l’a souligné pour les 
États-Unis, la période est aux passages et aux basculements incessants 
entre le concept de « race » et celui de « population 4 ». La fondation de 
l’Institute of Racial Biology en 1924 à l’université de Chicago marque, en 
contexte démocratique, ce projet d’une « nouvelle biologie des popula-
tions » basée sur la génétique 5.
Natalisme, maternalisme, eugénisme, virilisme :  
dissonances européennes
Le projet eugéniste se déploie de façon différenciée suivant les pays et 
les régimes. Pour l’essentiel, les pays européens demeurent pronatalistes. 
Cette politique nataliste prend la forme d’une politique maternaliste, 
qu’elle soit d’initiative patronale ou étatique, qui se manifeste dans les lois  
sociales, le droit du travail, la protection des femmes enceintes, les aides 
au moment de la naissance. L’obsession nataliste française s’exprime avec 
la loi de 1920 qui criminalise l’avortement. Si la France, comme la plupart 
des pays catholiques, ne connaît pas de politique active de prévention 
des « naissances indésirables », cette réalité concerne les États-Unis, 
la Suède, la Norvège ou la Suisse. Elle se traduit par des politiques de 
1. Vienne 2006.
2. Marks 2001.
3. Vienne 2006.
4. Haraway 2003.
5. Clarke 1998 (p. 114).

366	
delphine gardey
stérilisation visant des populations spécifiques et touche hommes et 
femmes (en témoigne la stérilisation des « criminels » et des épilep-
tiques aux États-Unis entre 1907 et 1920). On estime, par exemple, que 
le programme suédois de stérilisation encore effectif dans les années 
1970 concerne au total 60 000 personnes 1.
L’Allemagne nazie se différencie par la violence de sa politique, mais 
aussi par son antimaternalisme. Dans le cas nazi, le racisme hygiénique 
et anthropologique l’emporte sur le pronatalisme. La question n’est pas 
celle du nombre mais du contrôle de la « qualité ». Le culte est ici celui 
de la paternité et de la virilité, non celui de la maternité 2. Du côté de 
l’eugénisme et de la sélection des populations, on rappellera quelques 
aspects de cette politique meurtrière. L’euthanasie de 150 000 patients 
et la stérilisation de 300 000 « malades héréditaires » témoignent des 
liens profonds entre « psychiatrie ordinaire » et « médecine nazie », 
délimitant une « biocratie » qui sera renforcée et redéployée dans l’entre-
prise plus systématique d’extermination entreprise par le Troisième Reich 3. 
5 000 personnes (des femmes pour l’essentiel) trouvent la mort du fait  
de la seule politique de stérilisation qui est remplacée à partir de 1939 par 
une politique d’euthanasie et d’extermination dont le nombre des victimes 
est, on le sait, autrement plus important 4. Culte de la paternité (biologique 
et raciale), culte de la virilité sont perceptibles dans d’autres domaines. 
Si l’endocrinologie européenne et l’organothérapie se concentrent pour 
l’essentiel sur le féminin comme maladie et comme objet de traitement, 
les performances viriles des hommes occupent au premier plan l’Alle-
magne nazie, allant jusqu’à dessiner les prémisses d’une andrologie 
(pendant biochimique du champ gynécologique), dont le programme 
est interrompu avec la chute du régime 5.
Soigner, être soigné.e : le genre du care
Durablement, la production des savoirs sur le féminin et le masculin, 
comme plus largement l’exercice de la médecine sont des privilèges 
masculins. Le « corps médical » dans sa partie haute et prestigieuse 
semble éternellement mâle. Cette asymétrie structurante est à peine 
ébranlée par l’apparition des étudiantes en médecine puis des premières 
femmes médecins, qui se voient généralement limitées aux clientèles 
1. Marks 2001.
2. Bock 1996.
3. Massin 1996.
4. Marks 2001.
5. Gaudillière 2003.

	
genre, corps et biomédecine	
367
« particulières » que sont les enfants ou les femmes 1. Pourtant, la contri-
bution des femmes aux activités médicales est ancienne et s’incarne 
dans la figure désormais laïque de l’infirmière ou celle, traditionnelle, 
de la sage-femme. La division sociale et genrée du travail médical n’est 
pas sans conséquences sur la définition, le contenu et la conduite des 
activités. Bashford montre ainsi la persistance en Angleterre au début 
du siècle des concepts sanitaires s’appuyant sur les savoirs des nurses, 
en dépit du développement d’une médecine bactériologique et micro-
biologique 2. Elle insiste sur la longue durée des pratiques féminines de 
régulation des malades par le travail infirmier consistant à « mesurer, 
enregistrer, réguler les entrées et sorties de nourriture (vomir, déféquer, 
boire, uriner, suer) ». Si le médecin « guérit », l’infirmière soigne. Ce qui 
est de l’ordre de la charité, de la vocation, du don de soi ou du sacrifice 
est assimilé au féminin ; ce qui est de l’ordre de la connaissance, de 
l’expérimentation, de la maîtrise des techniques, de la carrière et du 
« professionnalisme » se décline au masculin. L’engagement des personnes 
(et des corps) dans le travail médical est en lui-même le fait d’un rapport  
de genre.
Corps au travail
Ce dernier point ouvre plus généralement à la question du corpus 
laborans et de l’impact des conditions de travail sur la santé. Peu de 
travaux s’intéressent de façon systématique à l’analyse des pathologies 
contractées du fait de l’activité professionnelle dans une perspective de 
genre 3. Quelques cas classiques jalonnent l’histoire industrielle : les mouve-
ments des ouvrières de l’industrie des allumettes à la fin du xixe siècle 
en France qui dénoncent les effets délétères du phosphore blanc sur leur 
santé 4 ; les agonies publiques des ouvrières des dial painters du New Jersey 
affligées de nécroses de la mâchoire en raison de l’utilisation de peinture 
à base de radium 5 et dont les mouvements de protestation contribuent à 
une première campagne de prévention des risques en milieu industriel 6. 
Ici, le recours à l’expertise scientifique autorise ou non « l’établissement 
des faits » et des responsabilités juridiques, tant pour obtenir réparation 
que pour définir des mesures protectrices 7. À l’évidence, hommes et 
1. Edelman 2005.
2. Bashford 1998.
3. Bruno et Omnès 2004.
4. Gordon 1993.
5. Pour rendre les aiguilles lumineuses.
6. Clark 1997.
7. Bale 1990.

368	
delphine gardey
femmes ne partagent pas les mêmes risques au travail en raison de la 
ségrégation sexuée des activités et des professions.
Du côté du travail scientifique, on peut mentionner le secteur de la 
radiochimie en France et le laboratoire Curie où travaillent en moyenne 
30 % de femmes entre 1920 et 1940. En dépit des alertes et des symptômes,  
les radiochimistes négligent et minimisent les risques causés par la 
manipulation du radium et des radioéléments. Peut-être la résistance de 
ces femmes scientifiques devant certaines pathologies est-elle le meilleur 
témoignage de la conviction partagée du sacrifice qu’il est nécessaire  
de consentir comme scientifique (et comme femme, pour devenir scien-
tifique) en cette période pionnière 8 ? Il est intéressant de noter qu’en 
dépit de recherches sur les effets des rayons sur les corps (et en parti-
culier sur les gonades sexuelles et la stérilité féminine) les « experts » ne 
recommandent pas de mesures préventives. La question de la stérilité 
n’apparaît clairement qu’après 1945 comme une préoccupation méritant 
la mise en œuvre de protections au moment où les effets mutagènes des 
radiations sont reconnus. Ici encore, c’est le corps des travailleuses en 
tant que corps (possiblement) gestant qui retient l’attention, témoignant 
de cette focalisation durable de l’intervention médicale sur l’appareil 
reproducteur féminin.
Le temps de la modernité : transformations,  
médicalisation et émancipation ? (1950-1980)
Les transformations de la condition maternelle
La femme enceinte est une figure de choix pour passer d’un âge à un 
autre, dans cette époque de boom démographique qu’est l’après-guerre. 
Les conditions sanitaires autour de la grossesse et de l’accouchement 
s’améliorent dans les pays industrialisés dans la période 1935-1950 en 
dépit des années dures du conflit. On assiste alors à une réduction impor-
tante de la mortalité maternelle et périnatale. Après 1950, ces taux de 
mortalité atteignent un niveau plancher qui sonne comme une conquête 
définitive. Depuis les années 1930, de nombreux facteurs contribuent à 
l’amélioration des conditions sanitaires de l’accouchement : les sulfamides, 
la pénicilline, la transfusion sanguine, l’amélioration de la sécurité de 
l’anesthésie, mais aussi de nouvelles formes de surveillance de la grossesse 
qui permettent, par exemple, de diagnostiquer une hypertension avant la 
8. Fellinger 2010.

	
genre, corps et biomédecine	
369
crise d’éclampsie. La mise en place des États-providences et de nouvelles 
assurances sociales accompagne et favorise ce mouvement après 1945 1. 
Dans les années 1950-1960, la césarienne comme l’anesthésie semblent 
moins risquées, et de nouveaux appareils de surveillance de la grossesse 
et de l’accouchement se développent cependant que les hôpitaux se 
modernisent.
Le boom démographique de l’après-guerre n’altère sans doute pas 
les tendances malthusiennes de plus long terme. Les couples inves-
tissent dans des naissances moins nombreuses et moins répétées. Mettre 
au monde un bébé en bonne santé dans de bonnes conditions semble 
devenir une exigence de la modernité. Durant le conflit, des campagnes 
en faveur du Distilbène vantent aux femmes enceintes américaines  
les vertus d’un produit destiné à éviter les fausses couches et, alors que 
les soldats rentrent enfin dans leur foyer, garantissent la naissance de 
« beaux et gros bébés 2 ». Prescrit à des millions de femmes américaines, 
le Distilbène est également utilisé en France pour prévenir les avorte-
ments spontanés entre 1948 et 1977, produisant des effets secondaires 
dramatiques : cancers, stérilité.
De la maternité heureuse à la sexualité heureuse ?
Que faut-il entendre par « maternité heureuse » ? L’expression emprunte 
aux mouvements du contrôle des naissances et du planning familial qui 
se développent et, pour certains pays, s’institutionnalisent au cours de 
cette période. À la convergence des mouvements féministes et d’associa-
tions de militants et professionnels, le planning familial vise surtout dans 
les années 1960 le bonheur conjugal et une harmonie familiale et sociale 
obtenue grâce au contrôle et à la limitation des naissances. La méthode 
Ogino, technique phare de l’époque, renvoie à ces dimensions conjugales 
de la contraception. La « maternité heureuse », c’est aussi l’accouchement 
sans douleur, dont les préceptes et le contenu idéologique sont définis et 
propagés (quoique de façon encore limitée) pendant cette période 3. La 
« maternité heureuse », c’est enfin, et aussi, le débouché d’une sexualité 
conjugale heureuse. L’histoire de la sexologie met en évidence la façon dont 
cette spécialité médicale (à mi-chemin entre la psychiatrie et la gynéco-
logie) se déploie comme science de la conjugalité. L’objet de la sexologie 
consiste pour l’essentiel à soigner les dysfonctions et pathologies dites 
1. Merci à Marilène Vuille pour son aide sur ces points.
2. Langston 2010.
3. Vuille 1998.

370	
delphine gardey
« mineures » du couple, une orientation qui contribue autant à produire 
l’hétérosexualité comme sexualité normale qu’à indiquer ce qui doit 
être rejeté dans l’ordre de la pathologie ou de la déviance, comme c’est 
le cas de l’homosexualité 1. Le chemin vers la « sexualité heureuse » n’est, 
on le voit, pas à l’ordre du jour pour tous. Bien qu’elle concerne peu de 
personnes, la sexologie contribue, cependant, et depuis son autorité toute 
médicale, à légitimer la recherche du plaisir et, en particulier, l’orgasme 
féminin, comme objectif d’une sexualité épanouie.
Quel genre de transformations de l’environnement sanitaire ?
Avant d’envisager comment le « jouir sans entraves » s’inscrit à l’ordre du 
jour idéologique et concret des vies des un.e.s et des autres, revenons sur 
la figure du bébé « sain », le DES (Distilbène), et ce qu’ils disent des trans-
formations en cours. L’après-guerre « bénéficie » du boom de l’industrie 
chimique et des transferts technologiques qui en résultent vers la société 
civile. Sortant de l’usine ou du laboratoire, les substances susceptibles 
d’être toxiques et d’affecter les corps sont de plus en plus présentes et 
diverses. La période est en effet à la multiplication des risques de troubles 
endocrinologiques liés à la multiplication des produits chimiques et de 
leurs usages. Au-delà des prises orales pour prévenir les fausses couches, 
le DES est employé largement dans les élevages de volailles et de bovins. 
Au summun de son utilisation dans les années 1960, il est présent dans 
l’alimentation de 95 % des bovins américains, remplissant cet objectif 
moderne de « bien » nourrir la population et de lui permettre d’accéder 
en masse aux protéines avec des effets secondaires pathogènes impor-
tants sur des millions d’enfants. Cette réalité nouvelle d’un environnement 
devenu toxique pour les corps (parmi lesquels le DTT est un agent de 
premier plan) n’est en fait connue et reconnue qu’à partir des années 
1970. L’indépendance des expertises scientifiques, la politique commer-
ciale agressive des groupes chimiques et pharmaceutiques commencent 
à être questionnées 2. Les conséquences sanitaires de ces transformations 
environnementales sont encore peu étudiées dans une perspective de 
genre. Les années Distilbène et, parmi les conséquences les plus graves, 
la prévalence des cancers du vagin pour les filles des mères en ayant 
consommé pendant leur grossesse, tout comme les années thalidomide et 
les nombreuses déformations des enfants nés sous thalidomide comptent 
comme des aspects marquants de cette histoire.
1. Burgnard 2012.
2. Langston 2010, Boudia et Jas (dans ce volume, p. 381).

	
genre, corps et biomédecine	
371
 
Vers les libertés et les droits contraceptifs
Participant de ces transformations, les œstrogènes synthétiques sont 
largement utilisés dès l’entre-deux-guerres pour « soulager » les femmes 
de la ménopause qui est de plus en plus considérée comme une condition 
nécessitant traitement. La médicalisation de la ménopause, définie 
comme une maladie de la déficience, témoigne de la force des concep-
tions culturelles quant à la nature désordonnée des femmes et au fait 
que la régulation hormonale (et chimique) puisse en venir à bout. En 
cette période de confiance pleine et entière dans le progrès, il n’est pas 
anodin qu’une solution « universelle » (suivant le concept de Margaret 
Sanger) à la question de la limitation des naissances trouve à se réaliser 
sous la forme d’une « simple pilule ». « Entreprise moderniste par excel-
lence 1 », la pilule contraceptive appartient pleinement au fordisme et 
à la production de masse. Présentée comme une solution scientifique 
à la préoccupation occidentale de l’« explosion démographique » du 
tiers-monde, elle marque l’aboutissement des orientations prises dans 
le champ des sciences reproductives durant les décennies précédentes, 
répond aux préoccupations géopolitiques et idéologiques du moment, 
et signale l’aboutissement (et le point de départ) d’un projet féministe 
qui fait de la libre disposition des corps, du contrôle des naissances et 
bientôt de la liberté sexuelle un objectif majeur.
La pilule contraceptive conquiert ses utilisatrices par la liberté qu’elle 
offre (notamment par rapport à leurs partenaires sexuels) puisque sa prise 
peut être détachée de l’acte sexuel, à la différence d’autres techniques 
contraceptives (diaphragme, préservatif, retrait). D’abord très contrôlée 
et utilisée dans le seul contexte conjugal, elle connaît un essor sans 
précédent aux États-Unis puis en Europe, réclamée par des millions 
de femmes des classes moyennes. Elle participe ainsi d’un mouvement 
social et culturel plus vaste. La pilule devient un agent de la « révolution 
sexuelle », dans un contexte de remise en cause des normes familiales 
et des formes traditionnelles d’autorité, et de revendication à l’auto-
nomie. La bataille pour les droits contraceptifs et le droit à l’avortement 
caractérise le renouveau féministe de cette période. La légalisation de 
l’avortement, qui intervient au cours des années 1970 dans la plupart 
des pays occidentaux, marque, à terme, une rupture fondamentale dans 
l’histoire longue des relations entretenues entre les femmes, leur corps 
(la matrice ou l’utérus), les hommes, les médecins, l’État. Cette relation 
1. Marks 2001.

372	
delphine gardey
est profondément modifiée par une revendication (et une possibilité) 
de souveraineté inédite qui manifeste la fin du contrôle patriarcal sur le 
corps féminin et, ainsi, d’un certain ordre biopolitique.
Il n’est pas anodin que le premier « contraceptif universel » se soit 
décliné au féminin. Cette réussite scientifique (sociale et culturelle) est 
le fruit d’une accumulation des connaissances et de pratiques (gynéco-
logie, obstétrique, endocrinologie) sur le corps des femmes et exprime 
cette assimilation toujours (re) naturalisée entre reproduction humaine et 
corps féminin. L’exclusion de la fertilité masculine des discours et pratiques 
médicales comme des discours et pratiques politiques est durable 1. Pas 
de contraceptifs masculins commercialisés avant les dernières décennies 
du xxe siècle, autant par défaut de science que d’habitudes culturelles 
permettant d’envisager sérieusement le corps masculin comme corps sexué 
et reproducteur. La pilule contribue à la médicalisation de la sexualité 
des femmes. Le mouvement est ambivalent : les femmes acquièrent de 
l’autonomie via les technologies et l’offre médicale et modifient ainsi les 
possibilités de maîtrise de leur destinée. Ces nouvelles capacités d’agir 
sont pourtant profondément liées à l’extension du champ du médical 
qui est à la fois appropriable et non appropriable, source d’autonomie et 
source de nouvelles contraintes, comme s’en plaignent les mouvements 
féministes du type selfhelp contestant l’autorité médicale dans les années 
1970-1980.
Les promesses de la postmodernité :  
coûts et gains de genre (1980-2014)
« Libertés reproductives » et « enfant par projet » :  
troubles dans la reproduction ?
Il est désormais commun de constater l’importance prise par les questions 
médicales dans nombre d’aspects de nos vies sociales. Cette évolution prend 
deux formes principales à la fin du xxe siècle : à l’évidence un nouveau 
mode de « gouvernement des corps » se met en place, et les questions du 
« corps », de la « santé » et de la « vie » prennent une place croissante dans le 
« gouvernement des affaires humaines 2 ». Par ailleurs, nombre d’expériences 
nouvelles de l’identité sociale ou de genre, l’existence, la vie ou la survie 
du sujet sont définies ou conditionnées par les ressources biomédicales.
1. Pfeffer 1993.
2. Fassin et Memmi 2004 (p. 10).

	
genre, corps et biomédecine	
373
Le premier bébé-éprouvette (1978) marque pour Adèle Clarke une 
« vraie frontière », puisque s’y joue la modification des « faits de vie 1 ». 
Si le temps « moderne » des sciences reproductives était celui de la 
normation, de la régulation et du soin, les « nouvelles technologies 
de la reproduction » marquent un régime inédit de manipulation des 
processus et des produits. La dissociation entre sexualité et engen-
drement, rendue normativement possible par les méthodes contraceptives 
et l’avortement, ouvre l’ère de « l’enfant par projet ». Le mariage (ou l’ins-
titution) ne fait plus l’enfant ; l’enfant est le fruit du désir et de la volonté 
des parents (quel que soit leur sexe) 2. Ces transformations dans l’ordre des 
mœurs, des relations familiales et de genre coïncident avec le temps du 
management « par projet » et trouvent dans l’offre biotechnologique des 
ressources qui apparaissent comme autant de réponses et de promesses 
en particulier à ces nouveaux fléaux contemporains que semblent 
être l’infertilité et la stérilité, dont l’histoire reste à écrire en contexte  
occidental.
Le développement des technologies de la reproduction réitère de l’ancien 
et produit du nouveau en termes de genre. Du côté de l’ancien, les inter-
ventions médicales s’exercent surtout sur le corps des femmes – l’appareil 
reproducteur féminin pouvant même être intensément mobilisé pour régler 
la question de l’infertilité masculine (comme c’est le cas avec la technique 
d’injection intracellulaire du sperme du géniteur biologique, ISCI) 3. Du 
côté du nouveau, le déplacement de la fécondation et de la gestation vers 
le laboratoire contribue à inventer de nouvelles catégories médicales et 
sociales : le couple stérile, le donneur de sperme, la donneuse d’ovules ou 
d’utérus, le patient « non né 4 ». Ces technologies peuvent être utilisées 
comme des ressources dans le cadre des parentés homosexuelles et condi-
tionnent ainsi la réalisation de nombre de projets parentaux. Finalement, 
les moyens d’accès sans précédent sur le contenu de l’utérus (échographie, 
amniocentèse, chirurgie fœtale) contribuent à la dissociation du couple 
mère-fœtus et à l’émergence du fœtus sur la scène publique. Les impli-
cations sociales, ethniques et de genre du processus de commodification 
des « produits gestatifs » (gamètes, utérus), comme on le voit dans le cas 
du don marchandisé de sperme dans le cadre libéral américain ou les 
formes contractuelles de gestation pour autrui, sont donc complexes et 
encore en devenir 5.
1. Clarke 1998.
2. Boltanski 2004.
3. Löwy 2000.
4. Casper 2000.
5. Becker 2000.

374	
delphine gardey
On retiendra de cette période que les frontières du genre et des corps 
sont profondément déplacées et redéfinies via les biotechnologies contem-
poraines. Plus encore, les conditions de ce qui permet que la vie soit, les 
éléments de définition de ce que la vie est ou peut être, ainsi que l’instan-
ciation des vies qui comptent et méritent d’être soutenues (on pensera par 
exemple aux développements considérables de la médecine de la grande 
prématurité depuis trois décennies), sont particulièrement engagés dans 
ces processus. La réalité d’un possible nouvel eugénisme (c’est-à-dire 
l’organisation de la non-venue au monde de certaines naissances : sourds, 
trisomiques) n’est pas à vrai dire substantiellement explorée pour les 
sociétés occidentales.
Dissonances de sexe et de genre : la fin de la norme hétérosexuelle ?
Judith Butler ouvre sa réflexion dans Gender Trouble en avouant le 
caractère existentiel d’un travail qui veut contribuer à « rendre les vies 
possibles 1 ». Reprenant la généalogie du gouvernement des corps et des 
populations « déviantes » (homosexuelles et intersexes) marquée par 
la criminalisation, la répression et la médicalisation, elle interroge la  
possibilité pour ces « vies » d’exister tant au plan subjectif, social que 
politique. Le temps ouvert par la « révolution sexuelle » est celui de l’affir-
mation et de la reconnaissance des sexualités et des identités sexuelles 
différentes. Sortir d’une condition criminalisée, déviante ou patho­logique 
(la médicalisation marque le passage de l’un à l’autre) a été un enjeu 
majeur des mouvements homosexuels contemporains. Alors que les 
homosexuels sont parvenus à disparaître de la classification du Manuel 
diagnostique et statistique des troubles mentaux en 1974, une population 
nouvelle, les transsexuels, y entre comme objets / sujets médicaux  
et d’intervention.
Dès l’entre-deux-guerres, des personnes intersexes font l’objet de prises 
en charge médicales qui visent à « réparer » et réorienter dans le sens 
d’une plus grande lisibilité (médicale et sociale) des indicateurs et des 
organes sexués jugés problématiques. Ces pratiques cliniques reposent 
sur l’endocrinologie et la chirurgie réparatrice. À partir des années 1950 
et de la plateforme médicale mise en place à l’université Johns Hopkins, 
la pratique dominante jusqu’aux années 1990 vise à assigner chirurgica-
lement et socialement un sexe à l’enfant intersexué, souvent à l’insu des 
parents et des enfants eux-mêmes.
Détaillant cette histoire, les travaux de Bernice Hausman insistent 
1. Butler 2006.

	
genre, corps et biomédecine	
375
sur la singularité de la question transsexuelle 1. Elle montre que le trans-
sexualisme n’est pas l’expression nouvelle d’un désir atemporel mais qu’il 
se trouve au contraire
complètement dépendant, comme fait social et scientifique, du dévelop-
pement de techniques médicales telles que l’endocrinologie et la chirurgie 
plastique, et de leur capacité à établir les conditions nécessaires à l’émergence 
d’une demande pour le changement de sexe, comprise comme indicateur de 
la subjectivité transsexuelle.
Les transsexuels se définissent comme des personnes devant obtenir 
un traitement médical pour être reconnus tels. Leur position subjective 
dépend de leur relation à l’ordre médical. Il s’agit donc d’une catégorie 
de l’expérience et de l’identité qui est le reflet de conditions sociales et 
culturelles indissociables de pratiques scientifiques et techniques spéci-
fiques. Ici, c’est de façon très active que la médecine « fait » des femmes 
ou des hommes susceptibles ou non d’avoir une vie sexuelle (de préfé-
rence hétérosexuelle) à défaut de pouvoir avoir une vie procréative. 
Hausman met en évidence que l’homophobie est centrale dans l’histoire 
des technologies de la transsexualité.
Depuis les années 1990, la contestation de ces techniques médicales 
ainsi que des protocoles qui permettent d’y entrer ou d’en bénéficier 
a débouché sur de nouvelles revendications de la part de collectifs ou 
d’associations « trans, inter, queer ». On notera, par exemple, les revendi-
cations de l’association suisse « Zwischengeschlecht » qui milite, au nom 
des « droits humains », contre les opérations réalisées à la naissance et 
attire l’attention des autorités médicales et internationales sur les consé-
quences des chirurgies imposées aux nouveau-nés et aboutissant à des 
mutilations génitales irréversibles.
La construction médicale du masculin et de la sexualité :  
de la médication à la performance
Moins invasif, le traitement médical de l’impuissance masculine via le 
Viagra ne semble être que promesse. D’abord objet de la psychanalyse 
puis de la sexologie, l’impuissance masculine est profondément redéfinie 
au cours des années 1980 par les urologues. Loin de s’enquérir de soigner 
la personne ou la relation sexuelle, ils donnent un cadre plus limité à leur 
domaine d’intervention, passant du vocabulaire de l’impuissance (et du 
1. Hausman 1995.

376	
delphine gardey
« psychogène ») à celui de la « dysfonction érectile » (et de l’organique) 1. 
Dans le cadre de cette nouvelle étiologie organiciste, la neuroendocri-
nologie joue un rôle central comme c’est le cas pour les traitements 
contemporains des troubles de la sexualité chez les femmes, tels que la 
perte de désir post-partum 2. Les firmes pharmaceutiques, ici celle qui a 
inventé le silnédafil (molécule du Viagra), jouent un rôle déterminant. 
L’extension de la définition clinique du trouble érectile s’avère considérable, 
comprenant de plus en plus de patients potentiels. Dans le cas français, le 
cadre préalable de régulation du médicament (les autorités conditionnent 
drastiquement prescription et remboursement) est rapidement subverti 
et dépassé par l’utilisation « récréative » du Viagra par automédication.
Ces performances masculines et au masculin (hétéro et gay), ces 
injonctions à la performance sexuelle semblent contemporaines de la 
« performance » au double sens proposé par Judith Butler pour penser 
une nouvelle politique de l’identité et des sexualités 3. Elles sont des 
pratiques de signification et de réitération des identités de genre et des 
normes sexuelles ainsi que la manifestation d’une agency (ou « puissance 
d’agir »). Pourtant, manque à cette connotation émancipatrice une décli-
naison managériale. L’« accomplissement de soi » via les biotechnologies 
se décline dans un contexte d’offre médicale puissant, susceptible de 
définir tout autant la « nature » et le « cadre » des problèmes à traiter, les 
frontières de ce qui peut ou doit relever du médical, que les solutions 
thérapeutiques qui conviennent. Il intervient dans un contexte producti-
viste et néolibéral qui voit l’extension constante de la sphère du marchand 
et de ce qui peut être marchandisé.
Faut-il, dans ces conditions, continuer à placer sous le seul registre 
de l’individu, de sa liberté et de ses choix, ces nouvelles pratiques ? La 
« pharmacopée » ou le « marché du désir » et de la performance sexuelle  
au féminin 4 marchent-ils avec l’offre technologique de pointe et les 
nouveaux marchés de la fécondation artificielle, de la gestation pour autrui, 
de la réparation sexuelle ou de l’amélioration de soi, tels que la chirurgie 
esthétique ? Le dépassement de soi (de ses propres limites procréa-
tives, sexuelles ou de genre) peut-il être envisagé sans tenir compte des 
marchés considérables que constituent les technologies biomédicales et 
la pharmacopée ? Alors que le « nouvel esprit du capitalisme 5 » semble 
faire reposer sur les individus la charge de valoriser leur corps comme 
1. Giami 2004.
2. Hirt 2009.
3. Butler 2006.
4. Fishman 2004.
5. Boltanski et Chiapello 1999.

	
genre, corps et biomédecine	
377
force de travail, comment concilier normes managériales et proclama-
tions iréniques de l’accomplissement de soi ?
Multiplication des possibilités d’enhancement, pharmacopées neuro-
leptiques des troubles de la menstruation et du désir, traitements au long 
cours des désordres comportementaux des enfants « suractifs », nous 
rappellent les enjeux économiques mais aussi normatifs et politiques 
de ces « solutions » médicales qui sont aussi des « solutions sociales et 
politiques ». Une réflexion qu’il semble urgent de poursuivre dans une 
perspective politique et de genre à propos de la position actuellement 
hégémonique des neurosciences 1.
Références bibliographiques
Akrich Madeleine et Laborie Françoise (dir.), 1999, « De la contraception à l’enfan-
tement. L’offre technologique en question », Les Cahiers du genre, no 25.
Bale Anthony, 1990, « Women Toxic Experience », in  Rima D.  Apple, Women, 
Health and Medicine in America, New Brunswick (NJ), Rutgers University Press, 
p. 403-431.
Bashford Alison, 1998, Purity and Pollution : Gender, Embodiment and Victorian 
Medicine, Basingtoke, Macmillan.
Becker Gay, 2000, « Espoir à vendre. Commercialisation et consommation de 
techniques d’assistance médicale à la procréation aux États-Unis », Sciences sociales 
et santé, vol. 18, no 4, p. 105-126.
Bock Gisela, 1996, « Equality and Difference in National Socialist Racism », in Joan 
Wallach Scott (dir.), Feminism and History, Oxford, Oxford University Press, 
p. 267-292.
Boltanski Luc, 2004, La Condition fœtale. Sociologie de l’engendrement, Paris, 
Gallimard.
Boltanski Luc et Chiapello Ève, 1999, Le Nouvel Esprit du capitalisme, Paris, 
Gallimard.
Bruno Anne-Sophie et Omnès Catherine, 2004, Les Mains inutiles. Inaptitude au 
travail et emploi en Europe, Paris, Belin.
Burgnard Sylvie, 2012, Produire, diffuser et contester les savoirs sur le sexe. Une 
sociohistoire de la sexualité dans la Genève des années 1970, thèse de doctorat de 
sociologie, université de Genève.
Butler Judith, 2006, Trouble dans le genre. Le féminisme et la subversion de l’identité, 
Paris, La Découverte.
–	 2010, Ce qui fait une vie. Essai sur la violence, la guerre et le deuil, Paris, Zones.
Casper Monica, 2000, The Making of the Unborn Patient, New Brunswick (NJ), 
Rutgers University Press.
Chabaud-Rychter Danielle et Gardey Delphine (dir.), 2002, L’Engendrement 
des choses. Des hommes, des femmes et des techniques, Paris, Éd.  des Archives 
contemporaines.
Chamayou Grégoire, 2008, Les Corps vils, Paris, La Découverte.
1. Dussauge et Kaiser 2012.

378	
delphine gardey
Clark Claudia, 1997, Radium Girls : Women and Industrial Health Reform (1910-
1935), Chapel Hill (NC), University of North Carolina Press.
Clarke Adele, 1998, Disciplining Reproduction : Modernity, American Life and the 
Problem of Sex, Berkeley (CA), California University Press.
Dussauge Isabelle et Kaiser Annelyse, 2012, « Re-Queering the Brain », in Robyn 
Bluhm et al. (dir.), Neurofeminism, Palgrave Macmillan.
Edelman Nicole, 2005, « Les femmes médecins », in Jacqueline Carroy et al. (dir.), 
Les Femmes dans les sciences de l’homme (xixe-xxe siècle). Inspiratrices, collabora-
trices ou créatrices, Paris, Seli Arslan.
Fassin Didier et Memmi Dominique, 2004, Le Gouvernement des corps, Paris, Éd. de 
l’EHESS.
Fellinger Anne, 2010, « Femmes, risque et radioactivité en France. Les scientifiques 
et le danger professionnel », Travail, genre et sociétés, no 1, p. 147-165.
Fishman Jennifer, 2004, « Manufacturing Desire : The Commodification of Female 
Sexual Dysfunction », Social Studies of Science, vol. 34, no 2, p. 187-218.
Fressoz Jean-Baptiste, 2012, L’Apocalypse joyeuse, Paris, La Découverte.
Gardey Delphine et Löwy Ilana (dir.), 2000, L’Invention du naturel. Les sciences et 
la fabrication du féminin et du masculin, Paris, Éd. des Archives contemporaines.
Gaudillière Jean-Paul, 2003, « La fabrique moléculaire du genre. Hormones 
sexuelles, industrie et médecine avant la pilule », in Ilana Löwy et Hélène Rouch 
(dir.), dossier « La distinction entre sexe et genre », Cahiers du genre, no 34, p. 81-104.
Giami Alain, 2004, « De l’impuissance à la dysfonction érectile », in  D.  Fassin et 
D. Memmi (dir.), Le Gouvernement des corps, Paris, EHESS, p. 77-108.
Gordon Bonnie, 1993, « Ouvrières et maladies professionnelles sous la Troisième 
République. La victoire des allumettiers français sur la nécrose phosphorée de la 
mâchoire », Le Mouvement social, vol. 3, no 164, p. 77-138.
Haraway Donna, 2003, The Haraway Reader, New York, Routledge.
Hausman Bernice, 1995, Changing Sex : Transsexualism, Technology and the Idea of 
Gender, Durham (NC), Duke University Press.
Hirt Caroline, 2009, « La sexualité postnatale : un objet d’étude négligé par les sciences 
humaines et sociales », in Catherine Deschamps, Laurent Gaissad et Christelle 
Taraud (dir.), Hétéros. Discours, lieux, pratiques, Paris, EPEL, p. 145-153.
Langston Nancy, 2010, Toxic Bodies : Hormone Disruptors and the Legacy of DES, 
New Haven (CT), Yale University Press.
Löwy Ilana, 2000, « Assistance médicale à la procréation et traitement de la stérilité 
masculine en France », Sciences sociales et santé, vol. 18, no 4, p. 75-102.
–	 2006, L’Emprise du genre. Masculinité, féminité, inégalité, Paris, La Dispute.
Marks Lara, 2001, Sexual Chemistry : A History of the Contraceptive Pill, New Haven 
(CT), Yale University Press.
Massin Benoît, 1996, « L’euthanasie psychiatrique sous le IIIe Reich. La question de 
l’eugénisme », L’Information psychiatrique, no 8, p. 811-822.
Oudshoorn Nelly, 1993, « United We Stand : The Pharmaceutical Industry, 
Laboratory and Clinics in the Development of Sex Hormones into Scientific Drugs 
(1920-1940) », Science, Technology and Human Values, vol. 18, no 1, p. 5-24.
–	 1994, Beyond the Natural Body : Archeology of Sex Hormons, Londres, Routledge.
Pfeffer Naomi, 1993, A  Political History of Reproductive Medicine, Cambridge, 
Polity Press.
Thébaud Françoise, 1986, Quand nos grand-mères donnaient la vie. La maternité en 
France dans l’entre-deux-guerres, Lyon, Presses universitaires de Lyon.

	
genre, corps et biomédecine	
379
Vienne Florence, 2006, Une science de la peur. La démographie avant et après 1933, 
Francfort, Peter Lang.
Vuille Marilène, 1998, Accouchement et douleur. Une étude sociologique, Lausanne, 
Antipodes.
Walzer Leavitt Judith, 1995, « Gender Expectations : Women and Early Twentieth-
Century Public Health », in Linda K. Kerber, Alice Kessler-Harris et Kathryn 
Kish Sklar (dir.), US History as Women’s History : New Feminist Essays, Chapel 
Hill (NC), University of North Carolina Press, p. 147-169.


18 Gouverner un monde contaminé. 
Les risques techniques,  
sanitaires et environnementaux
S o r a y a  B o u d i a  e t  N at h a l i e  J a s
Ce chapitre se propose de retracer les transformations des modes de 
gouvernement des risques techniques, sanitaires et environnementaux 
au cours du xxe siècle, c’est-à-dire la manière dont scientifiques, experts, 
autorités publiques, industriels et groupes critiques pensent, conçoivent 
et gèrent les dangers posés par les activités technoscientifiques et indus-
trielles. En nous appuyant sur un ensemble de travaux réunis dans deux 
ouvrages collectifs que nous avons dirigés 1, nous proposons de distinguer 
trois modes de gouvernement : le gouvernement par la norme, le gouver-
nement par le risque et le gouvernement par l’adaptation. Ceux-ci émergent 
dans cet ordre mais ils cohabitent et s’hybrident plus qu’ils ne se remplacent. 
Nous examinons ces différents modes de gouvernement au travers des 
concepts, des paradigmes et des régulations auxquels ils donnent lieu. 
Notre thèse est que les systèmes de régulation qui sont mis en place tout 
au long du siècle se développent selon une logique d’accommodements 
qui cherchent certes à protéger la santé publique et l’environnement, mais 
sans trop contraindre le développement des activités industrielles jugées 
indispensables. Nous défendons aussi l’idée que les transformations du 
gouvernement des dangers techniques, sanitaires et environnementaux 
au cours du siècle résultent des changements dans la gravité et l’échelle 
des problèmes posés, des échecs des politiques mises en œuvre, des 
apprentissages qui en découlent, et des renouvellements des critiques et 
mobilisations. Pour expliciter ces transformations, nous suivons un cas 
emblématique, celui des toxiques – les substances chimiques et les agents 
physiques potentiellement dangereux pour la santé et l’environnement.
1. Boudia et Jas 2013 et 2014.
 Les déchets électroniques marquent l’expansion quasi continue du monde toxique comme la persistance 
des inégalités d’exposition. Qingyuan, Chine.

382	
soraya boudia et nathalie jas
 
Gouverner par la norme
Les dangers pour l’environnement et la santé posés par le dévelop-
pement de nouvelles technologies tout au long du xxe siècle prennent 
racine dans le siècle précédent. Dès le xixe siècle en effet, les transfor-
mations industrielles galopantes entraînent des modifications profondes 
de l’environnement au prix d’accidents industriels, de contaminations 
chimiques et d’empoisonnement des corps ouvriers et consommateurs. 
Ces multiples effets ne sont pas ignorés. Ils donnent lieu au dévelop-
pement de différentes formes de gestion : commissions d’experts, procès, 
assurances, indemnisations, amélioration des systèmes techniques et 
développement de réglementations et de nouvelles administrations 1.
Au-delà de leur grande variété, ces dispositifs de gestion contribuent 
tous à mettre en place un mode de gouvernement des dangers techniques, 
sanitaires et environnementaux qui repose sur l’idée de la maîtrise et du 
contrôle : maîtrise de techniques dangereuses, les explosions de machines 
à vapeur par exemple, ou contrôle des flux des contaminants et de leurs 
effets par la mise en place de normes techniques ou de valeurs limites 
d’émission et d’exposition. Ainsi, les systèmes réglementaires œuvrent 
moins à interdire les usages de techniques ou de substances reconnues 
comme dangereuses qu’à les confiner dans certains espaces, à certaines 
populations ou à certains usages. C’est cette logique de confinement qui 
structure les lois sur les établissements classés qui sont adoptées au cours 
du xixe siècle. À partir de la fin du xixe, on assiste à l’essor de systèmes 
de régulation nationaux. Ces systèmes portent notamment sur la sécurité 
des établissements industriels, le contrôle des pollutions, la protection 
de la santé des travailleurs, la sécurité des produits alimentaires et la 
production et circulation des substances toxiques 2. Ces développements 
ont lieu à un moment où l’État étend ses domaines de compétence tout 
en transformant ses modes de gestion en installant des administrations 
dans lesquelles l’expertise technique et scientifique joue un rôle essentiel. 
La mise en place de ces systèmes s’accompagne du développement de 
nouvelles disciplines telles que l’hygiène industrielle ou la toxicologie, 
et de nouvelles professions comme celles des juristes, des chimistes- 
experts ou des inspecteurs. Ces systèmes de régulation se renforcent dans 
l’entre-deux-guerres sans parvenir à empêcher des scandales sanitaires 
résultant de l’extension de certains secteurs d’activité – pollutions par 
1. Bernhardt 2004, Massard-Guilbaud 2010, Le Roux 2011, Fressoz 2012.
2. Carpenter 2001, Buzzi, Devinck et Rosental 2006, Dessaux 2007.

	
gouverner un monde contaminé	
383
accidents industriels, empoisonnements collectifs par des médicaments 
ou des substances toxiques ayant contaminé des objets de consommation 
ou des aliments 1.
Au sortir de la Seconde Guerre mondiale, l’échelle des problèmes 
posés par les effets délétères sur la santé et l’environnement de l’essor des 
technosciences change radicalement. L’installation de nouvelles techno-
logies comme le nucléaire et la chimie de synthèse ouvre une nouvelle 
ère. On assiste à ce qui peut être considéré comme l’une des caracté-
ristiques du second xxe siècle, la dilatation des échelles spatiales et 
temporelles auxquelles les problèmes se posent. Désormais, les problèmes 
ne sont plus locaux mais peuvent toucher l’ensemble de la planète. Ils ne 
concernent plus uniquement la santé d’individus ou de petits collectifs mais 
l’ensemble des écosystèmes. Leurs conséquences ne sont pas simplement 
immédiates mais peuvent se prolonger sur plusieurs générations. Du fait 
même qu’ils prennent place à des échelles inédites, de l’infiniment petit à 
l’infiniment grand, les effets sanitaires et environnementaux des innova-
tions se traduisent par des problèmes jusque-là inconnus et auxquels se 
heurtent les industriels, les experts et les institutions publiques.
Le cas des substances potentiellement toxiques illustre parfaitement 
l’avènement de problèmes auparavant inconnus et les limites des approches 
développées jusqu’alors pour les gérer. Dans l’après-guerre, les systèmes 
de régulation, en profonde recomposition, doivent ainsi faire face à un flot 
continu de substances nouvelles mises sur le marché et dont la plupart 
n’ont fait l’objet d’aucune évaluation ni d’aucune régulation préalable. La 
solution conçue pour limiter leurs éventuels effets toxiques est embléma-
tique du développement d’un gouvernement par les normes : la production 
de valeurs limites d’exposition. Il s’agit, sur la base de connaissances 
scientifiques supposées sûres, de fixer les concentrations maximales de 
chaque contaminant que peuvent contenir un aliment, une ambiance 
de travail, un médicament, ces concentrations devant garantir l’absence 
de danger. Ce recours aux valeurs limites est légitimé par le dogme dit 
fondamental de la toxicologie selon lequel « la dose fait le poison », et par 
l’idée selon laquelle la nature est capable d’absorber sans dommage une 
certaine quantité de polluants. Il semble alors possible de déterminer pour 
chaque toxique et pour chaque usage un seuil d’exposition en dessous 
duquel aucun effet délétère significatif ne pourra être observé. Le recours 
aux valeurs limites comme moyen de gestion des effets des toxiques se 
généralise progressivement à partir des années 1950, que ce soit dans  
la santé au travail, l’alimentation, les pollutions environnementales ou le 
1. Whorton 1974, Sellers 1997.

384	
soraya boudia et nathalie jas
médicament. Cette généralisation s’effectue cependant sans que la plupart 
des milliers de substances mises en circulation au cours de la seconde 
moitié du siècle ne fassent l’objet de la moindre évaluation toxicologique. 
Les valeurs limites produites sont ainsi souvent le fruit d’un compromis 
entre experts, administrations et industriels et traduisent un accommo-
dement progressif avec une circulation et des usages des toxiques, y compris 
des substances particulièrement dangereuses comme les carcinogènes. 
Le cas de ces derniers est significatif de cette logique d’accommodement 
qui accompagne le gouvernement par les normes.
Dès les années 1950, à la suite notamment des controverses internatio-
nales sur les essais atomiques, des travaux scientifiques montrent qu’il est 
impossible de déterminer des seuils d’exposition pour ces carcinogènes. 
Une logique protectrice aurait pu mener à leur interdiction. Cependant, 
faire ce choix aurait signifié se priver de nombreuses substances indispen-
sables à certains secteurs industriels. L’interdiction étant inenvisageable 
économiquement, il est rapidement admis par les instances de régulation 
que les carcinogènes puissent être autorisés dans des proportions fixées 
par des valeurs limites. Ce positionnement n’est cependant pas aisé à tenir 
publiquement dans le cadre d’un discours qui promeut la garantie d’une 
protection totale. Il se traduit par des contradictions, particulièrement 
visibles dans le domaine sensible de l’alimentation. Différents comités 
d’experts nationaux et internationaux recommandent ainsi, au milieu 
des années 1950, l’interdiction totale de carcinogènes dans les aliments. 
Cette disposition est même officiellement adoptée aux États-Unis en 
1958 sous la forme de la Delaney Clause. Cependant, l’interdiction s’avère 
impossible à mettre en œuvre et, dès la fin des années 1950, les comités 
d’experts américains et internationaux construisent des systèmes de tests 
de la carcinogénicité ainsi que des modèles d’extrapolation et d’inter-
prétation des données expérimentales pour produire des valeurs limites 
d’exposition, y compris pour des carcinogènes avérés 1.
Les accommodements vis-à-vis des principes théoriques de protection 
absolue de la santé sur lesquels repose le gouvernement par les normes, 
et la prise de risque qu’ils engendrent, ne prennent pas place sans inquié-
tudes, interrogations et critiques. Les activités industrielles et leurs effets 
sur les paysages, la qualité des eaux et de l’air atmosphérique, la santé 
humaine, les forêts, les cultures ou les animaux d’élevage, ou encore leurs 
conséquences sociales, avaient suscité dès le xixe siècle de nombreuses 
contestations qui avaient pris la forme de mouvements sociaux impulsés 
par des syndicats ouvriers, de procès engagés par des riverains ou encore 
1. Jas 2013, Boudia 2013a, Vogel 2012.

	
gouverner un monde contaminé	
385
de campagnes de presse. Ces mouvements se sont poursuivis dans l’entre-
deux-guerres et au cours des années 1950, donnant même lieu, aux 
États-Unis, aux premières mobilisations d’ampleur nationale contre les 
industries chimiques 1. À la fin des années 1960, les systèmes de régulation 
de l’ensemble des activités technoscientifiques et industrielles développés 
depuis la Seconde Guerre mondiale sont placés sous les feux de critiques 
virulentes et les cibles de mobilisations environnementalistes multiformes 
et connaissent alors une première crise importante. Les initiatives insti-
tutionnelles et privées visant à leur apporter des réponses débouchent 
sur la conception d’un deuxième mode de gouvernement, le gouver-
nement par le risque.
Gouverner par le risque
À partir du milieu des années 1950, l’idée selon laquelle l’homme 
contribue à rendre son environnement toxique pour la santé ne cesse  
de gagner en importance. Après la vague de controverses sur les effets  
des retombées radioactives, les pollutions chimiques, en particulier liées 
aux pesticides et à certaines substances comme les PCB, deviennent l’objet 
de larges débats. Ces préoccupations apparaissent dans certains cercles 
professionnels, en particulier celui des spécialistes des cancers ou de la 
génétique environnementale, mais également dans l’expérience quoti-
dienne des classes moyennes confrontées à la pollution de l’air qu’elles 
respirent et à la dégradation environnementale des lieux qu’elles habitent. 
Signe de ce phénomène, la publication en 1962 de Silent Spring 2, écrit 
par une spécialiste de biologie marine, Rachel Carson, connaît un succès 
mondial rapide et marque le début effectif d’un mouvement qui gagne 
en importance à partir de la fin des années 1960 3.
L’environnementalisme qui se développe alors met en avant de nouvelles 
problématiques qui couvrent un large spectre de questions, au premier 
rang desquelles les effets irréversibles des pollutions environnementales, 
l’épuisement des ressources, l’importance des impacts sanitaires ou encore 
l’absence de démocratie dans les choix technoscientifiques 4. Ces diffé-
rentes thématiques sont portées par une myriade d’associations animées 
par des figures comme celle de l’avocat américain Ralph Nader, promoteur 
du mouvement des consommateurs. Ces associations ne fleurissent pas 
1. Ross et Amter 2010.
2. Carson 1962.
3. Hays 1987, Brooks 2009.
4. Boudia et Jas 2007, Brown et Mikkelsen 1990.

386	
soraya boudia et nathalie jas
qu’aux États-Unis. Les nombreuses conférences préparatoires à la Confé-
rence des Nations unies sur l’environnement humain de 1972 montrent 
l’existence d’un activisme similaire dans les différents pays de l’Europe du 
Nord. Dans des logiques d’alliances, de mises en commun de ressources 
et d’augmentation des capacités d’action, des associations nationales se 
fédèrent et constituent des réseaux 1 qui, à partir du début des années 
1970, se renforcent par la création d’une nouvelle génération d’organisa-
tions non gouvernementales à vocation transnationale, comme les Amis 
de la Terre créés en 1969 ou Greenpeace fondé en 1971.
Ces mobilisations critiques, organisées et multiformes, agissant 
aussi bien sur les terrains politiques, juridiques que scientifiques, s’ins-
crivent dans les nombreux mouvements revendicatifs de l’époque et leur 
empruntent parfois leurs méthodes de contestation radicale. Elles ouvrent 
une période de crise qui débouche sur des transformations importantes 
des conceptions et des systèmes de régulation des risques sanitaires et 
environnementaux. Elles débouchent ainsi sur l’adoption de nouvelles 
législations, comme le National Environmental Policy Act 1969, le Clean 
Air Act 1970, le Clean Water Act 1972, le Toxic Chemical Substances 
Act 1976 aux États-Unis 2, ou les premières législations environnemen-
tales européennes qui prennent la forme de règlements ou de directives 
comme la directive 67/548/CEE de 1967 sur les substances chimiques, la 
directive 75/442/EEC de 1975 sur les déchets, la directive dite Seveso de 
1982 sur les sites industriels présentant des risques majeurs ou la directive 
Environmental Impact Assessment (EIA) de 1985 qui rend obligatoire la 
réalisation d’évaluations des impacts environnementaux de certains projets 
industriels et d’aménagement. L’adoption de ces législations s’accompagne 
de la création de nouvelles institutions, notamment d’agences d’évaluation 
et de régulation comme l’Environmental Protection Agency (EPA) en 1970 
ou le National Toxicological Program en 1978 ou, à l’échelle européenne, 
l’Agence européenne pour l’environnement (EEA) en 1993 et l’Agence 
européenne de sécurité des aliments (EFSA) en 2002. Cette période est 
également marquée par un essor sans précédent de l’expertise interna-
tionale sur les risques sanitaires environnementaux. L’Organisation de 
coopération et de développement économiques (OCDE) et différentes 
organisations onusiennes engagent par exemple de nouveaux programmes 
de recherche et d’expertise et développent des systèmes de mesure et 
d’évaluation des risques. C’est ainsi qu’est réuni un important corpus de 
données et que des communautés internationales d’experts se structurent ; 
1. Keck 2005, Tarrow 1998, Pellow 2007.
2. Jones 1975, Milazzo 2006.

	
gouverner un monde contaminé	
387
par exemple dans le cadre du Scientific Committee on the Problems of the 
Environment (SCOPE) créé en 1969 au sein de l’International Council for 
Science (ICSU), et qui fait fonction de comité scientifique du Programme 
des Nations unies pour l’environnement (PNUE) ; ou dans l’International 
Programme on Chemical Safety (IPCS) 1 installé en 1980 et dépendant 
de l’Organisation mondiale de la santé (OMS).
L’analyse de ces transformations institutionnelles pointe un changement 
dans la manière de concevoir et gouverner les risques techniques, sanitaires 
et environnementaux. Face à la multiplication des contestations, des scien-
tifiques et des hommes politiques estiment qu’il est nécessaire d’apporter 
de nouvelles réponses aux débordements engendrés par des activités 
technoscientifiques et industrielles. Désormais, il s’agit moins de prétendre 
à une maîtrise totale de ces débordements que de proposer un nouveau 
contrat social reposant sur une reconnaissance partielle des risques et sur 
de nouvelles procédures d’évaluation, de gestion et de réparation. Dans 
un climat de controverses et de tensions politiques, plusieurs commu-
nautés d’experts investissent la question de la prise de décision relative 
aux technologies à risques et cherchent à définir de nouvelles démarches 
d’action dans un univers controversé 2. En plus de réflexions sectorielles, 
ce qui retient l’attention est la définition d’un cadre général et d’une 
procédure générique permettant de comparer et de classer différents 
risques. Le thème du risk assessment devient l’objet du travail de plusieurs 
agences fédérales américaines à partir de la fin des années 1960. L’EPA, la 
Food and Drug Administration (FDA), les National Institutes of Health 
(NIH), en collaboration avec la National Academy of Science (NAS), le 
National Research Council (NRC) et la National Science Foundation 
(NSF) engagent une réflexion conjointe sur la définition d’une méthodo-
logie transversale d’évaluation et de décision des activités présentant des 
risques technologiques, sanitaires ou environnementaux. Ces travaux ont 
un écho important dans des organisations internationales telles que le 
PNUE, l’OCDE, l’OMS ou l’International Institute for Applied Systems 
Analysis (IIASA), qui mettent également en place des comités d’experts 
pour explorer ces questions.
Tout au long des années 1970, ces différentes réflexions sur les modalités 
de gestion des risques techniques, sanitaires et environnementaux se 
concentrent sur la définition des critères et procédures de décision 
en situation d’incertitude, que cette incertitude concerne le degré de 
dangerosité ou les perturbations techniques et sociales que ces risques 
1. IARC 1972.
2. Boudia 2013b, Cranor 1993, Jasanoff 1990.

388	
soraya boudia et nathalie jas
engendrent. Deux grandes thématiques sont investies de façon priori-
taire par les experts : celle de l’acceptabilité sociale des risques d’une part, 
celle de la mise en œuvre des méthodes d’analyse économique de l’autre. 
Si, sur la scène publique, les différents comités d’experts affichent une 
grande confiance dans leur capacité à définir une méthodologie générale, 
dans les espaces confinés des mondes experts les critiques et les doutes 
sont nombreux. Certains acteurs défendent l’idée d’une démarche plus 
modeste et cherchent d’abord à définir un instrument d’évaluation des 
risques et les modalités d’articulation institutionnelle entre science  
et décision pour des cas précis : ceux en particulier des radiations, des 
substances chimiques et des médicaments. Une association, l’American 
Industrial Health Council (AIHC), créée à l’automne 1977 par 130 entre-
prises de la chimie et de la pétrochimie, et au premier rang desquelles se 
trouvent Shell, Proctor & Gamble et Monsanto, se fait le porte-parole 
de ce point de vue. Certaines de ces entreprises protestent en effet 
contre l’essor des politiques de régulation qui, au long des années 1970, 
n’ont pas réussi à régler les conflits entourant la gestion des activités de 
production, de commercialisation et les usages des substances potentiel-
lement dangereuses. L’AIHC s’investit dans une série d’actions de lobbying 
qui promeuvent la séparation de la fonction d’identification et de quanti-
fication des risques, de celle du processus de régulation, laquelle inclut 
des jugements sociaux et économiques. L’AIHC se prévaut du soutien du 
président de l’Office of Science and Technology Policy (OSTP), favorable 
à ses analyses 1.
Des réunions régulières entre des membres du groupement industriel 
et des dirigeants de la NAS et du NRC ont lieu fin 1979 et début 1980. 
L’AIHC conduit parallèlement un lobbying actif auprès des membres  
du Congrès et du Sénat, en particulier du président du Subcommittee  
on Agriculture and Related Agencies dépendant du Senate Committee on 
Appropriations. Au final, le Sénat approuve l’allocation d’une somme de 
500 000 dollars pour une étude sur l’analyse de risque (risk assessment). 
Les objectifs que le Sénat assigne à l’étude sont formulés dans des termes 
proches de ceux utilisés par l’AIHC. Le rapport produit par le comité 
d’experts chargé de l’étude, connu sous le nom de « livre rouge » de l’éva-
luation des risques 2, propose une démarche qui est aujourd’hui adoptée 
par la majorité des institutions nationales et internationales en charge 
de la gestion des substances toxiques. Cette démarche se veut rigou-
reuse, fondée sur des méthodes scientifiques, celles de la modélisation 
1. Boudia 2010.
2. NAS-NRC 1983.

	
gouverner un monde contaminé	
389
­mathématique, et applicable à un large ensemble de problèmes et de 
situations. Elle se caractérise par la mise en œuvre d’une procédure par 
étapes qui sépare analyse et décision, par une évaluation des risques, des 
bénéfices et des coûts additionnels de la protection, et d’une optimisation 
qui rend compte des différentes alternatives.
La démarche d’analyse et de gestion des risques développée par le « livre 
rouge » incorpore une certaine conception du politique et du social 1. 
Les problèmes à traiter ne sont pas des controverses ou des contesta-
tions publiques mais des risques faisant l’objet d’un traitement abstrait 
et formalisé. L’analyse des risques propose de prendre le problème en 
toute généralité et d’optimiser son traitement en considérant l’ensemble 
des variables qui le déterminent. Il ne s’agit plus de discuter de la  
pertinence de développer une activité donnée ou du choix de telle ou 
telle politique, mais de prendre pour acquis son développement et de 
travailler sur les meilleures conditions de confinement des problèmes 
qu’elle peut engendrer. Il s’agit de prendre une décision en tenant compte 
des ressources, des contraintes et des possibilités, ainsi que de l’antici-
pation de l’acceptabilité des décisions par les différents acteurs. L’idée qui 
sous-tend l’analyse de risque est qu’il est possible de définir une méthode 
« rationnelle » qui prenne en compte les différents intérêts en présence 
et qui, par une procédure réglée, permette d’aboutir à des consensus 
collectifs et de décider.
Gouverner par l’adaptation
Depuis les années 1980, les technologies du risque se sont régulièrement 
transformées avec l’incorporation d’une série de nouvelles procédures. Les 
comités d’experts d’institutions américaines et d’organisations internatio-
nales, comme l’OCDE, le Codex Alimentarius ou l’Agence internationale 
pour l’énergie atomique (AIEA), ont ainsi régulièrement enrichi l’arsenal 
des outils pouvant servir à l’évaluation et à la prévention des risques – par 
exemple en prenant en compte les incertitudes liées au manque de 
connaissances scientifiques 2. Ainsi, dans le domaine des toxiques, de 
nouveaux modèles d’extrapolation ont été régulièrement développés pour 
répondre aux critiques sur la pertinence de ceux utilisés. Dans d’autres 
domaines, comme celui de la sécurité des sites industriels ou des catas-
trophes naturelles, l’analyse des causes d’accidents routiniers ou majeurs, 
1. Boudia et Demortain 2014.
2. Jasanoff 2006, Demortain 2011.

390	
soraya boudia et nathalie jas
avec retour d’expériences, a été un moyen d’améliorer tel ou tel aspect 
de l’évaluation et de la gestion des risques. Certaines crises, qui pointent 
l’opacité du travail réalisé au sein des comités d’experts et les nombreux 
conflits d’intérêts dans lesquels ces derniers sont pris, ont conduit, dès les 
années 1980 dans certains États américains et au cours des années 1990 
en Europe – notamment sous l’effet de la crise de l’ESB –, à l’adoption de 
démarches plus rigoureuses et plus protectrices. Par ailleurs, les différentes 
agences d’évaluation et de gestion des risques ont investi massivement la 
question de la communication publique afin améliorer la « perception » 
des risques. L’expertise et la régulation des risques prônent désormais la 
« transparence » et la « participation », et cherchent à la fois à informer 
les populations concernées et à les impliquer dans la prise de décision 
sur les niveaux de risque acceptables 1.
Toutefois, ces politiques et dispositifs, qui visent à garantir des niveaux 
de risque « acceptables » socialement, n’ont jamais réussi à construire  
des consensus durables. La réalité matérielle des dégradations a obligé  
les industriels comme les autorités publiques à renouveler leurs modes de 
gouvernement des dangers techniques, sanitaires et environnementaux. 
La montée en puissance de problèmes à caractère global comme le 
changement climatique, la diminution de la biodiversité et la généralisation 
des contaminations, ou la visibilité d’accidents majeurs – de Tchernobyl 
à Fukushima en passant par Bhopal – donnent aux problèmes sanitaires 
et environnementaux une existence quasi permanente dans l’espace 
public. Ces problèmes et leurs conséquences dramatiques nourrissent 
des controverses et contestations sans cesse renouvelées, et obligent les 
autorités publiques et les industries à chercher à étoffer leur arsenal d’ins-
truments de gestion des problèmes et de leurs conséquences politiques. 
C’est que, depuis le début du xxie siècle, un nouveau mode de gouver-
nement des effets délétères des technosciences a émergé, le gouvernement 
par l’adaptation.
Pour saisir le déplacement qui s’opère avec ce nouveau mode d’action, il 
convient de rappeler que le gouvernement par le risque repose sur l’idée 
d’un contrôle de la dangerosité en rendant « naturel » un degré d’atteinte 
jugé « acceptable » à un moment donné. Ce mode de gouvernement suggère 
que soient mises en œuvre des dispositions pour qu’une catastrophe – crise 
sanitaire ou accident industriel majeurs par exemple – n’advienne pas. 
Si des atteintes ne peuvent être radicalement écartées, elles restent peu 
probables. En cas d’atteintes, des dispositifs de compensation et de 
remédiation existent et tentent de réparer les dommages occasionnés.  
1. Irwin et Wynne 1996, Irwin 2006, Pestre 2013.

	
gouverner un monde contaminé	
391
La logique qui structure le gouvernement par l’adaptation est tout autre.  
Ce gouvernement a pour point de départ le constat de l’existence d’un 
monde intrinsèquement dangereux. C’est cette fois l’ampleur de dommages 
plus ou moins irréversibles, comme la quasi-certitude que des catas-
trophes industrielles et environnementales adviendront, qui est naturalisée.  
Il en résulte que, au quotidien, chacun doit faire face à un environnement 
dans lequel les dangers sont multiples et nombreux et avec lesquels il 
doit apprendre à vivre. Le gouvernement par l’adaptation cherche donc, 
non pas à maîtriser le risque, prévenir toute catastrophe ou protéger 
les populations contre tous les dangers, mais avant tout à fournir aux 
populations des outils leur permettant de faire face et de vivre dans  
cet environnement incertain et hostile. Dit autrement, il s’agit d’organiser 
le « vivre avec » – avec les risques, les catastrophes et les contaminations. 
Ce nouveau mode de gouvernement des effets délétères des technos-
ciences met au cœur de sa philosophie et de son discours l’adaptabilité 
des individus et des populations.
Les territoires hautement et durablement affectés – par des catastrophes 
industrielles ou des contaminations pérennes – servent alors de labora-
toires pour expérimenter l’organisation du « vivre avec ». Il ne s’agit plus 
de réhabiliter les sites endommagés par des pollutions importantes – ce 
qui a été rarement fait de manière satisfaisante –, ni de maîtriser les 
effets collatéraux des activités industrielles polluantes, ni de réparer 
et compenser ces effets. La démarche à l’œuvre se traduit au contraire 
par la promotion d’outils devant permettre aux populations de gérer 
et d’organiser leur vie dans des situations de pollutions durables. Cette 
démarche est ainsi par exemple au cœur des politiques récentes engagées 
aux Antilles françaises pour la gestion de la contamination massive et 
pérenne engendrée par l’usage massif d’un insecticide, le chlordécone 1, 
ou dans la gestion des conséquences désastreuses des pollutions radio­
actives causées par l’accident de la centrale nucléaire de Tchernobyl 2. Le 
rôle des autorités politiques et administratives consiste ainsi à fournir des 
informations aux populations permettant à chacun de gérer son niveau 
de contamination et de là le niveau de risque qu’il prend. Des guides sont 
produits et mis à disposition. Ils informent sur les taux de contaminants 
contenus dans les aliments produits localement, poissons, légumes, et 
offrent des recommandations sur qui peut consommer quoi, avec quelle 
fréquence, dans quelles conditions, ou sur comment cultiver. Ils peuvent 
aussi fournir des recommandations sur l’entretien de sa maison, le choix 
1. Torny 2013.
2. Topçu 2013.

392	
soraya boudia et nathalie jas
des moments de sortie dans une journée ou la fréquence de pratique de 
telle ou telle activité. Ce type d’outils n’est pas simplement promu dans les 
zones très contaminées. De telles recommandations existent désormais 
aussi dans des zones « ordinaires » : ne pas boire d’eau provenant d’une 
source qui contient des taux élevés de contaminants, ne pas sortir quand 
un certain niveau de pollution atmosphérique est atteint, ne pas manger 
plus d’un certain nombre de fois dans le mois certains aliments parce 
qu’ils contiennent des contaminants. À chacun, ensuite, de les mettre en 
œuvre ou non, et de gérer les risques qu’il prend avec sa santé.
Ce gouvernement par l’adaptation est nourri par les transformations à 
l’œuvre depuis le milieu des années 1990 dans la gestion internationale 
des crises et catastrophes naturelles. Une série de concepts, auxquels 
sont associés des pratiques, des groupes professionnels et des institu-
tions, ont ainsi acquis progressivement de l’importance. Le premier 
d’entre eux est celui de la preparedness qui avait été développé pendant 
la guerre froide aux États-Unis pour préparer les populations à réagir 
en cas d’attaque atomique, et qui a été ensuite réinvesti dans le domaine 
de la gestion des grandes catastrophes naturelles 1. Il s’agit de donner 
aux populations les outils nécessaires pour agir au mieux dans les situa-
tions d’urgence qu’induisent ces catastrophes. Dans ces politiques, le 
concept de vulnérabilité occupe une place importante 2. Il rend compte 
du caractère multidimensionnel des catastrophes en s’intéressant aux 
relations que les populations entretiennent avec leur environnement et 
aux forces sociales qui les façonnent. Il s’agit de comprendre l’ensemble  
des éléments matériels, mais aussi politiques et sociaux, qui rendent 
certaines populations particulièrement vulnérables à certaines catas-
trophes, voire produisent les catastrophes. Avec le concept de vulnérabilité, 
on assiste à un déplacement de l’attention. Les causes des catastrophes 
naturelles ne sont pas seulement environnementales. Elles sont aussi 
politiques et sociales. C’est sur ces dernières causes qu’il faut agir.
Ce déplacement est encore plus radical avec l’apparition, à la suite de 
celui de vulnérabilité, du concept de résilience 3. Introduit par les Nations 
unies après qu’elles ont déclaré les années 1990 International Decade for 
Natural Disaster Reduction Effort, ce concept est mobilisé pour construire 
un nouveau discours du mode de gestion des catastrophes, qu’elles soient 
naturelles ou qu’elles résultent des activités industrielles 4. Des expressions 
comme sustainable and resilient communities ou building community 
1. Collier et Lakoff 2008.
2. Revet et Langumier 2013.
3. Revet 2009a.
4. Revet 2009b, Cabane 2012.

	
gouverner un monde contaminé	
393
resilience ont ainsi fleuri dans les rapports d’expertise comme dans la 
littérature de recherche. La mise en œuvre des stratégies de résilience 
témoigne de déplacements importants. Il ne s’agit plus de travailler à 
supprimer la catastrophe mais de reconnaître son caractère inéluctable 
et de préparer les individus en leur donnant les outils qui doivent leur 
permettre de réduire eux-mêmes l’impact de la catastrophe à venir, ses 
conséquences immédiates comme à plus long terme. C’est aux individus 
qu’est prêtée cette capacité de résilience et d’adaptation aux conditions 
difficiles que produira la catastrophe. L’État et les industries qui peuvent 
être à l’origine de certaines catastrophes, ou les organisations natio-
nales ou internationales qui interviennent dans les situations de crise, se 
mettent donc en retrait et transfèrent aux individus une part importante 
de la gestion des problèmes. Leur responsabilité se limite désormais à  
la mise à disposition de recommandations sur ce qu’il convient de faire 
ou de ne pas faire dans telle ou telle situation. Le succès que rencontre le 
gouvernement par l’adaptation se traduit par la circulation des concepts. 
L’adaptation est ainsi par exemple devenue le nouveau mot d’ordre 
dans la gestion des effets du changement climatique. La multiplication 
d’appels à projets dont la finalité est le développement d’outils visant à 
aider les individus à faire face aux environnements hostiles dans lesquels 
ils doivent vivre comme la diffusion du concept de résilience dans diffé-
rents univers de la gestion des risques et des catastrophes rendent aussi 
compte du succès de cette approche.
Ces politiques développées par diverses organisations internationales 
et États montrent que l’art de gouverner des populations, des terri-
toires ou des problèmes consiste désormais à « peser sur les conduites » 
de ceux qui sont atteints par les aléas climatiques récurrents, les catas-
trophes, les environnements contaminés, les guerres, voire les crises 
financières et économiques. Ce type de gestion est à rapprocher de 
l’essor d’un gouvernement de soi dans lequel chacun a à considérer son 
corps comme un capital à faire fructifier 1. Toutefois, comme outil de 
gestion d’un environnement pollué ou profondément endommagé, ou 
encore d’un environnement politique, social et économique instable, la 
fructification du capital individuel prend un sens particulier. Elle rend 
chacun responsable de son existence endommagée et de sa capacité à 
réagir dans un univers hostile. Elle consiste surtout à faire de chacun son 
propre « manageur » de risque et met l’individu en demeure de définir 
des solutions devant lui permettre de limiter les effets délétères produits 
par cet univers. Cette situation n’est pas dénuée de paradoxes, voire de 
1. Franklin (dans ce volume, p. 211), Rose 1999, Dean 1999.

394	
soraya boudia et nathalie jas
cynisme. La reconnaissance de la responsabilité individuelle de chacun 
peut en arriver à être présentée comme une solution libératrice, un 
empowerment des individus pour faire face à une situation dont ils ne 
sont pourtant pas responsables et sur laquelle ils n’ont qu’une influence 
très limitée. Dans un monde devenu intrinsèquement dangereux, cette 
interprétation de la mise en œuvre d’une « gouvernementalité néoli-
bérale » se heurte ainsi au fait que, dans bien des cas, ceux qui subissent 
les effets n’ont en pratique d’autre choix que de continuer à vivre dans 
ces conditions potentiellement ou effectivement dangereuses pour leur 
santé et leur vie même.
Conclusion
Pour conclure, nous voudrions tout d’abord souligner que les modes 
de gouvernement que nous avons identifiés tout au long du xxe siècle, 
loin de se remplacer, coexistent, s’hybrident parfois et se sédimentent. 
Ainsi, le discours de la maîtrise et du contrôle n’a pas disparu et est 
régulièrement mobilisé, y compris dans le cadre du gouvernement par 
l’adaptation. Par exemple, le Japon, une année et demie seulement après 
l’accident de Fukushima du 11 mars 2011, a commencé à vendre ses compé-
tences en matière de sécurité des installations nucléaires. En mettant en 
avant l’expérience accumulée lors de l’accident, les entreprises japonaises 
affirment avoir gagné une maîtrise inégalable des questions de sécurité, 
en particulier dans des contextes environnementaux complexes – des 
zones sismiques, de tempêtes, etc. –, compétences qu’elles cherchent 
désormais à valoriser et monnayer. De même, les approches par le risque 
continuent de structurer nombre de systèmes de régulation : sécurité 
des installations industrielles, substances chimiques, biotechnologies, 
nanotechnologies… Enfin, lorsqu’on les examine de près, les dispositifs 
visant à organiser le « vivre et faire avec » reprennent et incorporent, en 
partie, les logiques antérieures de confinement ou d’optimisation de la 
prise de risque.
Nous voudrions pour finir revenir sur un aspect que nous avons assez 
peu abordé dans cet article, les victimes, celles et ceux qui subissent les 
effets de ces dangers techniques, sanitaires et environnementaux. Les 
mobilisations de, et autour de, ces victimes ont été essentielles dans  
la mise en visibilité des effets délétères des déploiements industriels  
et technologiques sans précédents qui ont lieu depuis la fin de la Seconde 
Guerre mondiale. Ces mobilisations ont développé des critiques multi-
formes que les autorités publiques et les industries ont gérées de manière 

	
gouverner un monde contaminé	
395
différenciée, en en intégrant certaines, en en neutralisant d’autres, voire 
en recourant à une répression violente. Ces mobilisations ont largement 
contribué à la création de formes de protection et de précaution, de 
nouvelles législations et de nouveaux droits. Cependant, malgré des 
avancées indéniables, un bilan des méfaits, des compensations pour les 
dommages causés et des réhabilitations des territoires contaminés laisse 
clairement entrevoir que les modes de gouvernement qui ont résulté  
des transformations du xxe siècle n’ont en rien réglé des problèmes qui 
ont été laissés en héritage aux sociétés du xxie siècle 1.
Références bibliographiques
Bernhardt Christoph (dir.), 2004, Environmental Problems in European Cities in the 
19th and 20th Century, Münster, Waxmann.
Boudia Soraya, 2010, Gouverner les risques, gouverner par les risques, mémoire 
d’habilitation à diriger des recherches, université de Strasbourg.
–	 2013a, « From Threshold to Risk : Exposure to Low Dose of Radiation and Its Effects 
on Toxicants Regulation », in  Soraya Boudia et Nathalie Jas (dir.), Toxicants, 
Health and Regulation since 1945, Londres, Pickering & Chatto, p. 71-87.
–	 2013b, « La genèse d’un gouvernement par le risque », in  Dominique Bourg, 
Pierre-Benoît Joly et Alain Kaufmann, Retour sur la société du risque, Paris, PUF, 
p. 57-76.
Boudia Soraya et Demortain David, 2014, « La production d’un instrument 
générique de gouvernement. Le “livre rouge” de l’analyse des risques », Gouver-
nement et action publique, vol. 3, no 3, p. 33-53.
Boudia Soraya et Jas Nathalie (dir.), 2007, « Risk and Risk Society in Historical 
Perspective », dossier thématique de History and Technology, vol. 23, no 4, p. 317- 
331.
–	 (dir.), 2013, Toxicants, Health and Regulation since 1945, Londres, Pickering & 
Chatto.
–	 (dir.), 2014, Powerless Science ? Science and Politics in a Toxic World, New York et 
Oxford, Berghahn Books.
Brooks Karl Boyd, 2009, Before Earth Day : The Origins of American Environmental 
Law (1945-1970), Lawrence (KS), University Press of Kansas.
Brown Phil et Mikkelsen Edwin J., 1990, No Safe Place : Toxic Waste, Leukemia, and 
Community Action, Berkeley (CA), California University Press.
Buzzi Stéphane, Devinck Jean-Claude et Rosental Paul-André (dir.), 2006, La Santé 
au travail (1880-2006), Paris, La Découverte.
Cabane Lydie, 2012, Gouverner les catastrophes. Politiques, savoirs et organisation 
de la gestion des catastrophes en Afrique du Sud, thèse de doctorat de sociologie, 
Sciences Po, Paris.
Carpenter Daniel P., 2001, The Forging of Bureaucratic Autonomy : Reputations, 
Networks, and Policy Innovation in Executive Agencies (1862-1928), Princeton (NJ), 
Princeton University Press.
1. Pestre et Fressoz 2013.

396	
soraya boudia et nathalie jas
Carson Rachel, 2014 [1962], Printemps silencieux, Wildproject Éditions.
Collier Stephen J. et Lakoff Andrew, 2008, « Distributed Preparedness : The Spatial 
Logic of Domestic Security in the United States », Environment and Planning D : 
Society and Space, vol. 26, no 1, p. 7-28.
Cranor Carl, 1993, Regulating Toxic Substances : A Philosophy of Science and the 
Law, Oxford, Oxford University Press.
Dean Mitchell, 1999, Governmentality : Power and Rule in Modern Society, Londres, 
Sage.
Demortain David, 2011, Scientists and the Regulation of Risk : Standardising Control, 
Cheltenham et Aldershot, Edward Elgar Publishing.
Dessaux Pierre Antoine, 2007, « Chemical Expertise and Food Market Regulation in 
Belle Époque, France », History and Technology, vol. 23, no 4, p. 351-368.
Fressoz Jean-Baptiste, 2012, L’Apocalypse joyeuse. Une histoire du risque technolo-
gique, Paris, Seuil.
Hays Samuel, 1987, Beauty, Health, and Permanence : Environmental Politics in the 
United States (1955-1985), Cambridge, Cambridge University Press.
IARC, 1972, IARC Monographs on the Evaluation of Carcinogenic Risk of Chemicals 
to Man, Genève, IARC / WHO, vol. 1.
Irwin Alan, 2006, « The Politics of Talk : Coming to Terms with the “New” Scientific 
Governance », Social Studies of Science, vol. 36, no 2, p. 299-320.
Irwin Alan et Wynne Brian (dir.), 1996, Misunderstanding Science ? The Public 
Reconstruction of Science and Technology, Cambridge, Cambridge University  
Press.
Jas Nathalie, 2013, « Adapting to Reality : The Emergence of an International 
Expertise on Food Additives and Contaminants in the 1950’s and Early 1960’s », 
in Soraya Boudia et Nathalie Jas (dir.), Toxicants, Health and Regulation since 
1945, Londres, Pickering & Chatto, p. 47-69.
Jasanoff Sheila, 1990, The Fifth Branch : Science Advisers as Policymakers, Cambridge 
(MA), Harvard University Press.
–	 1992, « Science, Politics, and the Renegotiation of Expertise at EPA », Osiris, vol. 7, 
p. 194-217.
Jones Charles O., 1975, Clean Air : The Policies and Politics of Pollution Control, 
Pittsburgh (PA), University of Pittsburgh Press.
Keck Margaret, 2005, Activists beyond Borders : Advocacy Networks in International 
Politics, Ithaca (NY), Cornell University Press.
Le Roux Thomas, 2011, Le Laboratoire des pollutions industrielles. Paris (1770-
1830), Paris, Albin Michel.
Massard-Guilbaud Geneviève, 2010, Histoire de la pollution industrielle en France 
(1789-1914), Paris, Éd. de l’EHESS.
Milazzo Paul Charles, 2006, Unlikely Environmentalists : Congress and Clean Water 
(1945-1972), Lawrence (KS), University Press of Kansas.
NAS-NRC, 1983, Risk Assessment in the Federal Government : Managing the Process, 
Washington (DC), National Academy Press.
Pellow David Naguib, 2007, Resisting Global Toxics : Transnational Movements for 
Environmental Justice, Cambridge (MA), MIT Press.
Pestre Dominique, 2013, À contre-science. Politiques et savoirs des sociétés contem-
poraines, Paris, Seuil.
Pestre Dominique et Fressoz Jean-Baptiste, 2013, « Critique historique du satisfecit 
postmoderne. Risque et “société du risque” depuis deux siècles », in Dominique 

	
gouverner un monde contaminé	
397
Bourg, Pierre-Benoît Joly et Alain Kaufmann (dir.), Retour sur la société du 
risque, Paris, PUF, p. 19-56.
Revet Sandrine, 2009a, « De la vulnérabilité aux vulnérables. Approche critique 
d’une notion performative », in  Sylvia Becerra et Anne Peltier (dir.), Risque 
et environnement. Recherches interdisciplinaires sur la vulnérabilité des sociétés, 
Paris, L’Harmattan, p. 89-99.
–	 2009b, « Les organisations internationales et la gestion des risques et des catas-
trophes “naturels” », Études du CERI, no 157.
Revet Sandrine et Langumier Julien (dir.), 2013, Le Gouvernement des catastrophes, 
Paris, Karthala.
Rose Nikolas, 1999, Powers of Freedom : Reframing Political Thought, Cambridge, 
Cambridge University Press.
Ross Benjamin et Amter Steven, 2010, The Polluters : The Making of Our Chemically 
Altered Environment, Oxford, Oxford University Press.
Sellers Christopher, 1997, Hazards of the Job : From Industrial Disease to Environ-
mental Health Science, Chapel Hill (NC), University of North Carolina Press.
Tarrow Sidney, 1998, The New Transnational Activism, Cambridge, Cambridge 
University Press.
Topçu Sezin, 2013, « Chernobyl Empowerment ? Exporting “Participatory Gover-
nance” to Contaminated Territories », in  Soraya Boudia et Nathalie Jas (dir.), 
Toxicants, Health and Regulation since 1945, Londres, Pickering & Chatto, p. 135- 
158.
Torny Didier, 2013, « Managing an Everlastingly Polluted World : Food Policies and 
Community Health Actions in the French West Indies », in  Soraya Boudia et 
Nathalie Jas (dir.), Toxicants, Health and Regulation since 1945, Londres, Pickering 
& Chatto, p. 117-134.
Vogel Sarah, 2012, Is It Safe ? BPA and the Struggle to Define the Safety of Chemicals, 
Berkeley (CA), University of California Press.
Whorton James, 1974, Before Silent Spring : Pesticides and Public Health in Pre-DDT 
America, Princeton (NJ), Princeton University Press.


19 Gouverner le système Terre
P a u l  N .  E d wa r d s
En 2007, le prix Nobel de la paix est attribué conjointement au 
Groupement d’experts intergouvernemental sur l’évolution du climat 
(GIEC) et à l’ancien vice-président des États-Unis Al Gore. Le GIEC, 
composé de conseillers scientifiques auprès de la Convention-cadre 
des Nations unies sur les changements climatiques (CCNUCC), réalise 
depuis 1990 des évaluations scientifiques sur les causes et conséquences 
du changement climatique global. En 2006, Al Gore présente un aperçu 
des savoirs scientifiques sur le changement climatique d’origine humaine 
dans son documentaire Une vérité qui dérange, largement diffusé dans 
le monde.
Ce prix Nobel est révélateur de la convergence entre sciences du système 
Terre et gouvernance environnementale apparue dans les dernières 
décennies du xxe siècle. Ce chapitre commence par une brève introduction 
sur les sciences de la Terre avant 1945. Il montre ensuite le développement 
des sciences du système Terre entre 1945 et 1990, en grande partie dû à 
l’intérêt suscité dans les cercles politiques et militaires pendant la guerre 
froide (soucieux notamment de la question des retombées radioactives), 
accompagné de la construction d’infrastructures mondiales de surveil-
lance environnementale et d’analyse de données. Dans la deuxième 
partie de cette période, les inquiétudes liées à la destruction de la couche 
d’ozone, à l’« hiver nucléaire » et au réchauffement climatique marquent 
l’entrée des sciences du système Terre dans le débat politique. Avec les 
progrès réalisés dans les années 1970, les modélisations par simulation 
informatique sont amenées à jouer un rôle croissant dans la gouvernance 
environnementale.
Il existe un schéma dans lequel les scientifiques lancent des alertes et 
en appellent à l’action politique par le biais d’agences des Nations unies 
et/ou de sociétés scientifiques internationales, mais aussi, parfois avec 
succès, via des organisations non gouvernementales ou technocratiques 
 Visualisation du « trou » dans la couche d’ozone par la NASA, 11 septembre 2005.

400	
paul n. edwards
privées comme le Club de Rome. Au fil du temps, diverses structures de 
gouvernance sont mises en œuvre. Des succès remarquables sont obtenus 
dans certains cas, comme avec le protocole de Montréal sur la couche 
d’ozone en 1988. Cependant, depuis 1990, les tentatives pour reproduire 
ces succès se heurtent à de nombreuses difficultés, peut-être insurmon-
tables, en matière de gouvernance globale.
Les sciences de la Terre avant 1945
Le xixe siècle est le théâtre de nombreuses avancées dans le domaine des 
sciences de la Terre, allant de la géologie au géomagnétisme en passant par 
la météorologie. Dans les années 1820, Fourier est le premier à caracté-
riser le piège à chaleur que constitue l’« effet de serre ». L’uniformitarisme 
géologique d’Hutton et Lyell jette une lumière nouvelle sur l’histoire de la 
Terre, plus longue et plus variée qu’auparavant, avec des oscillations impor-
tantes entre périodes de glaciation et climat chaud du Crétacé. À partir des 
années 1860, Tyndall puis, plus tard, Chamberlin étudient la régulation 
de la température de la Terre par les gaz, notamment la vapeur d’eau et 
le dioxyde de carbone. Ils théorisent ainsi que le changement climatique 
géologique résulte d’un cycle du carbone impliquant des dégagements 
gazeux liés aux éruptions volcaniques, l’absorption par des organismes 
marins, des dépôts calcaires, des perturbations de surface, et l’érosion des 
roches calcaires qui boucle le cycle par renvoi du carbone dans l’atmo­
sphère. En 1895, Arrhenius évalue les effets du dioxyde de carbone sur le 
climat, aboutissant à la conclusion qu’un doublement du CO2 entraînerait 
une augmentation de 5 à 6 °C de la température de la Terre (une valeur à 
peine supérieure à la fourchette allant de 2 à 4,5 °C avancée aujourd’hui). 
Il soutient l’idée selon laquelle l’utilisation de combustibles fossiles par  
les hommes peut finir par provoquer une augmentation de température 1. 
En 1883, von Hann propose dans son Manuel de climatologie (le principal 
manuel du domaine pour les cinquante années qui suivent) une vision 
des forces d’échelle planétaire responsables de la répartition du rayon-
nement solaire, de la température, des précipitations, etc.
La seconde moitié du xixe siècle voit naître de nombreuses organi-
sations scientifiques internationales. Des spécialistes des milieux 
marins embarquent dès les années 1850 sur des bateaux pour collecter 
de manière systématique des données météorologiques et océanogra-
phiques, produisant ainsi un enregistrement global continu. De nombreux 
1. Weart 2003.

	
gouverner le système terre	
401
observatoires astronomiques ont également des stations météo dont ils 
conservent les enregistrements. Les directeurs des services nationaux 
de météorologie, constitués en Europe et aux États-Unis dans les années 
1840, fondent l’Organisation météorologique internationale (OMI) 
en 1873. La première Année polaire internationale (API, 1882-1883) 
implique des scientifiques de 11 pays qui établissent 12 stations de 
recherche à l’intérieur du cercle arctique. Bien que marqués par le drame 
d’une expédition américaine pendant laquelle 17 hommes meurent de 
faim, les scientifiques de l’API collectent des quantités importantes  
de données météorologiques, géomagnétiques et orales. Des concepts 
comme la « science mondiale » et la « géophysique », visant à unifier les 
nombreuses sciences concernées par les phénomènes à l’échelle de la 
Terre, apparaissent tout au long du xixe siècle, mais leurs promoteurs 
ne réussissent généralement pas à obtenir d’engagements institutionnels 
avant le début du xxe siècle. La fondation de l’Union géodésique et géo­­
physique internationale en 1919 marque la stabilisation d’une « assemblée » 
interdisciplinaire associant géodésie, géomagnétisme, sismologie, météo-
rologie, hydrologie et océanographie 1.
À leurs débuts, les sciences géophysiques ont des ambitions qui excèdent 
souvent les capacités disponibles. Par exemple, Léon Teisserenc de Bort, 
avec son idée d’un « Réseau mondial » pour la météorologie, envisage 
en 1905 un système de collecte de données globales en temps réel basé 
sur le télégraphe. En fait, le réseau mondial réellement développé par 
l’OMI consistait simplement en un réseau climatologique fonctionnant 
grâce au courrier, dans lequel la rédaction des rapports annuels prenait 
généralement plusieurs années. L’enthousiasme suscité par le traitement 
mécanique des données grâce au système de cartes perforées atteint des 
sommets dans les années 1930 : dans un délire d’optimisme, le service 
météorologique tchèque propose ainsi de faire des cartes perforées  
pour les données météorologiques du monde entier 2. Une deuxième 
API a lieu en 1932-1933, avec 44 pays participants, mais elle doit revoir 
ses ambitions à la baisse à cause de la crise financière et de la Grande 
Dépression 3.
Cette période qui précéde la Seconde Guerre mondiale est fondatrice, 
avec la définition de concepts importants, l’institutionnalisation des 
liens entre disciplines et la création de structures et de normes interna-
tionales. L’objectif partagé de ce « globalisme informationnel » (collecte 
1. Good 2000.
2. Edwards 2010.
3. Elzinga 2010.

402	
paul n. edwards
de données sur la planète dans son ensemble) ne peut pas toutefois être 
atteint avant que ne soit adopté un « globalisme infrastructurel », avec 
des dispositifs institutionnels et technologiques permanents et unifiés 
produisant des informations globales 1. De nombreux projets de ce type 
sont certes lancés bien avant la Première Guerre mondiale, mais très 
peu d’entre eux aboutissent avant les années 1950. La « friction des 
données » (conflits de normes, disparités des pratiques et incompati-
bilité des technologies) fait obstacle aux efforts visant à produire les 
données globales. Dans le même temps, la « friction computationnelle » 
(mélange de sources hétérogènes dans des modèles de données) empêche 
de rendre les données globales à une époque où les calculs s’effectuent 
par des moyens manuels et mécaniques 2. La période est tout de même 
marquée par quelques succès. Au cours des deux guerres mondiales, 
les armées, qui ont besoin de prévisions météorologiques fiables, de 
cartes des courants océaniques et d’autres informations géophysiques, 
développent des réseaux d’observation et de communication et permettent 
une professionnalisation de la météorologie et de l’océanographie 3. Dès 
les années 1930, des données météorologiques de base sont déjà dispo-
nibles pour une grande partie de la surface continentale via un réseau 
décentralisé basé sur le télégraphe, le télex, les radios ondes courtes et  
d’autres médias.
L’idée des impacts humains sur le climat remonte au moins au  
xviiie siècle. Ces impacts sont alors considérés comme ayant une dimension 
régionale plutôt que globale. Cependant, dans certains domaines impor-
tants, comme la gestion forestière, les scientifiques pensent qu’ils détectent 
des effets d’origine anthropique globaux et certains gouvernements 
envisagent des politiques visant à les limiter 4. Toutefois, ces interventions 
n’atteignent un niveau de gouvernance transnationale que bien après la 
Seconde Guerre mondiale.
Les géosciences et la gouvernance  
environnementale : 1945-1990
La première moitié de la guerre froide est marquée par la mise en 
place d’infrastructures de savoir concevant la Terre comme un ensemble 
1. Edwards 2006.
2. Edwards 2010.
3. Fleming 1996, Mukerji 1989, Weart 2003.
4. Brückner et al. 2000, Fleming 1998, Grove 1997, Locher et Fressoz 2013.

	
gouverner le système terre	
403
de systèmes physiques liés 1. Vers 1970, les géosciences commencent à 
s’inquiéter des impacts humains sur les systèmes globaux, comme la 
dégradation de la couche d’ozone, l’hiver nucléaire et le réchauffement 
climatique. À partir des années 1980, elles jouent un rôle majeur dans la 
gouvernance environnementale globale.
En plus de la guerre froide elle-même, cette période est marquée 
par trois évolutions. D’abord, le système des Nations unies donne un 
cadre institutionnel à la standardisation de la surveillance environne-
mentale globale, ainsi qu’une structure pour la gouvernance éventuelle 
des problèmes environnementaux globaux. En second lieu, des projets 
de recherche majeurs tels que l’Année géophysique internationale et le 
Programme mondial de recherche atmosphérique apportent la démons-
tration de l’utilité d’infrastructures permanentes pour la surveillance 
environnementale globale. Enfin, le développement de la pensée des 
systèmes associée à la modélisation informatique aboutit à des techniques 
permettant à la fois de rassembler des données globales et de simuler 
des scénarios pour le futur.
La guerre froide
Les sciences du système Terre d’aujourd’hui doivent beaucoup à leur 
relation étroite et complexe à la guerre froide 2. Cette période est carac-
térisée par ce que j’ai appelé un « discours du monde fini » associant 
les objectifs globaux et hégémoniques, américains et soviétiques, une 
stratégie de déploiement technologique, un discours apocalyptique et 
un langage des systèmes intégrés 3. Le discours du monde fini implique 
aussi que les militaires ont besoin non seulement de capacités techno-
logiques, mais aussi de savoirs leur permettant de surveiller la planète 
entière et de projeter leur force à tout endroit 4.
Aux États-Unis, en Grande-Bretagne et en Union soviétique, la 
Seconde Guerre mondiale propulse les scientifiques et les ingénieurs à 
des positions de pouvoir. La recherche militaire contribue au dévelop-
pement rapide de la météorologie (pour les prévisions et le contrôle 
du temps), l’océanographie (pour la guerre navale et sous-marine), 
la géodésie (pour le guidage des missiles) et la sismologie (pour la 
1. Edwards 2010.
2. Barth 2003, Cloud 2003, Doel 2003, Krige et Barth 2006.
3. Edwards 1996.
4. Doel et Harper 2006, Hamblin 2000, Harper 2008, McDougall 1985, Miller 2001a, Needell 
et al. 1992, Turchetti 2012.

404	
paul n. edwards
détection des essais nucléaires) 1. Et le contrôle des armes nucléaires 
est à l’origine des premiers mécanismes de gouvernance environne-
mentale mondiale.
Dans les années 1950, les programmes d’armement nucléaire connaissent 
un développement rapide. Les retombées radioactives des essais nucléaires 
(ou d’une guerre éventuelle) commencent à susciter de graves inquiétudes, 
et les militaires s’intéressent à l’utilisation des retombées radioactives 
pour détecter les essais et préparer la défense civile. Les États-Unis, la 
Grande-Bretagne et l’Organisation météorologique mondiale bâtissent 
des systèmes à l’échelle mondiale pour surveiller les dépôts radioactifs  
au niveau du sol. On commence à suivre les déplacements de paquets 
d’air particuliers autour du globe, et les stations de surveillance aériennes, 
comme les ballons de haute altitude, permettent aux météorologues 
de connaître la structure de la stratosphère. Le carbone 14 radioactif 
produit par les essais nucléaires facilite également le suivi du parcours 
du carbone de l’atmosphère aux océans, aux plantes et aux animaux. 
Dans les années 1960, ce besoin de comprendre le chemin probable des 
retombées radioactives après un échange nucléaire entraîne le dévelop-
pement des modèles de transport atmosphérique qui seront utilisés  
par la suite pour étudier la pollution de l’air en ville. Ces études du cycle 
du carbone représentent finalement un des premiers croisements entre 
géophysique et sciences de la vie 2.
L’existence de systèmes de surveillance sismiques et atmosphériques 
capables de détecter les essais nucléaires n’importe où dans le monde 
constitue une condition sine qua non de l’application du Traité d’inter-
diction partielle des essais nucléaires de 1963 (TIP). En fait, de longs débats 
portent sur une interdiction totale des essais mais les États-Unis estiment 
que la surveillance sismique est insuffisamment sensible pour permettre 
une interdiction des essais souterrains. L’interdiction s’applique donc 
seulement aux essais atmosphériques, sous-marins (facilement détec-
tables à l’aide de dispositifs de contrôle hydroacoustiques et sismiques) 
et extra-atmosphériques 3. En ce sens, le TIP peut être considéré comme 
le premier traité international sur l’environnement.
L’idée de ce que nous désignons aujourd’hui par le terme de « géo- 
ingénierie » émerge également au cours de la guerre froide. C’est à cette 
époque que certains scientifiques comme Langmuir et Teller imaginent 
guider des tempêtes à l’aide d’explosions nucléaires, ensemencer les 
1. Barth 2003, Cloud 2002, Edwards 2012, Fleming 2010, Mukerji 1989, Oreskes 2003.
2. Edwards 2012, Machta 2002.
3. Cette interdiction ne fut toutefois pas respectée par tous les pays ; la France et la Chine ont 
poursuivi leurs essais atmosphériques respectivement jusqu’en 1974 et 1980.

	
gouverner le système terre	
405
nuages pour enliser les troupes ennemies dans la boue, ou recourir à 
la guerre climatique en privant un pays de précipitations. En 1955, von 
Neumann déclare :
Il ne fait guère de doute que l’on pourrait […] intervenir à n’importe quelle 
échelle souhaitée et finalement obtenir des résultats assez extraordinaires. 
Le climat et les régimes de précipitation de certaines régions spécifiques 
pourraient être modifiés. […] Plus que les guerres récentes ou à venir, ou 
encore que l’économie à n’importe quel moment, la portée de telles actions 
serait directement et véritablement mondiale […] 4.
Aucune tentative de modification délibérée du climat n’a eu lieu mais la 
modification de la météo (ensemencement des nuages) a été fréquemment 
utilisée pendant la guerre du Vietnam (avec des résultats peu concluants 
dans l’ensemble) et des projets militaires de contrôle des tempêtes ont 
continué dans les années 1970 5.
Les Nations unies : institutions d’experts  
et mécanismes de gouvernance
Après la Seconde Guerre mondiale, de nombreuses institutions inter-
gouvernementales se développent sous l’égide de la nouvelle Organisation 
des Nations unies. La plupart sont des organisations d’experts, souvent 
issues d’institutions plus anciennes telles que l’Organisation météorolo-
gique internationale ou l’Institut international d’agriculture. Le système 
intergouvernemental de l’ONU confère une grande autorité à ces organi-
sations d’experts qui propagent les normes, collectent les données et 
promeuvent des pratiques uniformes.
Dans son principe, le système de l’ONU remet en cause le système 
westphalien de souveraineté basé sur le territoire 6. L’Organisation 
météorologique mondiale (OMM), issue en 1950 de l’OMI, non gouver-
nementale, est confrontée à cette question de la souveraineté au cours 
de ses dix premières années alors qu’elle s’essaie à standardiser les 
pratiques météorologiques. Dans les discussions sur l’établissement des 
normes, ses membres se disputent sur le sens de mots comme « doit », 
« devrait » ou « obligation ». Certaines normes sont finalement adoptées 
4. Neumann 1955.
5. Fleming 2010.
6. Les grandes puissances ne se sont sans doute peut-être jamais véritablement senties tenues 
par l’ONU. Le Conseil de sécurité et le droit de veto dont elles disposent dans cette instance 
sont le reflet de leur position de domination.

406	
paul n. edwards
tandis que d’autres restent mouvantes et créent une forme de « friction 
des données » empêchant la mondialisation du système de prévisions 
météorologiques. Au fil du temps, néanmoins, l’autorité et l’organi-
sation centralisée de l’OMM permettent de transformer les prévisions 
météorologiques et la surveillance du climat en entreprises globales, 
coordonnées et normalisées 1.
La Conférence des Nations unies sur l’environnement humain de 
Stockholm en 1972 est considérée par beaucoup comme un tournant 
essentiel dans l’histoire de l’environnementalisme. Elle a des effets impor-
tants sur les gouvernements européens et aboutit à la mise en place du 
Programme des Nations unies pour l’environnement, qui renforce les 
programmes de l’OMM en matière de systèmes de surveillance globaux. 
Dans les années 1980, la commission Brundtland de l’ONU essaie d’arti-
culer une voie équilibrée permettant le développement économique et 
la justice sociale tout en préservant l’environnement sur le long terme, 
soulignant que ces problèmes étroitement liés nécessitent des solutions 
complémentaires 2.
Plus important peut-être, les mécanismes de gouvernance des Nations 
unies créent un cadre pour les traités de régulation du patrimoine environ-
nemental mondial. À partir des années 1950, une série de conventions sur 
le droit de la mer traite des ressources minérales (y compris le pétrole), des 
ressources halieutiques et de la pollution marine. Alors que les preuves 
augmentent à propos du rôle des chlorofluocarbures (CFC) dans la  
disparition de la couche d’ozone, des mécanismes similaires permettent  
la négociation rapide de la Convention de Vienne, signée en 1985 et ratifiée 
en 1988 (actuellement ratifiée par 197 pays, la Convention de Vienne est 
l’un des traités les plus réussis de l’histoire mondiale).
La négociation de ce traité se déroule en deux temps : une conven-
tion-cadre commence par établir les règles de base et engage les signataires 
à négocier des limites sur les produits chimiques responsables de la 
disparition de la couche d’ozone. Ensuite, une série de réunions entre 
responsables nationaux en consultation avec des organismes scienti-
fiques aboutit au protocole de Montréal sur la protection de la couche 
d’ozone 3. La Convention de Vienne a l’effet escompté : les concentra-
tions de produits chimiques appauvrissant la couche d’ozone baissent 
de manière régulière depuis 1995 et les premiers signes de récupération 
du « trou » situé au-dessus de l’Antarctique sont détectés en 2011 4. 
1. Edwards 2006, Miller 2001a.
2. World Commission on Environment and Development 1987.
3. Parson 2003.
4. Salby et al. 2011.

	
gouverner le système terre	
407
Cependant, le fait que les CFC soient seulement produits par un nombre 
limité d’entreprises, et que des produits chimiques de substitution soient 
déjà disponibles (et sont rentables pour le « premier entrant », la société 
DuPont), rend ce cas particulier par rapport à tous les autres problèmes 
environnementaux au niveau mondial.
Infrastructures scientifiques globales
Les projets scientifiques internationaux de grande échelle jouent un  
rôle central pendant la guerre froide. D’un côté, ils séduisent scientifiques 
et citoyens en tant qu’instruments de paix et de compréhension mutuelle. 
D’un autre côté, ils servent les intérêts de gouvernements voulant être 
perçus comme avisés et avancés sur le plan technologique. Cette partie 
en présente quelques exemples parmi les plus importants.
L’Année géophysique internationale (AGI) (1957-1958) est imaginée 
en 1950 par des géophysiciens. Son organisation commence par un 
appel au Conseil international des unions scientifiques (CIUS) 1. L’Orga-
nisation météorologique internationale y apporte rapidement son 
soutien, à condition que le projet soit étendu au monde entier et non 
simplement aux régions polaires. Cela aboutit à la saisissante « hypothèse 
du système physique unique » de l’AGI, à savoir que la terre ferme, les 
océans, l’atmosphère, la magnétosphère et la cryosphère ne forment 
qu’un unique système Terre. La plupart des pays du monde participent 
à cette initiative.
L’AGI est conçue en partie pour faire contrepoids au nationalisme et à 
la guerre froide 2. Le CIUS met en place des centres mondiaux de données 
(CMD) hébergés par des agences nationales pour stocker et répartir  
les données. De manière générale, il en existe au moins deux par disci-
pline, l’un situé dans la sphère d’influence soviétique et l’autre dans la 
sphère d’influence occidentale. La complémentarité des responsabilités 
entre CMD garantit une coopération continue, ce qui constitue un moyen  
de dépasser les divisions de la guerre froide. Conçus à l’origine pour prendre 
en charge les données des systèmes d’observation et des expériences de 
l’AGI, un grand nombre de CMD évoluent pour devenir des entrepôts  
de données permanents, formant de facto une infrastructure mondiale de 
données pour les sciences du système Terre. En 2009, le CIUS fait évoluer 
les CMD pour créer le Système mondial de données. En attendant, le 
programme de fusées et de satellites de l’AGI sert de couverture pour les 
1. Needell et al. 1992.
2. Krige 2006, Miller 2006, Needell 2000.

408	
paul n. edwards
entreprises de surveillance militaire. Le lancement réussi du Spoutnik 
en 1957 est ainsi annoncé par les Soviétiques pendant une réception de 
l’AGI organisée à Washington 1.
L’exploration et les recherches menées sous l’égide de l’AGI en Antarc-
tique ont pour aboutissement le traité sur l’Antarctique de 1959, signé au 
départ par les 12 pays présents sur place, par 38 autres par la suite. Le traité 
réserve le continent aux seules activités scientifiques, gèle toute reven-
dication territoriale et interdit les activités militaires. Il s’agit en réalité 
du premier accord de contrôle des armements de la guerre froide 2. Les 
zones polaires du nord du globe restent en revanche la scène d’un conflit 
potentiel entre États-Unis et Union soviétique, et elles sont rapidement 
truffées de stations d’observation, de réseaux de radars et même d’une 
« ville sous la glace » alimentée par un réacteur nucléaire, au Groenland 3.
L’avènement de l’ère de la surveillance par satellite accompagnant  
l’AGI suscite l’intérêt des météorologues. Galvanisée par le discours 
de John F. Kennedy sur le contrôle des armements devant les Nations 
unies en 1963, dans lequel il promet « des efforts accrus de coopération 
entre les nations dans le domaine des prévisions météorologiques et 
éventuellement du contrôle de la météo », l’OMM commence à imaginer  
un système de veille météorologique mondial intégrant les agences natio-
nales au sein d’un système global d’observation, de communication et de 
traitement des données devant être opérationnel en 1968.
Le Programme de recherche sur l’atmosphère globale (GARP) de 
l’OMM-CIUS lance l’Expérience tropicale du GARP dans l’Atlantique 
(ETGA) à l’été 1974. Des navires et des avions, avec 4 000 personnes  
de 20 pays différents, parcourent l’océan Atlantique tropical entre 
les côtes occidentales d’Afrique et les côtes orientales d’Amérique du  
Sud. En 1978-1979, cette Expérience devient le premier test à grande 
échelle de météorologie par satellite 4, et on a dit d’elle qu’elle « consti-
tuait la plus grande concentration de ressources scientifiques jamais 
assemblée », avec 140 pays participants 5. Le domaine de la cartographie 
est déjà en pleine transformation suite au lancement des premiers satel-
lites d’imagerie Landsat en 1972. Signe du rôle que devront jouer ces 
systèmes d’imagerie dans la gouvernance du système Terre, les respon-
sables parlent du programme Landsat comme de « la première étape 
de la fusion des technologies spatiales et de détection à distance dans 
1. McDougall 1985.
2. Naylor et al. 2008.
3. Nielsen et al. 2014.
4. Edwards 2010 (chap. 9).
5. National Oceanic and Atmospheric Administration 1981 (p. iii).

	
gouverner le système terre	
409
un système permettant d’inventorier et de gérer les ressources de  
la Terre 1 ».
Les systèmes d’observation par satellite rendent possible la réalisation 
de mesures uniformes, répétées et calibrées de la végétation, de la glace de 
mer, de la neige et de nombreuses autres caractéristiques géographiques 
et géophysiques. Dans le même temps, ils permettent une surveillance 
uniforme des activités humaines à grande échelle comme l’agriculture, 
l’exploitation minière, la déforestation et les barrages. Les données des 
satellites météo civils sont pour la plupart librement partagées entre 
les nations. Landsat et les autres programmes de satellites environne-
mentaux rendent également leurs données accessibles à une utilisation 
inter­nationale. En revanche, les données de satellites militaires restent 
secrètes.
Au milieu des années 1980, un certain nombre d’infrastructures globales 
de surveillance de la Terre arrivent ainsi à maturité. Il s’agit généralement 
de réseaux d’institutions nationales organisés de manière assez libre  
par des agences mondiales telles que le CIUS et l’OMM. Les principaux 
pays développés fournissent l’essentiel des ressources, même si d’autres 
pays y participent. Une séparation marquée demeure toutefois entre  
les pays du Nord et le Sud, où les capacités scientifiques restent limitées. 
Le soutien de l’OMM permet toutefois de mettre en place des services 
météorologiques dans de nombreux pays en développement, même 
si l’augmentation des coûts et le caractère hautement technique de la 
recherche cantonnent souvent leur travail à un rôle de soutien.
Modéliser le système Terre  
grâce à la simulation informatique
L’environnementalisme sous sa forme moderne émerge dans les années 
1960 dans le sillage des inquiétudes liées aux « pollutions », d’abord 
vécues comme locales. À la fin de la décennie, toutefois, des notions telles  
que « Terre entière », « vaisseau spatial Terre » ou « penser global » sont 
déjà d’usage courant. La prise de conscience de l’existence de problèmes 
environnementaux globaux commence à émerger dans les années 
1970, soutenue par le développement des techniques de simulation par 
ordinateur 2.
Avec les chercheurs en armes nucléaires, les prévisionnistes météo 
1. Williams et Carter 1976 (p. iii) ; voir également Cloud 2002, Mack 1990.
2. Sur le développement généralisé de la « pensée des systèmes », voir Hughes (A.C.) et Hughes 
(T.P.) 2000.

410	
paul n. edwards
sont parmi les premiers à développer des simulations sur ordinateur – les 
premières prévisions météorologiques sont exécutées en 1950 sur l’ENIAC, 
le premier ordinateur électronique américain. En 1970, les modèles  
de prévisions météorologiques permettent de simuler l’atmosphère du 
globe dans son entier. Cela entraîne une amélioration rapide et specta-
culaire de la qualité des prévisions, levant le doute quant à leur valeur et 
leur intérêt. La modélisation du climat (simulant la circulation atmos-
phérique globale sur des décennies ou plus) apparaît dans les années 
1960, transformant radicalement les méthodes de la climatologie 1. Les 
océans jouant un rôle majeur dans le système climatique, les scientifiques 
combinent les modèles atmosphériques aux modèles de circulation des 
océans dans les années 1970.
Les premiers modèles climatiques accentuent les inquiétudes à  
propos du réchauffement de la planète provoqué par l’augmentation du 
dioxyde de carbone dans l’atmosphère. En 1970 et en 1971, dans le cadre 
de la préparation de la Conférence des Nations unies sur l’environnement 
humain, deux rapports rédigés par des géophysiciens et des spécialistes 
de l’environnement européens, états-uniens et japonais expriment des 
préoccupations quant à la « modification par inadvertance du climat 2 ». 
Dès 1979, un consensus émerge sur la probable augmentation globale 
des températures de 1,5° à 4,5 °C en cas de doublement des concentra-
tions en dioxyde de carbone, une fourchette de prévisions qui n’a que 
peu évolué depuis 3.
Vers 1970, les projets de construction de grandes flottes d’avions 
supersoniques (SST) suscitent des inquiétudes sur la dégradation de  
la couche d’ozone, ainsi que sur l’éventuel refroidissement de la planète 
provoqué par les aérosols contenus dans les gaz d’échappement des 
avions. La controverse sur les SST aux États-Unis est à l’origine du 
plus grand programme d’évaluation environnementale conduit à ce 
jour, le CIAP (Climate Impact Assessment Program), d’une durée de 
trois ans et doté d’un budget de 20 millions de dollars. Des membres 
de ce groupe poursuivent leur travail dans les années 1980 en créant 
des simulations informatiques de l’effet refroidissant des fumées et des 
poussières rejetées dans la stratosphère en cas d’échange nucléaire majeur. 
1. Edwards 2010, Weart 2013.
2. Study of Critical Environmental Problems 1970, Study of Man’s Impact on Climate 1971.
3. Oreskes 2007. La « sensibilité du climat » fait référence à l’augmentation de température 
prévue suite à un doublement de la concentration en dioxyde de carbone dans l’atmosphère 
par rapport au niveau de 280 ppm de la période préindustrielle. En 2013, cette valeur a pour 
la première fois dépassé 400 ppm. Si le consensus évoqué est resté remarquablement stable, 
Sluijs et al. (1998) ont souligné avec raison les incohérences dans la manière dont il a été établi 
à partir des résultats des modèles.

	
gouverner le système terre	
411
À peu près au même moment, des scientifiques soviétiques produisaient  
des modèles similaires. Ces modèles soulignent l’impact considérable 
qu’aurait un tel événement sur le climat. Selon le scénario du pire,  
l’essentiel de l’hémisphère Nord pouvait se retrouver soumis à des tempé-
ratures glaciales pendant plusieurs mois ou plus, ainsi qu’à une diminution  
de la lumière du soleil provoquant l’arrêt de la photosynthèse, de 
mauvaises récoltes et une catastrophe écologique 1. Apparues en 1983, 
les études sur l’hiver nucléaire jouent un rôle majeur dans les accords 
de désarmement nucléaire signés par Reagan et Gorbatchev en 1986- 
1987 2.
Dans le même temps, d’autres modélisateurs s’intéressent aux limites 
planétaires. Le Club de Rome produit des rapports influents sur les  
effets de l’évolution rapide des technologies, l’augmentation des niveaux 
de pollution et l’explosion démographique au niveau mondial. Halte à la 
croissance ? Rapport sur les limites de la croissance, publié en 1972, est 
celui qui a eu le plus grand retentissement 3. Reposant sur des modèles 
de « dynamique des systèmes mondiaux », Halte à la croissance ? soutient 
qu’une croissance exponentielle incontrôlée de la population, de la 
pollution (y compris le dioxyde de carbone), de la consommation de 
ressources naturelles et de la production alimentaire entraînera un 
« effondrement » des systèmes mondiaux à un horizon de 50 à 100 ans 4. 
Le rapport propose également des scénarios de futurs possibles allant 
d’un « monde supertechnologique » à un « monde stabilisé » avec  
une population et une consommation de ressources réduites. Halte à la  
croissance ? devient un phénomène international, vendu à plus de 
7 millions d’exemplaires dans plus de 30 langues, et les membres du 
Club de Rome présentent leur rapport à des hommes politiques de  
premier plan.
Halte à la croissance ? fait l’objet de critiques féroces de la part 
d’économistes et de technolâtres qui en dénoncent le catastrophisme 
néomalthusien. Pour ces détracteurs, l’ingéniosité humaine, l’amélio-
ration de la technologie et le marché libre dénoueront les contraintes 5. 
Cependant, au cours des vingt années qui suivent sa publication, l’approche 
modélisatrice adoptée dans le rapport non seulement est acceptée mais elle 
devient la norme sous le nom de « modèles d’évaluation intégrés 6 ». Tout 
1. Badash 2009, Dörries 2011, Thompson et Schneider 1986, Turco et al. 1983.
2. Robock 2010, Robock et Toon 2012.
3. Elichirigoity 1999.
4. De façon inquiétante, le chemin réel suivi par le développement global entre 1972 et 2002 
correspond de près à celui projeté dans le rapport comme « scénario standard ». Turner 2008.
5. Cole et al. 1973, Simon 1981, Simon et Kahn 1984.
6. Vieille-Blanchard 2010 et 2012.

412	
paul n. edwards
au long des années 1970 et 1980, les modèles de simulation deviennent des 
outils de plus en plus essentiels dans la conduite des politiques, en parti-
culier dans les domaines de l’énergie, de l’économie et de l’environnement.
Changement climatique, science et gouvernance  
du système Terre : 1990-2010
La crainte de l’apocalypse nucléaire s’estompe avec l’effondrement de 
l’Union soviétique, mais elle est remplacée par la menace d’un changement 
climatique à l’échelle planétaire. Les appels à l’action lancés par les climato-
logues se font de plus en plus insistants. Ils sont repris par des responsables 
politiques (notamment Margaret Thatcher et Al Gore) et des organisa-
tions non gouvernementales comme Greenpeace, l’Institut des ressources 
mondiales et le Sierra Club. Les négociations qui aboutissent à la Conven-
tion-cadre sur les changements climatiques débutent en 1988. L’OMM et  
le Programme des Nations unies pour l’environnement créent le Groupement 
d’experts intergouvernemental sur l’évolution du climat (GIEC) chargé 
de passer en revue l’ensemble de la littérature scientifique pertinente  
et d’évaluer l’état des connaissances. Le groupement est subdivisé en trois 
groupes de travail couvrant respectivement les sciences physiques, les 
impacts humains et les réponses à apporter.
Le 1er rapport d’évaluation du GIEC, publié en 1990, contient des 
conclusions prudentes sur le fait que « les émissions résultant des activités 
[…] provoqueront une augmentation de l’effet de serre, aboutissant en 
moyenne à un réchauffement supplémentaire de la surface de la Terre 1 ». 
Ce 1er rapport identifie une fourchette de sensibilité du climat entre 1,5° et 
4,5 °C, avec une « valeur la plus probable » estimée à environ 2,5 °C. Les 
modèles prévoient une série d’impacts physiques, dont l’augmentation 
du niveau des mers, la déglaciation et des conséquences diverses sur les 
ressources en eau, suffisants pour entraîner des contraintes importantes 
pour les sociétés humaines dans le monde entier (en particulier celles qui 
ne disposent pas des ressources nécessaires pour s’adapter). Toutefois, 
des incertitudes importantes empêchent les prévisions précises, et les 
capacités de modélisation ne permettent pas encore d’assurer une analyse 
fiable de la manière dont les effets seront répartis. Tous les chemins vers 
un futur plus soutenable passent cependant par une diminution radicale 
de la consommation de combustibles fossiles, ainsi que par des évolu-
tions majeures en agriculture et en gestion forestière.
1. Houghton et al. 1990 (p. xi).

	
gouverner le système terre	
413
Le succès remarquable du traité sur la couche d’ozone conduit à entre-
tenir des espoirs quant à la possibilité d’un accord similaire pour le 
réchauffement climatique. Comme la Convention de Vienne, la Conven-
tion-cadre sur les changements climatiques (CCCC) signée à Rio en 1992 
engage ses signataires à développer un traité. Le processus de rédaction 
des rapports du GIEC est central 1. Même si ceux-ci (établis selon un 
cycle de 5 à 7 ans) sont entièrement rédigés par des scientifiques, l’agence 
sollicite les gouvernements, des organisations non gouvernementales et 
d’autres parties prenantes pour la révision des rapports intermédiaires. 
Et les auteurs des rapports sont tenus de répondre de manière explicite 
à tous les commentaires 2.
Cette méthode a pour objectif d’améliorer la qualité des rapports, mais 
aussi de faciliter une assimilation plus rapide des connaissances scienti-
fiques par les gouvernements. En assurant par avance le consensus sur  
les faits, on espère que les délibérations pourront rapidement aboutir 
à des solutions politiques. Or le GIEC est déjà attaqué au moment de 
la publication de son 2e rapport d’évaluation en 1995. Une industrie 
artisanale de contre-expertise voit le jour, généreusement financée par les 
groupes pétroliers et des groupes politiques conservateurs. Les membres 
de l’Organisation des pays exportateurs de pétrole (OPEP), ainsi que des 
lobbies industriels tels que la Western Fuels Association aux États-Unis, 
exploitent les incertitudes réelles inhérentes à la science climatique  
pour s’opposer aux conclusions du GIEC ou en limiter la portée 3. Ils sont 
rejoints par des groupes climatosceptiques qui considèrent le GIEC et 
les climatologues au sens large comme les acteurs d’un complot mondial. 
L’usage d’Internet (notamment par le biais de blogs) procure de nouveaux 
moyens à ces groupes pour promouvoir leurs points de vue. Au cours 
des années 1990 et 2000, leur influence est beaucoup plus grande aux 
États-Unis que dans le reste du monde, mais ils commencent à percer 
en Europe et ailleurs depuis 2010 4.
L’excès de carbone présent dans l’atmosphère incombe d’abord aux 
sociétés grandes consommatrices d’énergie que sont les pays d’Europe 
occidentale, la Russie et en particulier les États-Unis, qui sont à l’origine 
de 25 % des émissions mondiales. Les pays en développement reven-
diquent eux aussi un droit à la croissance économique alimentée par la 
consommation d’énergie, dont les autres pays ont profité avant eux. En 
1. Miller 2001b.
2. Edwards et Schneider 2001.
3. Hoggan et Littlemore 2009, Oreskes et Conway 2010.
4. Sur les sceptiques et leurs positions, avec des liens vers des revues scientifiques à comité de 
lecture, voir < http://www.skepticalscience.com > .

414	
paul n. edwards
conséquence, le protocole de Kyoto adopte une approche à deux niveaux, 
avec des objectifs contraignants et des calendriers pour les nations 
(développées) de l’« Annexe 1 », en vue de la réduction de leurs émissions  
en dessous des niveaux de 1990, et des objectifs définis simplement sur 
la base du volontariat pour les pays (en développement). Les projets 
d’échanges de quotas d’émissions et de mise en œuvre commune sont 
autorisés au sein de l’Annexe 1, tandis qu’un « mécanisme de dévelop-
pement propre » autorise les pays de l’Annexe 1 à augmenter leurs limites 
en matière d’émissions en échange d’une aide apportée à des projets 
permettant de réduire les émissions dans les pays du Sud. En 2005, 
190 pays ont ratifié le protocole de Kyoto – mais les États-Unis refusent de 
le ratifier parce que la Chine n’est pas soumise à un engagement contrai-
gnant. En 2002, ils se retirent totalement du processus.
La plupart des bilans du protocole de Kyoto soulignent sa faiblesse 
et son inefficacité. En 2012, les pays de l’Annexe 1 et les États-Unis ont 
réduit leurs émissions collectives d’environ 16 %, principalement du fait 
de l’effondrement du bloc soviétique et du ralentissement de l’économie 
mondiale à partir de 2008. Mais la croissance dans les pays en dévelop-
pement donne lieu à une augmentation de près de 50 % des émissions 
de CO2 au niveau mondial en 2011 1. Au cours de la période 1990-2010, 
les émissions de carbone chinoises passent de 2 à 8 milliards de tonnes, 
la Chine dépassant les États-Unis et l’Union européenne pour devenir 
le plus gros émetteur de CO2 du monde. Cependant, les émissions par 
habitant de la Chine restent bien inférieures à celles des États-Unis 2.
Le protocole de Kyoto devait expirer en 2012, mais comme les projets 
visant à le remplacer par un nouveau traité plus ambitieux ont échoué, le 
protocole a été reconduit jusqu’en 2020, avec une suspension des négocia-
tions sur un nouveau traité jusqu’en 2015.
Les actions délibérées les plus réussies dans la lutte contre le changement 
climatique n’ont pas lieu à l’échelle globale, mais au niveau des villes, des 
pays et des régions. En 2003, le Chicago Climate Exchange, un système 
d’échanges de quotas d’émissions basé sur le volontariat, a suscité des 
engagements juridiquement contraignants de la part de services publics 
et d’entreprises des États-Unis, du Canada et de 14 autres pays, pour un 
volume global de 700 millions de tonnes de CO2 ; en 2010, les participants 
au programme ont collectivement réduit leurs émissions de plus de 20 %. 
En 2005, l’Union européenne a adopté un système similaire couvrant 
environ la moitié de ses émissions de CO2. Les marchés carbone se sont 
1. Schiermeier 2012.
2. Données de la Banque mondiale, < http://bit.ly/16b19uD > .

	
gouverner le système terre	
415
avérés volatils et sujets à manipulation, ce qui a nui à leur crédibilité, 
mais ils restent néanmoins un mécanisme de gouvernance relativement 
efficace 1. Les engagements de l’UE sur le marché carbone, ainsi que les 
négociations de la CCNUCC, ont joué un rôle symbolique important 2. 
Parmi les autres exemples d’action efficace, on peut citer l’ambitieux 
plan Energiewende mis en œuvre par l’Allemagne, dont l’objectif est de 
produire 80 % de son électricité à partir de sources d’énergie renouve-
lables (n’incluant pas le nucléaire) d’ici 2050 3.
Diverses raisons sont avancées pour expliquer l’échec de la CCNUCC. 
Les plus importantes sont certainement la complexité inhérente au 
problème climatique, le côté radical et le coût des solutions, ou encore les 
résistances secrètes ou ouvertes liées aux intérêts particuliers des États 
producteurs de pétrole et des groupes pétroliers. On peut également citer 
parmi les raisons importantes de cet échec l’incapacité des États-Unis à 
exercer un véritable leadership et sa défaillance sur le protocole de Kyoto.
Par ailleurs, plusieurs facteurs liés plus directement aux relations entre 
science et politique ont également joué un rôle important. Les scienti-
fiques semblent avoir été incapables de présenter leurs connaissances  
sur le climat sous des formes suffisamment parlantes pour le grand public. 
Inquiets d’établir des liens de causalité qu’ils ne pourraient justifier, la 
distinction entre météorologie et climat sur laquelle ils ont insisté s’est 
avérée extrêmement difficile à comprendre pour la plupart des gens. 
(En fait, les études montrent régulièrement que l’opinion des gens sur le 
changement climatique est fortement influencée par le temps qu’il fait.) 
L’insistance des scientifiques sur l’« incertitude », terme technique dont 
la signification est différente dans le langage courant, a créé le sentiment 
que les connaissances sur le climat n’étaient pas fiables. Ce sentiment a 
été amplifié par la couverture médiatique « équilibrée 4 » et les discours 
des climatosceptiques. Il a encore gagné du terrain lors de l’épisode du 
« Climategate » en 2009, avec des accusations de complot. Cela a obligé 
le GIEC à revoir ses processus. Cependant, quels que soient les efforts 
entrepris en matière de transparence et de responsabilité, rien ne saurait 
trouver grâce aux yeux des détracteurs les plus virulents de l’organisation 5. 
Cet épisode a démontré comment le développement de la communi-
cation par Internet a limité le pouvoir des groupes d’experts, peut-être 
de manière définitive. Dans le même temps, il permet aux non-initiés de 
1. MacKenzie 2007, Robert 2012.
2. Dahan et Aykut 2012.
3. Schiermeier 2013.
4. Boykoff (M.T.) et Boykoff (J.M.) 2007.
5. Beck 2012, Grundmann 2012, Jasanoff 2010b.

416	
paul n. edwards
tous bords politiques de participer au processus de production et d’inter-
prétation des savoirs utiles 1.
Enfin, la prédominance toujours plus grande des sciences économiques 
et du discours économiste est à l’origine d’une approche « attentiste ». 
Les désaccords profonds sur le véritable taux d’économies réalisées 
(utilisé pour calculer les bénéfices futurs des mesures de prévention du 
changement climatique engagées aujourd’hui) sont reflétés par l’éventail 
extrêmement large des avis des économistes, exactement comme lors 
du débat autour du Rapport sur les limites de la croissance quarante ans 
plus tôt 2. Le rapport Stern de 2006, qui fait autorité, est à classer dans  
la première catégorie qui souligne l’« urgence » de la situation (en 2013, 
Stern a été cité disant : « Je me suis trompé. Les choses sont encore bien 
pires. »)
Les difficultés rencontrées pour mettre en œuvre une gouvernance 
globale sont également liées à l’extrême étendue des échelles de temps et 
d’espace. En déphasage avec l’expérience humaine ordinaire, les change-
ments globaux survenant à l’échelle de décennies sont difficiles à prendre 
en compte dans les formes existantes de vie et les choix 3. D’autre part, le 
climat n’est pas simplement un phénomène physique : les climats ont des 
significations culturelles importantes et variables en lien avec les formes 
de vie (agriculture, alimentation, eau, vêtements), les peurs et les espoirs, 
les identités ethniques et la religion (solstice et fêtes des moissons). Ces 
multiples significations ainsi que les larges pans de savoirs locaux et 
indigènes mobilisables (en particulier parmi les peuples qui vivent sous 
des conditions climatiques extrêmes) ne semblent pas trouver leur place 
dans le monde des discours technocratiques des experts du GIEC 4. Enfin, 
avec le temps, le processus du GIEC s’est trouvé progressivement surchargé 
par l’accumulation de nouveaux éléments (questions de compensation 
relatives aux injustices historiques, de survie ethnique et nationale, etc.)
Au moment où ces lignes sont écrites, les échecs de la gouvernance 
environnementale mondiale ont abouti à des discussions sur la « gestion  
du rayonnement solaire ». Ces projets de géo-ingénierie rappellent ceux des 
années 1950, mais leur motivation semble maintenant être un sentiment 
de désespoir et non plus simplement d’hubris technologique. Parmi les 
idées avancées figurent la mise en orbite de miroirs Mylar de la taille 
de l’Inde, des flottes gigantesques de navires robots pulvérisant de fines 
gouttelettes dans l’air pour créer des nuages artificiels et des fusées qui 
1. Jasanoff 2010b, Edwards et al. 2013.
2. Goulder et Williams 2012, Stern et al. 2006.
3. Jasanoff et Martello 2004, Jasanoff 2010a.
4. Hulme 2009.

	
gouverner le système terre	
417
enverraient des particules de soufre dans la stratosphère afin de créer une 
poussière volcanique artificielle 1. Des scientifiques étudient les impacts 
potentiels de ce genre d’interventions radicales sur les modèles climatiques 
et ils commencent à comparer les risques induits avec ceux inhérents 
au fait de ne rien faire. La plupart considèrent la géo-ingénierie comme 
un moyen de gagner du temps pour restructurer de manière radicale les 
infrastructures énergétiques, mais d’autres envisagent ces « effets assez 
extraordinaires », selon la formule de von Neumann, comme une solution 
permanente. Les moyens techniques nécessaires pour ce genre d’inter-
vention sont à la portée de nombreux pays qui pourraient les entreprendre 
seuls. Cela pose une question majeure en termes de gouvernance sur  
la manière dont serait prise la décision de déployer ces techniques – et 
sur le fait que cette décision puisse être unilatérale, sans accord global.
La gouvernance du système Terre relève d’un problème classique 
de biens communs, mais à une échelle inédite. Comme pour d’autres 
« communs cosmopolitiques », le problème est en un certain sens créé 
par les infrastructures de savoir par lesquelles nous le percevons 2. Le 
« système Terre » n’est le « chez-soi » de personne ; en conséquence, peu 
d’individus, de groupes ou de pays placent les problèmes globaux en haut 
de la liste de leurs priorités. Il semblerait que seuls les scientifiques parlent 
pour la Terre elle-même, et pourtant l’éthique scientifique d’objectivité 
les empêche de jouer un rôle directement politique. Nous ne savons pas 
encore si « ceux qui se soucient de la Terre » constitueront un jour une 
communauté – s’ils vont, avec les scientifiques, se mettre « sur le sentier 
de la guerre » pour Gaïa, comme les y exhorte Bruno Latour 3.
Traduit par Cyril Le Roy
Références bibliographiques
Badash Lawrence, 2009, A Nuclear Winter’s Tale : Science and Politics in the 1980s, 
Cambridge (MA), MIT Press.
Barth Kai-Henrik, 2003, « The Politics of Seismology : Nuclear Testing, Arms 
Control, and the Transformation of a Discipline », Social Studies of Science, vol. 33, 
no 5, p. 743-781.
Beck Silke, 2012, « Between Tribalism and Trust : The IPCC Under the “Public Micro-
scope” », Nature and Culture, vol. 7, no 2, été, p. 151-173.
Boykoff Maxwell T. et Boykoff Jules M., 2007, « Climate Change and Journal-
istic Norms : A Case-Study of US Mass Media Coverage », Geoforum, vol. 38, no 6, 
p. 1190-1204.
1. Fleming 2010.
2. Disco et Kranakis 2013, Miller 2009.
3. Latour 2013.

418	
paul n. edwards
Brückner Eduard, 2000, The Sources and Consequences of Climate Change and 
Climate Variability in Historical Times, éd. par Nico Stehr et Hans von Storch, 
Boston (MA), Kluwer Academic Publishers.
Cloud John, 2002, « American Cartographic Transformations during the Cold War », 
Cartography and Geographic Information Science, vol. 29, no 3, p. 261-282.
–	 2003, « Introduction : Special Guest-Edited Issue on the Earth Sciences in the Cold 
War », Social Studies of Science, vol. 33, no 5, p. 629-633.
Cole H.S.D., Freeman Christopher, Jahoda Marie et Pavitt K.R. (dir.), 1973, 
Models of Doom : A  Critique of « The  Limits to Growth », New York, Universe  
Books.
Dahan Amy et Aykut Stefan C., 2012, De Rio 1992 à Rio 2012 : vingt années de 
négociations climatiques, Paris, Centre Alexandre Koyré (CNRS  / EHESS) et 
Institut francilien recherche innovation société (IFRIS).
Disco Nil et Kranakis Eda, 2013, Cosmopolitan Commons : Sharing Resources and 
Risks across Borders, Cambridge (MA), MIT Press.
Doel Ronald E., 2003, « Constituting the Postwar Earth Sciences : The Military’s 
Influence on the Environmental Sciences in the USA after 1945 », Social Studies of 
Science, vol. 33, no 5, p. 635-666.
Doel Ronald E. et Harper Kristine C., 2006, « Prometheus Unleashed : Science as 
a Diplomatic Weapon in the Lyndon B. Johnson Administration », Osiris, vol. 21, 
no 1, p. 66-85.
Dörries Matthias, 2011, « The Politics of Atmospheric Sciences : “Nuclear Winter” 
and Global Climate Change », Osiris, vol. 26, no 1, p. 198-223.
Edwards Paul N., 1996, The Closed World : Computers and the Politics of Discourse 
in Cold War America, Cambridge (MA), MIT Press.
–	 2006, « Meteorology as Infrastructural Globalism », Osiris, vol. 21, p. 229-250.
–	 2010, A Vast Machine : Computer Models, Climate Data, and the Politics of Global 
Warming, Cambridge (MA), MIT Press.
–	 2012, « Entangled Histories : Climate Science and Nuclear Weapons Research », 
Bulletin of the Atomic Scientists, vol. 68, no 4, p. 28-40.
Edwards Paul N. et  al. 2013, Knowledge Infrastructures : Intellectual Frameworks 
and Research Challenges, Ann Arbor (MI), Deep Blue.
Edwards Paul N. et Schneider Stephen H., 2001, « Governance and Peer Review in 
Science-for-Policy : The Case of the IPCC Second Assessment Report », in Clark 
A. Miller et Paul N. Edwards (dir.), Changing the Atmosphere : Expert Knowledge 
and Environmental Governance, Cambridge (MA), MIT Press, p. 219-246.
Elichirigoity Fernando, 1999, Planet Management : Limits to Growth, Computer 
Simulation, and the Emergence of Global Spaces, Evanston (IL), Northwestern 
University Press.
Elzinga Aant, 2010, « Achievements of the Second International Polar Year », 
in Susan Barr et Cornelia Luedecke (dir.), The History of the International Polar 
Years (IPYs), Berlin, Springer, p. 211-234.
Fleming James R. (dir.), 1996, Historical Essays on Meteorology (1919-1995), Boston 
(MA), American Meteorological Society.
–	 1998, Historical Perspectives on Climate Change, New York, Oxford University 
Press.
–	 2010, Fixing the Sky : The Checkered History of Weather and Climate Control, New 
York, Columbia University Press.
Good Gregory A., 2000, « The Assembly of Geophysics : Scientific Disciplines as 

	
gouverner le système terre	
419
Frameworks of Consensus », Studies in History and Philosophy of Modern Physics, 
vol. 31, no 3, p. 259-292.
Goulder Lawrence H. et Williams III Roberton C., 2012, « The Choice of Discount 
Rate for Climate Change Policy Evaluation », Climate Change Economics, vol. 3, 
no 4, p. 1250024-1 à 18.
Grove Richard, 1997, Ecology, Climate and Empire : Colonialism and Global Environ-
mental History (1400-1940), Cambridge, White Horse Press.
Grundmann Reiner, 2012, « The Legacy of Climategate : Revitalizing or Undermining 
Climate Science and Policy ? », WIREs Climate Change, vol. 3, no 3, p. 281-288.
Hamblin Jacob D., 2000, « Visions of International Scientific Cooperation : The Case 
of Oceanic Science (1920-1955) », Minerva, vol. 38, no 4, p. 393-423.
Harper Kristine C., 2008, Weather by the Numbers : The Genesis of Modern Meteo-
rology, Cambridge (MA), MIT Press.
Hoggan James et Littlemore Richard D., 2009, Climate Cover-Up : The Crusade to 
Deny Global Warming, Vancouver, Greystone Books.
Houghton John Theodore, Jenkins Geoffrey J. et Ephraums J.J. (dir.), 1990, Climate 
Change : The  IPCC Scientific Assessment, Cambridge, Cambridge University  
Press.
Hughes Agatha C. et Hughes Thomas P. (dir.), 2000, Systems, Experts, and Compu­­
ters : The Systems Approach in Management and Engineering, World War II and 
After, Cambridge (MA), MIT Press.
Hulme Mike, 2009, Why We Disagree about Climate Change : Understanding Contro-
versy, Inaction and Opportunity, Cambridge, Cambridge University Press.
Jasanoff Sheila, 2010a, « A New Climate for Society », Theory, Culture and Society, 
vol. 27, nos 2-3, p. 233-253.
–	 2010b, « Testing Time for Climate Science », Science, vol. 328, no 5979, p. 695-696.
Jasanoff Sheila et Martello Marybeth Long (dir.), 2004, Earthly Politics : Local 
and Global in Environmental Governance, Cambridge (MA), MIT Press.
Krige John, 2006, « Atoms for Peace, Scientific Internationalism, and Scientific Intel-
ligence », Osiris, vol. 21, p. 161-181.
Krige John et Barth Kai-Henrik, 2006, « Introduction : Science, Technology, and 
International Affairs », Osiris, vol. 21, no 1, p. 1-21.
Latour Bruno, 2013, Facing Gaia : Six Lectures on the Political Theology of Nature, 
Gefford Lectures, Édimbourg 18-28 février 2013.
Locher Fabien et Fressoz Jean-Baptiste, 2012, « Modernity’s Frail Climate : 
A Climate History of Environmental Reflexivity », Critical Inquiry, vol. 38, no 3, 
p. 579-598.
Machta Lester, 2002, « Meteorological Benefits from Atmospheric Nuclear Tests », 
Health Physics, vol. 82, no 5, p. 635-643.
Mack Pamela E., 1990, Viewing the Earth : The Social Construction of the Landsat 
Satellite System, Cambridge (MA), MIT Press.
MacKenzie Donald, 2007, « The Political Economy of Carbon Trading », London 
Review of Books, vol. 29, no 7, p. 29-31.
McDougall Walter A., 1985, The Heavens and the Earth : A Political History of the 
Space Age, New York, Basic Books.
Miller Clark A., 2001a, « Scientific Internationalism in American Foreign Policy : 
The Case of Meteorology (1947-1958) », in Clark A. Miller et Paul N. Edwards 
(dir.), Changing the Atmosphere : Expert Knowledge and Environmental Gover-
nance, Cambridge (MA), MIT Press, p. 167-218.

420	
paul n. edwards
–	 2001b, « Hybrid Management : Boundary Organizations, Science Policy, and 
­Environmental Governance in the Climate Regime », Science, Technology and 
Human Values, vol. 26, no 4, p. 478-500.
–	 2006, « “An Effective Instrument of Peace” : Scientific Cooperation as an Instrument 
of US Foreign Policy (1938-1950) », Osiris, vol. 21, p. 133-160.
–	 2009, « Epistemic Constitutionalism in International Governance : The  Case of 
Climate Change », in Michael A. Heazle, Martin Griffiths et Tom J. Conley (dir.),  
Foreign Policy Challenges in the 21st Century, Northampton (MA), Edward Elgar 
Publishing, p. 141-163.
Mukerji Chandra, 1989, A Fragile Power : Scientists and the State, Princeton (NJ), 
Princeton University Press.
National Oceanic and Atmospheric Administration 1981, The Global Weather  
Experiment : Final Report of US Operations, Washington (DC), National Oceanic 
and Atmospheric Administration.
Naylor Simon et al., 2008, « Science, Geopolitics and the Governance of Antarctica », 
Nature Geoscience, vol. 1, no 3, p. 143-145.
Needell Allan A., 2000, Science, Cold War and the American State : Lloyd V. Berkner 
and the Balance of Professional Ideals, Amsterdam, Harwood Academic.
Needell Allan A., Galison Peter L. et Hevly Bruce, 1992, « From Military Research 
to Big Science : Lloyd Berkner and Science Statesmanship in the Postwar Era », 
in Peter Louis Galison et Bruce Hevly (dir.), Big Science : The Growth of Large 
Scale Research, Palo Alto (CA), Stanford University Press, p. 290-311.
Neumann John von, 1955, « Can We Survive Technology ? », Fortune, vol. 51, no 6, 
juin, p. 106-108 et 151-152.
Nielsen Kristian, Nielsen Henry et Martin-Nielsen Janet, 2014, « City under the 
Ice : The Closed World of Camp Century in Cold War Culture », Science as Culture, 
vol. 23, no 4, p. 443-464.
Oreskes Naomi, 2003, « A Context of Motivation US Navy Oceanographic Research 
and the Discovery of Sea-Floor Hydrothermal Vents », Social Studies of Science, 
vol. 33, no 5, p. 697-742.
–	  2007, « The Scientific Consensus on Climate Change : How Do We Know We’re 
Not Wrong ? », in  Joseph F.  DiMento et Pamela Doughman (dir.), Climate 
Change : What It Means for Us, Our Children, and Our Grandchildren, Cambridge 
(MA), MIT Press, p. 65-100.
Oreskes Naomi et Conway Erik M., 2010, Merchants of Doubt, New York, 
Bloomsbury Press.
Parson Edward A., 2003, Protecting the Ozone Layer, New York, Oxford University 
Press.
Robert Aline, 2012, Carbone connexion : le casse du siècle, Paris, Max Milo.
Robock Alan, 2010, « Nuclear Winter », Wiley Interdisciplinary Reviews : Climate 
Change, vol. 1, no 3, p. 418-427.
Robock Alan et Toon Owen Brian, 2012, « Self-Assured Destruction : The Climate 
Impacts of Nuclear War », Bulletin of the Atomic Scientists, vol. 68, no 5, p. 66-74.
Salby Murry, Titova Evgenia et Deschamps Lilia, 2011, « Rebound of Antarctic 
Ozone », Geophysical Research Letters, vol. 38, no 9.
Schiermeier Quentin, 2012, « The Kyoto Protocol : Hot Air », Nature, vol.  491, 
no 7426, p. 656-658.
–	 2013, « Germany’s Energy Gamble », Nature, vol. 496, p. 156-158.
Simon Julian, 1981, The Ultimate Resource, Princeton (NJ), Princeton University Press.

	
gouverner le système terre	
421
Simon Julian et Kahn Herman (dir.), 1984, The Resourceful Earth : A Response to 
Global 2000, Oxford, Basil Blackwell.
Sluijs Jeroen P. van der et  al., 1998, « Anchoring Devices in Science for Policy : 
The  Case of Consensus around Climate Sensitivity », Social Studies of Science, 
vol. 28, no 2, p. 291-323.
Stern Nicholas H. et al., 2006, Stern Review : The Economics of Climate Change, 
Londres, HM Treasury.
Study of Critical Environmental Problems 1970, Man’s Impact on the Global 
Environment : Assessment and Recommendations for Action, Cambridge (MA), 
MIT Press.
Study of Man’s Impact on Climate 1971, Inadvertent Climate Modification : 
Report of the Study of Man’s Impact on Climate, Cambridge (MA), MIT Press.
Thompson Starley L. et Schneider Stephen H., 1986, « Nuclear Winter Reappraised », 
Foreign Affairs, vol. 64, p. 981-1005.
Turchetti Simone, 2012, « Sword, Shield and Buoys : A  History of the NATO 
Sub-Committee on Oceanographic Research (1959-1973) », Centaurus, vol.  54, 
no 3, p. 205-231.
Turco Richard P. et al., 1983, « Nuclear Winter : Global Consequences of Multiple 
Nuclear Explosions », Science, vol. 222, p. 1283-1292.
Turner Graham M., 2008, « A Comparison of The Limits to Growth with 30 Years of 
Reality », Global Environmental Change, vol. 18, no 3, p. 397-411.
Vieille-Blanchard Élodie, 2010, « Modelling the Future : An Overview of The Limits 
to Growth Debate », Centaurus, vol. 52, no 2, p. 91-116.
–	 2012, « The Origins of Integrated Models of Climate Change », Atoms For Peace : 
An International Journal, vol. 3, no 3, p. 238-255.
Weart Spencer R., 2003, The Discovery of Global Warming, Cambridge (MA), 
Harvard University Press.
–	 2013, « Rise of Interdisciplinary Research on Climate », Proceedings of the National 
Academy of Sciences, vol. 110, suppl. 1, p. 3657-3664.
Williams Richard S. Jr. et Carter William D., 1976, ERTS-1 : A New Window on Our 
Planet. Geological Survey Professional Paper 929, Washington (DC), US Geological 
Survey.
World Commission on Environment and Development 1987, Our Common 
Future, New York, Oxford University Press.


20 Manager l’innovation
c H r i s t o p H e  L É c u Y e r  1
« Aujourd’hui plus qu’hier et demain plus qu’aujourd’hui, la survie des 
hommes et de leurs institutions dépendra de l’innovation. L’innovation 
est indispensable à la formation et la croissance des entreprises et à la 
santé économique, sociale et politique des nations 2. » C’est ainsi qu’un 
dirigeant des Bell Telephone Laboratories, le plus grand laboratoire de 
recherche industriel aux États-Unis, résume en 1971 les enjeux et repré-
sentations qui ont animé les pratiques de l’innovation depuis le début du 
xxe siècle. L’innovation technique, c’est-à-dire l’ensemble du processus 
créatif qui va de l’invention au développement, à la commercialisation 
et au déploiement de nouvelles technologies, est en eff et un des projets 
centraux des sociétés industrielles. Elle est au cœur de la stratégie des 
entreprises et des politiques publiques. Elle suscite des investissements 
fi nanciers colossaux et est le fait d’ingénieurs et de scientifi ques toujours 
plus nombreux.
De ces investissements humains et fi nanciers et de cette focalisation 
sur l’innovation naissent de nouvelles techniques et secteurs industriels 
tels l’aérospatial, l’électronique, l’informatique, les nanotechnologies et les 
biotechnologies. Des matériaux révolutionnaires comme les plastiques, les 
matériaux composés et les semi-conducteurs apparaissent et permettent 
la conception et la production de nouveaux composants et systèmes. 
La structure même du monde technique change. Alors qu’il est électro -
mécanique à la fi n du xixe siècle, il devient un monde numérique – c’est-à-dire 
un monde où les équipements électromécaniques sont contrôlés par des 
dispositifs numériques comme les puces et les logiciels. Au cours du 
xxe siècle, les entreprises et organismes de recherche acquièrent aussi 
une maîtrise toujours plus grande du vivant.
1. L’auteur aimerait remercier l’Institut d’études avancées de la Central European University 
pour son soutien à la rédaction de ce chapitre.
2. Morton 1971.
 Le premier circuit intégré fabriqué à Fairchild semiconductor, août 1960.

424	
christophe lécuyer
À ces mutations techniques de grande ampleur sont associés des change-
ments dans la gestion de l’innovation elle-même. De nouvelles techniques 
managériales et de nouveaux mécanismes institutionnels apparaissent  
au sein des entreprises et des appareils étatiques pour gérer l’innovation. 
Les grandes entreprises créent, par exemple, des laboratoires de recherche 
qui assimilent l’innovation et leur permettent de concevoir de nouveaux 
procédés et produits. De nouvelles institutions telles que le capital-risque 
et les start-up apparaissent pour financer et accélérer le processus d’inno-
vation. Les États créent des organismes dont la fonction première est 
de promouvoir le changement technique. La gestion de l’innovation se 
professionnalise aussi avec l’apparition d’une littérature scientifique sur 
l’innovation et la création de programmes d’enseignement qui forment 
les spécialistes du management de l’innovation 1.
Comment les entreprises et les organismes publics gèrent-ils l’innovation 
technique ? Comment et pourquoi de nouvelles techniques, mécanismes 
et modes de financement de l’innovation apparaissent-ils ? Comment 
ces innovations sociales façonnent-elles les mutations techniques et en 
retour comment les changements techniques influent-ils sur la gestion 
de l’innovation ? Pour répondre à ces questions, ce chapitre se penchera 
sur le cas des États-Unis. Les États-Unis sont une très grande puissance 
technique au xxe siècle – le pays où de nombreuses innovations telles 
que les circuits intégrés et la recombinaison génétique sont créées et 
commercialisées. Ils sont aussi un grand lieu d’expérimentation dans 
les techniques de gestion et les institutions de l’innovation. Ces innova-
tions managériales et organisationnelles doivent beaucoup à la législation 
sur la concurrence, aux mutations de la jurisprudence sur la propriété 
industrielle et aux opportunités et contraintes offertes par les techno-
logies elles-mêmes. Les conflits mondiaux et les guerres économiques 
jouent aussi un rôle essentiel dans leurs transformations.
Les techniques et institutions du management de l’innovation se 
développent aux États-Unis en trois temps, des années 1900 à nos jours 2. 
Dans un premier temps, les grandes entreprises, qui jusqu’au début 
du xxe siècle achetaient les technologies sur le marché de la propriété 
industrielle, assimilent l’innovation pour mieux la contrôler. Elles créent 
des laboratoires de recherche, qui leur permettent de développer des 
capacités d’innovation internes, d’enrichir leurs portefeuilles de brevets 
et de constituer des quasi-monopoles dans les industries de pointe. La 
1. Hounshell 1996.
2. Le management de l’innovation est conçu ici comme l’organisation du processus de l’innovation. 
Il a trois aspects : les modes de financement, les institutions et les techniques managériales.

	
manager l’innovation	
425
crise de 1929 et les guerres qui la suivent transforment l’État fédéral 
en un grand acteur de l’innovation technique. L’État fédéral crée alors 
de nouveaux organismes pour accélérer le processus d’innovation et il 
finance massivement le développement de nouvelles technologies. Le 
rôle accru de l’État fédéral transforme la recherche industrielle et favorise 
l’apparition du capital-risque et des start-up. À partir de la fin des années 
1970, les États-Unis connaissent une crise de l’innovation, qui entraîne la 
fermeture de nombreux laboratoires industriels et le retour des grandes 
entreprises à l’innovation marchande telle qu’elle était pratiquée à la fin 
du xixe et au début du xxe siècle.
Contrôler l’innovation
L’innovation pendant la seconde moitié du xixe siècle est le fait d’inven-
teurs indépendants. Les inventeurs sont rarement des ingénieurs formés 
dans les écoles d’ingénieurs et les universités. Ce sont plutôt des prati-
ciens qui inventent et déposent des brevets. Ils identifient les problèmes 
techniques les plus importants liés à l’électrification et à l’essor des 
grands réseaux (comme le télégraphe, les chemins de fer et les réseaux 
électriques) et ils trouvent des solutions à ces problèmes. Pour monétiser 
leurs inventions, les inventeurs engagent souvent des agents commerciaux 
qui les aident à identifier des clients potentiels et à négocier la cession de 
leurs droits. Le marché de la propriété industrielle est en effet florissant.  
Au milieu des années 1880, il y a plus de 550 cabinets d’avocats spécialisés 
dans le commerce de la propriété industrielle aux États-Unis. Certains 
cabinets acquièrent des brevets et cherchent ensuite à vendre des licences 
d’exploitation en assignant les sociétés en justice pour violation de leurs 
droits de propriété industrielle. Ils profitent ainsi d’un régime de la 
propriété intellectuelle qui est très favorable aux détenteurs de brevets 1.
Dans cet environnement où l’innovation dépend souvent de l’achat et 
de la vente de brevets, les grandes entreprises développent des fonctions 
de veille technologique. Elles créent des services dont le but est d’iden-
tifier les inventions et les brevets les plus importants pour leur croissance. 
Par exemple, l’American Bell Telephone Company, la plus grande entre-
prise de téléphonie aux États-Unis, a un département des brevets dont 
la fonction est d’évaluer les inventions faites à l’extérieur de l’entreprise 
et de les acheter si elles semblent importantes pour son avenir (le danger 
est en effet que les brevets importants tombent dans les mains de ses 
1. Lamoreaux et Sokoloff 1999, Hughes 1989.

426	
christophe lécuyer
concurrents). Les entreprises intègrent alors les technologies acquises  
sur le marché dans leurs propres produits et systèmes, et les commer-
cialisent. Les sociétés se protègent aussi des cabinets d’avocats qui les 
assignent en justice en s’associant à des entreprises concurrentes pour 
créer des cartels de brevets. Ces cartels, qui sont particulièrement répandus 
dans l’industrie des chemins de fer, permettent aux sociétés de présenter 
un front uni vis-à-vis des détenteurs de brevets et de réduire ainsi le coût 
de la propriété intellectuelle 1.
Ces pratiques de management de l’innovation changent au début du 
xxe siècle dans les industries les plus en pointe, tout particulièrement 
les télécommunications, l’industrie électrique, l’industrie chimique et les 
industries de la photographie et des pneumatiques. Les grandes entre-
prises assimilent alors l’innovation technique et créent des laboratoires 
de recherche afin de mieux la diriger et la contrôler (elles continuent 
d’acheter des brevets et des technologies, mais ces achats représentent 
une part plus faible de leurs portefeuilles techniques, surtout à partir des 
années 1920). Elles suivent ainsi l’exemple des grandes sociétés chimiques 
allemandes qui comme Friedrich Bayer AG avaient créé des laboratoires 
de recherche à la fin du xixe siècle. Entre 1900 et 1913, 50 grandes entre-
prises américaines établissent leurs propres programmes de recherche. 
Parmi les plus connues sont DuPont, General Electric, AT & T (la nouvelle 
incarnation d’American Bell Telephone), Dow Chemical, Eastman Kodak, 
Goodyear et la Standard Oil of Indiana 2.
La création de ces laboratoires de recherche peut s’expliquer en partie 
par la concurrence et les lois qui régissent la concentration industrielle au 
début du xxe siècle. À cette époque, les entreprises allemandes concur-
rencent de plus en plus les grands groupes américains sur leurs propres 
marchés. En même temps, beaucoup de sociétés voient leurs brevets  
les plus importants expirer à cette époque. Un autre facteur est l’appa-
rition de nouvelles technologies telles que celles de la radio qui menacent 
à terme les réseaux téléphoniques. Or, pour répondre à ces défis qui 
mettent en danger leurs investissements et positions sur les marchés,  
les entreprises ne peuvent plus acheter leurs concurrents, comme elles le 
faisaient jusqu’alors. En 1890, le Congrès américain vote la loi Sherman 
contre les trusts. Cette loi, qui limite les comportements anticoncurren-
tiels des entreprises, est de plus en plus appliquée au début du xxe siècle 
et permet à l’État fédéral de démanteler plusieurs grandes sociétés comme 
Standard Oil et American Tobacco. Pour répondre aux défis techniques et 
1. Lamoreaux et Sokoloff 1999.
2. Noble 1977, Reich 1985, Hounshell et Smith 1988, Hounshell 1996, Gertner 2012.

	
manager l’innovation	
427
à la concurrence extérieure, les sociétés n’ont plus qu’une issue : développer 
davantage leurs capacités internes d’innovation, notamment en créant  
des laboratoires de recherche. Les laboratoires apportent aussi des 
avantages politiques et légaux. Ils permettent aux grandes entreprises 
d’améliorer leur image en s’associant à la science, qui est alors très valorisée 
aux États-Unis. En outre, les tribunaux, qui se prononcent sur l’avenir 
des trusts, considèrent les investissements dans la recherche de façon 
favorable et les prennent en compte dans leurs décisions 1.
Les laboratoires industriels créés dans les années 1900 et au début  
des années 1910 ont plusieurs caractéristiques en commun. À la diffé-
rence des ateliers dirigés par les inventeurs, ils emploient des ingénieurs, 
chimistes et physiciens formés dans les universités et les écoles d’ingé-
nieurs. Les chercheurs des laboratoires, comme les inventeurs avant 
eux, pratiquent l’ingénierie par tâtonnements. Mais – et ceci est la 
nouveauté – ils essaient aussi de mieux comprendre les principes scien-
tifiques qui sous-tendent produits et méthodes de fabrication, afin de les 
améliorer et d’en créer de nouveaux. Enfin, les laboratoires sont isolés 
des unités de production, de façon à ce que les chercheurs ne focalisent 
pas leurs efforts sur les problèmes immédiats de fabrication. Mais cela 
ne veut pas dire que les laboratoires sont distants des préoccupations 
commerciales des entreprises. Au contraire, leur fonction est d’établir  
des normes techniques, de concevoir de nouveaux dispositifs et méthodes 
de fabrication, d’acquérir des brevets et plus généralement de rationaliser 
la production et de renforcer les positions des sociétés sur les marchés. 
Pour ce faire, les laboratoires sont intégrés aux autres fonctions de l’entre-
prise par l’intermédiaire d’instances de pilotage et de comités techniques 2.
Les laboratoires industriels ont rapidement un impact important sur 
l’innovation. Ils permettent d’agir à la fois sur les techniques, les marchés 
et l’environnement légal. Dans le cas de General Electric, la première 
entreprise à se lancer dans la recherche aux États-Unis, la création 
d’un laboratoire de recherche stimule l’innovation dans les ampoules à 
incandescence. Utilisant la méthode des tâtonnements, William Coolidge, 
un physicien de General Electric, conçoit un filament en tungstène qui 
permet la production d’ampoules beaucoup plus fiables. Ces ampoules 
sont ensuite perfectionnées par Irving Langmuir, un chimiste et futur 
Prix Nobel, qui s’attaque au problème de leur noircissement. Langmuir 
1. Lamoreaux et Sokoloff 1999, Reich 1985, Hounshell et Smith 1988, Hounshell 1996, Gertner 
2012. Les plus grandes entreprises américaines visent à obtenir des économies de champ et 
d’échelle par l’intégration verticale et l’internalisation de l’innovation, de la production, du 
marketing et de la vente. Voir Chandler 1988.
2. Reich 1985, Hounshell et Smith 1988, Hounshell 1996, Gertner 2012, Lécuyer et Choi 2012.

428	
christophe lécuyer
fait des études sur les phénomènes physico-chimiques qui se passent  
à l’intérieur des ampoules. Ces études l’amènent à remplir les ampoules 
d’un gaz inerte, comme l’argon, qui règle le problème de leur noircissement 
et augmente leur durée de vie. Dans la décennie 1910, General Electric 
brevette ces inventions et beaucoup d’autres, qui leur sont associées.  
Le portefeuille de brevets de l’entreprise devient si solide qu’il lui  
permet de constituer un monopole dans les ampoules à incandescence – et 
cela malgré le démantèlement en 1912, par l’État fédéral, d’un cartel 
qui contrôle leur production et leur vente. Les brevets empêchent, en 
effet, d’autres entreprises de s’implanter sur le marché des ampoules à 
incandescence. Ils mettent aussi General Electric en position de force 
vis-à-vis des titulaires de licences qui doivent payer de fortes redevances 
s’ils dépassent leurs quotas de vente. Les titulaires de licences sont aussi 
obligés de mettre leurs propres brevets à la disposition de General Electric. 
En 1928, General Electric et les entreprises qu’elle contrôle par l’intermé-
diaire de ses brevets détiennent 96 % du marché américain des ampoules 
à incandescence. Les ampoules représentent alors la moitié des profits  
de l’entreprise. En bref, le laboratoire de recherche permet à General 
Electric de contourner la législation sur les trusts et d’en tirer des avantages 
économiques très substantiels 1.
Les succès techniques, commerciaux et légaux des premiers labora-
toires de recherche industriels stimulent la croissance très rapide de 
la recherche industrielle pendant l’entre-deux-guerres. Beaucoup de 
sociétés créent alors des laboratoires de ce type, tout particulièrement 
dans la sidérurgie, l’industrie automobile et l’industrie pharmaceutique. 
La création de laboratoires leur paraît essentielle pour maintenir leur 
compétitivité et leur emprise sur les marchés. Le nombre de sociétés  
qui ont des laboratoires de recherche passe de 50 en 1913, à 1 600 en 
1931 et 2 000 en 1940. En même temps, la taille des laboratoires grandit 
de façon substantielle. Par exemple, en 1925, les laboratoires d’AT & T, 
connus sous le nom des Bell Telephone Laboratories, ont 3 600 employés. 
Ils en ont plus de 5 000 en 1940 2.
Accélérer l’innovation
La crise de 1929 et plus encore la Seconde Guerre mondiale trans-
forment les mécanismes institutionnels et les modes de financement de 
1. Reich 1985.
2. Reich 1985, Hounshell 1996.

	
manager l’innovation	
429
l’innovation aux États-Unis. L’objectif principal des acteurs de l’inno-
vation devient alors d’accélérer l’innovation – de réduire le laps de 
temps entre invention et commercialisation et de favoriser l’essor de 
nouvelles industries. Les crises économiques et militaires poussent en effet  
l’État fédéral, qui a jusqu’alors pratiqué le laisser-faire technique, à inter-
venir de façon massive dans le processus d’innovation. Dans un premier 
temps, l’État fédéral cherche à briser les monopoles sur les brevets, afin 
d’accélérer l’innovation et la répandre au sein du tissu industriel. Pour 
beaucoup d’observateurs, la crise de 1929 montre les limites du système 
d’innovation tel qu’il est apparu au début du siècle. Les grands groupes 
contrôlent si bien l’innovation qu’ils l’étouffent et qu’ils ralentissent ainsi 
l’essor de nouvelles industries. Il faut donc ouvrir l’innovation à d’autres 
acteurs, notamment les petites et moyennes entreprises. Ce constat 
pousse Thurman Arnold, vice-ministre de la Justice dans l’administration 
Roosevelt, à lancer des enquêtes pour violation de la loi Sherman contre 
les trusts à la fin des années 1930. Son but est moins de démanteler les 
grandes entreprises que de ravir leurs brevets et les mettre à la dispo-
sition d’autres sociétés. Ces efforts sont relayés dans l’après-guerre par des 
juges fédéraux hostiles aux pratiques des années 1920 dans le domaine 
de la propriété industrielle 1.
Cette campagne juridique transforme le régime de la propriété intel-
lectuelle aux États-Unis. Elle force de nombreux groupes comme AT & T, 
IBM, Alcoa et DuPont à donner des licences d’exploitation gratuites à toutes 
les entreprises américaines qui veulent utiliser leurs brevets. Ainsi, près 
de 50 000 brevets, sur un total de 600 000, sont rendus « publics » de 1941 
à 1959. Nombre de ces brevets portent sur des innovations importantes 
comme le nylon conçu par des chimistes de DuPont à la fin des années 
1930. Les entreprises doivent aussi s’engager à vendre des licences à bas 
prix sur les brevets qu’elles obtiendront à l’avenir. Ces mesures ébranlent 
profondément les monopoles qui étaient apparus dans la chimie, l’élec-
tronique et l’industrie pharmaceutique pendant les premières décennies 
du xxe siècle. Elles empêchent les grandes entreprises d’utiliser leurs 
brevets pour limiter la concurrence. Les brevets eux-mêmes deviennent 
moins importants et sont utilisés de plus en plus de façon défensive 2.
Le second conflit mondial transforme aussi l’État fédéral en un très grand 
financeur et instigateur de l’innovation. L’administration Roosevelt fait 
des investissements considérables dans le développement de nouvelles 
technologies comme les systèmes radars et la bombe atomique. Elle crée 
1. Hart 1998, Mowery 2009.
2. Hart 1998.

430	
christophe lécuyer
aussi de nouveaux organismes pour promouvoir l’innovation technique. 
Dans l’après-guerre, les gouvernements successifs redoublent d’efforts afin 
de renforcer les capacités militaires des États-Unis. Ils font des investis-
sements gigantesques en recherche et développement. Ces financements 
passent de 1,8 milliard de dollars en 1951 à 19,8 milliards en 1975 et repré-
sentent pendant cette période plus de la moitié des investissements de 
recherche et développement aux États-Unis. L’État fédéral crée aussi une 
multiplicité d’agences pour financer et guider la conception de nouvelles 
technologies. Le plus innovateur de ces organismes d’un point de vue 
managérial est sans doute l’Advanced Research Projects Agency (ARPA, 
connue plus tard sous le nom de DARPA), une agence du ministère de 
la Défense. Les cadres de l’ARPA sélectionnent les meilleurs groupes  
de recherche dans les universités et les entreprises et suivent leurs progrès 
de très près. Ils intègrent ainsi les activités de ces groupes en un programme 
cohérent, qui développe des technologies avancées pour l’armée 1.
Le foisonnement des agences et programmes fédéraux crée un système 
décentralisé de l’innovation qui aiguillonne la concurrence entre les 
organismes publics et les institutions qu’ils financent et qui stimule ainsi 
la conception concomitante d’un très grand nombre de technologies 
militaires. Les programmes de R & D financés par l’État fédéral et les achats 
massifs de matériels par l’armée donnent une impulsion décisive à l’inno-
vation dans les matériaux, l’électronique, l’informatique et l’aéronautique. 
Par exemple, pour rendre ses systèmes d’armement et de communica-
tions plus fiables, le ministère de la Défense pousse à la numérisation des 
techniques. Par l’intermédiaire de contrats de recherche et la passation 
de marchés publics, il finance le développement des semi-conducteurs, 
ordinateurs et réseaux informatiques, ainsi que leur intégration dans les 
armements et les réseaux de communication 2.
Les investissements massifs de l’État fédéral dans l’innovation et la 
relance des procès antitrust dans l’immédiat après-guerre entraînent  
la croissance et la restructuration de la recherche dans les grandes entre-
prises. Les grands groupes reçoivent plus de la moitié des financements 
fédéraux de R & D. Ils accroissent beaucoup leurs propres investissements 
de R & D pour participer aux révolutions techniques que promettent 
les programmes de recherche fédéraux. Ces investissements industriels 
passent de 2,3 milliards de dollars en 1954 à 15,5 milliards en 1975. Le 
nombre de sociétés ayant leurs propres laboratoires quintuple en passant 
de 2 000 en 1940 à plus de 11 000 en 1975. En outre, beaucoup de sociétés 
1. Statistiques historiques de la NSF.
2. Lécuyer 2006.

	
manager l’innovation	
431
créent des laboratoires centraux qui renforcent leurs compétences scienti-
fiques et développent des produits dérivés de la recherche fondamentale 1.
Simultanément, certaines entreprises mettent en place de nouveaux 
mécanismes pour accélérer l’innovation et transférer le plus vite possible 
produits et procédés conçus dans les laboratoires vers les unités de fabri-
cation. Ces efforts d’accélération de l’innovation sont particulièrement 
prononcés dans les composants électroniques, essentiels pour les systèmes 
d’armement. Les composants sont aussi caractérisés par l’imbrication des 
dispositifs et des procédés de fabrication et demandent une collaboration 
particulièrement étroite entre R & D et production. Les Bell Telephone 
Laboratories sont à la pointe de ces innovations. En 1945, les dirigeants 
des Bell Labs réorganisent leurs laboratoires. Ils créent des groupes de 
recherche interdisciplinaires composés de physiciens, chimistes et spécia-
listes des matériaux pour s’attaquer à des problèmes difficiles tels que la 
création d’amplificateurs utilisant la physique des états solides. Un de ces 
groupes, dirigé par William Shockley, invente le transistor à pointes en 
1947 et met au point le transistor à jonction quelques années plus tard 2.
Les dirigeants des Bell Labs établissent aussi un laboratoire de dévelop-
pement dans une usine de Western Electric en 1945 (Western Electric est 
la filiale d’AT & T qui produit les équipements téléphoniques). Ce labora-
toire aide au transfert des tubes électroniques conçus par les équipes des 
Bell Labs aux lignes de fabrication de l’usine. Pendant les années 1950 et 
au début des années 1960, la direction des Bell Telephone Laboratories 
installe plusieurs laboratoires de ce type dans d’autres usines de Western 
Electric pour accélérer le transfert des transistors et des circuits intégrés 
à la production. Beaucoup de fabricants de composants électroniques 
suivent l’exemple des Bell Labs et créent des équipes de recherche pluri-
disciplinaires ainsi que leurs propres mécanismes de transfert interne 
pendant les années 1950 et 1960 3.
Dans l’après-guerre, les grandes entreprises sont néanmoins de plus en 
plus concurrencées par de nouveaux acteurs de l’innovation, les univer-
sités et les start-up. Pendant les premières décennies du xxe siècle, les 
universités avaient souvent travaillé pour les entreprises, s’attaquant à 
des problèmes d’ordre pratique que les sociétés leur soumettaient. Cette 
situation change avec la crise de 1929 qui tarit les financements indus-
triels et, plus encore, avec la Seconde Guerre mondiale et la guerre froide. 
1. Hounshell et Smith 1988, Hounshell 1996, Lécuyer et Choi 2012.
2. Morton 1971. Les grandes entreprises adoptent aussi la méthode PERT et les formes 
d’organisation matricielle pour accélérer la conception de nouveaux systèmes d’armement 
pendant les années 1950 et 1960.
3. Morton 1971.

432	
christophe lécuyer
Les investissements massifs du gouvernement fédéral dans la recherche 
universitaire font des universités des concurrents des grandes entreprises.  
Avec l’argent fédéral, des universités comme le MIT, Stanford et UC Berkeley 
se lancent dans le développement de nouveaux matériaux et composants 
et dans la conception de radars, d’ordinateurs, de systèmes de guidage et 
de télécommunications. Elles font des innovations importantes comme 
les noyaux magnétiques utilisés dans la mémoire des ordinateurs. Avec 
le soutien de l’armée, les universités transfèrent ces technologies vers 
l’industrie. Par exemple, un groupe autour de Jay Forrester, qui invente 
les noyaux magnétiques et conçoit l’ordinateur SAGE au MIT, transfère 
ces technologies à IBM, qui les produit et les intègre dans un système de 
détection de missiles balistiques. À la fin des années 1960, les universités 
sont devenues des acteurs à part entière de l’innovation, tout particuliè-
rement dans les hautes technologies 1.
La croissance du marché militaire et l’affaiblissement des droits de 
la propriété industrielle entraînent aussi l’apparition d’une nouvelle 
institution de l’innovation dans l’après-guerre : les start-up. Elles se 
concentrent sur les domaines techniques les plus en pointe et commer-
cialisent des technologies souvent conçues dans les universités et les 
grandes entreprises. L’essor des start-up doit beaucoup à la création  
de nouvelles formes de financement de l’innovation. Des financiers de la  
région de Boston et des administrateurs du MIT, qui s’inquiètent de  
la rareté des capitaux pour les nouvelles entreprises de haute techno-
logie, inventent le capital-risque à la fin des années 1940 et au début des 
années 1950. Le capital-risque se répand ensuite sur la côte Ouest, tout 
particulièrement dans la Silicon Valley, pendant les années 1960 et 1970. 
Les capital-risqueurs financent la constitution d’équipes de cadres et 
d’ingénieurs et la conception de nouveaux produits en prenant des parts 
dans l’actionnariat de jeunes sociétés. Ils récupèrent ensuite leurs inves-
tissements en introduisant les actions de ces entreprises sur les marchés 
boursiers. Les fonds de capital-risque investissent sur un horizon de  
cinq ans 2.
Cet objectif financier et l’âpre concurrence qui règne dans les industries 
de pointe conduisent les start-up à accélérer le processus d’innovation – à 
réduire le laps de temps entre invention et commercialisation. Il est 
en effet vital pour les jeunes sociétés d’introduire leurs produits sur le 
marché plus vite que les grands entreprises, qui disposent de ressources 
techniques et financières et de forces de vente bien supérieures. Pour 
1. Leslie 1992.
2. Lécuyer 2006.

	
manager l’innovation	
433
innover le plus vite possible, certaines start-up créent plusieurs groupes 
de développement internes, qui se concurrencent pour concevoir de 
nouvelles technologies et les mettre sur les marchés. D’autres start-up, 
surtout à partir de la fin des années 1960, décident de faire leur R & D 
sur les chaînes de production plutôt que dans des laboratoires indépen-
dants (les transferts entre laboratoires et unités de production, malgré 
les innovations organisationnelles des Bell Labs, sont en effet le point 
faible de la recherche industrielle, telle qu’elle est pratiquée depuis le 
début du siècle). Par exemple, Intel, une start-up de la Silicon Valley qui 
se spécialise dans les mémoires électroniques et les microprocesseurs, ne 
crée pas son propre laboratoire de recherche. Ses ingénieurs conduisent 
leurs projets de R & D directement sur les chaînes de fabrication. C’est 
en observant les procédés qu’ils conçoivent de nouveaux produits. Ce 
modèle d’innovation confère des avantages très substantiels aux start-up. 
Dans certaines industries comme les semi-conducteurs, elles concur-
rencent de plus en plus les grandes entreprises, au point de les forcer à 
abandonner des secteurs d’activité tout entiers 1.
Acheter l’innovation
À la fin des années 1970 et pendant les années 1980, les États-Unis 
connaissent une crise de l’innovation. Les grands laboratoires indus-
triels sont de plus en plus concurrencés par les start-up. Ils deviennent 
de moins en moins productifs et rencontrent de grandes difficultés à 
commercialiser leurs propres produits. Les dirigeants des grands groupes, 
qui font face à une concurrence étrangère accrue, réduisent les effectifs 
des laboratoires centraux et les réorientent vers les problèmes immédiats 
des entreprises. Certains laboratoires de recherche qui ont longtemps 
dominé l’innovation industrielle, comme les Bell Telephone Laboratories, 
déclinent, avant de fermer définitivement leurs portes. L’État fédéral 
se désengage en partie de la recherche militaire et se réoriente vers les 
sciences de la vie. Il voit aussi sa place relative dans le financement de la 
R & D diminuer 2. En même temps, de nouveaux rivaux commerciaux 
apparaissent, comme le Japon, qui maîtrise mieux le processus d’inno-
vation. Les entreprises japonaises introduisent les produits plus vite 
sur les marchés et ces produits sont souvent de meilleure qualité. Elles 
1. Lécuyer 2006, Lécuyer et Brock 2010, Lécuyer et Choi 2012.
2. La part de l’État fédéral dans le financement de la recherche et développement passe de près 
de la moitié du financement total de la R & D en 1982 au quart en 2000.

434	
christophe lécuyer
conquièrent ainsi le marché américain de l’électronique grand public et 
menacent la survie d’autres secteurs industriels comme l’automobile et 
les semi-conducteurs.
Pour répondre au défi japonais, l’administration Reagan encourage les 
acteurs de l’innovation à coopérer. Le consensus aux États-Unis est en effet 
que les sociétés japonaises doivent leurs succès techniques et commerciaux 
aux collaborations entre entreprises organisées par le MITI, le ministère 
japonais du Commerce extérieur et de l’Industrie. Pour créer le cadre 
législatif qui permettrait ce genre de collaborations aux États-Unis, le 
Congrès vote le National Cooperative Research Act en 1984, qui exempte 
les collaborations de R & D des lois contre les trusts. En coopération 
avec les entreprises, le ministère de la Défense finance aussi la création 
de consortiums, comme SEMATECH qui mène des recherches sur les 
procédés de fabrication pour l’industrie microélectronique. Jugeant qu’il 
est important de renforcer les liens entre entreprises et universités (les 
universités étant perçues comme un atout dans la guerre économique 
avec le Japon), la National Science Foundation (NSF), une autre agence 
de l’État fédéral, crée plus d’un millier de centres de recherche dans les 
universités. Ces centres sont financés à la fois par les sociétés et la NSF 
et conduisent des recherches sur des technologies génériques impor-
tantes pour les entreprises 1.
L’administration Reagan renforce aussi les droits de la propriété intellec-
tuelle pour protéger les sociétés américaines de la concurrence étrangère. 
Pour ce faire, elle crée en 1982 un nouveau tribunal fédéral, la Court of 
Appeals for the Federal Circuit, qui centralise à l’échelle des États-Unis 
tous les recours intentés à la suite de procès sur la violation de brevets. 
La création de cette chambre d’appel transforme profondément le régime 
de la propriété industrielle. À la différence des chambres régionales 
qu’il remplace, le nouveau tribunal soutient les droits des détenteurs  
de brevets. Il force les entreprises qui violent des brevets à payer de très 
fortes amendes et à cesser de vendre les produits incriminés. Ces décisions 
confèrent une valeur marchande beaucoup plus grande aux brevets et 
aux licences. Le nombre de dépôts de brevets et la valeur des licences 
augmentent très rapidement. La croissance du marché de la propriété 
industrielle attire des intermédiaires qui, comme leurs prédécesseurs à la 
fin du xixe siècle, vivent du commerce des brevets. De nouvelles entités, 
connues souvent sous le nom de « trolls », apparaissent aussi, qui forcent 
les grands groupes à leur payer des redevances sur les brevets qu’elles ont 
acquis. Pour se protéger des trolls, les sociétés de hautes technologies 
1. Block et Keller 2009.

	
manager l’innovation	
435
créent alors des portefeuilles collectifs de brevets, comme les entreprises 
de chemins de fer l’avaient fait un siècle plus tôt.
L’apparition d’un marché très actif de la propriété industrielle et le 
renforcement des collaborations entre entreprises et entre sociétés et 
universités recomposent le paysage de l’innovation aux États-Unis. Ils 
poussent les entreprises à revenir, en partie, aux formes d’organisation de 
l’innovation de la fin du xixe siècle et du début du xxe. Les grands groupes 
externalisent l’innovation. Ils s’appuient de plus en plus sur des acteurs 
externes pour concevoir et développer les technologies et les produits qu’ils 
commercialisent. La fonction principale des laboratoires de recherche 
devient alors d’identifier les savoirs et les technologies conçus à l’exté-
rieur, d’acheter les droits sur ces technologies et de les intégrer au sein 
des entreprises. Les achats de technologies prennent plusieurs formes. 
Certaines sociétés acquièrent des start-up pour innover. C’est le cas de 
Cisco Systems, l’entreprise de matériels de télécommunications installée 
dans la Silicon Valley, qui achète plusieurs dizaines de start-up spécia-
lisées dans l’interconnexion des réseaux pendant les années 1990. Cisco 
fait peu de recherche en interne et se repose beaucoup sur ces acquisi-
tions pour mettre de nouveaux produits sur le marché 1.
D’autres sociétés financent la R & D dans les universités pour accéder 
aux nouvelles technologies. Monsanto et les entreprises pharmaceutiques, 
par exemple, concluent de très gros contrats de recherche avec le MIT, 
Harvard et UCSF, pour acquérir les compétences dans les biotechno-
logies que ces universités ont développées avec les financements de l’État 
fédéral. Intel fait aussi appel aux universités pour développer les savoirs 
dont l’entreprise a besoin. Utilisant les techniques managériales conçues 
par la DARPA, les cadres d’Intel intègrent ces projets universitaires en 
des programmes cohérents et transfèrent les résultats les plus promet-
teurs des équipes de recherche universitaires aux groupes concevant 
les nouveaux microprocesseurs et procédés de fabrication au sein de la 
société. Intel se transforme aussi en capital-risqueur avec la formation 
de son propre fonds de capital-risque. Ce fonds finance les start-up qui 
accélèrent l’utilisation de ses produits 2.
Dans ce nouveau paysage de l’innovation, les universités assument 
de plus en plus les fonctions des laboratoires de recherche centraux de 
l’après-guerre. Elles font la recherche fondamentale que les grandes entre-
prises ne font plus et elles assument les risques et une partie du coût de 
ces travaux. Les contrats de recherche industriels représentent en effet 
1. Mowery 2009, Chesbrough 2003.
2. Chesbrough 2003.

436	
christophe lécuyer
une part croissante des financements. Pendant la première moitié des 
années 1990, ils financent plus du tiers des dépenses de recherche au 
MIT. Les universités, qui acquièrent, en 1980, avec le Bayh-Dole Act, 
le droit de breveter les inventions faites par leurs employés avec des 
financements fédéraux, revendiquent de plus en plus leurs droits sur 
les technologies et les logiciels produits par les étudiants et le corps 
professoral. Elles se lancent dans le commerce de la propriété intellec-
tuelle et créent des services de transfert de technologies dont le but est 
de maximiser les redevances venant de l’industrie. Ce commerce est très 
lucratif pour certaines d’entre elles. Par exemple, le brevet de Stanley 
Cohen et Herbert Boyer portant sur la recombinaison génétique rapporte 
plus de 200 millions de dollars à Stanford et à l’université de Californie 
pendant les années 1980 et 1990. En revanche, beaucoup d’autres univer-
sités perdent de l’argent dans leurs activités de transfert de technologie.
Afin de structurer le paysage de l’innovation et coordonner les activités 
de nombreux acteurs qui ont souvent des intérêts divergents, de nouvelles 
institutions apparaissent à la fin des années 1980 et pendant les années 
1990 : les feuilles de route. Les mécanismes de marché ne sont pas en effet 
suffisants pour organiser l’innovation et lui donner une direction sur le 
long terme. Les feuilles de route, qui sont dominées pour la plupart par 
les grandes entreprises, reprennent les fonctions d’impulsion des agences 
fédérales pendant la guerre froide et elles organisent les activités d’inno-
vation de nombreuses institutions. Elles apparaissent tout d’abord dans 
l’industrie microélectronique, où le modèle marchand de l’innovation 
s’est développé le plus tôt, et se répandent plus tard dans d’autres indus-
tries de pointe 1.
La feuille de route la plus connue est sans doute la National Technology 
Roadmap for Semiconductors (NTRS), qui est créée en 1992 par le syndicat 
patronal de l’industrie des semi-conducteurs. La NTRS définit les grands 
objectifs techniques pour l’industrie microélectronique tout entière sur 
un horizon de quinze ans. Elle identifie aussi les obstacles auxquels les 
ingénieurs doivent faire face pour atteindre ces objectifs et proposent des 
solutions à ces problèmes. La NTRS façonne profondément l’innovation 
dans les semi-conducteurs. En créant un consensus sur les orientations 
futures de la microélectronique, elle guide la complexification toujours 
plus grande des circuits intégrés et le développement de nouveaux 
matériaux et procédés de fabrication qui les rendent possibles. Les entre-
prises, les universités, les laboratoires nationaux, les agences fédérales 
et les consortiums tels que SEMATECH reprennent les objectifs de la 
1. Lécuyer et Choi 2012.

	
manager l’innovation	
437
NTRS et utilisent ses conclusions pour guider leurs investissements de 
R & D et établir des collaborations avec d’autres organisations. Depuis 
une quinzaine d’années, d’autres industries comme la chimie, l’énergie, 
les cellules photovoltaïques, les nanotechnologies et les biotechnologies 
créent leurs propres feuilles de route pour organiser et orienter l’inno-
vation. Les feuilles de route y ont la même fonction : elles permettent 
aux grandes sociétés d’imposer leurs objectifs techniques, de créer des 
réseaux de collaboration avec des petites et moyennes entreprises et 
d’orienter les financements de R & D de l’État fédéral 1.
Le triomphe de l’innovation marchande a un effet en retour et suscite 
l’apparition du mouvement open source en informatique, qui rejette les 
nouvelles formes du management de l’innovation. Beaucoup d’informati-
ciens, qui ont librement échangé leurs programmes pendant les années 1960 
et 1970, se rebellent contre la marchandisation de l’innovation et la place 
toujours plus grande de la propriété intellectuelle. Ils s’opposent tout parti-
culièrement à ce que les entreprises et les universités interdisent la libre 
distribution de leurs logiciels par l’intermédiaire des licences. En 1985, 
un groupe de programmeurs du MIT crée la Free Software Foundation 
qui conçoit et diffuse un nouveau type de licence donnant aux usagers 
d’un programme le droit de l’utiliser gratuitement, de le modifier et de 
le distribuer – sans pour autant pouvoir l’incorporer dans des produits 
commerciaux. En bref, la fondation emploie les outils de l’innovation 
marchande pour mieux lutter contre elle. La croissance de l’Internet permet 
au mouvement des logiciels libres de prendre une très grande ampleur 
pendant les années 1990. Des centaines de milliers de bénévoles conçoivent 
alors des logiciels de façon collective et utilisent les licences conçues par 
la Free Software Foundation pour les rendre libres d’accès et d’utilisation. 
Ce mouvement devient si puissant que de nombreuses sociétés d’infor-
matique doivent s’adapter et rendent publics certains de leurs logiciels. 
D’autres entreprises sont créées pour maintenir les logiciels open source, 
les améliorer et expliquer leur fonctionnement à leurs utilisateurs 2.
Conclusion
L’histoire de la gestion de l’innovation aux États-Unis au xxe siècle peut 
se résumer en trois verbes : contrôler, accélérer et acheter. Tout d’abord, les 
plus grandes sociétés assimilent l’innovation et créent des laboratoires de 
1. Lécuyer et Choi 2012.
2. Hippel 2005.

438	
christophe lécuyer
recherche pour mieux la contrôler. À partir de la Seconde Guerre mondiale, 
un nouvel acteur de l’innovation, l’État fédéral, ainsi qu’une nouvelle 
catégorie d’entreprises, les start-up, apparaissent. Ils se focalisent sur l’accé-
lération de l’innovation et la création de nouvelles industries. De la crise 
de la fin des années 1970 naissent de nouvelles formes du management 
de l’innovation. Avec le soutien de l’État fédéral, les sociétés externalisent 
alors le processus d’innovation. Elles se désengagent de la recherche et 
achètent les savoirs et les technologies sur les marchés de la propriété 
intellectuelle pour concevoir et commercialiser de nouveaux produits.
Ces mutations sont façonnées par l’évolution de la législation sur la 
concurrence et les transformations du régime de la propriété industrielle. 
Elles doivent aussi beaucoup aux politiques économiques et aux choix 
militaires des gouvernements successifs. Enfin, l’évolution des formes de 
gestion de l’innovation est liée à l’apparition de nouvelles technologies. 
L’innovation managériale suit, en quelque sorte, l’innovation technique. 
Mais cela ne veut pas dire que les technologies déterminent les formes 
sociales de leur exploitation. Les ingénieurs, cadres industriels et hauts 
fonctionnaires expérimentent avec de nouvelles formes d’organisation et 
de financement de l’innovation afin de tirer le meilleur parti des nouvelles 
technologies. Il se trouve que certaines innovations organisationnelles se 
prêtent mieux que d’autres à l’innovation technique dans certains secteurs. 
Par exemple, les dirigeants des Bell Telephone Laboratories créent des 
groupes de recherche interdisciplinaires et établissent des antennes de 
leurs laboratoires de développement dans les usines de Western Electric 
pour concevoir les transistors et les transférer le plus vite possible aux 
unités de production. Mais la forme organisationnelle qui se prête le 
mieux à l’exploitation des technologies de la microélectronique et à leur 
commercialisation apparaît ailleurs, dans les start-up de la Silicon Valley 
qui font toute leur R & D directement sur les lignes de fabrication.
Les États-Unis sont particulièrement novateurs dans l’organisation 
du processus d’innovation et constituent un modèle dans ce domaine 
tout au long du xxe siècle. Les entreprises américaines s’inspirent certes 
des laboratoires industriels allemands et des collaborations japonaises 
en R & D. Mais c’est aux États-Unis que les laboratoires de recherche indus-
triels se développent le plus et que de nouvelles formes du management 
de l’innovation telles que les start-up et le capital-risque sont inventées. 
C’est aussi en Amérique que le processus de démantèlement des grands 
laboratoires de recherche industrielle commence le plus tôt et que les 
grandes entreprises s’orientent le plus vers des acteurs extérieurs pour 
renforcer leurs processus d’innovation. Beaucoup de ces innovations 
managériales sont ensuite adoptées en Europe et au Japon. Par exemple, 

	
manager l’innovation	
439
le modèle marchand de l’innovation qui est souvent popularisé sous le 
nom d’« innovation ouverte » est de plus en plus adopté en Europe depuis 
une quinzaine d’années.
Références bibliographiques
Block Fred et Keller Matthew, 2009, « Where Do Innovations Come From ? Trans-
formations in the United States Economy (1970-2006) », Socio-Economic Review, 
vol. 7, p. 459-483.
Brock David et Lécuyer Christophe, 2012, « Digital Foundations : The Making 
of Silicon Gate Manufacturing Technology », Technology and Culture, vol.  53, 
p. 561-597.
Chandler Alfred, 1988, La Main visible des managers, Paris, Economica.
Chesbrough Henry, 2003, Open Innovation : The New Imperative for Creating and 
Profiting from Technology, Boston (MA), Harvard Business School Press.
Gertner Jon, 2012, The Idea Factory : Bell Labs and the Great Age of American 
Innovation, New York, Penguin Press.
Hart David, 1998, Forged Consensus : Science, Technology, and Economic Policy in the 
United States (1921-1953), Princeton (NJ), Princeton University Press.
Hippel Eric von, 2005, Democratizing Innovation, Cambridge (MA), MIT Press.
Hounshell David, 1996, « The Evolution of Industrial Research in the United 
States », in Richard Rosenbloom et William Spencer (dir.), Engines of Innovation, 
Boston (MA), Harvard Business School Press, p. 13-69.
Hounshell David et Smith John, 1988, Science and Corporate Strategy : DuPont 
R & D (1902-1980), New York, Cambridge University Press.
Hughes Thomas, 1989, American Genesis : A Century of Invention and Technological 
Enthusiasm, Chicago (IL), University of Chicago Press.
Lamoreaux Naomi et Sokoloff Kenneth, 1999, « Inventors, Firms, and the Market 
for Technology in the Nineteenth and Early Twentieth Centuries », in  Naomi 
Lamoreaux, Daniel Raff et Peter Temin (dir.), Learning by Doing in Markets, 
Firms, and Countries, Chicago (IL), University of Chicago Press, p. 19-60.
Lécuyer Christophe, 2006, Making Silicon Valley : Innovation and the Growth of High 
Tech (1930-1970), Cambridge (MA), MIT Press.
Lécuyer Christophe et Brock David, 2010, Makers of the Microchip : A Documentary 
History of Fairchild Semiconductor, Cambridge (MA), MIT Press.
Lécuyer Christophe et Choi Hyungsub, 2012, « Les secrets de la Silicon Valley ou 
les entreprises américaines de microélectronique face à l’incertitude technique », 
La Revue d’histoire moderne et contemporaine, vol. 59, no 3, p. 48-69.
Leslie Stuart, 1992, The Cold War and American Science : The Military-­Industrial-
Academic Complex at MIT and Stanford, New York, Columbia University Press.
Morton Jack, 1971, Organizing for Innovation : A Systems Approach to Technical 
Management, New York, McGraw-Hill.
Mowery David, 2009, « Plus ça change : Industrial R & D in the “Third Industrial 
Revolution” », Industrial and Corporate Change, vol. 18, p. 1-50.
Noble David, 1977, America by Design : Science, Technology and the Rise of Corporate 
Capitalism, New York, Knopf.
Reich Leonard, 1985, The Making of American Industrial Research : Science and 
Business at GE and Bell (1876-1926), New York, Cambridge University Press.


21 Chine : la fabrication  
d’une superpuissance  
technoscientifique
C o n g  C a o
Au Moyen Âge, la Chine est une des régions du monde les plus avancées 
sur le plan technologique et économique. Mais elle se satisfait du statu 
quo et reste isolée du reste du monde, en particulier de l’Europe de la 
Renaissance, où une vision systémique et structurelle de la nature se 
développe et s’épanouit. Armé de l’esprit capitaliste, l’Occident non 
seulement dépasse la Chine sur le plan économique, mais est également 
le théâtre de l’émergence de la science et de la technologie modernes. La 
série de défaites militaires de la Chine après 1840 marque le début d’une 
période sombre pour ce pays autrefois puissant.
En 1911, la révolution met fin à des milliers d’années de pouvoir féodal 
en Chine et, en 1919, le mouvement du 4 Mai défend le développement 
de la science et la démocratie, considérés comme les deux facteurs essen-
tiels du succès de l’Occident. Le régime nationaliste établit les bases 
institutionnelles nécessaires au développement de la science et de la 
technologie, et forme une génération de scientifiques et de professionnels 
pouvant servir le régime. Ceux-ci seront mobilisés par son successeur, la 
République populaire de Chine. Pour dynamiser son rêve de redevenir 
une grande puissance, l’État chinois place la science et la technologie au 
cœur de ses priorités. Malgré des progrès impressionnants, notamment 
dans les programmes d’armement stratégique et dans quelques domaines 
de recherche fondamentale, l’entreprise scientifique chinoise se voit 
cependant contrariée dans ses avancées par les coups d’arrêt répétés 
provoqués par les politiques radicales menées dans le pays, les scienti-
fiques étant soumis à des conditions de vie tumultueuses, régulièrement 
cibles d’attaques.
Après 1978, la Chine connaît une période beaucoup plus stable, pendant 
 100 étudiants chinois de retour avec leurs familles de leur formation aux États-Unis à bord du 
SS President Cleveland, septembre 1950.

442	
cong cao
laquelle la recherche scientifique n’est plus menacée par les chocs et 
déboires des trente premières années du régime communiste. La Chine 
ouvre alors un nouveau chapitre de sa tumultueuse histoire.
Premiers contacts
Les premiers contacts de la Chine avec la science et la technologie 
modernes furent fortuits. Dès le xvie siècle, conséquence indirecte de 
leurs entreprises d’évangélisation, les missionnaires étrangers introduisent 
les sciences en Chine, notamment les mathématiques et l’astronomie, 
avec la traduction en chinois de certains ouvrages scientifiques majeurs 
de l’époque 1. Par exemple, en 1607, le prêtre jésuite italien Matteo Ricci 
traduit les six premiers volumes des Éléments de géométrie d’Euclide avec 
l’aide du savant chinois Xu Guangqi.
Mais c’est la défaite de la Chine lors de la guerre de l’Opium en 1840 
qui oblige la cour des Qing à s’ouvrir à la science. Par la suite, certains 
attribuent de manière un peu hâtive l’échec des Qing au manque de 
moyens militaires et technologiques 2. Dans le but d’empêcher l’effon-
drement total de la dynastie Qing, des officiels et savants libéraux voulant 
s’adapter entreprennent alors des efforts d’occidentalisation. Ils achètent 
des équipements et des armes à l’Ouest, créent des usines modernes, 
invitent des experts étrangers, fondent des écoles d’un nouveau style et 
traduisent davantage d’ouvrages scientifiques.
À cette époque également, des étudiants, dont certains sont à peine 
âgés de neuf ans, sont envoyés aux États-Unis, au Japon et en Europe pour 
être immergés dans les idées occidentales, dans l’espoir qu’ils puissent 
à leur retour moderniser la Chine et assurer sa prospérité 3. Par la suite, 
certains d’entre eux en viennent effectivement à constituer l’élite du 
pays dans divers domaines et à jouer un rôle dans la société chinoise. Ils 
prennent également part au mouvement du 4 mai 1919, en défenseurs 
actifs de la « science » et de la « démocratie ».
Un certain nombre de Chinois ont également reçu une éducation 
scientifique et une formation d’ingénieur grâce aux bourses d’indemnités 
des Boxers, financées par les réparations de guerre payées par la Chine 
1. Les missions étrangères ont également joué un rôle majeur dans l’enseignement supérieur 
chinois au cours de la seconde moitié du xixe siècle et la première moitié du xxe.
2. En réalité, il est difficile de maintenir cette analyse concernant un empire dont l’économie 
représentait un tiers de l’économie mondiale dans les années 1820. La cause est plutôt à rechercher 
dans la corruption d’un système politique qui ne pouvait pas s’adapter au développement rapide 
du capitalisme. Cf. Zhu 2008.
3. Leibovitz et Miller 2011, Wang 1966, Ye 2001.

	
la chine puissance technoscientifique	
443
après la signature du protocole des Boxers 1. À la fin de leurs études, ils 
reviennent avec l’objectif d’introduire les sciences modernes en Chine, 
espérant sauver le pays au moyen du savoir scientifique. C’est grâce à 
leurs efforts que les premières organisations scientifiques, institutions 
de recherche indépendantes et universités modernes voient le jour sur 
le sol chinois.
Construction institutionnelle
La Société des sciences de Chine, fondée en 1914 par des étudiants 
chinois en sciences naturelles à l’université Cornell aux États-Unis, est 
parmi les plus influentes pour la diffusion de la science moderne. Ses 
membres estiment nécessaire de suivre un modèle étranger pour organiser 
les activités scientifiques. La Société des sciences imite ainsi la Société 
américaine pour l’avancement des sciences (American Association for the 
Advancement of Science, AAAS). Son nom entier, la « Société chinoise 
pour l’avancement des sciences », est d’ailleurs calqué sur celui de l’insti-
tution états-unienne, tout comme sa revue est nommée d’après la revue 
de l’AAAS. À leur retour en Chine, les membres de la société multiplient 
les efforts pour défendre l’importance des sciences, persuader le gouver-
nement et les citoyens de s’intéresser à la recherche scientifique et de la 
soutenir, et organiser la recherche scientifique elle-même 2.
Le Service géologique fondé en 1916 est le premier institut de recherche 
scientifique de la Chine moderne 3. En 1922, la Société des sciences crée 
le Service biologique de Nankin. Des hommes d’affaires investissent 
également dans les organismes de recherche. L’Institut d’ingénierie 
chimique de la mer Jaune est ainsi fondé par Fan Xudong de la société 
de sel raffiné Jiuda 4.
Les sciences modernes s’institutionnalisent avec la création d’univer-
sités. Après les années 1920, un grand nombre de Chinois titulaires de 
doctorats d’universités occidentales en mathématiques, en physique et 
dans les disciplines des sciences de l’ingénieur reviennent en Chine. Grâce 
à leur engagement, et grâce au soutien de la société, l’enseignement de 
premier cycle en Chine atteint rapidement les niveaux internationaux 
1. Hunt 1972. – Les indemnités du protocole des Boxers reçues par le gouvernement états-unien 
furent également utilisées pour fonder l’école Tsinghua. Créée à l’origine en tant qu’école 
préparatoire pour les étudiants qui devaient aller aux États-Unis, elle est devenue une institution 
connue aujourd’hui sous le nom d’université Tsinghua.
2. Buck 1980 (p. 91-98), Wang 2002, Ye 2001.
3. Furth 1970 (p. 34-65).
4. Yao et al. 1994 (p. 5).

444	
cong cao
et l’enseignement supérieur prend son envol. Dans certaines univer-
sités nationales comme Tsinghua et Pékin, des scientifiques font de la 
recherche, établissant ainsi les universités chinoises comme des lieux 
importants non seulement pour former du personnel scientifique, mais 
aussi pour conduire des recherches 1.
L’invasion japonaise de la Chine oblige l’université Tsinghua et les univer-
sités de Pékin et de Nankai à quitter respectivement Pékin (aujourd’hui 
Beijing) et Tianjin pour se replier sur Kunming, dans la province du 
Yunnan, où elles s’associent au sein de l’université du Sud-Ouest. Malgré 
sa courte histoire (d’avril 1938 à mai 1946) et les difficultés extrêmes liées 
à la guerre, cette université joue un rôle important 2.
La fondation d’une académie centrale est mise à l’ordre du jour peu 
de temps après le choix de Nankin comme capitale par le gouvernement 
nationaliste en 1927. L’Academia sinica est fondée un an plus tard avec  
Cai Yuanpei comme premier président. Enseignant ayant suivi à la fois  
une formation en confucianisme et en science occidentale, Cai est président 
de l’université de Pékin où il met en œuvre une vision d’inspiration germa-
nique de l’administration universitaire, ainsi que l’autonomie (ou « règle 
professorale ») et la liberté académique. Il introduit la même idée lors 
de la création de l’Academia sinica, qui occupe ensuite une place impor-
tante dans le paysage de la science chinoise de l’époque nationaliste 3.
En 1929 est fondée une autre grande institution de recherche,  
l’Académie de Pékin. La création de ces deux académies (l’Academia sinica 
au sud, qui regroupe principalement des scientifiques ayant étudié aux 
États-Unis, et l’Académie de Pékin au nord, qui réunit des savants de retour 
d’Europe) marque les débuts du système indépendant de recherche scien-
tifique en Chine. Ce système influence également le développement de 
la recherche et de l’éducation, ces deux académies ayant servi de fonda-
tions pour l’Académie chinoise des sciences (ACS).
L’influence soviétique
Le développement des sciences et des technologies des débuts de la 
République populaire de Chine fondée en 1949 est très fortement marqué 
par l’Union soviétique. L’ACS adopte un fonctionnement à la soviétique, 
calqué sur celui de l’Académie soviétique des sciences pour ce qui est de 
1. Hayhoe 1996 (p. 29-71).
2. Israel 1998.
3. Gao 1996 [1982] (p. 410-414).

	
la chine puissance technoscientifique	
445
l’administration de la recherche, du rôle du Parti communiste au sein de 
l’institution et de la mise en place d’un système d’adhésion par divisions 
académiques. La Chine suit également le modèle de l’Union soviétique 
pour la mise en place d’instituts et de laboratoires de recherche spécia-
lisés dans les ministères.
Également sur les conseils des Soviétiques, le gouvernement chinois 
réorganise son système universitaire en 1952, fusionnant les départe-
ments entre universités et facultés 1. Ainsi, les facultés d’arts, de droit et 
de sciences naturelles de l’université Tsinghua sont intégrées à l’université 
de Pékin, tandis que Tsinghua absorbe les départements d’ingénierie des 
universités de Pékin et de Yenching, pour devenir une école polytech-
nique. Des facultés spécialisées dans des domaines comme l’ingénierie, 
les sciences médicales et l’agriculture, sont constituées par transfert des 
enseignants et des étudiants depuis d’autres institutions. Par exemple,  
la faculté d’agriculture de Pékin est construite sur la faculté d’agriculture 
de Tsinghua. En conséquence, les universités commencent à se concentrer 
sur des enseignements spécialisés plutôt que sur des enseignements inter-
disciplinaires, tandis que leur activité de recherche est progressivement 
transférée soit vers l’ACS, soit vers des instituts de recherche minis-
tériels qui font exclusivement de la recherche, ce qui sépare d’autant 
plus recherche et éducation. La réorganisation des spécialités entraîne 
également une rupture du lien entre recherche fondamentale, recherche 
appliquée et développement, ce qui a à son tour un impact sur la formation 
du personnel scientifique à long terme. De plus, pour un grand nombre 
d’enseignants et de scientifiques de la période antérieure à 1949, la réorga-
nisation ne se limite pas à un simple changement institutionnel. S’ils 
disposent encore au cours des premières années du pouvoir commu-
niste d’une relative liberté académique via l’exercice de l’autonomie dans 
l’enseignement et la recherche et le maintien de leur autorité en tant que 
professeurs, ils perdent le confort institutionnel dans lequel ils poursui-
vaient leurs propres intérêts académiques.
Autre marque de fabrique soviétique, la planification a également eu 
une grande influence en Chine. Comme le développement économique 
et social, la science est planifiée et les scientifiques chinois mobilisés 
pour élaborer des plans avant de les mettre en œuvre. Le premier du 
genre est le plan sur douze ans (1956-1967) pour la science et la techno-
logie, dont l’objectif est de suivre le développement rapide de la science 
et de la technologie au niveau mondial, ainsi qu’un développement ciblé 
1. Hayhoe 1996 (p. 77). – Dans le cadre de la réorganisation, toutes les universités missionnaires 
ont également été fermées et intégrées dans des institutions nationales.

446	
cong cao
L’Académie chinoise des sciences (ACS)
L’Académie chinoise des sciences a été créée le 1er novembre 1949, exactement 
un mois après la fondation de la République populaire de Chine, sur les bases 
de l’Academia sinica et de l’Académie de Pékin, deux académies nationales 
issues du précédent régime nationaliste. Elle faisait initialement partie du 
gouvernement, et était chargée de diriger les initiatives scientifiques de la 
nation, en plus d’effectuer elle-même de la recherche. Elle a ensuite évolué 
pour devenir une institution purement universitaire. L’ACS a joué un rôle 
majeur dans les programmes d’armement stratégique de la Chine, et un grand 
nombre de ses instituts de recherche ont été créés au service de ces programmes 
avant d’être intégrés plus tard dans la structure de recherche de la défense  
nationale.
Comme dans les autres institutions d’enseignement chinoises, les activités 
de recherche se sont interrompues à l’ACS au moment de la Révolution cultu-
relle. L’académie a dû reconstruire par la suite son expertise dans de nombreuses 
disciplines scientifiques, ainsi que ses instituts de recherche. Mais ces efforts 
ont connu une nouvelle interruption avec la réforme du système de gestion 
des sciences et technologies au milieu des années 1980, lorsque l’ACS a mis en 
œuvre la politique « Une académie, deux systèmes », en concentrant l’essentiel 
de ses efforts sur la recherche qui bénéficie directement à l’économie chinoise, 
pour susciter la création d’entreprises répondant aux besoins du marché, tout 
en poursuivant son travail de recherche fondamentale. Il s’agissait là d’une 
évolution significative par rapport à la tradition d’institution purement univer-
sitaire de l’académie.
À partir de 1998, l’Académie a lancé le programme « Innovation par la connais-
sance », avec l’objectif d’en faire le centre d’innovation par la connaissance de 
la nation dans le domaine des sciences naturelles et des hautes technologies, 
ainsi qu’une base pour une recherche scientifique pointue, de classe mondiale, 
permettant l’émergence de talents scientifiques de premier plan et le dévelop-
pement des industries high-tech (Suttmeier, Cao et Simon 2006).
L’ACS est aujourd’hui l’institution de recherche qui domine la hiérarchie 
scientifique en Chine continentale. Son siège se trouve à Pékin. Elle comprend 
12 branches et 100 instituts de recherche où travaillent plus de 50 000 chercheurs 
répartis dans le pays. Elle assume également un rôle de direction académique 
dans la définition et la mise en œuvre de la politique scientifique, ainsi que dans 
la conduite des activités savantes pour la nation entière. Pour cela, des sections 
académiques (mathématiques et physique, chimie, sciences de la vie et médecine, 
sciences de la terre, sciences informatiques et sciences technologiques) ont été 
créées, regroupant des chercheurs de l’ACS ainsi que d’éminents scientifiques 
provenant de tout le pays. Prises ensemble, ces sections, même si elles sont 
affiliées à l’ACS d’un point de vue institutionnel, ressemblent désormais à une 
société honorifique, dont les membres sont issus de l’élite scientifique, à l’image 
de ceux de la Royal Society britannique ou de l’Académie nationale des sciences 
des États-Unis (Cao 2004).

	
la chine puissance technoscientifique	
447
dans des disciplines importantes. Il inclut notamment 57 programmes 
articulés autour de 12 priorités, dont 5 sont considérées comme urgentes 
(énergie atomique, électronique, moteurs à réaction, automatisation et 
extraction des minerais rares). Clairement, le principe de « dévelop-
pement disciplinaire par missions » du plan sur douze ans a pour objectif 
politique principal le développement de programmes d’armement 
stratégique, rendu possible grâce à l’expertise dans les disciplines  
académiques.
Pour mettre en œuvre le plan sur douze ans, le Parti communiste chinois 
(PCC) adopte une approche guerrière. Il utilise sa puissance politique 
pour mobiliser les institutions et les scientifiques, qu’il force à coopérer. 
En conséquence, le plan est réalisé en sept ans avec l’explosion de la 
première bombe atomique chinoise le 16 octobre 1964. Cet événement, 
tout comme le succès du programme d’armement stratégique de la 
Chine, constitue une réalisation remarquable au regard des difficultés  
et des retards pris dans la réalisation de certains programmes à cause 
de la campagne antidroitiste, du chaos provoqué par le Grand Bond en 
avant et du retrait de l’aide soviétique à la fin des années 1950 et au début 
des années 1960. Le plan sur douze ans devient alors le modèle à suivre 
pour la planification ultérieure, et est invoqué lors de l’élaboration du 
dernier Plan pour le développement des sciences et des technologies à 
moyen et long terme (2006-2020).
L’ingérence politique
Politique et science s’entremêlent en Chine comme ailleurs. Au cours 
de ses dix-sept premières années, le pays est le théâtre d’une succession 
de campagnes politiques, parmi lesquelles la campagne antidroitiste et 
la Révolution culturelle sont les plus préjudiciables pour la communauté 
scientifique.
Au milieu des années 1950, pour rallier les intellectuels et les mobiliser 
dans le soutien au régime, et pour les faire contribuer à la construction 
du socialisme et au plan sur douze ans, le PCC s’efforce d’améliorer signi-
ficativement les moyens de la recherche. Il admet également certains 
grands intellectuels influents dans ses rangs, devenant pour un temps plus 
réceptif à l’idée de liberté intellectuelle. Dans le même temps, à diverses 
occasions, particulièrement par la voix de son leader Mao Tsé-toung, le 
Parti vante une politique dans laquelle « les fleurs s’épanouissent et les 
écoles rivalisent », pour impliquer davantage les scientifiques, les artistes 
et autres intellectuels afin qu’émerge, directement ou indirectement, un 

448	
cong cao
flot d’idées nouvelles 1. La direction du Parti lance même une campagne 
de rectification, appelant les intellectuels à l’aider à éliminer le bureau-
cratisme, le factionnalisme et le subjectivisme 2.
Au début, les intellectuels chinois hésitent, craignant le piège politique. 
Mais ils ne peuvent faire autrement que de répondre aux demandes 
répétées du Parti. Ils expriment alors des revendications concernant leur 
mauvais environnement de travail, et pour plus de démocratie dans les 
instances dirigeantes de la communauté intellectuelle. Craignant que  
le développement de la contestation ne remette en cause son autorité, le  
Parti réplique de manière féroce en accusant de « droitisme » environ 
500 000 intellectuels, dont des étudiants d’université, qui furent humiliés, 
victimes de purges, violemment attaqués, privés de leur emploi et envoyés 
dans les campagnes ou les zones frontalières dans le cadre de la « réforme 
par le travail ».
Quatre années de campagne antidroitiste aboutissent non seulement 
à une nouvelle extension du contrôle exercé par le pouvoir commu-
niste, mais aussi à des tensions entre le Parti et les intellectuels. Même 
ceux qui sont considérés comme des scientifiques non « droitistes » 
sont critiqués pour avoir poursuivi des sujets de recherche étroits et 
spécialisés, parce qu’ils sont trop préoccupés par les théories et les  
publications, et parce qu’ils travaillent surtout sur des questions issues 
de la science internationale plutôt que sur des problèmes propres à la 
Chine. En plus d’attaquer les intellectuels, le Parti prend également des 
initiatives incitant les travailleurs et les paysans à occuper les univer-
sités et les instituts de recherche et forçant les experts à réorienter leur 
recherche vers la poursuite d’objectifs politiques ou pratiques.
La campagne antidroitiste crée également une atmosphère politique qui 
favorise les campagnes massives de mesures « gauchistes » ou radicales. La 
plus connue est le Grand Bond en avant qui, au nom de la modernisation 
de la production agricole et industrielle chinoise, est particulièrement 
dommageable pour l’économie nationale. Ces campagnes aboutissent 
in fine à la Révolution culturelle en 1966.
La Révolution culturelle : des dégâts définitifs
La Révolution culturelle est déclenchée par Mao Tsé-toung dans le but 
de régagner le pouvoir qu’il estime avoir perdu au détriment de rivaux au 
1. Bowie et Fairbank 1962 (p. 6).
2. Yao et al. 1994 (p. 86).

	
la chine puissance technoscientifique	
449
sein du Parti – désignés comme les « éléments au pouvoir qui suivent la 
voie capitaliste ». Elle s’attaque également aux « autorités académiques 
bourgeoises et réactionnaires », à savoir les intellectuels haut placés 
accusés d’avoir encouragé les partisans de la voie capitaliste. Les événe-
ments liés à la Révolution culturelle traumatisent profondément la Chine, 
mais laisse aussi des traces sur le système de recherche et d’éducation.
De nombreux intellectuels sont critiqués par voie d’affiches en gros 
caractères placardées sur les murs, humiliés lors de rassemblements 
publics et soumis à des interrogatoires par des militants radicaux. Les 
professeurs d’université sont accusés d’« empoisonner les étudiants » 
avec leur enseignement. Ceux qui étudient à l’étranger sont considérés 
comme des « espions » américains ou soviétiques. Leurs maisons sont 
fouillées et leurs propriétés confisquées. Ils sont victimes d’exactions et 
torturés physiquement et psychologiquement. Un grand nombre d’entre 
eux sont interdits d’enseignement et de recherche et certains perdent 
même la vie du fait des persécutions subies. Cela donne aussi un sévère 
coup d’arrêt aux activités de recherche, y compris dans les programmes 
de développement d’armes stratégiques.
Après avoir été critiqués et dénoncés en public, un grand nombre d’intel-
lectuels sont envoyés à la campagne ou dans des usines où ils endurent 
des souffrances physiques et mentales. Pendant ce temps, les militants 
radicaux placent des travailleurs non qualifiés, des paysans et des soldats 
dans les institutions d’éducation, promouvant la participation des masses 
pour contrer l’orientation élitiste de la science chinoise.
En s’attaquant à l’intellectualisme, la Révolution culturelle transforme 
également le système éducatif chinois. Dénoncée comme un système 
favorisant l’émergence des idées révisionnistes, l’éducation supérieure 
« formelle » est supprimée en 1966. Les étudiants de l’enseignement 
supérieur sont contraints d’arrêter leurs études pour être rééduqués 
dans des usines, à la campagne ou dans des camps militaires. Lorsque 
les universités rouvrent leurs portes en 1973 – les études supérieures 
ne reprennent cependant pas avant 1978 –, les politiques radicales sont 
encore en vigueur. Le cursus d’études standard est raccourci à trois ans à 
l’Université, venant en sus d’une réduction de six à quatre ans de la durée 
de la scolarité au collège et au lycée. Les étudiants admis à l’Université 
sont des travailleurs, des paysans et des soldats qui ont une expérience 
pratique. La plupart de ces « étudiants travailleurs-paysans-soldats » 
arrivent dans les universités mal préparés, contraints de consacrer un 
temps considérable à l’acquisition de savoirs élémentaires.
D’autre part, les enseignements sont devenus moins professionnels et 
plus orientés vers la production. Les professeurs et les enseignants font 

450	
cong cao
fréquemment de longues visites dans les usines et les communes pour 
étudier et résoudre les problèmes pratiques, et la recherche s’oriente vers 
l’application. Les conditions et les dispositifs ainsi créés ne produisent 
pas une atmosphère propice à la formation de la génération suivante. 
Selon les estimations, la Chine a perdu au moins 1 million d’étudiants de 
premier cycle et 100 000 étudiants de second cycle pendant la Révolution 
culturelle 1, ce qui se ressent encore aujourd’hui, quarante ans plus tard, 
en termes de capital humain.
Réforme et ouverture : la renaissance  
d’une puissance scientifique et technique
La Chine entre dans une nouvelle ère après la Révolution culturelle.  
Sur le front des sciences et des technologies, le gouvernement fait immé­
diatement d’énormes efforts pour restaurer le système de recherche et 
d’éducation afin de combler le retard par rapport aux pays avancés où 
s’annonce une nouvelle révolution technologique. Les universités recom-
mencent à admettre les élèves issus du lycée. En réhabilitant les membres 
de la communauté intellectuelle et en les rappelant aux postes qu’ils 
occupaient auparavant, le pouvoir communiste essaie de les rallier à 
l’élan modernisateur du pays. Sensibles à leur affirmation professionnelle 
comme aux mots d’ordre patriotiques de l’essor national, les scientifiques 
répondent positivement à l’appel.
Les sciences et l’éducation montent alors en tête des priorités du pouvoir 
politique. En 1995, le « redressement de la nation par les sciences, les 
technologies et l’éducation » devient la nouvelle stratégie de dévelop-
pement, suivie d’une accentuation du travail autour de la question des 
talents, avec une politique de « renforcement de la nation en cultivant 
les talents ». Les scientifiques se voient accorder davantage de liberté et 
d’autonomie dans leur domaine de compétence professionnelle – pour 
autant qu’ils consacrent leur énergie à leur travail et non à la contestation 
politique du régime.
Ces initiatives aboutissent progressivement, au début 2006, à l’élaboration 
d’un nouveau Plan pour le développement des sciences et des techno-
logies à moyen et long terme (2006-2020) (MLP) 2. En garantissant une 
croissance soutenue des ressources financières et humaines, l’objectif est 
de faire de la Chine une nation tournée vers l’innovation à l’horizon 2020.  
1. Qu 1993 (p. 648).
2. Cao, Simon et Suttmeier 2006 et 2009.

	
la chine puissance technoscientifique	
451
Le MLP a comme principal objectif le développement de la capacité  
d’innovation nationale au service d’une économie de haute croissance 
et d’une société riche et harmonieuse. Les sciences sont appelées à 
doper l’innovation et mobilisées pour rendre durable un modèle de 
développement économique actuellement non soutenable du fait d’un 
surinvestissement, d’une dépendance excessive vis-à-vis des exporta-
tions, d’une utilisation inefficace des ressources et d’une destruction de 
l’environnement.
Le gouvernement restructure sa politique technoscientifique au milieu 
des années 1980 pour la rendre plus réactive aux besoins de l’économie. 
Depuis 1995, les dépenses de la Chine en recherche et développement 
(R & D) augmentent à un rythme presque deux fois supérieur à celui de la 
croissance économique globale. En 2011, la Chine a dépensé 861 milliards 
de yuans (137 milliards de dollars) en R & D, soit 1,83 % de son produit 
intérieur brut (PIB), ce qui place la Chine en deuxième position dans le 
monde après les États-Unis. Les établissements d’enseignement supérieur 
chinois forment un nombre croissant de diplômés de bon niveau en 
sciences et technologies. En 2011, 430 000 étudiants ont obtenu un 
diplôme de master ou un doctorat, en plus des 6,82 millions d’étudiants 
de premier cycle, ce qui constitue la première force de travail diplômée  
du monde.
La communauté scientifique et technologique chinoise a également 
connu une croissance régulière. En prenant comme base le nombre 
d’articles inclus dans le Science Citation Index (SCI), la base de données 
bibliométrique publiée par Thomson Reuters en 2011, la Chine se situe au 
troisième rang dans le monde, derrière les États-Unis et le Royaume-Uni. 
En 2007, la Chine passe devant les États-Unis en tant que pays le plus 
contributeur en articles au répertoire de l’Engineering Index (EI). Même 
si le pays a encore du retard par rapport aux leaders mondiaux dans de 
nombreux domaines scientifiques et technologiques, des réalisations 
importantes sont enregistrées dans un certain nombre de domaines 
émergents tels que la génomique et les nanotechnologies 1.
La Chine connaît une croissance importante du nombre de brevets 
enregistrés dans le cadre du Traité international de coopération en 
matière de brevets (PCT). En 2011, les inventeurs chinois ont déposé 
16 000 brevets, ce qui place le pays au quatrième rang dans le monde, après 
les États-Unis, le Japon et l’Allemagne. Cette année-là, ZTE Communica-
tions, géant chinois de matériel de télécommunications, prend la première 
place mondiale des dépôts de brevet des entreprises avec 2 826 brevets 
1. Voir par exemple Zhou et Leydesdorff 2006.

452	
cong cao
Plan pour le développement des sciences 
En janvier 2006, le Conseil des affaires de l’État de Chine a adopté un Plan 
pour le développement des sciences et des technologies (2006-2020) (MLP) sur 
quinze ans. Définissant divers objectifs chiffrés et appelant la Chine à devenir 
une « nation tournée vers l’innovation » d’ici 2020, et un leader mondial en 
sciences et technologies à l’horizon 2050, le plan engage la Chine sur la voie du 
développement d’une capacité d’innovation indigène, pour la faire progresser 
radicalement vers des positions de leader dans les nouvelles industries basées sur 
les sciences d’ici à la fin de la période du plan (Cao, Suttmeier et Simon 2006 et  
2009).
D’un point de vue stratégique, le MLP peut être considéré comme répondant à 
quatre problèmes essentiels liés au développement scientifique et technologique 
en particulier en Chine (Cao, Suttmeier et Simon 2006 et 2009). En premier lieu, 
avec sa stratégie de « marché pour la technologie » visant à persuader les entreprises 
multinationales à investir et transférer des technologies en échange d’opportu-
nités commerciales, la Chine est devenue l’usine du monde, ce qui a entraîné 
l’impressionnante croissance de ses exportations en produits de haute technologie. 
Cependant, cette politique a peut-être atteint ses limites avec l’augmentation des 
coûts en Chine et la mise en place de mesures protectionnistes dans les pays qui 
ont perdu des emplois suite aux délocalisations.
Deuxièmement, il se peut que les régimes internationaux des droits de propriété 
intellectuelle (DPI) et les standards techniques globaux ne servent pas les intérêts 
de la Chine, de sorte qu’elle pourrait se retrouver sous une pression croissante et 
contraignante de la part des leaders mondiaux dans le domaine de l’innovation, 
qui contrôlent l’architecture des systèmes technologiques mondiaux. Ainsi, l’éco-
nomie industrielle chinoise du xxie siècle devrait développer ses propres standards, 
et devrait également produire et mettre en œuvre ses propres DPI.
Troisièmement, les capacités technologiques chinoises ne permettent pas de 
répondre aux besoins sociaux du pays dans d’importants domaines comme l’énergie, 
l’eau et l’utilisation des ressources de manière générale, ainsi que la protection de 
l’environnement et la santé publique.
Enfin, les défis technologiques liés aux besoins de défense du pays sont au cœur 
des objectifs formulés dans le MLP. Malgré les réalisations de la Chine dans le 
internationaux, tandis qu’une autre entreprise du secteur, Huawei, se 
classe troisième 1.
L’économie chinoise a beaucoup profité des transferts de technologies 
et des investissements de l’étranger. Récemment, un grand nombre 
d’entreprises parmi les plus innovantes dans le monde, dont des sociétés 
figurant dans le classement Fortune 500, installent des centres de R & D en 
1. World Intellectual Property Organization 2012.

	
la chine puissance technoscientifique	
453
domaine de l’armement stratégique, ses capacités globales d’innovation techno-
logique en matière de défense n’ont jusqu’à récemment pas été extraordinaires. Le 
pays manque encore d’une culture technologique civile forte, qui pourrait servir 
de base pour répondre aux besoins importants du domaine militaire.
Le MLP commence par définir un ensemble de lignes directrices et de principes 
généraux issus des objectifs qui sont de faire des sciences et des technologies les 
moteurs du développement économique futur, permettant à la Chine de progresser 
de manière radicale vers des positions de leadership dans les industries émergentes 
basées sur la science, et de développer ses capacités en matière d’innovation indigène.
La deuxième partie du plan identifie des domaines et des programmes priori-
taires, dont 11 grands « domaines clés » relatifs aux besoins nationaux et 8 domaines 
de « technologie frontière ». Au sein de ces domaines, le plan identifie une série 
de projets prioritaires, et en particulier 16 « mégaprogrammes » d’ingénierie et 
4 « mégaprogrammes » scientifiques (deux ont été ajoutés par la suite).
La troisième section porte sur les réformes en cours des systèmes de recherche et 
d’innovation et l’avancement du développement du système d’innovation national. 
Elle souligne des objectifs importants relatifs à la réforme continue d’un grand 
nombre d’instituts de recherche gouvernementaux, au changement dans la politique 
de gestion des sciences et technologies, et à la nécessité d’encourager les entre-
prises chinoises à assumer un rôle de leader dans le système d’innovation du pays, 
avec des politiques de promotion de la recherche industrielle dans les entreprises 
et le soutien aux petites et moyennes entreprises.
Les dernières sections du plan, ainsi qu’un document annexe, portent sur le 
cadre politique de la mise en œuvre du plan, incluant des incitations fiscales, des 
politiques de zones dédiées aux industries de haute technologie, l’assimilation 
des technologies étrangères et le renforcement de la protection des DPI pour les 
innovateurs chinois. Les politiques menées dans le domaine du développement 
des ressources humaines pour les sciences et les technologies incluent des mesures 
pour faire émerger des experts expérimentés de niveau mondial, l’élargissement du 
rôle des scientifiques et des ingénieurs dans l’industrie, le recrutement de talents à 
l’étranger et la réforme du système éducatif pour répondre aux objectifs en matière 
de créativité et d’innovation.
et des technologies à moyen et long terme (2006-2020)
Chine pour développer de nouveaux produits et services pour le marché 
mondial, ainsi que pour le marché intérieur chinois.
Pour résumer, la Chine, considérée un temps comme l’un des pays en 
développement les plus en retard, est devenue l’une des puissances écono-
miques et technologiques les plus solides du monde. Le scénario envisagé 
il y a quatre-vingt-dix ans par le philosophe britannique Bertrand Russell 
semble être en passe de se réaliser :

454	
cong cao
[…] si les Chinois pouvaient se doter d’un gouvernement stable et de finance-
ments suffisants, ils pourraient commencer à produire des travaux remarquables 
dans le domaine des sciences au cours des trente prochaines années. Il est 
fort probable qu’ils nous dépasseront, parce qu’ils arrivent avec une grande 
fraîcheur et toute la dynamique liée à une renaissance 1.
Les défis à relever restent nombreux
Malgré les progrès remarquables mentionnés ci-dessus, la Chine doit 
encore relever de nombreux défis difficiles pour devenir la « nation 
tournée vers l’innovation » qu’elle ambitionne d’être 2.
Premièrement, la recherche privée reste faible ou routinière. Si les statis-
tiques globales indiquent que trois quarts des dépenses en R & D en Chine 
sont maintenant à mettre au compte d’entreprises 3, ces dernières n’ont 
en réalité que peu contribué financièrement à des initiatives de R & D 
innovantes. Les entreprises comme Huawei et ZTE constituent des  
exceptions et non la norme. En outre, l’innovation ne se résume pas à 
la R & D. Elle nécessite le développement d’une culture de la créativité 
qui récompense les idées nouvelles et originales, ainsi que l’esprit d’entre-
prise. À la recherche d’intérêts rapides et à court terme, les entreprises 
chinoises ont tendance à importer des technologies et des équipements 
de l’étranger pour mettre à niveau leurs technologies de production.  
C’est alors l’équipement qui prime par rapport aux logiciels, sous forme de 
brevets, de savoir-faire, de modèles, etc. De manière générale, les entre-
prises chinoises consacrent plus d’argent à l’importation de technologies 
qu’à la R & D. Or l’importation d’un équipement limite les ressources 
financières qui sont disponibles pour l’absorption, l’assimilation et l’inno-
vation, ce qui aboutit à un cercle vicieux « d’importation, de retard pris, 
de réimportation, qui provoque un nouveau retard ».
Les entreprises ne montrent par ailleurs pas un intérêt soutenu pour les 
institutions éducatives du pays en ce qui concerne les efforts de R & D. La 
réforme du système de recherche et d’enseignement supérieur engagée 
depuis le milieu des années 1980 a suscité un certain engouement chez 
les chercheurs de ces institutions mais la demande entrepreneuriale 
d’innovations technoscientifiques ne suit pas. Les entreprises se sont 
montrées réticentes à acquérir des technologies d’origine chinoise. En 
1. Russell 1922 (p. 193).
2. Cao, Simon et Suttmeier 2009.
3. Ces chiffres peuvent aussi refléter une inflation des déclarations des entreprises en matière 
de dépenses de R & D.

	
la chine puissance technoscientifique	
455
d’autres termes, les problèmes profondément enracinés de la séparation 
entre les secteurs de la recherche et de l’économie, et entre le monde de 
l’entreprise et le système éducatif, n’ont pas été résolus.
Deuxièmement, dans de nombreux cas, on peut se demander si les 
budgets croissants de R & D tant au niveau national qu’aux niveaux 
territoriaux sont dépensés à bon escient. On sait qu’une partie signi-
ficative de la recherche conduite en Chine, même dans les grands 
programmes nationaux, est dérivée de ce qui a déjà été fait autre part, 
ce qui constitue un gaspillage de ressources certes croissantes, mais qui 
restent néanmoins limitées. Les détournements de fonds destinés à la 
recherche sont également monnaie courante. Plus grave, la corruption 
dans le domaine de la recherche scientifique non seulement a aspiré 
une partie non marginale des fonds alloués, mais elle a terni l’image de 
la recherche avec la révélation des cas de fraude et de détournement 
d’argent dans les médias et sur Internet.
Troisièmement, même si la législation et les réglementations relatives 
aux droits de la propriété intellectuelle (DPI) en Chine ont pris la même 
inflexion que le mouvement international d’affirmation et de durcissement 
de ces droits, leur mise en œuvre et leur application restent probléma-
tiques. En effet, la décentralisation de la Chine a rendu l’application des 
DPI au niveau local plus difficile, voire impossible 1. La popularité de la 
culture shanzhai de l’imitation, même si elle résulte d’une manière de 
penser innovante et originale, peut avoir un effet dissuasif sur les entre-
prises leaders des hautes technologies dans des domaines où des DPI durs 
sont le moteur du business model. La faiblesse de la Chine en matière  
de protection des DPI constitue aussi une préoccupation majeure pour les 
entreprises étrangères. Prônant le développement de la capacité d’inno-
vation, le MLP appelle à un renforcement de la protection des DPI, non 
seulement pour se mettre en conformité avec le régime international 
actuel des DPI, mais aussi pour préparer les entreprises chinoises à 
générer leurs propres DPI pour s’affirmer mondialement.
Quatrièmement, la Chine est confrontée à un véritable problème 
de talents 2. Les effets de la Révolution culturelle se font encore sentir 
sur le nombre et la qualité de la force de travail scientifique. Les initia-
tives favorisant les études à l’étranger, qui ont accompagné la politique 
1. Même si un certain nombre d’entreprises chinoises ont été récemment poursuivies pour 
infraction aux droits de la propriété intellectuelle, cela a été essentiellement pour des affaires 
relativement simples de violation du copyright ou de fabrication de produits shanzhai – avec 
certaines exceptions comme Huawei, qui a été accusée de violation de brevet par Cisco Systems, 
le plus grand fabricant mondial d’équipements de réseau et de communication.
2. Simon et Cao 2009.

456	
cong cao
d’ouverture de la Chine, ont par ailleurs entraîné une importante « fuite 
des cerveaux ». Même s’il est vrai qu’il continue d’y avoir des liens 
fructueux entre ceux qui sont restés à l’étranger, souvent les meilleurs 
et les plus intelligents, et la communauté scientifique et technologique 
du pays, le départ de certaines personnes parmi les plus compétentes 
et présentant le plus de potentiel a eu un impact négatif sur les efforts 
de développement des capacités scientifiques, technologiques et indus-
trielles. La majorité des diplômés chinois n’ont pas encore le niveau 
requis pour répondre à la demande croissante en compétences de l’éco-
nomie dans son ensemble. De manière encore plus visible, alors que la 
population chinoise commence à vieillir, l’évolution de la composition 
démographique de la population active, notamment en termes d’âge et 
d’expérience professionnelle, a aussi un impact sur le potentiel de progrès  
à venir.
Enfin et surtout reste la question de savoir si la Chine peut devenir 
une nation tournée vers l’innovation sans s’ouvrir à des manières de 
penser différentes, ce qui dépasse le simple cadre philosophique. Si les 
chercheurs et entrepreneurs chinois sont en apparence encouragés à 
produire des idées innovantes et originales sans craindre l’échec, il n’en 
reste pas moins, de manière tout aussi importante, que les autres ingré-
dients nécessaires à une véritable culture de l’innovation (autonomie, libre 
accès aux informations et disponibilité de ces dernières, en particulier 
les informations dissidentes, aussi bien dans le domaine scientifique que 
politique) ne sont pas suffisamment cultivés ni tolérés. Pour stimuler la 
créativité et l’innovation 1, il s’agirait alors non simplement de transformer 
l’innovation en nouvelle « religion » nationale, mais surtout de donner 
un nouveau sens au mot d’ordre « Les fleurs s’épanouissent et les écoles  
rivalisent ».
En somme, si les efforts de la Chine pour devenir une nation tournée 
vers l’innovation peuvent certainement modifier la géopolitique et le 
paysage mondiaux des technosciences, il n’en reste pas moins que le 
pays doit assez rapidement relever de nombreux défis intérieurs pour 
atteindre les objectifs qu’elle s’est fixés. En particulier, le « côté logiciel » 
de l’équation de l’innovation doit faire l’objet d’une plus grande attention 
pour qu’émerge une véritable « culture de créativité » mise en œuvre et 
alimentée dans l’ensemble du spectre des organisations et institutions 
chinoises de recherche au niveau national et local.
1. Florida 2003.

	
la chine puissance technoscientifique	
457
 
Conclusion : L’innovation « à la chinoise »
Le développement des sciences pendant l’essentiel de l’histoire de la 
République populaire de Chine se caractérise par leur politisation, le rôle 
de l’État dans la recherche, les grands projets scientifiques (big science), la 
mobilisation et la concentration des ressources et l’ingérence du pouvoir 
politique. Cette manière chinoise de produire de la science et de l’inno-
vation a abouti à des réalisations extraordinaires dans certains domaines, 
en particulier dans celui de l’ingénierie et de la technologie, qu’il s’agisse 
des programmes d’armement stratégique ou, plus récemment, des trains 
à grande vitesse.
La Chine est maintenant passée du statut d’acteur secondaire à celui 
de leader sur la scène économique et technologique mondiale. Mais 
les effets des politiques menées, notamment entre 1949 et 1978, conti-
nuent de se faire sentir dans la communauté scientifique, dont elle a sapé 
l’enthousiasme. Cet héritage a rendu d’autant plus saillante la nécessité de 
réformer le système de recherche et d’innovation en Chine. Cependant, 
de manière assez paradoxale, la Chine a raté le meilleur moment pour 
cela et la fenêtre d’opportunité est actuellement en train de se rétrécir. 
Pour bénéficier pleinement du potentiel de ses prouesses en matière 
d’innovation, la Chine doit s’attaquer aux problèmes de manque de 
coordination entre les différents acteurs au niveau macro, d’inadéquation 
des mécanismes de répartition des financements au niveau méso et 
d’évaluation des performances individuelles des scientifiques au niveau 
micro. La renaissance des sciences chinoises passe obligatoirement par 
la résolution de ces problèmes.
Traduit par Cyril Le Roy
Références bibliographiques
Bowie Robert R. et Fairbank John King (dir.), 1962, Communist China (1955-1959) : 
Policy Documents and Analysis, Cambridge (MA), Harvard University Press.
Buck Peter, 1980, American Science and Modern China (1876-1936), Cambridge, 
Cambridge University Press.
Cao Cong, 2004, China’s Scientific Elite, Londres et New York, RoutledgeCurzon.
Cao Cong, Simon Denis Fred et Suttmeier Richard P., 2009, « China’s Innovation 
Challenge », Innovation : Management, Policy and Practice, vol. 11, no 2, p. 253-259.
Cao Cong, Suttmeier Richard P. et Simon Denis Fred, 2006, « China’s 15-Year 
Science and Technology Plan », Physics Today, vol. 59, no 12, p. 38-43.
–	 2009, « Success in State Directed Innovation ? Perspectives on China’s Plan for 
the Development of Science and Technology », in Govindan Parayil et Anthony 

458	
cong cao
P.  D’Costa (dir.), The  New Asian Innovation Dynamics : China and India in 
Perspective, Londres, Palgrave Macmillan, p. 247-264.
Florida Richard, 2003, The Rise of the Creative Class : And How It’s Transforming 
Work, Leisure, Community and Everyday Life, New York, Basic Books.
Furth Charlotte, 1970, Ting Wen-chiang : Science and China’s New Culture, 
Cambridge (MA), Harvard University Press.
Gao Pingshu, 1996 [1982], « Cai Yuanpei’s Contributions to China’s Science », in Fan 
Dainian et Robert S. Cohen (dir.), Chinese Studies in the History and Philosophy 
of Science and Technology, Dordrecht, Kluwer Academic, p. 395-417.
Hayhoe Ruth, 1996, China’s Universities (1895-1995) : A Century of Cultural Conflict, 
Londres et New York, Routledge.
Hunt Michael H., 1972, « The American Remission of the Boxer Indemnity : 
A Reappraisal », The Journal of Asian Studies, vol. 31, no 3, p. 539-559.
Israel John, 1998, Lianda : A Chinese University in War and Revolution, Palo Alto 
(CA), Stanford University Press.
Leibovitz Liel et Miller Matthew, 2011, Fortunate Sons : The 120 Chinese Boys Who 
Came to America, Went to School, and Revolutionized an Ancient Civilization, New 
York et Londres, W.W. Norton.
Qu Shipei, 1993, A  Developmental History of University Education in China 
(en chinois), Shanxi Education Press.
Russell Bertrand, 1922, The Problem of China, Londres, George Allen & Unwin.
Simon Denis Fred et Cao Cong, 2009, China’s Emerging Technological Edge : Assessing 
the Role of High-End Talent, Cambridge et New York, Cambridge University Press.
Suttmeier Richard P., Cao Cong et Simon Denis Fred, 2006, « China’s Innovation 
Challenge and the Remaking of the Chinese Academy of Sciences », Innovations : 
Technology, Governance, Globalization, vol. 1, no 3, p. 78-97.
Wang Yi Chu, 1966, Chinese Intellectuals and the West (1872-1949), Chapel Hill 
(NC), University of North Carolina Press.
Wang Zuoyue, 2002, « Saving China through Science : The Science Society of China, 
Scientific Nationalism, and Civil Society in Republican China », Osiris, vol.  17, 
p. 291-322.
World Intellectual Property Organization (WIPO), 2012, « International 
Patent Filings Set New Record in 2011 », Genève, WIPO, < http://www.wipo.int/
pressroom/en/articles/2012/article_0001.html > (consulté le 22 juillet 2012).
Yao Shuping, Wei Luo, Li Peishan et Zhang Wei, 1994, « A Developmental History 
of the Chinese Academy of Sciences » (en chinois), in Qian Linzhao et Gu Yu (dir.), 
The Chinese Academy of Sciences (en chinois), 3 vol., Beijing, Contemporary China 
Press, vol. 1, p. 1-230.
Ye Weili, 2001, Seeking Modernity in China’s Name : Chinese Students in the United 
States (1900-1927), Palo Alto (CA), Stanford University Press.
Zhou Ping et Leydesdorff Loet, 2006, « The Emergence of China as a Leading 
Nation in Science », Research Policy, vol. 35, no 1, p. 83-104.
Zhu Weizheng, 2008, Financial Times (en  chinois), <  http://www.ftchinese.com/
story/001016455 > (consulté le 4 juin 2012).

 
Conclusions générales  
des trois tomes


Savoirs et sciences  
de la Renaissance à nos jours. 
Une lecture de longue durée
D o m i n i q u e  P e s t r e
Dire les régimes de savoir et de science au fil du temps
Les histoires rapportées dans ces volumes oscillent entre histoires de 
savoirs et histoires de sciences – c’est inévitable 1. De la Renaissance à nos 
jours, ces termes n’ont pas de stabilité sémantique ; « la science » (dont 
nous serions les porteurs ou les héritiers) ne sort pas tout armée d’une 
révolution conceptuelle qui aurait eu lieu au xviie siècle en Europe – pace 
Koyré 2 ; elle n’est pas non plus une « chose » qui existerait indépendamment 
de nos catégorisations ; et les sciences n’englobent pas, aujourd’hui, 
l’ensemble des savoirs pertinents, utiles ou intéressants.
Un « ancien régime des savoirs » – de la Renaissance aux Lumières
Tenter d’être fidèle à l’histoire conduit à parler d’abord d’un « ancien 
régime des savoirs » – parler simplement de science serait risquer l’ana-
chronisme et manquer la spécificité de ce qui a cours. Le cœur des savoirs 
est alors fait des savoirs « antiquaires », pas des sciences de laboratoire 3. Il 
n’est pas de « professionnels » à cette époque et collectionner est indisso-
ciablement pratique de savoir et jeu esthétique, réflexion et monstration 
d’un statut. Produire des livres est (aussi) paraphrase, copie, adaptation, 
commentaire sans fin des textes des autres. La propriété intellectuelle, 
comme la notion d’auteur, n’ont pas la forme que nous leur connaissons 
aujourd’hui. « Bibliothèque » ne signifie pas collection de livres imprimés 
1. Lorsqu’il n’indique pas de référence, ce texte renvoie aux divers chapitres de cette Histoire. 
Je tiens à remercier Pierre-Benoît Joly, Jacques Revel et Simon Schaffer pour leurs remarques 
et commentaires sur la première version de ce texte. Sans eux ce texte eût été bien plus 
sommaire encore.
2. Koyré 1962.
3. Van Damme 2014.

462	
dominique pestre
mais mélange intime avec des manuscrits et des opus copiés. Et la valeur 
intellectuelle et marchande de ces collections est fonction des interven-
tions scripturaires dans les livres, et du choix ainsi assemblé et manifesté 1.
Les cours princières sont des lieux majeurs de savoir – même s’il est des 
dénonciations par maints savants de ces civilités trop faciles et séduisantes. 
Elles sont adossées à des académies locales, nombreuses au xviie siècle, à 
des collections d’instruments ou de machines, à des cabinets « de curio-
sités ». L’ordre aristocratique fixe les modes de la conversation, la vie 
des salons comme la circulation des livres. Mais les cours sont concur-
rencées par les réseaux épistolaires (la république des lettres) qui, comme 
les grandes académies (la Royal Society de Londres ou l’Académie royale  
à Paris), redonnent, dans le second xviie siècle, un minimum d’entre 
soi et d’autonomie aux savants. Les artisans, constructeurs d’instru-
ments, démonstrateurs, promoteurs de nouveautés – ou élites libertines 
ou philosophiques –, occupent, pour leur part, un espace public friand 
d’objets, de théories, de spectacles « scientifiques ». Pamphlets, échoppes, 
cafés, journaux et clubs francs-maçons fourmillent de « curiosités » et 
de nouveautés techniques. À la fin du xviie siècle, Amsterdam, plaque 
tournante du commerce mondial, est un entrepôt de savoirs – de cartes, 
de collections de naturalia, de jardins d’apothicaire.
Face à une historiographie qui a souvent associé « révolution scienti-
fique » et Europe du Nord, voire protestantisme, il convient de rappeler 
la vivacité des terres ibériques, de Mexico, de Lisbonne – et de Rome.  
La puissance de l’Église donne à la ville pontificale des collections uniques ; 
les richesses qui y affluent font de la ville un chantier permanent attirant 
architectes et ingenio. La taille des congrégations religieuses, qui couvrent 
souvent la planète, fait de leurs centres romains des lieux exceptionnels 
de rassemblement de savoirs, de confrontation de connaissances anthro-
pologiques, linguistiques, cartographiques, humaines.
Mais il est des évolutions, faciles à identifier. L’Université médiévale 
perd de sa superbe, ses savoirs et modes de travail sont contestés – elle est 
contournée. Ainsi des mathématiques mixtes, favorisées par les capitaines 
d’armées ou de navires, les princes et hommes d’État, et qui se donnent, 
au xvie siècle, comme essentielles pour le développement des machines 
de guerre, la vie industrieuse et le théâtre. La géodésie, la mesure des 
édifices et des profondeurs marines, la détermination de l’heure, l’éva-
luation des poids, la levée des plans, le pointage des pièces d’artillerie, 
la mesure des parcours en mer sont, pour Alberti, ce qui fait leur force.
Ces manières pratiques de faire fascinent les philosophes ; ils se les 
1. Delon 2007.

	
conclusions générales des trois tomes	
463
approprient et « réinventent » la philosophie naturelle. Vers le milieu 
du xviie siècle, cette dernière advient en philosophie expérimentale 
réglée par des instruments et des artifices déployés par des démons-
trateurs-constructeurs. Elle vise la production de « faits », d’atomes 
élémentaires de savoir. Collectivement, philosophes et mathématiciens 
(le grand Newton lui-même) accompagnent les entrepreneurs et projec-
teurs d’affaires ; et ils apportent leurs crédits aux Compagnies des Indes 
qui se constituent aux Pays-Bas, en France, en Angleterre.
C’est toutefois l’histoire naturelle qui est peut-être le savoir le plus 
prisé de ce moment. Elle se pratique sur le terrain, dans les cabinets 
et maisons (celle d’Aldovrandi à Bologne), à travers les échanges de 
spécimens, dans l’invention de systèmes de classement. Elle est liée à 
la médecine, à la pharmacopée, à l’acclimatation des plantes, à l’agri-
culture, au commerce – et dit offrir, au xviiie siècle, des solutions au 
problème de la dégénérescence des peuples 1. Le voyage d’exploration, la 
conquête, le transfert des plantes – et donc l’espionnage et les expéditions 
pirates – sont son quotidien. Confrontés au flot de découvertes venues 
d’au-delà des mers, les Modernes passent progressivement moins par les 
Anciens pour lire la nature. En fin de période, le recours au microscope 
et au scalpel conduit aussi à la physiologie, l’étude de la nutrition, de la 
reproduction, de la circulation de la sève.
Naissance des « sciences » et d’un premier complexe  
« État-nation-savants » – du dernier tiers du xviiie siècle  
au second tiers du xixe siècle
Au long de cette centaine d’années, observer et comparer, puis décom-
poser analytiquement en éléments simples, est commun à maints 
mouvements de savoir – du moins à Paris où l’analyse mathématique 
est reine et où la chimie, après Lavoisier, devient la science de la combi-
naison des éléments en proportions données. Mais les tissus deviennent 
eux aussi les éléments de l’anatomie générale (chez Bichat) et les strates 
géologiques le fondement d’une histoire de la Terre. Par ailleurs, des faire 
s’autonomisent (pratique des essais grandeur nature dans la construction 
navale en Angleterre ou le génie civil en France, avec des ingénieurs 
comme Coulomb par exemple) et une pratique « physicienne » nouvelle 
se solidifie à Arcueil, autour de Berthollet et Laplace. Celle-ci est une 
généralisation de la forme mathématique newtonienne aux problèmes 
sublunaires (pensez à l’électrostatique du même Coulomb) et elle est 
1. Spary 2005 [2000].

464	
dominique pestre
couplée à des techniques nouvelles d’observation et à une exigence  
de précision. Même s’il n’est pas une causalité simple à ces évolutions, 
la création des écoles des Ponts et Chaussées (en 1747) et du génie de 
Mézières (en 1748) ne peut être anecdotique. Y domine en effet un goût 
pour les choses pratiques tout autant que pour la spéculation théorique, 
l’instrumentation et les mesures très précises. Ainsi l’enseignement à 
Mézières à la fin des années 1770 repose-t-il sur la géométrie descriptive, 
la théorie des machines et les sciences physiques – et les expériences sur 
la composition de l’eau sont réalisées au laboratoire de chimie en 1783 1.
Deux aspects caractérisent ces sciences. L’un est d’ordre épistémolo-
gique : il s’agit de techniques de collecte, de classement, de comparaison 
et d’analyse. Ce qui est en cause est, en un mot, un ordre intellectuel. 
Mais le second aspect est social et moral : loin des formes esthétiques des 
salons et du jeu aristocratique, s’énonce un autre éthos, celui du travail, un 
détachement des normes de la « curiosité » et de la bienséance mondaine 
ou bourgeoise en faveur de la précision et du devoir. Les professeurs des 
hôpitaux parisiens, ceux qui enseignent au Muséum, à l’École polytech-
nique, ou encore les ingénieurs des corps du premier xixe siècle, sont 
« protégés » par les institutions d’État qui les emploient. Ils y vivent dans 
des espaces séparés du commun, exclusivement masculins (ce qui est 
bien sûr différent des salons et n’est pas anodin), et ils bénéficient d’une 
certaine latitude pour définir les règles de leur métier pourvu qu’ils restent 
efficaces, utiles au pays. Se déclarant comme ceux qui font et savent 
(des professionnels), ils s’inventent comme autonomes et récusent, plus 
nettement qu’avant, la pertinence des formes de savoir qui prévalent  
dans l’espace public, chez les « amateurs » ou les « charlatans » (c’est le sens 
de l’action entreprise par l’Académie contre Messmer et son magnétisme).
Ils promeuvent aussi de nouveaux discours sur « les sciences » – pensez 
à l’apparition du vocable de scientist dans le monde anglophone dans les 
premières décennies du xixe siècle – ou sur la méthode et l’ordre des savoirs 
scientifiques – pensez cette fois à Auguste Comte. Ils tendent maintenant 
à se situer dans des « disciplines » : la physique et ses sous-parties : la 
mécanique, l’optique, l’électricité et le magnétisme, la thermodyna-
mique ; mais aussi la chimie, l’anatomie comparée, la géologie… Tout 
aussi symptomatique et essentiel : un esprit quantificateur se répand 
dans tous les milieux, et l’avalanche des nombres commence. Elle affecte 
la production des données (pour la mesure des longitudes, des données 
météorologiques ou sociales), leur distribution et les espoirs de maîtrise 
qu’on place en elles – et ce mouvement n’est évidemment pas indépendant 
1. Belhoste, Picon et Sakarovitch 1990.

	
conclusions générales des trois tomes	
465
des idéaux de toute-puissance des individus qui prennent alors forme 
en termes politiques dans ces années. Elle appelle des tentatives pour 
définir des métriques standardisées et normer les données, et les États 
s’en emparent, au nom d’une meilleure gestion des populations et des 
ressources.
Comprendre le succès de ces pratiques demande d’élargir la focale,  
d’analyser le nouvel environnement auquel ces scientists participent – qui 
est d’abord celui de l’ordre libéral, au sens politique comme au sens écono-
mique 1. Ce qui le définit est l’existence d’un assemblage nouveau d’élites 
industrielles, scientifiques, techniques et étatiques, d’une oligarchie qui 
donne la priorité à des formes nationales de développement. Les nouveaux 
« scientifiques entrepreneurs » y jouent un grand rôle, par leur science, 
leurs offres techniques, leur rôle d’entrepreneurs ; mais aussi par leur place 
dans la vie politique, ce qui est plus net en France qu’ailleurs ; et par le 
fait qu’ils peuvent jouer la plupart des rôles à la fois, à l’image du baron 
Chaptal sous le Consulat qui est à la fois médecin, chimiste, industriel  
et ministre de l’Intérieur. Cette oligarchie privilégie les marchés et l’ordre 
administratif – qui est l’ordre des scientifiques et industriels aidés des 
grands commis de l’État, qui est l’ordre des comités d’experts chargés de 
définir règles et normes 2.
Les nouvelles règles de production, techniquement plus gagées, créent 
le besoin d’un environnement législatif et financier plus stable – mais 
jamais pleinement satisfait. Du fait de l’ampleur des investissements 
que demandent les nouvelles manières de produire, les entrepreneurs 
exigent visibilité à long terme et garantie contre les aléas 3. D’où l’impor-
tance d’un droit positif qu’il faut écrire. D’où la définition, en France et 
aux États-Unis, au début des années 1790, d’un droit des brevets très 
neuf, individualisant, et qui fait du créateur individuel le propriétaire 
plein et entier de ses idées données comme sans racine ni antécédent 4. 
D’où la création de zones de marché plus homogènes et isotropes (l’espace 
national construit par la loi et l’administration), la définition d’un nouvel 
ordre des produits (il faut définir ce qui fait la soude par-delà les soudes 
d’Espagne ou de Rouen, définition que la science fournit). Cette logique 
de production doit aussi être protégée des interférences politiques et des 
mouvements de l’espace public. Les penseurs économistes inventent ainsi 
l’univers économique autonome qui suit ses logiques propres, optimales 
si on les laisse à elles-mêmes.
1. Graber 2009.
2. Fressoz 2012.
3. Fressoz et Pestre 2013.
4. Biagioli 2006.

466	
dominique pestre
 
Épanouissement d’un complexe « technoscience,  
industrie, militaire », des années 1870 aux années 1970
Proposer une vision temporelle large, comme nous le tentons dans 
cette conclusion, ne peut se faire indépendamment de la prise en compte 
des espaces : la géographie qui est privilégiée détermine la chronologie 
(la focale sur la France dans la section précédente induit ainsi nettement 
la périodisation). Proposer une vision de longue durée ne peut se faire 
non plus sans quelque arbitraire, sans le choix de principes organisa-
teurs – et je souhaiterais maintenant tester l’hypothèse que le siècle 
qui va des années 1870 aux années 1970 a suffisamment de cohérence 
pour être pris d’un bloc 1. Ce moment voit d’abord une transformation 
de l’image publique des sciences. Les grandes Expositions internatio-
nales qui commencent lors du second xixe siècle glorifient l’entreprise 
techno-industrielle. En Europe et aux États-Unis, celle-ci est globa-
lement acceptée : les luddites ont perdu la bataille et les cours de science 
du dimanche matin au Conservatoire des arts et métiers à Paris attirent 
massivement les ouvriers. À travers la mise en place d’une métrologie 
scientifique, du déploiement de standards, de la production en série et 
du management scientifique, les technologies et productions se « scien-
tifisent » et les pratiques de sciences s’industrialisent. Dans les années 
1930, à partir des expériences de guerre et de la gestion des colonies, 
« l’économie » (the economy) émerge comme catégorie et devient objet 
de gouvernement.
Des sciences expérimentales revues et étroitement gagées sur l’art 
technique et industriel dominent dorénavant le monde académique ; 
les laboratoires d’enseignement se généralisent – à Berlin, Cambridge, 
à la Sorbonne –, même si les sciences d’inventaires restent essentielles 
dans le monde colonial, que se constituent les « sciences sociales » et que  
s’épanouissent les humanités. Comme partout, le laboratoire devient  
un lieu plus organisé et hiérarchisé, et la division du travail comme 
la spécialisation y prévalent. L’entreprise science explose numéri-
quement – les laboratoires Bell, qui dépendent d’American Telegraph 
& Telephone, emploient plusieurs milliers de professionnels dès les 
années 1920. Cette extension, cette transformation des pratiques, cette 
émergence d’un nouveau régime technoscientifique, se font en rapport 
étroit avec une redéfinition de l’État – l’invention, en fait, d’une place et 
1. Ce geste est pour nous une manière de désacraliser nos propres chronologies – celle qui 
fonde l’ordre de cette Histoire en trois tomes. Pour une défense de cette chronologie-ci, voir 
Pestre 2003. Pour une défense de celle-là, voir l’« Ouverture générale en tête du tome 1 ».

	
conclusions générales des trois tomes	
467
d’un rôle radicalement neufs. Cet État nouveau est un État bien doté grâce 
à l’impôt ; un État scientifique, un État entrepreneur de science finançant 
recherches et sociétés (créant la Kaiser-Wilhelm Gesellschaft par exemple), 
préoccupé de techniques et d’innovation pour le bien supérieur du pays ; 
un État guerrier attentif à la qualité de ses armes, préparant la défense  
du pays et de ses intérêts économiques, politiques et impériaux grâce à 
la science (le Mémorial de l’artillerie est la plus grande revue française de 
statistique à la fin du xixe siècle) ; un État social qui vise l’intégration des 
« classes dangereuses » par les assurances, les sciences sociales et le vote 
masculin – c’est l’avènement des démocraties de masse ; et, finalement, 
un État régulateur qui entend contrôler les effets négatifs du progrès 
technique sur la santé ou l’environnement par des normes élaborées par 
les producteurs, les associations professionnelles et l’État 1.
Concernant les savoirs, trois grandes tendances sont à l’œuvre. D’abord 
le déploiement, dans le monde industriel et scientifique, d’une attitude 
pragmatique qui mobilise tous les moyens disponibles, au-delà des disci-
plines constituées de l’Université. Présentes dès la fin du siècle dernier 
dans le monde industriel, ces pratiques, qui sont aussi trans-métiers, sont 
généralisées par les militaires durant et après le second conflit mondial. 
Cette philosophie du everything goes où ce qui compte avant tout est le 
résultat conduit à une rupture vis-à-vis des valeurs publiques et morales 
attribuées à « la science 2 ». Un réductionnisme généralisé rendu efficace 
par la maîtrise des microphénomènes au laboratoire est la seconde chose 
qui frappe. Typique de la physique des électrons, des atomes, des noyaux, 
des particules, elle est vraie aussi de la microbiologie (les microbes), de la 
génétique – et plus tard de la molécularisation du vivant. Ce détour par 
le « micro » est extraordinairement productif car il autorise la création 
d’univers artificiels vastes et démiurgiques. Est finalement sensible, au 
fil de ces années, un usage accru des mathématiques, des processus de 
formalisation. Les approches statistiques et probabilistes, par exemple, 
se généralisent, pour les sciences physiques à la fin du xixe siècle, pour 
la biologie des populations et l’économie dans les premières décennies 
du xxe. Les modélisations s’étendent dans les années 1920 et 1930, tandis 
que le champ des « mathématiques appliquées » et des premières simula-
tions prend son essor dans les années 1950.
Concluons sur un point. Ce moment est celui d’une « scientifisation » 
des sociétés du Nord, d’une acceptation des sciences comme forces 
premières de transformation. « La Science » (l’emploi du singulier majuscule 
1. Pestre 2003.
2. La formule everything goes est de Paul Feyerabend 1979, dans Against Method.

468	
dominique pestre
se généralise dans le dernier tiers du xixe siècle en France) devient 
l’alter ego de « l’État », le moyen de dire le bien collectif et de proclamer 
sa neutralité face aux intérêts particuliers. La science, car c’est elle qui 
guide le progrès social, économique et technique, devient une institution 
de référence – ce qui n’est pas le cas en 1850 –, elle devient l’institution 
sur laquelle l’État s’appuie pour fonder ou justifier ses choix. Il le fait 
quand cela l’arrange, bien sûr, mais la science, qui en profite dans ses 
financements et son poids symbolique, devient alors l’une des institu-
tions centrales de la « modernité ».
Régime de libéralisation globale, biotechnologies,  
simulations et numérique – des années 1980 aux années 2010
Depuis quatre décennies, ce ne sont plus les mêmes savoirs, les mêmes 
disciplines ou les mêmes valeurs qui dominent le jeu scientifique. Dans les 
premiers temps du xxe siècle, ce sont les sciences physiques « fondamen-
tales » qui ont façonné les normes de la « bonne science » – la relativité, la 
mécanique quantique. Depuis les années 1980, d’autres (techno) sciences 
ont acquis la prééminence, notamment les sciences de la vie, les bio- 
techno-nano-sciences – des activités capables de recombiner le matériau 
biologique. Ces sciences, intrinsèquement orientées vers la production 
technologique, sont au cœur de nouvelles pratiques marchandes et de 
nouveaux modes d’exercice de la propriété. Leur rôle dans l’économie 
n’est pas aussi massif que celui des sciences et industries chimiques, par 
exemple, mais elles conduisent à des formes de biopolitique nouvelles 
dont la maîtrise est dans les mains des individus plus que dans celles 
des États 1. En cela, elles contribuent à des transformations profondes 
des mondes sociaux.
Les pratiques de sciences se sont aussi trouvées recomposées par 
le déploiement des outils informatiques et des banques de données, 
par le déploiement des grandes simulations – comme pour l’étude du 
changement climatique. De nouveaux « champs de science » ont émergé 
autour du système Terre et de ses équilibres, des sciences de l’environ-
nement terrestre, de la protection de la biodiversité – toutes choses 
nouvelles par rapport à ce qui a fait la gloire historique des Sciences mais 
qui renouent en partie, par-delà une parenthèse d’un siècle, avec l’histoire 
naturelle et ses inventaires. Or les conséquences sociales et politiques  
de ces nouvelles sciences et des biotechnologies sont considérables, et 
elles occupent l’espace public.
1. Rose 2007.

	
conclusions générales des trois tomes	
469
Mais il est un autre ensemble de nouveautés – le fait qu’une nouvelle 
économie politique et morale des savoirs est apparue dans les dernières 
décennies. Les règles productives ont été transformées et le pouvoir est 
globalement passé, dans la vie économique, des manageurs aux action-
naires et acteurs financiers – si l’on accepte de rester excessivement 
laconique. Au cours de ce processus, le politique tel que défini depuis les 
démocraties de masse s’est trouvé redéfini – du moins dans la plupart des 
pays –, voire marginalisé 1. Dans l’ordre géopolitique, nous sommes passés 
d’un univers régulé, dans le cadre de nations en équilibre westphalien, par 
des instances élues définissant des priorités, à des systèmes plus intégrés, 
économiquement globalisés, régulés dans des espaces de « gouvernance » 
multiples par des acteurs aux légitimités variables : de grandes compa-
gnies, la Banque mondiale, une pléiade d’ONG. Finalement, le monde 
paraît en équilibre instable, ouvert à tous les renversements. Ce sentiment 
est à l’opposé du sentiment de prévisibilité et de stabilité qui prévalait du 
temps de la guerre froide ; le résultat en est une impression d’incertitude 
croissante, un sentiment de « risque » à travers lequel les gens perçoivent 
leur relation au monde et à l’environnement 2.
Cette mutation s’est accompagnée d’une transformation des manières 
de produire les savoirs. D’abord, les intérêts présents dans le champ acadé-
mique se sont multipliés. Le capital-risque, le Nasdaq, les start-up, les 
avocats d’affaires, les grands programmes nationaux sont devenus plus 
importants dans l’orientation de la recherche, dans les formes qu’elle 
prend, dans ce qui est étudié et ce qui est oublié. Pour sa part, la recherche 
industrielle, elle aussi en concurrence globale toujours plus acharnée, 
s’est émancipée du cadre territorial qui demeure, par définition, celui 
des universités et des populations. La localisation de ses recherches  
est maintenant définie à l’échelle planétaire, au gré des potentialités et 
des opportunités. Dans les entreprises, le travail d’innovation a changé  
de nature lui aussi. La conception de produits et de lignées génériques –  
et non plus la R & D, comme entre 1870 et 1970 – est devenue la pierre 
angulaire du travail d’innovation. La « recherche » est ainsi devenue un 
paramètre qu’on tend à externaliser 3. Finalement, la définition et les règles 
de la propriété intellectuelle ont été profondément modifiées – ce qui a 
conduit à des formes de parcellisation des savoirs d’une part, des formes 
de monopole et de judiciarisation de l’autre 4. Une économie politique et 
morale des savoirs nouvelle s’est donc installée.
1. Supiot 2010.
2. Beck 1992.
3. Le Masson et al. 2006.
4. Pestre 2003.

470	
dominique pestre
En matière de pollutions et d’effets négatifs du progrès, la nouveauté est 
que la question est aujourd’hui posée en généralité : la question environ-
nementale et climatique est maintenant donnée comme universelle (elle 
concerne l’humanité) et globale (Gaïa est une). Au tournant des xviiie 
et xixe siècles, la contestation qui vise la chimie des soudes est déjà très 
puissante et ressemble beaucoup à celle que nous connaissons aujourd’hui 
(on ne voit d’ailleurs pas pourquoi nos ancêtres auraient réagi très diffé-
remment de nous face à la destruction de leurs cadres de vie proches) ; 
elle reste toutefois locale et ce sont les riverains qui mènent les études 
épidémiologiques et toxicologiques, pétitionnent et portent plainte. 
Même si ce type de contestation n’a pas perdu de sa vigueur, ces actions 
tendent aujourd’hui à être absorbées dans des discours plus vastes – là 
est la nouveauté : la question est devenue une bataille pour la survie 
de l’espèce et de la planète, et c’est la science qui seule peut en dire la 
réalité – qui, à part la science la plus équipée, peut dire s’il y a réchauf-
fement climatique ou non ?
Dire ce qui a fait le monde des savoirs et des sciences  
sur le long terme : huit thèses
Saisir ce qui se passe depuis cinq siècles autour des sciences et des savoirs 
ne saurait se réduire à l’exercice de périodisation très synthétique qui 
précède, forcément mutilant dans ce qu’il choisit d’ignorer ou de singulariser.  
Non qu’il ne soit pas essentiel de toujours reprendre ce travail – au contraire, 
il convient d’essayer d’identifier et de qualifier les régimes cognitifs et sociaux, 
techniques et économiques, intellectuels et moraux qui se succèdent et 
se chevauchent au fil du temps. Ce travail de périodisation ne peut bien 
sûr qu’être simplifié, partiel et partial – mais il ne peut en être autrement, 
et d’autres suggéreront des lectures alternatives, ou complémentaires.
Dans ce qui suit, je propose de reprendre la question autrement, de 
m’attarder plutôt sur des permanences, des constances entêtantes, des 
traits qui perdurent – et d’en profiter pour questionner l’inanité de huit 
lieux communs. À savoir : que l’Europe aurait inventé « les sciences », que 
celles-ci résulteraient d’abord et avant tout de la créativité de ses savants ; 
que les sciences ne seraient qu’affaires intellectuelles et conceptuelles, 
que les savoir-faire, les pratiques matérielles, artisanales ou industrielles, 
ne sauraient lui être décisives ; que le déploiement des sciences ne serait 
pas organiquement lié au commerce et au monde des affaires ; que les 
sciences ne seraient pas centralement affaire des États ; que les « commu-
nautés scientifiques » régleraient « en interne » leurs questions et qu’elles 

	
conclusions générales des trois tomes	
471
seraient au-dessus de l’espace et des opinions publiques ; que les sciences 
seraient à part des idéologies sociales, qu’elles pourraient dire la réalité 
objective des races et du genre, par exemple ; que nos ancêtres auraient 
développé leurs activités sans se soucier des dégâts infligés à la nature, qu’ils 
n’auraient pas eu la (rare) réflexivité qui est la nôtre aujourd’hui ; que les 
sciences progresseraient selon leurs logiques propres, finalement – alors 
que ces mutations peuvent avantageusement se lire comme déplacement 
des espaces où elles s’élaborent.
Thèse 1. Les institutions scientifiques européennes ou nord-américaines 
n’ont pas été les seules à remodeler les savoirs et à faire émerger les sciences. 
La rencontre avec les nouveaux mondes et les « Suds », leurs savoirs et leurs 
élites, a été tout aussi décisive.
La première thèse consisterait à dire que les transformations specta-
culaires qu’ont connues les sciences et savoirs durant ces cinq siècles 
ne trouvent pas simplement leur source dans la découverte, au Nord, 
d’une manière neuve de faire – « la science ». Ces savoirs ont impliqué au 
contraire des va-et-vient, des échanges entre Européens et populations  
et lettrés des Suds. Les diverses « mondialisations » qui commencent  
avec la fin du xve siècle ont impliqué des appropriations continues – sur 
les plantes, les animaux et les écosystèmes, les savoirs agricoles et la 
pharmacopée, les savoirs géographiques, cartographiques et de navigation, 
les savoirs linguistiques, anthropologiques, artistiques, les techniques 
et les manières de produire. En ces matières, une circulation intense 
d’informations a résulté des rencontres, et cela a conduit à une transfor-
mation radicale des questions, des savoirs et des images de soi, alors et 
aujourd’hui, ici et ailleurs.
On peut se faire une idée de la complexité du phénomène en nommant 
quelques intermédiaires : traducteurs, guérisseurs, éclaireurs, coureurs de 
bois, esclaves – et aujourd’hui paysans et « peuples indigènes ». En face ? 
Humanistes, missionnaires, explorateurs, administrateurs coloniaux, 
médecins naturalistes – et aujourd’hui associations et ONG, touristes et 
ingénieurs agronomes, Banque mondiale et OMS. Des lieux ? La pirogue 
des explorateurs européens guidée par des Amérindiens, le pont des navires 
sur lesquels les captifs sont interrogés, les tavernes et salles d’audience 
des ports où savoirs et règles de droit se confrontent – et aujourd’hui 
forum de Davos, forum social mondial et programme REDD (Réduction 
des émissions liées au déboisement et à la dégradation des forêts dans 
les pays en développement).

472	
dominique pestre
Mais il faut dire que, dans le moment même où il profite de ces échanges 
et réordonne les savoirs qu’il amasse, le Nord efface sa dette et orientalise 
« l’autre ». Ces nouveaux savoirs naissent certes à travers les échanges 
mais ces emprunts et appropriations sont rayés des récits et mémoires, 
et l’autre de la rencontre est raconté, essentialisé comme incapable, non 
scientifique – sauvage. Et le Nord se construit sur cet oubli, ce refou-
lement – il se définit dans le geste même qui singularise « la science » 
qui le met à part et le place seul dans l’ordre d’une raison dont l’autre est 
exclu. Les alliés de ces nouveaux savoirs, commerçants ou militaires, sont 
par ailleurs organisés pour la conquête ou la prédation. Les conséquences 
sont donc souvent terribles : décimation des populations d’Amérique  
par les Européens et les pathogènes qu’ils importent ; traite des Noirs 
entre les ports européens, la Côte-de-l’Or et le continent américain ; 
destruction des formes traditionnelles d’agriculture (par l’imposition 
violente de la propriété privée dans la vallée du Nil au xixe siècle par 
exemple 1) – ou, pour les dernières décennies, refonte radicale du monde 
par les politiques d’ajustement structurel, toujours données comme les 
seules possibles et rationnelles.
Mais penser en termes d’intermédiaires reste encore limité. Les guerres 
permanentes qui ravagent l’Europe (et le monde) au cours des cinq siècles 
engendrent des mouvements massifs de populations, des diasporas – les 
Morisques, Séfarades, Grecs, Arméniens au xviie siècle par exemple. Ces 
diasporas vivent souvent du commerce, mais certaines sont centrales 
pour les savoirs et les transferts techniques – c’est le cas des huguenots 
hors de France à l’époque moderne et des jacobites hors d’Angleterre. 
Et si l’on souhaite faire un saut brutal dans le temps, on retiendra l’émi-
gration des intellectuels juifs hors d’Allemagne dans les années 1930, 
dont un effet fut une recomposition profonde des sciences, d’abord aux 
États-Unis et, de là, dans le monde entier.
Thèse 2. Les savoir-faire, les savoirs artisanaux et de production ont été 
(et sont toujours) décisifs dans l’émergence (et la transformation continue) 
des sciences ; cela est notamment vrai pour la culture de précision et les 
pratiques expérimentales.
Mais il ne s’agit pas que des savoirs sur la nature, les plantes, les eaux, les 
cieux. Il s’agit aussi des savoirs pratiques des artisans et ouvriers, de ceux 
des constructeurs et ingénieurs sans lesquels les sciences « techniciennes  
1. Mitchell 2002.

	
conclusions générales des trois tomes	
473
et industrieuses » n’auraient pu se constituer et se réinventer à chaque 
siècle. C’est que les savoir-faire sont vitaux dès qu’il s’agit d’opérer matériel-
lement. Le point est acquis depuis longtemps que les savoirs tacites et les 
tours de main sont au cœur de la vie de laboratoire et de la vie productive. 
Ce sont eux qui garantissent les succès et qui sont les plus délicats à 
acquérir. Leur transfert n’opère que par le « faire avec » et leur formali-
sation, qui est un travail de Sisyphe, est à jamais insuffisante 1.
Aux xviie et xviiie siècles, la pratique expérimentale est encore très 
composite, peu codifiée – et Pascal peut être accusé par Boyle de ne pas 
avoir réellement fait ses expériences sur le vide. Le travail pratique dans 
les Académies de Londres ou de Paris n’existe souvent que grâce aux 
démonstrateurs (qui ne sont pas toujours considérés comme des savants), 
les seuls à travailler de leurs mains. Les gentlemen des Académies savent 
aussi que les artisans ont des connaissances pratiques qu’ils n’ont pas, et 
qu’il faut apprendre auprès d’eux. Cela est vrai de la construction navale 
aux xviie et xviiie siècles – une activité risquée pour qui veut innover 
trop radicalement, comme le montre le désastre du Wasa en Suède sous 
le roi Gustave II Adolphe – et Réaumur visite les ateliers parisiens à cette 
époque pour augmenter les capacités des sciences et du royaume 2.
Un siècle et demi plus tard, la physique de précision se construit en 
lien étroit avec les producteurs d’instruments (Zeiss pour l’optique  
en Allemagne) et l’industrie de la machine-outil ; la nouvelle valeur de la 
précision, dans les sciences de laboratoire, dérive de l’organisation de la 
production, de l’industrie des pièces détachées, du management des ateliers 
et chantiers, de l’organisation scientifique du travail – de la domestication 
des savoir-faire et des gestes ouvriers ; et cette pratique de la précision se 
retrouve dans d’autres milieux, les grands jardins botaniques ou la mise 
en œuvre des surveys 3. Dans le second xixe siècle, elle se traduit par la 
détermination de l’« équation personnelle » des observateurs en astro-
nomie, la formation pratique des apprentis physiciens dans les laboratoires 
d’enseignement, le contrôle strict des travaux des jardiniers. Et, bien sûr, 
la Société allemande de physique créée en 1845 n’est pas qu’« acadé-
mique » : comme toutes les autres sociétés de physique jusqu’au milieu 
du xxe siècle, elle regroupe, auprès des universitaires et ingénieurs, des 
artisans, des producteurs, des entrepreneurs.
1. Collins 1985.
2. Licoppe 1996.
3. Drayton 2000, Secord 1986. Merci à Simon Schaffer pour le rappel de ces références.

474	
dominique pestre
 
Thèse 3. Le monde des affaires, du commerce et de la production a toujours 
constitué une ressource majeure du déploiement des savoirs et des sciences.
Il est souvent acquis, dans les milieux scientifiques, que la science est 
mère des développements techniques. Est aussi présent aujourd’hui le 
discours d’une certaine « colonisation » de la science par le monde des 
affaires, dont les « intérêts » réduiraient les marges de manœuvre des 
sciences elles-mêmes. Cela n’est pas sans fondement, loin s’en faut, mais 
la mise en perspective historique à laquelle incitent ces trois volumes 
conduit à une histoire plus contrastée.
Depuis cinq siècles, les producteurs de savoir ont en effet entretenu 
des relations étroites avec le monde du commerce et de la production. 
Entre commerce et collectionneurs, savants, aristocrates et marchands, 
la chose va de soi : les savoirs sur les objets (l’origine d’un manuscrit, la 
propriété médicinale d’une plante, la qualité d’un objet d’art) sont à la fois 
au cœur du travail savant, au fondement de la collection et aux origines 
des marchés. L’exemple est particulièrement probant au xviie siècle : 
les objets manufacturés, les produits exotiques, les plantes, les manus-
crits sont choisis au loin par les marchands et leurs experts ; ils sont 
transportés et regroupés dans des entrepôts ou des jardins, où ils sont 
classés ; ils sont « reconditionnés » comme objets de consommation, et 
des Bourses de commerce sont établies pour en établir les prix. Dans  
ce mouvement, science et commerce se déploient de concert, sans rupture 
de l’un à l’autre, largement au bénéfice des deux. L’essentiel est ici de 
comprendre que la dynamique des échanges est d’abord marchande, et 
que c’est sur cette circulation globale d’objets que se greffent les savoirs 
et leur circulation propre.
Mais le lien, quoique de nature différente, est tout aussi central pour la 
vie industrieuse. Nous l’avons dit pour le début du xixe siècle et la chimie 
des soudes et des acides – les savants et inventeurs sont eux-mêmes entre-
preneurs et régulateurs. Dans la chimie de synthèse et les laboratoires 
industriels de la fin du xixe siècle, les professeurs de l’Université sont 
consultants dans l’industrie et les réseaux d’échange d’étudiants, d’argent 
ou de produits sont continus entre les deux mondes. Les pionniers de la 
microbiologie développent, sur le même modèle, des liens étroits avec 
l’industrie du médicament. Behring devient un entrepreneur à succès, 
tandis que l’Institut Pasteur développe ses propres centres de production. 
Dans l’industrie électrique des États-Unis au tournant du xxe siècle, ce 
sont des inventeurs (souvent ingénieurs) qui déploient des stratégies de 
contrôle via les brevets et fondent les firmes les plus puissantes (Edison, 

	
conclusions générales des trois tomes	
475
Sperry). Dans tous les cas, l’intrication est vitale – ce qui ne veut pas dire 
que tout le monde fait le même métier.
Pour le reste du xxe siècle, les exemples sont tout aussi infinis, et les 
recompositions scientifiques qui en découlent spectaculaires : ainsi, la 
constitution de la physique des solides dans les années qui chevauchent 
la Seconde Guerre mondiale résulte moins de logiques disciplinaires 
(l’évolution de « la » science) que du rôle de grandes compagnies (de l’élec-
tronique) visant des objectifs pratiques (développer des semi-conducteurs 
artificiels) et qui mobilisent pour ce faire une gamme de techniques 
(physique électronique, chimie des matériaux, cristallographie, fabri-
cation de cristaux, mécanique quantique) que personne n’avait articulée 
ainsi. Ces entreprises développent des stratégies de développement axées 
sur les responsables des brevets (qui ont des formations scientifiques  
et d’ingénieurs), qui circulent entre départements et identifient points de 
résistance et synergies possibles, et font advenir un autre arrangement, 
une autre manière de combiner les savoirs théoriques et les faire – ce 
qui devient la « physique des solides ». À la fin des années 1950, le même 
phénomène va se répéter avec la science des matériaux, les militaires 
servant cette fois de force de convergence, de pivot financier et organisa-
tionnel pour permettre l’émergence et l’institutionnalisation de ce champ 
nouveau, et nécessaire pour eux 1.
Thèse 4. Les savoirs, et les sciences en particulier, ont toujours dépendu 
centralement de l’État, acteur direct ou indirect de beaucoup de leurs 
développements.
La densité de l’alliance entre les porteurs de savoirs et les mondes 
du commerce et de la production est toutefois encore plus frappante 
si l’on inclut l’État dans l’équation – même s’il est bien clair que l’État  
n’est jamais unifié et que les variations sont infinies de la Florence de 
1500 aux États-Unis de 2010, et de la Chine des Ming à celle de Mao. 
Les liens entre savoirs et États tiennent d’abord aux questions de sécurité 
et de guerre : pour les militaires, les savoirs pratiques sont choses trop 
sérieuses pour être laissées aux seuls savants ou ingenio. Cela tient ensuite 
à des questions d’inventaire, de connaissance des ressources naturelles, 
humaines et productives – ce qui est au cœur des activités scientifiques 
de l’État fédéral américain au xixe siècle par exemple. Pour des raisons 
économiques, les milieux d’affaires demandent aussi souvent à l’État 
1. Pestre 1992.

476	
dominique pestre
de les aider – par les armes, la législation ou d’autres politiques des 
brevets, ce qu’il apprend à faire. Enfin, lorsque les sciences de labora-
toire deviennent des composantes majeures du progrès industriel, à la 
fin du xixe siècle, les États eux-mêmes se font entrepreneurs de science, 
financeurs et organisateurs. Et puisqu’il vaut d’être clair, précisons que 
ce rôle est aujourd’hui plus massif que jamais du fait de la concurrence 
économique, maintenant planétaire, et de la croyance que le futur des 
nations relève de leur capacité à développer chacune leur Silicon Valley.
Il faut donc partir du postulat, au rebours de récits bien connus, que la 
rencontre entre sciences, État et militaires ne constitue pas une « erreur 
de casting » mais est une réalité ordinaire depuis cinq siècles, et qu’elle 
n’implique pas que quiconque ait « vendu son âme » dans cette affaire. Les 
princes des villes italiennes du xvie siècle entretiennent des mathémati-
ciens-ingénieurs, les royaumes ibériques puis l’Angleterre se construisent 
des marines qui forment la colonne vertébrale de l’État, le royaume de 
France favorise au xviiie siècle l’ingénieur-savant – l’oncle hollandais 
de la physique, comme dit Lewis Pyenson dans une belle formule. La 
Prusse et la République française promeuvent l’artillerie scientifique  
à la fin du xixe siècle, et les Britanniques, les Italiens et les Français 
utilisent les premiers avions pour ramener la « paix » dans les colonies via 
le bombardement stratégique des populations civiles 1. Les Britanniques 
développent la recherche opérationnelle pendant la Seconde Guerre 
mondiale, les États-Unis et l’URSS les missiles de la guerre froide – et  
les États-Unis d’aujourd’hui et Israël les drones de la guerre contre le 
terrorisme. Et il est toujours suffisamment de savants et d’ingénieurs 
prêts à faire ce travail, même s’il est toujours des refus.
Le rôle des États est plus large que celui des militaires. Les souverains 
soutiennent les arts et les sciences pour des raisons variées. Dans l’âge 
baroque, des raisons de prestige et de renommée, des raisons de puissance 
économique ou mercantile, des raisons de production agricole ou fores-
tière, des raisons fiscales – puisque les guerres sont permanentes et qu’il 
faut fonder en nombre les taxes qui sont prélevées. Mais le caméra-
lisme, l’arithmétique politique, l’économie politique, la statistique sont  
les pratiques de sciences associées le plus ordinairement aux États  
qui développent par ailleurs des formes de gouvernementalité nouvelles : 
vaccinations ou encadrements hygiénistes au début du xixe siècle, 
assurances sociales et droit du travail un siècle plus tard.
Trois points peuvent ici être retenus. D’abord, la « question statis-
tique » n’est pas partout le monopole des États : dans beaucoup de 
1. Lindqvist 2000.

	
conclusions générales des trois tomes	
477
pays elle est le fait d’associations et se décline autrement dans l’espace 
public. Les institutions chargées des données essaient ensuite, réguliè-
rement, de normaliser leurs procédures (de collecte, de traitement), mais  
ceci est historiquement et à jamais inachevable. Nous ne devons donc 
pas être trop « wébériens » dans nos récits : la quête avide des faits est 
un rêve de maîtrise qui produit des effets lorsqu’elle est soutenue par les 
pouvoirs, mais qui rencontre toujours de grandes limites et n’est efficace 
que ponctuellement.
Quant aux technologies utilisées, elles ont été remarquablement  
stables du xviiie siècle aux années de la guerre froide. Les technologies 
de papier permettant de prendre des notes, écrire, copier, classer et 
retrouver l’information varient peu durant ces deux siècles, et on les 
retrouve du travail intellectuel à l’activité administrative, du classement 
naturaliste à l’organisation du commerce par correspondance. Elles ont 
connu, de 1850 à 1950, des modifications via la mécanisation, l’électrifi-
cation des machines et les meubles du bureau moderne – inventions qui 
relèvent surtout du monde des affaires – mais sans que la visée intellec-
tuelle soit radicalement bouleversée 1. À lire les promoteurs actuels des 
mondes numériques et du data mining, en revanche, cela serait en train 
de changer – de façon extrêmement radicale. Au point que les sciences 
de laboratoire elles-mêmes, leurs démarches hypothético-déductives 
comme la construction de leurs données, seraient en voie d’obsolescence : 
de nouveaux outils et logiciels feraient simplement sortir les informa-
tions pertinentes de la masse de données assemblées à chaque minute 
par les machines du commerce et de la communication 2. Heureusement, 
en un sens, les prophéties de bonheur (la NSA comme pourvoyeur des 
vrais savoirs) n’engagent que ceux qui y croient.
Thèse 5. Convaincre l’opinion et séduire les populations des merveilles des 
« sciences » a constitué, et constitue toujours, une préoccupation constante 
des savants, du monde politique et de celui des affaires.
Les savoirs et les sciences ne se construisent pas que dans le labora-
toire ou le bureau du statisticien. Parce qu’ils sont liés à la vie technique, 
politique, économique, et à la consommation, savoirs et sciences parti-
cipent des enjeux de la sphère publique. Et promouvoir ces sciences et 
1. Gardey 2008.
2. Voir l’éditorial étonnant de Chris Anderson, « The End of Theory : The Data Deluge Makes 
the Scientific Method Obsolete », Wired Magazine, 2008, vol. 16, no 7.

478	
dominique pestre
savoirs auprès de l’opinion publique, éclairée ou populaire, est pratique 
courante de la Renaissance au xxie siècle.
La promotion des savoirs et techniques est d’abord moyen d’osten-
tation, de mise en scène des fastes et capacités du prince, de l’industrie, 
de la nation. Les grands spectacles d’artifices et les entrées royales 
(au xviie siècle), les grands spectacles des boulevards parisiens et les 
Expositions internationales (aux xixe et xxe siècles), relèvent de ce monde. 
Mais cette promotion se fait plus quotidiennement dans les almanachs, 
les affiches publicitaires (à la fin du xviiie siècle), via les démonstrateurs 
itinérants du Nouveau Monde (encore au xixe siècle), par la publicité et 
les institutions de science elles-mêmes (depuis deux siècles). La Royal 
Institution est créée à Londres pour donner des cours publics au début du 
xixe siècle, comme à Paris le Conservatoire national des arts et métiers ; 
les ondes électriques découvertes par Hertz en 1888 sont immédiatement 
exhibées en public à Londres, Paris et Vienne ; et le CERN, à Genève, porte 
une attention soutenue, depuis cinquante ans, à montrer à des publics 
nombreux ses machines, les particules et leur physique.
Les publics friands de sciences se déploient à partir du second xviie siècle. 
Cela renvoie à la montée de la philosophie expérimentale, à la vénération 
pour les machines et instruments, à la fascination pour le merveilleux 
(au cœur de toute « vulgarisation », hier comme aujourd’hui), à la porosité 
des communautés savantes et des monteurs de spectacles, à une culture 
urbaine à la fois érudite, ludique et esthétique. À la fin du xviiie siècle, 
l’essor du commerce, la montée du débat politique et la vivacité des cafés, 
clubs et journaux conduisent à un élargissement des publics fascinés 
par les sciences, les machines et les objets nouveaux de consommation. 
À partir du milieu du xixe siècle, c’est le spectacle des réussites de la 
technologie, de l’industrie et du progrès qui s’impose. À la fois pour des 
raisons scientifiques ou pratiques, commerciales ou de démonstration 
de puissance, mais aussi pour des raisons politiques : après les révolu-
tions de 1848 et le surgissement des populations ouvrières sur la scène 
publique, les expositions sont repensées pour célébrer un progrès qui 
soit dorénavant inclusif d’un point de vue social.
À partir du milieu du xixe siècle, la concurrence des Expositions inter-
nationales fait rage, comme fait rage la concurrence entre industries 
nationales au sein de chaque exposition. Des réalisations monumen-
tales (une lunette astronomique de 60 mètres de foyer est exposée à 
Paris en 1900 1) côtoient des produits commerciaux et, bien sûr, l’exhi-
bition de « spécimens » des populations coloniales. L’architecture suit le 
1. Launay 2007.

	
conclusions générales des trois tomes	
479
mouvement et aux formes classiques succèdent le Crystal Palace et la 
tour Eiffel. Avec le milieu du xxe siècle, les changements dans la nature 
des médias amènent à d’autres formes de mise en scène (le cinéma, les 
centres de culture scientifique, les simulations). Mais une chose reste 
stable : la saturation des espaces publics par des images de science, de 
technique, de progrès – et de promesses sans fin.
Thèse 6. Les savoirs et les sciences ont largement contribué, au long de  
ces siècles, à la définition des races, du genre, et à la domestication des 
corps vils.
Une question obsède les cinq derniers siècles, du moins au Nord, celle 
des races. Reposée par les « grandes découvertes », la traite et la coloni-
sation, elle s’inscrit dans l’obsession chrétienne antérieure, qui ne disparaît 
pas, et qui lie sang, race et religion autour des musulmans, ennemis  
de l’extérieur, et des juifs, ennemis perfides de l’intérieur. Que ce soit dans 
les sciences et savoirs, ou dans les consciences ordinaires, les itinéraires 
du sang, des religions et des races ne cessent de se croiser, de la contro-
verse de Valladolid à l’extermination des Juifs et Tsiganes par les nazis 1. 
Fonder en nature les races (et le genre) est d’ailleurs une activité que les 
sciences ont toujours prisée – avec des retenues après la Shoa –, et les 
sciences sociales ont contribué, comme elles l’ont pu, aux discours de 
justification de la colonisation.
Dans les discours savants, l’obsession de la race se mêle, dès le début, 
à la question de la sexualité et de ses excès, à la question des femmes et 
de leur contrôle, à la question du genre et des déviants (ne devrait-on 
pas penser ici un lien tout simple avec la nature très masculine de la cité 
savante ?). Dès l’abord, toutefois, cette obsession se tient aussi au plus 
près de la question de l’animal et de ses continuités biologiques avec les 
races inférieures de l’humanité. Avec les Lumières, des évolutions sont 
notables et la racialisation se donne plutôt comme une affaire de stades 
ou d’étapes dans le progrès de l’humanité, et comme une question de 
classification naturelle. Le discours de la race dit alors une série qui 
va de l’orang-outang au Noir et au Blanc sur une échelle d’évolution 
naturelle-et-civilisationnelle.
À partir de 1850, cette compréhension de la race comme catégorie 
indissociablement biologique et sociale est admise, banalisée au cœur 
de l’Europe et des territoires qu’elle modèle. Au centre de cette science 
1. Anidjar 2014.

480	
dominique pestre
des races se trouvent la biologie et l’anthropologie physique mais les 
complicités sont plus vastes et l’histoire naturelle, l’anatomie comparée, 
la géographie et la linguistique sont impliquées. L’objectif de cette science 
raciale, qui connaît des opposants et d’infinies nuances, est de montrer la 
hiérarchisation de l’humanité, de dire les oppositions entre « Sémites » et 
« Aryens » par exemple, ou la victoire inéluctable des races supérieures 
à cerveau plus développé (« indo-germaniques » notamment) – ce que 
ne renie pas la théorie de l’évolution.
Dans le mouvement d’expansion impériale et coloniale des Européens, 
il est toutefois des sources plus ordinaires à la formation des catégories 
raciales-genrées que les productions de connaissance. À savoir la réduction 
en esclavage des Noirs, massive et continue ; les longues durées de  
l’antisémitisme et de la persécution des minorités entre Europe et espaces 
atlantiques ; et les rapports de genre qui se masculinisent lourdement au 
long du xixe et premier xxe siècle. Ces savoirs et pratiques sont traduits 
en discours qui permettent de tenir à distance, suivant les contextes,  
les Maures, les juifs, les protestants, les catholiques, les Indiens, les Noirs, 
les femmes, les homosexuels, les fous, les francs-maçons, les criminels, 
les classes dangereuses. Des hommes d’Église, des savants, des médecins, 
des scientifiques participent à ce travail de désignation et d’assignation – à 
partir d’observations cliniques, de mesures anthropologiques, de classe-
ments, d’enquêtes, d’expériences sur des sujets, de théories et de grandes 
expositions publiques.
Après la Seconde Guerre mondiale, les discours antisémites ont moins 
voix au chapitre sur la scène publique. Vingt ans plus tard, les discours 
racistes sont à leur tour plus difficiles à tenir, et les discours sexistes se 
réduisent – même si ces situations s’inversent à nouveau depuis une 
vingtaine d’années. Dans les mondes savants, en revanche, les races ne 
constituent plus une catégorie acceptable depuis 1945 et les biologistes 
travaillent sur la « diversité humaine ». Ce qui n’évacue peut-être pas les 
impensés et l’immensité des ambiguïtés.
Thèse 7. Les transformations que l’humanité impose à la nature ont préoccupé 
les savants depuis des siècles. Cela n’est pas un phénomène des dernières 
décennies.
Il est un lieu commun aujourd’hui qui veut que nos ancêtres aient été 
modernisateurs sans souci des conséquences de leurs actes, des dégâts 
infligés à la nature. Nous seuls, depuis quelques décennies, serions 
attentifs à nos environnements et gérerions ceux-ci avec attention et  

	
conclusions générales des trois tomes	
481
réflexivité 1. Ce que montre cet ouvrage est combien cette image de notre 
exceptionnalité est fausse. À la fois car la modernisation a suscité ces 
questions très tôt, mais aussi car notre grande conscience ne semble pas 
conduire, si l’on en juge par les courbes du phénomène « Anthropocène », 
à des résultats qualitativement différents de ceux du passé.
Deux remarques pour donner chair à cet énoncé. D’abord nos ancêtres 
ont souvent réagi avec vigueur lorsqu’ils étaient victimes de dégâts 
sanitaires ou environnementaux – lors d’une pollution de rivière par 
une production artisanale ou industrielle par exemple. Cela ne doit  
pas surprendre outre mesure puisque c’est le droit de propriété qui est 
d’abord impliqué – même s’il n’est pas le seul. Et depuis que cette question 
a été mise à l’agenda des historiens, les exemples abondent où l’on voit, 
depuis deux siècles, des populations mobiliser des savoirs multiples (épidé-
miologiques, statistiques, de laboratoire) pour qualifier les dommages 
dont ils sont victimes, défendre leurs droits, monter des coalitions, aller 
en justice, demander des réparations pour les dégâts sanitaires ou environ-
nementaux subis et des mesures de prévention pour le futur.
Reste qu’on pourrait croire que c’est tout de même bien notre époque 
qui seule a pu penser les phénomènes globaux, le changement climatique 
par exemple – puisqu’il faut souvent, à cette échelle, des savoirs et outils 
(calculateurs, satellites) que nos ancêtres n’avaient pas. Mais là aussi, 
chaque époque fait avec les moyens qu’elle a. Le paradoxe est peut-être 
ici que c’est en contexte colonial que se construit d’abord l’inquiétude 
que l’humain altère radicalement les climats. Humboldt impute ainsi 
certains assèchements de réserves d’eau qu’il rencontre en Amérique 
latine aux colonisateurs espagnols et à leurs pratiques. En découlent, aux 
xviiie et xixe siècles, de longs débats législatifs, des enquêtes scientifiques 
nombreuses, et des politiques de reboisement à travers toute l’Europe.
Le paradigme d’une modification anthropique des climats (et d’une 
responsabilité humaine) s’estompe toutefois quelque peu entre 1870 
et 1970. Certainement parce que ce siècle croit profondément au progrès 
industriel et aux capacités démiurgiques des sciences. Il est vrai que les 
succès de l’industrie sont spectaculaires – et que la « science » devient une 
institution de référence pensée comme capable de trouver la solution à 
tout problème. Mais c’est aussi que la microbiologie, qui s’invente alors, 
tend à déconnecter les maladies des environnements en les attribuant 
aux seuls microbes – un réductionnisme qui fait du laboratoire le remède 
absolu contre les inerties naturelles. On peut encore invoquer le libéra-
lisme, qui l’emporte dans ces années et croit aux équilibres qui s’instituent 
1. Beck, Giddens et Lash 1994.

482	
dominique pestre
d’eux-mêmes ; ou convoquer la sociologie qui tend alors à penser les 
sociétés comme hors-sol et à réduire toute question aux seuls rapports 
entre humains 1. En revanche, avec les années 1960, l’explosion des pollu-
tions (chimiques notamment) et la montée d’une jeunesse passée par les 
écoles et les sciences, la place des humains dans les dérèglements plané-
taires revient fortement au premier plan des préoccupations sociales – et 
émerge une sociologie assez neuve qui tente de penser humains et milieux 
naturels dans le même mouvement.
Thèse 8. Une bonne manière de comprendre l’évolution des savoirs et des 
sciences est de suivre la géographie physique et sociale de leurs lieux de 
production.
Dans la première partie de ce texte, j’ai tenté une narration des régimes de 
savoir et de science depuis cinq siècles. Pour terminer, j’aimerais reprendre 
cette question à partir d’une autre idée, d’une idée plus simple – qu’une 
bonne manière de rendre compte de l’évolution des savoirs consiste à 
suivre les déplacements des lieux, espaces et institutions dans lesquels 
ils s’élaborent.
Une thèse a longtemps prévalu – celle d’une révolution scientifique ayant 
eu lieu quelque part entre xvie et xviiie siècle en Europe. Cette révolution 
a connu des couleurs variées – elle fut une révolution dans les visions du 
monde (copernicienne), une révolution métaphysique (platonicienne), 
une révolution dans le respect des faits (expérimentale), une révolution 
des régularités numériques (mathématique) – en tout cas une révolution 
de l’esprit, du regard. On aura compris à lire ces trois volumes que cela 
est peut-être trop simple pour être gardé en l’état. Non que les idées et 
les ruptures n’aient pas de sens – elles sont choses majeures et ont à voir 
avec cette histoire. Qui oserait dire, d’ailleurs, qu’il n’est pas d’idées dans 
la production des savoirs, qu’aucune révolution n’advient jamais ? Mais 
mon point n’est pas ici celui-là, il est plutôt de partir du fait très simple 
que, comme toute activité humaine, l’activité de savoir dépend (aussi)  
de qui produit les énoncés, de quand, où et comment ils sont élaborés. Il 
est donc essentiel de nommer ces divers espaces producteurs de science 
et de savoir, de caractériser les questions qu’ils autorisent, écartent ou 
ne voient pas, de dire ce qu’ils considèrent valide, intéressant ou dénué 
de sens. Et, partant de leurs évolutions, d’imaginer la transformation des 
formes de savoir qui en découlent.
1. Fressoz et Locher 2012.

	
conclusions générales des trois tomes	
483
C’est toutefois au lecteur que je laisserai cette fois le soin (en guise 
de homework et à la suite de la lecture de cette Histoire) de qualifier les 
effets de ces déplacements d’espaces sur les savoirs qui importent. Je me 
limiterai donc à l’évocation de quelques lieux souvent rencontrés dans 
ces trois volumes. Ce qui se passe entre Renaissance et xxie siècle est 
d’abord l’apparition de milieux humanistes et d’échoppes d’imprimeurs, 
de villes libres et marchandes, de voyages de découverte et du voyage 
des plantes, de rencontres sur les plages et dans les villes du monde, 
d’arsenaux et d’armées préparant une autre guerre – de la nouvelle artil-
lerie au tracé à l’italienne. Mais encore de congrégations partant convertir 
le monde et cherchant à le connaître, de cours princières et de cabinets 
de curiosités, de la république des lettres, de magasins d’instruments, 
de Compagnies des Indes, de salons aristocratiques à Paris, de salons 
créoles à Mexico, d’académies à Calcutta, de publics curieux et payants 
à Londres et Bogota, de tavernes et cafés à Amsterdam ou Manille, de 
loges maçonniques, d’académies royales à Saint-Pétersbourg et Vienne, 
de jardins et collections nouvelles, de muséums d’histoire naturelle et 
d’écoles d’ingénieurs-scientifiques et militaires.
Au cours des deux derniers siècles la liste s’allongerait de la création de 
nombreuses écoles (en commençant peut-être avec l’École polytechnique 
de Paris), des écoles d’officiers en au moins aussi grand nombre (West 
Point), d’universités nouvelles (allemandes pour commencer), de surveys 
de toute nature (en Europe d’abord, en Amérique du Nord et dans les 
colonies ensuite), d’ateliers d’inventeurs et de laboratoires d’entreprises, de  
laboratoires de recherche et d’enseignement, de bureaux des brevets,  
de chaînes de production chimiques et mécaniques, de centres et colloques 
internationaux de métrologie, de firmes d’ingénieurs conseils, de bureaux 
des méthodes et de l’organisation scientifique du travail ; mais encore de 
facultés et de nouvelles sciences, très nombreuses autour des humanités 
et de la question sociale à la fin du xixe siècle, de politiques publiques, 
technologiques, industrielles ou scientifiques, d’avocats d’affaires, de 
crédits d’impôts, de bourses technologiques comme le Nasdaq, d’ordi-
nateurs et d’outils financiers ultrarapides ; et d’amateurs et de musées 
de sciences, de spectacles et d’expositions internationales, de revues de 
« vulgarisation », d’émissions de télévision et de radio, du Web ; et de 
bourses de thèse, et d’une « démocratisation » des sciences, et de hackers, 
et de sciences participatives, et de conférences de consensus…
Et si quelqu’un souhaite dire qu’il y a bien eu une « révolution scien-
tifique » entre 1500 et 2010, ce qui ne semble pas un énoncé saugrenu, 
alors ces deux derniers paragraphes disent comment il pourrait faire 
pour commencer à la raconter.

484	
dominique pestre
 
Références bibliographiques
Anidjar Gil, 2014, Blood : A Critique of Christianity, New York, Columbia University 
Press.
Beck Ulrich, 2008 [1992], La Société des risques, Flammarion.
Beck Ulrich, Giddens Anthony et Lash Scott, 1994, Reflexive Modernization : 
Politics, Tradition and Asthetics in the Modern Social Order, Cambridge, Polity 
Press.
Belhoste Bruno, Picon Antoine et Sakarovitch Joël, 1990, « Les exercices dans 
les écoles d’ingénieurs sous l’Ancien Régime et la Révolution », Histoire de l’édu-
cation, no 46, p. 53-109.
Biagioli Mario, 2006, « Patent Republic : Representing Inventions, Constructing 
Rights and Authors », Social Research, vol. 73, no 4, p. 1129-1172.
Collins Harry M., 1985, Changing Order : Replication and Induction in Scientific 
Practice, Londres, Sage.
Delon Michel, 2007, « xviiie siècle », in Jean-Yves Tadié, La Littérature française. 
Dynamique et histoire II, Paris, Gallimard, coll. « Folio Essais », p. 9-294.
Drayton Richard Harry, 2000, Nature’s Government : Science, Imperial Britain, and the 
« Improvement » of the World, New Haven (CT), Yale University Press.
Feyerabend Paul, 1979 [1re éd. en anglais 1975], Contre la méthode. Esquisse d’une 
théorie anarchiste de la connaissance, Paris, Seuil.
Fressoz Jean-Baptiste, 2012, L’Apocalypse joyeuse. Une histoire du risque technolo-
gique, Paris, Seuil.
Fressoz Jean-Baptiste et Locher Fabien, 2012, « The Frail Climate of Modernity : 
A Climate History of Environmental Reflexivity », Critical Inquiry, vol. 38, no 3, 
p. 579-598.
Fressoz Jean-Baptiste et Pestre Dominique, 2013, « Risque et société du risque 
depuis deux siècles », in Dominique Bourg, Pierre-Benoît Joly et Alain Kaufmann 
(dir.), Du risque à la menace. Penser la catastrophe, Paris, PUF, p. 19-56.
Gardey Delphine, 2008, Écrire, calculer, classer. Comment une révolution de papier a 
transformé les sociétés contemporaines (1800-1940), Paris, La Découverte.
Graber Frédéric, 2009, Paris a besoin d’eau. Projet, disputes et délibération technique 
dans la France napoléonienne, Paris, CNRS Éditions.
Koyré Alexandre, 1962, Du monde clos à l’univers infini, Paris, PUF.
–	 1966 [1939], Études galiléennes, Paris, Hermann.
Launay Françoise, 2007, « The Great Paris Exhibition Telescope of 1900 », Journal for 
the History of Astronomy, vol. 38, p. 459-475.
Le Masson Pascal, Weil Benoît et Hatchuel Armand, 2006, Les Processus d’inno-
vation. Conceptions innovantes et croissance des entreprises, Paris, Hermès-Lavoisier.
Licoppe Christian, 1996, La Formation de la pratique scientifique, Paris, 
La Découverte.
Lindqvist Sven, 2000, Une histoire du bombardement, La Découverte, 2012.
Mitchell Timothy, 2002, Rule of Experts : Egypt, Techno-Politics, Modernity, Berkeley 
(CA), University of California Press.
Pestre Dominique, 1992, « Les physiciens dans les sociétés occidentales de l’après-
guerre. Une mutation des pratiques techniques et des comportements sociaux et 
culturels », Revue d’histoire moderne et contemporaine, vol. 39, no 1, p. 56-72.
–	 2003, Science, argent et politique. Un essai d’interprétation, Paris, Quæ.

	
conclusions générales des trois tomes	
485
Rose Nikolas, 2007, The Politics of Life Itself : Biomedicine, Power, and Subjectivity in 
the Twenty-First Century, Princeton (NJ), Princeton University Press.
Secord James A., 1986, « The Geological Survey of Great Britain as a Research School 
(1839-1855) », History of Science, vol. 24, p. 223-275.
Spary Emma C., 2005 [1re éd. en anglais 2000], Le Jardin de l’utopie. L’histoire naturelle 
en France entre Ancien Régime et Révolution, Paris, Éd. du Muséum national d’his-
toire naturelle.
Supiot Alain, 2010, L’Esprit de Philadelphie. La justice sociale face au marché total, 
Paris, Seuil.
Van Damme Stéphane, 2014, À toutes voiles vers la vérité. Une autre histoire de la 
philosophie au temps des Lumières, Paris, Seuil.


Index


A
Académie chinoise des sciences, 444, 
446.
Adorno, Theodor W., 200, 208.
AGI, Année géophysique internationale, 
403, 407, 408.
Alberti, Leon Battista, 462.
Alma Ata, 134.
Amis de la Terre, 386.
Anderson, Philip, 332.
Arrow, Kenneth, 23, 242, 243.
ASE, Agence spatiale européenne, 77, 
137.
ASTRA, Application de la science et de 
la technologie aux espaces ruraux, 
Inde, 116.
AT&T, American Telegraph & Telephone, 
97, 128, 426, 428, 429, 431.
Auschwitz-Birkenau, 53.
B
Banque mondiale, 24, 119, 133, 142, 227, 
242, 248, 349, 414, 469, 471.
Barabási, Albert-László, 314, 315.
Barlow (comité), 32, 37.
Bateson, William, 310, 311.
Baudrillard, Jean, 305, 315.
Bayer, 87-89, 94, 102, 128, 426.
Bayh-Dole Act, 14, 99, 100, 102, 104, 436.
Beadle, George W., 311.
Becker, Gary, 247, 249, 250.
Becquerel, Henri, 167, 173.
Bell Telephone Laboratories, 13, 87, 104, 
109, 122, 128, 423, 425, 426, 428, 431, 
433, 438, 439, 466.
Bethe, Hans, 63, 65, 325, 505.
Bhopal, 139, 161, 390.
Biogen, 39.
Bloch, Marc, 55, 198, 200.
Boas, Franz, 261, 271.
Bohr, Niels, 63, 167, 174, 321.
Boltzmann, Ludwig, 340, 356.
Born, Max, 321.
Bose, Subhas Chandra, 112.
Braudel, Fernand, 200, 202, 207, 208.
Bretton Woods, 133, 243, 245.
Bridges, Calvin B., 308.
British Ecological Society, 278.
Broglie, Louis de, 321.
Bruck, Carl, 256, 257, 270.
Brundtland (rapport), 114, 119, 406.
Buitenzorg (jardin botanique de), 277.
Bush, Vannevar, 31, 32, 36, 42, 51, 324, 
343.
C
Cai Yuanpei, 444, 458.
Cannon, Walter, 149.
Carlsberg (brasserie), 301, 306.

490	
index
Carnegie (fondation), 196, 270.
Carson, Rachel, 154-156, 163, 164, 385, 
396.
Cattell, James, 34, 42.
CCNUCC, Convention-cadre des 
Nations unies sur les changements 
climatiques, 399, 415.
Celera, 96.
Chipko (mouvement), 115.
Churchill, Winston, 67.
Clements, Frederic, 280-282, 293, 294.
Club de Rome, 345, 349, 352, 400, 411.
CMD, Centres mondiaux de données, 
407.
CNR, Consiglio Nazionale delle Ricerche, 
12, 127.
CNRS, Centre national de la recherche 
scientifique, 127, 418, 484, 501, 502.
Codex Alimentarius, 88, 389.
Collins, Francis, 40, 96, 473.
Coolidge, William, 427.
Coomaraswamy, Ananda Kentish, 109, 
118, 122.
Cowles, Alfred, 240, 348.
Crick, Francis, 298, 312.
Croce, Benedetto, 191.
Curzon, Lord, 108.
D
DARPA, Defense Advanced Research 
Projects Agency, 430, 435.
Darwin, Charles, 28, 212, 229, 259-260, 
303, 305, 309, 315, 316.
Davenport, Charles B., 260, 270, 313.
Davos (forum de), 471.
Dawkins, Richard, 314, 316.
Debreu, Gérard, 23, 242, 243.
De Vries, Hugo, 299, 301, 304, 307-310, 
313, 316, 317.
Dirac, Paul, 321, 323, 325.
Dobzhansky, Theodosius, 261, 262, 265, 
270.
Dow Jones Industrial Average, 235.
DSIR, Department of Scientific and 
Industrial Research, 12, 127.
Dunn, Leslie Clarence, 261, 262, 263.
DuPont, 87, 103, 152, 407, 426, 429, 439.
Durkheim, Émile, 22, 189, 191, 194, 197, 
199, 202.
Dyson, Freeman, 325, 326.
E
Eames, Charles et Ray, 171, 172, 184.
Einstein, Albert, 41, 44, 61, 62, 321, 322, 
341.
Eisenhower, Dwight D., 29, 34, 42, 61, 
180.
Elias, Norbert, 198.
Elton, Charles, 279.
English, Don (Donald Ernest), 167.
ENIAC, Electronic Numerical Integrator 
Analyser and Computer, 343, 410.
EPA, Environment Protection Agency, 
13, 386, 387, 396.
ESA, Ecological Society of America, 
287, 288.
Eugenics Record Office, 260.
F
FDA, Food and Drug Administration, 
92, 97, 154, 155, 160, 387.
Febvre, Lucien, 200.
Feynman, Richard P., 325, 326, 335, 505.
Fisher, Ronald, 238, 342, 347.
Fleck, Ludwig, 253, 270.
Ford (fondation), 196, 242, 308.
Foucault, Michel, 30, 42, 203, 204, 208, 
211-229, 230, 250.
Frascati (Manuel de), 32, 43, 129.
Free Software Foundation, 437.
Freud, Sigmund, 199.
Friedman, Milton, 198, 243, 245.
Friedrich, Ernst, 275.
Fukushima, 16, 390, 394.

	
index	
491
G
Gagarine, Youri, 77.
Galbraith, John Kenneth, 237.
Galton, Francis, 42, 303, 309, 316.
Gandhi, Indira, 114.
Gandhi, Mohandas, 108-110, 117, 122.
Gates, Bill, 96.
Geddes, Patrick, 110, 111, 122, 123.
Geertz, Clifford, 204, 208.
Geigy, 93, 94.
Gell-Mann, Murray, 326.
Genentech, 39, 43.
General Electric, 87, 301, 426, 427.
George, David Lloyd, 54.
GIEC, Groupe d’experts intergouver-
nemental sur l’évolution du climat, 
352-354, 399, 412, 413, 415, 416.
Gödel, Kurt, 341.
Goffman, Erwing, 203.
Gore, Al (Albert Arnold Gore, dit), 399, 
412.
Greenpeace, 134, 386, 412.
H
Haavelmo, Trygve, 348, 356.
Haeckel, Ernst, 276.
Halbwachs, Maurice, 198.
Hamilton, Alice, 147, 148, 163.
HapMap Project, 268.
Hauser, Henri, 189, 209.
Hawking, Stephen, 41, 44.
Hays, Willet M., 157, 163, 307, 385, 396.
Heisenberg, Werner, 321, 329.
Helmholtz, Hermann von, 49, 127.
Heritage Foundation, 16, 132, 141, 142.
Hiernaux, Jean, 266, 271.
Higgs, Peter, 167, 319, 327-329, 334.
Hilbert, David, 341.
Hippocrate, 145.
Hiroshima, 41, 50, 63, 179, 183, 324.
Hofmann, Wilhelm August von, 170.
Holling, Crawford S., 289, 290, 293.
Hollywood, 179.
Hueper, Wilhelm, 152, 154, 163, 164.
Human Genome Diversity Project, 263, 
268.
I
Index, 451.
Isaac, Jules, 61, 64, 197, 209.
J
Jacob, François, 297, 301, 312, 316.
Johannsen, Wilhelm, 299, 304-306, 308, 
309, 315, 316.
Johnson, Lyndon, 33, 287, 418.
Joliot-Curie, Frédéric, 62, 64, 167.
Jünger, Ernst, 47.
K
Kaiser-Wilhelm Gesellschaft, 89, 127, 
467.
Kaiser-Wilhelm Institut, 55, 301.
Kennedy, John Fitzgerald, 408.
Keynes, John Maynard, 114, 198, 236, 
240, 243, 251, 347, 356.
Khrouchtchev, Nikita, 77.
Kodak, 87, 175, 426.
KSSP, Kerala Sastra Sahitya Parishad, 
116.
Kuhn, Thomas, 19, 22, 34, 43, 204, 209.
Kuznets, Simon, 239.
L
Langevin, Paul, 61.
Langmuir, Irving, 404, 427.
Laue, Max von, 174.
Lazarsfeld, Paul, 198.
Levi, Primo, 63, 64.
Lévi-Strauss, Claude, 22, 202, 208, 216, 
218, 230.
Levitt, Steven, 247.

492	
index
Lewontin, Richard, 265, 267, 271.
Livingstone, Frank, 265, 266, 272.
Los Alamos, 29.
Lotka, Alfred James, 342.
Love Canal, 160.
Lucas, Robert, 246, 251.
M
Mallon, Mary, 361, 362.
Mandelbrot, Benoît, 350.
Manhattan (projet), 47, 49, 52, 58, 62, 
63, 167, 176, 183, 324.
Mao Tsé-toung, 447, 448.
Marcuse, Herbert, 203, 262.
Marshall, Alfred, 233, 236, 237, 250, 251.
Marsh, George P., 275, 294.
Marx, Karl, 22, 199.
Mather, Kenneth, 253, 265, 272.
Mauss, Marcel, 198.
Mayr, Ernst, 312, 316.
McNamara, Robert, 57.
Mendel, Gregor, 259, 260, 297-299, 309, 
310, 315, 317.
Merck, 89, 90, 97, 103.
Merton, Robert, 29, 42, 198, 349, 350.
Mézières (école du génie de), 464.
Mills, Charles Wright, 203, 327, 329, 337.
Monod, Jacques, 297, 301, 312, 313, 316.
Montagu, Ashley, 265, 271, 272.
Montréal (protocole de), 65, 138, 400, 
406.
Morgan, Thomas H., 298, 306, 308, 341, 
348.
N
Nagasaki, 50, 179, 324.
NASA, 83.
Nasdaq, 99, 469, 483.
National Environmental Policy Act, 
288, 386.
Natural Capital Project, 292.
Nehru, Jawaharlal, 107, 113, 114.
Nernst, Walther, 55.
Neumann, John von, 60, 343, 344, 405, 
417, 420.
New Deal, 241, 279, 282, 343.
Newton, Isaac, 41, 463.
NIH, National Institute of Health, 40, 
96, 387.
Nkrumah, Kwame, 107.
NRC, National Research Council, 51, 
387, 388, 396.
NSF, National Science Foundation, 32, 
34, 37, 44, 387, 430, 434.
NTRS, National Technology Roadmap 
for Semiconductors, 436.
O
Oak Ridge (laboratoire national d’), 29, 
180, 285, 293.
OCDE, Organisation de coopération et 
de développement économiques, 32, 
37, 129, 133, 386, 387, 389.
Odum, Eugen P., 275, 284, 285, 294.
Oliver, Thomas, 147, 148, 164.
OMI, Organisation météorologique 
internationale, 401, 405.
OMM, Organisation météorologique 
mondiale, 405, 406, 408, 409, 412.
OMS, Organisation mondiale de la santé, 
133, 142, 263, 272, 387, 471.
Oppenheimer, Robert, 28, 29, 30, 44, 
63, 65, 505.
OSRD, Office of Scientific Research and 
Development, 32, 36.
P
Painlevé, Paul, 47.
Pareto, Vilfredo, 197, 198, 350.
Parsons, Talcott, 197, 202, 209.
Pasteur, Louis, 102, 145, 301, 316, 474.
Pauling, Linus, 63, 170, 174.

	
index	
493
Pearl Harbor, 33.
PPST, groupe pour la Science et la 
technologie de la patrie et du peuple, 
Inde, 116.
Price, Derek John de Solla, 33-35, 44.
Programme biologique international, 
286.
R
RAINS (modèle), 356.
RAND Corporation, 129.
Reagan, Ronald, 56, 79, 161, 346, 411, 
434.
Reclus, Élisée, 275, 294.
REDD, Réduction des émissions liées 
au déboisement et à la dégradation 
des forêts dans les pays en dévelop-
pement, 471.
Rio (conférence de), 311, 346, 413, 418.
Rockefeller (fondation), 42, 196, 262, 
301, 316.
Roosevelt, Franklin D., 11, 31, 51, 283, 
429.
Rorty, Richard, 204.
Ross, Edward, 196, 209, 238, 251, 385, 
397.
Rotblat, Józef, 63.
Russell, Bertrand, 61, 63, 196, 341, 453, 
454, 458.
Rutherford, Ernest, 30, 251, 324.
S
SAGE, 432.
Saha, Meghnad, 112, 113.
Sakharov, Andreï, 63.
Salam, Abdul, 327.
Samuelson, Paul, 198, 243.
Sanger, Margaret, 364, 371.
Sanghvi, L.D., 263, 265, 268, 272.
Santa Fe Institute, 288, 315.
Schering, 89, 90, 93, 102-104.
Schiemann, Elisabeth, 260, 261, 272.
Schrödinger, Erwin, 313, 321, 333.
Schumacher, Fritz, 114, 122.
Servier, 85, 86.
Shanghai (classement de), 131.
Sherman (loi), 426, 429.
Shiva, Vandana, 115, 121, 123.
Shockley, William, 431.
Shull, George H., 306.
Sierra Club, 412.
SIG, Systèmes d’informations 
géographiques, 291, 292.
Silicon Valley, 12, 138, 142, 432, 433, 
435, 438, 439, 476, 503.
Simiand, François, 193.
Simon, Herbert, 198.
Skinner, Burrhus F., 189, 195, 206.
Smith, Adam, 42, 43, 87, 103, 163, 233, 
426, 427, 431, 439.
Smuts, Jan C., 278.
Snow, Charles P., 30, 35, 44.
Starr, Frederick, 275, 294.
Steelman, John R., 32-36, 44.
Steinbeck, John, 283.
Stern (rapport), 353, 416, 421.
Stockholm (conférence de), 250, 345, 
406.
Sturtevant, Alfred H., 297, 308, 317.
Svalöf (Station de sélection végétale 
de), 308.
Svedberg, Theodor, 301.
Szilard, Leo, 62.
T
Tagore, Rabindranath, 110, 117, 123.
Tarde, Gabriel, 189.
Tarski, Alfred, 341.
Tatum, Edward L., 311.
Tchernobyl, 16, 138, 139, 167, 175, 224, 
390, 391.
Teisserenc de Bort, Léon, 401.
Tela Botanica, 136, 137.
Thatcher, Margaret, 79, 82, 346, 412.

494	
index
TIP, Traité d’interdiction partielle des 
essais nucléaires, 404.
Tomonaga, Sin-Itiro, 325.
Truman, Harry S., 32, 62, 107, 183, 324.
U
USDA, US Department of Agriculture, 
301, 307.
V
Van der Pol, Balthasar, 342.
Varga (institut), 236.
Vavilov, Nikolaï, 120, 261, 273, 311, 317.
Venter, John Craig, 27, 40, 41, 44, 45, 96.
Vidal de La Blache, Paul, 18, 191.
Vienne (cercle de), 19, 341, 343, 356.
Vienne (Convention de), 406, 413.
Vilmorin, Philippe de, 304.
Volker, Paul, 103.
Volterra, Vito, 342.
W
Wallace, Alfred, 108, 111, 120, 123.
Walras, Léon, 235, 237, 243.
Warming, Eugenius, 142, 278, 295, 356, 
418, 419, 421, 502.
Watson, James D., 297, 298.
Watson, John B., 195, 209.
Watt, Keneth, 286, 295.
Weaver, Warren, 60, 301.
Webber, Herbert J., 306, 307.
Weber, Max, 22, 29, 194, 197, 199.
Weinberg, Steven, 30, 34, 45, 319, 327, 
328, 330.
Weismann, August, 259, 304, 305, 309, 
310, 317.
Weissenberg, Samuel, 262, 273.
Wells, Herbert George, 50.
Whitehead, Alfred North, 341.
Wiener, Norbert, 284, 343, 344.
Wiley, Harvey, 42, 153, 164, 251, 420.
Wilson, Kenneth, 51, 330.
Wittgenstein, Ludwig, 204.
WWF, World Wildlife Fund, 134, 291.

 
Table


Introduction au tome 3 :
Le siècle des technosciences (depuis 1914)
par Christophe Bonneuil et Dominique Pestre. .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . 	
9
première partie
Sciences, économies, sociétés
	 1.	Figures de scientifiques
par Steven Shapin. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
27
	2.	Sciences et guerres
par Anne Rasmussen. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
47
	 3.	L’État entrepreneur de science
par David Edgerton. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
67
	4.	Une manière industrielle de savoir
par Jean-Paul Gaudillière. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . 	
85
	 5.	Sciences et savoirs dans l’État développementiste
par Shiv Visvanathan. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
107
	6.	Les savoirs du social
par Dominique Pestre. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
125
	 7.	Un siècle toxique.  
L’émergence de la « santé environnementale »
par Linda Nash. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
145
	8.	Le siècle de l’atome en images
par Charlotte Bigg. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
167

deuxième partie
Champs de sciences
	9.	L’avènement des sciences sociales
par Jacques Revel. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
189
	10.	Foucault et les transformations du biopouvoir
par Sarah Franklin. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
211
	11.	Les savoirs de l’économie
par Timothy Shenk et Timothy Mitchell. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . 	
233
	12.	Les savoirs de la diversité humaine
par Veronika Lipphardt. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
253
	13.	L’écologie. Connaître et gouverner la nature
par Yannick Mahrane. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
275
	14.	Le siècle du gène
par Christophe Bonneuil. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . 	
297
	15.	Les théories fondamentales de la matière
par Silvan S. Schweber et Jean-Marc Lévy-Leblond. .  .  .  .  .  .  .  .  .  . . . . . . . . . . 	
319
	16.	Modèles. De la représentation à l’action
par Michel Armatte et Amy Dahan. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . 	
339
troisième partie
Les sciences  
et le gouvernement du monde
	17.	Genre, corps et biomédecine
par Delphine Gardey . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
361
	18.	Gouverner un monde contaminé. 
	 	 Les risques techniques, sanitaires et environnementaux
par Soraya Boudia et Nathalie Jas. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . 	
381
	19.	Gouverner le système Terre
par Paul N. Edwards. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	 399
	20. Manager l’innovation
par Christophe Lécuyer. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
423

	21.	Chine : la fabrication d’une superpuissance 
technoscientifique
par Cong Cao. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
441
Conclusions générales des trois tomes : 
Savoirs et sciences de la Renaissance à nos jours. 
Une lecture de longue durée
par Dominique Pestre. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
461
Index. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
487
Les auteurs. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	
501
crédits photographiques et sources. .  .  .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . .	
507


Les auteurs
Michel Armatte
Statisticien et économiste, maître de conférences à l’université Paris- 
Dauphine et chercheur au Centre Alexandre-Koyré. Il travaille sur l’histoire 
et la préhistoire de l’économétrie et s’intéresse depuis une dizaine d’années 
aux modèles et scénarios du changement climatique. Il a notamment écrit 
La Science économique comme ingénierie. Quantification et modélisation 
(Presses des Mines, 2010).
Charlotte Bigg
Chargée de recherche au CNRS, au Centre Alexandre-Koyré. Ses travaux 
portent sur la circulation des savoirs et des pratiques entre milieux scienti-
fiques, industriels et espaces publics aux xixe et xxe siècles. Elle a notamment 
publié Atombilder. Ikonografie des Atoms in Wissenschaft und Öffentlichkeit 
des 20. Jahrhunderts (Wallstein, 2009).
Christophe Bonneuil
Chargé de recherche au CNRS, au Centre Alexandre-Koyré. Il s’intéresse aux 
transformations conjointes des savoirs biologiques et des formes de gouver-
nement de la nature et de l’agriculture. Il a notamment publié Gènes, pouvoirs 
et profits (Quæ, 2009, avec Frédéric Thomas), Sciences, techniques et société 
(La Découverte, 2013), et L’Événement Anthropocène. La Terre, l’histoire et 
nous (Seuil, 2013, avec Jean-Baptiste Fressoz).
Soraya Boudia
Historienne et sociologue des sciences et des techniques, professeure à l’uni-
versité Paris Descartes. Ses travaux portent sur la thématique science et 
politique, notamment sur les transformations du gouvernement des risques 
techniques, sanitaires et environnementaux. Elle a publié Toxicants, Health 

and Regulation since 1945 (Pickering & Chatto, 2013, avec Nathalie Jas) et 
dirigé Powerless Science ? Science and Politics in a Toxic World (Berghahn 
Books, 2014, avec Nathalie Jas).
Cong Cao
Sociologue, professeur à la School of Contemporary Chinese Studies de 
l’université de Nottingham. Il est spécialiste du système des relations entre 
science, innovation, État et industrie en Chine et auteur de China’s Scientific 
Elite (RoutledgeCurzon, 2004), et de China’s Emerging Technological Edge : 
Assessing the Role of High-End Talent (Cambridge University Press, 2009, 
avec Denis Fred Simon)
Amy Dahan
Directrice de recherche émérite au CNRS, au Centre Alexandre-Koyré. Mathé-
maticienne et historienne des sciences, ses travaux se sont portés depuis 
quinze ans sur les sciences, l’expertise et la gouvernance du changement 
climatique. Elle est notamment l’auteure de Les Sciences pour la guerre (1940-
1960) (Éd. de l’EHESS, 2004, avec Dominique Pestre), Les Modèles du futur 
(La Découverte, 2007) et de Gouverner le climat ? 20 ans de négociations 
climatiques (Presses de Sciences Po, 2015, avec Stefan Aykut).
David Edgerton
Professeur d’histoire des sciences et des techniques au King’s College 
de Londres où il dirige le Centre for the History of Science, Technology 
and Medicine. Il a notamment publié Warfare State : Britain (1920-1970) 
(Cambridge University Press, 2005), Britain’s War Machine : Weapons, 
Resources and Experts in the Second World War (Allen Lane, 2011) et Quoi 
de neuf ? Du rôle des techniques dans l’histoire globale (Seuil, 2013).
Paul N. Edwards
Professeur à la School of Information et au département d’histoire de l’uni-
versité du Michigan. Il explore l’histoire, la politique et les aspects culturels 
des ordinateurs, des infrastructures d’information et communication et  
les sciences du climat global. Il a écrit The Closed World : Computers and 
the Politics of Discourse in Cold War America (MIT Press, 1997) et A Vast 
Machine : Computer Models, Climate Data, and the Politics of Global Warming 
(MIT Press, 2010).

Sarah Franklin
Professeure de sociologie à l’université de Cambridge. Elle explore depuis 
vingt ans les enjeux de genre et l’anthropologie de la technologie de la 
procréation assistée. Elle a publié Embodied Progress : A Cultural Account 
of Assisted Conception (Routledge, 1997), Dolly Mixtures : The Remaking of 
Genealogy (Duke University Press, 2007), et Biological Relatives : IVF, Stem 
Cells, and the Future of Kinship (Duke University Press, 2013).
Delphine Gardey
Professeure d’histoire à l’université de Genève et directrice des Études genre. 
Ses travaux portent sur l’histoire du travail, des techniques et des relations 
de genre. Elle a écrit Écrire, calculer, classer. Comment une révolution de 
papier a transformé les sociétés contemporaines (1800-1940) (La Découverte,  
2008), Le féminisme change-t-il nos vies ? (Textuel, 2011) et Le Linge du 
Palais-Bourbon. Corps, matérialité et genre du politique à l’ère démocratique 
(Le Bord de l’Eau, 2015).
Jean-Paul Gaudillière
Directeur de recherche à l’Inserm et directeur d’études à l’École des hautes 
études en sciences sociales. Il dirige le Cermes3. Spécialiste de l’histoire des 
sciences, et des politiques et industries biomédicales, il a publié Inventer la 
biomédecine. La France, l’Amérique et la production des savoirs du vivant 
après 1945 (La Découverte, 2002) et La Médecine et les sciences (xixe-xxe siècle) 
(La Découverte, 2006).
Nathalie Jas
Chargée de recherche à l’INRA et directrice de l’unité de recherche RiTME. 
Ses travaux portent sur la gestion des problèmes de santé publique posés par 
les pesticides et les productions chimiques, avec une attention particulière 
pour la période 1945-2010. Elle a publié Toxicants, Health and Regulation 
since 1945 (Pickering & Chatto, 2013, avec Soraya Boudia) et dirigé Powerless 
Science ? Science and Politics in a Toxic World (Berghahn Books, 2014, avec 
Soraya Boudia).
Christophe Lécuyer
Professeur d’histoire des sciences et des techniques à l’université Pierre-et-
Marie-Curie à Paris. Il travaille sur l’histoire de l’innovation, de l’informatique, 
des nouvelles technologies et des relations industrie-universités. Il a publié 
Making Silicon Valley : Innovation and the Growth of High Tech (1930-1970) 

(MIT Press, 2006) et Makers of the Microchip : A Documentary History of 
Fairchild Semiconductor (MIT Press, 2010, avec David C. Brock).
Jean-Marc Lévy-Leblond
Physicien, épistémologue et essayiste, professeur émérite de l’université 
de Nice. Auteur de plusieurs ouvrages de réflexion critique sur la science 
contemporaine, il dirige la collection « Science ouverte » au Seuil, ainsi que 
la revue Alliage (culture, science, technique).
Veronika Lipphardt
Professeure d’étude des sciences et des technologies à l’université de Fribourg-
en-Brisgau et associée à l’Institut Max-Planck pour l’histoire des sciences. 
Spécialiste des savoirs sur les « races » et la diversité humaine, elle est l’auteure 
de Biologie der Juden. Jüdische Wissenschaftler über « Rasse » und Vererbung 
(1900-1935) (Vandenhoeck & Ruprecht, 2008).
Yannick Mahrane
Doctorant en histoire des sciences à l’École des hautes études en sciences 
sociales, au Centre Alexandre-Koyré. Ses recherches explorent l’histoire 
de l’écologie et de la protection de la nature, de l’âge des empires à celui du 
développement durable.
Timothy Mitchell
Professeur d’études moyen-orientales à l’université Columbia. Politologue  
et spécialiste de l’histoire de l’Égypte et du Moyen-Orient au xixe-xxe siècle. 
Ces dernières années, il a exploré les bases énergétiques des arrangements 
politiques et l’histoire des savoirs économiques. Il a écrit Rule of Experts : 
Egypt, Techno-Politics, Modernity (University of California Press, 2002) et 
Carbon Democracy. Le pouvoir politique à l’ère du pétrole (La Découverte, 
2013).
Linda Nash
Professeure d’histoire à l’université de Washington où elle dirige le Center 
for the Study of the Pacific Northwest. Ses travaux portent sur l’histoire 
de la médecine et de l’environnement. Elle est notamment l’auteure d’Ines-
capable Ecologies : A History of Environment, Disease, and Knowledge (University 
of California Press, 2006).

Dominique Pestre
Directeur d’études à l’École des hautes études en sciences sociales, au Centre 
Alexandre-Koyré. Après avoir travaillé sur l’histoire de la physique et les 
relations entre sciences et guerre, il s’intéresse aux transformations des 
régimes de savoir et à une réflexion historiographique et théorique sur les 
sciences en société. Il est l’auteur de Heinrich Hertz. L’administration de la 
preuve (PUF, 2002, avec Michel Atten), Science, argent et politique (Quæ, 
2003) et À contre-science (Seuil, 2013).
Anne Rasmussen
Maître de conférences en histoire des sciences à l’université de Strasbourg. 
Spécialiste de l’internationalisation des sciences, de la médecine et des 
relations entre sciences et guerres, elle a notamment dirigé Vrai et faux 
dans la Grande Guerre (La Découverte, 2004, avec Christophe Prochasson), 
Histoire et médicament aux xixe et xxe siècles (Glyphe, 2005, avec Christian 
Bonah) et Dans la guerre 1914-1918. Accepter, endurer, refuser (Les Belles 
Lettres, 2015 avec Nicolas Beaupré et Heather Jones).
Jacques Revel
Historien, directeur d’études à l’École des hautes études en sciences sociales 
et professeur émérite à New York University. Il a notamment dirigé Jeux 
d’échelle (Seuil-Gallimard, 1996), Penser par cas (Éd. de l’EHESS, 2005, avec 
Jean-Claude Passeron), et publié Un parcours critique (Galaade, 2006).
Silvan S. Schweber
Professeur émérite de physique et Richard Koret Professor en histoire des idées à 
l’université Brandeis. Il travaille sur la théorie quantique de la mesure, la théorie 
des champs, la visualisation des processus élémentaires. Il a publié QED and the 
Men Who Made It : Dyson, Feynman, Schwinger, and Tomonaga (Princeton 
University Press, 1994), In the Shadow of the Bomb : Oppenheimer, Bethe, 
and the Moral Responsibility of the Scientist (Princeton University Press, 
2000) et Nuclear Forces : The Making of the Physicist Hans Bethe (Harvard 
University Press, 2012).
Steven Shapin
Professeur d’histoire des sciences à Harvard. Il est notamment l’auteur 
de Léviathan et la pompe à air. Hobbes et Boyle entre science et politique 
(La Découverte, 1993, avec Simon Schaffer), de Science Incarnate : Historical 
Embodiments of Natural Knowledge (University of Chicago Press, 1998, avec 

Christopher Lawrence), de La Révolution scientifique (Flammarion, 1998) 
et de The Scientific Life : A Moral History of a Late Modern Vocation (University 
of Chicago Press, 2008).
Timothy Shenk
Doctorant en histoire à l’université Columbia. Il prépare une thèse sur 
Inventing the American Economy (1917-1981) qui explore l’émergence de 
l’économie conjointement comme objet de savoir économique et comme 
objet d’intervention au xxe siècle. Il est l’auteur de Maurice Dobb : Political 
Economist (Palgrave Macmillan, 2013).
Shiv Visvanathan
Professeur à l’O.P. Jindal Global University, Sonipat (Inde). Explorant les 
relations entre sciences, savoirs et État développementaliste et les questions 
de pluralité des savoirs et de justice cognitive, il est l’auteur d’Organizing for 
Science (Oxford University Press, 1985) et de A Carnival for Science (Oxford 
University Press, 1997).

Crédits photographiques et sources
Page 8 : L. Berkhouse, S.E. Davis, F.R. Gladeck, J.H. Hallowell, C.B. Jones, 
E.J. Martin, F.W. McMullan, M.J. Osborne, «Operation Greenhouse : 
1951, United States Atmospheric Nuclear Weapons Tests Nuclear Test 
Personnel Review, Washington, DC, Defense Nuclear Agency, 1983. 
© Cliché nr. 342_FH-3B-44430-45903AC, US National Archives and 
Records Administration.
Page 26 : En haut Keystone ; en bas © Getty images/The Age.
Page 46 : © Paris-Musée de l’Armée, Dist. RMN-Grand Palais, Paris/
Anne-­Sylvaine Marre-Noël.
Page 66 : Library of Congress Prints and Photographs Division 
Washington, D.C. 20540 USA.
Pages 84 : © Novartis (archives Geigy).
Pages 106 : © Getty images/Dinodia Photos.
Page 124 : © Diybio.org.
Page 136 : © ESA-C. Carreau.
Page 144 : © Los Angeles Public Library.
Page 166 : © Las Vegas News Bureau.
Page 170 : Linus Pauling et Roger Hayward, The Architecture of Molecules, 
San Francisco (CA), W.H. Freeman & Co., 1964. © Coll. part.
Page 172 : D.M. Eigler et E.K. Schweizer, « Positioning Single Atoms 
with a Scanning Tunnelling Microscope », Nature, vol. 344, 5 avril 1990, 
p. 524-526. © Coll. part.
Page 174 : Max von Laue, « Les phénomènes d’interférences des rayons de 
Röntgen produits par le réseau tridimensionnel des cristaux », La Structure 
de la matière. Rapports et discussions du conseil de physique tenu à Bruxelles 
du 27 au 31 octobre 1913 sous les auspices de l’Institut international de 
physique Solvay, Paris, Gauthier-Villars, 1921, p. 75-112. © Coll. part.
Page 178 : US National Archives and Records Administration.
Pages 188 : © BSIP/Sam Falk.

Page 201: En haut : Gérard Delille, Le Maire et le Prieur. Pouvoir central 
et pouvoir local en Méditerranée occidentale (xve-xviiie siècle), Paris, 
EHESS, 2003, p. 329. En bas, Alain Cottereau, Mokhtar Mohatar Marzok, 
Une famille andalouse. Ethnocomptabilité d’une économie invisible, Paris, 
Bouchène, 2011, p. 297.
Page 210 : © Laurent Valère, Memorial Cap 110, Anse Caffard. © Adagp, 
Paris, 2015.
Page 221 : © National Museums Scotland, Édimbourg.
Page 232 : © Science Museum / Science & Society Picture Library, 
Londres.
Page 252 : Katrin von Lehmann, World Citizen (détail), 2013. © Adagp, 
Paris, 2015.
Page 274 : Avec l’aimable autorisation de Hargrett Rare Book and 
Manuscript Library / University of Georgia Libraries.
Page 282 : F.E. Clements et G.W. Goldsmith, The Phytometer Method 
in Ecology, Carnegie Institution of Washington, 1920. © Coll. part.
Page 296 : « Sensational study of heredity may produce a new race of 
man », dans Popular Science, novembre 1934. © Coll. part.
Page 318 : © CERN.
Page 338 : D’après Earth System Science Overview: A Program for Global 
Change (NASA Science Advisory Committee, 1986, p. 19).
Page 360 : © Bettmann/CORBIS.
Page 380 : © The BIT ROT Project / Valentino Bellini.
Page 398 : © NASA.
Page 422 : © Coll. part.
Page 440 : © Coll. part., avec l’aimable autorisation de Jianli Kan.
Page 459 : À gauche, aérostat, 1783 © Rue des Archives/PVDE. Au 
milieu : Crystal Palace, 1851 © Bridgeman Art Library. À droite, Operation 
Greenhouse, 1951 © NARA.

réalisation : pao éditions du seuil
impression : normandie roto impression s.a.s. à lonrai
dépôt légal : octobre 2015. n° 107678 (       )
imprimé en france

LES CONTRIBUTIO S 
BOUDIA, c. CAO, A. DA HAN, D. EDGERTON, P.N. E 
P. GAUDILLIÈRE, N. JAS, c. LÉCUYER, J 
: . 
v. LIPPHARDT, Y. M 
if. M ITCHELL, L. NASH, D. 
SCHWEBER, s. SHAI'IN, 
llllllllllllllllllllllllllllll 
9 782021 076783 

