Kensuke Sekihara
Srikantan S. Nagarajan
Electromagnetic 
Brain Imaging
A Bayesian Perspective

Electromagnetic Brain Imaging

Kensuke Sekihara
• Srikantan S. Nagarajan
Electromagnetic Brain
Imaging
A Bayesian Perspective
123

Kensuke Sekihara
Department of Systems Design
and Engineering
Tokyo Metropolitan University
Tokyo
Japan
Srikantan S. Nagarajan
Department of Radiology and Biomedical
University of California
San Francisco, CA
USA
ISBN 978-3-319-14946-2
ISBN 978-3-319-14947-9
(eBook)
DOI 10.1007/978-3-319-14947-9
Library of Congress Control Number: 2014959853
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained
herein or for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)

Preface
Neuronal activities in a human brain generate coherent synaptic and intracellular
currents in cortical columns, which generate electric potentials on the scalp surface
and magnetic ﬁelds outside the head. Electroencephalography (EEG) measures
these potentials and magnetoencephalography (MEG) measures these magnetic
ﬁelds to obtain information on the state of the brain. The class of methodologies
that reconstruct and visualize the neuronal activities based on MEG/EEG sensor
measurements is referred to as the electromagnetic brain imaging.
In the past two decades there have been signiﬁcant advances in signal processing
and source reconstruction algorithms used in electromagnetic brain imaging.
However, electromagnetic brain imaging can still be considered a young ﬁeld. This
is primarily due to the complexity associated with the electrophysiological brain
activity underlying the signals. Also, it is true that applying the electromagnetic
brain imaging with conﬁdence requires an understanding of the relevant mathe-
matics, physics, biology, and engineering as well as a broad perspective on human
brain science. Due to its interdisciplinary nature, such a broad knowledge base takes
years to acquire.
This book is intended to provide a coherent introduction to the body of main-
stream algorithms used in electromagnetic brain imaging, with speciﬁc emphasis on
novel Bayesian algorithms that we have developed. It is intended as a graduate level
textbook with the goal of helping readers more easily understand the literature in
biomedical engineering and in related ﬁelds, and be ready to pursue research in
either the engineering or the neuroscientiﬁc aspects of electromagnetic brain
imaging. We hope that this textbook will not only appeal to graduate students but
all scientists and engineers engaged in research on electromagnetic brain imaging.
This book begins with an introductory overview of electromagnetic brain
imaging in Chap. 1 and then discusses dominant algorithms that are used in elec-
tromagnetic brain imaging in Chaps. 2–4. Minimum-norm-based methods, which
are classic algorithms and still widely used in this ﬁeld, are described in Chap. 2,
but with a Bayesian perspective in mind. Chapter 3 presents a concise review of
adaptive beamformers, which have become a standard tool for analyzing brain
v

spontaneous activity such as resting-state MEG data, and we also include a
Bayesian perspective on adaptive beamformers.
Chapters 4–6 review Bayesian algorithms for electromagnetic brain imaging that
we have developed in the past decade. Chapter 4 presents a novel Bayesian algo-
rithm, called Champagne, which has been developed by our group. Since we
believe that the Champagne algorithm is a powerful next-generation imaging
algorithm, the derivation of the algorithm is presented in detail. Chapter 5 presents
Bayesian factor analysis, which is a group of versatile algorithms used for deno-
ising, interference suppression, and source localization. Chapter 6 describes a
uniﬁed theoretical Bayesian framework, which provides insightful perspective into
various source imaging methods and reveals similarities and equivalences between
methods that appear to be very different.
Chapters 7–9 deal with newer topics that are currently in vogue in electro-
magnetic brain imaging—functional connectivity, causality, and cross-frequency
coupling analyses. Chapter 7 reviews functional connectivity analysis using
imaginary coherence. Chapter 8 provides a review of several directional measures
that can detect causal coupling of brain activities. Chapter 9 presents novel
empirical results showing that the phase-amplitude coupling can be detected using
MEG source-space analysis, and demonstrates that the electromagnetic brain
imaging holds great potential in elucidating the mechanisms of brain information
processing. This chapter was contributed by Eiichi Okumura and Takashi Asakawa.
The ﬁrst two chapters in the Appendix provide concise explanations of bioelec-
tromagnetic forward modeling and basics of Bayesian inference, and the third
chapter provides supplementary mathematical arguments. These chapters are
included for the reader’s convenience.
Many people have made valuable contributions together with our own efforts in
this area. Special mention must be made of Hagai Attias who is an invaluable
collaborator. Hagai introduced us to probabilistic graphical models and Bayesian
inference methods. He was an integral person in our fruitful collaboration, on which
this book is based.
Many students, postdocs, fellows, and UCSF faculty members have collaborated
with us over the years. These include Leighton Hinkley, David Wipf, Johanna
Zumer, Julia Owen, Sarang Dalal, Isamu Kumihashi, Alex Herman, Naomi Kort,
Ethan Brown, Rodney Gabriel, Maneesh Sahani, Adrian Guggisberg, Juan Martino,
Kenneth Hild, Matthew Brookes, Carsten Stahlhut, Oleg Portniaguine, Erik
Edwards, Ryan Canolty, David McGonigle, Tal Kenet, Theda Heinks-Maldonado,
Aliu Sheye, Dameon Harrell, Josiah Ambrose, Sandeep Manyam, William McClay,
Virginie van Wassenhove, Ilana Hairston, Maria Ventura, Zhao Zhu, Corby Dale,
Tracy Luks, Kelly Westlake, Kamalini Ranasinghe, Karuna Subramanium, Carrie
Niziolek, Zarinah Agnew, Carly Demopoulos, Megan Thomson, Peter Lin, Tulaya
Limpiti, Lisa Dyson, Pew Puthividya, Jiucang Hao, Phiroz Tarapore, Dario Englot,
Heidi Kirsch, Sophia Vinogradov, Michael Merzenich, Christoph Schreiner, Eliza-
beth Disbrow, Mitchel Berger, Edward Chang, Nick Barbaro, Roland Henry, Sarah
Nelson, William Dillon, Jim Barkovich, Nancy Byl, David Blake, Keith Vossel,
Elliot Sherr, Elysa Marco, Josh Wooley, Marilu Gorno-Tempini, Robert Knight, and
vi
Preface

Pratik Mukherjee. Also, we are deeply indebted to the following fantastic and
incredibly dedicated staff at the Biomagnetic Imaging Laboratory: Susanne Honma,
Mary Mantle, Anne Findlay-Dowling, Danielle Mizuiri, and Garrett Coleman.
Furthermore, several people have been particularly inﬂuential, and have greatly
helped us to enhance our understanding of this ﬁeld. Such people include Thomas
Ferree, Mike X. Cohen, Stephen E. Robinson, Jiri Vrba, Guido Nolte, and Barry
Van Veen. We would also like to thank Daniel Palomo for his effort in editing the
manuscript of this book. Finally, we thank the following collaborators for their
friendship and invaluable support to our work over nearly 20 years: John Houde,
Steven Cheung, David Poeppel, Alec Marantz, and Tim Roberts.
We kindly ask readers to visit www.electromagneticbrainimaging.info. Sup-
plementary information, as well as error corrections (if necessary), is uploaded to
this website.
November 2014
Kensuke Sekihara
Srikantan S. Nagarajan
Preface
vii

Contents
1
Introduction to Electromagnetic Brain Imaging . . . . . . . . . . . . . .
1
1.1
Functional Brain Imaging and Bioelectromagnetic
Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Sensing Magnetic Fields from the Brain . . . . . . . . . . . . . . . .
2
1.3
Electromagnetic Brain Imaging. . . . . . . . . . . . . . . . . . . . . . .
3
1.3.1
Forward Model. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3.2
Inverse Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4
From Source Imaging to Functional Connectivity Imaging . . . .
6
1.5
Examples of Clinical Applications . . . . . . . . . . . . . . . . . . . .
6
1.5.1
Functional Mapping for Preoperative Neurosurgical
Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.5.2
Functional Connectivity Imaging . . . . . . . . . . . . . . .
7
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Minimum-Norm-Based Source Imaging Algorithms . . . . . . . . . . .
9
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2
Definitions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.3
Sensor Lead Field. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.4
Voxel Source Model and Tomographic Source
Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.5
Maximum Likelihood Principle and the Least-Squares
Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.6
Derivation of the Minimum-Norm Solution . . . . . . . . . . . . . .
14
2.7
Properties of the Minimum-Norm Solution. . . . . . . . . . . . . . .
15
2.8
L2-Regularized Minimum-Norm Solution. . . . . . . . . . . . . . . .
17
2.9
L1-Regularized Minimum-Norm Solution. . . . . . . . . . . . . . . .
19
2.9.1
L1-Norm Constraint. . . . . . . . . . . . . . . . . . . . . . . . .
19
2.9.2
Intuitive Explanation for Sparsity . . . . . . . . . . . . . . .
20
2.9.3
Problem with Source Orientation Estimation. . . . . . . .
23
ix

2.10
Bayesian Derivation of the Minimum-Norm Method . . . . . . . .
24
2.10.1
Prior Probability Distribution and Cost Function. . . . .
24
2.10.2
L2-Regularized Method . . . . . . . . . . . . . . . . . . . . . .
24
2.10.3
L1-Regularized Method . . . . . . . . . . . . . . . . . . . . . .
26
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3
Adaptive Beamformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.1
Introduction and Basic Formulation. . . . . . . . . . . . . . . . . . . .
29
3.2
Classical Derivation of Adaptive Beamformers . . . . . . . . . . . .
30
3.2.1
Minimum-Variance Beamformers with Unit-Gain
Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.2.2
Minimum-Variance Beamformer with Array-Gain
Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.2.3
Minimum-Variance Beamformer
with Unit-Noise-Gain Constraint. . . . . . . . . . . . . . . .
32
3.3
Semi-Bayesian Derivation of Adaptive Beamformers. . . . . . . .
33
3.4
Diagonal-Loading and Bayesian Beamformers . . . . . . . . . . . .
35
3.5
Scalar Adaptive Beamformer with Unknown
Source Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.5.1
Expressions for the Unit-Gain Constraint
Beamformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.5.2
Expressions for the Array-Gain
and Weight-Normalized Beamformers . . . . . . . . . . . .
37
3.6
Vector-Type Adaptive Beamformer . . . . . . . . . . . . . . . . . . . .
38
3.6.1
Vector Beamformer Formulation. . . . . . . . . . . . . . . .
38
3.6.2
Semi-Bayesian Formulation . . . . . . . . . . . . . . . . . . .
40
3.7
Narrow-Band Beamformer . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.7.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.7.2
Time-Domain Implementation . . . . . . . . . . . . . . . . .
42
3.7.3
Frequency-Domain Implementation . . . . . . . . . . . . . .
43
3.7.4
Five-Dimensional Brain Imaging. . . . . . . . . . . . . . . .
44
3.8
Nonadaptive Spatial Filters. . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.8.1
Minimum-Norm Filter . . . . . . . . . . . . . . . . . . . . . . .
44
3.8.2
Weight-Normalized Minimum-Norm Filter. . . . . . . . .
46
3.8.3
sLORETA Filter . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.9
Recursive Null-Steering (RENS) Beamformer. . . . . . . . . . . . .
47
3.9.1
Beamformer Obtained Based on Beam-Response
Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.9.2
Derivation of RENS Beamformer . . . . . . . . . . . . . . .
48
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
4
Sparse Bayesian (Champagne) Algorithm . . . . . . . . . . . . . . . . . .
51
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4.2
Probabilistic Model and Method Formulation . . . . . . . . . . . . .
52
x
Contents

4.3
Cost Function for Marginal Likelihood Maximization . . . . . . .
54
4.4
Update Equations for α . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.5
Modified Algorithm Integrating Interference Suppression . . . . .
58
4.6
Convexity-Based Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.6.1
Deriving an Alternative Cost Function. . . . . . . . . . . .
59
4.6.2
Update Equation for z . . . . . . . . . . . . . . . . . . . . . . .
61
4.6.3
Update Equation for xk . . . . . . . . . . . . . . . . . . . . . .
61
4.6.4
Update Equation for ν . . . . . . . . . . . . . . . . . . . . . . .
62
4.6.5
Summary of the Convexity-Based Algorithm . . . . . . .
62
4.7
The Origin of the Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.8
Extension to Include Source Vector Estimation. . . . . . . . . . . .
65
4.8.1
Update Equation for Zj . . . . . . . . . . . . . . . . . . . . . .
66
4.8.2
Update Equation for sjðtkÞ . . . . . . . . . . . . . . . . . . . .
67
4.8.3
Update Equation for ¤j . . . . . . . . . . . . . . . . . . . . . .
68
4.9
Source Vector Estimation Using Hyperparameter Tying. . . . . .
69
4.10
Appendix to This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.10.1
Derivation of Eq. (4.21). . . . . . . . . . . . . . . . . . . . . .
71
4.10.2
Derivation of Eq. (4.29). . . . . . . . . . . . . . . . . . . . . .
72
4.10.3
Proof of Eq. (4.50) . . . . . . . . . . . . . . . . . . . . . . . . .
73
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
5
Bayesian Factor Analysis: A Versatile Framework
for Denoising, Interference Suppression,
and Source Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.2
Bayesian Factor Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.2.1
Factor Analysis Model . . . . . . . . . . . . . . . . . . . . . .
75
5.2.2
Probability Model . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.2.3
EM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5.2.4
Computation of Marginal Likelihood. . . . . . . . . . . . .
79
5.2.5
Summary of the BFA Algorithm. . . . . . . . . . . . . . . .
81
5.3
Variational Bayes Factor Analysis (VBFA) . . . . . . . . . . . . . .
82
5.3.1
Prior Distribution for Mixing Matrix . . . . . . . . . . . . .
82
5.3.2
Variational Bayes EM Algorithm (VBEM). . . . . . . . .
84
5.3.3
Computation of Free Energy . . . . . . . . . . . . . . . . . .
92
5.3.4
Summary of the VBFA Algorithm . . . . . . . . . . . . . .
95
5.4
Partitioned Factor Analysis (PFA). . . . . . . . . . . . . . . . . . . . .
96
5.4.1
Factor Analysis Model . . . . . . . . . . . . . . . . . . . . . .
96
5.4.2
Probability Model . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.4.3
VBEM Algorithm for PFA. . . . . . . . . . . . . . . . . . . .
97
5.4.4
Summary of the PFA Algorithm . . . . . . . . . . . . . . . .
100
5.5
Saketini: Source Localization Algorithm Based
on the VBFA Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.5.1
Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
Contents
xi

5.5.2
Probability Model . . . . . . . . . . . . . . . . . . . . . . . . . .
102
5.5.3
VBEM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .
103
5.5.4
Summary of the Saketini Algorithm . . . . . . . . . . . . .
107
5.6
Numerical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.7
Appendix to This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . .
112
5.7.1
Proof of Eq. (5.84) . . . . . . . . . . . . . . . . . . . . . . . . .
112
5.7.2
Proof of Eq. (5.94) . . . . . . . . . . . . . . . . . . . . . . . . .
114
5.7.3
Proof of Eq. (5.103) . . . . . . . . . . . . . . . . . . . . . . . .
115
5.7.4
Proof of Eq. (5.166) . . . . . . . . . . . . . . . . . . . . . . . .
115
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
6
A Uniﬁed Bayesian Framework for MEG/EEG Source
Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6.2
Bayesian Modeling Framework. . . . . . . . . . . . . . . . . . . . . . .
121
6.3
Bayesian Modeling Using General Gaussian Scale
Mixtures and Arbitrary Covariance Components . . . . . . . . . . .
122
6.3.1
The Generative Model. . . . . . . . . . . . . . . . . . . . . . .
122
6.3.2
Estimation and Inference . . . . . . . . . . . . . . . . . . . . .
123
6.3.3
Source MAP or Penalized Likelihood Methods. . . . . .
127
6.3.4
Variational Bayesian Approximation . . . . . . . . . . . . .
131
6.4
Selection of Covariance Components C . . . . . . . . . . . . . . . . .
133
6.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
7
Source-Space Connectivity Analysis Using Imaginary
Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
7.2
Source-Space Coherence Imaging . . . . . . . . . . . . . . . . . . . . .
140
7.3
Real and Imaginary Parts of Coherence . . . . . . . . . . . . . . . . .
141
7.4
Effects of the Leakage. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
7.4.1
Leakage Effects on the Magnitude Coherence. . . . . . .
143
7.4.2
Leakage Effects on the Imaginary Coherence . . . . . . .
144
7.5
Corrected Imaginary Coherence . . . . . . . . . . . . . . . . . . . . . .
145
7.5.1
Modification of Imaginary Coherence . . . . . . . . . . . .
145
7.5.2
Factorization of Mutual Information . . . . . . . . . . . . .
146
7.5.3
Residual Coherence. . . . . . . . . . . . . . . . . . . . . . . . .
148
7.5.4
Phase Dependence of the Corrected Imaginary
Coherences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
7.6
Canonical Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
7.6.1
Canonical Magnitude Coherence. . . . . . . . . . . . . . . .
151
7.6.2
Canonical Imaginary Coherence . . . . . . . . . . . . . . . .
154
7.6.3
Canonical Residual Coherence . . . . . . . . . . . . . . . . .
157
xii
Contents

7.6.4
Computing Coherence When Each Voxel
has Multiple Time Courses . . . . . . . . . . . . . . . . . . .
158
7.7
Envelope Correlation and Related Connectivity Metrics . . . . . .
159
7.7.1
Envelope Correlation. . . . . . . . . . . . . . . . . . . . . . . .
159
7.7.2
Residual Envelope Correlation . . . . . . . . . . . . . . . . .
160
7.7.3
Envelope Coherence . . . . . . . . . . . . . . . . . . . . . . . .
160
7.8
Statistical Thresholding of Coherence Images . . . . . . . . . . . . .
161
7.9
Mean Imaginary Coherence (MIC) Mapping . . . . . . . . . . . . .
162
7.10
Numerical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
8
Estimation of Causal Networks: Source-Space Causality
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
8.2
Multivariate Vector Autoregressive (MVAR) Process . . . . . . .
171
8.2.1
MVAR Modeling of Time Series . . . . . . . . . . . . . . .
171
8.2.2
Coherence and Partial Coherence
of the MVAR Process . . . . . . . . . . . . . . . . . . . . . . .
173
8.3
Time-Domain Granger Causality. . . . . . . . . . . . . . . . . . . . . .
174
8.3.1
Granger Causality for a Bivariate Process . . . . . . . . .
174
8.3.2
Multivariate Granger Causality . . . . . . . . . . . . . . . . .
175
8.3.3
Total Interdependence . . . . . . . . . . . . . . . . . . . . . . .
177
8.4
Spectral Granger Causality: Geweke Measures . . . . . . . . . . . .
178
8.4.1
Basic Relationships in the Frequency Domain . . . . . .
178
8.4.2
Total Interdependence and Coherence . . . . . . . . . . . .
179
8.4.3
Deriving Causal Relationships in the Frequency
Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
8.5
Other MVAR-Modeling-Based Measures . . . . . . . . . . . . . . . .
182
8.5.1
Directed Transfer Function (DTF). . . . . . . . . . . . . . .
182
8.5.2
Relationship Between DTF and Coherence. . . . . . . . .
183
8.5.3
Partial Directed Coherence (PDC). . . . . . . . . . . . . . .
184
8.6
Transfer Entropy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
8.6.1
Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
8.6.2
Transfer Entropy Under Gaussianity Assumption . . . .
186
8.6.3
Equivalence Between Transfer Entropy
and Granger Causality . . . . . . . . . . . . . . . . . . . . . . .
187
8.6.4
Computation of Transfer Entropy . . . . . . . . . . . . . . .
188
8.7
Estimation of MVAR Coefficients . . . . . . . . . . . . . . . . . . . .
190
8.7.1
Least-Squares Algorithm . . . . . . . . . . . . . . . . . . . . .
190
8.7.2
Sparse Bayesian (Champagne) Algorithm. . . . . . . . . .
191
8.8
Numerical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
8.8.1
Experiments Using Bivariate Causal Time Series . . . .
192
8.8.2
Experiments Using Trivariate Causal Time Series . . . .
194
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
Contents
xiii

9
Detection of Phase–Amplitude Coupling in MEG Source Space:
An Empirical Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
9.2
Types of Cross-Frequency Coupling . . . . . . . . . . . . . . . . . . .
200
9.3
Local PAC and Cross-Location PAC. . . . . . . . . . . . . . . . . . .
201
9.4
Quantification of Phase–Amplitude Coupling . . . . . . . . . . . . .
202
9.4.1
Instantaneous Amplitude and Phase. . . . . . . . . . . . . .
202
9.4.2
Amplitude–Phase Diagram. . . . . . . . . . . . . . . . . . . .
202
9.4.3
Modulation Index (MI) . . . . . . . . . . . . . . . . . . . . . .
203
9.4.4
Phase-Informed Time-Frequency Map . . . . . . . . . . . .
204
9.5
Source Space PAC Analysis: An Example Study
Using Hand-Motor MEG Data . . . . . . . . . . . . . . . . . . . . . . .
204
9.5.1
Experimental Design and Recordings . . . . . . . . . . . .
204
9.5.2
Data Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
9.5.3
Results of Local PAC Analysis. . . . . . . . . . . . . . . . .
206
9.5.4
Results of Analyzing Cross-Location PACs . . . . . . . .
209
9.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
Appendix A: Bioelectromagnetic Forward Modeling . . . . . . . . . . . . . .
215
Appendix B: Basics of Bayesian Inference . . . . . . . . . . . . . . . . . . . . .
231
Appendix C: Supplementary Mathematical Arguments . . . . . . . . . . . .
247
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
xiv
Contents

Chapter 1
Introduction to Electromagnetic
Brain Imaging
1.1 Functional Brain Imaging and Bioelectromagnetic
Measurements
Noninvasive functional brain imaging has made a tremendous impact in improving
our understanding of the human brain. Functional magnetic resonance imaging
(fMRI)hasbeenthepredominantmodalityforimagingthefunctioningbrainsincethe
middle of the 1990s. fMRI measures changes in blood oxygenation-level-dependent
(BOLD) signals caused by neuronal activation. It is a noninvasive method that allows
for whole-brain measurement and the examination of activity in deep brain structures.
However, fMRI lacks the temporal resolution required to image the dynamic
and oscillatory spatiotemporal patterns associated with activities in a brain. This is
because the BOLD signal, which is only an indirect measure of neural activity, is
fundamentally limited by the rate of oxygen consumption and subsequent blood ﬂow.
Furthermore, since the BOLD signal is only an approximate and indirect measure
of neuronal activity, it might not accurately reﬂect true neuronal processes. Hence,
to observe neurophysiological processes more directly within relevant timescales,
imaging techniques that have both high temporal and adequate spatial resolution
are needed.
Neurons in the brain function electrically, as well as chemically. Therefore, their
activity generates associated electric and magnetic ﬁelds that can be detected outside
the head. Electromagnetic brain imaging is a term intended to encompass noninva-
sive techniques which probe brain electromagnetic activity and properties. Two tech-
niques currently exist for detecting electrical brain activities: electroencephalography
(EEG) and magnetoencephalography (MEG). EEG measures the electric potential
by means of electrodes placed on the scalp, and MEG measures magnetic ﬁelds by
means of sensors placed near the head.
In contrast to fMRI, both MEG and EEG directly measure electromagnetic ﬁelds
emanating from the brain with excellent temporal resolution of less than 1ms,
and allow the study of neural oscillatory processes over a wide frequency range
(1–600Hz). Because of such high temporal resolution, MEG and EEG can provide
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_1
1

2
1
Introduction to Electromagnetic Brain Imaging
information not obtainable with other functional brain imaging techniques. Most
notably, MEG and EEG track neural population activity on millisecond timescales,
revealing large-scale dynamics that are crucial for understanding brain function.
Furthermore, by applying modern inverse algorithms, it is possible to obtain three-
dimensional images, which are reasonable estimates of neural activity. Such images
are extremely useful for answering many questions on brain science.
While MEG is mainly sensitive to tangential currents in the brain closer to the
surface and relatively insensitive to the conductive properties of the skull, EEG is
primarily sensitive to radial sources while being highly sensitive to the conductive
properties of the brain, skull, and scalp. Therefore, MEG and EEG can be viewed
as being complementary in terms of the sensitivity to underlying neural activity.
However, since magnetic ﬁelds generated from neurons are not distorted by the
heterogeneous electrical properties of a brain, these magnetic ﬁelds can be considered
an undistorted signature of underlying cortical activity.
In addition, there are several practical or physiological reasons why neuroscien-
tists prefer MEG to EEG. First, MEG setup time is very short, because MEG mea-
surement does not require much preparation in attaching and checking electrodes, as
is needed in performing EEG measurement. This simpliﬁes matters both for exper-
imenters and subjects. Second, the anatomical location of primary sensory cortices
in sulci makes MEG ideally suited for electrophysiological studies. Furthermore,
with whole-head sensor arrays, MEG is also well-suited to investigate hemispheric
lateralization effects. Therefore, this chapter is primarily dedicated to giving a review
of the methodologies associated with MEG.
1.2 Sensing Magnetic Fields from the Brain
The long apical dendrites of cortical pyramidal cells are arranged perpendicularly to
the cortical surface and parallel to each other. This fortuitous anatomical arrangement
of these cells allows the magnetic ﬁelds to sum up to magnitudes large enough to
detect at the scalp. Synchronously ﬂuctuating dendritic currents result in equivalent
currentdipolesthatproducesuchmagneticﬁelds.However,biomagneticﬁeldsfroma
brain are extremely small, (in range of tens-to-hundreds of femto-Tesla (fT)) which
is about seven orders of magnitude smaller than the earth’s magnetic ﬁeld. As a
result, appropriate data collection necessitates a magnetically shielded room and
highly sensitive detectors known as superconducting quantum interference devices
(SQUIDs). Biomagnetic ﬁelds from a brain are typically sensed using detection coils
called ﬂux transformers or magnetometers, which are positioned close to the scalp
and connected to SQUIDs. A SQUID acts as a magnetic-ﬁeld-to-voltage converter,
and its nonlinear response is linearized by ﬂux-locked loop electronics. SQUIDs
have a sensitivity of up to 5 femto-Tesla per square root of Hz, which is adequate for
the detection of brain-generated magnetic ﬁelds.
MEG sensors are often conﬁgured for measuring differential magnetic ﬁelds
so as to reduce ambient noise in measurements. Such sensors are referred to as

1.2 Sensing Magnetic Fields from the Brain
3
gradiometers, although some MEG systems do not use gradiometers relying on clever
noise cancellation methods. The two commonly used gradiometer conﬁgurations are
axial and planar gradiometers. Axial (ﬁrst order) gradiometers consist of two coils
that share the same axis, whereas planar (ﬁrst order) gradiometers consist of two
coils that share the same plane. The sensitivity proﬁle of a planar gradiometer is
somewhat similar to EEG, whereby a sensor is maximally sensitive to a source clos-
est to it. In contrast, the sensitivity proﬁle of an axial gradiometer can be somewhat
counterintuitive because it is not maximally sensitive to sources closest to the sensors.
Modern MEG systems consist of simultaneous recordings from many sensors that
providewholeheadcoverage.Thetotalnumberofsensorsvariesfrom100to300.The
advent of such large sensor-array systems has signiﬁcantly advanced MEG studies.
Although the maximum sampling rate for many MEG systems reaches more than
10kHz, most MEG data is usually recorded at a sampling rate of around 1,000Hz,
which still provides excellent temporal resolution for measuring the dynamics of
cortical neuronal activity at millisecond order.
1.3 Electromagnetic Brain Imaging
MEG sensor data only provides qualitative information about underlying brain activ-
ities. The analysis of the sensor data is typically performed based on the intuitions
of experienced users regarding the sensitivity proﬁle of the sensors. To extract more
precise information from the observed sensor data, it is essential to apply imaging-
type analysis involving the reconstruction of brain activities from the sensor data.
Major components for the electromagnetic brain imaging are the forward model and
the inverse algorithms.
1.3.1 Forward Model
The forward model consists of three subcomponents: the source model, the volume
conductor model, and the measurement model. Typical source models assume that
the brain magnetic ﬁelds are generated by equivalent current dipoles in the brain. This
model is consistent with available measurements of coherent synaptic and intracellu-
lar currents in cortical columns that are thought to be major contributors to MEG and
EEG signals. Although several more complex source models have been proposed,
the equivalent current dipole is the dominant source model. This is because, given the
distance between the sources and sensors, the dipole is a reasonable approximation
of brain sources.
The volume conductor model refers to the equations that govern the relation
between the source model and the sensor measurements, namely the electric poten-
tials or the magnetic ﬁelds. These surface integral equations, obtained by solving
Maxwell’s equations under quasi-static conditions, can be solved analytically for

4
1
Introduction to Electromagnetic Brain Imaging
special geometries of the volume conductor, such as the sphere or an ellipsoid. A
concise review on the spherical homogeneous conductor model is found in Appen-
dixA. For realistic volume conductors, various numerical techniques such as ﬁnite-
element and boundary-element methods may be employed, although these methods
are generally time-consuming.
Measurement models refer to the speciﬁc measurement systems used in EEG and
MEG including the position of the sensors relative to the head. For instance, dif-
ferent MEG systems measure axial versus planar gradients of the magnetic ﬁelds
with respect to different locations of sensors. The measurement model incorporates
such information about the type of measurement and the geometry of the sensors.
Measurement of the position of the head relative to the sensor array is accomplished
by attaching head-localization coils to ﬁducial landmarks on the scalp. Modern MEG
systems are sometimes equipped with continuous head-localization procedures that
enable constant updating of sensor locations relative to the head to compensate for
subjects head movements. The source, volume conductor, and measurement models
are typically combined into a concept called the lead ﬁeld that describes a linear rela-
tionship between sources and the measurements. When discussing inverse methods,
we assume that the lead ﬁeld matrix is known.
1.3.2 Inverse Algorithms
Inverse algorithms are used for solving the bioelectromagnetic inverse problem,
i.e., for estimating the parameters of neural sources from MEG and EEG sensor
data. When implementing electromagnetic brain imaging, this estimation of spatial
locations and timing of brain sources is a challenging problem because it involves
solving for unknown brain activity across thousands of voxels from the recordings
of just a few hundred sensors. In general, there are no unique solutions to the inverse
problem because there are many source conﬁgurations that could produce sensor
data equal to the sensor observations, even in the absence of noise and (if given)
inﬁnite spatial or temporal sampling. This nonuniqueness is referred to as the ill-
posed nature of the inverse problem. Nevertheless, to get around this nonuniqueness,
various estimation procedures incorporate prior knowledge and constraints about
source characteristics.
Inverse algorithms can be classiﬁed into two categories: model-based dipole ﬁt-
ting and (non-model-based) imaging methods. Dipole ﬁtting methods assume that
a small set of current dipoles can adequately represent an unknown source distri-
bution. In this case, the dipole locations and its moments form a set of unknown
parameters, which are typically estimated using the least-squares ﬁt. The dipole ﬁt-
ting method—particularly the single-dipole ﬁtting method—has clinically been used
for localization of early sensory responses in somatosensory and auditory cortices.
However, two major problems exist in a dipole ﬁtting procedure. First, the
nonlinear optimization causes a problem of local minima when more than two dipole
parameters are estimated. A second, more difﬁcult problem is that the dipole ﬁtting
methods require a priori knowledge of the number of dipoles. Often, such information

1.3 Electromagnetic Brain Imaging
5
about the model order is not available a priori, especially for complex brain map-
ping conditions, and the resulting localization of higher order cortical functions can
sometimes be unreliable.
An alternative approach is whole-brain source imaging methods. These methods
apply voxel discretization over a whole brain volume, and assume a ﬁxed source at
each voxel. These methods estimate the amplitudes (and directions) of the sources
by minimizing a cost function. One classic method of this kind is the minimum-norm
method [1]. The minimum-norm and related methods are the topic of Chap.2.
Since the number of voxels is usually much larger than the number of sensors,
the cost function should contain a constraint term that is derived from various prior
information about the nature of the sources. We have developed a powerful imaging
algorithm that incorporates a sparsity constraint that facilitates the sparse source
conﬁgurations. This algorithm, called the Champagne algorithm, is described in
detail in Chap.4.
The other class of imaging algorithm is the spatial ﬁlter. The spatial ﬁlter esti-
mates source amplitude at each location independently. It is often called a virtual
sensor method, because it forms a virtual sensor, which scans an entire source space
to produce a source image. The most popular spatial ﬁlter algorithms are adaptive
spatial ﬁltering techniques, more commonly referred to as adaptive beamformers.
Adaptive beamformers are the topic of Chap.3 in which a concise review on adap-
tive beamformers is presented. A comprehensive review of these algorithms is found
in [2]. The implementation of adaptive beamformers is quite simple, and they have
proven to be powerful algorithms for characterizing cortical oscillations. Therefore,
they are popular for the source localization from spontaneous brain activity, par-
ticularly resting-state MEG. We have recently proposed novel scanning algorithms
[3, 4]. These methods have shown performance improvements over the adaptive
beamformers. One of these methods, called the Saketini algorithm, is described in
Chap.5.
In Chap.6, we discuss a hierarchical Bayesian framework which encompasses
various source imaging algorithms in a uniﬁed framework and reveals a close rela-
tionship between algorithms that are considered quite different [5].
An enduring problem in MEG imaging is that the brain evoked responses to
sensory or cognitive events are small compared to the interfering magnetic ﬁeld.
Typical sources of such interference include the background room interference from
power lines and electronic equipment, the interference with biological origins such as
heartbeat, eye blink, or other muscle artifacts. Ongoing brain activity itself, including
the drowsy-state alpha rhythm, is also a major source of interference. All existing
methods for brain source imaging are hampered by such interferences present in
MEG data. Several signal-processing methods have been developed to reduce inter-
ferences by preprocessing the sensor data before submitting it to source localization
algorithms. One such algorithm, called partitioned factor analysis, is described in
Chap.5.

6
1
Introduction to Electromagnetic Brain Imaging
1.4 From Source Imaging to Functional
Connectivity Imaging
It is now well recognized in neuroscience that it is necessary to examine how the
brainintegratesinformationacrossmultipleregions.Thetermfunctionalconnectivity
essentially deﬁnes the complex functional interaction between separate brain areas.
Although functional connectivity analysis using fMRI is common, MEG is better
suited for modeling and detecting such interactions, because of its high temporal
resolution.
Functional connectivity analysis can be applied either in sensor-space or in source-
space. In sensor-space analysis, the ﬁeld spread across many sensors arriving from
a single brain region leads to uncertainties in interpreting the estimation results of
brain interactions [6]. Therefore, a number of studies have begun to use source-space
analysis, in which voxel time courses are ﬁrst estimated by an inverse algorithm
and brain interactions are then analyzed using those estimated voxel time courses.
However, a serious problem arises from the leakage of an inverse algorithm, and such
leakages are more or less inevitable in any inverse algorithm. Chapter7 describes the
imaginary coherence analysis in source space. The imaginary coherence analysis is
known to be robust to the leakage of an inverse algorithm, and has become a popular
method in analyzing functional connectivity using MEG.
Connectivity measures commonly used these days—such as coherence—are bidi-
rectional measures, which cannot detect the directionality of brain interactions. There
has been growing interest in analyzing causal networks in the brain, and directional
measures are needed to detect such causal networks. Measures for detecting causal
networks are the topic of Chap.8.
Neural mechanisms of brain information processing are the subject of intense
investigation. A prevailing hypothesis is that the brain uses temporal encoding based
on ﬁring phase, and that there may exist a coupling between oscillators of different
frequencies. Chapter9 presents empirical results that such phase-amplitude coupling
can be detected using MEG source-space analysis. The results in Chap.9 demon-
strate that MEG functional connectivity imaging holds great potential in revealing
mechanisms of brain information processing.
1.5 Examples of Clinical Applications
1.5.1 Functional Mapping for Preoperative Neurosurgical
Planning
The surgical management of brain tumors or epilepsy often requires detailed func-
tional mapping of cortical regions around a tumor or epileptogenic zone. Preop-
erative mapping techniques aim at accurately estimating the location of functional

1.5 Examples of Clinical Applications
7
areas in relation to a tumor or an epileptic focus to minimize surgical risk. Mass
lesions can frequently distort normal neuroanatomy, which makes the identiﬁcation
of eloquent cortices inaccurate with normal neuroanatomical landmarks. MEG is
increasingly being used for preoperative functional brain imaging. By mapping rel-
evant somatosensory, auditory, and motor cortices preoperatively, retained areas of
function can be delineated. Preoperative localization of functionally intact brain tis-
sue helps guide neurosurgical planning and limits the region of resection, allowing
for improved long-term patient morbidity and neurological function.
Examples that show how MEG imaging is used to map the motor cortex in presur-
gical patients are given in [7]. This study performed localization of β-band desyn-
chronization preceding the index ﬁnger ﬂexion for a subject with a frontal tumor,
and showed the location of hand motor cortex relative to a single dipole localization
of hand somatosensory cortex. These results were conﬁrmed with electrical cortical
stimulation. This investigation demonstrated that MEG source images obtained using
beta-band event-related desynchronization reliably localize the hand motor cortex.
1.5.2 Functional Connectivity Imaging
Functional connectivity analysis has been shown to have profound clinical signiﬁ-
cance,becausedisturbancesinnetworksaremanifestedasabnormalitiesinfunctional
connectivity and such abnormalities can be detected using resting state MEG. Recent
studies have shown this to be the case, and abnormalities in functional connectivity
during resting state are observed in many clinical conditions such as brain tumors
[8], strokes [9], traumatic brain injury [10], schizophrenia [11, 12], and Alzheimer
disease [13].
Utilizing MEG imaging, changes in the alpha-band functional connectivity in
patients with traumatic brain injury were measured and compared to healthy con-
trols in [10]. In this investigation, mean imaginary coherence (MIC) (described in
Sect.7.9) at each brain voxel was calculated as an index for functional connectivity.
In one male patient’s case, his initial MEG scan was obtained 9months after his
injury and the results of MIC mapping demonstrated several regions of decreased
resting state functional connectivity compared to the control group. The follow-up
second MEG scan was obtained 23months after the initial scan and the results of MIC
mapping demonstrated a decrease in the volume of cortex with reduced connectivity,
although some of the cortical regions with the greatest reductions in functional con-
nectivity seen in the initial MEG remained abnormal even in the second MEG scan.
The investigation described next identiﬁes brain regions that exhibited abnor-
mal resting-state connectivity in patients with schizophrenia [11, 12]. Associations
between functional connectivity and clinical symptoms were found in stable outpa-
tientparticipants.Resting-sateMEGwasmeasuredfromthirtyschizophreniapatients
and ﬁfteen healthy control subjects. The MIC mapping was computed in the alpha

8
1
Introduction to Electromagnetic Brain Imaging
frequency band, and the results showed that the functional connectivity of the left
inferior parietal cortex was negatively related to positive symptoms and the left
prefrontal cortical connectivity was associated with negative symptoms. This study
demonstrates direct functional disconnection in schizophrenia patients between spe-
ciﬁc cortical ﬁelds within low-frequency resting state oscillations. Such ﬁndings
indicate that this level of functional disconnection between cortical regions is an
important treatment target in schizophrenia patients.
References
1. M.S. Hämäläinen, R.J. Ilmoniemi, Interpreting measured magnetic ﬁelds of the brain: estimates
of current distributions. Technical Report TKK-F-A559, Helsinki University of Technology
(1984)
2. K. Sekihara, S.S. Nagarajan, Adaptive Spatial Filters for Electromagnetic Brain Imaging
(Springer, Berlin, 2008)
3. J.M. Zumer, H.T. Attias, K. Sekihara, S.S. Nagarajan, A probabilistic algorithm integrating
source localization and noise suppression for MEG and EEG data. NeuroImage 37, 102–115
(2007)
4. J.M. Zumer, H.T. Attias, K. Sekihara, S.S. Nagarajan, Probabilistic algorithms for MEG/EEG
source reconstruction using temporal basis functions learned from data. NeuroImage 41(3),
924–940 (2008)
5. D. Wipf, S.S. Nagarajan, A uniﬁed Bayesian framework for MEG/EEG source imaging. Neu-
roImage 44, 947–966 (2009)
6. J.-M. Schoffelen, J. Gross, Source connectivity analysis with MEG and EEG. Hum. Brain
Mapp. 30, 1857–1865 (2009)
7. P.E. Tarapore, M.C. Tate, A.M. Findlay, S.M. Honma, D. Mizuiri, M.S. Berger, S.S. Nagarajan,
Preoperative multimodal motor mapping: a comparison of magnetoencephalography imaging,
navigated transcranial magnetic stimulation, and direct cortical stimulation: clinical article. J.
Neurosurg. 117(2), 354–362 (2012)
8. A.G. Guggisberg, S.M. Honma, A.M. Findlay, S.S. Dalal, H.E. Kirsch, M.S. Berger, S.S.
Nagarajan, Mapping functional connectivity in patients with brain lesions. Ann. Neurol. 63(2),
193–203 (2008)
9. K.P. Westlake, L.B. Hinkley, M. Bucci, A.G. Guggisberg, A.M. Findlay, R.G. Henry, S.S.
Nagarajan, N. Byl, Resting state alpha-band functional connectivity and recovery after stroke.
Exp. Neurol. 237(1), 160–169 (2012)
10. P.E. Tarapore, A.M. Findlay, S.C. LaHue, H. Lee, S.M. Honma, D. Mizuiri, T.L. Luks, G.T.
Manley, S.S. Nagarajan, P. Mukherjee, Resting state magnetoencephalography functional con-
nectivity in traumatic brain injury: clinical article. J. Neurosurg. 118(6), 1306–1316 (2013)
11. L.B. Hinkley, J.P. Owen, M. Fisher, A.M. Findlay, S. Vinogradov, S.S. Nagarajan, Cognitive
impairments in schizophrenia as assessed through activation and connectivity measures of
magnetoencephalography (MEG) data. Front. Hum. Neurosci. 3, 73 (2009)
12. L.B. Hinkley, S. Vinogradov, A.G. Guggisberg, M. Fisher, A.M. Findlay, S.S. Nagarajan,
Clinical symptoms and alpha band resting-state functional connectivity imaging in patients
with schizophrenia: implications for novel approaches to treatment. Biol. Psychiatry 70(12),
1134–1142 (2011)
13. K.G. Ranasinghe, L.B. Hinkley, A.J. Beagle, D. Mizuiri, A.F. Dowling, S.M. Honma, M.M.
Finucane, C. Scherling, B.L. Miller, S.S. Nagarajan et al., Regional functional connectivity
predicts distinct cognitive impairments in Alzheimers disease spectrum. NeuroImage: Clin. 5,
385–395 (2014)

Chapter 2
Minimum-Norm-Based Source Imaging
Algorithms
2.1 Introduction
In this chapter, we describe the minimum-norm and related methods, which are
classic algorithms for electromagnetic brain imaging [1, 2]. In this chapter, the
minimum-norm method is ﬁrst formulated based on the maximum-likelihood princi-
ple, and the properties of the minimum-norm solution are discussed. This discussion
leads to the necessity of regularization when implementing the minimum-norm
method. We discuss two different representative regularization methods: the
L2-norm regularization and the L1-norm regularization. The minimum-norm method
is, then, formulated based on Bayesian inference—Bayesian formulation providing
a form of the minimum-norm method where the regularization is already embedded.
2.2 Deﬁnitions
In electromagnetic brain imaging, we use an array of sensors to obtain
bioelectromagnetic measurements. We deﬁne the output of the mth sensor at time
t as ym(t), and the column vector containing outputs from all sensors, such that
y(t) =
⎡
⎢⎢⎢⎣
y1(t)
y2(t)
...
yM(t)
⎤
⎥⎥⎥⎦,
(2.1)
where M is the total number of sensors. This column vector y(t) expresses the outputs
of the sensor array, and it may be called the data vector or array measurement.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_2
9

10
2
Minimum-Norm-Based Source Imaging Algorithms
A spatial location is represented by a three-dimensional vector r: r = (x, y, z).
A source vector is deﬁned as a three-dimensional column vector s(r, t):
s(r, t) =
⎡
⎣
sx(r, t)
sy(r, t)
sz(r, t)
⎤
⎦,
(2.2)
where sx(r, t), sy(r, t), and sz(r, t) are the x, y, and z components. The physical
nature of the source vector is the electromotive force generated by neuronal activities
in the brain. Additional discussion regarding the nature of the sources is presented
in Sect.A.1 in the Appendix. The magnitude of the source vector is denoted as a
scalar s(r, t), and the orientation of the source is denoted as a three-dimensional unit
vector η(r) = [ηx(r), ηy(r), ηz(r)]T , where the superscript T indicates the matrix
transpose. Then, the relationship
s(r, t) = s(r, t)η(r) = s(r, t)
⎡
⎣
ηx(r)
ηy(r)
ηz(r)
⎤
⎦
(2.3)
holds.
2.3 Sensor Lead Field
We assume that a unit-magnitude source exists at r. We denote the output of the mth
sensor due to this unit-magnitude source as lx
m(r), l y
m(r), and lz
m(r) when the unit-
magnitude source is directed in the x, y, and z directions, respectively. The column
vectors lx(r), l y(r), and lz(r) are deﬁned as
lx(r) = [lx
1(r),lx
2(r), . . . ,lx
M(r)]T ,
l y(r) = [l y
1(r),l y
2(r), . . . ,l y
M(r)]T ,
lz(r) = [lz
1(r),lz
2(r), . . . ,lz
M(r)]T .
These vectors express the sensor array sensitivity for a source located at r and directed
in the x, y, and z directions. Using these column vectors, the sensitivity of the whole
sensor array for a source at r is expressed using an M × 3 matrix:
L(r) = [lx(r), l y(r), lz(r)].
(2.4)
This matrix L(r) is called the lead-ﬁeld matrix. We also deﬁne the lead-ﬁeld vector,
l(r), that expresses the sensitivity of the sensor array in a particular source direction
η(r), such that
l(r) = L(r)η(r).
(2.5)

2.3 Sensor Lead Field
11
Theproblemofestimatingthesensorleadﬁeldisreferredtoasthebioelectromagnetic
forward problem. Arguments on how to compute the sensor lead ﬁeld are presented
in Appendix A.
2.4 Voxel Source Model and Tomographic Source
Reconstruction
Using the lead-ﬁeld matrix in Eq.(2.4), the relationship between the sensor data,
y(t), and the source vector, s(r, t), is expressed as
y(t) =

Ω
L(r)s(r, t) dr.
(2.6)
Here, dr indicates the volume element, and the integral is performed over a volume
where sources are assumed to exist. This volume is called the source space, which
is denoted Ω. Equation (2.6) expresses the relationship between the sensor outputs
y(t) and the source distribution s(r, t).
The bioelectromagnetic inverse problem is the problem of estimating the source-
vector spatial distribution, s(r, t), from the measurements, y(t). Here, we assume
that we know the sensor lead ﬁeld L(r), although our knowledge of the sensor lead
ﬁeld is to some degree imperfect because it must be estimated using an analytical
model or numerical computations.
When estimating s(r, t) from y(t), s(r, t) is continuous in space, while y(t) is
discrete in space. A common strategy here is to introduce voxel discretization over the
source space. Let us deﬁne the number of voxels as N, and the locations of the voxels
are denoted as r1, r2, . . . , r N. Then, the discrete form of Eq.(2.6) is expressed as:
y(t) =
N
	
j=1
L(r j)s(r j, t) =
N
	
j=1
L(r j)s j(t).
(2.7)
where the source vector at the jth voxel, s(r j, t), is denoted s j(t) for simplicity. We
introduce the augmented lead-ﬁeld matrix over all voxel locations as
F = [L(r1), L(r2), . . . , L(r N)],
(2.8)
which is an M ×3N matrix. We deﬁne a 3N ×1 column vector containing the source
vectors at all voxel locations, x(t), such that
x(t) =
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦.
(2.9)

12
2
Minimum-Norm-Based Source Imaging Algorithms
Equation (2.7) is then rewritten as
y(t) = [L(r1), L(r2), . . . , L(r N)]
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦= Fx(t).
(2.10)
Here, since the augmented lead-ﬁeld matrix F is a known quantity, the only unknown
quantity is the 3N ×1 column vector, x(t). This vector x(t) is called the voxel source
vector.
The spatial distribution of the source orientation, η(r), may be a known quantity
if accurate subject anatomical information (such as high-precision subject MRI) can
be obtained with accurate co-registration between the MRI coordinate and the sensor
coordinate. In this case, the inverse problem is the problem of estimating the source
magnitude, s(r, t), instead of the source vector, s(r, t). Let us consider a situation
in which the source orientations at all voxel locations are predetermined. Deﬁning
the orientation of a source at the jth voxel as η j, the lead ﬁeld at the jth voxel is
expressed as the column vector l j, which is obtained as l j = L(r j)η j, according to
Eq.(2.5). Thus, the augmented lead ﬁeld is expressed as an M × N matrix H deﬁned
such that
H = [L(r1)η1, L(r2)η2, . . . , L(r N)ηN] = [l1, l2, . . . , l N],
(2.11)
whereby Eq.(2.10) can be reduced as follows:
y(t) = [L(r1), L(r2), . . . , L(r N)]
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦
= [L(r1), L(r2), . . . , L(r N)]
⎡
⎢⎢⎢⎢⎣
η1
0
· · ·
0
0
η2
·
...
...
·
...
0
0
· · ·
0
ηN
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦
= [L(r1)η1, L(r2)η2, . . . , L(r N)ηN]
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦= Hx(t).
(2.12)

2.4 Voxel Source Model and Tomographic Source Reconstruction
13
Thus, the voxel source vector x(t), in this case, is an N × 1 column vector,
x(t) =
⎡
⎢⎢⎢⎣
s1(t)
s2(t)
...
sN(t)
⎤
⎥⎥⎥⎦,
(2.13)
in which the jth component of x(t) is s j(t), which is the scalar intensity at the jth
voxel. In this book, the same notation x(t) is used to indicate either the 3N ×1 vector
in Eq. (2.9) or the N × 1 vector in Eq. (2.13), unless any confusion arises.
In summary, denoting the additive noise in the sensor data ε, the relationship
between the sensor data y(t) and the voxel source vector x(t) is expressed as
y(t) = Fx(t) + ε,
(2.14)
where x(t) is a 3N ×1 column vector in Eq. (2.9). When voxels have predetermined
orientations, using the augmented lead ﬁeld matrix H in Eq. (2.11), the relationship
between y(t) and x(t) is expressed as
y(t) = Hx(t) + ε,
(2.15)
where x(t) is an N × 1 column vector in Eq. (2.13).
2.5 Maximum Likelihood Principle and the Least-Squares
Method
When estimating the unknown quantity x from the sensor data y, the basic principle
is to interpret the data y as a realization of most probable events. That is, the sensor
data y is considered the result of the most likely events. We call this the maximum
likelihood principle. In this chapter, we ﬁrst derive the maximum likelihood solution
of the unknown source vector x.
We assume that the noise distribution is Gaussian, i.e.,
ε ∼N(ε|0, σ2I).
Namely, the noise in the sensor data is the identically and independently distributed
Gaussian noise with a mean of zero, and the same variance σ2. According to (C.1)
in the Appendix, the explicit form of the noise probability distribution is given by
p(ε) =
1
(2πσ2)M/2 exp

−1
2σ2 ∥ε∥2

.
(2.16)

14
2
Minimum-Norm-Based Source Imaging Algorithms
Since the linear relationship in Eq. (2.14) holds, the probability distribution of the
sensor data y(t) is expressed as
p(y) =
1
(2πσ2)M/2 exp

−1
2σ2 ∥y −Fx∥2

,
(2.17)
where the explicit time notation (t) is omitted from the vector notations x(t) and
y(t) for simplicity.1
This p(y) as a function of the unknown parameter x is called the likelihood
function, and the maximum likelihood estimate x is obtained such that2
x = argmax
x
log p(y),
(2.18)
where log p(y) is called the log-likelihood function. Using the probability distribu-
tion in Eq. (2.17), the log-likelihood function log p(y) is expressed as
log p(y) = −1
2σ2 ∥y −Fx∥2 + C,
(2.19)
where C expresses terms that do not contain x. Therefore, the x that maximizes
log p(y) is equal to the one that minimizes F(x) deﬁned such that
F(x) = ∥y −Fx∥2.
(2.20)
That is, the maximum likelihood solution x is obtained using
x = argmin
x
F(x) :
where F = ∥y −Fx∥2.
(2.21)
This F(x) in Eq. (2.20) is referred to as the least-squares cost function, and the
method that estimates x through the minimization of the least-squares cost function
is the method of least-squares.
2.6 Derivation of the Minimum-Norm Solution
In the bioelectromagnetic inverse problem, the number of voxels N, in general, is
much greater than the number of sensors M. Thus, the estimation of the source vector
x is an ill-posed problem. When applying the least-squares method to such an ill-
posed problem, the problem arises that an inﬁnite number of x could make the cost
1 For the rest of this chapter, the explicit time notation is omitted from these vector notations, unless
otherwise noted.
2 The notation argmax indicates the value of x that maximizes log p(y) which is an implicit function
of x.

2.6 Derivation of the Minimum-Norm Solution
15
function equal to zero. Therefore, we cannot obtain an optimum solution of x based
only on the least-squares method.
A general strategy for overcoming this problem is to integrate a “desired property”
of the unknown parameter x into the estimation problem. That is, we choose x so as
to maximize this “desired property,” and also satisfy y = Fx. Quite often, a small
norm of the solution vector is used as this “desired property,” and in this case, the
optimum estimate x is obtained using
x = argmin
x
∥x∥2
subject to
y = Fx.
(2.22)
In the optimization above, the notation of “subject to” indicates a constraint,
(i.e., the above optimization requires that the estimate x be chosen such that x
minimizes ∥x∥2 as well as satisﬁes y = Fx.) To solve the constraint optimization
problem in Eq. (2.22), we use the method of Lagrange multipliers that can convert a
constrained optimization problem to an unconstrained optimization problem. In this
method, using an M × 1 column vector c as the Lagrange multipliers, we deﬁne a
function called the Lagrangian L(x, c) such that
L(x, c) = ∥x∥2 + cT (y −Fx) .
(2.23)
The solution x is obtained by minimizing L(x, c) above with respect to x and
c—the solution x being equal to x obtained by solving the constrained optimization
in Eq. (2.22).
To derive an x that minimizes Eq. (2.23), we compute the derivatives of L(x, c)
with respect to x and c, and set them to be zero, giving
∂L(x, c)
∂x
= 2x −FT c = 0,
(2.24)
∂L(x, c)
∂c
= y −Fx = 0.
(2.25)
Using the equations above, we can derive
x = FT 
FFT −1
y.
(2.26)
The solution in Eq. (2.26) is called the minimum-norm solution, which is well known
as a solution for the ill-posed linear inverse problem.
2.7 Properties of the Minimum-Norm Solution
The minimum-norm solution is expressed as
x = FT (FFT )−1(Fx + ε) = FT (FFT )−1Fx + FT (FFT )−1ε.
(2.27)

16
2
Minimum-Norm-Based Source Imaging Algorithms
The ﬁrst term on the right-hand side is expressed as E(x), which indicates the
expectation of x. This term represents how the solution deviates from its true value
even in the noiseless cases . The second term indicates the inﬂuence of the noise ε.
The ﬁrst term is rewritten as
E(x) = FT (FFT )−1Fx = Qx,
(2.28)
where
Q = FT (FFT )−1F.
(2.29)
Apparently, the ﬁrst term is not equal to the true value x, and the matrix Q in
Eq. (2.29) expresses the relationship between the true value x and the estimated
value E(x).
Denoting the (i, j)th element of Q as Qi, j, the jth element of E(x), E(x j), is
expressed as
E(x j) =
N
	
k=1
Q j,kxk.
(2.30)
The above equation shows how each element of the true vector x affects the value of
E(x j). That is, Q j,k expresses the amount of leakage of xk into x j when j ̸= k. If
the weight Q j,1, . . . , Q j,N has a sharp peak at j, x j may be close to the true value
x j. If the weight has no clear peak or if the weight has a peak at j′ that is different
from j, x j may be very different from x j. Because of such properties, the matrix Q
is called the resolution matrix.
We next examine the second term, which expresses the noise inﬂuence. The noise
inﬂuence is related to the singular values of F. The singular value decomposition of
F is deﬁned as
F =
M
	
j=1
γ ju jvT
j ,
(2.31)
where we assume that M < N, and the singular values are numbered in decreasing
order. Using
FT (FFT )−1 =
N
	
j=1
1
γ j
v j uT
j ,
(2.32)
we can express the second term in Eq. (2.27) as
N
	
j=1
(uT
j ε)
γ j
v j.
(2.33)
The equation above shows that the denominator contains the singular values. Thus,
if higher order singular values are very small and close to zero, the terms containing
such small singular values amplify the noise inﬂuence, resulting in a situation where

2.7 Properties of the Minimum-Norm Solution
17
0
50
100
150
200
250
300
10 7
10 6
10 5
10 4
10 3
10 2
10 1
100
order
normalized value
Fig. 2.1 Typical plot of the singular values of the lead ﬁeld matrix F. We assume the 275-channel
CTF Omega whole-head MEG sensor system (VMS MedTech Ltd., BC, Canada). A typical location
of the subject head relative to the whole head sensor array is assumed. An 8 × 8 × 10 cm region is
also assumed as the source space within the subject’s head. The spherical homogeneous conductor
model is used for computing the sensor lead ﬁeld. The singular values are normalized with the
maximum (i.e., the ﬁrst) singular value
the second term is dominated in Eq. (2.27), and the minimum norm solution would
contain large errors due to the noise.
A plot of a typical singular-value spectrum of the lead ﬁeld matrix F is shown
in Fig.2.1. To obtain the plot, we used the sensor array of the 275-channel CTF
Omega whole-head MEG sensor system (VMS MedTech Ltd., BC, Canada) and
spherical homogeneous conductor model to compute the sensor lead ﬁeld [3].3 The
plot shows that higher order singular values of the lead ﬁeld matrix are very small.
In Fig.2.1, the ratio of the maximum and minimum singular values reaches the order
of 10−7. Therefore, the minimum-norm method in Eq. (2.26) generally produces
results highly susceptible to the noise in the sensor data.
2.8 L2-Regularized Minimum-Norm Solution
When a large amount of noise is overlapped onto the sensor data y, if we seek a
solution that satisﬁes y = Fx, the resultant solution x would be severely affected
by the noise. In other words, when noise exists in the sensor data, it is more or less
3 Computing the lead ﬁeld using the spherical homogeneous conductor model is explained in
Sect.A.2.4 in the Appendix.

18
2
Minimum-Norm-Based Source Imaging Algorithms
meaningless to impose the constraint y = Fx, so, instead of using the optimization
in Eq. (2.22), we should use
x = argmin
x
∥x∥2
subject to ∥y −Fx∥2 ≤d,
(2.34)
where d is a positive constant. In Eq. (2.34), the condition ∥y −Fx∥2 ≤d does not
require Fx to be exactly equal to y, but allow Fx to be different from y within a
certain range speciﬁed by d. Therefore, the solution x is expected to be less affected
by the noise in the sensor data y.
Unfortunately, there is no closed-form solution for the optimization problem in
Eq. (2.34), because of the inequality constraint. Although we can solve Eq. (2.34)
numerically, we proceed in solving it by replacing the inequality constraint with
the equality constraint. This is possible because the solution of Eq. (2.34) generally
exists on the border of the constraint. Thus, we can change the optimization problem
in Eq. (2.34) to
x = argmin
x
∥x∥2
subject to ∥y −Fx∥2 = d.
(2.35)
Since this is an equality-constraint problem, we can use the method of Lagrange
multipliers. Using the Lagrange multiplier λ, the Lagrangian is deﬁned as
L(x, c) = ∥x∥2 + λ

∥y −Fx∥2 −d

.
(2.36)
Thus, the solution x is given as
x = argmin
x
L(x, c) = argmin
x

∥x∥2 + λ∥y −Fx∥2
.
(2.37)
In the above expression, we disregard the term −λd, which does not affect the results
of the minimization. Also, we can see that the multiplier λ works as a balancer
between the L2-norm4 term ∥x∥2 and the squared error term ∥y −Fx∥2.
To derive the solution of x that minimizes L(x, c), we compute the derivative of
L(x, c) with respect to x and set it to zero, i.e.,
L(x, c)
∂x
= 1
∂x

yT y −xT FT y −yT Fx + xT FT Fx + ξxT x

= −2FT y + 2

FT F + ξI

x = 0,
(2.38)
where we use 1/λ = ξ. We can then derive
x =

FT F + ξI
−1
FT y.
(2.39)
4 A brief summary of the norm of vectors is presented in Sect.C.4 in the Appendix.

2.8
L2-Regularized Minimum-Norm Solution
19
Using the matrix inversion lemma in Eq. (C.92), we obtain
x = FT 
FFT + ξI
−1
y.
(2.40)
ThesolutioninEq.(2.40)iscalledthe L2-norm-regularizedminimum-normsolution,
or simply L2-regularized minimum-norm solution.
Let us compute the noise inﬂuence term for the L2-regularized minimum-norm
solution. Using Eq. (2.31), we have
FT 
FFT + ξI
−1
=
N
	
j=1
γ j
γ2
j + ξ v j uT
j ,
(2.41)
and the L2-regularized minimum-norm solution is expressed as
x = FT 
FFT + ξI
−1
(Hx + ε)
= FT 
FFT + ξI
−1
Hx +
N
	
j=1
γ j
γ2
j + ξ v j uT
j ε.
(2.42)
The second term, expressing the inﬂuence of noise, is
N
	
j=1
γ j(uT
j ε)
γ2
j + ξ v j.
(2.43)
In the expression above, the denominator contains the positive constant ξ, and it is
easy to see that this ξ prevents the terms with smaller singular values from being
ampliﬁed.
One problem here is how to choose an appropriate value for ξ. Our argument
above only suggests that if the noise is large, we need a large ξ, but if small, a smaller
ξ can be used. However, the arguments above do not lead to the derivation of an
appropriate ξ. We will return to this problem in Sect.2.10.2 where L2-regularized
minimum-norm solution is re-derived based on a Bayesian formulation, in which
deriving the optimum ξ is embedded.
2.9 L1-Regularized Minimum-Norm Solution
2.9.1 L1-Norm Constraint
In the preceding section, we derived a solution that minimizes the L2-norm of the
solution vector x. In this section, we argue for a solution that minimizes the L1-norm
of x, which is deﬁned in Eq. (C.64). The L1-norm-regularized solution is obtained

20
2
Minimum-Norm-Based Source Imaging Algorithms
using [4–6]
x = argmin
x
N
	
j=1
|x j| subject to ∥y −Fx∥2 = d.
(2.44)
The only difference between the equation above and Eq. (2.35) is to minimize either
L2 norm ∥x∥2 in Eq. (2.35) or L1 norm, ∥x∥1 = 
j |x j| in Eq. (2.44). Although it
may look as if there is no signiﬁcant difference between the two methods, the results
of source estimation are signiﬁcantly different. The L1-norm regularization gives a
“so-called” sparse solution, in which only few x j have nonzero values and a majority
of other x j have values close to zero.
Using the method of Lagrange multipliers and following exactly the same argu-
ments as in Sect.2.8, the L1-norm solution can be obtained by minimizing the cost
function F, i.e.,
x = argmin
x
F :
F = ∥y −Fx∥2 + ξ
N
	
j=1
|x j|,
(2.45)
where again ξ is a positive constant that controls the balance between the ﬁrst and the
second terms in the cost function above. Unfortunately, the minimization problem
in Eq. (2.45) does not have a closed-form solution, so numerical methods are used
here to obtain the solution x.
2.9.2 Intuitive Explanation for Sparsity
Actually, it is not easy to provide an intuitive explanation regarding why the opti-
mization in Eq. (2.44) or (2.45) causes a sparse solution. The straightforward (and
intuitively clear) formulation to obtain a sparse solution should use the L0-norm
minimization, such that
x = argmin
x
N
	
j=1
T (x j) subject to ∥y −Hx∥2 = d,
(2.46)
where the function T (x) is deﬁned in Eq. (C.65). In the above formulation, since
N
j=1 T (x j) indicates the number of nonzero x j,x is the solution that has the smallest
number of nonzero x j and still satisﬁes ∥y −Hx∥2 = d. The optimization prob-
lem in Eq. (2.46) is known to require impractically long computational time. The
optimization for the L1-norm cost function in Eq. (2.44) approximates this L0-norm
optimization in Eq. (2.46) so as to obtain a sparse solution within a reasonable range
of computational time [7].

2.9
L1-Regularized Minimum-Norm Solution
21
The regularization methods mentioned above can be summarized to have a form
of the cost functions expressed as
F = ∥y −Hx∥2 + ξφ(x).
(2.47)
The ﬁrst term is the data-ﬁtting term, and the second term φ(x) expresses the con-
straint, which has the following form for general L p-norm cases (0 ≤p ≤1):
φ(x) =
N
	
j=1
T (x j) for L0-norm,
(2.48)
φ(x) =
N
	
j=1
|x j| for L1-norm,
(2.49)
φ(x) =
⎡
⎣
N
	
j=1
x p
j
⎤
⎦
1/p
for L p-norm.
(2.50)
The plots of φ(x) with respect to the x j axis are shown in Fig.2.2. In this ﬁgure, the
four kinds of plots of φ(x) = ∥x∥p when p = 0, p = 0.3, p = 0.7, and p = 1
1
0.8
0.6
0.4
0.2
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xj
cost func
p=1
p=0.7
p=0.3
p=0
Fig. 2.2 Plots of objective function φ(x) deﬁned in Eqs. (2.48)–(2.50) with respect to the x j axis.
The four cases of p = 0, p = 0.3, p = 0.7, and p = 1 are shown. The cases of p = 0, and p = 1
correspond to the L0 and L1 norm constraints (A brief summary of the norm of vectors is presented
in Sect.C.4 in the Appendix.)

22
2
Minimum-Norm-Based Source Imaging Algorithms
are shown. It can be seen in this ﬁgure that the L0-norm constraint is approximated
by the L p-norm constraint, and as p becomes closer to 0, the L p-norm provides a
better approximation.
Let us see how the L p-norm regularization causes sparse solutions when 0 ≤
p ≤1. To do so, we consider a simplest estimation problem in which only two
voxels exist and the voxels have source intensity of x1 and x2. We assume a noiseless
measurement using a single-sensor; the sensor data being represented by a scalar y.
The optimization for the L1-norm solution is expressed in this case as
x = argmin
x
(|x1| + |x2|)
subject to
y = h1x1 + h2x2,
(2.51)
where x = (x1,x2)T , and h1 and h2 are the sensor lead ﬁeld. For the sake of
comparison, we also argue the L2-norm regularization whose optimization is given
as follows:
x = argmin
x

x2
1 + x2
2
1/2
subject to
y = h1x1 + h2x2.
(2.52)
The optimization process is depicted in Fig.2.3. In Fig.2.3a, the tetragon at the
center represents the L1-norm objective function, |x1|+|x2| = constant. The broken
line represents the x1 and x2 that satisfy the measurement equation y = h1x1 +h2x2.
Thus, as a result of the optimization in Eq. (2.51), the x1 and x2 on the broken line that
minimize |x1|+|x2| should be chosen as the solution, i.e., the point (x1, x2) at which
the tetragon touches the broken line is chosen as the solution for the optimization.
Such solution is indicated by the small ﬁlled circle in Fig.2.3a. In this solution, x2
has a nonzero value but x1 is zero, i.e., a sparse solution is obtained. It can be seen
in this ﬁgure that in most cases, the point at which the tetragon touches the broken
x1
x2
(a)
x1
x2
(b)
x1
x2
(c)
Fig. 2.3 The optimization process is depicted for the simple case in which a single sensor and
two voxels exist. Source magnitudes at the voxels are represented by x1 and x2. The broken lines
represent the x1 and x2 that satisfy the measurement equation, y = h1x1 + h2x2. The ﬁlled black
circles indicate an example of the solution for each case. a L1-norm regularization in Eq. (2.51).
The tetragon at the center represents the L1-norm objective function |x1| + |x2| = constant.
b L2-norm regularization in Eq. (2.52). The circle at the center represents the L2-norm objec-
tive function x2
1 + x2
2 = constant. c L p-norm regularization where 0 < p < 1

2.9
L1-Regularized Minimum-Norm Solution
23
line is likely to be located at one of its vertices, so a sparse solution is likely to be
obtained.
Figure2.3b shows the case of the L2-norm minimization in Eq. (2.52). In this
ﬁgure, the broken line again represents the x1 and x2 that satisfy the measurement
equation y = h1x1 + h2x2, and the circle represents the L2-norm objective function
x2
1 + x2
2 = constant. In this case, the x1 and x2 on the broken line that minimizes
x2
1 + x2
2 should be chosen, and the resultant solution is (x1, x2) at which the circle
touches the broken line. An example of such solution is indicated by the small ﬁlled
circle. In this case, both x1 and x2 have nonzero values, and a non-sparse solution is
likely to be obtained using L2-norm regularization.
Finally, Fig.2.3c shows a case of the general L p norm minimization (0 < p < 1).
An example of such solution is indicated by the small, ﬁlled circle. Using the general
L p norm regularization, the solution is more likely to be sparse than the case of the
L1-norm minimization. However, the computational burden for the general L p norm
minimization is so high that it is seldom used in practical applications.
2.9.3 Problem with Source Orientation Estimation
When applying the L1-norm regularization to the bioelectromagnetic source localiza-
tion, it has been known that the method fails in estimating correct source orientations.
The reason for this is described as follows: The components of the solution vector x
is denoted explicitly as
x =

sx
1 , sy
1 , sz
1, . . . , sx
j , sy
j , sz
j, . . . , sx
N, sy
N, sz
N
T
,
where sx
j , sy
j , sz
j are the x, y, and z components of the source at the jth voxel. When
the jth voxel has a source activity, it is generally true that sx
j , sy
j , sz
j have nonzero
values. However, when using the L1 regularization, only one of sx
j , sy
j , sz
j tends to
have nonzero value, and others tend to be close to zero because of the nature of a
sparse solution. As a result, the source orientation may be erroneously estimated.
To avoid this problem, the source orientation is estimated in advance using some
other method [4] such as the L2-norm minimum-norm method. Then, the L1-norm
method is formulated using the orientation-embedded data model in Eq. (2.15). That
is, we use
x = argmin
x
F :
F = ∥y −Hx∥2 + ξ
N
	
j=1
|x j|.
(2.53)
In this case, the sparsity is imposed on the source vector magnitude, s1, s2, . . . , sN,
and only a few of s1, s2, . . . , sN have nonzero values, allowing for the reconstruction
of a sparse source distribution.

24
2
Minimum-Norm-Based Source Imaging Algorithms
2.10 Bayesian Derivation of the Minimum-Norm Method
2.10.1 Prior Probability Distribution and Cost Function
Inthissection,wederivetheminimum-normmethodbasedonBayesianinference.As
in Eq. (2.16), we assume that the noise ε is independently and identically distributed
Gaussian, i.e.,
ε ∼N(ε|0, β−1I),
(2.54)
where the precision β is used, which is the inverse of the noise variance, β−1 = σ2.
Thus, using Eq. (2.14), the conditional probability distribution of the sensor data for
a given x, p(y|x) is
p(y|x) =
 β
2π
M/2
exp

−β
2 ∥y −Fx∥2

.
(2.55)
This conditional probability p(y|x) is equal to the likelihood p(y) in the arguments
in Sect.2.5. Since x is a random variable in the Bayesian arguments, we use the
conditional probability p(y|x), instead of p(y).
Let us derive a cost function for estimating x. Taking a logarithm of the Bayes’s
rule in Eq. (B.3) in the Appendix, we have
log p(x|y) = log p(y|x) + log p(x) + C,
(2.56)
whereC representstheconstantterms.NeglectingC,thecostfunctionF(x)ingeneral
form is obtained as
F(x) = −2 log p(x|y) = β∥y −Fx∥2 −2 log p(x).
(2.57)
The ﬁrst term on the right-hand side is a squared error term, which expresses how
well the solution x ﬁts the sensor data y. The second term −2 log p(x) is a constraint
imposed on the solution. The above equation indicates that the constraint term in the
cost function is given from the prior probability distribution in the Bayesian formu-
lation. The optimum estimate of x is obtained by minimizing the cost function F(x).
2.10.2 L2-Regularized Method
Let us assume the following Gaussian distribution for the prior probability distribu-
tion of x,
p(x) =
 α
2π
N/2
exp

−α
2 ∥x∥2
.
(2.58)

2.10 Bayesian Derivation of the Minimum-Norm Method
25
Substituting Eq. (2.58) into (2.57), we get the cost function
F(x) = β∥y −Fx∥2 + α∥x∥2.
(2.59)
The cost function in Eq. (2.59) is the same as the cost function in Eq. (2.37), assuming
λ = β/α. Thus, the solution obtained by minimizing this cost function is equal to the
solution of the L2-norm regularized minimum-norm method introduced in Sect.2.8.
To obtain the optimum estimate of x, we should compute the posterior distribution.
In this case, the posterior is known to have a Gaussian distribution because p(y|x)
and p(x) are both Gaussian, and the mean and the precision matrix of this posterior
distribution is derived as in Eqs.(B.24) and (B.25). Substituting Φ = αI and Λ = βI
into these equations, we have
Γ = αI + βFT F,
(2.60)
¯x(t) =

FT F + α
β I
−1
FT y(t).
(2.61)
The Bayesian solution which minimizes the cost function in Eq. (2.59) is given in
Eq. (2.61). This solution is the same as Eq. (2.39). Comparison between Eqs.(2.61)
and (2.39) shows that the regularization constant is equal to α/β, which is the inverse
ofthesignal-to-noiseratioofthesensordata.Thisisinaccordancewiththearguments
in Sect.2.8 that when the sensor data contains larger amounts of noise, a larger
regularization constant must be used.
The optimum values of the hyperparameters α and β can be obtained using the EM
algorithm, as described in Sect.B.5.6. The update equations for the hyperparameters
are:
α−1 =
1
3N

1
K
K
	
k=1
¯xT (tk)¯x(tk) + tr

Γ −1
,
(2.62)
β−1 = 1
M

1
K
K
	
k=1
∥y(tk) −F ¯x(tk)∥2 + tr

FT FΓ −1
.
(2.63)
Here, we assume that multiple K time-point data is available to determine α and β.
The Bayesian minimum-norm method is summarized as follows. First, Γ and
¯x(tk) are computed using Eqs. (2.60) and (2.61) with initial values set to α and β.
Then, the values of α and β are updated using (2.62) and (2.63). Using the updated
α and β, the values of Γ and ¯x(tk) are updated using Eqs. (2.60) and (2.61). These
procedures are repeated and the resultant ¯x(tk) is the optimum estimate of x(tk).
The EM iteration may be stopped by monitoring the marginal likelihood, which
is obtained using Eq. (B.29) as
log p(y(t1), . . . , y(tK )|α, β) = −1
2 K log |Σ y| −1
2
K
	
k=1
yT (tk)Σ−1
y y(tk), (2.64)

26
2
Minimum-Norm-Based Source Imaging Algorithms
where according to Eq. (B.30), Σ y is expressed as
Σ y = β−1I + α−1FFT .
(2.65)
If the increase of the likelihood in Eq. (2.64) with respect to the iteration count
becomes very small, the iteration may be stopped.
2.10.3 L1-Regularized Method
The method of L1-norm regularization can also be derived based on the Bayesian
formulation. To derive the L1-regularization. we use the Laplace distribution as the
prior distribution
p(x) =
N

j=1
1
2b exp

−1
b|x j|

.
(2.66)
Then, using Eq. (2.57), (and replacing F with H), the cost function is derived as
F(x) = β∥y −Hx∥2 + 2b
N
	
j=1
|x j|,
(2.67)
which is exactly equal to Eq. (2.53), if we set ξ = 2b/β.
Another formulation for deriving the L1-regularized method is known. It uses the
framework of the sparse Bayesian learning described in Chap.4. In Chap.4, assuming
the Gaussian prior,
p(x|α) =
N

j=1
N(x j|0, α−1
j ) =
N

j=1
α j
2π
1/2
exp

−α j
2 x2
j

,
(2.68)
we derive the marginal likelihood for the hyperparameter α = [α1, . . . , αN],
p(y|α), using,
p(y|α) =

p(y|x)p(x|α)dx,
(2.69)
and eventually derive the Champagne algorithm. However, instead of implementing
Eq. (2.69), there is another option in which we compute the posterior distribution
p(x|y) using
p(x|y) ∝

p(y|x)p(x|α)p(α)dα = p(y|x)p(x),
(2.70)

2.10 Bayesian Derivation of the Minimum-Norm Method
27
where
p(x) =

p(x|α)p(α)dα.
(2.71)
The estimate x is, then, obtained by
x = argmax
x
p(y|x)p(x).
To compute p(x) using Eq. (2.71), we need to specify the hyperprior p(α).
However, we usually have no such information and may use noninformed prior
p(α) = const. Substituting this ﬂat prior into Eq. (2.71), we have
p(x) ∝

p(x|α)dα.
However, the integral in the above equation is difﬁcult to compute. The formal
procedure to compute p(x) in this case is to ﬁrst assume the Gamma distribution for
the hyperprior p(α), such that
p(α) =
N

j=1
p(α j) =
N

j=1
Γ (a)−1ba(α j)a−1e−bα j .
(2.72)
Then, p(x) in Eq. (2.71) is known to be obtained as Student t-distribution, such
that [8]
p(x j) =

p(x j|α j)p(α j)dα j
=
 α j
2π
1/2
exp

−α j
2 x2
j

ba
Γ (a)

α j
a−1 e−bα j dα j
= baΓ (a + 1
2)
√
2πΓ (a)

b +
x2
j
2
−(a+ 1
2 )
.
(2.73)
We then assume that a →0 and b →0, (which is equivalent to making p(α) a
noninformed prior,) p(x j) then becomes
p(x j) →
1
|x j|
i.e.
p(x) →
N

j=1
1
|x j|.
(2.74)
Using Eq. (2.57), the cost function, in this case, is derived as
F(x) = β∥y −Fx∥2 +
N
	
j=1
log |x j|.
(2.75)

28
2
Minimum-Norm-Based Source Imaging Algorithms
This Eq. (2.75) is not exactly equal to the L1-norm cost function, since the constraint
term is not equal to N
j=1 |x j| but has form of N
j=1 log |x j|. Since these constraint
terms have similar properties, the solution obtained by minimizing this cost function
has a property very similar to the L1-norm-regularized minimum-norm solution.
Related arguments are found in Chap.6.
References
1. M.S. Hämäläinen, R.J. Ilmoniemi, Interpreting measured magnetic ﬁelds of the brain: estimates
of current distributions. Technical Report TKK-F-A559, Helsinki University of Technology
(1984)
2. M.S. Hämäläinen, R.J. Ilmoniemi, Interpreting magnetic ﬁelds of the brain: minimum norm
estimates. Med. Biol. Eng. Comput. 32, 35–42 (1994)
3. J. Sarvas, Basic mathematical and electromagnetic concepts of the biomagnetic inverse problem.
Phys. Med. Biol. 32, 11–22 (1987)
4. K. Uutela, M. Hämäläinen, E. Somersalo, Visualization of magnetoencephalographic data using
minimum current estimate. NeuroImage 10, 173–180 (1999)
5. B.D. Jeffs, Maximally sparse constrained optimization for signal processing applications. Ph.D.
thesis, University of Southern California (1989)
6. K. Matsuura, Y. Okabe, Multiple current-dipole distribution reconstructed by modiﬁed selective
minimum-norm method, in Biomag 96, (Springer, Heidelberg, 2000), pp. 290–293
7. R. Tibshirani, Regression shrinkage and selection via the Lasso. J. R. Stat. Soc. Ser. B (Method-
ological) 58(1), 267–288 (1996)
8. M.E. Tipping, Sparse Bayesian learning and relevance vector machine. J. Mach. Learn. Res. 1,
211–244 (2001)

Chapter 3
Adaptive Beamformers
3.1 Introduction and Basic Formulation
The beamformer is a location-dependent spatial ﬁlter applied to the sensor data. It
is used for estimating the strength of brain activity at a particular spatial location,
which is referred to as the beamformer pointing location. Thus, the beamformer may
be interpreted as a technique that forms a virtual sensor whose sensitivity pattern
is localized at its pointing location. By postprocessing, the pointing location can be
scanned over the source space to obtain the source three-dimensional reconstruction.
Let us assume momentarily that the source orientation is predetermined and avoid
the orientation estimation. In this case, using the data vector, y(t), the beamformer
reconstructs the source magnitude, s(r, t), using
s(r, t) = wT (r)y(t),
(3.1)
wheres(r, t) is the estimated source magnitude at location r and time t. In Eq. (3.1),
a column vector w(r) expresses the beamformer’s weight, which characterizes the
properties of the beamformer. There are two types of beamformers. One is the non-
adaptive beamformer in which the weight vector depends solely on the lead ﬁeld
of the sensor array, and the other is the adaptive beamformer in which the weight
depends on the measured data as well as the lead ﬁeld of the sensor array.
Adaptive beamformers were originally developed in the ﬁeld of seismic explo-
ration [1] and introduced later into the ﬁeld of electromagnetic brain imaging [2–5].
In recent years, brain imaging with adaptive beamformers has increasingly been used
for clinical and basic human neuroscience studies, and it is a very popular method
for analyzing rhythmic brain activity. You may ﬁnd detailed arguments on various
aspects of adaptive beamformers in [6].
This chapter presents a concise review on the adaptive beamformers. It presents
Bayesian-ﬂavored formulations of scalar and vector adaptive beamformers, as well
as the conventional derivations. It also describes the narrow-band beamformer and
its application to ﬁve-dimensional (space–time–frequency) brain imaging [7], which
can be used for the source-space connectivity analysis.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_3
29

30
3
Adaptive Beamformers
3.2 Classical Derivation of Adaptive Beamformers
3.2.1 Minimum-Variance Beamformers
with Unit-Gain Constraint
The weight vector of the adaptive beamformer is derived using the optimization:
w(r) = argmin
w(r)
wT (r)Rw(r),
subject to wT (r)l(r) = 1.
(3.2)
Here, R is the data covariance matrix obtained using ⟨y(t)yT (t)⟩where ⟨· ⟩indi-
cates the ensemble average. In Eq.(3.2), the inner product wT (r)l(r) represents the
beamformer output from a unit-magnitude source located at r. Therefore, setting
wT (r)l(r) = 1 guarantees that the beamformer passes the signal from r with the
gain equal to one. The constraint wT (r)l(r) = 1 is called the unit-gain constraint.
The output power of the beamformer wT (r)Rw(r) generally contains not only the
noise contributions but also unwanted contributions such as the inﬂuence of sources
at locations other than r. Accordingly, by minimizing the output power with this
unit-gain constraint, we can derive a weight that minimizes such unwanted inﬂuence
without affecting the signal coming from r, the pointing location of the beamformer.
This constrained minimization problem can be solved using a method of the
Lagrange multiplier. We deﬁne the Lagrange multiplier as a scalar ζ, and the
Lagrangian as L(w, ζ), such that
L(w, ζ) = wT Rw + ζ(wT l(r) −1),
(3.3)
where the explicit notation of (r) is omitted from w(r) for simplicity. The weight
vector satisfying Eq. (3.2) can be obtained by minimizing the Lagrangian L(w, ζ)
in Eq. (3.3) with no constraints.
The derivative of L(w, ζ) with respect to w is given by:
∂L(w, ζ)
∂w
= 2Rw + ζl(r).
(3.4)
By setting the right-hand side of the above equation to zero, we obtain
w = −ζ R−1l(r)/2.
(3.5)
Substituting this relationship back into the constraint equation wT l(r) = 1, we
get ζ = −2/[lT (r)R−1l(r)]. Substituting this ζ into Eq. (3.5), the weight vector
satisfying Eq. (3.2) is obtained as
w(r) =
R−1l(r)
[lT (r)R−1l(r)]
.
(3.6)

3.2 Classical Derivation of Adaptive Beamformers
31
UsingEq.(3.1),theestimatedsourcemagnitude(thebeamformeroutput)isexpressed
as
s(r, t) = lT (r)R−1 y(t)
[lT (r)R−1l(r)]
,
(3.7)
and the output power is given by
⟨s(r, t)2⟩=
1
[lT (r)R−1l(r)]
.
(3.8)
3.2.2 Minimum-Variance Beamformer
with Array-Gain Constraint
We have derived the minimum-variance beamformer with the unit-gain constraint,
wT (r)l(r) = 1. However, imposing the unit-gain constraint is somewhat ad hoc, and
there may be other possibilities. In electromagnetic brain imaging, the norm of the
lead ﬁeld ∥l(r)∥represents the gain of the sensor array, and is spatially dependent.
Particularly, when the spherical homogeneous conductor model1 is used, ∥l(r)∥is
zero at the center of the sphere and a false intensity increase around the center of
the sphere arises, because the weight vector (in Eq. (3.6)) becomes inﬁnite at the
center of the sphere. Such false results occur because the unit-gain constraint forces
a nonzero gain at a location where the sensor array has zero gain.
When ∥l(r)∥has a spatial dependence, it is more reasonable to use the constraint
wT (r)l(r) = ∥l(r)∥. We can derive, by using wT (r)l(r) = ∥l(r)∥, a beamformer
whose gain exactly matches the gain of the sensor array. The weight, in this case, is
obtained using
w(r) = argmin
w(r)
wT (r)Rw(r),
subject to wT (r)l(r) = ∥l(r)∥.
(3.9)
Exactly, the same derivation used for deriving Eq. (3.6) leads to the weight vector:
w(r) =
R−1l(r)
[lT (r)R−1l(r)]
,
(3.10)
where l(r) is the normalized lead ﬁeld vector deﬁned as l(r) = l(r)/∥l(r)∥. In
Eq. (3.10), the weight is independent of the norm of the lead ﬁeld, and we can avoid
the artifacts around the center of the sphere. This type of beamformer is referred to as
the array-gain minimum-variance beamformer [6], and the constraint wT (r)l(r) =
∥l(r)∥is referred to as the array-gain constraint. The estimated source magnitude
for the array-gain minimum-variance beamformer is given by
1 An explanation of the spherical homogeneous conductor model is given in the Appendix A.

32
3
Adaptive Beamformers
s(r, t) =
lT (r)R−1 y(t)
[lT (r)R−1l(r)]
,
(3.11)
and the output power is expressed as
⟨s(r, t)2⟩=
lT (r)l(r)
[lT (r)R−1l(r)]
.
(3.12)
3.2.3 Minimum-Variance Beamformer
with Unit-Noise-Gain Constraint
Another possible constraint is the unit-noise-gain constraint, which is expressed as
wT (r)w(r) = 1. That is, the ﬁlter weight is obtained using
w(r) = argmin
w(r)
wT (r)Rw(r), subject to wT (r)l(r) = τ,
and wT (r)w(r) = 1,
(3.13)
where the minimization problem is solved with the ﬁrst constraint, wT (r)l(r) = τ.
The scalar constant τ is determined by the second constraint, wT (r)w(r) = 1. To
obtain the weight vector derived from the above minimization, we ﬁrst calculate the
weight using
w(r) = argmin
w(r)
wT (r)Rw(r) subject to wT (r)l(r) = τ.
(3.14)
Following the same steps from Eqs. (3.3) to (3.6), the weight satisfying Eq. (3.14) is
obtained as
w(r) = τ
R−1l(r)
[lT (r)R−1l(r)]
.
(3.15)
Substituting this expression to wT (r)w(r) = 1 leads to
τ =
lT (r)R−1l(r)

lT (r)R−2l(r)
,
(3.16)
and the weight is given by:
w(r) =
R−1l(r)

lT (r)R−2l(r)
.
(3.17)

3.2 Classical Derivation of Adaptive Beamformers
33
This weight vector again does not depend on the norm of the lead ﬁeld ∥l(r)∥. The
output power of this beamformer is given by
⟨s(r, t)2⟩= lT (r)R−1l(r)
[lT (r)R−2l(r)]
.
(3.18)
This beamformer was ﬁrst proposed by Borgiotti and Kaplan [8] and it is referred to
as the unit-noise-gain (constraint) minimum-variance beamformer,2 or the weight-
normalized minimum-variance beamformer.
3.3 Semi-Bayesian Derivation of Adaptive Beamformers
The adaptive beamformer can be derived based on a Bayesian formulation [9]. Let
the relationship between the sensor data y(t) and the voxel source distribution x(t)
be expressed in Eq. (2.15), rewritten here as:
y(t) = Hx(t) + ε,
where H is the lead ﬁeld matrix deﬁned in Eq. (2.11). We also assume that the noise
in the sensor data is assumed such that
ε ∼N(ε|0, σ2I).
(3.19)
We assume that the prior distribution for the source vector x(t) is the zero mean
Gaussian with a diagonal precision matrix, i.e.,
p(x(t)) = N(x(t)|0, Φ−1),
(3.20)
where the precision matrix is expressed as
Φ =
⎡
⎢⎣
α1 · · · 0
... ...
...
0 · · · αN
⎤
⎥⎦.
An entirely rigorous Bayesian treatment using this prior distribution leads to the
Champagne algorithm, which is the topic of Chap.4. In the following, we show that
adaptive beamformer algorithm can also be derived using this prior distribution.
2 This name comes from the fact that spatial ﬁlter’s noise gain is equal to the squared weight norm
∥w(r)∥2.

34
3
Adaptive Beamformers
The prior distribution in Eq. (3.20) leads to Bayes’ estimate of x expressed in
Eq. (B.26) in Appendix. Setting the noise precision Λ equal to σ−2I in Eq. (B.26),
we get
¯x(t) = Φ−1HT Σ−1
y y(t),
(3.21)
where
Σ y = HΦ−1HT + σ2I
is the model data covariance matrix, which is the covariance of the marginal dis-
tribution p(y(t)). Taking a look at the jth component of ¯x(t) in Eq. (3.21), we
have
¯x j(t) = s(r j, t) = α−1
j lT
j Σ−1
y y(t),
(3.22)
where l j is the lead ﬁeld vector: l j = l(r j). In the equation above, the voxel
precision α j is an unknown quantity. Thus, computing ¯x j(t) ﬁrst requires estimating
α j. A rigorous Bayesian treatment for estimating the voxel precision leads to the
Champagne algorithm. Here, we perform a non-Bayesian treatment for estimating
α j.
Let us treat Φ as a deterministic (i.e., nonrandom variable) unknown matrix. Using
Eq. (3.22), we compute the power of ¯x j(t) as
⟨¯x j(t)2⟩= α−2
j lT
j Σ−1
y ⟨y(t)yT (t)⟩Σ−1
y l j.
(3.23)
If we assume that ⟨y(t)yT (t)⟩is equal to the model data covariance Σ y, Eq. (3.23)
may be reexpressed as
⟨s(r j, t)2⟩= α−2
j lT
j Σ−1
y Σ yΣ−1
y l j = α−2
j lT
j Σ−1
y l j.
(3.24)
Next we assume that the relationship,
⟨s(r j, t)2⟩= α−1
j
(3.25)
holds. This relationship indicates that the reconstructed power of a brain source is
equal to its true power, and this is equal to the unit-gain constraint.
Assuming that the unit-gain constraint holds, we have
α−1
j
= α−2
j lT
j Σ−1
y l j,
and thus,
α−1
j
=
1
lT
j Σ−1
y l j
.
(3.26)

3.3 Semi-Bayesian Derivation of Adaptive Beamformers
35
Substituting this equation into Eq. (3.22), we can derive the beamformer expression,
s(r j, t) = wT (r j)y(t),
(3.27)
where the weight vector is expressed as
w(r j) =
Σ−1
y lT
j
lT
j Σ−1
y l j
.
(3.28)
If the model data covariance matrix Σ y is replaced with the sample data covariance
matrix R, the weight equation (3.28) becomes
w(r j) =
R−1lT (r j)
lT (r j)R−1l(r j)
,
(3.29)
which is exactly equal to the weight expression of the minimum-variance beam-
former. To derive the array-gain constraint beamformer, we use the relationship,
⟨s(r j, t)2⟩= α−1
j ∥l(r j)∥,
(3.30)
The weight vector in this case is obtained as
w(r j) =
R−1l(r j)
[lT (r j)R−1l(r j)]
,
(3.31)
which is equal to the weight expression of the array-gain constraint minimum-
variance beamformer.
3.4 Diagonal-Loading and Bayesian Beamformers
The sample covariance matrix sometimes has a large condition number. This situation
happens, for example, when the number of time samples is considerably fewer than
the size of the matrix or when the SNR of the sensor data is very high. In such
cases, since the direct inversion of the sample covariance R−1 may cause numerical
instability, the regularized inverse, (R+κI)−1, may be a better approximation of the
inverse of the model data covariance Σ−1
y , where κ is a small positive real number
called the regularization constant. This gives the weight expression,
w(r) =
(R + κI)−1l(r)
[lT (r)(R + κI)−1l(r)]
.
(3.32)

36
3
Adaptive Beamformers
The beamformer that uses the above weight is called the diagonal-loading minimum-
variancebeamformer[10].Thediagonalloadingisaformofregularizationusedwhen
inverting the sample covariance matrix, and is nearly equivalent to adding noise to
the sensor data. Contrary to the L2-regularization in the minimum-norm method, the
diagonal loading is not needed when the SNR of the sensor data is low, because the
condition number of the sample covariance is generally low in such cases.
The diagonal loading is needed when the sensor data has high SNR, because,
in such cases, a signiﬁcant SNR degradation is caused due to the problem of array
mismatch [6], which indicates a situation where the lead ﬁeld used for computing the
beamformer’s weight vector is different from the true lead ﬁeld. When computing a
weight vector, the exact lead ﬁeld is generally unknown and the lead ﬁeld is usually
estimated from some kind of forward models such as the homogeneous conductor
model. Thus, we cannot completely avoid this array mismatch problem. The diagonal
loading can reduce the SNR degradation due to the array mismatch. On the other
hand, however, it degrades the spatial resolution of reconstructed source distribution,
so it provides a trade-off between the SNR degradation and spatial resolution [6].
The sample data covariance R, which is the maximum-likelihood estimate of the
model data covariance, is not necessarily the best estimate of Σ y. We can obtain
a better estimate by applying the Bayesian factor analysis described in Chap.5.
Applying the VBFA algorithm, we can get an estimate of the data covariance ¯R, as
shown in Eq. (5.104). The beamformer with a weight expression computed using a
Bayesian-inferred data covariance matrix, (such as ¯R in Eq. (5.104)), is called the
Bayesian beamformer [11]. Note that a Bayesian-inferred data covariance matrix
has an intrinsic regularization term, and thus the regularization is embedded in the
Bayesian beamformer.
3.5 Scalar Adaptive Beamformer with Unknown
Source Orientation
3.5.1 Expressions for the Unit-Gain Constraint Beamformer
So far, we have derived the weight of the adaptive beamformer by assuming that the
source orientation is predetermined. The source orientation may be predetermined
using an accurate three-dimensional anatomical image of the subject, if available.
However, in general, the source orientation η(r) is an unknown quantity, and should
be estimated from the data. There are two types of beamformers that can handle the
estimation of the source vector. One is the scalar-type and the other is the vector-
type adaptive beamformer. In the following, we ﬁrst describe the scalar adaptive
beamformer [5].
In the scalar-type adaptive beamformer, the weight is ﬁrst formulated using the
unknown source orientation η, such that [5]

3.5 Scalar Adaptive Beamformer with Unknown Source Orientation
37
w(r, η) =
R−1L(r)η
[ηT LT (r)R−1L(r)η],
(3.33)
and the output power obtained using this weight is given by
⟨s(r, η)2⟩=
1
ηT [LT (r)R−1L(r)]η .
(3.34)
The source orientation is then determined by maximizing this output power. That is,
the optimum orientation ηopt(r) is derived as [5]
ηopt(r) = argmax
η(r)

1
ηT (r)LT (r)R−1L(r)η(r)

.
(3.35)
According to the Rayleigh–Ritz formula in Sect.C.9, the orientation ηopt(r) is
obtained as
ηopt(r) = argmin
η(r)

ηT (r)[LT (r)R−1L(r)]η(r)

= ϑmin{LT (r)R−1L(r)},
(3.36)
where ϑmin{ · } indicates the eigenvector corresponding to the minimum eigenvalue
of the matrix in the curly braces.3 Namely, the optimum orientation ηopt(r) is given
by the eigenvector corresponding to the minimum eigenvalue of LT (r)R−1L(r).
Once ηopt(r) is obtained, the explicit form of the weight vector for the scalar
minimum-variance beamformer is expressed as
w(r) =
R−1L(r)ηopt(r)
[ηT
opt(r)LT (r)R−1L(r)ηopt(r)].
(3.37)
The output power is given by
⟨s(r, t)2⟩=
1
[ηT
opt(r)LT (r)R−1L(r)ηopt(r)] =
1
Smin{LT (r)R−1L(r)}, (3.38)
where Smin{ · } is the minimum eigenvalue of the matrix in the curly braces.
3.5.2 Expressions for the Array-Gain and Weight-Normalized
Beamformers
For the scalar-type array-gain minimum-variance beamformer, the optimum orien-
tation is derived using
3 The notations such as ϑmin{ · } and Smin{ · } are deﬁned in Sect.C.9 in the Appendix.

38
3
Adaptive Beamformers
ηopt(r) = argmax
η(r)

ηT (r)[LT (r)L(r)]η(r)
ηT (r)[LT (r)R−1L(r)]η(r)

,
(3.39)
which can be computed using
ηopt(r) = ϑmin{LT (r)R−1L(r), LT (r)L(r)}.
(3.40)
Once ηopt(r) is obtained, the weight vector is obtained using Eq. (3.10) with l(r) =
L(r)ηopt(r) andl(r) = l(r)/∥l(r)∥. The output power of this scalar beamformer is
given by
⟨s(r, t)2⟩=
1
Smin{LT (r)R−1L(r), LT (r)L(r)}.
(3.41)
For the scalar-type weight-normalized minimum-variance beamformer, the opti-
mum orientation is derived using
ηopt(r) = argmax
η(r)

ηT (r)[LT (r)R−1L(r)]η(r)
ηT (r)[LT (r)R−2L(r)]η(r)

,
(3.42)
which can be computed using
ηopt(r) = ϑmin{LT (r)R−2L(r), LT (r)R−1L(r)}.
(3.43)
Once ηopt(r) is obtained, the weight vector is obtained using Eq. (3.17) with l(r) =
L(r)ηopt(r). The output power of this scalar beamformer is given by
⟨s(r, t)2⟩=
1
Smin{LT (r)R−2L(r), LT (r)R−1L(r)}.
(3.44)
3.6 Vector-Type Adaptive Beamformer
3.6.1 Vector Beamformer Formulation
The vector adaptive beamformer is another type of adaptive beamformers that can
reconstruct the source orientation as well as the source magnitude. It uses a set of
three weight vectors wx(r), wy(r), and wz(r), which, respectively, detect the x, y,
and z components of the source vector s(r, t). That is, the weight matrix W(r) is
deﬁned as
W(r) = [wx(r), wy(r), wz(r)],
(3.45)

3.6 Vector-Type Adaptive Beamformer
39
and the source vector can be estimated using
s(r, t) = [sx(r, t),sy(r, t),sz(r, t)]T = W T (r)y(t).
(3.46)
To derive the weight matrix of a vector-type minimum-variance beamformer, we
use the optimization
W(r) = argmin
W(r)
tr[W T (r)RW(r)], subject to
W T (r)L(r) = I.
(3.47)
A derivation similar to that for Eq. (3.6) leads to the solution for the weight matrix
W(r), which is expressed as [4]
W(r) = R−1L(r)[LT (r)R−1L(r)]−1.
(3.48)
The beamformer that uses the above weight is called the vector-type adaptive beam-
former. Using the weight matrix above, the source-vector covariance matrix is esti-
mated as
⟨s(r, t)sT (r, t)⟩= [LT (r)R−1L(r)]−1,
(3.49)
and the source power estimate is obtained using
⟨∥s(r, t)∥2⟩= ⟨s(r, t)2⟩= tr

[LT (r)R−1L(r)]−1
.
(3.50)
This vector-type beamformer is sometimes referred to as the linearly constrained
minimum-variance (LCMV) beamformer [4].
The weight matrix of the array-gain vector beamformer is derived, such that
W(r) = argmin
W(r)
tr

W T (r)RW(r)

,
subject to
W T (r)L(r) = ∥L(r)∥.
(3.51)
The vector version of the array-gain minimum-variance beamformer is expressed as
W(r) = R−1L(r)[LT (r)R−1L(r)]−1,
(3.52)
where L(r) is the normalized lead ﬁeld matrix deﬁned as L(r) = L(r)/∥L(r)∥.
The source-vector covariance matrix is estimated as
⟨s(r, t)sT (r, t)⟩= [LT (r)R−1L(r)]−1.
(3.53)

40
3
Adaptive Beamformers
The source power estimate is obtained by
⟨∥s(r, t)∥2⟩= ⟨s(r, t)2⟩= tr

[LT (r)R−1L(r)]−1
.
(3.54)
The vector-type beamformer can be formulated with the unit-noise-gain constraint,
and the detail of the formulation is found in [6, 12].
3.6.2 Semi-Bayesian Formulation
The weight matrix of a vector-type adaptive beamformer can be derived using the
same Bayesian formulation in Sect.3.3. The prior distribution in Eq. (3.20) and the
noise assumption in Eq. (3.19) lead to the Bayesian estimate of x in Eq. (3.21), which
is written as
¯x(t) = Φ−1FT Σ−1
y y(t),
(3.55)
where, ¯x(t) is expressed as
¯x(t) =
⎡
⎢⎢⎢⎣
s(r1, t)
s(r2, t)
...
s(r N, t)
⎤
⎥⎥⎥⎦.
(3.56)
To derive the vector beamformer, we use the matrix Φ−1 which is a 3N×3N Block
diagonal matrix such that
Φ−1 =
⎡
⎢⎢⎢⎢⎣
Υ 1 0 · · ·
0
0 Υ 2 ·
...
...
·
...
0
0 · · · 0 Υ N
⎤
⎥⎥⎥⎥⎦
,
(3.57)
where the 3 × 3 matrix Υ j is the prior covariance matrix of the source vector at the
jth voxel. Thus, the estimated source vector for the jth voxel is obtained as
s(r j, t) = Υ j LT
j Σ−1
y y(t).
(3.58)
Computing the estimated source-vector covariance matrix, ⟨s(r j, t)sT (r j, t)⟩,
leads to

3.6 Vector-Type Adaptive Beamformer
41
⟨s(r j, t)sT (r j, t)⟩= Υ j LT
j Σ−1
y ⟨y(t)yT (t)⟩Σ−1
y L jΥ j
= Υ j LT
j Σ−1
y L jΥ j,
(3.59)
where we assume ⟨y(t)yT (t)⟩= Σ y.
In this vector case, the unit-gain constraint is expressed as
⟨s(r j, t)sT (r j, t)⟩= Υ j.
(3.60)
Imposing this relationship, we get,
Υ j = Υ j LT
j Σ−1
y L jΥ j,
and
Υ j =

LT
j Σ−1
y L j
−1
.
(3.61)
Substituting the above equation into Eq. (3.58), we obtain
s(r j, t) =

LT
j Σ−1
y L j
−1
LT
j Σ−1
y y(t).
(3.62)
Assuming that the model data covariance can be replaced by the sample data covari-
ance R, Eq. (3.62) is rewritten as
s(r, t) = W T (r j)y(t),
(3.63)
where the weight matrix is given by
W(r j) = R−1L(r j)

LT (r j)R−1L(r j)
−1
.
(3.64)
The weight matrix of the vector array-gain minimum-variance beamformer can be
derived using the array-gain constraint,
⟨s(r j, t)sT (r j, t)⟩= Υ j∥L(r j)∥.
(3.65)
Substituting this relationship into Eq. (3.59) and using Eq. (3.58), the resultant weight
matrix is expressed as
W(r j) = R−1L(r j)

LT (r j)R−1L(r j)
−1
.
(3.66)

42
3
Adaptive Beamformers
3.7 Narrow-Band Beamformer
3.7.1 Background
Stimulus-induced power modulation of spontaneous brain activity has been the sub-
ject of intense investigations. Such power modulation is sometimes referred to as the
event-related spectral power change. When the power change is negative, it is cus-
tomarily termed as event-related desynchronization (ERD), and when it is positive,
it is termed as event-related synchronization (ERS) [13].
The narrow-band dual-state beamformer [7] is a powerful tool for localization of
speciﬁc brain activities related to these power changes. This is because the power
change is frequency-speciﬁc, and the narrow-band beamformer uses a weight tuned
to a speciﬁc target frequency. In this section, we describe the time-domain and the
frequency-domain implementations of the narrow-band beamformer.
3.7.2 Time-Domain Implementation
Since the induced brain activity is not time-locked to the stimulus, the sample covari-
ance matrix should be computed from nonaveraged raw trials. We assume that total
NE trials denoted as b1(t), . . . , bNE are obtained. To compute a frequency-speciﬁc
weight, these raw trials are band-pass ﬁltered with a speciﬁc frequency band of inter-
est. The band-pass ﬁltered sensor data for the nth trial is denoted asbn(t, f ), where
f represents the frequency band of interest.4
The sample covariance matrix is computed using the band-pass ﬁltered data, such
that
R( f ) =
1
NE
NE

n=1
K

k=1
bn(tk, f )bT
n (tk, f ).
(3.67)
Since we use the nonaveraged trial data, the data naturally contains a signiﬁcant
amount of inﬂuence from brain activities that are not related to the activity of interest.
To remove the inﬂuence of such unwanted brain activities, we use dual-state
datasets: one from the target time period and the other from the baseline period. We
try to reconstruct source activities that cause the power change in a speciﬁc frequency
band. That is, using Eq.(3.67), we compute the frequency-speciﬁc covariance matrix
for the target period, RT ( f ), and for the baseline period, RC( f ).
We then reconstruct frequency-speciﬁc power changes between the target and
baseline periods by using the F-ratio method. That is, using RT ( f ) and RC( f ), the
F-ratio method computes the frequency-speciﬁc ﬁlter weight such that
4 The notation f may indicate the center frequency of the frequency band of interest.

3.7 Narrow-Band Beamformer
43
w(r, f ) =
R−1
total( f )l(r)
lT (r)R−1
total( f )l(r)
,
(3.68)
where Rtotal( f ) = RT ( f ) + RC( f ). Then, the F-ratio image, F(r, f ), is obtained
such that
F(r, f ) = wT (r, f )RT ( f )w(r, f ) −wT (r, f )RC( f )w(r, f )
wT (r, f )RC( f )w(r, f )
.
(3.69)
On the right-hand side of Eq. (3.69), the ﬁrst term in the numerator represents the
reconstruction of the source power in the target period and the second represents that
in the control period. Thus, F(r, f ) represents the ratio of the reconstructed source
power change to the power of baseline activities.
3.7.3 Frequency-Domain Implementation
The narrow-band beamformer can also be implemented in the frequency domain. We
ﬁrst deﬁne the Fourier transform of the raw-trial vector bn(t) as gn( f ). The sample
cross-spectrum matrix R( f ) is computed using gn( f ), such that
R( f ) =
1
NE
NE

n=1
gn( f )gn( f )H,
(3.70)
where the superscript H indicates the Hermitian transpose (complex conjugation
plus matrix transpose). Using Eq. (3.70), we compute the frequency-speciﬁc cross-
spectral matrix for the target period, RT ( f ), and for the control period, RC( f ). The
frequency-selective weight w(r, f ) is obtained such that
w(r, f ) =
R−1
total( f )l(r)
lT (r)R−1
total( f )l(r)
,
(3.71)
where Rtotal( f ) = RT ( f ) + RC( f ). The pseudo F-ratio image is computed using
F(r, f ) = wH(r, f )RT ( f )w(r, f ) −wH(r, f )RC( f )w(r, f )
wH(r, f )RC( f )w(r, f )
.
(3.72)

44
3
Adaptive Beamformers
frequency
Baseline window
Target window
( ,
)
j
k
f t
( ,
)
j
c
f t
kt
jf
1
jf
1
jf
1
kt
2
kt
ct
time
Fig. 3.1 Depiction of time–frequency domain discretization. The target window is set at the one
represented by ( f j, tk), and the baseline window is set at the one represented by ( f j, tc)
3.7.4 Five-Dimensional Brain Imaging
Using the narrow-band dual-state beamformer, we can implement the ﬁve-
dimensional (time–frequency–space) imaging of brain activities [7]. In this imple-
mentation, we use the sliding window method in which the target window is moved
along the time and frequency directions. The discretization of the time–frequency
domain is depicted in Fig.3.1.
We assign the window denoted ( f j, tk) to the target time–frequency window, and
the window denoted ( f j, tc) to the baseline window. By implementing the narrow-
band dual-state beamformer using these two time–frequency windows, we obtain
the pseudo F-ratio image, F(r, f j, tk), which represents the source power difference
at the frequency f j between the time windows at tk and tc. If we move the target
window along the time and frequency directions, we can obtain the ﬁve-dimensional
source power difference map of the induced brain activity, F(r, f j, tk), where j =
1, . . . , N f and k = 1, . . . , K, where N f is the number of frequency bins and K is
the number of time windows.
3.8 Nonadaptive Spatial Filters
3.8.1 Minimum-Norm Filter
There is a different class of beamformers that uses the nonadaptive weight, the
weight computed only using the sensor lead ﬁeld. These are customarily called as

3.8 Nonadaptive Spatial Filters
45
the nonadaptive spatial ﬁlters.5 A representative and basic nonadaptive spatial ﬁlter
is the minimum-norm ﬁlter, which is the spatial ﬁlter version of the minimum-norm
method described in Chap.2.
The minimum-norm source reconstruction method is formulated as a nonadaptive
spatial ﬁlter in the following manner. In Eq. (2.26), FFT is expressed such that [14]
FFT =
N

n=1
L(rn)LT (rn) ≈

Ω
L(r)LT (r) dr = G,
(3.73)
where Ω indicates the source space. That is, FFT is equal to the gram matrix G, if
we ignore the voxel discretization error. Thus, let us rewrite Eq. (2.26) as
⎡
⎢⎢⎢⎣
s(r1, t)
s(r2, t)
...
s(r N, t)
⎤
⎥⎥⎥⎦= FT G−1 y(t) =
⎡
⎢⎢⎢⎣
LT (r1)
LT (r2)
...
LT (r N)
⎤
⎥⎥⎥⎦G−1 y(t).
(3.74)
We can see that, at each voxel location rn, the relationship,
s(rn, t) = LT (rn)G−1 y(t),
(3.75)
holds.
Theequationabovehasthesameformasthevector-typebeamformerinEq.(3.46),
i.e., Eq. (3.75) is rewritten as
s(r, t) = W T (r)y(t),
(3.76)
where the weight matrix W(r) is given by:
W(r) = G−1L(r).
(3.77)
The two equations above indicate that the minimum-norm method is formulated as
a nonadaptive spatial ﬁlter, in which the weight matrix is given by Eq. (3.77). Also,
the L2-regularized version of the weight matrix is given by:
W(r) = (G + ξI)−1L(r),
(3.78)
where a scalar ξ is the regularization constant. The spatial ﬁlter whose weight is
expressed in either Eq. (3.77) or (3.78) is called the minimum-norm ﬁlter.
5 They might be also called as the nonadaptive beamformers, but this usage is uncommon.

46
3
Adaptive Beamformers
3.8.2 Weight-Normalized Minimum-Norm Filter
The weight-normalized minimum-norm ﬁlter has been proposed by Dale et al. and
the method is often called as dynamic statistical parametric mapping (dSPM) [15].
The idea is to normalize the minimum-norm weight with its weight’s norm to ensure
that the spatial distribution of the noise is uniform. The scalar-type weight is thus
given by
w(r) =
G−1l(r)
∥G−1l(r)∥
=
G−1l(r)

lT (r)G−2l(r)
.
(3.79)
The idea of weight normalization can be extended to derive the weight matrix for
the vector-type spatial ﬁlter, such that
W(r) =
G−1L(r)

tr

LT (r)G−2L(r)
.
(3.80)
Using this weight matrix, the source vector is estimated as
s(r, t) = W T (r)y(t) =
LT (r)G−1 y(t)

tr

LT (r)G−2L(r)
.
(3.81)
The weight matrix for the L2-regularized version is given by
W(r) =
(G + ξI)−1L(r)

tr

LT (r)(G + ξI)−2L(r)
.
(3.82)
3.8.3 sLORETA Filter
Standardizedlowresolutionelectromagnetictomography(sLORETA)wasoriginally
proposed by Pascual-Marqui [16]. The method can be reformulated as a nonadaptive
spatial ﬁlter. In this method, the minimum-norm ﬁlter outputs are normalized by the
quantity

lT (r)G−1l(r). This normalization is called as the standardization. The
scalar-type weight is given by
w(r) =
lG−1

lT (r)G−1l(r)
.
(3.83)
The extension to the vector-type sLORETA ﬁlter results in the weight matrix
expressed as
W(r) = G−1L(r)[LT (r)G−1L(r)]−1/2,
(3.84)

3.8 Nonadaptive Spatial Filters
47
and the source vectors(r, t) is estimated as
s(r, t) = W T (r)y(t) = [LT (r)G−1L(r)]−1/2LT (r)G−1 y(t).
(3.85)
The weight matrix for the L2-regularized version is given by,
W(r) = (G + ξI)−1L(r)[LT (r)(G + ξI)−1L(r)]−1/2.
(3.86)
Although the standardization makes the sLORETA ﬁlter to have no localization bias
[6], the theoretical basis of this standardization is not entirely clear.
3.9 Recursive Null-Steering (RENS) Beamformer
3.9.1 Beamformer Obtained Based on Beam-Response
Optimization
In the beamformer formulation, the product, wT (r)l(r′), is called the beam response,
which expresses the sensitivity of the beamformer pointing at r to a source located
at r′. We wish to derive a beamformer that only passes the signal from a source at the
pointing location and suppresses the leakage from sources at other locations. Such a
beamformer may be derived by imposing a delta-function-like property on the beam
response. That is, the weight vector is obtained using
w(r) = argmin
w(r)

Ω

wT (r)l(r′) −δ(r −r′)
2
dr′.
(3.87)
The weight obtained from the optimization above is expressed as
w(r) = G−1l(r).
(3.88)
The beamformer in Eq. (3.88) is exactly equal to the minimum-norm ﬁlter.
Variants of the minimum-norm ﬁlter can be obtained by adding various constraints
to the optimization in Eq. (3.87) [17]. For example, the optimization
w(r) = argmin
w(r)

Ω

wT (r)l(r′) −δ(r −r′)
2
dr′
subject to wT (r)l(r) = 1,
(3.89)
leads to the weight expression
w(r) = [lT (r)G−1l(r)]−1G−1l(r).
(3.90)

48
3
Adaptive Beamformers
The beamformer using the above weight is called the unit-gain (constraint) minimum-
norm ﬁlter. Using the optimization
w(r) = argmin
w(r)

Ω

wT (r)l(r′) −δ(r −r′)
2
dr′
subject to wT (r)l(r) = ∥l(r)∥,
(3.91)
we obtain the array-gain (constraint) minimum-norm ﬁlter, such that
w(r) = [lT (r)G−1l(r)]−1G−1l(r),
(3.92)
wherel(r) = l(r)/∥l(r)∥.
3.9.2 Derivation of RENS Beamformer
The recursive null-steering (RENS) beamformer is derived from the following opti-
mization [18]:
w(r) = argmin
w(r)

Ω

wT (r)l(r′) −δ(r −r′)
2
s(r, t)2 dr′
subject to wT (r)l(r) = ∥l(r)∥,
(3.93)
wheres(r, t)2 istheinstantaneoussourcepower.Theideabehindtheaboveoptimiza-
tion is that we wish to impose a delta-function-like property on the beam response
only over a region where sources exist, instead of imposing that property over the
entire source space.
The ﬁlter weight is obtained as
w(r) = [lT (r) ¯G−1l(r)]−1 ¯G−1l(r),
(3.94)
where
¯G =

Ω
s(r, t)2l(r)lT (r) dr.
(3.95)
However, to compute the weight in Eq. (3.94), we need to know the source magni-
tude distribution s(r, t)2, which is unknown. Therefore, we use an estimated source
magnitudes(r, t)2 when computing ¯G, i.e.,
¯G =

Ω
s(r, t)2l(r)lT (r) dr.
(3.96)

3.9 Recursive Null-Steering (RENS) Beamformer
49
The proposed weight is therefore derived in a recursive manner. That is, by setting an
initials(r, t)2 to have a uniform value, the weight w(r) is derived using Eqs. (3.94)
and (3.96). The estimated source intensity s(r, t) is obtained using Eq. (3.1). This
s(r, t) is then used in Eqs. (3.94) and (3.96) to update w(r). These procedures are
repeated until some stopping criterion is satisﬁed. Here we describe a derivation of
the scalar-type RENS beamformer. An extension for deriving the vector-type RENS
beamformer is straightforward [18].
The RENS beamformer described here provides spatial resolution higher than
that of the nonadaptive spatial ﬁlters described in Sect.3.8. Moreover, it is free from
the limitations of adaptive beamformers. That is, the RENS beamformer is robust
to the source correlation problem. It does not require large time samples and works
even with single time point data.
References
1. J. Capon, High-resolution frequency wavenumber spectrum analysis. Proc. IEEE 57, 1408–
1419 (1969)
2. S.E. Robinson, D.F. Rose, Current source image estimation by spatially ﬁltered MEG, in
Biomagnetism Clinical Aspects, ed. by M. Hoke, et al. (Elsevier Science Publishers, New
York, 1992), pp. 761–765
3. M.E. Spencer, R.M. Leahy, J.C. Mosher, P.S. Lewis, Adaptive ﬁlters for monitoring localized
brain activity from surface potential time series, in Conference Record for 26th Annual Asilomer
Conference on Signals, Systems, and Computers, pp. 156–161, November 1992
4. B.D. Van Veen, W. Van Drongelen, M. Yuchtman, A. Suzuki, Localization of brain electrical
activity via linearly constrained minimum variance spatial ﬁltering. IEEE Trans. Biomed. Eng.
44, 867–880 (1997)
5. K. Sekihara, B. Scholz, Generalized Wiener estimation of three-dimensional current distribu-
tion from biomagnetic measurements, in Biomag 96: Proceedings of the Tenth International
Conference on Biomagnetism, ed. by C.J. Aine et al. (Springer, New York, 1996), pp. 338–341
6. K. Sekihara, S.S. Nagarajan, Adaptive Spatial Filters for Electromagnetic Brain Imaging
(Springer, Berlin, 2008)
7. S.S.Dalal,A.G.Guggisberg,E.Edwards,K.Sekihara,A.M.Findlay,R.T.Canolty,M.S.Berger,
R.T. Knight, N.M. Barbaro, H.E. Kirsch, S.S. Nagarajan, Five-dimensional neuroimaging:
localization of the time-frequency dynamics of cortical activity. NeuroImage 40, 1686–1700
(2008)
8. G. Borgiotti, L.J. Kaplan, Superresolution of uncorrelated interference sources by using adap-
tive array technique. IEEE Trans. Antennas Propag. 27, 842–845 (1979)
9. K. Sekihara, B. Scholz, Generalized Wiener estimation of three-dimensional current distribu-
tion from biomagnetic measurements. IEEE Trans. Biomed. Eng. 43, 281–291 (1996)
10. H. Cox, R.M. Zeskind, M.M. Owen, Robust adaptive beamforming. IEEE Trans. Signal
Process. 35, 1365–1376 (1987)
11. M. Woolrich, L. Hunt, A. Groves, G. Barnes, MEG beamforming using Bayesian PCA for
adaptive data covariance matrix regularization. Neuroimage 57(4), 1466–1479 (2011)
12. K. Sekihara, S.S. Nagarajan, D. Poeppel, A. Marantz, Y. Miyashita, Reconstructing spatio-
temporal activities of neural sources using an MEG vector beamformer technique. IEEE Trans.
Biomed. Eng. 48, 760–771 (2001)
13. G. Pfurtscheller, F.H. Lopes da Silva, Event-related EEG/MEG synchronization and desyn-
chronization: basic principles. Clin. Neurophysiol. 110, 1842–1857 (1999)

50
3
Adaptive Beamformers
14. K. Sekihara, M. Sahani, S.S. Nagarajan, Location bias and spatial resolution of adaptive and
non-adaptive spatial ﬁlters for MEG source reconstruction. NeuroImage 25, 1056–1067 (2005)
15. A.M. Dale, A.K. Liu, B.R. Fischl, R.L. Buckner, J.W. Belliveau, J.D. Lewine, E. Halgren,
Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imag-
ing of cortical activity. Neuron 26, 55–67 (2000)
16. R.D. Pascual-Marqui, Standardized low resolution brain electromagnetic tomography
(sLORETA): technical details. Methods. Find. Exp. Clin. Pharmacol. 24, 5–12 (2002)
17. R.E. Greenblatt, A. Ossadtchi, M.E. Pﬂieger, Local linear estimators for the bioelectromagnetic
inverse problem. IEEE Trans. Signal Process. 53, 3403–3412 (2005)
18. I. Kumihashi, K. Sekihara, Array-gain constraint minimum-norm spatial ﬁlter with recursively
updated gram matrix for biomagnetic source imaging. IEEE Trans. Biomed. Eng. 57(6), 1358–
1365 (2010)

Chapter 4
Sparse Bayesian (Champagne) Algorithm
4.1 Introduction
In this chapter, we provide a detailed description of an algorithm for electromagnetic
brain imaging, called the Champagne algorithm [1, 2]. The Champagne algorithm
is formulated based on an empirical Bayesian schema, and can provide a sparse
solution, since the sparsity constraint is embedded in the algorithm. The algorithm is
free from the problems that cannot be avoided in other sparse-solution methods, such
as the L1-regularized minimum-norm method. Such problems include the difﬁculty
in reconstructing voxel time courses or the difﬁculty in incorporating the source-
orientation estimation.
In Sect.2.10.2, we show that the L2-regularized minimum-norm method is derived
using the Gaussian prior for the jth voxel value,1
x j ∼N(x j|0, α−1),
(4.1)
where the precision α is common to all x j. In this chapter, we use the Gaussian prior
whose precision (variance) is speciﬁc to each x j, i.e.,
x j ∼N(x j|0, α−1
j ).
(4.2)
We show that this “slightly different” prior distribution gives a solution totally dif-
ferent from the L2-norm solution. Actually, the prior distribution in Eq.(4.2) leads to
a sparse solution. The estimation method based on the prior in Eq.(4.2) is called the
sparse Bayesian learning in the ﬁeld of machine learning [3, 4], and the source recon-
struction algorithm derived using Eq.(4.2) is called the Champagne algorithm [1].
In this chapter, we formulate the source reconstruction problem as the spatiotem-
poral reconstruction, i.e., the voxel time series x1, x2, . . . , xK is reconstructed using
the sensor time series y1, y2, . . . , yK where y(tk) and x(tk) are denoted yk and
xk. We use collective expressions x and y, indicating the whole voxel time series
1 We use the notational convenience N(variable|mean, covariance matrix) throughout this book.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_4
51

52
4
Sparse Bayesian (Champagne) Algorithm
x1, x2, . . . , xK and the whole sensor time series y1, y2, . . . , yK . We ﬁrst formulate
the Champagne algorithm omitting the source orientation estimation. That is, we
assume that the source orientation is predetermined at each voxel and use the mea-
surement model in Eq.(2.15). The relationship between xk and yk is expressed as
yk = Hxk + ε,
(4.3)
where the lead ﬁeld matrix H is deﬁned in Eq.(2.11).
4.2 Probabilistic Model and Method Formulation
Deﬁning a column vector α such that α = [α1, . . . , αN]T , the prior distribution is
expressed as
p(x|α) =
K

k=1
p(xk|α) =
K

k=1
N(xk|0, Φ−1),
(4.4)
where Φ is equal to Φ = diag(α), which indicates a diagonal matrix whose diagonal
elements are those of a vector in the parenthesis. Thus, we have
p(xk|α) = N(xk|0, Φ−1) =
N

j=1
N(x j|0, α−1
j ).
(4.5)
Since we assume that the noise ε is independent across time, the conditional proba-
bility p(y|x) is expressed as
p(y|x) =
K

k=1
p(yk|xk) =
K

k=1
N(yk|Hxk, β−1I).
(4.6)
In this chapter, the noise precision matrix is assumed to be βI where β is known.
Therefore, the parameters we must estimate is the voxel source distribution x and
the hyperparameter α.
In truly Bayesian formulation, we have to derive the joint posterior distribution
of all the unknown parameters, p(x, α|y), using
p(x, α|y) = p(y|x, α)p(x, α)
p(y)
.
(4.7)
However, we cannot compute2
p(y) =

p(y|x, α)p(x, α)dxdα,
(4.8)
2 In Eq.(4.8), the notation dx indicates dx1dx2 . . . dxK .

4.2 Probabilistic Model and Method Formulation
53
because the integral on the right-hand side does not have a closed-form solution.
Accordingly, the explicit form of p(y), and thus the joint distribution p(x, α|y)
cannot be obtained.
Therefore, instead, using
p(x, α|y) = p(x|y, α)p(α|y),
(4.9)
we can derive the posterior distribution p(x|y), such that
p(x|y) =

p(x, α|y)dα =

p(x|y, α)p(α|y)dα,
(4.10)
where we eliminate the unknown hyperparameter α by marginalization. Denoting
α such that α = argmax p(α|y), and assuming that p(α|y) has a (hopefully sharp)
peak at α, we can use the approximation
p(x|y) =

p(x|y, α)p(α|y)dα ≈

p(x|y, α)δ(α −α)dα = p(x|y, α),
(4.11)
where δ(α) indicates the delta function. The above equation shows that p(x|y, α),
(the posterior distribution obtained with setting α to α), approximates the true pos-
terior distribution p(x|y).
Using the arguments in Sect.B.3 in the Appendix, when p(x|α) and p(y|x) are
expressed in Eqs.(4.4) and (4.6), the posterior distribution p(x|y, α) is given by
p(x|y, α) =
K

k=1
p(xk|yk, α) =
K

k=1
N(xk|¯xk, Γ −1),
(4.12)
where the precision and the mean are obtained as
Γ = Φ + βHT H,
(4.13)
¯xk = βΓ −1HT yk.
(4.14)
Therefore, once α is obtained, we can compute Γ and ¯xk by substituting Φ =
diag(α) into Eqs.(4.13) and (4.14), and we obtain p(x|y, α) using Eq.(4.12).
The problem here is how to estimate α. The posterior distribution of the hyper-
parameter α, p(α|y), is expressed using the Bayes’ rule,
p(α|y) ∝p(y|α)p(α).
(4.15)
When we assume the ﬂat (noninformative) prior for p(α), we have
p(α|y) ∝p(y|α).
(4.16)
Thus, since the relationship,
argmax
α
p(α|y) = argmax
α
p(y|α)

54
4
Sparse Bayesian (Champagne) Algorithm
holds, α is obtained as the one that maximizes p(y|α). This p(y|α) is referred to
as the data evidence or the marginal likelihood.
Let us summarize the procedure to estimate the source distribution x. First, we
estimate the hyperparameter α by maximizing the marginal likelihood function,
α = argmax
α
p(y|α).
Next, this α is substituted into the posterior distribution p(x|y, α) to obtain
p(x|y, α). When this posterior is the Gaussian distribution in Eq.(4.12), the pre-
cision and mean are obtained by substituting Φ = diag(α) into Eqs.(4.13) and
(4.14). The voxel time courses are reconstructed by computing ¯xk in Eq.(4.14) for
k = 1, . . . , K.
4.3 Cost Function for Marginal Likelihood Maximization
Asdescribedintheprecedingsection,thehyperparameterα isestimatedbymaximiz-
ing the marginal likelihood p(y|α). In this section, we describe the maximization
of the marginal likelihood, and to do so, let us derive an explicit form of the log
marginal likelihood, log p(y|α). Substituting3
p(x|α) =
K

k=1
p(xk|α) =
 |Φ|1/2
(2π)N/2
K
exp

−1
2
K

k=1
xT
k Φxk
	
,
(4.17)
and
p(y|x) =
K

k=1
p(yk|xk) =

 β
2π
 M
2
	K
exp

−β
2
K

k=1
∥yk −Hxk∥2
	
,
(4.18)
into
p(y|α) =

p(y, x|α)dx =

p(y|x)p(x|α)dx,
we obtain
p(y|α) =

|Φ|1/2
(2π)N/2

 β
2π
M/2	K 
exp [−D] dx,
(4.19)
where
D = β
2
K

k=1
∥yk −Hxk∥2 + 1
2
K

k=1
xT
k Φxk,
(4.20)
3 Note that M and N are respectively the sizes of yk and xk.

4.3 Cost Function for Marginal Likelihood Maximization
55
and this D can be rewritten as
D = 1
2
K

k=1
(xk −¯xk)T Γ (xk −¯xk) + .
(4.21)
The derivation of the above equation is presented in Sect.4.10.1, and  on the right-
hand side of Eq.(4.21) is given in Eq.(4.97).
Using the results in Eq.(4.21), let us compute the integral on the right-hand side
of Eq.(4.19). First, we have

exp [−D] dx = exp [−]

exp

−1
2
K

k=1
(xk −¯xk)T Γ (xk −¯xk)
	
dx.
(4.22)
Considering the Gaussian with its mean ¯xk and precision Γ , the integral on the
right-hand side is computed such that,

exp

−1
2 (xk −¯xk)T Γ (xk −¯xk)

dxk = (2π)N/2
|Γ |1/2 .
Substituting the above results into Eqs.(4.22) and (4.19), we have
p(y|α) =

|Φ|1/2
(2π)N/2

 β
2π
M/2	K
exp [−]
(2π)N/2
|Γ |1/2
K
.
(4.23)
Taking the logarithm of both sides, and omitting constant terms, which are unrelated
to the arguments, we get
log p(y|α) = K

−1
2 log |Γ | + 1
2 log |Φ| + M
2 log β

−.
(4.24)
Let us deﬁne Σ y such that
Σ y = β−1I + HΦ−1HT .
(4.25)
Using the formula in Eq.(C.95) in the Appendix, the relationship
|Φ||β−1I + HΦ−1HT | = |β−1I||Φ + βHT H|
(4.26)
holds. Thus, substituting Eqs.(4.25) and (4.13) into the above equation, we get
|Φ||Σ y| = |β−1I||Γ |,
(4.27)
and
log |Σ y| = log |Γ | −M log β −log |Φ|.
(4.28)

56
4
Sparse Bayesian (Champagne) Algorithm
On the other hand, according to Sect.4.10.2,  is expressed as
 = 1
2
K

k=1
yT
k Σ−1
y yk,
(4.29)
where Σ y is given in Eq.(4.25). Thus, substituting Eqs.(4.28) and (4.29), into (4.24),
we get
log p(y|α) = −1
2 K log |Σ y| −1
2
K

k=1
yT
k Σ−1
y yk.
(4.30)
The above equation indicates that p(y|α) is Gaussian with the mean equal to zero
and covariance matrix equal to Σ y. This Σ y is called the model data covariance.
Therefore, the estimate of α, α, is obtained by maximizing log p(y|α) expressed
above. Alternatively, deﬁning the cost function such that
F(α) = log |Σ y| + 1
K
K

k=1
yT
k Σ−1
y yk,
(4.31)
the estimate α is obtained by minimizing this cost function.
4.4 Update Equations for α
In this section, we derive the update equation for α. As will be shown, the updated
equation contains the parameters of the posterior distribution. Since the value of α
is needed to compute the posterior distribution, the algorithm for computing α is
a recursive algorithm, as is the case of the EM algorithm presented in Sect.B.5 in
the Appendix. That is, ﬁrst setting an initial value for α, the posterior distribution
is computed. Then, using the parameters of the posterior distribution, α is updated.
These procedures are repeated until a certain stopping condition is met.
Let us derive the update equation for α by minimizing the cost function F(α), i.e.,
α = argmin
α
F(α).
The derivative of F(α) with respect to α is computed,
∂F(α)
∂α j
=
∂
∂α j
log |Σ y| +
∂
∂α j
1
K
K

k=1
yT
k Σ−1
y yk.
(4.32)
The ﬁrst term in the right-hand side is expressed using Eq.(4.28) as
∂
∂α j
log |Σ y| =
∂
∂α j

−M log β −log |Φ| + log |Γ |

= −∂
∂α j
log |Φ| +
∂
∂α j
log |Γ |.
(4.33)

4.4 Update Equations for α
57
First, we have
∂
∂α j
log |Φ| =
∂
∂α j
N

j=1
log α j = α−1
j .
(4.34)
Deﬁning Π j, j as an (N × N) matrix in which the ( j, j)th component is equal to 1
and all other components are zero, we can compute the second term of Eq.(4.33),
such that
∂
∂α j
log |Γ | = tr

Γ −1 ∂
∂α j
Γ

= tr

Γ −1 ∂
∂α j

Φ + βHT H

= tr

Γ −1 ∂
∂α j
Φ

= tr

Γ −1Π j, j

= [Γ −1] j, j,
(4.35)
where [ · ] j, j indicates the ( j, j)th element of a matrix in the squared brackets. Note
that [Γ −1] j, j above is the ( j, j)th element of the posterior covariance matrix.
Next, we compute the second term of the right-hand side of Eq.(4.32). Using
Eqs.(4.98) and (4.99), we can derive,
∂
∂α j
1
K
K

k=1
yT
k Σ−1
y yk =
∂
∂α j
1
K
K

k=1

β∥yk −H ¯xk∥2 + ¯xT
k Φ ¯xk

= 1
K
K

k=1
¯xT
k
 ∂
∂α j
Φ

¯xk = 1
K
K

k=1
¯xT
k Π j, j ¯xk
= 1
K
K

k=1
¯x2
j (tk).
(4.36)
Therefore, denoting [Γ −1] j, j as  j, j, the relationship
∂F(α)
∂α j
=  j, j −α−1
j
+ 1
K
K

k=1
¯x2
j (tk) = 0
(4.37)
holds, and we get
α−1
j
=  j, j + 1
K
K

k=1
¯x2
j (tk).
(4.38)
This is equal to the update equation in the EM algorithm derived in Eq.(B.42).
On the other hand, we can derive a different update equation. To do so, we rewrite
the expression for the posterior precision matrix in Eq.(4.13) to I −Γ −1Φ =
βΓ −1HT H, where the ( j, j)th element of this equation is
1 −α j[Γ −1] j, j =

βΓ −1HT H

j, j .
(4.39)

58
4
Sparse Bayesian (Champagne) Algorithm
We then rewrite Eq.(4.37) as
1 −α j[Γ −1] j, j = α j
1
K
K

k=1
¯x2
j (tk).
(4.40)
Thus, we can derive the following equation:
α j =

βΓ −1HT H

j, j
1
K
K
k=1 ¯x2
j (tk)
.
(4.41)
Equation(4.41) is called the MacKay update equation [5]. The MacKay update is
known to be faster than the EM update in Eq.(4.38) particularly when the estimation
problem is highly ill-posed, i.e., M < N, although there is no theoretical proof that
guarantees the convergence of the MacKay update.
Let us summarize the algorithm to estimate the source distribution xk. First, α
is set to an appropriate initial value and the parameters of the posterior distribution,
¯xk and Γ , are computed using Eqs.(4.13) and (4.14). Then, the hyperparameter α is
updated using Eq.(4.41) with the values of ¯xk and Γ obtained in the preceding step.
The algorithm is similar to the EM algorithm and the only difference is the update
equation for α.
4.5 Modiﬁed Algorithm Integrating Interference
Suppression
A simple modiﬁcation of the algorithm described in the preceding sections leads to
an algorithm robust to the interference overlapped onto the sensor data y. The idea is
similar to the one for the PFA algorithm described in Sect.5.4, and the prerequisite
is that a control measurement, which contains the interference but not the signal of
interest, be available. Using the factor analysis model, the data is expressed such that
yk = Buk + ε
for control data,
(4.42)
yk = Hxk + Buk + ε for target data.
(4.43)
In the above equations, L × 1 column vector uk is the factor activity and B is an
M × L mixing matrix. As in Sect.5.4, Buk represents the interference.
Similar to the PFA algorithm, this modiﬁed Champagne algorithm has a two-step
procedure. The ﬁrst step applies the VBFA (or BFA) algorithm to the control data,
and estimates the interference mixing matrix B and the sensor-noise precision β.
The target data is modeled as
yk = Hxk + Buk + ε = [H, B]
 xk
uk

+ ε = Hczk + ε,
(4.44)

4.5 Modiﬁed Algorithm Integrating Interference Suppression
59
where
Hc = [H, B] and
zk =
 xk
uk

.
Here, Hc and zk can respectively be called the extended lead ﬁeld matrix and the
extended source vector. The second step applies the Champagne algorithm to the
targetdatausingtheextendedleadﬁeldmatrix Hc where B isgivenfromtheﬁrststep.
The prior probability for the extended source vector zk is:
p(zk) = p(xk)p(uk) = N(xk|0, Φ−1)N(uk|0, I)
=

Φ
2π

1/2
exp

−1
2 xT
k Φxk
 
I
2π

1/2
exp

−1
2 uT
k uk

=

Φ
2π

1/2
exp

−1
2 zT
k Φzk

= N(zk|0, Φ),
(4.45)
where the (N + L) × (N + L) matrix Φ is the prior precision matrix of the extended
source vector, expressed as
Φ =
 Φ
0
0
I

=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
α1
· · ·
0
0 · · · 0
...
...
...
...
...
...
0
· · · αN
0 · · · 0
0
· · ·
0
1 · · · 0
...
· · ·
...
...
...
...
0
· · ·
0
0 · · · 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.46)
In this modiﬁed version of the Champagne algorithm, we use the same update equa-
tion for α with Φ replaced with Φ. Thus, when updating the hyperparameter α, only
components up to the Nth diagonal element of Φ are updated, and the rest of the
elements are ﬁxed to 1.
4.6 Convexity-Based Algorithm
In this section, we describe an alternative algorithm that minimizes the cost function
in Eq.(4.31). The algorithm is called the convexity-based algorithm [1, 2]. It is faster
than the EM algorithm. Unlike the MacKay update, this algorithm is guaranteed to
converge. The algorithm also provides a theoretical basis for the sparsity analysis
described in Sect.4.7.
4.6.1 Deriving an Alternative Cost Function
The convexity-based algorithm makes use of the fact that log |Σ y| is a concave
function of 1/α1, . . . , 1/αN, which is the voxel variance of the prior probability

60
4
Sparse Bayesian (Champagne) Algorithm
distribution. Because of this, we use the voxel variance instead of the voxel precision
in this section. We deﬁne prior voxel variance of the jth voxel as ν j, which is equal
to 1/α j, and deﬁne the column vector ν such that ν = [ν1, . . . , νN]T . We rewrite
the cost function in Eq.(4.31) using ν,
F(ν) = log |Σ y| + 1
K
K

k=1
yT
k Σ−1
y yk,
(4.47)
where the model data covariance Σ y is expressed as
Σ y = β−1I + HΥ HT ,
(4.48)
and Υ is the covariance matrix of the voxel prior distribution, deﬁned as Υ =
diag ([ν1, . . . , νN]).
The ﬁrst term in Eq.(4.47), log |Σ y|, is a concave function of ν1, . . . , νN. Thus,
for an arbitrary ν, we can ﬁnd z that satisﬁes the relationship
zT ν −zo ≥log |Σ y|,
(4.49)
where z is the column vector z = [z1, . . . , zN], which contains auxiliary variables
z j ( j = 1, . . . , N), and zo is a scalar term that depends on z.
The second term in Eq.(4.47) can be rewritten using
yT
k Σ−1
y yk = min
xk

β∥yk −Hxk∥2 + xT
k Υ −1xk

.
(4.50)
The proof for the equation above is presented in Sect.4.10.3. Therefore, we have the
relationship
1
K
K

k=1
yT
k Σ−1
y yk = min
x
1
K
K

k=1

β∥yk −Hxk∥2 + xT
k Υ −1xk

.
(4.51)
Using auxiliary variables z and x (where x collectively expresses x1, . . . , xK ),
we deﬁne a new cost function 
F(ν, x, z), such that

F(ν, x, z) = 1
K
K

k=1

β∥yk −Hxk∥2 + xT
k Υ −1xk

+ zT ν −zo.
(4.52)
Equations(4.49) and (4.51) guarantee that the relationship

F(ν, x, z) ≥F(ν),
always holds. That is, the alternative cost function 
F(ν, x, z) forms an upper bound
of the true cost function F(ν). When we minimize 
F(ν, x, z) with respect to ν, x
and z, such ν also minimizes the true cost function F(ν).

4.6 Convexity-Based Algorithm
61
4.6.2 Update Equation for z
Let us derive the update equation for the auxiliary variable z. The update value of z,
z, minimizes the cost function 
F(ν, x, z) and at the same time satisﬁes the constraint
in Eq.(4.49). Since zT ν −zo are the only terms that depend on z in 
F(ν, x, z), this
minimization problem is expressed as
z = argmin
z

zT ν −zo

subject to
zT ν −zo ≥log |Σ y|.
(4.53)
That is, the minimization problem is equivalent to ﬁnding the hyperplane zT ν −zo
that forms a closest upper bound of log |Σ y|. Such a hyperplane is found as the plane
that is tangential to log |Σ y|[6]. Therefore, the update valuez is given by
z = ∂
∂ν log |Σ y|.
(4.54)
This z forms a bound tightest to the concave function log |Σ y|, and minimizes the
cost function 
F(ν, x, z) with respect to z. To computez, we use
z j =
∂
∂ν j
log |Σ y| = tr

Σ−1
y
∂
∂ν j
Σ y

,
(4.55)
and
∂
∂ν j
Σ y =
∂
∂ν j

β−1I + HΥ HT 
= H ∂
∂ν j
Υ HT = HΠ j, j HT = ljlT
j ,
(4.56)
where lj is the lead ﬁeld vector at the jth voxel, which is the jth column of H.
Substituting Eqs.(4.56) into (4.55), we get
∂
∂ν j
log |Σ y| = tr

Σ−1
y ljlT
j

= tr

lT
j Σ−1
y lj

= lT
j Σ−1
y lj.
(4.57)
Using the equation above, we arrive at
z j = argmin
z j

F(ν, x, z) = lT
j Σ−1
y lj
(4.58)
for the update equation ofz j.
4.6.3 Update Equation for xk
Now, let us derive the update equation for the auxiliary variable x. Since the
relationship
xk = argmin
xk

β∥yk −Hxk∥2 + xT
k Υ −1xk

(4.59)

62
4
Sparse Bayesian (Champagne) Algorithm
holds, according to Eq.(4.101) in Sect.4.10.3, xk is obtained as
xk = β

Υ −1 + βHT H
−1
HT yk.
(4.60)
This is the update equation for xk. Using the matrix inversion formula in Eq.(C.92),
this equation can be rewritten as
xk = Υ HT 
β−1I + HΥ HT −1
yk = Υ HT Σ−1
y yk.
(4.61)
Since this expression uses Υ and Σ y, it is conveniently used in the convexity-based
algorithm. Note that the auxiliary variable xk is equal to the posterior mean of
the voxel source distribution because the update equation above is exactly equal to
Eq.(B.26).
4.6.4 Update Equation for ν
Let us derive ν that minimizes the cost function 
F(ν, z, x). Since only the second
and the third terms in 
F(ν, z, x) contains ν (as shown in Eq.(4.52)), we have:
ν = argmin
ν

F(ν, x, z) = argmin
ν

1
K
K

k=1
xT
k Υ −1xk + zT ν
	
= argmin
ν
N

j=1

z jν j +
1
K
K
k=1 x2
j (tk)
ν j
	
.
(4.62)
Thus, considering the relationship
∂
∂ν j
N

k=1

z jν j +
1
K
K
k=1 x2
j (tk)
ν j
	
= z j −
1
K
K
k=1 x2
j (tk)
ν2
j
= 0,
we can derive
ν j = argmin
ν j

F(ν, z, x) =




1
K
K
k=1 x2
j (tk)
z j
.
(4.63)
4.6.5 Summary of the Convexity-Based Algorithm
The update for z in Eq.(4.58) and the update for x in Eq.(4.61) require ν to be known.
The update equation for ν in Eq.(4.63) requires x and z to be known. Therefore, the
convexity-based algorithm updates z, x and ν using Eqs.(4.58), (4.61) and (4.63), in

4.6 Convexity-Based Algorithm
63
a recursive manner. Since the auxiliary variable xk is the posterior mean of the voxel
source distribution, the value of the auxiliary variable xk is equal to the Bayesian
estimate of the source distribution, ¯xk, after the iterative procedure is terminated.
4.7 The Origin of the Sparsity
Why does the Champagne algorithm produce sparse solutions? This section tries to
answer this question, and consider the origin of the sparsity by analyzing the cost
function of the Champagne algorithm [7]. For simplicity, we set K = 1, and by
omitting the time index, x1 and y1 are denoted x and y. Using Eq.(4.47), we have
ν = argmin
ν

log |Σ y| + yT Σ−1
y y

.
As shown in Eq.(4.50), we have
yT Σ−1
y y = min
x

β∥y −Hx∥2 + xT Υ −1x

.
Combining the two equations above, the cost function for estimating x is given by
x = argmin
x
F :
F = β∥y −Hx∥2 + φ(x),
(4.64)
where the constraint φ(x) is expressed as
φ(x) = min
ν

xT Υ −1x + log |Σ y|

= min
ν
⎛
⎝
N

j=1
x2
j
ν j
+ log |Σ y|
⎞
⎠.
(4.65)
However, computing φ(x) in Eq.(4.65) is not so easy because log |Σ y| contains ν
as in Eq.(4.48). Therefore, we further introduce a simpliﬁcation by assuming that the
columns in the matrix H are orthogonal, i.e., the relationship lT
i l j = Ii, j is assumed
to hold. In this case, using Eq.(4.48), we get
log |Σ y| =
N

j=1
log(β−1 + ν j),
and, accordingly,
φ(x) = min
ν
N

j=1
"
x2
j
ν j
+ log(β−1 + ν j)
#
.
(4.66)

64
4
Sparse Bayesian (Champagne) Algorithm
By computing the minimum on the right-hand side, we ﬁnally have
φ(x) =
N

j=1
ϕ(x j),
(4.67)
where
ϕ(x) =
2|x|
$
x2 + 4β−1 + |x|
+ log

β−1 + x2
2 + 1
2|x|
$
x2 + 4β−1

.
(4.68)
The constraint ϕ(x) in Eq.(4.68) is plotted in Fig.4.1. In Fig.4.1a, the plot of ϕ(x)
is shown by the solid line. For comparison, the constraint for the L1-norm solution,
|x|, is also shown by the broken line. The plots in Fig.4.1a show that the constraint
of the Champagne algorithm ϕ(x) is very similar to (but sharper than) the L1-norm
constraint |x|, suggesting that the Champagne algorithm produces sparse solutions.
In Fig.4.1b, the plots of ϕ(x) when β−1 = 0.1, β−1 = 1 and β−1 = 10 are shown
by the dot-and-dash, broken, and solid lines, respectively. The vertical broken line at
x = 0 shows the L0-norm constraint, ∥x∥0, for comparison. It is shown here that the
shape of the constraint ϕ(x) depends on β−1—namely the noise variance. When the
noise variance is small (i.e., a high SNR), ϕ(x) is much sharper than |x|, and becomes
closer to the L0-norm constraint. That is, the Champagne algorithm uses an adaptive
constraint. When the SNR of the sensor data is high, it gives solutions with enhanced
sparsity. In contrast, when the sensor data is noisy, the shape of ϕ(x) becomes similar
to the shape of the L1-norm constraint and the algorithm gives solutions with mild
sparsity.
5
0
5
0
1
2
3
4
5
x
cost func
(a)
5
0
5
0
0.2
0.4
0.6
0.8
1
x
cost func
(b)
Fig. 4.1 Plots of the cost function ϕ(x) shown in Eq.(4.68). a The solid line shows the plot of
ϕ(x) with β−1 = 1, and the broken line the plot of |x|, which is the constraint for the L1-norm
minimum-norm solution. b Plots of ϕ(x) when β−1 = 0.1, β−1 = 1 and β−1 = 10 shown by the
dot-and-dash, broken, and solid lines, respectively. In this ﬁgure, the vertical broken line at x = 0
shows the constraint for the L0-norm solution

4.8 Extension to Include Source Vector Estimation
65
4.8 Extension to Include Source Vector Estimation
In Sect.4.6, we derive the convexity-based algorithm when the source orientation
at each voxel is known, i.e., when the relationship between the sensor data and the
voxel source distribution is expressed as in Eq.(4.3).
In this section, we derive the convexity-based algorithm when the source orien-
tation at each voxel is unknown and the relationship,
yk = Fxk + ε,
(4.69)
holds, where xk is given by
xk =

sT
1 (tk), . . . , sT
N(tk)
T
,
(4.70)
and s j(tk) is the 3 × 1 source vector at the jth voxel. In other words, we describe an
extension that enables estimating the source vector at each voxel. The algorithms in
the preceding sections use a diagonal prior covariance (or precision) matrix, and the
use of the diagonal matrix is possible because the source orientation is known. There-
fore, a naïve application of those algorithms to cases where the source orientation is
unknown generally results in the shrinkage of source vector components. Namely,
the naïve application possibly leads to a solution where only a single component of
a source vector has nonzero value and other components have values close to zero,
leading to incorrect estimation of source orientations.
Since the unknown parameter at each voxel is not a scalar quantity but the 3 × 1
vector quantity in this section, the algorithm being developed here uses the nondi-
agonal covariance matrix of the prior distribution. That is, the prior distribution is
assumed to be [1];
p(xk|Υ ) =
N

j=1
N(s j(tk)|0, Υ j),
(4.71)
where Υ j is a 3×3 covariance matrix of the source vector s j(tk). Thus, the covariance
matrix of the voxel source distribution, Υ , is a 3N × 3N block diagonal matrix
expressed as
Υ =
⎡
⎢⎢⎢⎣
Υ 1 0 · · ·
0
0 Υ 2 · · ·
0
...
0 ...
...
0
0 · · · Υ N
⎤
⎥⎥⎥⎦.
(4.72)
The cost function in this case has the same form:
F(Υ ) = log |Σ y| + 1
K
K

k=1
yT
k Σ−1
y yk.
(4.73)

66
4
Sparse Bayesian (Champagne) Algorithm
However, Σ y is given by
Σ y = β−1I +
N

j=1
L jΥ j LT
j ,
(4.74)
where the M×3 lead ﬁeld matrix at the jth voxel, L(r j), is denoted L j for simplicity.
In agreement with Eq.(4.49), since log |Σ y| is a concave function, we can ﬁnd
3 × 3 auxiliary parameter matrices, Z j, ( j = 1, . . . , N) that satisfy
N

j=1
tr

ZT
j Υ j

−Zo ≥log |Σ y|,
(4.75)
where Zo is a scalar term that depends on Z j. Regarding the second term in the
right-hand side of Eq.(4.73), we have the relationship
yT
k Σ−1
y yk = min
xk

β∥yk −Fxk∥2 + xT
k Υ −1xk

= min
xk
⎡
⎣β∥yk −Fxk∥2 +
N

j=1
sT
j (tk)Υ −1
j s j(tk)
⎤
⎦.
(4.76)
Let us use Z to collectively express Z1, . . . , ZN, and use s to collectively express
s j(tk), where j = 1, . . . , N and k = 1, . . . , K. The alternative cost function,

F(Υ , s, Z), is obtained as

F(Υ , s, Z) = 1
K
K

k=1
⎡
⎣β∥yk −Fxk∥2 +
N

j=1
s j(tk)Υ −1
j sT
j (tk)
⎤
⎦
+
N

j=1
tr

ZT
j Υ j

−Zo.
(4.77)
This 
F(Υ , s, Z) forms an upper bound of the true cost function in Eq.(4.73). Accord-
ingly, when we minimize 
F(Υ , s, Z) with respect to Υ , s, and Z, we can minimize
the true cost function F(Υ ).
4.8.1 Update Equation for Zj
Using the same arguments in Sect.4.6.2, the update equation for Zj is derived as
Z j =
∂
∂Υ j
log |Σ y|.
(4.78)

4.8 Extension to Include Source Vector Estimation
67
Here, the hyperplane N
j=1 tr

ZT
j Υ j

−Zo forms a tightest upper bound of the
concave function log |Σ y|. Let us express the lead ﬁeld matrix at the jth voxel using
its column vectors,
L j =

lx(r j), l y(r j), lz(r j)

=

lx, l y, lz

,
where explicit notation of the voxel location (r j) is omitted. Using exactly the same
derivation from Eqs.(4.55) to (4.58), we can derive the update equation
Z j =
∂
∂Υ j
log |Σ y|
=
⎡
⎢⎣
∂log |Σ y|/∂[Υ j]1,1 ∂log |Σ y|/∂[Υ j]1,2 ∂log |Σ y|/∂[Υ j]1,3
∂log |Σ y|/∂[Υ j]2,1 ∂log |Σ y|/∂[Υ j]2,2 ∂log |Σ y|/∂[Υ j]2,3
∂log |Σ y|/∂[Υ j]3,1 ∂log |Σ y|/∂[Υ j]3,2 ∂log |Σ y|/∂[Υ j]3,3
⎤
⎥⎦
=
⎡
⎢⎣
lT
x Σ−1
y lx lT
x Σ−1
y l y lT
x Σ−1
y lz
lT
y Σ−1
y lx lT
y Σ−1
y l y lT
y Σ−1
y lz
lT
z Σ−1
y lx lT
z Σ−1
y l y lT
z Σ−1
y lz
⎤
⎥⎦= LT
j Σ−1
y L j,
(4.79)
where [Υ j]ℓ,m indicates the (ℓ, m)th element of the matrix Υ j.
4.8.2 Update Equation for sj(tk)
The update equation for xk can be obtained using
xk = argmin
xk

β∥yk −Fxk∥2 + xT
k Υ −1xk

.
(4.80)
The solution is given by Eq.(4.61) with replacing H with F, which is rewritten as
⎡
⎢⎢⎢⎣
s1(tk)
s2(tk)
...
sN(tk)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
Υ 1 0 · · ·
0
0 Υ 2 · · ·
0
...
0 ...
...
0
0 · · · Υ N
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
LT
1
LT
2...
LT
N
⎤
⎥⎥⎥⎦Σ−1
y yk.
(4.81)
Therefore, the source vector at the jth voxels j(tk) is given by
s j(tk) = Υ j LT
j Σ−1
y yk,
(4.82)
and Eq.(4.82) is the update equation for s j(tk).

68
4
Sparse Bayesian (Champagne) Algorithm
4.8.3 Update Equation for Υ j
The update equation for Υ j is obtained using

Υ j = argmin
Υ j

1
K
K

k=1
sT
j (tk)Υ −1
j s j(tk) + tr

ZT
j Υ j
	
.
(4.83)
Taking
∂
∂Υ j
sT
j (tk)Υ −1
j s j(tk) = −Υ −1
j s j(tk)sT
j (tk)Υ −1
j
(4.84)
into consideration, we have
∂
∂Υ j

1
K
K

k=1
sT
j (tk)Υ −1
j s j(tk) + tr

ZT
j Υ j
	
= −Υ −1
j

1
K
K

k=1
s j(tk)sT
j (tk)
	
Υ −1
j
+ Z j.
(4.85)
Setting the right-hand side to zero, we get the equation,
Υ j Z jΥ j =

1
K
K

k=1
s j(tk)sT
j (tk)
	
.
(4.86)
However, there are multiple solutions for Υ j that satisfy Eq.(4.86). We should ﬁnd
a positive semideﬁnite matrix that satisﬁes Eq.(4.86). Deﬁning Ξ = (1/K) K
k=1
s j(tk)sT
j (tk), and using
Ξ = Z−1/2
j
(Z1/2
j
Ξ Z1/2
j
)Z−1/2
j
= Z−1/2
j
(Z1/2
j
Ξ Z1/2
j
)1/2(Z1/2
j
Ξ Z1/2
j
)1/2Z−1/2
j
= Z−1/2
j
(Z1/2
j
Ξ Z1/2
j
)1/2Z−1/2
j
Z j Z−1/2
j
(Z1/2
j
Ξ Z1/2
j
)1/2Z−1/2
j
,
(4.87)
the solution for Υ j that is a positive semideﬁnite matrix is derived such that

Υ j = Z−1/2
j

Z1/2
j

1
K
K

k=1
s j(tk)sT
j (tk)
	
Z1/2
j
	1/2
Z−1/2
j
.
(4.88)
Equation(4.88) is the update equation for Υ j.
In summary, Z j, s j(tk), and Υ j are updated using Eqs.(4.79), (4.82) and (4.88),
respectively. Since the auxiliary variable xk is equal to the posterior mean of the

4.8 Extension to Include Source Vector Estimation
69
voxel source distribution, the Bayes estimate of the voxel source distribution, ¯xk,
can be obtained as the ﬁnal updated results ofs j(tk).
4.9 Source Vector Estimation Using Hyperparameter Tying
In Sect.4.8, we describe an algorithm that reconstructs the source distribution when
the source orientation is unknown. The algorithm uses the block diagonal covariance
matrix Υ expressed in Eq.(4.72). However, because the matrix Υ is a nondiagonal
matrix, the resultant algorithm is rather complex and computationally expensive,
compared to the algorithm that uses a diagonal covariance matrix. In this section,
we describe an extension of the Champagne algorithm that still uses a diagonal
covariance matrix for the prior distribution but enables us to estimate the voxel
source vectors.
The method uses a technique called hyperparameter tying, and simple modiﬁ-
cation of the algorithm described in Sects.4.2–4.6 leads to a successful estimation
of voxel source vectors. We use the forward relationship given in Eq.(4.69), and a
3N × 3N diagonal covariance matrix for the Gaussian prior:
Υ =
⎡
⎢⎢⎢⎢⎢⎢⎣
...
...
...
· · · ν3n+1
0
0
· · ·
· · ·
0
ν3n+2
0
· · ·
· · ·
0
0
ν3n+3
· · ·
...
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎦
,
(4.89)
where we show a diagonal block matrix corresponding to the nth voxel. The equation
in Eq.(4.63) is applied to update ν j. In the method proposed here, the three hyper-
parameters ν3n+1, ν3n+2, and ν3n+3 from the same voxel are tied together to have
the same new update value. That is, for ν3n+1, ν3n+2, and ν3n+3, we compute a new
single update value by using
νnew
3n+m = 1
3
3

i=1
ν3n+i,
for m = 1, 2, 3
(4.90)
where ν3n+i (for i = 1, 2, 3) are the update values obtained from Eq.(4.63), and
νnew
3n+m (m = 1, 2, 3) is a new update value for these hyperparameters.
The hyperparameter tying method can be implemented in exactly the same manner
when using the MacKay update in Eq.(4.41). A new single update value for the voxel
precision corresponding to the nth voxel is computed by using
αnew
3n+m = 1
3
3

i=1
α3n+i,
for m = 1, 2, 3
(4.91)

70
4
Sparse Bayesian (Champagne) Algorithm
where α3n+i (for i = 1, 2, 3) is the update value from Eq.(4.41) and αnew
3n+m (for
m = 1, 2, 3) is a new update value for these hyperparameters.
The rationale for this hyperparameter tying can be explained using the cost
function analysis described in Sect.4.7. Let us compute the constraint function in
Eq.(4.66) for a two-dimensional case in which the unknown parameters are denoted
x1 and x2. The constraint is rewritten in this case as,
φ(x1, x2) = min
ν1,ν2
2

j=1
"
x2
j
ν j
+ log(β−1 + ν j)
#
.
(4.92)
When the hyperparameters ν1 and ν2 are tied together, i.e., when we set these hyper-
parameters at the same value ν, the constraint function is changed to
φ(x1, x2) = min
ν
"
x2
1 + x2
2
ν
+ 2 log(β−1 + ν)
#
.
(4.93)
By implementing this minimization, the value of ν that minimizes the right-hand
side of the above equation, ν, is derived as
ν = a +
$
a2 + 8β−1a
4
,
where a = x2
1 + x2
2. Substituting this ν into Eq.(4.93), we derive the constraint
function,
φ(x1, x2) =
4a
a +
$
a2 + 8β−1a
+ 2 log

β−1 + a +
$
a2 + 8β−1a
4
	
.
(4.94)
The plot of the constraint function in Eq.(4.94) is shown in Fig.4.2a. For com-
parison, the Champagne constraint function when untying ν1 and ν2 (Eq.4.92) is
shown in Fig.4.2b. The constraint functions for the L2 and L1-norm regularizations
are also shown in Fig.4.2c, d for comparison. These plots show that the Champagne
constraint function when tying ν1 and ν2 has a shape very similar to the constraint
function for the L2 regularization. According to the arguments in Sect.2.9.2, this
type of constraint does not generate a sparse solution. Thus, when tying the hyper-
parameter update values, the sparsity is lost among the solutions of x2n+1, x2n+2,
and x2n+3 and there is no shrinkage over the source vector components. However,
since the sparsity is maintained across voxels, a sparse source distribution can still
be reconstructed.

4.10 Appendix to This Chapter
71
 x1
 x2
(a)
200
400
600
800
1000
200
400
600
800
1000
 x1
(b)
200
400
600
800
1000
 x1
 x2
(c)
200
400
600
800
1000
200
400
600
800
1000
 x1
(d)
200
400
600
800
1000
Fig. 4.2 Plots of the constraint functions for the two-dimensional case. a The plot of φ(x1, x2) in
Eq.(4.94). b The plot of φ(x1, x2) in Eq.(4.92). c The plot of the constraint function for the L2-norm
constraint: φ(x1, x2) = x2
1 + x2
2. d The plot of the constraint function for the L1-norm constraint:
φ(x1, x2) = |x1| + |x2|. The parameter β was set to 1 when computing the plots in a and b
4.10 Appendix to This Chapter
4.10.1 Derivation of Eq.(4.21)
D in Eq.(4.20) is rewritten as
D =
K

k=1
1
2 xT
k

Φ + βHT H

xk −βxT
k HT yk

+ C.
(4.95)
Here, terms not containing xk are expressed as C. We apply the completion of the
square with respect to xk, i.e., we change Eq.(4.95) to have a form,
D = 1
2
K

k=1
(xk −mk)T A (xk −mk) + ,

72
4
Sparse Bayesian (Champagne) Algorithm
where A is a real symmetric matrix and mk is a column vector, and  represents
remaining terms. The ﬁrst term on the right-hand side is rewritten as
1
2
K

k=1
(xk −mk)T A (xk −mk) =
K

k=1
1
2 xT
k Axk −xT
k Amk + · · ·

.
Comparing the right-hand side of the equation above with that of Eq.(4.95), we get
A = Φ + βHT H,
Amk = βHT yk
namely
mk = β

Φ + βHT H
−1
HT yk.
Comparing the equations above with Eqs.(4.13) and (4.14), we have the relation-
ships A = Γ and mk = ¯xk, giving
D = 1
2
K

k=1
(xk −¯xk)T Γ (xk −¯xk) + .
(4.96)
This equation shows that D reaches the minimum at xk = ¯xk, and the minimum value
is equal to . The value of  is obtained by substituting xk = ¯xk into Eq.(4.20),
such that
 = β
2
K

k=1
∥yk −H ¯xk∥2 + 1
2
K

k=1
¯xT
k Φ ¯xk.
(4.97)
4.10.2 Derivation of Eq.(4.29)
The derivation starts from  in Eq.(4.97). On the right-hand side of this equation,
the kth terms containing xk are
β
2 ∥yk −H ¯xk∥2 + 1
2 ¯xT
k Φ ¯xk = β
2

yT
k yk −2¯xT
k HT yk + ¯xT
k HT H ¯xk

+ 1
2 ¯xT
k Φ ¯xk
= 1
2

β yT
k yk −2¯xT
k βHT yk + ¯xT
k (Φ + βHT H)¯xk

= 1
2

β yT
k yk −2¯xT
k βHT yk + ¯xT
k Γ ¯xk

.
(4.98)
Using the relationship βHT yk = Γ ¯xk, we have
1
2

β yT
k yk −2¯xT
k Γ ¯xk + ¯xT
k Γ ¯xk

= 1
2

β yT
k yk −¯xT
k Γ ¯xk

= 1
2

β yT
k yk −(βΓ −1HT yk)T Γ (βΓ −1HT yk)


4.10 Appendix to This Chapter
73
= 1
2 yT
k

βI −βHΓ −1HT β

yk
= 1
2 yT
k

βI −βH(Φ + βHT H)−1HT β

yk
= 1
2 yT
k

β−1I + HΦ−1HT −1
yk.
(4.99)
In the equation above, we use the matrix inversion formula in Eq.(C.91). Using the
model covariance matrix Σ y deﬁned in Eq.(4.25), we get
 = 1
2
K

k=1
yT
k

β−1I + HΦ−1HT −1
yk = 1
2
K

k=1
yT
k Σ−1
y yk.
The above equation is equal to Eq.(4.29).
4.10.3 Proof of Eq.(4.50)
The proof of Eq.(4.50) begins with
xk = argmin
xk

β∥yk −Hxk∥2 + xT
k Υ −1xk

.
(4.100)
The solution of this minimization, xk, is known as the weighted minimum-norm
solution. To derive it, we deﬁne the cost function F,
F = β∥yk −Hxk∥2 + xT
k Υ −1xk.
Let us differentiate F with respect to xk, and set it to zero,
∂
∂xk
F = −2βHT %
yk −Hxk
&
+ 2Υ −1xk = 0.
Thus, the weighted minimum-norm solution is given by
xk = β

Υ −1 + βHT H
−1
HT yk.
(4.101)
The above xk is exactly the same as the posterior mean ¯xk in Eq.(4.14). Therefore,
according to the arguments in the preceding subsection, we have the relationship,
min
xk

β∥yk −Hxk∥2 + xT
k Υ −1xk

=

β∥yk −H ¯xk∥2 + ¯xT
k Υ −1 ¯xk

= yT
k Σ−1
y yk.
(4.102)

74
4
Sparse Bayesian (Champagne) Algorithm
References
1. D.P. Wipf, J.P. Owen, H.T. Attias, K. Sekihara, S.S. Nagarajan, Robust Bayesian estimation
of the location, orientation, and time course of multiple correlated neural sources using MEG.
NeuroImage 49, 641–655 (2010)
2. D. Wipf, S. Nagarajan, A uniﬁed Bayesian framework for MEG/EEG source imaging. NeuroIm-
age 44(3), 947–966 (2009)
3. M.E. Tipping, Sparse Bayesian learning and relevance vector machine. J. Mach. Learn. Res. 1,
211–244 (2001)
4. M.E. Tipping, Bayesian inference: an introduction to principles and practice in machine learning,
in Advanced Lectures on Machine Learning (Springer, New York, 2004), pp. 41–62
5. D.J.C. MacKay, Bayesian interpolation. Neural Comput. 4, 415–447 (1992)
6. M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, L.K. Saul, An introduction to variational methods
for graphical models. Mach. Learn. 37, 183–233 (1999)
7. D.P. Wipf, S.S. Nagarajan, A new view of automatic relevance determination, in Advances in
Neural Information Processing Systems, pp. 1625–1632 (2008)

Chapter 5
Bayesian Factor Analysis: A Versatile
Framework for Denoising, Interference
Suppression, and Source Localization
5.1 Introduction
This chapter describes Bayesian factor analysis (BFA), which is a technique that
can decompose multiple sensor time courses into time courses of independent factor
activities, where the number of factors is much smaller than the number of sensors.
The factors are artiﬁcially-introduced “abstract” causes that explain the temporal
behavior of the sensor data, and do not necessarily correspond to physical sources.
Since the factor activities cannot directly be observed, they are considered latent
variables.
The Bayesian factor analysis is a versatile framework. It can be used for selectively
extracting signal components from noise-contaminated sensor data. It can provide
an estimate of the data covariance matrix better than a sample covariance matrix,
and such covariance estimate can be used in source reconstruction algorithms such
as adaptive beamformers. It is extended to suppress interference components in
interference-overlapped sensor data [1]. It is also extended to a virtual-sensor type
source localization method, which is called the Saketini algorithm [2].
We start this chapter by explaining the basic form of the Bayesian factor analysis
and then extend it to the variational Bayesian factor analysis (VBFA) [3] in which the
model order (the number of factors) is determined by the algorithm itself. Following
VBFA arguments, we describe the extensions for interference suppression and source
localization.
5.2 Bayesian Factor Analysis
5.2.1 Factor Analysis Model
As in the previous chapters, let us deﬁne the output of the mth sensor at time t as
ym(t), and the data vector as y(t), such that
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_5
75

76
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
y(t) =
⎡
⎢⎢⎢⎣
y1(t)
y2(t)
...
yM(t)
⎤
⎥⎥⎥⎦,
(5.1)
where M is the total number of sensors. This column vector y(t) expresses the outputs
of the whole sensor array. We assume that y(t) is measured at discrete time points
t = t1, . . . , tK where K is the total number of time points, and y(tk) is denoted yk
for simplicity. In the factor analysis model, the data vector yk is expressed as
yk = Auk + ε,
(5.2)
where ε is the sensor noise. On the right-hand side of the equation above, the ﬁrst
term, Auk, expresses a signal component which is a product of an unknown matrix
A and unknown variables uk. The matrix A is an M × L matrix, referred to as the
mixing matrix, and uk is an L-dimensional column vector,
uk =
⎡
⎢⎣
u1(tk)
...
uL(tk)
⎤
⎥⎦,
(5.3)
where the number of factors L is much smaller than the number of sensors M;
i.e., L ≪M. In the factor analysis model, the temporal variation of the sensor
data yk is explained by the temporal variation of a small number of L independent
factors, u1(tk), . . . , uL(tk). Here, since the number of factors L speciﬁes the model
complexity, this number is called the model order. The factor is a latent variable,
which cannot be directly observed, and each factor, in general, does not correspond
to any physical source. The Bayesian factor analysis estimates the mixing matrix A
and the factor activity uk from the data yk where k = 1, . . . , K.
5.2.2 Probability Model
The prior probability distribution of the factor uk is assumed to be the zero-mean
Gaussian with its precision matrix equal to the identity matrix, i.e.,
p(uk) = N(uk|0, I).
(5.4)
The factor activity is assumed to be independent across time. Thus, the joint prior
probability distribution for all time points is given by
p(u) = p(u1, . . . , uK ) =
K

k=1
p(uk) =
K

k=1
N(uk|0, I),
(5.5)

5.2 Bayesian Factor Analysis
77
where u1, . . . , uK are collectively denoted u. The noise ε is assumed to be Gaussian
with the mean of zero, i.e.,
p(ε) = N(ε|0, Λ−1),
(5.6)
where Λ is a diagonal precision matrix. With this assumption, the conditional prob-
ability p(yk|uk) is expressed as
p(yk|uk) = N(yk|Auk, Λ−1).
(5.7)
The noise ε is also assumed to be independent across time. Thus, we have
p(y|u) = p(y1, . . . , yK |u1, . . . , uK ) =
K

k=1
p(yk|uk) =
K

k=1
N(yk|Auk, Λ−1),
(5.8)
where y1, . . . , yK are collectively denoted y. Using the probability distributions
deﬁned above, the Bayesian factor analysis can factorize the sensor data into L
independent factor activity and additive sensor noise. This factorization is achieved
using the EM algorithm [4, 5]. Explanation of the basics of the EM algorithm is
provided in Sect.B.5 in the Appendix.
5.2.3 EM Algorithm
The E-step of the EM algorithm derives the posterior distribution p(uk|yk). Deriva-
tion of the posterior distribution in the Gaussian model is described in Sect.B.3 in the
Appendix. Since the posterior distribution is also Gaussian, we deﬁne the posterior
distribution p(uk|yk) such that
p(uk|yk) = N(uk|¯uk, Γ −1),
(5.9)
where ¯uk is the mean and Γ is the precision matrix. Using Eqs.(B.24) and (B.25)
with setting Φ to I and H to A in these equations, we get
Γ = (AT ΛA + I),
(5.10)
¯uk = (AT ΛA + I)−1 AT Λyk.
(5.11)
The equations above are the E-step update equations in the Bayesian factor analysis.
Let us derive the M-step update equations for the mixing matrix A and the noise
precision matrix Λ. To do so, we derive the average log likelihood, Θ(A, Λ), and
according to Eqs.(B.34), (5.5), and (5.8), it is expressed as

78
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Θ(A, Λ) = Eu
	
log p(y1, . . . , yK , u1, . . . , uK )

= Eu
	
log p(y1, . . . , yK |u1, . . . , uK )

+ Eu
	
log p(u1, . . . , uK )

= Eu
 K

k=1
log p(yk|uk)

+ Eu
 K

k=1
log p(uk)

,
(5.12)
where Eu[ · ] indicates the expectation with respect to the posterior probability
p(u|y). Taking a look at Eqs.(5.4) and (5.7), only the ﬁrst term on the right-hand
side of Eq.(5.12) contains A and Λ. Thus, we have
Θ(A, Λ) = Eu
 K

k=1
	
log p(yk|uk)


+ C
= K
2 log |Λ| −1
2 Eu
 K

k=1
(yk −Auk)T Λ(yk −Auk)

+ C,
(5.13)
where C expresses terms not containing A and Λ.
The derivative of Θ(A, Λ) with respect to A is given by
∂Θ(A, Λ)
∂A
= ΛEu
 K

k=1
(yk −Auk)uT
k

= Λ(Ryu −ARuu),
(5.14)
where
Ruu = Eu
 K

k=1
ukuT
k

=
K

k=1
¯uk ¯uT
k + KΓ −1,
(5.15)
Ryu = Eu
 K

k=1
ykuT
k

=
K

k=1
yk ¯uT
k ,
(5.16)
Ruy = RT
yu.
(5.17)
Setting the right-hand side of Eq.(5.14) to zero gives Ryu = ARuu. That is, the
M-step update equation for A is derived as
A = Ryu R−1
uu .
(5.18)
Next, the update equation for Λ is derived. The partial derivative of Θ(A, Λ) with
respect to Λ is expressed as
∂Θ(A, Λ)
∂Λ
= K
2 Λ−1 −1
2 Eu
 K

k=1
(yk −Auk)(yk −Auk)T

.
(5.19)

5.2 Bayesian Factor Analysis
79
Setting the right-hand side to zero gives
Λ−1 = 1
K Eu
 K

k=1
(yk −Auk)(yk −Auk)T

.
(5.20)
The right-hand side is further changed to
Eu
 K

k=1
(yk −Auk)(yk −Auk)T

=
K

k=1
Eu[yk yT
k ] −
K

k=1
Eu[ykuT
k ]AT −
K

k=1
AEu[uk yT
k ] +
K

k=1
AEu[ukuT
k ]AT
=
K

k=1
yk yT
k −
K

k=1
yk ¯uT
k AT −
K

k=1
A¯uk yT
k +
K

k=1
AEu[ukuT
k ]AT
= Ryy −Ryu AT −ARuy + ARuu AT ,
(5.21)
where Ryy = K
k=1 yk yT
k . Using Ryu = ARuu derived from Eq.(5.18), we get
1
K Eu
 K

k=1
(yk −Auk)(yk −Auk)T

= 1
K
	
Ryy −ARuy

.
(5.22)
Considering that Λ−1 is a diagonal matrix, we ﬁnally derive
Λ−1 = 1
K diag(Ryy −ARuy),
(5.23)
where diag( · ) indicates a diagonal matrix obtained using the diagonal entries of a
matrix between the brackets.
5.2.4 Computation of Marginal Likelihood
Let us derive an expression to compute the marginal likelihood p(y|θ), which can be
used for monitoring the progress of the EM algorithm. Here, θ is used to collectively
express the hyperparameters A and Λ. Using the marginalization, we obtain p(y|θ),
such that
p(y|θ) =
∞

−∞
p(y|u, θ)p(u)du.
(5.24)

80
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Based on the arguments in Sect.B.4 of the Appendix, p(y|θ) is derived such that
p(y|θ) =
K

k=1
p(yk|θ) where
p(yk|θ) = N(yk|0, Σ y),
(5.25)
where Σ y is the model data covariance. Setting A to H and I to Φ−1 in Eq.(B.30),
we derive Σ y, such that
Σ y = Λ−1 + AAT .
(5.26)
According to Eq.(B.29), the log likelihood function is obtained as
log p(y|θ) =
K

k=1
p(yk|θ) = K
2 log |Σ−1
y | −1
2
K

k=1
yT
k Σ−1
y yk.
(5.27)
Using the matrix inversion formula in Eq.(C.91), we have
Σ−1
y
=

Λ−1 + AAT −1
= Λ −ΛA

AT ΛA + I
−1
AT Λ = Λ −ΛAΓ −1 AT Λ,
(5.28)
Using the equation above and Eq.(5.11), we get
yT
k Σ−1
y yk = yT
k

Λ −ΛAΓ −1 AT Λ

yk = yT
k Λyk −¯uT
k Γ ¯uk.
Also, since the relationship
|Σ−1
y | = |Λ −ΛAΓ −1 AT Λ| = |I −ΛAΓ −1 AT ||Λ| = |Γ −1||Λ|
(5.29)
holds,1 by substituting the above two equations into Eq.(5.27), we ﬁnally obtain the
expression for the likelihood:
log p(y|θ) =
K

k=1
p(yk|θ) = K
2 log |Λ|
|Γ | −1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯uT
k Γ ¯uk. (5.30)
A more informative derivation of Eq.(5.30) uses the free energy expression in
Eq.(B.63) from the Appendix, which claims the following relationship holds:
1 Deﬁning C = I −ΛAΓ −1 AT , we have
AT C = AT −AT ΛAΓ −1 AT = AT −(Γ −I)Γ −1 AT = Γ −1 AT .
Taking the determinant of the equation above, we get |C| = |Γ −1|.

5.2 Bayesian Factor Analysis
81
log p(y|θ) =

dup(u|y)[log p(u, y|θ) −log p(u|y)]
= Eu
	
log p(u, y|θ)

+ H
	
p(u|y)

,
(5.31)
where the second term on the right-hand side is the entropy regarding p(u|y). Using
Eqs.(5.9) and (C.9) in the Appendix, we get
H
	
p(u|y)

=
K

k=1
H
	
p(uk|yk)

= −K
2 log |Γ |.
(5.32)
Using Eqs.(5.5) and (5.8), the ﬁrst term in Eq.(5.31) can be rewritten as
Eu
	
log p(u, y|θ)

= Eu
	
log p(y|u, θ) + log p(u)

= −1
2
K

k=1
Eu

(yk −Auk)T Λ(yk −Auk) + uT
k uk

+ K
2 log |Λ|,
(5.33)
where constant terms are omitted. On the right-hand side of the equation above,
Eu

uT
k AT ΛAuk + uT
k uk

= Eu

uT
k

AT ΛA + I

uk

= Eu

uT
k Γ uk

= Eu

tr

ukuT
k Γ

= tr

Eu

ukuT
k

Γ

= tr

(¯uk ¯uT
k + Γ −1)Γ

= ¯uT
k Γ ¯uk,
(5.34)
where a constant term is again omitted. Also, we can show that
Eu

yT
k ΛAuk + uT
k AT Λyk

= yT
k ΛA¯uk + ¯uT
k AT Λyk
= yT
k ΛAΓ −1Γ ¯uk + ¯uT
k Γ Γ −1 AT Λyk = 2¯uT
k Γ ¯uk,
(5.35)
where Eq.(5.11) is used. Substituting the above four equations into Eq.(5.31), we
can get Eq.(5.30).
5.2.5 Summary of the BFA Algorithm
Let us summarize the BFA algorithm. The algorithm is based on the factor analysis
model presented in Eq.(5.2). The algorithm estimates the factor activity uk, the
mixing matrix A, and the sensor-noise precision matrix Λ. In the estimation, ﬁrst

82
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
the model order L is set according to some prior knowledge, and appropriate initial
values are given to A and Λ. In the E-step, Γ and ¯uk (k = 1, . . . , L) are updated
according to Eqs.(5.10) and (5.11). In the M-step, parameters A and Λ are updated
using Eqs.(5.18) and (5.23). The marginal likelihood p(y|A, Λ) in Eq.(5.30) can
be used for monitoring the progress of the EM iteration. If the likelihood increase
becomes very small with respect to the iteration count, the EM iteration may be
stopped.
The BFA can be applied to denoising the sensor data. Using the BFA algorithm,
the estimate of the signal component, yS
k , is given by
yS
k = Eu[Auk] = AEu[uk] = A¯uk.
(5.36)
ThisyS
k may be used for further analysis such as source localization. We can compute
the sample data covariance using only the signal component Auk, which is
¯R = 1
K
K

k=1
Eu

(Auk)(Auk)T 
= 1
K
K

k=1
AEu

ukuT
k

AT
= 1
K
K

k=1
A

¯uk ¯uT
k + Γ −1
AT = A

1
K
K

k=1
¯uk ¯uT
k

AT + AΓ −1 AT . (5.37)
This ¯R can be used in source imaging algorithms such as adaptive beamformers.
Note that, on the right-hand side of Eq.(5.37), the second term AΓ −1 AT works
as a regularization term and ¯R has a form in which the regularization is already
incorporated.
5.3 Variational Bayes Factor Analysis (VBFA)
5.3.1 Prior Distribution for Mixing Matrix
In the BFA algorithm described in the preceding section, a user must determine
the model order L, according to prior knowledge of the measurement. However,
determination of the model order is not easy in most practical applications. We
describe here an extension of the Bayesian factor analysis based on the variational
Bayesian method [6], The method is called the variational Bayesian factor analysis
(VBFA), in which the model order determination is embedded in the algorithm. On
the basis of the factor analysis model in Eq.(5.2), the VBFA algorithm estimates the
posterior probability distributions not only for the factor activity uk but also for the
mixing matrix A.

5.3 Variational Bayes Factor Analysis (VBFA)
83
For convenience in the following arguments, we deﬁne the jth row of the mixing
matrix A as the column vector a j, i.e., the M × L matrix A is expressed as
A =
⎡
⎢⎢⎢⎣
A1,1 . . . A1,L
A2,1 . . . A2,L
...
...
...
AM,1 . . . AM,L
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
aT
1
aT
2...
aT
M
⎤
⎥⎥⎥⎦,
(5.38)
where
a j =
	
A j,1, . . . , A j,L

T .
(5.39)
The prior distribution of a j is assumed to be2
p(a j) = N(a j|0, (λ jα)−1),
(5.40)
where the jth diagonal element of the noise precision matrix Λ is denoted λ j, and
α is a diagonal matrix given by
α =
⎡
⎢⎢⎢⎣
α1 0 . . . 0
0 α2 . . . 0
...
... ...
...
0
0 . . . αL
⎤
⎥⎥⎥⎦.
(5.41)
Here, note that we assume that elements of a j are statistically independent because
the precision matrix in Eq.(5.40) is diagonal. We further assume that ai and a j are
statistically independent when i ̸= j. Thus, the prior probability distribution for the
whole mixing matrix A is expressed as
p(A) =
M

j=1
N(a j|0, (λ jα)−1).
(5.42)
This equation is equivalent to
p(A) =
M

j=1
L

ℓ=1
N(A j,ℓ|0, (λ jαℓ)−1).
(5.43)
2 In the prior distribution in Eq.(5.40), the precision matrix has a form of a diagonal matrix α
multiplied by a scalar λ j. The inclusion of this scalar, λ j, is just for convenience in the mathematical
expressions for the update equations of Ψ and ¯a j. The inclusion of λ j actually makes these update
equations signiﬁcantly simpler.

84
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Since p(A) is a so-called conjugate prior, the posterior probability distribution
has the form of the Gaussian distribution:
p(a j|y) = N(a j|¯a j, (λ jΨ )−1),
and
p(A|y) = p(a1, . . . , aM|y) =
M

j=1
p(a j|y) =
M

j=1
N(a j|¯a j, (λ jΨ )−1),
(5.44)
where ¯a j and λ jΨ are the mean and precision matrix of the posterior distribution.
Namely, the posterior distribution p(A|y) has a form identical to the prior distribution
p(A) with the diagonal α replaced by the non-diagonal Ψ . We deﬁne for later use
the matrix ¯A, such that
¯A =
⎡
⎢⎢⎢⎢⎣
¯aT
1
¯aT
2...
¯aT
M
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎣
¯A1,1 . . . ¯A1,L
¯A2,1 . . . ¯A2,L
...
...
...
¯AM,1 . . . ¯AM,L
⎤
⎥⎥⎥⎦.
(5.45)
In the VBFA algorithm, an overspeciﬁed model order L is used, i.e., the value of L
is set greater than the true model order L0. The posterior mean of a j, ¯a j:
¯a j =
	 ¯A j,1, . . . , ¯A j,L0, ¯A j,L0+1, . . . , ¯A j,L

T
is the Bayes estimate of the jth row of the mixing matrix. In this estimated mix-
ing matrix, the matrix elements ¯A j,L0+1, . . . , ¯A j,L are those corresponding to non-
existing factors. In the VBFA algorithm, those elements are estimated to be sig-
niﬁcantly small, and the inﬂuence of the overspeciﬁed factors are automatically
eliminated in the ﬁnal estimation results.
5.3.2 Variational Bayes EM Algorithm (VBEM)
5.3.2.1 E-Step
We follow the arguments in Sect.B.6 in the Appendix, and derive the variational
Bayes EM algorithm. The posterior distribution p(u, A|y) is approximated by
p(u, A|y) = p(u|y)p(A|y),
(5.46)

5.3 Variational Bayes Factor Analysis (VBFA)
85
The E-step of the VBEM algorithm is a step that estimates p(u|y), and according
to the arguments in Sect.B.6, the estimate of the posterior distribution, p(u|y), is
obtained as
log p(u|y) = E A
	
log p(u, y, A)

,
(5.47)
where E A [·] indicates the expectation with respect to p(A|y). To obtain p(u|y), we
substitute
p(u, y, A) = p(y|u, A)p(u, A) = p(y|u, A)p(u)p(A),
(5.48)
into Eq.(5.47). Neglecting constant terms, we can derive,
log p(u|y) = E A
	
log p(y|u, A) + log p(u) + log p(A)

= E A
	
log p(y|u, A) + log p(u)

=
K

k=1
E A
	
log p(yk|uk, A) + log p(uk)

.
(5.49)
In the equation above, the term log p(A) is omitted because it does not contain u.
Since we have assumed that the prior and the noise probability distributions are
independent across time, we have the independence of the posterior with respect to
time, i.e.,
p(u|y) =
K

k=1
p(uk|yk).
(5.50)
Using Eqs.(5.49) and (5.50), we obtain
log p(uk|yk) = E A
	
log p(yk|uk, A) + log p(uk)

.
(5.51)
Substituting Eqs.(5.4) and (5.7) into (5.51), we get
log p(uk|yk) = E A

−1
2(yk −Auk)T Λ(yk −Auk) −1
2 uT
k uk

.
(5.52)
Since the posterior p(uk|yk) is Gaussian, we assume
p(uk|yk) = N(uk|¯uk, Γ −1),
(5.53)
where ¯uk and Γ are the mean and the precision of this posterior distribution.

86
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
The E-step of the VBEM algorithm estimates p(uk|yk), i.e., it estimates ¯uk, and
Γ . For this estimation, we compute the derivative
∂
∂uk log p(uk|yk). Using Eq.(5.52),
the derivative is given by
∂
∂uk
log p(uk|yk) = E A

AT Λ(yk −Auk)

−uk
= ¯AT Λyk −E A

AT ΛA

uk −uk,
(5.54)
where E A(A) = ¯A, i.e., ¯A (deﬁned in Eq.(5.45)) is the mean of the posterior p(A|y).
Let us compute E A
	
AT ΛA

contained in the right-hand side of Eq.(5.54). Noting
that the diagonal elements of Λ are denoted λ1, . . . , λM, AT ΛA is rewritten as
AT ΛA = [a1, . . . , aM]
⎡
⎢⎣
λ1 . . . 0
...
0 . . . λM
⎤
⎥⎦
⎡
⎢⎣
aT
1...
aT
M
⎤
⎥⎦=
M

j=1
λ j a j aT
j ,
giving
E A

AT ΛA

=
M

j=1
λ j E A

a j aT
j

.
(5.55)
Since the precision matrix of the posterior distribution of a j is λ jΨ , we have
E A

a j aT
j

= ¯a j ¯aT
j + 1
λ j
Ψ −1.
Therefore, the relationship
E A

AT ΛA

=
M

j=1
λ j

¯a j ¯aT
j + 1
λ j
Ψ −1

=
M

j=1
λ j ¯a j ¯aT
j + MΨ −1 = ¯AT Λ ¯A + MΨ −1
(5.56)
holds.
Substituting Eq.(5.56) into (5.54), we get
∂
∂uk
log p(uk|yk) = ¯AT Λyk −

¯AT Λ ¯A + MΨ −1 + I

uk.
(5.57)

5.3 Variational Bayes Factor Analysis (VBFA)
87
The posterior precision matrix Γ is obtained as the coefﬁcient of uk in the right-hand
side of Eq.(5.57), i.e., Γ is given by
Γ = ¯AT Λ ¯A + MΨ −1 + I.
(5.58)
The mean ¯uk is obtained as the value of uk that makes the right-hand side of Eq.(5.57)
equal to zero. That is, we have
¯uk =

¯AT Λ ¯A + MΨ −1 + I
−1 ¯AT Λyk.
(5.59)
5.3.2.2 M-Step
The M-step of the VBEM algorithm estimates p(A|y). According to the arguments
in Sect.B.6 in the Appendix, the estimate of the posterior distribution, p(A|y), is
obtained as
log p(A|y) = Eu
	
log p(u, y, A)

,
(5.60)
where Eu [ · ] indicates the expectation with respect to the posterior distribution,
p(u|y). To obtain p(A|y), we substitute Eq.(5.48) into (5.60). Neglecting terms
(such as log p(u)) that do not contain A, we derive,
log p(A|y) = Eu
	
log p(y|u, A) + log p(A)

.
(5.61)
Substituting Eqs.(5.8) and (5.42) into (5.61), omitting terms not containing A, we get
log p(A|y) =
K

k=1
Eu

−1
2(yk −Auk)T Λ(yk −Auk)

−1
2
M

j=1
λ j aT
j αa j.
(5.62)
As in Eq.(5.44), the posterior p(a j|y) is Gaussian with the mean ¯a j and the
precision λ jΨ . The precision λ jΨ can be obtained by the coefﬁcient of a j in the
derivative
∂
∂A log p(A|y), and ¯a j can be obtained as a j that makes this derivative
equal to zero. To compute the derivative, we ﬁrst rewrite:
M

j=1
λ j aT
j αa j =
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ,
(5.63)
and we can ﬁnd
∂
∂A j,ℓ
1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ= λ jαℓA j,ℓ= [ΛAα] j,ℓ.
(5.64)

88
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Therefore, we have
∂
∂A
1
2
M

j=1
λ j aT
j αa j = ΛAα.
(5.65)
We also have
∂
∂A
K

k=1
−1
2(yk −Auk)T Λ(yk −Auk) = Λ
K

k=1
(yk −Auk)uT
k .
(5.66)
Consequently, we can derive
∂
∂A log p(A|y) = Eu

Λ
K

k=1
(yk −Auk)uT
k

−ΛAα
= ΛRyu −ΛA(Ruu + α).
(5.67)
Setting the right-hand side of the equation above to zero, we get
¯A = Ryu(Ruu + α)−1.
(5.68)
where Ruu and Ryu are deﬁned in Eqs.(5.15) and (5.16). The precision λ jΨ is
obtained as the coefﬁcient of a j in the right-hand side of Eq.(5.67). The second term
in the right-hand side of this equation can be rewritten as
ΛA(Ruu + α) =
⎡
⎢⎣
λ1aT
1
...
λM aT
M
⎤
⎥⎦(Ruu + α) =
⎡
⎢⎣
λ1aT
1 (Ruu + α)
...
λM aT
M(Ruu + α)
⎤
⎥⎦.
(5.69)
Thus, Ψ is obtained as
Ψ = Ruu + α.
(5.70)
Equations(5.68) and (5.70) are the M-step update equations in the VBFA algorithm.
However, to compute these equations, we need to know the hyperparameters α and
Λ. The next subsection deals with the estimation of α.
5.3.2.3 Update Equation for Hyperparameter α
The hyperparameter α is estimated by maximizing the free energy.3 According to
the arguments in Sect.B.6, the free energy is expressed as
3 An estimate that maximizes the free energy is the MAP estimate under the assumption of the
non-informative prior for the hyperparameter.

5.3 Variational Bayes Factor Analysis (VBFA)
89
F[α, Λ] = E(A,u)
	
log p(u, y, A) −log p(u|y) −log p(A|y)

= E(A,u)[log p(y|u, A) + log p(u)
+ log p(A) −log p(u|y) −log p(A|y)],
(5.71)
where E(A,u)[ · ] indicates the average regarding both p(A|y) and p(u|y). On the
right-most-side of the equation above, the terms containing α are only log p(A).
Thus, omitting the terms not containing α, we can rewrite the free energy as
F[α, Λ] = E(A,u)
	
log p(A)

.
(5.72)
Considering
log p(A) = log
M

j=1
N(a j|0, λ jα) = 1
2
M

j=1
log |λ jα| −1
2
M

j=1
aT
j λ jαa j
= 1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ,
(5.73)
The free energy in Eq.(5.72) is rewritten as
F[α, Λ] = E(A,u)
⎡
⎣1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦.
(5.74)
To derive the estimate of α, we differentiate the free energy in (5.74) with respect
to α, resulting in
∂
∂αF[α, Λ] = E A
⎡
⎣∂
∂α
⎡
⎣1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦
⎤
⎦,
(5.75)
where the expectation Eu[ · ] is omitted because the terms on the right-hand side of
the equation above do not contain u. To compute the equation above, we consider
the relationship
∂
∂αℓ
⎡
⎣1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦= M
2 α−1
ℓ
−1
2
M

j=1
λ j A2
j,ℓ,
(5.76)

90
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
and obtain
∂
∂α
⎡
⎣1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦
= M
2 α−1 −1
2
⎡
⎢⎢⎢⎢⎣
M
j=1 λ j A2
j,1
0
. . .
0
0
M
j=1 λ j A2
j,2 . . .
0
0
0
...
0
0
0
. . . M
j=1 λ j A2
j,L
⎤
⎥⎥⎥⎥⎦
= 1
2(Mα−1 −diag[AΛAT ]),
(5.77)
where diag[ · ] indicates a diagonal matrix whose diagonal entries are equal to those
of a matrix in the brackets.4 The relationship
E A
⎡
⎣
M

j=1
λ j A2
j,ℓ
⎤
⎦=
M

j=1
λ j ¯A2
j,ℓ+
M

j=1
λ j
1
λ j
[Ψ −1]ℓ,ℓ=
M

j=1
λ j ¯A2
j,ℓ+ M[Ψ −1]ℓ,ℓ
alsoholds.(NotethatEq.(5.168)showsthegeneralcaseofcomputing E A[Ai,k A j,ℓ].)
Thus, we ﬁnally obtain
∂
∂αF[α, Λ] = E A
⎡
⎣∂
∂α
⎡
⎣1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦
⎤
⎦
= 1
2 Mα−1 −1
2
⎡
⎢⎢⎢⎢⎣
M
j=1 λ j ¯A2
j,1
0
. . .
0
0
M
j=1 λ j ¯A2
j,2 . . .
0
0
0
...
0
0
0
. . . M
j=1 λ j ¯A2
j,L
⎤
⎥⎥⎥⎥⎦
−1
2 M
⎡
⎢⎢⎢⎣
[Ψ −1]1,1
0
. . .
0
0
[Ψ −1]2,2 . . .
0
0
0
...
0
0
0
. . . [Ψ −1]L,L
⎤
⎥⎥⎥⎦
= 1
2(Mα−1 −diag[ ¯AT Λ ¯A] −M diag[Ψ −1]).
(5.78)
4 Here computing the derivative of a scalar X with a diagonal matrix A is equal to creating a diagonal
matrix whose ( j, j)th diagonal element is equal to ∂X/∂A j, j where A j, j is the ( j, j)th diagonal
element of A.

5.3 Variational Bayes Factor Analysis (VBFA)
91
Therefore, setting the right-hand side of the equation above to zero, we have the
update equation for α, such that,
α−1 = diag
 1
M
¯AT Λ ¯A + Ψ −1

.
(5.79)
5.3.2.4 Update Equation for the Noise Precision Λ
We next derive the update equation for Λ. To do so, we maximize the free energy in
Eq.(5.71)withrespecttoΛ.Ontheright-hand-sideofEq.(5.71),thetermscontaining
Λ are log p(y|u, A) and log p(A). Thus, omitting the terms not containing Λ, the
free energy is expressed as
F[α, Λ] = E(A,u)
	
log p(y|u, A) + log p(A)

= E(A,u)
 K
2 log |Λ| −1
2
K

k=1
(yk −Auk)T Λ(yk −Auk)
+ 1
2
M

j=1
L

ℓ=1
log(λ jαℓ) −1
2
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ

.
(5.80)
To maximize F[α, Λ] with respect to Λ, we consider the relationship
1
∂λ j
M

j=1
L

ℓ=1
1
2 log(λ jαℓ) −1
2λ jαℓA2
j,ℓ

=
L

ℓ=1
1
2
1
λ j
−1
2αℓA2
j,ℓ

= L
2
1
λ j
−1
2
L

ℓ=1
αℓA2
j,ℓ,
and obtain
1
∂Λ
M

j=1
L

ℓ=1
1
2 log(λ jαℓ) −1
2λ jαℓA2
j,ℓ

= L
2
⎡
⎢⎢⎢⎣
1/λ1
0
. . .
0
0
1/λ2 . . .
0
0
...
...
0
0
0
. . . 1/λM
⎤
⎥⎥⎥⎦
−1
2
⎡
⎢⎢⎢⎢⎣
L
ℓ=1 αℓA2
1,ℓ
0
. . .
0
0
L
ℓ=1 αℓA2
2,ℓ. . .
0
0
...
...
0
0
0
. . . L
ℓ=1 αℓA2
M,ℓ
⎤
⎥⎥⎥⎥⎦

92
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
= L
2 Λ−1 −1
2 diag[AαAT ].
(5.81)
Then,usingEq.(5.80),wederivethederivativeofF(Λ, α)withrespecttoΛsuchthat
∂
∂ΛF(Λ, α)
= E(A,u)

K
2 Λ−1 −1
2
K

k=1
(yk −Auk)(yk −Auk)T + L
2 Λ−1 −1
2 AαAT

= K
2 Λ−1 + 1
2 E A[−Ryy + Ryu AT + ARuy −ARuu AT + LΛ−1 −AαAT ]
= K
2 Λ−1 −1
2(Ryy −Ryu ¯AT −¯ARuy) + L
2 Λ−1 −1
2 E A[ARuu A + AαAT ].
(5.82)
The following relationship holds:
E A[ARuu A + AαAT ] = E A[A(Ruu + α)AT ] = E A[AΨ AT ]
(5.83)
Also, we can prove the relationship
E A[AΨ AT ] = ¯AΨ ¯AT + LΛ−1 = Ryu ¯AT + LΛ−1.
(5.84)
The proof of this equation is provided in Sect.5.7.1. Thus, the relationship
∂
∂ΛF(Λ, α) = K
2 Λ−1 −1
2(Ryy −Ryu ¯AT −¯ARuy)
+ L
2 Λ−1 −1
2(Ryu ¯AT + LΛ−1) = K
2 Λ−1 −1
2(Ryy −¯ARuy)
(5.85)
holds. Setting the right-hand side of the equation above zero, we obtain the update
equation for Λ as
Λ−1 = 1
K diag(Ryy −¯ARuy).
(5.86)
5.3.3 Computation of Free Energy
Although we cannot compute the exact marginal likelihood in the VBEM algorithm,
we can compute its lower bound using the free energy. The free energy after the
E-step of the VBEM algorithm is expressed as

5.3 Variational Bayes Factor Analysis (VBFA)
93
F = E(A,u)[log p(y, u, A) −log p(u|y) −log p(A|y)]
= E(A,u)
	
log p(y|u, A) + log p(u) + log p(A)

+ H(p(u|y)) + H(p(A|y)).
(5.87)
Substituting
log p(y|u, A) = K
2 log |Λ| −1
2
K

k=1
(yk −Auk)T Λ(yk −Auk),
(5.88)
log p(u) = −1
2
K

k=1
ukuT
k ,
(5.89)
log p(A) = 1
2
M

j=1
log |λ jα| −1
2
M

j=1
aT
j λ jαa j,
(5.90)
and
H(p(u|y)) = −K
2 log |Γ |,
(5.91)
H(p(A|y)) = −1
2
M

j=1
log |λ jΨ |,
(5.92)
into Eq.(5.87), we obtain
F = K
2 log |Λ| + E(A,u)

−1
2
K

k=1
(yk −Auk)T Λ(yk −Auk) −1
2
K

k=1
ukuT
k

+ 1
2
M

j=1
log |λ jα| −1
2 E A
⎡
⎣
M

j=1
aT
j λ jαa j
⎤
⎦−K
2 log |Γ | −1
2
M

j=1
log |λ jΨ |.
(5.93)
Note that the following relationships:
E(A,u)

−1
2
K

k=1
(yk −Auk)T Λ(yk −Auk) −1
2
K

k=1
ukuT
k

= −1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯uT
k Γ ¯uk
(5.94)

94
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
and
M

j=1
log |λ jα| −
M

j=1
log |λ jΨ | =
M

j=1
log
λM
j |α|
λM
j |Ψ | = M log |α|
|Ψ |
(5.95)
hold. The proof of Eq.(5.94) is presented in Sect.5.7.2. Also, we can show
E A
⎡
⎣
M

j=1
aT
j λ jαa j
⎤
⎦= E A
⎡
⎣
M

j=1
L

ℓ=1
λ jαℓA2
j,ℓ
⎤
⎦
=
M

j=1
L

ℓ=1
λ jαℓ¯A2
j,ℓ+
M

j=1
L

ℓ=1
λ jαℓ
1
λ j
[Ψ −1]ℓ,ℓ
=
M

j=1
L

ℓ=1
λ jαℓ¯A2
j,ℓ+ M
L

ℓ=1
αℓ[Ψ −1]ℓ,ℓ
=
M

j=1
¯aT
j λ jα¯a j + M tr

αΨ −1
.
(5.96)
By substituting the above three equations into Eq.(5.93), we obtain the expression
for the free energy:
F = K
2 log |Λ|
|Γ | −1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯uT
k Γ ¯uk
+ M
2 log |α|
|Ψ | −1
2
M

j=1
¯aT
j λ jα¯a j −M
2 tr

αΨ −1
.
(5.97)
Note here that
M

j=1
¯aT
j λ jα¯a j = tr

¯Aα ¯AT Λ

= tr

α ¯AT Λ ¯A

.
(5.98)
Thus, we have
M

j=1
¯aT
j λ jα¯a j + M tr

αΨ −1
= tr

α

¯AT Λ ¯A + MΨ −1
= tr

αMα−1
= ML.
(5.99)

5.3 Variational Bayes Factor Analysis (VBFA)
95
In the equation above, we use the update equation for α in Eq.(5.79). Therefore,
ignoring constant terms, the free energy is ﬁnally expressed as
F = K
2 log |Λ|
|Γ | −1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯uT
k Γ ¯uk + M
2 log |α|
|Ψ |.
(5.100)
Since the free energy is limited (i.e., upper bounded) by the likelihood log p(y|θ)
(where θ collectively expresses the hyperparameters), increasing the free energy
increases the likelihood.
5.3.4 Summary of the VBFA Algorithm
Let us summarize the VBFA algorithm. The algorithm is based on the factor analysis
model in Eq.(5.2). The algorithm estimates the factor activity uk, the mixing matrix
A, and the sensor-noise precision matrix Λ. In the estimation, an overspeciﬁed value
is set for the model order L, and appropriate initial values are set for ¯A, Ψ , and Λ.
In the E-step, Γ and ¯uk (k = 1, . . . , K) are updated according to Eqs.(5.58) and
(5.59). In the M-step, values of ¯A, and Ψ are updated using Eqs.(5.68), and (5.70).
The hyperparameters, α and Λ are updated using Eqs.(5.79) and (5.86). Since
we cannot compute the marginal likelihood, the free energy in Eq.(5.100)—which
is the lower limit of the marginal likelihood—is used for monitoring the progress of
the VBEM iteration. That is, if the increase of F in Eq.(5.100) becomes very small
with respect to the iteration count, the VBEM iteration can be terminated.
Using the VBFA algorithm, the estimate of the signal component in the sensor
data, yS
k , is given by
yS
k = E(A,u)[Au] = E A[A]Eu[u] = ¯A¯uk.
(5.101)
In the equation above, we use the values of ¯uk and ¯A obtained when the VBEM
iteration is terminated. The sample covariance matrix can be computed using only
the signal component, and such a covariance matrix is derived as
¯R = 1
K
K

k=1
E(u,A)

(Auk)(Auk)T 
= 1
K
K

k=1
EuE A

AukuT
k AT 
= 1
K
K

k=1
E A

AEu

ukuT
k

AT 
= 1
K E A

ARuu AT 
,
(5.102)
where Ruu is deﬁned in Eq.(5.15). We can show
E A

ARuu AT 
= ¯ARuu ¯AT + Λ−1 tr

RuuΨ −1
,
(5.103)

96
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
where the proof is presented in Sect.5.7.3. Using Eq.(5.103), we have
¯R = 1
K
¯ARuu ¯AT + 1
K Λ−1 tr

RuuΨ −1
.
(5.104)
This ¯R can be used in source imaging algorithms such as the adaptive beamformers.
5.4 Partitioned Factor Analysis (PFA)
5.4.1 Factor Analysis Model
The bioelectromagnetic data is often contaminated not only by sensor noise but also
by various types of interference of biological and nonbiological origins. A simple
modiﬁcation of the VBFA algorithm enables the removal of such interferences. The
modiﬁedalgorithmiscalledthepartitionedfactoranalysis(PFA)[1].Theprerequisite
for the PFA algorithm is that a control measurement, which contains the interferences
but does not contain the signal of interest, be available. The factor analysis model
for PFA is expressed as
yk = Bvk + ε
for control data,
(5.105)
yk = Auk + Bvk + ε for target data.
(5.106)
In the equations above, L × 1 column vector uk is the factor activity that represents
the signal of interest and A is an M ×L mixing matrix. Also, Lv ×1 column vector vk
is the factor activity that represents the interference and B is an M × Lv interference
mixing matrix.
The PFA algorithm has a two-step procedure. The ﬁrst step applies the VBFA
algorithm to the control data and estimates the mixing matrix B and the sensor-
noise precision Λ. The second step estimates the mixing matrix A and the signal
factor activity uk. In the second step, the interference mixing matrix B and the noise
precision Λ are ﬁxed, and the VBFA algorithm is applied to the target data, which is
expressed as
yk = Auk + Bvk + ε = [A, B]
 uk
vk

+ ε = Aczk + ε,
(5.107)
where
Ac = [A, B] and
zk =
 uk
vk

.
The equation above indicates that the second step can also be expressed by the factor
analysis model using the augmented factor vector zk and the augmented mixing
matrix Ac.

5.4 Partitioned Factor Analysis (PFA)
97
5.4.2 Probability Model
The prior probability distributions for the factor vectors are:
P(uk) = N(uk|0, I),
(5.108)
P(vk) = N(vk|0, I).
(5.109)
Thus, we have the prior probability distribution for the augmented factor vector zk,
such that
p(zk) = p(uk)p(vk) = N(uk|0, I)N(vk|0, I)
=

I
2π

1/2
exp

−1
2 uT
k uk
 
I
2π

1/2
exp

−1
2vT
k vk

=

I
2π

1/2
exp

−1
2 zT
k zk

= N(zk|0, I).
(5.110)
The noise is assumed to be Gaussian,
P(ε) = N(ε|0, Λ−1).
Therefore, we have the conditional probability, P(yk|uk, vk), such that
P(yk|uk, vk) = P(yk|zk) = N(yk|Auk + Bvk, Λ−1) = N(yk|Aczk, Λ−1).
(5.111)
We assume the same prior for A as in Eq.(5.42).
5.4.3 VBEM Algorithm for PFA
5.4.3.1 E-Step
The E-step estimates the posterior probability p(zk|yk). Using the same arguments
in Sect.5.3.2.1, we have,
log p(zk|yk) = E A
	
log p(zk, yk, Ac)

= E A
	
log p(yk|zk, Ac) + log p(zk) + log p(Ac)

.
(5.112)
Note that, since B is ﬁxed, p(Ac) is the same as p(A), which is expressed in
Eq.(5.42). Omitting the terms not containing zk, and using Eqs.(5.110) and (5.111),
we obtain
log p(zk|yk) = E A

−1
2(yk −Aczk)T Λ(yk −Aczk)

−1
2 zT
k zk.
(5.113)

98
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Since p(zk|yk) is Gaussian, we assume that
p(zk|yk) = N(zk|¯zk, Γ −1).
The mean ¯zk is obtained as zk that makes
∂
∂zk log p(zk|yk) equal to zero, and the
precision Γ is obtained from the coefﬁcient of zk in this derivative. Using Eq.(5.113),
the derivative is given by
∂
∂zk
log p(zk|yk) = E A[AT
c Λ

yk −Aczk

] −zk
= ¯AT
c Λyk −E A[AT
c ΛAc]zk −zk.
(5.114)
In the expression above, using Eq.(5.56), E A[AT
c ΛAc] is expressed as
E A[AT
c ΛAc] = E A

AT
BT

Λ[AT BT ]

=

E A[AT ΛA] ¯AT ΛB
BT Λ ¯A
BT ΛB

=

¯AT Λ ¯A + MΨ −1 ¯AT ΛB
BT Λ ¯A
BT ΛB

= ¯AT
c Λ ¯Ac + MΨ −1
c ,
(5.115)
where
Ψ −1
c
=

Ψ −1 0
0
0

,
(5.116)
and
¯Ac =
	 ¯A, B

.
(5.117)
Therefore, we ﬁnally have
∂
∂zk
log p(zk|yk) = ¯AT
c Λyk −( ¯AT
c Λ ¯Aczk + MΨ −1
c
+ I)zk.
(5.118)
The precision matrix of the posterior distribution is obtained as
Γ = ¯AT
c Λ ¯Ac + MΨ −1
c
+ I,
(5.119)
and the mean of the posterior as
¯zk =
 ¯uk
¯vk

= Γ −1

¯AT
BT

Λyk.
(5.120)
Equations(5.119) and (5.120) are the E-step update equations in the PFA algorithm.

5.4 Partitioned Factor Analysis (PFA)
99
5.4.3.2 M-Step
The M-step estimates the posterior probability p(Ac|yk). Using the same arguments
as in Sect.5.3.2.2, we have
log p(Ac|y)
= Ez
	
log p(y, z, Ac)

= Ez
	
log p(y|z, Ac) + log p(z) + log p(Ac)

= Ez
 K

k=1
log p(yk|zk, Ac) + log p(Ac)

= Ez
⎡
⎣−1
2
K

k=1
(yk −Auk −Bvk)T Λ(yk −Auk −Bvk) −1
2
M

j=1
aT
j λ jαa j
⎤
⎦,
(5.121)
where z collectively expresses z1, . . . , zK , and terms not containing A are omitted.
The form of the posterior distribution in Eq.(5.44) is also assumed:
p(Ac|y) = p(A|y) =
M

j=1
N(a j|¯a j, (λ jΨ )−1).
The mean ¯A is obtained as the A that makes
∂
∂A log p(A|y) equal to zero, and the
precision λ jΨ as the coefﬁcient of a j in that derivative. Using Eqs.(5.65) and (5.66)
the derivative is computed as
∂
∂A log p(A|yk) = Ez

Λ
K

k=1
(yk −Auk −Bvk)uT
k

−ΛAα
= ΛRyu −ΛBRvu −ΛA(Ruu + α).
(5.122)
In Eq.(5.122), the coefﬁcient of a j is λ j(Ruu + α), and thus the matrix Ψ is equal
to
Ψ = Ruu + α,
(5.123)
and ¯A is obtained as
¯A = (Ryu −BRvu)Ψ −1,
(5.124)
where Ryu is obtained as
Ryu =
K

k=1
yk ¯uT
k .
(5.125)

100
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Also, Rvu and Ruu are obtained in the following manner. We ﬁrst deﬁne
Γ −1 =
 Γ uu Γ uv
Γ vu Γ vv

,
(5.126)
and have
 Ruu Ruv
Rvu Rvv

= Ez
 K

k=1
 uk
vk

[uT
k , vT
k ]

=
K
k=1 ¯uk ¯uT
k
K
k=1 ¯uk ¯vT
k
K
k=1 ¯vk ¯uT
k
K
k=1 ¯vk ¯vT
k

+ K
 Γ uu Γ uv
Γ vu Γ vv

.
(5.127)
Thus, we obtain,
Ruu =
K

k=1
¯uk ¯uT
k + K Γ uu,
(5.128)
Rvu =
K

k=1
¯vk ¯uT
k + K Γ vu.
(5.129)
The hyperparameter α is obtained by maximizing the free energy, which is
expressed as
F = E(A,u)[log p(y|z, Ac) + log p(z) + log p(Ac)
−log p(z|y) −log p(Ac|y)].
(5.130)
However, since α is contained only in log p(Ac), (which is equal to log p(A)), the
update equation for α is exactly the same as that in Eq.(5.79).
5.4.4 Summary of the PFA Algorithm
The PFA algorithm is summarized as follows. The ﬁrst step estimates the interference
mixing matrix B and the diagonal noise precision matrix Λ by applying the VBFA
algorithm to the control data. The second step applies the PFA-VBEM algorithm to
the target data, and estimates the signal factor vector uk and the signal mixing matrix
A. In the second step, B and Λ are ﬁxed at the values obtained in the ﬁrst step. There
is a different version of the PFA algorithm in which B and Λ are also updated in the
second step. The details of the algorithm are given in [1].
The free energy is computed in exactly the same manner as in Eq.(5.100) except
that ¯uk is replaced by ¯zk. Thus, the free energy is expressed as

5.4 Partitioned Factor Analysis (PFA)
101
F = K
2 log |Λ|
|Γ | −1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯zT
k Γ ¯zk + M
2 log |α|
|Ψ |.
(5.131)
This free energy is used for monitoring the progress of the PFA-VBEM iteration.
Using the PFA algorithm, the estimate of the signal of interest, yS
k , is given by.
yS
k = E(A,u)[Auk] = E A[A]Eu[uk] = ¯A¯uk.
(5.132)
The sample covariance matrix computed only using the signal of interest is given by
¯R = 1
K
¯ARuu ¯AT + 1
K Λ−1 tr

RuuΨ −1
,
(5.133)
where Ruu is deﬁned in Eq.(5.128). This ¯R can be used in source imaging algorithms
such as the adaptive beamformers, and resultant images can be free from the inﬂuence
of interferences.
5.5 Saketini: Source Localization Algorithm
Based on the VBFA Model
5.5.1 Data Model
This section describes a virtual-sensor type source localization algorithm, called
Saketini [2]. The Saketini algorithm is based on Bayesian factor analysis, and enables
the estimation of source activity. In the Saketini algorithm, the sensor data y(t) is
modeled using
y(t) = L(r)s(r, t) + Au(t) + ε,
(5.134)
where s(r, t) is the source vector deﬁned in Eq.(2.3). An M × 3 matrix, L(r), is the
lead ﬁeld matrix at r, which is deﬁned in Eq.(2.4), and ε is the additive sensor noise.
In Eq.(5.134), the signal from the source activity at r is represented by L(r)s(r, t).
On the basis of the factor analysis model, the interference is modeled using Au(t)
where A is an M × L factor mixing matrix, and u(t) is an L × 1 factor vector. Here,
Au(t) represents all interference and source activities except the source activity at r.
As in the preceding sections, y(tk), s(r, tk), and u(tk) are denoted yk, sk, and uk,
respectively. L(r) is denoted L for simplicity. Then, Eq.(5.134) is rewritten as
yk = Lsk + Auk + ε.
(5.135)
The goal of this algorithm is to estimate sk from the data yk. Once sk is obtained, the
algorithm pointing location r is scanned over the whole brain region to obtain the spa-
tiotemporal reconstruction of the source activity over the whole brain. To formulate

102
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
the estimation problem based on the VBFA algorithm, Eq.(5.135) is rewritten as
yk = Lsk + Auk + ε = [L, A]
 sk
uk

+ ε = As zk + ε,
(5.136)
where zk = [sT
k , uT
k ]T , and As = [L, A]. This equation indicates that the problem
of estimating sk can be expressed by the factor analysis model using the augmented
factor vector zk and the augmented mixing matrix As. Equation(5.136) is, in princi-
ple, the same as Eq.(5.107). Therefore, the algorithm developed here is very similar
to the PFA algorithm described in Sect.5.4.
5.5.2 Probability Model
We assume the prior distribution for sk to be zero-mean Gaussian, such that
p(sk) = N(sk|0, Φ−1),
(5.137)
where Φ is a 3 × 3 (non-diagonal) precision matrix of this prior distribution. The
prior distribution of the factor vector uk is assumed to be:
p(uk) = N(uk|0, I).
(5.138)
Then, the prior distribution for zk is derived as
p(zk) = p(sk)p(uk) = N(sk|0, Φ−1)N(uk|0, I)
=

Φ
2π

1/2
exp

−1
2 sT
k Φsk
 
I
2π

1/2
exp

−1
2 uT
k uk

=

Φ
2π

1/2
exp

−1
2 zT
k Φzk

= N(zk|0, Φ−1),
(5.139)
where
Φ =
Φ 0
0 I

.
Equation(5.139) indicates that the prior distribution of zk is the mean-zero Gaussian
with the precision matrix of Φ. The sensor noise is also assumed to be zero-mean
Gaussian with a diagonal precision matrix Λ. Hence, we have
p(yk|sk, uk) = N(yk|Lsk + Auk, Λ−1) = N(yk|As zk, Λ−1).
(5.140)
Weassumethesamepriordistributionforthemixingmatrix A,asshowninEq.(5.42).

5.5 Saketini: Source Localization Algorithm Based on the VBFA Model
103
5.5.3 VBEM Algorithm
5.5.3.1 E-Step
The posterior p(z|y) is Gaussian, and assumed to be,
p(z|y) =
K

k=1
p(zk|yk),
and
p(zk|yk) = N(zk|¯zk, Γ −1),
(5.141)
where ¯zk and Γ are, respectively, the mean and precision of p(zk|yk). Using similar
arguments as for the E-step of the PFA algorithm, the estimate of p(zk|yk), p(zk|yk),
is given by
log p(zk|yk) = E A[log p(yk|zk, A)] + log p(zk)],
(5.142)
where we omit terms that do not contain zk. The precision Γ of the posterior is
derived from the coefﬁcient of
∂
∂zk p(zk|yk), and the mean ¯zk is derived as the zk that
makes this derivative zero.
Deﬁning ¯As = E A ([L, A]) = [L, ¯A], we obtain
∂
∂zk
log p(zk|yk) = E A[AT
s Λ(yk −As zk)] −Φzk
= E A[AT
s Λyk] −E A[AT
s ΛAs]zk −Φzk,
(5.143)
where
E A[AT
s Λyk] = ¯AT
s Λyk
and
E A[AT
s ΛAs] =
 E A[LT ΛL] E A[LT ΛA]
E A[AT ΛL] E A[AT ΛA]

=

LT ΛL
LT Λ ¯A
¯AT ΛL
¯AT Λ ¯A + MΨ −1

= ¯AT
s Λ ¯As + M
0
0
0 Ψ −1

.
(5.144)
Hence, we have
E A[AT
s ΛAs]zk = ( ¯AT
s Λ ¯As + MΨ −1
s )zk,
(5.145)
where
Ψ −1
s
=
 0
0
0 Ψ −1

.
(5.146)

104
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
Thus, we derive
∂
∂zk
log p(zk|yk) = ¯AT
s Λyk −( ¯AT
s Λ ¯As + MΨ −1
s
+ Φ)zk.
(5.147)
Consequently, the coefﬁcient of zk gives
Γ = ¯AT
s Λ ¯As + MΨ −1
s
+ Φ.
(5.148)
The mean ¯zk is obtained as
¯zk =
 ¯sk
¯uk

= Γ −1

LT
¯AT

Λyk.
(5.149)
5.5.3.2 M-Step
The derivation for the M step is also very similar to the M step of the PFA-VBEM
algorithm in Sect.5.4.3.2. The estimation of p(A|y) is based on
log p(A|y) = −1
2 Ez
 K

k=1
(yk −Lsk −Auk)T Λ(yk −Lsk −Auk)

−1
2
M

j=1
aT
j λ jαa j.
(5.150)
The derivative
∂
∂A log p(A|y) is obtained as,
∂
∂A log p(A|y) = Ez

Λ
K

k=1
(yk −Lsk −Auk)uT
k

−ΛAα
= ΛRyu −ΛL Rsu −ΛA(Ruu + α).
(5.151)
The precision, λ jΨ , is equal to the coefﬁcient of a j, and it is obtained as λ j(Ruu+α).
Hence we have
Ψ = (Ruu + α).
(5.152)
Setting the derivative to zero gives ¯A, i.e.,
¯A = (Ryu −L Rsu)(Ruu + α)−1 = (Ryu −L Rsu)Ψ −1.
(5.153)

5.5 Saketini: Source Localization Algorithm Based on the VBFA Model
105
In the equations above, Rsu and Ruu are obtained in the following manner. Deﬁning
Γ −1 =
 Γ ss Γ su
Γ us Γ uu

,
(5.154)
we have
 Rss Rsu
Rus Ruu

=Ez
 K

k=1
 sk
uk

[sT
k , uT
k ]

=
 K
k=1 ¯sk ¯sT
k
K
k=1 ¯sk ¯uT
k
K
k=1 ¯uk ¯sT
k
K
k=1 ¯uk ¯uT
k

+ K
 Γ ss Γ su
Γ us Γ uu

.
(5.155)
Therefore, we obtain:
Rss =
K

k=1
¯sk ¯sT
k + K Γ ss,
(5.156)
Rsu =
K

k=1
¯sk ¯uT
k + K Γ su,
(5.157)
Ruu =
K

k=1
¯uk ¯uT
k + K Γ uu.
(5.158)
Also, Ryu is obtained as
Ryu =
K

k=1
yk ¯uT
k .
(5.159)
5.5.3.3 Update Equations for Hyperparameters
The update equations for Φ, Λ, and α are derived by maximizing the free energy,
which is rewritten as
F(Λ, α, Φ) = E(z,A)[log p(y|z, As) + log p(z) + log p(A)],
(5.160)
where the terms −log p(z|y) −log p(A|y) are ignored because these terms contain
none of these hyperparameters. To compute F(Λ, α, Φ), we use the relationships,

106
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
log p(y|z, As) =
K

k=1
log p(yk|zk, As)
= K
2 log |Λ| −1
2
K

k=1
(yk −Lsk −Auk)T Λ(yk −Lsk −Auk),
(5.161)
log p(z) =
K

k=1
log p(zk) = K
2 log |Φ| −1
2
K

k=1
zT
k Φzk
= K
2 log |Φ| −1
2
K

k=1
sT
k Φsk −1
2
K

k=1
uT
k uk,
(5.162)
and
log p(A) = 1
2
M

j=1
log |λ jα| −1
2
M

j=1
aT
j λ jαa j.
(5.163)
Let us derive the update equation for Φ. In Eq.(5.160), the only term that contains
Φ is log p(z). Thus, we have
1
∂Φ F(Λ, α, Φ) =
1
∂Φ
K
2 log |Φ| −1
∂Φ
1
2 E(z,A)
 K

k=1
sT
k Φsk

= K
2 Φ−1 −1
2 Es
 K

k=1
sksT
k

= K
2 Φ−1 −1
2 Rss,
(5.164)
and setting the right-hand side to zero gives the update equation,
Φ−1 = 1
K Rss,
(5.165)
where Rss is obtained in Eq.(5.156). We can derive the update equation for α in
a similar manner. However, Since in Eq.(5.160), the only term that contains α is
log p(A), the update equation for α is exactly the same as that in Eq.(5.79). The
update equation for Λ is given by
Λ−1 = 1
K diag[Ryy −Rys LT −L Rsy + L Rss LT −¯AΨ ¯AT ].
(5.166)
Since the derivation of the equation above is lengthy, it is presented in Sect.5.7.4.

5.5 Saketini: Source Localization Algorithm Based on the VBFA Model
107
5.5.4 Summary of the Saketini Algorithm
The Saketini algorithm is summarized as follows. It estimates the source time course
at a pointing location, based on the data model in Eq.(5.134). It uses the VBEM algo-
rithm in which the E-step updates the posterior precision Γ and the posterior mean ¯zk
by using Eqs.(5.148) and (5.149). The M-step updates Ψ and ¯A using Eqs.(5.152)
and (5.153). The hyperparameter Φ is updated using Eq.(5.165). The hyperparame-
ter α is updated using (5.79). The noise precision Λ is updated using Eq.(5.166).
The free energy can be computed using the same equation as in Eq.(5.131) with the
sole modiﬁcation of adding log |Φ|. That is, the free energy is expressed as
F = K
2 log |Λ||Φ|
|Γ |
−1
2
K

k=1
yT
k Λyk + 1
2
K

k=1
¯zT
k Γ ¯zk + M
2 log |α|
|Ψ |.
(5.167)
This free energy forms the lower bound of the marginal likelihood, and it is used for
monitoring the progress of the VBEM iteration.
5.6 Numerical Examples
Computer simulation was performed to present typical results from the methods
mentioned in this chapter. An alignment of the 275-sensor array from the Omega™
(VMS Medtech, Coquitlam, Canada) neuromagnetometer was used. The coordinate
system and source-sensor conﬁguration used in the computer simulation are depicted
in Fig.5.1. A vertical plane was assumed at the middle of the whole-head sensor
array (x = 0cm), and three sources were assumed to exist on this plane. The time
courses shown in the upper three panels of Fig.5.2 were assigned to the three sources.
Fig. 5.1 The coordinate
system and source-sensor
conﬁguration used in the
computer simulation. A
vertical plane was assumed at
the middle of the whole-head
sensor array (x = 0cm), and
three sources were assumed
to exist on this plane at
x = 0cm. The large circle
shows the source space and
the small ﬁlled circles show
the locations of the three
sources
-2
0
2
  8
10
y (cm)
sensor array
z
y
z (cm)
6
first source
second source
third source

108
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
0.5
0
0.5
0.5
0
0.5
0.5
0
0.5
0
200
400
600
800
1000
0.1
0
0.1
time points
Fig. 5.2 The time courses of the three sources are shown in the upper three panels. The bottom
panel shows the signal-only magnetic recordings (before noise is added). The selected three time
courses from sensor #1, #101, and #201 are shown. The ordinates show relative values, and the
abscissa shows the time points. A total of 1,200 time point data was generated
Simulated magnetic recordings were computed by projecting the time courses of the
three sources onto the sensor space using the sensor lead ﬁeld at the source locations.
These recordings are shown for three selected sensor channels in the bottom panel
of Fig.5.2.
We ﬁrst performed denoising experiments using the BFA and VBFA algorithms.
We generated simulated MEG data by adding the sensor noise onto the computed
magnetic recordings shown in the bottom panel of Fig.5.2. Here, the noise generation
was performed using a Gaussian random number, and the signal-to-noise ratio (SNR)
was set to one. The noise-added simulated sensor data is shown in the top panel of
Fig.5.3 for the same selected sensor channels.
The BFA algorithm was applied to these simulated MEG data and the signal
component was estimated by computing A¯uk. The results of this experiment are
presented in Fig.5.3. The results with the number of factors L set to 3 are shown in
the second panel from the top, and the results with L set to 20 are in the third panel.
Since we assume that three sources exist in this numerical experiment, the correct
value of L is three.

5.6 Numerical Examples
109
0.2
0.1
0
0.1
0.2
0.1
0
0.1
0.1
0
0.1
0
200
400
600
800
1000
0.1
0
0.1
time points
Fig. 5.3 Results of denoising experiments using BFA and VBFA. The top panel shows the sensor
recordings before denoising. The second panel from the top shows the denoising results by BFA
with the model order L set to three. The third panel shows the denoising results by BFA with L set
to twenty. The bottom panel shows the denoising results by VBFA with L set to twenty. The time
courses of the selected three sensor channels, (channel #1, channel #101, and channel #201), are
shown. The ordinates show relative values, and abscissa shows the time points
Results in the second panel (results with the correct L) are better than the results in
the third panel (results with a signiﬁcantly overespeciﬁed L), and we can see that the
overspeciﬁcation of the number of factors, L, reduces the denoising capability of the
algorithm. The VBFA algorithm was then applied, and the results are shown in the
bottom panel of Fig.5.3. Here, although the number of factors L was set to 20, results
close to those obtained using the BFA algorithm with the correct value of L were
obtained. The results demonstrate the fact that the VBFA algorithm incorporates the
model order determination and is insensitive to the overspeciﬁcation of the number
of factors.
We next performed experiments on the interference removal using the PFA algo-
rithm. In this experiment, the simulated MEG recordings were generated with 2,400
time points where the ﬁrst half period does not contain the signal activity, which
is contained in the second half period. The interference data was computed by

110
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
generating ﬁfty interference sources in the source space. The interference sources
had Gaussian random time courses, and the interference magnetic recordings were
generated by projecting these time courses onto the sensor space using the sensor
lead ﬁeld at the interference source locations. The simulated MEG recordings were
generated by adding the interference (and a small amount of sensor noise) onto the
same simulated signal-only sensor data in the previous denoting experiment.
In the PFA application, the ﬁrst half period was used as the control period and the
mixing matrix of the interference B and the diagonal noise precision matrix Λ were
estimated using this control period. We then estimated the signal mixing matrix A
and the signal factor activity uk using the second half period. The signal component
was estimated using yS
k in Eq.(5.132).
Results are shown in Fig.5.4. In this ﬁgure, the top panel shows the control period
(the ﬁrst half period) of the original interference-added sensor data, and the middle
panel shows the target period (the second half period) of the original interference-
added sensor data. The bottom panel shows the interference-removed sensor data,yS
k .
Here, the number of factors for the interference was set to 100, and that for the signal
of interest was set to 10. Note that the true number of independent interference activ-
ities is 50, and that for the signal activity is 3. The results in Fig.5.4 demonstrate that
0.2
0
0.2
0.2
0
0.2
0
200
400
600
800
1000
0.1
0
0.1
time points
Fig.5.4 Resultsofinterference-removalexperimentsusingthePFAalgorithm.Thetoppanel shows
the control period (the ﬁrst 1,200 time points) of the interference-added simulated sensor data. The
middle panel shows the target period (the second 1,200 time points) of the interference-added
simulated sensor data. The bottom panel shows the results of interference removal experiments.
Here, results of yS
k in Eq.(5.132) are plotted. The selected three sensor time courses (channel #1,
channel #101, and channel #201) are shown in these three panels

5.6 Numerical Examples
111
the PFA algorithm successfully removes the interference, even though the number
of factors for the signal and interference components are signiﬁcantly overspeciﬁed.
We next performed a source reconstruction experiment. We computed the
interference-removed covariance matrix ¯R in Eq.(5.133), and used it with the adap-
tive beamformer algorithm. The results are shown in Fig.5.5. In Fig.5.5a, the results
of source reconstruction, obtained using the signal-only data (data before adding
the interferences), are shown. The results of source reconstruction obtained with the
interference-added data are shown in Fig.5.5b. We can observe a certain amount of
distortion in Fig.5.5b. The results of reconstruction using ¯R are shown in Fig.5.5c.
The distortion is signiﬁcantly reduced, demonstrating the effectiveness of the PFA
algorithm.
Finally, a source reconstruction experiment using the Saketini algorithm was per-
formed. Simulated MEG data was generated in which the signal-to-noise ratio (SNR)
was set equal to two. We computed the power map using tr(Φ−1) with the number of
factors L set to 20. The results are shown in Fig.5.6. The reconstructed source power
map on the plane of x = 0 is shown in Fig.5.6a. The reconstructed time courses
y (cm)
z (cm)
(a)
4
2
0
2
4
5
6
7
8
9
10
11
12
y (cm)
z (cm)
(b)
4
2
0
2
4
5
6
7
8
9
10
11
12
y (cm)
z (cm)
(c)
4
2
0
2
4
5
6
7
8
9
10
11
12
Fig. 5.5 Results of source reconstruction experiments using the interference-removed covariance
matrix ¯R in Eq.(5.133). The adaptive beamformer algorithm was used for source reconstruction,
and the reconstructed source distribution at x = 0cm is shown. a The results obtained with a sample
covariance matrix computed from signal-only simulated recordings. b The results obtained with
a sample covariance matrix computed from the interference-added sensor data. c The results of
the interference-removal experiment, obtained with ¯R in Eq.(5.133). The (y, z) coordinates of the
three sources were set to (−2.0, 10.2)cm, (2.5, 10.2)cm, and (1.0, 7.2)cm in this experiment

112
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
y (cm)
z (cm)
(a)
4
2
0
2
4
4
6
8
10
12
y (cm)
z (cm)
(c)
4
2
0
2
4
4
6
8
10
12
0
500
1000
(b)
time point
0
500
1000
(d)
time point
Fig. 5.6 Results of source reconstruction experiments using the Saketini algorithm. a The recon-
structed source power map on the plane at x = 0cm. b The reconstructed time courses of the
three sources: The time courses of the ﬁrst, second, and third sources are shown in the top, middle,
and bottom panels, respectively. c The reconstructed source power map on the plane at x = 0cm,
obtained using the Champagne algorithm. d The reconstructed time courses of the three sources,
obtained using the Champagne algorithm. The locations (y, z) of the ﬁrst, second, and third sources
were set to (−1.8, 9.8), (2.0, 9.8), and (1.3, 6.0), respectively
of the voxels closest to the three sources are shown in Fig.5.6b. The source power
map and source time courses obtained using the Champagne algorithm are shown
in Fig.5.6c, d for comparison. The results show that both algorithms are capable of
providing accurate spatiotemporal reconstruction of the three sources.
5.7 Appendix to This Chapter
5.7.1 Proof of Eq.(5.84)
We present the proof of Eq.(5.84), which is rewritten as
E A[AΨ AT ] = ¯AΨ ¯AT + LΛ−1 = Ryu ¯AT + LΛ−1.

5.7 Appendix to This Chapter
113
First, we have
[AΨ AT ]i, j =

k,ℓ
Ai,k[Ψ ]k,ℓA j,ℓ=

k,ℓ
[Ψ ]k,ℓAi,k A j,ℓ.
Equation(5.44) expresses the posterior distribution of the mixing matrix A where
the two elements, Ai,k and A j,ℓ, are independent when i ̸= j and when i = j, the
covariance of these elements is equal to [λ−1
j Ψ −1]k,ℓ. Thus, we have
E A[Ai,k A j,ℓ] = ¯Ai,k ¯A j,ℓ+ δi, j
1
λ j
[Ψ −1]k,ℓ.
(5.168)
Therefore, we get
E A

[AΨ AT ]i, j

=

k,ℓ
[Ψ ]k,ℓ¯Ai,k ¯A j,ℓ+

k,ℓ
[Ψ ]k,ℓδi, j
1
λ j
[Ψ −1]k,ℓ.
(5.169)
The ﬁrst term on the right-hand side of the equation above is rewritten as

k,ℓ
[Ψ ]k,ℓ¯Ai,k ¯A j,ℓ= [ ¯AΨ ¯AT ]i, j.
Considering

k,ℓ
[Ψ ]k,ℓ[Ψ −1]k,ℓ= tr

Ψ Ψ −1
= tr (I) = L,
(5.170)
the second term in Eq.(5.169) is rewritten as

k,ℓ
[Ψ ]k,ℓδi, j
1
λ j
[Ψ −1]k,ℓ= Lδi, j
1
λ j
= L[Λ−1]i, j,
(5.171)
because Λ is diagonal. Consequently, we get the following relationship:
E A[AΨ AT ] = ¯AΨ ¯AT + LΛ−1.
(5.172)
Using Eqs.(5.68) and (5.70), we obtain the relationship
¯A = RyuΨ −1
or
¯AΨ = Ryu.
Thus, we ﬁnally derive Eq.(5.84), such that
E A[AΨ AT ] = Ryu ¯AT + LΛ−1.
(5.173)

114
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
5.7.2 Proof of Eq.(5.94)
We provide a proof of Eq.(5.94), which is repeated here for convenience.
E(A,u)

−1
2
K

k=1
(yk −Auk)T Λ(yk −Auk) −1
2
K

k=1
ukuT
k

= −1
2
K

j=1
yT
k Λyk + 1
2
K

j=1
¯uT
k Γ ¯uk.
The left-hand side of the equation above is changed to:
E(A,u)

−1
2
K

k=1
(yk −Auk)T Λ(yk −Auk) −1
2
K

k=1
ukuT
k

= −1
2
K

k=1
yT
k Λyk
−1
2
K

k=1
E(A,u)

−uT
k AT Λyk −yT
k ΛAuk + uT
k AT ΛAuk + uT
k uk

.
(5.174)
On the right-hand side of Eq.(5.174), the following relationship:
E(A,u)

uT
k AT ΛAuk + uT
k uk

= Eu

uT
k

E A

AT ΛA

+ I

uk

= Eu

uT
k

¯AT Λ ¯A + MΨ −1 + I

uk

= Eu

uT
k Γ uk

= Eu

tr

ukuT
k Γ

= tr

Eu

ukuT
k

Γ

= tr

(¯uk ¯uT
k + Γ −1)Γ

= ¯uT
k Γ ¯uk
(5.175)
holds, where constant terms are omitted. In the equation above, we use Eq.(5.56).
Also, the relationship,
E(A,u)

yT
k ΛAuk + uT
k AT Λyk

= yT
k Λ ¯A¯uk + ¯uT
k ¯AT Λyk
= yT
k Λ ¯AΓ −1Γ ¯uk + ¯uT
k Γ Γ −1 ¯AT Λyk
= 2¯uT
k Γ ¯uk
(5.176)
holds. Substituting Eqs.(5.175) and (5.176) into (5.174), we get Eq.(5.94).

5.7 Appendix to This Chapter
115
5.7.3 Proof of Eq.(5.103)
Next, the proof of Eq.(5.103) is presented, which is
E A

ARuu AT 
= ¯ARuu ¯AT + Λ−1 tr (RuuΨ ) .
The proof is quite similar to the one provided in Sect.5.7.1. First, we have
[ARuu AT ]i, j =

k,ℓ
Ai,k[Ruu]k,ℓA j,ℓ=

k,ℓ
[Ruu]k,ℓAi,k A j,ℓ.
Using Eq.(5.168),we get
E A

[ARuu AT ]i, j

=

k,ℓ
[Ruu]k,ℓ¯Ai,k ¯A j,ℓ+

k,ℓ
[Ruu]k,ℓδi, j
1
λ j
[Ψ −1]k,ℓ.
(5.177)
The ﬁrst term on the right-hand side of the equation above is rewritten as

k,ℓ
[Ruu]k,ℓ¯Ai,k ¯A j,ℓ= [ ¯ARuu ¯AT ]i, j.
Considering

k,ℓ
[Ruu]k,ℓ[Ψ −1]k,ℓ= tr(RuuΨ −1),
(5.178)
the second term in Eq.(5.177) is rewritten as

k,ℓ
[Ruu]k,ℓδi, j
1
λ j
[Ψ −1]k,ℓ= [Λ−1]i, j tr(RuuΨ −1).
(5.179)
Consequently, we get the following relationship:
E A[ARuu AT ] = ¯ARuu ¯AT + Λ−1 tr(RuuΨ −1).
(5.180)
5.7.4 Proof of Eq.(5.166)
Next,theproofofEq.(5.166)ispresented.Toderivethisequation,weshouldconsider
log p(y|z, A) and log p(A), because Λ is contained in these two terms. We have

116
5
Bayesian Factor Analysis: A Versatile Framework for Denoising …
∂
∂Λ log p(y|z, A)
= ∂
∂Λ

K
2 log |Λ| −1
2
K

k=1
(yk −Lsk −Auk)T Λ(yk −Lsk −Auk)

= K
2 Λ−1 −1
2
K

k=1
(yk −Lsk −Auk)(yk −Lsk −Auk)T .
(5.181)
According to Eq.(5.81), we have
∂
∂Λ log p(A) = L
2 Λ−1 −1
2 diag

AαAT 
.
(5.182)
We can thus compute the derivative of F(Λ, α, Φ) with respect to Λ, which is
∂F
∂Λ = E(A,z)
 K
2 Λ−1 −1
2
K

k=1
(yk −Lsk −Auk)(yk −Lsk −Auk)T
+ L
2 Λ−1 −1
2 AαAT

= K
2 Λ−1 −1
2[Ryy −Ryu ¯AT −¯ARuy −Rys LT −L Rsy
+ L Rsu ¯AT + ¯ARus LT + L Rss LT ]
−1
2 E A[ARuu AT ] + L
2 Λ−1 −1
2 E A[AαAT ].
(5.183)
According to Eqs.(5.83) and (5.84), we get
E A[ARuu AT + AαAT ] = E A[A(Ruu + α)AT ]
= E A[AΨ AT ] = ¯AΨ ¯AT + LΛ−1.
(5.184)
Using the equation above and the relationship ¯AΨ = (Ryu −L Rsu), and setting
the right-hand side of Eq.(5.183) equal to zero, we derive the update equation for Λ,
such that,
Λ−1 = 1
K diag[Ryy −Rys LT −L Rsy + L Rss LT −¯AΨ ¯AT ].
The equation above is equal to Eq.(5.166).

References
117
References
1. S.S. Nagarajan, H.T. Attias, K.E. Hild, K. Sekihara, A probabilistic algorithm for robust inter-
ference suppression in bioelectromagnetic sensor data. Stat. Med. 26, 3886–3910 (2007)
2. J.M. Zumer, H.T. Attias, K. Sekihara, S.S. Nagarajan, A probabilistic algorithm integrating
source localization and noise suppression for MEG and EEG data. NeuroImage 37, 102–115
(2007)
3. H. Attias, Inferring parameters and structure of latent variable models by variational bayes,
in Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, (Morgan
Kaufmann Publishers Inc, 1999), pp. 21–30
4. A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete data via the EM
algorithm. J. R. Stat. Soc. Ser. B (Methodological) 39, 1–38 (1977)
5. R.H. Shumway, D.S. Stoffer, An approach to time series smoothing and forecasting using the
EM algorithm. J. Time Ser. Anal. 3, 253–263 (1982)
6. H.T. Attias, A variational Bayesian framework for graphical models, in Advances in Neural
Information Processing (MIT Press, Cambridge, 2000), pp. 209–215

Chapter 6
A Uniﬁed Bayesian Framework
for MEG/EEG Source Imaging
6.1 Introduction
Magnetoencephalography (MEG) and related electroencephalography (EEG) use
an array of sensors to take electromagnetic ﬁeld (or voltage) measurements from
on or near the scalp surface with excellent temporal resolution. In both MEG and
EEG, the observed ﬁeld can in many cases be explained by synchronous, compact
current sources located within the brain. Although useful for research and clinical
purposes, accurately determining the spatial distribution of these unknown sources
is a challenging inverse problem. The relevant estimation problem can be posed
as follows: The measured electromagnetic signal is y ∈Rdy×dt , where dy equals
the number of sensors and dt is the number of time points at which measurements
are made. Each unknown source sr ∈Rdc×dt is a dc-dimensional neural current
dipole, at dt timepoints, projecting from the rth (discretized) voxel or candidate
location distributed throughout the brain. These candidate locations can be obtained
by segmenting a structural MR scan of a human subject and tessellating the brain
volume with a set of vertices, y and each sr are related by the likelihood model
y =
ds

r=1
Lr sr + ε,
(6.1)
where ds is the number of voxels under consideration and Lr ∈Rdy×dc is the
so-called lead-ﬁeld matrix for the rth voxel. The kth column of Lr represents the
signal vector that would be observed at the scalp given a unit current source/dipole
at the rth vertex with a orientation in the kth direction. It is common to assume
dc = 2 (for MEG with a single spherical shell model) or dc = 3 (for EEG), which
allows ﬂexible source orientations to be estimated in 2D or 3D space. Multiple
methods based on the physical properties of the brain and Maxwell’s equations are
available for the computation of each Li, as described in Appendix A. Finally, ε
is a noise-plus-interference term where we assume, for simplicity, that the columns
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_6
119

120
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
are drawn independently from N(0, Σϵ). However, temporal correlations can easily
be incorporated if desired using a simple transformation outlined in [1] or using
the spatio-temporal framework introduced in [2]. In this chapter, we will mostly
assume that Σϵ is known; however, robust procedures for its estimation can be found
in Chap.5, and can naturally be incorporated into the proposed model. However, it
should be noted that joint estimation of brain source activity and Σϵ potentially leads
to identiﬁability issues, and remains an unsolved problem to date.
To obtain reasonable spatial resolution, the number of candidate source locations
will necessarily be much larger than the number of sensors (ds ≫dy). The salient
inverse problem then becomes the ill-posed estimation of regions with signiﬁcant
brain activity, which are reﬂected by voxels i such that |si| > 0; we refer to these
as active dipoles or sources. The severe underdetermine nature of this MEG (or
related EEG) source localization problem (since the mapping from source activity
conﬁguration s ≜[sT
1 , . . . , sT
ds]T to sensor measurement y is many to one), requires
the incorporation of prior assumptions when choosing an appropriate solution out of
an inﬁnite set of candidates.
Bayesian approaches are useful in this capacity because they allow these assump-
tions to be explicitly quantiﬁed using postulated prior distributions. However, the
means by which these priors are chosen, as well as the estimation and inference pro-
cedures that are subsequently adopted to affect localization, have led to a daunting
array of algorithms with seemingly very different properties and assumptions.
While seemingly quite different in many respects, we present a generalized frame-
work that encompasses all of these methods and points to intimate connections
between algorithms. The underlying motivation here is to leverage analytical tools
and ideas from machine learning, Bayesian inference, and convex analysis that have
not as of yet been fully exploited in the context of MEG/EEG source localization.
Speciﬁcally, here we address how a simple Gaussian scale mixture prior with ﬂexible
covariance components underlie and generalize all of the above. This process demon-
strates a number of surprising similarities or out-right equivalences between what
might otherwise appear to be very different methodologies. Theoretical properties
related to convergence, global and local minima, and localization bias are analyzed
and fast algorithms are derived that improve upon existing methods. This perspec-
tive leads to explicit connections between many established algorithms and suggests
natural extensions for handling unknown dipole orientations, extended source conﬁg-
urations, correlated sources, temporal smoothness, and computational expediency.
Speciﬁc imaging methods elucidated under this paradigm include weighted min-
imum ℓ2-norm, FOCUSS [3], MCE [4], VESTAL [5], sLORETA [6], ReML [7]
and covariance component estimation, beamforming, variational Bayes, the Laplace
approximation, and automatic relevance determination (ARD) [8, 9]. Perhaps sur-
prisingly, all of these methods can be formulated as particular cases of covariance
component estimation using different concave regularization terms and optimization
rules, making general theoretical analyses and algorithmic extensions/improvements
particularly relevant. By providing a unifying theoretical perspective and compre-
hensive analyses, neuroelectromagnetic imaging practitioners will be better able to

6.1 Introduction
121
assess the relative strengths of many Bayesian strategies with respect to particular
applications; it will also help ensure that different methods are used to their full
potential and not underutilized.
6.2 Bayesian Modeling Framework
In a Bayesian framework all prior assumptions are embedded in the distribution p(s).
If under a given experimental or clinical paradigm this p(s) were somehow known
exactly, then the posterior distribution p(s|y) can be computed via Bayes rule:
p(s|y) = p(y|s)p(s)
p(y)
.
(6.2)
This distribution contains all possible information about the unknown s conditioned
on the observed data y. Two fundamental problems prevent using p(s|y) for source
localization. First, for most priors p(s), the distribution p(y) given by:
p(y) =

p(y|s)p(s)ds.
(6.3)
cannot be computed. Because this quantity, which is sometimes referred to as the
model evidence or marginal likelihood, is required to compute posterior moments
and is also sometimes used to facilitate model selection, this deﬁciency can be very
problematic. Of course if only a point estimate for s is desired, then this normalizing
distribution may not be needed. For example, a popular estimator involves ﬁnding
the value of s that maximizes the posterior distribution, often called the maximum a
posteriori or MAP estimate, and is invariant to p(y). However MAP estimates may be
unrepresentative of posterior mass and are unfortunately intractable to compute for
most p(s) given reasonable computational resources. Secondly, we do not actually
know the prior p(s) and so some appropriate distribution must be assumed, perhaps
based on neurophysiological constraints or computational considerations. In fact, it
is this choice, whether implicitly or explicitly, that differentiates a wide variety of
localization methods at a very high level.
Such a prior is often considered to be ﬁxed and known, as in the case of minimum
ℓ2-norm approaches [2], minimum current estimation (MCE), FOCUSS, sLORETA,
and minimum variance beamformers. Alternatively, a number of empirical Bayesian
approaches have been proposed that attempt a form of model selection by using
the data to guide the search for an appropriate p(s). In this scenario, candidate
priors are distinguished by a set of ﬂexible hyperparameters γ that must be estimated
via a variety of data-driven iterative procedures including hierarchical covariance
component models, automatic relevance determination (ARD), and several related
variational Bayesian methods (for a more comprehensive list of citations see Wipf
et al. [10]).

122
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
6.3 Bayesian Modeling Using General Gaussian Scale
Mixtures and Arbitrary Covariance Components
In this section, we present a general-purpose Bayesian framework for source
localization and discuss a central distinction between ﬁxed-prior MAP estimation
schemes and empirical Bayesian approaches that adopt a ﬂexible, parameterized
prior. While often derived using different assumptions and methodology, they can
be related via a simple hierarchical structure based on general Gaussian scale mix-
ture distributions with arbitrary covariance components. Numerous special cases of
this model have been considered previously in the context of MEG and EEG source
localization and related problems as will discussed in subsequent sections.
6.3.1 The Generative Model
To begin we invoke the noise model from (6.1), which fully deﬁnes the assumed data
likelihood
p(y|s) ∝exp
⎛
⎝−1
2
y −
ds

i=1
Li si

2
Σ−1
ϵ
⎞
⎠,
(6.4)
where ∥X∥2
W denotes the weighted matrix norm
	
trace[X T W X].
While the unknown noise covariance can also be parameterized and seamlessly
estimated from the data via the proposed paradigm, for simplicity we assume that
Σϵ is known, estimated from the data using a variational Bayesian factor analy-
sis (VBFA) model as discussed in Sect.5.3 and that it is ﬁxed. Next we adopt the
following source prior for s:
p (s|γ) ∝exp

−1
2trace[sT Σ−1
s
s]

, Σs =
dγ

i=1
γiCi.
(6.5)
This is equivalent to applying independently, at each time point, a zero-mean
GaussiandistributionwithcovarianceΣs toeachcolumnof s.Hereγ ≜[γ1, . . . , γdγ]
is a vector of dγ nonnegative hyperparameters that control the relative contribution of
each covariance basis matrix Ci. While the hyperparameters are unknown, the set of
components C ≜{Ci : i = 1, . . . , dγ} is assumed to ﬁxed and known. Such a formu-
lation is extremely ﬂexible however, because a rich variety of candidate covariance
bases can be proposed as will be discussed in more detail later. Moreover, this struc-
ture has been advocated by a number of others in the context of neuroelectromagnetic
source imaging [7, 11]. Finally, we assume a hyperprior on γ of the form
p(γ) =
dγ

i=1
1
2 exp[−fi(γi)]
(6.6)

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
123
where each fi(.) is an unspeciﬁed function that is assumed to be known. The implicit
prior on s, obtained by integrating out (marginalizing) the unknown γ, is known as
a Gaussian scale mixture.
p(s) =

p(s|γ)p(γ)dγ.
(6.7)
6.3.2 Estimation and Inference
Estimation and inference using the proposed model can be carried out in multiple
ways depending how the unknown quantities s and γ are handled. This leads to
a natural partitioning of a variety of inverse methods.We brieﬂy summarize three
possibilities before discussing the details and close interrelationships.
6.3.2.1 Hyperparameter MAP or Empirical Bayesian
Theﬁrstoptionisifγ weresomehowknown(andassumingΣs isknownaswell),then
the conditional distribution p(s|y, γ) ∝p(y|s)p(s|γ) is a fully speciﬁed Gaussian
distribution with mean and covariance given by
Ep(s|y,γ) [s] = γLT 
Σϵ + LΣs LT −1
B
Covp(s j|y,γ)

s j

= Σs −Σs LT 
Σϵ + LΣs LT −1
LΣs, ∀j,
(6.8)
where L ≜[L1, . . . , Lds] and s j denotes the jth column of s (i.e., the sources at
the jth time point) and individual columns are uncorrelated.
It is then common to use the simple estimator ˆs = E p(s|y,γ)[s] for the unknown
sources. However, since γ is actually not known, a suitable approximation ˆγ ≈γ
must ﬁrst be found. One principled way to accomplish this is to integrate out the
sources s and then solve
ˆγ = arg max
γ

p(y|s)p(s|γ)p(γ)ds
(6.9)
This treatment is sometimes referred to as empirical Bayes because the γ-dependent
prior on s, p(s|γ), is empirically learned from the data, often using expectation-
maximization (EM) algorithms which treat s as hidden data. Additionally, the process
of marginalization provides a natural regularizing mechanism that can shrink many
elements of γ, to exactly zero, in effect pruning the associated covariance compo-
nent from the model, with only the relevant components remaining. Consequently,
estimation under this model is sometimes called automatic relevance determination
(ARD). This procedure can also be leveraged to obtain a rigorous lower bound on

124
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
logp(y). While knowing p(s|y) is useful for source estimation given a particular
model, access to p(y) (or equivalently logp(y)) can assist model selection.
γ-MAP obtains a point estimate for the unknown γ by ﬁrst integrating out the
unknown sources s producing the hyperparameter likelihood equation
p(y|γ) =

p(y|s)p(s|γ)ds ∝exp

−1
2 yT Σ−1
y
y

,
(6.10)
where
Σy = Σϵ + LΣs LT .
(6.11)
To estimate γ we then solve:
ˆγ = arg max
γ
p(γ|y) = arg max
γ
p(y|γ)p(γ),
(6.12)
which is equivalent to minimizing the cost function
L(γ) ≜−2 log p(y|γ)p(γ) ≡trace[CyΣ−1
y ] + log |Σy| + 1
n
 n

i=1
fi(γi)

,
(6.13)
where Cy ≜n−1 y yT is the empirical covariance. This cost function is composed of
three parts. The ﬁrst is a data ﬁt term based on the dissimilarity between the empirical
covariance and the model covariance; in general, this factor encourages γ to be large.
The second term provides the primary regularizing or sparsifying effect, penalizing a
measure of the volume formed by the model covariance. Since the volume of any high
dimensional space is more effectively reduced by collapsing individual dimensions
as close to zero as possible (as opposed to reducing all dimensions isometrically),
this penalty term promotes a model covariance that is maximally degenerate (or
non-spherical), which pushes elements of γ to exactly zero.
Finally, the third term follows directly from the hyperprior, which we have thus
far assumed to be arbitrary. This term can be useful for incorporating speciﬁc prior
information, perhaps from fMRI data or some other imaging modality. It can also be
used to expedite hyperparameter pruning or conversely, to soften the pruning process
or prevent pruning altogether. One popular choice is the inverse-Gamma hyperprior
[12] given by
pi(γ−1
i
) = Gam(ai)−1bai
i γ1−ai
i
exp

−bi
γi

(6.14)
for the ith hyper parameter, where Gam(x) =
 inf
0
zx−1 exp(−z)dz is the standard
Gamma function and ai, bi are the shape parameters. In the limit as ai, bi →0
then the prior converges to a non-informative Jeffreys prior, when optimization is
performed on log γi space as is customary in some applications, the effective prior
is ﬂat and can therefore be ignored. In contrast, ai, bi →∞, all hyperparameters

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
125
are constrained to have equal value and essentially no learning (or pruning) occurs.
Consequently, the standard weighted minimum ℓ2-norm solution can be seen as a
special case.
We will often concern ourselves with ﬂat hyperpriors when considering the γ-
MAP option. In this case, the third term in Eq.(6.13) vanishes and the only regular-
ization will come from the log |.| term. In this context, the optimization problem with
respect to the unknown hyperparameters is sometimes referred to as type-II maxi-
mum likelihood. It is also equivalent to the restricted maximum likelihood (ReML)
cost function, discussed by Friston et al. [7] Regardless of how γ is optimized, once
some ˆγ is obtained, we compute ˆΣs which fully speciﬁes our assumed empirical
prior on s. To the extent that the “learned” prior p(s|ˆγ) is realistic, this posterior
quantiﬁes regions of signiﬁcant current density and point estimates for the unknown
sources can be obtained by evaluating the posterior mean.
6.3.2.2 Optimization of γ-MAP
The primary objective of this section is to minimize Eq. (6.13) with respect to γ. For
simplicity, we will ﬁrst present updates with fi(γi) = 0 (i.e. a ﬂat hyper prior). We
then address natural adaptations to the more general cases (e.g., not just conjugate
priors). Of course one option is to treat the problem as a general nonlinear optimiza-
tion task and perform gradient descent or some other generic procedure. In contrast,
here we will focus on methods speciﬁcally tailored for minimizing Eq. (6.13) using
principled methodology. We begin with methods based directly on the EM algorithm
and then diverge to alternatives that draw on convex analysis to achieve faster con-
vergence. One approach to minimizing Eq. (6.13) is the restricted likelihood method
(ReML) which utilizes what amounts to EM-based updates treating s as hidden data.
For the E-step, the mean and covariance of s are computed given some estimate of
the hyperparameters ˆγ. For the M-step, we then must update ˆγ using these moments
as the true values. Unfortunately, the optimal value of ˆγ cannot be obtained in closed
form for arbitrary covariance component sets, so a second-order Fisher scoring pro-
cedure is adopted to approximate the desired solution. While effective for estimating
small numbers of hyperparameters, this approach requires inverting a dγ ×dγ Fisher
information matrix, which is not computationally feasible for large dγ. Moreover,
unlike exact EM implementations, there is no guarantee that such a Fisher scoring
method will decrease the likelihood function Eq. (6.13) at each iteration.
Consequently, here we present alternative optimization procedures that apply to
the arbitrary covariance model discussed above, and naturally guarantee that γi ⩾0
for all i. All of these methods rely on reparameterizing the generative model such
that the implicit M-step can be solved in closed form. First, we note that L(γ) only
depends on the data y through the dy × dy sample correlation matrix Cy. Therefore,
to reduce the computational burden, we replace y with a matrix y ∈Rdy×rank(y)
such that y yT = yyT . This removes any per-iteration dependency on n, which can
potentially be large, without altering that actual cost function. It also implies that, for
purposes of computing γ, the number of columns of s is reduced to match rank(y).

126
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
Next we introduce the decomposition
s =
dγ

i=1
Aisi = As,
(6.15)
where each Ai is selected such that Ai AT
i
= Ci and A ≜[A1 . . . Adγ], s ≜
[sT
1 . . . sT
dγ]T . Letting L ≜[L1, . . . , Ldγ] = L[A1, . . . Adγ], this allows us to re-
express the original hierarchical Bayesian model as
p(y|s) ∝exp(−1
2||y −Ls||2
Σ−1
ϵ )
(6.16)
p(si|γi) ∝exp(−1
2γi
||si||2
F),
∀i = 1, . . . , dγ,
(6.17)
where ||X||F is the standard Frobenius norm
	
trace[X X T ]. The hyperprior remains
unaltered. It is easily veriﬁed by the rules for transformation of random variables
that Eq. (6.16) and the original model are consistent. It also follows that
Σy = Σϵ + L(
dγ

i=1
γiCi)LT = Σϵ +
dγ

i=1
γiLiLi T = Σϵ + LΣsLT
(6.18)
where Σs is the diagonal, γ-dependent prior covariance of the pseudo sources.
There are three ways to derive update rules for γ: EM iteration, Fixed-point
(MacKay) updates, and Convexity based updates. Although the EM iteration update
rules are the most straightforward to derive, they have empirically slow convergence
rates. In contrast, the ﬁxed-point updates have fast convergence rates but no conver-
gence guarantees. In contrast, the convexity based updates have both fast convergence
properties as well as guaranteed convergence properties. Details of derivations and
properties of these update rules can be found elsewhere, but the three types of update
rules are listed here for completeness.
1. EM-Updates
γ(k+1)
i
→1
nri
||γ(k)
i
L
T
i (Σ(k)
y )−1y||2
F + 1
ri
trace

γ(k)
i
I −γ(k)
i
L
T
i (Σ(k)
y )−1 Liγ(k)
i

(6.19)
2. MacKay update:
γ(k+1)
i
→1
n ||γ(k)
i
L
T
i (Σ(k)
y )−1y||2
F(trace[γ(k)
i
L
T
i (Σ(k)
y )−1 Li])−1
(6.20)
3. Convexity-based update
γ(k+1)
i
→γ(k)
i
n ||Li(Σ(k)
y )−1y||F(trace[L
T
i (Σ(k)
y )−1 Li])−1/2
(6.21)

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
127
6.3.2.3 Analysis of γ-MAP
Previously, we have claimed that the γ-MAP process naturally forces excessive/
irrelevant hyperparameters to converge to zero, thereby reducing model complexity.
Note that, somewhat counterintuitively, this occurs even when a ﬂat hyperprior is
assumed. While this observation has been veriﬁed empirically by ourselves and
others in various application settings, there has been relatively little corroborating
theoretical evidence, largely because of the difﬁculty in analyzing the potentially
multimodal, non-convex γ-MAP cost-function. We can then show that: Every local
minimum of the generalized γ-MAP cost function, is achieved at a solution with
utmost rank(y)dy ⩽d2
y non-zero hyper parameters if fi(γi) is concave and non-
decreasing for all i, including ﬂat hyper priors. Therefore, we can be conﬁdent that
the pruning mechanism of γ-MAP is not merely an empirical phenomena. Nor is it
dependent on a particular sparse hyperprior, the result holds when a ﬂat (uniform)
hyperprior is assumed.
The number of observation vectors n also plays an important role in shaping γ-
MAP solutions. Increasing n has two primary beneﬁts: (i) it facilitates convergence
to the global minimum (as opposed to getting stuck in a suboptimal extrema) and (ii),
it improves the quality of this minimum by mitigating the effects of noise. Finally, a
third beneﬁt to using n > 1 is that it leads to temporal smoothing of estimated time
courses (i.e., rows of ˆs. This occurs because the selected covariance components do
not change across time, as would be the case if a separate set of hyperparameters
were estimated at each time point. For purposes of model selection, a rigorous bound
on log p(y) can be derived using principles from convex analysis that have been
successfully applied in general-purpose probabilistic graphical models (see Wipf
and Nagarajan [13]).
6.3.3 Source MAP or Penalized Likelihood Methods
The second option is to integrate out the unknown γ, we can treat p(s) as the effective
prior and attempt to compute a MAP estimate of s via
ˆs = arg max
s

p(y|s)p(s|γ)p(γ)dγ = arg max
s
p(y|s)p(s)
(6.22)
While it may not be immediately transparent, solving s-MAP also leads to a
shrinking and pruning of superﬂuous covariance components. In short, this occurs
because the hierarchical model upon which it is based leads to a convenient, iterative
EM algorithm-based implementation, which treats the hyperparameters γ as hidden
data and computes their expectation for the E-step. Over the course of learning, this
expectation collapses to zero for many of the irrelevant hyperparameters, removing
them from the model in much the same way as γ-MAP.

128
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
In a general setting Eq. (6.22) can be a difﬁcult optimization problem and fur-
thermore, the nature of the underlying cost function is not immediately transpar-
ent. Consequently, we advocate an indirect alternative utilizing the pseudo-source
decomposition given by s described previously, which leads to an efﬁcient EM imple-
mentation and a readily interpretable cost function. It also demonstrates that both
FOCUSS and MCE can be viewed as EM algorithms that are readily generalized to
handle more complex spatio-temporal constraints. Explicitly, we will minimize
L(s) ≜−2 log

p(y|s)p(s|γ)p(γ)dγ
= −2 log p(y|s)p(s)
(6.23)
≡||y −Ls||2
Σ−1
ϵ
+
dγ

i=1
gi(||s||2
F),
where gi(.) is deﬁned as
gi(||s||2
F) ≜−2 log

p(si|γi)pi(γi)dγi.
(6.24)
For many choices of the hyperprior, the associated gi(.) may not be available
in closed form. Moreover, it is often more convenient and transparent to directly
assume the form of gi(.) rather than infer its value from some postulated hyperprior.
Virtually any non-decreasing, concave function gi(.) of interest can be generated by
the proposed hierarchical model. In other words, there will always exist some pi(γi),
possibly improper, such that the stated Gaussian mixture representation will produce
any desired concave gi(.). For example, a generalized version of MCE and FOCUSS
can be produced from the selection gi(z) = ciz p/2, which is concave and amenable
to a Gaussian scale-mixture representation for any p ∈(0, 2] and constant ci > 0.
Presumably, there are a variety of ways to optimize Eq. (6.23). One particularly
straightforward and convenient method exploits the hierarchical structure inherent
in the assumed Bayesian model. This leads to simple and efﬁcient EM-based update
rules. It also demonstrates that the canonical FOCUSS iterations are equivalent to
principled EM updates. Likewise, regularized MCE solutions can also be obtained
in the same manner.
Ultimately, we would like to estimate each si, which in turn gives us the true
sources S. If we knew the values of the hyperparameters γ this would be straightfor-
ward; however, these are of course unknown. Consequently, in the EM framework,
γ is treated as hidden data whose distribution (or relevant expectation) is computed
during the E-step. The M-step then computes the MAP estimate ofs assuming that
γ equals the appropriate expectation. For the (k + 1)th E-step, the expected value
of each γ−1
i
under the distribution p(γ|y,s(k)) is required (see the M-step below)
and can be computed analytically assuming gi(.) is differentiable, regardless of the
underlying form of p(γ). Assuming gi(z) ∝z p/2, it can be shown that

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
129
γ(k+1)
i
≜E p(γi|y,s(k)
i
)[γ−1
i
]−1 =
 1
rin ||s(k)
i ||2
F
 2−p
2
(6.25)
The associated M-step is then readily computed using
s(k+1)
i
→γi L
T
i (Σϵ +
dγ

i=1
γi Li L
T
i )−1y.
(6.26)
s(k+1) = As(k+1) is then the (k + 1)th estimate of the source activity. Each iteration
of this procedure decreases the cost function of Eq. (6.23) and converges to some
ﬁxed point is guaranteed. From a computational stand point, the s-MAP updates are
of the same per-iteration complexity as the γ-MAP updates. Roughly speaking, the
EM iterations above can be viewed as coordinate descent over a particular auxiliary
cost function dependent on both s and γ. There exists a formal duality between s-
MAP and γ-MAP procedures that is beyond the scope of this chapter, but details of
which are elaborate in [10, 14].
Given n = 1, and Ai = ei, where each ei is a standard indexing vector of zeros
with a “1” for the ith element, we get,
γ(k+1)
i
→[γk
i LT
i (λI + L diag[γk)]LT )−1 y](2−p)
(6.27)
where Li is the ith column of L. We then recover the exact FOCUSS updates when
p →0 and a FOCUSS-like update for MCE when p = 1. Note however, that
while previous applications of MCE and FOCUSS to electromagnetic imaging using
n = 1 require a separate iterative solution to be computed at each time point in
isolation, here the entire s can be computed at once with n > 1 for about the same
computational cost as a single FOCUSS run.
The nature of the EM updates for s-MAP, where an estimate of γ is obtained via the
E-step, suggest that this approach is indirectly performing some form of covariance
component estimation. But if this is actually the case, it remains unclear exactly
what cost function these covariance component estimates are minimizing. This is
unlike the case of γ-MAP where it is more explicit. The fundamental difference
between s-MAP and γ-MAP lies in the regularization mechanism of the covariance
components.Unlike γ-MAP the penalty term in Eq. (6.23), is a separable summation
thatdependsonthevalueof p toaffecthyperparameterpruning;andimportantlythere
is no volume-based penalty. For p < 1 the penalty is concave in γ and hence, every
local minimum of Eq. (6.23) is achieved at a solution with at most rank(y)dy ⩽d2
y
non-zero hyper parameters. Rather than promoting sparsity at the level of individual
sourceelementsatagivenvoxelandtime(asoccurswithstandardMCEandFOCUSS
when n = 1, Ci = ei, here sparsity is encouraged at the level of the pseudo-sources,
s. The function gi(.) operates on the Frobenius norm of each Si and favors solutions
with many ||s||F = 0 for many indices of i. Notably though, by virtue of the
Frobenius norm over time, within a non-zero si temporally smooth (non-sparse)

130
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
solutions are favored. This is why, one obtains continuous source estimates over
time, as opposed to application of FOCUSS or MCE at each individual time-point.
Even though sparsity bounds for s-MAP and γ-MAP are somewhat similar, the
actual performance in practical situations is quite distinct. This is because of global
minimum convergence properties of s-MAP that are impacted by the choice of gi(.)
or p. For instance, although setting p = 1 leads to a convex cost function devoid of
non-global minima, and the update rules for generalized-MCE will converge to the a
global minimum, the resultant estimate can have problems recovering the true source
estimate, especially for conditions related to MEG and EEG source reconstructions.
This is because lead-ﬁelds L required for MEG and EEG are highly correlated, and
violate the restricted isometric properties (RIP) that are required for accurate perfor-
mance of ℓ1-norm procedures. More details on this point can be found in [14]. These
theoretical restrictions essentially render the conditions for MCE performance to
reconstruction of 1–2 dipoles at best, with no localization bias guarantees. The prob-
lem with MCE is not the existence of local minima, rather, it is that the global mini-
mum may be unrepresentative of the true source distribution even for simple dipolar
source conﬁgurations. In this situation, and especially when lead ﬁeld columns are
highly correlated, the MCE solution may fail to ﬁnd sufﬁciently sparse source rep-
resentations consistent with the assumption of a few equivalent current dipoles, and
this issue will also persist with more complex covariance components and source
conﬁgurations. Nevertheless, the unimodal, convex nature of the generalized MCE
like procedures are its attractive advantage.
Furthermore, if p < 1 (which implies that gi(z2) is concave in z) then more
pseudo sources will be pruned at any global solution, which often implies that those
which remain may be more suitable than the MCE estimate. Certainly this is true
when estimating dipolar sources, but it likely holds in more general situations as
well. However, local minima can be an unfortunate menace with p < 1. For instance
when p →0, we get the the canonical FOCUSS algorithm, and it has a combinatorial
number of local minima satisfying
ds −1Cdy + 1 ⩽# of FOCUSS Local Minima ⩽ds Cdy.
(6.28)
which is a huge number for practical lead-ﬁeld matrices, and this property largely
explains the sensitivity of FOCUSS to initialization and noise. While the FOCUSS
cost function can be shown to have zero localization bias at the global solution,
because of the tendency to become stuck at local optima, in practice a bias can be
observed when recovering even a single dipolar source. Other selections of p between
zero and one can lead to a similar fate. In the general case, a natural trade-off exists
with s-MAP procedures: greater sparsity of solutions at the global minimum the
less possibility that this minimum is biased, but the higher the chance of suboptimal
convergence to a biased local minimum, and the optimal balance could be application
dependent. Nevertheless, s-MAP is capable of successfully handling large numbers
of diverse covariance components, and therefore simultaneous constraints on the
source space.

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
131
6.3.4 Variational Bayesian Approximation
From the perspective of a Bayesian purist however, the pursuit of MAP estimates
for unknown quantities of interest, whether parameters s or hyperparameters γ, can
be misleading since these estimates discount uncertainty and may not reﬂect regions
of signiﬁcant probability mass, unlike (for example) the posterior mean. Variational
Bayesian methods, which have successfully been applied to a wide variety of hier-
archical Bayesian models in the machine learning literature offer an alternative to
s-MAP and γ-MAP. Therefore, a third possibility involves ﬁnding formal approxi-
mations to p(s|y) as well as the marginal p(y) using an intermediary approximation
for p(y). However, because of the intractable integrations involved in obtaining
either distribution, practical implementation requires additional assumptions lead-
ing to different types of approximation strategies. The principle idea here is that all
unknown quantities should either be marginalized (integrated out) when possible
or approximated with tractable distributions that reﬂect underlying uncertainty and
have computable posterior moments. Practically, we would like to account for ambi-
guity regarding γ when estimating p(s|y), and potentially, we would like a good
approximation for p(y), or a bound on the model evidence log p(y) for applica-
tion to model selection. The only meaningful difference between VB and γ-MAP,
at least in the context of the proposed generative model, involves approximations to
the model evidence log p(y), with VB and γ-MAP giving different estimates.
In this section, we discuss two types of variational approximations germane to the
source localization problem: the mean ﬁeld approximation (VB-MF), and a ﬁxed-
form, Laplace approximation (VB-LA). It turns out that both are related to γ-MAP
but with important distinctions. A mean-ﬁeld approximation makes the simplifying
assumption that the joint distribution over unknowns s and γ and factorizes, mean-
ing p(s, γ|y) ≈ˆp(s|y) ˆp(γ|y) where ˆp(s|y) and ˆp(γ|y) are chosen to minimize
the Kullback-Leibler divergence between the factorized and full posterior. This is
accomplished via an iterative process akin to EM, effectively using two E-steps (one
for s and one for γ). It also produces a rigorous lower bound on logp(y) simi-
lar to γ-MAP. A second possibility applies a second-order Laplace approximation
to the posterior on the hyperparameters (after marginalizing over the sources s),
which is then iteratively matched to the true posterior; the result can then be used
to approximate p(s|y) and log p(y). Both of these VB methods lead to posterior
approximations ˆp(s|y) = p(s|y, γ = ˆγ), ˆγ is equivalently computed via γ-MAP.
Consequently, VB has the same level of component pruning as γ-MAP.
6.3.4.1 Mean-Field Approximation
The basic strategy here is to replace intractable posterior distributions with approxi-
mate ones that, while greatly simpliﬁed and amenable to simple inference procedures,
still retain important characteristics of the full model. In the context of our presumed
model structure, both the posterior source distribution p(s|y), which is maximized

132
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
with s-MAP, as well as the hyperparameter posterior (γ|y), which is maximized via
γ-MAP, are quite complex and can only be expressed up to some unknown scal-
ing factor (the integration required for normalization is intractable). Likewise, the
joint posterior p(s, γ|y) over all unknowns is likewise intractable and complex. VB
attempts to simplify this situation by ﬁnding an approximate joint posterior that
factorizes as
p(s, γ|y) ≈ˆp(s, γ|y) = ˆp(s|y) ˆp(γ|y),
(6.29)
where ˆp(s|y) and ˆp(γ|y) are amenable to closed-form computation of posterior
quantities such as means and variances (unlike the full posteriors upon which our
model is built). This is possible because the enforced factorization, often called the
mean-ﬁeld approximation reﬂecting its origins in statistical physics, simpliﬁes things
signiﬁcantly. The cost function optimized to ﬁnd this approximate distribution is
ˆp(s|y), ˆp(γ|y) = argmin
q(s),q(γ)
KL[q(s)q(γ)||p(s, γ|y)],
(6.30)
where q(s) and q(γ) are arbitrary probability distributions and KL[.||.] indicates the
Kullback-Leibler divergence measure.
Recall that γ-MAP iterations effectively compute an approximate distribution for
s (E-step) and then a point estimate for γ (M-step); s-MAP does the exact opposite.
In contrast, here an approximating distribution is required for both parameters s and
hyperparameters γ. While it is often convenient that conjugate hyperpriors must be
employed such that Eq. (6.30) is solvable, in fact this problem can be solved by
coordinate descent over q(s) and q(γ) for virtually any hyperprior. It can be shown
that γ-MAP and mean-ﬁeld approximations can be equivalent in terms of the cost
function being optimized and the source activity estimates obtained, given certain
choice of hyperpiors [10]. Nevertheless offer a whole class of algorithms within
this framework with different covariance component sets, and possible hyperpriors
selected, and how the optimization is performed. The main advantage of VB is that
strict lower bounds on log p(y) automatically fall out of the VB framework, given by:
log p(y) ⩾F ≜log p(y) −KL[q(s)q(γ)||p(s, γ|y)]
=

ˆp(s|y) ˆp(γ|y) log
p(y, s, γ)
ˆp(s|y) ˆp(γ|y)dγ,
(6.31)
where the inequality follows by the non-negativity of the Kullback-Leibler diver-
gence. The quantity F is sometimes referred to the variational free energy. Evaluation
of F requires the full distribution ˆp(γ|y) and therefore necessitates using conjugate
priors or further approximations.
6.3.4.2 Laplace Approximation
The Laplace approximation has been advocated to ﬁnding a tractable posterior dis-
tribution on the hyperparameters and then using this ˆp(γ|y) to ﬁnd approximations

6.3 Bayesian Modeling Using General Gaussian Scale Mixtures …
133
to p(s|y) and log p(y). To facilitate this process, the hyperparameters are re-
parameterized via the transformation λi ≜log γi, ∀i, which now allows them to have
positive or negative values. The Laplace approximation then involves the assump-
tion that the posterior distribution of these new hyperparameters λ ≜[λ1, . . . , λdγ]T
satisﬁes p(λ|y) ≈N(λ|μλ, Σλ). There are a variety of ways μλ and Σλ can be
chosen to form the best approximation. For instance, a MAP estimate of λ can be
computed by maximizing log p(y, λ) using a second-order Fisher scoring procedure
[1], although this method not guaranteed to increase the cost function and requires
a very expensive O(d3
γ) inverse computation. Once the mode ˆλ is obtained, setting
μλ = ˆλ and match the second-order statistics of the true and approximate distribu-
tions at this mode using Σλ = −[ℓ′′(μλ)]−1 where ℓ(μλ) ≜log p(y, λ = ˆλ) and
ℓ′′(μλ) is the corresponding Hessian matrix evaluated at μλ. Furthermore, because
of the non-negativity of the KL divergence, it always holds that
log p(y) ⩾F ≜log p(y) −KL[N(λ|μλ, Σλ)||p(λ|y)]
=

N(λ|μλ, Σλ) log
p(y, λ)
N(λ|μλ, Σλ)dλ.
(6.32)
Unfortunately however, this bound cannot be computed in closed form, so the com-
putable approximation is used as a surrogate instead [1].
In summary, the approximate distributions ˆp(s|y) obtained from both variational
methods can be shown to be equivalent to a γ-MAP estimate given the appropriate
equalizing hyperprior. Consequently, the optimization procedures for γ-MAP can be
adapted for VB. Furthermore, the sparsifying properties of the underlying γ-MAP
cost functions also apply to VB. Whereas VB-MF gives more principled bounds on
thelikelihood,theLaplaceapproximationmethodsdonot.However,somelimitations
of the Laplace’s approximation method can potentially be ameliorated by combining
the Laplace and mean-ﬁeld approximations as outline in Appendix D of [13].
6.4 Selection of Covariance Components C
While details and assumptions may differ γ-MAP, s-MAP, and VB can all be lever-
aged to obtain a point estimate of S expressed in the Tikhonov regularized form.
ˆS = ˆΣS LT (Σϵ + L ˆΣS LT )−1 y with
ˆΣS =

i
ˆγiCi.
(6.33)
Importantly, since each method intrinsically sets ˆγi = 0 for superﬂuous covariance
components, we can in principle allow the cardinality of C to be very large, and rely
on a data-learning process to select the most appropriate subset. Speciﬁc choices for
C lead to a variety of established algorithms as discussed next.
In the simplest case, the single-component assumption ΣS = γ1C1 = γ1I, where
I is the Identity matrix, leads to a weighted minimum-ℓ2-norm algorithm. More

134
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
interesting covariance component terms have been used to effect spatial smoothness,
depth bias compensation, and candidate locations of likely activity. With regard to
the latter, it has been suggested that prior information about a source location can be
coded by including a second term C2 i.e. C = C1C2, with all zeros except a patch of
1’s along the diagonal signifying a location of probable source activity, perhaps based
on fMRI data. For s-MAP, γ-MAP, VB, we will obtain a source estimate representing
a trade-off (modulated by the relative values of the associated ˆγ1 and ˆγ2) between
honoring the prior information imposed byC2 and the smoothness implied byC1. The
limitation of this proposal is that we generally do not know, a priori, the regions where
activity is occurring with both high spatial and temporal resolution. Therefore, we
cannot reliably known how to choose an appropriate location-prior term C2 in many
situations. A potential solution to this dilemma is to try out many different (or even all
possible) combinations of location priors. For example, if we assume the underlying
source currents are formed from a collection of dipolar point sources located at each
vertex of the leadﬁeld grid, then we may choose C = ei eT
i i = 1, . . . , dS, where each
ei is a standard indexing vector of zeros with a “1” for the ith element (and so Ci =
ei eT
i encodes a prior preference for a dipolar source at location i). This speciﬁcation
for the prior involves the counterintuitive addition of an unknown hyperparameter for
every candidate source location which, on casual analysis may seem prone to severe
overﬁtting. As suggested previously however, s-MAP, γ-MAP, VB all possess an
intrinsic, sparsity-based regularization mechanism. This ameliorates the overﬁtting
problem substantially and effectively reduces the space of possible active source
locations by choosing a small relevant subset of active dipolar locations. In general,
the methodology is quite ﬂexible and other prior speciﬁcations can be included as
well, such as temporal and spectral constraints.
In summary, there are two senses with which to understand the notion of covari-
ance component selection. First, there is the selection of which components to include
in the model before any estimation takes place, i.e., the choice of C. Second, there is
the selection that occurs within C as a natural byproduct of many hyperparameters
being driven to zero during the learning process. Such components are necessarily
pruned by the model; those that remain have therefore been ‘selected’ in some sense.
Here we have argued that in many cases the later, data-driven selection can be used
to ease the burden of often ad hoc user-speciﬁed selections.
6.5 Discussion
The efﬁcacy of modern Bayesian techniques for quantifying uncertainty and explic-
itly accounting for prior assumptions make them attractive candidates for source
localization. However, it is not always transparent how these methods relate, nor
how they can be extended to handle more challenging problems, nor which ones
should be expected to perform best in various situations relevant to MEG/EEG source
imaging.APtarting from a hierarchical Bayesian model constructed using Gaussian
scale mixtures with ﬂexible covariance components, we analyze and, where pos-

6.5 Discussion
135
sible, extend three broad classes of Bayesian inference methods: γ-MAP, which
involves integrating out the unknown sources and optimizing the hyperparameters,
and s-MAP, which integrates out the hyperparameters and directly optimizes over
the sources, and variational approximation methods, which attempt to account for
uncertainty in all unknowns. Together, these three encompass a surprisingly wide
range of existing source reconstruction approaches, which makes general theoretical
analyses and algorithmic extensions/improvements pertaining to the particularly rel-
evant. Thus far, we have attempted to relate and extend three large classes of Bayesian
inverse methods, all of which turn out to be performing covariance component esti-
mation/pruning using different sparsity promoting regularization procedures. We
now provide some summary points related to connections to existing methods.
1. s-MAP, γ-MAP, and VB can be viewed as procedures for learning a source
covariance model using a set of predetermined symmetric, positive semi-deﬁnite
covariance components. The number of components in this set, each of which
acts as a constraint on the source space, can be extremely large, potentially much
larger than the number of sensors. However, a natural pruning mechanism effec-
tively discards components that are unsupported by the data. This occurs because
of an intrinsic sparsity preference in the Gaussian scale mixture model, which is
manifested in an explicit sparsityinducing regularization term. Consequently, it
is not crucial that the user/analyst manually determine an optimal set of compo-
nents a priori; many components can be included initially allowing the learning
process to remove superﬂuous ones.
2. The wide variety of Bayesian source localization methods that fall under this
framework can be differentiated by the following factors: (1) Selection of covari-
ance component regularization term; (2) Choice of initial covariance compo-
nent set C; (3) Optimization method/ Update rules; and (4) Approximation to
log p(y); this determines whether we are ultimately performing s-MAP, γ-MAP,
or VB.
3. Covariance component possibilities include geodesic neural basis functions for
estimating distributed sources [34], spatial smoothing factors [24], indicator
matrices to couple dipole components or learn ﬂexible orientations [36], fMRI-
based factors [31], and temporal and spectral constraints [21].
4. With large numbers of covariance components, s-MAP, γ-MAP, and VB provably
remove or prune a certain number of components which are not necessary for
representing the observed data.
5. In principle, the noise-plus-interference covariance can be jointly estimated as
well, competing with all the other components to model the data. However,
identiﬁability issues can be a concern here and so we consider it wiser to estimate
Σϵ via other means (e.g., using VBFA applied to prestimulus data as described
in Chap.5).
6. The latent structure inherent to the Gaussian scale-mixture model leads to an
efﬁcient, principled family of update rules for s-MAP, γ-MAP, and VB. This
facilitates the estimation of complex covariance structures modulated by very
large numbers of hyperparameters (e.g., 105+) with relatively little difﬁculty.

136
6
A Uniﬁed Bayesian Framework for MEG/EEG Source Imaging
7. Previous focal source imaging techniques such as FOCUSS and MCE display
undesirable discontinuities across time as well signiﬁcant biases in estimat-
ing dipole orientations. Consequently, various heuristics have been proposed
to address these deﬁciencies. However, the general spatiotemporal framework
of s-MAP, γ-MAP, and VB handles both of these concerns in a robust, principled
fashion by the nature of their underlying cost function. The standard weighted
minimum norm can be seen as a limiting case of γMAP.
8. As described in other chapters here, adaptive beamformers are spatial ﬁlters that
pass source signals in particular focused locations while suppressing interfer-
ence from elsewhere. The widely-used minimum variance adaptive beamformer
(MVAB) creates such ﬁlters using a sample covariance estimate; however, the
quality of this estimate deteriorates when the sources are correlated or the num-
ber of samples n is small. The simpler γ-MAP strategy can also be used to
enhance beamforming in a way that is particularly robust to source correla-
tions and limited data [15]. Speciﬁcally, the estimated γ-MAP data covariance
matrix ˆΣy = Σϵ + 
i ˆγi LCi LT can be used to replace the problematic sam-
ple covariance Cy = y yT . This substitution has the natural ability to remove
the undesirable effects of correlations or limited data. When n becomes large
and assuming uncorrelated sources, this method reduces to the exact MVAB.
Additionally, the method can potentially enhance a variety of traditional signal
processing methods that rely on robust sample covariance estimates.
9. It can be shown that sLORETA is equivalent to performing a single iteration
of a particular γ-MAP optimization procedure. Consequently, the latter can be
viewed as an iterative reﬁnement of sLORETA. This is exactly analogous to the
view of FOCUSS as an iterative reﬁnement of a weighted minimum ℓ2-norm
estimate.
10. γ-MAP and VB have theoretically zero localization bias estimating perfectly
uncorrelated dipoles given the appropriate hyperprior and initial set of covariance
component.
11. The role of the hyperprior p(γ) is heavily dependent on the estimation algorithm
being performed. In the s-MAP framework, the hyperprior functions through
its role in creating the concave regularization function gi(.). In practice, it is
much more transparent to formulate a model directly based on a desired gi(.) as
opposed to working with some supposedly plausible hyperprior p(γ) and then
inferring the what the associated gi(.) would be. In contrast, with γ-MAP and
VB the opposite is true. Choosing a model based on the desirability of some gi(.)
can lead to a model with an underlying hyperprior p(γ) that performs poorly.
Both VB and γ-MAP give rigorous bounds on the model evidence log p(y).
In summary, we hope that these ideas help to bring an insightful perspective to
Bayesian source imaging methods, reduce confusion about how different techniques
relate, expand the range of feasible applications of these methods. We have observed
a number of surprising similarities or out-right equivalences between what might
otherwise appear to be very different methodologies. Additionally, there are numer-
ous promising directions for future research, including time-frequency extensions,

6.5 Discussion
137
alternative covariance component parameterizations, and robust interference sup-
pression that inspire our own current body of work in this area, and we hope will
also inspire other researchers!
References
1. K. Friston, L. Harrison, J. Daunizeau, S. Kiebel, C. Phillips, N. Trujillo-Barreto, R. Henson,
G. Flandin, J. Mattout, Multiple sparse priors for the M/EEG inverse problem. NeuroImage
39(3), 1104–1120 (2008)
2. A. Bolstad, B.V. Veen, R. Nowak, Space-time event sparse penalization for magneto-
/electroencephalography. NeuroImage 46(4), 1066–1081 (2009)
3. I.F. Gorodnitsky, J.S. George, B.D. Rao, Neuromagnetic source imaging with FOCUSS: a
recursive weighted minimum norm algorithm. Electroencephalogr. Clin. Neurophysiol. 95,
231–251 (1995)
4. K. Uutela, M. Hämäläinen, E. Somersalo, Visualization of magnetoencephalographic data
using minimum current estimate. NeuroImage 10, 173–180 (1999)
5. M.-X. Huang, A.M. Dale, T. Song, E. Halgren, D.L. Harrington, I. Podgorny, J.M. Canive,
S. Lewis, R.R. Lee, Vector-based spatial-temporal minimum l1-norm solution for MEG. Neu-
roImage 31(3), 1025–1037 (2006)
6. R.D. Pascual-Marqui, Standardized low resolution brain electromagnetic tomography
(sLORETA): technical details. Methods Find. Exp. Clin. Pharmacol. 24, 5–12 (2002)
7. K.J. Friston, W. Penny, C. Phillips, S. Kiebel, G. Hinton, J. Ashburner, Classical and Bayesian
inference in neuroimaging: theory. NeuroImage 16(2), 465–483 (2002)
8. D.J.C. MacKay, Bayesian interpolation. Neural Comput. 4, 415–447 (1992)
9. R.R. Ramírez, S. Makeig, Neuroelectromagnetic source imaging using multiscale geodesic
neural bases and sparse Bayesian learning, in 12th Annual Meeting of the Organization for
Human Brain Mapping (Florence, Italy, 2006)
10. D.P. Wipf, J.P. Owen, H.T. Attias, K. Sekihara, S.S. Nagarajan, Robust Bayesian estimation
of the location, orientation, and time course of multiple correlated neural sources using MEG.
NeuroImage 49, 641–655 (2010)
11. J. Mattout, C. Phillips, W.D. Penny, M.D. Rugg, K.J. Friston, MEG source localization under
multiple constraints: an extended Bayesian framework. NeuroImage 30(3), 753–767 (2006)
12. M.-A. Sato, T. Yoshioka, S. Kajihara, K. Toyama, N. Goda, K. Doya, M. Kawato, Hierarchical
Bayesian estimation for MEG inverse problem. NeuroImage 23(3), 806–826 (2004)
13. D. Wipf, S. Nagarajan, A uniﬁed Bayesian framework for MEG/EEG source imaging. Neu-
roImage 44(3), 947–966 (2009)
14. D.P. Wipf, B.D. Rao, S. Nagarajan, Latent variable Bayesian models for promoting sparsity.
IEEE Trans. Inf. Theory 57(9), 6236–6255 (2011)
15. D. Wipf, S. Nagarajan, Beamforming using the relevance vector machine, in Proceedings of
the 24th International Conference on Machine Learning (ACM, 2007), pp. 1023–1030

Chapter 7
Source-Space Connectivity Analysis Using
Imaginary Coherence
7.1 Introduction
There has been tremendous interest in estimating the functional connectivity of
neuronal activities across different brain regions using electromagnetic brain imag-
ing.Functionalconnectivityanalysishastraditionallybeenimplementedinthesensor
space, but lately, a number of studies have begun to use source-space analysis, in
which voxel time courses are ﬁrst estimated by an inverse algorithm, and brain inter-
actions are then analyzed using those estimated voxel time courses [1–3]. Although a
certain degree of inaccuracy exists in the source estimation process, the source-space
analysis has the potential of providing more accurate information regarding which
brain regions are functionally coupled.
Source-space connectivity analysis computes a metric of brain interaction called
a connectivity metric, using the voxel time courses. Among existing connectivity
metrics, a representative metric is the coherence [2–5]. However, in the source-space
coherence analysis, a serious problem arises from spurious coherence caused by the
leakage of an inverse algorithm. Such leakages are more or less inevitable in all types
of inverse algorithms [6]. One representative ramiﬁcation of this spurious coherence
is an artifactual large peak around the seed voxel, called seed blur, in the resulting
coherence image. To remove such spurious coherence, the use of the imaginary
part of coherence, which is called the imaginary coherence, has been proven to
be effective [7]. It should be mentioned that the use of imaginary coherence was
originally proposed by Nolte et al. [8] to remove the spurious coherence caused by
the volume conduction in EEG sensor-space analysis.
This chapter reviews the source-space imaginary coherence analysis and related
methods. We ﬁrst provide a detailed theoretical analysis on how the use of imaginary
coherence leads to the removal of the spurious coherence caused by the algorithm
leakage. We then discuss several related methods of imaginary coherence, includ-
ing corrected imaginary coherence, canonical coherence, and envelope-to-envelope
correlation/coherence. We present numerical examples that conﬁrm our arguments.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_7
139

140
7
Source-Space Connectivity Analysis Using Imaginary Coherence
7.2 Source-Space Coherence Imaging
In the source-space analysis, the ﬁrst step estimates voxel time courses using an
inverse algorithm. Since the source is a three-dimensional vector with three (x, y,
and z) components,1 most source reconstruction algorithms produce component-
wise, multiple time courses at each voxel. Accordingly, we should compute a “rep-
resentative” single time course by projecting the multiple time courses onto the
direction of the source orientation.
However, in practical applications, the source orientation is generally unknown,
and it must be estimated from the data. One quick and easy way to estimate the source
orientation is to use the direction that maximizes the reconstructed voxel power. The
reconstructed source-time courses at the jth voxel is expressed as
S j =
⎡
⎣
sx(r j, t1) sx(r j, t2) · · · sx(r j, tK )
sy(r j, t1) sy(r j, t2) · · · sy(r j, tK )
sz(r j, t1) sz(r j, t2) · · · sz(r j, tK )
⎤
⎦,
(7.1)
where we assume that the data is collected at time points, t1, t2, . . . , tK , and r j
indicates the location of the jth voxel. Denoting the orientation of the source at the
jth voxel as η j, the estimate of η j,η j, is obtained using the following maximization:
η j = argmax
η j
ηT
j

S jST
j

η j.
The optimum estimate is obtained as the eigenvector corresponding to the maximum
eigenvalue of a matrix S jST
j , i.e.,
η j = ϑmax{S jST
j },
(7.2)
where the notation ϑmax{·}, deﬁned in Sect.C.9, indicates the eigenvector corre-
sponding to the maximum eigenvalue of a matrix between the parentheses. The
representative time course at the jth voxel, u j(t), is obtained using
	
u j(t1), u j(t2), . . . , u j(tK )

= ηT
j S j.
(7.3)
Once a representative voxel time course is obtained at each voxel, the next step
is to compute a voxel-pairwise coherence. This step involves ﬁrst setting a reference
point, called the seed point, and computing the coherence between the time course
from the seed point and that from another voxel’s location, referred to as the target
location. By scanning through all target locations in a brain, a three-dimensional
1 Although the source has two components when the homogeneous spherical conductor model
is used, for arguments in this chapter, we assume that the source vector has three x, y, and z
components.

7.2 Source-Space Coherence Imaging
141
mapping of source coherence—a source coherence image with respect to the seed
location—can be obtained. We can further scan not only the target location but also
the seed location to obtain a six-dimensional coherence image, called the voxel-to-
voxel coherence matrix.
Let us deﬁne the spectra of the seed and the target voxels as σS(f ) and σT (f ),
respectively. The coherence φ(f ) is obtained by computing the correlation of these
spectra,
φ(f ) =
⟨σT (f )σ∗
S(f )⟩

⟨|σT (f )|2⟩⟨|σS(f )|2⟩
,
(7.4)
where the superscript ∗indicates the complex conjugate, and the brackets ⟨· ⟩indicate
the ensemble average. In practical applications, this ensemble average is computed by
averaging across multiple trials. When only a single continuous data set is measured,
the single data set is divided into many trials and coherence is obtained by averaging
across these trials.
It is apparent in Eq.(7.4) that if the seed and target spectra contain common
components that do not result from true brain interactions, then the coherence may
contain spurious components. In source coherence imaging, the leakage of the imag-
ing algorithm is a major source of such spurious coherence.
7.3 Real and Imaginary Parts of Coherence
We here take a look at the nature of real and imaginary parts of coherence before
proceeding with the arguments of the leakage inﬂuence on the coherence imaging.
Let us deﬁne the time course from the seed voxel as uS(t) and the time course from
the target voxel as uT (t). The cross correlation of the seed and target time courses,
R(τ), is then deﬁned such that
R(τ) = ⟨uT (t + τ)uS(t)⟩=
∞

−∞
uT (t + τ)uS(t) dt.
(7.5)
The cross correlation is related to the cross spectrum density through
R(τ) =
∞

−∞
Ψ (f )e−i2πf τ df ,
(7.6)
where the cross spectrum is
Ψ (f ) = ⟨σT (f )σ∗
S(f )⟩.
(7.7)

142
7
Source-Space Connectivity Analysis Using Imaginary Coherence
According to Eq.(7.6), we have
R(0) =
∞

−∞
Ψ (f ) df =
∞

−∞
ℜ
	
Ψ (f )

df =
∞

−∞
ℜ
	
⟨σT (f )σ∗
S(f )⟩

df ,
(7.8)
where ℜ[·] indicates the real part of the complex number in the square brackets.
The right-hand side of the above equation holds because the real part of Ψ (f ) is an
even function and the imaginary part is an odd function. (This is because R(τ) is a
real-valued function.) Therefore, for a narrow-band signal,
R(0) =
∞

−∞
ℜ
	
⟨σT (f )σ∗
S(f )⟩

df ≈ℜ
	
⟨σT (f )σ∗
S(f )⟩

.
(7.9)
Here, R(0), which represents the zero-time-lag correlation, is related to the real part
of the cross spectrum, and thus, the real part of coherence represents the instantaneous
interaction.
Applying Parseval’s theorem to the Fourier transform relationship in Eq.(7.6),
we get
∞

−∞
R(τ)2 dτ =
∞

−∞
|Ψ (f )|2 df =
∞

−∞
|⟨σT (f )σ∗
S(f )⟩|2 df .
(7.10)
Thus, for a narrow-band signal, the following relationship holds:
∞

−∞
R(τ)2 dτ ≈|⟨σT (f )σ∗
S(f )⟩|2 = ℜ
	
⟨σT (f )σ∗
S(f )⟩

+ ℑ
	
⟨σT (f )σ∗
S(f )⟩

,
(7.11)
where ℑ[·] indicate the imaginary part of the complex number in the square brackets.
Using Eq.(7.9), we have
ℑ
	
⟨σT (f )σ∗
S(f )⟩

≈
∞

−∞
R(τ)2 dτ −R(0)2 =

τ̸=0
R(τ)2.
(7.12)
The imaginary part of the cross spectrum is equal to the sum of nonzero-lag corre-
lations. Therefore, we can see that the imaginary part of coherence represents the
non-instantaneous interaction.

7.4 Effects of the Leakage
143
7.4 Effects of the Leakage
7.4.1 Leakage Effects on the Magnitude Coherence
We analyze the effects of leakage of imaging algorithms in source coherence imaging.
When the leakage is taken into account, the estimated source time courses at the seed
and the target voxels, uS(t) and uS(t), can be expressed as
uS(t) = uS(t) + d1uT (t) + cS(t),
(7.13)
uT (t) = uT (t) + d2uS(t) + cT (t).
(7.14)
where uS(t) and uT (t) are the true source time courses of the seed and the target
locations. In the above equations, d1uT (t) indicates the leakage of the target signal
in the estimated seed signal, and d2uS(t) indicates the leakage of the seed signal in
the estimated target signal. The real-valued constants d1 and d2 express the relative
amount of these leakages.
In the equations above, cS(t) and cT (t) express the interference terms, which may
include the leakage from the third source, contributions from external disturbances,
and that of the sensor noise. The inﬂuence of these interference terms can be consid-
ered separately from the inﬂuence of the leakage terms [7]. In the arguments here,
ignoring cS(t) and cT (t), the estimated spectra at the seed and target voxels, σS(f )
and σT (f ), are expressed as
σS = σS + d1σT ,
(7.15)
σT = σT + d2σS.
(7.16)
where we omit the explicit notation of (f ) for simplicity.
The magnitude coherence between the seed and the target voxels is expressed as
|φ| =

⟨σTσ∗
S⟩

⟨|σT |2⟩⟨|σS|2⟩
 .
(7.17)
Using Eqs.(7.15) and (7.16), we have
⟨σTσ∗
S⟩= ⟨σT σ∗
S⟩+ d1⟨|σT |2⟩+ d2⟨|σS|2⟩+ d1d2⟨σSσ∗
T ⟩,
(7.18)
⟨|σT |2⟩= ⟨|σS|2⟩+ d2
1⟨|σT |2⟩+ 2d1ℜ

⟨σT σ∗
S⟩

,
(7.19)
⟨|σS|2⟩= ⟨|σT |2⟩+ d2
2⟨|σS|2⟩+ 2d2ℜ

⟨σT σ∗
S⟩

.
(7.20)
The equations above show that even when there is no true source interaction, i.e.,
even when ⟨σT σ∗
S⟩= 0 and ⟨σSσ∗
T ⟩= 0, |φ| has a nonzero value, which is equal to

144
7
Source-Space Connectivity Analysis Using Imaginary Coherence
|φ| =
|d1⟨|σT |2⟩+ d2⟨|σS|2⟩|

⟨|σS|2⟩+ d2
1⟨|σT |2⟩
 
⟨|σT |2⟩+ d2
2⟨|σS|2⟩
.
(7.21)
7.4.2 Leakage Effects on the Imaginary Coherence
We next analyze the effects of the algorithm leakage on the imaginary coherence.
Using Eq.(7.18) and the relationship
⟨σT σ∗
S⟩+ ⟨σSσ∗
T ⟩= 2ℜ

⟨σSσ∗
T ⟩

,
the cross spectrum ⟨σTσ∗
S⟩can be expressed as
⟨σTσ∗
S⟩= (1 −d1d2)⟨σT σ∗
S⟩+ d1⟨|σT |2⟩+ d2⟨|σS|2⟩+ 2d1d2ℜ

⟨σT σ∗
S⟩

.
(7.22)
By taking the imaginary part of Eq.(7.22), we can derive
ℑ

⟨σTσ∗
S⟩

= (1 −d1d2)ℑ

⟨σT σ∗
S⟩

.
(7.23)
We can then obtain the imaginary part of the estimated coherence ℑ(φ) as
ℑ(φ) =
ℑ

⟨σTσ∗
S⟩


⟨|σT |2⟩⟨|σS|2⟩
= (1 −d1d2)ℑ

⟨σT σ∗
S⟩


⟨|σT |2⟩⟨|σS|2⟩
= Λ ℑ(φ) ,
(7.24)
where ℑ(φ) indicates the true value of the imaginary coherence. Using Eqs.(7.19)
and (7.20), Λ is obtained as
Λ = (1 −d1d2)
√τ1τ2
(7.25)
where
τ1 = 1 + d2
1
⟨|σT |2⟩
⟨|σS|2⟩+ 2d1
ℜ

⟨σT σ∗
S⟩

⟨|σS|2⟩
,
(7.26)
and
τ2 = 1 + d2
2
⟨|σS|2⟩
⟨|σT |2⟩+ 2d2
ℜ

⟨σT σ∗
S⟩

⟨|σT |2⟩
.
(7.27)
Equation(7.24) shows that when ℑ(φ) = 0, we have ℑ(φ) = 0, indicating that no
spurious imaginary coherence has been caused. However, Eq.(7.24) also indicates
that the value of ℑ(φ) differs from the true value ℑ(φ), i.e., the intensity of the
estimatedimaginarycoherenceisbiasedandthebiasisrepresentedbyΛinEq.(7.25).

7.4 Effects of the Leakage
145
Let us calculate the intensity bias factor Λ assuming a simple scenario in which
d1 = d2 = d and ⟨|σS|2⟩= ⟨|σT |2⟩. Under these assumptions, the bias factor is
simpliﬁed to
1 + |d|
1 −|d| ≥Λ =
1 −d2
1 + d2 + 2ℜ(φ)d ≥1 −|d|
1 + |d|.
(7.28)
The equation above shows how the amount of leakage |d| affects the bias factor Λ. It
shows that if |d| is as small as |d| < 0.1, the intensity bias is less than 10%. However,
when |d| is as large as 0.4, we have 2.3 ≥Λ ≥0.4 and the intensity value of the imag-
inary coherence may have a very large bias. In the following, we introduce a metric
referred to as the corrected imaginary coherence, which avoids this intensity bias.
7.5 Corrected Imaginary Coherence
7.5.1 Modiﬁcation of Imaginary Coherence
The imaginary coherence can avoid spurious results caused by the algorithm leak-
age. However, its intensity value is affected by the algorithm leakage. In this section,
we introduce a modiﬁed form of the imaginary coherence whose intensity values are
unaffected by the algorithm leakage. Let us deﬁne the corrected imaginary coherence
as
ξ =
ℑ(φ)

1 −ℜ(φ)2 .
(7.29)
Let us express the estimate of ξ, ξ, such that
ξ =
ℑ(φ)

1 −ℜ(φ)2 =
ℑ

⟨σTσ∗
S⟩


⟨|σT |2⟩⟨|σS|2⟩−ℜ

⟨σTσ∗
S⟩
.
Then, considering
ℜ

⟨σTσ∗
S⟩

= d2
1⟨|σT |2⟩+ d2
2⟨|σS|2⟩+ (1 + d1d2)ℜ

⟨σT σ∗
S⟩

,
(7.30)
and using Eqs.(7.19) and (7.20), we derive
⟨|σT |2⟩⟨|σS|2⟩−ℜ

⟨σTσ∗
S⟩

= (1 −d1d2)2 
⟨|σT |2⟩⟨|σS|2⟩−ℜ

⟨σT σ∗
S⟩

.
(7.31)

146
7
Source-Space Connectivity Analysis Using Imaginary Coherence
Combining this with Eq.(7.23), we ﬁnally obtain
ξ =
ℑ(φ)

1 −ℜ(φ)2
=
ℑ

⟨σTσ∗
S⟩


⟨|σT |2⟩⟨|σS|2⟩−ℜ

⟨σTσ∗
S⟩

=
(1 −d1d2)ℑ

⟨σT σ∗
S⟩

(1 −d1d2)

⟨|σT |2⟩⟨|σS|2⟩−ℜ

⟨σT σ∗
S⟩

=
ℑ(φ)

1 −ℜ(φ)2
.
(7.32)
The above equation indicates that the corrected imaginary coherence computed using
voxel spectra is exactly equal to the true corrected imaginary coherence. This implies
that the corrected imaginary coherence is unaffected by the algorithm leakage.
In the above arguments, the corrected imaginary coherence is introduced some-
what in an ad hoc manner. In the following, we derive the corrected imaginary coher-
ence in two different manners: factorization of the mutual coherence and regression
of the target signal with the seed signal. These derivations provide some insights into
the nature of corrected imaginary coherence.
7.5.2 Factorization of Mutual Information
Hereweshowthatthecorrectedimaginarycoherenceisderivedfromthefactorization
of the mutual information into the instantaneous and non-instantaneous components.
We assume that σT and σS are Gaussian distributed, circular complex random vari-
ables. Concise explanations on the complex Gaussian random variable are found in
Sect.C.2 in the Appendix.
To derive the mutual information in the frequency domain, we deﬁne the entropy
of voxel spectra σT and σS. To do so, we ﬁrst deﬁne real-valued 2 × 1 vectors
ζT = [ℜ(σT ), ℑ(σT )]T and ζS = [ℜ(σS), ℑ(σS)]T . Using these vectors, the entropy
is deﬁned for σT and σS such that,
H(σT ) = −

p(ζT ) log p(ζT ) dζT ,
(7.33)
H(σS) = −

p(ζS) log p(ζS) dζS.
(7.34)
The entropy is a metric for uncertainty. H(σT ) represents the uncertainty when σT
is unknown, and H(σS) represents the uncertainty when σS is unknown. The joint
entropy is deﬁned as

7.5 Corrected Imaginary Coherence
147
H(σT ,σS) = −

p(ζT , ζS) log p(ζT , ζS)dζT dζS.
(7.35)
The mutual information between σT and σS is then deﬁned as
I(σT ,σS) = H(σT ) + H(σS) −H(σT ,σS).
(7.36)
When σT and σS are independent, we have I(σT ,σS) = 0.
Under the assumption that σT and σS are complex Gaussian distributed, the
entropy is expressed as
H(σT ) = log

|σT |2
,
(7.37)
H(σS) = log

|σS|2
.
(7.38)
The joint entropy is given by
H(σT ,σS) = log


σT
σS

φ
	
σ∗
T σ∗
S


= log


|σT |2 
σTσ∗
S


σSσ∗
T
 
|σS|2

= log

|σT |2 
|σS|2
−

σTσ∗
S
2
(7.39)
Therefore, the mutual information between σT and σS is obtained as
I(σT ,σS) = H(σT ) + H(σS) −H(σT ,σS)
= log

|σT |2 
|σS|2

|σT |2 
|σS|2
−

σTσ∗
S
2 .
(7.40)
It is easy to see that the mutual information I(σT ,σS) is related to the magnitude
coherence |φ| such that
I(σT ,σS) = −log(1 −|φ|2),
(7.41)
where
|φ|2 =

σTσ∗
S
2

|σT |2 
|σS|2.
(7.42)
Let us consider the factorization of the mutual information using the real and
imaginary parts of coherence. Using |φ|2 = ℜ(φ)2 + ℑ(φ)2, we have

148
7
Source-Space Connectivity Analysis Using Imaginary Coherence
I(σT ,σS) = −log(1 −|φ|2) = −log(1 −ℜ(φ)2 −ℑ(φ)2)
= −log

1 −ℜ(φ)2 
(1 −
ℑ(φ)2
1 −ℜ(φ)2

= −log(1 −ℜ(φ)2) −log

1 −
ℑ(φ)2
1 −ℜ(φ)2

.
(7.43)
On the right-hand side, the ﬁrst term,
IR(σT ,σS) = −log(1 −ℜ(φ)2)
represents a component of the mutual information corresponding to the real part
of coherence. This IR(σT ,σS) can be interpreted as the instantaneous component,
which corresponds to the zero-lag correlation between the target and the seed time
courses, as discussed in Sect.7.3.
The second term,
II(σT ,σS) = −log

(1 −
ℑ(φ)2
1 −ℜ(φ)2

represents a component of the mutual information corresponding to the imaginary
part of coherence. It is interpreted as the non-instantaneous component, which cor-
responds to the nonzero-lag correlation between uT (t) and uS(t). It is easy to see
II(σT ,σS) = −log

(1 −
ℑ(φ)2
1 −ℜ(φ)2

= −log(1 −ξ2),
(7.44)
where ξ is the corrected imaginary coherence. The equation above indicates that the
corrected imaginary coherence can be interpreted as a coherence-domain expression
of the non-instantaneous component of the mutual information. Arguments similar
to those in this section are found in [9].
7.5.3 Residual Coherence
We next show that the corrected imaginary coherence can be derived using regression
analysis. Let us regress the target spectrumσT using the seed spectrumσS, such that
σT = ασS + v,
(7.45)
where α is a real-valued constant, and v is a residual signal of this regression. The
value for α is determined using the least-squares ﬁt:

7.5 Corrected Imaginary Coherence
149
α = argmin
α
⟨|σT −ασS|2⟩,
(7.46)
and is derived as
α = ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩
.
(7.47)
Thus, the residual signal is expressed as
v = σT −ασS = σT −ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩
σS.
(7.48)
We then deﬁne the residual coherence, φR, such that
φR =
⟨vσ∗
S⟩

⟨|v|2⟩⟨|σS|2⟩
.
(7.49)
Let us show that this residual coherence is equal to the corrected imaginary coher-
ence. First, the cross spectrum between the residual and seed signals is expressed
as
⟨vσ∗
S⟩= ⟨σTσ∗
S⟩−ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩
⟨|σS|2⟩
= ⟨σTσ∗
S⟩−ℜ[⟨σTσ∗
S⟩]
= iℑ[⟨σTσ∗
S⟩].
(7.50)
The equation above shows that the cross spectrum between v and σS is equal to
the imaginary part of the cross spectrum between σT and σS. Using Eq.(7.48), we
express ⟨|v|2⟩such that
⟨|v|2⟩=

σT −ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩
σS
 
σT −ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩
σS
∗
= ⟨|σT |2⟩−ℜ[⟨σTσ∗
S⟩]
⟨|σS|2⟩

σSσ∗
T + σTσ∗
S

+ ℜ[⟨σTσ∗
S⟩]2
⟨|σS|2⟩2
⟨|σS|2⟩
= ⟨|σT |2⟩−ℜ[⟨σTσ∗
S⟩]2
⟨|σS|2⟩
.
(7.51)
Therefore, we get
⟨|v|2⟩⟨|σS|2⟩= ⟨|σT |2⟩⟨|σS|2⟩−ℜ[⟨σTσ∗
S⟩]2.
(7.52)

150
7
Source-Space Connectivity Analysis Using Imaginary Coherence
Substituting Eqs.(7.52) and (7.50) into Eq.(7.49), and neglecting the constant i,
the residual coherence is expressed as
φR =
ℑ[⟨σTσ∗
S⟩]

⟨|σT |2⟩⟨|σS|2⟩−ℜ[⟨σTσ∗
S⟩]2
=
ℑ(φ)

1 −ℜ(φ)2 = ξ.
(7.53)
The above equation shows that the residual coherence is exactly equal to the corrected
imaginary coherence ξ. That is, the corrected imaginary coherence is derived as the
coherence between the seed signal and the residual signal, which is obtained by
regressing out the seed signal from the target signal.
7.5.4 Phase Dependence of the Corrected
Imaginary Coherences
The coherence φ is complex valued, and it is expressed as the amplitude and phase,
such that
φ = |φ|eiθ,
(7.54)
where θ is the phase of the coherence, and the notation (f ) is omitted for simplicity.
When the coherence is caused by true brain interactions, we naturally interpret that
the amplitude |φ|, which is equal to the magnitude coherence, represents the strength
of the brain interactions.
The imaginary coherence is expressed as
ℑ(φ) = |φ| sin θ.
(7.55)
The value of the imaginary coherence depends on the phase θ. This is one weak point
of the imaginary coherence, because even if there is a strong brain interaction, the
imaginary coherence becomes very small when the phase θ has a value close to one
of multiples of π.
Let us see how the corrected imaginary coherence ξ depends on the phase. Using
Eq.(7.54), we have
ξ =
|φ| sin θ

1 −|φ|2 cos2 θ
.
(7.56)
The plots of ξ with respect to the phase are shown in Fig.7.1 with four values of the
magnitude coherence |φ|. This ﬁgure shows that the corrected imaginary coherence
becomesasymptoticallyindependentofthephasewiththelimitof|φ| →1.However,
the corrected imaginary coherence has almost the same phase dependence as the
imaginary coherence, when the magnitude coherence |φ| is less than 0.6. When |φ|
becomes closer to 1, the corrected imaginary coherence ξ becomes signiﬁcantly
greater than the imaginary coherence ℑ(φ).

7.6 Canonical Coherence
151
0
0.2
0.4
0.6
0.8
1
π/2
π
3π/2
2π
0
(c)
π/2
π
3π/2
2π
0
(d)
0
0.2
0.4
0.6
0.8
1
π/2
π
3π/2
2π
0
(a)
π/2
π
3π/2
2π
0
(b)
Fig. 7.1 Plots of the imaginary coherence and corrected imaginary coherence with respect to the
phase for four values of magnitude coherence. a |φ| = 0.99. b |φ| = 0.9. c |φ| = 0.8. d |φ| = 0.6.
The solid line shows the corrected imaginary coherence and the broken line shows the imaginary
coherence
7.6 Canonical Coherence
7.6.1 Canonical Magnitude Coherence
So far, we have discussed methods to compute voxel-based coherence. However,
when the number of voxels is large and our target is to analyze all-voxel-to-all-
voxel connections, the interpretation and visualization of analysis results may not
be easy because the number of voxel connections becomes huge. One way to reduce
this difﬁculty is to compute coherence between regions determined based on the
neurophysiology and/or neuroanatomy of a brain. However, in this case, since a brain
region contains multiple voxels, we must compute the coherence between groups of
multiple time courses. We here extend the theory of canonical correlation described
in Sect.C.3 in the Appendix to compute the canonical coherence, which should be
effective for region-based connectivity analysis. The explanation here follows those
in [10, 11].
In this section, random variables x and y are complex-valued column vectors in
the frequency domain, i.e.,

152
7
Source-Space Connectivity Analysis Using Imaginary Coherence
x(f ) =
⎡
⎢⎢⎢⎣
x1(f )
x2(f )
...
x p(f )
⎤
⎥⎥⎥⎦
and
y(f ) =
⎡
⎢⎢⎢⎣
y1(f )
y2(f )
...
yq(f )
⎤
⎥⎥⎥⎦.
(7.57)
In the following arguments, the notation (f ) is omitted for simplicity. Using the same
arguments as those for the canonical correlation in Sect.C.3, the random vectors x
and y are, respectively, projected in the directions of a and b where a and b are
complex-valued p × 1 and q × 1 column vectors. That is, deﬁning x = aH x and
y = bH y, the coherence between x and y, φ, is given by
φ =
⟨x y ∗⟩

⟨x x ∗⟩

⟨y y ∗⟩
=
aH⟨x yH⟩b

[aH⟨xxH⟩a][bH⟨y yH⟩b]
=
aHΣxyb

[aHΣxx a][bHΣ yyb]
,
(7.58)
where the superscript H indicates the Hermitian transpose. Here the cross-spectral
matrices are deﬁned as Σxy = ⟨x yH⟩, Σxx = ⟨xxH⟩, and Σ yy = ⟨y yH⟩. We
derive a formula to compute the canonical magnitude coherence. Using Eq.(7.58),
the magnitude coherence between x and y is expressed as
|φ|2 =
aHΣxyb
2
[aHΣxx a][bHΣ yyb]
.
(7.59)
The canonical squared magnitude coherence |ψ|2 is deﬁned as the maximum of |φ|2
(with respect to a and b), which is obtained by solving the optimization problem
|ψ|2 = max
a,b
aHΣxyb

2
subject to aHΣxx a = 1 and
bHΣ yyb = 1.
(7.60)
This constrained optimization problem can be solved in the following manner.
Since
aHΣxyb
2 is a scalar, we have the relationship,
aHΣxyb

2
=

aHΣxyb
H 
aHΣxyb

= bHΣ H
xyaaHΣxyb.
(7.61)
We ﬁrst ﬁx a, and solve the maximization problem with respect to b. The maximiza-
tion problem is
max
b
bHΣ H
xyaaHΣxyb subject to
bHΣ yyb = 1.
(7.62)

7.6 Canonical Coherence
153
Using the same derivation mentioned in Sect.C.9, the maximum value is equal to
the largest eigenvalue of the matrix
Σ−1
yy Σ H
xyaaHΣxy.
The matrix above is a rank-one matrix and it only has a single nonzero eigenvalue.
According to Sect.C.8 (Property No.10), this eigenvalue is equal to
aHΣxyΣ−1
yy Σ H
xya,
(7.63)
which is a scalar.
Thus, the optimization
|ψ|2 = max
a,b bHΣ H
xyaaHΣxyb
subject to aHΣxx a = 1 and
bHΣ yyb = 1,
(7.64)
is now rewriten as
|ψ|2 = max
a
aHΣxyΣ−1
yy Σ H
xya
subject to
aHΣxx a = 1.
(7.65)
Using again the derivation described in Sect.C.9, the solution of this maximization
is obtained as the maximum eigenvalue of the matrix
Σ−1
xx ΣxyΣ−1
yy Σ H
xy.
That is, denoting the eigenvalues of this matrix as γ j where j = 1, . . . , d and
d = min{p, q}, the canonical squared magnitude coherence is derived as
|ψ|2 = Smax{Σ−1
xx ΣxyΣ−1
yy Σ H
xy} = γ1,
(7.66)
where the notation Smax{·} indicates the maximum eigenvalue of a matrix between
the parentheses, as is deﬁned in Sect.C.9.
This canonical squared magnitude coherence is considered the best overall mag-
nitude coherence measure between the two sets of multiple spectra x1, . . . , x p and
y1, . . . , yq, and it is equal to the maximum eigenvalue γ1 in Eq.(7.66). However,
other eigenvalues may have information complementary to γ1, and therefore, a met-
ric that uses all the eigenvalues may be preferable. Let us assume the random vectors
x and y to be complex Gaussian. According to Eq.(C.52), we can then deﬁne the
mutual information between x and y such that
I(x, y) =
d

j=1
log
1
1 −γ j
,
(7.67)

154
7
Source-Space Connectivity Analysis Using Imaginary Coherence
which is a metric using all the eigenvalues. In the arguments in Sect.7.5.2, the
relationship between the coherence and the mutual information is given in Eq.(7.41).
This relationship can lead to an alternative deﬁnition of canonical magnitude coher-
ence based on the mutual information, such that
| 
ψ|2 = 1 −exp[−I(x, y)] = 1 −
d
!
j=1
(1 −γ j).
(7.68)
This metric uses all the eigenvalues, and since its value is normalized between 0 and
1, the interpretation is intuitively easy, compared to the original mutual information
I(x, y).
Also, the mutual information I(x, y) in Eq.(7.67) depends on d, which is the sizes
of the vectors x and y. Such property may not be appropriate for a brain interaction
metric, and the metric independent of the sizes of vectors can be deﬁned such that,
 I(x, y) = 1
d
d

j=1
log
1
1 −γ j
.
(7.69)
The alternative deﬁnition of canonical magnitude coherence in this case is expre-
ssed as
| ˘ψ|2 = 1 −exp[− I(x, y)] = 1 −
⎡
⎣
d
!
j=1
(1 −γ j)
⎤
⎦
1/d
.
(7.70)
The above metric may be more effective than the one in Eq.(7.68).
7.6.2 Canonical Imaginary Coherence
We next derive a formula to compute the canonical imaginary coherence. Following
the arguments of Evald et al. [12] we deﬁne
Σxx = ℜ(Σxx) + iℑ(Σxx) = Γ xx + iΥ xx,
(7.71)
Σ yy = ℜ(Σ yy) + iℑ(Σ yy) = Γ yy + iΥ yy,
(7.72)
Σxy = ℜ(Σxy) + iℑ(Σxy) = Γ xy + iΥ xy.
(7.73)
We use real-valued a and b and express the imaginary part of the coherence between
aT x and bT y:
ℑ(φ) =
aT ℑ

Σxy

b
	
aT Σxx a

 	
bT Σ yyb

 =
aT Υ xyb
	
aT Σxx a

 	
bT Σ yyb

.
(7.74)

7.6 Canonical Coherence
155
The canonical imaginary coherence ψI is deﬁned as the maximum of ℑ(φ), which
is obtained using the maximization,
ψI = max
a,b aT Υ xyb,
subject to
aT Σxx a = 1 and
bT Σ yyb = 1.
(7.75)
The formulation above is exactly the same as that for the canonical correlation in
Eq.(C.31). Thus, as in Sect.C.3.1, this maximization problem might seem to be
rewritten as the eigenvalue problem,

Σ−1
xx Υ xyΣ−1
yy Υ T
xy

a = ψ2
I a,
(7.76)
and ψ2
I is obtained as its maximum eigenvalue. However, in this eigenproblem, the
eigenvector a is generally complex-valued, conﬂicting with the assumption that the
vectors a and b are real-valued.
We instead use real-valued α and β obtained such that
α = Γ 1/2
xx a,
(7.77)
β = Γ 1/2
yy b.
(7.78)
Then, using α and β, we have
aT Σxx a = αT Γ −1/2
xx
ΣxxΓ −1/2
xx
α
= αT Γ −1/2
xx
(Γ xx + iΥ xx) Γ −1/2
xx
α
= ∥α∥2 + iαT Γ −1/2
xx
Υ xxΓ −1/2
xx
α = ∥α∥2
(7.79)
and
bT Σ yyb = βT Γ −1/2
yy
Σ yyΓ −1/2
yy
β
= βT Γ −1/2
yy

Γ yy + iΥ yy

Γ −1/2
yy
β
= ∥β∥2 + iβT Γ −1/2
yy
Υ yyΓ −1/2
yy
β = ∥β∥2.
(7.80)
In the equations above, we use the fact that the matrices Γ −1/2
xx
Υ xxΓ −1/2
xx
and
Γ −1/2
yy
Υ yyΓ −1/2
yy
are skew-symmetric.2
Using α and β, the optimization in Eq.(7.75) can be rewritten as
ψI = max
α,β αT Γ −1/2
xx
Υ xyΓ −1/2
yy
β
subject to αT α = 1 and βT β = 1. (7.81)
2 If AT = −A holds, matrix A is skew-symmetric. Since αT Aα is a scalar, if A is skew-symmetric,
αT Aα = 0.

156
7
Source-Space Connectivity Analysis Using Imaginary Coherence
Following the derivation in Sect.C.3.1, the solution of the above optimization
problem is expressed as the eigenvalue problem,

ΠxyΠT
xy

α = ψ2
I α,
(7.82)
where Πxy = Γ −1/2
xx
Υ xyΓ −1/2
yy
. In the above equation, since the matrix, ΠxyΠT
xy is
a real symmetric matrix, the eigenvector α is real-valued. The vector β is obtained
using
β = 1
ψI
Υ T
xyα,
which is also real-valued.
The matrices ΠxyΠT
xy and Γ −1
xx Υ xyΓ −1
yy Υ T
xy have the same eigenvalues, accord-
ing to Sect.C.8 (Property
No. 9). Let us deﬁne the eigenvalues of the matrix
Γ −1
xx Υ xyΓ −1
yy Υ T
xy as ζ j ( j = 1, . . . , d). The canonical imaginary coherence ψ2
I
is derived as
ψ2
I = Smax{Γ −1
xx Υ xyΓ −1
yy Υ T
xy} = ζ1.
(7.83)
Using the same arguments in the preceding section, the imaginary-coherence-based
mutual information is given by
Is(x, y) =
d

j=1
log
1
1 −ζ j
.
(7.84)
The alternative deﬁnition of canonical imaginary coherence, based on Is(x, y), is
obtained as
 
ψ2
I = 1 −exp[−Is(x, y)] = 1 −
d
!
j=1
(1 −ζ j),
(7.85)
which uses all the eigenvalues ζ1, . . . , ζd. When we can assume ζ j ≪1 (where
j = 1, . . . , d), we have
 
ψ2
I ≈
d

j=1
ζ j.
(7.86)
The connectivity metric above has been proposed and called the multivariate inter-
action measure (MIM) in [12].
The mutual information independent of the sizes of vectors is deﬁned such that,
 Is(x, y) = 1
d
d

j=1
log
1
1 −ζ j
.
(7.87)

7.6 Canonical Coherence
157
The alternative canonical imaginary coherence, based on  Is(x, y), is obtained as
˘ψ2
I = 1 −exp[−Is(x, y)] = 1 −
⎡
⎣
d
!
j=1
(1 −ζ j)
⎤
⎦
1/d
.
(7.88)
When we can assume ζ j ≪1 (where j = 1, . . . , d), we have
˘ψ2
I ≈1
d
d

j=1
ζ j.
(7.89)
The connectivity metric above is called the global interaction measure (GIM) [12].
7.6.3 Canonical Residual Coherence
We can compute the canonical residual coherence, which is a multivariate version
of the residual coherence described in Sect.7.5.3. Let us assume that y is the target
spectra, x is the seed spectra, and consider the regression
y = Ax + v.
(7.90)
The real-valued regression coefﬁcient matrix A is obtained by the least-squares ﬁt.
The optimum A is obtained as
A = ℜ

Σ yxΣ−1
xx

.
(7.91)
Thus, the residual signal v is expressed as
v = y −Ax = y −ℜ

Σ yxΣ−1
xx

x.
(7.92)
Let us deﬁne Σvv and Σvx such that Σvv = ⟨vvH⟩and Σvx = ⟨vxH⟩. They are
obtained as
Σvv = ⟨(y −Ax)(y −Ax)H⟩= Σ yy −AΣxy −Σ yxAT + AΣxxAT ,
(7.93)
and
Σvx = ⟨(y −Ax)xH⟩= Σ yx −AΣxx.
(7.94)
We just follow the arguments in Sect.7.6.1, and derive an expression for the
canonical magnitude coherence between v and x. Using complex-valued a and b,
we deﬁnev = aHv andx = bH x, the magnitude coherence betweenv andx, |φR|2,

158
7
Source-Space Connectivity Analysis Using Imaginary Coherence
is given by
|φR|2 =
|⟨vx ∗⟩|2

⟨vv ∗⟩

⟨x x ∗⟩
=
|aHΣvx b|2
[aHΣvva][bHΣxx b]
.
(7.95)
The canonical squared residual coherence |ψR|2 is deﬁned as the maximum of |φR|2,
which is obtained by solving the optimization problem,
|ψR|2 = max
a,b
aHΣvx b

2
, subject to aHΣvva = 1 and
bHΣxx b = 1. (7.96)
This maximization problem is exactly the same as that in Eq.(7.60), and the solu-
tion of this maximization is known to be the maximum eigenvalue of the matrix
Σ−1
vv ΣvxΣ−1
xx Σ H
vx. That is, the canonical residual coherence is derived as
|ψR|2 = Smax{Σ−1
vv ΣvxΣ−1
xx Σ H
vx},
(7.97)
where Σvv is derived using Eq.(7.93), and Σvx is derived using Eq.(7.94).
7.6.4 Computing Coherence When Each Voxel
has Multiple Time Courses
When the source vector has x, y, and z components, most source imaging algorithms
generate three time courses corresponding to the x, y, and z components at each
voxel. A usual way is to obtain a single representative time course at each voxel and
compute coherence using such representative time courses, as described in Sect.7.2.
Here, we describe an alternative method to compute voxel coherence when each
voxel has multiple time courses [13].
The method is a straightforward application of the canonical coherence. Let us
assume that the voxel time courses are expressed as in Eq.(7.1). The voxel spectra
at the target and seed voxels are respectively denoted using 3 × 1 vectors x and y,
which are given by
x(f ) =
⎡
⎣
x1(f )
x2(f )
x3(f )
⎤
⎦
and
y(f ) =
⎡
⎣
y1(f )
y2(f )
y3(f )
⎤
⎦,
(7.98)
where x1(f ), x2(f ), and x3(f ) are the spectra obtained from the source time courses
in the x, y and z directions at the target voxel, and y1(f ), y2(f ), and y3(f ) are
the spectra obtained from the source time courses in the x, y and z directions at
the seed voxel. Using these x and y, we compute canonical magnitude coherence
|ψ| in Eq.(7.66) or | 
ψ| in Eq.(7.68), and the canonical imaginary coherence ψI in
Eq.(7.83) or  
ψI in Eq.(7.85). This method is equivalent to determining the source

7.6 Canonical Coherence
159
orientations at the seed and target voxels by simultaneously maximizing the canonical
(magnitude/imaginary) coherence between these voxels.
7.7 Envelope Correlation and Related Connectivity Metrics
7.7.1 Envelope Correlation
Coherence is a metric that measures the phase relationship between two spectra, and
naturally it is sensitive to phase jitters. This property becomes problematic when we
try to estimate brain interactions at higher frequencies, such as gamma and high-
gamma activities. This is because, at high frequencies, a small time jitter could cause
a large phase jitter, and coherence may not be able to detect connectivity relation-
ships. That is, the coherence may not be appropriate as a connectivity metric at high
frequencies. The envelope-to-envelope correlation [14] is considered an appropriate
metric for such cases.
To compute the envelope correlation, we ﬁrst convert the seed and target time
courses into their analytic signals, such that
A[u(t)] = u(t) + i
π

u(t′)
t −t′ dt′.
(7.99)
On the left-hand side of the equation above, A[ · ] indicates an operator that creates
an analytic signal of the real-valued time signal in the parentheses. Let us deﬁne
analytic signals from the seed- and target-voxel time courses uS(t) and uT (t), such
that
A[uS(t)] = AS(t)eiθS(t),
(7.100)
A[uT (t)] = AT (t)eiθT (t),
(7.101)
where AS(t) and AT (t) are the amplitudes of the seed and target analytic signals,
and θS(t) and θT (t) are their instantaneous phases. The envelope correlation is the
correlation between the amplitudes, AS(t) and AT (t), and is computed such that
Θ =
"K
j=1 AT (t j)AS(t j)
#$"K
j=1 AT (t j)2
% $"K
j=1 AS(t j)2
%.
(7.102)
It is obvious in the above equation that common interferences such as the algorithm
leakage cause spurious correlation, and the seed blur should exist in an image of
envelope correlation.

160
7
Source-Space Connectivity Analysis Using Imaginary Coherence
7.7.2 Residual Envelope Correlation
Residual envelope correlation, which is free from the problem of seed blur, is pro-
posed in [15]. To compute the residual envelope correlation, we ﬁrst compute the
residual time course u R(t) using
u R(t) =
∞

−∞
v(f )ei2πf t df .
(7.103)
Here v(f ) is the residual spectrum obtained using Eq.(7.48). Since v(f ) is deﬁned
only for f ≥0, we create the residual spectrum for f < 0, such that
v(−f ) = v(f )∗,
where the superscript ∗indicates the complex conjugation. Since the Hermitian
symmetry holds for v(f ), u R(t) is guaranteed to be a real-valued time course. The
envelope of u R(t) is computed using
A[u R(t)] = AR(t)eiθR(t),
(7.104)
where AR(t) is the envelope of u R(t) called the residual envelope. Once AR(t) is
computed, the residual envelope correlation ΘR is computed using
ΘR =
"K
j=1 AR(t j)AS(t j)
#$"K
j=1 AR(t j)2
% $"K
j=1 AS(t j)2
%.
(7.105)
Since in the residual time course the seed signal is regressed out, the residual envelope
correlationisfreefromthespuriouscorrelationcausedbythealgorithmleakage.Note
that a method similar to the one mentioned here was reported in [16].
7.7.3 Envelope Coherence
The coherence can be computed between the envelope time courses AT (t) and AS(t).
Let us deﬁne the spectra obtained from the envelope AS(t) and AT (t) as ΣS(f ) and
ΣT (f ), respectively. The complex envelope coherence is computed using
φE(f ) =
⟨ΣT (f )Σ∗
S(f )⟩

⟨|ΣT (f )|2⟩⟨|ΣS(f )|2⟩
.
(7.106)

7.7 Envelope Correlation and Related Connectivity Metrics
161
The imaginary and corrected imaginary coherences are obtained as ℑ(φE(f )) and
ℑ(φE(f ))/

(1 −ℜ(φE(f ))2.
7.8 Statistical Thresholding of Coherence Images
In practical applications, we need to assess the statistical signiﬁcance of the obtained
coherence images. The surrogate data method [17, 18] has been used for this assess-
ment. In this method, the surrogate voxel spectra are created by multiplying random
phases with the original voxel spectra. The surrogate spectra for the seed and target
voxels,  σS(f ) and  σT (f ), are expressed as
 σS = σSei2πδS
and  σT = σT ei2πδT ,
(7.107)
where δS and δT are uniform random numbers between 0 and 1, and the explicit
notation of (f ) is again omitted for simplicity. Note that the surrogate spectra have
the same original power spectra but the phase relationship is destroyed by multiplying
the random phases to the original spectra.
Any coherence-based metric, such as the magnitude/imaginary coherence or cor-
rected imaginary coherence, can be computed using the surrogate spectra,  σS and
 σT . The metric computed using the surrogate spectra is denoted ω. As an example,
the imaginary coherence is computed using the surrogate spectra. It is expressed as3
ω =
|ℑ

⟨ σT σ∗
S⟩

|

⟨| σT |2⟩⟨| σS|2⟩
= |ℑ

⟨σTσ∗
Sei2πΔδ⟩

|

⟨|σT |2⟩⟨|σS|2⟩
,
(7.108)
where Δδ = δT −δS. The generation of ω is repeated B times, and a total of B
values of ω, denoted ω1, ω2, . . . , ωB, are obtained. These ω1, . . . , ωB can form an
empirical null distribution at each voxel.
We could derive a voxel-by-voxel statistical threshold using this empirical null
distribution. However, the statistical threshold derived in this manner does not take
the multiple comparisons into account and it generally leads to a situation in which
many false-positive voxels arise, i.e., many voxels that contain no brain interaction
are found to be interacting. To avoid this problem, the statistical signiﬁcance is
determined using a procedure that takes multiple comparisons into account. For this
purpose, we can use the maximal statistics4 [19, 20].
To utilize maximum statistics, the values ωℓ(ℓ= 1, . . . , B) are ﬁrst standardized
and converted into pseudo-t values, such that
3 The absolute value of the imaginary coherence is usually computed, because its sign has no
meaning in expressing the connectivity.
4 We can apply another method such as the false discovery rate to this multiple comparison problem.

162
7
Source-Space Connectivity Analysis Using Imaginary Coherence
T ℓ= ωℓ−⟨ω⟩
σω
,
(7.109)
where ⟨ω⟩and σ2
ω are the average and the variance of ωℓ(ℓ= 1, . . . , B). Since these
⟨ω⟩and σ2
ω are obtained at each voxel, the values at the jth voxel are denoted ⟨ω( j)⟩
and σ2
ω( j). The maximum value of T ℓobtained at the jth voxel is denoted T max( j).
Deﬁning a total number of voxels as NV , we have T max(1), . . . , T max(NV ) to form
a null distribution. We then sort these values in an increasing order:
T max(˜1) ≤T max(˜2) ≤· · · ≤T max( ˜NV ),
where T max(˜k) is the kth minimum value.
We set the level of the statistical signiﬁcance to α, and choose T max( ˜p) where
˜p = ⌈αNV ⌉and ⌈αNV ⌉indicates the maximum integer not greater than αNV . The
threshold value for the jth voxel, ωth( j), is ﬁnally derived as
ωth( j) = T max( ˜p)σω( j) + ⟨ω( j)⟩.
(7.110)
At the jth voxel, we evaluate the statistical signiﬁcance of the imaginary coherence
value by comparing it with ωth( j). When the metric value is greater than ωth( j), it
is considered to be statistically signiﬁcant; if not, it is considered to be statistically
insigniﬁcant.
7.9 Mean Imaginary Coherence (MIC) Mapping
Guggisberg et al. [3] have proposed to compute a metric called the mean imaginary
coherence. Deﬁning the coherence computed between the jth and kth voxels as
φ j,k(f ), the mean imaginary coherence for the jth voxel, M j(f ), is obtained using
M j(f ) = tanh
⎡
⎣1
NV
NV

k=1
tanh−1 
|ℑ(φ j,k(f )|

⎤
⎦.
(7.111)
On the right-hand side, the absolute value of the imaginary coherence |ℑ(φ j,k(f ))|
is averaged across all voxel connections. In Eq.(7.111),
z = tanh−1(r) = 1
2 log
&1 + r
1 −r
'
and r = tanh(z) = e2z −1
e2z + 1
are the inverse hyperbolic and hyperbolic functions, respectively. The idea of using
these functions is to average the voxel coherence values in the Fisher’s Z-transform
domain. We may use the corrected imaginary coherence instead of using the imagi-
nary coherence in Eq.(7.111).

7.9 Mean Imaginary Coherence (MIC) Mapping
163
The mean imaginary coherence M j(f ) is hypothesized to express the communi-
cation capability (healthiness) of the brain tissue at the jth voxel location. This
hypothesis has been tested by applying the mean imaginary coherence (MIC)
mappingtovarioustypesofclinicaldata,andresultspositivelyconﬁrmingthevalidity
of the hypothesis have been obtained. Such investigations include the MIC mapping
for stroke-recovery patients [21], and patients with traumatic brain lesions [22]. In
these applications, mapping of M j(f ) can predict the degree of a patient’s recovery
from stroke or brain injury. The MIC mapping has also been applied to MEG data
from patients with schizophrenia [23] and Alzheimer’s disease [24], and the values
of M j(f ) are found to be signiﬁcantly correlated with patient symptom scores at
particular brain areas.
7.10 Numerical Examples
Computer simulation was performed to illustrate results of our arguments in this
chapter. A sensor alignment of the 275-sensor array from the Omega™(VMS
Medtech, Coquitlam, Canada) neuromagnetometer was used. The coordinate sys-
tem and source-sensor conﬁguration used in the computer simulation are depicted
in Fig.5.1 in Chap.5. A vertical plane (x = 0cm) was assumed at the middle of the
whole-head sensor array, and three sources were assumed to exist on this plane.
Multiple-trial measurements were simulated, in which a total of 120 trials were
generated with each trial consisting of 600 time points. The data were assumed to
be collected with 2ms sampling. We ﬁrst conducted numerical experiments with
1
0
1
1
0
amplitude
1
0
1
1
0
amplitude
100
200
300
400
500
600
1
0
1
1
0
amplitude
time points
(a)
0
100
200
300
400
500
600
time points
(b)
Fig. 7.2 Examples of the time courses a for the alpha-band experiments and b for the gamma-band
experiments. The top, middle, and bottom panels, respectively, show the time courses for the ﬁrst,
second, and third sources

164
7
Source-Space Connectivity Analysis Using Imaginary Coherence
alpha-band activity. The time courses of the three sources for the ﬁrst trial are shown
in Fig.7.2a. The time courses had trial-to-trial time jitters. The time jitters for the
three time courses were generated using Gaussian random numbers with the same
standard deviation of 20 time points. These time courses were projected onto the
sensor space using the sensor lead ﬁeld to obtain the signal magnetic recordings. The
simulated sensor recordings were computed by adding spontaneous MEG data to the
signal magnetic recordings. The signal-to-interference ratio was set to 0.5.
The voxel time courses were reconstructed using the vector-type narrow-band
adaptive beamformer described in Chap.3. The data-covariance matrix was tuned to
the alpha frequency band. Reconstructed source power images on the plane x = 0cm
are shown in Fig.7.3a. The three sources are clearly resolved. Since the spherical
homogeneous conductor [25] was used for forward modeling, the vector adaptive
beamformer reconstructed two time courses corresponding to the two tangential
directions. We estimated a single representative time course at each voxel using
the method described in Sect.7.2, and computed voxel spectra. Coherence images
were then computed with respect to the seed voxel, which was set at the second
y (cm)
z (cm)
(a)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
(b)
6
4
2
0
2
4
6
y (cm)
z (cm)
(c)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
(d)
6
4
2
0
2
4
6
Fig. 7.3 Results of imaging the alpha-band voxel coherence on the plane x = 0cm. a Results of
source reconstruction. The asterisk shows the location of the seed voxel. b Magnitude coherence
image. c Imaginary coherence image. d Corrected imaginary coherence image

7.10 Numerical Examples
165
source location. The magnitude coherence image, imaginary coherence image, and
corrected imaginary coherence image are respectively shown in Fig.7.3b–d.
In the magnitude coherence image (Fig.7.3b), the seed blur dominates and
obscures the other sources. On the contrary, in the imaginary coherence image
(Fig.7.3c), and in the corrected imaginary coherence image (7.3d), the intensity
of the seed blur is much reduced and the two sources that interact with the second
source can clearly be observed. Also, the imaginary and the corrected imaginary
coherence images are very similar, because the magnitude coherence is as small as
0.26–0.35 in this computer simulation.
We next implemented the method described in Sect.7.6.4 in which a single rep-
resentative voxel time course is not computed, but instead, the canonical coher-
ence is directly computed by using multiple voxel time courses. The image of
canonical imaginary coherence ψI in Eq.(7.83) is shown in Fig.7.4a. The image
of mutual-information-based canonical imaginary coherence,  
ψI in Eq.(7.85) is
shown in Fig.7.4b. The canonical magnitude coherence |ψ| in Eq.(7.66) is shown
in Fig.7.4c. An image of mutual-information-based canonical magnitude coherence
y (cm)
z (cm)
(a)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
(b)
6
4
2
0
2
4
6
y (cm)
z (cm)
(c)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
(d)
6
4
2
0
2
4
6
Fig. 7.4 Results of imaging canonical coherence on the plane x = 0cm. a Image of canonical
imaginary coherence ψI in Eq.(7.83). The asterisk shows the location of the seed voxel. b Image of
mutual-information-based canonical imaginary coherence,  ψI in Eq.(7.85). c Image of canonical
magnitude coherence |ψ| in Eq.(7.66). d Image of mutual-information-based canonical magnitude
coherence | ψ| in Eq.(7.68)

166
7
Source-Space Connectivity Analysis Using Imaginary Coherence
| 
ψ| in Eq.(7.68) is shown in Fig.7.4d. In these results, imaginary-coherence-based
images ((a) and (b)), clearly detect the two interacting sources but the seed blur
dominates in the magnitude-coherence-based images ((c) and (d)).
We carried out the numerical experiment simulating gamma-band signals with
theta-band envelopes. The time courses of the three sources for the ﬁrst trial are
shown in Fig.7.2b. The time courses had trial-to-trial time jitters generated using
Gaussian random numbers with the same standard deviation of 20 time points.
The voxel time courses were estimated using the narrow-band adaptive beam-
former with a data-covariance tuned to the gamma frequency band, and the coherence
images were computed. The seed was set at the second source location. The coher-
ence images are shown in Fig.7.5a. Here, the magnitude, imaginary, and corrected
imaginary coherence images are, respectively, shown in the top, middle, and bottom
panels. In the magnitude coherence image, the seed blur dominates and only the seed
source is detected. However, since the time jitter created a large phase jitter for the
gamma-band signals, neither the imaginary nor the corrected imaginary coherence
image contain meaningful information on the source connectivity.
z (cm)
6
8
10
12
y (cm)
z (cm)
6
8
10
12
y (cm)
z (cm)
(a)
5
0
5
6
8
10
12
y (cm)
(b)
5
0
5
Fig. 7.5 Results of imaging the gamma-band source coherence on the plane x = 0cm. The seed
was set at the second source location. The coherence images were computed using the gamma-band
voxel time courses. The top, middle, and bottom panels in a, respectively, show the magnitude,
imaginary, and corrected imaginary coherence images. The top, middle, and bottom panels in b,
respectively, show the magnitude, imaginary, and corrected imaginary envelope coherence images

7.10 Numerical Examples
167
y (cm)
z (cm)
(a)
5
0
5
6
8
10
12
y (cm)
(b)
5
0
5
Fig. 7.6 Results of imaging envelope correlation on the plane x = 0cm. The seed was set at the
second source location. a Image of envelope correlation. b Image of residual envelope correlation
y (cm)
(a)
z (cm)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
z (cm)
(b)
6
4
2
0
2
4
6
6
7
8
9
10
11
12
13
14
y (cm)
(c)
6
4
2
0
2
4
6
Fig. 7.7 Results of statistical thresholding of the alpha-band source coherence images. a Magnitude
coherence image. b Imaginary coherence image. c Corrected imaginary coherence image. Statistical
thresholding using the surrogate data method described in Sect.7.8 was applied with the number of
surrogate data set to 200 and the level of the statistical signiﬁcance set to 0.99

168
7
Source-Space Connectivity Analysis Using Imaginary Coherence
We then computed the Hilbert envelopes of the reconstructed voxel time courses,
and computed the envelope-to-envelope coherence. The results of the envelope-to-
envelope coherence are shown in Fig.7.5b in which the top, middle, and bottom
panels show the magnitude, imaginary, and corrected imaginary coherence images,
respectively. The magnitude coherence image detects only the seed source. In the
imaginary coherence image, the interacting sources are not very clearly detected,
but in the corrected imaginary coherence image, the ﬁrst and the third sources are
detectedwithreasonableclarity.Intheseresults,thedifferencebetweentheimaginary
coherence and corrected imaginary coherence images is signiﬁcantly large because
the magnitude envelope coherence between the source time courses is as large as 0.7.
We computed the image of the envelope-to-envelope correlation, and the results
are shown in Fig.7.6. In this ﬁgure, the image of the envelope correlation is shown
in (a) and the image of the residual envelope correlation is in (b). In both images,
the ﬁrst and the third sources that were interacting with the second (seed) source can
be observed. While the original envelope correlation image contains seed blur, the
residual envelope correlation image is free from such spurious activity.
We ﬁnally show the results of statistical thresholding method described in
Sect.7.8. The method was applied to the alpha-band coherence imaging results in
Fig.7.3. The thresholded images are shown in Fig.7.7. The spurious baseline activity
existing in the unthresholded images in Fig.7.3 are removed, and it is much easier
to interpret the results in the thresholded images in Fig.7.7.
References
1. J.-M. Schoffelen, J. Gross, Source connectivity analysis with MEG and EEG. Hum. Brain
Mapp. 30, 1857–1865 (2009)
2. J. Gross, J. Kujara, M. Hämäläinen, L. Timmermann, A. Schnitzler, R. Salmelin, Dynamic
imaging of coherent sources: studying neural interactions in the human brain. Proc. Natl.
Acad. Sci. U.S.A. 98, 694–699 (2001)
3. A.G. Guggisberg, S.M. Honma, A.M. Findlay, S.S. Dalal, H.E. Kirsch, M.S. Berger, S.S.
Nagarajan, Mapping functional connectivity in patients with brain lesions. Ann. Neurol. 63,
193–203 (2007)
4. P. Belardinelli, L. Ciancetta, M. Staudt, V. Pizzella, A. Londei, N.B.G.L. Romani, C. Braun,
Cerebro-muscular and cerebro-cerebral coherence in patients with pre- and perinatally acquired
unilateral brain lesions. NeuroImage 37, 1301–1314 (2007)
5. W.H.R. Miltner, C. Braun, M. Arnold, H. Witte, E. Taub, Coherence of gamma-band EEG
activity as a basis for associative learning. Nature 397, 434–436 (1999)
6. K. Sekihara, S.S. Nagarajan, Adaptive Spatial Filters for Electromagnetic Brain Imaging
(Springer, Berlin, 2008)
7. K. Sekihara, J.P. Owen, S. Trisno, S.S. Nagarajan, Removal of spurious coherence in MEG
source-space coherence analysis. IEEE Trans. Biomed. Eng. 58, 3121–3129 (2011)
8. G. Nolte, O.B.L. Wheaton, Z. Mari, S. Vorbach, M. Hallett, Identifying true brain interaction
from EEG data using the imaginary part of coherency. Clin. Neurophysiol. 115, 2292–2307
(2004)
9. R.D. Pascual-Marqui, Instantaneous and lagged measurements of linear and nonlinear depen-
dence between groups of multivariate time series: frequency decomposition (2007). arXiv
preprint arXiv:0711.1455

References
169
10. E.J. Hannan, Multiple Time Series, vol. 38 (Wiley, New York, 2009)
11. D.R. Brillinger, Time Series: Data Analysis and Theory, vol. 36 (Siam, Philadelphia, 2001)
12. A. Ewald, L. Marzetti, F. Zappasodi, F.C. Meinecke, G. Nolte, Estimating true brain con-
nectivity from EEG/MEG data invariant to linear and static transformations in sensor space.
NeuroImage 60(1), 476–488 (2012)
13. F. Shahbazi Avarvand, A. Ewald, G. Nolte, Localizing true brain interactions from EEG and
MEG data with subspace methods and modiﬁed beamformers. Comput. Math. Methods Med.
2012, 402341 (2012)
14. A. Bruns, R. Eckhorn, Task-related coupling from high- to low-frequency signals among visual
cortical areas in human subdural recordings. Int. J. Psychophysiol. 51, 97–116 (2004)
15. K. Sekihara, S.S. Nagarajan, Residual coherence and residual envelope correlation in
MEG/EEG source-space connectivity analysis, in Conference of Proceedings of the IEEE
Engineering in Medicine and Biology Society, pp. 4417–7 (2013)
16. J.F. Hipp, D.J. Hawellek, M. Corbetta, M. Siegel, A.K. Engel, Large-scale cortical correlation
structure of spontaneous oscillatory activity. Nat. Neurosci. 15(6), 884–890 (2012)
17. J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J.D. Farmer, Testing for nonlinearity in time
series: the method of surrogate data. Phys. D 58, 77–94 (1992)
18. L. Faes, G.D. Pinna, A. Porta, R. Maestri, G. Nollo, Surrogate data analysis for assessing the
signiﬁcance of the coherence function. IEEE Trans. Biomed. Eng. 51, 1156–1166 (2004)
19. Y. Benjamini, Y. Hochberg, Controlling the false discovery rate: a practical and powerful
approach to multiple testing. J. R. Stat. Soc. B 57, 289–300 (1995)
20. T.E. Nichols, S. Hayasaka, Controlling the familywise error rate in functional neuroimaging:
Aj neural eng. comparative review. Stat. Methods Med. Res. 12, 419–446 (2003)
21. K.P. Westlake, L.B. Hinkley, M. Bucci, A.G. Guggisberg, A.M. Findlay, R.G. Henry, S.S.
Nagarajan, N. Byl, Resting state alpha-band functional connectivity and recovery after stroke.
Exp. Neurol. 237(1), 160–169 (2012)
22. P.E. Tarapore, A.M. Findlay, S.C. LaHue, H. Lee, S.M. Honma, D. Mizuiri, T.L. Luks, G.T.
Manley, S.S. Nagarajan, P. Mukherjee, Resting state magnetoencephalography functional con-
nectivity in traumatic brain injury: clinical article. J. Neurosurg. 118(6), 1306–1316 (2013)
23. L.B. Hinkley, J.P. Owen, M. Fisher, A.M. Findlay, S. Vinogradov, S.S. Nagarajan, Cognitive
impairments in schizophrenia as assessed through activation and connectivity measures of
magnetoencephalography (MEG) data. Front. Hum. Neurosci. 3, 73 (2009)
24. K.G. Ranasinghe, L.B. Hinkley, A.J. Beagle, D. Mizuiri, A.F. Dowling, S.M. Honma, M.M.
Finucane, C. Scherling, B.L. Miller, S.S. Nagarajan et al., Regional functional connectivity
predicts distinct cognitive impairments in Alzheimers disease spectrum. NeuroImage: Clin. 5,
385–395 (2014)
25. J. Sarvas, Basic mathematical and electromagnetic concepts of the biomagnetic inverse prob-
lem. Phys. Med. Biol. 32, 11–22 (1987)

Chapter 8
Estimation of Causal Networks:
Source-Space Causality Analysis
8.1 Introduction
This chapter reviews the methodology for estimating causal relationships among
cortical activities in MEG/EEG source space analysis. The source space causality
analysiscomputessometypesofcausalitymeasuresusingtheestimatedtimeseriesof
the source activities of interest. Commonly-used measures are the Granger causality
related measures, which are based on the MVAR modeling of the voxel time series.
We ﬁrst describe these Granger causality based measures, including the original def-
inition in the temporal domain [1, 2], Geweke’s extension to the spectral domain
[3, 4], and related measures such as the directed transfer function (DTF) [5] and the
partial directed coherence (PDC) [6]. The transfer entropy [7], which is a representa-
tive non-parametric measure, is also discussed, with a proof of its equivalence to the
Granger causality under the Gaussianity assumption. The last section of this chapter
describes methods of estimating the MVAR coefﬁcients. Here we present a con-
ventional least-square-based algorithm and a method based on the sparse Bayesian
inference for this MVAR estimation problem. The sparse Bayesian algorithm out-
performs the conventional method when the sensor data contains a large amount of
interferences.
8.2 Multivariate Vector Autoregressive (MVAR) Process
8.2.1 MVAR Modeling of Time Series
The Granger-causality measures make use of multivariate vector auto regressive
(MVAR) process of the source time series. We ﬁrst explain the modeling of time
series using the MVAR process. A general approach to the source-space causality
analysis ﬁrst chooses a relatively small number of voxels corresponding to the source
activities of interest, and the causal relationships among time series of the selected
voxels are estimated.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_8
171

172
8
Estimation of Causal Networks: Source-Space Causality Analysis
Denoting the number of selected voxels q, we express the q-channel voxel time
series using the vector y(t): y(t) = [y1(t), . . . , yq(t)]T where y j(t) is the time series
of the jth selected voxel at time t. Here the time t is expressed using a unit-less value.
We impose multivariate vector autoregressive (MVAR) modeling on the time series
y(t), such that
y(t) =
P

p=1
A(p)y(t −p) + e(t).
(8.1)
Here, A(p) is the AR coefﬁcient matrix, P is the model order, and e(t) is the residual
vector. The MVAR process is expressed in the frequency domain. By computing the
Fourier transform of Eq.(8.1), we get
y( f ) =
P

p=1
A(p)e−2πipf y( f ) + e( f ),
(8.2)
where the Fourier transforms of y(t) and e(t) are expressed in y( f ) and e( f ). We
here use the relationship,

y(t −p) exp(−2πi f t) dt = e−2πipf y( f ).
(8.3)
Equation (8.2) is also expressed as
⎡
⎣I −
P

p=1
A(p)e−2πipf
⎤
⎦y( f ) = e( f ).
(8.4)
Deﬁning a q × q matrix ¯A( f ) such that
¯A( f ) = I −
P

p=1
A(p)e−2πipf ,
(8.5)
we can obtain
¯A( f )y( f ) = e( f ).
(8.6)
Also, deﬁning H( f ) = ¯A( f )−1, the relationship
y( f ) = H( f )e( f )
(8.7)
can be obtained. According to the equation above, e( f ) and y( f ) can be interpreted
as the input and the output of a linear-system whose transfer function is H( f ).

8.2 Multivariate Vector Autoregressive (MVAR) Process
173
8.2.2 Coherence and Partial Coherence of the MVAR Process
Using Eq.(8.7), we can derive the relationship
S( f ) = H( f )Σ H H( f ),
(8.8)
where the superscript H indicates the Hermitian transpose.1 In the equation above,
S( f ) is the cross spectrum matrix, which is given by S( f ) = ⟨y( f )yH( f )⟩where
⟨· ⟩indicates the ensemble average. Also, Σ is the covariance matrix of the residual,
which is equal to Σ = ⟨e( f )eH( f )⟩. Using these deﬁnitions, the coherence between
the time series of the jth and the kth channels is expressed as
φ j,k( f ) =
Sj,k
Sj, j Sk,k
=
hH
j Σhk
	
[hH
j Σh j][hH
k Σhk]
,
(8.9)
where h j is the jth column of H H such that H H = [h1, . . . , hq].
The partial coherence between the jth and the kth channels, κ j,k, is expressed as
[8, 9]
κ j,k( f ) =
M j,k
M j, j Mk,k
,
(8.10)
where M j,k is the minor of the matrix S( f ); the minor is a determinant value of a
matrix formed by the jth row and the kth column removed from S( f ). The partial
coherence κ j,k( f ) is a measure of the coherence between the time series of the jth
and the kth channels where the inﬂuence of other channels is removed. In other
words, it expresses the direct interaction between these two channels.
Let us derive a convenient formula for computing the partial coherence. We use
the relationship called Cramer’s rule [10]:

S−1( f )

j,k = (−1) j+k Mk, j
|S( f )|
,
where [S−1( f )] j,k indicates the ( j, k) component of the matrix S−1( f ). Considering
the fact that S( f ) is a positive semideﬁnite Hermitian matrix, the equation above
can be changed to
M j,k = (−1) j+k 
S−1( f )

j,k |S( f )|.
(8.11)
1 The Hermitian transpose is the matrix transpose with the complex conjugation.

174
8
Estimation of Causal Networks: Source-Space Causality Analysis
We then rewrite Eq.(8.10) into
κ j,k( f ) =
(−1) j+k 
S−1( f )

j,k |S( f )|
	
(−1)2 j 
S−1( f )

j, j |S( f )|(−1)2k 
S−1( f )

k,k |S( f )|
=

S−1( f )

j,k
	
S−1( f )

j, j

S−1( f )

k,k
.
(8.12)
On the other hand, using Eq.(8.8), we have the relationship
S−1( f ) =

H( f )Σ H H( f )
−1
=

H−1( f )
H
Σ−1H−1( f ) = ¯AH( f )Σ−1 ¯A( f ).
(8.13)
The kth column vector of ¯A( f ) is denoted ¯ak, i.e., ¯A( f ) =

¯a1, . . . , ¯aq

. We can
then obtain

S−1( f )

j,k = ¯aH
j Σ−1 ¯ak,
and thus derive
κ j,k( f ) =
¯aH
j Σ−1 ¯ak
	
[¯aH
j Σ−1 ¯a j][¯aH
k Σ−1 ¯ak]
.
(8.14)
The equation above is used for deriving partial directed coherence in Sect.8.5.3.
8.3 Time-Domain Granger Causality
8.3.1 Granger Causality for a Bivariate Process
Let us ﬁrst consider bivariate cases in which only a pair of two time series y1(t) and
y2(t) are considered. In the ﬁrst case, y1(t) and y2(t) are determined by using only
their own past values, i.e., the following relationships hold:
y1(t) =
∞

p=1
A1,1(p)y1(t −p) + e1(t),
(8.15)
y2(t) =
∞

p=1
A2,2(p)y2(t −p) + e2(t).
(8.16)

8.3 Time-Domain Granger Causality
175
We then consider the second case in which the following relationships hold:
y1(t) =
∞

p=1
A1,1(p)y1(t −p) +
∞

p=1
A1,2(p)y2(t −p) + ϵ1(t),
(8.17)
y2(t) =
∞

p=1
A2,1(p)y1(t −p) +
∞

p=1
A2,2(p)y2(t −p) + ϵ2(t).
(8.18)
The difference between the ﬁrst and the second cases above is that in the second
case, time series y1(t) and y2(t) are determined by the past values of both the time
series of y1(t) and y2(t), while in the ﬁrst case, the time series y1(t) is determined
only by the past values of y1(t) and the time series y2(t) is determined only by the
past values of y2(t).
We deﬁne the variances of the residual terms appearing above, such that
V (e1(t)) = Σ1
e ,
(8.19)
V (e2(t)) = Σ2
e ,
(8.20)
V (ϵ1(t)) = Σ1
ϵ ,
(8.21)
V (ϵ2(t)) = Σ2
ϵ ,
(8.22)
where V ( · ) indicates the variance. Then, the Granger causality, G2→1, is deﬁned as
G2→1 = log
Σ1
e
Σ1ϵ

.
(8.23)
The meaning of G2→1 is that, if the residual variance is reduced by using both the
past values of y1(t) and y2(t) when estimating the current value of y1(t), the past of
y2(t) affects the current value of y1(t). In such a case, we can conclude that there
is an information ﬂow from the time series y2(t) to the time series y1(t). The above
G2→1 can quantify the amount of this information ﬂow. Similarly, we can deﬁne
G1→2 such that
G1→2 = log
Σ2
e
Σ2ϵ

.
(8.24)
This G1→2 expresses the information ﬂow from the time series y1(t) to the time series
y2(t).
8.3.2 Multivariate Granger Causality
The idea of Granger causality is extended to a general multivariate case. Let us deﬁne
q-dimensional time series as y(t) and r-dimensional time series as x(t). We consider

176
8
Estimation of Causal Networks: Source-Space Causality Analysis
a case in which x(t) and y(t) obey the MVAR process such that
x(t) =
P

p=1
Ax(p)x(t −p) + ex(t),
(8.25)
and
y(t) =
P

p=1
Ay(p)y(t −p) + ey(t).
(8.26)
We deﬁne covariance matrices of the residuals in this case, such that
Σx
e = ⟨ex(t)eT
x (t)⟩,
(8.27)
Σ y
e = ⟨ey(t)eT
y (t)⟩,
(8.28)
where ⟨· ⟩indicates the ensemble average.
We next assume that x(t) and y(t) obey the following MVAR process
x(t) =
P

p=1
Ax(p)x(t −p) +
P

p=1
By(p)y(t −p) + ϵx(t),
(8.29)
and
y(t) =
P

p=1
Ay(p)y(t −p) +
P

p=1
Bx(p)x(t −p) + ϵy(t).
(8.30)
We can deﬁne covariance matrices of the residuals, such that
Σx
ϵ = ⟨ϵx(t)ϵT
x (t)⟩,
(8.31)
Σ y
ϵ = ⟨ϵy(t)ϵT
y (t)⟩.
(8.32)
Using the same idea for Eqs.(8.23) and (8.24), the multivariate Granger causality
Gx→y and Gy→x are given by
Gx→y = log |Σ y
e|
|Σ y
ϵ |,
(8.33)
Gy→x = log Σx
e
|Σx
ϵ |,
(8.34)
where | · | indicates the matrix determinant.

8.3 Time-Domain Granger Causality
177
8.3.3 Total Interdependence
Let us deﬁne the (q +r)-dimensional augmented time series z(t) = [xT (t), yT (t)]T ,
which obeys
z(t) =
P

p=1
Az(p)z(t −p) + ϵz(t).
(8.35)
The covariance matrix of the residual vector ϵz(t) is deﬁned as Σz
ϵ. When the time
series x(t) and y(t) are independent, ϵz(t) is expressed as ϵz(t) = [ex(t), ey(t)]T ,
and Σz
ϵ is expressed as
Σz
ϵ = ⟨ϵz(t)ϵT
z (t)⟩=
 Σx
e
0
0 Σ y
e

.
(8.36)
When x(t) and y(t) are not independent, it is expressed as
Σz
ϵ =
 Σx
ϵ Σxy
ϵ
Σ yx
ϵ
Σ y
ϵ

,
(8.37)
where
Σxy
ϵ
= ⟨ϵx(t)ϵT
y (t)⟩,
(8.38)
Σ yx
ϵ
= ⟨ϵy(t)ϵT
x (t)⟩= (Σxy
ϵ )T .
(8.39)
The total interdependence between x and y, I{x,y}, is deﬁned such that
I{x,y} = log

Σx
e
0
0 Σ y
e

Σz
ϵ

= log
Σx
e
 Σ y
e

Σz
ϵ

.
(8.40)
This total interdependence expresses the deviation of the value of
Σz
ϵ
 from its value
obtained when x(t) and y(t) are independent. Using Eqs.(8.33) and (8.34), it is easy
to show the relationship
I{x,y} −Gx→y −Gy→x = log
Σx
ϵ
 Σ y
ϵ

Σz
ϵ

.
(8.41)
According to Geweke [3], the right-hand side of the equation above is deﬁned as
I{x·y}:
I{x·y} = log
Σx
ϵ
 Σ y
ϵ

Σz
ϵ

,
(8.42)

178
8
Estimation of Causal Networks: Source-Space Causality Analysis
and this I{x·y} is called the instantaneous dependence. The following relationship
holds among the causality measures mentioned above,
I{x,y} = Gx→y + Gy→x + I{x·y}.
That is, the total interdependence between the two time series x(t) and y(t) is
expressed as the summation of the Granger causality from x(t) to y(t), the Granger
causality from y(t) to x(t), and the instantaneous dependence.
The rationale of this instantaneous dependence can be explained in the following
manner. Using the determinant identity in Eq.(C.94), Eq.(8.42) is rewritten as
I{x·y} = log
Σx
ϵ
 −log |Δ| ,
(8.43)
where
Δ = Σx
ϵ −Σxy
ϵ (Σ y
ϵ )−1Σ yx
ϵ .
(8.44)
Let us consider the linear regression in which the residual signal ϵx(t) is regressed
by the other residual signal ϵy(t). In Eq.(8.44), Δ expresses the covariance matrix
of the residual of this linear regression, according to the arguments in Sect.C.3.3.
Let us suppose a case where x(t) and y(t) contain a common instantaneous
interaction ν(t), which does not exist in the past values of x(t) and y(t). In such a
case, both the residual signals ϵx(t) and ϵy(t) contain the common component ν(t),
and because of this, ν(t) is regressed out when ϵx(t) is regressed by ϵy(t). Therefore,
log |Δ| does not contain the inﬂuence of this common instantaneous component,
while log
Σx
ϵ
 does. Accordingly, I{x·y} represents the inﬂuence of the instantaneous
component alone.
8.4 Spectral Granger Causality: Geweke Measures
8.4.1 Basic Relationships in the Frequency Domain
Since brain activities have spectral dependence, we naturally like to perform causality
analysis in the spectral domain. The extension of Granger causality analysis into the
spectraldomainhasbeeninvestigatedbyGeweke[3, 4].Theargumentsinthissection
are according to Ding [11], and we restrict our arguments to a bivariate process2:
 x(t)
y(t)

=
∞

p=1
A(p)
 x(t −p)
y(t −p)

+
ex(t)
ey(t)

,
(8.45)
2 The extension of the arguments here to a general multivariate case remains unknown.

8.4 Spectral Granger Causality: Geweke Measures
179
where A(p) is a (2×2) coefﬁcient matrix. We can derive Eq.(8.7), which is explicitly
written as
 x( f )
y( f )

=
 Hxx Hxy
Hyx Hyy
  ex( f )
ey( f )

.
(8.46)
In the expressions above, the explicit notation of the frequency f is omitted from
the components of H( f ). Denoting the residual covariance matrix such that

⟨|ex( f )|2⟩
⟨ex( f )ey( f )∗⟩
⟨ey( f )ex( f )∗⟩
⟨|ey( f )|2⟩

=
Σx γ
γ∗Σy

,
(8.47)
wherethesuperscript∗indicatesthecomplexconjugation.Equation (8.8)isexplicitly
written as

⟨|x( f )|2⟩
⟨x( f )y( f )∗⟩
⟨y( f )x( f )∗⟩
⟨|y( f )|2⟩

= H
Σx γ
γ∗Σy

H H.
(8.48)
8.4.2 Total Interdependence and Coherence
We ﬁrst deﬁne the total interdependence between x and y, which corresponds to
I{x,y} in Eq.(8.40). Using the arguments in Sect.8.3.3, the total interdependence
between x and y in the spectral domain is deﬁned as
f{x,y} = log

⟨|x( f )|2⟩
0
0
⟨|y( f )|2⟩

|S( f )|
= log
⟨|x( f )|2⟩⟨|y( f )|2⟩
⟨|x( f )|2⟩⟨|y( f )|2⟩−|⟨x( f )y( f )∗|2 .
(8.49)
Comparing the equation above to Eq.(7.40) in Chap.7, it can immediately be seen
that under the Gaussianity assumption, the spectral domain total interdependence is
equal to the mutual information discussed in Sect.7.5.2. Therefore, using the same
arguments, the total interdependence f{x,y} is related to the magnitude coherence
|φ( f )| through
f{x,y}( f ) = −log(1 −|φ( f )|2),
(8.50)
where
|φ( f )|2 =
|⟨x( f )y( f )∗⟩|2
⟨|x( f )|2⟩⟨|y( f )|2⟩.
(8.51)

180
8
Estimation of Causal Networks: Source-Space Causality Analysis
8.4.3 Deriving Causal Relationships in the Frequency Domain
In order to derive causal relationships between two channels, we should distinguish
between the amount of the signal power coming from its own past and the amount
of the power coming from the past of the other channel. On the basis of Eq.(8.46),
we are able to do this separation. That is, using Eq.(8.46), we have
x( f ) = Hxxex( f ) + Hxyey( f ).
(8.52)
On the right-hand side of the equation above, the ﬁrst term is the intrinsic term
representing the inﬂuence of the past of x(t), and the second term represents the
inﬂuence of the past of y(t). We then decompose the power of the signal x( f ), such
that
⟨|x( f )|2⟩= HxxΣx H∗
xx + HxyΣy H∗
xy + 2ℜ

γHxx H∗
xy

.
(8.53)
Ontheright-handsideabove,theﬁrsttermrepresentstheintrinsictermandthesecond
term expresses the inﬂuence from the past of y( f ), i.e., the causal term. However,
it is unclear how much amount of the information represented by the third term is
attributed to the intrinsic or the causal terms. Therefore, to attain the intrinsic/causal
factorization, we ﬁrst transform Eq.(8.46) into a domain where the innovation terms
are uncorrelated, and, in this domain, we factorize the total signal power into the
intrinsic and causal terms.
Let us factorize ⟨|x( f )|2⟩in this manner. The transformation is performed in this
case using Π deﬁned such that
Π =

1
0
−γ∗/Σx 1

.
(8.54)
Using Eq.(8.46) and
Π−1 =

1
0
γ∗/Σx 1

,
we have
 x( f )
y( f )

=
 Hxx Hxy
Hyx Hyy
 
1
0
γ∗/Σx 1
 
1
0
−γ∗/Σx 1
 ex( f )
ey( f )

.
(8.55)
We then have
 x( f )
y( f )

=
⎡
⎣Hxx + γ∗
Σx Hxy Hxy
Hyx + γ∗
Σx Hyy Hyy
⎤
⎦

ex( f )
−γ∗
Σx ex( f ) + ey( f )

.
(8.56)

8.4 Spectral Granger Causality: Geweke Measures
181
The cross correlation of the two innovation components, ex( f ) and −γ∗
Σx ex( f ) +
ey( f ) is zero, because

−γ∗
Σx
ex( f ) + ey( f )

ex( f )∗

= −γ∗
Σx
⟨|ex( f )|2⟩+ ⟨ey( f )ex( f )∗⟩= −γ∗
Σx
Σx + γ∗= 0
(8.57)
We can thus derive

⟨|x( f )|2⟩
⟨x( f )y( f )∗⟩
⟨y( f )x( f )∗⟩
⟨|y( f )|2⟩

=
⎡
⎣Hxx + γ∗
Σx Hxy Hxy
Hyx + γ∗
Σx Hyy Hyy
⎤
⎦

Σx
0
0 Σy −|γ|2
Σx
 
H∗
xx +
γ
Σx H∗
xy H∗
yx +
γ
Σx H∗
yy
H∗
xy
H∗
yy

.
(8.58)
Therefore, the signal power ⟨|x( f )|2⟩is factorized to
⟨|x( f )|2⟩=

Hxx + γ∗
Σx
Hxy

Σx

H∗
xx + γ
Σx
H∗
xy

+ Hxy

Σy −|γ|2
Σx

H∗
xy.
(8.59)
On the right-hand side of this equation, the ﬁrst term is interpreted as the intrin-
sic term, which represents the inﬂuence of the past of x(t) on the power spectrum
⟨|x( f )|2⟩. The second term is interpreted as the causal inﬂuence term, which rep-
resents the inﬂuence of the past of y(t) on ⟨|x( f )|2⟩. Thus, the spectral Granger
causality, fy→x( f ), is deﬁned as
fy→x( f ) = log
⟨|x( f )|2⟩
(Hxx + γ∗
Σx Hxy)Σx(H∗xx +
γ
Σx H∗xy)
= −log
⟨|x( f )|2⟩−Hxy(Σy −|γ|2
Σx )H∗
xy
⟨|x( f )|2⟩
= −log
⎡
⎣1 −
(Σy −|γ|2
Σx )|Hxy( f )|2
⟨|x( f )|2⟩
⎤
⎦.
(8.60)
The spectral Granger causality, fx→y( f ), can be derived in exactly the same
manner, using
Π =
 1 −γ∗/Σy
0
1

,

182
8
Estimation of Causal Networks: Source-Space Causality Analysis
and the results are given by
fx→y( f ) = −log
⎡
⎣1 −
(Σx −|γ|2
Σy )|Hyx( f )|2
⟨|y( f )|2⟩
⎤
⎦.
(8.61)
8.5 Other MVAR-Modeling-Based Measures
8.5.1 Directed Transfer Function (DTF)
The directed transfer function (DTF) [5] is derived from a deﬁnition of causality that
is somewhat different from the Granger causality. For a bivariate AR process, we
have the relationships
y1(t) =
P

p=1
A1,1(p)y1(t −p) +
P

p=1
A1,2(p)y2(t −p) + ϵ1(t)
y2(t) =
P

p=1
A2,1(p)y1(t −p) +
P

p=1
A2,2(p)y2(t −p) + ϵ2(t).
It can be intuitively clear that, for example, the inﬂuence of the past values of y2(t)
on the current value of y1(t) can be assessed by the values of the AR coefﬁcients
A1,2(p), (where p = 1, . . . , P). Namely, the causal inﬂuence of y2(t) on y1(t) can
be assessed by using




P

p=1
A2
1,2(p).
(8.62)
The spectral domain quantities that play a role similar to the above quantity are
| ¯A1,2( f )| and |H1,2( f )|,
(8.63)
where the matrix ¯A( f ) is deﬁned in Eq.(8.5) and the transfer matrix H( f ) is deﬁned
as H( f ) = ¯A( f )−1. The directed transfer function (DTF) makes use of H( f ) to
express the causal relationship.
We deﬁne the (unnormalized) directed transfer function (DTF) using the elements
of the transfer function H( f ). Namely, the unnormalized directed transfer function
that represents the causal inﬂuence of y2(t) on y1(t) is deﬁned as H1,2( f ). For a
bivariate case, The relationship,

8.5 Other MVAR-Modeling-Based Measures
183
H1,2( f ) =
¯A1,2( f )
| ¯A( f )|
indicates that the directed transfer function is equal to ¯A1,2( f ) aside from the scaling
constant | ¯A( f )|.
One advantage of this deﬁnition is that the extension to a general multivariate case
is straightforward. In a general multivariate case, the unnormalized transfer function
is equal to
|H1,2( f )| = |M2,1( f )|
| ¯A( f )|
,
(8.64)
where M2,1( f ) is a minor of the (2, 1) element of the matrix ¯A( f ). In a trivariate
case, using Eq.(8.64), the unnormalized transfer function is derived as
|H1,2( f )| = | ¯A1,2 ¯A3,3 −¯A3,1 ¯A2,3|
| ¯A( f )|
,
(8.65)
where the explicit notation ( f ) is omitted in the numerator. On the right hand side,
¯A1,2 represents the direct inﬂuence from channel #1 to channel #2. The equation
above shows that even when ¯A1,2 is equal to zero, |H1,2| cannot equal zero because
the term ¯A3,1 ¯A2,3 can also not be zero. This term represents the indirect inﬂuence of
channel #1 on channel #2 via channel #3. That is, DTF contains the indirect causal
inﬂuence, as well as the direct causal inﬂuence.
The normalized DTF can be deﬁned by normalizing the causal inﬂuence on the jth
channel from all other channels. That is, when total q channels exist, the normalized
DTF is deﬁned as
μk→j = Hj,k( f )
	
[hH
j h j]
=
Hj,k( f )
	q
m=1 |Hj,m( f )|2
.
(8.66)
8.5.2 Relationship Between DTF and Coherence
The coherence can be expressed as a result of factorization of the normalized DTF.
To show this, we deﬁne the directed coherence γ j,k such that
γ j,k =
Hj,k( f )
	
hH
j Σh j
.
(8.67)
We then deﬁne the column vector
γ j( f ) = [γ∗
j,1, . . . , γ∗
j,q]T ,
(8.68)

184
8
Estimation of Causal Networks: Source-Space Causality Analysis
and using this column vector, coherence is expressed as3
φ j,k( f ) =
Sj,k
Sj, j Sk,k
=
hH
j Σhk
	
[hH
j Σh j][hH
k Σhk]
= γ H
j ( f )Σγk( f ).
(8.69)
This directed coherence γ j,k is known to represent the directional inﬂuence from
the kth to the jth channels. It contains the residual covariance matrix Σ whose off-
diagonal terms may represent the instantaneous interaction between two channels,
according to the arguments in Sect.8.3.3. Setting Σ equal to I, we have
γ j,k = Hj,k( f )
	
|hH
j h j|
,
(8.70)
which is exactly equal to the normalized DTF in Eq.(8.66). Namely, the DTF is a
measure for directional inﬂuences obtained by removing instantaneous components
from the directed coherence. Under the assumption that Σ = I, coherence φ j,k( f )
can be decomposed of the sum of the product of DTFs, such that
φ j,k( f ) =
q
m=1 Hj,m( f )H∗
k,m( f )
	
[hH
j h j][hH
k hk]
=
q

m=1
μm→j μ∗
m→k.
(8.71)
8.5.3 Partial Directed Coherence (PDC)
In Sect.8.5.2, DTF is expressed using a factorization of coherence. We apply similar
factorization to partial coherence to obtain the partial directed coherence (PDC)
[6, 12]. The starting point is Eq.(8.14), which is re-written as,
κ j,k( f ) =
¯aH
j Σ−1 ¯ak
	
[¯aH
j Σ−1 ¯a j][¯aH
k Σ−1 ¯ak]
.
(8.72)
Here, recall that ¯a j is the jth column of the matrix ¯A( f ) and ¯a j = [ ¯A1, j, . . . , ¯Aq, j].
We deﬁne
¯π j,k( f ) =
¯A j,k( f )
	
¯aH
k Σ−1 ¯ak
,
(8.73)
3 Note that since h j is the jth column vector of H H, h j is equal to h j = [H∗
j,1, . . . , H∗
j,q]T .

8.5 Other MVAR-Modeling-Based Measures
185
and a column vector ¯πk( f ) as
¯πk( f ) = [¯π1,k, . . . , ¯πq,k]T .
(8.74)
We can then express the partial coherence as
κ j,k( f ) = ¯πH
j ( f )Σ−1 ¯πk( f ).
(8.75)
The equation above indicates that the partial coherence κ j,k( f ) is factorized, and
the form of the factorization is very similar to the factorization of the coherence in
Eq.(8.69).
The partial directed coherence (PDC) is deﬁned using ¯π j,k in Eq.(8.73) with
replacing Σ with I for removing the instantaneous interaction. That is, the PDC
from the kth to the jth channels, πk→j, is deﬁned as
πk→j =
¯A j,k( f )
	
¯aH
k ¯ak
.
(8.76)
Assuming Σ−1 = I, the partial coherence κ j,k( f ) is factorized using PDC, such
that
κ j,k( f ) =
q
m=1 ¯A∗
m, j( f ) ¯Am,k( f )
	
[¯aH
j ¯a j][¯aH
k ¯ak]
=
q

m=1
π∗
j→m πk→m.
(8.77)
As discussed in the previous sections, the non-diagonal elements of the MVAR
coefﬁcient matrix ¯A( f ) should contain information on causal interaction. The PDC
directly makes use of this information. The DTF also uses this information but it
does so in an indirect manner through the transfer matrix H( f ), which is the inverse
of ¯A( f ). Although PDC is derived in a somewhat heuristic manner, it represents
causality through ¯A j,k( f ). The major difference between PDC and DTF is that PDC
only represents the direct causal inﬂuence and is not affected by the indirect inﬂuence.
8.6 Transfer Entropy
8.6.1 Deﬁnition
Granger causality and its related measures such as DTF and PDC relies on the MVAR
modeling of voxel time series. In that sense, these measures are model-based. In
this section, we introduce transfer entropy [7, 13], which does not use the MVAR
modeling of voxel time series. Explanations on fundamentals of entropy and mutual
information are found in Sect.C.3.2 in the Appendix.

186
8
Estimation of Causal Networks: Source-Space Causality Analysis
We denote two random vector time series x(t) and y(t). Let us deﬁne vectorsx(t)
and y(t) as those made by concatenating their past values such that
x(t) =
⎡
⎢⎢⎢⎣
x(t −1)
x(t −2)
...
x(t −P)
⎤
⎥⎥⎥⎦
and y(t) =
⎡
⎢⎢⎢⎣
y(t −1)
y(t −2)
...
y(t −P)
⎤
⎥⎥⎥⎦.
(8.78)
We then deﬁne the conditional entropy of y(t), given its past values y(t), such that
H(y|y) = −

p(y,y) log p(y|y) d y dy.
(8.79)
Similarly, we deﬁne the conditional entropy of y(t), given the past values x(t) and
y(t), such that
H(y|x,y) = −

p(x, y,y) log p(y|x,y) d y dx dy.
(8.80)
The transfer entropy Hx→y is deﬁned as
Hx→y = H(y|y) −H(y|x,y)
=

p(x, y,y) log log p(y|x,y)
log p(y|y)
d y dx dy.
(8.81)
In the equations above, the explicit time notation (t) is omitted for simplicity. In
Eq.(8.81), H(y|y) represents the uncertainty on the current value of y, when we
knowy, which is the past values of y. Also, H(y|x,y) represents the uncertainty on
the current value of y, when we know both x and y, which are the past values of x
and y. Therefore, the transfer entropy is equal to the reduction of uncertainty of the
current value of y as a result of knowing the past values of x.
8.6.2 Transfer Entropy Under Gaussianity Assumption
Assuming that the random vectors x and y follow a Gaussian distribution, using
Eq.(C.61), the conditional entropy H(y|y) is expressed as [14]
H(y|y) = 1
2 log
Σ yy −Σ y ˜yΣ−1
˜y ˜y ΣT
y ˜y
 ,
(8.82)

8.6 Transfer Entropy
187
where4
Σ yy = ⟨y yT ⟩,
(8.83)
Σ y ˜y = ⟨yyT ⟩,
(8.84)
Σ ˜y ˜y = ⟨yyT ⟩.
(8.85)
Similarly, the conditional entropy H(y|x,y) is expressed as
H(y|x,y) = H(y|z) = 1
2 log
Σ yy −Σ y˜zΣ−1
˜z˜z ΣT
y˜z
 ,
(8.86)
wherez = [yT , xT ]T and
Σ y˜z = ⟨yzT ⟩,
(8.87)
Σ ˜z˜z = ⟨zzT ⟩.
(8.88)
Thus, the transfer entropy Hx→y is given by
Hx→y = H(y|y) −H(y|x,y)
= H(y|y) −H(y|z) = 1
2 log
Σ yy −Σ y ˜yΣ−1
˜y ˜y ΣT
y ˜y

Σ yy −Σ y˜zΣ−1
˜z˜z ΣT
y˜z

.
(8.89)
8.6.3 Equivalence Between Transfer Entropy and Granger
Causality
We will show that the transfer entropy and Granger causality are equivalent under the
Gaussianity assumption. The arguments here follow those in [14]. As discussed in
Sect.8.3.2, we consider the two forms of the regression to deﬁne Granger causality.
In the ﬁrst regression, y(t) is regressed using only its past values, such that
y(t) =
P

p=1
A(p)y(t −p) + e = Ay(t) + e,
(8.90)
where A = [A(1), . . . , A(P)],y(t) is deﬁned in Eq.(8.78), and e is a residual vector.
In the second regression, y(t) is regressed using not only its past values but also the
past values of x(t), such that
4 Note that in Sect.C.3.3 the expectation operator E[ · ] is used, instead of the averaging operator
⟨· ⟩. They have the same meaning in the arguments here.

188
8
Estimation of Causal Networks: Source-Space Causality Analysis
y(t) =
P

p=1
A(p)y(t −p) +
P

p=1
B(p)x(t −p) + ϵ
= Ay(t) + Bx(t) + ϵ = Cz + ϵ.
(8.91)
where B = [B(1), . . . , B(P)], C = [A, B], and ϵ is a residual vector.
The Granger causality from the time series x to y, Gx→y is deﬁned as
Gx→y = log |Σe|
|Σϵ|,
(8.92)
where Σe is the covariance matrix of the residual e in Eq.(8.90), and Σϵ is the
covariance matrix of the residual ϵ in Eq.(8.91). Using Eq.(C.57), we have
|Σe| = |Σ yy −Σ y ˜yΣ−1
˜y ˜y ΣT
y ˜y|,
(8.93)
and
|Σϵ| = |Σ yy −Σ y˜zΣ−1
˜z˜z ΣT
y˜z|.
(8.94)
Substituting Eqs. (8.93) and (8.94) into (8.92), we get
Gx→y = log
|Σ yy −Σ y ˜yΣ−1
˜y ˜y ΣT
y ˜y|
|Σ yy −Σ y˜zΣ−1
˜z˜z ΣT
y˜z|
.
(8.95)
The right-hand side of the equation above is exactly equal to that of Eq.(8.89) except
for the multiplicative constant 1/2, indicating that these two measures are equivalent.
8.6.4 Computation of Transfer Entropy
In Sect.C.3.2 in the Appendix, we show that, assuming the Gaussian processes for
the r × 1 real random vector x and the q × 1 real random vector y, the mutual
information between these two vectors is expressed as
I(x, y) = 1
2 log
1
I −Σ−1
yy Σ yxΣ−1
xx ΣT
yx

= 1
2
d

j=1
log
1
1 −λ j
,
(8.96)
where we assume d = min{q,r}, and λ j is the jth eigenvalue of the matrix,
Σ−1
yy Σ yxΣ−1
xx ΣT
yx.

8.6 Transfer Entropy
189
We can derive a similar formula to compute the transfer entropy. To do so, we use the
similarity between the transfer entropy and mutual information. Using Eq.(C.45),
the conditional entropy H(y|x,y) is rewritten as
H(y|x,y) = H(y,x|y) −H(x|y).
(8.97)
Substituting this equation into Eq.(8.81), we get
Hx→y = H(x|y) + H(y|y) −H(y,x|y).
(8.98)
Comparing the equation above with Eq.(C.46), it can be seen that the transfer entropy
is equal to the mutual information between y(t) and x(t), when y(t) is given.
We deﬁne Σu,v|w as
Σu,v|w = Σuv −ΣuwΣ−1
wwΣT
vw.
(8.99)
Then, using Eq.(C.61), we can express H(y|y), and H(x|y), such that
H(y|y) = 1
2 log
Σ y,y| ˜y
 ,
(8.100)
H(x|y) = 1
2 log
Σ ˜x,˜x| ˜y
 .
(8.101)
On the basis of Eq.(C.59), we also derive
H(y,x|y) = 1
2 log

Σ y,y| ˜y ΣT
˜x,y| ˜y
Σ ˜x,y| ˜y Σ ˜x,˜x| ˜y
 .
(8.102)
Thus, substituting the equations above into Eq.(8.98), we obtain
Hx→y = 1
2 log
1
I −Σ−1
y,y| ˜yΣ y,˜x| ˜yΣ−1
˜x,˜x| ˜yΣT
y,˜x| ˜y

.
(8.103)
Accordingly, deﬁning the eigenvalues of a matrix,
Σ−1
y,y| ˜yΣ y,˜x| ˜yΣ−1
˜x,˜x| ˜yΣT
y,˜x| ˜y,
(8.104)
as χ j ( j = 1, . . . , d), the transfer entropy is given by
Hx→y = 1
2
d

j=1
log
1
1 −χ j
.
(8.105)

190
8
Estimation of Causal Networks: Source-Space Causality Analysis
8.7 Estimation of MVAR Coefﬁcients
8.7.1 Least-Squares Algorithm
The Granger-causality and related measures rely on the modeling of the multivariate
vector auto-regressive (MVAR) process of the source time series. Use of these mea-
sures requires estimating the MVAR coefﬁcient matrices. This section deals with the
estimation of MVAR coefﬁcients. A q × 1 random vector y(t) is modeled using the
MVAR process, such that
y(t) =
P

p=1
A(p)y(t −p) + e(t).
(8.106)
We can estimate the MVAR coefﬁcients Ai, j(p) where i, j = 1, . . . , q and p =
1, . . . , P based on the least-squares principle. To derive the least-squares equation,
let us explicitly write the MVAR process for the ℓth component yℓ(t) as
yℓ(t) =
q

j=1
Aℓ, j(1)y j(t −1) +
q

j=1
Aℓ, j(2)y j(t −2)
+ · · · +
q

j=1
Aℓ, j(P)y j(t −P) + eℓ(t).
(8.107)
We assume that the source time series are obtained at t = 1, . . . , K where K ≫
q × P. Then, since Eq.(8.107) holds for t = (P +1), . . . , K, a total of K −P linear
equations are obtained by setting t = (P + 1), . . . , K in Eq.(8.107).
These equations are formulated in a matrix form,
yℓ= Gxℓ+ eℓ.
(8.108)
Here, the (K −P) × 1 column vector yℓis deﬁned as
yℓ= [yℓ(P + 1), yℓ(P + 2), . . . , yℓ(K)]T .
(8.109)
In Eq.(8.108), G is a (K −P) × Pq matrix expressed as
G =
⎡
⎢⎢⎢⎣
y1(P)
· · ·
yq(P)
· · ·
y1(1)
· · ·
yq(1)
y1(P + 1) · · · yq(P + 1) · · ·
y1(2)
· · ·
yq(2)
...
y1(K −1) · · · yq(K −1) · · · y1(K −P) · · · yq(K −P)
⎤
⎥⎥⎥⎦.
(8.110)

8.7 Estimation of MVAR Coefﬁcients
191
The column vector xℓis expressed as
xℓ=

Aℓ,1(1), . . . , Aℓ,q(1), . . . , Aℓ,1(P), . . . , Aℓ,q(P)
T .
(8.111)
The residual vector eℓis given by
eℓ= [eℓ(P + 1), . . . , eℓ(K)]T .
(8.112)
Equation (8.108) is called the Yule-Walker equation. The least-squares estimate of
xℓ, xℓ, is then obtained using,
xℓ= (GT G)−1GT yℓ.
(8.113)
8.7.2 Sparse Bayesian (Champagne) Algorithm
Since causality analysis is generally performed using the source time series esti-
mated from non-averaged data, the estimated time series inevitably contains a large
inﬂuence of noise, which may cause errors in the MVAR-coefﬁcient estimation. One
approach to reduce such errors is to impose a sparsity constraint when estimating the
MVAR coefﬁcients. The key assumption here is that true brain interaction causes a
small number of MVAR coefﬁcients to have non-zero values, and most of the MVAR
coefﬁcients remain zero. If this is true, the sparsity constraint should be able to pre-
vent MVAR coefﬁcients that must be equal to zero from having erroneous non-zero
values.
We can apply a simpler version of the Champagne algorithm described in Chap.4
to this MVAR coefﬁcient estimation. In the Champagne algorithm, the prior proba-
bility distribution of xℓ, p(xℓ), is assumed to be Gaussian:
p(xℓ) = N(xℓ|0, Φ−1),
(8.114)
where Φ is a diagonal precision matrix. The probability distribution of yℓgiven xℓ,
p(yℓ|xℓ), is also assumed to be Gaussian:
p(yℓ|xℓ) = N(yℓ|Gxℓ, Λ−1),
(8.115)
where Λ is a diagonal noise precision matrix. Then, the posterior distribution of xℓ,
p(xℓ|yℓ), is shown to be Gaussian, and it is expressed as
p(xℓ|yℓ) = N(xℓ|¯xℓ, Γ −1).
(8.116)
Unlike the Champagne algorithm in Chap.4, the update equations for Φ and Λ
can be derived using the expectation-maximization (EM) algorithm. This is because,

192
8
Estimation of Causal Networks: Source-Space Causality Analysis
since (K −P) ≫Pq holds, the estimation problem is not an ill-posed problem.
The EM algorithm for the Gaussian model is described in Sect.B.5 in the Appendix.
According to Eqs.(B.24) and (B.25), the E-step update equations are given by
Γ = GT ΛG + Φ
(8.117)
¯xℓ= Γ −1GT Λyℓ
(8.118)
The parameters, Φ and Λ, are estimated in the M step of the EM algorithm. The
updateequationforΦ isobtainedfromEq.(B.42)withsetting K = 1inthatequation,
resulting in
Φ−1 = diag

¯xℓ¯xT
ℓ+ Γ −1
,
(8.119)
where diag[ · ] indicates a diagonal matrix whose diagonal elements are equal to
those of a matrix in the parentheses. According to Eq.(B.45), the noise precision
matrix Λ is given by
Λ−1 = diag

∥yℓ−G ¯xℓ∥2 + GΓ −1GT 
.
(8.120)
The estimate of the MVAR coefﬁcients is obtained as ¯xℓ, after the EM iteration
is terminated. This algorithm is similar to, but considerably simpler than the one
proposed in [15].
8.8 Numerical Examples
8.8.1 Experiments Using Bivariate Causal Time Series
Numerical experiments were performed to illustrate the properties of the causality
measures described in this chapter. The source-space causality analysis was applied
in which source time series are ﬁrst estimated from simulated MEG recordings,
and the MVAR coefﬁcients are then computed using the time series at selected
voxels. Here, we use a sensor alignment of the 275 whole-head MEG sensor array
from OmegaTM (VMS Medtech, Coquitlam, Canada) neuromagnetometer. Three
sources are assumed to exist on the vertical single plane of x = 0cm, as in the
numerical experiments in the previous chapters. The source-sensor conﬁguration and
the coordinate system are depicted in Fig.8.1. As shown in this ﬁgure, we assume
three sources and the time series of these three sources are denoted s1(t), s2(t),
and s3(t). The ﬁrst experiments assume a causal relationship between bivariate time
series, and an information ﬂow exists from s1(t) to s2(t). The time series s1(t) and
s2(t) are generated by using the MVAR process [11]

8.8 Numerical Examples
193
-2
0
2
  8
10
y (cm)
sensor array
z
y
z (cm)
6
first source
second source
third source
(a)
-2
0
2
  8
10
y (cm)
sensor array
z
y
z (cm)
6
first source
second source
third source
(b)
Fig. 8.1 The coordinate system and source-sensor conﬁguration used in the numerical experiments.
The plane at x = 0cm is shown. The small circles show the locations of the three sources, and the
bold arrows schematically show their causal relationships assumed in the experiments. a The ﬁrst
experiment with bivariate causal time series. b The second experiments with trivariate causal time
series
 s1(t)
s2(t)

=
 0.9
0
0.16 0.8
  s1(t −1)
s2(t −1)

+
 −0.5
0
−0.2 −0.5
  s1(t −2)
s2(t −2)

+ e(t).
(8.121)
The time series of the third source, s3(t), was generated using Gaussian random
numbers. Thus, the third source activity was independent from either the ﬁrst or
the second source activities. The causal relationship assumed in this experiment is
depicted in Fig.8.1a.
Then, simulated sensor recordings were computed by projecting the time series
of the three sources onto the sensor space by using the sensor lead ﬁeld. A small
amount of simulated sensor noise was added. The Champagne source reconstruction
algorithm was applied to the simulated sensor recordings. Here, three-dimensional
reconstruction was performed on a region deﬁned as −4 ≤x ≤4, −4 ≤x ≤4,
and 6 ≤z ≤12cm with a voxel interval equal to 0.5cm. Reconstructed source time
seriess1(t),s2(t), ands3(t) were obtained as the time series at voxels nearest to the
assumed source locations.
Onces1(t),s2(t) were obtained, the MVAR coefﬁcients between these time series
were estimated by using the least-squares method in Sect.8.7.1. Using the esti-
mated MVAR coefﬁcients, we computed the spectral Geweke causality described in
Sect.8.4, as well as coherence. The results are shown in Fig.8.2a.
In these results, coherence (Eq.(8.51)) can detect an interaction between the ﬁrst
and second source activities. The spectral Geweke causality in Eqs.(8.60) and (8.61)
detects the unidirectional information ﬂow from the ﬁrst source activity to the second
source activity. We also computed the partial directed coherence (PDC) and the
directed transfer function (DTF), with results shown in Fig.8.2b. The PDC and DTF

194
8
Estimation of Causal Networks: Source-Space Causality Analysis
0
0.2
0.4
0
0.1
0.2
amplitude
causality:2
>1
frequency
(a)
0
0.2
0.4
causality:1
>2
frequency
0
0.2
0.4
0
0.5
1
amplitude
coherence
0
0.2
0.4
0
0.5
1
pdc:2
>1
0
0.2
0.4
pdc:1
>2
0
0.2
0.4
0
0.5
1
dtf:2
>1
frequency
(b)
0
0.2
0.4
dtf:1
>2
frequency
Fig. 8.2 Results of computing bivariate measures betweens1(t) ands2(t) in the ﬁrst experiment.
a Results of computing spectral causality measures and the coherence. Results for the coherence are
shown in the top panel. Results for the spectral Geweke causality are shown on the bottom-left, and
bottom-right panels. b Results of computing the partial directed coherence (PDC) (top panels) and
the directed transfer function (DTF) (bottom panels). The abscissas of plots indicate the normalized
frequency. (The frequency is normalized by the sampling frequency in which the value 0.5 indicates
the Nyquist frequency.) The ordinates indicate the relative amplitudes of corresponding measures
can also detect the information ﬂow from the ﬁrst to the second sources. In this
two-channel experiment, the PDC and DTF provide identical results, because, aside
from the scaling, both measures are equal to | ¯A1,2( f )| in a bivariate case.
The causality measures were computed betweens1(t) ands3(t), and these results
are shown in Fig.8.3. Since there was no interaction between s1(t) and s3(t), all the
causal measures used in the experiment, as well as the coherence, are close to zero
in these results.
8.8.2 Experiments Using Trivariate Causal Time Series
Next, we conducted experiments using trivariate causal time series. In these exper-
iments, the time series of the three sources are generated using the MVAR process
introduced in [11],
⎡
⎣
s1(t)
s2(t)
s3(t)
⎤
⎦=
⎡
⎣
0.8 0 0.4
0 0.9 0
0 0.5 0.5
⎤
⎦
⎡
⎣
s1(t −1)
s2(t −1)
s3(t −1)
⎤
⎦
+
⎡
⎣
−0.5
0
0
0
−0.8
0
0
0
−0.2
⎤
⎦
⎡
⎣
s1(t −2)
s2(t −2)
s3(t −2)
⎤
⎦+ e(t).
(8.122)
This MVAR process represents the causal relationship depicted in Fig.8.1b, in which
the second source has a directional causal inﬂuence on the third source and the third

8.8 Numerical Examples
195
0
0.2
0.4
0
0.1
0.2
amplitude
causality:3−−>1
frequency
(a)
0
0.2
0.4
causality:1−−>3
frequency
0
0.2
0.4
0
0.5
1
amplitude
coherence
0
0.2
0.4
0
0.5
1
pdc:3−−>1
0
0.2
0.4
pdc:1−−>3
0
0.2
0.4
0
0.5
1
dtf:3−−>1
frequency
(b)
0
0.2
0.4
dtf:1−−>3
frequency
Fig. 8.3 Results of computing bivariate measures betweens1(t) ands3(t) in the ﬁrst experiments.
a Results of computing spectral causality measures and the coherence. Results for the coherence are
shown in the top panel. Results for the spectral Geweke causality are shown in the bottom panels.
b Results of computing the partial directed coherence (PDC) (top panels) and the directed transfer
function (DTF) (bottom panels). Explanations on the ordinate and abscissa variables are found in
the caption for Fig.8.2
0
0.5
1
2−−>1
amplitude
1−−>2
0
0.5
1
3−−>1
amplitude
1−−>3
0
0.2
0.4
0
0.5
1
3−−>2
frequency
amplitude
(a)
0
0.2
0.4
2−−>3
2−−>1
1−−>2
3−−>1
1−−>3
3−−>2
2−−>3
frequency
0
0.2
0.4
frequency
(b)
0
0.2
0.4
frequency
Fig. 8.4 a The plot of partial directed coherence (PDC) and b directed transfer function (DTF)
computed using the model MVAR coefﬁcients in Eq.(8.122). Explanations on the ordinate and
abscissa variables are found in the caption for Fig.8.2
source has a directional inﬂuence on the ﬁrst source. The PDC and DTF computed
using the model MVAR coefﬁcients which appear in Eq.(8.122) are shown in Fig.8.4.
The results in Fig.8.4 show the ground truth in the following experiments.

196
8
Estimation of Causal Networks: Source-Space Causality Analysis
The signal part of simulated sensor recordings, bs(t), was generated by projecting
the time series of the three sources onto the sensor space using the sensor lead
ﬁeld. The ﬁnal form of simulated MEG recordings b(t) was computed by adding
spontaneous MEG to bs(t), such that b(t) = bs(t) + ϱbI(t), where bI(t) is the
spontaneous MEG measured using the same sensor array, and ϱ is a constant that
controls the signal-to-interference ratio(SIR) of the generated sensor recordings. We
ﬁrst set ϱ in order for SIR to be equal to 8, and computed the simulated MEG
recordings b(t). The Champagne source reconstruction algorithm was applied to
b(t), and the estimated time series of the three sources,s1(t),s2(t), ands3(t) were
obtained. The MVAR coefﬁcients were estimated using the least-squares method.
The results of computing PDC and DTF are shown in Fig.8.5.
In Fig.8.5, results very close to the ground truth in Fig.8.4 were obtained. The
DTF detects the information ﬂow from the second to the ﬁrst sources, which is the
indirect causal coupling via s3(t), but the PDC does not detect this indirect coupling.
These results are consistent with the explanation in Sect.8.5.1.
We next generated the simulated sensor recordings with setting SIR at 2. The
results of computing the PDC and DTF are shown in Fig.8.6. Here, the MVAR
coefﬁcients were estimated using the least-squares method. The ﬁgure shows that
large spurious causal relationships exist, due to the low SIR of the generated data.
We then applied the sparse Bayesian algorithm described in Sect.8.7.2 for estimating
the MVAR coefﬁcients, and computed the PDC and DTF. The results are shown in
0
0.5
1
2−−>1
amplitude
1−−>2
0
0.5
1
3−−>1
amplitude
1−−>3
0
0.2
0.4
0
0.5
1
3−−>2
frequency
amplitude
(a)
0
0.2
0.4
2−−>3
2−−>1
1−−>2
3−−>1
1−−>3
3−−>2
2−−>3
frequency
0
0.2
0.4
frequency
(b)
0
0.2
0.4
frequency
Fig. 8.5 a The plot of partial directed coherence (PDC) and b the directed transfer function (DTF)
computed using MVAR coefﬁcients estimated from the simulated MEG data with SIR equal to 8.
The least-squares method in Sect.8.7.1 was used for estimating MVAR coefﬁcients. Explanations
on the ordinate and abscissa variables are found in the caption for Fig.8.2

8.8 Numerical Examples
197
0
0.5
1
2−−>1
amplitude
1−−>2
0
0.5
1
3−−>1
amplitude
1−−>3
0
0.2
0.4
0
0.5
1
3−−>2
frequency
amplitude
(a)
0
0.2
0.4
2−−>3
frequency
2−−>1
1−−>2
3−−>1
1−−>3
0
0.2
0.4
3−−>2
frequency
(b)
0
0.2
0.4
2−−>3
frequency
Fig. 8.6 a Plots of partial directed coherence (PDC) and b directed transfer function (DTF) com-
puted using MVAR coefﬁcients estimated from the simulated MEG data with SIR equal to 2. The
least-squares method in Sect.8.7.1 was used for estimating MVAR coefﬁcients. Explanations on
the ordinate and abscissa variables are found in the caption for Fig.8.2
0
0.5
1
2−−>1
amplitude
1−−>2
0
0.5
1
3−−>1
amplitude
1−−>3
0
0.2
0.4
0
0.5
1
3−−>2
frequency
amplitude
(a)
0
0.2
0.4
2−−>3
frequency
2−−>1
1−−>2
3−−>1
1−−>3
0
0.2
0.4
3−−>2
frequency
(b)
0
0.2
0.4
2−−>3
frequency
Fig. 8.7 a Plots of partial directed coherence (PDC) and b the directed transfer function (DTF)
computed using MVAR coefﬁcients estimated from the simulated MEG data with SIR equal to 2.
The sparse Bayesian algorithm described in Sect.8.7.2 was used for estimating MVAR coefﬁcients.
Explanations on the ordinate and abscissa variables are found in the caption for Fig.8.2

198
8
Estimation of Causal Networks: Source-Space Causality Analysis
Fig.8.7. The results are very close to the ground truth in Fig.8.4, demonstrating the
effectiveness of the sparse Bayesian method in the MVAR estimation.
References
1. C.W. Granger, Investigating causal relations by econometric models and cross-spectral meth-
ods. Econom.: J. Econom. Soc. 37(3), 424–438 (1969)
2. C. Granger, Investigating causal relations by econometric models and cross-spectral methods.
Econom. Soc. Monogr. 33, 31–47 (2001)
3. J. Geweke, Measures of conditional linear dependence and feedback between time series. J.
Am. Stat. Assoc. 77, 304–313 (1982)
4. J. Geweke, Inference and causality in economic time series models, in Handbook of Econo-
metrics, vol. 2 (Elsevier, Amsterdam, 1984), pp. 1101–1144
5. M.J. Kami´nski, K.J. Blinowska, A new method of the description of the information ﬂow in
the brain structure. Biol. Cybern. 65, 203–210 (1991)
6. L.A. Baccalá, K. Sameshima, Partial directed coherence: a new concept in neural structure
determination. Biol. Cybern. 84, 463–474 (2001)
7. R. Vicente, M. Wibral, M. Lindner, G. Pipa, Transfer entropy—a model-free measure of effec-
tive connectivity for the neurosciences. J. Comput. Neurosci. 30, 45–67 (2011)
8. K.J. Blinowska, M. Kami´nski, Multivariate signal analysis by parametric models, in Handbook
of Time Series Analysis: Recent Theoretical Developments and Applications (2006), p. 373
9. P. Franaszczuk, K. Blinowska, M. Kowalczyk, The application of parametric multichannel
spectral estimates in the study of electrical brain activity. Biol. Cybern. 51(4), 239–247 (1985)
10. C.D. Meyer, Matrix Analysis and Applied Linear Algebra (Society for Industrial and Applied
Mathematics, Philadelphia, 2000)
11. M. Ding, Y. Chen, S.L. Bressler, Granger causality: basic theory and application to neuro-
science, in Handbook of Time Series Analysis, ed. by B. Schelter et al. (Wiley-VCH, Weinheim,
2006), pp. 500–600
12. K. Sameshima, L.A. Baccalá, Using partial directed coherence to describe neuronal ensemble
interactions. J. Neurosci. Methods 94(1), 93–103 (1999)
13. T. Schreiber, Measuring information transfer. Phys. Rev. Lett. 85, 461–464 (2000)
14. L. Barnett, A.B. Barrett, A.K. Seth, Granger causality and transfer entropy are equivalent for
Gaussian variables. Phys. Rev. Lett. 103, 238701 (2009)
15. W.D. Penny, S.J. Roberts, Bayesian multivariate autoregressive models with structured priors.
IEE Proc. 149, 33–41 (2002)

Chapter 9
Detection of Phase–Amplitude Coupling
in MEG Source Space: An Empirical Study
9.1 Introduction
Neural oscillations across multiple frequency bands have consistently been observed
in EEG and MEG recordings. The alpha rhythm (8–13Hz), the best known brain
oscillation, is observed throughout the brain. Other well-known oscillations include
the delta (1–3Hz), theta (4–7Hz), beta (13–30Hz), and gamma (>30Hz) bands.
Gamma-band oscillations are considered to be those most closely associated with
ﬁring of cortical neurons [1].
Recently, neural substrates responsible for the genesis of such brain rhythms and
their functional role in brain information processing have been the subject of intense
investigations. As a result, a number of hypotheses for explaining the role of neural
oscillations have emerged, some including the concept that sensory input information
is coded using multiple temporal scales [2–6]. One of these hypotheses claims that
temporal coding through regulation of oscillators’ ﬁring is essential for the effective
exchange of information in the mammalian brain [7]. Another hypothesis claims
that temporal coding organized by coupled alpha and gamma oscillations prioritizes
visual processing [8].
Arecent study[9] suggests that informationgatingviaselectiveattentionis closely
related to temporal encoding. Another study suggests that temporal encoding com-
bined with the encoding by the phase of ﬁring is expected to carry more information
than temporal encoding by spike rates or spike patterns [10]. Therefore, temporal
encoding due to phase of ﬁring is considered to be one of the most plausible expla-
nations behind mechanisms of information processing in the brain.
On the other hand, if the brain uses temporal encoding based on the phase of ﬁring,
there should be an oscillator that regulates neural ﬁring based on its phase. There-
fore, the hypothesis for temporal encoding based on phase of ﬁring suggests a pos-
sible coupling between oscillators with different frequencies. That is, the amplitude
The authors of this chapter are Eiichi Okumura, Ph.D. and Takashi Asakawa, Ph.D. who are
with Neuroscience Project Ofﬁce, Innovation Headquaters, Yokogawa Electric Corporation.
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9_9
199

200
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
of the high-frequency oscillator increases at a certain phase of the low-frequency
oscillator. Such relationship between oscillators with different frequencies is called
cross-frequency coupling [11], which is the main topic of this chapter.
The aim of this chapter is to demonstrate that MEG source space analysis is
able to detect such cross-frequency interactions. It is a widely accepted notion that
low-frequency oscillations, such as alpha, are “encoders” of information processing,
and high-frequency oscillations, such as gamma, represent particular neuronal activ-
ities. Therefore, analysis of cross-frequency interactions between different cortical
regions would be extremely important for the investigation of information processing
mechanisms in a human brain.
9.2 Types of Cross-Frequency Coupling
Connections between brain oscillators at the same frequency, termed as same-
frequency coupling (SFC) in this chapter, have been investigated in various functional
connectivity studies. In such studies, the coherence (or phase-locking value) between
target oscillations is commonly used as a measure of connectivity.1 In contrast to
SFC, the cross-frequency coupling (CFC) is a phenomenon in which oscillations
with different frequencies interact with each other. In general, CFC is classiﬁed into
the following coupling types between the oscillators:
• Phase–amplitude coupling (PAC): The phase of a low-frequency (LF) oscillation
drives the amplitude of a high-frequency (HF) oscillation with the highest ampli-
tude of the HF oscillation occurring at a speciﬁc phase of the LF oscillation, which
is referred to as the “preferred coupling phase”.
• Amplitude–amplitude coupling (AAC): The amplitude of an oscillation correlates
the amplitude of other oscillations. Even if the oscillators are not directly coupled,
their amplitude can be co-modulated.
• Phase–phase coupling (PPC): The phase of an LF oscillation is related to the phase
of an HF oscillation. This is sometimes referred to as “n:m phase locking” where
phase locking occurs between n cycles of the HF oscillation and m cycles of the
LF oscillation.
Figure9.1 depicts typical time courses of oscillations when a coupling exists.
When a pair of oscillations is interacting with each other, the interaction causes
a constant phase relationship between the two HF oscillations, resulting in same-
frequency coupling. This situation is depicted in Fig.9.1a. When phase–amplitude
coupling exists between two oscillators, the phase of the LF oscillation regulates the
amplitude of the HF oscillation. In this case, the HF oscillation increases in amplitude
at a certain phase of the LF oscillation. Such a situation is depicted in Fig.9.1b. One
special case of the PAC is that, when a common LF oscillation simultaneously drives
the amplitudes of two HF oscillators, amplitude–amplitude coupling (AAC) between
1 Detection of SFC in the MEG source space is the scope of Chap.7.

9.2 Types of Cross-Frequency Coupling
201
Fig. 9.1 a Typical time
courses of two oscillators
when same-frequency
coupling (SFC) exists.
b Typical time courses of
two oscillators when phase–
amplitude coupling (PAC)
exists. c Typical time courses
of two oscillators when PAC
exists and a common lower
frequency (LF) component
simultaneously drives
the amplitudes of two higher
frequency (HF) oscillators.
In this case, an amplitude–
amplitude coupling (AAC)
between the two HF
oscillations exists even when
they are not directly coupled
(a)
1
HF2
(b)
LF
PAC
(c)
PAC
LF
AAC
HF
HF
1
HF2
HF
PAC
the two oscillators exists. Namely, AAC can exist, even when two HF oscillators are
not directly coupled. This situation is depicted in Fig.9.1c.
This chapter is based on the hypothesis that, in brain information processing,
the phases of the low-frequency oscillations encode the amplitudes of the high-
frequency oscillations that represent particular neuronal activities. If this hypothesis
is true, analyzing the relationship between the phases of the LF oscillations and
the amplitudes of the HF oscillations, i.e., analyzing the phase–amplitude coupling
(PAC) between the LF and HF signals is extremely important in investigating brain
information processing. Therefore, we focus on the PAC and related topics in this
chapter.
9.3 Local PAC and Cross-Location PAC
When phase–amplitude coupling is observed between the HF and LF signals mea-
sured at the same location in a brain, this PAC is called the local PAC, and termed
lPAC. On the other hand, when the PAC is observed between the HF and LF signals
measured at different locations, this PAC is called the cross-location PAC, and here
termed xPAC.
So far, most studies have investigated lPAC, but some recent studies have extended
their scope to xPAC. For example, in a study using an electrocorticogram (ECoG)
recording, the presence of xPACs during working memory tasks was reported [12].
The xPAC between cortical gamma oscillations and an LF oscillation from the

202
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
thalamus was reported in [13, 14]. We also show, in Sect.9.5, that lPAC and xPAC
can be detected from a healthy subject using hand-motor MEG data.
9.4 Quantiﬁcation of Phase–Amplitude Coupling
9.4.1 Instantaneous Amplitude and Phase
In the PAC analysis, it is essential to calculate the instantaneous amplitude of the
high-frequency signal and the instantaneous phase of the low-frequency signal. The
procedure to obtain these quantities in the source space analysis is summarized as
follows. Let us deﬁne the voxel time course as x(t). The voxel time course is band-
pass ﬁltered into speciﬁc HF and LF bands of interest, resulting in the LF signal
xL(t) and the HF signal xH(t).
The Hilbert transform is then applied to extract their instantaneous amplitudes
and phases, such that
A [xL(t)] = AL(t) exp [iθL(t)] ,
(9.1)
A [xH(t)] = AH(t) exp [iθH(t)] ,
(9.2)
where A[ · ] indicates an operator that converts a real-valued signal into its analytic
signal. In the equations above, AL(t) and AH(t) are the instantaneous amplitudes of
the LF and HF signals, and θL(t) and θH(t) are the instantaneous phases of the LF
and HF signals. Once amplitudes and phases of the LF and HF signals are obtained,
we proceed with the evaluation of the strength of the PAC using measures described
in the following subsections.
9.4.2 Amplitude–Phase Diagram
To evaluate the strength of the PAC, it is necessary to derive an empirical distribution
of the HF signal amplitude, AH(t), with respect to the phase of the LF signal, θL(t).
To obtain such an empirical distribution, we observe AH(t) for a time window con-
taining several cycles of the LF signal, and the amplitude values AH(t) are classiﬁed
according to the phase of the LF signal. Let us divide the phase value into total q
intervals such as Δ1, Δ2, . . . , Δq, where Δ j = [2π j−1
q , 2π j
q ]. There are multiple
(and usually many) values of AH(t) obtained when θL(t) has a value within Δ j. The
mean of such values of AH(t) is computed, and it is denoted Ψ j, i.e.,
Ψ j = ⟨AH(t)⟩θL(t)∈Δ j ,
(9.3)

9.4
Quantiﬁcation of Phase–Amplitude Coupling
203
where ⟨· ⟩θL(t)∈Δ j indicates taking the mean of amplitude values obtained when
θL(t) belongs to Δ j. The mean amplitude Ψ j is computed for all the phase bins
Δ1, . . . , Δq, and the resultant set of Ψ1, . . . , Ψq represents the distribution of the
mean HF signal amplitude with respect to the phase of the LF signal.
The plot of these mean amplitudes Ψ j with respect to the phase bins is called the
amplitude–phase diagram, which directly expresses the dependence of the HF signal
amplitude on the phase of the LF signal. Therefore, if no PAC occurs, AH(t) does not
depend on θL(t). Thus, the value of Ψ j is independent from the index j, resulting in
a uniform amplitude–phase diagram. On the contrary, if a PAC occurs, the amplitude
of the HF signal becomes stronger (or weaker) at speciﬁc phase values of the LF
signal, and the amplitude–phase diagram deviates from the uniform distribution. We
show several examples of the amplitude–phase diagram in Sect.9.5.
9.4.3 Modulation Index (MI)
The modulation index [15] is a measure that quantizes the deviation of the amplitude–
phase diagram from a uniform distribution. To compute the modulation index, we
ﬁrst normalize the mean amplitude values, Ψ1, . . . , Ψq, by their total sum. That is,
the normalized mean amplitude value for the jth phase bin, expressed as p( j), is
obtained as
p( j) =
Ψ j
q
i=1 Ψi
.
(9.4)
This p( j) is the normalized version of the amplitude–phase diagram, which can be
interpreted as the empirically derived probability distribution of the occurrence of
AH(t) obtained when θL(t) belongs to Δ j. If no PAC occurs, there is no speciﬁc
relationship between AH(t) and θL(t), resulting in p( j) having a uniform value.
Thus, the difference of the empirical distribution p( j) from the uniform distribution
can be a measure of the strength of PAC. The modulation index, MI, employs the
Kullback–Leibler distance to assess this difference, and is deﬁned as
MI = K [p( j)∥u( j)] =
q

j=1
p( j) log
 p( j)
u( j)

,
(9.5)
where u( j) is the uniform distribution, which is u( j) = 1/q ( j = 1, . . . , q).
The time variation of the modulation index expresses temporal dynamics of the
PAC, which represents the changes in the functional states of a brain. To compute the
modulation index time variation, the source time course is divided into multiple time
windows, and a value of the modulation index is obtained from each time window.
Assuming that a total of K windows are used, we obtain a series of modulation index
values, MI(t1), MI(t2), . . . , MI (tK ), which expresses the temporal change of the
PAC strength.

204
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
9.4.4 Phase-Informed Time-Frequency Map
A “phase-informed” time-frequency map is the time-frequency map of the HF signal
xH(t) displayed with the overlay of the sinusoidal time course cos(θL(t)). By dis-
playing the time-frequency map of xH(t) in this manner, the relationship between
the amplitude of the HF signal and the phase of the LF signal can be visualized in an
intuitive manner. Particularly, the preference of the HF signal amplitude to the phase
of the LF signal is clearly shown. This type of map was ﬁrst introduced by Canolty
et al. [11] and it is often called as the Canolty map. Examples of the phase-informed
TF map obtained using hand-motor MEG data are shown in Sect.9.5.
9.5 Source Space PAC Analysis: An Example Study Using
Hand-Motor MEG Data
9.5.1 Experimental Design and Recordings
Here, we present results of an example study to demonstrate the noninvasive detection
of PAC using the MEG source space analysis. We used a hand-motor task similar to
the task used in [16]. A healthy, right-handed subject (30 years old male) participated
in the MEG study.2 The subject was asked to perform grasping movements (grasping
a soft toy) with his left hand every 10s. An auditory cue was provided to inform the
grasp timing. The subject repeated grasping 64 times with his eyes open.
MEGdatawasacquiredusingawhole-head160-channelMEGsystem(PQA160C,
Yokogawa Electric Corporation, Japan) with a sample rate of 2,000Hz. A band-pass
ﬁlter with a bandwidth of 0.16–500Hz was applied. Electromyogram was recorded
from the subject’s left forearm to monitor subject’s hand movements. During the
recordings, the subject lay in the supine position on a bed in a magnetically shielded
room. The subject’s T1–weighted MRI was obtained, which consists of 166 slices
of 1.2mm thickness. A single-slice image has 512 × 512 voxels in a ﬁeld of view of
261 × 261mm.
9.5.2 Data Analysis
9.5.2.1 Preprocessing
An ICA-based interference removal procedure was applied to remove eyeblink and
MCGartifacts.Anofﬂinedigitalnotchﬁlterwiththefrequencyof60,120,and180Hz
2 Informed consent was obtained from the subject before the experiment. Handedness was deter-
mined using the Edinburgh Handedness Inventory [17].

9.5
Source Space PAC Analysis: An Example Study Using Hand-Motor …
205
was applied to remove the power line interference. Continuous MEG recordings were
converted into multiple trials. A single trial had a time period between −6 and +6s
allowing an overlap of about 2s to its adjacent trials. Here, the time origin was deﬁned
as the time of starting the initial rise in the electromyogram waveform. We excluded
trials with the magnetic ﬁeld strength exceeding 2.5pT in any sensor channels.
9.5.2.2 Source Estimation
The watershed algorithm in FreeSurfer [18] was applied to the subject’s MRI to gen-
erate triangulations on the inner skull surface. The dense triangulation on the folded
cortical surface was divided into a grid of 15,000 voxels. The source space includes
both hemispheres. The average distance between voxels is 2mm. The sensor lead
ﬁeld was computed using the boundary element model provided by OpenMEEG [19].
The source estimation was performed using the depth-weighted minimum L2 norm
method implemented in Brainstorm [20]. The anatomical labels were identiﬁed auto-
matically by using the FreeSurfer software with the Desikan–Killiany atlas [21].
Since in our study, we focused only on the contralateral sensorimotor area, we deﬁned
the region of interest as one containing subareas such as precentral, postcentral, para-
central, caudal media frontal areas, and a part of the superior frontal area. The total
number of voxels in the ROI resulted in 1,258. We computed a single time course at
each voxel using the procedure described in Sect.7.2 in Chap.7. Since the estimated
source orientation has a 180◦ambiguity, we use either the estimated orientation or
its opposite orientation that is closer to the direction of the cortical surface provided
by Freesurfer.
9.5.2.3 Modulation Index Analysis
The LF and HF signals are extracted from voxel time courses by applying a ﬁnite
impulse response band-pass ﬁlter. The frequency band of the LF signal ranges from
8.5 to 14.5Hz, which was divided into 11 sub-bands with a bandwidth of 1Hz; each
sub-band half-overlapped with adjacent sub-bands. The frequency band of the HF
signal ranges from 50 to 200Hz; the band is divided into ﬁve half-overlapped sub-
bands with the bandwidth of 50Hz. We computed the modulation index described
in Sect.9.4.3 using all the combinations of the LF and HF sub-bands. The number
of the phase bins, q, was set to 12.
Whencomputingthetimecourseofthemodulationindex,wedivideda12-second-
long interval of trial data into 21 time windows in which the window width is 2s
with a 1.5s overlap. The modulation index MI was computed using all 21 sets of
the time-windowed data to obtain the time course, MI(t1), . . . , MI(t21), where
t1, . . . , t21 indicates the midpoints of the 21 time windows. We also computed the
maximum of the modulation index, such that
MP = max (MI(tk))
(9.6)

206
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
This MP is used as a measure of the strength of the PAC. The surrogate-data method,
similar to the one described in Sect.7.8, is applied to assess the statistical signiﬁcance
of MP.
9.5.3 Results of Local PAC Analysis
9.5.3.1 Time Course Estimation and Spatial Mapping
of the Modulation Index
We ﬁrst analyzed local PAC (lPAC), and computed the modulation index time courses
for all the sub-bands of the LF signal at all voxel locations. The bandwidth of the
HF signal was ﬁxed to 75 ± 25Hz. The resultant time courses were classiﬁed using
K-means clustering analysis, which thereby found three types of clusters. The ﬁrst
kind of the cluster, called type I, consists of the time courses that peaked before the
onset of the hand movements. The time course averaged across all the type I time
courses is shown in Fig.9.2a. We computed MP in Eq.(9.6) using the type I time
courses, and assessed the statistical signiﬁcance of MP at all voxel locations. Voxels
containing statistically signiﬁcant lPAC activity were found with the LF signal of
12.5 ± 0.5Hz near the primary motor area. These voxels are shown in Fig.9.2b.
The second kind of the modulation index time courses, called type II, peaked
immediately before or during the hand movements. The mean time course of this
type is shown in Fig.9.2c. Voxels containing statistically signiﬁcant MP (computed
using the type II time courses) were found with the LF signal of 10.5 ± 0.5Hz near
the lateral part of the precentral area, as shown in Fig.9.2d. The third kind of time
courses, type III, peaked after the hand movements. The mean time course is shown
in Fig.9.2e. Voxels containing signiﬁcant MP (computed using the type III time
courses) were found with the LF signal of 12.5 ± 0.5Hz near the postcentral or
central sulcus areas. These voxels are shown in Fig.9.2f.
In summary, we found signiﬁcant lPAC activities near the contralateral precentral
area before the execution. However, following the execution, lPAC activities were
found near the postcentral area and central sulcus. These ﬁndings may indicate that
a certain relationship exists between these three types of lPAC activities, and the
preparation and rebound of the motor and sensory systems.
9.5.3.2 Amplitude–Phase Diagram and Phase-Informed TF Map
An amplitude–phase diagram was derived using a voxel time course in the area
labeled as Region A in Fig.9.3a. Note that Region A shows signiﬁcant lPAC activity
showing the type I time course. In this derivation, the LF signal was set to 12.5 ±
0.5Hz, and the HF signal to 75 ± 25Hz. The data in the time window from −4.5
to −2.5s (when the type-I lPAC was nearly maximum) was used. The resultant
amplitude–phase diagram is shown in Fig.9.3b in which each bar indicates the mean

9.5
Source Space PAC Analysis: An Example Study Using Hand-Motor …
207
(a)
-5
-4
-3
-2
-1
0
1
2
3
4
5
-5
-4
-3
-2
-1
0
1
2
3
4
5
-5
-4
-3
-2
-1
0
1
2
3
4
5
0.2
0.3
0.4
0.5
Time (s)
*10^-4
0.2
0.3
0.4
0.5
*10^-4
0.2
0.3
0.4
0.5
*10^-4
Anterior
Central sulcus
Central sulcus
MI
IM
IM
Time (s)
Time (s)
(c)
(e)
(b)
(d)
(f)
Fig.9.2 a Modulationindextime course ofthe type-IlPAC activity. bVoxelscontainingstatistically
signiﬁcant lPAC activity having the type I time course. c Modulation index time course of the
type II lPAC activity. d Voxels containing statistically signiﬁcant lPAC activity having the type II
time course. e Modulation index time course of the type III lPAC activity. f Voxels containing
statistically signiﬁcant lPAC activity having the type III time course. The signiﬁcant activities in
(b) and (f) were found when the LF signal of 12.5 ± 0.5Hz and the HF signal of 75 ± 25Hz were
used. The signiﬁcant activities in (d) were found when the LF signal of 10.5 ± 0.5Hz and the HF
signal of 75 ± 25Hz were used. In (a), (c), and (e), the average time course is shown and the error
bar indicates the range of ±standard deviation. Here, the time origin is equal to the onset of the
hand movement. In (b), (d), and (f), the blank circles are used to indicate the locations of the voxels
with signiﬁcant activity

208
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
-100
-50
0
50
100
150
30
50
100
150
200
Time (ms)
-150
Frequency (Hz)
(c)
(b)
Anterior
Lateral
Central sulcus
Region A
(a)
 Amplitude of HF signal
strong
-90
-180
0
90
180
Phase (deg.)
 Amplitude of HF signal
Fig. 9.3 Results of local PAC (lPAC) analysis. a Location of Region A where a signiﬁcant type I
lPAC activity was observed. The lPAC activity at Region A was analyzed. b Amplitude–phase
diagram. Each bar indicates the mean amplitude value at that phase bin. The error bar indicates
the ±standard deviation. In this analysis, the LF signal was set to 12.5 ± 0.5Hz, and the HF signal
was set to 75 ± 25Hz. The data in the time window from −4.5 to −2.5s was analyzed. c The
phase-informed TF map. The amplitude of the HF signal is color-coded and displayed according to
the color bar. The sinusoidal plot shows cos(θL(t)) where θL(t) is the instantaneous phase of the
LF signal. Only the portion between ±150 ms from the LF peaks in the time window is displayed
amplitude value Ψ j at that phase bin. It can be seen that the distribution of Ψ j
signiﬁcantly deviates from the uniform distribution, indicating that a strong lPAC
activity occurs. The preferred phase of the HF signal can be observed. That is, the
amplitude of the HF signal increases signiﬁcantly at a speciﬁc phase of the LF signal
around −150◦, and decreases around +60◦.

9.5
Source Space PAC Analysis: An Example Study Using Hand-Motor …
209
The relationship between the HF signal amplitude and the LF signal phase can be
seen more clearly in the phase-informed TF map in Fig.9.3c. Here, the same voxel
data from Region A has been used. The phase-informed TF map intuitively shows the
preference of the HF signal amplitude to a speciﬁc phase of the LF signal. The map
clearly indicates that the amplitude of the HF signal from 60 to 100Hz signiﬁcantly
increases near −150◦of the phase of the LF signal.
9.5.3.3 LPAC Temporal Trend and Comparison to Event-Related
Power Changes
Event-related power changes such as desynchronization (ERD) and synchronization
(ERS) are representative aspects of brain activities. Comparison between PAC tem-
poral trends and ERD/ERS should provide useful insights into brain information
processing. Using the lPAC signal from Region A, we computed modulation index
time courses with all 11 sub-bands of the LF signal. The HF signal was ﬁxed to the
frequency band of 75 ± 25Hz. The results are shown in Fig.9.4a. In this ﬁgure, the
sizes of the circles represent the strength of the modulation index, and their colors
represent the LF sub-bands.
The results in Fig.9.4a show that LF signals at two sub-bands, 10 ± 0.5 and
12.5 ± 0.5Hz, have signiﬁcantly large modulation index values. The lPAC with LF
signal of 12.5 ± 0.5Hz reaches its maximum between −5 and −3s but the lPAC
with the LF signal of 10 ± 0.5Hz reaches its maximum between −0.5 and +0.5s.
This observation suggests the presence of multiple brain rhythm scales underlying
motor planning and execution.
A time-frequency representation of the broadband MEG signal from Region A is
showninFig.9.4b.Inthisﬁgure,wecanobservethattheERDwhosecenterfrequency
is approximately equal to 12.5Hz starts near −3s. On the other hand, Fig.9.4a shows
that lPAC with LF of 12.5 ± 0.5Hz starts decreasing around −3s. Comparison
betweenFig.9.4a,bshowsthatastrong12.5HzERSstartsat+3.5satwhichthelPAC
with the LF signal of 12.5±0.5Hz restarts. On the other hand, a stable lPAC activity
with the LF band between 9 and 11Hz (marked by the dotted circle in Fig.9.4a) is
observedatthehand-movementperiodwhenasigniﬁcantlystrongERDisobservedat
the same frequency band near 10Hz. Such coincidence between ERD/ERS activities
and lPAC time courses suggests that lPAC and ERD/ERS activities are interrelated.
9.5.4 Results of Analyzing Cross-Location PACs
Finally, the cross-location PAC (xPAC) was analyzed using the same hand-motor
MEG data. In this analysis, the LF signal is measured in voxels of the precentral
area; these voxels are labeled Voxel B in Fig.9.5a. The HF signal is measured at the
voxel near the primary motor area; the voxel is labeled Voxel A in Fig.9.5a. We ﬁrst

210
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
Time (s)
13
9
30
150
200
Frequency (Hz)
100
50
2
−5
−4
−3
−2
−1
0
1
2
3
4
5
(b) 
−5
−4
−3
−2
−1
0
1
2
3
4
5
9
10
11
12
13
14
Time (s)
Frequency of LF signal (Hz)
(a)
MI  value
0.14
0.12
0.10
0.08
0.06
0.04
*10 -3
100
-100
Relative change[%]
0
Fig. 9.4 a Modulation index (MI) time courses for the 11 sub-bands of the LF signal. The lPAC
activity at Region A was analyzed, and the HF signal is ﬁxed to 75 ± 25Hz. Sizes of the circles
represent the values of the modulation index, and their colors represent the LF sub-bands. A dotted
circle indicates an lPAC activity with the LF band near 10Hz observed when a signiﬁcantly strong
ERD is observed near 10Hz. b The time-frequency representation of the (broadband) voxel time
courses at Region A. The TF representation shows relative power change from the baseline, which
is color-coded and displayed according the color bar. The time origin is deﬁned as the onset of
hand grasping

9.5
Source Space PAC Analysis: An Example Study Using Hand-Motor …
211
-100
-50
0
50
100
150
30
50
100
150
200
Time  (ms)
-150
Frequency (Hz)
-9
0
8
1
0
9
0
0
8
1
-
0
HF:100 Hz
HF:75 Hz
(b)
HF:150 Hz
Anterior
Lateral
Voxel A
Voxel B
Central sulcus
(a)
(c)
Phase (deg.)
 Amplitude of HF signal
strong
Fig. 9.5 Results of cross-location PAC (xPAC) analysis. a Voxel locations for measuring the LF
and HF signals. The LF signal is measured at Voxel B and the HF signal is measured at Voxel A.
b The amplitude–phase diagram for three HF sub-bands: 75 ± 25Hz (top panel), 100 ± 25Hz
(middle panel), and 150 ± 25Hz (bottom panel). Each bar indicates the mean amplitude value at
that phase bin. The error bar indicates the ±standard deviation. The data in the time window from
−4.5 to −2.5s was analyzed and the frequency of the LF signal was ﬁxed at 12.5 ± 0.5Hz. c The
phase-informed TF map. The amplitude of the HF signal is color-coded and displayed according to
the color bar. The sinusoidal plot shows cos(θL(t)) where θL(t) is the instantaneous phase of the
LF signal. Only the portion between ±150 ms from the LF peaks in the time window is displayed
computed the amplitude–phase diagrams using three sub-bands of the HF signal:
75 ± 25, 100 ± 25, and 150 ± 25Hz. The data in the time window of −4 ± 1s was
used, and the frequency band of the LF signal was ﬁxed at 12.5±0.5Hz. The results
are shown in Fig.9.5b.

212
9
Detection of Phase–Amplitude Coupling in MEG Source Space …
In this ﬁgure, signiﬁcant increases in HF signal amplitudes were observed at
speciﬁc phases of the LF signal. For the HF band of 75 ± 25Hz, the amplitude of
the HF signal increases signiﬁcantly near −105◦of the LF phase. For the HF band
of the 100 ± 25Hz, the amplitude of the HF signal increases signiﬁcantly near 135◦
of the LF phase. For the HF band of the 150 ± 25Hz, the amplitude of the HF signal
reaches its maximum near 15◦of the LF phase.
Figure9.5c shows the phase-informed TF map in which three HF components are
observed near 75,100, and 150Hz. These HF components have different preferred
phases for the LF signal, indicating that the three HF components have different
timing dependencies on the same LF signal. Such LF phase dependence of the HF
signal amplitude across locations suggests the possibility of dynamic multiplexing
neuronal codes that exploit timing differences across regions due to synaptic or
propagation delays.
9.6 Summary
We have shown that the phase–amplitude coupling (PAC), both local PAC and cross-
location PAC, can successfully be detected in MEG source space analysis. PAC is
considered to reﬂect temporal coding of a brain. The measurement of PAC could
therefore be a promising tool for monitoring information processing in the human
brain. Speciﬁcally, PAC analysis can provide coupling-related information, including
the frequency of brain rhythms, and the speciﬁc phases of LF components, which are
preferred by the HF components. These preferred phases are considered to reﬂect the
timing of information exchange. We believe that exploring xPAC is one of the most
promising approaches to reveal the mechanisms of brain information processing. Our
studies suggest that MEG source space analysis could be a powerful tool in xPAC
studies, because of MEG’s wide coverage of the brain and its noninvasiveness.
References
1. E. Privman, R. Malach, Y. Yeshurun, Modeling the electrical ﬁeld created by mass neural
activity. Neural Netw. 40, 44–51 (2013)
2. A.K. Engel, P. Roelfsema, P. Fries, M. Brecht, W. Singer, Role of the temporal domain for
response selection and perceptual binding. Cereb. Cortex 7(6), 571–582 (1997)
3. A.K. Engel, P. Fries, P. König, M. Brecht, W. Singer, Temporal binding, binocular rivalry, and
consciousness. Conscious. Cogn. 8(2), 128–151 (1999)
4. P. Lakatos, G. Karmos, A.D. Mehta, I. Ulbert, C.E. Schroeder, Entrainment of neuronal oscil-
lations as a mechanism of attentional selection. Science 320(5872), 110–113 (2008)
5. S. Panzeri, N. Brunel, N.K. Logothetis, C. Kayser, Sensory neural codes using multiplexed
temporal scales. Trends Neurosci. 33(3), 111–120 (2010)
6. T. Akam, D.M. Kullmann, Oscillatory multiplexing of population codes for selective commu-
nication in the mammalian brain. Nat. Rev. Neurosci. 15(2), 111–122 (2014)
7. G. Buzsaki, Rhythms of the Brain (Oxford University Press, New York, 2006)

References
213
8. O. Jensen, B. Gips, T.O. Bergmann, M. Bonnefond, Temporal coding organized by coupled
alpha and gamma oscillations prioritize visual processing. Trends Neurosci. 37(7), 357–369
(2014)
9. E.M. Zion Golumbic, N. Ding, S. Bickel, P. Lakatos, C.A. Schevon, G.M. McKhann, R.R.
Goodman, R. Emerson, A.D. Mehta, J.Z. Simon et al., Mechanisms underlying selective neu-
ronal tracking of attended speech at a cocktail party. Neuron 77(5), 980–991 (2013)
10. C. Kayser, M.A. Montemurro, N.K. Logothetis, S. Panzeri, Spike-phase coding boosts and
stabilizes information carried by spatial and temporal spike patterns. Neuron 61(4), 597–608
(2009)
11. R.T. Canolty, E. Edwards, S.S. Dalal, M. Soltani, S.S. Nagarajan, H.E. Kirsch, M.S. Berger,
N.M. Barbaro, R.T. Knight, High gamma power is phase-locked to theta oscillations in human
neocortex. Science 313(5793), 1626–1628 (2006)
12. R.vanderMeij,M.Kahana,E.Maris,Phase-amplitudecouplinginhumanelectrocorticography
is spatially distributed and phase diverse. J. Neurosci. 32(1), 111–123 (2012)
13. T. Staudigl, T. Zaehle, J. Voges, S. Hanslmayr, C. Esslinger, H. Hinrichs, F.C. Schmitt, H.-J.
Heinze, A. Richardson-Klavehn, Memory signals from the thalamus: early thalamocortical
phase synchronization entrains gamma oscillations during long-term memory retrieval. Neu-
ropsychologia 50(14), 3519–3527 (2012)
14. T.H. FitzGerald, A. Valentin, R. Selway, M.P. Richardson, Cross-frequency coupling within
and between the human thalamus and neocortex. Front. Hum. Neurosci. 7, 84 (2013)
15. A.B. Tort, R. Komorowski, H. Eichenbaum, N. Kopell, Measuring phase-amplitude coupling
between neuronal oscillations of different frequencies. J. Neurophysiol. 104(2), 1195–1210
(2010)
16. T. Yanagisawa, O. Yamashita, M. Hirata, H. Kishima, Y. Saitoh, T. Goto, T. Yoshimine, Y.
Kamitani, Regulation of motor representation by phase-amplitude coupling in the sensorimotor
cortex. J. Neurosci. 32(44), 15467–15475 (2012)
17. R.C. Oldﬁeld, The assessment and analysis of handedness: the Edinburgh inventory. Neuropsy-
chologia 9(1), 97–113 (1971)
18. B. Fischl, Freesurfer. NeuroImage 62(2), 774–781 (2012)
19. A. Gramfort, T. Papadopoulo, E. Olivi, M. Clerc et al., OpenMEEG: opensource software for
quasistatic bioelectromagnetics. Biomed. Eng. Online 9(1), 45 (2010)
20. F. Tadel, S. Baillet, J.C. Mosher, D. Pantazis, R.M. Leahy, Brainstorm: a user-friendly appli-
cation for MEG/EEG analysis. Comput. Intell. Neurosci. 2011, 8 (2011)
21. R.S. Desikan, F. Ségonne, B. Fischl, B.T. Quinn, B.C. Dickerson, D. Blacker, R.L. Buckner,
A.M. Dale, R.P. Maguire, B.T. Hyman et al., An automated labeling system for subdividing the
human cerebral cortex on MRI scans into gyral based regions of interest. NeuroImage 31(3),
968–980 (2006)

Appendix A
Bioelectromagnetic Forward Modeling
A.1 Neuronal Basis of MEG and EEG Signals
Neurons in the brain function electrically, as well as chemically, and have associated
electric and magnetic ﬁelds which can be detected outside the head. We ﬁrst discuss
the anatomical and physiological properties of neurons in the mammalian brain, that
contribute to the electric and magnetic ﬁelds detected by MEG and EEG. Neuronal
currents are considered at a level sufﬁcient for understanding the primary source of
these ﬁelds.
A.1.1 Neurons and Synapses
Neurons are cells that are highly specialized for signal processing and conduction
via electrochemical and electrical processes. The morphological structure of a neu-
ron includes a cell body, called the soma, and elaborate branching structures called
dendrites and axons that enable communication with sensory receptors, distant neu-
rons, etc. Inputs to a neuron are collected in a continuous fashion by the dendrites,
and represented as a spatially and temporally continuous variation of the transmem-
brane voltage. Multiple inputs are summed in the dendritic tree, and the net input is
often represented as transmembrane voltage at the soma. When the somatic voltage
reaches some threshold, a discrete voltage pulse is generated, called an action poten-
tial, which propagates down the axon as a discrete output. The end of the axon has
elaborate branching to enable communication with target neurons.
Neuronal communication occurs for the most part via chemical transmission
through synapses, although a small minority of neurons also communicate elec-
trically through gap junctions. When an action potential reaches the presynaptic
terminal of a synapse with another neuron, it causes the release of neurotransmitter
into the synaptic cleft between two cells. The released neurotransmitter binds with
some probability to sites of the postsynaptic terminal, stimulating a ﬂow of current
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9
215

216
Appendix A: Bioelectromagnetic Forward Modeling
across the membrane of the postsynaptic neuron. Postsynaptic currents may ﬂow
inward or outward, depending upon the type of neurotransmitter involved (gluta-
mate or GABA), and the type of ion that ﬂows in response. Excitatory synapses
have the effect of increasing the membrane potential of the postsynaptic cell, i.e.,
making it more positive, while inhibitory synaptic connections decrease it. Neurons
are exclusively inhibitory or excitatory.
A.1.2 Cortical Anatomy
The mammalian cerebral cortex is the outer mantle of cells surrounding the central
structures, e.g., brainstem and thalamus. It is unique to mammals, and is believed
to be necessary for most higher-level brain functions. Topologically the cerebral
cortex is comprised of two spherical shells, corresponding to the two hemispheres
and connected by the corpus callosum. Cortical thickness varies between 2 and 3
mm in the human, and is folded around the subcortical structures so as to appear
wrinkled. Its average surface area is about 3000 cm2. It is estimated that there are
roughly 1011 neurons in the human brain, and 1010 of these in the cortex.
The diversity of structures of neurons is extraordinary. Approximately 85 % are
pyramidal cells whose dendritic trees have a distinctive, elongated geometry that
makes possible the generation of extracellular ﬁelds at large distances. The remaining
15 % may be broadly classiﬁed as stellate cells, whose dendritic trees are approxi-
mately spherical, and make little or no contribution to distant ﬁeld. Both cell types
are interconnected and together form a single dynamical network, but it is believed
that the ﬁelds at large distances are dominated by pyramidal cells because of their
size and number.
Synaptic connections in the cortex are dense. Each cortical neuron receives 104–
105 synaptic connections, with most inputs coming from distinct neurons. Pyrami-
dal cells make excitatory connections to both cell types. They make intracortical
connections over lengths ranging 0.5–3 mm, and cortico-cortical connections over
lengths ranging1–20cm. Stellatecells makeinhibitoryconnections tobothcell types.
They make intracortical connections over lengths ranging only 0.02–0.03 mm, much
shorter than pyramidal cells. Thus connections in the cerebral cortex are said to
exhibit long-range excitation and short-range inhibition.
An interesting distinction exists between intracortical and cortico-cortical con-
nections. Intracortical connections are made locally between neighboring neurons,
such that the probability of a connection between two neurons falls off smoothly
as a function of distance. In contrast, corticocortical connections are made via sub-
cortical white-matter ﬁbers, and behave non-locally in the sense that connection to
a distant neuron does not imply connections with intermediate neurons. Because
of the diversity of scales of these synaptic connections, and the nonlocal nature of
the cortico-cortical connections, the cerebral cortex exhibits rich spatio-temporal
dynamics spanning a wide range of length and time scales.

Appendix A: Bioelectromagnetic Forward Modeling
217
A.1.3 Neuronal Currents
Electric currents in biological tissue are primarily due to ions, e.g., K+, Na+, Cl−,
Ca2+, etc. These ions ﬂow in response to the local electric ﬁeld, according to Ohm’s
law, but also in response to their local concentration gradient, according to Fick’s
law [1]. In the resting state of the membrane, the concentration gradients and electric
ﬁeld are due to ion channel pumps, which use energy acquired from ATP to move
ions across the membrane against their diffusion gradient.
To a good approximation, the concentration of each ion inside and outside the
membrane may be assumed constant in time. The transmembrane voltage, however,
changes radically in time, the strongest example being the action potential. Thus for
the purposes of discussing neural activity, we take the transmembrane potential Vm
as the primary dynamic state variable to be considered. By convention, Vm is deﬁned
as the potential inside relative to that outside, i.e., Vm = Vi −Vo. The current ﬂowing
across the membrane maybe be viewed as a function of Vm, and therefore as the basis
of the extracellular ﬁelds detected by MEG and EEG.
In the resting state of the neuron, the balance between electric and diffusive ﬂows
determine the resting membrane potential of the neuron [2]. To consider this balance,
we deﬁne two related quantities: the charge current density, J [C/m2 s], and the ionic
ﬂux j [mol/m2 s]. For a single ionic species, these are trivially related by: J = zFj,
where z = q/e is the signed integer number of charges carried by an ion, and F is the
Faraday’s constant (96,500 C/mol). The ﬂux j has two contributions, arising from
the local concentration gradient ∇C and the local electric ﬁeld E, where C indicates
the ionic concentration for different ions deﬁned as the number of ions (in mol) per
unit volume. Fick’s law states that ions diffuse down their concentration gradient
according to the linear relation j = −D∇C, where D is the diffusion coefﬁcient.
Furthermore, an ion within an applied electric ﬁeld E accelerates initially and
reaches a terminal velocity v given by v = μ(z/|z|)E where the factor (z/|z|) accounts
for the fact that negative ions travel in the opposite direction of the electric ﬁeld. The
quantity μ is called the mobility and the diffusion coefﬁcient, D =
RT
|z|F μ, where
R = 8.314 J/(mol K) is the ideal gas constant. Counting ﬂow of particles through a
parallelepiped, the ionic ﬂux is μC z
|z|E. This equation is related to the more common
way of expressing the ﬂow of charges in an electric ﬁeld via Ohm’s law, which in a
volume conductor is written as J = σE. Therefore, the total ionic ﬂux is the linear
sum of its diffusive and electrical contributions:
j = jS + jE = −μ
|z|
RT
F ∇C + zC∇Φ

,
(A.1)
where the electric potential Φ is deﬁned as E = −∇Φ. This equation can be used
to derive the Nernst equation, and the Goldman-Hodgkin-Katz equation [3], for the
resting potential of a neuron, and also form the basis for modeling synaptic activity.

218
Appendix A: Bioelectromagnetic Forward Modeling
A.1.4 Neuronal Populations
Neuronal activity gives rise to extracellular electric and magnetic ﬁelds, which are
detectedinMEGandEEG.Theﬁeldsgeneratedbyasingleneuronaremuchtoosmall
to be detected at the scalp, but the ﬁelds generated by synchronously active neurons,
with advantageous geometric alignment, can be detected. Dendrites are more able
than axons to generate ﬁelds which are detectable at large distances. Stellate cells
have approximately spherical dendritic trees, so the resulting extracellular ﬁelds tend
to add with all possible orientations, and effectively cancel at distance.
Pyramidal cells have similar dendritic trees, but the tree branches are connected
to the cell body (soma) by a long trunk, called the apical dendrite. It is a fortuitous
anatomical feature of the cortex that pyramidal cells have their apical dendrites
aligned systematically along the local normal to the cortical surface. In this way,
the ﬁelds of synchronously active pyramidal neurons superimpose geometrically
to be measurable at the scalp. Consider an approximately 1 cm3 region of cortex,
containing an order of 107 aligned pyramidal cells. If only 1 % of these neurons were
synchronously active, then the relative contribution of synchronous to asynchronous
neurons would be 105/
√
107 ∼30. Thus, scalp MEG and EEG are largely dominated
by synchronous neural activity.
A.2 Electromagnetic Fields in Conductive Media
A.2.1 Maxwell’s Equations
Outside the scalp, we measure the net ﬁelds produced by synchronously active neu-
rons. For calculations at this scale, the brain and other head tissues can be considered
bulk materials, characterized by properties such as electric conductivity σ. The chal-
lenge is to compute magnetic ﬁelds and scalp potentials as a function of neuronal
source currents in a head-shaped conductive medium. We begin here with the fun-
damentals of electric and magnetic ﬁelds in matter, and later specialize to biological
tissue.
The physics of electric and magnetic ﬁelds are summarized by Maxwell’s equa-
tions. In matter the macroscopic ﬁelds obey
∇· E = ρ
ϵ ,
(A.2)
∇· B = 0,
(A.3)
∇× E = −∂B
∂t ,
(A.4)
∇× B = μJ + μϵ ∂E
∂t .
(A.5)

Appendix A: Bioelectromagnetic Forward Modeling
219
where E is the electric ﬁeld, B the magnetic ﬁeld, ϵ the dielectric permittivity, and μ
the magnetic permeability. The Maxwell’s equations above have source terms given
by the charge density ρ and the current density J. Additional contributions arise from
the time derivatives of the ﬁelds.
According to Eq. (A.1), the electric current density J is expressed as
J = JS + JE = JS + σE,
(A.6)
where JE is the “ohmic” current which ﬂows in response to the local electric ﬁeld, and
JS caused by the ionic ﬂux jS is the “source” current (or often called the “impressed”
current), which ﬂows in response to transmembrane concentration gradients whose
sum corresponds to the net currents summed over all ions. In Eq. (A.6), σ is the bulk
conductivity of the material, which is the main parameter governing the spread of
ionic currents through the volume. Variations in conductivity affect both EEG and
MEG spatial structure, although MEG is less sensitive to this parameter than EEG.
Charge conservation law is derived from Maxwell’s equations in the following
manner. Taking the divergence of Eq. (A.5) and using Eq. (A.2), we obtain
∇· J + ϵ ∂∇· E
∂t
= ∇· J + ∂ρ
∂t = 0,
(A.7)
where the relationship ∇· (∇× B) = 0 is used. Integrating over a closed volume V
bounded by a surface S and using the Gauss theorem yield:

S
J · ˆn dS = −∂
∂t

V
ρ dV,
(A.8)
where ˆn is the outward unit normal to S. The integral on the left is the total current
ﬂowing outward across the S, and the integral on the right is the total charge in
volume, so this relation states that the current ﬂowing outward across S is equal to
the rate of change of the charge in the volume.
A.2.1.1 Potential Formulation
It is convenient to re-express Maxwell’s equations in terms of potential functions,
which are related to the ﬁelds by simple derivatives. Because ∇·B = 0, it is possible
to write B as a gradient of some other vector ﬁeld A, such that
B = ∇× A,
(A.9)
where A is called the magnetic vector potential. Substituting the equation above into
Eq. (A.4), we obtain

220
Appendix A: Bioelectromagnetic Forward Modeling
∇× E + ∂
∂t ∇× A = ∇×

E + ∂A
∂t

= 0.
(A.10)
Using a scalar potential Φ, we can write
E + ∂A
∂t = −∇Φ,
(A.11)
because for any scalar ﬁeld Φ, the identity ∇× (∇Φ) = 0 holds. Thus, we have
E = −∇Φ −∂A
∂t .
(A.12)
A.2.1.2 Gauge Transformations
Since electric and magnetic ﬁelds are deﬁned as derivatives of the potentials, the
potentials are not uniquely speciﬁed by their respective electromagnetic ﬁelds. This
ﬂexibility in deﬁning A and Φ is referred to as the choice of gauge. Considering again
the relationship ∇× ∇ξ = 0 for any scalar ﬁeld ξ, the vector potential A is only
determined up to the gradient of a scalar function, i.e., we can make the replacement
A →A −∇ξ without changing the value of the magnetic ﬁeld B. This implies that,
E →−∇Φ −∂
∂t (A −∇ξ)
= −∇(Φ −∂ξ
∂t ) −∂A
∂t .
(A.13)
In other words, to keep E from being unchanged, we must also make the replacement
of
Φ →Φ + ∂ξ
∂t .
(A.14)
This freedom to choose ξ allows us to impose an additional relationship between
A and Φ. Common choices include the “Coulomb” gauge where ∇· A = 0 or the
“Lorentz” gauge, where ∇· A + μϵ(∂Φ/∂t) = 0. In the following, we use the
convenient “Gulrajani” gauge [4] for decoupling the differential equations for A
and Φ, thereby reﬂecting the fundamental symmetry between electric potentials and
magnetic ﬁelds. The Gulrajani gauge is expressed as
∇· A + μϵ ∂Φ
∂t + μσΦ = 0.
(A.15)

Appendix A: Bioelectromagnetic Forward Modeling
221
A.2.2 Magnetic Field and Electric Potential in an Unbounded
Conducting Medium
A.2.2.1 Potential Equations
We have shown that using the vector and scalar potentials, the Maxwell’s equations
in Eqs. (A.3) and (A.4) are rewritten, respectively, as
B = ∇× A,
(A.16)
E = −∇Φ −∂A
∂t .
(A.17)
Let us express the other two Maxwell’s equations using these potentials and the
source current JS. Let us substitute the two equations above into Eq. (A.5), resulting
in
∇× (∇× A) = μJ −μϵ∇∂Φ
∂t −μϵ ∂2A
∂t2 .
(A.18)
The relationship in Eq. (A.6) is expressed using the potentials such that
J = JS −σ∇Φ −σ ∂A
∂t .
(A.19)
Substituting Eq. (A.19) into (A.18), and using the identity ∇× (∇× A) =
∇(∇· A) −∇2A, we obtain
∇2A −∇·

∇· A + μϵ ∂Φ
∂t + μσΦ

−μϵ ∂2A
∂t2 −μσ ∂A
∂t = −μJS,
(A.20)
and ﬁnally
∇2A −μϵ ∂2A
∂t2 −μσ ∂A
∂t = −μJS,
(A.21)
where the gauge relationship in Eq. (A.15) is used in Eq. (A.20). From Maxwell’s
equation in Eq. (A.2), a similar derivation leads to
∇2Φ −μϵ ∂2Φ
∂t2 −μσ ∂Φ
∂t = −ρ
ϵ .
(A.22)
Finally, we use Eqs. (A.6), (A.7), and (A.11), to obtain the time derivative of the
charge ρ, expressed as

222
Appendix A: Bioelectromagnetic Forward Modeling
∂ρ
∂t = −∇· J
= −∇· (σE + JS)
= −σ∇·

−∇Φ −∂A
∂t

−∇· JS
= σ∇2Φ + σ ∂∇· A
∂t
−∇· JS
= σ∇2Φ −σ ∂
∂t

μϵ ∂Φ
∂t + μσΦ

−∇· JS
= σ∇2Φ −ϵμσ ∂2Φ
∂t2 −μσ 2 ∂Φ
∂t −∇· JS,
(A.23)
where we again use the gauge relationship in Eq.(A.15).
A.2.2.2 Derivation of Potentials
To facilitate the solution of the equations for potentials, we can exploit linearity and
also assume harmonic time dependence with angular frequency ω. This assump-
tion does not require that the signals have perfect sinusoidal dependence, and is
merely a mathematical convenience that helps subsequent discussion for determin-
ing the quasi-static regime. Namely, we express the solutions of the partial differential
equations in Eqs.(A.21), (A.22), and (A.23), such that
A(t, r) = A(r)eiωt,
(A.24)
(t, r) = Φ(r)eiωt,
(A.25)
ρ(t, r) = ρ(r)eiωt,
(A.26)
JS(t, r) = JS(r)eiωt.
(A.27)
Substituting A(t, r) and JS(t, r) into Eq. (A.21), we obtain the following partial
differential equation of space alone,
∇2A(r) + k2A(r) = −μJS(r),
(A.28)
where, k2 = −iωμ(σ + iωϵ).
Substitution of (t, r) and ρ(t, r) into Eq. (A.22) also results in
∇2Φ(r) + k2Φ(r) = −ρ(r)
ϵ
.
(A.29)
Substitution of (t, r), ρ(t, r), and JS(t, r) into Eq. (A.23) gives the relationship:
iωρ(r) = σ∇2Φ(r) −ϵμσ(iω)2Φ(r) −μσ 2(iω)Φ(r) −∇· JS(r),

Appendix A: Bioelectromagnetic Forward Modeling
223
which is simpliﬁed to
ρ(r) = 1
iωσ∇2Φ(r) −
	
ϵμσ(iω) + μσ 2
Φ(r) −1
iω∇· JS(r).
(A.30)
By substituting Eq. (A.30) into (A.29), we obtain
∇2Φ(r) −iωμ(σ + iωϵ)Φ(r)
= −1
iωϵ σ∇2Φ(r) +

μσ(iω) + μσ 2
ϵ

Φ(r) +
1
iωϵ ∇· JS(r).
The equation above results in
∇2Φ(r) + k2Φ(r) = ∇· JS(r)
σ + iωϵ .
(A.31)
In the absence of boundary conditions, Eqs. (A.28) and (A.31) have the following
well-known solutions.
A(r) = μ
4π

V
JS(r′)
|r −r′|e−ik|r−r′|d3r′,
(A.32)
and
Φ(r) = −
1
4π(σ + iωϵ)

V
∇′ · JS(r′)
|r −r′| e−ik|r−r′|d3r′.
(A.33)
Note that the derivation of these solutions has not involved any assumptions about the
frequency ω or the tissue parameters σ, μ, ϵ. The typical values of these parameters,
together with assumptions about the frequency ω, allow simpliﬁcations of the above
equations suitable for EEG and MEG. The phase shifts of the electric potentials due
to capacitive effects are accounted by the denominator of Eq. (A.33):
(σ + iωϵ) = σ(1 + iωϵ
σ ).
(A.34)
Thus capacitive effects may be ignored if ωϵ/σ ≪1. Although the very highest
frequencies generated by the neural activity corresponding to action potentials may
reach 1000 Hz, MEG and EEG signals are expected to be dominated by synaptic
and dendritic activity that involves slower time scales less than 100Hz (ω < 200 π
rad/s).
We make use of measured biological parameters to evaluate the dimensionless
quantity ωϵ/σ. To a best approximation, biological tissues are not magnetizable,
thus μ = 4π ×10−7 H/m, the value of vacuum. The dielectric permittivity ϵ is tissue
and frequency dependent such that for brain and scalp, ωϵ/σ ≃0.01 at 100 Hz, and
ωϵ/σ ≃0.03 at 1000 Hz. Thus, capacitive effects may be ignored to within 1%
errors.

224
Appendix A: Bioelectromagnetic Forward Modeling
We then apply the Taylor series expansion:
e−ik|r−r′| ≃1 −ik|r −r′| + ...
(A.35)
For MEG and EEG applications, we assume that |r −r′| < 0.1 m, typical for head-
radius and distance from cortical sources to the sensors. With ωϵ/σ ≪1, we have
|k| ≃√ωμϵ. Head tissue conductivity is highly variable, but the nominal value is
σ ≈0.1 s/m. Using this value leads to |k||r−r′| ≈0.0013. Thus, propagation delays
may also be ignored to within a 1% error. With these simpliﬁcations, we can obtain
A(r) = μ0
4π

V
JS(r′)
|r −r′|d3r′,
(A.36)
and
Φ(r) = −
1
4πσ

V
∇′ · JS(r′)
|r −r′| d3r′.
(A.37)
These simpliﬁed solutions are termed the quasi-static solutions, because the depen-
dence on ω has been eliminated. This rigorous derivations often ignored in MEG
and EEG literature, are given here to make explicit assumptions about tissue para-
meters and source frequencies which underly the quasi-static formulations of EEG
and MEG.
A.2.2.3 Computation of Magnetic Field and Electric Potential
From the magnetic vector potential, the magnetic ﬁeld is computed using:
B(r) = ∇× A(r) = μ0
4π

V
∇× JS(r′)
|r −r′|d3r′.
(A.38)
We use the identity:
∇× JS(r′)
|r −r′| =∇
1
|r −r′| × JS(r′) +
1
|r −r′|∇× JS(r′)
=JS(r′) × (r −r′)
|r −r′|3
.
(A.39)
Note that since ∇is applied to r, the relationships ∇|r−r′|−1 = −(r−r′)/|r−r′|−3
and∇×JS(r′) = 0hold.Therefore,themagneticﬁeldinanunboundedhomogeneous
medium is expressed as
B(r) = μ0
4π

V
JS(r′) × (r −r′)
|r −r′|3
d3r′.
(A.40)

Appendix A: Bioelectromagnetic Forward Modeling
225
This equation shows that the magnetic ﬁeld in an unbounded homogeneous medium
is expressed by the famous Biot-Savart law with replacing the total current J with
the source (impressed) current JS. In other words, the ohmic current JE does not
contribute to the magnetic ﬁeld in an unbounded homogeneous medium.
The expression for the scalar potential in Eq. (A.37) can be simpliﬁed in a fol-
lowing manner. Let us use the identity

V
∇′ · JS(r′)
|r −r′|d3r′ =

V
∇′ · JS(r′)
|r −r′| d3r′ +

V
JS(r′) · ∇′
1
|r −r′|d3r′.
(A.41)
The Gauss theorem is expressed as

V
∇′ · JS(r′)
|r −r′|d3r′ =

S
JS(r′)
|r −r′| ·ndS′,
where n is again the outward unit normal to the surface S. If we assume that JS(r′)
becomes zero on S, the surface integral on the right-hand side of the equation above
becomes zero, and we can then get the relationship:

V
∇′ · JS(r′)
|r −r′| d3r′ = −

V
JS(r′) · ∇′
1
|r −r′|d3r′.
(A.42)
Therefore, using
∇′
1
|r −r′| =
r −r′
|r −r′|3 ,
(A.43)
Eq. (A.37) is rewritten as
Φ(r) =
1
4πσ

V
JS(r′) ·
r −r′
|r −r′|3 d3r′.
(A.44)
The above equation is the expression to compute the electric potential in an
unbounded homogeneous medium.
A.2.2.4 Dipoles in an Unbounded Homogeneous Medium
The transmembrane current density JS arises due to concentration gradients, Source
models for JS fall into two categories: rigorous and phenomenological. Although a
rigorous source model accounts reasonably accurately for each of the microscopic
currents, it is difﬁcult to derive such source models. A phenomenological source
model is one which produces the same external ﬁelds, but is artiﬁcial in the sense
that it does not actually reﬂect the microscopic details of the problem.
A representative phenomenological source model is the dipole model. The dipole
is the simplest source for both Φ and B and can be written as:

226
Appendix A: Bioelectromagnetic Forward Modeling
JS = Q(r0) = Qδ(r −r0).
(A.45)
Substituting this for the equations for electric potential in Eq. (A.44) and magnetic
ﬁeld in Eq. (A.40) in a homogenous inﬁnite volume conductor, we get
Φ(r) =
1
4πσ Q ·
r −r0
|r −r0|3 ,
(A.46)
and
B(r) = μ0
4π Q ×
r −r0
|r −r0|3 .
(A.47)
The equations above are the expressions for computing the electric potential and
magnetic ﬁeld produced by an current dipole at r0 embedded in an unbounded homo-
geneous conductive medium.
A.2.3 Magnetic Field from a Bounded Conductor
with Piecewise-Constant Conductivity
We next consider the magnetic ﬁeld generated by an inhomogeneous conductor. We
assume that the region V can be divided into subregions Vj, j = 1, . . . , ϖ, and the
region Vj has conductivity σj. The surface of Vj is denoted Sj. We also assume that
the conductivity, σ(r), is zero outside V. We start the derivation from the Bio-Savart
law:
B(r) = μ0
4π

V
J(r′) ×
r −r′
|r −r′|3 d3r′ = μ0
4π

V
J(r′) × G(r, r′) d3r′,
(A.48)
where
G(r, r′) =
r −r′
|r −r′|3 .
Substituting J(r′) = JS(r′) −σ(r′)∇Φ(r′) into Eq. (A.48), we can obtain
B(r) = μ0
4π

V

JS(r′) −σ(r′)∇Φ(r′)

× G(r, r′) d3r′
= μ0
4π

V
JS(r′) × G(r, r′) d3r′ −μ0
4π
ϖ

j=1
σj

Vj
∇Φ(r′) × G(r, r′) d3r′.
(A.49)
Using
∇Φ(r′) × G(r, r′) = ∇× [Φ(r′)G(r, r′)],
(A.50)

Appendix A: Bioelectromagnetic Forward Modeling
227
the second term on the right-hand side of Eq. (A.49) can be rewritten as

Vj
∇Φ(r′) × G(r, r′) d3r′ =

Vj
∇× [Φ(r′)G(r, r′)] d3r′
=

Sj
n(r′)dS × [Φ(r′)G(r, r′)]
=

Sj
Φ(r′)n(r′) × G(r, r′)dS,
(A.51)
where Sj indicates the surface of Vj, and n(r′) is the outward unit normal vector of
a surface element on Sj. We use the Gauss theorem to derive the right-hand side
of Eq. (A.51). Substituting Eq. (A.51) into (A.49), we can derive the following
Geselowitz formula [5]:
B(r) = B0(r) −μ0
4π
ϖ

j=1
(σj −σ ′
j )

Sj
Φ(r′)n(r′) × G(r, r′)dS,
(A.52)
where σ ′
j is the conductivity just outside Vj. Also, in Eq. (A.52), B0(r) is the magnetic
ﬁeld for the inﬁnite homogeneous conductor in Eq. (A.40).
A.2.4 Magnetic Field from a Homogeneous Spherical
Conductor
Here, we assume that Vj is spherically symmetric, and we set the coordinate origin
at the center of Vj. We have
B(r) · er = B0(r) · er −μ0
4π
ϖ

j=1
(σj −σ ′
j )

Sj
Φ(r′)n(r′) × G(r, r′) · erdS, (A.53)
where er is the unit vector in the radial direction, which is deﬁned as er = r/|r|.
Sincen(r) = er in the case of a spherically symmetric conductor, the second term is
equal to zero, and we have the relationship
B(r) · er = B0(r) · er.
(A.54)
This equation indicates that the radial component of the magnetic ﬁeld is not affected
by the volume current and that the radial component is determined solely by the
primary current.
We next derive a closed-form formula for the magnetic ﬁeld outside a spherically-
symmetric homogeneous conductor. The derivation is according to Sarvas [6]. The

228
Appendix A: Bioelectromagnetic Forward Modeling
relationship ∇× B(r) = 0 holds outside the volume conductor, because there is no
electric current. Thus, B(r) can be expressed in terms of the magnetic scalar potential
U(r), as
B(r) = −μ0∇U(r).
(A.55)
This potential function is derived from
U(r) = 1
μ0
 ∞
0
B(r + τer) · erdτ = 1
μ0
 ∞
0
B0(r + τer) · erdτ,
(A.56)
where we use the relationship in Eq. (A.54). Assuming that a dipole source exists at
r0, by substituting Eq. (A.47) into Eq. (A.56) and performing the integral, we obtain
U(r) = 1
μ0
 ∞
0
B0(r + τer) · erdτ
= 1
4π Q × (r −r0) · er
 ∞
0
dτ
|r + τer −r0|3
= −1
4π
(Q × r0) · r

,
(A.57)
where
 = |r −r0|(|r −r0|r + |r|2 −r0 · r).
(A.58)
The well-known Sarvas formula [6] for B(r) is then obtained by substituting
Eq. (A.57) into Eq. (A.55) and performing the gradient operation. The results are
expressed as
B(r) = μ0
4π ∇(Q × r0) · r

= μ0
4π
Q × r0

−1
2 (Q × r0) · r∇

=
μ0
4π2

Q × r0 −[(Q × r0) · r]∇

,
(A.59)
where
∇ =
|r −r0|2
|r|
+ (r −r0) · r
|r −r0|
+ 2|r −r0| + 2|r|

r
−

|r −r0| + 2|r| + (r −r0) · r
|r −r0|

r0.
(A.60)
We can see that when r0 approaches the center of the sphere, B(r) becomes zero,
and no magnetic ﬁeld is generated outside the conductor from a source at the origin.
Also, if the source vector Q and the location vector r0 are parallel, i.e., if the primary
current source is oriented in the radial direction, no magnetic ﬁelds are generated
outside the spherical conductor from such a radial source. This is because the two

Appendix A: Bioelectromagnetic Forward Modeling
229
Fig. A.1 The three
orthogonal directions
(er, eφ, eθ) used to express
the source vector when the
spherically-symmetric
conductor model is used for
the forward calculation
x
y
z
er
eϕ
eθ
θ
ϕ
terms on the left-hand side of Eq. (A.59) contain the vector product Q × r0, which
is equal to zero when Q and r0 are parallel.
Therefore, when using the spherically-homogeneous conductor model, instead
of the x, y, z directions, we usually use the three orthogonal directions (er, eφ, eθ)
to express the source vector. These directions are illustrated in Fig. A.1. Because
the er component of a source never creates a measurable magnetic ﬁeld outside the
spherical conductor, we can disregard this component and only deal with the eφ and
eθ components of the source vector.
References
1. R. Plonsey, Bioelectric phenomena. Wiley Online Library, 1969.
2. H. Tuckwell, “Introduction to Theoretical Neurobiology vols. 1 and 2, 1988,”
Cambridge University Press, Cambridge, Longtin A., Bulsara A., Moss F., Phys.
Rev. Lett, vol. 65, p. 656, 1991.
3. B. Hille, Ion channels of excitable membranes, vol. 507. Sinauer Sunderland,
MA, 2001.
4. R. M. Gulrajani, Bioelectricity and biomagnetism. John Wiley and Sons, 1998.
5. D. B. Geselowitz, “On the magnetic ﬁeld generated outside an inhomogeneous
volume conductor by internal current sources,” IEEE Trans. Biomed. Eng., vol. 2,
pp. 346–347, 1970.
6. J. Sarvas, “Basic mathematical and electromagnetic concepts of the biomagnetic
inverse problem,” Phys. Med. Biol., vol. 32, pp. 11–22, 1987.

Appendix B
Basics of Bayesian Inference
B.1 Linear Model and Bayesian Inference
This appendix explains the basics of Bayesian inference. Here, we consider the gen-
eral problem of estimating the vector x, containing N unknowns: x = [x1, . . . , xN]T,
from the observation (the sensor data) y, containing M elements: y = [y1, . . . , yM]T.
We assume a linear model between the observation y and the unknown vector x, such
that
y = Hx + ε,
(B.1)
where H is an M × N matrix that expresses the forward relationship between y and
x, and ε is an additive noise overlapped to the observation y.
To solve this estimation problem based on the Bayesian inference, we consider x a
vector random variable, and use the following three kinds of probability distributions.
(1) p(x): The probability distribution on the unknown x. This is called the prior
probability distribution. It represents our prior knowledge on the unknown x.
(2) p(y|x): The conditional probability of y given x. This conditional probability is
equal to the likelihood. The maximum likelihood method, mentioned in Chap. 2,
estimates the unknown x as the value of x that maximizes p(y|x).
(3) p(x|y): The probability of x given observation y. This is called the posterior
probability. The Bayesian inference estimates the unknown parameter x based
on this posterior probability.
The posterior probability is obtained from the prior probability p(x) and the like-
lihood p(y|x) using Bayes’ rule,
p(x|y) =
p(y|x)p(x)

p(y|x)p(x)dx.
(B.2)
Ontheright-handsideofEq.(B.2),thedenominatorisusedonlyforthenormalization

p(x|y)dx = 1, and often the denominator is not needed to estimate the posterior
p(x|y). Therefore, Bayes’ rule can be expressed in a simpler form such as
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9
231

232
Appendix B: Basics of Bayesian Inference
p(x|y) ∝p(y|x)p(x),
(B.3)
where the notation ∝means that both sides are equal, ignoring a multiplicative
constant.
Bayesian inference uses Bayes’ rule in Eq. (B.2) or (B.3) to estimate x. A problem
here is how to determine the prior probability distribution p(x). A general strategy
for this problem is to determine the prior probability by taking what we know about
the unknown parameters x into account. However, if there is no prior knowledge on
x, we must use the uniform prior distribution, i.e.,
p(x) = constant.
(B.4)
With Eq. (B.4), Bayes’ rule in Eq. (B.3) becomes
p(x|y) ∝p(y|x).
(B.5)
In this case, the posterior probability p(x|y) is equal to the likelihood p(y|x), resulting
in a situation that Bayesian and maximum likelihood methods give the same solution.
The prior distribution in Eq. (B.4) is referred to as the non-informative prior.
Even when some prior information on the unknown parameter x is available, exact
probability distributions are generally difﬁcult to determine. Therefore, the proba-
bility distribution is usually determined according to the convenience in computing
the posterior distribution. Some probability distributions have the same forms as
the prior and posterior distributions for given p(y|x). One representative example of
such distributions is the Gaussian distribution. That is, if the noise ε is Gaussian, a
Gaussian prior distribution gives a Gaussian posterior distribution. The derivation of
the posterior distribution in the Gaussian model is explained in Sect. B.3.
B.2 Point Estimate of Unknown x
In Bayesian inference, the unknown parameter x is estimated based on the posterior
probability p(x|y). Then, how can we obtain the optimum estimatex based on p(x|y)?
There are two ways to compute the estimatex based on a given posterior distribution.
One way chooses the x that maximizes the posterior, i.e.,
x = argmax
x
p(x|y).
(B.6)
Thisx is called the maximum a posteriori (MAP) estimate.
Theotherwayistochoose x thatminimizesthesquarederrorbetweentheestimate
x and the true value x. The squared error is expressed as E

x −x)T(x −x)

, and
the estimatex is obtained using

Appendix B: Basics of Bayesian Inference
233
x = argmin
x
E

(x −x)T(x −x)

.
(B.7)
This estimate is called the minimum mean squared error (MMSE) estimate. Here,
taking the relationship,
E

(x −x)T(x −x)

=
 ∞
−∞
(x −x)T(x −x)p(x, y)dxdy
=
 ∞
−∞
 ∞
−∞
(x −x)T(x −x)p(x|y)dx

p(y)dy,
(B.8)
and the fact that p(y) ≥0 into consideration, thex that minimizes E[(x−x)T(x−x)]
is equal to
x = argmin
x
 ∞
−∞
(x −x)T(x −x)p(x|y)dx.
(B.9)
Taking the derivative of the above integral with respect to x, and setting it to zero,
we have
∂
∂x
 ∞
−∞
(x −x)T(x −x)p(x|y)dx = 2
 ∞
−∞
(x −x)p(x|y)dx = 0.
(B.10)
Therefore, the MMSE estimate is equal to
x =
 ∞
−∞
xp(x|y)dx.
(B.11)
That is, the MMSE estimate is equal to the mean of the posterior. Note that, when
the posterior distribution is Gaussian, the MAP estimate and the MMSE estimate are
equal, because the Gaussian distribution reaches its maximum at the mean.
B.3 Derivation of Posterior Distribution
in the Gaussian Model
Let us consider the problem in which the time series of unknown parameters
x1, x2, . . . , xK are estimated using the time series of observation data y1, y2, . . . , yK
where y(tk) and x(tk) are denoted yk and xk. The relationship in Eq. (B.1) holds
between xk and yk, i.e.,
yk = Hxk + ε.
(B.12)

234
Appendix B: Basics of Bayesian Inference
In this chapter, the whole time series data y1, y2, . . . , yK is collectively denoted y,
and the whole time series data x1, x2, . . . , xK are collectively denoted x. We assume
that the noise ε is Gaussian and is identically and independently distributed across
time, i.e.,
ε ∼N(ε|0, Λ−1),
(B.13)
where we omit the notation of the time index k from ε. In Eq. (B.13), Λ is a diagonal
precision matrix of which the jth diagonal entry is equal to the noise precision for the
jth observation data. Then, using Eqs. (B.13) and (C.3), the conditional probability
p(yk|xk) is obtained as
p(yk|xk) = N(yk|Hxk, Λ−1).
(B.14)
The conditional probability of the whole time series of yk given the whole time series
of xk is given by
p(y|x) = p(y1, . . . , yK|x1, . . . , xK)
=
K

k=1
p(yk|xk) =
K

k=1
N(yk|Hxk, Λ−1).
(B.15)
The prior distribution of xk is assumed to be Gaussian and independent across
time:
p(xk) = N(xk|0, Φ−1).
(B.16)
The prior distribution for the whole time series of xk is expressed as
p(x) = p(x1, . . . , xK) =
K

k=1
p(xk) =
K

k=1
N(xk|0, Φ−1).
(B.17)
In this case, the posterior probability is independent across time, and given by
p(x|y) = p(x1, . . . , xK|y1, . . . , yK) =
K

k=1
p(xk|yk).
(B.18)
The posterior probability p(xk|yk) can be derived by substituting Eqs. (B.16) and
(B.14) into Bayes’ rule:
p(xk|yk) ∝p(yk|xk)p(xk).
(B.19)
Actual computation of p(xk|yk) is performed in the following manner. Since we
know that the posterior distribution is also Gaussian, the posterior distribution is
assumed to be
p(xk|yk) = N(xk|¯xk, Γ −1),
(B.20)

Appendix B: Basics of Bayesian Inference
235
where ¯xistheposteriormean,andΓ istheposteriorprecisionmatrix.Theexponential
part of the above Gaussian distribution is given by
−1
2(xk −¯xk)TΓ (xk −¯xk) = −1
2xT
k Γ xk + xT
k Γ ¯xk + C,
(B.21)
where C represents terms that do not contain xk. The exponential part of the right-hand
side of Eq. (B.19) is given by
−1
2

xT
k Φxk + (yk −Hxk)TΛ(yk −Hxk)

.
(B.22)
The above expression can be rewritten as
−1
2xT
k (Φ + HTΛH)xk + xT
k HTΛyk + C′,
(B.23)
where C′ again represents terms that do not contain xk. Comparing the quadratic and
linear terms of xk between Eqs. (B.21) and (B.23) gives the relationships
Γ = Φ + HTΛH,
(B.24)
¯xk = Γ −1HTΛyk = (Φ + HTΛH)−1HTΛyk.
(B.25)
The precision matrix and the mean of the posterior distribution are obtained in
Eqs. (B.24) and (B.25), respectively. This ¯xk is the MMSE (and also MAP) estimate
of xk. Using the matrix inversion formula in Eq. (C.92), ¯xk can also be expressed as
¯xk = Φ−1HT(HΦ−1HT + Λ−1)−1yk = Υ HTΣ−1
y yk,
(B.26)
where Υ , which is equal to Φ−1, is the covariance matrix of the prior distribution,
and Σy is expressed in Eq. (B.30).
B.4 Derivation of the Marginal Distribution p(y)
The probability for the observation y is obtained using
p(y) =
K

k=1
p(yk) =
K

k=1

p(xk, yk)dxk =
K

k=1

p(yk|xk)p(xk)dxk.
(B.27)
Substituting Eqs. (B.16) and (B.14) into the equation above, we get

236
Appendix B: Basics of Bayesian Inference
p(y) =
K

k=1

N(yk|Hxk, Λ−1)N(xk|0, Φ−1)dxk.
(B.28)
This p(y) is called the marginal likelihood. This integral is computed in Sect.4.3.
Using Eq. (4.30), we have
p(y) ∝
K

k=1
1
|Σy|1/2 exp

−1
2yT
k Σ−1
y yk

,
(B.29)
where Σy is expressed as
Σy = Λ−1 + HΦ−1HT.
(B.30)
This Σy is referred to as the model data covariance. Taking the normalization into
account, we get the marginal distribution p(y), such that
p(y) =
K

k=1
p(yk) where p(yk) = N(yk|0, Σy).
(B.31)
Thus, Σy, the model data covariance, is the covariance matrix of the marginal dis-
tribution p(yk).
B.5 Expectation Maximization (EM) Algorithm
B.5.1 Marginal Likelihood Maximization
The Bayesian estimate of the unknown x, ¯xk, is computed using Eq. (B.25). How-
ever, when computing ¯xk, Φ and Λ are needed. The precision matrix of the prior
distribution Φ and that of the noise Λ are called the hyperparameters. Quite often, the
hyperparameters are unknown, and should also be estimated from the observation y.
The hyperparameters may be estimated by maximizing p(y|Φ, Λ), which is the like-
lihood with respect to Φ and Λ. This p(y|Φ, Λ) is called the marginal likelihood or
the data evidence to discriminate it from the conventional likelihood p(y|x).
The marginal likelihood can be computed with p(x|Φ) and p(y|x, Λ) using1
p(y|Φ, Λ) =
 ∞
−∞
p(y|x, Λ)p(x|Φ)dx.
(B.32)
However, computation of Φ and Λ that maximize p(y|Φ, Λ) is generally quite trou-
blesome, as is demonstrated in detail in Chap. 4. In the following, instead of directly
1 The notation dx indicates dx1dx2 · · · dxK.

Appendix B: Basics of Bayesian Inference
237
maximizing the marginal likelihood, we describe an algorithm that maximizes a
quantity called the average data likelihood to obtain estimates for the hyperparame-
ters. This algorithm is called the expectation maximization (EM) algorithm, and is
described in the following.
B.5.2 Average Data Likelihood
The EM algorithm computes the quantity called the average data likelihood. Comput-
ingtheaveragedatalikelihoodismucheasierthancomputingthemarginallikelihood.
To deﬁne the average data likelihood, let us ﬁrst deﬁne the complete data likelihood,
such that
log p(y, x|Φ, Λ) = log p(y|x, Λ) + log p(x|Φ).
(B.33)
If we observed not only y but also x, we could have estimated Φ and Λ by maximizing
log p(y, x|Φ, Λ) with respect to these hyperparameters. However, since we do not
observe x, we must substitute for the unknown x in log p(y, x|Φ, Λ) with some
“reasonable” value.
Having observed y, we actually know which values of x are reasonable, and our
best knowledge on the unknown x is represented by the posterior distribution p(x|y).
Thus, the “reasonable” value would be the one that maximizes the posterior proba-
bility, and one solution would be to use the MAP estimate of x in log p(y, x|Φ, Λ). A
better solution would be to use all possible values of x in the complete data likelihood
and average over it with the posterior probability. This results in the average data
likelihood, Θ(Φ, Λ):
Θ(Φ, Λ) =

p(x|y) log p(y, x|Φ, Λ)dx
= E

log p(y, x|Φ, Λ)

= E

log p(y|x, Λ)

+ E

log p(x|Φ)

,
(B.34)
where the expectation E[ · ] is taken with respect to the posterior probability p(x|y).
The estimates of the hyperparameters, Φ and Λ are obtained using
Λ = argmax
Λ
Θ(Φ, Λ),
(B.35)
Φ = argmax
Φ
Θ(Φ, Λ).
(B.36)
In the Gaussian model discussed in Sect. B.3, p(x|Φ) and p(y|x, Λ) are expressed
in Eqs. (B.17) and (B.15), respectively. Substituting Eqs. (B.17) and (B.15) into
(B.33), the complete data likelihood is expressed as2
2 The constant terms containing 2π are ignored here.

238
Appendix B: Basics of Bayesian Inference
log p(y, x|Φ, Λ) = K
2 log |Φ| −1
2
K

k=1
xT
k Φxk
+ K
2 log |Λ| −1
2
K

k=1
(yk −Hxk)TΛ(yk −Hxk).
(B.37)
Thus, the average data likelihood is obtained as
Θ(Φ, Λ) = K
2 log |Φ| −1
2E
 K

k=1
xT
k Φxk

+ K
2 log |Λ| −1
2E
 K

k=1
(yk −Hxk)TΛ(yk −Hxk)

.
(B.38)
B.5.3 Update Equation for Prior Precision Φ
Let us ﬁrst derive the update equation for Φ. The updated value of Φ, Φ, is the one
that maximizes the average data likelihood, Θ(Φ, Λ). The derivative of Θ(Φ, Λ)
with respect to Φ is given by
∂
∂Φ Θ(Φ, Λ) = K
2 Φ−1 −1
2E
 K

k=1
xkxT
k

.
(B.39)
Here we use Eqs. (C.89) and (C.90). Setting this derivative to zero, we have
Φ−1 = 1
K E
 K

k=1
xkxT
k

.
(B.40)
Using the posterior precision Γ , the relationship
E
 K

k=1
xkxT
k

=
K

k=1
¯xk ¯xT
k + KΓ −1
(B.41)
holds. Therefore, the update equation for Φ is expressed as
Φ−1 = 1
K
K

k=1
¯xk ¯xT
k + Γ −1.
(B.42)

Appendix B: Basics of Bayesian Inference
239
B.5.4 Update Equation for Noise Precision Λ
WenextderivetheupdateequationforΛ.Thederivativeoftheaveragedatalikelihood
with respect to Λ is given by
∂
∂ΛΘ(Φ, Λ) = K
2 Λ−1 −1
2E
 K

k=1
(yk −Hxk)(yk −Hxk)T

.
(B.43)
Setting this derivative to zero, we get
Λ−1 = 1
K E
 K

k=1
(yk −Hxk)(yk −Hxk)T

.
(B.44)
Using the mean and the precision of the posterior distribution, the above equation is
rewritten as
Λ−1 = 1
K
K

k=1
(yk −H¯xk)(yk −H¯xk)T + HΓ −1HT.
(B.45)
B.5.5 Summary of the EM Algorithm
The EM algorithm is a recursive algorithm. We ﬁrst set appropriate initial values to
the hyperparameters Φ and Λ, which are then used for computing the posterior dis-
tribution, i.e., for computing the mean and the precision of the posterior distribution
using
Γ = Φ + HTΛH,
¯xk = Γ −1HTΛyk = (Φ + HTΛH)−1HTΛyk.
With these parameter values of the posterior distribution, the hyperparameters Φ and
Λ are updated using
Φ−1 = 1
K
K

k=1
¯xk ¯xT
k + Γ −1,
Λ−1 = 1
K
K

k=1
(yk −H¯xk)(yk −H¯xk)T + HΓ −1HT.
The step that computes the posterior distribution is called the E step, and the step
that estimates the hyperparameters is called the M step. The EM algorithm updates

240
Appendix B: Basics of Bayesian Inference
the posterior distribution and the values of hyperparameters by repeating the E and
M steps. As a result of this recursive procedure, the marginal likelihood p(y|Φ, Λ)
is increased. A proof may be found, for example, in [1].
B.5.6 Hyperparameter Update Equations when Φ = αI
and Λ = βI
In Sect. 2.10, we show that the Bayesian estimate of xk, ¯xk, becomes equal to the
L2-norm regularized minimum-norm solution if we use Φ = αI and Λ = βI in the
Gaussian model. Let us derive update equations for the scalar hyperparameters α
and β in this case.
The average data likelihood in this case is given by
Θ(α, β) = NK
2 log α −α
2 E
 K

k=1
xT
k xk

+ MK
2
log β −β
2 E
 K

k=1
(yk −Hxk)T(yk −Hxk)

.
Therefore, using
∂
∂α Θ(α, β) = NK
2α −1
2E
 K

k=1
xT
k xk

= 0
and
E
 K

k=1
xT
k xk

= tr
 K

k=1
E(xkxT
k )

= tr
 K

k=1
¯xk ¯xT
k + KΓ −1

=
K

k=1
¯xT
k ¯xk + K tr(Γ −1),
we get
α−1 =
1
NK E
 K

k=1
xT
k xk

= 1
N

1
K
K

k=1
∥¯xk∥2 + tr(Γ −1)

(B.46)
for the update equation of α.

Appendix B: Basics of Bayesian Inference
241
Regarding the hyperparameter β, using a similar derivation, we have
β−1 =
1
MK E
 K

k=1
(yk −Hxk)T(yk −Hxk)

.
(B.47)
Here, considering the relationships
E
 K

k=1
(yk −Hxk)T(yk −Hxk)

= E
 K

k=1
	
yT
k yk −xT
k HTyk −yT
k Hxk + xT
k HTHxk


=
K

k=1
	
yT
k yk −E[xT
k ]HTyk −yT
k HE[xk] + E[xT
k HTHxk]

=
K

k=1
	
yT
k yk −¯xT
k HTyk −yT
k H¯xk + E[xT
k HTHxk]

(B.48)
and
E[xT
k HTHxk] = E[tr(HTHxkxT
k )]
= tr

HTHE
	
xkxT
k


= tr

HTH
	
¯xk ¯xT
k + Γ −1

= ¯xT
k HTH¯xk + tr
	
HTHΓ −1
,
(B.49)
we ﬁnally obtain
β−1 = 1
M

1
K
K

k=1
∥yk −H¯xk∥2 + tr
	
HTHΓ −1

.
(B.50)
B.6 Variational Bayesian Inference
In the preceding sections, the unknown x is estimated based on the posterior distribu-
tion p(x|y), and the hyperparameters are estimated by using the EM algorithm. In this
section, we introduce a method called variational Bayesian inference, which makes
it possible to derive approximate posterior distributions for hyperparameters [2].

242
Appendix B: Basics of Bayesian Inference
B.6.1 Derivation of the EM Algorithm Using the Free Energy
B.6.1.1 Derivation of Posterior Distribution (E-step)
As a preparation for introducing the variational technique, we derive the EM algo-
rithm in a different manner based on an optimization of a functional called the free
energy. In this section, the hyperparameters are collectively expressed as θ. We deﬁne
a functional such that,
F[q(x), θ] =

dxq(x)[log p(x, y|θ) −log q(x)].
(B.51)
This F[q(x), θ] is a function of hyperparameters θ and an arbitrary probability distri-
bution q(x). This F[q(x), θ] is called the free energy using a terminology in statistical
physics. We show, in the following, that maximizing the free energy F[q(x), θ] with
respect to q(x) results in the E step, and maximizing it with respect to the hyperpa-
rameters results in the M step of the EM algorithm.
When maximizing F[q(x), θ] with respect to q(x), since q(x) is a probability
distribution, the constraint
 ∞
−∞q(x)dx = 1 must be imposed. Therefore, this maxi-
mization problem can be formulated such that,
q(x) = argmax
q(x)
F[q(x), θ],
subject to
 ∞
−∞
q(x)dx = 1.
(B.52)
Such a constrained optimization problem can be solved by using the method of
Lagrange multipliers, in which deﬁning the Lagrange multiplier as γ , the Lagrangian
is deﬁned as
L[q, γ ] = F[q, θ] + γ
 ∞
−∞
q(x)dx −1

=
 ∞
−∞
dxq(x)[log p(x, y|θ) −log q(x)] + γ
 ∞
−∞
q(x)dx −1

. (B.53)
The constrained optimization problem in Eq. (B.52) is now rewritten as the uncon-
strained optimization problem in Eq. (B.53). The probability distribution q(x) that
maximizes the Lagrangian L[q, γ ] is the solution of the constrained optimization
problem in Eq. (B.52).
Differentiating L[q, γ ] with respect to q(x), and setting the derivative to zero, we
have
δL[q(x), γ ]
δq(x)
= log p(x, y|θ) −log q(x) −1 + γ = 0.
(B.54)
A brief explanation on the differentiation of a functional, as well as the derivation of
Eq. (B.54), is presented in Sect. C.5 in the Appendix. Differentiating L[q, γ ] with
respect to γ gives

Appendix B: Basics of Bayesian Inference
243
∂L[q(x), γ ]
∂γ
=
 ∞
−∞
q(x)dx −1 = 0.
(B.55)
Thus, using Eq. (B.54), we have
q(x) = eγ −1p(x, y|θ),
(B.56)
and using Eq. (B.55), we also have
 ∞
−∞
q(x)dx = eγ −1
 ∞
−∞
p(x, y|θ)dx = eγ −1p(y|θ) = 1.
(B.57)
We thereby get
eγ −1 =
1
p(y|θ).
(B.58)
Substitution of Eq. (B.58) into (B.56) results in the relationship
q(x) = p(x, y|θ)
p(y|θ)
= p(x|y).
(B.59)
The above equation shows that the probability distribution that maximizes the free
energy F[q(x), θ] is the posterior distribution p(x|y).
Note that to derive the posterior distribution, we do not explicitly use Bayes’ rule.
Instead, we use the optimization of the functional called the free energy. This idea is
further extended in variational Bayesian inference in the following sections to derive
an approximate posterior distribution in a more complicated situation.
B.6.1.2 Derivation of M-step
We next maximize the free energy with respect to the hyperparameter θ. Once F[q, θ]
is maximized with respect to q(x), the free energy is written as
F[p(x|y), θ] =

dxp(x|y)[log p(x, y|θ) −log p(x|y)]
= Θ(θ) + H[p(x|y)],
(B.60)
where
Θ(θ) =

dxp(x|y) log p(x, y|θ).
(B.61)
This Θ(θ) is equal to the average data likelihood, and
H[p(x|y)] = −

dxp(x|y) log p(x|y)
(B.62)

244
Appendix B: Basics of Bayesian Inference
is the entropy of the posterior distribution. Since the entropy does not depend on
θ, maximizing F[p(x|y), θ] with respect to θ is equivalent to maximizing Θ(θ)
with respect to θ. Namely, the maximization of the free energy with respect to the
hyperparameters results in the M step of the EM algorithm.
Note also that the free energy after the maximization with respect to q(x) is equal
to the marginal likelihood, log p(y|θ). This can be seen by rewriting Eq. (B.60) such
that
F[p(x|y), θ] =

dxp(x|y) [log p(x, y|θ) −log p(x|y)]
=

dxp(x|y) log p(y|θ) = log p(y|θ).
(B.63)
This relationship is used to derive the expressions of the marginal likelihood for the
Bayesian factor analysis in Chap. 5.
For an arbitrary probability distribution q(x), the relationship between the free
energy and the marginal likelihood is expressed as
F[q(x), θ] = log p(y|θ) −KL

q(x)||p(x|y)

,
(B.64)
where KL

q(x)||p(x|y)

is the Kullback-Leibler (KL) distance deﬁned in
KL

q(x)||p(x|y)

=

q(x) log q(x)
p(x|y)dx.
(B.65)
The KL distance represents a distance between the true posterior distribution p(x|y)
and the arbitrary probability distribution q(x). It always has a nonnegative value,
and is equal to zero when the two distributions are identical. Hence, for an arbitrary
q(x), the inequality F[q(x), θ] ≤log p(y|θ) holds, and the free energy forms a
lower-bound of the marginal likelihood.
B.6.2 Variational Bayesian EM Algorithm
In Bayesian inference, to estimate the unknown parameter x, we must ﬁrst derive
the posterior distribution p(x|y), assuming the existence of an appropriate prior dis-
tribution p(x|θ). We then obtain an optimum estimate of the unknown x based on
the posterior distribution p(x|y). When the hyperparameter θ is unknown, a truely
Bayesian approach is to ﬁrst derive the joint posterior distribution p(x, θ|y), and to
estimate x and θ simultaneously based on this joint posterior distribution. To derive
the joint posterior p(x, θ|y), we can use Bayes’ rule, and obtain,
p(x, θ|y) ∝p(y|x, θ)p(x, θ) = p(y|x, θ)p(x|θ)p(θ).
(B.66)

Appendix B: Basics of Bayesian Inference
245
In order to compute the posterior distribution p(x, θ|y), we must compute the integral
Z =
 ∞
−∞
p(y|x, θ)p(x)p(θ)dxdθ.
However, in general, this integral Z does not have a closed-form solution, and it is
hard to compute the posterior distribution p(x, θ|y) using Bayes’ rule, i.e., Eq. (B.66).
In Sect. B.6.1, the posterior distribution is obtained using the optimization of the
factional called the free energy, and not using Bayes’ rule. We here use this variational
technique to obtain the posterior distribution. The free energy is expressed as
F[q(x, θ), θ] =

dθdxq(x, θ)[log p(x, y, θ) −log q(x, θ)].
(B.67)
We use an approximation that the joint posterior distribution is factorized, i.e.,
q(x, θ) = q(x)q(θ),
(B.68)
which is called the variational approximation. Using this approximation, the free
energy is given by
F[q(x), q(θ), θ] =

dθdxq(x)q(θ)[log p(x, y, θ)−log q(x)−log q(θ)].
(B.69)
The best estimate of the posterior distributions p(x|y) and p(θ|y) are derived by
jointly maximizing the free energy in Eq. (B.69). That is, the estimated posterior
distributions,p(x|y) andp(θ|y), are given by
p(x|y), p(θ|y) = argmax
q(x),q(θ)
F[q(x), q(θ), θ],
subject to
 ∞
−∞
dxq(x) = 1, and
 ∞
−∞
dθq(θ) = 1.
(B.70)
Therefore, according to Eq. (B.64), the estimated posterior distributionsp(x|y) and
p(θ|y) satisfy the relationship:
p(x|y), p(θ|y) = argmin
q(x),q(θ)
KL

p(x, θ|y)||q(x, θ|y)

,
where q(x, θ|y) = q(x|y)q(θ|y).
(B.71)
Namely, the estimated posterior distributions p(x|y) and p(θ|y) obtained by max-
imizing the free energy jointly minimizes the KL distance between the true and
approximated posterior distributions. In other words, the estimated joint posterior
p(x, θ|y) = p(x|y)p(θ|y) is the “best” estimate in the sense that it minimizes the KL
distance from the true posterior distribution.

246
Appendix B: Basics of Bayesian Inference
Let us ﬁrst derivep(x|y). Using arguments similar to those in Sect. B.6.1, deﬁning
the Lagrangian as
L[q(x)] =

dxdθq(x)q(θ)[log p(x, y, θ) −log q(x) −log q(θ)]
+ γ
 ∞
−∞
q(x)dx −1

,
(B.72)
differentiating it with respect to q(x), and setting the derivative to zero, we have

dθq(θ)

log p(x, y, θ) −log q(x)

+ C = 0,
(B.73)
where
C = −

dθq(θ) log q(θ) −1 + γ.
Neglecting C, we obtain
log p(x|y) =

dθq(θ) log p(x, y, θ) = Eθ

log p(x, y, θ)

,
(B.74)
where Eθ [ · ] indicates computing the mean with respect to the posterior distribution
q(θ). Using exactly the same derivation, we obtain the relationship,
log p(θ|y) =

dxq(x) log p(x, y, θ) = Ex

log p(x, y, θ)

,
(B.75)
where Ex [ · ] indicates computing the mean with respect to the posterior distribution
q(x).
Equations (B.74) and (B.75) indicate that, to compute the posterior distribution
p(x|y), we need q(θ), namelyp(θ|y), and to compute the posterior distributionp(θ|y),
we need q(x), namelyp(x|y). Therefore, in variational Bayesian inference, we need
an EM-like recursive procedure. We updatep(x|y) andp(θ|y) by repeatedly applying
Eqs. (B.74) and (B.75). When computing p(x|y) using Eq. (B.74), we use p(θ|y)
obtained in the preceding step. When computing p(θ|y) using Eq. (B.75), we use
p(x|y) obtained in the preceding step. This recursive algorithm is referred to as the
variational Bayesian EM (VBEM) algorithm.
References
1. C. M. Bishop, Pattern recognition and machine learning. Springer, New York,
2006.
2. H. T. Attius, “A variational Bayesian framework for graphical models,” in
Advances in Neural information processing, pp. 209–215, MIT Press, 2000.

Appendix C
Supplementary Mathematical Arguments
C.1 Multi-dimensional Gaussian Distribution
Let us deﬁne a column vector of N random variables as x. The multi-dimensional
Gaussian distribution for x is expressed as
p(x) =
1
(2π)N/2|Σ|1/2 exp

−1
2(x −μ)TΣ−1(x −μ)

.
(C.1)
Here μ is the mean of x deﬁned as μ = E(x) and Σ is the covariance matrix of x
deﬁned as Σ = E

(x −μ)(x −μ)T
where E( · ) is the expectation operator. Also,
|Σ| indicates the determinant of Σ. The Gaussian distribution in Eq. (C.1) is often
written as
p(x) = N(x|μ, Σ).
When the random variable x follows a Gaussian distribution with mean μ and the
covariance matrix Σ, the expression
x ∼N(x|μ, Σ)
(C.2)
is often used.
Two vector random variables x1 and x2 have the linear relationship,
x2 = Ax1 + c,
where A is a matrix of deterministic variables, and c is a vector of deterministic
variables. Then, if we have
x1 ∼N(x1|μ, Σ),
we also have
x2 ∼N(x2|Aμ + c, AΣAT).
(C.3)
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9
247

248
Appendix C: Supplementary Mathematical Arguments
Namely, x2 obeys the Gaussian distribution with a mean equal to Aμ + c and a
covariance matrix equal to AΣAT.
Bayesian inference quite often uses the precision, instead of the variance. Let
us deﬁne the precision matrix Λ that corresponds to the covariance natrix Σ, i.e.,
Λ = Σ−1. When the precision Λ is used, the notation in Eq. (C.2) is used such that
x ∼N(x|μ, Λ−1).
(C.4)
Here, since we maintain the notational convenience
N(random variable|mean, covariance matrix),
we must use Λ−1 in this notation. Using the precision matrix, Λ, the explicit form
of the Gaussian distribution is given by
p(x) =
|Λ|1/2
(2π)N/2 exp

−1
2(x −μ)TΛ(x −μ)

.
(C.5)
Let us compute the entropy when the probability distribution is Gaussian. The
deﬁnition of the entropy for a continuous random variable is
H[p(x)] = −

p(x) log p(x)dx = −E

log p(x)

.
(C.6)
Substituting the probability distribution in Eq.(C.1) into the equation above, we get
H[p(x)] = −E

log

1
(2π)N/2|Σ|1/2

+ 1
2E

(x −μ)TΣ−1(x −μ)

,
(C.7)
where constant terms are omitted. In the equation above, the ﬁrst term on the right-
hand side is equal to 1
2 log |Σ|. The second term is equal to
1
2E

(x −μ)TΣ−1(x −μ)

= 1
2E

tr

Σ−1(x −μ)(x −μ)T
= 1
2 tr

Σ−1E

(x −μ)(x −μ)T
= 1
2 tr

Σ−1Σ

= N
2 .
(C.8)
Therefore, omitting constant terms, the entropy is expressed as
H[p(x)] = H(x) = 1
2 log |Σ|.
(C.9)

Appendix C: Supplementary Mathematical Arguments
249
Note that we use a simpliﬁed notation H(x) in this book to indicate the entropy of
the probability distribution p(x), which should formally be written as H[p(x)]3
C.2 Complex Gaussian Distribution
We derive the probability distribution when the random variable is a complex-valued
Gaussian. The arguments in this section follows those of Neeser and Massey [1].
Let us deﬁne a column vector of N complex random variables as z, and deﬁne
z = x + iy where x = ℜ(z), y = ℑ(z) and x and y are real-valued N-dimensional
Gaussian random vectors. Here, we assume that E(z) = 0, and as a result, E(x) = 0
and E(y) = 0. The covariance matrix of z is deﬁned as Σzz = E

zzH
.
We next deﬁne a (2N × 1) vector φ such that φ = [xT, yT]T. The covariance
matrix of φ, Σφφ, is given by
Σφφ = E
 x
y
 
xT, yT
=
Σxx ΣT
yx
Σyx Σyy

,
(C.10)
where Σxx = E(xxT), Σyx = E(yxT), and Σyy = E(yyT). We assume that the joint
distribution of x and y is given by
p(x, y) = p(φ) =
1
(2π)N|Σφφ|1/2 exp

−1
2φTΣ−1
φφφ

.
(C.11)
In this section, we show that this joint distribution is equal to
p(z) =
1
πN|Σzz| exp

−zHΣ−1
zz z

,
(C.12)
which is called the complex Gaussian distribution.
To show this equality, we assume a property, called “proper”, on the complex
random variable z. The complex random variable z is proper if its pseudo covariance
¯Σzz is equal to zero. The pseudo covariance is deﬁned such that ¯Σzz = E

zzT
,
which is written as
¯Σzz = E

(x + iy)(x + iy)T
= Σxx −Σyy + i
	
Σyx + ΣT
yx

.
Therefore, ¯Σzz = 0 is equivalent to the relationships
Σxx = Σyy
and
−Σyx = ΣT
yx.
(C.13)
3 The notation H(x) may look as if the entropy H(x) is a function of x, but the entropy is a functional
of the probability distribution p(x), not a function of x.

250
Appendix C: Supplementary Mathematical Arguments
NotethatpropercomplexGaussianrandomvectorswithzeromeanarecalledcircular.
In this section, we assume that the circularity holds for z. Under the assumption that
z is a proper complex random vector, we derive
Σzz = E

(x + iy)(x −iy)T
= 2

Σxx + iΣyx

,
(C.14)
and Σ−1
zz is found to be4
Σ−1
zz = 1
2−1 	
I −iΣyxΣ−1
xx

,
(C.15)
where
 = Σxx + ΣyxΣ−1
xx Σyx.
(C.16)
We ﬁrst derive the relationship between the determinants |Σzz| and |Σφφ|. Using
Eq. (C.14), we have
ΣT
zz = 2
	
Σxx + iΣT
yx

= 2

Σxx −iΣyx

,
and Σ−1
zz is expressed as
Σ−1
zz = 1
2−1 
Σxx −iΣyx

Σ−1
xx = 1
4−1ΣT
zzΣ−1
xx .
Taking the determinant of both sides of the equation above, we get
|Σzz|−1 = 1
4N ||−1 |Σzz| |Σxx|−1,
and ﬁnally,
|Σzz| = 2N
|Σxx|||.
(C.17)
On the other hand, using the determinant identity in Eq. (C.94), |Σφφ| is expressed
as
|Σφφ| =

Σxx ΣT
yx
Σyx Σyy
 = |Σxx||Σyy −ΣyxΣ−1
xx ΣT
yx| = |Σxx|||,
(C.18)
where the relationships in Eq. (C.13) are used to obtain the right-most expression.
Comparing Eqs. (C.17) and (C.18), we ﬁnally obtain the relationship,
|Σzz| = 2N
|Σφφ|.
(C.19)
4 It is easy to see that Σ−1
zz Σzz = I holds.

Appendix C: Supplementary Mathematical Arguments
251
We next show the equality of the two quadratic forms, i.e.,
zHΣ−1
zz z = 1
2φTΣ−1
φφφ.
To show this relationship, we use the matrix inversion formula in Eq. (C.93), and
rewrite Σ−1
φφ such that5
Σ−1
φφ =

−1
Σ−1
xx Σyx−1
−−1ΣyxΣ−1
xx
−1

,
(C.20)
where  is deﬁned in Eq. (C.16). Let us deﬁne:
Mc = −1,
(C.21)
Ms = −Σ−1
xx Σyx−1.
(C.22)
Then, Mc is symmetric because  is symmetric. Using
Σ−1
xx Σyx =
	
Σxx + ΣyxΣ−1
xx Σyx

Σ−1
xx Σyx = ΣyxΣ−1
xx ,
we also get the relationship:
Ms = −Σ−1
xx Σyx−1 = −−1ΣyxΣ−1
xx −1 = −−1ΣyxΣ−1
xx
= −
	
Σ−1
xx ΣT
yx−1
T
=
	
Σ−1
xx Σyx−1
T
= −MT
s ,
(C.23)
where ΣT
yx = −Σyx is used. Thus, Ms is skew-symmetric.
We deﬁne a matrix M as
M = (Mc + iMs) ,
(C.24)
and compute the quadratic form zHMz. Since we have the relationship,
	
zHMz

H
= zHMHz = zH (Mc + iMs)H z
= zH 	
MT
c −iMT
s

z = zH (Mc + iMs) z = zHMz,
(C.25)
5 In Eq. (C.20), the (2, 2)th element is −1, which can be shown as follows. According to Eq. (C.93),
this element is equal to
Σ−1
yy + Σ−1
yy Σyx
	
Σxx + ΣyxΣ−1
xx Σyx

ΣT
yxΣ−1
yy .
Using Eqs. (C.13) and (C.91), this is equal to (Σxx + ΣyxΣ−1
xx Σyx)−1 = −1.

252
Appendix C: Supplementary Mathematical Arguments
the quadratic form zHMz is real-valued. Therefore, we have
zHMz = ℜ

zHMz

= ℜ

(xT −iyT) (Mc + iMs) (x + iy)

= xTMcx −xTMsy + yTMsx + yTMcy
= [xT, yT]
Mc −Ms
Ms Mc
 x
y

= φTΣ−1
φφφ.
(C.26)
We now compute M, such that
M = −1 −i−1ΣyxΣ−1
xx = −1 	
I −iΣyxΣ−1
xx

.
By comparing the above M with Eq. (C.15) we can see that Σ−1
zz
=
1
2M, and
therefore, from Eq. (C.26), we can prove the relationship
zHΣ−1
zz z = 1
2φTΣ−1
φφφ.
(C.27)
On the basis of Eqs. (C.17) and (C.27), it is now clear that the real-valued joint
Gaussian distribution in Eq. (C.11) and the complex Gaussian distribution in
Eq. (C.12) are equivalent. Using exactly the same derivation for Eq. (C.9) and ignor-
ing the constants, the entropy for the complex-valued Gaussian is obtained as
H(z) = log |Σzz|.
(C.28)
C.3 Canonical Correlation and Mutual Information
C.3.1 Canonical Correlation
This appendix provides a concise explanation on canonical correlation. Let us deﬁne
real-valued random vectors x and y such that
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xp
⎤
⎥⎥⎥⎦,
and y =
⎡
⎢⎢⎢⎣
y1
y2
...
yq
⎤
⎥⎥⎥⎦,
(C.29)
and consider computing the correlation between x and y. One way is to compute
correlation coefﬁcients between all combinations of (xi, yj). However, this gives
total p × q correlation coefﬁcients and the interpretation of these results may not be
easy. The canonical correlation method ﬁrst projects column vectors x and y onto the

Appendix C: Supplementary Mathematical Arguments
253
direction of a and b such thatx = aTx andy = bTy, where a and b are real-valued
vectors. It then computes the correlation betweenx andy. This correlation depends
on the choices of a and b. The maximum correlation betweenx andy is considered
to represent the correlation between x and y, and this maximum correlation is called
the canonical correlation.
The correlation betweenx andy, ρ, is expressed as
ρ =
E(xy)

E(x2)

E(y2)
=
aTE(xyT)b

aTE(xxT)a
 
bTE(yyT)b
 =
aTΣxyb

aTΣxxa
 
bTΣyyb
.
(C.30)
where Σxy = E(xyT), Σxx = E(xxT), and Σyy = E(yyT) and the notation E( · )
indicates the expectation. Here, Σxx and Σyy are symmetric matrices, and the rela-
tionship ΣT
xy = Σyx holds. The canonical correlation ρc is obtained by solving the
following maximization problem:
ρc = max
a,b aTΣxyb subject to aTΣxxa = 1 and bTΣyyb = 1.
(C.31)
To solve this maximization problem, we ﬁrst compute whitened vectors α and β,
such that
α = Σ1/2
xx a,
(C.32)
β = Σ1/2
yy b.
(C.33)
Using these vectors, the optimization problem in Eq. (C.31) is rewritten as
ρc = max
α,β αTΣ−1/2
xx
ΣxyΣ−1/2
yy
β
subject to αTα = 1 and βTβ = 1.
(C.34)
The constrained maximization problem in Eq. (C.34) can be solved using the
Lagrange multiplier method. Denoting the Lagrange multipliers ν1 and ν2, the
Lagrangian is deﬁned as
L = αTΣ−1/2
xx
ΣxyΣ−1/2
yy
β −ν1
2
	
αTα −1

−ν2
2
	
βTβ −1

.
(C.35)
The optimum α and β are obtained by maximizing L with no constraints. The deriv-
atives of L with respect to α and β are expressed as

254
Appendix C: Supplementary Mathematical Arguments
∂L
∂α = Πxyβ −ν1α = 0,
(C.36)
∂L
∂β = ΠT
xyα −ν2β = 0,
(C.37)
where Πxy = Σ−1/2
xx
ΣxyΣ−1/2
yy
. Using the constraint αTα = 1, and left multiplying
αT to Eq. (C.36) gives
αTΠxyβ = ν1.
Using the constraint βTβ = 1, and left multiplying βT to Eq. (C.37) gives
βTΠT
xyα = ν2.
Since the left-hand sides of the two equations above are equal, we have ν1 = ν2 = ρc.
Also, using Eq. (C.37), we get
β = 1
ρc
ΠT
xyα,
and substituting the equation above into Eq. (C.36), we get
	
ΠxyΠT
xy

α = ρ2
c α.
(C.38)
This equation indicates that the squared canonical correlation ρ2
c is obtained as the
eigenvalues of a matrix ΠxyΠT
xy, and the corresponding eigenvector is the solution
for the vector α. Note that ΠxyΠT
xy is a real symmetric matrix, the eigenvectors α
and β are real-valued, and therefore a and b are real-valued because Σxx and Σyy
are real-valued matrices.
Denoting the eigenvalues in Eq. (C.38) as μj where j = 1, . . . , d and d =
min{p, q}, the canonical correlation between the two sets of random variables
x1, . . . , xp and y1, . . . , yq is obtained as the largest eigenvalue μ1, which is the
best overall measure of the association between x and y. However, other eigenvalues
may provide complementary information on the linear relationship between those
two sets of random variables. The mutual information described in Sect.C.3.2 is a
measure that can take all the eigenvalues into account.
Also, it is worth mentioning that the matrices,
ΠxyΠT
xy = Σ−1/2
xx
ΣxyΣ−1
yy ΣT
xyΣ−1/2
xx
and
Σ−1
xx ΣxyΣ−1
yy ΣT
xy
(C.39)

Appendix C: Supplementary Mathematical Arguments
255
have the same eigenvalues, according to Property No. 9 in Sect. C.8. Thus, the
canonical squared correlation can be obtained as the largest eigenvalue of the matrix
Σ−1
xx ΣxyΣ−1
yy ΣT
xy.
C.3.2 Mutual Information
Next we introduce mutual information, which has a close relationship with the canon-
ical correlation under the Gaussianity assumption. We also assume real-valued ran-
dom vectors x and y, and their probability distribution as p(x) and p(y). The entropy
is deﬁned for x and y such that,
H(x) = −

p(x) log p(x)dx,
(C.40)
H(y) = −

p(y) log p(y)dy.
(C.41)
The entropy is a measure of uncertainty; H(x) represents the uncertainty when x is
unknown and H(y) represents the uncertainty when y is unknown. The joint entropy
is deﬁned as
H(x, y) = −

p(x, y) log p(x, y)dxdy,
(C.42)
which represents the uncertainty when both x and y are unknown. When x and y are
independent, we have the relationship
H(x, y) = −

p(x, y) log p(x, y)dxdy
= −

p(x)p(y) (log p(x) + log p(y)) dxdy = H(x) + H(y).
(C.43)
The conditional entropy is deﬁned as
H(x|y) = −

p(x, y) log p(x|y)dxdy,
(C.44)
which represents the uncertainty when x is unknown, once y is given. We then have
the relationship,
H(x, y) = H(x|y) + H(y).
(C.45)
The above indicates that the uncertainty when both x and y are unknown is equal to
the uncertainty on x when y is given plus the uncertainty when y is unknown.

256
Appendix C: Supplementary Mathematical Arguments
The mutual information between x and y is deﬁned as
I(x, y) = H(x) + H(y) −H(x, y).
(C.46)
When x and y are independent, we have I(x, y) = 0 according to Eq. (C.43). Let us
deﬁne z as z = [xT, yT]T. Assuming the Gaussian processes for x and y, the mutual
information is expressed as
I(x, y) = H(x) + H(y) −H(x, y)
= 1
2 log |Σxx| + 1
2 log
Σyy
 −1
2 log |Σzz| .
(C.47)
Here, |Σzz| is rewritten as
|Σzz| =

Σxx Σxy
ΣT
xy Σyy
 =
Σyy

Σxx −ΣxyΣ−1
yy ΣT
xy
 ,
(C.48)
where the determinant identity in Eq. (C.94) is used. Substituting Eq. (C.48) into
(C.47), we have
I(x, y) = 1
2 log
|Σxx|
Σxx −ΣxyΣ−1
yy ΣT
xy

= 1
2 log
1
I −Σ−1
xx ΣxyΣ−1
yy ΣT
xy

.
(C.49)
Let us deﬁne the eigenvalues of Σ−1
xx ΣxyΣ−1
yy ΣT
xy as γj where j = 1, . . . , d and
d = min{p, q}.6 Using these eigenvalues, we can derive
I(x, y) = 1
2 log
1
I −Σ−1
xx ΣxyΣ−1
yy ΣT
xy

= 1
2 log
1
 d
j=1(1 −γj)
= 1
2
d

j=1
log
1
1 −γj
.
(C.50)
Note that γ1 is equal to the canonical squared correlation ρ2
c , according to the argu-
ments in Sect. C.3.1.
When the random vectors x and y are complex-valued, the mutual information for
the complex random vectors is expressed as
I(x, y) = H(x) + H(y) −H(x, y) = log |Σxx| + log
Σyy
 −log |Σzz| .
(C.51)
6 Here, remember that p and q are the sizes of the column vector x and y

Appendix C: Supplementary Mathematical Arguments
257
Here, Σxx, Σyy, and Σzz are covariance matrices of the complex Gaussian distribu-
tion. Using exactly the same derivation, we can obtain
I(x, y) = log
1
I −Σ−1
xx ΣxyΣ−1
yy ΣT
xy

=
d

j=1
log
1
1 −γj
,
(C.52)
where γj is the jth eigenvalue of Σ−1
xx ΣxyΣ−1
yy ΣT
xy.
C.3.3 Covariance of Residual Signal and Conditional Entropy
Let us next consider the regression problem:
y = Ax + e,
(C.53)
where the vector e represents the residual of this regression. The p × q coefﬁcient
matrix A can be estimated using the least-squares ﬁt,
A = argmin
A
E

∥e∥2
= argmin
A
E

∥y −Ax∥2
.
(C.54)
Here, E

∥e∥2
is expressed as
E

∥e∥2
= E

yyT −xTATy −yTAx + xTATAx

.
Differentiating E

∥e∥2
with respect to A gives
∂
∂AE

∥e∥2
= −2E(yxT) + 2AE(xxT).
Setting this derivative to zero, we obtain
A = E(yxT)E(xxT)−1 = ΣyxΣ−1
xx ,
(C.55)
where Σyx = E(yxT) and Σxx = E(xxT). The covariance matrix of the residual e is
deﬁned as

258
Appendix C: Supplementary Mathematical Arguments
Σee = E

y −Ax
 
y −Ax
T
= E

yyT −yxTAT −AxyT + AxxTAT
= E(yyT) −E(yxT)AT −AE(xyT) + AE(xxT)AT
= Σyy −ΣyxAT −AΣxy + AΣxxAT,
(C.56)
where Σyy = E(yyT) and Σxy = E(xyT). Substituting Eq. (C.55) into the equation
above gives,
Σee = Σyy −ΣyxΣ−1
xx ΣT
yx.
(C.57)
Let us assume that the random variables x and y follow the Gaussian distribution.
According to Sect. C.1, we have the relationship,
H(x) = 1
2 log |Σxx| and H(y) = 1
2 log |Σyy|,
(C.58)
where we ignore constants that are not related to the current arguments. We also have
H(x, y) = 1
2 log
E
 x
y
 
xT, yT
= 1
2 log

E(xxT) E(xyT)
E(yxT) E(yyT)
 = 1
2 log

Σxx ΣT
yx
Σyx Σyy
 .
(C.59)
Thus, the conditional entropy is expressed as
H(y|x) = H(x, y) −H(x) = 1
2 log

Σxx ΣT
yx
Σyx Σyy
 −1
2 log |Σxx|.
(C.60)
Using the determinant identity in Eq. (C.94), we ﬁnally obtain the formula to compute
the conditional entropy
H(y|x) = 1
2 log

Σxx ΣT
yx
Σyx Σyy
 −1
2 log |Σxx|
= 1
2 log
Σyy −ΣyxΣ−1
xx ΣT
yx
 = 1
2 log |Σee| .
(C.61)
The conditional entropy is expressed by the covariance of the residual signal e
obtained by regressing y with x.

Appendix C: Supplementary Mathematical Arguments
259
C.4 Deﬁnitions of Several Vector Norms
The pth order norm (p ≥0) of an N-dimensional vector, x = [x1, . . . , xN]T is deﬁned
such that
∥x∥p =

|x1|p + |x2|p + · · · + |xN|p1/p .
(C.62)
When p = 2, the norm is called the L2-norm, which is equal to the conventional
Euclidean norm,
∥x∥2 =
!
x2
1 + x2
2 + · · · + x2
N.
(C.63)
The L2-norm is usually denoted ∥x∥. When p = 1, the norm is expressed as
∥x∥1 = |x1| + |x2| + · · · + |xN|,
(C.64)
which is called the L1-norm. When p = 0, we can deﬁne T (x), which is called the
indicator function, such that
T (x) =
"
0 x = 0
1 x ̸= 0,
(C.65)
with the norm expressed as
∥x∥0 =
N

i=1
T (xi).
(C.66)
Namely, ∥x∥0 is equal to the number of non-zero components of x. When p = ∞,
the norm becomes
∥x∥∞= max{x1, x2, . . . , xN}.
(C.67)
Namely, the ∥x∥∞is equal to the maximum component of x.
C.5 Derivative of Functionals
This section provides a brief explanation of the derivative of functionals. A conven-
tional function relates input variables to output variables. A functional relates input
functions to output variables. A simple example of a functional is:
I =
 β
α
f (x)dx,
(C.68)
where f (x) is deﬁned in [α, β]. This I is a functional of the function f (x). The
functional is often denoted as I[f (x)].

260
Appendix C: Supplementary Mathematical Arguments
Let us consider a function F(x) that contains another function f (x). One example
is F(x) = f (x)2 + 2x where F(x) consists of f (x) and x. The integral of F(x),
I[f (x)] =
 β
α
F(x)dx,
(C.69)
is a functional of f (x). Let us deﬁne a small change in f (x) as δf (x), and a change
in I[f (x)] due to δf (x) as δI[f (x)]. For simplicity, δI[f (x)] is denoted δI in the
following arguments. To derive the relationship between δI and δf (x), the region
[α, β] is divided into small regions x. A contribution from the jth small region onto
δI, δIj, is equal to
δIj = Ajδf (xj)x.
(C.70)
Here we assume that δIj is proportional to δf (xj) and Aj is a proportional constant.
Then, δI is derived as a sum of contributions from all small regions such that
δI =

j
Ajδf (xj)x.
(C.71)
Accordingly, when x →0, the relationship between δf (x) and δI becomes
δI =
 β
α
A(x)δf (x)dx.
(C.72)
In Eq. (C.72), A(x) is deﬁned as the derivative of the functional I[f (x)] with respect
to f (x), and it is denoted δI/δf .
Let us compute this derivative of a functional. When a functional is given in
Eq. (C.68), i.e., F(x) = f (x), the amount of change δI due to the change from f (x)
to f (x) + δf (x) is expressed as
δI =
 β
α

f (x) + δf (x)

dx −
 β
α
f (x)dx =
 β
α
δf (x)dx.
(C.73)
Therefore, we obtain A(x) = δI/δf = 1. In the general case where F(x) consists of
f (x), the relationship
δI =
 β
α
[F(x) + δF(x)] dx −
 β
α
F(x)dx =
 β
α
δF(x)dx
(C.74)
holds. Using the relationship
δF(x) = ∂F
∂f δf (x),

Appendix C: Supplementary Mathematical Arguments
261
we have
δI =
 β
α
δF(x)dx =
 β
α
∂F
∂f δf (x)dx,
(C.75)
and thus,
δI
δf = ∂F
∂f .
(C.76)
As an example of the use of Eq. (C.76), let us derive the derivative of the functional
L[q(x)] in Eq.(B.53). Ignoring the last term, which does not contain q(x), Eq.(B.53)
is rewritten as
L[q(x)] =
 ∞
−∞
dxq(x) log p(x, y|θ)
−
 ∞
−∞
dxq(x) log q(x) + γ
 ∞
−∞
q(x)dx.
(C.77)
In the ﬁrst term, since F(x) = q(x) log p(x, y|θ), we have
∂F
∂q = log p(x, y|θ).
(C.78)
In the second term, since F(x) = q(x) log q(x), we have
∂F
∂q =
1
∂q(x)

q(x) log q(x)

= log q(x) + q(x) 1
q(x) = log q(x) + 1.
(C.79)
In the third term, since F(x) = γ q(x), we have
∂F
∂q = γ.
(C.80)
Therefore, we have
∂L[q(x)]
∂q(x)
= log p(x, y|θ) −log q(x) −1 + γ,
(C.81)
which is equal to Eq. (B.54).
C.6 Vector and Matrix Derivatives
Differentiating a scalar F with a column vector x is deﬁned as creating a column
vector whose jth element is equal to ∂F/∂xj. Assuming that a is a column vector and
A is a matrix, The following relationships hold,

262
Appendix C: Supplementary Mathematical Arguments
∂xTa
∂x
= ∂aTx
∂x
= a,
(C.82)
∂xTAx
∂x
= (A + AT)x,
(C.83)
∂tr(xaT)
∂x
= ∂tr(axT)
∂x
= a.
(C.84)
Let us denote the (i, j)th element of a matrix A as Ai,j, Differentiating a scalar
F with a matrix A is deﬁned as creating a matrix whose (i, j)th element is equal
to ∂F/∂Ai,j. Representative identities are the following, where x and y are column
vectors and A and B are matrices.
∂tr(A)
∂A
= I,
(C.85)
∂tr(AB)
∂A
= BT,
(C.86)
∂tr(ATB)
∂A
= B,
(C.87)
∂tr(ABBT)
∂A
= A(B + BT),
(C.88)
∂xTAy
∂A
= xyT,
(C.89)
∂log |A|
∂A
= (A−1)T.
(C.90)
C.7 Several Formulae for Matrix Computations
Used in this Book
The following are representative formulae for the matrix inversion.
(A + BD−1C)−1 = A−1 −A−1B(D + CA−1B)−1CA−1,
(C.91)
(A−1 + BTC−1B)−1BTC−1 = ABT(BABT + C)−1.
(C.92)
Also, we have
 A B
C D
−1
=

M
−MBD−1
−D−1CM D−1 + D−1CMBD−1

,
(C.93)
where
M = (A −BD−1C)−1.

Appendix C: Supplementary Mathematical Arguments
263
Regarding the matrix determinant, we have the following identity,

A B
C D
 = |A||D −CA−1B| = |D||A −BD−1C|.
(C.94)
When a matrix A is an invertible (n × n) matrix, and B and C are (n × m) matrices,
the following matrix determinant lemma holds:
|A||I + BTA−1C| = |A + CBT|.
(C.95)
C.8 Properties of Eigenvalues
Representative properties of eigenvalues that may be used in this book are listed
below.
1. If A is a Hermitian matrix and positive deﬁnite, all eigenvalues are real and
greater than 0.
2. If A is a Hermitian matrix and positive semideﬁnite, all eigenvalues are real and
greater than or equal to 0.
3. If A is a real symmetric matrix and positive deﬁnite, all eigenvalues are real and
greater than 0. Eigenvectors are also real.
4. If A is a real symmetric matrix and positive semideﬁnite, all eigenvalues are real
and greater than or equal to 0. Eigenvectors are also real.
5. If A and B are square matrices and B is nonsingular, eigenvalues of A are also
eigenvalues of B−1AB.
6. If A and B are square matrices and B is unitary, eigenvalues of A are also
eigenvalues of BHAB.
7. If A and B are square matrices and B is orthogonal, eigenvalues of A are also
eigenvalues of BTAB.
8. If A and B are square matrices and B is positive deﬁnite, eigenvalues of BA are
also eigenvalues of B1/2AB1/2.
9. If A and B are square matrices and B is positive deﬁnite, eigenvalues of B−1A
are also eigenvalues of B−1/2AB−1/2.
10. Let us assume that A is an (m × n) matrix, B is an (n × m) matrix, and n ≥m. If
λ1, . . . , λm are eigenvalues of AB, λ1, . . . , λm, 0, . . . , 0 are eigenvalues of BA.
C.9 Rayleigh-Ritz Formula
This section provides a proof of the Rayleigh-Ritz formula, which is according to [2].
We deﬁne A and B as positive deﬁnite matrices of the same dimension. We introduce
the following notations and use them throughout the book:

264
Appendix C: Supplementary Mathematical Arguments
• The minimum and maximum eigenvalues of a matrix A are denoted Smin{A} and
Smax{A}.
• The eigenvectors corresponding to the minimum and maximum eigenvalues of a
matrix A are denoted ϑmin{A} and ϑmax{A}.
• The minimum and maximum generalized eigenvalues of a matrix A with a metric
B are denoted Smin{A, B} and Smax{A, B}, and the corresponding eigenvectors are
denoted ϑmin{A, B} and ϑmax{A, B}.
Here, if the matrix B is nonsingular, the following relationships hold:
Smax{A, B} = Smax{B−1A},
ϑmax{A, B} = ϑmax{B−1A},
Smin{A, B} = Smin{B−1A},
ϑmin{A, B} = ϑmin{B−1A}.
Using x to denote a column vector with its dimension commensurate with the size
of the matrices, this appendix shows that
max
x
xTAx
xTBx = Smax{A, B},
(C.96)
and
argmax
x
xTAx
xTBx = ϑmax{A, B}.
(C.97)
Since the value of the ratio (xTAx)/(xTBx) is not affected by the norm of x, we set
the norm of x so as to satisfy the relationship xTBx = 1. Then, the maximization
problem in Eq. (C.96) is rewritten as
max
x
xTAx subject to xTBx = 1.
(C.98)
We change this constrained maximization problem to an unconstrained maximization
problem by introducing the Lagrange multiplier κ. We deﬁne the Lagrangian L(x, κ)
such that
L(x, κ) = xTAx −κ(xTBx −1).
(C.99)
The maximization in Eq. (C.98) is equivalent to maximizing L(x, κ) with no con-
straints.
To obtain the maximum of L(x, κ), we calculate the derivatives
∂L(x, κ)
∂x
= 2(Ax −κBx),
(C.100)
∂L(x, κ)
∂κ
= −(xTBx −1).
(C.101)

Appendix C: Supplementary Mathematical Arguments
265
By setting these derivatives to zero, we can derive the relationships, Ax = κBx
and κ = xTAx. Therefore, the maximum value of xTAx is equal to the maximum
eigenvalue of Ax = κBx, and the x that attains this maximum value is equal to the
eigenvector corresponding to this maximum eigenvalue. Namely, we have
max
x
xTAx
xTBx = Smax{A, B} = Smax{B−1A},
and
argmax
x
xTAx
xTBx = ϑmax{A, B} = ϑmax{B−1A}.
Using exactly the same derivation, it is easy to show that
min
x
xTAx
xTBx = Smin{A, B},
(C.102)
and
argmin
x
xTAx
xTBx = ϑmin{A, B}.
(C.103)
References
1. F. D. Neeser and J. L. Massey, “Proper complex random processes with applica-
tions to information theory,” IEEE Transactions on Information Theory, vol. 39,
pp. 1293–1302, 1993.
2. F. R. Gantmacher, The Theory of Matrices. New York, NY: Chelsea Publishing
Company, 1960.

Index
Symbols
L0-norm minimization, 20
L1-norm, 259
L1-norm regularization, 26
L1-regularized minimum-norm solution, 19
L2-norm, 259
L2-norm
regularized
minimum-norm
method, 25
L2-regularized minimum-norm solution, 17
Lp-norm regularization, 22
A
Action potential, 215
Adaptive beamformer, 29
Alternative cost function, 59
Amplitude–amplitude coupling (AAC), 200
Amplitude–phase diagram, 202, 206, 211
Analytic signal, 202
Array measurement, 9
Array-gain constraint, 31, 35
Augmented factor vector, 102
Automatic relevance determination (ARD),
123
Auxiliary variables , 60
Average data likelihood, 237
Average log likelihood, 77
Axon, 215
B
Bayes’ rule, 24, 231
Bayesian beamformer, 36
Bayesian factor analysis, 75
Bayesian inference, 231
Bayesian minimum-norm method, 25
Bayesian modeling framework, 121
Beam response, 47
Beamformer, 29
Bioelectromagnetic forward modeling, 215
Biot-Savart law, 225
Bounded conductor with piecewise-constant
conductivity, 226
Brain oscillation, 199
Brain rhythm, 199
C
Canonical correlation, 252
Canonical imaginary coherence, 154
Canonical magnitude coherence, 151
Capacitive effect, 223
Champagne algorithm, 51
Charge conservation law, 219
Circular, 250
Coherence, 183
Coherence of the MVAR process, 173
Complete data likelihood, 237
Complex Gaussian distribution, 249
Concave function, 59
Concentration gradient, 217
Conditional entropy, 258
Convexity-based algorithm, 59
Convexity-based update, 126
Corrected imaginary coherence, 145
Cortical anatomy, 216
Cortico-cortical connection, 216
Covariance component, 122, 133
Covariance of residual, 257
Cross-frequency coupling, 200
Cross-location PAC, 201, 209
© Springer International Publishing Switzerland 2015
K. Sekihara and S.S. Nagarajan, Electromagnetic Brain Imaging,
DOI 10.1007/978-3-319-14947-9
267

268
Index
D
Data evidence, 54, 236
Data likelihood, 231
Data vector, 9
Derivative of functional, 259
Determinant identity, 263
Determinant lemma, 263
Diagonal-loading, 35
Dielectric permittivity, 223
Dipole model, 225
Directed transfer function (DTF), 182
Dual-state beamformer, 42
E
EM algorithm, 25, 77, 191
EM update, 58, 126
Empirical Bayesian, 123
Empirical Bayesian schema, 51
Entropy, 248
Envelope coherence, 160
Envelope correlation, 159
ERD/ERS activities, 209
Event-related desynchronization (ERD), 42
Event-related power change, 209
Event-related spectral power change, 42
Event-related synchronization (ERS), 42
F
F-ratio image, 44
Factor analysis model, 75
Five-dimensional brain imaging, 44
FOCUSS, 128
Free energy, 88, 92, 105, 242
G
Gamma distribution, 27
Gamma-band oscillation, 199
Gauge Transformations, 220
Gauss theorem, 225, 227
Gaussian prior, 51
Gaussian scale mixtures, 122
Generative model, 122
Geselowitz formula, 227
Geweke measure, 178
Global interaction measure (GIM), 157
Grangercausalityforabivariateprocess,174
Granger-causality measures, 171
Gulrajani gauge, 220
H
Head tissue conductivity, 224
High-frequency oscillator, 200
Hilbert transform, 202
Hyperparameter, 69, 236
Hyperparameter MAP, 123
I
Ideal gas constant, 217
Imaginary coherence, 139
Impressed current, 219
Inhibitory synaptic connections, 216
Instantaneous amplitude, 202
Instantaneous dependence, 178
Instantaneous interaction, 142
Instantaneous phase, 202
Intensity bias, 145
Interference mixing matrix, 96
Interference suppression, 75
Intracortical connection, 216
Inverse-Gamma, 124
K
Kullback–Leibler distance, 203, 244
Kullback-Leibler divergence, 132
L
Lagrange multiplier, 15, 30, 242
Lagrangian, 242, 264
Laplace approximation, 132
Laplace distribution, 26
Lead-ﬁeld matrix, 10
Lead-ﬁeld vector, 10
Leakage effects, 143
Linearly
constrained
minimumvariance
(LCMV) beamformer, 39
Local PAC, 201
Local PAC analysis, 206
Log-likelihood function, 14
Low-frequency oscillator, 200
M
MacKay update, 58, 126
Magnetic scalar potential, 228
Magnetic vector potential, 219
Marginal distribution, 235
Marginal likelihood, 54, 79, 236
Maximum a posteriori (MAP) estimate, 232
Maximum likelihood principle, 13
Maximum statistics, 161

Index
269
Maxwell’s equations, 218
MCE, 128
Mean imaginary coherence (MIC) mapping,
162
Mean-Field approximation, 131
Method of least-squares, 14
Minimum mean squared error (MMSE) esti-
mate, 233
Minimum-norm ﬁlter, 44
Minimum-norm solution, 15
Minimum-variance beamformer, 30
Mixing matrix, 76
Model data covariance, 236
Model order, 75
Modulation index (MI), 203
Modulation index analysis, 205
Multi-dimensional Gaussian distribution,
247
Multiple comparison problem, 161
Multivariate Granger causality, 175
Multivariate interaction measure (MIM),
156
Multivariate vector auto-regressive (MVAR)
process, 171, 190
Mutual information, 146, 188, 255
N
Narrow-band beamformer, 42
Neuronal current, 215
Neurotransmitter, 216
Noise precision, 239
Non-informative prior, 232
Non-zero-lag correlation, 142
Nonadaptive spatial ﬁlter, 44
O
Objective function, 22
Ohm’s law, 217
Ohmic current, 219
P
Parseval’s theorem, 142
Partial coherence, 173
Partial directed coherence, 184
Partitioned factor analysis (PFA), 96
Penalized likelihood method, 127
Phase of ﬁring, 199
Phase–amplitude coupling (PAC), 200
Phase-informed TF map, 206, 212
Phase-informed time-frequency map, 204
Phase–phase coupling (PPC), 200
Posterior probability, 231
Postsynaptic terminal, 216
Potential equation, 221
Preferred phases, 212
Prior precision, 238
Prior probability distribution, 231
Proper, 249
Pyramidal cells, 216
R
Rayleigh-Ritz formula, 263
Recursive
null-Steering
(RENS)
beam-
former, 47
Residual canonical coherence, 157
Residual coherence, 148
Residual envelope correlation, 160
Resolution matrix, 16
Restricted maximum likelihood (ReML),
125
S
Saketini algorithm, 101
Same-frequency coupling (SFC), 200
Sarvas formula, 228
Scalar potential, 220
Scalar-type adaptive beamformer, 36
Seed point, 140
Seed voxel, 141
Semi-Bayesian derivation, 33
Semi-Bayesian formulation, 40
Singular value decomposition, 16
Singular-value spectrum, 17
sLORETA ﬁlter, 46
Somatic voltage, 215
Source current, 219
Source MAP, 127
Source space, 11
Source space causality analysis, 171
Source vector, 10
Source-space connectivity analysis, 139
Sparse Bayesian learning, 26, 51
Sparse Bayesian (Champagne) algorithm,
191
Sparse solution, 20, 22
Sparsity, 63
Spatial ﬁlter, 29
Spectral Granger causality, 178
Spherically-symmetric homogeneous con-
ductor, 227
Student t-distribution, 27
Surrogate data method, 161, 206
Synapses, 215

270
Index
Synaptic connection, 216
T
Target location, 140
Target voxel, 141
Temporal encoding, 199
Time-Domain Granger causality, 174
Total interdependence, 177
Transfer entropy, 185
Transmembrane voltage, 215, 217
Type-II maximum likelihood, 125
U
Unbounded homogeneous medium, 224
Unit-gain constraint, 30, 34
Unit-noise-gain constraint, 32
V
Variational Bayes EM algorithm (VBEM),
84
Variational Bayes Factor Analysis (VBFA),
82
Variational Bayesian approximation, 131
Variational Bayesian inference, 241
VBEM algorithm, 97, 103
Vector norm, 259
Vector-type beamformer, 38
Virtual sensor, 29
Voxel, 11
Voxel-by-voxel statistical threshold, 161
W
Weight-normalized minimum-norm ﬁlter,
46
Weight-normalized
minimum-variance
beamformer, 33
Y
Yule-Walker equation, 191
Z
Zero-time-lag correlation, 142

