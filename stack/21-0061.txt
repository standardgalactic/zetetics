Journal of Machine Learning Research 23 (2022) 1-71
Submitted 1/21; Revised 10/21; Published 1/22
Score Matched Neural Exponential Families
for Likelihood-Free Inference
Lorenzo Pacchiardi
lorenzo.pacchiardi@stats.ox.ac.uk
Department of Statistics
University of Oxford
Oxford, OX1 3LB
United Kingdom
Ritabrata Dutta
ritabrata.dutta@warwick.ac.uk
Department of Statistics
University of Warwick
Coventry, CV4 7AL
United Kingdom
Editor: Mohammad Emtiyaz Khan
Abstract
Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distribu-
tions for stochastic models with intractable likelihood, by relying on model simulations. In
Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics
are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to
the observation in order to sample from an approximate posterior, whose form depends
on the chosen statistics. In this work, we introduce a new way to learn ABC statistics:
we ﬁrst generate parameter-simulation pairs from the model independently on the ob-
servation; then, we use Score Matching to train a neural conditional exponential family to
approximate the likelihood. The exponential family is the largest class of distributions with
ﬁxed-size suﬃcient statistics; thus, we use them in ABC, which is intuitively appealing and
has state-of-the-art performance. In parallel, we insert our likelihood approximation in an
MCMC for doubly intractable distributions to draw posterior samples. We can repeat that
for any number of observations with no additional model simulations, with performance
comparable to related approaches. We validate our methods on toy models with known
likelihood and a large-dimensional time-series model.
Code for reproducing the experiments is available at https://github.com/LoryPack/
SM-ExpFam-LFI.
Keywords:
Likelihood-Free Inference, Score Matching, Approximate Bayesian Compu-
tation, MCMC for doubly intractable distributions, Exponential Family.
1. Introduction
Stochastic simulator models are used to simulate realizations of physical phenomena; usu-
ally, a set of parameters θ govern the simulation output. As the model is stochastic, repeated
simulations with ﬁxed θ yield diﬀerent outputs; their distribution is the model’s likelihood.
Simple models provide an analytic expression of the likelihood, which is however unavailable
for more complex ones.
c⃝2022 Lorenzo Pacchiardi and Ritabrata Dutta.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v23/21-0061.html.

Pacchiardi and Dutta
Upon observing a real-world realization of the phenomenon the model is describing,
researchers may want to obtain a posterior distribution over parameters. If the likelihood
is known, standard Bayesian inference tools (such as MCMC or variational inference) allow
to get posterior samples; if otherwise the likelihood is missing, Likelihood-Free Inference
(LFI) techniques are the only viable solution. LFI has been applied in several domains,
including genomics (Tavar´e et al., 1997), biological science (Dutta et al., 2018), meteorology
(Hakkarainen et al., 2012), geological science (Pacchiardi et al., 2020), genomics (Tavar´e
et al., 1997; Toni et al., 2009; Marttinen et al., 2015), and epidemiology (McKinley et al.,
2018; Minter and Retkute, 2019; Dutta et al., 2021a).
In some LFI techniques (Wood, 2010; Thomas et al., 2020; Price et al., 2018), the
missing likelihood is replaced with an explicit approximation built from model simulations
at each parameter value. Alternatively, Approximate Bayesian Computation (ABC) (Marin
et al., 2012; Lintusaari et al., 2017) circumvents the unavailability of the likelihood by
comparing model simulations with the observation according to some notion of distance.
To reduce data dimensionality, ABC usually relies on summary statistics, whose choice is
not straightforward. Recently, the expressive capabilities of Neural Networks (NNs) have
been leveraged to learn ABC statistics (Jiang et al., 2017; Wiqvist et al., 2019; Pacchiardi
et al., 2020; ˚Akesson et al., 2020). These techniques train NNs parametrizing the statistics
by minimizing a suitable loss on a training set of parameter-simulation pairs generated from
the model.
In the present work, we propose a new way to learn ABC statistics with NNs. Most pre-
vious works (Jiang et al., 2017; Wiqvist et al., 2019; ˚Akesson et al., 2020) trained a single NN
with the regression loss introduced in Fearnhead and Prangle (2012). Instead, we consider
an exponential family with two NNs parametrizing respectively the suﬃcient statistics and
natural parameters and ﬁt this to the model. As the exponential family is the most general
class of distributions with suﬃcient statistics of ﬁxed size (Appendix A), it makes intuitive
sense to use the learned suﬃcient statistics in ABC. Indeed, this approach empirically yields
superior or equivalent performance with respect to state-of-the-art approaches.
As in previous approaches, we consider a training set of parameter-simulation pairs;
however, we train the NNs with Score Matching (SM) or its Sliced approximation (SSM),
which do not require evaluating the normalizing constant of the exponential family. We
extend the SM and SSM objectives to the setting of conditional densities, thus ﬁtting the
likelihood approximation for all values of data and parameters.
In contrast to related approaches (Jiang et al., 2017; Wiqvist et al., 2019; Pacchiardi
et al., 2020; ˚Akesson et al., 2020), our method provides a full likelihood approximation. We
test therefore direct sampling from the corresponding posterior (in place of ABC) with an
MCMC algorithm for doubly intractable distributions. This approach achieves satisfactory
performance with no additional model simulations and is therefore a valid alternative to
standard LFI schemes for expensive simulator models. The computational gain is even larger
when inference is performed for several observations, as the same likelihood approximation
can be used.
The rest of our paper is organized as follows. In Section 2, we brieﬂy review some LFI
methods. Next, in Section 3 we introduce the neural conditional exponential family and
show how to ﬁt it with SM or SSM. In Section 4 we discuss how to exploit the exponential
family to extract ABC statistics or for direct sampling. We extensively validate our proposed
2

Score Matched Neural Exponential Families for LFI
approaches in Section 5. Finally, we discuss related works in Section 6 and conclude in
Section 7, where we also highlight directions for future research.
1.1 Notation
We set here notation for the rest of our manuscript. We will denote respectively by X ⊆Rd
and Θ ⊆Rp the data and parameter space. Upper case letters will denote random variables
while lower case ones will denote observed (ﬁxed) values. Subscripts will denote vector
components and superscript in brackets sample indices, while ∥· ∥will denote the ℓ2 norm.
2. Likelihood-Free Inference
Let us consider a model which allows to generate a simulation x ∈X at any parameter
value θ ∈Θ, but for which it is not possible to evaluate the likelihood p0(x|θ). Given an
observation x0 and a prior on the parameters π(θ), Bayesian inference obtains the posterior
distribution π0(θ|x0) = π(θ)p0(x0|θ)
p0(x0)
. However, obtaining that explicitly (or even sampling
from it with Markov Chain Monte Carlo, MCMC) is impossible without having access to
the likelihood, at least up to a normalizing constant that is independent on x.
Likelihood-Free Inference techniques yield approximations of the posterior distribution
by replacing likelihood evaluations with model simulations. Broadly, they can be split into
two kinds of approaches: Surrogate Likelihood, which explicitly builds a likelihood function,
and Approximate Bayesian Computation (ABC), which instead uses discrepancy between
simulated and observed data. We detail those two approaches in the following.
2.1 Approximate Bayesian Computation (ABC)
Approximate Bayesian Computation (ABC, Marin et al. 2012; Lintusaari et al. 2017) algo-
rithms sample from an approximate posterior distribution by ﬁnding parameter values that
yield simulated data resembling the observations to a suﬃcient degree. Usually, ABC meth-
ods rely on statistics function to reduce the dimensionality of simulated and observed data
and thus improve the computational eﬃciency of the algorithm. Therefore, the similarity
between simulated and observed data is normally deﬁned via a distance function between
the corresponding statistics. ABC algorithms sample from:
πϵ(θ|s(x0)) ∝π(θ)
Z
X
1[d(s(x0), s(xsim)) ≤ϵ] p0(xsim|θ)dxsim,
where s and d denote respectively the statistics function and the discrepancy measure, while
1[·] is an indicator function and ϵ is an acceptance threshold.1
It is easy to sample from πϵ(θ|s(x0)) with rejection, by generating (θ(j), x(j)) ∼π(θ)p0(x|θ)
and accepting θ(j) if d(s(x0), s(x(j))) ≤ϵ. This is ineﬃcient when ϵ is small, as most simula-
tions end up being rejected; more eﬃcient ABC algorithms have been developed, based for
instance on Markov Chain Monte Carlo (Marjoram et al., 2003), Population Monte Carlo
1. In general, 1(d(s(x0), s(xsim))) can be replaced with Kϵ(d(s(x0), s(xsim))), where Kϵ is a rejection kernel
which concentrates around 0 and goes to 0 for argument going to ±∞, and whose width decreases when
ϵ decreases. We restrict here on 1, which is the most common choice.
3

Pacchiardi and Dutta
(Beaumont, 2010) or Sequential Monte Carlo (Del Moral et al., 2012), which reach lower
values of ϵ with the same computational budget.
If d is a distance measure and the statistic s is suﬃcient for p(x|θ), then the ABC poste-
rior πϵ(θ|s(x0)) converges to the true one as ϵ →0 (Lintusaari et al. 2017; see Appendix A
for deﬁnition of suﬃcient statistics).
In general, however, suﬃcient statistics are unavailable so that ABC can at best give
an approximation of the exact posterior even with ϵ →0. Hand-picking the best summary
statistics for a given model limits the applicability of ABC. Therefore, some approaches
to automatically build them have been developed (Fearnhead and Prangle, 2012; Nunes
and Balding, 2010; Joyce and Marjoram, 2008; Aeschbacher et al., 2012; Blum et al., 2013;
Pacchiardi et al., 2020). We describe here the approach that is most commonly used.
ABC with semi-automatic statistics.
The summary statistic s⋆(x) = E[θ|x] yields an
approximate posterior πϵ(θ|s⋆(x0)) whose mean minimizes the expected squared error loss
from the true parameter value, as ϵ →0 (Fearnhead and Prangle, 2012). Of course, E[θ|x] is
unknown; therefore, Fearnhead and Prangle (2012) suggested to consider a set of functions
sβ(x) and ﬁnd the one that best approximates E[θ|x].
Empirically, this corresponds to
ﬁnding:
ˆβ = arg min
β
1
N
N
X
j=1
sβ(x(j)) −θ(j)
2
2 ,
(1)
where (θ(j), x(j))N
j=1, θ(j) ∼π(θ), x(j) ∼p0(·|θ(j)) are N data-parameter pairs generated
before performing ABC.
In practice, Fearnhead and Prangle (2012) considered sβ(x) to be a linear function of
the weights β, such that solving the minimization problem in Eq. (1) amounts to linear
regression. Jiang et al. (2017); Wiqvist et al. (2019) and ˚Akesson et al. (2020) instead used
Neural Networks to parametrize sβ and train them to minimize the loss in Eq. (1).
2.2 Surrogate Likelihood
Surrogate likelihood approaches exploit simulations to build an explicit likelihood approxi-
mation, which is then inserted in standard likelihood-based sampling schemes (say, MCMC;
Wood 2010; Thomas et al. 2020; Fasiolo et al. 2018; An et al. 2019, 2020; Price et al. 2018).
Here, we discuss two methods that fall under this framework.
Synthetic Likelihood.
Synthetic Likelihood (SL, Wood 2010) replaces the exact likeli-
hood with a normal distribution for the summary statistics s = s(x); speciﬁcally, it assumes
S|θ ∼N(µθ, Σθ), where the mean vector µθ and covariance matrix Σθ depend on θ. For
each θ, an estimate of µθ and Σθ is built with model simulations, and the likelihood of the
observed statistics s(x0) is evaluated.
Using the statistics likelihood to deﬁne a posterior distribution yields a Bayesian SL
(BSL). In Price et al. (2018), MCMC is used to sample from the BSL posterior; as in
ABC, this approach requires generating simulations for each considered θ. However, one
single simulation per parameter value is usually suﬃcient in ABC, while estimating µθ, Σθ
in BSL requires multiple simulations. Nevertheless, a parametric likelihood approximation
seemingly allows scaling to larger dimensional statistics space (Price et al., 2018).
4

Score Matched Neural Exponential Families for LFI
Ratio Estimation.
Thomas et al. (2020) tackle LFI by estimating the ratio between
likelihood and data marginal: r(x, θ) = p(x|θ)/p(x). As r(x, θ) ∝p(x|θ) with respect to
θ, this can be inserted in a likelihood-based sampling scheme in order to sample from the
posterior, similarly to BSL. This method is termed Ratio Estimation (RE).
In practice, for each value of θ, an estimate ˆr(x, θ) is built by generating simulations
from the model p0(x|θ) and from the marginal2 p(x), and then ﬁtting a logistic regression
discriminating between the two sets of samples. In fact, logistic regression attempts to ﬁnd
a function ˆh(x; θ) for which eˆh(x;θ) ≈r(x; θ).
In Thomas et al. (2020), ˆh is chosen in the class of functions hβ(x; θ) = Pk
i=1 βi(θ)ψi(x) =
β(θ)T ψ(x), where ψ is a vector of summary statistics and β(θ) are coeﬃcients determined
independently for each θ; this therefore boils down to a linear logistic regression. Viewed
diﬀerently, this approach approximates the likelihood with an exponential family, as in fact
it assumes p(x|θ) ∝ˆr(x, θ) = exp(ˆβ(θ)T ψ(x)), for some coeﬃcients ˆβ determined by data;
it is thus a more general likelihood assumption than SL (as the normal distribution belongs
to the exponential family).
Remark 1 (Reducing the number of simulations.) Several approaches have attempted
to reduce the number of simulations required in SL, RE and ABC. Speciﬁcally, Gaussian
Processes can be exploited to replace evaluations of simulation-based quantities with emu-
lated values, while at the same time providing an uncertainty quantiﬁcation used to guide
the next model simulations; that has been done for both ABC (Meeds and Welling, 2014;
Wilkinson, 2014; Gutmann et al., 2016; J¨arvenp¨a¨a et al., 2019; Jarvenpaa et al., 2020)
and SL (Meeds and Welling, 2014; Wilkinson, 2014; Moores et al., 2015; J¨arvenp¨a¨a et al.,
2021). These approaches all consider a ﬁxed observation x and emulate over parameter
values.
In Hermans et al. (2020), a classiﬁer more powerful than logistic regression is used to
learn the likelihood ratio estimate for all (x, θ) using parameter-simulation pairs (similarly
to what we propose in Section. 3.3), amortizing therefore RE with respect to the observation.
3. Likelihood approximation with the exponential family
In this Section, we ﬁrst introduce the parametric family which we use to approximate the
likelihood.
Then, we describe Score Matching (SM) and Sliced Score Matching (SSM),
which allow us to ﬁt distributions with unknown normalizing constants to data. Finally,
we discuss how to extend SM and SSM to the setting of conditional densities, to obtain a
likelihood approximation valid for all data and parameter values.
3.1 Conditional exponential family
A probability distribution belongs to the exponential family if it has a density of the fol-
lowing form:3
2. Simulating from the marginal can be done by drawing θj ∼p(θ), xj ∼p(x|θj) and discarding θj.
3. As we are concerned with continuous random variables, across the work we use the Lebesgue measure
as a base measure, without explicitly referring to it.
5

Pacchiardi and Dutta
p(x|θ) = eη(θ)T f(x)h(x)
Z(θ)
,
(2)
where x ∈X, θ ∈Θ.
Here, f : X →Rds is a function of the data (suﬃcient statis-
tics), η : Θ →Rds is a function of the parameters (natural parameters) and h(x) :
X →R is a scalar function of data (base measure). The normalizing constant Z(θ) =
R
X exp{η(θ)T f(x)}h(x)dx is intractable and assumed to be ﬁnite ∀θ ∈Θ; we will discuss
later (Sec. 3.3) why this assumption is not an issue.
Across this work, we refer to Eq. (2) as conditional exponential family to stress that we
learn an approximation valid for all x’s and θ’s by selecting functions f, η and h.
In the following, we rewrite Eq. (2) as: p(x|θ) = exp(¯η(θ)T ¯f(x))/Z(θ), where ¯η(θ) =
(η(θ), 1) ∈Rds+1 and ¯f(x) = (f(x), log h(x)) ∈Rds+1. For simplicity, we will drop the bar
notation, using as convention that η(θ) contains the natural parameters of the exponential
family plus an additional constant term, and that the last component of f(x) is log h(x).
Neural conditional exponential family.
Let now fw and ηw denote two Neural Net-
works (NNs) with collated weights w (in practice the two NNs do not share parameters);
then, the neural conditional exponential family is deﬁned as:
pw(x|θ) = eηw(θ)T fw(x)
Zw(θ)
.
(3)
Remark 2 Ratio Estimation (Sec. 2.2) parametrizes the likelihood as an exponential family
with user-speciﬁed statistics and natural parameters β independently learned for each θ ∈Θ.
In contrast, here we learn both the fw(x) and the ηw(θ) transformations over all X and Θ
at once by selecting the best w.
Remark 3 (Identiﬁability) For a family of distributions indexed by a parameter φ, iden-
tiﬁability means that pφ(x|θ) = pφ′(x|θ) ∀x, θ =⇒φ = φ′. The weights w in the neural
conditional exponential family are not identiﬁable for two reasons: ﬁrst, NNs have many
intrinsic symmetries. Secondly, replacing fw in Eq. (3) with A · fw and ηw with (AT )−1 · ηw
does not change the probability density. In Khemakhem et al. (2020), two suitable con-
cepts of function identiﬁability up to some linear transformations are deﬁned. fw and ηw in
Eq. (3) are identiﬁable according to those deﬁnitions, under strict conditions on the archi-
tectures of the Neural Networks parametrizing them (Khemakhem et al., 2020). Moreover,
they empirically verify that NNs not satisfying the above assumptions result in approximately
identiﬁable fw and ηw, according to their deﬁnition. More details are given in Appendix B.1.
Remark 4 (Universal approximation) Using larger ds in Eq. (2) increases the expres-
sive power of the approximating family.
Khemakhem et al. (2020) proved that, by con-
sidering a freely varying ds and generic f and η, the conditional exponential family has
universal approximation capabilities for the set of conditional probability densities; we give
more details in Appendix B.2. This result does not consider the practicality of ﬁtting the
approximating family to data, which arguably becomes harder when ds increases.
6

Score Matched Neural Exponential Families for LFI
3.2 Score Matching
In order to ﬁt a parametric density pw to data, the standard approach is ﬁnding the Maxi-
mum Likelihood Estimator (MLE) for w; in the limit of inﬁnite data, that corresponds to
minimizing the Kullback-Leibler divergence from the data distribution. MLE requires how-
ever the normalizing constant of pw to be known, which is not the case for our approximating
family (Eq. 3).
Score Matching (SM, Hyv¨arinen 2005) is a possible way to bypass the intractability of
the normalizing constant. In this Subsection, we review SM for unconditional densities,
discuss a faster version and provide an extension for distributions with bounded domain.
The original Score Matching (SM)
Let us discard now the conditional dependency
on θ and, following Hyv¨arinen (2005), consider a random variable X distributed according
to p0(x). We want to use samples from p0 to ﬁt a generic model pw(x) = ˜pw(x)/Zw, where
˜pw is unnormalized and Zw is intractable.
Deﬁnition 5 Score Matching (SM) corresponds to ﬁnding:
arg min
w
DF (p0∥pw),
where the Fisher Divergence DF is deﬁned as:
DF (p0∥pw) = 1
2
Z
X
p0(x)∥∇x log p0(x) −∇x log pw(x)∥2dx.
(4)
The Fisher divergence depends only on the the logarithmic derivatives ∇x log p0(x) and
∇x log pw(x), which are termed scores4; computing DF does not require therefore knowing
the normalizing constant, as in fact:
∇x log pw(x) = ∇x(log ˜pw(x) −log Zw) = ∇x log ˜pw(x).
Nevertheless, a Monte Carlo estimate of DF in Eq. (4) using samples from p0 would re-
quire knowing the logarithmic gradient of p0; for this reason, Eq. (4) is usually termed
implicit Fisher divergence. If X = Rd and under mild conditions, Theorem 1 in Hyv¨arinen
(2005) obtains an equivalent form for DF in which p0 only appears as the distribution over
which expectation needs to be computed. We give here a similar result holding for more
general X = Nd
i=1(ai, bi), where ai, bi ∈R ∪{±∞}; speciﬁcally, we consider the following
assumptions:
A1 p0(x) ∂
∂xi log pw(x) →0 when xi ↘ai and xi ↗bi, ∀w, i,
A2 Ep0[∥∇x log p0(X)∥2] < ∞, Ep0[∥∇x log pw(X)∥2] < ∞, ∀w,
A3 Ep0

∂2
∂xi∂xj log pw(X)
 < ∞, ∀w, ∀i, j = 1, . . . , d.5
4. In most of the statistics literature, score usually refer to the derivative of the log-likelihood with respect
to the parameter; here, the nomenclature is slightly diﬀerent.
5. We remark that this assumption was not present in Hyv¨arinen (2005), but it is necessary to apply
Fubini-Tonelli theorem in the proof (see Appendix C.1), as already discussed in Yu et al. (2019).
7

Pacchiardi and Dutta
With the above, we can state the following:
Theorem 6 Let X = Nd
i=1(ai, bi), where ai, bi ∈R ∪{±∞}. If p0(x) is diﬀerentiable and
pw(x) doubly diﬀerentiable over X, then, under assumptions A1, A2, A3, the objective in
Eq. (4) can be rewritten as:
DF (p0∥pw) =
Z
X
p0(x)
d
X
i=1
"
1
2
∂log pw(x)
∂xi
2
+
∂2 log pw(x)
∂x2
i
#
dx + C.
(5)
Here, C is a constant w.r.t. pw and xi is the i-th coordinate of x.
A proof is given in Appendix C.1. We refer to Eq. (5) as the explicit Fisher Divergence, as
it is now immediate to build an unbiased Monte Carlo estimate using samples from p0.
From Eq. (4), it is clear that DF (p0∥pw) ≥0 ∀w, and that choosing pw = p0 implies
DF (p0∥pw) = 0; however, the converse can not be said in general unless p0 is supported
over all X, as stated in the following theorem:
Theorem 7 (Theorem 2 in Hyv¨arinen, 2005) Assume p0(x) > 0 ∀x ∈X. Then:
DF (p0∥pw) = 0 ⇐⇒p0(x) = pw(x) ∀x ∈X.
We prove Theorem 7 in Appendix C.2. If p0(x) is zero for some x ∈X, there could be a
distribution pw such that DF (p0∥pw) = 0 even if pw ̸= p0 (as discussed in Appendix C.6).
Sliced Score Matching (SSM)
In the explicit Fisher Divergence (Eq. 5), the second
derivatives of the log-density with respect to all components of x are required. When using
the neural conditional exponential family (Eq. 3), this amounts to evaluating the second
derivatives of fw with respect to its input (see Appendix C.10). Practically, automatic
diﬀerentiation libraries to obtain the derivatives eﬀortlessly; however, the computational
cost of the second derivatives is substantial, as it requires a number of forward and backward
passes proportional to the dimension of the data x (see Appendix E.1)6.
Some approaches to reducing the computational burden have been proposed; we review
some in Section 6. Here, we consider Sliced Score Matching (SSM, Song et al. 2020), which
considers projections of the scores on random directions; matching the projections on all
random directions ensures the two distributions are identical (under the same conditions as
the original SM). More precisely, let v ∈V ⊆Rd be a noise vector with a distribution q;
SSM is deﬁned as follows:
Deﬁnition 8 Sliced Score Matching (SSM) corresponds to ﬁnding:
arg min
w
DFS(p0∥pw),
where the Sliced Fisher Divergence is deﬁned as:
DFS(p0∥pw) = 1
2
Z
V
q(v)
Z
X
p0(x)(vT ∇x log p0(x) −vT ∇x log pw(x))2dxdv.
(6)
6. Computing the derivatives during the forward pass oﬀers some speedup, as automatic diﬀerentiation re-
peats some computations several times. However, this approach requires custom NN implementation (see
Appendix E.1.1) and is thus not scalable to complex architectures. More importantly, the computational
gain is limited with respect to what is achieved by the Sliced SM version introduced next.
8

Score Matched Neural Exponential Families for LFI
We will require the noise distribution q(v) to satisfy the following Assumption:
A4 For the random vector V ∼q, the covariance matrix E[V V T ] ≻0 is positive deﬁnite
and E∥V ∥2
2 < ∞.
As for SM, we can obtain an explicit formulation from the implicit one in Eq. (6). This
is done in the following Theorem, which we prove for convenience in Appendix C.3:
Theorem 9 (Theorem 1 in Song et al. (2020)) Let X = Nd
i=1(ai, bi), where ai, bi ∈
R ∪{±∞}. If p0(x) is diﬀerentiable and pw(x) doubly diﬀerentiable over X, then, under
assumptions A1, A2, A3, A4, the objective in Eq. (6) can be rewritten as:
DFS(p0∥pw) =
Z
V
q(v)
Z
X
p0(x)

vT (Hx log pw(x))v + 1
2
 vT ∇x log pw(x)
2
dxdv + C, (7)
where Hx log pw(x) denotes the Hessian matrix of log pw(x) with respect to components of x
and C is a constant w.r.t. pw.
Assumption A4 is satisﬁed by, among others, Gaussian and Rademacher random vari-
ables (Song et al., 2020). Additionally, these two distributions allow to computes explicitly
the expectation with respect to v of
 vT ∇x log p0(x)
2 in Eq. (7), which leads to:
DFS(p0∥pw) =
Z
X
p0(x)
Z
V
q(v)

vT (Hx log pw(x))v

dv + 1
2 ∥∇x log pw(x)∥2
2

dx+C; (8)
in Song et al. (2020), the Monte Carlo estimate of the latter expression is found to perform
better than the one for Eq. (7); across this work, we will therefore consider Eq. (8) with the
Rademacher noise when using SSM.
Analogously to SM, a non-negative p0 ensures that the sliced Fisher divergence is zero
if and only if pw = p0:
Theorem 10 (Lemma 1 in Song et al. (2020)) Assume Assumption A4 holds and that
p0(x) > 0 ∀x ∈X. Then:
DFS(p0∥pw) = 0 ⇐⇒p0(x) = pw(x) ∀x ∈X.
The above result is proven in Appendix C.4.
SSM has a reduced computational cost with respect to SM when automatic diﬀeren-
tiation is used. In fact, computing the second derivatives in Eq. (5) requires d backward
propagations, while the quadratic form involving the Hessian in Eq. (8) only requires two
backward propagations independently on the dimension of x (see Appendix E.1).
SM and SSM over transformed domain.
If the domain X is unbounded (ai = −∞,
bi = +∞∀i), A1 is usually satisﬁed. Instead, it is easy for that assumption to be violated
if X is bounded: for instance, if p0(x) converges to a constant at the boundary of X, A1
requires ∇x log pw(x) to go to 0. Further, if p0(x) diverges, ∇x log pw(x) has to converge to
0 faster than some rate, which is in general a strong requirement.
To apply SM to distributions with bounded domain X under looser conditions, we then
transform X to Y = Rd with a suitable bijection mapping t; this creates a new random
variable Y = t(X), with distributions pY
0 (y) and pY
w(y) induced by p0 and pw on X.
9

Pacchiardi and Dutta
Deﬁnition 11 We deﬁne respectively as Transformed Score Matching (TranSM) and Trans-
formed Sliced Score Matching (TranSSM) the procedures:
arg min
w
DF (pY
0 ∥pY
w)
and
arg min
w
DFS(pY
0 ∥pY
w).
TranSM and TranSSM enjoy similar properties as SM and SSM, as stated in the following
Theorem, which mirrors Theorem 7 and Theorem 10:
Theorem 12 Let Y = t(X) ∈Y for a bijection t. Assume Assumption A4 is satisﬁed,
p0(x) > 0 ∀x ∈X, and let D denote either DF or DFS; then:
D(pY
0 ∥pY
w) = 0 ⇐⇒pw(x) = p0(x) ∀x ∈X.
We prove this Theorem (in Appendix C.5) by relying on the equivalence between distribu-
tions for the random variables Y and X.
Motivated by Theorem 12, across this work we will apply TranSM and TranSSM when-
ever X is bounded; precisely, we adopt the same bijections as in the Stan package for
statistical modeling (Appendix C.8, Carpenter et al. 2017).
Remark 13 Another way to extend SM to distributions with bounded support involves mul-
tiplying the scores in the implicit Fisher divergence by a correction factor that allows to
obtain an explicit for under looser assumptions. This Corrected SM (CorrSM) approach
was ﬁrst proposed for non-negative random variables in Hyv¨arinen (2007), which also re-
marked how CorrSM and TranSM are equivalent (Appendix C.7). TranSM is however more
practically viable, as a single SM implementation is needed, while CorrSM requires separate
implementations for diﬀerent kinds of domains. Additionally, the transformations can be
straightforwardly applied to SSM, while (to the best of our knowledge) a correction approach
for SSM has not yet been proposed.
Remark 14 Across this work we restrict to domains X deﬁned by independent constraints
across the coordinates, i.e. X = Nd
i=1(ai, bi). However, TranSM, TranSSM and CorrSM
can be potentially applied to distributions with more general supports. We brieﬂy review this
and other extensions in Appendix C.9.
3.3 Score Matching for conditional densities
We now go back to conditional densities p0(·|θ) and deﬁne an expected Fisher divergence
(Arbel and Gretton, 2018) by considering θ ∼π(θ):
DE
F (p0∥pw) =
Z
Θ
π(θ)DF (p0(·|θ)∥pw(·|θ))dθ
= 1
2
Z
Θ
Z
X
p0(x|θ)π(θ)∥∇x log p0(x|θ) −∇x log pw(x|θ)∥2dxdθ.
(9)
Analogously, we deﬁne an expected sliced Fisher divergence DE
FS(p0∥pw) from DFS.
Note that DE
F (p0∥pw) ≥0; moreover, under the assumption that p0(x|θ) is supported
on the whole domain X ∀θ, the above objective is equal to 0 if and only if p0(·|θ) = pw(·|θ)
10

Score Matched Neural Exponential Families for LFI
π(θ)-almost everywhere (Arbel and Gretton, 2018). In fact, DF (p0(·|θ)∥pw(·|θ)) ≥0 ∀θ,
with equality holding if and only if the two conditional distributions are the same, as long
as they are both supported on the whole X (by Theorem 7). Exploiting Theorem 10, a
similar result can be obtained for DE
FS.
Requiring p0(x|θ) > 0 ∀x ∈X, ∀θ ∈Θ means that the model is capable of generating
all possible values of x ∈X with non-zero probability for all θ ∈Θ; otherwise, there may
be distributions pw ̸= p0 minimizing the objective (see Appendix C.6).
Analogously to Eqs. (5) and (7), we can obtain explicit formulations7of DE
F and DE
FS:
DE
F (p0∥pw) =
Z
Θ
Z
X
p0(x|θ)π(θ)
d
X
i=1
"
1
2
∂log pw(x|θ)
∂xi
2
+
∂2 log pw(x|θ)
∂x2
i
#
dxdθ
|
{z
}
J(w)
+C,
DE
FS(p0∥pw) =
Z
V
Z
Θ
Z
X
q(v)p0(x|θ)π(θ)

vT (Hx log pw(x|θ))v + 1
2 ∥∇x log pw(x|θ)∥2
2

dxdθdv
|
{z
}
JS(w)
+C,
(10)
where C denotes constants with respect to w. For these two expressions, it is immediate to
obtain Monte Carlo estimates using samples (θ(j), xj)N
j=1, θ(j) ∼π and x(j) ∼p(·|θ(j)), and
draws from the noise model {v(j,k)}1≤j≤N,1≤k≤M :
ˆJ(w) = 1
N
N
X
j=1


d
X
i=1

1
2
 
∂log pw(x(j)|θ(j))
∂xi
!2
+
 
∂2 log pw(x(j)|θ(j))
∂x2
i
!


,
ˆJS(w) =
1
NM
N
X
j=1
M
X
k=1

v(j,k),T (Hx log pw(x(j)|θ(j)))v(j,k) + 1
2
∇x log pw(x(j)|θ(j))

2
2

.
(11)
As we discussed before, computing the square bracket term in ˆJS(w) only requires two
backward propagations, while the square bracket term in ˆJ(w) requires d. However, ˆJS(w)
sums over M · N terms, while ˆJ(w) sums over N. Nevertheless, when using ˆJS(w) to train
a neural network, good results can be obtained by using a single diﬀerent noise realization
for each training sample (θ(j), xj) at each epoch, thus leading to smaller computational cost
with respect to ˆJ(w) (Song et al., 2020).
7. Requiring p0(x|θ) to be diﬀerentiable and pw(x|θ) doubly diﬀerentiable over X for all θ and:
A1. p0(x|θ)
∂
∂xi log pw(x|θ) →0 for xi ↘ai and xi ↗bi, ∀w, i, θ,
A2. Ep0[∥∇x log p0(X|θ)∥2] < ∞, Ep0[∥∇x log pw(X|θ)∥2] < ∞, ∀w, θ,
A3. Ep0

∂2
∂xi∂xj log pw(X|θ)
 < ∞, ∀w, θ, ∀i, j = 1, . . . , d.
.
11

Pacchiardi and Dutta
Score Matching for conditional exponential family.
Both the implicit and explicit
versions DE
F (p0∥pw) and DE
FS(p0∥pw) are well deﬁned only if pw is a proper density, which
requires Zw(θ) < ∞. In practice, we are interested in:
ˆw = arg min ˆJ(w)
or
ˆw = arg min ˆJS(w).
(12)
When we compute ˆJ(w) or ˆJS(w), we only need to evaluate fw in a ﬁnite set of points
{x(j)}N
j=1 which belong to a bounded subset of X, say A ⊂X. We can thus redeﬁne the
approximating family as follows:
p′
w(x|θ) = ˜p′
w(x|θ)
Z′w(θ) ,
˜p′
w(x|θ) =
(
exp(ηw(θ)T fw(x))
if x ∈A,
gw(x, θ)
otherwise,
where gw is such that Z′
w(θ) < ∞, and can always be chosen such that ˜p′
w(x|θ) is a smooth
and continuous function of x.
Replacing pw in Eq. (11) with p′
w does not change the value of ˆJ(w) and ˆJS(w); however,
as p′
w is normalized, Eqs. (9) and (10) are now well deﬁned for all w. Additionally, we do
not need to specify A or gw explicitly as we never evaluate the normalizing constant Z′
w(θ)
(which depends on them). In the following we will thus use interchangeably pw and p′
w.
Remark 15 (Notation) For notational simplicity, in the rest of the work we drop the hat
in ˆw, denoting by pw the approximation obtained by one of the strategies in Eq. (12), and
by fw and ηw the corresponding suﬃcient statistics and natural parameters networks.
4. Inference using the likelihood approximation
By ﬁtting the neural conditional exponential family (Eq. 3) with SM or SSM to parameter-
simulation pairs generated from the model, we obtain an approximation of the likelihood up
to the normalization constant Zw(θ): for each ﬁxed θ, the function x 7→exp(ηw(θ)T fw(x))
is approximately proportional to p0(x|θ). We exploit pw in two ways: ﬁrst, by using fw
as summaries in ABC; secondly, using the full approximation to draw samples from the
posterior with MCMC for doubly intractable distributions. Both approaches are illustrated
in Figure 1 and discussed next.
4.1 ABC with neural conditional exponential family statistics
The exponential family is the most general set of distributions with suﬃcient statistics of
a given size (see Appendix A). Using fw as summaries in ABC is therefore intuitively ap-
pealing: fw represents in fact the suﬃcient statistics of the best exponential family approx-
imation to p0, according to the (sliced) Fisher divergence. If p0 belongs to the exponential
family, DE
F (p0∥pw) = DE
FS(p0∥pw) = 0 if fw and ηw are suﬃcient statistics and natural
parameters for p0.
To use fw in ABC, some practicalities are needed: ﬁrst, we discard the last component
of fw, which represents the base measure hw(x). Secondly, as discussed in Section 3.1, fw is
identiﬁable only up to a scale parameter, so that the magnitude of the diﬀerent components
of fw can vary signiﬁcantly. Before using ABC, then, we rescale the diﬀerent components
12

Score Matched Neural Exponential Families for LFI
Model 
ABC
ExchangeMCMC
(Sliced) Score
Matching
Neural exponential 
family
Simulate from the model
Simulate
from the
model
ABC posterior
samples
ExchangeMCMC 
posterior samples
Training samples
Training samples
Figure 1: Diagram showing the roles of the diﬀerent components in the inference
routines. SSM or SM are used to train the neural exponential family on model simulations.
Then, the suﬃcient statistics fw are used in ABC or, alternatively, the neural exponential
family is inserted in ExchangeMCMC. Notice how additional model simulation are needed
for ABC but not for ExchangeMCMC.
by their standard deviation on new samples generated from the model, to prevent the ones
with larger magnitude from dominating the ABC distance.
In the following, we will call the approach described here ABC with Score Matching
statistics, for short ABC-SM, or ABC-SSM in the Sliced Score Matching case.
4.2 ExchangeMCMC with neural conditional exponential family
In contrast to other statistics learning methods (Jiang et al., 2017; Wiqvist et al., 2019;
Pacchiardi et al., 2020; ˚Akesson et al., 2020), our technique provides a full likelihood ap-
proximation; it is therefore tempting to sample from the corresponding posterior directly,
bypassing in this way ABC (and the additional model simulations required) altogether.
Unfortunately, pw is known only up to the normalizing constant Zw(θ); therefore, stan-
dard MCMC cannot be directly exploited , and methods for doubly-intractable distributions
are required (see Park and Haran, 2018 for a review). Here, we use ExchangeMCMC (Mur-
ray et al. (2012), Algorithm 1 in Appendix E.4), an MCMC where, for each proposed θ′,
a simulation from the distribution pw(x|θ′) is used within a Metropolis-Hastings rejection
step.
In our case, we cannot generate samples from pw(·|θ′) directly; to circumvent such issue,
Murray et al. (2012) suggested to run an MCMC targeting pw(·|θ′) for each ExchangeMCMC
step; if the chain is long enough, the last step can be considered as a draw from pw. Empirical
results (Caimo and Friel, 2011; Alquier et al., 2016; Everitt, 2012; Liang, 2010) show that
a relatively small number of inner MCMC steps are enough for good performance and that
initializing the inner chain at the observation improves convergence; we employ therefore
this strategy in our work.
Further, a variant of ExchangeMCMC employs Annealed Importance Sampling to im-
prove the mixing of the chain (Murray et al., 2012). Speciﬁcally, a sequence of K auxiliary
variables are drawn from Metropolis-Hastings kernels targeting some intermediate distri-
butions, and the ExchangeMCMC acceptance rate is modiﬁed accordingly. This approach,
termed bridging, decreases the number of rejections in ExchangeMCMC due to poor auxil-
iary variables. See Appendix E.4 for more details.
13

Pacchiardi and Dutta
In the following, we will refer to using ExchangeMCMC with our likelihood approxi-
mation as Exc-SM or Exc-SSM, according to whether SM or SSM are used to obtain the
likelihood approximation. Exc-SM and Exc-SSM avoid additional model simulations (be-
yond the ones required to train pw) at the cost of running a nested MCMC. However, the
number of steps in the inner MCMC required to obtain good performance increases with
the dimension of X. Exc-SM and Exc-SSM are therefore ideally applied to models with
moderate dimension x (up to a few hundred), for which simulation is expensive. The same
likelihood approximation can be used to perform inference on several observations, which
makes the computational gain even greater in this case.
Remark 16 (Model misspeciﬁcation) The neural exponential family approximation is
only valid close to where training data was distributed. Speciﬁcally, if x0 is far from that
region, p ˆw(x0|θ) may be assigned a large value rather than a (correct) small one. This could
happen when the model p0 is unable to generate data resembling the observation for any
parameter value, such that standard Bayesian inference in presence of the likelihood would
also perform poorly. To get better inference in such scenarios, we could use the generalized
posterior introduced in Matsubara et al. (2021), which is robust to outliers and suitable for
doubly-intractable distributions (as the exponential family).
In Exc-SM and Exc-SSM, we may wonder whether running MCMC over x targeting
pw(x|θ) for a ﬁxed θ can fail due to what we discussed above. We believe this is not the
case: if the MCMC is initialized close to training data and pw(·|θ) is a good representation
of p0(·|θ) in that region, pw(·|θ) is small for values of x close to the boundary of the region
where training data was distributed. Then, the MCMC is “trapped” inside that region and
has no way of reaching regions of X where pw may behave irregularly.
5. Simulation Studies
We perform simulation studies with our proposed approaches (Exc-SM, Exc-SSM, ABC-SM
and ABC-SSM) and we compare with:
• ABC with statistics learned with the rejection approach discussed in Section 2.1
(Fearnhead and Prangle, 2012; Jiang et al., 2017), which we term ABC-FP.
• Population Monte Carlo (PMC, Capp´e et al. 2004) with Ratio Estimation (PMC-RE).
• PMC with Synthetic Likelihood, using the robust covariance matrix estimator devel-
oped in Ledoit and Wolf (2004) to estimate Σθ. We will denote this as PMC-SL.
Speciﬁcally, we consider three exponential family models and two time-series models
(AR(2) and MA(2)) for which the exact likelihood is available, as well as a large-dimensional
model with unknown likelihood (the Lorenz96 model, Lorenz 1996; Wilks 2005).
Exc-SM and Exc-SSM do not require additional simulations and run an MCMC, in con-
trast to sequential algorithms for the other methods, which we run with parallel computing.
Comparing the computational cost is therefore not easy; in Appendix G.3, we report the
number of simulations needed by the diﬀerent methods to reach the same performance
achieved by Exc-SM for the models with known likelihood.
14

Score Matched Neural Exponential Families for LFI
Choice of neural network architecture
In our exponential family approximation, we
ﬁx ds to the number of parameters in each model. We added a Batch Normalization layer
(see Appendix E.2) on top of the neural network representing ηw to reduce the unidentiﬁabil-
ity introduced by the dot product between fw(x) and ηw(θ) (as discussed in Appendix C.10).
For all techniques, we use 104 training samples. All NNs use SoftPlus nonlinearity (NNs
using the more common ReLU nonlinearity cannot be used with SM and SSM as they have
null second derivative with respect to the input).
For all models, ηw is represented by fully connected NNs. For the exponential family
models, fully connected NNs are also used for fw and sβ. For the time series and Lorenz96
models, we parametrize fw and sβ with Partially Exchangeable Networks (PENs, Wiqvist
et al. 2019). The output of an r-PEN is invariant to input permutations which do not
change the probability density of data distributed according to a Markovian model of order
r (see Appendix E.3). As AR(2) is a 2-Markovian model, we use a 2-PEN; similarly, a
1-PEN is used for the Lorenz96 model, which is 1-Markovian. Finally, we use a 10-PEN
for the MA(2) model; albeit not being Markovian, Wiqvist et al. (2019) argued that MA(2)
can be considered as “almost” Markovian so that the loss of information in imposing a PEN
structure of high enough order is negligible. Further details are given in Appendix F.
Choice of inferential parameters.
For ABC, we employ Simulated annealing ABC
(SABC, Albert et al. 2015), which considers a set of particles and updates their position
in parameter space across several iterations.
We use 100 iterations and 1000 particles
(posterior samples), corresponding to 1000 model simulations per iteration.
In Exc-SM and Exc-SSM, we run Exchange MCMC for 20000 steps, of which the ﬁrst
10000 are burned-in. During burn-in, at intervals of 100 outer steps, we tune the proposal
sizes according to the acceptance rate in the previous 100 steps. Our implementation of
the Exchange algorithm is detailed in Appendix E.4.1.
For the exponential family and
time-series models, we test diﬀerent numbers of inner MCMC steps, and eventually use 30
for the former and 100 for the latter, above which there was no noticeable performance
improvement (more details in Appendix G.1.3 and Appendix G.2).
In PMC-SL and PMC-RE, we run the PMC algorithm with 10 iterations, 1000 posterior
samples and respectively 100 (with SL) and 1000 (with RE) simulations per parameter
value in order to estimate the approximate likelihood; such a large number of simulations
(respectively 105 and 106 for each iteration) is required for the likelihood estimate to be
numerically stable. For the exponential family models, we use the true suﬃcient statistics;
for AR(2) and MA(2), we instead use autocovariances with lag 1 and 2 (as for instance in
Marin et al. 2012). For PMC-RE, the cross-product of the statistics is also added to the
list of statistics, as PMC-SL implicitly uses it.
5.1 Exponential family models
First, we consider three models for which a sample is deﬁned as a 10-dimensional inde-
pendent and identical distributed (i.i.d.) draw from either a Gaussian, Gamma or Beta
distribution (all belonging to the exponential family). We put uniform priors on the param-
eters, with bounds given in Table 1. These models have diﬀerent data ranges: unbounded
for Gaussian, and respectively lower bounded by 0 and bounded in [0, 1] for Gamma and
15

Pacchiardi and Dutta
Model
Beta
Gamma8
Gaussian
AR(2)
MA(2)
Lorenz96
Parameter
α
β
k
θ
µ
σ
θ1
θ2
θ1
θ2
b0
b1
σe
φ
Lower bound
1
1
1
1
−10
1
−1
−1
−1
0
1.4
0
1.5
0
Upper bound
3
3
3
3
10
10
1
0
1
1
2.2
1
2.5
1
Table 1: Bounds of uniform priors for the considered models.
Beta. Therefore, we directly apply SM and SSM to the Gaussian model, while TranSM and
TranSSM are applied to Gamma and Beta.
Inferred suﬃcient statistics and natural parametrization.
With these models, our
exponential family approximation is well speciﬁed. Thus, we compare the learned fw and
ηw with the exact suﬃcient statistics and natural parameters using the Mean Correlation
Coeﬃcient (MCC, Appendix B.1.1) metric, which ranges in [0, 1] and measures how well
a multivariate function is recovered. We report results obtained with SM in Table 2; the
values are close to 1, indicating that the suﬃcient statistics and natural parameters are
recovered quite well by our method. In Appendix G.1, we report values corresponding to
SSM (Table 9), as well as comparisons of exact and learned embeddings in Figures 9 and
10.
Model
MCC weak in
MCC weak out
MCC strong in
MCC strong out
Beta (statistics)
0.964
0.958
0.723
0.723
Beta (nat. par.)
0.990
0.991
0.807
0.812
Gamma (statistics)
0.911
0.924
0.894
0.883
Gamma (nat. par.)
0.967
0.967
0.872
0.873
Gaussian (statistics)
0.944
0.937
0.808
0.824
Gaussian (nat. par.)
0.974
0.974
0.970
0.972
Table 2: MCC for exponential family models between exact embeddings and
those learned with SM. We show weak and strong MCC values; MCC is between 0 and
1 and measures how well an embedding is recovered up to permutation and rescaling of its
components (strong) or linear transformation (weak); the larger, the better. “in” denotes
MCC on training data used to ﬁnd the best transformation, while “out” denote MCC on
test data. We used 500 samples in both training and test data sets.
Inferred posterior distribution.
Figure 2 shows posteriors obtained with the proposed
methods, for a possible observation for each model; we see that all approximate posteriors
are remarkably close to the exact one; moreover, the results with SM and SSM are indis-
tinguishable. For all methods, we also estimate the Wasserstein distance between true and
approximate posterior and compute the Root Mean Squared Error (RMSE) between mean
of the true and approximate posterior; this is repeated for 100 simulated observations, with
8. The scale parameter of the Gamma distribution is usually called θ; in contrast, in all the rest of this
work we use θ to denote all parameters. Please beware of the diﬀerence.
16

Score Matched Neural Exponential Families for LFI
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
True posterior
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
ABC-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
ABC-SSM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
Exc-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
1.0
1.5
2.0
2.5
3.0
Exc-SSM
Posterior mean
True value
(a) Beta
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
True posterior
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
ABC-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
ABC-SSM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
Exc-SM
Posterior mean
True value
1.0
1.5
2.0
2.5
3.0
k
1.0
1.5
2.0
2.5
3.0
Exc-SSM
Posterior mean
True value
(b) Gamma
10
5
0
5
10
1
4
7
10
True posterior
Posterior mean
True value
10
5
0
5
10
1
4
7
10
ABC-SM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
ABC-SSM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
Exc-SM
Posterior mean
True value
10
5
0
5
10
1
4
7
10
Exc-SSM
Posterior mean
True value
(c) Gaussian
Figure 2: True and approximate posteriors for exponential family models, for a
single observation per model. Dashed line represents posterior mean, while green solid line
represents the exact parameter value.
results reported in Figure 3. ABC-FP is here the worst method; additionally, ABC-SM and
ABC-SSM perform similarly to PMC-SL and PMC-RE. Finally, Exc-SM and Exc-SSM are
marginally worse.
5.2 Time series models
The Moving Average model of order 2, or MA(2), and the AutoRegressive model of order
2, or AR(2), are special cases of the ARMA time-series model. The MA(2) model is deﬁned
by:
X1 = ξ1,
X2 = ξ2 + θ1ξ1,
Xj = ξj + θ1ξj−1 + θ2ξj−2,
j = 3, . . . , t,
while the AR(2) is deﬁned as:
X1 = ξ1,
X2 = ξ2 + θ1X1,
Xj = ξj + θ1Xj−1 + θ2Xj−2,
j = 3, . . . , t;
in both, ξj’s are i.i.d. standard normal error terms (unobserved). We take here t = 100
and we put uniform priors on the parameters of the two models, with bounds given in
17

Pacchiardi and Dutta
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
Beta
0.0
0.2
0.4
0.6
0.8
1.0
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0
2
4
6
8
10
Wasserstein distance
Gaussian
0
2
4
6
8
10
RMSE posterior mean
Figure 3: Performance of the diﬀerent techniques for exponential family models.
Wasserstein distance from the exact posterior and RMSE between exact and approximate
posterior means are reported for 100 observations using boxplots. Boxes span from 1st
to 3rd quartile, whiskers span 95% probability density region and horizontal line denotes
median. The numerical values are not comparable across examples, as they depend on the
range of parameters. Here, SL and RE used the true suﬃcient statistics.
Table 1. For these models, the true likelihood can be evaluated, but they do not belong to
the exponential family9.
Inferred posterior distribution.
Figure 4 shows the posterior obtained with our pro-
posed methods, for a possible observation for each model; again, our approximations are
close to the exact posterior, with Exc-SM and Exc-SSM leading to slightly broader poste-
riors. Again, we assess performance with the Wasserstein distance from the true posterior
and the RMSE between the means of true and approximate posterior for all methods, over
100 fresh observations. The results are reported in Figure 5; here, ABC-FP is the best
method, with ABC-SM and ABC-SSM marginally worse. Exc-SM and Exc-SSM follow and
perform better or comparably to PMC-RE and PMC-SL.
5.3 Lorenz96 meteorological model
The Lorenz96 model (Lorenz, 1996) is a toy model of chaotic atmospheric behaviour, in-
cluding interacting slow and fast variables. We use here a modiﬁed version (Wilks, 2005)
where the eﬀect of the fast variables on the slow ones is replaced by a stochastic eﬀective
term depending on a set of parameters. Speciﬁcally, this model is deﬁned by the following
coupled Diﬀerential Equations (DEs):
dyk
dt = −yk−1(yk−2 −yk+1) −yk + 10 −g(yk, t; θ);
k = 1, . . . , K,
where yk(t) is the value at time t of the k-th variable and indices are cyclic (index K + 1
corresponds to 1, and so on). The eﬀective term g depends on θ = (b0, b1, σe, φ), and is
9. More precisely, they cannot be written as an exponential family with embedding dimension ﬁxed with
data size; in fact, MA(2) can be written as a Gaussian distribution with t × t covariance matrix, which
is an exponential family whose embedding dimension increases with data size.
18

Score Matched Neural Exponential Families for LFI
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
True posterior
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
ABC-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
ABC-SSM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
Exc-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
1.0
0.8
0.6
0.4
0.2
0.0
2
Exc-SSM
Posterior mean
True value
(a) AR(2)
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
True posterior
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
ABC-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
ABC-SSM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
Exc-SM
Posterior mean
True value
1.0
0.5
0.0
0.5
1.0
1
0.0
0.2
0.4
0.6
0.8
1.0
2
Exc-SSM
Posterior mean
True value
(b) MA(2)
Figure 4: True and approximate posteriors for AR(2) and MA(2) models, for a
single observation per model. Dashed line represents posterior mean, while green solid line
represents the exact parameter value.
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
AR(2)
0.0
0.2
0.4
0.6
0.8
RMSE posterior mean
PMC-SL
PMC-RE
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
Exc-SSM
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
MA(2)
0.0
0.2
0.4
0.6
0.8
RMSE posterior mean
Figure 5: Performance of the diﬀerent techniques for AR(2) and MA(2) models.
Wasserstein distance from the exact posterior and RMSE between exact and approximate
posterior means are reported for 100 observations using boxplots. Boxes span from 1st
to 3rd quartile, whiskers span 95% probability density region and horizontal line denotes
median.
deﬁned upon discretizing the DEs with a timestep ∆t:
g(y, t; θ) =
linear deterministic term
z
}|
{
b0 + b1y
+
zero mean AR(1)
z
}|
{
φ · r(t −∆t) + σe(1 −φ2)1/2η(t),
where η(t) ∼N(0, 1). We numerically integrate the model using 4th order Runge-Kutta
method with ∆t = 1/30 in an interval [0, T]; further, we ﬁx the initial condition to a value
y(0) which is independent on parameters. Here, the true likelihood is unaccessible, so that
sampling from the exact posterior is impossible.
19

Pacchiardi and Dutta
We consider the model in a small and large conﬁguration: in the small one, we take
K = 8 and T = 1.5, which lead to 45 timesteps and a data dimension of d = 45 · 8 = 360.
In the large one, we take instead K = 40 and T = 4, corresponding to 120 timesteps and a
data dimension of d = 120 · 40 = 4800.
Inferred posterior distribution.
For both conﬁgurations, we perform inference with
ABC-SSM and ABC-FP; additionally, we use Exc-SSM for the small conﬁguration (as
running the exchange algorithm in the large one is too costly). In ExchangeMCMC, we
used 500 inner MCMC steps and 200 bridging steps for each outer step. Figure 6 reports
posteriors for a single observation in both setups. The ABC-FP posterior is narrower than
the one for ABC-SSM, but concentrated around similar parameter values. The posterior for
Exc-SSM looks slightly diﬀerent: it concentrates on similar parameter values as the other
two for θ1 and θ2, but is broader for σe and φ.
Posterior predictive validation.
We assess the performance of the posterior predictive
distribution p(x|x0) =
R
p(x|θ)π(θ|x0)dθ, in which π(θ|y) is the posterior obtained with one
of the considered techniques. Speciﬁcally, we use the Kernel and Energy Scoring Rules (SRs)
to evaluate how well the predictive p(x|x0) predicts the observation x0. A SR (Gneiting
and Raftery, 2007) is a function of a distribution and an observation, and assesses the
mismatch between them (the smaller, the better). More details on Scoring Rules are given
in Appendix D, together with the deﬁnition of those used here.
As the Lorenz96 model returns a (multivariate) time-series, we compare predictive dis-
tribution and observation at each timestep independently. We repeat this validation with
100 diﬀerent observations and corresponding posteriors, for all techniques and both model
conﬁgurations. We obtain cumulative scores by summing the scores over timesteps; median
and some quantile values over the 100 observations are shown in Fig. 7. In both model
conﬁgurations, the posterior predictive generated by ABC-SSM and Exc-SSM marginally
outperform ABC-FP, according to both SRs. Score values at each timestep, together with
more details on the validation technique, are reported in Appendix G.4.
6. Related works
ABC statistics with NN.
Using NNs for learning statistics for ABC has been previously
suggested, ﬁrst with the regression approach discussed in Sec. 2.1 (used in Jiang et al. 2017;
Wiqvist et al. 2019; ˚Akesson et al. 2020). In Pacchiardi et al. (2020), a NN is trained so
that the distance between pairs of simulated statistics best reproduces the distance between
the corresponding parameter values. Chen et al. (2021) instead ﬁt a NN by maximizing an
estimate of the mutual information between statistics and parameters.
ABC statistics from auxiliary models.
Some similarities with our work can be found
in Gleim and Pigorsch (2013) and Ruli et al. (2016), which consider respectively an auxiliary
model and a composite likelihood alongside the simulator model and obtain summary statis-
tics from them; similarly, our approach can be seen as building an auxiliary exponential
family model with easily accessible summary statistics.
LFI with NNs.
The idea of sampling directly from the approximate posterior deﬁned by
pw is related to a suite of LFI methods using NNs. A large part of these works use normaliz-
20

Score Matched Neural Exponential Families for LFI
1
2
e
1
2
e
ABC-FP
1
2
e
1
2
e
ABC-SSM
1
2
e
1
2
e
Exc-SSM
(a) Lorenz96 - Small conﬁguration
1
2
e
1
2
e
ABC-FP
1
2
e
1
2
e
ABC-SSM
(b) Lorenz96 - Large conﬁguration
Figure 6: Example of posterior inference for a single Lorenz96 observation with
diﬀerent approaches, for both small (ﬁrst row) and large (second row) conﬁgurations.
In each panel, the diagonal plots represent the univariate marginal distributions, while the
oﬀ-diagonal ones are bivariate density contour plots. Moreover, the green and red lines
represent respectively exact parameter value and posterior mean. All axes span full prior
range (see Table 1).
ing ﬂows (NNs implementing invertible transformations, suitable for eﬃciently representing
probability densities; see Papamakarios et al., 2021 for a review); speciﬁcally, Papamakarios
et al. (2019); Lueckmann et al. (2019) use them to learn an approximation of the likeli-
hood, while Papamakarios and Murray (2016); Lueckmann et al. (2017); Greenberg et al.
(2019); Radev et al. (2020) learn the posterior. Most of the above approaches focus on
inference for a single observation, tailoring the simulations to better approximate the pos-
terior for the relevant parameter values at lower computational cost; instead, Radev et al.
(2020) and Lueckmann et al. (2019) propose to amortize across observations, similarly to
our approach. Besides normalizing ﬂows, Klein et al. (2020) casts the LFI problem as a
distributional regression one. Finally, Tabak et al. (2020) does not employ NNs but rather
21

Pacchiardi and Dutta
ABC-FP
ABC-SSM
Exc-SSM
8000
9000
10000
11000
Energy Score
Small Lorenz96
100
110
120
130
140
150
160
170
Kernel Score
ABC-FP
ABC-SSM
160000
180000
200000
220000
Energy Score
Large Lorenz96
1000
1500
2000
2500
3000
Kernel Score
Figure 7: Predictive performance of the diﬀerent methods according to the Ker-
nel and Energy Scoring Rules. Each boxplot represents cumulative (i.e., summed over
the time index) scoring rule value for a given method for the small (left) and large (right)
Lorenz96 conﬁguration. Boxes span from 1st to 3rd quartile, whiskers span 95% probability
density region and horizontal line denotes median.
solves a Wasserstein barycenter problem to model conditional maps which allow sampling
from the distribution of an observation conditional on some covariates.
Fitting unnormalized models.
Several techniques besides Score Matching have been
proposed for ﬁtting unnormalized models: MCMC-MLE (Geyer, 1991) exploits MCMC
to estimate the normalizing constant for diﬀerent values of the parameter and uses that in
MLE. Contrastive Divergence (CD, Hinton 2002) instead uses MCMC to obtain a stochastic
approximation of the gradient of the log-likelihood; this requires a smaller number of MCMC
steps with respect to MCMC-MLE, but the stochastic gradient estimate is biased. Minimum
Probability Flow (MPF, Sohl-Dickstein et al. 2011) considers a dynamics from data to model
distribution, and minimizes the Kullback Leibler divergence between the data distribution
and the one obtained by running the dynamics for a short time; however, the eﬃcacy
of MPF depends signiﬁcantly on the considered dynamics. Noise Contrastive Estimation
(NCE, Gutmann and Hyv¨arinen 2012) converts the parameter estimation problem to ratio
estimation between data distribution and a suitable noise distribution; in practice, NCE uses
logistic regression to discriminate the observed data and data generated from the noise; in
the loss, the normalizing constant appears explicitly and can be estimated independently. To
be eﬀective, NCE requires the noise distribution to overlap well with the data density while
being easy to sample from and to evaluate, which is not easy to get. Finally, some works
(Dai et al., 2019a,b) use the dual formulation of MLE to avoid estimating the normalizing
constant at the price of introducing dual variables to be jointly estimated.
Fitting unnormalized conditional models.
The approaches above running dynamics
to estimate quantities (CD, MCMC-MLE, MPF) cannot easily be applied to the conditional
setting. NCE has been instead used in Ton et al. (2021) with a single noise distribution for all
values of the conditioning variable θ; it however requires an independent NN to parametrize
22

Score Matched Neural Exponential Families for LFI
the normalizing constant Z(θ). SM can be instead easily applied to the conditional setting,
as previously done in Arbel and Gretton (2018) and further demonstrated in our work. To
the best of our knowledge, SSM was not applied to a conditional setting before, although
the extension straightforwardly follows what is done for SM.
Fast approximations of SM.
Besides SSM, some approximations to SM have been in-
vestigated, all of which only require ﬁrst derivatives. For instance, Denoising Score Matching
(Vincent, 2011) computes the Fisher divergence between the model and a Kernel Density
Estimate of the data distribution, which is equal to a quantity independent on the second
derivative. Alternatively, Kernel Stein Discrepancy (Liu and Wang, 2016) intrinsically de-
pends on ﬁrst derivatives only. For both techniques, however, several samples for each θ
would be required in the conditional setting. Finally, Wang et al. (2020) exploits a connec-
tion between the Fisher divergence and gradient ﬂows in the 2-Wasserstein space to develop
an approximation that only relies on ﬁrst-order derivatives.
Kernel Conditional Exponential Families (KCEFs).
In KCEFs (Arbel and Gret-
ton, 2018), the summary statistics and natural parameters are functions in a Reproducing
Kernel Hilbert Space, whose properties allow to evaluate the density although an inﬁnite-
dimensional embedding space is used (using the kernel trick). In Arbel and Gretton (2018),
SM was used to ﬁt KCEFs instead of our neural conditional exponential families. They
build on Sriperumbudur et al. (2017), which ﬁrst used SM to perform density estimation
with (non-conditional) Kernel Exponential Families (KEFs). In Wenliang et al. (2019) NNs
are used to parametrize the kernel in a KEF and trained with the SM loss. However, as
KCEFs do not have ﬁnite-dimensional suﬃcient statistics, they are unsuitable for learning
ABC statistics. Additionally, KCEFs have a worse complexity in terms of training dataset
size with respect to NN-based methods. KCEFs have also been used with a dual MLE
objective in Dai et al. (2019a).
7. Conclusions and extensions
We proposed a technique to approximate the likelihood using a neural conditional expo-
nential family, trained via (Sliced) Score Matching to handle the intractable normalizing
constant.
We tested this approximation in two setups: ﬁrst, by using the exponential family
suﬃcient statistics as ABC statistics, which is intuitively appealing as the exponential
family is the largest class of distributions with ﬁxed-size suﬃcient statistics. We empirically
showed this to be comparable or outperform ABC with summaries built via the state-of-
the-art regression approach (Fearnhead and Prangle, 2012; Jiang et al., 2017).
Secondly, we used MCMC for doubly-intractable distributions to sample from the pos-
terior corresponding to the likelihood approximation, which we found to have performance
comparable to the other approaches. This can be repeated for any new observation without
additional model simulations, making it advantageous for expensive simulator models.
Our proposed direct sampling approach based on exponential family likelihood approx-
imation and ExchangeMCMC could be improved as follows:
• we used ExchangeMCMC (Murray et al., 2012) to handle double intractability, but
other algorithms could be more eﬃcient, (for instance the one in Liang et al. (2016),
23

Pacchiardi and Dutta
which makes use of parallel computing). Alternatively, we could exploit the general-
ized posterior introduced in Matsubara et al. (2021), which allows standard MCMC
to be used for double intractable distributions and is robust to outliers.
• To infer the posterior for a single observation, approximating the likelihood for all x’s
and θ’s as we do now is suboptimal. Similar performance may in fact be obtained
with fewer simulations tailored to the observation. Sequential schemes implementing
such ideas have been introduced for LFI using normalizing ﬂows (see for instance
Papamakarios et al. 2019) and could be extended to our setup.
• The motivation for the current work was learning ABC statistics, hence the exponen-
tial family formulation. However, the dot-product structure between fw and ηw is not
beneﬁcial for the direct sampling approach. An energy-based model, which employs
a single NN with input (x, θ) in the exponent, may be more expressive and easier to
train.
• An energy-based posterior approximation πw(θ|x) could be trained by minimizing the
expectation over the data marginal p(x) of the (sliced) Fisher divergence with respect
to the true posterior. This is complementary to the strategy employed in this work
to ﬁt pw(x|θ). Interestingly, πw(θ|x) would be known up to a normalizing constant
depending on x only, making use of standard MCMC possible.
Acknowledgments
We acknowledge fruitful discussion with prof. Christian Robert, prof. Antonietta Mira
and prof. GeoﬀNicholls. LP is supported by the EPSRC and MRC through the OxWaSP
CDT programme (EP/L016710/1), which also funds the computational resources used to
perform this work. RD is funded by EPSRC (grant nos. EP/V025899/1, EP/T017112/1)
and NERC (grant no. NE/T00973X/1).
All summary statistics learning strategies used in this manuscript (the regression ap-
proach and the exponential family with SM and SSM) have been implemented in the Python
library ABCpy (Dutta et al., 2021b), which exploits Pytorch (Paszke et al., 2019) to train
the NNs. ABCpy was also used to run PMC-SL, PMC-RE and the ABC experiments.
24

Score Matched Neural Exponential Families for LFI
Appendix A. Suﬃcient statistics
Consider a conditional probabilistic model p(x|θ); moreover, abusing notation, we will also
denote as p(x|t) the density pX|T (X = x|T = t), as well as p(x|t; θ) the density pX|T,Θ(X =
x|T = t; Θ = θ), where here Θ denotes the random variable which takes values θ. Finally,
we use π to denote distributions over the parameter values θ; speciﬁcally, π(θ|x) denotes the
standard posterior, and π(θ|t) is an abuse of notation for the density πΘ|T (Θ = θ|T = t).
Deﬁnition 17 A statistic t = t(x) is suﬃcient if p(x|t; θ) = p(x|t), where θ is a parameter
of the distribution. Alternatively, we have, in the Bayesian setting:
π(θ|x) = π(θ|t(x)),
for any (non-degenerate) choice of prior distribution π(θ).
The existence of suﬃcient statistics implies a precise form of the distribution:
Theorem 18 (Fisher-Neyman factorization theorem): A statistic is suﬃcient ⇐⇒
p(x|θ) = h(x)g(t(x)|θ), where h and g are non-negative functions.
It is clear from the above theorem that f(x) in the exponential family is suﬃcient.
A stronger result regarding exponential family also exists, which goes under the name
of Pitman–Koopman–Darmois theorem. This theorem says that the exponential family is
the most general family of distributions for which there is a suﬃcient statistics whose size is
ﬁxed with the number of samples, provided that the domain of the probability distribution
does not vary with the parameter θ (Koopman, 1936).
Appendix B. Properties of the conditional exponential family
B.1 Linear identiﬁability
Let us consider the exponential family model pw(x|θ) = exp(ηw(θ)T fw(x))/Zw(θ), where
here ηw and fw are not restricted to be Neural Networks. Khemakhem et al. (2020) studies
the identiﬁability properties of the above family; speciﬁcally, they consider identiﬁability
properties of the feature extractors ηw and fw. Identiﬁability of representations is useful
as it means that two diﬀerent models from the above family learn similar representations
when trained with diﬀerent initialization on the same data set. In the framework of our
likelihood-free inference task, it also means that if the true model belongs to the family we
are considering, it is theoretically possible to recover the natural parameters and the true
suﬃcient statistics.
If we consider now ηw and fw to be Neural Networks, this causes problems as they are
not identiﬁable in the standard sense. In fact, many diﬀerent parameter conﬁgurations lead
to the same function (as there are many symmetries in how the transformations in a Neural
Network layer are deﬁned).
Therefore, Khemakhem et al. (2020) introduces, following
previous works, two more general notions of identiﬁable representations. Subsequently, let
W denote the space of the possible Neural Network weights w.
25

Pacchiardi and Dutta
Deﬁnition 19 Weak identiﬁability (Section 2.2 in Khemakhem et al., 2020). Let
∼f
w and ∼η
w be equivalence relations on W deﬁned as:
w ∼f
w w′ ⇐⇒fw(x) = Afw′(x) + c
w ∼η
w w′ ⇐⇒ηw(θ) = Bηw′(θ) + e
where A and B are (ds ×ds)-matrices of rank at least min(ds, d) and min(ds, p) respectively,
and c and e are vectors.
Deﬁnition 20 Strong identiﬁability (Section 2.2 in Khemakhem et al., 2020).
Let ∼f
s and ∼η
s be equivalence relations on W deﬁned as:
w ∼f
s w′ ⇐⇒∀i, fi,w(x) = aifσ(i),w′(x) + ci
w ∼η
s w′ ⇐⇒∀i, ηi,w(θ) = biηγ(i),w′(θ) + ei
where σ and γ are permutations of [[1, n]], ai and bi are non-zero scalars and ci and ei are
scalars.
Weak identiﬁability means that two parameters are equivalent if the corresponding fea-
ture extractors are the same up to linear transformation. Strong identiﬁability is a speciﬁc
case of the weak one, in which the linear transformation is restricted to be a scaled permu-
tation.
After introducing these concepts, Khemakhem et al. (2020) provides two theorems (The-
orem 1 and 2) in which weak or strong identiﬁability of the representations are implied if
diﬀerent parameter values lead to same distributions, i.e. pw(x|θ) = pw′(x|θ)∀x, θ
=⇒
w ∼f
w w′ or pw(x|θ) = pw′(x|θ)∀x, θ =⇒w ∼f
s w′ (and similar for ∼η
w and ∼η
s). These two
results hold under some strict conditions on the functional form of the feature extractors f
and η (concerning diﬀerentiability, rank of the Jacobian and other properties).
Then, they verify explicitly these conditions for a simple fully connected Neural Network
architecture, in which they restrict the activation functions ot be LeakyReLUs and the layer
widths to be monotonically increasing or decreasing. Of course, this architecture is quite
restrictive; it is in fact impossible to study theoretically the properties of more complex ar-
chitectures and to show that they satisfy the necessary conditions for identiﬁability to hold.
However, they show empirically that the conditional structure, even with more complex
architectures, is helpful in increasing identiﬁability of the representations (as computed by
the MCC, see Appendix B.1.1).
Therefore, even if the architectures which are used throughout this work do not satisfy
the assumptions needed to explicitly show identiﬁability of the representations, we argue
that the presence of such results is an hint towards the fact that identiﬁability (according
to the above deﬁnitions) is actually achievable.
B.1.1 Mean Correlation Coefficient (MCC)
In order to evaluate empirically whether the above identiﬁability results are satisﬁed, we
need a way to measure similarity between two embeddings of the same set of data. By
following Khemakhem et al. (2020), we describe here the Mean Correlation Coeﬃcient
26

Score Matched Neural Exponential Families for LFI
(MCC), which is a simple measure to do that. In the main text, we used this technique to
assess how well our approximating family recovered the exact suﬃcient statistics and natural
parameters, in the case where the data generating model p0 belongs to the exponential
family.
In the following, we describe two versions of MCC, which are directly linked to the weak
and strong identiﬁability deﬁnitions in Appendix B.1.
Strong MCC.
Let us consider two sets of embeddings {yi ∈Rds}n
i=1 and {zi ∈Rds}n
i=1,
which can be thought of as samples from two multivariate random variables Y = t1(X) and
Z = t2(X), for two functions t1, t2 and a random variable X; in general, the order of the
components of these two vectors is arbitrary, so that we cannot say, for instance, that the
1st component of Y corresponds to the 1st of Z. Then, MCC computes all the correlation
coeﬃcients between each pair of components of Y and Z.
Next, it solves a linear sum
assignment problem which identiﬁes each component of Y with a component of Z aiming
to maximize the sum of the absolute value of the corresponding correlation coeﬃcient. In
this way, it tries to couple the embeddings which are most linear one to the other. Finally,
the MCC is computed as the mean of the absolute correlation coeﬃcients after the right
permutation of elements of the vector.
MCC is therefore a metric between 0 and 1 which measures how well each component
of the original embedding (say, Y ) has been recovered independently by the other one (say,
Z). Moreover, as it relies on the correlation coeﬃcient, it is not sensitive to rescaling or
translation of each of the embeddings (in fact, the correlation coeﬃcient of two univariate
random variables is ±1 when a deterministic linear relationship between the two exists,
unless the relationship is is perfectly horizontal or vertical).
Finally, in order to get a better estimate of the MCC, we can split the set of embeddings
in two: one which will be used to determine the permutation and which will give an in-
sample estimate of the MCC, and the other one to which the previous permutation will be
applied and will give an out-of-sample estimate of MCC.
Weak MCC.
The above deﬁnition of MCC aims to estimate how well each single element
of the embedding is recovered, independently on the other; we call it, following Khemakhem
et al. (2020), strong MCC. However, there may be cases where the recovered embedding
is equal to the correct one up to a more generic linear transformation A. In that case,
we would like to have a way to measure, up to a linear transformation, how far are two
embeddings. Following Khemakhem et al. (2020), we therefore apply Canonical Correlation
Analysis (CCA) Hotelling (1936) to learn A, and after that compute MCC, which we will
call weak MCC. Speciﬁcally, CCA is a way to compute linear transformation A so that the
correlations between corresponding components of A·Y and Z is maximized. The so-deﬁned
weak MCC is therefore a number between 0 and 1 which measures how close is Y to Z after
the best linear transformation is applied to Y. Similarly as before, a part of data is used
to learn the best embedding; we can therefore use fresh samples to get an out-of-sample
estimate along the in-sample one.
27

Pacchiardi and Dutta
B.2 Universal approximation capability
Khemakhem et al. (2020) provide a result (Theorem 3) in which they prove universal ap-
proximation capability of the conditional exponential family. Speciﬁcally, they show that,
considering the dimension of the representations ds as an additional parameter, it is possi-
ble to ﬁnd an arbitrarily good approximation of any conditional probability density p0(x|θ),
provided that X and Θ are compact Hausdorﬀspaces. In practice, good approximation may
be achieved with a value of ds larger than d or p. Moreover, as remarked in the main text,
this result is not concerned with the way the approximating family is ﬁt; indeed, we expect
that this task becomes (both statistically and computationally) more challenging when ds
increases.
Appendix C. Some properties of Score Matching
C.1 Proof of Theorem 6
Our Theorem 6 extends Theorem 1 in Hyv¨arinen (2005), that considers the case X = Rd,
and which is recovered in the case ai = −∞and bi = +∞∀i. Our proof follows Hyv¨arinen
(2005), which however explicitly stated only Assumptions A1 and A2. Following Yu et al.
(2019), we add the additional Assumption A3 which is required for Fubini-Tonelli theorem
to apply.
In order to prove Theorem 6, Assumption A3 can be weakened to Ep0
 ∂2
∂x2
i log pw(X)
 <
∞, ∀w, ∀i = 1, . . . , d.
We state the more general one in the main text as that allows
Theorem 9 for SSM to be proved as well.
Proof Let s0(x) = ∇x log p0(x) denote the score of the data distribution, and analogously
s(x; w) = ∇x log pw(x). Then, Eq. (4) can be rewritten as:
DF (p0∥pw) = 1
2
Z
X
p0(x)∥s0(x) −s(x; w)∥2dx
= 1
2
Z
X
p0(x)

∥s0(x)∥2 + ∥s(x; w)∥2 −2s0(x)T s(x; w)

dx
= 1
2
Z
X
p0(x)∥s0(x)∥2dx
|
{z
}
C
+ 1
2
Z
X
p0(x)∥s(x; w)∥2dx
|
{z
}
A
−
Z
X
p0(x)s0(x)T s(x; w)dx
|
{z
}
B
Note that we can split the integral into the three parts as the ﬁrst two are assumed to be
ﬁnite in A2; as a consequence, B is also ﬁnite thanks to |2ab| ≤a2 + b2.
The ﬁrst element does not depend on w, so that we can safely ignore that. The second
one appears as it is in the ﬁnal Eq. (5). Therefore, we will focus on the last term, which we
can write as:
B = −
d
X
i=1
Z
X
p0(x)s0,i(x)si(x; w)dx;
(13)
Let’s now consider a single i, which we can rewrite as:
−
Z
X
p0(x)∂log p0(x)
∂xi
si(x; w)dx = −
Z
X
p0(x)
p0(x)
∂p0(x)
∂xi
si(x; w)dx = −
Z
X
∂p0(x)
∂xi
si(x; w)dx.
28

Score Matched Neural Exponential Families for LFI
Now, let’s consider the integral over xi ﬁrst, and apply partial integration to it. Doing this
relies on the Fubini-Tonelli’s theorem, which can be safely applied due to B being ﬁnite as
discussed above. We now apply partial integration to the integral over xi:
−
Z b1
a1
Z b2
a2
. . .
Z bd
ad
∂p0(x)
∂xi
si(x; w)dx = −
Z b1
a1
. . .
Z bi−1
ai−1
Z bi+1
ai+1
. . .
Z bd
ad
"
p0(x)si(x; w)|xi↗bi
xi↘ai
−
Z bi
ai
p0(x)∂si(x; w)
∂xi
dxi
#
dx1 . . . dxi−1dxi+1 . . . dxd
=
Z b1
a1
. . .
Z bi−1
ai−1
Z bi+1
ai+1
. . .
Z bd
ad
" Z bi
ai
p0(x)∂si(x; w)
∂xi
dxi
#
dx1 . . . dxi−1dxi+1 . . . dxd
=
Z
X
p0(x)∂si(x; w)
∂xi
dx.
where the second equality holds thanks to Assumption A1. For stating the last equality
rigorously, we need to invoke Fubini-Tonelli’s theorem again; this relies on the assumption:
Z
X
p0(x)∂si(x; w)
∂xi
 dx = Ep0

∂si(X; w)
∂xi
 < ∞,
which is equivalent to Assumption A3.
By repeating this argument for all terms in the sum in Eq. (13), we obtain that:
B =
Z
X
p0(x)
d
X
i=1
∂si(x; w)
∂xi
dx =
Z
X
p0(x)
d
X
i=1
∂2 log pw(x)
∂x2
i
dx,
which concludes our proof.
C.2 Proof of Theorem 7
We give here an extended version of Theorem 7, which we then prove following Hyv¨arinen
(2005).
Theorem 21 Assume ∃w⋆: p0(·) = pw⋆(·). Then:
w = w⋆=⇒DF (p0∥pw) = 0.
Further, if p0(x) > 0 ∀x ∈X, you also have:
DF (p0∥pw) = 0 =⇒pw(·) = p0(·).
Finally, if no other value w ̸= w⋆gives a pdf pw that is equal to pw⋆:
DF (p0∥pw) = 0 =⇒w = w⋆.
29

Pacchiardi and Dutta
Proof The ﬁrst statement is straightforward. For the second one, if DF (p0∥pw) = 0 and
p0(x) > 0 ∀x ∈X, then ∇x log pw(x) = ∇x log p0(x); this in turn implies that log pw(x) =
log p0(x) + c ∀x ∈X for a constant c, which however is 0 as both p0 and pw are pdf’s.
The third statement follows from the second as, for the additional assumption, w⋆is
the only choice of w which gives pw = p0.
C.3 Proof of Theorem 9
Similarly to the SM case, our Theorem 9 extends Theorem 1 in Song et al. (2020), that
considers the case X = Rd, and which is recovered in the case ai = −∞and bi = +∞∀i.
Our proof follows Song et al. (2020), which however explicitly stated only Assumptions A1,
A2 and A4. Following Yu et al. (2019), we add the additional Assumption A3 which is
required for Fubini-Tonelli theorem to apply. The strategy of the proof is very similar to
Theorem 6.
Proof
As before, let s0(x) = ∇x log p0(x) denote the score of the data distribution, and
analogously s(x; w) = ∇x log pw(x). Then, Eq. (6) can be rewritten as:
DFS(p0∥pw) = 1
2
Z
V
Z
X
q(v)p0(x)[vT s0(x) −vT s(x; w)]2dxdv
= 1
2
Z
V
Z
X
q(v)p0(x)

(vT s0(x))2 + (vT s(x; w))2 −2(vT s0(x))(vT s(x; w))

dxdv
=
Z
V
Z
X
q(v)p0(x)
1
2(vT s(x; w))2 −(vT s0(x))(vT s(x; w))

dxdv + C
Note that we can split the integral into the three parts thanks to Assumptions A2 and A4,
which ensure that the expectation of each term is bounded. Additionally, we have absorbed
in the constant C the term which does not depend on w.
In the last row of the above Equation, the ﬁrst term in the square brackets appears in
Eq. (7), which is what we want to prove. We focus therefore on the second term:
−
Z
V
Z
X
q(v)p0(x)

(vT s0(x))(vT s(x; w))

dxdv
= −
Z
V
Z
X
q(v)p0(x)

(vT ∇x log p0(x))(vT ∇x log pw(x))

dxdv
= −
Z
V
Z
X
q(v)

(vT ∇xp0(x))(vT ∇x log pw(x))

dxdv
= −
d
X
i=1
Z
V
Z
X
q(v)

vi
∂p0(x)
∂xi
(vT ∇x log pw(x))

dxdv
(14)
We will now consider one single element of the sum for a chosen i, and consider the integral
over xi ﬁrst. Swapping the order of integration relies on Fubini-Tonelli’s theorem, which
can be safely applied due to the above quantity being ﬁnite as discussed above. We then
30

Score Matched Neural Exponential Families for LFI
apply partial integration to the integral over xi:
−
Z
V
Z b1
a1
Z b2
a2
. . .
Z bd
ad
q(v)

vi
∂p0(x)
∂xi
(vT ∇x log pw(x))

dxdv
= −
Z
V
Z b1
a1
. . .
Z bi−1
ai−1
Z bi+1
ai+1
. . .
Z bd
ad
q(v)
"
vip0(x)(vT ∇x log pw(x))
xi↗bi
xi↘ai
−
Z bi
ai
vip0(x)

vT ∂
∂xi
∇x log pw(x)

dxi
#
dx−i,
where dx−i = dx1 . . . dxi−1dxi+1 . . . dxd. The ﬁrst element in the square bracket goes to 0
thanks to Assumption A1. We are left therefore with:
Z
V
Z b1
a1
. . .
Z bi−1
ai−1
Z bi+1
ai+1
. . .
Z bd
ad
" Z bi
ai
q(v)p0(x)vi

vT ∂
∂xi
∇x log pw(x)

dxi
#
dx−i
=
Z
V
Z
X
q(v)p0(x)vi

vT ∂
∂xi
∇x log pw(x)

dxdv.
The last equality again exploits Fubini-Tonelli theorem, which in this case requires:
Z
V
Z
X
q(v)p0(x)
vi

vT ∂
∂xi
∇x log pw(x)
 dxdv = EV ∼qEX∼p0
Vi

V T ∂
∂xi
∇x log pw(X)
 < ∞.
This is satisﬁed by combining Assumptions A3 and A4, as in fact:
EV ∼qEX∼p0
Vi

V T ∂
∂xi
∇x log pw(X)
 = EV ∼qEX∼p0

Vi
d
X
j=1
Vj
∂2
∂xi∂xj
log pw(X)

≤
d
X
j=1
EV ∼qEX∼p0
ViVj
∂2
∂xi∂xj
log pw(X)
 =
d
X
j=1
EV ∼q |ViVj| · EX∼p0

∂2
∂xi∂xj
log pw(X)

≤
d
X
j=1
q
EV ∼qV 2
i · EV ∼qV 2
j · EX∼p0

∂2
∂xi∂xj
log pw(X)
 ,
where the last inequality holds thanks to Cauchy-Schwarz inequality; Assumptions A3 and
A4 ensure the last row is < ∞.
Back to Eq. (14), we have therefore:
−
d
X
i=1
Z
V
Z
X
q(v)

vi
∂p0(x)
∂xi
(vT ∇x log pw(x))

dxdv
=
Z
V
Z
X
d
X
i=1
q(v)p0(x)vi

vT ∂
∂xi
∇x log pw(x)

dxdv
=
Z
V
Z
X
d
X
i=1
d
X
j=1
q(v)p0(x)vivj
∂2
∂xi∂xj
log pw(x)dxdv
=
Z
V
Z
X
q(v)p0(x)

vT (Hx log pw(x)) v

dxdv.
31

Pacchiardi and Dutta
C.4 Proof of Theorem 10
We give here an extended version of Theorem 10. This is a version of Lemma 1 in Song
et al. (2020), whose proof we adapt.
Theorem 22 Assume ∃w⋆: p0(·) = pw⋆(·). Then:
w = w⋆=⇒DFS(p0∥pw) = 0.
Further, if p0(x) > 0 ∀x ∈X, you also have:
DFS(p0∥pw) = 0 =⇒pw(·) = p0(·).
Finally, if no other value w ̸= w⋆gives a pdf pw that is equal to pw⋆:
DFS(p0∥pw) = 0 =⇒w = w⋆.
Proof The ﬁrst statement is straightforward.
For the second one, if DF (p0∥pw) = 0 and p0(x) > 0 ∀x ∈X, then:
Z
V
q(v)(vT (∇x log p0(x) −∇x log pw(x)))2dv = 0
⇐⇒
Z
V
q(v)vT (∇x log p0(x) −∇x log pw(x))(∇x log p0(x) −∇x log pw(x))T vdv = 0
⇐⇒(∇x log p0(x) −∇x log pw(x))T E[V V T ](∇x log p0(x) −∇x log pw(x)) = 0
(⋆)
⇐⇒∇x log p0(x) −∇x log pw(x) = 0
⇐⇒log pw(x) = log p0(x) + c ∀x ∈X,
where in the third line above V is a random variable distributed according to q, for which
therefore E[V V T ] is positive deﬁnite by Assumption A4, which ensures equivalence (⋆)
holds. Additionally, as both p0 and pw are pdf’s (and therefore normalized), the constant
c = 0.
The third statement follows from the second as, for the additional assumption, w⋆is
the only choice of w which gives pw = p0.
C.5 Proof of Theorem 12
We give here an extended version of Theorem 12, which we then prove.
Theorem 23 Let Y = t(X) ∈Y for a bijection t, and denote by pY
0 and pY
w the distributions
on Y induced by the distributions p0 and pw on X. Assume ∃w⋆: p0(·) = pw⋆(·), and let D
denote either DF or DFS. Then:
w = w⋆=⇒D(pY
0 ∥pY
w) = 0.
32

Score Matched Neural Exponential Families for LFI
Further, if p0(x) > 0 ∀x ∈X and Assumption A4 holds, you also have:
D(pY
0 ∥pY
w) = 0 =⇒pw(·) = p0(·).
Finally, if no other value w ̸= w⋆gives a pdf pw that is equal to pw⋆:
D(pY
0 ∥pY
w) = 0 =⇒w = w⋆.
Proof The proof relies on the equivalence between distributions for the random variables
Y and X; in fact, by ﬁxing y = t(x) and denoting by |Jt(x)| the determinant of the Jaco-
bian matrix of t evaluated in x, we have that pY
0 (y) =
p0(x)
|Jt(x)| and pY
w(y) =
pw(x)
|Jt(x)|, so that
p0(·) = pw(·) ⇐⇒pY
0 (·) = pY
w(·). The ﬁrst and third statements follow directly from this
fact by applying Theorem 21 (if D is chosen to be DF ) or Theorem 22 (if D = DFS) to
D(pY
0 ∥pY
w); for the second, notice also that p0(x) > 0 ∀x ∈X =⇒pY
0 (y) ∀y ∈Y > 0 as t is
a bijection; then, Theorem 21 or Theorem 22 imply that pY
0 (·) = pY
w(·) =⇒p0(·) = pw(·).
C.6 Discussion on the positivity condition for SM
We follow here the discussion in Appendix D of Arbel and Gretton (2018).
Consider again the Fisher divergence in Eq. (4); we want to understand the conditions
under which this is a divergence between probability measures, which essentially means it
is 0 if and only if the probability distributions are the same.
Besides the fact that both p0 and pw need to be continuous (otherwise the gradient
would be a delta function), it turns out that a necessary condition is that p0 is positive
on the whole space to which the random variable belong (let’s say X), if we don’t put any
restrictions on pw. Otherwise, the following scenario may happen (see Appendix D in Arbel
and Gretton, 2018): consider the case in which p0(x) is a mixture of two densities supported
on disjoint sets: p0(x) = αApA(x) + αBpB(x), with non-negative weights αA, αB, such that
pA(x) > 0 ⇐⇒x ∈XA,
pB(x) > 0 ⇐⇒x ∈XB,
XA, XB ∈X;
XA ∩XB = ∅.
Note that this implies ∃x ∈X : p0(x) = 0. In this setting, any pw of the form pw(x) =
βApA(x) + βBpB(x) will give DF (p0∥pw) = 0. This can be seen by computing the Fisher
divergence directly:
33

Pacchiardi and Dutta
DF (p0∥pw) = 1
2
Z
X
p0(x)∥∇x log p0(x) −∇x log pw(x)∥2dx
= 1
2
Z
X
p0(x)∥∇x log(αApA(x) + αBpB(x)) −∇x log(βApA(x) + βBpB(x))∥2dx
= 1
2
Z
XA
p0(x)∥∇x log(αApA(x)) −∇x log(βApA(x))∥2dx+
1
2
Z
XB
p0(x)∥∇x log(αBpB(x)) −∇x log(βBpB(x))∥2dx
= 1
2
Z
XA
p0(x) ∥∇x log pA(x) −∇x log pA(x)∥2
|
{z
}
=0
dx+
1
2
Z
XB
p0(x) ∥∇x log pB(x) −∇x log pB(x)∥2
|
{z
}
=0
dx
= 0,
where the third equality relies on splitting the integration domain over the two subsets on
which pA and pB are supported (and the other is 0) and the fourth equality relies on the
presence of the logarithmic derivatives, that removes the importance of the mixture weights.
Due to the above, in the case of the conditional Fisher divergence in Eq. (9), p0(x|θ)
needs to be supported on the whole X for each θ in order for DE
F (p0|pw) = 0 ⇐⇒p0(·|θ) =
pw(·|θ) π(θ)-almost everywhere.
This can be seen by considering the case of univariate
parameter θ and by choosing p0(x|θ) = pA(x)H(θ) + (1 −H(θ))pB(x), where pA and pB
are as above and H(·) represents the Heaviside function. In this case, choosing pw(x|θ) =
q(x), where q(x) = βApA(x) + βBpB(x) will give DE
F (p0|pw) = 0, as for each ﬁxed θ,
DF (p0(·|θ)|pw(·|θ)) falls in the case considered above.
C.7 Equivalence of Correction Factor and Transformed Score Matching
As discussed in the main text (Section 3.2), the ﬁrst extension of score matching to non-
negative random variables was given in Hyv¨arinen (2007):
D+
F (p0∥pw) = 1
2
Z
Rd
+
p0(x)∥∇x log p0(x) ⊙x −∇x log pw(x) ⊙x∥2dx,
(15)
where ⊙denotes element wise product between vectors and Rd
+ is the positive octant of Rd.
The correction factor x relaxes assumption A1 to p0(x)x2
i
∂
∂xi log pw(x) →0, so that it is
possible to get an explicit form of the above with looser assumptions. This is an example
of Corrected Score Matching (CorrSM, 3.2), in which the issue arising due to distribution
having a compact support is ﬁxed by introducing a correction factor in the formulation of
the objective.
Yu et al. (2019) proposed a more general score matching for non-negative random vari-
ables by allowing freedom of choice in the factor that is used in the integrand to correct for
the integration by parts step (Appendix C.1), leading to the following objective:
D+
F (p0∥pw) = 1
2
Z
Rn
+
p0(x)∥(∇x log p0(x)) ⊙
p
h(x) −(∇x log pw(x)) ⊙
p
h(x)|∥2dx,
(16)
34

Score Matched Neural Exponential Families for LFI
where h(x) has the same dimension as x, and has positive elements almost surely.
The explicit formulation associated to Eq. (16) can be obtained under the following
assumptions:
A1b p0(x)hj(xj)∂j log pw(x) →0 for xi ↘0 and xi ↗∞, ∀w, i,
A2b Ep0∥∇x log p0(X) ⊙h1/2(X)∥2
2 < ∞, Ep0∥∇x log pw(X) ⊙h1/2(X)∥2
2 < ∞∀w,
A3b Ep0∥(∇x log pw(X)⊙h(X)))′∥1 < ∞∀w, where the prime symbol denotes element-wise
diﬀerentiation.
Under the above assumption, Eq. (16) is equal to:
D+
F (p0∥pw) =
Z
Rn
+
p0(x)
d
X
i=1
"
1
2hi(x)
∂log pw(x)
∂xi
2
+
hi(x)
∂2 log pw(x)
∂x2
i

+ h′
i(x)∂log pw(x)
∂xi
#
dx + C,
where C is a constant with respect to pw.
In Proposition 2 in Yu et al. (2019), they give a result similar to our Theorems 7
and 12 guaranteeing that minimization of D+
F (p0∥pw) is a valid procedure for estimating a
probabilistic model. When considering the ﬁnite-sample estimate of C.7, diﬀerent choices
of h(x) may allow to focus more on smaller/larger values of x, which may in practice have
better properties than the original form for non-negative data in Hyv¨arinen (2007), which
is recovered for h(x) = x2 (where the square is applied element-wise).
This formulation in Eq. (16), albeit originally considered for non-negative random vari-
ables only, can be extended to random variables with any bounded domain, by choosing a
suitable function h and modifying A1b to hold for xi going to the limits of the domain.
In the next Sections, we therefore compare this approach with TranSM without specifying
the domain; we will show that both the implicit and explicit formulation are the same
with both TranSM and CorrSM, implying that the two are equivalent (we will show this in
the speciﬁc case in which the transformation and the function h act independently on the
diﬀerent coordinates, but we believe this to be the case more in general; see Appendix C.9).
C.7.1 Equivalence of the implicit form
As mentioned in the main text (Section 3.2), another approach to apply Fisher divergence to
distributions with bounded domain (on one side or both) is to transform the data space to
the real line and then apply standard Fisher divergence; we called this Transformation Score
Matching (TranSM). Let us denote t such a transformation, which we assume to be bijective.
Then, starting from p0(x) and pw(x) we get p0(y) = p0(x)
|Jt(x)| and pw(y) = pw(x)
|Jt(x)| for y = t(x),
where Jt is the Jacobian matrix of t and | · | denotes here the determinant; here, diﬀerently
from the main text, we use a lighter notation where p0(y) and p0(x) are two diﬀerent
densities associated to the diﬀerent name of the argument (same for pw). We investigate
what is the Fisher divergence between the densities of the transformed distributions. Recall
that p0(y)dy = p0(x)dx, for y = t(x). Moreover, we also have that:
35

Pacchiardi and Dutta
∇yg(y) = Jt−1(t(x))∇xg(t(x)) = (Jt(x))−1∇xg(t(x)),
where the second equality comes from the fact that Jt−1(t(x)) = (Jt(x))−1 due to the inverse
function theorem. Then, we can compute the Fisher Divergence between p0(y) and pw(y)
(corresponding to the TranSM objective):
DF (p0(y)∥pw(y)) = 1
2
Z
p0(y)∥∇y log p0(y) −∇y log pw(y)∥2dy
= 1
2
Z
p0(x)∥(Jt(x))−1[∇x log p0(x) −∇x log |Jt(x)| −∇x log pw(x) + ∇x log |Jt(x)|]∥2dx
= 1
2
Z
p0(x)∥(Jt(x))−1[∇x log p0(x) −∇x log pw(x)]∥2dx.
In the rather common case in which the transformation t acts on each component inde-
pendently, the Jacobian matrix is diagonal; in this case, the latter is equivalent to Eq. (16)
upon deﬁning h(x) to be a vector containing the squares of the diagonal elements of the
Jacobian, i.e. putting
p
hi(x) = (Jt(x))−1
ii .
C.7.2 Equivalence of the explicit form
For both TranSM and CorrSM it is possible to get an explicit form of the objective (Eqs. 5
and C.7), in which the integrand does not depend on the data distribution p0. In case in
which the transformation t acts on the diﬀerent components independently, the original
explicit divergence for the transformed variable Y = t(X) is equivalent to the corrected
explicit divergence for the original X, analogously to the implicit Fisher divergence form.
In fact, by applying the deﬁnition of explicit Fisher divergence (Eq. 5) to the transformed
Y , you get:
DF (p0(y)∥pw(y)) =
Z
p0(y)
d
X
i=1
"
1
2
∂log pw(y)
∂yi
2
+
∂2 log pw(y)
∂y2
i
#
|
{z
}
⋆
dy + C
where C is a constant with respect to pw. Considering only the term in square brackets,
denoting ∂i =
∂
∂xi and setting
p
hi(x) = (Jt(x))−1
ii , we get:
⋆= 1
2hi(x)

(∂i log pw(x))2 + 1
4 (∂i log hi(x))2 + ∂i log pw(x) · ∂i log hi(x)

+ 1
2h′
i(x) · ∂i log pw(x) + 1
4h′
i(x)∂i log hi(x) + hi(x)∂2
i log pw(x) + 1
2hi(x)∂2
i log hi(x)
= 1
2hi(x) (∂i log pw(x))2 + 1
8hi(x) (∂i log hi(x))2
+ h′
i(x) · ∂i log pw(x) + 1
4h′
i(x)∂i log hi(x) + hi(x)∂2
i log pw(x) + 1
2hi(x)∂2
i log hi(x).
The blue terms are the same that appear in the CorrSM explicit formulation (Eq. C.7),
while all other terms are constants with respect to pw.
We have shown therefore that CorrSM and TranSM are equivalent in both the explicit
and implicit formulation if the transformation is applied independently on the elements of
36

Score Matched Neural Exponential Families for LFI
x. Therefore, the two approaches are completely equivalent when it comes to minimizing
them.
C.8 Speciﬁc formulation of TranSM
We discuss here the transformations we apply in this work in TranSM; speciﬁcally, we only
consider the case in which the support for the multivariate x is deﬁned by an intersection of
element-wise inequalities, i.e. x ∈Nd
i=1(ai, bi), where ai, bi can take on the values ±∞as
well. In this case, then, a transformation can be applied independently on each element of
x. We consider here the following transformations (which are also used in the Stan package
Carpenter et al., 2017):
• When X ∈[0, ∞)d, the transformation we use is yi = log(xi) ∈Rd. This corresponds
to diagonal Jacobian with elements (Jt(x))−1
ii
= xi, so that the above expression
becomes the same as the original Fisher divergence for non-negative random variables
discussed in Eq. (15).
• More generally, if xi ∈[ai, +∞) for |ai| < ∞, we can transform the data as yi =
log(xi −ai) ∈R, while if xi ∈(−∞, bi] for |bi| < ∞, we simply reverse the trans-
formation: yi = log(bi −xi) ∈R.
These correspond to (Jt(x))−1
ii
= xi −ai and
(Jt(x))−1
ii = bi −xi.
• Finally, if xi ∈(ai, bi) for |ai|, |bi| < ∞, we can use the transformation deﬁned as:
yi = t(xi) = logit

xi−ai
bi−ai

with inverse transformation xi = t−1(yi) = a+(b−a)
eyi
eyi+1.
This corresponds to (Jt(x))−1
ii = (xi−ai)(bi−xi)
bi−ai
.
C.9 Score matching for distributions with more general domain
As highlighted in the main text, across this work we are concerned with applying score
matching to distributions whose support is deﬁned by independent constraints on the dif-
ferent coordinates, as for instance X = Nd
i=1(ai, bi). That is arguably the most common
case in the literature. However, there have been some recent works which applied SM to
more general cases. For instance, Mardia et al. (2016) devised a way to apply it to a di-
rectional distribution deﬁned on an oriented Riemannian manifold (for instance, a sphere).
It is interesting how their derivation of the explicit form from the implicit one relies on
the classical divergence theorem (also known as Stokes’ theorem), of which the partial in-
tegration trick used in Theorem 6 is a speciﬁc case. Liu and Kanamori (2019) introduced
instead a way to apply score matching for a distribution on Euclidean space with complex
truncation boundaries; their approach boils down to introducing a smart correction factor
which goes to 0 at the boundary (thus allowing partial integration) but still being tractable;
again, they need a more general version of Theorem 6 to obtain an objective for which the
integrand does not depend on the data distribution.
In Appendix C.7, we established that CorrSM and TranSM are equivalent if the trans-
formation is applied independently on the elements of x, which requires the domain to be
deﬁned by independent constraints on the coordinates. In the more general case of a ir-
regular subset of Euclidean space (as in the setup of Liu and Kanamori, 2019), it is not
37

Pacchiardi and Dutta
clear whether it is always possible to associate a correction factor to a transformation which
maps the space to Rd. That seems to be plausible if the domain satisﬁes some regularity
conditions which may be related to convexity (for instance think of a triangle in R2, which
can be easily stretched to cover the full space). We are not aware however of any work
investigating this.
C.10 Score matching with exponential family
We consider here the exponential family:
pw(x|θ) = exp(ηw(θ)T fw(x))/Zw(θ),
and want to ﬁnd the value of w minimizing either DE
F (p0∥pw) or DE
FS(p0∥pw), which are
deﬁned in Eq. (10).
Under the conditions discussed in Section 3.3, if π(θ) > 0 ∀θ, then DE
F (p0∥pw) = 0 and
DE
FS(p0∥pw) = 0 ⇐⇒pw(x|θ) = p0(x|θ) π(θ)-almost everywhere. In this case, if fw and
ηw satisfy the conditions required for the theorems mentioned in Appendix B.1 to hold,
then fw and ηw are respectively suﬃcient statistics and natural parameters of p0.
In order to ﬁnd the value of the empirical estimate of the explicit for of both DE
F (p0∥pw)
and DE
FS(p0∥pw), we insert the deﬁnition of the exponential family with in Eq. 11, which
leads to:
ˆJ(w) = 1
N
N
X
j=1
" d
X
i=1
 
1
2

ηw(θ(j))T ∂
∂xi
fw(x(j))
2
+ ηw(θ(j))T ∂2
∂x2
i
fw(x)
!#
,
ˆJS(w) =
1
NM
N
X
j=1
M
X
k=1
"
v(j,k),T Hx(ηw(θ(j))T fw(x(j)))v(j,k) + 1
2
d
X
i=1

ηw(θ(j))T ∂
∂xi
fw(x(j))
2#
.
Note that the objective does not change if you set fw(x) to c + fw(x), for a constant
vector c; in fact, this constant gets absorbed into the normalizing constant in pw(x|θ).
Similarly, ηw(θ)T fw(x) = (1/c · ηw(θ))T (c · fw(x)) for some constant c ̸= 0. Therefore,
statistics and corresponding parameters are only deﬁned up to a scale with respect to
one another; if you use two Neural Networks to learn both of them, diﬀerent network
initializations may lead to diﬀerent learned statistics and natural parameters, but their
product should be ﬁxed (up to translation of fw(x)).
However, this degeneracy may make training the approximate likelihood pw with the
score matching approach harder. In order to improve training, we usually add a Batch
Normalization layer on top of the ηw network. Basically, Batch Normalization ﬁxes the
scale of the output of ηw over a training batch, therefore removing this additional degree of
freedom and making training easier. We discuss in more detail this in Appendix E.2.
Appendix D. Scoring Rules
A Scoring Rule (SR) S (Dawid and Musio, 2014; Gneiting and Raftery, 2007) is a function
of a probability distribution over X and of an observation in X.
In the framework of
38

Score Matched Neural Exponential Families for LFI
probabilistic forecasting, S(P, y) represents the penalty which you incur when stating a
forecast P for an observation y.10
If the observation y is a realization of a random variable Y with distribution Q, the
expected Scoring Rule can be deﬁned as:
S(P, Q) := EY ∼QS(P, Y ),
where we overload notation in the second argument of S. The Scoring Rule S is said to be
proper relative to a set of distributions P(X) over X if
S(Q, Q) ≤S(P, Q) ∀P, Q ∈P(X),
i.e., if the expected Scoring Rule is minimized in P when P = Q. Moreover, S is strictly
proper relative to P(X) if P = Q is the unique minimum:
S(Q, Q) < S(P, Q) ∀P, Q ∈P(X) s.t. P ̸= Q;
when minimizing an expected strictly proper Scoring Rule, a forecaster would provide their
true belief (Gneiting and Raftery, 2007).
By following Dawid and Musio (2014), we deﬁne the divergence related to a proper
Scoring Rule as D(P, Q) := S(P, Q) −S(Q, Q) ≥0. Notice that P = Q =⇒D(P, Q) = 0,
but there may be P ̸= Q such that D(P, Q) = 0. However, if S is strictly proper, D(P, Q) =
0 ⇐⇒P = Q, which is the commonly used condition to deﬁne a statistical divergence
(as for instance the Kullback-Leibler, or KL, divergence). Therefore, each strictly proper
Scoring Rule corresponds to a statistical divergence between probability distributions
In the following, we detail the two Scoring Rules which we consider in the main text
(Section 5.3).
Energy score
The energy score is given by:
S(β)
E (P, y) = 2 · E
h
∥X −y∥β
2
i
−E
h
∥X −X′∥β
2
i
,
X ⊥⊥X′ ∼P,
where β ∈(0, 2) and ⊥⊥denotes independence between random variables. This is a strictly
proper Scoring Rule for the class Pβ(X) of probability measures P such that EX∼P ∥X∥β <
∞(Gneiting and Raftery, 2007). The related divergence is the square of the energy distance,
which is a metric between probability distributions (Rizzo and Sz´ekely 2016)11:
DE(P, Q) = 2 · E
h
∥X −Y ∥β
2
i
−E
h
∥X −X′∥β
2
i
−E
h
∥Y −Y ′∥β
2
i
,
for X ⊥⊥X′ ∼P and Y ⊥⊥Y ′ ∼Q.
10. Some authors (Gneiting and Raftery, 2007) use the convention of S(P, y) representing a reward rather
than a penalty, which is equivalent up to change of sign.
11. The probabilistic forecasting literature (Gneiting and Raftery, 2007) use a diﬀerent convention of the
energy score and the subsequent kernel score, which amounts to multiplying our deﬁnitions by 1/2. We
follow here the convention used in the statistical inference literature (Rizzo and Sz´ekely, 2016; Ch´erief-
Abdellatif and Alquier, 2020; Nguyen et al., 2020)
39

Pacchiardi and Dutta
In our case of interest (Section 5.3, Appendix G.4), we are unable to evaluate exactly
S(β)
E
as we do not have a closed form for P. Thus, we obtain samples {xj}m
j=1, xj ∼P, and
unbiasedly estimate the Energy Score with:
ˆS(β)
E ({xj}m
j=1, y) = 2
m
m
X
j=1
∥xj −y∥β
2 −
1
m(m −1)
m
X
j,k=1
k̸=j
∥xj −xk∥β
2 .
In the main text (Section 5.3), we use β = 1, in which case we simplify notation S(1)
E
= SE.
Kernel score
For a positive deﬁnite kernel k(·, ·) (see Gretton et al. 2012), the kernel
Scoring Rule for k is deﬁned as (Gneiting and Raftery, 2007):
Sk(P, y) = E[k(X, X′)] −2 · E[k(X, y)],
X ⊥⊥X′ ∼P.
The corresponding divergence is the squared Maximum Mean Discrepancy (MMD, Gretton
et al., 2012) relative to the kernel k:
Dk(P, Q) = E[k(X, X′)] + E[k(Y, Y ′)] −2 · E[k(X, Y )],
for X ⊥⊥X′ ∼P and Y ⊥⊥Y ′ ∼Q.
The Kernel Score is proper for the class of probability distributions for which E[k(X, X′)]
is ﬁnite (by Theorem 4 in Gneiting and Raftery, 2007). Additionally, it is strictly proper
under conditions which ensure that the MMD is a metric for probability distributions on
X (see for instance Gretton et al., 2012). These conditions are satisﬁed, among others, by
the Gaussian kernel (which we use in this work):
k(x, y) = exp

−∥x −y∥2
2
2γ2

;
(17)
there, γ is a scalar bandwidth, which is tuned as described in Appendix G.4.1. As for the
Energy Score, when the exact form of P is inaccessible and therefore Sk is impossible to
be evaluated exactly, we use samples {xj}m
j=1, xj ∼P, and unbiasedly estimate the Kernel
Score Sk(P, y) with:
ˆSk({xj}m
j=1, y) =
1
m(m −1)
m
X
j,k=1
k̸=j
k(xj, xk) −2
m
m
X
j=1
k(xj, y).
Appendix E. Computational practicalities
E.1 Computational cost of SM and SSM
For SM, as discussed in Section 3.2, exploiting automatic diﬀerentiation libraries to compute
the second derivatives of the log density requires d times more backward derivative compu-
tations with respect to the ﬁrst derivatives. In fact, automatic diﬀerentiation libraries are
able to compute derivatives of a scalar with respect to several variables at once. One single
call is therefore suﬃcient to obtain ∇x log pw(x). However, d additional calls are required to
40

Score Matched Neural Exponential Families for LFI
obtain the second derivatives
∂2
∂x2
i log pw(x), i = 1, . . . , d, which are the diagonal elements of
the Hessian matrix of log pw(x); each additional call computes the gradient of
∂
∂xk log pw(x)
with respect to all components of x, for some k ∈[1, 2, . . . , d]. Algorithm 2 in Song et al.
(2020) gives a pseudocode implementation of this approach. Computational improvement
can be obtained by implementing custom code which performs the gradient computation
in the forward pass (i.e., along the computation of the neural network output for a given
input x). This avoids repeating some computations multiple times, which is done when
performing the backward step repeatedly; however, the implementation is tricky and needs
custom code for each diﬀerent neural network type (we discuss how it can be done for a fully
connected neural network in Appendix E.1.1). Additionally, the computational speed-up
achievable in this way is limited with respect to what is oﬀered by, for instance, SSM.
SSM instead requires only two backward propagation steps independently on the input
size of the network. This is possible by exploiting the vector-Hessian product structure and
computing the linear products with v (which is independent on the input x, thus can be
swapped with gradient computation) after the gradient has been computed once, so that
you only ever require the gradient of a scalar quantity. See Algorithm 1 in Song et al. (2020)
for a precise description of how that can be done.
E.1.1 Forward computation of derivatives
In the standard score matching approach, the ﬁrst and second derivatives of Neural Network
outputs with respect to the inputs are required, namely:
∂fw(x)
∂xi
and
∂2fw(x)
∂x2
i
.
Neural network training is possible thanks to the use of autodiﬀerentiation libraries, which
allow to keep trace of the diﬀerent operations and to automatically compute the gradients
required for training. These libraries can be used to obtain the above derivatives.
However, as discussed previously, it is more eﬃcient to compute the required derivatives
during the forward pass of training (i.e.
when the output of the Neural Network for a
given input is computed).
This requires additional coding eﬀort speciﬁc to the chosen
Neural Network architecture. For instance, Avrutskiy (2017) provide formulas to compute
derivatives of any order recursively for fully connected Neural Networks. For more complex
NN architectures, this approach is not viable as the coding eﬀort becomes substantial.
Additionally, with large d the improvement obtained by forward derivatives computation is
much smaller than what oﬀered, for instance, by SSM.
In the current work, the forward derivatives approach has been implemented for fully
connected Neural Networks and Partially Exchangeable Networks (Appendix E.3). The
computational advantage is evident for the computation of the second derivatives, as shown
below. This approach allowed us to apply SM to relatively high dimensional data spaces
(up to 100 dimensional for the MA(2) and AR(2) case), but not to the larger Lorenz96
model.
Forward computation of derivatives of fully connected NNs
We revisit here the
work in Avrutskiy (2017). Let us consider a fully connected Neural Networks with L layers,
where the weights and biases of the l-th layer are denoted by Wl and bl, l = 1, . . . , L. Let
41

Pacchiardi and Dutta
us denote by x the input of the Neural Network, and by zl the hidden values after the
l-th layer, before the activation function (denoted by σ) is applied. Speciﬁcally, the Neural
Network hidden values are determined by:
z1 = W1 · x + b1,
zl = Wl · σ(zl−1) + bl,
l = 2, . . . , L,
where the activation function is applied element wise. Similar recursive expressions can be
given for the ﬁrst derivatives:
∂z1
∂xi
= (W1)i,·,
∂zl
∂xi
= Wl ·

σ′(zl−1) ⊙∂zl−1
∂xi

,
l = 2, . . . , L,
where (W1)i,· denotes the i-th column of W1, and ⊙denotes element wise multiplication.
Expressions for the second derivatives are instead:
∂2z1
∂x2
i
= 0,
∂2zl
∂x2
i
= Wl ·
"
σ′′(zl−1) ⊙
∂zl−1
∂xi
2
+ σ′(zl−1) ⊙∂2zl−1
∂x2
i
#
,
l = 2, . . . , L,
where here 0 denotes a 0 vector with the same size as z1.
When instead we are interested in cross terms of the form
∂zl
∂xi∂xj , we can apply the
following:
∂z1
∂xi∂xj
= 0,
∂zl
∂xi∂xj
= Wl·

σ′′(zl−1) ⊙∂zl−1
∂xi
⊙∂zl−1
∂xj
+ σ′(zl−1) ⊙∂2zl−1
∂xi∂xj

, l = 2, . . . , L.
With respect to naively using auto-diﬀerentiation libraries, computing the derivatives
in the forward step is much cheaper; speciﬁcally, for fully connected NNs, we found empir-
ically the ﬁrst to scale quadratically with the output size, while the second scales linearly
(Figure 8).
E.2 Batch normalization
The exponential family form used as pw depends on ηw(θ)T fw(x); if you multiply fw(x)
with an invertible matrix A and multiply η with (AT )−1, the product does not change. In
order to remove this additional degeneracy, we use a Batch Normalization (BatchNorm)
layer (Ioﬀe and Szegedy, 2015) to normalize the outputs of ηw(θ). Essentially, BatchNorm
rescales the diﬀerent features to have always the same range across diﬀerent batches. More
speciﬁcally, BatchNorm performs the following operation on y:
˜y =
y −E[Y ]
p
V[Y ] + ϵ
∗γ + b,
where ϵ is a small constant used for numerical stability, and γ and b are two (optionally
learnable) sets of constants with dimension equal to y (set to 1 and 0 respectively by default).
During training, the expectation E and variance V are estimated over the batch of training
samples fed to the Neural Network. In testing mode, BatchNorm rescales the features by
using a ﬁxed estimate of E and V; this estimate is obtained as a running mean over the
batches; let s(Y ) represent either the population expectation or variance. When the t-th
42

Score Matched Neural Exponential Families for LFI
5
10
15
20
Output size (ds)
0
100
200
300
400
500
600
Time (s)
Naive autodif
5
10
15
20
Output size (ds)
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Time (s)
Forward computation
Figure 8: Computational complexity for second order derivatives of Neural Net-
work outputs versus size of the output; we compare the forward computation of derivatives
with naive autodiﬀerentiation. Here, input size is ﬁxed to 100, and one single batch of 5000
samples is fed to the network. Computations are done on a CPU machine with 8 cores.
batch is fed through the network in training mode, the running mean estimate is updated
as follows:
ˆsnew(Y ) = (1 −p) · ˆsold(Y ) + p · st(Y ),
where st represent the estimate on the current batch, ˆsold and ˆsnew respectively the old and
updated running mean and p is a momentum constant which determines how quickly the
running mean changes (the smaller it is, the slower the change of the estimate).
For instance, if the training of a Neural Network is quite unstable, the running estimate
may not be a correct estimate of E and V, so that the test loss across training epochs may
be very spiky, until network training stabilized. To solve this issue, you can either increase
p (so that the running estimate “forgets” past information faster) or, alternatively, do a
forward pass of the training data set (without computing gradients) before evaluating the
test loss, so that the running estimate is more precisely estimated.
Across this work, we apply BatchNorm to y = ηw(θ). Moreover, we do not learn the
translation parameters γ, b, and rather we ﬁx them to be a vector of 1s and 0s.
E.3 Partially Exchangeable Networks
Partially Exchangeable Networks (PENs) were introduced in Wiqvist et al. (2019) as a
Neural Network architecture that satisﬁes the probabilistic invariance of Markovian models.
Speciﬁcally, let consider the case in which x = (x1, . . . , xd) comes from a Markovian
model of order r, i.e.:
p(x|θ) = p(x1|θ)p(x2|x1; θ)p(x3|x2, x1; θ)
d
Y
i=4
p(xi|x1, . . . , xi−1; θ)
= p(x1|θ)
d
Y
i=2
p(xi|xi−r, . . . , xi−1; θ).
43

Pacchiardi and Dutta
This deﬁnition is an extension of the standard Markovianity assumption (of order 1),
which corresponds to p(x|θ) = p(x1|θ) Qd
i=2 p(xi|xi−1; θ), and it means that each element of
x only depends on the last r elements. When r = 0, this corresponds to i.i.d. assumption.
When a model is r-Markovian, the probability density of an observation x is invariant
to r-block-switch transformation, which is deﬁned as follows:
Deﬁnition 24 r-block-switch transformation (Wiqvist et al., 2019) Let xi:j and
xk:l be two non-overlapping blocks with xi:(i+r) = xk:(k+r) and x(j−r):j = x(l−r):l. Then,
denoting b = (i, j, k, l), with j −i ≥r and l −k ≥r:
x = x1:i−1 xi:j x(j+1):(k−1) xk:l x(l+1):M
T (r)
b
(x) = x1:i−1 xk:l x(j+1):(k−1) xi:j x(l+1):M.
Otherwise, if xi:(i+r) ̸= xk:(k+d) or x(j−r):j ̸= x(l−r):l, then T (r)
b
(x) = x.
For instance, let us consider the case in which the data space is X = {0, 1 . . . 9}16, and
r = 2. An example of the above transformation is the following:
x = 1 7 2 3 6 4 5 8 1 7 7 2 9 5 8 1
T (2)
(2,8,11,15)(x) = 1 7 2 9 5 8 1 7 7 2 3 6 4 5 8 1.
The authors of Wiqvist et al. (2019) provide a simple Neural Network structure which
is invariant to the r-block-switch transformation, motivated by the following theorem:
Theorem 25 ((Wiqvist et al., 2019)) Let f : X M →A r-block-switch invariant. If X
is countable, ∃φ : X r+1 →R and ρ : X r × R →A such that:
∀x ∈X M, f(x) = ρ
 
x1:r,
M−r
X
i=1
φ(xi:(i+r))
!
.
(18)
In practice, φ and ρ are two independent Neural Networks (which we take to be fully
connected in our case), giving rise to a PEN of order r.
In Wiqvist et al. (2019), the authors show that the posterior mean of a Markovian
variable of order r needs to be invariant to the r-block-switch transformation. Therefore, this
motivates using a PEN for learning a summary statistics as in the approach by Fearnhead
and Prangle (2012). Here, we use PEN for parametrizing the statistics in the approximating
exponential family as well; this choice implies that the approximating family has the same
Markovianity property as the true distribution, as we discuss in the following.
E.3.1 Results for the exponential family
A deeper connection between r-Markovian probability models and r-block-switch transfor-
mation exists. We can in fact state the following result:
Lemma 26 A probability model p(x|θ) is r-Markovian
⇐⇒
the function x 7→p(x|θ) is
r-block-switch invariant, i.e. p(x|θ) = p(T (r)
b
(x)|θ) ∀T (r)
b
.
44

Score Matched Neural Exponential Families for LFI
Proof The forward direction is straightforward and can be seen by considering the decom-
position of an r-Markovian model.
The converse direction can be shown by contradiction; assume in fact that x 7→p(x|θ) is
r-block-switch invariant but not Markovian. As it is not Markovian, ∃x = (xi, x2, . . . , xn)
for which xi:(i+r) = xk:(k+r) and x(j−r):j = x(l−r):l such that, deﬁning b = (i, j, k, l),
p(x|θ) ̸= p(T (r)
b
(x)|θ).
This is however in contradiction with r-block-switch invariance,
which leads to our result.
In the case where the model we consider has a suﬃcient statistic, we can write p(x|θ) =
h(x)g(t(x)|θ). We get therefore the following corollary, which can be seen by applying the
above result:
Corollary 27 Consider a distribution p(x|θ) = h(x)g(t(x)|θ); if the function h(x) and t(x)
are r-block-switch invariant, then p(x|θ) is r-Markovian.
Without any further assumptions, this result is not enough to say that the suﬃcient
statistics t(x) for a Markovian model is r-block-switch invariant; in fact, the choice t(x) = x
always constitutes a suﬃcient statistic; moreover, in the decomponsition p(x|θ) = h(x)g(t(x)|θ),
it may be that the function t(x) is not r-block-switch invariant but g(t(x)|θ) is, or otherwise
that the product h(x)g(t(x)|θ) is r-block-switch invariant even if the individual terms are
not. We can however get the following result:
Lemma 28 Consider a r-Markovian distribution p(x|θ) = h(x)g(t(x)|θ); if x 7→t(x) is not
an injection mapping, then x 7→h(x) is r-block-switch invariant.
Proof If x 7→t(x) is not an injection, ∃x, x′ such that t(x) = t(x′). If the density is not
degenerate, moreover, ∃θ : p(x|θ), p(x′|θ) > 0. Therefore, we consider the following ratio:
p(x|θ)
p(x′|θ) = h(x)
h(x′) · g(t(x)|θ)
g(t(x′)|θ) = h(x)
h(x′).
Now, the left hand side is r-block-switch invariant with respect to both x and x′ indepen-
dently, implying that h(x) is as well.
We remark that it does not seem possible in general to say anything about t(x), as in
fact it may be that the function t(x) is not r-block-switch invariant but g(t(x)|θ) is. In the
speciﬁc case of an exponential family, however, a more speciﬁc result can be obtained:
Lemma 29 Consider an exponential family distribution p(x|θ) = h(x) exp(η(θ)T f(x))/Z(θ)
which is r-Markovian distribution; if x 7→f(x) is not an injection mapping, then x 7→f(x)
is r-block-switch invariant.
Proof Without loss of generality, we consider the case in which all elements of η(θ) are not
constant with respect to θ; if this is not the case, in fact, you can redeﬁne the exponential
family by incorporating the elements of f(x) corresponding to the constant ones of η in the
h(x) factor.
45

Pacchiardi and Dutta
Now, h(x) is r-block-switch invariant thanks to Lemma 28. Consider now the following
decomposition:
log p(x|θ) = log h(x) −log Z(θ) +
X
i
fi(x)ηi(θ);
as that needs to be r-block-switch invariant for any θ, this can happen only if each of the
fi(x) elements are r-block-switch invariant.
Overall, these results imply that an exponential family in which f is parametrized with
a PEN of order r is r-Markovian. Moreover, provided that f is not an injection mapping, all
r-Markovian exponential families have f which satisfy the r-block-switch invariant property,
which is imposed by using a PEN network of order r.
E.3.2 Forward computation of derivatives for PENs
We give here the derivation for the forward computation of derivatives with PENs. If we
pick here φ and ρ to be fully connected Neural Networks, we can moreover apply the forward
computation of derivatives for them and we are able to compute the derivatives for PENs
at a much lower cost with respect to using automatic diﬀerentiation libraries.
In Eq. (18), let us denote for brevity z = PM−r
i=1 φ(xi:(i+r)). We are interested now in
computing the derivative:
∂f
∂xj
=
∂
∂xj
ρ(x1:r, z),
where note that z depends in general on xi. Therefore, in computing the above, we need to
compute the derivative with respect to both arguments; let us denote by
∂′
∂xj the derivative
with respect to the ﬁrst argument. Then, we have:
∂f
∂xj
= ∂′
∂xj
ρ(x1:r, z) · 1[j ≤r] + ∂
∂z ρ(x1:r, z) · ∂z
∂xj
,
where the second term is (note that z and ρ are multivariate, so that
∂′
∂xj ρ(x1:r, z) is a
Jacobian matrix):
∂z
∂xj
=
∂
∂xj
M−r
X
i=1
φ(xi:(i+r)) =
j
X
i=j−r
i≥1
∂
∂xj
φ(xi:(i+r)),
where all other terms of the sum disappear as they do not contain xj.
Now, we are interested in obtaining the second derivative terms:
∂2f
∂x2
j
=
∂
∂xj
∂f
∂xj
=
∂
∂xj
"
∂′
∂xj
ρ(x1:r, z) · 1[j ≤r] +
X
k
∂
∂zk
ρ(x1:r, z)∂zk
∂xj
#
= ∂′2
∂x2
j
ρ(x1:r, z) · 1[j ≤r] + 2
X
k
∂′
∂xj
∂
∂zk
ρ(x1:r, z) · ∂zk
∂xj
· 1[j ≤r]
+
X
k,k′
∂2
∂zk∂zk′ ρ(x1:r, z)∂zk
∂xj
∂zk′
∂xj
+
X
k
∂
∂zk
ρ(x1:r, z)∂2zk
∂x2
j
;
46

Score Matched Neural Exponential Families for LFI
in the above expression, P
k runs over the elements of z and ∂′2
∂x2
j denotes second derivative
with respect to the ﬁrst element.
Note that all terms appearing in the above formulas
contain ﬁrst and second derivatives of the Neural Networks φ and ρ with respect to one
single input, except for the terms highlighted in red. In order to compute that, obtaining
the full hessian matrix of ρ is required. We remark that the latter can be very large in case
the input dimension is large, therefore leading to memory overﬂow issues.
E.4 Exchange MCMC
For convenience, we describe here the ExchangeMCMC algorithm by Murray et al. (2012).
We consider the task of sampling from a posterior distribution π(θ|x). We can evaluate
an unnormalized version of the likelihood ˜p(x|θ), and we denote the normalized version as
p(x|θ) = ˜p(x|θ)/Z(θ), Z(θ) being an intractable normalizing constant. We want to build an
MCMC chain by using a proposal distribution q(·|θ; x) (which optionally depends on the
considered x as well). Usually, the standard Metropolis acceptance threshold for a proposal
θ′ is deﬁned as:
α = π(θ′|x)q(θ|θ′; x)
π(θ|x)q(θ′|θ; x) = ˜p(x|θ′)q(θ|θ′; x)π(θ′)
˜p(x|θ)q(θ′|θ; x)π(θ) · Z(θ)
Z(θ′),
where the last factor cannot be evaluated, as we do not have access to the normalizing
constant.
The ExchangeMCMC algorithm proposed by Murray et al. (2012) bypasses this issue
by drawing an auxiliary observation x′ ∼p(·|θ′) and deﬁning the acceptance probability as:
α = p(x|θ′)q(θ|θ′; x)π(θ′)
p(x|θ)q(θ′|θ; x)π(θ) · p(x′|θ)
p(x′|θ′) = ˜p(x|θ′)˜p(x′|θ)q(θ|θ′; x)π(θ′)
˜p(x|θ)˜p(x′|θ′)q(θ′|θ; x)π(θ) · 
Z(θ)

Z(θ′) · 
Z(θ′)

Z(θ) .
(19)
Here, all the normalizing constants cancel out, so that the acceptance threshold can
be evaluated explicitly, at the expense of drawing a simulation from the likelihood for each
MCMC step. Murray et al. (2012) showed that an MCMC chain using the above acceptance
rate targets the correct posterior π(θ|x). The resulting algorithm is given in Algorithm 1:
Algorithm 1 Original exchangeMCMC algorithm (Murray et al., 2012).
Require: Initial θ, number of iterations T, proposal distribution q, observation x.
1: for i = 1 to T do
2:
Propose θ′ ∼q(θ′|θ; x)
3:
Generate auxiliary observation x′ ∼p(·|θ′)
4:
Compute acceptance threshold α as in Eq. (19)
5:
With probability α, set θ ←θ′
6: end for
Bridging.
When considering more closely the acceptance rate in Eq. (19), it can be
seen that it depends on two ratios:
p(x|θ′)
p(x|θ) represents how well the proposed parameter
value explains the observation with respect to the previous parameter value, while instead
p(x′|θ)
p(x′|θ′) measures how well the auxiliary variable (generated using θ′) can be explained with
47

Pacchiardi and Dutta
parameter θ. Therefore, even if the former is large and θ would be a suitable parameter
value, α can still be small if the auxiliary random variable is not explained well by the
previous parameter value. This can lead to slow mixing of the chain; to improve on this,
Murray et al. (2012) proposed to sample a set of auxiliary variables (x′
0, x′
1, . . . , x′
K) from
intermediate distributions in the following way12: consider a set of densities
˜pk(x|θ, θ′) = ˜p(x|θ′)βk ˜p(x|θ)1−βk,
βk = K −k + 1
K + 1
;
x′
0 is generated from p(·|θ′) as before, and then each x′
k is generated from R(·|x′
k−1; θ, θ′),
which denotes a Metropolis-Hastings transition kernel starting from x′
k−1 with stationary
density ˜pk(·|θ, θ′). Then, the acceptance rate is modiﬁed as follows:
α = ˜p(x|θ′)q(θ|θ′; x)π(θ′)
˜p(x|θ)q(θ′|θ; x)π(θ) ·
K
Y
k=0
˜pk+1(x′
k|θ, θ′)
˜pk(x′
k|θ, θ′) .
(20)
The overall algorithm is given in Algorithm 2. Note that K = 0 recovers the original
ExchangeMCMC. This procedure generally improves the acceptance rate as it basically
introduces a sequence of intermediate updates to the auxiliary data which by smoothening
out the diﬀerence between the two distributions.
Algorithm 2 ExchangeMCMC algorithm with bridging (Murray et al., 2012).
Require: Initial θ, number of iterations T, proposal distribution q, number of bridging
steps K, observation x.
1: for i = 1 to T do
2:
Propose θ′ ∼q(θ′|θ; x)
3:
Generate auxiliary observation x′
0 ∼p(·|θ′)
4:
for k = 1 to K do
▷Bridging steps
5:
Generate x′
k ∼R(·|x′
k−1; θ, θ′)
6:
end for
7:
Compute acceptance threshold α as in Eq. (20)
8:
With probability α, set θ ←θ′
9: end for
ExchangeMCMC without perfect simulations.
If, as in the setup considered across
this work, we are not able to sample from p(·|θ′) as it is required in the ExchangeMCMC
algorithm (line 3 in Alg. 1), Murray et al. (2012) suggested to run Tin steps of an MCMC
chain on x targeting p(·|θ′) at each step of ExchangeMCMC; if Tin is large enough, the
last sample can be considered as (approximately) drawn from p(·|θ′) itself and used in
place of the unavailable perfect simulation.
In practice, however, this only leads to an
approximate ExchangeMCMC algorithm, as at each iteration of the inner chain a ﬁnite
Tin is used, so that the inner chain would not perfectly converge to its target; for this
reason, even an inﬁnitely long outer chain would not target the right posterior for any ﬁnite
12. Diﬀerently from the rest of the work, here subscripts do not denote vector components, but rather
diﬀerent auxiliary variables.
48

Score Matched Neural Exponential Families for LFI
Tin. Nonetheless, this approach was shown empirically to work satisfactorily in Caimo and
Friel (2011); Everitt (2012); Liang (2010). Some theoretical guarantees (albeit under strong
conditions), are given in in Appendix B by Everitt (2012), which bounds the total variation
distance between target of approximate ExchangeMCMC with ﬁnite Tin and the target of
the exact one, and shows that they become equal when Tin →∞.
In Liang (2010), they argue that starting the inner chain from the observation value
improves convergence; we adapt this approach in our implementation (Algorithm 3).
Algorithm 3 ExchangeMCMC algorithm (Murray et al., 2012) with inner MCMC.
Require: Initial θ, number of iterations T and Tin, proposal distributions q and qx, obser-
vation x.
1: for i = 1 to T do
▷Outer chain
2:
Propose θ′ ∼q(θ′|θ; x)
3:
Set x′ = x
▷Start inner chain from the observation
4:
for j = 1 to Tin do
▷Inner chain
5:
Propose x′′ ∼qx(x′′|x′)
6:
With probability pw(x′′|θ′)qx(x′|x′′)
pw(x′|θ′)qx(x′′|x′) , set x′ = x′′
7:
end for
8:
Compute acceptance threshold α as in Eq. (19)
▷This uses last point of inner
MCMC
9:
With probability α, set θ ←θ′
10: end for
Note that it is still possible to run bridging steps after the inner MCMC to sample from
p(·|θ′); the algorithm combining bridging and inner MCMC, which is used across this work,
is given in Algorithm 4).
Related algorithms.
Algorithms for sampling from doubly-intractable targets which are
suitable for parallel computing exist, for instance Caimo and Friel (2011) propose a parallel-
chain MCMC algorithm, while Everitt et al. (2017) build instead an SMC-type algorithm
which is also capable of recycling information from past simulations. However, in this work
we stick to using ExchangeMCMC, which turned out to be relatively cheap to use and easy
to implement.
Finally, we remark that Liang et al. (2016) proposed an algorithm which is inspired
from ExchangeMCMC and, in the case of impossible perfect sampling, still targets the right
invariant distribution. It works by considering a set of parallel chains targeting p(·|θ(i)) for
a ﬁxed set of {θ(i)} and iteratively updating those and the main chain over θ. The algorithm
relies on some assumptions which are probably satisﬁed in practice (as discussed in Park
and Haran, 2018). It also requires some hand-tuning and needs to keep in memory a large
amount of data, which may hinder its applicability. For this reason, we do not investigate
using that here.
E.4.1 Implementation details
Acceptance rate tuning.
In our implementation of ExchangeMCMC, we discarded some
burn-in steps to make sure the chain forgets its initial state. During burn-in, moreover, after
49

Pacchiardi and Dutta
Algorithm 4 ExchangeMCMC algorithm (Murray et al., 2012) with inner MCMC and
bridging.
Require: Initial θ, number of iterations T and Tin, number of bridging steps K, proposal
distributions q and qx, observation x.
1: for i = 1 to T do
▷Outer chain
2:
Propose θ′ ∼q(θ′|θ; x)
3:
Set x′ = x
▷Start inner chain from the observation
4:
for j = 1 to Tin do
▷Inner chain
5:
Propose x′′ ∼qx(x′′|x′)
6:
With probability pw(x′′|θ′)qx(x′|x′′)
pw(x′|θ′)qx(x′′|x′) , set x′ = x′′
7:
end for
8:
Set x′
0 = x′
9:
for k = 1 to K do
▷Bridging steps
10:
Generate x′
k ∼R(·|x′
k−1; θ, θ′)
11:
end for
12:
Compute acceptance threshold α as in Eq. (20)
13:
With probability α, set θ ←θ′
14: end for
each window of 100 outer steps, we tuned the proposal sizes considering the acceptance rate
in the window, and increasing (respectively decreasing) if that is too large (small) with
respect to a chosen interval (see below). Notice that we applied this strategy independently
to the proposal size for the outer chain, the inner chain and the bridging step, when the
latter is used. When tuning the proposal size for the inner chain or bridging steps, we
considered the overall acceptance rate over each window of 100 outer steps. We remark how
this was only done during burn-in, so that the convergence properties of the chain were not
aﬀected.
For inner and outer steps as well as bridging, we found that a target acceptance rate
in the interval [0.2, 0.5] lead to good performance; this is consistent with the recommended
range for Metropolis-Hastings MCMC (Roberts et al., 1997).
MCMC on bounded space.
When the inner (or outer) MCMC chain is run on a
bounded domain, we apply the transformations discussed in Appendix C.8 to map it to
an unbounded domain, and therefore run the MCMC on that space. Notice that the Jaco-
bian factor arising from the transformations has therefore to be taken into account when
computing the acceptance rate.
Appendix F. Details on Neural Networks training
For all experiments, we used the Pytorch library (Paszke et al., 2019) to train NNs. In
Tables 3, 4, 5, 6 and 7 we report the Neural Network architectures used in the diﬀerent
experiments. FC(n, m) denotes a fully connected layer with n inputs and m outputs. For the
time-series and Lorenz96 experiments, φw and ρw represent the two Neural Networks used
to build the PEN network fw as described in Eq. (18), and similarly φβ and ρβ represent
the ones used in building sβ. Finally, BN(p) represents a BatchNorm layer with momentum
50

Score Matched Neural Exponential Families for LFI
p (as described in Appendix E.2), with γ, b, ﬁxed respectively to be vectors of 1s and 0s.
We remark that the momentum value does not impact on the training of the network, but it
modiﬁes the evaluation of the test loss, which we use for early stopping, as discussed below.
Network
fw
ηw
sβ
Structure
FC(10,30)
FC(30,50)
FC(50,50)
FC(50,20)
FC(20,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(10,30)
FC(30,50)
FC(50,50)
FC(50,20)
FC(20,2)
Table 3: Architectures used for the exponential family models (Gaussian, Gamma
and Beta).
Network
fw
ηw
sβ
φw
ρw
φβ
ρβ
Structure
FC(3,50)
FC(50,50)
FC(50,50)
FC(30,20)
FC(22,50)
FC(50,50)
FC(50,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(3,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(22,50)
FC(50,50)
FC(50,2)
Table 4: Architectures used for the AR(2) model.
Network
fw
ηw
sβ
φw
ρw
φβ
ρβ
Structure
FC(11,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(30,50)
FC(50,50)
FC(50,3)
FC(2,15)
FC(15,30)
FC(30,30)
FC(30,15)
FC(15,2)
BN(0.9)
FC(11,50)
FC(50,50)
FC(50,30)
FC(30,20)
FC(30,50)
FC(50,50)
FC(50,2)
Table 5: Architectures used for the MA(2) model.
In all experiments, stochastic gradient descent with a batch size of 1000 samples is used
with Adam optimizer (Kingma and Ba, 2015), whose parameters are left to the default
values as implemented in Pytorch. Finally, we evaluate the test loss on a test set with same
size as the training set at intervals of Tcheck epochs and early-stop training if the test loss
increased with respect to the last evaluation. In order to have a better running estimate
51

Pacchiardi and Dutta
Network
fw
ηw
sβ
φw
ρw
φβ
ρβ
Structure
FC(16,50)
FC(50,100)
FC(100,50)
FC(50,20)
FC(28,40)
FC(40,90)
FC(90,35)
FC(35,5)
FC(4,30)
FC(30,50)
FC(50,50)
FC(50,30)
FC(30,4)
BN(0.9)
FC(16,50)
FC(50,100)
FC(100,50)
FC(50,20)
FC(28,40)
FC(40,90)
FC(90,35)
FC(35,4)
Table 6: Architectures used for the Lorenz96 model in the small setup.
Network
fw
ηw
sβ
φw
ρw
φβ
ρβ
Structure
FC(80,120)
FC(120,160)
FC(160,120)
FC(120,20)
FC(60,80)
FC(80,100)
FC(100,80)
FC(80,5)
FC(4,30)
FC(30,50)
FC(50,50)
FC(50,30)
FC(30,4)
BN(0.9)
FC(80,120)
FC(120,160)
FC(160,120)
FC(120,20)
FC(60,80)
FC(80,100)
FC(100,80)
FC(80,4)
Table 7: Architectures used for the Lorenz96 model in the large setup.
of the quantities of interest for the BatchNorm layer, before each test epoch we perform a
forward pass of the whole training data set, without computing gradients, as discussed in
Appendix E.2. We do not perform early stopping before epoch Tstart. The nets are trained
for a maximum of T training epochs with Ntrain training samples. In some experiments,
we used an exponential learning rate scheduler, which decreases progressively the learning
rate by multiplying it by a factor ζ < 1 at each epoch. We ﬁxed ζ = 0.99. The values of the
parameters are reported in Table 8 for all the diﬀerent models and setups, together with
the values of the learning rates (lr) used and whether the scheduler was used or not (Sch).
We remark that, in order to make the comparison fair, the learning rates for the FP
experiments were chosen by cross validation with several learning rates choices. Similarly,
we hand-picked the best learning rate values for training the Neural Networks with SM
and SSM, even if in that case it was not possible, due to computational constraints, to
perform a full search on a large set of values of learning rates for all experiments. In fact,
the computational cost of training the exponential family approximation with SM or SSM
is larger than the cost of learning the summary statistics with the FP approach. Moreover,
in this scenario we have two independent learning rates values to tune, as we train two
networks simultaneously. For this reason, we did not spend too much time trying diﬀerent
lr values for models for which we already got satisfactory results.
52

Score Matched Neural Exponential Families for LFI
Model
Setup
Learning rates
Ntrain
T
Tstart
Tcheck
Sch
Gaussian
SM
lr(fw) = 0.0003, lr(ηw) = 0.003
104
500
150
10
Yes
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
200
10
Yes
FP
lr(sβ) = 0.01
104
1000
300
25
No
Gamma
SM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
200
10
Yes
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
200
10
Yes
FP
lr(sβ) = 0.001
104
1000
300
25
Yes
Beta
SM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
200
10
Yes
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
200
10
Yes
FP
lr(sβ) = 0.01
104
1000
250
50
Yes
AR(2)
SM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
100
25
Yes
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
100
25
Yes
FP
lr(sβ) = 0.001
104
1000
500
25
Yes
MA(2)
SM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
100
25
Yes
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
500
100
25
Yes
FP
lr(sβ) = 0.001
104
1000
500
25
Yes
Lorenz96
Small
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
1000
500
50
Yes
FP
lr(sβ) = 0.001
104
1000
200
25
Yes
Lorenz96
Large
SSM
lr(fw) = 0.001, lr(ηw) = 0.001
104
1000
500
50
Yes
FP
lr(sβ) = 0.001
104
1000
200
25
Yes
Table 8: Hyperparameter values for NN training: “FP” denotes the least squares
regression by Fearnhead and Prangle (2012); Jiang et al. (2017) used for the ABC-FP
experiment, while “SM” and “SSM” denote respectively exponential family trained with
Score Matching and Sliced Score Matching.
53

Pacchiardi and Dutta
Appendix G. Additional experimental results
G.1 Exponential family models
G.1.1 Mean Correlation Coefficients for Neural Networks trained with
SSM
We report in Table 9 the weak and strong Mean Correlation Coeﬃcient (MCC, Appendix B.1.1)
for Neural Networks trained with SSM, for the exponential family models. MCC is a metric
in [0, 1], with 1 denoting perfect recovery up to a linear transformation (weak) or permu-
tation (strong). As it can be seen in Table 9, our method leads to values quite close to 1,
particularly for the weak MCC, implying that our method is able to recover the embeddings
up to a linear transformation, as expected.
Model
MCC weak in
MCC weak out
MCC strong in
MCC strong out
Beta (statistics)
0.990
0.986
0.982
0.979
Beta (nat. par.)
0.987
0.989
0.983
0.985
Gamma (statistics)
0.939
0.928
0.723
0.709
Gamma (nat. par.)
0.977
0.977
0.792
0.794
Gaussian (statistics)
0.874
0.844
0.623
0.638
Gaussian (nat. par.)
0.861
0.862
0.581
0.543
Table 9: MCC for exponential family models between exact embeddings and
those learned with SSM. We show weak and strong MCC values; MCC is between 0 and
1 and measures how well an embedding is recovered up to permutation and rescaling of its
components (strong) or linear transformation (weak); the larger, the better. “in” denotes
MCC on training data used to ﬁnd the best transformation, while “out” denote MCC on
test data. We used 500 samples in both training and test data sets.!
G.1.2 Learned and exact embeddings for the exponential family models
We compare here the exact and learned suﬃcient statistics and natural parameters of the
exponential family models; precisely, we draw samples (x(j), θ(j)) and then plot the learned
statistics fw(x(j)) versus the exact one, and similarly for the natural parameters. Figure 9
reports the results for Neural Networks trained with SM, while Figure 10 reports the results
for Neural Networks trained with SSM.
54

Score Matched Neural Exponential Families for LFI
980
960
940
920
900
First learned statistics
10
0
10
First true statistics
600
580
560
540
520
500
480
Second learned statistics
0
100
200
300
400
Second true statistics
(a) Statistics Gaussian
35.0
32.5
30.0
27.5
25.0
22.5
First learned statistics
1
0
1
2
First true statistics
110
105
100
Second learned statistics
2.5
5.0
7.5
10.0
Second true statistics
(b) Statistics Gamma
162.5
160.0
157.5
155.0
152.5
150.0
First learned statistics
2
1
First true statistics
160
155
150
145
140
Second learned statistics
2
1
Second true statistics
(c) Statistics Beta
0
1
2
3
First learned parameters
5
0
5
First true parameters
4
2
0
2
4
Second learned parameters
0.5
0.4
0.3
0.2
0.1
Second true parameters
(d) Parameters Gaussian
2
1
0
1
2
First learned parameters
0.0
0.5
1.0
1.5
2.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
0.8
0.6
0.4
Second true parameters
(e) Parameters Gamma
2
1
0
1
2
First learned parameters
1.0
1.5
2.0
2.5
3.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
1.5
2.0
2.5
3.0
Second true parameters
(f) Parameters Beta
Figure 9: Learned and exact embeddings for exponential family models obtained
with SM. Each point represents a diﬀerent x (for the statistics) or θ (for the natural
parameters); 1000 of each were used here.
55

Pacchiardi and Dutta
1540
1520
1500
1480
1460
1440
First learned statistics
10
0
10
First true statistics
240
260
280
300
320
Second learned statistics
0
100
200
300
400
Second true statistics
(a) Statistics Gaussian
254
252
250
248
246
First learned statistics
1
0
1
2
First true statistics
68
66
64
62
60
58
56
54
Second learned statistics
2
4
6
8
10
Second true statistics
(b) Statistics Gamma
298
296
294
292
290
288
286
284
First learned statistics
2.5
2.0
1.5
1.0
0.5
First true statistics
42
44
46
48
50
52
54
Second learned statistics
2.5
2.0
1.5
1.0
0.5
Second true statistics
(c) Statistics Beta
1.0
0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
First learned parameters
7.5
5.0
2.5
0.0
2.5
5.0
7.5
First true parameters
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.5
1.0
Second learned parameters
0.5
0.4
0.3
0.2
0.1
Second true parameters
(d) Parameters Gaussian
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
First learned parameters
0.0
0.5
1.0
1.5
2.0
First true parameters
2
1
0
1
2
Second learned parameters
1.0
0.8
0.6
0.4
Second true parameters
(e) Parameters Gamma
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
First learned parameters
1.0
1.5
2.0
2.5
3.0
First true parameters
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
Second learned parameters
1.0
1.5
2.0
2.5
3.0
Second true parameters
(f) Parameters Beta
Figure 10: Learned and exact embeddings for exponential family models obtained
with SSM. Each point represents a diﬀerent x (for the statistics) or θ (for the natural
parameters); 1000 of each were used here.
56

Score Matched Neural Exponential Families for LFI
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Beta
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
RMSE post mean
10
30
100
200
Inner MCMC steps
0
2
4
6
8
10
Wasserstein distance
Gaussian
0
2
4
6
8
10
RMSE post mean
(a) Exc-SM
10
30
100
200
Inner MCMC steps
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
Beta
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Wasserstein distance
Gamma
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
RMSE post mean
10
30
100
200
Inner MCMC steps
0
1
2
3
4
5
6
7
Wasserstein distance
Gaussian
0
1
2
3
4
5
RMSE post mean
(b) Exc-SSM
Figure 11: Performance of Exc-SM with diﬀerent number of inner MCMC steps,
for exponential family models.
Wasserstein distance from the exact posterior and
RMSE between exact and approximate posterior means are reported for 100 observations
using boxplots. Boxes span from 1st to 3rd quartile, whiskers span 95% probability density
region and horizontal line denotes median. The numerical values are not comparable across
examples, as they depend on the range of parameters.
G.1.3 Performance of Exc-SM and Exc-SSM
We study here the performance of Exc-SM and Exc-SSM with diﬀerent numbers of inner
steps in the ExchangeMCMC algorithm (Algorithm 4) for the Exponential family models.
Speciﬁcally, we run the inference with 10, 30, 100 and 200 inner MCMC steps, and we
evaluate the performance in these 4 cases (Figure 11); considering the diﬀerent models, we
observe that the performance with 30 steps is almost equivalent to the one with 100 and
200, albeit being faster (see the computational time in Table 10). In the main text, we
therefore present results using 30 inner MCMC steps.
Inner MCMC steps
10
30
100
200
Time (minutes)
≈2
≈4
≈16
≈28
Table 10: Approximate computational time of Exc-SM with diﬀerent number of
inner MCMC steps for the exponential family models. These values were obtained
by running on a single core.
57

Pacchiardi and Dutta
Inner MCMC steps
10
30
100
200
Time (minutes)
≈2
≈5
≈17
≈29
Table 11: Approximate computational time of Exc-SM with diﬀerent number of
inner MCMC steps, for the AR(2) and MA(2) models. These values were obtained
by running on a single core.
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
AR(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
MA(2)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
RMSE post mean
(a) Exc-SM
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Wasserstein distance
AR(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
RMSE post mean
10
30
100
200
Inner MCMC steps
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
MA(2)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
RMSE post mean
(b) Exc-SSM
Figure 12: Performance of Exc-SM with diﬀerent number of inner MCMC steps,
for AR(2) and MA(2) models.
Wasserstein distance from the exact posterior and
RMSE between exact and approximate posterior means are reported for 100 observations
using boxplots. Boxes span from 1st to 3rd quartile, whiskers span 95% probability density
region and horizontal line denotes median.
G.2 AR(2) and MA(2) models
We study here the performance of Exc-SM and Exc-SSM with diﬀerent numbers of inner
steps in the ExchangeMCMC algorithm (Algorithm 4) for the AR(2) and MA(2) models.
Speciﬁcally, we run the inference with 10, 30, 100 and 200 inner MCMC steps, and we
evaluate the performance in these 4 cases (Figure 12); considering the diﬀerent models, we
observe that the performance with 30 steps is almost equivalent to the one with 100 and
200, albeit being faster (see the computational time in Table 11). In the main text, we
therefore present results using 30 inner MCMC steps.
G.3 Simulation cost to reach equivalent performance as Exc-SM
Above, we have showed that Exc-SM and Exc-SSM are competitive with the other ap-
proaches, even if they require no additional model simulations.
Here, we quantify how
many model simulations are needed for the other techniques to reach the same performance
of Exc-SM in the exponential family and time-series models. The same analysis could be
done with respect to Exc-SSM but, as its performance is generally close to the one achieved
by Exc-SM, we avoid repeating it.
We compute therefore the performance of ABC-SM, ABC-SSM, ABC-FP, PMC-SL,
PMC-RE at each iteration for all 100 observations and ﬁnd when their median performance
58

Score Matched Neural Exponential Families for LFI
(as quantiﬁes by the Wasserstein distance with respect to the true posterior) becomes com-
parable or better than the median one achieved by Exc-SM. The number of required sim-
ulations are reported in Table 12. Notice that some techniques are not able to outperform
Exc-SM for some models; we highlight that by a dash in the Table. SL and RE reach
similar performance to Exc-SM with one single iteration, when they are able to do so; in
fact, we found empirically that the performance of them does not signiﬁcantly improve with
iterations. Still, we remark that one single iteration of SL and RE requires a very large
number of model simulations.
In Table 12, we also give the number of simulations required for the preliminary NN
training in the methods which use one; further, we compute the overall cost of inference in
terms of model simulations (taking into account both NN training and inference) for diﬀerent
number of observations; we remark that, as discussed previously, our method requires no
additional model simulations to perform inference after the NNs have been trained. From
Table 12, it can be seen that, for all models, ABC-SM, ABC-FP, PMC-RE and PMC-SL
require a number of simulations larger than the one needed to train the NNs in Exc-SM to
reach the performance achieved by Exc-SM, which makes the latter an interesting option
for models in which simulations are very expensive.
In Figures 13 and 14 we represent the performance attained by ABC-FP, ABC-SM,
ABC-SSM, PMC-SL and PMC-RE at each iteration of the iterative algorithm. On the
horizontal axis of all plots, we report the number of model simulations corresponding to the
iteration of the algorithm.
G.4 Validation with Scoring Rules for Lorenz96 model
Recall that the Lorenz96 model (Section 5.3) is a multivariate time-series model. Therefore,
we use the Scoring Rules (on which more details are given in Appendix D) to evaluate the
predictive performance at each timestep separately.
First, let us recall the deﬁnition of the posterior predictive density:
p(x|x0) =
Z
p(x|θ)π(θ|x0),
whose corresponding posterior predictive distribution we denote as P(·|x0). In the above
expression, π(θ|x0) may represent the posterior obtained with any of the considered methods
(Exc-SM, Exc-SSM, ABC-FP, ABC-SM, ABC-SSM).
Let P (t)(·|x0) denote the posterior predictive distribution at time t conditioned on an
observation x0, and let x0,(t) denote the t-th timestep of the observation. Then, we are
interested in:
SE(P (t)(·|x0), x0,(t))
and
Sk(P (t)(·|x0), x0,(t));
we estimate these using samples from P (t)(·|x0) with the unbiased estimators discussed
in Appendix D. In the Kernel Score, the bandwidth of the Gaussian kernel is set from
simulations as described in Appendix G.4.1.
In this way, we obtain a score for an observation x0 at each timestep of the model. This
procedure is repeated for 100 diﬀerent observations; the scores at each timestep are reported
in Figure 15, while the summed scores over timesteps were reported in Figure 7 in the main
text. In both Figures, we report the median value and various quantiles over the considered
59

Pacchiardi and Dutta
Beta
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 · 104
2 · 104
2 · 104
2 · 104
-
-
Inference
0
2.5 · 104
1.8 · 104
-
1 · 105
1 · 106
Total 1 obs
2 · 104
4.5 · 104
3.8 · 104
-
1 · 105
1 · 106
Total 3 obs
2 · 104
9.5 · 104
7.4 · 104
-
3 · 105
3 · 106
Total 100 obs
2 · 104
2.52 · 106
1.82 · 106
-
1 · 107
1 · 108
Gamma
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 · 104
2 · 104
2 · 104
2 · 104
-
-
Inference
0
4.7 · 104
2.9 · 104
-
1 · 105
1 · 106
Total 1 obs
2 · 104
6.7 · 104
4.9 · 104
-
1 · 105
1 · 106
Total 3 obs
2 · 104
1.61 · 105
1.07 · 105
-
3 · 105
3 · 106
Total 100 obs
2 · 104
4.72 · 106
2.92 · 106
-
1 · 107
1 · 108
Gaussian
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 · 104
2 · 104
2 · 104
2 · 104
-
-
Inference
0
2.7 · 104
2.6 · 104
-
-
1 · 106
Total 1 obs
2 · 104
4.7 · 104
4.6 · 104
-
-
1 · 106
Total 3 obs
2 · 104
1.01 · 105
9.8 · 104
-
-
3 · 106
Total 100 obs
2 · 104
2.72 · 106
2.62 · 106
-
-
1 · 108
AR2
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 · 104
2 · 104
2 · 104
2 · 104
-
-
Inference
0
3.2 · 104
3.1 · 104
2.3 · 104
1 · 105
-
Total 1 obs
2 · 104
5.2 · 104
5.1 · 104
4.3 · 104
1 · 105
-
Total 3 obs
2 · 104
1.16 · 105
1.13 · 105
8.9 · 104
3 · 105
-
Total 100 obs
2 · 104
3.22 · 106
3.12 · 106
2.32 · 106
1 · 107
-
MA2
Exc-SM
ABC-SM
ABC-SSM
ABC-FP
PMC-SL
PMC-RE
NN training
2 · 104
2 · 104
2 · 104
2 · 104
-
-
Inference
0
3.1 · 104
2.1 · 104
2.0 · 104
1 · 105
-
Total 1 obs
2 · 104
5.1 · 104
4.1 · 104
4.0 · 104
1 · 105
-
Total 3 obs
2 · 104
1.13 · 105
8.3 · 104
8.0 · 104
3 · 105
-
Total 100 obs
2 · 104
3.12 · 106
2.12 · 106
2.02 · 106
1 · 107
-
Table 12: Model simulations needed for the diﬀerent techniques, for both NN train-
ing and inference; for ABC-FP, ABC-SM, ABC-SSM, PMC-SL and PMC-RE, we report
simulations needed to obtain performance at least as good as Exc-SM; in case the approach
does not reach the same performance as Exc-SM, we denote that by a dash. Notice that
PMC-SL and PMC-RE do not require NN training before performing inference. We also
show the total number of simulations needed to apply the diﬀerent approaches on 1, 3 and
100 observations, by taking into account NN training and inference steps.
60

Score Matched Neural Exponential Families for LFI
20
40
60
80
100
Number of simulations (×1000)
0.0
0.2
0.4
0.6
0.8
1.0
Wasserstein distance
Beta
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (×1000)
0.0
0.2
0.4
0.6
0.8
1.0
Wasserstein distance
Gamma
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (×1000)
0
2
4
6
8
10
Wasserstein distance
Gaussian
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
200
400
600
800
1000
Number of simulations (×1000)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Wasserstein distance
Beta
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (×1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
Gamma
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (×1000)
0
1
2
3
4
5
Wasserstein distance
Gaussian
PMC-SL
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (×1000)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Wasserstein distance
Beta
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (×1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
Gamma
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (×1000)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Wasserstein distance
Gaussian
PMC-RE
Exc-SM
Figure 13: Wasserstein distance between approximate and exact posterior at dif-
ferent iterations of the sequential algorithms for the exponential family models,
for 100 diﬀerent observations. The solid line denotes median, while colored regions denote
95% probability density region; an horizontal line denoting the value obtained with Exc-SM
is also represented, 95% probability density region denoted by dotted horizontal lines. The
horizontal axis reports the number of model simulations corresponding to the iteration of
the diﬀerent algorithms.
61

Pacchiardi and Dutta
20
40
60
80
100
Number of simulations (×1000)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
AR(2)
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
20
40
60
80
100
Number of simulations (×1000)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Wasserstein distance
MA(2)
ABC-FP
ABC-SM
ABC-SSM
Exc-SM
200
400
600
800
1000
Number of simulations (×1000)
0.00
0.05
0.10
0.15
0.20
0.25
Wasserstein distance
AR(2)
PMC-SL
Exc-SM
200
400
600
800
1000
Number of simulations (×1000)
0.0
0.1
0.2
0.3
0.4
0.5
Wasserstein distance
MA(2)
PMC-SL
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (×1000)
0.0
0.2
0.4
0.6
0.8
Wasserstein distance
AR(2)
PMC-RE
Exc-SM
2000
4000
6000
8000
10000
Number of simulations (×1000)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Wasserstein distance
MA(2)
PMC-RE
Exc-SM
Figure 14: Wasserstein distance between approximate and exact posterior at dif-
ferent iterations of the sequential algorithms for the AR(2) and MA(2) models,
for 100 diﬀerent observations. The solid line denotes median, while colored regions denote
95% probability density region; an horizontal line denoting the value obtained with Exc-SM
is also represented, 95% probability density region denoted by dotted horizontal lines. The
horizontal axis reports the number of model simulations corresponding to the iteration of
the diﬀerent algorithms.
62

Score Matched Neural Exponential Families for LFI
100 observations. It can be seen that ABC-FP is slightly outperformed by our proposed
methods for both the large and small Lorenz96 conﬁguration. Additionally, notice how, in
the small conﬁguration, the Energy score has an increasing trend over t, while the Kernel
one instead decreases.
0
0.5
1
1.5
t
100
200
300
400
500
Energy Score
Small Lorenz96
ABC-FP
ABC-SSM
Exc-SSM
0
1
2
3
4
t
1000
1250
1500
1750
2000
2250
2500
2750
Energy Score
Large Lorenz96
ABC-FP
ABC-SSM
0
0.5
1
1.5
t
0
1
2
3
4
5
6
7
8
Kernel Score
Small Lorenz96
ABC-FP
ABC-SSM
Exc-SSM
0
1
2
3
4
t
0
5
10
15
20
25
30
35
40
Kernel Score
Large Lorenz96
ABC-FP
ABC-SSM
Figure 15: Posterior predictive performance of the diﬀerent methods at each
timestep according to the Kernel and Energy Scores; the smaller, the better. Sam-
ples from the posterior predictive were obtained for 100 observations, and both Scoring
Rules estimated. The solid lines denote medians over the 100 observations, while colored
regions denote 95% probability density region.
G.4.1 Setting γ in the kernel Scoring Rule
In the kernel score Sk, the Gaussian kernel in Eq. (17) is used across this work. There,
the kernel bandwidth γ is a free parameter. In order to ensure comparability between the
Scoring Rule values for diﬀerent observations and inference methods, the value of γ needs
to be ﬁxed independently on both.
Inspired by Park et al. (2016), we exploit an empirical procedure to set γ for the speciﬁc
case of multivariate time-series models (of which our Lorenz96 model is an instance, see
Section 5.3). Speciﬁcally, we use the following procedure:
1. Draw a set of parameter values θj ∼π(θ) and simulations xj ∼p(·|θj), for j =
1, . . . , m.
2. Estimate the median of {||x(t)
j
−x(t)
k ||2}m
jk and call it ˆγ(t), for all values of t.
3. Set the estimate for γ as the median of ˆγ(t) over all considered timesteps t.
63

Pacchiardi and Dutta
Empirically, we use m = 1000. Note that the above strategy uses medians rather than
means as those are more robust to outliers in the estimates. With this method, we obtain
γ ≈1.54 for the small version of the Lorenz96 model and γ = 6.38 for the large one.
References
Simon Aeschbacher, Mark A Beaumont, and Andreas Futschik.
A novel approach for
choosing summary statistics in approximate Bayesian computation. Genetics, 192(3):
1027–1047, 2012.
Mattias ˚Akesson, Prashant Singh, Fredrik Wrede, and Andreas Hellander. Convolutional
neural networks as summary statistics for approximate Bayesian computation.
arXiv
preprint arXiv:2001.11760, 2020.
Carlo Albert, R. K¨unsch Hans, and Andreas Scheidegger. A simulated annealing approach
to approximate Bayesian computations. Statistics and Computing, 25:1217–1232, 2015.
Pierre Alquier, Nial Friel, Richard Everitt, and Aidan Boland. Noisy Monte Carlo: Conver-
gence of Markov chains with approximate transition kernels. Statistics and Computing,
26(1-2):29–47, 2016.
Ziwen An, Leah F South, David J Nott, and Christopher C Drovandi. Accelerating Bayesian
synthetic likelihood with the graphical lasso. Journal of Computational and Graphical
Statistics, 28(2):471–475, 2019.
Ziwen An, David J Nott, and Christopher Drovandi. Robust Bayesian synthetic likelihood
via a semi-parametric approach. Statistics and Computing, 30(3):543–557, 2020.
Michael Arbel and Arthur Gretton. Kernel conditional exponential family. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 1337–1346. PMLR, 2018.
V.I. Avrutskiy.
Backpropagation generalized for output derivatives.
arXiv preprint
arXiv:1712.04185, 2017.
Mark A Beaumont. Approximate Bayesian computation in evolution and ecology. Annual
review of ecology, evolution, and systematics, 41:379–406, 2010.
Michael GB Blum, Maria Antonieta Nunes, Dennis Prangle, and Scott A Sisson. A com-
parative review of dimension reduction methods in approximate Bayesian computation.
Statistical Science, 28(2):189–208, 2013.
Alberto Caimo and Nial Friel. Bayesian inference for exponential random graph models.
Social Networks, 33(1):41–55, 2011.
Olivier Capp´e, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert. Population
Monte Carlo. Journal of Computational and Graphical Statistics, 13(4):907–929, 2004.
Bob Carpenter, Andrew Gelman, Matthew D Hoﬀman, Daniel Lee, Ben Goodrich, Michael
Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A prob-
abilistic programming language. Journal of statistical software, 76(1), 2017.
64

Score Matched Neural Exponential Families for LFI
Yanzhi Chen, Dinghuai Zhang, Michael U Gutmann, Aaron Courville, and Zhanxing Zhu.
Neural approximate suﬃcient statistics for implicit models. In Ninth International Con-
ference on Learning Representations 2021, 2021.
Badr-Eddine Ch´erief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian esti-
mation via maximum mean discrepancy.
In Symposium on Advances in Approximate
Bayesian Inference, pages 1–21. PMLR, 2020.
Bo Dai, Hanjun Dai, Arthur Gretton, Le Song, Dale Schuurmans, and Niao He. Kernel
exponential family estimation via doubly dual embedding. In The 22nd International
Conference on Artiﬁcial Intelligence and Statistics, pages 2321–2330. PMLR, 2019a.
Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans.
Exponential family estimation via adversarial dynamics embedding. In Advances in Neu-
ral Information Processing Systems, pages 10979–10990, 2019b.
Alexander Philip Dawid and Monica Musio. Theory and applications of proper scoring
rules. Metron, 72(2):169–183, 2014.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. An adaptive sequential Monte Carlo
method for approximate Bayesian computation. Statistics and Computing, 22(5):1009–
1020, 2012.
Ritabrata Dutta, Bastien Chopard, Jonas L¨att, Frank Dubois, Karim Zouaoui Boudjel-
tia, and Antonietta Mira. Parameter estimation of platelets deposition: Approximate
Bayesian computation with high performance computing.
Frontiers in physiology, 9,
2018.
Ritabrata Dutta, Susana N Gomes, Dante Kalise, and Lorenzo Pacchiardi. Using mobility
data in the design of optimal lockdown strategies for the covid-19 pandemic.
PLoS
Computational Biology, 17(8):e1009236, 2021a.
Ritabrata Dutta, Marcel Schoengens, Lorenzo Pacchiardi, Avinash Ummadisingu, Nicole
Widmer, Pierre K¨unzli, Jukka-Pekka Onnela, and Antonietta Mira. ABCpy: A high-
performance computing perspective to approximate bayesian computation. Journal of
Statistical Software, 100(7):1–38, 2021b. doi: 10.18637/jss.v100.i07. URL https://www.
jstatsoft.org/index.php/jss/article/view/v100i07.
Richard G Everitt. Bayesian parameter estimation for latent Markov random ﬁelds and
social networks. Journal of Computational and graphical Statistics, 21(4):940–960, 2012.
Richard G Everitt, Dennis Prangle, Philip Maybank, and Mark Bell. Marginal sequential
Monte Carlo for doubly intractable models. arXiv preprint arXiv:1710.04382, 2017.
Matteo Fasiolo, Simon N Wood, Florian Hartig, and Mark V Bravington. An extended
empirical saddlepoint approximation for intractable likelihoods. Electronic Journal of
Statistics, 12(1):1544–1578, 2018.
65

Pacchiardi and Dutta
Paul Fearnhead and Dennis Prangle.
Constructing summary statistics for approximate
Bayesian computation: semi-automatic approximate Bayesian computation [with Discus-
sion]. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 74(3):
419–474, 2012. ISSN 1369-7412.
Charles J Geyer. Markov chain Monte Carlo maximum likelihood. In Computing science
and statistics: Proceedings of 23rd Symposium on the Interface Interface Foundation,
Fairfax Station, 1991, pages 156–163, 1991.
Alexander Gleim and Christian Pigorsch.
Approximate Bayesian computation with
indirect
summary
statistics.
Draft
paper:
http://ect-pigorsch.
mee.
uni-bonn.
de/data/research/papers, 2013.
Tilmann Gneiting and Adrian E Raftery.
Strictly proper scoring rules, prediction, and
estimation. Journal of the American statistical Association, 102(477):359–378, 2007.
David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transfor-
mation for likelihood-free inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 2404–2414. PMLR, 09–15 Jun 2019.
URL http://proceedings.mlr.press/v97/greenberg19a.html.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander
Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–
773, 2012.
Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized
statistical models, with applications to natural image statistics.
Journal of Machine
Learning Research, 13(Feb):307–361, 2012.
Michael U Gutmann, Jukka Corander, et al.
Bayesian optimization for likelihood-free
inference of simulator-based statistical models. Journal of Machine Learning Research,
2016.
J Hakkarainen, A Ilin, A Solonen, M Laine, H Haario, J Tamminen, E Oja, and H J¨arvinen.
On closure parameter estimation in chaotic systems. Nonlinear processes in Geophysics,
19(1):127–143, 2012.
Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free MCMC with amortized
approximate ratio estimators. In International Conference on Machine Learning, pages
4239–4248. PMLR, 2020.
Geoﬀrey E Hinton.
Training products of experts by minimizing contrastive divergence.
Neural computation, 14(8):1771–1800, 2002.
Harold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377,
1936.
Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. Jour-
nal of Machine Learning Research, 6(Apr):695–709, 2005.
66

Score Matched Neural Exponential Families for LFI
Aapo Hyv¨arinen.
Some extensions of score matching.
Computational statistics & data
analysis, 51(5):2499–2512, 2007.
Sergey Ioﬀe and Christian Szegedy. Batch Normalization: Accelerating deep network train-
ing by reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceed-
ings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL
http://proceedings.mlr.press/v37/ioffe15.html.
Marko J¨arvenp¨a¨a, Michael U Gutmann, Arijus Pleska, Aki Vehtari, and Pekka Marttinen.
Eﬃcient acquisition rules for model-based approximate Bayesian computation. Bayesian
Analysis, 14(2):595–622, 2019.
Marko Jarvenpaa, Aki Vehtari, and Pekka Marttinen. Batch simulations and uncertainty
quantiﬁcation in gaussian process surrogate approximate Bayesian computation. In Con-
ference on Uncertainty in Artiﬁcial Intelligence, pages 779–788. PMLR, 2020.
Marko J¨arvenp¨a¨a, Michael U Gutmann, Aki Vehtari, and Pekka Marttinen. Parallel Gaus-
sian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian
Analysis, 16(1):147–178, 2021.
Bai Jiang, Tung-yu Wu, Charles Zheng, and Wing H Wong. Learning summary statistic
for approximate Bayesian computation via deep neural network. Statistica Sinica, pages
1595–1618, 2017.
Paul Joyce and Paul Marjoram. Approximately suﬃcient statistics and Bayesian computa-
tion. Statistical applications in genetics and molecular biology, 7(1), 2008.
Ilyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, and Aapo Hyv¨arinen. ICE-
BeeM: Identiﬁable conditional energy-based deep models. CoRR, abs/2002.11537, 2020.
URL https://arxiv.org/abs/2002.11537.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
2015. URL http://arxiv.org/abs/1412.6980.
Nadja Klein, David J Nott, and Michael Stanley Smith. Marginally calibrated deep dis-
tributional regression. Journal of Computational and Graphical Statistics, pages 1–17,
2020.
Bernard Osgood Koopman. On distributions admitting a suﬃcient statistic. Transactions
of the American Mathematical society, 39(3):399–409, 1936.
Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covari-
ance matrices. Journal of multivariate analysis, 88(2):365–411, 2004.
Faming Liang. A double Metropolis–Hastings sampler for spatial models with intractable
normalizing constants. Journal of Statistical Computation and Simulation, 80(9):1007–
1022, 2010.
67

Pacchiardi and Dutta
Faming Liang, Ick Hoon Jin, Qifan Song, and Jun S Liu. An adaptive exchange algorithm
for sampling from distributions with intractable normalizing constants. Journal of the
American Statistical Association, 111(513):377–393, 2016.
Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Coran-
der. Fundamentals and recent developments in approximate Bayesian computation. Sys-
tematic Biology, 66(1):e66–e82, 2017. ISSN 1076836X. doi: 10.1093/sysbio/syw077. URL
https://doi.org/10.1093/sysbio/syw077.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian
inference algorithm. In Advances in neural information processing systems, pages 2378–
2386, 2016.
Song Liu and Takafumi Kanamori. Estimating density models with complex truncation
boundaries. arXiv preprint arXiv:1910.03834, 2019.
Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on predictabil-
ity, volume 1, 1996.
Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan ¨Ocal, Marcel Non-
nenmacher, and Jakob H Macke. Flexible statistical inference for mechanistic models of
neural dynamics. In Advances in Neural Information Processing Systems, pages 1289–
1299, 2017.
Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke.
Likelihood-free inference with emulator networks. In Symposium on Advances in Approx-
imate Bayesian Inference, pages 32–53. PMLR, 2019.
Kanti V Mardia, John T Kent, and Arnab K Laha. Score matching estimators for directional
distributions. arXiv preprint arXiv:1604.08470, 2016.
Jean-Michel Marin, Pierre Pudlo, Christian P Robert, and Robin J Ryder. Approximate
Bayesian computational methods. Statistics and Computing, 22(6):1167–1180, 2012.
Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavar´e. Markov chain Monte
Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26):
15324–15328, 2003.
Pekka Marttinen, Nicholas J Croucher, Michael U Gutmann, Jukka Corander, and
William P Hanage. Recombination produces coherent bacterial species clusters in both
core and accessory genomes. Microbial Genomics, 1(5), 2015.
Takuo Matsubara,
Jeremias Knoblauch,
Fran¸cois-Xavier Briol,
Chris Oates,
et al.
Robust generalised Bayesian inference for intractable likelihoods.
arXiv preprint
arXiv:2104.07359, 2021.
Trevelyan J McKinley, Ian Vernon, Ioannis Andrianakis, Nicky McCreesh, Jeremy E Oakley,
Rebecca N Nsubuga, Michael Goldstein, Richard G White, et al. Approximate Bayesian
computation and simulation-based inference for complex stochastic epidemic models. Sta-
tistical science, 33(1):4–18, 2018.
68

Score Matched Neural Exponential Families for LFI
Edward Meeds and Max Welling.
GPS-ABC: Gaussian process surrogate approximate
Bayesian computation.
In Proceedings of the Thirtieth Conference on Uncertainty in
Artiﬁcial Intelligence, pages 593–602, 2014.
Amanda Minter and Renata Retkute. Approximate Bayesian computation for infectious
disease modelling. Epidemics, 29:100368, 2019.
Matthew T Moores, Christopher C Drovandi, Kerrie Mengersen, and Christian P Robert.
Pre-processing for approximate Bayesian computation in image analysis. Statistics and
Computing, 25(1):23–33, 2015.
Iain Murray, Zoubin Ghahramani, and David MacKay. MCMC for doubly-intractable dis-
tributions. arXiv preprint arXiv:1206.6848, 2012.
Hien Duy Nguyen, Julyan Arbel, Hongliang L¨u, and Florence Forbes. Approximate Bayesian
computation via the energy statistic. IEEE Access, 8:131683–131698, 2020.
Matthew A Nunes and David J Balding. On optimal selection of summary statistics for
approximate Bayesian computation. Statistical applications in genetics and molecular
biology, 9(1), 2010.
Lorenzo Pacchiardi, Pierre K¨unzli, Marcel Sch¨ongens, Bastien Chopard, and Ritabrata
Dutta.
Distance-learning for approximate Bayesian computation to model a volcanic
eruption. Sankhya B, Jan 2020. ISSN 0976-8394. doi: 10.1007/s13571-019-00208-8. URL
https://doi.org/10.1007/s13571-019-00208-8.
George Papamakarios and Iain Murray. Fast ε-free inference of simulation models with
Bayesian conditional density estimation. In Advances in Neural Information Processing
Systems, pages 1028–1036, 2016.
George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast
likelihood-free inference with autoregressive ﬂows. In Kamalika Chaudhuri and Masashi
Sugiyama, editors, Proceedings of Machine Learning Research, volume 89 of Proceedings
of Machine Learning Research, pages 837–848. PMLR, 16–18 Apr 2019.
URL http:
//proceedings.mlr.press/v89/papamakarios19a.html.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Bal-
aji Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. Jour-
nal of Machine Learning Research, 22(57):1–64, 2021. URL http://jmlr.org/papers/
v22/19-1028.html.
Jaewoo Park and Murali Haran.
Bayesian inference in the presence of intractable nor-
malizing functions. Journal of the American Statistical Association, 113(523):1372–1390,
2018.
Mijung Park, Wittawat Jitkrittum, and Dino Sejdinovic. K2-ABC: Approximate Bayesian
computation with kernel embeddings. In Artiﬁcial Intelligence and Statistics, 2016.
69

Pacchiardi and Dutta
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-
dreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An
imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alch´e Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.
Leah F Price, Christopher C Drovandi, Anthony Lee, and David J Nott. Bayesian synthetic
likelihood. Journal of Computational and Graphical Statistics, 27(1):1–11, 2018.
Stefan T Radev, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich K¨othe.
BayesFlow: Learning complex stochastic models with invertible neural networks. IEEE
Transactions on Neural Networks and Learning Systems, 2020.
Maria L Rizzo and G´abor J Sz´ekely.
Energy distance.
Wiley interdisciplinary reviews:
Computational statistics, 8(1):27–38, 2016.
Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal
scaling of random walk Metropolis algorithms. The annals of applied probability, 7(1):
110–120, 1997.
Erlis Ruli, Nicola Sartori, and Laura Ventura. Approximate Bayesian computation with
composite score functions. Statistics and Computing, 26(3):679–692, 2016.
Jascha Sohl-Dickstein, Peter B Battaglino, and Michael R DeWeese.
New method for
parameter estimation in probabilistic models: minimum probability ﬂow. Physical review
letters, 107(22):220601, 2011.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable
approach to density and score estimation. In Ryan P. Adams and Vibhav Gogate, editors,
Proceedings of The 35th Uncertainty in Artiﬁcial Intelligence Conference, volume 115 of
Proceedings of Machine Learning Research, pages 574–584, Tel Aviv, Israel, 22–25 Jul
2020. PMLR. URL http://proceedings.mlr.press/v115/song20a.html.
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo Hyv¨arinen, and Revant
Kumar. Density estimation in inﬁnite dimensional exponential families. The Journal of
Machine Learning Research, 18(1):1830–1888, 2017.
Esteban G Tabak, Giulio Trigila, and Wenjun Zhao. Conditional density estimation and
simulation through optimal transport. Machine Learning, pages 1–24, 2020.
Simon Tavar´e, David J Balding, Robert C Griﬃths, and Peter Donnelly. Inferring coales-
cence times from DNA sequence data. Genetics, 145(2):505–518, 1997.
Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U Gutmann,
et al. Likelihood-free inference by ratio estimation. Bayesian Analysis, 2020.
70

Score Matched Neural Exponential Families for LFI
Jean-Francois Ton, CHAN Lucian, Yee Whye Teh, and Dino Sejdinovic. Noise contrastive
meta-learning for conditional density estimation using kernel mean embeddings. In In-
ternational Conference on Artiﬁcial Intelligence and Statistics, pages 1099–1107. PMLR,
2021.
Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael PH Stumpf. Ap-
proximate Bayesian computation scheme for parameter inference and model selection in
dynamical systems. Journal of the Royal Society Interface, 6(31):187–202, 2009.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural
computation, 23(7):1661–1674, 2011.
Ziyu Wang, Shuyu Cheng, Li Yueru, Jun Zhu, and Bo Zhang. A Wasserstein minimum
velocity approach to learning unnormalized models. In Silvia Chiappa and Roberto Ca-
landra, editors, Proceedings of the Twenty Third International Conference on Artiﬁcial
Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,
pages 3728–3738. PMLR, 26–28 Aug 2020.
URL https://proceedings.mlr.press/
v108/wang20j.html.
Li Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton. Learning deep
kernels for exponential family densities. In International Conference on Machine Learn-
ing, pages 6737–6746. PMLR, 2019.
Richard Wilkinson.
Accelerating ABC methods using Gaussian processes.
In Artiﬁcial
Intelligence and Statistics, pages 1015–1023. PMLR, 2014.
Daniel S Wilks. Eﬀects of stochastic parametrizations in the Lorenz’96 system. Quarterly
Journal of the Royal Meteorological Society, 131(606):389–407, 2005.
Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, and Jes Frellsen. Partially
exchangeable networks and architectures for learning summary statistics in approximate
Bayesian computation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pages 6798–6807. PMLR, 09–15 Jun 2019. URL
http://proceedings.mlr.press/v97/wiqvist19a.html.
Simon N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature,
466(7310):1102, 2010.
Shiqing Yu, Mathias Drton, and Ali Shojaie. Generalized score matching for non-negative
data. Journal of Machine Learning Research, 20(76):1–70, 2019. URL http://jmlr.
org/papers/v20/18-278.html.
71

