86	
Copublished by the IEEE CS and the AIP	
1521-9615/09/$25.00 © 2009 IEEE
Computing in Science & Engineering
S c i e n t i f i c  P r o g r a m m i n g
Editors: Konstantin Läufer, laufer@cs.luc.edu
Konrad Hinsen, hinsen@cnrs-orleans.fr
S
ince the early days of com-
puting, software development 
techniques 
have 
changed 
almost as much as computer tech-
nology itself. Ever more powerful 
hardware made it possible to write 
ever more complex software, which 
both required ever better develop-
ment tools and techniques and made 
their implementation possible. Pro-
grammers thus moved from machine 
code to assembly languages and then 
problem-oriented programming lan-
guages, which have evolved to inte-
grate techniques such as structural 
and object-oriented programming. 
Another evolution went from mono-
lithic programs via separately compil-
able modules and libraries to software 
component technologies. However, in 
one respect, today’s popular program-
ming techniques are still the same as 
those the pioneers used: our programs 
consist of statements that modify data 
stored in the computer’s memory until 
that memory contains the desired re-
sult. This approach closely resembles 
how a computer works at the hardware 
level: the processor fetches data from 
memory, performs elementary opera-
tions on it, and writes the result back 
to a memory cell.
In fact, this approach is so com-
mon that most of you have probably 
never questioned it. And yet, an al-
ternative approach was developed 
as mathematical theory in the 1930s 
(Alonzo Church’s λ-calculus) and as a 
programming technique in the 1950s 
(John McCarthy’s Lisp language)—
functional 
programming. 
Although 
functional programming has been 
very popular in computer science 
research for several decades, its use 
for writing real-life programs is rela-
tively recent. Of the many reasons 
for this, the two most important are 
that functional programming is very 
different from traditional program-
ming (also referred to as imperative 
programming) and thus requires a lot 
of learning and unlearning, and that 
computer hardware implements the 
imperative programming model, so 
imperative programs are easier to 
compile into efficient machine code 
than functional programs. However, 
several clear signs indicate a growing 
interest in functional programming 
techniques—recent 
programming 
languages (such as Sun’s Fortress or 
Microsoft’s F#) have the explicit goal 
of supporting it. The reason is that 
functional programming has several 
advantages for concurrent and par-
allel programming. Experience also 
suggests that functional programs are 
more robust and easier to test than 
imperative ones.
In scientific computing, research-
ers have mainly used functional pro-
gramming for symbolic processing. In 
fact, the most widely used functional 
programming language in science is 
probably Mathematica, although most 
Mathematica users don’t write com-
plex functional programs. Another 
example of symbolic processing is the 
widely used fast Fourier transform li-
brary, FFTW,1 which uses a function-
al code optimizer written in OCaml 
to produce optimized C code for a 
Fourier transform of a given length. 
In numerical applications, functional 
programming doesn’t yet play an im-
portant role. Perhaps the most ambi-
tious project to introduce it into the 
number-crunching world was Law-
rence Livermore National Laborato-
ry’s Streams and Iteration in a Single 
Assignment Language (SISAL) proj-
ect, which started in 1983. SISAL is a 
functional language with paralleliza-
tion support, designed specifically for 
the needs of scientific applications. 
Unfortunately, it didn’t attract the 
attention it deserved, and funding 
stopped in 1996. Today, SISAL lives 
on as an open source project (http://
sorceforge.net/projects/sisal).
This article’s purpose is to explain 
what functional programming is and 
how it differs from traditional impera-
tive programming. I also explain how 
functional programming helps with 
concurrent and parallel program-
ming. The language I use in the ex-
amples is Clojure, a modern dialect 
of Lisp (see the sidebar “Clojure and 
the Lisp Family”), but everything said 
here applies equally to other function-
Adopting a functional programming style could make your programs more robust, more compact, and more 
easily parallelizable. However, mastering it requires some effort.
The Promises  
of Functional Programming 
By Konrad Hinsen

July/August 2009
87
al languages, only the syntax will be 
different. If you’re interested in func-
tional programming, you should also 
read (or reread) Jerzy Karczmarczuk’s 
1999 article in this magazine,2 which 
illustrates how functional program-
ming can yield elegant solutions to 
problems in mathematical modeling.
The Basics 
The fundamental principle of func-
tional programming is that you re-
alize a computation by composing 
functions. The word “function” is 
used here in the mathematical sense—
a mapping from input values to output 
values—whereas what most program-
ming languages call “functions” are 
subroutines that return a value. One 
important difference is that a func-
tion in the mathematical sense always 
produces the same output when given 
the same input. An operation such as 
“get the next line from a file” isn’t a 
function because each time you call, 
it produces a different return value. 
Another important difference is that 
a mathematical function doesn’t “do” 
anything other than return a value. It 
isn’t supposed to have side effects—for 
example, it shouldn’t write anything to 
a file or change a variable in memory. 
If a program is composed of func-
tions, and functions aren’t supposed 
to change any variables, then what are 
variables good for? Nothing, and that’s 
why functional programming doesn’t 
have variables. This is probably the 
biggest surprise to those who discover 
functional programming because vari-
ables are so very fundamental to our 
traditional ways of writing programs. 
The other missing fundamental con-
cept is loops. After all, what’s the point 
of a loop if nothing can change be-
tween iterations because there are no 
variables? By now, you might be con-
vinced that it’s absolutely impossible to 
write a useful program in a functional 
style, but keep reading.
What replaces loops in functional 
programs is recursion. A function 
is called recursive if it calls itself—­
directly or indirectly. Of course, 
calling itself makes sense only if the 
arguments change between calls. 
Moreover, if you ever want to get out 
of the call-yourself chain, a recursive 
function must return without calling 
itself for some arguments. Let’s look 
at a simple example of recursion:
(defn countdown [n] 
  (if (zero? n) 
    (list 0) 
    (cons n 
          (countdown 
            (- n 1)))))
These six lines define a function 
countdown of a single argument n, 
which should be an integer (although 
this is neither said nor enforced, Clo-
jure being a dynamicallly typed lan-
guage like Python and JavaScript). If n 
is zero, the return value of countdown 
is (list 0), a list containing the sin-
gle element 0. Otherwise (we’re look-
ing at lines four to six now), the return 
value is a list constructed by prepend-
ing n to the return value of count-
down for (- n 1), which is Clojure’s 
way of writing n-1. The function call 
(countdown 1) thus returns (cons 
1 (countdown 0)), which, after ex-
ecuting the recursive function call, 
becomes (cons 1 (list 0)). Look-
ing up the definitions of the functions 
cons and list will tell you that the 
final result is the two-element list (1 
0). This simple example illustrates 
how to use recursion for looping: with 
each recursive call, the argument be-
comes smaller, up to the point where 
it’s handled directly without any fur-
ther recursion.
A closer look at the chain of recur-
sive calls to countdown also reveals 
why functional programming can live 
without variables. At each recursive 
Clojure and the Lisp Family 
T
he language used in the main text’s examples is Clojure, a modern dialect 
of the Lisp language family. Lisp stands for “list processing,” which hints at 
the motivations of John McCarthy, who developed the first Lisp language in 
the late 1950s for use in artificial intelligence research. Today, the most widely 
used Lisp dialects are Scheme and Common Lisp. 
The Lisp language family’s distinctive feature is the principle that “code is 
data.” Lisp provides a syntax for a simple yet flexible data structure—nested lists. 
It then defines how to interpret nested lists that follow specific conventions as 
executable code. The advantage of expressing code in a data structure is that 
Lisp code can easily generate (and then execute) other Lisp code, a fact that pro-
grammers have exploited from the beginning through Lisp macros, the world’s 
first meta-programming system and probably still its most powerful one. 
Lisp has the reputation of being slow, but that isn’t true anymore. Many 
modern compilers can produce code that can compete with C in performance 
when given appropriate optimization hints. However, it remains very difficult 
to produce programs that are both efficient and portable between compilers 
and platforms. 
Clojure is a recent Lisp dialect that sets itself apart by three features: it has 
four highly optimized data structures (lists, vectors, maps, and sets) designed 
for pure functional programming, it offers extensive support for concurrency, 
and it was designed for the Java Virtual Machine with the goal of easy inter­
operability with other JVM languages, including Java itself. For more informa-
tion about Clojure, see its Web site at http://clojure.org.

S c i e n t i f i c  P r o g r a m m i n g
88
Computing in Science & Engineering
call, n decreases by one, which looks a 
bit as if n were a variable decremented 
from its initial value to zero; indeed, 
a compiler could transform the recur-
sive function into a subroutine with a 
loop over n for efficiency reasons. The 
crucial difference is that n isn’t a refer-
ence to a piece of memory that could 
be modified at will, intentionally or 
by mistake. In fact, as anyone with 
debugging experience has learned the 
hard way, the problem with variables 
is often that looking at a variable’s 
value doesn’t tell you where that value 
came from. In contrast, you can al-
ways trace the call chain that leads to 
n having a specific value at a specific 
point in the function countdown back 
to its beginning. The function call 
chain is a complete description of the 
data flow through the program, and 
it’s a very useful feature for verifying a 
program’s correctness.
I hope I’ve convinced you that vari-
ables and loops aren’t as essential as 
you might have thought. But what 
about other side effects? Is it really 
practical to work with programs that 
can’t write data to a file? Or, in fact, 
produce any output? Of course, the 
answer is no: the pure functional pro-
grams I’ve described to this point will 
just heat up your computer. You need 
side effects if you want your program 
to have any interaction with the real 
world. But you can—and should—
limit side effects to very few places in 
a program.
We can categorize functional pro-
gramming languages by their attitude 
to unavoidable side effects. Pure lan-
guages (such as Haskell) allow them 
only inside special language con-
structs, permitting the compiler to 
verify the absence of accidental side 
effects everywhere else. Impure lan-
guages (the majority) leave the respon-
sibility for the use of side effects fully 
to the programmer. As is so often the 
case, both approaches have their good 
and bad sides.
Functional Abstractions 
Abstractions are fundamental to writ-
ing nontrivial programs. They permit 
expressing an algorithm in terms of 
concepts that are useful in its context, 
rather than in terms of operations that 
the computer or the programming 
language already happens to provide. 
A programmer would write a least-
squares fit problem, for example, in 
terms of linear-algebra operations—
such as matrix multiplication and 
solving linear systems of equations—
that work on an array data structure. 
Compilers and libraries (written by 
you or by someone else) then trans-
form the algorithm into something 
that a computer can handle.
The abstractions provided by popu-
lar programming languages for sci-
entific computing include basic data 
structures (integer, real and complex 
numbers, arrays, and structures) 
and a notation for numerical expres-
sions that’s similar to mathemati-
cal notation. Programmer-provided 
abstractions are mainly subroutines. 
Object-oriented languages add power-
ful constructs for data abstraction: the 
programmer can add problem-specific 
data types to the general ones provid-
ed by compiler and runtime systems.
In functional programming, al-
gorithmic abstractions are the most 
prominent. A developer identifies and 
implements patterns that occur re-
peatedly in algorithms in the form of 
functions, which is made possible by 
the fact that functional programming 
languages consider functions to be 
data. It’s possible, and even very com-
mon, to write functions that take oth-
er functions as parameters, returning 
yet another function. Such functions 
are called higher-order functions, as op-
posed to first-order functions whose 
arguments and return values are all 
standard data items. In mathematics, 
you would use the term operator, an 
example being the derivative operator 
that maps a function to its derivative, 
which is itself a function.
As a simple example, let’s consider 
the following operations: calculating 
the sum of a list of numbers, calculat-
ing the product of a list of numbers, 
and finding the set of all items that oc-
cur in a list of values. What these (and 
many more) operations have in com-
mon is a simple algorithmic pattern: 
you start with an initially “empty” ac-
cumulator value (0, 1, the empty set) 
and then iterate over a list, combining 
at each step the current accumula-
tor value with one list element. The 
combination operations for the three 
examples given are addition, multi-
plication, and adding an item to a set. 
This algorithmic pattern is known as 
reduction; it’s implemented in Clojure 
via the function reduce. We can thus 
write our three examples as
(defn sum [numbers] 
  (reduce + 0 numbers)) 
(defn product [numbers] 
  (reduce * 1 numbers)) 
(defn set-of-items [items] 
  (reduce conj #{} items)) 
The first argument to reduce is the 
combination operation, which is a 
function of two arguments. In Clojure, 
we can simply use + and * for addition 
and multiplication because they’re 
plain functions—there’s no special 
notation for mathematical operators. 
In the last example, conj is a function 
that adds an item to a collection.
It’s interesting to see how we could 
implement reduce if it weren’t provid-
ed already. Here’s one way to write it:

July/August 2009
89
(defn my-reduce 
  [op initial items] 
  (if (empty? items) 
    initial 
    (my-reduce 
      op 
(op initial  
    (first items)) 
(rest items)))) 
This is again a recursive function. If 
its input list items is empty, it just re-
turns the initial value. This is the re-
cursion’s exit; otherwise, it applies the 
function op to the initial value and the 
first element of the list ((op ini-
tial (first items))) and feeds 
the result to a recursive call on what’s 
left of the list after removing the first 
element ((rest items)). Note that 
Clojure doesn’t treat the argument op 
in any special way merely because it’s 
a function. Functions are perfectly 
ordinary data items, just like integers 
and text strings.
Functions can also create and re-
turn other functions, as illustrated in 
the following (somewhat contrived) 
example, which defines a function 
make-adder that takes a numerical 
argument x and returns a function of 
another numerical argument y that 
adds x and y:
(defn make-adder [x] 
  (fn [y] (+ x y)))
Calling (make-adder 2) returns a 
function that adds 2 to its argument. 
We can use this function just like any 
other one, such as 
((make-adder 2) 3) 
which yields 5. Note that the result 
of (make-adder 2) is a function 
that stores an integer value (2) inter-
nally that was passed as an argument 
to make-adder. A function that cap-
tures a value in this way is called a clo-
sure; it’s a widely used programming 
technique in functional programs. As 
you might have guessed, this is where 
the Clojure language derived its name, 
with the “j” hinting at Java. 
Concurrency and Parallelism 
Concurrency (the existence of sev-
eral execution threads operating on 
the same data) and parallelism (the 
division of a computational task into 
multiple communicating processes 
running in parallel) are two aspects 
of computing rapidly gaining in im-
portance. The main reason is that 
single-processor performance is no 
longer improving at the pace it used 
to; instead, computers are becom-
ing more powerful by integrating 
more computational cores. Exploiting 
such machines requires concurrency, 
parallelism, or both. Unfortunately, 
today’s mainstream techniques for 
concurrent and parallel programming 
are difficult to learn and quite error-
prone in practice. 
The big issue with concurrency is 
the difficulty of maintaining the data 
in a coherent state: it must be impos-
sible for one execution thread to mod-
ify data that another one is accessing 
(reading or writing) at the same time. 
So, if several threads need to modify a 
data item, they must do so in a coordi-
nated way. This is currently achieved 
via locks, but they’re difficult to use, 
and their incorrect use can go un-
Other Functional Languages 
W
e can group the most popular languages that sup-
port functional programming into just a few families. 
The oldest one, the Lisp family, is described in the “Clojure 
and the Lisp Family” sidebar. 
The second group is the ML family, which first appeared 
in the 1970s. Today, its most prominent members are Stan-
dard ML and OCaml, but Microsoft’s recently published F# 
language might soon catch up with them. The ML languag-
es differ from the Lisp family in two respects: they’re stati-
cally typed, using the Hindley-Milner inference algorithm to 
permit the compiler to deduce the types of most functions 
from the way they’re used, and they propose a pattern-
matching syntax for defining functions that makes many 
definitions look similar to common mathematical notation. 
The Haskell language is the result of a collective effort to 
define a common functional language for use in program-
ming language research. It’s similar in many respects to the 
ML family, the most important difference being lazy evalu-
ation: a data item (such as a list element) is evaluated only 
when its value is actually required in a computation. This 
is possible only in a pure functional setting because side 
effects would otherwise occur at completely unpredictable 
times. Lazy evaluation makes it possible to work with infi-
nite data structures, avoid unnecessary computations, and 
define control structures as simple functions. However, lazy 
evaluation also makes a program’s CPU time and memory 
usage profile more difficult to understand and generally 
leads to slower programs overall because of unavoidable 
bookkeeping overhead. 
Two recent languages, Scala (for the Java Virtual Ma-
chine) and Nemerle (for the .NET platform), are hybrid 
languages that add functional programming features to 
otherwise quite standard object-oriented languages. Sun’s 
new Fortress language, whose main intended application 
domain is high-performance computing, also proposes a 
mixture of functional and object-oriented features. 
Moving on to special-purpose languages, Wolfram’s 
computer algebra system Mathematica is based on a 
proprietary functional programming language. Other 
computer algebra systems also propose functional features, 
although not always to the point of allowing a full func-
tional programming style.

S c i e n t i f i c  P r o g r a m m i n g
90
Computing in Science & Engineering
noticed for a long time before errors 
show up. As for parallelism, the main 
difficulties are identifying independent 
computations inside a program and 
coordinating them with the required 
communication operations such that 
the resulting program always produces 
the correct result and does so efficient-
ly for typical input parameters. 
Functional programming is fre-
quently cited as a promising tech-
nique in this context. Pure functional 
code has no variables and thus no data 
coherence issues or need for locking. 
Moreover, all data dependencies are 
explicit, making it possible to apply a 
large number of program transforma-
tions (with the goal of parallelization) 
while guaranteeing that the program’s 
result won’t change. 
I
f this makes you hope that auto-
matic parallelization will be your 
welcome gift once you succeed in 
entering the world of functional 
programming, you’re in for disap-
pointment. Although it’s true that 
compilers for functional languages 
could in principle transform a se-
rial into an equivalent parallel pro-
gram automatically, there remains 
the problem of finding such a trans-
formation that actually yields an ef-
ficient program for a given parallel 
computer and given input data. Com-
piler technology isn’t yet up to this 
task, although this could well change 
in the future, in particular with par-
allelizing just-in-time compilers that 
have access to a program’s execution 
time profile. For the near future, it’s 
reasonable to expect compilers that 
create parallel programs semiauto-
matically based on programmer-pro-
vided performance hints. This would 
already be a significant step forward 
compared to today’s parallel pro-
gramming techniques.
References 
M. Frigo and S.G. Johnson, “The Design and 
1.	
Implementation of FFTW3,” Proc. IEEE, vol. 
93, no. 2, 2005, pp. 216–231. 
J. Karczmarczuk, “Scientific Computation and 
2.	
Functional Programming,” Computing in Sci-
ence & Eng., vol. 1, no. 3, 1999, pp. 64–72. 
Konrad Hinsen is a researcher at the Centre 
de Biophysique Moléculaire in Orléans and at 
the Synchrotron Soleil in Saint Aubin, France. 
His research interests include protein struc-
ture and dynamics and scientific computing. 
Hinsen has a PhD in theoretical physics from 
RWTH Aachen University, Germany. Contact 
him at hinsen@cnrs-orleans.fr. 
The American Institute of Physics is a not-for-profit membership corporation chartered 
in New York State in 1931 for the purpose of promoting the advancement and diffusion 
of the knowledge of physics and its application to human welfare. Leading societies in 
the fields of physics, astronomy, and related sciences are its members.
In order to achieve its purpose, AIP serves physics and related fields of science and 
technology by serving its member societies, individual scientists, educators, students, 
R&D leaders, and the general public with programs, services, and publications—
information that matters. The Institute publishes its own scientific journals as well as 
those of its member societies; provides abstracting and indexing services; provides 
online database services; disseminates reliable information on physics to the public; 
collects and analyzes statistics on the profession and on physics education; encourages 
and assists in the documentation and study of the history and philosophy of physics; 
cooperates with other organizations on educational projects at all levels; and collects 
and analyzes information on federal programs and budgets.
The scientists represented by the Institute through its member societies number more 
than 134 000. In addition, approximately 6000 students in more than 700 colleges and 
universities are members of the Institute’s Society of Physics Students, which includes 
the honor society Sigma Pi Sigma. Industry is represented through the membership of 37 
Corporate Associates.
Governing Board: Louis J. Lanzerotti (chair)*, Lila M. Adair, David E. Aspnes, Anthony 
Atchley*, Arthur Bienenstock, Charles W. Carter Jr*, Timothy A. Cohn*, Bruce H. Curran*, 
Morton M. Denn*, Alexander Dickison, Michael D. Duncan, H. Frederick Dylla (ex 
officio)*, Janet Fender, Judith Flippen-Anderson, Judy R. Franz*, Brian J. Fraser, Jaime 
Fucugauchi, John A. Graham, Timothy Grove, Mark Hamilton, Warren W. Hein*, William 
Hendee, James Hollenhorst, Judy C. Holoviak, Leo Kadanoff, Angela R. Keyser, Timothy 
L. Killeen, Harvey Leff, Rudolf Ludeke*, Kevin B. Marvel*, Patricia Mooney, Cherry 
Murray, Elizabeth A. Rogan*, Bahaa E. A. Saleh, Charles E. Schmid, Joseph Serene, 
Benjamin B. Snavely (ex officio)*, A. F. Spilhaus Jr, Gene Sprouse, Hervey (Peter) 
Stockman, Quinton L. Williams.
 
*Members of the Executive Committee.
Management Committee: H. Frederick Dylla, Executive Director and CEO; Richard 
Baccante, Treasurer and CFO; Theresa C. Braun, Vice President, Human Resources; 
Catherine O’Riordan, Vice President, Physics Resources; John S. Haynes, Vice 
President, Publishing; Benjamin B. Snavely, Secretary.
www.aip.org

For access to more content from the IEEE Computer Society, 
see computingnow.computer.org.
This article was featured in
Top articles, podcasts, and more.
computingnow.computer.org

