A Generalist Framework for Panoptic Segmentation of Images and Videos
Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton†, David J. Fleet†
Google Research, Brain Team
{iamtingchen,lala,srbs,geoffhinton,davidfleet}@google.com
Abstract
Panoptic segmentation assigns semantic and instance ID
labels to every pixel of an image. As permutations of in-
stance IDs are also valid solutions, the task requires learn-
ing of high-dimensional one-to-many mapping. As a re-
sult, state-of-the-art approaches use customized architec-
tures and task-speciﬁc loss functions. We formulate panop-
tic segmentation as a discrete data generation problem,
without relying on inductive bias of the task. A diffusion
model based on analog bits [12] is used to model panoptic
masks, with a simple, generic architecture and loss function.
By simply adding past predictions as a conditioning signal,
our method is capable of modeling video (in a streaming
setting) and thereby learns to track object instances auto-
matically. With extensive experiments, we demonstrate that
our generalist approach can perform competitively to state-
of-the-art specialist methods in similar settings.
1. Introduction
Panoptic segmentation [30] is a fundamental vision task
that assigns semantic and instance labels for every pixel of
an image. The semantic labels describe the class of each
pixel (e.g., sky, vertical), and the instance labels provides
a unique ID for each instance in the image (to distinguish
different instances of the same class). The task is a combi-
nation of semantic segmentation and instance segmentation,
providing rich semantic information about the scene.
While the class categories of semantic labels are ﬁxed a
priori, the instance IDs assigned to objects in an image can
be permuted without affecting the instances identiﬁed. For
example, swapping instance IDs of two cars would not af-
fect the outcome. Thus, a neural network trained to predict
instance IDs should be able to learn a one-to-many map-
ping, from a single image to multiple instance ID assign-
ments. The learning of one-to-many mappings is challeng-
ing and traditional approaches usually leverage a pipeline
of multiple stages involving object detection, segmentation,
† Equal advising.
Figure 1. We formulate panoptic segmentation as a conditional
discrete mask (m) generation problem for images (left) and videos
(right), using a Bit Diffusion generative model [12].
merging multiple predictions [14, 30, 34, 40, 65]. Recently,
end-to-end methods [16, 17, 33, 35, 58, 67–69] have been
proposed, based on a differentiable bipartite graph match-
ing [7]; this effectively converts a one-to-many mapping
into a one-to-one mapping based on the identiﬁed matching.
However, such methods still require customized architec-
tures and specialized loss functions with built-in inductive
bias for the panoptic segmentation task.
Eshewing task-speciﬁc architectures and loss functions,
recent generalist vision models, such as Pix2Seq [10, 11],
OFA [60], UViM [31], and Uniﬁed I/O [43], advocate a
generic, task-agnostic framework, generalizing across mul-
tiple tasks while being much simpler than previous models.
For instance, Pix2Seq [10, 11] formulates a set of core vi-
sion tasks in terms of the generation of semantically mean-
ingful sequences conditioned on an image, and they train a
single autoregressive model based on Transformers [55].
Following the same philosophy, we formulate the panop-
tic segmentation task as a conditional discrete data genera-
tion problem, depicted in Figure 1. We learn a generative
model for panoptic masks, treated as an array of discrete
tokens, conditioned on an input image. One can also apply
the model to video data (in an online/streaming setting), by
simply including predictions from past frames as an addi-
tional conditioning signal. In doing so, the model can then
learn to track and segment objects automatically.
Generative modeling for panoptic segmentation is very
challenging as the panoptic masks are discrete/categorical
and can be very large. To generate a 512×1024 panoptic
1
arXiv:2210.06366v2  [cs.CV]  29 Dec 2022

mask, for example, the model has to produce more than 1M
discrete tokens (of semantic and instance labels). This is
expensive for auto-regressive models as they are inherently
sequential, scaling poorly with the size of data input. Diffu-
sion models [23, 50–52] are better at handling high dimen-
sion data but they are most commonly applied to continuous
rather than discrete domains. By representing discrete data
with analog bits [12] we show that one can train a diffusion
model on large panoptic masks directly, without the need to
also learn an intermediate latent space.
In what follows, we introduce our diffusion-based model
for panoptic segmentation, and then describe extensive ex-
periments on both image and video datasets. In doing so we
demonstrate that the proposed method performs competi-
tively to state-of-the-art methods in similar settings, proving
a new, generalist approach to panoptic segmentation.
2. Preliminaries
Problem Formulation.
Introduced in [30], panoptic seg-
mentation masks can be expressed with two channels, m ∈
ZH×W ×2. The ﬁrst represents the category/class label. The
second is the instance ID. Since instance IDs can be per-
muted without changing the underlying instances, we ran-
domly assign integers in [0, K] to instances every time an
image is sampled during training. K is maximum number
of instances allowed in any image (0 denotes the null label).
To solve the panoptic segmentation problem, we simply
learn an image-conditional panoptic mask generation model
by maximizing P
i log P(mi|xi), where mi is a random
categorical variable corresponding to the panoptic mask for
image xi in the training data. As mentioned above, panoptic
masks may comprise hundreds of thousands or even mil-
lions of discrete tokens, making generative modeling very
challenging, particularly for autoregressive models.
Diffusion Models with Analog Bits.
Unlike autoregres-
sive generative models, diffusion models are more effective
with high dimension data [23, 50–52] . Training entails
learning a denoising network. During inference, the net-
work generates target data in parallel, using far fewer itera-
tions than the number of pixels.
In a nutshell, diffusion models learn a series of state tran-
sitions to transform noise ϵ from a known noise distribution
into a data sample x0 from the data distribution p(x). To
learn this mapping, we ﬁrst deﬁne a forward transition from
data x0 to a noisy sample xt as follows,
xt =
p
γ(t) x0 +
p
1 −γ(t) ϵ,
(1)
where ϵ is drawn from standard normal density, t is from
uniform density on [0,1], and γ(t) is a monotonically de-
creasing function from 1 to 0. During training, one learns a
neural network f(xt, t) to predict x0 (or ϵ) from xt, usually
formulated as a denoising task with an ℓ2 loss:
Lx0 = Et∼U(0,T ),ϵ∼N(0,1),xt∥f(xt, t) −x0∥2.
(2)
To generate samples from a learned model, it starts with a
sample of noise, xT , and then follows a series of (reverse)
state transitions xT →xT −∆→· · · →x0 by iteratively
applying the denoising function f with appropriate transi-
tion rules (such as those from DDPM [23] or DDIM [51]).
Conventional diffusion models assume continuous data
and Gaussian noise, and are not directly applicable to dis-
crete data.
To model discrete data, Bit Diffusion [12]
ﬁrst converts integers representing discrete tokens into bit
strings, the bits of which are then cast as real numbers
(a.k.a., analog bits) to which continuous diffusion models
can be applied. To draw samples, Bit Diffusion uses a con-
ventional sampler from continuous diffusion, after which a
ﬁnal quantization step (simple thresholding) is used to ob-
tain the categorical variables from the generated analog bits.
3. Method
We formulate panoptic segmentation as a discrete data
generation problem conditioned on pixels,
similar to
Pix2Seq [10, 11] but for dense prediction; hence we coin
our approach Pix2Seq-D. In what follows we ﬁrst specify
the architecture design, and then the training and inference
algorithms based on Bit Diffusion.
3.1. Architecture
Diffusion model sampling is iterative, and hence one
must run the forward pass of the network many times dur-
ing inference. Therefore, as shown in Fig. 2, we intention-
ally separate the network into two components: 1) an im-
age encoder; and 2) a mask decoder. The former maps raw
pixel data into high-level representation vectors, and then
the mask decoder iteratively reads out the panoptic mask.
Pixel/image Encoder.
The encoder is a network that
maps a raw image x ∈RH×W ×3 into a feature map in
RH′×W ′×d where H′ and W ′ are the height and width of
the panoptic mask. The panoptic masks can be the same
size or smaller than the original image. In this work, we fol-
low [7,10] in using ResNet [22] followed by transformer en-
coder layers [55] as the feature extractor. To make sure the
output feature map has sufﬁcient resolution, and includes
features at different scales, inspired by U-Net [23, 45, 47]
and feature pyramid network [38], we use convolutions with
bilateral connections and upsampling operations to merge
features from different resolutions. More sophisticated en-
coders are possible, to leverage recent advances in architec-
ture designs [20,25,26,41,70], but this is not our main focus
so we opt for simplicity.
2

Image
hidden features
Noised mask
Mask
Cross attention
Channel-wise 
concatenation
Image enc oder
Mas k dec oder
X steps
Figure 2. The architecture for our panoptic mask generation framework. We separate the model into image encoder and mask decoder so
that the iterative inference at test time only involves multiple passes over the decoder.
Algorithm 1 Pix2Seq-D training algorithm.
def train_loss(images, masks):
"""images: [b, h, w, 3], masks: [b, h’, w’, 2]."""
# Encode image features.
h = pixel_encoder(images)
# Discrete masks to analog bits.
m_bits = int2bit(masks).astype(float)
m_bits = (m_bits * 2 - 1) * scale
# Corrupt analog bits.
t = uniform(0, 1)
# scalar.
eps = normal(mean=0, std=1) # same shape as m_bits.
m_crpt = sqrt(
gamma(t)) * m_bits +
sqrt(1 - gamma(t)) * eps
# Predict and compute loss.
m_logits, _ = mask_decoder(m_crpt, h, t)
loss = cross_entropy(m_logits, masks)
return loss.mean()
Mask Decoder.
The decoder iteratively reﬁnes the panop-
tic mask during inference, conditioned on the image fea-
tures. Speciﬁcally, the mask decoder is a TransUNet [8]. It
takes as input the concatenation of image feature map from
encoder and a noisy mask (randomly initialized or from pre-
vious step), and outputs a reﬁned prediction of the mask.
One difference between our decoder and the standard U-Net
architecture used for image generation and image-to-image
translation [23, 45, 48] is that we use transformer decoder
layers on the top of U-Net, with cross-attention layers to in-
corporate the encoded image features (before upsampling).
3.2. Training Algorithm
Our main training objective is the conditional denoising
of analog bits [12] that represent noisy panoptic masks. Al-
gorithm 1 gives the training algorithm (with extra details
in A), the key elements of which are introduced below.
Algorithm 2 Pix2Seq-D inference algorithm.
def infer(images, steps=10, td=1.0):
"""images: [b, h, w, 3]."""
# Encode image features.
h = pixel_encoder(images)
m_t = normal(mean=0, std=1) # same shape as m_bits.
for step in range(steps):
# Get time for current and next states.
t_now = 1 - step / steps
t_next = max(1 - (step + 1 + td) / steps, 0)
# Predict analog bits m_0 from m_t.
_, m_pred = mask_decoder(m_t, h, t_now)
# Estimate m at t_next.
m_t = ddim_step(m_t, m_pred, t_now, t_next)
# Analog bits to masks.
masks = bit2int(m_pred > 0)
return masks
Analog Bits with Input Scaling.
The analog bits are real
numbers converted from the integers of panoptic masks.
When constructing the analog bits, we can shift and scale
them into {−b, b}. Typically, b is set to be 1 [12] but we
ﬁnd that adjusting this scaling factor has an signiﬁcant ef-
fect on the performance of the model. This scaling factor
effectively allows one to adjust the signal-to-noise ratio of
the diffusion process (or the noise schedule), as visualized
in Fig. 3. With b = 1, we see that even with a large time
step t = 0.7 (with t ∈[0, 1]), the signal-to-noise ratio is
still relatively high, so the masks are visible to naked eye
and the model can easily recover the mask without using
the encoded image features. With b = 0.1, however, the
denoising task becomes signiﬁcantly harder as the signal-
to-noise ratio is reduced. In our study, we ﬁnd b = 0.1
works substantially better than the default of 1.0.
Softmax Cross Entropy Loss.
Conventional diffusion
models (with or without analog bits) are trained with an
3

b = 1.0
(a) t = 0.1.
(b) t = 0.3.
(c) t = 0.5.
(d) t = 0.7.
(e) t = 0.9.
b = 0.1
(f) t = 0.1.
(g) t = 0.3.
(h) t = 0.5.
(i) t = 0.7.
(j) t = 0.9.
Figure 3. Noisy masks at different time steps under two input scaling factors, b = 1.0 (top row) and b = 0.1 (bottom row). Decreasing the
input scaling factor leads to smaller signal-to-noise ratio (at the same time step), which gives higher weights to harder cases.
(a) p = 0.1.
(b) p = 0.2.
(c) p = 0.3.
(d) p = 0.4.
Figure 4.
The effect of p on loss weighting for panoptic masks. With p = 0, every mask token is weighted equally (equivalent to no
weighting). As p increases, larger weight is given to mask tokens of smaller instances (indicated by warmer colors).
ℓ2 denoising loss. This works reasonably well for panop-
tic segmentation, but we also discovered that a loss based
on softmax cross entropy yields better performance. Al-
though the analog bits are real numbers, they can be seen
as a one-hot weighted average of base categories. For ex-
ample, ‘01’ = α0‘00’ + α1‘01’ + α2‘10’ + α3‘11’ where
α1 = 1, and α0 = α2 = α3 = 0. Instead of modeling
the analog bits in ‘01’ as real numbers, with a cross entropy
loss, the network can directly model the underlying distri-
bution over the four base categories, and use the weighted
average to obtain the analog bits. As such, the mask de-
coder output not only analog bits (m_pred), but also the
corresponding logits (m_logits), ˜y ∈RH×W ×K, with a
cross entropy loss for training; i.e.,
L =
X
i,j,k
yijk log softmax(˜yijk)
Here, y is the one-hot vector corresponding to class or in-
stance label. During inference, we still use analog bits from
the mask decoder instead of underlying logits for the reverse
diffusion process.
Loss Weighting.
Standard generative models for discrete
data assign equal weight to all tokens. For panoptic segmen-
tation, with a loss deﬁned over pixels, this means that large
objects will have more inﬂuence than small objects. And
this makes learning to segment small instances inefﬁcient.
To mitigate this, we use an adaptive loss to improve the
segmentation of small instances by giving higher weights
to mask tokens associated with small objects. The speciﬁc
per-token loss weighting is as follows:
wij = 1/cp
ij , and w′
ij = H ∗W ∗wij/
X
ij
wij ,
where cij is the pixel count for the instance at pixel location
(i, j), and p is a tunable parameter; uniform weighting oc-
curs when p = 0, and for p > 0, a higher weight is assigned
to mask tokens of small instances. Fig. 4 demonstrate the
effects of p in weighting different mask tokens.
3.3. Inference Algorithm
Algorithm 2 summarizes the inference procedure. Given
an input image, the model starts with random noise as the
initial analog bits, and gradually reﬁnes its estimates to be
closer to that of good panoptic masks.
Like Bit Diffu-
sion [12], we use asymmetric time intervals (controlled by
a single parameter td) that is adjustable during inference
time. It is worth noting that the encoder is only run once, so
the cost of multiple iterations depends on the decoder alone.
4

Noised mask t
Mask t
mask t-k
mask t-1
hidden features
Figure 5. Mask decoder extended for video settings. The image
conditional signal to the mask decoder is concatenated with mask
predictions from previous frames of the video.
3.4. Extension to Videos
Our image-conditional panoptic mask modeling with
p(m|x) is directly applicable for video panoptic segmen-
tation by considering 3D masks (with an extra time dimen-
sion) given a video. To adapt for online/streaming video
settings, we can instead model p(mt|xt, mt−1, mt−k),
thereby generating panoptic masks conditioned on the im-
age and past mask predictions. This change can be eas-
ily implemented by concatenating the past panoptic masks
(mt−1, mt−k) with existing noisy masks, as demonstrated
in Fig. 5. Other than this minor change, the model remains
the same as that above, which is simple and allows one to
ﬁne-tune an image panoptic model for video.
With the past-conditional generation (using the denois-
ing objective), the model automatically learns to track and
segment instances across frames, without requiring explicit
instance matching through time. Having an iterative reﬁne-
ment procedure also makes our framework convenient to
adapt in a streaming video setting where there are strong de-
pendencies across adjacent frames. We expect fewer infer-
ence steps to arrive at good segmentation results when there
are relative small change in video frames, it may be possible
to set reﬁnement steps adaptively across video frames.
4. Experiments
4.1. Setup and Implementation Details
Datasets.
For image panoptic segmentation, we conduct
experiments on the two commonly used benchmarks: MS-
COCO [39] and Cityscapes [19]. MS-COCO contains ap-
proximately 118K training images and 5K validation im-
ages used for evaluation. Cityscapes contains 2975 images
for training, 500 for validation and 1525 for testing. We
report results on Cityscapes val set, following most exit-
ing papers. For expedience we conduct most model abla-
tions on MS-COCO. Finally, for video segmentation we use
DAVIS [46] in the challenging unsupervised setting, with
no segmentation provided at test time. DAVIS comprises
60 training videos and 30 validation videos for evaluation.
Training.
MS-COCO is larger and more diverse than
Cityscapes and DAVIS. Thus we mainly train on MS-
COCO, and then transfer trained models to Cityscapes
and DAVIS with ﬁne-tuning (at a single resolution). We
ﬁrst separately pre-train the image encoder and mask de-
coder before training the image-conditional mask genera-
tion on MS-COCO. The image encoder is taken from the
Pix2Seq [10] object detection checkpoint, pre-trained on
objects365 [49]. It comprises a ResNet-50 [22] backbone,
and 6-layer 512-dim Transformer [55] encoder layers. We
also augment image encoder with a few convolutional up-
sampling layers to increase its resolution and incorporate
features at different layers. The mask decoder is a Tran-
sUNet [8] with base dimension 128, and channel multipli-
ers of 1×,1×,2×,2×, followed by 6-layer 512-dim Trans-
former [55] decoder layers. It is pre-trained on MS-COCO
as an unconditional mask generation model without images.
Directly training our model on high resolution images
and panoptic masks can be expensive as the existing ar-
chitecture scales quadratically with resolution. So on MS-
COCO, we train the model with increasing resolutions, sim-
ilar to [24, 53, 54].
We ﬁrst train at a lower resolution
(256×256 for images; 128×128 for masks) for 800 epochs
with a batch size of 512 and scale jittering [21, 64] of
strength [1.0, 3.0]. We then continue train the model at full
resolution (1024×1024 for images; 512×512 for masks) for
only 15 epochs with a batch size of 16 without augmenta-
tion. This works well, as both convolution networks and
transformers with sin-cos positional encoding generalize
well across resolutions. More details on hyper-parameter
settings for training can be found in Appendix B.
Inference.
We use DDIM updating rules [51] for sam-
pling. By default we use 20 sampling steps for MS-COCO.
We ﬁnd that setting td= 2.0 yields near optimal results.
We discard instance predictions with fewer than 80 pixels.
For inference on DAVIS, we use 32 sampling steps for the
ﬁrst frame and 8 steps for subsequent frames. We set td= 1
and discard instance predictions with fewer than 10 pixels.
4.2. Main Results
We compare with two families of state-of-the-art meth-
ods, i.e., specialist approaches and generalist approaches.
Table 1 summarizes results for MS-COCO. Pix2Seq-D
achieves competitive Panoptic Quality (PQ) to state-of-the-
art methods with the ResNet-50 backbone. When compared
with other recent generalist models such as UViM [31], our
model performs signiﬁcantly better while being much more
efﬁcient. Similar results are obtained for Cityscape, the de-
tails of which are given in Appendix C.
5

Method
Backbone
# of Params
PQ
PQthing
PQstuff
Specialist approaches:
MaskFormer [17]
ResNet-50
45M
46.5
51.0
39.8
K-Net [69]
ResNet-50
-
47.1
51.7
40.3
CMT-DeepLab [67]
ResNet-50
-
48.5
-
-
Panoptic SegFormer [35]
ResNet-50
51M
49.6
54.4
42.4
Mask2Former [16]
ResNet-50
44M
51.9
57.7
43.0
kMaX-DeepLab [68]
ResNet-50
57M
53.0
58.3
44.9
DETR [7]
ResNet-101
61.8M
45.1
50.5
37.0
Mask2Former [13]
Swin-L
216M
57.8
-
-
kMaX-DeepLab [68]
ConvNeXt-L
232M
58.1
64.3
48.8
MasK DINO [33]
Swin-L
223M
59.4
-
-
Generalist approaches:
UViM [31]
ViT
939M
45.8
-
-
Pix2Seq-D (steps=5)
ResNet-50
94.5M
47.5
52.2
40.3
Pix2Seq-D (steps=10)
ResNet-50
94.5M
49.4
54.4
41.9
Pix2Seq-D (steps=20)
ResNet-50
94.5M
50.3
55.3
42.9
Pix2Seq-D (steps=50)
ResNet-50
94.5M
50.2
55.1
42.8
Table 1. Results on MS-COCO. Pix2Seq-D achieves competitive results to state-of-the-art specialist models with ResNet-50 backbone.
Method
Backbone
J &F
J -Mean
J -Recall
F-mean
F-Recall
Specialist approaches:
RVOS [56]
ResNet-101
41.2
36.8
40.2
45.7
46.4
STEm-Seg [3]
ResNet-101
64.7
61.5
70.4
67.8
75.5
MAST [32]
ResNet-18
65.5
63.3
73.2
67.6
77.7
UnOVOST [44]
ResNet-101
67.9
66.4
76.4
69.3
76.9
Propose-Reduce [37]
ResNeXt-101
70.4
67.0
-
73.8
-
Generalist approaches:
Pix2Seq-D (ours)
ResNet-50
68.4
65.1
70.6
71.7
77.1
Table 2. Results of unsupervised video object segmentation on DAVIS 2017 validation set.
Table 2 compares Pix2Seq-D to state-of-the-art methods
on unsupervised video object segmentation on DAVIS, us-
ing the standard J &F metrics [46]. Baselines do not in-
clude other generalist models as they are not directly appli-
cable to the task. Our method achieves results on par with
state-of-the-art methods without specialized designs.
4.3. Ablations on Training
Ablations on model training are performed with MS-
COCO dataset. To reduce the computation cost while still
be able to demonstrate the performance differences in de-
sign choices, we train the model for 100 epochs with a batch
size of 128 in a single-resolution stage (512×512 image
size, and 256×256 mask size).
Input Scaling of Analog Bits.
Table 3 shows the depen-
dence of PQ results on input scaling of analog bits. The
scale factor of 0.1 used here yield results that outperform
the standard scaling of 1.0 in previous work [12].
Loss Functions.
Table 4 compares our proposed cross en-
tropy loss to the ℓ2 loss normally used by diffusion models.
Input scaling
0.03
0.1
0.3
1.0
PQ
40.8
43.9
38.7
21.3
Table 3. Ablation on input scaling
Loss function
ℓ2 Regression
Cross Entropy
PQ
41.9
43.9
Table 4. Ablation on loss function.
Loss weight p
0
0.2
0.4
0.6
PQ
40.4
43.9
43.7
41.3
Table 5. Ablation on loss weighting.
Interestingly, cross entropy yields substantial gains over ℓ2.
Loss weighting.
Table 5 shows the effects of p for loss
weighting. Weighting with p = 0.2 appears near optimal
and clearly outperforms uniform weighting (p = 0).
6

5.0
10.0
20.0
30.0
40.0
50.0
100.0
Inference steps
0.300
0.325
0.350
0.375
0.400
0.425
0.450
0.475
0.500
PQ
(a)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Time difference
0.20
0.25
0.30
0.35
0.40
0.45
0.50
PQ
(b)
0.0
10.0
20.0
40.0
80.0
160.0
320.0
Minimum pixels
0.40
0.42
0.44
0.46
0.48
0.50
0.52
PQ
(c)
Figure 6. Inference ablations on MS-COCO.
0
10
20
30
Steps for the 1st frame
0.62
0.63
0.64
0.65
0.66
0.67
0.68
0.69
0.70
Mean J&F
0
10
20
30
Steps for subsequent frames
0.62
0.63
0.64
0.65
0.66
0.67
0.68
0.69
0.70
Figure 7. Effect of inference steps on DAVIS. Left: we vary the
number of steps for the 1st frame while using a ﬁxed of 8 steps
for the remaining frames. Right: we use 8 steps for the 1st frame,
while varying the number of steps for the remaining frames. The
ﬁrst frame requires more inference steps due to the cold start.
4.4. Ablations on Inference
Figure 6 explores the dependence of model performance
on hyper-parameter choices during inference (sampling),
namely, the number of inference steps, time differences and
threshold on the minimum size of instance regions, all on
MS-COCO. Speciﬁcally, Fig. 6a shows that inference steps
of 20 seems sufﬁcient for near optimal performance on MS-
COCO. Fig. 6b shows that the td parameter as in asymmet-
ric time intervals [12] has a signiﬁcant impact, with inter-
mediate values (e.g., 2-3) yielding the best results. Fig. 6c
shows that the right choice of threshold on small instances
leads to small performance gains.
Figure 7 shows how performance varies with the number
of inference steps for the ﬁrst frame, and for the remain-
ing frames, on DAVIS video dataset. We ﬁnd that more
inference steps are helpful for the ﬁrst frame compared to
subsequent frames of video. Therefore, we can reduce the
total number of steps by using more steps for the ﬁrst frames
(e.g., 32), and fewer steps for subsequent frames (e.g., 8).
It is also worth noting that even using 8 steps for the ﬁrst
frame and only 1 step for each subsequent frame, the model
still achieves an impressive J &F of 67.3.
4.5. Case study
Figure 8, 9 and 10 show example results of Pix2Seq-D
on MS-COCO, Cityscape, and DAVIS. One can see that our
model is capable of capturing small objects in dense scene
well. More visuslizations are shown in Appnedix D.
5. Related Work
Image Panoptic Segmentation.
Panoptic segmentation,
introduced in [30], uniﬁes semantic segmentation and in-
stance segmentation. Previous approaches to panoptic seg-
mentation involve pipelines with multiple stages, such as
object detection, semantic and instance segmentation, and
the fusion of separate predictions [7, 14, 30, 34, 40, 65].
With multiple stages involved, learning is often not end-to-
end. Recent work has proposed end-to-end approaches with
Transformer based architectures [16, 17, 33, 35, 58, 67–69],
for which the model directly predicts segmentation masks
and optimizes based on a bipartite graph matching loss.
Nevertheless, they still require customized architectures
(e.g., per instance mask generation, and mask fusion mod-
ule). Their loss functions are also specialized for optimizing
metrics used in object matching.
Our approach is a signiﬁcant departure to existing meth-
ods with task-speciﬁc designs, as we simply treat the task
as image-conditional discrete mask generation, without re-
liance on inductive biases of the task. This results in a sim-
pler and more generic design, one which is easily extended
to video segmentation with minimal modiﬁcations.
Video Segmentation.
Among the numerous video seg-
mentation tasks, video object segmentation (VOS) [46, 61]
is perhaps most canonical task, which entails the segmen-
tation of key objects (of unknown categories). Video in-
stance segmentation (VIS) [66] is similar to VOS, but re-
quires category prediction of instances.
Video panoptic
segmentation (VPS) [28] is a direct extension of image
panoptic segmentation to the video domain. All video seg-
mentation tasks involve two main challenges, namely, seg-
mentation and object tracking. And like image segmen-
tation, most existing methods are specialist models com-
prising multiple stages with pipe-lined frameworks, e.g.,
track-detect [6, 36, 44, 57, 66], clip-match [3, 5], propose-
reduce [37]. End-to-end approaches have also been pro-
posed recently [29,62], but with a specialized loss function.
In this work we directly take the Pix2Seq-D model,
pretrained on COCO for panoptic segmentation, and ﬁne-
tune it for unsupervised video object segmentation (UVOS),
where it performs VOS without the need for manual ini-
tialization. Model architectures, training losses, input aug-
mentations and sampling methods all remained largely un-
changed when applied to UVOS data. Because of this, we
believe it is just as straightforward to apply Pix2Seq-D to
the other video segmentation tasks as well.
Others.
Our work is also related to recent generalist vi-
sion models [10,11,31,43,60] where both architecture and
loss functions are task-agnostic. Existing generalist models
7

Image
Prediction
Groundtruth
Figure 8. Predictions on MS-COCO val set.
Image
Prediction
Groundtruth
Figure 9. Predictions on Cityscapes val set.
Figure 10. Predictions on DAVIS val set.
are based on autoregressive models, while our work is based
on Bit Diffusion [12,23,50,51]. Diffusion models have been
applied to semantic segmentation, directly [1,27,63] or in-
directly [2, 4]. However none of these methods model seg-
mentation masks as discrete/categorical tokens, nor are their
models capable of video segmentation.
8

6. Conclusion and Future Work
This paper proposes a novel generalist framework for
panoptic segmentation of images and videos, based on con-
ditional generative models of discrete panoptic masks. By
leveraging the powerful Bit Diffusion model, we are able
to model large number of discrete tokens (106 in our ex-
periments), which is difﬁcult with existing generalist mod-
els. We believe both the architecture, modeling choices, and
training procedure (including augmentations) we use here
can be further improved to boost the performance. Fur-
thermore, the required inference steps can also be further
reduced with techniques like progressive distillation.
Acknowledgements
We specially thank Liang-Chieh Chen for the helpful
feedback on our initial draft.
References
[1] Tomer Amit, Eliya Nachmani, Tal Shaharbany, and Lior
Wolf. Segdiff: Image segmentation with diffusion proba-
bilistic models. arXiv preprint arXiv:2112.00390, 2021. 8
[2] Emmanuel Brempong Asiedu, Simon Kornblith, Ting Chen,
Niki Parmar, Matthias Minderer, and Mohammad Norouzi.
Decoder denoising pretraining for semantic segmentation. In
Computer Vision and Pattern Recognition workshop, 2022. 8
[3] Ali Athar, Sabarinath Mahadevan, Aljosa Osep, Laura Leal-
Taix´e, and Bastian Leibe. Stem-seg: Spatio-temporal em-
beddings for instance segmentation in videos. In European
Conference on Computer Vision, pages 158–177. Springer,
2020. 6, 7
[4] Dmitry
Baranchuk,
Ivan
Rubachev,
Andrey
Voynov,
Valentin Khrulkov, and Artem Babenko. Label-efﬁcient se-
mantic segmentation with diffusion models. arXiv preprint
arXiv:2112.03126, 2021. 8
[5] Gedas Bertasius and Lorenzo Torresani. Classifying, seg-
menting, and tracking object instances in video with mask
propagation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9739–
9748, 2020. 7
[6] Jiale Cao, Rao Muhammad Anwer, Hisham Cholakkal, Fa-
had Shahbaz Khan, Yanwei Pang, and Ling Shao. Sipmask:
Spatial information preservation for fast image and video in-
stance segmentation. In European Conference on Computer
Vision, pages 1–18. Springer, 2020. 7
[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision, pages 213–229. Springer, 2020. 1,
2, 6, 7
[8] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan
Adeli, Yan Wang, Le Lu, Alan L. Yuille, and Yuyin Zhou.
Transunet: Transformers make strong encoders for medi-
cal image segmentation. arXiv preprint arXiv:2102.04306,
2021. 3, 5
[9] Liang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scal-
ing wide residual networks for panoptic segmentation.
arXiv:2011.11675, 2020. 12
[10] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A language modeling framework for
object detection. arXiv preprint arXiv:2109.10852, 2021. 1,
2, 5, 7
[11] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J
Fleet, and Geoffrey Hinton. A uniﬁed sequence interface for
vision tasks. arXiv preprint arXiv:2206.07669, 2022. 1, 2, 7
[12] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog
bits: Generating discrete data using diffusion models with
self-conditioning. arXiv preprint arXiv:2208.04202, 2022.
1, 2, 3, 4, 6, 7, 8, 12
[13] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexan-
der Kirillov, Rohit Girdhar, and Alexander G Schwing.
Mask2former for video instance segmentation.
arXiv
preprint arXiv:2112.10764, 2021. 6
[14] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,
Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.
Panoptic-deeplab:
A simple, strong, and fast baseline
for bottom-up panoptic segmentation.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 12475–12485, 2020. 1, 7
[15] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,
Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.
Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for
Bottom-Up Panoptic Segmentation. In CVPR, 2020. 12
[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar.
Masked-attention mask
transformer for universal image segmentation. CVPR, 2022.
1, 6, 7, 12
[17] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classiﬁcation is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems,
34:17864–17875, 2021. 1, 6, 7
[18] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In CVPR, 2017. 12
[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld,
Markus Enzweiler,
Rodrigo Benenson,
Uwe
Franke, Stefan Roth, and Bernt Schiele.
The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 3213–3223, 2016. 5
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 2
[21] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-
Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple
copy-paste is a strong data augmentation method for instance
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2918–
2928, 2021. 5
9

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 2, 5, 12
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840–6851, 2020. 2, 3, 8
[24] Jeremy Howard.
Training imagenet in 3 hours for 25
minutes. https://www.fast.ai/2018/04/30/dawnbench-fastai/,
2018. 5
[25] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning, pages 4651–4664. PMLR, 2021.
2
[26] Salman
Khan,
Muzammal
Naseer,
Munawar
Hayat,
Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak
Shah. Transformers in vision: A survey. ACM computing
surveys (CSUR), 54(10s):1–41, 2022. 2
[27] Boah Kim, Yujin Oh, and Jong Chul Ye. Diffusion adver-
sarial representation learning for self-supervised vessel seg-
mentation. arXiv preprint arXiv:2209.14566, 2022. 8
[28] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So
Kweon. Video panoptic segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 9859–9868, 2020. 7
[29] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu,
Hong-Seok Kim, Hartwig Adam, In So Kweon, and Liang-
Chieh Chen. Tubeformer-deeplab: Video mask transformer.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 13914–13924, 2022.
7
[30] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll´ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9404–9413, 2019. 1, 2, 7
[31] Alexander Kolesnikov, Andr´e Susano Pinto, Lucas Beyer,
Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby. Uvim:
A uniﬁed modeling approach for vision with learned guiding
codes. arXiv preprint arXiv:2205.10337, 2022. 1, 5, 6, 7
[32] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-
augmented self-supervised tracker.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020. 6
[33] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni,
Heung-Yeung Shum, et al. Mask dino: Towards a uniﬁed
transformer-based framework for object detection and seg-
mentation. arXiv preprint arXiv:2206.02777, 2022. 1, 6, 7
[34] Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan
Huang, Dalong Du, and Xingang Wang. Attention-guided
uniﬁed network for panoptic segmentation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 7026–7035, 2019. 1, 7
[35] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima
Anandkumar, Jose M Alvarez, Ping Luo, and Tong Lu.
Panoptic segformer: Delving deeper into panoptic segmen-
tation with transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1280–1289, 2022. 1, 6, 7
[36] Chung-Ching Lin, Ying Hung, Rogerio Feris, and Linglin
He. Video instance segmentation tracking with a modiﬁed
vae architecture. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
13147–13157, 2020. 7
[37] Huaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Ji-
aya Jia. Video instance segmentation with a propose-reduce
paradigm. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pages 1739–1748,
October 2021. 6, 7
[38] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
Feature pyra-
mid networks for object detection.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 2
[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision, pages 740–755.
Springer, 2014. 5
[40] Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu
Liu, Gang Yu, and Wei Jiang. An end-to-end network for
panoptic segmentation.
In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pages 6172–6181, 2019. 1, 7
[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012–10022, 2021. 2, 12
[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. CVPR, 2022. 12
[43] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi.
Uniﬁed-io: A uniﬁed
model for vision, language, and multi-modal tasks. arXiv
preprint arXiv:2206.08916v2, 2022. 1, 7
[44] Jonathon Luiten, Idil Esen Zulﬁkar, and Bastian Leibe. Un-
ovost: Unsupervised ofﬂine video object segmentation and
tracking. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision (WACV), March
2020. 6, 7
[45] Alex Nichol and Prafulla Dhariwal. Improved denoising dif-
fusion probabilistic models. arXiv 2102.09672, 2021. 2, 3
[46] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc
Van Gool, Markus Gross, and Alexander Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video
object segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 724–732,
2016. 5, 6, 7
[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015. 2
10

[48] Chitwan Saharia, William Chan, Huiwen Chang, Chris A
Lee, Jonathan Ho, Tim Salimans, David J Fleet, and Mo-
hammad Norouzi. Palette: Image-to-image diffusion mod-
els. SIGGRAPH, 2022. 3
[49] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A
large-scale, high-quality dataset for object detection. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision, pages 8430–8439, 2019. 5
[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, pages 2256–2265. PMLR, 2015.
2, 8
[51] Jiaming
Song,
Chenlin
Meng,
and
Stefano
Ermon.
Denoising diffusion implicit models.
arXiv preprint
arXiv:2010.02502, 2020. 2, 5, 8
[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations, 2021. 2
[53] Mingxing Tan and Quoc Le. Efﬁcientnetv2: Smaller models
and faster training. In International Conference on Machine
Learning, pages 10096–10106. PMLR, 2021. 5
[54] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv´e
J´egou. Fixing the train-test resolution discrepancy. Advances
in neural information processing systems, 32, 2019. 5
[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 1, 2, 5
[56] Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Sal-
vador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: End-
to-end recurrent network for video object segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 5277–5286, 2019. 6
[57] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon
Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger,
and Bastian Leibe. Mots: Multi-object tracking and segmen-
tation. In Proceedings of the ieee/cvf conference on computer
vision and pattern recognition, pages 7942–7951, 2019. 7
[58] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and
Liang-Chieh Chen.
Max-deeplab:
End-to-end panoptic
segmentation with mask transformers.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 5463–5474, 2021. 1, 7, 12
[59] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,
Alan Yuille, and Liang-Chieh Chen. Axial-DeepLab: Stand-
Alone Axial-Attention for Panoptic Segmentation. In ECCV,
2020. 12
[60] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Unifying architectures, tasks, and modalities
through a simple sequence-to-sequence learning framework.
arXiv preprint arXiv:2202.03052, 2022. 1, 7
[61] Wenguan Wang, Tianfei Zhou, Fatih Porikli, David Crandall,
and Luc Van Gool. A survey on deep learning technique for
video segmentation. arXiv preprint arXiv:2107.01153, 2021.
7
[62] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,
Baoshan Cheng, Hao Shen, and Huaxia Xia.
End-to-end
video instance segmentation with transformers. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 8741–8750, 2021. 7
[63] Julia Wolleb, Robin Sandk¨uhler, Florentin Bieder, Philippe
Valmaggia, and Philippe C Cattin.
Diffusion models for
implicit image segmentation ensembles.
arXiv preprint
arXiv:2112.03145, 2021. 8
[64] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen
Lo, and Ross Girshick. Detectron2. https://github.
com/facebookresearch/detectron2, 2019. 5
[65] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu,
Min Bai, Ersin Yumer, and Raquel Urtasun.
Upsnet: A
uniﬁed panoptic segmentation network. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8818–8826, 2019. 1, 7
[66] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 5188–5197, 2019. 7
[67] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao,
Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,
and Liang-Chieh Chen.
Cmt-deeplab:
Clustering mask
transformers for panoptic segmentation. In CVPR, 2022. 1,
6, 7, 12
[68] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,
Yukun Zhu, Hatwig Adam, Alan Yuille, and Liang-Chieh
Chen. k-means mask transformer, 2022. 1, 6, 7, 12
[69] Wenwei
Zhang,
Jiangmiao
Pang,
Kai
Chen,
and
Chen Change Loy.
K-net: Towards uniﬁed image seg-
mentation.
Advances in Neural Information Processing
Systems, 34:10326–10338, 2021. 1, 6, 7
[70] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan ¨O
Arik, and Tomas Pﬁster. Nested hierarchical transformer:
Towards accurate, data-efﬁcient and interpretable visual un-
derstanding. In Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence, volume 36, pages 3417–3425, 2022. 2
11

A. Algorithm details
For completeness, we include Algorithm 3 and 4 from
Bit Diffusion [12] to provide more detailed implementa-
tions of functions in Algorithm 1 and 2.
Algorithm 3 Binary encoding and decoding algorithms (in Ten-
sorﬂow).
import tensorflow as tf
def int2bit(x, n=8):
# Convert integers into corresponding binary bits.
x = tf.bitwise.right_shift(
tf.expand_dims(x, -1), tf.range(n))
x = tf.math.mod(x, 2)
return x
def bit2int(x):
# Convert binary bits into corresponding integers.
x = tf.cast(x, tf.int32)
n = x.shape[-1]
x = tf.math.reduce_sum(x * (2 ** tf.range(n)), -1)
return x
Algorithm 4 xt estimation with DDIM updating rule.
def gamma(t, ns=0.0002, ds=0.00025):
# A scheduling function based on cosine function.
return numpy.cos(
((t + ns) / (1 + ds)) * numpy.pi / 2)**2
def ddim_step(x_t, x_pred, t_now, t_next):
# Estimate x at t_next with DDIM updating rule.
γnow = gamma(t_now)
γnext = gamma(t_next)
x_pred = clip(x_pred, -scale, scale)
eps =
1
√1−γnow
* (x_t - √γnow * x_pred)
x_next = √γnext * x_pred + √1 −γnext * eps
return x_next
B. More details on training and inference
hyper-parameters
MS-COCO. For unconditional pretraining of the mask
decoder, we train the model on mask resolution 128×128
for 800 epochs on MS-COCO with a batch size of 512 and
scale jittering of strength [1.0, 3.0]. For both unconditional
pretraining of the mask decoder, and image-conditional
training of mask generation (encoder and decoder), we use
input scaling of 0.1, loss weighting p = 0.2, learning rate
of 1e−4, with EMA decay of 0.999.
Cityscapes. For ﬁne-tuning on Cityscapes, we train for
800 epochs using a batch size of 16 and learning rate of
3e−5 linearly decayed to 3e−6 with no warmup and no
scale jittering augmentation.
Image size of 1024×2048
with mask size of 512×1024 are used. During eval, we use
td= 1.5 and resize the predicted mask to the full mask size
of 1024 × 2048 using nearest neighbour resizing and ﬁlter
out annotations with less than 80 pixels.
DAVIS. For ﬁne-tuning on DAVIS, we train for 20k steps
with batch size of 32, loss weighting of p = 0.2, a constant
method
backbone
params
PQ
Specialist approaches:
Mask2Former [16]
ResNet-50 [22]
-
62.1
kMaX-DeepLab [68]
ResNet-50 [22]
56M
64.3
Panoptic-DeepLab [15]
Xception-71 [18]
47M
63.0
Axial-DeepLab [59]
Axial-ResNet-L [59]
45M
63.9
Axial-DeepLab [59]
Axial-ResNet-XL [59]
173M
64.4
CMT-DeepLab [67]
MaX-S [58]
-
64.6
Panoptic-DeepLab [15]
SWideRNet-(1,1,4.5) [9]
536M
66.4
Mask2Former [16]
Swin-B (W12) [41]
-
66.1
Mask2Former [16]
Swin-L (W12) [41]
-
66.6
kMaX-DeepLab [68]
MaX-S [58]
74M
66.4
kMaX-DeepLab [68]
ConvNeXt-B [42]
121M
68.0
kMaX-DeepLab [68]
ConvNeXt-L [42]
232M
68.4
Generalist approaches:
Pix2Seq-D (steps=10)
ResNet-50 [22]
94.8M
62.2
Pix2Seq-D (steps=20)
ResNet-50 [22]
94.8M
63.2
Pix2Seq-D (steps=40)
ResNet-50 [22]
94.8M
63.4
Pix2Seq-D (steps=80)
ResNet-50 [22]
94.8M
64.0
Table 6. Cityscapes val set results.
Mask size
Image size
256 × 512
512 × 1024
512 × 1024
52.1
53.8
1024 × 2048
55.7
59.9
Table 7. PQ for various image sizes and panoptic mask sizes for
Cityscapes val set. Model is trained for 100 epochs for ablation.
learning rate of 1e−5, EMA decay of 0.99, and scale jitter-
ing of strength [0.7, 2]. Image size of 512×1024 and mask
size of 256×512 are used. For evaluation, as the dataset is
quite small, we run inference 50 times for our model, and
report the mean (the standard deviation for J &F is around
1.5).
C. Results on Cityscape
Table 6 compares our results on Cityscapes val set with
prior work.
Our main results are with an image size of
1024 × 2048 and mask size of 512 × 1024. In Table 7 we
show an ablation with varying image and mask sizes and we
ﬁnd that both a larger image size and a larger mask size are
important.
D. Extra Visualization
Figure 11 shows the inference trajectory of or model for
two MS-COCO validation examples, we see that the model
iteratively reﬁnes the panoptic mask outputs so that they be-
come globally and locally consistent.
Figure 12 and 13 present more visualization of our
model’s predictions on MS-COCO validation set. Figure 14
and 15 present extra visualization of our model’s predic-
tions on Cityscapes and DAVIS validation sets, respectively.
12

Figure 11. Inference trajectory. Predicted m0 at different time steps (1, 2, 4, 8, 16) out of total 20 steps.
Image
Prediction
Groundtruth
Figure 12. Predictions on MS-COCO val set.
13

Image
Prediction
Groundtruth
Figure 13. Predictions on MS-COCO val set.
14

Image
Prediction
Groundtruth
Figure 14. Predictions on Cityscapes val set.
Figure 15. Predictions on DAVIS val set.
15

