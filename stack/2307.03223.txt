arXiv:2307.03223v2  [hep-th]  13 Dec 2023
Neural Network Field Theories:
Non-Gaussianity, Actions, and Locality
Mehmet Demirtas1,2i, James Halverson1,2ii, Anindita Maiti1,2,4iii,
Matthew D. Schwartz1,3 iv, and Keegan Stoner1,2v
1The NSF AI Institute for Artiﬁcial Intelligence and Fundamental Interactions
2Department of Physics, Northeastern University,
Boston, MA 02115 USA
3Department of Physics, Harvard University,
Cambridge, MA 02138 USA
4School of Engineering and Applied Sciences, Harvard University,
Cambridge, MA 02138 USA
Both the path integral measure in ﬁeld theory and ensembles of neural networks describe
distributions over functions. When the central limit theorem can be applied in the inﬁnite-
width (inﬁnite-N) limit, the ensemble of networks corresponds to a free ﬁeld theory. Al-
though an expansion in 1/N corresponds to interactions in the ﬁeld theory, others, such as
in a small breaking of the statistical independence of network parameters, can also lead to
interacting theories. These other expansions can be advantageous over the 1/N-expansion,
for example by improved behavior with respect to the universal approximation theorem.
Given the connected correlators of a ﬁeld theory, one can systematically reconstruct the ac-
tion order-by-order in the expansion parameter, using a new Feynman diagram prescription
whose vertices are the connected correlators. This method is motivated by the Edgeworth
expansion and allows one to derive actions for neural network ﬁeld theories. Conversely,
the correspondence allows one to engineer architectures realizing a given ﬁeld theory by
representing action deformations as deformations of neural network parameter densities.
As an example, φ4 theory is realized as an inﬁnite-N neural network ﬁeld theory.
Keywords: Neural Network ﬁeld theory correspondence; Feynman rules for Neural Network
ﬁeld theories; non-perturbative ﬁeld theories via Neural Networks.
im.demirtas@northeastern.edu
iij.halverson@northeastern.edu
iiiamaiti@perimeterinstitute.ca
ivschwartz@g.harvard.edu
vstoner.ke@northeastern.edu

Contents
1. Introduction
2
1.1. NN-FT Correspondence
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
1.2. Organizing Principles and Related Work
.
.
.
.
.
.
.
.
.
.
.
.
.
6
1.3. Summary of Results and Paper Organization
.
.
.
.
.
.
.
.
.
.
.
10
2. Connected Correlators and the Central Limit Theorem
11
2.1. Review: Central Limit Theorem from Generating Functions
.
.
.
.
.
.
13
2.2. Non-Gaussianity from Independence Breaking
.
.
.
.
.
.
.
.
.
.
.
15
2.3. Connected Correlators in NN-FT
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.3.1.
Finite-N Corrections with Independent Neurons .
.
.
.
.
.
.
.
19
2.3.2.
Generalized Connected Correlators from Independence Breaking .
.
21
3. Computing Actions from Connected Correlators
23
3.1. Field Density from Connected Correlators: Edgeworth Expansion
.
.
.
24
3.2. Computing the Action with Feynman Diagrams
.
.
.
.
.
.
.
.
.
.
27
3.2.1.
Example: Non-local φ4 Theory.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30
3.3. General Interacting Actions in NN-FT
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
3.4. Example Actions in NN-FT
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
4. Engineering Actions: Generalities, Locality, and φ4 Theory
36
4.1. Non-Gaussian Deformation of a Neural Network Gaussian Process
.
.
.
39
4.2. φ4 Theory as a Neural Network Field Theory
.
.
.
.
.
.
.
.
.
.
.
41
4.3. Cluster Decomposition and Space Independence Breaking
.
.
.
.
.
.
42
5. Conclusions
48
Appendix A. Continuum Hermite Polynomials
49
Appendix B. Details of Examples
50
Appendix C. CGF and Edgeworth Expansion for NNFT
54
Appendix D. Fourier Transformation Trick for G(2)
c (x, y)−1
59
Appendix E. Gaussian Processes: Locality and Translation Invariance
60
References
61
1

1. Introduction
The last decade has seen remarkable progress in machine learning (ML) in a wide variety
of ﬁelds, including traditional ML ﬁelds such as natural language processing, image recog-
nition, and gameplay (see [1] for reviews, and [2] for some breakthroughs in the literature),
but also in the physical sciences [3], and more recently to obtain rigorous results in pure
mathematics [4]. This progress has been facilitated in part by the increasing complexity of
deep neural networks, both in terms of the number of parameters appearing in them and
their architecture. However, despite their empirical success, the theoretical foundations of
deep neural networks are still not fully understood. Natural questions emerge:
• Are ideas from the sciences, such as physics, useful in neural network theory?
• As it develops, does ML theory lead to progress in the sciences?
A growing literature (see below), gives an aﬃrmative answer to the ﬁrst, but the second is
less clear; it is applied ML, not theoretical ML, that is primarily used in the sciences.
In this paper we explore both of these questions by further developing a correspondence
between neural networks and ﬁeld theory. This connection was already implicit in Neal’s
Ph.D. thesis [5] in the 1990’s, where he demonstrated that an inﬁnite width single-layer
neural network is (under appropriate assumptions) a draw from a Gaussian process (GP).
This is the so-called neural network / Gaussian process (NNGP) correspondence, and in
recent years it has been shown that most modern NN architectures [6,7] have a parameter
N such that the NN is drawn from a GP in the N →∞limit. The NNGP correspondence
is of interest from a physics perspective because Gaussian processes are generalized non-
interacting (free) ﬁeld theories, and neural networks provide a novel way to realize them.
Non-Gaussianities emerge at ﬁnite-N, which correspond to turning on interactions that
are generally non-local, and may be captured by statistical cumulant functions, known as
connected correlators in physics. As we will see, since Gaussianity in the N →∞limit
emerges by the Central Limit Theorem (CLT), non-Gaussianities may be studied more
generally by parametrically violating necessary conditions of the CLT.
These results provide a ﬁrst glimpse that there is a more general NN-FT correspondence
that should be developed in its own right, taking inspiration from both physics and ML.
In this introduction we will review the central ideas of the correspondence and introduce
principles for understanding the literature, which we review in part. Readers familiar with
the background are directed to Section 1.3 for a summary of our results.
2

NN
FT
Fig. 1: In a NN-FT correspondence, ideas from one may give insights into the other. In
this paper we are primarily interested in understanding when neural network ﬁeld theories
exhibit physical principles such as non-Gaussianity and locality, with an eye towards appli-
cations in both ML and especially physics in the future.
1.1. NN-FT Correspondence
At ﬁrst glance, neural networks and ﬁeld theories seem very diﬀerent from one another.
However, in both cases, the central objects of study are random functions. The random
function φ associated to a neural network is deﬁned by its architecture, which is a composi-
tion of simpler functions that involves parameters θ. At program initialization, parameters
are drawn as θ ∼P(θ), yielding a randomly initialized neural network, i.e.
a random
function. In ﬁeld theory, the random functions are simply the ﬁelds themselves, typically
described by specifying their probability density function directly, P(φ) = exp(−S[φ]), via
the Euclidean action functional S[φ]; we work in Euclidean signature throughout.
We therefore have two diﬀerent origins for the statistics of a ﬁeld theory. To exemplify
the point, consider a ﬁeld theory deﬁned by an ensemble of networks or ﬁelds φ : R →R,
φ(x) = a σ(b σ(c x))
a ∼P(a), b ∼P(b), c ∼P(c),
(1.1)
where σ : R →R acts element-wise and is generally taken to be non-linear.
Here the
statistics of the ensemble arise from how it is constructed, rather than from the density
exp(−S[φ]) over functions from which it is drawn. We will refer to such a description as
the parameter space description of a neural network ﬁeld theory.
The construction of φ
deﬁned in (1.1) has two parts, the architecture that deﬁnes its functional form, and the
choice of distributions from which the parameters a, b, and c are drawn. This particular
architecture is a feedforward network with depth two, width one, and activation function σ.
In this description of the ﬁeld theory, one does not necessarily know the action S[φ], but
the theory may nevertheless be studied because the architecture and parameter densities
deﬁne its statistics.
For instance, the correlation functions of a neural network ﬁeld theory can be expressed
3

as
G(n)(x1, . . . , xn) := E[φ(x1) . . . φ(xn)] =
Z
dθ P(θ) φ(x1) . . . φ(xn),
(1.2)
where we denote the set of parameters of the neural network by θ, and the network / ﬁeld φ
depends on parameters through its architecture. Alternatively, we could provide a function
space description of the theory by specifying the action S[φ] and express the correlation
functions as
G(n)(x1, . . . , xn) =
Z
Dφ e−S[φ] φ(x1) . . . φ(xn),
(1.3)
as in a ﬁrst course on quantum ﬁeld theory. These expressions may be derived from the
partition function
Z[J] = E[e
R
ddxJ(x)φ(x)],
(1.4)
where the parameter space and function space results arise by specifying how the expecta-
tion value is computed,
Z[J] =
Z
dθ P(θ) e
R ddxJ(x)φ(x)
(1.5)
Z[J] =
Z
Dφ e−S[φ]+
R
ddxJ(x)φ(x).
(1.6)
In this work, many calculations will be carried out in terms of a general expectation value
E[·] that denotes agnosticism towards the origin of the statistics; explicit calculations may
be carried out by replacing E with one description or the other, as in passing from a general
expression (1.4) to those of parameter space (1.5) and function space (1.6).
Parameter space and function space provide two diﬀerent descriptions of a ﬁeld theory,
which could be thought of as diﬀerent duality frames [8]. When one deﬁnes a ﬁeld theory by
a neural network architecture, the parameter space description is readily available, but the
action is not known, a priori. However, if the parameter distributions are easy to sample
then the ﬁelds are also easy to sample: one just initializes neural networks on the computer.
On the other hand, in ﬁeld theory we normally proceed by ﬁrst specifying an action; in
this case, the probability of a given ﬁeld conﬁguration is known because P[φ] = exp(−S[φ])
is known, but ﬁelds are notoriously hard to sample, as evidenced by the proliferation of
Monte Carlo techniques in lattice ﬁeld theory.
Example: NNGP Correspondence in Parameter Space and Function Space
Let us study an example to make the abstract notions more concrete.
Consider a
fully-connected feedforward network φ : Rd →R with depth one and width N,
φ(x) =
N
X
i=1
d
X
j=1
ai σ (bijxj),
a ∼N (0, σ2/N), b ∼N (0, σ2/d),
(1.7)
4

where σ is an elementwise non-linearity such as tanh or ReLU(z) := max(0, z). Here, the
set of parameters θ is given by the union of the a-parameters and the b-parameters. As
we will see in detail in Section 2, if the parameters are drawn independently then the
connected correlation functions
G(2k)
c
(x1, . . . , x2k) ∝
1
Nk−1,
(1.8)
and the odd-point correlation functions vanish due to a having zero mean. In the N →
∞limit, also known as the Gaussian Process (GP) limit, then, the only non-vanishing
connected correlator has two points,
G(2)
c (x1, x2)
(1.9)
which demonstrates that the theory is Gaussian; this is the NNGP correspondence. Con-
cretely, following [9], we may compute the two-point function as
G(2)(x, y) = E[φ(x)φ(y)] =
Z
da db P(a)P(b) ai1σ(bi1j1xj1) ai2σ(bi2j2yj2)
(1.10)
where we have used Einstein summation and left the details of the Gaussian parameter
densities P(a) and P(b) implicit. For a ﬁxed choice of σ one may evaluate this integral
analytically or via Monte Carlo sampling, resulting in the two-point function; analytic
integrated results for σ = tanh and σ = Erf are presented in [9]. Since the parameter space
calculation establishes Gaussianity of the theory, we infer the action
S[φ] =
Z
ddx ddy φ(x) G(2)(x, y)−1 φ(y),
(1.11)
where the inverse of the two-point function satisﬁes
R
ddy G(2)(x, y)−1G(2)(y, z) = δ(d)(x −
z).
As a concrete example, we refer the reader to Section 4.2, which recalls a neural network
realization of free scalar ﬁeld theory from [10] that uses a cos activation. In that case we
have
G(2)(x, y)−1 = δ(x −y)(∇2 + m2)
(1.12)
which reproduces the usual free scalar action
S[φ] =
Z
ddx φ(x)
 ∇2 + m2
φ(x),
(1.13)
in this case realized via a concrete neural network architecture.
Thus, in the GP limit, both the parameter space and function space descriptions of
the ﬁeld theory are readily available. Building on [11] using the Edgeworth expansion, we
will see methods for computing approximate actions at ﬁnite-N, and we will also develop
techniques to engineer desired actions.
5

1.2. Organizing Principles and Related Work
We have discussed a foundational principle underlying the NN-FT correspondence, that
parameter space and function space provide two diﬀerent descriptions of the statistics of
an ensemble of neural networks or ﬁelds. Though we have given an example, and there are
many more, we are still in very general territory and it is not clear where to go. Accordingly,
we would like to provide other organizing principles:
• NN-for-FT vs. FT-for-NN: are we aiming to better understand physics or ML?
• Fixed Initialization vs. Learning: are we aiming to understand a ﬁxed NN-FT at
initialization, or a one-parameter family of NN-FTs deﬁned by some dynamics, such
as ML training dynamics or ﬁeld theory ﬂows?
Much of the existing literature can be classiﬁed within each of these principles, and they
also set context for discussing our results. We will ﬁrst review some results for network
ensembles at initialization, and then during and after training. With these ideas in place,
we will turn to the idea of using NN-FT in service of ﬁeld theory.
For literature that is most similar in perspective to this introduction (prior to this
reference section), see [10] and the works that preceded it [8,12], by subsets of the authors.
Initialization. A neural network with parameters θ and parameter distribution P(θ) is
initialized on a computer by drawing θ ∼P(θ) and inserting them into the architecture,
generating a random function φ(x) that is sampled from a distribution P(φ) that may or
may not be known. In the N →∞NNGP limit, P(φ) is Gaussian. This was shown for
feed forward networks in Neal’s thesis [5], as well as more recently in [6]; was generalized
to a plethora of architectures, e.g. convolutional layers [7, 13, 14], recurrent layers, graph
convolutions [15], skip connections [16], attention [17], and batch /layer normalization in
[18], pooling [14], and transformers [19, 20]. The generality of this result arises from the
generality in which central limit theorem behavior manifests itself in neural networks; see [7]
for a systematic treatment in the tensor programs formalism.
Since Gaussianity follows from the central limit theorem, one generally expects non-
Gaussianities in the form of 1/N-corrections. Study of these non-Gaussianities was initiated
a few years ago; e.g., [21] computed leading non-Gaussianities via the connected four-
point function, [22] showed for deep feedforward networks how P(φ) is perturbed by 1/N-
corrections, [12] proposed using eﬀective ﬁeld theory to model non-Gaussian P(φ) for neural
networks, and [23] developed an eﬀective theory approach and an L/N expansion that
6

controls feature learning in deep feedforward networks; for concreteness in our examples,
we are interested in the distribution of networks at initialization and take L = 1. This
L/N expansion allowed [23] to also study signal propagation through the network, identify
universality classes, and tune hyperparameters to criticality.
Methods borrowed from ﬁeld theory have been useful in studying NNs at initialization.
For example, perturbative methods like Feynman diagrams were employed in [12, 24, 25].
Various schemes for renormalization group ﬂow, including non-perturbative ones, were ap-
plied to NNs in [26]. Global symmetries of NN-FTs were shown to arise from symmetry
invariances of NN parameter distributions in [8]. While the results of this paper were being
ﬁnalized, a recent paper [27] brought forward a diﬀerent diagrammatic approach to eﬀective
ﬁeld theories in deep feedforward networks.
Learning. Although we do not study the dynamics of learning in this paper, it is a goal
for future work. Therefore, we would like to review some of the literature.
Neural networks may be trained to perform useful tasks via a variety of learning schemes,
such as supervised learning or reinforcement learning, that utilizes a learning algorithm to
update the system, such as stochastic gradient descent. In practice this involves training one
or a handful of randomly initialized neural networks to convergence. However, in general
there is nothing special about the initial networks that were trained; in the absence of
compute limitations, one would prefer to train all the networks and compute an ensemble
average at convergence. Theoretically, this amounts to tracking the distributional ﬂow of
the neural network ensemble, and in principle it may be done in either parameter space or
function space.
In the N →∞limit, most known architectures deﬁne neural networks that are draws
from Gaussian processes. Since the architecture deﬁnes a GP, it could be used as a prior
in Bayesian inference, the learning algorithm of interest in Neal’s original work [5]. On
the other hand, gradient descent with continuous time is governed by the neural tangent
kernel (NTK) [28], which becomes deterministic and training time t-independent in the
so-called frozen-NTK limit. In this limit, N →∞and the neural network dynamics is
well-approximated by that of a model that is linear in the neural network parameters.
This frozen behavior is a vast simpliﬁcation of the dynamics and is known to exist for
many architectures, such as convolutional neural networks [29], graph neural networks [30],
recurrent networks [31], and attention layers [19]. For supervised learning with MSE loss,
the neural network ensemble trained under gradient descent remains a GP for all times t,
including t →∞, with known mean and covariance; the dynamics becomes that of kernel
regression, with kernel given by the frozen-NTK. How is this related to Neal’s desire to
7

relate Bayesian inference and trained neural networks? If all but the last layer’s weights
are frozen, then the NTK is the NNGP kernel and the distribution of the neural network
ensemble converges to the GP Bayesian posterior as t →∞.
In summary, in the N →∞limit, the distribution of the neural network ensemble is
Gaussian. If it undergoes supervised training with MSE loss, it remains Gaussian at all
times and converges to the Bayesian GP posterior in a particular case [32]. In general,
however, gradient descent induces non-Gaussianities.
At ﬁnite-N, the neural network ensemble is non-Gaussian. In the Bayesian context, this
deﬁnes a non-Gaussian prior, and inference may be performed for weakly non-Gaussian pri-
ors via a 1/N-expansion [21]. In the gradient descent context, the NTK is no longer frozen
and evolves during training, signiﬁcantly complicating the dynamics.
Work by Roberts,
Yaida, and Hanin develops a theory of an evolving NTK in [23]. They apply it in detail
to fully-connected networks of depth L, demonstrate the relevance of L/N as an expansion
parameter, and develop an eﬀective model for the dynamics. Such 1/N corrections to dy-
namical NTK were previously studied by other authors in [24, 33]. Pehlevan et. al. have
developed a systematic understanding of the evolution of NTK and parametric interpola-
tions between rich and lazy training regimes using the framework of dynamical mean ﬁeld
theory, see [34]. Some of these authors have studied the O(1/N) suppressed corrections
to training dynamics of ﬁnite width Bayesian NNs in [35]. A separate work, [36], presents
close-to-Gaussian NN processes including stationary Bayesian posteriors in the joint limit
of large width and large data set, using 1/N as an expansion parameter. Moreover, the
authors of [37] explore a correspondence between learning dynamics in the continuous time
limit and early Universe cosmology, and [38] analyzes connected correlation functions prop-
agating through neural networks.
NN-for-FT.
Neural networks, including the ones we have discussed thus far, generally have Rn as
their domain and therefore naturally live in Euclidean signature. They deﬁne statistical
ﬁeld theories that may or may not have analytic continuations to quantum ﬁeld theories
in Lorentzian signature. Nevertheless, statistical ﬁeld theories are interesting in their own
right and NN-FT provides a novel way to study them.
Using an architecture to deﬁne a ﬁeld theory enables a parameter space description
that makes sampling, and therefore numerical simulation on a lattice, easy.
If one can
determine an easily sampled NN architecture that engineers standard Euclidean φ4 theory,
for instance, this could lead to improved results on the lattice by avoiding Monte Carlo
8

entirely 1 This is an engineering problem that is work-in-progress; it is not clear that the φ4
NN-FT realization in this work is easily sampled. Alternatively, by simply ﬁxing an easily
sampled architecture with interesting physical properties such as symmetries and strong
coupling, lattice simulation could be performed immediately.
For uses in fundamental and formal quantum physics, one might wish to know when a
neural network architecture deﬁnes a quantum ﬁeld theory (QFT). Since NN architectures
are usually deﬁned in Euclidean signature, we may instead ask when a Euclidean ﬁeld
theory admits an analytic continuation to Lorentzian signature that deﬁnes a QFT. The
situation is complicated by the fact that in general we do not know the action, but instead
have access to the Euclidean correlation functions, expressed in parameter space.
Fortunately, the Osterwalder-Schrader (OS) theorem [40] of axiomatic ﬁeld theory gives
necessary and suﬃcient conditions, expressed in terms of the correlators, for the existence
of a QFT after continuation. The axioms include
• Euclidean Invariance. Correlation functions must be Euclidean invariant, which be-
comes Lorentz invariance after analytic continuation. See [10] for an inﬁnite ensemble
of NN architectures realizing Euclidean invariance.
• Permutation Symmetry. Correlation functions must be invariant under permuta-
tions of their arguments, a collection of points in Euclidean space. This is automatic
in NN-FTs with scalar outputs.
• Reﬂection Positivity.
Correlation functions must satisfy a positivity condition
known as reﬂection positivity, which is necessary for unitarity and the absence of
negative-norm states in the analytically continued theory.
• Cluster Decomposition. Correlation functions must satisfy cluster decomposition,
which says that interactions must shut oﬀat inﬁnite distance.
As a condition on
connected correlators, cluster decomposition is
lim
b→∞G(n)
c (x1, . . . , xp, xp+1 + b, · · · , xn + b) →0,
(1.14)
for any value of 1 < p < n. We have assumed permutation symmetry to simplify
notation, putting the shifts into xp+1 into xn.
1This lattice approach should be contrasted with works [39] that train a normalizing ﬂow to give proposals
for the accept / reject step of MCMC.
9

These ideas were utilized in [10] to deﬁne neural network quantum ﬁeld theories: a NN-
QFT is a neural network architecture whose correlation functions satisfy the OS axioms,
and therefore deﬁnes a QFT upon analytic continuation. To date, the only known example
is a NN architecture that engineers a standard free scalar ﬁeld theory in d-dimensions,
though we improve the situation in this work by developing techniques to engineer local
Lagrangians, which automatically satisfy the OS axioms.
To make further progress on
NN-QFT in a general setting, one needs especially a deeper understanding of reﬂection
positivity and cluster decomposition in interacting NN-FTs; we study the latter.
1.3. Summary of Results and Paper Organization
Since there are a number of diﬀerent themes and concepts in this paper, we would like to
highlight some of the major conceptual results:
• Parametric Non-Gaussianity: 1/N and Independence Breaking.
Section 2 approaches interactions in NN-FT (non-Gaussianity) by parametrically break-
ing necessary conditions for the central limit theorem to hold. Violating the inﬁnite-N
limit is well studied, but we also systematically study interactions arising from the
breaking of statistical independence, and apply these ideas in examples.
• Computing Actions with Feynman Diagrams.
In Section 3 we develop a general ﬁeld theory technique for computing the action
diagrammatically. The coupling functions are computed with a new type of connected
Feynman diagram, whose vertices are the connected correlators. This is a swapping of
the normal role of couplings and connected correlators, which arises from a “duality”
that becomes apparent via the Edgeworth expansion. The technique is also applied to
NN-FT, including an analysis of how actions may be computed in the two regimes of
parameteric non-Gaussianity developed in Section 2, 1/N and independence breaking.
• Engineering Actions in NN-FT.
In Section 4 we develop techniques for engineering actions in NN-FT. This is to
be distinguished from the approach of Section 3: instead of ﬁxing an architecture,
computing its correlators, and then computing its action via Feynman diagrams, in
Section 4 we ﬁx a desired action and develop techniques for designing architectures
that realize the action. Adding a desired term to the action manifests itself in NN-
FT by deforming the parameter distribution, which breaks statistical independence
if it is a non-Gaussianity. Using this technique, local actions may be engineered at
inﬁnite-N.
10

• φ4 as a NN-FT.
In Section 4.2 we design an inﬁnite width neural network architecture that realizes
φ4 theory, using the techniques that we developed.
• The Importance of N →∞for Interacting Theories. In physics, interesting
theories deﬁned by a ﬁxed action S generally have a wide variety of ﬁnite action
ﬁeld conﬁgurations, which have non-zero probability density. This is potentially at
odds with the universal approximation theorem: if a single ﬁnite-action conﬁguration
cannot be realized by an architecture A, but only approximated, then any NN-FT
associated to A cannot realize the ﬁeld theory associated to S.
If the 1/N is an
expansion parameter for both non-Gaussianities and the degree of approximation, as
e.g.
with single-layer width-N networks, this simple no-go theorem suggests that
exact NN-FT engineering of well-studied theories in physics occurs most naturally at
inﬁnite-N, as we saw in the case of φ4 theory.
These are highlights of the paper. For more detailed summaries of results, we direct you
to the beginning of each section.
2. Connected Correlators and the Central Limit Theorem
Interacting ﬁeld theories with a Lagrangian description are deﬁned by non-Gaussian ﬁeld
densities exp(−S[φ]). If the non-Gaussianities are small, the theory is close to Gaussian
and weakly interacting, in which case correlation functions may be computed in perturba-
tion theory using Feynman diagrams. The non-Gaussianities are captured by the higher
connected correlation functions, which vanish in the Gaussian limit. They are known as
cumulants in the statistics literature and may be obtained from a generating functions
W[J] as
G(n)
c (x1, . . . , xn) :=

δ
δJ(x1) . . .
δ
δJ(xn)W[J]
 
J=0
,
W[J] := ln Z[J].
(2.1)
In the absence of a known Lagrangian description, connected correlators still encode the
presence of non-Gaussianities, since the theory is Gaussian if G(n)
c
= 0 for n > 2.
In this section we systematically study non-Gaussianities in NN-FT. Since the parameter
space description exists for any NN-FT, we choose to study non-Gaussianities via connected
correlators (rather than actions), which may be studied in parameter space even when the
action is unknown.
We are interested in non-Gaussianities in NN-FT for a number of
11

reasons.
In the NN-for-FT direction, it is important for understanding interactions in
the associated ﬁeld theories. Conversely, in the FT-for-NN direction, understanding non-
Gaussianities is important for capturing the statistics of ﬁnite networks and networks with
correlations in the parameter distributions, which generally develop during training.
The essential idea in our approach is to recall the origin of Gaussianity, and then para-
metrically move away from it. Speciﬁcally, many ﬁeld theories deﬁned by neural network
architectures admit an N →∞limit in which they are Gaussian, and the Gaussianity has a
statistical origin: the Central Limit Theorem (CLT). The CLT states that the distribution
of the standardized sum of N independent and identically distributed random variables
approaches a Gaussian distribution in the limit N →∞. Therefore we may systematically
study non-Gaussianities in neural network ﬁeld theories by violating assumptions of the
CLT, e.g. via 1/N corrections and breaking the independence condition, both of which
aﬀect connected correlators.
There are a number of results and themes in this section, which is organized as follows:
• Central Limit Theorem. In Section 2.1 we review the CLT from the perspective
of cumulant generating functionals, which will be useful in NN-FT since in general
we do not have a simple expression for the action but do have access to cumulants.
• Independence Breaking. In Section 2.2 we introduce how non-Gaussianities may
also arise by violating the statistical independence assumption of the CLT. We char-
acterize this by a family of joint densities with parameter α that factorize (become
independent) when α = 0. We study the α-dependence of cumulants via Taylor series,
showing that α controls non-Gaussianities independently of those arising from 1/N-
corrections. A simple example of independence-breaking induced non-Gaussianities
at N = ∞is given in Section 2.2.
• Connected Correlators and Interactions in NN-FT. In Section 2.3 we study
non-Gaussianities in NN-FT, decomposing the ﬁeld φ(x) into N constituent neurons
as in [10].
We study the case of independent neurons in Section 2.3.1, where we
present the N-scaling of connected correlators and also two examples: single-layer
Cos-net, which exhibits full Euclidean symmetry in all of its correlators, and d = 1
ReLU-net, which we show exhibits an interesting bilocal structure in its two-point
and four-point functions.
In Section 2.3.2 we turn to breaking neuron independence in NN-FT, building on
the independence breaking results of [10], which gives a new source of interactions
12

and a generalized formula for connected correlators.
Speciﬁcally, we introduce a
general formalism for the expansion of the cumulant generating functional in terms
of independence-breaking parameters, and therefore the computation of connected
correlators. As an example, we deform the Cos-net theory to have non-independent
neurons via non-independent input weights, doing the deformation in a way that
preserves Euclidean invariance, and compute the independence-breaking correction to
the connected four-point function.
• Identical-ness Breaking. Interactions may also arise from breaking the identical-
ness assumption of the CLT. See Appendix B for an example of a NN-FT with
non-Gaussianities arising from identical-ness breaking.
Equipped with two diﬀerent types of parameters that induce non-Gaussianity in connected
correlators, 1/N and independence-breaking parameters, we will see how this may be used
to approximate actions in Section 3.
2.1. Review: Central Limit Theorem from Generating Functions
In order to understand non-Gaussianities in NN-FTs, it is useful to recall essential aspects
of the Central Limit Theorem in the case of a single random variable, since they carry over
to the NN-FT case. We will do so using the language of generating functions and cumulants
(connected correlators), since we may use them to study Gaussianity and non-Gaussianity
even if the NN-FT action is unknown.
Of course, the CLT is among the most fundamental theorems of statistics.
There
are many variants of it in the literature, with diﬀerent sets of assumptions.
Here, we
will describe a particularly simple version of it and provide a proof, showing how key
assumptions come into play. For a more in depth discussion of the CLT, see e.g. [41].
Consider N random variables Xi. Assume that they are identical, independent, mean-
free, and have ﬁnite variance. The CLT states that the standardized sum
φ =
1
√
N
N
X
i=1
Xi
(2.2)
is drawn from a Gaussian distribution in the limit N →∞. In other words, even if the
Xi are sampled from complicated, non-Gaussian distributions, these details wash out and
their sum is drawn from a Gaussian distribution.
13

To see the Gaussianity in a way that may be extrapolated to NN-FT, it is useful to
introduce generating functions. The moment generating function of φ is deﬁned as
Zφ[J] := E[eJφ] = E[eJ P
i Xi/
√
N],
(2.3)
from which we can extract the moments by taking derivatives,
µφ
r := E[φr] =
 ∂
∂J
r
Zφ[J].
(2.4)
In physics language, J is the source, Zφ[J] is the partition function, and µφ
r is the rth
correlator of φ. The cumulant generating functional (CGF) of φ is the logarithm of the
moment generating functional
Wφ[J] := log E[eJφ] = log E[eJ P
i Xi/
√
N],
(2.5)
and the cumulants κφ
r are computed by taking derivatives of Wφ[J],
κφ
r :=
 ∂
∂J
r
Wφ[J].
(2.6)
A random variable is Gaussian only if its cumulants κφ
r>2 vanish. Fundamental properties
of CGFs include
WX+c[J] = cJ + WX[J],
(2.7)
WcX[J] = log E[eJ cx] = WX[cJ]
(2.8)
where c ∈R is a constant, which imply
κX+c
1
= κX
1 + c
κX+c
r>1 = κX
r>1
(2.9)
κcX
r
= cr κX
r ,
(2.10)
respectively.
We would like to see the Gaussianity of φ under CLT assumptions by computing cu-
mulants. This is possible since κr>2 = 0 is necessary for Gaussianity; conversely, we may
study non-Gaussianities in terms of non-vanishing higher cumulants. Speciﬁcally, for a sum
of independent random variables the moment generating function factorizes,
ZX1+···+XN[J] =
N
Y
i
ZXi[J].
(2.11)
14

Consequently, the CGF and the cumulants become
WX1+···+XN[J] = WX1[J] + · · · + WXN[J],
(2.12)
κX1+···+XN
r
= κX1
r
+ · · · + κXN
r
.
(2.13)
Using the identities in (2.10) we can write the cumulants of φ as
κφ
r = κX1
r
+ · · · + κXN
r
Nr/2
.
(2.14)
When the Xi are identical this simpliﬁes to
κφ
r =
κXi
r
Nr/2−1.
(2.15)
The cumulants κφ
r>2 vanish in the N →∞limit. To establish that φ is Gaussian, we also
need to show that κφ
1 and κφ
2 are ﬁnite. As the Xi are mean-free, κφ
1 = κXi
1 /
√
N = 0, while
κφ
2 = κXi
2
is ﬁnite by assumption. Thus, φ is Gaussian distributed. This is the Central
Limit Theorem, cast into the language of cumulants.
We emphasize that this result relies not only on the N →∞limit, but also on the
independence assumption (2.13).
2.2. Non-Gaussianity from Independence Breaking
We wish to study the emergence of non-Gaussianity by breaking the independence condi-
tion.
To do so, we must parameterize the breaking of statistical independence. Let p(X; α)
be a family of joint distributions on Xi parameterized by a hyperparameter α that must
be chosen in order to deﬁne the problem. We choose the family of joint distributions to
be of the form
p(X; α = 0) =
Y
i
p(Xi),
(2.16)
i.e. p(X) is independent in the α →0 limit, but α ̸= 0 in general controls the breaking of
independence. Then we obtain
Wφ[J] = log E[eJ P
i Xi/
√
N] = log
Z Y
j
dXj p(X; α) eJ P
i Xi/
√
N
(2.17)
which when expanded around α = 0 yields
Wφ[J] = log
"Y
j
Ep(X,α=0)[eJXj/
√
N] +
∞
X
k=1
αk
k!
Z Y
j
dXj eJ P
i Xi/
√
N ∂k
αp(X; α)|α=0
#
, (2.18)
15

where the ﬁrst term of the log uses independence of p(X; α = 0).
To deal with the α-dependent terms, we generalize a trick appearing regularly in ma-
chine learning, e.g. in the policy gradient theorem in reinforcement learning. There, the
fact that p ∂α log p = ∂αp allows us to write
∂αE[O] = E[O ∂α log p]
(2.19)
for any α-independent operator O. Generalizing, we deﬁne
Pk := 1
p∂k
αp,
(2.20)
and note that it satisﬁes the recursion relation
Pk+1 = P1Pk + ∂αPk,
(2.21)
which allows for eﬃcient computation. We can then write (2.18) as
Wφ[J] = log
"Y
j
Ep(X,α=0)
h
eJXj/
√
Ni
+
∞
X
k=1
αk
k! Ep(X,α=0)
h
eJ P
i Xi/
√
N Pk|α=0
i#
.
(2.22)
In the limit α →0, the Xj become independent, and we have
lim
α→0 Wφ[J] =
X
j
log Ep(Xj)
h
eJXj/
√
Ni
=
X
j
lim
α→0 WXj/
√
N[J],
(2.23)
where φ is now a sum of N independent variables Xj, and its CGF is the sum of CGFs of
Xj/
√
N, as expected; details of the calculations are in Appendix (C).
We have now discussed two mechanisms that result in non-Gaussianities: 1/N correc-
tions and independence breaking. While one can use either or both of these mechanisms
to generate and control non-Gaussianities, more caution is required to use independence
breaking alone, at inﬁnite N. This is because the non-Gaussianities that are generated
by independence breaking might depend on N as well as α. For example, if the leading
corrections to higher cumulants κφ
r scale as αNar with ar < 0 for all r > 2, φ will be
Gaussian regardless of independence breaking. While if ar > 0, κφ
r will diverge, which is
undesirable. In the following, we will present an example where ar = 0 for all r and the
non-Gaussianities are generated by independence breaking alone.
16

Example: Independence Breaking at Inﬁnite N
Let us provide an example of independence breaking non-Gaussianities that persist in the
N →∞limit, showing how one can control higher cumulants by adjusting the correlations
between random variables. Consider the normalized sum of N random variables,
φ =
1
√
N
N
X
i=1
Xi,
(2.24)
where Xi is the product of two random variables ai and hi,
Xi = aihi.
(2.25)
This architecture can be interpreted as the last layer of a fully connected neural network,
where hi are the outputs of the neurons in the previous layer, ai/
√
N are the weights, and
φ is the output. First, let us consider the simple case where ai and hi are independent,
Gaussian random variables2,
P(⃗a,⃗h) = Pind(⃗a,⃗h) = (2πσaσh)−N exp

−1
2σ2a
N
X
i=1
a2
i −
1
2σ2
h
N
X
i=1
h2
i

,
where σa and σh are positive and ﬁnite. Since ai and hi are independent, so are Xi. The
CLT applies and φ is Gaussian.
Next, we will perturb P(⃗a,⃗h) to break independence. To that end, we introduce an
auxiliary random variable H and deﬁne,
P(⃗a,⃗h, H) = Pind(⃗a,⃗h, H)
= (2πσaσh)−N(
√
2πσh)−1 exp

−1
2σ2a
N
X
i=1
a2
i −
1
2σ2
h
N
X
i=1
h2
i −
1
2σ2
h
H2
,
(2.26)
where we set the standard deviation of H to σh, for simplicity. We then deﬁne a correction
term,
Pcorr(⃗a,⃗h, H) = Pind(⃗a,⃗h, H) · exp

−1
2σ2
h
N
X
i=1
(hi −H)2
.
(2.27)
Finally, putting these together we deﬁne,
P(⃗a,⃗h, H; α) = (1 −α)Pind(⃗a,⃗h) + αPcorr(⃗a,⃗h).
(2.28)
2The word “Gaussian” happens to appear many times in this example. To clarify: though a and h are
both Gaussian by construction, ah is not, and φ is Gaussian in the CLT limit.
17

When α = 0, the second term vanishes and both ai and hi are independent. As we turn
on α > 0, the ai remain independent, but correlations are induced between the hi through
a direct coupling to H in Pcorr(⃗a,⃗h).
To quantify the non-Gaussianity of φ as a function of α, we compute the CGF,
Wφ[J] = log E[eJ P
i Xi/
√
N]
= log
Z
N
Y
i=1
daidxiP(⃗a,⃗h, H; α)eJ P
i aihi/
√
N.
(2.29)
As P(⃗a,⃗h, H; α) is Gaussian, (2.29) can be evaluated analytically to give
Wφ[J] = log
"
(1 −α)
 
N
N −J2σ2
aσ2
h
!N/2
+ α
 
NN/2(N −J2σ2
aσ2
h)
1−N
2
p
N −(N + 1)J2σ2aσ2
h
!#
.
(2.30)
The odd cumulants vanish, as the φ ensemble has a Z2 symmetry φ →−φ (due to evenness
of P(a)), while the even cumulants κφ
r can be computed by taking derivatives of Wφ[J]. For
example, the second and the fourth cumulants are
κφ
2 = σ2
aσ2
h(1 + α),
(2.31)
κφ
4 = σ4
aσ4
h

9α −3α2 + 6 + 12α
N

.
(2.32)
In the limit N →∞, α →0, the second cumulant is ﬁnite while all higher cumulants vanish,
and φ is Gaussian as expected. At ﬁnite α > 0, all even cumulants are ﬁnite and in general
nonzero. The ability to tune α thus allows one to control the degree of non-Gaussianity of
φ. Note that breaking independence in the large N limit is not a particularly eﬃcient way
to sample from a non-Gaussian distribution of a single variable.
2.3. Connected Correlators in NN-FT
We wish to establish that the ideas exempliﬁed above — that non-Gaussianities may arise
via ﬁnite-N corrections or independence breaking — generalize to continuum NN-FT.
In outline, one may think of this conceptually as passing from a single random variable
φ (0d ﬁeld theory) to a discrete number of random variables φi (lattice ﬁeld theory), and
ﬁnally to a continuous number of random variables φ(x) (continuum ﬁeld theory), where
x ∈Rd. This is a textbook procedure in the context of the function-space path integral.
Here we wish to instead emphasize the general procedure and parameter space perspective.
Consider the case that the continuum ﬁeld φ(x) is built out of neurons hi(x) [10] as
φ(x) =
1
√
N
N
X
i=1
hi(x).
(2.33)
18

If the hi(x) are independent, the CLT states that φ(x) is Gaussian in the limit N →∞.
This is the essence of the NNGP correspondence.
Motivated by the single variable case, we will study non-Gaussianities arising from both
ﬁnite-N corrections and breaking of the independence condition. The cumulant generating
functional of φ(x) is
Wφ[J] = log Zφ[J] =
∞
X
r=1
Z
rY
i=1
ddxi
J(x1) . . . J(xr)
r!
G(r)
c (x1, . . . , xr),
(2.34)
where we have performed a series expansion in terms of the cumulants, a.k.a. the connected
correlation functions G(r)
c
of φ.
This is a straightforward generalization of (2.5) to the
continuum. When the odd-point functions vanish the connected four-point function is
G(4)
c (x1, . . . , x4) = G(4)(x1, . . . , x4) −(G(2)(x1, x2)G(2)(x3, x4) + 2 perms),
(2.35)
which will capture leading-order non-Gaussianities in many of our examples.
In the following, we will quantify non-Gaussianities in terms of non-vanishing cumulants,
as well as directly in the action via an Edgeworth expansion.
2.3.1. Finite-N Corrections with Independent Neurons
We ﬁrst study non-Gaussianities in the case where the neurons hi(x) are i.i.d. but N is
ﬁnite, e.g. single hidden layer networks, shown in [42]. We can express the CGF (2.34) in
terms of the connected correlation functions of the neurons,
Wφ(x)[J] = log E
h
exp
 1
√
N
N
X
i=1
Z
ddxJ(x)hi(x)
i
=
∞
X
r=1
Z
rY
i=1
ddxi
J(x1) · · ·J(xr)
r!
G(r)
c,hi(x1, · · · , xr)
Nr/2−1
.
(2.36)
This result relies on the fact that for independent hi, the expectation of the product is the
product of the expectations, which turns the ﬁrst expression into a sum on neuron CGFs.
For identically distributed neurons the sum gives a factor of N, and the normalization
1/
√
N gives the r-dependent N-scaling. This result lets us express the connected correlators
of φ(x) in terms of the connected correlators of hi(x),
G(r)
c (x1, · · · , xr) =
G(r)
c,hi(x1, · · · , xr)
Nr/2−1
.
(2.37)
This N-scaling implies
lim
N→∞G(r>2)
c
(x1, · · · , xr) = 0,
(2.38)
establishing Gaussianity in the limit.
19

Examples: Single Layer Cos-net and ReLU-net
We will now consider two single
hidden layer architectures with ﬁnite N and i.i.d.
parameters.
While the methods we
describe in this section can be employed to study neural networks with arbitrary depth
L > 1, inducing statistical correlations among neurons [42], single hidden layer architectures
suﬃce to demonstrate their utility.
ReLU-net
First, we will consider an architecture with a single hidden layer and ReLU
activation functions. As ReLU activations are ubiquitous in machine learning applications,
this is a natural example to study. Consider
φ(x) = W 1
i R(W 0
ijxj) where R(z) =



z, for z ≥0
0, otherwise
,
(2.39)
with d = dout = 1, W 0 ∼N (0,
σ2
W0
d ), W 1 ∼N (0,
σ2
W1
N ).
We compute the two-point function in the parameter space description 1.2 to obtain
G(2)
c,ReLU(x, y) = σ2
W0σ2
W1
2

R(x)R(y) + R(−x)R(−y)

,
(2.40)
which has a factorized structure in the terms that one might call bi-local: the function
depends independently on x and y, regardless of any relation between them. This result is
exact and does not receive 1/N corrections. Non-Gaussianities induced by 1/N corrections
manifest as a nonzero 4-pt connected correlation function,
G(4)
c,ReLU(x1, x2, x3, x4) = 1
N
 
15σ4
W0σ4
W1
4d2
 X
j=±1
R(jx1)R(jx2)R(jx3)R(jx4)

−σ4
W0σ4
W1
4d2
 X
P(abcd)
X
j=±1
R(jxa)R(jxb)R(−jxc)R(−jxd)
!
.
(2.41)
As expected, G(4)
c,ReLU(x1, x2, x3, x4) scales as 1/N.
Cos-net
Next, let us study a single hidden layer network with cosine activation functions.
The NN-FT associated to Cos-net (and its generalizations) is Euclidean invariant [10], which
is interesting on physical grounds, e.g. to satisfy one of the Osterwalder-Schrader axioms
to establish an NN-QFT. Euclidean invariance may be established using the mechanism
of [8] for determining symmetries from parameter space correlators, which absorbs sym-
metry transformations into parameter redeﬁnitions, yielding invariant correlators when the
relevant parameter distributions are invariant under the symmetry.
20

Cos-net was deﬁned in [10], where its 2-point function and connected 4-point function
were also computed. The architecture is
φ(x) = W 1
i cos(W 0
ijxj + b0
i )
(2.42)
where W 1 ∼N (0, σ2
W1/N), W 0 ∼N (0, σ2
W0/d), and b0 ∼Unif[−π, π].
As before, the
correlation functions are computed in parameter space (1.2). The 2-pt function
G(2)
c,Cos(x1, x2) = σ2
W1
2 e−1
2d σ2
W0(∆x12)2
(2.43)
is manifestly translation invariant, with ∆x12 = x1 −x2. The 4-pt correlation function is
G(4)
c,Cos(x1, x2, x3, x4) = σ4
W1
8N
X
P(abcd)

3e−
σ2
W0 (∆xab+∆xcd)2
2d
−2e−
σ2
W0((∆xab)2+(∆xcd)2)
2d

,
(2.44)
where ∆xij := xi −xj and P(abcd) denotes the three independent ways of drawing pairs
(xa, xb), (xc, xd) from the list of external vertices (x1, x2, x3, x4).
We see the manifest Euclidean invariance of these correlators, and that non-Gaussianities
are encoded in G(4)
c,Cos as a 1/N corrections.
2.3.2. Generalized Connected Correlators from Independence Breaking
We now wish to generalize our theories and connected correlators to including the possibility
that non-Gaussianities arise not only from 1/N-corrections, but also from independence
breaking, e.g. by developing correlations between the neurons hi(x). Previously, [10, 42]
studied mixed non-Gaussianities at ﬁnite N and statistical correlations among neurons.
Generalizing our approach from section 2.2, we parameterize breaking of statistical
independence by promoting the distribution of neurons P(h) to depend on a vector of
hyperparameters ⃗α ∈Rq, P(h; ⃗α).
Since independence is necessary for Gaussianity via the CLT, and we will sometimes
wish to perturb around the Gaussian ﬁxed point, we require
P(h; ⃗α = ⃗0) =
Y
i
P(hi),
(2.45)
where the hyperparameter vector ⃗α must be chosen as part of the architecture deﬁnition.
From this expression, the neurons become independent when ⃗α = 0.
For a general P(h; ⃗α), the CGF is
Wφ[J] = log
" Z  N
Y
i=1
Dhi

P(h; ⃗α)e
1
√
N
N
P
i=1
R dx hi(x)J(x)
#
.
(2.46)
21

For small values of α, we can expand P(h; ⃗α),
P(h; ⃗α) = P(h; ⃗α = 0) +
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · · αsr
r!
∂αs1 · · · ∂αsr P(h; ⃗α)

⃗α=0.
(2.47)
Analogous to the single variable case, we deﬁne
Pr,{s1,··· ,sr} :=
1
P(h; ⃗α)∂αs1 · · · ∂αsr P(h; ⃗α)
(2.48)
satisfying the recursion relation
Pr+1,{s1,··· ,sr+1} =
1
r + 1
r+1
X
γ=1
(P1,sγ + ∂αsγ )Pr,{s1,··· ,sr+1}\sγ.
(2.49)
Finally, we can expand (2.34) in ⃗α,
Wφ[J] = log
"
eWφ,⃗α=0[J]+
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · · αsr
r!
N
Y
i=1
EPi(hi)
h
e
1
√
N
R ddx hi(x)J(x)·Pr,{s1,··· ,sr}

⃗α=0
i#
,
(2.50)
where Wφ,⃗α=0[J] is given in (2.36). This form of Wφ[J] makes it clear how one can tune N
and ⃗α to generate and manipulate non-Gaussianities; for details see Appendix (C).
For appropriately small independence breaking hyperparameter ⃗α, and other attributes
of the architecture, the ratio of second term to ﬁrst term in the logarithm of (2.50) is small.
In such cases, one can approximate (2.50) using Taylor series expansion log(1 + x) ≈x
around x = 0. The CGF becomes
Wφ[J] = Wφ,⃗α=0[J] +
q
X
s=1
αs
eWφ,⃗α=0[J]
N
Y
i=1
EPi(hi)
h
e
1
√
N
R
ddx hi(x)J(x) · P1,s

⃗α=0
i
,
(2.51)
and the cumulants
G(r)
c (x1, · · · , xr) =
∂rWφ[J]
∂J(x1) · · · ∂J(xr)

J=0,
= G(r),i.i.d.
c
+ ⃗α · ∆G(r)
c
+ O(⃗α2).
(2.52)
are proportional to ⃗α at the leading order. The leading order expression in ⃗α is evaluated
in (C.21).
Example: Single Layer Cos-net
Let us exemplify the non-Gaussianities generated by
statistical independence breaking of a single layer Cos-net architecture given in (2.42). We
22

can break this independence by modifying the distribution from which the weights W 0
ij (an
N × d matrix) are sampled
P(W 0) = c exp
"
−
X
i,j

d
2σ2
W0
(W 0
ij)2 + αIB
N2
X
i1,j1,i2,j2
(W 0
i1j1)2(W 0
i2j2)2#
,
(2.53)
where c is a normalization constant. The rotational invariance preserving term αIB(Tr(W 0T W 0))2
N2
introduces mixing between the weights W 0
ij and parametric independence is explicitly bro-
ken. The degree of independence breaking can be controlled by tuning αIB.
We wish to compute the connected correlation functions to quantify the non-Gaussianities
generated by independence breaking. In general, this is a diﬃcult problem. However, when
αIB ≪1, we can perform a perturbative expansion in αIB. Setting d = 1 for simplicity, we
obtain
G(2)
c,Cos(x1, x2) =αIBσ4
W0σ2
W1e−
σ2
W0 (∆x12)2
2
2N

−
 1 −5σ2
W0(∆x12)2 + σ4
W0(∆x12)4
N
+ σ2
W0(∆x12)2

,
(2.54)
G(4)
c,Cos(x1, · · · , x4) = G(4)
c,Cos
i.i.d.
(x1, · · · , x4) + αIBσ4
W 0σ4
W 1
8N2
X
P(abcd)
"
6 −

2σ2
W 0(∆x2
ab + ∆x2
cd)
+ 2σ4
W 0∆x2
ab∆x2
cd

e−
σ2
W0
2 (∆x2
ab+∆x2
cd) +

3 + 3σ2
W 0(∆xab + ∆xcd)2
e−
σ2
W0
2
(∆xab+∆xcd)2
#
,
(2.55)
to leading order in αIB, where G(4)
c,Cos
i.i.d.
(x1, · · · , x4) is obtained at d = 1 from (2.44). Non-
Gaussianities at ﬁnite N, and αIB ̸= 0 still preserve the translation invariance of the 2nd
and 4th cumulants of Cos-net architecture.
We refer the reader to Appendix (B) for details, where we also compute leading order
non-Gaussian corrections to ﬁrst two cumulants in a single hidden layer Gauss-net at αIB ̸=
0, ﬁnite N, for d = 1.
3. Computing Actions from Connected Correlators
In Section 2 we systematically studied non-Gaussianities in neural network ﬁeld theories
by parametrically violating two assumptions of the Central Limit Theorem:
inﬁnite-N
23

and independence. The study was performed at the level of connected correlators, rather
than actions, because every NN-FT admits a parameter space description of connected
correlators, even if an action is not known.
In this section we will develop these techniques for calculating actions from connected
correlators, including in terms of Feynman diagrams in which the connected correlators are
vertices. More speciﬁcally:
• Field Density from Connected Correlators: Edgeworth Expansion. In Section
3.1 we review how knowledge of the cumulants of a single random variable may
be used to approximate its probability density, and then we generalize to the ﬁeld
theory case, which has a continuum of random variables. This gives an expression
for P[φ] = exp(−S[φ]) in terms of connected correlation functions. We present an
explicit example in the case of a single variable.
• Computing the Action with Feynman Diagrams.
Given the Edgeworth ex-
pansion, we develop a method to compute the action perturbatively via Feynman
diagrams, which becomes clear due to a formal similarity between the Edgeworth ex-
pansion and the partition function of a ﬁeld theory. This is a result that is applicable
to general ﬁeld theories.
• Neural Network Field Theory Actions. In Section 3.3 we specify the analysis of
Section 3.2 to the case of neural network ﬁeld theories. We derive the leading order
form of the action for the case of non-Gaussianities induced either by 1/N-corrections
or independence breaking.
• Neural Network Field Theory Examples. In Section 3.4 we derive the leading-
order action in 1/N for concrete neural network architectures.
3.1. Field Density from Connected Correlators: Edgeworth Expansion
The Edgeworth expansion from statistics (see e.g. [43] for a textbook statistics description
and [11] for an ML study) can be used to construct the probability density from the
cumulants. The key observation which allows the Edgeworth expansion to be applied in a
ﬁeld theory is that the normal relation for the generating function in terms of the action
eW [J] =
Z
dφ e−S[φ]+Jφ
(3.1)
24

can be inverted to express the action in terms of the generating functional.
Adding a
source term in the exponent, mapping J →iJ and integrating over J, we have
Z
dJeW [iJ]−iJφ =
Z
dJe−iJφ
Z
dφ′e−S[φ′]+iJφ′ = e−S[φ]
(3.2)
where
Z
dJeiJ(φ′−φ) = δ[φ′ −φ]
(3.3)
has been used. Deforming the J integration contour back to real J then results in
P[φ] = e−S[φ] =
Z
dJeW [J]−Jφ,
(3.4)
This gives the probability density and action in terms of W[J]. This result can also be
thought of as arising from an inverse Fourier transform of the characteristic function.
Then to apply the Edgeworth expansion for a single random variable φ, we write W[J]
in terms of cumulants
W[J] =
∞
X
r=1
κr
r! Jr,
(3.5)
which lets us write
P[φ] = exp
" ∞
X
r=3
κr
r! (−∂φ)r
# Z
dJeκ2 J2
2 +κ1J−Jφ,
= exp
" ∞
X
r=3
κr
r! (−∂φ)r
#
e−(φ−κ1)2
2κ2
,
(3.6)
where the Gaussian integral has been performed by mapping J →iJ (alternatively, working
with the characteristic function the whole time) and we have neglected the normalization
factor. We have an expression for the density Pφ as an expansion around the Gaussian
with mean κ1 and variance κ2.
The result may be extended to the ﬁeld theory case, where φ is replaced by φ(x), a
continuum of mean free random variables. Then the relation is
e−S[φ] = 1
Z exp

∞
X
r=3
(−1)r
r!
Z
rY
i=1
ddxiG(r)
c (x1, · · · , xr)
δ
δφ(x1) · · ·
δ
δφ(xr)

e−SG[φ],
(3.7)
where the Gaussian Process action SG is deﬁned as
SG[φ] = 1
2
Z
ddx1ddx2 φ(x1)G(2)
c (x1, x2)−1φ(x2),
(3.8)
25

To the extent that there is a perturbative ordering to the correlators through some ex-
pansion parameter (such as
1
N or independence breaking), this expression can be evaluated
perturbatively to systematically construct an action from the cumulants. 3
1D Example: Sum of N Uniform Random Variables
Let us demonstrate the Edgeworth expansion in a simple example. Consider the standard-
ized sum of N i.i.d. random variables sampled from a uniform distribution
φ =
1
√
N
N
X
i=1
Xi,
Xi ∼Unif(−1/2, 1/2) ∀i.
(3.10)
The cumulants of Xi are
κXi
1
= 0,
(3.11)
κXi
r
= Br
r for r ≥2,
(3.12)
where Br is the rth Bernoulli number.4 Plugging this into (2.15), the cumulants of φ are
κφ
r =
Br
rNr/2−1.
(3.13)
At ﬁnite N, the cumulants κφ
r>2 are nonzero and φ is non-Gaussian. Using these cumulants,
we can write down the probability distribution function of φ via an Edgeworth expansion,
Pφ = 1
Z exp
" ∞
X
r=3
κφ
r
r! (−∂φ)r
#
e−φ2/2κφ
2
= 1
Z exp
" ∞
X
r=3
Br
r!rNr/2−1(−∂φ)r
#
e−φ2/B2
(3.14)
3In the ﬁnite-dimensional version of the Edgeworth expansion, it is sometimes convenient to further
express the powers of derivatives in terms of probabilist’s Hermite polynomials using
(−∂x)r e−x2
2 =: Hr(x) e−x2
2 ,
(3.9)
In the ﬁeld theory case, using Hermite polynomials provides no obvious advantage.
4Br vanishes for odd r ≥2. First few nonzero Bernoulli numbers are: B2 = 1
6, B4 = −1
30, B6 =
1
42.
26

Truncating the sum at r = 4, expanding the exponential, and keeping terms up to O(1/N)
we get
Pφ = 1
Z
"
1 + κφ
4

1
8(κφ
2)2 −
1
4(κφ
2)3φ2 +
1
24(κφ
2)4φ4
+ O(1/N3/2)
#
e−φ2/2κφ
2,
= 1
Z′ exp
"
−
 1
2κφ
2
+
κφ
4
4(κφ
2)3

φ2 +
κφ
4
24(κφ
2)4φ4 + O(1/N3/2)
#
,
= 1
Z′ exp
"
−6 + 18
5N

φ2 −36
5N φ4 + O(1/N3/2)
#
,
(3.15)
where on the second line we absorbed the constant term into the normalization constant
Z′. At order O(N0), the exponent in (3.15) is quadratic and φ is Gaussian distributed.
Gaussianity is then broken by a quartic interaction at order O(1/N).
It is worth noting that the cumulants of φ are given by simple closed form expressions,
see Equation (3.13), while Pφ involves a perturbative expansion in 1/N. This is in contrast
to weakly coupled ﬁeld theories, where we often start from a simple action expressed in
closed form and calculate the connected correlation functions via a perturbative expansion
in the coeﬃcients of interaction terms.
3.2. Computing the Action with Feynman Diagrams
In a ﬁeld theory a powerful tool for organizing a perturbation expansion is with Feynman
diagrams. Just as Feynman diagrams can be used to compute the cumulants perturbatively
in an expansion parameter from an action, they can also be used to compute the action
perturbatively from the cumulants. To understand the derivation, recall the expression for
the partition function
eW [J] = Z[J] = c′ exp

∞
X
r=3
Z
rY
i=1
ddxi gr(x1, · · · , xr)
δ
δJ(x1) · · ·
δ
δJ(xr)

e−S0[J],
(3.16)
where we have introduced couplings gr instead of gr/r!,
S0[J] =
Z
dx1dx2J(x1)∆(x1, x2)J(x2),
(3.17)
and ∆(x1, x2) is the free propagator.
The expression (3.16) arises by taking the usual
expression for the partition function
Z[J] =
Z
Dφ e−Sfree[φ]−Sint+
R
ddxJ(x)φ(x)
(3.18)
27

Field Picture
Source Picture
Field
φ(x)
J(x)
CGF
W[J] = log(Z[J])
S[φ] = −log(P[φ])
Cumulant
G(r)
c (x1, . . . , xr)
gr(x1, . . . , xr)
Table 1: The Edgeworth expansion for P[φ] and the interaction expansion of Z[J] are
formally related by a change of variables, given here up to constant factors. Due to this
relationship, non-local couplings and connected correlators may both be computed by ap-
propriate connected Feynman diagrams.
and replacing the φ’s in the interaction terms
Sint =
∞
X
r=3
Z
rY
i=1
ddxi gr(x1, . . . , xr) φ(x1) . . . φ(xr)
(3.19)
by δ/δJ’s. Pulling the J-derivatives outside of the
R
Dφ in (3.18) and performing the Gaus-
sian integral yields (3.16). These manipulations closely mirror the Edgeworth expansion.
The Edgeworth expansion (3.7) is related to the partition function (3.16) by a simple
change of variables, given in Table 1, which one might think of as a duality map between
a ﬁeld picture and a source picture. This relationship between the Edgeworth expansion
and the partition function immediately tells us that the analog of gr(x1, . . . , xn) are the
connected correlation functions G(r)
c (x1, · · · , xr) in (3.7).
We may therefore compute the couplings gr(x1, . . . , xn) in the same way that we com-
pute the connected correlators G(r)
c (x1, · · · , xr). In a weakly coupled ﬁeld theory, one can
compute the connected correlation functions G(r)
c (x1, · · · , xr) in terms of the couplings
gr(x1, · · · , xr) perturbatively via Feynman diagrams. An Edgeworth expansion allows us to
do the converse and compute the couplings gr(x1, · · · , xr) in terms of the connected corre-
lation functions G(r)
c (x1, · · · , xr). The similarity between (3.7) and (3.16) suggests that the
terms in the expansion for gr(x1, · · · , xr) can be represented by Feynman diagrams, whose
vertices are connected correlators, e.g.
G(6)
c
(3.20)
in the case of a six-point vertex. Notably, the vertex is itself a function and lines enter the
n-point vertex at n locations.
28

To compute the coupling gr(x1, . . . , xr) in terms of Feynman diagrams, one sums over
all connected r-point Feynman diagrams made out of G(n)
c
vertices.
By convention, we
do not label internal points on the vertices, in order to simplify the combinatorics. For
instance, the four-point coupling g4(x1, . . . , x4) has a diagram
G(4)
c
x1
x2
x3
x4
(3.21)
where it is to be understood that connections to internal points in a vertex appear in all
possible combinations. Analytic expressions may be obtained from the diagrams via the
Feynman rules given in Table 2. If G(2)
c (xi, yj)−1 =
δ2SG
δφ(xi)δφ(yj) involves diﬀerential operators,
it can be evaluated by Fourier transformation, see Appendix (D)
Feynman Rules for gr(x1, . . . , xr).
1. Internal points associated to vertices are unlabeled, for diagrammatic simplicity. Prop-
agators therefore connect to internal points in all possible ways.
2. For each propagator between zi and zj,
zi
zj = G(2)
c (zi, zj)−1.
(3.22)
3. For each vertex,
G(n)
c
= (−1)n
Z
ddy1 · · · ddyn G(n)
c (y1, · · · , yn).
(3.23)
4. Divide by symmetry factor and insert overall (−).
Table 2: Feynman rules for computing gr from each connected diagram with G(n)
c
vertices.
As an example, let us compute a contribution to the quartic coupling g4(x1, x2, x3, x4)
29

from a G(4)
c
vertex
g4(x1, . . . , x4) = −1
4!
h Z
dy1dy2dy3dy4 G(4)
c (y1, y2, y3, y4) G(2)
c (y1, x1)−1G(2)
c (y2, x2)−1
G(2)
c (y3, x3)−1G(2)
c (y4, x4)−1 + perms
i
+ . . .
(3.24)
=
G(4)
c
x1
x2
x3
x4
+ . . . ,
(3.25)
where the dots represent contributions from other diagrams, and “perms” represents other
diagrams from permutations over internal points. A combinatoric factor of 4! from summing
over internal points cancels out the prefactor 1/4! from Edgeworth expansion.
The Edgeworth expansion (3.7) involves an inﬁnite sum. Correspondingly, computing
gr(x1, · · · , xr) requires summing over inﬁnitely many Feynman diagrams.
When all but
ﬁnitely many terms in the expansion are parametrically suppressed, the expansion can be
truncated at ﬁnite order to provide an approximation of gr(x1, · · · , xr).
We will apply
these rules to concrete examples later in this section and demonstrate how approximations
to gr(x1, · · · , xr) can be obtained systematically.
While our focus is on neural network ﬁeld theories, we emphasize that Edgeworth ex-
pansions can be utilized in any ﬁeld theory where the connected correlation functions are
known, and the expansion in (3.7) is not divergent.
3.2.1. Example: Non-local φ4 Theory.
Aside from any application in NN-FT, it is interesting to study the self-consistency of the
Edgeworth expansion. We do so in a famous case, φ4 theory, generalized to the case of
non-local quartic interactions, in order to demonstrate the ability of the Edgeworth method
to handle non-locality. Consider the action
S[φ] =
Z
ddx1ddx2
1
2φ(x1)G(2)
G,φ(x1, x2)−1φ(x2)
+ 1
4!
Z
ddx1ddx2ddx3ddx4λ(x1, x2, x3, x4)φ(x1)φ(x2)φ(x3)φ(x4),
(3.26)
where G(2)
G,φ(x1, x2)−1 and λ(x1, x2, x3, x4) are both totally symmetric, and G(2)
G,φ(x1, x2)−1
is the operator in the free action SG[φ].
We denote the free propagator D(x1, x2) so
that
R
ddx′ G(2)
G,φ(x1, x2)−1 D(x′, x2) = δd(x1 −x2).
We can then expand G(2)
c (x1, x2) in
30

λ(x1, x2, x3, x4), and at leading order,
G(2)
c (x1, x2) = D(x1, x2) + 1
2
Z
ddy1 · · · ddy4λ(y1, y2, y3, y4)D(x1, y1)D(y2, y3)D(y4, x2),
(3.27)
where the 1
2 is a symmetry factor. Similarly,
G(4)
c (x1, · · · , x4) =
Z
ddx′
1 · · · ddx′
4 λ(x′
1, x′
2, x′
3, x′
4)D(x1, x′
1)D(x2, x′
2)D(x3, x′
3)D(x4, x′
4)
+ O(λ2).
(3.28)
There are no other connected correlators that have contributions at O(λ). To perform an
Edgeworth expansion, we ﬁrst need to write down the inverse propagator,
G(2)
c (x1, x2)−1 = G(2)
G,φ(x1, x2)−1 −1
2
Z
ddx3ddx4λ(x1, x2, x3, x4)D(x3, x4) + O(λ2).
(3.29)
Given (3.29), it is easy to verify that
Z
dx′G(2)
c (x1, x′)−1G(2)
c (x′, x2) = δ(x1 −x2) + O(λ2).
(3.30)
At this point, let us introduce a shorthand notation to improve readability, rewriting
R
ddx1ddx2 G(2)
c (x1, x2),
R
ddx1ddx2 G(2)
c (x1, x2)−1 and
R
ddx1 · · · ddx4 G(4)
c (x1, · · · , x4) as,
Gxy = Dxy + 1
2λ1234D1xD23D4y + O(λ2),
(3.31)
G−1
xy = G(2)
G,φ(x, y)−1 −1
2λxy12D12 + O(λ2),
(3.32)
G1234 = λ1′2′3′4′D1′1D2′2D3′3D4′4 + O(λ2),
(3.33)
respectively. Finally, we obtain the Edgeworth expansion at O(λ) by plugging in (3.29)
and (3.28) into (3.7),
P[φ] = 1
Z exp
 1
4!G1234δ1δ2δ3δ4

exp

−1
2φxG−1
xy φy

+ O(λ2),
(3.34)
where δ1 := δ/δφ(x1). Expanding the ﬁrst exponential and performing the derivatives we
obtain
P[φ] = 1
Z
h
1 −λ1234
8
D12D34 −λ1234
4! φ1φ2φ3φ4
i
exp

−1
2φxG(2)
G,φ(x, y)−1φy

+ O(λ2), (3.35)
with λ1234 :=
R
ddx1 · · · ddx4 λ(x1, · · · , x4), and φx := φ(x).
The second term does not
depend on φ and can be absorbed into the normalization factor, resulting in
P[φ] = 1
Z′ exp

−1
2φxG(2)
G,φ(x1, x2)−1φy −λ1234
4! φ1φ2φ3φ4

+ O(λ2).
(3.36)
We have recovered the φ4 action at O(λ), as expected.
31

3.3. General Interacting Actions in NN-FT
We now study the Edgeworth expansion in neural network ﬁeld theories. We will modify the
general analysis of the previous section to the case where non-Gaussianities are generated
by the two mechanisms we described in Section 2, namely, by violating assumptions of the
CLT by ﬁnite N corrections and independence breaking.
Interactions from 1/N-corrections. As we discussed in section 2.3.1, non-Gaussianities
arising due to 1/N corrections result in connected correlation functions that scale as
G(r)
c (x1, · · · , xr) ∝
1
Nr/2−1,
(3.37)
for a single hidden layer network. At large N, the action can be approximated systemati-
cally by organizing the Edgeworth expansion in powers of 1/N, calculating the couplings
via Feynman diagrams, and truncating at a ﬁxed order in 1/N.
To do so, we need to know how the couplings scale with N. We have studied a case
in (3.25) where only the even-point correlators are non-zero, and clearly there is a 1/N
contribution to g4 from a single G(4)
c
vertex; any higher order correlator G(r>4)
c
contributes
at 1/Nr/2−1 and higher. Consider now contributions to the couplings gr>4. There is a tree-
level 1/Nr/2−1 contribution from a single G(r)
c
vertex and there are 1/Nn/2−1 contributions
from a G(n>r)
c
vertex with an appropriate number of loops; both are more suppressed
than the 1/N contribution to g4. Finally, consider contributions from V number of G(n<r)
c
vertices. Forming a connected diagram requires nV > r, which implies V ≥2 and therefore
the contribution is of order 1/N≥n−1, which is more suppressed than 1/N since n begins
at 3 in the Edgeworth expansion. Therefore, the single-vertex tree-level contribution to g4
is the leading contribution in 1/N.
The quartic coupling g4(x1, x2, x3, x4), at leading order in G(4)
c
∝1/N, is
g4(x1, . . . , x4) = −1
4!
" Z
dy1dy2dy3dy4 G(4)
c (y1, y2, y3, y4) G(2)
c (y1, x1)−1G(2)
c (y2, x2)−1
G(2)
c (y3, x3)−1G(2)
c (y4, x4)−1 + perms
#
+ O
 1
N2

,
(3.38)
=
G(4)
c
x1
x2
x3
x4
+ O
 1
N2

.
(3.39)
We may compute this coupling in a NN-FT by ﬁrst computing G(4)
c
in parameter space.
32

In summary, the leading-order in 1/N action for a single layer NN-FT is
S = SG +
Z
ddx1 . . . ddx4 g4(x1, . . . , x4) φ(x1) . . . φ(x4) + O
 1
N2

,
(3.40)
where g4 at O(1/N) is given in (3.39), under the assumption that the odd-point functions
are zero, as in the architectures of Section (3.4).
Interactions from Independence Breaking. Non-Gaussianities generated via indepen-
dence breaking alone are qualitatively diﬀerent than those from 1/N corrections.
We wish to determine the leading-order action due to independence breaking. Focusing
on the case where independence breaking is controlled by a single parameter α for simplicity,
it follows from (2.52), that the connected correlation functions scale as
G(r)
c (x1, · · · , xr) ∝α
∀r > 2
(3.41)
at N →∞limit, since the connected correlators G(r),free
c
|r>2 of the free theory vanish.
As a result, each coupling gr(x1, · · · , xr) receives contributions from tree-level diagrams
of all connected correlators, at leading order in α.
More generally, at any given order
in α, there are inﬁnitely many diagrams from all connected correlators to gr(x1, · · · , xr).
For example, the expansion for g4(x1, x2, x3, x4) at O(α) includes terms proportional to
G(2n)
c
(x1, · · · , x2n) for all n > 1,
g4(x1, x2, x3, x4) = −
∞
X
n=2
(−1)2n
(2n)!
h Z
dy1 · · ·dy2n G(2n)
c
(y1, · · · , y2n) G(2)
c (y1, x1)−1
G(2)
c (y2, x2)−1 G(2)
c (y3, x3)−1 G(2)
c (y4, x4)−1
2n−1
Y
m=5
G(2)
c (ym, ym+1)−1 + perms
i
+ O(α2),
(3.42)
=(−1)2n
 
G(4)
c
x1
x2
x3
x4
+
G(6)
c
x1
x2
x3
x4
+
G(8)
c
x1
x2
x3
x4
+ · · ·
!
+ O(α2), (3.43)
where summing over internal points yi cancels out
1
2n! prefactor from each G(2n)
c
. The terms
in the parenthesis constitute an inﬁnite sum.
This structure makes it impossible to systematically approximate gr(x1, · · · , xr) with
a ﬁnite number of terms via a perturbative expansion in α, unless some other structure
correlates with it. Note that this is a feature of neural network ﬁeld theories where non-
Gaussianities are generated only by independence breaking.
Approximation via a ﬁnite
33

number of terms would be possible in cases where connected correlation functions scale
with both α and 1/N. In the limit of N →∞, the leading-order in α action for a NN-FT
is
S = SG +
∞
X
r=4
Z
ddx1 . . . ddxr gr(x1, . . . , xr) φ(x1) . . . φ(xr) + O(α2),
(3.44)
where gr>4’s are computed similar to (3.43). Such an action can not be approximated by
a ﬁnite truncation, unless the theory exhibits additional structure.
3.4. Example Actions in NN-FT
Next, we exemplify the Feynman rules from Section (3.2) in a few single layer NN archi-
tecture examples at ﬁnite width and i.i.d. parameters, and evaluate the leading order in
1/N quartic coupling and NN-FT action. The quartic coupling is
g4(x1, · · · , x4) = −1
4!
h Z
ddy1 · · · ddy4 G(4)
c (y1, · · · , y4)G(2)
c (y1, x1)−1 · · ·G(2)
c (y4, x4)−1 + perms
i
,
(3.45)
at O(1/N). When G(2)
c (x1, y1)−1 involves diﬀerential operators, we use the methods from
Appendix (D) to evaluate g4.
Single Layer Cos-net
Recall the Cos-net architecture introduced earlier, φ(x) = W 1
i cos(W 0
ijxj + b0
i ), for W 1 ∼
N (0, σ2
W1/N), W 0 ∼N (0, σ2
W0/d), and b0 ∼Unif[−π, π]. We will consider the case where
all parameters are independent and non-Gaussianities arise due to ﬁnite N corrections. To
evaluate the leading order quartic coupling for this NNFT, let us ﬁrst compute the inverse
propagator G(2)
c,Cos(x1, x2)−1, starting from the 2-pt function
G(2)
c,Cos(x1, x2) = σ2
W1
2 e−
σ2
W0 (x1−x2)2
2d
,
(3.46)
and inversion relation
R
ddy G(2)
c,Cos(x, y)−1 G(2)
c,Cos(y, z) = δd(x −z). Translation invariance
of the 2-pt function and delta function constraints G(2)
c,Cos(x, y)−1 as a translation invariant
operator. Then, performing a Fourier transformation of the 2-pt function and its inverse
operator, followed by an inverse Fourier transformation, we obtain
G(2)
c,Cos(x, y)−1 = 2σ2
W0
σ2
W1d e−
σ2
W0 ∇2x
2d
δd(x −y),
(3.47)
34

where ∇2
x := ∂2/∂x2. Here, we use (D.3) to evaluate the quartic coupling as,
gCos
4
(x1, · · · , x4) = −
Z
ddp1 · · · ddp4 ˜G(4)
c,Cos(p1, · · · , p4) ˜G(2)
c,Cos(−p1)−1 · · · ˜G(2)
c,Cos(−p4)−1
e−ip1x1···−ip4x4,
(3.48)
where ˜G(4)
c,Cos(p1, · · · , p4) is from (B.8), and ˜G(2)
c,Cos(−p)−1 =
2σW0
√
dσ2
W1
e
dp2
2σ2
W0 . Using this,
gCos
4
(x1, x2, x3, x4) = −4
√
6π3/2σ4
W0
Nd2σ4
W1
X
P(abcd)
e−
σ2
W0 ∇2rabcd
6d
+ 8πσ4
W0
Nd2σ4
W1
X
P(ab,cd)
e−
σ2
W0 (∇2rab +∇2rcd )
2d
.
(3.49)
We introduce the abbreviation rabcd := xa + xb −xc −xd, and P(abcd) = 12 refers to the
number of ways ordered list of indices a, c, b, d ∈{1, 2, 3, 4} can be chosen. Similarly, rab :=
xa −xb, and P(ab, cd) = 12 is the number of ways ordered pairs (a, c), (b, d) ∈{1, 2, 3, 4}
can be drawn.
With this, Cos-net ﬁeld theory action at O(1/N) is
SCos[φ] = 2σ2
W0
σ2
W1d
Z
ddx φ(x) e−
σ2
W0 ∇2x
2d
φ(x) −
Z
ddx1 · · · ddx4
"
4
√
6π3/2σ4
W0
Nd2σ4
W1
X
P(abcd)
e−
σ2
W0 ∇2rabcd
6d
−8πσ4
W0
Nd2σ4
W1
X
P(ab,cd)
e−
σ2
W0 (∇2rab +∇2rcd )
2d
#
φ(x1) · · ·φ(x4) + O(1/N2).
(3.50)
The NNGP action is local, but the leading order quartic interaction is non-local.
Single Layer Gauss-net
As our next example, consider the output of a single-layer Gauss-net
φ(x) =
W 1
i exp(W 0
ijxj + b0
i )
q
exp[2(σ2
b0 +
σ2
W0
d x2)]
,
(3.51)
for parameters drawn i.i.d. from W 0 ∼N (0,
σ2
W0
d ), W 1 ∼N (0,
σ2
W1
N ), and b0 ∼N (0, σ2
b0).
The propagator is identical to Cos-net ﬁeld theory, and so is G(2)
c,Gauss(x1, x2)−1. We evaluate
35

Gauss-net quartic coupling g4, using (D.3), and (B.12) for ˜G(4)
c,Gauss, as
gGauss
4
(x1, · · · , x4) = −4
√
2 π3/2σ4
W0
√
3N2d4σ4
W1
X
P(abcd)

d2N + 2σ4
W0 −σ5
W0(d −σ2
W0∇2
rabcd)
d3/2

e−
σ2
W0 ∇2rabcd
6d
+
8πσ4
W0
N2d4σ4
W1
X
P(ab,cd)

d2N + 6σ4
W0 −4d3σ5
W0 + σ6
W0
d
+
2σ7
W0
d3/2 −σ8
W0
d2

(∇2
rab + ∇2
rcd)
+ σ10
W0
d3 ∇2
rab∇2
rcd

e−
σ2
W0 (∇2rab +∇2rcd )
2d
,
(3.52)
where P(ab, cd) and P(abcd) are deﬁned as before.
Thus, Gauss-net ﬁeld theory action at O(1/N),
SGauss[φ] = 2σ2
W0
σ2
W1d
Z
ddx φ(x) e−
σ2
W0 ∇2x
2d
φ(x) +
Z
ddx1 · · · ddx4 gGauss
4
φ(x1) · · · φ(x4),
(3.53)
diﬀers from Cos-net ﬁeld theory at the level of quartic interaction.
4. Engineering Actions: Generalities, Locality, and φ4 Theory
In Section 3 we used the Edgeworth expansion and a “duality” between ﬁelds and sources to
compute couplings (including non-local ones) in the action as connected Feynman diagrams
whose vertices are given by the usual connected correlators G(n)
c (x1, . . . , xn). This general
ﬁeld theory result is applicable in NN-FT of ﬁxed architectures, but it doesn’t answer the
question of how to engineer an architecture that realizes a given action.
In this section we study how to design actions of a given type by deforming a Gaussian
theory by an arbitrary operator. The result is simple and exploits the duality between the
parameter-space and function-space descriptions of a ﬁeld theory. The main results are:
• Action Deformations. We develop a mechanism for expressing an arbitrary defor-
mation of a Gaussian action as a deformation of the parameter density of a NN-FT.
• Local Lagrangians. We utilize the mechanism to engineer local interactions.
• φ4 Theory as a NN-FT. Using a previous result that achieves free scalar ﬁeld theory
as a NN-FT, we engineer local φ4 theory as an NN-FT.
• Cluster Decomposition. We develop an approach to cluster decomposition, another
notion of locality that is weaker than local interactions.
36

We also discuss why it might have been expected that φ4 theory (and other well-studied
ﬁeld theories) arises naturally at inﬁnite-N.
To begin our analysis, consider the partition function of a Gaussian theory
ZG[J] = EG[e
R ddx J(x)φ(x)],
(4.1)
where we have labelled both the partition function and the expectation with a G subscript
to emphasize Gaussianity.
Now we wish to deﬁne a deformed theory that diﬀers from the original only by an
operator insertion, treating it in both function space and parameter space. The deformed
partition function is given by
Z[J] = EG[e−λ
R
ddx1...ddxr Oφ(x1,...,xr)e
R
ddx J(x)φ(x)],
(4.2)
where Oφ is a non-local operator (though it may be chosen to be local) that has a subscript
φ, denoting that it may depend on φ and its derivatives. In the function space, the partition
function of the Gaussian theory is
ZG[J] =
Z
Dφ e−SG[φ]+R ddxJ(x)φ(x),
(4.3)
and the operator insertion corresponds to a deformation of the partition function to
Z[J] =
Z
Dφ e−S[φ]+R ddxJ(x)φ(x)
(4.4)
where the action has been deformed
SG[φ] →S[φ] = SG[φ] + λ
Z
ddx1 . . . ddxr Oφ(x1, . . . , xr).
(4.5)
We may treat this theory in perturbation theory in the usual way:
correlators in the
non-Gaussian theory are expanded perturbatively in λ and evaluated using the Gaussian
expectation EG, which utilizes the Gaussian action when expressed in function-space.
How is this deformation expressed in parameter space, i.e., how do we think of this
deformation from a neural network perspective? In parameter space, the Gaussian partition
function is
ZG[J] =
Z
dθPG(θ) e
R ddxJ(x)φθ(x),
(4.6)
We remind the reader that in such a case Gaussianity is not obvious, but requires a judicious
choice of parameter density P(θ) and architecture φθ(x) such that we have a neural network
Gaussian process via the central limit theorem. In parameter space, the deformation yields
Z[J] =
Z
dθPG(θ) e−λ R ddx1...ddxr Oφθ(x1,...,xr)e
R ddxJ(x)φθ(x),
(4.7)
37

where we assume that where the operator Oφθ doesn’t involve an explicit φ(x), but instead
its parameter space representation; we will exemplify this momentarily. Again, correlators
may be computed in perturbation theory in λ by expanding and evaluating in the Gaussian
expectation, this time in the parameter space formulation.
We emphasize that if the function space and parameter space descriptions (4.3) and
(4.6) represent the same partition function, then the deformed theories (4.4) and (4.7) are
the same theory. That is, we see how an arbitrary deformation of the action induces an
associated deformation of the parameter space description. We will use this in Section 4.2
to engineer φ4 theory as a neural network ﬁeld theory, and in 4.1 we will more explicitly
deform a neural network Gaussian process.
We end our general discussion with some theoretical considerations in neural network
ﬁeld theory, interpreting a non-Gaussian deformation Oφθ in terms of the framework of
Section 2, and also taking into account the universal approximation theorem.
A non-Gaussian deformation Oφθ must violate an assumption of the CLT. The archi-
tecture itself is still the same φθ(x) as in the Gaussian theory. Instead, in (4.7) we may
interpret the operator insertion as
P(θ) := PG(θ) e−λ R ddx1...ddxr Oφθ(x1,...,xr),
(4.8)
i.e., same architecture, but with a deformed parameter distribution. This makes it clear that
our non-Gaussian theory is still at inﬁnite-N and therefore cannot receive non-Gaussianities
in 1/N-corrections. Instead, it receives non-Gaussianities because the deformed parameter
distribution has independence breaking via the non-trivial relationship amongst the pa-
rameters in the deformation. There may also exist schemes for controlling non-Gaussian
deformations in 1/N, instead of via independence breaking, but it is beyond our scope.
Was it inevitable that systematic control over non-Gaussianities arises most naturally
via independence breaking rather than 1/N-corrections? The general answer is not clear,
but we may use the control over non-Gaussianities to yield common theories, such as φ4
theory in the next section. In that context we may ask a related question: was it inevitable
that we obtain common interacting theories via independence breaking rather than 1/N
corrections? This question has a better answer. Finite action conﬁgurations of a common
theory, say φ4 theory
S[φ] =
Z
ddx

φ(x)(∇2 + m2)φ(x) + λ
4! φ(x)4

,
(4.9)
are not arbitrary functions, since there may be some functions φ(x) that have inﬁnite action.
38

However, ﬁnite action conﬁgurations are still fairly general functions, and since they have
ﬁnite action they occur with non-zero probability in the ensemble.
On the other hand, there are universal approximation theorems for neural networks,
where the error in the approximation to a target function may decrease with increasing N.
In such a case this theorem that is usually cited as a feature in ML may actually be a bug:
at ﬁnite-N there exist functions that can’t be explicitly realized by a ﬁxed architecture,
but only approximated. We therefore ﬁnd it reasonable to expect that there is at least one
ﬁnite-action conﬁguration φ(x) in φ4 theory that cannot be realized by a ﬁnite-N neural
network of ﬁxed architecture; in such a case, a NN-FT realization of φ4 theory must be
at inﬁnite-N. This comment only scratches the surface, but we ﬁnd the interplay between
universal approximation theorems and realizable ﬁeld theories at ﬁnite-N to be worthy of
further study.
4.1. Non-Gaussian Deformation of a Neural Network Gaussian Process
To make the general picture more concrete, we would like to consider non-Gaussian defor-
mations of any neural network Gaussian process. The main result is that we may deform
any NNGP by any operator we like, which breaks independence by deforming the parameter
density, explaining the origin of non-Gaussianities by violating the independence.
As before, we consider a ﬁeld built out of neurons,
φθ(x) =
1
√
N
N
X
i=1
aihi(x)
(4.10)
where the full set of parameters θ is realized by the set of parameters ai and the set of
parameters θh of the post-activations or neurons h. This equation forms the ﬁeld out of a
linear output layer with weights ai acting on the post-activations, which could themselves
be considered as the N-dimensional output of literally any neural network. If the reader
wishes, one may take φ to be a single-layer network by further choosing
hi(x) = σ(bijxj + ci)
(4.11)
with σ : R →R a non-linear activation function such as ReLU or tanh; with this additional
choice we now have θh comprised of b-parameters and c-parameters. Taking the parameter
densities PG(a) and PG(θh) to be independent and N →∞, φ(x) = φθ(x) is drawn from
a Gaussian process; we have again used a subscript G to emphasize that these are the
parameter densities of the Gaussian theory.
39

Deforming the Gaussian theory by an operator insertion, which in general is non-
Gaussian, we have
Z[J] =
Z
da dθh PG(a)PG(θh) e
−λ R ddx1...ddxr Oφa,θh (x1,...,xr)e
R ddxJ(x)φθ(x).
(4.12)
We may interpret the operator insertion as deforming the independent Gaussian parameter
density PG(a)PG(θh) to a non-trivial joint density
P(a, θh) = PG(a)PG(θh) e
−λ
R
ddx1...ddxr Oφa,θh (x1,...,xr).
(4.13)
The partition function is then
Z[J] =
Z
da dθh P(a, θh) e
R
ddxJ(x)φθ(x),
(4.14)
an inﬁnite-N non-Gaussian NN-FT where the operator insertion deforms the parameter
density. At initialization, if one draws the parameters θh ﬁrst, one may think of this as
aﬀecting the density from which the a-parameters are drawn; the draws of a-parameters
are no longer independent.
For the sake of concreteness, consider the case of the single-layer network and take a
general non-local quartic deformation. Then the operator insertion is
e−R ddx1...ddx4 g4(x1,...x4) φa,b,c(x1)...φa,b,c(x4),
(4.15)
where Einstein summation is implied and we have absorbed the overall λ into the deﬁnition
of the non-local coupling g4(x1, . . . , x4). Inserting the equation for the neural network
φa,b,c(x) =
1
√
N
aiσ(bijxj + ci),
(4.16)
into the deformation, we obtain
e−R ddx1...ddx4 g4(x1,...x4) ai1...ai4σ(bi1j1xj1+ci1)...σ(bi4j4xj4+ci4)/N2,
(4.17)
which deﬁnes a deformed parameter density
P(a, b, c) = PG(a)PG(b)PG(c) e−R ddx1...ddx4 g4(x1,...x4) ai1...ai4σ(bi1j1xj1+ci1)...σ(bi4j4xj4+ci4)/N2.
(4.18)
Then
Z[J] =
Z
da db dc P(a, b, c) e
R
ddxJ(x) aiσ(bijxj+ci)/
√
N
(4.19)
is the partition function of a inﬁnite-N NN-FT, as we impose lim N →∞, with quartic
non-Gaussianity induced by the breaking of independence in the joint parameter density
P(a, b, c).
40

4.2. φ4 Theory as a Neural Network Field Theory
To end this section and demonstrate the power of this technique, we would like to engineer
the ﬁrst interacting theory that any student learns: local φ4 theory. The action is
S[φ] =
Z
ddx

φ(x)(∇2 + m2)φ(x) + λ
4! φ(x)4

.
(4.20)
Following our prescription, we
• Engineer the NNGP. Using the result of [10], we take
φa,b,c(x) =
X
i
ai cos(bijxj + ci)
p
b2
i + m2
,
(4.21)
where the sum runs from 1 to N = ∞, bi is the vector that is the ith row of the
matrix bij, and the parameter densities of the Gaussian theory are
PG(a) =
Y
i
e
−N
2σ2a aiai
(4.22)
PG(b) =
Y
i
PG(bi) with PG(bi) = Unif(Bd
Λ)
(4.23)
PG(c) =
Y
i
PG(ci) with PG(ci) = Unif([−π, π]),
(4.24)
where Bd
Λ is a d-sphere of radius Λ. The density PG(bi) is not independent in the
vector index j, but all that is needed for Gaussianity is independence in the i index,
which is clear due to the product nature of PG(b).
The power spectrum (Fourier-
transform of the two-point function) is
G(2)(p) = σ2
a(2π)d
2 vol(Bd
Λ)
1
p2 + m2,
(4.25)
which becomes the standard free scalar result 1/(p2 + m2) by a trivial rescaling
φa,b,c(x) =
s
2 vol(Bd
Λ)
σ2a(2π)d
X
i,j
ai cos(bijxj + ci)
p
b2
i + m2
.
(4.26)
This neural network Gaussian process is equivalent to the free scalar theory of mass
m in d Euclidean dimensions, with
G(2)(p) =
1
p2 + m2,
(4.27)
where Λ plays the role of a hard UV cutoﬀon the momentum.
41

• Introduce the Operator Insertion. Given the NNGP above, or any other NNGP
realizing the free scalar ﬁeld theory, we wish to insert the operator
e−λ
4!
R
ddx φa,b,c(x)4,
(4.28)
associated to a local φ4 interaction.
• Absorb the Operator into a Parameter Density Deformation. The non-Gaussian
operator insertion deforms the parameter density to
P(a, b, c) = PG(a)PG(b)PG(c) e−λ
4!
R
ddx φa,b,c(x)4,
(4.29)
where for φa,b,c(x) it is to be understood that the RHS of (4.26) is inserted, yielding
an expression that is only a function of a’s, b’s, and c’s.
• Write the Partition Function. We then have a partition function for the deformed
theory, given by
Z[J] =
Z
da db dc P(a, b, c) e
R
ddxJ(x) φa,b,c(x),
(4.30)
where again it is to be understood that we insert the RHS of (4.26) for φa,b,c and
(4.29) for P(a, b, c); there are no explicit ﬁelds in the expression, it depends only on
the architecture (which includes parameters a,b,c) and the joint parameter density.
Thus, the architecture (4.26) and parameter density (4.29) realize local φ4 theory via the
partition function (4.30). We discuss the connections between Gaussian Processes, locality,
and translation invariance in Appendix (E).
Let us brieﬂy address RG ﬂows. The deﬁnition of a ﬁxed non-Gaussian theory here
involves the choice of a ﬁxed value of λ, in addition to the choice of a ﬁxed value of Λ that
was implicit in the ﬁxing of the GP. From that starting point, decreasing Λ while keeping
the correlators ﬁxed induces an RG ﬂow for λ governed by the usual Callan-Symanzik
equation. In the language of the neural network architecture, this is interpreted as a ﬂow
in the parameter density that is necessary to ﬁx the correlators as Λ is decreased.
4.3. Cluster Decomposition and Space Independence Breaking
We now turn to a weaker notion of locality: cluster decomposition. Given a ﬁeld φ(x) (or
neural network in our context) we say that it satisﬁes cluster decomposition if all connected
correlation functions G(r)
c (x1, . . . , xr) asymptote to zero in the limit where the separation
between any two space points xi, xj, i ̸= j is taken to ∞,
lim
|xi−xj|→∞G(r)
c (x1, . . . , xr) = 0.
(4.31)
42

If the probability density function of φ has the form
P(φ) = 1
Z exp

−
Z
dx L

x, φ(x), ∂φ
∂x, . . . , ∂nφ
∂xn

(4.32)
where Z is a normalization constant and n is ﬁnite, we say that φ(x) has a local Lagrangian
density. This is a stronger notion of locality compared to cluster decomposition, as any
theory with a local Lagrangian density satisﬁes cluster decomposition, but the converse is
not true [44].
Checking whether a theory satisﬁes cluster decomposition requires knowledge of the
asymptotic behavior of correlation functions, but not the probability density function. As
calculating the probability density function of an NN-FT is more challenging than com-
puting the correlation functions, checking cluster decomposition is easier than determining
whether there exists a local Lagrangian density that describes the system.
The main result we describe in this section is a framework that enables engineering
neural network architectures that satisfy cluster decomposition.
Space Independent Field Theory
We will perform our analysis by studying, and then moving away from, a case with a very
strong assumption: ﬁeld theories that are deﬁned by ﬁelds that have independent statistics
at diﬀerent space (or space) points xI.
We call these ﬁeld theories space independent
(SI) ﬁeld theories.
While one can still view such ﬁelds as random functions deﬁned on
a continuously diﬀerentiable space, in general the ﬁeld conﬁgurations are discontinuous;
avoiding this would require statistical correlations between nearest neighbors, violating the
assumption. This “d-dimensional” ﬁeld theory is really a collection of uncountably many
independent 0-d theories. This means that the partition function factorizes
ZφSI[J] = E[e
R
ddxIJ(xI)φ(xI)] =
Y
I
EφSI[eJ(xI)φ(xI)],
(4.33)
where the product runs over all space points xI. This form is agnostic about the origin
of the statistics and may be speciﬁed in either the function space or parameter space
description. In parameter space, independent statistics at diﬀerent space points xI means
that the SI theory has partition function
ZφSI[J] =
Y
i
Z
dθI PI(θI) eJ(xI) φθI (xI),
(4.34)
i.e. each space point xI has its own ensemble of neural networks φθI(xI) with its own set of
parameters θI that is independent of θJ for I ̸= J. In function space, independence means
43

that
ZφSI[J] =
Y
I
Z
DφI e−S[φ(XI)]+J(xI)φ(xI),
(4.35)
i.e., the action is such that the path integral factorizes. An immediate consequence of this
factorization is that the action cannot contain derivatives of φ(xI), as these would depend
on the value of φ not only at point xI, but a local neighborhood around it. Then, the
action is of the form,
S(φ(xI)) = V [φ(xI)],
(4.36)
which, turning the product into a sum in the exponent, gives the more canonical form
ZφSI[J] =
Z  Y
I
DφI
!
e−
R
ddxI (V [φ(XI)]−J(xI)φ(xI)).
(4.37)
This is a ﬁeld theory with a potential, but no derivatives. The ﬁeld values at diﬀerent points
of space are independent random variables. If they are identically distributed V [φ(xI)] is
ﬁxed ∀I and the diﬀerent factors in ZSI[J] enjoy an SL permutation symmetry, where the
number of space points L is inﬁnite in the continuum limit.
Before introducing correlations between the ﬁeld values at diﬀerent space points, let us
ﬁrst study the statistics of the SI theory. Denote the cumulants of φ at a given point xI
as κφ
r(xI). For simplicity, we will assume that the ﬁeld values at diﬀerent space points are
identically distributed, i.e. κφ
r(xI) = κφ
r is ﬁxed for all I, which will also be important for
translation invariance. We also assume that they are mean free, κφ
1 = 0. Next, we consider
the cumulant generating functional, which takes the form
WφSI[J] = log

ZφSI[J]

= log
Y
I
EφSI[eJ(xI)φ(xI)]

,
=
Z
dx log

EφSI[eJ(x)φ(x)]

,
=
Z
dx W[J; x],
(4.38)
where W[J; x] is the CGF of φ at space point x.
Just as the partition function Z[J]
factorizes into a product of partition functions associated to individual space points, the
CGF W[J] = log Z[J] becomes a sum (or integral, in this case). The connected correlators
44

are easily computed by taking derivatives 5 , where ∂J(xI)/∂J(xJ) = δ(xI −xJ),
G(n)
c (x1, . . . , xn) =
 n
Y
I=1
∂
∂J(xI)

WφSI[J],
=
Z
dx
 n
Y
I=1
∂
∂J(xI)

W[J; x].
(4.39)
and the connected correlation functions of SI networks φSI simpliﬁes to
G(n)
c (x1, . . . , xn) =
Z
dx

∂
∂J(x)
n
W[J; x]
n
Y
I=1
δ(x −xI),
=
Z
dx κφ
n
n
Y
I=1
δ(x −xI),
(4.40)
with n delta functions. The n-point connected correlator is nonzero only when x1 = x2 =
· · · = xn, and its magnitude is determined by κφ
n.
The correlation functions can be written in terms of the connected correlators.
For
example, the two point function of φ is
G(2)(x1, x2) = Eφ[φ(x1)φ(x2)],
= κφ
2 δ(x1 −x2) + (κφ
1)2
= κφ
2 δ(x1 −x2).
(4.41)
As φ(x1) and φ(x2) are independent and mean free, G(2)
φSI(x1, x2) is nonzero only when
x1 = x2. Similarly, the four point function is
G(4)(x1, x2, x3, x4) = κφ
4 δ(x1 −x2)δ(x1 −x3)δ(x1 −x4) + (κφ
2)2
δ(x1 −x2)δ(x3 −x4)
+ δ(x1 −x3)δ(x2 −x4) + δ(x1 −x4)δ(x2 −x3)

.
(4.42)
The statistics of the theory is completely determined by the space independence assumption
and the cumulants κφ
r . The general n-point function can be expressed as
G(n)(x1, . . . , xn) =
X
α∈Sn
Y
r∈α
G(r)
φSI,c(x1, . . . , xn),
(4.43)
where Sn denotes partitions of the set {1, . . . , n}.
5We remind the reader that space derivatives are ill-deﬁned, as φ(x) is discontinuous everywhere. However,
derivatives with respect to J(xI) are still well deﬁned.
45

Space-Time Independence Breaking
Clearly we don’t want to stop with space independent theories. We will now introduce
correlations between diﬀerent space points to ‘stitch together’ the L 0-dimensional theories
(associated to the space independent ﬁelds) into a d-dimensional ﬁeld theory. This requires
modifying the theory in some way so that there are non-trivial correlations between ﬁeld
values at diﬀerent points.
One way to do so is to deﬁne new ﬁeld variables Φ(xI) as a function of the SI ﬁelds
φ(xI),
Φ(xI) = Φ

φ(x1), . . . , φ(xL)

.
(4.44)
As the value of Φ at site xI in principle depends on the values of φ at all space points, Φ(xI)
and Φ(xJ) are correlated in general, even when I ̸= J. The statistics of Φ(xI) are then
determined by the functional form of (4.44), as well as the statistics of φ(xI). However,
such a general formulation (4.44) is unwieldy, and we therefore simplify the picture.
We will describe a family of architectures where Φ(xI) is constructed by a simpler
ans¨atz, a smearing of φ(a)a∈{x1,··· ,xL} across all space points, and write down a necessary
and suﬃcient condition to satisfy cluster decomposition. Consider the architecture,
Φ(xI) =
Z ∞
−∞
da f(xI −a) φ(a)
(4.45)
for some continuous and diﬀerentiable function f(xI −a).
First, note that although a
generic draw of φ(a) is discontinuous due to independence across diﬀerent points in space,
Φ(xI) is rendered continuous by the smearing. Furthermore, if the function f is nonzero
everywhere, Φ(xI) will have correlations between all pairs of lattice sites.
We wish to check whether cluster decomposition is satisﬁed, and therefore need to
compute correlation functions of Φ(x). The Φ-correlators are given by
G(n)
Φ (x1, . . . , xn) = Eφ

Φ(x1) · · ·Φ(xn)

,
= Eφ
h
n
Y
i=1
Z
dai f(xi −ai) φ(ai)
i
.
(4.46)
As f does not depend on φ, we can carry out the expectation value over φ to obtain
G(n)
Φ (x1, . . . , xn) =
Z
n
Y
i=1
dai f(xi −ai) G(n)
φ (a1, . . . , an),
(4.47)
where G(n)
φ (a1, . . . , an) is the n-point correlation function of φ. The only contribution to
the connected correlator of Φ(x) comes from the connected piece of G(n)
φ (a1, . . . , an) with
46

n delta functions6,
G(n)
c (x1, . . . , xn) = κφ
n
Z
dx
n
Y
i=1
dai f(xi −ai) δ(x −ai).
(4.48)
Evaluating the integral, we obtain
G(n)
c (x1, . . . , xn) = κφ
n
Z
dx
n
Y
i
f(xi −x).
(4.49)
Cluster decomposition is satisﬁed if and only if (4.49) asymptotes to zero in the limit where
the separation between any two of the space points xI, xJ is taken to ∞. Any smearing
function f(x) that decays faster than 1/x asymptotically satisﬁes this condition.7
Example: Gaussian Smearing
We now present an example with a particular choice of the smearing function f and show
that the resulting theory satisﬁes cluster decomposition. Let
f(x) = e−x2
β ,
(4.50)
Φ(x) =
Z
da e−(x−a)2
β
φ(a),
(4.51)
for some β > 0. As before, we will consider a case where φ(x) at diﬀerent space points
are identically distributed, with cumulants κn
φ.8 Following equation (4.49), the cumulants
of Φ(x) are then given by
G(n)
c (x1, . . . , xn) = κφ
n
Z
dx
n
Y
i=1
e−(xi−x)2
β
,
= κφ
n
r
πβ
n exp
h
Mijxixj/β
i
,
(4.52)
where
Mij =



2
n −2,
if i = j
2
n,
otherwise
(4.53)
This matrix is negative semideﬁnite, with eigenvalues λ1 = · · · = λn−1 = −β/2, λn =
0, and the eigenvector corresponding to λn is (1, · · · , 1).
Consequently, the cumulant
6The remaining terms factorize and do not contribute to the connected correlator.
7Note that the SI theory automatically satisﬁes cluster decomposition as the connected correlator, c.f.
(4.40), vanishes unless all space points coincide.
8As φ(x) are identically distributed, the cumulants do not depend on the space coordinates x.
47

G(n)
c (x1, . . . , xn) vanishes when any of the xi are taken to be large, unless they coincide
x1 = · · · = xn. This theory thus satisﬁes cluster decomposition.
The dependence of the connected correlators (4.52) on the space coordinates xi is com-
pletely determined by the choice of smearing function f, while their magnitudes depend
both on f as well as the cumulants κn
φ. Although our main motivation here has been to en-
gineer neural network architectures that satisfy cluster decomposition, smearing layers oﬀer
great ﬂexibility in manipulating the connected correlators and might be useful in designing
neural networks with other desired properties.
5. Conclusions
In this paper we continued the development of neural network ﬁeld theory (NN-FT), a new
approach to ﬁeld theory in which a theory is speciﬁed by a neural network architecture
and a parameter density. This description enables a parameter space description of the
statistics, yielding a diﬀerent method for computing correlation functions.
For a more
detailed introduction to NN-FT, see the introduction and references therein.
We focused on three foundational aspects of NN-FT: non-Gaussianity, actions, and
locality. Via the central limit theorem (CLT), many architectures admit an N →∞limit in
which the associated NN-FT is Gaussian, i.e., a generalized free ﬁeld theory. In the machine
learning literature, these are called neural network Gaussian processes (NNGPs). In Section
2 we demonstrated that interactions arise from parametrically violating assumptions of the
CLT, yielding non-Gaussianities arising from 1/N-corrections, as well as the breaking of
statistical independence and the identicalness assumption. These interactions are apparent
via parameter-space calculations of connected correlation functions, but manifest themselves
as non-Gaussianities in the ﬁeld density P[φ] = exp(−S[φ]). In Section 3 we developed
a technique that allows for the action to be computed from the connected correlation
functions, via connected Feynman diagrams. This is an inversion of the usual approach
in ﬁeld theory: we compute coupling functions in terms of connected correlators, rather
than the other way around. The technique was applied to NN-FT, including an analysis
involving the parametric non-Gaussianities we studied.
In Section 4 we studied how to
design architectures that realize a given action. We do so by deforming an NNGP by an
operator insertion that, from a function-space perspective, corresponds to a deformation
of the GP action.
However, since we know the architecture we may also express the
deformation in parameter space, in which case the non-Gaussianity associated to a given
deformation of the action has a natural interpretation as a deformation of the neural
48

network parameter density. That is, the interactions arise from independence breaking. We
apply this technique to induce local interactions, and derive an architecture that realizes
φ4 theory as an inﬁnite NN-FT.
Acknowledgements. We thank Sergei Gukov, Mathis Gerdes, Jessica Howard, Ro Jeﬀer-
son, Gowri Kurup, Joydeep Naskar, Fabian Ruehle, Jiahua Tian, Jacob Zavatone-Veth, and
Kevin Zhang for discussions. This work is supported by the National Science Foundation
under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artiﬁcial Intelli-
gence and Fundamental Interactions). This work was performed in part at Aspen Center
for Physics, which is supported by National Science Foundation grant PHY-2210452. A.M.
thanks ECT* and the ExtreMe Matter Institute EMMI at GSI, Darmstadt, for support in
the framework of an ECT* Workshop during which part of this work has been completed.
J.H. is supported by NSF CAREER grant PHY-1848089.
Appendix A. Continuum Hermite Polynomials
Let us ﬁrst recall the deﬁnition of continuum Hermite polynomials for convenience,
H(φ, x1, · · · , xn) = (−1)neSG
δ
δφ(x1) · · ·
δ
δφ(xn)e−SG.
(A.1)
Deﬁning
Si =
δSG
δφ(xi),
Si,j =
δ2SG
δφ(xi)δφ(xj),
(A.2)
the ﬁrst six Hermite polynomials are,
H1(φ, x1) = S1,
H2(φ, x1, x2) = S1S2 −S1,2,
H3(φ, x1, x2, x3) = S1S2S3 −S1,2S3[3],
H4(φ, x1, x2, x3, x4) = S1S2S3S4 −S1,2S3S4[6] + S1,2S3,4[3],
H5(φ, x1, x2, x3, x4, x5) = S1S2S3S4S5 −S1,5S2S3S4[10] + S1,2S3,4S5[15],
H6(φ, x1, x2, x3, x4, x5, x6) = S1S2S3S4S5S6 −S1,6S2S3S4S5[15]
+ S1,2S3,4S5S6[45] −S1,2S3,4S5,6[15],
(A.3)
where the square brackets denote sums over all terms with a given index structure, for
example S1,2S3[3] = S1,2S3 + S1,3S2 + S2,3S1.
49

Appendix B. Details of Examples
ReLU-net Cumulants at Finite N, i.i.d. Parameters
Let us study the output distribution of a single hidden layer network at width N, ReLU
activation function, d = dout = 1, given by
φ(x) = W 1
i R(W 0
ijxj) where R(z) =



z, for z ≥0
0, otherwise
.
(B.1)
The parameters are sampled i.i.d., W 0 ∼N (0,
σ2
W0
d ), W 1 ∼N (0,
σ2
W1
N ), and bias = 0. The
2-pt function is G(2)
c,ReLU(x, y) = σ2
W0σ2
W1
 R(x)R(y) + R(−x)R(−y)

/2, and higher order
cumulants are
G(4)
c,ReLU(x1, · · · , x4) = 1
N
 
15σ4
W0σ4
W1
4d2
 X
j=±1
R(jx1)R(jx2)R(jx3)R(jx4)

−σ4
W0σ4
W1
4d2
 X
P(abcd)
X
j=±1
R(jxa)R(jxb)R(−jxc)R(−jxd)
!
,
(B.2)
G(6)
c,ReLU(x1, · · · , x6) = 1
N2
"
225 σ6
W0σ6
W1
2d3
 X
j=±1
R(jx1)R(jx2)R(jx3)R(jx4)R(jx5)R(jx6)

−
X
P(abcdef)
 
9 σ6
W0σ6
W1
4d3
 X
j1=±1
R(j1xa)R(j1xb)R(j1xc)R(j1xd)
 X
j2=±1
R(j2xe)R(j2xf)

−σ6
W0σ6
W1
4d3
 X
j1=±1
R(j1xa)R(j1xb)
 X
j2=±1
R(j2xc)R(j2xd)
 X
j3=±1
R(j3xe)R(j3xf)
!#
,
(B.3)
where P(abcd) denotes all combinations of non-identical a, b, c, d drawn from {1, 2, 3, 4},
and similarly for P(abcdef).
Cos-net Cumulants at Finite N, Non-i.i.d. Parameters
The output of a single hidden layer, ﬁnite N, fully connected feedforward network with
cosine activation function is given by,
φ(x) = W 1
i cos(W 0
ijxj + b0
i ).
(B.4)
50

For i.i.d. parameters, e.g. W 1 ∼N (0, σ2
W1/N), W 0 ∼N (0, σ2
W0/d), and b0 ∼Unif[−π, π],
the 2-pt function is given by G(2)
c,Cos(x, y) =
σ2
W1
2 e−1
2d σ2
W0(x−y)2. For simplicity, we focus on
the d = 1 case; the statistical independence of ﬁrst linear layer weights can be broken by
a hyperparameter αIB ≪1, then the correlated weight distribution is
P(W 0) = c exp
"
−
X
i
(W 0
i )2
2σ2
W0
+ αIB
N2
X
i1,i2
(W 0
i1)2(W 0
i2)2#
,
(B.5)
where c is a normalization constant.
The cumulative non-Gaussian eﬀects due to ﬁnite
width and non-i.i.d. parameters alter all correlation functions, including the 2-pt function
at ﬁnite width. Using perturbation theory at leading order in αIB, the 2nd and 4th cumulants
are evaluated as the following,
G(2)
c,Cos(x1, x2) = αIBσ4
W0σ2
W1e−
σ2
W0 (∆x12)2
2
2N
h  1 + σ2
W0(∆x12)2
−
 1 −5σ2
W0(∆x12)2 + σ4
W0(∆x12)4
N
i
,
(B.6)
G(4)
c,Cos(x1, x2, x3, x4) = σ4
W1
8N
X
P(abcd)
"
−2e−
σ2
W0((∆xab)2+(∆xcd)2)
2
+ 3e−
σ2
W0 (∆xab+∆xcd)2
2

+ αIBσ4
W0
N

−6e−
σ2
W0((∆xab)2+(∆xcd)2)
2
+ 3e−
σ2
W0 (∆xab+∆xcd)2
2
+ 3σ2
W0(∆xab + ∆xcd)2
e−
σ2
W0 (∆xab+∆xcd)2
2
−2σ2
W0
 (∆xab)2 + (∆xcd)2
e−
σ2
W0((∆xab)2+(∆xcd)2)
2
−2σ4
W0(∆xab)2(∆xcd)2
e−
σ2
W0((∆xab)2+(∆xcd)2)
2
#
,
(B.7)
where ∆xij := xi −xj. The Fourier transformation of this cumulant at αIB = 0 is
˜G(4)
c,Cos = 3π3/2σ4
W1
√
d
2
√
2NσW0
"
e
−
p2
1d
2σ2
W0

δd(p1 + p2)δd(p1 + p3)δd(p4 −p1) + δd(p2 −p1)δd(p1 + p3)
δd(p1 + p4) + δd(p1 + p2)δd(p3 −p1)δd(p1 + p4)
#
−πσ4
W1d
2Nσ2
W0
"
e
−
(p2
1+p2
2)d
2σ2
W0 δd(p1 + p4)δd(p2 + p3)
e
−
(p2
1+p2
2)d
2σ2
W0 δd(p1 + p3)δd(p2 + p4) + e
−
(p2
1+p2
3)d
2σ2
W0 δd(p1 + p2)δd(p3 + p4)
#
+ p1 ↔p2, p3, p4,
(B.8)
where use the convention ei(p1x1+p2x2+p3x3+p4x4).
Next, we present another example where non-Gaussianities arise due to both ﬁnite width
and non-i.i.d. parameters.
51

Gauss-net at Finite N, Non-i.i.d. Parameters
We deﬁne the Gauss-net architecture as a single hidden layer, width N, feedforward network
with exponential activation function, and an overall normalizing factor, such that the output
is
φ(x) =
W 1
i exp(W 0
ijxj + b0
i )
q
exp[2(σ2
b0 +
σ2
W0
d x2)]
.
(B.9)
For i.i.d. parameter distributions, W 0 ∼N (0,
σ2
W0
d ), W 1 ∼N (0,
σ2
W1
N ), and b0 ∼N (0, σ2
b0),
the 2-pt function is G(2)
c,Gauss(x, y) =
σ2
W1
2 e−1
2d σ2
W0(x−y)2, identical as Cos-net. We break the
statistical independence of the ﬁrst linear layer weights similar to the previous example, at
d = 1. Then, the 2nd and 4th order cumulants at leading order in αIB are,
G(2)
c,Gauss(x1, x2) = −αIBσ4
W0σ2
W1
2N
e−
σ2
W0 (∆x12)2
2
h  σ2
W0X2
12 −1

−
 1 + 5σ2
W0X2
12 + σ4
W0X4
12

N
i
,
(B.10)
and,
G(4)
c,Gauss(x1, x2, x3, x4) =
3σ4
W1 exp

−
σ2
W0(x2
1−2x1(x2+X34)+x2
2−2x2X34+(∆x34)2)
2

4N
+ αIBσ4
W0σ4
W1
4N2
 
3 exp

−σ2
W0 (x2
1 −2x1(x2 + X34) + x2
2 −2x2X34 + (∆x34)2)
2

−3σ2
W0(X12 + X34)2 exp

−σ2
W0 (x2
1 −2x1(x2 + X34) + x2
2 −2x2X34 + (∆x34)2)
2

−
X
P(abcd)

3 e−
σ2
W0((∆xab)2+(∆xcd)2)
2
−σ2
W0
 X2
ab + X2
cd

e−
σ2
W0((∆xab)2+(∆xcd)2)
2
+ σ4
W0X2
ab X2
cd e−
σ2
W0((∆xab)2+(∆xcd)2)
2
!
−
X
P(abcd)
σ4
W1
4N e−
σ2
W0((∆xab)2+(∆xcd)2)
2
,
(B.11)
where Xij := xi + xj, and ∆xij := xi −xj.
52

At αIB = 0, the Fourier transformation of this cumulant becomes the following
˜G(4)
c,Gauss =
π3/2σ4
W1
2
√
2N2d3/2σW0
"
e
−
p2
1d
2σ2
W0 (d2N −dp2
1σ2
W0 + 2σ4
W0)

δd(p1 + p2)δd(p1 + p3)
δd(p4 −p1) + δd(p2 −p1)δd(p1 + p3)δd(p1 + p4) + δd(p1 + p2)δd(p3 −p1)δd(p1 + p4)
#
−
πσ4
W1
2N2σ2
W0d
"
 d2(N + p2
1p2
2) −2d(p2
1 + p2
2)σ2
W0 + 6σ4
W0

e
−
(p2
1+p2
2)d
2σ2
W0 δd(p1 + p4)δd(p2 + p3)
e
−
(p2
1+p2
2)d
2σ2
W0 δd(p1 + p3)δd(p2 + p4)

+
 d2(N + p2
1p2
3) −2d(p2
1 + p2
3)σ2
W0 + 6σ4
W0

e
−
(p2
1+p2
3)d
2σ2
W0
δd(p1 + p2)δd(p3 + p4)
#
+ p1 ↔p2, p3, p4,
(B.12)
using the same convention as Cos-net.
Non-Gaussianity from Non-Identical Parameter Distributions
We discussed examples of NN architectures where non-Gaussianities arise at various widths,
from the choice of identical but correlated parameter distributions. In addition to this, it
is possible to violate CLT through independently drawn dissimilar NN parameter distri-
butions; this too induces non-Gaussianities in NN output distributions.
Let us present
an architecture where non-Gaussianities at inﬁnite width limit arise due to dissimilar and
independent parameter distributions. Consider the NN architecture with output
φ(xk) =
N
X
j=−N
e−j2
σ2 W L
j hL−1
j
(xk) + bL,
(B.13)
with parameters drawn from W 1 ∼N (0, σ2
WL), bL ∼N (0, σ2
bL), and hL−1
j
(xk) denotes the
output of jth neuron in (L−1)th hidden layer, from input xk. The presence of the prefactor
e−j2
σ2 at the jth node of ﬁnal linear layer leads to dissimilarities in the ﬁnal layer parameter
distributions. Let us study the ﬁrst three leading order cumulants at lim N →∞,
G(2)
c (x1, x2) = lim
N→∞
N
X
j=−N
e−2j2
σ2 σ2
WLE[hL−1
j
(x1)hL−1
j
(x2)] =
rπ
2 σ σ2
WLE[h(x1)h(x2)],
(B.14)
G(4)
c (x1, · · · , x4) =
rπ
4 σ σ4
WL
h
3 E[h(x1) · · ·h(x4)] −
X
P(abcd)
E[h(xa)h(xb)]E[h(xc)h(xd)]
i
,
(B.15)
53

and
G(6)
c (x1, x2, x3, x4, x5, x6) =
rπ
6 σ σ6
WL
h
15 E[h(x1)h(x2)h(x3)h(x4)h(x5)h(x6)] −3
X
P(abcdef)

E[h(xa)h(xb)h(xc)h(xd)]E[h(xe)h(xf)] −2 E[h(xa)h(xb)]E[h(xc)h(xd)]E[h(xe)h(xf)]
i
.
(B.16)
We used h(x) := hL−1(x), and identities E[(W L
j )6] = 15 σ6
W1, E[(W L
j )4] = 3 σ4
W1. All these
cumulants are nonvanishing at lim N →∞; similarly, one can show that other higher order
cumulants are non-vanishing too, adding non-Gaussianities to the output distribution.
Appendix C. CGF and Edgeworth Expansion for NNFT
We express the output of a single hidden layer width N neural network as a sum over N
continuous variables
φ(x) =
1
√
N
N
X
i=1
hi(x),
(C.1)
where hi(x) are the outputs of each neuron before they get summed up into the ﬁnal
output.
Finite N and I.I.D. Parameters
The cumulant generating functional for i.i.d. param-
eters P(h; ⃗α = ⃗0) =
NQ
i=1
Pi(hi) become the following
Wφ(x)[J] = log E
h
e
1
√
N
N
P
i=1
R dxJ(x)hi(x)i
= log
h N
Y
i=1
Z
Dhi Pi(hi) exp
 1
√
N
Z
dxJ(x)hi(x)
i
=N log
h
∞
X
r=0
rY
i=1
Z
dxi
G(r)
hi (x1, · · · , xr)J(x1) · · · J(xr)
r!Nr/2
i
=
∞
X
r=0
rY
i=1
Z
dxi
G(r)
c,hi(x1, · · · , xr)J(x1) · · ·J(xr)
r!Nr/2−1
(C.2)
54

where J(x) and hi(x) are the source current and output of ith neuron, respectively. In the
second last step, we have used the following relation,
∞
X
r=0
rY
i=1
Z
dxi
G(r)
hi (x1, · · · , xr)J(x1) · · ·J(xr)
r!Nr/2
= e
∞
P
r=0
1
r!Nr/2
R  rQ
i=1
dxi

G(r)
c,hi(x1,··· ,xr)J(x1)···J(xr)
.
(C.3)
Lastly, we use W[J] =
∞
P
r=0
 rQ
i=1
R
dxi
 G(r)
c
(x1,··· ,xr)J(x1)···J(xr)
r!
to obtain
G(r)
c (x1, · · · , xr)J(x1) · · ·J(xr) =
G(r)
c,hi(x1, · · · , xr)J(x1) · · ·J(xr)
Nr/2−1
,
(C.4)
with a N-scaling of cumulants, as expected.
Correlated Parameters at Finite N
Let ⃗α = {α1, · · · , αq} be parameters breaking
statistical independence between neurons. For small ⃗α, one can write
P(h; ⃗α) = P(h; ⃗α = ⃗0) +
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · ·αsr
r!
∂αs1 · · · ∂αsr P(h; ⃗α)

⃗α=0 .
(C.5)
One can deﬁne the rth derivative as ∂αs1 · · · ∂αsr P(h; ⃗α) = P(h; ⃗α)Pr,{s1,··· ,sr}; the recursive
relation satisﬁed by Pr,{s1,··· ,sr} is
Pr+1,{s1,··· ,sr+1} =
1
r + 1
r+1
X
γ=1
(P1,sγ + ∂αsγ )Pr,{s1,··· ,sr+1}\sγ.
(C.6)
With this, the NN parameter distribution can be expressed as
P(h; ⃗α) = P(h; ⃗α = ⃗0) +
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · · αsr
r!
P(h; ⃗α)Pr,{s1,··· ,sr}

⃗α=0
(C.7)
Next, let us derive the CGF for the NN functional distribution,
W⃗α[J] = log
" Z
Dh P(h; ⃗α) e
1
√
N
N
P
i=1
R
dx hi(x)J(x)
#
= log
" N
Y
i=1
EPi(hi)
"
1 +
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · · αsr
r!
Pr,{s1,··· ,sr}

⃗α=0

e
1
√
N
R
dx hi(x)J(x)
##
= log
"
eWfree[J] +
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · ·αsr
r!
N
Y
i=1
EPi(hi)
h
e
1
√
N
R
dx hi(x)J(x) · Pr,{s1,··· ,sr}

⃗α=0
i#
. (C.8)
55

The last line is obtained using
N
Y
i=1
EPi(hi)
h
e
1
√
N
R
dx J(x)hi(x)i
= exp

N
∞
X
r=0
Z
rY
i=1
dxi
G(r)
c,hi(x1, · · · , xr)J(x1) · · · J(xr)
r!Nr/2−1

= eWfree[J].
(C.9)
At lim N →∞, we obtain Wfree[J] =
R
dx1dx2
J(x1) G(2)
c,hi(x1,x2) J(x2)
2
.
The partition function of a ﬁeld theory is related to its CGF as
Z[J(x)] = E[ei
R
J(x)φ(x)] =
N
Y
i=1
Z
DhPi(hi) e
i
√
N
R
dxJ(x) hi(x).
(C.10)
Under the transformation J →iJ, the CGF becomes,
W[J] =
∞
X
r=1
Z
rY
j=1
dxj
ir
r!G(r)
c (x1, · · · , xr)J(x1) · · ·J(xr) =:
∞
X
r=1
Z
rY
j=1
dxj
ir
r!G(r)
c Jr,
(C.11)
the inverse Fourier transform of which, up to renormalization, is
P[φ] ∝
Z
DJ eW [J]−i
R
dxJ(x)φ(x)
=
Z
DJ e
∞
P
r=1
R dx1···dxr ir
r! G(r)
c
(x1,··· ,xr) Jr −i R dxJ(x)φ(x)
=
Z
DJ e
∞
P
r=3
R
dx1···dxr ir
r! G(r)
c
(x1,··· ,xr) Jre−i R dx J(x)φ(x) e
i
R
dx1 G(1)
c
(x1) J1
1!
−
R
dx1dx2 G(2)
c
(x1,x2) J2
2!
=
Z
DJ e
∞
P
r=3
R
dx1···dxr
(−1)r
r!
G(r)
c
(x1,··· ,xr) ∂re−i
R
dx J(x)φ(x)
ei
R
dx1 J(x1)G(1)
c
(x1)−1
2
R
dx1 dx2 J(x1)G(2)
c
(x1,x2)J(x2),
(C.12)
where ∂r =
δ
δφ(x1) · · ·
δ
δφ(xr). Next, we evaluate the integral associated with the Gaussian
process,
Z
DJe−i R dx1 J(x1)φ(x1)+i R dx1 J(x1)G(1)
c
(x1)−1
2
R dx1 dx2 J(x1)G(2)
c
(x1,x2)J(x2),
(C.13)
using a change of variables J′(x) →J(x) + i
R
dx′ G(2)
c (x, x′)−1[φ(x′) −G(1)
c (x′)] that keeps
56

the measure of the source DJ →DJ′ invariant. We obtain
−SG = −i
Z
dx J(x)[φ(x) −G(1)
c (x)] −1
2
Z
dx1dx2 J(x1) G(2)
c (x1, x2) J(x2)
= −1
2
Z
dx1 dx2

J(x1) + i
Z
dx′ G(2)
c (x1, x′)−1[φ(x′) −G(1)
c (x′)]

G(2)
c (x1, x2)

J(x2)
+ i
Z
dx′′ G(2)
c (x2, x′′)−1[φ(x′′) −G(1)
c (x′′)]

−1
2
Z
dx′′ dx′ dx1 dx2 [φ(x′′) −G(1)
c (x′′)]
G(2)
c (x′′, x2)−1 G(2)
c (x2, x1)G(2)
c (x1, x′)−1 [φ(x′) −G(1)
c (x′)]
= −1
2
Z
dx1 dx2 J′(x1) G(2)
c (x1, x2) J′(x2) −1
2
Z
dx dx′ [φ(x) −G(1)
c (x)] G(2)
c (x, x′)−1
[φ(x′) −G(1)
c (x′)]
(C.14)
An integration over J′ results in the distribution
e−1
2
R
dx dx′ [φ(x)−G(1)
c
(x)] G(2)
c
(x,x′)−1 [φ(x′)−G(1)
c
(x′)],
such that
P[φ] = e
∞
P
r=3
R dx1···dxr
(−1)r
r!
G(r)
c
(x1,··· ,xr) ∂re−1
2
R dx dx′ [φ(x)−G(1)
c
(x)] G(2)
c
(x,x′)−1 [φ(x′)−G(1)
c
(x′)].
(C.15)
We obtain perturbative corrections around the Gaussian ﬁeld density by expanding the ﬁrst
exponential term in (C.15) as a series; contributions from higher order cumulants become
increasingly less signiﬁcant in most cases.
4-pt Function at Finite N, Non-i.i.d. Parameters
Next, we evaluate the 4-pt func-
tion of this NNFT with the following cumulant generating functional
Wφ[J] = log
"
eWφ,⃗α=0[J]+
∞
X
r=1
q
X
s1,··· ,sr=1
αs1 · · · αsr
r!
N
Y
i=1
EPi(hi)
h
e
1
√
N
R ddx hi(x)J(x)·Pr,{s1,··· ,sr}

⃗α=0
i#
.
(C.16)
For appropriately small ⃗α, the ratio of the second term in the logarithm to the ﬁrst is
small, and one can Taylor expand log(1 + x) ≈x to obtain,
Wφ[J] = Wφ,⃗α=0[J] +
q
X
s=1
αs
eWφ,⃗α=0[J]
N
Y
i=1
EPi(hi)
h
e
1
√
N
R
ddx hi(x)J(x) · P1,s

⃗α=0
i
.
(C.17)
The 4-pt function is obtained as G(4)
c (x1, · · · , x4) =
∂4Wφ[J]
∂J(x1)···∂J(x4)

J=0. We abbreviate
M =
q
X
s=1
αs
eWφ,⃗α=0[J]
N
Y
i=1
EPi(hi)
h
e
1
√
N
R ddx hi(x)J(x) · P1,s

⃗α=0
i
,
(C.18)
57

then, G(4)
c (x1, · · · , x4) =
∂4Wφ,⃗α=0[J]
∂J(x1)···∂J(x4)

J=0 +
∂4M
∂J(x1)···∂J(x4)

J=0.
Next, we evaluate the fourth J-derivative of M and turn the source J oﬀ,
∂4M
∂J1 · · · ∂J4

J=0
=
q
X
s=1
αs
eWφ,⃗α=0[J]
 N
Y
i=1
EPi(hi)
h Z
ddx1 · · · ddx4
hi(x1) · · · hi(x4)
N2
e
1
√
N
R ddx hi(x)J(x)P1,s

⃗α=0
i
+
X
P(abce)
"∂Wφ,⃗α=0[J]
∂Ja
∂Wφ,⃗α=0[J]
∂Jb
−∂2Wφ,⃗α=0[J]
∂Ja∂Jb
 N
Y
i=1
EPi(hi)
h Z
ddxcddxe
hi(xc)hi(xe)
N
· e
1
√
N
R ddx hi(x)J(x) · P1,s

⃗α=0
i
−
∂Wφ,⃗α=0[J]
∂Ja
∂Wφ,⃗α=0[J]
∂Jb
∂Wφ,⃗α=0[J]
∂Jc
−∂2Wφ,⃗α=0[J]
∂Ja∂Jb
· ∂Wφ,⃗α=0[J]
∂Jc
+ ∂3Wφ,⃗α=0[J]
∂Ja∂Jb∂Jc
 N
Y
i=1
EPi(hi)
h Z
ddxe
hi(xe)
√
N
e
1
√
N
R ddx hi(x)J(x)P1,s

⃗α=0
i
+
∂Wφ,⃗α=0[J]
∂J1
∂Wφ,⃗α=0[J]
∂J2
∂Wφ,⃗α=0[J]
∂J3
∂Wφ,⃗α=0[J]
∂J4
+ ∂2Wφ,⃗α=0[J]
∂Ja∂Jb
∂2Wφ,⃗α=0[J]
∂Jc∂Je
−∂Wφ,⃗α=0[J]
∂Ja
∂Wφ,⃗α=0[J]
∂Jb
∂2Wφ,⃗α=0[J]
∂Jc∂Je
+ ∂3Wφ,⃗α=0[J]
∂Ja∂Jb∂Jc
∂Wφ,⃗α=0[J]
∂Je
−∂4Wφ,⃗α=0[J]
∂J1∂J2∂J3∂J4

N
Y
i=1
EPi(hi)
h
e
1
√
N
R
ddx hi(x)J(x)P1,s

⃗α=0
i
−
N
Y
i=1
EPi(hi)
h Z
ddxbddxcddxe
hi(xb)hi(xc)hi(xe)
N3/2
e
1
√
N
R ddx hi(x)J(x)P1,s

⃗α=0
i∂Wφ,⃗α=0[J]
∂Ja
#!
J=0
:= ⃗α · ∆G(4)
c (x1, · · · , x4),
where we use the abbreviation J(xi) := Ji. In the mean-free case,
∂Wφ,⃗α=0[J]
∂Ja

J=0 = ∂3Wφ,⃗α=0[J]
∂Ja∂Jb∂Jc

J=0 = 0,
(C.19)
∂4Wφ,⃗α=0[J]
∂J1∂J2∂J3∂J4

J=0 = G(4),i.i.d.
c
(x1, · · · , x4),
∂2Wφ,⃗α=0[J]
∂Ja∂Jb

J=0 = G(2),i.i.d.
c
(xa, xb).
(C.20)
58

Thus, the 4-pt function is
G(4)
c (x1, · · · , x4) = G(4),i.i.d.
c
(x1, · · · , x4)
+
q
X
s=1
αs
eWφ,⃗α=0[J=0]
 N
Y
i=1
EPi(hi)
h Z
ddx1 · · · ddx4
hi(x1) · · · hi(x4)
N2
P1,s

⃗α=0
i
+
X
P(abce)
h
−G(2),i.i.d.
c
(xa, xb)
N
Y
i=1
EPi(hi)
h Z
ddxcddxe
hi(xc)hi(xe)
N
P1,s

⃗α=0
i
+

G(2),i.i.d.
c
(xa, xb)G(2),i.i.d.
c
(xc, xe) −G(4),i.i.d.
c
(x1, · · · , x4)
 N
Y
i=1
EPi(hi)
h
P1,s

⃗α=0
ii!
,
= G(4),i.i.d.
c
(x1, · · · , x4) + ⃗α · ∆G(4)
c (x1, · · · , x4) + O(⃗α2).
(C.21)
at leading order.
Appendix D. Fourier Transformation Trick for G(2)
c (x, y)−1
Let us evaluate the expression
Z
dy1 · · ·dyn G(n)
c (y1, · · · , yn) G(2)
c (y1, x1)−1 · · ·G(2)
c (yn, xn)−1,
(D.1)
when G(2)
c (yi, xi)−1 involves diﬀerential operators. The integrals over yi cannot be directly
evaluated as the eigenvalues of each G(2)
c (yi, xi)−1 are unknown. To avoid this problem, we
substitute the operators and cumulant with their Fourier transformations,
Z
ddy1 · · · ddyn ddp1 · · ·ddpn ddq1 · · · ddqn ddr1 · · · ddrn ˜G(n)
c (p1, · · · , pn) ˜G(2)
c (q1, r1)−1
· · · ˜G(2)
c (qn, rn)−1 eiy1(p1+q1)+ir1x1···+iy1(pn+qn)+irnxn
=
Z
ddp1 · · · ddpn ddr1 · · · ddrn ˜G(n)
c (p1, · · · , pn) ˜G(2)
c (−p1, r1)−1 · · · ˜G(2)
c (−pn, rn)−1e
i
n
P
j=1
rjxj
.
(D.2)
Here ˜f is the Fourier transformation of f, and we obtained the second line by evaluating
yi integrals to get δd(pi + qi), then integrating qi variables.
When G(2)
c
is translation invariant, we have G(2)
c (yi, xi)−1 ∝δd(yi−xi), leading to further
simpliﬁcation of the above expression as,
Z
ddp1 · · · ddpn ˜G(n)
c (p1, · · · , pn) ˜G(2)
c (−p1)−1 · · · ˜G(2)
c (−pn)−1e−ip1x1···−ipnxn.
(D.3)
We exemplify this expression for Cos-net and Gauss-net architectures.
59

Appendix E. Gaussian Processes: Locality and Translation Invariance
Any Gaussian process (GP) can be described as a function space distribution given by
action S,
S =
Z
dx dy f(x) G(2)
c (x, y)−1 f(y),
(E.1)
where G(2)
c (x, y)−1 is the precision function, related to the GP kernel by the inversion
formula
Z
dy G(2)
c (x, y)−1 K(y, z) = δ(x −z).
(E.2)
A local GP can be deﬁned as a family of functions with a completely diagonalizable preci-
sion operator, resulting in the action
S =
Z
dx f(x) G(2)
c (x)−1 f(x),
(E.3)
with the inversion relation simpliﬁed into
G(2)
c (x)−1 K(x, z) = δ(x −z).
(E.4)
This can be seen by considering G(2)
c (x, y)−1 = δ(x −y)Σ(x) and performing the integral
over y in Eqn. (E.2). A Gaussian process can always be written in a local basis, as we
will show below.
Gaussian Process Action in the local basis. Any Gaussian Process f(x), when eval-
uated at a discrete set of inputs {xi}i, forms a multivariate Gaussian distribution. The
covariance matrix of a multivariate Gaussian is a real symmetric matrix, and thus can be di-
agonalized. We can use this diagonalization procedure on the Gaussian process distribution
itself, thereby rewriting it with a kernel proportional to a Dirac delta function,
S = −1
2
Z
ddxi ddxl f(xi) G(2)
c (xi, xl)−1 f(xl),
= −1
2
Z
ddxiddxjddxkddxlf(xi)V (xi, xj)D(xj, xk)V −1(xk, xl)f(xl),
= −1
2
Z
ddxk
Z
ddxi V (xi, xk)f(xk)

Σ(xk)
Z
ddxl V −1(xk, xl)f(xl)

,
= −1
2
Z
ddx φT(x)Σ(x)φ(x),
(E.5)
where φ(x) :=
R
ddy f(y)V (y, x) and last step of (E.5) is obtained by xk −→x. D(x, y) is
deﬁned as D(xi, xl) = δ(xi −xl)Σ(xi) =
R
ddxjddxk V −1(xi, xj)G(2)
c (xj, xk)−1V (xk, xl).
60

References
[1] Y. LeCun,
Y. Bengio & G. Hinton,
“Deep learning”,
Nature 521, 436 (2015),
https://doi.org/10.1038/nature14539 ;
I. J. Goodfellow, Y. Bengio & A. Courville, “Deep Learning”, MIT Press (2016), Cam-
bridge, MA, USA, http://www.deeplearningbook.org.
[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser
& I. Polosukhin, “Attention is All you Need”, in “Advances in Neural Information Pro-
cessing Systems”, ed: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan & R. Garnett, Curran Associates, Inc. (2017);
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driess-
che, T. Graepel & D. Hassabis, “Mastering the game of Go without human knowledge”,
Nature 550, 354 (2017), https://doi.org/10.1038/nature24270 .
[3] G.
Carleo,
I.
Cirac,
K.
Cranmer,
L.
Daudet,
M.
Schuld,
N.
Tishby,
L.
Vogt-Maranto
&
L.
Zdeborov´a,
“Machine
learn-
ing
and
the
physical
sciences”,
Rev. Mod. Phys. 91, 045002 (2019),
https://link.aps.org/doi/10.1103/RevModPhys.91.045002 .
[4] J. Cariﬁo, J. Halverson, D. Krioukov & B. Nelson, “Machine Learning in the String
Landscape”, Journal of High Energy Physics 2017, (2017);
S.
Gukov,
J.
Halverson,
F.
Ruehle
&
P.
Su lkowski,
“Learning
to
unknot”,
Machine Learning: Science and Technology 2, 025035 (2021),
https://dx.doi.org/10.1088/2632-2153/abe91f ;
A. Davies, P. Veliˇckovi´c, L. Buesing, S. Blackwell, D. Zheng, N. Tomaˇsev, R. Tanburn,
P. Battaglia,
C. Blundell,
A. Juh´asz,
M. Lackenby,
G. Williamson,
D. Hass-
abis & P. Kohli, “Advancing mathematics by guiding human intuition with AI”,
Nature 600, 70 (2021), https://doi.org/10.1038/s41586-021-04086-x ;
S. Gukov, J. Halverson, C. Manolescu & F. Ruehle, “Searching for ribbons with
machine learning”, arXiv preprint arXiv:2304.09304 600, F. Ruehle (2023).
[5] R. M. Neal, “BAYESIAN LEARNING FOR NEURAL NETWORKS” .
[6] A. G. de G. Matthews, M. Rowland, J. Hron, R. E. Turner & Z. Ghahramani,
“Gaussian Process Behaviour in Wide Deep Neural Networks”, ArXiv abs/1804.11271,
Z. Ghahramani (2018);
R. Novak, L. Xiao, J. Lee, Y. Bahri, D. A. Abolaﬁa, J. Pennington & J. Sohl-Dickstein,
“Bayesian Convolutional Neural Networks with Many Channels are Gaussian Processes”,
61

ArXiv abs/1810.05148, J. Sohl (2018);
A. Garriga-Alonso, L. Aitchison & C. E. Rasmussen, “Deep Convolutional Networks as
shallow Gaussian Processes”, ArXiv abs/1808.05587, C. E. Rasmussen (2019).
[7] G. Yang, “Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Pro-
cess Behavior, Gradient Independence, and Neural Tangent Kernel Derivation”, ArXiv
abs/1902.04760, G. Yang (2019);
G. Yang, “Tensor Programs I: Wide Feedforward or Recurrent Neural Networks
of Any Architecture are Gaussian Processes”,
arXiv
e-prints
abs/1902.04760,
arXiv:1910.12478 (2019), arXiv:1910.12478 [cs.NE];
G. Yang, “Tensor Programs II: Neural Tangent Kernel for Any Architecture”, ArXiv
abs/2006.14548, G. Yang (2020).
[8] A. Maiti, K. Stoner & J. Halverson, “Symmetry-via-Duality: Invariant Neural Network
Densities from Parameter-Space Correlators”, arXiv:2106.00694 [cs.LG].
[9] C. K. Williams, “Computing with inﬁnite networks”, in “Advances in neural information
processing systems”, 295–301.
[10] J.
Halverson,
“Building
Quantum
Field
Theories
Out
of
Neurons”,
arXiv:2112.04527 [hep-th].
[11] G.
Naveh,
O.
B.
David,
H.
Sompolinsky
&
Z.
Ringel,
“Pre-
dicting
the
outputs
of
ﬁnite
deep
neural
networks
trained
with
noisy
gradients”,
Physical Review E 104, Z. Ringel (2021),
https://doi.org/10.1103/Fphysreve.104.064301 .
[12] J.
Halverson,
A.
Maiti
&
K.
Stoner,
“Neural Networks and Quantum Field
Theory”,
Machine Learning: Science and Technology 104, K. Stoner (2021),
http://dx.doi.org/10.1088/2632-2153/abeca3 .
[13] K. Fukushima, “Cognitron: A self-organizing multilayered neural network”, Biologi-
cal Cybernetics 20, 121 (1975);
K. Fukushima, “Neocognitron: A self-organizing neural network model for a mechanism
of pattern recognition unaﬀected by shift in position”, Biological Cybernetics 36, 193
(1980);
D. E. Rumelhart, G. E. Hinton & R. J. Williams, “Learning internal representations
by error propagation” .
[14] Y. LeCun, L. Bottou, Y. Bengio & P. Haﬀner, “Gradient-based learning applied to
document recognition”, Proceedings of the IEEE 86, 2278 (1998);
62

Y. LeCun, P. Haﬀner, L. Bottou & Y. Bengio, “Object recognition with gradient-based
learning”, in “Shape, contour and grouping in computer vision”, Springer (1999), 319–
345.
[15] J. Bruna, W. Zaremba, A. Szlam & Y. LeCun, “Spectral networks and locally connected
networks on graphs”, arXiv preprint arXiv:1312.6203 86, Y. LeCun (2013);
M. Henaﬀ, J. Bruna & Y. LeCun, “Deep convolutional networks on graph-structured
data”, arXiv preprint arXiv:1506.05163 86, Y. LeCun (2015);
D.
K.
Duvenaud,
D.
Maclaurin,
J.
Iparraguirre,
R.
Bombarell,
T.
Hirzel,
A.
Aspuru-Guzik
&
R.
P.
Adams,
“Convolutional
Networks
on
Graphs
for
Learning
Molecular
Fingerprints”,
in
“Advances
in
Neural
Informa-
tion Processing Systems 28”,
ed:
C.
Cortes,
N.
D.
Lawrence,
D.
D.
Lee,
M.
Sugiyama
&
R.
Garnett,
Curran
Associates,
Inc.
(2015),
2224–2232,
http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecu
;
Y. Li, D. Tarlow, M. Brockschmidt & R. Zemel, “Gated graph sequence neural
networks”, arXiv:1511.05493 [cs.LG];
M. Deﬀerrard, X. Bresson & P. Vandergheynst, “Convolutional neural networks
on graphs with fast localized spectral ﬁltering”, in “Advances in neural information
processing systems”, 3844–3852;
T. N. Kipf & M. Welling, “Semi-supervised classiﬁcation with graph convolutional
networks”, arXiv preprint arXiv:1609.02907 86, M. Welling (2016).
[16] K. He, X. Zhang, S. Ren & J. Sun, “Deep Residual Learning for Image Recognition”,
in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)”;
G. Huang, Z. Liu, L. van der Maaten & K. Q. Weinberger, “Densely Connected Con-
volutional Networks”, arXiv:1608.06993 [cs.CV].
[17] D. Bahdanau, K. Cho & Y. Bengio, “Neural machine translation by jointly learning to
align and translate”, arXiv preprint arXiv:1409.0473 86, Y. Bengio (2014);
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser &
I. Polosukhin, “Attention is all you need”, in “Advances in neural information processing
systems”, 5998–6008.
[18] S. Ioﬀe & C. Szegedy, “Batch normalization: Accelerating deep network training by
reducing internal covariate shift”, arXiv preprint arXiv:1502.03167 86, C. Szegedy
(2015);
63

J.
L.
Ba,
J.
R.
Kiros
&
G.
E.
Hinton,
“Layer
Normalization”,
arXiv:1607.06450 [stat.ML].
[19] J. Hron, Y. Bahri, J. Sohl-Dickstein & R. Novak, “Inﬁnite attention: NNGP and
NTK for deep attention networks”, in “International Conference on Machine Learning”,
4376–4386.
[20] E. Dinan, S. Yaida & S. Zhang, “Eﬀective Theory of Transformers at Initialization”,
arXiv:2304.02034 [cs.LG].
[21] S. Yaida, “Non-Gaussian processes and neural networks at ﬁnite widths”, ArXiv
abs/1910.00019, S. Yaida (2019).
[22] J. M. Antognini, “Finite size corrections for neural network Gaussian processes”,
arXiv:1908.10030 [cs.LG].
[23] D. A. Roberts, S. Yaida & B. Hanin, “The Principles of Deep Learning Theory”, Cam-
bridge University Press (2022).
[24] E. Dyer & G. Gur-Ari, “Asymptotics of Wide Networks from Feynman Diagrams”,
ArXiv abs/1909.11304, G. Gur (2020).
[25] J. Erdmenger,
K. T. Grosvenor & R. Jeﬀerson, “Towards quantifying informa-
tion ﬂows: relative entropy in deep neural networks and the renormalization group”,
arXiv:2107.06898 [hep-th];
K. T. Grosvenor & R. Jeﬀerson, “The edge of chaos: quantum ﬁeld theory and deep
neural networks”, arXiv:2109.13247 [hep-th].
[26] H.
Erbin,
V.
Lahoche
&
D.
O.
Samary,
“Non-perturbative
renormalization
for
the
neural
network-QFT
correspon-
dence”,
Machine Learning: Science and Technology 3, 015027 (2022),
https://doi.org/10.1088/F2632-2153/Fac4f69 ;
H. Erbin, V. Lahoche & D. O. Samary, “Renormalization in the neural network-
quantum ﬁeld theory correspondence”, arXiv:2212.11811 [hep-th].
[27] I. Banta, T. Cai, N. Craig & Z. Zhang, “Structures of Neural Network Eﬀective Theo-
ries”, arXiv:2305.02334 [hep-th].
[28] A. Jacot, F. Gabriel & C. Hongler, “Neural Tangent Kernel: Convergence and Gener-
alization in Neural Networks”, in “NeurIPS”.
[29] S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov & R. Wang, “On Exact Computa-
tion with an Inﬁnitely Wide Neural Net”, arXiv:1904.11955 [cs.LG].
64

[30] S. S. Du, K. Hou, B. P´oczos, R. Salakhutdinov, R. Wang & K. Xu, “Graph
Neural Tangent Kernel:
Fusing Graph Neural Networks with Graph Kernels”,
arXiv:1905.13192 [cs.LG].
[31] S. Alemohammad, Z. Wang, R. Balestriero & R. Baraniuk, “The Recurrent Neural
Tangent Kernel”, arXiv:2006.10246 [cs.LG];
S. Alemohammad, R. Balestriero, Z. Wang & R. Baraniuk, “Enhanced Recurrent Neu-
ral Tangent Kernels for Non-Time-Series Data”, arXiv:2012.04859 [cs.LG].
[32] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein & J. Penning-
ton, “Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient
Descent”, ArXiv abs/1902.06720, J. Pennington (2019).
[33] J. Huang & H.-T. Yau, “Dynamics of Deep Neural Networks and Neural Tangent Hier-
archy”, in “Proceedings of the 37th International Conference on Machine Learning”, ed:
H. D. III & A. Singh, 4542–4551, PMLR (2020);
K. Aitken & G. Gur-Ari, “On the asymptotics of wide networks with polynomial acti-
vations”, arXiv:2006.06687 [cs.LG].
[34] B. Bordelon & C. Pehlevan, “Dynamics of Finite Width Kernel and Prediction Fluctu-
ations in Mean Field Neural Networks”, arXiv:2304.03408 [stat.ML].
[35] J. Zavatone-Veth, A. Canatar, B. Ruben & C. Pehlevan, “Asymptotics of representa-
tion learning in ﬁnite Bayesian neural networks”, in “Advances in Neural Information
Processing Systems”, ed: M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang & J. W.
Vaughan, 24765–24777, Curran Associates, Inc. (2021).
[36] I. Seroussi, G. Naveh & Z. Ringel, “Separation of Scales and a Thermodynamic De-
scription of Feature Learning in Some CNNs”, arXiv:2112.15383 [stat.ML].
[37] S. Krippendorf & M. Spannowsky, “A duality connecting neural network and cosmolog-
ical dynamics”, arXiv:2202.11104 [gr-qc].
[38] K.
Fischer,
A.
Ren´e,
C.
Keup,
M.
Layer,
D.
Dahmen
&
M.
Helias,
“Decomposing
neural
networks
as
mappings
of
cor-
relation
functions”,
Physical Review Research 4, M. Helias (2022),
http://dx.doi.org/10.1103/PhysRevResearch.4.043143 ;
M. Dick, A. van Meegen & M. Helias, “Linking Network and Neuron-level Correlations
by Renormalized Field Theory”, arXiv:2309.14973 [cond-mat.dis-nn];
H.
Huang,
“Mechanisms
of
dimensionality
reduction
and
decorrela-
65

tion
in
deep
neural
networks”,
Physical Review E 98, H. Huang (2018),
http://dx.doi.org/10.1103/PhysRevE.98.062313 .
[39] M. Albergo, G. Kanwar & P. Shanahan, “Flow-based generative models for Markov
chain Monte Carlo in lattice ﬁeld theory”, Physical Review D 100, P. Shanahan (2019),
https://doi.org/10.1103%2Fphysrevd.100.034515 ;
R. Abbott, M. S. Albergo,
D. Boyda,
K. Cranmer,
D. C. Hackett,
G. Kan-
war,
S.
Racani`ere,
D.
J.
Rezende,
F.
Romero-L´opez,
P.
E.
Shanahan,
B. Tian & J. M. Urban,
“Gauge-equivariant ﬂow models for sampling in lat-
tice ﬁeld theories with pseudofermions”, Physical Review D 106, J. M. Urban (2022),
https://doi.org/10.1103%2Fphysrevd.106.074506 ;
M.
Gerdes,
P.
de
Haan,
C.
Rainone,
R.
Bondesan
&
M.
C.
N.
Cheng,
“Learning Lattice Quantum Field Theories with Equivariant Continuous Flows”,
arXiv:2207.00283 [hep-lat].
[40] K.
Osterwalder
&
R.
Schrader,
“Axioms
for
Euclidean
Green’s
functions”,
Communications in Mathematical Physics 31, 83 (1973),
https://doi.org/10.1007/BF01645738 .
[41] J. Dedecker, P. Doukhan, G. Lang, L. R. Jos´e Rafael, S. Louhichi & C. Prieur, “Central
Limit theorem”, in “Weak Dependence: With Examples and Applications”, 153–197,
Springer New York (2007), New York, NY.
[42] B. Hanin, “Random Fully Connected Neural Networks as Perturbatively Solvable Hier-
archies”, arXiv:2204.01058 [math.PR].
[43] P. McCullagh, “Tensor methods in statistics”, Chapman and Hall (1987), London [u.a.]
.
[44] S. Weinberg, “The Quantum Theory of Fields”, Cambridge University Press (1995).
66

