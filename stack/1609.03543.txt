arXiv:1609.03543v5  [cs.AI]  7 Dec 2020
Logical Induction
Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor
{scott,tsvi,critch,nate,jessica}@intelligence.org
Machine Intelligence Research Institute
Abstract
We present a computable algorithm that assigns probabilities to every logical
statement in a given formal language, and reﬁnes those probabilities over time.
For instance, if the language is Peano arithmetic, it assigns probabilities to
all arithmetical statements, including claims about the twin prime conjecture,
the outputs of long-running computations, and its own probabilities. We show
that our algorithm, an instance of what we call a logical inductor, satisﬁes a
number of intuitive desiderata, including: (1) it learns to predict patterns
of truth and falsehood in logical statements, often long before having the
resources to evaluate the statements, so long as the patterns can be written
down in polynomial time; (2) it learns to use appropriate statistical summaries
to predict sequences of statements whose truth values appear pseudorandom;
and (3) it learns to have accurate beliefs about its own current beliefs, in a
manner that avoids the standard paradoxes of self-reference. For example, if
a given computer program only ever produces outputs in a certain range, a
logical inductor learns this fact in a timely manner; and if late digits in the
decimal expansion of π are diﬃcult to predict, then a logical inductor learns
to assign ≈10% probability to “the nth digit of π is a 7” for large n. Logical
inductors also learn to trust their future beliefs more than their current beliefs,
and their beliefs are coherent in the limit (whenever φ →ψ, P∞(φ) ≤P∞(ψ),
and so on); and logical inductors strictly dominate the universal semimeasure
in the limit.
These properties and many others all follow from a single logical induction
criterion, which is motivated by a series of stock trading analogies. Roughly
speaking, each logical sentence φ is associated with a stock that is worth $1
per share if φ is true and nothing otherwise, and we interpret the belief-state
of a logically uncertain reasoner as a set of market prices, where Pn(φ) = 50%
means that on day n, shares of φ may be bought or sold from the reasoner
for 50¢. The logical induction criterion says (very roughly) that there should
not be any polynomial-time computable trading strategy with ﬁnite risk tol-
erance that earns unbounded proﬁts in that market over time. This criterion
bears strong resemblance to the “no Dutch book” criteria that support both
expected utility theory (von Neumann and Morgenstern 1944) and Bayesian
probability theory (Ramsey 1931; de Finetti 1937).
Contents
1
Introduction
4
1.1
Desiderata for Reasoning under Logical Uncertainty
. . . . . . . . .
5
1.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
See https://intelligence.org/ﬁles/LogicalInductionAbridged.pdf for an abridged version of
this paper.
1

2
Notation
12
3
The Logical Induction Criterion
14
3.1
Markets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.2
Deductive Processes
. . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.3
Eﬃcient Computability
. . . . . . . . . . . . . . . . . . . . . . . . .
16
3.4
Traders
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.5
Exploitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.6
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
4
Properties of Logical Inductors
21
4.1
Convergence and Coherence . . . . . . . . . . . . . . . . . . . . . . .
22
4.2
Timely Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
4.3
Calibration and Unbiasedness . . . . . . . . . . . . . . . . . . . . . .
27
4.4
Learning Statistical Patterns
. . . . . . . . . . . . . . . . . . . . . .
30
4.5
Learning Logical Relationships
. . . . . . . . . . . . . . . . . . . . .
32
4.6
Non-Dogmatism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.7
Conditionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
4.8
Expectations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
4.9
Trust in Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
4.10 Reasoning about Halting . . . . . . . . . . . . . . . . . . . . . . . . .
44
4.11 Introspection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.12 Self-Trust . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
5
Construction
49
5.1
Constructing MarketMaker
. . . . . . . . . . . . . . . . . . . . . . .
50
5.2
Constructing Budgeter
. . . . . . . . . . . . . . . . . . . . . . . . .
52
5.3
Constructing TradingFirm
. . . . . . . . . . . . . . . . . . . . . . .
54
5.4
Constructing LIA . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.5
Questions of Runtime and Convergence Rates . . . . . . . . . . . . .
57
6
Selected Proofs
58
6.1
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
6.2
Limit Coherence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
6.3
Non-dogmatism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
6.4
Learning Pseudorandom Frequencies . . . . . . . . . . . . . . . . . .
64
6.5
Provability Induction . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
7
Discussion
67
7.1
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
7.2
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
7.3
Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
7.4
Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
7.5
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
References
73
A Preliminaries
79
A.1 Organization of the Appendix . . . . . . . . . . . . . . . . . . . . . .
79
A.2 Expressible Features . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
A.3 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
B Convergence Proofs
82
B.1
Return on Investment
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
B.2
Aﬃne Preemptive Learning . . . . . . . . . . . . . . . . . . . . . . .
88
B.3
Preemptive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
B.4
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
B.5
Persistence of Aﬃne Knowledge . . . . . . . . . . . . . . . . . . . . .
92
B.6
Persistence of Knowledge
. . . . . . . . . . . . . . . . . . . . . . . .
95
2

C Coherence Proofs
95
C.1 Aﬃne Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
C.2 Aﬃne Provability Induction . . . . . . . . . . . . . . . . . . . . . . .
97
C.3 Provability Induction . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
C.4 Belief in Finitistic Consistency
. . . . . . . . . . . . . . . . . . . . .
97
C.5 Belief in the Consistency of a Stronger Theory
. . . . . . . . . . . .
98
C.6 Disbelief in Inconsistent Theories . . . . . . . . . . . . . . . . . . . .
98
C.7 Learning of Halting Patterns
. . . . . . . . . . . . . . . . . . . . . .
98
C.8 Learning of Provable Non-Halting Patterns
. . . . . . . . . . . . . .
98
C.9 Learning not to Anticipate Halting . . . . . . . . . . . . . . . . . . .
98
C.10 Limit Coherence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
C.11 Learning Exclusive-Exhaustive Relationships
. . . . . . . . . . . . .
99
D Statistical Proofs
99
D.1 Aﬃne Recurring Unbiasedness
. . . . . . . . . . . . . . . . . . . . .
99
D.2 Recurring Unbiasedness . . . . . . . . . . . . . . . . . . . . . . . . . 102
D.3 Simple Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
D.4 Aﬃne Unbiasedness From Feedback
. . . . . . . . . . . . . . . . . . 103
D.5 Unbiasedness From Feedback . . . . . . . . . . . . . . . . . . . . . . 104
D.6 Learning Pseudorandom Aﬃne Sequences . . . . . . . . . . . . . . . 105
D.7 Learning Varied Pseudorandom Frequencies . . . . . . . . . . . . . . 106
D.8 Learning Pseudorandom Frequencies . . . . . . . . . . . . . . . . . . 107
E Expectations Proofs
107
E.1
Consistent World LUV Approximation Lemma
. . . . . . . . . . . . 107
E.2
Mesh Independence Lemma . . . . . . . . . . . . . . . . . . . . . . . 108
E.3
Expectation Preemptive Learning . . . . . . . . . . . . . . . . . . . . 109
E.4
Expectations Converge . . . . . . . . . . . . . . . . . . . . . . . . . . 110
E.5
Limiting Expectation Approximation Lemma . . . . . . . . . . . . . 110
E.6
Persistence of Expectation Knowledge . . . . . . . . . . . . . . . . . 110
E.7
Expectation Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 111
E.8
Expectation Provability Induction
. . . . . . . . . . . . . . . . . . . 111
E.9
Linearity of Expectation . . . . . . . . . . . . . . . . . . . . . . . . . 111
E.10 Expectations of Indicators . . . . . . . . . . . . . . . . . . . . . . . . 112
E.11 Expectation Recurring Unbiasedness . . . . . . . . . . . . . . . . . . 112
E.12 Expectation Unbiasedness From Feedback . . . . . . . . . . . . . . . 112
E.13 Learning Pseudorandom LUV Sequences . . . . . . . . . . . . . . . . 113
F Introspection and Self-Trust Proofs
113
F.1
Introspection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
F.2
Paradox Resistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
F.3
Expectations of Probabilities
. . . . . . . . . . . . . . . . . . . . . . 115
F.4
Iterated Expectations
. . . . . . . . . . . . . . . . . . . . . . . . . . 115
F.5
Expected Future Expectations
. . . . . . . . . . . . . . . . . . . . . 115
F.6
No Expected Net Update
. . . . . . . . . . . . . . . . . . . . . . . . 116
F.7
No Expected Net Update under Conditionals . . . . . . . . . . . . . 116
F.8
Self-Trust . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
G Non-Dogmatism and Closure Proofs
118
G.1 Parametric Traders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
G.2 Uniform Non-Dogmatism
. . . . . . . . . . . . . . . . . . . . . . . . 119
G.3 Occam Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
G.4 Non-Dogmatism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
G.5 Domination of the Universal Semimeasure . . . . . . . . . . . . . . . 123
G.6 Strict Domination of the Universal Semimeasure
. . . . . . . . . . . 125
G.7 Closure under Finite Perturbations . . . . . . . . . . . . . . . . . . . 126
G.8 Conditionals on Theories . . . . . . . . . . . . . . . . . . . . . . . . . 127
3

1
Introduction
Every student of mathematics has experienced uncertainty about conjectures for
which there is “quite a bit of evidence”, such as the Riemann hypothesis or the twin
prime conjecture. Indeed, when Zhang (2014) proved a bound on the gap between
primes, we were tempted to increase our credence in the twin prime conjecture. But
how much evidence does this bound provide for the twin prime conjecture? Can we
quantify the degree to which it should increase our conﬁdence?
The natural impulse is to appeal to probability theory in general and Bayes’
theorem in particular. Bayes’ theorem gives rules for how to use observations to
update empirical uncertainty about unknown events in the physical world. However,
probability theory lacks the tools to manage uncertainty about logical facts.
Consider encountering a computer connected to an input wire and an output
wire. If we know what algorithm the computer implements, then there are two
distinct ways to be uncertain about the output. We could be uncertain about the
input—maybe it’s determined by a coin toss we didn’t see. Alternatively, we could
be uncertain because we haven’t had the time to reason out what the program does—
perhaps it computes the parity of the 87,653rd digit in the decimal expansion of π,
and we don’t personally know whether it’s even or odd.
The ﬁrst type of uncertainty is about empirical facts. No amount of thinking
in isolation will tell us whether the coin came up heads.
To resolve empirical
uncertainty we must observe the coin, and then Bayes’ theorem gives a principled
account of how to update our beliefs.
The second type of uncertainty is about a logical fact, about what a known
computation will output when evaluated. In this case, reasoning in isolation can
and should change our beliefs: we can reduce our uncertainty by thinking more
about π, without making any new observations of the external world.
In any given practical scenario, reasoners usually experience a mix of both em-
pirical uncertainty (about how the world is) and logical uncertainty (about what
that implies). In this paper, we focus entirely on the problem of managing logical
uncertainty. Probability theory does not address this problem, because probability-
theoretic reasoners cannot possess uncertainty about logical facts.
For example,
let φ stand for the claim that the 87,653rd digit of π is a 7. If this claim is true,
then (1 + 1 = 2) ⇒φ. But the laws of probability theory say that if A ⇒B then
Pr(A) ≤Pr(B). Thus, a perfect Bayesian must be at least as sure of φ as they are
that 1 + 1 = 2! Recognition of this problem dates at least back to Good (1950).
Many have proposed methods for relaxing the criterion Pr(A) ≤Pr(B) until such
a time as the implication has been proven (see, e.g, the work of Hacking [1967] and
Christiano [2014]). But this leaves open the question of how probabilities should be
assigned before the implication is proven, and this brings us back to the search for a
principled method for managing uncertainty about logical facts when relationships
between them are suspected but unproven.
We propose a partial solution, which we call logical induction. Very roughly,
our setup works as follows. We consider reasoners that assign probabilities to sen-
tences written in some formal language and reﬁne those probabilities over time.
Assuming the language is suﬃciently expressive, these sentences can say things like
“Goldbach’s conjecture is true” or “the computation prg on input i produces the
output prg(i)=0”. The reasoner is given access to a slow deductive process that
emits theorems over time, and tasked with assigning probabilities in a manner that
outpaces deduction, e.g., by assigning high probabilities to sentences that are even-
tually proven, and low probabilities to sentences that are eventually refuted, well
before they can be veriﬁed deductively. Logical inductors carry out this task in a
way that satisﬁes many desirable properties, including:
1. Their beliefs are logically consistent in the limit as time approaches inﬁnity.
2. They learn to make their probabilities respect many diﬀerent patterns in logic,
at a rate that outpaces deduction.
3. They learn to know what they know, and trust their future beliefs, while
avoiding paradoxes of self-reference.
4

These claims (and many others) will be made precise in Section 4.
A logical inductor is any sequence of probabilities that satisﬁes our logical induc-
tion criterion, which works roughly as follows. We interpret a reasoner’s probabili-
ties as prices in a stock market, where the probability of φ is interpreted as the price
of a share that is worth $1 if φ is true, and $0 otherwise (similar to Beygelzimer,
Langford, and Pennock [2012]). We consider a collection of stock traders who buy
and sell shares at the market prices, and deﬁne a sense in which traders can exploit
markets that have irrational beliefs. The logical induction criterion then says that
it should not be possible to exploit the market prices using any trading strategy
that can be generated in polynomial-time.
Our main ﬁnding is a computable algorithm which satisﬁes the logical induction
criterion, plus proofs that a variety of diﬀerent desiderata follow from this criterion.
The logical induction criterion can be seen as a weakening of the “no Dutch
book” criterion that Ramsey (1931) and de Finetti (1937) used to support standard
probability theory, which is analogous to the “no Dutch book” criterion that von
Neumann and Morgenstern (1944) used to support expected utility theory. Under
this interpretation, our criterion says (roughly) that a rational deductively limited
reasoner should have beliefs that can’t be exploited by any Dutch book strategy
constructed by an eﬃcient (polynomial-time) algorithm. Because of the analogy,
and the variety of desirable properties that follow immediately from this one crite-
rion, we believe that the logical induction criterion captures a portion of what it
means to do good reasoning about logical facts in the face of deductive limitations.
That said, there are clear drawbacks to our algorithm: it does not use its resources
eﬃciently; it is not a decision-making algorithm (i.e., it does not “think about what
to think about”); and the properties above hold either asymptotically (with poor
convergence bounds) or in the limit. In other words, our algorithm gives a theo-
retically interesting but ultimately impractical account of how to manage logical
uncertainty.
1.1
Desiderata for Reasoning under Logical Uncertainty
For historical context, we now review a number of desiderata that have been pro-
posed in the literature as desirable features of “good reasoning” in the face of logical
uncertainty. A major obstacle in the study of logical uncertainty is that it’s not
clear what would count as a satisfactory solution. In lieu of a solution, a common
tactic is to list desiderata that intuition says a good reasoner should meet. One can
then examine them for patterns, relationships, and incompatibilities. A multitude
of desiderata have been proposed throughout the years; below, we have collected a
variety of them. Each is stated in its colloquial form; many will be stated formally
and studied thoroughly later in this paper.
Desideratum 1 (Computable Approximability). The method for assigning proba-
bilities to logical claims (and reﬁning them over time) should be computable.
(See Section 5 for our algorithm.)
A good method for reﬁning beliefs about logic can never be entirely ﬁnished, be-
cause a reasoner can always learn additional logical facts by thinking for longer.
Nevertheless, if the algorithm reﬁning beliefs is going to have any hope of practical-
ity, it should at least be computable. This idea dates back at least to Good (1950),
and has been discussed in depth by Hacking (1967) and Eells (1990), among others.
Desideratum 1 may seem obvious, but it is not without its teeth. It rules out
certain proposals, such as that of Hutter et al. (2013), which has no computable
approximation (Sawin and Demski 2013).
Desideratum 2 (Coherence in the Limit). The belief state that the reasoner is
approximating better and better over time should be logically consistent.
(Discussed in Section 4.1.)
First formalized by Gaifman (1964), the idea of Desideratum 2 is that the belief
state that the reasoner is approximating—the beliefs they would have if they had
5

inﬁnite time to think—should be internally consistent. This means that, in the limit
of reasoning, a reasoner should assign Pr(φ) ≤Pr(ψ) whenever φ ⇒ψ, and they
should assign probability 1 to all theorems and 0 to all contradictions, and so on.
Desideratum 3 (Approximate Coherence). The belief state of the reasoner should
be approximately coherent. For example, if the reasoner knows that two statements
are mutually exclusive, then it should assign probabilities to those sentences that
sum to no more than 1, even if it cannot yet prove either sentence.
(Discussed in sections 4.2 and 4.5.)
Being coherent in the limit is desirable, but good deductively limited reasoning
requires approximate coherence at ﬁnite times. Consider two claims about a par-
ticular computation prg, which takes a number n as input and produces a number
prg(n) as output.
Assume the ﬁrst claim says prg(7)=0, and the second says
prg(7)=1. Clearly, these claims are mutually exclusive, and once a reasoner real-
izes this fact, they should assign probabilities to the two claims that sum to at most
1, even before they can evaluate prg(7). Limit coherence does not guarantee this:
a reasoner could assign bad probabilities (say, 100% to both claims) right up until
they can evaluate prg(7), at which point they start assigning the correct probabili-
ties. Intuitively, a good reasoner should be able to recognize the mutual exclusivity
before they’ve proven either claim. In other words, a good reasoner’s beliefs should
be approximately coherent.
Desideratum 3 dates back to at least Good (1950), who proposes a weakening of
the condition of coherence that could apply to the belief states of limited reasoners.
Hacking (1967) proposes an alternative weakening, as do Garrabrant, Fallenstein,
et al. (2016).
Desideratum 4 (Learning of Statistical Patterns). In lieu of knowledge that bears
on a logical fact, a good reasoner should assign probabilities to that fact in accordance
with the rate at which similar claims are true.
(Discussed in Section 4.4.)
For example, a good reasoner should assign probability ≈10% to the claim “the
nth digit of π is a 7” for large n (assuming there is no eﬃcient way for a reasoner
to guess the digits of π for large n). This desideratum dates at least back to Savage
(1967), and seems clearly desirable. If a reasoner thought the 10100th digit of π
was almost surely a 9, but had no reason for believing this, we would be suspicious
of their reasoning methods. Desideratum 4 is diﬃcult to state formally; for two
attempts, refer to Garrabrant, Benson-Tilsen, et al. (2016) and Garrabrant, Soares,
and Taylor (2016).
Desideratum 5 (Calibration). Good reasoners should be well-calibrated. That is,
among events that a reasoner says should occur with probability p, they should in
fact occur about p proportion of the time.
(Discussed in Section 4.3.)
Calibration as a desirable property dates back to Pascal, and perhaps farther. If
things that a reasoner says should happen 30% of the time actually wind up hap-
pening 80% of the time, then they aren’t particularly reliable.
Desideratum 6 (Non-Dogmatism). A good reasoner should not have extreme be-
liefs about mathematical facts, unless those beliefs have a basis in proof.
(Discussed in Section 4.6.)
It would be worrying to see a mathematical reasoner place extreme conﬁdence in
a mathematical proposition, without any proof to back up their belief. The virtue
of skepticism is particularly apparent in probability theory, where Bayes’ theorem
says that a probabilistic reasoner can never update away from “extreme” (0 or 1)
probabilities. Accordingly, Cromwell’s law (so named by the statistician Lindley
[1991]) says that a reasonable person should avoid extreme probabilities except
when applied to statements that are logically true or false. We are dealing with
logical uncertainty, so it is natural to extend Cromwell’s law to say that extreme
probabilities should also be avoided on logical statements, except in cases where
6

the statements have been proven true or false.
In settings where reasoners are
able to update away from 0 or 1 probabilities, this means that a good reasoner’s
beliefs shouldn’t be “stuck” at probability 1 or 0 on statements that lack proofs or
disproofs.
In the domain of logical uncertainty, Desideratum 6 can be traced back to Carnap
(1962, Sec. 53), and has been demanded by many, including Gaifman and Snir (1982)
and Hutter et al. (2013).
Desideratum 7 (Uniform Non-Dogmatism). A good reasoner should assign a non-
zero probability to any computably enumerable consistent theory (viewed as a limit
of ﬁnite conjunctions).
(Discussed in Section 4.6.)
For example the axioms of Peano arithmetic are computably enumerable, and if we
construct an ever-growing conjunction of these axioms, we can ask that the limit
of a reasoner’s credence in these conjunctions converge to a value bounded above
0, even though there are inﬁnitely many conjuncts. The ﬁrst formal statement of
Desideratum 7 that we know of is given by Demski (2012), though it is implic-
itly assumed whenever asking for a set of beliefs that can reason accurately about
arbitrary arithmetical claims (as is done by, e.g., Savage [1967] and Hacking [1967]).
Desideratum 8 (Universal Inductivity). Given enough time to think, the beliefs
of a good reasoner should dominate the universal semimeasure.
(Discussed in Section 4.6.)
Good reasoning in general has been studied for quite some time, and reveals some
lessons that are useful for the study of good reasoning under deductive limitation.
Solomonoﬀ(1964a, 1964b), Zvonkin and Levin (1970), and Li and Vitányi (1993)
have given a compelling formal treatment of good reasoning assuming logical om-
niscience in the domain of sequence prediction, by describing an inductive process
(known as a universal semimeasure) with a number of nice properties, including (1)
it assigns non-zero prior probability to every computable sequence of observations;
(2) it assigns higher prior probability to simpler hypotheses; and (3) it predicts as
well or better than any computable predictor, modulo a constant amount of error.
Alas, universal semimeasures are uncomputable; nevertheless, they provide a formal
model of what it means to predict sequences well, and we can ask logically uncer-
tain reasoners to copy those successes. For example, we can ask that they would
perform as well as a universal semimeasure if given enough time to think.
Desideratum 9 (Approximate Bayesianism). The reasoner’s beliefs should admit
of some notion of conditional probabilities, which approximately satisfy both Bayes’
theorem and the other desiderata listed here.
(Discussed in Section 4.7.)
Bayes’ rule gives a fairly satisfying account of how to manage empirical uncertainty
in principle (as argued extensively by Jaynes [2003]), where beliefs are updated by
conditioning a probability distribution. As discussed by Good (1950) and Glymour
(1980), creating a distribution that satisﬁes both coherence and Bayes’ theorem
requires logical omniscience.
Still, we can ask that the approximation schemes
used by a limited agent be approximately Bayesian in some fashion, while retaining
whatever good properties the unconditional probabilities have.
Desideratum 10 (Introspection). If a good reasoner knows something, she should
also know that she knows it.
(Discussed in Section 4.11.)
Proposed by Hintikka (1962), this desideratum is popular among epistemic logicians.
It is not completely clear that this is a desirable property. For instance, reasoners
should perhaps be allowed to have “implicit knowledge” (which they know without
knowing that they know it), and it’s not clear where the recursion should stop
(do you know that you know that you know that you know that 1 = 1?).
This
desideratum has been formalized in many diﬀerent ways; see Christiano et al. (2013)
and Campbell-Moore (2015) for a sample.
7

Desideratum 11 (Self-Trust). A good reasoner thinking about a hard problem
should expect that, in the future, her beliefs about the problem will be more accurate
than her current beliefs.
(Discussed in Section 4.12.)
Stronger than self-knowledge is self-trust—a desideratum that dates at least back to
Hilbert (1902), when mathematicians searched for logics that placed conﬁdence in
their own machinery. While Gödel, Kleene, and Rosser (1934) showed that strong
forms of self-trust are impossible in a formal proof setting, experience demonstrates
that human mathematicians are capable of trusting their future reasoning, relatively
well, most of the time. A method for managing logical uncertainty that achieves
this type of self-trust would be highly desirable.
Desideratum 12 (Approximate Inexploitability). It should not be possible to run
a Dutch book against a good reasoner in practice.
(See Section 3 for our proposal.)
Expected utility theory and probability theory are both supported in part by “Dutch
book” arguments which say that an agent is rational if (and only if) there is no way
for a clever bookie to design a “Dutch book” which extracts arbitrary amounts of
money from the reasoner (von Neumann and Morgenstern 1944; de Finetti 1937).
As noted by Eells (1990), these constraints are implausibly strong: all it takes to
run a Dutch book according to de Finetti’s formulation is for the bookie to know a
logical fact that the reasoner does not know. Thus, to avoid being Dutch booked
by de Finetti’s formulation, a reasoner must be logically omniscient.
Hacking (1967) and Eells (1990) call for weakenings of the Dutch book con-
straints, in the hopes that reasoners that are approximately inexploitable would
do good approximate reasoning. This idea is the cornerstone of our framework—in
particular, we consider reasoners that cannot be exploited in polynomial time, using
a formalism deﬁned below. See Deﬁnition 3.0.1 for details.
Desideratum 13 (Gaifman Inductivity). Given a Π1 statement φ (i.e., a universal
generalization of the form “for every x, ψ”), as the set of examples the reasoner has
seen goes to “all examples”, the reasoner’s belief in φ should approach certainty.
(Discussed below.)
Proposed by Gaifman (1964), Desideratum 13 states that a reasoner should “gener-
alize well”, in the sense that as they see more instances of a universal claim (such
as “for every x, ψ(x) is true”) they should eventually believe the universal with
probability 1. Desideratum 13 has been advocated by Hutter et al. (2013).
Desideratum 14 (Eﬃciency). The algorithm for assigning probabilities to logical
claims should run eﬃciently, and be usable in practice.
(Discussed in Section 7.1.)
One goal of understanding “good reasoning” in the face of logical uncertainty is to
design algorithms for reasoning using limited computational resources. For that,
the algorithm for assigning probabilities to logical claims needs to be not only com-
putable, but eﬃcient. Aaronson (2013) gives a compelling argument that solutions
to logical uncertainty require understanding complexity theory, and this idea is
closely related to the study of bounded rationality (Simon 1982) and eﬃcient meta-
reasoning (Russell and Wefald 1991b).
Desideratum 15 (Decision Rationality). The algorithm for assigning probabilities
to logical claims should be able to target speciﬁc, decision-relevant claims, and it
should reason about those claims as eﬃciently as possible given the computing re-
sources available.
(Discussed in
Section 7.4.)
This desideratum dates at least back to Savage (1967), who asks for an extension
to probability theory that takes into account the costs of thinking. For a method
of reasoning under logical uncertainty to aid in the understanding of good bounded
reasoning, it must be possible for an agent to use the reasoning system to reason ef-
ﬁciently about speciﬁc decision-relevant logical claims, using only enough resources
to reﬁne the probabilities well enough for the right decision to become clear. This
desideratum blurs the line between decision-making and logical reasoning; see Rus-
sell and Wefald (1991a) and Hay et al. (2012) for a discussion.
8

Desideratum 16 (Answers Counterpossible Questions). When asked questions
about contradictory states of aﬀairs, a good reasoner should give reasonable answers.
(Discussed in Section 7.4.)
In logic, the principle of explosion says that from a contradiction, anything follows.
By contrast, when human mathematicians are asked counterpossible questions, such
as “what would follow from Fermat’s last theorem being false?”, they often give
reasonable answers, such as “then there would exist non-modular elliptic curves”,
rather than just saying “anything follows from a contradiction”. Soares and Fallen-
stein (2015) point out that some deterministic decision-making algorithms reason
about counterpossible questions (“what would happen if my deterministic algorithm
had the output a vs b vs c?”). The topic of counterpossibilities has been studied by
philosophers including Cohen (1990), Vander Laan (2004), Brogaard and Salerno
(2007), Krakauer (2012), and Bjerring (2014), and it is reasonable to hope that a
good logically uncertain reasoner would give reasonable answers to counterpossible
questions.
Desideratum 17 (Use of Old Evidence). When a bounded reasoner comes up with
a new theory that neatly describes anomalies in the old theory, that old evidence
should count as evidence in favor of the new theory.
(Discussed in Section 7.4.)
The problem of old evidence is a longstanding problem in probability theory (Gly-
mour 1980). Roughly, the problem is that a perfect Bayesian reasoner always uses
all available evidence, and keeps score for all possible hypotheses at all times, so no
hypothesis ever gets a “boost” from old evidence. Human reasoners, by contrast,
have trouble thinking up good hypotheses, and when they do, those new hypotheses
often get a large boost by retrodicting old evidence. For example, the precession of
the perihelion of Mercury was known for quite some time before the development
of the theory of General Relativity, and could not be explained by Newtonian me-
chanics, so it was counted as strong evidence in favor of Einstein’s theory. Garber
(1983) and Jeﬀrey (1983) have speculated that a solution to the problem of logical
omniscience would shed light on solutions to the problem of old evidence.
Our solution does not achieve all these desiderata. Doing so would be impossible;
Desiderata 1, 2, and 13 cannot be satisﬁed simultaneously. Further, Sawin and
Demski (2013) have shown that Desiderata 1, 6, 13, and a very weak form of 2
are incompatible; an ideal belief state that is non-dogmatic, Gaifman inductive,
and coherent in a weak sense has no computable approximation. Our algorithm is
computably approximable, approximately coherent, and non-dogmatic, so it cannot
satisfy 13. Our algorithm also fails to meet 14 and 15, because while our algorithm
is computable, it is purely inductive, and so it does not touch upon the decision
problem of thinking about what to think about and how to think about it with
minimal resource usage. As for 16 and 17, the case is interesting but unclear; we
give these topics some treatment in Section 7.
Our algorithm does satisfy desiderata 1 through 12. In fact, our algorithm is
designed to meet only 1 and 12, from which 2-11 will all be shown to follow. This
is evidence that our logical induction criterion captures a portion of what it means
to manage uncertainty about logical claims, analogous to how Bayesian probability
theory is supported in part by the fact that a host of good properties follow from
a single criterion (“don’t be exploitable by a Dutch book”). That said, there is
ample room to disagree about how well our algorithm achieves certain desiderata,
e.g. when the desiderata is met only in the asymptote, or with error terms that
vanish only slowly.
1.2
Related Work
The study of logical uncertainty is an old topic. It can be traced all the way back
to Bernoulli, who laid the foundations of statistics, and later Boole (1854), who
was interested in the uniﬁcation of logic with probability from the start.
Refer
to Hailperin (1996) for a historical account. Our algorithm assigns probabilities
9

to sentences of logic directly; this thread can be traced back through Łoś (1955)
and later Gaifman (1964), who developed the notion of coherence that we use in
this paper. More recently, that thread has been followed by Demski (2012), whose
framework we use, and Hutter et al. (2013), who deﬁne a probability distribution
on logical sentences that is quite desirable, but which admits of no computable
approximation (Sawin and Demski 2013).
The objective of our algorithm is to manage uncertainty about logical facts
(such as facts about mathematical conjectures or long-running computer programs).
When it comes to the problem of developing formal tools for manipulating uncer-
tainty, our methods are heavily inspired by Bayesian probability theory, and so can
be traced back to Pascal, who was followed by Bayes, Laplace, Kolmogorov (1950),
Savage (1954), Carnap (1962), and Jaynes (2003), and many others. Polya (1990)
was among the ﬁrst in the literature to explicitly study the way that mathematicians
engage in plausible reasoning, which is tightly related to the object of our study.
We are interested in the subject of what it means to do “good reasoning” under
logical uncertainty. In this, our approach is quite similar to the approach of Ramsey
(1931), de Finetti (1937), von Neumann and Morgenstern (1944), Teller (1973),
Lewis (1999), and Joyce (1999), who each developed axiomatizations of rational
behavior and produced arguments supporting those axioms.
In particular, they
each supported their proposals with Dutch book arguments, and those Dutch book
arguments were a key inspiration for our logical induction criterion.
The fact that using a coherent probability distribution requires logical omni-
science (and is therefore unsatisfactory when it comes to managing logical uncer-
tainty) dates at least back to Good (1950).
Savage (1967) also recognized the
problem, and stated a number of formal desiderata that our solution in fact meets.
Hacking (1967) addressed the problem by discussing notions of approximate coher-
ence and weakenings of the Dutch book criteria. While his methods are ultimately
unsatisfactory, our approach is quite similar to his in spirit.
The ﬂaw in Bayesian probability theory was also highlighted by Glymour (1980),
and dubbed the “problem of old evidence” by Garber (1983) in response to Glymor’s
criticism. Eells (1990) gave a lucid discussion of the problem, revealed ﬂaws in Gar-
ber’s arguments and in Hacking’s solution, and named a number of other desiderata
which our algorithm manages to satisfy. Refer to Zynda (1995) and Sprenger (2015)
for relevant philosophical discussion in the wake of Eells. Of note is the treatment of
Adams (1996), who uses logical deduction to reason about an unknown probability
distribution that satisﬁes certain logical axioms. Our approach works in precisely
the opposite direction: we use probabilistic methods to create an approximate dis-
tribution where logical facts are the subject.
Straddling the boundary between philosophy and computer science, Aaronson
(2013) has made a compelling case that computational complexity must play a role
in answering questions about logical uncertainty. These arguments also provided
some inspiration for our approach, and roughly speaking, we weaken the Dutch book
criterion of standard probability theory by considering only exploitation strategies
that can be constructed by a polynomial-time machine. The study of logical un-
certainty is also tightly related to the study of bounded rationality (Simon 1982;
Russell and Wefald 1991a; Rubinstein 1998; Russell 2016).
Fagin and Halpern (1987) also straddled the boundary between philosophy and
computer science with early discussions of algorithms that manage uncertainty in
the face of resource limitations. (See also their discussions of uncertainty and knowl-
edge [Fagin et al. 1995; Halpern 2003].) This is a central topic in the ﬁeld of artiﬁcial
intelligence (AI), where scientists and engineers have pursued many diﬀerent paths
of research. The related work in this ﬁeld is extensive, including (but not limited
to) work on probabilistic programming (Vajda 1972; McCallum, Schultz, and Singh
2009; Wood, Meent, and Mansinghka 2014; De Raedt and Kimmig 2015); proba-
bilistic inductive logic programming (Muggleton and Watanabe 2014; De Raedt and
Kersting 2008; De Raedt 2008; Kersting and De Raedt 2007); and meta-reasoning
(Russell and Wefald 1991b; Zilberstein 2008; Hay et al. 2012). The work most closely
related to our own is perhaps the work of Thimm (2013a) and others on reasoning
using inconsistent knowledge bases, a task which is analogous to constructing an
10

approximately coherent probability distribution. (See also Muiño [2011], Thimm
[2013b], Potyka and Thimm [2015], and Potyka [2015].) Our framework also bears
some resemblance to the Markov logic network framework of Richardson and Domin-
gos (2006), in that both algorithms are coherent in the limit. Where Markov logic
networks are specialized to individual restricted domains of discourse, our algorithm
reasons about all logical sentences. (See also Kok and Domingos [2005], Singla and
Domingos [2005], Tran and Davis [2008], Lowd and Domingos [2007], Mihalkova,
Huynh, and Mooney [2007], Wang and Domingos [2008], and Khot et al. [2015].)
In that regard, our algorithm draws signiﬁcant inspiration from Solomonoﬀ’s
theory of inductive inference (Solomonoﬀ1964a, 1964b) and the developments on
that theory made by Zvonkin and Levin (1970) and Li and Vitányi (1993). Indeed,
we view our algorithm as a Solomonoﬀ-style approach to the problem of reasoning
under logical uncertainty, and as a result, our algorithm bears a strong resemblance
to many algorithms that are popular methods for practical statistics and machine
learning; refer to Opitz and Maclin (1999) and Dietterich (2000) for reviews of
popular and successful ensemble methods. Our approach is also similar in spirit
to the probabilistic numerics approach of Briol, Oates, Girolami, Osborne, and
Sejdinovic (2015), but where probabilistic numerics is concerned with algorithms
that give probabilistic answers to individual particular numerical questions, we are
concerned with algorithms that assign probabilities to all queries in a given formal
language. (See also [Briol, Oates, Girolami, and Osborne 2015; Hennig, Osborne,
and Girolami 2015].)
Finally, our method of interpreting beliefs as prices and using prediction markets
to generate reasonable beliefs bears heavy resemblance to the work of Beygelzimer,
Langford, and Pennock (2012) who use similar mechanisms to design a learning
algorithm that bets on events. Our results can be seen as an extension of that idea
to the case where the events are every sentence written in some formal language,
in a way that learns inductively to predict logical facts while avoiding the standard
paradoxes of self-reference.
The work sampled here is only a small sample of the related work, and it neglects
contributions from many other ﬁelds, including but not limited to epistemic logic
(Gärdenfors 1988; Meyer and Van Der Hoek 1995; Schlesinger 1985; Sowa 1999;
Guarino 1998), game theory (Rantala 1979; Hintikka 1979; Bacharach 1994; Lipman
1991; Battigalli and Bonanno 1999; Binmore 1992), paraconsistent logic (Blair and
Subrahmanian 1989; Priest 2002; Mortensen 2013; Fuhrmann 2013; Akama and
Costa 2016) and fuzzy logic (Klir and Yuan 1995; Yen and Langari 1999; Gerla
2013). The full history is too long and rich for us to do it justice here.
1.3
Overview
Our main result is a formalization of Desideratum 12 above, which we call the logical
induction criterion, along with a computable algorithm that meets the criterion,
plus proofs that formal versions of Desiderata 2-11 all follow from the criterion.
In Section 2 we deﬁne some notation. In Section 3 we state the logical induction
criterion and our main theorem, which says that there exists a computable logical
inductor. The logical induction criterion is motivated by a series of stock trading
analogies, which are also introduced in Section 3.
In Section 4 we discuss a number of properties that follow from this crite-
rion, including properties that hold in the limit, properties that relate to pattern-
recognition, calibration properties, and properties that relate to self-knowledge and
self-trust.
A computable logical inductor is described in Section 5. Very roughly, the idea
is that given any trader, it’s possible to construct market prices at which they make
no trades (because they think the prices are right); and given an enumeration of
traders, it’s possible to aggregate their trades into one “supertrader” (which takes
more and more traders into account each day); and thus it is possible to construct
a series of prices which is not exploitable by any trader in the enumeration.
In Section 6 we give a few selected proofs. In Section 7 we conclude with a
discussion of applications of logical inductors, variations on the logical induction
11

framework, speculation about what makes logical inductors tick, and directions for
future research. The remaining proofs can be found in the appendix.
2
Notation
This section deﬁnes notation used throughout the paper. The reader is invited to
skim it, or perhaps skip it entirely and use it only as a reference when needed.
Common sets and functions. The set of positive natural numbers is denoted by
N+, where the superscript makes it clear that 0 is not included. We work with N+
instead of N≥0 because we regularly consider initial segments of inﬁnite sequences
up to and including the element at index n, and it will be convenient for those lists to
have length n. Sums written P
i≤n(−) are understood to start at i = 1. We use R to
denote the set of real numbers, and Q to denote the set of rational numbers. When
considering continuous functions with range in Q, we use the subspace topology
on Q inherited from R. We use B to denote the set {0, 1} interpreted as Boolean
values. In particular, Boolean operations like ∧, ∨, ¬, →and ↔are deﬁned on B,
for example, (1 ∧1) = 1, ¬1 = 0, and so on.
We write Fin(X) for the set of all ﬁnite subsets of X, and XN+ for all inﬁnite
sequences with elements in X. In general, we use BA to denote the set of functions
with domain A and codomain B. We treat the expression f : A →B as equivalent
to f ∈BA, i.e., both state that f is a function that takes inputs from the set A
and produces an output in the set B. We write f : A 7→B to indicate that f is a
partial function from A to B. We denote equivalence of expressions that represent
functions by ≡, e.g., (x−1)2 ≡x2 −2x+1. We write ∥−∥1 for the ℓ1 norm. When
A is an aﬃne combination, ∥A∥1 includes the trailing coeﬃcient.
Logical sentences. We generally use the symbols φ, ψ, χ to denote well-formed
formulas in some language of propositional logic L (such as a theory of ﬁrst order
logic; see below), which includes the basic logical connectives ¬, ∧, ∨, →, ↔, and
uses modus ponens as its rule of inference. We assume that L has been chosen so
that its sentences can be interpreted as claims about some class of mathematical
objects, such as natural numbers or computer programs. We commonly write S for
the set of all sentences in L, and Γ for a set of axioms from which to write proofs
in the language. We write Γ ⊢φ when φ can be proven from Γ via modus ponens.
We will write logical formulas inside quotes “−”, such as φ := “x = 3”. The
exception is after ⊢, where we do not write quotes, in keeping with standard con-
ventions. We sometimes deﬁne sentences such as φ := “Goldbach’s conjecture”, in
which case it is understood that the English text could be expanded into a precise
arithmetical claim.
We use underlines to indicate when a symbol in a formula should be replaced
by the expression it stands for. For example, if n := 3, then φ := “x > n” means
φ = “x > 3”, and ψ := “φ →(x = n + 1)” means ψ = “x > 3 →(x = 3 + 1)”. If φ
and ψ denote formulas, then ¬φ denotes “¬(φ)” and φ ∧ψ denotes “(φ) ∧(ψ)” and
so on. For instance, if φ := “x > 3” then ¬φ denotes “¬(x > 3)”.
First order theories and prime sentences. We consider any theory in ﬁrst order
logic (such as Peano Arithmetic, PA) as a set of axioms that includes the axioms of
ﬁrst order logic, so that modus ponens is the only rule of inference needed for proofs.
As such, we view any ﬁrst order theory as speciﬁed in a propositional calculus
(following Enderton [2001]) whose atoms are the so-called “prime” sentences of
ﬁrst order logic, i.e., quantiﬁed sentences like “∃x: · · · ”, and atomic sentences like
“t1 = t2” and “R(t1, . . . , tn)” where the ti are closed terms. Thus, every ﬁrst-order
sentence can be viewed as a Boolean combination of prime sentences with logical
connectives (viewing “∀x: · · · ” as shorthand for “¬∃x: ¬ · · · ”). For example, the
sentence
φ := “((1 + 1 = 2) ∧(∀x: x > 0)) →(∃y: ∀z : (7 > 1 + 1) →(y + z > 2))”
is decomposed into “1 + 1 = 2”, “∃x: ¬(x > 0)” and “∃y: ∀z : (7 > 1 + 1) →
(y + z > 2)”, where the leading “¬” in front of the second statement is factored
12

out as a Boolean operator. In particular, note that while (7 > 1 + 1) is a prime
sentence, it does not occur in the Boolean decomposition of φ into primes, since it
occurs within a quantiﬁer. We choose this view because we will not always assume
that the theories we manipulate include the quantiﬁer axioms of ﬁrst-order logic.
Deﬁning values by formulas. We often view a formula that is free in one variable
as a way of deﬁning a particular number that satisﬁes that formula. For example,
given the formula X(ν) = “ν2 = 9
∧
ν > 0”, we would like to think of X
as representing the unique value “3”, in such a way that that we can then have
“5X + 1” refer to the number 16.
To formalize this, we use the following notational convention. Let X be a formula
free in one variable. We write X(x) for the formula resulting from substituting x
for the free variable of X. If
Γ ⊢∃x∀y: X(y) →y = x,
then we say that X deﬁnes a unique value (via Γ), and we refer to that value as
“the value” of X. We will be careful in distinguishing between what Γ can prove
about X(ν) on the one hand, and the values of X(ν) in diﬀerent models of Γ on
the other.
If X1, . . . , Xk are all formulas free in one variable that deﬁne a unique value
(via Γ), then for any k-place relationship R, we write “R(X1, X2, . . . , Xk)” as an
abbreviation for
“∀x1x2 . . . xk : X1(x1) ∧X2(x2) ∧. . . ∧Xk(xk) →R(x1, x2, . . . , xk)”.
For example, “Z = 2X + Y ” is shorthand for
“∀xyz : X(x) ∧Y (y) ∧Z(z) →z = 2x + y”.
This convention allows us to write concise expressions that describe relationships
between well-deﬁned values, even when those values may be diﬃcult or impossible
to determine via computation.
Representing computations. When we say a theory Γ in ﬁrst order logic “can
represent computable functions”, we mean that its language is used to refer to
computer programs in such a way that Γ satisﬁes the representability theorem
for computable functions. This means that for every (total) computable function
f : N+ →N+, there exists a Γ-formula γf with two free variables such that for all
n, y ∈N+,
y = f(n) if and only if Γ ⊢∀ν : γf(n, ν) ↔ν = y,
where “γf(n, ν)” stands, in the usual way, for the formula resulting from substituting
an encoding of n and the symbol ν for its free variables. In particular, note that
this condition requires Γ to be consistent.
When Γ can represent computable functions, we use “f(n)” as shorthand for the
formula “γf(n, ν)”. In particular, since “γf(n, ν)” is free in a single variable ν and
deﬁnes a unique value, we use “f(n)” by the above convention to write, e.g.,
“f(3) < g(3)”
as shorthand for
“∀xy: γf(3, x) ∧γg(3, y) →x < y”.
In particular, note that writing down a sentence like “f(3) > 4” does not involve
computing the value f(3); it merely requires writing out the deﬁnition of γf. This
distinction is important when f has a very slow runtime.
Sequences. We denote inﬁnite sequences using overlines, like x := (x1, x2, . . .),
where it is understood that xi denotes the ith element of x, for i ∈N+. To deﬁne
sequences of sentences compactly, we use parenthetical expressions such as φ :=
(“n > 7”)n∈N+, which deﬁnes the sequence
(“1 > 7”, “2 > 7”, “3 > 7”, . . .).
13

We deﬁne x≤n := (x1, . . . , xn). Given another element y, we abuse notation in the
usual way and deﬁne (x≤n, y) = (x1, . . . , xn, y) to be the list x≤n with y appended
at the end. We write () for the empty sequence.
A sequence x is called computable if there is a computable function f such that
f(n) = xn for all n ∈N+, in which case we say f computes x.
Asymptotics. Given any sequences x and y, we write
xn ≂n yn
for
lim
n→∞xn −yn = 0,
xn ≳n yn
for
lim inf
n→∞xn −yn ≥0, and
xn ≲n yn
for
lim sup
n→∞xn −yn ≤0.
3
The Logical Induction Criterion
In this section, we will develop a framework in which we can state the logical
induction criterion and a number of properties possessed by logical inductors. The
framework will culminate in the following deﬁnition, and a theorem saying that
computable logical inductors exist for every deductive process.
Deﬁnition 3.0.1 (The Logical Induction Criterion). A market P is said to
satisfy the logical induction criterion relative to a deductive process D if
there is no eﬃciently computable trader T that exploits P relative to D.
A
market P meeting this criterion is called a logical inductor over D.
We will now deﬁne markets, deductive processes, eﬃcient computability, traders,
and exploitation.
3.1
Markets
We will be concerned with methods for assigning values in the interval [0, 1] to
sentences of logic. We will variously interpret those values as prices, probabilities,
and truth values, depending on the context. Let L be a language of propositional
logic, and let S be the set of all sentences written in L. We then deﬁne:
Deﬁnition 3.1.1 (Valuation). A valuation is any function V : S →[0, 1]. We
refer to V(φ) as the value of φ according to V. A valuation is called rational if its
image is in Q.
First let us treat the case where we interpret the values as prices.
Deﬁnition 3.1.2 (Pricing). A pricing P : S →Q∩[0, 1] is any computable rational
valuation. If P(φ) = p we say that the price of a φ-share according to P is p, where
the intended interpretation is that a φ-share is worth $1 if φ is true.
Deﬁnition 3.1.3 (Market). A market P = (P1, P2, . . .) is a computable se-
quence of pricings Pi : S →Q ∩[0, 1].
We can visualize a market as a series of pricings that may change day by day. The
properties proven in Section 4 will apply to any market that satisﬁes the logical
induction criterion. Theorem 4.1.2 (Limit Coherence) will show that the prices of a
logical inductor can reasonably be interpreted as probabilities, so we will often speak
as if the prices in a market represent the beliefs of a reasoner, where Pn(φ) = 0.75
is interpreted as saying that on day n, the reasoner assigns 75% probability to φ.
In fact, the logical inductor that we construct in Section 5 has the additional
property of being ﬁnite at every timestep, which means we can visualize it as a
series of ﬁnite belief states that a reasoner of interest writes down each day.
14

Deﬁnition 3.1.4 (Belief State). A belief state P : S →Q ∩[0, 1] is a computable
rational valuation with ﬁnite support, where P(φ) is interpreted as the probability
of φ (which is 0 for all but ﬁnitely many φ).
We can visualize a belief state as a ﬁnite list of (φ, p) pairs, where the φ are unique
sentences and the p are rational-number probabilities, and P(φ) is deﬁned to be p
if (φ, p) occurs in the list, and 0 otherwise.
Deﬁnition 3.1.5 (Computable Belief Sequence). A computable belief sequence
P = (P1, P2, . . .) is a computable sequence of belief states, interpreted as a reasoner’s
explicit beliefs about logic as they are reﬁned over time.
We can visualize a computable belief sequence as a large spreadsheet where each
column is a belief state, and the rows are labeled by an enumeration of all logical
sentences. We can then imagine a reasoner of interest working on this spreadsheet,
by working on one column per day.
Philosophically, the reason for this setup is as follows. Most people know that
the sentence “1 + 1 is even” is true, and that the sentence “1 + 1 + 1 + 1 is even” is
true. But consider, is the following sentence true?
“1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 is even”
To answer, we must pause and count the ones.
Since we wish to separate the
question of what a reasoner already knows from what they could infer using further
computing resources, we require that the reasoner write out their beliefs about logic
explicitly, and reﬁne them day by day.
In this framework, we can visualize a reasoner as a person who computes the
belief sequence by ﬁlling in a large spreadsheet, always working on the nth column
on the nth day, by reﬁning and extending her previous work as she learns new
facts and takes more sentences into account, while perhaps making use of computer
assistance. For example, a reasoner who has noticed that “1 + · · · + 1 is even” is
true iﬀthe sentence has an even number of ones, might program her computer to
write 1 into as many of the true “1 + · · · + 1 is even” cells per day as it can before
resources run out. As another example, a reasoner who ﬁnds a bound on the prime
gap might go back and update her probability on the twin prime conjecture. In our
algorithm, the reasoner will have more and more computing power each day, with
which to construct her next belief state.
3.2
Deductive Processes
We are interested in the question of what it means for reasoners to assign “reasonable
probabilities” to statements of logic. Roughly speaking, we will imagine reasoners
that have access to some formal deductive process, such as a community of mathe-
maticians who submit machine-checked proofs to an oﬃcial curated database. We
will study reasoners that “outpace” this deductive process, e.g., by assigning high
probabilities to conjectures that will eventually be proven, and low probabilities to
conjectures that will eventually be disproven, well before the relevant proofs are
actually found.
Deﬁnition 3.2.1 (Deductive Process). A deductive process D : N+ →
Fin(S) is a computable nested sequence D1 ⊆D2 ⊆D3 . . . of ﬁnite sets of
sentences. We write D∞for the union S
n Dn.
This is a rather barren notion of “deduction”. We will consider cases where we
ﬁx some theory Γ, and Dn is interpreted as the theorems proven up to and including
day n. In this case, D can be visualized as a slow process that reveals the knowledge
of Γ over time. Roughly speaking, we will mainly concern ourselves with the case
where D eventually rules out all and only the worlds that are inconsistent with Γ.
15

Deﬁnition 3.2.2 (World). A world is any truth assignment W : S →B.
If
W(φ) = 1 we say that φ is true in W. If W(φ) = 0 we say that φ is false in W.
We write W for the set of all worlds.
Observe that worlds are valuations, and that they are not necessarily consistent.
This terminology is nonstandard; the term “world” is usually reserved for consistent
truth assignments.
Logically uncertain reasoners cannot immediately tell which
truth assignments are inconsistent, because revealing inconsistencies requires time
and eﬀort. We use the following notion of consistency:
Deﬁnition 3.2.3 (Propositional Consistency). A world W is called proposition-
ally consistent, abbreviated p.c., if for all φ ∈S, W(φ) is determined by Boolean
algebra from the truth values that W assigns to the prime sentences of φ. In other
words, W is p.c. if W(φ ∧ψ) = W(φ) ∧W(ψ), W(φ ∨ψ) = W(φ) ∨W(ψ), and so
on.
Given a set of sentences D, we deﬁne PC(D) to be the set of all p.c. worlds where
W(φ) = 1 for all φ ∈D. We refer to PC(D) as the set of worlds propositionally
consistent with D.
Given a set of sentences Γ interpreted as a theory, we will refer to PC(Γ) as the
set of worlds consistent with Γ, because in this case PC(Γ) is equal to the set of
all worlds W such that
Γ ∪{φ | W(φ) = 1} ∪{¬φ | W(φ) = 0} ⊬⊥.
Note that a limited reasoner won’t be able to tell whether a given world W is in
PC(Γ). A reasoner can computably check whether a restriction of W to a ﬁnite
domain is propositionally consistent with a ﬁnite set of sentences, but that’s about
it. Roughly speaking, the deﬁnition of exploitation (below) will say that a good
reasoner should perform well when measured on day n by worlds propositionally
consistent with Dn, and we ourselves will be interested in deductive processes that
pin down a particular theory Γ by propositional consistency:
Deﬁnition 3.2.4 (Γ-Complete). Given a theory Γ, we say that a deductive process
D is Γ-complete if
PC(D∞) = PC(Γ).
As a canonical example, let Dn be the set of all theorems of PA provable in at
most n characters.1 Then D is PA-complete, and a reasoner with access to D can
be interpreted as someone who on day n knows all PA-theorems provable in ≤n
characters, who must manage her uncertainty about other mathematical facts.
3.3
Eﬃcient Computability
We use the following notion of eﬃciency throughout the paper:
Deﬁnition 3.3.1 (Eﬃciently Computable). An inﬁnite sequence x is called
eﬃciently computable, abbreviated e.c., if there is a computable function f
that outputs xn on input n, with runtime polynomial in n (i.e. in the length of
n written in unary).
Our framework is not wedded to this deﬁnition; stricter notions of eﬃciency (e.g.,
sequences that can be computed in O(n2) time) would yield “dumber” inductors
with better runtimes, and vice versa.
We use the set of polynomial-time com-
putable functions because it has some closure properties that are convenient for our
purposes.
1. Because PA is a ﬁrst-order theory, and the only assumption we made about L is that
it is a propositional logic, note that the axioms of ﬁrst-order logic—namely, specialization
and distribution—must be included as theorems in D.
16

3.4
Traders
Roughly speaking, traders are functions that see the day n and the history of market
prices up to and including day n, and then produce a series of buy and sell orders,
by executing a strategy that is continuous as a function of the market history.
A linear combination of sentences can be interpreted as a “market order”, where
3φ −2ψ says to buy 3 shares of φ and sell 2 shares of ψ. Very roughly, a trading
strategy for day n will be a method for producing market orders where the coeﬃ-
cients are not numbers but functions which depend (continuously) on the market
prices up to and including day n.
Deﬁnition 3.4.1 (Valuation Feature). A valuation feature α : [0, 1]S×N+ →R
is a continuous function from valuation sequences to real numbers such that α(V)
depends only on the initial sequence V≤n for some n ∈N+ called the rank of the
feature, rank(α). For any m ≥n, we deﬁne α(V≤m) in the natural way. We will
often deal with features that have range in [0, 1]; we call these [0, 1]-features.
We write F for the set of all features, Fn for the set of valuation features of
rank ≤n, and deﬁne an F-progression α to be a sequence of features such that
αn ∈Fn.
The following valuation features ﬁnd the price of a sentence on a particular day:
Deﬁnition 3.4.2 (Price Feature). For each φ ∈S and n ∈N+, we deﬁne a price
feature φ∗n ∈Fn by the formula
φ∗n(V) := Vn(φ).
We call these “price features” because they will almost always be applied to a market
P, in which case φ∗n gives the price Pn(φ) of φ on day n as a function of P.
Very roughly, trading strategies will be linear combinations of sentences where the
coeﬃcients are valuation features. The set of all valuation features is not computably
enumerable, so we deﬁne an expressible subset:
Deﬁnition 3.4.3 (Expressible Feature). An expressible feature ξ ∈F is a val-
uation feature expressible by an algebraic expression built from price features φ∗n
for each n ∈N+ and φ ∈S, rational numbers, addition, multiplication, max(−, −),
and a “safe reciprocation” function max(1, −)−1. See Appendix A.2 for more details
and examples. 2
We write EF for the set of all expressible features, EFn for the set of expressible
features of rank ≤n, and deﬁne an EF-progression to be a sequence ξ such that
ξn ∈EFn.
For those familiar with abstract algebra, note that for each n, EFn is a commu-
tative ring. We will write 2 −φ∗6 for the function V 7→2 −φ∗6(V) and so on, in the
usual way. For example, the feature
ξ := max(0, φ∗6 −ψ∗7)
checks whether the value of φ on day 6 is higher than the value of ψ on day 7. If
so, it returns the diﬀerence; otherwise, it returns 0. If ξ is applied to a market P,
and P6(φ) = 0.5 and P7(ψ) = 0.2, then ξ(P) = 0.3. Observe that rank(ξ) = 7, and
that ξ is continuous.
The reason for the continuity constraint on valuation features is as follows.
Traders will be allowed to use valuation features (which depend on the price his-
tory) to decide how many shares of diﬀerent sentences to buy and sell. This creates
a delicate situation, because we’ll be constructing a market that has prices which
2. In particular, expressible features are a generalization of arithmetic circuits.
The
speciﬁc deﬁnition is somewhat arbitrary; what matters is that expressible features be (1)
continuous; (2) compactly speciﬁable in polynomial time; and (3) expressive enough to
identify a variety of ineﬃciencies in a market.
17

depend on the behavior of certain traders, creating a circular dependency where the
prices depend on trades that depend on the prices.
This circularity is related to classic paradoxes of self-trust. What should be the
price on a paradoxical sentence χ that says “I am true iﬀmy price is less than 50
cents in this market”? If the price is less than 50¢, then χ pays out $1, and traders
can make a fortune buying χ. If the price is 50¢ or higher, then χ pays out $0, and
traders can make a fortune selling χ. If traders are allowed to have a discontinuous
trading strategy—buy χ if P(χ) < 0.5, sell χ otherwise—then there is no way to
ﬁnd prices that clear the market.
Continuity breaks the circularity, by ensuring that if there’s a price where a
trader buys χ and a price where they sell χ then there’s a price in between where
they neither buy nor sell. In Section 5 we will see that this is suﬃcient to allow
stable prices to be found, and in Section 4.11 we will see that it is suﬃcient to
subvert the standard paradoxes of self-reference. The continuity constraint can be
interpreted as saying that the trader has only ﬁnite-precision access to the market
prices—they can see the prices, but there is some ε > 0 such that their behavior is
insensitive to an ε shift in prices.
We are almost ready to deﬁne trading strategies as a linear combination of
sentences with expressible features as coeﬃcients. However, there is one more com-
plication. It will be convenient to record not only the amount of shares bought and
sold, but also the amount of cash spent or received. For example, consider again
the market order 3φ −2ψ. If it is executed on day 7 in a market P, and P7(φ) = 0.4
and P7(ψ) = 0.3, then the cost is 3 · 40¢ −2 · 30¢ = 60¢. We can record the whole
trade as an aﬃne combination −0.6 + 3φ −2ψ, which can be read as “the trader
spent 60 cents to buy 3 shares of φ and sell 2 shares of ψ”. Extending this idea to
the case where the coeﬃcients are expressible features, we get the following notion:
Deﬁnition 3.4.4 (Trading Strategy). A trading strategy for day n, also called
an n-strategy, is an aﬃne combination of the form
T = c + ξ1φ1 + · · · + ξkφk,
where φ1, . . . , φk are sentences, ξ1, . . . , ξk are expressible features of rank ≤n, and
c = −
X
i
ξiφi
∗n
is a “cash term” recording the net cash ﬂow when executing a transaction that buys
ξi shares of φi for each i at the prevailing market price. (Buying negative shares is
called “selling”.) We deﬁne T [1] to be c, and T [φ] to be the coeﬃcient of φ in T ,
which is 0 if φ ̸∈(φ1, . . . , φk).
An n-strategy T can be encoded by the tuples (φ1, . . . φk) and (ξ1, . . . ξk) because
the c term is determined by them. Explicitly, by linearity we have
T = ξ1 · (φ1 −φ1
∗n) + · · · + ξk · (φk −φk
∗n),
which means any n-strategy can be written as a linear combination of (φi −φi
∗n)
terms, each of which means “buy one share of φi at the prevailing price”.
As an example, consider the following trading strategy for day 5:
h
(¬¬φ)∗5 −φ∗5i
·
 φ −φ∗5
+
h
φ∗5 −(¬¬φ)∗5i
·

¬¬φ −(¬¬φ)∗5
.
This strategy compares the price of φ on day 5 to the price of ¬¬φ on day 5. If
the former is less expensive by δ, it purchases δ shares of φ at the prevailing prices,
and sells δ shares of ¬¬φ at the prevailing prices. Otherwise, it does the opposite.
In short, this strategy arbitrages φ against ¬¬φ, by buying the cheaper one and
selling the more expensive one.
We can now state the key deﬁnition of this section:
18

Deﬁnition 3.4.5 (Trader). A trader T is a sequence (T1, T2, . . .) where each
Tn is a trading strategy for day n.
We can visualize a trader as a person who gets to see the day n, think for a while,
and then produce a trading strategy for day n, which will observe the history of
market prices up to and including day n and execute a market order to buy and
sell diﬀerent sentences at the prevailing market prices.
We will often consider the set of eﬃciently computable traders, which have to
produce their trading strategy in a time polynomial in n. We can visualize e.c.
traders as traders who are computationally limited: each day they get to think for
longer and longer—we can imagine them writing computer programs each morning
that assist them in their analysis of the market prices—but their total runtime may
only grow polynomially in n.
If s := Tn[φ] > 0, we say that T buys s shares of φ on day n, and if s < 0, we
say that T sells |s| shares of φ on day n. Similarly, if d := Tn[1] > 0, we say that T
receives d dollars on day n, and if d < 0, we say that T pays out |d| dollars on day
n.
Each trade Tn has value zero according to Pn, regardless of what market P it is
executed in. Clever traders are the ones who make trades that are later revealed by
a deductive process D to have a high worth (e.g., by purchasing shares of provable
sentences when the price is low). As an example, a trader T with a basic grasp
of arithmetic and skepticism about some of the market P’s conﬁdent conjectures
might execute the following trade orders on day n:
Table 1: Visualizing markets and trades
Sentence
Market prices
Trade
φ :↔1 + 1 = 2
Pn(φ) = 90¢
Tn[φ] = 4 shares
ψ :↔1 + 1 ̸= 2
Pn(ψ) = 5¢
Tn[ψ] = −3 shares
χ :↔“Goldbach’s conjecture”
Pn(χ) = 98¢
Tn[χ] = −1 share
The net value of the shares bought and sold at these prices would be
4 · 90¢ −3 · 5¢ −1 · 98¢ = $2.47,
so if those three sentences were the only sentences bought and sold by Tn, Tn[1]
would be −2.47.
Trade strategies are a special case of aﬃne combinations of sentences:
Deﬁnition 3.4.6 (Aﬃne Combination). An F-combination A : S ∪{1} →F is
an aﬃne expression of the form
A := c + α1φ1 + · · · + αkφk,
where (φ1, . . . , φk) are sentences and (c, α1, . . . , αk) are in F.
We deﬁne R-
combinations, Q-combinations, and EF-combinations analogously.
We write A[1] for the trailing coeﬃcient c, and A[φ] for the coeﬃcient of φ,
which is 0 if φ ̸∈(φ1, . . . , φk). The rank of A is deﬁned to be the maximum rank
among all its coeﬃcients. Given any valuation V, we abuse notation in the usual
way and deﬁne the value of A (according to V) linearly by:
V(A) := c + α1V(φ1) + · · · + αkV(φk).
An F-combination progression is a sequence A of aﬃne combinations where An
has rank ≤n. An EF-combination progression is deﬁned similarly.
Note that a trade T is an F-combination, and the holdings T (P) from T against
P is a Q-combination. We will use aﬃne combinations to encode the net holdings
P
i≤n Ti(P) of a trader after interacting with a market P, and later to encode linear
inequalities that hold between the truth values of diﬀerent sentences.
19

3.5
Exploitation
We will now deﬁne exploitation, beginning with an example. Let L be the language
of PA, and D be a PA-complete deductive process. Consider a market P that assigns
Pn(“1 + 1 = 2”) = 0.5 for all n, and a trader who buys one share of “1 + 1 = 2”
each day. Imagine a reasoner behind the market obligated to buy and sell shares at
the listed prices, who is also obligated to pay out $1 to holders of φ-shares if and
when D says φ. Let t be the ﬁrst day when “1 + 1 = 2” ∈Dt. On each day, the
reasoner receives 50¢ from T, but after day t, the reasoner must pay $1 every day
thereafter. They lose 50¢ each day, and T gains 50¢ each day, despite the fact that
T never risked more than $t/2. In cases like these, we say that T exploits P.
With this example in mind, we deﬁne exploitation as follows:
Deﬁnition 3.5.1 (Exploitation). A trader T is said to exploit a valuation
sequence V relative to a deductive process D if the set of values
n
W
P
i≤n Ti
 V
  n ∈N+, W ∈PC(Dn)
o
is bounded below, but not bounded above.
Given a world W, the number W(P
i≤n Ti(P)) is the value of the trader’s net
holdings after interacting with the market P, where a share of φ is valued at $1 if φ is
true in W and $0 otherwise. The set {W(P
i≤n Ti(P)) | n ∈N+, W ∈PC(Dn)} is the
set of all assessments of T’s net worth, across all time, according to worlds that were
propositionally consistent with D at the time. We informally call these plausible
assessments of the trader’s net worth. Using this terminology, Deﬁnition 3.5.1 says
that a trader exploits the market if their plausible net worth is bounded below, but
not above.
Roughly speaking, we can imagine that there is a person behind the market who
acts as a market maker, obligated to buy and sell shares at the listed prices. We
can imagine that anyone who sold a φ-share is obligated to pay $1 if and when D
says φ. Then, very roughly, a trader exploits the market if they are able to make
unbounded returns oﬀof a ﬁnite investment.
This analogy is illustrative but incomplete—traders can exploit the market even
if they never purchase a sentence that appears in D. For example, let φ and ψ
be two sentences such that (φ ∨ψ) is provable in PA, but such that neither φ nor
ψ is provable in PA. Consider a trader that bought 10 φ-shares at a price of 20¢
each, and 10 ψ-shares at a price of 30¢ each. Once D says (φ ∨ψ), all remaining
p.c. worlds will agree that the portfolio −5 + 10φ + 10ψ has a value of at least +5,
despite the fact that neither φ nor ψ is ever proven. If the trader is allowed to
keep buying φ and ψ shares at those prices, they would exploit the market, despite
the fact that they never buy decidable sentences. In other words, our notion of
exploitation rewards traders for arbitrage, even if they arbitrage between sentences
that never “pay out”.
3.6
Main Result
Recall the logical induction criterion:
Deﬁnition 3.0.1 (The Logical Induction Criterion). A market P is said to satisfy
the logical induction criterion relative to a deductive process D if there is no
eﬃciently computable trader T that exploits P relative to D. A market P meeting
this criterion is called a logical inductor over D.
We may now state our main result:
20

Theorem 3.6.1. For any deductive process D, there exists a computable belief
sequence P satisfying the logical induction criterion relative to D.
Proof. In Section 5, we show how to take an arbitrary deductive process D and
construct a computable belief sequence LIA. Theorem 5.4.2 shows that LIA is a
logical inductor relative to the given D.
Deﬁnition 3.6.2 (Logical Inductor over Γ). Given a theory Γ, a logical inductor
over a Γ-complete deductive process D is called a logical inductor over Γ.
Corollary 3.6.3. For any recursively axiomatizable theory Γ, there exists a com-
putable belief sequence that is a logical inductor over Γ.
4
Properties of Logical Inductors
Here is an intuitive argument that logical inductors perform good reasoning under
logical uncertainty:
Consider any polynomial-time method for eﬃciently identifying patterns
in logic. If the market prices don’t learn to reﬂect that pattern, a clever
trader can use that pattern to exploit the market. Thus, a logical induc-
tor must learn to identify those patterns.
In this section, we will provide evidence supporting this intuitive argument, by
demonstrating a number of desirable properties possessed by logical inductors. The
properties that we demonstrate are broken into twelve categories:
1. Convergence and Coherence: In the limit, the prices of a logical inductor
describe a belief state which is fully logically consistent, and represents a
probability distribution over all consistent worlds.
2. Timely Learning: For any eﬃciently computable sequence of theorems, a
logical inductor learns to assign them high probability in a timely manner,
regardless of how diﬃcult they are to prove. (And similarly for assigning low
probabilities to refutable statements.)
3. Calibration and Unbiasedness: Logical inductors are well-calibrated and,
given good feedback, unbiased.
4. Learning Statistical Patterns: If a sequence of sentences appears pseu-
dorandom to all reasoners with the same runtime as the logical inductor, it
learns the appropriate statistical summary (assigning, e.g., 10% probability
to the claim “the nth digit of π is a 7” for large n, if digits of π are actually
hard to predict).
5. Learning Logical Relationships: Logical inductors inductively learn to
respect logical constraints that hold between diﬀerent types of claims, such as
by ensuring that mutually exclusive sentences have probabilities summing to
at most 1.
6. Non-Dogmatism: The probability that a logical inductor assigns to an inde-
pendent sentence φ is bounded away from 0 and 1 in the limit, by an amount
dependent on the complexity of φ. In fact, logical inductors strictly dominate
the universal semimeasure in the limit. This means that we can condition
logical inductors on independent sentences, and when we do, they perform
empirical induction.
7. Conditionals: Given a logical inductor P, the market given by the condi-
tional probabilities P(−| ψ) is a logical inductor over D extended to include
ψ. Thus, when we condition logical inductors on new axioms, they continue
to perform logical induction.
21

8. Expectations: Logical inductors give rise to a well-behaved notion of the
expected value of a logically uncertain variable.
9. Trust in Consistency: If the theory Γ underlying a logical inductor’s deduc-
tive process is expressive enough to talk about itself, then the logical inductor
learns inductively to trust Γ.
10. Reasoning about Halting: If there’s an eﬃcient method for generating
programs that halt, a logical inductor will learn in a timely manner that
those programs halt (often long before having the resources to evaluate them).
If there’s an eﬃcient method for generating programs that don’t halt, a logical
inductor will at least learn not to expect them to halt for a very long time.
11. Introspection: Logical inductors “know what they know”, in that their be-
liefs about their current probabilities and expectations are accurate.
12. Self-Trust: Logical inductors trust their future beliefs.
For the sake of brevity, proofs are deferred to Section 6 and the appendix. Some
example proofs are sketched in this section, by outlining discontinuous traders that
would exploit any market that lacked the desired property. The deferred proofs
deﬁne polynomial-time continuous traders that approximate those discontinuous
strategies.
In what follows, let L be a language of propositional logic; let S be the set of
sentences written in L; let Γ ⊂S be a computably enumerable set of propositional
formulas written in L (such as PA, where the propositional variables are prime sen-
tences in ﬁrst-order logic, as discussed in Section 2); and let P be a computable
logical inductor over Γ, i.e., a market satisfying the logical induction criterion rela-
tive to some Γ-complete deductive process D. We assume in this section that Γ is
consistent.
Note that while the computable belief sequence LIA that we deﬁne has ﬁnite
support on each day, in this section we assume only that P is a market. We do this
because our results below hold in this more general case, and can be applied to LIA
as a special case.
In sections 4.8-4.12 we will assume that Γ can represent computable functions.
This assumption is not necessary until Section 4.8.
4.1
Convergence and Coherence
Firstly, the market prices of a logical inductor converge:
Theorem 4.1.1 (Convergence). The limit P∞: S →[0, 1] deﬁned by
P∞(φ) := lim
n→∞Pn(φ)
exists for all φ.
Proof sketch.
(Proof in: 6.1 or B.4.)
Roughly speaking, if P never makes up its mind about φ, then it can be
exploited by a trader arbitraging shares of φ across diﬀerent days. More
precisely, suppose by way of contradiction that the limit P∞(φ) does
not exist. Then for some p ∈[0, 1] and ε > 0, we have Pn(φ) < p −ε
inﬁnitely often and also Pn(φ) > p+ε inﬁnitely often. A trader can wait
until Pn(φ) < p−ε and then buy a share in φ at the low market price of
Pn(φ). Then the trader waits until some later m such that Pm(φ) > p+ε,
and sells back the share in φ at the higher price. This trader makes a
total proﬁt of 2ε every time Pn(φ) oscillates in this way, at no risk, and
therefore exploits P. Since P implements a logical inductor, this is not
possible; therefore the limit P∞(φ) must in fact exist.
22

This sketch showcases the main intuition for the convergence of P, but elides a
number of crucial details. In particular, the trader we have sketched makes use of
discontinuous trading functions, and so is not a well-formed trader. These details
are treated in Section 6.1.
Next, the limiting beliefs of a logical inductor represent a coherent probability
distribution:
Theorem 4.1.2 (Limit Coherence). P∞is coherent, i.e., it gives rise to an inter-
nally consistent probability measure Pr on the set PC(Γ) of all worlds consistent
with Γ, deﬁned by the formula
Pr(W(φ) = 1) := P∞(φ).
In particular, if Γ contains the axioms of ﬁrst-order logic, then P∞deﬁnes a prob-
ability measure on the set of ﬁrst-order completions of Γ.
Proof sketch.
(Proof in: 6.2 or C.10.)
The limit P∞(φ) exists by the convergence theorem, so Pr is well-
deﬁned. Gaifman (1964) shows that Pr deﬁnes a probability measure
over PC(D∞) so long as the following three implications hold for all
sentences φ and ψ:
• If Γ ⊢φ, then P∞(φ) = 1,
• If Γ ⊢¬φ, then P∞(φ) = 0,
• If Γ ⊢¬(φ ∧ψ), then P∞(φ ∨ψ) = P∞(φ) + P∞(ψ).
Let us demonstrate each of these three properties.
First suppose that Γ ⊢φ, but P∞(φ) = 1−ε for some ε > 0. Then shares
of φ will be underpriced, as they are worth 1 in every consistent world,
but only cost 1−ε. There is a trader who waits until φ is propositionally
provable from Dn, and until Pn(φ) has approximately converged, and
then starts buying shares of φ every day at the price Pn(φ). Since φ has
appeared in D, the shares immediately have a minimum plausible value
of $1. Thus the trader makes 1 −Pn(φ) ≈ε proﬁt every day, earning
an unbounded total value, contradicting the logical induction criterion.
But P cannot be exploited, so P∞(φ) must be 1.
Similarly, if Γ ⊢¬φ but P∞(φ) = ε > 0, then a trader could exploit P
by selling oﬀshares in φ for a proﬁt of Pn(φ) ≈ε each day.
Finally, suppose that Γ ⊢¬(φ ∧ψ), but for some ε > 0,
P∞(φ ∨ψ) = P∞(φ) + P∞(ψ) ± ε.
Then there is a trader that waits until Pn has approximately converged
on these sentences, and until ¬(φ ∧ψ) is propositionally provable from
Dn. At that point it’s a good deal to sell (buy) a share in φ ∨ψ, and
buy (sell) a share in each of φ and ψ; the stocks will have values that
cancel out in every plausible world. Thus this trader makes a proﬁt of
≈ε from the price diﬀerential, and can then repeat the process. Thus,
they would exploit P. But this is impossible, so P∞must be coherent.
Theorem 4.1.2 says that if P were allowed to run forever, and we interpreted its
prices as probabilities, then we would ﬁnd its beliefs to be perfectly consistent. In
the limit, P assigns probability 1 to every theorem and 0 to every contradiction. On
independent sentences, its beliefs obey the constraints of probability theory; if φ
provably implies ψ, then the probability of ψ converges to a point no lower than the
limiting probability of φ, regardless of whether they are decidable. The resulting
probabilities correspond to a probability distribution over all possible ways that Γ
could be completed.
23

This justiﬁes interpreting the market prices of a logical inductor as probabilities.
Logical inductors are not the ﬁrst computable procedure for assigning probabilities
to sentences in a manner that is coherent in the limit; the algorithm of Demski
(2012) also has this property. The main appeal of logical induction is that their
beliefs become reasonable in a timely manner, outpacing the underlying deductive
process.
4.2
Timely Learning
It is not too diﬃcult to deﬁne a reasoner that assigns probability 1 to all (and only)
the provable sentences, in the limit: simply assign probability 0 to all sentences, and
then enumerate all logical proofs, and assign probability 1 to the proven sentences.
The real trick is to recognize patterns in a timely manner, well before the sentences
can be proven by slow deduction.
Logical inductors learn to outpace deduction on any eﬃciently computable se-
quence of provable statements.3 To illustrate, consider our canonical example where
Dn is the set of all theorems of PA provable in at most n characters, and suppose
φ is an e.c. sequence of theorems which are easy to generate but diﬃcult to prove.
Let f(n) be the length of the shortest proof of φn, and assume that f is some
fast-growing function. At any given time n, the statement φn is ever further out
beyond Dn—it might take 1 day to prove φ1, 10 days to prove φ2, 100 days to prove
φ3, and so on. One might therefore expect that φn will also be “out of reach” for
Pn, and that we have to wait until a much later day close to f(n) before expecting
Pf(n)(φn) to be accurate. However, this is not the case! After some ﬁnite time N,
P will recognize the pattern and begin assigning high probability to φ in a timely
manner.
Theorem 4.2.1 (Provability Induction). Let φ be an e.c. sequence of theorems.
Then
Pn(φn) ≂n 1.
Furthermore, let ψ be an e.c. sequence of disprovable sentences. Then
Pn(ψn) ≂n 0.
Proof sketch.
(Proof in: 6.5 or C.3.)
Consider a trader that acts as follows. First wait until the time a when
Pa(φa) drops below 1 −ε and buy a share of φa. Then wait until φa is
worth 1 in all worlds plausible at time f(a). Then repeat this process.
If Pn(φn) drops below 1 −ε inﬁnitely often, then this trader makes ε
proﬁt inﬁnitely often, oﬀof an initial investment of $1, and therefore
exploits the market. P is inexploitable, so Pn(φn) must converge to 1.
By a similar argument, Pn(ψn) must converge to 0.4
In other words, P will learn to start believing φn by day n at the latest, despite
the fact that φn won’t be deductively conﬁrmed until day f(n), which is potentially
much later. In colloquial terms, if φ is a sequence of facts that can be generated
eﬃciently, then P inductively learns the pattern, and its belief in φ becomes accurate
faster than D can computationally verify the individual sentences.
For example, imagine that prg(n) is a program with fast-growing runtime, which
always outputs either 0, 1, or 2 for all n, but such that there is no proof of this in
the general case. Then
“∀x: prg(x) = 0 ∨prg(x) = 1 ∨prg(x) = 2”
3. Recall that a sequence x is eﬃciently computable iﬀthere exists a computable func-
tion n 7→xn with runtime polynomial in n.
4. The traders sketched here are optimized for ease of proof, not for eﬃciency—a clever
trader trying to proﬁt from low prices on eﬃciently computable theorems would be able
to exploit the market faster than this.
24

is not provable. Now consider the sequence of statements
prg012 :=
 “prg(n) = 0 ∨prg(n) = 1 ∨prg(n) = 2”

n∈N+
where each prg012n states that prg outputs a 0, 1, or 2 on that n in particular.
Each individual prg012n is provable (it can be proven by running prg on input
n), and prg012 is eﬃciently computable (because the sentences themselves can be
written down quickly, even if prg is very diﬃcult to evaluate). Thus, provability
induction says that any logical inductor will “learn the pattern” and start assigning
high probabilities to each individual prg012n no later than day n.
Imagine that D won’t determine the output of prg(n) until the f(n)th day, by
evaluating prg(n) in full. Provability induction says that P will eventually recognize
the pattern prg012 and start assigning high probability to prg012n no later than
the nth day, f(n) −n days before the evaluation ﬁnishes. This is true regardless of
the size of f(n), so if f is fast-growing, P will outpace D by an ever-growing margin.
Analogy: Ramanujan and Hardy. Imagine that the statements φ
are being output by an algorithm that uses heuristics to generate math-
ematical facts without proofs, playing a role similar to the famously
brilliant, often-unrigorous mathematician Srinivasa Ramanujan. Then
P plays the historical role of the beliefs of the rigorous G.H. Hardy who
tries to verify those results according to a slow deductive process (D).
After Hardy (P) veriﬁes enough of Ramanujan’s claims (φ≤n), he begins
to trust Ramanujan, even if the proofs of Ramanujan’s later conjectures
are incredibly long, putting them ever-further beyond Hardy’s current
abilities to rigorously verify them. In this story, Hardy’s inductive rea-
soning (and Ramanujan’s also) outpaces his deductive reasoning.
This idiom of assigning the right probabilities to φn no later than day n will be
common throughout the paper, so we give it a name.
Deﬁnition 4.2.2 (Timely Manner). Let φ be an e.c. sequence of sentences, and p
be an e.c. sequence of rational numbers. We say that P assigns p to φ in a timely
manner if for every ε > 0, there exists a time N such that for all n > N,
|Pn(φn) −pn| < ε.
In other words, P assigns p to φ in a timely manner if
Pn(φn) ≂n pn.
Note that there are no requirements on how large N gets as a function of ε. As
such, when we say that P assigns probabilities p to φ in a timely manner, it may
take a very long time for convergence to occur. (See Section 5.5 for a discussion.)
As an example, imagine the reasoner who recognizes that sentences of the form
“1+1+· · ·+1 is even” are true iﬀthe number of ones is even. Let φ be the sequence
where φn is the version of that sentence with 2n ones. If the reasoner starts writing
a probability near 100% in the φn cell by day n at the latest, then intuitively, she has
begun incorporating the pattern into her beliefs, and we say that she is assigning
high probabilities to φ in a timely manner.
We can visualize ourselves as taking P’s belief states, sorting them by φ on one
axis and days on another, and then looking at the main diagonal of cells, to check
the probability of each φn on day n. Checking the nth sentence on the nth day is a
rather arbitrary choice, and we might hope that a good reasoner would assign high
probabilities to e.c. sequences of theorems at a faster rate than that. It is easy to
show that this is the case, by the closure properties of eﬃcient computability. For
example, if φ is an e.c. sequence of theorems, then so are φ2n and φ2n+1, which each
enumerate half of φ at twice the speed, so by Theorem 4.2.1 (Provability Induction),
P will eventually learn to believe φ at a rate of at least two per day. Similarly, P
25

will learn to believe φ3n and φn2 and φ10n3+3 in a timely manner, and so on. Thus,
up to polynomial transformations, it doesn’t really matter which diagonal we check
when checking whether a logical inductor has begun “noticing a pattern”.
Furthermore, we will show that if P assigns the correct probability on the main
diagonal, then P also learns to keep them there:
Theorem 4.2.3 (Persistence of Knowledge). Let φ be an e.c. sequence of sentences,
and p be an e.c. sequence of rational-number probabilities. If P∞(φn) ≂n pn, then
sup
m≥n
|Pm(φn) −pn| ≂n 0.
Furthermore, if P∞(φn) ≲n pn, then
sup
m≥n
Pm(φn) ≲n pn,
and if P∞(φn) ≳n pn, then
inf
m≥n Pm(φn) ≳n pn.
(Proof in: B.6.)
In other words, if P assigns p to φ in the limit, then P learns to assign probability
near pn to φn at all times m ≥n. This theorem paired with the closure properties
of the set of eﬃciently computable sequences means that checking the probability of
φn on the nth day is a ﬁne way to check whether P has begun recognizing a pattern
encoded by φ. As such, we invite the reader to be on the lookout for statements
of the form Pn(φn) as signs that P is recognizing a pattern, often in a way that
outpaces the underlying deductive process.
Theorems 4.2.1 (Provability Induction) and 4.2.3 (Persistence of Knowledge)
only apply when the pattern of limiting probabilities is itself eﬃciently computable.
For example, consider the sequence of sentences
πAeq7 :=
 “π[Ack(n, n)] = 7”

n∈N+
where π[i] is the ith digit in the decimal expansion of π and Ack is the Ackermann
function. Each individual sentence is decidable, so the limiting probabilities are 0
for some πAeq7n and 1 for others. But that pattern of 1s and 0s is not eﬃciently
computable (assuming there is no eﬃcient way to predict the Ackermann digits of
π), so provability induction has nothing to say on the topic.
In cases where the pattern of limiting probabilities are not e.c., we can still
show that if P is going to make its probabilities follow a certain pattern eventually,
then it learns to make its probabilities follow that pattern in a timely manner. For
instance, assume that each individual sentence πAeq7n (for n > 4) is going to spend
a long time sitting at 10% probability before eventually being resolved to either 1
or 0. Then P will learn to assign Pn(πAeq7n) ≈0.1 in a timely manner:
Theorem 4.2.4 (Preemptive Learning). Let φ be an e.c. sequence of sentences.
Then
lim inf
n→∞Pn(φn) = lim inf
n→∞sup
m≥n
Pm(φn).
Furthermore,
lim sup
n→∞Pn(φn) = lim sup
n→∞
inf
m≥n Pm(φn).
(Proof in: B.3.)
Let’s unpack Theorem 4.2.4. The quantity supm≥n Pm(φn) is an upper bound
on the price Pm(φn) on or after day n, which we can interpret as the highest price
tag that that P will ever put on φn after we ﬁrst start checking it on day n. We
can imagine a sequence of these values: On day n, we start watching φn. As time
goes on, its price travels up and down until eventually settling somewhere. This
26

happens for each n. The limit inﬁmum of supm≥n Pm(φn) is the greatest lower
bound p past which a generic φn (for n large) will deﬁnitely be pushed after we
started watching it. Preemptive Learning says that if P always eventually pushes
φn up to a probability at least p, then it will learn to assign each φn a probability
at least p in a timely manner (and similarly for least upper bounds).
For example, if each individual πAeq7n is eventually recognized as a claim about
digits of π and placed at probability 10% for a long time before being resolved, then
P learns to assign it probability 10% on the main diagonal. In general, if P is going
to learn a pattern eventually, it learns it in a timely manner.
This leaves open the question of whether a logical inductor P is smart enough to
recognize that the πAeq7 should each have probability 10% before they are settled
(assuming the Ackermann digits of π are hard to predict). We will return to that
question in Section 4.4, but ﬁrst, we examine the reverse question.
4.3
Calibration and Unbiasedness
Theorem 4.2.1 (Provability Induction) shows that logical inductors are good at
detecting patterns in what is provable.
Next, we ask: when a logical inductor
learns a pattern, when must that pattern be real? In common parlance, a source of
probabilistic estimates is called well calibrated if among statements where it assigns
a probability near p, the estimates are correct with frequency roughly p.
In the case of reasoning under logical uncertainty, measuring calibration is not
easy. Consider the sequence clusters constructed from correlated clusters of size 1,
10, 100, 1000, . . . , where the truth value of each cluster is determined by the parity
of a late digit of π:
clusters1 :↔“π[Ack(1, 1)] is even”
clusters2 :↔· · · :↔clusters11 :↔“π[Ack(2, 2)] is even”
clusters12 :↔· · · :↔clusters111 :↔“π[Ack(3, 3)] is even”
clusters112 :↔· · · :↔clusters1111 :↔“π[Ack(4, 4)] is even”
and so on. A reasoner who can’t predict the parity of the Ackermann digits of π
should assign 50% (marginal) probability to any individual clustersn for n large.
But consider what happens if the 9th cluster turns out to be true, and the next
billion sentences are all true. A reasoner who assigned 50% to those billion sen-
tences was assigning the right probabilities, but their calibration is abysmal: on the
billionth day, they have assigned 50% probability a billion sentences that were over-
whelmingly true. And if the 12th cluster comes up false, then on the trillionth day,
they have assigned 50% probability to a trillion sentences that were overwhelmingly
false! In cases like these, the frequency of truth oscillates eternally, and the good
reasoner only appears well-calibrated on the rare days where it crosses 50%.
The natural way to correct for correlations such as these is to check P’s con-
ditional probabilities instead of its marginal probabilities. This doesn’t work very
well in our setting, because given a logical sentence φ, the quantity that we care
about will almost always be the marginal probability of φ. The reason we deal
with sequences is because that lets us show that φ has reasonable probabilities
relative to various related sentences. For example, if φ := “prg(32) = 17”, then
we can use our theorems to relate the probability of φ to the probability of the
sequence (“prg(n) = 17”)n∈N+, and to the sequence (“prg(32) = n”)n∈N+, and to
the sequence (“prg(n) > n”)n∈N+, and so on, to show that φ eventually has reason-
able beliefs about prg (hopefully before P has the resources to simply evaluate prg
on input 32). But at the end of the day, we’ll want to reason about the marginal
probability of φ itself. In this case, approximately-well-calibrated conditional proba-
bilities wouldn’t buy us much: there are 2n−1 possible truth assignments to the ﬁrst
n −1 elements of φ, so if we try to compute the marginal probability of φn from
all the diﬀerent conditional probabilities, exponentially many small errors would
render the answer useless. Furthermore, intuitively, if φ is utterly unpredictable to
P, then the probabilities of all the diﬀerent truth assignments to φ≤n−1 will go to
27

0 as n gets large, which means the conditional probabilities won’t necessarily be
reasonable. (In Section 4.4 will formalize a notion of pseudorandomness.)
Despite these diﬃculties, we can recover some good calibration properties on
the marginal probabilities if we either (a) restrict our consideration to sequences
where the average frequency of truth converges; or (b) look at subsequences of φ
where P has “good feedback” about the truth values of previous elements of the
subsequence, in a manner deﬁned below.
To state our ﬁrst calibration property, we will deﬁne two diﬀerent sorts of indi-
cator functions that will prove useful in many diﬀerent contexts.
Deﬁnition 4.3.1 (Theorem Indicator). Given a sentence φ, deﬁne ThmΓ(φ) to be
1 if Γ ⊢φ and 0 otherwise.
Deﬁnition 4.3.2 (Continuous Threshold Indicator). Let δ > 0 be a rational num-
ber, and x and y be real numbers. We then deﬁne
Indδ(x > y) :=







0
if x ≤y
x −y
δ
if
y < x ≤y + δ
1
if
y + δ < x.
Notice that Indδ(x > y) has no false positives, and that it is linear in the region
between y and y + δ. We deﬁne Indδ(x < y) analogously, and we deﬁne
Indδ(a < x < b) := min(Indδ(x > a), Indδ(x < b)).
Observe that we can generalize this deﬁnition to the case where x and y are express-
ible features, in which case Indδ(x > y) is an expressible [0, 1]-feature.
Now we can state our calibration theorem.
Theorem 4.3.3 (Recurring Calibration). Let φ be an e.c. sequence of decidable
sentences, a and b be rational numbers, δ be an e.c. sequence of positive rational
numbers, and suppose that P
n
 Indδi(a < Pi(φi) < b)

i∈N+ = ∞. Then, if the se-
quence
 P
i≤n Indδi(a < Pi(φi) < b) · ThmΓ(φi)
P
i≤n Indδi(a < Pi(φi) < b)
!
n∈N+
converges, it converges to a point in [a, b]. Furthermore, if it diverges, it has a limit
point in [a, b].
(Proof in: D.3.)
Roughly, this says that if Pn(φn) ≈80% inﬁnitely often, then if we look at the
subsequence where it’s 80%, the limiting frequency of truth on that subsequence is
80% (if it converges).
In colloquial terms, on subsequences where P says 80% and it makes sense to
talk about the frequency of truth, the frequency of truth is 80%, i.e., P isn’t seeing
shadows. If the frequency of truth diverges—as in the case with clusters—then P is
still well-calibrated inﬁnitely often, but its calibration might still appear abysmal
at times (if they can’t predict the swings).
Note that calibration alone is not a very strong property: a reasoner can always
cheat to improve their calibration (i.e., by assigning probability 80% to things that
they’re sure are true, in order to bring up the average truth of their “80%” pre-
dictions). What we really want is some notion of “unbiasedness”, which says that
there is no eﬃcient method for detecting a predictable bias in a logical inductor’s
beliefs. This is something we can get on sequences where the limiting frequency of
truth converges, though again, if the limiting frequency of truth diverges, all we can
guarantee is a limit point.
Deﬁnition 4.3.4 (Divergent Weighting). A divergent weighting w ∈[0, 1]N+ is
an inﬁnite sequence of real numbers in [0, 1], such that P
n wn = ∞.
28

Note that divergent weightings have codomain [0, 1] as opposed to {0, 1}, meaning
the weightings may single out fuzzy subsets of the sequence. For purposes of in-
tuition, imagine that w is a sequence of 0s and 1s, in which case each w can be
interpreted as a subsequence. The constraint that the wn sum to ∞ensures that
this subsequence is inﬁnite.
Deﬁnition 4.3.5 (Generable From P). A sequence of rational numbers q is called
generable from P if there exists an e.c. EF-progression q† such that q†
n(P) = qn
for all n.
In this case we say that q is P-generable.
P-generable R-sequences,
Q-combination sequences, and R-combination sequences are deﬁned analogously.
Divergent weightings generable from P are fuzzy subsequences that are allowed
to depend continuously (via expressible market features) on the market history. For
example, the sequence (Ind0.01(Pn(φn) > 0.5))n∈N+ is a P-generable sequence that
singles out all times n when Pn(φn) is greater than 50%.
Note that the set of
P-generable divergent weightings is larger than the set of e.c. divergent weightings,
as the P-generable weightings are allowed to vary continuously with the market
prices.
Theorem 4.3.6 (Recurring Unbiasedness). Given an e.c. sequence of decidable
sentences φ and a P-generable divergent weighting w, the sequence
P
i≤n wi · (Pi(φi) −ThmΓ(φi))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
(Proof in: D.2.)
Letting w = (1, 1, . . .), this theorem says that the diﬀerence between the average
probability Pn(φn) and the average frequency of truth is 0 inﬁnitely often (and 0 al-
ways, if the latter converges). Letting each wn be Indδ(a < Pn(φn) < b), we recover
Theorem 4.3.3 (Recurring Calibration). In general, the fraction in Theorem 4.3.6
can be interpreted as a measure of the “bias” of P on the fuzzy subsequence of φ
singled out by w. Then this theorem says that P is unbiased on all P-generable
subsequences where the frequency of truth converges (and unbiased inﬁnitely often
on subsequences where it diverges). Thus, if an e.c. sequence of sentences can be
decomposed (by any P-generable weighting) into subsequences where the frequency
of truth converges, then P learns to assign probabilities such that there is no eﬃcient
method for detecting a predictable bias in its beliefs.
However, not every sequence can be broken down into well-behaved subsequences
by a P-generable divergent weighting (if, for example, the truth values move “pseu-
dorandomly” in correlated clusters, as in the case of clusters). In these cases, it
is natural to wonder whether there are any conditions where P will be unbiased
anyway. Below, we show that the bias converges to zero whenever the weighting w
is sparse enough that P can gather suﬃcient feedback about φn in between guesses:
Deﬁnition 4.3.7 (Deferral Function). A function f : N+ →N+ is called a defer-
ral function if
1. f(n) > n for all n, and
2. f(n) can be computed in time polynomial in f(n), i.e., if there is some algo-
rithm and a polynomial function h such that for all n, the algorithm computes
f(n) within h(f(n)) steps.
If f is a deferral function, we say that f defers n to f(n).
Theorem 4.3.8 (Unbiasedness From Feedback). Let φ be any e.c. sequence of
decidable sentences, and w be any P-generable divergent weighting. If there exists a
29

strictly increasing deferral function f such that the support of w is contained in the
image of f and ThmΓ(φf(n)) is computable in O(f(n + 1)) time, then
P
i≤n wi · (Pi(φi) −ThmΓ(φi))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on φ”.
(Proof in: D.5.)
In other words, P is unbiased on any subsequence of the data where a polynomial-
time machine can ﬁgure out how the previous elements of the subsequence turned
out before P is forced to predict the next one. This is perhaps the best we can
hope for: On ill-behaved sequences such as clusters, where the frequency of truth
diverges and (most likely) no polynomial-time algorithm can predict the jumps, the
Pn(φn) might be pure guesswork.
So how well does P perform on sequences like clusters? To answer, we turn
to the question of how P behaves in the face of sequences that it ﬁnds utterly
unpredictable.
4.4
Learning Statistical Patterns
Consider the digits in the decimal expansion of π. A good reasoner thinking about
the 101,000,000th digit of π, in lieu of any eﬃcient method for predicting the digit
before they must make their prediction, should assign roughly 10% probability to
that digit being a 7.
We will now show that logical inductors learn statistical
patterns of this form.
To formalize this claim, we need some way of formalizing the idea that a sequence
is “apparently random” to a reasoner.
Intuitively, this notion must be deﬁned
relative to a speciﬁc reasoner’s computational limitations. After all, the digits of
π are perfectly deterministic; they only appear random to a reasoner who lacks
the resources to compute them. Roughly speaking, we will deﬁne a sequence to be
pseudorandom (relative to P) if there is no e.c. way to single out any one subsequence
that is more likely true than any other subsequence, not even using expressions
written in terms of the market prices (by way of expressible features):
Deﬁnition 4.4.1 (Pseudorandom Sequence). Given a set S of divergent weightings
(Deﬁnition 4.3.4), a sequence φ of decidable sentences is called pseudorandom
with frequency p over S if, for all weightings w ∈S,
lim
n→∞
P
i≤n wi · ThmΓ(φi)
P
i≤n wi
exists and is equal to p.
Note that if the sequence φ is actually randomly generated (say, by adding
(c1, c2, . . .) to the language of Γ, and tossing a coin weighted with probability p
towards heads for each i, to determine whether to add ci or ¬ci as an axiom) then
φ is pseudorandom with frequency p almost surely.5 Now:
Theorem 4.4.2 (Learning Pseudorandom Frequencies). Let φ be an e.c. sequence
of decidable sentences. If φ is pseudorandom with frequency p over the set of all
P-generable divergent weightings, then
Pn(φn) ≂n p.
(Proof in: 6.4 or D.8.)
5. Note that actually adding randomness to Γ in this fashion is not allowed, because
we assumed that the axioms of Γ are recursively enumerable. It is possible to construct a
logical inductor that has access to a source of randomness, by adding one bit of randomness
to the market each day, but that topic is beyond the scope of this paper.
30

For example, consider again the sequence πAeq7 where the nth element says
that the Ack(n, n)th decimal digit of π is a 7. The individual πAeq7n statements
are easy to write down (i.e., eﬃciently computable), but each one is diﬃcult to
decide. Assuming there’s no good way to predict the Ackermann digits of π using a
P-generable divergent weighting, P will assign probability 10% to each πAeq7n in a
timely manner, while it waits for the resources to determine whether the sentence
is true or false. Of course, on each individual πAeq7n, P’s probability will go to 0
or 1 eventually, i.e., limm→∞Pm(πAeq7n) ∈{0, 1}.
Theorem 4.4.2 still tells us nothing about how P handles clusters (deﬁned above),
because the frequency of truth in that sequence diverges, so it does not count as
pseudorandom by the above deﬁnition.
To handle this case we will weaken our
notion of pseudorandomness, so that it includes more sequences, yielding a stronger
theorem. We will do this by allowing sequences to count as pseudorandom so long
as the limiting frequency of truth converges on “independent subsequences” where
the n+1st element of the subsequence doesn’t come until after the nth element can
be decided, as described below. Refer to Garrabrant, Soares, and Taylor (2016) for
a discussion of why this is a good way to broaden the set of sequences that count
as pseudorandom.
Deﬁnition 4.4.3 (f-Patient Divergent Weighting). Let f be a deferral function.
We say that a divergent weighting w is f-patient if there is some constant C such
that, for all n,
f(n)
X
i=n
wi(P) ≤C
In other words, w is f-patient if the weight it places between days n and f(n) is
bounded.
While we are at it, we will also strengthen Theorem 4.4.2 in three additional
ways: we will allow the probabilities on the sentences to vary with time, and with
the market prices, and we will generalize ≂n to ≳n and ≲n.
Deﬁnition 4.4.4 (Varied Pseudorandom Sequence). Given a deferral function f,
a set S of f-patient divergent weightings, an e.c. sequence φ of Γ-decidable sen-
tences, and a P-generable sequence p of rational probabilities, φ is called a p-varied
pseudorandom sequence (relative to S) if, for all w ∈S,
P
i≤n wi · (pi −ThmΓ(φi))
P
i≤n wi
≂n 0.
Furthermore, we can replace ≂n with ≳n or ≲n, in which case we say φ is varied
pseudorandom above p or varied pseudorandom below p, respectively.
Theorem 4.4.5 (Learning Varied Pseudorandom Frequencies). Given an e.c. se-
quence φ of Γ-decidable sentences and a P-generable sequence p of rational proba-
bilities, if there exists some f such that φ is p-varied pseudorandom (relative to all
f-patient P-generable divergent weightings), then
Pn(φn) ≂n pn.
Furthermore, if φ is varied pseudorandom above or below p, then the ≂n may be
replaced with ≳n or ≲n (respectively).
(Proof in: D.7.)
Thus we see that P does learn to assign marginal probabilities Pn(clustersn) ≈0.5,
assuming the Ackermann digits of π are actually diﬃcult to predict. Note that while
Theorem 4.4.5 requires each pn to be rational, the fact that the theorem is general-
ized to varied pseudorandom above/below sequences means that Theorem 4.4.5 is
a strict generalization of Theorem 4.4.2 (Learning Pseudorandom Frequencies).
In short, Theorem 4.4.5 shows that logical inductors reliably learn in a timely
manner to recognize appropriate statistical patterns, whenever those patterns
(which may vary over time and with the market prices) are the best available method
for predicting the sequence using P-generable methods.
31

4.5
Learning Logical Relationships
Most of the above properties discuss the ability of a logical inductor to recognize
patterns in a single sequence—for example, they recognize e.c. sequences of theorems
in a timely manner, and they fall back on the appropriate statistical summaries in
the face of pseudorandomness. We will now examine the ability of logical inductors
to learn relationships between sequences.
Let us return to the example of the computer program prg which outputs either
0, 1, or 2 on all inputs, but for which this cannot be proven in general by Γ.
Theorem 4.2.1 (Provability Induction) says that the pattern
prg012 :=
 “prg(n) = 0 ∨prg(n) = 1 ∨prg(n) = 2”

n∈N+
will be learned, in the sense that P will assign each prg012n a probability near 1 in
a timely manner. But what about the following three individual sequences?
prg0 :=
 “prg(n) = 0”

n∈N+
prg1 :=
 “prg(n) = 1”

n∈N+
prg2 :=
 “prg(n) = 2”

n∈N+
None of the three sequences is a sequence of only theorems, so provability induction
does not have much to say. If they are utterly pseudorandom relative to r, then The-
orem 4.4.5 (Learning Varied Pseudorandom Frequencies) says that P will fall back
on the appropriate statistical summary, but that tells us little in cases where there
are predictable non-conclusive patterns (e.g., if prg(i) is more likely to output 2
when helper(i) outputs 17). In fact, if P is doing good reasoning, the probabilities
on the (prg0n, prg1n, prg2n) triplet ought to shift, as P gains new knowledge about
related facts and updates its beliefs. How could we tell if those intermediate beliefs
were reasonable?
One way is to check their sum. If P believes that prg(i) ∈{0, 1, 2} and it knows
how disjunction works, then it should be the case that whenever Pn(prg012t) ≈1,
Pn(prg0t) + Pn(prg1t) + Pn(prg2t) ≈1. And this is precisely the case. In fact,
logical inductors recognize mutual exclusion between eﬃciently computable tuples
of any size, in a timely manner:
Theorem 4.5.1 (Learning Exclusive-Exhaustive Relationships). Let φ1, . . . , φk be
k e.c. sequences of sentences, such that for all n, Γ proves that φ1
n, . . . , φk
n are
exclusive and exhaustive (i.e. exactly one of them is true). Then
Pn(φ1
n) + · · · + Pn(φk
n) ≂n 1.
Proof sketch.
(Proof in: C.11.)
Consider the trader that acts as follows. On day n, they check the prices
of φ1
n . . . φk
n. If the sum of the prices is higher (lower) than 1 by some
ﬁxed threshold ε > 0, they sell (buy) a share of each, wait until the
values of the shares are the same in every plausible world, and make a
proﬁt of ε. (It is guaranteed that eventually, in every plausible world
exactly one of the shares will be valued at 1.) If the sum goes above 1+ε
(below 1 −ε) on the main diagonal inﬁnitely often, this trader exploits
P. Logical inductors are inexploitable, so it must be the case that the
sum of the prices goes to 1 along the main diagonal.
This theorem suggests that logical inductors are good at learning to assign prob-
abilities that respect logical relationships between related sentences. To show that
this is true in full generality, we will generalize Theorem 4.5.1 to any linear inequal-
ities that hold between the actual truth-values of diﬀerent sentences.
First, we deﬁne the following convention:
32

Convention 4.5.2 (Constraint). An R-combination A can be viewed as a con-
straint, in which case we say that a valuation V satisﬁes the constraint if
V(A) ≥0.
For example, the constraint
AND := −2 + φ + ψ
says that both φ and ψ are true, and it is satisﬁed by W iﬀW(φ) = W(ψ) = 1. As
another example, the pair of constraints
XOR := (1 −φ −ψ, φ + ψ −1)
say that exactly one of φ and ψ is true, and are satisﬁed by P7 iﬀP7(φ)+P7(ψ) = 1.
Deﬁnition 4.5.3 (Bounded Combination Sequence). By BCS(P) (mnemonic:
bounded combination sequences) we denote the set of all P-generable R-
combination sequences A that are bounded, in the sense that there exists some bound
b such that ∥An∥1 ≤b for all n, where ∥−∥1 includes the trailing coeﬃcient.
Theorem 4.5.4 (Aﬃne Provability Induction). Let A ∈BCS(P) and b ∈R. If, for
all consistent worlds W ∈PC(Γ) and all n ∈N+, it is the case that W(An) ≥b,
then
Pn(An) ≳n b,
and similarly for = and ≂n, and for ≤and ≲n.
(Proof in: C.2.)
For example, consider the constraint sequence
A :=
 1 −prg0n −prg1n −prg2n

n∈N+
For all n and all consistent worlds W ∈PC(Γ), the value W(An) is 0, so applying
Theorem 4.5.5 to A, we get that Pn(An) ≂n 0. By linearity, this means
Pn(prg0n) + Pn(prg1n) + Pn(prg2n) ≂n 1,
i.e., P learns that the three sequences are mutually exclusive and exhaustive in a
timely manner, regardless of how diﬃcult prg is to evaluate. Aﬃne Provability
Induction is a generalization of this idea, where the coeﬃcients may vary (day by
day, and with the market prices).
We can push this idea further, as follows:
Theorem 4.5.5 (Aﬃne Coherence). Let A ∈BCS(P). Then
lim inf
n→∞
inf
W∈PC(Γ) W(An) ≤lim inf
n→∞P∞(An) ≤lim inf
n→∞Pn(An),
and
lim sup
n→∞Pn(An) ≤lim sup
n→∞P∞(An) ≤lim sup
n→∞
sup
W∈PC(Γ)
W(An).
(Proof in: C.1.)
This theorem ties the ground truth on A, to the value of A in the limit, to the value
of A on the main diagonal. In words, it says that if all consistent worlds value An
in (a, b) for n large, then P∞values An in (c, d) ⊆(a, b) for n large (because P∞is a
weighted mixture of all consistent worlds), and P learns to assign probabilities such
that Pn(An) ∈(c, d) in a timely manner. In colloquial terms, P learns in a timely
manner to respect all linear inequalities that actually hold between sentences, so
long as those relationships can be enumerated in polynomial time.
For example, if helper(i)=err always implies prg(i)=0, P will learn this pat-
tern, and start assigning probabilities to Pn(“prg(n)=0”) which are no lower than
33

those of Pn(“helper(n)=err”). In general, if a series of sentences obey some com-
plicated linear inequalities, then so long as those constraints can be written down
in polynomial time, P will learn the pattern, and start assigning probabilities that
respect those constraints in a timely manner.
This doesn’t mean that P will assign the correct values (0 or 1) to each sentence
in a timely manner; that would be impossible for a deductively limited reasoner.
Rather, P’s probabilities will start satisfying the constraints in a timely manner.
For example, imagine a set of complex constraints holds between seven sequences,
such that exactly three sentences in each septuplet are true, but it’s diﬃcult to tell
which three. Then P will learn this pattern, and start ensuring that its probabilities
on each septuplet sum to 3, even if it can’t yet assign particularly high probabilities
to the correct three.
If we watch an individual septuplet as P reasons, other constraints will push the
probabilities on those seven sentences up and down. One sentence might be refuted
and have its probability go to zero. Another might get a boost when P discovers
that it’s likely implied by a high-probability sentence. Another might take a hit
when P discovers it likely implies a low-probability sentence. Throughout all this,
Theorem 4.5.5 says that P will ensure that the seven probabilities always sum to
≈3. P’s beliefs on any given day arise from this interplay of many constraints,
inductively learned.
Observe that Aﬃne Coherence is a direct generalization of Theorem 4.2.1 (Prov-
ability Induction). One way to interpret this theorem is that it says that P is very
good at learning inductively to predict long-running computations. Given any e.c.
sequence of statements about the computation, if they are true then P learns to
believe them in a timely manner, and if they are false then P learns to disbelieve
them in a timely manner, and if they are related by logical constraints (such as by
exclusivity or implication) to some other e.c. sequence of statements, then P learns
to make its probabilities respect those constraints in a timely manner. This is one
of the main reasons why we think this class of algorithms deserves the name of
“logical inductor”.
Aﬃne Coherence can also be interpreted as an approximate coherence condition
on the ﬁnite belief-states of P. It says that if a certain relationship among truth
values is going to hold in the future, then P learns to make that relationship hold
approximately in its probabilities, in a timely manner.6
In fact, we can use this idea to strengthen every theorem in sections 4.2-4.4, as
below. (Readers without interest in the strengthened theorems are invited to skip
to Section 4.6.)
Aﬃne Strengthenings
Observe that Theorem 4.5.4 (Aﬃne Provability Induction) is a strengthening of
Theorem 4.2.1 (Provability Induction).
Theorem 4.5.6 (Persistence of Aﬃne Knowledge). Let A ∈BCS(P). Then
lim inf
n→∞
inf
m≥n Pm(An) = lim inf
n→∞P∞(An)
and
lim sup
n→∞
sup
m≥n
Pm(An) = lim sup
n→∞P∞(An).
(Proof in: B.5.)
6. Another notion of approximate coherence goes by the name of “inductive coherence”
(Garrabrant, Fallenstein, et al. 2016).
A reasoner is called inductively coherent if (1)
Pn(⊥) ≂n 0; (2) Pn(φn) converges whenever φ is eﬃciently computable and each φn
provably implies φn+1; and (3) for all eﬃciently computable sequences of provably mutually
exclusive and exhaustive triplets (φn, ψn, χn), Pn(φn)+Pn(ψn)+Pn(χn) ≂n 1. Garrabrant,
Fallenstein, et al. show that inductive coherence implies coherence in the limit, and argue
that this is a good notion of approximate coherence. Theorems 4.1.2 (Limit Coherence)
and 4.5.5 (Aﬃne Coherence) imply inductive coherence, and indeed, logical induction is a
much stronger notion.
34

To see that this is a generalization of Theorem 4.2.3 (Persistence of Knowledge), it
might help to ﬁrst replace A with a sequence p of rational probabilities.
Theorem 4.5.7 (Aﬃne Preemptive Learning). Let A ∈BCS(P). Then
lim inf
n→∞Pn(An) = lim inf
n→∞sup
m≥n
Pm(An)
and
lim sup
n→∞Pn(An) = lim sup
n→∞
inf
m≥n Pm(An) .
(Proof in: B.2.)
Deﬁnition 4.5.8 (Determined via Γ). We say that a R-combination A is deter-
mined via Γ if, in all worlds W ∈PC(Γ), the value W(A) is equal. Let ValΓ(A)
denote this value.
Similarly, a sequence A of R-combinations is said to be determined via Γ if An
is determined via Γ for all n.
Theorem 4.5.9 (Aﬃne Recurring Unbiasedness). If A ∈BCS(P) is determined
via Γ, and w is a P-generable divergent weighting,
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
(Proof
in: D.1.)
Theorem 4.5.10 (Aﬃne Unbiasedness from Feedback). Given A ∈BCS(P) that is
determined via Γ, a strictly increasing deferral function f such that ValΓ(An) can
be computed in time O(f(n+1)), and a P-generable divergent weighting w such that
the support of w is contained in the image of f,
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on A”.
(Proof in: D.4.)
Theorem 4.5.11 (Learning Pseudorandom Aﬃne Sequences). Given a A ∈BCS(P)
which is determined via Γ, if there exists deferral function f such that for any
P-generable f-patient divergent weighting w,
P
i≤n wi · ValΓ(Ai)
P
i≤n wi
≳n 0,
then
Pn(An) ≳n 0,
and similarly for ≂n, and ≲n.
(Proof in: D.6.)
4.6
Non-Dogmatism
Cromwell’s rule says that a reasoner should not assign extreme probabilities (0
or 1) except when applied to statements that are logically true or false. The rule
was named by Lindley (1991), in light of the fact that Bayes’ theorem says that a
Bayesian reasoner can never update away from probabilities 0 or 1, and in reference
to the famous plea:
35

I beseech you, in the bowels of Christ, think it possible that you may be
mistaken.
– Oliver Cromwell
The obvious generalization of Cromwell’s rule to a setting where a reasoner is uncer-
tain about logic is that they also should not assign extreme probabilities to sentences
that have not yet been proven or disproven. Logical inductors do not satisfy this
rule, as evidenced by the following theorem:
Theorem 4.6.1 (Closure under Finite Perturbations). Let P and P′ be markets
with Pn = P′
n for all but ﬁnitely many n. Then P is a logical inductor if and only
if P′ is a logical inductor.
(Proof in: G.7.)
This means that we can take a logical inductor, completely ruin its beliefs on the
23rd day (e.g., by setting P23(φ) = 0 for all φ), and it will still be a logical inductor.
Nevertheless, there is still a sense in which logical inductors are non-dogmatic, and
can “think it possible that they may be mistaken”:
Theorem 4.6.2 (Non-Dogmatism). If Γ ⊬φ then P∞(φ) < 1, and if Γ ⊬¬φ then
P∞(φ) > 0.
Proof sketch.
(Proof in: G.4.)
Consider a trader that watches φ and buys whenever it gets low, as
follows. The trader starts with $1. They spend their ﬁrst 50 cents when
Pn(φ) < 1/2, purchasing one share. They spend their next 25 cents when
Pn(φ) < 1/4, purchasing another share. They keep waiting for Pn(φ) to
drop low enough that they can spend the next half of their initial wealth
to buy one more share. Because φ is independent, there always remains
at least one world W such that W(φ) = 1, so if Pn(φ) →0 as n →∞then
their maximum plausible proﬁts are $1 + $1 + $1 +. . . which diverges,
and they exploit the market. Thus, P∞(φ) must be bounded away from
zero.
In other words, if φ is independent from Γ, then P’s beliefs about φ won’t get
stuck converging to 0 or 1. By Theorem 4.6.1 (Closure under Finite Perturbations),
P may occasionally jump to unwarranted conclusions—believing with “100% cer-
tainty”, say, that Euclid’s ﬁfth postulate follows from the ﬁrst four—but it always
corrects these errors, and eventually develops conservative beliefs about independent
sentences.
Theorem 4.6.2 guarantees that P will be reasonable about independent sentences,
but it doesn’t guarantee reasonable beliefs about theories, because theories can
require inﬁnitely many axioms. For example, let Γ be a theory of pure ﬁrst-order
logic, and imagine that the language L has a free binary relation symbol “ ∈”.
Now consider the sequence ZFCaxioms of ﬁrst-order axioms of Zermelo-Fraenkel
set theory (ZFC) which say to interpret “∈” in the set-theoretic way, and note that
ZFCaxioms is inﬁnite.
Each individual sentence ZFCaxiomsn is consistent with
ﬁrst-order logic, but if P∞’s odds on each axiom were 50:50 and independent, then
it would say that the probability of them all being true simultaneously was zero.
Fortunately, for any computably enumerable sequence of sentences that are mutually
consistent, P∞assigns positive probability to them all being simultaneously true.
Theorem 4.6.3 (Uniform Non-Dogmatism). For any computably enumerable se-
quence of sentences φ such that Γ ∪φ is consistent, there is a constant ε > 0 such
that for all n,
P∞(φn) ≥ε.
(Proof in: G.2.)
If φn is the conjunction of the ﬁrst n axioms of ZFC, Theorem 4.6.3 shows that P∞
assigns positive probability to theories in which the symbol “∈” satisﬁes all axioms
of ZFC (assuming ZFC is consistent).
36

Reasoning about individual sentences again, we can put bounds on how far each
sentence φ is bounded away from 0 and 1, in terms of the preﬁx complexity κ(φ) of
φ, i.e., the length of the shortest preﬁx that causes a ﬁxed universal Turing machine
to output φ.7
Theorem 4.6.4 (Occam Bounds). There exists a ﬁxed positive constant C such
that for any sentence φ with preﬁx complexity κ(φ), if Γ ⊬¬φ, then
P∞(φ) ≥C2−κ(φ),
and if Γ ⊬φ, then
P∞(φ) ≤1 −C2−κ(φ).
(Proof in: G.3.)
This means that if we add a sequence of constant symbols (c1, c2, . . .) not men-
tioned in Γ to the language L, then P’s beliefs about statements involving those
constants will depend on the complexity of the claim. Roughly speaking, if you ask
after the probability of a claim like “c1 = 10 ∧c2 = 7 ∧. . . ∧cn = −3” then the
answer will be no lower than the probability that a simplicity prior assigns to the
shortest program that outputs (10, 7, . . ., −3).
In fact, the probability may be a fair bit higher, if the claim is part of a particu-
larly simple sequence of sentences. In other words, logical inductors can be used to
reason about empirical uncertainty as well as logical uncertainty, by using P∞as a
full-ﬂedged sequence predictor:
Theorem 4.6.5 (Domination of the Universal Semimeasure). Let (b1, b2, . . .) be a
sequence of zero-arity predicate symbols in L not mentioned in Γ, and let σ≤n =
(σ1, . . . , σn) be any ﬁnite bitstring. Deﬁne
P∞(σ≤n) := P∞(“(b1 ↔σ1 = 1) ∧(b2 ↔σ2 = 1) ∧. . . ∧(bn ↔σn = 1)”),
such that, for example, P∞(01101) = P∞(“¬b1 ∧b2 ∧b3 ∧¬b4 ∧b5”). Let M be
a universal continuous semimeasure. Then there is some positive constant C such
that for any ﬁnite bitstring σ≤n,
P∞(σ≤n) ≥C · M(σ≤n).
(Proof in: G.5.)
In other words, logical inductors can be viewed as a computable approximation to
a normalized probability distribution that dominates the universal semimeasure. In
fact, this dominance is strict:
Theorem 4.6.6 (Strict Domination of the Universal Semimeasure). The universal
continuous semimeasure does not dominate P∞; that is, for any positive constant C
there is some ﬁnite bitstring σ≤n such that
P∞(σ≤n) > C · M(σ≤n).
(Proof in: G.6.)
In particular, by Theorem 4.6.3 (Uniform Non-Dogmatism), logical inductors as-
sign positive probability to the set of all completions of theories like PA and ZFC,
7. We use preﬁx complexity (the length of the shortest preﬁx that causes a UTM to
output φ) instead of Kolmogorov complexity (the length of the shortest complete program
that causes a UTM to output φ) because it makes the proof slightly easier. (And, in the
opinion of the authors, preﬁx complexity is the more natural concept.)
Both types of
complexity are deﬁned relative to an arbitrary choice of universal Turing machine (UTM),
but our theorems hold for every logical inductor regardless of the choice of UTM, because
changing the UTM only amounts to changing the constant terms by some ﬁxed amount.
37

whereas universal semimeasures do not. This is why we can’t construct approxi-
mately coherent beliefs about logic by ﬁxing an enumeration of logical sentences
and conditioning a universal semimeasure on more axioms of Peano arithmetic each
day: the probabilities that the semimeasure assigns to those conjunctions must go
to zero, so the conditional probabilities may misbehave. (If this were not the case,
it would be possible to sample a complete extension of Peano arithmetic with posi-
tive probability, because universal semimeasures are approximable from below; but
this is impossible. See the proof of Theorem 4.6.6 for details.) While P∞is limit-
computable, it is not approximable from below, so it can and does outperform the
universal semimeasure when reasoning about arithmetical claims.
4.7
Conditionals
One way to interpret Theorem 4.6.5 (Domination of the Universal Semimeasure)
is that when we condition P∞on independent sentences about which it knows
nothing, it performs empirical (scientiﬁc) induction. We will now show that when
we condition P, it also performs logical induction.
In probability theory, it is common to discuss conditional probabilities such as
Pr(A | B) := Pr(A ∧B)/Pr(B) (for any B with Pr(B) > 0), where Pr(A | B)
is interpreted as the probability of A restricted to worlds where B is true.
In
the domain of logical uncertainty, we can deﬁne conditional probabilities in the
analogous way:
Deﬁnition 4.7.1 (Conditional Probability). Let φ and ψ be sentences, and let V
be a valuation with V(ψ) > 0. Then we deﬁne
V(φ | ψ) :=
V(φ ∧ψ)/V(ψ)
if V(φ ∧ψ) < V(ψ)
1
otherwise.
Given a valuation sequence V, we deﬁne
V(−| ψ) := (V1(−| ψ), V2(−| ψ), . . .).
Deﬁning V(φ | ψ) to be 1 if V(ψ) = 0 is nonstandard, but convenient for our theorem
statements and proofs. The reader is welcome to ignore the conditional probabilities
in cases where V(ψ) = 0, or to justify our deﬁnition from the principle of explosion
(which says that from a contradiction, anything follows). This deﬁnition also caps
V(φ | ψ) at 1, which is necessary because there’s no guarantee that V knows that
φ ∧ψ should have a lower probability than ψ. For example, if it takes P more than
17 days to learn how “∧” interacts with φ and ψ, then it might be the case that
P17(φ ∧ψ) = 0.12 and P17(ψ) = 0.01, in which case the uncapped “conditional
probability” of φ ∧ψ given ψ according to P17 would be twelve hundred percent.
This fact doesn’t exactly induce conﬁdence in P(−| ψ). Nevertheless, we have
the following theorem:
Theorem 4.7.2 (Closure Under Conditioning). The sequence P(−| ψ) is a logical
inductor over Γ ∪{ψ}. Furthermore, given any eﬃciently computable sequence ψ
of sentences, the sequence
(P1(−| ψ1), P2(−| ψ1 ∧ψ2), P3(−| ψ1 ∧ψ2 ∧ψ3), . . .) ,
where the nth pricing is conditioned on the ﬁrst n sentences in ψ, is a logical inductor
over Γ ∪{ψi | i ∈N+}.
(Proof in: G.8.)
In other words, if we condition logical inductors on logical sentences, the result
is still a logical inductor, and so the conditional probabilities of a logical inductor
continues to satisfy all the desirable properties satisﬁed by all logical inductors. This
also means that one can obtain a logical inductor for Peano arithmetic by starting
with a logical inductor over an empty theory, and conditioning it on PA.
With that idea in mind, we will now begin examining questions about logical
inductors that assume Γ can represent computable functions, such as questions
about P’s beliefs about Γ, computer programs, and itself.
38

4.8
Expectations
In probability theory, it is common to ask the expected (average) value of a variable
that takes on diﬀerent values in diﬀerent possible worlds.
Emboldened by our
success with conditional probabilities, we will now deﬁne a notion of the expected
values of logical variables, and show that these are also fairly well-behaved. This
machinery will be useful later when we ask logical inductors for their beliefs about
themselves.
We begin by deﬁning a notion of logically uncertain variables, which play a
role analogous to the role of random variables in probability theory. For the sake of
brevity, we will restrict our attention to logically uncertain variables with their value
in [0, 1]; it is easy enough to extend this notion to a notion of arbitrary bounded real-
valued logically uncertain variables. (It does, however, require carrying a variable’s
bounds around everywhere, which makes the notation cumbersome.)
To deﬁne logically uncertain variables, we will need to assume that Γ is capable
of representing rational numbers and proving things about them. Later, we will
use expected values to construct sentences that talk about things like the expected
outputs of a computer program.
Thus, in this section and in the remainder of
Section 4, we will assume that Γ can represent computable functions.
Deﬁnition 4.8.1 (Logically Uncertain Variable). A logically uncertain vari-
able, abbreviated LUV, is any formula X free in one variable that deﬁnes a unique
value via Γ, in the sense that
Γ ⊢∃x: (X(x) ∧∀x′ : X(x′) →x′ = x).
We refer to that value as the value of X. If Γ proves that the value of X is in [0, 1],
we call X a [0, 1]-LUV.
Given a [0, 1]-LUV X and a consistent world W ∈PC(Γ), the value of X in
W is deﬁned to be
W(X) := sup {x ∈[0, 1] | W(“X ≥x”) = 1} .
In other words, W(X) is the supremum of values that do not exceed X according to
W. (This rather roundabout deﬁnition is necessary in cases where W assigns X a
non-standard value.)
We write U for the set of all [0, 1]-LUVs. When manipulating logically uncertain
variables, we use shorthand like “X < 0.5” for “∀x: X(x) →x < 0.5”. See Section 2
for details.
As an example, Half := “ν = 0.5” is a LUV, where the unique real number that
makes Half true is rather obvious. A more complicated LUV is
TwinPrime := “1 if the twin prime conjecture is true, 0 otherwise”;
this is a deterministic quantity (assuming Γ actually proves the twin prime conjec-
ture one way or the other), but it’s reasonable for a limited reasoner to be uncertain
about the value of that quantity. In general, if f : N+ →[0, 1] is a computable func-
tion then “f(7)” is a LUV, because “f(7)” is shorthand for the formula “γf(7, ν)”,
where γf is the predicate of Γ representing f.
With LUVs in hand, we can deﬁne a notion of P’s expected value for a LUV X
on day n with precision k. The obvious idea is to take the sum
lim
k→∞
k−1
X
i=0
i
k Pn(“i/k < X ≤(i + 1)/k”).
However, if Pn hasn’t yet ﬁgured out that X pins down a unique value, then it
might put high probability on X being in multiple diﬀerent intervals, and the sim-
ple integral of a [0, 1]-valued LUV could fall outside the [0, 1] interval. This is a
nuisance when we want to treat the expectations of [0, 1]-LUVs as other [0, 1]-LUVs,
39

so instead, we will deﬁne expectations using an analog of a cumulative distribution
function. In probability theory, the expectation of a [0, 1]-valued random variable
V with density function ρV is given by E(V ) =
R 1
0 x · ρV (x)dx. We can rewrite this
using integration by parts as
E(V ) =
Z 1
0
Pr(V > x)dx.
This motivates the following deﬁnition of expectations for LUVs:
Deﬁnition 4.8.2 (Expectation). For a given valuation V, we deﬁne the approxi-
mate expectation operator EV
k for V with precision k by
EV
k(X) :=
k−1
X
i=0
1
k V(“X > i/k”).
where X is a [0, 1]-LUV.
This has the desirable property that EV
k(X) ∈[0, 1], because V(−) ∈[0, 1].
We will often want to take a limit of EPn
k (X) as both k and n approach ∞. We
hereby make the fairly arbitrary choice to focus on the case k = n for simplicity,
adopting the shorthand
En := EPn
n .
In other words, when we examine how a logical inductor’s expectations change
on a sequence of sentences over time, we will (arbitrarily) consider approximate
expectations that gain in precision at a rate of one unit per day.
We will now show that the expectation operator En possesses properties that
make it worthy of that name.
Theorem 4.8.3 (Expectations Converge). The limit E∞: S →[0, 1] deﬁned by
E∞(X) := lim
n→∞En(X)
exists for all X ∈U.
(Proof in: E.4.)
Note that E∞(X) might not be rational.
Because P∞deﬁnes a probability measure over PC(Γ), E∞(X) is the average
value of W(X) across all consistent worlds (weighted by P∞). In other words, every
LUV X can be seen as a random variable with respect to the measure P∞, and E∞
acts as the standard expectation operator on P∞. Furthermore,
Theorem 4.8.4 (Linearity of Expectation). Let a, b be bounded P-generable se-
quences of rational numbers, and let X, Y , and Z be e.c. sequences of [0, 1]-LUVs.
If we have Γ ⊢Zn = anXn + bnYn for all n, then
anEn(Xn) + bnEn(Yn) ≂n En(Zn).
(Proof in: E.9.)
For our next result, we want a LUV which can be proven to take value 1 if φ is
true and 0 otherwise.
Deﬁnition 4.8.5 (Indicator LUV). For any sentence φ, we deﬁne its indicator
LUV by the formula
1(φ) := “(φ ∧(ν = 1)) ∨(¬φ ∧(ν = 0))”.
Observe that 1(φ)(1) is equivalent to φ, and 1(φ)(0) is equivalent to ¬φ.
40

Theorem 4.8.6 (Expectations of Indicators). Let φ be an e.c. sequence of sentences.
Then
En(1(φn)) ≂n Pn(φn).
(Proof in: E.10.)
In colloquial terms, Theorem 4.8.6 says that a logical inductor learns that asking
for the expected value of 1(φ) is the same as asking for the probability of φ.
To further demonstrate that expectations work as expected, we will show that
they satisfy generalized versions of all theorems proven in sections 4.2-4.5. (Readers
without interest in the versions of those theorems for expectations are invited to
skip to Section 4.9.)
Collected Theorems for Expectations
Deﬁnition 4.8.7 (LUV Valuation). A LUV valuation is any function U : U →
[0, 1]. Note that EV
n and EV
∞are LUV valuations for any valuation V and n ∈N+,
and that every world W ∈PC(Γ) is a LUV valuation.
Deﬁnition 4.8.8 (LUV Combination). An F-LUV-combination B : U ∪{1} →
F is an aﬃne expression of the form
B := c + α1X1 + · · · + αkXk,
where (X1, . . . , Xk) are [0, 1]-LUVs and (c, α1, . . . , αk) are in F.
An EF-LUV-
combination, an R-LUV-combination, and a Q-LUV-combination are de-
ﬁned similarly.
The following concepts are all deﬁned analogously to how they are deﬁned for
sentence combinations: B[1], B[X], rank(B), U(B) for any LUV valuation U, F-
LUV-combination progressions, EF-LUV-combination progressions, and
P-generable LUV-combination sequences. (See deﬁnitions 3.4.6 and 4.3.5 for de-
tails.)
Deﬁnition
4.8.9
(Bounded
LUV-Combination
Sequence).
By
BLCS(P)
(mnemonic:
bounded LUV-combination sequences) we denote the set of
all P-generable R-LUV-combination sequences B that are bounded, in the sense
that there exists some bound b such that ∥Bn∥1 ≤b for all n, where ∥−∥1 includes
the trailing coeﬃcient.
Theorem 4.8.10 (Expectation Provability Induction). Let B ∈BLCS(P) and
b ∈R. If, for all consistent worlds W ∈PC(Γ) and all n ∈N+, it is the case that
W(Bn) ≥b, then
En(Bn) ≳n b,
and similarly for = and ≂n, and for ≤and ≲n.
(Proof in: E.8.)
Theorem 4.8.11 (Expectation Coherence). Let B ∈BLCS(P). Then
lim inf
n→∞
inf
W∈PC(Γ) W(Bn) ≤lim inf
n→∞E∞(Bn) ≤lim inf
n→∞En(Bn),
and
lim sup
n→∞En(Bn) ≤lim sup
n→∞E∞(Bn) ≤lim sup
n→∞
sup
W∈PC(Γ)
W(Bn).
(Proof in: E.7.)
Theorem 4.8.12 (Persistence of Expectation Knowledge). Let B ∈BLCS(P).
Then
lim inf
n→∞
inf
m≥n Em(Bn) = lim inf
n→∞E∞(Bn)
and
lim sup
n→∞
sup
m≥n
Em(Bn) = lim sup
n→∞E∞(Bn).
(Proof in: E.6.)
41

Theorem 4.8.13 (Expectation Preemptive Learning). Let B ∈BLCS(P). Then
lim inf
n→∞En(Bn) = lim inf
n→∞sup
m≥n
Em(Bn)
and
lim sup
n→∞En(Bn) = lim sup
n→∞
inf
m≥n Em(Bn) .
(Proof in: E.3.)
Deﬁnition 4.8.14 (Determined via Γ (for LUV-Combinations)). We say that a R-
LUV-combination B is determined via Γ if, in all worlds W ∈PC(Γ), the value
W(B) is equal. Let ValΓ(B) denote this value.
Similarly, a sequence B of R-LUV-combinations is said to be determined via Γ
if Bn is determined via Γ for all n.
Theorem 4.8.15 (Expectation Recurring Unbiasedness). If B ∈BLCS(P) is de-
termined via Γ, and w is a P-generable divergent weighting weighting such that the
support of w is contained in the image of f,
P
i≤n wi · (Ei(Bi) −ValΓ(Bi))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
Theorem
4.8.16
(Expectation Unbiasedness From Feedback). Given B
∈
BLCS(P) that is determined via Γ, a strictly increasing deferral function f such
that ValΓ(An) can be computed in time O(f(n + 1)), and a P-generable divergent
weighting w,
P
i≤n wi · (Ei(Bi) −ValΓ(Bi))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on B”.
(Proof in: E.12.)
Theorem 4.8.17 (Learning Pseudorandom LUV Sequences). Given a B
∈
BLCS(P) which is determined via Γ, if there exists a deferral function f such that
for any P-generable f-patient divergent weighting w,
P
i≤n wi · ValΓ(Bi)
P
i≤n wi
≳n 0,
then
En(Bn) ≳n 0.
(Proof in: E.13.)
4.9
Trust in Consistency
The theorems above all support the hypothesis that logical inductors develop rea-
sonable beliefs about logic. One might then wonder what a logical inductor has to
say about some of the classic questions in meta-mathematics. For example, what
does a logical inductor over PA say about the consistency of Peano arithmetic?
Deﬁnition 4.9.1 (Consistency Statement). Given a recursively axiomatizable the-
ory Γ′, deﬁne the n-consistency statement of Γ′ to be the formula with one free
variable ν such that
Con(Γ′)(ν) := “There is no proof of ⊥from Γ′ with ν or fewer symbols”,
42

written in L using a Gödel encoding. For instance, Con(PA)(“ Ack(10, 10)”) says
that any proof of ⊥from PA requires at least Ack(10, 10) symbols.
We further deﬁne “Γ′ is consistent” to be the universal generalization
“∀n: there is no proof of ⊥from Γ′ in n or fewer symbols”,
and “Γ′ is inconsistent” for its negation.
Theorem 4.9.2 (Belief in Finitistic Consistency). Let f be any computable func-
tion. Then
Pn(Con(Γ)(“f(n)”)) ≂n 1.
(Proof in: C.4.)
In other words, if Γ is in fact consistent, then P learns to trust it for arbitrary ﬁnite
amounts of time. For any fast-growing function f you can name, P eventually learns
to believe Γ is consistent for proofs of length at most f(n), by day n at the latest.
In colloquial terms, if we take a logical inductor over PA and show it a computable
function f that, on each input n, tries a new method for ﬁnding an inconsistency
in PA, then the logical inductor will stare at the function for a while and eventually
conclude that it’s not going to succeed (by learning to assign low probability to f(n)
proving ⊥from PA by day n at the latest, regardless of how long f runs). That is
to say, a logical inductor over PA learns to trust Peano arithmetic inductively.
By the same mechanism, a logical inductor over Γ can learn inductively to
trust the consistency of any consistent theory, including consistent theories that are
stronger than Γ (in the sense that they can prove Γ consistent):
Theorem 4.9.3 (Belief in the Consistency of a Stronger Theory). Let Γ′ be any
recursively axiomatizable consistent theory. Then
Pn(Con(Γ′)(“f(n)”)) ≂n 1.
(Proof in: C.5.)
For instance, a logical inductor over PA can learn inductively to trust the consistency
of ZFC for ﬁnite proofs of arbitrary length (assuming ZFC is in fact consistent).
These two theorems alone are unimpressive. Any algorithm that assumes con-
sistency until proven otherwise can satisfy these theorems, and because every in-
consistent theory admits a ﬁnite proof of inconsistency, those naïve algorithms will
disbelieve any inconsistent theory eventually. But those algorithms will still believe
inconsistent theories for quite a long time, whereas logical inductors learn to distrust
inconsistent theories in a timely manner:
Theorem 4.9.4 (Disbelief in Inconsistent Theories). Let Γ′ be an e.c. sequence of
recursively axiomatizable inconsistent theories. Then
Pn(“Γ′
n is inconsistent”) ≂n 1,
so
Pn(“Γ′
n is consistent”) ≂n 0.
(Proof in: C.6.)
In other words, logical inductors learn in a timely manner to distrust inconsistent
theories that can be eﬃciently named, even if the shortest proofs of inconsistency
are very long.
Note that Theorem 4.9.2 (Belief in Finitistic Consistency) does not say
P∞(“Γ is consistent”)
is equal to 1, nor even that it’s particularly high. On the contrary, by Theorem 4.6.2
(Non-Dogmatism), the limiting probability on that sentence is bounded away from
43

0 and 1 (because both that sentence and its negation are consistent with Γ). In-
tuitively, D never reveals evidence against the existence of non-standard numbers,
so P remains open to the possibility. This is important for Theorem 4.7.2 (Closure
Under Conditioning), which say that logical inductors can safely be conditioned on
any sequence of statements that are consistent with Γ, but it also means that P will
not give an aﬃrmative answer to the question of whether PA is consistent in full
generality.
In colloquial terms, if you hand a logical inductor any particular computation, it
will tell you that that computation isn’t going to output a proof ⊥from the axioms
of PA, but if you ask whether PA is consistent in general, it will start waxing
philosophical about non-standard numbers and independent sentences—not unlike
a human philosopher.
A reasonable objection here is that Theorem 4.9.2 (Belief in Finitistic Consis-
tency) is not talking about the consistency of the Peano axioms, it’s talking about
computations that search for proofs of contradiction from PA.
This is precisely
correct, and brings us to our next topic.
4.10
Reasoning about Halting
Consider the famous halting problem of Turing (1936). Turing proved that there
is no general algorithm for determining whether or not an arbitrary computation
halts. Let’s examine what happens when we confront logical inductors with the
halting problem.
Theorem 4.10.1 (Learning of Halting Patterns). Let m be an e.c. sequence of
Turing machines, and x be an e.c. sequence of bitstrings, such that mn halts on
input xn for all n. Then
Pn(“mn halts on input xn”) ≂n 1.
(Proof in: C.7.)
Note that the individual Turing machines do not need to have fast runtime. All
that is required is that the sequence m be eﬃciently computable, i.e., it must be
possible to write out the source code specifying mn in time polynomial in n. The
runtime of an individual mn is immaterial for our purposes. So long as the mn all
halt on the corresponding xn, P recognizes the pattern and learns to assign high
probability to “mn halts on input xn” no later than the nth day.
Of course, this is not so hard on its own—a function that assigns probability 1
to everything also satisﬁes this property. The real trick is separating the halting
machines from the non-halting ones. This is harder. It is easy enough to show that
P learns to recognize e.c. sequences of machines that provably fail to halt:
Theorem 4.10.2 (Learning of Provable Non-Halting Patterns). Let q be an e.c.
sequence of Turing machines, and y be an e.c. sequence of bitstrings, such that qn
provably fails to halt on input yn for all n. Then
Pn(“qn halts on input yn”) ≂n 0.
(Proof in: C.8.)
Of course, it’s not too diﬃcult to disbelieve that the provably-halting machines
will halt; what makes the above theorem non-trivial is that P learns in a timely
manner to expect that those machines won’t halt.
Together, the two theorems
above say that if there is any eﬃcient method for generating computer programs
that deﬁnitively either halt or don’t (according to Γ) then P will learn the pattern.
The above two theorems only apply to cases where Γ can prove that the machine
either halts or doesn’t. The more interesting case is the one where a Turing ma-
chine q fails to halt on input y, but Γ is not strong enough to prove this fact. In this
case, P∞’s probability of q halting on input y is positive, by Theorem 4.6.2 (Non–
Dogmatism). Nevertheless, P still learns to stop expecting that those machines will
halt after any reasonable amount of time:
44

Theorem 4.10.3 (Learning not to Anticipate Halting). Let q be an e.c. sequence
of Turing machines, and let y be an e.c. sequence of bitstrings, such that qn does
not halt on input yn for any n. Let f be any computable function. Then
Pn(“qn halts on input yn within f(n) steps”) ≂n 0.
(Proof in: C.9.)
For example, let y be an enumeration of all bitstrings, and let q be the constant
sequence (q, q, . . .) where q is a Turing machine that does not halt on any input. If
Γ cannot prove this fact, then P will never be able to attain certainty about claims
that say q fails to halt, but by Theorem 4.10.3, it still learns to expect that q will
run longer than any computable function you can name. In colloquial terms, while
P won’t become certain that non-halting machines don’t halt (which is impossible),
it will put them in the “don’t hold your breath” category (along with some long-
running machines that do halt, of course).
These theorems can be interpreted as justifying the intuitions that many com-
puter scientists have long held towards the halting problem: It is impossible to tell
whether or not a Turing machine halts in full generality, but for large classes of
well-behaved computer programs (such as e.c. sequences of halting programs and
provably non-halting programs) it’s quite possible to develop reasonable and accu-
rate beliefs. The boundary between machines that compute fast-growing functions
and machines that never halt is diﬃcult to distinguish, but even in those cases,
it’s easy to learn to stop expecting those machines to halt within any reasonable
amount of time. (See also the work of Calude and Stay [2008] for other formal
results backing up this intuition.)
One possible objection here is that the crux of the halting problem (and of the
Γ-trust problem) are not about making good predictions, they are about handling
diagonalization and paradoxes of self-reference. Gödel’s incompleteness theorem
constructs a sentence that says “there is no proof of this sentence from the axioms
of PA”, and Turing’s proof of the undecidability of the halting problem constructs
a machine which halts iﬀsome other machine thinks it loops. P learning to trust Γ
is diﬀerent altogether from P learning to trust itself. So let us turn to the topic of
P’s beliefs about P.
4.11
Introspection
Because we’re assuming Γ can represent computable functions, we can write sen-
tences describing the beliefs of P at diﬀerent times. What happens when we ask P
about sentences that refer to itself?
For instance, consider a sentence ψ := “Pn(φ) > 0.7” for some speciﬁc n and
φ, where P’s beliefs about ψ should depend on what its beliefs about φ are on the
nth day. Will P ﬁgure this out and get the probabilities right on day n? For any
particular φ and n it’s hard to say, because it depends on whether P has learned
how ψ relates to P and φ yet. If however we take an e.c. sequence of ψ which all
say “φ will have probability greater than 0.7 on day n” with n varying, then we can
guarantee that P will learn the pattern, and start having accurate beliefs about its
own beliefs:
Theorem 4.11.1 (Introspection). Let φ be an e.c. sequence of sentences, and a,
b be P-generable sequences of probabilities. Then, for any e.c. sequence of positive
rationals δ →0, there exists a sequence of positive rationals ε →0 such that for all
n:
1. if Pn(φn) ∈(an + δn, bn −δn), then
Pn(“an < Pn(φn) < bn”) > 1 −εn,
2. if Pn(φn) /∈(an −δn, bn + δn), then
Pn(“an < Pn(φn) < bn”) < εn.
45

(Proof in: F.1.)
In other words, for any pattern in P’s beliefs that can be eﬃciently written down
(such as “P’s probabilities on φ are between a and b on these days”), P learns to
believe the pattern if it’s true, and to disbelieve it if it’s false (with vanishing error).
At a ﬁrst glance, this sort of self-reﬂection may seem to make logical inductors
vulnerable to paradox. For example, consider the sequence of sentences
χ0.5 := (“Pn(χ0.5
n ) < 0.5”)n∈N+
such that χ0.5
n
is true iﬀP assigns it a probability less than 50% on day n. Such
a sequence can be deﬁned by Gödel’s diagonal lemma. These sentences are prob-
abilistic versions of the classic “liar sentence”, which has caused quite a ruckus in
the setting of formal logic (Grim 1991; McGee 1990; Glanzberg 2001; Gupta and
Belnap 1993; Eklund 2002). Because our setting is probabilistic, it’s perhaps most
closely related to the “unexpected hanging” paradox—χ0.5
n
is true iﬀP thinks it is
unlikely on day n. How do logical inductors handle this sort of paradox?
Theorem 4.11.2 (Paradox Resistance). Fix a rational p ∈(0, 1), and deﬁne an
e.c. sequence of “paradoxical sentences” χp satisfying
Γ ⊢χp
n ↔

Pn(χp
n) < p

for all n. Then
lim
n→∞Pn(χp
n) = p.
(Proof in: F.2.)
A logical inductor responds to paradoxical sentences χp by assigning probabilities
that converge on p. For example, if the sentences say “P will assign me a probability
less than 80% on day n”, then Pn (once it has learned the pattern) starts assigning
probabilities extremely close to 80%—so close that traders can’t tell if it’s slightly
above or slightly below. By Theorem 4.3.6 (Recurring Unbiasedness), the frequency
of truth in χp
≤n will have a limit point at 0.8 as n →∞, and by the deﬁnition of
logical induction, there will be no eﬃciently expressible method for identifying a
bias in the price.
Let us spend a bit of time understanding this result. After day n, χ0.8
n
is “easy”
to get right, at least for someone with enough computing power to compute Pn(χ0.8
n )
to the necessary precision (it will wind up very close to 0.8 for large n). Before day
n, we can interpret the probability of χ0.8
n
as the price of a share that’s going to
pay out $1 if the price on day n is less than 80¢, and $0 otherwise. What’s the
value of this share? Insofar as the price on day n is going to be low, the value is
high; insofar as the price is going to be high, the value is low. So what actually
happens on the nth day? Smart traders buy χ0.8
n
if its price is lower than 80¢, and
sell it if its price is higher than 80¢. By the continuity constraints on the traders,
each one has a price at which they stop buying χ0.8
n , and Theorem 4.11.2 (Paradox
Resistance) tells us that the stable price exists extremely close to 80¢. Intuitively,
it must be so close that traders can’t tell which way it’s going to go, biased on the
low side, so that it looks 80% likely to be below and 20% likely to be above to any
eﬃcient inspection. For if the probability seemed more than 80% likely to be below,
traders would buy; and if it seemed anymore than 20% likely to be above, traders
would sell.
To visualize this, imagine that your friend owns a high-precision brain-scanner
and can read oﬀyour beliefs. Imagine they ask you what probability you assign
to the claim “you will assign probability <80% to this claim at precisely 10am
tomorrow”. As 10am approaches, what happens to your belief in this claim? If you
become extremely conﬁdent that it’s going to be true, then your conﬁdence should
drop. But if you become fairly conﬁdent it’s going to be false, then your conﬁdence
should spike. Thus, your probabilities should oscillate, pushing your belief so close
46

to 80% that you’re not quite sure which way the brain scanner will actually call it.
In response to a paradoxical claim, this is exactly how P behaves, once it’s learned
how the paradoxical sentences work.
Thus, logical inductors have reasonable beliefs about their own beliefs even in
the face of paradox. We can further show that logical inductors have “introspective
access” to their own beliefs and expectations, via the medium of logically uncertain
variables:
Theorem 4.11.3 (Expectations of Probabilities). Let φ be an eﬃciently computable
sequence of sentences. Then
Pn(φn) ≂n En(“Pn(φn)”).
(Proof in: F.3.)
Theorem 4.11.4 (Iterated Expectations). Suppose X is an eﬃciently computable
sequence of LUVs. Then
En(Xn) ≂n En(“En(Xn)”).
(Proof in: F.4.)
Next, we turn our attention to the question of what a logical inductor believes
about its future beliefs.
4.12
Self-Trust
The coherence conditions of classical probability theory guarantee that a probabilis-
tic reasoner trusts their future beliefs, whenever their beliefs change in response to
new empirical observations. For example, if a reasoner Pr(−) knows that tomor-
row they’ll see some evidence e that will convince them that Miss Scarlet was the
murderer, then they already believe that she was the murderer today:
Pr(Scarlet) = Pr(Scarlet | e)Pr(e) + Pr(Scarlet | ¬e)Pr(¬e).
In colloquial terms, this says “my current beliefs are already a mixture of my ex-
pected future beliefs, weighted by the probability of the evidence that I expect to
see.”
Logical inductors obey similar coherence conditions with respect to their future
beliefs, with the diﬀerence being that a logical inductor updates its belief by gaining
more knowledge about logical facts, both by observing an ongoing process of deduc-
tion and by thinking for longer periods of time. Thus, the self-trust properties of a
logical inductor follow a slightly diﬀerent pattern:
Theorem 4.12.1 (Expected Future Expectations). Let f be a deferral function (as
per Deﬁnition 4.3.7), and let X denote an e.c. sequence of [0, 1]-LUVs. Then
En(Xn) ≂n En(“Ef(n)(Xn)”).
(Proof in: F.5.)
Roughly speaking, Theorem 4.12.1 says that a logical inductor’s current expectation
of X on day n is already equal to its expected value of X in f(n) days. In particular,
it learns in a timely manner to set its current expectations equal to its future
expectations on any LUV. In colloquial terms, once a logical inductor has ﬁgured
out how expectations work, it will never say “I currently believe that the X variables
have low values, but tomorrow I’m going to learn that they have high values”.
Logical inductors already expect today what they expect to expect tomorrow.
It follows immediately from theorems 4.12.1 (Expected Future Expectations)
and 4.8.6 (Expectations of Indicators) that the current beliefs of a logical inductor
are set, in a timely manner, to equal their future expected beliefs.
47

Theorem 4.12.2 (No Expected Net Update). Let f be a deferral function, and let
φ be an e.c. sequence of sentences. Then
Pn(φn) ≂n En(“Pf(n)(φn)”).
(Proof in: F.6.)
In particular, if P knows that its future self is going to assign some sequence p of
probabilities to φ, then it starts assigning p to φ in a timely manner.
Theorem 4.12.1 (Expected Future Expectations) can be generalized to cases
where the LUV on day n is multiplied by an expressible feature:
Theorem 4.12.3 (No Expected Net Update under Conditionals). Let f be a de-
ferral function, and let X denote an e.c. sequence of [0, 1]-LUVs, and let w denote
a P-generable sequence of real numbers in [0, 1]. Then
En(“Xn · wf(n)”) ≂n En(“Ef(n)(Xn) · wf(n)”).
(Proof in: F.7.)
To see why Theorem 4.12.3 is interesting, it helps to imagine the case where X is a
series of bundles of goods and services, and wn is Indδn(Ef(n)(Xn) > 0.7) for some
sequence of rational numbers δ →0, as per Deﬁnition 4.3.2. This value is 1 if P
will expect the nth bundle to be worth more than 70¢ on day f(n), and 0 otherwise,
and intermediate if the case isn’t quite clear. Then
En

“Xn · Indδn

Ef(n)(Xn) > 0.7

”

can be interpreted as P’s expected value of the bundle on day n, in cases where
P is going to think it’s worth at least 70¢ on day f(n).
Now assume that
Indδn(Ef(n)(Xn)) > 0 and divide it out of both sides, in which case the theorem
roughly says
Enow(X | Elater(X) > 0.7) ≂Enow(Elater(X) | Elater(X) > 0.7),
which says that P’s expected value of the bundle now, given that it’s going to think
the bundle has a value of at least 70¢ later, is equal to whatever it expects to think
later, conditioned on thinking later that the bundle is worth at least 70¢.
Combining this idea with indicator functions, we get the following theorem:
Theorem 4.12.4 (Self-Trust). Let f be a deferral function, φ be an e.c. sequence of
sentences, δ be an e.c. sequence of positive rational numbers, and p be a P-generable
sequence of rational probabilities. Then
En

“1(φn) · Indδn

Pf(n)(φn) > pn

”

≳n pn · En

“Indδn

Pf(n)(φn) > pn

”

.
(Proof in: F.8.)
Very roughly speaking, if we squint at Theorem 4.12.4, it says something like
Enow(φ | Plater(φ) > p) ≳p,
i.e., if we ask P what it would believe about φ now if it learned that it was going
to believe φ with probability at least p in the future, then it will answer with a
probability that is at least p.
As a matter of fact, Theorem 4.12.4 actually says something slightly weaker,
which is also more desirable.
Let each φn be the self-referential sentence
“Pf(n)(φn) < 0.5” which says that the future Pf(n) will assign probability less than
48

0.5 to φn. Then, conditional on Pf(n)(φn) ≥0.5, Pn should believe that the proba-
bility of φn is 0. And indeed, this is what a logical inductor will do:
Pn

“φn ∧(Pf(n)(φn) ≥0.5)”

≂n 0,
by Theorem 4.2.3 (Persistence of Knowledge), because each of those conjunctions is
disprovable. This is why Theorem 4.12.4 uses continuous indicator functions: With
discrete conjunctions, the result would be undesirable (not to mention false).
What Theorem 4.12.4 says is that P attains self-trust of the “if in the future
I will believe x is very likely, then it must be because x is very likely” variety,
while retaining the ability to think it can outperform its future self’s beliefs when
its future self confronts paradoxes.
In colloquial terms, if we ask “what’s your
probability on the paradoxical sentence φn given that your future self believes it
with probability exactly 0.5?” then P will answer “very low”, but if we ask “what’s
your probability on the paradoxical sentence φn given that your future self believes
it with probability extremely close to 0.5?” then P will answer “roughly 0.5.”
Still speaking roughly, this means that logical inductors trust their future beliefs
to be accurate and only change for good reasons. Theorem 4.12.4 says that if you
ask “what’s the probability of φ, given that in the future you’re going to believe it’s
more than 95% likely?” then you’ll get an answer that’s no less than 0.95, even if
the logical inductor currently thinks that φ is unlikely.
5
Construction
In this section, we show how given any deductive process D, we can construct a
computable belief sequence, called LIA, that satisﬁes the logical induction criterion
relative to D. Roughly speaking, LIA works by simulating an economy of traders
and using Brouwer’s ﬁxed point theorem to set market prices such that no trader
can exploit the market relative to D.
We will build LIA from three subroutines called MarketMaker, Budgeter, and
TradingFirm. Intuitively, MarketMaker will be an algorithm that sets market prices
by anticipating what a single trader is about to do, Budgeter will be an algorithm
for altering a trader to stay within a certain budget, and TradingFirm will be an
algorithm that uses Budgeter to combine together an inﬁnite sequence of carefully
chosen e.c. traders (via a sum calculable in ﬁnite time) into a single trader that
exploits a given market if any e.c. trader exploits that market. Then, LIA will work
by using MarketMaker to make a market not exploitable by TradingFirm and hence
not exploitable by any e.c. trader, thereby satisfying the logical induction criterion.
To begin, we will need a few basic data types for our subroutines to pass around:
Deﬁnition 5.0.1 (Belief History). An n-belief history P≤n = (P1, . . . , Pn) is a
ﬁnite list of belief states of length n.
Deﬁnition 5.0.2 (Strategy History). An n-strategy history T≤n = (T1, . . . , Tn)
is a ﬁnite list of trading strategies of length n, where Ti is an i-strategy.
Deﬁnition 5.0.3 (Support). For any valuation V we deﬁne
Support(V) := {φ ∈S | V(φ) ̸= 0},
and for any n-strategy Tn we deﬁne
Support(Tn) := {φ ∈S | Tn[φ] ̸≡0}.
Observe that for any belief state P and any n-strategy Tn, Support(P) and
Support(Tn) are computable from the ﬁnite lists representing P and Tn.
49

5.1
Constructing MarketMaker
Here we deﬁne the MarketMaker subroutine and establish its key properties. In-
tuitively, given any trader T as input, on each day n, MarketMaker looks at the
trading strategy Tn and the valuations P≤n−1 output by MarketMaker on previous
days. It then uses an approximate ﬁxed point (guaranteed to exist by Brouwer’s
ﬁxed point theorem) that sets prices Pn for that day such that when the trader’s
strategy Tn reacts to the prices, the resulting trade Tn(P≤n) earns at most a very
small positive amount of value in any world. Intuitively, the ﬁxed point ﬁnds the
trader’s “fair prices”, such that they abstain from betting, except possibly to buy
sentences at a price very close to $1 or sell them at a price very close to $0, thereby
guaranteeing that very little value can be gained from the trade.
Lemma 5.1.1 (Fixed Point Lemma). Let Tn be any n-strategy, and let P≤n−1 be
any (n−1)-belief history. There exists a valuation V with Support(V) ⊆Support(Tn)
such that
for all worlds W ∈W:
W (Tn(P≤n−1, V)) ≤0.
(5.1.1)
Proof. We will use Brouwer’s ﬁxed point theorem to ﬁnd “prices” V such that Tn
only ever buys shares for $1 or sells them for $0, so it cannot make a proﬁt in
any world. Intuitively, we do this by making a “price adjustment” mapping called
ﬁx that moves prices toward 1 or 0 (respectively) as long as Tn would buy or sell
(respectively) any shares at those prices, and ﬁnding a ﬁxed point of that mapping.
First, we let S′ = Support(Tn) and focus on the set
V′ := {V | Support(V) ⊆S′}.
Observe that V′ is equal to the natural inclusion of the ﬁnite-dimensional cube
[0, 1]S′ in the space of all valuations V = [0, 1]S. We now deﬁne our “price adjust-
ment” function ﬁx : V′ →V′ as follows:
ﬁx(V)(φ) := max(0, min(1, V(φ) + Tn(P≤n−1, V)[φ])).
This map has the odd property that it adds prices and trade volumes, but it does the
trick. Notice that ﬁx is a function from the compact, convex space V′ to itself, so if it
is continuous, it satisﬁes the antecedent of Brouwer’s ﬁxed point theorem. Observe
that ﬁx is in fact continuous, because trade strategies are continuous. Indeed, we
required that trade strategies be continuous for precisely this purpose. Thus, by
Brouwer’s ﬁxed point theorem, ﬁx has at least one ﬁxed point Vﬁx that satisﬁes, for
all sentences φ ∈S′,
Vﬁx(φ) = max(0, min(1, Vﬁx(φ) + Tn(P≤n−1, Vﬁx)[φ])).
Fix a world W and observe from this equation that if Tn buys some shares of
φ ∈S′ at these prices, i.e. if Tn(P≤n−1, Vﬁx)[φ] > 0, then Vﬁx(φ) = 1, and in
particular, W(φ) −Vﬁx(φ) ≤0.
Similarly, if Tn sells some shares of φ, i.e. if
Tn(P≤n−1, Vﬁx)[φ] < 0, then Vﬁx(φ) = 0, so W(φ) −Vﬁx(φ) ≥0. In either case, we
have
0 ≥(W(φ) −Vﬁx(φ)) · Tn(P≤n−1, Vﬁx)[φ]
since the two factors always have opposite sign (or at least one factor is 0). Summing
over all φ, remembering that Tn(V≤n)[φ] = 0 for φ /∈S′, gives
0 ≥
X
φ∈S
(W(φ) −Vﬁx(φ)) · Tn(P≤n−1, Vﬁx)[φ]
= W(Tn(P≤n, Vﬁx)) −Vﬁx(Tn(P≤n−1, Vﬁx))
since the values of the “cash” terms W(Tn(P≤n, Vﬁx)[1]) and Vﬁx(Tn(P≤n, Vﬁx)[1])
are by deﬁnition both equal to Tn(P≤n, Vﬁx)[1] and therefore cancel. But
Vﬁx(Tn(P≤n−1, Vﬁx)) = 0
50

by deﬁnition of a trading strategy, so for any world W, we have
0 ≥W(Tn(P≤n−1, Vﬁx)).
Deﬁnition/Proposition 5.1.2 (MarketMaker). There exists a computable func-
tion, henceforth named MarketMaker, satisfying the following deﬁnition.
Given
as input any n ∈N+, any n-strategy Tn, and any (n −1)-belief history P≤n−1,
MarketMakern(Tn, P≤n−1) returns a belief state P with Support(P) ⊆Support(Tn)
such that
for all worlds W ∈W:
W(Tn(P≤n−1, P)) ≤2−n.
(5.1.2)
Proof. Essentially, we will ﬁnd a rational approximation P to the ﬁxed point Vﬁx
in the previous lemma, by brute force search. This requires some care, because the
set of all worlds is uncountably inﬁnite.
First, given Tn and P≤n−1, let S′ := Support(Tn), V′ := {V | Support(V) ⊆S′},
and take Vﬁx ∈V′ satisfying (5.1.1). Let W′ := {W | Support(W) ⊆S′}, and for
any world W, deﬁne W′ ∈W′ by
W′(φ) :=
W(φ)
if φ ∈S′,
0
otherwise.
Observe that for any W ∈W, the function V′ →R given by
V 7→W(Tn(P≤n−1, V)) = W′(Tn(P≤n−1, V))
is a continuous function of V that depends only on W′. Since the set W′ is ﬁnite,
the function
V 7→sup
W∈W
W(Tn(P≤n−1, V)) = max
W′∈W′ W′(Tn(P≤n−1, V))
is the maximum of a ﬁnite number of continuous functions, and is therefore con-
tinuous.
Hence there is some neighborhood in V′ around Vﬁx with image in
(−∞, 2−n) ⊂R. By the density of rational points in V′, there is therefore some
belief state P ∈V′ ∩QS satisfying (5.1.2), as needed.
It remains to show that such a P can in fact be found by brute force search.
First, recall that a belief state P is a rational-valued ﬁnite-support map from S
to [0, 1], and so can be represented by a ﬁnite list of pairs (φ, q) with φ ∈S and
q ∈Q ∩[0, 1]. Since S and [0, 1] ∩Q are computably enumerable, so is the set of all
belief states.
Thus, we can computably “search” though all possible Ps, so we need only
establish that given n, Tn, and P≤n−1 we can computably decide whether each
P in our search satisﬁes (5.1.2) until we ﬁnd one. First note that the ﬁnite set
Support(Tn) can be computed by searching the expression specifying Tn for all the
sentences φ that occur within it. Moreover, equation (5.1.2) need only be be checked
for worlds W′ ∈W′, since any other W returns the same value as its corresponding
W′. Now, for any ﬁxed world W′ ∈W′ and candidate P, we can compute each value
in the language of expressible features
W′(Tn(P≤n−1, P)) = Tn(P≤n−1, P)[1] +
X
φ∈S′
W′(φ) · Tn(P≤n−1, P)[φ]
directly by evaluating the expressible features Tn[φ] on the given belief history
(P≤n−1, P), as φ ∈S′ varies. Since W′ is a ﬁnite set, we can do this for all W′
with a ﬁnite computation. Thus, checking whether a belief state P satisﬁes con-
dition (5.1.2) is computably decidable, and a solution to (5.1.2) can therefore be
found by enumerating all belief states P and searching through them for the ﬁrst
one that works.
51

Lemma 5.1.3 (MarketMaker Inexploitability). Let T be any trader. The sequence
of belief states P deﬁned recursively by
Pn := MarketMakern(Tn, P≤n−1),
with base case P1 = MarketMaker(T1, ()), is not exploited by T relative to any
deductive process D.
Proof. By the deﬁnition of MarketMaker, we have that for every n, the belief state
P = Pn satisﬁes equation (5.1.2), i.e.,
for all worlds W ∈W and all n ∈N+:
W(Tn(P)) ≤2−n.
Hence by linearity of W, for all n ∈N+ we have:
W
P
i≤nTi(P)

=
X
i≤n
W(Ti(P)) ≤
X
i≤n
2−i < 1.
Therefore, given any deductive process D,
sup
n
W
P
i≤nTi(P)
  n ∈N+, W ∈PC(Dn)
o
≤1 < ∞,
so T does not exploit P relative to D.
5.2
Constructing Budgeter
Here we introduce a subroutine for turning a trader with potentially inﬁnite losses
into a trader that will never have less than −$b in any world W ∈PC(Dn) on any
day n, for some bound b, in such a way that does not aﬀect the trader if it wouldn’t
have fallen below −$b to begin with.
Deﬁnition/Proposition 5.2.1 (Budgeter). Given any deductive process D, there
exists a computable function, henceforth called BudgeterD, satisfying the following
deﬁnition. Given inputs n and b ∈N+, an n-strategy history T≤n, and an (n −1)-
belief history P≤n−1, BudgeterD returns an n-strategy BudgeterD
n (b, T≤n, P≤n−1),
such that
if:
W
P
i≤m Ti(P≤i)

≤−b for some m < n and W ∈PC(Dm),
then:
BudgeterD
n (b, T≤n, P≤n−1) = 0,
else:
BudgeterD
n (b, T≤n, P≤n−1) =
(5.2.1)
Tn ·
inf
W∈PC(Dn)

max

1,
−W(Tn)
b + W
P
i≤n−1 Ti(P≤i)





−1
.
Proof. Let S′ = S
i≤n Support(Ti), W′ = {W | Support(W) ⊆S′}, and for any
world W, write
W′(φ) :=
W(φ)
if φ ∈S′,
0
otherwise.
Now, observe that we can computably check the “if” statement in the function
deﬁnition. This is because W(P
i≤m Ti(P≤i)) depends only on W′ ∈W′, a ﬁnite
set.
We can check whether W′ ∈PC(Dm) in ﬁnite time by checking whether
any assignment of truth values to the ﬁnite set of prime sentences occurring in
52

sentences of Dn yields the assignment W′ on Support(W′). The set of sentences Dn
is computable given n, because D is computable by deﬁnition.
It remains to show that the “else” expression can be computed and returns an
n-trading strategy. First, the inﬁmum can be computed over W′ ∈W′ ∩PC(Dn),
a ﬁnite set, since the values in the inf depend only on W′, and the inf operator
itself can be re-expressed in the language of expressible features using max and
multiplication by (−1).
The values W′(Tn) and W′(P
i≤n−1 Ti(P≤i)) are ﬁnite
sums, and the denominator b + W(P
i≤n−1 Ti(P≤i)) is a ﬁxed positive rational (so
we can safely multiply by its reciprocal). The remaining operations are all single-
step evaluations in the language of expressible valuation features, completing the
proof.
Let us reﬂect on the meaning of these operations. The quantity b+W(P
i<n Ti(P≤i))
is the amount of money the trader has available on day n according to W (assuming
they started with a budget of b), and −W(Tn) is the amount they’re going to lose
on day n according to W as a function of the upcoming prices, and so the inﬁmum
above is the trader’s trade on day n scaled down such that they can’t overspend
their budget according to any world propositionally consistent with Dn.
Lemma 5.2.2 (Properties of Budgeter). Let T be any trader, and P be any sequence
of belief states. Given n and b, let Bb
n denote BudgeterD
n (b, T≤n, P≤n−1). Then:
1. for all b, n
∈
N+, if for all m
≤
n and W
∈
PC(Dm) we have
W
P
i≤m Ti(P)

> −b, then
Bb
n(P) = Tn(P);
2. for all b, n ∈N+ and all W ∈PC(Dn), we have
W
P
i≤n Bb
i (P)

≥−b;
3. If T exploits P relative to D, then so does B
b for some b ∈N+.
Part 1.
Proof. Suppose that for some time step n, for all m ≤n and all worlds W ∈PC(Dm)
plausible at time m we have
W
P
i≤m Ti(P)

> −b,
so by linearity of W(−), we have in particular that
b + W
P
i≤n−1 Ti(P)

> −W
 Tn(P)

.
Since n −1 ≤n, the LHS is positive, so we have
1 >
−W
 Tn(P)

b + W
P
i≤n−1 Ti(P)
.
Therefore, by the deﬁnition of BudgeterD (and Ti(P) = Ti(P≤i)), since the “if”
clause doesn’t trigger by the assumption on the W
P
i≤m Ti(P)

for m < n,
Bb
n(P) ≡Tn(P) ·
inf
W∈PC(Dn) 1
,
max

1,
−W(Tn(P))
b + W
P
i≤n−1 Ti(P)



= Tn(P≤n) ·
inf
W∈PC(Dn) 1/1
= Tn(P)
as needed.
53

Part 2.
Proof. Suppose for a contradiction that for some n and some W ∈PC(Dn),
W
P
i≤n Bb
i (P)

< −b.
Assume that n is the least such day, and ﬁx some such W ∈PC(Dn).
By
the minimality of n it must be that W(Bb
n(P)) < 0, or else we would have
W
P
i≤n−1 Bb
i (P)

< −b. Since Bb
n(P) is a non-negative multiple of Tn(P), we
also have W(Tn(P)) < 0. However, since Bb
n ̸≡0, from the deﬁnition of BudgeterD
we have
W
 Bb
n

= W
 Tn(P)

·
 
inf
W′∈PC(Dn) 1
,
max
 
1,
−W′(Tn(P))
b + W′(P
i≤n−1 Ti(P))
!!
≥W
 Tn(P)

· 1
,
max
 
1,
−W(Tn(P))
b + W(P
i≤n−1 Ti(P))
!
(since W
 Tn(P)

< 0)
≥W
 Tn(P)

·
b + W(P
i≤n−1 Ti(P))
−W(Tn(P))
since −W
 Tn(P)

> 0 and Bb
n ̸≡0 implies b + W(P
i≤n−1 Ti(P)) > 0. Hence, this
= −b −W
P
i≤n Ti(P)

.
Further, since Bb
n ̸≡0, we have
for all j ≤n −1:
W
P
i≤j Ti(P)

> −b, which by Part 1 implies that
for all j ≤n −1:
Bb
j(P) = Tj(P), therefore
W(Bb
n) ≥−b −W
P
i≤n−1 Bb
i (P)

, hence
W
P
i≤n Bb
i (P)

≥−b.
Part 3.
Proof. By deﬁnition of exploitation, the set
n
W
P
i≤n Ti(P)
  n ∈N+, W ∈PC(Dn)
o
is unbounded above, and is strictly bounded below by some integer b. Then by Part
1, for all n we have Tn(P) = Bb
n(P). Thus,
n
W
P
i≤n Bb
i (P)
  n ∈N+, W ∈PC(Dn)
o
is unbounded above and bounded below, i.e., B
b exploits P relative to D.
5.3
Constructing TradingFirm
Next we deﬁne TradingFirm, which combines an (enumerable) inﬁnite sequence of
e.c. traders into a single “supertrader” that exploits a given belief sequence P relative
to D if any e.c. trader does. It does this by taking each e.c. trader, budgeting it,
and scaling its trades down so that traders later in the sequence carry less weight
to begin with.
To begin, we will need a computable sequence that includes every e.c. trader at
least once. The following trick is standard, but we include it here for completeness:
54

Proposition 5.3.1 (Redundant Enumeration of e.c. Traders). There exists a com-
putable sequence (T
k)k∈N+ of e.c. traders such that every e.c. trader occurs at least
once in the sequence.
Proof. Fix a computable enumeration of all ordered pairs (Mk, fk) where Mk is a
Turing machine and fk is a polynomial with coeﬃcients in Z. We deﬁne a com-
putable function
ECT : {Turing machines} × {Integer polynomials} × (n ∈N+) →{n-strategies}
that runs as follows: ECT(M, f, n) ﬁrst runs M(n) for up to f(n) time steps, and if
in that time M(n) halts and returns a valid n-strategy Tn, then ECT(M, f, n)
returns that strategy, otherwise it returns 0 (as an n-strategy).
Observe that
ECT(Mk, fk, −) is always an e.c. trader, and that every e.c. trader occurs as
ECT(Mk, fk, −) for some k.
Deﬁnition/Proposition 5.3.2 (TradingFirm). Given any deductive process D,
there exists a computable function, henceforth called TradingFirmD, satisfying the
following deﬁnition.
By Proposition 5.3.1, we ﬁx a computable enumeration T
k
including every e.c. trader at least once, and let
Sk
n =
T k
n
if n ≥k
0
otherwise.
Given input n ∈N+ and an (n −1)-belief history P≤n−1, TradingFirmD returns an
n-strategy given by
TradingFirmD
n (P≤n−1) =
X
k∈N+
X
b∈N+
2−k−b · BudgeterD
n (b, Sk
≤n, P≤n−1).
(5.3.2)
Proof. We need only show that the inﬁnite sum in equation (5.3.2) is equivalent to
a computable ﬁnite sum. Writing
Bb,k
n
= BudgeterD
n (b, Sk
≤n, P≤n−1),
(an n-strategy), the sum on the RHS of (5.3.2) is equivalent to
X
k∈N+
X
b∈N+
2−k−b · Bb,k
n .
Since Sk
n = 0 for k > n, we also have Bb,k
n
= 0 for k > n, so the sum is equivalent
to
=
X
k≤n
X
b∈N+
2−k−b · Bb,k
n .
Now, assume Cn is a positive integer such that P
i≤n ∥Sk
i (V)∥1 < Cn for all k ≤
n and any valuation sequence V (we will show below that such a Cn can be computed
from P≤n−1). Since the valuations W and P are always [0, 1]-valued, for any m ≤n
the values W
P
i≤m Sk
i (P≤m)

are bounded below by −P
i≤m ∥Sk
i (P≤m)∥1 > −Cn.
By property 1 of BudgeterD (Lemma 5.2.2.1), Bb,k
n
= Sk
n when b > Cn, so the sum
is equivalent to
=

X
k≤n
X
b≤Cn
2−k−b · Bb,k
n

+

X
k≤n
X
b>Cn
2−k−b · Sk
n


=

X
k≤n
X
b≤Cn
2−k−b · Bb,k
n

+

X
k≤n
2−k−Cn · Sk
n


55

which is a ﬁnite sum of trading strategies, and hence is itself a trading strategy. Since
the Bb,k
n
and the Sk
n are computable from P≤n−1, this ﬁnite sum is computable.
It remains to justify our assumption that integers Cn can be computed from
P≤n−1 with Cn > P
i≤n ∥Sk
i (V)∥1 for all k ≤n and V. To see this, ﬁrst consider
how to bound a single expressible feature ξ. We can show by induction on the
structure of ξ (see A.2) that, given constant bounds on the absolute value |ζ(V)| of
each subexpression ζ of ξ, we can compute a constant bound on |ξ(V)|; for example,
the bound on ζ ·η is the product of the bound on ζ and the bound on η. Thus, given
a single trading strategy Sk
i and any φ, we can compute a constant upper bound on
|Sk
i [φ](V)| for all V. Since ∥Sk
i (V)∥1 ≤P
φ∈Support(Sk
i ) 2|Sk
i [φ](V)| and Support(Sk
i )
is computable, we can bound each ∥Sk
i (V)∥1, and hence also P
i≤n ∥Sk
i (V)∥1, as
needed.
Lemma 5.3.3 (Trading Firm Dominance). Let P be any sequence of belief states,
and D be a deductive process. If there exists any e.c. trader T that exploits P relative
to D, then the sequence

TradingFirmD
n (P≤n−1)

n∈N+
also exploits P (relative to D).
Proof. Suppose that some e.c. trader exploits P. That trader occurs as T
k for some
k in the enumeration used by TradingFirmD. First, we show that S
k (from the
deﬁnition of TradingFirmD) also exploits P. It suﬃces to show that there exist
constants c1 ∈R+ and c2 ∈R such that for all n ∈N+ and W ∈PC(Dn),
W
P
i≤n Sk
i (P)

≥c1 · W
P
i≤n T k
i (P)

+ c2.
Taking c1 = 1 and c2 = −P
i<k ∥T k
i (P)∥1, where ∥· ∥1 denotes the ℓ1 norm on
R-combinations of sentences, we have
W
P
i≤n Sk
i (P)

≥1 · W
P
i≤n T k
i (P)

−
 P
i<k ∥T k
i (P)∥1

,
so S
k exploits P. By Lemma 5.2.2.3, we thus have that for some b ∈N+, the trader
B
b,k given by
Bb,k
n
:= BudgeterD
n (b, Sk
≤n, P≤n−1)
also exploits P.
Next, we show that the trader F given by
Fn := TradingFirmD
n (P≤n−1)
exploits P. Again, it suﬃces to show that there exist constants c1 ∈R+ and c2 ∈R
such that for all n ∈N+ and W ∈PC(Dn),
W

X
i≤n
Fi

≥c1 · W

X
i≤n
Bb,k
i

+ c2.
56

It will suﬃce to take c1 = 2−k−b and c2 = −2, because we have
W

X
i≤n
Fi

−2−k−b · W

X
i≤n
Bb,k
i


=
X
(k′,b′)̸=(k,b)
2−k′−b′ · W

X
i≤n
Bb′,k′
i


≥
X
(k′,b′)̸=(k,b)
2−k′−b′ · (−b′) ≥−2
by Lemma 5.2.2.2, hence
W

X
i≤n
Fi

≥2−k−b · W

X
i≤n
Bb,k
i

−2.
Thus, F exploits P.
5.4
Constructing LIA
We are ﬁnally ready to build LIA. With the subroutines above, the idea is now fairly
simple: we pit MarketMaker and TradingFirm against each other in a recursion, and
MarketMaker wins. Imagine that on each day, TradingFirm outputs an ever-larger
mixture of traders, then MarketMaker carefully examines that mixture and outputs
a belief state on which that mixture makes at most a tiny amount of money on net.
Deﬁnition/Algorithm 5.4.1 (A Logical Induction Algorithm). Given a deductive
process D, deﬁne the computable belief sequence LIA = (LIA1, LIA2, . . .) recursively
by
LIAn := MarketMakern(TradingFirmD
n (LIA≤n−1), LIA≤n−1),
beginning from the base case LIA≤0 := ().
Theorem 5.4.2 (LIA is a Logical Inductor). LIA satisﬁes the logical induction
criterion relative to D, i.e., LIA is not exploitable by any e.c. trader relative to the
deductive process D.
Proof. By Lemma 5.3.3, if any e.c. trader exploits LIA (relative to D), then so does
the trader F := (TradingFirmD
n (LIA≤n−1))n∈N+.
By Lemma 5.1.3, F does not
exploit LIA. Therefore no e.c. trader exploits LIA.
5.5
Questions of Runtime and Convergence Rates
In this paper, we have optimized our deﬁnitions for the theoretical clarity of results
rather than for the eﬃciency of our algorithms. This leaves open many interesting
questions about the relationship between runtime and convergence rates of logical
inductors that have not been addressed here. Indeed, the runtime of LIA is under-
speciﬁed because it depends heavily on the particular enumerations of traders and
rational numbers used in the deﬁnitions of TradingFirm and MarketMaker.
For logical inductors in general, there will be some tradeoﬀbetween the runtime
of Pn as a function of n and how quickly the values Pn(φ) converge to P∞(φ) as
n grows.
Quantifying this tradeoﬀmay be a fruitful source of interesting open
problems. Note, however, the following important constraint on the convergence
rate of any logical inductor, regardless of its implementation, which arises from the
halting problem:
57

Proposition 5.5.1 (Uncomputable Convergence Rates). Let P be a logical inductor
over a theory Γ that can represent computable functions, and suppose f : S×Q+ →N
is a function such that for every sentence φ, if Γ ⊢φ then Pn(φ) > 1 −ε for all
n > f(φ, ε). Then f must be uncomputable.
Proof. Suppose for contradiction that such a computable f were given. We will
show that f could be used to computably determine whether Γ ⊢φ for an arbitrary
sentence φ, a task which is known to be impossible for a ﬁrst-order theory that can
represent computable functions. (If we assumed further that Γ were sound as a
theory of the natural numbers, this would allow us to solve the halting problem by
letting φ be a sentence of the form “M halts”.)
Given a sentence φ, we run two searches in parallel. If we ﬁnd that Γ ⊢φ, then
we return True. If we ﬁnd that for some b, n ∈N+ we have
n > f

φ, 1
b

and Pn(φ) ≤1 −1
b ,
(5.5.1)
then we return False. Both of these conditions are computably enumerable since f,
Pn, and verifying witnesses to Γ ⊢φ are computable functions.
Suppose ﬁrst that Γ ⊢φ. Then by deﬁnition of f we have Pn(φ) > 1 −1
b for all
n > f
 φ, 1
b

, and hence we ﬁnd a witness for Γ ⊢φ and return True. Now suppose
that Γ ⊬φ. Then by Theorem 4.6.2 (Non-Dogmatism) we have that P∞(φ) < 1 −ε
for some ε > 0, and hence for some b and all suﬃciently large n we have Pn(φ) <
1 −1/b. Therefore 5.5.1 holds and we return False. Thus our search always halts
and returns a Boolean value that correctly indicates whether Γ ⊢φ.
6
Selected Proofs
In this section, we exhibit a few selected stand-alone proofs of certain key theorems.
These theorems hold for any P satisfying the logical induction criterion, which we
recall here:
Deﬁnition 3.0.1 (The Logical Induction Criterion). A market P is said to satisfy
the logical induction criterion relative to a deductive process D if there is no
eﬃciently computable trader T that exploits P relative to D. A market P meeting
this criterion is called a logical inductor over D.
Only our notation (Section 2), framework (Section 3), and continuous threshold
indicator (Deﬁnition 4.3.2) are needed to understand the results and proofs in this
section. Shorter proofs of these theorems can be found in the appendix, but those
rely on signiﬁcantly more machinery.
6.1
Convergence
Recall Theorem 4.1.1 and the proof sketch given:
Theorem 4.1.1 (Convergence). The limit P∞: S →[0, 1] deﬁned by
P∞(φ) := lim
n→∞Pn(φ)
exists for all φ.
Proof sketch.
Roughly speaking, if P never makes up its mind about φ, then it can be
exploited by a trader arbitraging shares of φ across diﬀerent days. More
precisely, suppose by way of contradiction that the limit P∞(φ) does
not exist. Then for some p ∈[0, 1] and ε > 0, we have Pn(φ) < p −ε
inﬁnitely often and also Pn(φ) > p+ε inﬁnitely often. A trader can wait
until Pn(φ) < p−ε and then buy a share in φ at the low market price of
58

Pn(φ). Then the trader waits until some later m such that Pm(φ) > p+ε,
and sells back the share in φ at the higher price. This trader makes a
total proﬁt of 2ε every time Pn(φ) oscillates in this way, at no risk, and
therefore exploits P. Since P implements a logical inductor, this is not
possible; therefore the limit P∞(φ) must in fact exist.
We will deﬁne a trader T that executes a strategy similar to this one, and hence ex-
ploits the market P if limn→∞Pn(φ) diverges. To do this, there are two technicalities
we must deal with. First, the strategy outlined above uses a discontinuous function
of the market prices Pn(φ), and therefore is not permitted. This is relatively easy
to ﬁx using the continuous indicator functions of Deﬁnition 4.3.2.
The second technicality is more subtle. Suppose we deﬁne our trader to buy
φ-shares whenever their price Pn(φ) is low, and sell them back whenever their price
is high. Then it is possible that the trader makes the following trades in sequence
against the market P: buy 10 φ-shares on consecutive days, then sell 10 φ-shares;
then buy 100 φ-shares consecutively, and then sell them oﬀ; then buy 1000 φ-shares,
then sell them oﬀ; and so on. Although this trader makes proﬁt on each batch, it
always spends more on the next batch, taking larger and larger risks (relative to
the remaining plausible worlds). Then the plausible value of this trader’s holdings
will be unbounded below, and so it does not exploit P. In short, this trader is not
tracking its budget, and so may have unboundedly negative plausible net worth.
We will ﬁx this problem by having our trader T track how many net φ-shares it has
bought, and not buying too many, thereby maintaining bounded risk. This will be
suﬃcient to prove the theorem.
Proof of Theorem 4.1.1. Suppose by way of contradiction that the limit P∞does
not exist. Then, for some sentence φ and some rational numbers p ∈[0, 1] and
ε > 0, we have that Pn(φ) < p −ε inﬁnitely often and Pn(φ) > p + ε inﬁnitely often.
We will show that P can be exploited by a trader T who buys below and sells above
these prices inﬁnitely often, contrary to the logical induction criterion.
Deﬁnition of the trader T. We will deﬁne T recursively along with another
sequence of EF-combinations H (mnemonic: “holdings”) which tracks the sum of
the trader’s previous trades. Our base cases are
T1 := 0
H1 := 0.
For n > 1, we deﬁne a recurrence whereby T will buy some φ-shares whenever
φ∗n < p−ε/2, up to (1−Hn−1[φ]) shares when φ∗n < p−ε, and sells some φ-shares
whenever φ∗n > p + ε/2, up to Hn−1 shares when φ∗n > p + ε:
Tn[φ] := (1 −Hn−1[φ]) · Indε/2(φ∗n < p −ε/2)
−Hn−1[φ] · Indε/2(φ∗n > p + ε/2),
Tn := Tn[φ] · (φ −φ∗n)
Hn := Hn−1 + Tn.
(6.1.1)
The trade coeﬃcients T [φ] are chosen so that the number of φ-shares Hn[φ] that it
owns is always in [0, 1] (it never buys more than 1 −Hn−1[φ] and never sells more
than Hn−1[φ]). Observe that each Tn is a valid trading strategy for day n (see
Deﬁnition 3.4.4) because it is of the form ξ · (φ −φ∗n).
To complete the deﬁnition, we must argue that T is eﬃciently computable. For
this, observe that the 3n + 2 deﬁnition (:=) equations deﬁning T1, . . . , Tn above
can be written down in time polynomial in n.
Thus, a combination of feature
expressions deﬁning Tn from scratch can be written down in poly(n) time (indeed,
the expression is just a concatenation of n copies of the three “:=” equations written
above, along with the base cases), so T is eﬃciently computable.
59

Proof of exploitation. To show T exploits P over D, we must compute upper
and lower bounds on the set of plausible values W(Hn(P)) (since Hn = P
i≤n Tn)
for worlds W ∈PC(Dn).
While proving exploitation, we leave the constant argument P implicit to reduce
clutter, writing, e.g., φ∗i for φ∗i(P) = Pi(φ), Tn[φ] for Tn[φ](P), and so on.
First, since each Ti[1] = −Ti[φ] · φ∗i, the trader’s “cash” held on day n is
Hn[1] =
X
i≤n
Ti[1] = −
X
i≤n
Ti[φ] · φ∗i
which we can regroup, to compare the prices φ∗i to p, as
Hn[1] =
X
i≤n
 Ti[φ] · (p −φ∗i)

−p ·
X
i≤n
Ti[φ]
=
X
i≤n
 Ti[φ] · (p −φ∗i)

−p · Hn[φ].
Now, if φ∗i < p −ε/2 then Ti[φ] ≥0, if φ∗i > p + ε/2 then Ti[φ] ≤0, and if
p −ε/2 ≤φ∗i ≤p + ε/2 then Ti[φ] = 0, so for all i the product Ti[φ] · (p −φ∗i) is
equal to or greater than |Ti[φ]| · ε/2:
Hn[1] ≥−p · Hn[φ] +
X
i≤n
|Ti[φ]| · ε/2.
Moreover, by design, Hn[φ] ∈[0, 1] for all n, so
Hn[1] ≥−p +
X
i≤n
|Ti[φ]| · ε/2.
Now, by assumption, φ∗i lies above and below (p −ε, p + ε) inﬁnitely often, so
from equation (6.1.1), Hi[φ] = 0 and Hi[φ] = 1 inﬁnitely often. Since the sum
P
i≤n |Ti[φ]| is the total variation in the sequence Hi[φ], it must diverge (by the
triangle inequality) as n →∞, so
lim
n→∞Hn[1] = ∞.
Moreover, in any world W, the trader’s non-cash holdings Hn[φ] · φ have value
W(Hn[φ] · φ) = Hn[φ] · W(φ) ≥0 (since Hn[φ] > 0), so its combined holdings
Hn = Hn[1] + Hn[φ] · φ have value
W(Hn) = W(Hn[1] + Hn[φ] · φ) = Hn[1] + Hn[φ] · W(φ) ≥Hn[1]
so in every world W we have
lim
n→∞W(Hn) = ∞.
This contradicts that P is a logical inductor; therefore, the limit P∞(φ) must exist.
6.2
Limit Coherence
Recall Theorem 4.1.2:
Theorem 4.1.2 (Limit Coherence). P∞is coherent, i.e., it gives rise to an inter-
nally consistent probability measure Pr on the set PC(Γ) of all worlds consistent
with Γ, deﬁned by the formula
Pr(W(φ) = 1) := P∞(φ).
In particular, if Γ contains the axioms of ﬁrst-order logic, then P∞deﬁnes a prob-
ability measure on the set of ﬁrst-order completions of Γ.
60

Proof of Theorem 4.1.2. By Theorem 4.1.1 (Convergence), the limit P∞(φ) exists
for all sentences φ ∈S. Therefore, Pr(W(φ) = 1) := P∞(φ) is well-deﬁned as a
function of basic subsets of the set of all consistent worlds PC(D∞) = PC(Γ).
Gaifman (1964) shows that Pr extends to a probability measure over PC(Γ) so
long as the following three implications hold for all sentences φ and ψ:
• If Γ ⊢φ, then P∞(φ) = 1.
• If Γ ⊢¬φ, then P∞(φ) = 0.
• If Γ ⊢¬(φ ∧ψ), then P∞(φ ∨ψ) = P∞(φ) + P∞(ψ).
Since the three conditions are quite similar in form, we will prove them simultane-
ously using four exemplar traders and parallel arguments.
Deﬁnition of the traders. Suppose that one of the three conditions is violated
by a margin of ε, i.e., one of the following four cases holds:
(L1) Γ ⊢φ, but
(I1) P∞(φ) < 1 −ε;
(L2) Γ ⊢¬φ, but
(I2) P∞(φ) > ε;
(L3) Γ ⊢¬(φ ∧ψ), but
(I3) P∞(φ ∨ψ) < P∞(φ) + P∞(ψ) −ε; or
(L4) Γ ⊢¬(φ ∧ψ), but
(I4) P∞(φ ∨ψ) > P∞(φ) + P∞(ψ) + ε.
Let i ∈{1, 2, 3, 4} be the case that holds. Since the limit P∞exists, there is some
suﬃciently large time sε such that for all n > sε, the inequality Ii holds with n
in place of ∞. Furthermore, since D is a Γ-complete deductive process, for some
suﬃciently large sΓ and all n > sΓ, the logical condition Li holds with Dn in place
of Γ. Thus, letting s := max(sε, sΓ), for n > s one of the following cases holds:
(L1
n) Dn ⊢φ, but
(I1
n) Pn(φ) < 1 −ε;
(L2
n) Dn ⊢¬φ, but
(I2
n) Pn(φ) > ε;
(L3
n) Dn ⊢¬(φ ∧ψ), but
(I3
n) Pn(φ ∨ψ) < Pn(φ) + Pn(ψ) −ε; or
(L4
n) Dn ⊢¬(φ ∧ψ), but
(I4
n) Pn(φ ∨ψ) > Pn(φ) + Pn(ψ) + ε.
(When interpreting these, be sure to remember that each Dn is ﬁnite, and D ⊢
indicates using provability using only propositional calculus, i.e., modus ponens. In
particular, the axioms of ﬁrst order logic are not assumed to be in Dn.)
We now deﬁne, for each of the above four cases, a trader that will exploit the
market P. For n > s, let
T 1
n := φ −φ∗n
T 2
n := −(φ −φ∗n)
T 3
n := ((φ ∨ψ) −(φ ∨ψ)∗n) −(φ −φ∗n) −(ψ −ψ∗n)
T 4
n := (φ −φ∗n) + (ψ −ψ∗n) −((φ ∨ψ) −(φ ∨ψ)∗n)
and for n ≤s let T i
n = 0. Each T i
n can be written down in O(log(n)) time (the
constant s can be hard-coded at a ﬁxed cost), so these T
i are all e.c. traders.
Proof of exploitation.
We leave the constant argument P implicit to reduce
clutter, writing, e.g., φ∗i for φ∗i(P) = Pi(φ), Tn[φ] for Tn[φ](P), and so on.
Consider case 1, where L1
n and I1
n hold for n > s, and look at the trader T
1. For
any n > s and any world W ∈PC(Dn), by linearity of W we have
W
P
i≤n T 1
i

=
X
i≤n
T 1
i [φ] ·
 W(φ) −φ∗i
61

but T 1
i [φ] ≡1 iﬀi > s, so this sum is
=
X
s<i≤n
1 ·
 W(φ) −φ∗i
.
Now, by our choice of s, W(φ) = 1, and i > s implies φ∗i < 1 −ε, so this is
≥
X
s<i≤n
(1 −(1 −ε))
= ε · (n −s)
→∞as n →∞.
In particular, T
1 exploits P, i.e., the set of values
n
W
P
i≤n Ti
 P
  n ∈N+, W ∈PC(Dn)
o
is bounded below but not bounded above. The analysis for case 2 is identical: if
L2
n and I2
n hold for n > s, then T
2 exploits P.
Now consider case 3, where L3
n and I3
n hold for n > s. Then for any time step
n > s and any world W ∈PC(Dn),
W
P
i≤n T 3
i

=
X
i≤n
 (W(¬(φ ∧ψ)) −(φ ∨ψ)∗i) −(W(φ) −φ∗i) −(W(ψ) −ψ∗i)

=
X
s<i≤n
(W(φ ∨ψ) −W(φ) −W(ψ)) −
 (φ ∨ψ)∗i −φ∗i −ψ∗i
but by our choice of s, W(φ∨ψ)−W(φ)−W(ψ) = 0, and i > s implies the inequality
(φ ∨ψ)∗i −φ∗i −ψ∗i < −ε, so the above sum is
≥
X
s<i≤n
ε
= ε · (n −s) →∞as n →∞.
So T
3 exploits P, contradicting the logical induction criterion. The analysis for case
4 is identical. Hence, all four implications must hold for P to satisfy the logical
induction criterion.
6.3
Non-dogmatism
Recall Theorem 4.6.2:
Theorem 4.6.2 (Non-Dogmatism). If Γ ⊬φ then P∞(φ) < 1, and if Γ ⊬¬φ then
P∞(φ) > 0.
Proof of Theorem 4.6.2. We prove the second implication, since the ﬁrst implication
is similar, with selling in place of buying. Suppose for a contradiction that Γ ⊬¬φ
but that P∞(φ) = 0.
Deﬁnition of the trader T. We deﬁne T recursively, along with helper functions
β
k that will ensure that for every k, our trader will buy one share of φ for a price
62

of at most 2−k:
for k = 1, . . . , n:
βk
k := 0
for i = k + 1, . . . , n:
βk
i := Ind2−k−1(φ∗i < 2−k) ·

1 −
i−1
X
j=k
βk
j


Ti[φ] :=
X
j≤i
βk
j
Ti := Ti[φ] · (φ −φ∗i)
Note that all the equations deﬁning Tn can be written down (from scratch) in
O(n3 log(n)) time, so T is an e.c. trader.
Proof of exploitation.
We leave the constant argument P implicit to reduce
clutter, writing, e.g., φ∗i for φ∗i(P) = Pi(φ), Tn[φ] for Tn[φ](P), and so on.
Observe from the recursion above for T that for all i > 0 and k > 0,
0 ≤
i
X
j=k
βk
j ≤1
and for any i and any k ≤i,
βk
i ≥0.
Next, observe that for any k > 0, for i ≥some threshold f(k), we will have φ∗i <
2−k−1, in which case the indicator in the deﬁnition of βk
i will equal 1, at which
point Pi
j=k βk
j = 1. Thus, for all n ≥f(k),
n
X
i=k
βk
i = 1.
Letting Hn = P
i≤n Ti, the following shows that our trader will eventually own an
arbitrarily large number of φ-shares:
Hn[φ] =
X
i≤n
X
k≤i
βk
i
=
X
k≤n
X
k≤i≤n
βk
i
≥
X
k≤n
f(k)≤n
X
k≤i≤n
βk
i
=
X
k≤n
f(k)≤n
1
→∞as n →∞
(6.3.1)
Next we show that our trader never spends more than a total of $1.
Hn[1] = −
X
i≤n
X
k≤i
βk
i · φ∗i,
63

but the indicator function deﬁning βk
i ensures that φ∗i ≤2−k whenever βk
i is non-
zero, so this is
≥−
X
i≤n
X
k≤i
βk
i · 2−k
= −
X
k≤n
2−k ·
X
k≤i≤n
βk
i
≥−
X
k≤n
2−k · 1
Now, for any world W, since Hn[φ] ≥0 for all n and W(φ) ≥0, we have
W(Hn) = Hn[1] + Hn[φ]W(φ)
≥−1 + 0 · 0 ≥−1
so the values W(Hn) are bounded below as n varies. Moreover, since Γ ⊬¬φ, for
every n there is always some W ∈PC(Dn) where W(φ) = 1 (since any consistent
truth assignment can be extended to a truth assignment on all sentences), in which
case
W(Hn) ≥−1 + Hn[φ] · 1
But by equation 6.3.1, this limn→∞Hn[φ] = ∞, so limn→∞W(Hn) = ∞as well.
Hence, our e.c. trader exploits the market, contradicting the logical induction crite-
rion. Therefore, if P∞(φ) = 0, we must have Γ ⊢¬φ.
6.4
Learning Pseudorandom Frequencies
Recall Theorem 4.4.2:
Theorem 4.4.2 (Learning Pseudorandom Frequencies). Let φ be an e.c. sequence
of decidable sentences. If φ is pseudorandom with frequency p over the set of all
P-generable divergent weightings, then
Pn(φn) ≂n p.
Before beginning the proof, the following intuition may be helpful. If the theorem
does not hold, assume without loss of generality that P repeatedly underprices the
φn. Then a trader can buy φn-shares whenever their price goes below p−ε. By the
assumption that the truth values of the φn are pseudorandom, roughly p proportion
of the shares will pay out. Since the trader only pays at most p −ε per share, on
average they make ε on each trade, so over time they exploit the market. All we
need to do is make the trades continuous, and ensure that the trader does not go
below a ﬁxed budget (as in the proof of Theorem 4.1.1).
Proof of Theorem 4.4.2. Suppose for a contradiction that φ is an e.c. sequence of
Γ-decidable sentences such that for every P-generable divergent weighting w,
lim
n→∞
P
i<n wi · ThmΓ φi
P
i<n wi
= p,
but nevertheless, for some ε > 0 and inﬁnitely many n, |Pn(φn) −p| > ε. Without
loss of generality, assume that for inﬁnite many n,
Pn(φn) < p −ε.
(The argument for the case where Pn(φn) > p + ε inﬁnitely often will be the same,
and one of these two cases must obtain.)
64

Deﬁnition of the trader T.
We deﬁne Open : (S × N) →B to be the following
(potentially very slow) computable function:
Open(φ, n) =
0
if Dn ⊢φ or Dn ⊢¬φ;
1
otherwise.
Open is computable because (remembering that ⊢stands for propositional provabil-
ity) we can just search through all truth assignments to the prime sentences of all
sentences in Dn that make the sentences in Dn true, and see if they all yield the
same truth value to φ. We now deﬁne a much faster function MO : (N × N) →B
(mnemonic: “maybe open”) by
MO(φ, n) =





0
if for some m ≤n, Open(φ, m)
returns 0 in ≤n steps
1
otherwise.
Observe that MO(φ, n) runs in O(n2) time, and that for any decidable φ,
• MO(φ, n) = 0 for some suﬃciently large n;
• if MO(φ, n) = 0 then Open(φ, n) = 0;
• if MO(φ, m) = 0 and n > m then MO(φ, n) = 0.
(Note that MO may assign a value of 1 when Open does not, hence the mnemonic
“maybe open”.)
We will now use MO to deﬁne a trader T recursively, along with a helper function
β to ensure that it never holds a total of more than 1 unit of open (fractional) shares.
We let β1 = 0 and for n ≥1,
βn := 1 −
X
i<n
MO(φi, n)Ti[φi];
Tn[φn] := βn · Indε/2(φ∗n
n < p −ε/2);
Tn := Tn[φn] · (φn −φ∗n
n ).
Observe that the expressible feature Tn can be computed (from scratch) in poly(n)
time using MO, so T is an e.c. trader. Notice also that βn and all the Tn(φ) are
always in [0, 1].
A divergent weighting. For the rest of the proof, we leave the constant argument
P implicit to reduce clutter, writing, e.g., φ∗i
i for φ∗i
i (P) = Pi(φi), Tn[φ] for Tn[φ](P),
and so on.
We will show that the sequence of trade coeﬃcients wn = Tn[φn] made by T
against the market P form a P-generable divergent weighting.
Our trader T is
eﬃciently computable and Tn[φn] ∈[0, 1] for all n, so it remains to show that, on
input P≤n,
X
n∈N+
Tn[φn] = ∞.
Suppose this were not the case, so that for some suﬃciently large m,
X
m<j
Tj[φj] < 1/2.
(6.4.1)
By the deﬁnition of MO, there exists some large m′ such that for all i < m,
MO(φi, m′) = 0. At that point, for any n > m′, we have
βn := 1 −
X
i<n
Ti[φi] · MO(φi, n)
= 1 −
X
m<i<n
Ti[φi] · MO(φi, n)
≥1 −
X
m<i
Ti[φi]
65

which, by equation (6.4.1), means that
βn ≥1/2.
Then, by the earlier supposition on P, for some n > m′ we have Pn(φn) < p −ε, at
which point
Tn[φn] = βn · Indε/2(φ∗n
n < p −ε/2) ≥βn · 1 ≥1/2
which contradicts the 1/2 bound in equation (6.4.1). Hence, the sum P
i Ti[φn] must
instead be bounded. This means (Tn[φn])n∈N+ is a P-generable divergent weighting.
Proof of exploitation. Now, by deﬁnition of φ being pseudorandom with fre-
quency p over the class of P-generable divergent weightings, we have that
lim
n→∞
P
i≤n Ti[φi] · ThmΓ(φi)
P
i≤n Ti[φi]
= p.
Thus, for all suﬃciently large n,
X
i≤n
Ti[φi] · ThmΓ(φi) ≥(p −ε/4) ·
X
i≤n
Ti[φi].
Now, since our construction makes βn ∈[0, 1] for all n, we have
X
i≤n
Ti[φi] · MO(φi, n) ≤1.
Also,
W(φi) ≥ThmΓ(φi) −MO(φi, n).
Multiplying this by Ti[φi] and summing over i gives
X
i≤n
Ti[φi] · W(φi) ≥

X
i≤n
Ti[φi] · ThmΓ(φi)

−

X
i≤n
Ti[φi] · MO(φi, n)


≥

X
i≤n
Ti[φi] · ThmΓ(φi)

−1
≥−1 + (p −ε/4)
X
i≤n
Ti[φi].
By the deﬁnition of T, and since φ∗i
i ≤(p −ε/2) whenever Ti[φi] ̸= 0,
−
X
i≤n
Ti[φi] · φ∗i
i ≥−(p −ε/2)
X
i≤n
Ti[φi].
Adding the above two inequalities gives
W

X
i≤n
Ti

≥−1 + (ε/4)
X
i≤n
Ti[φi]
→∞as n →∞
because Ti[φi] is a divergent weighting (as shown above). Hence, T exploits the
market P, contradicting the logical induction criterion. Therefore, for P to satisfy
the logical induction criterion, we must have
lim
n→∞Pn(φn) = p.
66

6.5
Provability Induction
Recall Theorem 4.2.1:
Theorem 4.2.1 (Provability Induction). Let φ be an e.c. sequence of theorems.
Then
Pn(φn) ≂n 1.
Furthermore, let ψ be an e.c. sequence of disprovable sentences. Then
Pn(ψn) ≂n 0.
Proof of Theorem 4.2.1. Suppose φ is an e.c. sequence of sentences with Γ ⊢φn for
all n. Notice that for every i, the indicator ThmΓ(φi) evaluates to 1. Therefore we
immediately have that for any divergent weighting w at all,
lim
n→∞
P
i<n wi · ThmΓ φi
P
i<n wi
= 1.
That is, the sequence φ is pseudorandom (over any class of weightings) with fre-
quency 1. Hence, by Learning Pseudorandom Frequencies (Theorem 4.4.2),
Pn(φn) ≂n 1,
as desired. The proof that Pn(ψn) ≂n 0 proceeds analogously.
Examining the proof of Theorem 4.4.2 (Learning Pseudorandom Frequencies) in the
special case of provability induction yields some intuition. In this case, the trader
deﬁned in that proof essentially buys φn-shares every round that Pn(φn) < 1 −ε.
To avoid overspending, it tracks which φn have been proven so far, and never has
more than 1 total share outstanding. Since eventually each φn is guaranteed to be
valued at 1 in every plausible world, the value of the trader is increased by at least
ε (times the number of φn-shares it purchased) inﬁnitely often. In this way, the
trader makes proﬁts for so long as P fails to recognize the pattern φ of provable
sentences.
7
Discussion
We have proposed the logical induction criterion as a criterion on the beliefs of
deductively limited reasoners, and we have shown that reasoners who satisfy this
criterion (logical inductors) possess many desirable properties when it comes to de-
veloping beliefs about logical statements (including statements about mathematical
facts, long-running computations, and the reasoner themself). We have also given a
computable algorithm LIA for constructing a logical inductor. We will now discuss
applications of logical induction (Section 7.1) and speculate about how and why we
think this framework works (Section 7.2). We then discuss a few variations on our
framework (Section 7.3) before concluding with a discussion of a few open questions
(Section 7.4).
7.1
Applications
Logical inductors are not intended for practical use. The algorithm to compare with
logical induction is not Belief Propagation (an eﬃcient method for approximate in-
ference in Bayesian networks [Pearl 1988]) but Solomonoﬀ’s theory of inductive
inference (an uncomputable method for making ideal predictions about empirical
facts [Solomonoﬀ1964a]). Just as Solomonoﬀ’s sequence predictor assigns probabil-
ities to all possible observations and learns to predict any computable environment,
logical inductors assign probabilities to all possible sentences of logic and learns to
recognize any eﬃciently computable pattern between logical claims.
67

Solomonoﬀ’s theory involves a predictor that considers all computable hypothe-
ses about their observations, weighted by simplicity, and uses Bayesian inference
to zero in on the best computable hypothesis. This (uncomputable) algorithm is
impractical, but has nevertheless been of theoretical use: its basic idiom—consult
a series of experts, reward accurate predictions, and penalize complexity—is com-
monplace in statistics, predictive analytics, and machine learning. These “ensemble
methods” often perform quite well in practice. Refer to Opitz and Maclin (1999)
and Dietterich (2000) for reviews of popular and successful ensemble methods.
One of the key applications of logical induction, we believe, is the development
of an analogous idiom for scenarios where reasoners are uncertain about logical
facts. Logical inductors use a framework similar to standard ensemble methods,
with a few crucial diﬀerences that help them manipulate logical uncertainty. The
experts consulted by logical inductors don’t make predictions about what is going
to happen next; instead, they observe the aggregated advice of all the experts
(including themselves) and attempt to exploit ineﬃciencies in that aggregate model.
A trader doesn’t need to have an opinion about whether or not φ is true; they can
exploit the fact that φ and ¬¬φ have diﬀerent probabilities without having any idea
what φ says or what that’s supposed to mean. This idea and others yield an idiom
for building models that integrate logical patterns and obey logical constraints.
In a diﬀerent vein, we expect that logical inductors can already serve as a drop-in
replacement for formal models of reasoners that assume logical omniscience and/or
perfect Bayesianism, such as in game theory, economics, or theoretical models of
artiﬁcial reasoners.
The authors are particularly interested in tools that help AI scientists attain
novel statistical guarantees in settings where robustness and reliability guarantees
are currently diﬃcult to come by. For example, consider the task of designing an
AI system that reasons about the behavior of computer programs, or that reasons
about its own beliefs and its own eﬀects on the world. While practical algorithms
for achieving these feats are sure to make use of heuristics and approximations, we
believe scientists will have an easier time designing robust and reliable systems if
they have some way to relate those approximations to theoretical algorithms that are
known to behave well in principle (in the same way that Auto-Encoding Variational
Bayes can be related to Bayesian inference [Kingma and Welling 2013]). Modern
models of rational behavior are not up to this task: formal logic is inadequate when
it comes to modeling self-reference, and probability theory is inadequate when it
comes to modeling logical uncertainty.
We see logical induction as a ﬁrst step
towards models of rational behavior that work in settings where agents must reason
about themselves, while deductively limited.
When it comes to the ﬁeld of meta-mathematics, we expect logical inductors to
open new avenues of research on questions about what sorts of reasoning systems
can achieve which forms of self-trust. The speciﬁc type of self-trust that logical
inductors achieve (via, e.g., Theorem 4.12.4) is a subtle subject, and worthy of a
full paper in its own right. As such, we will not go into depth here.
7.2
Analysis
Mathematicians, scientists, and philosophers have taken many diﬀerent approaches
towards the problem of unifying logic with probability theory. (For a sample, refer
to Section 1.2.) In this subsection, we will speculate about what makes the logical
induction framework tick, and why it is that logical inductors achieve a variety of
desiderata. The authors currently believe that the following three points are some
of the interesting takeaways from the logical induction framework:
Following Solomonoﬀand Gaifman. One key idea behind our framework is our
paradigm of making predictions by combining advice from an ensemble of experts in
order to assign probabilities to all possible logical claims. This merges the framework
of Solomonoﬀ(1964a) with that of Gaifman (1964), and it is perhaps remarkable
that this can be made to work. Say we ﬁx an enumeration of all prime sentences of
ﬁrst-order logic, and then hook LIA (Algorithm 5.4.1) up to a theorem prover that
68

enumerates theorems of PA (written using that enumeration). Then all LIA ever
“sees” (from the deductive process) is a sequence of sets like
{#92305 or #19666 is true; #50105 and #68386 are true; #8517 is false}.
From this and this alone, LIA develops accurate beliefs about all possible arith-
metical claims. LIA does this in a manner that outpaces the underlying deductive
process and satisﬁes the desiderata listed above. If instead we hook LIA up to a
ZFC-prover, it develops accurate beliefs about all possible set-theoretic claims. This
is very reminiscent of Solomonoﬀ’s framework, where all the predictor sees is a se-
quence of 1s and 0s, and they start ﬁguring out precisely which environment they’re
interacting with.
This is only one of many possible approaches to the problem of logical uncer-
tainty. For example, Adams’ probability logic (1996) works in the other direction,
using logical axioms to put constraints on an unknown probability distribution and
then using deduction to infer properties of that distribution.
Markov logic net-
works (Richardson and Domingos 2006) construct a belief network that contains a
variable for every possible way of grounding out each logical formula, which makes
them quite ill-suited to the problem of reasoning about the behavior of complex
Turing machines.8 In fact, there is no consensus about what form an algorithm
for “good reasoning” under logical uncertainty should take. Empiricists such as
Hintikka (1962) and Fagin et al. (1995) speak of a set of modal operators that help
diﬀerentiate between diﬀerent types of knowledge; AI scientists such as Russell and
Wefald (1991b), Hay et al. (2012), and Lin et al. (2015) speak of algorithms that
are reasoning about complicated facts while also making decisions about what to
reason about next; mathematicians such as (Briol, Oates, Girolami, Osborne, and
Sejdinovic 2015; Briol, Oates, Girolami, and Osborne 2015; Hennig, Osborne, and
Girolami 2015) speak of numerical algorithms that give probabilistic answers to
particular questions where precise answers are diﬃcult to generate.
Our approach achieves some success by building an approximately-coherent dis-
tribution over all logical claims. Of course, logical induction does not solve all the
problems of reasoning under deductive limitation—far from it! They do not engage
in meta-cognition (in the sense of Russell and Wefald [1991b]) to decide which facts
to reason about next, and they do not give an immediate practical tool (as in the
case of probabilistic integration [Briol, Oates, Girolami, Osborne, and Sejdinovic
2015]), and they have abysmal runtime and uncomputable convergence bounds. It
is our hope that the methods logical inductors use to aggregate expert advice will
eventually yield algorithms that are useful for various applications, in the same way
that useful ensemble methods can be derived from Solomonoﬀ’s theory of inductive
inference.
Keep the experts small. One of the key diﬀerences between our framework and
Solomonoﬀ-inspired ensemble methods is that our “experts” are not themselves pre-
dicting the world. In standard ensemble methods, the prediction algorithm weighs
advice from a number of experts, where the experts themselves are also making
predictions. The “master algorithm” rewards the experts for accuracy and penal-
izes them for complexity, and uses a weighted mixture of the experts to make their
own prediction. In our framework, the master algorithm is still making predictions
(about logical facts), but the experts themselves are not necessarily predictors. In-
stead, the experts are “traders”, who get to see the current model (constructed by
aggregating information from a broad class of traders) and attempt to exploit inef-
ﬁciencies in that aggregate model. This allows traders to identify (and eliminate)
inconsistencies in the model even if they don’t know what’s actually happening in
the world. For example, if a trader sees that P(φ)+P(¬φ) ≪1, they can buy shares
8. Reasoning about the behavior of a Turing machine using a Markov logic network
would require having one node in the graph for every intermediate state of the Turing
machine for every input, so doing inference using that graph is not much easier than simply
running the Turing machine. Thus, Markov logic networks are ill-suited for answering
questions about how a reasoner should predict the behavior of computations that they
cannot run.
69

of both φ and ¬φ and make a proﬁt, even if they have no idea whether φ is true
or what φ is about. In other words, letting the experts buy and sell shares (instead
of just making predictions), and letting them see the aggregate model, allows them
to contribute knowledge to the model, even if they have no idea what’s going on in
the real world.
We can imagine each trader as contributing a small piece of logical knowledge
to a model—each trader gets to say “look, I don’t know what you’re trying to
predict over there, but I do know that this piece of your model is inconsistent”. By
aggregating all these pieces of knowledge, our algorithm builds a model that can
satisfy many diﬀerent complicated relationships, even if every individual expert is
only tracking a single simple pattern.
Make the trading functions continuous. As stated above, our framework gets
signiﬁcant mileage from showing each trader the aggregate model created by input
from all traders, and letting them proﬁt from identifying inconsistencies in that
model. Showing traders the current market prices is not trivial, because the market
prices on day n depend on which trades are made on day n, creating a circular
dependency. Our framework breaks this cycle by requiring that the traders use
continuous betting strategies, guaranteeing that stable beliefs can be found.
In fact, it’s fairly easy to show that something like continuity is strictly necessary,
if the market is to have accurate beliefs about itself. Consider again the paradoxical
sentence χ := “Pn(χ) < 0.5” which is true iﬀits price in P is less than 50¢ on day
n. If, on day n, traders were allowed to buy when χ < 0.5 and sell otherwise, then
there is no equilibrium price. Continuity guarantees that the equilibrium price will
always exist.
This guarantee protects logical inductors from the classic paradoxes of self-
reference—as we have seen, it allows P to develop accurate beliefs about its current
beliefs, and to trust its future beliefs in most cases. We attribute the success of
logical inductors in the face of paradox to the continuity conditions, and we suspect
that it is a general-purpose method that deductively limited reasoners can use to
avoid the classic paradoxes.
7.3
Variations
One notable feature of the logical induction framework is its generality. The frame-
work is not tied to a polynomial-time notion of eﬃciency, nor to any speciﬁc model
of computation. All the framework requires is a method of enumerating possible
patterns of logic (the “traders”) on the one hand, and a method of enumerating
provable sentences of logic (the “deductive process”) on the other. Our algorithm
then gives a method for aggregating those patterns into a combined model that
respects the logical patterns that actually hold.
The framework would work just as well if we used the set of linear-time traders
in place of the set of poly-time traders. Of course, the market built out of linear-
time traders would not satisfy all the same desirable properties—but the method of
induction, which consists of aggregating knowledge from a collection of traders and
letting them all see the combined model and attempt to exploit it, would remain
unchanged.
There is also quite a bit of ﬂexibility in the deﬁnition of a trader. Above, traders
are deﬁned to output continuous piecewise-rational functions of the market prices.
We could restrict this deﬁnition (e.g., by having traders output continuous piecewise-
linear functions of the market prices), or broaden it (by replacing piecewise-rational
with a larger class), or change the encoding scheme entirely. For instance, we could
have the traders output not functions but upper-hemicontinuous relations specifying
which trades they are willing to purchase; or we could give them oracle access to the
market prices and have them output trades (instead of trading strategies). Alterna-
tively, we could refrain from giving traders access to the market prices altogether,
and instead let them sample truth values for sentences according to that sentence’s
probability, and then consider markets that are almost surely not exploited by any
of these traders.
70

In fact, our framework is not even speciﬁc to the domain of logic.
Strictly
speaking, all that is necessary is a set of atomic events that can be “true” or “false”,
a language for talking about Boolean combinations of those atoms, and a deductive
process that asserts things about those atoms (such as “a ∧¬b”) over time. We
have mainly explored the case where the atoms are prime sentences of ﬁrst order
logic, but the atoms could just as easily be bits in a webcam image, in which case
the inductor would learn to predict patterns in the webcam feed. In fact, some
atoms could be reserved for the webcam and others for prime sentences, yielding
an inductor that does empirical and logical induction simultaneously.
For the sake of brevity, we leave the development of this idea to future works.
7.4
Open Questions
With Deﬁnition 3.0.1, we have presented a simple criterion on deductively limited
reasoners, such that any reasoner who meets the criterion satisﬁes a large number
of desiderata, and any reasoner that fails to meet the criterion can have their beliefs
exploited by an eﬃcient trader. With LIA we have shown that this criterion can be
met in practice by computable reasoners.
The logical induction criterion bears a strong resemblance to the “no Dutch
book” criteria used by Ramsey (1931), de Finetti (1937), Teller (1973), and Lewis
(1999) to support Bayesian probability theory. This fact, and the fact that a wide
variety of desirable properties follow directly from a single simple criterion, imply
that logical induction captures a portion of what it means to do good reasoning un-
der deductive limitations. That said, logical induction leaves a number of problems
wide open. Here we discuss four, recalling desiderata from Section 1.1:
Desideratum 15 (Decision Rationality). The algorithm for assigning probabili-
ties to logical claims should be able to target speciﬁc, decision-relevant claims, and
it should reason about those claims as eﬃciently as possible given the computing
resources available.
In the case of logical inductors, we can interpret this desideratum as saying that
it should be possible to tell a logical inductor to reason about one sentence in
particular, and have it eﬃciently allocate resources towards that task. For example,
we might be curious about Goldbach’s conjecture, and wish to tell a logical inductor
to develop its beliefs about that particular question, i.e. by devoting its computing
resources in particular to sentences that relate to Goldbach’s conjecture (such as
sentences that might imply or falsify it).
Our algorithm for logical induction does not do anything of this sort, and there
is no obvious mechanism for steering its deliberations. In the terminology of Hay
et al. (2012), LIA does not do metalevel reasoning, i.e., it does nothing akin to
“thinking about what to think about”. That said, it is plausible that logical induc-
tion could play a role in models of bounded decision-making agents. For example,
when designing an artiﬁcial intelligence (AI) algorithm that does try to reason about
Goldbach’s conjecture, it would be quite useful for that algorithm to have access
to a logical inductor that tells it which other mathematical facts are likely related
(and how). We can imagine a resource-constrained algorithm directing computing
resources while consulting a partially-trained logical inductor, occasionally deciding
that the best use of resources is to train the logical inductor further. At the moment,
these ideas are purely speculative; signiﬁcant work remains to be done to see how
logical induction bears on the problem of allocation of scarce computing resources
when reasoning about mathematical facts.
Desideratum 16 (Answers Counterpossible Questions). When asked questions
about contradictory states of aﬀairs, a good reasoner should give reasonable answers.
In the year 1993, if you asked a mathematician about what we would know about
mathematics if Fermat’s last theorem was false, they would talk about how that
would imply the existence of non-modular elliptic curves. In the year 1994, Fermat’s
last theorem was proven true, so by the principle of explosion, we now know that
71

if Fermat’s last theorem were false, then 1=2 and
√
2 is rational, because from a
contradiction, anything follows. The ﬁrst sort of answer seems more reasonable,
and indeed, reasoning about counterpossibilities (i.e., proving a conjecture false by
thinking about what would follow if it were true) is a practice that mathematicians
engage in regularly. A satisfactory treatment of counterpossibilities has proven elu-
sive; see (Cohen 1990; Vander Laan 2004; Brogaard and Salerno 2007; Krakauer
2012; Bjerring 2014) for some discussion and ideas. One might hope that a good
treatment of logical uncertainty would naturally result in a good treatment of coun-
terpossibilities.
There are intuitive reasons to expect that a logical inductor has reasonable
beliefs about counterpossibilities. In the days before D has (propositionally) ruled
out worlds inconsistent with Fermat’s last theorem, P has to have beliefs that allow
for Fermat’s last theorem to be false, and if the proof is a long time in coming, those
beliefs are likely reasonable. However, we do not currently have any guarantees of
this form—P∞still assigns probability 0 to Fermat’s last theorem being false, and
so the conditional probabilities are not guaranteed to be reasonable, so we haven’t
yet found anything satisfactory to say with conﬁdence about P’s counterpossible
beliefs.
While the discussion of counterpossibilities may seem mainly academic, Soares
and Fallenstein (2015) have argued that counterpossibilities are central to the prob-
lem of designing robust decision-making algorithms. Imagine a deterministic agent
agent evaluating three diﬀerent “possible scenarios” corresponding to three diﬀerent
actions the agent could take. Intuitively, we want the nth scenario (modeled inside
the agent) to represent what would happen if the agent took the nth action, and
this requires reasoning about what would happen if agent(observation) had the
output a vs b vs c. Thus, a better understanding of counterpossible reasoning could
yield better decision algorithms. Signiﬁcant work remains to be done to understand
and improve the way that logical inductors answer counterpossible questions.
Desideratum 17 (Use of Old Evidence). When a bounded reasoner comes up with
a new theory that neatly describes anomalies in the old theory, that old evidence
should count as evidence in favor of the new theory.
The canonical example of the problem of old evidence is Einstein’s development of
the theory of general relativity and its retrodiction of the precession in Mercury’s
orbit.
For hundreds of years before Einstein, astronomers knew that Newton’s
equations failed to model this precession, and Einstein’s retrodiction counted as
a large boost for his theory.
This runs contrary to Bayes’ theorem, which says
that a reasoner should wring every drip of information out of every observation the
moment that the evidence appears. A Bayesian reasoner keeps tabs on all possible
hypotheses at all times, and so they never ﬁnd a new hypothesis in a burst of insight,
and reward it for retrodictions. Humans work diﬀerently—scientists spent centuries
without having even one good theory for the precession of Mercury, and the diﬃcult
scientiﬁc labor of Einstein went into inventing the theory.
There is a weak sense in which logical inductors solve the problem of old
evidence—as time goes on, they get better and better at recognizing patterns in the
data that they have already seen, and integrating those old patterns into their new
models. That said, a strong solution to the problem of old evidence isn’t just about
ﬁnding new ways to use old data every so often; it’s about giving a satisfactory
account of how to algorithmically generate new scientiﬁc theories. In that domain,
logical induction has much less to say: they “invent” their “theories” by sheer brute
force, iterating over all possible polynomial-time methods for detecting patterns in
data.
There is some hope that logical inductors will shed light on the question of how
to build accurate models of the world in practice, just as ensemble methods yield
models that are better than any individual expert in practice. However, the task
of using logical inductors to build practical models in some limited domain is wide
open.
72

Desideratum 14 (Eﬃciency). The algorithm for assigning probabilities to logical
claims should run eﬃciently, and be usable in practice.
Logical inductors are far from eﬃcient, but they do raise an interesting empirical
question. While the theoretically ideal ensemble method (the universal semimea-
sure [Li and Vitányi 1993]) is uncomputable, practical ensemble methods often make
very good predictions about their environments. It is therefore plausible that prac-
tical logical induction-inspired approaches could manage logical uncertainty well
in practice. Imagine we pick some limited domain of reasoning, and a collection of
constant- and linear-time traders. Imagine we use standard approximation methods
(such as gradient descent) to ﬁnd approximately-stable market prices that aggregate
knowledge from those traders. Given suﬃcient insight and tweaking, would the re-
sulting algorithm be good at learning to respect logical patterns in practice? This
is an empirical question, and it remains to be tested.
7.5
Acknowledgements
We acknowledge Abram Demski, Alex Appel, Benya Fallenstein, Daniel Filan,
Eliezer Yudkowsky, Jan Leike, János Kramár, Nisan Stiennon, Patrick LaVictoire,
Paul Christiano, Sam Eisenstat, Scott Aaronson, and Vadim Kosoy, for valuable
comments and discussions. We also acknowledge contributions from attendees of
the MIRI summer fellows program, the MIRIxDiscord group, the MIRIxLA group,
and the MIRIχ group.
This research was supported as part of the Future of Life Institute (future-
oﬂife.org) FLI-RFP-AI1 program, grant #2015-144576.
References
Aaronson, Scott. 2013. “Why philosophers should care about computational complexity.”
In Computability: Turing, Gödel, Church, and Beyond, edited by B. Jack Copeland,
Carl J. Posy, and Oron Shagrir. MIT Press.
Adams, Ernest W. 1996. A Primer of Probability Logic. University of Chicago Press.
Akama, Seiki, and Newton C. A. da Costa. 2016. “Why Paraconsistent Logics?” In Towards
Paraconsistent Engineering, edited by Seiki Akama, 7–24. Springer.
Bacharach, Michael. 1994. “The epistemic structure of a theory of a game.” Theory and
Decision 37 (1): 7–48.
Battigalli, Pierpaolo, and Giacomo Bonanno. 1999. “Recent results on belief, knowledge
and the epistemic foundations of game theory.” Research in Economics 53 (2): 149–
225.
Beygelzimer, Alina, John Langford, and David M. Pennock. 2012. “Learning Perfor-
mance of Prediction Markets with Kelly Bettors.” In 11th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2012), edited by Vincent
Conitzer, Michael Winikoﬀ, Wiebe van der Hoek, and Lin Padgham, 1317–1318. In-
ternational Foundation for Autonomous Agents / Multiagent Systems.
Binmore, Kenneth. 1992. “Foundations of game theory.” In Advances in Economic Theory:
Sixth World Congress, edited by Jean-Jacques Laﬀont, 1:1–31.
Bjerring, Jens Christian. 2014. “On Counterpossibles.” Philosophical Studies 168 (2): 327–
353.
Blair, Howard A., and V.S. Subrahmanian. 1989. “Paraconsistent Logic Programming.”
Theoretical Computer Science 68 (2): 135–154.
Boole, George. 1854. An investigation of the laws of thought: on which are founded the
mathematical theories of logic and probabilities. Dover Publications.
73

Briol, François-Xavier, Chris Oates, Mark Girolami, and Michael A. Osborne. 2015. “Frank-
Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees.”
In Advances in Neural Information Processing Systems 28 (NIPS 2015), edited by C.
Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1162–1170. Curran
Associates, Inc.
Briol, François-Xavier, Chris Oates, Mark Girolami, Michael A. Osborne, and Dino Sejdi-
novic. 2015. “Probabilistic Integration,” arXiv: 1512.00933 [stat.ML].
Brogaard, Berit, and Joe Salerno. 2007. “Why Counterpossibles are Non-Trivial.” The
Reasoner 1 (1): 5–6.
Calude, Cristian S, and Michael A Stay. 2008. “Most programs stop quickly or never halt.”
Advances in Applied Mathematics 40 (3): 295–308.
Campbell-Moore, Catrin. 2015. “How to Express Self-Referential Probability. A Kripkean
Proposal.” The Review of Symbolic Logic 8 (04): 680–704.
Carnap, Rudolf. 1962. Logical Foundations of Probability. University of Chicago Press.
Christiano, Paul. 2014. Non-Omniscience, Probabilistic Inference, and Metamathematics.
Technical report 2014–3. Berkeley, CA: Machine Intelligence Research Institute. htt
p://intelligence.org/files/Non-Omniscience.pdf.
Christiano, Paul F., Eliezer Yudkowsky, Marcello Herreshoﬀ, and Mihály Bárász. 2013.
Deﬁnability of Truth in Probabilistic Logic. Early Draft. Berkeley, CA: Machine Intel-
ligence Research Institute, April 2, 2013. https://intelligence.org/files/Definability
TruthDraft.pdf.
Cohen, Daniel. 1990. “On What Cannot Be.” In Truth or Consequences: Essays in Honor
of Nuel Belnap, 123–132. Kluwer Academic Publishers.
de Finetti, Bruno. 1937. “Foresight: Its Logical Laws, Its Subjective Sources.” In Stud-
ies in Subjective Probability, edited by Henry E. Kyburg and Howard E.K. Smokler.
Huntington, New York: Roger E. Kreiger Publishing Co.
De Raedt, Luc. 2008. Logical and Relational Learning. Cognitive Technologies. Springer.
De Raedt, Luc, and Kristian Kersting. 2008. “Probabilistic Inductive Logic Programming.”
In Probabilistic Inductive Logic Programming, 1–27. Springer.
De Raedt, Luc, and Angelika Kimmig. 2015. “Probabilistic (logic) programming concepts.”
Machine Learning 100 (1): 5–47.
Demski, Abram. 2012. “Logical Prior Probability.” Edited by Joscha Bach, Ben Goertzel,
and Matthew Iklé. Artiﬁcial General Intelligence. 5th International Conference, AGI
2012 (New York), no. 7716, 50–59.
Dietterich, Thomas G. 2000. “Ensemble Methods in Machine Learning.” In 1st Interna-
tional workshop on Multiple Classiﬁer Systems (MCS 2000), 1–15. Springer.
Eells, Ellery. 1990. “Bayesian problems of old evidence.” Scientiﬁc Theories 14:205–223.
Eklund, Matti. 2002. “Inconsistent Languages.” Philosophy and Phenomenological Research
64 (2): 251–275.
Enderton, Herbert B. 2001. A Mathematical Introduction to Logic. Academic Press.
Fagin, Ronald, and Joseph Y. Halpern. 1987. “Belief, awareness, and limited reasoning.”
Artiﬁcial intelligence 34 (1): 39–76.
Fagin, Ronald, Joseph Y. Halpern, Yoram Moses, and Moshe Vardi. 1995. Reasoning about
knowledge, vol. 4. MIT Press Cambridge.
Fuhrmann, André. 2013. “Relevant logics, modal logics and theory change.” PhD diss.,
Australian National University - Research School of Social Sciences.
Gaifman, Haim. 1964. “Concerning Measures in First Order Calculi.” Israel Journal of
Mathematics 2 (1): 1–18.
Gaifman, Haim, and Marc Snir. 1982. “Probabilities over rich languages, testing and ran-
domness.” Journal of Symbolic Logic 47 (03): 495–548.
74

Garber, Daniel. 1983. “Old evidence and logical omniscience in Bayesian conﬁrmation
theory.” Testing scientiﬁc theories 10:99–131.
Gärdenfors, Peter. 1988. Knowledge in Flux: Modeling the Dynamics of Epistemic States.
Bradford Books, MIT Press.
Garrabrant, Scott, Tsvi Benson-Tilsen, Siddharth Bhaskar, Abram Demski, Joanna
Garrabrant, George Koleszarik, and Evan Lloyd. 2016. “Asymptotic Logical Uncer-
tainty and the Benford Test.” In 9th Conference on Artiﬁcial General Intelligence
(AGI-16), edited by Bas Steunebrink, Pei Wang, and Ben Goertzel, 9782:202–211.
Lecture Notes in Artiﬁcial Intelligence. Springer International Publishing.
Garrabrant, Scott, Benya Fallenstein, Abram Demski, and Nate Soares. 2016. “Inductive
Coherence,” arXiv: 1604.05288 [cs.AI].
Garrabrant, Scott, Nate Soares, and Jessica Taylor. 2016. “Asymptotic Convergence in
Online Learning with Unbounded Delays,” arXiv: 1604.05280.
Gerla, Giangiacomo. 2013. Fuzzy logic: mathematical tools for approximate reasoning.
Vol. 11. Trends in Logic. Springer.
Glanzberg, Michael. 2001. “The Liar in Context.” Philosophical Studies 103 (3): 217–251.
Glymour, Clark. 1980. Theory and Evidence. Princeton University Press.
Gödel, Kurt, Stephen Cole Kleene, and John Barkley Rosser. 1934. On Undecidable Propo-
sitions of Formal Mathematical Systems. Princeton, NJ: Institute for Advanced Study.
Good, Irving J. 1950. Probability and the Weighing of Evidence. Charles Griﬃn, London.
Grim, Patrick. 1991. The Incomplete Universe: Totality, Knowledge, and Truth. Mit Press.
Guarino, Nicola. 1998. “Formal Ontology and Information Systems.” In Formal Ontol-
ogy in Information Systems: Proceedings of FOIS’98, 46:3–15. Frontiers in Artiﬁcial
Intelligence and Applications. Amsterdam: IOS Press.
Gupta, Anil, and Nuel D. Belnap. 1993. The Revision Theory of Truth. MIT Press.
Hacking, Ian. 1967. “Slightly More Realistic Personal Probability.” Philosophy of Science
34 (4): 311–325.
Hailperin, Theodore. 1996. “Sentential probability logic.”
Halpern, Joseph Y. 2003. Reasoning about Uncertainty. Cambridge, MA: MIT Press.
Hay, Nicholas, Stuart J. Russell, Solomon Eyal Shimony, and David Tolpin. 2012. “Select-
ing Computations: Theory and Applications.” In Uncertainty in Artiﬁcial Intelligence
(UAI-’12), edited by Nando de Freitas and Kevin Murphy, 346–355.
Hennig, Philipp, Michael A. Osborne, and Mark Girolami. 2015. “Probabilistic numerics
and uncertainty in computations.” Proceedings of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences 471 (2179).
Hilbert, David. 1902. “Mathematical Problems.” Bulletin of the American Mathematical
Society 8 (10): 437–480.
Hintikka, Jaakko. 1962. Knowledge and belief: An introduction to the logic of the two
notions. Cornell University Press.
. 1979. “Impossible Possible Worlds Vindicated.” In Game-Theoretical Semantics:
Essays on Semantics by Hintikka, Carlson, Peacocke, Rantala, and Saarinen, edited
by Esa Saarinen, 367–379. Springer.
Hutter, Marcus, John W. Lloyd, Kee Siong Ng, and William T. B. Uther. 2013. “Proba-
bilities on Sentences in an Expressive Logic.” Journal of Applied Logic 11 (4): 386–
420.
Jaynes, E. T. 2003. Probability Theory: The Logic of Science. Edited by G. Larry Bretthorst.
New York: Cambridge University Press.
Jeﬀrey, Richard. 1983. “Bayesianism with a human face.” Testing scientiﬁc theories, min-
nesota studies in the philosophy of science 10:133–156.
75

Joyce, James M. 1999. The Foundations of Causal Decision Theory. Cambridge Studies in
Probability, Induction and Decision Theory. New York: Cambridge University Press.
Kersting, Kristian, and Luc De Raedt. 2007. “Bayesian Logic Programming: Theory and
Tool.” Chap. 10 in Introduction to Statistical Relational Learning, edited by Lise
Getoor and Ben Taskar, 291–322. MIT Press.
Khot, Tushar, Niranjan Balasubramanian, Eric Gribkoﬀ, Ashish Sabharwal, Peter Clark,
and Oren Etzioni. 2015. “Markov Logic Networks for Natural Language Question
Answering,” arXiv: 1507.03045 [stat.ML].
Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes,” arXiv:
1312.6114 [stat.ML].
Klir, George, and Bo Yuan. 1995. Fuzzy Sets and Fuzzy Logic. New Jersey: Prentice Hall.
Kok, Stanley, and Pedro Domingos. 2005. “Learning the structure of Markov logic net-
works.” In 22nd International Conference on Machine Learning (ICML ’05), 441–448.
ACM.
Kolmogorov, A.N. 1950. “Foundations of the Theory of Probability.”
Krakauer, Barak. 2012. “Counterpossibles.” PhD diss., University of Massachusetts -
Amherst.
Kucera, Antonin, and Andre Nies. 2011. “Demuth randomness and computational com-
plexity.” Annals of Pure and Applied Logic 162 (7): 504–513.
Lewis, David. 1999. Papers in metaphysics and epistemology. Vol. 2. Cambridge University
Press.
Li, Ming, and Paul M. B. Vitányi. 1993. An Introduction to Kolmogorov Complexity and
its Applications. 1st ed. New York: Springer.
Lin, Christopher H, Andrey Kolobov, Ece Kamar, and Eric Horvitz. 2015. “Metareasoning
for Planning Under Uncertainty,” arXiv: 1505.00399 [cs.AI].
Lindley, Dennis V. 1991. “Making Decisions.”
Lipman, Barton L. 1991. “How to decide how to decide how to...: Modeling limited ratio-
nality.” Econometrica: Journal of the Econometric Society 59 (4): 1105–1125.
Łoś, Jerzy. 1955. “On the Axiomatic Treatment of Probability.” Colloquium Mathematicae
3 (2): 125–137.
Lowd, Daniel, and Pedro Domingos. 2007. “Eﬃcient Weight Learning for Markov Logic
Networks.” In 11th European Conference on Principles and Practice of Knowledge
Discovery in Databases (PKDD 2007), edited by Joost N. Kok, Jacek Koronacki,
Ramon Lopez de Mantaras, Stan Matwin, Dunja Mladenič, and Andrzej Skowron,
200–211. Springer.
McCallum, Andrew, Karl Schultz, and Sameer Singh. 2009. “Factorie: Probabilistic pro-
gramming via imperatively deﬁned factor graphs.” In Advances in Neural Information
Processing Systems 22 (NIPS 2009), edited by Y. Bengio, D. Schuurmans, J. D. Laf-
ferty, C. K. I. Williams, and A. Culotta, 1249–1257. Curran Associates, Inc.
McGee, Vann. 1990. Truth, Vagueness, and Paradox: An Essay on the Logic of Truth.
Hackett Publishing.
Meyer, J-J Ch., and Wiebe Van Der Hoek. 1995. Epistemic logic for AI and computer
science. Cambridge Tracts in Theoretical Computer Science 41. Cambridge University
Press.
Mihalkova, Lilyana, Tuyen Huynh, and Raymond J Mooney. 2007. “Mapping and Revis-
ing Markov Logic Networks for Transfer Learning.” In 22nd Conference on Artiﬁcial
Intelligence (AAAI-07), 1:608–614. AAAI Press.
Mortensen, Chris. 2013. Inconsistent Mathematics. Vol. 312. Mathematics and Its Appli-
cations. Springer.
Muggleton, Stephen H., and Hiroaki Watanabe. 2014. Latest Advances in Inductive Logic
Programming. World Scientiﬁc.
76

Muiño, David Picado. 2011. “Measuring and Repairing Inconsistency in Probabilistic
Knowledge Bases.” International Journal of Approximate Reasoning 52 (6): 828–840.
Opitz, David, and Richard Maclin. 1999. “Popular Ensemble Methods: An Empirical
Study.” Journal of Artiﬁcial Intelligence Research 11:169–198.
Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo, CA: Morgan Kaufmann.
Polya, George. 1990. Mathematics and Plausible Reasoning: Patterns of plausible inference.
Vol. 2. Princeton University Press.
Potyka, Nico. 2015. “Solving Reasoning Problems for Probabilistic Conditional Logics with
Consistent and Inconsistent Information.” PhD diss., Fernuniversität.
Potyka, Nico, and Matthias Thimm. 2015. “Probabilistic Reasoning with Inconsistent Be-
liefs Using Inconsistency Measures.” In 24th International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI-15), 3156–3163. Buenos Aires, Argentina: AAAI Press.
Priest, Graham. 2002. “Paraconsistent Logic.” In Handbook of Philosophical Logic, 6:287–
393. Kluwer Academic Publishers.
Ramsey, Frank Plumpton. 1931. “Truth and Probability.” In The Foundations of Mathe-
matics and other Logical Essays, edited by Richard Bevan Braithwaite, 156–198. New
York: Harcourt, Brace.
Rantala, Veikko. 1979. “Urn Models: A New Kind of Non-Standard Model for First-Order
Logic.” In Game-Theoretical Semantics: Essays on Semantics by Hintikka, Carlson,
Peacocke, Rantala, and Saarinen, edited by Esa Saarinen, 347–366. Springer.
Richardson, Matthew, and Pedro Domingos. 2006. “Markov Logic Networks.” Machine
Learning 62 (1-2): 107–136.
Rubinstein, Ariel. 1998. Modeling Bounded Rationality. MIT Press.
Russell, Stuart J. 2016. “Rationality and Intelligence: A Brief Update.” In Fundamental
Issues of Artiﬁcial Intelligence, 376:7–28. Synthese Library. Springer.
Russell, Stuart J., and Eric H. Wefald. 1991a. Do the Right Thing: Studies in Limited
Rationality. MIT Press.
. 1991b. “Principles of Metareasoning.” Artiﬁcial intelligence 49 (1-3): 361–395.
Savage, Leonard J. 1954. “The foundations of statistics.”
. 1967. “Diﬃculties in the theory of personal probability.” Philosophy of Science 34
(4): 305–310.
Sawin, Will, and Abram Demski. 2013. “Computable probability distributions which con-
verge on believing true Π1 sentences will disbelieve true Π2 sentences.”
Schlesinger, George N. 1985. Range of Epistemic Logic. Scots Philosophical Monograph.
Elsevier Science Ltd.
Simon, Herbert Alexander. 1982. Models of bounded rationality: Empirically grounded eco-
nomic reason. Vol. 3. MIT Press.
Singla, Parag, and Pedro Domingos. 2005. “Discriminative Training of Markov Logic Net-
works.” In 20th National Conference on Artiﬁcial Intelligence (AAAI-05), 2:868–873.
AAAI Press.
Soares, Nate, and Benja Fallenstein. 2015. “Toward Idealized Decision Theory,” arXiv:
1507.01986 [cs.AI].
Solomonoﬀ, Ray J. 1964a. “A Formal Theory of Inductive Inference. Part I.” Information
and Control 7 (1): 1–22.
. 1964b. “A Formal Theory of Inductive Inference. Part II.” Information and Control
7 (2): 224–254.
Sowa, John F. 1999. Knowledge Representation: Logical, Philosophical, and Computational
Foundations. Brooks-Cole.
77

Sprenger, Jan. 2015. “A Novel Solution to the Problem of Old Evidence.” Philosophy of
Science 82 (3): 383–401.
Teller, Paul. 1973. “Conditionalization and observation.” Synthese 26 (2): 218–258.
Thimm, Matthias. 2013a. “Inconsistency Measures for Probabilistic Logics.” Artiﬁcial In-
telligence 197:1–24.
. 2013b. “Inconsistency measures for probabilistic logics.” Artiﬁcial Intelligence
197:1–24.
Tran, Son D, and Larry S Davis. 2008. “Event modeling and recognition using markov
logic networks.” In European Conference on Computer Vision, 610–623. Springer.
Turing, Alan M. 1936. “On Computable Numbers, with an Application to the Entschei-
dungsproblem.” Proceedings of the London Mathematical Society, 2nd ser., 42 (230–
265).
Vajda, Steven. 1972. Probabilistic Programming. Probability and Mathematical Statistics:
A Series of Monographs and Textbooks. Academic Press.
Vander Laan, David. 2004. “Counterpossibles and similarity.” Chap. 20 in Lewisian
Themes: The Philosophy of David K. Lewis, edited by Frank Jackson and Graham
Priest, 258–275. Oxford University Press.
von Neumann, John, and Oskar Morgenstern. 1944. Theory of Games and Economic Be-
havior. 1st ed. Princeton, NJ: Princeton University Press.
Wang, Jue, and Pedro M. Domingos. 2008. “Hybrid Markov Logic Networks.” In 23rd
Conference on Artiﬁcial Intelligence (AAAI-08), 2:1106–1111. AAAI Press.
Wood, Frank, Jan-Willem van de Meent, and Vikash Mansinghka. 2014. “A New Approach
to Probabilistic Programming Inference.” In Proceedings of the 17th International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 33:1024–1032. JMLR
Workshop and Conference Proceedings.
Yen, John, and Reza Langari. 1999. Fuzzy Logic: Intelligence, Control, and Information.
Vol. 1. Pearson.
Zhang, Yitang. 2014. “Bounded gaps between primes.” Annals of Mathematics 179 (3):
1121–1174.
Zilberstein, Shlomo. 2008. “Metareasoning and bounded rationality.” Metareasoning:
Thinking about Thinking, MIT Press, forthcoming.
Zvonkin, Alexander K., and Leonid A. Levin. 1970. “The Complexity of Finite Objects
and the Development of the Concepts of Information and Randomness by Means of
the Theory of Algorithms.” Russian Mathematical Surveys 25 (6): 83–124.
Zynda, Lyle. 1995. “Old evidence and new theories.” Philosophical Studies 77 (1): 67–95.
78

A
Preliminaries
A.1
Organization of the Appendix
The appendix is organized diﬀerently from the paper. Here we describe the broad
dependency structure of the proofs and mention the theorems that are proven by
constructing explicit traders (rather than as corollaries). Note that theorems that
were proven in Section 6 are also proven here, but diﬀerently (generally much more
concisely, as a corollary of some other theorem).
A. Preliminaries. Appendix A.2 describes expressible features in full detail.
Appendix A.3 deﬁnes some notions for combinations, and deﬁnes when a sequence
of traders can be “eﬃciently emulated”, which will be useful in B, D.1, and G.
B. Convergence.
Appendix B.1 introduces a tool for constructing traders
(Lemma B.1.3, Return on Investment) that is used in B and D.1.
Appendices
B.2 (Aﬃne Preemptive Learning) and B.5 (Persistence of Aﬃne Knowledge) prove
those theorems using Lemma B.1.3, and the remainder of B derives some corollaries
(convergence and non-aﬃne special cases).
C. Coherence. Appendix C.1 proves Aﬃne Coherence, giving (Aﬃne) Prov-
ability Induction as corollaries. The remainder of C derives corollaries of Provability
Induction (consistency and halting) and of Aﬃne Provability Induction (coherence
and exclusive-exhaustive relationships).
D. Statistics.
Appendix D.1 proves Aﬃne Recurring Unbiasedness using
Lemma B.1.3, giving Simple Calibration (D.3) as a corollary.
Appendices D.4
(Aﬃne Unbiasedness From Feedback) and D.6 (Learning Pseudorandom Aﬃne Se-
quences) prove those theorems by constructing traders, and the remainder of Ap-
pendix D derives corollaries (varied and non-aﬃne cases).
E. Expectations. Appendix E.2 proves the Mesh Independence Lemma by
constructing a trader, and E.1 and E.5 prove two other lemmas on expectations;
basic properties of expectations such as convergence and linearity are also proved.
These proofs rely on theorems proven in B and C. The remainder of E proves analogs
for expectations of the convergence, coherence, and statistical theorems by applying
their aﬃne versions to F-combinations expressing expectations.
F. Introspection and Self-Trust. The ﬁrst part of Appendix F proves intro-
spection properties using Aﬃne Provability Induction and Expectation Provability
Induction. The remainder derives the self-trust properties as applications of theo-
rems proven in Appendix E.
G. Non-Dogmatism and Closure.
Appendix G is mostly self-contained.
Appendix G.1 proves a simple analog of the return on investment lemma with
stronger hypotheses; this is applied to constructing traders in G.2 (Uniform Non-
Dogmatism), G.3 (Occam Bounds), and G.5 (Domination of the Universal Semimea-
sure), with non-dogmatism and strict domination as corollaries.
Appendix G.8
(Conditionals on Theories) uses uniform non-dogmatism, preemptive learning, and
G.7 (Closure under Finite Perturbations).
A.2
Expressible Features
This section can be safely skipped and referred back to as desired.
Recall that a trading strategy for day n is given by an aﬃne combination of
sentences with expressible feature coeﬃcients. As such, a machine that implements
a trader must use some notation for writing down those features. Here, to be fully
rigorous, we will make an explicit choice of notation for expressible features. Recall
their deﬁnition:
Deﬁnition 3.4.3 (Expressible Feature). An expressible feature ξ ∈F is a val-
uation feature expressible by an algebraic expression built from price features φ∗n
for each n ∈N+ and φ ∈S, rational numbers, addition, multiplication, max(−, −),
and a “safe reciprocation” function max(1, −)−1.
We write EF for the set of all expressible features, EFn for the set of expressible
features of rank ≤n, and deﬁne an EF-progression to be a sequence ξ such that
ξn ∈EFn.
79

A (multi-line) string representing an expressible feature will be called a well-
formed feature expression, and will be built from smaller expressions involving vari-
ables (mainly to save space when a particular expression would otherwise need to
be repeated many times).
We deﬁne the set of variable feature expressions Ξ inductively to include:
• Past and present market prices: for all i ≤n and for all ψ ∈S, there is a
symbol ψ∗i ∈Ξ.
• Rationals: Q ⊂Ξ.
• Variables: V ⊂Ξ.
Further, if ξ ∈Ξ and ζ ∈Ξ, then the following operations on them are as well:
• Addition: ξ + ζ ∈Ξ.
• Multiplication: ξ · ζ ∈Ξ.
• Maximum: max(ξ, ζ) ∈Ξ.
• Safe reciprocation: 1/ max(1, ξ) ∈Ξ.
These operations are suﬃcient to generate all the expressible features we will need.
For example,
−ξ := (−1) · ξ;
min(ξ, ζ) := −max(−ξ, −ζ);
|ξ| := max(ξ, −ξ);
and when ζ ≥ε for some constant ε > 0, we can deﬁne
ξ/ζ := (1/ε) · ξ/ max(1, (1/ε) · ζ).
We now deﬁne a well-formed feature expression to be a (multi-line) string of the
following form:
v1 := (feature expression with no variables);
v2 := (feature expression involving v1);
· · ·
vk := (feature expression involving v1, . . . , vk−1);
return (feature expression involving v1, . . . , vk),
where the ﬁnal expression after “return ” is the expression evaluated to actually
compute the expressible feature deﬁned by this code block.
Examples
The following well-formed feature expression deﬁnes a rank 7 expressible feature:
v1 := φ∗7
1 + φ∗4
2
v2 := v1 −1
return 3 · max(v1, v2).
If the market at time 7 has P7(φ1) = 0.8 and the market at time 4 had P4(φ2) = 0,
then this expressible feature evaluates to
3 · max(v1, v2) = 3 · max(0.8, −0.2) = 2.4.
80

An n-strategy can now be written down in a very similar format, sharing variable
deﬁnitions used in the various coeﬃcients to save space. For example, the following
code deﬁnes a 7-strategy:
v1 := φ∗7
1 + φ∗4
2
v2 := v1 · v1
T [φ1] := 3 · max(v1, v2)
T [φ2] := 6 · max(v1, v2).
T :=
2
X
i=1
T [φi] · (φi −φ∗n
i )
return T
Notice that the function φ∗7
1
returning the current market price of φ1 aﬀects
(via v1) how many shares of φ1 this trader buys. This is permitted, and indeed is
crucial for allowing traders to base their trades on the current market prices.
Dynamic programming for traders
We will often deﬁne traders that make use of indexed variables that are deﬁned
recursively in terms of previous indices, as in e.g. the proof of Theorem 4.1.1 (Con-
vergence) in Section 6.1. In particular, we often have traders refer to their own
past trades, e.g. using expressible features of the form Ti[φ] for i < n to deﬁne their
trade at time n. This can be written down in polynomial time using the expression
language for features, via dynamic programming. For example, to use previous
trades, a trader can recapitulate all the variables used in all its previous trading
strategies. As long as the trading strategies are eﬃciently computable given previ-
ous trades as variables, they are still eﬃciently computable without them (possibly
with a higher-degree polynomial).
A.3
Deﬁnitions
Price of a Combination
Deﬁnition A.3.1 (Price of a Combination). Given any aﬃne combination
A = c + ξ1φ1 + · · · + ξkφk
of rank ≤n, observe that the map V 7→Vn(A) is an expressible feature, called the
price of A on day n, and is given by the expression
A∗n := c + ξ1φ1
∗n + · · · + ξkφk
∗n.
For any valuation sequence U, observe by linearity and associativity that
(V(A))(U) = V(A(U)) = c(U) +
X
φ
ξφ(U)V(φ).
Buying a Combination
Deﬁnition A.3.2 (Buying a Combination). Given any EF-combination A† of
rank ≤n, we deﬁne a corresponding n-strategy called buying A† on day n to
equal
A† −A†∗n.
Observe that buying A on day n is indeed an n-strategy.
81

F-Combinations Corresponding to F-LUV Combinations
Deﬁnition A.3.3 (Ex). Let B := c+ξ1X1 +· · ·+ξkXk be an F-LUV combination.
Deﬁne
Exm(A) = c + ξ1
m−1
X
i=0
1
m(“X1 > i/m”) + · · · + ξk
m−1
X
i=0
1
m(“Xk > i/m”)
to be a F-aﬃne combination corresponding to B. Note that V(Exm(B)) = EV
m(B).
Also note that if (Bn)n is bounded, then (Exn(Bn))n is bounded; we will use this
fact freely in what follows.
Eﬃciently Emulatable Sequence of Traders
In Appendices B, D.1, and G, we will construct traders that allocate their money
across multiple strategies for exploiting the market. In order to speak unambigu-
ously about multiple overlapping long-term strategies for making trades, we deﬁne
the notion of a sequence of traders that can be eﬃciently emulated by one trader.
Deﬁnition A.3.4 (Eﬃciently Emulatable Sequence of Traders). We say that a
sequence of traders (T
k)k is eﬃciently emulatable if
• the sequence of programs that compute the T
k can be eﬃciently generated;
• those programs for T
k have uniformly bounded runtime, i.e., there exists a
constant c such that for all k and all times n, the program that computes T
k
runs in time O(nc); and
• for all k and all n < k, we have that T
k
n is the zero trade.
Eﬃciently emulatable sequences are so named because a single trader T can emulate
the entire sequence of traders (T
k)k. That is, on time n, T can directly compute
all the trading strategies T k
n for k ≤n by listing the appropriate programs and
running them on input n. This can be done in polynomial time by deﬁnition of an
eﬃciently emulatable sequence. We require that T
k does not make non-zero trades
before time k so that the emulator T need not truncate any trades made by the T
k.
B
Convergence Proofs
B.1
Return on Investment
This section provides a useful tool for constructing traders, which will be applied
in Appendix B and Appendix D.1. The reader may wish to ﬁrst begin with the
proof in Appendix B.2 of Theorem 4.5.7 as motivation of the return on investment
lemma.
Statement of the ε-ROI lemma.
If we have a logical inductor P, we know
that P cannot be exploited by any trader. It will often be easy to show that if
P fails to satisfy some property, then there is a trader T that takes advantage
of a speciﬁc, one-shot opportunity to trade against the market in a way that is
guaranteed to eventually be signiﬁcantly higher value than the size of the original
investment; and that such opportunities arise inﬁnitely often. In order to use such
a situation to ensure that the market P satisﬁes the property, we will now show that
logical inductors are not susceptible to repeatable methods for making a guaranteed,
substantial proﬁt.
82

To deﬁne a notion of return on investment, we ﬁrst deﬁne the “magnitude” of
a trade made by a trader, so that we can talk about traders that are proﬁtable in
proportion to the size of their trades:
∥T (P)∥mg :=
X
φ∈S
|T [φ](P)|.
This number will be called the magnitude of the trade. It is just the total number
of shares traded by T against the market P, whether the shares are bought or sold.
Note that the magnitude is not the same as the ∥−∥1-norm of T (P); the magnitude
omits the constant term T [1](P).
The magnitude is a simple bound on the value of the holdings Tn(P): for any
world W (plausible or not),

X
φ∈S
Tn[φ](P) · (W(φ) −Pn(φ))

≤
X
φ∈S
Tn[φ](P)
 · 1 = ∥Tn(P)∥mg,
since W(φ) ∈{0, 1} and Pn(φ) ∈[0, 1]. Now we deﬁne the total magnitude of a
trader over time.
Deﬁnition B.1.1 (Magnitude of a Trader). The magnitude ∥T(P)∥mg of a trader
T against the market P is
∥T(P)∥mg :=
X
n∈N+
∥Tn(P)∥mg ≡
X
n∈N+
X
φ∈S
|Tn[φ](P)|.
The magnitude of T is the total number of shares it trades (buys or sells) over
all time.
Now we deﬁne what it means for a trader to increase its net value by a substantial
fraction of its investment, i.e., its magnitude.
Deﬁnition B.1.2 (ε Return on Investment). For ε > 0, we say that a trader T
trading against P has ε return on investment or ε-ROI if, for all W ∈PC(Γ),
lim
n→∞W

X
i≤n
Ti(P)

≥ε∥T(P)∥mg.
In words, a trader T has ε-ROI if, in the limit of time and deduction, the value of
its holdings is, in every world, at least ε times its total investment ∥T(P)∥mg. Note
that this does not merely say that T recoups at least an ε fraction of its original
cost; rather, the net value is guaranteed in all worlds consistent with Γ to have
increased by an ε fraction of the magnitude ∥T(P)∥mg of T’s trades.
Recall from Deﬁnition 4.3.5 that a sequence α of rationals is P-generable if there
is some e.c. EF-progression α† such that α†
n(P) = αn for all n.
Lemma B.1.3 (No Repeatable ε-ROI ). Let P be a logical inductor with respect to
some deductive process D, and let (T
k)k∈N+ be an eﬃciently emulatable sequence of
traders (Deﬁnition A.3.4). Suppose that for some ﬁxed ε > 0, each trader T
k has
ε-ROI. Suppose further that there is some P-generable sequence α such that for all
k,
∥T
k(P)∥mg = αk.
Then
lim
k→∞αk = 0.
In words, this says roughly that there is no eﬃcient, repeatable method for
producing a substantial guaranteed return on an investment. The condition that α
is P-generable will help with the budgeting done by the trader that emulates the
sequence (T
k)k.
83

Proof strategy.
We will construct a trader T that emulates the sequence (T
k) in
a manner such that if the traders T
k did not make trades of vanishing limiting value,
then our trader T would accrue unbounded proﬁt by repeatedly making investments
that are guaranteed to pay out by a substantial amount. Very roughly, on time n,
T will sum together the trades T k
n made by all the T
k with k ≤n. In this way, T
will accrue all the proﬁts made by each of the T
k.
The main problem we have to deal with is that T risks going deeper and deeper
into debt to ﬁnance its investments, as discussed before the proof of Theorem 4.1.1
(Convergence) in Section 6.1. That is, it may be that each T
k makes an investment
that takes a very long time for all the worlds W ∈PC(Dn) plausible at time n to
value highly. In the meanwhile, T continues to spend money buying shares and
taking on risk from selling shares that might plausibly demand a payout. In this
way, despite the fact that each of its investments will eventually become proﬁtable,
T may have holdings with unboundedly negative plausible value.
To remedy this, we will have our trader T keep track of its “debt” and of which
investments have already paid oﬀ, and then scale down new traders T
k so that T
maintains a lower bound on the plausible value of its holdings. Roughly speaking,
T at time n checks whether the current holdings of T
k are guaranteed to have a
positive value in all plausible worlds, for each k ≤n. Then T sums up the total
magnitudes αk of all the trades ever made by those T
k whose trades are not yet
guaranteed to be proﬁtable. This sum is used to scale down all trades made by T
n,
so that the total magnitude of the unsettled investments made by T will remain
bounded.
Proof of Lemma B.1.3.
Proof. We now prove Lemma B.1.3.
We can assume without loss of generality that each αn ≤1 by dividing T
k’s
trades by max(1, αk).
Checking proﬁtability of investments.
At time n, our trader T runs a (pos-
sibly very slow) search process to enumerate traders T
k from the sequence (T
k)k
that have made trades that are already guaranteed to be proﬁtable, as judged by
what is plausible according to the deductive process D with respect to which P is
a logical inductor. That is, T runs a search for pairs of numbers k, m ∈N+ such
that:
X
i≤m
∥T k
i (P)∥mg ≥(1 −ε/3)αk
(few future trades), and
inf
W∈PC(Dm) W
P
i≤m T k
i (P)

≥(2ε/3)αk
(guaranteed proﬁt).
If the trader T
k has few future trades and guaranteed proﬁt at time m then we say
that the trader’s holdings have matured. We denote the least such m by m(k).
The ﬁrst condition (few future trades) says that T
k has made trades of total
magnitude at least (1 −ε/3)αk after time k up until time m. By the assumption
that ∥T
k(P)∥mg = αk, for each k there is some time step m such that this condition
holds. By that same assumption, T
k will make trades of total magnitude at most
(ε/3)αk in all time steps after m.
The second condition (guaranteed proﬁt) says that the minimum value plausible
at time m of all trades made by T
k up until time m is at least (2ε/3)αk. By the
assumption that T
k has ε-ROI, i.e., that the minimum value of T
k is eventually at
least ε∥T k
k ∥mg, the condition of guaranteed proﬁt will hold at some m.
84

The idea is that, since T
k will trade at most (ε/3)αk shares after m and the
holdings of T
k from trades up until the current time have minimum plausible value
at least (2ε/3)αk, it is guaranteed that the holdings of T
k at any time after m will
have minimum plausible value at least (ε/3)αk. This will allow our trader T to
“free up” funds allocated to emulating T
k, in the sense that there is no longer ever
any plausible net downside to the holdings from trades made by T
k.
Deﬁnition of the trader T .
For ﬁxed k and m, these two conditions refer to
speciﬁc individual computations (namely Dm, the T k
i (P) for i ≤m, and αk). On
time step n, for all k, j ≤n, our trader T sets Boolean variables open(k, j) := 0
if it is veriﬁed in j steps of computation that the holdings of T
k have matured;
and open(k, j) := 1 if T
k has open investments. Since the holdings of each T
k will
eventually mature, for all k there is some n such that open(k, n) = 0.
Let α† be an e.c. EF progression such that for each n we have α†
n(P) = αn. Then
T outputs the trading strategy
Tn :=
X
k≤n
β†
k · T k
n,
where the β†
k are deﬁned recursively by
β†
k := 1 −
X
i<k
open(i, k)β†
i α†
i.
That is, the machine computing T outputs the deﬁnitions of the budget variables
β†
k for each k ≤n, and then lists the trades
return φ :=
X
k≤n
β†
kT k
n[φ]
for each φ listed by any of the trades T k
n for k ≤n.
As shorthand, we write
βk := β†
k(P). Notice that since (T
k)k is eﬃciently emulatable, we have ∀k : ∀i < k :
T k
i ≡0, and therefore
∀n : Tn(P) =
X
k∈N+
βkT k
n(P).
Note that each open(i, k) is pre-computed by the machine that outputs our trader
T and then is encoded as a constant in the expressible feature β†
k.
The trade
coordinate Tn[φ] is an expressible feature because the β†
k and T k
n[φ] are expressible
features.
Budgeting the traders T
k.
Since we assumed each αk ≤1, it follows from the
deﬁnition of the budget variable β†
k that
βkαk ≤1 −
X
i<k
open(i, k)βiαi,
and then βk is used as the constant scaling factor for T
k in the sum deﬁning T’s
trades. In this way, we maintain the invariant that for any n,
X
k≤n
open(k, n)βkαk ≤1.
Indeed, by induction on k, using the fact that open(i, n) implies open(i, m) for
m ≥n, we have βk ≥0 and the above invariant holds.
In words, this says that out of all the traders T
k with investments still open at
time n, the sum of the magnitudes βkαk of their total investments (as budgeted by
the βk) is bounded by 1.
85

Analyzing the value of T ’s holdings.
Now we lower bound the value of the
holdings of T from trades against the market P. Fix any time step n and world
W ∈PC(Dn) plausible at time n. Then we have that the value of T’s holdings at
time n is
W(P
i≤n Ti(P)) =
X
k≤n
W(P
i≤n βkT k
i (P))
by linearity and by deﬁnition of our trader T;
=
X
k≤n
open(k,n)
W(P
i≤n βkT k
i (P)) +
X
k≤n
¬open(k,n)
W(P
i≤n βkT k
i (P))
again by linearity. We analyze the ﬁrst term, the value of the holdings that have
not yet matured, as follows:
X
k≤n
open(k,n)
W(P
i≤n βkT k
i (P)) ≥−
X
k≤n
open(k,n)
βk
X
i≤n
∥T k
i (P)∥mg
≥−
X
k≤n
open(k,n)
βk
X
i∈N+
∥T k
i (P)∥mg
= −
X
k≤n
open(k, n)βkαk
≥−1,
by the previous discussion of the βk.
In short, the βk were chosen so that the
total magnitude of all of T’s holdings from trades made by any T
k that haven’t yet
matured stays at most 1, so that its plausible value stays at least −1.
Now we analyze the second term in the value of T’s holdings, representing the
value of the holdings that have already matured, as follows:
X
k≤n
¬open(k,n)
W(P
i≤n βkT k
i (P))
=
X
k≤n
¬open(k,n)

W(P
i≤m(k) βkT k
i (P)) + W(P
m(k)<i≤n βkT k
i (P))

where m(k) is minimal such that T
k has guaranteed proﬁt and makes few future
trades at time m(k), as deﬁned above;
≥
X
k≤n
¬open(k,n)

βk(2ε/3)αk −
X
i>m(k)
βk∥T k
i (P)∥mg


since by deﬁnition of m(k) and the guaranteed proﬁt condition, the value of the
holdings of T
k from its trades up until time m(k) is at least (2ε/3)αk in any world
in Dn;
≥
X
k≤n
¬open(k,n)
(βk(2ε/3)αk −βk(ε/3)αk)
since T
k is guaranteed to make trades of magnitude at most (ε/3)αk after time
m(k);
=
X
k≤n
¬open(k,n)
βk(ε/3)αk.
86

Completing our analysis, we have a lower bound on the value in W of the holdings
of T at time n:
W(P
i≤n Ti(P)) ≥−1 +
X
k≤n
¬open(k,n)
βk(ε/3)αk.
T exploits P unless α vanishes.
Since T is an eﬃcient trader and P is a logical
inductor, T does not exploit P. That is, the set
n
W
P
i≤n Ti(P)
  n ∈N+, W ∈PC(Dn)
o
is bounded above, since it is bounded below by −1 by the above analysis. In words,
the plausible value of T’s holdings is always at least −1, so by the logical induction
criterion it cannot go to inﬁnity. Therefore, again by the above analysis, we must
have
lim
n→∞
X
k≤n
¬open(k,n)
βk(ε/3)αk < ∞.
As shown above, for any k the conditions for ¬open(k, n) will eventually be met by
all suﬃciently large n. Thus
lim
n→∞
X
k≤n
¬open(k,n)
βk(ε/3)αk =
X
k
(ε/3)βkαk < ∞.
Now we show that limk→∞αk = 0. Suppose by way of contradiction that for some
δ ∈(0, 1), αk > δ for inﬁnitely many k, but nevertheless for some suﬃciently large
time step n, we have
X
i>n
βiαi < 1/2.
Recall that for each i ≤n, at some time n(i), open(i, n(i)) = 0 veriﬁes that the
holdings of T
i have matured. Let N be any number greater than n(i) for all i ≤n.
Then
X
i<N
open(i, N)βiαi =
X
i≤n
0 · βiαi +
X
n<i<N
open(i, N)βiαi
≤0 +
X
n<i<N
βiαi
≤1/2.
So for inﬁnitely many suﬃciently large k we have
αkβk = αk
 
1 −
X
i<k
open(i, k)βiαi
!
≥αk(1 −1/2)
≥δ/2.
Thus
X
k
(ε/3)βkαk = ∞,
contradicting that this sum is bounded. Therefore in fact αk ≂k 0, as desired.
87

B.2
Aﬃne Preemptive Learning
Theorem 4.5.7 (Aﬃne Preemptive Learning). Let A ∈BCS(P). Then
lim inf
n→∞Pn(An) = lim inf
n→∞sup
m≥n
Pm(An)
and
lim sup
n→∞Pn(An) = lim sup
n→∞
inf
m≥n Pm(An) .
Proof strategy: buying combinations that will appreciate, and ROI.
The
inequality
lim inf
n→∞Pn(An) ≥lim inf
n→∞sup
m≥n
Pm(An)
states roughly that Pn cannot inﬁnitely often underprice the R-combination An by
a substantial amount in comparison to any price Pm(An) assigned to An by Pm at
any future time m ≥n.
Intuitively, if the market P did not satisfy this inequality, then P would be
exploitable by a trader that buys the R-combination An when its price is low, and
then sells it back when, inevitably, the price is substantially higher. If we have sold
back all our shares in some sentence φ, then there is no contribution, positive or
negative, to our net value from our φ-shares (as opposed to their prices); for every
share we owe, there is a matching share that we hold. So if we buy low and sell
high, we have made a proﬁt oﬀof the price diﬀerential, and once the inter-temporal
arbitrage is complete we have not taken on any net risk from our stock holdings.
The fact that we can accrue stock holdings that we are guaranteed to eventually
sell back for more than their purchase price is not suﬃcient to exploit the market.
It may be the case that at every time n we spend $1 on some R-combination that
we eventually sell back at $2, but not until time 4n. (That is, until time 4n, the
price remains low.) Then at every time n we owe −$n in cash, but only have around
$2(n/4) worth of cash from shares we have sold oﬀ, for a net value of around −n/2.
Thus we have net value unbounded below and hence do not exploit the market,
despite the fact that each individual investment we make is eventually guaranteed
to be proﬁtable.
To avoid this obstacle, we will apply the ε-return on investment lemma
(Lemma B.1.3) to the sequence of traders (T
k)k that enforce the inequality at time
k as described above. That is, T
k myopically “keeps P sensible about A” at time
k by buying the R-combination Ak described above if that R-combination is under-
priced at time k, and otherwise T
k does nothing. The ROI Lemma guarantees that
the inequality cannot inﬁnitely often fail substantially, or else this sequence would
have δ-ROI for some δ.
The main technical diﬃculty is that we have to buy the R-combination An
at time n (if it is underpriced), wait for the price of the combination to increase
substantially, and then sell it oﬀ, possibly over multiple time steps. The traders T
k
will therefore have to track what fraction of their initial investment they have sold
oﬀat any given time.
Proof.
Proof. We show the ﬁrst equality; the second equality follows from the ﬁrst by
considering the negated sequence (−An)n.
Since for all n we have supm≥n Pm(An) ≥Pn(An), the corresponding inequality
in the limit inﬁmum is immediate.
Suppose for contradiction that the other inequality doesn’t hold, so that
lim inf
n→∞Pn(An) < lim inf
n→∞sup
m≥n
Pm(An) .
88

Then there are rational numbers ε > 0 and b such that we have
lim inf
n→∞Pn(An) < b −ε < b + ε < lim inf
n→∞sup
m≥n
Pm(An) .
Therefore we can ﬁx some suﬃciently large sε such that:
• for all n > sε, we have supm≥n Pm(An) > b + ε, and
• for inﬁnitely many n > sε, we have Pn(An) < b −ε.
We will assume without loss of generality that each ∥An∥mg ≤1; they are
assumed to be bounded, so they can be scaled down appropriately.
An eﬃciently emulatable sequence of traders.
Let A† be an EF-combination
progression such that A†
n(P) = An for all n. We now deﬁne our sequence of traders
(T
k)k. For k ≤sε, deﬁne T
k to be the zero trading strategy at all times n.
For k > sε, deﬁne T k
n to be the zero trading strategy for n < k, and deﬁne T k
k
to be the trading strategy
T k
k := Underk ·

A†
k −A†∗k
k

,
where
Underk := Indε/2

A†∗k
k
< b −ε/2

.
This is a buy order for the R-combination Ak, scaled down by the continuous indi-
cator function Underk for the event that Pk has underpriced that R-combination at
time k. Then, for times n > k, we deﬁne T
k to submit the trading strategy
T k
n := −Fn ·

Underk ·

A†
k −A†∗k
k

,
where we deﬁne Fn ≥0 recursively in the previous fractions Fi:
Fn := Overk
n ·
 
1 −
X
k<i<n
Fi
!
,
using the continuous indicator Overk
n
:=
Indε/2

A†∗n
k
> b + ε/2

of the R-
combination being overpriced at time n.
In words, T k
n is a sell order for the R-combination Ak, scaled down by the fraction
Underk of this R-combination that T
k purchased at time k, and also scaled down
by the fraction Fn of the original purchase T k
k that will be sold on this time step.
That is, P
k<i<n Fi the total fraction of the original purchase Underk · Ak that has
already been sold oﬀon all previous rounds since time k. Then T k
n sells oﬀthe
remaining fraction 1 −P
k<i<n Fi of the R-combination Underk · Ak, scaled down
by the extent Overk
n to which Ak is overpriced at time n.
Notice that since Overk
i
∈[0, 1] for all i, by induction on n we have that
P
k<i≤n Fi ≤1 and Fn ≥0. This justiﬁes thinking of the Fi as portions of the
original purchase being sold oﬀ.
By assumption, the EF-combination progression A† is e.c. Also, each trader T
k
does not trade before time k. Therefore the sequence of traders (T
k)k is eﬃciently
emulatable (see A.2 on dynamic programming). (The constant sε before which the
T
k≤sε make no trades can be hard-coded in the eﬃcient enumeration.)
89

(ε/2) return on investment for T
k.
Now we show that each T
k has (ε/2)-ROI;
i.e., for all W ∈PC(Γ),
lim
n→∞W
P
i≤n T k
i (P)

≥(ε/2)∥T
k(P)∥mg.
In words, this says that the trades made by T
k across all time are valued positively
in any W ∈PC(Γ), by a ﬁxed fraction (ε/2) of the magnitude of T
k. For k ≤sε,
this is immediate since ∥T
k(P)∥mg = 0 by deﬁnition.
For each k > sε, by deﬁnition T
k makes a trade of magnitude ∥T
k
k(P)∥mg =
Underk(P) · ∥Ak∥mg, followed by trades of magnitude
X
n>k
Fn∥T
k
k(P)∥mg ≤∥T
k
k(P)∥mg ,
by the earlier comment that the Fn are non-negative and sum to at most 1. Further-
more, by assumption, there is some m > k such that Pm(Ak) > b+ε. At that point,
Overk
m(P) = 1, so that Fm(P) =
 1 −P
k<i<m Fi(P)

; intuitively this implies that
at time m, T
k will sell oﬀthe last of its stock holdings from trades in Ak. Formally
we have
X
k<i≤m
∥T
k
i (P)∥mg =
 
Fm(P) +
X
k<i<m
Fi(P)
!
· Underk(P) · ∥Ak∥mg
= ∥T
k
k(P)∥mg .
Furthermore, for all times M > m we have FM(P) = 0, so that T k
M(P) ≡0. There-
fore ∥T
k(P)∥mg = P
k≤i≤n ∥T
k
i (P)∥mg = 2∥T
k
k(P)∥mg.
Now ﬁx any world W ∈PC(Γ). Then the limiting value of T
k in W is:
lim
n→∞W
P
i≤n T k
i (P)

= W
P
k≤i≤m T k
i (P)

since by the above analysis, T k
i is the zero trade for i < k and for i > m;
= W

T k
k (P) + P
k<i≤m T k
i (P)

=
Underk(P) · W ( Ak −Pk(Ak) )
+ Underk(P) · W
P
k<i≤m(−Fi(P)) · ( Ak −Pi(Ak) )

by linearity, by the deﬁnition of the trader T
k, and since by deﬁnition A†∗k
k
(P) =
Pk(A†
k(P)) = Pk(Ak). Note that the prices Pi(Ak) of Ak in the summation change
with the time step i. Then
=
Underk(P) · W ( Ak −Ak )
+ Underk(P) · W

−Pk(Ak) + P
k<i≤m Fi · Pi(Ak)

lim
n→∞W
P
i≤n T k
i (P)

= Underk(P) ·

−Pk(Ak) + P
k<i≤m Fi · Pi(Ak)

,
using linearity to rearrange the terms in the ﬁrst and second lines, and using that
P
k<i≤m Fi = 1 as shown above. Note that this last quantity does not contain any
stock holdings whose value depends on the world; intuitively this is because T
k
sold oﬀexactly all of its initial purchase. The remaining quantity is the diﬀerence
between the price at which the R-combination Ak was bought and the prices at
which it was sold over time, scaled down by the fraction Underk(P) of Ak that T
k
purchased at time k.
90

If at time step k the R-combination Ak was not underpriced, i.e., Underk(P) = 0,
then
lim
n→∞W
P
i≤n T k
i (P)

= 0 = (ε/2)∥T
k(P)∥mg ,
as desired. On the other hand, suppose that Underk(P) > 0. That is,
Indε/2 (Pk(Ak) < b −ε/2) > 0 ,
i.e., Ak was actually underpriced at time k. Therefore
lim
n→∞W
P
i≤n T k
i (P)

≥Underk(P) ·

−(b −ε/2) + P
k<i≤m Fi(P) · (b + ε/2)

since when Fi(P) > 0 we have Overk
n(P) > 0 and hence Pi(Ak) ≥b + ε/2, and using
the fact that Underk(P) is nonnegative;
= Underk(P) · ε
≥(ε/2)∥T
k(P)∥mg ,
since
∥T
k(P)∥mg = 2∥T
k
k(P)∥mg = 2 · Underk(P) · ∥Ak∥mg ≤2 · Underk(P)
Thus T
k has (ε/2)-ROI.
Deriving a contradiction.
We have shown that the sequence of traders (T
k)k is
bounded, eﬃciently emulatable, and has (ε/2)-return on investment. The remaining
condition to Lemma B.1.3 states that for all k, the magnitude ∥T
k(P)∥mg of all
trades made by T
k must equal αk for some P-generable αk.
This condition is
satisﬁed for αk := 2∥T
k
k(P)∥mg, since as shown above, ∥T
k(P)∥mg = 2∥T
k
k(P)∥mg.
Therefore we can apply Lemma B.1.3 (the ROI lemma) to the sequence of traders
(T
k)k. We conclude that αk ≂k 0. Recall that we supposed by way of contradiction
that the R-combinations in Ak are underpriced inﬁnitely often. That is, for inﬁnitely
many days k, Pk(Ak) < b −ε. But for any such k > sε, T
k
k purchases a full R-
combination Ak, and then sells oﬀthe resulting stock holdings for at least b + ε/2,
at which point T
k has proﬁted by at least ε. More precisely, for these k we have
Underk(P) = Indε/2 (Pk(Ak) < b −ε/2) = 1.
Since
αk = 2∥T
k
k(P)∥mg = 2 · Underk(P) · ∥Ak∥mg = 2∥Ak∥mg
and ∥Ak∥mg ≥ε/2 (since Pm(Ak) −Pk(Ak) ≥ε for some m), we have αk ≥ε for
inﬁnitely many k, which contradicts αk ≂k 0.
B.3
Preemptive Learning
Theorem 4.2.4 (Preemptive Learning). Let φ be an e.c. sequence of sentences.
Then
lim inf
n→∞Pn(φn) = lim inf
n→∞sup
m≥n
Pm(φn).
Furthermore,
lim sup
n→∞Pn(φn) = lim sup
n→∞
inf
m≥n Pm(φn).
Proof. This is a special case of Theorem 4.5.7 (Aﬃne Preemptive Learning), using
the combination An := φn.
91

B.4
Convergence
Theorem 4.1.1 (Convergence). The limit P∞: S →[0, 1] deﬁned by
P∞(φ) := lim
n→∞Pn(φ)
exists for all φ.
Proof. By Theorem 4.2.4 (Preemptive Learning),
lim inf
n→∞Pn(φ) = lim inf
n→∞sup
m≥n
Pm(φ)
= lim
k→∞inf
n≥k sup
m≥n
Pm(φ)
= lim sup
n→∞Pn(φ).
Since the lim inf and lim sup of Pn(φ) are equal, the limit exists.
B.5
Persistence of Aﬃne Knowledge
Let A ∈BCS(P). Then
lim inf
n→∞
inf
m≥n Pm(An) = lim inf
n→∞P∞(An)
and
lim sup
n→∞
sup
m≥n
Pm(An) = lim sup
n→∞P∞(An).
Proof strategy: keeping Pm reasonable on all AFn≤m.
In the same vein
as the proof in Appendix B.2 of Theorem 4.5.7, the inequality
lim inf
n→∞
inf
m≥n Pm(An) ≥lim inf
n→∞P∞(An)
says roughly that Pm cannot underprice the R-combination An by a substantial
amount inﬁnitely often, where the R-combination is “underpriced” in comparison
to the value of the R-combination as judged by the limiting belief state P∞.
As the proof of the present theorem is quite similar to the proof of Theorem 4.5.7,
we will highlight the diﬀerences in this proof, and otherwise give a relatively terse
proof.
Intuitively, if the market P did not satisfy the present inequality then P would
be exploitable by a trader that buys An at any time m such that its price Pm(An)
is lower than its eventual price, and then sells back the R-combination when the
price rises.
We would like to apply the return on investment lemma (Lemma B.1.3) as in
Theorem 4.5.7. One natural attempt is to have, for each n, a trader for that watches
the price of An at all times m ≥n, buying low and selling high. This proof strategy
may be feasible, but does not follow straightforwardly from the ROI lemma: those
traders may be required to make multiple purchases of An in order to guard against
their prices ever dipping too low. This pattern of trading may violate the condition
for applying the ROI lemma that requires traders to have a total trading volume
that is predictable by a P-generable EF-progression (in order to enable veriﬁable
budgeting).
Thus we ﬁnd it easier to index our traders by the time rather than by An. That
is, we will deﬁne a sequence of traders (T
k)k, where the trader T
k ensures that Pk
does not assign too low a price to any An for n ≤k. Speciﬁcally, T
k at time k buys
any R-combination An for n ≤k with Pk(An) suﬃciently low, and then sells back
each such purchase as the price Pm(An) rises. In this way, if R-combinations are
ever underpriced at any time above the main diagonal, there is a trader ready to
buy that R-combination in full.
92

Proof.
Proof. We show the ﬁrst equality; the second equality follows from the ﬁrst by
considering the negated progression (−An)n.
For every n, since P∞is the limit of the Pm and since V(An) is continuous as
a function of the valuation V, we have that infm≥n Pm(An) ≤P∞(An). Therefore
the corresponding inequality in the limit inﬁmum is immediate.
Suppose for contradiction that the other inequality doesn’t hold, so that
lim inf
n→∞
inf
m≥n Pm(An) < lim inf
n→∞P∞(An).
Then there are rational numbers ε > 0 and b such that we have
lim inf
n→∞
inf
m≥n Pm(An) < b −ε < b + ε < lim inf
n→∞P∞(An) ,
and therefore we can ﬁx some suﬃciently large sε such that
• for all n > sε, we have P∞(An) > b + ε, and
• for inﬁnitely many n > sε, we have infm≥n Pm(An) < b −ε.
We will assume without loss of generality that each ∥An∥mg ≤1; they are
assumed to be bounded, so they can be scaled down appropriately.
An eﬃciently emulatable sequence of traders.
Now we deﬁne our sequence
of traders (T
k)k. Let A† be an EF-combination progression such that A†n(P) = An
for all n. For n < k, deﬁne T k
n to be the zero trading strategy. Deﬁne T k
k to be the
trading strategy
T k
k :=
X
sε<n≤k
 Undern
k ·
 A†
n −A†∗k
n

,
where
Undern
k := Indε/2
 A†∗k
n
< b −ε/2

.
This is a buy order for each R-combination An for sε < n ≤k, scaled down by the
continuous indicator function Undern
k for the event that An is underpriced at time
k by P. Then, for time steps m > k, we deﬁne T k
m to be the trading strategy
T k
m := Fm ·
X
sε<n≤k
 −Undern
k ·
 A†
n −A†∗m
n

,
where
Fm :=
 
1 −
X
k<i<m
Fi
!
·
Y
sε<n≤k
Overn
m
and
Overn
m := Indε/2
 A†∗m
n
> b + ε/2

.
This trade is a sell order for the entire R-combination comprising the sum of the
scaled R-combinations Undern
k(P) · An for sε < n ≤k purchased at time k by T k
k ,
scaled down by the fraction Fm(P). We deﬁne Fm so that it represents the fraction
of the original purchase made by the trader T
k that has not yet been sold oﬀby
time m, scaled down by the continuous indicator Q
sε<n≤k Overn
m for the event that
all of those R-combinations An for sε < n ≤k are overpriced at time m.
By assumption, the EF-combination progression A† is e.c., and each trader T
k
does not trade before time k. Therefore the sequence of traders (T
k)k is eﬃciently
emulatable (see A.2 on dynamic programming).
93

(ε/2) return on investment for T
k.
Now we show that each T
k has (ε/2)-ROI,
i.e., for all W ∈PC(Γ):
lim
n→∞W
P
i≤n T k
i (P)

≥(ε/2)∥T
k(P)∥mg .
Roughly speaking, T
k gets (ε/2)-ROI for the same reason as the traders in the
proof of Theorem 4.5.7: the stock holdings from each An that T
k purchased will
be sold oﬀfor at least (ε/2)-ROI, so the sum of the R-combinations is sold oﬀfor
(ε/2)-ROI.
Since Overn
i (P) ∈[0, 1] for all n and i, by induction on m we have that
P
k<i≤m Fi(P) ≤1 and Fm(P) ≥0. Therefore for each k > sε, by deﬁnition T
k
makes a trade of magnitude ∥T
k
k(P)∥mg = P
sε<n≤k Undern
k(P) · ∥An∥mg, followed
by trades of magnitude
X
n>k
Fn(P)∥T
k
k(P)∥mg ≤∥T
k
k(P)∥mg .
By assumption, there is some time m such that Pm(An) > b+ε/2 for all sε < n ≤k.
At that point, Overn
m(P) = 1 for each such n, so that Fn(P) =
 1 −P
k<i<n Fi(P)

.
Then at time m, T
k will sell of the last of its stock holdings from trades in An, so
that P
k<i≤m ∥T
k
i (P)∥mg is equal to
 
Fm(P) +
X
k<i<m
Fi(P)
!
·
X
sε<n≤k
Undern
k(P) · ∥An∥mg = ∥T
k
k(P)∥mg .
Furthermore, for all times M > m we have FM(P) = 0, so that T k
M(P) ≡0. There-
fore ∥T
k(P)∥mg = P
k≤i≤n ∥T
k
i (P)∥mg = 2∥T
k
k(P)∥mg.
From this point, the proof of return on investment is essentially identical to the
analogous proof of Theorem 4.5.7. The only diﬀerence is that here the trader T
k
holds a combination of R-combinations. Therefore we will not belabor the details;
inserting a summation P
sε<n≤k in front of the trades made by the traders in the
proof of Theorem 4.5.7 will produce the precise derivation.
In short, since T
k will eventually hold no net shares, the value of its holdings is
determined by the prices of the shares it trades, regardless of plausible worlds. By
deﬁnition, T
k purchases a mixture of R-combinations
X
sε<n≤k
 Undern
k(P) · An

,
where each An with Undern
k(P) > 0 has price Pk(An) at most b −ε/2 at time k.
Then T
k sells oﬀthat mixture, at times for which each R-combination has price at
least b + ε/2. Thus T
k eventually has holdings with value at least
X
sε<n≤k
 Undern
k(P) · (b + ε/2 −(b −ε/2))

=
X
sε<n≤k
 Undern
k(P) · ε

≥ε
X
sε<n≤k
 Undern
k(P) · |An|

≥ε∥T k
k (P)∥mg
= (ε/2)∥T
k(P)∥mg .
Thus, T
k has (ε/2)-ROI.
94

Deriving a contradiction.
We have shown that the sequence of traders (T
k)k is
bounded, eﬃciently emulatable, and has (ε/2)-return on investment. The remaining
condition to Lemma B.1.3 states that for all k, the magnitude ∥T
k(P)∥mg of all
trades made by T
k must equal αk for some P-generable αk.
This condition is
satisﬁed for αk := 2∥T
k
k(P)∥mg, since as shown above, ∥T
k(P)∥mg = 2∥T
k
k(P)∥mg.
Therefore we can apply Lemma B.1.3 (the ROI lemma) to the sequence of traders
(T
k)k. We conclude that αk ≂k 0. Recall that we supposed by way of contradiction
that inﬁnitely often, some An is underpriced. That is, for inﬁnitely many times k
and indices sε < n ≤k, Pk(An) < b −ε.
But for any such k and n, T k
k will purchase the full R-combination An, as
Undern
k(P) = Indε/2 (Pk(An) < b −ε/2) = 1 .
Now αk = 2∥T
k
k∥mg ≥2 · Undern
k∥An∥mg = 2∥An∥mg, and ∥An∥mg ≥ε/2 (since
Pm(An) −Pk(An) ≥ε for some m).
So αk ≥ε inﬁnitely often, contradicting
αk ≂k 0.
B.6
Persistence of Knowledge
Theorem 4.2.3 (Persistence of Knowledge). Let φ be an e.c. sequence of sentences,
and p be an e.c. sequence of rational-number probabilities. If P∞(φn) ≂n pn, then
sup
m≥n
|Pm(φn) −pn| ≂n 0.
Furthermore, if P∞(φn) ≲n pn, then
sup
m≥n
Pm(φn) ≲n pn,
and if P∞(φn) ≳n pn, then
inf
m≥n Pm(φn) ≳n pn.
Proof. The second and third statements are a special case of Theorem 4.5.6 (Per-
sistence of Aﬃne Knowledge), using the combination An := φn; the ﬁrst statement
follows from the second and third.
C
Coherence Proofs
C.1
Aﬃne Coherence
Theorem 4.5.5 (Aﬃne Coherence). Let A ∈BCS(P). Then
lim inf
n→∞
inf
W∈PC(Γ) W(An) ≤lim inf
n→∞P∞(An) ≤lim inf
n→∞Pn(An),
and
lim sup
n→∞Pn(An) ≤lim sup
n→∞P∞(An) ≤lim sup
n→∞
sup
W∈PC(Γ)
W(An).
Proof. We show the ﬁrst series of inequalities; the second series follows from the
ﬁrst by considering the negated progression (−An)n. Let A† be an EF-combination
progression such that A†n(P) = An for all n.
95

Connecting PC(Γ) to P∞.
First we show that
lim inf
n→∞
inf
W∈PC(Γ) W(An) ≤lim inf
n→∞P∞(An).
It suﬃces to show the stronger statement that for any n ∈N+,
inf
W∈PC(Γ) W(An) ≤P∞(An).
This is a generalization of coherence in the limit to aﬃne relationships; its proof will
follow a strategy essentially identical to the one used in the proof of Theorem 4.1
(coherence) to show the particular coherence relationships that are suﬃcient to
imply ordinary probabilistic coherence. That is, we will construct a trader that
waits for the coherence relationship to approximately hold (so to speak) and for
the price of the corresponding R-combination to approximately converge, and then
buys the combination repeatedly if it is underpriced.
Suppose by way of contradiction that the inequality does not hold, so for some
ﬁxed n there are rational numbers ε > 0 and b and a time step sε such that for all
m > sε we have
Pm(An) < b −ε < b + ε <
inf
W∈PC(Dm) W(An) .
Therefore we can deﬁne a trader T that waits until time sε, and thereafter buys
a full R-combination An on every time step. That is, we take Tm to be the zero
trading strategy for m ≤sε, and we deﬁne Tm for m > sε to be
Tm := A†
n −A†∗m
n
.
Intuitively, since the inﬁmum over plausible worlds of the value of the stocks in this
R-combination is already substantially higher than its price, the value of the total
holdings of our trader T immediately increases by at least 2ε. More formally, we
have that for any time m and any W ∈PC(Dm),
W

X
i≤m
Ti(P)

= W

X
sε<i≤m
Ti(P)


since Tm ≡0 for m ≤sε;
=
X
sε<i≤m
W(An) −Pi(An)
by linearity, by deﬁnition of Ti, and since A†
n(P) ≡An and A†∗m
n
(P) ≡Pm(An);
≥
X
sε<i≤m
b + ε −(b −ε)
= 2ε(m −sε).
This is bounded below by 0 and unbounded above as m goes to ∞. Thus T exploits
the market P, contradicting that P is a logical inductor. Therefore in fact we must
have
lim inf
n→∞
inf
W∈PC(Γ) W(An(P)) ≤lim inf
n→∞P∞(An(P)),
as desired.
Connecting Pn to P∞and to fast diagonals.
Now we show that
lim inf
n→∞P∞(An) ≤lim inf
n→∞Pn(An).
96

This says, roughly speaking, that aﬃne relationships that hold in the limiting belief
state P∞also hold along the main diagonal. We show this inequality in two steps.
First, by Theorem 4.5.6 (Persistence of aﬃne knowledege), we have
lim inf
n→∞P∞(An) ≤lim inf
n→∞
inf
m≥n Pm(An).
This says roughly that if the limiting beliefs end up satisfying some sequence of
aﬃne relationships, then eventually all belief states above the main diagonal satisfy
that relationship to at least the same extent. Second, it is immediate that
lim inf
n→∞
inf
m≥n Pm(An) ≤lim inf
n→∞Pn(An),
since for all n, infm≥n Pm(Am) ≤Pn(An). Thus we have the desired inequality.
C.2
Aﬃne Provability Induction
Theorem 4.5.4 (Aﬃne Provability Induction). Let A ∈BCS(P) and b ∈R. If, for
all consistent worlds W ∈PC(Γ) and all n ∈N+, it is the case that W(An) ≥b,
then
Pn(An) ≳n b,
and similarly for = and ≂n, and for ≤and ≲n.
Proof. We prove the statement in the case of ≥; the case of ≤is analogous, and the
case of = follows from the conjunction of the other two cases. By Theorem 4.5.5
(Aﬃne Coherence),
lim inf
n→∞Pn(An) ≥lim inf
n→∞
inf
W∈PC(Γ) W(An) ≥b.
We will usually apply this theorem using the = case.
C.3
Provability Induction
Theorem 4.2.1 (Provability Induction). Let φ be an e.c. sequence of theorems.
Then
Pn(φn) ≂n 1.
Furthermore, let ψ be an e.c. sequence of disprovable sentences. Then
Pn(ψn) ≂n 0.
Proof. Since φ is a sequence of theorems, for all n and W ∈PC(Γ), W(φn) = 1. So
by Theorem 4.5.4 (Aﬃne Provability Induction),
Pn(φn) ≂n 1.
Similarly, since ψ is a sequence of disprovable sentences, for all n and W ∈PC(Γ),
W(ψn) = 0. So by Theorem 4.5.4 (Aﬃne Provability Induction),
Pn(ψn) ≂n 0.
C.4
Belief in Finitistic Consistency
Theorem 4.9.2 (Belief in Finitistic Consistency). Let f be any computable func-
tion. Then
Pn(Con(Γ)(“f(n)”)) ≂n 1.
Proof. Since each statement Con(Γ)(“f(n)”) is computable and true, and Γ can
represent computable functions, each of these statements is provable in Γ. Now
apply Theorem 4.2.1 (Provability Induction) to get the desired property.
97

C.5
Belief in the Consistency of a Stronger Theory
Theorem 4.9.3 (Belief in the Consistency of a Stronger Theory). Let Γ′ be any
recursively axiomatizable consistent theory. Then
Pn(Con(Γ′)(“f(n)”)) ≂n 1.
Proof. Since each statement Con(Γ′)(“f(n)”) is computable and true, and Γ can
represent computable functions, each of these statements is provable in Γ. Now
apply Theorem 4.2.1 (Provability Induction) to get the desired property.
C.6
Disbelief in Inconsistent Theories
Theorem 4.9.4 (Disbelief in Inconsistent Theories). Let Γ′ be an e.c. sequence of
recursively axiomatizable inconsistent theories. Then
Pn(“Γ′
n is inconsistent”) ≂n 1,
so
Pn(“Γ′
n is consistent”) ≂n 0.
Proof. Since each statement “Γ′
n is inconsistent” is provable in PA, and Γ can rep-
resent computable functions, each of these statements is provable in Γ. Now apply
Theorem 4.2.1 (Provability Induction) to get the ﬁrst desired property.
Similarly, since each statement “Γ′
n is consistent” is disprovable in PA, and Γ can
represent computable functions, each of these statements is disprovable in Γ. Now
apply Theorem 4.2.1 (Provability Induction) to get the second desired property.
C.7
Learning of Halting Patterns
Theorem 4.10.1 (Learning of Halting Patterns). Let m be an e.c. sequence of
Turing machines, and x be an e.c. sequence of bitstrings, such that mn halts on
input xn for all n. Then
Pn(“mn halts on input xn”) ≂n 1.
Proof. Since each statement “mn halts on input xn” is computable and true, and Γ
can represent computable functions, each of these statements is provable in Γ. Now
apply Theorem 4.2.1 (Provability Induction) to get the desired property.
C.8
Learning of Provable Non-Halting Patterns
Theorem 4.10.2 (Learning of Provable Non-Halting Patterns). Let q be an e.c.
sequence of Turing machines, and y be an e.c. sequence of bitstrings, such that qn
provably fails to halt on input yn for all n. Then
Pn(“qn halts on input yn”) ≂n 0.
Proof. Each statement “qn halts on input yn” is disprovable in Γ. Now apply The-
orem 4.2.1 (Provability Induction) to get the desired property.
C.9
Learning not to Anticipate Halting
Theorem 4.10.3 (Learning not to Anticipate Halting). Let q be an e.c. sequence
of Turing machines, and let y be an e.c. sequence of bitstrings, such that qn does
not halt on input yn for any n. Let f be any computable function. Then
Pn(“qn halts on input yn within f(n) steps”) ≂n 0.
Proof. Since
each
statement
“qn halts on input yn within f(n) steps”
is
com-
putable and false, and Γ can represent computable functions, each of these state-
ments is disprovable in Γ. Now apply Theorem 4.2.1 (Provability Induction) to get
the desired property.
98

C.10
Limit Coherence
Theorem 4.1.2 (Limit Coherence). P∞is coherent, i.e., it gives rise to an inter-
nally consistent probability measure Pr on the set PC(Γ) of all worlds consistent
with Γ, deﬁned by the formula
Pr(W(φ) = 1) := P∞(φ).
In particular, if Γ contains the axioms of ﬁrst-order logic, then P∞deﬁnes a prob-
ability measure on the set of ﬁrst-order completions of Γ.
Proof. The limit P∞(φ) exists by Theorem 4.1.1 (Convergence), so P∞is well-
deﬁned. Gaifman (1964) shows that P∞deﬁnes a probability measure over PC(Γ)
so long as the following three implications hold for all sentences φ and ψ:
• If Γ ⊢φ, then P∞(φ) = 1,
• If Γ ⊢¬φ, then P∞(φ) = 0,
• If Γ ⊢¬(φ ∧ψ), then P∞(φ ∨ψ) = P∞(φ) + P∞(ψ).
Let us demonstrate each of these three properties.
• Assume that Γ ⊢φ. By Theorem 4.2.1 (Provability Induction), P∞(φ) = 1.
• Assume that Γ ⊢¬φ. By Theorem 4.2.1 (Provability Induction), P∞(φ) = 0.
• Assume that Γ ⊢¬(φ∧ψ). For all W ∈PC(Γ), W(φ∨ψ) = W(φ)+W(ψ). So
by Theorem 4.5.4 (Aﬃne Provability Induction), P∞(φ∨ψ) = P∞(φ)+P∞(ψ).
C.11
Learning Exclusive-Exhaustive Relationships
Theorem 4.5.1 (Learning Exclusive-Exhaustive Relationships). Let φ1, . . . , φk be
k e.c. sequences of sentences, such that for all n, Γ proves that φ1
n, . . . , φk
n are
exclusive and exhaustive (i.e. exactly one of them is true). Then
Pn(φ1
n) + · · · + Pn(φk
n) ≂n 1.
Proof. Deﬁne An := φ1
n + · · · + φk
n. Note that for all W ∈PC(Γ), W(An) = 1.
So by Theorem 4.5.4 (Aﬃne Provability Induction) and linearity,
Pn(φ1
n) + · · · + Pn(φk
n) = Pn(φ1
n + · · · + φk
n) = Pn(An) ≂n 1.
D
Statistical Proofs
D.1
Aﬃne Recurring Unbiasedness
Theorem 4.5.9 (Aﬃne Recurring Unbiasedness). If A ∈BCS(P) is determined
via Γ, and w is a P-generable divergent weighting,
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
Proof. Deﬁne
Biasn :=
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
.
Our proof consists of three steps:
99

1. Proving lim supn→∞Biasn ≥0.
2. Noting that the ﬁrst argument can be applied to the sequence (−An)n to
prove lim infn→∞Biasn ≤0.
3. Proving that, given these facts, (Biasn)n has 0 as a limit point.
The ﬁrst step will be deferred. The second step is trivial. We will now show that
the third step works given that the previous two do:
Let a := lim infn→∞Biasn ≤0 and b := lim supn Biasn ≥0.
If a = 0 or
b = 0 then of course 0 is a limit point.
Otherwise, let a < 0 < b. If 0 is not
a limit point of (Biasn)n, then there are ε > 0 and N ∈N such that ∀n > N :
Biasn /∈(−ε, ε) ⊆(a, b).
Choose M > N such that BiasM ∈(ε, b] and for all
n > M, Biasn −Biasn+1 < ε; suﬃciently late adjacent terms are close because
P
i≤n wi goes to ∞and the absolute diﬀerence between successive numerators is at
most 1. Then (Biasn)n>M must remain positive (it cannot cross the 2ε-wide gap),
contradicting that a is also a limit point and a < 0.
At this point we have shown that the second and third steps follow from the
ﬁrst step, so we need only show the ﬁrst step: lim supn→∞Biasn ≥0. Suppose this
is not the case. Then there is some natural N and rational ε ∈(0, 1) such that for
all n ≥N,
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
< −2ε
or equivalently,
X
i≤n
wi · (ValΓ(Ai) −Pi(Ai)) > 2ε
X
i≤n
wi.
An eﬃciently emulatable sequence of traders.
We will consider an inﬁnite
sequence of traders, each of which will buy a “run” of R-combinations, and which
will have ε-ROI. Then we will apply Lemma B.1.3 to derive a contradiction.
Without loss of generality, assume each ∥An∥mg ≤1; they are uniformly bounded
and can be scaled down without changing the theorem statement. Let w† be an
e.c. EF progression such that w†
n(P) = wn.
Let A† be an e.c. EF-combination
progression such that A†
n(P) = An. Let A† be equal to
A†
n = c + ξ1φ1 + · · · + ξl(n)φl(n),
and deﬁne the expressible feature ∥A†
n∥mg := Pl(n)
i=1 |ξi|.
For k < N, trader T
k will be identically zero. For k ≥N, trader T
k will buy
some number of copies of A†
n on day n; formally,
T k
n := γ†
k,n · (A†
n −A†∗n
n
),
with γ†
k,n to be deﬁned later. To deﬁne γ†
k,n, we will ﬁrst deﬁne a scaling factor on
the trader’s purchases:
δk :=
ε
1 + k .
Now we recursively deﬁne
γ†
k,n := [n ≥k] min

δk · w†
n, 1 −
X
i≤n−1
γ†
k,i∥A†
n∥mg

,
where [n ≥k] is Iverson bracket applied to n ≥k, i.e. the 0-1 indicator of that
condition. This sequence of traders is eﬃciently emulatable, because A† and w† are
e.c., and T
k makes no trades before day k.
100

Analyzing the trader.
On day n, trader T
k attempts to buy δkwn copies of An,
but caps its total budget at 1 dollar; the min in the deﬁnition of γ†
k,n ensures that
∥T
k(P)∥mg ≤1.
Observe that P
i≤n ∥T
k
i (P)∥mg = max{1, Pn
i=k δkwn∥An∥mg}. We can use this
to show that ∥T
k(P)∥mg = 1. For all n ≥N,
2ε
X
i≤n
wi <
X
i≤n
wi · (ValΓ(Ai) −Pi(Ai)) ≤2
X
i≤n
wi∥Ai∥mg.
Since the left hand side goes to ∞as n →∞, so does the right hand side. So indeed,
P∞
n=k δkwn∥An∥mg = ∞, and ∥T
k(P)∥mg = 1.
T
k has ε return on investment.
We will now show that each trader T
k has ε
return on investment. Trivially, for k < N, T
k has ε return on investment, because
∥T
k(P)∥mg = 0. So consider k ≥N.
As shorthand, let γk,n := γ†
k,n(P). Let mk be the ﬁrst m ≥k for which γk,m <
δkwm. We have γk,n = δkwn for k ≤n < mk, and γk,n = 0 for n > mk. So for all
W ∈PC(Γ),
W
 ∞
X
n=1
T k
n(P)
!
=
∞
X
n=1
W(T k
n(P))
=
X
k≤n≤mk
W(T k
n(P))
=
X
k≤n≤mk
δkwn(W(An) −Pn(An)) −(δkwmk −γk,mk)(W(Amk) −Pn(Amk))
≥
X
k≤n≤mk
δkwn(W(An) −Pn(An)) −δk
=
X
n≤mk
δkwn(W(An) −Pn(An)) −
X
n<k
δkwn(W(An) −Pn(An)) −δk
≥
X
n≤mk
δkwn(W(An) −Pn(An)) −(kδk + δk)
=
X
n≤mk
δkwn(ValΓ An −Pn(An)) −ε
≥2ε −ε
= ε.
So each trader T
k with k ≥N makes at least ε proﬁt with trades of total magnitude
1, ensuring that it has ε return on investment.
Deriving a contradiction.
Note that the magnitudes of the traders are
P-generable (the ﬁrst N −1 have magnitude 0 and the rest have magnitude 1).
By Lemma B.1.3, ∥T
k∥mg ≂k 0. ∥T
k∥mg = 1 for all k ≥N (by the above analysis),
so this is a contradiction.
101

D.2
Recurring Unbiasedness
Theorem 4.3.6 (Recurring Unbiasedness). Given an e.c. sequence of decidable
sentences φ and a P-generable divergent weighting w, the sequence
P
i≤n wi · (Pi(φi) −ThmΓ(φi))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
Proof. This is a special case of Theorem 4.5.9 (Aﬃne Recurring Unbiasedness).
D.3
Simple Calibration
Theorem 4.3.3 (Recurring Calibration). Let φ be an e.c. sequence of decidable
sentences, a and b be rational numbers, δ be an e.c. sequence of positive rational
numbers, and suppose that P
n
 Indδi(a < Pi(φi) < b)

i∈N+ = ∞. Then, if the se-
quence
 P
i≤n Indδi(a < Pi(φi) < b) · ThmΓ(φi)
P
i≤n Indδi(a < Pi(φi) < b)
!
n∈N+
converges, it converges to a point in [a, b]. Furthermore, if it diverges, it has a limit
point in [a, b].
Proof. Deﬁne wi := Indδi(a < φi
∗i < b). By Theorem 4.3.6 (Recurring Unbiased-
ness), the sequence
 P
i≤n wi · (Pi(φi) −ThmΓ(φi))
P
i≤n wi
!
n∈N+
has 0 as a limit point.
Let n1, n2, . . . be a the indices of a subsequence of this
sequence that converges to zero. We also know that for all n high enough,
a ≤
P
i≤n wiPi(φi)
P
i≤n wi
≤b
because wi = 0 whenever Pi(φi) ̸∈[a, b]. Now consider the sequence
 P
i≤nk wi · ThmΓ(φi)
P
i≤nk wi
!
k∈N+
=
 P
i≤nk wiPi(φi)
P
i≤n wi
−
P
i≤nk wi · (Pi(φi) −ThmΓ(φi))
P
i≤nk wi
!
k∈N+
The ﬁrst term is bounded between a and b, and the second term goes to zero, so
the sequence has a lim inf at least a and a lim sup no more than b. By the Bolzano-
Weierstrass theorem, this sequence has a convergent subsequence, whose limit must
be between a and b. This subsequence is also a subsequence of
 P
i≤n wi · ThmΓ(φi)
P
i≤n wi
!
n∈N+
which proves that this sequence has a limit point in [a, b], as desired.
102

D.4
Aﬃne Unbiasedness From Feedback
Theorem 4.5.10 (Aﬃne Unbiasedness from Feedback). Given A ∈BCS(P) that is
determined via Γ, a strictly increasing deferral function f such that ValΓ(An) can
be computed in time O(f(n+1)), and a P-generable divergent weighting w such that
the support of w is contained in the image of f,
P
i≤n wi · (Pi(Ai) −ValΓ(Ai))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on A”.
Proof. Without loss of generality, assume that each ∥An∥1 ≤1. Deﬁne
Biask :=
P
i≤k wf(i) · (Pf(i)(Af(i)) −ThmΓ(Af(i)))
P
i≤k wf(i)
and observe that Biask ≂k 0 implies the theorem statement, since we need only
consider the sum over n in the support of f. We wish to show that Biask ≂k 0. We
will show a trader that exploits P under the assumption that lim infk→∞Biask < 0,
proving Biask ≳k 0. We can apply the same argument to the sequence (−An)n to
get Biask ≲k 0.
Suppose lim infk→∞Biask < 0. Under this supposition, inﬁnitely often, Biask <
−3ε for some rational 0 < ε < 1/6.
Deﬁning the trader.
Let w† be an e.c. EF progression such that w†
n(P) = wn.
Let A† be an e.c. EF-combination progression such that A†
n(P) = An. Recursively
deﬁne
β†
i := ε · Wealth†
i · w†
i
Wealth†
i := 1 +
X
j≤i−1
β†
j ·

A†∗f(j+1)
f(j)
−A†∗f(j)
f(j)

in order to deﬁne the trader
Tn :=
(
β†
i ·

A†
f(i) −A†∗n
f(i)

−[i > 1] · β†
i−1 ·

A†
f(i−1) −A†∗n
f(i−1)

if ∃i : n = f(i)
0
otherwise.
Note that β†
i and Wealth†
i have rank at most f(i), and so Tn has rank at most n.
Analyzing the trader.
As shorthand, let βi
:=
β†
i (P), and Wealthi
:=
Wealth†
i(P).
Intuitively, on day f(i), T buys Af(i) according to a fraction of its “wealth”
Wealthi (how much money T would have if it started with one dollar), and then
sells Af(i) at a later time f(i + 1). Thus, T makes money if the price of Af(i) is
greater at time f(i + 1) than at time f(i).
Betting according to a fraction of wealth resembles the Kelley betting strategy
and ensures that the trader never loses more than $1.
Wealthi −1 is a lower
bound on T’s worth in any world W ∈PC(Df(i)), based on trades T makes on R-
combinations Af(1) through Af(i−1). Thus, since the number of copies of An that
T buys is no more than ε times its current wealth, and ∥An∥≤1, T ’s minimum
worth is bounded below by −1.
Now it will be useful to write Wealthi in log space.
Intuitively, this should
be enlightening because T always bets a fraction of its wealth (similar to a Kelley
bettor), so its winnings multiply over time rather than adding. By induction,
log Wealthi =
X
j≤i−1
log
 1 + εwj(Pf(j+1)(Af(j)) −Pf(j)(Af(j)))

103

This statement is trivial when i = 1. For the inductive step, we have
log Wealthi+1 = log
 Wealthi + βi(Pf(i+1)(Af(i)) −Pf(i)(Af(i)))

= log
 Wealthi + ε · Wealthi · wi(Pf(i+1)(Af(i)) −Pf(i)(Af(i)))

= log Wealthi + log
 1 + εwj(Pf(j+1)(Af(j)) −Pf(j)(Af(j)))

=
X
j≤i
log
 1 + εwj(Pf(j+1)(Af(j)) −Pf(j)(Af(j)))

For |x| ≤1/2, we have log(1 + x) ≥x −x2. Therefore, since ε < 1/4, wj ≤1,
and |Pf(j+1)(Af(j)) −Pf(i)(Af(j))| ≤1,
log Wealthi ≥
X
j≤i−1

εwj(Pf(j+1)(Af(j)) −Pf(j)(Af(j)))
−ε2w2
j (Pf(j+1)(Af(j)) −Pf(j)(Af(j)))2
≥
X
j≤i−1
 εwj(Pf(j+1)(Af(j)) −Pf(j)(Af(j))) −ε2wj

=
X
j≤i−1
εwj
 (Pf(j+1)(Af(j)) −Pf(j)(Af(j))) −ε

At this point, it will be useful to show a relation between Pf(j+1)(Af(j)) and
ValΓ(Af(j)). Consider the sequence
A′
n :=
Af(i) −ValΓ(Af(i))
if ∃i : f(i + 1) = n
0
otherwise
which is in BCS(P) because ValΓ(Af(j)) is computable in time polynomial in f(j+1).
Of course, each A′
n has value 0 in any world W ∈PC(Γ). So by Theorem 4.5.4
(Aﬃne Provability Induction),
Pn(A′
n) ≂n 0
so for all suﬃciently high j, Pf(j+1)(Af(j)) ≥ValΓ(Af(j)) −ε.
Thus, for some
constant C,
log Wealthi ≥
X
j≤i−1
εwj
 (ValΓ(Af(j)) −Pf(j)(Af(j))) −2ε

−C
= ε

X
j≤i−1
wj

(−Biasi−1 −2ε) −C
Now, for inﬁnitely many i it is the case that Biasi−1 < −3ε. So it is inﬁnitely
often the case that log Wealthi ≥ε2 P
j≤i−1 wi −C.
Since w is divergent, T ’s
eventual wealth (and therefore max proﬁt) can be arbitrarily high. Thus, T exploits
P.
D.5
Unbiasedness From Feedback
Theorem 4.3.8 (Unbiasedness From Feedback). Let φ be any e.c. sequence of
decidable sentences, and w be any P-generable divergent weighting. If there exists a
strictly increasing deferral function f such that the support of w is contained in the
image of f and ThmΓ(φf(n)) is computable in O(f(n + 1)) time, then
P
i≤n wi · (Pi(φi) −ThmΓ(φi))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on φ”.
Proof. This is a special case of Theorem 4.5.10 (Aﬃne Unbiasedness from Feedback).
104

D.6
Learning Pseudorandom Aﬃne Sequences
Theorem 4.5.11 (Learning Pseudorandom Aﬃne Sequences). Given a A ∈BCS(P)
which is determined via Γ, if there exists deferral function f such that for any
P-generable f-patient divergent weighting w,
P
i≤n wi · ValΓ(Ai)
P
i≤n wi
≳n 0,
then
Pn(An) ≳n 0,
and similarly for ≂n, and ≲n.
Proof. We will prove the statement in the case of ≳n; the case of ≲n follows by
negating the R-combination sequence A, and the case of ≂n follows from the con-
junction of the other two cases. Suppose it is not the case that Pn(An) ≳n 0. Then
there is a rational ε > 0 such that Pn(An) < −2ε inﬁnitely often.
Deﬁning the trader.
Let A† be an e.c. EF-combination progression such that
A†
n(P) = An. Let an aﬃne combination A be considered settled by day m if W(A) =
ValΓ(A) for each W ∈PC(Dm). We may write Settled(n, m) to be the proposition
that An is settled by day m. Settled(n, m) is decidable; let settled be a Turing
machine deciding Settled(n, m) given (n, m). Now we deﬁne a lower-approximation
to Settled:
DeﬁnitelySettled(n, m) :↔∃i ≤m : settled(n, i) returns true within m steps.
Note that
• DeﬁnitelySettled(n, m) can be decided in time polynomial in m when n ≤m,
• DeﬁnitelySettled(n, m) →Settled(n, m), and
• If Settled(n, m), then DeﬁnitelySettled(n, M) for some M ≥m.
To deﬁne the trader, ﬁrst we will deﬁne α† recursively by
α†
n := (1 −C†
n) Indε(A†∗n
n
< −ε)
C†
n :=
X
i<n
[¬DeﬁnitelySettled(i, n) ∨f(i) > n] · α†
i.
The trader itself buys α†
n(P) copies of the combination A†n(P) on day n:
Tn := α†
n · (A†
n −A†∗n
n
).
Intuitively, C†
n is the total number of copies of R-combinations that the trader
has bought that are either possibly-unsettled (according to DeﬁnitelySettled), or
whose deferral time f(i) is past the current time n.
Analyzing the trader.
As shorthand, deﬁne αn := α†
n(P) and Cn := C†
n(P).
Some important properties of T are:
• Each Cn ≤1.
• αn = 1 −Cn when Pn(An) < −2ε.
• Whenever αn > 0, Pn(An) < −ε.
105

• P
n∈N+ αn = ∞. Suppose this sum were ﬁnite. Then there is some time N
for which P
n≥N αn < 1/2. For some future time N ′, and each n < N, we
have DeﬁnitelySettled(n, N ′) ∧f(n) ≤N ′. This implies that Cn < 1/2 for
each n ≥N ′. However, consider the ﬁrst n ≥N ′ for which Pn(An) < −2ε.
Since Cn < 1/2, αn ≥1/2. But this contradicts P
n≥N αn < 1/2.
Let b ∈Q be such that each ∥An∥mg < b. Consider this trader’s proﬁt at time
m in world W ∈PC(Dm):
X
n≤m
αn(W(An) −Pn(An))
≥
X
n≤m
αn(W(An) + ε)
≥
X
n≤m
αn(ValΓ(An) + ε) −2b
where the last inequality follows because P
n≤m[¬Settled(n, m)]αn ≤1, and an
unsettled copy of An can only diﬀer by 2b between worlds, while settled copies of
An must have the same value in all worlds in PC(Dm).
T holds no more than 1 copy of R-combinations unsettled by day M, and an
unsettled combination’s value can only diﬀer by 2b between diﬀerent worlds while
settled aﬃne combinations must have the same value in diﬀerent worlds in PC(Dm).
We now show that this quantity goes to ∞, using the fact that A is pseudorandomly
positive.
Observe that α is a divergent weighting. It is also f-patient, since P
n≤m[f(n) ≥
m]αn ≤1. So by assumption,
lim inf
m→∞
P
n≤m αn · ValΓ(An)
P
n≤m αn
≥0.
At this point, note that
X
n≤m
αn(ValΓ(An) + ε) =

X
n≤m
αn


 P
n≤m αn · ValΓ(An)
P
n≤m αn
+ ε
!
.
For all suﬃciently high m,
P
n≤m αn·ValΓ(An)
P
n≤m αn
≥−ε/2, and P
n∈N+ αn = ∞, so
lim inf
m→∞
X
n≤m
αn(ValΓ(An) + ε) = ∞.
If we deﬁne g(m) to be the minimum plausible worth at time m over plausible
worlds W ∈PC(Dm), we see that g(m) limits to inﬁnity, implying that the trader’s
maximum worth goes to inﬁnity. The fact that g(m) limits to inﬁnity also implies
that g(m) is bounded from below, so the trader’s minimum worth is bounded from
below. Thus, this trader exploits the market P.
D.7
Learning Varied Pseudorandom Frequencies
Deﬁnition 4.4.4 (Varied Pseudorandom Sequence). Given a deferral function f,
a set S of f-patient divergent weightings, an e.c. sequence φ of Γ-decidable sen-
tences, and a P-generable sequence p of rational probabilities, φ is called a p-varied
pseudorandom sequence (relative to S) if, for all w ∈S,
P
i≤n wi · (pi −ThmΓ(φi))
P
i≤n wi
≂n 0.
106

Furthermore, we can replace ≂n with ≳n or ≲n, in which case we say φ is varied
pseudorandom above p or varied pseudorandom below p, respectively.
Theorem 4.4.5 (Learning Varied Pseudorandom Frequencies). Given an e.c. se-
quence φ of Γ-decidable sentences and a P-generable sequence p of rational proba-
bilities, if there exists some f such that φ is p-varied pseudorandom (relative to all
f-patient P-generable divergent weightings), then
Pn(φn) ≂n pn.
Furthermore, if φ is varied pseudorandom above or below p, then the ≂n may be
replaced with ≳n or ≲n (respectively).
Proof. We will prove the statement in the case of pseudorandom above; the case of
pseudorandom below is analogous, and the case of pseudorandom follows from the
other cases.
Deﬁne An := φn −pn and note that A ∈BCS(P). Observe that, because A is
varied pseudorandomness above p, for any f-patient divergent weighting w,
P
i≤n wi ValΓ An
P
i≤n wi
≳n 0.
Now apply Theorem 4.5.11 (Learning Pseudorandom Aﬃne Sequences) to get
Pn(An) = Pn(φn) −pn ≳n 0.
D.8
Learning Pseudorandom Frequencies
Deﬁnition 4.4.1 (Pseudorandom Sequence). Given a set S of divergent weightings
(Deﬁnition 4.3.4), a sequence φ of decidable sentences is called pseudorandom
with frequency p over S if, for all weightings w ∈S,
lim
n→∞
P
i≤n wi · ThmΓ(φi)
P
i≤n wi
exists and is equal to p.
Theorem 4.4.2 (Learning Pseudorandom Frequencies). Let φ be an e.c. sequence
of decidable sentences. If φ is pseudorandom with frequency p over the set of all
P-generable divergent weightings, then
Pn(φn) ≂n p.
Proof. Let q be any rational number less than p. Note that φ is varied pseudorandom
above q, so by Theorem 4.4.5 (Learning Varied Pseudorandom Frequencies),
Pn(φn) ≳n q.
But we could have chosen any rational q < p, so Pn(φn) ≳n p.
An analogous
argument shows Pn(φn) ≲n p.
E
Expectations Proofs
E.1
Consistent World LUV Approximation Lemma
Lemma E.1.1. Let B ∈BLCS(P) be a R-LUV combination bounded by some
rational number b. For all natural numbers n and all W ∈PC(Γ), we have
|EW
n (B) −W(B)| ≤b/n.
107

Proof. Let W ∈PC(Γ). For any [0, 1]-LUV X, by Deﬁnition 4.8.2,
|EW
n (X) −W(X)| =

n−1
X
i=0
1
nW(“X > i/n”) −W(X)

Since Γ can represent computable functions, the number of i values in {0, . . ., n−1}
for which W(“X > i/n”) = 1 is at least ⌊nW(X)⌋≥nW(X) −1, so
n−1
X
i=0
1
nW(“X > i/n”) ≥W(X) −1/n.
Similarly, the number of i values in {0, . . ., n −1} for which W(“X > i/n”) is no
more than ⌈nW(X)⌉≤nW(X) + 1, so
n−1
X
i=0
1
nW(“X > i/n”) ≤W(X) + 1/n.
We now have
|EW
n (B) −W(B)| = |c1(EW
n (X1) −W(X1)) + · · · + ck(EW
n (Xk) −W(Xk))|
≤c1|EW
n (X1) −W(X1)| + · · · + ck|EW
n (Xk) −W(Xk)|
≤c1/n + · · · + ck/n
≤b/n.
E.2
Mesh Independence Lemma
Lemma E.2.1. Let B ∈BLCS(P). Then
lim
n→∞sup
m≥n
|EPm
n (Bn) −Em(Bn)| = 0.
Proof. We will prove the claim that
lim sup
m→∞max
n≤m
 |EPm
n (Bn) −Em(Bn)| −(2/n)

≤0.
This claim implies that, for any ε > 0, there are only ﬁnitely many (n, m) with
n ≤m such that |EPm
n (Bn) −Em(Bn)| > 2/n + ε, which in turn implies that, for
any ε′ > 0, there are only ﬁnitely many (n, m) with n ≤m such that |EPm
n (Bn) −
Em(Bn)| > ε′. This is suﬃcient to show the statement of the theorem.
We will now prove
lim sup
m→∞max
n≤m
 EPm
n (Bn) −Em(Bn) −(2/n)

≤0.
The proof with Em(Bn)−EPm
n (Bn) instead is analogous, and together these inequal-
ities prove the claim.
Suppose this inequality does not hold. Then there is some rational ε > 0 such
that for inﬁnitely many m,
max
n≤m(EPm
n (Bn) −Em(Bn) −(2/n)) > ε.
Let B† be an e.c. EF-combination progression such that B†
n(P) = Bn. Assume
without loss of generality that each ∥Bn∥1 ≤1 (they are assumed to be bounded
and can be scaled down appropriately). Deﬁne EF-combinations
A†
n,m := Exn(B†
n) −Exm(B†
n) −2/n,
108

using the F-combinations Ex deﬁned in A.3.
As shorthand, we write An,m :=
A†
n,m(P). By Lemma E.1.1, for all W ∈PC(Γ), W(An,m) ≤0. We aim to show
Pm(An,m) < ε for all suﬃciently high m and n ≤m, but we cannot immediately
derive this using Theorem 4.5.4 (Aﬃne Provability Induction), since A has two
indices. We get around this diﬃculty by taking a “softmax” over possible values of
n given a ﬁxed value of m. Speciﬁcally, for n ≤m, deﬁne expressible features (of
rank m)
α†
n,m := Indε/2
 A†∗m
n,m > ε/2

·
 
1 −
X
i<n
α†
i,m
!
.
As shorthand, we write αn,m := α†
n,m(P). Intuitively, αn,m will distribute weight
among n values for which Am,n is overpriced at time m. Now we deﬁne the EF-
combination progression
G†
m :=
X
n≤m
αn,m · A†
n,m.
As shorthand, we write Gm := G†
m(P). Fix m and suppose that Pm(An,m) ≥ε for
some n ≤m. Then P
n≤m αn,m = 1. Therefore,
Pm(Gm) =
X
n≤m
αn,mPm(An,m) ≥
X
n≤m
αn,m · ε/2 = ε/2.
So if we can show Pm(Gm) ≲m 0, that will be suﬃcient to show that
maxn≤m Pm(An,m) < ε for all suﬃciently high m.
We now show this.
Let
W ∈PC(Γ).
Since each αn,m ≥0, and W(An,m) ≤0, we have W(Gm) ≤0.
So by Theorem 4.5.4 (Aﬃne Provability Induction), Pm(Gm) ≲m 0; here we
use that (Gm)m is bounded, since the An,m are bounded and since for each m,
P
n≤m αn,m ≤1 by construction.
So for all suﬃciently high m we have maxn≤m Pm(An,m) < ε (or equivalently,
maxn≤m(EPm
n (Bn)−Em(Bn)) < 2/n+ε). But this contradicts our assumption that
for inﬁnitely many m,
max
n≤m(EPm
n (Bn) −Em(Bn) −(2/n)) > ε.
E.3
Expectation Preemptive Learning
Theorem 4.8.13 (Expectation Preemptive Learning). Let B ∈BLCS(P). Then
lim inf
n→∞En(Bn) = lim inf
n→∞sup
m≥n
Em(Bn)
and
lim sup
n→∞En(Bn) = lim sup
n→∞
inf
m≥n Em(Bn) .
Proof. We prove only the ﬁrst statement; the proof of the second statement is
analogous.
Apply Theorem 4.5.7 (Aﬃne Preemptive Learning) to the bounded
sequence (Exn(Bn))n to get
lim inf
n→∞EPn
n (Bn) = lim inf
n→∞sup
m≥n
EPm
n (Bn),
using that by deﬁnition Pm(Exn(Bn)) = EPm
n (Bn). By Lemma E.2.1,
lim
n→∞sup
m≥n
|EPm
n (Bn) −Em(Bn)| = 0
so
lim inf
n→∞sup
m≥n
EPm
n (Bn) = lim inf
n→∞sup
m≥n
Em(Bn).
109

E.4
Expectations Converge
Theorem 4.8.3 (Expectations Converge). The limit E∞: S →[0, 1] deﬁned by
E∞(X) := lim
n→∞En(X)
exists for all X ∈U.
Proof. By applying Theorem 4.8.13 (Expectation Preemptive Learning) to the con-
stant sequence X, X, . . ., we have
lim inf
n→∞En(X) = lim inf
n→∞sup
m≥n
Em(X) = lim sup
n→∞En(X).
E.5
Limiting Expectation Approximation Lemma
Lemma E.5.1. For any B ∈BLCS(P),
|EP∞
n (Bn) −E∞(Bn)| ≂n 0.
Proof. By Lemma E.2.1 and by continuity of V 7→EV
n(Bn),
lim
n→∞|EP∞
n (Bn) −E∞(Bn)| = lim
n→∞lim
m→∞|EPm
n (Bn) −Em(Bn)|
≤lim
n→∞sup
m≥n
|EPm
n (Bn) −Em(Bn)|
= 0.
E.6
Persistence of Expectation Knowledge
Theorem 4.8.12 (Persistence of Expectation Knowledge). Let B ∈BLCS(P).
Then
lim inf
n→∞
inf
m≥n Em(Bn) = lim inf
n→∞E∞(Bn)
and
lim sup
n→∞
sup
m≥n
Em(Bn) = lim sup
n→∞E∞(Bn).
Proof. We prove only the ﬁrst statement; the proof of the second statement is
analogous. Apply Theorem 4.5.6 (Persistence of Aﬃne Knowledge) to (Exn(Bn))n
to get
lim inf
n→∞
inf
m≥n EPm
n (Bn) = lim inf
n→∞EP∞
n (Bn).
We now show equalities on these two terms:
1. By Lemma E.2.1,
lim
n→∞sup
m≥n
|EPm
n (Bn) −Em(Bn)| = 0
so
lim inf
n→∞
inf
m≥n Em(Bn) = lim inf
n→∞
inf
m≥n EPm
n (Bn).
2. By Lemma E.5.1,
lim inf
n→∞EP∞
n (Bn) = lim inf
n→∞E∞(Bn).
Together, these three equalities prove the theorem statement.
110

E.7
Expectation Coherence
Theorem 4.8.11 (Expectation Coherence). Let B ∈BLCS(P). Then
lim inf
n→∞
inf
W∈PC(Γ) W(Bn) ≤lim inf
n→∞E∞(Bn) ≤lim inf
n→∞En(Bn),
and
lim sup
n→∞En(Bn) ≤lim sup
n→∞E∞(Bn) ≤lim sup
n→∞
sup
W∈PC(Γ)
W(Bn).
Proof. We prove only the ﬁrst statement; the proof of the second statement is
analogous. Apply Theorem 4.5.5 (Aﬃne Coherence) to (Exn(Bn))n to get
lim inf
n→∞
inf
W∈PC(Γ) EW
n (Bn) ≤lim inf
n→∞EP∞
n (Bn) ≤lim inf
n→∞En(Bn),
We now show equalities on the ﬁrst two terms:
1. Let b be the bound on B. By Lemma E.1.1,
lim inf
n→∞
inf
W∈PC(Γ) |EW
n (Bn) −W(Bn)| ≤lim inf
n→∞
inf
W∈PC(Γ) b/n = 0
so
lim inf
n→∞
inf
W∈PC(Γ) EW
n (Bn) = lim inf
n→∞
inf
W∈PC(Γ) W(Bn).
2. By Lemma E.5.1,
lim inf
n→∞|EP∞
n (Bn) −E∞(Bn)| = 0
so
lim inf
n→∞EP∞
n (Bn) = lim inf
n→∞E∞(Bn).
Together, these three equalities prove the theorem statement.
E.8
Expectation Provability Induction
Theorem 4.8.10 (Expectation Provability Induction). Let B ∈BLCS(P) and
b ∈R. If, for all consistent worlds W ∈PC(Γ) and all n ∈N+, it is the case that
W(Bn) ≥b, then
En(Bn) ≳n b,
and similarly for = and ≂n, and for ≤and ≲n.
Proof. We prove the statement in the case of ≥; the case of ≤is analogous, and the
case of = follows from the conjunction of the other two cases. By Theorem 4.8.11
(Expectation Coherence),
lim inf
n→∞En(Bn) ≥lim inf
n→∞
inf
W∈PC(Γ) W(Bn) ≥b.
We will usually apply this theorem using the = case.
E.9
Linearity of Expectation
Theorem 4.8.4 (Linearity of Expectation). Let a, b be bounded P-generable se-
quences of rational numbers, and let X, Y , and Z be e.c. sequences of [0, 1]-LUVs.
If we have Γ ⊢Zn = anXn + bnYn for all n, then
anEn(Xn) + bnEn(Yn) ≂n En(Zn).
Proof. Observe that W(anXn + bnYn −Zn) = 0 for all n and W ∈PC(Γ). So by
Theorem 4.8.10 (Expectation Provability Induction), En(anXn + bnYn −Zn) ≂n 0;
the theorem statement immediately follows from the deﬁnition of En applied to a
LUV-combination (where anXn + bnYn −Zn is interpreted as a LUV-combination,
not another LUV).
111

E.10
Expectations of Indicators
Theorem 4.8.6 (Expectations of Indicators). Let φ be an e.c. sequence of sentences.
Then
En(1(φn)) ≂n Pn(φn).
Proof. Observe that W(Exn(1(φn))) = W(φn) for all W ∈PC(Γ); either
• W(φ) = 0 and W(Exn(1(φn))) = Pn−1
i=0
1
nW(“1(φn) > i/n”) = Pn−1
i=0 0 = 0,
or
• W(φ) = 1 and W(Exn(1(φn))) = Pn−1
i=0
1
nW(“1(φn) > i/n”) = Pn−1
i=0
1
n = 1.
So by Theorem 4.5.4 (Aﬃne Provability Induction),
En(1(φn)) ≂n Pn(φn).
E.11
Expectation Recurring Unbiasedness
Theorem 4.8.15 (Expectation Recurring Unbiasedness). If B ∈BLCS(P) is de-
termined via Γ, and w is a P-generable divergent weighting weighting such that the
support of w is contained in the image of f,
P
i≤n wi · (Ei(Bi) −ValΓ(Bi))
P
i≤n wi
has 0 as a limit point. In particular, if it converges, it converges to 0.
Proof. Let W ∈PC(Γ). Apply Theorem 4.5.9 (Aﬃne Recurring Unbiasedness) to
(Exn(Bn))n and w to get that
 P
i≤n wi(Ei(Bi) −EW
i (Bi))
P
i≤n wi
!
n∈N+
has 0 as a limit point. Furthermore, by Lemma E.1.1, |EW
i (Bi)−ValΓ(Bi)| ≤b/i
where b is a bound on B. As a result, for any subsequence of
 P
i≤n wi(Ei(Bi) −EW
i (Bi))
P
i≤n wi
!
n∈N+
that limits to zero, the corresponding subsequence of
 P
i≤n wi(Ei(Bi) −ValΓ(Bi))
P
i≤n wi
!
n∈N+
also limits to zero, as desired.
E.12
Expectation Unbiasedness From Feedback
Theorem
4.8.16
(Expectation Unbiasedness From Feedback). Given B
∈
BLCS(P) that is determined via Γ, a strictly increasing deferral function f such
that ValΓ(An) can be computed in time O(f(n + 1)), and a P-generable divergent
weighting w,
P
i≤n wi · (Ei(Bi) −ValΓ(Bi))
P
i≤n wi
≂n 0.
In this case, we say “w allows good feedback on B”.
112

Proof. Let W ∈PC(Γ). Note that if ValΓ Bn can be computed in time polynomial
in g(n+1), then so can ValΓ Exk(Bn). Apply Theorem 4.5.10 (Aﬃne Unbiasedness
from Feedback) to (Exn(Bn))n to get
P
i≤n wi · (Ei(Bi) −EW
i (Bi))
P
i≤n wi
≂n 0.
Furthermore, by Lemma E.1.1, |EW
i (Bn) −ValΓ(Bn)| ≤b/n where b is a bound on
B. As a result,
P
i≤n wi · (Ei(Bi) −ValΓ(Bi))
P
i≤n wi
≂n 0.
as desired.
E.13
Learning Pseudorandom LUV Sequences
Theorem 4.8.17 (Learning Pseudorandom LUV Sequences). Given a B
∈
BLCS(P) which is determined via Γ, if there exists a deferral function f such that
for any P-generable f-patient divergent weighting w,
P
i≤n wi · ValΓ(Bi)
P
i≤n wi
≳n 0,
then
En(Bn) ≳n 0.
Proof. We will prove the statement in the case of ≳; the case of ≲is analogous, and
the case of ≂follows from the other cases.
Let b be the bound of B. Let W ∈PC(Γ). First, note that by Lemma E.1.1,
|EW
i (Bi) −ValΓ(Bi)| ≤b/i. Therefore,
P
i≤n wi · EW
i (Bi)
P
i≤n wi
≂n
P
i≤n wi · ValΓ(Bi)
P
i≤n wi
≳n 0.
So we may apply Theorem 4.5.11 (Learning Pseudorandom Aﬃne Sequences) to
(Exn(Bn))n to get
En(Bn) ≳n 0.
F
Introspection and Self-Trust Proofs
F.1
Introspection
Theorem 4.11.1 (Introspection). Let φ be an e.c. sequence of sentences, and a,
b be P-generable sequences of probabilities. Then, for any e.c. sequence of positive
rationals δ →0, there exists a sequence of positive rationals ε →0 such that for all
n:
1. if Pn(φn) ∈(an + δn, bn −δn), then
Pn(“an < Pn(φn) < bn”) > 1 −εn,
2. if Pn(φn) /∈(an −δn, bn + δn), then
Pn(“an < Pn(φn) < bn”) < εn.
Proof. Deﬁne ψn := “an < Pn(φn) < bn”.
113

Proof of the ﬁrst statement.
Observe that for all n, and all W ∈PC(Γ),
Indδn(an < Pn(φn) < bn) · (1 −W(ψn)) = 0,
since regardless of Pn(φn), one of the two factors is 0. Thus, applying Theorem 4.5.4
(Aﬃne Provability Induction) gives
Indδn(an < Pn(φn) < bn) · (1 −Pn(ψn)) ≂n 0.
Deﬁne
εn := Indδn(an < Pn(φn) < bn) · (1 −Pn(ψn)) + 1/n
and note that εn > 0 and εn ≂n 0. For any n for which Pn(φn) ∈(an + δn, bn −δn),
the ﬁrst factor is 1, so Pn(ψn) = 1 −εn + 1/n > 1 −εn.
Proof of the second statement.
Observe that for all n, and all W ∈PC(Γ),
 Indδn(Pn(φn) < an) + Indδn(Pn(φn) > bn)

· W(ψn) = 0,
since regardless of Pn(φn), one of the factors is 0. Thus, applying Theorem 4.5.4
(Aﬃne Provability Induction) gives
 Indδn(Pn(φn) < an) + Indδn(Pn(φn) > bn)

· Pn(ψn) ≂n 0.
Deﬁne
εn :=
 Indδn(Pn(φn) < an) + Indδn(Pn(φn) > bn)

· Pn(ψn) + 1/n
and note that εn > 0 and εn ≂n 0. For any n for which Pn(φn) /∈(an −δn, bn + δn),
the ﬁrst factor is 1, so Pn(ψn) < εn.
F.2
Paradox Resistance
Theorem 4.11.2 (Paradox Resistance). Fix a rational p ∈(0, 1), and deﬁne an
e.c. sequence of “paradoxical sentences” χp satisfying
Γ ⊢χp
n ↔

Pn(χp
n) < p

for all n. Then
lim
n→∞Pn(χp
n) = p.
Proof. We prove Pn(φn) ≳n p and Pn(φn) ≲n p individually.
1. Suppose it is not the case that Pn(φn) ≳n p, so Pn(φn) < p−ε inﬁnitely often
for some ε > 0. Observe that for all n, and all W ∈PC(Γ),
Ind1/n(Pn(φn) < p) · (1 −W(φn)) = 0,
since regardless of Pn(φn), one of the factors is 0.
Thus, applying Theo-
rem 4.5.4 (Aﬃne Provability Induction) yields
Ind1/n(Pn(φn) < p) · (1 −Pn(φn)) ≂n 0.
(F.2.1)
But inﬁnitely often,
Ind1/n(Pn(φn) < p) · (1 −Pn(φn)) ≥1 · (1 −(p −ε)) ≥ε
which contradicts equation (F.2.1).
114

2. Suppose it is not the case that Pn(φn) ≲n p, so Pn(φn) > p+ε inﬁnitely often
for some ε > 0. Observe that for all n, and all W ∈PC(Γ),
Ind1/n(Pn(φn) > p) · W(φn) = 0,
since regardless of Pn(φn), one of the factors is 0.
Thus, applying Theo-
rem 4.5.4 (Aﬃne Provability Induction) yields
Ind1/n(Pn(φn) > p) · Pn(φn) ≂n 0.
(F.2.2)
But inﬁnitely often,
Ind1/n(Pn(φn) > p) · Pn(φn) ≥1 · (p + ε) ≥ε
which contradicts equation (F.2.2).
F.3
Expectations of Probabilities
Theorem 4.11.3 (Expectations of Probabilities). Let φ be an eﬃciently computable
sequence of sentences. Then
Pn(φn) ≂n En(“Pn(φn)”).
Proof. Observe that for all n, and for all W ∈PC(Γ), W(Pn(φn) −“Pn(φn)”) = 0
(where Pn(φn) is a number and “Pn(φn)”) is a LUV). Thus, by Theorem 4.8.10
(Expectation Provability Induction),
Pn(φn) −En(“Pn(φn)”) ≂n 0.
F.4
Iterated Expectations
Theorem 4.11.4 (Iterated Expectations). Suppose X is an eﬃciently computable
sequence of LUVs. Then
En(Xn) ≂n En(“En(Xn)”).
Proof. Observe that for all n, and for all W ∈PC(Γ), W(En(Xn) −“En(Xn)”) = 0
(where En(Xn) is a number and “En(Xn)”) is a LUV). Thus, by Theorem 4.8.10
(Expectation Provability Induction),
En(Xn) −En(“En(Xn)”) ≂n 0.
F.5
Expected Future Expectations
Theorem 4.12.1 (Expected Future Expectations). Let f be a deferral function (as
per Deﬁnition 4.3.7), and let X denote an e.c. sequence of [0, 1]-LUVs. Then
En(Xn) ≂n En(“Ef(n)(Xn)”).
Proof. Let Ym := Xn if m = f(n) for some n, and Ym := “0” otherwise. Observe
that (Ym)m is e.c.. By Theorem 4.11.4 (Iterated Expectations),
Ef(n)(Xn) ≂n Ef(n)(“Ef(n)(Xn)”).
115

We now manipulate the encodings f(n) (for the number f(n)) and f(n) (for the
program computing f and its input n). Observe than for all W ∈PC(Γ),
W(“Ef(n)(Xn)”) = W(“Ef(n)(Xn)”).
So by Theorem 4.8.10 (Expectation Provability Induction),
Ef(n)(Xn) ≂n Ef(n)(“Ef(n)(Xn)”).
By Theorem 4.8.13 (Expectation Preemptive Learning),
En(Xn) ≂n En(“Ef(n)(Xn)”).
F.6
No Expected Net Update
Theorem 4.12.2 (No Expected Net Update). Let f be a deferral function, and let
φ be an e.c. sequence of sentences. Then
Pn(φn) ≂n En(“Pf(n)(φn)”).
Proof. Let ψm := φn if m = f(n) for some n, and ψm := ⊥otherwise. Observe
that (ψm)m is e.c.. By Theorem 4.11.3 (Expectations of Probabilities),
Pf(n)(φn) ≂n Ef(n)(“Pf(n)(φn)”).
We now manipulate the encodings f(n) and f(n). Observe that for all W ∈
PC(Γ),
W(“Pf(n)(φn)”) = W(“Pf(n)(φn)”).
So by Theorem 4.8.10 (Expectation Provability Induction),
Pf(n)(φn) ≂n Ef(n)(“Pf(n)(φn)”).
By Theorem 4.8.6 (Expectations of Indicators),
Ef(n)(1(φn)) ≂n Ef(n)(“Pf(n)(φn)”).
By Theorem 4.8.13 (Expectation Preemptive Learning),
En(1(φn)) ≂n En(“Pf(n)(φn)”).
By Theorem 4.8.6 (Expectations of Indicators),
Pn(φn) ≂n En(“Pf(n)(φn)”).
F.7
No Expected Net Update under Conditionals
Theorem 4.12.3 (No Expected Net Update under Conditionals). Let f be a de-
ferral function, and let X denote an e.c. sequence of [0, 1]-LUVs, and let w denote
a P-generable sequence of real numbers in [0, 1]. Then
En(“Xn · wf(n)”) ≂n En(“Ef(n)(Xn) · wf(n)”).
116

Proof. By Theorem 4.11.4 (Iterated Expectations) and Theorem 4.8.10 (Expecta-
tion Provability Induction),
Ef(n)(Xn) ≂n Ef(n)(“Ef(n)(Xn)”) ≂n Ef(n)(“Ef(n)(Xn)”)
and thus
Ef(n)(Xn) · wf(n) ≂n Ef(n)(“Ef(n)(Xn)”) · wf(n).
Observe that for all n, and for all W ∈PC(Γ),
W(Xn) · wf(n) = W(“Xn · wf(n)”).
So by Theorem 4.8.10 (Expectation Provability Induction),
Ef(n)(Xn) · wf(n) ≂n Ef(n)(“Xn · wf(n)”).
Similarly, for all n and all W ∈PC(Γ),
W(“Ef(n)(Xn)”) · wf(n) ≂n W(“Ef(n)(Xn) · wf(n)”).
So by Theorem 4.8.10 (Expectation Provability Induction),
Ef(n)(“Ef(n)(Xn)”) · wf(n) ≂n Ef(n)(“Ef(n)(Xn) · wf(n)”).
Combining these,
Ef(n)(“Ef(n)(Xn) · wf(n)”) ≂n Ef(n)(“Xn · wf(n)”).
So by Theorem 4.8.13 (Expectation Preemptive Learning),
En(“Ef(n)(Xn) · wf(n)”) ≂n En(“Xn · wf(n)”).
F.8
Self-Trust
Theorem 4.12.4 (Self-Trust). Let f be a deferral function, φ be an e.c. sequence of
sentences, δ be an e.c. sequence of positive rational numbers, and p be a P-generable
sequence of rational probabilities. Then
En

“1(φn) · Indδn

Pf(n)(φn) > pn

”

≳n pn · En

“Indδn

Pf(n)(φn) > pn

”

.
Proof. Deﬁne αn := Indδn(Pf(n)(φn) > pn). By Theorem 4.11.3 (Expectations of
Probabilities),
Pf(n)(φn) ≂n Ef(n)(“Pf(n)(φn)”)
and so
Pf(n)(φn) · αn ≂n Ef(n)(“Pf(n)(φn)”) · αn.
Observe that for all W ∈PC(Γ),
W(1(φn)) · αn = W(“1(φn) · αn”).
So by Theorem 4.8.6 (Expectations of Indicators) and Theorem 4.8.10 (Expectation
Provability Induction),
Pf(n)(φn) · αn ≂n Ef(n)(1(φn)) · αn ≂n Ef(n)(“1(φn) · αn”).
117

By two more similar applications of Theorem 4.8.10 (Expectation Provability In-
duction),
Ef(n)(“Pf(n)(φn)”) · αn ≂n Ef(n)(“Pf(n)(φn) · αn”) ≳n pn · Ef(n)(“αn”).
Combining these,
Ef(n)(“1(φn) · αn”) ≳n pn · Ef(n)(“αn”).
By Theorem 4.8.13 (Expectation Preemptive Learning),
En(“1(φn) · αn”) ≳n pn · En(“αn”).
G
Non-Dogmatism and Closure Proofs
G.1
Parametric Traders
Now we show that there is no uniform strategy (i.e., eﬃciently emulatable sequence
of traders) for taking on increasing ﬁnite amounts of possible reward with uniformly
bounded possible losses.
Lemma G.1.1 (Parametric Traders). Let P be a logical inductor over D. Then
there does not exist an eﬃciently emulatable sequence of traders (T
k)k such that for
all k, the set
n
W
P
i≤n T k
i
 P
  n ∈N+, W ∈PC(Dn)
o
of plausible values of T
k’s holdings is bounded below by −1 and has supremum at
least k.
In words, this lemma states that if P is a logical inductor then there is no
eﬃciently emulatable sequence of traders (T
k)k such that each T
k never risks more
than $1, and exposes P to at least $k of plausible risk. To show this lemma, roughly
speaking we sum together scaled versions of some of the T
k so that the sum of their
risks converges but the set of their plausible proﬁts diverges. In this proof only we
will use the abbreviation h(j) := j2j for j ∈N+.
Proof. Suppose for contradiction that such a sequence (T
k)k exists. Deﬁne a trader
T by the formula
Tn :=
X
j:h(j)≤n
T h(j)
n
2j
.
This is well-deﬁned as it is a ﬁnite sum of trading strategies, and it is eﬃciently
computable in n because (T
k)k is eﬃciently emulatable. Then for any time n and
any world W ∈PC(Dn),
W
P
i≤n Ti(P)

= W

X
i≤n
X
j:h(j)≤n
T h(j)
i
(P)
2j


by deﬁnition of T and since T h(j)
n
≡0 if h(j) > n;
=
X
j:h(j)≤n
1
2j W
P
i≤n T h(j)
i
(P)

118

by linearity;
≥
X
j∈N+
1
2j · (−1)
≥−1,
by the assumption that the plausible values W
P
i≤n T h(j)
i
(P)

are bounded below
by −1. Furthermore, for any k ∈N+, consider the trader T
h(k). By assumption,
for some time n and some world W ∈PC(Dn), we have
W
P
i≤n T h(k)
i
(P)

≥h(k) ≡k2k.
Then, by the above analysis, we have
W
P
i≤n Ti(P)

≥1
2k · W
P
i≤n T h(k)
i
(P)

+
X
j∈N+
1
2j · (−1)
≥k2k
2k −1
= k −1.
Thus we have shown that the plausible values W
P
i≤n Ti(P)

of our trader T are
bounded below by −1 but unbounded above, i.e. T exploits the market P. This
contradicts that P is a logical inductor, showing that this sequence (T
k)k cannot
exist.
G.2
Uniform Non-Dogmatism
Recall Theorem 4.6.3:
Theorem 4.6.3 (Uniform Non-Dogmatism). For any computably enumerable se-
quence of sentences φ such that Γ ∪φ is consistent, there is a constant ε > 0 such
that for all n,
P∞(φn) ≥ε.
Roughly speaking, to show this, we will construct a parametric trader using
Lemma G.1.1 by deﬁning an eﬃciently emulatable sequence (T
k)k of traders. Each
trader T
k will attempt to “defend” the probabilities of the φi from dropping too far
by buying no more than k + 1 total shares in various φi when they are priced below
1/(k + 1). If the property doesn’t hold of P, then each T
k will buy a full (k + 1)-
many shares, at a total price of at most −1. But since the φi are all collectively
consistent, there is always a plausible world that values the holdings of T
k at no
less than k+1−1 = k. Then the parametric trader that emulates (T
k)k will exploit
P, contradicting that P is a logical inductor.
Proof. We can assume without loss of generality that for each φi that appears in
the sequence of sentences φ, that same sentence φi appears in φ inﬁnitely often,
by transforming the machine that enumerates φ into a machine that enumerates
(φ1, φ1, φ2, φ1, φ2, φ3, φ1, · · · ). Futhermore, we can assume that φ is eﬃciently com-
putable by, if necessary, padding φ with copies of ⊤while waiting for the enumera-
tion to list the next φi.
119

Constructing the traders.
We now deﬁne our sequence (T
k)k of traders. For
n < k, let T k
n be the zero trading strategy. For n ≥k, deﬁne T k
n to be the trading
strategy
T k
n := (k + 1 −Boughtk
n) · Lowk
n · (φn −φ∗n
n ),
where
Lowk
n := Ind1/(2k + 2)

φ∗n
n <
1
k + 1

and
Boughtk
n :=
X
i≤n−1
∥T k
i ∥mg.
We will make use of the convention for writing the coeﬃcients of a trade:
T k
n[φn] ≡(k + 1 −Boughtk
n) · Lowk
n.
In words, T k
n is a buy order for (k + 1 −Boughtk
n)-many shares of φn, scaled down
by the extent Lowk
n to which φn is priced below 1/(k + 1) at time n. The quantity
Boughtk
n measures the total number of shares that T
k has purchased before the
current time step n.
For some ﬁxed polynomial independent of k, the T
k are uniformly computable
with runtime bounded by that polynomial as a function of n, using the discussion
in A.2 on dynamic programming. Furthermore, T k
n ≡0 for n < k by deﬁnition.
Hence (T
k)k is an eﬃciently emulatable sequence of traders as deﬁned in A.3.4.
Note that by the deﬁnition of T k
n, the magnitude ∥T k
n(P)∥mg of the trade is
bounded by k + 1 −Boughtk
n(P). By deﬁnition of Boughtk
n and by induction on n,
we have that
Boughtk
1(P) = 0 ≤k + 1
as Boughtk
1 is an empty sum, and
Boughtk
n+1(P) =
X
i≤n
∥T k
i (P)∥mg
= Boughtk
n(P) + ∥T k
n(P)∥mg
≤Boughtk
n(P) + k + 1 −Boughtk
n(P)
= k + 1.
In words, T
k never trades more than k + 1 shares in total. Furthermore, since by
deﬁnition Low is always non-negative, we have that
∥T k
n(P)∥mg = |T k
n[φn](P)| = |(k + 1 −Boughtk
n(P)) · Lowk
n(P)| ≥0.
Analyzing the value of T
k.
Fix T
k and a time step n. For any plausible world
W ∈PC(Dn), the value in W of holdings from trades made by T
k up to time n is
W
P
i≤n T k
i (P)

= W
P
i≤n T k
i [φi](P) · (φi −φ∗i
i (P))

=
X
i≤n
T k
i [φi](P) · W(φi)
+
X
i≤n
T k
i [φi](P) · (−Pi(φi)),
by linearity and by the deﬁnition φ∗i
i (P) ≡Pi(φi). We analyze the second term ﬁrst,
which represents the contribution to the value of T
k from the prices of the φi-shares
120

that it has purchased up to time n. We have that
X
i≤n
T k
i [φi](P) · (−Pi(φi))
≥
X
i≤n
∥T k
i ∥mg ·

−
1
k + 1

since ∥T k
i (P)∥mg = T k
i [φi](P) ≥0 and since Pi(φi) ≤1/(k + 1) whenever Lowk
i (P)
is non-zero;
≥−k + 1
k + 1
= −1,
since T
k never purchases more than k + 1 shares. Now consider the value
X
i≤n
T k
i [φi](P) · W(φi)
in W of the stock holdings from trades made by T
k up to time n. Since both W(φi)
and T k
i [φi](P) are non-negative, this value is non-negative. Hence we have shown
that
W
P
i≤n T k
i (P)

≥−1 + 0 = −1,
i.e. the total value of T
k is bounded below by −1.
Furthermore, since Γ ∪φ is consistent, there is always a plausible world W ∈
PC(Dn) such that ∀i ≤n : W(φi) = 1, and therefore
W
P
i≤n T k
i (P)

≥−1 +
X
i≤n
T k
n[φ](P).
Exploitation by the parametric trader.
Now suppose by way of contradiction
that the market P does not satisfy the uniform non-dogmatism property. Then for
every k, in particular the property does not hold for ε = 1/(2k + 2), so there is
some φi in the sequence φ such that P∞(φi) < 1/(2k + 2). Since by assumption
φi appears inﬁnitely often in φ, for some suﬃciently large n we have Pn(φn) ≡
Pn(φi) < 1/(2k + 2), at which point
Lowk
n(P) = Ind1/(2k + 2)

Pn(φn) <
1
k + 1

= 1.
Therefore
T k
n[φn] = (k + 1 −Boughtk
n),
so that
X
i≤n
∥T k
i (P)∥mg = Boughtk
n(P) + k + 1 −Boughtk
n(P) = k + 1.
Thus
W
P
i≤n T k
i (P)

≥−1 + k + 1 = k.
In words, once the price of some φi dips below 1/(2k+2), the trader T
k will purchase
the remaining k + 1 −Boughtk
n(P) shares it will ever buy. Then in a world W that
witnenesses that φ is consistent with Γ, all the shares held by T
k are valued at $1
each, so T
k has stock holdings valued at k + 1, and cash holdings valued at no less
than −1.
121

Therefore each T
k has plausible value bounded below by −1 and at least k
in some plausible world at some time step, and therefore Lemma G.1.1 applies,
contradicting that P is a logical inductor.
Therefore in fact P does satisfy the
uniform non-dogmatism property.
G.3
Occam Bounds
Recall Theorem 4.6.4:
Theorem 4.6.4 (Occam Bounds). There exists a ﬁxed positive constant C such
that for any sentence φ with preﬁx complexity κ(φ), if Γ ⊬¬φ, then
P∞(φ) ≥C2−κ(φ),
and if Γ ⊬φ, then
P∞(φ) ≤1 −C2−κ(φ).
We show the result for φ such that Γ ̸⊢¬φ; the result for Γ ̸⊢φ follows by
considering ¬¬φ and using the coherence of P∞.
Roughly speaking, we will construct an eﬃciently emulatable sequence of traders
(T
k)k where T
k attempts to ensure that Pn(φ) does not drop below 2−κ(φ)/(k + 1)
for any φ. We do this by having T
k purchase shares in any φ that are underpriced
in this way, as judged by a computable approximation from below of 2−κ(φ). The
trader T
k will purchase at most k + 1 shares in each φ, and hence spend at most
$2−κ(φ) for each φ and at most $1 in total. On the other hand, if the market P
does not satisfy the Occam property with constant C = 1/(k + 1), then for some φ
with Γ ̸⊢¬φ, we will have that T
k purchases a full k + 1 shares in φ. Since there
is always a plausible world that values φ at $1, T
k will have a plausible value of at
least $k, taking into account the $1 maximum total prices paid. This contradicts
Lemma G.1.1, so in fact P satisﬁes the Occam property.
To implement this strategy, we will use tools similar to those used in the proof
of Theorem 4.6.3, and the proof is similar in spirit, so we will elide some details.
Proof. Observe that 2−κ(φ) is approximable from below uniformly in φ, since we
can (slowly) enumerate all preﬁxes on which our ﬁxed UTM halts and outputs φ.
Let φ be an eﬃciently computable enumeration of all sentences. Let M be a Turing
machine that takes an index i into our enumeration and takes a time n, and outputs
a non-negative rational number. We further specify that M runs in time polynomial
in i + n, satisﬁes ∀n, i : M(n, i) ≤2−κ(φi), and satisﬁes limn→∞M(n, i) = 2−κ(φi).
Note that since we are using preﬁx complexity, we have P
φ∈S 2−κ(φ) ≤1. (We can
assume without loss of generality that M(n, i) > 0 for all i ≤n, by padding the
sequence φ with φ1 while waiting until M(n, i) > 0 to enumerate φi, using the fact
that our UTM outputs each φ for some preﬁx.)
We deﬁne a sequence of traders (T
k)k. For n < k, deﬁne T k
n to be the zero
trading strategy. For n ≥k, deﬁne T k
n to be the trading strategy given by
T k
n :=
X
i≤n
(k + 1 −Boughtk
n(i)) · Lowk
n(i) · (φi −φ∗n
i ),
where
Lowk
n(i) := IndM(n, i)/(2k + 2)

φ∗n < M(n, i)
k + 1

and
Boughtk
n(i) :=
X
j≤n−1
T k
j [φi].
122

This is similar to the parametric trader used in the proof of Theorem 4.6.3, except
that here on time n, T
k buys any φi when Pn(φi) is too low, and Boughtk
n(i) tracks
the number of shares bought by T
k in each φi individually up to time n.
By
the discussion of dynamic programming in A.2, (T
k)k is an eﬃciently emulatable
sequence of traders. (We use that M(n, i) is pre-computed by the machine that
computes T
k, and hence appears in the feature expression for T k
n as a constant
which is strictly positive by assumption.)
Observe that for each i, by induction on n we have Boughtk
n(i) ≤k + 1, so
that T
k only buys positive shares in the various φi, and T
k only buys up to k + 1
shares of φi. Further, T
k only buys φi-shares at time n if the price Pn(φi) is below
1/(k + 1)-th of the approximation M(n, i) to 2−κ(φ), i.e.
Pn(φi) < M(n, i)
k + 1
≤2−κ(φi)
k + 1 .
Therefore T
k spends at most $2−κ(φi) on φi-shares, and hence spends at most $1 in
total.
Suppose for contradiction that P does not satisfy the Occam property. Then for
every k there exists some φi such that the limiting price of φi is
P∞(φi) < 2−κ(φi)
(2k + 2),
but nevertheless Γ ⊬¬φi. Then for some time step n we will have that Pn(φi) <
M(n, i)/(2k + 2), and hence Lowk
n(i) = 1.
At that point T
k will purchase k +
1 −Boughtk
n(i) shares in φi, bringing Boughtk
n+1(i) to k + 1; that is, T
k will have
bought k+1 shares of φi. Since φ is consistent with Γ, there is some plausible world
W ∈PC(Dn) that values those shares at $1 each, so that the total value of all of the
holdings from trades made by T
k is at least k. By Lemma G.1.1 this contradicts
that P is a logical inductor, so in fact P must satisfy the Occam property.
G.4
Non-Dogmatism
Theorem 4.6.2 (Non-Dogmatism). If Γ ⊬φ then P∞(φ) < 1, and if Γ ⊬¬φ then
P∞(φ) > 0.
Proof. This is a special case of 4.6.4, since κ(φ) > 0 for any φ.
G.5
Domination of the Universal Semimeasure
Theorem 4.6.5 (Domination of the Universal Semimeasure). Let (b1, b2, . . .) be a
sequence of zero-arity predicate symbols in L not mentioned in Γ, and let σ≤n =
(σ1, . . . , σn) be any ﬁnite bitstring. Deﬁne
P∞(σ≤n) := P∞(“(b1 ↔σ1 = 1) ∧(b2 ↔σ2 = 1) ∧. . . ∧(bn ↔σn = 1)”),
such that, for example, P∞(01101) = P∞(“¬b1 ∧b2 ∧b3 ∧¬b4 ∧b5”). Let M be
a universal continuous semimeasure. Then there is some positive constant C such
that for any ﬁnite bitstring σ≤n,
P∞(σ≤n) ≥C · M(σ≤n).
Proof. Let (σi)i be an e.c. enumeration of all ﬁnite strings. Let l(σ) be the length
of σ. Deﬁne
φi := “(b1 ↔σi
1 = 1) ∧(b2 ↔σi
2 = 1) ∧. . . ∧(bl(σi) ↔σi
l(σi) = 1)”
to be the sentence saying that the string (b1, b2, . . .) starts with σi. Suppose the
theorem is not true; we will construct a sequence of parametric traders to derive a
contradiction through Lemma G.1.1.
123

Deﬁning a sequence of parametric traders.
To begin, let A(σ, n) be a lower-
approximation of M(σ) that can be computed in time polynomial in n and the
length of σ. Speciﬁcally, A must satisfy
• A(σ, n) ≤M(σ), and
• limn→∞A(σ, n) = M(σ).
Now, recursively deﬁne
α†
k,n,i :=



Ind
1
4(k+1)

φ∗n
i
A(σi,n) <
1
2(k+1)

if A(σi, n) > 0 ∧n ≥k ∧i ≤n
0
otherwise
β†
k,n,i := min(α†
k,n,i, 1 −γ†
k,n,i)
γ†
k,n,i :=
X
m<n,j≤m
β†
k,m,jφ∗m
j
+
X
j<i
β†
k,n,jφ∗n
j
in order to deﬁne a parametric trader
T k
n :=
X
i≤n
βk,n,i · (φi −φ∗n
i )
As shorthand, we write αk,n,i := α†
k,n,i(P), βk,n,i := β†
k,n,i(P), γk,n,i := γ†
k,n,i(P).
Intuitively,
• αk,n,i is the number of copies φi that T
k would buy on day n if it were not
for budgetary constraints. It is high if Pn obviously underprices φi relative to
M (which can be checked by using the lower-approximation A).
• βk,n,i is the actual number of copies of φi that T
k buys, which is capped by
budgetary constraints.
• γk,n,i is the amount of money T k has spent on propositions φ1, . . . , φn−1 “be-
fore considering” buying φi on day n. We imagine that, on each day n, the
trader goes through propositions in the order φ1, . . . , φn.
Analyzing the sequence of parametric traders.
Observe that T k spends at
most $1 in total, since βk,n,i ≤1−γk,n,i. Now we will analyze the trader’s maximum
payout. Assume by contradiction that P∞does not dominate M. Deﬁne
Purchasedk,n,i :=
X
m≤n
βk,m,i
to be the number of shares of σi that T has bought by time n, and
MeanPayoutk,n :=
X
i∈N+
M(σi)Purchasedk,n,i.
to be the “mean” value of stocks purchased by time n according to the semimeasure
M. Both of these quantities are nondecreasing in n. Now we show that there is
some N such that MeanPayoutk,n ≥k + 1 for all n ≥N:
• Every purchase costing c corresponds to MeanPayoutk,n increasing by at least
c · 2(k + 1). This is because the trader only buys φi when
Pn(φi)
A(σi,n) <
1
2(k+1),
and A(σi, n) ≤M(σi).
124

• For some N, MeanPayoutk,N ≥k + 1. Suppose this is not the case. Since we
are supposing that P∞does not dominate the universal semimeasure, there
is some i such that P∞(φi) <
M(σi)
8(k+1). So we will have Pn(φi) <
M(σi)
8(k+1) for
inﬁnitely many n; let N be the set of such n.
For all suﬃciently high n we have A(σi, n) ≥M(σi)/2, so for all suﬃciently
high n ∈N,
Pn(φi)
A(σi, n) ≤
Pn(φi)
M(σi)/2 ≤
1
4(k + 1)
and so there is some inﬁnite subset N ′ ⊆N for which αk,n,i = 1.
By
assumption, ∀n : MeanPayoutk,n < k + 1, so the trader never has spent
more than $1/2 (using the previous step), so γk,n,i ≤1/2.
This means
βk,n,i ≥1/2, which implies an increase in mean payout MeanPayoutk,n −
MeanPayoutk,n−1 ≥M(σi) > 0.
But this increase happens for inﬁnitely
many n, so limn→∞MeanPayoutk,n = ∞. This contradicts the assumption
that MeanPayoutk,N < k + 1 for all N.
• MeanPayoutk,N is nondecreasing in N, so MeanPayoutk,n ≥k + 1 for all
n ≥N.
Using this lower bound on MeanPayoutk,n, we would now like to show that T k’s
purchases pay out at least k + 1 in some W ∈PC(D∞). To do this, deﬁne
MaxPayoutk,n :=
sup
σ∈B≤N+
X
σ′i preﬁx of σ
Purchased(σi, n)
to be the maximum amount that T k’s purchases pay out over all possible strings
(ﬁnite and inﬁnite). Since M is a semimeasure over ﬁnite and inﬁnite bitstrings,
we have MeanPayout(n) ≤MaxPayout(n). Since each φi is independent of Γ, T
k’s
maximum worth is at least
lim sup
n→∞MaxPayout(ε, n) −1 ≥lim sup
n→∞MeanPayout(ε, n) −1 ≥k + 1 −1 = k.
This is suﬃcient to show a contradiction using Lemma G.1.1.
G.6
Strict Domination of the Universal Semimeasure
Recall Theorem 4.6.6 (Strict Domination of the Universal Semimeasure):
Theorem 4.6.6 (Strict Domination of the Universal Semimeasure). The universal
continuous semimeasure does not dominate P∞; that is, for any positive constant C
there is some ﬁnite bitstring σ≤n such that
P∞(σ≤n) > C · M(σ≤n).
Proof. Consider the sets of codes for Turing machines
A0 := {M | M halts on input 0 and outputs 0}
and
A1 := {M | M halts on input 0 and outputs 1}.
Both of these sets are computably enumerable and disjoint, so by Theorem 4.6.3
(Uniform Non-Dogmatism), P∞assigns positive measure to the set [A] of inﬁnite
bitstrings that encode a separating set for A0 and A1, i.e., a set A such that A∩A0 =
∅and A ⊇A1.
Thus it suﬃces to show that the universal semimeasure assigns 0 measure to [A].
This is a known result from computability theory, using the fact that A0 and A1
125

are recursively inseparable; see for example Kucera and Nies 2011. Here we give an
elementary proof sketch.
Suppose for contradiction that m computes a universal semimeasure and
m([A]) = r > 0; we argue that we can compute some separating set A.
Let
q ∈[4r/5, r] ∩Q. There is some ﬁxed k such that the ﬁnite binary subtree Ak
consisting of ﬁnite preﬁxes of length k of strings in [A] is assigned m(Ak) ∈[r, 6r/5].
On input n, we can run m on the set of strings of length up to n until the
set of extensions of strings in Ak has measure at least q; this will happen because
m([A]) > q. Then we output 0 if the majority of the measure is on strings with nth
bit equal to 0, and we output 1 otherwise. If we output 0 but in fact n ∈A1, then
there is measure at most 6r/5 −2r/5 = 4r/5 on extensions of strings in Ak that are
consistent with separating A0 and A1; but this is impossible, as [A] has measure
r. Likewise if we output 1 then it can’t be that n ∈A0. Thus we have recursively
separated A0 and A1, contradicting that A0 and A1 are recursively inseparable.
G.7
Closure under Finite Perturbations
Recall Theorem 4.6.1 (Closure under Finite Perturbations):
Theorem 4.6.1 (Closure under Finite Perturbations). Let P and P′ be markets
with Pn = P′
n for all but ﬁnitely many n. Then P is a logical inductor if and only
if P′ is a logical inductor.
In short, a trader that exploits P also exploits P′ since all but ﬁnitely many of
its trades are identically valued. The proof mainly concerns a minor technical issue;
we have to make a small adjustment to the trader to ensure that it makes exactly
the same trades against P′ as it does against P.
Proof. Assume there is a trader T which exploits P. We will construct a new trader
T
′ that exploits P′. Fix N large enough that Pn = P′
n for all n ≥N.
We will deﬁne T
′ so that it makes the same trades against the market P′ as the
trader T makes against P. That is, we want that for all n,
T ′
n(P′) = Tn(P).
It is insuﬃcient to set the trading strategy T ′
n equal to Tn for all n. This is because
Tn may inﬁnitely often make diﬀerent trades given the history P′
≤n instead of the
history P≤n. For example, it may be that every day T buys V1(φ)-many shares
in φ against V; in this case if P′
1(φ) ̸= P1(φ), then at each time n, Tn(P′) will
buy a diﬀerent number of shares from Tn(P). Roughly speaking, we will patch this
problem by copying T, but feeding it “false reports” about the market prices so that
it appears to the Tn that they are reacting to P rather than P′.
More precisely, let F be a computable function from feature expressions to
feature expressions, in the expression language discussed in A.2.
For a feature
expression α, we deﬁne F(α) to be identical to α but with all occurrences of an
expression φ∗i for i < N replaced by a constant Pi(φ).
Note that F is eﬃciently computable: by the assumption that Pn = P′
n for all
n ≥N, only ﬁnitely many constants Pi(φ) are needed, and can be hard-coded into
F. Furthermore, F behaves as intended: for any α, we have F(α)(P′) = α(P) (using
a slight abuse of notation, treating α as both an expression and as the feature thus
expressed). This follows by structural induction the expression α, where every step
is trivial except the base cases for symbols φ∗i with i < N, which follow from the
deﬁnition of F. Now we deﬁne
T ′
n :=
X
φ∈S
F(Tn[φ])(φ −φ∗n)
126

for any n. This is eﬃciently computable because Tn and F are both e.c. Furthermore,
for all n ≥N, we have that T ′
n(P′) = Tn(P). Therefore for any n we have that
W
P
i≤n Ti
 P

−W
P
i≤n T ′
i
 P′
≤
W
 P
i<N Ti
 P

−W
 P
i<N T ′
i
 P′ ,
which is a ﬁxed constant, where we use that all terms for i ≥N cancel with each
other. This says that at all times and all plausible worlds, there is a ﬁxed upper
bound on the diﬀerence between the values of T against P and of T
′ against P′.
Thus if
n
W
P
i≤n Ti
 P
  n ∈N+, W ∈PC(Dn)
o
is bounded below but unbounded above, then so is
n
W
P
i≤n T ′
i
 P′  n ∈N+, W ∈PC(Dn)
o
.
Therefore, if some trader exploits P, so that P is not a logical inductor, then some
trader exploits P′, so P′ also fails to be a logical inductor. Symmetrically, if P′ is
not a logical inductor, then neither is P.
G.8
Conditionals on Theories
Theorem 4.7.2 (Closure Under Conditioning). The sequence P(−| ψ) is a logical
inductor over Γ ∪{ψ}. Furthermore, given any eﬃciently computable sequence ψ
of sentences, the sequence
(P1(−| ψ1), P2(−| ψ1 ∧ψ2), P3(−| ψ1 ∧ψ2 ∧ψ3), . . .) ,
where the nth pricing is conditioned on the ﬁrst n sentences in ψ, is a logical inductor
over Γ ∪{ψi | i ∈N+}.
Since P is a logical inductor over Γ, we can ﬁx some particular Γ-complete
deductive process D over which P is a logical inductor, which exists by deﬁnition of
“logical inductor over Γ”. Let D
′ be any other e.c. deductive process. Write
ψ◦
n :=
^
ψ∈D′n
ψ
for the conjunction of all sentences ψ that have appeared in D
′ up until time n. (We
take the empty conjunction to be the sentence ⊤.) Write P
◦to mean the market
(Pn(−| ψ◦
n))n∈N+.
We will show the slightly more general fact that for any e.c. D
′, if the theory
Γ ∪{ψ′ | ∃n : ψ′ ∈D′
n}
is consistent, then P
◦is a logical inductor over the deductive process D
◦deﬁned
for any n by D◦n := Dn ∪D′
n, which is complete for that theory. This implies the
theorem by specializing to the {ψ}-complete deductive process ({ψ}, {ψ}, {ψ}, . . .),
and to the Ψ-complete deductive process ({ψ1}, {ψ1, ψ2}, {ψ1, ψ2, ψ3}, . . .) (where
we pad with ⊤to ensure this sequence is eﬃciently computable).
Roughly speaking, we’ll take a supposed trader T
◦that exploits P
◦and construct
a trader T that exploits P. We’d like our trader T to mimic T
◦“in the worlds where
ψ◦
n is true”, and otherwise remain neutral. A ﬁrst attempt would be to have our
trader buy the combination
φ ∧ψ◦
n −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· ψ◦
n
127

whenever T
◦buys a share in φ. The idea is to make a purchase that behaves like a
conditional contract that pays out if φ is true but only has any eﬀect in worlds where
ψ◦
n is true. That is, the hope is that the price of this combination is 0; in worlds
where ψ◦
n is false, the stock holdings from this trade are valued at 0; and in worlds
where ψ◦
n is true, the stock holdings have the same value as that of purchasing a
φ-share against P
◦.
There are some technical problems with the above sketch. First, the ratio of
probabilities in front of ψ◦
n in the above trade is not well-deﬁned if Pn(ψ◦
n) =
0. We will ﬁx this using a safe reciprocation for the ratio. To avoid having this
aﬀect the performance of T in comparison to T
◦, we will ﬁrst correct the market
using Lemma G.7 (closure under ﬁnite perturbations) so that, essentially, the safe
reciprocation never makes a diﬀerence.
Second, if Pn(φ∧ψ◦
n) is greater than Pn(ψ◦
n), then their ratio Pn(φ∧ψ◦
n)
Pn(ψ◦n)
is greater
than the conditional probability Pn(φ | ψ◦
n) = 1 as deﬁned in 4.7.1 (which is capped
at 1). In this case, our trader T has stock holdings with a diﬀerent (and possibly
lower) value from those of the original trader T
◦exploiting P
◦, and therefore T pos-
sibly has less value overall across time than T
◦, which breaks the desired implication
(that is, maybe the original trader exploits P
◦, but our new, less successful trader
does not exploit P). If we simply replace the ratio with the conditional probability
Pn(φ | ψ◦
n), then when Pn(φ ∧ψ◦
n) > Pn(ψ◦
n), the value of the cash holdings for T
may be non-zero (in particular, may be negative). Instead we will have T cut oﬀ
its trades when both Pn(φ ∧ψ◦
n) > Pn(ψ◦
n) and also T
◦is buying φ; this is no loss
for T relative to T
◦, since in this case T
◦is buying φ at the price of 1, and so is not
making any proﬁt anyway.
We now implement this construction strategy.
Proof. Let D, D
◦, and P
◦be deﬁned as above.
We may assume that the collection of sentences that appear in D
◦is consistent.
If not, then no trader exploits P
◦: for all suﬃciently large n the set of plausible
worlds PC(D◦n) is empty, so the set of plausible values of any trader’s holdings is
a ﬁnite set, and hence bounded above.
We may further assume without loss of generality that there exists a rational
ε > 0 such that Pn(ψ◦
n) > ε for all n. Indeed, by Theorem 4.6.3 (uniform non-
dogmatism), since D
◦is consistent, there is some ε > 0 such that P∞(ψ◦
n) > ε
for all suﬃciently large n. Hence by Theorem 4.2.4 (preemptive learning), we have
lim infn→∞Pn(ψ◦
n) > ε. This implies that there are only ﬁnitely many time steps n
such that Pn(ψ◦
n) ≤ε. Therefore by Lemma G.7 (closure under ﬁnite perturbations),
the market P
′ deﬁned to be identical to P except with P′
n(ψ◦
n) with 1 for all such
n is still a logical inductor, and has the desired property. If we show that P
◦′ is a
logical inductor, then again by Lemma G.7, P
◦is also a logical inductor.
Now suppose some trader T
◦exploits P
◦. We will construct a trader T that
exploits P.
Consider the EF-combination
Buyn(φ) := φ ∧ψ◦
n −
(φ ∧ψ◦
n)∗n
max(ε, ψ◦n
∗n) · ψ◦
n
parametrized by a sentence φ. We write (Buyn(φ))∗n for the expressible feature
that computes the price of the EF-combination Buyn(φ) at time n, deﬁned in the
natural way by replacing sentences with their ∗n duals. Intuitively, this combination
is a “conditional contract” which is roughly free to buy (and valueless) in worlds
where ψ◦
n is false, but behaves like a φ-share in worlds where ψ◦
n is true.
128

Now deﬁne the trader T by setting
Tn :=
X
φ
αn · (Buyn(φ) −(Buyn(φ))∗n)
αn := min

T ◦
n[φ]◦, T ◦
n[φ]◦· Indεn
 (φ ∧ψ◦
n)∗n
max(ε, ψ◦n
∗n) < 1 + εn

εn :=
2−n
max(1, ∥T ◦n∥mg)
where T ◦
n[φ]◦is deﬁned to be the market feature T ◦
n[φ] with every occurrence of a
sub-expression χ∗i for some sentence χ replaced with
max

1,
(χ ∧ψ◦
i )∗i
max(ε, ψ◦
i
∗i)

.
That is, T ◦
n[φ]◦is deﬁned so that T ◦
n[φ]◦(P) = T ◦
n[φ](P
◦), i.e., this market fea-
ture T ◦
n[φ]◦behaves against the market P just as T ◦
n[φ] behaves against the condi-
tional market P
◦. Note that Indεn is a valid expressible feature: Indεn(x < y) :=
max(0, min(1, 2n max(1, ∥T ◦
n∥mg)(y −x))).
The idea is that T will roughly implement the conditional contracts as described
above, and will thus perform just as well against P as T
◦performs against P
◦. The
catch is that it may be that Pn(φ ∧ψ◦
n) > Pn(ψ◦
n), in which case Buyn(φ) will no
longer quite function as a conditional contract, since P◦n(φ) is capped at 1. To
prevent T from losing relative to T
◦, we use αn to quickly stop T from buying once
Pn(φ ∧ψ◦
n) > Pn(ψ◦
n); no proﬁt is lost, as the price of φ for T
◦is in that case just 1.
We now formalize this analysis of the value of the trades made by T against P
according to each term in the above summation and by cases on the traded sentences
φ.
Case 1. First suppose that T ◦
n[φ]◦(P) ≤0 and/or Pn(φ ∧ψ◦
n)/Pn(ψ◦
n) = Pn(φ |
ψ◦
n). Then αn = T ◦
n[φ]◦(P). Let W be any world; using linearity throughout, we
have
W( αn · (Buyn(φ) −(Buyn(φ))∗n) )(P)
= T ◦
n[φ]◦(P) · W(Buyn(φ)(P) −(Buyn(φ))∗n(P))
=
T ◦
n[φ]◦(P) · W

φ ∧ψ◦
n −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· ψ◦
n

−T ◦
n[φ]◦(P) ·

Pn(φ ∧ψ◦
n) −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· Pn(ψ◦
n)

by the deﬁnition of Buy;
=
T ◦
n[φ]◦(P) ·

W(φ ∧ψ◦
n) −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· W(ψ◦
n)

by distribution, and where the cash term simply cancels;
≥
T ◦
n[φ]◦(P) · (W (φ ∧ψ◦
n) −Pn(φ | ψ◦
n) · W (ψ◦
n)) ,
by deﬁnition of Pn(φ | ψ◦
n), and by the assumptions on Pn(φ ∧ψ◦
n), Pn(ψ◦
n), and
T ◦
n[φ]◦(P). Note that if W (ψ◦
n) = 0 then this quantity is 0, and if W (ψ◦
n) = 1 then
this quantity is
T ◦
n[φ](P
◦) · (W (ψ◦
n) −Pn(φ | ψ◦
n)) ,
which is just the value of T ◦
n’s holdings in φ from trading against P
◦.
To lower-bound the value of the −ψ◦
n term by −Pn(φ | ψ◦
n) · W (ψ◦
n), we use
the fact that T ◦
n[φ]◦(P) ≤0, the fact that W (ψ◦
n) ≥0, and the fact that Pn(φ ∧
129

ψ◦
n)/Pn(ψ◦
n) ≥Pn(φ | ψ◦
n); or just the fact that Pn(φ ∧ψ◦
n)/Pn(ψ◦
n) = Pn(φ | ψ◦
n).
(Intuitively: T sells (the equivalent of) φ at the price of Pn(φ∧ψ◦
n)
Pn(ψ◦n) , while T
◦sells φ
at the no greater price of Pn(φ | ψ◦
n); or else T buys (the equivalent of) φ at the
same price as T
◦; and so T does at least as well as T
◦.)
Case 2. Now suppose that T ◦
n[φ]◦(P) ≥0, and also Pn(φ ∧ψ◦
n)/Pn(ψ◦
n) > Pn(φ |
ψ◦
n). Then αn = T ◦
n[φ]◦· Indεn

(φ∧ψ◦
n)∗n
max(ε,ψ◦n
∗n) < 1 + εn

. Let W be any world. We
have:
W( αn · (Buyn(φ) −(Buyn(φ))∗n) )(P)
=
T ◦
n[φ](P
◦) · Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn

· W

φ ∧ψ◦
n −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· ψ◦
n

.
If W(ψ◦
n) = 0 then this quantity is 0. If W(ψ◦
n) = 1, then if we subtract oﬀthe
value of T ◦
n’s holdings in φ from trading against P
◦, we have:
T ◦
n[φ](P
◦) · Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn

· W

φ ∧ψ◦
n −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
· ψ◦
n

−T ◦
n[φ](P
◦) · (W (φ) −Pn(φ | ψ◦
n))
= T ◦
n[φ](P
◦) · Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn

·

W(φ) −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)

−T ◦
n[φ](P
◦) · (W (φ) −1)
by the assumption that W(ψ◦
n) = 1, and since Pn(φ | ψ◦
n) = 1 by the assumption
that Pn(φ ∧ψ◦
n)/Pn(ψ◦
n) > Pn(φ | ψ◦
n);
= T ◦
n[φ](P
◦) ·
 
Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn

−1

· W(φ)
+ 1 −Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn

· Pn(φ ∧ψ◦
n)
Pn(ψ◦n)

by rearranging;
≥T ◦
n[φ](P
◦) ·

Indεn
Pn(φ ∧ψ◦
n)
Pn(ψ◦n)
< 1 + εn
 
1 −Pn(φ ∧ψ◦
n)
Pn(ψ◦n)

since Indεn ≤1, so in the worst case W(φ) = 1;
≥T ◦
n[φ](P
◦) · (−εn) ,
by deﬁnition of Indεn.
Combining the cases.
Now summing over all φ, for any world W such that
W(ψ◦
n) = 1, we have:
W

Tn(P
◦)

−W

T ◦
n(P
◦)

=
X
φ

αn(P
◦) · (Buyn(φ)(P
◦) −(Buyn(φ))∗n(P
◦))

−T ◦
n[φ](P
◦)

φ −P
◦(φ)

≥
X
φ
T ◦
n[φ](P
◦) · (−εn)
since for each φ the corresponding inequality holds by the above analyses;
≥−∥T ◦
n∥mg ·
2−n
max(1, ∥T ◦n∥mg)
130

by deﬁnition of ∥T ◦
n∥mg and of εn;
≥−2−n.
In particular, for any world W ∈PC(D◦n) plausible at time n according to D
◦,
W
P
i≤n Ti(P)

≥W
P
i≤n T ◦
i (P
◦)

−1.
Since T
◦exploits P
◦over D
◦, by deﬁnition the set
n
W
P
i≤n T ◦
i (P
◦)

| n ∈N+, W ∈PC(D◦n)
o
is bounded below and unbounded above. Therefore the set
n
W
P
i≤n Ti(P)

| n ∈N+, W ∈PC(Dn)
o
is unbounded above, since for all n we have D◦n ⊇Dn and hence PC(D◦n) ⊆
PC(Dn).
It remains to show that this set is unbounded below. Suppose for contradiction
that it is not, so there is some inﬁnite sequence {(Wi, ni)} with Wi ∈PC(Dni) on
which the value Wi
P
j≤ni Tj(P)

of T is unbounded below.
We may assume without loss of generality that each Wi is inconsistent with
D
◦. Indeed, if there is no subsequence with this property and with the values of T
unbounded below, then the Wi consistent with D
◦have the corresponding values
Wi
P
j≤ni Tj(P)

≥Wi
P
j≤ni T ◦
j (P
◦)

−1 unbounded below, contradicting that
T
◦exploits P
◦over D
◦. Having made this assumption, there is an inﬁnite sequence
mi with Wi(ψ◦
mi−1) = 1 ∧Wi(ψ◦
mi) = 0 for all i.
We may further assume without loss of generality that for each i, we have ni ≤
mi −1. Indeed, for any n ≥mi, we have by the above analysis that Wi
 Tn(P)

≥0;
in this case replacing ni with mi−1 would only decrease the values Wi(P
j≤ni Tj(P)),
and hence would preserve that this sequence is unbounded below.
In particular, it is the case that ψ◦
mi−1 propositionally implies ψ◦
ni. Because
Wi(ψ◦
mi−1) = 1 and Wi ∈PC(Dni), this implies Wi ∈PC(D◦ni), i.e., Wi was plau-
sible at time step ni according to D
◦. But then we have that the sequence of values
Wi(P
j≤ni Tj(P)) ≥Wi(P
j≤ni T ◦
j (P
◦)) −1 is unbounded below, contradicting that
T
◦exploits P
◦over D
◦.
Thus we have shown that, assuming that T
◦exploits P
◦over D
◦, also T exploits
P over D. This contradicts that P is a logical inductor, so in fact it cannot be that
T
◦exploits P
◦; thus P
◦is a logical inductor over D
◦, as desired.
131

