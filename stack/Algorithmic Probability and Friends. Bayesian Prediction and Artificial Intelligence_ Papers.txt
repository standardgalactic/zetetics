David L. Dowe (Ed.)
Algorithmic Probability
and Friends
Bayesian Prediction
and Artificial Intelligence 
Festschrift
LNAI 7070
Papers from the Ray Solomonoff 85th Memorial Conference
Melbourne, VIC, Australia, November/December 2011
 123

Lecture Notes in Artiﬁcial Intelligence
7070
Subseries of Lecture Notes in Computer Science
LNAI Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Yuzuru Tanaka
Hokkaido University, Sapporo, Japan
Wolfgang Wahlster
DFKI and Saarland University, Saarbrücken, Germany
LNAI Founding Series Editor
Joerg Siekmann
DFKI and Saarland University, Saarbrücken, Germany

David L. Dowe (Ed.)
AlgorithmicProbability
and Friends
Bayesian Prediction
and Artiﬁcial Intelligence
Papers from the Ray Solomonoff 85th Memorial Conference
Melbourne,VIC,Australia, November 30 – December 2, 2011
1 3

Volume Editor
David L. Dowe
Monash University
Faculty of Information Technology
Clayton School of Information Technology
Bldg. 63, Wellington Road
Clayton, VIC 3800, Australia
E-mail: david.dowe@monash.edu
Cover illustration: Ray often made abstract drawings on his pages of notes.
The cover image is from 1970.
© Grace Solomonoff
ISSN 0302-9743
e-ISSN 1611-3349
ISBN 978-3-642-44957-4
e-ISBN 978-3-642-44958-1
DOI 10.1007/978-3-642-44958-1
Springer Heidelberg New York Dordrecht London
Library of Congress Control Number: 2013951870
CR Subject Classiﬁcation (1998): I.2, F.1, H.3, I.4, I.5, H.4
LNCS Sublibrary: SL 1 – Theoretical Computer Science and General Issues
© Springer-Verlag Berlin Heidelberg 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered and
executedonacomputersystem,forexclusiveusebythepurchaserofthework.Duplicationofthispublication
or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location,
in its current version, and permission for use must always be obtained from Springer. Permissions for use
may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution
under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication,
neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or
omissions that may be made. The publisher makes no warranty, express or implied, with respect to the
material contained herein.
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Picture of Ray taken by Grace on March 18, 1984, to accompany the 1985
article, “The Time Scale of Artiﬁcial Intelligence”

Preface
The Ray Solomonoﬀ85th memorial conference was held from Wednesday 30th
November to Friday 2nd December 2011, to honour the work and life of Ray J.
Solomonoﬀ. Ray was not just a pioneer in computer science, artiﬁcial intelligence
and machine learning, he was a visionary whose work is still today increasingly
impacting on the philosophy of science. He was well aware of the human and
societal implications of his work, and he will probably one day also be seen
as a pioneer in statistics, econometrics and (the relatively new terms) knowl-
edge discovery, data mining, terabyte science, data science, big data, and data
management and processing, etc.
The conference was to honour Ray’s 85th birthday in 2011 but, after a full
life well lived (and pioneering inﬂuential research), sadly, Ray died in Decem-
ber 2009. The conference was held at Monash University’s Clayton campus in
Melbourne, Australia.
With a strong multi-disciplinary and international Program Committee in-
cluding 2 Turing Award winners (and a correspondingly strong set of reviewers),
there were 40 submissions (in a variety of areas of Solomonoﬀ’s work), each of
which was reviewed at least twice and of which 30 were accepted as long papers
and 1 as a short paper. These were accompanied by invited (talks and) papers by
none less than Grace Solomonoﬀ, Prof. Leonid Levin (Boston University, U.S.A.)
and Prof. Ming Li (University of Waterloo, Canada).
There were over 40 conference delegates from countries including Australia,
Brazil, Canada, China and England, Finland, France, Germany, Japan, Latvia,
New Zealand, Poland, Sweden, Turkey and U.S.A., with the list of paper au-
thors and co-authors also including Kuwait and Malaysia. (Other papers were
submitted from at least two other countries.) Here and elsewhere, we thank these
contributors, the sponsors [Air Force Oﬃce of Scientiﬁc Research, Asian Oﬃce of
Aerospace Research & Development, Grant number FA2386-11-1-1020 AFOSR
and AOARD; Faculty of Information Technology, Monash University, Australia;
National ICT Australia (NICTA), Australia], the publishers (Springer) and other
contributors.
Electronic computers were involved in the conference in terms of (e.g.) elec-
tronic type-setting, electronic presentations and (in some papers) rapid (faster
than human) computer simulation of statistical and machine learning experi-
ments. In the spirit of Solomonoﬀ’s work, one wonders if, when and how com-
puters might have an increasing - and, ultimately, super-human - involvement
and inﬂuence on such activities. No matter how mathematical, philosophical or
otherwise, and no matter how overtly or subtly, this theme of genuine machine
intelligence underlies almost all (if not all) the papers in this volume. Please en-
joy the various angles and threads of this discussion throughout the conference

VIII
Preface
proceedings, honouring a true pioneer who led by example, taught us so much,
and gave good direction for the work now before you and work to follow.
In choosing text for the front cover (title, inter-title and sub-title), vari-
ous (combinations of) terms were considered - including Universal Turing Ma-
chine (UTM). For an idea of some of the other terms and notions considered,
see the back cover and also the titles - and even the contents - of the con-
tributed papers. In addition to the front cover ﬁgure, see Ray’s famous equation
at http://world.std.com/∼rjs, together with the link there to Ray’s doo-
dles (and perhaps also the use of parts of two of Ray’s doodles by RJD to give
the ﬁgure at www.csse.monash.edu.au/∼dld/RaySolomonoffsVision.html or
www.dowe.org/RaySolomonoffsVision.html). As well as the inside photo of
Ray, see other photos of Ray in some of the contributed papers in the volume,
and also photos of Ray at or linked to from http://world.std.com/∼rjs.
From Ray’s inaugural Kolmogorov lecture in 2003 (see end of sec. 3 of Ray’s
corresponding 2003 paper), following the directions Ray has given us should
largely be “a never ending source of joy in discovery!”. And, among other things,
our work to follow (into the future) perhaps (starting now) includes the devoted
thought and discussion which Ray advocated in 1967 that we have regarding the
problems of the realization of artiﬁcial intelligence – before they arise.
May 2013
David L. Dowe

Organization
Organizing Committee
General and Program Chair
David L. Dowe
Monash University, Australia
Co-ordinator
Dianne Nguyen
Monash University, Australia
Program Committee
Andrew Barron
Statistics, Yale University, USA
Greg Chaitin
IBM T.J. Watson Research Center, USA
Fouad Chedid
Notre Dame University, Lebanon
Bertrand Clarke
Medical Statistics, University of Miami, USA
Peter Gacs
Boston University, USA
Alex Gammerman
Royal Holloway, University of London, UK
John Goldsmith
Linguistics, University of Chicago, USA
Marcus Hutter
Australian National University, Australia
Leonid Levin
Boston University, USA
Ming Li
Mathematics, University of Waterloo, Canada
John McCarthy
(4 September 1927 -
24 October 2011)
Stanford University, USA (Turing Award
winner)
Marvin Minsky
MIT, USA (Turing Award winner)
Kee Siong Ng
ANU & EMC Corp., Australia
David Paganin
Physics, Monash University, Australia
Teemu Roos
University of Helsinki, Finland
Juergen Schmidhuber
IDSIA, Switzerland
William Uther
NICTA and University of New South Wales,
Australia
Farshid Vahid
Econometrics, Monash University, Australia
Paul Vitanyi
Centrum Wiskunde & Informatica (CWI),
Amsterdam, The Netherlands
Vladimir Vovk
Royal Holloway, University of London, UK

X
Organization
Reviewers
David Albrecht
Monash University, Australia
Eric Allender
Rutgers – State University of New Jersey, USA
Luis Antunes
Porto University, Portugal
James Breen
Monash University, Australia
Mike Cameron-Jones
University of Tasmania, Australia
Douglas Campbell
University of Canterbury, New Zealand
Fouad Chedid
Notre Dame University, Lebanon
Bertrand Clarke
University of Miami, USA
Adam Day
University of California, USA
Steven De Rooij
Centrum Wiskunde & Informatica (CWI),
Amsterdam, The Netherlands
Karl Friston
UCL, London, UK
Peter Gacs
Boston University, USA
Alex Gammerman
Royal Holloway, University of London, UK
Toby Handﬁeld
Monash University, Australia
Marcus Hutter
Australian National University (ANU),
Australia
Asad I. Khan
Monash University, Australia
Tor Lattimore
Australian National University, Australia
Ming Li
University of Waterloo, Canada
Kar Seng Loke
Monash University, Malaysia
Enes Makalic
University of Melbourne, Australia
Simon Musgrave
Monash University, Australia
Kee Siong Ng
ANU & EMC Corp., Australia
David Paganin
Monash University, Australia
Ronald Pose
Monash University, Australia
Teemu Roos
University of Helsinki, Finland
Daniel Schmidt
University of Melbourne, Australia
Carl Shulman
University of Oxford, UK
Martin Strauss
University of Michigan, USA
Peter Tischer
Monash University, Australia
Andrea Torsello
Ca’ Foscari University of Venice, Italy
William Uther
NICTA and University of New South Wales,
Australia
Farshid Vahid
Monash University, Australia
Tim van Erven
University of Paris-Sud, France
Joel Veness
University of Alberta, Canada
Gerhard Visser
Monash University, Australia
Vladimir Vovk
Royal Holloway, University of London, UK
John Woodward
University of Nottingham, Ningbo, PRC

Organization
XI
Acknowledgements
We gratefully acknowledge our Sponsors for their support for the Ray
Solomonoﬀ85th Memorial Conference, thank you.
Sponsors
Air Force Office of Scientific Research (AFOSR)
Asian Office of Aerospace Research & Development (AOARD)
Faculty of Information Technology, 
                      
Monash University, Australia 
National ICT Australia, Australia 
A further word of gratitude to Dr Rebecca Robinson and Mrs. Genevieve Oreski
for their excellent combined eﬀorts in proof-reading, formatting and other ad-
ministrative assistance. We further thank Rebecca for her expert assistance with
handling various LaTeX ﬁles. We also gratefully acknowledge Elke Werner and
the team at Springer for the production of these proceedings.

Table of Contents
Introduction
Introduction to Ray Solomonoﬀ85th Memorial Conference . . . . . . . . . . . .
1
David L. Dowe
Invited Papers
Ray Solomonoﬀand the New Probability . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Grace Solomonoﬀ
Universal Heuristics: How Do Humans Solve “Unsolvable” Problems? . . .
53
Leonid A. Levin
Partial Match Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Ming Li
Long Papers
Falsiﬁcation and Future Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
David Balduzzi
The Semimeasure Property of Algorithmic Probability – “Feature” or
“Bug”? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
Douglas Campbell
Inductive Inference and Partition Exchangeability in Classiﬁcation . . . . .
91
Jukka Corander, Yaqiong Cui, and Timo Koski
Learning in the Limit: A Mutational and Adaptive Approach . . . . . . . . . .
106
Reginaldo Inojosa da Silva Filho,
Ricardo Luis de Azevedo da Rocha, and
Ricardo Henrique Gracini Guiraldelli
Algorithmic Simplicity and Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
Jean-Louis Dessalles
Categorisation as Topographic Mapping between Uncorrelated
Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
T. Mark Ellison
Algorithmic Information Theory and Computational Complexity . . . . . . .
142
R¯usi¸nˇs Freivalds

XIV
Table of Contents
A Critical Survey of Some Competing Accounts of Concrete Digital
Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Nir Fresco
Further Reﬂections on the Timescale of AI . . . . . . . . . . . . . . . . . . . . . . . . . .
174
J. Storrs Hall
Towards Discovering the Intrinsic Cardinality and Dimensionality of
Time Series Using MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
Bing Hu, Thanawin Rakthanmanon, Yuan Hao, Scott Evans,
Stefano Lonardi, and Eamonn Keogh
Complexity Measures for Meta-learning and Their Optimality . . . . . . . . .
198
Norbert Jankowski
Design of a Conscious Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
P. Allen King
No Free Lunch versus Occam’s Razor in Supervised Learning . . . . . . . . . .
223
Tor Lattimore and Marcus Hutter
An Approximation of the Universal Intelligence Measure . . . . . . . . . . . . . .
236
Shane Legg and Joel Veness
Minimum Message Length Analysis of the Behrens–Fisher Problem. . . . .
250
Enes Makalic and Daniel F. Schmidt
MMLD Inference of Multilayer Perceptrons. . . . . . . . . . . . . . . . . . . . . . . . . .
261
Enes Makalic and Lloyd Allison
An Optimal Superfarthingale and Its Convergence over a Computable
Topological Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
Kenshi Miyabe
Diverse Consequences of Algorithmic Probability . . . . . . . . . . . . . . . . . . . . .
285
Eray ¨Ozkural
An Adaptive Compression Algorithm in a Deterministic World . . . . . . . .
299
Kristiaan Pelckmans
Toward an Algorithmic Metaphysics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
Steve Petersen
Limiting Context by Using the Web to Minimize Conceptual Jump
Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
318
Rafal Rzepka, Koichi Muramoto, and Kenji Araki
Minimum Message Length Order Selection and Parameter Estimation
of Moving Average Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
Daniel F. Schmidt

Table of Contents
XV
Abstraction Super-Structuring Normal Forms: Towards a Theory of
Structural Induction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
Adrian Silvescu and Vasant Honavar
Locating a Discontinuity in a Piecewise-Smooth Periodic Function
Using Bayes Estimation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
Alex Solomonoﬀ
On the Application of Algorithmic Probability to Autoregressive
Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
Ray J. Solomonoﬀand Elias G. Saleeby
Principles of SolomonoﬀInduction and AIXI . . . . . . . . . . . . . . . . . . . . . . . .
386
Peter Sunehag and Marcus Hutter
MDL/Bayesian Criteria Based on Universal Coding/Measure . . . . . . . . . .
399
Joe Suzuki
Algorithmic Analogies to Kamae-Weiss Theorem on Normal
Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411
Hayato Takahashi
(Non-)Equivalence of Universal Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
Ian Wood, Peter Sunehag, and Marcus Hutter
A Syntactic Approach to Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
426
John Woodward and Jerry Swan
Short Paper
Developing Machine Intelligence within P2P Networks Using a
Distributed Associative Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
Amiza Amir, Anang Hudaya M. Amin, and Asad Khan
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445

Introduction to Ray Solomonoﬀ
85th Memorial Conference
David L. Dowe
Computer Science and Software Engineering, Clayton School of Information
Technology, Monash University, Vic. 3800, Australia
david.dowe@monash.edu
Abstract. This piece is an introduction to the proceedings of the Ray
Solomonoﬀ85th memorial conference, paying tribute to the works and
life of Ray Solomonoﬀ, and mentioning other papers from the conference.
Keywords: Ray
Solomonoﬀ,
Solomonoﬀ,
Solomonoﬀ
memorial,
Solomonoﬀtheory of prediction, algorithmic probability, ALP, conver-
gence, completeness, algorithmic information theory, AIT, Kolmogorov
complexity, non-parametrics, training sequences, technological singular-
ity, realization of artiﬁcial intelligence, dangers.
1
Introduction - and Summary
The Solomonoﬀ85th memorial conference was, unsurprisingly, to pay tribute
to Ray Solomonoﬀ(1926 - 2009) in November - December 2011, in the year of
what would have been Solomonoﬀ’s 85th birthday. As we then moved from this
(2011) to 2012 and the centenary year of Alan Turing (1912 - 1954), and as
we note the (Universal) Turing machine [or (U)TM] [172] and Turing’s notion
of machine intelligence (and his Imitation Game, also known as the “Turing
test”) [173], if we are genuine in following Turing’s legacy then it would be a sad
and unfortunate omission to neglect the inﬂuence of Solomonoﬀ- and also his
contemporaries (e.g., [76][77, p662, col. 2 and p664, col. 1]) and successors.
As per [114], Ray Solomonoﬀwas one of 10 researchers [118] at a conference
at Dartmouth, U.S.A. in 1956 [148, sec. 3.2]. Those in attendance included John
McCarthy (4/Sep/1927 - 24/Oct/2011, whose letter as convenor inviting the
others to the 1956 Dartmouth conference apparently coined the term “artiﬁcial
intelligence” [114, sec. 4][148, sec. 3.2]) and Marvin Minsky (who would also
- like Ray Solomonoﬀ[147] [and [149]] - be an invited speaker at the August
1996 Information, Statistics and Induction in Science [ISIS] conference). Both
McCarthy (in 1971) and Minsky (in 1969) became Turing Award winners, and
both would much later agree to be on the Solomonoﬀ85th memorial Program
Committee.
(For details on so much of Ray Solomonoﬀ’s life, I can only encourage the
reader to read [114] and any other of Grace Solomonoﬀ’s writings on Ray.)
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 1–36, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

2
D.L. Dowe
1.1
Short Summary
Ray Solomonoﬀ(25/July/1926 - 7/December/2009) was a Bayesian ahead of
his time in the 1950s. He was one of 10 people present at the 1956 Dartmouth
conference on artiﬁcial intelligence, 2 of whom would later win Turing Awards
(the Award being inaugurated in 1966).
Ray was the original pioneer, in the early 1960s, of the use of (universal) Tur-
ing machines (using algorithmic information theory and algorithmic probability)
for prediction problems in statistics, machine learning, econometrics and data
mining - seeing this multi-disciplinary connection perhaps before at least half of
these disciplines (not to mention the likes of terabyte science, data science, big
data, data management and processing, etc.) had been named. (This [then] new
area of algorithmic information theory has since become widely known as Kol-
mogorov complexity, despite A. N. Kolmogorov’s independent work having been
less general - with no study of probability and modelling data - and [slightly]
later, most certainly published later than Solomonoﬀ’s technical reports from
1960 and papers from 1964.) Solomonoﬀshared Turing’s vision of genuine ma-
chine intelligence - and, in the 1960s, described how this might occur with the
use of training sequences, later articulating the notion of conceptual jump size
(CJS). Still in the 1960s, Ray was surely one of the ﬁrst to express this important
sentiment: “It would be well if a large number of intelligent humans devote a lot
of thought to these problems before they arise. It is my feeling that the realiza-
tion of artiﬁcial intelligence will be a sudden occurrence.” (Ray would later note
that “Learning to train very intelligent machines should give very useful insights
on how to train human students as well”.)
Despite his position at one of the world’s top universities (M.I.T.) relatively
early in life, Ray got out of the university caper after something like only 9
months. I still remember his voice: “I ... I didn’t like my lack of freedom.” But,
as below, throughout this survey, throughout these conference proceedings and
elsewhere, his research most certainly continued.
Later (again), in the mid-1980s, Ray articulated seven stages (A to G) of
machine intelligence, with what we now refer to as the “technological singularity”
(sometimes also called the “inﬁnity point”, loosely when machines are as smart
as humans) perhaps corresponding to somewhere between stages E and F. At
about the same time (also in the mid-1980s), Ray was involved in the formation
of the uncertainty in AI (UAI) “workshop”, largely in response to a curious
push from certain parts within the U.S.A. AI community to downplay the rˆole
of probability. The (so-called) UAI workshop lives on to this day.
Ray Solomonoﬀwas a pioneer scientist ahead of his time whose work was
worthy of more than a Turing Award (if not also, in time, a Nobel Prize in
Economics). Perhaps more importantly, Ray was a ﬁne human being - not
just brilliant and willing to listen to the ideas of others, but also humble, fun,
principled and with a ﬁrm and gentle resolve, a ﬁrm and gentle independence.
His ability to live frugally, fund himself, do pioneering world-class research and
be happy (see, e.g., footnote 18) is an inspiration. (If you somehow dislike his

Introduction to Ray Solomonoﬀ85th Memorial Conference
3
research enough to want to fund mine, please feel welcome to see the challenge
from [32, end of footnote 79].)
During his life-time and since, all these various aspects of Ray Solomonoﬀ’s
work have caught on and continue to do so in one place or another. There are
many places one can read (of this). The best place to start that I can think of is
most probably the invited talk by Grace Solomonoﬀ[114]. Other papers in this
collection of conference papers are (from) the two other invited talks [82] (by
L. A. Levin, a former research student of A. N. Kolmogorov’s and later close
friend of Ray Solomonoﬀ’s) and [84] (by Ming Li, author of, e.g., [85]), some
longer papers (in no particular order) [98, 30, 54, 67, 168, 75, 57, 89, 78, 200,
201, 5, 170, 107, 88, 87, 169, 27, 71, 53, 80, 92, 47, 166, 96, 112, 113, 50, 103, 20]
and [4].
Below is a sketch of Ray’s work, with some thoughts of things to come.
1.2
(Universal) Turing Machines and Prediction
Re Turing, his (Universal) Turing machine (or [U]TM) [172], his Imitation Game
(or “Turing test”) and (machine) intelligence [173] (and his legacy), Solomonoﬀ
[125, 126, 132] pioneered the innovative use of (Universal) Turing machines [172]
for that part of intelligence [173] which pertains to predictive data analysis and
prediction. If it were not for the (Bayesian) choice of which (universal) TM (of
which there are inﬁnitely many) and [172] the undecidability of the Halting prob-
lem (or Entscheidungsproblem), then −given suﬃcient computational resources
−the universality of UTMs tells us that this gives an optimum amongst com-
putable predictions (again, given suﬃcient computational resources). We follow
Solomonoﬀin later commenting on how optimal inference and optimal prediction
is inherently incomputable1 and2 inherently Bayesian [149, sec. 2, p4].
Many misunderstand the diﬀerence between Solomonoﬀ’s work −which in-
volves a (typically inﬁnite) mixture of theories for the purposes of prediction
(see, e.g., [148, sec. 2.6]) −and the later independent Minimum Message Length
(MML) work of Wallace (and co-authors) starting in 1968 [188, 195, 191, 25, 26,
187, 37, 32, 34], which involves ﬁnding a single theory for the purposes of induc-
tion (or inductive inference, or explanation). (See, e.g., [147][191, sec. 8][193, sec.
4][187, (sec. 4.8, p208 and) sec. 10.1][32, footnotes 223 (and surrounding text),
108 and 18][34, sec. 4.1][37, sec. 6.1.4][35, sec. 4.5, p92], and sec. 4.2.) (Read-
ers might also wish to look at Rissanen’s later3 notion of Minimum Description
Length (MDL) [100, 101, 99] (and [102, 74, 73]), or possibly also at some compar-
isons between MML and MDL [193, abstract][186][192, sec. 3][187, sec. 10.2][26,
sec. 11.4.3, pp272-273][8][32, sec. 0.2.4 p536, sec. 0.2.5, and elsewhere][34, sec.
6.7].) The reader can decide for herself how much Solomonoﬀadvocated the
notion of taking the single best inferred theory (see, e.g., [125, 126, 133, 134]
from the early 1960s, and the earlier [120, perhaps especially early sections] on
1 See, e.g., [149, sec. 2, p2][153, sec. 3, p4].
2 Via the choice of (Universal) Turing Machine.
3 See, e.g., [148, Timeline of Papers Referenced].

4
D.L. Dowe
inductive inference to a single theory [124, sec. 2.6 (especially top of page 10)]),
before he opted to favour combining all theories for the purposes of prediction.
This work being described is −or, at least, will (when more properly noticed)
become −crucial in the ﬁelds of econometrics and statistics (see, e.g., conver-
gence result in sec. 3.1), just as it gains an ever increasing foothold in artiﬁcial
intelligence and machine learning. As another way of emphasising how much
this work is (still, to my mind) ahead of its time, (as per sec. 1.1) this work
would have been worthy of both the Nobel Prize in Economics and the Turing
Award. Quite possibly at least one of those acknowledgements will be given to
a researcher lucky enough to still be living when this work catches on.
So, this (above) is at least one way in which Solomonoﬀtook Turing’s legacy
to lead the way. More on this will be given in sec. 3 and beyond.
1.3
Technological Singularity (and Training Sequences)
Prior to Vernor Vinge [176] and - I am led to believe - independently of both Ulam
(and von Neumann) [174] and I. J. Good [56] (and a seemingly weaker claim of C.
S. Wallace’s [180, pp244-245, one-sentence ﬁnal paragraph][32, bullet-point from
p534 (col 2) to p535 (col 1)]), Solomonoﬀ[136, 143] anticipated a more detailed
realisation of Turing’s vision from Turing’s (“Turing test” or) Imitation game
[173]. Solomonoﬀ[143] not only described the (inﬁnity point or) technological
singularity in 1985, but (as per sec. 1.1) also detailed it in 7 stages (A to G),
where what we regard as the technological singularity occurs perhaps between
stages E and F (the 5th and 6th of the 7 stages).
For some decades, I and surely others have thought that one way to advance
towards the technological singularity is - or would be - via software code that
continually incrementally re-writes itself. We see Solomonoﬀexploring a variant
of this idea not only at least as far back as 1990, but also seemingly while
still in typical full embrace of algorithmic information theory [94, secs. 3, 4 and
perhaps especially 5][151, sec. 7.1 and perhaps elsewhere] (and perhaps [152], and
possibly also elsewhere). Indeed, even back in the early 1960s (1962) and mindful
of probabilities (and information theory), we see Solomonoﬀexploring (self-re-
writing or) self-organizing systems which are guided by correct answers to select
problems (or training) [131, especially secs. 1 and 2]. More speciﬁcally in 1962,
we see this theoretical approach being applied to develop “training sequences”,
“to devise an “intelligent” machine capable of improving its own operation”
[131, sec. 1, 1st page] and whose “results will be more reliable and will look just
as “creative” as the results obtained using partly random choices” [131, sec. 1,
2nd page]. See also, e.g., self-improvement and “improving its own performance”
[135, sec. I, p1687], and the re-write rules (and training sequences) used earlier by
Solomonoﬀ[131][135, sec. II, p1689 col. 2]. Further notes from the 1960s include
“the realization of artiﬁcial intelligence will be a sudden occurrence.” [136, sec.
1] and (again in the same paper) “The Problem of the Ambitious Subordinate”
[136, sec. 6], and “More generally, we would like the machines to learn to improve
their operation after working on a set of problems.” [137, p 8, bottom half, col.
3] (the last comment of which is from 1968 [137]), etc.

Introduction to Ray Solomonoﬀ85th Memorial Conference
5
Having thus brieﬂy outlined a couple of Solomonoﬀ’s major contributions in
secs. 1.1, 1.2 and 1.3, let us now attempt to go into some greater detail.
2
Papers - Beginning in 1950
Historically, in his early-to-mid 20s (from 1950 to 1952), we see Ray’s writing
on properties of random networks within mathematical biophysics (which might
be known nowadays as biocomputing or even bioinformatics) [164, 165, 115],
including mathematical genetics, epidemiology and (from artiﬁcial intelligence)
neural networks.
Still in the early 1950s but in a diﬀerent area of what can still be regarded as
artiﬁcial intelligence, we have “An optically driven airborne chopper” [116] (and
“Photoelectric chopper for guided missiles” [108]) (possibly cf. [32, sec. 0.2.4,
footnote 102, p534]). And then in the mid-1950s with Ray still in his 20s, we
have a captivatingly titled paper - at least partly motivated by a comment of
Fano’s (from the Shannon-Fano code) - showing Ray’s knowledge of physics and
(electrical) engineering which was perhaps the ﬁrst of what would be his many
published ventures into information theory: “Eﬀects of Heisenberg’s Principle on
Channel Capacity” [117], beginning with the words “The limitations imposed
by thermodynamics on the amount of energy necessary to transmit one bit of
information ...”.
Then we come to the abovementioned 1956 conference at Dartmouth, U.S.A.
- perhaps the formal birthplace of the the newly-named ﬁeld of “artiﬁcial intelli-
gence”, with ten keen researchers [114], two of whom would later win the Turing
Award. At least in hindsight, intelligence might be viewed as having several
constituent parts - e.g., (i) rote learning and memory, (ii) logical, mathematical
and deductive reasoning, (iii) inductive inference and predictive ability, and (iv)
planning, etc. Many of Ray’s contemporaries in the mid-1950s were pushing for
the approach (or line) of automating logical and deductive reasoning. Ray’s con-
tribution, a privately circulated 62-page report entitled “An inductive inference
machine” [118], saw his position. Later, modern theories of intelligence (mention
information theory [24] and) seem to increasingly emphasise the importance of
(the similar but subtly diﬀerent [information-theoretic] approaches of) inductive
inference and prediction [38, 39, 64, 86, 40, 58, 79][59]4[69, 70, 80, 41] and the
accompanying need for probability (see also sec. 4.4), but Ray was clearly well
ahead of his time. (As per sec. 1.1, even as late as the 1980s, we saw a curious
push from certain parts within the U.S.A. AI community to downplay the rˆole
of probability. The uncertainty in AI (UAI) “workshop” was apparently born as
a response to this curious push [114, sec. 6].)
Solomonoﬀwould stay with this new-found interest in inductive inference and
prediction for the rest of his life. In the ensuing years after the 1956 conference
[118], he would continue with the notions of inductive learning (of grammar
4 The diﬀerence between inference to the single best theory and prediction (as they
pertain to intelligence) is discussed in [59, p1514], [42] and sec. 4.2.

6
D.L. Dowe
and language), inductive inference and prediction [119, 120]5[122]. (I haven’t
yet found or read [9] from 1957 [although it is conceivably the same as [119]]
or [121] from 1958.) In 1959, well before Bayesianism would become fashion-
able, trendy and (perhaps nowadays) even standard - and again before we heard
much about the probabilistic approach to AI - we see Ray’s work on induc-
tive inference and prediction now introducing probability and Bayesian priors
[124, sec. 2.6]6[123, Appendix I and Appendix II], including, e.g., “We will use
the a priori likelihood of a language as a quantiﬁcation of complexity. Simple
languages have high a priori probability; complex languages have low a priori
probability.” [123, Appendix II, p950]. (Possibly cf. [18, sec. 4][19, Appendix]
[32, sec. 0.3.1, p547, col. 2].)
The scene is now set. The presentation here is surely better read with accom-
paniment by [114]. Of course, the reader with suﬃcient inclination and time is
encouraged to read the relevant Solomonoﬀpapers herself.
Ray once told me in person - in Melbourne in August 1996 (I think) or possibly
in Boston in January 1997 or January 2001 - that 1960 was the year in which
his theory (of algorithmic information-theoretic [inference and] prediction) was
born. And, indeed, so the internal working papers and technical reports began,
with Ray’s bringing what I believe was his then relatively recently acquired
knowledge of Universal Turing Machines (UTMs) [172] to these problems of
inductive inference and prediction.
3
Birth of the Theory in 1960 - and Onwards
So, as in sec. 1.1 and above at the end of sec. 2, Ray’s general theory of (Uni-
versal) Turing Machines and prediction (as summarised in sec. 1.2) was born in
1960 [125, 126] - and, as below, would ﬁrst be published in 1964 [133, 134].
In [125, sec. 14]7[126, sec. 14] on curve-ﬁtting, we see clear mention of inductive
inference to one single explanation [125, sec. 14, p21][126, sec. 14, p18]8, possibly
as a precursor to Minimum Message Length (MML) [188, 189, 191, 187] later in
the 1960s (and the later Minimum Description Length, or MDL, principle [100]).
And, also in these works from 1960, we then see [125, sec. 15][126, sec. 15] on
combining theories.
5 Perhaps especially the early sections of [120] discuss inductive inference to a single
theory. See also footnotes 6 and 8 (and possibly 15) and surrounding text for pointers
to other places where Solomonoﬀconsiders this issue no later than 1960.
6
[124, sec. 2.6 (especially top of page 10)] suggests inductive inference to a single
theory. See also footnotes 5 and 8 (and possibly 15) and surrounding text for pointers
to other places where Solomonoﬀconsiders this issue no later than 1960.
7 Complete with hand-written annotations in the online version.
8 Cf. the earlier [120, perhaps especially early sections] on inductive inference to a
single theory [124, sec. 2.6 (especially top of page 10)]. See also footnotes 5 and 6
(and possibly 15) and surrounding text for pointers to other places where Solomonoﬀ
considers this issue no later than 1960.

Introduction to Ray Solomonoﬀ85th Memorial Conference
7
In 1961, we have that [128, page 1] “The study of inductive inference gives
us one way to evaluate quantitatively the size of the “conceptual jump” that is
involved in the presentation of a particular new idea.”. More work on conceptual
jump size (CJS) would follow. This same paper makes it clear that the approach
using Turing machines is Bayesian [128, sec. 1.3], using a priori probability [128,
sec. 2.5 and Appendix I] (cf. [32, sec. 0.3.1 (Solomonoﬀ) and footnote 225][34,
sec. 2.4, p213 (and sec. 6.5, pp944-946)]). We also see the introduction of the
notion of “signiﬁcant subsequence” (of text of English or another language, music
or other sequences of any other type of sequence of symbols) in information-
theoretic terms [128, sec. 2.3] and the re-iteration that the “coding method
deﬁnes certain special code symbols to be equivalent to certain substrings of
symbols in the original sequence” [130, abstract]9. This is done in part by using
Laplace estimates for (multinomial) Bernoulli sequences [127] (cf. the later [12,
188]). The ideas are then extended and generalised in [130], where we see a
discussion of “Monte Carlo music” [130, sec. VII, p16] and a “penny matching
machine” [130, sec. VII, p17].
As early as 1962, we now see this theoretical approach being applied to develop
“training sequences”, “to devise an “intelligent” machine capable of improving
its own operation” [131, sec. 1, 1st page] and whose “results will be more reliable
and will look just as “creative” as the results obtained using partly random
choices” [131, sec. 1, 2nd page].
A (4-page) comment relating to the mind-body problem, giving a nice analogy
with theories of quantum mechanics and the easier to use consequences that are
chemistry, is given in [129].
Although ﬁrst written down in [125, 126], his formal theory (of algorithmic
information theory and probabilistic prediction) was ﬁrst published in 1964 in
[133, 134]10. The interpretation of Occam’s razor mentioned in [133, sec. 2, p3
and sec. 3, p7] is that of “the more “simple” or “economical” of several hy-
potheses” (cf. [148, sec. 4, p13]), in the same spirit as used in Minimum Message
Length (MML) [91][34, sec. 4]. The ﬁtting of curves from [125, sec. 14][126, sec.
14] is discussed again [133, sec. 2.1, p5]. Solomonoﬀ’s formal theory includes
“System in the form of a model to account for all regularities in the observ-
able universe” [133, sec. 3.2] and “System using a universal machine with all
possible input strings of a ﬁxed length” [133, sec. 3.3]. The 1964 paper11 [133]
(1964a, which had secs. 1-3) continues into [134] (1964b, secs. 4-5) with some
nice combinatorial mathematics [134, sec. 4]. This includes extending work on
9 As a simple case in point, if the term “algorithmic probability” occurs frequently in
a corpus of text, we can write “algorithmic probability (ALP)” the ﬁrst time and
then forever after on (the) subsequent occasions replace it with “ALP”.
10 See footnote 11.
11
Ray once told me that he submitted this 1964 paper as one paper but that the
journal editor at the time said that it was too long as one paper and so had to be
split into two - and so became 1964a [133] and 1964b [134].

8
D.L. Dowe
the multinomial distribution (from [127])12, work on grammatical induction13
from data such as (e.g.) “The Use of Phrase Structure Grammars in Coding ...”
[134, sec. 4.3]14, “A Criterion for “Goodness of Fit” of a PSG to a Particular Set
Of Strings” [134, sec. 4.3.2]15, “How to Find a PSG That Best “Fits” a Given
Set of Strings” [134, sec. 4.3.3]16, and (indeed) “Use of the Theory for Decision
Making” [134, sec. 5]17.
Solomonoﬀ’s survey [135] in 1966 “Some Recent Work in Artiﬁcial Intelli-
gence” mentions “simulation of organic evolution” [135, sec. I, p1687, abstract],
self-improvement and “improving its own performance” [135, sec. I, p1687], and
the re-write rules (and training sequences) used earlier by Solomonoﬀ[131][135,
sec. II, p1689, col. 2] (and possibly also earlier in 1961). This 1966 survey [135]
also mentions work by Evans (“a program for recognizing geometric analogies
such as are used in “intelligence tests””[48, 49]) [135, sec. II (p1690 col. 2 and
later)], which was a precursor to programs which would later pass I.Q. tests [104,
41, 167].
Following the description of [93] (1963) from [134, sec. 5], the description of
[93] from [135, sec. III, p1691] suggests a diﬀerence from Minimum Message
Length (MML).
Solomonoﬀ(1967) [136] more than hints at the technological singularity (which
he returns to discuss in 1985 [143]; and see also his recollection from circa 1952
as published in 1997 [148, sec. 3.2]), with the comment
“It would be well if a large number of intelligent humans devote a lot of thought
to these problems before they arise. It is my feeling that the realization of arti-
ﬁcial intelligence will be a sudden occurrence.” [136, sec. 1]
and (again in the same paper).
“The Problem of the Ambitious Subordinate” [136, sec. 6], in which the ma-
chine has means such as deliberate under-performance in order to obtain control
over its operator. The paper also discusses Dolphin talk [136, sec. 7], with notions
following on Solomonoﬀ’s earlier works on (language learning and) grammar in-
duction and on training sequences.
In “The search for artiﬁcial intelligence” [137] (January 1968), there is again
mention of self-improvement which —
given his work on (self-re-writing or)
self-organizing systems which are guided by correct answers to select problems
(or training) from 1962 [131, especially secs. 1 & 2] through to his last writings
(e.g., [163, sec. 6]) —
I think Solomonoﬀwould join others and me [61] in
12 Cf. the later [12].
13 Perhaps cf. later independent work on grammatical inference of Probabilistic Finite
State Automata (PFSAs) using MML from the 1980s [196][187, sec. 7.1].
14 Indeed, in [150, sec. 1] in 1999, Solomonoﬀrefers to J. Horning (1971) [66] and
Solomonoﬀ(1964b) [134] in saying that “Algorithmic probability gives a criterion
for goodness of ﬁt of such grammars” [66][134, pp 240-251].
15 Cf. footnotes 5, 6 and 8 and surrounding text for pointers to places where Solomonoﬀ
considers the issue of inductive inference to a single theory no later than 1960.
16 PSG is Phrase Structure Grammar.
17 Where this last section, [134, sec. 5], mentions [93] from 1963, seemingly intimating
a possible similarity between [93] and Minimum Message Length [188].

Introduction to Ray Solomonoﬀ85th Memorial Conference
9
(again) relating this to the (inﬁnity point or) technology singularity. From this
January 1968 paper [137], we have: “More generally, we would like the machines
to learn to improve their operation after working on a set of problems.” [137,
p 8, bottom half, col. 3], a section on “Minimal machine” [137, pp10-11] (the
ﬁrst part of which seems to suggest Occam’s razor) and a section on “Complex
concepts” [137, p11, col. 2] mentioning “the capacity for self improvement”.
Ray had a break from publishing from the rest of 1968 until some time in 1975
(see, e.g., http://world.std.com/∼rjs/pubs.html)18. Co-incidentally and perhaps
intriguingly to some, this is exactly the same period in which the ﬁrst Wallace
and Boulton Minimum Message Length (MML) papers [188, 12, 13, 10, 16, 15,
14, 17, 189, 11] independently appeared —
from 1968 [188] to 1975 [189, 17,
11]. Wallace and Boulton would not become aware of the work of Solomonoﬀ,
Kolmogorov [76, 77], Chaitin [22, 21] and others until about 1975 — my un-
derstanding is that this was apparently from an article (involving Greg Chaitin)
in Scientiﬁc American in circa 1975 [23] (cf. a not unrelated comment in [114]).
The ﬁrst Minimum Description Length (MDL) paper would appear before the
end of the 1970s in 1978 [100] (although some might contend 1976 [99]).
3.1
End of the 1970s, and Fundamental Convergence Result
Resuming in 1975, we see the claim that “Goodman’s paradoxes involving various
linguistic transformation of inferential data, are also easy to analyse from this
point of view” [138, p20, col. 1] - a point which Ray re-iterates later regarding
Goodman’s “grue” paradox in [147][149, sec. 5]. For related discussion of the grue
paradox from (algorithmic) information theory, see also, e.g., [26, sec. 11.4.4][32,
footnotes 128, 184 and 227][34, sec. 7.2].
The survey in [139] mentions that Ray’s approach (of algorithmic information
theory and probabilistic prediction) is Bayesian [139, Introduction and sec. 1]
and, then, almost in passing, mentions the key convergence result [139, sec. 1]
E(m
i=1(δ′
i −δi)2) ≤b log
√
2 = (b/2) log 2
This then becomes the crucial, key, important and currently under-appreciated
convergence result [140, p426, col. 1, Theorem 3 (17)] (1978)
EP (n−1
i=1 (δn
i −δn′
i )2) ≤k log
√
2 = (k/2) log 2
18 As well as wondering about and perhaps envying Ray’s work-life balance (and joie
de vivre without unnecessary expense), I’m reminded of a story Ray once told me
about his wanting to learn to surf. At the time, he wasn’t sure whether this story was
from when he was 35 (putting it at about 1961) or 45 (putting it at about 1971). He
described deciding that, if one wanted to learn to surf, one should go to California,
and, so, Ray being Ray, he hitch-hiked from the U.S. east coast to California. When
he got to California, he apparently learnt that, if one really wanted to learn to surf,
then one had to go to Hawai’i. As able and resourceful as so many of us know Ray
was, I still sometimes wonder how he might have hitch-hiked or otherwise got across
the Paciﬁc from California to Hawai’i. I don’t think he told me what happened next.

10
D.L. Dowe
(and occurred in the same year as what many people regard as the ﬁrst paper
on Minimum Description Length [100]). (Perhaps see also the more recent [68].)
This fundamental convergence result (which should perhaps intrigue or be core
to those from statistics, econometrics, machine learning, etc.) takes us to the
end of the 1970s. It also suggests connections with other convergence results
pertaining to Minimum Message Length (MML) [195, sec. 2, p241][7][184][187,
sec. 3.4.5] and (because of the connection that both approaches - ALP19 and
MML - have with algorithmic information theory [191][187, chap. 2]) might pro-
vide insight into at least two related conjectures (pertaining to Strict MML and
approximations such as those from [32, footnotes 62-65 and surrounds]) [35, sec.
6, p93][46, sec. 5.3][191, p282][194, sec. 5, p78][26, sec. 11.3.1, p269][37, sec. 8][32,
sec. 0.2.5][33, p454][34, sec. 6.5, p945].
3.2
Notes on Papers from the 1980s
Continuing with the abovementioned earlier work on training sequences (“an
ordered set of problems arranged so that a student or program solving each
problem becomes more able to solve subsequent problems in the set” [141, ab-
stract]), [141] (re-)introduces the notions of conceptual jump size (CJS) (“a
measure of the computational cost needed to solve a problem, given that certain
information has already been acquired” [141, abstract][128, page 1], and see also
[146, sec. 4][148, secs. 4.2 and 4.3][103]) and perfect training sequences (“having
characteristics that make it particularly easy for certain kinds of machines to
solve them”, such as the CJS of each problem in the sequence being acceptably
small [141, abstract] - perhaps cf. [61, Deﬁnitions 16 and 11]).
“Optimum Sequential Search” [142] (1984) re-visits the part of [81] pertaining
to what we now know as universal Levin search, or universal Lsearch - a universal
optimisation algorithm. The paper then builds on [81] to (re-)raise two issues:
“Given the ﬁnite string x, how can we ﬁnd in minimal time, a string p such that
M(p) = x?” and “The problem is to ﬁnd within a limited search time, τ, an
input that yields the highest possible value of G.” He later discusses the notions
of “resource limited ALP” [147] (or resource bounded probability [RBP]) [149].
I haven’t been able to locate [144], but “The Time Scale of Artiﬁcial Intelli-
gence ...” (1985) [143] is perhaps the main paper on the technological singularity,
following on [136] as discussed in sec. 1.3.
As per secs. 1.1 and 2, even as late as the 1980s, we apparently saw a curious
push from certain parts within the U.S.A. AI community to downplay the rˆole
of probability. The uncertainty in AI (UAI) “workshop” - which lives on strong
almost 3 decades later - was apparently born in 1986 as a response to this curious
push [114, sec. 6]. Ray says there20 that extreme value theory would probably
be better done using algorithmic information theory: “Certain scientists have
19 ALP is algorithmic probability.
20 These papers from the 2000s [151, 152, 153] cite the 1986 paper [145] as having been
in the inaugural 1986 UAI workshop - with [151] and [152] (both from the 2000s)
possibly being diﬀerent versions of the same paper.

Introduction to Ray Solomonoﬀ85th Memorial Conference
11
expressed much conﬁdence in their estimates of probability of catastrophic failure
in nuclear reactors, national defense systems, and in public safety regulations
for genetic engineering. The aforementioned considerations lead one to question
this conﬁdence.” [145, sec. 1]. It would be hard to agree more with this21. In
[145, sec. 3.3] while discussing compressing “the information in our probability
distribution”, regarding training sequences22, Ray comments: “Newton’s laws
were much easier to discover as an outgrowth of Kepler’s laws, than it would
be for Newton to derive them directly from purely experimental data” and that
this process of developing and pursuing training sequences “gives us the closest
thing to true creativity that we can expect in a mechanized device”. Still in the
same paper, [145, sec. 4.3] discusses the concept of “Analogy” from intelligence
tests [48, 49] in the context of algorithmic information theory (cf. [38, 39, 64,
40, 58, 86, 104] and sec. 4.4), and [145, sec. 4.4] discusses clustering in terms of
algorithmic information theory (cf. [188] (especially) and other papers detailing
how to do this by Minimum Message Length (MML) [181, 183, 182, 190, 185,
46, 45, 194][187, sec. 6.8][90, 178, 179]).
We see more on training sequences in [146], with “The machine starts out like
a human infant ...”, “A suitable training sequence of problems ...”, and “The
principal activity of the present research is the design of training sequences
of this kind.” [146, Introduction]. This paper continues: “Perhaps of most im-
portance: If there is any describable regularity in a body of data, algorithmic
probability is guaranteed to eventually describe that regularity ... [140]. It is
the only deﬁnition of probability known to have this property.” [146, sec. 1]. I
certainly agree with the convergence results from [140, p426, col. 1, Theorem 3
(17)] referred to here, but I would also point to the MML convergence results
and conjectures from the end of sec. 3.1 (at least partly because of the connec-
tion that both approaches - ALP and MML - have with algorithmic information
theory [191][187, chap. 2]). Any statisticians, econometricians, machine learners,
data miners, epistemologists and/or philosophers of science reading this, please
take note.
In [146, sec. 4, “How inversion problems are solved”], Ray re-visits the notion
of Conceptual Jump Size (CJS) [141][148, secs. 4.2 and 4.3][128, page 1][103] as
follows. Relevant abridged notes include: “The most eﬃcient search procedure -
one whose expected time to solution is minimal - tests trial programs (i.e., strings
of concepts) in order of increasing ti/pi. Here pi is the probability of success of
the ith trial string of concepts and ti is the time needed to generate and test
that trial. ... If pj is the probability assigned to a particular program, Aj, that
solves a problem and it takes time tj to generate and test that program, then
this entire search procedure will take a time less than 2tj/pj to discover Aj. We
call tj/pj the “Conceptual Jump Size” (CJS) of Aj. It tells us if the machine is
21 A couple of decades or so after this 1986 paper, I have seen able well-funded people
collaborating with large groups of people on disaster-averting projects of extreme
value theory - but whose models (last time I checked) weren’t then attempting to
embrace anything near the generality oﬀered by algorithmic information theory.
22 An idea Ray was developing in 1962 [131, especially secs. 1 & 2], as per sec. 1.3.

12
D.L. Dowe
practically able to ﬁnd a particular solution to a problem at a particular stage of
its development. CJS is a critical parameter in the design of training sequences
...” [146, sec. 4]. The paper goes on [146, sec. 6, “How Training Sequences Are
Written”] : “... The task of designing such sequences is very similar to writing
“Top Down” computer programs or writing lesson plans for human students.
... In the early stages of its training, the machine will be given only problems
of small CJS (Conceptual Jump Size) - problems that are easy for it to solve.”
[146, sec. 6].
3.3
Notes on Papers from the 1990s
Ray did not co-author many papers, but “Autonomous Theory Building Sys-
tems” [94] from 1990 was one of them23, and it included [94, sec. 4, “Training
plans”]. Then, as he did from 1968-1975 (as per sec. 3, and perhaps recall foot-
note 18), Ray took several years in the early 1990s oﬀpublishing.
As mentioned in sec. 1, Ray Solomonoﬀwas then an invited speaker at the
Information, Statistics and Induction in Science (ISIS) conference in August
199624, where he published [147], which was spelt out and elaborated upon into
[149], entitled “Does algorithmic probability solve the problem of induction?”.
This includes mention of Occam’s razor [149, sec. 2, p2] (see also [148, sec. 4,
p13]), which then leads into a re-statement of the claim from [146, sec. 1] (recall
earlier in sec. 3.2, and which we repeat later in this section) about [140] - namely,
that: “ALP is the only induction technique known to be complete.
By this we mean that if there is any describable regularity in a body of data,
ALP will discover it using a relatively small sample of the data25. As a necessary
consequence of its completeness, this kind of probability must be incomputable.
Conversely, any computable probability measure cannot be complete.”. The com-
ments “one cannot do prediction without a priori information” and “makes ALP
very subjective” from [149, sec. 2, p4] remind one of the inherent Bayesianism of
this approach (and perhaps also of Ray’s Bayesianism going back to the 1950s
[124, sec. 2.6] - and see also, e.g., [155, sec. 4]). Goodman’s “grue” paradox
from [138, p20, col. 1][147] is discussed again in [149, sec. 5, p8]. For those of
us who quibble about the use - or confuse the meaning(s) - of terms such as
induction, inductive inference, explanation and prediction (e.g., [34, sec. 4.1]), I
think it appropriate to have a careful read of [147][149, sec. 7]. The last comment
from [149, sec. 7] “Scientiﬁc laws are usually compactly expressed, enabling easy
communication of good ideas” is later echoed in [42].
Solomonoﬀ(1997), “The Discovery of Algorithmic Probability” [148], opens
with the words: “This paper will describe a voyage of discovery −the discovery
23 As were [164, 165, 108, 9], as might possibly have been [95], and as is [166].
24 Also attended by Jorma Rissanen, Paul Vitanyi, Chris Wallace [184] and 1969 Turing
Award winner, Marvin Minsky - see also [26, sec. 11.4.4].
25 Here the text from [149, sec. 2, p2] goes to its [149, footnote 1], which says: “1 See
Appendix A for The Convergence Theorem, which is a more exact description of
“completeness”. Sol78 contains a proof of the theorem.”, where Sol78 is [140].

Introduction to Ray Solomonoﬀ85th Memorial Conference
13
of Algorithmic Probability”. Although Ray seemed both above it and good at
avoiding (or ignoring) it (cf. sec. 1.1), I can’t help but notice the passing ob-
servation of “political skullduggery” [148, sec. 1]26. Further highlights from this
survey and recollection include “without explicitly considering various theories
or “explanations” of the data” [148, sec. 2.6 (Carnap)], and meeting Marvin Min-
sky and John McCarthy (both later Turing Award winners) in 1952, persuading
“Minsky that our machines would eventually go well beyond human capabilities”
and “In 1956, McCarthy and Shannon organized the ‘Summer Study Group in
Artiﬁcial Intelligence’ at Dartmouth - ...” [148, sec. 3.2 (Minsky and McCarthy)].
Also of interest is the modiﬁcation of the Turing machine to have a “unidirec-
tional output tape, a unidirectional input tape” [148, sec. 3.4 (and Appendix A.1,
footnote 12)] (see also [163, sec. 1]). We also see “the idea of Occam’s razor - that
“simple” hypotheses are more likely to be correct” [148, sec. 4, p13], mention
[148, sec. 4.2 (Levin)] that Dean, Thomas and Boddy (1988) [29] have coined
the term “anytime algorithm” (as was much later used in [59]), and discussion
of Conceptual Jump Size and (relatedly) T/P values [148, secs. 4.2 and 4.3].
There is further discussion of training sequences and discussion of training envi-
ronments in [148, sec. 4.3]. The paper then goes on in [148, sec. 5.2.1] to repeat
earlier claims from [146, sec. 1][149, sec. 2, p2], stating that algorithmic probabil-
ity “is the only induction system we know of that is “complete”. By this we mean
that if there is any describable regularity in a body of data, Algorithmic Proba-
bility is guaranteed to discover it using a relative small sample of the data” [148,
sec. 5.2.1]. There is further discussion in [148, sec. 5.2.2]. The reader is referred
to sec. 3.1 and the results referred to from [195, sec. 2, p241][7][184][187, sec.
3.4.5] (and possibly also [34, sec. 6.5] and its conjectures, and [35, p93][46, sec.
5.3][191, p282][194, sec. 5, pp539-540][26, sec. 11.3.1, p269][37][32, sec. 0.2.5][33,
sec. Statistical Consistency]) regarding the statistical consistency of Minimum
Message Length (MML). The paper [148] also mentions notions of “sequential
property”, “process” and “monotonic machine” [148, Appendix A.1, footnote
12] and a timeline of relevant research up to 1997 [148, Timeline] (see also [77,
p662, col. 2 and p664, col. 1]).
We deal with sequential prediction and sequentially ordered data in [150, secs.
1 and 2][160, sec. 1] and unordered data and unordered sets (of ﬁnite strings) of
data in [150, sec. 3][160, sec. 2] - both in 1999 and again later [150, 160] (and we
later [160, sec. 3] also re-visit earlier work - such as [141][152, secs. 1 and 4 and
Appendix B] - on “Q, A” question and answer [training sequences and] operator
learning).
3.4
Notes on Papers from the 2000s
Moving into the 2000s, as the title suggests, “Progress in incremental machine
learning” (2002) [151] (revision 2.0, 30 Oct 2003, so really [152]) is something of
a survey of (incremental) [machine] learning by training, with [152, secs. 1 and
4 and Appendix B] continuing the “Q,A” (question and answer) scheme from
26 Cf. political observation next to footnote 28, and also text near footnote 27.

14
D.L. Dowe
(e.g.) [141][160, sec. 3]. The paper also includes mention of (improving) universal
Lsearch (universal Levin search) (e.g., [152, secs. 2.1 and 3.1]). Next, [152, sec. 3
(Time Limited Optimization Problems)] relates at least somewhat to “resource
limited ALP” [147] (or resource bounded probability [RBP]) [149]. Then there
is discussion of training sequences [152, sec. 5], (updating) eﬃciency (and prob-
lem solving techniques [PSTs]) [152, sec. 6] and a comment that Schmidhuber’s
OOPS [105] could do a little “edit”ing of old programs [152, sec. 7.2].
Next, in 2003, Ray had the honour of giving the inaugural Kolmogorov lec-
ture: “The universal distribution and machine learning” [153], which brings us
a survey of the notion of universal distribution, underpinning most of Ray’s
work since the seminal [133, 134, 125, 126]. It begins [153, sec. 1 (Universal
Probability Distribution)] “I will discuss two main topics in this lecture: First,
the Universal Distribution and some of its properties: its accuracy, its incom-
putability, its subjectivity. Secondly, I’m going to tell how to use this distribution
to create very intelligent machines.” The paper contains some excellent quota-
tions, two of which [153, secs. 3 and 5] are so memorable that I’ve repeated
them verbatim below (where the works being referred to at the end of these are
[145][146][151, 152]).
From [153, sec. 3], “Many scientists are repeatedly disturbed by the need
to revise their understanding of their sciences. They look forward to a “Final
Theory” that will put an end to all revisions. However, the incomputability
of the Universal Distribution assures us that this cannot happen. With ﬁnite
computing resources, we can never be certain that we’ve found the best, the
“Final Theory”.
I, personally, am not disturbed by this state of aﬀairs, but ﬁnd it instead to
be a never ending source of joy in discovery!”
And, from [153, sec. 5] about a couple of pages later, “About 1984, roughly
25 years later, at an annual meeting of the American Association for Artiﬁcial
Intelligence (AAAI), a vote was taken and it was decided that probability was
in no way relevant to artiﬁcial intelligence.
A protest group quickly formed at the AAAI meeting devoted to Probability
and Uncertainty in A.I. ...
As part of the protest at the ﬁrst workshop, I gave a paper on applying the
universal distribution to problems in A.I. (Sol 86). This was an early version of
the system that I’ve been developing since that time (Sol89, 02).”
These memorable words speak for themselves, but I’ll presume to comment.
We see Ray’s using his work (algorithmic information theory, undecidability, pre-
diction, inference, etc.) to show that (even in a world of ongoing and often myopic
funding cuts) [and I paraphrase] even if we might hypothetically have found the
“ﬁnal” “best” model(s), we can never be sure, and there is a never-ending need
to keep (joyously) looking. We also see (what I shall call) the technical activist27
in him emerging when he thought the relevant community had lost the plot, and
we also see some evidence of his joy.
27 Cf. political observations next to footnotes 26 and 28.

Introduction to Ray Solomonoﬀ85th Memorial Conference
15
In 2005, Ray gave a recorded lecture [154] at the Midwest NKS conference
and two lectures at M.I.T. [155, 156].
The ﬁrst M.I.T. lecture [155] mentions reasons why we haven’t (yet) reached
the technological singularity [155, Introduction] and the related matter of train-
ing sequences (recall sec. 1.3), as well as Levin search [155, Introduction] and
the subjectivity [155, sec. 4] of algorithmic probability (ALP) (recall [149, sec. 2,
p4], sec. 3.3, and [153, sec. 1]). The second M.I.T. lecture [156] makes the point
about the merits of letting all theories contribute (as ALP does) [and comments
that “crowds ... become very stupid in mobs or in committees in which a single
person is able to strongly inﬂuence the opinions in the crowd” 28] by discussing
[156, sec. 1 (ALP and “The Wisdom of Crowds”)], as well as discussing gram-
matical inference [156, sec. 3 (Context Free Grammar Discovery)] (recall secs. 2
and 3).
The following year, we have [157] - with discussion of conceptual jumps [157,
Abstract and sec. 3] (recall [141][146, sec. 4][148, secs. 4.2 and 4.3][128, page
1]), (Prediction by Partial Matching) “PPM and Grammar Discovery” [157, Ab-
stract] (recall secs. 2 and 3 and [156, sec. 3 (Context Free Grammar Discovery)]),
the “Q, A” (question and answer) formalism [156, sec. 3 (SA, The Scientist’s As-
sistant)] (recall [141][152, secs. 1 and 4 and Appendix B] and perhaps see [160,
sec. 3]) and “How far are we from serious A.I.?” [156, sec. 5 (The future of A.I.)].
From 2007, I couldn’t ﬁnd [158], but [159] raises the interesting question of
the probability that a UTM, given random input, will not generate any output
beyond a certain point: “a probability, Pi+1(u) that the continued random input
string will not generate any output for the i + 1th symbol” [159, Abstract]. This
and related concepts have also been considered by others [187, sec. 10.1.3][20].
If we consider the halting probability (the probability that a machine given an
inﬁnite random input [with each bit having probability of 0.5 of 0 and 1, and
i.i.d.] will halt), the inﬁnite loop probability (the probability that some ﬁnite
preﬁx of random input will cause the machine from that point on not to be able
to terminate - which relates to the abovementioned Pi+1(u) [159, Abstract])
and the universality probability [32, footnote 70][34, sec. 2.5][6][61, p181 and
footnote 11], then, because any program which has gone into an inﬁnite loop
or halted has lost its universality, we have that these three probabilities add
up to less than (or equal to) 1. (Related ideas are explored with a view to
biological and immunological analogies with halting as (cell) death, inﬁnite loop
as cancer and (loss of) universality as (loss of) full cell function in [60, footnote
4]. This includes the possibly immunologically analogous study of sets of strings
which do or don’t cause halting, inﬁnite loop[ing], loss of universality or some
depletion of function[s] to a set of [universal] Turing machines29.) The appendix
[159, Appendix] also mentions (universal) “monotonic machines and associated
probability distributions” (recall [148, Appendix A.1, footnote 12]).
28 Cf. political observation next to footnote 26, and also text near footnote 27.
29 With or without the notion of redundant TMs from sec. 4, footnote 31 and [32,
sec. 0.2.7, p544, col. 2][59, p1514, footnote 6], one might further wonder whether a
suitable analogy can be made of the reparative value of stem cells in this framework.

16
D.L. Dowe
As in sec. 3.3 above, Solomonoﬀ(2008) [160] follows on from [150] (including
mentioning approximations [160, Introduction] in passing), but then [160, sec.
3] also re-visits earlier work - such as [141][152, secs. 1 and 4 and Appendix B]
- on question and answer “Q, A” [training sequences and] operator learning.
Next is Solomonoﬀ(2009) [161], the last of Ray’s papers published before his
death in December 2009. After that would follow [162], [163] and a joint pa-
per written up after Ray’s death [166]. Discussion of “Compression and ALP”
is given in [161, sec. 2]. The (so-called) “elusive model paradox” [32, footnote
211][33, p455][34, sec. 7.5][110][83][42, sec. 2.2][62, footnote 9] is raised and dis-
cussed in [161, sec. 3], with the elusive model paradox’s “red herring sequence”
[34, sec. 7.5] also discussed in [162, Appendix B].
We also have that “This lack of an objective prior distribution makes ALP
very subjective - as are all Bayesian systems.” [161, sec. 4 (Subjectivity)], from
which we can recall [149, sec. 2, p4], sec. 3.3 and [153, sec. 1][155, sec. 4]. We
then follow [156, sec. 1 (ALP and “The Wisdom of Crowds”)] (and footnote 28)
with [161, sec. 5 (Diversity and Understanding)] and [161, sec. 5.1 (ALP and
“The Wisdom of Crowds”)]. We then follow on unordered data and unordered
sets (of ﬁnite strings) of data [150, sec. 3][160, sec. 2] in [161, secs. 7.1 and
7.2], leading into [161, sec. 9 (Context Free Grammar Discovery)] (recall secs.
2 and 3 and [156, sec. 3 (Context Free Grammar Discovery)][157, Abstract], cf.
[196][187, sec. 7.1]). Penultimately, following [81][142][152, secs. 2.1 and 3.1][155,
Introduction], we have more discussion of universal Levin search (Lsearch) [161,
sec. 10]. Finally, [161, sec. 11 (The future of ALP - some open problems)] brings
many of the above issues and training sequences together, and is clearly good
reading (from the proverbial horse’s mouth) for budding researchers. It begins
with the words “We have described ALP and some of its properties:
First, its completeness: Its remarkable ability to ﬁnd any irregularities in an
apparently small amount of data.
Second: That any complete induction system like ALP must be formally
incomputable.”.
Recall sec. 3.1 and [146, sec. 1][149, sec. 2, p2][148, sec. 5.2.1][140] regard-
ing completeness of ALP, and also the likes of [195, sec. 2, p241][7][184][187,
sec. 3.4.5][34, sec. 6.5][35, p93][46, sec. 5.3][191, p282][194, sec. 5, pp539-540][26,
sec. 11.3.1, p269][37][32, sec. 0.2.5][33, sec. Statistical Consistency] re statisti-
cal consistency (or completeness) of Minimum Message Length (MML), whose
relationship with algorithmic information theory (and undecidability and incom-
putability) is described in (e.g.) [191][187, chap. 2].
Ray was to present [162] in early 2010, but died in late 2009. The section
heading “Subjectivity” makes familiar points (e.g., [128, sec. 1.3][128, sec. 2.5
and Appendix I][139, Introduction and sec. 1][149, sec. 2, p4], sec. 3.3 and [153,
sec. 1][155, sec. 4][161, sec. 4 (Subjectivity)] and the later [163, sec. 3 (Subjectiv-
ity)]), as does the section heading “Diversity” (e.g., sec. 1.2, text near footnote
28, [156, sec. 1 (ALP and “The Wisdom of Crowds”)][161, sec. 5 (Diversity and
Understanding)][161, sec. 5.1 (ALP and “The Wisdom of Crowds”)], the later
[163, sec. 4 (Diversity)] and perhaps also [147][149, sec. 7]) re using a weighted

Introduction to Ray Solomonoﬀ85th Memorial Conference
17
mixture of theories - rather than a single theory - for prediction. With other
section headings (like) “Putting it all together”, “The Guiding Probability Dis-
tribution” (GPD) and “Training Sequences” (mentioning Conceptual Jump Size
[CJS], and see also [163, sec. 6 (Training Sequences)]), the paper could be sum-
marised as telling of using universal Levin search (Lsearch) [162, Appendix A]
with a GPD to help construct training sequences.
The ﬁrst words and the last words (both in quotations, and separated) of the
conclusions in this paper from [162, sec. “In Conclusion”] are:
“We have a method for designing training sequences. We have a method for
updating the guiding probability distribution. We have a method for detect-
ing/measuring “learning” in the system.
These three techniques are adequate for designing a true AGI.” and “... en-
abling the system to signiﬁcantly improve itself.” [162, sec. “In Conclusion”].
And, ﬁnally, [162, Appendix B] (like [161, sec. 3]) seems to be a variant of the
elusive model paradox and “red herring sequence” [32, footnote 211][33, p455][34,
sec. 7.5][110][83].
Apart from the later co-authored [166], Ray Solomonoﬀ’s last paper was the
posthumously published Solomonoﬀ(2011) [163], which is appropriately a survey
summary with an eye to the future, initially mentioning [163, sec. 1 (Discovery)]
the use of “... unidirectional input and output tapes, and an inﬁnite bidirectional
work tape.”. The point is also made (as it is in [187, sec. 2.2.5]) that “general
purpose programming languages such as Fortran, LISP, C, C++, Basic, APL,
Maple, Mathematica, ...” are also universal. In [163, sec. 2 (Completeness and
Incomputability)], he highlights: “It is notable that completeness and incom-
putability are complementary properties: It is easy to prove that any complete
prediction method must be incomputable. Moreover, any computable prediction
method can not be complete - there will always be a large space of regularities
for which its predictions are catastrophically poor.”
The reader is referred to sec. 3.1 and [139][140, p426, col. 1, Theorem 3
(17)][148][161] variously for (re-)statements of the fundamental convergence re-
sult [140, p426, col. 1, Theorem 3 (17)] and for statements about complete-
ness and incomputability (and the reader is also referred to the relevant places
in this current paper where these papers [139][140, p426, col. 1, Theorem 3
(17)][148][161, secs. 3 and 11] are cited and there is accompanying discussion of
MML’s statistical consistency and [what I see as its] completeness - together with
its [consequent] incomputability [191][187, chap. 2]). Statisticians and econome-
tricians should take particular note of this, even if some might contend that it is
retrospectively trivial. In short, if there is a (Turing computable) language from
which the data is being generated but which the inference method is not able to
express, the inference method will not be able to ﬁnd the regularity.
Let us make this point immediately above more concrete (as many current
statisticians, econometricians and data miners have less than little training in
algorithmic information theory and computability). Consider the construction
from [32, footnote 211][34, sec. 7.5] but instead modify it to have some com-
putable inference method, C (whether this is Akaike’s AIC [2, 1, 3], Schwarz’s

18
D.L. Dowe
BIC [109] or whatever). Let C(x1, ..., xi, ..., xn) be the predicted continuation
of the sequence (x1, ..., xi, ..., xn) to xn+1. In the spirit of [32, footnote 211],
let A(xn+1) = C(x1, ..., xi, ..., xn) + 1. In the spirit of [33, p455][34, sec. 7.5]
(with the function C restricted to take values of 0 or 1), let B(xn+1) = 1 −C
(x1, ..., xi, ..., xn). The functions A and B are clearly computable (and, indeed,
inferrable), but the computability of C prevents it from being able to learn the
likes of A and B. This all said, being suﬃciently expressive does not on its own
guarantee statistical consistency - as (e.g.) with Maximum Likelihood. Similarly,
being suﬃciently expressive and incomputable does likewise not guarantee sta-
tistical consistency - see, e.g., [32, footnote 130].
The issue of Bayesianism and subjectivity (from [128, sec. 1.3][128, sec. 2.5 and
Appendix I][139, Introduction and sec. 1][149, sec. 2, p4], sec. 3.3 and [153, sec.
1][155, sec. 4][161, sec. 4 (Subjectivity)][162, Subjectivity]) is raised again in [163,
sec. 3 (Subjectivity)], with another comment for statisticians, econometricians
and others on the non-Bayesian approach advocated by Sir Ronald A. Fisher
(1890 - 1962), regarded by many as the father30 of modern statistics: “The great
statistician, R. A. Fisher, ... wanted to make statistics a “true science” free of
the subjectivity that had been so much a part of its history.
I feel that Fisher was seriously wrong in this matter, and this his work in this
area has profoundly damaged the understanding of statistics in the scientiﬁc
community - damage from which it is recovering all too slowly.” [163, sec. 3]. (A
seemingly similar sentiment is given by I. J. Good in [55, sec. 7, p112].)
To presume to quote from elsewhere: “Note the fact that the statistical con-
sistency is coming from the information-theoretic properties of MML and the”
statistical “invariance is actually coming from the Bayesian prior.” [32, sec. 0.2.5,
p540]. Perhaps digressing, we note that both the Solomonoﬀposterior-weighted
predictive distribution and the single best (or MML) inference (both of which,
from sec. 4.2, are Bayesian) are statistically invariant under re-parameterisation.
Next is [163, sec. 4 (Diversity)], which follows on from earlier notes - such
as, e.g., sec. 1.2, text near footnote 28, [156, sec. 1 (ALP and “The Wisdom
of Crowds”)][161, sec. 5 (Diversity and Understanding)][161, sec. 5.1 (ALP and
“The Wisdom of Crowds”)][162, sec. Diversity] and perhaps also [147][149, sec.
7]. Like at least (many or) most of these, [163, sec. 4 (Diversity)] concerns using
a weighted mixture of theories - rather than a single theory - for prediction.
The completeness and statistical consistency (and other merits) of approaches
to inference and prediction based on algorithmic information theory have been
discussed here a few times, but “There is, however another aspect of algorithmic
probability that people ﬁnd disturbing - it would seem to take too much time
and memory to ﬁnd a good prediction. ... There is a technique for implementing
ALP that seems to take as little time as possible to ﬁnd regularities in data”
[163, sec. 2], and this is then discussed in [163, sec. 5 (Computation Costs)],
which gives an example advocating a universal Levin search.
30 Perhaps after the Rev. Thomas Bayes (circa 1701 - 1761) and Carl F. Gauss
(1777 - 1855).

Introduction to Ray Solomonoﬀ85th Memorial Conference
19
Next, [163, sec. 6 (Training Sequences)] returns to one of those issues in artiﬁ-
cial intelligence special to Ray - that of training sequences, also addressed earlier
(as far back as the 1960s) in sec. 1.3 and [131][135, sec. II, p1689 col. 2][136, sec.
7][141, 145, 146][94, sec. 4][148, sec. 4.3][151][162, sec. “In Conclusion”]. The dis-
cussion of training sequences is understandably accompanied by a related early
concept (also from the 1960s, as per [128, page 1][141, abstract][146, sec. 4][148,
secs. 4.2 and 4.3][162, sec. “Training sequences”]), namely Ti/Pi from [163, sec.
6, equation (4)], accompanied with (its name) “I call this upper bound the “con-
ceptual jump size” (CJS)” [163, sec. 6 (Training Sequences)]. For those of us
interested in education, we note the last paragraph of this section: “Learning to
train very intelligent machines should give very useful insights on how to train
human students as well”.
Ray’s last sole-authored paper ﬁnishes with [163, sec. 7 (Where Are We
Now?)], and refers - among other places - to the “open problems” from [146,
sec. 8 (Present State of Development of the System? Near Term Goals, More
Distant Goals Time Scales for These Goals)].
Just as we read and re-read [173] and Turing’s thoughts re the future, it
seems appropriate to (re-)read [146, sec. 8], [161, sec. 11 (The future of ALP -
some open problems)] and [163, sec. 7 (Where Are We Now?)], accompanied (of
course) by [114].
4
Further Notes (And Perhaps Some Afterthoughts)
Here are some further notes regarding Ray Solomonoﬀ’s work and, perhaps par-
ticularly, (algorithmic) information theory.
4.1
Uniqueness of Logarithm-Loss Information-Theoretic Cost
The log(arithm)-loss scoring system, which sees probabilistic predictions pi made
for various events (ei) and gives a score of −log pj if event ej (of probability
pj) occurs, was advocated for the binomial distribution [55] (and presumably
also the multinomial distribution [55, p112]) by I. J. Good. It is very much
in the spirit of Shannon’s information theory [111], where code length equals
negative logarithm of probability. This log-loss scoring system can be applied
more generally to multinomial [55, p112][43, p4, Table 3], Gaussian [36][34, sec.
3.4] and other distributions [60, footnote 5].
But, unlike the popular “right”/“wrong” scoring system used widely in
areas including machine learning, data mining and information retrieval, the
information-theoretic log-loss scoring system is unique (plus or minus an addi-
tive or multiplicative constant) in being invariant to re-framing of questions (see
[32, footnote 175][33, pp437-438][34, sec. 3] for details).
In similar vein, the Kullback-Leibler divergence (from true to inferred distri-
bution) [and its reverse, from the inferred to the true distribution] is likewise
unique in being invariant to re-framing [33, p438][34, sec. 3.6].
We make three further comments here before summarising. First, an
information-theoretic compression-based competition has been running using the

20
D.L. Dowe
abovementioned log-loss scoring system since 1995 [44][32, sec. 0.2.5, p541, col.
2] for Australian AFL football (with a Gaussian competition based on the mar-
gin starting in 1996 [36][34, sec. 3.5]). Second, one can introduce a Bayesian
prior for the log-loss scoring system, as originally tried somewhat unsuccessfully
(with ratios of logarithms) in [65]. The correction (stated verbally in 2002) from
[171, sec. 4.2][32, footnote 176][34, sec. 3.4] takes logarithms of ratios of prob-
abilities (equivalent to diﬀerences in logarithms of probabilities when at least
one of these is ﬁnite). This is appropriate when, e.g., in quiz shows [34, sec.
3.3], some questions might be deemed a priori easier/harder than others. Third,
log-loss scoring can be used on Bayesian networks (or directed graphical models)
[25, sec. 9], where it has previously been used as a numerical approximation to
the Kullback-Leibler divergence between two Bayesian nets [25, sec. 9][34, sec.
3.6][60, footnote 5].
In summary, as in this section and elsewhere, information theory has much
to recommend it (including uniqueness). Universal Turing Machines (UTMs)
are (as the name suggests) universal, and Solomonoﬀ’s approach of combining
UTMs and information theory might be argued by many to be (retrospectively)
self-evident.
4.2
Prediction, Inference, Induction, Explanation
The issue of inductive inference (or induction) to a single theory was considered
by Ray Solomonoﬀ[120, perhaps especially early sections][124, sec. 2.6 (espe-
cially top of page 10)] before (as above) he later settled into prediction using a
weighted mixture of theories. Other places to see this distinction between predic-
tion (using a Solomonoﬀweighted mixture of theories) and inductive inference
(using a single theory, as with Minimum Message Length [MML]) discussed in-
clude [187, sec. 10.1][59, p1514][34, sec. 4.1]. See also secs. 1.2 and 3.3 (and
perhaps elsewhere).
A related issue is prediction not by a weighted mixture of models, but rather
by the best single predictive model within the class of models being considered -
a notion which has been referred to as the minimum expected Kullback-Leibler
distance estimator (often abbreviated to minEKL or MEKLD) [35][187, sec.
4.7][37][34, sec. 4.1]. See also [27].
One of the many reasons for appreciating these distinctions is Wallace’s res-
olution of entropy’s (supposedly, but) not being the arrow of time by observing
that we infer the past but predict the future [187, chap. 8].
Let us now turn to “approximations” and approximations.
In abovementioned writings, Ray would refer to Minimum Message Length
(MML) [188, 189, 195, 191, 187] (designed to be inference to the best - or most
probable - explanation) and (the later) Minimum Description Length (MDL)
[100] as “approximations”, even if their developers might not have thought or
think of them in precisely those terms [187, chap. 10]. Re the MML “approxi-
mation”, its purest from is Strict MML [189][187, chap. 3][34, sec. 6], but ap-
proximations to this exist in turn, including those from [195][187, chap. 4-5] and
(e.g.) MMLD (or I1D) [32, sec. 0.2.2][51][187, secs. 4.10 and 4.12.2 and chap. 8,

Introduction to Ray Solomonoﬀ85th Memorial Conference
21
p360][34, sec. 6.3]. The modiﬁcation to MMLD presented by L. J. Fitzgibbon
in [51, eq (7)] was later modiﬁed by D. F. Schmidt [106][32, footnotes 64-65].
The MML approach - of the single best explanation - can be used for hypoth-
esis testing [32, sec. 0.2.5, p539, col. 1], to fortify claims supporting Occam’s
razor [91][32, footnote 18], and to address the modelling choice of generative vs
discriminative [32, sec. 0.2.5].
A discussion is made in [33, pp451-452] of the appearance of lattice constants
(κd) in the Wallace-Freeman (1987) approximation [195] and the appearance
over a decade later of hexagonal-like regions for Strict MML for the trinomial
distribution (with d = 2, for which the hexagon is the optimally tesselating
Voronoi region) [187, p166, ﬁg. 3.1]. I am grateful to Emanuele Viterbo for
explaining to me that this is due to approximately quadratic behaviour of the
logarithm (or other) function(s) within the relevant region(s). And one indeed
expects a similar result re SMML and optimally tesselating Voronoi regions for
higher d.
Penultimately, let us now use a decision tree example to compare the (single
model) MML framework with a version of the (weight across several models)
predictive framework. The case we take is in the spirit of [197, sec. 4], where
some data can be modelled either as having no split and simply being [150, 50]
(denoting 150 in class 1 and 50 in class 2) or (alternatively) as having a split
on attribute 1 (Att1) with [90, 10] in the left (yes) path and [60, 40] in the right
(no) path. Describing the tree topology will be more expensive if there is a split,
with an additional cost being the logarithm of the number of attributes to de-
scribe which attribute is being split on [197, sec. 4][187, sec. 7.2]. We assume
this additional cost not to be very large. On the other hand, the puriﬁcation
of the data as [90, 10] and [60, 40] will result in it being far cheaper to describe
the contents of these two leaves than to describe the unsplit [150, 50]. In terms
of “right”/“wrong” predictive accuracy, the split oﬀers nothing, as both the un-
split and the split version will always predict the left class (class 1) - and there
are probably still some in the machine learning and presumably data mining
communities who would advocate not doing the split. But, whether for market-
ing (campaign success vs cost), medical or other matters (see, e.g., [55, sec. 7
(vi)][161, sec. 1]), we often need to estimate probabilities, as the uniqueness re-
sults from sec. 4.1 also tell us. The information-theoretic MML approach tells us
this, and it is perhaps no great surprise that the Solomonoﬀpredictive approach
will put far greater weight on the models with the split than the models without
the split (even if the leaf probability estimates are slightly diﬀerent in the two
approaches, with the predictive approach advocating the MEKLD [equivalently
posterior mean] estimator). Both the MML approach and the Solomonoﬀpredic-
tive approach will have similar predictive probabilities [35] and (in similar vein)
similar code-lengths for compressing the data. I note almost parenthetically that
extensions to Naive Bayes such as AODE and AnDE [198] could probably be
enhanced by this more principled way of combining and weighting probabilities
- and possibly with relatively little additional computational cost.

22
D.L. Dowe
Papers in this collection using one or more of these MML-based approxima-
tions or some variations include [107, 88, 87]. Papers in this collection using some
MDL-based approximations include [169, 67].
4.3
How to Choose a Bayesian Prior?
As discussed in many places throughout this paper, these techniques (as sum-
marised in sec. 4.2) are Bayesian, but just how might we (“objectively”) choose
the prior? There are discussions in [187, sec. 2.3.12][31] of how to choose a sim-
plest UTM. Another possibility would be to choose the simplest UTM, U, such
that U = V in [6, corollary 3.5].
On a much restricted class of problems where the statistical likelihood is
not universal but rather is twice diﬀerentiable, some advocate the use of the (so-
called) Jeﬀreys “prior” [72], although this often does not normalise. One attempt
to salvage the Jeﬀreys “prior” (to the degree that this can be done) is given in
[34, sec. 7.1].
4.4
Information Theory, (Artiﬁcial) Intelligence and Recognising It
Following so much of Ray Solomonoﬀ’s abovementioned work(s), in 1982, Greg
Chaitin (author of [22, 21, 23]) suggested [24, sec. 6 (Directions for Future Re-
search)] “f. Develop formal deﬁnitions of intelligence and measures of its various
components; apply information theory and complexity theory to AI.”. As per
sec. 2, independent work in the 1990s [38, 39, 64, 40, 58, 86] and [104] advocated
the use of (algorithmic) information theory (and compression) to give (static)
universal tests of intelligence [34, sec. 7.3].
Legg and Hutter [79] later suggested having a universal Solomonoﬀdistri-
bution over environments, and suggested that intelligence could be quantiﬁed
as a doubly inﬁnite sum (with some incomputabilities). This was intended as a
deﬁnition (or measure) rather than a test, partly (but not only) because it did
not incorporate time. A test was developed following similar principles in [59]
and humans were compared to machines on a supposedly equal footing in such
a test (even if with somewhat unclear results) in [69, 70], with a similar theme
followed slightly later in [80].
The measure (or deﬁnition) in [79] could be made more general (or “more
universal”) by ﬁrst noting the following observations:
– Ayumu the chimpanzee in Kyoto, Japan, can remember the locations of the
digits 1 to 9 randomly placed on a screen in a time-frame in which many
humans would struggle to recall the location of any one of these. In similar
vein, many half-ﬂuent speakers of a language will be able to recall and process
a sentence if it is spoken to them slowly, but will simply become lost if the
sentence is spoken at full speed. These comments concern the speed of the
environment’s output and (correspondingly) the agent’s input - and they
remain true no matter how much extra thinking time the agent has after the
(all too fast) input has ﬁnished.

Introduction to Ray Solomonoﬀ85th Memorial Conference
23
– regarding the issue of the speed of the agent’s output, the prominent physi-
cist, Stephen W. Hawking, often requires relatively long periods of time to
communicate what would otherwise be far faster from him. The problem is
not his internal neural processing ability but rather the speed of his out-
put (devices or) communication. A similar comment applies to people with
locked-in syndrome.
– redundant TMs [32, sec. 0.2.7, p544, col. 2][59, p1514, footnote 6] are TMs
with redundant input and output which (for this reason) can not be universal
(in the sense of a UTM) but, with the correct pre-processing (create redun-
dancies) and post-processing (remove redundancies), can mimic UTMs31. A
giraﬀe and a whale are both bigger than a human, who is in turn bigger than
an ant - and (whether or not it incorporates redundant [U]TMs) appropriate
spatial resolution needs to be given for these agents.
– at least partly relevant is the case of Canadian Scott Routley, who can com-
municate - albeit, like Stephen Hawking, slowly - via the use of an additional
communication channel, namely, an fMRI scanner.
– possibly also see [136, sec. 7 (Dolphin Talk)] from sec. 3.
The Legg-Hutter deﬁnition [79] might be “more universal” if it allowed varying
input and output durations (possibly with a universal distribution over these
speeds, bearing in mind that some agents might have similar speeds of operation
in processing input but vastly diﬀerent in output or vice versa), and possibly
optimising over spatial resolution. The issue of recognising life and intelligence
(from the greater machine kingdom [63]) from elsewhere [32, sec. 0.2.5, p542]
should be facilitated by a willingness to be “more universal” and allow diﬀerent
spatio-temporal resolutions (and speeds). Some of these criticisms possibly also
apply to at least parts of the test(s) in [59]. There is also the observation that,
like us predominantly earth-bound humans, it is reasonable to put large weight
on environments in which agents have a reasonable chance of survival [62].
The Monte Carlo AIXI (MC-AIXI) work [175] is most impressive. As has been
stated elsewhere (and as per [42] and sec. 4.2), it would be good to re-visit this
using MML. Regarding the comparison of the predictive and MML approaches
for the decision tree example in sec. 4.2, it seems that some of the MC-AIXI
calculations [175] might need re-visiting.
4.5
A Music Note
We are often taught that the major chord in music of a note, 4 semitones
above the note and then 3 semitones above that (7 semitones above the original
note) sounds harmonic and/or melodic because the frequencies are in the ratio
4:5:6. And, yet, they are not. Rather, with 12 semitones in an octave and each
semitone step having a frequency ratio of 1:21/12, the notes’ frequencies are in
the ratio 4:(4 × 21/3):(4 × 27/12) [or (5 × 2−1/3):5:(5 × 21/4)], or approximately
4.0000:5.0397:5.9932 [or approximately 3.9685:5.0000:5.9460]. In short, I guess
that the approximation (to 4:5:6) works well enough.
31 Possibly see the loosely related footnote 29.

24
D.L. Dowe
It has been said of the American composer and music theorist, Harry Partch,
that (rather than 12 equal tones) he advocated the division of the octave into
43 unequal tones (and I have heard the number 37 mentioned). To some degree,
this is a problem of creating simple ratios (the octave is 1:2) and near (or ap-
proximately) simple ratios - perhaps balanced with relatively few divisions per
octave. (Possibly see also [130, sec. VII].)
4.6
Originality, Creativity, Humour, Illusion
Perhaps following sec. 4.4, the originality of something is presumably measured
by how diﬀerent it is to things which preceded it. This could be quantiﬁed by
(e.g.) (i) such an idea’s having a large Hamming distance from existing ideas,
and/or (ii) it requiring a large amount of information to describe/encode it
in terms of (a suitable encoding of) existing ideas, and/or (iii) the underlying
model’s having a large Kullback-Leibler distance from the other models.
Something is presumably creative if it can repeatedly demonstrate originality.
Humour [32, sec. 0.2.7, p545][34, sec. 7.7, p967] (especially puns), illusions and
(cryptic) crossword clues often require ﬁnding connections (simple and concise,
of relatively little information content) between things not previously recognised
to be related.
I think that the above suggestions re something’s being “creative” are consis-
tent with Ray Solomonoﬀ’s notions from [131, 145] and secs. 1.3 and 3.2.
4.7
Some Further Work
Open problems and exercises to be worked on have been given in (e.g.) [146, sec.
8][151, sec. 8][161, sec. 11][163, sec. 7][32, sec. 0.2.7][34, sec. 7.7].
Other problems include (whether, as per sec. 4.2, by prediction, induction or
whatever) using the discussed approaches of (algorithmic) information theory to
address (e.g.) the following:
– k-nearest neighbours (k-NN) and recommender systems,
– building upon MML Bayesian networks with continuous and discrete at-
tributes [25, 26], MML Bayesian nets with latent factors [177], Wallace’s
earlier work on MML Bayesian nets [187, sec. 7.4][32, sec. 0.2.2, p526, col. 1]
and MML time series [52, 107] to re-visit or generalise the dynamic Bayesian
network (DBN) scheme from [97],
– extending MML time series [52, 107] to re-visit (econometric) GARCH (or
introduce moving averages [MA] to extend it to GARCHMA),
– ﬂeshing out the ideas from [28] (extending [190, 194]),
– re-visiting the lovely story of early ideas re information retrieval (IR) [114,
sec. 5] to (re-)address the mixture modelling approaches of (e.g.) [199],
– replacing Maximum Likelihood in Approximate Bayesian Computation
(ABC),
– addressing (so-called) non-parametrics (in statistics and econometrics) with
some appropriate Turing machine (prior),

Introduction to Ray Solomonoﬀ85th Memorial Conference
25
– re-visiting K. Zuse and S. Wolfram on the universe as a cellular automaton
- perhaps as per [133, sec. 3.2],
– Inference@Home (similarly parallel to SETI@Home and Folding@Home),
– exploring the notion of LNPPP [32, sec. 0.2.7][34, sec. 5.3], where the (uni-
versal) distribution of statistical environments was and is intended to be
generated using a (universal) Turing machine,
– in physics, entropy is often deﬁned as 
i −pi log pi, where a multinomial
distribution is assumed, the Maximum Likelihood estimates are used (or
presumed), and the entropy is then said to be the negative log-likelihood
divided by (N) the number of particles. Whether or not we divide by N,
are we not interested in the information content [12]? Can this be applied
to other likelihood functions rather than just the multinomial - and, more
speciﬁcally, can this be re-framed in terms of a universal distribution?
– footnotes 26 and 28 (or the text next to them) and the Peter principle
[34, sec. 7.7, p969] draw our attention to political and other problems (e.g.,
[workplace] psychopaths, bullies, convincing liars, etc.) that many of us have
known in (e.g.) the workplace. Could the methods of inductive inference and
prediction mentioned here be used (perhaps with some notions from graph
theory) to analyse (possibly experimental) data to create one or more better
(and more robust) hierarchies of information ﬂow, obtaining of opinions and
views (and evidence) and decision making? (Possibly see [55, sec. 7 (vi)].)
4.8
From Here
2012 was the centenary of the birth of Alan M. Turing, on whose Universal Turing
Machines and (the) quest for machine intelligence so much of Ray Solomonoﬀ’s
work has been built. 2013 is appropriately The International Year of Statistics
(Statistics2013, www.statistics2013.org), an area in which Ray’s work will con-
tinue to grow. 2013 is also the International Year of Mathematics of Planet Earth
(http://mpe2013.org , http://mathsofplanetearth.org.au). Among other things,
I think of Ray’s “never ending source of joy in discovery!” [153, sec. 3] and his
questioning the conﬁdence in (current) extreme value theory (from sec. 3.2) [145,
sec. 1]. I also think of his warning from [136, sec. 1] in 1967 which I expand upon
from secs. 1.1 and 3: “... the dangers posed are very serious and the problems
very diﬃcult. It would be well if a large number of intelligent humans devote
a lot of thought to these problems before they arise. It is my feeling that the
realization of artiﬁcial intelligence will be a sudden occurrence. ...”
Acknowledgements. Thank you (again) to all those thanked at the front of
this volume. Thank you to the sponsors, the Program Committee, the referees
(at least one of whom wished not to be named), Grace Solomonoﬀ(who has
contributed the wonderful [114]), the other invited speakers, Alex Solomonoﬀ,
the other authors, their co-authors, the presenters, and the other conference
delegates. Thank you to the late Ray Solomonoﬀfor a wonderful body of work,
for a life well lived and for bringing us together. Thank you also to Joy Reynolds,
Genevieve Oreski, Dr Rebecca Robinson, RJD and (with apologies) anyone who
I might have forgotten.

26
D.L. Dowe
P.S.: As well as the earlier picture of Ray and the cover ﬁgure, perhaps also see a
ﬁgurebyRJDatwww.csse.monash.edu.au/~dld/RaySolomonoffsVision.html
References
1. Akaike, H.: Statistical prediction information. Ann. Inst. Statist. Math. 22,
203–217 (1970)
2. Akaike, H.: Information theory and an extension of the maximum likelihood prin-
ciple. In: Petrov, B.N., Csaki, F. (eds.) Proceedings of the 2nd International Sym-
posium on Information Theory, pp. 267–281 (1973)
3. Akaike, H.: Factor Analysis and AIC. Psychometrika 52(3), 317–332 (1987)
4. Amir, A., Amin, A.H.M., Khan, A.: Developing machine intelligence within P2P
networks using a distributed associative memory. In: Dowe, D.L. (ed.) Solomonoﬀ
Festschrift. LNCS (LNAI), vol. 7070, pp. 439–443. Springer, Heidelberg (2013)
5. Balduzzi, D.: Falsiﬁcation
and future performance. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 65–78. Springer, Heidelberg
(2013)
6. Barmpalias, G., Dowe, D.L.: Universality probability of a preﬁx-free machine.
Philosophical Transactions of the Royal Society A [Mathematical, Physical &
Engineering Sciences] (Phil Trans. A) 370, 3488–3511 (2012)
7. Barron, A.R., Cover, T.M.: Minimum complexity density estimation. IEEE Trans-
actions on Information Theory 37, 1034–1054 (1991)
8. Baxter, R.A., Oliver, J.J.: MDL and MML: Similarities and diﬀerences. Techni-
cal report TR 94/207, Dept. of Computer Science, Monash University, Clayton,
Victoria 3168, Australia (1995)
9. Bergen, M.S., Bishop, W.B., Buchanan, B.L., Dilworth, R.P., Ackerlind, E.,
Solomonoﬀ, R.J., et al.: Part n-circuit theory; information theory. In: IEEE In-
ternational Convention Record, p. 293. Institute of Electrical and Electronics
Engineers, U.S.A. (1957)
10. Boulton, D.M.: Numerical classiﬁcation based on an information measure. Mas-
ter’s thesis, M.Sc. thesis, Basser Computing Dept., University of Sydney, Sydney,
Australia (1970)
11. Boulton, D.M.: The Information Measure Criterion for Intrinsic Classiﬁcation.
PhD thesis, Dept. Computer Science, Monash University, Clayton, Australia (Au-
gust 1975)
12. Boulton, D.M., Wallace, C.S.: The information content of a multistate distribu-
tion. J. Theor. Biol. 23, 269–278 (1969)
13. Boulton, D.M., Wallace, C.S.: A program for numerical classiﬁcation. Computer
Journal 13(1), 63–69 (February 1970)
14. Boulton, D.M., Wallace, C.S.: A comparison between information measure classi-
ﬁcation. In: Proc. of the Australian & New Zealand Association for the Advance-
ment of Science (ANZAAS) Congress (August 1973) (abstract)
15. Boulton, D.M., Wallace, C.S.: An information measure for hierarchic classiﬁca-
tion. Computer Journal 16(3), 254–261 (1973)
16. Boulton, D.M., Wallace, C.S.: Occupancy of a rectangular array. Computer Jour-
nal 16(1), 57–63 (1973)
17. Boulton, D.M., Wallace, C.S.: An information measure for single link classiﬁca-
tion. Computer Journal 18(3), 236–238 (1975)

Introduction to Ray Solomonoﬀ85th Memorial Conference
27
18. Brennan, M.H.: Data processing in the early cosmic ray experiments in Sydney.
Computer Journal 51(5), 561–565 (2008); Christopher Stewart WALLACE (1933-
2004) memorial special issue
19. Brennan, M.H., Millar, D.D., Wallace, C.S.: Air showers of size greater than 105
particles - (1) core location and shower size determination. Nature 182, 905–911
(October 4, 1958)
20. Campbell, D.: The Semimeasure Property of Algorithmic Probability - “Feature”
or “Bug”? In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070,
pp. 79–90. Springer, Heidelberg (2013)
21. Chaitin, G.J.: On the length of programs for computing ﬁnite sequences. Journal
of the Association for Computing Machinery 13, 547–569 (1966)
22. Chaitin, G.J.: On the simplicity and speed of programs for computing inﬁnite sets
of natural numbers. Journal of the Association for Computing Machinery 16(3),
407–422 (1969)
23. Chaitin, G.J.: Randomness and Mathematical Proof. Scientiﬁc American 232(5),
47–52 (May 1975)
24. Chaitin, G.J.: Godel’s theorem and information. International J. of Theoretical
Physics 21(12), 941–954 (1982)
25. Comley, J.W., Dowe, D.L.: General Bayesian networks and asymmetric languages.
In: Proc. Hawaii International Conference on Statistics and Related Fields, June
5-8 (2003)
26. Comley, J.W., Dowe, D.L.: Minimum message length and generalized Bayesian
nets with asymmetric languages. In: Gr¨unwald, P., Pitt, M.A., Myung, I.J. (eds.)
Advances in Minimum Description Length: Theory and Applications (MDL Hand-
book), ch. 11, pp. 265–294. M.I.T. Press (April 2005) ISBN 0-262-07262-9; Final
camera-ready copy submitted in October 2003. [Originally submitted with ti-
tle: “Minimum Message Length, MDL and Generalised Bayesian Networks with
Asymmetric Languages”.]
27. Balduzzi, D.: Falsiﬁcation
and future performance. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 65–78. Springer, Heidelberg
(2013)
28. Dale, P.E.R., Dale, M.B., Dowe, D.L., Knight, J.M., Lemckert, C.J., Low Choy,
D.C., Sheaves, M.J., Sporne, I.: A conceptual model for integrating physical ge-
ography research and coastal wetland management, with an Australian example.
Progress in Physical Geography 34(5), 605–624 (October 2010)
29. Dean, Thomas, Boddy: An analysis of time-dependent planning. In: Proc. 7th
National Conference on Artiﬁcial Intelligence, pp. 49–54 (1998)
30. Dessalles, J.-L.: Algorithmic simplicity and relevance. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 119–130. Springer, Heidelberg
(2013)
31. Dowe, D.L.: Discussion following “Hedging predictions in machine learning, A.
Gammerman and V. Vovk”. Computer Journal 2(50), 167–168 (2007)
32. Dowe, D.L.: Foreword re C. S. Wallace. Computer Journal 51(5), 523–560 (2008);
Christopher Stewart WALLACE (1933-2004) memorial special issue
33. Dowe, D.L.: Minimum Message Length and statistically consistent invariant (ob-
jective?) Bayesian probabilistic inference - from (medical) “evidence”. Social Epis-
temology 22(4), 433–460 (2008)
34. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Bandyopadhyay, P.S., Forster, M.R. (eds.)
Handbook of the Philosophy of Science. Philosophy of Statistics, vol. 7, pp. 901–982.
Elsevier (2011)

28
D.L. Dowe
35. Dowe, D.L., Baxter, R.A., Oliver, J.J., Wallace, C.S.: Point estimation using the
Kullback-Leibler loss function and MML. In: Wu, X., Kotagiri, R., Korb, K. (eds.)
PAKDD 1998. LNCS (LNAI), vol. 1394, pp. 87–95. Springer, Heidelberg (1998)
36. Dowe, D.L., Farr, G.E., Hurst, A.J., Lentin, K.L.: Information-theoretic football
tipping. Technical report TR 96/297, Dept. of Computer Science, Monash Uni-
versity, Clayton, Victoria 3168, Australia (1996)
37. Dowe, D.L., Gardner, S., Oppy, G.R.: Bayes not bust! Why simplicity is no prob-
lem for Bayesians. British Journal for the Philosophy of Science 58(4), 709–754
(2007)
38. Dowe, D.L., Hajek, A.R.: A computational extension to the Turing test. In: Pro-
ceedings of the 4th Conference of the Australasian Cognitive Science Society,
Newcastle, NSW, Australia (September 1997)
39. Dowe, D.L., Hajek, A.R.: A computational extension to the Turing test. Techni-
cal Report 97/322, Dept. Computer Science, Monash University, Australia 3168
(October 1997)
40. Dowe, D.L., Hajek, A.R.: A non-behavioural, computational extension to the
Turing test. In: Proceedings of the International Conference on Computational
Intelligence & Multimedia Applications (ICCIMA 1998), Gippsland, Australia,
pp. 101–106 (February 1998)
41. Dowe, D.L., Hern´andez-Orallo, J.: I.Q. tests are not for machines, yet. Intelli-
gence 40(2), 77–81 (March 2012)
42. Dowe, D.L., Hern´andez-Orallo, J., Das, P.K.: Compression and intelligence: Social
environments and communication. In: Schmidhuber, J., Th´orisson, K.R., Looks,
M. (eds.) AGI 2011. LNCS, vol. 6830, pp. 204–211. Springer, Heidelberg (2011)
43. Dowe, D.L., Krusel, N.: A decision tree model of bushﬁre activity. Technical report
TR 93/190, Dept. of Computer Science, Monash University, Clayton, Vic. 3800,
Australia (September 1993)
44. Dowe, D.L., Lentin, K.L.: Information-theoretic footy-tipping competition -
Monash. Computer Science Association Newsletter (Australia), 55–57 (Decem-
ber 1995)
45. Edgoose, T., Allison, L.: MML Markov classiﬁcation of sequential data. Stats.
and Comp. 9(4), 269–278 (1999)
46. Edwards, R.T., Dowe, D.L.: Single factor analysis in MML mixture modelling.
In: Wu, X., Kotagiri, R., Korb, K.B. (eds.) PAKDD 1998. LNCS, vol. 1394, pp.
96–109. Springer, Heidelberg (April 1998)
47. Ellison, T.M.: Categorisation as topographic mapping between uncorrelated
spaces. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070,
pp. 131–141. Springer, Heidelberg (2013)
48. Evans, T.: A heuristic program of solving geometric analogy problems. PhD the-
sis, Mass. Inst. Tech., Cambridge, Mass., U.S.A. (1963) Also available from AF
Cambridge Research Lab, Hanscom AFB, Bedford, Mass., U.S.A.: Data Sciences
Lab., Phys. and Math. Sci. Res. Paper 64, Project 4641 (1963)
49. Evans, T.: A heuristic program to solve geometric-analogy problems. In: Proc.
SJCC, vol. 25, pp. 327–339 (1965)
50. Da Silva Filho, R.I., da Rocha, R.L.A., Guiraldelli, R.H.G.: Learning in the limit:
A mutational and adaptive approach. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift.
LNCS (LNAI), vol. 7070, pp. 106–118. Springer, Heidelberg (2013)
51. Fitzgibbon, L.J., Dowe, D.L., Allison, L.: Univariate polynomial inference by
Monte Carlo message length approximation. In: Proceedings of the 19th Inter-
national Conference on Machine Learning (ICML 2002), pp. 147–154. Morgan
Kaufmann (2002)

Introduction to Ray Solomonoﬀ85th Memorial Conference
29
52. Fitzgibbon, L.J., Dowe, D.L., Vahid, F.: Minimum message length autoregressive
model order selection. In: Proc. Int. Conf. on Intelligent Sensors and Information
Processing, Chennai, India, pp. 439–444 (January 2004)
53. Freivalds, R.: Algorithmic information theory and computational complexity. In:
Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 142–154.
Springer, Heidelberg (2013)
54. Fresco, N.: A critical survey of some competing accounts of concrete digital com-
putation. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070,
pp. 155–173. Springer, Heidelberg (2013)
55. Good, I.J.: Rational decisions. J. Roy. Statist. Soc. (B) 14(1), 107–114 (1952)
56. Good, I.J.: Speculations concerning the ﬁrst ultraintelligent machine. Advances
in Computers 6, 31–88 (1965)
57. Hall, J.S.: Further reﬂections on the timescale of AI. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 174–183. Springer, Heidel-
berg (2013)
58. Hern´andez-Orallo, J.: Beyond the Turing test. Journal of Logic, Language and
Information 9(4), 447–466 (2000)
59. Hern´andez-Orallo, J., Dowe, D.L.: Measuring universal intelligence: Towards an
anytime intelligence test. Artiﬁcial Intelligence Journal 174(18), 1508–1539 (2010)
60. Hern´andez-Orallo, J., Dowe, D.L.: Potential Properties of Turing Machines. Tech-
nical report 2012/271, Clayton School of I.T., Monash University, Clayton, Vic.
3168, Australia, 22 pp. (August 3, 2012)
61. Hern´andez-Orallo, J., Dowe, D.L.: On Potential Cognitive Abilities in the Machine
Kingdom. Minds and Machines 23, 179–210 (2013),
http://dx.doi.org/10.1007/s11023-012-9299-6
62. Hern´andez-Orallo, J., Dowe, D.L., Espa˜na-Cubillo, S., Hern´andez-Lloreda, M.V.,
Insa-Cabrera, J.: On more realistic environment distributions for deﬁning, eval-
uating and developing intelligence. In: Schmidhuber, J., Th´orisson, K.R., Looks,
M. (eds.) AGI 2011. LNCS, vol. 6830, pp. 82–91. Springer, Heidelberg (2011)
63. Hern´andez-Orallo, J., Dowe, D.L., Hern´andez-Lloreda, M.V.: Universal Psycho-
metrics: Measuring Cognitive Abilities in the Machine Kingdom. Accepted to
Cognitive Systems Research (See also Technical report 2012/267, Clayton School
of I.T., Monash University)
64. Hernandez-Orallo, J., Minaya-Collado, N.: A formal deﬁnition of intelligence
based on an intensional variant of Kolmogorov complexity. In: Proceedings of
the International Symposium of Engineering of Intelligent Systems, pp. 146–163.
ICSC Press (1998)
65. Hope, L.R., Korb, K.: Bayesian information reward. In: McKay, B., Slaney, J.K.
(eds.) AI 2002. LNCS (LNAI), vol. 2557, pp. 272–283. Springer, Heidelberg (2002)
66. Horning, J.: A procedure for grammatical inference. In: Proc. IFIP Congress,
Amsterdam, North Holland, vol. 71, Amsterdam, North Holland
67. Hu, B., Rakthanmanon, T., Hao, Y., Evans, S., Lonardi, S., Keogh, E.: Towards
discovering the intrinsic cardinality and dimensionality of time series using MDL.
In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 184–197.
Springer, Heidelberg (2013)
68. Hutter, M.: New Error Bounds for SolomonoﬀPrediction. J. Comput. Syst.
Sci. 62(4), 653–667 (2001)
69. Insa-Cabrera, J., Dowe, D.L., Espa˜na-Cubillo, S., Hern´andez-Lloreda, M.V.,
Hern´andez-Orallo, J.: Comparing humans and AI agents. In: Schmidhuber, J.,
Th´orisson, K.R., Looks, M. (eds.) AGI 2011. LNCS, vol. 6830, pp. 122–132.
Springer, Heidelberg (2011)

30
D.L. Dowe
70. Insa-Cabrera, J., Dowe, D.L., Hern´andez-Orallo, J.: Evaluating a reinforcement
learning algorithm with a general intelligence test. In: Lozano, J.A., G´amez,
J.A., Moreno, J.A. (eds.) CAEPIA 2011. LNCS, vol. 7023, pp. 1–11. Springer,
Heidelberg (2011)
71. Jankowski, N.: Complexity measures for meta-learning and their optimality. In:
Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 198–210.
Springer, Heidelberg (2013)
72. Jeﬀreys, H.: An invariant form for the prior probability in estimation problems.
Proc. of the Royal Soc. of London A 186, 453–454 (1946)
73. Langdon Jr., G.G.: An introduction to arithmetic coding. IBM Journal of Re-
search and Development 28(2), 135–149 (1984)
74. Langdon Jr., G.G., Rissanen, J.J.: A simple general binary source code. IEEE
Transactions on Information Theory 28(5), 800–803 (1982)
75. King, P.A.: Design of a conscious machine. In: Dowe, D.L. (ed.) Solomonoﬀ
Festschrift. LNCS (LNAI), vol. 7070, pp. 211–222. Springer, Heidelberg (2013)
76. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1, 4–7 (1965)
77. Kolmogorov, A.N.: Logical basis for information theory and probability theory.
IEEE Transactions on Information Theory 14, 662–664 (1968)
78. Lattimore, T., Hutter, M.: No free lunch versus occam’s razor in supervised learn-
ing. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp.
223–235. Springer, Heidelberg (2013)
79. Legg, S., Hutter, M.: Universal intelligence: A deﬁnition of machine intelligence.
Minds and Machines 17(4), 391–444 (November 2007)
80. Legg, S., Veness, J.: An approximation of the universal intelligence measure. In:
Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 236–249.
Springer, Heidelberg (2013)
81. Levin, L.A.: Universal sequential search problems. Problems of Information Trans-
mission 9(3), 265–266 (1973)
82. Levin, L.A.: Universal heuristics: How do humans solve “Unsolvable” problems?
In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 53–54.
Springer, Heidelberg (2013)
83. Lewis, D.K., Shelby-Richardson, J.: Scriven on human unpredictability. Philo-
sophical Studies: An International Journal for Philosophy in the Analytic Tradi-
tion 17(5), 69–74 (1966)
84. Li, M.: Partial match distance. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS
(LNAI), vol. 7070, pp. 55–64. Springer, Heidelberg (2013)
85. Li, M., Vit´anyi, P.M.B.: An Introduction to Kolmogorov Complexity and its ap-
plications. Springer (1997)
86. Mahoney, M.: Text compression as a test for artiﬁcial intelligence. In: Proc. Na-
tional Conf. on Artiﬁcial Intelligence, U.S.A., p. 970. AAAI / John Wiley & Sons
(1999)
87. Makalic, E., Allison, L.: MMLD inference of multilayer perceptrons. In: Dowe,
D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 261–272. Springer,
Heidelberg (2013)
88. Makalic, E., Schmidt, D.F.: Minimum message length analysis of the behrens–
ﬁsher problem. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI),
vol. 7070, pp. 250–260. Springer, Heidelberg (2013)
89. Miyabe, K.: An optimal superfarthingale and its convergence over a computable
topological space. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI),
vol. 7070, pp. 273–284. Springer, Heidelberg (2013)

Introduction to Ray Solomonoﬀ85th Memorial Conference
31
90. Molloy, S.B., Albrecht, D.W., Dowe, D.L., Ting, K.M.: Model-Based Clustering
of Sequential Data. In: Proceedings of the 5th Annual Hawaii International Con-
ference on Statistics, Mathematics and Related Fields (January 2006)
91. Needham, S.L., Dowe, D.L.: Message length as an eﬀective Ockham’s razor in
decision tree induction. In: Proc. 8th Int. Workshop on Artif. Intelligence and
Statistics (AI+STATS 2001), pp. 253–260 (January 2001)
92. ¨Ozkural, E.: Diverse consequences of algorithmic probability. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 285–298. Springer, Heidelberg
(2013)
93. van Heerden, P.J.: A general theory of prediction. Technical report, Polaroid
Corp., Cambridge 39, Massachusetts, U.S.A., Privately circulated report (1963)
94. Paul, W.J., Solomonoﬀ, R.J.: Autonomous theory building systems. Neural Net-
works and Adaptive Learning, Schloss Reisenberg, Knowledge Processing and its
Applications Series (1990)
95. Paul, W.J., Solomonoﬀ, R.J.: Autonomous theory building systems. Annals of
Operations Research 55(1), 179–193 (1995)
96. Pelckmans, K.: An adaptive compression algorithm in a deterministic world. In:
Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 299–305.
Springer, Heidelberg (2013)
97. P´erez-Ariza, C.B., Nicholson, A.E., Korb, K.B., Mascaro, S., Hu, C.H.: Causal
discovery of dynamic Bayesian networks. In: Thielscher, M., Zhang, D. (eds.) AI
2012. LNCS, vol. 7691, pp. 902–913. Springer, Heidelberg (2012)
98. Petersen, S.: Toward an algorithmic metaphysics. In: Dowe, D.L. (ed.) Solomonoﬀ
Festschrift. LNCS (LNAI), vol. 7070, pp. 306–317. Springer, Heidelberg (2013)
99. Rissanen, J.J.: Generalized Kraft inequality and arithmetic coding. IBM J. Res.
Develop. 20(3), 198–203 (1976)
100. Rissanen, J.J.: Modeling by shortest data description. Automatica 14, 465–471
(1978)
101. Rissanen, J.J.: Information and Complexity in Statistical Modeling. Information
Science and Statistics. Springer (2007)
102. Rissanen, J.J., Langdon Jr., G.G.: Arithmetic coding. IBM Journal of Research
and Development 23(2), 149–162 (1979)
103. Rzepka, R., Muramoto, K., Araki, K.: Limiting context by using the web to
minimize conceptual jump size. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS
(LNAI), vol. 7070, pp. 318–326. Springer, Heidelberg (2013)
104. Sanghi, P., Dowe, D.L.: A computer program capable of passing I.Q. tests. In:
4th International Conference on Cognitive Science (and 7th Australasian Society
for Cognitive Science Conference), Univ. of NSW, Sydney, Australia, vol. 2, pp.
570–575 (July 2003)
105. Schmidhuber, J.: Optimal ordered problem solver. Technical report TR IDSIA-
12-02, IDSIA, Lugano, Switzerland, (July 31, 2002),
http://www.idsia.ch/~juergen/oops.html
106. Schmidt, D.F.: Minimum Message Length Inference of Autoregressive Moving Av-
erage Models. PhD thesis, Faculty of Information Technology, Monash University
(2008)
107. Schmidt, D.F.: Minimum message length order selection and parameter estimation
of moving average models. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS
(LNAI), vol. 7070, pp. 327–338. Springer, Heidelberg (2013)
108. Schwartz, J., Solomonoﬀ, R.J.: Photoelectric chopper for guided missiles. Elec-
tronics (November 1954)

32
D.L. Dowe
109. Schwarz, G.: Estimating dimension of a model. Ann. Stat. 6, 461–464 (1978)
110. Scriven, M.: An essential unpredictability in human behavior. In: Wolman, B.B.,
Nagel, E. (eds.) Scientiﬁc Psychology: Principles and Approaches, pp. 411–425.
Basic Books (Perseus Books) (1965)
111. Shannon, C.E.: A mathematical theory of communication. The Bell System Tech-
nical Journal 27, 379–423 (July 1948), 623–656 (October 1948)
112. Silvescu, A., Honavar, V.: Abstraction super-structuring normal forms: Towards a
theory of structural induction. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS
(LNAI), vol. 7070, pp. 339–350. Springer, Heidelberg (2013)
113. Solomonoﬀ, A.: Locating a discontinuity in a piecewise-smooth periodic func-
tion using bayes estimation. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS
(LNAI), vol. 7070, pp. 351–365. Springer, Heidelberg (2013)
114. Solomonoﬀ, G.: Ray solomonoﬀand the new probability. In: Dowe, D.L. (ed.)
SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 37–52. Springer, Heidelberg
(2013)
115. Solomonoﬀ, R.J.: An exact method for the computation of the connectivity of
random nets. Bulletin of Mathematical Biophysics 14(2), 153–157 (1952)
116. Solomonoﬀ, R.J.: An optically driven airborne chopper. In: Proceedings of the
3rd Typhoon Symposium, p. 205 (1953)
117. Solomonoﬀ, R.J.: Eﬀects of Heisenberg’s principle on channel capacity. Proceed-
ings of the I.R.E. 43, 484 (April 1955)
118. Solomonoﬀ, R.J.: An inductive inference machine. Dartmouth Summer Research
Project on Artiﬁcial Intelligence, A privately circulated report (August 1956)
119. Solomonoﬀ, R.J.: An inductive inference machine. In: IRE Convention Record,
Section on Information Theory, Part 2, pp. 56–62 (1957)
120. Solomonoﬀ, R.J.: The mechanization of linguistic learning. In: Proceedings of
the Second International Congress on Cybernetics, Namur, Belgium, pp. 180–193
(May 1958)
121. Solomonoﬀ, R.J.: Utility evaluation. Publication VI23 30, Zator Co. and Air Force
Oﬃce of Scientiﬁc Research, U.S.A. (April 1958)
122. Solomonoﬀ, R.J.: A new method for discovering the grammars of phrase struc-
ture languages. In: Proceedings of the International Conference on Information
Processing. UNESCO, Paris, France (1959)
123. Solomonoﬀ, R.J.: A progress report on machines to learn to translate languages
and retrieve information. In: Advances in Documentation and Library Science,
Vol. III, Part 2 (Reprint from Proceedings of International Conference for Stan-
dards on a Common Language for Machine Searching and Translation 1959),
vol. III, pp. 941–953. Interscience Publishers (September/October 1959)
124. Solomonoﬀ, R.J.: Progress report: Research on inductive inference for the year
ending 31 March 1959. Technical Report ZTB-130, Zator Co. and Air Force Oﬃce
of Scientiﬁc Research, U.S.A. (May 1959)
125. Solomonoﬀ, R.J.: A preliminary report on a general theory of inductive inference.
Technical Report V-131, Zator Co. and Air Force Oﬃce of Scientiﬁc Research,
Cambridge, Mass., U.S.A. (February 1960)
126. Solomonoﬀ, R.J.: A preliminary report on a general theory of inductive inference
(revision of Report V-131). Technical Report ZTB-138, Zator Co. and Air Force
Oﬃce of Scientiﬁc Research, Cambridge, Mass., U.S.A. (November 1960)
127. Solomonoﬀ, R.J.: A coding method for inductive inference. Technical Report ZTB-
140, Zator Co. [and perhaps Rockford Research Co.] (Prepared for Air Force Oﬃce
of Scientiﬁc Research, Air Research and Development Command, U.S. Air Force),
Cambridge, Mass., U.S.A. (April 1961)

Introduction to Ray Solomonoﬀ85th Memorial Conference
33
128. Solomonoﬀ, R.J.: Progress report: Research in inductive inference for the period 1
April 1959 to 30 November 1960. Technical Report ZTB 139, Rockford Research
Co. and Air Force Oﬃce of Scientiﬁc Research, U.S.A. (January 1961)
129. Solomonoﬀ, R.J.: Comments on Dr. S. Watanabe’s paper. Synthese 14(2), 97–100
(September 1962)
130. Solomonoﬀ, R.J.: An inductive inference code employing deﬁnitions. Technical
Report ZTB-141, Zator Co. [and perhaps Rockford Research Co.] (Prepared for
Air Force Oﬃce of Scientiﬁc Research, Air Research and Development Command,
U.S. Air Force), Cambridge, Mass., U.S.A. (April 1962)
131. Solomonoﬀ, R.J.: Training sequences for mechanized induction. In: Yovits, M.,
Jacobi, Goldstein (eds.) Self-Organizing Systems, pp. 425–434. Spartan Books
(1962)
132. Solomonoﬀ, R.J.: A formal theory of inductive inference. Information and Con-
trol 7, 1–22, 224–254 (1964)
133. Solomonoﬀ, R.J.: A formal theory of inductive inference: Part I. Information and
Control 7(1), 1–22 (March 1964)
134. Solomonoﬀ, R.J.: A formal theory of inductive inference: Part II. Information and
Control 7(2), 224–254 (June 1964)
135. Solomonoﬀ, R.J.: Some recent work in artiﬁcial intelligence. Proceedings of the
IEEE 54(12), 1687–1697 (December 1966)
136. Solomonoﬀ, R.J.: Inductive inference research status, spring, 1967. Technical Re-
port RTB 154, Rockford Research Co. and Air Force Oﬃce of Scientiﬁc Research,
140 1/2 Mt, Auburn St., Cambridge, Mass., U.S.A. (July 1967)
137. Solomonoﬀ,
R.J.:
The
search
for
artiﬁcial
intelligence.
Electronics
and
Power 14(1), 8–11 (January 1968)
138. Solomonoﬀ, R.J.: The adequacy of complexity models of induction. In: Logic,
Methodology and Philosophy of Science: Proceedings of the Fifth International
Congress, London, Ontario, Canada, pp. 19–20 (September 1975) (Section VI)
139. Solomonoﬀ, R.J.: Inductive inference theory - a uniﬁed approach to problems
in pattern recognition and artiﬁcial intelligence. In: Proceedings of the Fourth
International Joint Conference on Artiﬁcial Intelligence, Tbilisi, Georgia, U.S.S.R,
vol. 1, pp. 274–280 (September 1975), http://world.std.com/~rjs/pubs.html,
http://world.std.com/~rjs/tblisi75.pdf
140. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and con-
vergence theorems. IEEE Transaction on Information Theory, IT-24(4), 422–432
(1978)
141. Solomonoﬀ, R.J.: Perfect training sequences and the costs of corruption — a
progress report on inductive inference research. Technical report, Oxbridge Re-
search, Cambridge, MA, U.S.A. (August 1982)
142. Solomonoﬀ, R.J.: Optimum sequential search. Technical report, Oxbridge Re-
search, Cambridge, Mass., U.S.A. (June 1984)
143. Solomonoﬀ, R.J.: The time scale of artiﬁcial intelligence; reﬂections on social
eﬀects. Human Systems Management 5, 149–153 (1985)
144. Solomonoﬀ, R.J.: Two kinds of complexity. Technical report, Oxbridge Research,
Cambridge, Mass., U.S.A. (1985)
145. Solomonoﬀ, R.J.: The application of algorithmic probability to problems in artiﬁ-
cial intelligence. In: Kanal, L.N., Lemmer, J.F. (eds.) Uncertainty in Artiﬁcial In-
telligence, pp. 473–491. Elsevier Science Publishers B.V. (1986); Also in: Kochen,
M., Hastings, H.M.: Advances in Cognitive Science. AAAS Selected Symposia
Series, pp. 210–227. AAAS, Washington, D.C. (1988)

34
D.L. Dowe
146. Solomonoﬀ, R.J.: A system for incremental learning based on algorithmic probabil-
ity. In: Proceedings of the Sixth Israeli Conference on Artiﬁcial Intelligence, Com-
puterVisionandPatternRecognition,TelAviv,Israel,pp.515–527(December1989)
147. Solomonoﬀ, R.J.: Does algorithmic probability solve the problem of induction?
In: Dowe, D.L., Korb, K.B., Oliver, J.J. (eds.) Proceedings of the Information,
Statistics and Induction in Science (ISIS) Conference, Melbourne, Australia, pp.
7–8. World Scientiﬁc (August 1996) ISBN 981-02-2824-4
148. Solomonoﬀ, R.J.: The discovery of algorithmic probability. Journal of Computer
and System Sciences 55(1), 73–88 (1997)
149. Solomonoﬀ, R.J.: Does algorithmic probability solve the problem of induction? Re-
port, Oxbridge Research, P.O.B. 400404, Cambridge, Mass. 02140, U.S.A. (1997),
http://world.std.com/~rjs/isis96.pdf
150. Solomonoﬀ, R.J.: Two kinds of probabilistic induction. Computer Journal 42(4),
256–259 (1999); Special Issue on Kolmogorov Complexity
151. Solomonoﬀ, R.J.: Progress in incremental machine learning. In: NIPS Workshop
on Universal Learning Algorithms and Optimal Search, Whistler, BC, Canada.
NIPS (2002)
152. Solomonoﬀ, R.J.: Progress in incremental machine learning (Preliminary report
for NIPS 2002 workshop on universal learners and optimal search). Technical
report, Technical Report IDSIA-16-03, IDSIA, Lugano, Switzerland (2003); Given
at NIPS Conference, Whistler, B.C., Canada (December 14, 2002)
153. Solomonoﬀ, R.J.: The universal distribution and machine learning. The Computer
Journal 46(6), 598–601 (2003); Inaugural Kolmogorov Lecture, CLRC, Royal Hol-
loway, University of London, England, U.K. (February 27, 2003)
154. Solomonoﬀ, R.J.: Algorithmic probability, AI and NKS (given at Midwest NKS
Conference, U.S.A.) (October 2005), http://world.std.com/~rjs/lects.html;
also www.cs.indiana.edu/~dgerman/2005midwestNKSconference/keynotes/
ray-j-solomonoff.ram
155. Solomonoﬀ, R.J.: Lecture 1: Algorithmic probability (given at M.I.T., Cambridge,
Ma., U.S.A.) (2005), http://world.std.com/~rjs/lects.html
156. Solomonoﬀ, R.J.: Lecture 2: Applications of algorithmic probability. (given at
M.I.T., Cambridge, Ma., U.S.A.) (2005),
http://world.std.com/~rjs/lects.html
157. Solomonoﬀ, R.J.: Machine learning - past and future, Dartmouth, N.H., U.S.A.,
(July 13-15, 2006); Lecture given in 2006 at AI@50, The Dartmouth A. I. Con-
ference: The Next Fifty Years. (Revision August 11, 2009)
158. Solomonoﬀ, R.J.: Incomputability in games, wars and economics — inductive infer-
ence in hostile environments. Logic, Computability and Randomness, page 19 (2007)
159. Solomonoﬀ, R.J.: The probability of “undeﬁned” (non-converging) output in
generating the universal probability distribution. Information Processing Let-
ters 106(6), 238–240 (2007)
160. Solomonoﬀ, R.J.: Three kinds of probabilistic induction: Universal distributions
and convergence theorems. Computer Journal 51(5), 566–570 (2008); Christopher
Stewart WALLACE (1933-2004) Memorial Special Issue
161. Solomonoﬀ, R.J.: Algorithmic probability: Theory and applications. In: Dehmer,
M., Emmert-Streib, F. (eds.) Information Theory and Statistical Learning.
Springer Science and Business Media, pp. 1–23. Springer, N.Y., U.S.A. (2009)
162. Solomonoﬀ, R.J.: Algorithmic probability, heuristic programming and AGI. In:
Proceedings of the Third Conference on Artiﬁcial General Intelligence, AGI 2010,
Lugano, Switzerland, pp. 251–257. IDSIA (March 2010)
163. Solomonoﬀ, R.J.: Algorithmic Probability – Its Discovery – Its Properties and Ap-
plication to Strong AI, pp. 149–157. World Scientiﬁc Publishing Company (2011)

Introduction to Ray Solomonoﬀ85th Memorial Conference
35
164. Solomonoﬀ, R.J., Rapoport, A.: Structure of random nets. In: Proc. Int. Cong.
Mathematicians, Providence, R.I., U.S.A., pp. 674–675. American Mathematical
Society (1950)
165. Solomonoﬀ, R.J., Rapoport, A.: Connectivity of random nets. Bulletin of Math-
ematical Biophysics 13(2), 107–117 (1951)
166. Solomonoﬀ, R.J., Saleeby, E.G.: On the application of algorithmic probability to
autoregressive models. In: Dowe, D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI),
vol. 7070, pp. 366–385. Springer, Heidelberg (2013)
167. Strannegard, C., Amirghasemi, M., Ulfsbacker, S.: An anthropomorphic method
for number sequence problems. In: Cognitive Systems Research (in press, 2013),
doi:10.1016/j.cogsys.2012.05.003
168. Sunehag, P., Hutter, M.: Principles of solomonoﬀinduction and AIXI. In: Dowe,
D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 386–398. Springer,
Heidelberg (2013)
169. Suzuki, J.: MDL/Bayesian criteria based on universal coding/Measure. In: Dowe,
D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 399–410. Springer,
Heidelberg (2013)
170. Takahashi, H.: Algorithmic analogies to Kamae-Weiss theorem on normal num-
bers. In: Proceedings of Solomonoﬀ85th Memorial Conference. Springer (2013)
171. Tan, P.J., Dowe, D.L.: Decision forests with oblique decision trees. In: Gelbukh,
A., Reyes-Garcia, C.A. (eds.) MICAI 2006. LNCS (LNAI), vol. 4293, pp. 593–603.
Springer, Heidelberg (2006)
172. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proc. London Math. Soc. 2 42, 230–265 (1936)
173. Turing, A.M.: Computing machinery and intelligence. Mind 59, 433–460 (1950)
174. Ulam, S.: Tribute to John von Neumann. Bull. American Mathematical Soc. 64(3),
1–49 (1958)
175. Veness, J., Ng, K.S., Hutter, M., Uther, W., Silver, D.: A Monte-Carlo AIXI
Approximation. J. Artiﬁcial Intelligence Research 40, 95–142 (2011)
176. Vinge, V.: Technological singularity. In: VISION-21 Symposium Sponsored by
NASA Lewis Research Center and the Ohio Aerospace Institute, vol. 30, p. 31
(March 1993)
177. Visser, G., Dale, P.E.R., Dowe, D.L., Ndoen, E., Dale, M.B., Sipe, N.: A novel
approach for modeling malaria incidence using complex categorical household
data: The minimum message length (MML) method applied to Indonesian data.
Computational Ecology and Software 2(3), 140–159 (2012)
178. Visser, G., Dowe, D.L.: Minimum message length clustering of spatially-correlated
data with varying inter-class penalties. In: Proc. 6th IEEE International Conf. on
Computer and Information Science (ICIS) 2007, pp. 17–22 (July 2007)
179. Visser, G., Dowe, D.L., Uotila, J.P.: Enhancing MML clustering using context
data with climate applications. In: Nicholson, A., Li, X. (eds.) AI 2009. LNCS,
vol. 5866, pp. 350–359. Springer, Heidelberg (2009)
180. Wallace, C.S.: Digital computers. In: Butler, S.T., Messel, H. (eds.) Atoms to
Andromeda, pp. 215–245. Shakespeare-Head, Sydney (1966)
181. Wallace, C.S.: An improved program for classiﬁcation. In: Proc. of the 9th Aus-
tralian Computer Science Conference (ACSC-9), pp. 357–366 (February 1986);
Published as Proc. of ACSC-9, vol. 8(1)
182. Wallace, C.S.: Classiﬁcation by minimum-message-length encoding. In: Akl, S.G.,
Fiala, F., Koczkodaj, W.W. (eds.) Advances in Computing and Information -
ICCI 1990. LNCS, vol. 468, pp. 72–81. Springer, Heidelberg (1990)

36
D.L. Dowe
183. Wallace, C.S.: Classiﬁcation by minimum-message-length inference. In: Working
Notes AAAI Spring Symposium Series, Stanford Uni., Calif., U.S.A., pp. 65–69
(1990)
184. Wallace, C.S.: False oracles and SMML estimators. In: Dowe, D.L., Korb, K.B.,
Oliver, J.J. (eds.) Proceedings of the Information, Statistics and Induction in
Science (ISIS) Conference, Melbourne, Australia, pp. 304–316. World Scientiﬁc
(August 1996) ISBN 981-02-2824-4; Was previously Tech. Rept. 89/128, Dept.
Comp. Sci., Monash Univ., Australia (June 1989)
185. Wallace, C.S.: Intrinsic classiﬁcation of spatially correlated data. Computer Jour-
nal 41(8), 602–611 (1998)
186. Wallace, C.S.: The MIT Encyclopedia of the Cognitive Sciences (MITECS), chap-
ter Minimum description length (major review), pp. 550–551. The MIT Press,
London (1999) ISBN: 0-262-73124-X
187. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Springer (May 2005)
188. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Com-
puter J. 11(2), 185–194 (1968)
189. Wallace, C.S., Boulton, D.M.: An invariant Bayes method for point estimation.
Classiﬁcation Society Bulletin 3(3), 11–34 (1975)
190. Wallace, C.S., Dowe, D.L.: Intrinsic classiﬁcation by MML - the Snob program.
In: Proc. 7th Australian Joint Conf. on Artiﬁcial Intelligence, pp. 37–44. World
Scientiﬁc (November 1994)
191. Wallace, C.S., Dowe, D.L.: Minimum message length and Kolmogorov complexity.
Computer J. 42(4), 270–283 (1999)
192. Wallace, C.S., Dowe, D.L.: Reﬁnements of MDL and MML coding. Computer
Journal 42(4), 330–337 (1999)
193. Wallace, C.S., Dowe, D.L.: Rejoinder. Computer Journal 42(4), 345–347 (1999)
194. Wallace, C.S., Dowe, D.L.: MML clustering of multi-state, Poisson, von Mises
circular and Gaussian distributions. Statistics and Computing 10, 73–83 (January
2000)
195. Wallace, C.S., Freeman, P.R.: Estimation and inference by compact coding. Jour-
nal of the Royal Statistical Society Series B 49(3), 240–252 (1987); See also Dis-
cussion on pp. 252-265
196. Wallace, C.S., Georgeﬀ, M.P.: A general objective for inductive inference. Techni-
cal Report #83/32, Department of Computer Science, Monash University, Clay-
ton, Australia, Reissued in June 1984 as TR No. 44 (March 1983)
197. Wallace, C.S., Patrick, J.D.: Coding decision trees. Machine Learning 11, 7–22
(1993)
198. Webb, G.I., Boughton, J., Zheng, F., Ting, K.M., Salem, H.: Learning by extrap-
olation from marginal to full-multivariate probability distributions: Decreasingly
naive Bayesian classiﬁcation. Machine Learning 86(2), 233–272 (2012)
199. Wei Xing, Croft, W.B.: LDA-based document models for ad-hoc retrieval. In:
Proc. 29th ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2006, New York, NY, USA, pp. 178–185 (2006)
200. Wood, I., Sunehag, P., Hutter, M. (Non-)Equivalence of universal priors. In: Dowe,
D.L. (ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 417–425. Springer,
Heidelberg (2013)
201. Woodward, J., Swan, J.: A syntactic approach to prediction. In: Dowe, D.L.
(ed.) SolomonoﬀFestschrift. LNCS (LNAI), vol. 7070, pp. 426–438. Springer,
Heidelberg (2013)

Ray Solomonoﬀand the New Probability
Grace Solomonoﬀ⋆
Oxbridge Research
Cambridge, Mass. 02140
trovaxo@yahoo.com
http://world.std.com/~rjs
Abstract. This is the story of Ray Solomonoﬀ’s Life and Times. He
was there at Dartmouth in 1956 when Artiﬁcial Intelligence was ﬁrst
given its name, and took part in the major events during this unique
era right up to his death in 2009. His invention in 1960 of Algorithmic
Probability, with its multiple descriptions of data, led to better ways of
handling data and prediction for machine learning. The theorems that are
part of his discovery lie at the heart of Algorithmic Information Theory.
Ray championed probability in AI during the decades it was unpopular
and lived to see a renaissance in systems that learn and reason using
probability. The story of his life is the story of a great adventure.
Keywords: Solomonoﬀ, Artiﬁcial Intelligence, Algorithmic Probabil-
ity, Algorithmic Information Theory, Inductive Inference, Kolmogorov
Complexity.
1
Introduction
This is the story of Ray Solomonoﬀ’s life, beginning with the immigration of his
parents from Russia to the U.S. in the early 1900’s and Ray’s birth in 1926. There
was a cheerful dedication in Ray’s life: from his childhood enthusiasm for math
and science, to his contribution in 1956 to the birth of Artiﬁcial Intelligence,
his invention of a new probability and his participation in major events during
this unique era right up to his death in 2009. His greatest achievement was the
invention of Algorithmic Probability. His vision of probability and machines that
think will be part of our future.
2
Early Years
Ray’s mother, Sarah Mashman, was born in Sevastopol, a port city in Ukraine,
located on the Black Sea coast of the Crimea peninsula. She graduated from
high school in 1911.
⋆Thanks to Donald Rosenthal and Julie Sussman for their invaluable help. Some
of this report appeared in (open sources) Algorithms, 3(3), 2010, and Oxbridge
Research. Thanks to Oxbridge Research (open source), Marcus Hutter, Henry Tirri,
Juergen Schmidhuber, Alex Solomonoﬀfor permission to reprint pictures.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 37–52, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

38
G. Solomonoﬀ
Before the Russian Revolution of 1917 there was a part of Russia called the
Pale of Settlement: It included present-day Poland, Lithuania, the Crimea and
the Ukraine. Jewish people were restricted to that area.
During around 1890-1910, the Czarist government was coming apart and
blaming it on the Jews, and anti-Jewish persecution was particularly intense
at this time. About 1/3 or 1/2 of all the Jews in Russia emigrated, mostly to
the U.S. Ray’s parents were part of this wave.
Sevastopol, a wealthy city, was exempt from the Ukrainian Pale. Only the
fact that Sarah’s father was the local blacksmith, with specialized knowledge,
enabled his family to live there. Sarah immigrated to New York about three
years after high school. She got a job as a nurse’s aide, and on the side began
an amateur career of acting.
Rays father, Phillip Julius Solomonoﬀ, would never tell where he was from,
but Vilna (in Lithuania) is on his passport. He got inspired with revolutionary
fervor and in 1917 when the Russian revolution started, he and cousin Esoc went
to “visit revolution.” They returned soon after discovering that the shooting was
with real bullets.
Julius joined a Polish ship, and made his way to the U.S., where he jumped
ship and ended up in New York. Julius was an illegal alien all his life. But he
studied at Cooper Union in New York City to be a plumber and was ﬁrst in his
class. He met Sarah, and they lived in New York, where in 1922 their ﬁrst son
George was born. In 1924 they moved to Cleveland, Ohio, a Midwest city, called
the largest small town in the U.S.
And Ray was born July 25, 1926.
Pictures: 1. Ray and his father. 2. Ray with his brother George. Ray is on
the left, already with his penetrating gaze. 3. The obligatory picture of Ray on
a horsie.

Ray Solomonoﬀand the New Probability
39
Ray grew up during the depression. His father was a mechanic but his work
never paid enough to cover the rent. His parents had to move frequently, which
was a trauma to them; but Ray and George thought it was great — they got to
meet new kids, have new adventures.
When they ﬁnally settled down. Ray built a lab in his parents’ cellar. To vent
the smoke from his experiments, he drilled a hole through the wall behind some
bushes. His parents never found it, and it remained unseen when the house was
sold and was there until the day the place was torn down, around 1998. But his
parents were really tolerant anyway. Sarah once described sweeping the rug and
hearing a multitude of tiny explosions from some grains of something scattered
about from a experiment gone awry.
In later life Ray made quite a few inventions, for friends or home use. For
example he made a Hurry clock for Marvin Minsky — a clock that was labeled
“Hurry”; he removed a gear so it ran very fast — I guess you’d know you were
always late.
But his greatest pleasure was in theories and discovery
He wrote “I ﬁrst experienced the pure joy of mathematical discovery when I
was very young — learning algebra. I didn’t really see why one of the axioms
was ‘obvious’, so I tried reversing it and exploring the resultant system. For a
few hours I was in this wonderful never-never land that I myself had created!
The joy of exploring it only lasted until it became clear that my new axiom
wouldn’t work, but the motivation of the joy of discovery continued for the rest of
my life.”
“The motivation for the discovery of Algorithmic Probability was somewhat
diﬀerent. It was the result of ‘motivated discovery’ — like the discovery of the
double helix in biology, but with fewer people involved and relatively little po-
litical skullduggery. The goal I set grew out of my early interest in science and
mathematics. I found that while the discoveries of the past were interesting to
me, I was even more interested in how people discovered things. Was there a gen-
eral technique to solve all mathematical problems? Was there a general method
by which scientists could discover all scientiﬁc truths?”[18]
And so by 1942, around the age of 16, Ray became captivated by the idea of
ﬁnding a general technique to solve all problems. He felt already that scientists
used probability in induction when they invented theories to account for data.
This was when he ﬁrst began to study induction. Even at this age, probability
was part of his thinking.
He organized notebooks using key letters: TM meant Thinking Machine. His
later notes all use key letters such as ALP for Algorithmic Probability, TS for
Training Sequences, even HR for horse racing.
He went to Glenville High School, noted for its famous graduates. After grad-
uation there was a hiatus in his studies, for World War II intervened. The draft
began in 1940, and in November 1944 Ray joined the Navy to train in and then
teach radio and was stationed in Gulfport, Mississippi.

40
G. Solomonoﬀ
After the war, in 1946 he went to the University of Chicago on the GI bill. The
University was at its height: Enrico Fermi, Rudolf Carnap, Nicolas Rashevsky,
Anatole Rapoport were among Ray’s teachers.
3
From the University to the Birth of AI
In 1950 in a letter Ray writes “for the last 4 or 5 years cybernetics has been my
chief scientiﬁc interest” ...“about a month and a half ago I worked out a method
of devising a machine that would ‘think’.” But “the bubble broke. One found that
the machine wouldn’t work as well as expected” ... “But it was all a very wonderful
adventure — somewhat disturbing at times, but nonetheless wonderful.” At that
time he also tried to get together a group of scientists interested in thinking ma-
chines, but did not succeed. By then Ray was convinced that thinking machines
were feasible (the name Artiﬁcial Intelligence did not yet exist).
Claude Shannon’s paper in 1948, and subsequent developments in information
theory over the next few years, greatly inﬂuenced Ray during college. Shannon
thought that information was something that could be quantiﬁed and that the
quantity of information was related to its probability.
Rudolf Carnap was Ray’s professor of Philosophy and Probability. Carnap
was a “logical positivist.” He felt that logical syntax should provide a system of
concepts, a language, by the help of which the results of logical analysis would be
exactly formulable. Problems, such as what language to use, presented too many
diﬃculties to be successful, but the idea of combining language, information and
probability was part of what led to Ray’s discovery.
Ray studied mathematical biology with Rapoport and Rashevsky. His ﬁrst
published reports were three papers on neural nets, two with Anatole Rapoport
in 1950-52, that are regarded as the earliest statistical analysis of networks.
Here are some of the items Ray was reading during college:
“Foundations of the Theory of Probability” (Kolmogorov); “Experience and
Prediction” (Reichenbach); “Journal of Applied Physics”; “The Impact of
Science on Society” (Russell)...
Here are some more of the items Ray was reading during college:
“Astounding Science Fiction”; “Thrilling Wonder Stories”; “Operation
Fantast”; “Flash Gordon”...
Actually he was quite inﬂuenced by science ﬁction. His favorite movie was
‘The Shape of Things to Come’, that tells of a visionary people who would save
a dying world through technology.
In the 40’s and 50’s Science Fiction also represented something that was hap-
pening in the U.S., and in Europe also, a view that amazing things were pos-
sible through technology. The government began sponsoring projects that used
information technology.
Meanwhile, the computer was rapidly becoming important. In 1941 the elec-
tronic computer had been introduced to the public. Ray was fascinated by them.
Later, in the early 1970’s Ray even built a computer, hand-wired it — sort of
tapestry-like.
These were some of the early inventions and ideas that inﬂuenced Ray.

Ray Solomonoﬀand the New Probability
41
Pictures: 1. A family portrait when Ray was in high school (Ray on the left).
2. Ray in the Navy. 3. The computer that Ray made.
4
The Beginnings of AI
After college, Ray got a job in New York, working at Technical Research Group.
He met Marvin Minsky at a conference. Minsky and Ray became lifelong close
friends, and Ray often visited Cambridge, Massachusetts, where Minsky lived.
This was when Minsky and John McCarthy introduced him to another great
idea: The Turing machine. It was a revelation.
The Turing machine was a concept of Alan Turing’s, a British mathematician
who lived 1912-1954. In 1936 Turing described a “universal computing machine”
that could theoretically be programmed to solve any problem capable of solution
by a specially designed machine. The “Turing machine” foreshadowed the digital
computer.
Consider a computer as a ﬁnite state machine operating on a ﬁnite symbol
set. A program tape on which a binary program is written feeds from left to right
into the machine. The machine looks at the tape, changes its state according to
the program, writes some output, and then gets the next part of the tape. A
special Turing machine, the universal Turing machine, has an inﬁnite memory,
an input tape that goes only one way, and a work tape — this machine could
mimic any other computer.
Ray wrote “It gave me a quick intuitive grasp of many ideas that I had before
found incomprehensible. It is not unusual for the translation of a problem into
a new language to have this wonderful eﬀect.”[18]
In Cambridge, Ray had frequent discussions with Marvin Minsky and John
McCarthy and others about thinking, mathematics and machines. Then in 1956
John McCarthy organized a little group to brainstorm on machine intelligence.
They picked Dartmouth, New Hampshire for their venue. The principal par-
ticipants would include John, Ray, Marvin, Tenchard More, Alan Newell, Herb
Simon, and 4 other researchers.

42
G. Solomonoﬀ
McCarthy decided to give this new science a name: And so in the summer of
1956, the science named Artiﬁcial Intelligence was born.
The idea was that the 10 participants would brainstorm during 2 months at
Dartmouth, and then in 10 months would come up together with some Great AI
Idea. Oh sure! You have 10 mad scientists with 10 totally diﬀerent orientations
toward AI. What do you get? (10 totally diﬀerent Great AI Ideas.)
From most of these ideas came logic-based, deterministic programs. Ray pre-
sented a paper on how machines could be made to improve themselves, by using
unsupervised learning from examples. This paper was not probabilistic, but was
new in that it replaced semantic with symbolic representation.
In 2010 Marvin wrote me: “In the early 1950’s I developed many theories
about ways to make neural networks learn — but none of these seemed likely
to lead to high-level kinds of idea. But once Ray showed me his early ideas
about symbolic ‘inductive inference scheme’, these completely replaced my older
approach. So Ray’s ideas provided the start of many, later, steps toward Artiﬁcial
Intelligence.”
The ﬁrst question in Ray’s outline for work in Thinking Machines at the
Dartmouth Summer was “What is Probability?” Ray advocated a probabilistic
approach to machine intelligence at this ﬁrst meeting on AI in 1956, and con-
tinued to push on this dream for decades when such a view was controversial.
The Dartmouth Summer had diﬃculties; the Ford Foundation gave less fund-
ing than McCarthy hoped for, and some of the attendees spent little time there.
Of the programs from that summer, the most well known is Newell and Si-
mon’s Logical Theorist, and soon after that, their General Problem Solver. Their
focus, as was that of McCarthy, was logic based and highly specialized. This led
to “expert system” programs — for example, aids to medical diagnosis, or miner-
als prospecting. Really at this point there came a schism between the specialized,
deterministic and the more generalized kinds of programs. The more rigidly log-
ical projects were immediately applicable, and so AI became known by these
programs. Probabilistic based programs were largely ignored. They were not
well integrated into the world of computing, while logic based programs were
ideal for “if-then” and “do loops”.
5
The Discovery of Algorithmic Probability
Calvin Mooers was an attendee at the MIT Information Theory Symposium
in September 1956. Ray’s paper from the Dartmouth Summer Project, “An
Inductive Inference Machine,” was circulated there. Calvin liked Ray’s ideas and
invited him to come work with him in information retrieval. He got the perfect
government contract for Ray to work with him. In creating his proposal he wrote
Ray: “There seems to be developing a good possibility that would permit me to
put you to work on doing long-range thinking on ‘inductive inference machines’
and whatever else they may touch upon. This would be a program of purely
speculative thinking, and I would set up the support so that is understood.
While the money would come from an interest in information retrieval, I would

Ray Solomonoﬀand the New Probability
43
want the thinking untrammeled, and my thesis is that what you are doing, and
what I am doing in information retrieval, have a common meeting ground that
will develop in due time.” And he got the grant! Don’t you wish you got a grant
like that!
Ray received government funding during the next few years. He never would
again.
So Ray moved up to Cambridge, to work full time on his ideas about induction
and machine intelligence.
Here he is at Marvin’s house — and here — his thinking is pretty
untrammeled...
Over the next few years Ray worked on machine intelligence, programming
computers, use of the Turing machine, and coding methods. He still didn’t have
a really good way of combining them.
Ray had met Noam Chomsky in the mid 50’s and read Chomsky’s works
on context-free language. Ray realized this language could be a basis for solv-
ing Carnap’s problem. Working at Calvin’s Zator Company, Ray expanded this
language into a stochastic form for all types of patterns: it was a probabilistic
language. Normally one thinks of a language where either something is a sentence
or is not a sentence. In a probabilistic language something has a probability of
being a sentence. This provided the breakthrough he needed.
Using the Universal Turing Machine as the operator, Ray established that
the generator of these patterns could be put in a binary, probabilistic form; the
grammar of this language was somewhat like descriptions where simple descrip-
tions were more likely than complex ones. He called this Algorithmic Probability,
and he used it in a General Theory of Inductive Inference. In this discovery, in
1960, Ray became a founder of Algorithmic Information Theory.[21]
Prior to this discovery, the usual method of calculating probability was based
on frequency: take the ratio of favorable results to the total number of trials. Ray
seriously revised this deﬁnition of probability. Algorithmic Probability is based
on the length of random programs (algorithms) input into a universal Turing
machine that produce a given sequence of symbols as output — the shorter
programs being most likely.
His paper, “A Preliminary Report on a General Theory of Inductive Infer-
ence” is his ﬁrst known publication of Algorithmic Probability and was published
by Calvin’s company in February 1960,[10] with a revision in November 1960.

44
G. Solomonoﬀ
[11] He published a more complete exposition in two 1964 papers for the Journal
of Information and Control.[12],[13]
In a letter in 2011, Marcus Hutter wrote: “Ray Solomonoﬀ’s universal proba-
bility distribution M(x) is deﬁned as the probability that the output of a univer-
sal monotone Turing machine U starts with string x when provided with fair coin
ﬂips on the input tape. Despite this simple deﬁnition, it has truly remarkable
properties, and constitutes a universal solution of the induction problem.”(See
also [7])
Algorithmic Probability combines several major ideas; of these, two might be
considered more philosophical and two more mathematical.
The ﬁrst is related to the idea of Occam’s Razor: the simplest theory is the
best. Ray’s 1960 paper states “We shall consider a sequence of symbols to be
‘simple’ and have high a priori probability if there exists a very brief description
of this sequence — using of course some stipulated description method. More
exactly, if we use only the symbols 0 or 1 to express our description, we will
assign the probability of 2−N to a sequence of symbols, if its shortest possible
binary description contains N digits.”[11][10]
The second idea is similar to that of Epicurus: it is an expansion on the
shortest code theory; if more than one theory explains the data, keep all of the
theories. Ray writes “Equation 1 uses only the ‘minimal binary description’ of
the sequence it analyzes. It would seem that if there are several diﬀerent methods
of describing a sequence, each of these methods should be given some weight in
determining the probability of that sequence.”[11][10]
P(x)M =
∞

i=1
2−|si(x)|
This is the formula he developed to give each possible explanation the right
weight. (The probability of sequence x with respect to Turing Machine M is the
total sum of 2 to the minus length of each string s that produces an output that
begins with x.)
Closely related is the third idea of its use in a Bayesian framework. The
universal prior is taken over the class of all computable measures; no hypothesis
will have a zero probability. Using program lengths of all programs that could
produce a particular start of the string, x, Ray gets the prior distribution for
x, used in Bayes rule for accurate probabilities to predict what is most likely to
come next as the start is extrapolated. The Universal Probability Distribution
functions by its sum to deﬁne the probability of a sequence, and by using the
weight of individual programs to give a ﬁgure of merit to each program that
could produce the sequence. [11][12]
The fourth idea shows that the choice of machine, while it could add a constant
factor, would not change the probability ratios very much. These probabilities
are machine independent; this is the invariance theorem that is considered a
foundation of Algorithmic Information Theory.[11][13]

Ray Solomonoﬀand the New Probability
45
Here is another beautiful picture of this formula in a little diﬀerent
format: The photoshopped picture is by Marcus Hutter, using a photo by J¨urgen
Schmidhuber.
In 1965 the great Russian mathematician Andrey Kolmogorov published the
ﬁrst idea (simplicity) and fourth idea (machine independence) in the journal
Problems of Information Transmission.[4] Kolmogorov had also been working
with Turing machines, and had been giving lectures at Moscow University on the
subject of complexity. Like Ray, he revised the frequentist view of probability. He
wrote “The basic discovery, which I have accomplished independently from and
simultaneously with R. Solomonoﬀ, lies in the fact that the theory of algorithms
enables us to eliminate this arbitrariness by the determination of a ‘complexity’
which is almost invariant (the replacement of one method by another leads only
to the supplement of the bounded term).”[21]
Paul Vit´anyi writes that Kolmogorov’s introduction of complexity “was moti-
vated by information theory and problems of randomness. Solomonoﬀintroduced
algorithmic complexity independently and earlier and for a diﬀerent reason: in-
ductive reasoning. Universal a priori probability, in the sense of a single prior
probability that can be substituted for each actual prior probability in Bayes’s
rule was invented by Solomonoﬀwith Kolmogorov complexity as a side product,
several years before anybody else did.”[21]
Vit´anyi notes: “we will associate Solomonoﬀ’s name with the universal distri-
bution,
and
Kolmogorov’s
name
with
the
descriptional
complexity.”[6]
Kolmogorov was interested in the information content of a string, while Ray
was interested in the predictive power of a string.
With respect to thinking machines: If a machine is working on a set of prob-
lems, the algorithm it uses may only work for some problems. If you can use
probability in the best possible way to estimate how likely each one in a whole
set of algorithms is, then if one program (algorithm) doesn’t work, you have a
good method, a probabilistic way, to search for another.

46
G. Solomonoﬀ
Ray’s ideas were ahead of his time, especially here in America. He gave some
lectures at local universities. Often students didn’t know what he was talking
about (he wasn’t too good at explaining this stuﬀanyway!). Students would fall
asleep or quietly escape out the side door.
Never mind; meanwhile he bought some land in New Hampshire and built a
house. Since he didn’t know anything about building a house, he got a book. He
built the house as a cube, since that was easiest. He had real sloping roof, to let
the snow oﬀ.
He didn’t know about heating, but he knew about electric light bulbs, and in
those days a light bulb was 80% heat and only 20% light, so he heated the house
with light bulbs. His friend Al Jenks told me recently that Al would bring his
friends over, ostensibly to meet Ray, but actually to show them that Ray really
did heat his house entirely with light bulbs — two long rows along the ceiling
from wall to wall.
Pictures: 1. the house. 2: house decor. 3. A later picture by Alex Solomonoﬀ
of Ray working on the house.
It had no windows, but you could pull out long sections of the wall from ﬂoor
to roof and put in screens during the summer.
And it was shortly after he ﬁnished the house that he met...ME.
Pictures: 1. My baby picture. 2. A few years after meeting Ray. 3. Ray, a few
years before meeting me.

Ray Solomonoﬀand the New Probability
47
In 1969, I met Ray at a party at a friend’s house. We often went to the house
Ray built in New Hampshire.
I wrote poetry and saw that good poems often have good metaphors. A
metaphor is an alternate description of something; it helps you see in a dif-
ferent way. The goal is to make the subject of the poem meaningful or beautiful.
When Ray told me what he was doing, just like those students, I didn’t know
what he was talking about, but his multiple theories and multiple descriptions
seemed like a mathematical analogue to poetry. It felt like we were on some kind
of similar adventure. But even if our goals had been diﬀerent, we shared interests
and feelings — our life has been this way forever.
We had an apartment in Harvard Square, Cambridge, which we kept through
the 90’s. Harvard Square was where you met everybody as you wandered around
with your backpack. We were there during the Vietnam War protests. From
the building roof we watched the Weathermen break bank windows. We were
there during the days of the ﬂower children. Ray was a friend of the poet Allen
Ginsberg. Whenever we were at Ginsberg’s poetry readings he would hand Ray
his watch to monitor the time.
Our apartment had a lot of stuﬀ: electronics and computer “toys” and build-
ing materials; but mainly our apartment had books — stacks and stacks of pa-
pers and books. All the walls had bookshelves, even the ceiling had bookshelves
hanging from it across the hallway.
Ray read voraciously about all kinds of things, but was focused on his predic-
tion theory, talking to others, often at the AI Lab at MIT. It was a very exciting
time.
In 1970 since the military had stopped funding civilian science, Ray formed his
own one-man company, “Oxbridge Research,” an extremely eclectic organization.
On the right, a portrait of Ray in 1984. On the left, a portrait of Ray in
Oxbridge Research’s Chemical Experimentation Lab, making our most impor-
tant invention: the liquid helium zonk. The zonks are a whole class of drinks.
One example: take a banana, two kinds of rum, lemon and honey and blend them
in a blender, then freeze into a subzero mush. Other inventions were the pickled
herring cocktail and studies of the brightness of various drinks with mixes that
ﬂuoresced in ultraviolet light.

48
G. Solomonoﬀ
In 1975 Chaitin, who had developed a version of descriptional (Kolmogorov)
complexity later in the 1960’s, wrote about complexity in the Scientiﬁc American.
In 1978 Leonid Levin and Peter G´acs came to America and were very supportive
and interested in Ray’s probability theory.
6
The Guerrilla Workshop
However — the dominant researchers used deductive, logic-based methods; de-
terministic expert systems were the popular products of AI. Many scientists felt
uncertainty just wouldn’t work in this ﬁeld. The turning point, at least in the
U.S.— was in 1985.
The ﬁrst workshop on Uncertainty in AI (UAI) was in 1985. How did it
happen? The program note was sort of like the Declaration of Independence.
It says: “This workshop came about as a result of a panel discussion at the
AAAI conference at Austin, Texas, U.S., in 1984. This panel was on the problem
of the representation of uncertainty in Artiﬁcial intelligence.” ... “Some speak-
ers implied that more than one number was necessary to represent uncertainty,
while others stated that numbers should not be used at all! Except for a valiant
rearguard defense by Judea Pearl, everyone on the panel agreed that probability
as a representation of uncertainty either was misguided or inadequate for the
task. Several of us who have been using probability within AI, as well as engi-
neers and physicists, know this conclusion to be false, and our outrage at this
denigrating of probability was the spur that triggered this workshop.”
This workshop has continued ever since, and was a starting point that led to
a revolution in mainstream AI. It is now the annual Conference in Uncertainty
in AI (UAI), hosted by AAAI. For this workshop Ray gave a paper on how to
apply the universal distribution to problems in AI: the best order of searching
for solutions is T 1/P1, where T 1 is the time needed to test the trial and P1
the probability of success of that trial — the shorter codes getting the higher
probability. This is based on the search technique invented by Leonid Levin,[5]
so Ray called it Lsearch.
I remember that ﬁrst meeting: Ray called it a guerrilla workshop. But it
marked an acceptance of many aspects of probability by mainstream AI. There
still is conﬂict in AI circles over the use of probability. But the logic and prob-
abilistic reasoning are moving closer. For example, I believe John McCarthy
shifted his focus from deductive logic in his development of what he calls ‘cir-
cumscription’, which is more probabilistic. Things like fuzzy logic and Bayesian
work lie in the probabilistic area.
Ray inspired many people, especially young people. Our nephew Alex would
visit from Cleveland. He says “There was nothing like this for me in Cleveland.
Ray talked about mathematics in a way that made it exciting.” Ray inﬂuenced
him to get his Ph.D. in mathematics, and we have remained close to Alex ever
since.

Ray Solomonoﬀand the New Probability
49
Pictures: 1. Alex’s Ph.D. graduation, May 1992. 2. Ray, me, our niece Nickie,
and Alex in 2007.
7
Later Work
Ray spent the rest of his life discovering, proving aspects, reﬁning and enlarging
his General Theory of Inductive Inference, with the goal of having machines that
could solve hard problems. He stressed that machine intelligence did not need
to emulate human intelligence, and probably that it would not.
He wrote about the problem of incomputability. His quotable quote: “Incom-
putability — it’s not a bug, it’s a feature!” Systems that are computable will
not be complete. The incomputability is because some algorithms can never be
evaluated because it would take too long. But these programs will at least be
recognized as possible solutions. On the other hand, any computable system is
incomplete. There will always be descriptions outside that system’s search space
which will never be acknowledged or considered, even in an inﬁnite amount
of time. Computable predictions models hide this fact by ignoring such algo-
rithms. Minimum Description Length, for example, ﬁrst cuts out some space and
then chooses the shortest program. Minimum Message Length of Chris Wallace
does acknowledge incomputability; it is closer to Algorithmic Probability.[22][23]
It chooses only the shortest code to work with, however, while Algorithmic
Probability uses as many as there is time for.
In other papers Ray wrote about how best to limit search by limits on time
or computation cost. He developed methods for working with other types of
data, not just sequences. He categorized problems into various types such as
“inversion problems” and “time-limited optimization problems” and developed
ways of dealing with the diﬀerent types.
Throughout his career Ray was concerned with the potential beneﬁts and
dangers of AI, discussing it in many of his published reports. In 1985 he analyzed
a likely evolution of AI, giving a formula predicting when it would reach the
“Inﬁnity Point.” This Inﬁnity Point is an early version of the “Singularity” later
made popular by Ray Kurzweil.[16]
During most of his life, Ray worked independently, developing his theories
without any academic or industrial support. Paul Vit´anyi notes “it is unusual
to ﬁnd a productive major scientist that is not regularly employed at all.”

50
G. Solomonoﬀ
However, he would meet frequently with Minsky, Shannon, and others at MIT,
and with other researchers throughout the world. We went to many countries
and conferences meeting amazing and dedicated people. There was Saarland
University in Saarbr¨ucken, Germany, where Wolfgang Paul invited Ray to do
research in 1990-1991. There was the ISIS conference in Australia in 1996; after it
the indefatigable David Dowe drove us for miles along the Great Ocean Road and
we explored the ancient and beautiful rain forest there. Later we stayed with Paul
Vit´anyi, who traveled with us by bus and car, and then we visited other areas.
When Ray was a little boy, he thought about being a naturalist; Dr. Doolittle was
one of his favorite books. How could he not be entranced by seeing Cassowaries?
There was his visiting professorship at the Dalle Molle Institute for Artiﬁcial
Intelligence in Lugano, Switzerland, run by J¨urgen Schmidhuber. The researchers
at that Institute were so cohesive. We had community meals — memorable
spaghetti dinners with coﬀee from their super espresso machine. There was our
visit, in 1998, to the dynamic Computer Research Learning Center, at Royal
Holloway, University of London, which Alex Gammerman had just founded.
Later Ray gave the ﬁrst Kolmogorov lecture there, receiving the Kolmogorov
Award. Ray was a visiting professor there until his death.
Here are some pictures from over the years:
Pictures: 1. Ming Li, his wife Jessie Zou, Ray and me, maybe in the 1990’s.
2. and 3. Pictures by J¨urgen Schmidhuber and Henry Tirri. at NIPS Workshop
on Universal Learning Algorithms and Optimal Search, 2002.
Ray was happy when the AGI08 conference occurred; this was ﬁrst of the
AGI conferences focused on Artiﬁcial General Intelligence, moving as far away
as possible from narrowly focused and highly specialized programs.
Eric Horvitz, for many years the President of AAAI, notes Ray “advocated
the probabilistic approach to machine intelligence at the ﬁrst meeting on AI
in 1956, continued to push on this dream for decades when such a view was
controversial, and lived to see a renaissance in systems that learn and reason
under uncertainty, relying on representations of probability — a perspective
that is now at the foundation of modern AI research.”

Ray Solomonoﬀand the New Probability
51
But of all his productive life, his greatest invention is Algorithmic Probability
and his General Theory of Inductive Inference.
Leonid Levin wrote that Ray “had a very powerful and general approach. In
the future, his ideas will have more inﬂuence.”
And his whole life Ray did what he loved doing. Vit´anyi notes “But from all
the elder people (not only scientists) I know, Ray Solomonoﬀwas the happiest,
the most inquisitive, and the most satisﬁed. He continued publishing papers right
up to his death at 83.”[3]
Ray enjoyed life up to the end, organizing a gorgeous costume for Halloween
at the end of October; and also continued with his serious side, his work on
Algorithmic Probability and prediction, completing a paper for AGI 10, in late
November 2009, just before he died.
In his last paper, for AGI10, Ray discussed what he called “The Guiding
Probability Distribution” — a nice updating method, and a name that seems to
me like something that was guiding him too. So he had a long happy life, which
is something to celebrate. And even more than that, he had a shining vision he
followed for his whole life, and it guided him right to the very end.
References
1. Carnap, R.: Logical Foundations of Probability (1950)
2. Chomsky, A.N.: Three models for the description of language. IRE Transactions
on Information Theory, IT-2(3), 113–124 (1956)
3. G´acs, P., Vit´anyi, P.: Raymond J. Solomonoﬀ1926 – 2009. IEEE Information
Theory Society Newsletter 61(11) (March 2011)
4. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1(1), 1–7 (1965)
5. Levin, L.A.: Universal search problems. Problemy Peredaci Informacii (9), 115–116
(1973); Translated in Problems of Information Transmission 9, 265–266
6. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions, 3rd edn. Springer, N.Y. (2008)
7. Rathmanner, S., Hutter, M.: A philosophical treatise of universal induction. En-
tropy (13), 1076–1136 (2011)

52
G. Solomonoﬀ
8. Shannon, C.E.: The mathematical theory of communication. Bell System Technical
Journal (27), 379–423, 623–656 (1948)
9. Solomonoﬀ, R.J.: An inductive inference machine. Dartmouth Summer Research
Project on Artiﬁcial Intelligence (August 1956); A privately circulated report
10. Solomonoﬀ, R.J.: A preliminary report on a general theory of inductive inference.
Technical Report V-131, Zator Co. and Air Force Oﬃce of Scientiﬁc Research,
Cambridge, Mass. (February 1960)
11. Solomonoﬀ, R.J.: A preliminary report on a general theory of inductive inference
(revision of Report V-131). Technical Report ZTB-138, Zator Co. and Air Force
Oﬃce of Scientiﬁc Research, Cambridge, Mass. (November 1960)
12. Solomonoﬀ, R.J.: A formal theory of inductive inference: Part I. Information and
Control 7(1), 1–22 (1964)
13. Solomonoﬀ, R.J.: A formal theory of inductive inference: Part II. Information and
Control 7(2), 224–254 (1964)
14. Solomonoﬀ, R.J.: Inductive inference theory - a uniﬁed approach to problems in
pattern recognition and artiﬁcial intelligence. In: Proceedings of the Fourth In-
ternational Joint Conference on Artiﬁcial Intelligence, Tbilisi, Georgia, U.S.S.R.,
vol. 1, pp. 274–280 (September 1975)
15. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Transactions on Information Theory, IT-24(4), 422–432
(1978)
16. Solomonoﬀ, R.J.: The time scale of artiﬁcial intelligence; reﬂections on social ef-
fects. Human Systems Management 5, 149–153 (1985)
17. Solomonoﬀ, R.J.: The application of algorithmic probability to problems in arti-
ﬁcial intelligence. In: Kanal, L.N., Lemmer, J.F. (eds.) Uncertainty in Artiﬁcial
Intelligence, pp. 473–491. Elsevier Science Publishers, B.V., B.V (1986); Kochen,
M., Hastings, H.M. (eds.) Advances in Cognitive Science. AAAS Selected Symposia
Series, pp. 210–227. AAAS, Washington, D.C (1988)
18. Solomonoﬀ, R.J.: The discovery of algorithmic probability. Journal of Computer
and System Sciences 55(1), 73–88 (1997)
19. Solomonoﬀ, R.J.: Three kinds of probabilistic induction: Universal distributions
and convergence theorems. The Computer Journal 51(5), 566–570 (2008); Christo-
pher Stewart Wallace (1933-2004) Memorial Special issue
20. Turing, A.M.: On computable numbers, with an application to the entshei-
dungsproblem. Proc. London Math. Soc. 42, 230–265 (1937)
21. Vit´anyi, P.: Obituary: Ray Solomonoﬀ, founding father of algorithmic information
theory. CWI, Amsterdam (2009)
22. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. The Com-
puter Journal 11, 185–194 (1968)
23. Wallace, C.S., Dowe, D.L.: Minimum message length and Kolmogorov complexity.
Computer Journal 42(4), 270–283 (1999); Special Issue on Kolmogorov Complexity

Universal Heuristics: How Do Humans Solve
“Unsolvable” Problems?
Leonid A. Levin⋆
Computer Science Department, Boston University, 111 Cummington St.,
Boston, MA 02215, USA
Abstract. Lots of crucial problems defeat current computer arts but
yield to our brains.
A great many of them can be put in the form of inverting easily
computable functions. Still other problems, such as extrapolation, are
related to this form. We have no idea which diﬃculties are intrinsic to
these problems and which just reﬂect our ignorance. We will remain
puzzled pending major foundational advances such as, e.g., on P=?NP.
And yet, traveling salesmen do get to their destinations, mathemati-
cians do ﬁnd proofs of their theorems, and physicists do ﬁnd patterns in
transformations of their elementary particles! How is this done, and how
could computers emulate their success?
Brains of insects solve problems of such complexity and with such eﬃciency,
as we cannot dream of. Yet, few of us would be ﬂattered with a comparison
to the brain of an insect :-). What advantage do we, humans, have? One is the
ability to solve new problems, those on which evolution did not train generations
of our ancestors. We must have some pretty universal methods, not restricted
to the speciﬁcs of focused problems. Of course, it is hard to tell how, say, the
mathematicians search for their proofs. Yet, the diversity and dynamism of math
achievements suggest that some pretty universal methods must be at work.
In fact, whatever the diﬃculty of inverting functions x=f(y) is, we know
a “theoretically” optimal algorithm for all such problems, one that cannot be
sped-up1 by more than a constant factor, even on a subset of instances x. It
searches for solutions y, but in order of increasing complexity Kt, not increasing
length: short solutions may be much harder to ﬁnd than long ones. Kt(y|x) can
be deﬁned as the minimal sum of (1) the bit-length of a preﬁxless program p
transforming x into y and (2) the log of the running time of p.2
Extrapolations could be done by double-use of this concept. The likelihood
of a given extrapolation consistent with known data decreases exponentially
⋆Supported by NSF grant CCF-1049505.
1 The speed is deﬁned to include the time for running f on the solution y to check it.
2 Realistically, p runs on data which specify the instance, but also encompass other
available relevant information, possibly including access to a huge database, such as
a library, or even the Internet.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 53–54, 2013.
c⃝Springer-Verlag Berlin Heidelberg 2013

54
L.A. Levin
with the length of its shortest description. This principle, known as Occam’s
Razor, was clariﬁed in papers by Ray Solomonoﬀand his followers (see also
http://arxiv.org/abs/1107.1458v5 and its references).
Decoding short descriptions should not take more time than the complexity of
the process that generated the data. The major hurdle in implementing Occam’s
Razor is ﬁnding short descriptions: it may be exponentially hard. Yet, this is
an inversion problem, and the above optimal search applies. Such approaches
contrast with the methods employed currently by CS - universal algorithms are
used heavily, but mostly for negative results.
The point of this note is to emphasize the following problem:
The above methods are optimal only up to constant factors. Nothing is known
about these factors, and simplistic attempts make them completely unreason-
able. Current theory cannot even answer straight questions, such as, e.g., is it
true that some such optimal algorithm cannot be sped-up 10-fold on inﬁnitely
many instances? Yet humans do seem to use such generic methods successfully,
raising hopes for a reasonable approach to these factors.

Partial Match Distance
In Memoriam Ray Solomonoﬀ1926-2009
Ming Li
School of Computer Science, University of Waterloo,
Waterloo, Ont. N2L 3G1, Canada
mli@uwaterloo.ca
http://www.cs.uwaterloo.ca/~mli
Abstract. In this expository article, we discuss a complementary no-
tion of information distance for partial matches. Information Distance
Dmax(x, y) = max{K(x|y), K(y|x)} measures the distance between two
objects by the minimum number of bits that are needed to convert
them to each other. However, in many applications, often some ob-
jects contain too much irrelevant information which overwhelms the
relevant information we are interested in. Information distance based
on partial information also should not satisfy the triangle inequality.
To deal with such problems, we have introduced an information dis-
tance for partial matching. It turns out the new notion is precisely
Dmin(x, y) = min{K(x|y), K(y|x)}, complementary to the Dmax dis-
tance. I will give some recent applications of the Dmin distance.
1
Introduction
We are here to celebrate the 85th birthday of our dear friend Ray Solomonoﬀ,
poshumously. The photo in Figure 1 was taken over 10 years ago in front of Ray
and Grace’s home, in Summerville, Massachusetts. Two people in the picture
have already left us: Grace’s husband Ray and my wife Jessie. During that visit,
my wife was very impressed with Ray’s adventurous spirit of experimenting
things. This photo reminds me of the quick process of randomization of a human
life and thus we should only do research that matters. Ray did just that.
In the 1960s, Solomonoﬀ[31, 32], Kolmogorov [14], and Chaitin [4] indepen-
dently introduced what we call today Kolmogorov complexity. Ray’s work was
several years before the other two inventors. The fact that the ﬁeld was named
after Kolmogorov is perhaps unfair to Ray, but he has never really complained
to us [21]. A relevant theory of message length was introduced by Boulton and
Wallace in 1968; see [37]. If Kolmogorov complexity may be seen as about infor-
mation in one string, then in the 1990s, we [2] have deﬁned Information Distance
to measure the amount of information needed to convert between two strings.
The Information Distance between strings x and y is
Dmax = max{K(x|y), K(y|x)},
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 55–64, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

56
M. Li
Fig. 1. Right: Grace, Ray. Left: Jessie and Ming
where K(x|y) is the Kolmogorov complexity of x condition on y. This work
has motivated active theoretical investigations in [6, 22, 23, 30, 35, 36].
Started in [3, 18], this notion has also been applied to many applications in
[1, 5, 7–10, 12, 13, 15–17, 19, 24–29, 33]. A more complete list of practical appli-
cations and theoretical investigations of Information Distance can be found in
the 3rd edition our our book [21].
Despite of its wonderful theoretical properties such as universality and the
abovementioned applications, Information Distance cannot deal with applica-
tions with overwhelming irrelevant information. This article intends to discuss
an extension of the Dmax distance to solve the partial matching problem and
describe some practical applications, as reported in [38–40]. This is an informal
expository article, instead of an original research publication. Especially, the
work of [38] is underway and it will be reported in full elsewhere.
2
Partial Matching
The theoretical universality of Dmax did not imply that it can be universally
applied in practical applications. The reason mainly comes from partial matching
where the triangle inequality does not hold. Fagin and Stockmeyer gave one such
example [11]. As Veltkamp puts it [34]: under partial matching, the distance
between a man and a horse is larger than the sum of the distances between a
man and a centaur, Figure 2, and between a centaur and a horse.

Partial Match Distance
57
Fig. 2. A centaur
Perhaps this is because we pay more attention to things that are similar, as
they cannot happen by coincidence, and we ignore the diﬀerences.
When you met a stranger, and discovered that you were from the same high
school, didn’t you suddenly feel you were much closer to the stranger? When
you went to the ﬁrst date, didn’t you try to come up with some topic that
would bring the two of you closer? In the academic world, this phenomenon is
reﬂected by the “Erd¨os number”. We all feel closely related via a third person
Paul Erd¨os. All these are cases of partial matching in our daily lives. Next we
give three examples of partial matching in our research.
When we studied whole genome phylogeny in the early 1990’s, we thought
about using Dmax to measure distances between two genomes. However, sister
species like E. coli and H. inﬂuenzae have very diﬀerent genome lengths. E. coli’s
genome contains about 4.8 million basepairs while H. inﬂuenzae’s genome 1.8
million basedpairs. Thus, the Information Distance Dmax between H. inﬂuenzae
genome any genome of similar length would be closer than that of H. inﬂuenzae
and E. coli. In genome evolution, deletion is easy, especially when a species is
under environmental pressure. Information Distance Dmax does not model this
situation well. We have tried to use normalized information distance to alleviate
this problem in [18] and later in [19]. Especially, we have deﬁned in [19] the
normalized information distance
dmax(x, y) = max{K(x|y), K(y|x)}
max{K(x), K(y)}
.
Later when we design our question answering (QA) system QUANTA, we met
three problems caused by partial matching [39]. The ﬁrst problem is how to re-
move the impact of irrelevant information. Consider the question: “Which city is
Lake Washington by?” There are several cities around Lake Washington: Seattle,
Kirkland, and Bellevue, which are all good answers. The most popular answer
Seattle contains overwhelmingly irrelevant information, not related to the lake.
The dmax measure chooses a city with higher complexity (lower probability) such
as Bellevue. Thus the most obvious answer Seattle is excluded. The second prob-
lem is: should an “information distance” really satisfy the triangle inequality?
Consider a QA problem: The concept of “Marilyn Monroe” is pretty far from
the concept “president”. However, Marilyn Monroe is very close to “JFK” and
“JFK” is very close to the concept of “president”. An information distance must

58
M. Li
reﬂect what “we think” to be similar. What “we think” to be similar apparently
does not really satisfy triangle inequality. The third problem is the neighborhood
density issue. Some objects are popular, such as Seattle mentioned above, and
they are close to many other concepts. To model this phenomenon properly is
our third problem. We need to relax the neighborhood constraints of Eq. 1 below

y
2−D(x,y) ≤1.
(1)
to allow some selected (very few) elements to have much denser neighborhoods.
In fact, for the Dsum(x, y) = K(x|y) + K(y|x) distance deﬁned in [2], we have
already observed similar phenomenon many years ago, and proved a theorem
about “tough guys having fewer neighbors”, [21], Theorem 8.3.8. Note that this
third problem is closely related to the ﬁrst problem: only when we allow a few
popular objects to have very dense neighborhoods, it is then possible that they
are selected more often.
Recently we have been working on a QA system with voice input. Voice recog-
nition technologies are not yet ready for practical usage due to noisy environ-
ment and speaker diversity. We solve this question by collecting a set Q of over
30 million human asked questions from the internet. When the voice recognition
system generates several candidates qi, i = 1, 2, 3, from a human asked question,
we wish to compute a question q such that q is “supported” by Q and q is close
to qi, i = 1, 2, 3. The information distance between q and Q cannot be mea-
sured by Dmax, as obviously Q contains an overwhelming amount of irrelevant
information.
3
The Dmin Distance
Recall the original motivation of information distance, in Eq (2),
Dmax(x, y) =deﬁnition E(x, y) = min{|p| : U(x, p) = y, U(y, p) = x}
(2)
we asked for the smallest number of bits that must be used to convert between
x and y. While keeping this original motivation in mind, we notice some infor-
mation in x or y are not relevant to this conversion, they can be kept aside in
this conversion process. We thus deﬁne [20]: with respect to a universal Turing
machine U, the cost of conversion between x and y is:
Emin(x, y) = min{|p| : U(x, p, r) = y, U(y, p, q) = x,
|p| + |q| + |r| ≤E(x, y)},
(3)
To interpret, the above deﬁnition separates r as the information for x and q as
the information for y. Deﬁne Dmin(x, y) = Emin(x, y). We have only informally
proved following theorem in the past [20]. Here we give the complete proof.
Theorem 1. Modulo an additive O(log(|x| + |y|)) factor,
Dmin(x, y) = min{K(x|y), K(y|x)}.

Partial Match Distance
59
Proof. Without loss of generality, assume that K(y|x) ≥K(x|y). In the origi-
nal proof of E(x, y) = max{K(x|y), K(y|x)}, in [2, 21], it is known that there
exists a program p of length K(x|y) and q of length K(y|x) −K(x|y),
such that
U(xq, p) = y;
U(y, p) = xq.
Thus p is a program that, on input x and q, outputs y, and, on input y, outputs
x. Since |p| = K(x|y), we know
Dmin(x, y) ≤min{K(x|y), K(y|x)}.
We also need to show that q contains no information about x by proving K(x|q) =
K(x). By the Symmetry of Information theorem (Theorem 2.8.2 in [21], page
182), we have
K(xq) = K(x|q) + K(q) = K(q|x) + K(x).
Note that p is always the shortest length program, but q may not be. Thus if
K(x|q) < K(x), then we must have K(q|x) < K(q). Thus we can construct y
via x, p, plus K(q|x) bits. That is: K(y|x) < K(q)+|p| = K(y|x), contradiction.
Now we show
Dmin(x, y) ≥min{K(x|y), K(y|x)}.
Assume this is not true, i.e. assume that p ≤K(x|y). Then let σ = K(x|y) −|p|.
Then K(y|x) ≤|p|+|q| where q is the shortest description of information needed
to compute y from x, p, and q. However in this case r ≥σ as this is the minimum
amount of information needed to compute x from y and p. Thus
|p| + |q| + r|
≥K(y|x) + |r|
≥K(y|x) + σ
> E(x, y) = max{K(x|y), K(y|x)},
violating the condition in
3. Thus we have shown Dmin(x, y) ≥min{K(x|y),
K(y|x)}. This ﬁnishes the proof of the theorem. □
Observe the following interesting phenomena:
– The extra information q in the proof of Theorem 1 contains no information
about x, it is the irrelevant information in y, in a minimal sense.
– On the other hand, r = ϵ, for extra information of x. Essentially, this because
we need K(x|y) + |q| bits to specify y, and need no extra information to
specify x.
– While Dmin(x, y) is symmetric and positive, it does not satisfy the triangle
inequality. To see this, let x and y be independent long Kolmogorov ran-
dom strings, and z = ϵ the empty string. Then Dmin(x, y) > Dmin(x, z) +
Dmin(z, y) = O(1).
– Dmin(x, y) satisﬁes Eq. 1 only for random x’s. This is perhaps not a surprise
as Dsum(x, y) = Dmin(x, y) + Dmax(x, y). In the new metric Dmin, “good
guys” (Kolmogorov simple objects) have even more neighbors than Dsum.

60
M. Li
We can also normalize Dmin similar to that of Dmax:
dmin(x, y) = min{K(x|y), K(y|x)}
min{K(x), K(y)}
.
(4)
dmin(x, y) is symmetric and positive, but it does not satisfy triangle inequality.
To see this, let K(X) ≥|X| = n for a large n. Partition X = x1x2x3, with
|xi| = n/3. Let z = x1, x = x1x2, y = x1x3. Then
dmin(x, y) ≈1/2
which is much bigger than
dmin(x, z) + dmin(z, y) = O(1/n).
While it is clear that Dmin(x, y) ≤Dmax(x, y), it is not clear if such a re-
lationship would hold for dmin vs dmax. The following theorem shows that this
holds after all.
Theorem 2. For all x, y, dmin(x, y) ≤dmax(x, y).
Proof. Given a pair of x and y, without loss of generality, assume that K(x|y) ≤
K(y|x). By the Symmetry of Information Theorem (Theorem 2.8.2 in [21], page
182), we know that up to an additive logarithmic factor,
K(xy) = K(x|y) + K(y) = K(y|x) + K(x).
Thus, from K(x|y) ≤K(y|x), we derive K(x) ≤K(y). Hence dmin(x, y) =
K(x|y)/K(x), and dmax(x, y) = K(y|x)/K(y). Since K(x|y) ≤K(x), we have
dmin(x, y) = K(x|y)
K(x)
≤K(x|y) + Δ
K(x) + Δ ,
for any Δ ≥0. Setting Δ = K(y)−K(x), and using the Symmetry of Information
Theorem again on the top, the righthand side becomes dmax = K(y|x)/K(y),
and the theorem hence follows. □
4
Question Answering
We have been working on a Question Answering (QA) system. In [39, 40], we have
used dmin(x,y) to compute the distance between a question x and an answer y.
The dmin prefers more popular answers such at Seattle over less popular answers
such as Bellevue. The Kolmogorov complexity terms in the dmin expression was
approximated using the Shannon-Fano encoding of the internet frequencies, [8].
This system will be combined with the vioce input system to become a QA-
via-voice system. Part of this project was supported by the Canadian IDRC for
building a cross language QA system for the users in the third world countries.
The ability of using the voice input allows the illiterate or visually impaired users
in the third world countries to also search the web via our cross language QA
engine. In the developed countries, such an application will allow drivers to talk
to their GPS’s, children to talk to their Talking Toms and R2-D2s, and mothers,
or fathers, to tell their smart phones to tell a bed time story for their kids, as I
am already doing.

Partial Match Distance
61
5
Voice Recognition Correction
We now discuss an on-going project that uses our new information distance the-
ory. Mobile devices have given QA systems a very promising application plat-
form. A QA system can be of practical usage on a smart phone if the questions
can be input via voice. However the current voice recognition technologies, the
best of which by Google, Dragon, Microsoft, are some steps away from being
practical. This is due to noisy environment and speaker diversity. We [38] have
decided to overcome this problem by using the internet information. We have
downloaded over 30 million questions asked by the users from the internet. Let
Q be this set of 30 million questions. Let q1, q2, q3 be three candidates generated
by the speech recognition software from the user speech input. Informally, we
wish to compute a quesion q such that q is close to (in some sense) Q, and close
to q1, q2, q3, as shown below
{q1, q2, q3} ←→q ←→Q
and hopefully q is what the user had in mind. By q being “close” in some sense
to Q, we mean that there some sort of patterns one can extract from Q, for
example “who is the mayor of CITY”. We have done one experiment using
a Microsoft QA set of 300 questions, and found 99% of these questions have
reasonable corresponding “patterns” in Q. Apparently, for each q, we are only
interested in some small number of questions in Q not most other questions in Q.
This is clearly a matter of partial matching and we are interested in the shortest
way of using Q to encode q. Thus we can formalize our problem as: ﬁnd q that
minimizes
δDmin(Q, q) + Dmax({q1, q2, q3}, q)
where δ > 1 is a constant. The rest of the story is encoding which we will ignore
in this article. This was implemented on as part of a QA-by-voice system on the
Iphones.
Testing such a system is tricky, as it heavily depends on individual speakers
and questions. However in one particular test of 164 questions by a mixture of
non-native and native English speakers, Google voice recognition system recog-
nized 110 questions correctly, our system corrected 39 google results that were
previously erroneous and but wrongly changed 5 correct results to wrong ques-
tions. Thus in total, our system had 144 correct answers out of 164 total, or
88%, whereas the Google original voice recognition output had 66% correct.
Here are some examples where we have corrected Google voice recognition out-
puts. I asked “what do frogs eat?” The Google voice recognition returned: “What
is front seat”, “what is frogs eat”, and “what does the front seat”. Our server
returned “What does frogs eat”. Another example: I asked “How many Indian
groups are in New Hampshire”. Google voice recognition returned 3 candidates:
“How many indian groups are you new hampshire”, “How many indian groups
are the new hampshire”, “albany indian groups are you new hampshire”, our
server returned: “How many Indian groups are in New Hampshire”. Another
example: I asked “who won the world series in 1997”. Google voice recognition

62
M. Li
returned 3 candidates: “holanda series in 1997”, “hole on the world series in
1997”, “hol on the world series in 1997”. Our system returned: “who won the
world series in 1997”.
Sometimes even if the voice recognition software is perfect, the speakers might
not speak correctly. Our system corrects those errors too. The following is one
such example (from my own speech; I forgot to say “get”). The Google voice
reconition recognized my speech perfectly and output three candidates with the
ﬁrst one having the highest probability: Why do people addicted to the games
(my original speech); Why do people addicted to the names; Why do people
addicted to the canes; Our system returned: Why do people get addicted to the
games.
Acknowledgements. This work was supported in part by NSERC Grant
OGP0046506, Canada Research Chair program and a CFI infrastructure grant,
an NSERC Collaborative Grant, Premier’s Discovery Award, Killam Prize, and
an IDRC grant.
References
1. An´e, C., Sanderson, M.J.: Missing the Forest for the Trees: Phylogenetic Compres-
sion and Its Implications for Inferring Complex Evolutionary Histories. Systematic
Biology 54(1), 146–157 (2005)
2. Bennett, C.H., Gacs, P., Li, M., Vitanyi, P., Zurek, W.: Information Distance.
IEEE Trans. Inform. Theory 44(4), 1407–1423 (1998) (STOC 1993)
3. Bennett, C.H., Li, M., Ma, B.: Chain letters and evolutionary histories. Scientiﬁc
American 288(6), 76–81 (2003) (feature article)
4. Chaitin, G.J.: On the Simplicity and Speed of Programs for Computing Inﬁnite
Sets of Natural Numbers. Journal of the ACM 16(3), 407
5. Chen, X., Francia, B., Li, M., Mckinnon, B., Seker, A.: Shared information and
program plagiarism detection. IEEE Trans. Information Theory 50(7), 1545–1550
(2004)
6. Chernov, A.V., Muchnik, A.A., Romashchenko, A.E., Shen, A.K., Vereshchagin,
N.K.: Upper semi-lattice of binary strings with the relation “x is simple conditional
to y”. Theoret. Comput. Sci. 271, 69–95 (2002)
7. Cilibrasi, R., Vit´anyi, P.M.B., de Wolf Algorithmic, R.: clustring of music based
on string compression. Comput. Music J. 28(4), 49–67 (2004)
8. Cilibrasi, R., Vit´anyi, P.M.B.: The Google similarity distance. IEEE Trans. Knowl-
edge and Data Engineering 19(3), 370–383 (2007)
9. Cilibrasi, R., Vit´anyi, P.M.B.: Clustering by compression. IEEE Trans. Inform.
Theory 51(4), 1523–1545 (2005)
10. Cuturi, M., Vert, J.P.: The context-tree kernel for strings. Neural Networks 18(4),
1111–1123 (2005)
11. Fagin, R., Stockmeyer, L.: Relaxing the triangle inequality in pattern matching.
Int’l J. Comput. Vision 28(3), 219–231 (1998)
12. Keogh, E., Lonardi, S., Ratanamahatana, C.A.: Towards parameter-free data min-
ing. In: KDD 2004, pp. 206–215 (2004)
13. Kirk, S.R., Jenkins, S.: Information theory-baed software metrics and obfuscation.
J. Systems and Software 72, 179–186 (2004)

Partial Match Distance
63
14. Kolmogorov, A.N.: Three Approaches to the Quantitative Deﬁnition of Informa-
tion. Problems Inform. Transmission 1(1), 1–7 (1965)
15. Kraskov, A., St¨ogbauer, H., Andrzejak, R.G., Grassberger, P.: Hierarchical clus-
tering using mutual information. Europhys. Lett. 70(2), 278–284 (2005)
16. Kocsor, A., Kertesz-Farkas, A., Kajan, L., Pongor, S.: Application of compression-
based distance measures to protein sequence classiﬁcation: a methodology study.
Bioinformatics 22(4), 407–412 (2006)
17. Krasnogor, N., Pelta, D.A.: Measuring the similarity of protein structures by means
of the universal similarity metric. Bioinformatics 20(7), 1015–1021 (2004)
18. Li, M., Badger, J., Chen, X., Kwong, S., Kearney, P., Zhang, H.: An information-
based sequence distance and its application to whole mitochondrial genome phy-
logeny. Bioinformatics 17(2), 149–154 (2001)
19. Li, M., Chen, X., Li, X., Ma, B., Vitanyi, P.M.B.: The similarity metric. IEEE
Trans. Information Theory 50(12), 3250–3264 (2004)
20. Li, M.: Information distance and its applications. Int’l J. Found. Comput.
Sci. 18(4), 669–681 (2007)
21. Li, M., Vitanyi, P.: An introduction to Kolmogorov complexity and its applications,
3rd edn. Springer (2008)
22. Muchnik, A.A.: Conditional comlexity and codes. Theoretical Computer Sci-
ence 271(1), 97–109 (2002)
23. Muchnik, A.A., Vereshchagin, N.K.: Logical operations and Kolmogorov complex-
ity II. In: Proc. 16th Conf. Comput. Complexity, pp. 256–265 (2001)
24. Nykter, M., Price, N.D., Larjo, A., Aho, T., Kauﬀman, S.A., Yli-Harja, O., Shmule-
vich, I.: Critical networks exhibit maximal information diversity in structure-
dynamics relationships. Phy. Rev. Lett. 100, 058702(4) (2008)
25. Nykter, M., Price, N.D., Aldana, M., Ramsey, S.A., Kauﬀman, S.A., Hood, L.E.,
Yli-Harja, O., Shmulevich, I.: Gene expression dynamics in the macrophage exhibit
criticality. Proc. Nat. Acad. Sci. USA 105(6), 1897–1900 (2008)
26. Otu, H.H., Sayood, K.: Bioinformatics 19(6), 2122–2130 (2003); A new sequence
distance measure for phylogenetic tree construction
27. Pao, H.K., Case, J.: Computing entropy for ortholog detection. In: Int’l Conf.
Comput. Intell., Istanbul, Turkey, December 17-19 (2004)
28. Parry, D.: Use of Kolmogorov distance identiﬁcation of web page authorship,
topic and domain. In: Workshop on Open Source Web Inf. Retrieval (2005),
http://www.emse.fr/OSWIR05
29. Costa Santos, C., Bernardes, J., Vit´anyi, P.M.B., Antunes, L.: Clustering fetal
heart rate tracings by compression. In: Proc. 19th IEEE Intn’l Symp. Computer-
Based Medical Systems, Salt Lake City, Utah, June 22-23 (2006)
30. Shen, A.K., Vereshchagin, N.K.: Logical operations and Kolmogorov complexity.
Theoret. Comput. Sci. 271, 125–129 (2002)
31. Solomonoﬀ, R.: A Formal Theory of Inductive Inference, Part I. d Information and
Control 7(1), 1–22 (1964)
32. Solomonoﬀ, R.: A Formal Theory of Inductive Inference, Part II. Information and
Control 7(2), 224–254 (1964)
33. Varre, J.S., Delahaye, J.P., Rivals, E.: Transformation distances: a family of dissim-
ilarity measures based on movements of segments. Bioinformatics 15(3), 194–202
(1999)
34. Veltkamp, R.C.: Shape Matching: Similarity Measures and Algorithms, invited
talk. In: Proc. Int’l Conf. Shape Modeling Applications 2001, Italy, pp. 188–197
(2001)

64
M. Li
35. Vereshchagin, N.K., V’yugin, M.V.: Independent minimum length programs to
translate between given strings. Theoret. Comput. Sci. 271, 131–143 (2002)
36. V’yugin, M.V.: Information distance and conditional complexities. Theoret. Com-
put. Sci. 271, 145–150 (2002)
37. Wallace, C.S., Dowe, D.L.: Minimum Message Length and Kolmogorov Complexity.
Computer Journal 42(4) (1999)
38. Yang, T., Wang, D., Zhu, X., Li, M.: Information distance between what I said
and what it heard. Manuscript in preparation (August. 2011)
39. Zhang, X., Hao, Y., Zhu, X., Li, M.: Information distance from a question to an
answer. In: Proc. 13th ACM SIGKDD, August 12-15, pp. 874–883 (2007)
40. Zhang, X., Hao, Y., Zhu, X., Li, M.: New information measure and its application
in question answering system. J. Comput. Sci. Tech. 23(4), 557–572 (2008)

Falsiﬁcation and Future Performance
David Balduzzi
MPI for Intelligent Systems, Tuebingen, Germany
david.balduzzi@tuebingen.mpg.de
Abstract. We information-theoretically reformulate two measures of
capacity from statistical learning theory: empirical VC-entropy and em-
pirical Rademacher complexity. We show these capacity measures count
the number of hypotheses about a dataset that a learning algorithm fal-
siﬁes when it ﬁnds the classiﬁer in its repertoire minimizing empirical
risk. It then follows from that the future performance of predictors on
unseen data is controlled in part by how many hypotheses the learner
falsiﬁes. As a corollary we show that empirical VC-entropy quantiﬁes the
message length of the true hypothesis in the optimal code of a particular
probability distribution, the so-called actual repertoire.
1
Introduction
This note relates the number of hypotheses falsiﬁed by a learning algorithm to the
expected future performance of the predictor it outputs. It does so by reformu-
lating two basic results from statistical learning theory information-theoretically.
Suppose we wish to predict an unknown physical process σ∗: X →Y occur-
ring in nature after observing its outputs (y1, . . . , yl) on sample D = (x1, . . . , xl)
of its inputs, where inputs arise according to unknown distribution P. One
method is to take a repertoire F of functions from X →Y and choose the
predictor ˆf ∈F that best approximates σ∗on the observed data. How conﬁdent
can we be in ˆf’s future performance on unseen data?
Statistical learning theory provides bounds on ˆf’s expected future perfor-
mance by quantifying a tradeoﬀimplicit in the choice of repertoire F. At ﬁrst
glance, the bigger the repertoire the better since the best approximation to σ∗in
F can only improve as more more functions are added to F. However, increas-
ing F, and improving the approximation on observed data, can reduce future
performance due to overﬁtting. As a result, the bounds depend on both the ac-
curacy with which ˆf approximates σ∗on the observed data and the capacity of
repertoire F, see Theorems 9 and 10.
We wish to connect statistical learning theory with Popper’s ideas about fal-
siﬁcation. Popper argued that no amount of positive evidence conﬁrms a the-
ory [11]. Rather, theories should be judged on the basis of how many hypotheses
they falsify. A theory is falsiﬁable if there are possible hypotheses about the
world (i.e. data) that are not consistent with the theory. A bold theory falsi-
ﬁes (disagrees with) many potential hypotheses about observed data. Testing a
bold theory, by checking that the hypotheses it disagrees with are in fact false,
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 65–78, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

66
D. Balduzzi
provides corroborating evidence. If a theory has been thoroughly tested then
(perhaps) we can have conﬁdence in its predictions. Popper’s criticism of posi-
tive conﬁrmation was devastating. However, and hence the “perhaps”, he failed
to provide a rationale for trusting the predictions of severely tested theories.
To understand how falsifying hypotheses aﬀects future performance we refor-
mulate learning as a kind of measurement. Before doing so, we need to describe
precisely what we mean by measurement.
Given physical system X with state space S(X), a classical measurement is
a function f : S(X) →R. For example a thermometer f maps conﬁgurations
(positions and momenta) of particles in the atmosphere to real numbers. When
the thermometer outputs 15◦C it generates information by specifying that at-
mospheric particles were in a conﬁguration in f −1(15) ⊂S(X). The information
generated by the thermometer is a brute physical fact depending on how the
thermometer is built and its output. We quantify the information, see §2, by
comparing the size of the total conﬁguration space S(X) with the size of the
pre-image f −1(15). The smaller the pre-image, the more informative the mea-
surement, see §2 for details.
More generally, any (classical) physical process f : X →Y can be thought of
as performing measurements by taking inputs in X to outputs in Y. Section §4
introduces an important example, the min-risk RF,D : Σ(X, Y) →R, which
outputs the minimum value of the empirical risk over repertoire F on a hy-
pothesis space Σ(X, Y). Finding the min-risk is a necessary step in ﬁnding the
best approximation ˆf to σ∗in F. Since computing the min-risk requires actually
implementing it as a physical process somehow or other, the measurements it
performs and the eﬀective information it generates are brute physical facts, no
diﬀerent in kind than the information generated by a thermometer.
It turns out that the min-risk categorizes hypotheses in Σ according to how
well they are approximated by predictors in repertoire F. Proposition 12 shows
that the eﬀective information generated by the min-risk is (essentially) the em-
pirical VC-entropy. Moreover, the eﬀective information generated by the min-risk
“counts” the number of hypotheses about D that F falsiﬁes, see Eq. (13). As a
consequence, Corollary 13, we obtain that the future performance of predictor ˆf
is controlled by (i) how well ˆf ﬁts the observed data; (ii) how many hypotheses
about the data the min-risk rules out and (iii) a conﬁdence term.
It follows that, assuming the assumptions of the theorems below hold, bounds
on future performance are brute physical facts resulting from the act of mini-
mizing empirical risk, and so falsifying potential hypotheses, on observed data.
A consequence of our results, Corollary 15, is that empirical VC-entropy is es-
sentially the minimal length of the true hypothesis under the optimal code for the
actual repertoire (a distribution depending on the min-risk). This suggests there
may be interesting connections between VC-theory and the minimum message
length (MML) approach to induction proposed by Wallace and Boulton [15,16].
Finally, section §4.2 reformulates empirical Rademacher complexity via fal-
siﬁcation. Here we build on Solomonoﬀ’s probability distribution introduced
in [12]. In short, we take Solomonoﬀ’s deﬁnition and substitute the min-risk in

Falsiﬁcation and Future Performance
67
place of the universal Turing machine, thereby obtaining what we refer to as the
Rademacher distribution – a non-universal analog of Solomonoﬀ’s distribution.
Rademacher complexity is then computed using the expectation of the min-risk
over the Rademacher distribution, see Proposition 17.
The min-risk thus provides a bridge that not only connects VC-theory to a
computable analog of Solomonoﬀ’s seminal distribution, but also sheds light on
how falsiﬁcation provides guarantees on future performance.
Related Work. The connection between Popper’s ideas on falsiﬁability and
statistical learning theory was pointed out in [5,7,14]. However, these works focus
on VC-dimension, which does not relate to falsiﬁcation as directly as VC-entropy
and Rademacher complexity which we consider here. Further, VC-entropy is a
more fundamental concept in statistical learning theory than VC-dimension since
VC-dimension is deﬁned in terms of the limit behavior of the growth function,
which is an upper bound on VC-entropy [14]. For more details on the link between
MML and algorithmic probability, see [17].
2
Measurement
We consider a toy universe containing probabilistic mechanisms (input/output
devices) of the following form
Deﬁnition 1. Given ﬁnite sets X and Y, a mechanism is a Markov matrix m
deﬁned by conditional probability distribution pm(y|x).
Mechanisms generate information about their inputs by assigning them to
outputs [1,2].
Deﬁnition 2. The actual repertoire (or measurement) speciﬁed by m
outputting y is the probability distribution
pm(x|y) := pm(y|x)
p(y)
· punif(x),
where punif(x) =
1
|X | is the uniform distribution. The eﬀective information
generated by the measurement is
ei(m, y) := H

pm(X|y)
punif(X)

,
where H[p∥q] = 
i pi log2
pi
qi is Kullback-Leibler divergence.
The Kullback-Leibler divergence H[p∥q] can be interpreted informally as the
number of Y/N questions needed to get from distribution q to distribution p.
However, as pointed out in [6], Kullback-Leibler divergence is invariant with
respect to the “framing of the problem” – the ordering and structure of the
questions – suggesting it is a suitable measure of information-theoretic “eﬀort”.

68
D. Balduzzi
The deﬁnition of measurement is motivated by the special case where pm
assigns probabilities that are either 0 or 1; in other words, when it corresponds
to a set-valued function f : X →Y. The measurement performed by f is
pf(x|y) =

1
|f −1(y)| if f(x) = y
0
else,
where | · | denotes cardinality. The support of pf(X|y) is the preimage f −1(y) ⊂
X. All elements of the support are assigned equal probability – they are treated
as an undiﬀerentiated list. The measurement pm(X|y) therefore generalizes the
notion of preimage to the probabilistic setting.
The eﬀective information generated by f outputting y is ei(f, y) = log2
|X |
|f −1(y)|:
ei(f, y) =
log2 |X|
−
log2 |f −1(y)|
=

no. potential inputs
	
−

no. inputs in pre-image
	
=

no. inputs ruled out
	
,
(1)
where inputs are counted in bits (after logarithming). Eﬀective information is
maximal (log2 |X| bits) when a single input leads to y, and is minimal (0 bits)
when all inputs lead to y. In the ﬁrst case, observing f output y tells us exactly
what the input was, and in the latter case, it tells us nothing at all.
2.1
Semantics
Next we consider two approaches to characterizing the meaning of measurements.
The ﬁrst relates to possible world semantics [9]. Here, the meaning of a sentence is
given by the set of possible worlds in which it is true. Meaning is thus determined
by considering all counterfactuals. For example, the meaning of “That car is 10
years old” is the set of possible worlds where the speaker is pointing to a car
manufactured 10 years previously. Since the set of contains cars of many diﬀerent
colors, we see that color is irrelevant to the meaning of the sentence.
More precisely, the meaning of sentence S is a map from possible worlds W
to truth values vS : W →{0, 1}. Equivalently, the meaning of a sentence is
W
⊃
v−1
S (1)

possible worlds
	
⊃

worlds where S is true
	
.
(2)
Inspired by possible world semantics, we propose
Deﬁnition 3. The meaning of output y by mechanism m is
punif(X)
→
pm(X|y)

possible inputs
	
→

inputs that cause y
	
.
(3)
For a deterministic function this reduces to X ⊃f −1(y).

Falsiﬁcation and Future Performance
69
Fig. 1. The eﬀective information generated by measurements.
(A) A de-
terministic device can receive 144 inputs and produce 3 outputs. (B): Each input is
implicitly assigned to a category (shaded areas). The information generated by the
dark gray output is log2 144 −log2 9 = 4 bits.
Grounding meanings in mechanisms yields four advantages over the possible
worlds approach. First, it replaces the diﬃcult to deﬁne notion of a possible
world with the concrete set of inputs the mechanism is physically capable of
receiving. Second, in possible world semantics the work of determining whether
or not a sentence is true is performed somewhat mysteriously oﬀstage, whereas
the meaning of a measurement is determined via Bayes’ rule. Third, the approach
generalizes to probabilistic mechanisms. Finally, we can compute the eﬀective
information generated by a measurement, whereas there is no way to quantify
the information content of a sentence in possible world semantics.
2.2
Risk
The second, pragmatic notion of meaning characterizes usefulness. We consider
a special case, well studied in statistical learning theory, where usefulness relates
to predictions [14].
Let Σ(X, Y) = {σ : X →Y} be the set of all functions (deterministic mech-
anisms) mapping X to Y = {−1, +1}. We will often write Σ for short. Suppose
there is a random variable X taking values in X with unknown distribution

70
D. Balduzzi
P and an unknown mechanism σ∗∈Σ, the supervisor, who assigns labels to
elements of X.
Deﬁnition 4. The risk quantiﬁes how well mechanism f approximates an un-
known or partially known mechanism σ∗:
R(f) =

x∈X
I

f(x) ̸= σ∗(x)

· p(x).
(4)
It is the probability that f and σ∗disagree on elements of X.
Unfortunately, the risk cannot be computed since P and σ∗are unknown.
Deﬁnition 5. Given a ﬁnite sample D = (x1, . . . , xl) ∈X l with labels L =
σ∗D = (y1, . . . , yl) ∈Yl, the empirical risk of f : X →Y
R(f, D, L) = 1
l
l

i=1
I

f(xi) ̸= yi

(5)
is the fraction of the data D on which f and σ∗disagree.
The empirical risk provides a computable approximation to the (true) risk.
Remark 6. Note that in this paper, sets X and Y are both ﬁnite. Similarly, the
training data D ∈X l and labels L ∈Yl also live in ﬁnite sets.
3
Statistical Learning Theory
Suppose we wish to predict the unknown supervisor σ∗based on its behavior on
labeled data (D, L). A simple way to ﬁnd a mechanism in repertoire F ⊂Σ(X, Y)
that approximates σ∗well is to minimize the empirical risk.
Deﬁnition 7. Given repertoire F ⊂Σ and unlabeled data D ∈X l, deﬁne
learning algorithm
AF,D : Σ →F : σ →arg min
f∈F R(f, D, σD)
(6)
which ﬁnds the mechanism in F that minimizes empirical risk.
Learning algorithm AF,D ﬁnds the mechanism in F that appears, based on
the empirical risk, to best approximate σ∗. Empirical risk stays constant or
decreases as F is enlarged, suggesting that the larger the repertoire the better.
This is not true in general since minimizing risk – and not empirical risk –
is the goal. There is a tradeoﬀ: increasing the size of F leads to overﬁtting the
data which can increase risk even as empirical risk is reduced.
The tendency of a repertoire to overﬁt data depends on its size or capacity.
We recall two measures of capacity that are used to bound risk: empirical VC-
entropy [13] and empirical Rademacher complexity [8].

Falsiﬁcation and Future Performance
71
Deﬁnition 8. Given unlabeled data D ∈X l and repertoire F ⊂Σ let
qD : F →Rl : f →

f(x1), . . . , f(xl)
	
.
(7)
The empirical VC-entropy1 of F on D is V(F, D) := log2 |qD(F)|, where
|qD(F)| is the number of distinct points in the image of qD.
The empirical Rademacher complexity of F on D is
R(F, D) =
1
|Σ|

σ∈Σ

sup
f∈F
1
l
l

i=1
σ(xi) · f(xi)

.
(8)
VC-entropy “counts” how many labelings of D the classiﬁers in F ﬁt perfectly.
Rademacher complexity is a weighted count of how many labelings of D functions
in F ﬁt well.
The following theorems are shown in [3] and [4] respectively:
Theorem 9 (empirical VC-entropy bound)
With probability 1 −δ, the expected risk is bounded by
R(f) ≤R(f, D, L) + c1

V(F, D)
l
+ c2

1 −log2 δ
l
(9)
for all f ∈F, where the constants are c1 =

6
log2 e and c2 =

1
log2 e.
Theorem 10 (empirical Rademacher bound)
For all δ > 0, with probability at least 1 −δ,
R(f) ≤R(f, D, L) + R(F, D) + c3

1 −log2 δ
l
,
(10)
for all f ∈F, where c3 =

2
log2 e.
The tradeoﬀbetween empirical risk and capacity is visible in the ﬁrst two terms
on the right-hand sides of the bounds.
The left-hand sides of Eqs (9) and (10) cannot be computed since P and
σ∗are unknown. Remarkably, the right-hand sides depend only on mechanism
f chosen from repertoire F, labeled data (D, L) and desired conﬁdence δ. The
theorems assume data is drawn i.i.d. according to P and labeled according to
σ∗; it make no assumptions about the distribution P on X or supervisor σ∗,
except that they are ﬁxed.
1 VC-entropy is the expectation of empirical VC-entropy [14]. Also, note the standard
deﬁnition of VC-entropy uses loge rather than log2.

72
D. Balduzzi
4
Falsiﬁcation
This section reformulates the results from statistical learning theory to show how
the past falsiﬁcations performed by a learning algorithm control future perfor-
mance. We show that the empirical VC-entropies and Rademacher complexities
admit interpretations as “counting” (in senses made precise below) the number
of hypotheses falsiﬁed by a particular measurement performed when learning.
We start by introducing a special mechanism, the min-risk, which is used
implicitly in learning algorithm AF,D. As we will see, the structure of the mea-
surements performed by the min-risk determine the capacity of the learning
algorithm.
Deﬁnition 11. Given repertoire F ⊂Σ and unlabeled data D ∈X l, deﬁne the
min-risk as the minimum of the empirical risk on F:
RF,D : Σ →R : σ →min
f∈F R(f, D, σD).
(11)
The min-risk is a mechanism mapping supervisors σ in Σ to the empirical
risk of their best approximations AF,D(σ) in F, see Fig. 2. Note that inputs to
the min-risk are themselves mechanisms.
We suggestively interpret the setup as follows. Suppose a scientist studies a
universe where inputs in X appear according to distribution P, and are assigned
labels in Y by unknown physical process σ∗. The hypothesis space is Σ(X, Y),
the set of all possible (deterministic) physical processes that take X to Y.
The scientist’s goal is to learn to predict physical process σ∗, on the basis
of a small sample of labeled data (D, L). She has a theory, repertoire F, and a
method, AF,D, which she uses to ﬁt some particular ˆf ∈F given L.
The most important question for the scientist is: How reliable are predictions
made by ˆf on new data? We will show that ˆf’s reliability depends on the mea-
surements performed by the min-risk – i.e. on the work done by the scientist
when she applies method AF,D to ﬁnd ˆf.
4.1
Empirical VC Entropy
Empirical VC-entropy is, essentially, the eﬀective information generated by the
min-risk when it outputs a perfect ﬁt:
Proposition 12 (VC-entropy via eﬀective information)
Empirical VC entropy is
V(F, D) = l −ei (RF,D, 0) .
(12)
Proof: Let X = D ∪Dc and |X| = m. Then Σ = {σ : D →Y} × {σ : Dc →Y}.
By deﬁnition
ei (RF,D, 0) = log2 |Σ| −log2 |R−1
F,D(0)|,
with log2 |Σ| = m. It remains to show that |R−1
F,D(0)| = 2m−l · |qD(F)|. Points
in the image of qD correspond to labelings σ of the data by functions in F.

Falsiﬁcation and Future Performance
73
Fig. 2. The structure of the measurement performed by the min-risk. The
min-risk categorizes potential hypothesis in Σ according to how well they are ﬁt by
mechanisms in theory F.
Thus, |qD(F)| counts distinct labelings of D that F ﬁts perfectly. These oc-
cur with multiplicity 2m−l in the pre-image by the product decomposition of Σ
above.
■
We interpret the result as follows. Suppose the scientist applies theory F
to explain her labeled data and perfectly ﬁts function ˆf = AF,D(σ∗) with
risk ϵ = 0.
By Deﬁnition 3, the meaning of her work is Σ ⊃R−1
F,D(0): the set of mech-
anisms that her theory F ﬁts perfectly. The eﬀective information generated by
her work is
ei(RF,D, 0) =
log2 |Σ|
−
log2 |R−1
F,D(0)|
=

total no. of hypotheses
	
−

no. that theory ﬁts
	
=

no. of hypotheses falsiﬁed
	
,
(13)
where hypotheses are counted in bits (after logarithming). A theory is informa-
tive if it rules out many potential hypotheses [11].
The number of hypotheses the scientist falsiﬁes when using theory F to ﬁt ˆf
has implications for its future performance:
Corollary 13 (information-theoretic empirical VC bound)
With probability 1 −δ, the risk of predictor ˆf = AF,D(σ∗) outputted by learning
algorithm AF is bounded by
R(f) ≤R(f, D, L) + c1

1 −ei(RF,D, 0)
l
+ c2

1 −log2 δ
l
.
(14)

74
D. Balduzzi
Proof: By Theorem 9 and Proposition 12.
■
The corollary states that minimizing empirical risk embeds expectations about
the future into predictors. So long as the corollary’s assumptions hold, future
performance by ˆf is controlled by: (i) the output of the min-risk, i.e. the fraction
ϵ of the data that ˆf ﬁts; (ii) the eﬀective information generated by the min-risk,
i.e. the number (in bits) of hypotheses the learning algorithm falsiﬁes if it ﬁts
perfectly; and (iii) a conﬁdence term. The only assumption made by the corollary
is that P and σ∗are ﬁxed.
Remark 14. The theorem provides no guarantees on the future performance of
a theory that “explains everything”, i.e. F = Σ, no matter how well it ﬁts the
data. This follows since eﬀective information is zero when F = Σ, and so the
second term on the right-hand side of Eq. (14) is c1 ≈2.
Reformulating the above result in terms of code lengths suggests a connection
between VC-theory and minimum message length (MML), see [16] and §6.6 of [6].
Recall that, given probability distribution p(X), the message length of event x
in an optimal binary code is len(x) := −log2 p(x).
Corollary 15 (VC-entropy controls code length of true hypothesis)
Denote the min-risk by m = RF,D. The length of the true hypothesis ˆσ in the
optimal code for the actual repertoire speciﬁed by the min-risk, pm(Σ|ϵ = 0), is
len(ˆσ) = V(F, D) +

|X| −|D|

.
Proof: By Proposition 12 we have −log2 pm(ˆσ|ϵ = 0) = log2 |R−1
F,D(0)|.
■
The length of the message describing the true hypothesis in the actual reper-
toire’s optimal code is the empirical VC-entropy plus a term, (|X|−|D|) = (m−l),
that decreases as the amount of training data increases. The shorter the message,
the better the predictor’s expected performance (for ﬁxed empirical risk).
4.2
Empirical Rademacher Complexity
VC-entropy only considers hypotheses that theory F ﬁts perfectly. Rademacher
complexity is an alternate capacity measure that considers the distribution of risk
across the entire hypothesis space. This section explains Rademacher complexity
via an analogy with Solomonoﬀprobability [12,17].
We ﬁrst recall Solomonoﬀ’s deﬁnition. Given universal Turing machine T ,
deﬁne (unnormalized) Solomonoﬀprobability
pT (s) :=

{i|T (i)=s•}
2−len(i),
(15)
where the sum is over strings2 i that cause T to output s as a preﬁx, and len(i) is
the length of i. We adapt Eq. (15) by replacing Turing machine T with min-risk
RF,D : Σ →R.
2 A technical point is that no proper preﬁx of i should output s.

Falsiﬁcation and Future Performance
75
Deﬁnition 16. Equipping hypothesis space with the uniform distribution punif
(Σ), all hypotheses have length len(σ) = |X| = log2 |Σ| in the optimal code. Set
the Rademacher distribution for the min-risk m = RF,D as
pm(ϵ) :=

{σ|RF,D(σ)=ϵ}
2−len(σ) =
⎧
⎪
⎨
⎪
⎩
R−1
F,D(ϵ)

|Σ|
if ϵ ∈RF,D(Σ)
0
else.
(16)
The Rademacher distribution is constructed following Solomonoﬀ’s approach
after substituting the min-risk as a “special-purpose Turing machine” that only
accepts hypotheses in ﬁnite set Σ as inputs. It tracks the fraction of hypotheses
in Σ that yield risk ϵ.
The Rademacher distribution arises naturally as the denominator when using
Bayes’ rule to compute the actual repertoire pm(Σ|ϵ):
pm(σ|ϵ) = pm(ϵ|σ)
pm(ϵ) · punif(σ),
where pm(ϵ|σ) =
⎧
⎨
⎩
1 if RF,D(σ) = ϵ
0
else.
Proposition 17 (Rademacher complexity via min-risk)
R(F, D) = 1 −2 · E

ϵ
 pm(ϵ)

.
(17)
Proof: We refer to E

ϵ
 pm(ϵ)

as the expected min-risk. From Eq. (8),
R(F, D) =
1
|Σ|

σ∈Σ

sup
f∈F
1
l
l

i=1
σ(xi) · f(xi)

.
Observe that 1
l
l
i=1 σ(xi)·f(xi) = 1−2R(f, D, σ). It follows that supf∈F
1
l
l
i=1
σ(xi) · f(xi) = 1 −2RF,D(σ), which implies
R(F, D) = 1 −2

σ∈Σ
RF,D(σ)
|Σ|
= 1 −2

ϵ
ϵ ·
R−1
F,D(ϵ)

|Σ|
.
■
Rademacher complexity is low if the expected min-risk is high. The expected
min-risk admits an interesting interpretation. For any hypothesis σ ∈R−1
F,D(ϵ)
the classiﬁer ˆfσ := AF,D(σ) ∈F outputted by the learning algorithm yields
incorrect answers on fraction ϵ =
1
l
l
i=1 I

 ˆfσ(xi) ̸= σ(xi)

of the data. It
follows that

ϵ pm(ϵ) · ϵ = 
ϵ
R−1
F,D(ϵ)

|Σ|
·
1
l

l I

 ˆfσ(xi) ̸= σ(xi)

= 
ϵ

fraction of hypotheses falsiﬁed
	
·

on fraction ϵ of the data
	
.

76
D. Balduzzi
A bold theory F is one for which E[ϵ|pm(ϵ)] is high, meaning that its predictors
(the classiﬁers it tries to ﬁt to data) are suﬃciently narrow that it would falsify
most hypotheses on most of the data.
When a bold theory happens to ﬁt labeled data well, it is guaranteed to perform
well in future:
Corollary 18 (information-theoretic empirical Rademacher bound)
With probability 1 −δ, the risk of predictor ˆf = AF(D, L) outputted by learning
machine AF is bounded by
R(f) ≤R(f, D, L) +

1 −2

ϵ
ϵ · 2−ei(RF,D,ϵ)

+ c3

1 −log2 δ
l
(18)
Proof: By Proposition 17 and deﬁnition of eﬀective information we have
R(F, D) = 1 −2

ϵ
ϵ ·
R−1
F,D(ϵ)

|Σ|
= 1 −2

ϵ
ϵ
2ei(RF,D,ϵ) .
The result follows by Theorem 10.
■
Rademacher
complexity
is
low
if
the
min-risk’s sharp
measurements
(high ei) are accurate (low ϵ), and conversely. Analogously to Corollary 13,
the Rademacher bound implies the future performance of a classiﬁer depends
on: (i) the fraction ϵ of the data that ˆf ﬁts; (ii) the weighted (by the fraction
ϵ of data that falsiﬁes them) sum of the fraction of hypotheses falsiﬁed; and
(iii) a conﬁdence term. Once again, the only assumption is that P and σ∗are
ﬁxed.
5
Discussion
Learning according to algorithm AF,D entails computing the min-risk, which
classiﬁes hypotheses about D according to how well they are approximated by
predictors in repertoire F. Repertoires that rule out many hypotheses when
they ﬁt labeled data (D, L) generate more eﬀective information than repertoires
that “approximate everything”. As a consequence, when and if an informative
repertoire ﬁts labeled data well, Corollary 13 implies we can be conﬁdent in
future predictions on unseen data.
A pleasing consequence of reformulating empirical VC-entropy and empirical
Rademacher complexity in terms of falsifying hypotheses is that it directly con-
nects Popper’s intuition about falsiﬁable theories to statistical learning theory,
thereby providing a rigorous justiﬁcation for the former.
Our motivation for reformulating learning theory information-theoretically
arises from a desire to better understand the role of information in biology.
Although Shannon information has been heavily and successfully applied to bio-
logical questions, it has been argued that it does not fully capture what biologists

Falsiﬁcation and Future Performance
77
mean by information since it is not semantic. For example, Maynard Smith states
that “In biology, the statement that A carries information about B implies that
A has the form it does because it carries that information” [10]. Shannon in-
formation was invented to study communication across prespeciﬁed channels,
and lacks any semantic content. Maynard Smith therefore argues that a diﬀer-
ent notion of information is needed to understand in what sense evolution and
development embed information into an organism.
It may be fruitful to apply statistical learning theory to models of develop-
ment. One possible approach is to consider analogs of repertoire F. For example,
F may correspond to the repertoire of possible adult forms a zygote could develop
into. The particular adult form chosen, ˆf ∈F, depends on the historical interac-
tions (D, L) between the organism and its environment, assuming these can be
suitably formalized. The information generated by the organism’s development
would then have implications for its future interactions with its environment.
More speculatively, a similar tactic could be applied to quantify the information
embedded in populations by inheritance and natural selection.
Acknowledgements. I thank David Dowe and Samory Kpotufe for useful
comments on an earlier version of this paper.
References
1. Balduzzi, D., Tononi, G.: Integrated Information in Discrete Dynamical Systems:
Motivation and Theoretical Framework. PLoS Comput. Biol. 4(6), e1000091 (2008)
2. Balduzzi, D., Tononi, G.: Qualia: the geometry of integrated information. PLoS
Comput. Biol. 5(8), e1000462 (2009)
3. Boucheron, S., Lugosi, G., Massart, P.: A Sharp Concentration Inequality with
Applications. Random Structures and Algorithms 16(3), 277–292 (2000)
4. Bousquet, O., Boucheron, S., Lugosi, G.: Introduction to Statistical Learning The-
ory. In: Bousquet, O., von Luxburg, U., R¨atsch, G. (eds.) Machine Learning 2003.
LNCS (LNAI), vol. 3176, pp. 169–207. Springer, Heidelberg (2004)
5. Corﬁeld, D., Sch¨olkopf, B., Vapnik, V.: Falsiﬁcation and Statistical Learning The-
ory: Comparing the Popper and Vapnik-Chervonenkis Dimensions. Journal for
General Philosophy of Science 40(1), 51–58 (2009)
6. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of the Philosophy of Science. Phi-
losophy of Statistics, vol. 7, pp. 901–982. Elsevier (2011)
7. Harman, G., Kulkarni, S.: Reliable Reasoning: Induction and Learning Theory.
MIT Press (2007)
8. Koltchinskii, V.: Rademacher penalties and structural risk minimization. IEEE
Trans. Inf. Theory 47, 1902–1914 (2001)
9. Lewis, D.: On the Plurality of Worlds. Basil Blackwell, Oxford (1986)
10. Maynard Smith, J.: The Concept of Information in Biology. Philosophy of Sci-
ence 67, 177–194 (2000)
11. Popper, K.: The Logic of Scientiﬁc Discovery. Hutchinson (1959)

78
D. Balduzzi
12. Solomonoﬀ, R.J.: A formal theory of inductive inference I, II. Inform. Control 7,
1–22, 224-254 (1964)
13. Vapnik, V.: Estimation of Dependencies Based on Empirical Data. Springer (1982)
14. Vapnik, V.: Statistical Learning Theory. John Wiley & Sons (1998)
15. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Springer (2005)
16. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. The Com-
puter Journal 11, 185–194 (1968)
17. Wallace, C.S., Dowe, D.L.: Minimum Message Length and Kolmogorov Complexity.
The Computer Journal 42(4), 270–283 (1999)

The Semimeasure Property of Algorithmic
Probability – “Feature” or “Bug”?
Douglas Campbell
Philosophy Programme, University of Canterbury, Christchurch 8041, New Zealand
douglas.campbell@canterbury.ac.nz
Abstract. An unknown process is generating a sequence of symbols,
drawn from an alphabet, A. Given an initial segment of the sequence,
how can one predict the next symbol? Ray Solomonoﬀ’s theory of in-
ductive reasoning rests on the idea that a useful estimate of a sequence’s
true probability of being outputted by the unknown process is provided
by its algorithmic probability (its probability of being outputted by a
species of probabilistic Turing machine). However algorithmic proba-
bility is a “semimeasure”: i.e., the sum, over all x ∈A, of the condi-
tional algorithmic probabilities of the next symbol being x, may be less
than 1. Solomonoﬀthought that algorithmic probability must be normal-
ized, to eradicate this semimeasure property, before it can yield accept-
able probability estimates. This paper argues, to the contrary, that the
semimeasure property contributes substantially, in its own right, to the
power of an algorithmic-probability-based theory of induction, and that
normalization is unnecessary.
Keywords: Algorithmic probability, sequence prediction, inductive rea-
soning, Solomonoﬀinduction, Solomonoﬀnormalization, semimeasure,
convergence theorem.
1
Introduction
This paper is about whether a certain property of algorithmic probability (ALP)
– namely, its so-called “semimeasure” property – should be regarded as a “bug”
(i.e., as a source of theoretical weakness, that must be worked around and cor-
rected for) or as a “feature” (i.e., as serving a useful or necessary function) within
the context of an ALP-based theory of inductive reasoning. I will begin by de-
scribing ALP and its application to inductive inference. Next I will describe the
semimeasure property of ALP, and explain why it is commonly considered to be
a bug that must be eradicated and patched over with an ad hoc normalization
procedure. Finally I will contend that this negative assessment of the semimea-
sure property’s worth is incorrect. I will argue that the semimeasure property is
properly seen as being a valuable and important feature of ALP, which makes
a major contribution to the power, scope and elegance of an ALP-based theory
of inductive reasoning. I will demonstrate that to normalize ALP is to pay a
high price, in terms of lost theoretical elegance, in order to attain a result – the
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 79–90, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

80
D. Campbell
elimination of the semimeasure property – that is wholly undesirable in the ﬁrst
place. It is to “cut oﬀthe nose of ALP to spite its face”, so to speak.
2
Notation
The symbol, ∧, denotes the empty string, “”. |x| denotes the length, in symbols,
of the string, x. E.g., | ∧| = 0 and |ABC| = 3. xy denotes the concatenation
of strings x and y. ∧x = x = x∧, and |xy| = |x| + |y|. The preﬁxes of a string
include all initial segments of the string. Every string is a preﬁx of itself, ∧is a
preﬁx of every string, and x is a preﬁx of xy.
3
Algorithmic Probability (ALP)
The concept of ALP involves a type of computing device that I shall here call a
Solomonoﬀmachine. Such a machine is a ﬁnite state automaton equipped with
an indeﬁnitely expandable internal working memory, which accepts a sequence
of randomly generated binary digits as input, and which emits another sequence
of binary digits as output. At each step, the machine either might or might
not accept a randomly generated digit of input, and might or might not emit
a digit of output. Over the full course of its operation it might accept either a
ﬁnite, or an inﬁnite, number of input digits, and it might emit either a ﬁnite,
or an inﬁnite, number of output digits. It has no capacity to retract or modify
its output, so each digit of output is “set in stone” the moment it is produced.
The indeterministic process that generates its input is “fair”, 0s and 1s being
equiprobable.
A Solomonoﬀmachine can be concretely realized as a probabilistic “mono-
tonic” Turing machine with three tapes, these being: (i) a two-way, read-only,
initially blank work tape; (ii) a one-way, read-only input tape pre-inscribed with
an ongoing randomly generated binary sequence; and (iii) a one-way, write-only,
unidirectionally accessible, initially blank output tape with the alphabet, {0, 1}.
Let Sx denote a particular Solomonoﬀmachine (having some particular state-
transition table).
The string, y, encodes the string, z, on Sx, iﬀany input to Sx preﬁxed by
y will result in Sx’s output being preﬁxed by z. (So, for example, if Sa’s out-
put must start with 11 provided its input starts with 010, then 010 encodes
11 on Sa.)1
Let Fx be the function computed by Sx. Fx(y) = z iﬀz is the longest string
encoded by y on Sx. (So, for example, if 010 encodes 11 on Sa, but if it encodes
neither 110 or 111 on Sa, then Fa(010) = 11.)
A given Solomonoﬀmachine will usually produce any one of a variety of diﬀer-
ent outputs with diﬀerent probabilities, its output depending on which particular
random input it is fed with. Let Px denote the probability distribution, over bi-
nary output strings, associated with Sx. Px(z) = q just in case the probability
1 This is similar to the notion of Educated Turing Machine in [1, sec. 4][2, sec. 2.3].

The Semimeasure Property of Algorithmic Probability – “Feature” or “Bug”?
81
of Sx’s output being preﬁxed by the binary string, z, is q. Px(w|z) denotes the
conditional probability of Sx’s next symbols of output constituting the binary
string w, given that Sx’s output to date is z. Px(w|z) = Px(zw)/Px(z).
The string, p, is a program that causes Sx to simulate Sy, iﬀ, for any string
z, Fx(pz) = Fy(z). In other words, the eﬀect of Sx receiving an input pre-
ﬁxed by a program that causes Sx to simulate Sy is to cause Sx to “change
personalities” (so to speak), by thereafter exhibiting input-output behavior in-
distinguishable from that of Sy. (∧is a program that causes every Solomonoﬀ
machine to simulate itself.)
A universal Solomonoﬀmachine is a Solomonoﬀmachine that can be pro-
grammed to simulate any Solomonoﬀmachine. That is, if Su is universal,
then for any Solomonoﬀmachine, Sx, there is a program that causes Su to
simulate Sx.
Let the reference machine, Sm, be some particular universal Solomonoﬀma-
chine that has been selected, by us, to serve as our benchmark for measuring
the ALP of strings. The ALP of any string, x, is simply Pm(x). That is, a
string’s ALP is the probability of our reference machine’s output being preﬁxed
by the string. ALP is obviously machine-dependent, in the sense that the ALP
of a string will tend to vary depending on which particular universal Solomonoﬀ
machine we choose to be our reference machine.
4
The Semimeasure Property of ALP
A probability distribution, ρ, over binary sequences is a measure if and only if
ρ(x) = ρ(x0) + ρ(x1) for any string, x. In other words, it is a measure if the
conditional probabilities it assigns to the next symbol after x being a 0 and
to the next symbol after x being a 1 must always, for any x, sum to unity.
On the other hand, ρ is a semimeasure if and only if there is some string, x,
such that ρ(x) > ρ(x0) + ρ(x1). For example, suppose that ρ(010) = 0.6, while
ρ(0100) = 0.3 and ρ(0101) = 0.1. This being so, not all the probability assigned
by ρ to the sequence “010” is split between and inherited by the two, longer
strings “0100” and “0101”. Some of the shorter string’s probability (0.2 of the
0.6) instead “goes missing”, so to speak. This makes ρ a semimeasure.
Recall that Pm(x) is the probability of Sm’s output being preﬁxed by x.
Having outputted x, Sm must next do one of three diﬀerent things: (i) it might
output another 0; (ii) it might output another 1; or (iii) it might stop out-
putting 0s and 1s once and for all as a result of either having halted or having
gone into an inﬁnite, unproductive loop. Sm always, for any x, has a non-zero
probability of doing the last of these things (there being a non-zero probability
that Sm’s random input will start with a program that causes it to simulate a
second Solomonoﬀmachine that will always, regardless of its input, output x
and then halt). It follows that Pm(x) > Pm(x0) + Pm(x1), which makes Pm a
semimeasure, not a measure.

82
D. Campbell
5
ALP’s Application to Induction, and the Semimeasure
Problem
ALP was discovered by Ray Solomonoﬀ, who used it as the central ingredient of
a theory of inductive reasoning [3, 4]. The theory concerns a method for accom-
plishing a certain type of sequence prediction task. By way of illustrating the task,
let’s imagine that a black box has fallen to Earth from a place unknown. Attached
to the black box’s exterior is a symbol-stamping mechanism, through which is
threaded an initially blank tape. Casual inspection of the mechanism reveals that
it is capable of stamping only two types of symbols onto the tape – 0s and 1s –
and that each symbol will be stamped on the tape to the immediate right of its
predecessor. Both the ordering of these symbols, and the timing of each symbol’s
delivery, are under the control of a process hidden within the black box. We have
little or no idea what this process might be, but the gradually accumulating se-
quence of symbols it produces is exposed to our view. The black box receives no
input. Let the black box task be the task of making a probabilistic prediction about
the black box’s next symbol of output, based on its observed output-to-date.
Two types of method for accomplishing the black box task may be distin-
guished. A three-way method is a method which accepts any given binary string
of the black box’s output-to-date, and then assigns conditional probabilities to
each of three distinct possibilities, these being: (i) the next symbol will be a 0;
(ii) the next symbol will be a 1; and (iii) the black box will never output another
0 or 1 again, and so the next symbol on the black box’s output tape (together
with all subsequent symbols) will default to
(where
represents a blank). A
two-way method, on the other hand, assigns conditional probabilities to only two
possibilities: (i) the next symbol will be a 0; and (ii) the next symbol will be a 1.
A two-way method should obviously be used only if the possibility of the black
box’s output terminating can be dismissed out of hand. Such might be the case
because one knows from the outset that the process operating in the box will
keep producing binary digits forever (e.g., perhaps one has been told as much
by a trustworthy source who has looked into the box).
Let μ(x) denote the true, objective probability of the black box’s output being
preﬁxed by the binary string, x, and let μ(y|x) denote the conditional probability
of the black box’s next symbols of output comprising the string, y, given that
its output-to-date is x. μ(y|x) = μ(xy)/μ(x). If the process in the black box is
somehow guaranteed by facts about its constitution to keep producing 0s and
1s forever, then μ will be a measure. Otherwise, if there is a non-zero objective
probability of the black box’s output terminating at some point, then μ will be a
semimeasure. If the process in the black box is deterministic then, for any string
x, either μ(x) = 0 or μ(x) = 1. If it is indeterministic then there will be some
strings x such that 0 < μ(x) < 1.
The essential idea behind Solomonoﬀ’s theory of induction is that we should
predict the output of the black box (or equivalent symbol source) by assuming
it has the same output producing dispositions as our reference machine. In its
simplest form, the idea is that we should use Pm(x) as an estimate of μ(x) (or,
equivalently, Pm(y|x) as an estimate of μ(y|x)). So, for instance, if the reference

The Semimeasure Property of Algorithmic Probability – “Feature” or “Bug”?
83
machine would, if its output-to-date were “0011”, have a probability of 0.3 of
next outputting a 0, and if the black box’s output-to-date is “0011”, then, so the
idea goes, we should assign a probability of 0.3 to the black box’s next symbol
of output being a 0.2
Solomonoﬀfocused speciﬁcally on using ALP to develop a two-way method for
predicting the extension of a binary string. He seems not to have considered us-
ing it to construct a three-way method. Hence, at least as far as Solomonoﬀwas
concerned, μ must be a measure, and there are only two things that the black
box might legitimately do next – output a 0, or output a 1. (Indeed, Li and Vi-
tanyi report that Solomonoﬀ, “viewed the notion of measure as sacrosanct” [12,
p. 280].) But, as we have seen, Pm is a semimeasure, and there are, at any point
in time, three things the reference machine might do next – output a 0, output
a 1, or stop producing binary output. Hence a problem arises (the “semimeasure
problem”, as I will call it). Since a two-way method must divide conditional prob-
ability only between the possibilities of the next symbol being a 0 or of it being a
1, the conditional probabilities it assigns to these two possibilities should sum to
unity. However, because the reference machine divides probability between three
future possibilities, not just two, the conditional probabilities it apportions to 0
and to 1 may (and in fact, always will) sum to a value less than unity.
Solomonoﬀaddressed this problem by describing a normalization operation
that converts the semimeasure, Pm, into a corresponding measure, Pm′ [4, 13].
This operation works by, in eﬀect, taking the probability of the reference ma-
chine receiving a random input that will cause it to terminate its output after
outputting the binary string, x, and then redistributing this probability back
over all random inputs to the reference machine that will cause it to output at
least one more 0 or 1 after x. This is done recursively, for progressively longer
strings, x. That is:
Pm′(∧) = 1
Pm′(x0) = Pm′(x)
Pm(x0)
Pm(x0) + Pm(x1)
Pm′(x1) = Pm′(x)
Pm(x1)
Pm(x0) + Pm(x1)
2 Solomonoﬀ’s theory of induction is to be contrasted with the closely related Mini-
mum Message Length (MML) approach of Wallace and Boulton [5–10]. For a com-
parison of the two approaches, see [1] and [2, p. 404]. Some proponents of MML
argue that Solomonoﬀ’s theory isn’t really a theory of “induction” at all (see, for
instance, [2, p. 405–407] and [11, p. 930–931]), with one of the thoughts being that,
whereas genuine induction involves reasoning from a body of observations to a gen-
eral hypothesis, Solomonoﬀ’s procedure yields no such general hypothesis, and in-
stead yields only predictions about future observations (of upcoming 0s and 1s). I
contend that Solomonoﬀ’s procedure is genuinely inductive, in at least the sense
that it yields predictions about the future behaviour of the black box that are not
deductively implied by anything that is known about the black box or its output to
date. However pursuing this issue would take me far from the topic of this paper.

84
D. Campbell
Solomonoﬀ’s considered proposal was that we should predict the black box’s
output by using the measure, Pm′, rather than the un-normalized semimeasure,
Pm, as an estimate of μ [4].
6
“Bug” or “Feature”?
Solomonoﬀhimself made mention of a distinction between the “features” and
“bugs” of ALP while defending his theory of induction from a pair of criti-
cisms [14]. The ﬁrst criticism concerns the fact that the values of Pm(x) and
Pm′(x) are uncomputable, and hence largely unknowable in practice. The second
concerns the fact that these values are also radically dependent on our partic-
ular choice of reference machine, and to this extent arbitrary and subjective.
Solomonoﬀresponded to these criticisms by maintaining that both the uncom-
putability and the machine-dependence of ALP are to be properly seen as playing
useful, and indeed indispensible, roles in his theory of induction, rather than as
being sources of theoretical weakness. Speciﬁcally, he held that uncomputability
is simply a necessary ﬂipside of completeness: that ALP is uncomputable pre-
cisely because it can be used to detect and extrapolate any computable regularity
or pattern in a sequence of data [15]. In a similar vein, he held that machine de-
pendence is vital in enabling us to factor in whatever prior information we might
possess about the symbol source. Our prior knowledge about the symbol source
should, Solomonoﬀmaintained, be directly reﬂected in our particular choice of
reference machine [16]. He summed up the situation by saying that both ALP’s
uncomputability and its machine-dependence count as “necessary features” of
his theory, not as “bugs” [14].
Following Solomonoﬀ’s lead, let’s count among the “features” of ALP any of
its properties that should be celebrated by a proponent of an ALP-based theory
of induction for the valuable role they play in the theory, and let’s count among
its “bugs” those of its properties (if any) that are to be regretted for the problems
and weaknesses they introduce. When Solomonoﬀheld that uncomputability and
machine-dependence are features, not bugs, of ALP, he was charging critics of
his theory of induction with overlooking ways in which these properties can be
turned to the theory’s advantage, by being made to serve useful or necessary
functions within it. It is clear that Solomonoﬀhimself regarded the semimeasure
property as a genuine “bug” in the idea that we should use ALP to predict the
output of the symbol source, for – as just explained – he used a normalization
procedure to eradicate it, and did not attempt to show that it can be exploited to
play a useful role in the theory. I will now try to show that it is instead properly
regarded as being a very valuable “feature”.
7
Another Way of Tackling the Semimeasure Problem
We’ve seen that the semimeasure problem arises because, whereas the black box
must do one of only two things next – output a 0, or output a 1 – the reference
machine can instead do either one of three things next – output a 0, output a 1,

The Semimeasure Property of Algorithmic Probability – “Feature” or “Bug”?
85
or terminate its output. However this disparity between the ranges of behaviours
the two devices can exhibit arises only when it is stipulated from the outset that
the black box’s output can’t terminate. When a three-way method is used to
predict the black box’s output, no such stipulation is in force. Hence, provided
we use ALP to construct a three-way method, rather than a two-way method,
then each and every possible behaviour of the reference machine corresponds
directly to a possible behaviour of the black box, and vice versa.
The following proposal for resolving the semimeasure problem therefore sug-
gests itself: whereas Solomonoﬀused ALP to construct a two-way method for
predicting the extension of a binary sequence, we will instead use it to construct
a three-way method. In other words, we will include the possibility of the black
box’s output terminating among the set of alternative outcomes to which a prob-
ability must be assigned. The probability we assign to the black box’s output
terminating after it has outputted the string, x, will be identical to the prob-
ability of the reference machine’s output terminating after it has outputted x.
Under this proposal, ALP’s semimeasure property doesn’t merely cease to be a
“bug” in an ALP-based theory of induction, but instead acquires the status of
being a useful and necessary “feature”, for in order for a three-way method to
assign a certain quantity of probability to the possibility that the sequence has
terminated, it must leave the selfsame quantity of probability unassigned either
to the possibility that the next symbol will be a 0 or to the possibility that it
will be a 1. Hence the probability distribution that such a method is based on
must be a semimeasure, and cannot be a measure.
We now have two proposals on the table, which I will call Solomonoﬀ’s pro-
posal (it being the proposal that Solomonoﬀchampioned) and the new proposal
respectively. According to Solomonoﬀ’s proposal, the proper goal of an ALP-
based theory of induction is to construct a maximally reliable two-way method
for predicting the continuation of a binary series, and the normalized measure,
Pm′(x), should be used as an estimate of μ(x). According to the new proposal,
on the other hand, ALP is best used to construct a three-way method for making
such predictions, and the unnormalized semimeasure, Pm(x), should be used as
an estimate of μ(x). Both proposals circumvent the semimeasure problem, and
are on an equal footing in this respect, but I will now oﬀer reasons to believe
that the new proposal is nevertheless superior to Solomonoﬀ’s.
The ﬁrst and most important reason concerns Solomonoﬀ’s own grounds for
thinking that ALP-based predictions about the black box’s output are likely to
be any good. I will argue that these grounds oﬀer stronger support to the new
proposal than they do to Solomonoﬀ’s own proposal.
The following concepts and notation will be useful. Let a padded string be
a binary string with a
appended to its rightmost end. (E.g., “0110 ” is a
padded string.) Let ρ(x ) denote the probability assigned by ρ to the possibility
that the binary string, x, won’t be followed by any more 0s or 1s. That is,
ρ(x ) = ρ(x) −ρ(x0) −ρ(x1). Notice that if ρ assigns a non-zero probability
to any padded string, then ρ is a semimeasure. Let’s say that the distribution,
ρ dominates the distribution, ν, iﬀthere is some non-zero probability, p, such

86
D. Campbell
that, for any binary or padded string x, ρ(x) ≥pν(x). So, for example, if, for
any binary or padded x, the probability assigned by ρ to x never undershoots
the probability assigned by ν to x by more than a multiplicative factor of, say,
0.3, then ρ dominates ν (to within a factor of 0.3).
Now, let’s suppose that we use a distribution, ρ as an estimate of μ. It can
be shown [4] that, on assumption that ρ dominates μ, then, as time goes by and
as the black box’s output-to-date grows in length, the conditional probabilities
that we assign to the next symbol by using ρ will rapidly converge to match the
true, objective probabilities that are assigned to this symbol by μ. That is, if
o1...n denotes the black box’s ﬁrst n symbols of output and on denotes its nth
symbol of output, then limn→∞[ρ(on|o1...n−1) −μ(on|o1...n−1)] = 0.
This is encouraging, for it means that our estimate of μ will yield good predic-
tions in the long run provided that it dominates μ. But how can we arrange for
our estimate of μ to dominate μ when – ignorant as we are about what is in the
black box – we know little or nothing about the nature of μ itself? Solomonoﬀ’s
answer is that we can maximize our chances of “catching” μ within the set of
distributions that are dominated by our estimate simply by casting our net very
widely indeed. A probability distribution, ρ is computable iﬀthere is a classical
Turing machine that will, when given a binary string, x, as input, output an
encoding of ρ(x). Solomonoﬀshowed that Pm′ is “universal” in the sense that
it dominates every computable measure [4]. Hence Pm′ will dominate μ if μ is a
computable measure.
That’s the good news. The bad news is that if μ is a semimeasure – which
is to say, if the process in the black box has the capacity to produce a termi-
nating output – then Solomonoﬀ’s convergence result doesn’t provide us with
any assurance that Pm′ will yield accurate probabilistic predictions in the long
run. This limitation of Pm′ is unsurprising, for, after all, Pm′ was designed by
Solomonoﬀto provide us with a two-way method for making predictions, and
as such it is to be used only when it is known that the black box’s output won’t
terminate. But it is still a very serious limitation, for, after all, it is perfectly
possible that the process in the black box will stop outputting 0s and 1s at some
point.3 Ideally, we would like our estimate of μ to yield accurate predictions
irrespective of what is in the black box, and irrespective of the nature of μ. The
more distributions that are dominated by our estimate of μ, the smaller the risk
of μ escaping domination by it, and so the greater the chances that the estimate
will lead us to the true probabilities [17, p. 28]. In order for the estimate to
be able to lead us to the true probabilities even if the black box can produce
3 For example, C.S. Wallace [2, p. 407] imagines a process that examines all the stable
isotopes of the chemical elements, one by one, in order of their atomic weight, out-
putting extensive data about their physical, chemical and spectroscopic properties
as it goes. After examining lead-208 the process will stop, lead-208 being the last
stable isotope, and so the sequence of data it is producing will terminate at this
point. A predictor who has observed a suﬃciently long initial segment of this data
sequence should ideally be able to predict both that the sequence will eventually
terminate, and the point at which it will do so.

The Semimeasure Property of Algorithmic Probability – “Feature” or “Bug”?
87
a terminating output, we need it to dominate, not just distributions that are
measures, but also distributions that are semimeasures.
Is there a distribution we might use as our estimate of μ, which dominates,
not just all computable measures, but also all computable semimeasures? Indeed,
there is, and it is none other than the original, unnormalized version of ALP,
Pm. As a ﬁrst step to understanding why Pm dominates such a large class of
distributions, it will help to introduce the notion of a Solomonoﬀdistribution,
this simply being a distribution that is associated with some particular (universal
or non-universal) Solomonoﬀmachine. That is, ρ is a Solomonoﬀdistribution
iﬀthere is some Solomonoﬀmachine, Sy, such that, for all binary strings x,
ρ(x) = Py(x). Some Solomonoﬀdistributions are measures, while others are
semimeasures. The Solomonoﬀdistribution, Px, is a measure iﬀ, regardless of
which randomly generated input the corresponding Solomonoﬀmachine, Sx, is
supplied with, its binary output will never stop. On the other hand, if there is at
least one possible input to Sx that will result in its output terminating at some
point, then Px is a semimeasure.
It is easily shown that Pm dominates all Solomonoﬀdistributions, including
all those that are measures and all those that are semimeasures. To see this,
suppose ρ is some Solomonoﬀdistribution. This being so, there will be some
Solomonoﬀmachine, St, whose probability of producing an output preﬁxed by
x is ρ(x). Since our reference machine, Sm, is universal, there will be some
program, g, that will cause it to simulate St. The probability of g occurring as
a preﬁx of Sm’s randomly generated input is simply 1/2|g|. If g does occur as
a preﬁx of Sm’s input, then, when Sm has read in g, it will begin simulating
St, and from this point forward it will exhibit output-producing propensities
indistinguishable from those of St. Thus Sm has a probability of at least 1/2|g| of
simulating St, and if it does simulate St then it will, like St, have a probability of
ρ(x) of producing an output preﬁxed by x. This means that Sm’s own probability
of producing an output preﬁxed by x must be at least 1/2|g|ρ(x), from which it
follows that Pm dominates ρ (to within a factor of 1/2|g|). As for ρ, so for any
Solomonoﬀdistribution, whether it be a measure or a semimeasure.
Every computable distribution – whether it be a measure or a semimeasure –
is a Solomonoﬀdistribution. In order to see this, recall that if ρ is computable
then there is a classical Turing machine that, when given a binary string x as
input, will produce an encoding of ρ(x) as output. Given this classical Turing
machine, we can easily engineer a Solomonoﬀmachine, Sh, that simulates the
classical Turing machine in order to determine, for any binary string x, the value
of ρ(x), and which then outputs a sequence preﬁxed by x with a probability of
ρ(x), while using the randomly generated sequence of 0s and 1s on its own input
tape as a source of indeterminism. Since Sh’s design ensures that Sh(x) = ρ(x)
for all binary x, ρ is a Solomonoﬀdistribution.
Since Pm dominates all Solomonoﬀdistributions, and since all computable
distributions are Solomonoﬀdistributions, Pm dominates all computable distri-
butions, including not just all the computable measures that are dominated by
Pm′, but also all computable semimeasures. This being so, a three-way inductive

88
D. Campbell
reasoning method based on Pm is inherently less risky than a two-way method
based on Pm′. The conditions that μ must satisfy in order for the former method
to be guaranteed to yield accurate probabilistic predictions in the long run are
considerably weaker and less demanding than those it must satisfy in order for
the latter method to yield the same guarantee. The point might be put by saying
that Pm is “more universal” than Pm′, in the sense that Pm dominates a much
larger class of computable distributions than Pm′ does.
The second reason for preferring the new proposal to Solomonoﬀ’s proposal
is pragmatic, and concerns the practical utility of the inductive methods they
prescribe. We can safely use Solomonoﬀ’s two-way method only when we can be
certain, ahead of time, that the process we are predicting will keep outputting
symbols forever. It is, however, surely impossible to ﬁnd any real-life example
of a process that satisﬁes this condition. Science teaches us that the universe
we inhabit is governed by the second law of thermodynamics, and that it is
fated, if not to a Big Crunch, then to heat-death. Hence, far from it being the
case that we can ever be perfectly certain that a symbol source we are dealing
with will keep producing output for eternity, the smart money will always be on
its output eventually terminating. Our background knowledge about our world
may provide us with but little information about the likely symbol-producing
propensities of the black box, but it does at least tell us that terminations of
output are probably in the oﬃng. This being so, we must, when we are doing
induction in the real world, use a method that will yield acceptable results if μ
is a semimeasure. A three-way method based on Pm satisﬁes this requirement,
while a two-way method based on Pm′ does not.
The third reason has to do with the comparative simplicity and elegance of the
two proposals. The fundamental idea behind an ALP-based theory of induction
is that an hypothesis that attributes certain output-producing dispositions to a
symbol source can be represented by a program that causes the reference machine
to itself manifest these selfsame output-producing dispositions. Some programs,
of course, cause the reference machine to have a non-zero probability of produc-
ing a terminating output. Solomonoﬀthought that such terminating programs
“do not result in useful output” [13, p. 567]. But if we take to its natural conclu-
sion the idea that programs represent hypotheses about the output-producing
dispositions of the symbol source, then why should we not hold instead that
such a program, which causes the reference machine to have a certain chance
of producing an output that ends, represents a hypothesis that says the symbol
source has this same chance of producing an output that ends? Why not indeed!
There is no principled reason not to, and if we do then the resulting theory of
induction is both more elegant, in view of the fact that it doesn’t arbitrarily
treat terminations of output diﬀerently than 0s and 1s, and more predictively
powerful, since it yields a three-way method that can cope with terminations
of output, rather than a two-way method that can’t. On the other hand, if,
like Solomonoﬀ, we treat the reference machine’s terminating outputs as being
predictively meaningless, then we are left to confront the semimeasure problem,
and must wheel in a normalization operation to surmount it. The inclusion of

The Semimeasure Property of Algorithmic Probability – “Feature” or “Bug”?
89
a normalization operation further complicates the theory and detracts from its
elegance.
The ﬁnal reason I oﬀer for thinking that the new proposal should be pre-
ferred over Solomonoﬀ’s proposal concerns certain technical objections to the
normalization operation that the new proposal avoids by simply dispensing with
normalization altogether. These include, for instance, the objection that there
are several rival methods of normalizing, each yielding measures with diﬀerent
properties, and no very compelling reason to choose one over another [12, p. 281];
the objection (due to Robert M. Solovay) that every choice of normalization op-
eration has an unboundedly large impact on the relative probabilities assigned
to some particular sequence [12, p. 301] (but c.f. [18]); and the objection that,
while Pm is at least lower semicomputable, Pm′ is not even computable in this
restricted sense.
To conclude, it is my contention that ALP’s property of being a semimeasure
appears to be a “bug” in an ALP-based theory of induction only if one insists
on trying to whack the round peg of ALP into the square hole of a two-way
method for predicting a black box’s output. The semimeasure problem evapo-
rates entirely if one accepts terminations of output as being events worthy of
prediction, and therefore uses ALP to construct a three-way method for making
predictions.
References
1. Wallace, C., Dowe, D.: Minimum message length and Kolmogorov complexity.
Computer Journal 42, 270–283 (1999)
2. Wallace, C.: Statistical and Inductive Inference by Minimum Message Length. In-
formation Science and Statistics. Springer (2005)
3. Solomonoﬀ, R.: A formal theory of inductive inference: Parts 1 and 2. Information
and Control 7, 1–22, 224–254 (1964)
4. Solomonoﬀ, R.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Trans. Infor. Theory IT-24, 422–432 (1978)
5. Wallace, C., Boulton, D.: An information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)
6. Boulton, D., Wallace, C.: The information content of a multistate distribution. J.
Theoret. Biol. 23, 269–278 (1969)
7. Boulton, D., Wallace, C.: A program for numerical classiﬁcation. The Computer
Journal 13(1), 63–69 (1970)
8. Boulton, D., Wallace, C.: An information measure for hierarchic classiﬁcation. The
Computer Journal 16(3), 254–261 (1973)
9. Wallace, C., Boulton, D.: An invariant Bayes method for point estimation. Classi-
ﬁcation Society Bull. 3(3), 11–34 (1975)
10. Boulton, D., Wallace, C.: An information measure for single-link classiﬁcation. The
Computer Journal 18(3), 236–238 (1975)
11. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Bandyopadhyay, P., Forster, M. (eds.) Philos-
ophy of Statistics. Handbook of the Philosophy of Science, vol. 7, Elsevier (2011)
12. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its applications,
2nd edn. Springer (1997)

90
D. Campbell
13. Solomonoﬀ, R.: Three kinds of probabilistic induction: universal distributions and
convergence theorems. The Computer Journal: Christopher Stewart Wallace (1933-
2004); Memorial Special Issue 51, 566–570 (2008)
14. Solomonoﬀ, R.: The Kolmogorov lecture: The universal distribution and machine
learning. Computer Journal 46, 598–601 (2003)
15. Solomonoﬀ, R.: The discovery of algorithmic probability. Journal of Computer and
System Sciences 55, 73–88 (1997)
16. Solomonoﬀ, R.: Does algorithmic probability solve the problem of induction? Tech-
nical Report (1997)
17. Hutter, M.: On generalized computable universal priors and their convergence,
IDSIA-05-05 (2005)
18. Solomonoﬀ, R.: The probability of “undeﬁned” (non-converging) output in gener-
ating the universal probability distribution. Inf. Process. Lett. 106, 238–240 (2008)

Inductive Inference and Partition
Exchangeability in Classiﬁcation
Jukka Corander1,3, Yaqiong Cui1, and Timo Koski2
1 Department of Mathematics and Statistics, University of Helsinki,
FI-00014, Finland
{jukka.corander,yaqiong.cui}@helsinki.fi
2 Department of Mathematics, Royal Institute of Technology,
S-100 44 Stockholm, Sweden
tjtkoski@kth.se
3 Department of Mathematics, ˚Abo Akademi University, FI-20500 Turku, Finland
Abstract. Inductive inference has been a subject of intensive research
eﬀorts over several decades. In particular, for classiﬁcation problems sub-
stantial advances have been made and the ﬁeld has matured into a wide
range of powerful approaches to inductive inference. However, a consid-
erable challenge arises when deriving principles for an inductive super-
vised classiﬁer in the presence of unpredictable or unanticipated events
corresponding to unknown alphabets of observable features. Bayesian
inductive theories based on de Finetti type exchangeability which have
become popular in supervised classiﬁcation do not apply to such prob-
lems. Here we derive an inductive supervised classiﬁer based on partition
exchangeability due to John Kingman. It is proven that, in contrast to
classiﬁers based on de Finetti type exchangeability which can optimally
handle test items independently of each other in the presence of inﬁnite
amounts of training data, a classiﬁer based on partition exchangeability
still continues to beneﬁt from a joint prediction of labels for the whole
population of test items. Some remarks about the relation of this work
to generic convergence results in predictive inference are also given.
Keywords: Bayesian learning, classiﬁcation, exchangeability, inductive
inference.
1
Introduction
Ever since Ray Solomonoﬀintroduced his pioneering views on inductive inference
[1], machine learning community has witnessed a steadily growing interest at the
quest of unravelling the foundations of this ﬁeld. This, of course, is an extremely
bold enterprise given the universal relevance of the questions related to induction,
and therefore, one could argue that the most concrete leaps forward have been
achievable by narrowing down the focus to particular problems of inductive
inference. Indeed, when considering the realm of classiﬁcation, we have witnessed
a tremendous development of both the theory and applications since the early
inductive approaches [2-4]. At the conceptual level, most intensive developments
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 91–105, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

92
J. Corander, Y. Cui, and T. Koski
of inductive classiﬁers have taken place in the context of tree- and network-like
constructs that are used to map conditional probabilities of both observable
features of items, their labels and other latent random variables involved in the
models. Broad overviews of various methods based on trees, forests and networks
can be found in [5-7]. As some of these constructs are too ﬂexible to be pursuable
in the presence of sparse training data, simpler generative classiﬁers such as naive
Bayes, have continued to thrive despite of the more advanced alternatives [8].
Over the past century, numerous arguments have been put forward to sup-
port the view that in a world where uncertainty and noisy observations reign,
inductive inference should be Bayesian. In particular, de Finetti’s work on ex-
changeability has played a central role in these developments [9],[10]. However,
the Bayesian perspective is far from universally accepted and many other ap-
proaches to inductive reasoning have had a peaceful coexistence with it over
several decades [11-13].
In the context of classiﬁcation, a particularly intriguing challenge arises when
we attempt to build principles of classiﬁcation out of observed discrete-valued
features possessing unknown alphabets. de Finetti type arguments do not ap-
ply to such data, and consequently it is less intuitive how a coherent Bayesian
inductive classiﬁer could look like when unpredictable events can occur. One
feasible answer to our quest lies in the theory of exchangeable random partitions
developed by Sir John Kingman [14-17], see also the elegant discussion of its ap-
pearances within several ﬁelds of study and its inductive implications by Sandy
Zabell [10]. Here we harness the particular form of partition exchangeability to
arrive at an inductive supervised classiﬁer as a generalization of more standard
classiﬁers arising under de Finetti type exchangeability.
It is worth noting that in other ﬁelds of machine learning, such as in compres-
sion, prediction and unsupervised modeling of text data, problems of inference
related to unknown alphabets have gained more attention and several solutions
have been proposed [18-20]. Interestingly, the line of work on data compres-
sion with unknown alphabets presented in [19] is very closely related to that of
Kingman and his predecessors, but it appears to have been derived completely
independently. The Bayesian hierarchical Dirichlet prior introduced in [18] is
combined with a multinomial likelihood for prediction of a single test item fea-
ture using a training set that may be very sparse in relation to the size of the
underlying alphabet over which predictions are made. An example of unsuper-
vised text analysis in such large discrete domains is represented by the discrete
hierarchical Dirichlet process model in [20]. The main diﬀerence between these
works and the current problem is that we consider simultaneous prediction of
labels for a population of test items when feature distributions are inductively
learned from training data.
We demonstrate that classiﬁers based on partition exchangeability behave dif-
ferently from classiﬁers derived under de Finetti type exchangeability in [21],[22],
where it was shown that marginal and simultaneous predictive classiﬁers of test
data become congruent almost surely when the amount of training data tends to
inﬁnity. Under partition exchangeability a positive probability always remains

Inductive Inference and Partition Exchangeability in Classiﬁcation
93
for the event that marginal and simultaneous predictive classiﬁers do not co-
incide, such that the population of test data items will play a clear role when
assessing how surprising any particular observed event is. The gain in classiﬁ-
cation accuracy achieved by a joint labeling of the population of test items in
the presence of sparse training data was also demonstrated in practice in [21]
and the optimality of such a predictive approach has also been considered in
[23],[24].
The article is structured as follows. In Section 2 we consider the behavior
of supervised predictive classiﬁers in relation to generic convergence results in
predictive inference. Classiﬁers under partition exchangeability are introduced
and their properties are examined in Section 3. The last section provides some
concluding comments and remarks.
2
Supervised Predictive Classiﬁcation under Partition
Exchangeability
Seymour Geisser pioneered the idea that generative supervised classiﬁcation of
multiple test items should be performed jointly, instead of predicting the labels
independently (marginally) for each test item [25],[26]. Recently, this idea was
made explicitly operational through introduction of random urn models for the
simultaneous classiﬁcation and also developed further using a marginalization
operation over joint labels based on decision-theoretic arguments [21],[22].
Denote the set of m available training items by M and correspondingly the set
of n test items by N. For each item we observe a ﬁnite vector of d features, such
that the element for the feature j takes values in an alphabet Xj, j = 1, ..., d.
In particular, these alphabets are not assumed known or ﬁxed a priori, or even
after observing the training items, such that the test items may harbor feature
values not previously seen. This poses a challenge for the inductive inference, as
an assumption of a ﬁxed alphabet for a feature would necessitate a retrospective
change of the predictive probabilities for all previous items whenever a new test
item with a previously unobserved feature value appears. Examples of features
where such a characteristic is an intrinsic property of the observation process
are genes with novel alleles, corpuses extended with new words, etc.
A solution to the problem with induction in this setting arises by an appli-
cation of the theory of partition exchangeability [14], which acknowledges the
presence of previously unanticipated phenomena when predictive probabilities
are formed from data. Partition exchangeability has been used in the context of
unsupervised classiﬁcation of genetic data [27],[28], however, we are not aware of
any earlier attempts to deﬁne generative supervised classiﬁcation models based
on this theory.
For notational simplicity, we will consider each feature alphabet as the set
Xj = {1, . . ., rj}, where rj is a positive, non-ﬁxed integer remaining implicit in
our analysis for all j = 1, . . . , d. A training item i ∈M is then characterized by
a feature vector zi, with the elements zij ∈Xj, j = 1, ..., d. Similarly, we have
for a test item i ∈N the feature vector xi, with elements xij ∈Xj. Collections
of the training and test data vectors are denoted by z(M) and x(N), respectively.

94
J. Corander, Y. Cui, and T. Koski
Let the training data represent k classes, such that T is a joint labeling of all
the training items. In simultaneous supervised classiﬁcation we will assign labels
to all the items in N in a joint fashion. A joint labeling S = (s1, . . . , sk), sc ⊆
N, c = 1, ..., k, is an ordered partition of N such that the classes are identiﬁed
by those present in T . Since any particular subset of classes present in T must
be allowed to lack test items, some classes can remain empty in S, in which case
the actual number of classes in S is 1 ≤k′ ≤k. A structure S implies a partition
of the feature vectors, such that x(sc), sc ⊆N, represents the collection of data
for the items in class c = 1, ..., k. Let S denote the space of eligible simultaneous
classiﬁcation structures for a given N.
Using a stochastic urn model for supervised classiﬁcation [21], a prior prob-
ability p(S|T ) of a classiﬁcation structure S is obtained conditionally on the
known ﬁxed classiﬁcation T of the training data. Here we simply assume that
the prior p(S|T ) is the uniform distribution over S as constructed in [21]. Given
a prior and the collections of training and test data, a simultaneous classiﬁer is
based on the posterior distribution over S, which is deﬁned as
p(S|x(N), z(M), T ) =
p(x(N)|z(M), S, T )p(S|T )

S∈S p(x(N)|z(M), S, T )p(S|T ),
(1)
where p(x(N)|z(M), S, T ) is the conditional predictive probability for the entire
observed population of test data. In contrast, a marginal classiﬁer speciﬁes the
predictive probabilities independently for each test item, such that the joint
distribution becomes
p(x(N)|z(M), ˚S, T ) =
k

c=1

i∈˚sc
p(xi|z(c), ˚S, T ),
(2)
where ˚S,˚sc indicate the overall labeling implied by the individual assignment of
the labels to the items in N and z(c) is the training data for class c.
The predictive inferences under a simultaneous and a marginal classiﬁer can
be related to certain generic results about learnability and merging of predic-
tive measures. Consider a sequence of stochastic variables {Xi}∞
i=1 in ﬁnite state
spaces with corresponding σ-ﬁelds. Given any {Xi}n
i=1, one may assess the prob-
ability of any event for a ﬁnite horizon prediction {Xi}n+l
i≥n+1. As in [29], one can
deﬁne a merging between two measures P m (A) = P(A|{Xi}m
i=1), Qm (A) =
Q(A|{Xi}m
i=1) such that
max
n≥m,A∈σ({Xi}n+l
i≥n+1)
| P m (A) −Qm (A) |< ϵ,
(3)
with Q -probability one. Consider that Q is the ‘true’ generating measure and
P is our model. Then the ﬁnite horizon event predictions at arbitrary times in
the future will approach the true forecasts provided by Q. In [30] Solomonoﬀ
shows that the subjective posterior one step prediction merges in mean square
with the ‘the true computable measure’. Under a stronger version of merging,
it follows as in [31] that if P and Q are two measures for a sequence of discrete

Inductive Inference and Partition Exchangeability in Classiﬁcation
95
random variables {Xn}n≥1 and Q << P, then P merges with Q. Here Q << P
means that Q is dominated by P, and thus Q(A) > 0 implies that P(A) > 0,
i.e., we cannot be surprised by an event that actually happens.
If P is chosen such that it corresponds to the predictive probability mass
distribution
p(x(N)|S) =

Θ
p(x(N) | θ,S)dF(θ|S),
(4)
and correspondingly, if Q corresponds to the predictive probability mass
distribution
q(x(N)|S) =

Θ
p(x(N) | θ,S)dQ(θ|S),
(5)
with appropriately deﬁned parameter space Θ, then if also dQ|S << dF|S on
Θ, we have that Q << P. Assume that
P m ↔p(x(N)|S,T ,z(M)) =

Θ
p(x(N) | θ,S)dP(θ|T ,z(M)),
(6)
Qm ↔q(x(N)|S,T ,z(M)) =

Θ
p(x(N) | θ,S)dQ(θ|T ,z(M)),
(7)
then we have
max
n≥m,A∈σ({Xi}n+l
i≥n+1)
| P m (A) −Qm (A) |< ϵ.
(8)
The interpretation of this is that the ﬁnite horizon event predictions at arbi-
trary times in the future will approach the true forecasts provided by Q, where
predictions based on P are conditioned on S,T ,z(M). This property of P can be
termed as a ‘suﬃcient for prediction’ in supervised classiﬁcation using simulta-
neous predictive probabilities.
Another form of learnability related to Qm given by
q(x(N)|S,T ,z(M)) =

Θ
p(x(N) | θ,S)dQ(θ|T ,z(M)),
(9)
is obtained using exchangeability, since by de Finetti, Q merges with the measure
Qθ given by any p(x(N)|θ, S) chosen from Q(θ|S). From the inductive perspective
it is essential that for prediction and supervised classiﬁcation we actually need
not estimate the (randomly chosen) ‘true’ Qθ, which can as well be regarded as a
tool for making inﬁnite horizon event predictions. Finally, since S is ﬁnite, under
the de Finetti type assumptions with a priori given alphabets yielding merging
and learnability, it is shown in [21] that the diﬀerence
| max
S∈S p(x(N)|S,T ,z(M)) −max
S∈S q(x(N)|S,T ,z(M))|
(10)
becomes arbitrarily small for large m. Moreover, it is also shown that the same
convergence behavior is obtained even if p(x(N)|S,T ,z(M)) is replaced by p(x(N)|
z(M), ˚S, T ), i.e. the marginal classiﬁer can accurately approximate the predic-
tions under the true model in the presence of a very large amount of training

96
J. Corander, Y. Cui, and T. Koski
data. The result can be intuitively understood in terms of consistent predictive
learning, as the predictive generating measures of classes are inﬁnite mixtures in
Bayesian learning, which tend to ﬁxed measures as the amount of training data
increases. Consequently, the dependence of labels arising from the uncertainty
about these measures gradually vanishes towards the limit. Here we will explore
how the simultaneous and marginal forms of inductive inference will relate to
each other under the circumstances where alphabets of discrete-valued features
are not ﬁxed a priori.
For the purpose of later examination of predictive probabilities, we will con-
sider separately the process of establishing suﬃcient statistics from the data x(N)
alone and jointly from z(M) and x(N), conditional on S and T . Given a structure
S, suﬃcient statistics arise under partition exchangeability for each subset of
data x(sc). More speciﬁcally, we assume an unrestricted form of exchangeability
[21],[28] which implies a product predictive measure over the d features. The
operative interpretation of this form of exchangeability is that the features are
modeled as conditionally independent given S.
Given a structure S with k classes, a predictive model is obtained by assum-
ing exchangeable equivalence relations for each feature in each class [28],[32]. For
counting purposes, we assign indices to items in sc using an arbitrary permuta-
tion of integers 1, ..., |sc|. To simplify the notation we use nc = |sc| to denote the
cardinality of a class in the sequel. Let I(·) be an indicator function and l index
a value in the alphabet Xj. Deﬁne ncjl = 
i∈sc I(xij = l) as the frequency of
items in class c that carried the value l for the feature j. Then, the information
in x(sc) can be compressed for each feature in terms of the count
ρcjt =
∞

l=1
I(ncjl = t),
(11)
which deﬁnes the frequency of distinct feature values that have been observed
exactly t times for feature j in class c. Further, let ρcj = (ρcjt)nc
t=1 denote a
partition of the integer nc into a vector of such counts. In a series of works
by John Kingman, e.g. [17], an explicit predictive model for observed values of
single feature of the above type was established using the following deﬁnition.
Deﬁnition 1. Partition exchangeability. A random partition ρcj is said to
be exchangeable, if any two partitions of nc having the same partition vector,
have the same predictive probability p(ρcj).
Kingman [17] discussed the relationship between the de Finetti type exchange-
ability and the above alternative form of exchangeability. A more general discus-
sion of these exchangeabilities in the context of inductive inference is found in
[10]. Kingman’s representation theorem establishes that the predictive distribu-
tion of the observations for a single feature equals under the partition exchange-
ability a Poisson-Dirichlet(ψ) distribution with parameter ψ
p(ρcj) =
nc!
ψ(ψ + 1) · · · (ψ + nc −1)
nc

t=1

(ψ
t )ρcjt
1
ρcjt!

.
(12)

Inductive Inference and Partition Exchangeability in Classiﬁcation
97
This particular probability result is also known as the Ewens sampling formula
and it arises as an inﬁnite mixture of probability measures over an inﬁnite-
dimensional simplex. By assuming that the partitions for distinct classes and
features are unrestrictedly exchangeable given S, we obtain the following product
predictive measure for the entire collection of test data:
p(x(N)|S) =
k

c=1
d

j=1
nc!
ψ(ψ + 1) · · · (ψ + nc −1)
nc

t=1

(ψ
t )ρcjt
1
ρcjt!

.
(13)
We now turn our attention to the inductive inference process where predictions
about x(N) are also conditioned on the training data z(M) and their a priori
given labeling T . Given that mc and mcjl are deﬁned analogously to nc and
ncjl, respectively, we obtain the partition vector ˜ρcj = (ρcjt)nc+mc
t=1
with elements
given by
˜ρcjt =
∞

l=1
I(ncjl + mcjl = t).
(14)
Using Bayes’ theorem and the above result based on the Kingman’s represen-
tation theorem, the predictive probability of the test data supervised by the
training data and the two joint labelings S, T can be written as
p(x(N)|z(M), S, T ) = p(x(N), z(M)|S, T )
p(z(M)|T )
=
k
c=1
d
j=1
(mc+nc)!
ψ(ψ+1)···(ψ+mc+nc−1)
mc+nc
t=1

( ψ
t )˜ρcjt
1
˜ρcjt!

k
c=1
d
j=1
mc!
ψ(ψ+1)···(ψ+mc−1)
mc
t=1

( ψ
t )ρcjt
1
ρcjt!

=
k

c=1
d

j=1
(mc+nc)!
mc!
(ψ + mc)(ψ + mc + 1) · · · (ψ + mc + nc −1)
mc+nc
t=1

( ψ
t )˜ρcjt
1
˜ρcjt!

mc
t=1

( ψ
t )ρcjt
1
ρcjt!
 .
(15)
3
Asymptotic Properties of Supervised Classiﬁers under
Partition Exchangeability
We will now examine how the predictive probabilities for simultaneous and
marginal supervised classiﬁers introduced in the previous section relate to each
other in the presence of increasing amounts of training data. As discussed earlier,
these two predictive probabilities need not in general be equal, even if the test
data were generated i.i.d. from the same underlying distribution as the train-
ing data. The asymptotic equality of simultaneous and marginal classiﬁers with
respect to an increasing amount of available training data was established in
[21],[22] for two families of distributions under de Finetti type of exchangeabil-
ity. Here we show that this property does not hold under the classiﬁcation model

98
J. Corander, Y. Cui, and T. Koski
arising from the partition exchangeability. An intuitive implication of this result
is that the simultaneous classiﬁer enjoys the advantage of jointly modeling the
entire population of labels of test items, such that it can better assess the level of
surprise in any particular observed event in relation to other events. Moreover,
this eﬀect does not wear out almost surely even if the amount of training data
tends to inﬁnity.
First, we need to introduce some additional notation for the marginal clas-
siﬁer. Let ˜ρcjt(i) denote the updated suﬃcient statistic (see eq. (14)) when
data xi from only a single test item is taken into account, deﬁned as ˜ρcjt(i) =
∞
l=1 I(mcjl+ni;cjl = t), t = 1, . . . , mc+1, where ni;cjl is the observed frequency
of category l for feature j in item i. Thus, there is only a single value l(i)
j , such
that ni;cjl(i)
j
= 1, and ni;cjl(i)
j
= 0, otherwise. Consequently, we can rewrite ˜ρcjt(i)
as
˜ρcjt(i) =
∞

l=1
I(mcjl +ni;cjl = t) = ρcjt −I(mcjl(i)
j
= t)+I(mcjl(i)
j
= t−1). (16)
It further follows that, if mcjl(i) > 0, there is always a single frequency t(i)
j , such
that mcjl(i) = t(i)
j
and ˜ρcjt(i) = ρcjt for all t ̸= t(i)
j
and t ̸= t(i)
j
+ 1. In cases
where mcjl(i) = 0, item i carries a unique value for feature j and the updating
of suﬃcient statistics behaves slightly diﬀerently in the following two cases. If
no other unique values are present in class c for this feature, then ˜ρcjt(i) = ρcjt
for all t > 1 and ˜ρcj1(i) = 1. If there are multiple unique values present in class
c for this feature, then ˜ρcjt(i) = ρcjt for all t > 1 and ˜ρcj1(i) = ρcj1 + 1. In order
to avoid burdening the notation excessively, we will not explicitly consider this
distinction in the lemma below. Note that the above deﬁnition of ˜ρcjt(i) still
holds in all these diﬀerent cases.
To compare the predictive probabilities of simultaneous and marginal classi-
ﬁers, we now consider the case with S = ˚S. The following lemma shows how
the two probabilities are related to each other when the amount of training data
increases.
Lemma 1. Asymptotic behavior of the diﬀerence of log predictive probabilities
for simultaneous and marginal classiﬁers under partition exchangeability. Let mc
be very large for all c = 1, ..., k. Then
log p(x(N)|z(M), S, T ) −log p(x(N)|z(M), ˚S, T )
≈
k

c=1
d

j=1
mc+nc

t=1
˜ρcjt log(ψ
t ) −
mc

t=1
ρcjt log(ψ
t ) +

i∈sc
log(
t(i)
j
+ 1
t(i)
j
)
−
mc+nc

t=1
log(˜ρcjt!) +
mc

t=1
log(ρcjt!) −

i∈sc
log(ρcjt(i)
j ) +

i∈sc
log(ρcj(t(i)
j
+1))

.

Inductive Inference and Partition Exchangeability in Classiﬁcation
99
Proof. The logarithm log p(x(N)|z(M), S, T ) of the predictive probability
equals
k

c=1
d

j=1
{log[(mc + nc)!] −log(mc!) −log[(ψ + mc)(ψ + mc + 1) · · · (ψ + mc + nc −1)]
+
mc+nc

t=1
˜ρcjt log(ψ
t ) −
mc

t=1
ρcjt log(ψ
t ) −
mc+nc

t=1
log(˜ρcjt!) +
mc

t=1
log(ρcjt!)}.
Correspondingly, the logarithm for the marginal classiﬁer is
log p(x(N)|z(M), ˚S, T ) =
k

c=1

i∈sc
d

j=1
{log[(mc + 1)!] −log(mc!) −log(ψ + mc)
+
mc+1

t=1
˜ρcjt(i) log(ψ
t ) −
mc

t=1
ρcjt log(ψ
t ) −
mc+1

t=1
log(˜ρcjt(i)!) +
mc

t=1
log(ρcjt!)

.
To arrive at the stated result, we will ﬁrst show that the diﬀerences of the
foremost terms in the two expressions of the log probabilities tend to 0 as mc
increases. To start, recall the Ramanujan equation for log factorial, which is
given by
log(a!) = a log a −a + 1
6 log{a + 4a2 + 8a3 + ϵ(a)/30} + 1
2 log π,
where the error term ϵ(a) is bounded as 3/10 < ϵ(a) < 1 and ϵ(a) →1 when
a →∞. It then follows that
log[(mc + nc)!] −log(mc)! −

i∈sc
log[(mc + 1)!] +

i∈sc
log(mc!)
≈(mc + nc) log(mc + nc) −(mc + nc) + 1
6 log[(mc + nc) + 4(mc + nc)2 + 8(mc + nc)3]
−mc log(mc)+mc−1
6 log(mc + 4mc
2 + 8mc
3) + ncmc log(mc) −ncmc+ nc
6 log(mc + 4mc
2+8mc
3)
−nc(mc + 1) log(mc + 1) + nc(mc + 1) −nc
6 log[(mc + 1) + 4(mc + 1)2 + 8(mc + 1)3]
= mc log(1 + nc
mc
) −mcnc log(1 +
1
mc
) + nc log(1 + nc −1
mc + 1 )
+ 1
6 log[ (mc + nc) + 4(mc + nc)2 + 8(mc + nc)3
mc + 4mc + 8mc
] −nc
6 log[ (mc + 1) + 4(mc + 1)2 + 8(mc + 1)3
mc + 4mc2 + 8mc3
].
Using the standard series expansion log(1 + y) = y −1
2y2 + 1
3y3 −· · · , the result
above can be further simpliﬁed to the expression
log[(mc + nc)!] −log(mc!) −

i∈sc
log[(mc + 1)!] +

i∈sc
log(mc!)
= mc[ nc
mc
−1
2 ( nc
mc
)2 + · · · ] −mcnc[ 1
mc
−1
2 ( 1
mc
)2 + · · · ] + nc[ nc −1
mc + 1 −1
2 ( nc −1
mc + 1 )2 + · · · ]
+ 1
6 log{ (mc + nc) + 4(mc + nc)2 + 8(mc + nc)3
mc + 4mc2 + 8mc3
}
−nc
6 log{ (mc + 1) + 4(mc + 1)2 + 8(mc + 1)3
mc + 4mc2 + 8mc3
},

100
J. Corander, Y. Cui, and T. Koski
which tends to 0 when mc increases. For the diﬀerence of the intermediate terms
in the log predictive probabilities a similar convergence result holds such that
−log[(ψ + mc)(ψ + mc + 1) · · · (ψcj + mc + nc −1)] +

i∈sc
log(ψ + mc)
= log
(ψ + mc)nc
(ψ + mc)(ψ + mc + 1) · · · (ψ + mc + nc −1)
→
mc→∞0.
Note that the order of the error terms is not explicitly included in the above
derivations to avoid too extensive and tedious expressions. To establish the re-
maining parts of the lemma, note ﬁrst that ˜ρcjt(i)
j
(i) = ρcjt(i)
j −1 and ˜ρcj(t(i)
j
+1)
(i) =
ρcj(t(i)
j
+1) + 1. Then, we obtain that mc+1
t=1
˜ρcjt(i) log( ψ
t ) equals
mc

t=1
ρcjt log(ψ
t ) −ρcjt(i)
j log( ψ
t(i)
j
) −ρcj(t(i)
j
+1) log(
ψ
t(i)
j
+ 1
)
+˜ρcjt(i)
j
(i) log( ψ
t(i)
j
) + ˜ρcj(t(i)
j
+1)
(i) log(
ψ
t(i)
j
+ 1
)
=
mc

t=1
ρcjt log(ψ
t ) −log( ψ
t(i)
j
) + log(
ψ
t(i)
j
+ 1
) =
mc

t=1
ρcjt log(ψ
t ) −log(
t(i)
j
+ 1
t(i)
j
).
By using the same relationships between the original and updated suﬃcient
statistics, we have that mc+1
t=1
log(˜ρcjt(i)!) equals
mc

t=1
log(ρcjt!) −log(ρcjt(i)
j !) −log(ρcj(t(i)
j
+1)!) + log(˜ρcjt(i)
j
(i)!) + log(˜ρcj(t(i)
j
+1)
(i)!)
=
mc

t=1
log(ρcjt!) −log(ρcjt(i)
j ) + log(ρcj(t(i)
j
+1) + 1).
Using these two results, the stated lemma follows after some tedious re-
arrangements of the terms.
■
The result in the above lemma highlights the diﬀerence between the simultane-
ous and marginal classiﬁers in terms of inductive reasoning. In the simultaneous
classiﬁer the log predictive probabilities contrast the changes of suﬃcient statis-
tics between the training data and the entire population of test items as a whole.
Instead, the marginal classiﬁer sums the changes induced by each test item sep-
arately and therefore, it by deﬁnition never pays attention to how surprising any
observed event is in relation to the events observed for the remaining population
of test items. The following theorem shows explicitly that this diﬀerence can
persist even in the presence of increasing amounts of training data.
Theorem 1. Convergence of log predictive probabilities for simultaneous and
marginal classiﬁers under partition exchangeability. For any m ∈Z+, and any
mc ≥m, c = 1, ..., k, there exists ϵ > 0 such that
P[| log p(x(N)|z(M), S, T ) −log p(x(N)|z(M), ˚S, T )| > ϵ] > 0.

Inductive Inference and Partition Exchangeability in Classiﬁcation
101
Proof. To arrive at the stated result, it is necessary to show that a positive
probability remains for the event that the two classiﬁers disagree, despite of the
amount of training data present. To demonstrate this, we will consider log ratios
of the predictive probabilities under two particular classiﬁcation structures and
show that their diﬀerence need not converge to zero. Let S = ˚S be a labeling of
n test items and S′ the labeling derived from S by re-assigning a single item i
from class c1 to class c2. Let n′
c1 = nc1 −1, n′
c2 = nc2 + 1, and let ˜ρ′
c1jt, ˜ρ′
c2jt
denote the updated suﬃcient statistics under S′. Since the two labelings S, S′
are identical apart from the classes c1, c2, log ratio of the predictive probabilities
log p(x(N)|z(M), S, T ) −log p(x(N)|z(M), S′, T ) under the simultaneous classiﬁer
can be written as
log p(x(sc1), z(M)|S, T )+ log p(x(sc2 ), z(M)|S, T )−log p(x(sc1), z(M)|S′, T )−log p(x(sc2 ), z(M)|S′, T )
=
d

j=1
⎧
⎨
⎩
⎡
⎣log
(mc1 + nc1)!
ψ(ψ + 1) · · · (ψ + mc1 + nc1 −1) +
mc1 +nc1

t=1
˜ρc1jt log(ψ
t ) +
mc1 +nc1

t=1
log
1
˜ρc1jt!
⎤
⎦
+
⎡
⎣log
(mc2 + nc2)!
ψ(ψ + 1) · · · (ψ + mc2 + nc2 −1) +
mc2 +nc2

t=1
˜ρc2jt log(ψ
t ) +
mc2 +nc2

t=1
log
1
˜ρc2jt!
⎤
⎦
−
⎡
⎣log
(mc1 + n′
c1)!
ψ(ψ + 1) · · · (ψ + mc1 + n′c1 −1) +
mc1 +n′
c1

t=1
˜ρ′
c1jt log(ψ
t ) +
mc1 +n′
c1

t=1
log
1
˜ρ′
c1jt!
⎤
⎦
−
⎡
⎣log
(mc2 + n′
c2)!
ψ(ψ + 1) · · · (ψ + mc2 + n′
c2 −1) +
mc2 +n′
c2

t=1
˜ρ′
c2jt log(ψ
t ) +
mc2+n′
c2

t=1
log
1
˜ρ′
c2jt!
⎤
⎦
⎫
⎬
⎭.
Given the simple relationships between the suﬃcient statistics, the above ex-
pression can be further simpliﬁed to
d

j=1
{log(mc1 + nc1) −log(mc2 + nc2 + 1) −log(ψ + mc1 + nc1 −1) + log(ψ + mc2 + nc2)
+
⎡
⎣
mc1+nc1

t=1
˜ρc1jt log(ψ
t ) −
mc1+n′
c1

t=1
˜ρ′
c1jt log(ψ
t )
⎤
⎦+
⎡
⎣
mc2+nc2

t=1
˜ρc2jt log(ψ
t ) −
mc2 +n′
c2

t=1
˜ρ′
c2jt log(ψ
t )
⎤
⎦
−
⎡
⎣
mc1+nc1

t=1
log(˜ρc1jt!) −
mc1+n′
c1

t=1
log(˜ρ′
c1jt!)
⎤
⎦−
⎡
⎣
mc2+nc2

t=1
log(˜ρc2jt!) −
mc2 +n′
c2

t=1
log(˜ρ′
c2jt!)
⎤
⎦
⎫
⎬
⎭.
Assume now that item i carries a unique value for feature j and no other unique
values are present in classes c1, c2 for this feature. This event has a strictly
positive probability [10]. Then, the vectors of suﬃcient statistics ˜ρc1jt, ˜ρ′
c1jt are
identical apart from the ﬁrst element, such that ˜ρc1j1 = 1, ˜ρ′
c1j1 = 0. Conse-
quently, the diﬀerence in the ﬁrst square bracket equals log(ψ). Similarly, we
obtain for the second square bracket: ˜ρc2j1 = 0, ˜ρ′
c2j1 = 1, and thus, the diﬀer-
ence equals −log(ψ). Using a similar argument, diﬀerences in both remaining
two square brackets equal zero. Finally, with an increasing mc the log ratio of
predictive probabilities then converges to zero under the simultaneous classiﬁer.

102
J. Corander, Y. Cui, and T. Koski
This can be interpreted as an intuitive result, since the surprising event observed
for item i is equally surprising in both populations represented by sc1, sc2 and
the corresponding training data sets.
Consider now the log ratio of predictive probabilities under the marginal clas-
siﬁer. Since the predictive probabilities of all items apart from i are identical,
the log ratio simpliﬁes to log p(xi, z(M)|i ∈sc1, T ) −log p(xi, z(M)|i ∈sc2, T )
which equals
d

j=1
⎧
⎨
⎩
⎡
⎣log
(mc1 + 1)!
ψ(ψ + 1) · · · (ψ + mc1 + 1 −1) +
mc1 +1

t=1
˜ρc1jt(i) log(ψ
t ) −
mc1 +1

t=1
log(˜ρc1jt(i)!)
⎤
⎦
−
⎡
⎣log
(mc2 + 1)!
ψ(ψ + 1) · · · (ψ + mc2 + 1 −1) +
mc2 +1

t=1
˜ρc2jt(i) log(ψ
t ) −
mc2 +1

t=1
log(˜ρc2jt(i)!)
⎤
⎦
⎫
⎬
⎭.
As for the simultaneous classiﬁer, diﬀerence of the foremost terms not involving
the suﬃcient statistics converges to zero when mc increases. However, here the
diﬀerence of the latter terms can be arbitrarily large, as it depends basically on
the diﬀerence between the suﬃcient statistics derived from the training data for
the two classes. Thus, there is a positive probability for the event that the two
classiﬁers disagree, even if the diﬀerence of log ratios would converge to zero for
the remaining d −1 features.
■
4
Discussion
How should a classiﬁer reason when an unpredictable event is observed, and
more importantly, how could a classiﬁer be built when one does not know what
can be anticipated in the future? Standard Bayesian inductive inference does
not easily provide the tools necessary for coherently updating the conditional
probabilities of events when we see values that were not previously seen or even
anticipated. Retrospectively changing an alphabet upon arrival of new obser-
vations can of course be done, but this is an ad hoc approach not based on a
probabilistic framework which inductively updates its predictions about the fu-
ture in an autonomous manner. A truly inductive approach to prediction of a
single future feature value using a hierarchical Dirichlet prior was introduced in
[18], however, in a context diﬀerent from classiﬁcation.
We have demonstrated that an inductive supervised classiﬁcation principle
in the presence of unpredictable or unanticipated events corresponding to un-
known alphabets of observable features arises under the assumption of partition
exchangeability for data from a categorical feature. Moreover, joint modeling of
the labels of a population of test items appears as a natural approach, since it
is capable of assessing the degree of surprise of an observed event for a test item
in relation to both the other test items and the training data.
The basic rationale behind the use of joint classiﬁcation models under de
Finetti type exchangeability stems from the fact that item labels perceived as
random quantities remain dependent even under an i.i.d. sampling model, as long

Inductive Inference and Partition Exchangeability in Classiﬁcation
103
as the generating probability measures for the classes are not exactly known. In
contrast to a marginal classiﬁer which predicts the label for each item sepa-
rately based on an i.i.d. assumption, i.e. conditionally independent feature data
given a ﬁxed generating measure learned from the training items, a joint, or
simultaneous classiﬁer exploits the dependence which may have a considerable
eﬀect in the presence of sparse training data. A marginalized predictive classiﬁer
takes an additional step further for any single item by treating the labels of
remaining items as nuisance parameters and deriving the predictive distribution
of a label through its marginal posterior from the joint distribution of all the
labels.
A concrete application of a classiﬁer based on exchangeable partitions arises
for instance when categories of features cannot be directly observed, but only
their frequencies within a pool of items, which represents either training or test
data. An example of such a problem is classiﬁcation of documents with identical
or partially identical content which are encrypted with diﬀerent keys. In fact,
some central elements of the probability calculus related to the frequency dis-
tributions of words identiﬁed as suﬃcient statistics were already recognized by
Alan Turing and Irving Good as a part of the decryption activities at Bletchley
Park during World War II as noted in [10].
As mentioned earlier, gains in classiﬁcation accuracy are accessible through
joint modeling of multiple test items and the optimality of such an approach
has been considered by multiple authors [21-24] in diﬀerent classiﬁcation con-
texts. An interesting issue for future work would be to consider instead a log
loss scoring as suggested in [33],[34]. Another line of future development would
be to examine how the models in [18],[20] would behave asymptotically for a
simultaneous prediction of a population of test items when supervised by an
inﬁnite amount of training data. It is currently an open question whether their
asymptotic predictions would show similar characteristics as the predictions de-
rived here under partition exchangeability, or if they would be reducible to a set
of marginal predictions as under de Finetti type exchangeability.
References
1. Solomonoﬀ, R.J.: A formal theory of inductive inference. Part I. Inf. Control 7,
1–22 (1964)
2. Hunt, E.B., Marin, J., Stone, P.J.: Experiments in induction. Academic Press, New
York (1966)
3. Chow, C.K., Liu, C.N.: Approximating discrete probability distributions with de-
pendence trees. IEEE Trans. Inf. Theory 14, 462–467 (1968)
4. Bailey, N.T.J.: Probability methods of diagnosis based on small samples. In: Math-
ematics and Computer Science in Biology and Medicine. H.M. Stationery Oﬃce,
London (1965)
5. Jain, A.K., Duin, R.P.W., Mao, J.: Statistical pattern recognition: A review. IEEE
Trans. Patt. Anal. Mach. Intell. 22, 4–37 (2000)
6. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern classiﬁcation, 2nd edn. Wiley, New
York (2000)

104
J. Corander, Y. Cui, and T. Koski
7. Bishop, C.M.: Pattern recognition and machine learning. Springer, New York
(2007)
8. Hand, D.J., Yu, K.: Idiot’s Bayes: not so stupid after all. Int. Stat. Rev. 69, 385–398
(2001)
9. Jeﬀrey, R.: Probabilism and induction. Topoi 5, 51–58 (1986)
10. Zabell, S.L.: Predicting the unpredictable. Synthese 90, 205–232 (1992)
11. Angluin, D., Smith, C.H.: Inductive inference: Theory and methods. ACM Comput.
Surv. 15, 237–268 (1983)
12. Michalski, R.S.: A theory and methodology of inductive learning. Artif. Intell. 20,
111–161 (1983)
13. Solomonoﬀ, R.J.: Three kinds of probabilistic induction: universal distributions
and convergence theorems. Christopher Stewart WALLACE (1933-2004); Memorial
Special Issue. Comput. J. 51, 566–570 (2008)
14. Kingman, J.F.C.: The population structure associated with the Ewens sampling
formula. Theor. Pop. Biol. 11, 274–283 (1977)
15. Kingman, J.F.C.: The representation of partition structures. J. London Math.
Soc. 18, 374–380 (1978)
16. Kingman, J.F.C.: Random partitions in population genetics. Proc. Roy. Soc. A 361,
1–20 (1978)
17. Kingman, J.F.C.: Uses of exchangeability. Ann. Probab. 6, 183–197 (1978)
18. Friedman, N., Singer, Y.: Eﬃcient Bayesian parameter estimation in large discrete
domains. In: Kearns, M.J., Solla, S.A., Cohn, D.A. (eds.) Advances in Neural In-
formation Processing Systems, vol. 11, pp. 417–423 (1998)
19. Orlitsky, A., Santhanam, N.P., Zhang, J.: Universal compression of memoryless
sources over unknown alphabets. IEEE Trans. Inf. Theory 50, 1469–1481 (2004)
20. Wang, C., Blei, D.M.: Decoupling sparsity and smoothness in the discrete hier-
archical Dirichlet process. In: Bengio, Y., Schuurmans, D., Laﬀerty, J., Williams,
C.K.I., Culotta, A. (eds.) Advances in Neural Information Processing Systems,
vol. 22, pp. 1982–1989 (2009)
21. Corander, J., Cui, Y., Koski, T., Sir´en, J.: Have I seen you before? Principles of
predictive classiﬁcation revisited. Stat. Comput. 23, 59–73 (2013)
22. Cui, Y., Corander, J., Koski, T., Sir´en, J.: Predictive Gaussian classiﬁers. Submit-
ted to Bayesian Analysis (2013)
23. Ripley, B.D.: Pattern Recognition and Neural Networks. Cambridge University
Press, Cambridge (1996)
24. N´adas, A.: Optimal solution of a training problem in speech recognition. IEEE
Transactions on Acoustics, Speech, and Signal Processing 33, 326–329 (1985)
25. Geisser, S.: Predictive discrimination. In: Krishnajah, P.R. (ed.) Multivariate anal-
ysis, pp. 149–163. Academic Press, New York (1966)
26. Geisser, S.: Predictive Inference: An introduction. Chapman & Hall, London (1993)
27. Dawson, K.J., Belkhir, K.: A Bayesian approach to the identiﬁcation of panmictic
populations and the assignment of individuals. Genet. Res. 78, 59–77 (2001)
28. Corander, J., Gyllenberg, M., Koski, T.: Random partition models and exchange-
ability for Bayesian identiﬁcation of population structure. Bull. Math. Biol. 69,
797–815 (2007)
29. Jackson, M.O., Kalai, E., Smorodinsky, R.: Bayesian representation of stochastic
processes under learning: de Finetti revisited. Econometrics 67, 875–893 (1999)
30. Solomonoﬀ, R.J.: Complexity-based induction systems: comparisons and conver-
gence theorems. IEEE Trans. Inf. Theory 24, 422–432 (1978)

Inductive Inference and Partition Exchangeability in Classiﬁcation
105
31. Blackwell, D., Dubins, L.: Merging of opinions with increasing information. Ann.
Math. Stat. 33, 882–886 (1962)
32. Joyce, P.: Partition Structures and suﬃcient statistics J. Appl. Prob. 35, 622–632
(1998)
33. Dowe, D.L.: Foreword re C. S. Wallace. Christopher Stewart WALLACE (1933-
2004); Memorial Special Issue. Comput. J. 51, 523–560 (2008)
34. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Bandyopadhyay, P.S., Forster, M.R. (eds.)
Handbook of the Philosophy of Science - Philosophy of Statistics, pp. 901–982.
Elsevier, Oxford (2011)

Learning in the Limit:
A Mutational and Adaptive Approach
Reginaldo Inojosa da Silva Filho, Ricardo Luis de Azevedo da Rocha,
and Ricardo Henrique Gracini Guiraldelli
Computing Engineering Department
Engineering School of the University of S˜ao Paulo,
Av. Luciano Gualberto s/n Trav 3, n.158 S˜ao Paulo - SP, Brazil
reginaldo.uspoli@gmail.com, {rlarocha,rguira}@usp.br
Abstract. The purpose of this work is to show the strong connection
between learning in the limit and the second-order adaptive automaton.
The connection is established using the mutating programs approach,
in which any hypothesis can be used to start a learning process, and
produces a correct ﬁnal model following a step-by-step transformation
of that hypothesis by a second-order adaptive automaton. Second-order
adaptive automaton learner will be proved to acts as a learning in the
limit one1.
Keywords: Inductive Inference, Learning Model, Code
Mutation,
Adaptive Devices, Automata Theory.
1
Introduction
Ray Solomonoﬀwas the father of the general theory of inductive inference [11], a
fruitful area of study that generated many developments in artiﬁcial intelligence
[5,13]. In short, the goal of inductive inference is to identify the unknown object
by picking out one of a (typically inﬁnite) set of hypothesis for this object [1]. The
hypothesis is a ﬁnite representation of the object and may be consistent with the
given incrementally growing segments of object example inputs. It is possible to
deﬁne many ways to the hypothesis choice and each one, in practice, determines
a whole new learning model; the main ones are the probabilistic approach [5,13]
and the enumerations strategies [4]. But, as pointed out by Wallace, Dowe and
Solomonoﬀhimself, the so-called “SolomonoﬀInduction” is actually prediction
[13,2,12], rather than induction. Therefore, the approach used in this work is
closely related to Wallace’s Minimum Message Length (MML) approach, but
was inspired by Solomonoﬀ’s paper [11].
Thus, consider the following constraint: what if the only way to generate a
new hypothesis was by “recycling” a former one? What would the behavior of
the learner be if the generation of a new hypothesis implies the transformation
of an older one? What kind of transformation would be necessary?
1 The work reported here received support through FAPESP grant 2010/09586-0.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 106–118, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Learning in the Limit: A Mutational and Adaptive Approach
107
This kind of hypothesis needs to be a “changeable” one to be reused, that is,
it would have a “plasticity” feature to adapt to new inputs for which it is not
prepared; therefore, the changes must happen in the representation structure
used to describe the hypothesis. Using a biological metaphor, the hypothesis
must have the mutational property, with the learner having the responsibility
to apply transformations in the hypothesis. The use of this kind of metaphor,
nonetheless, is not new: Solomonoﬀwas also interested in the study of mutating
programs [6]. Now, this work proposes a mutational computational model, called
second-order adaptive automaton, aimed at the problem of inductive inference
and in which this “changeable” behavior is an essential property of the model
itself.
A successful mutation means that the learner has adapted to the new inputs.
The fact that learning can be represented by an adaptive process is the funda-
mental premise of this work. The last two decades presented the development and
emergence of new computational models that deviate from a basis on mechanical
machines structures and become similar to evolutive, collaborative and biologi-
cally inspired models; among these, self-modifying devices [8,10] are prominent
ones, which have been developed under a formalism based on automata theory
and is one of the basis to represent the adaptive behavior of the model deﬁned
here.
The description of the inductive inference under an adaptive aspect will be
made using the Emil Mark Gold learning in the limit model [3], also called identi-
ﬁcation in the limit. Responsible for branch of inductive inference, Gold studied
the learning problem for recursive functions and formal languages. In his model,
the inductive inference is an inﬁnite process; a learner identiﬁes a language if the
generation of hypotheses converges to one and no other changes occur, although
new inputs of the language are presented to the learner indeﬁnitely.
The text is organized as follows: section 2 describes the notation and technical
preliminaries concerning automata theory and ﬁrst-order adaptive automata,
basis for the second-order adaptive automata; section 3 presents the the second-
order adaptive automata. Section 4 shows the relationship between second-order
adaptive automata and learning in the limit. Finally, section 5 presents the
conclusion and further works to be developed.
2
The First-Order Adaptive Automaton
Adaptive automaton belongs to the category of self-modifying devices. It is
a computational model equivalent to Turing Machine [7] and has the non-
deterministic ﬁnite state automaton as formulation basis. Its major characteristic
is the ability to decide how to modify its own structure in response to some exter-
nal input, without the interference of any external agent. The ﬁrst appearance of
the adaptive automata has some inconsistencies in its description, but this fact
was corrected later in a complete formalization, performed using the automata
transformations concept [10]. This formalization, developed in the present sec-
tion, is known as ﬁrst-order adaptive automata (FOAA). However, some

108
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
introductory concepts are presented before for unequivocal understanding of the
FOAA deﬁnition.
2.1
Notations and Technical Preliminaries
The main concepts used in this work, mostly concerning automata theory, as
well the pertinent notation, are summarized in table 1.
Table 1. Notation for technical preliminaries related to the automata theory
N = {0, 1, 2, . . . }
The set of natural numbers
I = {i0, i1, . . . , im}
Finite arbitrary indexed set
rem(I, x) = {I −{x} : x ∈I} with (x /∈I)
Removal function
ins(I, x) = {i0, i1, . . . , im+1} with im+1 = x
Insertion function
Σ
An alphabet of symbols
α ∈Σ
A symbol of the alphabet
L ⊆Σ∗
A language over Σ
t ∈L
A string of
L
ε
An empty string
θ = (t0, t1, ...), with tk ∈L for k ∈N
A text of
L
θ[n] = (t1, t2, ...tn), with tk ∈L for 0 ≤k ≤n the n-initial segment of θ
seq(θ)
the family of all segments
M 0 = (Q, q0, E, Σ, ∂)
A non-deterministic automaton
Q = {q1, . . . , qn}
The set of states of
M 0
q0 ∈Q
The initial state of
M 0
E ⊆Q
The accepting states set of
M 0
∂⊆Q × {Σ ∪{ε}} × Q
The state transition relation
∂= {δ1, δ2, . . . , δi}
The transitions set of M 0
δ = (q′, α, q′′) with {q′, q′′} ⊆Q and α ∈Σ
A transition of
M 0
(q′, t) ∈Q × Σ∗with q′ ∈Q
A conﬁguration of
M 0
(q0, t)
The initial conﬁguration of M 0
A scalar hierarchical structure[9] is indicated by ⟨an ⟨an−1 . . . ⟨a1 ⟨a0⟩⟩⟩⟩,
and is interpreted as follows: if ai+1 is a formal system deﬁned by an ordered
n-tuple, then ai is a n-tuple element, for 0 ≤i ≤(n −1).
2.2
Automata Transformations
Given the non-numerical set M 0 of all non-deterministic ﬁnite state automaton
under an alphabet Σ, for any element M 0 ∈M 0 and the state transition relation
∂of M 0, a proper transition is deﬁned as:
δpro = δ : δ ∈∂
(1)
Otherwise, a foreign transition is deﬁned as:
δfor = δ : δ /∈∂
(2)

Learning in the Limit: A Mutational and Adaptive Approach
109
A sequence of proper transitions belong to M 0 and represented by
λpro = (δpro1, . . . , δprom)
(3)
is called a positive sequence. In turn, a transitions sequence
λfor = (δfor1, . . . , δforn)
(4)
of foreign transition for M 0 is called a negative sequence.
Given a negative and a positive sequence for an automaton M 0, the sequence:
φ = (λfor, λpro)
(5)
is deﬁned as a ﬁrst-order transformation pair.
Employing the proper and foreign transition concepts, as well the deﬁnition
of removal and insertion functions, it is possible to deﬁne transformation func-
tions for all members of M 0. Thus, the δ-removal operation and δ-insertion
operation are deﬁned, respectively, by:
f −
k M 0 = f −(δprok, M 0) = (Q, q0, E, Σ, rem(∂, δprok))
(6)
f +
k M 0 = f +(δfork, M 0) = (ins(ins(Q, q′), q′′), q0, E, Σ, ins(∂, δfork))
(7)
with δprok ∈λpro and δfork ∈λfor.
Now, using this two operators, it is possible to introduce the concept of ﬁrst-
order adaptive function:
FφM 0 ≜F(φ, M 0) = F −
λproF +
λforM 0
(8)
in which
F −
λproM 0 ≜F −(λpro, M 0) = (f −
m ◦f −
m−1 ◦. . . ◦f −
2 ◦f −
1 )M 0
(9)
F +
λforM 0 ≜F +(λfor, M 0) = (f +
n ◦f +
n−1 ◦. . . ◦f +
2 ◦f +
1 )M 0
(10)
are the ﬁrst-order removal transformation and ﬁrst-order insertion
transformation.
Deﬁnition 1 (First-order Adaptive Automata). A First-Order Adaptive
Automata (FOAA) is the quadruple M 1 = (M 0, Φ, φ∅, ∂1), in which M 0 ∈M 0
is called ﬁrst-order subjacent device. Set Φ of the ﬁrst-order transformation
pairs is called adaptive behavior set. The element φ∅∈Φ is a void transfor-
mation pair called null behavior. Set ∂1 is the ﬁrst-order adaptive transi-
tion relation. Each element of ∂1 takes the form δ1
i,k = (δi, ⟨M 1⟨FφkM 0⟩⟩), for

110
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
φk ∈Φ and δi ∈∂in which ∂is the ﬁrst-order subjacent device state-transition
relation.
Any extension of the automaton concept implies a new expression for it.
Hence, the traditional elements of the automata theory (step function, etc.)
were brought into the FOAA model.
The one step function shows how the FOAA changes from one conﬁguration
to another:
(q′, t) ⊢[FφkM0] (q′′, w) ⇔∃α ∈Σ : αw = t
(11)
in which q′′ is a state of FφkM 0 and ((q′, α, q′′), ⟨M 1⟨Fφk(M 0)⟩⟩) ∈∂1 for φk ∈Φ.
The closure of the one step function for a FOAA is deﬁned as:
(q′, t) ⊢∗
[Fφkj ...Fφk2 Fφk1 M0] (q′′, w)
(12)
iﬀ(q′ = q′′) and (w = t) or rules 1, 2 and 3 are all satisﬁed as deﬁned below:
1. t = a0a1 . . . ajw with ai ∈Σ for 0 ≤i ≤j
2. ∃(φk1, φk2, . . . φkj+1) with φki ∈Φ for 1 ≤i ≤j
3. ∃p1, p2 . . . pj ∈Q in which Q belongs to ﬁrst-order subjacent device, such
that,
for j ∈N:
(q′, t) ⊢[Fφk1 M0] (p1, a1a2a3 . . . ajw)
⊢[Fφk2 Fφk1 M0] (p2, a2a3 . . . ajw) ⊢[Fφk3 Fφk2 Fφk1 M0] . . .
⊢[Fφkj ...Fφk2 Fφk1 M0] (pj, ajw)
⊢[Fφkj+1 Fφkj ...Fφk2 Fφk1 M0] (q′′, w)
The language recognized by the FOAA is
L(M 1) = {t : (q0, t) ⊢∗
[Fφkj ...Fφk2 Fφk1 M0] (qf, ε)}
(13)
Special case in which the behavior set is Φ = {φ∅}, the necessary condition
for a string to be accepted by a FOAA assuming the form:
(q0, t) ⊢∗
[Fφ∅...Fφ∅Fφ∅M0] (qf, ε) = (q0, w) ⊢∗
M0 (qf, ε)
3
The Second-Order Adaptive Automaton
Now, taking the set M 1 of all ﬁrst-order adaptive automata M 1 = (M 0, Φ, φ∅, ∂1)
for a ﬁxed Σ and applying the same method used for the set M 0 (the deﬁnition
of basic insertion and removal operations for FOAAs transitions), analogous to
what occurred in the previous section, it is possible to obtain similar concepts,

Learning in the Limit: A Mutational and Adaptive Approach
111
Table 2. Set of operations that structures the SOAA
δ1
pro = δ1 : δ1 ∈∂1
δ1-proper transition
δ1
for = δ1 : δ1 /∈∂1
δ1-foreign transition
λ1
pro = (δ1
pro1, . . . , δ1
prom)
δ1-positive sequence
λ1
for = (δ1
for1, . . . , δ1
form)
δ1-negative sequence
ψ ≜(λ1
for, λ1
pro)
δ1-transformation pair
g−
k M 1 = g−(δ1
prok, M 1) = (M 0, Φ, φ∅, rem(∂1, δ1))
δ1-removal operation
g+
k M 1 = g+(δ1
fork, M 1) = (M 0, ins(Φ, φ), φ∅, ins(∂1, δ1)) δ1-insertion operation
now to study the ﬁrst-order adaptive automata set features under a set of opera-
tors. Therefore, the table below summarizes these concepts and their deﬁnitions:
The second-order adaptive function is the operator
Gψ ≜G(ψ, M 1) = G−
λ1proG+
λ1
forM 1
(14)
in which
G−
λ1proM 1 ≜G−(λ1
pro, M 1) = (g−
m ◦g−
m−1 ◦. . . ◦g−
2 ◦g−
1 )M 1
(15)
G+
λ1
forM 1 ≜G+(λ1
for, M 1) = (g+
n ◦g+
n−1 ◦. . . ◦g+
2 ◦g+
1 )M 1
(16)
are the second-order removal transformation and second-order insertion
transformation, respectively. Similar with the ﬁrst-order case, the pair ψ∅is
called of void second-order characteristic pair. Thus, for an empty second-order
characteristic pair, Gψ∅M 1, it is equal to M 1.
Deﬁnition 2 (Second-Order Adaptive Automata). A Second-Order Adap-
tive Automata (SOAA) is the quadruple M 2 = (M 1, Ψ, ψ0, ∂2), in which M 1 ∈
M 1 is called second-order subjacent device. The set Ψ = {ψ0, ψ1, . . . , ψn}
of second-order transformation pairs is called second-order adaptive behav-
ior set. The element ψ0 is a void transformation pair called null behavior.
In the second-order adaptive transition relation ∂2, each element take the
form δ2
i,k,j = (δ1
i,k, ⟨M 2⟨GψjM 1⟩⟩), for ψj ∈Ψ and δ1
i,k ∈∂1, in which ∂1 is the
second-order subjacent device state-transition relation.
The one step function shows how the SOAA changes from one conﬁguration
to another and is deﬁned below:
(q′, t) ⊢[⟨Gψj M1⟨FφkM0⟩⟩] (q′′, w) ⇔∃α ∈Σ : αw = t
(17)
in which q′′ is a state of FφkM 0 for δi,k,j = (δi,k, ⟨M 2⟨GψjM 1⟩⟩) ∈∂2, δi,k =
(δi, ⟨M 1⟨FφkM 0⟩⟩) ∈∂1 and δi = (q′, α, q′′) ∈∂with φk ∈Φ and ψj ∈Ψ.
For any s ∈N, the closure of the one step function for a SOAA is
deﬁned as:
(q′, t) ⊢∗
[⟨Gψjs ...Gψj2 Gψj1 M1⟨Fφks ...Fφk2 Fφk1 M0⟩⟩] (q′′, w)
(18)

112
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
iﬀ(q′ = q′′) and (w = t) or rules 1 to 4 are all satisﬁed as deﬁned below:
1. t = a0a1 . . . asw with az ∈Σ for 0 ≤z ≤s
2. ∃(φk1, φk2, . . . φks+1) with φkz ∈Φ for 1 ≤z ≤s
3. ∃(ψj1, ψj2, . . . ψjs+1) with ψjz ∈Ψ for 1 ≤z ≤s
4. ∃p1, p2 . . . ps ∈Q with Q belongs to First-order subjacent device such that:
(q′, t) ⊢[⟨Gψj1 M1⟨Fφk1 M0⟩⟩] (p1, a1a2a3 . . . asw)
⊢[⟨Gψj2 Gψj1 M1⟨Fφk2 Fφk1 M0⟩] (p2, a2a3 . . . asw)
⊢[⟨Gψj3 Gψj2 Gψj1 ⟨Fφk3 Fφk2 Fφk1 M0⟩⟩] . . .
⊢[⟨Gψjs ...Gψj2 Gψj1 M1⟨Fφks ...Fφk2 Fφk1 M0⟩⟩] (ps, asw)
⊢[⟨Gψjs+1 Gψjs ...Gψj2 Gψj1 M1⟨Fφks+1 Fφks ...Fφk2 Fφk1 M0⟩⟩] (q′′, w)
The language recognized by the SOAA is:
L(M 2) = {t : (q0, t) ⊢∗
[⟨Gψjs ...Gψj2 Gψj1 M1⟨Fφkj ...Fφk2 Fφk1 M0⟩⟩] (qf, ε)}
(19)
In the special case in which the second-order behavior set is Ψ = {ψ∅}, the
necessary condition for a string to be accepted by a SOAA assumes the form:
(q0, t) ⊢∗
[⟨Gψ∅...Gψ∅Gψ∅M1⟨Fφkj ...Fφk2 Fφk1 M0⟩⟩] (qf, ε) =
(q0, t) ⊢∗
[Fφkj ...Fφk2 Fφk1 M0] (qf, ε)
In this case, the recognition of string t is made by the initial second-order sub-
jacent device. The SOAA assumes the behavior of the FOAA, which is equivalent
to the Turing Machine.
4
Second-Order Adaptive Automata and Learning
in the Limit
This section will show the advantage of using the SOAA as an identiﬁcation
in the limit Inductive Inference Machine for formal languages. By deﬁnition,
SOAA transforms FOAAs by applying on them second-order adaptive actions.
Now, it is necessary to demonstrate how this behavior can be used to “recycle”
the former hypothesis created in a learning in the limit process, as stated in the
Introduction, and what kind of formal languages a SOAA can learn in the limit.
Firstly, the main deﬁnitions related to the Gold identiﬁcation in the limit are
presented. Then, an illustrating example of learning in the limit using a SOAA
is presented in subsection 4.1.
Deﬁnition 3 (Inductive Inference Machine)
Let a target formal language indexable class L and a hypothesis set H com-
posed by an useful enumerable formal model class (grammars, Turing machines,
recursive functions, etc) to represent the members of the target languages class.
Given the family seq(θ) for θ ∈text(L), in which L belongs to L, an inductive
inference machine (IIM in short) is deﬁned as an eﬀective procedure in which
it computes any partial or total mapping IIM ⊆seq(θ) × H.

Learning in the Limit: A Mutational and Adaptive Approach
113
The IIM changes its mind if two consecutives output hypotheses are dif-
ferent, i.e., IIM(θ[m]) ̸= IIM(θ[m + 1]) for m ≥0.
The expression
IIM(θ) ↓= h ⇔∃(n ∈N)∃(h ∈H)(∀m ≥n)[IIM(θ[m])] = h
(20)
means that the inductive inference machine converges, i.e., the potential inﬁnite
sequence [IIM(θ[m])]m∈N of outputs converges on θ to h ∈H.
Deﬁnition 4 (Identiﬁcation in the Limit). Let L be an indexed family of
languages, given a convenient hypothesis space H. IIM Lim-identiﬁes L ⇔
∀(L ∈L)∃(h ∈H : L(h) = L)[IIM(θ) ↓= h].
The second-order adaptive function concept allows deriving deﬁnitions of lan-
guage classes based only on the SOAA characteristics. One of these classes is
shown below.
Deﬁnition 5 (Conﬁned Adaptive Problem). Given a FOAA M 1 and a
language L in which L ̸= L(M 1), let C∝= (Gψi, . . . , Gψj, . . . , Gψk) be a sequence
of second-order adaptive functions. If the language L can be expressed in terms
of M 1 and C∝as follows in equation 21:
L = L((Gψi . . . (Gψj . . . (Gψk(M 1)) . . . ) . . . ))
(21)
then L is called a conﬁned adaptive problem, sequence C∝is called meta-
morphosis sequence and M 1 is a seed for L.
Deﬁnition 6 (Linear Conﬁned Adaptive Problem Class)
Given a ﬁnite second-order adaptive functions set G, let the indexable class
LG = {Ln}n∈N of conﬁned adaptive problems, all based on the same seed L0,
in which the metamorphosis of Li is a subsequence of the Li+1 metamorphosis.
For all languages Li ∈LG, if all elements of the sequence C∝i, belongs to Li,
are elements of CG, then LG is called a Linear Conﬁned Adaptive Problem
Class and set G is called a mutation set.
Theorem 1. Given a Conﬁned Adaptive Problem L and any text θ of L, there
is a SOAA M 2 and a Natural number n > 0, for which:

L ̸= L(M 2)
for θ[n]
L = L(M 2)
for θ[n + 1]
Proof (by construction). Take a SOAA in which its subjacent device M 1 is a
seed for a Conﬁned Adaptive Problem L. Let Ψ be a behavior set in which all el-
ements of C∝are elements of Ψ, too. With the valid seed M 1 for L, it is possible
to deﬁne a second-order adaptive transition relation in which the computation
of text θ assumes the form:

114
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
(q0, t1) ⊢∗
[⟨Gψjs ...Gψj2 Gψj1 M1⟩] (q′, w′)
(q0, t2) ⊢∗
[⟨Gψjt ...Gψj2 Gψj1 (M1)′⟩] (q′′, w′′)
. . .
(q0, tn−1) ⊢∗
[⟨Gψju ...Gψj2 Gψj1 (M1)(n−2)⟩] (qn−1, wn−1)
(q0, tn) ⊢∗
[⟨Gψjw ...Gψj2 Gψj1 (M1)(n−1)⟩] (qn, wn)
(q0, tn+1) ⊢∗
[⟨Gψjp ...Gψj2 Gψj1 (M1)n⟩] (qf, ϵ)
(q0, tn+2) ⊢∗
[⟨Gψjv ...Gψj2 Gψj1 (M1)(n+1)⟩] (qf, ϵ)
. . .
(q0, tn+k) ⊢∗
[⟨Gψjl ...Gψj2 Gψj1 (M1)(n+(k−1))⟩] (qf, ϵ)
for q′, q′′, . . . , qn−1, qn diﬀerent from qf;
w′, w′′, . . . , wn−1, wn diﬀerent from
ϵ and
(M 1)′ = Gψjs . . . Gψj2 Gψj1 M 1
(M 1)′′ = Gψjt . . . Gψj2 Gψj1(M 1)′
. . .
(M 1)n−1 = Gψju . . . Gψj2Gψj1 (M 1)n−2
(M 1)n = Gψjw . . . Gψj2Gψj1 (M 1)n−1
(M 1)n+1 = Gψjp . . . Gψj2 Gψj1 (M 1)n
. . .
such that the execution of adaptive transitions generates the sequence of the
adaptive transformations below
L = L(Gψjp . . . Gψjw . . . Gψju . . . Gψjt . . . Gψjs . . . Gψj2 Gψj1M 1)
and, for any tn+k in which k > 1, the following expression holds
(M 1)(n+(k−1)) = (M 1)(n+1)
Thus, for n > 0, there is a SOAA for the Conﬁned Adaptive Problem such
that

L ̸= L(M 2)
for (t1, t2, . . . , tn)
L = L(M 2)
for (tn+1, . . . )
⊓⊔
Theorem 2. For m ∈N, and given a function P(M 2, θ[m]) = M 1 that returns
M 1, which is the subjacent device FOAA of the second-order M 2, then after
processing the segment text θ[m], there is a SOAA M 2 in which the function
P(M 2, .) is an IIM and P Lim-identiﬁes
any language Li of LC∝, for any
text θ belonging to Li.
Proof. As seen in theorem 1, for any Conﬁned Adaptive Problem L, it is possible
to construct a SOAA M 2 such that:

Learning in the Limit: A Mutational and Adaptive Approach
115

L ̸= L(M 2)
for θ[n]
L = L(M 2)
for θ[n + 1]
Thus, it is possible to claim that
∃(n ∈N)∃(M 1 ∈M 1 : L(M 1) = L)(∀m ≥n)[P(M 2, θ[m]) = M 1]
in other words,
∃(M 1 ∈M 1 : L(M 1) = L)[P(M 2, θ[m]) ↓= M 1]
meaning that the function P(M 2, .) Lim-identiﬁes L.
According to deﬁnition 6, the metamorphosis sequence of any Li (with i ≥0)
of LC∝is a subsequence of the metamorphosis sequence of Lz, with z ≥i. Thus,
using theorem 2, the following assertions holds:
for L0 ∃(P(M 2
0 , .) : P(M 2
0 , .) Lim-identiﬁes (L0)
for L1 ∃(P(M 2
1 , .) : P(M 2
1 , .) Lim-identiﬁes (L0, L1)
for L2 ∃(P(M 2
2 , .) : P(M 2
2 , .) Lim-identiﬁes (L0, L1, L2)
. . .
for Li ∃(P(M 2
i , .) : P(M 2
i , .) Lim-identiﬁes (L0, L1, L2, . . . , Li)
. . .
for Ln ∃(P(M 2
n, .) : P(M 2
n, .) Lim-identiﬁes LC∝
⊓⊔
An important consequence of Theorem 2 has an immediate impact on the
choice relation over the hypotheses space.
Corollary 1. Set M 1 is an admissible hypotheses space.
4.1
Illustrating Example
Let Σ = {a, b, c} be an alphabet, and t = abc a string over Σ∗. Based on the
string t, it is possible to deﬁne the class of formal languages below:
I = {L0 = anbnc, L1 = anbcn, L2 = abncn}
Now, consider the following situation: there is a text θ that belongs to an
unknown language X. The only information about language X is the fact that
it belongs to class I. The question is: would it be possible to obtain a SOAA
that identiﬁes the language X represented by sequence θ? If I is a Conﬁned
Adaptive Problem Class, then the response is yes. Thus, to answer the question,
it is necessary to verify whether I is a Conﬁned Adaptive Problem Class or not.
Proof (I is a Conﬁned Adaptive Problem Class).
The proof that I is a Conﬁned Adaptive Problem Class is lengthy. For space
limitation reasons, only the proof sketch will be given here. All elements of I can
surely be represented by FOAAs. Let M 1
0 represents the FOAA for the language

116
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
L0 of I. One possible adaptive function F0 for this FOAA is “for a number n of
symbols ‘a’ recognized, transform the M 0 to accept the same number of symbols
‘b’ in the string”.
The next step is to verify that all language members of I are Conﬁned Adap-
tive Problems. Thus, it is necessary to verify if there are metamorphosis se-
quences for L1 and L2. If there are such metamorphosis sequences, then the ﬁrst
sequence is
C∝2 = (G2, G1)
and it performs the transformation sequence below:
L2 = L((G2(G1(M 1
0 ))))
in which the two second-order adaptive functions G2 and G2 must have the
following characteristics:
– G1: Replace the second symbol of F0 with the symbol ‘c’ and transform
F0 in F1.
– G2: Replace the ﬁrst symbol of F1 with the symbol ‘b’ and transform
F1 in F2.
And the second sequence is
C∝1 = (G1)
that performs the transformation sequence below:
L1 = L(G1(M 1
0 ))
But sequence C∝1 is a subsequence of C∝2 and generates the language L1.
Considering that the second-order adaptive functions set used to create the trans-
formation sequences is ﬁnite, then it is a mutation set. Thus, I is a Conﬁned
Adaptive Problem.
⊓⊔
5
Conclusion
As stated in the Introduction it is possible to deﬁne many ways to the hypothesis
choice and each one, in practice, determines a whole new learning model; the
main ones are the probabilistic approach and the enumerations strategies. The
approach used in this work is closely related to Wallace’s Minimum Message
Length (MML) approach, but was inspired by Solomonoﬀ’s paper [11].
A strong connection between learning in the limit and the SOAA was shown
by Theorems 1 and 2. The connection is established using Solomonoﬀ’s approach
to mutating programs. The purpose is to represent a learning process using the
SOAA, and this learner acts as a learning in the limit one. Thus, from this point
of view, any hypothesis can be used to start a learning process, and, following a

Learning in the Limit: A Mutational and Adaptive Approach
117
step-by-step transformation of that hypothesis by a SOAA, produces a correct
ﬁnal model, when computational learning can be eﬀective.
Hence, inductive inference can be envisioned in a new and diﬀerent way using
this kind of learner. The SOAA can be used as a learner for formal languages,
as illustrated by the example in section 4.1. There are many applications for the
learning process deﬁned in this paper; one clearly comes from on-line learning.
Since it is a non-stop process, it is suitable for continuous learning, and as the
previous hypothesis can be used to produce new ones, the second-order adaptive
automaton seems to be an appropriate choice for diverse environments.
5.1
Future Work
As a future work, some of the applications mentioned here will be implemented,
to run them in order to generate a benchmark comparing the second-order adap-
tive approach to some others. A lot of work need to be done before a prod-
uct ready to be used can be generated, but the path to be followed has been
established.
It is necessary to deﬁne all the limitations of the computational model and
then deﬁne the learning limitations of this adaptive learning process. Some con-
straints and limits of the adaptive automata hierarchy have to be formally de-
ﬁned. For the purposes of this work, the second-order was suﬃcient. Another
task to be carried out is investigate the necessity of third or higher order.
References
1. Chater, N., Vit´anyi, P.: “Ideal learning” of natural language: positive results
about learning from positive evidence. Journal of Mathematical Psychology 51(3),
135–163 (2007)
2. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of the Philosophy of Science (HPS).
Philosophy of Statistics, vol. 7, pp. 901–982. Elsevier (2011)
3. Gold, E.: Language identiﬁcation in the limit. Information and Control 10(5),
447–474 (1967)
4. Li, M., Vit´anyi, P.: Computational Machine Learning in Theory and Praxis. In:
van Leeuwen, J. (ed.) Computer Science Today. LNCS, vol. 1000, pp. 518–535.
Springer, Heidelberg (1995)
5. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its applications,
3rd edn. Springer Publishing Company, Incorporated (2008)
6. Paul, W.J., Solomonoﬀ, R.J.: Autonomous theory building systems. Technical Re-
port D 6600, Computer Science Department, University of Saarbruecken, Germany
(1990)
7. de Azevedo da Rocha, R. L., Neto, J.J.: Adaptive automaton, limits and complexity
compared to the Turing machine - in Portuguese Autˆomato Adaptativo, Limites
e Complexidade em Compara¸c˜ao com M´aquina de Turing. In: Proceedings of the
I Congress of Logic Applied to Technology, LAPTEC 2000, S˜ao Paulo, Faculdade
SENAC de Ciˆencias Exatas e Tecnologia, pp. 33–48 (2000)

118
R.I. da Silva Filho, R.L. de Azevedo da Rocha, R.H.G. Guiraldelli
8. Rubinstein, R.S., Shutt, J.N.: Self-modifying ﬁnite automata. In: Pehrson, B., Si-
mon, I. (eds.) Proceedings of the 13th IFIP World Computer Congress, Amsterdam.
Technology and Foundations: Information Processing 1994, vol. I, pp. 493–498.
North-Holland (1994)
9. Salthe, S., Matsuno, K.: Self-organization in hierarchical systems. Journal of Social
and Evolutionary Systems 18(4), 327–338 (1995)
10. Silva Filho, R.I., de Azevedo da Rocha, R.L.: Adaptive Finite Automaton: a New
Algebraic Approach. In: Dobnikar, A., Lotriˇc, U., ˇSter, B. (eds.) ICANNGA 2011,
Part II. LNCS, vol. 6594, pp. 275–284. Springer, Heidelberg (2011)
11. Solomonoﬀ, R.J.: A formal theory of inductive inference. parts I and II. Information
and Control 7(2), 224–254 (1964)
12. Solomonoﬀ, R.J.: Does algorithmic probability solve the problem of induction? In
D. L. Dowe, K. B. Korb, and J. J Oliver, editors. In: Dowe, D.L., Korb, K.B.,, J.
(eds.) Proceedings of the Information, Statistics and Induction in Science (ISIS)
Conference, Melbourne, Australia, pp. 7–8. World Scientiﬁc (August 1996)
13. Wallace, C.S.: Statistical and Inductive Inference by Mininum Message Length.
Springer (2005)

 
D.L. Dowe (Ed.): Solomonoff Festschrift, LNAI 7070, pp. 119–130, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
Algorithmic Simplicity and Relevance 
Jean-Louis Dessalles 
Telecom ParisTech, 46 rue Barrault, F-75013 Paris, France 
dessalles@Telecom-ParisTech.fr, www.dessalles.fr 
Abstract. The human mind is known to be sensitive to complexity. For in-
stance, the visual system reconstructs hidden parts of objects following a prin-
ciple of maximum simplicity. We suggest here that higher cognitive processes, 
such as the selection of relevant situations, are sensitive to variations of com-
plexity. Situations are relevant to human beings when they appear simpler to 
describe than to generate. This definition offers a predictive (i.e. falsifiable) 
model for the selection of situations worth reporting (interestingness) and for 
what individuals consider an appropriate move in conversation.  
Keywords: Simplicity, relevance, interestingness, unexpectedness. 
1 
Complexity, Simplicity and the Human Mind 
Almost half a century ago, Ray Solomonoff suggested that inductive learning is 
guided by simplicity [1]. In 1999, Nick Chater drew attention to the fact that several 
other human cognitive processes are guided by a principle of minimum complexity 
[2-3]. Figure 1 illustrates the fact that our visual system reconstructs hidden parts of 
shapes by preferring simplest patterns. 
 
Fig. 1. Hidden shapes are as the least complex ones (after [2]) 
Surprisingly, the same principle seems to be at work also in higher cognitive 
processes. Simplicity explains the sensitivity to coincidences [4-5]. It accounts for 
cognitive biases such as representativeness [6]. It makes predictions about how news 
elicit emotion, depending on parameters such as proximity, rarity or prominence [7]. 
Simplicity, defined as the discrepancy between universal probability distribution and 
uniform distribution, has been used to measure subjective improbability [8]. Com-
plexity has also been related to the feeling of beauty and its variations has been 
claimed to define interestingness [9]. Complexity is also claimed to be involved in 
several human traits such as supernatural beliefs, creativity, humor and fiction  
([9]; [10] p. 545; [11] p. 967). 

120 
J.-L. Dessalles 
 
Our own work on interestingness led us to develop simplicity theory (ST) [7], [12]. 
The central claim of ST is that subjective probability depends on the difference be-
tween generation complexity and description complexity. In these previous develop-
ments of ST, interestingness (which is a form of relevance) was merely equated with 
unexpectedness. The present paper is an attempt to bring the notion of relevance clos-
er to an operational definition, based on algorithmic theory, by defining feature relev-
ance and by distinguishing first-order from second-order relevance. In what follows, I 
will first mention some previous attempts to define relevance, and remind that it is a 
crucial issue. Then I will briefly outline the central notions of Simplicity Theory, and 
show how ST can provide a formal definition of relevance. I will then illustrate 
through examples the explanatory power of the definition. In the conclusion, I will 
consider the current limits of the theory by mentioning recent work on the impact of 
emotion on relevance. 
2 
Relevance 
Relevance is an empirical phenomenon. Human conversation is a risky game in which 
speakers dare not misjudge what is worth telling, lest they be punished by being con-
sidered socially inept [13]. Being relevant is an essential part of what constitutes hu-
man intelligence, as opposed to what we share with other animals. Historically, the 
main contribution to a theory of relevance was offered by Dan Sperber and Deirdre 
Wilson [14]. These authors introduce two new notions: cognitive effect and cognitive 
cost. They define a linguistic utterance as relevant if it maximizes the former and 
minimizes the latter. 
The main merit of this definition is to place the problem on the cognitive ground. 
Relevance is no longer a question of social convention [15], nor a mere statistical 
observation about what is generally said and not said in specific contexts. Relevant 
utterances are supposed to result from genuine computations. Unfortunately, Sperber 
and Wilson do not provide details about how these computations are performed.  
In previous studies, we distinguished two forms of relevance in language, depend-
ing on the conversational mode [7]. Narrative relevance corresponds to interesting-
ness. A good model of narrative relevance should explain how speakers select events 
worth reporting in conversational narratives. Argumentative relevance, on the other 
hand, controls the appropriateness of conversational moves during a discussion. To-
gether, conversational narratives and discussions represent more than 90% of conver-
sational time [7]. 
Algorithmic simplicity may offer a predictive model of narrative relevance [7]. The 
present paper proposes a few formal definitions about what makes an event relevant. 
It will also explore how far the same model can be extended to argumentative  
relevance. 

 
Algorithmic Simplicity and Relevance 
121 
 
3 
Simplicity Theory 
Simplicity Theory (ST) defines simplicity, not in absolute terms, but as the difference 
in complexity between expectations and observation. To do so, it distinguishes the 
standard notion of description complexity from generation complexity.  
The description complexity C(s) of a situation s is the length of the shortest de-
scription of s that the observer may achieve. This notion coincides with Kolmogorov 
complexity, but the choice of the machine is not free. The description machine is 
bound to be the observer, with her previous knowledge and computing abilities. 
Generation complexity Cw(s) measures expectations about s, in complexity terms. 
It is defined as the length of the minimal program that must be given to the “world” 
for it to generate s. Again, this corresponds to the standard definition of Kolmogorov 
complexity, except that the machine is bound to function according to what the ob-
server knows about the world’s constraints (note that the “world” has no objective 
character here). In particular, s is supposed to be generated according to some causal 
process. This constraint affects Cw(s), which may therefore depart from C(s). 
The restriction to particular machines, together with a limited time constraint as in 
[9], makes C(s) and Cw(s) computable. Both computations, however, differ signifi-
cantly. The description complexity C(s) of a situation that the observer already  
encountered may be given by its address in memory. If the observer’s memory is 
organized as a binary tree, then C(s) depends on the location of s on that tree. In the 
computation of C(s) and Cw(s), each bit counts. It is therefore crucial that addresses in 
memory be minimal in length. A minimal (maximally compact) code is easy to design 
for lists, using a positional code. Table 1 offers an example of positional code for a 
list. Note that the code is not self-delimited: this is the price to pay for having a com-
pact code; as a consequence, one must allow the use of a punctuation symbol, or 
equivalently suppose that code segmentation is performed at a preprocessing stage. 
When the observer’s memory structure is unavailable, it is sometimes possible to 
assess the relative complexity of items (objects, people, places…) by comparing their 
relative ranks on the Web using the number of hits given by a search engine. It is for 
instance easy to compute the complexity of US presidents, as shown in table 1 (note 
that G.H. Bush is downgraded, as many pages about him do not mention the middle 
initial). The code used to compute complexity is designed to be maximally compact 
(more compact that a prefix-free code). With this code, once the list of US-president 
is available, its first item, B. Obama, is meant by default and its complexity is zero. 
Generation complexity Cw(s) is computed in a different way. The most basic gen-
eration machine is a uniform lottery among N objects, in which case Cw(s) = log(N). 
When several independent lotteries are used, the complexities add up to give Cw(s). In 
contrast to the observation machine, the generation machine can be considered to be 
memory-less. If the same number n comes out twice in a row in a Lottery game, the 
generation complexity of the double event is 2×Cw(n), whereas its description  
complexity is only C(n). 
 

122 
J.-L. Dessalles 
 
Table 1. Web-complexity of the 10 last US presidents, using a non self-delimited code 
President 
Number of hits 
Code 
Complexity 
Barak Obama 
263000000 
 
0 
George W. Bush 
63300000 
0 
1 
John Kennedy 
57500000 
1 
1 
Bill Clinton 
46200000 
00 
2 
Ronald Reagan 
32200000 
01 
2 
Jimmy Carter  
18200000 
10 
2 
Richard Nixon 
14200000 
11 
2 
Lyndon Johnson 
13200000 
000 
3 
Gerald Ford 
9900000 
001 
3 
George H. Bush 
6260000 
010 
3 
 
The central notion of ST is unexpectedness U(s). It is defined as: 
 
U(s) = Cw(s) – C(s) 
(1) 
In a Lottery game, all draws are supposed to have the same generation complexity 
Cw(s) ≈ 6×log(49) (supposing that 6 numbers are drawn between 1 and 49). When a 
remarkable combination such as 1–2–3–4–5–6 comes out, description complexity 
C(s) is much smaller, and U(s) reaches significant value.  
Figure 2 illustrates the difference between generation and description. The black 
ball has to go trough five binary choices before reaching a leaf s of the tree. There-
fore, Cw(s) = 5. If s is indistinguishable from other leaves, then C(s) = log(32) = 5 and 
U(s) = 0. If s happens to be the only white leaf, then in this case C(s) = 0 (as ‘being 
white’ is the only apparent feature among leaves) and U(s) = 5. 
 
 
Fig. 2. Generation complexity vs. description complexity 
More generally, generation complexity is given by the complexity of the simplest 
causal scenario or theory that may have produced the actual situation (about causality 
and complexity, see [11], section 7.4). Dowe proposes a computation in which theo-
ries are ranked by their complexity ([10] p. 545, note 206): a special list of theories, 
called ‘miracles’, is located somewhere in the theory hierarchy; for some observers, 
for whom miracles are not stored too deep in the complexity-based hierarchy,  

 
Algorithmic Simplicity and Relevance 
123 
 
invoking miracles might be a parsimonious way to account for the generation of a 
given state of affairs. This illustrates the fact that Cw(s) is observer-dependent, as well 
as description complexity C(s). Whenever these computations are available, relevance 
can be quantitatively defined.  
4 
Relevance from an Algorithmic Perspective 
We must distinguish two cases. A situation, or a property of a situation, may be rele-
vant because it contributes to making a topic interesting. We call this quality first-
order relevance. A situation or a property may also be relevant because it makes a 
previous (relevant) topic less relevant. We call this quality second-order relevance, or 
2-relevance. We define these two notions in turn. 
4.1 
First-Order Relevance 
Relevance cannot be equated with failure to anticipate [16]: white noise is ‘boring’, 
although it impossible to predict and is thus always ‘surprising’, even for an optimal 
learner. Our definition of unexpectedness, given by (1), correctly declares white noise 
uninteresting, as its value s at a given time is hard to describe but also equally hard to 
generate (since a white noise amounts to a uniform lottery), and therefore U(s) = 0. 
Following definition (1), some situations can be ‘more than expected’. For in-
stance, if s is about the death last week of a 40-year old woman who lived in a far 
place hardly known to the observer, then U(s) is likely to be negative, as the minimal 
description of the woman will exceed in length the minimal parameter settings that 
the world requires to generate her death. If death is compared with a uniform lottery, 
then Cw(s) is the number of bits required to ‘choose’ the week of her death: Cw(s) ≈ 
log2(52×40) = 11 bits. If we must discriminate the woman among all currently living 
humans, we need C(s) = log2(7×109) = 33 bits, and U(s) = 11 – 33 = –22 is negative. 
Relevant situations are unexpected situations. 
 
s  is relevant if   U(s) = Cw(s) – C(s) > 0 
(2) 
Relevant situations are thus simpler to describe than to generate. In our previous 
example, this would happen if the dying woman lives in the vicinity, or is an acquain-
tance, or is a celebrity. Relevance is detected either because the world generates a 
situation that turns out to be simple for the observer, or because the situation that is 
observed was thought by the observer to be ‘impossible’ (i.e. hard to generate).  
In other contexts, some authors have noticed the relation between interestingness 
and unexpectedness [9, 16], or suggested that the originality of an idea could be 
measured by the complexity of its description using previous knowledge ([10], 
p. 545). All these definitions compare the complexity of the actual situation s to some 
reference, which represents the observer’s expectations. For instance, the notion of 
randomness deficiency ([8], ch. 4 p. 280) compares actual situation to the output of a 
uniform lottery. The present proposal differs by making the notion of expectation 

124 
J.-L. Dessalles 
 
(here: generation) explicit, and by contrasting its complexity Cw(s) with description 
complexity C(s).  
Situations correspond to states of the world. As such, they cannot be grasped in 
every detail. This is not a problem, however, as we can focus on specific aspects of a 
given situation. Relevant aspects constitute the essential part of narratives. Consider a 
feature f that is present in situation s. For instance, the fact that a given individual, 
Ryan, is eating a hot-dog. Considering f as a logical predicate, this means that f(s) is 
regarded as true. We may write, on the generation side: 
 
Cw(s) < Cw(f(s)) + Cw(s | f(s)) 
(3) 
If Ryan could choose freely among 16 possible sandwiches, the first term Cw(f(s)) 
amounts to Cw(f(s)) = log(16) = 4 bits. However, if we know that Ryan is Muslim, 
Cw(f(s)) can reach significant values as, by default, non-eating pork is a low mutable 
property (see section 5.2) for Muslims. On the description side, we have: 
 
C(s) < C(f) + C(s | f(s)) 
(4) 
In contrast with (3), f(s) needs only to be described through a description of f and 
not to be generated as a fact. The term C(f) measures the conceptual complexity of f 
for the observer in the current context. In the example of Ryan’s meal, the conceptual 
complexity of ‘hot-dog’ can be estimated by the logarithm of the rank of this type of 
food in a list of typical meals or in short-term memory.  
Features are relevant with respect to a given situation if they contribute to  
unexpectedness. 
 
f is relevant w.r.t. s if   U(f(s)) = Cw(f(s)) – C(f) > 0 
(5) 
Definitions (2) and (5) control what is worth telling when reporting or signaling an 
event in conversation. Note that if f is the conjunction of several sub-properties, those 
sub-properties need not be relevant separately. The art of telling narratives is to as-
semble elements that, together, produce unexpectedness. A conjecture is that every 
descriptive element, in spontaneous narratives, is intended to make relevance  
eventually maximal.  
4.2 
Second-Order Relevance 
An admissible reaction to relevant topics consists in attempting to diminish their un-
expectedness. The following definition concerns a piece of information t that may 
alter unexpectedness. 
 
if U(s | t) < U(s), then t is 2-relevant w.r.t. s  
In the previous example, t may be the fact that Ryan lost faith. More generally, any 
move that diminishes U(s) is 2-relevant w.r.t. s. This definition covers not only the 
phenomenon of trivialization [7] (“The same happened to me…”), but also any at-
tempt to diminish Cw(s) by simplifying the generation scenario (i.e. by providing an 
explanation). 

 
Algorithmic Simplicity and Relevance 
125 
 
5 
Examples 
5.1 
The ‘Nude Model’ Story 
The story in figure 3 is adapted from a spontaneous conversation analyzed by Neal 
Norrick [17]. The story is about a fortuitous encounter with a model who was pre-
viously seen posing in the nude. Elements indicated in bold face are commented on 
below. 
B: It was just about two weeks ago. And then we did some figure drawing. Every-
one was kind of like, “oh my God, we can’t believe it.” We- y’know, Midwest 
College, y’know,  
[…]  
B: like a … nude models and stuff. And it was really weird, because then, like, 
just last week, we went downtown one night to see a movie, and we were  
sitting in [a restaurant], like downtown, waiting for our movie, and we saw her 
in the [restaurant], and it was like, “that’s our model” (laughing) in clothes 
A: (laughs) Oh my God. 
B: we were like “oh wow.” It was really weird. But it was her. (laughs) 
A: Oh no. Weird. 
B: I mean, that’s weird when you run into somebody in Chicago. 
A: yeah.
Fig. 3. The Nude Model story (after [17]) 
The mention “just last week” is not here by chance. Recent events are simple to de-
scribe, what makes them more likely to appear unexpected. Intuitively, the story is 
better so, than if the time reference had been “one year ago”. Formula (1) explains 
why. If a is the typical duration of this kind of episode, then the complexity of locat-
ing the event at time location T in the past amounts to log2(T/a) bits. Formula (1) pre-
dicts logarithmic recency effects: unexpectedness varies as –log2(T). If B had not 
made temporal location explicit, she would have implicitly meant “at some point in 
my life”. The mention “just last week” is thus relevant according to (5).  
When B locates the initial episode by mentioning “two weeks ago”, she also makes 
a relevant move. This story is about a coincidence. It depicts two situations in which 
B has encountered the model. When two independently generated situations s1 and s2 
bear some resemblance, the joint event is unexpected. It has been observed that Kol-
mogorov complexity is the right tool to quantify the intensity of coincidences ([11]  
p. 967). Coincidences can indeed be shown to be unexpected, according to defini-
tion (1). Let us first observe that generation complexity captures the idea that the 
coinciding situations are independent: 
 
s1 and s2 are independent if   Cw(s1 ∧ s2) = Cw(s1) + Cw(s2) 
(6) 
We get: 
 
U(s1 ∧ s2) > Cw(s1) + Cw(s2) – C(s1) – C(s2 | s1) 
(7) 

126 
J.-L. Dessalles 
 
We see from (7) that the resemblance between s1 and s2 is crucial for producing 
unexpectedness, as it makes C(s2 | s1) smaller than C(s2). In particular, the temporal 
location of s1 may be used to locate s2. If Δ is the temporal distance between s1 and s2, 
then s2 can be located from s1 using only log2(Δ/a) bits. The economy in the descrip-
tion generates unexpectedness. The mention “two weeks ago” is thus essential. 
The model’s nudity is essential to the story. With a dressed model, the story would 
be much poorer indeed. This simple property, having been seen naked, makes the 
model simple to B’s eyes, in two ways. The model belongs to the restricted set of the 
n people who were naked in B’s company. Her complexity is at most C(model|naked) 
< log2(n). But nudity makes the model simple in another way. She was that (unique) 
person who was seen posing in the nude in a Midwest College.  
The mention “Midwest College” indeed contributes to the story’s unexpectedness. 
B makes it explicit that figure drawing with a nude model is a truly exceptional situa-
tion in such an institution. Interest would lessen if B had been attending an art school 
with regular life drawing. This time, unexpectedness is due to the difficulty of gene-
rating the situation. Nude models do not belong to Midwest colleges. Generating a 
situation that contradicts this statement is as complex as the statement’s mutability is 
low (see section 5.2). 
The obvious mention “in clothes” contributes to the complexity contrast: the mi-
nimal scenario that allowed B to see the same person in public twice, once naked, 
once in clothes, cannot be simple.  
The actual presence of the model in the restaurant (“But it was her”) is crucial. The 
description complexity of the dressed person would have been significantly larger if 
she just looked like the nude one. The mention “we saw her” is relevant in a similar 
way. B reports the event as a first-hand story. The same anecdote would appear less 
interesting if it had happened to one of B’s friend C. The complexity of C would have 
been subtracted from U(s). 
B feels the necessity of mentioning a fact which is also obvious to her interlocutor, 
when she specifies “in Chicago”. The size of the city, measured for instance by the 
number N of its buildings, matters here. If the second encounter is supposed to be 
generated through a lottery, then generating the presence of the model in a specified 
place amount to log2(N). So the relevance increases as log2(N)  (note that if there were 
k people in that place, then a term –log2(k) comes from the description side, due to the 
indeterminacy). 
We observe that by equating relevance with simplicity (complexity drop) and with 
unexpectedness, we are able to account for the various parameters that control interest 
in this story. This is a non-trivial and falsifiable result, which is in line with the  
importance of algorithmic complexity in cognitive computations. 
5.2 
The ‘Rally’ Discussion 
Let’s consider now relevance within a discussion. The discussion in figure 4 occurred 
between French students. F will graduate in a few months and will no longer be a 
student next year. When F claims he wants to participate in the rally next year,  
 

 
Algorithmic Simplicity and Relevance 
127 
 
F- This rally, wonderful! I’m ready to come back from Toulouse next year to  
participate.  
G- Yes, but it is only for students, isn’t it?  
T- No, no, it’s open to everyone. 
F- There were people from Arcade! 
G- Yes, but they were sponsors! 
Fig. 4. The Rally discussion (translated from French) 
G points to an inconsistency. The reminder of the discussion is about whether the 
contradiction is real or not. 
In the discussion of figure 4, G draws attention to a logical clash between three 
propositions: ¬f1 = ‘not being a student’, f2 = ‘participate in the rally’ and f3 = ‘the 
rally is only for students’ (¬ refers to negation). As we will see, the effect of G’s first 
utterance is to increase the generation complexity of f2, and so to make the situation 
unexpected. This is what makes G’s move relevant. 
Several links exist between logic and generation complexity. Some are listed below 
(⊃ refers to implication): 
 
Cw(a ∨ b) = min(Cw(a), Cw(b)) 
(8) 
 
Cw(a ∧ b) < Cw(a) + Cw(b) 
(9) 
 
If (a ⊃ b), then Cw(a) > Cw(b) 
(10) 
In our example, the incompatibility between ¬f1, f2 and f3 can be rewritten: f2 ⊃ (f1 
∨ ¬f3). We get:  
 
Cw(f2) > Cw(f1 ∨ ¬f3 ) 
and thus: 
 
Cw(f2) > min(Cw(f1), Cw(¬f3)) 
(11) 
G’s point is that both f1 (F will still be a student next year) and ¬f3 (the rally is 
open to anyone) are hard to generate, and so is f2 (F’s participation). Due to the large 
value of Cw(f2), f2 appears unexpected and G’s point is relevant. 
Generation complexity can be linked to the notion of mutability [18]. Facts about 
the world are memorized with ‘necessity’ values that are due to beliefs. I believe that 
my bank account balance is positive and I believe that the capital city of France is 
Paris, but the former belief is more mutable than the latter. To measure mutability, we 
have to consider the least complex combination of circumstances that can change the 
observer’s belief toward a proposition f. For my bank account to be in the red right 
now, I must imagine an abnormal expense that I would have forgotten, or some com-
puter error, or that my salary has been seized by some unknown court decision. Let’s 
call H(f) the least complex scenario that can produce f. We may write: 
 

128 
J.-L. Dessalles 
 
 
Cw(f) = Cw(H(f)) 
(12) 
The mutability M(f) of f can be defined as:  
 
M(f) = – Cw(H(¬f)) = – Cw(¬f) 
(13) 
(note that mutability is always negative). When H(¬f) is complex, ¬f is complex to 
generate and the fact f is not mutable (M(f) << –1). M(f) might be retrieved from 
memory, or directly computed by finding out the most convincing (or least uncon-
vincing) scenario H(¬f). It can be also inherited through (10) which can be rewritten 
Cw(¬b) > Cw(¬a): 
 
If (a ⊃ b), then M(a) > M(b) 
(14) 
The conversation of figure 4 offers an example of 2-relevance. F’s second utter-
ance: “There were people from Arcade”, is meant to refute f3. Those people work in a 
company and are not students, and yet they were among participants. So (¬f1 ∧ f2) is 
easy to generate. Since f3 ⊃ ¬ (¬f1 ∧ f2), we get from (14) that M(f3) > –Cw((¬f1 ∧ f2)). 
f3 is therefore highly mutable. Relation (11) no longer constrains Cw(f2) to be large. 
F’s second utterance is thus 2-relevant.  
More generally, any attempt to solve a problematic fact f will be 2-relevant. The 
solution may be a belief revision or a new and simpler scenario H(f). In any case, it 
leads to a diminution of Cw(f), which may be named ‘compression’, as in Gregory 
Chaitin’s aphorism “comprehension is compression” [19] (see [11], section 7.3 for a 
review of ideas about compression and explanation). 
6 
Discussion 
What precedes is an attempt to account for the phenomenon of relevance in terms of 
complexity. The principal departure from standard algorithmic theory is that we dis-
tinguish between generation complexity and description complexity, and that all com-
putations are performed on specific ‘machines’. This approach offers numerous  
advantages. For instance, it predicts that the relevance of an event occurring at dis-
tance d from the observer varies like 2×log2(d) [12]. It also predicts that for an object 
s randomly taken from a class r to be relevant, both the class and the reason f that 
makes s unique must be simple: 
 
C(s) < C(r) + C(f | r) + C(s| r∧f) 
If there are N objects in the class, then Cw(s) = log(N). If we assume uniqueness of 
s knowing r and f, C(s| r∧f) = 0 and: 
 
U(s) > log2(N) – C(r) – C(f | r) 
(15) 
Relation (15) may be tested by its predictions of relevance in a collection of  
records such as the Guinness Book. It also open the way to automated news selection. 

 
Algorithmic Simplicity and Relevance 
129 
 
Another advantage of the algorithmic approach to relevance is that it is closer to 
the possibility of implementation than alternative definitions, such as [14]. As illus-
trated in table 1, description complexity values can be assessed through various prac-
tical means. Generation complexity values can be computed using (6), (8), (10) to 
combine the parameter settings of simple machines such as lotteries. 
One limitation of the above definition of relevance is that it does not take emotion 
into account. The emotional scale E(s) on which an event or discussion topic s is 
placed is an essential ingredient of relevance. It is not equivalent to speak about the 
loss of people’s life or the loss of ten Euros. Relevance I(s) is a function of the emo-
tional scale and of unexpectedness: 
 
I(s) = F(E(s), U(s)) 
(16) 
Relation (16) means that once the emotional scale is determined, emotional inten-
sity (and thus relevance) is entirely controlled by unexpectedness. F is an increasing 
function of its two arguments. Determining the nature of F remains a problem and is a 
topic of future investigations. 
We are aware of the fact that the notions developed in this paper may benefit from 
a formal description of the generation machine and of the observation machine. This 
research program is motivated by the assumption that the human brain is sensitive to 
algorithmic complexity [2], even at higher cognitive levels where relevance is proc-
essed. The results already obtained are encouraging. They show that algorithmic 
complexity is not bound to deal with theoretical computer science and prove mathe-
matical theorems, but can also be used to model particular machines such as the  
human mind. We expect that other important aspects of cognitive processes will be 
analyzed using an algorithmic complexity approach, and that these new insights will 
lead to implementations.  
References 
1. Solomonoff, R.J.: A Formal Theory of Inductive Inference. Information and Control 7(1), 
1–22 (1964), http://world.std.com/~rjs/1964pt1.pdf 
2. Chater, N.: The search for simplicity: A fundamental cognitive principle? The Quaterly J. 
of Exp. Psychol. 52 (A), 273–302 (1999) 
3. Chater, N., Vitányi, P.: Simplicity: a unifying principle in cognitive science? Trends in 
cogn. Sc. 7(1), 19–22 (2003) 
4. Feldman, J.: How surprising is a simple pattern? Quantifying ‘Eureka!’. Cognition 93, 
199–224 (2004) 
5. Dessalles, J.-L.: Coincidences and the encounter problem: A formal account. In: Love, 
B.C., McRae, K., Sloutsky, V.M. (eds.) Pr. of the 30th Annual Conf. of the Cognitive 
Science Society, pp. 2134–2139. Cognitive Science Society, Austin (2008),  
http://www.dessalles.fr/papiers/pap.conv/ 
Dessalles_08020201.pdf 
6. Kahneman, D., Tversky, A.: Subjective probability: A judgement of representativeness. 
Cogn. Psychol. 3, 430–454 (1972) 

130 
J.-L. Dessalles 
 
7. Dessalles, J.-L. (2008), La pertinence et ses origines cognitives - Nouvelles théories. 
Hermes-Science Publications, Paris, http://pertinence.dessalles.fr 
8. Li, M., Vitányi, P.: An introduction to Kolmogorov complexity and its applications, 3rd 
edn. Springer, New York (1997) 
9. Schmidhuber, J.: Simple Algorithmic Theory of Subjective Beauty, Novelty, Surprise, In-
terestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes. Journal of 
SICE 48(1), 21–32 (2009), http://www.idsia.ch/~juergen/sice2009.pdf 
10. Dowe, D.L.: Foreword re C. S. Wallace. The Computer Journal 51(5), 523–560 (2008) 
11. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consistency, in-
variance and uniqueness. In: Bandyopadhyay, P.S., Forster, M.R. (eds.) Handbook of the 
Philosophy of Science. Philosophy of Statistics, vol. 7, pp. 901–982. Elsevier, Amsterdam 
(2011) 
12. http://www.simplicitytheory.org 
13. Polanyi, L.: So What’s the point? Semiotica 25(3), 207–241 (1979) 
14. Sperber, D., Wilson, D.: Relevance: Communication and cognition. Blackwell, ed., Oxford 
(1986,1995) 
15. Grice, H.P.: Logic and conversation. In: Cole, P., Morgan, J.L. (eds.) Syntax and Seman-
tics. Speech acts, vol. III, pp. 41–58. Academic Press, New York (1975) 
16. Schmidhuber, J.: What’s interesting? Lugano, CH: Technical Report IDSIA-35-97 (1997), 
ftp://ftp.idsia.ch/pub/juergen/interest.ps.gz 
17. Norrick, N.R.: Conversational narrative: storytelling in everyday talk. J. Benjamins Publ. 
Comp., Amsterdam (2000) 
18. Kahneman, D., Miller, D.T.: Norm theory: Comparing reality to its alternatives. Psychol. 
Rev. 93(2), 136–153 (1986) 
19. Chaitin, G.J.: On the intelligibility of the universe and the notions of simplicity, complexi-
ty and irreducibility. In: Hogrebe, Bromand (eds.) Grenzen und Grenzüberschreitungen, 
vol. XIX, pp. 517–534. Akademie Verlag, Berlin (2004) 

Categorisation as Topographic Mapping
between Uncorrelated Spaces
T. Mark Ellison
IO Institute of Linguistics, Australia
mark@iolinguistics.com
Abstract. In this paper, I propose a neurophysiologically plausible ac-
count for the evolution of arbitrary, categorical mental relationships.
Topographic, or structure-preserving, mappings are widespread within
animal brains. If they can be shown to generate behaviours in simulation,
it is plausible that they are responsible for them in vivo. One behaviour
has puzzled philosophers, psychologists and linguists alike: the categori-
cal nature of language and its arbitrary associations between categories
of form and meaning. I show here that arbitrary categorical relationships
can arise when a topographic mapping is developed between continuous,
but uncorrelated activation spaces. This is shown ﬁrst by simulation,
then identiﬁed in humans with synaesthesia. The independence of form
and meaning as sensory or conceptual spaces automatically results in a
categorial structure being imposed on each, as our brains attempt to link
the spaces with topographic maps. This result suggests a neurophysio-
logically plausible explanation of categorisation in language.
1
Introduction
Solomonoﬀ’s 1964 paper A Formal Theory of Inductive Inference [29] was a land-
mark in the study of learning. It brought together the philosophical problem of
induction, probability theory, information theory, computability and formal lan-
guage theory. It related shortness of coding length with goodness of ﬁt, presaging
the well-known induction methods of MML/MDL [35,24]. Of particular inter-
est to cognitive science and natural language processing was his exploration of
grammar induction, an analogue of which occurs in the development of every
speaker of every natural language. In the current paper, I explore an aspect of
language learning which complements the grammar induction task addressed by
Solomonoﬀ, asking: how do our brains construct categorial symbols out of the
continous inputs they receive? Where the universal Turing machine was the ab-
stract computational notion at the basis of Solomonoﬀ’s exploration of learning,
the topographic mapping is the basis of the current paper.
Topographic mappings are relations between two spaces which preserve sim-
ilarity: similar points in one space are related to similar points in the other.
As the name suggests, this is an ideal property for maps of territory: nearby
geographic locations should be nearby on the map, places far from each other
should appear distant on the map.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 131–141, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

132
T.M. Ellison
Topographic maps in the brain were ﬁrst noticed as a result of the work of Pen-
ﬁeld and Boldrey [20,26]. Directly stimulating the brains of locally anaesthetised
epilepsy patients allowed them to explore the qualia associated with particular
points on the brain. For the most part, they found that sensations attributed to
nearby points on the human surface or entrails resulted from stimulating nearby
parts of the brain.
Since this initial work, researchers have both found new topographic struc-
tures in the brain [28], and identiﬁed previously known structures as resulting
from constructing maximally topographic connections from complex inputs [7].
Section 2 discusses these natural topographic mappings in more detail, support-
ing the premiss that such mappings are widespread and a plausible neurophysi-
ological explanation for cognitive phenomena.
It is possible to deﬁne formally what constitutes a topographic map [8], and
to construct them algorithmically. Section 3 considers some general deﬁnitions
and implementations. The crux of the paper comes in section 3.5. When a to-
pographic map is constructed between two spaces on the basis of uncorrelated
pairs of inputs and outputs, it is found to exhibit a remarkable form. It shows
sectioning of the input into contiguous chunks that map onto a single value or a
small connected range of values.
This result is important, because it suggests a neurophysiological explanation
for categorisation in language. Language is inherently categorical. The word cat
consists of the same sequence of phonemes whether said by a small child with
a lisp, or sung by an operatic contralto. These variations do not matter for the
identity of the word, but there is a limit. Pronounced like cute, and the word cat
is no longer there at all, even though the sounds are similar.
Likewise the meanings of words are categorical. The class refered to by cat
does not blend, even in response to variations in pronunciation, into kite or coat.
Why is this the case? It is logically possible to imagine a language which was
directly iconic: a language of drawn pictures, or as Bickerton imagined, dolphins
refering to objects by mimicking their sonar proﬁle [3].
In section 4, I discuss linguistic and cognitive categories and argue that these
can be driven by forcing topographic mappings between uncorrelated spaces, one
of which consequently becomes the space of representation, and the other, the
space of meaning. For example, the well-studied colour categories could arise by
mappings from the sensory colour space (along with other sensory and conceptual
spaces) onto a space of phonetic expressions.
In fact, we can ﬁnd direct evidence for this type of categorisation in the ex-
perience of synaesthetes. Section 5 relates the categorical nature of synaesthetic
associations to a likely neurophysiological explanation: a topographic mapping
between cortical areas dealing with the associated qualia.
The ﬁnal section 6 draws the argument together to support a neurophysio-
logically plausible account of the development of linguistic and other categorical
cognitive processes: topographic mappings created between spaces receiving un-
correlated inputs.

Categorisation by Topographic Mapping
133
2
Topographic Mappings in the Brain
Topographic mappings relate similar or nearby inputs to similar or nearby out-
puts. These kinds of mappings are found between the senses and the cortex,
between diﬀerent parts of the cortex (and other parts of the brain), and between
the cortex and the motor system. They seem to be a ubiquitous part of neural
processing [32].
2.1
Easy to See Mappings
In animal sensory systems, the inputs are primary sensors arranged over a sense
ﬁeld as retinas in the case of sight, the skin in the case of touch, the organs
of Corti in the case of hearing, and so on. The outputs are areas of the cortex
devoted to sythesising information from these inputs, such as the primary visual
cortex. The mapping of the sense of touch onto the cortex is relatively simple.
Penﬁeld and Boldrey [20,26] sought to express only the area of cortex assigned
to particular somatosensory organs in their ﬁrst homunculus. Consequently, the
mapping onto the cortical slice was not shown, but it was implied. If an area
of cortex could be mapped onto the hand, so that its appropriate size could be
determined, this implied that points within the hand were within a single region
in the cortex, and stimuli from outside the hand were processed further away. A
more direct mapping between body part and cortex is seen in the homunculus
of Penﬁeld and Rasmussen [21]. The topographic nature is clearly visible, for
example, a point in the arm is mapped closer to nearer points on the same
appendage than to further points. (See [26] for a history of the homunculus.)
Recent ﬁne-grained studies such as [11] have used techniques such as magnetic
resonance imaging to create ﬁne-grained maps of the motor and somatosensory
cortices. These studies show that while there some overlap of cortical areas re-
sponding to the actions of individual ﬁngers, there are orderly somatotopies.
Topographic mappings are also found repeatedly in the visual cortex. The
next section deals with the relation between topographic mappings and ocu-
lar dominance stripes. Suﬃce it to say for the moment that the primary visual
cortex maps inputs from the visual ﬁeld according to their position in retina.
Recently, specialised topographic mappings of visual inputs that also integrate
infromation from other senses have been found [28,25,30]. In fact, some [9] as-
sume retinotopicity to determine boundaries for optical representations in the
cortex.
These kinds of mapping also occur with auditory stimuli. [15] ﬁnds topo-
graphic mappings of modulation density by frequency in the cortex.
2.2
Continuous and Discrete - Ocular Dominance Stripes
Some topographic mappings into cortical structure are easy to see. A two-
dimensional sensory ﬁeld is mapped into a two-dimensional cortical area, with
geometry preserved. Other kinds of topographic mapping are not so transparent,
as they appear to display rampant discontinuity.

134
T.M. Ellison
Ocular dominance stripes (or columns) were ﬁrst named such by Hubel and
Wiesel [13], although they had noted the phenomenon earlier [12]. They identiﬁed
columns of cells in the cortex which responded to input from one eye only. These
columns were arranged into contiguous striping patterns over the visual cortex.
In work for which they were awarded the Nobel prize, they showed that these
stripes were developmentally conditioned and aﬀected by environmental stimuli.
When young kittens lost stimuli from one eye, the other took over the cortical
areas normally used for signals from the occluded eye. Similar eﬀects have been
shown in primates [16].
Ocular dominance stripes have been found in non-mammals such as owls [22]
and (in extraordinary circumstances) frogs [14].
Despite our considerable understanding of forces that can cause or disrupt
the development of ocular dominance columns, there is no consensus as to their
function [1]. There is a model of their development, however, that sees them as
a natural (but not necessarily inevitable) consequence of constructing a topo-
graphic mapping from two positively correlated planar sources of information
onto the cortex.
Goodhill [7] introduced a model for the mapping from eye to brain which is
capable of developing both the topographic structure of the mapping as well as
the ocular dominance striping. The striping occurs because the inputs to the
two eyes are positively correlated in binocular vision. The model predicts that
increasing correlation leads to narrower stripes. More to the point, it suggests
that when correlations of this kind are present we should not be surprised to
ﬁnd dominance striping along with it.
This prediction seems to have been born out in two results in diﬀerent species.
In one study of frogs [14], the authors implanted a node which developed into
a third eye in the growing frog, on the same side as one of the existing eyes,
creating binocular vision on that side. The two eyes developed, with some of
the visual input from one eye sharing cortical projection with the other. In the
shared cortex, ocular dominance stripes resulted.
The two eyes on this side had highly correlated inputs, having overlapping
visual ﬁelds. The fact that striping occurred in just this case supports Good-
hill’s analysis of striping as a side-eﬀect of constructing a topographic map from
multiple correlated inputs.
Further evidence for this analysis comes from studies of platypus bill sensing
reported by Pettigrew [22]. The platypus has electrosensors and mechanosensors
intermingled in its bill, and despite these sensors having separate pathways to
the brain, they map cortically with striping similar to ocular dominance striping.
This is no surprise to the topographic explanation: if the animal is integrating
information from two highly correlated topographic arrays (two eyes looking
in the same direction or electro- and mechano-sensors spread out on the same
surface), then dominance striping is a likely way to bring relevant information
together in the brain.
So ocular dominance stripes despite, or even because of, their discontinuities
can be regarded as optimal topographic mappings from one space, made up

Categorisation by Topographic Mapping
135
of two disjointed but correlated parts, to contiguous space. As we shall see in
section 3.5, this is not the only way to get chunking and discontinuities from
topographic mappings.
In summary then, topographic mappings arise frequently in the brain; the
more we look for them, the more we ﬁnd them. They integrate knowledge from
diﬀerent receptive ﬁelds, and even diﬀerent sense types. Furthermore, they can
give rise to complex discontinuous structures.
3
The Topographic Extrapolation
This section presents a measure of topographicity, and extends it to ﬁnding the
most topographic extensions of given data, in a manner similar to that described
by Ellison [6].
3.1
Measuring Topographicity
Goodhill and Sejnowski [8] oﬀered the general evaluation measure for the to-
pographicity of functions shown in (1) (symbols have been changed to avoid
confusion).
C(f) =

i∈I

j∈I
sI(i, j)sO(f(i), f(j))
(1)
Here sI and sO are similarity measures on the input and output spaces respec-
tively. C(f) is the measure of how topographic the space is, with larger values
for more topographic mappings. Both the sI and sO are assumed to be every-
where non-zero. Consequently C(f) (if ﬁnite) is maximised for f which match
small values of sI with small values of sO and large with large. See Goodhill and
Sejnowski’s discussion for more details.
The deﬁnition can be extended to apply to any ﬁnite collection D of input-
output data points (2).
C(D) =

(i,o)∈D

(j,p)∈D
sI(i, j)sO(o, p)
(2)
3.2
Extrapolation
Solomonoﬀ[29], following Carnap [5], couched learning problems as sequence
extrapolation coding problems. He started by deﬁning a cost for a sequence as
the length of the shortest program for a given universal Turing machine which
would generate that sequence. From this was constructed an answer to the ex-
trapolation problem: the best extrapolation of a sequence is the one which gives
the smallest increment in the cost function.

136
T.M. Ellison
The same approach can determine the most topographic extrapolation from
a given set of input-output pairs D. Since sI and sO are similarity measures,
rather than cost functions or distances, larger C(D) is better. Consequently, we
seek to maximise the change in C(D) resulting from the addition of the new pair
(x, y), as expressed in (3).
C(D ∪{(x, y)}) −C(D) = 2 ∗

(i,o)∈D
sI(i, x)sO(o, y) + sI(x, x)sO(y, y) (3)
We assume that the self-similarity of each point in the input space and the
output space are constant: sI(x, x) does not depend on x, nor sO(y, y) on y. This
means that the whole equation is maximised when the summation is maximised,
allowing us the concise form shown in (4) for the best extrapolation.
In what follows, we will write S(x, y|D) for the term 
(i,o)∈D sI(i, x)sO(o, y).
E(D) = (x, y)|x ∈I, y ∈O that minimise S(x, y|D)
(4)
The extrapolation can be limited to selecting the best output for a given
input x. If this is done for all values of x ∈I, we exrapolate a most topographic
function ﬁtting the data (5).
F(D) = {(x, y)|x ∈I, y ∈O where y uniquely minimises S(x, y|D)}
(5)
Frequently, F(D) is only a partial function. If there are multiple maxima for
S(x, y) for some x, then no y uniquely minimises this function. For these values
of x, the function remains undeﬁned.
3.3
A Normal Similarity Measure
In the simulations which follow, I use a probability-based similarity measure.
This measure parallels the notion of confusion probabilities as discussed by the
author in an earlier paper on induction [6]. It is deﬁned, for an arbitrary space in
equation (6). One parameter to the measure is scale constant σ assumed constant
over the whole space.
s(x, y) = N(x −y; σ)
(6)
The measure has not been normalised over either x or y to retain its com-
mutativity, so will not act here as a probability distribution. It can be proven
that given this similarity measure, for any ﬁnite data set D, the extrapolated
function F(D) is continuous wherever it is deﬁned.

Categorisation by Topographic Mapping
137
3.4
Extrapolation from Highly Topographic Functions
At this point, it is worth looking at the extrapolation from data which is highly
topographic. For simplicity, take the one-dimensional unit interval [0, 1] as both
input and output space.
A program, dubbed OPTOPO, was devel-
oped in PYTHON to extrapolate pointwise
the most topographic function matching a
given input data set, based on the optimisa-
tion derived in section 3.2.
Data was generated by mapping each of
20 points equally spaced along the unit in-
terval to itself. These were then used to ex-
trapolate values covering the same interval
at 0.001 spacing. A graph of the resulting
extrapolations appears in ﬁgure 1. The simi-
larity measures are normal density functions
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
"x.data"
Fig. 1. Topographic
extrapolation
from 20 points of io = i on the unit
interval to the 1000 points
as described in section 3.3, with scale constants σ set for input and output spaces
to 0.04.
In general, we can expect strongly topographic data, in the sense of equation
(2), to give rise to extrapolations matching those functions.
3.5
Independently Varying Spaces
In the last example, a thousand points were extrapolated from 20 belonging to
the already topographic identity function. This function was recovered, apart
from small curvature at the extremes ends of the interval. Now we explore the
other extreme: what happens when the training data is random and uncorre-
lated?
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
"x.data"
Fig. 2. Input
data
to
the
topo-
graphic mapping between uncorre-
lated spaces: 500 points in [0, 1]×[0, 1]
Training data was generated, consisting
of 500 points with random input and output
values drawn from a ﬂat distribution over
the unit interval. A scatter plot of this data
is shown in ﬁgure 2.
Applied to this training data, OPTOPO
returned the function shown in ﬁgure 3 as
the most topographic that could be deduced
from the data. The most remarkable fea-
ture of this graph is that it shows a largely
smooth, continuous function with only a
handful of discontinuities.
Between the discontinuities, we have continuous maps from input to out-
put. Although the mapping output is usually fairly level, there is sometimes
variation. In these cases the variation is smooth. An input point between two
close neighbours maps onto an output point between the projections of those
neighbours.

138
T.M. Ellison
The explanation of this combination of smooth mapping plus discontinuity lies
in the fact that we are looking for the most topographic output value o for each
input i. The smooth similarity measures turn the scattered host of input data
points into a continuously contoured scalar
ﬁeld over the input-output plane. Construct-
ing the most topographic extrapolation func-
tion is a process of maximising the evalua-
tion for each input value. The scalar ﬁeld
over the plane has ridges of higher values,
and these are what we see deﬁning the con-
tiguous categories in the input. Transitions
from one ridge to another happen when a
falling ridge of values is overtaken by a rising
ridge, as the focus moves along input values.
This transitional discontinuity is of the kind
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0
 0.2
 0.4
 0.6
 0.8
 1
"x.data"
Fig. 3. The most topographic (i.e.
the most probable) function account-
ing for the uncorrelated input data
made famous in the Catastrophe Theory of Rene Thom [33].
Functions with this kind of discontinuity deﬁne a categorisation of the input.
Inputs within a connected region belong to one category. Inputs from regions sep-
arated by discontinuities come from diﬀerent categories. Topographic mappings
oﬀer an easy route from continous spaces to categorisation, and this suggests
elegant new explanations for the origin and nature of language.
4
Explaining the Categorical Nature of Language
Language is both symbolic and categorical. It is symbolic because there is no
intrinsic, or to use Peirce’s [19] terms, no indexical and no iconic, relation be-
tween the space of meanings, and the space of linguistic representations. No in-
Fig. 4. Iconic
representation
of ﬁre
dexical relationship means that there is no physical association
of our units of meanings with what they represent: our word
ﬁre does not share the same relationship with actual ﬁre that
actual smoke does. No iconic relationship means that there is
no structure-preserving mapping which relates the word ﬁre to
what it represents. In contrast, the sign for ﬁre shown in ﬁgure 4,
is an iconic representation: while this image has no physical con-
nection to ﬁre, the qualia of looking at the image share structural
similarities to a view of a ﬁre. But not being iconic or indexical
does not account for the categorical nature of natural language symbols.
So language is a connection between uncorrelated spaces of meaning and
representation. Could language be a topographic mapping between these spaces?
If so, it would explain the categorical nature of both meanings (eg. colour classes,
genders, emotions, etc.) and forms in language (eg. the phonological forms of
words). If this were the case, we would expect two features of language: arbitrary
categorisation, with categories varying from language to language, and a globally
non-topographic function from form to meaning.
Language categories are arbitrary, and they certainly vary from language
to language, although there may be functional pressures which limit variation.

Categorisation by Topographic Mapping
139
Variation in the number and phonetic realisation of phonemes is one example.
Language-speciﬁcity is more hotly debated, however, with regard to semantic
categories. Nevertheless researchers report tracking shifts in categorisation as
children adapt to the language they are acquiring [4,17,18].
Vocabulary also shows no strong large scale topographic relation between
meaning and form. Shillcock et al [27] and Tamariz [31] looked for large-scale
systematicity in the meanings of monomorphemic words in English and Spanish
respectively. Eﬀects were found, but primarily for localised domains such as
pause ﬁllers, and for limited phonetic features.
So the evidence agrees with a model in which word forms are related to their
meanings by a topographic mapping linking uncorrelated spaces.
5
Synaesthesia
Synaesthesia is the leakage of qualia from one sensory input to another. For
example, someone might see – in their mind’s eye – the colour red when they
hear the word hurry, or smell olives when seeing square objects. For an overview
of synaesthesia, see [10].
De Thornley Head [34] ﬁnds that pitch-colour synaesthetes have reliable map-
pings from pitches onto colours (unlike non-synaesthetes), and takes this as
evidence that synaesthesia is a perceptual phenomenon apart from memory,
metaphor or imagery. He interprets the mapping itself in the following way.
Pitches and colours are broken into regions by some unclear mechanism. These
regions are then associated in an arbitrary fashion. There can be smooth topo-
graphic mappings within the regions so associated, but no such connections hold
when region boundaries are crossed.
These ﬁndings agree with a model in which synaesthesia is the result of addi-
tional connectivity, a topographic map, linking an auditory representation with
a representation of colour. That synaesthesia results from structural brain dif-
ferences is supported experimentally [2,23]. For the most part, auditory and
colour stimulus arrive from the senses uncorrelated. Consequently, the topo-
graphic map developed has the form of ﬁgure 3: discontinuous regions of input
mapped smoothly onto arbitrary and disconnected regions of output.
6
Conclusion
The purpose of this paper was to propose a neurophysiologically plausible ex-
planation for the formation of arbitrary categorisations. The core of this ex-
planation is that, as seen in section 2, topographic mappings are commonplace
in the brain, turning up even when their nature is less than obvious, as in the
case of ocular dominance striping. To make the explanation concrete, a formal
description of topographic mappings was presented in section 3. This section
ﬁnished with the core result of this paper: the most topographic linkage between
two uncorrelated spaces can be a categorical mapping with, at most, in-category
smooth mapping. Section 4 and section 5 explored some of the phenomena which
might be explained using this result. Monomorphemic vocabulary shows little

140
T.M. Ellison
large-scale correlation of similarity of phonetic form and similarity of meaning,
instead linking somewhat arbitrary categories of meaning to arbitrary (though
systematic) categories of form. Similarly, pitch-colour synaesthetics show only
local smoothness in their mapping of one modality to the other; the mapping
consists of connections between somewhat arbitrary chunks of the pitch and
colour spaces. Topographic mappings between uncorrelated spaces explain both
these phenomena.
References
1. Adams, D.L., Horton, J.C.: Ocular dominance columns: Enigmas and challenges.
The Neuroscientist 15(1), 62–77 (2009)
2. Bargary, G., Mitchell, K.J.: Synaesthesia and cortical connectivity. Trends in Neu-
rosciences 31(7), 335–342 (2008)
3. Bickerton, D.: King of the sea, 1st edn. Random House (1979)
4. Borovsky, A., Elman, J.: Language input and semantic categories: A relation
between cognition and early word learning. Journal of Child Language 33(04),
759–790 (2006)
5. Carnap, R.: Logical Foundations of Probability. University of Chicago Press,
Chicago (1950)
6. Ellison, T.M.: Induction and inherent similarity. In: Similarity and Categorization,
pp. 29–50. Oxford University Press (2001)
7. Goodhill, G.J.: Topography and ocular dominance: a model exploring positive cor-
relations. Biological Cybernetics 69(2), 109–118 (1993); PMID: 8373882
8. Goodhill, G.J., Sejnowski, T.J.: A unifying objective function for topographic map-
pings. Neural Computation 9(6), 1291–1303 (1997)
9. Greenlee, M.W.: Human cortical areas underlying the perception of optic ﬂow:
brain imaging studies. International Review of Neurobiology 44, 269–292 (2000)
10. Harrison, J.E., Baron-Cohen, S.: Synaesthesia: Classic and Contemporary Read-
ings. Wiley-Blackwell (January 1997)
11. Hlutk, P., Solodkin, A., Gullapalli, R.P., Noll, D.C., Small, S.L.: Somatotopy in
human primary motor and somatosensory hand representations revisited. Cerebral
Cortex 11(4), 312–321 (2001)
12. Hubel, D.H., Wiesel, T.N.: Receptive ﬁelds, binocular interaction and functional
architecture in the cat’s visual cortex. The Journal of Physiology 160(1), 106–154
(1962) PMID: 14449617 PMCID: 1359523
13. Hubel, D.H., Wiesel, T.N.: Receptive ﬁelds and functional architecture of monkey
striate cortex. The Journal of Physiology 195(1), 215–243 (1968)
14. Ide, C., Fraser, S., Meyer, R.: Eye dominance columns from an isogenic double-
nasal frog eye. Science 221(4607), 293–295 (1983)
15. Langers, D.R.M., Backes, W.H., van Dijk, P.: Spectrotemporal features of the
auditory cortex: the activation in response to dynamic ripples. NeuroImage 20(1),
265–275 (2003) PMID: 14527587
16. Le Vay, S., Wiesel, T.N., Hubel, D.H.: The development of ocular dominance
columns in normal and visually deprived monkeys. The Journal of Comparative
Neurology 191(1), 1–51 (1980)
17. Majid, A., Boster, J.S., Bowerman, M.: The cross-linguistic categorization of ev-
eryday events: A study of cutting and breaking. Cognition 109(2), 235–250 (2008)

Categorisation by Topographic Mapping
141
18. Majid, A., Gullberg, M., van Staden, M., Bowerman, M.: How similar are semantic
categories in closely related languages? a comparison of cutting and breaking in
four germanic languages. Cognitive Linguistics 18(2), 179–194 (2007)
19. Peirce, C.S., Hartshorne, C., Weiss, P., Burks, A.W.: Collected Papers of Charles
Sanders Peirce. Harvard University Press (1931, 1958),
http://books.google.com/books?id=G7IzSoUFx1YC
20. Penﬁeld, W., Boldrey, E.: Somatic motor and sensory representation in the cerebral
cortex of man as studied by electrical stimulation. Brain 60(4), 389–443 (1937)
21. Penﬁeld, W., Rasmussen, T.: The Cerebral Cortex of Man. Macmillian (1950)
22. Pettigrew, J.D.: Bi-sensory, striped representations: comparative insights from owl
and platypus. Journal of Physiology-Paris 98(1-3), 113–124 (2004)
23. Ramachandran, V., Hubbard, E.: Synaesthesia – a window into perception, thought
and language. Journal of Consciousness Studies 8(12), 3–34 (2001)
24. Rissanen, J.: Modeling by shortest data description. Automatica 14, 465–471 (1978)
25. Schluppeck, D., Glimcher, P., Heeger, D.J.: Topographic organization for delayed
saccades in human posterior parietal cortex. Journal of Neurophysiology 94(2),
1372–1384 (2005) PMID: 15817644
26. Schott, G.D.: Penﬁeld’s homunculus: a note on cerebral cartography. Journal of
Neurology, Neurosurgery, and Psychiatry 56(4), 329–333 (1993) PMID: 8482950
PMCID: 1014945
27. Shillcock, R., Kirby, S., McDonald, S., Brew, C.: Filled pauses and their status in
the mental lexicon (2001)
28. Silver, M.A., Kastner, S.: Topographic maps in human frontal and parietal cortex.
Trends in Cognitive Sciences 13(11), 488–495 (2009)
29. Solomonoﬀ, R.J.: A formal theory of inductive inference. Information and Control,
1–22, 224–254 (1964)
30. Swisher, J.D., Halko, M.A., Merabet, L.B., McMains, S.A., Somers, D.C.: Visual
topography of human intraparietal sulcus. The Journal of Neuroscience 27(20),
5326–5337 (2007)
31. Tamariz,
M.:
Exploring
systematicity
between
phonological
and
context-
cooccurrence representations of the mental lexicon. The Mental Lexicon 3, 259–278
(2008)
32. Thivierge, J., Marcus, G.F.: The topographic brain: from neural connectivity to
cognition. Trends in Neurosciences 30(6), 251–259 (2007)
33. Thom, R.: Structural stability and morphogenesis. Addison Wesley Publishing
Company (1989)
34. de Thornley Head, P.: Synaesthesia: Pitch-Colour isomorphism in RGB-Space?
Cortex 42(2), 164–174 (2006)
35. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11(2), 195–209 (1968)

Algorithmic Information Theory
and Computational Complexity
R¯usi¸nˇs Freivalds⋆
Institute of Mathematics and Computer Science, University of Latvia, Rai¸na bulv¯aris
29, Riga, LV-1459, Latvia
Abstract. We present examples where theorems on complexity of com-
putation are proved using methods in algorithmic information theory.
The ﬁrst example is a non-eﬀective construction of a language for which
the size of any deterministic ﬁnite automaton exceeds the size of a prob-
abilistic ﬁnite automaton with a bounded error exponentially. The sec-
ond example refers to frequency computation. Frequency computation
was introduced by Rose and McNaughton in early sixties and devel-
oped by Trakhtenbrot, Kinber, Degtev, Wechsung, Hinrichs and others.
A transducer is a ﬁnite-state automaton with an input and an output.
We consider the possibilities of probabilistic and frequency transducers
and prove several theorems establishing an inﬁnite hierarchy of relations.
We consider only relations where for each input value there is exactly one
allowed output value. Relations computable by weak ﬁnite-state trans-
ducers with frequency km
kn but not with frequency m
n are presented in a
non-constructive way using methods of algorithmic information theory.
1
Introduction
Physicists are well aware that physical indeterminism is a complicated phe-
nomenon and probabilistical models are merely reasonably good approximations
of reality. The problem “What is randomness?” has always been interesting not
only for philosophers and physicists but also for computer scientists. The term
“nondeterministic algorithm” has been deliberately coined to diﬀer from “inde-
terminism” [24].
Probabilistic (randomized) algorithms form one of central notions in Theory
of Computation [23,9,6,7]. However, since long ago computer scientists have at-
tempted to develop notions and technical implementations of these notions that
would be similar to but not equal to randomization.
The notion of frequency computation was introduced by G. Rose [25] as an
attempt to have an absolutely deterministic mechanism with properties similar
to probabilistic algorithms. The deﬁnition was as follows. A function f: w →w
⋆The research was supported by Agreement with the European Regional Develop-
ment Fund (ERDF) 2010/0206/2DP/2.1.1.2.0/10/APIA/VIAA/011 and by Project
271/2012 from the Latvian Council of Science.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 142–154, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Algorithmic Information Theory and Computational Complexity
143
is (m, n)-computable, where 1 ≤m ≤n, iﬀthere exists a recursive function R:
wn →wn such that, for all n-tuples (x1, · · · , xn) of distinct natural numbers,
card{i : (R(x1, · · · , xn))i = f(xi)} ≥m.
Frequency computations became increasingly popular when relation between
frequency computation and computation with a small number of queries was
discovered [3,4,20]. Many papers have been written to distinguish properties
of
frequency
algorithms from
the
properties of
probabilistic algorithms
[27,28,18,19,1,6,8,9,17].
We consider problems similar to those in the classical papers [27,18,2,12] for
ﬁnite-state transducers. We found that the situation is very much diﬀerent. We
do not attempt to present a full theory of frequency ﬁnite-state transducers.
Quite the opposite. We prove only one technical theorem which is not yet pub-
lished anywhere but this theorem has an important feature. Theorem 5 proves
the existence of certain relations and their complexity but these relations are
not shown explicitly. Their existence is proved using methods of algorithmic
information theory originated by R. Solomonoﬀ[26].
2
Tools from Algorithmic Information Theory
Deﬁnition 1. We say that the numbering Ψ = {Ψ0(x), Ψ1(x), Ψ2(x), . . .} of 1-
argument partial recursive functions is computable if the 2-argument function
U(n, x) = Ψn(x) is partial recursive.
Deﬁnition 2. We say that a numbering Ψ is reducible to the numbering η if
there exists a total recursive function f(n) such that, for all n and x, Ψn(x) =
ηf(n)(x).
Deﬁnition 3. We say that a computable numbering ϕ of all 1-argument partial
recursive functions is a G¨odel numbering if every computable numbering (of
any class of 1-argument partial recursive functions) is reducible to ϕ.
Deﬁnition 4. We say that a G¨odel numbering ϑ is a Kolmogorov numbering
if for arbitrary computable numbering Ψ (of any class of 1-argument partial
recursive functions) there exist constants c > 0, d > 0, and a total recursive
function f(n) such that:
1. for all n and x, Ψn(x) = ϑf(n)(x),
2. for all n, f(n) ≤c · n + d.
Kolmogorov Theorem. [21] There exists a Kolmogorov numbering.
3
Mirage Codes Using Algorithmic Information Theory
Linear codes form the simplest class of codes. The alphabet used is a ﬁxed choice
of a ﬁnite ﬁeld GF(q) = Fq with q elements. For most of this paper we consider
a special case of GF(2) = F2. These codes are binary codes.

144
R. Freivalds
A generating matrix G for a linear [n, k] code over Fq is a k-by-n matrix with
entries in the ﬁnite ﬁeld Fq, whose rows are linearly independent. The linear code
corresponding to the matrix G consists of all the qk possible linear combinations
of rows of G. The requirement of linear independence is equivalent to saying that
all the qk linear combinations are distinct. The linear combinations of the rows in
G are called codewords. However we are interested in something more. We need
to have the codewords not merely distinct but also as far as possible in terms
of Hamming distance. Hamming distance between two vectors v = (v1, . . . , vn)
and w = (w1, . . . , wn) in Fqk is the number of indices i such that vi ̸= wi.
The textbook [13] contains
Theorem A. For any integer n ≥4 there is a [2n, n] binary code with a mini-
mum distance between the codewords at least n/10.
However the proof of the theorem in [13] has a serious defect. It is non-
constructive. It means that we cannot ﬁnd these codes or describe them in a
useful manner. This is why P.Garret calls them mirage codes. Algorithmic infor-
mation theory was used in the paper [11] to describe this non-constructivity in
terms of algorithms and Kolmogorov complexity. A useful additional property
was also obtained which made these codes to be “nearly cyclic”.
We would wish to prove a reasonable counterpart of Theorem A for cyclic
mirage codes, but this attempt fails. Instead we consider binary generating ma-
trices of a bit diﬀerent kind. Let p be an odd prime number, and x be a binary
word of length p. The generating matrix G(p, x) has p rows and 2p columns.
Let x = x1x2x3 . . . xp. (In the crucial application of these matrices the word
x = x1x2x3 . . . xp will be of maximal Kolmogorov complexity, i.e. there will be
no possibility to compress this word in such a way that the word can be re-
stored uniquely.) The ﬁrst p columns (and all p rows) make a unit matrix with
elements 1 on the main diagonal and 0 in all the other positions. The last p
columns (and all p rows) make a cyclic matrix with x = x1x2x3 . . . xp as the ﬁrst
row, x = xpx1x2x3 . . . xp−1 as the second row, and so on.
Lemma 1. For arbitrary x, if the word h1h2h3 . . . hphp+1hp+2hp+3 . . . h2p
is a codeword in the linear code corresponding to G(p, x), then the word
hph1h2 . . . hp−1h2php+1hp+2 . . . h2p−1 is also a codeword.
Above we introduced a special type generating matrices G(p, x) where p is an
odd prime and x is a binary word of length p. Now we introduce two technical
auxiliary functions. If z is a binary word of length 2p, then d(z) is the subword
of z containing the ﬁrst p symbols, and e(z) is subword of z containing the last
p symbols. Then z = d(z)e(z).
There exist many distinct Kolmogorov numberings. We now ﬁx one of them
and denote it by η. Since Kolmogorov numberings give indices for all partial
recursive functions, for arbitrary x and p, there is an i such that ηi(p) = x. Let
i(x, p) be the minimal i such that ηi(p) = x. It is easy to see that if x1 ̸= x2,
then i(x1, p) ̸= i(x2, p). We consider all binary words x of length p and denote
by x(p) the word x such i(x, p) exceed i(y, p) for all binary words y of length p
diﬀerent from x. It is obvious that i ≥2p −1.

Algorithmic Information Theory and Computational Complexity
145
We introduce a partial recursive function μ(z, ϵ, p) deﬁned as follows. To deﬁne
μ(z, ϵ, p) we consider all 2p binary words x of the length p. If z is not a binary
word of length 2p, then μ(z, ϵ, p) is not deﬁned. If ϵ is not in {0, 1}, then μ(z, ϵ, p)
is not deﬁned. If z is a binary word of length 2p and ϵ ∈{0, 1}, then we consider
all x ∈{0, 1}p such that the ﬁrst p symbols of the codeword generated by G(p, x)
equal the last p symbols of the same codeword.
If there are no such x, then μ(z, ϵ, p) is not deﬁned. If there is only one such
x, then μ(z, ϵ, p) = x. If there are two such x, then
μ(z, ϵ, p) =

the ﬁrst such x in the lexicographical order, for ϵ = 1
the second such x in the lexicographical order, for ϵ = 0
If there are more than two such x, then μ(z, ϵ, p) is not deﬁned.
Now we introduce a computable numbering of some partial recursive func-
tions. This numbering is independent of p.
For each p (independently from other values of p) we order the set of all the
22p binary words z of length 2p: z0, z1, z2, . . . , z22p−1. We deﬁne z0 as the word
000 . . .0. We strictly follow a rule “if the word zi contains less symbols 1 than
the word zj, then i < j”. Words with equal number of the symbol 1 are ordered
lexicographically. Hence z22p−1 = 111 . . .1.
For each p, we deﬁne
Ψ0(p) = μ(z0, 0, p)
Ψ1(p) = μ(z0, 1, p)
Ψ2(p) = μ(z1, 0, p)
Ψ3(p) = μ(z1, 1, p)
Ψ4(p) = μ(z2, 0, p)
Ψ5(p) = μ(z2, 1, p)
. . .
Ψ22p+1−2(p) = μ(z22p−1, 0, p)
Ψ22p+1−1(p) = μ(z22p−1, 1, p)
For j ≥22p+1, Ψj(p) is undeﬁned.
We have ﬁxed a Kolmogorov numbering η and we have just constructed a
computable numbering Ψ of some partial recursive functions.
Lemma 2. There exist constants c > 0 and d > 0 (independent of p) such that
for arbitrary i there is a j such that
1. Ψi(t) = ηj(t) for all t, and
2. j ≤ci + d.
Proof. Immediately from Kolmogorov Theorem.
Until now we considered generating matrices G(p, x) for independently chosen
p and x. From now on we consider only special primes p and special words x.
Namely, we consider only odd primes p such that 2 is a primitive root modulo
p, and we consider only words x of length p such that i(x, p) exceed i(y, p) for

146
R. Freivalds
all binary words y of length p diﬀerent from x. It is obvious that i ≥2p −1. We
denote such an x by x(p).
We wish to prove that the matrices G(p, x(p)) have an important property: if
p is suﬃciently large, then Hamming distances between arbitrary two codewords
in this linear code is at least 4p
19.
We consider generating matrices G(p, x(p)) for linear codes where p is an odd
prime such that 2 is a primitive root modulo p, and, as deﬁned above, x(p) is a
binary word of length p such that ηi(p) = x(p) implies i ≥2p −1. We denote
the corresponding linear code by LC2(p).
Several technical lemmas are proved in [10,11]. They imply that, if p is suf-
ﬁciently large and the Hamming distance between arbitrary two codewords is
less than 4p
19 then the word x used in the deﬁnition of the generating matrix of
the linear code can be compressed. Since we choose x to be incompressible (i.e.
having maximum Kolmogorov complexity) there is a contradiction. Hence for
such linear codes Hamming distances between arbitrary two codewords are no
less than 4p
19.
4
Probabilistic Automata
We say that a language L is recognized with bounded error with an interval
(p1, p2) if p1 < p2 where p1 = sup{probx|x ̸∈L} and p2 = inf{probx|x ∈L}.
We say that a language L is recognized with a probability p > 1
2 if the language
is recognized with interval (1 −p, p).
In the preceding section we constructed a binary generating matrix G(p, p(x))
for a linear code. Now we use this matrix to construct a probabilistic reversible
automaton R(p).
The matrix G(p, x(p)) has 2p columns and p rows. The automaton R(p) has
4p + 1 states, 2p of them being accepting and 2p + 1 being rejecting. The input
alphabet consists of 2 letters.
The (rejecting) state q0 is special in the sense that the probability to enter
this state and the probability to exit from this state during the computation
equals 0. This state always has the probability
17
36. The states q1, q2, . . . , g4p
are related to the columns of G(p, x(p)) and should be considered as 2p pairs
(q1, q2), (q3, q4), . . . , . . . (q4p−1, q4p) corresponding to the 2p columns of G(p, x(p)).
The states
q1, q3, q5, q7, . . . , q4p−1 are accepting and the states q2, q4, q6, q8, . . . , q4p are re-
jecting. The initial probability distribution is as follows:
⎧
⎨
⎩
17
36, for q0,
19
72p, for each of q1, q3, . . . , q4p−1,
0, for each of q2, q4, . . . , q4p.
The processing of the input symbols a, b is deterministic. Under the input
symbol a the states are permuted as follows:

Algorithmic Information Theory and Computational Complexity
147
q1 →q3
q2 →q4
q2p+1 →q2p+3
q2p+2 →q2p+4
q3 →q5
q4 →q6
q2p+3 →q2p+5
q2p+4 →q2p+6
q5 →q7
q6 →q8
q2p+5 →q2p+7
q2p+6 →q2p+8
· · ·
· · ·
· · ·
· · ·
q2p−3 →q2p−1
q2p−2 →q2p
q4p−3 →q4p−1
q4p−2 →q4p
q2p−1 →q1
q2p →q2
q4p−1 →q2p+1
q4p →q2p+2
The permutation of the states under the input symbol b depends on G(p, x(p)).
Let the entries of G(p, x(p)) be given by
G(p, x(p)) =
⎛
⎜
⎜
⎝
g11 g12 . . . g1 2p
g21 g22 . . . g2 2p
· · · · · · · · ·
· · ·
gp1 gp2 . . . gp 2p
⎞
⎟
⎟
⎠.
For arbitrary i ∈{1, 2, . . ., p},
⎧
⎪
⎪
⎨
⎪
⎪
⎩
q2i−1 →q2i−1 , if g1i = 0
q2i →q2i
, if g1i = 0
q2i−1 →q2i
, if g1i = 1
q2i →q2i−1
, if g1i = 1.
Theorem 1. If 2 is a primitive root for inﬁnitely many distinct primes then
there exists an inﬁnite sequence of regular languages L1, L2, L3, . . . in a 2-letter
alphabet and a sequence of positive integers p(1), p(2), p(3), . . . such that for ar-
bitrary j:
1. any deterministic ﬁnite automaton recognizing Lj has at least 2p(j) states,
2. there is a probabilistic reversible automaton with (4p(j)+1) states recognizing
Lj with the probability 19
36.
Corollary 1. Assume Generalized Riemann Hypothesis. Then conclusions of
Theorem 1 hold.
5
Deﬁnitions of Transducers
A ﬁnite state transducer is a ﬁnite state machine with two tapes: an input tape
and an output tape. These tapes are one-way, i.e. the automaton never returns
to the symbols once read or written. Transducers compute relations between the
input words and output words. A deterministic transducer produces exactly one
output word for every input word processed.
In this paper we consider strengths and weaknesses of deterministic and fre-
quency transducers. Of course, if a relation is such that several output words
are possible for the same input word, then the relation cannot be computed by
a deterministic transducer. Hence for such relations there is no way to expect a
similar determinization theorem. Hence we restrict ourselves to consider in this
paper only relations producing exactly one output word for every input word
processed.

148
R. Freivalds
Deﬁnition 5. We say that a relation R(x, y) is left-total, if for arbitrary x there
is exactly one y satisfying R(x, y).
We use standard deﬁnitions of deterministic, nondeterministic and proba-
bilistic transducers, which are well-established in theoretical computer science
literature [14].
We consider all our transducers as machines working inﬁnitely long. At every
moment, let x be the word having been read from the input tape up to this mo-
ment, and let y be the word written on the output tape up to this moment. Then
we say that the pair (x, y) belongs to the relation computed by the transducer.
Deﬁnition 6. A frequency transducer with parameters (m, n) is a transducer
with n input tapes and n output tapes. Every state of the transducer is deﬁned as
(a1, a2, · · · , an)-accepting where each ai ∈{ accepting , nonaccepting }. We say
that a left-total relation R is (m, n)-computed by the transducer if for arbitrary
n-tuple of pairwise distinct input words (x1, x2, · · · , xn) there exist at least m
distinct values of xi such that the i-th output yi satisﬁes (xi, yi) ∈R.
We consider frequency transducers like all other transducers as machines work-
ing inﬁnitely long. At every moment, let x be the word having been read from
the input tape up to this moment, and let y be the word written (and accepted)
on the output tape up to this moment. Then we say that the pair (x, y) belongs
to the relation computed by the transducer. However, in the case when the in-
put words are too short, there is a problem how to deﬁne (m, n)-computation
correctly. We have concentrated on so-called weak frequency computation when
the transducer forces the input words to be of diﬀerent length.
Deﬁnition 7. We say that a frequency transducer is performing a weak (m, n)-
computation with parameters (b1, b2, · · · bn), where b1, b2, · · · bn are distinct in-
tegers, if the transducer is constructed in such a way that in the beginning of
the work the transducer reads the ﬁrst b1 symbols from the input word x1, the
ﬁrst b2 symbols from the input word x2, · · ·, the ﬁrst bn symbols from the input
word xn and at all subsequent moments reads exactly 1 new symbol from every
input tape. This ensures that at all moments the input words (x1, x2, · · · , xn) are
distinct. There is no requirement of the correctness of the results when the length
of input words is (b1, b2, · · · bn) but at all moments afterwards there exist at least
m distinct values of xi such that the i-th output yi satisﬁes (xi, yi) ∈R.
6
Frequency Transducers
First of all, it should be noted that a frequency transducer does not specify
uniquely the relation computed by it. For instance, a transducer with 3 input
tapes and 3 output tapes (2, 3)-computing the relation
R(x, y) =
⎧
⎨
⎩
true, if x = y and x ̸= 258714,
true, if y = 0 and x = 258714,
false, if
otherwise.

Algorithmic Information Theory and Computational Complexity
149
can output y = x for all possible inputs and, nonetheless, the result is always
correct for at least 2 out of 3 inputs since the inputs always distinct. Please
notice that the program of the frequency transducer does not contain the
“magical” number 258714. Hence the number of states for an equivalent deter-
ministic transducer can be enormously larger.
Theorem 2. There exists a left-total relation R (2, 3)-computed by a weak ﬁnite-
state frequency transducer and not computed by any deterministic ﬁnite-state
transducer.
Proof. We consider the relation
R(x, y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
true, if y = 1|x|
and | x |≡0(mod3),
true, if y = 1|x|
and | x |≡1(mod3)
true, if y = 1|x|−1 and | x |≡2(mod3)
false, if
otherwise.
The weak ﬁnite-state frequency (2, 3)-transducer starts with reading 1 symbol
from the ﬁrst input tape, 2 symbols from the second input tape and 3 symbols
from the third input tape and reads exactly 1 symbol from each input tape at any
subsequent moment. Hence at any moment the transducer has no more than one
input word xi with | xi |≡2(mod3). To have a correct result for the other two
input words, it suﬃces to keep the length of the output being equal the length
of the corresponding input. In case if the length of the input is | xi |≡2(mod3),
the state becomes nonaccepting.
The relation cannot be computed by a deterministic transducer because the
length of the output y decreases when the length of of the input increases from
3k + 1 to 3k + 2.
⊓⊔
This proof can easily be extended to prove the following Theorem 3.
Theorem 3. If m < n then there exists a left-total relation R (m, n)-computed
by a weak ﬁnite-state frequency transducer and not computed by any determin-
istic ﬁnite-state transducer.
Theorem 4. There exists a left-total relation R (2, 4)-computed by a weak ﬁnite-
state frequency transducer and not computed by any ﬁnite-state (1, 2)-frequency
transducer.
Proof. We consider the relation
R(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
true, if y = 1|x| and | x |≡2(mod7),
true, if y = 1|x| and | x |≡4(mod7),
true, if y = 1|x| and | x |≡5(mod7),
true, if y = 1|x| and | x |≡6(mod7),
true, if y = 0
and | x |≡0(mod7),
true, if y = 0
and | x |≡1(mod7),
true, if y = 0
and | x |≡3(mod7),
false, if
otherwise.

150
R. Freivalds
(1) The weak ﬁnite-state frequency (2, 4)-transducer starts with reading 1 symbol
from the ﬁrst input tape, 2 symbols from the second input tape and 3 symbols
from the third input tape, 4 symbols from fourth input tape and reads exactly
1 symbol from each input tape at any moment. The transducer always outputs
yi = 1|xi| on the i-th output tape. Since the transducer can count the length of
the input modulo 7, the false outputs (in cases | xi | congruent to 0,1 or 3 (mod
7)) are not accepted.
At every moment the lengths of the input words are k, k + 1, k + 2, k + 3 for
some natural k. At least two of them are congruent to 2,4,5 or 6 (mod 7).
(2) Assume that the relation is (1, 2)-computed by a transducer performing a
weak (1, 2)-computation with parameters (b1, b2). Whatever the diﬀerence d =
b2 −b1, there exists a value of s such that both s + b1 and s + b2 are congruent
to 2,4,5 or 6 (mod 7). Hence the transducer produces two wrong results on the
pair 1s+b1, 1s+b2 in contradiction with the (1, 2)-computability.
⊓⊔
Unfortunately, it is not clear how to generalize the explicit construction of the
relation R(x, y) in Theorem 4 to prove distinction between (m, n)-computability
and (km, kn)-computability for weak ﬁnite-state frequency transducers. Luckily,
there is a non-constructive method to do so. This method is based on usage of
algorithmic information theory.
Deﬁnition 8. We deﬁne a transformation I which takes words x ∈{0, 1}∗into
I(x) ∈{0, 1}∗by the following rule. Every symbol 0 is replaced by 1100110100
and every symbol 1 is replaced by 1011001010.
Deﬁnition 9. We deﬁne a transformation J which takes words x ∈{0, 1}∗into
J(x) ∈{0, 1}∗by the following rule. Every symbol 0 is replaced by 0100110100
and every symbol 1 is replaced by 0011001010.
Lemma 3. For arbitrary x ∈{0, 1}∗the result of the transformation I is a word
I(x) such that | I(x) |= 10 | x |, and I(x) contains equal number of zeros and
ones.
Lemma 4. For arbitrary x ∈{0, 1}∗the result of the transformation J is a
word J(x) such that | J(x) |= 10 | x |, and every subword y of J(x) such that
| J(x) |= 10 contains no more than 5 symbols 1.
Deﬁnition 10. We deﬁne a transformation K which takes words x ∈{0, 1}∗
into a 2-dimensional array
K(x) =
⎛
⎜
⎜
⎝
K11 K12 · · · K1n
K21 K22 · · · K2n
· · ·
· · · · · · · · ·
Kn1 Kn2 · · · Knn
⎞
⎟
⎟
⎠
of size 10 | x | × 10 | x | by the following rule. The ﬁrst row

K11 K12 · · · K1n

copies I(x). Every next row is a cyclic copy of the preceding one:
Ks1 Ks2 · · · Ksn

=
K(s−1)2 K(s−1)3 · · · K(s−1)1

.

Algorithmic Information Theory and Computational Complexity
151
Deﬁnition 11. We deﬁne a transformation L which takes words x ∈{0, 1}∗
into a 2-dimensional array
L(x) =
⎛
⎜
⎜
⎝
L11 L12 · · · L1n
L21 L22 · · · L2n
· · · · · · · · · · · ·
Ln1 Ln2 · · · Lnn
⎞
⎟
⎟
⎠
of size 10 | x | × 10 | x | by the following rule. The ﬁrst row

L11 L12 · · · L1n

copies J(x). Every next row is a cyclic copy of the preceding one:

Ls1 Ls2 · · · Lsn

=

L(s−1)2 L(s−1)3 · · · L(s−1)1

.
There is a dichotomy: 1) either there exist 4 rows (say, with numbers b1, b2, b3,
b4) in K(x) such that in every column Z such that among the values K(b1)z,
K(b2)z, K(b3)z, K(b4)z there are exactly 2 zeros and 2 ones, or 2) for arbitrary 4
rows (with numbers b1, b2, b3, b4 in K(x)) there is a column z such that among the
values K(b1)z, K(b2)z,K(b3)z, K(b4)z there are at least 3 values equal to 1. (Please
remember that by Lemma 4 the total number of zeros and ones in every row is
the same.) We will prove that if the Kolmogorov complexity of x is maximal and
the length of x is suﬃciently large then the possibility 1) does not exist.
Lemma 5. If n is suﬃciently large and x is a Kolmogorov-maximal word of
length n then for arbitrary 4 rows (with numbers b1, b2, b3, b4 in K(x)) there is
a column z such that among the values K(b1)z, K(b2)z, K(b3)z, K(b4)z there are at
least 3 values equal to 1.
Proof. Assume from the contrary that there exist 4 rows (say, with num-
bers b1, b2, b3, b4) in K(x) such that in every column z such that among the
values K(b1)z, K(b2)z, K(b3)z, K(b4)z there are exactly 2 zeros and 2 ones. By def-
inition of K, K(bi)z = K1(z+bi−1). Hence every assertion “among the values
K(b1)z, K(b2)z, K(b3)z, K(b4)z there are exactly 2 zeros and 2 ones” can be written
as “among the values K1(c1), K1(c2), K1(c3), K1(c4) there are exactly 2 zeros and
2 ones” which is equivalent to “among the values I(d1), I(d2), I(d3), I(d4) there
are exactly 2 zeros and 2 ones”.
Every value I(d) was obtained from a single letter in the word x. Namely,
the letters I(10j + 1), I(10j + 2), · · · , I(10j + 10) were obtained from the j-th
letterx(j) of x. I(10j + 1) = 1 both for x(j) being a or b. I(10j + 2) = 1 if x(j)
equals a but not b. I(10j + 3) = 1 if x(j) equals b but not a. Hence we introduce
a functional
h(x(j)) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
, if d ≡0(mod10)
x(j) , if d ≡1 or 4 or 5 or 7(mod10)
x(j) , if d ≡2 or 3 or 6 or 8(mod10)
0
, if d ≡9(mod10)
Using this functional we transform every assertion of “among the values
I(d1), I(d2), I(d3), I(d4) there are exactly 2 zeros and 2 ones” type into a Boolean

152
R. Freivalds
formula “among the values h(xj1), h(xj2), h(xj3), h(xj4) there are exactly 2 zeros
and 2 ones”.
Let a set S of such Boolean formulas be given. We say that another formula
F is independent from the set S if F cannot be proved using formulas from the
set S. For instance, if F contains a variable not present in any formula of S then
F is independent from S.
Take a large integer n and consider the set T of all binary words from {a, b}2n.
There are 22n words in T . Let T1 be the subset of T containing all words with
equal number of a’s and b’s. Cardinality of T1 equals 22n−o(n). The set S contains
2n formulas but not all of them are independent. However, since each formula
contains only 4 variables, there are at least 2n
4 independent formulas in S. Apply
one-by-one these independent formulas and removes from T1 all the words where
some formula fails. Notice that application of a new formula independent from
the preceding ones remove at least half of the words. Hence after all removals
no more than 2
3n
2 −o(n) words remain. Eﬀective enumeration of all the remaining
words and usage of Kolmogorov numbering as in Section 4 gives a method to
compress each x to a length not exceeding
3n
2 −o(n). This contradicts non-
compressibility of Kolmogorov-maximal words.
⊓⊔
Since independence of formulas in our argument was based only on the used
variables the same argument proves the following lemma.
Lemma 6. If n is suﬃciently large and x is a Kolmogorov-maximal word of
length n then for arbitrary 4 rows (with numbers b1, b2, b3, b4 in L(x)) there is
a column z such that among the values L(b1)z, L(b2)z, L(b3)z, L(b4)z there are at
least 3 values equal to 1.
We are ready to prove the main theorem of this paper.
Theorem 5. There exists a left-total relation R (3, 6)-computed by a weak ﬁnite-
state frequency transducer and not computed by any ﬁnite-state (2, 4)-frequency
transducer.
Proof. Consider the relation
R(x, y) =
⎧
⎨
⎩
true, if y = 1|x| and | x |≡j(modn) and L1j = 0
true, if y = 0
and | x |≡j(modn) and L1j = 1
false, if
otherwise.
where L(x) is as described above.
⊓⊔
References
1. Ablayev, F.M., Freivalds, R.: Why sometimes probabilistic algorithms can be more
eﬀective. In: Wiedermann, J., Gruska, J., Rovan, B. (eds.) MFCS 1986. LNCS,
vol. 233, pp. 1–14. Springer, Heidelberg (1986)
2. Austinat, H., Diekert, V., Hertrampf, U., Petersen, H.: Regular frequency compu-
tations. Theoretical Computer Science 330(1), 15–20 (2005)

Algorithmic Information Theory and Computational Complexity
153
3. Beigel, R., Gasarch, W.I., Kinber, E.B.: Frequency computation and bounded
queries. Theoretical Computer Science 163(1/2), 177–192 (1996)
4. Case, J., Kaufmann, S., Kinber, E.B., Kummer, M.: Learning recursive functions
from approximations. Journal of Computer and System Sciences 55(1), 183–196
(1997)
5. Degtev, A.N.: On (m,n)-computable sets. In: Moldavanskij, I., Gos, I. (eds.) Alge-
braic Systems, pp. 88–99. Universitet (1981)
6. Freivalds, R., Karpinski, M.: Lower Space Bounds for Randomized Computation.
In: Shamir, E., Abiteboul, S. (eds.) ICALP 1994. LNCS, vol. 820, pp. 580–592.
Springer, Heidelberg (1994)
7. Freivalds, R., Karpinski, M.: Lower Time Bounds for Randomized Computation.
In: F¨ul¨op, Z. (ed.) ICALP 1995. LNCS, vol. 944, pp. 183–195. Springer, Heidelberg
(1995)
8. Freivalds, R.: Complexity of probabilistic versus deterministic automata. In:
Barzdins, J., Bjorner, D. (eds.) Baltic Computer Science. LNCS, vol. 502,
pp. 565–613. Springer, Heidelberg (1991)
9. Freivalds, R.: Models of computation, Riemann Hypothesis and classical mathe-
matics. In: Rovan, B. (ed.) SOFSEM 1998. LNCS, vol. 1521, pp. 89–106. Springer,
Heidelberg (1998)
10. Freivalds, R.: Non-constructive methods for ﬁnite probabilistic automata. Interna-
tional Journal of Foundations of Computer Science 19(3), 565–580 (2008)
11. Freivalds, R.: Amount of nonconstructivity in ﬁnite automata. Theoretical Com-
puter Science 411(38-39), 3436–3443 (2010)
12. Freivalds, R., Zeugmann, T., Pogosyan, G.R.: On the Size Complexity of Deter-
ministic Frequency Automata. In: Dediu, A.-H., Mart´ın-Vide, C., Truthe, B. (eds.)
LATA 2013. LNCS, vol. 7810, pp. 287–298. Springer, Heidelberg (2013)
13. Garret, P.: The Mathematics of Coding Theory. Pearson Prentice Hall, Upper
Saddle River (2004)
14. Gurari, E.: An Introduction to the Theory of Computation, ch. 2.2. Computer
Science Press, an imprint of E. H. Freeman (1989)
15. Harizanova, V., Kummer, M., Owings, J.: Frequency computations and the cardi-
nality theorem. The Journal of Symbolic Logic 57(2), 682–687 (1992)
16. Hinrichs, M., Wechsung, G.: Time bounded frequency computations. Information
and Computation 139, 234–257 (1997)
17. Ka¸neps, J., Freivalds, R.: Minimal Nontrivial Space Complexity of Probabilis-
tic One-Way Turing Machines. In: Rovan, B. (ed.) MFCS 1990. LNCS, vol. 452,
pp. 355–361. Springer, Heidelberg (1990)
18. Kinber, E.B.: Frequency calculations of general recursive predicates and frequency
enumeration of sets. Soviet Mathematics Doklady 13, 873–876 (1972)
19. Kinber, E.B.: Frequency computations in ﬁnite automata. Kibernetika (2), 7–15
(1976), Russian; English translation in Cybernetics 12, 179–187 (1976)
20. Kinber, E.B., Gasarch, W.I., Zeugmann, T., Pleszkoch, M.G., Smith, C.H.: Learn-
ing Via Queries With Teams and Anomalies. In: Proceedings of COLT 1990,
pp. 327–337 (1990)
21. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems in Information Transmission 1, 1–7 (1965)
22. McNaughton, R.: The Theory of Automata, a Survey. Advances in Computers 2,
379–421 (1961)
23. Rabin, M.O.: Probabilistic automata. Information and Control 6(3), 230–245
(1963)

154
R. Freivalds
24. Michael, O.: Rabin and Dana Scott. Finite automata and their decision problems.
IBM Journal of Research and Development 3(2), 115–125 (1959)
25. Rose, G.F.: An extended notion of computability. In: Abstracts of International
Congress for Logic, Methodology and Philosophy of Science, p. 14 (1960)
26. Solomonoﬀ, R.: A Formal Theory of Inductive Inference, Part II. Information and
Control 7(2), 224–254 (1964)
27. Trakhtenbrot, B.A.: On the frequency computation of functions. Algebra i Logika 2,
25–32 (1964) (Russian)
28. Trakhtenbrot, B.A.: Frequency Algorithms and Computations. In: Becvar, J. (ed.)
MFCS 1975. LNCS, vol. 32, pp. 148–161. Springer, Heidelberg (1975)

D.L. Dowe (Ed.): Solomonoff Festschrift, LNAI 7070, pp. 155–173, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
A Critical Survey of Some Competing Accounts  
of Concrete Digital Computation 
Nir Fresco 
School of History and Philosophy, The University of New South Wales, Sydney, Australia 
Fresco.Nir@Gmail.com  
Abstract. This paper deals with the question: what are the key requirements for 
a physical system to perform digital computation? Oftentimes, cognitive 
scientists are quick to employ the notion of computation simpliciter when 
asserting basically that cognitive activities are computational. They employ this 
notion as if there is a consensus on just what it takes for a physical system to 
compute. Some cognitive scientists in referring to digital computation simply 
adhere to Turing computability. But if cognition is indeed computational, then it 
is concrete computation that is required for explaining cognition as an 
embodied phenomenon. Three accounts of computation are examined here: 1. 
Formal Symbol Manipulation. 2. Physical Symbol Systems and 3. The 
Mechanistic account. I argue that the differing requirements implied by these 
accounts justify the demand that one commits to a particular account when 
employing the notion of digital computation in regard to physical systems, 
rather than use these accounts interchangeably. 
Keywords: Concrete computation, Computability, Symbols, Semantics, 
Information Processing, Cognitive Systems, Turing Machines. 
1 
Introduction 
All too often, cognitive scientists are quick to employ the notion of computation 
simpliciter when asserting basically that cognitive activities are computational. 
Unfortunately, it seems that a clearer understanding of computation is distorted by 
philosophical concerns about cognition. Some researchers in referring to digital 
computation simply adhere to Alan Turing’s notion of computability when attempting 
to explain cognitive behaviour. Still, classical computability theory studies what 
functions on the natural numbers are computable, and not the spatiotemporal 
constraints that are inherent to cognitive phenomena. 
Any analysis of cognitive phenomena that is based solely on mathematical 
formalisms of computability, is at best incomplete. It has been proven that Emil Post’s 
machines, Stephen Kleene’s formal systems model, Kurt Gödel’s recursive functions 
model, Alonzo Church’s lambda calculus, and Turing Machines (TMs) – are all 
extensionally equivalent. They all identify the same class of functions, in terms of the 
sets of arguments and values that they determine, as computable (Kleene 2002:  
pp. 232-233).  

156 
N. Fresco 
However, concrete digital computation as it is actualised in physical systems 
seems to be a more appropriate candidate for the job of explaining cognitive 
phenomena.
1 It is not in vain that the reigning trends in contemporary cognitive 
science (whether it be connectionism or dynamicism) emphasise the embeddedness 
and embodiment of cognitive agents. This is one motivation for examining extant 
accounts of concrete computation, before we can make any sense of talk about 
'cognitive computation', 'neural computation' or 'biological computation'. 
There are many extant accounts of digital computation in physical systems on 
offer. Only three accounts are examined in this paper for lack of space.
2  
1. According to the Formal Symbol Manipulation (FSM) account, a physical system 
performs digital computation when it processes semantically interpreted (not just 
interpretable) symbols (Pylyshyn 1984: pp. 62, 72). 
2. According to the Physical Symbol Systems (PSS) account, a physical system 
performs digital computation when it consists of symbols and processes operating 
on these symbols that designate other entities (Newell 1980: p. 157). 
3. According to the Mechanistic account, a physical system performs digital 
computation if it manipulates input strings of digits, depending on the digits’ type 
and their location on the string, in accordance with a rule defined over the strings 
(and possibly the system’s internal states) (Piccinini and Scarantino 2011: p. 8). 
No novel account of computation is offered here. The goal of this paper is to 
examine the conflict among well-known accounts and argue that they imply 
sufficiently distinct requirements for a physical system to compute to justify the 
demand that one commits to a particular account when employing the notion of 
concrete digital computation. Whilst the main driver here is cognitive science, this 
demand is unbiased. It applies just as well to biology, astronomy and any other 
science in which 'computation' is employed as explanans for some physical 
phenomenon. In the following three sections, I survey the FSM, PSS and Mechanistic 
accounts respectively. In the fifth section, I defend my argument for the non-
equivalence of extant accounts of concrete computation. 
2 
The Formal Symbol Manipulation Account 
According to this account digital computing systems are formal symbol manipulators. 
They manipulate symbol tokens which themselves are representations of the subject 
matter the computation is about, in accordance with some purely formal principles 
                                                           
1 For the purposes of this paper, I shall remain neutral on whether cognition can indeed be fully 
explained computationally. Arguably, cognition involves the processing of information, and it 
is not entirely clear that information processing is equivalent to digital computation. This 
question can remain unanswered for now. 
2 These particular three accounts nicely demonstrate that extant accounts of computation are 
not only intensionally different, but also extensionally different, irrespective of their 
representational character. For a detailed analysis of Turing’s account, Hilary Putnam & John 
Searle’s trivialisation of computation, a reconstruction of Brian C. Smith’s participatory 
account and the Algorithm Execution account see Fresco (2011). 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
157 
(Scheutz 2002: p. 13). Although these manipulated symbols have both semantic and 
syntactic properties, only the latter are causally efficacious. Chief proponents of this 
account are Jerry Fodor (1975), Zenon Pylyshyn (1984; 1989) and John Haugeland 
(1985). Fodor asserts that ”computations just are processes in which representations 
have their causal consequences in virtue of their form” (Fodor 1980: p. 68). 
Haugeland’s well-known formalist’s motto stated that “if you take care of the syntax, 
the semantics will take care of itself” (Haugeland 1985: p. 106). A computing system 
as an interpreted automatic formal system takes care of the syntax. 
Furthermore, such systems are organised in three distinct levels: the semantic level, 
the symbolic level and the physical level (Pylyshyn 1989: pp. 58-59). This 
explanation framework of complex systems has some similarity to David Marr’s 
tripartite model of complex systems: the computational level, the algorithmic level 
and the physical level (Marr 1982: p. 22). At the semantic level, symbolic expressions 
are transformed in the computing system in a way that coherently preserve their 
meaning and ensures that they continue to "make sense" when semantically 
interpreted. 
Marr’s 
computational 
level, 
however, 
is 
a 
function-theoretic 
characterisation of the system in terms of the function it computes (i.e., its 
computational capacity). This computation may contingently involve the assignment 
of semantic contents. At the symbolic level the system operates in terms of 
representations and their transformations. Computing systems can operate at the 
semantic level only because of this middle level. Marr’s algorithmic level also need 
not be aligned with the symbolic level, for his analysis is not necessarily committed to 
symbol-manipulation computation. At the physical level, the state transitions of the 
computing system correspond to some symbolic expressions and are connected by 
physical laws (Pylyshyn 1984: p. 58). This particular level is indeed analogous to 
Marr’s physical level.  
The FSM account identifies six key requirements for a physical system to perform 
digital computation.
3 The first requirement is that the system be programmable to 
allow maximal plasticity of function. In order to exclude such systems as mere 
calculators and interpreted automatic systems that are not formal (e.g., analogue 
computers), the class of computing systems is restricted to those that are 
programmable (Haugeland 1985: pp. 258-259). It is one of the foundational principles 
of computer science that the operations of digital computing systems be fully 
programmable (ibid: p. 126). Despite the rigidity of the physical structure of digital 
computing systems and the interconnections of their components, these systems are 
capable of maximal plasticity of function. This plasticity is enabled by their operation 
                                                           
3 Pylyshyn argues that there is a missing requirement specifying what makes it the case that a 
symbol X represents, say, a particular daisy, rather than something else. The computational 
theory of mind has always been missing that part (Pylyshyn, personal communication). 
Specifically, he argues that the minimum function needed for this representation relation to 
obtain is that there be some causal or nomologically supported dependency between the daisy 
and X (Pylyshyn 2007: p. 82). However, it is not clear that conventional digital computing 
systems require that a similar causal relation obtain between a symbol and an external 
represented object for them to compute (a representation internal to the computing system, 
e.g., an instruction in memory, is not problematic). 

158 
N. Fresco 
being programmable to behave in accordance with any finitely specifiable function 
(Pylyshyn 1984: p. 53). It is also the basis for Turing’s vision that a computer can (in 
principle) be made to exhibit intelligent activity to an arbitrary degree (thereby 
passing the Turing’s test). 
The second requirement is that the system operate using internally represented 
rule-governed transformations of interpretable symbolic expressions. As a formal 
system, by following the formal rules of transformation operating on symbolic 
expressions the semantic interpretation must make sense of those expressions. The 
computing system operates as a black box that automatically manipulates the 
symbolic tokens according to formal rules and when interpreted they make “sense in 
the contexts in which they’re made” (Haugeland 1985: p. 106). The regularities of 
computing systems are rule-governed, rather than law-like. So any explanation of a 
computational process must make reference to what is represented by the 
(semantically interpreted) computational states and rules, rather than just to causal 
state transitions (Pylyshyn 1984: p. 57). 
Moreover, it is a key property of computing systems that semantic interpretations 
of computational states must be consistent. Since computations follow a particular set 
of semantically interpreted rules, semantic interpretations of computational states 
cannot be given capriciously (ibid: p. 58), still these interpretations need not be 
unique. This is analogous to the rules of existential generalisations, universal 
instantiations etc. that apply to formulas in virtue of their syntactic form, but their 
salient property is semantical in that they are truth preserving (Fodor & Pylyshyn 
1988: p. 29). 
The third requirement is that the computational states of the system must 
correspond to equivalence classes of physical states such that their members are 
indistinguishable from the point of view of their function (Pylyshyn 1984: p. 56). 
There exists a primitive mapping from atomic symbols to relatively elementary 
physical states, and a mapping specification of the structure of complex expressions 
onto the structure of relatively complex physical states. The structure-preserving 
mapping is typically given recursively. This ensures that the relation between atomic 
symbols (e.g., ‘A’ and ‘B’), and composite expressions (e.g., ‘A&B’), is encoded in 
terms of a physical relation between constituent states that is functionally equivalent 
to the physical relation used to encode the relation between more complex 
expressions (e.g., ‘A&B’ and ‘C’) and their composite expression (e.g., ‘(A&B)&C’). 
Furthermore, the physical counterparts of the symbolic expressions and their 
structural properties cause the behaviour of the computing system. If you change the 
symbols, the system will behave differently (Fodor & Pylyshyn 1988: pp. 14, 17). 
The fourth requirement is that the system support an arbitrarily large number of 
representations (Pylyshyn 1984: p. 62). Conventional computing systems’ 
architecture requires that there be distinct symbolic expressions for each object, event 
or state of affairs it can represent (Fodor & Pylyshyn 1988: p. 57). This raises the 
question how so many semantically interpreted operations are possible if the number 
of expressions is arbitrary large. For a fixed number of expressions some sort of a 
lookup table could be implemented. However, this is not possible for an arbitrarily 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
159 
large number of representations (Pylyshyn 1984: pp. 61-62). Instead, this capability is 
achieved by the fifth requirement. 
The fifth requirement is that the system be capable of capitalising on the 
compositional nature of expressions as determined by the constituent expressions and 
the rules used to combine them. By supporting simple rules that operate on simple 
individual symbols the system is capable of an arbitrary large number of symbolic 
expressions. Complex expressions are realised and transformed by means of 
instantiating constituent expressions of representations (ibid). The semantics of 
composite symbolic expression is determined in a consistent way by the semantics of 
its constituents (Fodor & Pylyshyn 1988: p. 16). For instance, the semantics of ‘the 
daisies in the vase on the table by the door’ is determined by ‘the daisies in the vase 
on the table’, which is determined by ‘the daisies in the vase’. Most of the symbolic 
expressions in computing systems as interpreted automatic formal systems are 
complexes, whose semantics is determined by their systematic composition 
Haugeland 1985: p. 96). 
Finally, implicitly, the sixth requirement is that the system’s functional 
architecture include an accessible memory. As in the idealised TM, a computing 
system must have a memory that allows writing of symbolic expressions and then 
reading them. This memory may consist of a running tape, a set of registers or any 
other storage media (Pylyshyn 1989: p. 56). The memory’s capacity, its organisation 
and means of accessing it are properties of the specific functional architecture of the 
system. Most modern architectures are register-based, in which symbols and symbolic 
expressions are stored and later retrieved by their numeric or symbolic address (ibid: 
pp. 72-73). Although the set of computable functions does not depend on the 
particular system implementation of the memory, the time complexity of computation 
does vary (retrieving a particular string from a table could be, under certain 
conditions, be made independent of the number of strings stored and the size of the 
table) (Pylyshyn 1984: p. 97-99). 
3 
The Physical Symbol Systems Account 
According to this account, championed by Allen Newell and Herbert Simon, digital 
computing systems are physical symbol systems.4 They consist of sets of symbols, 
which are physical patterns that can occur as components of symbol structures or 
expressions (Newell and Simon 1976: p. 116). Computing systems also include a 
collection of processes that operate on these expressions to create, modify or destroy 
other expressions. Further, a physical symbol system is situated in a world of objects 
that is wider than just these symbolic expressions. The PSS account is indeed similar 
to the FSM account (as will be shown below), but there are also some differences that 
cannot be easily dismissed.  
                                                           
4 The PSS hypothesis deals primarily with the intelligence of symbol systems and relates to 
minds and artificial intelligence. I will mostly limit my discussion to the PSS account of 
computation. 

160 
N. Fresco 
Newell and Simon (1976: p. 117) maintained that a physical symbol system is an 
instance of a Universal Turing machine (UTM). They discovered that UTMs always 
contain within them a particular notion of symbol and symbolic behaviour. 
Tautologically, physical symbol systems are universal (Newell 1980: p. 155). Their 
capacity to solve problems is accomplished by producing and progressively 
modifying symbol structures until they produce a solution structure, particularly by 
means of a heuristic search5 (Newell and Simon 1976: p. 120). There are two basic 
aspects to each search, namely its object (i.e., what is being searched) and its scope 
(i.e., the set of objects within which the search is conducted). In computing systems 
each aspect must be made explicit in terms of specific structures and processes, since 
a system cannot search for an object that it cannot recognise (Haugeland 1985: p. 
177). Computing systems (as all UTMs) solve problems mostly by using heuristic 
search, for they have limited processing resources. 
The PSS account identifies seven key requirements for a physical system to 
perform digital computation. The first requirement is that the system consist of a set 
of symbols and a set of processes that operate on them and produce through time an 
evolving collection of symbolic expressions. At any given time, the system contains a 
collection of symbolic structures and processes operating on expressions to produce 
other expressions. Such processes are the creation, modification, reproduction and 
destruction of symbolic expressions through time (ibid: p. 116). These processes 
operate on and transform internal symbolic structures, or in other words the system 
executes computer programs that operate on data structures (Bickhard and Terveen 
1995: p. 92). 
The second requirement is that the system either affect a designated object or 
behave in ways that are dependent on that object. An entity (i.e., a symbol) X 
designates (i.e., is about or stands for) an entity (e.g., an object or a symbol) Y 
relative to a process P, if when X is P’s input, P’s behaviour depends on Y. 
Designation is grounded in the physical behaviour of P when its action could be at a 
distance if X (the input to P) stands for a distal object. This ‘action at a distance’ is 
accomplished by a mechanism of access (that is realised in physical computing 
systems) to three types of entities: symbol structures, operators6 and roles in symbol 
structures. The set of processes includes programs, whose input could also be 
symbolic expressions. If an expression can be created at Ti that is dependent on an 
entity in some way, processes can exist in the system that at Ti+1, take that expression 
as input and behave in a way dependent on that entity. Thus, these expressions 
designate that entity (Newell 1980: pp. 156-157). 
The third requirement is that the system be capable of interpreting an expression, if 
it designates some process and given that expression the system can execute that 
process. Interpretation is defined as the act of accepting an expression that designates 
a process as input and then executing that process (ibid: pp. 158-159). This is similar 
                                                           
5 It is not clear that artificial digital computing systems (e.g., physical instantiations of UTMs) 
must use heuristic search as the only means for solving computational problems. Clearly, 
many algorithms are not based on any search mechanism, but rather a finite sequence of 
instructions to solve a particular problem. 
6 Operators are symbols that have an external semantics built into them (Newell 1980: p. 159). 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
161 
to the process of indirectly executing computer programs by an interpreter program. 
The interpreter reads an expression E as input and if it is recognised as a program (or 
a procedure), rather than a data structure, it is then executed. This capability is 
necessary to allow the flexibility of UTMs to create expressions for their own 
behaviour and then produce that very behaviour. The total processes in the computing 
system can be decomposed to the basic structure of (control + (operators + data)) that 
is paradigmatic in all programming languages. The control continuously brings 
together operators and data to yield the desired behaviour. 
The fourth requirement is the existence of expressions that designate every process 
of which the machine is capable (Newell and Simon 1976: p. 116). This requirement 
is self-explanatory and is necessary to support the full plasticity of behaviour of 
UTMs. 
The fifth requirement is that the system be capable of distinguishing between some 
expressions as data and others as programs (Newell 1980: p. 166). This is a property 
of all UTMs that must be able to recognise some expressions as data when creating or 
modifying them at time Ti and then interpret them as programs at time Tj. The concept 
of universality, which is one of Turing’s seminal contributions, unifies data and 
programs by way of the UTM taking programs of other (simulated) machines as data 
(as well as the inputs inscribed on the tapes of those simulated machines).  
The sixth requirement is that the system have a stable memory to ensure that once 
expressions are created they continue to exist until they are explicitly modified or 
deleted (Newell and Simon 1976: p. 116). This requirement stems from the coupling 
of read/write operations in computing systems. Each of these operations requires its 
counterpart to be productive in affecting the system’s behaviour. A read operation 
only retrieves expressions that were written to memory (and persisted). Conversely, a 
write operation of expressions, which are never subsequently read, is redundant 
(Newell 1980: p. 163). 
Lastly, the seventh requirement is that the system be capable of handling an 
unbounded number of expressions and realising the absolute maximal class of 
input/output functions using these expressions. This requirement is weaker than the 
requirement for unbounded memory. The structural requirements for universality are 
not dependent on unbounded memory. Rather they are dependent on the system’s 
capability to handle an unbounded number of expressions (Newell and Simon 1976: 
p. 116) and realise the absolute maximal class of input/output functions using these 
expressions (Newell 1980: p. 178). 
4 
The Mechanistic Account of Computation 
According to the Mechanistic account, proposed by Gualtiero Piccinini (2007), digital 
computing systems are digit-processing mechanisms. They are mechanisms, which 
can be ascribed the function of generating output strings from input strings in 
accordance with a general rule (or map) that applies to all strings and depends on the 
input strings and (possibly) internal states for its application (ibid: p. 516). This 
account relies essentially on three conceptual elements: I. Medium independence of 

162 
N. Fresco 
the vehicles (digits) processed. They could be implemented in a variety of ways (such 
as mechanical components, electronic components, optical components etc.); II. The 
function of the system is to process those vehicles irrespective of their particular 
physical implementation; III. The operation of the system is performed in accordance 
with rules, which need not necessarily be algorithms or programs (as in the case of 
special purpose TMs or finite state automata, hereafter FSA).  
Moreover, the mathematical notion of computation (i.e., computability) only 
applies directly to abstract systems, such as TMs or FSA, but not to physical systems. 
Computability is typically defined over strings of letters (often called symbols) from a 
finite alphabet (ibid: pp. 509-510). But not every process that is defined over strings 
of letters counts as computation (e.g., the generation of a random string of letters). To 
overcome this gap, Piccinini (2007: pp. 510-512) introduces the notion of a digit as 
the concrete counterpart to the formal notion of a letter. A digit is a stable state of a 
component that is processed by the mechanism.7 Strings of digits (i.e., sequences of 
digits) can be either data or rules, so they are essentially the same kind of thing and 
differ only in the functional role they play during processing by the computing system 
(Piccinini and Scarantino 2011: pp. 7-8). Digits are permutable. Components that 
process digits of one type are functionally capable of processing digits of any other 
type. 
The mechanistic account identifies four key requirements for a physical system to 
perform digital computation. The first requirement is that the system process tokens of 
the same digit type in the same way and tokens of different digit types in different 
ways. Under normal conditions, digits of the same type in a computing system affect 
primitive components of the system in sufficiently similar ways, thereby their 
dissimilarities make no difference to the output produced. For instance, two inputs to 
a XOR gate that are sufficiently close to a certain voltage (labelled type '1') yield an 
output of a voltage sufficiently close to a different specific value (labelled type '0'). 
However, that does not imply that for any two input types, a primitive component 
always yields outputs of different types. Two different inputs can yield the same 
computational output, such in the case of a NOR gate. Input types '1,1', '0,1' and '1,0' 
give rise to outputs of type ‘0’. Still, it is essential that the NOR gate yield different 
responses to tokens of different types, thus responding to input types '0,0' differently 
from other input types. Differences between digit types must suffice for the 
component to differentiate between them, so as to yield the correct outputs. 
The second requirement is that the system process all digits belonging to a string 
(of digits) during the same functionally relevant time interval and in a way that 
respects the ordering of the digits within that string. When a computing system is 
sufficiently large and complex, there has to be some way to ensure synchronisation 
among all digits belonging to a particular string. The components of a computing 
system interact over time, and given their physical characteristics, there is only a 
                                                           
7 In ordinary electronic computers digits are states of physical components of the machine 
(e.g., memory cells). In other cases, such as old punched card computers, strings of digits 
were implemented as sequences of holes (or lack thereof) on cards (Piccinini, personal 
communication). 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
163 
limited amount of time during which their interaction can produce the correct result, 
which is consistent with the ordering of digits within strings. In primitive computing 
components and simple circuits it is mostly the temporal ordering of digits that is 
responsible for producing the correct result. So if, for example, digits, which are 
supposed to be summed together, enter an adder mechanism at times that are too far 
apart, they will not be added correctly (ibid: p. 513). In more complex components, 
processing of all digits belonging to a string must proceed in a way that also respects 
the spatial ordering of the digits within the string. Each digit in the sequence must be 
processed until we reach the last digit in the string. In some atypical cases the 
ordering of digits makes no difference to a computation (e.g., summing up all the 
numbers in an array or calculating the length of a sequence of symbols). 
The third requirement is that all the system’s components that process digits stabilise 
only on states that count as digits. Components can be in one of several stable states. In 
a binary computing system memory cells, for instance, can be in either of two stable 
states, each of which constitutes a digit. Upon receiving some physical stimulus (e.g., 
the pressing of a key), a memory cell enters a state on which it must stabilise. Memory 
cells stabilise on states corresponding to either of two digit types, typically labeled '0' 
and '1', that are processed by the computing system. If memory cells did not have the 
capacity to stabilise on one of these digit types, the memory would cease to function as 
such and the computer would cease to operate normally (ibid: p. 511). 
The fourth requirement is that the components of the system be functionally 
organised and synchronised so that external inputs, together with the digits stored in 
memory, be processed by the relevant components in accordance with a set of 
instructions.8 During each time interval, the processing components transform 
external input (if such exists) and previous memory states in a manner that 
corresponds to the transition of each computational state to its successor. The external 
input combined with the initial memory state constitute the initial string of a particular 
computation. Intermediate memory states constitute the relevant intermediate strings. 
Similarly, the output produced by the system (together with the final memory state) 
constitutes the final string. As long as the components of the system are functionally 
organised and synchronised so that their processing respects the well-defined ordering 
of the manipulated digits, the operation of the system can be described as a series of 
snapshots. The computational rule specifies the relationship that obtains between 
inputs and their respective outputs produced by modifying snapshots according to a 
set of instructions (ibid: pp. 509, 515). 
5 
Discussion 
The literature contains many attempts to clarify the notions of computation simpliciter 
and digital computation, in particular. Matthias Scheutz, for example, has argued that 
                                                           
8 Strictly, this requirement applies to systems that Piccinini dubs “fully digital” computing 
systems (Piccinini, personal communication). Other systems, which he dubs “input-output” 
digital computing systems, take digital inputs and produce digital outputs in accordance with 
a rule, but do not execute a step-by-step program (e.g., some connectionist networks). 

164 
N. Fresco 
there is no satisfactory account of implementation to answer questions critical for 
computational cognitive science (1999: p. 162). He does not offer a new account of 
concrete computation. Instead he suggests approaching the implementational issue by 
starting with physical digital systems progressively abstracting away from some of 
their physical properties until a (mathematical) description remains of the function 
realised. In a similar vein, David Chalmers (2011) also focuses on the 
implementational issue, only to offer a new mathematical formalism of computability 
that is based on combinatorial state automata (supplanting the traditional finite state 
automata). He too argues that a theory of implementation is crucial for (digital) 
computation to establish its foundational role in cognitive science. The motivation 
behind both Scheutz and Chalmers’ efforts to clarify the notion of implementation is 
to block attempts by Putnam and Searle (and others) to trivialise computation (and 
undermine computationalism). 
Other notable discussions of computation in cognitive science include David Israel 
(2002), Oron Shagrir (2006), Piccinini (2006, Piccinini & Scarantino 2011), Smith 
(2002, 2010) and the (long) list continues. Israel (2002) claims that often it seems that 
a better understanding of computation is hampered by philosophical concerns about 
mind or cognition. Yet “[o]ne would, alas, have been surprised at how quick and 
superficial such a regard [to computation] has been” (ibid: p. 181). Shagrir (2006) 
examines a variety of individuating conditions of computation showing that most of 
them are inadequate for being either too narrow or too wide. Although he does not 
provide a definitive answer as to what concrete computation is, he points out that 
neither connectionism nor neural computation nor computational neuroscience is 
compatible with the widespread assumption that digital computation is executed over 
representations with combinatorial structure. 
Importantly, two uncommon examples of genuine attempts to explicate the notion 
of computation are Piccinini's and Smith's. Piccinini (2006) demonstrates how on 
various readings of computation, some have argued that computational explanation is 
applicable to psychology, but not, for instance, to neuroscience. Still, neuroscientists 
routinely appeal to computations in their research. Elsewhere, Piccinini examines the 
implications of different types of digital computation (as well as their extensions’ 
relations of class inclusion) for computational theses of cognition (Piccinini and 
Scarantino 2011). 
But as far as I am aware, nobody else in the literature has ever undertaken a more 
ambitious project than Smith to systematically examine the extant accounts of 
computation and their role in both computer science and cognitive science. In his 
2002 “The foundations of computing”, Smith lists the following six construals of 
computation: FSM, Effective Computability, Algorithm Execution, Digital State 
Machines, Information Processing and PSS. 9 His Age of Significance project (which 
is now long coming) aims to shed some light on the murky notion of computation, 
putting each one of these construals under careful scrutiny (Smith 2010). Surprisingly 
enough, Smith concludes that there is no adequate account of computation and never 
                                                           
9 In an unpublished chapter from the Age of Significance, Smith adds the following construals: 
Calculation of a Function, Interactive Agents, Dynamics and Complex Adaptive Systems. 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
165 
will be one. For computers per se are not “a genuine subject matter: they are not 
sufficiently special” (ibid: p. 38). Pace Smith, I do not believe that there is a 
compelling reason to reject all accounts of computation as inadequate, let alone to 
preclude the possibility of ever coming up with an adequate account. Still, I strongly 
agree that the accounts are different and many of them are indeed inadequate for 
explaining concrete computation. 
My main argument here proceeds as follows: 
• 
(P1) There are many accounts of digital computation at our disposal. 
• 
(P2) These accounts establish different (but not all irreducibly different) 
requirements for a physical system to perform digital computation. 
• 
(P3) Therefore, extant accounts of computation are non-equivalent. 
• 
(P4) Cognitive capacities are sometimes explained by invoking digital 
computation terminology. 
• 
(P5) When employing an equivocal interpretation, one needs to commit 
to an explicit interpretation (or account). 
• 
Therefore, one needs to commit to an explicit account of computation 
when explaining cognitive capacities by invoking digital computation 
terminology. Specifically, any computational thesis of cognition is 
unintelligible without a commitment to a specific account of 
computation. 
The truth of the first premise is evident in the philosophical literature (cf. Piccinini 
2007; Shagrir 1999; and Smith 2002, 2010). In addition to the FSM, PSS and 
Mechanistic accounts examined here, there are also the Algorithm Execution account, 
the Gandy-Sieg account, the Information Processing account as well as others. 
Similarly, premise four (at least) seems self-evident. Computationalists take 
premise four for granted (Fodor 1975, Pylyshyn 1984, Newell & Simon 1976, Marr 
1982, van Rooij 2008) and so do some connectionists. Radical dynamicists do not 
subscribe to the computational theory of mind (Van Gelder & Port 1995, Thelen & 
Smith 1994), yet they reject it without committing to any particular account of 
computation proper. For they presuppose that digital computation is inherently 
representational. Other dynamicists do not deny that some aspects of cognition may 
be representational and be subject to a computational explanation. 
Yet, 
this 
presupposition 
is 
unjustified. 
Digital 
computation 
(but 
not 
computationalism) could be explained without invoking any representational 
properties (barring internal representations) by appealing to causal or functional 
properties instead (see Fresco 2010 and Piccinini 2008a). As van Rooij (2008: p. 964) 
rightly points out, computation and computationalism have become associated with 
the symbolic tradition, but only sometimes with specific models in this tradition. 
Some accounts of concrete digital computation are indeed representational (cf. the 
reconstruction of Smith’s participatory account in Fresco (2011) as well as the FSM 
and PSS accounts discussed above), but others need not be (cf. Copeland 1996, 
Chalmers 1994, the Mechanistic account discussed above). This simply reinforces the 
need to commit to a particular account of computation. 

166 
N. Fresco 
Moreover, premise five simply calls for disambiguation when there is an 
equivocation in terms. When some phenomenon is open to two interpretations or 
more, we should commit to one interpretation to avoid ambiguity. For instance, the 
concept depression has at least two typical meanings. In the sentence, “The great 
depression started in most countries in 1929 and lasted for a long time”, it is clear that 
‘depression’ means a long-term downturn in economic activity. On the other hand, in 
the sentence, “Long depression leads to making irrational decisions”, ‘depression’ 
means something different. Similarly, when one asserts that hierarchical planning or 
linguistic tasks, for example, are computational, one ought to commit to a particular 
account of computation.10 Is it in virtue of executing an algorithm, formally 
manipulating symbols, or implementing a TM that cognitive agents engage in 
hierarchical planning?  
Furthermore, the commitment to a particular interpretation should be consistent to 
avoid further ambiguity. From the two sentences above it follows that irrational 
decisions were made in the countries that suffered the Great Depression in 1929. This 
conclusion would only validly follow from its premises, if 'depression' had the same 
interpretation in both premises. Otherwise, whilst this conclusion may be plausible, it 
does not follow. This is also known as the fallacy of equivocation. Similarly, if one 
explains a particular cognitive capacity in virtue of an explicit account of concrete 
digital computation, one has to consistently adhere to that account. An explanation of 
a linguistic task in virtue of formal symbol manipulation and then in virtue of 
algorithm execution ceases to be a coherent story, since they are not equivalent. 
Importantly, not only are the extant accounts of concrete digital computation 
intensionally different, they are also extensionally different. These accounts offer 
different perspectives on what a physical computing system does. But rather than 
having the same extension, these accounts end up denoting different classes of 
computing systems. For example, the second requirement of the FSM account 
excludes computing systems that are neither program-controlled nor programmable11. 
For such systems do not follow semantically represented rules, instead the “rules” are 
hardwired. A physical symbol system is explicitly classified by Newell and Simon as 
an instance of a UTM (1976: p. 117). This classification is also derivable from the 
conjunction of the fourth and fifth requirements of the PSS account. Also, the FSM 
and PSS accounts exclude computing systems such as Gandy machines12 and discrete 
neural networks, for they violate Turing’s locality condition and do not necessarily 
operate on explicit symbolic expressions. The Mechanistic account, on the other hand,  
 
                                                           
10 To be clear, digital computation is not ambiguous in the same sense that depression is. The 
aforementioned accounts offer specific proposals for how 'digital computation' should be 
understood, but they are still related by a more general sense of 'digital computation'. The 
example above is simply meant to emphasise the need for disambiguation. 
11 A special purpose TM is an example of a program-controlled system that is not 
programmable. 
12 A Gandy machine (introduced by Turing’s student, Robin Gandy in 1980) can be 
conceptualised as multiple TMs working in parallel, sharing the same tape and possibly 
writing on overlapping regions of it. 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
167 
is far less restrictive in terms of the systems it classifies as digital computing systems, 
including UTMs, and special purpose TMs, but also FSAs, discrete neural networks, 
primitive logic gates and even hypercomputers (see figure 1). 
 
 
The Mechanistic Account –  
 
Logic gates, Flip-flops, Discrete neural nets, hypercomputers 
                             
     The FSM Account – 
                                             
     Special purpose TMs 
                                                             The PSS Account -  
                       
    
    Universal TMs 
 
 
Fig. 1. With the exception of hypercomputers, UTMs are the most powerful and flexible 
computing systems in the class above (e.g., they can simulate any discrete neural net). Still, 
UTMs (and physical approximations thereof) do not exhaust all types of digital computing 
systems. The Mechanistic account is the broadest of the three accounts examined. 
Prima facie, it might seem that premise two is self-defeating, but this is not the 
case. A possible consequence of all the requirements not being irreducibly different is 
some overlap between requirements of various accounts. Thus, the requirements that 
are implied by one account could be reduced to some of the other requirements.13 And 
if all the requirements could be reduced to a coherent minimal set of key 
requirements, then this would constitute a single account of computation. Premise 
three would then no longer follow from the preceding premises. However, premise 
two suggests that although some of the requirements may overlap, not all of them do. 
For instance, the fourth key requirement of the Mechanistic account presupposes in 
some cases the existence of memory (whose cells stabilise on certain digits). Similarly, 
the sixth requirement of the PSS account and the sixth requirement of the FSM account 
both demand memory for storing and retrieving symbolic expressions. Still, some 
requirements of one account, such as spatiotemporal synchronisation of processing 
digits belonging to the same string (i.e., the Mechanistic account second requirement), 
cannot be reduced to any of the requirements of the competing accounts. 
Possible challenges to my conclusion might be that some of the key requirements 
implied by different accounts could be synthesised or that one could simultaneously 
subscribe to two accounts or more. The first challenge may result in sidestepping the 
demand to commit to an explicit account. But even if that were the case, such a 
synthesis would simply yield a new (possibly adequate!) account of computation. The 
second challenge needs unpacking. It can be interpreted in one of two ways. Firstly, it 
could be interpreted as subscribing to more than one account simultaneously for 
                                                           
13 An overlap among requirements clearly does not imply a reduction from one requirement to 
another. My intent here is to address a possible criticism to the effect that premise three 
would no longer follow as an intermediate conclusion from its preceding premises. 

168 
N. Fresco 
explaining different cognitive capacities respectively. I do not see that as a problem. 
There is still a need to commit to a particular account for each relevant cognitive 
capacity. But this could have some other consequences, such as explaining cognitive 
behaviour in a non-unified manner by resorting to a plethora of computational 
models. It will require a compelling account of how the different computational 
(cognitive) subsystems interrelate. 
Secondly, the challenge could be interpreted as subscribing to several accounts 
simultaneously, since cognitive explanations by nature span multiple levels. This is 
consistent with Marr’s (1982) tripartite analysis. For instance, we could hold that (1) 
cognitive computations are inherently representational. At the same time, we could 
also hold without being inconsistent that (2) these computations are constrained in 
terms of any one of the formalisms of computability, and lastly that (3) they occur in 
the brain, which is embodied and situated in the real world. This is all well and good. 
Still, as I have argued above, concrete computation (but perhaps not cognitive 
computation) could be explained without necessarily invoking any representational 
properties (e.g., by the Mechanistic account above or the Algorithm Execution 
account in Copeland 1996). If one wishes to commit to a representational account of 
digital computation, since cognition is representational, one should firstly justify why 
computation proper is representational. Also, subscribing to an account of concrete 
computation and to a formalism of computability simultaneously does not introduce 
any conflict. 
Although some of the key requirements of the three accounts overlap, others do 
not, suggesting that there is sufficient dissimilarity between these accounts. For 
example, the conjunction of the fourth14 and fifth requirements of the PSS account can 
be reduced to the FSM account’s first requirement. The PSS account’s fourth and fifth 
requirements amount to the universality property of soft-programmable computing 
systems that is achieved by means of symbolic expressions used either as data or as 
programs ensuring maximal plasticity of function. In addition, both the FSM 
account’s fourth requirement and the first part of the PSS account’s seventh 
requirement demand the capacity to handle an unbounded number of representations15 
(or symbolic expressions designating some entities). 
Another seemingly important similarity between the FSM and PSS accounts (but 
not the Mechanistic account) is that computing systems engage in information 
processing at the symbolic level. For instance, Fodor and Pylyshyn (1988: p. 52) 
claim that “conventional computers typically operate in a 'linguistic mode', inasmuch  
 
                                                           
14 The PSS account’s fourth requirement demands the existence of expressions that represent 
every process of which the machine is capable to support the full plasticity of behaviour of 
the computing system. But it is not clear why it is necessary that every such expression exist. 
There could be a mismatch between the set of all functions and the set of all expressions 
representing them. For instance, some functions could be the serial invocation of several 
expressions (themselves representing other functions). 
15 Only the FSM account explicitly states that the unbounded number of representations is 
produced by means of compositionality. 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
169 
as they process information by operating on syntactically structured expressions”. As 
well, Newell and Simon’s (1972: p. 870) fundamental working assumption is that “the 
programmed computer and human problem solver are both species belonging to the 
genus IPS” (that is information processing systems). The Mechanistic account, on the 
other hand, does not equate information processing and digital computation.16 Still, it 
is not clear in what sense the information-processing characterisation of computing 
systems adds anything operative to the classification of certain physical systems as 
performing digital computation. This is stipulating that processing of information 
amounts to the production, modification and deletion of information. 
A well-known non-semantic reading of information is based on Claude Shannon’s 
concept of information (1948), but it is not clear what processing of Shannon 
Information amounts to. His theory dealt with information syntactically: whether and 
how much information is conveyed. Its basic idea is coding messages into a binary 
system at the bare minimum of bits we need to send to get our message across while 
abstracting from the physical media of communication. The amount of information 
conveyed is defined as the uncertainty (or entropy) associated with particular 
messages. The deletion and modification of information is only possible in the 
presence of noise. Noisy channels may displace information, but this is not the same 
as a deliberate deletion of information in computing systems, say to free up memory 
resources or reduce the size of a database. Although error detection and correction 
methods modify information to offset noise, symbolic expressions could be modified 
for many purposes other than error correction. The production of new information is 
more problematic, for the only source of new information, according to Shannon 
(1948: p. 12), is uncertainty. 
Another possible non-semantic reading of information is based on Algorithmic 
Information, which was introduced by Ray Solomonoff and Andrei Kolmogorov. But 
even on this reading, it is not immediately clear what information processing amounts 
to in the context of concrete computation. The algorithmic information of a string X is 
defined as the length of the shortest program on a UTM that generates X as its output. 
Algorithmic information seems a more suitable candidate as the basis of an 
information processing characterisation of computation. For it is defined over 
algorithms, rather than over randomness of messages. Yet, the problem of processing 
algorithmic information remains, as it is invariant to the process of computation itself.  
Importantly, as stated by Solomonoff, the actual value of Kolmogorov complexity 
of a string is incomputable, it can only be approximated (2009: pp. 6-7). This 
limitation prevents us from actually having a full description of all the possible 
optimal algorithms (that are also enumerable) to solve a specific problem (Calude 
2009: p. 82, Calude et al, 2011). Still, a variation on algorithmic information theory, 
which is not based on UTMs but rather on Finite State Transducers, does allow us to 
compute the complexity of strings. This variation, however, comes at the cost of 
                                                           
16 Instead, according to the Mechanistic account, information processing entails generic 
computation (the superclass of both digital computation and analogue computation) 
(Piccinini and Scarantino 2011: pp. 33-34). For information is a medium-independent 
concept. However, digital computation does not entail information processing, because 
although digits could carry information, they need not do so essentially.  

170 
N. Fresco 
Turing universality that does not apply to finite state transducers, since there is no 
universal transducer (Calude et al, 2011). Yet, algorithmic information theory will 
have a limited capacity to explain cases in which information is deleted and/or 
modified whilst the overall information complexity of the computing system does not 
decrease. 
Other possible candidates for the information processing characterisation of 
computing systems alluded to by the FSM and PSS accounts are based on a semantic 
reading of information. Two main types of semantic information are factual 
information and instructional information. The former type is objective propositional 
information representing some facts or states of affairs, and arguably only qualifies as 
information if it is true (yet, this is a contentious claim). The latter type is not about 
facts or state of affairs, so it is not qualified alethically (Floridi 2009: pp. 35-36). 
Instructional information is conveyed either unconditionally (e.g., step 1: do this, step 
2: do that) or conditionally (e.g., if X do this, otherwise do that). The subtleties of 
semantic information are not discussed further here for lack of space. However, since 
algorithms are finite sets of instructions, instructional information seems a plausible 
candidate as the basis of characterising digital computation as information processing.  
Moreover, the Mechanistic account is grounded in physical mechanisms that 
perform computations, whereas both the FSM and PSS accounts are grounded in 
symbolic computation and semantics. Digits in the Mechanistic account are not 
symbols, but rather states of components (and are as physical as it gets). So, they have 
no representational character and their processing is independent of any (external) 
semantics. The second requirement of the FSM account, in contrast, emphasises that 
symbolic expressions are manipulated according to formal rules and must always be 
semantically interpretable even following numerous manipulations. The second 
requirement of the PSS account emphasises that symbols are manipulated in virtue of 
their semantics. 
Incidentally, the semantics of symbols and their manipulation is a key difference 
between the PSS and FSM accounts. Although both accounts are based on the 
manipulation of symbols at the heart of the computational process, they diverge on 
how semantics enters this process. According to the FSM account’s second 
requirement, symbols are formally manipulated in virtue of their syntax, but they are 
always semantically interpretable. It is a property of automatic formal systems that 
symbolic expressions continue to “make sense” when manipulated by truth-preserving 
rules. On this view, the formal manipulation of symbols based on their syntax is 
sufficient for the operation of the computing system. And the semantics of the 
manipulated symbolic expressions is epiphenomenal on their syntax.  
However, the second requirement of the PSS account reveals that processes in 
computing systems are causally affected by the semantics of symbols. The behaviour 
of a process P (with X as its input) depends on a potentially distal entity Y, which is 
designated by X. The designation requirement is vague, for it leaves the ways in 
which a process depends on some entity unspecified. It might be the case that Newell 
and Simon took it for granted that symbols symbolise by definition and so they have 
not explicated where their external semantics comes from. If indeed external 
semantics is required for computation, then this gap is too big to be left unexplicated. 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
171 
Internal access to symbols and expressions in a conventional digital computer is an 
assignment operation of, say, a symbol to some other internal entity and it is a 
primitive in its architecture (e.g., for memory retrieval). But there is no similar 
primitive for the external environment (Bickhard and Terveen 1995: pp. 93-94).  
Additionally, 
the 
Mechanistic 
account 
emphasises 
the 
importance 
of 
synchronisation of processing digits belonging to the same string, whereas both the 
PSS and FSM accounts ignore temporal constraints of concrete computation. 
According to the second requirement of the Mechanistic account, with the growth in 
complexity of the computing system it becomes more crucial that digits belonging to 
the same string be processed in the same functionally relevant time interval. The other 
two accounts, while recognising the temporal aspects of concrete computation, do not 
explicitly mandate any temporal constraints on computing systems. 
In sum, the above differences discussed as well as others clearly confirm that the 
extant accounts of concrete digital computation are not equivalent. The key 
motivation behind both the FSM and PSS accounts is advancing a substantive 
empirical hypothesis about how human cognition works, namely, that cognition is 
essentially a computational system of the specified sort. The Mechanistic account, on 
the other hand, has a different and less ambitious motivation. Rather than advancing 
an empirical hypothesis about cognition. Piccinini's goal in formulating his account is 
to provide a general characterisation of digital computing systems. He attempts to 
exclude as many paradigmatic cases of non-computing systems (such as planetary 
systems, digestive systems, mouse traps, etc.) as possible. At the same time, his 
account classifies many (but not too many) systems as performing digital 
computation. The FSM account is more restrictive and excludes any systems that are 
neither programmable nor program-controlled from the class of computing systems. 
The PSS account is even more restrictive, as it includes only UTMs (and physical 
approximations thereof) as genuinely computational.17 Regardless of the (doubtful) 
representational character of computation presumed by the FSM and PSS accounts, 
they are simply too restrictive as accounts of concrete computation proper. 
6 
Conclusion 
There is no question whether mathematical formalisms of computability are adequate 
analyses of abstract computation, but they are of the wrong kind to explain concrete 
computation. Any particular formalism does not specify the relationship between 
abstract and concrete computation. It is at the physical level that the algorithm (or 
more precisely, program) is specified and constrained by the implementing physical 
medium. So, stipulating that any complete account of a physical phenomenon must 
also consider its physical implementation, an explicit account of concrete 
computation has to be specified for a complete account of concrete computing 
systems. 
                                                           
17 PSS yields a very restrictive class of computing systems that makes sense when considering 
cognitive systems. Since cognition exhibits substantial flexibility, it is unreasonable to 
assume that it is an instance of, say, a special purpose TM. 

172 
N. Fresco 
My main argument was that well-known accounts of concrete computation entail 
sufficiently distinct requirements for a physical system to compute, justifying the 
demand that one commits to a particular account when employing the notion of 
concrete computation. But despite the apparent straightforwardness of this argument, 
all too often its implied moral is surprisingly ignored by philosophers and cognitive 
scientists alike. The notions of computation simpliciter and digital computation, in 
particular, are employed without much awareness of what they mean exactly. At 
times, extant accounts are even used interchangeably as though they were equivalent 
(when they are not even extensionally equivalent). If we take cognition to be a 
physical phenomenon that can be explained computationally, we should state 
explicitly what we mean by (digital) computation. Otherwise, a computational thesis 
of cognition remains unintelligible. 
 
Acknowledgments. Thanks are due to Eli Dresner, Gualtiero Piccinini and Frances 
Egan for providing helpful comments on a recent draft of this paper. I would also like 
to thank two anonymous referees for the Solomonoff 85th memorial conference for 
useful comments. I am grateful to Phillip Staines for detailed comments on various 
drafts of this paper. All these comments contributed to this final version, which, I 
trust, is much improved. 
References 
1. Bickhard, M.H., Terveen, L.: Foundational issues in artificial intelligence and cognitive 
science: Impasse and solution. Elsevier Scientific, Amsterdam (1995) 
2. Calude, C.S.: Information: The algorithmic paradigm. In: Sommaruga, G. (ed.) Formal 
Theories of Information. LNCS, vol. 5363, pp. 79–94. Springer, Heidelberg (2009) 
3. Calude, C.S., Salomaa, K., Roblot, T.K.: Finite state complexity. Theoretical Computer 
Science 412(41), 5668–5677 (2011), doi:10.1016/j.tcs.2011.06.021 
4. Chalmers, D.: On implementing a computation. Minds and Machines 4, 391–402 (1994) 
5. Chalmers, D.J.: A computational foundation for the study of cognition. Journal of 
Cognitive Science 12(4), 323–357 (2011) 
6. Copeland, B.J.: What is computation? Synthese 108, 335–359 (1996) 
7. Floridi, L.: Philosophical conceptions of information. In: Sommaruga, G. (ed.) Formal 
Theories of Information. LNCS, vol. 5363, pp. 13–53. Springer, Heidelberg (2009) 
8. Fodor, J.A.: The language of thought. Harvard University Press, Cambridge (1975) 
9. Fodor, J.A.: Methodological solipsism considered as a research strategy in cognitive 
science. Behavioral and Brain Sciences 3, 63–73 (1980) 
10. Fodor, J.A., Pylyshyn, Z.W.: Connectionism and cognitive architecture: a critical analysis. 
Cognition 28, 3–71 (1988) 
11. Fresco, N.: Explaining computation without semantics: keeping it simple. Minds and 
Machines 20, 165–181 (2010) 
12. Fresco, N.: Concrete Digital Computation: What Does it Take for a Physical System to 
Compute? Journal of Logic, Language and Information 20(4), 513–537 (2011), 
doi:10.1007/s10849-011-9147-8 
13. Gandy, R.: Church’s thesis and principles for mechanisms. In: Barwise, J., Keisler, H.J., 
Kunen, K. (eds.) The Kleene Symposium, pp. 123–148. North-Holland, Amsterdam (1980) 

 
A Critical Survey of Some Competing Accounts of Concrete Digital Computation 
173 
14. Haugeland, J.: AI: the very idea. The MIT Press, Cambridge (1985) 
15. Israel, D.: Reflections on Gödel’s and Gandy’s reflections on Turing’s thesis. Minds and 
Machines 12, 181–201 (2002) 
16. Kleene, S.C.: Mathematical logic. Dover, New York (2002) 
17. Marr, D.: Vision: a computational investigation into the human representation and 
processing visual information. Freeman & Company, New York (1982) 
18. Newell, A., Simon, H.A.: Human problem solving. Prentice-Hall, Englewood (1972) 
19. Newell, A., Simon, H.A.: Computer science as an empirical enquiry: symbols and search. 
Communications of the ACM 19, 113–126 (1976) 
20. Newell, A.: Physical symbol systems. Cognitive Science 4, 135–183 (1980) 
21. Piccinini, G.: Computational explanation in neuroscience. Synthese 153, 343–353 (2006) 
22. Piccinini, G.: Computing mechanisms. Philosophy of Science 74, 501–526 (2007) 
23. Piccinini, G.: Computation without representation. Philosophical Studies 137, 205–241 
(2008a) 
24. Piccinini, G.: Computers. Pacific Philosophical Quarterly 89, 32–73 (2008b) 
25. Piccinini, G., Scarantino, A.: Information processing, computation, and cognition. Journal 
of Biological Physics 37, 1–38 (2011) 
26. Pylyshyn, Z.W.: Computation and cognition. The MIT Press, Cambridge (1984) 
27. Pylyshyn, Z.W.: Computing in cognitive science. In: Posner, M.I. (ed.) Foundations of 
Cognitive Science, pp. 51–91. The MIT Press, Cambridge (1989) 
28. Pylyshyn, Z.W.: Things and places: how the mind connects with the world (Jean Nicod 
Lectures). The MIT Press, Cambridge (2007) 
29. Scheutz, M.: When physical systems realize functions. Minds and Machines 9, 161–196 
(1999) 
30. Scheutz, M.: Computationalism – the next generation. In: Scheutz, M. (ed.) 
Computationalism: New Directions, pp. 1–22. The MIT Press, Cambridge (2002) 
31. Shagrir, O.: What is computer science about? The Monist 82, 131–149 (1999) 
32. Shagrir, O.: Why we view the brain as a computer. Synthese 153, 393–416 (2006) 
33. Shannon, C.E.: A mathematical theory of communication. Mobile Computing and 
Communications Review 5, 1–55 (1948) 
34. Smith, B.C.: The foundations of computing. In: Scheutz, M. (ed.) Computationalism: New 
Directions, pp. 23–58. The MIT Press, Cambridge (2002) 
35. Smith, B.C.: Age of significance: Introduction (2010), 
http://www.ageofsignificance.org (retrieved May 3, 2010) 
36. Solomonoff, R.J.: Algorithmic probability - theory and applications. In: Emmert-Streib, F., 
Dehmer, M. (eds.) Information Theory and Statistical Learning, pp. 1–23. Springer 
Science+Business Media, NY (2009) 
37. Thelen, E., Smith, L.B.: A dynamical systems approach to the development of cognition 
and action. The MIT press, Cambridge (1994) 
38. van Gelder, T., Port, R.F.: It’s about time: an overview of the dynamical approach to 
cognition. In: van Gelder, T., Port, R.F. (eds.) Mind as Motion. The MIT Press, Cambridge 
(1995) 
39. van Rooij, I.: The tractable cognition thesis. Cognitive Science 32, 939–984 (2008) 

Further Reﬂections on the Timescale of AI
J. Storrs Hall
Institute for Molecular Manufacturing, Palo Alto, CA 94301 USA
Abstract. Solomonoﬀ[9] explored the possibilities of the future course
of AI development, including social eﬀects of the development of intel-
ligent machines which can be produced with exponentially decreasing
costs. He introduced arguably the ﬁrst formal mathematical model of
what has since come to be known as the technological Singularity. Since
that time a veritable plethora of such models has appeared [8]. We ex-
amine the milestones and model in light of 25 years more experience,
and oﬀer a revised version.
Milestones for AI
Ray Solomonoﬀ, in [9], proposes a list of milestones for a discussion of the further
course of AI. Brieﬂy, they are:
A. Dartmouth, 1956: AI began as a distinct enterprise. This is the only milestone
which was considered to have been accomplished at the time of the paper.
B. A general theory of problem solving is achieved: it would cover such ca-
pabilities associated with intelligence as learning, concept formation, and
knowledge representation.
C. A self-improving machine (or program) is created.
D. Computers are able to learn by reading existing natural language text.
E. An AI has a problem solving capacity equivalent to a human.
F. An AI has a problem solving capacity near that of computer science
community.
G. An AI has a problem solving capacity many times that of CS community.
In the intervening 25 years, much has happened, but milestone A is still the only
one which has actually occured. However, at least something has been learned,
so we can say more about the milestones as they might happen in the future.
Milestone B, a general theory, was presumably to be the sort of crowning
formalization to AI that the science of thermodynamics was to the engineering
of steam engines. In some sense, Solomonoﬀ’s own work and the succeeding
eﬀorts by such researchers as Levin and Hutter remain the best attempts we
have in that direction. Furthermore, there is a substantial subﬁeld of machine
learning theory, and outside of AI there has grown up a substantial ﬁeld of
optimal control theory and estimation. We might well look for the ultimate
origins of an AI theory in the uniﬁcation of these ﬁelds.
Even so, it should be remembered that engineers had been building steam
engines for a century before Carnot conceived the basic principles of thermody-
namics. What current best theories of AI lack is the ability to analyze a given
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 174–183, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Further Reﬂections on the Timescale of AI
175
learning or problem-solving design and predict its performance, the way we can
analyze any given steam engine using thermodynamics. Historical precedent tells
us that we will probably have working AIs, built heuristically and improved by
experimentation, before we have a proper theory. Thus we should expect Mile-
stone B to be one of the later, rather than one of the earlier, ones to occur.
It is not clear that Milestones C and D are distinct, especially as AI has
developed since 1985. Consider how a human learns: the vast majority (for many
people, the entirety) of what we learn is not original discoveries but the takeup
of culturally accumulated knowledge from peers, parents, teachers, and books. A
completely competent AI might operate exactly the same way; it would not be a
Newton or an Einstein (nor indeed a Solomonoﬀ!), but it would be human-level
as measured against the average representative of homo sapiens.
This is not to say that such a machine would not need endogenous learning
capability. It is clear that human learning is not simply storing symbolic repre-
sentations, but the construction of mental programs for prediction and control.
Contrast, for example, the verbal instruction with the internal control-system
formation attendant on learning to ride a bicycle. Solomonoﬀwrites:
... many of the more interesting human activities are mainly per-
formed by the unconscious mind. If the unconscious mind works very
much like the conscious mind (but we are merely less aware of its work-
ings), then there is no diﬃculty here. However, if as is widely suspected,
the unconscious mind is signiﬁcantly diﬀerent from the conscious, then
the present expansion of expert systems will have serious limitations.
And so it proved. Today’s more competent AI and robotic systems have, in
addition to the logical superstructure of the 80s’ expert systems, a substantial
semantics of their domains of expertise comprised of predictive simulation codes,
control systems, statistical models derived from vast corpuses of relevant data,
and so forth. An AI that learned from reading (or other symbolic instruction)
would presumably use the symbolic input to form a scaﬀolding that would greatly
accelerate, but not eliminate, the subsequent (perhaps search-based) program-
construction eﬀort.
Thus at least one reasonable view of the ultimate shape of learning machines
implies that milestones C and D are complementary, rather than separable,
achievements.
If the capabilities of a C-D AI are integrated into an appropriate cogni-
tive architecture, one would be fairly close to milestone E, human equivalence.
Solomonoﬀ[10] discusses the qualities desirable in a human-equivalent (or bet-
ter) machine:
I would like it to give a better understanding of the relation of quan-
tum mechanics to general relativity. I would like it to discover cures for
cancer and AIDS. I would like it to ﬁnd some very good high tempera-
ture superconductors. I would not be disappointed if it were unable to
pass itself oﬀas a rock star.

176
J.S. Hall
It seems reasonable to imagine that if such a machine had a human brain’s equiv-
alent of processing power, it might make discoveries at the rate of a human. (We
discuss this in more detail below.) If so, milestones E, F, and G are primarily
a function of available processing power. When we have a working version of a
human-equivalent AI, we can readily duplicate it into one or more research com-
munities, corporations, or similar enterprises. It might or might not be possible
to integrate minds, enhance communication, or the like, by redesigning various
mental processes and/or applying more processing power or memory to a given
individual AI than the human brain aﬀords. However, for the sake of this (or
Solomonoﬀ’s original) argument, that would be merely an optimization and not
necessary to the main point.
Singularity
[9] contains arguably the ﬁrst formal model of an increase in processing power
leading to a mathematical singularity of intellectual capacity.1 This predicted an
asymptotic rise at a date that depended on various model parameters, and very
likely was inﬂuential in the adoption of the name “Singularity” for the concept.
The intuition for such a hyperbolic growth curve is straightforward. Suppose
that technological progress can be parceled into a series of ﬁxed-size problems,
the solution of each of which can be done with some ﬁxed number x of machine
operations. Current progress, with roughly a ﬁxed number of human minds (and
thus a linear rate of problems solved), appears capable of inducing an exponential
improvement rate in computing hardware (measured in operations per dollar).
The rate is usually measured in time, but in the model, could just as well be
measured in (the linearly related) total number of problems solved.
Once we can obtain, for an aﬀordable outlay, problem-solving capability in
AI form of a size comparable to the current technical community, the dynamic
changes. If we double the brains available, we halve the time to solve the next
problem in our list. Since we’re measuring computational improvement in prob-
lems solved, the next generation of computers is available in half the time, and
(again for a ﬁxed cost) we can redouble our problem-solving ability. Thus the
next set of problems is solved in one quarter the original time, and the next in
one eighth, and so forth, providing us the solution of an inﬁnite series of problems
in a ﬁnite time.
As Solomonoﬀhastens to point out,
Usually, when inﬁnities like this one occur in science, they indicate
a breakdown of the validity of the equations as we approach the inﬁnity
point.
He proceeds to speculate that the breakdown in this case would be the end
of the Moore’s Law computational improvement. In practice, it could be any
of a vast number of things. Computational speedup is only one of the many
1 Von Neumann and Good had earlier advanced informal models, and Asimov pre-
dicted “an Intellectual Revolution” by analogy to the Industrial Revolution.

Further Reﬂections on the Timescale of AI
177
feedback loops in the improvement of technology as a whole. Most of these
have to go through various physical processes (starting with building the capital
equipment which fabricates microchips, but including such mundane necessities
as the transportation of raw materials and ﬁnished machines). Any one of these
would form a bottleneck if the rest were signiﬁcantly accelerated.
There is a second, perhaps more subtle problem with a hyperbolic model.
As the problem-solving intervals decrease exponentially in duration, causing an
asymptotic increase in problems solved, they also cause an asymptotic increase
in the ﬁxed-per-interval expenditure for machinery. In other words, the model
consumes an inﬁnite amount of physical resources in ﬁnite time, as well.
We can form a more realistic model of the self-improvement process by con-
sidering a sector where Moore’s Law is strongly active already, namely the semi-
conductor industry.
Moore’s Law
Consider the (US) semiconductor industry over the past 15 or so years, where
a measurable Moore’s Law has been operating. Notably, in this period, design
automation has contributed enormously to the ability of mere human beings
to design the staggeringly complex microprocessors and other components that
make up current-day computers. This ranges from architecture to electronic
phenomena to geometric design of chip masks to allow for refraction eﬀects of
the UV light used to expose the chips under construction.
Thus feedback from increasing processing power and software sophistication
in semiconductor manufacture is an excellent proxy for the kind of increased
smarts to be expected in an AI takeoﬀ, and in a sector with nicely measurable
results. Furthermore, for the past 15 years, the industry (in the US2) has been
remarkably stable: it can be considered statistically to have a constant annual
output value of $75 billion, spending $15 billion on raw material, $8.25 billion
on capital equipment, and $12.75 billion on research and development.3 The
number of production workers has been more or less even over the period at
about 85 thousand, but the number of non-production workers has shrunk from
93 thousand in 1997 to 62 thousand in 2007.
Meanwhile the performance of the product has been increasing at a faster-
than-exponential rate:
The equation for the quadratic ﬁt is f(t) = 0.000723t2 −2.69t + 2496, i.e.
ops/sec/$1000 is predicted to be Q(t) = 10f(t)for t the year number.4 This is
2 The semiconductor industry in other regions, notably Asia/Paciﬁc, has expanded
considerably over the period.
3 US Bureau of the Census, Economic census numbers for semiconductor manufactur-
ing sector (NIACS code 334413), economic census for 1992, 1997, 2002, and 2007.
cf. http://factﬁnder.census.gov/home/saﬀ/main.html? lang=en. After correcting for
correlation with the overall US economy, a null hypothesis of the steady state given
cannot be rejected at a 95% conﬁdence level.
4 Figures before 2000 from Kurzweil 2005, ﬁgures from 2004 and 2011 from machines
constructed by the author.

178
J.S. Hall
Fig. 1. Moore’s Law with a quadratic growth-rate ﬁt, from Kurzweil’s and the author’s
data
the form of the integral of the product of a linear term and an exponential, which
suggests a model with a linear network eﬀect as well as capital re-investment.
Given that the raw material input to the industry is essentially constant and
the dollar value is constant, Q(t) can be taken to represent the intelligence with
which the atoms of raw materials are rearranged into products; a relatively pure
measure of innovation. Thus the semiconductor industry as a whole manages a
Q(t) improvement from a ﬁxed 28% reinvestment rate.
According to the curve, cost of computation has dropped by a factor of 152,000
since 1985. The net present value of a human engineer of the sort participating
in the semiconductor industry is about $1 million. The amount of computation
necessary to host an AI at a human level, given reasonably optimal algorithms,
can be estimated at (very roughly) 100 teraops, or 1e14 ops/sec. On the curve,
this will be available for $1 million in 2013, and for $1000 in 2026.
A Series of Growth Modes
Hanson [3] analyzes the economic history of the human race as a series of distinct
changes in the growth rate. At least two major accelerations can be identiﬁed.
The growth rate for humanity’s gross output jumped by about a factor of 250
with agriculture, and by a factor of 60 with the Industrial Revolution.
The fact that the Moore’s Law growth rate is changing lends credence to a
proposition that it might be in the process of shifting from one mode to another.
Hanson speculates that the next jump in the series, assuming the pattern
holds, might well occur within the coming century: the industrial economy has
seen roughly as many doubling times as the agrarian period did.
We ﬁnd the analysis of historical growth as a series of growth modes with
intervening phase changes to be compelling, but the endogenous model in which
phase changes are precipitated simply by the number of doubling times from a
previous one to be little short of numerology. We will advance an alternative
theory below.

Further Reﬂections on the Timescale of AI
179
Fig. 2. Hanson’s data for agrarian and industrial growth modes, with the Renaissance
/ Industrial Revolution knee between
What Technology Wants
Von Neumann [6] hypothesized what he called the “complexity barrier:”
There is thus this completely decisive property of complexity, that there
exists a critical size below which the process of synthesis is degererative,
but above which the phenomenon of synthesis, if properly arranged, can
become explosive, in other words, where syntheses of automata can pro-
ceed in such a manner that each automaton will produce other automata
which are more complex and of higher potentialities than itself.
Three recent studies of the phenomenon of technological progress in human
communities [5,4,7], give credence to von Neumann’s concept of the complexity
barrier. Technological progress in human societies depends on the complexity –
size and network eﬀects – of the economic and intellectual community within
which economic and intellectual intercourse are possible. Isolated societies, even
though composed of people just as industrious and intelligent as the larger ones,
tend to regress and lose technologies instead of progressing and inventing new
ones. Aboriginal Tasmania is a classic example of this phenomenon. There seems
to be some point of complexity above which a society is self-improving, and below
which it is not.
We hypothesize that the complexity barrier is closer to milestone F (CS com-
munity) than to E (single human) – and possibly past it.
Kelly’s work in particular is strongly based on the notion that at any particular
state of knowledge, there is a certain set of inventions available to be discovered
as the next step. Thus in some sense, technological creativity is analogous to
explorers in a landscape following valleys and mountain passes; the directions
they take are determined by the landscape rather than their internal compasses.
Visionary geniuses who are ahead of their time – da Vinci, Babbage, and Drexler

180
J.S. Hall
spring to mind – ﬁnd their ideas impossible to implement. Ideas on the frontier
of the technium, to use Kelly’s term for the corpus of technical knowledge at a
given time, tends to be independently and simultaneously reinvented. Substantial
empirical evidence is given for this position.
An Exogenous Model of Technological Growth
On this view, Hanson’s model of growth-rate phase changes can be reinterpreted.
Instead of some internal dynamic in the technium producing a phase change after
some number of doubling times, the roughly regular progression of phase shifts
can be better explained as a result of the terrain in the idea-space through which
the technium is expanding. A fractal distribution of “fertile valleys” – of volumes
of idea-space aﬀording rapid growth and high productivity – would account for
the overall shape of the series of growth phase changes. The timing and other
parameters, however, would depend on particulars of the terrain and could not
be predicted in detail from the preceding series. Valleys would occur at random
with a frequency inversely proportional to their sizes.
On this view, the technium is an expanding volume in an idea space of high
dimensionality. When it contacts a valley, expansion into the valley proceeds at
a higher-than-normal rate, producing the super-exponential growth characteris-
tic of a phase shift. Once the valley is saturated, growth reverts to the simple
exponential but at a higher rate due to the increased size and dimensionality of
the frontier.
For example, a very early technium consisting of ﬁre, clothing, crude shelter,
and chipped ﬂint tools – the “Mousterian toolkit” – vastly expanded the inven-
tory of geographical locations in which humans could live. This aﬀorded a higher
population growth rate than a steady-state existence in clement climes. In this
case the entropy increase in the accessible space was very literally determined
by the actual physical terrain.
While it is perilous to generalize, it is possible to discern a pattern to the phase
changes, which we hope tells us something about the typical forms of the ter-
rain. Arguably, each phase change in physical capability is accompanied by (and
somewhat preceded by) a corresponding increase in informational capability:
Informational development physical development
Manual dexterity
Mousterian toolkit
Language
Agriculture
Writing
Civilization
Printing press, science
Industrial Revolution
Internet, AI
?
One striking aspect of the list is that each stage involved mastery of some
self-replicating phenomenon: ﬁre, crops, ideas, machine tools, and, we presume,
ultimately programs.
It is diﬃcult to attribute growth rates to many of these phenomena, but
the penultimate pair has a compelling symmetry: with the introduction of the

Further Reﬂections on the Timescale of AI
181
printing press and the acceptance of the scientiﬁc method, scientiﬁc publication
shifted from a growth mode esentially the same as the existing overall economic
one of about 0.1% to about 5%. Within a few centuries – very quickly on the
timescales under discussion – physical economic output shifted into a 5% growth-
rate mode.
The speculation that suggests itself is that the core of an idea-space valley
aﬀording a given growth mode is surrounded by a halo, which the frontier of the
technium reaches ﬁrst, of techniques by which information can be handled in the
growth mode. The obvious inference is that current Moore’s Law information
technology growth will be completed by a revolution in physical capability that
brings the rest of the economy up to a Moore’s Law-like growth rate. We cannot
say where this will stabilize; our Q(t) ﬁt indicates a growth rate of 100%, doubling
time one year, around 2069.
A (Very Vague) Prognostication
Here is a revised list of milestones, with speculation on when they might occur:
1. Solomonoﬀ’s C and D, a program able to learn from the corpus of human-
readable information, might well happen in the coming decade, although it
would not be too surprising if it happened in the 2020s instead.
2. Convincing evidence of the economic productivity of 1. causes capital to ﬂow
into AI in a manner similar to the build-up of the internet in the 1990s.
3. Within a decade, programs have been constructed and educated to the point
of being productive scientists, engineers, doctors, lawyers, accountants, etc.
Investment in these is in proportion for the demand for them in the overall
economy.
4. A Moore’s Law-like trend in physical manufacturing, already visible today
in technologies like rapid prototyping, produces by 2030 or so a capability
for Moore’s Law-like growth rates in physical capital formation.
5. AI inventors and designers are necessary – and suﬃcient – to ﬁnd something
useful to do with the capacity in 4.
6. The conﬂuence of design and manufacturing capability in 5., and neither
factor by itself, will make it possible for the economy as a whole to move
into a 70% to 100% annual growth mode.
The key to natural language understanding has been known since Winograd’s
work [11] in the 1970s. With a deep semantic model – programs able to simulate,
predict, and perform what is being talked about – just about any parsing method
will work, albeit some more eﬃciently than others. The “road not taken” in AI
was to combine this insight with machine learning and automatic programming
into a system which could use linguistic context to formulate learning problems
at the frontier of its knowledge, e.g. picking up the meaning of new words from
context, that were small enough to be tractable to the machine learning and
automatic programming techniques.
Since that time, machine learning has acquired a substantial theoretical basis
and an arsenal of powerful methods. Automatic programming has received less

182
J.S. Hall
attention but computing power is now such that relatively primitive approaches
such as genetic programming have achieved substantive results, e.g. patentable
designs.
Meanwhile, interest in inferring information from text has burgeoned, along
with the amount of text available on the Internet. The rapidly increasing amount
of video available means that the primary human venue for learning new words
– examples of their use in reference to objects and phenomena than can be
independently seen and heard – is now a viable pathway to language acquisition.
Progress in the ﬁeld is such that an estimate of success within a decade is mildly
optimistic but not outrageously so.
The single most important determiner of the economic growth rate is the pro-
ductivity of capital: how long it takes a given unit of capital to produce an equiv-
alent unit of product. Currently this is about 15 years, for a growth rate of 5%.
Moore’s original observation had to do with shrinking transistors, making
them not only more numerous and cheaper but faster as well. The same phe-
nomenon holds for physical devices: physical production machinery with parts
the size of current VLSI transistors (22 nm) could operate at megahertz me-
chanical frequencies [1], making them thousands of times faster than current
machines at capital-replication tasks.
To sum up: the “Singularity” can best be thought of as the second half of the
information technology revolution, extending it to most physical and intellectual
work. Overall economic growth rates will shift from their current levels of roughly
5% to Moore’s Law-like rates of 70% to 100%. The shift will probably take on
the order of a decade (paralleling the growth of the internet), and probably fall
somewhere in the 30s or 40s.
Reﬂections on Social Eﬀects
If we built a system of 1000 AIs, all running 1000 times faster than biological
humans and connected by appropriate communications networks, it would be
diﬃcult to avoid the impression that a hyperhuman intellect had been created.
But it wouldn’t, in principle, be able to understand anything that those humans
couldn’t have understood given enough centuries. In some sense, the point is
moot: No individual can understand all that the scientiﬁc community knows, in
any event. What happens in the future will become more and more fantastic,
and understandable only in broad, vague generalities by current-day humans,
whether done by AIs or ﬂesh-and-blood humans. With AIs it may happen faster,
but it will follow the same track. Given a graph of knowledge and capabilities
as they would have increased in a purely human future, the one with AIs is the
same, following the same valleys in idea space, but with the dates moved up.
The scientiﬁc community in some sense isn’t composed of people, but of ideas.
Human brains are a substrate, and AIs can be, if we are reasonably careful how
we build them, a similar substrate. This is particularly obvious in the case of AIs
which learn primarily from uptake of the cultural corpus. AIs will be composed
entirely of human ideas, and everything they ever think of will be traceable back
to us in direct line of memetic descent.

Further Reﬂections on the Timescale of AI
183
There is one proviso: a future in which we understand AI could well be dif-
ferent from one in which we don’t understand it [2]. It seems possible that the
knowledge of how to build a formal, mechanical system that nevertheless exhibits
learning, adaptability, and common sense, could revolutionize the eﬀectiveness
of our corporate and political structures — whether we built physical robots or
not.
We can only end by agreeing with Solomonoﬀ’s closing sentiment:
What seems most certain is that the future of man – both scientﬁic
and social – will be far more exciting than the wildest eras of the past.
References
1. Drexler, K.E.: Nanosystems: Molecular Machinery, Manufacturing, and Computa-
tion. Wiley Interscience, New York (1992)
2. Hall, J.S.: Beyond AI: Creating the Conscience of the Machine. Prometheus,
Amherst (2007)
3. Hanson, R.: Long-term growth as a sequence of exponential modes. Tech. rep.,
George Mason University (2000), http://hanson.gmu.edu/longgrow.pdf
4. Johnson, S.: Where Ideas Come From. Penguin/Riverhead, New York (2010)
5. Kelly, K.: What Technology Wants. Viking, New York (2010)
6. von Neumann, J.: Theory of Self-Reproducing Automata. University of Michigan
(1966)
7. Ridley, M.: The Rational Optimist. Harper, New York (2010)
8. Sandberg, A.: An overview of models of technological singularity. In: AGI10 work-
shop on Roadmaps to AGI and the Future of AGI (2010)
9. Solomonoﬀ, R.J.: The time scale of artiﬁcial intelligence: Reﬂections on social ef-
fects. Human Systems Management 5, 149–153 (1985)
10. Solomonoﬀ, R.J.: Machine learning - past and future. In: AI@50, The Dartmouth
Artiﬁcial Intelligence Conference (2006)
11. Winograd, T.: Understanding Natural Language. Academic Press (1972)

 
D.L. Dowe (Ed.): Solomonoff Festschrift, LNAI 7070, pp. 184–197, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
Towards Discovering the Intrinsic Cardinality  
and Dimensionality of Time Series Using MDL 
Bing Hu1, Thanawin Rakthanmanon1, Yuan Hao1, Scott Evans2,  
Stefano Lonardi1, and Eamonn Keogh1 
1 Department of Computer Science & Engineering, University of California,  
Riverside, CA 92502, USA 
2 GE Global Research 
{bhu002,rakthant,yhao002}@ucr.edu, evans@ge.com, 
{stelo,eamonn}@cs.ucr.edu 
Abstract. Most algorithms for mining or indexing time series data do not 
operate directly on the original data, but instead they consider alternative 
representations that include transforms, quantization, approximation, and multi-
resolution abstractions. Choosing the best representation and abstraction level 
for a given task/dataset is arguably the most critical step in time series data 
mining. In this paper, we investigate techniques that discover the natural 
intrinsic representation model, dimensionality and alphabet cardinality of a time 
series. The ability to discover these intrinsic features has implications beyond 
selecting the best parameters for particular algorithms, as characterizing data in 
such a manner is useful in its own right and an important sub-routine in 
algorithms for classification, clustering and outlier discovery. We will frame the 
discovery of these intrinsic features in the Minimal Description Length (MDL) 
framework. Extensive empirical tests show that our method is simpler, more 
general and significantly more accurate than previous methods, and has the 
important advantage of being essentially parameter-free.   
Keywords: Time Series, MDL, Dimensionality Reduction. 
1 
Introduction 
Most algorithms for indexing or mining time series data operate on higher-level 
representations of the data, which include transforms, quantization, approximations 
and multi-resolution approaches. For instance, Discrete Fourier Transform (DFT), 
Discrete Wavelet Transform (DWT), Adaptive Piecewise Constant Approximation 
(APCA) and Piecewise Linear Approximation (PLA) are models that all have their 
advocates for various data mining tasks and each has been used extensively [3]. 
However the question of choosing the best abstraction level and/or representation of 
the data for a given task/dataset still remains open. In this work, we investigate this 
problem by discovering the natural intrinsic model, dimensionality and (alphabet) 
cardinality of a time series. We will frame the discovery of these intrinsic features in 
the Minimal Description Length (MDL) framework [11] [21] [28]. MDL is the 

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
185 
 
cornerstone of many bioinformatics algorithms [7][20], but it is arguably 
underutilized in data mining [9].  
The ability to discover the intrinsic dimensionality and cardinality of time series 
has implications beyond setting the best parameters for data mining algorithms, as 
characterizing data in such a manner is useful in its own right to understand/describe 
the data and an important sub-routine in algorithms for classification, clustering and 
outlier discovery. To illustrate this, consider the three unrelated datasets in Fig.1. 
 
Fig. 1. Three unrelated industrial time series with low intrinsic cardinality. I) Evaporator 
(channel one). II) Winding (channel five). III) Dryer (channel one). 
The number of unique values in each time series is, from left to right, 14, 500 and 
62. However, we might reasonably claim, that the intrinsic alphabet cardinality is 
instead 2, 2, and 12 respectively. As it happens, an understanding of the processes that 
produced these data would perhaps support this claim [10]. In these datasets, and 
indeed in many real-world datasets, there is a big difference between the actual and 
intrinsic cardinality. Similar remarks apply to dimensionality. 
Before we define more precisely what we mean by actual versus intrinsic 
cardinality, we should elaborate on the motivations behind our considerations. Our 
objective is generally not simply to save memory: if we are wastefully using eight 
bytes per time point instead of using the mere three bytes made necessary by the 
intrinsic cardinality, the memory space saved is significant, but memory is getting 
cheaper every day, which is rarely a bottleneck in data mining tasks. There are instead 
many other reasons why we may wish to find the true intrinsic model, cardinality and 
dimensionality of the data: 
• 
There is an increasing interest in using specialized hardware for data mining [24]. 
However, the complexity of implementing data mining algorithms in hardware 
typically grows super linearly with the cardinality of the alphabet. For example, 
FPGAs usually cannot handle cardinalities greater than 256[24]. 
• 
Some data mining algorithms benefit from having the data represented in the 
lowest meaningful cardinality. As a trivial example, in the stream: ..0, 0, 1, 0, 0, 1, 
0, 0, 1, we can easily find the rule that a ‘1’ follows two appearances of ‘0’.  
However, notice that this rule is not apparent in this string: ..0, 0, 1.0001, 0.0001, 
0, 1, 0.000001, 0, 1 even though it is essentially the same time series. 
• 
Most time series indexing algorithms critically depend on the ability to reduce the 
dimensionality [3]or the cardinality [16]of the time series (or both[1][2]) and 
searching over the compacted representation in main memory. However, setting 
the best level of representation remains a black art. 
• 
In resource-limited devices, it may be helpful to remove the spurious precision 
induced by a cardinality/dimensionally that is too high (see example below). 
• 
Knowing the intrinsic model, cardinality and dimensionality of a dataset allows 
us to create very simple outlier detection models. We simply look for data where 
20
40
60
80
100
0
I
100
200
300
400
500
0
II
0
300
600
900
III

186 
B. Hu et al. 
 
the parameters discovered in new data differ from our expectations learned on 
training data. This is a simple idea, but as we show, it can be very effective.     
 
To enhance our appreciation of the potential utility of knowing the intrinsic 
cardinality and dimensionality of the data, we briefly consider an application in 
classification. Suppose we wish to build a time series classifier into a device with a 
limited memory footprint such as a cell phone, pacemaker, or autonomous robotic 
drone. Let us suppose we have only 20kB available for the classifier, and that (as is 
the case with the benchmark dataset, TwoPat) each time series exemplar has a 
dimensionality of 128 and takes 4 bytes per value [10]. 
One could choose decision trees or Bayesian classifiers because they are space 
efficient, however it is well known that nearest neighbor classifiers are very difficult 
to beat for time series problems [3]. If we had simply stored forty random samples in 
the memory for our nearest neighbor classifier, the average error rate over fifty runs 
would be a respectable 58.7% for a four-class problem. However, we could also 
down-sample the dimensionality by a factor of two, either by skipping every second 
point, or by averaging pairs of points (as in SAX [16]) and place eighty reduced 
quality samples in memory. Or perhaps we could instead reduce the alphabet 
cardinality, by reducing the precision of the original four bytes to just one byte, thus 
allowing 160 reduced-fidelity objects to be placed in memory. Many other 
combinations of dimensionality and cardinality reduction could be tested, which 
would trade reduced fidelity to the original data for more exemplars stored in 
memory. In this case, a dimensionality of 32 and a cardinality of 6 allow us to place 
852 objects in memory and achieve an error rate of about 90.75%, a remarkable 
accuracy improvement given the limited resources. 
In general, testing all the combinations of parameters is computationally infeasible. 
Furthermore, while in this case we have class labels to guide us through the search of 
parameter space, this would not be the case for other unsupervised data mining 
algorithms, such as clustering, motif discovery [15], outlier discovery etc.  
As we shall show, our MDL framework allows us to automatically discover the 
parameters that reflect the intrinsic model/cardinality/dimensionally of the data 
without requiring external information or expensive cross validation search.  
2 
Definitions and Notation 
We begin with the definition of a time series:  
Definition 1: A time series T is an ordered list of numbers. T=t1,t2,...,tm. Each value ti 
is a finite precision number and m is the length of time series T.  
Before continuing we must justify the decision of (slightly) quantizing the time 
series. MDL is only defined for discrete values1, but most time series are real-valued. 
                                                           
1 The closely related technique of MML (see Minimum Message Length footnote 61 footnotes 
137-138 in 4, footnote 88 and sec. 6.5 in 5 and 26) does allow for continuous real-valued 
data. However, here we stick the more familiar MDL formulation. 

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
187 
 
The obvious solution is to reduce the original number of possible values to a 
manageable amount. However the reader may object that such a drastic reduction in 
precision must surely lose some significant information. However this is not the case. 
To illustrate this point, we performed a simple experiment. From each of the twenty 
diverse datasets in the UCR archive [10] we randomly extracted one hundred pairs of 
time series. For each pair of time series we measured their Euclidean distance in the 
original high dimensional space, and then in the quantized 256-cardinality space, and 
used these pair of distances to plot a point in a scatter plot.   
Fig.2 shows the results. The figure illustrates that all the points fall close to the 
diagonal, thus the quantization makes no perceptible difference. Beyond this 
subjective visual test, we also reproduced the heavily cited UCR time series 
classification benchmark experiments [10], replacing the original data with the 256-
cardinality version. In no case did it make more than one tenth of one percent 
difference to classification accuracy. Given this, we simply reduce all the time series 
data to its 256 cardinality version in this work, by using a discretization function : 
Definition 2: Discretization is a function that is used to normalize a real-valued time 
series T into b-bit discrete values in the range [-2b-1, 2b-1-1]. It is defined as following: 
ܦ݅ݏܿݎ݁ݐ݅ݖܽݐ݅݋ܾ݊ሺܶሻൌݎ݋ݑ݊݀൬
ܶെ݉݅݊
݉ܽݔെ݉݅݊൰כ ሺ2ܾെ1ሻെ2ܾെ1 
where min and max are the minimum and maximum value in T, respectively. 
 
Fig. 2. Each point on this plot corresponds to a pair of time series: the x-axis corresponds to 
their Euclidean distance, while the y-axis corresponds to the Euclidean distance between the 8-
bit quantized representations of the same pair 
For any time series T, we are interested in determining how many bits it takes to 
represent it. We can thus define the description length of a time series. Note that in 
our experiments all real values are discretized to 8-bit discrete values. 
Definition 3: A description length DL of a time series T is the total number of bits 
required to represent it. When Huffman coding is used to compress the time series T, 
the description length of time series T is defined by: 
DL (T) = | HuffmanCoding(T) | 
In the current literature, the number of bits required to store the time series depends 
on the idiosyncrasies of the data format or hardware device, not on any intrinsic 
0
10
20
30
40
0
10
20
30
40
Euclidean dist of real-valued pairs
Euclidean dist of reduced 
cardinality pairs

188 
B. Hu et al. 
 
properties of the data or domain. However we are really interested in knowing the 
minimum number of bits to exactly represent the data, the intrinsic amount of 
information in the time series. Unfortunately, in the general case this is not calculable, 
as it is the Kolmogorov complexity of the time series [14]. However, we can 
approximate the Kolmogorov complexity by compressing the data, using say 
Huffman coding[28][29]. The (lossless) compressed file size is clearly an upper 
bound to the DL of the time series [30]. 
One of the key steps in finding the intrinsic cardinality and/or dimensionality is 
converting a given time series to other representation or model, e.g., by using DFT or 
DWT. We call that representation, a hypothesis: 
Definition 4: A hypothesis H is a representation of a discrete time series T after 
applying a transformation M.  
In general, there are many possible transforms. Examples include the Discrete 
Wavelet Transform (DWT), the Discrete Fourier transform (DFT), the Adaptive 
Piecewise Linear Approximation (APCA), the Piecewise Linear Approximation 
(PLA), etc[3]. Fig.3. show three illustrative examples, DFT, APCA, and PLA. In this 
paper, we demonstrate our ideas using one these commonly used representations 
(relegating the other two to our expanded technical report [31]). However our ideas 
apply to all time series models (see [3]for a survey of representations).We use the 
term model interchangeably with the term hypothesis in this work.  
 
Fig. 3. “Consider a very long sequence of symbols ...We shall consider such a sequence of 
symbols to be simple if there exists a very brief description of this sequence - using, some sort 
of stipulated description method”. Ray Solomonoff [25]. A time series T shown in bold/blue 
and three different models of it shown in fine/red: from left to right: DFT, APCA, and PLA. In 
each case the coefficients of DFT, APCA, or PLA can be considered the stipulated description 
method. 
Definition 5: A reduced description length of a time series T given hypothesis H is the 
number of bits used for encoding time series T, exploiting information in the hypothesis 
H, i.e., DL (T│H), and the number of bits used for encoding H, i.e., DL (H). Thus, the 
reduced description length is defined as: DL (T, H) = DL (H) + DL (T│H) 
The first term, DL (H), called the model cost, is the number of bits required to store 
the hypothesis H. We will give concrete examples later, but in brief, the model cost 
for say the piecewise linear approximation would include the bits need to encode the 
mean, slope and length of each linear segment. The second term, DL (T│H), called 
the correction cost (in some works it is called the description cost or error term) is 
the number of bits required to rebuild whole time series T from the given hypothesis 
H. There are many possible ways to encode T by H. However, if we just simply store 
0
40
80
120
0
40
80
120
0
40
80
120

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
189 
 
the differences between T and H, we can easily re-generate a whole time series T from 
the information we have. Thus, in this paper we simply use DL (T│H) = DL (T-H).  
3 
MDL Modeling of Time Series 
For concreteness, we will consider a simple worked example comparing two possible 
dimensionalities of data. Note that here we are assuming a cardinality of 16, and a 
model of APCA. However, more generally we do not need to make such assumptions. 
Let us consider a sample time series T of length 24: 
T = 1 1 1 2 3 4 5 6 7 8 9 10 11 11 12 12 12 12 11 11 10 10 9 7 
In Fig.4. left, we show a plot of this data. 
 
Fig. 4. left)A sample time series T. middle)Time series T (blue/fine), approximated by a one-
dimensional APCA approximation H1 (red/bold). The error for this model is represented by the 
vertical lines. right) Time series T (blue/fine), approximated by a two-dimensional APCA 
approximation H2 (red/bold). Vertical lines represent the error for this model. 
We can attempt to model this data with a single constant line, a special case of 
APCA. We begin by finding the mean of all the data, which (rounding in our integer 
space) is eight. We can create a hypothesis H1 to model this data, which as shown in 
Fig.4.middle. It is simply a constant line with a mean of eight. There are 16 possible 
values this model could have had. Thus DL (H1) = 4 bits. This model H1 has a 
significant amount of error2 modeling T, and we must account for this. The errors e1, 
represented by the length of the vertical lines in Fig.4.middle are: 
e1 = 7  7  7  6  5  4  3  2  1  0  -1 -2 -3 -3 -4 -4 -4 -4 -3 -3 -2 -2 -1  1 
As noted in Definition 5, the cost to represent these errors is the correction cost or 
the number of bits encoding e1 using Huffman coding, which is 82 bits. Thus the 
overall cost to represent T with a one-dimensional model or its reduced description 
length is:  
DL (T, H1) = DL (T|H1) + DL (H1) 
DL (T, H1) = 82 + 4 = 86 bits 
Now we can test to see if hypothesis H2, which models the data with two constant 
lines could reduce the description length. Fig.4.right shows the two segment 
approximation lines created by APCA.As we expect, the error e2 shown as the vertical 
lines in Fig.4. right is smaller than the error e1. In particular, the error e2 is:  
e2 =2  2  2  1  0 -1 -2 -3  3  2  1  0 -1 -1 -2 -2 -2 -2 -1 -1  0  0  1  3 
                                                           
2 The word error has a pejorative meaning not intended here, some authors prefer to use 
correction cost. 
1 2
4
6
8
10
12
14
16
18
20
22
24
1 2
4
6
8
10
12
14
16
18
20
22
24
1 2
4
6
8
10
12
14
16
18
20
22
24

190 
B. Hu et al. 
 
The number of bits encoding e2 using Huffman coding or the correction cost to 
generate the time series T given the hypothesis H2, DL (T│H2), is 65 bits. Although 
the correction cost is smaller than one-dimensional APCA, the model cost is larger. In 
order to store two constant lines, two constant numbers corresponding to the height of 
each line and a pointer indicating the end position of the first line are required. Thus, 
the reduced description length of model H2 is: 
DL (T, H2) = DL (T|H2) + DL (H2) 
DL (T, H2) = 65 + 2*log2 (16) + [log2 (24)] = 78bits 
Because we have DL (T, H2) < DL (T, H1), we prefer H2 as a proper number of 
segments for our data. Clearly we are not done yet, we should also test H3, H4, etc., 
corresponding to 3, 4, etc. piecewise constant segments. Moreover, we can also test 
alterative models corresponding to different levels of DFT or PLA representation. In 
addition, we can also test different cardinalities, because it is possible that the 16-
value cardinality was unnecessary for this domain. For example, suppose we had been 
given T2 instead:    
T2 = 0 0 0 0 4 4 4 4 4 0 0 0 0 8 8 8 8 8 8 12 12 12 12 12 
Here, if we tested multiple hypotheses as to the cardinality of this data, we would 
hope to find that the hypothesis ܪସ
஼ that attempts to encode the data with a 
cardinality of just 4 would result in the smallest model. 
We have shown a detailed example for APCA. However essentially all the time 
series representations can be encoded in a similar way. As shown with three 
representative examples in Fig.3, essentially all the time series models consist of a set 
of basic functions that are linearly combined to produce an approximation of the data.  
As we apply our ideas to each representation, we must be careful to correctly 
“charge” each model for its approximation level. For example, each APCA segment 
requires two numbers, to encode its mean value and its length. However, PLA 
segments require three numbers, mean value, segment length and slope. Each DFT 
coefficient requires two numbers to encode the amplitude and phase of each sine 
wave, however, because of the complex conjugate property, we get a “free” 
coefficient for each one we record[2][3]. In previous comparisons of the indexing 
performance of various time series representations, many authors (including one of 
the current authors [13]) have given an advantage to one representation by the 
counting cost to represent an approximation incorrectly. The ideas in this work do 
explicitly assume a fair comparison. Fortunately, the community seems more aware of 
this problem in recent years[2][9]. 
3.1 
Generic MDL for Time Series Algorithm 
In the last section, we use a toy example for demonstrating how to compute the 
reduced description length of a time series with competing hypothesis. In this section, 
we will show a detailed generic version of our algorithm, and then explain our 
algorithm in detail for the three most commonly used time series representations. 
Our algorithm can not only discover the intrinsic cardinality and dimensionality of 
an input time series, but also be used to find the right model or data representation for 
the given time series. The following pseudo code shows a high-level view of our 

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
191 
 
algorithm for discovering the best model, cardinality, and dimensionality which will 
minimize the total number of bits required to store the input time series. 
Because MDL is at the heart of our algorithm, the first step in our algorithm is to 
quantize a real-valued time series into a discrete-value (but fine-grained) time series, 
T (line 1). Next, we consider each model, cardinality, and dimensionality one by one 
(line 3-5). Then a hypothesis H is created based on the selected model and parameters 
(line 6). For example, a hypothesis H shown in Fig.4.middle is created when the 
model M=APCA, cardinality c=16, and dimensionality d=1. 
The reduced description length defined in Definition 5 is then calculated (line 7), 
and our algorithm returns the model and parameters which can minimize the reduced 
description length for encoding T (line 8-13). 
Algorithm: Generic MDL Algorithm 
Input: TS: time series 
Output:    best_model:  best model  
best_card :  best cardinality 
best_dim  :  best dimensionality 
  1.  T = Discretization (TS)  
  2.  Bsf = ∞ 
  3.  for all M in {APCA, PLA, DFT} 
  4.      for all cardinality c 
  5.          for all dimensionality d 
  6.             H = ModelRespresentation(T,M,c,d) 
  7.             total_cost = DL(H) + DL(T|H) 
  8.             if ( bsf>total_cost) 
 9.                  bsf = total_cost 
  10.                 best_model = M 
  11.                 best_card = c 
  12.                 best_dim = d 
  13.            end if 
  14.         end for 
  15.     end for 
  16.  end for 
For concreteness we will now consider one specific version of our generic 
algorithm, for Adaptive Piecewise Constant Approximation. In an extended technical 
report version of this paper we consider Piecewise Linear Approximation and 
Discrete Fourier Transformation to the same level of detail [31].   
3.2 
Adaptive Piecewise Constant Approximation 
As we have seen in previous section, APCA model is simple; it only contains constant 
lines. The following pseudo code for APCA is very similar to the generic algorithm. 
First of all, we do quantization on the input time series (line 1). Then, we evaluate all 
cardinalities from 2 to 256 and dimensionalities from 2 to the maximum possible 
number, which is a half of the length of input time series TS (line 3-4). Note that if the 
dimensionality was more than m/2, some segments will contain only one point. 

192 
B. Hu et al. 
 
Algorithm: IntrinsicDiscovery for APCA 
  1.  T = Discretization (TS)  
  2.  bsf = ∞ 
  3.  for c = 2 to 256 
  4.      for d = 2 to m/2 
  5.             H = APCA(T,c,d) 
  6.             model_cost = d*log2(c)+(d-1)*log2(m) 
  7.             total_cost = model_cost + DL(T-H) 
  8.             if ( bsf>total_cost) 
  9.                  bsf = total_cost 
  10.                 best_card = c 
  11.                 best_dim = d 
  12.            end if 
  13.    end for 
  14. end for 
Then, a hypothesis H is created using the values of cardinality c and dimensionality 
d, as shown in Fig.4. Right when c=16and d=2. The model contains d constant 
segments so the model cost is the number of bits required for storing d constant 
numbers, and d-1 pointers to indicate the offset of the end of each segment (line 6). 
The difference between T and H is also required to rebuild T. The correction cost 
(Definition 5) is computed; then the reduced description length is the combination of 
the model cost and the correction cost (line 7). Finally, the hypothesis which 
minimized this value is returned as an output of the algorithm (line 8-13). 
4 
Experimental Evaluation 
We begin with a simple sanity check on the classic problem specifying the correct 
time series model, cardinality and dimensionality, given an observation of a corrupted 
version of it. While this problem has received significant attention in the literature 
[6][22][23], our MDL method has two significant advantages over existing works; It 
is parameter-free, whereas most other methods require several parameters to be set, 
and MDL can specify the model, cardinality and dimensionality, whereas other 
methods typically only consider model and/or dimensionality. To eliminate the 
possibility of data bias [12]we consider a ten year-old instantiation [23]of a classic 
benchmark problem[6]. In Fig.5we show the classic Donoho-Johnstone block 
benchmark. The underlying model used to produce it is a twelve piecewise constant 
sections with Gaussian noise added. 
 
Fig. 5. A version of the Donoho-Johnstone block benchmark created ten years ago and 
downloaded from [23] 
 
Donoho-Johnstone Benchmark
0
500
1000
1500
2000

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
193 
 
The task is challenging because some of the piecewise constant sections are very 
short and thus easily dismissed during a model search. There have been dozens of 
algorithms that applied to this problem (indeed, to this exact instance of data) in the 
last decade, which should we compare to? Most of these algorithms have several 
parameters, in some cases as many as six [8].We argue that comparisons to such 
methods are pointless, since our explicit aim is to introduce a parameter-free method. 
The most cited parameter-free method to address this problem is the L-Method of 
[22].In essence, the L-Method is a “knee-finding” algorithm. It attempts to explain the 
residual error vs. size-of-model curve using all possible pairs of two regression lines. 
Fig.6.top shows one such pairs of lines, from one to ten and from eleven to the end. 
The location that produces the minimum sum of the residual errors of these two 
curves R, is offered as the optimal model, as we can see in Fig.6.bottom this occurs at 
location ten, a good estimate of the true value of twelve. 
 
Fig. 6. The knee finding L-Method top) A residual error vs. size-of-model curve (bold/blue) is 
modeled by all possible pairs of regression lines (light/red). Here just one possibility is shown. 
bottom) The location that minimizes the summed residual error of the two regression lines is 
given as the optimal “knee”. 
We also tested several other methods, including [27]. However no other parameter-
free or parameter-lite method we found produced intuitive (much less correct) results.  
We can now attempt to solve this problem with our MDL approach. Fig.7shows 
that of the 64 different piecewise constant models it evaluated, MDL chose the 
twelve-segment model, the correct answer. The left part in Fig.7uses a cardinality of 
256, but the same answer is retuned for(at least) every cardinality from 8 to 256. 
 
Fig. 7. left) The description length of the Donoho-Johnstone block benchmark time series is 
minimized at a dimensionality corresponding to twelve piecewise constant segments, which is 
the correct answer [23]. right) The description length of the Donoho-Johnstone block 
benchmark time series is minimized with a cardinality of ten, which is the true cardinality [23]. 
0
10
20
30
40
50
60
70
80
90
100
0
100,000
0
10
20
30
40
50
60
70
80
90
100
Two regression lines from 1 to K, and from  K+1 to end. Here K =10  (lines shifted up for visual clarity)
The sum of the residual errors of these two lines is denoted R
Residual error between piecewise constant model and benchmark data for different numbers of segments, K
R
The value of R for all values of K from 2 to 100
The minimum is at 10
 
0
10
20
30
40
50
60
0
500
1000
DL(H)
DL(T |H)
DL(T |H) +  DL(H)
The minimum is at 12
0
500
1000
0 10
50
100
150
200
250
DL(T |H ) + DL(H)
DL (T|  H )
The minimum is at 10
DL( H  )

194 
B. Hu et al. 
 
Beyond outperforming other techniques at the task of finding the correct 
dimensionality of a model, MDL can also find the intrinsic cardinality of a dataset, 
something that methods[22][27]are not even defined for. In the right part of Fig.7we 
have repeated the previous experiment, but this time fixing the dimensionality to 
twelve as suggested above, and testing all possible cardinality values from 2 to 256. 
Here MDL indicates a cardinality of ten, which is the correct answer [23]. We also 
re-implemented the most referenced recent paper on time series discretization [18]. 
The algorithm is stochastic, and requires the setting of five parameters. In one 
hundred runs over multiple parameters we found it consistently underestimated the 
cardinality of the data (the mean cardinality was 7.2). 
Before leaving this example, we show one further significant advantage of MDL 
over existing techniques. Both [22][27]try to find the optimal dimensionality, 
assuming the underlying model is known. However in many circumstances we may 
not know the underlying model. As we show in Fig.8, with MDL we can relax even 
this assumption. If our MDL scoring scheme is allowed to choose over the cross 
product of model = {APCA, PLA, DFT}, dimensionality = {1 to 512} and cardinality 
= {2 to 256}, it correctly chooses the right model, dimensionality and cardinality. 
 
Fig. 8. The description length of the Donoho-Johnstone block benchmark time series is 
minimized with a piecewise constant model (APCA), not a piecewise linear model (PLA) or 
Fourier representation (DFT) 
4.1 
An Example Application in Physiology 
The Muscle dataset studied by Mörchen and Ultsch[17]describes the muscle 
activation of a professional inline speed skater. The authors calculated the muscle 
activation from the original EMG (Electromyography) measurements by taking the 
logarithm of the energy derived from a wavelet analysis. Fig.9.Left.top shows an 
excerpt. At first glance it seems to have two states, which corresponds to our 
(perhaps) naive intuitions about skating and muscle physiology. 
 
Fig. 9. Left .top) An excerpt from the Muscle dataset. Left.bottom) A zoomed-in section of the 
Muscle dataset which had its model, dimensionality and cardinality set by MDL. Right.left) The 
description length of the muscle activation time series is minimized with a cardinality of three, 
which is the correct answer. Right. right) The Persist algorithm predicts a value of four. 
DL(T |H) +  DL(H)
DFT
0
10
20
30
40
50
60
0
500
1000
PLA
APCA
0
1000
2000
3000
4000
0
10000
20000
stroke
stroke
glide
glide
glide
push off
push off
Persistance
DL(H)
0
10
20
30
40
50
60
0
2000
4000
DL(T |H)
DL(T |H) +  DL(H)
The minimum is at 3
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
k

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
195 
 
We can test this binary assumption by using MDL to find the model, dimensionality 
and cardinality. The results for model and dimensionality are objectively correct, as we 
might have expected given the results in the previous section, but the results for 
cardinality, shown in Fig.9.Right. left in are worth examining.  
Our MDL method suggests a cardinality of three. Glancing back at Fig.9 shows 
why. At the end of the stroke there is an additional level corresponding to an 
additional push-off by the athlete. This feature was noted by physiologists that worked 
with Mörchen and Ultsch [17]. However, their algorithm weakly predicts a value of 
four3. Here once again we find the MDL can beat rival approaches, even though the 
rival approach attempted the most favorable parameter tuning.  
5 
Discussion of Time and Space Complexity and Conclusions 
The space complexly of our algorithm is linear in the size of the original data. The 
time complexity of the algorithms in pseudo code in section 3.2 and 3.3 are optimized 
for simplicity, and appear quadratic in the time series length. However, we can do the 
DFT/PLA/APCA decomposition once at the finest granularity, and cache the results, 
leaving only a loop that performs efficient calculations on integers. After this 
optimization, the time taken for our algorithms becomes an inconsequential fraction 
of the time it takes to do PLA or APCA once, and only slightly slower than the time it 
takes to do DFT once. We therefore omit timing experiments.  
We have shown that a simple methodology based on MDL can robustly specify the 
intrinsic model, cardinality and dimensionality of time series data from a wide variety 
of domains. While we have followed the more familiar MDL notation of Rissanen 
[20][21] and Grünwald [28], the basic idea we are leveraging off harkens back to the 
ideas of Solomonoff [25]and Wallace [26]. These ideas seem underexploited and 
underappreciated in the data mining community (to which the current authors most 
closely identify), and we hope our empirical research efforts will partly redress this.  
Our method significant advantages over revival methods in that it is more general 
and is essentially parameter-free. We have further shown applications of our ideas to 
resource-limited classification and anomaly detection.  
Acknowledgments and Notes. This project was supported by the Department of the 
United States Air Force, Air Force Research Laboratory under Contract FA8750-10-
C-0160, and by NSF grants 0803410/ 0808770. The first two authors contributed 
equally and did the bulk of the work, and should be consider joint first authors. 
References 
1. Assent, I., Krieger, R., Afschari, F., Seidl, T.: The TS-Tree: Efficient Time Series Search 
and Retrieval. In: EDBT (2008) 
2. Camerra, A., Palpanas, T., Shieh, J., Keogh, E.: iSAX 2.0: Indexing and Mining One 
Billion Time Series. In: International Conference on Data Mining (2010) 
                                                           
3 The values for k = 3, 4 or 5 do not differ by more than 1%. 

196 
B. Hu et al. 
 
3. Ding, H., Trajcevski, G., Scheuermann, P., Wang, X., Keogh, E.: Querying and mining of 
time series data: experimental comparison of representations and distance measures. 
PVLDB 1(2), 1542–1552 (2008) 
4. Dowe, D.L.: Foreword re C. S. Wallace. Computer Journal 51(5), 523–560 (2008) 
5. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consistency, 
invariance and uniqueness. In: Handbook of the Philosophy of Science Philosophy of 
Statistics, pp. 901–982. Elsevier (2011) 
6. Donoho, D.L., Johnstone, I.M.: Ideal spatial adaptation via wavelet shrinkage. Journal of 
Biometrika 81(3), 425–455 (1994) 
7. Evans, S.C., et al.: MicroRNA target detection and analysis for genes related to breast 
cancer using MDL compress. EURASIP J. Bioinform. Syst. Biol., 1–16 (2007) 
8. García-López, D.-A., Acosta-Mesa, H.-G.: Discretization of Time Series Dataset with a 
Genetic Search. In: Aguirre, A.H., Borja, R.M., Garciá, C.A.R. (eds.) MICAI 2009. LNCS, 
vol. 5845, pp. 201–212. Springer, Heidelberg (2009) 
9. Jonyer, I., Holder, L.B., Cook, D.J.: Attribute-Value Selection Based on Minimum 
Description Length. In: International Conference on Artificial Intelligence (2004) 
10. Keogh, E., Xi, X., Wei, L., Ratanamahatana, C.A.: The UCR Time Series 
Classification/Clustering Homepage (2006),  
http://www.cs.ucr.edu/~eamonn/time_series_data/ 
11. Kontkanen, Pand Myllym, P.: MDL histogram density estimation. In: Proceedings of the 
Eleventh International Workshop on Artificial Intelligence and Statistics (2007) 
12. Keogh, E., Kasetty, S.: On the Need for Time Series Data Mining Benchmarks: A Survey 
and Empirical Demonstration. Journal of Data Mining and Knowledge Discovery 7(4), 
349–371 (2003) 
13. Keogh, E.J., Pazzani, M.J.: A Simple Dimensionality Reduction Technique for Fast 
Similarity Search in Large Time Series Databases. In: Terano, T., Liu, H., Chen, A.L.P. 
(eds.) PAKDD 2000. LNCS, vol. 1805, pp. 122–133. Springer, Heidelberg (2000) 
14. Li, M., Vitanyi, P.: An Introduction to Kolmogorov Complexity and Its Applications, 2nd 
edn. Springer (1997) 
15. Lin, J., Keogh, E., Lonardi, S., Patel, P.: Finding motifs in time series. In: Proc. of 2nd 
Workshop on Temporal Data Mining (2002) 
16. Lin, J., Keogh, E., Wei, L., Lonardi, S.: Experiencing SAX: a novel symbolic 
representation of time series. Journal of Data Mining and Knowledge Discovery 15(2), 
107–144 (2007) 
17. Mörchen, F., Ultsch, A.: Optimizing time series discretization for knowledge discovery. 
In: KDD, pp. 660–665 (2005) 
18. Pednault, E.P.D.: Some Experiments in Applying Inductive Inference Principles to Surface 
Reconstruction. In: IJCAI, pp. 1603–1609 (1989) 
19. Palpanas, T., Vlachos, M., Keogh, E.J., Gunopulos, D.: Streaming Time Series 
Summarization Using User-Defined Amnesic Functions. IEEE Trans. Knowl. Data 
Eng. 20(7), 992–1006 (2008) 
20. Rissanen, J.: Stochastic Complexity in Statistical Inquiry. World Scientific, Singapore 
(1989) 
21. Rissanen, J., Speed, T., Yu, B.: Density estimation by stochastic complexity. IEEE Trans. 
On Information Theory 38, 315–323 (1992) 
22. Salvador, S., Chan, P.: Determining the Number of Clusters/Segments in Hierarchical 
Clustering/Segmentation Algorithms. In: ICTAI, pp. 576–584 (2004) 
23. Sarle, W.S.: Donoho-Johnstone Benchmarks: Neural Net Results (1999), 
ftp://ftp.sas.com/pub/neural/dojo/dojo.html 

 
Towards Discovering the Intrinsic Cardinality and Dimensionality 
197 
 
24. Sart, D., Mueen, A., Najjar, W., Niennattrakul, V., Keogh, E.: Accelerating Dynamic Time 
Warping Subsequence Search with GPUs and FPGAs. In: IEEE International Conference 
on Data Mining (2010) 
25. Solomonoff. R. J.: A Preliminary Report on a General Theory of Inductive Inference, 
Contract AF 49(639)-376. Report ZTB-138, Zator Co., Cambridge, Mass. (November 
1960) 
26. Wallace, C.S., Boulton, D.M.: An information measure for classification. Computer 
Journal 11(2), 185–194 (1968) 
27. Zhao, Q., Hautamaki, V., Fränti, P.: Knee point detection in BIC for detecting the number 
of clusters. In: Blanc-Talon, J., Bourennane, S., Philips, W., Popescu, D., Scheunders, P. 
(eds.) ACIVS 2008. LNCS, vol. 5259, pp. 664–673. Springer, Heidelberg (2008) 
28. Grünwald, P. (ed.): Advances in Minimum Description Length: Theory and Applications. 
MIT Press (2005) 
29. Vereshchagin, N., Vitanyi, P.: Rate distortion and denoising of individual data using 
Kolmogorov complexity. IEEE Trans. Information Theory 56(7), 3438–3454 (2010) 
30. De Rooij, S., Vitányi, P.: Approximating Rate-Distortion Graphs of Individual Data: 
Experiments in Lossy Compression and Denoising. IEEE Transactions on Computers 
(2011) 
31. URL: Expanded Technical Report version of this paper, 
http://www.cs.ucr.edu/~bhu002/LNCS 

Complexity Measures for Meta-learning
and Their Optimality
Norbert Jankowski
Department of Informatics, Nicolaus Copernicus University, Toru´n, Poland
Abstract. Meta-learning can be seen as alternating the construction
of conﬁguration of learning machines for validation, scheduling of such
tasks and the meta-knowledge collection.
This article presents a few modiﬁcations of complexity measures and
their application in advising to scheduling test tasks—validation tasks of
learning machines in meta-learning process. Additionally some comments
about their optimality in context of meta-learning are presented.
1
Introduction
To talk about complexity and its connections with meta-learning we need to
deﬁne the learning problem and the learning machine ﬁrst. The learning problem
P
is represented by the pair of data D ∈D and model space M. Finding the
best solution for P is usually an NP-hard problem and we must be satisﬁed with
suboptimal solutions found by learning machines deﬁned as processes
L : KL × D →M
(1)
where KL is the space of conﬁguration parameters of L.
The task of ﬁnding the best possible model for ﬁxed data D, can be replaced
by the task of ﬁnding the best possible learning machine L.
Note that learning machines may be simple or complex. In case of complex
machines a learning machine is composed of several other learning machines:
L = L1 ◦L2 ◦. . . ◦Lk.
(2)
For example, a composition of a data transformation machine and a classiﬁer
machine or classiﬁers committee which is composed of several machines. Ad-
ditionally it is not necessary to distinguish between learning and non-learning
machines. Test procedures estimating some measures of learning machines accu-
racies, adaptation etc. can also be seen as another machines. And then the time
and space complexity may touch more sophisticated structure which consists of
learning machines and some testing procedures as well.
Meta-learning algorithm (MLA) is just another type of learning algorithm
(with another data and model spaces). The most characteristic feature of meta-
learning is that it learns how to learn. Typically meta-learning uses conclusions of
other learning tasks formulated and evaluated (during the run of meta-learning).
This is why meta-learning algorithms realize followings actions in a loop: formu-
lation and starting of the test tasks, collection of results and meta-knowledge.
For a review of diﬀerent meta-learning approaches, see [1].
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 198–210, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Complexity Measures for Meta-learning and Their Optimality
199
The goal of meta-learning may be deﬁned by: maximization of the probability of
ﬁnding best possible solution of given problem P within given search space in as
short time as possible.
In consequence the meta-learning algorithms should carefully order testing
tasks during the progress of the search, basing on information about complexity
(as it will be seen) and build meta-knowledge based on the experience from
passed tests. It is highly necessary to avoid test tasks that could accumulate too
large amount of time, since it would decrease the probability of ﬁnding the most
interesting solutions.
This is why the complexity approach is presented in further part of the text.
2
Complexity Measures for Learning Machines
The above deﬁnition of a learning problem describes what data analysts usually
do, but it neglects the fact that the time that can be spent on solving the
problem is always limited. We never have an inﬁnite amount of time for ﬁnding
satisfactory solutions, even in the case of scientiﬁc research. This is why the
meta-learning should work with respect to time limits, but indeed it should
based on reformulated deﬁnition of learning problem.
The restricted learning problem is a learning problem P with deﬁned time
limit for providing the solution.
Such deﬁnition of the problem of learning from data is much more realistic
in comparison to the original one (whenever it is used to research purpose or
commercial purpose). It should be preferred also for research purposes. In a
natural way, it reﬂects the reality of data analysis challenges, where deadlines
are crucial, but it is also more adequate for real business applications, where
solutions must be found in appropriate time and any other applications with
explicit or implicit time constraints.
Meta-learning algorithms should favor simple solutions and start the machines
that provide them before the more complex ones. It means that MLAs should
start with maximally plain learning machines. But what does (should) plain
mean? In that case plain or not plain is not equivalent with single or complex
learning machine (when looking at the structure of machine). The complexity
measure should be used to reﬂect the space and time needs of given algorithm
(also for learning machines of complex structure). What’s more, sometimes a
composition of a transformation and a classiﬁer may be indeed of smaller com-
plexity than the classiﬁer without transformation. It is true because when using
a transformation, the data passed to the learning process of the classiﬁer may
be of smaller complexity and, as a consequence, classiﬁer’s learning is simpler
and the diﬀerence between the classiﬁer learning complexities, with and without
transformation may be bigger than the cost of the transformation. This proves
that real complexity is not reﬂected directly by structure of a learning machine.
Complexity measures by Solomonoﬀ, Kolmogorov and Levin. To obtain the right
order in the searching queue of learning machines, a complexity measure should

200
N. Jankowski
be used. Solomonoﬀ[2,3,4] and Kolmogorov [5] (see [6] too) deﬁned complexity
by:
CK(P) = min
p {lp : if program p prints P and halts},
(3)
where lp is the length of program p. Unfortunately, this deﬁnition is inadequate
from the practical side, because it is not computable. Levin’s deﬁnition [6] in-
troduced a term responsible for time consumption:
CL(P) = min
p {cL(p) : if program p prints P
and then halts within time tp}
(4)
where
cL(p) = lp + log tp.
(5)
In the case of Levin’s deﬁnition (Eq. 4) it is possible to realize [7] the Levin
Universal Search (LUS) [8,6] but the problem is that this algorithm is NP-hard.
This means that, in practice, it is impossible to ﬁnd an exact solution to the
optimization problem.
Generators ﬂow deﬁnes search space. The strategy of meta-learning is diﬀerent
than the one of LUS. The LUS searches in an inﬁnite space between all possible
programs. Meta-learning uses the functional deﬁnition of the search space, which
is not inﬁnite. This means that the search space is, indeed, strongly limited.
The generators ﬂow is assumed to generate machine conﬁgurations which are
“rational” from the point of view of given problem P.
Generators ﬂow is a directed graph of machine generators. Thanks to the
graph connection the deﬁnition of appropriate graph is natural. However, a deep
description of this topic will, most certainly, exceed the page limit. The gener-
ators ﬂow is deﬁned individually for each learning problem P. However similar
conﬁgurations of the generator ﬂow may be shared by classiﬁers (for example).
Generators ﬂow should reﬂect all accessible knowledge about problem P and pro-
vide several conﬁgurations of machines which performance should be tested—it
increases the probability of ﬁnding a better solution. Thanks to such deﬁnition
of generators ﬂow, the searching space is ﬁnite, however it may consist of many
conﬁgurations, usually so many as to make it impossible to validate all of them.
To validate appropriate conﬁgurations the complexity measure will be used to
order the test task basing on machine conﬁgurations.
Complexity in the context of machines. The complexity can not be computed
from the learning machines themselves, but from conﬁgurations of learning ma-
chines and descriptions of their inputs. Because meta-learning for organizing the
order has to know the complexity before the machine is ready for use. In most
cases, there is no direct analytical way of computing the machine complexity
on the basis of its conﬁguration. The halting problem directly implies impossi-
bility of calculation of time and memory usage of a learning process in general.
Therefore the approximators of complexity have to be contracted.

Complexity Measures for Meta-learning and Their Optimality
201
Modiﬁcations of complexity measures and their optimality. Each meta-learning
algorithm may be seen in the following way: it postulates some learning machines
and provides appropriate tests to estimate the quality of each learning machine.
But in case of time-restricted learning there is a time limit t. In such case,
testing all postulated learning machines may be impossible because summed
testing times of all the candidates may exceed the time limit t.
Assume that we have restricted the learning problem P with a time limit
t and a meta-learning algorithm postulates learning machines m1, m2, . . . , mq,
with testing times of t1, t2, . . . , tq respectively. When not all of the tests can be
done within the time limit (
i ti > t) and if we assume that each test is equally
promising, then to maximize the expected value of the best test result obtained
within the time, we need to run as many tests as possible. Therefore, an optimal
order of the tests is an order i1, i2, . . . , im of nondecreasing testing times:
ti1 ≤ti2 ≤. . . ≤tim,
(6)
and the choice of ﬁrst m shortest tests such that
m = arg max
1≤k≤q
⎡
⎣
l=1,...,k
til ≤t
⎤
⎦,
(7)
is an optimal choice of respective learning machines. The proof of this property
is trivial. Thanks to the sorted permutation in Eq. 6 the m from above equation
is highest. And this directly maximizes the probability of ﬁnding best solution
on above assumptions.
This implies that meta-learning in complexity deﬁnition have to use time in
calculation of complexity:
ct(p) = tp.
(8)
This property is very important because it clearly answers the main problem
of MLA about the choice and order of learning machine tests.
Even if we do not assume that learning machines are not equally promising,
the complexity measure may be adjusted to maximize the expected value of test
results (see Eq. 11).
The property of optimal ordering the test tasks means that in case of order-
ing programs (machines) only on the basis of their length (as it was deﬁned
in Eq. 3) is not rational in the context of learning machines (and non-learning
machines too). The problem of using Levin’s additional term of time Eq. 5, in
real applications, is that it is not rigorous enough in respecting time. For ex-
ample, a program running 1024 times longer than another one may have just
a little bigger complexity (just +10). Note that in case of Levin’s complexity
(Eq. 5) one unit more of space was equivalent to double time. This in not
acceptable in practice of computational intelligence.
On the other hand, total rejection of the length lp is also not recommended
because machines always have to ﬁt in the memory, which is limited too.

202
N. Jankowski
In learning machines, typically the time complexity exceeds signiﬁcantly the
memory complexity. This is why measure of complexity should combine time
and memory together:
ca(p) = lp + tp/ log tp.
(9)
Although the time part may seem to be respected less than the memory part,
it is not really so, because, as mentioned above, almost always time complexity
exceeds signiﬁcantly memory complexity. Thus, in fact, memory part dominates
anyway, so the order of performing tests is very sensitive to time requirements.
What’s more, the order of machines based on the time tp and based on tp/ log tp
are equivalent. The length component lp just “keeps in mind” the limited re-
sources of memory and the whole ca deﬁnes some balance between the two
complexities.
Complexity measure and quarantine. The approximation of the complexity of
a machine is used. Because of this approximation and because of the halting
problem (we never know whether given test task will ﬁnish or not) an additional
penalty term is added to the above deﬁnition:
cb(p) = [lp + tp/ log tp]/q(p),
(10)
where q(p) is a function term responsible for reﬂecting reliability of ca(p) estimate
and is used to correct the approximation in case it was wrong. In other case it
is equal to 1.
At start of the test task p the MLAs use q(p) = 1 (generally q(p) ≤1) for the
estimations, but in the case when the estimated time (as a part of the complexity)
is not enough to ﬁnish task test p, the test task p is aborted and the reliability is
decreased. The aborted test task is moved to a quarantine according to the new
value of complexity reﬂecting the change of the reliability term and another task
is started according to the order. This mechanism prevents from running test
tasks for unpredictably long time of execution or even inﬁnite time. Otherwise
the MLA would be very brittle and susceptible to running tasks consuming
unlimited CPU resources. This is extremely useful in the framework of restricted
learning problem.
Corrections of complexity for unequally promising machines. Another extension
of the complexity measure is possible thanks to the fact that MLAs are able to
collect meta-knowledge during learning or for a priori knowledge. Such knowledge
may inﬂuence the order of waiting test tasks. The optimal way of doing this, is
adding a new term to the cb(p) to shift the start time of given test in appropriate
direction:
cm(p) = [lp + tp/ log tp]/[q(p) · a(p)].
(11)
a(p) reﬂects the attractiveness of the test task p.
While ca(p) was a measure aimed at such order of the tasks that maximizes
the expected value of results, the factor 1/a(p) is a correction to the initial
assumption of equal a priori probabilities in optimal ordering of diﬀerent pro-
grams (or machines). This means that the assumption that machines are equally
promising may be corrected by additional term in complexity.

Complexity Measures for Meta-learning and Their Optimality
203
What machines are we interested in the complexity of? The generators ﬂow
provides machine conﬁgurations which subsequently one by one are nested inside
separate copies of test template. The test template is used to deﬁne the goal of
the problem P. It may be deﬁned in diﬀerent ways depending on the problem
P, for example it may calculate average error. However the common divisor of
all testing procedures is that it consists of learning of the given machine and
testing of this machine.
A good deﬁnition of the testing procedure should naturally reﬂect the usage of
tested machine. And by the same complexity of such test procedure with nested
tested machine reﬂects the complete behaviour of the machine really well: a
part of the complexity formula reﬂects the complexity of learning of the given
machine and the rest reﬂects the complexity of computing the test (for example,
classiﬁcation or approximation test). The costs of learning are very important
because without learning there is no model. The complexity of the testing part
is also very important, because it reﬂects the simplicity of further use of the
model. Some machines learn quickly and require more eﬀort to make use of their
outputs (like kNN classiﬁers), while others learn for a long time and after that
may be very eﬃciently exploited (like many neural networks). Therefore, the test
procedure should be as similar to the whole life cycle of a machine as possible
(and of course as trustful as possible).
Approximation of complexity basing on conﬁguration and input description. To
understand the needs of complexity computing we need to go back to the task
of learning. To provide a learning machine, regardless whether it is a simple
one or a complex machine or a machine constructed to help in the process of
analysis of other machines, its conﬁguration and inputs must be speciﬁed. Com-
plexity computation must reﬂect the information from conﬁguration and inputs.
The recursive nature of conﬁgurations, together with input–output connections,
may compose quite complex information ﬂow. Sometimes, the inputs of subma-
chines become known just before they are started, i.e. after the learning of other
machines1 is ﬁnished. This is one of the most important reasons why determi-
nation of complexity, on the contrary to actual learning processes, must base
on meta-inputs, not on exact inputs (which remain unknown). Meta-inputs are
counterparts of inputs in the “meta-world”. Meta-inputs contain descriptions (as
informative as possible) of inputs which “explain” or “comment” every useful
aspect of each input that could be helpful in determination of the complexity.
To facilitate recurrent determination of complexity—which is obligatory be-
cause of basing on a recurrent deﬁnition of machine conﬁguration and recur-
rent structure of real machines—the functions, which compute complexity, must
also provide meta-outputs, because such meta-outputs will play a crucial role in
computation of complexities of machines which read the outputs through their
inputs. In conclusion, a function computing the complexity for machine L should
be a transformation
DL : KL × M+ →R2 × M+,
(12)
1 Machines which provide necessary outputs.

204
N. Jankowski
where the domain is composed by the conﬁgurations space KL and the space of
meta-inputs M+, and the results are the time complexity, the memory complex-
ity and appropriate meta-outputs.
The problem is not as easy as the form of the function in Eq. 12. Finding
the right function for the given learning machine L may be impossible. This is
caused by unpredictable inﬂuence of some conﬁguration elements and of some in-
puts (meta-inputs) a on the machine complexity. Conﬁguration elements are not
always as simple as scalar values. In some cases conﬁguration elements are rep-
resented by functions or by subconﬁgurations. Similar problem concerns meta-
inputs. In many cases, meta-inputs can not be represented by a simple chain of
scalar values. Often, meta-inputs need their own complexity determination tool
to reﬂect their functional form. For example, a committee of machines, which
plays a role of a classiﬁer, will use other classiﬁers (inputs) as “slave” machines.
It means that the committee will use classiﬁers’ outputs, and the complexity
of using the outputs depends on the outputs, not on the committee itself. This
shows that sometimes, the behaviour of meta-inputs/outputs is not trivial and
proper complexity determination requires another encapsulation.
Meta Evaluators. To enable such high level of generality, the concept of meta-
evaluators has been introduced. The general goal of meta-evaluator is
– to evaluate and exhibit appropriate aspects of complexity representation bas-
ing on some meta-descriptions like meta-inputs or conﬁguration2.
– to exhibit a functional description of complexity aspects (comments) useful
for further reuse by other meta evaluators3.
To enable complexity computation, every learning machine gets its own meta
evaluator.
Because of recurrent nature of machines (and machine conﬁguration) and
because of nontriviality of inputs (its role, which sometimes have complex func-
tional form), meta evaluators are constructed not only for machines, but also for
outputs and other elements with “nontrivial inﬂuence” on machine complexity.
Learning Evaluators. The approximation framework is deﬁned in very general
way and enables building evaluators using approximators for diﬀerent elements
like learning time, size of the model, etc. Additionally, every evaluator that uses
the approximation framework, may deﬁne special functions for estimation of
complexity. This is useful for example to estimate time of instance classiﬁcation
etc.
The complexity control in meta-learning does not require very accurate in-
formation about tasks complexities. It is enough to know, whether a task needs
a few times more of time or memory than another task. Moreover, although
for some algorithms the approximation of complexity is not so accurate, the
quarantine prevents from capturing too much resources.
2 In case of a machine to exhibit complexity of time and memory.
3 In case of a machine the meta-outputs are exhibited to provide source of complexity
information for readers of machine inputs.

Complexity Measures for Meta-learning and Their Optimality
205
The approximation framework enables to construct series of approximators for
single evaluators. The approximators are functions approximating a real value
on the basis of a vector of real values. They are learned from examples, so before
the learning process, the learning data has to be collected for each approximator.
START
Init: {startEnv,
envScenario}
Generate next
observation
conﬁguration ‘oc’.
Succeeded?
Train each
approximator
Run machines ac-
cording to ‘oc’
Return series
of approximators
for evaluator
Extract in-out pairs
from the project
for each approximator
STOP
no
yes
data collection loop
Fig. 1. Process of building approximators for single evaluator
Figure 1 presents the general idea of creating the approximators for an evalua-
tor. To collect the learning data, proper information is extracted from observations
of “machine behavior”. To do this an “environment” for machine monitoring must
be deﬁned. The environment conﬁguration is sequentially adjusted, realized and
observed (compare the data collection loop in the ﬁgure). Observations bring the
subsequent instances of the training data (corresponding to current state of the
environment and expected approximation values). Changes of the environment
facilitate observing the machine in diﬀerent circumstances and gathering diverse
data describing machine behaviour in diﬀerent contexts.
The environment changes are determined by initial representation of the en-
vironment (the input variable startEnv) and specialized scenario, which deﬁnes
how to modify the environment to get a sequence of machine observation conﬁg-
urations i.e. conﬁgurations of the machine being examined which is nested in a
more complex machine structure. Generated machine observation conﬁgurations
should be as realistic as possible—the information ﬂow similar to expected ap-
plications of the machine, allows better approximation desired complexity func-
tions. Each time, a next conﬁguration ‘oc’ is constructed, machines are created
and run according to ‘oc’, and when the whole project is ready, the learning data
is collected. Full control of data acquisition is possible thanks to proper methods
implemented by the evaluators.

206
N. Jankowski
Generators ﬂow
output
Rankings
generator
Feature selection
of rankings
generator
Classiﬁers
generator
MPS for classiﬁers
generator
Transform and classify gener-
ator
MPS/Feature selection of
Transform and classify
generator
Fig. 2. Generators ﬂow used in tests
After the data are collected appropriate approximators are learned and meta-
evaluator becomes ready to approximate the complexity or even other quantities.
3
Example of an Application
The generators ﬂow used in my experiments is presented in Fig. 2. Without
going into the details (because of the page limit) of meaning of the presented
generators ﬂow, assume it generates machines presented in Table 1.
Machine conﬁguration notation. To present complex machine conﬁgurations in
a compact way, I introduce a special notation that enables us to sketch complex
conﬁgurations of machines inline as a single term. For example, the following
text:
[[[RankingCC], FeatureSelection], [kNN (Euclidean)], TransformAndClassify]
denotes
a
complex
machine,
where
a
feature
selection
submachine
(FeatureSelection) selects features from the top of a correlation coeﬃcient based
ranking (RankingCC), and next, the dataset composed of the feature selection
is an input for a kNN with Euclidean metric—the combination of feature selec-
tion and kNN classiﬁer is controlled by a TransformAndClassify machine. The
following notation represents an MPS (ParamSearch) machine which optimizes
parameters of a kNN machine:
ParamSearch [kNN (Euclidean)]
Benchmarks. Because complexity analysis is not the main thread of this article, I
have presented examples on two datasets selected from the UCI machine learning
repository [9]: vowel and splice.
Diagram description. The results obtained for the benchmarks are presented
in the form of diagrams: Fig. 3 and Fig. 4. The diagrams present information
about times of starting, stopping and breaking of each task, about complexities
(global, time and memory) of each test task, about the order of the test tasks

Complexity Measures for Meta-learning and Their Optimality
207
Table 1. Machine conﬁgurations produced by the generators ﬂow of Fig. 2 and the
enumerated sets of classiﬁers and rankings
1
kNN (Euclidean)
2
kNN [MetricMachine (EuclideanOUO)]
3
kNN [MetricMachine (Mahalanobis)]
4
NBC
5
SVMClassiﬁer [KernelProvider]
6
LinearSVMClassiﬁer [LinearKernelProvider]
7
[ExpectedClass, kNN [MetricMachine (EuclideanOUO)]]
8
[LVQ, kNN (Euclidean)]
9
Boosting (10x) [NBC]
10 [[[RankingCC], FeatureSelection], [kNN (Euclidean)], TransformAndClassify]
11 [[[RankingCC], FeatureSelection], [kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
12 [[[RankingCC], FeatureSelection], [kNN [MetricMachine (Mahalanobis)]], TransformAndClassify]
13 [[[RankingCC], FeatureSelection], [NBC], TransformAndClassify]
14 [[[RankingCC], FeatureSelection], [SVMClassiﬁer [KernelProvider]], TransformAndClassify]
15 [[[RankingCC], FeatureSelection], [LinearSVMClassiﬁer [LinearKernelProvider]], TransformAndClassify]
16 [[[RankingCC], FeatureSelection], [ExpectedClass, kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
17 [[[RankingCC], FeatureSelection], [LVQ, kNN (Euclidean)], TransformAndClassify]
18 [[[RankingCC], FeatureSelection], [Boosting (10x) [NBC]], TransformAndClassify]
19 [[[RankingFScore], FeatureSelection], [kNN (Euclidean)], TransformAndClassify]
20 [[[RankingFScore], FeatureSelection], [kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
21 [[[RankingFScore], FeatureSelection], [kNN [MetricMachine (Mahalanobis)]], TransformAndClassify]
22 [[[RankingFScore], FeatureSelection], [NBC], TransformAndClassify]
23 [[[RankingFScore], FeatureSelection], [SVMClassiﬁer [KernelProvider]], TransformAndClassify]
24 [[[RankingFScore], FeatureSelection], [LinearSVMClassiﬁer [LinearKernelProvider]], TransformAndClassify]
25 [[[RankingFScore], FeatureSelection], [ExpectedClass, kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
26 [[[RankingFScore], FeatureSelection], [LVQ, kNN (Euclidean)], TransformAndClassify]
27 [[[RankingFScore], FeatureSelection], [Boosting (10x) [NBC]], TransformAndClassify]
28 ParamSearch [kNN (Euclidean)]
29 ParamSearch [kNN [MetricMachine (EuclideanOUO)]]
30 ParamSearch [kNN [MetricMachine (Mahalanobis)]]
31 ParamSearch [NBC]
32 ParamSearch [SVMClassiﬁer [KernelProvider]]
33 ParamSearch [LinearSVMClassiﬁer [LinearKernelProvider]]
34 ParamSearch [ExpectedClass, kNN [MetricMachine (EuclideanOUO)]]
35 ParamSearch [LVQ, kNN (Euclidean)]
36 ParamSearch [Boosting (10x) [NBC]]
37 ParamSearch [[[RankingCC], FeatureSelection], [kNN (Euclidean)], TransformAndClassify]
38 ParamSearch [[[RankingCC], FeatureSelection], [kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
39 ParamSearch [[[RankingCC], FeatureSelection], [kNN [MetricMachine (Mahalanobis)]], TransformAndClassify]
40 ParamSearch [[[RankingCC], FeatureSelection], [NBC], TransformAndClassify]
41 ParamSearch [[[RankingCC], FeatureSelection], [SVMClassiﬁer [KernelProvider]], TransformAndClassify]
42 ParamSearch [[[RankingCC], FeatureSelection], [LinearSVMClassiﬁer [LinearKernelProvider]], TransformAndClassify]
43 ParamSearch [[[RankingCC], FeatureSelection], [ExpectedClass, kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
44 ParamSearch [[[RankingCC], FeatureSelection], [LVQ, kNN (Euclidean)], TransformAndClassify]
45 ParamSearch [[[RankingCC], FeatureSelection], [Boosting (10x) [NBC]], TransformAndClassify]
46 ParamSearch [[[RankingFScore], FeatureSelection], [kNN (Euclidean)], TransformAndClassify]
47 ParamSearch [[[RankingFScore], FeatureSelection], [kNN [MetricMachine (EuclideanOUO)]], TransformAndClassify]
48 ParamSearch [[[RankingFScore], FeatureSelection], [kNN [MetricMachine (Mahalanobis)]], TransformAndClassify]
49 ParamSearch [[[RankingFScore], FeatureSelection], [NBC], TransformAndClassify]
50 ParamSearch [[[RankingFScore], FeatureSelection], [SVMClassiﬁer [KernelProvider]], TransformAndClassify]
51 ParamSearch [[[RankingFScore], FeatureSelection], [LinearSVMClassiﬁer [LinearKernelProvider]], TransformAndClassify]
52 ParamSearch [[[RankingFScore], FeatureSelection], [ExpectedClass, kNN [MetricMachine (EuclideanOUO)]], TransformAndClas-
sify]
53 ParamSearch [[[RankingFScore], FeatureSelection], [LVQ, kNN (Euclidean)], TransformAndClassify]
54 ParamSearch [[[RankingFScore], FeatureSelection], [Boosting (10x) [NBC]], TransformAndClassify]

208
N. Jankowski
(according to their complexities, compare Table 1) and about accuracy of each
tested machine.
In the middle of the diagram—see the ﬁrst diagram in Fig. 3—there is a
column with task ids (the same ids as in table 1). But the order of rows in the
diagram reﬂects complexities of test tasks. It means that the most complex tasks
are placed at the top and the task of the smallest complexities is visualized at the
bottom. For example, in Fig. 3, at the bottom, I can see task ids 4 and 31 which
correspond to the Naive Bayes Classiﬁer and the ParamSearch [NBC] classiﬁer.
At the top, I can see task ids 54 and 45, as the most complex ParamSearch test
tasks of this benchmark. Task order in the second example (Fig. 4) is completely
diﬀerent.
On the right side of the Task id column, there is a plot presenting starting,
stopping and breaking times of each test task. For an example of restarted task
please look at Fig. 3, at the topmost task-id 54—there are two horizontal bars
corresponding to the two periods of the task run. The break means that the
task was started, broken because of exceeded allocated time and restarted when
the tasks of larger complexities got their turn. The breaks occur for the tasks,
for which the complexity prediction was too optimistic. Two diﬀerent diagrams
(in Fig. 3 and 4) easily bring the conclusion that the amount of inaccurately
predicted time complexity is quite small (there are very few broken bars). Note
that, when a task is broken, its subtasks, that have already been computed are
not recalculated during the test-task restart (due to the machine uniﬁcation
mechanism and machine cache [10]). At the bottom, the Time line axis can be
seen. The scope of the time is [0, 1] interval to show the times relative to the
start and the end of the whole MLA computations.
On the left side of the Task-id column, the accuracies of classiﬁcation test
tasks and their approximated complexities are presented. At the bottom, there
is the Accuracy axis with interval from 0 (on the right) to 1 (on the left side).
Each test task has its own gray bar starting at 0 and ﬁnished exactly at the
point corresponding to the accuracy. So the accuracies of all the tasks are easily
visible and comparable. Longer bars show higher accuracies.
The leftmost column of the diagram presents ranks of the test tasks (the
ranking of the accuracies).
Between the columns with the task ids and the accuracy-ranks, on top of the
gray bars corresponding to the accuracies, some thin solid lines can be seen.
The lines start at the right side (just like the accuracy bars) and go to the
right according to proper magnitudes. For each task, the three lines correspond
to total complexity (the upper line), memory complexity (the middle line) and
time complexity (the lower line)4. All three complexities are the approximated
complexities (see Eq. 10 and 11). Approximated complexities presented on the
left side of the diagram can be easily compared visually to the time-schedule
obtained in the real time on the right side of the diagram. Longer lines mean
higher complexities. The longest line is spread to maximum width. The others
are proportionally shorter. So the complexity lines at the top of the diagram
4 In the case of time complexity, t/ log t is plotted, not the time t itself.

Complexity Measures for Meta-learning and Their Optimality
209
are long while the lines at the bottom are almost invisible. It can be seen that
sometimes the time complexity of a task is smaller while the total complexity is
larger and vice versa. For example, see tasks 42 and 48 again in Fig. 3 (table 1
describes tasks numbers).
Fig. 3. vowel
Fig. 4. splice
If you like to see more results or in better resolution please download the docu-
ment at http://www.is.umk.pl/˜norbert/publications/11-njCmplxEval-Figs.pdf.
4
Summary
Modiﬁed measures of the complexity in context of meta-learning are really an
eﬃcient tool, especially for restricted learning problems.
The most important feature of the presented MLA is that thanks to complex-
ity measures it facilitates ﬁnding accurate solutions in the order of increasing
complexity. Simple solutions are started before the complex ones, to support
ﬁnding simple and accurate solutions as soon as possible. Presented illustrations
(diagrams) clearly show that approximation of complexity can be used to control
the order of testing tasks to maximize the probability that right solution will be
ﬁnd.
What’s more with the help of collected meta-knowledge the complexity mea-
sure adjusts the order of test tasks online to favor more promising learning
machines.

210
N. Jankowski
References
1. Brazdil, P., Giraud-Carrier, C., Soares, C., Vilalta, R.: Metalearning: Applications
to Data Mining. Springer (2009)
2. Solomonoﬀ, R.: A preliminary report on a general theory of inductive inference.
Technical Report V-131, Cambridge, Ma.: Zator Co. (1960)
3. Solomonoﬀ, R.: A formal theory of inductive inference part i. Information and
Control 7(1), 1–22 (1964)
4. Solomonoﬀ, R.: A formal theory of inductive inference part ii. Information and
Control 7(2), 224–254 (1964)
5. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Prob. Inf. Trans. 1, 1–7 (1965)
6. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions. Text and Monographs in Computer Science. Springer (1993)
7. Jankowski, N.: Applications of Levin’s universal optimal search algorithm. In:
K¸acki, E. (ed.) System Modeling Control 1995, vol. 3, pp. 34–40. Polish Society of
Medical Informatics, L´od´z (May 1995)
8. Levin, L.A.: Universal sequential search problems. Problems of Information Trans-
mission 9 (1973), translated from Problemy Peredachi Informatsii (Russian)
9. Merz, C.J., Murphy, P.M.: UCI repository of machine learning databases (1998),
http://www.ics.uci.edu/~mlearn/MLRepository.html
10. Gr¸abczewski, K., Jankowski, N.: Saving time and memory in computational in-
telligence system with machine uniﬁcation and task spooling. Knowledge-Based
Systems 24(5), 570–588 (2011)

 
D.L. Dowe (Ed.): Solomonoff Festschrift, LNAI 7070, pp. 211–222, 2013. 
© Springer-Verlag Berlin Heidelberg 2013 
Design of a Conscious Machine 
P. Allen King 
30 Gibson St, Needham Mass, USA, 02492 
allen@a-king.us 
Abstract. We define here the primary concepts needed to make a hardware 
machine with intelligence capabilities similar to animals and humans, a 
machine that innately imitates, unconsciously initiates attention shifts, wonders 
what if, learns why, and improves from mistakes. We will specify the functional 
requirements for a few particularly important parts, as connectionist schemas. 
This poses several problems for established disciplines of AI, to produce 
computational algorithms of the kinds required here.  These notes will guide the 
next phase, where the schemas described here will be turned into code intended 
to learn to play the Super Mario Brothers video game. 
Keywords: connectionism, semantic networks, world model, brain state, 
reenactment, 
simulation, 
consciousness, 
cognition, 
prediction 
error, 
evolutionary computing, machine learning. 
1 
Introduction 
What is the essence of the functions that the mechanisms of our nerves and brains 
perform as we live our lives and learn to do better? We seek functions that soak up the 
ways of the world like sponges, and allow the being they control to prosper. How 
might these be implemented by cortical minicolumns as atoms of the cortex? As a 
starting point, we explore some massively parallel algorithms, algorithms that might 
underlie these human-like effects. Our architecture is constructed of a synergetic 
combination of mechanisms, each of which embodies an aspect of cognition, and 
together synergize important elements. How we came up with the architecture is of 
less importance than how well the pieces actually perform together.  
Neurology is useful to our work here mostly because it attempts to understand the 
best existence proof of intelligence we have, the brain. We must constantly study the 
brain, especially when charting key architectural decisions. We get glimpses of a 
simpler set of organizing principles for animal intelligence through the study of 
different kinds of animals, as well as the embryonic development. To keep combined 
explanations understandably simple, some very simple individual models have been 
used. They explain essential operation just enough to achieve the desired system’s 
synergies, but avoid any undue complexity to mimic human behavior. Two examples: 
the simple model that “evolution learns” something is a welcomed simplification from 
the precise form. We also resurrect Kornski’s concept of the Grandmother Cell [1], to 
be used as a simplistic short-hand for the distributed data representation of the cortex, 
nicely hiding the complexity full distributed data representations.  

212 
P.A. King 
 
Engineering is useful in our work here too. This is the start of an engineering plan, 
to build a computational medium containing the hard parts of cognition, into which 
experience will naturally grow un-aided. I have been involved in a half dozen large 
engineering projects, like the world’s largest shared memory multi-processor and the 
world’s largest scalable router. Every one of those projects involved dozens of people 
and had an architectural document at its core which started by describing a set of 
mathematical, almost cartoon-like operations upon which the rest was built. This 
document hopes to at least start such a plan for an AI. There are many ties to existing 
work given throughout, in areas such as Evolutionary Computing, Machine Learning, 
and Connectionism. This theory poses interesting questions for each, to come up with 
implementations for the various parts of the straw man pieces we propose. Once an 
operational prototype is constructed, the work of psychology and other fields will be 
able to directly contribute muscle to our straw man. 
Section 2 describes how the being’s worldly knowledge is held in elements, and 
how they work. Section 3 follows the operation of system so constructed. It ends with 
a list of things not in the model, lest they be forgotten. Finally, in Section 4, some 
critical implementation choices and details are discussed. 
2 
Functional Requirements 
Here we define requirements that a lower level implementation must satisfy to 
implement factals as we imagine them, so factals work as advertised. Currently it is 
easier to express this functionality as the operation of groups. The operation of 
individual factals should be derived from that. 
2.1 
Worldly Facts 
The computing elements we will define are called Factals. Each factal maintains a 
particular one-bit predicate fact1 about the world state 2. As an example, one such fact 
might be “I am standing”, which is at this moment either true or false (or characterized by 
a probability). All information the being has about the current conditions of the world is 
held in the active state of its factals; All information about models of the world are held in 
their interconnection patterns. Having this one form provides a major simplification to our 
understanding of the cortex, since the inner workings of all factals are similar. These 
functions include, we believe, a) constantly instantiating new factals to record newly 
explored parts of the enormous state space. Perhaps thousands must be born per second. 
We believe factals in a garden have the ability to learning at birth the fact that establishes 
their identity, thus each claiming its unique informational territory in a hierarchy of 
sophistication, where facts there range from very simple (e.g. “it is red”) to very highly 
sophisticated (e.g. “He will succeed”).  b) A factal continually makes an estimation of the 
current truth of its fact in the being’s perceived environment. Finally, c) a factal polls the 
desires of neighbors, and, acting through other factals makes the fact become true in the 
world if it can. (E.g. That I desire that ‘my body is I standing’ factal should be true causes 
                                                           
1 This definition could be expanded to an analog bimodal probability if needed. 
2 We presume the simple case that all observers agree on the facts. 

 
Design of a Conscious Machine 
213 
 
me to stand.) The architecture proposed here provides the being with many special 
abilities to support its learning of the world – it provides a computational environment that 
automatically embodies most of the capabilities required for consciousness.3 The use of 
binary predicates preclude world models from having scalar world computations such as x 
= 3.72 + v*t supported.  This decision simplifies the core, but requires multi-factal 
solutions, using fuzzy coding methods [2].  
There are several ways of encoding the estimate that a factal makes.  It might be a) 
the probability of truth, a value between 0 and 1.0 representing occurrence frequency 
statistically. This probability is updated when new information arrives, using 
Bayesian-like rules where new information is combined with existing a-priori 
information to produce an a-posteriori prediction (which is then compared to the 
actual for learning). b) This may extend as far as the belief networks of Pearl [3]. 
Another promising scheme is to c) encode the raw importance of this truth as its 
estimate. If two estimates compete, the more important one wins. d) Still other 
encoding strategies involve changing and averaging, needed for global convergence 
of the backbone of a simulation network. 
2.2 
Smaller Factal Groups 
A Worldly Variable (or variable for short) encodes information which is best 
expressed as an enumerated range of possible values (e.g. color={red, 
green,white}). It is implemented as a finite array of factals, one for each 
potential value. The range of a world variable may take on may change after 
learning4. Although a world variable may take on arbitrary estimates for its values, it 
is said to be “well behaved” if statistically it has at most one significantly active value 
at a time (and nil data is coded with all 0 values). We use algorithms which work best 
if this is the case, such as those in [4].  
Related Aspects are a groups of related variables in the data plane that are known 
to all refer to the same underlying physical object. Plants are groups of connected 
factals with a common purpose and reward structure. (Section 4 describe two plants: 
trees and coincidence records.)  Gardens are Evolutionary Computing environments 
which support the growth of plants. 
2.3 
Cognitive Functions 
A factal has a physical manifestation which manages the operation of a fact in the 
cognitive mechanism being constructed.  We also assume all mechanisms of a factal are 
collocated (possibly accounting for the neurological form of the minicolumn5). The 
functionality we believe is required to animate a factal is three-fold: a) situational 
reactions, b) reenactment simulation, and c) long term digital storage. To explain of their 
function, we resort to a metaphor called the Forest Castle Journal, as shown in Figure 1 
below.  (Although the description is metaphoric, precise algorithms are implied.) 
                                                           
3 We do not define consciousness, although we identify some of its functional correlates. 
4 This may require lower level support, involving neural or tubulin fascillation. 
5 Thus we might predict that older 3-layer archio cortex has no Reenactment simulator and/or 
Semantic Network. 

214 
P.A. King 
 
 
Fig. 1. Shows the Forest-Castle-Journal, with their attributes, and packaging as Factal 
There is an “Enchanted Forest” and in the forest there is a “Magical Castle” and in 
the castle there is the “Journal of All”.  The castle (enlivened by the actions of its 
inhabitants) controls and manages the forest for the castle’s purpose (e.g. to quarry 
granite or collect honey). Things in the forest will continue on as they were without the 
castle’s attention. All the history of the forest, including how castle has interacted with 
it, is kept in the Journal. It also contains a complete set of models of what happens in the 
forest. Castle inhabitants read the journal, and go into the forest and improve it. The 
journal is also indexed so that, at any moment of time, it is poised to have the castle 
predict the future of the forest.  The Journal is thought of as software, while the castle 
and forest hardware. The forest is thought of as analog while the castle and journal 
digital. The forest part of each factal is where the unconscious real time data flow 
processing of the data plane occurs, that determines the being’s situation and its instant 
reaction to it [5]. The castle part selects a particular forest area (but usually just one at a 
time6) to supervise its learning. (An interesting example is given in Section 4.3.) A 
forest area can get the castle’s attention in dire circumstances like fire. The mechanisms 
of the castle are strong functional correlates of consciousness. 
2.4 
Macro Structures of the Whole Cortex (Brodmann) 
The Data Plane operates real-time, as the forest. It contains selected factals of the 
brain which represents the experiences in various ways in different places. It can be 
thought of as a billion-bit computer word of factals, with specific sparse operations 
[6]. There is much internal structure to this word, since most of a factal’s connections 
are local, thus they are grouped into various kinds, modalities, and specialties of 
information, called information domains. These are probably the origin of Brodmann 
areas. The protocol binding the areas together into a single data plane is formed in 
layers, as are factals themselves, with a well defined interface for each: a) it maintains  
 
                                                           
6 This is just to keep things simple initially. Later, all kinds of synergies may be found by 
splitting the data plane into independent sections. 
Forest
senses/motors 
A Factal 
Castle 
Brain States 
of simulation 
Dedicated 
Filters 
Semantic 
Network 
cortex 
Journal 
Metaphoric 
Implementa-
FUNCTION 
1bit 
Modally-
Dedicated 
Processing 
Consciousness 
History and 
Memory 

 
Design of a Conscious Machine 
215 
 
consistent representations across the cortex of the current brain state, using data flow 
machine built from these related aspects, and processes associations as well. [7] With 
the ability to extract causality and representations, factals organize to (simplistically) 
detect lines in V1, 2d objects in V2, V3 … V4 motion, color, etc. We here believe 
that there are many further bidirectional filter layers above that, embodying the upper 
levels of our understanding. There are filters formed by all of our knowledge to the 
most sophisticated explanations we have of the world, passing through many 
cognitive areas such as the “what/where” paths and “the remembered present” 
[Edelman] All of the nuances we understand are held in this data plane. 
3 
Processing in an Information Domain 
Figure 2 shows the processing of a domain in more detail. 
 
Fig. 2. Shows a given information domain, with its four interacting areas: the world, a real-time 
area, an attentional area, a semantic network 
 
Situation Output 
Read/Write head 
Simulator Settings 
cauldron 
witch 
stir 
slowly 
reactions 
Action 
Tree
Situation 
Tree 
moment 
of now 
(add 
models 
below) 
Variable 
Attention 
Telescope 
Pan/Zoom 
Real 
Time 
Consciousness 
History 
history 
tape 
Associations 
#1
#2
#3
#5
#7
#9
#10
#11
#12
Unique 
Brain 
State: 
Compact 
Brain 
State: 
Binary 
Factals 
Castle 
plans 
Reenactment 
Simulator 
Learning 
Contexts 
#8
Language 
motors 
sensors 
C
Semantic 
Network: 
Jour nal 
Forest 
#4
Root Causes 
#6
Experience 
1 factal 
world 
models 
#13
World 
An Information Domain: 
#4

216 
P.A. King 
 
The “Information Domain” a machine can construct is limited by the information 
at its inputs. Restricting the number of factals inputs to domain is an effective way to 
limit the number of factals required to implement it. The same processing applies 
whether the information domain is very small (e.g. finite state machine sized), or very 
large (e.g. a whole brain). (There is much of  interest in between as well.) The being 
in the figure is experiencing a scene from Macbeth, as shown in a cartoon (#1). A 
single factal (of many) is show in red.  The various languages used for descriptions 
are shown arrayed vertically: factals, two kinds of brain states, and semantic 
networks. Remember throughout. All information flow in the brain is bidirectional, it 
is possible to recall a particular brain state from a history tape and move it down 
through layers into the simulator. There it is re-enacted it in simulation, and re-
experience. (To try this out now: think of your first kiss. Hopefully, you just recalled 
a brain state from memory and re-experienced it.) 
A dedicated real-time area in the bottom contains a Situation Tree (#2), Action 
trees (#3) and containing known behaviors, and low level reactions between them 
(e.g. stranger  don’t trust) (#4). Situation dynamically marks the being’s cognitive 
information space, as cairns mark trails above the tree-line. Where we are (amongst 
the cairns) can be evaluated at any time (presuming each estimates how close we are 
to it), and we can desire to be at a different cairn. Information for situation is 
determined at each level of the sophistication hierarchy, in one of three ways: a) 
Sensation: involves detecting causes based on the coincidences in lower level more 
sensory factals. This is sometimes called spatial pooling [8]. b) Modeling  involves 
detecting causes based on coincidences between past and present, to form a forward 
predictive model. This temporal pooling also helps recover hidden state. c) Expecting 
comes from higher (more global) places in the hierarchy and bias the local choices. 
They focus attention and set learning contexts. Information from each of these may or 
may not be present at any one moment.  They are combined by Bayesian methods in 
the data plane. Presence of redundant modalities fosters learning. 
An attentional mechanism can connect to various real-time areas of the forest. 
This is shows this as a telescope (#5), which can pan and zoom to various things we 
call zombies. Zombies are areas of the forest which do not currently have the attention 
of consciousness. The attentional mechanism feeds the Reenactment Simulator, a 
generative or forward model which predicts sensory information by mapping from 
causes to sensory consequences. It provides an internal model of the causal 
dependencies amongst states of the external world.  
We believe that to experience some phenomenon (#6) one must first simulate it 
inside the brain7. The Reenactment Simulator provides a single detailed analysis for a 
wide variety of patterns presented it. Descartes describes a similar element of 
consciousness, which Daniel Dennett names the Cartesian Theater [9]. (Descartes 
falsely claims it was localized in one spot.) Many others, including Helmholtz 
explored the central role of simulators in cognition. All this happened before our 
minds knew computers and could build simulators. 
                                                           
7 We do not speculate here on why this might be so, but believe that once this part of the 
design is working, answers to such questions will  be clearer. 

 
Design of a Conscious Machine 
217 
 
The simulation parameters (#7) (bidirectional) are the highest level of the 
Reenactment Simulator, and become part of the Situation Output. The typology of the 
sub-simulator configuration can be serialized by language (#8) and then de-serialized 
in the brain of another8, effectively transferring a meme. The Situation Output is the 
final output of situation, which is very much like a generalized form of place cells 
[10]. Limiting its values is important. Its elements are the basis for expressing every 
experience. The situation output is not well behaved, and must be fed into a schema of 
Coincidence Records (#9) to form a Unique Brain State (#10), a single-factal 
representation for every experienced pattern of the simulation output. It is built as a 
series of trees with shared leaves. These trees may also include internal analysis, 
attitudes, and actions associated with an experience. May additional factals may have 
to be added to encode each new experience, especially if it is novel. 
The Semantic Network has nodes which are Unique Brain States and links which 
indicate related states. The Reenactment Simulator can act as a write head, creating a 
tape as links recording past experiences. A list of these past now’s form a history tape, 
a perfect linear history tape of the past (#11).  An organic network editing processes 
“condenses meaning” (#12) and morphs the linear tapes into individual, separately 
usable models (#13), which are then inserted into the operation of the Reenactment 
Simulator, and the Situation and Action trees.  The factals of the Semantic Network 
also have links that are read by the Reenactment Simulator, to guide it in the 
prediction of the future. Links have two ends to connect factals.  (Links also have 
additional pointers to type and strength.) Some link types carry truth estimates during 
Situation, to create the estimate of a factal from its neighbors.  Other types connect 
factals of the Semantic Network, adding meanings by their presence. 
3.1 
Caveats 
We have described the simple model of cognition, but there are still many critical 
details which must be added for this design to be used in scalable systems the size of 
the brain. What is missing includes adding: continuous time, organic (not toy) 
encodings, decentralizing data, noisy data, world variables that are not well behaved, 
inaccuracy of efficient optimizations, seamless module boundaries, non-ideal 
situations, managing combinatorial explosions, detecting equivalent forms, limited 
fact instantiation rates, and distribution of centralized functions.  
4 
Implementation Notes 
Implementations may take multiple incidents to learn (e.g. to actually get the factals 
into a network). Implementations may use lossy information compression if it 
attempts to preserve meaning as much as possible. We believe that much of the robust 
nature of human thought is a synergetic byproduct of such mechanisms. 
Implementations may contain statistics on the number of occurrences akin to those of 
N-Arm Bandit processes [11]e to aid growth and culling.  
                                                           
8 This is easiest to understand when the a-priori informational structures in the two beings are 
identical. When they aren’t it’s harder. 

218 
P.A. King 
 
4.1 
The Modeling Strategy 
The brain knows it has good models of the world, if they are able to predict the future 
properly. We uses many small models, rather than one big model for two reasons: a) 
model building algorithms generally scale supra-linearly with the number of inputs 
(making big models is harder), and b) smaller models can be wired together 
differently in different situations, giving greater cognitive power to the simulation. A 
small model is shown in Figure 3 below. In it, the function G(Zi), the predicted value 
 
 
Fig. 3. An modeling element, in modeling mode 
of a World Variable U, is computed using a tree. The tree acts as a dynamic decoder 
[12], switching experiences to the leaf that best characterizes the current Zi, and growing 
new leaves as new experiences enter. The estimate G(Zi) is retrieved from selected leaf; 
the answer returns back the same path to the trunk. When the result inaccurately predicts 
U, an error signal X is generated and the tree updated to accept blame. This might 
involve adjusting Stoller split thresholds [13], but commonly involves adding new 
branches at the culpable part of the tree, to distinguish the new condition. New branches 
are usually chosen to be switched by the most active unused dimension in the elements 
of Zi. The conditions that caused the error are active in the path and this makes updating 
easier. For delayed reinforcement, a mechanism to save and recall those conditions, as 
they were at the time the estimate was made, is required. 
4.2 
Coincidence Records 
Coincidence Records are used to record that a particular set of things (those connected 
to the inputs) have ever occurred together. It is instantiated as quickly as possible after 
the first occurrence. Coincidence Records are composable – it is possible to make a 
new Coincidence Record from the outputs of several more detailed ones.  (Thus the 
lower levels of coincidence trees tend to live scattered (but contained within) specific 
Brodmann areas, and they are integrated in other areas of the data plane.) A schema of 
Coincidence Records is defined to generating a very large variable called the Unique 
Situation, which is well-defined and has a single-factal representations of all patterns 
encountered in the Situation Output.  
 
- 
predicted 
actual 
U
Zi 
error 
Optimizer 
X 
G(Zi) 
changes 
Zi 
X 
tree 
lookup 
 
Zi 
prediction 
G(Zi) 
value 

 
Design of a Conscious Machine 
219 
 
 
Fig. 4. A Coincidence Recorder, formed as a Bidirectional AND 
Bidirectional Coincidence Record elements are used. Figure 4 above shows one 
with two inputs and two users, but they can have many more (up to some 
implementation limit). The elements inside this bidirectional unit are unidirectional. In 
it, activating all the lower “sensory inputs” in the pattern activates the upper “output”. 
(The names “input” and “output” are with respect to these sensory signals, shown here 
in red going up the hierarchy.) Companion expectation signals shown in green, flow 
down the hierarchy. Activating the expectation of an upper “output” causes an 
expectation of the pattern to appear on the lower “inputs”. Coincidence Records also 
have an output D, that indicates that its association is TRUE but not expected. Other 
mechanisms monitor these unexpected signals during learning, and construct minimal 
trees to contain them. A generalized form performs a weighted sum and threshold, 
instead of the AND. By adjusting the threshold, it can perform fuzzy matches by using 
mid-threshold values, and a bidirectional OR. The OR is useful in declaring that all of 
its inputs are equivalent, hence decreases combinatorial explosions.  
4.3 
Hippocampal Routing  
Evolution seems to have an insatiable desire for more associations, beyond what we 
have shown with coincidence records. We propose the hippocampus performs a vital 
role in determining associations at the macroscopic level, where it acts like a “castle” in 
a central role closely akin to short term memory. Figure 5 below shows the system 
operation, as follows: Absent pre-filters and sharing, the current situation is fed directly 
to the hippocampus, which determines new associations. The hippocampus can learn 
new associations quickly, but can hold only a limited number of them. Once an 
association has been learned, the hippocampus also has the ability to transfer it to a 
much less expensive and more effective form, as a cortical pre-filter build from new 
axons and synapses. With the pre-filter in place, the hippocampus no longer has 
indication of the association, freeing space for new associations to be learned. One can 
think of the hippocampus as a CPU cache, writing its data back to memory, the cortex. 
 
S 
(sensed) 
E 
(expected)
S+
2 
S-
2 
upper “output”: 
lower inputs: 
S+
1 
D 
(unexpected) 
E+
1 
E+
2 
E-
2 
S-
1 E-
1 

220 
P.A. King 
 
 
Fig. 5. Shows the brain of a being, where the associations located in the hippocampus are used 
to grow filters 
Learned associations are moved to the cortex as Pre-filters using a process I call 
Hebbian Arborization. In it, axons are attracted to dendrite sites with which they co-
fire. It operates using axonal growth cones which are hyper-sensitive to the direction 
of signals that are coincident in time with their firing. This forms a Hebbian network 
which is much denser than classical Hebbian networks, and operates by growth rather 
than adjustment. The hippocampus repeatedly co-fires the coincident ports in a form 
of rehearsal, which dramatically accelerates the speed of Hebbian Arborization. (It 
can proceed continually when the brain is idle.) In our metaphor, this is the castle 
teaching the forest. The brain might have evolved this way if the computation for 
learning associations grows supra-linearly but can be shared amongst groups of 
factals. 
Event Sharing is yet another way to significantly increase the number of potential 
inputs signals, by organizing them into mutually exclusive groups in advance. We 
imagine a concentration multiplexor in the entorhinal cortex (EC) which selects which 
group is active. 
In summary, we have described a simple model with significantly human 
characteristics. In Algorithms to seeks out causes have been postulated. Much more 
background is available at http://brain-gears.blogspot.com. 
References 
1. Gross, C.: Genealogy of the “Grandmother Cell”. History of Neurosci. (2002) 
2. Stoica, A., Wingate, M., McLaughlin, P.: Fuzzy modelling and simulation for a fencing ro-
bot. In: 2nd Int. Conf. on Modelling and Simulation MS 1993, July 12-15 (1993) 
3. Halpern, J., Pearl, J.: Causes and Explanations: A Structural-Model Approach. In: 
Proceedings of the Seventeenth Conference on Uncertainly in AI (2001) 
4. Hawkins, J: On intelligence. Times Books (2003) 
5. Gladwell, M.: Blink: the power of thinking without thinking. Little Brown & Co. 
6. Kanerva, P.: Sparse Distributed Memory. MIT Press (1988) 
 
event sharing 
pre-filters 
situation 
senses 
co-firing 
growth 
attention 
detect 
associations 
produces 
hippocampus 
entorhinal 
cortex 
cortex 
V1…, Assoc 
Castle 
Forest 

 
Design of a Conscious Machine 
221 
 
7. Rumelhart. D.: Parallel Distributed Processing (1986) 
8. George, D., Hawkins, J.: Hierarchical Temporal Memory Concepts, Theory, and 
Terminology. Numenta Inc. 
9. Dennett, D.: Consciousness Explained (1991) 
10. Moser, E., Kropff, E., Britt-Moser, M.: Place Cells, Grid Cells, and the Brain’s Spatial 
Representation System. Annual Review of Neuroscience (2008) 
11. Gittins, J.C.: Bandit Processes and Dynamic Allocation Indices. J. R. Statist. Soc. B 41(2), 
148–177 (1979) 
12. Weidemann, H., Stear, E.: Entropy Analysis of Parameter Estimation. Information and 
Control 14 (1969) 
13. de Sá, J.P.M., Gama, J., Sebastião, R., Alexandre, L.A.: Decision trees using the minimum 
entropy-of-error principle. In: Jiang, X., Petkov, N. (eds.) CAIP 2009. LNCS, vol. 5702, 
pp. 799–807. Springer, Heidelberg (2009) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

222 
P.A. King 
 
In Dedication to Ray Solomonoff 
I worked at the MIT AI Lab at the same time as Ray did in the ‘60’s although never 
directly with him. After leaving MIT and making a good living in the 128 electronics 
belt, I got to know Ray as a regular in Arthur Shurcliff’s “Science Discussion Group”, 
which met monthly in a Chinese restaurant in back of MIT. It was formed by people 
who enjoyed of discussing Science. Ray was an old timer when I joined mid ‘80’s. He 
always exhibited the ability to take a set of concepts into some level of advanced 
mathematical understanding. He would listen for a while, and then offer some pretty 
mathematical understanding that would make me smile. When I pushed on him, he 
seemed to have an impressive weight to his understanding, but unfortunately we did 
not pursue it with him. I remember once driving him home, when he excitedly shared 
how if “he could get X to do Y, than it was probable that Z”. I never quite understood 
what X was, but I knew he’d be up late that night figuring it out. 
My wife asked me how Ray’s work affected mine, and I had to answer that in fact, 
I did not really know. Perhaps it supplies others such as myself a measurement tool. If 
you know, tell me. I do know that both of us came from the same place and have a 
strong desire to describe some part of human cognition, and thus make the task of 
understanding the human mind easier. And so I dedicate this work to Ray 
Solomonoff’s always-curious spirit, which has often inspired mine. 
 
 
 
 
5 
 
 
 
 
 
 
 
 
Circa 2005, on the left with Murry Denolsky, in the middle facing Arthur Shurcliff, 
discussing world oil models, if I remember right. To Ray’s left at the table are Fred Happgood, 
Eric Anderson, unknown, Bill Skocpol, Arthur Shurcliff and Hugh Field. On the right, Rays 
wife Grace, Steve Wintham, … 

No Free Lunch versus Occam’s Razor
in Supervised Learning
Tor Lattimore1 and Marcus Hutter1,2,3
1 Australian National University, Canberra, Australia
2 ETH Z¨urich, Switzerland
3 NICTA
{tor.lattimore,marcus.hutter}@anu.edu.au
Abstract. The No Free Lunch theorems are often used to argue that
domain speciﬁc knowledge is required to design successful algorithms.
We use algorithmic information theory to argue the case for a universal
bias allowing an algorithm to succeed in all interesting problem domains.
Additionally, we give a new algorithm for oﬀ-line classiﬁcation, inspired
by Solomonoﬀinduction, with good performance on all structured (com-
pressible) problems under reasonable assumptions. This includes a proof
of the eﬃcacy of the well-known heuristic of randomly selecting training
data in the hope of reducing the misclassiﬁcation rate.
Keywords: Supervised learning, Kolmogorov complexity, no free lunch,
Occam’s razor.
1
Introduction
The No Free Lunch (NFL) theorems, stated and proven in various settings and
domains [16,26,27], show that no algorithm performs better than any other when
their performance is averaged uniformly over all possible problems of a particular
type.1 These are often cited to argue that algorithms must be designed for a
particular domain or style of problem, and that there is no such thing as a
general purpose algorithm.
On the other hand, Solomonoﬀinduction [18,19] and the more general AIXI
model [9] appear to universally solve the sequence prediction and reinforcement
learning problems respectively. The key to the apparent contradiction is that
Solomonoﬀinduction and AIXI do not assume that each problem is equally
likely. Instead they apply a bias towards more structured problems. This bias
is universal in the sense that no class of structured problems is favored over
another. This approach is philosophically well justiﬁed by Occam’s razor.
The two classic domains for NFL theorems are optimisation and classiﬁcation.
In this paper we will examine classiﬁcation and only remark that the case for
optimisation is more complex. This diﬀerence is due to the active nature of
optimisation where actions aﬀect future observations.
1 Such results have been less formally discussed long before by Watanabe in 1969 [25].
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 223–235, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

224
T. Lattimore and M. Hutter
Previously, some authors have argued that the NFL theorems do not disprove
the existence of universal algorithms for two reasons.
1. That taking a uniform average is not philosophically the right thing to do,
as argued informally in [7].
2. Carroll and Seppi in [1] note that the NFL theorem measures performance as
misclassiﬁcation rate, where as in practise, the utility of a misclassiﬁcation
in one direction may be more costly than another.
We restrict our consideration to the task of minimising the misclassiﬁcation rate
while arguing more formally for a non-uniform prior inspired by Occam’s razor
and formalised by Kolmogorov complexity. We also show that there exist algo-
rithms (unfortunately only computable in the limit) with very good properties
on all structured classiﬁcation problems.
The paper is structured as follows. First, the required notation is introduced
(Section 2). We then state the original NFL theorem, give a brief introduction
to Kolmogorov complexity, and show that if a non-uniform prior inspired by
Occam’s razor is used, then there exists a free lunch (Section 3). Finally, we give
a new algorithm inspired by Solomonoﬀinduction with very attractive properties
in the classiﬁcation problem (Section 4).
2
Preliminaries
Here we introduce the required notation and deﬁne the problem setup for the
No Free Lunch theorems.
Strings. A ﬁnite string x over alphabet X is a ﬁnite sequence x1x2x3 · · · xn−1xn
with xi ∈X. An inﬁnite string x over alphabet X is an inﬁnite sequence
x1x2x3 · · · . Alphabets are usually countable or ﬁnite, while in this paper they
will almost always be binary. For ﬁnite strings we have a length function deﬁned
by ℓ(x) := n for x = x1x2 · · · xn. The empty string of length 0 is denoted by ϵ.
The set Xn is the set of all strings of length n. The set X∗is the set of all ﬁnite
strings. The set X∞is the set of all inﬁnite strings. Let x be a string (ﬁnite or
inﬁnite) then substrings are denoted xs:t := xsxs+1 · · · xt−1xt where s ≤t. A
useful shorthand is x<t := x1:t−1. Let x, y ∈X∗and z ∈X∞with ℓ(x) = n and
ℓ(y) = m then
xy := x1x2, · · · xn−1xny1y2 · · · ym−1ym
xz := x1x2, · · · xn−1xnz1z2z3 · · ·
As expected, xy is ﬁnite and has length ℓ(xy) = n + m while xz is inﬁnite. For
binary strings, we write #1(x) and #0(x) to mean the number of 0’s and number
of 1’s in x respectively.
Classiﬁcation. Informally, a classiﬁcation problem is the task of matching fea-
tures to class labels. For example, recognizing handwriting where the features
are images and the class labels are letters. In supervised learning, it is (usu-
ally) unreasonable to expect this to be possible without any examples of correct

No Free Lunch versus Occam’s Razor in Supervised Learning
225
classiﬁcations. This can be solved by providing a list of feature/class label pairs
representing the true classiﬁcation of each feature. It is hoped that these exam-
ples can be used to generalize and correctly classify other features.
The following deﬁnitions formalize classiﬁcation problems, algorithms capable
of solving them, as well as the loss incurred by an algorithm when applied to
a problem, or set of problems. The setting is that of transductive learning as
in [3].
Deﬁnition 1 (Classiﬁcation Problem). Let X and Y be ﬁnite sets repre-
senting the feature space and class labels respectively. A classiﬁcation problem
over X, Y is deﬁned by a function f : X →Y where f(x) is the true class label
of feature x.
In the handwriting example, X might be the set of all images of a particular
size and Y would be the set of letters/numbers as well as a special symbol for
images that correspond to no letter/number.
Deﬁnition 2 (Classiﬁcation Algorithm). Let f be a classiﬁcation problem
and Xm ⊆X be the training features on which f will be known. We write fXm
to represent the function fXm : Xm →Y with fXm(x) := f(x) for all x ∈Xm.
A classiﬁcation algorithm is a function, A, where A(fXm, x) is its guess for the
class label of feature x ∈Xu := X −Xm when given training data fXm. Note we
implicitly assume that X and Y are known to the algorithm.
Deﬁnition 3 (Loss function). The loss of algorithm A, when applied to classi-
ﬁcation problem f, with training data Xm is measured by counting the proportion
of misclassiﬁcations in the testing data, Xu.
LA(f, Xm) :=
1
|Xu|

x∈Xu
[[A(fXm, x) ̸= f(x)]]
where [[]] is the indicator function deﬁned by, [[expr]] = 1 if expr is true and 0
otherwise.
We are interested in the expected loss of an algorithm on the set of all problems
where expectation is taken with respect to some distribution P.
Deﬁnition 4 (Expected loss). Let M be the set of all functions from X to
Y and P be a probability distribution on M. If Xm is the training data then the
expected loss of algorithm A is
LA(P, Xm) :=

f∈M
P(f)LA(f, Xm)
3
No Free Lunch Theorem
We now use the above notation to give a version of the No Free Lunch Theorem
of which Wolpert’s is a generalization.

226
T. Lattimore and M. Hutter
Theorem 1 (No Free Lunch). Let P be the uniform distribution on M. Then
the following holds for any algorithm A and training data Xm ⊆X.
LA(P, Xm) = |Y −1|/|Y |
(1)
The key to the proof is the following observation. Let x ∈Xu, then for all y ∈Y ,
P(f(x) = y|f|Xm) = P(f(x) = y) = 1/|Y |. This means no information can be
inferred from the training data, which suggests no algorithm can be better than
random.
Occam’s razor/Kolmogorov Complexity. The theorem above is often used
to argue that no general purpose algorithm exists and that focus should be placed
on learning in speciﬁc domains.
The problem with the result is the underlying assumption that P is uniform,
which implies that training data provides no evidence about the true class labels
of the test data. For example, if we have classiﬁed the sky as blue for the last
1,000 years then a uniform assumption on the possible sky colours over time
would indicate that it is just as likely to be green tomorrow as blue, a result
that goes against all our intuition.
How then, do we choose a more reasonable prior? Fortunately, this question
has already been answered heuristically by experimental scientists who must
endlessly choose between one of a number of competing hypotheses. Given any
experiment, it is easy to construct a hypothesis that ﬁts the data by using
a lookup table. However such hypotheses tend to have poor predictive power
compared to a simple alternative that also matches the data. This is known as the
principle of parsimony, or Occam’s razor, and suggests that simple hypotheses
should be given a greater weight than more complex ones.
Until recently, Occam’s razor was only an informal heuristic. This changed
when Solomonoﬀ, Kolmogorov and Chaitin independently developed the ﬁeld of
algorithmic information theory that allows for a formal deﬁnition of Occam’s
razor. We give a brief overview here, while a more detailed introduction can
be found in [13]. An in depth study of the philosophy behind Occam’s razor
and its formalisation by Kolmogorov complexity can be found in [12,15]. While
we believe Kolmogorov complexity is the most foundational formalisation of
Occam’s razor, there have been other approaches such as MML [23] and MDL
[8]. These other techniques have the advantage of being computable (given a
computable prior) and so lend themselves to good practical applications.
The idea of Kolmogorov complexity is to assign to each binary string an integer
valued complexity that represents the length of its shortest description. Those
strings with short descriptions are considered simple, while strings with long
descriptions are complex. For example, the string consisting of 1,000,000 1’s can
easily be described as “one million ones”. On the other hand, to describe a string
generated by tossing a coin 1,000,000 times would likely require a description
about 1,000,000 bits long. The key to formalising this intuition is to choose a
universal Turing machine as the language of descriptions.
Deﬁnition 5 (Kolmogorov Complexity). Let U be a universal Turing ma-
chine and x ∈B∗be a ﬁnite binary string. Then deﬁne the plain Kolmogorov

No Free Lunch versus Occam’s Razor in Supervised Learning
227
complexity C(x) to be the length of the shortest program (description) p such
that U(p) = x.
C(x) := min
p∈B∗{ℓ(p) : U(p) = x}
It is easy to show that C depends on choice of universal Turing machine U only
up to a constant independent of x and so it is standard to choose an arbitrary
reference universal Turing machine.
For technical reasons it is diﬃcult to use C as a prior, so Solomonoﬀintroduced
monotone machines to construct the Solomonoﬀprior, M. A monotone Turing
machine has one read-only input tape which may only be read from left to right
and one write-only output tape that may only be written to from left to right. It
has any number of working tapes. Let T be such a machine and write T (p) = x
to mean that after reading p, x is on the output tape. The machines are called
monotone because if p is a preﬁx of q then T (p) is a preﬁx of T (q). It is possible
to show there exists a universal monotone Turing machine U and this is used to
deﬁne monotone complexity Km and Solomonoﬀ’s prior, M.
Deﬁnition 6 (Monotone Complexity). Let U be the reference universal
monotone Turing machine then deﬁne Km, M and KM as follows,
Km(x) := min {ℓ(p) : U(p) = x∗}
M(x) :=

U(p)=x∗
2−ℓ(p)
KM(x) := −log M(x)
where U(p) = x∗means that when given input p, U outputs x possibly followed
by more bits.
Some facts/notes follow.
1. For any n, 
x∈Bn M(x) ≤1.
2. Km, M and KM are incomputable.
3. 0 < KM(x) ≈Km(x) ≈C(x) < ℓ(x) + O(1)2
To illustrate why M gives greater weight to simple x, suppose x is simple then
there exists a relatively short monotone Turing machine p, computing it. There-
fore Km(x) is small and so 2−Km(x) ≈M(x) is relatively large.
Since M is a semi-measure rather than a proper measure, it is not appropri-
ate to use it in place of P when computing expected loss. However it can be
normalized to a proper measure, Mnorm deﬁned inductively by
Mnorm(ϵ) := 1
Mnorm(xb) := Mnorm(x)
M(xb)
M(x0) + M(x1)
2 The approximation C(x) ≈Km(x) is only accurate to log ℓ(x), while KM ≈Km
is almost always very close [5,6]. This is a little surprising since the sum in the
deﬁnition of M contains 2−Km. It shows that there are only comparitively few short
programs for any x.

228
T. Lattimore and M. Hutter
Note that this normalisation is not unique, but is philosophically and tech-
nically the most attractive and was used and defended by Solomonoﬀ. For
a discussion of normalisation, see [13, p.303]. The normalised version satisﬁes

x∈Bn Mnorm(x) = 1.
We will also need to deﬁne M/KM with side information, M(y; x) := M(y)
where x∗is provided on a spare tape of the universal Turing machine. Now deﬁne
KM(y; x) := −log M(y; x). This allows us to deﬁne the complexity of a function
in terms of its output relative to its input.
Deﬁnition 7 (Complexity of a function). Let X = {x1, · · · , xn} ⊆Bk and
f : X →B then deﬁne the complexity of f, KM(f; X) by
KM(f; X) := KM(f(x1)f(x2) · · · f(xn); x1, x2, · · · , xn)
An example is useful to illustrate why this is a good measure of the complexity
of f.
Example 1. Let X ⊆Bn for some n, and Y = B and f : X →Y be deﬁned by
f(x) = [[xn = 1]]. Now for a complex X, the string f(x1)f(x2) · · · might be diﬃ-
cult to describe, but there is a very short program that can output f(x1)f(x2) · · ·
when given x1x2 · · · as input. This gives the expected result that KM(f; X) is
very small.
Free Lunch Using SolomonoﬀPrior. We are now ready to use Mnorm as a
prior on a problem family. The following proposition shows that when problems
are chosen according to the Solomonoﬀprior that there is a (possibly small) free
lunch.
Before the proposition, we remark on problems with maximal complexity,
KM(f; X) = O(|X|). In this case f exhibits no structure allowing it to be
compressed, which turns out to be equivalent to being random in every intuitive
sense [14]. We do not believe such problems are any more interesting than trying
to predict random coin ﬂips. Further, the NFL theorems can be used to show
that no algorithm can learn the class of random problems by noting that almost
all problems are random. Thus a bias towards random problems is not much of
a bias (from uniform) at all, and so at most leads to a decreasingly small free
lunch as the number of problems increases.
Proposition 1 (Free lunch under Solomonoﬀprior). Let Y = B and ﬁx a
k ∈N. Now let X = Bn and Xm ⊂X such that |Xm| = 2n −k. For suﬃciently
large n there exists an algorithm A such that
LA(Mnorm, Xm) < 1/2
The proof is omitted due to space limitations, but the idea is very simple.
Consider the algorithm such that A(f|Xm, x) = 1 if f(x) = 1 for all x ∈Xm and
A(f|Xm, x) is random otherwise. Then show that if the amount of training data
is extremely large relative to the testing data then the Solomonoﬀprior assigns

No Free Lunch versus Occam’s Razor in Supervised Learning
229
greater weight to the function f1(x) := 1 for all x than the set of functions
satisfying f(x) = 1 for all x ∈Xm but f(x) ̸= 1 for some x ∈Xu.
The proposition is unfortunately extremely weak. It is more interesting to
know exactly what conditions are required to do much better than random. In
the next section we present an algorithm with good performance on all well
structured problems when given “good” training data. Without good training
data, even assuming a Solomonoﬀprior, we believe it is unlikely that the best
algorithm will perform well.
Note that while it appears intuitively likely that any non-uniform distribution
such as Mnorm might oﬀer a free lunch, this is in fact not true. It is shown in
[17] that there exist non-uniform distributions where the loss over a problem
family is independent of algorithm. These distributions satisfy certain symmetry
conditions not satisﬁed by Mnorm, which allows Proposition 1 to hold.
4
Complexity-Based Classiﬁcation
Solomonoﬀinduction is well known to solve the online prediction problem where
the true value of each classiﬁcation is known after each guess. In our setup,
the true classiﬁcation is only known for the training data, after which the al-
gorithm no longer receives feedback. While Solomonoﬀinduction can be used
to bound the number of total errors while predicting deterministic sequences, it
gives no indication of when these errors may occur. For this reason we present a
complexity-inspired algorithm with better properties for the oﬄine classiﬁcation
problem.
Before the algorithm we present a little more notation. As usual, let X =
{x1, x2, · · · , xn} ⊆Bk, Y = B and let Xm ⊆X be the training data. Now deﬁne
an indicator function χ by χi := [[xi ∈Xm]].
Deﬁnition 8. Let f ∈Y X be a classiﬁcation problem. The algorithm A∗is
deﬁned in two steps.
˜f := arg min
˜f∈Y X

KM( ˜f; X) : χi = 1 =⇒˜f(xi) = f(xi)

A∗(fXm, xi) := ˜f(xi)
Essentially A∗chooses for its model the simplest ˜f consistent with the training
data and uses this for classifying unseen data. Note that the deﬁnition above
only uses the value yi = f(xi) where χi = 1, and so it does not depend on
unseen labels.
If KM(f; X) is “small” then the function we wish to learn is simple so we
should expect to be able to perform good classiﬁcation, even given a relatively
small amount of training data. This turns out to be true, but only with a good
choice of training data. It is well known that training data should be “broad
enough”, and this is backed up by the example below and by Theorem 2, which
give an excellent justiﬁcation for random training data based on good theoretical

230
T. Lattimore and M. Hutter
Fig. 1. A simple problem
(Theorem 2) and philosophical (AIT) underpinnings. The following example
demonstrates the eﬀect of bad training data on the performance of A∗.
Example 2. Let X = {0000, 0001, 0010, 0011, · · · , 1101, 1110, 1111} and f(x) be
deﬁned to be the ﬁrst bit of x as in Figure 1. Now suppose χ = 1808 (So the
algorithm is only allowed to see the true class labels of x1 through x8). In this
case, the simplest ˜f consistent with the ﬁrst 16 data points, all of which are
zeros, is likely to be ˜f(x) = 0 for all x ∈X and so A∗will fail on every piece of
testing data!
On the other hand, if χ = 001010011101101, which was generated by tossing
a coin 16 times, then ˜f will very likely be equal to f and so A∗will make no
errors. Even if χ is zero about the critical point in the middle (χ8 = χ9 = 0)
then ˜f should still match f mostly around the left and right and will only be
unsure near the middle.
Note, the above is not precisely true since for small strings the dependence
of KM on the universal monotone Turing machine can be fairly large. However
if we increase the size of the example so that |X| > 1000 then these quirks
disappear for natural reference universal Turing machines.
Deﬁnition 9 (Entropy). Let θ ∈[0, 1]
H(θ) :=

−[θ log θ + (1 −θ) log(1 −θ)]
if θ ̸= 0 and θ ̸= 1
0
otherwise
Theorem 2. Let θ ∈(0, 1) be the proportion of data to be given for training
then:
1. There exists a χ ∈B∞(training set) such that for all n ∈N, θn −c1 <
#1(χ1:n) < θn + c1 and nH(θ) −c2 < KM(χ1:n) for some c1, c2 ∈R+.
2. For n = |X|, the loss of algorithm A∗when using training data determined
by χ is bounded by
LA∗(f, Xm) <
2KM(f; X) + KM(X) + c2 + c3
n(1 −θ −c1/n) log(1 −θ + c1/n)−1
where c3 is some constant independent of all inputs.

No Free Lunch versus Occam’s Razor in Supervised Learning
231
This theorem shows that A∗will do well on all problems satisfying KM(f; X) =
o(n) when given good (but not necessarily a lot) of training data. Before the
proof, some remarks.
1. The bound is a little messy, but for small θ, large n and simple X we get
LA∗(f, Xm)
≈< 2KM(f; X)/(nθ).
2. The loss bound is extremely bad for large θ. We consider this unimportant
since we only really care if θ is small. Also, note that if θ is large then the
number of points we have to classify is small and so we still make only a few
mistakes.
3. The constants c1, c2 and c3 are relatively small (around 100-500). They rep-
resent the length of the shortest programs computing simple transformations
or encodings. This is dependent on the universal Turing machine used to de-
ﬁne the Solomonoﬀdistribution, but for a natural universal Turing machine
we expect it to be fairly small [9, sec.2.2.2].
4. The “special” χ is not actually that special at all. In fact, it can be generated
easily with probability 1 by tossing a coin with bias θ inﬁnitely often. More
formally, it is a μ Martin-L¨of random string where μ(1|x) = θ for all x. Such
strings form a μ-measure 1 set in B∞.
Proof (Theorem 2). The ﬁrst is a basic result in algorithmic information the-
ory [13, p.318]. Essentially choosing χ to be Martin-L¨of random with respect
to a Bernoulli process parameterized by θ. From now on, let ¯θ = #1(χ)/n.
For simplicity we write x := x1x2 · · · xn, y := f(x1)f(x2) · · · f(xn), and ˜y :=
˜f(x1) ˜f(x2) · · · ˜f(xn). Deﬁne indicator ψ by ψi := [[χi = 0 ∧yi = ˜yi]]. Now note
that there exists c3 ∈R such that
KM(χ1:n) < KM(ψ1:n; y, ˜y) + KM(y; x) + KM(˜y; x) + KM(x) + c3
(2)
This follows since we can easily use y, ˜y and ψ1:n to recover χ1:n by χi = 1 if and
only if yi = ˜yi and ψi ̸= 1. The constant c3 is the length of the reconstruction
program. Now KM(˜y; x) ≤KM(y; x) follows directly from the deﬁnition of ˜f.
We now compute an upper bound on KM(ψ). Let α := LA∗(f, Xm) be the
proportion of the testing data on which A∗makes an error. The following is easy
to verify:
1. #1(ψ) = (1 −α)(1 −¯θ)n
2. #0(ψ) = (1 −(1 −α)(1 −¯θ))n
3. yi ̸= ˜yi =⇒ψi = 0
4. #1(y ⊕˜y) = α(1 −¯θ)n where ⊕is the exclusive or function.
We can use point 3 above to trivially encode ψi when ˜yi ̸= yi. Aside from these,
there are exactly ¯θn 0’s and (1 −α)(1 −¯θ)n 1’s. Coding this subsequence using
frequency estimation gives a code for ψ1:n given y and ˜y, which we substitute
into (2).
nH(¯θ) −c2 ≤KM(χ1:n) ≤KM(ψ1:n; y, ˜y) + KM(y; x) + KM(˜y; x)
+ KM(x) + c3
(3)
≤2KM(y; x) + KM(x) + nJ(¯θ, α) + c3

232
T. Lattimore and M. Hutter
where J(¯θ, α) :=

¯θ + (1 −¯θ)(1 −α)

H
¯θ/

¯θ + (1 −¯θ)(1 −α)

. An easy tech-
nical result (Lemma 1 in the appendix) shows that for ¯θ ∈(0, 1)
0 ≤α(1 −¯θ) log
1
1 −¯θ ≤H(¯θ) −J(¯θ, α)
Therefore nα(1 −¯θ) log
1
1−¯θ ≤2KM(y; x) + KM(x) + c2 + c3. The result follows
by rearranging and using part 1 of the theorem.
⊓⊔
Since the features are known, it is unexpected for the bound to depend on their
complexity, KM(X). Therefore it is not surprising that this dependence can be
removed at a small cost, and with a little extra eﬀort.
Theorem 3. Under the same conditions as Theorem 2, the loss of A∗is bounded
by
LA∗(f, Xm) < 2KM(f; X) + 2 [log |X| + log log |X|] + c
n(1 −θ −c1/n) log(1 −θ + c1/n)−1
where c is some constant independent of inputs.
This version will be preferred to Theorem 2 in cases where KM(X) > 2
[log |X| + log log |X|]. The proof of Theorem 3 is almost identical to that of
Theorem 2.
Proof sketch: The idea is to replace equation (2) by
KM(χ1:n, x) < KM(ψ1:n; y, ˜y) + KM(y; x) + KM(˜y; x) + KM(x) + c3
(4)
Then use the following identities K(χ1:n; x, K(x))+K(x)<K(χ1:n, x)−K(ℓ(x))<
KM(χ1:n, x) where the inequalities are true up to constants independent of x and
χ. Next a counting argument in combination with Stirling’s approximation can be
used to show that for most χ satisfying the conditions in Theorem 2 have KM(χ1:n)
< K(χ1:n) < K(χ1:n; x, K(x)) + log ℓ(x) + r for some constant r > 0 indepen-
dent of x and χ. Finally use KM(x) < K(x) for all x and K(ℓ(x)) < log ℓ(x) +
2 log log ℓ(x) + r for some constant r > 0 independent of x to rearrange (4) into
KM(χ1:n) < KM(ψ1:n; y, ˜y) + KM(y; x) + KM(˜y; x) + 2 log ℓ(x)
+ 2 log log ℓ(x) + c
for some constant c > 0 independent of χ, ψ, x and y. Finally use the techniques
in the proof of Theorem 2 to complete the proof.
⊓⊔
5
Discussion
Summary. Proposition 1 shows that if problems are distributed according to
their complexity, as Occam’s razor suggests they should, then a (possibly small)
free lunch exists. While the assumption of simplicity still represents a bias to-
wards certain problems, it is a universal one in the sense that no style of struc-
tured problem is more favoured than another.
In Section 4 we gave a complexity-based classiﬁcation algorithm and proved
the following properties:

No Free Lunch versus Occam’s Razor in Supervised Learning
233
1. It performs well on problems that exhibit some compressible structure,
KM(f; X) = o(n).
2. Increasing the amount of training data decreases the error.
3. It performs better when given a good (broad/randomized) selection of train-
ing data.
Theorem 2 is reminiscent of the transductive learning bounds of Vapnik and
others [3,20,21], but holds for all Martin-L¨of random training data, rather than
with high probability. This is diﬀerent to the predictive result in Solomonoﬀ
induction where results hold with probability 1 rather than for all Martin-L¨of
random sequences [11]. If we assume the training set is sampled randomly, then
our bounds are comparable to those in [3].
Unfortunately, the algorithm of Section 4 is incomputable. However Kol-
mogorov complexity can be approximated via standard compression algorithms,
which may allow for a computable approximation of the classiﬁer of Section
4. Such approximations have had some success in other areas of AI, including
general reinforcement learning [22] and unsupervised clustering [2].
Occam’s razor is often thought of as the principle of choosing the simplest
hypothesis matching your data. Our deﬁnition of simplest is the hypothesis that
minimises KM(f; X) (maximises M(f; X)). This is perhaps not entirely natural
from the informal statement of Occam’s razor, since M(x) contains contribu-
tions from all programs computing x, not just the shortest. We justify this by
combining Occam’s razor with Epicurus principle of multiple explanations that
argues for all consistent hypotheses to be considered. In some ways this is the
most natural interpretation as no scientist would entirely rule out a hypothe-
sis just because it is slightly more complex than the simplest. A more general
discussion of this issue can be found in [4, sec.4]. Additionally, we can argue
mathematically that since KM ≈Km, the simplest hypothesis is very close to
the mixture.3 Therefore the debate is more philosophical than practical in this
setting.
An alternative approach to formalising Occam’s razor has been considered
in MML [23]. However, in the deterministic setting the probability of the data
given the hypothesis satisﬁes P(D|H) = 1. This means the two part code reduces
to the code-length of the prior, log(1/P(H)). This means the hypothesis with
minimum message length depends only on the choice of prior, not the complexity
of coding the data. The question then is how to choose the prior, on which MML
gives no general guidance. Some discussion of Occam’s razor from a Kolmogorov
complexity viewpoint can be found in [10,12,15], while the relation between MML
and Kolmogorov complexity is explored in [24].
Assumptions. We assumed ﬁnite X, Y , and deterministic f, which is the
standard transductive learning setting. Generalisations to countable spaces may
still be possible using complexity approaches, but non-computable real numbers
prove more diﬃcult. One can either argue by the strong Church-Turing thesis
3 The bounds of Section 4 would depend on the choice of complexity at most logarith-
mically in |X| with KM providing the uniformly better bound.

234
T. Lattimore and M. Hutter
that non-computable reals do not exist, or approximate them arbitrarily well.
Stochastic f are interesting and we believe a complexity-based approach will still
be eﬀective, although the theorems and proofs may turn out to be somewhat
diﬀerent.
Acknowledgements. We thank Wen Shao and reviewers for valuable feedback
on earlier drafts and the Australian Research Council for support under grant
DP0988049.
References
1. Carroll, J., Seppi, K.: No-free-lunch and Bayesian optimality. In: IJCNN Workshop
on Meta-Learning (2007)
2. Cilibrasi, R., Vitanyi, P.: Clustering by compression. IEEE Transactions on Infor-
mation Theory 51(4), 1523–1545 (2005)
3. Derbeko, P., El-yaniv, R., Meir, R.: Error bounds for transductive learning via
compression and clustering. In: NIPS, vol. 16 (2004)
4. Dowe, D.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of Philosophy of Statistics, vol. 7,
pp. 901–982. Elsevier (2011)
5. G´acs, P.: On the relation between descriptional complexity and algorithmic prob-
ability. Theoretical Computer Science 22(1-2), 71–93 (1983)
6. G´acs, P.: Expanded and improved proof of the relation between description com-
plexity and algorithmic probability (2008) (unpublished)
7. Giraud-Carrier, C., Provost, F.: Toward a justiﬁcation of meta-learning: Is the
no free lunch theorem a show-stopper. In: ICML Workshop on Meta-Learning,
pp. 9–16 (2005)
8. Gr¨unwald, P.: The Minimum Description Length Principle. MIT Press Books,
vol. 1. The MIT Press (2007)
9. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability. Springer, Berlin (2004)
10. Hutter, M.: A complete theory of everything (will be subjective). Algorithms 3(4),
329–350 (2010)
11. Hutter, M., Muchnik, A.: On semimeasures predicting Martin-L¨of random se-
quences. Theoretical Computer Science 382(3), 247–261 (2007)
12. Kirchherr, W., Li, M., Vitanyi, P.: The miraculous universal distribution. The
Mathematical Intelligencer 19(4), 7–15 (1997)
13. Li, M., Vitanyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions, 3rd edn. Springer (2008)
14. Martin-L¨of, P.: The deﬁnition of random sequences. Information and Control 9(6),
602–619 (1966)
15. Rathmanner, S., Hutter, M.: A philosophical treatise of universal induction. En-
tropy 13(6), 1076–1136 (2011)
16. Schaﬀer, C.: A conservation law for generalization performance. In: Proceedings of
the Eleventh International Conference on Machine Learning, pp. 259–265. Morgan
Kaufmann (1994)
17. Schumacher, C., Vose, M., Whitley, L.: The no free lunch and problem descrip-
tion length. In: Spector, L., Goodman, E.D. (eds.) GECCO 2001: Proc. of the
Genetic and Evolutionary Computation Conf., pp. 565–570. Morgan Kaufmann,
San Francisco (2001)

No Free Lunch versus Occam’s Razor in Supervised Learning
235
18. Solomonoﬀ, R.: A formal theory of inductive inference, Part I. Information and
Control 7(1), 1–22 (1964)
19. Solomonoﬀ, R.: A formal theory of inductive inference, Part II. Information and
Control 7(2), 224–254 (1964)
20. Vapnik, V.: Estimation of Dependences Based on Empirical Data. Springer, New
York (1982)
21. Vapnik, V.: The Nature of Statistical Learning Theory, 2nd edn. Springer, Berlin
(2000)
22. Veness, J., Ng, K.S., Hutter, M., Uther, W., Silver, D.: A Monte Carlo AIXI ap-
proximation. Journal of Artiﬁcial Intelligence Research 40, 95–142 (2011)
23. Wallace, C., Boulton, D.: An information measure for classiﬁcation. The Computer
Journal 11(2), 185–194 (1968)
24. Wallace, C., Dowe, D.: Minimum message length and Kolmogorov complexity. The
Computer Journal 42(4), 270–283 (1999)
25. Watanabe, S., Donovan, S.: Knowing and guessing; a quantitative study of inference
and information. Wiley, New York (1969)
26. Wolpert, D.: The supervised learning no-free-lunch theorems. In: Proc. 6th Online
World Conference on Soft Computing in Industrial Applications, pp. 25–42 (2001)
27. Wolpert, D., Macready, W.: No free lunch theorems for optimization. IEEE Trans-
actions on Evolutionary Computation 1(1), 67–82 (1997)
A
Technical Proofs
Lemma 1 (Entropy inequality).
0 ≤α(1 −θ) log
1
1 −θ
(5)
≤H(θ) −[θ + (1 −θ)(1 −α)] H
,
θ
θ + (1 −θ)(1 −α)
-
(6)
With equality only if θ ∈{0, 1} or α = 0
Proof. First, (5) is trivial. To prove (6), note that for α = 0 or θ ∈{0, 1},
equality is obvious. Now, ﬁxing θ ∈(0, 1) and computing.
∂
∂α
.
H(θ) −[θ + (1 −θ)(1 −α)] H
,
θ
θ + (1 −θ)(1 −α)
-/
= (1 −θ) log 1 −α(1 −θ)
(1 −α)(1 −θ)
≥(1 −θ) log(1 −θ)−1
Therefore integrating both sides over α gives,
α(1 −θ) log(1 −θ)−1 ≤H(θ) −[θ + (1 −θ)(1 −α)] H
,
θ
θ + (1 −θ)(1 −α)
-
as required.
⊓⊔

An Approximation of the Universal
Intelligence Measure
Shane Legg1 and Joel Veness2
1 DeepMind Technologies Ltd
shane@deepmind.com
2 University of Alberta, Edmonton, AB, Canada
veness@cs.ualberta.ca
Abstract. The Universal Intelligence Measure is a recently proposed
formal deﬁnition of intelligence. It is mathematically speciﬁed, extremely
general, and captures the essence of many informal deﬁnitions of intelli-
gence. It is based on Hutter’s Universal Artiﬁcial Intelligence theory, an
extension of Ray Solomonoﬀ’s pioneering work on universal induction.
Since the Universal Intelligence Measure is only asymptotically com-
putable, building a practical intelligence test from it is not straightfor-
ward. This paper studies the practical issues involved in developing a
real-world UIM-based performance metric. Based on our investigation,
we develop a prototype implementation which we use to evaluate a num-
ber of diﬀerent artiﬁcial agents.
1
Introduction
A fundamental problem in strong artiﬁcial intelligence is the lack of a clear and
precise deﬁnition of intelligence itself. This makes it diﬃcult to study the theo-
retical or empirical aspects of broadly intelligent machines. Of course there is the
well-known Turing Test [20], however this paradoxically seems to be more about
dodging the diﬃcult problem of explicitly deﬁning intelligence than addressing
the real issue. We believe that until we have a more precise deﬁnition of intel-
ligence, the quest for generally intelligent machines will lack reliable techniques
for measuring progress.
One recent attempt at an explicit deﬁnition of intelligence is the Universal
Intelligence Measure [13]. This is a mathematical, non-anthropocentric deﬁni-
tion of intelligence that draws on a range of proposed informal deﬁnitions of
intelligence, algorithmic information theory [14], Solomonoﬀ’s model of univer-
sal inductive inference [17,18], and Hutter’s AIXI theory of universal artiﬁcial
intelligence [8,9]. This paper conducts a preliminary investigation into the po-
tential for this particular measure of intelligence to serve as a practical metric
for evaluating real-world agent implementations.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 236–249, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

An Approximation of the Universal Intelligence Measure
237
2
Background
We now brieﬂy describe the recently introduced notion of a Universal Intelligence
Test, the Universal Intelligence Measure and the practical issues that arise when
attempting to evaluate the performance of broadly intelligent agents.
2.1
Universal Intelligence Tests
[4] introduce the notion of a Universal Intelligence Test, a test designed to be
able to quantitatively assess the performance of artiﬁcial, robotic, terrestrial or
even extra-terrestrial life, without introducing an anthropocentric bias. Related
discussion on the motivation behind such tests is given by [1,3,16]. With re-
spect to our goal of wanting to build more powerful artiﬁcial agents, we strongly
support the introduction of such general purpose tests. Having a suite of such
tests, with each emphasizing diﬀerent, measurable aspects of intelligence, would
clearly help the community build more powerful and robust general agents. This
paper introduces our own such test, which works by approximating the Universal
Intelligence Measure.
2.2
Universal Intelligence Measure
After surveying some 70 informal deﬁnitions of intelligence proposed by various
psychologists and artiﬁcial intelligence researchers, Legg and Hutter [13] argue
that the informal deﬁnition:
“intelligence measures an agent’s ability to achieve goals in a wide
range of environments”,
broadly captures many important properties associated with intelligence. To for-
malise this intuition, they used reinforcement learning [19], a general framework
for goal achieving agents in unknown environments. In this setting, cycles of in-
teraction occur between the agent and the environment. At each cycle, the agent
sends an action to the environment, that then responds with an observation and
(scalar) reward. The agent’s goal is to choose its actions, based on its previous
observations and rewards, so as to maximise the rewards it receives over time.
With a little imagination, it is not hard to see that practically any problem
can be expressed in this framework, from playing a game of chess to writing an
award-winning novel.
In their setup, both the agent and environment are expressed as conditional
probability measures over interaction sequences. To formalise a ‘wide range of
environments’, the set of all Turing computable environments is used, with the
technical constraint that the sum of returned rewards is ﬁnitely bounded. Finally,
the agent’s performance over diﬀerent environments is then aggregated into a
single result. To encourage agents to apply Occam’s Razor, as advocated by
[13], each environment is weighted according to its complexity, with simpler
environments being weighted more heavily. This is elegantly achieved by using

238
S. Legg and J. Veness
the algorithmic prior distribution [14]. The universal intelligence of an agent π
can then be deﬁned as,
Υ(π) :=

μ∈E
2−K(μ)V π
μ
(1)
where μ is an environment from the set E of all computable reward bounded
environments, K(·) is the Kolmogorov complexity, and V π
μ := E(∞
i=1 Ri) is the
expected sum of future rewards when agent π interacts with environment μ.
This theoretical measure of intelligence has a range of desirable properties.
For example, the most intelligent agent under this measure is Hutter’s AIXI,
a universal agent that converges to optimal performance in any environment
where this is possible for a general agent [9]. At the other end of the scale,
it can be shown that the Universal Intelligence Measure sensibly orders the
performance of simple adaptive agents. Thus, the measure spans an extremely
wide range of capabilities, from the simplest reactive agents up to universally
optimal agents. Unlike the pass or fail Turing test, universal intelligence is a
continuous measure of performance and so it is more informative of incremental
progress. Furthermore, the measure is non-anthropocentric as it is based on the
fundamentals of mathematics and computation rather than human imitation.
The major downside is that the Universal Intelligence Measure is only a the-
oretical deﬁnition, and is not suitable for evaluating real-world agents directly.
3
Algorithmic Intelligence Quotient
The aim of the Universal Intelligence Measure was to deﬁne intelligence in the
most general, precise and succinct way possible. While these goals were achieved,
this came at the price of asymptotic computability. In this section we will show
how a practical measure of machine intelligence can be deﬁned via approximating
this notion. While we will endeavor to retain the spirit of the Universal Intelli-
gence Measure, the emphasis of this section will be on practicality rather than
theoretical purity. We will call our metric the Algorithmic Intelligence Quotient
or AIQ1 for short.
3.1
Environment Sampling
One way to deﬁne an Occam’s Razor prior is to use the Universal Distribution
[17]. The universal prior probability, with respect to a reference machine U, of a
sequence beginning with a ﬁnite string of bits x is deﬁned as
MU(x) :=

p:U(p)=x∗
2−ℓ(p),
1 IQ was originally a quotient, but is now normalised to a Gaussian. AIQ is also not
a quotient, however we use the name since “IQ” is well understood to be a measure
of intelligence.

An Approximation of the Universal Intelligence Measure
239
where U(p) = x∗means that the universal Turing machine U computes an output
sequence that begins with x when it runs program p, and ℓ(p) is the length of
p in bits. As the Kolmogorov complexity of x∗is the length of the shortest
program for x∗, by deﬁnition, it follows that the largest term in M is given by
2−K(x∗). Thus, the set of all sequences that begin with a low complexity string
will have a high prior probability under M, in accordance with Occam’s Razor.
The diﬀerence now is that the lengths of all programs that generate strings
beginning with x are used to deﬁne the prior, not just the shortest program.
The advantage of switching to this related distribution is that it is much easier
to sample from. As the probability of sampling a program p by uniformly sam-
pling consecutive bits is 2−ℓ(p), to sample a sequence from M we just randomly
sample a program p and run it on U. This method of sampling has been used to
create the test data sequences that make up the Generic Compression Benchmark
[15]. Here we will use this technique to sample environments for the Universal
Intelligence Measure. More precisely, having deﬁned a preﬁx-free universal Tur-
ing machine U, we generate a ﬁnite sample of N programs S := p1, p2, . . . , pN
by uniformly generating bits until we reach the end of each program. This is not
a set as the same program can be sampled many times. The estimate of agent
π’s universal intelligence is then,
ˆΥ(π) := 1
N
N

i=1
ˆV π
pi,
where we have replaced the expectation V π
μ with ˆV π
pi which is deﬁned to be
the empirical total reward returned from a single trial of environment U(pi)
interacting with agent π. Since we are sampling the space of programs that
deﬁne environments, rather than the space of environments directly, multiple
programs can deﬁne the same environment. Notice that the weighting by 2−ℓ(pi)
is no longer needed as the probability of a program being sampled decreases by
1
2 for every additional bit. The natural idea of performing a Monte Carlo sample
over environments is also used by [4] and [16] in their related work.
3.2
Environment Simulation
We need to be able to run each sampled program on our reference machine U.
A technical problem we face is that some programs will not halt, and due to the
infamous halting problem, we know there is no process that can always determine
when this is the case. The extent of this problem can be reduced by choosing
a reference machine where non-halting programs are relatively unlikely, or one
which aids the detection of many non-halting programs. Even so, we would still
have non-halting problems to deal with.
From a practical perspective there is not much diﬀerence between a program
that does not halt and one that simply runs for too long: in both cases the
program needs to be discarded. To determine if this is the case, we ﬁrst run the
program on the reference machine. If the program exceeds our computation limit

240
S. Legg and J. Veness
in any cycle, the program is discarded. In the future, more powerful hardware
will allow us to increase this limit to obtain more accurate AIQ estimates.
3.3
Temporal Preference
In the Universal Intelligence Measure, the total reward that an environment
can return is upper bounded by one. Because all computable environments that
respect this constraint are considered, in eﬀect the Universal Intelligence Measure
considers all computable distributions of rewards. Theoretically this is elegant,
but practically we have no way of knowing if a program will respect the bound.
A more practical alternative is geometric discounting [19] where we allow the
environment to generate any reward in any cycle so long as the reward belongs
to a ﬁxed bounded interval. Rewards are then scaled by a factor that decreases
geometrically with each interaction cycle. Under such a scheme the reward sum
is bounded and thus we can bound the remaining reward left in a trial. For
example, we can terminate each trial once the possible remaining reward drops
below a certain value.
While this is elegant, it is not very computationally eﬃcient when we are
interested in learning over longer time frames. This is since the later cycles, where
the agent has most likely learnt the most, are the most heavily discounted. Thus,
we will focus here on undiscounted, bounded rewards over ﬁxed length trials.
3.4
Reference Machine Selection
When looking at converting the Universal Intelligence Measure into a concrete
test of intelligence, a major issue is the choice of a suitable reference machine.
Unfortunately, there is no such thing as a canonical universal Turing machine,
and the choice that we make can have a signiﬁcant impact on the test results.
Very powerful agents such as AIXI will achieve high universal intelligence no
matter what reference machine we choose, assuming we allow agents to train
from samples prior to taking the test, as suggested in [13]. For more limited
agents however, the choice of reference machine is important. Indeed, in the
worst case it can cause serious problems [7]. When used with typical modern
reinforcement learning algorithms and a fairly natural reference machine, we ex-
pect the performance of the test to lie between these two extremes. That is, we
expect that the reference machine will be important, but perhaps not so impor-
tant that we will be unable to construct a useful test of machine intelligence.
Providing some empirical insight into this is one of the main aims of this paper.
Before choosing a reference machine, it is worth considering, in broad terms,
the eﬀect that diﬀerent reference machines will have on the intelligence measure.
For example, if the reference machine is like the Lisp programming language, en-
vironments that can be compactly described using lists will be more probable.
This would more heavily weight these environments in the measure, and thus
if we were trying to increase the universal intelligence of an agent with respect
to this particular reference machine, we would progress most rapidly if we fo-
cused our eﬀort on our agent’s ability to deal with this class of environments.

An Approximation of the Universal Intelligence Measure
241
Table 1. Standard BF program symbols along with their C equivalents
BF
C
>
move pointer right
p++;
<
move pointer left
p--;
+
increment cell
*p++;
-
decrement cell
*p--;
.
write output
putchar(*p);
,
read input
*p = getchar();
[ if cell is non-zero, start loop
while(*p) {
]
return to start of loop
}
On the other hand, with a more Prolog like reference machine, environments
with a logical rule structure would be more important. More generally, with a
simple reference machine, learning to deal with small mathematical, abstract
and logical problems would be emphasised as these environments would be the
ones computed by small programs. These tests would be more like the sequence
prediction and logical puzzle problems that appear in some IQ tests.
What about very complex reference machines? This would permit all kinds
of strange machines, potentially causing the most likely environments to have
bizarre structures. As we would like our agents to be eﬀective in dealing with
problems in the real world, if we do use a complex reference machine, it seems the
best choice would be to use a machine that closely resembles the structure of the
real world. Thus, the Universal Intelligence Measure would become a simulated
version of reality, where the probability of encountering any given challenge
would reﬂect its real world likelihood. Between these extremes, a moderately
complex reference machine might include three dimensional space and elemen-
tary physics. While complex reference machines allow the intelligence measure
to be better calibrated to the real world, they are far more diﬃcult to develop.
Thus, at least for our ﬁrst set of tests, we focus on using a very simple reference
machine.
3.5
BF Reference Machine
One important property of a reference machine is that it should be easy to sample
from. The easiest languages are ones where all programs are syntactically valid
and there is a unique end of program symbol. One language with this feature is
Urban M¨uller’s BF language. It has just 8 symbols, listed in Table 1 along with
their C equivalents, where we have used C stdin and stdout at the input and
output tapes, and p is a pointer to the work tape.
To convert BF for use as a reference machine the agent’s action information
is placed on input tape cells, then the program is run, and the reward and obser-
vation information is collected from the output tape. Reward is the ﬁrst symbol
on the output tape and is normalised to the range -100 to +100. The following
symbol is the observation. All symbols on the input, output and work tapes

242
S. Legg and J. Veness
are integers, with a modulo applied to deal with under/over ﬂow conditions. As
discussed in Section 3.2, we set a time limit for the environment’s computation
in each interaction cycle, here 1000 computation steps. To encourage programs
to terminate, we interpret any attempt to write excess reward and observation
cells as a signal to halt computation for that interaction cycle. As a result about
90% of programs do not exceed the computation limit and halt with output for
each cycle.
As we do not wish our environments to always be deterministic, we have
added to BF the instruction % which writes a random symbol to the current
work tape cell. Furthermore, we also place a history of previous agent actions on
the input tape. This solves the problem of what to do when a program reads too
many input symbols, and it also makes it easier for the environment to compute
functions of the agent’s past actions. Finally, after randomly sampling a program
we remove any pointless code, such as “+-”, “><” and “[]”. This produces faster
and more compact programs, and discards the most common type of pointless
inﬁnite loop. We also discard programs that do not contain any instructions to
either read from the input or write to the output.
Finally, the ﬁrst bit of the program indicates whether the reward values are
negated or not. By randomly setting this bit, randomly acting agents have an
AIQ of zero, a natural baseline suggested by [4].
3.6
Variance Reduction Techniques for AIQ Estimation
Obtaining an accurate estimate of an agent’s AIQ using simple Monte-Carlo
sampling can be time consuming. This is due to the relatively slow rate at which
the standard error decays as the number of samples increases, along with the
fact that for many agents, simulating even a single episode is quite demanding.
To help our implementation provide statistically signiﬁcant results within rea-
sonable time constraints, we applied a number of techniques that signiﬁcantly
reduced the variance of our AIQ estimates.
The ﬁrst technique was to simply exploit the parallel nature of Monte Carlo
sampling so that the test could be run on multiple cores. On present day hard-
ware, this can easily lead to a 10x performance improvement over a single core
implementation.
The second technique was to use stratiﬁed sampling. It works as follows: ﬁrst,
the sample space Ω is partitioned into k mutually exclusive sets Ω1, Ω2, . . . , Ωk
such that 0k
i=1 Ωi = Ω. Each Ωi is called a stratum. The total probability mass
Pr[X ∈Ωi] associated with each of the k strata needs to be known in advance.
Given a sample (X1, X2, . . . , Xn), the stratiﬁed estimate ˆXss is given by,
ˆXss :=
k

i=1
Pr[X ∈Ωi]
⎛
⎝1
ni
n

j=1
XjI[Xj ∈Ωi]
⎞
⎠
where nk := n
i=1 I[Xi ∈Ωk]. It can be interpreted as a convex combination
of k simple Monte Carlo estimates, and is easily shown to be unbiased. For a

An Approximation of the Universal Intelligence Measure
243
ﬁxed sample size, the optimal way to allocate samples is in proportion to the
standard deviation of each stratum, weighted by the stratum’s probability mass.
More precisely, if fX(x) is the density function of X and fk(x)∝I[x ∈Ωk]fX(x)
is the density function associated with the random variable Yk associated with
stratum k, the optimal allocation ratio is achieved when nk ∝
1
Var[Yk] Pr[X ∈
Ωk]. To do this we must estimate Var[Yk] during sampling and adapt which
strata we are drawing samples from accordingly. Intuitively, the algorithm is
identifying those parts of the sample space which have the most variance and are
of the most signiﬁcance to the ﬁnal result, and concentrating the sampling eﬀort
in these regions. There are various algorithms for adaptive stratiﬁed sampling,
however we have chosen the method developed by [2] as they have derived the
conﬁdence intervals for the estimate of the mean, a feature we will use when
reporting our results. In AIQ, we stratiﬁed on a combination of simple properties
of each environment program, including the length and the presence of particular
patterns of BF symbols. This particular technique gave roughly a 4x performance
increase.
Another variance reduction technique we used was common random num-
bers. Rather than estimating the AIQ of two agents π and π′ from independent
samples from the environment distribution, we instead estimate the diﬀerence,
ˆ
Δ(π, π′) := ˆΥ(π′) −ˆΥ(π)
using a single set of program samples. This technique is particularly important
when an agent designer is deciding whether or not to accept a new version of
the agent. Intuitively, common random numbers reduces the chance of one agent
performing better due to being evaluated on an easier sample. More precisely,
Var[ ˆ
Δ(π, π′)] = Var[ ˆΥ(π′)] + Var[ ˆΥ(π)] −2Cov[ ˆΥ(π′), ˆΥ(π)].
If independent samples were used for ˆΥ(π′) and ˆΥ(π) the covariance would van-
ish. However, since we are using a single sample and have assumed that the AIQs
of π and π′ are positively correlated (which makes sense if π′ is an incremen-
tal improvement over π), Cov[ ˆΥ(π′), ˆΥ(π)] is positive and thus Var[ ˆ
Δ(π, π′)] is
reduced.
The ﬁnal variance reduction technique we used was antithetic variates. The
intuition is quite straightforward: instead of using one sample, use two samples
in such a way that the resultant estimators for the ﬁrst and second sample are
negatively correlated. These can then be combined to balance each other out,
thus reducing the total variance. More formally, if ˆY1and ˆY2 are two unbiased
estimates of a quantity of interest, then ˆX =
1
2[ ˆY1 + ˆY2] is also an unbiased
estimator, with
Var( ˆ
X) = 1
4

Var( ˆY1) + Var( ˆY2) + 2Cov( ˆY1, ˆY2)

.
Thus if the two estimates are negatively correlated, Var( ˆ
X) is reduced. A com-
mon way to achieve this is to sample in pairs, with each element of the pair

244
S. Legg and J. Veness
1k
3k
10k
30k
100k
Episode Length
35
40
45
50
55
60
AIQ score
MC-AIXI
HLQ
Q(l)
Q(0)
Freq
Fig. 1. Estimated AIQ scores of agents as a function of episode length
directly opposing the other in some sense. In our AIQ implementation, since the
ﬁrst bit of each program speciﬁes whether or not to negate the rewards, applying
antithetic variates was trivial: we simply ran each program twice, once with the
ﬁrst bit oﬀ, once with the ﬁrst bit on. This lead to a performance improvement
that varied based on the agent being tested. With the exception of the Ran-
dom agent (where there was a massive negative correlation), the performance
improvements were typically smaller than a factor of 1.5x.
4
Empirical Results
We implemented AIQ with the variance reduction techniques previously de-
scribed, along with the extended BF reference machine. Our code is open source
and available for download at www.vetta.org/aiq. It should run on any plat-
form containing Python and the Scipy library. We have also implemented a
number of reinforcement learning agents to test AIQ with. The simplest agent is
called Random, which makes uniformly random actions. A slightly more complex
agent is Freq, that computes the average reward associated with each action, ig-
noring observation information. It chooses the best action in each cycle except
for a ﬁxed fraction of the time when it tries a random action. We have imple-
mented the Q(λ) algorithm [23], which subsumes the simpler Q(0) algorithm as
a special case, and also HLQ(λ) which is similar except that it automatically
adapts its learning rate [10]. Finally, we have created a wrapper for MC-AIXI
[21,22], a more advanced reinforcement learning agent that can be viewed as an
approximation to Hutter’s AIXI.

An Approximation of the Universal Intelligence Measure
245
2
4
8
16
32
Context Depth
1
2
3
4
6
AIQ Score
5
10
25
50
100
Monte Carlo Simulations
20
0
0
50
0
AIQ Score
Fig. 2. Estimated AIQ of MC-AIXI as the context depth and search eﬀort is varied
4.1
Comparison of Artiﬁcial Agents
For our ﬁrst set of tests we took the BF reference machine and set the number
of symbols on the tape to 5. We then tested all our agents without discounting
on a range of diﬀerent episode lengths. With the exception of MC-AIXI, which
is signiﬁcantly more computationally expensive, we performed 10,000 samples in
each test. As expected, the AIQ of the Random agent was zero. For the other
agents we ran parameter sweeps to ﬁnd the best performing settings. These
results appear in Figure 1, with the error bars representing approximate 95%
conﬁdence intervals.
For 100k length episodes the agents’ AIQ scores appear in the order that we
would expect: Random (not shown), Freq, Q(0), Q(λ), HLQ(λ) and MC-AIXI.
As the episode lengths decrease, the agent’s have less learning time in each trial
and thus their scores decline. Except for MC-AIXI, the relative ranking of the
agents remained the same. It seems MC-AIXI’s complex world model is relatively
slow to learn but ultimately the most powerful. Our initial attempts at modifying
MC-AIXI to be similarly high scoring on shorter runs failed. Longer tests may
be needed in order to determine whether some of the more complicated agents
have reached their maximal AIQ.
Similar tests to the above were performed with 2, 10 and 20 symbol tapes.
The results were qualitatively the same, but with larger action and observation
spaces the learning times increased for all agents. We also increased the number
of cells used to represent the observations, usually set to 1, which had the same
eﬀect. We then tried reversing the order of the observation and the reward on the
tape, which lead to results that were qualitatively the same. We experimented
with discounting, and the results were consistent with the undiscounted results
using shorter episodes lengths. We also increased the computation limit per cycle
and did not see any measurable eﬀect. Thus our initial ﬁndings were that the
results seemed relatively robust to minor modiﬁcations of the reference machine.

246
S. Legg and J. Veness
Program length in BF instructions
Cumulative sample proportion
Fig. 3. A comparison of the BF program lengths in the environment distribution com-
pared to the environments chosen by the adaptive sampler. The dashed blue line shows
the cumulative proportion of BF environments satisfying a given maximum program
length. The solid green line shows the cumulative proportion of BF environments sam-
pled by our variance reduction enhanced adaptive sampling procedure.
4.2
Measuring Agent Scalability
The MC-AIXI agent has a parameter that sets the context depth of its prediction
algorithm, in eﬀect controlling the maximal size of the world model that it
can learn. It also has a parameter that speciﬁes the number of Monte Carlo
simulations it generates, in eﬀect controlling the amount of eﬀort that it puts
into planning for each interaction cycle. These two parameters allow us to vary
the power of the MC-AIXI agent along two fundamentally diﬀerent dimensions.
We did this with a 5 symbol BF reference machine, as before, and with 50k
length episodes. The results of these tests appear in Figure 2. While increasing
the agent’s search eﬀort consistently increased its AIQ score, the results for the
context depth appear to have plateaued at a depth of 8, though with the present
error bars it is impossible to tell for sure. This warrants future investigation. For
example, it may be the case that larger context depths help only if the episode
length is longer than 50,000.
4.3
Environment Distribution
We next ran some tests to help characterise our environment sampling procedure.
Our ﬁrst test involved generating 2 × 105 legal BF environment programs
satisfying the criteria listed in Section 3.5. For example, programs that ran too
long or didn’t have both a read and a write instruction were discarded. The
dashed blue line in Figure 3 shows the resultant empirical cumulative distribution
of program lengths across the space of BF environments. Although the number
of programs at any given length decays exponentially, this result shows that a

An Approximation of the Universal Intelligence Measure
247
signiﬁcant amount of the total probability mass is still allocated to relatively
complex environments with description lengths of 20 symbols or more.
Our next test involved inspecting the distribution of programs sampled by our
adaptive sampler when evaluating the HLQ agent. This is shown by the green
line on Figure 3. This shows that the adaptive sampler reduces the proportion of
programs of length 10 or less from almost 40% to 20%. On the other hand, from
length 20 to 40 the green line climbs more quickly then the blue one. Thus we see
that the adaptive sampler has moved the sampling eﬀort away from programs
shorter than 20 symbols, and focused its eﬀort on the 20 to 40 symbol range.
We also visually inspected a variety of generated environments. While it is
true that extremely short programs, for example those of less than 5 symbols,
do not generate very interesting environments, we found that by the time we
got to programs of length 30, many environments (at least to our eyes) seemed
quite incomprehensible.
5
Related Work and Discussion
[5] developed a related test, called the C-test, that is also based on a very simple
reference machine. Like BF it uses a symbolic alphabet with an end wrap around.
Unlike BF, which is a tape based machine, the C-test uses a register machine with
just three symbol registers. This means that the state space for programs is much
smaller than in BF. Another key diﬀerence is that the C-test considers generated
sequences of symbols, rather than fully interactive environments. In our view,
this makes it not a complete test of intelligence. For example, the important
problem of exploration does not feature in a non-interactive setting. Extending
the C-test reference machine to be interactive would likely be straightforward:
simply add instructions to read and write to input and output tapes, the same
way BF does. It would be interesting to see how AIQ behaves when using such
a reference machine.
A diﬀerent approach is used in [12] and [11]. Here an interactive reinforcement
learning setting is considered, however the space of environments is no longer
sampled from a Turing complete reference machine. Instead a small MDP is used
(3, 6 and 9 states) with uniformly random transitions. Which state is punishing
or rewarding follows a ﬁxed random path through this state space. To measure
the complexity of environments, the gzip compression algorithm is applied to a
description of the environment. While this makes the test tractable, in our view
it does so in a way that deviates signiﬁcantly from the Universal Intelligence
Measure that we are attempting to approximate with AIQ. Interestingly, in
their setting human performance was not better than the simple tabular Q-
learning algorithm. We suspect that this is because their environments have a
simple random pattern structure, something that algorithms are well suited for
compared to humans.
Another important diﬀerence in our work is that we have directly sampled
from program space. This is analogous to the conventional construction of the
Solomonoﬀprior, which samples random bit sequences and treats them as pro-
grams. With this approach all programs that compute some environment count

248
S. Legg and J. Veness
towards the environment’s eﬀective complexity, not just the shortest, though the
shortest clearly has the largest impact. This makes AIQ very eﬃcient in prac-
tice since we can just run sampled programs directly, avoiding the need to have
to compute complexity values through techniques such as brute force program
search. For example, to compute the complexity of a 15 symbol program, the
C-test required the execution of over 2 trillion programs. For longer programs,
such as many that we have used in our experiments, this would be completely
intractable. One disadvantage of our approach, however, is that we never know
the complexity of any given environment; instead we know just the length of one
particular program that computes it.
6
Conclusion
We have taken the theoretical model of Universal Intelligence set out in [13] and
converted it into a practical test for machine intelligence. To do this we have
randomly sampled programs from a simple universal Turing machine, drawing
inspiration at points from [4], and the related work in [6]. In all of our tests the
AIQ scores behaved sensibly, with agents expected to be more intelligent having
higher AIQ. Naturally, no empirical test can conﬁrm that a test of intelligence is
indeed “correct”, rather it can only conﬁrm that the theoretical model behaves
as expected when suitably approximated, and that no insurmountable diﬃcul-
ties arise when attempting this. We believe that our present eﬀorts have been
successful in this regard, but more work is clearly required.
Perhaps the most worrying potential problem with the Universal Intelligence
Measure is its dependence on the choice of reference machine, as highlighted by
[7]. While we accept that problematic reference machines exist, it was our belief
that if we chose a fairly simple and natural reference machine, the resulting intel-
ligence test would behave sensibly. While we have only provided one data point to
support this claim here, the fact that it was the ﬁrst and only reference machine
that we tried gives us hope that it is not overly special. Furthermore, we found
that the results were qualitatively the same for a range of minor modiﬁcations to
the BF reference machine. Obviously, further reference machines will need to be
implemented and tested to gain a greater understanding of these issues.
Acknowledgements. This research was supported by Swiss National Science
Foundation grant number PBTIP2-133701.
References
1. Dowe, D.L., Hajek, A.R.: A non-behavioural, computational extension to the Tur-
ing Test. In: Intl. Conf. on Computational Intelligence & Multimedia Applications
(ICCIMA 1998), Gippsland, Australia, pp. 101–106 (February 1998)
2. ´Etor´e, P., Jourdain, B.: Adaptive optimal allocation in stratiﬁed sampling methods.
Methodology and Computing in Applied Probability 12(3), 335–360 (2010)
3. Hern´andez-Orallo, J.: Beyond the Turing Test. J. Logic, Language & Informa-
tion 9(4), 447–466 (2000)

An Approximation of the Universal Intelligence Measure
249
4. Hern´andez-Orallo, J., Dowe, D.L.: Measuring universal intelligence: Towards an
anytime intelligence test. Artiﬁcial Intelligence 174(18), 1508–1539 (2010)
5. Hern´andez-Orallo, J., Minaya-Collado, N.: A formal deﬁnition of intelligence based
on an intensional variant of Kolmogorov complexity. In: Proc. Intl. Symposium of
Engineering of Intelligent Systems (EIS 1998), pp. 146–163. ICSC Press (1998)
6. Hern´andez-Orallo, J.: A (hopefully) Non-biased Universal Environment Class for
Measuring Intelligence of Biological and Artiﬁcial Systems. In: Baum, E., Hut-
ter, M., Kitzelmann, E. (eds.) 3rd Intl. Conf. on Artiﬁcial General Intelligence,
pp. 182–183. Atlantis Press (2010)
7. Hibbard, B.: Bias and no free lunch in formal measures of intelligence. Journal of
Artiﬁcial General Intelligence 1(1), 54–61 (2009)
8. Hutter, M.: Towards a universal theory of artiﬁcial intelligence based on algorithmic
probability and sequential decisions. In: Flach, P.A., De Raedt, L. (eds.) ECML
2001. LNCS (LNAI), vol. 2167, pp. 226–238. Springer, Heidelberg (2001)
9. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability, 300 pages. Springer, Berlin (2005),
http://www.hutter1.net/ai/uaibook.htm
10. Hutter, M., Legg, S.: Temporal diﬀerence updating without a learning rate. In:
Advances in Neural Information Processing Systems, vol. 20, pp. 705–712. MIT
Press, Cambridge (2008)
11. Insa-Cabrera, J., Dowe, D.L., Espa˜na-Cubillo, S., Hern´andez-Lloreda, M.V.,
Hern´andez-Orallo, J.: Comparing Humans and AI Agents. In: Schmidhuber, J.,
Th´orisson, K.R., Looks, M. (eds.) AGI 2011. LNCS (LNAI), vol. 6830, pp. 122–
132. Springer, Heidelberg (2011)
12. Insa-Cabrera, J., Dowe, D.L., Hern´andez-Orallo, J.: Evaluating a reinforcement
learning algorithm with a general intelligence test. In: Lozano, J.A., G´amez, J.A.,
Moreno, J.A. (eds.) CAEPIA 2011. LNCS, vol. 7023, pp. 1–11. Springer, Heidelberg
(2011)
13. Legg, S., Hutter, M.: Universal intelligence: A deﬁnition of machine intelligence.
Minds and Machines 17(4), 391–444 (2007)
14. Li, M., Vit´anyi, P.M.B.: An introduction to Kolmogorov complexity and its appli-
cations, 3rd edn. Springer (2008)
15. Mahoney, M.: Generic compression benchmark (2008),
http://www.mattmahoney.net/dc/uiq
16. Schaul, T., Togelius, J., Schmidhuber, J.: Measuring Intelligence through Games.
ArXiv e-prints (September 6, 2011), http://arxiv.org/abs/1109.1314v1
17. Solomonoﬀ, R.J.: A formal theory of inductive inference: Part 1 and 2. Inform.
Control 7(1-22), 224–254 (1964)
18. Solomonoﬀ, R.J.: Complexity-based induction systems: comparisons and conver-
gence theorems. IEEE Trans. Information Theory IT-24, 422–432 (1978)
19. Sutton, R., Barto, A.: Reinforcement learning: An introduction. MIT Press, Cam-
bridge (1998)
20. Turing, A.M.: Computing Machinery and Intelligence. Mind 59, 433–460 (1950)
21. Veness, J., Ng, K.S., Hutter, M., Silver, D.: Reinforcement learning via AIXI
approximation. In: Proc. 24th AAAI Conference
on Artiﬁcial Intelligence,
pp. 605–611. AAAI Press, Atlanta (2010)
22. Veness, J., Ng, K.S., Hutter, M., Uther, W., Silver, D.: A Monte-Carlo AIXI
Approximation. Journal of Artiﬁcial Intelligence Research (JAIR) 40(1), 95–142
(2011)
23. Watkins, C.J.C.H.: Learning from Delayed Rewards. PhD thesis, King’s College,
Oxford (1989)

Minimum Message Length Analysis
of the Behrens–Fisher Problem
Enes Makalic and Daniel F. Schmidt
The University of Melbourne
Centre for MEGA Epidemiology
Carlton VIC 3053, Australia
{emakalic,dschmidt}@unimelb.edu.au
Abstract. Given two sequences of Gaussian data, the Behrens–Fisher
problem is to infer whether there exists a diﬀerence between the two cor-
responding population means if the population variances are unknown.
This paper examines the Behrens–Fisher-type problem within the min-
imum message length framework of inductive inference. Using a spe-
cial bounding on a uniform prior over the population means, a simple
Bayesian hypothesis test is derived that does not require computationally
expensive numerical integration of the posterior distribution. The min-
imum message length procedure is then compared against well-known
methods on the Behrens–Fisher hypothesis testing problem and the esti-
mation of the common mean problem showing excellent performance in
both cases. Extensions to the generalised Behrens–Fisher problem and
the multivariate Behrens–Fisher problem are also discussed.
1
Introduction
Consider two mutually independent sequences of i.i.d. data denoted by y1 =
(y11, . . . , y1n1)′ and y2 = (y21, . . . , y1n2)′ and generated by the following Gaus-
sian model:
yij ∼N(μi, τi),
(1)
where (i = 1, 2; j = 1, . . . , ni) and μ = (μ1, μ2)′ and τ = (τ1, τ2)′ are the un-
known sequence means and variances respectively. The Behrens–Fisher problem
is to infer whether there exists a diﬀerence between the two population means;
that is, whether μ1 = μ2. This paper examines the Behrens–Fisher problem us-
ing the minimum message length (MML) principle of inductive inference. The
minimum message length approach provides a Bayesian solution that does not
require computationally expensive numerical integration of the posterior prob-
ability density. The corresponding solution is easily extendable to testing for
equality of variances and the generalised Behrens–Fisher problem where the
data comprises more than two sequences (that is, i > 2).
When the population variances τ are assumed to be known, or their ratio
ρ = τ1/τ2 is speciﬁed, a common frequentist solution to the Behrens–Fisher
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 250–260, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

MML Analysis of a Behrens–Fisher Problem
251
problem is a hypothesis test based on a Student t pivot. There does not exist
a non-randomised frequentist procedure independent of the data for obtaining
exact conﬁdence intervals if the population variances are unknown [1]. A common
practical solution in this case is to use the Student t pivot with Satterthwaite’s
approximation for the number of degrees of freedom [2]. An alternative solution
is to use a ﬁducial probability distribution [3,4] or a fully Bayesian approach.
An excellent review of the ﬁducial and Bayesian solutions to the Behrens–Fisher
problem is given in [5].
2
Minimum Message Length (MML)
The minimum message length (MML) principle [6,7,8] oﬀers a Bayesian frame-
work for inference that is rooted in information theory and is a practical imple-
mentation of the theory of inductive inference proposed initially by Solomonoﬀ[9],
Kolmogorov [10] and Chaitin [11]. The underlying idea is to view the problem
of estimation and model selection as one of data compression. Such an approach
naturally leads to criteria that balance a trade-oﬀbetween the model ﬁt and
the model complexity. The model ﬁt is measured by the amount of information,
I(y|θ), required to encode the data using a given model; I(y|θ) commonly in-
cludes the negative log-likelihood function. The model complexity denotes the
amount of information, I(θ), needed to encode the selected model relative to
some chosen prior beliefs. In this two part decomposition the statement of the
chosen model is generally named the assertion and the statement of the data
using this model is named the detail. The model that minimises the sum
I(y, θ) = I(θ) + I(y|θ)
of the assertion and the detail is accepted as the most a posteriori likely expla-
nation of the data in light of the chosen prior beliefs. While the quantity I(y, θ)
can be exactly calculated using the strict MML prescription of [12], this is gener-
ally computationally intractable and approximations are used instead. The most
commonly used approximation is the MML87 formula of [8], which under suit-
able regularity conditions gives the joint codelength of model θ ∈Θ ⊂Rk and
data y as
I87(y, θ) = −log π(θ) + 1
2 log |J(θ)| + k
2 log κk
2
34
5
I87(θ)
+ k
2 −log p(y|θ)
2
34
5
I87(y|θ)
,
(2)
where k is the number of free parameters, p(y|θ) is the likelihood function, π(·)
denotes a prior distribution over the parameter space Θ, J(θ) is the Fisher
information matrix, and κk is the normalised second moment of an optimal
quantising lattice in k-dimensions. For many dimensions κk is not known, and
it is common to use the approximation ([6], p. 237)
c(k) = k
2 log κk + k
2 ≈−k
2 log(2π) + 1
2 log(kπ) + ψ(1),

252
E. Makalic and D.F. Schmidt
where ψ(·) is the digamma function. In this paper, all codelengths are measured
in nits (nats), or base-e digits, and as such “log” denotes the natural logarithm.
The Wallace–Freeman approximation states that the model ˆθ87(y) that min-
imises (2) is the most a posteriori likely explanation of the data in the light
of the chosen priors. Note that the model space Θ may be enlarged to include
models of many diﬀerent classes if the parameter vector is suitably partitioned
into continuous parameters and discrete, structural parameters and these are
handled accordingly. In this way, MML treats both parameter estimation and
model class selection on the same footing. The Wallace–Freeman approximation
provides codelengths (and therefore estimates) that are invariant under smooth,
one-to-one reparameterisations of the parameters and has shown to be consistent
in diﬃcult inference problems; for example, the Neyman–Scott problem [13].
3
MML and the Behrens–Fisher Problem
Consider the Behrens–Fisher problem from (1) and let y = (y′
1, y′
2)′ denote
the vector comprising two sequences of data with n = (n1 + n2) total data
points. The solution to the Behrens–Fisher problem within the message length
framework requires: (1) the codelength of the data under the assumption that
the population means are equal (that is, μ1 = μ2), and (2) the codelength of the
data assuming there exist two population means (that is, μ1 ̸= μ2). The model
resulting in the shortest codelength is then deemed to be a posteriori most likely
to have generated the data.
3.1
Shared Population Mean
The model with a single population mean for two data sequences is examined
ﬁrst. The model parameters θ = (μ, τ ′)′ ∈R3 and τ = (τ1, τ2)′ are consid-
ered unknown and must be inferred from the data. Application of the Wallace–
Freeman codelength (2) requires a likelihood function, a corresponding Fisher
information and prior densities for all parameters. The negative log-likelihood
function is
−log p(y|θ) = n
2 log 2π + 1
2
2

i=1
⎛
⎝ni log τi + 1
τi
ni

j=1
(yij −μ)2
⎞
⎠.
(3)
The determinant of the Fisher information matrix, J(θ), is
|J(θ)| =
6 2

i=1
ni
2τ 2
i
7 ,n1
τ1
+ n2
τ2
-
.
(4)
It remains to specify the prior densities over the parameters θ. The popula-
tion variances are considered independent of the mean and given conjugate,
scale invariant, prior densities over some compact set Ξ; for example, let Ξ =

MML Analysis of a Behrens–Fisher Problem
253
(a, b) × (a, b) where 0 < a < b < ∞. This prior is reasonable as it indicates no
preference for any particular measurement scale of the data. Since the two vari-
ance parameters are common to both models under consideration, the choice of a
prior density and the support Ξ (that is, the choice of a and b) for the variances
has no eﬀect on the model selection procedure. Following the procedure in [14],
the population mean is given a uniform prior over a special compact support in
order to avoid the Jeﬀreys–Lindley paradox. The chosen support for this prior
density can obtained by the following argument. First note that the observed
data y is generated from the model
y = y∗+ ε,
where ε ∼Nn(0, Σn) and y∗is the noise-free data (the “signal”). It is clear that
E (y′y) = y′
∗y∗+ tr (Σn) .
(5)
where E(·) denotes the expectation operator. For any value of the population
mean μ, one can construct an estimate of y∗, say (1nˆμ) , and since tr (Σn) is
unknown and strictly positive, by (5), this estimate should satisfy
y′y ≥(1nˆμ)′ (1nˆμ) = nˆμ2,
(6)
where 1n is a (n×1) vector of ones. From (6), the feasible parameter set Λ1 ⊂R
is
Λ1 =
8
μ : nμ2 ≤y′y
9
.
A suitable prior for the population mean is then a uniform density deﬁned over
the support Λ1. Within the context of MML, this prior is perfectly acceptable as
the data component of the prior (that is, y′y/n) can be encoded, with codelength
O(log n), prior to encoding the parameters, and the data given the parameters
(see below). Further arguments for this choice of prior density are given in Ap-
pendix A. The complete prior density for all parameters is
π(θ) = πμ(μ)πτ (τ),
(7)
π(μ) =
1
vol(Λ1) =
,
n
4y′y
-1/2
,
μ ∈Λ1,
(8)
πτ (τ) = (Ωτ1τ2)−1 ,
τ1, τ2 ∈Ξ,
(9)
where Ω > 0 is a suitable normalisation constant. Substituting (3), (4) and (7)
into (2) yields the total codelength I87(y, μ, τ)
n
2 log 2π + 1
2
2

i=1
⎛
⎝ni log τi + 1
τi
ni

j=1
(yij −μ)2
⎞
⎠+ 1
2 log
,n1
τ1
+ n2
τ2
-
+1
2 log
6
Ω2(y′y)
n
2

i=1
ni
7
+ c(3),
(10)

254
E. Makalic and D.F. Schmidt
where c(3) = −2·32. Minimising (10) numerically yields the Wallace–Freeman
parameter estimates
(ˆμ, ˆτ) = arg min
μ,τ {I87(y, μ, τ)} .
(11)
The Wallace–Freeman estimate of μ is equal to the maximum likelihood estimate
only when the variances are known. The optimal Wallace–Freeman model under
the assumption the data shares a common mean has codelength I87(y, ˆμ, ˆτ).
In its current form, the total codelength (10) is not strictly valid as the prior
density for the population mean is a function of the observed data y′y. This is
easily rectiﬁed if one assumes existence of a suitable preamble code stating the
data constant (that is, y′y/n) prior to transmitting the data itself. The length of
this code can be shown to be approximately log(n)/2 nits. As the preamble code
is now common to both models under consideration (that is, both codelengths
are extended by log(n)/2 nits) it has no eﬀect on the choice of model made by
MML and is omitted from further discussion.
3.2
Diﬀerent Population Means
Consider now the model where the population mean diﬀers between the two data
sequences. The model parameters θ = (μ′, τ ′)′ ∈R4, where μ = (μ1, μ2)′, are
again considered unknown and must be inferred from the data. Following the
same argument as in Section 3.1 the feasible parameter set for the population
means is now the ellipsoid
Λ2 =

(μ1, μ2) :
2

i=1
niμ2
i ≤y′y

with volume vol(Λ2) = πy′y/√n1n2. The prior densities for the population
variances are taken to be equivalent to (9). The determinant of the Fisher infor-
mation matrix is
|J(θ)| =
2

i=1
, n2
i
2τ 3
i
-
.
Following the procedure in Section 3.1, the total Wallace–Freeman codelength
I87(y, ˆμ, ˆτ) is
n
2 log 2π + 1
2
6 2

i=1
(ni −1) log ˆτi
7
+ n −2
2
+ log (y′y√n1n2Ωπ/2) + c(4), (12)
where
ˆμi = 1
ni
ni

j=1
yij,
ˆτi =
1
ni −1
ni

j=1
(yij −ˆμi)2,
(i = 1, 2)
(13)
are the Wallace–Freeman parameter estimates ˆθ87(y), and c(4) = −3·14. In
this case, the Wallace–Freeman parameter estimates are the same as the regular
unbiased estimates.

MML Analysis of a Behrens–Fisher Problem
255
3.3
MML Hypothesis Testing
Let δ = (I87(y, ˆμ, ˆτ) −I87(y, ˆμ, ˆτ)) denote the diﬀerence in Wallace–Freeman
codelengths between the model with a shared population mean (10) and a model
with two population means (12). Within the minimum message length frame-
work, the optimal hypothesis is the one resulting in the briefest encoding. Thus,
if δ < 0, the hypothesis of a single population mean for the Behrens–Fisher
problem is deemed optimal, and vice versa. The term exp(−δ) can be directly
interpreted as the posterior odds in favour of the model with a common pop-
ulation mean. Large values of exp(−δ), perhaps ten or greater, indicate strong
preference to the simpler model with a single population mean.
4
Simulation and Discussion
The minimum message length procedure was compared against well-known meth-
ods on the Behrens–Fisher hypothesis testing problem (see Examples 1 and 2)
and the estimation of the common mean problem (see Example 3).
Example 1. Hypothesis testing. The minimum message length solution to the
Behrens–Fisher problem was compared against two alternative approaches: (1) a
popular frequentist method, and (2) a Bayesian approach [15]. For the frequentist
procedure, the null distribution of Δ = (μ2−μ1) was approximated by a Student
t density with Satterthwaite’s approximation for the relevant degrees of freedom.
In the Bayesian procedure, the prior density for the parameters was taken to be
the vague reference prior distribution π(θ) ∝(τ1τ2)−1 for μ and log τ and the
Cochran and Cox method was used to approximate the posterior density of
Δ (see equation (33) in [5]). Interestingly, a Bayesian procedure with these prior
distributions is numerically equivalent to Fisher’s ﬁducial inference procedure
for the Behrens–Fisher problem [5].
The testing setup was as follows: (1) randomly choose the true hypothesis
(μ1 = μ2) or (μ1 ̸= μ2) with equal probability, (2) sample all parameters from
their respective prior densities, and (3) sample data y from the resulting model
with (n1, n2) ∈{5, 10, 25, 50, 100, 500}2. The population variances were sampled
from the compact set τi ∈Ξ = [0·01, 20] for (i = 1, 2); the normalisation constant
is then Ω ≈57·77. The population mean(s) were uniformly sampled from (−5 ≤
μi ≤5). For each data set, the Wallace–Freeman, frequentist and Bayesian
procedures were asked to nominate which of the two possible hypotheses was
used to generate the data. To aid in comparison, the minimum message length
tests were completed ﬁrst and the resultant empirical type I error rate was
chosen as the signiﬁcance level for the frequentist and Bayesian procedures. This
is necessary as the MML principle has no in-built notion of type I and type II
error rates. By controlling the type I error rate, the performance of the three
methods can be compared solely on the number of type II errors. The number
of times each criterion selected the generating hypothesis was then recorded (see
Table 1). The entire procedure was repeated for 104 iterations. As an alternative,
the probability of choosing the hypothesis (μ1 = μ2) was set to the observed type

256
E. Makalic and D.F. Schmidt
I error rate of the MML procedure, and the experiments repeated as before. This
did not result in any signiﬁcant changes to results and their interpretation.
The MML criterion obtained superior scores when there was an imbalance in
the generated data; for example, (n1, n2) = (5, 500). When the sample size was
small, (n1, n2 < 25), the MML criterion obtained a higher proportion of correct
classiﬁcations compared to both the frequentist and the Bayesian approaches.
These diﬀerences in performance may potentially be attributed to the accura-
cies of the various approximations used in the three procedures as well as the
choice of prior density over the population means. As expected, all tested criteria
performed well for moderate and large samples sizes.
Table 1. Proportion of times each criterion correctly selected the data generating
hypothesis
Criterion
n1
n2
5
10
25
50
100
500
5
82·9
84·8
86·4
86·6
86·4
85·9
10
85·0
86·9
87·8
89·4
89·8
90·0
MML
25
85·9
89·2
90·7
92·3
92·5
93·2
50
86·9
89·3
91·8
93·4
93·6
94·8
100
86·8
90·2
92·5
93·8
95·0
96·1
500
86·5
89·8
93·7
95·1
96·0
97·3
5
81·4
83·2
84·7
84·3
83·7
82·6
10
83·5
86·3
87·4
88·7
88·9
89·3
Student t
25
84·1
88·3
90·5
91·5
92·1
92·6
50
84·9
88·3
91·6
93·1
93·3
94·5
100
83·9
88·7
92·1
93·7
95·0
95·9
500
82·7
88·0
93·2
94·8
96·1
97·2
5
81·3
83·2
84·7
84·2
83·6
82·4
10
83·2
86·4
87·4
88·7
88·9
89·2
Bayesian
25
83·9
88·3
90·5
91·5
92·2
92·6
50
84·8
88·4
91·6
93·1
93·3
94·5
100
83·6
88·7
92·0
93·7
95·0
95·9
500
82·5
88·0
93·2
94·8
96·1
97·2
Example 2. Driving time data ([16], p. 83; [5]). The driving times along two
diﬀerent routes from a person’s house to work were measured; there were n1 = 5
trips for the ﬁrst route and n2 = 11 trips for the second route. The complete
data set is given below
y1 = (6·5, 6·8, 7·1, 7·3, 10·2),
y2 = (5·8, 5·8, 5·9, 6·0, 6·0, 6·0, 6·3, 6·3, 6·4, 6·5, 6·5).
The task is to determine whether there is a diﬀerence in the average travel
times for the two routes. For this problem, both the frequentist and Bayesian
procedures ﬁnd that the diﬀerence between the two means is not signiﬁcant at a
signiﬁcance level of α = 0·05. The Wallace–Freeman codelengths for the model
with a common population mean and the model with two diﬀerent population
means were 19·30 nits and 20·14 nits respectively. Thus, the MML approach
prefers the model with one population mean with a posterior odds of 2·3.

MML Analysis of a Behrens–Fisher Problem
257
Example 3. Parameter estimation. The performance of the Wallace–Freeman es-
timator (11) is now compared against the maximum likelihood (ML) estimator
on the problem of inferring the common mean of two normal populations with
unknown variances; that is, the true hypothesis is assumed to be (μ1 = μ2)
and the MML and ML methods are compared solely on their parameter estima-
tion performance. The testing setup was as follows: (1) sample all parameters
θ = (μ, τ1, τ2)′ ∈R3 from their respective prior densities, (2) sample data y
from the resulting model with (n1, n2) ∈{5, 10, 25, 50, 100, 500}2. The popula-
tion variances were sampled from the compact set τi ∈Ξ = [0·1, 5] for (i = 1, 2);
the normalisation constant is then Ω ≈15·30. The common population mean
was uniformly sampled from (−5 ≤μ ≤5). For each data set, the Wallace–
Freeman (11) estimator and the maximum likelihood estimator were used to
infer the parameters θ. The entire procedure was repeated for 105 iterations.
Following each iteration, the Kullback–Leibler (KL) divergence [17] of the two
estimators from the data generating distribution was computed. The results ex-
pressed in terms of the median KL divergence are presented in Table 2. The
Wallace–Freeman estimator is clearly superior to the maximum likelihood esti-
mator for small samples sizes (n1, n2 ≤25). The two criteria performed similarly
when there was a large imbalance in the sample sizes which agrees with the re-
sults presented in [18]. Both the Wallace–Freeman and ML estimators performed
identically for all samples sizes (n1, n2 ≥50).
Table 2. The median Kullback–Leibler divergence computed over 105 iterations be-
tween the data generating distribution and the MML and ML estimators
Estimator n1
n2
5
10
25
50
100
500
5
0·329
0·208
0·126
0·094
0·074
0·055
10
0·207
0·137
0·082
0·059
0·045
0·029
MML
25
0·127
0·082
0·050
0·035
0·025
0·014
50
0·095
0·060
0·035
0·024
0·017
0·009
100
0·074
0·045
0·025
0·017
0·012
0·006
500
0·055
0·029
0·014
0·009
0·006
0·002
5
0·416
0·239
0·136
0·098
0·077
0·055
10
0·237
0·149
0·086
0·061
0·046
0·029
ML
25
0·137
0·086
0·051
0·036
0·025
0·014
50
0·099
0·062
0·036
0·025
0·017
0·009
100
0·077
0·046
0·026
0·017
0·012
0·006
500
0·056
0·029
0·014
0·009
0·006
0·002
5
Extensions
It is relatively straightforward to extend the Wallace–Freeman codelength for-
mulae from Section 3 to the generalised Behrens–Fisher problem. The data now
comprises (d > 2) mutually independent samples of i.i.d. sequences generated
by the following Gaussian model:
yij ∼N(μi, τi),
(14)

258
E. Makalic and D.F. Schmidt
where (i = 1, . . . , d; j = 1, . . . , ni) and μ = (μ1, . . . , μd)′ and τ = (τ1, . . . , τd)′
are the unknown sequence means and variances respectively. The complete data
set is denoted by y = (y′
1, . . . , y′
d)′ and comprises n = (n1 + · · · + nd) samples.
The generalised Behrens–Fisher problem is testing whether or not there exists a
diﬀerence between the population means; that is, whether (μ1 = μ2 = · · · = μd).
Consider ﬁrst the Wallace–Freeman codelength under the assumption that
there exists a common population mean across the d data sequences. The pa-
rameter vector is θ = (μ, τ ′)′ ∈Rd+1, and assuming the uniform prior density
for the population mean (8) and conjugate scale invariant prior densities for
the population variances πτ (τ) = (Ωdτ1τ2 . . . τd)−1, the total Wallace–Freeman
codelength is
n
2 log 2π + 1
2
d

i=1
⎛
⎝ni log τi + 1
τi
ni

j=1
(yij −μ)2
⎞
⎠+ 1
2 log
6 d

i=1
ni
τi
7
+1
2 log
6
Ω2
d(y′y)
n
d

i=1
ni
7
+ c(d + 1),
where Ωd is a suitable normalisation constant. As in Section 3, the total code-
length must be numerically minimised for (μ, τ ′)′.
The Wallace–Freeman codelength for the model in which (μ1, . . . , μd) are free
parameters is easily derived from (12). The feasible parameter set for the d
population means is now a hyper-ellipsoid
Λd =

(μ1, μ2, . . . , μd) :
d

i=1
niμ2
i ≤y′y

,
resulting in the uniform prior density
πμ(μ) =
1
vol(Λd) = Γ(d/2 + 1)
(πy′y)(d/2)
6 d

i=1
ni
7(1/2)
,
μ ∈Λd.
where Γ(·) is the gamma function. Invariant conjugate scale prior densities are
again used for the population variances. The total Wallace–Freeman codelength
for the model with d population means is then
n
2 log 2π + 1
2
6 d

i=1
(ni −1) log ˆτi
7
+ n −d
2
+ d
2 log (πy′y) + 1
2
d

i=1
log
ni
2
	
−log Γ(d/2 + 1) + log Ωd + c(2d)
where the Wallace–Freeman estimates of τ are equivalent to those in (13).
Testing the hypothesis of the existence of a common mean across the d data
sequences follows the same procedure as per Section 3.3. This process can also be
extended to other hypothesis tests, such as testing for a common population vari-
ance. Furthermore, an analysis of the multivariate Behrens–Fisher problem un-
der the minimum message length framework is possible given Wallace–Freeman
codelengths for a multivariate normal distribution ([6], pp. 261–264).

MML Analysis of a Behrens–Fisher Problem
259
A
Prior Distribution over the Population Means
Consider the standard linear regression model for data y ∈Rn
y = Xβ + ε
where X is a (n × p) design matrix, β ∈Rp denotes the coeﬃcient vector and
ε ∈Rn are zero mean i.i.d. Gaussian variates with covariance matrix Σ. The
Behrens–Fisher problem is then a special case of the linear regression model for
a suitable choice of design matrix X and noise covariance matrix Σ. The aim
here is to derive a prior density π(·) for the coeﬃcients β which can be used in
the absence of any subjective knowledge.
Ideally, the prior density should give each combination of regression coeﬃ-
cients the same probability. A possible choice is to use an independent uniform
prior for each coeﬃcient, however this requires arbitrary bounding of the param-
eter space and the resulting model selection criteria would be highly dependent
on the chosen support. An alternative approach is to exploit the fact that the
observed data y are generated by the model
y = y∗+ ε,
where y∗denotes the “true” signal. Note that
E [y′y] = y′
∗y∗+ tr (Σ) .
where E[·] denotes the expectation operator. Having observed y, one can form
an estimate, say ˆy = X ˆβ, of the true signal. Since the covariance matrix Σ is
strictly positive deﬁnite, it is expected that the estimate, ˆy′ˆy, should satisfy
y′y ≥ˆy′ˆy = ˆβ′ (X′X) ˆβ.
(15)
The least-squares estimates, the James-Stein shrunken least squares estimates [19],
and other estimates that obtain minimax squared error risk satisfy restriction
(15), which oﬀers strong support for this choice of prior. Hence, the feasible
parameter space for the regression coeﬃcients is given by the hyper-ellipsoid
Λ = {β : β′ (X′X) β ≤y′y} .
A suitable joint prior density for the regression coeﬃcients is then
π(β) =
1
vol(Λ) = Γ(p/2 + 1)
1
|X′X|
(πy′y)p/2
,
β ∈Λ
(16)
where Γ(·) is the gamma function. This is a uniform prior over the volume of
the feasible set Λ and is equivalent to assigning the same probability mass to
each possible combination of regressors. The prior density (16) has been used to
derive an MML model selection criterion for linear regression models that has
the desirable property of being invariant under full-rank aﬃne transformations
of the design matrix [14].

260
E. Makalic and D.F. Schmidt
References
1. Scheﬀ´e, H.: On solutions of the Behrens-Fisher problem, based on the t-
distribution. The Annals of Mathematical Statistics 14(1), 35–44 (1943)
2. Satterthwaite, F.E.: An approximate distribution of estimates of variance compo-
nents. Biometrics Bulletin 2(6), 110–114 (1946)
3. Fisher, R.A.: Inverse probability. Proceedings of the Cambridge Philosophical So-
ciety 26, 528–535 (1930)
4. Fisher, R.A.: The ﬁducial argument in statistical inference. Annals of Eugenics 6,
391–398 (1935)
5. Kim, S.H., Cohen, A.S.: On the Behrens–Fisher problem: A review. Journal of
Educational and Behavioral Statistics 23(4), 356–377 (1998)
6. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length,
1st edn. Information Science and Statistics. Springer (2005)
7. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)
8. Wallace, C.S., Freeman, P.R.: Estimation and inference by compact coding. Journal
of the Royal Statistical Society (Series B) 49(3), 240–252 (1987)
9. Solomonoﬀ, R.J.: A formal theory of inductive inference. Information and Con-
trol 7(2), 1–22, 224–254 (1964)
10. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1(1), 1–7 (1965)
11. Chaitin, G.J.: A theory of program size formally identical to information theory.
Journal of the Association for Computing Machinery 22(3), 329–340 (1975)
12. Wallace, C., Boulton, D.: An invariant Bayes method for point estimation. Classi-
ﬁcation Society Bulletin 3(3), 11–34 (1975)
13. Dowe, D.L., Wallace, C.S.: Resolving the Neyman-Scott problem by minimum mes-
sage length. In: Proc. 28th Symposium on the Interface, Sydney, Australia. Com-
puting Science and Statistics, vol. 28, pp. 614–618 (1997)
14. Schmidt, D.F., Makalic, E.: MML invariant linear regression. In: Nicholson, A., Li,
X. (eds.) AI 2009. LNCS, vol. 5866, pp. 312–321. Springer, Heidelberg (2009)
15. Jeﬀreys, H.: Note on the Behrens–Fisher formula. Annals of Eugenics 10, 48–51
(1940)
16. Lehmann, E.L.: Nonparametrics: Statistical methods based on ranks. Mcgraw–Hill
(1974)
17. Kullback, S., Leibler, R.A.: On information and suﬃciency. The Annals of Math-
ematical Statistics 22(1), 79–86 (1951)
18. Pal, N., Lin, J.J., Chang, C.H., Kumar, S.: A revisit to the common mean problem:
Comparing the maximum likelihood estimator with the Graybill-Deal estimator.
Computational Statistics & Data Analysis 51(12), 5673–5681 (2007)
19. Sclove, S.L.: Improved estimators for coeﬃcients in linear regression. Journal of
the American Statistical Association 63(322), 596–606 (1968)

MMLD Inference of Multilayer Perceptrons
Enes Makalic1 and Lloyd Allison2
1 Centre for MEGA Epidemiology, The University of Melbourne
Carlton, VIC 3053, Australia
emakalic@unimelb.edu.au
2 Faculty of Information Technology, Monash University
Clayton, VIC 3800, Australia
lloyd.allison@monash.edu.au
Abstract. A multilayer perceptron comprising a single hidden layer of
neurons with sigmoidal transfer functions can approximate any com-
putable function to arbitrary accuracy. The size of the hidden layer
dictates the approximation capability of the multilayer perceptron and
automatically determining a suitable network size for a given data set
is an interesting question. This paper considers the problem of inferring
the size of multilayer perceptron networks with the MMLD model selec-
tion criterion which is based on the minimum message length principle.
The two main contributions of the paper are: (1) a new model selection
criterion for inference of fully-connected multilayer perceptrons in regres-
sion problems, and (2) an eﬃcient algorithm for computing MMLD-type
codelengths in mathematically challenging model classes. Empirical per-
formance of the new algorithm is demonstrated on artiﬁcially generated
and real data sets.
1
Introduction
Artiﬁcial neural networks are commonly used in nonlinear modelling and have
become an important tool in the modern machine learning repertoire. Perhaps
the most commonly used neural network architecture in practice is the multi-
layer perceptron. A multilayer perceptron (MLP) with a single hidden layer of
sigmoidal neurons can approximate any computable function to arbitrary accu-
racy provided enough neurons are present in the hidden layer [1]. Applications
of MLPs exist in many diﬀerent areas including bond rating [2], pattern recog-
nition [3] and medicine [4], among others.
Formally, let Mkh correspond to a single hidden layer MLP comprising kh
hidden neurons. Given a kd-dimensional (kd > 0) input vector x ∈Rkd and a
target datum y ∈R, an MLP model Mkh for explaining the pair (x, y) is
y = f(v0, v, w0, w; x) + ϵ = v0 +
kh

j=1
vjφ(w′
jx + wj0) + ϵ
(1)
where ϵ ∼N(0, τ) is a zero-mean Gaussian random variable with variance τ > 0,
the nonlinear transfer function is a sigmoidal function φ(·) = tanh(·), and the
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 261–272, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

262
E. Makalic and L. Allison
network parameters v0 ∈R, v = (v1, . . . , vkh)′ ∈Rkh, w0 = (w01, . . . , w0kh)′ ∈
Rkh and w = (w′
1, . . . , w′
kh)′ ∈Rkd×kh denote the output bias, the output
weights, the hidden biases and the hidden weights, respectively. The neural net-
work weights and biases may be stacked into a single parameter vector θ =
(v0, v′, w0′, w′)′ ∈Rk, where k = kh(kd + 2) + 1 is the total number of net-
work parameters, not including the noise variance. The parameters (θ, τ) and
the number of hidden neurons (kh > 0) are considered unknown and must be
inferred from a data set
D = {(x1, y1), . . . , (xn, yn)} ,
xi ∈Rkd, yi ∈R, (i = 1, . . . , n)
(2)
comprising n input-output pairs. Increasing the number of hidden neurons kh
results in a more complex model with more free parameters. The task then is
to determine the smallest kh such that the resultant neural network ﬁts the
observed data well without overﬁtting.
This paper examines the problem of inferring fully connected, single hid-
den layer MLP networks within the minimum message length framework. The
two main contributions of the paper are: (1) a new model selection criterion
for inference of MLPs that is based on the MMLD codelength approximation
[5–7], and (2) an eﬃcient algorithm for computing MMLD-type codelengths in
mathematically challenging model classes.
2
Minimum Message Length (MML)
The minimum message length (MML) [8, 9, 7] principle of inductive inference
states that the model that yields the briefest encoding, or the best compression,
of data in a hypothetical message is optimal. MML can be thought of as a
practical implementation of the theory of inductive inference proposed initially
by Solomonoﬀ[10], Kolmogorov [11] and Chaitin [12]. Here, the hypothetical
message consists of two parts: (1) a statement, commonly called the assertion,
describing a particular model θ ∈Θ ⊂Rk, and (2) encoding of the data y using
the assertion model θ (referred to as the detail). Let I(θ) and I(y|θ) denote
the length of the assertion and the detail respectively; the length is measured
in some convenient unit such as a binary digit (bit) or a base-e digit (nit). The
total length of the two-part message, I(θ, y) stating the data y and a particular
model θ is then the sum of the lengths of the assertion and the detail, namely
I(θ, y) = I(θ) + I(y|θ).
(3)
The MML principle of model selection advocates choosing the model θ that
minimises the length (3) of the hypothetical two-part message. Although several
possible approximations to the codelength I(θ, y) exist in the literature, the
Wallace–Freeman approximation and the MMLD approximation are the most
popular in practice.

MMLD Inference of MLPs
263
2.1
The Wallace–Freeman Approximation
The Wallace-Freeman, or MML87 codelength approximation [9, 7] for a model
θ ∈Θ ⊂Rk and data y = (y1, . . . , yn)′ is
I87(y, θ) = −log π(θ) + 1
2 log |Jθ(θ)| + k
2 log κk
2
34
5
I87(θ)
+ k
2 −log p(y|θ)
2
34
5
I87(y|θ)
(4)
where π(·) denotes a prior distribution over the parameter space Θ, Jθ(θ) is
the Fisher information matrix, and κk is the normalised second moment of an
optimal quantising lattice in k-dimensions. Model selection by the the Wallace–
Freeman approximation proceeds by ﬁnding the model ˆθ87(y) that minimises (4).
The MML87 codelength approximation allows for both model selection and pa-
rameter estimation within the same Bayesian framework. Unlike the commonly
used maximum a posteriori estimates, the MML87 estimates obtained by min-
imising (4) are invariant under one-to-one re-parameterisation of the parameter
space Θ. To date, the MML87 estimator has been successfully applied to a wide
range of statistical models, including factor analysis [13] and mixture models [14],
among others.
The Wallace–Freeman approximation is derived under several critical assump-
tions that may not be satisﬁed in some statistical models (see pp. 226–227, [7]).
In particular, the approximation requires that: (1) the log-likelihood is approx-
imately quadratic around the maximum, (2) the Fisher information matrix is
positive deﬁnite over the entire parameter space Θ, and (3) the prior distribu-
tion π(·) is locally continuous and ‘slowly’ varying in the region determined by
Jθ(θ); note, this region is often referred to as the uncertainty region.
The Wallace–Freeman assumptions are generally not satisﬁed in multilayer
perceptron networks. The Fisher information matrix is known to be singular if
any of the following three conditions hold [15]:
1. ∃j > 0, vj = 0
2. ∃j > 0, wj = (0, 0, . . ., 0)′
3. ∃j1, j2 > 0, (wj1, w0j1) = ±(wj2, w0j2)
where vj and wj denote the network bias and weight parameters respectively (see
Section 1). The aforementioned conditions imply that the Fisher information will
be singular if the MLP contains redundant hidden neurons. In practice, it is quite
rare that any of the three conditions hold exactly. However, MLP networks for
which some of the above conditions are ‘almost’ satisﬁed are relatively common.
In this case, the MLP network exhibits a nearly singular Fisher information
matrix which creates problems for the Wallace–Freeman approximation. The
main issue here is not due to the (near) singularity of the Fisher information
matrix but in the size of the uncertainty region; the uncertainty region grows
unbounded as one gets closer to the singularity point which causes a breakdown
in the Wallace–Freeman codelength formula. The reader is referred to [16] for
a detailed analysis of the Wallace–Freeman approximation applied to model
selection of MLPs.

264
E. Makalic and L. Allison
2.2
The MMLD Approximation
A recent alternative to the Wallace–Freeman approximation is Dowe’s MMLD
codelength approximation [5–7]. The MMLD approximation does not explicitly
use the Fisher information matrix in the assertion alleviating some of the afore-
mentioned issues when computing codelengths for models like MLPs, where the
matrix is generally ill-conditioned. Let Ω(y) ⊂Rk denote a region in the param-
eter space Θ whose size and geometry explicitly depend on the observed data y.
The MMLD codelength for a model θ ∈Ω(y) and data y is:
ID (y, θ) = −log
6
Ω(y)
π(θ)dθ
7
−
1
:
Ω(y) π(θ)dθ

Ω(y)
π(θ) log p(y|θ)dθ. (5)
In practice, computing the MMLD codelength amounts to solving an optimi-
sation problem to determine the uncertainty region Ω. If no prior assumptions
are made about the geometry of Ω, determining the uncertainty region is a non-
trivial task especially in high dimensional parameter spaces. Furthermore, even
if the shape of the region Ω is known a priori, the integrals required to estimate
the MMLD codelength are often not analytically tractable. The next section
introduces an algorithm for computing MMLD codelengths, henceforth referred
to as MML07, that is applicable to many commonly used statistical models.
3
A General Algorithm for Computing MMLD
Codelengths
3.1
Spherical Uncertainty Region
It is possible to construct an eﬃcient algorithm for the computation of MMLD
codelengths if some assumptions are made about the geometry of the uncertainty
region Ω(y). To illustrate the general idea, assume ﬁrst that the uncertainty
region is a hypersphere with radius r > 0. In order to compute the MMLD
codelength, one requires the parameter estimate corresponding to the centre of
the hypersphere as well as the hypersphere radius. The hypersphere radius can
be determined from, say, the Wallace–Freeman codelength by noting that the
volume, V (Ω), of the region Ω is approximately:
V (Ω) = κ−k/2
k
|Jθ(ˆθ)|−1/2
(6)
and the volume of a k-dimensional hypersphere, S, with radius r > 0 is:
V (S) =
πk/2
Γ
 k
2 + 1
rk.
(7)
While the optimal value for κk is only known for some k > 0, Zador [17] has
derived a sphere-based lower bound
κk ≈
,
1
(k + 2)π
-
Γ
,k
2 + 1
-2/k
(8)

MMLD Inference of MLPs
265
which is suﬃciently accurate for this example. Substituting (8) for κk and solving
V (S) = V (Ω) for r yields [7]:
r ≈
√
k + 2
|Jθ(ˆθ)|1/(2k) .
(9)
The centre of the hypersphere ˆθ may be taken as the maximum likelihood
estimate or the maximum a posteriori estimate. Near singularities in the Fisher
information matrix in (9) lead to a large hypersphere radius r and are no longer
an issue in estimating codelengths since the assertion and detail are computed
by numerical integration. The MMLD codelength can now be computed using,
for example, cubature formulae for approximate integration over a k-dimensional
ball [18].
MMLD codelengths computed in this fashion will only be reasonable if there
is little correlation between the parameters resulting in a diagonal Fisher infor-
mation matrix. This is not the case in many statistical models, including MLP
networks, and an anisotropic region of integration that can capture correlations
in the parameter space should instead be utilised.
3.2
Ellipsoidal Uncertainty Region
A much improved approximation to MMLD codelengths can be obtained if one
opts for a (hyper-)ellipsoidal uncertainty region instead of the spherical region
discussed in the previous section. Uncertainty regions of ellipsoidal geometry are
able to capture varying levels of parameter correlation, including little to no
correlation, rendering the resulting codelength approximation usable in many
popular statistical models. Assume, as before, that the maximum likelihood or
the maximum a posteriori estimate is selected as the centre of the uncertainty
region. In order to compute MMLD codelengths, one must determine the size
and orientation of the k principal axes of the ellipsoid. This is in contrast to the
spherical uncertainty region where only the radius of the sphere is a unknown.
In order to determine the geometry of the uncertainty region, one may again
use the Hessian matrix or the Fisher information matrix of the log-likelihood
function. The spectral decomposition of the matrix Jθ(θ) is
Jθ(θ) =
k

j=1
λjuju′
j
(10)
where λ = (λ1, λ2, . . . , λk) and U = (u1, u2, . . . , uk) denote the eigenvalues and
the corresponding eigenvectors of Jθ(θ), respectively. Let r = (r1, r2, . . . , rk)′
and A = (a1, a2, . . . , ak) denote the radii and principal axes of the ellipsoid.
The orientation and size of the principal axes can be obtained from the afore-
mentioned eigenvalues and eigenvectors. In particular, we set
rj = λ−1/2
j
, (j = 1, 2, . . . , k)
(11)
A = U.
(12)

266
E. Makalic and L. Allison
The uncertainty region is therefore of the same orientation as the eigenvectors
of Jθ(θ) with radii set to the inverse of the singular values of Jθ(θ).
The radii r computed in this fashion will rarely minimise the codelength (5)
and may need to be perturbed to obtain a more accurate estimate of the MMLD
codelength. A method that works well in practice is to multiply all the radii
by a proportionality constant, say, t > 0, where t is allowed to vary until the
codelength (5) is minimum for a given ˆθ and data y.
Recall that the MMLD criterion requires integration over the uncertainty region
which is in general not analytically tractable. Given an uncertainty region Ω(y),
the MML07 algorithm estimates the integrals required in (5) with the Monte Carlo
method. Alternative approaches based on cubature formulae are also possible but
are not pursued further in this paper. Monte Carlo integration requires uniform
samples from the uncertainty region which can be obtained by a simple two-step
procedure: (1) sample uniformly from a k-dimensional unit sphere, and (2) linearly
transform the sphere into the hyperellipsoid deﬁned by ˆθ, (11) and (12). MML07uses
an eﬃcient algorithm for uniform sampling from a unit k-sphere [19] which only
requires samplers from the uniform and normal distributions.
Algorithm 1. MML07 algorithm for estimating MMLD codelengths
Require: proportionality constant t, number of samples B > 0
1: Compute parameter estimates ˆθ
ˆθ = arg max
θ∈Θ
{log π(θ)p(y|θ)}
(13)
2: Compute Jθ(ˆθ), the Fisher information matrix or the Hessian matrix
3: Compute uncertainty region radii r and axes orientation A; see eqn. (11) and (12)
4: Sample B vectors from the interior of a k-dimensional unit hypersphere centred at
ˆθ
S = {θ1, θ2, . . . , θB}
(14)
5: Transform samples S into samples E from the uncertainty region Ω
E = {Adiag(tr)θ1, Adiag(tr)θ2, . . . , Adiag(tr)θB}
(15)
6: Estimate MMLD codelength (5) by Monte Carlo integration using samples E
Algorithm 1 summarises the steps necessary to compute MMLD codelengths
with MML07 for a given a proportionality constant t. In Step 2, it is possible to
replace the Fisher information matrix or the Hessian matrix by an approximate
Hessian matrix obtained by, say, Richardson’s extrapolation. This may be useful
in cases where the Fisher information is diﬃcult to compute. Steps 4–5 are
necessary for integration by Monte Carlo and may not be required if alternate
numerical integration methods are employed. In particular, even if the Monte
Carlo approach is used, it is possible to replace the uniform sampling step with

MMLD Inference of MLPs
267
sampling from the prior density π(·). In this paper, the proportionality constant
t is determined by numerical optimisation where the objective function is the
MMLD codelength. In practice, a numerical optimisation procedure based on
the binary search algorithm converges in relatively few steps as the optimisation
function is concave and well-behaved.
An alternative method for estimating MMLD codelengths is the MMC algo-
rithm [6, 20]. Brieﬂy, MMC requires samples from the posterior distribution of
the parameter space which are then used with the boundary rule (pp. 171–173,
[7]) to estimate the uncertainty region, and hence the MMLD codelength. The
posterior distribution of multilayer perceptrons is complex and highly multi-
modal with large regions of ﬂat curvature. While algorithms for sampling from
such posteriors exist, see [21], these generally require a signiﬁcant amount of com-
putational resources and time. Furthermore, MMC requires grouping of MLPs
based on the Kullback–Leibler (KL) metric [22] once posterior samples have been
computed. This is highly ineﬃcient and computationally demanding.
3.3
A Simple Example: Univariate Normal Distribution
The MML07 algorithm is now demonstrated on a simple problem where MML87
is known to be well-behaved facilitating an easy comparison between the MML07
and MML87 codelengths. For this experiment, n = {10, 50, 100, 250, 500, 1000}
samples of data were generated from a univariate normal distribution with mean
μ = 20 and variance τ = 49. The prior distributions for the mean and the
variance parameters were identical in both codelength formulae and are not
relevant to the accompanying discussion. Following the data generation step,
codelengths of the data were computed using both the MML87 and the MML07
formulae. The maximum a posterior estimate was used as the centre point of the
uncertainty region in MML07. The MML07 and MML87 codelengths were virtually
identical in all the experiments even for small amounts of data; for example, given
a data set of n = 10 samples, the diﬀerence in MML07 and MML87 codelengths
was approximately 0.2 of a nit.
4
MMLD Inference of Multilayer Perceptrons
We next examine MMLD model selection of fully-connected, single hidden layer
MLP networks for regression problems. In order to compute MMLD codelengths
for such MLP networks, one requires a likelihood function and a prior density
over the parameter space (see Section 4.1). In addition, a prior density over
the architecture of the network is required since the MMLD codelength will be
used to select between candidate networks of varying sizes. This is discussed in
Section 4.2.
From (1), the MLP negative log-likelihood of a data set D (2) is
−log p(y|θ, τ) = 1
2τ
n

i=1
(yi −f(θ; xi))2 + n
2 log 2πτ
(16)
where θ ∈Rk denotes the k network weight (and bias) parameters.

268
E. Makalic and L. Allison
The Fisher information matrix (or Hessian matrix) required for the MML07
algorithm is readily available in the literature (see, for example, [23]) and is eﬃ-
ciently computed using Pearlmutter’s algorithm [24]. The next section discusses
a suitable prior density for MLP networks.
4.1
Prior Density for the Model Parameters
The prior distribution for the noise variance τ is chosen to be the log-uniform
prior πτ(τ) ∝1/τ deﬁned over some compact set, say [e−6, e6]; the range of
the support for this prior density does not aﬀect model selection and is omitted
from further discussion. This commonly used prior density is invariant to the
measurement scale used for the data and is thus a sensible choice. Selecting the
prior density for the network weights and biases is a nontrivial task as these
parameters have no interpretable meaning. Following Neal [21], we choose a
zero-mean, spherical Gaussian prior with a variance hyperparameter α which
advocates smaller weights and hence ‘smooth’ functions. The complete prior
density over the variance parameter and all the network parameters is then
π(θ, τ) ∝1
τ
,
1
√
2πα
-k
exp
,
−θ′θ
2α
-
.
(17)
The variance hyperparameter α > 0 can be set to a ﬁxed value or chosen
based on the observed data. If the data set is scaled to a have zero mean and
unit variance, a ﬁxed value for the hyperparameter works relatively well in prac-
tice. In this paper, we set the hyperparameter α as in the automatic relevance
determination framework [25]. Brieﬂy, the hyperparameter value is estimated
based on the Laplace approximation to the posterior distribution of θ result-
ing in a computationally eﬃcient rule for updating the hyperparameter as the
network is trained. The update formula is a simple implicit equation
1
α = θ′θ
γ .
(18)
Here, γ is usually referred to as the eﬀective number of parameters and is
γ =
k

j=1
λj
λj + α
(19)
where λ = (λ1, λ2, . . . , λk)′ are the eigenvalues of the Hessian matrix, or the
Fisher information matrix. Strictly, the hyperparameter α should also be stated
in the hypothetical two-part message for the MMLD criterion to be valid. How-
ever, since this hyperparameter is common to all networks being compared, one
may code α assuming an asymptotically eﬃcient code of order O(log k) [26]
thereby eliminating the eﬀect of the hyperparameter on model selection.

MMLD Inference of MLPs
269
4.2
Prior Density for the Network Architecture
A slight modiﬁcation to the MMLD codelength (5) is necessary to allow for
automatic selection of the MLP network architecture. The modiﬁcation involves
adding a codelength for stating the number of hidden neurons in the network
and has no impact on the MML07 algorithm. The total codelength for an MLP
network with kh hidden neurons is now
I(θ, y; Mkh) = ID(θ) + ID(y|θ) + ID(Mkh)
(20)
where ID(Mkh) denotes a prior density over the number of hidden layer neurons.
In this paper, we have chosen the geometric distribution for the number of
hidden neurons yielding a codelength
ID(Mkh) = −(kh −1) log(1 −ξ) −log ξ
(0 ≤ξ ≤1)
(21)
where ξ determines the strength of the penalty for larger networks; for the ex-
periments in Section 5, we set ξ = 1/5 implying that, on average, one expects
the inferred MLP network to have around ﬁve hidden neurons. In practice, the
codelength ID(Mkh) makes little diﬀerence to model selection and could also be
omitted.
5
Discussion and Results
The performance of the MML07 algorithm is now examined using artiﬁcially gen-
erated data as well as a real data set. In the following experiments, all MLPs
were trained using the Levenberg–Marquardt algorithm on the MATLAB nu-
merical computing platform and comprised a single hidden layer of nonlinear
neurons. For tests with artiﬁcial data, input-output data pairs were generated
from the following three test functions [27–29]:
y1 = sin(5x) + sin(15x) + sin(25x) + ϵ1,
x ∈(0, 1)
y2 = 4.26

e−x −4e−2x + 3e−3x
+ ϵ2,
x ∈(0, 2)
y3 = 10 sin(πx1x2) + 20(x3 −0.5)2 + 10x4 + 5x5 + ϵ3,
xi ∈(0, 1).
The input and output variables were standardised to have zero mean and
unit variance. Training data sets with sample sizes n = {100, 400, 700, 1000}
were used for all three test functions. Gaussian noise was added to the target
function with the signal-to-noise ratio (SNR) in the set {1, 4, 8, 16}. For the
MML07 algorithm, the maximum a posteriori estimate was used as the centre of
the uncertainty region (see Section 3). Each experiment was repeated for 1000
iterations. For each iteration, the prediction errors for the criteria were estimated
based on an independently generated test set of size n = 104.
The performance of the MML07 algorithm for computation of MMLD code-
lengths was measured in terms of the complexity of the selected MLP (that is,
the number of hidden neurons) and the mean squared prediction error on the

270
E. Makalic and L. Allison
Table 1. Mean squared prediction errors for data generated from functions y1, y2, y3
when signal-to-noise ratio (SNR) is 1
Training Sample
Model Selection Criteria
MML07
AIC
AICc
BIC
MDL
y1
100
1.380
116.9
110.7
1.328
1.417
400
1.118
3.513
1.066
1.062
1.114
700
1.125
1.031
1.031
1.028
1.086
1000
1.004
1.014
1.014
1.016
1.039
y2
100
1.155
1.267
1.251
1.174
1.174
400
1.018
1.056
1.056
1.037
1.037
700
1.022
1.031
1.031
1.027
1.028
1000
1.013
175.4
175.4
1.013
1.014
y3
100
1.650
6.073
4.631
4.435
2.008
400
1.068
1.278
1.228
1.068
1.073
700
1.052
1.127
1.126
1.052
1.045
1000
1.056
1.104
1.104
1.042
1.048
test set. The MML07 algorithm was compared against the AIC [30], AICc [31],
BIC [32] and MDL criteria [33]. A new AIC-like criterion, referred to as the net-
work information criterion (NIC), developed speciﬁcally for MLP networks [34]
was initially included in all experiments. However, this criterion required the
evaluation of the inverse of the Fisher information matrix which is often (nearly)
singular in MLP models. In preliminary experiments, NIC showed highly vari-
able performance especially when used with larger neural networks where many
parameters are redundant. This has made it diﬃcult to interpret the underly-
ing behaviour of the criterion and NIC was consequently not included in the
empirical comparisons.
Table 1 depicts the mean squared prediction errors when SNR=1 for the ﬁve
criteria considered in this paper using data generated from functions y1, y2 and
y3. As the SNR was increased, the ranking of the criteria considered in the pa-
per remained unchanged. The results presented in Table 1 are therefore represen-
tative of the complete experiment. The MML07 criterion performed well in terms
of prediction error and often selected a simpler MLP network in contrast to the
other criteria in a majority of the conducted experiments. The performance of
the AIC-type criteria was highly variable for small training sets and high levels of
noise. Both AIC and AICc tended to select more complex models in contrast to the
other criteria for all three functions considered in the paper. The BIC and MDL
criteria resulted in MLPs with similar prediction performance to MML07. For small
amounts of data, MLP networks selected by MML07 often had the lowest predic-
tion error amongst all the criteria considered and never resulted in unreasonable
models. As the amount of training data was increased, the BIC, MDL and MML07
criteria inferred virtually identical MLP networks in all the experiments consid-
ered. For a more detailed analysis of the MML07 algorithm, see [16].

MMLD Inference of MLPs
271
We also brieﬂy examined the performance of the MML07 algorithm on the
‘nelson’ data set [35] which comprised two inputs and one real target variable.
The total data set size was n = 128, from which 64 samples were used for
training while the remaining 64 samples were used for testing. The experiment
was repeated for 100 iterations. Here, the AIC, AICc and BIC criteria selected
MLP networks of three hidden neurons as optimal. In contrast, both the MDL
and MML07 criteria preferred a simpler MLP with one or two hidden neurons.
The MML07 algorithm obtained the best prediction score for this experiment
from all the criteria considered. A more detailed empirical comparison of the
MML07 criterion is available in [16].
References
1. Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are uni-
versal approximators. Neural Networks 2(5), 359–366 (1989)
2. Daniels, H., Kamp, B.: Application of MLP networks to bond rating and house
pricing. Neural Computing and Applications 8(3), 226–234 (1999)
3. Cardinaux, F., Sanderson, C., Marcel, S.: Comparison of MLP and GMM classiﬁers
for face veriﬁcation on XM2VTS. In: Kittler, J., Nixon, M.S. (eds.) AVBPA 2003.
LNCS, vol. 2688, pp. 911–920. Springer, Heidelberg (2003)
4. Duch, W., Adamczak, R., Grabczewski, K., Jankowski, N., Zal, G.: Medical diag-
nosis support using neural and machine learning methods. In: International Con-
ference on Engineering Applications of Neural Networks (EANN 1998), Gibraltar,
pp. 292–295 (1998)
5. Dowe, D.L.: Foreword re C. S. Wallace. The Computer Journal 51(5), 523–560
(2008)
6. Fitzgibbon, L.J., Dowe, D.L., Allison, L.: Univariate polynomial inference by Monte
Carlo message length approximation. In: Proceedings of the Nineteenth Interna-
tional Conference on Machine Learning (ICML 2002), pp. 147–154 (2002)
7. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length,
1st edn. Information Science and Statistics. Springer (2005)
8. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)
9. Wallace, C.S., Freeman, P.R.: Estimation and inference by compact coding. Journal
of the Royal Statistical Society (Series B) 49(3), 240–252 (1987)
10. Solomonoﬀ, R.J.: A formal theory of inductive inference. Information and Con-
trol 7(2), 1–22, 224–254 (1964)
11. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1(1), 1–7 (1965)
12. Chaitin, G.J.: A theory of program size formally identical to information theory.
Journal of the Association for Computing Machinery 22(3), 329–340 (1975)
13. Wallace, C.S., Freeman, P.R.: Single-factor analysis by minimum message length
estimation. Journal of the Royal Statistical Society (Series B) 54(1), 195–209 (1992)
14. Wallace, C., Dowe, D.L.: MML mixture modelling of multi-state, Poisson, von
Mises circular and Gaussian distributions. In: Proceedings of the 6th International
Workshop on Artiﬁcial Intelligence and Statistics, Ft. Lauderdale, Florida, U.S.A,
pp. 529–536 (1997)
15. Fukumizu, K.: A regularity condition of the information matrix of a multilayer
perceptron network. Neural Networks 9(5), 871–879 (1996)

272
E. Makalic and L. Allison
16. Makalic, E.: Minimum Message Length Inference of Artiﬁcial Neural Networks.
PhD thesis, Clayton School of Information Technology, Monash University (2007)
17. Zador, P.: Asymptotic quantization error of continuous signals and the quantization
dimension. IEEE Transactions on Information Theory 28(2), 139–149 (1982)
18. Stroud, A.H.: Approximate calculation of multiple integrals. Prentice-Hall (1971)
19. Petersen, W.P., Bernasconi, A.: Uniform sampling from an n-sphere: Isotropic
method. Technical Report TR-97-06, Swiss Center for Scientiﬁc Computing,
Z¨urich, Switzerland (1997)
20. Fitzgibbon, L.J., Dowe, D.L., Allison, L.: Bayesian posterior comprehension via
message from Monte Carlo. In: Proc. 2nd Hawaii International Conference on
Statistics and Related Fields. Springer (June 2003)
21. Neal, R.M.: Bayesian learning for neural networks. Lecture Notes in Statistics.
Springer (August 1996)
22. Kullback, S., Leibler, R.A.: On information and suﬃciency. Annals of Mathematical
Statistics 22(1), 79–86 (1951)
23. Scarpetta, S., Rattray, M., Saad, D.: Natural gradient matrix momentum. In:
Ninth International Conference on Artiﬁcial Neural Networks (ICANN 1999),
pp. 43–48 (1999)
24. Pearlmutter, B.A.: Fast exact multiplication by the hessian. Neural Computa-
tion 6(1), 147–160 (1994)
25. MacKay, D.J.C.: Comparison of approximate methods for handling hyperparame-
ters. Neural Computation 11(5), 1035–1068 (1999)
26. Makalic, E., Schmidt, D.F.: Minimum message length shrinkage estimation. Statis-
tics & Probability Letters 79(9), 1155–1161 (2009)
27. Lendasse, A., Simon, G., Wertz, V., Verleysen, M.: Fast bootstrap methodology
for regression model selection. Neurocomputing 64(2), 537–541 (2005)
28. Alippi, C.: FPE-based criteria to dimension feedforward neural topologies. IEEE
Transactions on Circuits and Systems – I: Fundamental Theory and Applica-
tions 56(8), 962–973 (1999)
29. Friedman, J.H.: Multivariate adaptive regression splines. The Annals of Statis-
tics 19(1), 1–67 (1991)
30. Akaike, H.: A new look at the statistical model identiﬁcation. IEEE Transactions
on Automatic Control 19(6), 716–723 (1974)
31. Hurvich, C.M., Tsai, C.L.: Model selection for extended quasi-likelihood models in
small samples. Biometrics 51(3), 1077–1084 (1995)
32. Schwarz, G.: Estimating the dimension of a model. The Annals of Statistics 6(2),
461–464 (1978)
33. Small, M., Tse, C.K.: Minimum description length neural networks for time
series prediction. Physical Review E (Statistical, Nonlinear, and Soft Matter
Physics) 66(6), 066701 (2002)
34. Murata, N., Yoshizawa, S., Ichi Amari, S.: Network information criterion-
determining the number of hidden units for an artiﬁcial neural network model.
IEEE Transactions on Neural Networks 5(6), 865–872 (1994)
35. Nelson, W.: Analysis of performance-degradation data. IEEE Transactions on Re-
liability 2(2), 149–155 (1981)

An Optimal Superfarthingale
and Its Convergence over a Computable
Topological Space
Kenshi Miyabe
Research Institute for Mathematical Sciences, Kyoto University
Abstract. We generalize the convergenece of the corresponding condi-
tional probabilities of an optimal semimeasure to a real probability in
algorithmic probability by using game-theoretic probability theory and
the theory of computable topology. Two lemmas in the proof give as
corollary the existence of an optimal test and an optimal integral test,
which are important from the point of view of algorithmic randomness.
We only consider an SCT3 space, where we can approximate the measure
of an open set. Our proof of almost-sure convergence to the real proba-
bility by a superfarthingale indicates why the convergence in Martin-L¨of
sense does not hold.
1
Introduction
Algorithmic probability [11,9] gives a formal theory of inductive inference. Its un-
derlying space is the space of sequences over a ﬁnite alphabet and the main tool is
a semimeasure. In this setting, Solomonoﬀ’s [19,20] central result is explained as
follows. Given a ﬁnite string, each semimeasure induces a subjective probability
of the next symbol. There exists an optimal semimeasure. Then the probability
induced by the optimal semimeasure converges to the real computable probabil-
ity almost surely.
In this paper, we make a ﬁrst step to generalize algorithmic probability to a
general space. The underlying space will be a space of sequences over a topo-
logical space. Our main results (Theorems 5 and 6) can be informally described
as follows. We want the probability that the next point falls in a subset of the
space given a ﬁnite sequence of points. There exists an optimal function and the
function induces the probability. Then the probability induced by the function
converges to the real probability almost surely.
A semimeasure is essentially an equivalent notion to a supermartingale in
algorithmic randomness [3,13]. Algorithmic randomness over a general space is
studied by some researchers [5,8,12]. The generalization uses computable topol-
ogy studied by [22,23]. To obtain the desired result, we require condition SCT3,
which is a computable separation axiom studied in [24]. We use a superfarthin-
gale [2,21], which is a prequential version of a supermartingale in game-theoretic
probability theory [17]. The eﬀectivization of game-theoretic probability the-
ory is a generalization of algorithmic probability. In this setting we prove the
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 273–284, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

274
K. Miyabe
existence of an optimal superfarthingale. We also prove convergence to a real
probability.
With lemmas developed in the proof of the existence of an optimal superfar-
thingale, we prove Theorem 3 and Theorem 4 as corollaries. These are important
from the point of view of algorithmic randomness.
Theorem 3 says that there exists an optimal test over an SCT3 space. Hoyrup
and Rojas [8,7] proved the existence of an optimal test over a computable metric
space and it is known that a computable metric space can be constructed from
an SCT3 space [6]. We give another more direct proof here.
Theorem 4 says that there exists an optimal integral test over an SCT3 space
with a computable measure. The existence of an optimal integral test over Can-
tor space is well-known [11]. Hoyrup and Rojas [8] proved the existence of an
optimal integral test over a computable metric space with a computable prob-
ability measure. We will prove the existence of an optimal integral test over a
SCT3 space with a computable measure. Note in particular that our proof does
not use the distance function.
In Section 3 we introduce a notion of approximation. Then we prove the
existence of an optimal test. In Sections 4 and 5 we prove the existence of an
optimal integral test and an optimal superfarthingale respectively and discuss
when and to which measure the prediction converges.
2
Preliminaries
We recall relevant results from various ﬁelds, as some of the terminology and
notations are not standard. Results in algorithmic probability and algorithmic
randomness are for comparison to our results.
We assume that the readers are familiar with computability theory [18,14,15]
on natural numbers. A subset of N is c.e. if there exists a computable function
such that the set is the domain of the function. A function f : X →R is c.e. if
{q ∈Q : q < f(x)} is c.e. uniformly in x ∈X. If X is countable, the meaning of
the deﬁnition is clear. If X is a topological space, we refer the reader to Section
2.4 for details. A function f : X →R is computable if f and −f are c.e.
2.1
Algorithmic Probability
For details see [11,9]. Here we only consider binary sequences. The set of ﬁnite
binary strings is denoted by 2∗. A semimeasure is a function μ : 2∗→R such that
μ(λ) ≤1 and μ(σ) ≥μ(σ0)+ μ(σ1) for all σ ∈2∗where λ is the empty string. If
the equalities hold, then μ is called a probability measure. A c.e. semimeasure μ0
is optimal 1 if, for all c.e. semimeasure μ, there exists a constant c > 0 such that
for all σ ∈2∗, we have μ0(σ) ≥c·μ(σ). There exists an optimal c.e. semimeasure.
For σ ∈2∗, σn is the n-th bit of σ, σt:n = σtσt+1 · · · σn−1σn and σ<n = σ1:n−1.
We also let μ(σn|σ<n) = μ(σ1:n)/μ(σ<n).
1 The terminology “universal” is more common.

An optimal superfarthingale and Its convergence
275
Theorem 1. Let μ0 be an optimal c.e. semimeasure and μ be a computable
probability measure. Then
μ0(σn|σ<n)
μ(σn|σ<n) →1 as n →∞
(1)
with μ-probability 1.
The convergence in diﬀerence is Solomonoﬀ’s [20] celebrated convergence result.
The above theorem is the convergence in ratio, which has ﬁrst been derived by
G´acs [11]. Informally speaking μ0(σn|σ<n) is the prediction induced from μ0.
If the sequence is sampled by the measure space with computable μ, then the
prediction converges to the real probability almost surely.
2.2
Algorithmic Randomness
For details see [13,3]. Cantor space 2ω (the set of inﬁnite sequences) is equipped
with the topology generated by cylinders [[σ]] = {A ∈2ω : σ ≺A} where σ ≺A
means that σ is a preﬁx of A. An open set U in 2ω is c.e. if U = 0{[[σ]] : σ ∈S}
where S is a c.e. set of binary strings. Let μ be the uniform measure on 2ω.
A μ-Martin-L¨of test is a sequence {Un} of uniformly c.e. open sets with
μ(Un) ≤2−n. A sequence α ∈2ω is μ-Martin-L¨of random if α ̸∈;
n Un for
all μ-Martin-L¨of tests. A μ-Martin-L¨of test {Un} is optimal if, for any other
μ-Martin-L¨of test {Vn}, there exists c such that Vn+c ⊆Un for all n. Let M be
an optimal semimeasure. Then a sequence α is μ-Martin-L¨of random iﬀthere is
a constant c such that M(α1:n) ≤c · μ(α1:n) for all n. The set of μ-Martin-L¨of
random sequences has μ-measure 1. By Theorem 1 the set of sequences satisfying
(1) also has μ-measure 1. Remarkably, the latter set does not contain the former
set in general. We will consider this problem in Section 5.2.
Theorem 2 (Hutter and Muchnik [10]). There exists an optimal semimea-
sure M and a computable measure μ and μ-Martin-L¨of random sequence α such
that
M(αn|α<n) −μ(αn|α<n) ̸→0 for n →∞.
2.3
Game-Theoretic Probability
Game-theoretic probability theory [17] is inﬂuenced by Dawid’s prequential prin-
ciple [1], which says that our evaluation of the quality of the forecasts p1, p2 . . .
in light of the observed outcomes x1, x2 . . . should not depend on Forecaster’s
model even if it exists and is known. In view of this a (super)farthingale was
introduced in [2].
Let X be a topological space and M be a space of probability measures on
X. Let Π = (M × X)∞and Π⋄= (M × X)∗. For π = (p1, x1, p2, x2, . . .) ∈Π,
let πn = (p1, x1, . . . , pn, xn) ∈Π⋄.

276
K. Miyabe
Deﬁnition 1 (superfarthingale). A farthingale is a function V
: Π⋄→
[−∞, ∞] satisfying
V (p1, y1, . . . , pn−1, yn−1) =

X
V (p1, y1, . . . , pn−1, yn−1, pn, x)dpn
(2)
for all n, all (p1, y1, p2, y2, . . .) ∈Π. If we replace “=” by “≥” in the equation
(2), we get the deﬁnition of a superfarthingale.
Game-theoretic probability assumes that a (super)farthingale V is bounded
almost surely. So a property stemming from the boundedness holds almost
surely. Such a theorem can be easily converted to a theorem in measure-theoretic
probability.
In [21], the case X = {0, 1} is considered and the existence of an optimal
superfarthingale is shown. Furthermore a game-random sequence is deﬁned as a
sequence π such that supn V (πn) < ∞for all c.e. superfarthingales V . The game-
randomness is a generalization of Martin-L¨of randomness. To deﬁne optimality
of a superfarthingale, we need the deﬁnition that a superfarthingale is c.e.
2.4
Computable Topology
We mainly refer to [23,24] for computable topology. Let Σ∗and Σω be the sets of
the ﬁnite and inﬁnite sequences, respectively, of symbols from a ﬁnite alphabet.
A function mapping ﬁnite or inﬁnite sequences of symbols from Σ is computable
if it can be computed by a Type-2 machine. A representation of M is a surjective
function γ :⊆Y →M where Y ∈{Σ∗, Σω}. An object x is γ-computable if it
has a computable representation p such that γ(p) = x. A sequence p ∈{Σ∗, Σω}
can be seen as a sequence of strings. We use v ≪p to mean that v is one of
strings in the sequence; see [23] for the detail.
A computable topological space is a 4-tuple X = (X, τ, β, ν) such that (X, τ) is
a topological T0-space, ν :⊆Σ∗→β is a notation of a base β of τ, dom(ν) is com-
putable and ν(u)∩ν(w) = 0{ν(w) : (u, v, w) ∈S} for all u, v ∈dom(ν) for some
c.e. set S ⊆(dom(ν))3. For the ﬁnite union of base sets, we use representation
0 νfs to mean 0 νfs = W ⇐⇒(∀v ≪w)v ∈dom(ν), W = 0{ν(v) : v ≪w}.
For the points, the open sets and the closed sets, we use representations δ, θ and
ψ−that are deﬁned as follows. For p ∈Σω and x ∈X, δ(p) = x iﬀp is a list of
all u ∈dom(ν) such that x ∈ν(u), θ(p) is the union of all ν(u) where u is listed
by p, and ψ−(p) = X\θ(p).
An example of a computable topological space is R
+ = (R
+, τ<, β<, ν<) where
R
+ is the set of non-negative reals and +∞, τ< is the lower topology, β< is the
set
{[0, +∞]} ∪{(q, +∞] : q ∈Q+},
Q+ is the set of non-negative rationals, and ν< is a computable notation of β<.
The representation δ of points in R
+ is denoted by ρ<.
Computable separations are studied in [24] in detail but we use only SCT3.

An optimal superfarthingale and Its convergence
277
Deﬁnition 2 (SCT3). There are a c.e. set R ⊆dom(ν) × dom(ν) and a com-
putable function r :⊆Σ∗× Σ∗→Σω such that for all u, w ∈dom(ν), ν(w) =
0{ν(u) : (u, w) ∈R} and (u, w) ∈R ⇒ν(u) ⊆ψ−◦r(u, w) ⊆ν(w).
For computability of measures we refer to [12]. One can ﬁnd an equivalent
representation in Schr¨oder [16]. Let M(X) be the space of ﬁnite non-negative
Borel measures on X. The following sets form a countable subbase of the A-
topology on M(X): {μ
:
μ(G) > q}, {μ
:
μ(X) < q} where G is the
ﬁnite union of base sets and q ∈Q. Let βA denote the base generated from the
above subbase and νA denote a natural computable notation of the base βA.
Then the 4-tuple M = (M(X), τA, βA, νA) is a computable topological space.
The representation δ of points in M(X) is denoted by δA. If U is an open set,
then μ(U) is approximated from below by a δA-representation of μ and a θ-
representation of U. If V is an closed set, then μ(V ) is approximated from above
by a δA-representation of μ and a ψ−-representation of V .
3
Optimal Test
In this section we give a proof of Lemma 1, which states an important property
of a SCT3 space with a computable measure and plays a central role in the proof
of Theorem 5. As a corollary we obtain the existence of an optimal test over a
SCT3 space.
3.1
Approximation
Let X = (X, τ, β, ν) be a computable topological space. Let μ be a measure on
X. Note that μ need not be computable.
We give an approximation of the measure of an open set. An approximation
is usually a sequence of rationals or sometimes a sequence of computable re-
als. However, the following approximation may be neither and carries a weaker
notion.
Deﬁnition 3 (Approximation). A μ-approximation of an open set U ⊆X is
a sequence (Un, an) such that Un is the ﬁnite union of base sets, Un ↑U, an is
a rational, μ(Un) ≤an and an −μ(Un) ≤2−n for each n.
We say that a μ-approximation (Un, an) is uniformly (0 νfs, ρ)-computable if
Un is uniformly 0 νfs-computable and an is uniformly ρ-computable.
To state lemmas and propositions clearly and concisely, we introduce the
notion of γ-relativeness for a representation γ.
Deﬁnition 4 (Relative computability). Let Xi = (Xi, τi, βi, νi) be com-
putable topological spaces for i = 1, 2. A point x2 ∈X2 is δ2-computable δ1-
relative to x1 ∈X1 if there exists a (δ1, δ2)-computable function f :⊆X1 →X2
such that f(x1) = x2.

278
K. Miyabe
Relative computability of open sets, closed sets and functions are similarly
deﬁned. For example, an open set U2 ⊆X2 is θ2-computable δA-relative to
μ1 ∈M(X1) iﬀthere exists a (δA, θ2)-computable function f :⊆M(X1) →τ2
such that f(μ1) = U2.
Lemma 1. Let X be a SCT3 space. For an open set U we can uniformly con-
struct a μ-approximation that is (0 νfs, ρ)-computable (θ, δA)-relative to (U, μ).
Proof. At ﬁrst we show the lemma in the case that U is a base set ν(w). Let R
and r be the c.e. set and the computable function for SCT3 from Deﬁnition 2.
Let {ui} be a computable enumeration of the c.e. set {u ∈dom(ν) : (u, w) ∈R}.
For each m, let Um = 0m
i=1 ν(ui) and Vm = 0m
i=1 ψ−◦r(ui, w). Note that Um
is open and Vm is closed. Then Um ⊆Vm ⊆ν(w) and Um ↑ν(w). It follows that
μ(Um) →μ(ν(w)) and μ(Vm) −ν(Um) →0 as m →∞.
Note that the measure μ(Um) is approximated from below and μ(Vm) from
above δA-relative to μ. Then we can compute mn from a representation of μ
such that μ(Vmn) −ν(Umn) ≤2−n.
Let μ(Umn)[s] and μ(Vmn)[s] be an approximation of rationals. Let sn be
the ﬁrst stage such that μ(Vmn)[sn] −ν(Umn)[sn] ≤2−n. Let an = μ(Vmn)[sn].
Then an is uniformly ρ-computable δA-relative to μ. Note that μ(Umn)[sn] ≤
μ(Umn) ≤μ(Vmn) ≤μ(Vmn)[sn] = an. It follows that an−μ(Umn) ≤2−n. Hence
(Umn, an) is a μ-approximation.
For a general open set U = 0
k ν(wk), construct U k
m and V k
m for each k. Let
Um = 0m
k=1 U k
m and Vm = 0m
k=1 V k
m. Then Um ⊆Vm ⊆U and Um ↑U. By a
similar argument with above, we have a μ-approximation of U.
⊓⊔
3.2
Existence of an Optimal Test
Deﬁnition 5. Let μ be a computable measure over X. A μ-test is a sequence
{Vn} of uniformly θ-computable sets with μ(Vn) ≤2−n. A μ-test {Wn} is optimal
if, for each μ-test {Vn}, there exists c such that Vn+c ⊆Wn for each n.
Theorem 3. Let X be a SCT3 space and μ be a computable measure on X.
Then there exists an optimal μ-test over X.
Proof. There exists a computable enumeration Vm,n of θ-computable sets. For
each Vm,n, there exists a μ-approximation (Us, as) by Lemma 1, which is uni-
formly (0 νfs, ρ)-computable. Let <Vm,n = Usup{s:as<2−n}. Then
μ(<Vm,n) = μ(Usup{s:as<2−n}) = sup{as : as < 2−n} ≤2−n.
Hence {<Vm,n}n is a μ-test for each m.
Suppose μ(Vm,n) < 2−n. Then there exists s0 such that μ(Vm,n) + 2−s < 2−n
for all s ≥s0. For such s we have as ≤μ(Us) + 2−s ≤μ(Vm,n) + 2−s < 2−n.
Hence <Vm,n = Vm,n.
Let Wn = 0
m <Vm,n+m+1. We claim that {Wn} is an optimal μ-test. Note that
μ(Wn) ≤
m 2−n−m−1 ≤2−n. Hence {Wn} is a μ-test.

An optimal superfarthingale and Its convergence
279
Let An be a μ-test. Then {Bn}n = {An+1}n is also a μ-test with μ(Bn) ≤
2−n−1. Hence there exists m such that Bn = Vm,n. It follows that Wn ⊇
Vm,n+m+1 = Bn+m+1 ⊇An+m+2 for each n. Then {Wn} is optimal.
⊓⊔
4
Optimal Integral Test
The goal in this section is to establish the existence of an optimal integral test
over a SCT3 space. The key of the proof is Lemma 2. As in Section 3, this lemma
is also the key to proving Theorem 5 in the next section.
4.1
Computable Bound
In the previous section we gave an approximation of an open set. With this we
give an approximation of a (δ, ρ<)-continuous function.
Lemma 2. Let f : X →R
+ be a total function and c be a ρ-computable real.
Then there exists a ([δ →ρ<], δA, [δ →ρ<])-computable function bc such that
(i)
:
bc(f, μ)dμ ≤c,
(ii) : fdμ < c ⇒bc(f, μ) = f.
Proof. Let Q+ be the set of all non-negative rationals. Let Qn = {qi : i ≤n}
where {qi} is a computable enumeration of Q+. Then Qn ↑Q+. Let s(n, i) be
the index of min{r ∈Qn : r > qi}. If qi is the maximum element in Qn and the
set is empty, then s(n, i) = ∞. Intuitively qsn(q) is the least element in Qn larger
than q or ∞if such an element does not exist. Then
μ(f) =

X
fdμ = lim
n→∞

i≤n
qi · μ({x : f(x) > qi}\{x : f(x) > qs(n,i)})
where q∞= ∞.
Let U i = {x : f(x) > qi}. Note that U i is a θ-computable open set. Then it
has a μ-approximation (U i
n, ai
n) satisfying the condition in Lemma 1. Note that
there exists a computable function b : N →N such that ∞
i=0 qi · 2−b(i) ≤1. Let
V i
n = U i
n+b(i)\U s(n,i)
n+b(s(n,i)).
Then μ(V i
n) →μ(U i\U s(n,i)). Note that μ(f) ≤n
i=0 qi · μ(V i
n) for each n and
μ(f) = lim
n→∞
n

i=0
qi · μ(U i
n+b(i)\U s(n,i)
n+b(s(n,i))) = lim
n→∞
n

i=0
qi · μ(V i
n).
Let c(n, i) = ai
n+b(i) −as(n,i)
n+b(s(n,i)). Then
ai
n+b(i) −2−(n+b(i)) ≤μ(U i
n+b(i)) ≤ai
n+b(i)

280
K. Miyabe
for each n and i. It follows that
ai
n+b(i) −as(n,i)
n+b(s(n,i)) −2−(n+b(i)) ≤μ(U i
n+b(i)\U sn(q)
n+b(s(n,i))),
μ(U i
n+b(i)\U sn(q)
n+b(s(n,i))) ≤ai
n+b(i) −as(n,i)
n+b(s(n,i)) + 2−(n+b(s(n,i)).
Hence
c(n, i) −2−(n+b(i)) ≤μ(V i
n) ≤c(n, i) + 2−(n+b(s(n,i))).
We construct b = b(f, μ) as the limit of functions bn. We deﬁne bn inductively
on n. Let b0(x) = 0 for all x. If

i≤n
qi · (c(n, i) + 2−(n+b(s(n,i)))) < c,
then let bn(x) = max{qi : x ∈U i
n+b(i)}, otherwise bn = bn−1.
Since U i
n is increasing on n, bn(x) ≥bn−1(x) for all n and x. Note that

bndμ =

i≤n
qi · μ(V i
n) ≤

i≤n
qi · (c(n, i) + 2−(n+b(s(n,i))) < c.
It follows that
:
bdμ =
:
limn bndμ = limn
:
bndμ ≤c.
Suppose : fdμ < c. Then : fdμ + 2−n+1 < c for a large n. Then for such n,
c >

i≤n
qi · μ(V i
n) + 2−n+1 ≥

i≤n
qi · c(n, i) −2−n + 2−n+1.
Combining the previous two results, we obtain

i≤n
qi · (c(n, i) + 2−(n+b(s(n,i))) ≤

i≤n
qi · c(n, i) + 2−n < c.
It follows that b(x) = limn bn(x) = sup{q : f(x) > q} = f(x) for all x.
⊓⊔
4.2
Computable Enumeration
Now it suﬃces to show that we have a computable enumeration of (δX, ρ<)-
computable functions.
Lemma 3. There is a computable enumeration of (δX, ρ<)-computable func-
tions fn : X →R
+.
To prove this, we use the following representation.
Deﬁnition 6 (−→δ 4-representation; see [24]). Deﬁne a multi-representation
−→
δ4 of the set CP(X1, X2) of all partial continuous functions f :⊆X1 →X2 as
follows:
f ∈−→
δ4(p) ⇐⇒

(w ≪p ⇒(∃u ∈dom(ν1), v ∈dom(ν2))w = ⟨u, v⟩)
and f −1[ν2(v)] = 0
⟨u,v⟩≪p ν1(u) ∩dom(f).

An optimal superfarthingale and Its convergence
281
Proof (Proof of Lemma 3). Let p be a −→
δ4-representation of a partial continuous
function f :⊆X →R
+. Then there exists only one total continuous function
f ′ : X →R
+ in −→
δ4(p). Hence −→
δ4 is also a representation of the set C(X, R
+)
of all total continuous functions. The function represented by p is determined
by the set {w ∈Σ∗: w ≪p}. Then there is a computable enumeration of all
computable −→
δ4-representations.
⊓⊔
4.3
The Existence of an Optimal Integral Test
Deﬁnition 7. A μ-integral test on X is a (δ, ρ<)-computable function f : X →
R
+ δA-relative to μ such that μ(f) ≤1. A μ-integral test f0 is optimal if, for
each μ-integral test f, there exists c such that f(x) ≤c · f0(x) for all x ∈X.
Theorem 4. Let X be a SCT3 space. For an arbitrary measure μ on X, there
exists an optimal μ-integral test. Furthermore we can construct an optimal μ-
integral test uniformly from δA-representation of μ.
Proof. By Lemma 3 there exists a computable enumeration fn : X →R
+ of
(δX, ρ<)-computable functions. Let f = ∞
n=0 2−n−1b1(fn, μ) where b1 is in
Lemma 2. Then : b1(fn, μ)dμ ≤1 and : fdμ ≤1. Hence f is an integral test.
If
:
fndμ < 1, then b1(fn, μ) = fn and fn(x) ≤2n · f(x) for all x ∈X.
Suppose
:
fndμ = 1. Then there exists m such that fn(x) = 2fm(x) for all x.
It follows that
:
fmdμ = 1/2 and fn(x) = 2fm(x) ≤2m+1 · f(x) for all x ∈X.
Hence f is optimal.
⊓⊔
5
Optimal Superfarthingale
5.1
Eﬀectivization of Game-Theoretic Probability
In the following we often use a ([δA, [δ →ρ<], δ]∗, δA, δ, ρ<)-computable super-
farthingale. For simplicity we call it a c.e. superfarthingale. Let V be the class
of all non-negative c.e. superfarthingales V with V (Λ) = 1.
Deﬁnition 8. A non-negative c.e. superfarthingale V is optimal if, for any
superfarthingale V ′ ∈V, there exists a constant c such that, for any σ ∈Π⋄,
cV (σ) ≥V ′(σ).
Theorem 5. If X is a SCT3 space, then there is an optimal superfarthingale.
Deﬁnition 9. A superfarthingale V is strict if the equation (2) in Deﬁnition 1
holds as a strict inequality (“>”) for all n.
Lemma 4. For a non-negative superfarthingale V , there exists a strict super-
farthingale V ′ such that 2V ′ ≥V .

282
K. Miyabe
Proof. Let V ′(πn) =
2n
2n+1−1 · V (πn). Then 2V ′ > V and

X
V ′(πn−1, pn, x)dpn =
:
X
2n
2n+1−1 · V (πn−1, pn, x)dpn
≤
2n
2n+1−1 · V (πn−1)
=
2n
2n+1−1 · 2n−1
2n−1 × 2n−1
2n−1 · V (πn−1)
=
22n−2n
22n−2n−1 × V ′(πn−1) < V ′(πn−1).
⊓⊔
Lemma 5. There exists a computable enumeration {Vm} ⊂V such that each
non-negative c.e. strict superfarthingale with V (Λ)
=
1 appears in the
enumeration.
Proof. Let V ′(πn) = supq<V ′(πn−1) bq(V (πn), pn) for all n ≥1 and V ′(Λ) = 1.
Then
:
X V ′(πn−1, pn, x)dpn ≤V ′(πn−1). Hence V ′ ∈V.
Suppose that V is strict. We prove V ′(πn) = V (πn) inductively on n. Suppose
that V ′(πn−1) = V (πn−1). Then :
X V (πn−1, pn, x)dpn < q ⇒bq(V (πn), pn) =
V (πn). Hence
:
X V (πn−1, pn, x)dpn < V ′(πn−1) ⇒V ′(πn) = V (πn). By the
strictness of V and the assumption, V ′(πn) = V (πn).
⊓⊔
Proof (Proof of Theorem 5). Let Vn be a computable enumeratation of strict
superfarthingales. Let V = 2−n−1Vn. We prove that V is optimal. Let V ′ be a
superfarthingale. Then by Lemma 4 and Lemma 5 there exists n and c such that
cVn ≥V ′. Then c · 2nV ≥c · 2n2−nVn ≥V ′.
⊓⊔
5.2
Convergence to a Measure
We will prove a generalized version of Theorem 1. Let νn be the measures induced
by the optimal superfarthingale V , that is, νn(A) =
:
A V (πn−1, pn, x)dpn where
V (πn) =
V (πn)
V (πn−1). We discuss what is the limit of the measures νn(A).
Fix π = (p1, y1, p2, y2, . . .) ∈Π. Let W be a non-negative c.e. farthingale and
μn be the measures induced by W. The Hellinger distance is deﬁned by
hn =

X
,
V (πn−1, pn, x) −

W(πn−1, pn, x)
-2
dpn
Let Nn =
:
X

V (πn)W(πn)dpn. Then Nn ≤1 −hn
2 ≤exp(−hn
2 ). The ﬁrst
inequality follows from hn =
:
X V (πn)dpn +
:
X W(πn)dpn −2Nn ≤2 −2Nn.
The second inequality follows from 1 −x ≤e−x. Then Nn exp( hn
2 ) ≤1.
There exists a superfarthingale Y such that
Y (μ1, y1, μ2, y2, . . .) =
=
V (πn)
W(πn) · exp(1
2
n

i=i
hi),

An optimal superfarthingale and Its convergence
283
because
:
X Y (πn)dμn
Y (πn−1)
=
 =
V (πn)
W(πn)dμn exp(hn
2 )
=
 
V (πn)W(πn)dpn exp(hn
2 ) = Nn exp(hn
2 ) ≤1.
Theorem 6. If supn Y < ∞then 
n hn < ∞. In particular supn Y < ∞
implies hn →0 as n →∞.
Proof. By optimality of V there exists c > 0 such that V (πn) ≥c·W(πn) for all
π. If ∞
i=1 hi = ∞, then V (πn)/W(πn) →0, this is a contradiction. Also recall
hi is non-negative.
⊓⊔
Hence we conclude that hn →0 as n →∞almost surely. By Theorem 2, Y
is not c.e. in general. This is because hi is not. To obtain the convergence on a
random sequence, we need a stronger randomness notion such as 2-randomness
or diﬀerence randomness [4].
Acknowledgement. The author thanks Akimichi Takemura for encourage-
ments and comments. This work was partly supported by GCOE, Kyoto
University.
References
1. Dawid, A.P.: Statistical theory: the prequential approach (with discussion). Journal
of the Royal Statistical Society A 147, 278–292 (1984)
2. Dawid,
A.P., Vovk, V.: Prequential probability: principles and
properties.
Bernoulli 5, 125–162 (1999)
3. Downey, R., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity. Springer,
Berlin (2010)
4. Franklin, J.N.Y., Ng, K.M.: Diﬀerence randomness. Proc. Amer. Math. Soc. 139,
345–360 (2011)
5. G´acs, P.: Uniform test of algorithmic randomness over a general space. Theoretical
Computer Science 341, 91–137 (2005)
6. Grubba, T., Schr¨oder, M., Weihrauch, K.: Computable metrization. Mathematical
Logic Quaterly 53(4-5), 381–395 (2007)
7. Hoyrup, M., Rojas, C.: An Application of Martin-L¨of Randomness to Eﬀective
Probability Theory. In: Ambos-Spies, K., L¨owe, B., Merkle, W. (eds.) CiE 2009.
LNCS, vol. 5635, pp. 260–269. Springer, Heidelberg (2009)
8. Hoyrup, M., Rojas, C.: Computability of probability measures and Martin-L¨of
randomness over metric spaces. Information and Computation 207(7), 830–847
(2009)
9. Hutter, M.: Universal artiﬁcial intelligence: Sequential decisions based on algorith-
mic probability. Springer (2005)
10. Hutter, M., Muchnik, A.: On semimeasures predicting Martin-L¨of random se-
quences. Theoretical Computer Science 382, 247–261 (2007)

284
K. Miyabe
11. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its applications,
3rd edn. Graduate Texts in Computer Science. Springer, New York (2009)
12. Miyabe, K.: Algorithmic randomness over general spaces (submitted)
13. Nies, A.: Computability and Randomness. Oxford University Press, USA (2009)
14. Odifreddi, P.: Classical Recursion Theory, vol. 1. North-Holland (1990)
15. Odifreddi, P.: Classical Recursion Theory, vol. 2. North-Holland (1999)
16. Schr¨oder, M.: Admissible Representations for Probability Measures. Mathematical
Logic Quarterly 53(4-5), 431–445 (2007)
17. Shafer, G., Vovk, V.: Probability and Finance: It’s Only a Game! Wiley (2001)
18. Soare, R.I.: Recursively enumerable sets and degrees. Perspectives in Mathematical
Logic. Springer, Berlin (1987)
19. Solomonoﬀ, R.J.: A formal theory of inductive inference I, II. Information and
Control 7, 1–22, 224–254 (1964)
20. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Transaction on Information Theory IT 24, 422–432 (1978)
21. Vovk, V., Shen, A.: Prequential randomness and probability. Theoretical Computer
Science 411, 2632–2646 (2010)
22. Weihrauch, K.: Computable Analysis: an introduction. Springer, Berlin (2000)
23. Weihrauch, K., Grubba, T.: Elementary Computable Topology. Journal of Univer-
sal Computer Science 15(6), 1381–1422 (2009)
24. Weihrauch, K.: Computable Separation in Topology, from T0 to T3. In: CCA (2009)

Diverse Consequences of Algorithmic Probability
Eray ¨Ozkural
Computer Engineering Department, Bilkent University, Ankara, Turkey
Abstract. We reminisce and discuss applications of algorithmic prob-
ability to a wide range of problems in artiﬁcial intelligence, philosophy
and technological society. We propose that Solomonoﬀhas eﬀectively ax-
iomatized the ﬁeld of artiﬁcial intelligence, therefore establishing it as a
rigorous scientiﬁc discipline. We also relate to our own work in incremen-
tal machine learning and philosophy of complexity.
1
Introduction
Ray Solomonoﬀwas a pioneer in mathematical Artiﬁcial Intelligence (AI), whose
proposal of Algorithmic Probability (ALP) has led to diverse theoretical conse-
quences and applications, most notably in AI. In this paper, we try to give a
sense of the signiﬁcance of his theoretical contributions, reviewing the essence
of his proposal in an accessible way, and recounting a few, seemingly unrelated,
diverse consequences which, in our opinion, hint towards a philosophically clear
world-view that has rarely been acknowledged by the greater scientiﬁc commu-
nity. That is to say, we try to give the reader a glimpse of what it is like to
consider the consequences of ALP, and what ideas might lie behind the theoret-
ical model, as we imagine them.
Let M be a reference machine which corresponds to a universal computer1
with a preﬁx-free code. In a preﬁx-free code, no code is a preﬁx of another. This
is also called a self-delimiting code, as most reasonable computer programming
languages are. Solomonoﬀinquired the probability that an output string x is
generated by M considering the whole space of possible programs. By giving
each program bitstring p an a priori probability of 2−|p|, we can ensure that
the space of programs meets the probability axioms (by the extended Kraft
inequality [2]). In other words, we imagine that we toss a fair coin to generate
each bit of a random program. This probability model of programs entails the
following probability mass function (p.m.f.) for strings x ∈{0, 1}∗:
PM(x) =

M(p)=x∗
2−|p|
(1)
which is the probability that a random program will output a preﬁx of x. PM(x)
is called the algorithmic probability of x for it assumes the deﬁnition of program
based probability. We use P when M is clear from the context to avoid clutter.
1 Optionally, it can be probabilistic to deal with general induction problems, i.e., it
has access to a random number generator [1, Section 4].
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 285–298, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

286
E. ¨Ozkural
2
SolomonoﬀInduction
Using this probability model of bitstrings, one can make predictions. Intuitively,
we can state that it is impossible to imagine intelligence in the absence of any
prediction ability: purely random behavior is decisively non-intelligent. Since, P
is a universal probability model, it can be used as the basis of universal predic-
tion, and thus intelligence. Perhaps, Solomonoﬀ’s most signiﬁcant contributions
were in the ﬁeld of AI, as he envisioned a machine that can learn anything from
scratch. Reviewing his early papers such as [3,4], we see that he has established
the theoretical justiﬁcation for machine learning and data mining ﬁelds. Few
researchers could ably make claims about universal intelligence as he did. Unfor-
tunately, not all of his ideas have reached fruition in practice; yet there is little
doubt that his approach was the correct basis for a science of intelligence.
His main proposal for machine learning is inductive inference [5,6] circa 1964,
for a variety of problems such as sequence prediction, set induction, operator
induction and grammar induction [7]. Without much loss of generality, we can
discuss sequence prediction on bitstrings. Assume that there is a computable
p.m.f. of bitstrings P1. Given a bitstring x drawn from P1, we can deﬁne the
conditional probability of the next bit simply by normalizing (1) [7]. Algorithmi-
cally, we would have to approximate (1) by ﬁnding short programs that generate
x (the shortest of which is the most probable). In more general induction, we
run all models in parallel, quantifying ﬁt-to-data, weighed by the algorithmic
probability of the model, to ﬁnd the best models and construct distributions [7];
the common point being determining good models with high a priori probability.
Finding the shortest program in general is undecidable, however, Levin search [8]
can be used for this purpose. There are two important results about Solomonoﬀ
induction that we shall mention here. First, Solomonoﬀinduction converges very
rapidly to the real probability distribution. The convergence theorem shows that
the expected total square error is related only to the algorithmic complexity of
P1, which is independent from x. The following bound [9] is discussed at length
in [10] with a concise proof:
EP

n

m=1
(P(am+1 = 1|a1a2...am) −P1(am+1 = 1|a1a2...am))2

≤−1
2 ln P(P1))
(2)
This bound characterizes the divergence of the ALP solution from the real prob-
ability distribution P1. P(P1) is the a priori probability of P1 p.m.f. according
to our universal distribution PM. On the right hand side of (2), −ln PM(P1)
is roughly k ln 2 where k is the Kolmogorov complexity of P1 (the length of
the shortest program that deﬁnes it), thus the total expected error is bounded
by a constant, which guarantees that the error decreases very rapidly as exam-
ple size increases. Secondly, there is an optimal search algorithm to approximate
Solomonoﬀinduction, which adopts Levin’s universal search method to solve the
problem of universal induction [8,11]. Universal search procedure time-shares all
candidate programs according to their a priori probability with a clever watch-
dog policy to avoid the practical impact of the undecidability of the halting

Consequences of ALP
287
problem [11]. The search procedure starts with a time limit t = t0, in its it-
eration tries all candidate programs c with a time limit of t.P(c), and while a
solution is not found, it doubles the time limit t. The time t(s)/P(s) for a solu-
tion program s taking time t(s) is called the Conceptual Jump Size (CJS), and
it is easily shown that Levin Search terminates in at most 2.CJS time. To obtain
alternative solutions, one may keep running after the ﬁrst solution is found, as
there may be more probable solutions that need more time. The optimal solution
is computable only in the limit, which turns out to be a desirable property of
Solomonoﬀinduction, as it is complete and uncomputable [12, Section 2]. An ex-
planation of Levin’s universal search procedure and its application to Solomonoﬀ
induction may be found in [8,11,13].
3
The Axiomatization of Artiﬁcial Intelligence
We believe in fact that Solomonoﬀ’s work was seminal in that he has single-
handedly axiomatized AI, discovering the minimal necessary conditions for any
machine to attain general intelligence (based on our interpretation of [1]).
Informally, these axioms are:
AI0. AI must have in its possession a universal computer M (Universality).
AI1. AI must be able to learn any solution expressed in M’s code (Learning
recursive solutions).
AI2. AI must use probabilistic prediction (Bayes’ theorem).
AI3. AI must embody in its learning a principle of induction (Occam’s razor).
While it may be possible to give a more compact characterization, these are
ultimately what is necessary for the kind of general learning that Solomonoﬀ
induction achieves. ALP can be seen as a complete formalization of Occam’s
razor (as well as Epicurus’s principle) [14] and thus serve as the foundation
of universal induction, capable of solving all AI problems of signiﬁcance. The
axioms are important because they allow us to assess whether a system is capable
of general intelligence or not.
Obviously, AI1 entails AI0, therefore AI0 is redundant, and can be omitted
entirely, however we stated it separately only for historical reasons, as one of the
landmarks of early AI research, in retrospect, was the invention of the universal
computer, which goes back to Leibniz’s idea of a universal language (character-
istica universalis) that can express every statement in science and mathematics,
and has found its perfect embodiment in Turing’s research [15,16]. A related
achievement of early AI was the development of LISP, a universal computer
based on lambda calculus (which is a functional model of computation) that has
shaped much of early AI research.
See also a recent survey about inductive inference [17] with a focus on Mini-
mum Message Length (MML) principle introduced in 1968 [18]. MML principle
is also a formalization of induction developed within the framework of classi-
cal information theory, which establishes a trade-oﬀbetween model complexity
and ﬁt-to-data by ﬁnding the minimal message that encodes both the model

288
E. ¨Ozkural
and the data [19]. This trade-oﬀis quite similar to the earlier forms of induc-
tion that Solomonoﬀdeveloped, however independently discovered. Dowe points
out that Occam’s razor means choosing the simplest single theory when data is
equally matched, which MML formalizes perfectly (and is functional otherwise
in the case of inequal ﬁts) while Solomonoﬀinduction maintains a mixture of
alternative solutions [17, Sections 2.4 & 4]. On the other hand, the diversity of
solutions in ALP is seen as desirable by Solomonoﬀhimself [12], and in a recent
philosophical paper which illustrates how Solomonoﬀinduction dissolves various
philosophical objections to induction [14]. Nevertheless, it is well worth men-
tioning that Solomonoﬀinduction (formal theory published in 1964 [5,6]), MML
(1968), and Minimum Description Length [20] formalizations, as well as Statis-
tical Learning Theory [21] (initially developed in 1960), all provide a principle
of induction (AI3). However, it was Solomonoﬀwho ﬁrst observed the impor-
tance of universality for AI (AI0-AI1). The plurality of probabilistic approaches
to induction supports the importance of AI3 (as well as hinting that diversity
of solutions may be useful). AI2, however, does not require much explanation.
Some objections to Bayesianism are answered using MML in [22]. Please also see
an intruging paper by Wallace and Dowe [23] on the relation between MML and
Kolmogorov complexity, which states that Solomonoﬀinduction is tailored to
prediction rather than inference, and recommends non-universal models in prac-
tical work, therefore becomes incompatible with the AI axioms (AI0-AI1). Ulti-
mately, empirical work will illuminate whether our AI axioms should be adopted,
or more restrictive models are suﬃcient for universal intelligence; therefore such
alternative viewpoints must be considered. In addition to this, Dowe discusses
the relation between inductive inference and intelligence, and the requirements
of intelligence as we do elsewhere [17, Section 7.3]. Also relevant is an adaptive
universal intelligence test that aims to measure the intelligence of any AI agent,
and discusses various deﬁnitions of intelligence [24].
4
Incremental Machine Learning
In solving a problem of induction, the aforementioned search methods suﬀer from
the huge computational complexity of trying to compress the entire input. For
instance, if the complexity of the p.m.f. P1 is about 400 bits, Levin search would
take on the order of 2400 times the running time of the solution program, which is
infeasible (quite impossible in the observed universe). Therefore, Solomonoﬀhas
suggested using an incremental machine learning algorithm, which can re-use
information found in previous solutions [13].
The following argument illustrates the situation more clearly. Let P1 and P2
be the p.m.f.’s corresponding to a training sequence of two induction problems
(any of them, not necessarily sequence prediction, to which others can be reduced
easily) with data < d1, d2 >. Assume that the ﬁrst problem has been solved
(correctly) with universal search. It has taken at most 2.CJS1 = 2.t(s1)/P(s1)
time. If the second problem is solved in an incremental fashion, making use of
the information from P1, then the running time of discovering a solution s2 for

Consequences of ALP
289
d2 reduces, depending on the success of information transfer across problems.
Here, we quantify how much in familiar probabilistic terms.
In [10], Solomonoﬀdescribes an information theoretic interpretation of ALP,
which suggests the following entropy function:
H∗(x) = −log2 P(x)
(3)
This entropy function has perfect sub-additivity of information according to the
corresponding conditional entropy deﬁnition:
P(y|x) = P(x, y)
P(x)
(4)
H∗(y|x) = −log2 P(y|x)
(5)
H∗(x, y) = H∗(x) + H∗(y|x)
(6)
This deﬁnition of entropy thus does not suﬀer from the additive constant terms
as in Chaitin’s version. We can instantly deﬁne mutual entropy:
H∗(x : y) = H∗(x) + H∗(y) −H∗(x, y) = H∗(y) −H∗(y|x)
(7)
which trivially follows.
A KUSP machine is a universal computer that can store data and methods
in additional storage. In 1984, Solomonoﬀobserved that KUSP machines are
especially suitable for incremental learning [11]. In our work [25] we found that,
the incremental learning approach was indeed useful (as in the preceding OOPS
algorithm[26]). Here is how we interpreted incremental learning. After each in-
duction problem, the p.m.f. P is updated, thus for every new problem a new
probability distribution is obtained. Although we are using the same M reference
machine for trial programs, we are referring to implicit KUSP machines which
store information about the experience of the machine so far, in subsequent prob-
lems. In our example of two induction problems, let the updated P be called P ′,
naturally there will be an update procedure which takes time tu(P, s1). Just how
much time can we expect to save if we use incremental learning instead of inde-
pendent learning? First, let us write the time bound 2.t(s)/P(s) as t(s).2H∗(s)+1.
If s1 and s2 are not algorithmically independent, then H∗(s2|s1) is smaller than
H∗(s2). Independently, we would have t(s1).2H∗(s1)+1+t(s2).2H∗(s2)+1, together,
we will have, in the best case t(s1).2H∗(s1)+1 + t(s2).2H∗(s2|s1)+1 for the search
time, assuming that recalling s1 takes no time for the latter search task (which is
an unrealistic assumption). Therefore in total, the latter search task can acceler-
ate 2H∗(s1:s2) times, and we can save t(s2).2H∗(s2)+1(1 −2−H∗(s1:s2)) −tu(P, s1)
total time in the best case (only an upper bound since we did not account for
recall time). Note that the maximum temporal gain is related to both how much
mutual information is discovered across solutions (thus Pi’s), and how much
time the update procedure takes. Clearly, if the update time dominates overall,
incremental learning is in vain. However, if updates are eﬀective and eﬃcient,
there is enormous potential in incremental machine learning.

290
E. ¨Ozkural
During the experimental tests of our Stochastic Context Free Grammar based
search and update algorithms [25], we have observed that in practice we can re-
alize fast updates, and we can still achieve actual code re-use and tremendous
speed-up. Using only 0.5 teraﬂop/sec of computing speed and a reference ma-
chine choice of R5RS Scheme [27], we solved 6 simple deterministic operator
induction problems in 245.1 seconds. This running time is compared to 7150
seconds without any updates. Scaled to human-level processing speed of 100 ter-
aﬂop/sec, our system would learn and solve the entire training sequence in 1.25
seconds, which is (arguably) better than most human students. In one particu-
lar operator induction problem (fourth power, x4), we saw actual code re-use:
(define (pow4 x ) (define (sqr x ) (* x x)) (sqr (sqr x ) )), and an actual
speedup of 272. The gains that we saw conﬁrmed the incremental learning pro-
posals of Solomonoﬀ, mentioned in a good number of his publications, but most
clearly in [11,13,1]. Based on our work and the huge speedup observed in OOPS
for a shorter training sequence [26], we have come to believe that incremental
learning has the epistemological status of an additional AI axiom:
AI4. AI must be able to use its previous experience to speed up subsequent
prediction tasks (Transfer Learning).
This axiom is justiﬁed by observing that many universal induction problems
are completely unsolvable by a system that does not have the adequate sort of
algorithmic memory, regardless of the search method.
The results above may be contrasted with inductive programming approaches,
since we predicted deterministic functions. One of the earliest and most success-
ful inductive programming systems is ADATE, which is optimized for a more
speciﬁc purpose. ADATE system has yielded impressive results in an ML variant
by user supplied primitives and constraining candidate programs [28]. Universal
representations have been investigated in inductive logic programming as well
[29], however U-learning unfortunately lacks the extremely accurate generaliza-
tion of Solomonoﬀinduction. It has been shown that incremental learning is
useful in the inductive programming framework [30], which supports our obser-
vation of the necessity of incremental machine learning. Another relevant work
is a typed higher-order logic knowledge representation scheme based on term
representation of individuals and a rich representation language encompassing
many abstract data types [31]. A recent survey on inductive programming may
be found in [32].
We should also account our brief correspondence with Solomonoﬀ. We ex-
pressed that the prediction algorithms were powerful but it seemed that mem-
ory was not used suﬃciently. Solomonoﬀresponded by mentioning the potential
stochastic grammar and genetic programming approaches that he was working
on at the time. Our present research was motivated by a problem he posed
during the discussions of his seminars in Turing Days ’06 at Bilgi University,
Istanbul: “We can use grammar induction for updating a stochastic context free
grammar, but there is a problem. We already know the grammar of the refer-
ence machine.”. We designed our incremental learning algorithms to address this

Consequences of ALP
291
particular problem2. Solomonoﬀhas also guided our research by making a valu-
able suggestion, that it is more important to show whether incremental learning
works over a sequence of simpler problems than solving a diﬃcult problem. We
have in addition investigated the use of PPM family of compressors following his
proposal, but as we expected, they were not suﬃcient for guiding LISP-like pro-
grams, and would require too many changes. Therefore, we proceeded directly to
the simplest kind of guiding p.m.f. that would work for Scheme, as we preferred
not to work on assembly-like languages for which PPM might be appropriate,
since, in our opinion, high-level languages embody more technological progress
(see also [33] which employs a Scheme subset). Colorfully speaking, inventing
a functional form in assembly might be like re-inventing the wheel. However, in
general, it would not be trivial for the induction system to invent syntax forms
that compare favorably to LISP, especially during preliminary training. There-
fore, much intelligence is already present in a high-level universal computer (AI0)
which we simply take advantage of.
5
Cognitive Architecture
Another important discussion is whether a cognitive architecture is necessary.
The axiomatic approach was seen counter-productive by some leading researchers
in the past. However, we think that their opinion can be expressed as follows:
the minimal program that realizes these axioms is not automatically intelligent,
because in practice an intelligent system requires a good deal of algorithmic in-
formation to take oﬀthe ground. This is not a bad argument, since obviously,
the human brain is well equipped genetically. However, we cannot either rule out
that a somewhat compact system may achieve human-level general intelligence.
The question therefore, is whether a simply described system like AIXI [34]
(an extension of Solomonoﬀinduction to reinforcement learning) is suﬃcient in
practice, or there is a need for a modular/extensible cognitive architecture that
has been designed in particular ways to promote certain kinds of mental growth
and operation. Some proponents of general purpose AI research think that such
a cognitive architecture is necessary, e.g., OpenCog [35]. Schmidhuber has sug-
gested the famous G¨odel Machine which has a mechanical model of machine
consciousness [36]. Solomonoﬀhimself has proposed early on in 2002, the design
of Alpha, a generic AI architecture which can ultimately solve free-form time-
limited optimization problems [13]. Although in his later works, Solomonoﬀhas
not made much mention of Alpha and has instead focused on the particulars
of the required basic induction and learning capability, nonetheless his proposal
remains as one of the most extensible and elegant self-improving AI designs.
2 We occassionally corresponded via e-mail. Before the AGI-10 conference, he had
reviewed a draft of my paper, and he had commented that the “learning program-
ming idioms” and “frequent subprogram mining” algorithms were interesting, which
was all the encouragement I needed. The last e-mail I received from him was on
11/Oct/2009. I regretfully learnt that he passed away a month later. His indepen-
dent character and true scientiﬁc spirit will always be a shining beacon for me.

292
E. ¨Ozkural
Therefore, this point is open to debate, though some researchers may want to
assume another, entirely optional, axiom:
AI5. AI must be arranged such that self-improvement is feasible in a realistic
mode of operation (Cognitive Architecture).
It is doubtful for instance whether a combination of incremental learning and
AIXI will result in a practical reinforcement learning agent. Neither is it well
understood whether autonomous systems with built-in utility/goal functions are
suitable for all practical purposes. We anticipate that such questions will be set-
tled by experimenters, as the complexity of interesting experiments will quickly
overtake theoretical analysis.
We do not consider human-like behavior, or a robotic body, or an autonomous
AI design, such as a goal-driven or reinforcement-learning agent, essential to
intelligence, hence we did not propose autonomy or embodiment as an axiom.
Solomonoﬀhas commented likewise on the preferred target applications [37]:
To start, I’d like to deﬁne the scope of my interest in A.I. I am not
particularly interested in simulating human behavior. I am interested in
creating a machine that can work very diﬃcult problems much better
and/or faster than humans can – and this machine should be embodied
in a technology to which Moore’s Law applies. I would like it to give a
better understanding of the relation of quantum mechanics to general
relativity. I would like it to discover cures for cancer and AIDS. I would
like it to ﬁnd some very good high temperature superconductors. I would
not be disappointed if it were unable to pass itself oﬀas a rock star.
6
Philosophical Foundation and Consequences
Solomonoﬀ’s AI theory is founded on a wealth of philosophy. Here, we shall
brieﬂy revisit the philosophical foundation of ALP and point out some of its
philosophical consequences. In his posthumous publication, Solomonoﬀmentions
the inspiration for some of his work: Carnap’s idea that the state of the world
can be represented by a ﬁnite bitstring (and that science predicts future bits
with inductive inference), Turing’s universal computer (AI0) as communicated
by Minsky and McCarthy, and Chomsky’s generative grammars [12]. The dis-
covery of ALP is described by Solomonoﬀin quite a bit of detail in [38], which
relates his discovery to the background of many prominent thinkers and con-
tributors. Carnap’s empiricism seems to have been a highly inﬂuential factor
in Solomonoﬀ’s research as he sought to ﬁnd how science is carried out, rather
than particular scientiﬁc ﬁndings; and ALP is a satisfactory solution to Carnap’s
program of inductive inference [14].
Let us then recall some philosophically relevant aspects of ALP discussed in
the most recent publications of Solomonoﬀ. First, the exact same method is
used to solve both mathematical and scientiﬁc problems. This means that there
is no fundamental epistemological diﬀerence between these problems; our inter-
pretation is that, this is well founded only when we observe that mathematical

Consequences of ALP
293
problems themselves are computational or linguistic problems, in practice math-
ematical problems can be reduced to particular computational problems, and
here is why the same method works for both kinds of problems. Mathematical
facts do not preside over or precede physical facts, they themselves are solutions
of physical problems ultimately (e.g., does this particular kind of machine halt
or not?). And the substance of mathematics, the lucid sort of mathematical lan-
guage and concepts that we have invented, can be fully explained by Solomonoﬀ
induction, as those are the kinds of useful programs, which have aided an intellect
in its training, and therefore are retained as linguistic and algorithmic informa-
tion. The subjectivity and diversity aspects of ALP [12, Sections 3 & 4] fully
explain why there can be multiple and almost equally productive foundations
of mathematics, as those merely point out somewhat equally useful formalisms
invented by diﬀerent mathematicians. There is absolutely nothing special about
ZFC theory, it is just a formal theory to explain some useful procedures that
we perform in our heads, i.e., it is more like the logical explanation of a set
module in a functional programming language than anything else, however, the
operations in a mathematician’s brain are not visible to their owner, thereby
leading to useless Platonist fantasies of some mathematicians owing to a dearth
of philosophical imagination. Therefore, it does not matter much whether one
prefers this or that formalization of set theory, or category theory as a foun-
dation, unless that choice restricts success in the solution of future scientiﬁc
problems. Since, such a problematic scientiﬁc situation does not seem to have
emerged yet (forcing us to choose among particular formalizations), the diver-
sity principle of ALP forces us to retain them all. That is to say, subscribing
to the ALP viewpoint has the unexpected consequence that we abandon both
Platonism and Formalism. There is a meaning in formal language, in the manner
which improves future predictions, however, there is not a single a priori fact,
in addition to empirical observations, and no such fact is ever needed to con-
duct empirical work, except a proper realization of axioms A1–A3 (and surely
no sane scientist would accept that there is a unique and empty set that exists
in a hidden order of reality). When we consider these axioms, we need to un-
derstand the universality of computation, and the principled manner in which
we have to employ it for reliable induction in our scientiﬁc inquiries. The only
physically relevant assumption is that of the computability of the distributions
which generate our empirical problems (regardless of whether the problem is
mathematical or scientiﬁc), and the choice of a universal computer which intro-
duces a necessary subjectivity. The computability aspect may be interpreted as
information ﬁnitism, all the problems that we can work with should have ﬁnite
entropy. Yet, this restriction on disorder is not at all limiting, for it is hardly
conceivable how one may wish to solve a problem of actually inﬁnite complexity.
Therefore, this is not much of an assumption for scientiﬁc inquiry, especially
given that both quantum mechanics and general relativity can be described in
computable mathematics (see for instance [39] about the applicability of com-
putable mathematics to quantum mechanics). And neither can one hope to ﬁnd

294
E. ¨Ozkural
an example of a single scientiﬁcally valid problem in any textbook of science
that requires the existence of distributions with inﬁnite complexity to solve.
With regards to general epistemology, ALP/AIT may be seen as largely in-
compatible with non-reductionism. Non-reductionism is quite misleading in the
manner it is usually conveyed. Instead, we must seek to understand irreducibil-
ity in the sense of AIT, of quantifying algorithmic information, which allows
us to reconcile the concept of irreducibility with physicalism (which we think
every empiricist should accept) [40]. In particular, we can partially formalize
the notion of knowledge by mutual information between the world and a brain.
Our paper proposed a physical solution to the problem of determining the most
“objective” universal computer: it is the universe itself. If digital physics were
true, this might be for instance a particular kind of graph automata, or if quan-
tum mechanics were the basis, then a universal quantum computer could be
used; however, for many tasks using such a low-level computer might be ex-
traordinarily diﬃcult. We also argued that extreme non-reductionism leads to
arguments from ignorance such as ontological dualism, and information theory
is much better suited to explaining evolution and the need for abstractions in
our language. It should also be obvious that the ALP solution to AI extends
the two main tenets of logical positivism, which are veriﬁcationism and uniﬁed
science, as it gives a ﬁnite cognitive procedure with which one can conduct all
empirical work, and allows us to develop a private language with which we can
describe all of science and mathematics. However, we should also mention that
this strengthened positivism does not require a strict analytic-synthetic distinc-
tion; a spectrum of analytic-synthetic distinction as in Quine’s philosophy seems
to be acceptable [41]. We have already seen that according to ALP, mathemat-
ical and scientiﬁc problems have no real distinction, therefore like Quine, ALP
would allow revising even mathematical logic itself, and we need not remind
that the concept of universal computer itself has not appeared out of thin air,
but has been invented due to the laborious mental work of scientists, as they ab-
stracted from the mechanics of performing mathematics; at the bottom these are
all empirical problems [42]. On the other hand, a “web of belief” as in Quine,
by no means suggests non-reductionism, for that could be true only if indeed
there were phenomena that had unscathable (inﬁnite) complexity, such as Tur-
ing oracle machines which were not proposed as physical machines, but only as
a hypothetical concept [16]. Quine himself was a physicalist; we do not think
that he would support the later vendetta against reductionism which may be
a misunderstanding of his holism. Though, it may be argued that his obscure
version of Platonism, which does not seem much scientiﬁc to us, may be the
culprit. Today’s Bayesian networks seem to be a good formalization of Quine’s
web of belief, and his instrumentalism is consistent with the ALP approach of
maintaining useful programs. Therefore, on this account, psychology ought to be
reducible to neurophysiology, as the concept of life to molecular biology, because
these are all ultimately sets of problems that overlap in the physical world, and
the relation between them cannot hold an inﬁnite amount of information; which
would require an inﬁnitely complex local environment, and that does not seem

Consequences of ALP
295
consistent with our scientiﬁc observations. That is to say, discovery of bridge
disciplines is possible as exempliﬁed by quantum chemistry and molecular bi-
ology, and it is not diﬀerent from any other kind of empirical work. Recently,
it has been perhaps better understood in the popular culture that creationism
and non-reductionism are almost synonymous (regarding the claims of “intelli-
gent design” that the ﬂagella of bacteria are too complex to have evolved). Note
that ALP has no qualms with the statistical behavior of quantum systems, as it
allows non-determinism. Moreover, the particular kind of irreducibility in AIT
corresponds to weak emergentism, and most certainly contradicts with strong
emergentism which implies supernatural events. Please see also [17, Section 7]
for a discussion of philosophical problems related to algorithmic complexity.
7
Intellectual Property towards Inﬁnity Point
Solomonoﬀhas proposed the inﬁnity point hypothesis, also known as the singu-
larity, as an exponentially accelerating technological progress caused by human-
level AI’s that complement the scientiﬁc community, to accelerate our progress
ad inﬁnitum within a ﬁnite, short time (in practice only a ﬁnite, but signiﬁcant
factor of improvement could be expected) in 1985 [43] (the ﬁrst paper on the
subject). Solomonoﬀhas proposed seven milestones of AI development: A: mod-
ern AI phase (1956 Dartmouth conference), B: general theory of problem solving
(our interpretation: SolomonoﬀInduction, Levin Search), C: self-improving AI
(our interpretation: Alpha architecture, 2002), D: AI that can understand En-
glish (our interpretation: not realized yet), E: human-level AI, F: an AI at the
level of entire computer science (CS) community, G: an AI many times smarter
than the entire CS community.
A weak condition for the inﬁnity point may be obtained by an economic ar-
gument, also covered in [43] brieﬂy. The human brain produces 5 teraﬂops/watt
roughly. The current incarnation of NVIDIA’s General Purpose Graphics Pro-
gramming Unit architectures called Fermi achieves about 6 gigaﬂops/watt [44].
Assuming 85% improvement in power eﬃciency per year (as seen in NVIDIA’s
projections), in 12 years, human-level energy eﬃciency of computing will be
achieved. After that date, even if mathematical AI fails due to an unforeseen
problem, we will be able to run our brain simulations faster than us, using less
energy than humans, eﬀectively creating a bio-information based AI which meets
the basic requirement of inﬁnity point. For this to occur, whole brain simulation
projects must be comprehensive in operation and eﬃcient enough [45]. Other-
wise, human-level AI’s that we will construct should match the computational
eﬃciency of the human brain. This weaker condition rests on an economic obser-
vation: the economic incentive of cheaper intellectual work will drive the prolif-
eration of personal use of brain simulations. According to NVIDIA’s projections,
thus, we can expect the necessary conditions for the inﬁnity point to materialize
by 2023, after which point technological progress may accelerate very rapidly.
According to a recent paper by Koomey, the energy eﬃciency of computing is
doubling every 1.5 years (about 60% per year), regardless of architecture, which
would set the date at 2026 [46].

296
E. ¨Ozkural
Assume that we are progressing towards the hypothetical inﬁnity point. Then,
the entire human civilization may be viewed as a global intelligence working on
technological problems. The practical necessity of incremental learning suggests
that when faced with more diﬃcult problems, better information sharing is re-
quired. If no information sharing is present between researchers (i.e., diﬀerent
search programs), then, they will lose time traversing overlapping program sub-
spaces. This is most clearly seen in the case of simultaneous inventions when an
idea is said to be “up in the air” and is invented by multiple, independent par-
ties on near dates. If intellectual property (IP) laws are too rigid and costly, this
would entail that there is minimal information sharing, and after some point, the
global eﬃciency of solving non-trivial technological problems would be severely
hampered. Therefore, to utilize the inﬁnity point eﬀects better, knowledge shar-
ing must be encouraged in the society. Maximum eﬃciency in this fashion can
be provided by free software licenses, and a reform of the patent system. Our
view is that no single company or organization can (or should) have a monopoly
on the knowledge resources to attack problems with truly large algorithmic com-
plexity (monopoly is mostly illegal presently at any rate). We tend to think that
sharing science and technology is the most eﬃcient path towards the inﬁnity
point. Naturally, free software philosophy is not acceptable to much commercial
enterprise, thus we suggest that as technology advances, the overhead of enforc-
ing IP laws are taken into account. If technology starts to advance much more
rapidly, the duration of the IP protection may be shortened, for instance, as
after the AI milestone F, the bureaucracy and restrictions of IP law may be a
serious bottleneck.
8
Conclusion
We have mentioned diverse consequences of ALP in axiomatization of AI, phi-
losophy, and technological society. We have also related our own research to
Solomonoﬀ’s proposals. We interpret ALP and AIT as a fundamentally new
world-view which allows us to bridge the gap between complex natural phe-
nomena and positive sciences more closely than ever. This paradigm shift has
resulted in various breakthrough applications and is likely to beneﬁt the society
in the foreseeable future.
Acknowledgements. We thank anonymous reviewers, David Dowe and Lau-
rent Orseau for their valuable comments, which substantially improved this
paper.
References
1. Solomonoﬀ, R.J.: Progress in incremental machine learning. Technical Report
IDSIA-16-03, IDSIA, Lugano, Switzerland (2003)
2. Chaitin, G.J.: A theory of program size formally identical to information theory.
J. ACM 22, 329–340 (1975)

Consequences of ALP
297
3. Solomonoﬀ, R.J.: An inductive inference machine. Dartmouth Summer Research
Project on Artiﬁcial Intelligence (1956) A Privately Circulated Report
4. Solomonoﬀ, R.J.: An inductive inference machine. In: IRE National Convention
Record, Section on Information Theory, Part 2, New York, USA, pp. 56–62 (1957)
5. Solomonoﬀ, R.J.: A formal theory of inductive inference, part i. Information and
Control 7(1), 1–22 (1964)
6. Solomonoﬀ, R.J.: A formal theory of inductive inference, part ii. Information and
Control 7(2), 224–254 (1964)
7. Solomonoﬀ, R.J.: Three kinds of probabilistic induction: Universal distributions
and convergence theorems. The Computer Journal 51(5), 566–570 (2008); Christo-
pher Stewart Wallace, memorial special issue (1933-2004)
8. Levin, L.A.: Universal sequential search problems. Problems of Information Trans-
mission 9(3), 265–266 (1973)
9. Solomonoﬀ, R.J.: Algorithmic probability: Theory and applications. In: Dehmer,
M., Emmert-Streib, F. (eds.) Information Theory and Statistical Learning,
pp. 1–23. Springer Science+Business Media, N.Y. (2009)
10. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Trans. on Information Theory IT-24(4), 422–432 (1978)
11. Solomonoﬀ, R.J.: Optimum sequential search. Technical report, Oxbridge Research,
Cambridge, Mass., USA (1984)
12. Solomonoﬀ, R.J.: Algorithmic Probability – Its Discovery – Its Properties and
Application to Strong AI. In: Randomness Through Computation: Some Answers,
More Questions, pp. 149–157. World Scientiﬁc Publishing Company (2011)
13. Solomonoﬀ, R.J.: A system for incremental learning based on algorithmic proba-
bility. In: Proceedings of the Sixth Israeli Conference on Artiﬁcial Intelligence, Tel
Aviv, Israel, pp. 515–527 (1989)
14. Rathmanner, S., Hutter, M.: A philosophical treatise of universal induction. En-
tropy 13(6), 1076–1136 (2011)
15. Davis, M.: The Universal Computer: The Road from Leibniz to Turing. W. W.
Norton & Company (2000)
16. Turing, A.M.: On computable numbers, with an application to the entschei-
dungsproblem. Proceedings of the London Mathematical Society s2-42(1), 230–265
(1937)
17. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of the Philosophy of Science (HPS
). Philosophy of Statistics, vol. 7, pp. 901–982. Elsevier (2011)
18. Wallace, C.S., Boulton, D.M.: A information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)
19. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Springer, Berlin (2005)
20. Barron, A., Rissanen, J., Yu, B.: The minimum description length principle in
coding and modeling (invited paper), pp. 699–716. IEEE Press, Piscataway (2000)
21. Vapnik, V.: Statistical Learning Theory. John Wiley and Sons, NY (1998)
22. Dowe, D.L., Gardner, S., Oppy, G.: Bayes not bust! why simplicity is no problem
for bayesians. The British Journal for the Philosophy of Science 58(4), 709–754
(2007)
23. Wallace, C.S., Dowe, D.L.: Minimum message length and kolmogorov complexity.
The Computer Journal 42(4), 270–283 (1999)
24. Hernndez-Orallo, J., Dowe, D.L.: Measuring universal intelligence: Towards an any-
time intelligence test. Artiﬁcial Intelligence 174(18), 1508–1539 (2010)

298
E. ¨Ozkural
25. ¨Ozkural,
E.:
Towards
heuristic
algorithmic
memory.
In:
Schmidhuber,
J.,
Th´orisson, K.R., Looks, M. (eds.) AGI 2011. LNCS, vol. 6830, pp. 382–387.
Springer, Heidelberg (2011)
26. Schmidhuber, J.: Optimal ordered problem solver. Machine Learning 54, 211–256
(2004)
27. Kelsey, R., Clinger, W., Rees, J.: Revised5 report on the algorithmic language
scheme. Higher-Order and Symbolic Computation 11(1) (1998)
28. Olsson, J.R.: Inductive functional programming using incremental program trans-
formation. Artiﬁcial Intelligence 74, 55–83 (1995)
29. Muggleton, S., Page, C.: A learnability model for universal representations. In:
Proceedings of the 4th International Workshop on Inductive Logic Programming,
vol. 237, pp. 139–160. Citeseer (1994)
30. Ferri-Ram´ırez, C., Hern´andez-Orallo, J., Ram´ırez-Quintana, M.J.: Incremental
learning of functional logic programs. In: Kuchen, H., Ueda, K. (eds.) FLOPS
2001. LNCS, vol. 2024, pp. 233–247. Springer, Heidelberg (2001)
31. Bowers, A., Giraud-Carrier, C., Lloyd, J., Sa, E.: A knowledge representation
framework for inductive learning (2001)
32. Kitzelmann, E.: Inductive programming: A survey of program synthesis techniques.
In: Schmid, U., Kitzelmann, E., Plasmeijer, R. (eds.) AAIP 2009. LNCS, vol. 5812,
pp. 50–73. Springer, Heidelberg (2010)
33. Looks, M.: Scalable estimation-of-distribution program evolution. In: Proceedings
of the 9th Annual Conference on Genetic and Evolutionary Computation (2007)
34. Hutter, M.: Universal algorithmic intelligence: A mathematical top→down ap-
proach. In: Goertzel, B., Pennachin, C. (eds.) Artiﬁcial General Intelligence. Cog-
nitive Technologies, pp. 227–290. Springer, Heidelberg (2007)
35. Goertzel, B.: Opencogprime: A cognitive synergy based architecture for artiﬁcial
general intelligence. In: Baciu, G., Wang, Y., Yao, Y., Kinsner, W., Chan, K.,
Zadeh, L.A. (eds.) IEEE ICCI, pp. 60–68. IEEE Computer Society (2009)
36. Schmidhuber, J.: Ultimate cognition `a la G¨odel. Cognitive Computation 1(2),
177–193 (2009)
37. Solomonoﬀ, R.J.: Machine learning - past and future. In: The Dartmouth Artiﬁcial
Intelligence Conference, pp. 13–15 (2006)
38. Solomonoﬀ, R.J.: The discovery of algorithmic probability. Journal of Computer
and System Sciences 55(1), 73–88 (1997)
39. Bridges, D., Svozil, K.: Constructive mathematics and quantum physics. Interna-
tional Journal of Theoretical Physics 39, 503–515 (2000)
40. ¨Ozkural, E.: A compromise between reductionism and non-reductionism. In: World-
views, Science and Us: Philosophy and Complexity. World Scientiﬁc Books (2007)
41. Quine, W.: Two dogmas of empiricism. The Philosophical Review 60, 20–43 (1951)
42. Chaitin, G.J.: Two philosophical applications of algorithmic information theory. In:
Calude, C.S., Dinneen, M.J., Vajnovszki, V. (eds.) DMTCS 2003. LNCS, vol. 2731,
pp. 1–10. Springer, Heidelberg (2003)
43. Solomonoﬀ, R.J.: The time scale of artiﬁcial intelligence: Reﬂections on social ef-
fects. Human Systems Management 5, 149–153 (1985)
44. Glaskowsk, P.N.: Nvidia’s fermi: The ﬁrst complete gpu computing architecture
(2009)
45. Sandberg, A., Bostrom, N.: Whole brain emulation: A roadmap. Technical report,
Future of Humanity Institute, Oxford University (2008)
46. Koomey, J.G., Berard, S., Sanchez, M., Wong, H.: Implications of historical trends
in the electrical eﬃciency of computing. IEEE Annals of the History of Comput-
ing 33, 46–54 (2011)

An Adaptive Compression Algorithm
in a Deterministic World
Kristiaan Pelckmans
SysCon, Box 337, Information Technology, Uppsala University, 75105, Sweden
kp@it.uu.se
http://www.it.uu.se/katalog/kripe367
Abstract. Assume that we live in a deterministic world, we ask our-
selves which place the device of randomness still may have, even in case
that there is no philosophical incentive for it. This note argues that
improved accuracy may be achieved when modeling the (deterministic)
residuals of the best model of a certain complexity as ‘random’. In or-
der to make this statement precise, the setting of adaptive compression
is considered: (1) accuracy is understood in terms of codelength, and
(2) the ‘random device’ relates to Solomonoﬀ’s Algorithmic Probability
(ALP) via arithmetic coding. The contribution of this letter is threefold:
(a) the proposed adaptive coding scheme possesses interesting behavior
in terms of its regret bound, and (b) a mathematical characterization of
a deterministic world assumption is given. (c) The previous issues then
facilitate the derivation of the Randomness- Complexity (RC) frontier of
the given algorithm.
Keywords: Adaptive Compression, Online Learning, Regret Analysis,
Algorithmic Probability.
1
Introduction
Traditional Algorithmic Probability (ALP) distinguishes between model deﬁni-
tion and the evidence of using this model to explain or predict data. Speciﬁcally,
the model is expressed in terms of the shortest program M describing the model,
as well as the evidence Sn of using this model to describe the n samples. This
leads to the deﬁnition of the ‘ﬁgure of merit’ of a program M on a sequence, given
as −|M| ln Sn. This distinction however vanishes when considering adaptive pro-
tocols where M is build up based on the previously observed data. Speciﬁcally,
one does not need to encode the program explicitly, but it is in an implicit way
constructed while receiving data. Conceptually however, a complexity term pro-
portional to |M| remains a governing factor of the performance as given in the
regret analysis of such adaptive algorithm, see e.g. [2] for a survey.
This realization prompts to what degree this complexity interacts with the
size of the remainders. It is only intuitive that proper choice of this trade-oﬀcan
result in increased performances: given a perfect - but large program, there is
no need whatsoever to spend eﬀort into modeling the remainders as ‘random’.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 299–305, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

300
K. Pelckmans
However in that case it is very diﬃcult to build up this program incrementally.
On the other hand when building up a program of low complexity, a convenient
way to model the remainders is in terms of ‘randomness’, implying increased
codelengths. As such, such optimal trade-oﬀdecides on what part of the data is
to be modeled explicitly as a function of the side information, and what part to
model using the device of ‘randomness’. Conversely, this deﬁnes ‘randomness’ in
the data as too complex deterministic behavior.
This work builds on the large body of results as described in [3,7,9,4,2] and
citing works. It diﬀers from traditional coding theoretic results (such as on MDL
[8], MML [11,5], ...) in terms of the deterministic world assumption we build on.
A detailed mapping of the presented ideas to their respective contexts will be
given in the extended version of this abstract. This abstract basically collects
standard ideas of adaptive ﬁlters and adaptive coding schemes together with
regret bounds (next section), and uses results to show how this procedure comes
with surprising theoretical guarantees (third section). We conclude with some
interesting upcoming challenges for this problem.
2
Adaptive Compression
The following setting is considered. Let yn = (y1, y2, . . . , yn) be a sequence of
n symbols in an alphabet of size m denoted as Σ = {σ1, . . . , σm}. Assume
that each symbol yt ∈Σ is a consequence of the state of the world xt right
before time t, where xt ∈Ft. For example, if yt denotes the 1th character in
Hamlet, Ft denotes the state of the world before Shakespeare actually wrote
down this character. For example xt can include the sequence {y1, . . . , yt−1}.
This concept relates closely to the concept of ﬁltrations in martingale theory.
The deterministic world assumption asserts intuitively that there exists a ﬁxed
(but unknown) mapping f0 : ∪kFk →Σ such that one has yt = f0(xt) for all
t = 1, . . . , n. In order to make our discourse more accessible, phrase xt as a
countable inﬁnite vector xt ∈R∞such that there exists a vector w0 ∈R∞and
yt = f0(xt) = wT
0 xt, ∀t = 1, . . . , n.
(1)
In Def. (2) we characterize the deterministic world assumption more precisely
for the considered model. If xt could be measured, and w0 were known, the
progression of history would be trivial. In our example, Hamlet could be com-
pressed simply as a number representing the length of the work. In order to ﬁx
the setting, consider the following ‘probability assigning function’.
Deﬁnition 1 (Prediction Model). For given x ∈R|S| and W ∈Rm×|S|, the
vector valued function pW : R|S| →[0, 1]m is deﬁned as
pW(x) =
exp(Wx)
m
k=1 exp(Wkx),
(2)
where we have exp(y)k = exp(yk) for all y ∈Rm and k = 1, . . . , m. such that
for all W, x one has that 1T
mpW(x) = 1 and pW(x) ≥0m. The derivative of

An Adaptive Compression Algorithm in a Deterministic World
301
ln pW(x)k with respect to W for any k = 1, . . . , m is given as
d ln pW(x)k
dW
= ekxT −pW(x)xT ,
(3)
where ek = (0, . . . , 1, . . . 0)T ∈{0, 1}m is the kth unit vector.
Then the Gradient Descent (GD) update formula becomes
Wt = Wt−1 + γ d ln pW(xt)yt
dW

W=Wt−1.
(4)
Now in the coding setup one tries to transfer the sequence yn from sender to
a receive through a channel. Adaptive coding en- and decodes the symbols in
an intelligent way based on the previous experience, as well as on available
side information. The protocol implemented both into sender and receiver, and
properly synchronized are given in Alg. (1) and Alg. (2).
Algorithm 1. Adaptive Coding: Sender
Require: Given S and γ > 0. Initiate W0 = 0m0T
d
for t=1,. . . ,n do
(1) Collect the side information xS
t ∈R|S|.
(2) Compute the corresponding pWt−1(xS
t ).
(3) Evaluate yt ∈Σ.
(4) Send the symbol ξt encoding yt using −ln pWt−1(xS
t )yt bits.
(5) Update Wt−1 →Wt using eq. (4).
end for
The encoding using −ln p bits can e.g. be performed using arithmetic coding
which ensures that one deals properly with non-integer valued numbers of bits
for the encoding (i.e. without a large excess of the resulting full code), see e.g.
[3,4]. At the receiving end of the channel the following algorithm is implemented.
Again, decoding can be performed using arithmetic decoding.
Algorithm 2. Adaptive Coding: Receiver
Require: Given S and γ > 0. Initiate W0 = 0m0T
|S|
for t=1,. . . ,n do
(1) Collect the side information xS
t ∈R|S|.
(2) Compute the corresponding pWt−1(xS
t ).
(3) Receive the symbol ξt.
(4) Decode ξt into yt using the vector pWt−1(xS
t ).
(5) Update Wt−1 →Wt using eq. (4).
end for

302
K. Pelckmans
3
Excess and the RC-Frontier
This protocol, however simple, comes with the following performance guarantee.
Lemma 1 (Regret Bound). Given n < ∞. Given a set S of |S| < ∞indices
of x which are relevant to the problem at hand, and deﬁne xS
t ∈R|S| correspond-
ingly. Let ¯
W ∈Rm×|S| be any matrix, and let maxt ∥xS
t ∥∞≤Rx < ∞, then
there exists a γ > 0 such that the sender/receiver protocols yield a code with
length bounded as
−
n

t=1
ln pWt−1(xS
t )yt ≤−
n

t=1
ln p ¯
W(xS
t )yt + c′1
n|S|∥¯
W∥F Rx,
(5)
with c′ > 0 an appropriate constant.
This result goes entirely along the lines as set out in [1]. This result can be
reﬁned in many ways. For example use of the log-loss hints towards strengthening
results as in [2], Ch.9. and cited works. For example, the √n dependence is
highly suboptimal compared to the ln(n) dependence of the minimax regret
for such algorithms. This bound has however an (implicit) √m dependence via
∥¯
W∥F , while one typically has O(m) dependence for other algorithms. The
1
|S|
dependence seems not to be achievable directly via the covering number approach
as discussed in [2], Ch.9.9. Another improvement of this result can be made
towards the case where the horizon n is not ﬁxed a priori, see e.g. [2], Ch.2. The
present algorithm seems not to be covered explicitly in the literature before, and
precise derivations of the above results remain to be given. Such GD algorithms in
practice need only to ﬁnd a proper gain γ > 0 in order to achieve the theoretical
bounds (the regret bounds state existence). This gain is often set in practice by
trial and error.
Now, the left hand side of eq. (5) trades oﬀtwo terms. On the one hand, one
can make −n
t=1 ln p ¯
W(xS
t )yt arbitrary small as the code sequence is assumed
to be deterministic. This however requires possibly very large sets S as well as
large norms ∥¯
W∥F . On the other hand one can control the term
1
n|S|∥¯
W∥F Rx
by keeping |S| and ∥¯
W∥F small, but then the coding protocol pays a price in
the sense that it has to model the more complex behaviors of the sequence
as random, resulting in a waste of coding bits. Now we address the question
how this trade-oﬀcan be made optimally, that is, until what threshold does it
pay oﬀto capture the eﬀects as deterministic while treating the ‘remainders’ as
‘random’ eﬀects. In order to make this trade-oﬀmore explicit, let us introduce
the following assumption:
Deﬁnition 2 (Deterministic World Assumption). Let y = (y1, . . . , yn) ∈
Σn be any sequence, let 0 < c < ∞be any number and let S be any set. Then
assume that there exists a ¯
W ∈Rm×|S| such that
1
|S|∥¯
W∥F ≤c and
n

t=1
−ln p ¯
W(xS
t )yt ≤q(n)
c
,
(6)

An Adaptive Compression Algorithm in a Deterministic World
303
where q : N →R+
0 is a positive increasing function interpolating between O(n)
and O(√n).
It should be kept in mind that this is a mere technical assumption, but one
which is in many cases not too far from reality. It states that asymptotically
when c →∞, no ‘randomness’ is needed.
The factor 1/c is motivated as follows. If sorting the features of the problem
at hand in decreasing relevance to the coding problem, it is not too unreasonable
that it will be less and less beneﬁcial to the coding problem to include the next
one, or to increase the complexity of the function.
The factor q(n) is motivated as follows. Suppose we have a sequence of length
l which consists of a fair share of the possible codes. If applying a nonadaptive
coding protocol using a ﬁxed
¯
W in our algorithm as in eq. (2), on all ⌊n/l⌋
identical blocks, one has an encoding of length ⌊n/l⌋times of what can be
achieved on one block. Hence one would expect a proportional factor n. However,
since we have this result for any ¯
W with appropriate norm, and we are allowed
to concentrate on the one which happened to perform best on the given series
y, the rate will often be improved from n to q(n). Two common choices are
q(n) = n and q(n) = √n. The choice between either depends on how well the
structure of the sequence as well as the side information is adapted to the model
(2). If q(n) = √n, then one has that one already advances to a deterministic
regime using models of relatively small complexity.
Proposition 1. Let a, b > 0 be constants, then
inf
ξ>0
a
ξ + ξb = 2
√
ab.
(7)
This is seen by choosing ξ = 1 a
b , obtained by equating the derivative to zero.
Combining the above results yields the main result of this paper.
Theorem 1 (Optimal Trade-oﬀ). Under the deterministic world assumption
of eq. (6), there exists a set S and γ > 0 such that
−
n

t=1
ln pWt−1(xS
t )yt ≤O
,
q(n)√nRx
-
,
(8)
and this is achieved by using a proper S such that there exists a ¯
W such that
1
|S|∥¯
W∥F ≤O(
1
q(n)/√nRx) and eq. (6) is satisﬁed.
In other words, the behavior which is encoded as ‘random’ when using the al-
gorithm with such S and ﬁxed ¯
W is lying beyond the RC-Frontier. If q(n) = n,
then the upperbound reduces to O(n3/4). This occurs when taking a set S such
that there exists a ¯
W satisfying (6) and such that
1
|S|∥¯
W∥F ≤O(√n). If
q(n) = √n, then the above bound is O(√n) by choosing S such that there exists
a ¯
W satisfying (6) and with
1
|S|∥¯
W∥F ≤O(1).
This is a surprising result: If q(n) ≥O(√n) the excess of the adaptive coding
algorithm is essentially at least as good as in case we were to know the best ¯
W in

304
K. Pelckmans
advance (i.e. there is no regret term here!). In the common case that q(n) > √n,
the adaptive coding strategy results in an order of magnitude smaller excess
than if we were to use a - however as good - nonadaptive strategy.
Another way of formulating the result is to say that ‘the frontier between
deterministic modeling and random modeling is given as the maximal complexity
which is allowed before degrading the worst-case performance’. In the present
case this is achieved by a S and corresponding ¯
W satisfying the deterministic
world assumption and such that |S|∥¯
W∥2
F = O(
1
nq(n)/Rx). This is of course
a statement relative to the deterministic world assumption, as well as to the
adaptive algorithm that we implement.
4
Discussion
The above derivations gives a glimpse of a more general line of results: the
validness of such derivations appears not to be tied to the precise algorithm, nor
to the application setting. This prompts the following challenges:
– Note that the RC-frontier is given in terms of the regret of the algorithm,
and that there exists a minimax version of such regret bound, see e.g. [2].
That is, we can quantify the worst case regret which is achieved with the
best prediction strategy possible. This implies that this frontier can be made
universal, i.e. independent of the algorithm. The term ln n in the minimax
bound would give rise to a O(
1
q(n) ln(n)) RC-frontier.
– The present trading oﬀof learnability and randomness - or the bias-variance
trade-oﬀin this context - is not tied to the adaptive compression setting
only. Similar derivations can be made for most online learning settings. In
order to make such work as here, one needs to ﬁnd an analogues to the
deterministic world assumption in the new setting. It may be argued that
this line of thinking remains valid in the batch learning setting as well, where
an algorithm is asked to induce a model based on all observed data. Optimal
trading complexity of a model while controlling variability has been a main
theme in inductive inference, see e.g. the context of penalized learning and
regularization, see e.g. [6] and citations.
– This derivation suggests that for increasing n, one could consider a se-
quence of model structures having increasing |S|. The latter however has
to increase at a proper, lower pace in order to guarantee eﬃciency. In case
q(n) = O(√n), the derivation suggests that it is not directly useful to let
|S| grow as the beneﬁt of such would not outweigh the consequent increased
regret. This is compatible with the lines set out in [10] where the author
discusses an incremental learning strategy for learning increasingly more
diﬃcult concepts.
The line of thinking as set out in this abstract indicates that the twilight zone
between randomness and deterministic assumptions gives a fertile area for novel
conceptual ideas, based on standard theoretical derivations. I am indebted to
the work of R. Solomonoﬀwhich indicated how to create new ideas based on
mathematical results.

An Adaptive Compression Algorithm in a Deterministic World
305
References
1. Cesa-Bianchi, N.: Analysis of two gradient-based algorithms for on-line regression.
Journal of Computer and System Sciences 59(3), 392–411 (1999)
2. Cesa-Bianchi, N., Lugosi, G.: Prediction, Learning, and Games. Cambridge Uni-
versity Press (2006)
3. Cover, T.M., Thomas, J.A.: Elements of Information Theory. Springer (1991)
4. Csisz´ar, I., Shields, P.C.: Information theory and statistics: A tutorial. Now Pub-
lishers Inc. (2004)
5. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of Philosophy of Science. Hand-
book of Philosophy of Statistics, vol. 7, pp. 901–982. Elsevier (June 2011)
6. Gy¨orﬁ, L., Kohler, M., Krzyzak, A., Walk, H.: A distribution-free theory of non-
parametric regression. Springer (2002)
7. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its applications.
Springer (1997)
8. Rissanen, J.: Modelling by shortest data description. Automatica 14, 465–471
(1978)
9. Shafer, G., Vovk, V.: Probability and ﬁnance: it’s only a game! Wiley Interscience
(2001)
10. Solomonoﬀ, R.J.: Progress in incremental machine learning. In: NIPS Workshop
on Universal Learning Algorithms and Optimal Search, Whistler, BC, Canada,
p. 27 (December 2002)
11. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)

Toward an Algorithmic Metaphysics
Steve Petersen
Department of Philosophy
Niagara University
Niagara University, NY 14109 USA
steve@stevepetersen.net
Abstract. There are writers in both metaphysics and algorithmic in-
formation theory (AIT) who seem to think that the latter could provide
a formal theory of the former. This paper is intended as a step in that
direction. It demonstrates how AIT might be used to deﬁne basic meta-
physical notions such as object and property for a simple, idealized world.
The extent to which these deﬁnitions capture intuitions about the meta-
physics of the simple world, times the extent to which we think the simple
world is analogous to our own, will determine a lower bound for basing
a metaphysics for our world on AIT.
Keywords: metaphysics,
formal
metaphysics,
computational
metaphysics, algorithmic metaphysics, algorithmic information theory,
real patterns.
Both philosophers and mathematicians have ﬂirted with the idea that algorith-
mic information theory (AIT) could provide some foundation for basic notions
in metaphysics. The main inspiration for this paper is one such hint from the
philosophy side: in Daniel Dennett’s sketch of a metaphysics based on “real
patterns”, he explicitly appeals to incompressibility, and oﬀhandedly mentions
the work of AIT theorist Gregory Chaitin in this connection.1 Meanwhile AIT
theorists since Andrey Kolmogorov frequently speak about, for example, the
“information content” of objects generally, rather than of binary strings in par-
ticular. (Of course, “the original incentive to develop a theory of algorithmic
information content of individual objects was Ray Solomonoﬀ’s invention of a
universal a priori probability . . . ”2)
This paper aims to help bridge this gap between metaphysics and AIT. As
beﬁts a philosophy paper, it contains little in the way of technical results, but
some ruminations aiming to pave the way for technical results to come. And as
is typical of work bridging discipline X to discipline Y , X theorists are likely to
complain that the treatment of X is far too simplistic and sloppy, and that the
treatment of Y engages trivial details—while Y theorists complain conversely.
1 See [2] p. 32. Others have developed Dennett’s metaphysics more fully than I have,
most notably the “Rainforest Realism” of [9]. I think everything I say here is com-
patible with this work, while extending it to make more explicit the ties to AIT.
2 [11] p. 333, my emphasis. Solomonoﬀ’s partially autobiographical (and posthumous)
publication [13] lists [12] as the paper that started it all.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 306–317, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Toward an Algorithmic Metaphysics
307
I happily undertake that risk, with hope for some indulgence and patience from
both.
My strategy is to construct an extremely simple toy world W, and give its
metaphysics in terms of AIT. That is, using AIT I’ll try to deﬁne key meta-
physical notions for W such as composite object and property. These basics of
synchronic metaphysics will be plenty to occupy this paper; in future work I hope
to build on this foundation in order to model important diachronic metaphysical
notions such as persisting object, change, and cause.
Eventually the goal is to connect these metaphysics to thriving programs in
algorithmic epistemology. Work such as [8] and [7] extend Solomonoﬀ’s original
insights for formalizing inductive reasoning. The Bayesian approach of [16] is
closely related (as argued in [17]), and has demonstrated potential to characterize
approximate truth [4] and perhaps more elusive philosophical fruit [3].
But ﬁrst, the metaphysics. The extent to which these deﬁnitions seem to
capture the intuitive metaphysics for the toy world W, times the extent to which
we think the metaphysics for W are analogous to our own world, will determine
an informal lower bound for the extent to which we think we might base a
metaphysics for our world on AIT.
1
The Toy World W
W is built out of a ﬁnite set of n “objects” {w1 . . . wn} that we will consider
unproblematically fundamental.3 We’ll need a fair number of them; one million
would be plenty. These fundamental objects cannot be created or destroyed from
one time to the next. In W there is one fundamental property, which each of
these objects either has or does not have at any time. We can think informally
of the wis at a time as a row of life-game-like cells that wraps into a circle, each
of which either possesses the fundamental property (represented as ‘■’ on the
cell, and encoded as ‘1’), or does not possess that property (represented as ‘□’
on the cell, and encoded as ‘0’).4 We can think of W at the next time step as
another circle of such cells just below. This W is, I hope, a very simple case for
the kind of important features we seem to need to construct a metaphysics in
our world: some unproblematically fundamental objects, one maximally simple
fundamental property, and a succession of times.
More formally the {wi} form a ﬁnite cyclic group with a successor-like func-
tion, and W at a time t, or wt, is a function wt : {wi} →{0, 1}. W, in turn,
3 This does not require that they be just-plain fundamental; if Ladyman and Ross
are right that objects are always patterns, “all the way down”, they still allow for
fundamental objects in the context of a ﬁxed resolution.
Also, the use of “build” here and elsewhere in the paper is meant to echo the wide
sense proposed by [1]; one of my many hopes for algorithmic metaphysics is that it
can characterize the important commonalities among the “building relations” she
discusses.
4 We want the cells to wrap into a circle basically because we do not want cells to the
“left” to be simpler, in the AIT sense, than cells to the “right”.

308
S. Petersen
can be seen as a function from discrete possible times (t ∈N) to such functions;
we could write the composite function as wt,i. Let L, the locations, be the set of
intervals on {wi}.5
In W the wi gain or lose their properties by the action of a ﬁxed universal
preﬁx Turing machine U. This machine has a unidirectional binary input tape,
a bidirectional read-write binary work tape (including a blank symbol), and a
write head on the wi. This head can add or remove the fundamental property
at its current cell or can move to the cell’s successor. We’ll abbreviate the ﬁrst
operations as ‘■’ (for adding the property) and ‘□’ (for removing the property),
and the successor movement as ‘S’. So that U does not have an unfair bias, we can
suppose U is (or is one of) the “simplest” universal preﬁx Turing machine(s) to
meet these speciﬁcations (on whatever chosen measure of simplicity for Turing
machines); this is thought by AIT theorists to be the rough equivalent to a
maximally uninformative prior.
Now consider any interval (location) l ∈L, and let xl represent the function
wt as restricted to that interval. An xl is thus an ordering of zeroes and ones—
in eﬀect, a binary string.6 Let X be the set of all such binary strings up to the
maximal length n, and for x ∈X let the standard notation KU(x) designate the
Kolmogorov complexity of x relative to U—that is to say, KU(x) is the length of
the shortest input required to cause our ﬁxed U, when starting at the “left” end
of the interval, to output x and then halt. Finally, let the idiosyncratic notation
x♯designate the length of the shortest program required to output x “literally”—
that is, the length of x plus some small constant for the computational overhead
to print any given string.
2
Things in W
One of the main potential advantages of Dennett’s “real patterns” approach to
metaphysics is that it can make sense of composition—that special, mysterious
way a bunch of things can come together to form a new thing, as when atoms
can make up a molecule, or molecules can make up a brick, or bricks can make
up a house. Peter van Inwagen calls this the special composition question: the
question of when it is true that some objects compose a new object—when it is
true that “∃y the xs compose y.”7 This is a classic philosophical question at least
in the sense that it looks like the answer should be obvious, but reﬂection shows
just about any consistent answer to be counterintuitive. At least for the simple
W, though, I think AIT can provide a relatively intuitive answer to this vexed
5 That is, given the cyclic ordering relation ⟨x, y, z⟩, the interval [a, b] is deﬁned as all
x such that x = a or x = b or ⟨a, x, b⟩.
6 I will sometimes conﬂate the function wt over interval l, the set of ordered pairs
associated with that function, and the resulting ordered binary string. I think (and
hope) that nothing hangs on this conﬂation.
7 [15] p. 30. Note that, at least for W , I am happy to use ‘thing’ and ‘object’ inter-
changeably.

Toward an Algorithmic Metaphysics
309
question.8 This deﬁnition of composite objects will serve as a kind of lynchpin
for the synchronic metaphysics that follow.
In W, an object is composed of other objects when the objects together form
a “real pattern”—that is, the arrangement of the objects and their properties is
easier to specify, computationally, than simply giving the complete details at the
fundamental level. A simple, standard example (adapted for W) is when many
of the fundamental objects (cells) in a row all possess the fundamental property.
If the string of ■s is suﬃciently long, a program equivalent to “for i = 1 to
10,000, ■S” will be much shorter than the literal-print “■S■S ... ■S.” It is
natural to see such a suﬃciently long string of ■s amidst an otherwise chaotic
jumble of squares as an object in its own right.
I propose this deﬁnition for identifying the non-fundamental things in W.
Deﬁnition 1. xl is a composite object if and only if
1. KU(xl) < x♯
l (the compressible clause)
2. There is no partition of l into intervals {l1 . . . ln} such that 
i KU(xli) ≤
KU(xl) (the minimal clause)
3. There is no interval l′ containing l such that KU(xl′) ≤KU(xl) (the maxi-
mal clause)
Does this deﬁnition adequately capture what we might call (non-fundamental)
“objects” in W? Since none of us has any experience in W, it might be hard to
tell what we could reasonably call a composite object there. Still, this deﬁnition
has intuitive features in W that would, if they were carried over into our world,
provide some grip on major challenges in metaphysics.
2.1
Composition and Division
One central challenge for any account of composition is to negotiate both the
Scylla of universalism, according to which any mereological sum of objects is
another object, and the Charybdis of nihilism, according to which the only ob-
jects are the fundamental ones. Universalists implausibly claim that “my desk
plus the Eiﬀel tower” is a genuine object in its own right, while nihilists implau-
sibly claim that desks and towers do not literally exist. They are driven to these
extremes basically because it is hard to ﬁnd a principled line to draw between
them. Dennett’s “real patterns” approach promises to provide just such a line.
The ﬁrst clause of Deﬁnition 1 already implies that universalism is false for
W, for most binary strings are not compressible and thus not objects. Some of
the problem that motivated universalism remains, however. Let o1 be 10,000 ■s
in a row, and let o2 be a binary representation of the ﬁrst 10,000 decimal digits
of π at a location immediately adjacent, and ﬁnally let o3 = o1∪o2. This o3 looks
suspiciously like an arbitrary mereological sum such as “my desk plus the Eiﬀel
tower”, and yet it is easily compressible. It is not an object, though, because of
the minimal clause; it possesses a natural decomposition into o1 and o2, where
8 One I hope more intuitive than van Inwagen’s answer for our world; he claims that
no composite objects exist except for living beings.

310
S. Petersen
KU(o1) + KU(o2) ≤KU(o3). In summary, there is no simplicity gain in treating
the two objects as one—and so, on the real patterns metaphysics, no ontological
gain either.
It may seem that the minimal clause will overgenerate, and likewise rule out o1
as a genuine object, since that long string of ■s can apparently be decomposed
into other compressible objects. For example, split o1 into two longish substrings;
call them o1L and o1R (for “left” and “right”). Each would be compressible on
their own, and their union is just o1—so o1 seems to run afoul of the minimal
clause, and thus of objecthood. This would be W’s analog to the Charybdis of
nihilism, since if anything is a composite object in W, surely o1 is!
But the uniﬁed o1 is diﬀerent from the chimera o3 in an important respect.
The programs for each of o1L and o1R will look very like the program for o1, and
summing them will double the computational overhead. In this way o1 gains in
simplicity over the sum of its substrings, and so is a genuine object by Deﬁni-
tion 1. Similar considerations presumably apply to any substrings of o2.
Still, substrings o1L and o1R seem to be objects in their own right; they
surely would be if they were “on their own.” If they are objects, though, we face
a dilemma. On the one hand, if we say that o1L and o1R compose o1, we must
also say the same for a great number of other such partitions into suﬃciently
long substrings. o1 is thus “composed” of way more overlapping “objects” than
would be intuitive. On the other hand, if we say that o1L and o1R do not compose
o1, then we must say that o1 is identical to the union of two objects, but not
composed of those objects.
I think the best solution is simply to deny object status to substrings o1L
and o1R, as the maximal clause does. Both o1L and o1R are subsets of o1, and
KU(o1) = KU(o1L) = KU(o1R), so substrings o1L and o1R are not objects in
their own right.9
The maximal clause has a natural analog in our world, since it prohibits the
ﬂipside of arbitrary summation—namely, arbitrary division. Just as in our world
it is intuitive to say there is no such “object” as the northern half, northern third,
or northern 3/17ths of my desk, so too in W our string o1 does not have its many
compressible substrings as genuine parts, according to Deﬁnition 1. The natural
motivation here is that a part that is more complex than its whole—because,
say, it depends in some way on a description of the whole to be speciﬁed—is not
an object in good standing on its own. (Deﬁnition 1 thus denies what [14] calls
the “Doctrine of Arbitrary Undetached Parts”.)
9 Actually this is complicated by the fact that, for example, the binary speciﬁcation
of 10,000 in an instruction like “for i = 1 to 10,000” will take more bits than if it
were 1,000 instead. Thus strictly speaking the 1,000-length substrings are not part
of a longer string with equal or lower complexity; their minimal generating programs
will be slightly shorter. I do not think this is a deep problem, though I can only think
of a few kludgy ﬁxes for it—for example, we could require the inequality not hold
within the logarithm of the length diﬀerence between the containing and contained
string, or perhaps set up the Turing machine so that it has typed registers for such
purposes that always take the maximal possible bits.

Toward an Algorithmic Metaphysics
311
But note that Deﬁnition 1 does allow for composing objects out of other
composites; there just needs to be some complexity savings in so composing. For
adjacent composite objects {oi} to compose o, we just need it to be the case that
KU(o) < 
i KU(oi), and that KU(oi) < KU(o) for each i. In other words, the
super-composite must be simpler than the sum of its parts, but not as simple
as any individual one. Suppose, for example, that wt has a streak of 5,000 ■s,
followed by 3,000 □s, and then another 5,000 ■s, followed by another 3,000 □s,
and so on, so that the black-white stripe alteration pattern is repeated (say)
six times. In this case we can write one loop around the loops required for each
stripe, and so we have a compressing program for the whole that is longer than
the compression for each natural part, but shorter than the sum of each. Each
stripe is an object on its own, and together those stripes compose the object
that is the larger stripe pattern. No two of those stripes taken together is an
object, though—and if we had only two stripes in a row like that, without the
repeating pattern, then they would each be a single object that do not compose
a new one together. This, I think, is an intuitively pleasing result.
2.2
Scattered Objects
The examples so far have concentrated on intuitively connected objects, but
notice that Deﬁnition 1 allows for something like scattered objects too. Just
as a cloud may count as one thing even without close bonds among the water
molecules, or a jigsaw puzzle might count as one thing even when it is in pieces,
so too a set of cells over an interval may count as one thing even when there is
a good deal of random noise over the interval, scattering the pattern. Consider,
for example, a large interval over which the odd-indexed cells are all ■s, while it
is random whether the even-numbered cells in between have the property. Such
a string would count as an object despite a certain intuitive lack of internal
cohesion. Minimally interesting objects in W (but most common, in the sense of
most probable to occur) will be long-enough strings where the preponderance of
one property over another is just suﬃcient to allow compression. These objects
will be, in some sense, maximally scattered.10
10 One odd consequence of Deﬁnition 1 is that whether such scattered bits of order
in the chaos count as one object or not will depend on exactly how spread out
they are. At some point a slight preponderance of order mixed in a lot of chaos no
longer gains enough simplicity advantage to make up the overhead required for the
compressing calculation. In our world it’s fairly natural to say that a group of water
molecules with suﬃcient average distance no longer constitute a cloud, and jigsaw
pieces suﬃciently removed from each other are no longer that original puzzle—if,
say, some pieces are in a box in the basement, and some in a landﬁll across town.
On the other hand it is not so intuitive that there is a precise boundary here; the
mere movement of one extra millimeter could hardly make the diﬀerence, but in W
just one extra bit of noise can be enough. Insisting on lack of such precision when
determining whether an object exists, however—insisting, in other words, that it is
vague whether some object exists or not—has its own very serious problems. See, for
example, [5] for a classic, one-page case against, and [19] for a much more extended

312
S. Petersen
Another consequence to consider is that scattered objects, in our world any-
way, allow for interpenetrability—a kite can ﬂy through a cloud, but that does
not intuitively destroy the kite in favor of a kite-cloud, for example. This leads
us to the complicated topic of spatial overlap and coincidence for objects in W.
2.3
Object Overlap and Coincidence
The possibility of building new objects out of more fundamental ones gives rise to
another standard metaphysical puzzle: that of constitution. For (worn) example,
a sculptor forms a lump of clay into an elegant statue. Intuitively, the lump of
clay has not disappeared; it has just been reshaped. And intuitively, the statue
did not exist before the lump gained that shape. Thus many are tempted to
say there are now two things (the lump of clay and the statue) where there was
once one—two material things that occupy exactly the same location, so are
coincident in at least this sense. The fairly neutral description of the case is that
the lump of clay constitutes the statue, and the puzzle is in explaining just what
this relation of “constitution” amounts to.11 Our toy world W has an analogous
puzzle, I think, and AIT provides at least some leverage against it.
As a warmup, consider ﬁrst the issue of object overlap, without exact spatial
coincidence. Suppose Lafayette Avenue and Grant Street intersect. That inter-
section is intuitively part of both streets; after all, you do not suddenly abandon
Lafayette while crossing Grant, nor vice-versa. If so, that rough square of pave-
ment belongs to both Lafayette Avenue and Grant Street, even though the two
roadways are not the same thing. The two roads overlap.
Here is what I take to be an analogous situation in our one-dimensional W:
consider a long string in wt that represents the ﬁrst 11,806 binary digits in the
decimal expansion of π; call it oπ. As it happens, this string ends in sixteen □s.12
But now suppose that those sixteen □s are also followed by a great many more
□s in a row, to make 10,000 overall, and call that string o0. Of course oπ and
o0 overlap, in the sense that their location intervals on wt intersect. To see that
both are objects, consider each clause of Deﬁnition 1:
Compressible. Each is clearly compressible to shorter than their literal
printings.
Minimal. Neither is decomposable into substrings that would save in complex-
ity over the whole. Though oπ might look like it has a natural division, there
is no complexity savings in so doing; it is better to have a π-calculating loop
for 11,806 rounds then to have the same π calculation for 11,790 rounds and
then a separate “repeat □16 times” loop.
Maximal. Neither object is part of a bigger object on a containing interval.
The union of oπ and o0 would have two reasonable compressing programs
(into “π for 11,806, then □for 9,984” or “π for 11,790, then □for 10,000”)
treatment of problems associated with vagueness at the metaphysical level. For a
defense of vague identity, see [15].
11 See [18].
12 Thank you, http://www.befria.nu/elias/pi/binpi.html !

Toward an Algorithmic Metaphysics
313
but neither such program would be shorter than those for oπ or o0 on their
own.
Thus it seems W can have overlapping objects. Perhaps now it is clear that it is
also possible in W to have one object entirely contained by another, as a yolk is
contained by an egg in our world. Consider, for example, the ﬁrst compressible
stretch of 0’s in a very long expansion of π. Again, I think this captures intuitions.
Object coincidence, however, appears to be a diﬀerent matter; while the story
of the statue and the clay makes it seem at least an open possibility in our world,
Deﬁnition 1 in eﬀect stipulates against it for W, since it individuates objects by
their locations. This is a problem, and I have two possible responses to it.
Here, I think, is a rough equivalent in W to the story of the clay and the
statue: oc (the “clay”) is a long string built out of a random mix of the short
strings ‘111’, ‘0010’, and ‘011010’. This fact makes oc compressible. The string os
(the “statue”), on the other hand, is made out of the same substrings, but their
succession follows some identiﬁable (if somewhat arcane) pattern. This string is
compressible in the same way oc was, but also in an even more eﬃcient way that
encodes the pattern of the component strings. In this sense, the “clay” of oc is
still there, but with a further pattern layered on top of it. (Note it is not a case
of object composition, though, since the short substrings are not objects.)
One way to capture our metaphysical intuitions for such a case would be to
alter our deﬁnition slightly: identify objects in W not with (the equivalent of)
compressible binary strings, but instead with the compressing programs for U
that could generate such strings. Very roughly speaking, we might say objects
are individuated by their Aristotelian formal causes, and not their material ones.
Since (at least) either of two diﬀerent compressing programs could potentially
produce the same string os over the same interval, there are then (at least) two
objects in that location.
Another possible option is to keep our deﬁnition and say that there is indeed at
most one object in any location, but the object has two compatible properties:
in our W example, the object both has the property of being a mix of three
certain substrings, and has the property of having those substrings arranged in
such-a-way. In the real world analog, there is one thing on the table, and it has
both the relatively important property of being a statue, and the compatible
property of being a lump of clay.
This approach worries many, because it requires the lump and statue to be
identical, but only contingently; the clay lump might not have been a statue,
even though the clay lump had to have been a clay lump. In other words the
lump and statue have diﬀerent modal properties, and that is at least odd if the
lump is the same thing as the statue. How problematic this is depends on how we
construe modal properties, but I suspect AIT has good prospects for modeling
the “counterpart theory” approach of [10].
At least when it comes to AIT metaphysics in W, my working hunch is that
not much hangs on the diﬀerence between these two options. To see why, though,
we need to discuss how AIT might pick out properties in W, and their relation
to algorithmic suﬃcient statistics.

314
S. Petersen
3
Properties in W
Properties in W built out of the fundamental are most naturally thought of as
sets of possible objects for W; they are in this sense intensional rather than
extensional. This allows for the possibility of uninstantiated properties; it may
be that for some P no object in P ever actually appears in W. I am okay with
this if the serious metaphysicians are. (Are they?) Since we want to be able to
talk about real patterns of such “abstract” sets that might not be realized in
wi concreta, programs causing U to output to cells in W will not be suﬃcient.
Instead, we treat the objects in question as functions from l ∈L to {0, 1}, encode
such functions into binary strings in a standard way, and encode sets of such
strings into a new binary string in a standard way.13 Thus we can speak of the
Kolmogorov complexity of such a set as the program required to output the
string encoding the set on U’s work tape.
It may be tempting to consider any such set of objects S that is compressible
(that is, a set where KU(S) < S♯) a property. This is no good, however; for
example, a set of m-length blocks at various locations (for suﬃciently large m)
plus one long representation of π digits will count as a property, on this view,
even though this set seems unnaturally gerrymandered. The challenge, then, is to
carve the total set of possible objects into its natural joints, whatever “natural”
means here.
I think—with somewhat less conﬁdence than before—that the best approach
is to follow the same technique that was used for deﬁning objects. Just as we
want “real” objects to be simple relative to the sum of their parts, and not
arbitrary sums or divisions, so we want “real” properties to be simple relative
to the sum of their objects, and not arbitrary disjunctions or conjunctions.
Deﬁnition 2. Let O be the set of all possible objects in W. Then P ⊆O is a
property if and only if
1. KU(P) < 
o∈P KU(o) (compressible)
2. There is no partition of P into {P1 . . . Pn} where 
i KU(Pi) ≤KU(P)
(minimal)
3. There is no P ′ ⊆O such that P ⊂P ′ and KU(P ′) ≤KU(P) (maximal)
I think Deﬁnition 2 does a respectable job capturing intuitions about properties.
First, any P satisfying it must have a relatively short program to generate
it, and thus a relatively short description—which seems to imply a relative nat-
uralness to it. The minimal clause rules out arbitrary disjunctive properties;
generating the set of all objects that are either solid blocks or π representations
is no more simple than generating the set of blocks and then the set of π repre-
sentations. The maximal clause rules out arbitrary conjunctive properties; the
set of all suﬃciently long blocks that terminate before w55,510 is not a proper
13 In some sense the set of possible objects should include ones bigger than those possi-
ble in W , including (perhaps) inﬁnite ones, or ones with more than one fundamental
property, etc. I don’t consider possibility in this sense here.

Toward an Algorithmic Metaphysics
315
property, I think, though I think the set of all suﬃciently long blocks is, and
maybe the set of all objects that terminate before w55,510 is too.14
One intriguing feature of Deﬁnition 2 I have only begun to explore is its tie
to algorithmic suﬃcient statistics.15 An algorithmic suﬃcient statistic for x is a
ﬁnite set S such that
KU(x) = KU(S) + log |S| + c
The c is a ﬁxed constant—namely, the length of the program U requires, when
given any set-generating program and index, to output the set element at that
index. The idea is that generating a set containing x and then locating x in that
set is as eﬃcient as the minimal program for x. Note that for any S containing
x, KU(x) will always be less than or equal to KU(S) + log |S| + c, since once
given S one can always ﬁnd x by simply enumerating its elements. Sets that are
algorithmic suﬃcient statistics for x are “optimal” for x in the intuitive sense
that important information about x is already captured by its membership in
set S, so that no further point of substance can then identify x within S. It is
thus natural to think of S as a model for x, while providing its index in S is like
setting the parameters of the model.
Theorem. If S ⊂O is an algorithmic suﬃcient statistic for some o ∈S, and
if it is maximal in the sense of Deﬁnition 2 ( i.e. there is no S′ ⊆O such that
S ⊂S′ and KU(S′) < KU(S)), then S is a property.
Proof. Compressible clause: Since S is optimal for o, we know KU(S) < KU(o)
(by log |S| + c), and KU(o) < 
x∈S KU(x), so KU(S) < 
x∈S KU(x).
Minimal clause: Suppose for contradiction that there is some partition {Si}
of S such that 
i KU(Si) ≤KU(S), and consider Sj ∋o. Then as for any set
containing o,
KU(o) ≤KU(Sj) + log |Sj| + c
And since S is optimal for o, that means in turn that
KU(S) + log |S| + c ≤KU(Sj) + log |Sj| + c
By supposition 
i KU(Si) ≤KU(S), so in particular KU(Sj) < KU(S). Thus
KU(Sj) + log |S| + c < KU(Sj) + log |Sj| + c
This implies log |S| < log |Sj|, where Sj ⊂S—a contradiction.
Maximal clause: By supposition.
⊓⊔
14 I confess whether “objects terminating before wi” is a property by this deﬁnition
has stumped me, for now; it is certainly diﬃcult to generate all objects so placed,
since that would require a way to recognize objects, and that would in turn require
computing Kolmogorov complexity. But generating such a set given O is, I think,
pretty straightforward, and maybe that’s the proper standard.
15 See [11] p. 406, or [6] p. 29.

316
S. Petersen
I suspect the connection between Deﬁnition 2 and algorithmic suﬃcient statistics
may run deeper, though I have been unable to demonstrate as much in time
to complete this paper. For example, I thought perhaps all minimal suﬃcient
statistics for all o ∈O would be properties, but haven’t been able to prove it, and
now doubt it. I also cannot yet prove anything interesting in the other direction.
At any rate, suﬃcient statistics are about summarizing the “meaningful in-
formation” in data; they provide as much information as the data set itself for
picking the best model out of a given model class. (In AIT this model class is
very wide—the set of all computable models.) This notion of separating out the
meaningful information from the happenstance details has natural connections
to the notion of a real property of an object, since we can think of properties
as fundamentally a matter of abstraction; to say that two non-identical objects
share a property is to neglect some information in each in order to highlight
substantive information they share.
Intuitively one can abstract from an abstraction to get another genuine prop-
erty, and Deﬁnition 2 allows for these more abstract properties in the same way
that Deﬁnition 1 allows for super-composed objects. If P ⊂O has a partition
into properties {Pi} where all KU(Pi) < KU(P), and KU(P) < 
i KU(Pi), then
intuitively P summarizes something important that the Pi have in common.
Thus consider for example these three sets:
– All blocks of ■s exactly 1,017 long (abstracting only from the location of
the object)
– All blocks of ■s long enough to compress (abstracting from both location
and block length)
– All blocks repeating any pattern short enough to be repeated often enough
to be compressed (abstracting from location, length, and pattern to repeat)
The program to generate each of these sets will be short, but longer than the
one before, and so each will (I think) meet both their minimal and maximal
requirements.16
Thus our “statue” os from section 2.3 has two properties that are both—if I un-
derstand correctly—algorithmic suﬃcient statistics. We could identify an object
with its suﬃcient statistic and index (the abstract speciﬁcation and its “realiza-
tion”?), or we could simply think of these as interesting properties of one object.
References
1. Bennett, K.: Construction area (no hard hat required). Philosophical Stud-
ies 154(1), 79–104 (2011)
2. Dennett, D.C.: Real patterns. The Journal of Philosophy 88(1), 27–51 (1991)
16 Such successively abstract properties stand to each other roughly as blue stands to
colored—both ﬁrst-order properties of objects. But blue, the property, intuitively
itself possesses a property: it is a color. The need to refer to higher-order properties
can be accommodated easily enough, I think; they should be compressible, minimal,
and maximal sets of properties of the order below.

Toward an Algorithmic Metaphysics
317
3. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical con-
sistency, invariance and uniqueness. In: Bandyopadhyay, P.S., Forster, M.R.
(eds.) Philosophy of Statistics, Handbook of the Philosophy of Science, vol. 7,
pp. 901–982. Elsevier Science & Technology (2011)
4. Dowe, D.L., Gardner, S., Oppy, G.: Bayes not bust! Why simplicity is no
problem for Bayesians. British Journal for the Philosophy of Science 58(4),
709–754 (2007)
5. Evans, G.: Can there be vague objects? Analysis 38(4), 208 (1978)
6. Gr¨unwald, P., Vit´anyi, P.: Shannon information and Kolmogorov complexity
(September 2004), http://de.arxiv.org/abs/cs.IT/0410002 (last accessed June
7, 2010)
7. Gr¨unwald, P.D.: The Minimum Description Length Principle. MIT Press (2007)
8. Hutter, M.: Universal Artiﬁcial Intelligence. Springer (2005)
9. Ladyman, J., Ross, D., Spurrier, D., Collier, J.: Everything Must Go: Metaphysics
Naturalized. Oxford University Press, Oxford (2007, 2009)
10. Lewis, D.: On the Plurality of Worlds. Blackwell, Oxford (1986)
11. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions, 3rd edn. Springer, New York (2008)
12. Solomonoﬀ, R.: A preliminary report on a general theory of inductive inference.
Tech. Rep. V-131, Zator Co. and Air Force Oﬃce of Scientiﬁc Research, Cambridge,
Mass. (February 1960)
13. Solomonoﬀ, R.: Algorithmic probability – Its discovery – Its properties and appli-
cation to strong AI. In: Zenil, H. (ed.) Randomness Through Computation: Some
Answers, More Questions, pp. 1–23. World Scientiﬁc Publishing Company (2011)
14. van Inwagen, P.: The doctrine of arbitrary undetached parts. Paciﬁc Philosophical
Quarterly 62, 123–137 (1981)
15. van Inwagen, P.: Material Beings. Cornell University Press, Ithaca (1990, 1995)
16. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Springer (2005)
17. Wallace, C.S., Dowe, D.L.: Minimum message length and Kolmogorov complexity.
The Computer Journal 42(4), 270–283 (1999)
18. Wasserman, R.: Material constitution. In: Zalta, E.N. (ed.) The Stanford Encyclo-
pedia of Philosophy (Spring 2009)
19. Williamson, T.: Vagueness. Routledge (1994, 1996)

Limiting Context by Using the Web
to Minimize Conceptual Jump Size
Rafal Rzepka, Koichi Muramoto, and Kenji Araki
Graduate School of Information Science and Technology, Hokkaido University, S
Kita-ku, Kita 14, Nishi 8, Sapporo, Japan
{kabura,koin,araki}@media.eng.hokudai.ac.jp
http://arakilab.media.hokudai.ac.jp
Abstract. In this paper we introduce our ideas on how experiences from
real situations could be processed to decrease what Solomonoﬀcalled
“Conceptual Jump Size”. We introduce applications based on common-
sense knowledge showing that vast corpora are able to automatically
conﬁrm the validity of the output, and also replace a “trainer”, which
could lead to decreasing human inﬂuence and speeding up the process of
ﬁnding solutions not provided by such a “trainer” or by real world de-
scriptions. Following this idea, we also suggest a shift toward combining
natural languages with programming languages to smoothen transitions
between layers of Solomonoﬀ’s “Concept net” leading from primitive
concepts to a problem solution.
Keywords: Conceptual Jump Size, artiﬁcial trainers, Wisdom of (Web)
Crowd, Natural Language Processing.
1
Introduction
In his work on Algorithmic Probability (ALP), Solomonoﬀoften underlined that
his approach, strongly inﬂuenced by the works of Turing, was to build algorithms
that are more universal and independent from human inﬂuence[1][2], diﬀering
from the approaches as of Lenat[3] or Newell[4]. We share his belief that acquiring
concepts of learning on diﬀerent levels is a shortcut to commonsense reasoning,
which constitutes a base for more complicated, high level problem solutions and
realizing Artiﬁcial General Intelligence (AGI). However, we chose a more real-
world data-driven approach.
1.1
Common Sense Knowledge as a Contextual Filter
From the beginning of A.I. history, we have been told that people have com-
monsense while computers do not. From early childhood, human beings acquire
various types of knowledge: about the physical world, social rules, and abstract
concepts. When it comes to using these experiences, although being bombarded
with large amounts of information while perceiving the world around us, we
are able to shadow out the irrelevant data and focus on the situation we face.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 318–326, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Minimizing Conceptual Jump Size
319
Today we know that Broca’s area, a part of our brain responsible for language
understanding, also plays an important role in ignoring irrelevant input[5]. We
can notice the importance of this context ﬁxation when evaluating commonsense
knowledge. The human judges opinions vary depending on how rich their imagi-
nation or experiences are. However, in real life situations without much time for
elaborate thinking, context awareness limits possibilities to the required mini-
mum. When you see Laika sitting inside Sputnik, your association that a dog can
be used to defend your house becomes shadowed out and the dogs can be used
for experiments set becomes stronger, while cats can be used for catching mice is
kept “switched oﬀ”. Our minds seem to prioritize related domains and avoid ir-
relevant areas of knowledge. For this reason, after several unsuccessful attempts
to use commonsense knowledge eﬀectively and evaluate it fairly, we decided to
use contextual restraints for retrieving concepts by limiting them to situations.
We chose “house” (rooms, kitchen, bathroom, etc.) as an experimental environ-
ment, furniture and utensils as objects, and family members (plus a robot) as
actors. We then performed an experiment for automatic discovery of common
and uncommon behaviors. It appears that limiting context can easily prevent
oversized Concept Jumps[1] (as explained in subsection 1.3) by decreasing the
number of strings to be searched. In this paper, we brieﬂy introduce our tri-
als, showing that vast linguistic resources can be used for training as Solomonoﬀ
predicted[1]. We also take a step further, suggesting that natural language might
be the key to faster concept creation and a faster learning process.
Fig. 1. Simple Concept net introduced by Solomonoﬀ. Our idea is to combine MIT Con-
ceptNet and our Web-based Commonsense Knowledge to smoothen the net processing
- its automatic generation and concept manipulation. Preferably by creating some new
paradigm which combines natural language based concepts and object-oriented pro-
gramming language (see Section 3).

320
R. Rzepka, K. Muramoto, and K. Araki
1.2
Subjectivity
Solomonoﬀdid not agree with an opinion that subjectivity is “evil” and should
not appear in science[2]. He treated ﬁnite sample size and model selection error as
important sources of error in statistics. In ALP, subjectivity occurs in the latter,
which is the choice of “reference” – a universal computer or universal computer
language. In the end of his life, Solomonoﬀwas working on an intelligent machine
mechanisms that are also visible also in a human infant – born with certain
capabilities that assume certain a priori characteristics of its environmenttobe.
It expects to breathe air, its immune system is designed for certain
kinds of challenges, it is usually able to learn to walk and converse in
whatever human language it ﬁnds in its early environment. As it matures,
its a priori information is modiﬁed and augmented by its experience.
Each time such a system solves a problem or fails, it updates the part of its a
priori information that is relevant to problem solving techniques. It resembles a
maturing human being and its a priori information grows as the life experience
of the system grows. Solomonoﬀunderlines that the subjectivity of algorithmic
probability is a necessary feature that enables an intelligent system to incorpo-
rate experience of the past into techniques for solving problems of the future.
Also in our experiments, especially on language[6] and knowledge acquisition[7]
using inductive learning [8], we are driven by similar observations and we of-
ten alternate models to see how system’s learning performance changes. Output
clearly depends not only on the data (environment) it is exposed to but also the
choices a researcher makes. Diﬀerent algorithms, programming languages, users
and their behavior always produce diﬀerent output and this variety of output
can lead to the best models but the main problem is that they must be found
automatically. To achieve this goal we decided to use large real-world data and
mimic what Damasio calls “primordial feelings”[9], an emotion that is a system
regulator and the base for rational choices of an universal algorithm. One of he
biggest challenges in this stage of our project is the task of limiting context to
achieve the shortest possible knowledge testing time.
1.3
What is Conceptual Jump Size
In [1], Solomonoﬀdescribes how he uses Levin’s search algorithm[10] to deal
with the “exponential explosion” problem. Assuming that pi is the probability
of success of the ith trial string of concepts and ti is the time needed to generate
and test that trial, by using the ti/pi ordering it is found that for one approximate
pi it is impossible to know ti before the ith trial, so we cannot make trials in the
exact ti/pi order. However, it is possible to obtain the ti/pi order by selecting
a small time limit T, and testing all strings, spending at most a length of time
piT on the ith string. When a solution is found, the algorithm stops. If not, T is
doubled and exhaustive testing is repeated. The process of doubling and testing
continues until a solution is found. Solomonoﬀwrites: “It is easy to estimate the
total search time needed to discover a particular known solution to a problem.

Minimizing Conceptual Jump Size
321
If pj is the probability assigned to a particular program, Aj , that solves a
problem and it takes time tj to generate and test that program, then this entire
search procedure will take a time less than 2tj/pj to discover Aj. We call tj/pj the
“Conceptual Jump Size” (CJS) of Aj”[1]. By using this method we can discover
if a machine is practically able to ﬁnd a particular solution to a problem at a
particular state of its development. It is said that CJS is a critical parameter
in the design of training sequences and in the overall operation of a system. We
hypothesized that using context ﬁltering and Web-based “semantic self-check”
could minimize searching time by prioritizing the most obvious clues when a
solution is to be found immediately. We introduce some of our experiments,
suggesting that this could be a useful shortcut.
2
Our Trials with Commonsense Knowledge
In our research we deﬁne “commonsense” quite broadly, by including not only
common knowledge of the physical or social world, but also shared beliefs on
history, geography or culture. Therefore we allow our programs to retrieve knowl-
edge on famous people or popular events, which is useful especially for dia-
log systems, when e.g.,a task-oriented mode is suddenly changed by the user’s
behavior[11]. In the following subsections, we show how such a system can elim-
inate its semantically erroneous utterances and then how similar ideas can im-
prove the system’s handling and generation of concepts.
Self-correcting Universal Dialog System. Non-task oriented dialog sys-
tems, usually called chatterbots or chatbots, can be used as free conversational
partners that allow the gathering of linguistic information or knowledge about a
user for further machine learning, or as a means for dealing with users who have
lost their interest in a task of, for example, an automatic information kiosk. The
ﬁrst such conversational system we developed was Modalin, described in detail in
[12]. Modalin is a free-topic keyword-based conversational system for Japanese
that automatically extracts sets of words related to a conversation topic from
Web resources, which was proved to outperform classic ELIZA-like[13] dialog sys-
tems and became a successful base for chatbots using emotions[11], humor[14]
or causal knowledge[15]. The basic idea of Modalin is simple – after the search
engine results extraction process, it generates an utterance, adds modality, and
veriﬁes the semantic reliability of the generated phrase before uttering it. Over
80% of the extracted word associations were evaluated as being correct, which
was mostly due to an automatic self-correcting process. When a proposition
including adjectives, nouns and verbs was created, the system searched for a
newly created string on the Internet. If there were only a few such combinations,
it discards the candidate and generates a new string with diﬀerent words. With
current search engines operating within seconds it, becomes much easier to avoid
“exponential explosion” of meaningless word combinations. However, to search
for concepts needed for ﬁnding possible solutions, a simple keyword search is not
suﬃcient.

322
R. Rzepka, K. Muramoto, and K. Araki
Toward Concept Search and Manipulation. For trials with automatic
Shankian-like script retrievals or dialog agents like [16] and [12], the context
and usualness are not particularly important, but when it comes to Ambient
Intelligence[17] or Machine Ethics[7], the context and unambiguity of results
become crucial. For instance, the act of killing a person is perceived diﬀer-
ently depending on factors like how emotionally close the victim was to the
observer, if it happened during a war, or who the victim was to a given so-
ciety. Solomonoﬀ’s“strings” become numerous, long and complicated as they
include more elaborate explanations of speciﬁc situations. Although the Web
is the biggest text resource that exists, there are many problems with retriev-
ing clean and credible knowledge, as many sites use colloquial language which
causes noise and makes frequency weights1 improper. Therefore, we decided to
experiment with ConceptNet[18] using the Japanese OMCS[19] database, which
is based not on WWW raw data, but on manual input from volunteers. We
performed two small experiments to check if a) existing concepts can produce
richer and less ambiguous new ones; b) limiting context can help eliminate errors
and improve the eﬃciency of automatic naturalness evaluation of automatically
generated concepts.
Generating Chains of Concepts. We tried to generate chains of concepts as
follows. First, a random noun is input to ConceptNet, retrieving related concepts.
For example, if the input is “a cook”, we retrieve AtLocation(cook, restaurant),
which means that you can nd a cook at a restaurant. Next, “restaurant” is sent
back to ConceptNet, and we acquire AtLocation(restaurant, department store),
as one usually ﬁnds restaurants inside department stores in Japan. Finally, we
can use this knowledge to create a statement saying one can ﬁnd a cook inside
a department store, or more speciﬁcally “at a restaurant inside a department
store”. We call these combined concepts Chains(x), where x is a number of
inputs to ConceptNet. We soon realized that x = 2 is probably the maximum
which can be useful for joining order levels in Solomonoﬀ’s Concept net (see
Fig. 1) but often creates nonsense as assumed earlier. To show the scale of
the inadequate generation problem, the authors performed a simple evaluation
experiment.
Evaluating Concept Triplets. We retrieved pairs of Relations and Concepts
using random nouns from the OMCS database for Japanese. From this set we
randomly chose one hundred related concepts and evaluated them with a simple
scale: “natural”, “uncommon” and “unnatural”. Only 49% of entries were agreed
to be natural relations, 22% were uncommon and 29% unnatural. After an anal-
ysis of the data and discussion between evaluators, we agreed that there are at
least eight reasons why people label a triplet as “uncommon” or “unnatural”.
– Evaluators are not sure about mutual relationship: AtLocation(tanker, sea)
+ AtLocation(sea, Yamashita Park). This park is a famous place by the sea,
1 These weights can be a base for calculating probabilities needed by ALP.

Minimizing Conceptual Jump Size
323
but couples use it for romantic dates and it is not likely you will see heavy
ships passing nearby.
– Something is not impossible, but diﬃcult to be evaluated as “natural” by
all evaluators. For example InstanceOf(salmon, sh) + AtLocation(sh, on
ship): if the input is “salmon”, one rather expects “the sea”, or “a plate” or
“a fridge” as a natural location for this kind of ﬁsh.
– Concepts are obviously related, but the relationship is weak. PartOf
(accelerator, car) + HasProperty(car, runs on gasoline) would probably
score higher if the dependency between using the accelerator and consuming
the gasoline was mentioned.
– The OMCS data input by volunteers are not always correct. PartOf
(Kunashiri, Japan) + HasProperty(Japan, crowded) suggests that there
is no conﬂict regarding whether the disputed island belongs to Russia or
Japan. Kunashiri Island also cannot be considered as crowded, as there are
very few inhabitants. Only 22% of random triplets were evaluated as “nat-
ural”, because the more speciﬁc the concepts are, the higher the possibility
of exceptions.
– Evidently wrong mutual relationship:
CapableOf(goose, swim) + HasSubevent(swim, wearing swimsuit) suggests
that geese swim in clothes.
The analysis showed that 54% of the patterns that scored low were related to
context. Therefore we decided to test our ideas about context by narrowing the
semantic environment and increasing its density.
Limiting Context. As mentioned in the Introduction, when creating a set
of context descriptors we chose a “house” as we are interested in housekeeping
robots and such places are often used for commonsense grounding research. We
assumed that a machine could name all signiﬁcant places (we picked up 9 nouns),
items (37 nouns) and actions (21 verbs). We designed an algorithm for creating
random acts from places, objects and actions. Its basic output was “ACTION
with ITEM at PLACE”, and this set was sent to Yahoo Japan Blog Search
Engine, together with shorter queries, “ACTION in a PLACE” and “ACTION
with a TOOL”, in order to ﬁnd frequencies of particular n-grams. The diﬀerences
between them were used to ﬁnd the most uncommon semantic components of an
action (e.g., eating ice-creams is natural but uncommon if eaten in a bathroom).
For this experiment we created the set of context descriptors by hand, but we are
currently working on automatic generation of such sets by web-mining techniques
supported by knowledge stored in WordNet[20] and ConceptNet. Context is not
being labeled (e.g. as house, shop, conference or street); rather, it is based on the
top ten semantically signiﬁcant keywords (actors, place nouns, also description
adjectives) and actions that are strongly associated with these keywords. So if
an utterer inputs e.g. “That was the best tennis tournament I’ve ever seen”,
the context-limiting module retrieves sets of actors (players, spectators, referees,
etc.), physical items (balls, rockets, seats, etc.), descriptions (fast, amazing, high-
level, etc.) and actions (to play, to watch, to win, etc.) using words from the
utterance as queries.

324
R. Rzepka, K. Muramoto, and K. Araki
Experiment and Its Results. First, the system itself evaluated permuta-
tions of places, items and actions and randomly chosen 100 natural and 100
unnatural self-evaluation outputs, which were shown to three graduate school
student evaluators. The self-scoring was done by comparing web frequencies of
exact matches. It appeared that the three human evaluators agreed with the sys-
tem’s judgment in 77.08% of cases, showing that context limitation signiﬁcantly
increases accuracy in usualness evaluation.
3
Object-Oriented Programming between Artiﬁcial and
Natural Languages
The more we work with concepts, the more we realize that they may become
instances for joining two realms of language - that is, combining programming
and natural languages. If objects, functions, modules or classes were phrases,
sentences, instructional stories, etc., the borderline between both worlds could
become blurred and Solomonoﬀ’s suggestions about training would become eas-
ier to realize. As he mentions in [1]:
Perhaps the most important kind of training sequence is one that
teaches the system to understand English text. By “understand” we mean
able to correctly answer questions (in English) about the text. This un-
derstanding need not be at all complete, but should be good enough so
that ordinary English texts can be a useful source of training for the sys-
tem. This “training” sequence will involve formal languages of increasing
complexity. The ﬁrst examples of English text will cover a ﬁeld that the
system will already be familiar with – so that it will only have to learn
the relationship of the syntax to facts it already knows.
In our opinion, smoother translation between written natural language and
computer-readable logical structures should be faster achieved thanks to enor-
mously growing data2, which, though seen as very noisy, can help to eliminate
a large part of the noise due to its coverage. Engineers from the ﬁeld of Natural
Language Processing (NLP) are making more and more progress in dealing with
vast text resources and automatic understanding of these. There are tools (many
being improved every month) that help to deal not only with morphological or
dependency analysis (at the lexical level) but also with synonyms, homonyms,
exceptions, emotive load, usualness and other tasks of the semantic realm. We
think that with help of the algorithm community, language engineers could be-
come very helpful for realizing Artiﬁcial General Intelligence. NLP is often as-
sociated with machine translation, summarization or question answering, which
are not associated with simulating mental processes, but the techniques used by
these tasks can be easily extended and used for enhancing concept learning and
training as Solomonoﬀproposed.
2 Nowadays it its mostly text, but one can imagine objects containing videos, sounds
or smells.

Minimizing Conceptual Jump Size
325
4
Conclusions
We have described the idea of Conceptual Jump Size as introduced by Solomonoﬀ,
and suggested how it can be minimized by vast raw corpora such as World Wide
Web textual resources. We introduced algorithms (dialog system and concept
generators) where the training process is made by the WWW instead of hu-
mans, and showed improvements in their accuracy after using a Web-based self-
correcting process. Although there are other research projects on commonsense
knowledge enrichment, and also for speciﬁed context[21], the main diﬀerence is
that we aim at universality; i.e., the same system must be able to limit any
context in any situation with any kind of agents, objects or places. Finally, we
brieﬂy suggested that object-oriented programming and concepts could become
a key for creating an intermediate instance between natural and programming
languages. We have not introduced any particular algorithm for minimizing Con-
ceptual Jump Size yet, but we believe that our thoughts and experiences on how
the lack of settled context makes it diﬃcult to work with common sense knowl-
edge, and how to deal with this problem, may give some clues that may help
researchers unfamiliar with NLP to get familiar with a diﬀerent approach to
achieving this goal. With this short introduction of our ideas and latest NLP ca-
pabilities, we want to encourage formal language-oriented specialists to cooperate
with web-mining and language engineers in the way that agricultural machinery
designers work together with soil specialists who know not only diﬀerent kinds
of soil, but also know how to prepare loam. We believe such co-research ten-
dencies could help make Solomonoﬀ’s dream of realizing universally intelligent
machines[2] become reality.
References
1. Solomonoﬀ, R.: A system for incremental learning based on algorithmic probability.
In: Proceedings of the Sixth Israeli Conference on Artiﬁcial Intelligence, Computer
Vision and Pattern Recognition, Tel Aviv, Israel, pp. 515–527 (1989)
2. Solomonoﬀ, R.: Algorithmic Probability – Its Discovery – Its Properties and Ap-
plication to Strong AI. In: Zenil, H. (ed.) Randomness Through Computation:
Some Answers, More Questions, ch. 11, pp. 149–157. World Scientiﬁc Publishing
Company (2011)
3. Lenat, D.: Theory Formation by Heuristic Search – The Nature of Heuristics II:
Background and Examples. Artiﬁcial Intelligence 21(1-2), 31–59 (1983)
4. Newell, A., Simon, H.: GPS, a program that simulates human thought. In: Feigen-
baum, E., Feldman, J. (eds.) Computers and Thought, pp. 279–293. McGraw–Hill,
New York (1963)
5. Haxby, J.V., Horwitz, B., Ungerleider, L.G., Maisog, J., Ma., P.P., Grady, C.L.:
The functional organisation of human extrastriate cortex: a PET-rCBF study of
selective attention to faces and locations. J. Neurosci. IR, 6336–6353 (1994)
6. Hasegawa, D., Rzepka, R., Araki, K.: Connectives Acquisition in a Humanoid
Robot Based on an Inductive Learning Language Acquisition Model. Humanoid
Robots, I-Tech Education and Publishing, Vienna (2009),
http://www.intechopen.com/download/pdf/pdfs_id/6235

326
R. Rzepka, K. Muramoto, and K. Araki
7. Rzepka, R., Komuda, R., Araki, K.: Bacteria Lingualis In The Knowledge Soup
– A Webcrawler With Aﬀect Recognizer For Acquiring Artiﬁcial Empathy. In:
Proceedings of The AAAI 2009 Fall Symposium on Biologically Inspired Cognitive
Architectures (BICA 2009), Washington, D.C., USA, p. 123 (2009)
8. Araki, K., Tochinai, K.: Eﬀectiveness of natural language processing method using
inductive learning. In: Proceedings of IASTED International Conference Artiﬁcial
Intelligence and Soft Computing, Mexico, pp. 295–300 (2001)
9. Damasio, A.: Self Comes to Mind: Constructing the Conscious Brain. Pantheon
(2010)
10. Levin, L.A.: Universal Search Problems. Problemy Peredaci Informacii 9, 115–116
(1973); Translated in Problems of Information Transmission 9, 265–266.
11. Rzepka, R., Higuchi, S., Ptaszynski, M., Dybala, P., Araki, K.: When Your Users
Are Not Serious – Using Web-based Associations, Aﬀect and Humor for Generat-
ing Appropriate Utterances for Inappropriate Input. Transactions of the Japanese
Society for AI 25(1), 114–121 (2010)
12. Higuchi, S., Rzepka, R., Araki, K.: A Casual Conversation System Using Modality
and Word Associations Retrieved from the Web. In: Proceedings of The 2008 Con-
ference on Empirical Methods on Natural Language Processing (EMNLP 2008),
Honolulu, USA, pp. 382–390 (2008)
13. Weizenbaum, J.: ELIZA – A computer program for the study of natural language
communication between man and machine. Commun. ACM 9(1), 36–45 (1966)
14. Dybala, P., Ptaszynski, M., Rzepka, R., Araki, K.: Activating Humans with Humor
– A Dialogue System that Users Want to Interact With. IEICE Transactions on
Information and Systems Journal, Special Issue on Natural Language Processing
and its Applications E92-D(12), 2394–2401 (2009)
15. Fujita, M., Rzepka, R., Araki, K.: Evaluation of Utterances Based on Causal
Knowledge Retrieved from Blogs. In: Proceedings of the International Conference
Artiﬁcial Intelligence and Soft Computing (ASC 2011), pp. 294–299 (2011)
16. Rzepka, R., Ge, Y., Araki, K.: Naturalness of an Utterance Based on the Automat-
ically Retrieved Common Sense. In: Proceedings of IJCAI 2005 – Nineteenth Inter-
national Joint Conference on Artiﬁcial Intelligence, Edinburgh, Scotland (2005),
http://www.ijcai.org/papers/post-0490.pdf
17. Rzepka, R., Araki, K.: What About Tests In Smart Environments? On Possible
Problems With Common Sense In Ambient Intelligence. In: Proceedings of 2nd
Workshop on Artiﬁcial Intelligence Techniques for Ambient Intelligence (IJCAI
2007), Hyderabad, India, pp. 92–96 (2007)
18. Havasi, C., Speer, R., Alonso, J.: ConceptNet 3: a Flexible, Multilingual Semantic
Network for Common Sense Knowledge. In: Proceedings of Recent Advances in
Natural Languages Processing, pp. 277–293 (2007)
19. Singh, P.: Open Mind Common Sense: Knowledge Acquisition from the General
Public. In: Meersman, R., Tari, Z. (eds.) CoopIS/DOA/ODBASE 2002. LNCS,
vol. 2519, pp. 1223–1237. Springer, Heidelberg (2002)
20. Fellbaum, C.: WordNet: An Electronic Lexical Database. MIT Press, Cambridge
(1998)
21. Gupta, R., Kochenderfer, M.K.: Commonsense data acquisition for indoor mobile
robots. In: McGuinness, D.L., Ferguson, G. (eds.) AAAI, pp. 605–610. AAAI Press
/ The MIT Press (2004)

Minimum Message Length Order Selection
and Parameter Estimation of Moving Average
Models
Daniel F. Schmidt
Centre for MEGA Epidemiology, The University of Melbourne
Carlton VIC 3053, Australia
dschmidt@unimelb.edu.au
Abstract. This paper presents a novel approach to estimating a moving
average model of unknown order from an observed time series based on
the minimum message length principle (MML). The nature of the exact
Fisher information matrix for moving average models leads to problems
when used in the standard Wallace–Freeman message length approxima-
tion, and this is overcome by utilising the asymptotic form of the infor-
mation matrix. By exploiting the link between partial autocorrelations
and invertible moving average coeﬃcients an eﬃcient procedure for ﬁnd-
ing the MML moving average coeﬃcient estimates is derived. The MML
estimating equations are shown to be free of solutions at the boundary
of the invertibility region that result in the troublesome “pile-up” eﬀect
in maximum likelihood estimation. Simulations demonstrate the excel-
lent performance of the MML criteria in comparison to standard moving
average inference procedures in terms of both parameter estimation and
order selection, particularly for small sample sizes.
1
Introduction
Moving average models are one of the fundamental building blocks in linear time
series analysis. A time series of length n, y = (y1, . . . , yn)′ ∈Rn, is generated by
a q∗-th order moving average model with coeﬃcients, η∗
q∗= (η∗
1, . . . , η∗
q∗)′, if
yt =
q∗

j=1
η∗
j vt−j + vt,
(1)
where vt ∼N(0, τ ∗) are the independently and identically distributed normal
innovations with variance τ∗. The moving average model describes a time series
as being composed of a linear combination of q∗unobserved random variables
from the series vt. In general, only the time series y is available for observation,
and the order and parameters must be estimated on the basis of the data alone. A
common approach to this problem is to combine maximum likelihood estimation
of the parameters with an information criterion based order selection procedure.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 327–338, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

328
D.F. Schmidt
Let θq = (η′
q, τ)′ denote the full parameter vector of a q-th order moving average.
To estimate a moving average model using an information criterion one solves
ˆq = arg min
q∈{0,...,Q}

−log pq(y|ˆθq) + α(q, n)

,
(2)
where pq(y|θq) is the likelihood of data y under a moving average model with
parameters θq,
ˆθq = arg max
θq∈Λq×R+
{pq(y|θq)}
is the maximum likelihood estimator of the parameters, and α(q, n) is a complex-
ity penalty function; common choices include α(q, n) = q (Akaike’s information
criterion [1]) and α(q, n) = (q/2) log n (the Bayesian information criterion [2]).
The set Λq is the invertibility region for a q-th order moving average model, i.e.,
the set of all coeﬃcients ηq for which the roots of the characteristic polynomial
1 + q
j=1 ηjz−j lie completely within the unit circle.
This paper examines the problem of estimating moving average models using
the information theoretic minimum message length (MML) principle [3]. The
MML principle has previously been applied to the problem of order selection of
moving average models in [4] by Sak et al. Unfortunately, their derivation of the
Fisher information matrix contains a serious mistake, and the resulting message
lengths are based on a quantity that is neither the exact nor the asymptotic
Fisher information. The main contribution of this paper, which is based in part
on unpublished work presented in the author’s PhD thesis [5], is the derivation
of a correct message length formula for moving average models that is based on
the asymptotic Fisher information matrix. The details of this new criterion, and
some important properties, are discussed in Section 2, and its performance is
compared against several moving average inference procedures in Section 3.
1.1
The Minimum Message Length Principle
The MML principle is based on the intimate connection between statistical
inference and data compression and has close links to deep concepts such as
Solomonoﬀ’s algorithmic information theory [6]. Under the MML principle, the
explanation that most concisely describes the data is considered the a posteriori
most likely; as the compression of the data must be decodable, the details of
the model used to compress the data must also be included in the description.
The length of the compressed data, usually expressed terms of base-e digits,
or nits, acts as a universal measure of a model’s goodness-of-ﬁt that naturally
takes into account both the capability and the complexity of the model. While
calculation of the exact message length is in general an NP-hard problem [7], the
Wallace–Freeman MML87 approximation [8] oﬀers a tractable alternative under
some regularity conditions involving the likelihood function and prior distribu-
tion (see pp. 226–227, [3]). Let ω ∈Ω denote a model class in a set of candidate
model classes Ω. The MML87 message length for data y compressed using a
fully speciﬁed model θω ∈Θω, with kω continuous parameters, is given by
I87(y, θω, ω) = −log pω(y|θω) + 1
2 log |J(θω)| −log π(θω, ω) + c(kω),
(3)

MML Inference of Moving Average Models
329
where π(·) is a joint prior distribution over the parameter space Θω and the set
of candidate model classes Ω, J(·) is the Fisher information matrix, and
c(k) = −k
2 log(2π) + 1
2 log(kπ) + ψ(1)
(4)
where ψ(·) is the digamma function. Inference is performed by seeking the pair
(ˆθ87
ˆω , ˆω87) that minimises the message length (3); in contrast to the information
criterion approach, there is no need to appeal to diﬀerent principles for parameter
estimation and order selection.
2
Message Lengths of Moving Average Models
The ingredients required to evaluate the MML87 message length (3) are the
likelihood function, a prior distribution over the continuous and structural pa-
rameters, and the Fisher information matrix. Data arising from model (1) can
be exactly characterised as being generated by an n-dimensional multivariate
normal distribution with zero mean and a special covariance matrix τΓ (ηq),
with entries Γi,j(ηq) = E [yiyj] /τ = γ|i−j|(ηq), where
γk(ηq) =
⎧
⎪
⎨
⎪
⎩
q−k

j=0
ηjηj+k
k ≤q
0
k > q
,
(5)
with η0 = 1 [9]. The negative log-likelihood of a time series, y, given a parameter
vector θq = (η′
q, τ)′, is
−log pq(y|θq) =
n
2
	
log(2πτ) + 1
2 log |Γ (ηq)| +
, 1
2τ
-
y′Γ −1(ηq)y.
(6)
Direct evaluation of (6) involves O(n3) operations, and therefore becomes infea-
sible for large sequences. An alternative, and computational eﬃcient approach,
is to evaluate the likelihood using the Kalman ﬁlter, which involves only O(n)
operations; see, e.g., [10] for details1.
The prior distribution for the moving average coeﬃcients is taken to be uni-
form over the invertibility region Λq (as in [11,4,5]), the prior distribution for the
innovation variance τ is taken to be scale-invariant over some suitable interval
(τ0, τ1), and the prior distribution for the model order is taken to be uniform
over the set {0, . . . , Q}, i.e.,
π(ηq, τ, q) = π(ηq)π(τ)π(q),
(7)
π(ηq) =
1
vol(Λq),
π(τ) ∝1
τ ,
π(q) ∝1.
1 There is a minor typographical error in the initialisation algorithm in [10] in which
the second matrix T is incorrectly transposed in Equation 12.

330
D.F. Schmidt
As the bounds (τ0, τ1), and the maximum order Q, appear only as constants in
the ﬁnal message length expression their values have no eﬀect on order selec-
tion or parameter estimation and may be safely ignored. For completeness, the
algorithm [12] for computing vol(Λq) is presented in Appendix A.
2.1
Fisher Information Matrix
By exploiting the fact that the moving average model is a multivariate Gaussian
distribution, the exact, ﬁnite sample, Fisher information matrix Jn(θq) may be
found using standard formulae. Unfortunately, there are two problems with the
exact information matrix: (i) even using fast algorithms, such as the one in [9],
computation of the exact information matrix is slow, requiring O(n2) operations
(except in the special case that q = 1 [13]), and (ii) the exact information matrix
is singular at the boundaries of the invertibility region. The latter problem arises
from identiﬁability issues in the moving average model and can lead to serious
violations of the regularity conditions under which the MML87 approximation
was derived. Instead, we consider the asymptotic information matrix
J(θq) = n · lim
n→∞
Jn(θq)
n

.
The entries of the asymptotic information matrix are given by Whittle’s asymp-
totic formula [14], and in the case of moving average models they are straight-
forward to calculate. Deﬁne the auxilliary autoregressive process
xt +
q

j=1
ηjxt−j = ut,
(8)
where ut ∼N(0, 1) are independently and identically distributed normal inno-
vations. The asymptotic information matrix is then given by
J(θq) = n ·
6Φ(ηq)
0
0
1
2τ 2
7
(9)
where Φ(ηq) is a (q × q) matrix with entries Φi,j(ηq) = E

xtxt+|i−j|

, which do
not depend on the innovation variance τ. This is expected, given that the signal-
to-noise ratio of a moving average model is also independent of τ. The asymptotic
information matrix for a moving average model is therefore equivalent to the
asymptotic information matrix for an autoregressive model with coeﬃcients ηq.
This implies that |J(ηq)| ≥nq for all ηq ∈Λq, and that |J(ηq)| →∞as the
coeﬃcients approach the boundary of the invertibility region. The entries of the
autocovariance matrix Φ(ηq) can be computed using the formulae presented in
[9]; however, in Section 2.2, a simpliﬁed expression for the message length is
presented in which there is no need to explicitly compute the autoregressive
autocovariance matrix. This results in signiﬁcant increases in both numerical
stability and computational eﬃciency.

MML Inference of Moving Average Models
331
2.2
Minimising the Message Length
The minimum message length estimates are the values of the parameters θq
that minimise the MML87 message length. Due to the diﬃculty in maximis-
ing the likelihood, these estimates must be found by a numerical search. The
invertibility region, Λq, which deﬁnes the set of permissible moving average co-
eﬃcients, forms a complex polyhedron for q ≥3, making a constrained numerical
search diﬃcult. An alternative, and convenient, reparameterisation is in terms
of reﬂection coeﬃcients or partial autocorrelations. There exists a one-to-one
transformation between partial autocorrelations, ρ, in the invertibility region,
and moving average coeﬃcients ηq(ρ) [15]. In partial autocorrelation space the
invertibility region, Pq, reduces to the interior of a hyper-cube
Pq = {ρ : |ρj| < 1, j = 1, . . . , q} ,
considerably simplifying the constrained minimisation problem. A further beneﬁt
to performing the numerical minimisation in partial autocorrelation space is that
the determinant of the coeﬃcient-space asymptotic Fisher information matrix,
(9), is given by the simple expression
|J(θq)| =
,nq+1
2τ 2
- ⎛
⎝
q

j=1
1
(1 −ρ2
j(ηq))j
⎞
⎠,
(10)
where ρ(ηq) = (ρ1(ηq), . . . , ρq(ηq))′ are the q partial autocorrelations corre-
sponding to the coeﬃcients ηq. In contrast to direct evaluation of |J(θq)| in
coeﬃcient space, this expression involves only O(q) operations and does not
require the direct computation of the autocovariances of the auxilliary autore-
gressive process (8). Using (6), (7) and (10) in (3) yields the following expression
for the MML87 message length, I87(y, ηq, τ, q),
−log pq(y|ηq, τ) + q
2 log n −1
2
q

j=1
j log(1 −ρ2
j(ηq)) + log vol(Λq) + c(q + 1)
+1
2 log
n
2
	
+ log
.
(Q + 1) log
,τ1
τ0
-/
,
(11)
where c(·) is given by (4). The last two terms of (11) are constant with respect
to q and θq, and therefore have no eﬀect on parameter estimation or order
selection, and may be ignored if we are only considering moving average models
to be possible explanations of the data. The above expression easily handles
the case that q = 0 by simply dropping the second through fourth terms. It is
important to note that (11) gives an expression for the message length in terms
of the coeﬃcients ηq; the corresponding partial autocorrelations, ρ(ηq), are only
used because they make evaluating and minimising this expression signiﬁcantly
easier. The MML87 estimate of the innovation variance, ˆτ87(ηq), conditional on
a coeﬃcient vector ηq, is the same as the maximum likelihood estimate,
ˆτ 87(ηq) = y′Γ (ηq)y
n
,

332
D.F. Schmidt
which itself may be calculated eﬃciently through the use of the same Kalman ﬁl-
ter recurrence relations used to calculate the negative log-likelihood. The MML87
parameter estimates, ˆη87
q , are found by searching for the partial autocorrelations
that solve
ˆρ87 = arg min
ρ∈Pq
8
I87(y, ηq(ρ), ˆτ 87(ηq(ρ)), q)
9
,
and transforming them to coeﬃcient space, i.e., ˆη87
q
≡ηq( ˆρ87). The MML esti-
mate of the order, ˆq87, may then be found by solving
ˆq87 = arg min
q∈{0,...,Q}
8
I87(y, ˆη87
q , ˆτ 87(ˆη87
q ), q)
9
.
An interesting result is that evaluation of the MML87 message length (11)
involves only O(q) additional operations over evaluation of the negative log-
likelihood, and thus the minimum message length estimates are theoretically as
quick to ﬁnd numerically as the maximum likelihood estimates. In fact, experi-
ments suggest that the extra “regularisation” introduced by the presence of the
asymptotic Fisher information term acts to signiﬁcantly improve convergence of
the search procedure for MML estimates in comparison to maximum likelihood
estimation, which is well known to be problematic for moving average models.
2.3
Properties of the MML87 Estimator
The MML87 estimate of order, ˆq87, is a strongly consistent estimate of q∗. To
see this, rewrite the message length (11) as
−log pq(y|ηq, τ) + q
2 log n + O(1),
where O(1) denotes terms that are constant with respect to n. Thus, the MML87
message length (11) asymptotically coincides with the Bayesian information cri-
terion (BIC), and from the arguments in [16], ˆq87 is a strongly consistent estimate
of q∗. Further, the MML87 estimates of the coeﬃcients and innovation variance
asymptotically coincide with the maximum likelihood estimates. This implies
that when q ≥q∗the MML87 parameter estimates are also strongly consis-
tent [17]. The MML87 estimates of the moving average coeﬃcients also possess
an interesting ﬁnite sample property.
Property 1. For all datasets, y, of all ﬁnite sample sizes n, the partial autocor-
relations corresponding to the MML87 estimates of the coeﬃcients, ˆη87, satisfy
||ρ(ˆη87)||∞< 1,
where || · ||∞denotes the ℓ∞norm.
Proof. The MML estimates minimise the sum of the negative log-likelihood
and the half log-determinant of the Fisher information matrix, as given in (11).

MML Inference of Moving Average Models
333
The negative log-likelihood is bounded from below for ﬁnite n for all ηq ∈Rq;
in contrast, the half log-determinant of the Fisher information matrix is un-
bounded from above as ||ρ(ηq)||∞→1. Thus, the message length will be ﬁnite
if and only if ||ρ(ˆη87)||∞< 1, implying that the parameter estimates that min-
imise (11) must satisfy ||ρ(ˆη87)||∞< 1.
□
This result implies that ˆη87 ∈Λq, and therefore the MML87 estimates of
the moving average coeﬃcients do not suﬀer from the so-called “pile-up” phe-
nomenon [18], in which coeﬃcients are estimated to lie exactly on the boundary
of the invertibility region; this problem is well known to aﬀect the maximum like-
lihood estimates. The removal of the troublesome pile-up eﬀect is attributable
to the “regularisation” introduced by the Fisher information terms, which also
corroborates the empirical observations that the message length surface is bet-
ter behaved than the likelihood surface when performing numerical optimisation.
3
Evaluation
Two measures of “closeness” to the true, generating moving average process
were used to assess the competing estimators: (i) normalized expected one-step-
ahead squared prediction error; and (ii) the directed Kullback–Leibler diver-
gence. The expected one-step-ahead squared prediction error is deﬁned as the
expected squared diﬀerence between the true conditional mean and the predicted
conditional mean for the next sample if the q previous innovations were avail-
able, and assesses the closeness of the estimated moving average model in ideal
conditions. Given a true model, η∗, and estimated model, ˆη, this is equal to
SPE1(η∗, ˆη) =
,
1
γ0(η∗)
-
(η∗−ˆη)′(η∗−ˆη),
(12)
where γ0(η∗) is the zero-order autocovariance of the generating process (found
using (5)), and the two parameter vectors are made to be the same dimension by
appending a suitable number of zero elements to the shorter vector. The scaling
by the inverse of the zero-order autocovariance of the generating process ren-
ders the resulting quantity unitless, and is done to ensure that the value of the
error metric is comparable between diﬀerent generating processes. This is essen-
tial if simulations involve sampling a large number of generating models from
the invertibility region, with correspondingly diﬀerent signal-to-noise ratios. The
second error metric used was the Kullback–Leibler (K–L) divergence [19]. This
is an important, parameterisation-invariant measure of the “distance” between
distributions, with strong information theoretic interpretations. The per sample
K–L divergence between a true, generating moving average process, θ∗, and an
approximating moving average process, ˆθ, for n data points is
1
2 log
, ˆτ
τ ∗
-
+
, 1
2n
-
log
, |Γ (ˆη|
|Γ (η∗)|
-
+
, 1
2n
-
Tr

Γ (η∗)Γ −1(ˆη)

−1
2,
(13)
where Γ (·) is an (n×n) autocovariance matrix as in (5). The choice of the size of
the autocovariance matrix is essentially arbitrary and the use of the sample size,

334
D.F. Schmidt
n, reﬂects the fact that the sequence y can be regarded as a randomly generated
vector from the n-dimensional multivariate normal distribution characterised
by τ∗Γ (η∗). Thus, the Kullback–Leibler divergence measures the closeness of
the estimated n-dimensional multivariate distribution to the distribution that
generated the sample.
3.1
Parameter Estimation
The parameter estimation performance of the MML87 estimator was compared
against two standard procedures from the literature: (i) the maximum likeli-
hood (ML) estimator, and (ii) the modiﬁed Durbin estimator (ARMASA) [20,21]
which exploits the duality between moving average and autoregressive models.
Given our choice of a uniform prior distribution for the moving average coeﬃ-
cients, the maximum likelihood estimator also coincides with the maximum a
posteriori (MAP) estimator. As we know that the mean of the generating mov-
ing average process is zero, we chose not to demean the data before estimation
by ARMASA to allow for a fair comparison with MML87 and ML.
The simulation setup was as follows: (i) sample an invertible moving average
model ηq∗uniformly from Λq∗(using the algorithm described in [22]); (ii) sample
a time series of length n from the process deﬁned by η∗
q, with τ∗= 1; (iii)
estimate coeﬃcients from the time series using MML87, maximum likelihood,
the ARMASA procedure, and compute appropriate measures of closeness to the
generating model. This was repeated for 103 iterations, for q = {1, 4, 7, 10}, with
sample sizes n = k(3q + 1), where k = {1, 2, 4}.
The results are presented in Table 1. Median expected one-step-ahead squared
error, (12), and median per sample Kullback–Leibler divergences, (13), were used
instead of arithmetic means as the tails of the empirical distributions of these error
measures were signiﬁcantly heavier than would be expected for a normal distribu-
tion. The results clearly show the strong performance of the MML87 estimator,
which is superior in terms of both squared prediction errors, and Kullback–Leibler
divergence, for every combination of sample size and true order. For the smallest
sample sizes the maximum likelihood/MAP estimator uniformly performed the
worst, often by a large margin; this can be attributed to the “pile-up” eﬀect as well
as the tendency of the maximum likelihood estimator to overestimate the magni-
tude of the zeros of the underlying process when the sample size is small. In con-
trast, observations suggested that the MML87 estimates tended to underestimate
the magnitudes of the zeros in comparison to the maximum likelihood estimates,
and the distributions of the estimates appeared unimodal, showing no sign of any
“pile-up” type eﬀect. For larger sample sizes, the modiﬁed Durbin’s method gener-
ally performed worse than the maximum likelihood estimator in terms of squared
errors. Of course, the true models used in the simulations have been sampled from
the prior distribution used by the MML87 estimator, which makes direct com-
parisons with maximum likelihood and the modiﬁed Durbin’s method somewhat
problematic. However, even taking this into account, the results demonstrate that
the MML87 estimator is clearly superior to the usual Bayesian MAP estimator
which utilises the same prior information.

MML Inference of Moving Average Models
335
Table 1. Parameter estimation experiment results
Order
n
Squared Prediction Error
Kullback–Leibler Divergence
MML87
ML
ARMASA
MML87
ML
ARMASA
1
4
0·071
0·143
0·085
0·139
0·182
0·152
8
0·032
0·055
0·051
0·089
0·105
0·090
16
0·015
0·022
0·025
0·040
0·048
0·048
4
13
0·158
0·297
0·238
0·182
0·313
0·236
26
0·070
0·102
0·111
0·096
0·147
0·120
52
0·031
0·038
0·053
0·048
0·063
0·066
7
22
0·164
0·320
0·266
0·210
0·382
0·261
44
0·077
0·116
0·126
0·108
0·173
0·132
88
0·033
0·041
0·058
0·052
0·070
0·068
10
31
0·172
0·315
0·294
0·209
0·390
0·278
62
0·079
0·117
0·123
0·111
0·185
0·133
124
0·035
0·043
0·058
0·051
0·075
0·069
One point of particular note is that despite the fact that the message length
formula (11) is based on the asymptotic Fisher information matrix, the MML87
estimates perform very well in the small sample regime. This suggests that fur-
ther reﬁnements of the message length formula to make use of the ﬁnite sample
Kullback–Leibler divergence, such as the new message length formulae discussed
in [5] (pp. 30–34) could lead to further performance improvements for small sam-
ples. This issue, along with a more complete characterisation of the behaviour of
the MML87 moving average estimates, are interesting topics for future research.
3.2
Order Selection
The ability of the MML87 criterion to estimate a moving average model of un-
known order from ﬁnite samples was compared against six standard procedures
from the literature: the Akaike information criterion (AIC) [1], the corrected AIC
(AICc) [23], the symmetric Kullback–Leibler divergence criterion (KIC) [24], the
corrected KIC (KICc) [25] and the ARMAsel procedure [26]. The BIC, AIC, KIC
and their corrected variants use maximum likelihood estimates, while the AR-
MAsel procedure uses modiﬁed Durbin estimates (without zero meaning the
data, as previously discussed).
The simulation setup was as follows: (i) sample an invertible moving average
model ηq∗uniformly from Λq∗; (ii) sample a time series of length n from the
process deﬁned by η∗
q, with τ∗= 1; (iii) ask all criteria to estimate q∗along with
estimates of the moving average coeﬃcients, and compute appropriate measures
of closeness to the generating model. This was repeated one thousand times
for each true model order q∗= {0, . . . , 10}, for a total of 11, 000 iterations per
sample size n = {10, 20, 50, 100}. At each iteration, all candidate models in
q = {0, . . . , r} were considered by the model selection criteria, with r = 4 for
n = 10, r = 7 for n = 20, and r = 10 for n > 20. These simulations were
designed to mimic real data situations in which the true, underlying process

336
D.F. Schmidt
Table 2. Order estimation experiment results
n
Measure
Model Selection Criteria
MML87
BIC
AIC
AICc
KIC
KICc
ARMASA
SPE1
0·387
0·412
0·414
0·426
0·414
0·444
0·405
10
KL
0·220
0·247
0·258
0·246
0·242
0·255
0·239
#{ˆq = q∗}
1534
1607
1670
1390
1499
1250
1481
SPE1
0·245
0·284
0·292
0·287
0·284
0·309
0·268
20
KL
0·175
0·206
0·236
0·206
0·206
0·207
0·186
#{ˆq = q∗}
2341
2292
2487
2186
2292
2032
2243
SPE1
0·065
0·093
0·109
0·091
0·086
0·090
0·086
50
KL
0·084
0·114
0·138
0·117
0·111
0·113
0·094
#{ˆq = q∗}
4343
3912
4235
4281
4343
3978
4192
SPE1
0·022
0·028
0·037
0·033
0·028
0·027
0·036
100
KL
0·036
0·047
0·056
0·052
0·046
0·045
0·047
#{ˆq = q∗}
6227
5799
5584
5817
6200
6178
5997
may be considerably more complex than any model that the data will allow us
to realistically consider.
The median expected one-step-ahead squared prediction error,(12), median per
sample Kullback–Leibler divergence, (13), and the number of times a criteria cor-
rectly estimated the true model order are presented in Table 2. In terms of squared
prediction errors, and Kullback–Leibler divergence, the MML87 criterion is uni-
formly the best for all sample sizes. For n = 10 and n = 20, the ARMAsel proce-
dure performed similar to MML87, although for n > 20 it performed noticeably
poorer. In terms of correct order selections, for all but n = 50 the AIC and/or KIC
perform amongst the best of all the methods. Not too much should be made of this
fact, however, as it is well known that as n grows these criteria are inconsistent
and will tend to overﬁt with non-vanishing probability. Interestingly, for smaller
sample sizes, despite performing the best in terms of predictive measures (squared
error and Kullback–Leibler divergence) the MML87 criterion does not, in general,
perform the best at selecting the true generating order. We believe this is because
MML, and related compression based methods, make no assumptions about the
existence of a “true” model; rather, they are designed to select a good, plausible
explanation about the data generating source from the available candidates.
3.3
The Southern Oscillation Index Time Series
Finally, we conclude this section with a brief experiment on a real time series.
The Southern Oscillation Index (SOI) is a time series of monthly measurements
of ﬂuctuations in air pressure diﬀerence between Tahiti and Darwin. The SOI
is commonly used to study and predict El Ni˜no phenomena. The time series
analysed contained n = 1, 619 monthly measurements taken from January, 1876
through to December, 2010, and was obtained from the Australian Government
Bureau of Metereology website. The ﬁrst 1, 000 samples in the series were used
as a training sample, and the remaining 619 samples were used for validation

MML Inference of Moving Average Models
337
Table 3. Southern Oscillation Index Experiment
Measure
Model Selection Criteria
MML87
BIC
AIC
AICc
KIC
KICc
ARMASA
SPE1
59·129
59·166
59·740
59·740
59·740
59·740
59·792
NLL
2490·1
2490·5
2492·3
2492·3
2492·3
2492·3
2491·4
Order
13
13
14
14
14
14
14
of the estimated models. All criteria were asked to estimate a suitable moving
average model from the training data, with a maximum candidate order of q =
20. The evaluation measures were mean squared prediction error and negative
log-likelihood obtained on the validation sample, conditional on the training
sample. These were computed by running the Kalman ﬁlter on the complete time
series using the models estimated from the training sample; this automatically
produces predictions of the mean and variance for all data points in the time
series, and these may be used to compute the squared error and negative log-
likelihood of the validation sample (i.e., the last 619 samples).
The mean squared error and negative log-likelihood scores are presented in
Table 3, along with the order of the moving average model selected by each
of the criteria. All criteria perform similarly, with MML87 obtaining a slight
improvement in squared prediction error. The two main points of interest are:
(i) even for this large sample (n = 1, 000), the MML87 criteria has selected a
slightly lower order model (ˆq87 = 13) than all other criteria except BIC; and (ii)
the MML87 estimates of coeﬃcients for the q = 13 moving average model still
diﬀer slightly from the maximum likelihood estimates used by BIC, despite the
high ratio of data-to-parameters.
Appendix A
For completeness we present the equations (taken from [12]) to calculate the
volume of the invertibility region, Λq, for a q-th order moving average process.
Deﬁne M1 = 2 and Mk = ((k −1)/k)Mk−2. Let Vq ≡vol(Λq), with V1 = 2; for
q > 1, Vq = q/2−1
k=0
M 2
2k+1 for q even and Vq = Vq−1Mq for q odd.
References
1. Akaike, H.: A new look at the statistical model identiﬁcation. IEEE Transactions
on Automatic Control 19(6), 716–723 (1974)
2. Schwarz, G.: Estimating the dimension of a model. The Annals of Statistics 6(2),
461–464 (1978)
3. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length,
1st edn. Information Science and Statistics. Springer (2005)
4. Sak, M., Dowe, D., Ray, S.: Minimum message length moving average time series
data mining. In: Proceedings of the ICSC Congress on Computational Intelligence
Methods and Applications (ACFM 2005), Istanbul, Turkey (2005)

338
D.F. Schmidt
5. Schmidt, D.F.: Minimum Message Length Inference of Autoregressive Moving Av-
erage Models. PhD thesis, Clayton School of Information Technology, Monash Uni-
versity (2008)
6. Solomonoﬀ, R.J.: A formal theory of inductive inference. Information and Con-
trol 7(2), 1–22, 224–254 (1964)
7. Farr, G.E., Wallace, C.S.: The complexity of strict minimum message length infer-
ence. Computer Journal 45(3), 285–292 (2002)
8. Wallace, C.S., Freeman, P.R.: Estimation and inference by compact coding. Journal
of the Royal Statistical Society (Series B) 49(3), 240–252 (1987)
9. Porat, B., Friedlander, B.: Computation of the exact information matrix of Gaus-
sian time series with stationary random components. IEEE Transactions on Acous-
tics, Speech and Signal Processing 34(1), 118–130 (1986)
10. Gardner, G., Harvey, A.C., Phillips, G.D.A.: Algorithm AS 154: An algorithm for
exact maximum likelihood estimation of autoregressive-moving average models by
means of Kalman ﬁltering. Applied Statistics 29(3), 311–322 (1980)
11. Fitzgibbon, L.J., Dowe, D.L., Vahid, F.: Minimum message length autoregressive
model order selection. In: Proceedings of the International Conference on Intelligent
Sensing and Information Processing (ICISIP), pp. 439–444 (2004)
12. Piccolo, D.: The size of the stationarity and invertibility region of an autoregressive
moving average process. Journal of Time Series Analysis 3(4), 245–247 (1982)
13. Makalic, E., Schmidt, D.F.: Fast computation of the Kullback-Leibler divergence
and exact Fisher information for the ﬁrst-order moving average model. IEEE Signal
Processing Letters 17(4), 391–393 (2009)
14. Whittle, P.: The analysis of multiple stationary time series. Journal of the Royal
Statistical Society, Series B (Methodological) 15(1), 125–139 (1953)
15. Barndorﬀ-Nielsen, O., Schou, G.: On the parametrization of autoregressive models
by partial autocorrelations. Journal of Multivariate Analysis 3, 408–419 (1973)
16. Haughton, D.M.A.: On the choice of a model to ﬁt data from an exponential family.
The Annals of Statistics 16(1), 342–355 (1988)
17. Rissanen, J., Caines, P.E.: The strong consistency of maximum likelihood estima-
tors for ARMA processes. The Annals of Statistics 7(2), 297–315 (1979)
18. Davidson, J.E.H.: Problems with the estimation of moving average processes. Jour-
nal of Econometrics 16(3), 295–310 (1981)
19. Kullback, S., Leibler, R.A.: On information and suﬃciency. The Annals of Math-
ematical Statistics 22(1), 79–86 (1951)
20. Durbin,
J.:
Eﬃcient
estimation
of
parameters
in
moving-average
models.
Biometrika 46(3/4), 306–316 (1959)
21. Broersen, P.M.T.: Autoregressive model orders for Durbin’s MA and ARMA esti-
mators. IEEE Transactions on Signal Processing 48(8), 2454–2457 (2000)
22. Jones, M.C.: Randomly choosing parameters from the stationarity and invertibility
regions of autoregressive-moving average models. Applied Statistics 36(2), 134–138
(1987)
23. Hurvich, C.M., Tsai, C.L.: Regression and time series model selection in small
samples. Biometrika 76(2), 297–307 (1989)
24. Cavanaugh, J.E.: A large-sample model selection criterion based on Kullback’s
symmetric divergence. Statistics & Probability Letters 42(4), 333–343 (1999)
25. Seghouane, A.K., Bekara, M.: A small sample model selection criterion based on
Kullback’s symmetric divergence. IEEE Transactions on Signal Processing 52(12),
3314–3323 (2004)
26. Broersen, P.M.T.: Automatic spectral analysis with time series models. IEEE
Transactions on Instrumentation and Measurement 51(2), 211–216 (2002)

Abstraction Super-Structuring Normal Forms:
Towards a Theory of Structural Induction
Adrian Silvescu and Vasant Honavar
Department of Computer Science, Iowa State University, Ames, IA, USA
Abstract. Induction is the process by which we obtain predictive laws
or theories or models of the world. We consider the structural aspect
of induction. We answer the question as to whether we can ﬁnd a ﬁ-
nite and minimalistic set of operations on structural elements in terms
of which any theory can be expressed. We identify abstraction (group-
ing similar entities) and super-structuring (combining topologically e.g.,
spatio-temporally close entities) as the essential structural operations
in the induction process. We show that only two more structural op-
erations, namely, reverse abstraction and reverse super-structuring (the
duals of abstraction and super-structuring respectively) suﬃce in order
to exploit the full power of Turing-equivalent generative grammars in
induction. We explore the implications of this theorem with respect to
the nature of hidden variables, radical positivism and the 2-century old
claim of David Hume about the principles of connexion among ideas.
1
Introduction
The logic of induction, the process by which we obtain predictive laws, theories,
or models of the world, has been a long standing concern of philosophy, science,
statistics and artiﬁcal intelligence. Theories typically have two aspects: structural
or qualitative (corresponding to concepts or variables and their relationships, or,
in philosophical parlance, ontology) and numeric or quantitative (corresponding
to parameters e.g., probabilities). Once the qualitative aspect of a certain law
is ﬁxed, the quantitative aspect becomes the subject of experimental science
and statistics. Induction is the process of inferring predictive laws, theories, or
models of the world from a stream of observations. In general, the observations
may be passive, or may be the outcomes of interventions by the learning agent.
Here, we limit ourselves to induction from passive observation alone.
Under the computationalistic assumption (i.e., the Church-Turing thesis, which
asserts that any expressible theory can be described by a Turing Machine [18]),
one way to solve the induction problem is to enumerate all the Turing machines
(and run them in parallel - dovetailing in order to cope with the countably in-
ﬁnite number of them) and pick one that strikes a good balance between the
predictability (of the ﬁnite experience stream) and size (complexity) [16], [17],
[15], [20] or within a Bayesian setting, using a weighted vote among the predic-
tions of the various models [7] (see [2] and references therein). In the general
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 339–350, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

340
A. Silvescu and V. Honavar
setting, a priori the number of types of possible structural laws that can be pos-
tulated is inﬁnite. This makes it diﬃcult to design general purpose induction
strategy. We ask whether a ﬁnite and minimalistic set of fundamental structural
operations suﬃce to construct any set of laws. In this case such a set will render
induction more tractable because at any step the learner will have to pick from
a small ﬁnite set of possible operations as opposed to an inﬁnite one.
Because Turing machines are rather opaque from a structural standpoint, we
use the alternative, yet equivalent, mechanism of generative grammars1. This al-
lows us to work with theories that can be built recursively by applying structural
operations drawn from a ﬁnite set. The intuition behind this approach is that
induction involves incrementally constructing complex structures using simpler
structures (e.g., using super-structuring, also called chunking), and simplifying
complex structures when possible (e.g., using abstraction). Such a compositional
approach to induction oﬀers the advantage of increased transparency over the
enumerate-and-select approach pioneered by Solomonoﬀ[16], [17]. It also oﬀers
the possibility of reusing intermediate structures as opposed to starting afresh
with a new Turing machine at each iteration, thereby replacing enumeration by
a process akin to dynamic programming or its heuristic variants such as the A*
algorithm.
We seek laws or patterns that explain a stream of observations through suc-
cessive applications of operations drawn from a small ﬁnite set. The induced
patterns are not necessarily described solely in terms of the input observations,
but may also use (a ﬁnite number of) additional internal or hidden (i.e., not
directly observable) entities. The role of these internal variables is to simplify
explanation. The introduction of internal variables to aid the explanation pro-
cess is not without perils [12]2. One way to preclude the introduction of internal
variables is to apply the following demarcation criterion: If the agent cannot dis-
tinguish possible streams of observations based on the values of an internal vari-
able, then the variable is non-sensical (i.e., independent of the data or “senses”)
3. The direct connection requirement restricts the no-nonsense theories to those
formed out empirical laws [1] (i.e., laws that relate only measurable quantities).
However several scientists, including Albert Einstein, while being sympathetic
to the positivist’s ideas, have successfully used in their theories, hidden variables
that have at best indirect connection to observables. This has led to a series of
revisions of the positivist’s doctrine culminating in Carnap’s attempt to accom-
modate hidden variables in scientiﬁc explanations [3]. The observables and the
internal variables in terms of which the explanation is oﬀered can be seen as the
1 See [10] for a similarly motivated attempt using Lambda calculus.
2 Consider for example, a hidden variable which stands for the truth value of the
sentence: “In heaven, if it rains, do the angels get wet or not?”
3 This is a radical interpretation of an idea that shows up in the history of Philosophy
from Positivism through the empiricists and scholastics down to Aristotle’s “Nihil
est in intellectu quod non prius fuerit in sensu” (There is nothing in the mind that
was not previously in the senses).

Abstraction Super-Structuring Normal Forms
341
ontology4 - i.e., the set of concepts and their interrelationships found useful by
the agent in theorizing about its experience. In this setting, structural induction
is tantamount to ontology construction.
The rest of the paper is organized as follows: Section 2 introduces Abstraction
Super-structuring Normal Forms that correspond to a general class of Turing-
equivalent generative grammars that can be used to express theories about
the world; and shows that: abstraction (grouping similar entities) and super-
structuring (combining topologically e.g., spatio-temporally close entities) as the
essential structural operations in the induction process; Only two more structural
operations, namely, reverse abstraction and reverse super-structuring (the du-
als of abstraction and super-structuring respectively), suﬃce in order to exploit
the full power of Turing-equivalent generative grammars in induction. Section 3
interprets the theoretical results in a larger context the nature of hidden vari-
ables, radical positivism and the 2-century old claim of David Hume about the
principles of connexion among ideas. Section 4 concludes with a summary.
2
Abstraction Super-Structuring Normal Forms
We start by recapitulating the deﬁnitions and notations for generative grammars
and the theorem that claims the equivalence between Generative Grammars
and Turing Machines. We then draw the connections between the process of
induction and the formalism of generative grammars and motivate the quest
for a minimalistic set of fundamental structural operations. We then get to the
main results of the paper: a series of characterization theorems of two important
classes of Generative Grammars: Context-Free and General Grammars, in terms
of a small set of fundamental structural operations.
2.1
Generative Grammars and Turing Machines
Deﬁnitions (Grammar). A (generative) grammar is a quadruple (N, T, S, R)
where N and T are disjoint ﬁnite sets called NonTerminals and Terminals, re-
spectively, S is a distinguished element from N called the start symbol and R
is a set of rewrite rules (a.k.a. production rules) of the form (l →r) where
l ∈(N ∪T )∗N(N ∪T )∗and r ∈(N ∪T )∗. Additionally, we call l the left hand
side (LHS) and r the right hand side (RHS) of the rule (l →r). The language
generated by a grammar is deﬁned by L(G) = {w ∈T ∗|S
∗→w} where
∗→stands
for the reﬂexive transitive closure of the rules from R. Furthermore
+
→stands
for the transitive (but not reﬂexive) closure of the rules from R. We say that
two grammars G,G′ are equivalent if L(G) = L(G′). The steps contained in a
set of transitions α
∗→β is called a derivation. If we want to distinguish between
derivations in diﬀerent grammars we will write α
∗
→G β or mention it explicitly.
We denote by ϵ the empty string in the language. We will sometimes use the
4 The ontology in this case is not universal as it is often the case in philosophy; it is
just a set of concepts and interrelations among them that aﬀord the expression of
theories.

342
A. Silvescu and V. Honavar
shorthand notation l →r1|r2|...|rn to stand for the set of rules {l →ri}i=1,n.
See e.g., [13] for more details and examples.
Deﬁnition (Grammar Types). Let G = (N, T, S, R) be a grammar. Then
1. G is a regular grammar (REG) if all the rules (l →r) ∈R have the
property that l ∈N and r ∈(T ∗∪T ∗N).
2. G is context-free grammar (CFG) if all the rules (l →r) ∈R have the
property that l ∈N.
3. G is context-sensitive grammar (CSG) if all the rules (l →r) ∈R
have the property that they are of the form αAβ →αγβ where A ∈N and
α, β, γ ∈(N ∪T )∗and γ ̸= ϵ. Furthermore if ϵ is an element of the language
one rule of the form S →ϵ is allowed and furthermore the restriction that S
does not appear in the right hand side of any rule is imposed. We will call
such a sentence an ϵ −Amendment.
4. G is general grammar (GG) if all the rules (l →r) ∈R have no additional
restrictions.
Theorem 1. The set of General Grammars is equivalent in power with the set
of Turing Machines. That is, for every Turing Machine T there exists a General
Grammar G such that L(G) = L(T ) and vice versa.
Proof. This theorem is a well known result. See for example [13] for a proof5.
□
2.2
Structural Induction, Generative Grammars and Motivation
Before proceeding with the main results of the paper we examine the connections
between the setting of generative grammars and the problem of structural induc-
tion. The terminals in the grammar formalism denote the set of observables in
our induction problem. The NonTerminals stand for internal variables in terms
of which the observations (terminals) are explained. The “explanation” is given
by a derivation of the stream of observations from the initial symbol S
∗→w. The
NonTerminals that appear in the derivation are the internal variables in terms of
which the surface structure given by the stream of observations w is explained.
Given this correspondence, structural induction aims to ﬁnd an appropriate set
of NonTerminals N and a set of rewrite rules R that will allow us to derive
(explain) the input stream of observations w from the initial symbol S. The pro-
cess of Structural Induction may invent a new rewrite rule l →r under certain
conditions and this new rule may contain in turn new NonTerminals (internal
variables) which are added to the already existing ones. The common intuition
is that l is a simpler version of r, as the ﬁnal goal is to reduce w to S. The
terminals constitute the input symbols (standing for observables), the NonTer-
minals constitute whatever additional “internal” variables that are needed, the
5 Similar results of equivalence exist for transductive versions of Turing machines
and grammars as opposed to the recognition versions given here (See e.g., [2] and
references therein). Without loss of generality, we will assume the recognition as
opposed to the transductive setting.

Abstraction Super-Structuring Normal Forms
343
rewrite rules describe their interrelationship and altogether they constitute the
ontology. The correspondence between the terms used in structural induction
and generative grammars is summarized in Table 1.
Table 1. Correspondence between Structural Induction and Generative Grammars
Structural Induction
Generative Grammar
Observables
Terminals T
Internal Variables
NonTerminals N
Law / Theory
production rule(s) l →r
Ontology
Grammar G
Observations Stream
word w
Explanation
Derivation S
∗→w
Partial Explanation
Derivation α
∗→w
Thus, in general, structural induction may invent any rewrite rule of the form
l →r, potentially introducing new NonTerminals, the problem is that there are
inﬁnitely many such rules that we could invent at any point in time. In order
to make the process more well deﬁned we ask whether it is possible to ﬁnd a
set of fundamental structural operations which is ﬁnite and minimalistic, such
that all the rules (or more precisely sets of rules) can be expressed in terms of
these operations. This would establish a normal form in terms of a ﬁnite set of
operations and then the problem of generating laws will be reduced to making
appropriate choices from this set without sacriﬁcing completeness. In the next
subsection we will attempt to decompose the rules l →r into a small ﬁnite set of
fundamental structural elements which will allow us to design better structure
search mechanisms.
2.3
ASNF (Abstraction SuperStructuring Normal Form) Theorems
Issue (ϵ −Construction). In the rest of the paper we will prove some theorems
that impose various sets of conditions on a grammar G in order for the grammar
to be considered in a certain Normal Form. If ϵ ∈L(G) however, we will allow two
speciﬁc rules of the grammar G to be exempted from these constraints and still
consider the grammar in the Normal Form. More exactly if ϵ ∈L(G) and given a
grammar G′ such that L(G′) = L(G\{ϵ}) and G′ = (N ′, T, S′, R′) is in a certain
Normal Form then the grammar G = (N ∪{S}, T, S, R = R′ ∪{S →ϵ, S →S′})
where S /∈N ′ will also be considered in that certain Normal Form despite the
fact that the two productions {S →ϵ, S →S′} may violate the conditions of the
Normal Form. These are the only productions that will be allowed to violate the
Normal Form conditions. Note that S is a brand new NonTerminal and does not
appear in any other productions aside from these two. Without loss of generality
we will assume in the rest of the paper that ϵ /∈L(G). This is because if ϵ ∈L(G)
we can always produce using the above-mentioned construction a grammar G′′
that is in a certain Normal Form and L(G′′) = L(G′) from a grammar G′ that is

344
A. Silvescu and V. Honavar
in that Normal Form and satisﬁes L(G′) = L(G\{ϵ}). We will call the procedure
just outlined the ϵ −Construction. We will call the following statement the
ϵ −Amendment: Let G = (N, T, S, R) be a grammar, if ϵ is an element of
the language L(G) one rule of the form S →ϵ is allowed and furthermore the
restriction that S does not appear in the right hand side of any rule is imposed.
First we state a weak form of the Abstraction SuperStructuring Normal Form
for Context Free Grammars.
Theorem 2 (Weak-CFG-ASNF). Let G = (N, T, S, R), ϵ /∈L(G) be a Con-
text Free Grammar. Then there exists a Context Free Grammar G′ such that
L(G) = L(G′) and G′ contains only rules of the following type:
1. A →B
2. A →BC
3. A →a
Proof . Since G is a CFG it can be written in the Chomsky Normal Form [13].
That is, such that it contains only productions of the forms 2 and 3. If ϵ ∈L(G)
then a rule of the form S →ϵ is allowed and S does not appear in the RHS of
any other rule (ϵ −Amendment). Since we have assumed that ϵ /∈L(G) we do
not need to deal with ϵ −Amendment and hence the proof.
□
Remarks.
1. We will call the rules of type 1 Renamings (REN).
2. We will call the rules of type 2 SuperStructures (SS) or compositions.
3. The rules of the type 3 are just convenience renamings of observables into
internal variables in order to uniformize the notation and we will call them
Terminal (TERMINAL).
We are now ready to state the the Weak ASNF theorem for the general case.
Theorem 3 (Weak-GEN-ASNF). Let G = (N, T, S, R), ϵ /∈L(G) be a
General (unrestricted) Grammar. Then there exists a grammar G′ such that
L(G) = L(G′) and G′ contains only rules of the following type:
1. A →B
2. A →BC
3. A →a
4. AB →C
Proof . See Appendix of [19].
Remark. We will call the rules of type 4 Reverse Super-Structuring (RSS).
In the next theorem we will strengthen our results by allowing only the renam-
ings (REN) to be non unique. First we deﬁne what we mean by uniqueness and
then we proceed to state and prove a lemma that will allow us to strengthen the
Weak-GEN-ASNF by imposing uniqueness on all the productions safe renamings.
Deﬁnition (strong-uniqueness). We will say that a production α →β
respects strong-uniqueness if this is the only production that has the property

Abstraction Super-Structuring Normal Forms
345
that it has α in the LHS and also this is the only production that has β on the
RHS.
Lemma 2. Let G = (N, T, S, R), ϵ /∈G a grammar such that all its productions
are of the form:
1. A →B
2. A →ζ , ζ /∈N
3. ζ →B , ζ /∈N
Modify the the grammar G to obtain G′ = (N ′, T, S′, R′) as follows:
1. Introduce a new start symbol S′ and the production S′ →S.
2. For each ζ /∈N that appears in the RHS of one production in G let {Ai →
ζ}i=1,n all the the productions that contain ζ in the RHS of a production.
Introduce a new NonTerminal Xζ and the productions Xζ →ζ and {Ai →
Xζ}i=1,n and eliminate the old productions {Ai →ζ}i=1,n.
3. For each ζ /∈N that appears in the LHS of one production in G let {ζ →
Bj}j=1,m all the the productions that contain ζ the LHS of a production.
Introduce a new NonTerminal Yζ and the productions ζ →Yζ and {Yζ →
Bj}j=1,m and eliminate the old productions {ζ →Bj}j=1,m.
Then the new grammar G′ generates the same language as the initial grammar
G and all the productions of the form A →ζ and ζ →B , ζ /∈N respect strong-
uniqueness. Furthermore, if the initial grammar has some restrictions on the
composition of the ζ /∈N that appears in the productions of type 2 and 3, they
are respected since ζ is left unchanged in the productions of the new grammar and
the only other types of productions introduced are renamings that are of neither
type 2 nor type 3.
Proof. See Appendix of [19].
By applying Lemma 2 to the previous two Weak-ASNF theorems we obtain
strong versions of these theorems which enforce strong-uniqueness in all the
productions safe the renamings.
Theorem 4 (Strong-CFG-ASNF). Let G = (N, T, S, R), ϵ /∈L(G) be a
Context Free Grammar. Then there exists a Context Free Grammar G′ such
that L(G) = L(G′) and G′ contains only rules of the following type:
1. A →B
2. A →BC - and this is the only rule that has BC in the RHS and this is the
only rule that has A in the LHS (strong-uniqueness).
3. A →a - and this is the only rule that has a in the RHS and this is the only
rule that has A in the LHS (strong-uniqueness).
Proof. Apply Lemma 2 to the grammar converted into Weak-CFG-ASNF
□
Theorem 5 (Strong-GEN-ASNF). Let G = (N, T, S, R), ϵ /∈L(G) be a gen-
eral (unrestricted) grammar. Then there exists a grammar G′ such that L(G) =
L(G′) and G′ contains only rules of the following type:

346
A. Silvescu and V. Honavar
1. A →B
2. A →BC - and this is the only rule that has BC in the RHS and this is the
only rule that has A in the LHS (strong-uniqueness).
3. A →a - and this is the only rule that has a in the RHS and this is the only
rule that has A in the LHS (strong-uniqueness).
4. AB →C - and this is the only rule that has C in the RHS and this is the
only rule that has AB in the LHS (strong-uniqueness).
Proof. Apply Lemma 2 to the grammar converted into Weak-GEN-ASNF
□
Remark. After enforcing strong uniqueness the only productions that contain
choice are those of type 1 - renamings (REN).
In the light of this theorem we proceed to introduce the concept of abstraction
and prove some additional results.
2.4
Abstractions and Reverse Abstractions
Deﬁnitions (Abstractions Graph). Given a grammar G = (N, T, S, R) which
is in an ASNF from any of the Theorems 1 - 4 we call an Abstractions Graph of
the grammar G and denote it by AG(G) a Directed Graph G = (N, E) whose
nodes are the NonTerminals of the grammar G and whose edges are constructed
as follows: we put a directed edge starting from A and ending in B iﬀA →B
is a production that occurs in the grammar. Without loss of generality, we can
assume that the graph has no self loops, i.e., edges of the form A →A; If
such self-loops exist, the corresponding productions can be eliminated from the
grammar without altering the language. In such a directed graph a node A has a
set of outgoing edges and a set of incoming edges which we refer to as out-edges
and in-edges respectively. We will call a node A along with its out-edges the
Abstraction at A and denote it ABS(A) = {A, OEA = {(A, B)|(A, B) ∈E}}.
Similarly, we will call a node A along with its in-edges the Reverse Abstraction
at A and denote it RABS(A) = {A, IEA = {(B, A)|(B, A) ∈E}}.
2.5
Grow Shrink Theorem
Theorem 6. Let G = (N, T, S, R), ϵ /∈L(G) be a General Grammar. Then we
can convert such a grammar into the Strong-GEN-ASNF i.e., such that all the
productions are of the following form:
1. A →B
2. A →BC - and this is the only rule that has BC in the RHS and this is the
only rule that has A in the LHS. (strong-uniqueness)
3. A →a - and this is the only rule that has A on the LHS and there is no
other rule that has a on the RHS. (strong uniqueness)
4. AB →C - and this is the only rule that has C in the RHS and this is the
only rule that has AB in the LHS. (strong-uniqueness)
And furthermore for any derivation w such that γ
∗→w , in G, γ ∈N + there
exists a derivation γ
∗→μ
∗→ν
∗→w such that μ ∈N +, ν ∈N ∗and γ
∗→μ

Abstraction Super-Structuring Normal Forms
347
contains only rules of type 1 and 2 (REN, SS), μ
∗→α contains only rules of the
type 1, more particularly only Reverse Abstractions and type 4 (REN(RABS),
RSS) and ν
∗→w contains only rules of type 3 (TERMINAL).
Proof. See Appendix of [19].
We have therefore proved that for each General Grammar G we can transform
it in a Strong-GEN-ASNF such that the derivation (explanation in structural
induction terminology) of any terminal string w can be organized in three phases
such that: Phase 1 uses only productions that grow (or leave unchanged) the
size of the intermediate string; Phase 2 uses only productions that shrink (or
leave unchanged) the size of the intermediate string; and Phase 3 uses only
TERMINAL productions6. In the case of grammars that are not in the normal
form as deﬁned above, the situation is a little more complicated because of
successive applications of grow and shrink phases. However, we have shown that
we can always transform an arbitrary grammar into one that in the normal
form. Note further that the grow phase in both theorems use only context free
productions.
We now proceed to examine the implications of the preceeding results in the
larger context including the nature of hidden variables, radical positivism and
the David Hume’s principles of connexion among ideas.
3
The Fundamental Operations of Structural Induction
Recall that our notion of structural induction entails: Given a sequence of ob-
servations w we attempt to ﬁnd a theory (grammar) that explains w and si-
multaneously also the explanation (derivation) S
∗→w. In a local way we may
think that whenever we have a production rule l →r that l explains r. In a
bottom up - data driven way we may proceed as follows: First introduce for
every observable a a production A →a. The role of these productions is sim-
ply to bring the observables into the realm of internal variables. The resulting
association is between the observables and the corresponding internal variables
unique (one to one and onto) and hence, once this association is established,
we can forget about the existence of observables (Terminals). Since establishing
these associations is the only role of the TERMINAL productions, they are not
true structural operations. With this in mind, if we are to construct a theory in
the GEN-ASNF we can postulate laws of the following form:
1. A →BC - Super-structuring (SS) which takes two internal variables B
and C that occur within proximity of each other (adjacent) and labels the
compound. Henceforth, the shorter name A can be used instead for BC.
This is the sole role of super-structuring - to give a name to a composite
structure to facilitate shorter explanations at latter stages.
6 At ﬁrst sight, it may seem that this construction oﬀers a way to solve the halting
problem. However, this is not the case, since we do not answer the question of
deciding when to stop expanding the current string and start shrinking, which is key
to solving the halting problem.

348
A. Silvescu and V. Honavar
2. A →B|C - Abstraction (ABS). Introduces a name for the occurrence of
either of the variables B or C. This allows for compactly representing two
productions that are identical except that one uses B and the uses C by a
single production using A. The role of Abstraction is to give a name to a
group of entities (we have chosen two only for simplicity) in order to facilitate
more general explanations at latter stages which in turn will produce more
compact theories.
3. AB →C - Reverse Super-structuring (RSS) which introduces up to two
existing or new internal variables that are close to each other (with respect
to a speciﬁed topology) that together “explain” the internal variable C.
4. A →C, B →C - Reverse Abstraction (RABS) which uses existing or new
internal variables A and B as alternative explanations of the internal variable
C (we have chosen two variables only for simplicity).
3.1
Reasons for Postulating Hidden Variables
Recall that are at least two types of reasons for creating Hidden Variables:
1. (OR type) - [multiple alternative hidden causes] The OR type corresponds
to the case when some visible eﬀect can have multiple hidden causes H1 →
Effect, H2 →Effect . In our setting, this case corresponds to Reverse
Abstraction. One typical example of this is: The grass is wet, and hence
either it rained last night or the sprinkler was on. In the statistical and
machine learning literature the models that use this type of hidden variables
are called mixture models [9].
2. (T-AND type) - [multiple concurrent hidden causes] The T-AND type, i.e.,
topological AND type, of which the AND is a sepcial case. This corresponds
to the case when one visible eﬀect has two hidden causes both of which
have to occur within proximity of each other (with respect to a speciﬁed
topology) in order to produce the visible eﬀect. H1H2 →Effect. In our
setting, this corresponds to Reverse Super-structuring. In the Statistical /
Graphical Models literature the particular case of AND hidden explanations
is the one that introduces edges between hidden variables in the dependence
graph [5], [9], [11].
The preceding discussion shows that we can associate with two possible reasons
for creating hidden variables, the structural operations of Reverse Abstraction
and Reverse Super Structuring respectively. Because these are the only two types
of productions that introduce hidden variables in the GEN-ASNF this provides
a characterization of the rationales for introducing hidden variables.
3.2
Radical Positivism
If we rule out the use of RSS and RABS, the only operations that involve the pos-
tulation of hidden variables, we are left with only SS and ABS which corresponds
to the radical positivist [1] stance under the computationalist assumption. An
explanation of a stream of observations w in the Radical Positivist theory of the
world is mainly a theory of how the observables in the world are grouped into

Abstraction Super-Structuring Normal Forms
349
classes (Abstractions) and how smaller chunks of observations are tied together
into bigger ones (Super-Structures). The laws of the radical positivist theory are
truly empirical laws as they only address relations among observations.
However, structural induction, if it is constrained to using only ABS and SS,
the class of theories that can be induced is necessarily a subset of the set of
theories that can be described by Turing Machines. More precisely, the resulting
grammars will be a strict subset of Context Free Grammars, (since CFG con-
tain SS, and REN(ABS+RABS)). Next we will examine how any theory of the
world may look like from the most general perspective when we do allow Hidden
Variables.
3.3
General Theories of the World
If structural induction is allowed to take advantage of RSS and RABS in ad-
dition to SS and ABS, the resulting theories can make use of hidden variables.
Observations are a derivative byproduct obtained from a richer hidden vari-
able state description by a reduction: either of size - performed by Reverse
SuperStructuring or of information - performed by Reverse Abstraction. Note
that, while in general, structural induction can alternate several times between
REN+SS and RABS+RSS, we have shown that three phases suﬃce: a growth
phase (REN+SS); a shrink phase (RABS+RSS); and a Terminal phase. Whether
we can push all the RABS from the ﬁrst phase into the second phase and make
the ﬁrst phase look like the one in the radical positivist stance (only ABS+SS)
remains an open question (See Appendix of [19] for a Conjecture to this eﬀect).
3.4
Hume’s Principles of Connexion among Ideas
We now examine, against the backdrop of GEN-ASNF theorem, a statement
made by philosopher David Hume more that 2 centuries ago: “I do not ﬁnd
that any philosopher has attempted to enumerate or class all the principles of
association [of ideas]. ... To me, there appear to be only three principles of con-
nexion among ideas, namely, Resemblance, Contiguity in time or place, and
Cause and Eﬀect” [6]. If we substitute Resemblance with Abstraction (since ab-
straction is triggered by resemblance or similarity), Contiguity in time or place
with Super-Structuring (since proximity, e.g., spatio-temporal proximity drives
Super-Structuring) and Cause and Eﬀect with the two types of explanations that
utilize hidden variables, it is easy to see that the GEN-ASNF theorem is simply
a precise restatement of Hume’s claim under the computationalist assumption.
4
Summary
We have shown that abstraction (grouping similar entities) and super-structuring
(combining topologically e.g., spatio-temporally close entities) as the essential
structural operations in the induction process. A structural induction process
that relies only on abstraction and super-structuring corresponds to the radi-
cal positivist stance. We have shown that only two more structural operations,

350
A. Silvescu and V. Honavar
namely, reverse abstraction and reverse super-structuring (the duals of abstrac-
tion and super-structuring respectively) (a) suﬃce in order to exploit the full
power of Turing-equivalent generative grammars in induction; and (b) opera-
tionalize two rationales for the introduction of hidden variables into theories of
the world. The GEN-ASNF theorem can be seen as simply a restatement, under
the computationalist assumption, of Hume’s 2-century old claim regarding the
principles of connexion among ideas.
References
1. Ayer, A.J.: Language, Truth, and Logic, 2nd edn. Gollancz, London (1936, 1946)
2. Burgin, M.: Super-Recursive Algorithms. Springer (2005)
3. Carnap, R.: An introduction to the Philosophy of Science. Basic Books (1966)
4. Chomsky, N.: Syntactic Structures. Mouton, The Hague (1957)
5. Elidan, G., Friedman, N.: Learning Hidden Variable Networks: The Information
Bottleneck Approach. Journal of Machine Learning Research (JMLR) 6, 81–127
(2005)
6. Hume, D.: An Enquiry Concerning Human Understanding. Hackett Publ. Co.
(1993)
7. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability. EATCS. Springer (2005)
8. Kuroda, S.-Y.: Classes of languages and linear-bounded automata. Information
and Control 7(2), 207–223 (1964)
9. Lauritzen, S.L.: Graphical Models. Clarendon Press, Oxford (1996)
10. Oates, T., Armstrong, T., Harris, J., Nejman, M.: On the Relationship Between
Lexical Semantics and Syntax for the Inference of Context-Free Grammars. In:
Proceedings of the 19th National Conference on Artiﬁcial Intelligence (AAAI),
pp. 431–436 (2004)
11. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann Pub-
lishers (1988)
12. Popper, K.R.: The Logic of Scientiﬁc Discovery, English ed. Basic Books (1934,
1959)
13. Salomaa, A.: Computation and Automata. Cambridge University Press (1985)
14. Savitch, W.: How to make arbitrary grammars look like context-free grammars.
SIAM Journal on Computing 2, 174–182 (1973)
15. Schmidhuber, J., Zhao, J., Wiering, M.: Shifting Bias with Success Story Algo-
rithm. Machine Learning 28, 105–130 (1997)
16. Solomonoﬀ, R.: A Formal Theory of Inductive Inference, Part I. Information and
Control 7(1), 1–22 (1964)
17. Solomonoﬀ, R.: A Formal Theory of Inductive Inference, Part II. Information and
Control 7(2), 224–254 (1964)
18. Turing, A.: On computable numbers with an application to the Entscheuidungs-
problem. Proc. Lond. Math. Soc. 2(42), 230–265 (1936)
19. Silvescu, A., Honavar, V.: Abstraction Super-structuring Normal Forms: Towards
a Computationalist Theory of Structural Induction. Technical Report. Department
of Computer Science. Iowa State University (2011),
http://arxiv.org/abs/1107.0434
20. Wallace, Dowe: Minimum Message Length and Kolmogorov complexity. Computer
Journal (1999)

Locating a Discontinuity in a Piecewise-Smooth
Periodic Function Using Bayes Estimation
Alex Solomonoﬀ
Camberville Research Institute, Somerville, Mass., 02143
Abstract. A method is presented for locating a jump discontinuity in
an otherwise-smooth function from its ﬁrst few Fourier coeﬃcients. It is
based on a statistical model for otherwise-smooth functions and Bayes
estimation, similar to and based on the work in [3]. Numerical results
are presented, showing respectable performance, and comparing it to
the jump locating algorithm of [1].
Fig. 1. Ray Solomonoﬀat his home in Arlington, Mass., USA, June 2008, a few days
after cataract surgery. The fez was a gift, received on a visit to the North Lebanon
campus (in Koura) of Notre Dame University in Lebanon, in March 2008. Ray was
invited to visit NDU by Fouad Chedid.
1
Introduction
This paper studies the problem of determining the location of a discontinuity
in a function f(x) with, say f : [−1, 1] →R, when several of f(x)’s Fourier
coeﬃcients are known. This problem has been addressed before by Gelb and
Tadmor[1][2]. Here we present a quite diﬀerent approach to the problem, based
on Bayesian/statistical ideas.
Many phenomena of interest to mathematics and science involve functions
that are only piecewise smooth, and it is not uncommon for our information
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 351–365, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

352
A. Solomonoﬀ
about them to consist of Fourier coeﬃcients. The problem that motivated this
work is numerically approximating equations of supersonic gas ﬂow using spec-
tral methods[9]. The function might be density or temperature of the air vs.
position in space, and the jumps are shock waves. Another example is CAT
scanning of a medical patient, where the information measured by the scanner
is closely related to Fourier coeﬃcients and every boundary between one type of
tissue and another is a jump discontinuity.
In either case, the function f(x) to be reconstructed is smooth and simple,
except for small number of jump discontinuities. If f(x) were completely smooth
without discontinuities, then it could be easily approximated from a few of its
Fourier coeﬃcients by the partial sum
f(x) ≈fN(x) =
N

k=−N
ˆfkeikπx.
(1)
This partial sum converges to f(x) rapidly as N increases if f(x) is smooth. But
if f(x) has any jump discontinuities, then no matter how smooth and simple it
is away from the jumps, fN(x) converges to f(x) very slowly, see [9], section 2.2.
And yet, if f(x) is smooth and simple except for a small number of jumps,
the total complexity of it is small, and intuitively one would expect that an
accurate way of reconstructing f(x) from a few Fourier coeﬃcients must exist.
But equation (1) is not it.
Approaches for doing this reconstruction have been developed by Gottlieb,
Gelb and others, for example [11]. In 1992, I developed a system [3] for doing this
reconstruction using Bayes estimation, which was quite successful. The current
work is based on this.
Two Kinds of Probability. In the more familiar kind of probability, an object
comes from some random source, and each type of object has some probability
of emanating from the source.
In the second, the situation is deterministic, but only partly known to the
observer. Here, a probability distribution for an event is a description of the
observer’s knowledge of the situation. The canonical example is the weather
report. When it says there is a 40% chance of rain today, there is no randomness
to the weather. A 40% chance means that if the weatherman were a betting
man, he would accept a bet that it would rain if and only if the odds were 3 to
2 or higher.
Probabilities in the shock locating system are of the second type. Bayesian
analysis can be done on events with either type of probability. In the second
case, a “correct” probability distribution is any one that contains most or all
of the observer’s knowledge of the system, and in some way does not make any
further assumptions.
If all or most of the observer’s information can be encoded in the mean and
variance of the probability distribution, then a Gaussian distribution is attrac-
tive, because of the “no further assumptions” issue. In [10] it is proven that

Bayes Shock Locating
353
among probability distributions having a given mean and variance, the Gaus-
sian distribution has the largest entropy. This can be interpreted as saying that
a Gaussian makes the fewest “further assumptions”.
Relationship to Algorithmic Probability. Ray Solomonoﬀ’s [12] Algorith-
mic Probability theory is based on a somewhat similar foundation, in that the
events or observations being predicted by AP are deterministic, not random. The
Universial A Priori Probability Distribution employed in AP is derived from a
belief that the universe has structure and can be described by rules, not derived
from frequencies of events.
Function Reconstruction. In the function reconstruction algorithm of [3]
the observations were the ﬁrst n Fourier coeﬃcients of f(x), the approximate
function source S′ was an inﬁnite-dimensional Gaussian distribution, and the
thing to be estimated was f(x) itself. S′ was constructed so that typical functions
drawn from it were smooth everywhere except at one speciﬁed point. Then f(x)
was estimated in the usual Bayes way as the conditional expectation of f(x)
given the Fourier coeﬃcients, under the probability distribution of S′.
This approach is familiar, even trivial in the world of statistics and probability,
(see, for example[5]) but much less familiar in the world of numerical analysis.
It worked quite well for estimating a function. The computations were some-
what laborious, but not conceptually diﬃcult. They consisted mostly of standard
matrix operations – products, inverses, etc, and were nicely carried out by LA-
PACK subroutines.
The fact that the location of the singularity needed to be known a priori was
a nagging detail that reduced it’s usefulness. This paper presents a method for
locating the discontinuity that builds on that previous work.
Conceptually the method is simple:
1. Instead of a single Gaussian distribution, a mixture of Gaussians is used,
each identical except that the discontinuity is in a diﬀerent location.
2. When the distribution is conditioned on the Fourier coeﬃcients, each Gaus-
sian member of the mixture has an a posteriori probability.
3. This posterior probability is a probability distribution for the location of the
discontinuity.
Function reconstruction using a system such as [3] or [11] can then be done.
The simplest way to proceed is to pick the point most likely to be the discontinu-
ity, and use existing function reconstruction methods based on this assumption.
More subtle ways of working with the posterior will not be considered here.
We are only considering the situation where the function to be considered is
periodic, has exactly one jump discontinuity and is smooth otherwise. Gelb and
Tadmor’s technique had the advantage of not requiring exactly one jump, but
that is a topic for another paper. Like their work, we assume that the ﬁrst k
Fourier coeﬃcients of the function are known.

354
A. Solomonoﬀ
Table of Contents
Section 2 gives notation and background needed for the Bayes shock locater.
Section 3 describes the shock locater itself.
Section 4 gives some experimental results.
Section 5 describes the shock locating technique presented by Gelb & Tadmor
in [1][2] and compares its performance with the statistical shock locater.
Section 6 is discussion, speculations and conclusions.
2
Mathematical Preliminaries
Random Functions
We will work with random sources of objects. Some sources dispense scalars,
others vectors, still others functions f : R →R. Let S be such a random source.
Notation: The operation of getting a (particular) function from S is written as
f ←S. The notation f ∼S means that f is a random variable taken from S.
cov(f) = cov(S) is the covariance matrix of S, deﬁned as the (inﬁnite) matrix
(or linear operator) cov(f) = E

f −¯f)(f −¯f)H
. The mean of f is ¯f = E(f).
The probability, or probability density of a particular function f being dis-
pensed by S is written S(f) or p(S, f). If S depends on some parameters we
write S(α, β, . . .) and S(f; α, β, . . .) for the probability.
Finite Set vs. Density Function vs. Measure. In the case of a source from
a ﬁnite set, each element of the set has some generally-positive probability. For
a source from a ﬁnite-dimensional vector space, there is a probability density
function. For a source of functions or inﬁnite-dimensional vectors, it is a measure.
To keep from diluting the focus of the paper, most functional-analysis-related
issues will be ignored. In that respect the presentation will be very casual.
2.1
Gaussian Distributions
A ﬁnite-dimensional Gaussian distribution N(μ, C) with mean μ and covariance
matrix C has the well-known [5] probability density
p(x) = N(x; μ, C) ≡
1
1
(2π)n det C
exp{−1
2(x −μ)tC−1(x −μ)}.
(2)
It can be shown fairly simply that for any matrix M, if p(x) = N(x, μ, C),
and y = Mx then
p(y) = N(y; Mμ, M tCM).
(3)
2.2
Gaussian Random Functions
Consider a smooth, nonperiodic function f(x) deﬁned on (say) the interval
[−1, 1]. It has Legendre coeﬃcients
˜fm = (Lf)m =
 1
−1
Pm(x)f(x)dx,

Bayes Shock Locating
355
where Pm(x) is the m−th Legendre polynomial (See [8] chapter 22.) and we
call L the Legendre transform operator. Since f(x) is smooth, these coeﬃcients
decay to zero rapidly. (See, for example [9], chapter 2.)
If f satisﬁes some extremely mild smoothness conditions then f(x) can be
written in terms of ˜f:
f(x) =

L−1 ˜f
	
(x) =
∞

m=0
˜fmPm(x).
A reasonable way of specifying a probability distribution for smooth functions
would be to say that the Legendre coeﬃcients are independent Gaussian ran-
dom variables, and that each has zero mean and a speciﬁed variance σm. The
smoothness of these functions can be expressed by making the variances decay
rapidly to zero as m increases.
Then such a random function is constructed by
˜fm ←N(0, σm),
f(x) =
∞

m=0
˜fmPm(x).
This can be written in a more all-at-once way as
˜f ∼N(0, C),
where C is the diagonal matrix C = diag

{σm}∞
m=0

.
We need to choose C somehow. The choice
σm = αm, for some 0 < α < 1,
corresponds to functions that are analytic, and the distance between [−1, 1]
and the nearest singularity in the complex plane is some function of α. See [9],
sections 2.11 and 2.13 for more details.
Diagonal Covariance Matrix. The intuition behind using a diagonal co-
variance matrix comes from the Proper Orthogonal Decomposition. If random
functions are expanded in terms of the eigenfunctions of their covariance matrix,
the convergence of the expansion is faster than any other set of basis functions,
and the coeﬃcients of the expansion are uncorrelated.
If the random functions are expanded in the fastest-known-converging set of
basis functions, and the coeﬃcients are still correlated, then there is another
basis set, possessing even faster convergence. By assumption, this basis set is
unknown, and so using a covariance matrix that is not diagonal violates the “no
further assumptions” rule.
Circular Domain: There are two diﬀerent ways the domain interval [−1, 1] can
be considered. In the ﬁrst, −1 and 1 are at the opposite ends of the domain,
and functions from the probability above are smooth. Call this domain I. In the
other, we wrap the interval around a circle so that −1 and 1 touch each other at
the back of the circle, and connect them together there. Now the functions from
the probability above are periodic, and smooth except for a discontinuity at ±1.
Call this version of the domain C. So if f(1) = f(−1) then f is continuous in C.

356
A. Solomonoﬀ
2.3
Matrices
Suppose we have the (inﬁnite vector of) Legendre coeﬃcients ˜f of the function
f(x), and the vector of Fourier coeﬃcients ˆf of the same function is needed.
There is an inﬁnite matrix A∞such that ˆf = A∞˜f.
A∞is a unitary matrix. If we want only the ﬁrst few Fourier coeﬃcients of
f(x) then we use a ﬁnite strip of A∞corresponding to only the −n, . . . , n Fourier
modes:
ˆfN = A ˜f.
The matrix A was examined at length in [3]. Its elements are
Akm =
 1
−1
Pm(x)eiπkxdx = im

m + 1/2
k
Jm+ 1
2 (πk),
where k ∈−N, . . . , N and m ∈0, . . . , ∞. Jn(x) is a Bessel function of the ﬁrst
kind, (See for example, [8] chapter 10.)
So if the Gaussian probability for the Legendre coeﬃcients of a random func-
tion is
p( ˜f) = N( ˜f, 0, C),
then by (3) the probability for the Fourier coeﬃcients is
p( ˆfN = A ˜f) = N( ˆfN, 0, ACAH).
2.4
Periodic Functions and Rotation
Let Rd be the linear operator that rotates a function a distance d around C so
(Rdf)(x) = f(x −d).
By “rotating around in C” is meant that if x −d is outside the interval [−1, 1]
then we wrap around to the other side of the domain:
r(x, d) =
⎧
⎨
⎩
x −d −2
if x −d > 1
x −d + 2 if x −d < −1
x −d
otherwise
(4)
so more precisely
(Rdf)(x) = f

r(x, d)

.
This operator has an analog in Fourier space:
ˆRd ˆf = {ωdk ˆfk}k,
where ωd = eiπd. So ˆRd is an (inﬁnite) diagonal matrix
ˆRd = diag({ωdk}k).

Bayes Shock Locating
357
The point of this section is that if
p( ˆfN = A ˜f) = N( ˆf; 0, ACAH)
is the probability for the Fourier coeﬃcients of otherwise-smooth periodic func-
tions with a discontinuity at ±1, then by (3) the probability for the Fourier
coeﬃcients of functions with a discontinuity at some other location r(1, d) is
p( ˆRd ˆfN) = N( ˆRd ˆfN; 0, ˆRdACAH ˆRH
d ).
Mixtures of Gaussians. A mixture is a source whose probability is a linear
combination of the probabilities of some set of other sources.
Suppose have a set of random sources of functions {Sd}d, where d ranges over
some index set K. and a separate random source N of indices from K. Then the
probability of the mixture M({Sd}, N) is
M(f; {S}, N) =

d∈K
Sd(f)N(d).
Dispensing a function f ←M is a two-step process: 1) d ←N,
2) f ←Sd.
Mixture for the Shock Locator. The particular mixture of interest is where
Sd is a random source of periodic functions that are smooth except for a jump
at x = d and N is a random source of points d in the circle C.
Distribution of d: Given a function f drawn from M({Sd}, N), the probability
that a given value of d occurred during the drawing is given by Bayes rule:
p(d|f, M) =
p(f|d)p(d)

d′ p(f|d′)p(d′).
If we are only interested in the most likely d, then the normalizing factor in
the denominator can be ignored, giving
dmost likely = arg max
d
p(f|d)p(d) = arg max
d
Sd(f)N(d).
(5)
3
The Shock Locating Algorithm
Our source for Legendre vector of a random function with a discontinuity at ±1
is N(0, C). The Fourier coeﬃcients of the same functions come from the random
Gaussian source N(0, ACAH). The Fourier coeﬃcients of functions with a dis-
continuity at r(1, d) come from the Gaussian source Sd = N(0, ˆRdACAH ˆRH
d ).
The random function source we want is the mixture
M({Sd}d∈C, K),
where K is some source of random points on the circle. (uniform in the simplest
case)

358
A. Solomonoﬀ
Then from Bayes rule, the likelihood that a function with Fourier coeﬃcients
ˆfN came from a smooth function with a discontinuity at r(1, d) is
p(d| ˆfN) = 1
Z N( ˆfN; 0, ˆRdACAH ˆRH
d )p(d),
(6)
where Z is the normalizing factor from Bayes Rule. p(d) is a prior probability
that the discontinuity occurred at a location r(1, d). This is a user-supplied
quantity. Assuming a uniform probability distribution for d gives
p(d| ˆfN) = 1
Z N( ˆfN; 0, ˆRdACAH ˆRH
d ).
As noted before,
N( ˆfN; 0, M) =
1
1
(2π)n det M
exp{−1
2
ˆf H
N M −1 ˆfN},
where M = ˆRdACAH ˆRH
d . Note that det M does not depend on d because Rd is
unitary and det RdMRH
d = det MRH
d Rd = det M. So p(d| ˆfN) can be written as
p(d| ˆfN) = 1
Z2
exp{−1
2
ˆf H
N ( ˆRdACAH ˆRH
d )−1 ˆfN}.
As a further simpliﬁcation, ˆRd is invertible and ˆR−1
d
= ˆRH
d , so
p(d| ˆfN) = 1
Z2
exp{−1
2( ˆRH
d ˆfN)H(ACAH)−1( ˆRH
d ˆfN)}.
(7)
If we just want to ﬁnd the most likely location for the shock, then exponential
in 7 can be ignored, which gives us
xmost likely = arg min
d ( ˆRH
d ˆfN)H(ACAH)−1( ˆRH
d ˆfN)}.
(8)
This is the the point of most likely rotation, but the actual shock is on the
opposite side of the circle, which gives the statistical shock locater (SSL)
xSSL = r
,
arg min
d ( ˆRH
d ˆfN)H(ACAH)−1( ˆRH
d ˆfN), 1
-
.
(9)
Given (ACAH)−1, it takes O(n2) operations to evaluate p(d| ˆfN) for each value
of d, and many evaluations would be needed to locate the most likely d. This is
rather a lot, but faster methods seem possible and are a topic of future work.
Noise and Ill-Conditioning. It may happen that the input/observed Fourier
coeﬃcients of f have been contaminated with noise. It may also happen that
the matrix (ACAH) is badly conditioned. A modiﬁcation of our algorithm can
deal with both problems.

Bayes Shock Locating
359
Let ˆfN = ˆn + ˆfclean. The covariance matrix of ˆfclean is (ACAH). The noise
ˆn has its own covariance matrix, and if the noise is independent of the function,
the covariance matrix for the sum ˆfN is just the sum of the covariance matrices.
So
cov( ˆfN) = ACAH + cov(ˆn).
The simplest characterization of the noise is cov(ˆn) = ϵ2I. If the eigenvalues of
ACAH are {λ2}i, and some of the λ are tiny, then the corresponding eigenvalues
of the inverse matrix cov( ˆfN)−1 are 1/(λ2 + ϵ2), which can greatly relax the ill-
conditioning of cov( ˆfN) if it is ill-conditioned.
4
Experimental Results
The ﬁrst test function is one heavily used by Gelb & Tadmor in [1][2]:
fa(x) =

sin π
2 (x + 1) x ∈[−1, 0)
sin π
2 (3x −1) x ∈(0, 1]
Note that this function, in addition to having a discontinuity at x = 0, also has
a jump in the derivative at x = ±1. As such it does not fully obey the single-
discontinuity assumption that our work depends on, and our shock locater is less
likely to work well than a fully-smooth function.
Figure 2. In this picture we have used 19 Fourier modes (10 cosine + 9 sine), and
a diagonal, exponentially-decaying covariance matrix for the Legendre modes —
Cmm = αm, suitable for analytic functions; the precise degree of smoothness is
controlled by α. We have used α = 0.9, a number picked somewhat at random.
We have assumed that a bit of noise was added to the Fourier coeﬃcients, with
cov(ˆn) = 10−10I. but no noise was actually added to ˆfN. All computations
were done using double precision arithmetic. Linear algebraic computations were
done using LAPACK[6]. Bessel functions were computed using the Gnu Scientiﬁc
Library[7].
Gelb’s function fa was used as a test function, after rotating it a bit to bring
both discontinuities into the interior of the domain. The probability of disconti-
nuity was scaled so the maximum value was 1.0.
In the ﬁgure we can see that the shock locator’s computed probability has a
single, fairly narrow spike, whose peak is quite close to the actual discontinuity.
The most likely location was calculated to be x = 0.695, and the actual discon-
tinuity was at x = 0.7. This error is much smaller than the width of oscillations
(about 0.1) in the Fourier approximation of fa. The width of the peak – about
0.015 – is also much smaller than the width of the Fourier oscillations. Note that
the error in shock location is comparable to the width of the peak.
So in this case we have excellent performance, in spite of the assumption-
violating presence of the secondary discontinuity.

360
A. Solomonoﬀ
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
f(x)
probability
x
α=0.9, k=10, expected noise level=10-6
Anne’s function fa(x)
19-term Fourier approx
Probability of discontinuity
Fig. 2. Test function and calculated likelihood of discontinuity
Figure 3: Convergence. The test function
farctan(x) = tan−1 
a(x −b)

has a shear layer of width O(1/a) at x = b, and a jump discontinuity at x = ±1,
so the distance between the shear layer and the discontinuity is |1 −|b||.
This picture examines the convergence of the most likely jump location to the
true location as k is increased. k = 10, 20, 40. were used. The purpose of these
values was that there should be enough information in the Fourier coeﬃcients
to reconstruct the test function with reasonable accuracy.
Figure 3 shows that, as k increases, the accuracy of shock location increases
fairly rapidly, but it is not clear exactly how fast. For small b it seems faster
than polynomial and for b near 1 it seems slower than polynomial. Averaging
over b, it seems that doubling k gives about a 10-fold reduction in error, which
is approximately err = O(k−3.2).
Conjecture 1: If there are enough Fourier coeﬃcients to resolve the function
with reasonable accuracy, then the shock locater works well, giving fairly rapid
convergence to the true shock location, and is not too sensitive to the smoothness
parameter α. But, if there are not enough Fourier coeﬃcients to resolve the
function, then the shock locater still works, but convergence is slower, and it is
intolerant of any poor choice of α.
Conjecture 2: If k is small, then the shock locater is not fussy about the choice
of α, but as k increases, it becomes more particular.

Bayes Shock Locating
361
1e-08
1e-07
1e-06
1e-05
0.0001
0.001
0.01
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
xdisc error
b
f(x)=atan(10(x-b)), expected noise level=10-7.5
k=10, α=0.9
k=20, α=0.9
k=40, α=0.9
Fig. 3. Error in jump location for the shear layer function farctan as b and k vary
5
The Concentration Shock Locater
In [1] and [2] Gelb and Tadmor developed methods of estimating the jump func-
tion of a function from its truncated Fourier series. The jump function was
deﬁned as

Jf

(x) = lim
ϵ→0 f(x + ϵ) −f(x −ϵ).
At points of continuity Jf(x) = 0 and at jump discontinuities Jf(x) is the height
of the jump. Suppose we write Jkf as the estimated jump function, using the
−k, . . . , k Fourier coeﬃcients. They converted this into a jump locater by simply
taking the point of largest value of the estimated jump function:
xshock = arg max
x
|

Jnf

(x)|
or, if multiple jumps were expected, by taking all the points at which the esti-
mated jump function exceeded a preset threshold:
{xshock} = {x : |

Jnf

(x)| > τ}.
They considered several diﬀerent approximate jump functions, all of the form
Jnf(x) =
N

k=1
σk

cksin(kπx) + skcos(kπx)

,

362
A. Solomonoﬀ
where ck and sk are the Fourier coeﬃcients:
f(x) = c0 +
∞

k=0
ck cos(kπx) + sin(kπx).
They called approximate jump functions of this type concentration ﬁlters, since
the emphasis was that the energy of the ﬁltered function was to be concentrated
near the jump.
The σk were a set of carefully-scaled and generally increasing-with-k coef-
ﬁcients. They examined many choices of σk, but one of the better-performing
choices was simply
σk = k
N ,
which is simply the Fourier spectral approximation of the derivative. The coef-
ﬁcients have to be scaled carefully to get the correct size of the jump, but for
simply locating the jump, the scaling does not matter.
This gives the Concentration Shock Locater (CSL)
xCSL = arg max
x
| d
dxFNf(x)|,
(10)
where FNf is the N-Fourier-term approximation of f.
1e-06
1e-05
0.0001
0.001
0.01
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
jump location error
b
f(x) = atan(10(x-b), expected noise level=10-7.5
k=10, statistical locator
k=20, statistical locator
k=40, statistical locator
k=10, Gelb’s locator
k=20, Gelb’s locator
k=40, Gelb’s locator
Fig. 4. Error in jump location for the shear layer function farctan as b and k vary. Both
the statistical and concentration locater are shown

Bayes Shock Locating
363
5.1
Experimental Comparison of the CSL and SSL
I expected the SSL to perform much better than Gelb and Tadmor’s very simple
locater. This did not happen, which was disappointing. Cases where the SSL
was better do exist, but it required a bit of parameter tweaking to ﬁnd them .
Convergence to the correct shock location for the SSL as k →∞appears
faster than the CSL, but for small k and diﬃcult functions, the CSL locater is
better.
It may be a resolution issue. If there isn’t enough information in the Fourier
coeﬃcients to resolve the function, the SSL is not as good as the CSL. If it isn’t
resolving the function, then it can’t really tell what is a shock and what isn’t.
The CSL somehow avoids this problem.
Figure 4. This picture displays the accuracy of shock location for a function
with a moderate-thickness shear layer, f(x) = tan−1 (10(x −b)). We show both
the statistical locater and CSL for k = 10, 20, 40, to give some picture of the
convergence behavior. The picture is rather busy, but
1. For small k, the concentration shock locater is better.
2. The statistical locater converges more quickly than the CSL, and for larger
k it is better.
3. As the shear layer moves towards the shock (b increasing) the statistical
locater gets better compared to the concentration locater.
-2.5
-2
-1.5
-1
-0.5
0
0.5
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-0.5
0
0.5
1
1.5
2
f(x)
probability
x
f(x) = atan(10(x-.4)-1.3x, k=20, expected noise level=10-7.5
0.0
f(x)
Fourier approximation
statistical locator
Gelb’s locator
Fig. 5. Shock locater output for the troublesome function. Both the statistical and
concentration locater are shown.

364
A. Solomonoﬀ
4. Both shock locator’s get worse, in absolute terms, as the shear layer moves
towards the shock.
5. Neither locater is dramatically better than the other.
Figure 5. One test function which I expected to give the CSL problems was a
function with a tall thin shear layer and a much tinier jump discontinuity:
ftrouble(x) = tan−1(a(x −b)) −cx,
(11)
with c chosen to cancel most, but not all of the jump discontinuity at x = ±1.
Figure 5 examines a function which fools the concentration shock locater but
not the statistical one. The function source smoothness parameter was set at
α = 0.9. A certain amount of parameter tweaking was needed to achieve this
case.
6
Discussion and Conclusions
Peak Widths and Scaling. For the CSL, the minimum width of a peak of
|FNf(x)| is O(1/N). A reasonable conclusion to jump to would be that the
typical error in the predicted jump location would also be O(1/N). But this is
not the case; the error is much smaller. It is clearly true that the CSL could
not possibly deal with two sharp features that were closer than O(1/N) apart,
because of the minimum peak width. It is also clearly true that the peak width
and the error of predicted shock location are unrelated. The peak width cannot
be used as an error estimator.
For the SSL, the width of a peak of p(d| ˆfN) is often much narrower than
O(1/N). There is no intrinsic limit to the narrowness of the peak. In addition, at
least some of the time, the peak width is comparable to the error of the predicted
jump location, and so the peak width can be used as an error estimator.
Resolution of the Function. I have conjectured that if the function is “re-
solved” then the statistical shock locater works well, and if it is not resolved it
works less well, but I haven’t said what that means. Presumably if there are n
Fourier coeﬃcients, and the function can be reconstructed accurately enough by
n terms of a Legendre polynomial expansion (with the domain properly rotated
around the jump), then it is “resolved”. Or it is resolved if the statistical ﬁlter[3]
can accurately reconstruct the function from n Fourier coeﬃcients.
I conjectured that if the function was not resolved, then the statistical shock
locater was fussy about requiring the correct value of α. The intuition behind
this this is that α is a piece of information relevant to the function, in addition
to the Fourier coeﬃcients. If the Fourier coeﬃcients have “enough” information,
then α is not used very much. If not, α is used more, and if it is wrong, the
output of the locater is wrong.
Gelb and Tadmor’s shock locater does not seem to require that the function
be resolved in any way. Why not? This seems like an advantage.

Bayes Shock Locating
365
Final Conclusions. We have presented a method of locating a jump in an
otherwise-smooth function. It works reasonably well and surpasses the CSL in
some situations, but does not clearly surpass it overall.
It is based on the ideas of the statistical ﬁlter[3] and since it works reasonably
well, this makes the statistical ﬁlter itself look more plausible and interesting
than it would otherwise.
References
1. Gelb, A., Tadmor, E.: Detection Of Edges in Spectral Data. Applied And Compu-
tational Harmonic Analysis 7, 101–135 (1999)
2. Gelb, A., Tadmor, E.: Detection Of Edges in Spectral Data II. Nonlinear Enhance-
ment. SIAM J. Numer. Anal. 38, 1389–1408 (2000)
3. Solomonoﬀ, A.: Reconstruction of a Discontinuous Function From a Few Fourier
Coeﬃcients Using Bayesian Estimation. J. Sci. Comput. 10(1) (1995)
4. Golub, G.H., Van Loan, C.F.: Matrix Computations, 2nd edn. Johns Hopkins Uni-
versity Press (1989)
5. Duda, R.O., Hart, P.E.: Pattern Classiﬁcation And Scene Analysis. Wiley Inter-
science (1973)
6. Anderson, E., Bai, Z., Bischof, C., Blackford, S., Demmel, J., Dongarra, J., Du
Croz, J., Greenbaum, A., Hammarling, S., McKenney, A., Sorensen, D.: LAPACK
Users’ Guide, 3rd edn. SIAM, Philadelphia (1999)
7. Galassi, M., et al.: GNU Scientiﬁc Library Reference Manual, revised 2nd edn.,
www.gnu.org/software/gsl/
8. Abramovitz, M., Stegun, I.A. (eds.): Handbook Of Mathematical Functions. Dover
Publications (1964)
9. Boyd, J.P.: Chebyshev and Fourier Spectral Methods, revised 2nd edn. Dover Pub-
lications (2001)
10. Cover, T.M., Thomas, J.A.: Determinant Inequalities Via Information Theory.
SIAM J. Matrix Anal. Appl. 9(3), 384–392 (1988)
11. Gottlieb, D., Shu, C.-W.: On the Gibbs Phenomenon IV: Recovering Exponential
Accuracy in a Subinterval from a Gegenbauer Partial Sum of a Piecewise Analytic
Function. Mathematics of Computation 64, 1081–1095 (1995)
12. Solomonoﬀ, R.: The Application of Algorithmic Probability to Problems in Ar-
tiﬁcial Intelligence. In: Kochen, M., Hastings, H.M. (eds.) Advances in Cognitive
Science. AAAS Selected Symposium Series, pp. 210–227 (1988)

On the Application of Algorithmic Probability
to Autoregressive Models
Ray J. Solomonoﬀand Elias G. Saleeby
Alfaisal University, Riyadh, Saudi Arabia
esaleeby@yahoo.com
Abstract. Typically, the ﬁrst step in carrying out predictions is to de-
velop an inductive model. In many instances, the best model is used, and
it is often selected based on certain relevant criteria that may possibly
ignore some of the model uncertainties. The Bayesian approach to model
selection uses a weighted average of a class of models thereby overcoming
some of the uncertainties associated with selecting the best model. This
approach is referred to in the literature as the Bayesian Model Averaging
(BMA) approach. It turns out that this approach has signiﬁcant over-
lap with the theory of Algorithmic Probability (ALP) developed by R.
J. Solomonoﬀin the early 1960s. The purpose of this article is to ﬁrst
highlight this connection by applying ALP to a set of nested stationary
autoregressive time series models, and to give an algorithm to compute
“relative weights” of models. This reveals, although empirically, a model
weight that can be compared with the Schwarz Bayesian Selection cri-
terion (BIC or SIC). We then develop an elementary algorithm of the
Monte Carlo type to evaluate multidimensional integrals over stability
domains, and use it to compute what we call the “trimmed weights”.
Keywords: algorithmic probability, predictive probability, model weights,
information criteria, autoregressive time series, Monte Carlo method.
1
Introduction
A general problem that we are interested in is this: given a data sequence of reals,
x1x2 · · · xn, generated by a stationary source, which could be either linear or non-
linear, we would like to try to predict its continuation using linear models only.
Often linear models are deﬁned by their coeﬃcients only. They give a prediction,
but no probability distribution. We do not intend to use models of this sort.
Note that when a nonlinear source is considered, the approximating family of
models will not contain the true generator of the data. More speciﬁcally, we are
interested in the question: will summing over linear models (as in ALP), give
better prediction than using the “best” linear model (say as selected by Akaike’s
AIC, Schwarz’s BIC, or Wallace’s MML)?
We do not address these problems fully in this report, instead, we focus on
developing an algorithm that would be useful later on in answering the prediction
problem with some degree of conﬁdence. We would like to mention that when
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 366–385, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

On the Application of Algorithmic Probability to Autoregressive Models
367
we initially started working on this project, neither of us was aware of the large
body of work available on BMA (see [9], [13]) — where problems of this sort are
addressed to diﬀerent extents. Furthermore, the order estimation problem has
been recently addressed by information theoretic approaches (see for example
[14],[7]). Despite the overlap with BMA, we think that our presentation includes
some novel aspects that are not emphasized elsewhere.
Before presenting our development, we give a brief introduction to ALP and
discuss its signiﬁcance to model selection. In the 1960s (see [23] & [24]), Ray J.
Solomonoﬀconsidered the probability that a universal computer outputs some
string x when fed with a program chosen at random. This probability was called
“Algorithmic Probability” (ALP), which is also known as the Universal a Priori
Probability or the Universal Distribution. It turned out that this probability
plays a central role in the theory of inductive inference. ALP is based on the
philosophical principles of Occam’s razor of simplicity and Epicurus’ principle of
multiple explanations. Using Bayes’s Rule and Universal Turing machines, R.J.
Solomonoﬀuniﬁed these principles in an algorithmic way. This probability was
a precursor to the deﬁnition and the quantiﬁcation of Algorithmic Complexity
that was introduced later on by Kolmogorov (see [15],[26],[8],[28]).
To carry out prediction, one needs inductive models — and there are several
types. In prediction one makes an estimate of what to occur in the future, and
assigns a probability to express the level of conﬁdence in the prediction. Suppose
an inductive model or an algorithm gives a probability for each symbol in a
sequence as a function of the previous symbols. One measure of “Merit” of this
induction algorithm is the probability that it assigns to the entire data set. This
will be the product of the probabilities assigned to each of the symbols: S =  pi.
For good prediction, we would like to have as large a value of S as possible (clearly
0 ≤S ≤1). Suppose Ri is an inductive algorithm. Ri predicts the probability of
an symbol aj in a sequence a1, a2, · · · , an by looking at the previous symbols, i.e.,
it is the conditional distribution of the unobserved quantity based on previous
symbols. Put
pj = Ri(aj|a1, a2, · · · , aj−1)
where aj is the symbol for which we want the probability. The value of aj can
vary over the entire “alphabet” of symbols that occur in the sequence. Then we
have that
Sn =
n

j=1
pj =
n

j=1
Ri(aj|a1, a2, · · · , aj−1).
Now we deﬁne the universal distribution for sequential prediction. Deﬁne the
function PM (an+1|a1, a2, · · · , an) — the weighted sum probability assigned by
ALP to the (n + 1)th symbol of the sequence (in view of the previous parts of
the sequence) by all the models. That is,
PM (an+1|a1, a2, · · · , an) =
∞

i=1
2−|Ri|SiRi(an+1|a1, a2, · · · , an).

368
R.J. Solomonoﬀand E.G. Saleeby
In other words, the ALP inductive model uses all possible models in parallel
for prediction with weights 2−|Ri|Si dependent on the ﬁgure of merit of each
model, |Ri|. 2−|Ri| is something like the a-priori probability assigned to the
model Ri, Si is the probability assigned by the Ri (the ith model) to the known
data, Ri is the probability assigned by the ith model to the predicted (n + 1)th
symbol of the sequence (see [26] for more details). In contrast, recall that the
“Maximum Likelihood” (ML) method of model selection uses S only to decide a
model. First a set of models is chosen by the statistician based on an educated
guess, then the model within that set having maximum S value is selected.
More reﬁned selection criteria for best models, that incorporate the principle of
Occam’s razor or parsimony, were developed later on by Wallace and Boulton
([27]), Akaike ([1],[2]), Schwarz ([21]), Rissanen ([20]) and others (see [3] for more
details).
2
Model Probabilities and Weights
In this section, we develop expressions for “relative probabilities” or “relative
weights” (for short we call weights) — in practice, probability ratios are of
interest (see [25]. First, we set some notation. Let Xi be the data sequence up
to and including xi, i.e., Xi = x1 · · · xi−1xi. R (y; Xn) is the probability density
at y for the element following xn , as a function of Xn (as given by model R).
For small Δ, R says that the probability that xn+1will be between y and y + Δ
is R (y; Xn) · Δ. A linear model with m + 1 coeﬃcients will be of the form
LΘ (y; Xn) =
√
2πσ2
	−1
exp
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−
6
y −
6
a0 +
m

j=1
ajxn−j
772
2σ2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(1)
This LΘ is deﬁned by m + 2 parameters, a0 · · · am and σ2. Θ denotes those
m + 2 parameters.
Consider the model R. Deﬁne
SR (x1 · · · xn) =
n

i=1
R (xi; Xi−1) .
(2)
This is the probability of the data (x1 · · · xn) given by model R. Recall that
ALP says to sum over all models, using weights proportional to the a priori
probability of each model, multiplied by probability of the known data as given
by that model — this is also the posterior probability. Since the parameters of
the models are all continuous, we will integrate instead of sum. We will give
below two diﬀerent derivations to carry out the process of integration to derive
the “weights”.

On the Application of Algorithmic Probability to Autoregressive Models
369
Note that the integrand is
C · SL⊖(x1 · · · xn) · L⊖(y; Xn) .
(3)
For now, we will consider deriving the models’ weights when the m+1 param-
eters a0, · · · , am have integration limits from −∞to +∞. The σ2 parameter will
have integration limits 0 to +∞. The “C” factor tells us that the m + 1 param-
eter models have the same a priori probability. We present two approaches for
this integration (which in fact are not that much diﬀerent) to derive the weights.
We call the ﬁrst approach the indirect method as we will not be integrating (3)
directly.
Consider a nested model space, and assume that the maximal number of lags a
model can have is p. For comparison purposes, it is desirable that all the models
use the same data.
Since SLΘ(xp+1 · · · xn) · LΘ(y, Xn) = SLΘ(xp+1 · · · xn, y), we will ﬁrst inte-
grate SLΘ(xp+1 · · · xn) — then we obtain the integral of SLΘ(xp+1 · · · xn, y) as
a special case of SLΘ(xp+1 · · · xn). Expanding SLΘ(xp+1 · · · xn), we get
n

i=p+1

(2πσ2)−1
2 exp(−
(xi −(a0 + m
j=1 ajxi−j))2
2σ2

= (2πσ2)−n−p
2
exp

−
n
i=p+1(xi −(a0 + m
j=1 ajxi−j))2
2σ2

,
(4)
the conditional probability density function given that the ﬁrst p values remain
ﬁxed. To simplify, let σ2 = V and
1
n−p
n
i=p+1(xi−(a0+m
j=1 ajxi−j))2 = f(Θ).
So (4) becomes
(2π)−n−p
2
V −n−p
2
exp(−(n −p) f(Θ)
2V
) = (2π)−n−p
2 (V e
f(Θ)
V
)−n−p
2 .
(5)
f(Θ) has a minimum equal to the mean square error of σ2
0, when Θ = >Θ. The
minimum of V e
f(Θ)
V
(and the maximum of (2)) occurs when Θ = >Θ and V =
f( >Θ) = σ2
0.
To integrate with respect to the ai components of >Θ, we will move the origin
to >a0 · · ·>am, then rotate the system so fΘ becomes of the form f0 +m
j=0 λ2
i b2
i ,
b0 · · · bm being the rotated coordinates. First centering the a0 · · · am coordinates
at the origin: Let a′
i = ai −>ai (i = 0, · · · , m). So
f(Θ) =
m

j=0
m

k=0
Aj,k a′
j a′
k + σ2
0,
(6)
where the A0,0 = 1, A0,k = Ak,0 =
1
n−pΣn
i=p+1xi−k, k > 0, and Aj,k = Ak,j =
1
n−pΣn
i=p+1xi−jxi−k for j, k > 0, and σ2
0 = f

>Θ
	
, the minimum mean square
value of f (Θ) . The matrix A is a symmetric and positive deﬁnite, since f (Θ) is

370
R.J. Solomonoﬀand E.G. Saleeby
at its minimum. So we can rotate coordinates a′
is to get the b′
is. The λ2
i are the
eigenvalues of the matrix A, and are positive and real. Substituting back into
(5), we obtain
(2π)−n−p
2
V −n−p
2
exp
,
−(n −p) (σ2
0 + m
i=0 λ2
i b2
i )
2V
-
.
(7)
Integrating each of b0 · · · bm from −∞to +∞, we have for n > p,

exp
,
−(n −p) m
i=0 λ2
i b2
i
2V
-
db =
6,
V
n −p
- m+1
2
· (2π)
m+1
2
·
m

i=0
1
λi
7
.
(8)
Deﬁne D = determinant of matrix A : D−1
2 = m
i=0 λ−1
i . Then from (8), the
integral of (7) can be written as
D−1
2 (2π)
−n+p+m+1
2
(n −p)−m+1
2
,
V
−n+p+m+1
2
· e−
σ2
0(n−p)
2V
-
.
(9)
Integrate (9) from 0 to ∞, with respect to V, we get
1
2D−1
2 (n −p)
−n+p+2
2
π
−n+p+m+1
2
σ−n+p+m+3
0
Γ
,n −p −m −3
2
-
.
(10)
As we mentioned, the probability associated with equation (10) is for the
sequence xp+1, · · · , xn. To obtain the equivalent expression for xp+1, · · · , xn, y ;
n becomes n+1, m+1 remains the same, but σ0 changes in a more complex way.
We want SLΘ(xp+1 · · · xn, y) as a function of y. To get the associated distribution
function and its variance, let
δ = y −(predicted value of xn+1) = y −
⎛
⎝>a0 +
m

j=1
>aj xn+1−j
⎞
⎠.
The ﬁrst n −p prediction errors are of mean zero, because >a0 was adjusted to
make it so. The mean of all of the n −p + 1 prediction errors will be
((n −p) · 0 + δ)/(n −p + 1) = δ/(n −p + 1).
The mean square error is the mean of the squares minus square of mean
(n −p) σ2
0 + δ2
n −p + 1
−
,
δ
n −p + 1
-2
= (n −p) σ2
0
n −p + 1
,
1 +
δ2
(n −p + 1) σ2
0
-
.
Since n →n + 1, the power of σ2
0 in equation (10) becomes −n+p+m+2
2
. We
take our modiﬁed σ2
0 to this power to obtain

On the Application of Algorithmic Probability to Autoregressive Models
371
,
n −p
n −p + 1 σ2
0
- −n+p+m+2
2
·
,
1 +
δ2
(n −p + 1) σ2
0
- −n+p+m+2
2
.
If
δ2
(n−p+1) σ2
0 << 1, the second factor is exp

−(n−p−m−2) δ2
2(n−p+1)σ2
0
	
, which is a
normal distribution of variance σ2
0
n−p+1
n−p−m−2. Deﬁne
σ2
final = σ2
0
n −p + 1
n −p −m −2.
σ2
final is not exact, but be made more exact if need be.
Omitting factors involving only constants and functions of n and p only, (10)
becomes
D−1
2 π
m
2 Γ
n −p −m −2
2

σ−n+p+m+2
0

n −p
n −p + 1
 m
2 σ0

2πσ2
final
−1
2
√n −p −m −2 e
−
δ2
2σ2
final .
(11)
If m << 2n then

n−p
n−p+1
	 m
2
1
√n−p−m−2 cancel to ﬁrst order in m
n . This gives
for (11)
D−1
2 π
m
2 Γ
,n −p −m −2
2
-
σ−n+p+m+3
0

2πσ2
final
−1
2 e
−
δ2
2σ2
final .
(12)
Next, consider Γ
 n−p−m−2
2

. Rewrite using n in place of n −p, and look at
 n+2
2

!
 n−m−4
2

! /
 n+2
2

! We neglect the ﬁrst
 n+2
2

! because it is canceled by
normalization. Now
 n−m−4
2

!
 n+2
2

!
=
.,n −m −4
2
+ 1
- ,n −m −4
2
+ 2
-
· · · n + 2
2
/−1
,
(13)
If m
2 is small, we employ a simple approximation. The product
,n −m −4
2
+ 1
- ,n −m −4
2
+ 2
-
· · ·
,n + 2
2
-
(14)
contains m+4
2
factors. The (arithmetic) mean of the factors is n
2 −m
4 ; and a rough
approximation to (14) is then
 n
2 −m
4
 m+4
2 . For m << n this is
n
2
	 m+4
2
·

1 −m
2n
	 m+4
2
≈
n
2
	2
·
n
2
	 m
2 · e−m(m+4)
4n
.
(15)
Discarding the
 n
2
2 factor and noting that we need the reciprocal of (14) we
obtain

372
R.J. Solomonoﬀand E.G. Saleeby
n−m
2 · 2
m
2 · e
m2
4n + m
n
(16)
Replacing n by n −p, and substituting this for
 n−m−4
2

! in (12), we obtain
e
m2
4(n−p) +
m
(n−p) D−1
2 (2π)
m
2 (n −p)−m
2 σ−n+p+m+3
0

2πσ2
final
−1
2 e
−
δ2
2σ2
final .
(17)
Now we deﬁne what we call the “weight” for a model with m−lags as
w (m) = e
m2
4(n−p)+
m
(n−p) D−1
2 (2π)
m
2 (n −p)−m
2 σ−n+p+m+3
0
.
(18)
We note here that we can also obtain, more directly, a derivation of an equiv-
alent of (10), which makes use of the so called General Gaussian Integral (see,
http://web.mit.edu/jaimevrl/www/Quantum/QFT/Gaussian%20Integrals.pdf)
Z (A, b) =

Rn exp
⎛
⎝−
n

i,j=1
1
2xiAijxj +
n

i=1
bixi
⎞
⎠dnx
= (2π)
n
2 (det A)−1
2 exp
⎛
⎝
n

i,j=1
1
2biΔijbj
⎞
⎠,
where Δ = A−1. This integral is also obtained by orthogonal diagonalization of
the matrix A and a translation based on the minimum of the quadratic form in
the exponential. The integral of (4)
(2πσ2)−n−p
2

exp

−
n
i=p+1(xi −(a0 + m
j=1 ajxi−j))2
2σ2

da0,
where a0 = (a0, a1, · · · , am)t, can be written as
(2πσ2)−n−p
2
exp
6
−
n
i=p+1 x2
i
2σ2
7 
exp
,−at
0A0a0
2σ2
+ St
0a0
σ2
-
da0,
(19)
where
A0 :=
⎡
⎢⎢⎢⎣
n
i=p+1 1
n
i=p+1 xi−1
n
i=p+1 xi−2
n
i=p+1 xi−1
n
i=p+1 x2
i−1
n
i=p+1 xi−1xi−2
n
i=p+1 xi−2
n
i=p+1 xi−1xi−2
n
i=p+1 x2
i−2
...
· · ·
⎤
⎥⎥⎥⎦
(m+1)×(m+1)
,
and
S0:=

n
i=p+1 xi
n
i=p+1 xi−1xi
n
i=p+1 xi−2xi · · ·t .

On the Application of Algorithmic Probability to Autoregressive Models
373
Now upon integrating (19), making use of the normal equations, and noting
that A−1
0 S0 = >a0, we obtain
(2πσ2)−n−p
2

σm+1(2π)
m+1
2
(det A0)
1
2

exp
6
−n
i=p+1 x2
i
2σ2
7
exp
,St
0>a0
2σ2
-
.
(20)
Next, let V := σ2 (as we did above). Then (2.20) becomes
(2π)
−n+m+p+1
2
(det A0)
1
2
V
−n+m+p+1
2
e
−n
i=p+1 x2
i+1+St
0
a0
2V
.
(21)
Setting A = −n+m+p+1
2
and B =
−n
i=p+1 x2
i +St
0a0
2
, then (21) reduces to
(det A0)−1
2 (2π)A V Ae−B
V .
(22)
Integrating (22) over V
from 0 to ∞, and making use of the integral
BA+1 : ∞
0
U −A−2e−UdU = BA+1Γ(−A −1), we obtain
(det A0)−1
2 (2π)
−n+m+p+1
2
6n
i=p+1 x2
i −St
0>a0
2
7 −n+m+p+3
2
Γ
,n −m −p −3
2
-
.
Hence, after discarding the constants that are not function of m or of the
data, we get
wt (m) = π
m
2 (det A0)−1
2
⎛
⎝
n

i=p+1
x2
i −St
0>a0
⎞
⎠
−n+m+p+3
2
Γ
,n −m −p −3
2
-
.
(23)
Note that (23) is given in terms of total square error — that is, without the
normalizing constant n −p. This constant will not make a diﬀerence when we
compute weights.
3
Weight Trimming
In our analysis so far we have assumed that the a priori distribution of the
coeﬃcients is uniform, and it does not change as we use models of higher order.
However, if we know that the time series is “stable” (and often it is), then we
know that the roots of the associated characteristic polynomial must all be within
the unit circle; and correspondingly, the coeﬃcients must lie in the so called
“stability domains” in parameter space. This suggests that weights (relative
probabilities) of prediction models should be modiﬁed to take into account this
additional information. To use this, we have to know that the maximum modulus
of the roots is less than one before we see the data — to make it legitimate a

374
R.J. Solomonoﬀand E.G. Saleeby
priori information. Of course, we are aware of the objection that it may not be
possible to know for certain that the time series is stable before looking at the
data. It may be possible to deal with this issue by looking at part of the data to
determine whether the maximum modulus is less than one for the whole data,
and then use this as a priori information for the remaining part. However, at this
point in our development, we will address the computational issues in ﬁnding
the weights assuming that the time series given is stationary.
In order to evaluate the integrals over the stability domains, we develop in
this section a Monte Carlo type method. We begin by showing how this method
works for computing the hypervolumes V (k) for the stability domains Dk. We
start with S = 0. From the origin of the k−dimensional coeﬃcient space, we
draw a random ray outward until it hits the “wall”. The “wall” is where the
largest root modulus is 1. If the length of this ray from the origin to the wall is
R, we then do
S = S +
 R
0
2π
k
2 xk−1
 k
2 −1

! dx.
(24)
If we select this random ray N times and update S each time, then, if N is
large, the hypervolume is about S/N.
Now suppose we have a multivariate distribution. With each ray we choose,
the distribution will be some function of x, the distance from the origin. Say this
function is M (x) along a particular ray — then our updating equation is
S = S +
 R
0
2π
k
2 xk−1
 k
2 −1

! M (x) dx.
(25)
As before, the ﬁnal integral approximation will be S/N.
Now we describe a somewhat ad-hoc procedure on how to evaluate (25). At
this stage, this procedure should be considered experimental. First, ui, 1 ≤i ≤k,
random variables were generated from the normal distribution with mean zero
and variance one. Divide ui, 1 ≤i ≤k, by the square root of the sum of squares to
obtain (us
1, · · · , us
k) . These vectors are uniformly distributed on the unit sphere
(see [12], Ch.3). We then multiply these vectors by a constant h to obtain a
vector of coeﬃcients c = (c1, · · · , ck) = h (us
1, · · · , us
k) . We call this factor h the
“tuning factor”. h was determined empirically in order for us to obtain a fairly
accurate estimate of the volume. This is possible as we know in advance the
volumes of the stability domains, and so we can “shoot” to get close to the exact
value of the volume. This shooting process seems to be necessary as the method
expressed in (25) is somewhat rigid, and one cannot expect a good estimate for
h = 1 by using relatively small size samples (see Table 2 below). This is so,
as in dimension two say, we are trying to capture the area of a triangle as the
average area of a covering by a certain set of discs. It turns out, an appropriate
value of h can be determined that enables us to obtain an approximate value
for V (k) , and integrals over this volume, using a single run (or a single shot)
if there is a need. We mention here that we have tried other schemes (of the
acceptance/rejection type) for generating the c′s. These c′s were only perhaps

On the Application of Algorithmic Probability to Autoregressive Models
375
approximately uniformly distributed on a sphere with a suitable radius chosen
to reduce the rejection rate — yet we were still able to carry out the shooting
successfully with a value of h ≃1. Now, a recurrence formula for V (k) was
obtained by Fam ([6], see also [18])
V (2k + 1) = 2kV (2k) V (2k −1)
(2k + 1) V (2k −2) , V (2k) = V 2 (2k −1)
V (2k −2) ,
k = 2, 3, · · · , under the initial conditions V (1) = 2, V (2) = 4, and V (3) = 16/3.
D. Piccolo ([19]) (cited in [FDV]) also have given a recursion to compute V (k) .
Now we describe how the values of R in (24) and (25) were determined. Take
one of the vectors c generated above and call it c1. Solve the characteristic
polynomial with coeﬃcients c1 for all roots. Determine the maximum modulus
value of the roots and denote it by r1. Multiplying each ci by ri scales the
zeros of a polynomial by r. Pick a constant rt (typically > 1) and generate
another k−dimensional vector c2, where c2
i = c1
i ri
t. Note that the calculations
were not sensitive to the value rt chosen. Now solve the characteristic polynomial
associated with c2, then ﬁnd the maximum modulus of its roots and denote
it by r2. Then employ linear interpolation/extrapolation to ﬁnd the value of
rp := 1 + (1 −r1) rt−1
r2−r1 . Compute rr = k
i=1(c1
i ri
p)2 and take R = √rr (i.e.,
we are using the Euclidean norm). As a further check, and to guard against not
expected performance of the root solver, the result was veriﬁed by solving the
characteristic polynomial associated with c3, where c3
i = c1
i ri
p, to conﬁrm that
we have gotten a root with maximum modulus equal to 1 to within a tolerance
of 10−9. The rejection rate of vectors not satisfying this criterion was zero or
extremely low (≤to 1 in 2000). This tolerance can be relaxed considerably, say
to 10−3, with no signiﬁcant eﬀects. It is known that the Dk′s are contractible
along curves (and thus are simply connected). The Dk′s are not convex for k > 2
(see [6]), and the bounding hypersurfaces are ruled surfaces (see [22] ). Only the
domain contractiblity property was necessary. We remark that it may be possible
to reﬁne the process of determining R and to employ an automated procedure
to determine an appropriate value of h. Note that h is not a correction factor for
the volume itself, and that h can be tuned to allow us to obtain accurate values
of the stability domains volumes for relatively low number of sample points.
The optimal number of sample points necessary to use for each k need to be
investigated further.
Table 1 shows the results for computing one estimate of the stability domains
volumes, Vk, using j vectors; and one estimate of Gaussian type integrals Jk given
by (25) with M (x) = e−x2
2 using mc points (for each j). As a further check, the
last column Ik gives the one-shot estimate of (25) obtained by employing the
recursion expression derived in Appendix A using j vectors. The values of J2
and J3 check reasonably well against those obtained by quadrature methods over
V2 and V3 (= 2.652 8 and 3. 379 7, respectively). All the results were obtained
from a single shooting-type run for the value of h shown. The values of h were
found by trial and error, and are shown to give an idea of the scaling needed.
However, the values of h in this instance depend on the seed values used for the

376
R.J. Solomonoﬀand E.G. Saleeby
pseudo-random data generator and the number of vectors c. One way to ﬁnd
an appropriate value of h is to initially choose the seeds at random, but then
to hold them ﬁxed while tuning to determine a value of h that allow us to hit
the targeted volume. Notice that the rough estimates found for the values of the
integrals are not available to us by other means for k > 3.
Table 1. Estimates of volumes and Gaussian integrals by shooting
(j = mc = 5000)
k
2
3
4
5
6
7
8
V (k)
h
Vk
Jk
Ik
4
1.26 3.990 2.664 2.670
5.333 1.25 5.337 3.281 3.271
7.111 1.555 7.114 3.916 3.890
7.585 1.599 7.610 4.064 4.027
8.091 1.652 8.098 4.115 4.081
7.397 1.695 7.401 3.777 3.746
6.763 1.585 6.774 3.190 3.161
Next, we examine the performance of the method a bit further. Table 2 shows
the results obtained by doing shooting-on-average to ﬁnd estimates. We give
results for k = 3 and k = 6 from 25 diﬀerent runs of the program. The notations is
as above, but mean values are reported. The subscript var indicates the variance
of the estimated quantity. No signiﬁcant changes in the results were observed
after generating an orthogonal random matrix (one for each run) to multiply
each of the generated vector ci within that run. It was observed that the average
values of estimates are almost independent of the number of vectors c employed.
Tables 3 and 4, are similar to Table 2, but with diﬀerent values of j and mc.
Shooting on the average is more time consuming, and it actually becomes harder
to tune as the values of k increase. Perhaps automating the tuning process would
be very desirable. However, we observed that it is possible to use smaller j and
mc values when shooting on average.
Table 2. Average values for volumes and and Gaussian integrals
(j = 5000, mc = 1000)
k V (k) h
V k
Vk,var
Ik
Ik,var
Jk
Jk,var
3 5.333 1
4.697 0.00094 3.040 0.00107 3.053 0.00703
3 5.333 1.45 5.344 0.01198 3.287 0.00161 3.291 0.00438
6 8.091 1
4.925 0.04567 2.807 0.00246 2.816 0.02153
6 8.091 1.64 8.099 0.07614 4.044 0.00434 4.053 0.04682
Table 3. j = 5000, mc = 500
k V (k) h
V k
Vk,var
Ik
Ik,var
Jk
Jk,var
3 5.333 1.45 5.313 0.00967 3.272 0.00100 3.292 0.01325
6 8.091 1.64 8.099 0.07614 4.044 0.00432 4.038 0.09623

On the Application of Algorithmic Probability to Autoregressive Models
377
Table 4. j = 2000, mc = 500
k V (k) h
V k
Vk,var
Ik
Ik,var
Jk
Jk,var
3 5.333 1.46 5.327 0.01687 3.278 0.00192 3.293 0.01229
6 8.091 1.615 8.101 0.04203 4.036 0.01713 4.031 0.10723
Similar results for j = 10000 or 15000 were obtained. Occasionally, it was
noticed that for certain repeat runs of the program, or upon a small change in
h, one of the volumes (out of 25) could become untypically large - what can
be characterized as an “outlier estimate”. This problem appears to be origi-
nating sometimes from the root-solver for the characteristic polynomial (and it
was observed for the higher k values). The root solver employed the Jenkins-
Traub algorithm that may not in certain instances be able to ﬁnd all the roots.
Therefore, one has to guard against these exceptional cases and remove them.
Changing the initial seed values helped to solve the problem in the trials we
did. Perhaps a more reﬁned implementation of the algorithm, or running the
program on a machine with a higher precision platform, may help reduce the
variances of the estimates obtained further.
Now we give the derivation of the trimmed weights. Let aj = cja1, j =
2, · · · , m, c1 := 1. Then from (4), the term n
i=p+1(xi −(a0 + m
j=1 ajxi−j))2
can be written, for m ≥1, after expansion and completing squares, as
Q1
,
a1 −Q2
Q1
-2
−Q1
,Q2
Q1
-2
+ Q3,
where
Q1 (m) =
n

i=p+1
⎛
⎝c2
0 + 2c0
m

j=1
cjxi−j +
m

j=1
c2
jx2
i−j + 2
m

j=2
cj−1xi−j+1
m

k=j
ckxi−k
⎞
⎠
Q2 (m) =
n

i=p+1
xi

c0 +
m

j=1
cjxi−j

, and Q3 =
n

i=p+1
x2
i .
So (4) can be written as
SLθ =

2πσ2−n−p
2
exp

−1
2σ2
6
Q1
,
a1 −Q2
Q1
-2
−Q1
,Q2
Q1
-2
+ Q3
7
. (26)
First, we integrate (26) over σ2 from 0 to ∞. Let B =
1
2
,
Q1

a1 −Q2
Q1
	2
−
Q1

Q2
Q1
	2
+ Q3
-
, A = −n−p
2 , and V = σ2. Let U = B
V , and so dV = −B
U2 dU.
Then we obtain
 ∞
0
SLθdσ2 = (2π)A
 ∞
0
V Ae−B
V dV = (2π)
−n+p
2
B
−n+p+2
2
Γ
,n −p −2
2
-
.

378
R.J. Solomonoﬀand E.G. Saleeby
Therefore, the integral over the coeﬃcient space, representing the trimmed
weight (un-normalized), can be expressed as
Q(m)Q5
 R
0
am−1
1
.
Q1

a1 −Q2
Q1
	2
+ Q4
/ n−p−2
2
da1,
(27)
where Q4 (m) = Q3 −Q1

Q2
Q1
	2
, Q5 = π
−n+p
2
Γ
 n−p−2
2

, and Q (m) =
π
m
2
( m
2 −1)!.
Using Integral Tables, it is possible to represent (27) in terms of some closed form
expressions, some of which involve ﬁnite sums. However, the resulting expressions
seems to be complex and a recurrence like that given in Appendix A does not
appear to be easy to derive. Therefore, at this point, these expressions seem to
oﬀer no advantages over evaluating (27) directly.
4
Numerical Example
In this section we demonstrate how to compute the weights described above by
considering an example.We generated a long enough stationary time series form
the linear model
xn+1 = 0.2xn + 0.1xn−1 + 0.5xn−2 + 1.0 ∗[noise ∼i.i.d.N(0, 1)].
(28)
with initial conditions: x1 = x2 = x3 = 0. We discarded the ﬁrst 200 points
to reduce the eﬀect of the initial conditions. We ﬁxed a bound on the order
of the models — p = 7 in this example, and took n = 100. We ﬁtted seven
linear autoregressive models 1 ≤m ≤p, each has m + 1 coeﬃcients and m
lags. We found the best ﬁt coeﬃcients using the maximum likelihood method
(or least squares) and found the corresponding mean square error for each of the
models. To compute the weights wt(m), we used (23) with n →n + 1, coupled
with a recursion procedure to ﬁnd the normalized weights. In implementing
this recursion procedure, we had to compute the ratio of determinants for two
successive models. In particular, we observed that the matrix A(m) is obtained
from A(m + 1) by deleting the (m + 1)th row and the (m + 1)thcolumn, and so
we have the ratio
det(A(m))
det(A(m + 1)) = b(m+1),(m+1),
where b(m+1),(m+1) is the (m + 1) × (m + 1) element of the inverse matrix of A.
Note that this element has already been computed when we found the coeﬃcients
— we employed an LU-decomposition to ﬁnd the inverse matrix. In other words,
we can compute the weights without computing determinants. To complete this
recursion procedure, the following equation can be used to compute the ratio of
gamma functions to a suﬃcient accuracy

On the Application of Algorithmic Probability to Autoregressive Models
379
(x + 0.5)! ≃x!
,
x + 0.75 +
1
32x
- 1
2
.
For comparison purposes, we computed for each model the Akaike Information
Criterion (AIC) and its correction (AICC) (which performs better for small n),
and the Schwarz Bayesian Criterion (BIC). Sample results are shown in Table
5. The normalized weights, Wtn(m) and Wn(m) are based on (23) and (18),
respectively.
Next we describe ALP relative weight for model selection. We used empirical
evidence to deﬁne ALP and to make comparisons with BIC. Consider again the
expression for w (m) given in (18). For convenience we drop p.
w(m) = e
m2+4m
4n
D−1
2 (2π)
m
2 n−m
2 σ−n+m+3
0
.
(29)
First we note that the term e
m2+4m
4n
≈1 when m ≪n. Then based on
the results from the analysis of data from example (28), we ﬁrst observe that
det(A(m + 1)) ≈det(A(m))S(m), where S(m) is the total square error. We also
observed that det (A (m)) /(S (m))m is approximately a constant times m for
m > 1. So D−1
2 · σm
0
is approximately a constant times √m. Based on these
observations, the weight in (29) reduces to the form
 n
2π
	−m
2 m
c
2 σ−n+3
0
,
where c is a constant. We also observed that σ0 varied little between contested
models, and so we can neglect the σ3
0 factor. Consequently, we arrived at what
we have called in the Tables 5 below the ALP relative weights. After writing k
in place of m, we obtain
ALP = n ln

σ2
+ k ln
 n
2π
	
+ c ln (k) .
(30)
Recall that AIC = n ln

σ2
+ 2k, AICC = n ln

σ2
+
2kn
n−k−1, and BIC =
n ln

σ2
+ k ln (n) (see [3]). Other criteria, the minimum message length MML
(see [27],[28]), and the minimum description length MDL (see [20]), can be
considered as derivative of ALP (see [25] ). MDL in the large-sample limit is
equivalent to BIC. Both MDL and MML have good properties but diﬀer from
ALP in that they select the best model from a space of computable models
instead of a weighted sum of models (see [26]). The study in [7] shows good per-
formance of an MML estimator when applied to stationary and non-stationary
autoregressive models. The value of the constant c in (30) was determined em-
pirically and for our example, it can be taken to be either 1, or equally well, it
can be chosen equal to 1
2. It was observed that c ln (k) varies relatively slowly,
and thus it was possible to drop this term from expression of ALP in (30). So
we end up with

380
R.J. Solomonoﬀand E.G. Saleeby
ALP = n ln

σ2
+ k ln
 n
2π
	
.
(31)
From (31) we see that the ALP-weight becomes equivalent to BIC selection,
but with n divided by 2π in the second term. Recall also that BIC is consistent.
This suggests that ALP weights could give consistent choices of model orders.
We observe from looking at other examples, that BIC, ALP and the normalized
weights Wtn (m) and Wn (m) , tend to select a lower order model more often
than AIC does. ALP seems to favor a more complex model than BIC does (as
expected from its form). Recall however that the idea of choosing a best model
is not what ALP advocates. In generating the data in Table 5, we set k = m + 2
(m + 1 coeﬃcients and σ2, m = number of lags), and replaced n by n −p. Some
authors/programs use k = m + 1 — the number of ﬁtted parameters. The ALP
column in Table 5 is based on (31).
Now, assuming “stability”, we take the V (k) ′s as a set of probabilistic weights
and divide the probabilities of the models by V (k). This is possible as we only
use a ﬁnite number of them, and moreover, the normalization constant would
be irrelevant in most practical applications. We start out with equation (2.13)
of Fam (see [6]) that gives an approximation for V (k) . The approximation has
11.6%, −3.3% and 4.9% error for k = 1, 2 and 3 respectively, and the error is less
than 1% for k > 13. Changing to base e, Fam’s equation becomes
V (k) = 0.5419005k−k
2 −1
4 e1.415735k.
(32)
Using (31), then ALP over V (k) is given by ALPV = σ−n  0.37023k
n
 k
2 k
1
4 , or
equivalently as
ALPV = n ln(σ2) −k ln
,0.37033k
n
-
−1
2 ln k.
(33)
If we make use of (30), it may be possible to drop the last term from (33) as
well. The results from (33) are also shown in Table 5.
Table 5. Information selection criteria and ALP relative weights, for n = 100
lags AIC
AICC
BIC
ALP
Wtn
Wn
ALPV
1
4.8409
5.1106
12.4386 6.9251
0.00195 0.00209 11.5744
2
6.4696
6.9241
16.5999 9.2485
0.00061 0.00067 14.3362
3
−8.7851 −8.0954 3.8779 −5.3115 0.56549 0.56401 −0.0058
4
−6.9136 −5.9368 8.2820 −2.7452 0.15790 0.15902 2.25974
5
−5.9410 −4.5233 11.8872 −0.9779 0.06392 0.06471 4.2483
6
−8.8725 −7.1582 11.3883 −3.3147 0.16484 0.16403 1.6621
7
−6.9908 −4.8221 15.8026 −0.7382 0.04528 0.04546 3.8717
Next, we compute and present the normalized trimmed weights based on
(27), with n →n + 1, and utilizing the MC method described in Section 3. Note
that no parameter estimation was needed to compute these relative weights. We
use the same notation as in Section 3. j was ﬁxed at 5000 points for each lag.

On the Application of Algorithmic Probability to Autoregressive Models
381
Note that the dimension of the stability domain used exceeds the number of lags
by 1 — to account for the intercept a0. The estimates of values of the normalized
trimmed weights obtained are shown in Table 6. Column ‘T wt.1’ was computed
using mc = 5000, while column ‘T wt.2’ was computed with mc = 1000. Note
that by a simple change of variable in (27), the mc points used to evaluate the
integral over any ray need only be generated once from the uniform distribution
on [0, 1] . Thus the estimates of these relative weights are not so sensitive to the
value of mc used. All the estimates in Table 6 were obtained using a single-shot
calculation, and can be considered as rough estimates. These rough estimates
are perhaps potentially good enough estimates for preliminary model selection
and model screening purposes.
Table 6. Normalized trimmed weights estimates, for n = 100
lags T wt.1
T wt.2
1
0.00906 0.00931
2
0.00305 0.00343
3
0.79711 0.79493
4
0.14202 0.14311
5
0.03158 0.03163
6
0.01349 0.01386
7
0.00370 0.00373
From Tables 5 and 6, we see that the trimmed weights are sharper than the
weights derived over the whole coeﬃcient space. For this example we have that
all the selection and weight criteria are in unison — but this is not always the
case. For large values of n, the multivariate distribution gets narrower, so one
would expect that the integration limits to inﬁnity become better and better
approximations. Of course it would be of interest to carry out a simulation
study that would give the frequency of selection, or order identiﬁcation, of the
data generator by each of these criteria over a large number of models.
5
Final Remarks
As was mentioned in the introduction, our ultimate goal is to study predictions.
The next step then would be to compute for each of the models a probability
L 
Θm (xn+1, Xn), the probability, in view of L
Θm, that the value following Xn
will be xn+1. Then obtain a “Combination Model Probability” for xn+1 (and
xn+2, etc.) using the associated normalized weights. This is much like what has
been done in BMA, where the distribution of future observations is expressed
as a mixture model in which the model speciﬁc distributions are weighted by
the posterior probabilities that measure the support that the data gives to each
model. It is well known that there are a number of estimation algorithms for
ARMA processes in the literature. However, it appears that there are no deﬁni-
tive consensus in the literature on the issue of accuracy of these estimation

382
R.J. Solomonoﬀand E.G. Saleeby
procedures for model selection purposes (see [5]). Thus before making predic-
tions, the parameter estimation method for the class of models used have to be
carefully examined — some comments are given in ([7]). The method for com-
puting the trimmed weights proposed above should be viewed as experimental
at this point. Further investigation is necessary to determine the accuracy and
the eﬃciency of computing these trimmed weights more precisely. A standard
issue with Monte Carlo type methods is to devise an appropriate distribution
of the sample over the domain of the integrand. This is also an important issue
for us. To begin with, a detailed study of the empirical distribution of R and
its relationship to the size of the initial sphere about the origin (or any other
“box”), and its relationship to the distribution type put on such a sphere, need
to be carried out. It seems that once a good procedure is established to compute
the volumes accurately, estimates of the other integrals can be evaluated with
some conﬁdence. From the limited number of trials that we have carried out,
it appears that the mean value of the Gaussian type integrals were less sensi-
tive to the parameters in the procedure than the average value of the stability
domain volume itself. In addition, it is important to understand better how to
characterize stationarity using a small training sample from the data.
Finally, we would like to point out a representation of the AR model in the
space of the reﬂection coeﬃcients (also known as the partial autocorrelation co-
eﬃcients), which are encountered in the Durbin-Levinson estimation procedure
(see [4],[16] cited in [22], and [17]). The stability region in reﬂection coeﬃcient
space is an n-dimensional hypercube obtained via an invertible continuous map-
ping given in terms of a recurrence relationship. The generation of uniform sam-
ples from the stationary region is discussed in [11]. It would be of interest to
examine the use of this approach to compute the trimmed weights, and to com-
pare its performance with the more direct semi-stochastic shooting MC method
we have suggested above.
Tribute and Acknowledgement. The second author was privileged to meet
and work with Ray on this project and is honored to be his coauthor — but with
great sadness as he is no longer with us today. Ray was a true scholar with a very
gentle and warm personality and a very kind heart. It was a pleasure to work
with him. The work with Ray on this project started in the spring of 2008. Ray
sent his last message to the second author on Nov. 17, 2009. During that time,
although the authors were burdened by other commitments, they have managed
to complete the material of sections 1 & 2, and start the work on sections 3 & 4.
Ray was very enthusiastic about this work and led the research eﬀort while he
was alive. The main goal of Ray was to use ALP to investigate predictions. Ray
was very creative and very generous with his ideas and remained very sharp until
the end. At the time I met Ray at Notre Dame University (NDU), Lebanon, I also
met his wonderful wife Grace. I was fortunate to spend a few memorable evenings
socializing with them. Later on, we communicated and socialized by email, and
both Ray and Grace often inquired about my oﬃce cats Vivo and Nutnoot. I
owe much to Grace for her friendship, and for her support and encouragement

On the Application of Algorithmic Probability to Autoregressive Models
383
to ﬁnish this article. I owe also a special thanks to Ray’s nephew, Dr. Alex
Solomonoﬀ, for several helpful suggestions on the ﬁrst draft of this article. Last
but not least, I would like to thank Prof. David Dowe and the referees for their
time and helpful suggestions.
References
1. Akaike, H.: A new look at statistical model identiﬁcation. IEEE Trans. Aut. Con-
trol 19, 716–723 (1974)
2. Akaike, H.: Information measures and model selection. Bull. of Int. Stat. Inst. 50,
277–290 (1983)
3. Burnham, K.P., Anderson, D.R.: Model selection and multimodel inference: a prac-
tical information-theoretical approach, 2nd edn. Springer, N.Y. (2002)
4. Brandoﬀ-Nielsen, O., Schou, G.: On the parametrization of autoregressive models
by partial autocorrelations. J. Multivar. Anal. 3, 408–419 (1973)
5. Chen, C., Davis, R.A., Brockwell, J.P., Bai, Z.D.: Order determination for autore-
gressive processes using resampling method. Stat. Sinica 3, 481–500 (1993)
6. Fam, A.T.: The volume of the coeﬃcient space stability domain of monic poly-
nomials. In: IEEE Int. Symp. Circuits and Systems, Portland, Oregon, vol. 2,
pp. 1780–1783 (1989)
7. Fitzgibbon, L.J., Dowe, D., Vahid, F.: Minimum message length autoregressive
model order selection. In: Palanaswami, M., Chandra Sekhar, C., Kumar Venayag-
amoorthy, G., Mohan, S., Ghantasala, M.K. (eds.) International Conference on
Intelligent Sensing and Information Processing (ICISIP), Chennai, India, January
4-7, pp. 439–444 (2004)
8. Hutter, M.: Algorithmic information theory. Scholarpedia 2(3), 2519 (2007)
9. Hoeting, J.A., Madigan, D., Raftery, A.E., Volinsky, C.T.: Bayesian Model Aver-
aging: A Tutorial. Statistical Science 14, 382–401 (1999)
10. Hurvich, C.M., Tsai, C.-L.: Regression and time series model selection in small
samples. Biometrika 76, 297–397 (1989)
11. Jones, M.C.: Randomly choosing parameters from the stationary and invertibility
regions of autoregressive-moving average models. J. Roy. Stat. Soc., Series C (Appl.
Stat.) 36, 134–138 (1987)
12. Knuth, D.: The art of computer programming, Volume 2: Seminumerical Algo-
rithms, 3rd edn. Addison-Wesley (1997)
13. Kass, R.E., Raftery, A.E.: Bayes Factors. J. Amer. Stat. Assoc. 90, 773–795 (1995)
14. Liang, F., Barron, A.: Minimax strategies for predictive density estimation, data
compression, and model selection. IEEE Trans. Info. Th. 50, 2708–2726 (2004)
15. Li, M., Vit´anyi, P.: An introduction to Kolomogorov complexity and its applica-
tions. Springer, N.Y. (1997)
16. Makhoul, J.: Linear prediction: a tutorial review. Proc. IEEE 63, 561–580 (1975)
17. Monahan, J.F.: A note on enforcing stationarity in autoregressive-moving average
models. Biometrika 71, 403–404 (1984)
18. Nikolaev, Y.P.: The multidimensional asymptotic stability domain of linear discrete
systems: Its symmetry and other properties. Aut. and Rem. Control 62, 109–120
(2001)
19. Piccolo, D.: The size of the stationarity and invertibility region of an autoregressive-
moving average process. J. of Time Series Analysis 3, 245–247 (1982)

384
R.J. Solomonoﬀand E.G. Saleeby
20. Rissanen, J.: Modeling by the shortest data description. Automatica 14, 465–471
(1978)
21. Schwarz, G.: Estimating the dimension of a model. Ann. of Stat. 6, 461–464 (1978)
22. Shlien, S.: A Geometric description of stable linear predictive coding digital ﬁlters.
IEEE Trans. Info. Th. 31, 545–548 (1985)
23. Solomonoﬀ, R.J.: A preliminary report on general theory of inductive inference
(1960)
24. Solomonoﬀ, R.J.: A formal theory of inductive inference. Inform. and Control, Part
I, 1-22, Part II 7, 224–254 (1964)
25. Solomonoﬀ, R.J.: The discovery of algorithmic probability. J. Comp. & Sys. Sci. 55,
73–88 (1997)
26. Solomonoﬀ, R.J.: Algorithmic Probability: Theory and Applications, Revision of
article. In: Emmert-Streib, F., Dehmer, M. (eds.) Information Theory and Statis-
tical Learning, pp. 1–23. Springer Science+Business Media, N.Y. (2009)
27. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Comput.
J. 11, 185–194 (1968)
28. Wallace, C.S., Dowe, D.L.: Minimum Message Length and Kolmogorov Complexity.
Computer J. 42, 270–283 (1999)
Appendix A. In this appendix we give a recursion formula that is helpful in
computing certain Gaussian type integrals. We are interested in the integral
Ik =
 R
0
xkexp(−(x −a)2
2σ2
)dx.
Let y = x −a, so x = y + a, dx = dy, then Ik =
: R−a
−a
(y + a)kexp(−y2
2σ2 )dy.
Let z2 =
y2
2σ2 , so z =
y
σ√2, y = zσ√2, y + a = zσ√2 + a, dy = dzσ√2, then
Ik =
: R−a
σ√2
a
σ√2 (z +
−a
σ√2)k(σ√2)k+1e−z2dz. Let b =
a
σ√2, then
Ik = (σ√2)k+1

R−a
σ√2
−a
σ√2
(z + b)ke−z2dz.
Let’s look at
:
(z + b)ke−z2dz, with “integration by parts” in mind: d(FG) =
FdG + GdF so FG =
:
FdG +
:
GdF. Let F = (z+b)k+1
k+1
and G = e−z2, so
(z + b)k+1
k + 1
e−z2 =
−2
k + 1

(z + b)k+1z e−z2dz +

(z + b)ke−z2dz
We note that
−2
k + 1(z + b)k+1z e−z2 =
2
k + 1(z + b)k+2e−z2 −
2
k + 1(z + b)k+1b e−z2
Substituting this value of
−2
k+1(z + b)k+1z e−z2into the previous equation and
multiplying by ez2, we can obtain that

On the Application of Algorithmic Probability to Autoregressive Models
385
(z + b)k+1
k + 1
−ez2bk+1
k + 1
=
−2
k + 1H (z, k + 2) +
2b
k + 1H (z, k + 1) + H (z, k) ,
where
H (z, k) = ez2  z
0
(ε + b)k e−ε2dε, H (z, 0) = ez2  z
0
e−ε2dε = 1
2
√πez2erf (z) ,
and
H (z, 1) = ez2  z
0
(ε + b) e−ε2dε = −1
2 + 1
2ez2 
b√πerf (z) + 1

.
As we are after Ik =

σ
√
2
k+1 : R−a
σ
√
2
−a
σ
√
2

ε +
a
σ
√
2
	k
e−ε2dε, we see that
Ik =

σ
√
2
	k+1 .
e
−

R−a
σ
√
2
2
H
,,R −a
σ
√
2
-
, k
-
−e
−

−a
σ
√
2
2
H
, −a
σ
√
2, k
-/
.
Picture: Elias receiving a certiﬁcate from Ray for completing a course that Ray
gave at NDU in March of 2008.

Principles of SolomonoﬀInduction and AIXI
Peter Sunehag1 and Marcus Hutter1,2
1 Research School of Computer Science, Australian National University, Canberra,
ACT, 0200, Australia
2 Department of Computer Science, ETH Zurich, Switzerland
{Peter.Sunehag,Marcus.Hutter}@anu.edu.au
Abstract. We identify principles characterizing SolomonoﬀInduction
by demands on an agent’s external behaviour. Key concepts are ratio-
nality, computability, indiﬀerence and time consistency. Furthermore, we
discuss extensions to the full AI case to derive AIXI.
Keywords: Computability, Representation, Rationality, Solomonoﬀ
induction.
1
Introduction
Ray Solomonoﬀ[17] introduced a universal sequence prediction method that in
[19,6,11] is argued to solve the general induction problem. [5] extended Solomonoﬀ
induction to the full AI (general reinforcement learning) setting where an agent
is taking a sequence of actions that may aﬀect the unknown environment to
achieve as large amount of reward as possible. The resulting agent was named
AIXI. Here we take a closer look at what principles underlie Solomonoﬀinduc-
tion and the AIXI agent. We are going to derive Solomonoﬀinduction from four
general principles and discuss how AIXI follows from extended versions of the
same.
Our setting consists of a reference universal Turing machine (UTM), a binary
sequence (produced by an environment program (not revealed) on the reference
machine) fed incrementaly to the agent and a loss function (or reward structure).
We give the agent in question the task of choosing a program for the reference
machine so as to minimize the loss. The loss is in general deﬁned to be a func-
tion from a pair of programs, an environment program and an agent program,
to real numbers. The loss function can be such that it is only the prediction (for
a certain number of bits) produced by the program that matters or it can care
about exactly which program was presented. A loss function of the latter kind
leads to the agent performing the task of prediction, which is what Solomonoﬀ
induction is primarily concerned with while the latter can be viewed as identi-
fying an explanatory hypothesis, which is more closely related to the minimum
message length principle [23,24,22,3] or the minimum description length princi-
ple [12,4,13]. Solomonoﬀinduction is using a mixture of hypothesis to achieve
the best possible prediction. Note that the fact that we pick one program does
not rule out that the choice is internally based on a mixture. In the case when
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 386–398, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

Principles of SolomonoﬀInduction and AIXI
387
the loss only cares about the prediction, the program is only a representation of
that prediction and not really a hypothesis.
The principles are designed to avoid stating what the internal workings of the
agent should be and instead derive those as a consequence of the demands on
the behaviour. Thus we demand rationality instead of stating explicitly that the
agent should have probabilistic beliefs and we demand time consistency instead
of explicitly stating probabilistic conditioning. The computability principle is
avoiding saying that the agent should have a hypothesis class that consists of all
computable environments by instead demanding that it deliver a computation
procedure (a program for our reference machine) that produces its prediction for
the next few bits.The indiﬀerence principle states what the initial preferences of
the agent must be, i.e. a demand for how the initial decision should be taken.
The choice is based on symmetry with respect to a chosen representation scheme
for sequences, e.g. programs on a reference machine. In other words we do not
allow the agent to be biased in a certain sense that depends on our reference
machine. Informally we state the principles as follows:
1. Computability: If we are going to guess the future of a sequence, we should
choose a computation procedure (a program for the reference machine) that
produces the predicted bits
2. Rationality: We should choose our predicted sequence such that the de-
pendence on the priorities (formalized by a reward (or loss) structure) is
consistent.
3. Indiﬀerence: The initial choice between programs only depends on their
length and the priorities (again formalized by reward (or loss))
4. Time Consistency: The choice of program does not change by a new ob-
servation if the program’s output is consistent with the oberservation and
the reward structure is still the same and concerned with the same bits
Our reasoning leading from external behavioural principles to a completely
deﬁned internal procedure can be summarized as follows; The rationality prin-
ciple tells us that we need to have probabilistic beliefs over some set of alter-
natives; The computability principle tells us what the alternatives are, namely
programs; The indiﬀerence principle leads to a choice of the original beliefs; The
time-consistency principle leads to a simple procedure for updating the beliefs
that the second principle tells us must exist, namely conditioning. In total it
leads to SolomonoﬀInduction.
We can not remove any of the principles without losing the complete speciﬁ-
cation of a procedure. The ﬁrst property is part of the set up of what we ask the
agent to do. Without the second we lose the restriction that we take decisions
based on maximum expected utility with respect to probabilistic beliefs and one
could then have an agent that always chose the same program (e.g. a very short
one). Without the third principle we could have any apriori beliefs and without
the fourth the agent could after a while change its mind regarding what beliefs
it started with.

388
P. Sunehag and M. Hutter
1.1
Setup
We are considering a setting where we give an agent a task that is deﬁned by a
reference machine (a UTM) , a reward structure (or loss function if we negate)
and a binary sequence that is presented one bit at a time. The binary sequence
is generated by a program for the reference machine.
The agent must (as stated by the ﬁrst principle) chose a program (whose
output must be consistent with anything that we have seen in case we have
made observations) for the reference machine and then use its output (which
can be of ﬁnite or inﬁnite length) as a prediction. If we want to predict at least
h bits we have to restrict ourself to machines that output at least h bits. We will
consider an enumeration of all programs Ti. We are also going to consider a class
of reward structures Ri,j. The meaning is that if we guess that the sequence is
(as the output of) Ti and the actual sequence is Tj, then we receive reward Ri,j.
Note that for any ﬁnite string there are always Turing machines that computes
it. We will furthermore suppose that ∀i, Ri,j →0 as j →∞. This means that
we consider it to be a harder and harder task to guess Tj as j gets really large.
This assumption is not strictly necessary as we will discuss later.
1.2
Outline
Section 2 provides background on Solomonoﬀinduction and AIXI. In Section 3
we deal with the ﬁrst two principles mentioned above about rationality and com-
putability. In Section 4, we discuss the third principle which deﬁnes a prior from
a (Universal Turing Machine) representation. Section 5 describes the sequence
prediction algorithm that results from adding the fourth principle to what has
been achieved in the previous sections. Section 6 extends our analysis to the case
where an agent takes a sequence of actions that may aﬀect its environment. Sec-
tion 7 concerns equivalence between our beliefs over deterministic environments
and beliefs over a much larger class of stochastic environments.
2
Background
2.1
Sequence Prediction
We consider both ﬁnite and inﬁnite sequences from a ﬁnite alphabet X. We
denote the ﬁnite strings by X ∗and we use the notation x1:t := x1, x2, ..., xt for
the ﬁrst t elements in a sequence x. A function ρ : X ∗→[0, 1] is a probability
measure if
ρ(x) =

a∈X
ρ(xa) ∀x ∈X ∗
(1)
and ρ(ϵ) = 1 where ϵ is the empty string. Such a function describes a priori
probabilistic beliefs about the sequence. If the equality in (1) is instead ≥and
ρ(ϵ) ≤1 then we have a semi-measure. We deﬁne the probability of seeing the

Principles of SolomonoﬀInduction and AIXI
389
string a after seeing x as being ρ(a|x) := ρ(xa)/ρ(x). If we have a loss function
L : X × X →R, we ([6]) choose, after seeing the string x, to predict
argmin
a∈X

b∈X
L(a, b)ρ(b|x).
(2)
More generally, if we have an alphabet Y of actions we can take and a loss
function L : Y × X →R we make the choice
argmin
a∈Y

b∈X
L(a, b)ρ(b|x).
(3)
2.2
The SolomonoﬀPrior
Ray Solomonoﬀ[17] deﬁned a set of priors that only diﬀer by a multiplicative
constant. We call them Solomonoﬀpriors. To deﬁne them we need to ﬁrst intro-
duce some notions about Turing machines [21].
A monotone Turing machine T (which we will just call Turing machine and
whose exact technical deﬁnition can be found in [8]) is a function from a set
of (binary) strings to binary sequences that can either be ﬁnite or inﬁnite. We
demand that it be possible to describe the function as a machine with unidi-
rectional input and output tapes, read/write heads, a bidirectional work tape
and a ﬁnite state machine that decides the next action of the machine given the
symbols under the head on the input and work tape. The input tape is read only
and the output tape is write only. We write that T (p) = x∗if output of T starts
with x when given input (program) p.
A universal Turing machine is a Turing machine that can emulate all other
Turing machines in the sense that for every Turing machine T there is at least one
preﬁx p, such that when px is fed to the universal Turing machine, it computes
the same output as T would when fed x (See [8,5] for further details).
A sequence is called computable if some Turing machine outputs it, or in other
words, if for every universal Turing machine there is a program p that leads to
this sequence being the output.
We can also deﬁne what we will call a computable environment from a Turing
machine. A computable environment is something which you (an agent) feed
an action to and the environment outputs a string which we call a perception.
We can for example have a ﬁnite number of possible actions and we put one
after another on the input tape of the machine. We wait until the previous input
has been processed and one of ﬁnitely many outputs has been produced. The
machine might halt after a ﬁnite number of actions have been processed or it
might run for ever.
Deﬁnition 1 (Semi-measure from Turing Machine). Given a Turing ma-
chine T , we let
λT (x) :=

p:T (p)=x∗
2−l(p)
(4)

390
P. Sunehag and M. Hutter
where l(p) is the length of the program (input) p and T (p) = x∗means that T
starts with outputting x when fed p, though it might continue and output more
afterwards.
If the Turing machine T in Deﬁnition 1 is universal we call λT a Solomonoﬀ
distribution. Solomonoﬀinduction is deﬁned by letting ρ in Section 2.1 be the
Solomonoﬀprior for some universal Turing machine. If U is a universal Turing
machine and T is any Turing machine there exists a constant c > 0 (namely
2−l(q) where q is the preﬁx that encodes T in U) such that
λU(x) ≥cλT (x) ∀x ∈X ∗.
(5)
The set {λT | T Turing} can be identiﬁed with [8] with all lower semi-computable
semi-measures (see [8] for deﬁnitions and proofs). The property expressed by
(5) is called universality (or dominance) and is the key to proving the strong
convergence results of SolomonoﬀInduction [18,8,5,6].
2.3
AIXI
In the active case where an agent is taking a sequence of actions to achieve
some sort of objective, we are trying to determine the best policy π, deﬁned as a
function from a history a1q1, ..., atqt of actions at and perceptions qt to a choice
of the next action at+1. The function ρ from the sequence prediction case is in
the active case of the form ρ(q1, ..., qt|a1, ..., at) and represent the probability of
seing q1, ..., qt given that we have chosen actions a1, ..., at. We can again deﬁne
a “learning” algorithm by conditioning on what we have seen to deﬁne
ρ(qt+1, ..., qt+k|q1, ..., qt, a1, ..., at+k) := ρ(q1, ..., qt+k|a1, ..., at+k)
ρ(q1, ..., qt|a1, ..., at)
.
(6)
If at = π(a1q1, ..., at−1qt−1) ∀t and q = q1, q2, ..., then we also write ρ(q|π) for
the left hand side in (6).
Suppose that we have an enumerated set of policies {πi} to choose from. Given
a deﬁnition of reward R(q) for a sequence of percepts q = q1, q2, ... that can for
example be deﬁned as in reinforcement learning by splitting qt into observation
ot and reward rt and using a discounted reward sum 
t γtrt [15,5], then we can
deﬁne
R(π) := EρR(q) :=

q
R(q)ρ(q|π)
(7)
and make the choice
π∗:= argmax
π
R(π).
(8)
If we have a class of environments {Tj} (say the computable environments)
and if ρ is deﬁned by saying that we assign probability pj to Tj being the true
environment, then we let Ri,j = R(q) if q is the sequence of perceptions resulting

Principles of SolomonoﬀInduction and AIXI
391
from using policy πi in environment Tj. Then R(πi) = 
j pjRi,j and we choose
the policy with index
argmax
i

pjRi,j.
(9)
As outlined in [5], one can choose a Solomonoﬀdistribution also over active
environments. The resulting agent is referred to as AIXI.
3
Choosing a Program
In this section we describe the setup of the second principle mentioned in the
introduction, namely rationality. The section is much briefer than what is suit-
able for the topic and we refer the reader to our companion paper [16] for a
more compherensive treatment. Rationality is meant in the sense of internal
consistency [20], which is how it has been used in [9] and [14]. We set up simple
axioms for a rational decision maker, which implies that the decisions can be
explained (or deﬁned) from probabilistic beliefs. The approach to probability
by [10,1] is interpreting probabilities as fair betting odds. There is an intuitive
similarity between our setup to the idea of explaining/deriving probabilities as
a bookmaker’s betting odds as done in [1] and [10].
Before we consider the question regarding which program we want to choose
we will ﬁrst consider the question if we are prepared to accept guessing Ti for
a given R = {Ri,j} (i.e. accepting this bet). We suppose that the alternative
is to abstain (reject) and receive zero reward. We introduce rationality axioms
and prove that we must have probabilistic beliefs over the possible sequences.
Note that for any given i, we have a sequence Ri,j in c0 (the space of real valued
sequences that converge to 0). We will set up some common sense rationality
axioms for the way we make our decisions. We will demand that a decision can
be taken for any reward structure r (Ri,j with ﬁxed i) from c0. If r is acceptable
and λ ≥0 then we want λr to be acceptable since this is simply a multiple
of the same. We also want the sum of two acceptable reward structures to be
acceptable. If we cannot lose (receive negative reward) we are prepared to accept
while if we are guaranteed to gain we are not prepared to reject it. We cannot
remove any axiom without losing the conclusion.
Deﬁnition 2 (Rationality). Suppose that we have a function z : c0 →{−1, 1, 0}
deﬁning the decision reject/accept/either (−1/1/0) and Z = {r ∈c0 | z(r) ∈
{0, 1}}.
1. z(r) ∈{0, 1} if and only if z(−r) ∈{−1, 0}
2. r, s ∈Z, λ, γ ≥0 then λr + γs ∈Z
3. If rk ≥0 ∀k then r ∈Z while if rk > 0 ∀k then z(r) = 1.
The following theorem connects our Rationality axioms with the Hahn-Banach
theorem [7] and concludes that rational decisions can be described with a positive
continuous linear functional on the space of reward structures. The Banach space
dual of c0 is ℓ1 which gives us a probabilistic representation of underlying beliefs.

392
P. Sunehag and M. Hutter
Theorem 1 (Linear Separation). Given the assumptions in Deﬁnition 2 there
exists a positive continuous linear functional f : c0 →R deﬁned by f(r) =

j rjpj where r = {rj}, pj ≥0 and 
j pj < ∞, such that
{x | f(r) > 0} ⊆Z ⊆{r | f(r) ≥0}.
(10)
Proof. The second property tells us that Z and −Z are convex cones. The ﬁrst
and third property tells us that Z ̸= Rm. Suppose that there is a point r that
lies in both the interior of Z and of −Z. Then the same is true for −r according
to the ﬁrst property and for the origin. That a ball around the origin lies in
Z means that Z = Rm which is not true. Thus the interiors of Z and −Z
are disjoint open convex sets and can, therefore, be separated by a hyperplane
(according to the Hahn-Banach theorem) which goes through the origin (since
according to the ﬁrst and third property z(0) = 0). The ﬁrst property tell us that
Z ∪−Z = Rm. Given a separating hyperplane (between the interiors of Z and
−Z), Z must contain everything on one side. This means that Z is a half space
whose boundary is a hyperplane that goes through the origin and the closure ¯Z
of Z is a closed half space and can be written as {r | f(r) ≥0} for some f in the
Banach space dual c′
0 = ℓ1 of c0. The third property tells us that f is positive.
Theorem 1 also leads us to how to choose between diﬀerent options. If we
consider picking Ti over Tk we will do (accept) that if Ri,· −Rk,· is accepted.
This is the case if  pjRi,j >  pjRk,j. The conclusion is that if we are presented
with Ri,j and a class {Tj} and we assign probability pj to Tj being the truth,
then we choose
argmax
i

j
Ri,jpj.
(11)
Remark 1. If we replace the space c0 by ℓ∞as the space of reward structures in
Theorem 1, the conclusion (see [16]) is instead that f is in the Banach space dual
ℓ′
∞of ℓ∞which contains ℓ1 (the countably additive measures) but also functions
that cannot be written on the form f(r) = 
j rjpj. ℓ′
∞is sometimes called the
ba space [2] and it consists of all ﬁnitely additive measures.
4
Representation
In this section we will discuss how indiﬀerence together with a representation
leads to a choice of prior weights. The representation will be given in terms
of codes that are strings of letters from a ﬁnite alphabet and it tells us which
distinctions we will apply our indiﬀerence principle to. Choosing the ﬁrst bit
can be viewed as choosing between two propositions, e.g. x is a vegetable or
x is a fruit. More choices follow until a full speciﬁcation (a code word for the
given reference machine) is reached. The section describes the usual material on
the Solomonoﬀdistribution (see [8]) in a way that highlights in what sense it is
based on indiﬀerence. The indiﬀerence principle itself is an external behavioural
principle.

Principles of SolomonoﬀInduction and AIXI
393
Deﬁnition 3 (Indiﬀerence). Given a reward structure for two alternative out-
comes of an event where we receive R1 or R2 depending on the outcome, then if
we are indiﬀerent we accept this bet if R1 + R2 > 0. For an agent with proba-
bilistic beliefs that maximize expected utility this means that equal probability is
assigned to both possibilities.
We will discuss examples that are based on considering the set {apple, orange,
carrot} and the representation that is deﬁned by ﬁrst separating fruit from veg-
etables and then the fruits into apples and oranges.
Example 1. We are about to open a box within which there is either a fruit or
a vegetable. We have no other information (except possibly, a list of what is a
fruit and what is a vegetable).
Example 2. We are about to open a box within which there is either an apple,
or an orange or a carrot. We have no other information.
Consider a representation where we use binary codes. If the ﬁrst digit is a
0 it means a vegetable, i.e. a carrot. No more digits are needed to describe
the object. If the ﬁrst digit is a 1 it means a fruit. If the next digit after the
1 is a 0 its an apple and if it is a 1 its an orange. In the absence of any other
background knowledge/information and given that we are going to be indiﬀerent
for this choice, we assign uniform probabilities for each choice of letter in the
string. For our examples this results in probabilities Pr(fruit) = Pr(vegetable) =
1/2. After concluding this we consider the next distinction and conclude that
Pr(apple|fruit) = Pr(orange|fruit) = 1/2. This means that the decision maker
has the prior beliefs Pr(carrot) = 1/2, Pr(apple) = Pr(orange) = 1/4.
An alternative representation would be to have a trinary alphabet and give
each object its own letter. The result of this is Pr(apple) = Pr(orange) =
Pr(carrot) = 1/3, Pr(fruit) = 2/3 and Pr(vegetable) = 1/3.
The following formalizes the deﬁnition of a code and a preﬁx free code. Since
we are assuming that the possible outcomes are never special cases of each other
we need our code to be preﬁx free. Furthermore, Kraft’s inequality says that

c∈C 2−length(c) ≤1 if the set of codes C is preﬁx free.
Deﬁnition 4 (Codes). A code for a set A is a set of strings C of letters from
a ﬁnite alphabet B and a surjective map from C to A. We say that a code is
preﬁx-free if no code string is a proper preﬁx of another.
Deﬁnition 5 (Computable Representation). We say that a code is a com-
putable representation if the map from code-strings to outcomes is a Turing
machine.
In the deﬁnition below we provide the formula for how a binary representation
of the letters in an alphabet leads to a choice of a distribution. It is easily
extended to non-binary representations.

394
P. Sunehag and M. Hutter
Deﬁnition 6 (Distribution from representation). Given a binary preﬁx-
free code for A (our possible outcomes), the expression
wa =

c code for a
2−length(c), a ∈A
deﬁnes a measure over A.
Though the formula in Deﬁnition 6 uniquely determines the weights given a
representation, there is still a very wide choice of representations. We are going
to deal with this concern to restrict ourself to the class of universal represen-
tations with the property that given any other computable representation, the
universal weights are at least a constant times the weights resulting from the
other representation. See [17,8,5] for a more extensive treatment. These universal
representations are deﬁned by having a universal Turing machine (in our case
the given reference machine) as the map from codes to outcomes.
Deﬁnition 7 (Universal Representation). If a universal Turing machine is
used for deﬁning the map from codes to outcomes we say that we have a universal
(computable) representation.
The weights that result from using a universal representation wU
a satisfy the
property that if wa are the resulting weights from another computable represen-
tation, then there is C > 0 such that wU
a ≥Cwa ∀a ∈A. This follows directly
from the universality of the Turing machine, which means that any other Turing
machine can be simulated on the universal one by adding an extra preﬁx (inter-
preter) to each code. That is, feeding ic to the universal machine gives the same
output as feeding c to the other machine. The constant C is 2−length(i).
Theorem 2. Applying Deﬁnition 6 together with a representation of ﬁnite strings
based on a universal Turing machine gives us the Solomonoﬀsemi-measure.
Proof. Given a universal Turing machine U we create a set of codes C from all
programs that generate an output of at least h bits. We let the code c ∈C
represent the ﬁnite string x ∈X ∗with l(x) = h if U(c) = x∗. We show below
that this representation together with Deﬁnition 6 leads to the Solomonoﬀdis-
tribution for the next h bits. By considering all h ≥1 we recover the Solomonoﬀ
semi-measure over X ∗.
Formally, given x ∈X ∗we let (in Deﬁnition 6) a = x and we deﬁne ρ(x) := wa
and conclude that
ρ(x) =

U(p)=x∗
2−length(p)
which is the Solomonoﬀsemi-measure.
Remark 2 (Unique Representation). Given a universal Turing machine, we could
choose to let only the shortest program that generates a certain output represent
that output, and not all the programs that generate this output. The length of

Principles of SolomonoﬀInduction and AIXI
395
the shortest program p that gives output x is called the Kolmogorov complexity
K(x) of x. Using only the shortest program leads to the slightly diﬀerent weights
wx = 2−K(x)
compared to Deﬁnition 6. Both weighting schemes are, however, equivalent
within a multiplicative constant [8].
5
Sequence Prediction
We will in this section summarize how SolomonoﬀInduction as described in
[6] follows from what we have presented in Section 3 and Section 4 together
with our fourth principle of time consistency. Consider a binary sequence that
is revealed to us one bit at a time. We are trying to predict the future of the
sequence, either one bit, several bits or all of them. By combining the conclusions
of Section 3 and 4, we can deﬁne a sequence prediction algorithm which turns
out to be SolomonoﬀInduction. The results from Section 3 tells us that if we
are going to be able to make rational guesses about which computable sequence
we will see, we need to have probabilistic beliefs.
If we are interested in predicting a ﬁnite number of bits we need to design the
reward structure in Section 3 to reﬂect what we are interested in. If we want to
predict the next bit we can let Ri,j = 1 if Ti and Tj have the same next bit and
Ri,j = −1 otherwise. This leads to (a weighted majority decision to) predicting 1
if 
j|Tj produces 1 pj > 
j|Tj produces 0 pj and 0 if the reverse inequality is true.
The reasoning and result generalizes naturally to predicting ﬁnitely many bits
and we can interpret this as minimizing the expected number of errors.
5.1
Updating
Suppose that we have observed a number of bits of the sequences. This result in
contradictions with many of the sequences and they can be ruled out. We next
formally state the fourth principle from the introduction.
Deﬁnition 8 (Time-consistency). Suppose that we are observing a sequence
x1, x2, ... one bit at a time (xt at time t). Suppose that we (at time t) want
to predict the next h bits of a sequence and our decisions (for any t and h)
are deﬁned by a function zt
h from the set of all reward structures (Rm×m where
m = 2h in the binary case) to the set of strings of length h.
Suppose that if zt
h+1(r) = y and y starts with xt+1. If it then follows that
zt+1
h
(r′) = y where r′ is the restriction of r to the strings that start with xt+1
(and we identify such a string of length h + 1 with the string of length h that
follow the ﬁrst bit) and if this implication is true for any t, r, h we say that we
have time-consistency.
Theorem 3. Suppose that we have a semi-measure ρ : X ∗→[0, 1] and that we
at time 0 (given any loss L) predict the next h bits according to
argmin
y1∈X h

y2∈X h
L(y1, y2)ρ(y2).
(12)

396
P. Sunehag and M. Hutter
If we furthermore assume time-consistency and observe x ∈X ∗, then we predict
argmin
y1∈X h

y2∈X h
L(y1, y2)ρ(xy2|x).
(13)
Proof. Suppose that there are y1, y2 and x such that ρ(xy1|x)
ρ(xy2|x) ̸= ρ(xy1)
ρ(xy2). This obvi-
ously contradicts time-consistency. In other words, time-consistency implies that
relative beliefs in strings that are not yet contradicted remains the same. There-
fore, the decision function after seeing x can be described by a semi-measure
where the inconsistent alternatives have been ruled out and the others just renor-
malized. This is what (13) is describing. The only remaining point to make is
that we have expressed (12) and (13) in terms of loss instead of reward though
it is simply a matter of changing the sign and max for min.
6
The AIXI Agent
In this section we discuss extensions to the case where an agent is choosing a
sequence of actions that aﬀect the environment it is in. We will simply replace the
principle that says that we predict computable sequences by one that says that
we predict computable environments. The environments are such that the agent
takes an action that is fed to the environment and the environment responds
with an output that we call a perception. There is a ﬁnite alphabet for the
action and one for the perception.
Our aim is to choose a policy for the agent. This is a function from the
history of the actions and perceptions that has appeared so far, to the action
which the agent chooses next. Suppose that a class {πi} of policies, a class of
(all) computable environments {Tj} and a reward structure Ri,j which is the
total reward for using policy πi in environment Tj. To assume the property that
limj Ri,j = 0 ∀i, would mean that we assume that the stakes are lower in the
environments of high index. This somewhat restrictive and there are alternatives
to making this assumption (that the reward structure is in c0) and we investigate
the result of assuming that we instead have the larger space ℓ∞(see Remark 1)
in a separate article [16] on rationality axioms and conclude that the diﬀerence
is that we get ﬁnite additivity instead of countable additivity for the probability
measure but that we can get back to countable additivity by adding an extra
monotonicity assumption. The arguments in Section 3 imply (given c0 reward
structure) that we must assign probabilities {pj} for the environment being Tj
and choose a policy with index
argmax
i

j
Ri,jpj.
(14)
This is what the AIXI agent described in [5] is doing. The AIXI choice of weights
pj correspond to the choice 2−K(ν) (as in Remark 2), but for the class of lower
semi-computable ν discussed below in Section 7.

Principles of SolomonoﬀInduction and AIXI
397
The same updating technique as in Section 5, where we eliminate the envi-
ronments which are inconsistent with what has occurred, is being used. This
is deduced from the same time-consistency principle as for sequence prediction,
just stating that the relative belief in environments that are still consistent will
remain unchanged. This leads to the AIXI agent from [5].
7
Remarks on Stochastic Lower Semi-computable
Environments
Having the belief that the environment is computable does seem like a restrictive
assumption though we will here argue that it is in an interesting way equivalent
to having beliefs over all lower semi-computable stochastic environments. The
Solomonoﬀprior is based on having belief 2−l(p) in having input program p
deﬁning the environment. We can (proven up to a multiplicative factor in [8] and
exact identity in [25]), however, rewrite this prior as a mixture 
ν wνν over all
lower semi-computable environments ν where wν > 0 for all ν. Therefore, acting
according to our Solomonoﬀmixture over computable enviroments is identical
to acting according to beliefs over a much larger set of environments where we
have randomness.
8
Conclusions
We deﬁned four principles for universal sequence prediction and showed that
Solomonoﬀinduction and AIXI are determined from them. These principles
are computability, rationality, indiﬀerence and time consistency. Computability
tells us that Turing machines are the explanations we consider for what we
are seeing. Rationality tells us that we have probabilistic beliefs over these.
Time-consistency leads to the conclusion that we update these beliefs based on
conditional probability and the principle of indiﬀerence tells us how to chose
the original beliefs based on how compactly the various Turing machines can be
implemented on the reference machine.
Acknowledgement. This work was supported by ARC grant DP0988049.
References
1. de Finetti, B.: La pr´evision: Ses lois logiques, ses sources subjectives. In: Annales
de l’Institut Henri Poincar, Paris, vol. 7, pp. 1–68 (1937)
2. Diestel, J.: Sequences and series in Banach spaces. Springer (1984)
3. Dowe, D.L.: MML, hybrid bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of the Philosophy of Science, HPS.
Philosophy of Statistics, vol. 7, pp. 901–982 (2011)
4. Gr¨unwald, P.: The Minimum Description Length Principle. MIT Press Books, The
MIT Press (2007)
5. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability. Springer, Berlin (2005)

398
P. Sunehag and M. Hutter
6. Hutter, M.: On universal prediction and Bayesian conﬁrmation. Theoretical Com-
puter Science 384, 33–48 (2007)
7. Kreyszig, E.: Introductory Functional Analysis With Applications. Wiley (1989)
8. Li, M., Vit´anyi, P.: Kolmogorov Complexity and its Applications. Springer (2008)
9. Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior. Prince-
ton University Press (1944)
10. Ramsey, F.: Truth and probability. In: Braithwaite, R.B. (ed.) The Foundations of
Mathematics and other Logical Essays, ch. 7, pp. 156–198. Brace & Co. (1931)
11. Rathmanner, S., Hutter, M.: A philosophical treatise of universal induction. En-
tropy 13(6), 1076–1136 (2011)
12. Rissanen, J.: Modeling By Shortest Data Description. Automatica 14, 465–471
(1978)
13. Rissanen, J.: Minimum description length principle. In: Sammut, C., Webb, G.
(eds.) Encyclopedia of Machine Learning, pp. 666–668. Springer (2010)
14. Savage, L.: The Foundations of Statistics. Wiley, New York (1954)
15. Sutton, R., Barto, A.: Reinforcement Learning: An Introduction (Adaptive Com-
putation and Machine Learning). The MIT Press (March 1998)
16. Sunehag, P., Hutter, M.: Axioms for rational reinforcement learning. In: Kivinen,
J., Szepesv´ari, C., Ukkonen, E., Zeugmann, T. (eds.) ALT 2011. LNCS, vol. 6925,
pp. 338–352. Springer, Heidelberg (2011)
17. Solomonoﬀ, R.: A Preliminary Report on a General Theory of Inductive Inference.
Report V-131, Zator Co., Cambridge, Ma. (1960)
18. Solomonoﬀ, R.J.: Complexity-based induction systems: comparisons and conver-
gence theorems. IEEE Transactions on Information Theory 24, 422–432 (1978)
19. Solomonoﬀ, R.J.: Does algorithmic probability solve the problem of induction?
In: Proceedings of the Information, Statistics and Induction in Science Conferece
(1996)
20. Sugden, R.: Rational choice: A survey of contributions from economics and philos-
ophy. Economic Journal 101(407), 751–785 (1991)
21. Turing, A.M.: On Computable Numbers, with an application to the Entschei-
dungsproblem. Proc. London Math. Soc. 2(42), 230–265 (1936)
22. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Information Science and Statistics. Springer (2005)
23. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11, 185–194 (1968)
24. Wallace, C.S., Dowe, D.L.: Minimum message length and Kolmogorov complexity.
Computer Journal 42, 270–283 (1999)
25. Wood, I., Sunehag, P., Hutter, M. (Non-) Equivalence of universal priors. In: Proc.
of SolomonoﬀMemorial Conference, Melbourne, Australia (2011)

MDL/Bayesian Criteria
Based on Universal Coding/Measure
Joe Suzuki
Department of Mathematics, Osaka University, Japan
suzuki@math.sci.osaka-u.ac.jp
Abstract. In the minimum description length (MDL) and Bayesian cri-
teria, we construct description length of data zn = z1 · · · zn of length n
such that the length divided by n almost converges to its entropy rate as
n →∞, assuming zi is in a ﬁnite set A. In model selection, if we knew
the true probability P of zn ∈An, we would choose a model F such
that the posterior probability of F given zn is maximized. But, in many
situations, we use Q : An →[0, 1] such that 
zn∈An Q(zn) ≤1 rather
than P because only data zn are available. In this paper, we consider an
extension such that each of the attributes in data can be either discrete
or continuous. The main issue is what Q is qualiﬁed to be an alternative
to P in the generalized situations. We propose the condition in terms
of the Radon-Nikodym derivative of P with respect to Q, and give the
procedure of constructing Q in the general setting. As a result, we obtain
the MDL/Bayesian criteria in a general sense.
Keywords: Bayesian/MDL, universal coding/measure, Markov order
estimation, feature selection.
1
Introduction
Consider feature selection: suppose we wish to estimate a set of random variables
on which another random variable depends from data actually emitted by them.
More precisely, for random variables X(1), · · · , X(m) and Y , we wish to estimate
the minimal F ⊆{1, · · · , m} such that P(Y |X(1), · · · , X(m)) = P(Y |{X(j)}j∈F )
from n data1 zn := {(x(1)
i
, · · · , x(m)
i
, yi)}n
i=1 ∈{X(1)(Ω) × · · · × X(m)(Ω) ×
Y (Ω)}n.
If we knew the probability P(zn, F) of each pair of data zn and a subset F,
then we would be able to calculate the posterior probability P(F|zn) of F given
zn. However, in many practical situations, only data zn are available. So, we
need to consider an alternative Q : Zn(Ω) →[0, 1] with 
zn Q(zn) ≤1 (not
depending on P) to the true probability P. If we can prepare such a Q, we only
choose F such that −log π(F) −log Q(zn|F) is minimized, or equivalently, one
such that π(F)Q(zn|F) is maximized, where the a prior probability π(F) of each
1 Thoughout the paper, by X(Ω) we mean the range of random variable X : Ω →R,
where Ω is the sample space.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 399–410, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

400
J. Suzuki
subset F is assumed to be available. We refer those evaluations to the minimum
description length (MDL) [10] and Bayesian criteria, respectively2
Fortunately, if each zi in zn = (zi)n
i=1 takes a value in a ﬁnite set A :=
Z(Ω), then such Q : An →[0, 1] with 
zn Q(zn) ≤1 have been developed
for compression without assuming the knowledge of the true probability such
as Lempel-Ziv codes, adaptive arithmetic codes [5] which are currently used
extensively in internet communications. In fact, any data zn can be compressed
into at most −log Q(zn) plus one bits, where the logarithm base is two, and it
is known that the quantity −1
n log Q(zn) almost surely converges to the entropy
that is the lower limit of the compression ratio when we encode zn using the
knowledge of P.
The idea of replacing the true P by such a Q to obtain a Bayesian solution has
been used thus far. Wray Buntine considered its application to construction of
classiﬁcation trees [2], Cooper and Herskovits estimated Bayesian network struc-
tures using such a Q [4], and modiﬁcation to the MDL principle and its application
to a Bayesian version of the Chow-Liu algorithm were considered in [14]. Since
then, many authors reported applications using similar techniques thus far.
However, if some attributes take continuous values, it is hard to construct such
a Q in order to identify the F given zn. Some might consider to quantize the
continuous data to take care of only discrete data. However, this only leads to
an approximation, and we do not know how to decide the sizes of the quantized
cells only from data: overestimation and underestimation may occur if the cell
sizes are too small and too large, respectively3.
In this paper, we propose how to choose an optimal F given zn in the sense
of MDL/Bayesian criteria, without assuming that the data are either discrete
or continuous. The main issue is what Q is qualiﬁed to be an alternative to P
in more general settings. We will give an answer to the problem in terms of the
Radon-Nikodym derivative [3] of P with respect to Q at the data zn.
The theory developed in this paper is partially due to Boris Ryabko’s density
estimation [11]. The idea is to prepare many histograms nested each other: one
histogram is a reﬁnement of another; for each histogram, the quantity like Q(zn)
is computed and divided by the cell volume (Lebesgue measure) of dimension n;
and the ﬁnal estimation is given by weighting those estimated density functions.
Ryabko proved the estimation is consistent [11].
Our contribution to theory is to remove the constraint that the density func-
tion should exist for universal coding. Obviously, just because a random variable
is not ﬁnite does not mean that its density function exists. For example, if the
distribution function is given by
FX(x) =
⎧
⎨
⎩
0
x < −1,
1
2,
−1 ≤x < 0
1
2 + 1
2
: x
0 h(t)dt, 0 ≤x
2 The two-part message scheme of Bayesian minimum message length (MML) [6,7]
precedes MDL by many measures.
3 Wallace et.al [7,16,17] considered a similar idea ﬁrst (strict MML), but this is for
estimating stochastic parameters.

MDL/Bayesian Criteria Based on Universal Coding/Measure
401
with
 ∞
0
h(x)dx = 1, then there exists no density function fX such that FX(x) =
 x
−∞
fX(t)dt.
As a result, the idea can be applied to many situations in estimation. In fact,
this paper proposes the general notion of the MDL/Bayesian criteria even if
coding is not possible. Besides the feature selection problem introduced above,
we illustrate how to estimate the order of Markov ergodic sources given data
sequence even if the random variables are continuous.
Section 2 introduces the notion of universal coding for ﬁnite sources. Sec-
tion 3 gives two illustrative examples by which we specify the scope of the
MDL/Bayesian criteria in this paper. Section 4 extends the notion of univer-
sal coding for general sources. Section 5 generalizes the MDL/Bayesian criteria
using the discussion in Sections 3 and 4. Section 6 summarizes the obtained
results and state future works.
2
Universal Coding
As preliminaries, we introduce the notion of universal coding which will play an
important role in this paper.
2.1
Coding
Let A be a ﬁnite set, and n ∈N := {1, 2, · · · }. We denote the set of binary
sequences of ﬁnite length by {0, 1}∗, and write |y| = m if y ∈{0, 1}m for y ∈
{0, 1}∗. We deﬁne coding c and its length lc by any mapping An →{0, 1}∗and
xn ∈An →|c(xn)| ∈N, respectively. We say that coding c : An →{0, 1}∗is
uniquely decodable if the map (n, xn) ∈N × An →c(xn) ∈{0, 1}n is one-to-one.
We say that map l : An →N satisﬁes Kraft’s inequality if

xn∈An
2−l(xn) ≤1.
It is known [5] that if c : An →{0, 1}∗is uniquely decodable, then lc satisﬁes
Kraft’s inequality, and that if l : An →N satisﬁes Kraft’s inequality, then there
exists a uniquely decodable c : An →{0, 1}∗such that l = lc.
Let Qn(xn) := 2−l(xn). Then, the problem of coding reduces to specifying
Qn : An →[0, 1] such that 
xn∈An Qn(xn) ≤1.
2.2
Sources
We say a sequence {Xi}∞
−∞(source) of random variables Xi to be ﬁnite if the
range A := Xi(Ω) of Xi is ﬁnite, and that it is stationary if for each k ∈N and
i ∈Z := {· · · , −1, 0, 1, · · ·}, P(Xi−k · · · Xi−1) = P(Xi−k+1 · · · Xi). For example,
for Xn := {Xi}n
i=1, if Xn is i.i.d (independently and identically distributed),
then P(Xn) =
n

i=1
P(Xi) and if Xn depends on a ﬁnite number of latest random

402
J. Suzuki
variables, then P(Xn|X−∞, · · · , X0) =
n

i=1
P(Xi|Xi−k · · · Xi−1), where we say
the source is Markov of order k if k is the minimal value satisfying the above
equation.
On the other hand, if {Xi}∞
−∞is stationary, there exists the limit (entropy)
H := lim
n→∞−1
n

xn∈An
P n(xn) log P n(xn). We only deal with stationary ergodic
sources, where for the deﬁnition of ergodicity in a general sense, see [3] for
example.
2.3
Universal Coding for i.i.d. Sources
If the probability P n is known, then H ≤E[−lc(Xn)] for any uniquely decodable
c, and lc(xn) := ⌈−log P n(xn)⌉(round-up) satisﬁes E[lc(Xn)] < H+1. However,
if P n is not known, those facts are not available. Instead, we can construct
Qn : An →[0, 1] such that

xn∈An
2−l(xn) ≤1, and
−1
n log Qn(xn) →H
(1)
for any stationary ergodic P n with probability one as n →∞(universal coding).
In fact, suppose P n is i.i.d. with unknown stochastic parameters. Let m := |A|
and denote by cn[x] the number of occurrences of x ∈A in xn ∈An, where
hereafter we write the cardinality of set S by |S|. Then, we can check that [8]
Qn(xn) :=
n

i=1
ci−1[xi] + 1
2
i −1 + m
2
=
Γ(m
2 )

x∈A
Γ(cn[x] + 1
2)
Γ(n + m
2 )Γ(1
2)m
(2)
satisﬁes (1), where Γ(x) :=
 ∞
0
tx−1e−tdt = (x −1)Γ(x −1) is the Gamma
function.
On the other hand, from the Shannon-McMillan-Breiman theorem [5], we
have −1
n log P n(xn) →H for any stationary ergodic P n with probability one as
n →∞, which together with (1) means
1
n log P n(xn)
Qn(xn) →0 .
(3)
3
MDL/Bayesian Criteria
Universal coding is used not just for data compression but also for estimation of
sources: the rule with which data can be described shortest explains the nature
of the data best.

MDL/Bayesian Criteria Based on Universal Coding/Measure
403
3.1
Markov Order Identiﬁcation
Suppose that the source P n is ergodic Markov with known order k and unknown
parameters. Let S := Ak be the set of states of a Markov source, and cn[x, s] the
number of occurrences of (xi−k · · · xi−1, xi) = (s, x) ∈S × A in xn ∈An. Then,
(2) is generalized as
Qn(xn|k) :=
n

i=1
ci−1[xi, si] + 1
2
{

x∈A
ci−1[x, si]} + m
2
=

s∈S
Γ(m
2 )

x∈A
Γ(cn[x, s] + 1
2)
Γ(n + m
2 )Γ(1
2)m
.
(4)
Similarly, we can show L(xn)
n
= −1
n log Qn(xn|k) →H with probability one as
n →∞.
Next, suppose that the Markov order k is unknown and only actually emitted
examples xn ∈An are available. The problem is to identify the Markov order k
given xn ∈An. To this end, we count cn[x, s] of occurrences of each x ∈A for
each s ∈S in xn ∈An and obtain the value of (4) for each k = 0, 1, · · · .
Let π(k) be the a prior probability of each Markov order k = 0, 1, · · · . If
we choose k such that −log π(k) −log Qn(xn|k) is minimized, equivalently,
such that π(k)Qn(xn|k) is maximized, then we obtain a solution based on the
MDL/Bayesian criteria.
3.2
Conditional Probabilities
Let X, Y be random variables, and X(Ω), Y (Ω) their ranges with |Y (Ω)| < ∞.
We specify the conditional probability P(Y |X) by deﬁning equivalent classes of
X(Ω) as follows: x ∼x′ ⇐⇒P(y|x) = P(y|x′), y ∈Y (Ω). Let [x] be the class
including x ∈X(Ω), and S := {[x]|x ∈X(Ω)} the set of equivalent classes
(equivalence relation), where we assume |S| < ∞.
Example 1 (feature selection). Suppose that the random variable X is ex-
pressed by a vector X = (X(1), · · · , X(m)) consisting of m random variables
X(1), · · · , X(m), and that s ∈S is unique given the values of random variables
{X(j)}j∈F . Then, we say the minimal subset F satisfying this property to be
a feature set. On the other hand, if we select F (feature selection), then the
equivalent relation S can be decided by S := 
j∈F X(j)(Ω).
The problem is to identify the equivalent relation S given n pairs of examples
{(xi, yi)}n
i=1 ∈X(Ω) × Y (Ω). To this end, we count the number cn[y, s] of
occurrences of each y ∈Y (Ω) for each s ∈S in {(xi, yi)}n
i=1, and obtain the
value of
Qn(yn|xn, S) :=

s∈S
Γ(m
2 )

y∈Y (Ω)
Γ(cn[y, s] + 1
2)
Γ(n + m
2 )Γ(1
2)m

404
J. Suzuki
for each S. Let π(S) be the prior probability of S. If we choose S such
that −log π(S) −log Qn(yn|xn, S) is minimized, or equivalently, such that
π(S)Qn(yn|xn, S) is maximized, then we obtain a solution based on the
MDL/Bayesian criteria.
4
Universal Coding in a General Sense
In this section, we construct a universal measure rather than a universal coding
in order to extend the notion to general sources because coding is available only
for ﬁnite sources.
4.1
Estimation of Density Functions
Let {Xi}n
i=1 be stationary ergodic with density function f n, which means that
the source {Xi}n
i=1 is not ﬁnite. Let {Ak}∞
k=0 be such that A0 := {Xi(Ω)}, and
Ak+1 is a reﬁnement of Ak.
Example 2. Suppose Xi(Ω) = [0, 1), and that we consider the following
sequence:
A0 = {[0, 1)}
A1 = {[0, 1/2), [1/2, 1)}
A2 = {[0, 1/4), [1/4, 1/2), [1/2, 3/4), [3/4, 1)}
. . .
Ak = {[0, 2−k), [2−k, 2 · 2−k), · · · , [(2k −1)2−k, 1)}
. . . .
Then m
=
2k,
and if
the
source is i.i.d.,
we have Qn
k(sk(xn))
:=
Γ(m
2 )

a∈Ak
Γ(cn[a] + 1
2)
Γ(n + m
2 )Γ(1
2)m
, where cn[a] is the number of occurrences of a ∈Ak in
sk(xn) ∈An
k. The Lebesgue measure is λ(sk(x)) = 2−k for x ∈Xi(Ω).
Let sk : Xi(Ω) →Ak be the projection, i.e. sk(x) = a if x ∈a ∈Ak. Similarly,
we write sk(xn) = an if xn ∈an ∈An
k. Let λ : B →R be the Lebesgue measure,
i.e. λ(a) = d −c if a = [c, d] with c ≤d. Similarly, we write λn(an) = n
i=1 λ(ai)
for an ∈Bn, where B is the Borel set ﬁeld of the entire real R.
For each k = 1, 2, · · ·, let P n
k be the probability measure of sk(Xn) with
alphabet Ak. Then, there exists Qn
k : An
k →[0, 1] such that 
an∈An
k Qn
k(an) ≤1
and 1
n log P n
k (sk(xn))
Qn
k(sk(xn)) →0 for any stationary ergodic f n with probabilities as
n →∞(see (3)).
Let gn
k (xn) := Qn
k(sk(xn))
λn(sk(xn)) and {ωk}∞
k=1 be such that  ωk = 1, ωk > 0.
Suppose we estimate f n by gn as gn(xn) :=
∞

k=1
ωkgn
k (xn).

MDL/Bayesian Criteria Based on Universal Coding/Measure
405
Let f n
k (xn) := P n
k (sk(xn))
λn(sk(xn)) , and deﬁne the diﬀerential entropy by
h(f) := lim
n→∞

−f(xn) log f(xn|x1, · · · , xn−1)dxn .
(5)
Proposition 1 (Ryabko, 2009 [11]). With probability one as n →∞,
1
n log f n(xn)
gn(xn) →0
(6)
for any stationary ergodic f n such that
h(fk) →h(f)
(7)
as k →∞.
Notice that Proposition 1 assumes the existence of a density function.
4.2
Conditional Probabilities
We say that a measure μ is absolutely continuous with respect to another measure
ν and write μ ≪ν if ν(A) = 0 =⇒μ(A) = 0 for each A ∈F, and that a measure
ν is σ-ﬁnite if there exists {Ai} such that ∪iAi = Ω and ν(Ai) < ∞.
Let μ, ν be σ-ﬁnite.
Proposition 2 (Radon-Nikodym [3]). Then, μ ≪ν if and only if there exists
F-measurable g : Ω →R such that μ(A) =
:
A g(ω)dν(ω), A ∈F.
We write such a g by dμ
dν .
Let X, Y be random variables. Then, μ(X ∈D) = 0 =⇒μ(X ∈D, Y ∈
D′) = 0 for D, D′ ∈B. From Proposition 2, for each D′ ∈B, there exists
μ(Y ∈D′|·) : R →R such that μ(X ∈D, Y ∈D′) =

D
μ(Y ∈D′|x)dμ(x) (the
conditional probability of Y given X).
4.3
Kullback-Leibler Divergence
When μ ≪ν, we deﬁne the Kullback-Leibler divergence of μ with respect to ν
by D(μ||ν) :=

dμ log dμ
dν . Let μ(X ≤x) := FX(x). Then, μ ≪λ if and only
if there exists fX = dμ
dλ : R →R such that μ(X ≤x) =
: x
−∞fX(t)dλ(t). In
particular,
D(μ||λ) =

dμ log dμ
dλ =

dμ log fX =
 ∞
−∞
fX(x) log fX(x)dx .
Let {Xi}n
i=1 ∼μn be stationary ergodic. Then, μn−1(Xn−1 ∈Dn−1) = 0 =⇒
μn(Xn ∈Dn) = 0 for Dn ∈Bn, so that there exists μ(Xn ∈Dn|·) : Rn−1 →R

406
J. Suzuki
such that μn(Xn ∈Dn) =

Dn−1 μ(Xn ∈Dn|xn−1)dμn−1(xn−1) (conditional
Probability μ(Xn ∈Dn|xn−1) of X ∈Dn given xn−1 ∈Rn−1).
Similarly, there exists ν(Xn ∈Dn|·) : Rn−1 →R as well for νn such that
:
xn∈Xn(Ω) dνn(xn) ≤1. When μn ≪νn, write the Radon-Nikodym derivative
at xn ∈Dn by dμ
dν (xn|xn−1), and deﬁne
D(μ||ν) := lim
n→∞

dμn(xn) log dμ
dν (xn|xn−1)
In particular, from (5), when μn ≪λn,
D(μ||λ) = −h(f) .
(8)
4.4
Universal Coding for General Sources
In this paper, we extend the constructions of Qn and gn achieving (3) and
(6), respectively, to that of the general measure without assuming to be either
discrete or continuous [15].
Let {Xi}n
i=1 be stationary ergodic with probability measure μn, and ηn be
such that μn ≪ηn. Let dνn
k
dηn (xn) := Qn
k(sk(xn))
ηn(sk(xn)) and {ωi}∞
k=0 such that
∞

k=0
ωk =
1, ωk > 0.
We estimate dμn
dηn (xn) by dνn
dηn (xn) :=
∞

k=0
ωk
dνn
k
dηn (xn). Let dμn
k
dηn (xn) :=
μn
k(sk(xn))
ηn(sk(xn)) .
Theorem 1. Let {Ak}∞
k=1 such that A0 := {Xi(Ω)} and Ak+1 is a reﬁnement of
Ak, and η a σ-ﬁnite measure. There exists νn such that
:
xn∈Xn(Ω) dνn(xn) ≤1
and with probability one
1
n log dμn
dνn (xn) →0
(9)
for any stationary ergodic μn such that μn ≪ηn and
D(μk||η) →D(μ||η)
(10)
as k →∞.
(see proof of Theorem 1 for Appendix).
Remark 1. When η = λ, (9) and (10) become (6) and (7) (see (8)).
Example 3. Suppose Xi(Ω) = {1, 2, · · ·}, and that the following sequence is
given:

MDL/Bayesian Criteria Based on Universal Coding/Measure
407
A0 = {{1, 2, · · ·}}
A1 = {{1}, {2, · · ·}}
A2 = {{1}, {2}, {3, · · ·}}
. . .
Ak = {{1}, · · · , {k}, {k + 1, · · · }}
. . .
Then
m
=
k + 1,
and
if
the
source
is
i.i.d.,
νn
k (sk(xn))
:=
Γ(n + m
2 )Γ(1
2)m
Γ(m
2 )

a∈Ak
Γ(cn[a] + 1
2)
, where cn[a] is the number of occurrences of a ∈Ak
in sk(xn) ∈An
k. If we set η as follows, then μ ≪η: η({j}) :=
1
j(j + 1) for
j = 1, 2, · · ·, and η({k + 1, · · ·}) =
∞

j=k+1
η({j}) =
1
k + 1. Then, we can compute
dνn
k
dηn =
νk(sk(xn))
n
i=1 η(sk(xi)) to obtain dνn
dηn =
∞

k=1
ωk
dνn
k
dηn .
5
MDL/Bayesian Criteria in a General Sense
Let {Xi}n
i=1 ∼μn be stationary ergodic. Given n examples xn ∈n
i=1 Xi(Ω),
we estimate μn based on the following assumptions:
1. the true μn is known to be speciﬁed with some model in a countable set M
and some parameters;
2. for each m ∈M, there exists νn[m] such that with probability one
1
n log dμn[m]
dνn[m](xn) →0
for μn[m] with model m and arbitrary parameters; and
3. the a prior probability π[m] of m ∈M is known.
By the MDL/Bayesian criteria, we mean to apply the following decision rule:
π[m]
π[m′] · dνn[m]
dνn[m′](xn) > 1 ⇐⇒m is prefer to m′ .
5.1
Markov Order Identiﬁcation for Continuous Sources
In general, by the Markov order of a stationary ergodic source {Xi}∞
i=−∞we
mean the minimal k such that {Xi}0
i=−∞and {Xi}∞
k+1 are conditionally inde-
pendent given {Xi}k
i=1.

408
J. Suzuki
Based on Theorem 1, for each k = 0, 1, · · ·, we can construct νn[k] such that

dνn[k](xn) ≤1, and 1
n log dμn[k]
dνn[k](xn) →0 with probability one for μn[k] with
order k and arbitrary parameters.
Suppose we are given actually emitted n examples xn ∈Xn(Ω). Let π[k]: the
a prior probability of k, and η such that μ[k] ≪η, k = 0, 1, · · · . Then, for each
k = 0, 1, · · · we can construct dν[k]
dη , and choose k such that π[k]dν[k]
dη (xn) is
maximized given the xn.
5.2
Feature Selection Including Continuous Attributes
Consider again the feature selection problem discussed in Example 2, and give
a solution to the problem raised in Introduction. Let X = {X(j)}m
j=1 and Y
random variables with |Yi(Ω)| < ∞.
Based on Theorem 1, for each F ⊆{1, · · · , m}, construct νn
XY [F] and νn
X[F]
such that
:
dνn
XY [F](xn, yn) ≤1 ,

dνn
X[F](xn) ≤1 and
1
n log dμn
XY [F]
dνn
XY [F] (xn, yn) →0 , 1
n log dμn
X[F]
dνn
X[F] (xn) →0
for μn
XY [F] and μX[F] with feature set F and arbitrary parameters.
Suppose we are given actually emitted n examples (xn, yn) ∈Xn(Ω) ×
Y n(Ω) with Xn(Ω) = n
i=1
m
j=1 X(j)
i
(Ω). Let π[F]: the a prior probabil-
ity of F
⊆
{1, · · · , m}, and η such that μX[F] ≪η. Then, for F
⊆
{1, · · · , m}, we can construct dνXY [F]
dη
and dνX[F]
dη
, and choose F such that
π[F]dνXY [F]
dη
(xn, yn)/dνX[F]
dη
(xn) is maximized given (xn, yn).
Example 4. Suppose X(1)(Ω)
=
[0, 1), X(2)(Ω)
=
{0, 1}, X(3)(Ω)
=
{1, 2, · · ·}. We estimate F ∈{{}, {1}, {2}, {1, 2}} given {(x(1)
i
, x(2)
i , x(3)
i )}n
i=1 ∈
{X(1)(Ω) × X(2)(Ω) × X(3)(Ω)}n. Let {A(1)
k } := {Ak} be as in Example 3, let
A(2)
0
= {{0, 1}}, A(2)
1
= A(2)
2
= · · · = {{0}, {1}}, and {A(3)
k } := {Ak} in Exam-
ple 4, respectively. Also, let η(1) := η in Example 3, η(2)(0) = η(2)(1) = 1
2, and
η(3) := η in Example 4, respectively. Let B := X(3)(Ω).
For example, let A := X(1)(Ω). We compute the value of dνn
dηn (bn|an) for
F = {1} and an ∈An, bn ∈Bn. For each k = 1, 2, · · ·, let Ak := A(1)
k ×A(3)
k
with
m := 2k(k+1), cn(a, b) the number of occurrences of (a, b) ∈Ak in (an, bn) ∈An
k,
and
νn
k (an, bn) :=
Γ(m
2 )

(a,b)∈Ak
Γ(cn(a, b) + 1
2)
Γ(n + m
2 )Γ(1
2)m
.

MDL/Bayesian Criteria Based on Universal Coding/Measure
409
Also, let η(a, b) := η(1)(a)η(3)(b) = 2−k ·
1
j(j + 1) for (a, b) ∈Ak if b = {j}, and
η(an, bn) := n
i=1{η(1)(ai)η(3)(bi)} for (an, bn) ∈An
k. We estimate dμn
k
dηn and dμn
dηn
by dνn
k
dηn (an, bn) = νn
k (an,bn)
ηn(an,bn) and dνn
dηn (an, bn) = ∞
k=1 ωk
dνn
k
dηn (an, bn), respectively.
On the other hand, from Example 3, we immediately obtain estimation dνn
dηn (an)
of dμn
dηn (an) for an ∈An. Then, we obtain the value of
dνn
dηn (bn|an) =
.dνn
dηn (an, bn)
/
/
.dνn
dηn (an)
/
for F = {1}. Similarly, we obtain the values of dνn
dηn for F = {2}, {}, {1, 2}.
Finally, we choose the maximum value among the four to choose F
∈
{{}, {1}, {2}, {1, 2}}.
6
Concluding Remarks
We proposed the MDL/Bayesian criteria in a general sense, and illustrate the
idea in the two examples:
– Markov order identiﬁcation for continuous sources
– feature selection containing continuous attributes
We removed the constraint that each attribute should take a value in a ﬁnite
set.
One could quantize continuous data to obtain the description length assum-
ing each underlying model. However, the cell size in the partition should be
optimized to obtain the correct MDL/Bayesian criteria: overestimation and un-
derestimation may occur if the cell size is too small and too large, respectively.
Intuitive speaking, our method utilizes weighting partitions to obtain more ro-
bust estimations compared with estimating only one partition.
Appendix: Proof of Theorem 1
Proof: for k = 1, 2, · · ·, dνn
dηn (xn) ≥ωk
dνn
k
dηn (xn), so that dμn
dνn (xn) ≤1
ωk
dμn
dνn
k
(xn),
thus
1
n log dμn
dνn (xn) ≤−1
n log ωk + 1
n log dμn
k
dνn
k
(xn) + 1
n log dμn
dμn
k
(xn)
with probability one. Note that for each k = 1, 2, · · ·, from (3), we have
1
n log dμn
k
dνn
k
(xn) = 1
n log Pk(sk(xn))
Qk(sk(xn)) →0 .

410
J. Suzuki
Notice that : dνn ≤1 and Proposition 3 imply 0 ≤D(μ||ν) and D(μ||ν) ≤
D(μ||μk) for k = 1, 2, · · ·, respectively. Since we assume D(μ||μk) →0 (k →∞),
D(μ||ν) = 0 is required, which implies Theorem 1.
□
Proposition 3 (Barron, 1985 [1]). With probability one as n →∞,
1
n log dμn
dνn (xn) →D(μ||ν) .
References
1. Barron, A.R.: The Strong Ergodic Theorem for Densities: Generalized Shannon-
McMillan-Breiman Theorem. Annals. Probability 13(4), 1292–1303 (1985)
2. Buntine, W.L.: Learning Classiﬁcation Trees. Statistics and Computing 2, 63–73
(1991)
3. Billingsley, P.: Probability & Measure, 3rd edn. Wiley, New York (1995)
4. Cooper, G.F., Herskovits, E.: A Bayesian Method for the Induction of Probabilistic
Networks from Data. Machine Learning 9, 309–347 (1992)
5. Cover, T.M., Thomas, J.A.: Elements of Information Theory, 2nd edn. Wiley, New
York (1995)
6. Dowe, D.L.: Foreword re C. S. Wallace. Computer Journal 51(5), 523–560 (2008),
Christopher Stewart WALLACE (1933-2004) memorial special issue
7. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Bandyopadhyay, P.S., Forster, M.R. (eds.)
Handbook of the Philosophy of Science (HPS). Philosophy of Statistics, vol. 7,
pp. 901–982. Elsevier (June 2011)
8. Krichevsky, R.E., Troﬁmov, V.K.: The Performance of Universal Encoding. IEEE
Trans. Inform. Theory 27(2), 199–207 (1981)
9. Kullback, S., Leibler, R.A.: On information and suﬃciency. Ann. Math. Statis-
tics 22(1), 79–86 (1951)
10. Rissanen, J.: Modeling by shortest data description. Automatica 14, 465–471 (1978)
11. Ryabko, B.: Compression-Based Methods for Nonparametric Prediction and Es-
timation of Some Characteristics of Time Series. IEEE Trans. on Inform. The-
ory 55(9), 4309–4315 (2009)
12. Solomonoﬀ, R.: A Formal Theory of Inductive Inference. Information and Con-
trol 22, 224–254 (1964)
13. Suzuki, J.: On Strong Consistency of Model Selection in Classiﬁcation. IEEE Trans.
on Inform. Theory 52(11), 4767–4774 (2006)
14. Suzuki, J.: A Construction of Bayesian Networks from Databases on an MDL
Principle. In: The Ninth Conference on Uncertainty in Artiﬁcial Intelligence, vol. 7,
pp. 266–273 (1993)
15. Suzuki, J.: The Universal Measure for General Sources and its Application to
MDL/Bayesian Criteria. In: Data Compression Conference 2011, Snowbird, Utah
(2011)
16. Wallace, C.S., Boulton, D.M.: An information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)
17. Wallace, C.S.: Statistical and Inductive Inference by Minimum Message Length.
Springer (2005) ISBN: 0-387-23795-X

Algorithmic Analogies to Kamae-Weiss Theorem
on Normal Numbers
Hayato Takahashi
The Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa,
Tokyo 190-8562, Japan
hayato.takahashi@ieee.org
Abstract. In this paper we study subsequences of random numbers.
In Kamae (1973), selection functions that depend only on coordinates
are studied, and their necessary and suﬃcient condition for the selected
sequences to be normal numbers is given. In van Lambalgen (1987), an
algorithmic analogy to the theorem is conjectured in terms of algorithmic
randomness and Kolmogorov complexity. In this paper, we show diﬀerent
algorithmic analogies to the theorem.
1
Introduction
In this paper we study subsequences of random numbers. A function from se-
quences to their subsequences is called selection function. In Kamae [5] selection
functions that depend only on coordinates are studied, and their necessary and
suﬃcient condition for the selected sequences to be normal numbers is given. In
the following we call the theorem Kamae-Weiss (KW) theorem on normal num-
bers since a part of the theorem is shown in Weiss [15]. In van Lambalgen [13],
an algorithmic analogy to KW theorem is conjectured in terms of algorithmic
randomness and complexity [2,7,9,11]. In this paper we show two algorithmic
analogies to KW theorem.
Let Ω be the set of inﬁnite binary sequences. For x, y ∈Ω, let x = x1x2 · · ·
and y = y1y2 · · · , ∀i xi, yi ∈{0, 1}. Let τ : N →N be a strictly increasing
function such that {i | yi = 1} = {τ(1) < τ(2) < · · · }. If 
i yi = n then τ(j) is
deﬁned for 1 ≤j ≤n. For x, y ∈Ω let x/y be the subsequence of x selected at
yi = 1, i.e., x/y = xτ(1)xτ(2) · · ·. For example, if x = 0011 · · · , y = 0101 · · · then
τ(1) = 2, τ(2) = 4 and x/y = 01 · · ·. For ﬁnite binary strings xn
1 := x1 · · · xn
and yn
1 := y1 · · · yn, xn
1 /yn
1 is deﬁned similarly. Let S be the set of ﬁnite binary
strings and |s| be the length of s ∈S. For s ∈S let Δ(s) := {sω|ω ∈Ω}, where
sω is the concatenation of s and ω. Let (Ω, B, P) be a probability space, where
B is the sigma-algebra generated by Δ(s), s ∈S. We write P(s) := P(Δ(s)).
In Kamae [5], it is shown that the following two statements are equivalent
under the assumption that lim inf 1
n
n
i=1 yi > 0:
Theorem 1 (KW)
(i) h(y) = 0.
(ii) ∀x ∈N x/y ∈N,
where h(y) is Kamae entropy [1,13] and N is the set of binary normal numbers.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 411–416, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

412
H. Takahashi
A probability p on Ω is called cluster point if there is a sequence {ni}
∀s ∈S p(s) = lim
i→∞#{1 ≤j ≤ni | xj · · · xj+|s|−1 = s}/ni.
From the deﬁnition, the cluster points are stationary measures. Let V (x) be
the set of cluster points deﬁned from x. From a standard argument we see that
V (x) ̸= ∅for all x. Kamae entropy is deﬁned by
h(x) = sup{h(p) | p ∈V (x)},
where h(p) is the measure theoretic entropy of p. If h(x) = 0, it is called com-
pletely deterministic, see [5,15,16]. The part (i)⇒(ii) is appeared in [15].
As a natural analogy, the following equivalence (algorithmic randomness ver-
sion of Kamae’s theorem) under a suitable restriction on y is conjectured in van
Lambalgen [13],
(i) limn→∞K(yn
1 )/n = 0.
(ii) ∀x ∈R x/y ∈R,
where K is the preﬁx Kolmogorov complexity and R is the set of Martin-L¨of
random sequences with respect to the uniform measure (fair coin ﬂipping), see
[8]. Note that limn→∞K(yn
1 )/n = h, P −a.s., for ergodic P and its entropy h,
see [1].
2
Results
In this paper, we show two algorithmic analogies to KW theorem. The ﬁrst
one is a Martin-L¨of randomness analogy and the second one is a complexity
rate analogy to KW theorem, respectively. In the following, P on Ω is called
computable if there is a computable function A such that ∀s, k |P(s)−A(s, k)| <
1/k. For A ⊂S, let ˜A := ∪s∈AΔ(s). A recursively enumerable (r.e.) set U ⊂N×S
is called (Martin-L¨of) test with respect to P if 1) U is r.e., 2) ˜Un+1 ⊂˜Un for all
n, where Un = {s : (n, s) ∈U}, and 3) P( ˜Un) < 2−n. A test U is called universal
if for any other test V , there is a constant c such that ∀n ˜Vn+c ⊂˜Un. In [9], it is
shown that a universal test U exists if P is computable and the set (∩∞
n=1 ˜Un)c
is called the set of Martin-L¨of random sequences with respect to P.
Our ﬁrst algorithmic analogy to the KW theorem is the following.
Proposition 1. Suppose that y is Martin-L¨of random with respect to some com-
putable probability P and ∞
i=1 yi = ∞. Then the following two statements are
equivalent:
(i) y is computable.
(ii) ∀x ∈R x/y ∈Ry,
where Ry is the set of Martin-L¨of random sequences with respect to the uniform
measure relative to y.
Proof: (i)⇒(ii). Since ∞
i=1 yi = ∞we have ∀s λ{x ∈Ω | s ⊏x/y} = 2−|s|,
where λ is the uniform measure. Let U be a universal test with respect to λ
and y(s) ⊂S be a ﬁnite set such that {x ∈Ω | s ⊏x/y} = ˜y(s). Then y(s) is

Algorithmic Analogies to Kamae-Weiss Theorem on Normal Numbers
413
computable from y and s, and hence U y := {(n, a) | a ∈y(s), s ∈Un} is a test
if y is computable. We have x ∈˜U y
n ↔x/y ∈˜Un. (Intuitively U y is a universal
test on subsequences selected by y). Then
x ∈R →x /∈∩n ˜U y
n ↔x/y /∈∩n ˜Un ↔x/y ∈R.
Since y is computable, Ry = R and we have (ii).
Conversely, suppose that y is a Martin-L¨of random sequence with respect to
a computable P and is not computable. From Levin-Schnorr theorem, we have
∀n Km(yn
1 ) = −log P(yn
1 ) + O(1),
(1)
where Km is the monotone complexity. Throughout the paper, the base of loga-
rithm is 2. By applying arithmetic coding to P, there is a sequence z such that z
is computable from y and yn
1 ⊏u(zln
1 ), ln = −log P(yn
1 )+O(1) for all n, where u
is a monotone function and we write s ⊏s′ if s is a preﬁx of s′. Since y is not com-
putable, we have limn ln = ∞. From (1), we see that ∀n Km(zln
1 ) = ln + O(1).
We show that if y ∈R then supn ln+1 −ln < ∞. Observe that if y ∈R then
∀n P(yn
1 ) > 0 and
sup
n ln+1 −ln < ∞↔sup
n −log P(yn+1 | yn
1 ) < ∞↔inf
n P(yn+1 | yn
1 ) > 0
↔lim inf
n
P(yn+1 | yn
1 ) > 0.
Let Un := {s | P(s | s|s|−1
1
) < 2−n}. Then P( ˜Un) < 2−n and U := {(n, s) | s ∈
Un} is a r.e. set. Since y ∈lim supn ˜Un ↔lim infn P(yn+1 | yn
1 ) = 0, if y ∈R then
supn ln+1 −ln < ∞. (If U is r.e. and P( ˜Un) < 2−n then R ⊂(lim supn ˜Un)c,
see [10].) Since ∀n Km(zln
1 ) = ln + O(1) and supn ln+1 −ln < ∞, we have
∀n Km(zn
1 ) = n + O(1) and z ∈R. Since z is computable from y we have
z/y /∈Ry.
⊓⊔
Note that if y is computable then y is a Martin-L¨of random sequence with
respect to a computable measure that has positive probability at y.
In order to show the second analogy, we introduce another notion of random-
ness. We say that y has maximal complexity rate with respect to P if
lim
n→∞
1
nK(yn
1 ) = lim
n→∞−1
n log P(yn
1 ),
(2)
i.e., both sides exist and are equal. For example, y has maximal complex-
ity rate with respect to the uniform measure (i.e., P(s) = 2−|s| for all s)
if limn→∞K(yn
1 )/n = 1. If y is Martin-L¨of random sequences with respect
to a computable ergodic P then from upcrossing inequality for the Shannon-
McMillan-Breiman theorem [4], the right-hand-side of (2) exists (see also [14])
and from (1), we see that (2) holds i.e., y has maximal complexity rate w.r.t. P.
Proposition 2. Suppose that y has maximal complexity rate with respect to a
computable probability and limn 1
n
n
i=1 yi > 0. Then the following two state-
ments are equivalent:
(i) limn→∞1
nK(yn
1 ) = 0.
(ii) ∀x limn→∞1
nK(xn
1 ) = 1 →limn→∞
1
|xn
1 /yn
1 |K(xn
1/yn
1 |yn
1 ) = 1.

414
H. Takahashi
Proof:
(i) ⇒(ii)
Let ¯y := ¯y1¯y2 · · · ∈Ω such that ¯yi = 1 if yi = 0 and ¯yi = 0 else for all i. Since
|K(xn
1 ) −K(xn
1|yn
1 )| ≤K(yn
1 ) + O(1)
and
K(xn
1|yn
1 ) = K(xn
1/yn
1 , xn
1/¯yn
1 |yn
1 ) + O(1),
if limn→∞K(yn
1 )/n = 0 and 0 < limn 1
n
n
i=1 yi < 1 then we have
lim
n→∞K(xn
1 )/n = 1
⇒lim
n→∞
1
nK(xn
1 /yn
1 , xn
1 /¯yn
1 |yn
1 ) = 1
⇒lim
n→∞
1
n(K(xn
1 /yn
1 |yn
1 ) + K(xn
1/¯yn
1 |yn
1 )) = 1
⇒lim
n→∞
n1
n
1
n1
K(xn
1/yn
1 |yn
1 ) + n −n1
n
1
n −n1
K(xn
1/¯yn
1 |yn
1 ) = 1
⇒lim
n→∞
1
n1
K(xn
1/yn
1 |yn
1 ) = 1 and
lim
n→∞
1
n −n1
K(xn
1 /¯yn
1 |yn
1 ) = 1.
where n1 = |xn
1/yn
1 | = n
i=1 yi. Similarly, if limn→∞K(yn
1 )/n = 0 and
limn 1
n
n
i=1 yi = 1 then we have limn→∞
1
n1 K(xn
1 /yn
1 |yn
1 ) = 1.
(ii) ⇒(i)
Suppose that
lim
n→∞
1
nK(yn
1 ) = lim
n→∞−1
n log P(yn
1 ) > 0,
(3)
for a computable P. Let ln be the least integer greater than −log P(yn
1 ). Then by
considering arithmetic coding, there is z = z1z2 · · · ∈Ω and a monotone function
u such that yn
1 ⊏u(zln
1 ). By considering optimal code for zln
1 we have Km(yn
1 ) ≤
Km(zln
1 ) + O(1). From (3), we have limn Km(yn
1 )/ln = limn Km(zln
1 )/ln = 1.
For ln ≤t ≤ln+1, we have Km(zln
1 )/ln+1 ≤Km(zt
1)/t ≤Km(zln+1
1
)/ln. From
(3), we have limn ln+1/ln = 1, and hence limn Km(zn
1 )/n = limn K(zn
1 )/n = 1.
Since 1) zln
1 is computable from yn
1 , 2) limn ln/n > 0 by (3), and
3) limn 1
n
n
i=1 yi > 0, we have lim supn→∞
1
|zn
1 /yn
1 |K(zn
1 /yn
1 |yn
1 ) < 1.
⊓⊔
Example 1. Champernowne sequence satisﬁes the condition of the proposition
and (i) holds, however its Kamae-entropy is not zero.
Example 2. If y is a Sturmian sequence generated by an irrational rotation model
with a computable parameter [6,12] then y satisﬁes the condition of the propo-
sition and (i) holds.

Algorithmic Analogies to Kamae-Weiss Theorem on Normal Numbers
415
3
Discussion
Both proofs of Proposition 1 and 2 have similar structure, i.e., the part (i) →
(ii) are straightforward and in order to show the converse, we construct random
sequences (in the sense of Proposition 1 and 2, respectively) by compression.
We may say that Proposition 1 is a Martin-L¨of randomness analogy and
Proposition 2 is a complexity rate analogy to KW theorem, respectively. These
results neither prove nor disprove the conjecture of van Lambalgen. However
Martin-L¨of randomness and complexity rate randomness give diﬀerent classes of
randomness, and a curious point of the conjecture is that it states equivalence
of statements described in terms of them.
As stated above we proved our propositions by constructing random sequences.
In [3] pp.962, a diﬀerent direction is studied, i.e., a sequence that is not predicted
by MML with respect to ﬁnite order Markov processes is considered. Such a se-
quence is called red herring sequence [3] and considered to be a non-random
sequence with respect to MML and ﬁnite order Markov processes, in the sense
that MML cannot ﬁnd a ﬁnite order Markov model for that sequence.
Acknowledgement.
The author thanks Prof. Teturo Kamae (Matsuyama
Univ.) for discussions and comments.
References
1. Brudno, A.A.: Entropy and the complexity of the trajectories of a dynamical sys-
tem. Trans. Mosc. Math. Soc. 44, 127–151 (1983)
2. Chaitin, G.J.: A theory of program size formally identical to information theory.
J. ACM 22, 329–340 (1975)
3. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Bandyopadhyay, P.S., Foster, M. (eds.) Hand-
book of the Philosophy of Science. Philosophy of Statistics, vol. 7, Elsevier (2011)
4. Hochman, M.: Upcrossing inequalities for stationary sequences and applications.
Ann. Probab. 37(6), 2135–2149 (2009)
5. Kamae, T.: Subsequences of normal numbers. Israel J. Math. 16, 121–149 (1973)
6. Kamae, T., Takahashi, H.: Statistical problems related to irrational rotations.
Ann. Inst. Statist. Math. 58(3), 573–593 (2006)
7. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Probl. Inf. Transm. 1(1), 1–7 (1965)
8. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and Its applications,
3rd edn. Springer, New York (2008)
9. Martin-L¨of, P.: The deﬁnition of random sequences. Information and Control 9,
602–609 (1966)
10. Shen, A.K.: On relations between diﬀerent algorithmic deﬁnitions of randomness.
Soviet Math. Dokl. 38(2), 316–319 (1989)
11. Solomonoﬀ, R.J.: A formal theory of inductive inference, part 1 and part2. In-
form. Contr. 7, 1–22, 224–254 (1964)

416
H. Takahashi
12. Takahashi, H., Aihara, K.: Algorithmic analysis of irrational rotations in a single
neuron model. J. Complexity 19, 132–152 (2003)
13. van Lambalgen, M.: Random sequences. PhD thesis, Universiteit van Amsterdam
(1987)
14. V’yugin,
V.V.:
Ergodic
theorems
for
individual
random
sequences.
Theor. Comp. Sci. 207, 343–361 (1998)
15. Weiss, B.: Normal sequences as collectives. In: Proc. Symp. on Topological Dy-
namics and Ergodic Theory. Univ. of Kentucky (1971)
16. Weiss, B.: Single Orbit Dynamics. Amer. Math. Soc. (2000)

(Non-)Equivalence of Universal Priors
Ian Wood1, Peter Sunehag1, and Marcus Hutter1,2
1 School of Computer Science, The Australian National University,
Canberra ACT 0200 Australia
2 Department of Computer Science, ETH Z¨urich, Switzerland
Abstract. Ray Solomonoﬀinvented the notion of universal induction
featuring an aptly termed “universal” prior probability function over all
possible computable environments [9]. The essential property of this prior
was its ability to dominate all other such priors. Later, Levin introduced
another construction — a mixture of all possible priors or “universal
mixture” [12]. These priors are well known to be equivalent up to mul-
tiplicative constants. Here, we seek to clarify further the relationships
between these three characterisations of a universal prior (Solomonoﬀ’s,
universal mixtures, and universally dominant priors). We see that the
the constructions of Solomonoﬀand Levin deﬁne an identical class of
priors, while the class of universally dominant priors is strictly larger.
We provide some characterisation of the discrepancy.
1
Introduction
In the study of universal induction, we consider an abstraction of the world in
the form of a binary string. Any sequence from a ﬁnite set of possibilities can
be expressed in this way, and that is precisely what contemporary computers
are capable of analysing. An “environment” provides a measure of probabil-
ity to (possibly inﬁnite) binary strings. Typically, the class M of enumerable
semimeasures is considered. Given the equivalence between M and the set of
monotone Turing machines (Lemma 1), this choice reﬂects the expectation that
the environment can be computed by (or at least approximated by) a Turing
machine.
Universal induction is an ideal Bayesian induction mechanism assigning prob-
abilities to possible continuations of a binary string. In order to do this, a prior
distribution, termed a universal prior, is deﬁned on binary strings. This prior has
the property that the Bayesian mechanism converges to the true (generating)
environment for any environment μ in M , given suﬃcient evidence.
There are three popular ways of deﬁning a universal prior in the literature:
Solomonoﬀ’s prior [9,12,4], as a universal mixture [12,4,5], or a universally dom-
inant semimeasure [4,5]. Brieﬂy, a universally dominant semimeasure is one that
dominates every other semimeasure in M (Deﬁnition 8), a universal mixture is a
mixture of all semimeasures in M with non-zero coeﬃcients (Deﬁnition 7), and
a Solomonoﬀprior assigns the probability that a (chosen) monotone universal
Turing machine outputs a string given random input (Deﬁnition 6). These and
other relevant concepts are deﬁned in more detail in Section 2.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 417–425, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

418
I. Wood, P. Sunehag, and M. Hutter
Solomonoﬀ’s and the universal mixture constructions have been known for
many years and they are often used interchangeably in textbooks and lecture
notes. Their equivalence has been shown in the sense that they dominate each
other [12,4,7]. We extend this result in Section 3, showing that they in fact deﬁne
exactly the same class of priors.
Further, it is trivial to see that both constructions produce universally dom-
inant semimeasures. The converse is, however, not true. Universally dominant
semimeasures are a larger class. We provide a simple example to demonstrate
this in Section 4.
These results are relatively undemanding technically, however given their fun-
damental nature, that they have not to our knowledge been published to date,
and the relevance to Ray Solomonoﬀ’s famous work on universal induction, we
present them here.
The following diagram summarises these inclusion relations:
Universally Dominant

Lemma3
T heorem 2
R
Universal Mixture 
T heorem 1
 SolomonoﬀPrior
Corollary 1

Fig. 1.
2
Deﬁnitions
We represent the set of ﬁnite/inﬁnite binary strings as B∗and B∞respectively.
ϵ denotes the empty string, xb the concatenation of strings x and b, ℓ(x) the
length of a string x. A cylinder set, the set of all inﬁnite binary strings which
start with some x ∈B∗is denoted Γx.
A string x is said to be a preﬁx of a string y if y = xz for some string
z. We write x ⊑y or x ⊏y if x is a proper substring of y (ie: z ̸= ϵ). We
denote the maximal preﬁx-free subset of a set of ﬁnite strings P by ⌊P⌋. It can
be obtained by successively removing elements that have a preﬁx in P. The
uniform measure of a set of strings is denoted |P| := 
p∈⌊P⌋2−ℓ(p). This is the
area of continuations of elements of P considered as binary decimal numbers.
There have been several deﬁnitions of monotone Turing machines in the lit-
erature [7], however we choose that which is now widely accepted [9,12,4,7] and
has the useful and intuitive property Lemma 1.
Deﬁnition 1. A monotone Turing machine is a computer with binary (one-way)
input and output tapes, a bidirectional binary work tape (with read/write heads as
appropriate) and a ﬁnite state machine to determine its actions given input and
work tape values. The input tape is read-only, the output tape is write-only.

(Non-)Equivalence of Universal Priors
419
The deﬁnitions of a universal Turing machine in the literature are somewhat
varied or unclear. Monotone universal Turing machines are relevant here for
deﬁning the Solomonoﬀprior. In the algorithmic information theory literature,
most authors are concerned with the explicit construction of a single reference
universal machine [4,7,9,11,12]. A more general deﬁnition is left to a relatively
vague statement along the lines of “a Turing machine that can emulate any other
Turing machine”. The deﬁnition below reﬂects the typical construction used and
is often referred to as universal by adjunction [2,3].
Deﬁnition 2 (Monotone Universal Turing Machine). A monotone uni-
versal Turing machine is a monotone Turing machine U for which there exist:
1. an enumeration {Ti : i ∈N} of all monotone Turing machines
2. a computable uniquely decodable self-delimiting code I : N →B∗
such that the programs for U that produce output coincide with the set {I(i)p :
i ∈N, p ∈B∗} of concatenations of I(i) and p, and
U(I(i)p) = Ti(p)
∀i ∈N , p ∈B∗
A key concept in algorithmic information theory is the assignment of
probability to a string x as the probability that some monotone Turing machine
produces output beginning with x given unbiased coin ﬂip input. This approach
was used by Solomonoﬀto construct a universal prior [9]. To better understand
the properties of such a function, we will need the concepts of enumerability and
semimeasures:
Deﬁnition 3. A function or number φ is said to be enumerable or lower
semicomputable (these terms are synonymous) if it can be approximated from
below (pointwise) by a monotone increasing set {φi : i ∈N} of ﬁnitely computable
functions/numbers, all calculable by a single Turing machine. We write φi ↗
φ. Finitely computable functions/numbers can be computed in ﬁnite time by a
Turing machine.
Deﬁnition 4. A semimeasure is a “defective” probability measure on the σ-
algebra generated by cylinder sets in B∞. We write μ(x) for x ∈B∗as shorthand
for μ(Γx). A probability measure must satisfy μ(ϵ) = 1, μ(x) = 
b∈B μ(xb). A
semimeasure allows a probability “gap”: μ(ϵ) ≤1 and μ(x) ≥
b∈B μ(xb). M
denotes the set of all enumerable semimeasures.
The following deﬁnition explicates the relationship between monotone Turing
machines and enumerable semimeasures.
Deﬁnition 5 (Solomonoﬀsemimeasure). For each monotone Turing ma-
chine T we associate a semimeasure
λT (x) :=

⌊p:T (p)=x∗⌋
2−ℓ(p) = |T −1(x∗)|
where ⌊P⌋indicates the maximal preﬁx-free subset of a set of ﬁnite strings P,
T (p) = x∗indicates that x is a preﬁx of (or equal to) T (p) and ℓ(p) is the length
of p. If there are no such programs, we set λT (x) := 0. [See [7] deﬁnition 4.5.4]

420
I. Wood, P. Sunehag, and M. Hutter
Note that this is the probability that T outputs a string starting with x
given unbiased coin ﬂip input. To see this, consider the uniform measure given
by λ(Γp) := 2−ℓ(p). This is the probability of obtaining p from unbiased coin
ﬂips. λT (x) is the uniform measure of the set of programs for T that produce
output starting with x, ie: the probability of obtaining one of those programs
from unbiased coin ﬂips. Note also that, since T is monotone, this set consists
of a union of disjoint cylinder sets {Γp : p ∈⌊q : T (q) = x∗⌋}. By dovetailing a
search for such programs and an lower approximation of the uniform measure λ,
we can see that λT is enumerable. See Deﬁnition 4.5.4 (p.299) and Lemma 4.5.5
(p.300) in [7].
An important lemma in this discussion establishes the equivalence between the
set of all monotone Turing machines and the set M of all enumerable semimea-
sures. It is equivalent to Theorem 4.5.2 in [7] (page 301) with a small correction:
λT (ϵ) = 1 for any T by construction, but μ(ϵ) may not be 1, so this case must
be excluded.
Lemma 1. A semimeasure μ is lower semicomputable if and only if there is a
monotone Turing machine T such that μ = λT except on Γϵ ≡B∞and μ(ϵ) is
lower semicomputable.
We are now equipped to formally deﬁne the 3 formulations for a universal
prior:
Deﬁnition 6 (Solomonoﬀprior). The Solomonoﬀprior for a given universal
monotone Turing machine U is
M := λU
The class of all Solomonoﬀpriors we denote UM.
Deﬁnition 7 (Universal mixture). A universal mixture is a mixture ξ with
non-zero positive weights over an enumeration {νi : i ∈N, νi ∈M} of all
enumerable semimeasures M:
ξ =

i∈N
wiνi
:
R ∋wi > 0 ,

i∈N
wi ≤1
We require the weights w() to be a lower semicomputable function. The mixture ξ
is then itself an enumerable semimeasure, i.e. ξ ∈M. The class of all universal
mixtures we denote Uξ.
Deﬁnition 8 (Universally dominant semimeasure). A universally domi-
nant semimeasure is an enumerable semimeasure δ for which there exists a real
number cμ > 0 for each enumerable semimeasure μ satisfying:
δ(x) ≥cμμ(x)
∀x ∈B∗
The class of all universally dominant semimeasures we denote Uδ.
Dominance implies absolute continuity: Every enumerable semimeasure is ab-
solutely continuous with respect to a universally dominant enumerable semimea-
sure. The converse (absolute continuity implies dominance) is however not true.

(Non-)Equivalence of Universal Priors
421
3
Equivalence between SolomonoﬀPriors and Universal
Mixtures
We show here that every Solomonoﬀprior M ∈UM can be expressed as a
universal mixture (i.e.: M ∈Uξ) and vice versa. In other words the class of
Solomonoﬀpriors and the class of universal mixtures are identical: UM = Uξ.
Previously, it was known [12,4,7] that a Solomonoﬀprior M and a universal
mixture ξ are equivalent up to multiplicative constants
M(x) ≤c1ξ(x)
∀x ∈B∗
ξ(x) ≤c2M(x)
∀x ∈B∗
The result we present is stronger, stating that the two classes are exactly iden-
tical. Again we exclude the case x = ϵ as M(ϵ) is always one for a Solomonoﬀ
prior, but ξ(ϵ) is never one for a universal mixture ξ (as there are μ ∈M with
μ(ϵ) < 1).
Lemma 2. For any monotone universal Turing machine U the associated
Solomonoﬀprior M can be expressed as a universal mixture. i.e. there exists an
enumeration {νi}∞
i=1 of the set of enumerable semimeasures M and computable
function w() : N →R such that
M(x) =

i∈N
wiνi(x)
∀x ∈B∗\ϵ
with 
i∈N wi ≤1 and wi > 0 ∀i ∈N. In other words the class of Solomonoﬀ
priors is a subset of the class of universal mixtures: UM ⊆Uξ.
Proof. We note that all programs that produce output from U are uniquely of
the form q = I(i)p. This allows us to split the sum in (1) below.
M(x) =

⌊q:U(q)=x∗⌋
2−ℓ(q)
=

i∈N

⌊p:U(I(i)p)=x∗⌋
2−ℓ(I(i)p)
(1)
=

i∈N
2−ℓ(I(i))

⌊p:Ti(p)=x∗⌋
2−ℓ(p)
=

i∈N
2−ℓ(I(i))λTi(x)
Clearly 2−l(I(i)) > 0 and is a computable function of i. Since I is a self-
delimiting code it must be preﬁx free, and so satisfy Kraft’s inequality:

i∈N
2−l(I(i)) ≤1
Lemma 1 tells us that the λTi cover every enumerable semimeasure if ϵ is
excluded from their domain, which shows that 
i∈N 2−l(I(i))λTi(x) is a universal
mixture. This completes the proof.
⊓⊔

422
I. Wood, P. Sunehag, and M. Hutter
Corollary 1. [12] The Solomonoﬀprior M for a universal monotone Turing
machine U is universally dominant. Thus, the class of Solomonoﬀpriors is a
subset of the class of universally dominant lower semicomputable semimeasures:
UM ⊆Uδ.
Proof. From Lemma 2 we have for each ν ∈M there exists j ∈N with ν = λTj
and for all x ∈B∗:
M(x) =

i∈N
2−l(I(i))λTi(x)
≥2−l(I(j))ν(x)
as required.
⊓⊔
Lemma 3. Every universal mixture ξ is universally dominant. Thus, the class
of universal mixtures is a subset of the class of universally dominant lower semi-
computable semimeasures: Uξ ⊆Uδ.
Proof. This follows from a similar argument to that in Corollary 1.
⊓⊔
Lemma 4. For every universal mixture ξ there exists a universal monotone
Turing machine and associated Solomonoﬀprior M such that
ξ(x) = M(x)
∀x ∈B∗\ϵ
In other words the class of universal mixtures is a subset of the class of
Solomonoﬀpriors: Uξ ⊆UM.
Proof. First note that by Lemma 1 we can ﬁnd (by dovetailing possible repeti-
tions of some indicies) parallel enumerations {νi}i∈N of M and {Ti = λνi}i∈N of
all monotone Turing machines, and lower semicomputable weight function w()
with
ξ =

i∈N
wiνi
,

i∈N
wi ≤1
Take a computable index and lower approximation φ(i, t) ↗wi:
wi =

t
|φ(i, t + 1) −φ(i, t)|
(2)
=

j
2−kij
(3)
i, j →kij computable
(4)
The K-C theorem [6,8,1,2] says that for any computable sequence of pairs {kij ∈
N, τij ∈B∗}i,j∈N with  2−kij ≤1, there exists a preﬁx Turing machine P and
strings {σij ∈B∗} such that
ℓ(σij) = kij , P(σij) = τij
(5)

(Non-)Equivalence of Universal Priors
423
Choosing distinct τij and the existence of preﬁx machine P ensures that {σij}
is preﬁx free. We now deﬁne a monotone Turing machine U. For strings of the
form σijp for some i, j:
U(σijp) := Ti(p)
(6)
For strings not of this form, U produces no output. U inherits monotonicity
from the Ti, and since {Ti}i∈N enumerates all monotone Turing machines, U is
universal. The Solomonoﬀprior associated with U is then:
λU(x) = |U −1(x∗)|
(7)
=

i,j
2−ℓ(σij)|T −1
i
(x∗)|
(8)
=

i
(

j
2−kij)λTi(x)
(9)
=

i
wiνi(x)
(10)
= ξ(x)
(11)
⊓⊔
The main theorem for this section is now trivial:
Theorem 1. The classes UM of Solomonoﬀpriors and Uξ of universal mixtures
are exactly equivalent. In other words, the two constructions deﬁne exactly the
same set of priors: UM = Uξ.
Proof. Follows directly from Lemma 2 and Lemma 4.
4
Not All Universally Dominant Enumerable
Semimeasures Are Universal Mixtures
In this section, we see that a universal mixture must have a “gap” in the semimea-
sure inequality greater than c 2−K(ℓ(x)) for some constant c > 0 independent of x,
and that there are universally dominant enumerable semimeasures that fail this
requirement. This shows that not all universally dominant enumerable semimea-
sures are universal mixtures.
Lemma 5. For every Solomonoﬀprior M and associated universal monotone
Turing machine U, there exists a real constant c > 0 such that
M(x) −M(x0) −M(x1)
M(x)
≥c 2−K(ℓ(x))
∀x ∈B∗
where the Kolmogorov complexity K(n) of an integer n is the length of the
shortest preﬁx code for n.

424
I. Wood, P. Sunehag, and M. Hutter
Proof. First, note that M(x) −M(x0) −M(x1) measures the set of programs
U −1(x) for which U outputs x and no more. Consider the set
P := {ql′p | p ∈B∗, U(p) ⊒x}
where l′ is a shortest preﬁx code for ℓ(x) and q is a program such that U(ql′p)
executes U(p) until ℓ(x) bits are output, then stops.
Now, for each r = ql′p ∈P we have U(r) = x since U(p) ⊒x and q executes
U(p) until ℓ(x) bits are output. Thus P ⊆U −1(x) and
|P| ≤|U −1(x)|
(12)
Also P = ql′U −1(x∗) := {s = ql′p | p ∈U −1(x∗)}, and so
|P| = 2−ℓ(ql′)|U −1(x∗)|
(13)
combining (12) and (13) and noting that M(x) −M(x0) −M(x1) = |U −1(x)|
and M(x) = |U −1(x∗)| we obtain
M(x) −M(x0) −M(x1) = |U −1(x)|
≥|P|
= 2−ℓ(ql′)|U −1(x∗)|
= 2−ℓ(q)2−K(ℓ(x))M(x)
Setting c := 2−ℓ(q) this proves the result.
⊓⊔
Theorem 2. Not all universally dominant enumerable semimeasures are uni-
versal mixtures: Uξ ⊂Uδ
Proof. Take some universally dominant semimeasure δ, then deﬁne δ′(ϵ) :=
1, δ′(0) = δ′(1) :=
1
2, δ′(bx) :=
1
2δ(bx) for b ∈B, x ∈B∗\ϵ. δ′ is clearly a
universally dominant enumerable semimeasure with δ′(0)+ δ′(1) = δ′(ϵ), and by
Lemma 5 it is not a universal mixture.
⊓⊔
5
Conclusions
One of Solomonoﬀ’s more famous contributions is the invention of a theoretically
ideal universal induction mechanism. The universal prior used in this mecha-
nism can be deﬁned/constructed in several ways. We clarify the relationships
between three diﬀerent deﬁnitions of universal priors, namely universal mix-
tures, Solomonoﬀpriors and universally dominant semimeasures. We show that
the class of universal mixtures and the class of Solomonoﬀpriors are exactly the
same while the class of universally dominant lower semicomputable semimea-
sures is a strictly larger set.
We have identiﬁed some aspects of the discrepancy between Solomonoﬀ
priors/universal mixtures and universally dominant lower semicomputable

(Non-)Equivalence of Universal Priors
425
semimeasures, however a clearer understanding and characterisation would be
of interest.
Since universal dominance is all that is needed to prove convergence for uni-
versal induction [4,10] it is interesting to ask whether the extra properties of the
smaller class of Solomonoﬀpriors have any positive consequences for universal
induction.
Acknowledgements. We would like to acknowledge the contribution of an
anonymous reviewer to a more elegant presentation of the proof of Lemma 4.
This work was supported by ARC grant DP0988049.
References
1. Chaitin, G.J.: A theory of program size formally identical to information theory.
Journal of the ACM 22, 329–340 (1975)
2. Downey, R.G., Hirschfeldt, D.R.: Kolmogorov complexity of ﬁnite strings. In: Al-
gorithmic Randomness and Complexity, pp. 110–153. Springer, New York (2010)
3. Figueira, S., Stephan, F., Wu, G.: Randomness and universal machines. Journal of
Complexity 22(6), 738–751 (2006)
4. Hutter, M.: Universal artiﬁcial intelligence: Sequential decisions based on algorith-
mic probability. Springer (2005)
5. Hutter, M.: On universal prediction and Bayesian conﬁrmation. Theoretical Com-
puter Science 384(1), 33–48 (2007)
6. Levin, L.A.: Some Theorems on the Algorithmic Approach to Probability Theory
and Information Theory. Phd dissertation, Moscow University, Moscow (1971)
7. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions, 3rd edn. Springer (2008)
8. Schnorr, C.: Process complexity and eﬀective random tests. Journal of Computer
and System Sciences 7(4), 376–388 (1973)
9. Solomonoﬀ, R.J.: A formal theory of inductive inference. parts I and II. Information
and Control 7(2), 224–254 (1964)
10. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Transactions on Information Theory 24(4), 422–432 (1978)
11. Turing, A.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society 42(2), 230–265
(1936)
12. Zvonkin, A.K., Levin, L.A.: The complexity of ﬁnite objects and the development of
the concepts of information and randomness by means of the theory of algorithms.
Russian Mathematical Surveys 25(6), 83–124 (1970)

A Syntactic Approach to Prediction
John Woodward and Jerry Swan
Computer Science, The University of Nottingham UK
{john.woodward,jerry.swan}@nottingham.ac.uk
http://www.cs.nott.ac.uk/~jrw,~jps
Abstract. A central question in the empirical sciences is; given a body
of data how do we best attempt to make predictions? There are subtle
diﬀerences between current approaches which include Minimum Message
Length (MML) and Solomonoﬀ’s theory of induction [24].
The nature of hypothesis spaces is explored and we observe a correla-
tion between the complexity of a function and the frequency with which
it is represented. There is not a single best hypothesis, as suggested by
Occam’s razor (which says prefer the simplest), but a set of functionally
equivalent hypotheses. One set of hypotheses is preferred over another
set because it is larger, thus giving the impression simpler functions gen-
eralize better. The probabilistic weighting of one set of hypotheses is
given by the relative size of its equivalence class. We justify Occam’s
razor by a counting argument over the hypothesis space.
Occam’s razor contrasts with the No Free Lunch theorems which state
that it impossible for one machine learning algorithm to generalize better
than any other. No Free Lunch theorems assume a distribution over
functions, whereas Occam’s razor assumes a distribution over programs.
Keywords: Bias, Conservation of Generalization, Induction, No Free
Lunch (NFL), Occam’s Razor, Complexity, Machine Learning, Genetic
Programming.
1
Introduction
There are at least three approaches to the problem of induction; (1) compression
(Kolmogorov Complexity), (2) probability (Algorithmic Complexity), and (3)
minimum message length (Information Theory), each with similarities and subtle
diﬀerences (see sections 1 of [4,24]). Turing machines feature in the ﬁrst two,
while a coding scheme features in the third. (1) and (3) look for an explanation
(i.e. a single theory), while (2) takes a weighted sum over all hypotheses in
order to make predictions (see section 8 of [24] and section 4 of [4]). While
Solomonoﬀ[22,23] laid the foundations for a predictive approach, and recently
emphasized genetic programming [21]. One of the reasons being that it can
embrace recursion, unlike some other methods. In addition, genetic programming
readily lends itself to both explanation and prediction motives, in that genetic
programming can either return a single program, or can make predictions based
on the current population (so called ensemble methods). This paper examines
induction from a genetic programming perspective.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 426–438, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

A Justiﬁcation of Occam’s Razor
427
A hypothesis is an eﬀective procedure mapping input data to output data.
Induction is the process of being given a set of input–output pairs, ﬁnding a
hypothesis which makes accurate predictions [2,13,14,22,23]. The process of gen-
erating hypotheses is largely done by scientists, but is now the focus of machine
learning [14]. This is realized by deﬁning a hypothesis space (program space or
search space). A hypothesis space is the set of all instances of some representation
(with some size limit imposed). It is then sampled, often using nature inspired
methods, to ﬁnd a hypothesis. The subject of this article is about the static
nature of hypothesis spaces, rather than the dynamics of how they are sampled.
Investigations into the nature of computational hypothesis spaces [10,12] leads
to two key observations;
KEY OBSERVATION 1: Simple functions are represented more frequently in
a hypothesis space, and complex functions are represented less frequently.
KEY OBSERVATION 2: Above a certain size of program, the frequency
distribution of functions represented in a hypothesis space does not alter.
Occam’s razor has been adopted by the machine learning community and has
been taken to mean; “The simplest explanation is best” [2], echoed BY Einstein;
“Make everything as simple as possible, but no simpler”. It has been argued that
simpler explanations are more likely to give better predictions on unobserved
data. Kearns et. al. [9] state that Occam’s razor has become a central doctrine
of scientiﬁc methodology. Occam’s razor is essentially a meta-hypothesis; it is a
hypothesis about hypotheses. Why is a simpler hypothesis more likely to gener-
alize to unseen data than a more complex hypothesis? Webb [25] states; “Several
attempts have been made to provide theoretical support for the principle of Oc-
cam’s razor in the machine learning context. However, these amount to no more
than proofs that there are few simple hypotheses and that the probability that
one of any such small selection of hypotheses will ﬁt the data is low”. We also
criticize these attempts in Section 3. While Webb’s paper [25] suggests diﬃcul-
ties with Occam’s Razor, Needham and Dowe [16] do cast doubt over this and
suggest further investigation in needed.
The NFL theorems [20,26,27] state that over all functions, no learning
algorithm has superior performance, negating Occam’s razor [3,8]. We should
therefore scrutinize the assumptions. NFL is argued in semantic terms (i.e. dis-
tributions over functions). We argue in syntactic terms (i.e. distributions over
programs) to support Occam’s Razor.
The remainder of this article is as follows; In Section 2, examples are given
where multiple explanations of the observations are plausible, illustrating that
hypotheses should only be falsiﬁed if they are shown to be incorrect. In Section
3, previous arguments for parsimony are examined. In Section 4, we examine the
assumptions behind NFL theorems. In Section 5, formal deﬁnitions are given.
Section 6 explores the nature of hypothesis spaces. In Section 7, Occam’s razor

428
J. Woodward and J. Swan
Fig. 1. What is the underlying trend which best explains the data? Two possibilities
are shown here, a straight line and a curved line. An Occam’s razor approach would
prefer a simple straight line, compared to a more complicated curvy line, assuming that
the straight line is simpler than the curved line. A NFL approach would say both lines
are equally good descriptions of the observation, as all functions have equal probability.
We will argue that the more complex wavy line is a possible explanation of the data,
but with a lower probability than the simpler straight line.
is proved based on assumptions which have empirical support. The ﬁnal Section
summarizes this article.
2
Illustrative Examples
We give examples in which alternative hypotheses are equally consistent with
observation. While a simpler hypothesis may be consistent with the data, we
can never discard the possibility that a more complex hypothesis is isomorphic
to the process which is responsible for what we observe. Empirically we cannot
diﬀerentiate two hypotheses which are functionally equivalent on the observa-
tions. An experiment can never be constructed which distinguishes them. We
can only make statements about the validity of a hypothesis at a semantic level,
(i.e. regarding the predictions about input and output).
Imagine we have a computer program which generates the sequence
(< input, output > pairs) < 1, 1 >, < 2, 2 >, < 3, 3 > and we are given the task
of ﬁnding a function which describes the data. Given the data the following
three hypothesis are all valid explanations;
H1 : y = x
H2 : y = x + eiπ + sin2(x) + cos2(x)
H3 : y = 0 if x = 100 else y = x

A Justiﬁcation of Occam’s Razor
429
All three hypotheses are syntactically diﬀerent. H1 is semantically equivalent
to H2, and therefore make identical predictions (like Newton and Lagrange, see
following paragraph). However H1 and H2 are semantically distinct from H3,
which is observationally identical to H1 and H2 except at the point x = 100.
The measurement at the point x = 100, and only this measurement, will allow
us to distinguish between these hypotheses and discard either H3 or H1 and
H2. No measurement will ever distinguish between H1 and H2. While H1 and
H2 are trivially semantically equivalent, it maybe the case that a computer pro-
gram equivalent to H2 is responsible for generating the observed data. We are
not interested in inducing a program syntactically equivalent to the program
responsible producing the data. We are interested in inducing a program seman-
tically equivalent to the program producing the data. Note that in example, as
in this paper, we have ignored noise.
Let us now take an example from physics. Newton’s 2nd law of motion is a
perfectly good description of “everyday mechanics”, where the path of particles
is determined by forces. An alternative formulation is Lagrangian mechanics,
which determines the path a particle takes by minimizing a quantity based on
kinetic and potential energies (i.e. the action functional is stationary). These two
approaches are formally equivalent (while a Relativistic formulation of mechanics
is diﬀerent). Based on experimental observation, we cannot diﬀerentiate between
these two theories. A similar situation exists in optics. Indeed (ignoring quantum
mechanics and relativity for a moment), assuming the universe is a physical
computer (i.e. hardware), we will never know if the software running on it is a
Newtonian or Lagrangian program, we can only make observations which will
never distinguish between them. Newton and Lagrange are analogous to H1 and
H2 in the preceding paragraph.
3
Justiﬁcations and Criticisms of Occam’s Razor
[14,19] provide similar arguments. They essentially reduce Occam’s razor to the
fact that there are fewer shorter hypotheses than longer ones. [19] states: “There
are far fewer simple hypotheses than complex ones, so that there is only a small
chance that any simple hypothesis that is wildly incorrect will be consistent with
all observations”. A simple hypothesis has fewer degrees of freedom, so if it is
consistent with the training data, it is more likely to be consistent with validation
data when compared to a more complex hypothesis.
[14] states; “There are fewer short hypotheses than long ones (based on
straightforward combinatorial arguments), it is less likely that one will ﬁnd a
short hypothesis that coincidentally ﬁts the training data. In contrast there are
often many very complex hypotheses that ﬁt the current training data but fail
to generalize correctly to subsequent data”. While this is true, it then raises the
question that, while there are also many complex hypotheses that ﬁt the data
but will fail to generalize, there are also many complex hypotheses that do ﬁt
subsequent data. It still does not explain why we should choose a shorter hy-
pothesis in preference to another one. We should not arbitrarily discard more

430
J. Woodward and J. Swan
complex hypotheses which are consistent with the data. For example, in Section
2, we should not discard Lagrange over Newton on grounds of complexity.
There has also been empirical work for and against Occam’s Razor. [3,15,25].
However, further investigations has revealed ﬂaws in the experimental approach.
For example, in the induction of decision tree, node cardinality was used as an
interpretation of Occam’s Razor [15]. However, the insightful work of Needham
and Dowe, suggests a more sophisticated Minimum Message Length approach
is needed (also see [4] section 4). And, as the title suggests from a paper, “very
simple classiﬁcation rules perform well on most commonly used datasets” [6].
4
No Free Lunch Theorems and Generalization
The NFL theorems are a collection of theorems, which broadly state that, any
gain in performance regarding generalization on one problem, is reﬂected by a
loss in performance on another problem, and clearly contradict Occam’s Ra-
zor. These are referred to as NFL theorems [26,27], the law of Conservation of
Generalization [20] and the Ugly Duckling theorems [5]. We will refer to these
collectively as NFL as this is the more established term. Over a set of all prob-
lems (functions), the positive performance over some learning situations must
be oﬀset by an equal degree of negative performance in others, where the per-
formance of a learning algorithm is measured in terms of its prediction of cases
not included in the training set. For every problem a learner performs well on,
there exists a problem on which the learner performs poorly. An alternative but
equivalent statement is, generalization is a zero sum game. There are various
ways of stating these theorems, but they are all essentially equivalent as one can
be proved from another. All of these arguments for NFL are in purely semantic
terms. That is, they are trying to prove a proposition about the distribution
over function (semantics), by making assumptions about the probability distri-
bution over functions (semantics). In section 7 we will assume a distribution over
programs to prove Occam’s razor.
5
Deﬁnitions
Deﬁnition 1 (Function). A function is a mapping from one set to another.
Deﬁnition 2 (Program). A program is an implementation of a function.
The semantic concept of “increment” is expressed in many diﬀerent ways by
a number of syntactically correct programs containing symbols from the set
{x, +, 1, ; }, see ﬁgure 2. One semantic entity (function) can be expressed by
many syntactic structures (programs). Functions correspond to semantic entities,
and programs correspond to syntactic structures. A hypothesis corresponds to a
program and a set of predictions corresponds to a function.
Deﬁnition 3 (Set of Base Functions). A set of base functions (or primi-
tives) is a ﬁnite set of functions {f1, . . . , fn} which can be used to generate new

A Justiﬁcation of Occam’s Razor
431
Fig. 2. The space of hypotheses (syntactic structures) maps to the space of concepts
(semantic entities). For example, a set of programs maps to a set of functions. Many
hypotheses can map to the same concept. The hypothesis space is partitioned into
equivalence classes deﬁned by the concepts they map onto. Two equivalence classes are
shown by dotted ellipses. Programs representing the same function belong to the same
equivalence class.
functions (e.g. a function h is constructed from f1 and f2; h(x) = f1(f2(x))).
We are not concerned with how the base functions are implemented, which can
be thought of as atomic black boxes. The set of base functions is the same as
functions included in the function set of Genetic Programming, where the goal
is to synthesize a target function from the base set [10].
Langdon [12]) uses diﬀerent sets of base functions for example {NAND},
{AND, OR, NAND, NOR}, and {+, −, ∗, %}, see Section 6 of this paper. Note
that none of these sets are capable of expressing the computable functions.
Deﬁnition 4 (Size). The size of a program is the total number of bits needed to
express it. (However, we could use any reasonable deﬁnition of size. For example,
in [12], the size is the number of nodes in the parse tree.)
Deﬁnition 5 (Descriptive Complexity). The descriptive complexity of a
function, c(f), is the size of the smallest program which expresses it, with respect
to a set of base functions and data structure. This is the same deﬁnition as often
used in Genetic Programming [28,29].
If the set of base functions is capable of expressing the computable functions,
and the deﬁnition of size is the number of bits, then this deﬁnition is equivalent
to Kolmogorov Complexity [13]. The descriptive complexity of a function will in
general depend on the set of base functions.

432
J. Woodward and J. Swan
Deﬁnition 6 (Equivalence Class of Hypotheses). An equivalence class of
hypotheses contains all hypotheses mapping to the same function.
For example, a hypothesis space of Java programs maps to a concept space
of computable functions. This mapping is achieved by the Java Virtual Machine
(I(p) = f, where p is a Java program, f is a computable function and I is the
Java Virtual Machine). I(pi) = I(pj) implies the programs pi and pj compute
the same function and are semantically equivalent, while pi ̸= pj implies the
programs are syntactically diﬀerent. A hypothesis space is a set of programs and
a concept space is a set of functions.
The hypothesis space is partitioned into equivalence classes of semantically
equivalent hypotheses. For example the Java/C programs {y = x + 1; }, {y =
1 + x; }, {y = x + +; }, and {y = + + x; } all belong to the same class called
increment (ﬁgure 2). Newtonian and Lagrangian formulations of mechanics be-
long to the same equivalence class (called classical mechanics), while Special
relativity falls into diﬀerent class as it makes diﬀerent predictions. During the
process of induction, we are eliminating functions inconsistent with the data, and
therefore discarding whole equivalence classes (sets of programs) which compute
these functions. This leaves us with a set of equivalence classes, all of which are
consistent with the data. We are going to argue that the largest equivalence class
of this set is the most probable to contain the hypothesis which generalizes best.
Deﬁnition 7 (Bias). Bias is any preference for choosing one concept (func-
tion) over another. Bias is a probability distribution over the set of functions
expressed by the hypothesis space.
Learning is the induction of a function which is (approximately) functionally
equivalent to the underlying process. An unbiased learning system, by deﬁnition,
is one where each function is equally likely to be produced. We state the deﬁnition
given by [14]: “Any basis for choosing one generalization over another, other than
strict consistency with the observed training instances”.
6
Exploring the Nature of Hypothesis Spaces
6.1
Koza’s Lens Eﬀect
Koza [10] considers the role of representation. He examines the probability of
generating a solution to the even-3-parity problem with three diﬀerent types of
representation; lookup tables, parse trees and Automatically Deﬁned Functions.
For lookup tables, there is the uniform chance of generating a correct lookup
table of 1 in 256. Such a representation has no bias as each function is as likely as
any other function to be generated. Given the function set {AND, OR, NAND,
NOR}, he generates 107 trees at random but ﬁnds no solutions. However, if
two, two argument Automatically Deﬁned Functions are used, 35 solutions are
generated. Koza calls this diﬀerence in the probability of generating a function
due to the representation “the lens eﬀect” (we call it representational bias), and

A Justiﬁcation of Occam’s Razor
433
talks about the problem environment being viewed through “the lens of a given
type of representation”. The diﬀerence between the parse tree results and the
Automatically Deﬁned Functions results is due entirely to the representation.
6.2
Langdon’s Program Spaces
Langdon [12] generates the hypothesis spaces for 2, 3, and 4 input programs using
the function set {NAND} and plots the proportion of programs that represent
diﬀerent functions. With the 2 input function plots, the functions are in lexical
order, and he points out that this is not the most convenient ordering, so in other
plots they are presented in order of decreasing frequency. From the plots for 3 and
4 inputs, it can be seen that more complex functions occur with less frequency
than simpler functions. All Boolean functions of 3 inputs are considered when
using base functions of {AND, OR, NAND, NOR} and {AND, OR, NAND,
NOR, XOR}. As a general rule, we can see from these two plots (ﬁgures 5 and
6 in [12]) that more complex functions are represented less frequently. As it is
impractical to analyze such large program spaces generated by 6 input programs,
Langdon concentrates on just two functions, namely the always-on-6 function
and the even-6-parity function. In this set of ﬁgures, [12] plots the error but
again a correlation between the error and frequency can be observed.
Langdon [12] uses a polynomial of degree 6 for function regression with a func-
tion set {+, −, ∗, %} (where % is divide, protected against division by zero) and
a terminal set consisting of the input x and random constants. Plots of the mean
error against the proportion of times that error is observed are presented for in-
creasing program length. Again, similar to the situation with Boolean program
spaces, there is a correlation between error and the proportion of programs.
The artiﬁcial ant problem is a standard benchmark problem in the genetic
programming literature. In some ways this is more complex than the previous
two domains of logic and arithmetic functions as it includes side eﬀects and
iteration, that is it is Turing Complete. A plot of ﬁtness against program length
against proportion is presented. Again there is a correlation between the ﬁtness
and the proportion of program corresponding to that ﬁtness similar to that seen
with the logical functions and symbolic regression.
7
Justiﬁcation of Occam’s Razor
A preference for one function over another is expressed as p(f1) > p(f2) where,
the p(f1) is the probability of function f1. We denote the complexity of a function
f as c(f), and c(f1) < c(f2) meaning, f1 is less complex than f2. We combine
these to provide a statement of Occam’s razor;
Theorem 1 (Occam’s razor). p(f1) > p(f2) ↔c(f1) < c(f2)
7.1
Occam’s Razor, Uniform Sampling of Program Space
We begin by deﬁning our notation. P is the hypothesis space (i.e. a set of pro-
grams). |P| is the size of the space (i.e. the cardinality of the set of programs).

434
J. Woodward and J. Swan
F is the concept space (i.e. a set of functions represented by the programs in
P). |F| is the size of the space (i.e. the cardinality of the set of functions). If two
programs pi and pj map to the same function (i.e. they are interpreted as the
same function, I(pi) = f = I(pj)), they belong to the same equivalence class
(i.e. pi ∈[pj] ↔I(pi) = I(pj)). The notation [pi] denotes the equivalence class
which contains the program pi (i.e. given I(pi) = I(pj),then [pi] = [pj]). The
size of an equivalence class [pi] is denoted by |[pi]|.
We make two assumptions. The ﬁrst assumption is that we uniformly sample
the hypothesis space, and therefore, the probability of sampling a given program
is 1/|P|. This assumption can be relaxed somewhat and in discussed later in
this section. The second assumption is that there are fewer hypotheses that
represent complex functions: |[p1]| > |[p2]| ↔c(f1) < c(f2), where I(p1) = f1
and I(p2) = f2. Note that |[p1]|/|P| = p(I(p1)) = p(f1), that is |[p1]|/|P| = p(f1)
i.e. the probability of sampling a function is given by the ratio of the size of the
equivalence class containing all the programs which are interpreted that function,
divided by the size of the hypothesis space. Indeed, one can show that if there
are many equivalent programs of the same length, then there must be a shorter
equivalent program and a proof sketch of this assumption is given in [7].
Proof (Occam’s Razor). We begin the proof by starting from the assumption;
|[p1]| > |[p2]| ↔c(f1) < c(f2)
Dividing the left hand side by |P|,
|[p1]|/|P| > |[p2]|/|P| ↔c(f1) < c(f2)
As |[p1]|/|P| = p(I(p1)) = p(f1), we can rewrite this as
p(f1) > p(f2) ↔c(f1) < c(f2)
which is a statement of Occam’s razor (theorem 1).
We take the probability distribution p(f) to be the frequency with which the
functions are represented in the hypothesis space (i.e. the size of the equivalence
class). The assumption is |[p1]| > |[p2]| ↔c(f1) < c(f2), where p(I(p1)) = p(f1),
which we have not proved but is reasonable based on empirical work in [12]. We
have not explicitly assumed more complex functions are less likely, but rather,
the size of equivalence classes are larger if they contain programs which represent
simple functions. For further empirical work supporting this see [16].
7.2
Occam’s Razor, Non-uniform Sampling of Program Space
Above we assumed the hypothesis space was uniformly sampled. Now we make
a much milder assumption, and theorem 1 is still true for a large number of
distributions, not just a uniform distribution over the hypothesis space.
The probability of sampling some function is p(f). Let the probability of
sampling some function, given a certain hypothesis size be p(f, s), and we know

A Justiﬁcation of Occam’s Razor
435
p(f, s) is independent of size above some threshold [12]. Let the limiting distribu-
tion be D(f). Thus, p(f, s) = D(f), when s > θ, where θ is the size threshold. S
is the size of the largest hypotheses in the hypothesis space. Instead of uniformly
sampling the search space, we weight hypotheses of size s with w(s). The ques-
tion becomes, what is the distribution of functions if we sample the hypothesis
space with an arbitrary weighting over program size above the threshold.
Proof (Independence of Size). We begin the proof by stating the expression for
the distribution of functions, weighted by w(s)
S
s>θ p(f, s).w(s)
As p(f, s) = D(f), when s > θ,
p(f, s). S
s>θ(w(s))
As S
s>θ(w(s)) = 1, this reduces to D(f). Therefore,
S
s>θ p(f, s).w(s) = D(f)
As the distribution over functions is independent of program size (above a certain
program size), if we take any distribution over any program size (above the
threshold), we obtain the same limiting probability distribution.
This second proof is vital to the full story. The ﬁrst proof says, if we uniformly
sample a program space, we encounter more simple functions than complex func-
tions. The second proof says, if we sample a program space given programs of
the same size equal weight (but programs of diﬀerent size may have diﬀerent
weight), then we observe the same distribution of functions as if we had sam-
pled the program space uniformly. In addition, there are issues if we attempt to
sample over an inﬁnite sized space; however we can take the limit as the size of
the space increases (and this limit appears to exist [12]).
8
Summary
Occam’s razor was a criticism of theories which became more complex without
a corresponding increase in predictive power. i.e. the added complexity was re-
dundant. As we are trying to prove a theorem which is about probabilities over
functions, we are not in a position to make assumptions about probabilities over
functions. such arguments can always be accused of circularity. This leaves us
with the approach of looking at probabilities over programs.
From the empirical work of [12] we make two key observations (see Section 1).
Firstly, simple functions can be expressed in more ways than complex function,
and that there are more programs that map to simple functions than programs
that map to complex functions. This suggests dividing the program space up into
equivalence classes, where members of each class represent the same function.
We make the assumption that the larger the equivalence class, the smaller the
smallest program is in that set. The larger the equivalence class, the smaller

436
J. Woodward and J. Swan
the complexity of the function represented by programs in that class. As these
equivalence classes are diﬀerent sizes, uniformly sampling the program space will
translate into a non-uniform sampling of the function space (i.e. a bias).
Secondly, as the limit of the of the programs in the hypothesis space in-
creases, the proportion of programs that correspond to diﬀerent functions does
not change. The probability distribution tends towards some limiting distribu-
tion. What is more, this convergence is fast. Thus, in many machine learning
paradigms, while a hard limit on the size of the hypothesis space is imposed,
this does not aﬀect the frequency with which functions are represented, as long
as the size of the hypothesis space is larger than a certain threshold.
Occam’s razor states a simpler function is more likely to generalize than a
more complex function. While we agree; we argue that the underlying reason
is that the simpler function is represented more frequently in the hypothesis
space than more complex functions and this is the reason it should be chosen.
We should prefer the function for reasons of probability rather than reasons
of complexity, as it is ultimately probabilities we are interested in. We restate
Occam’s razor: the most probable function is the one which belongs to the largest
class of functionally equivalent hypotheses, consistent with the observed data.
The most probable function to generalize the observed data is the one that is
most frequently represented in the hypothesis space. By making observations, we
can discard individual functions, but we cannot discard individual programs as
we can only discard a whole equivalence class of functionally identical programs.
There are two points to be made relating to this restatement of Occam’s razor.
Firstly, this is closer to the induction process. Secondly, if we take Occam’s
razor as “prefer the simpler”, this gives us no indication of statistical conﬁdence.
Our approach, allows us to do this by considering the ratio of the sizes of the
equivalence classes to which functions belong.
Occam’s razor and NFL are mutually exclusive. In the former case there is a
bias toward simpler functions, and in the latter case there is no bias. The NFL
results should come as no surprise to the reader. Paraphrasing the theorems; if
we assume a uniform distribution over a set of all functions, and after making
some observations, then there is still a uniform distribution over the remaining
functions consistent with observation, and hence we cannot prefer one function
over another. Initially, if there is no bias, then ﬁnally there is no bias. With
Occam’s razor, we start with a monotonically decreasing distribution over a set of
functions ordered by increasing complexity, and the act of observation dismisses
some functions, then there is still a monotonically decreasing distribution over
the remaining consistent functions. There are situations where NFL is applicable,
and situations where Occam’s razor is applicable.
I suspect that many people who regard NFL as the holy grail (as Hutter [7]
so eloquently puts it), have not understood the assumptions, or the reasons for
an alternative i.e. Occam’s Razor. We would not attempt to predict next week’s
lottery numbers based only on the sequence of past lottery numbers as this is
eﬀectively white noise. We could, however, in principle, given the initial state
of the lottery number generating machine, predict precisely next week’s lottery

A Justiﬁcation of Occam’s Razor
437
numbers, as this initial information uniquely determines future states of the sys-
tem. While Solomonoﬀ’s theory cannot be applied in practice, approximations
can and are applied through out the machine learning literature. NFL, on the
other hand, fails to have a single application (to the author’s best knowledge). In-
deed, nobody cares about the optimum of a white noise (incompressible) function
[7]. Nor do we care about the optimum of very simple functions (e.g. constants).
As Langdon [11] states; “most functions are constants and the remainder are
mostly parsimonious”. There is however a Goldilock zone of interesting func-
tions which are sensible to apply machine learning algorithms to (i.e. not too
simple, not too complex). Physicists are largely interesting in continuous func-
tions [18], rather than incompressible functions. In addition NFL is not valid for
continuous functions [1] or in a number of machine learning contexts (genetic
programming, neural networks, hyper-heuristics) [17].
References
1. Auger, A., Teytaud, O.: Continuous lunches are free plus the design of optimal
optimization algorithms. Algorithmica 57(1), 121–146 (2010)
2. Cover, T.M., Thomas, J.A.: Elements of information theory. Wiley-Interscience,
New York (1991)
3. Domingos, P.: The role of occam’s razor in knowledge discovery. Data Min. Knowl.
Discov. 3(4), 409–425 (1999)
4. Dowe, D.L.: MML, hybrid Bayesian network graphical models, statistical consis-
tency, invariance and uniqueness. In: Handbook of the Philosophy of Science (HPS).
Philosophy of Statistics, vol. 7, pp. 901–982 (2011)
5. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation, 2nd edn. Wiley-
Interscience (November 2000)
6. Holte, R.C.: Very simple classiﬁcation rules perform well on most commonly used
datasets. In: Machine Learning, pp. 63–91 (1993)
7. Hutter, M.: A complete theory of everything (will be subjective). Algorithms 3(7),
360–374 (2010)
8. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability, 300 pages. Springer, Berlin (2004),
http://www.idsia.ch/~marcus/ai/uaibook.htm
9. Kearns, M.J., Vazirani, U.V.: An introduction to computational learning theory.
MIT Press, Cambridge (1994)
10. Koza, J.R.: Genetic Programming II: Automatic Discovery of Reusable Programs.
The MIT Press, Cambridge (1994)
11. Langdon, W.B.: Scaling of program functionality. Genetic Programming and Evolv-
able Machines 10(1), 5–36 (2009)
12. William, B.: Langdon. Scaling of program ﬁtness spaces. Evolutionary Computa-
tion 7(4), 399–428 (1999)
13. Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its applications,
2nd edn. Springer-Verlag New York, Inc., Secaucus (1997)
14. Mitchell, T.M.: Machine Learning. McGraw-Hill, New York (1997)
15. Murphy, P.M., Pazzani, M.J.: Exploring the decision forest: An empirical investi-
gation of occams razor in decision tree induction. Journal of Artiﬁcial Intelligence
Research, 257–275 (1994)

438
J. Woodward and J. Swan
16. Needham, S.L., Dowe, D.L.: Message length as an eﬀective ockham’s razor in deci-
sion tree induction. In: Proc. 8th International Workshop on Artiﬁcial Intelligence
and Statistics (AI+STATS 2001), Key West, Florida, U.S.A., pp. 253–260 (January
2001)
17. Poli, R., Graﬀ, M., McPhee, N.F.: Free lunches for function and program induction.
In: Proceedings of the Tenth ACM SIGEVO Workshop on Foundations of Genetic
Algorithms (FOGA 2009), Orlando, Florida, USA, January 9-11, pp. 183–194.
ACM (2009)
18. Rogers, H.: Theory of recursive functions and eﬀective computability. McGraw-Hill
series in higher mathematics. MIT Press (1987)
19. Russell, S.J., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach. Pearson Ed-
ucation (2003)
20. Schaﬀer, C.: A conservation law for generalization performance. In: Proceedings of
the Eleventh International Conference on Machine Learning, pp. 259–265. Morgan
Kaufmann (1994)
21. Solomonoﬀ, R.: Machine learning - past and future. In: The Dartmouth Artiﬁcial
Intelligence Conference, AI@50, pp. 257–275. Dartmouth, N.H. (2006)
22. Solomonoﬀ, R.J.: A formal theory of inductive inference. part i. Information and
Control 7(1), 1–22 (1964)
23. Solomonoﬀ, R.J.: A formal theory of inductive inference. part ii. Information and
Control 7(2), 224–254 (1964)
24. Wallace, C.S., Dowe, D.L.: Minimum message length and kolmogorov complexity.
Computer Journal 42, 270–283 (1999)
25. Webb: Generality is more signiﬁcant than complexity: Toward an alternative to
occam’s razor. In: Australian Joint Conference on Artiﬁcial Intelligence (AJCAI)
(1994)
26. Wolpert, D.H., Macready, W.G.: No free lunch theorems for search. Technical Re-
port SFI-TR-95-02-010, Santa Fe, NM (1995)
27. Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation 1(1), 67–82 (1997)
28. Woodward, J.R.: Complexity and cartesian genetic programming. In: Collet, P.,
Tomassini, M., Ebner, M., Gustafson, S., Ek´art, A. (eds.) EuroGP 2006. LNCS,
vol. 3905, pp. 260–269. Springer, Heidelberg (2006)
29. Woodward, J.R.: Invariance of function complexity under primitive recursive func-
tions. In: Collet, P., Tomassini, M., Ebner, M., Gustafson, S., Ek´art, A. (eds.)
EuroGP 2006. LNCS, vol. 3905, pp. 310–319. Springer, Heidelberg (2006)

Developing Machine Intelligence within P2P
Networks Using a Distributed Associative
Memory
Amiza Amir1,3, Anang Hudaya M. Amin2, and Asad Khan1
1 Clayton School of IT, Monash University, Melbourne, Australia
{amiza.amir,asad.khan}@monash.edu
2 Multimedia University, Malaysia
anang.amin@mmu.edu.my
3 Universiti Malaysia Perlis, Malaysia
Abstract. In this paper, we discuss machine intelligence for conducting
routine tasks within the Internet. We demonstrate a technique, called the
Distributed Associative Memory Tree (DASMET), to deal with multi-
feature recognition in a peer-to-peer (P2P)-based system. Shared content
in P2P-based system is predominantly multimedia ﬁles. Multi-feature is
an appealing way to tackle pattern recognition in this domain. In our
scheme, the information held at individual peers is integrated into a
common knowledge base in a logical tree like structure and relies on
the robustness of a well-designed structured P2P overlay to cope with
dynamic networks. Additionally, we also incorporate a consistent and
secure backup scheme to ensure its reliability. We compare our scheme
to the Backpropagation network and the Radial Basis Function (RBF)
network on two standard datasets, for comparative accuracy. We also
show that our scheme is scalable as increasing the number of stored
patterns does not signiﬁcantly aﬀect the processing time.
1
Introduction
Machine intelligence that can perform better than human intellect depends on
the ability to recognize pattern regularities from given evidence which allows for
better inference [1, 2]. With pervasiveness of the Internet, such intelligence can
be provisioned using an in-network associative memory approach. This paper
looks at a practical application of network intelligence for recognising content
involving multi-feature information within a P2P system.
P2P-based systems encompass powerful capabilities but have become contro-
versial due to copyright infringement issues and more recently for sensitive data
leakage. P2P involves dealing with complex datasets such as image, audio, and
video, hence signiﬁcant eﬀort is required to analyse these datasets. Doing so is
infeasible owing to the computational complexity in automatic content recog-
nition and the reluctance on the part of peers to share their private resources
inordinately.
D.L. Dowe (Ed.): SolomonoﬀFestschrift, LNAI 7070, pp. 439–443, 2013.
c
⃝Springer-Verlag Berlin Heidelberg 2013

440
A. Amir, A.H.M. Amin, and A. Khan
Current approaches in pattern recognition [3–5] incur high computational
complexity that inhibits their capability to scale up to handle a large number of
features. This limits their ability to seamlessly and eﬀectively perform recogni-
tion and classiﬁcation involving complex datasets.
Multiple-feature implementations consider all signiﬁcant features which repre-
sent a particular set of patterns from many modalities such as vision and sound.
Therefore, the aim is to reduce the bias eﬀect of selecting only a single feature for
recognition. This is useful in the P2P domain where objects may be processed
by diﬀerent types of analyses and forms such as sound and vision at various
locations.
In this paper, we demonstrate the implementation of a fully distributed as-
sociative memory for multi-feature recognition within P2P networks. Our un-
derlying approach requires that all peers share their resources in recognizing
objects or patterns. Each peer is responsible for sensing, interpreting and re-
membering patterns by working collaboratively with other peers to recognise
an object. This forms a distributed recognition network which can continue to
scale-up with increasing number of features in the input patterns. Some P2P
networks may comprise hundreds of thousands or even to millions of peers. Such
large-scale networks carry vast amount of information and the in-network com-
putational resources for processing it through appropriately designed schemes
that can eﬀectively leverage the distributed content and resources.
2
The Distributed Associative Memory Tree (DASMET)
Our approach (called the Distributed Associative Memory Tree (DASMET) [6])
is adapted from the Hierarchical Graph Neuron (HGN) [7]. The HGN is an asso-
ciative memory for wireless sensor environments. Other than its highly-parallel
nature, the strength of this scheme is its lightweight processing and single cycle
learning. However, the number of processing nodes and communication costs are
signiﬁcantly increased with an increase in number of features of this scheme.
The DASMET inherits the single cycle learning and accuracy characteristics
of the HGN but signiﬁcantly reduces the number of nodes and communications
required by the HGN [6]. Each peer is only responsible to process a subset of
features. Each distinct pattern that occurs at a peer is stored and assigned a
unique index number. The most similar indices1 are then sent to the parent
node. The parent node then performs the same action based on the combined
information from its children. These results are combined iteratively in a tree
structure until these converge at the root. At the rootnode, the class label for
each distinct index is stored during the training phase. The recall decision is
made based on the majority class amongst the most similar indices. An increas-
ing number of features may be easily handled by recruiting more peers2 into
1 It is possible to have several indices with the highest similarity.
2 It is not a problem in most P2P systems which usually have in the order of hundred
thousands of nodes.

Developing Machine Intelligence
441
the associative memory network. Since all subsets of features are processed si-
multaneously, the learning time remains low resulting in a scalable algorithm
capable of handling high dimensional multi-feature datasets. Thus, DASMET is
a fully distributed associative memory algorithm that facilitates accurate, eﬃ-
cient and scalable multi-feature pattern classiﬁcation. Fig. 1 shows an example
of a DASMET structure for 8 diﬀerent feature sets. The number of leaves equals
the number of distinct feature sets. It may also be set to the number of feature
subsets which are split from the original set of features. This is done to reduce
the dimensionality of the feature set.
Rootnode
Leadernode
Hubnode
feature 
set 1 
feature 
set 2 
feature 
set 3 
feature 
set 4 
feature 
set 5 
feature 
set 6 
feature 
set 7 
feature 
set 8 
Fig. 1. Example of DASMET architecture for 8 diﬀerent feature sets. The predeﬁned
maximum children of each node, ϕ is 2. Each feature set is assigned to a hubnode. It
has 8 hubnodes and 6 leadernodes.
3
DASMET within a Dynamic Network
Inherent dynamics of a P2P system impede the system from working correctly.
We rely on structured P2P overlays such as Kademlia [8] and Chord [9] to link
peers. These overlays are well-designed to provide robustness. Furthermore, we
create data redundancy across the network to ensure data availability and sched-
uled maintenance mechanism to maintain the availability of data. A peer may
be responsible for a particular session before failure or may go oﬄine and an-
other peer will be able to replace it without a signiﬁcant degradation to the
system. In order to synchronise local replicas, consistent data update and con-
trol mechanisms are provided by byzantine fault tolerance replication protocols
[10]. The DASMET network periodically recovers through a scheduled recovery
mechanism. All nodes work collaboratively to detect any change to the system
and perform a node’s recovery if needed.
4
Experimental Result
For the purpose of ensuring the suitability of our scheme for P2P networks, we
restricted the processing nodes in our simulations to a single machine with a limit
of 2GB RAM. Owing to this resource restriction, we were unable to test large

442
A. Amir, A.H.M. Amin, and A. Khan
dataset for comparison with the BackPropagation network [4] and the Radial
Basis Function (RBF) network [11] since these methods require large amount
of memory to process large datasets. Therefore, to show the accuracy of our
distributed scheme compared to the state-of-the-art neural networks which run
on a single site, we made comparison with two smaller datasets. These were
Adult and Spam datasets from UCI Data Mining Repository [12]. The Adult
dataset consists of 14 features and the Spam dataset consists of 57 features. The
results are presented in the Table 1 below.
Table 1. Accuracy comparison of DASMET with single site implementations of neural
networks. Accuracy is the percentage of correct classiﬁcation.
Accuracy (%)
Methods
Adult (14 features) Spam (57 features)
Backpropagation Network
84
92
RBF Network
85
92
DASMET
79
98
Our distributed approach performs marginally less than the Backpropagation
network and the RBF network for the Adult dataset. However, it provides better
results for the Spam dataset. Note that it is not our intention to outperform other
methods in this work but to show that our scheme can be easily distributed over
a P2P network with comparable accuracy to a centralised scheme. More results
related to accuracy with larger dataset can be found in [6].
We further tested the scalability of the scheme in two experiments involving
750 features and 1500 features. We stored up to 40, 000 patterns in the system
in the both experiments. As shown in Fig. 2, our simulation result shows that
the store time increases slowly with the increase in stored pattern reﬂecting the
scalability of our scheme.
 1
 1.2
 1.4
 1.6
 1.8
 2
 0
 5
 10
 15
 20
 25
 30
 35
 40
Store Time (seconds)
Number of stored patterns (thousands)
(a) 750 features (84 sets)
 1
 1.2
 1.4
 1.6
 1.8
 2
 0
 5
 10
 15
 20
 25
 30
 35
 40
Store Time (seconds)
Number of stored patterns (thousands)
(b) 1500 features (150 sets)
Fig. 2. Store time per input pattern with increasing number of stored patterns

Developing Machine Intelligence
443
5
Conclusion
In this paper, we demonstrate the DASMET approach for multi-feature pattern
recognition within a P2P network. A peer only computes a subset of feature
from the feature set and this is performed separately in parallel by a group of
peers. The recognition results from these peers are then combined iteratively in
a tree structure. Thereby signiﬁcantly reducing the computation complexity at
a node in terms of space and time. The success of this preliminary work provides
the ﬁrst concrete steps toward achieving machine intelligence through a highly
scalable associative memory resource that forms within the body of the network
and allows rapid correlation of pattern data. Such a resource will provide a
practical test bed for advanced applications built on Solomonoﬀ’s theory.
References
1. Hern´andez-Orallo, J., Dowe, D.L.: Measuring universal intelligence: Towards an
anytime intelligence test. Artif. Intell. 174, 1508–1539 (2010)
2. Solomonoﬀ, R.: A formal theory of inductive inference: Part II. Information and
Control 7(2), 224–254 (1964)
3. Hopﬁeld, J.J.: Neural networks and physical system with emergent collective com-
putational properties, pp. 2554–2558 (1982)
4. Hecht-Nielsen, R.: Theory of the backpropagation neural network. In: International
Joint Conference on Neural Networks, IJCNN, pp. 593–605 (1989)
5. Browne, M., Ghidary, S.S., Mayer, N.M.: Convolutional neural networks for image
processing with applications in mobile robotics. In: Speech, Audio, Image and
Biomedical Signal Processing using Neural Networks, pp. 327–349 (2008)
6. Amir, A., Muhamad Amin, A., Khan, A.: A multi-feature pattern recognition
for P2P-based system using in-network associative memory. Technical Report
2011/265, Clayton School of IT, Monash University, Victoria, Australia (2011)
7. Nasution, B., Khan, A.I.: A hierarchical graph neuron scheme for real-time pattern
recognition. IEEE Transactions on Neural Networks, 212–229 (2008)
8. Maymounkov, P., Mazi`eres, D.: Kademlia: A peer-to-peer information system based
on the XOR metric. In: Druschel, P., Kaashoek, M.F., Rowstron, A. (eds.) IPTPS
2002. LNCS, vol. 2429, pp. 53–65. Springer, Heidelberg (2002)
9. Stoica, I., Morris, R., Karger, D., Kaashoek, M.F., Balakrishnan, H.: Chord: A
scalable peer-to-peer lookup service for internet applications. SIGCOMM Comput.
Commun. Rev. 31, 149–160 (2001)
10. Castro, M., Liskov, B.: Practical byzantine fault tolerance and proactive recovery.
ACM Transactions on Computer Systems (TOCS) 20(4), 398–461 (2002)
11. Buhmann, M.D.: Radial Basis Functions: Theory and Implementations. Cambridge
University Press (2003)
12. Frank, A., Asuncion, A.: UCI machine learning repository (2010)

Author Index
Allison, Lloyd
261
Amin, Anang Hudaya M.
439
Amir, Amiza
439
Araki, Kenji
318
Balduzzi, David
65
Campbell, Douglas
79
Corander, Jukka
91
Cui, Yaqiong
91
de Azevedo da Rocha, Ricardo Luis
106
Dessalles, Jean-Louis
119
Dowe, David L.
1
Ellison, T. Mark
131
Evans, Scott
184
Freivalds, R¯usi¸nˇs
142
Fresco, Nir
155
Gracini Guiraldelli, Ricardo Henrique
106
Hall, J. Storrs
174
Hao, Yuan
184
Honavar, Vasant
339
Hu, Bing
184
Hutter, Marcus
223, 386, 417
Inojosa da Silva Filho, Reginaldo
106
Jankowski, Norbert
198
Keogh, Eamonn
184
Khan, Asad
439
King, P. Allen
211
Koski, Timo
91
Lattimore, Tor
223
Legg, Shane
236
Levin, Leonid A.
53
Li, Ming
55
Lonardi, Stefano
184
Makalic, Enes
250, 261
Miyabe, Kenshi
273
Muramoto, Koichi
318
¨Ozkural, Eray
285
Pelckmans, Kristiaan
299
Petersen, Steve
306
Rakthanmanon, Thanawin
184
Rzepka, Rafal
318
Saleeby, Elias G.
366
Schmidt, Daniel F.
250, 327
Silvescu, Adrian
339
Solomonoﬀ, Alex
351
Solomonoﬀ, Grace
37
Solomonoﬀ, Ray J.
366
Sunehag, Peter
386, 417
Suzuki, Joe
399
Swan, Jerry
426
Takahashi, Hayato
411
Veness, Joel
236
Wood, Ian
417
Woodward, John
426

