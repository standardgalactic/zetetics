Proceedings of Machine Learning Research vol 120:1‚Äì16, 2020
2nd Annual Conference on Learning for Dynamics and Control
Objective Mismatch in Model-based Reinforcement Learning
Nathan Lambert
NOL@BERKELEY.EDU
University of California, Berkeley
Brandon Amos
BDA@FB.COM
Omry Yadan
OMRY@FB.COM
Roberto Calandra
RCALANDRA@FB.COM
Facebook AI Research
Editors: A. Bayen, A. Jadbabaie, G. J. Pappas, P. Parrilo, B. Recht, C. Tomlin, M. Zeilinger
Abstract
Model-based reinforcement learning (MBRL) is a powerful framework for data-efÔ¨Åciently learning
control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced
function approximators and planning schemes, with little development of the general framework.
In this paper, we identify a fundamental issue of the standard MBRL framework ‚Äì what we call
objective mismatch. Objective mismatch arises when one objective is optimized in the hope that a
second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize
the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the
one-step ahead prediction, and the overall goal of improving performance on a downstream control
task. For example, this issue can emerge with the realization that dynamics models effective for
a speciÔ¨Åc task do not necessarily need to be globally accurate, and vice versa globally accurate
models might not be sufÔ¨Åciently accurate locally to obtain good control performance on a speciÔ¨Åc
task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood
of one-step ahead predictions is not always correlated with control performance. This observation
highlights a critical limitation in the MBRL framework which will require further research to be
fully understood and addressed. We propose an initial method to mitigate the mismatch issue by
re-weighting dynamics model training. Building on it, we conclude with a discussion about other
potential directions of research for addressing this issue.
1. Introduction
Model-based reinforcement learning (MBRL) is a popular approach for learning to control nonlinear
systems that cannot be expressed analytically (Bertsekas, 1995; Sutton and Barto, 2018; Deisenroth
and Rasmussen, 2011; Williams et al., 2017). MBRL techniques achieve the state of the art perfor-
mance for continuous-control problems with access to a limited number of trials (Chua et al., 2018;
Wang and Ba, 2019) and in controlling systems given only visual observations with no observations
of the original system‚Äôs state (Hafner et al., 2018; Zhang et al., 2019). MBRL approaches typically
learn a forward dynamics model that predicts how the dynamical system will evolve when a set of
control signals are applied. This model is classically Ô¨Åt with respect to the maximum likelihood of
a set of trajectories collected on the real system, and then used as part of a control algorithm to be
executed on the system (e.g., model-predictive control).
In this paper, we highlight a fundamental problem in the MBRL learning scheme: the objective
mismatch issue. The learning of the forward dynamics model is decoupled from the subsequent
¬© 2020 N. Lambert, B. Amos, O. Yadan & R. Calandra.
arXiv:2002.04523v3  [cs.LG]  19 Apr 2021

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
Dynamics ùëì"
Policy ùúã"(ùë•)
Environment
State Transitions
Reward
Trajectories
Training: Maximum Likelihood
Objective Mismatch
Control
Interacts
Responses
Figure 1: Objective mismatch in MBRL arises when a model is trained to maximize the likelihood
but then used for control to maximize a reward signal not considered during training.
controller through the optimization of two different objective functions ‚Äì prediction accuracy or loss
of the single- or multi-step look-ahead prediction for the dynamics model, and task performance
for the policy optimization. While the use of log-likelihood (LL) for system identiÔ¨Åcation is an
historically accepted objective, it results in optimizing an objective that does not necessarily correlate
to controller performance. The contributions of this paper are to: 1) identify and formalize the
problem of objective mismatch in MBRL; 2) examine the signs of and the effects of objective
mismatch on simulated control tasks; 3) propose an initial mechanism to mitigate objective mismatch;
4) discuss the impact of objective mismatch and outline future directions to address this issue.
2. Model-based Reinforcement Learning
We now outline the MBRL formulation used in the paper. At time t, we denote the state st ‚ààRds,
the actions at ‚ààRda, and the reward r(st, at). We say that the MBRL agent acts in an environment
governed by a state transition distribution p(st+1|st, at). We denote a parametric model fŒ∏ to
approximate this distribution with pŒ∏(st+1|st, at). MBRL follows the approach of an agent acting
in its environment, learning a model of said environment, and then leveraging the model to act.
While iterating over parametric control policies, the agent collects measurements of state, action,
next-state and forms a dataset D = {(sn, an, s‚Ä≤
n)}N
n=1. With the dynamics data D, the agent learns
the environment in the form of a neural network forward dynamics model, learning an approximate
dynamics fŒ∏. This dynamics model is leveraged by a controller that takes in the current state st and
returns an action sequence at:t+T maximizing the expected reward EœÄŒ∏(st)
Pt+T
i=t r(si, ai), where T
is the predictive horizon and œÄŒ∏(st) is the set of state transitions induced by the model pŒ∏.
In our paper, we primarily use as probabilistic neural networks designed to maximize the LL
of the predicted parametric distribution pŒ∏, denoted as P, or ensembles of probabilistic networks
denoted PE, and compare to deterministic networks minimizing the mean squared error (MSE),
denoted D or DE. Unless otherwise stated we use the models as in PETS (Chua et al., 2018) with
an expectation-based trajectory planner and a cross-entropy-method (CEM) optimizer.
3. Objective Mismatch and its Consequences
The Origin of Objective Mismatch: The Subtle Differences between MBRL and System Iden-
tiÔ¨Åcation
Many ideas and concepts in model-based RL are rooted in the Ô¨Åeld of optimal control
and system identiÔ¨Åcation (Sutton, 1991; Bertsekas, 1995; Zhou et al., 1996; Kirk, 2012; Bryson,
2018). In system identiÔ¨Åcation (SI), the main idea is to use a two-step process where we Ô¨Årst
2

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
generate (optimal) elicitation trajectories œÑ to Ô¨Åt a dynamics model (typically analytical), and
subsequently we apply this model to a speciÔ¨Åc task. This particular scheme has several assumptions:
Start
Goal
Start
Goal
Figure 2: Sketches of state-action spaces. (Left) In
system identiÔ¨Åcation, the elicitation trajec-
tories are designed off-line to cover the en-
tire state-action space. (Right) In MBRL
instead, the data collected during learning
is often concentrated in trajectories towards
the goal, with other parts of the state-action
space being largely unexplored (grey area).
1) the elicitation trajectories collected cover
the entire state-action space; 2) the presence
of virtually inÔ¨Ånite amount of data; 3) the
global and generalizable nature of the model
resulting from the SI process. With these as-
sumptions, the theme of system identiÔ¨Åcation
is effectively to collect a large amount of data
covering the whole state-space to create a suf-
Ô¨Åciently accurate, global model that we can
deploy on any desired task, and still obtain
good performance. If these assumptions are
true, using the closed-loop of MBRL should
further improve performance over traditional
open-loop SI (Hjalmarsson et al., 1996).
When adopting the idea of learning the
dynamics model used in optimal control for
MBRL, it is important to consider if these
assumptions still hold. The assumption of
virtually inÔ¨Ånite data is visibly in tension with
the explicit goal of MBRL which is to reduce
the number of interactions with the environment by being ‚Äúsmart‚Äù about the sampling of new
trajectories. In fact, in MBRL the ofÔ¨Çine data collection performed via elicitation trajectories is
largely replaced by on-policy sampling to explicitly reduce the need to collect large amount of
data (Chua et al., 2018). Moreover, in the MBRL setting the data will not usually cover the entire
state-action space, since they are generated by optimizing one task. In conjunction with the use of
non-parametric models, this results in learned models that are strongly biased towards capturing
the distribution of the locally accurate, task-speciÔ¨Åc data. Nonetheless, this is not an immediate
issue since the MBRL setting rarely tests for generalization capabilities of the learned dynamics.
In practice, we can now see how the assumptions and goals of system identiÔ¨Åcation are in contrast
with the ones of MBRL. Understanding these differences and the downstream effects on algorithmic
approach is crucial to design new families of MBRL algorithms.
Objective Mismatch
During the MBRL process of iteratively learning a controller, the reward
signal from the environment is diluted by the training of a forward dynamics model with a independent
metric, as shown in Fig. 1. In our experiments, we highlight that the minimization of some network
training cost does not hold a strong correlation to maximization of episode reward. As dynamic
environments becoming increasingly complex in dimensionality, the assumptions of collected data
distributions become weaker and over-Ô¨Åtting to different data poses an increased risk.
Formally, the problem of objective mismatch appears as two de-coupled optimization problems
repeated over many cycles of learning, shown in Eq. (1a,b), which could be at the cost of minimizing
the Ô¨Ånal reward. This loop becomes increasingly difÔ¨Åcult to analyze as the dataset used for model
training changes with each experimental trial ‚Äì a step that is needed to include new data from
previously unexplored states. In this paper we characterize the problems introduced by the interaction
3

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
of these two optimization problems, but, for simplicity, we do not consider the interactions added by
the changes in the dynamics-data distribution during the learning process. In addition, we discuss
potential solutions, but do not make claims about the best way to do so, which is left for future work.
Training: arg max
Œ∏
N
X
i=1
log pŒ∏(s‚Ä≤
i|si, ai),
Control: arg max
at:t+T
EœÄŒ∏(st)
t+T
X
i=t
r(si, ai)
(1a,b)
4. Identifying Objective Mismatch
We now experimentally study the issue of objective mismatch to answer the following: 1) Does the
distribution of models obtained from running a MBRL algorithm show a strong correlation between
LL and reward? 2) Are there signs of sub-optimality in the dynamics models training process that
could be limiting performance? 3) What model differences are reÔ¨Çected in reward but not in LL?
Experimental Setting
In our experiments, we use two popular RL benchmark tasks: the cartpole
(CP) and half cheetah (HC). For more details on these tasks, model parameters, and control properties
see Chua et al. (2018). We use a set of 3 different datasets to evaluate how assumptions in MBRL
affect performance. We start with high-reward, expert datasets (cartpole r > 179, half cheetah
r > 10000) to test if on-policy performance is linked to a minimal, optimal exploration. The two
other baselines are datasets collected on-policy with the PETS algorithm and datasets of sampled
tuples representative of the entire state space. The experiments validate over a) many re-trained
models and b) many random seeds, to account for multiple sources of stochasticity in MBRL.
Additional details and experiments can be found at: https://sites.google.com/view/mbrl-mismatch.
4.1. Exploration of Model Loss vs Episode Reward Space
The MBRL framework assumes a clear correlation between model accuracy and policy performance,
which we challenge even in simple domains. We aggregated Mcp = 1000 cartpole models and
Mhc = 2400 half cheetah models trained with PETS. The relationships between model accuracy and
reward on data representing the full state-space (grid or sampled) show no clear trend in Fig. 3c,f.
The distribution of rewards versus LL shown in Fig. 3a-c shows substantial variance and points
of disagreement overshadowing a visual correlation of increased reward and LL. This bi-model
distribution on the half cheetah expert dataset, shown in Fig. 3(d)subÔ¨Ågure, relates to a unrecoverable
failure mode in early half cheetah trials. The contrast between Fig. 3(e)subÔ¨Ågure and Fig. 3d,f shows
a considerable per-dataset variation in the state-action transitions. The grid and sampled datasets,
Fig. 3c,f, suffer from decreased likelihood because they do not overlap greatly with on-policy data
from PETS.
If the assumptions behind MBRL were fully valid, the plots should show a perfect correlation
between LL and reward. Instead these results conÔ¨Årm that there exists an objective mismatch which
manifests as a decreased correlation between validation loss and episode reward. Hence, there is no
guarantee that increasing the model accuracy (i.e., the LL) will also improve the control performance.
4.2. Model Loss vs Episode Reward During Training
This section explores how model training impacts performance at the per-epoch level. These
experiments shed light onto the impact of the strong model assumptions outlined in Sec. 3. As a
dynamics model is trained, there are two key inÔ¨Çection points - the Ô¨Årst is the training epoch where
4

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
0
20
40
60
80
100
120
140
160
180
Log Likelihood
Episode Reward
(a) CP Expert (œÅ = 0.59)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
Log Likelihood
(b) CP On-Policy (œÅ = 0.34)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
Log Likelihood
(c) CP Grid (œÅ = ‚àí0.06)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
‚àí1000
0
1000
2000
3000
4000
5000
6000
Log Likelihood
Episode Reward
(d) HC Expert (œÅ = 0.07)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
Log Likelihood
(e) HC On-Policy (œÅ = 0.46)
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
Log Likelihood
(f) HC Sampled (œÅ = 0.19)
Figure 3: The distribution of dynamics models (Mmodels = 1000, 2400 for cartpole, half cheetah)
from our experiments plotting in the LL-Reward space on three datasets, with correlation
coefÔ¨Åcients œÅ. Each reward point is the mean over 10 trials. There is a trend of high reward
to increased LL that breaks down as the datasets contain more of the state-space than only
expert trajectories.
Validation Error (‚ñ≥: DE, PE, ‚ô¶: D, P)
Episode Reward (X: DE, PE, ‚ãÜ: D, P)
0
100
200
300
400
500
-6
-4
-2
0
2
4
6
0
50
100
150
200
Training Epoch
Negative LL
Reward
(a) P, PE models.
0
100
200
300
400
500
2
5
0.001
2
5
0.01
2
5
0.1
2
51
0
50
100
150
200
Training Epoch
MSE
Reward
(b) D, DE models.
Figure 4: The reward when re-evaluating the controller at each dynamics model training epoch for
different dynamics models, M = 50 per model type. Even for the simple cartpole environ-
ment, D, DE fail to achieve full performance, while P, PE reach higher performance
but eventually over-Ô¨Åt to available data. The validation loss is still improving slowly at
500 epochs, not yet over-Ô¨Åtting.
episode reward is maximized, and the second is when error on the validation set is optimized. These
experiments highlight the disconnect between three practices in MBRL a) the assumption that the
on-policy dynamics data can express large portions of the state-space, b) the idea that simple neural
5

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
Grid Data (‚ñ≥)
Policy Data (‚Éù)
Expert Data (‚ô¶)
Episode Reward (X)
0
100
200
300
400
500
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
0
50
100
150
200
Training Epoch
Negative LL
Reward
(a) Trained: grid
Tested: expert, on-policy
0
100
200
300
400
500
‚àí6
‚àí4
‚àí2
0
2
4
6
8
10
0
50
100
150
200
Training Epoch
Negative LL
Reward
(b) Trained: on-policy
Tested: expert, grid
Figure 5: The effect of the dataset choice on model (P) training and accuracy in different regions
of the state-space, N = 50 per model type. (Left) when training on the complete dataset,
the model begins over-Ô¨Åtting to the on-policy data even before the performance drops in
the controller. (Right) A model trained only on policy data does not accurately model
the entire state-space. The validation loss is still improving slowly at 500 epochs in both
scenarios.
networks can satisfactorily capture complex dynamics, c) and the practice that model training is a
simple optimization problem disconnected from reward. Note that in the Ô¨Ågures of this section we
use Negative Log-Likelihood (NLL) instead of LL, to reduce visual clutter.
For the grid cartpole dataset, Fig. 4 shows that the reward is maximized at a drastically different
time than when validation loss is minimized for P, PE models. Fig. 5 highlights how the trained
models are able to represent other datasets than they are trained on (with additional validation errors).
Fig. 5b shows that on-policy data will not lead to a complete dynamics understanding because the grid
validation data rapidly diverges. When training on grid data, the fact that the on-policy data diverges
in Fig. 5a before the reward decreases is encouraging as objective mismatch may be preventable in
simple tasks. Similar experiments on half cheetah are omitted because models for this environment
are trained incrementally on aggregated data rather then fully on each dataset (Chua et al. (2018)).
4.3. Decoupling Model Loss from Controller Performance
We now study how differences in dynamics models ‚Äì even if they have similar LLs ‚Äì are reÔ¨Çected in
control policies to show that a accurate dynamics model does not guarantee performance.
Start
End
4.6
4.7
 4.8
 4.9
 5
 5.1
100
120
140
160
180
0
2
4
6
8
Itr.
LL
Reward
Figure 7: Convergence of the CMA-ES pop-
ulation‚Äôs best member.
Adversarial attack on model performance
We
performed an adversarial attack (Szegedy et al., 2013)
on a deep dynamics model so that it attains a high like-
lihood but low reward. SpeciÔ¨Åcally, we Ô¨Åne-tune the
deep dynamics model‚Äôs last layer with a zeroth-order
optimizer, CMA-ES, (the cumulative reward is non-
differentiable) to lower reward with a large penalty
if the validation likelihood drops. As a starting point
for this experiment we sampled a P dynamics model
from the last trial of a PETS run on cartpole. This
model achieves reward of 176 and has a LL of 4.827 on it‚Äôs on-policy validation dataset. Using
6

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
0
20
40
60
80
100
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
Planned Trajectories
Expert Trajectory
Time (Environment Step)
State Value
0
20
40
60
80
100
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
Time (Environment Step)
LL: 4.83
Reward: 176
LL: 4.85
Reward: 98
Figure 6: Example of planned trajectories along the expert trajectory for (left) a learned model and
(right) the adversarially generated model trained to lower the reward. The planned control
sequences are qualitatively similar except for the peak at t = 25. There, the adversarial
attack applies a small nudge to the dynamics model parameters that signiÔ¨Åcantly inÔ¨Çuences
the control outcome with minimal change in terms of LL.
10
100
1000
0.1
1.0
10.0
Re-Weighting Dataset Size
Sampling Bound (Epsilon)
10
100
1000
0
30
60
90
120
150
Reward
Standard Dataset Size
Figure 8: Mean reward of PETS trials (Ntrials = 100), with (left) and without (right) model re-
weighting, on a log-grid of dynamics model training sets with number of points S ‚àà
[10, 2500] and sampling optimal-distance bounds œµ ‚àà[.28, 15.66]. The re-weighting
improves performance for smaller dataset sizes, but suffers from increased variance in
larger set sizes. The performance of PETS declines when the dynamics model is trained on
points too near to the optimal trajectory because the model lacks robustness when running
online with the stochastic MPC.
CMA-ES, we reduced the on-policy reward of the model to 98, on 5 trials, while slightly improving
the LL; the CMA-ES convergence is shown in Fig. 7 and the difference between the two models is
visualized in Fig. 6. Fine tuning of all model parameters would be more likely to Ô¨Ånd sub-optimal
performing controllers because the output layer consists of about 1% of the total parameters. This
experiment shows that the model parameters that achieve a low model loss inhabit a broader space
than the subset that also achieves high reward.
5. Addressing Objective Mismatch During Model Training
Tweaking dynamics model training can partially mitigate the problem of objective mismatch. Taking
inspiration from imitation learning, we propose that the learning capacity of the model would be
most useful when accurately modeling the dynamics along trajectories that are relevant for the task at
7

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
hand, while maintaining knowledge of nearby transitions for robustness under a stochastic controller.
Intuitively, it is more important to model accurately the dynamics along the optimal trajectory, rather
than modeling part of the state-action space that might never be visited to solve the task. For this
reason, we now propose a model loss aimed at alleviating this issue.
Œµ
Figure 9: We propose to re-weight
the loss of the dynamics
model w.r.t. the distance œµ
from the optimal trajec-
tory.
Given a element of a state space (si, ai), we quantify the
distance of any two tuples, di,j. With this distance, we re-
weight the loss, l(y), of points further from the optimal policy
to be lower, so that points in the optimal trajectory get a weight
œâ(y) = 1, and points at the edge of the grid dataset used in
Sec. 4 get a weight œâ(y) = 0. Using the expert dataset dis-
cussed in Sec. 4 as a distance baseline, we generated 25e6 tu-
ples of (s, a, s‚Ä≤) by uniformly sampling across the state-action
space of cartpole. We sorted this data by taking the minimum
orthogonal distance, d‚àó, from each of the points to the 200 ele-
ments in the expert trajectory. To create different datasets that
range from near-optimal to near-global, we vary the distance
bound œµ, and number of training points, S. This simple form of
re-weighting the neural network loss , shown in Eq. (2a,b,c),
demonstrated an improvement in sample efÔ¨Åciency to learn
the cartpole task, as seen in Fig. 8. Unfortunately, this ap-
proach is impractical when the optimal trajectory is not known
in advance. However, future work could develop an iterative
method to jointly estimate and re-weight samples in an online
training method to address objective mismatch.
Weighting œâ(y) = ce‚àíd‚àó(y)
Standard l(ÀÜy, y)
Re-weight l(ÀÜy, y) ¬∑ œâ(y)
(2a,b,c)
6. Discussion, Related Work, and Future Work
Objective mismatch impacts the performance of MBRL ‚Äì our experiments have gone deeper into this
fragility. Beyond the re-weighting of the LL presented in Sec. 5, here we summarize and discuss
other relevant works in the community.
Learning the dynamics model to optimize the task performance
Most relevant are research
directions on controllers that directly connect the reward signal back to the controller. In theory,
this exactly solves the model mismatch problem, but in practice the current approaches have proven
difÔ¨Åcult to scale to complex systems. One way to do this is by designing systems that are fully
differentiable and backpropagating the task reward through the dynamics. This has been investigated
with differentiable MPC (Amos et al., 2018) and Path Integral control (Okada et al., 2017), Universal
Planning Networks (Srinivas et al., 2018) propose a differentiable planner that unrolls gradient
descent steps over the action space of a planning network. Bansal et al. (2017) use a zero-order
optimizer to maximize the controller‚Äôs performance without having to compute gradients explicitly.
Add heuristics to the dynamics model structure or training process to make control easier
If
it is infeasible or intractable to shape the dynamics of a controller, an alternative is to add heuristics
to the training process of the dynamics model. These heuristics can manifest in the form of learning a
8

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
latent space that is locally linear, e.g., in Embed to Control and related methods (Watter et al., 2015),
by enforcing that the model makes long-horizon predictions (Ke et al., 2019), ignoring uncontrollable
parts of the state space (Ghosh et al., 2018), detecting and correcting when a predictive model steps
off the manifold of reasonable states (Talvitie, 2017), adding reward signal prediction on top of
the latent space Gelada et al. (2019), or adding noise when training transitions Mankowitz et al.
(2019). Farahmand et al. (2017); Farahmand (2018) also attempts to re-frame the transitions to
incorporate a notion of the downstream decision or reward. Finally, Singh et al. (2019) proposes
stabilizability constraints to regularize the model and improve the control performance. None of
these paper formalize or explore the underlying mismatch issue in detail.
Continuing Experiments
Our experiments represent an initial exploration into the challenges of
objective mismatch in MBRL. Sec. 4.2 is limited to cartpole due to computational challenges of
training with large dynamics datasets and Sec. 4.3 could be strengthened by deÔ¨Åning quantitative
comparisons in controller performance. Additionally, these effects should be quantiÔ¨Åed in other
MBRL algorithms such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).
7. Conclusion
This paper identiÔ¨Åes, formalizes and analyzes the issue of objective mismatch in MBRL. This
fundamental disconnect between the likelihood of the dynamics model, and the overall task reward
emerges from incorrect assumptions at the origins of MBRL. Experimental results highlight the
negative effects that objective mismatch has on the performance of a current state-of-the-art MBRL
algorithm. In providing a Ô¨Årst insight on the issue of objective mismatch in MBRL, we hope future
work will deeply examine this issue to overcome it with a new generation of MBRL algorithms.
Acknowledgments
We thank Rowan McAllister and Kurtland Chua for useful discussions.
9

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
References
Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. Differentiable MPC for
end-to-end planning and control. In Neural Information Processing Systems, pages 8289‚Äì8300,
2018.
S. Bansal, Roberto Calandra, T. Xiao, S. Levine, and C. J. Tomlin. Goal-driven dynamics learning via
Bayesian optimization. In IEEE Conference on Decision and Control (CDC), pages 5168‚Äì5173,
2017. doi: 10.1109/CDC.2017.8264425.
Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiÔ¨Åc
Belmont, MA, 1995.
Arthur Earl Bryson. Applied optimal control: optimization, estimation and control. Routledge, 2018.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Neural Information
Processing Systems, pages 4754‚Äì4765, 2018.
Marc P. Deisenroth and Carl E. Rasmussen. PILCO: A Model-Based and Data-EfÔ¨Åcient Approach to
Policy Search. In International Conference on Machine Learning (ICML), pages 465‚Äì472, 2011.
Amir-massoud Farahmand. Iterative value-aware model learning. In Advances in Neural Information
Processing Systems, pages 9072‚Äì9083, 2018.
Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for
model-based reinforcement learning. In ArtiÔ¨Åcial Intelligence and Statistics, pages 1486‚Äì1494,
2017.
Carles Gelada, Saurabh Kumar, Jacob Buckman, OÔ¨År Nachum, and Marc G Bellemare. Deep-
mdp: Learning continuous latent space models for representation learning.
arXiv preprint
arXiv:1906.02736, 2019.
Dibya Ghosh, Abhishek Gupta, and Sergey Levine. Learning actionable representations with
goal-conditioned policies. arXiv preprint arXiv:1811.07819, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
H√•kan Hjalmarsson, Michel Gevers, and Franky De Bruyne. For model-based control design,
closed-loop identiÔ¨Åcation gives better performance. Automatica, 32(12):1659‚Äì1673, 1996.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. arXiv preprint arXiv:1906.08253, 2019.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In International Conference on Autonomous Agents and Multiagent
Systems, pages 1181‚Äì1189, 2015.
10

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh,
and Dhruv Batra. Learning dynamics model in reinforcement learning by incorporating the long
term future. arXiv preprint arXiv:1903.01599, 2019.
Donald E Kirk. Optimal control theory: an introduction. Courier Corporation, 2012.
Daniel J Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy
Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control
with model misspeciÔ¨Åcation. arXiv preprint arXiv:1906.07516, 2019.
Masashi Okada, Luca Rigazio, and Takenobu Aoshima. Path integral networks: End-to-end differen-
tiable optimal control. arXiv preprint arXiv:1706.09597, 2017.
Sumeet Singh, Spencer M. Richards, Vikas Sindhwani, Jean-Jacques E. Slotine, and Marco Pavone.
Learning stabilizable nonlinear dynamics with contraction-based regularization, 2019.
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning
networks. arXiv preprint arXiv:1804.00645, 2018.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160‚Äì163, 1991. doi: 10.1145/122344.122377.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Erik Talvitie. Self-correcting models for model-based reinforcement learning. In Thirty-First AAAI
Conference on ArtiÔ¨Åcial Intelligence, 2017.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Neural information
processing systems, pages 2746‚Äì2754, 2015.
Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M Rehg, Byron Boots, and
Evangelos A Theodorou. Information theoretic MPC for model-based reinforcement learning. In
International Conference on Robotics and Automation (ICRA), pages 1714‚Äì1721, 2017.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey
Levine. Solar: Deep structured latent representations for model-based reinforcement learning. In
International Conference on Machine Learning (ICML), 2019.
Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and optimal control, volume 40.
Prentice hall New Jersey, 1996.
11

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
Appendix A. Effect of Dataset Distribution when Learning
Number of random transitions:
200,
2000,
4000,
20000
0
5
10
15
20
0
50
100
150
Trial Num
Cumulative Reward
Figure 10: Cartpole (Mujoco simulations) learn-
ing efÔ¨Åciency is suppressed when ad-
ditional data not relevant to the task is
added to the dynamics model training
set. This effect is related to the issue
of objective mismatch because model
training needs to account for potential
off-task data.
Learning speed can be slowed by many factors
in dataset distribution, such as adding additional
irrelevant transitions. When extra transitions
from a speciÔ¨Åc area of the state space are in-
cluded in the training set, the dynamics model
will spend increased expression on these transi-
tions. LL of the model will be biased down as
it learns this data, but it will reduce the learn-
ing speed as new, more relevant transitions are
added to the training set.
Running cartpole random data collection
with a short horizon of 10 steps (while forc-
ing initial babbling state to always be 0), for 20,
200,400 and 2000 babbling roll-outs (that sums
up to 200, 2000, 4000 and 20000 transitions in
the dataset Ô¨Ånally shows some regression in the
learning speed for runs with more useless data in
the motor babbling. This data highlights the im-
portance of careful exploration vs exploitation
trade-offs, or changing how models are trained
to be selective with data.
Appendix B. Task Generalization in Simple Environments
Validation Error
Episode Reward
Figure 11: Learning curve for the standard Cart-
pole task used in this paper (Xgoal =
0). The median reward from 10 trials
is plotted with the mean NLL of the dy-
namics models at each iteration. The
reward reaches maximum (180) well
before the NLL is at it‚Äôs minimum.
In this section, we compare the performance of
a model trained on data for the standard cartpole
task (x position goal at 0) to policies attempting
to move the cart to different positions in the x-
axis. Fig. 11 is a learning curve of PETS with
a PE model using the CEM optimizer. Even
though performance levels out, the NLL con-
tinues to decrease as the dynamics models ac-
crue more data. With more complicated systems,
such as halfcheetah, the reward of different tasks
verses global likelihood of the model would
likely be more interesting (especially with in-
cremental model training) - we will investigate
this in future work. Below, we show that the
dynamics model generalizes well to tasks close
to zero (both positive in (Fig. 12(b)subÔ¨Ågure)
and negative positions (Fig. 12(a)subÔ¨Ågure), but
performance drops off in areas the training set
does not cover as well.
12

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
(a) Xgoal < 0
(b) Xgoal > 0
|Xgoal| = .1
|Xgoal| = .2
|Xgoal| = .5
|Xgoal| = 1.0
Figure 12: MPC control with different reward functions with the same dynamics models loaded from
trials shown in Fig. 11. The cartpole solves tasks further from 0 proportional to the state
space coverage (Goal further from zero causes reduced performance). The distribution
of x data encountered is shown in Fig. 13.
‚àí2
‚àí1
0
1
2
0
2
4
6
8
10
12
X Position
Occurences
(a) x distribution after trial 1.
‚àí2
‚àí1
0
1
2
0
20
40
60
80
100
X Position
Occurences
(b) x distribution after trial 5.
‚àí2
‚àí1
0
1
2
0
50
100
150
200
250
300
X Position
Occurences
(c) x distribution after trial 20.
Figure 13: Distribution of x position encountered during the trials shown in Fig. 11. The distribution
converges to a high concentration around 0, making it difÔ¨Åcult for MPC to control outside
of the area close to 0.
Below the learning curves in Fig. 13, we include snapshots of the distributions of training data
used for these models at different trials, showing how coverage relates to reward in cartpole. It is
worth investigating how many points can be removed from the training set while maintaining peak
performance on each task.
Appendix C. Validating models with trajectories rather than random tuples
The goal of the dynamics model for planning is to be able to predict stable long term roll-outs
conditioned on different actions. This is because in sampling based control, the MPC chooses the
best planned trajectory, not the best collection of random one-step predictions (akin to random, small
batches). Different results could be found in short, simulated approaches such as Janner et al. (2019),
where predictive accuracy is validated under policy shift for one-step predictions. We propose that
evaluating the test set when training a dynamics model could be more reliable (in terms of relation
between loss and reward under the induced planning-based controller) if the model is validated on
batches consisting entirely of the same trajectory, rather then a random shufÔ¨Çe of points. When
randomly shufÔ¨Çing points, the test loss can be easily dominated by an outlier in each batch.
13

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
0
50
100
150
200
Log Likelihood
Episode Reward
(a) CP LL from trajectory based loss (œÅ = .36).
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
Log Likelihood
(b) CP LL for standard loss formulation (œÅ =
.34)
Figure 14: There is a slight increase in the correlation between LL and reward when training on
cartpole trajectories rather than random samples. This could be one small step in the
right direction of solving objective mismatch.
To test this, we re-ran experiments from Sec. 4.1 with the LL being calculated on trajectories
rather than random batches. Fig. 14 shows an improved trend (less variance in the relationship in
the form of a tighter grouping, and increased œÅ) for cartpole likelihood versus reward in the new
data (Fig. 14(a)subÔ¨Ågure) over the original experiments (Fig. 14(b)subÔ¨Ågure). For halfcheetah, the
trajectories are substantially longer (1000 timesteps) than the batch size (64), so we verify that
increasing the batch size of validation is not the only effect in improving the trend of likelihood
versus reward. Fig. 15(b)subÔ¨Ågure shows a tighter relationship between likelihood and reward than
the exploration using default PETS values in Fig. 15(c)subÔ¨Ågure. Finally, by validating on trajectories
versus large batches, the trend of likelihood versus reward is again improved in Fig. 15(a)subÔ¨Ågure.
Appendix D. Using simple reward as re-weight
r
Figure 16: re-weight
the
loss of the dy-
namics
model
w.r.t. the reward.
An alternative to re-weighting w.r.t. the optimal trajectory could be re-
weighting w.r.t. the reward of each state. The compelling advantage
of this would be the easy availability of the reward without access
to additional information (e.g., the optimal trajectory). However, the
simple reward does not topologically has the desired shape compared
to the optimal trajectory. In fact, for many rewards (e.g., distance
to the target) the isocurves of reward are orthogonal to the optimal
trajectory. This means that the resulting re-weighting would concen-
trate the dynamics to model accurately the part of the state-action
space closer to the target, but it would ignore the dynamics that lead
us to the reward in the Ô¨Årst space (e.g., along the optimal trajectory).
Intuitively, this is undesirable, as it might decrease performance in
the initial stages of the trajectory. More research will be necessary
to fully study alternatives forms of re-weighting.
14

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
(a) HC traj. loss (œÅ = 0.63)
(b) HC large batch (œÅ = 0.54)
(c) HC on-policy (œÅ = 0.49)
Figure 15: Validation of model LL versus reward with different types of validation of the half cheetah
models. (left) Is a new method for training, where each batch of the validation set is a
complete subsection of a trajectory in the aggregated dataset. (center) We compare the
trajectory loss to the regularization that would be provided when just validating with
larger batches, which would reduce variance from outliers. (right) Copied from Ô¨Ågure
Fig. 3e where validation is done on small batches randomly sampled.
Appendix E. Ways model
mismatch can harm the performance of a controller
Model mismatch between Ô¨Åtting the likelihood and optimizing the
task‚Äôs reward manifests itself in many ways. Here we highlight two of them and in Sec. 6 we discuss
how related work connects in with these issues.
Long-horizon roll-outs of the model may be unstable and inaccurate.
Time-series or dynamics
models that are unrolled for long periods of time easily diverge from the true prediction and can
easily step into predicting future states that are not on the manifold of reasonable trajectories. Taking
these faulty dynamics models and using them as a smaller part of a controller that optimizes some
cost function under a poor approximation to the dynamics. Issues can especially manifest if, e.g., the
approximate dynamics do not properly capture stationarity properties necessary for the optimality of
the true physical system being modeled.
Non-convex and non-smooth models may make the control optimization problem challenging
The approximate dynamics might have bad properties that make the control optimization problem
much more difÔ¨Åcult than on the true system, even when the true optimal action sequence is optimal
under the approximate model. This is especially true when using neural network as they introduce
non-linearities and non-smoothness that make many classical control approaches difÔ¨Åcult.
Sampling models with similar LLs, different rewards
To better understand the objective mis-
match, we also compared how a difference of model loss can impact a control policy. We sampled
models with similar LL‚Äôs and extremely different rewards from Fig. 3d-e and visualized the chosen
optimal action sequences along an expert trajectory. The control policies and dynamics models
appear to be converging to different regions of state spaces. In these visualizations, there is not a
emphatic reason why the models achieved different reward, so further study is needed to quantify the
15

OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING
impact of model differences. The interpretability of the difference between models and controllers
will be important to solving the objective-mismatch issue.
Appendix F. Hyper-paramters and Simulation Environment
Parameter
Cartpole
Half-Cheetah
Experiment Parameters
Trial Time-steps
200
1000
Random Sampling Parameters
Horizon
25
30
Trajectories
2000
2500
CEM Parameters
Horizon
25
30
Trajectories
400
500
Elites
40
50
CEM Iterations
5
5
Network Parameters
Width
500
200
Depth
2
3
E
5
5
Training Parameters
Training Type
Full
Incremental
Full / Initial Epochs
100
20
Incremental Epochs
- -
10
Optimizer
Adam
Adam
Batch Size
16
64
Learning Rate
1E-4
1E-4
Test Train Split
0.9
0.9
Table 1: PETS Hyper-parameters
Type
Number of points
Cartpole Datasets
Grid
16807
On-policy
3780
Expert
2400
Half Cheetah Datasets
Sampled
200000
On-policy
90900
Expert
3000
Table 2: Experimental Dataset Sizes
Tab. 1 includes the PETS parameters used for
our cartpole and half-cheetah experiments. Both
of these experiments were run with Mujoco ver-
sion 1.50.1 (which we found to be signiÔ¨Åcant
in replicating various papers across the Ô¨Åeld of
Deep RL).
Experimental datasets
We include in Tab. 2
the sizes of each dataset used in the experimental
section of this paper. The expert datasets em-
ployed are generated by a combination of a) run-
ning PETS with a true, environment-based dy-
namics model for prediction or soft actor-critic
at convergence. The on-policy data is taken from
the end of a trial that solved the given task (rather
then sampling from all on-policy data). The grid
dataset for cartpole is generated by slicing the
state and action spaces evenly. Due to the high
dimensionality of half cheetah, uniform slicing
does not work, so the dataset is generated by
uniformly sampling within the state and action
spaces.
Appendix G. Additional Related Works
Add inductive biases to the controller
Prior
knowledge can be added to the controller in the
form of hyper-parameters such as the horizon
length, or by penalizing unreasonable control
sequences by using, e.g., a slew rate penalty.
These heuristics can signiÔ¨Åcantly improve the
performance if done correctly but can be difÔ¨Åcult
to tune. Jiang et al. (2015) use complexity theory
to justify using a short planning horizon with
an approximate model to reduce the the class of
induced policies.
16

