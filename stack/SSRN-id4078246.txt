To Model the Mind: 
Speculative Engineering as Philosophy  
 
A Working Paper 
William Benzon • April 7, 2022 
 
 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
1 
To Model the Mind: 
Speculative Engineering as Philosophy 
 
 
William Benzon 
 
 
 
Abstract: Are brains computers? Some say yes, some say no. Does it 
matter? Ideas about computing have certainly proven fruitful in 
understanding how brains give rise to minds. That’s what this paper is 
about. The central section is a review of Grace Lindsey’s wonderful book 
Models of the Mind: How Physics, Engineering, and Mathematics Have Shaped 
Our Understanding of the Brain (2021). I precede it with a bit of philosophy 
and follow it with brief notices about five books, each proposing 
computationally inspired models of the mind. 
 
 
CONTENTS 
 
Introduction: Brains, machines, and computation ................................................................... 2 
Speculative Engineering as Philosophy ..................................................................................... 4 
To Understand the Mind We Must Build One, A Review of Models of the Mind – Bye Bye 
René, Hello Giambattista ...................................................................................................... 9 
Five Good Books .......................................................................................................................... 15 
 
 
About the cover image: I created the basic image (∑ 128.67), the black lines, on September 16, 1984 
using MacPaint running on a ‘classic’ 128K Macintosh computer. My friend David Porush offered 
advice as I was working. I embellished and fleshed out the image in Adobe Photoshop. The 
result: ∑ 128.67 TrickedOut.1. 
 
1301 Washington Street, Apartment 311 
Hoboken, New Jersey 07030 
bbenzon@mindspring.com 
 
 
 
This work is licensed under a Creative Commons Attribution-Share Alike 3.0 Unported License. 
 
 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
2 
Introduction: Brains, machines, and computation 
 
I remember when electronic digital computers were sometimes called electronic brains. 
The following graph from Google Ngram shows the rise and fall of the terms “electronic 
brain” and “electric brains.” 
 
 
 
Why the quick rise and fall? I’d guess that’s when these remarkable machines first 
gained public attention. It wasn’t clear what kind of beast they were. How do we refer to 
them? For a while, we tried out the idea that they were a kind of brain. After all, they 
did the kinds of things that brains did. They tabulated, sorted, and calculated.  
 
But they also inspired. During the interval of that peak the study of artificial intelligence 
was inaugurated at a conference at Dartmouth in 1956. Machine translation, the use of a 
computer to translate text from one language to another arose in the 1950s and then 
collapsed, alas, in the mid-1960s for lack of practical results. Noam Chomsky conceived 
of grammar in computational terms. Warren McCulloch and Walter Pitts conceived of 
neurons as tiny logic engines in the early 1940s and computational ideas began taking 
hold in neuroscience and philosophy. 
 
Are brains computers? Some say yes, some say no. Does it matter? For ideas about 
computing have certainly proven fruitful in understanding how brains give rise to 
minds. That’s what this paper is about. The central section is a review of Grace Lindsey’s 
wonderful book Models of the Mind: How Physics, Engineering, and Mathematics Have 
Shaped Our Understanding of the Brain (2021). I precede it with a bit of philosophy and 
follow it with brief notices about five books, each proposing computationally inspired 
models of the mind. 
 
* * * * * 
 
Speculative Engineering as Philosophy: Engineering is about how things are designed 
and constructed. I am interested in how the brain works, how it constructs a mind. 
When we theorize about that, thinking about models, experiments, or simulations, we 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
3 
are speculating about the engineering principles on which the brain operates. I argue 
that that is a form of philosophy, in the broadest sense of the term, though not 
necessarily as philosophy exists as an academic discipline. 
 
 
To Understand the Mind We Must Build One, A Review of Models of the Mind – Bye 
Bye René, Hello Giambattista: Descartes believed that truth is verified through 
observation. Vico had a different view, believing that “What is true is precisely what is 
made.” Grace Lindsey ‘s Models of the Mind is Viconian in spirit. Its subtitle tells the 
story: How Physics, Engineering, and Mathematics Have Shaped Our Understanding of the 
Brain. Lindsey traces the history of the of a wide variety of models and techniques, often 
back into the 19th and even 18th centuries, in a simple and direct way. I turn my review 
on a few cases: 1) the 1943 McCulloch and Pitts model of neurons as logical operators, 2) 
Frank Rosenblatt’s Perceptron from the late 1950s, and 3) Jerome Lettvin’s 1959 work on 
the frog’s visual system, which Nicholas Humphrey parlayed in a 1970 article on the 
monkey’s visual system. That last brings in an evolutionary angle. The whole thing is 
wrapped up by Joyce’s Finnegans Wake, and its Latin translation. 
 
 
 
Five Good Books: Short notices for five books, each about the mind and/or brain, each in 
a different style: 1) John von Neumann (1958), The Computer and the Brain, 2) Herbert A. 
Simon (1981), The Sciences of the Artificial, 3) William Powers (1973), Behavior: The Control 
of Perception, 4) David G. Hays (1981), Cognitive Structures, and 5) Valentine Braitenberg 
(1999), Vehicles: Experiments in Synthetic Psychology. 
 
 
 
 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
4 
Speculative Engineering as Philosophy 
 
Reality is not perceived, it is enacted – in a universe of great, perhaps unbounded, 
complexity.1 
 
This started, well, I don’t know, maybe way back when I was six or seven and thought 
the world was actually a movie projected on a giant screen for the enjoyment of the Baby 
Jesus, or perhaps when I was a bit older and wondered what there was before the 
universe came into existence, maybe still order when I thought, as a poem about poetry, 
“Kubla Khan” held the secret to literary criticism, but really, forget all that. It’s there to 
be sure, but has no direct bearing. This particular project started with a book, Grace 
Lindsay’s Models of the Mind, which I reviewed for 3 Quarks Daily. That review 
constitutes the next section of this working paper. The purpose of this section is to 
provide a context for that review.  
Philosophical beginnings 
I saw Lindsay’s book, and to some extent reviewed it, as a work of philosophy, though 
not philosophy as it exists in philosophy departments. I’m using the word in a different 
sense, one that I did in fact pick up from a philosopher, Peter Godfrey-Smith.2 In this 
view philosophy is a way of making sense of the world in the broadest possible 
conspectus. That is what philosophy was in the ancient world, but as we developed and 
accumulated knowledge, philosophers became specialists of various kinds, some 
became social and behavioral scientists, others became natural scientists, while still 
others practiced a humanities discipline. Philosophy itself became a humanities 
discipline, and, as such, became narrowly focused. 
But we still need to be able to make sense of the world, all of it, in some way or 
another. And so in the last several decades we have seen intellectual specialists of one 
sort or another write books for a general audience – Richard Dawkins, E.O. Wilson, 
Stephen Mithen, Jared Diamond, Stephen Hawking, and Murray Gell-Mann come to 
mind, but there are many others (see my post on John Brockman’s Third Culture3). 
While these books may be directed at the general audience, I suspect that they are 
written to serve their authors’ need to see how things fit together. That is to say, they are 
 
1 That statement is taken from William Benzon and David G. Hays, A Note on Why Natural 
Selection Leads to Complexity, Journal of Social and Biological Structures 13: 33-40, 1990, 
https://www.academia.edu/8488872/A_Note_on_Why_Natural_Selection_Leads_to_Complexity. 
2 See my post “What is Philosophy?” New Savanna, May 17, 2013, https://new-
savanna.blogspot.com/2013/05/what-is-philosophy.html.  
3 “Brockman’s Third Culture and the emergence of a new philosophical regime,” New Savanna, 
May 4, 2021, https://new-savanna.blogspot.com/2021/05/brockmans-third-culture-and-
emergence.html. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
5 
written out of philosophical hunger, if you will. As such, they are works of philosophy 
in this extended sense.4 
It is in that sense that Models of the Mind, is a work of philosophy. I might even 
hazard the assertion that it betokens a new philosophy of mind, but that might confuse it 
with the philosophy of mind that exists in philosophy departments. If I did that I’m 
afraid I’d be asking the little word “new” to do an awful lot of work. Maybe Lindsay 
took a course in the philosophy of mind at some point, maybe she even reads around in 
it, but this book certainly didn’t come out of the questions raised in that discipline.  
Where does this book come from? Look at the subtitle, How Physics, Engineering, and 
Mathematics Have Shaped Our Understanding of the Brain. That’s where it comes from. That 
is to say, it doesn’t come from any one, two, or three, or even five or eight academic 
disciplines. It comes from many and none. I see this book as part of a larger intellectual 
development, one not well-defined (which is probably a good thing), that will replace 
the traditional philosophy of mind, and a few other disciplines as well, with a more 
adequate approach to understanding the mind and the brain.  
The most fruitful conversations about mind and brain have been those between 
students of neural wetware, on the one hand, and software and hardware (digital and 
analog) on the other. In particular, it seems to me that those conversations have been far 
more consequential that philosophical discussions of whether or not computer can think 
or the brain is a computer. It is time to liberate those conversations from constraints 
imposed by our Cartesian legacy. That, in effect, is what Lindsay proposes. And that is 
how I framed my review. 
Speculative Engineering and the problem of design 
I coined the term “speculative engineering” in the preface to my book on music, 
Beethoven’s Anvil: Music in Mind and Culture. Here is what I said (p. xiii): 
Engineering is about design and construction: How does the nervous system 
design and construct music? It is speculative because it must be. The purpose of 
speculation is to clarify thought. If the speculation itself is clear and well-
founded, it will achieve its end even when it is wrong, and many of my 
speculations must surely be wrong. If I then ask you to consider them, not 
knowing how to separate the prescient speculations from the mistaken ones, it 
is because I am conﬁdent that we have the means to sort these malers out 
empirically. My aim is to produce ideas interesting, signiﬁcant, and clear 
enough to justify the hard work of investigation, both through empirical 
studies and through computer simulation. 
That book is speculative a way that Lindsay’s is not. I aimed for an account of how 
music works, from the nervous system, in performance, to the social group, from human 
origins to the present.  
Lindsay makes no pretense of presenting a theory about the mind or brain. Rather 
she offers a review of models, many models, that have been and are being used in 
 
4 I have used the label “philosophy new” to tag relevant posts at New Savanna. This is a link to 
those posts, https://new-savanna.blogspot.com/search/label/philosophy%20new. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
6 
studying the brain. But as her subtitle indicates, How Physics, Engineering, and 
Mathematics Have Shaped Our Understanding of the Brain, she understands that an 
engineering perspective is important.  
As I said, engineering, unlike physics, but perhaps not so unlike mathematics, is 
about design and construction. If we want to understand how the mind works, we must 
understand it from an engineering perspective. We want to see how things function, 
what the parts are and how they fit together to achieve a particular end.  Consider this 
passage where Newell and Simon talk about computer science: 
Computer science is an empirical discipline. We would have called it an 
experimental science, but like astronomy, economics, and geology, some of its 
unique forms of observation and experience do not ﬁt a narrow stereotype of 
the experimental method. None the less, they are experiments. Each new 
machine that is built is an experiment. Actually constructing the machine poses 
a question to nature; and we listen for the answer by observing the machine in 
operation and analyzing it by all analytical and measurement means available. 
Each new program that is built is an experiment. It poses a question to nature, 
and its behavior oﬀers clues to an answer. Neither machines nor programs are 
black boxes; they are artifacts that have been designed, both hardware and 
software, and we can open them up and look inside. We can relate their 
structure to their behavior and draw many lessons from a single experiment.5 
“They are artifacts that have been designed,” that’s a crucial statement. The human brain 
has been designed as well, but not by engineers. Rather it was ‘designed’ and 
‘constructed’ in a process of biological evolution taking place over hundreds of millions 
of years of years. But we must think like engineers if we are to understand how it works. 
In my review of Models of the Mind I pick out two models that are particularly 
important in understanding how the mind and brain have been engineered. One is the 
1943 model that McCulloch and Pitts proposed for the function of neurons: think of 
them as logic elements in an electronic circuit. The other is the Perceptron that Frank 
Rosenblatt constructed in the late 1950s. 
As you may know, McCulloch and Pitts proposed that neurons were logical units in 
brain-based electrochemical circuits (see p. 12). Their proposal was all but ignored by 
neuroscientists, found some favor among philosophers – Daniel Dennett was quite 
struck by it6 – but researchers in the emerging study of artificial intelligence loved it, for 
it squared with their preconceptions about the nature of higher mental functioning. 
Those researchers thought of the mind as processing symbols. This resulted in a great 
deal of research, at least one Nobel Prize – yes, Herbert Simon’s 1978 prize was awarded 
in economics but, really, everyone knows he got it for his work in psychology and AI – a 
lot of hope for useful systems, but not much practical success. The research enterprise, 
 
5 Allen Newell and Herbert A. Simon, Computer Science as Empirical Inquiry, Communications of 
the ACM, 19(3), p. 114. 
6 See his informal remarks, “The Normal Well-Tempered Mind,” Edge, May, 2013, where he also 
speculates about memes coming to tame neurons that have become “a little bit feral,” 
https://www.edge.org/conversation/daniel_c_dennett-the-normal-well-tempered-mind. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
7 
now known as GOFAI (good old-fashioned artificial intelligence) collapsed in the mid-
1980s.  
What happened? The systems were brittle, failing catastrophically without warning, 
and required many hours of meticulously crafted knowledge representation (KR), as it 
came to be known, knowledge generally grounded in some version of logic – many 
versions were tried. Unlike human or, for that matter, animal minds, GOFAI systems 
were explicitly designed by humans. 
Rosenblatt’s legacy was different (see p. 12). Perceptrons were learning machines. 
Their initial promise was high, but they failed to deliver. However, Rosenblatt’s ideas 
were revived and supplemented in the 1980s, as GOFAI was going down, under the 
guise of connectionism and eventually gave rise to the very powerful systems we see 
today, which are based on artificial neural nets (ANN). Researchers design a learning 
architecture – there must be at least as many such architectures as there were schemes 
for KR during the GOFAI era, programmers implement it, and it computes over some 
database, of text, images, speech, what have you, and ‘learns’ the structure of objects in 
the database. Just what it learns, and how, that is somewhat obscure, but it works, after a 
fashion, but generally much better than the GOFAI systems. 
The important point is that these systems teach themselves. AI researcher Yann 
LeCun has made some remarks that seem relevant to me. This is from a podcast quoted 
by Kenneth Church and Mark Liberman in a recent article, The Future of Computational 
Linguistics: On Beyond Alchemy7: 
All of AI relies on representations. The question is where do those 
representations come from? So, uh, the classical way to build a palern 
recognition system was . . . to build what’s called a feature extractor . . . a whole 
lot of papers on what features you should extract if you want to recognize, uh, 
wrilen digits and other features you should extract if you want to recognize 
like a chair from the table or something or detect... 
If you can train the entire thing end to end—that means the system learns its 
own features. You don’t have to engineer the features anymore, you know, they 
just emerge from the learning process. So that, that, that’s what was really 
appealing to me. 
That second paragraph is the important one. The system that performs the task at hand, 
the performing system – whether it is classifying images, translating from one language 
to another, playing chess, whatever – is not designed by humans. Rather it is designed by 
an architecture and that architecture is in turn designed by humans.  
None of those architectures so far look much like the human brain. That is one 
thing, and very important. Just as important, however, is the fact that some of them 
produce very powerful systems, some of practical value. They do so because the design 
of the performing system has be displaced from human designers and onto a machine.  
I take it then, that the problem of design is central to speculative engineering. And it 
is central, not just as an abstract issue, but as an inquiry into how the design process is 
 
7  Front. Artif. Intell., 19 April 2021 | https://doi.org/10.3389/frai.2021.625341. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
8 
implemented in a physical device, whether organic or artificial. How, specifically, does a 
system fit itself to, learn from, the environment in which it must function? 
A timely book 
Lindsay didn’t foreground the problem of design in Models of the Mind, but it is there, 
along with many other issues, or problematics as many humanists like to say. She lays 
them out for you, one after the other, chapter after chapter. I’ve been reading and 
thinking about mind and brain for half a century, but I’ve never seen a book quite like 
this.  
It is a timely book, a book we need. It feels foundational. Certainly, students of 
neuroscience need to encounter it early in their education. But so should students of 
psychology, of artificial intelligence, and of the mind more generally, even literary 
critics. That's where I started half a century ago, with questions about the structure of a 
poem “Kubla Khan,” a structure that somehow smelled of computation.8 I still haven’t 
figured out that structure. Possibly I never will. But I am content to leave that task to 
others, confident that that puzzle is more likely to be resolved in the conceptual universe 
implied by Models of the Mind than in the universe available to me back in the Jurassic 
Era. I call that progress. 
 
 
 
 
8 I tell that story in Touchstones • Strange Encounters • Strange Poems • the beginning of an 
intellectual life, November 2015, 
https://www.academia.edu/9814276/Touchstones_Strange_Encounters_Strange_Poems_the_begi
nning_of_an_intellectual_life.  That is revised and updated from an article originally published as 
Touchstones, Paunch 42-43:  4-16, December 1975. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
9 
To Understand the Mind We Must Build One, A 
Review of Models of the Mind – Bye Bye René, Hello 
Giambattista 
 
 
Grace Lindsay. Models of the Mind: How Physics, Engineering, and 
Mathematics Have Shaped Our Understanding of the Brain, Bloomsbury 
Publishing, 2021. 
 
“riverrun, past Eve and Adam’s, from swerve of shore to bend of bay, brings us by a 
commodius vicus of recirculation back to Howth Castle and Environs” – so began James 
Joyce’s (infamous) Finnegans Wake. That line is but the completion of the book’s last 
sentence, “A lone a last a loved a long the”. You can, of course, stitch the two halves 
together in order simply by reading first this string and then that one.  
Joyce was a notorious jokester. One of the jokes he tucked away in that first and 
final sentence is a pun on the name of a scholar who straddled the seventeenth and 
eighteenth centuries, Giambattista Vico. “Vicus” puns on the Latin for village, street, or 
quarter of a city and on Giambattista’s last name. Just why Joyce did that has prompted 
endless learned commentary, none of which is within the compass of this essay, though, 
be forewarned, we’ll return to the Wake at the end. 
Neither, for that matter, is Vico, not exactly. He had an epistemological principle: 
“Verum esse ipsum factum," often abbreviated as verum factum. It meant, “What is true is 
precisely what is made.” In this he opposed René Descartes, who believed that truth is 
verified through observation. Descartes, with his cogito ergo sum and his mind/matter 
dualism, is at the headwaters of the main tradition in Western thinking while Vico went 
underground but never disappeared, as Finnegans Wake bears witness. 
Perhaps this century will see our Viconian legacy eclipse the Cartesian in the study 
of the mind. With that in mind, let’s consider Grace Lindsay’s excellent Models of the 
Mind. 
What, you might ask, what kind of book is it? It could be a highly technical mid-career 
summary and synthesis, which would certainly be welcome. But no, it’s not that. It 
could be a textbook for an advanced undergraduate or a graduate level course in 
neuroscience. It’s not that either. There are a few footnotes, but each chapter has a 
reasonable bibliography at the back of the book. No, Models of the Mind is intended for 
the sophisticated and educated reader who is interested in how physics, engineering, and 
mathematics have shaped our understanding of the brain, to reprise the book’s subtitle. And 
that’s just the right audience if we want to pull off a paradigm change, from Cartesian to 
Viconian, in how we understand the mind.  
It is intended for you, gentle reader. 
The mind, so the saying goes, is what the brain does. But without (mathematical) 
models that saying collapses into a string of words without intellectual substance. 
What’s a model? 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
10 
My cousin, Erik Ronnberg, Jr.9, makes ship models, a craft he learned from his 
father. Most are models of sailing ships; some models are large, some are small; but no 
one would mistake such a model for a real ship. They are too small, and few, if any, 
would sail, even in a small protected pond. But they must look like a real ship. Thus in 
researching a project Erik will seek out the plans for the ship he is modeling and make 
his own plans accordingly.  
One of his clients was a marine artist. Erik would build the model and then the 
artist would pose it – in a shallow box filled with sand I believe – at an angle appropriate 
for his painting. The model had to resemble a real ship so closely that a marine artist 
could use it the way another artist would pose the subject of a portrait, or, for that 
matter, work from an artist’s model – a real person being posed for study purposes or as 
a subject in, for example, a historical tableau. 
The ship model is like the real thing only in physical appearance, shape primarily, 
but then color – but what color, when new and freshly painted or when weathered from 
years at sea? It is much smaller than a real ship and does not have an interior 
construction like a real ship. Most models have hulls carved from a single piece of wood, 
though more elaborate ones having planking over ribs attached to a keel, but even those 
models do not have interior decks, and so forth. See all those small pulley blocks in the 
rigging? Real pulley blocks are made of wood with metal hardware. Those model blocks 
are either cast from metal or carved from single pieces of word; they don’t have any 
moving parts. None of the rigging is actually functional, but it looks good.  
In one sense it’s all fake; but in another, none of it is. The model builder isn’t trying 
to deceive anyone. He’s willing to answer any questions you have about how the models 
are built – except, perhaps, for how they get them into the bottle; you have to give the 
secret handshake to learn that. 
The mind, alas, is not a physical thing like a ship, though the brain is. And brain 
processes are not amenable to direct observation in the way we can observe the 
movements of the planets, or little animalcules wriggling in water. Figuring out what 
the brain does is like trying to understand how an automobile engine works by listening 
to engine noises. Yes, there are correlations between those noises and the operation of 
engine mechanisms, but there are many ways of producing such noises. Which ones are 
actually used in the engine? Fortunately we can open up the hood and take a look. Heck, 
we could consult the engineers who designed the engines. 
Alas, the engineer or engineers who designed the brain are either beyond our reach 
or are mere fictions. While we can “open the hood” as it were – neuroscientists have 
devoted a lot of time and effort into figuring out various ways of doing that – the 
processes that most interest us cannot be directly observed. We must infer them 
indirectly, by building models. 
How do you model what the brain does? Here’s what Lindsay says in her opening 
chapter: 
 
9 Here is a news article about him, https://www.capeannmuseum.org/news/2019/08/22/maritime-
curator-erik-ronnberg-jr-receive-edgar-b-caffrey-award/. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
11 
Mathematical models are a way to describe a theory about how a biological 
system works precisely enough to communicate it to others. If this theory is a 
good one, the model can also be used to predict the outcomes of future 
experiments and to synthesise results from the past. And by running these 
equations on a computer, models provide a ‘virtual laboratory’, a way to 
quickly and easily plug in diﬀerent values to see how diﬀerent scenarios may 
turn out and even perform ‘experiments’ not yet feasible in the physical world. 
By working through scenarios and hypotheses digitally this way, models help 
scientists determine what parts of a system are important to its function and, 
importantly, which are not. 
 
We have three things: 1) experiments and observation, 2) computer simulations, and 3) 
mathematics.  
We use mathematics both to analyze the data and to specify the design of a 
simulation. Mathematics thus closes the circuit of inquiry so that the process is one of 
Viconian making, and not merely Cartesian observing. Though Models of the Mind is very 
much about mathematics, you don’t need much math to understand it. If you’re hungry 
for equations, they’re in an appendix, but the main text is relatively free of math, though 
it does have helpful diagrams and illustrations. 
Lindsay also traces the history of the various models and techniques she discusses, 
often back into the 19th and even 18th centuries. Just what, you’re asking, does she 
discuss? Why don’t I list the title headings? Boring, I know, but it gives you an idea of 
the book’s range. And, bonus points, it’s a very Joycean thing to do. Melville would 
approve as well. 
Chapters: 1) Spherical Cows, 2) How Neurons Get Their Spike, 3) Learning to 
Compute, 4) Making and Maintaining Memories, 5) Excitation and Inhibition, 6) Stages 
of Sight, 7) Cracking the Neural Code, 8) Movement in Low Dimensions, 9) From 
Structure to Function, 10) Making Rational Decisions, and 12) Grand Unified Theories of 
the Brain. 
Concerning those grand unified theories, after discussing the tango physicists have 
danced with their GUTs (Grand Unified Theories) Lindsay discusses Karl Friston’s free 
energy principle, Jeff Hawkins’ Thousand Brains Theory (which sounds a bit like AI 
maven Marvin Minsky’s society of mind idea10), and the integrated information theory 
(IIT) of Giulio Tononi. She’s skeptical, noting: 
Nervous systems evolved over eons to suit the needs of a series of speciﬁc 
animals in speciﬁc locations facing speciﬁc challenges. When studying such a 
product of natural selection, scientists aren’t entitled to simplicity. Biology took 
whatever route it needed to create functioning organisms, without regard to 
how understandable any part of them would be. It should be no surprise, then, 
to ﬁnd that the brain is a mere hodgepodge of diﬀerent components and 
mechanisms. That’s all it needs to be to function. In total, there is no 
 
10 Here’s Wikipedia’s article about Minsky’s book, 
https://en.wikipedia.org/wiki/Society_of_Mind. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
12 
guarantee – and maybe not even any compelling reasons to expect – that the 
brain can be described by simple laws. 
I agree. 
Now let’s examine a few specific cases.  
In 1943 Warren McCulloch, one of the Grand Old Men of neuroscience, and Walter 
Pitts, a young protégé, published a paper with the sobering title, “A logical calculus of 
the ideas immanent in nervous activity” (we’re in Chapter 3: Learning to Compute). 
Working from what little was known about the structure and connectivity of neurons, 
McCulloch and Pitts figured out how they could perform basic logical operations. 
Lindsay informs us: 
The radical story that McCulloch and Pils told with their model – that neurons 
were performing a logical calculus – was the ﬁrst alempt to use the principles 
of computation to turn the mind–body problem into a mind–body connection. 
Networks of neurons were now imbued with all the power of a formal logical 
system. [...]  
With this step in their research, McCulloch and Pils advanced the study of 
human thought and, at the same time, kicked it oﬀ its throne. The ‘mind’ lost its 
status as mysterious and ethereal once it was brought down to solid ground – 
that is, once its grand abilities were reduced to the ﬁring of neurons. To adapt a 
quote from Lelvin, the brain could now be thought of as ‘a machine, meaty and 
miraculous, but still a machine’.  
She goes on to observe that neuroscientists ignored the paper, perhaps because of its 
technical nature, and perhaps because it didn’t obviously lead to experiments. But the 
idea was taken up by the pioneers of artificial intelligence. 
We now know that the conception McCulloch and Pitts – and, for that matter, pretty 
much everyone else at the time – had of neurons was way too simple. If we are to think 
of each neuron as a computing device, then it’s a complex electro-chemical computer 
rather than a simple logic circuit. But the damage had been done. The good ship 
Descartes had been hit below the waterline. That the brain is a thinking machine was now 
both a thinkable and a sensible idea. 
A decade and a half after McCulloch and Pils broke the mind-maler barrier 
Frank Rosenblal ﬁgured out how a machine could learn. He called his device a 
Perceptron. It was a relatively simple network of artiﬁcial neurons 
implemented in a mass of “switches, plugboards, and gas tubes.” Objects were 
presented to it on “a 20x20 grid of light sensors” which were connected to an 
array of a thousand association units which were in turn connected to response 
units. The machine would guess what the object was based on the connections 
among those association units. If its guess was wrong, then connections from 
the association units are modiﬁed. If its guess was correct, nothing happened. 
The process would continue until the Perceptron stopped making errors.  
This procedure for learning was, in many ways, the most remarkable part of the 
Perceptron. It was the conceptual key that could open all doors. Rather than 
needing to tell a computer exactly how to solve a problem, you need only show 
it some examples of that problem solved. This had the potential to revolutionise 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
13 
computing and Rosenblal was not shy in saying so. He told the New York Times 
that Perceptrons would ‘be able to recognise people and call out their names’ 
and ‘to hear speech in one language and instantly translate it to speech or 
writing in another language’.   
Alas, Rosenblatt was ahead of himself, way ahead. It would be years, decades in 
fact, and many ideas later, not to mention vastly more powerful machines, before 
computers could do either of those things reasonably well, as they now can, at least for 
some purposes. 
Moreover, as Lindsay notes: 
The power of learning, however, came with a price. Leling the system decide 
its own connectivity eﬀectively divorced these connections from the concept of 
Boolean operators. The network could learn the connectivity that McCulloch 
and Pils had identiﬁed as required for ‘and’, ‘or’, etc. But there was no 
requirement that it does, nor any need to understand the system in this light. 
[...] Compared with the crisp and clear logic of the McCulloch-Pils networks, 
the Perceptron was an uninterpretable mess. But it worked. Interpretability was 
sacriﬁced for ability.   
And that remains true today. The most powerful AI engines can do remarkable 
things, but just how they do them, that is something of a mystery. In that respect they 
are dismayingly like the operations of the human brain. While it has long been a cliché 
that computers only do what they’re programmed to do, that is not true of these 
learning engines. We program them to learn, which they do. After that, though, they 
seem to have an agency, however minimal, of their own. 
Let’s consider one more example. One of Pitts’ colleagues in McCulloch’s laboratory 
was a man named Jerome Lettvin (we're in Chapter 6: Stages of Sight). He investigated 
the response properties of cells in a frog’s retina.  
In fact, in a 1959 paper ‘What the frog’s eye tells the frog’s brain’ he and his co-
authors describe four diﬀerent types of ganglion cells that each responded to a 
diﬀerent simple palern. Some responded to swift large movements, others to 
when light turned to dark and still others to curved objects that jilered about. 
These diﬀerent categories of responses proved that the ganglion cells were 
speciﬁcally built to detect diﬀerent elementary palerns. Not only did these 
ﬁndings align with Selfridge’s notions of low-level feature detectors, but they 
also supported the idea that these features are speciﬁc to the type of objects the 
system needs to detect.    
That paper quickly became a classic in the neuroscience of vision. A bit over twenty 
years later a British neuroscientist, Nicholas Humphrey, published a paper entitled 
‘What the Frog’s Eye Tells the Monkey’s Brain’.11 He had been doing experiments where 
he destroyed the visual cortex – a ‘higher’ visual center – of monkeys’ brains and 
discovered that the monkeys retained some visual ability. When present in humans this 
phenomenon came to be called blindsight.  
 
11 Nicholas Humphrey, What the Frog's Eye Tells the Monkey's Brain, Brain Behavior and Evolution 
3(1):324-37, February 1970 DOI: 10.1159/000125480. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
14 
How could that be? The mammalian cerebral cortex is a structure that’s lacking in 
amphibians and reptiles. But the mammalian visual system is not entirely located in the 
cortex. It has subcortical structures that correspond with structures in the frog’s brain. 
Humphrey hypothesized that those structures remain active in these monkeys thus 
affording them some measure of visual ability. 
Amphibians are evolutionarily older than mammals. The evolutionary process 
doesn’t necessarily discard structures when new ones develop. The old ones remain, 
perhaps serving a different function. Thus, we return to a passage we’ve already quoted, 
one where Lindsay notes, “the brain is a ... hodgepodge of different components and 
mechanisms.”  
That recapitulation, that recursus, in turn suggests another, one that will take us 
back to where we began: the opening sentence of James Joyce’s Finnegans Wake. A couple 
years ago Adam Roberts, a British jokester, punster, and writer of speculative fiction, 
decided that Finnegans Wake just had to be translated into Latin. It makes a kind of weird 
sense, doesn’t it? We take a book that no one reads, though they may claim to do so, and 
translate it into a language that is no longer spoken, Latin. The result: Pervigilium 
Finneganis: “Finnegans Wake” translated into Latin.12 
As Roberts explains in his helpful introduction, though he has some knowledge of 
Latin, he doesn’t have enough to undertake such a project. What does he do? He turns to 
Google Translate. Google Translate is the fruit, one of many, of Rosenblatt’s Perceptron. 
Roberts had to clean things up here and there before feeding Joyce’s text to the machine 
5000 characters at a time, and he did some post-editing as well. It was tedious and time 
consuming, but not so tedious and time consuming as doing a full-fledged human 
translation would have been. 
So, let us end this essay where we began, only in Latin: “flumenflue, transitum Eva 
et Adae, declinationem ab litore ad flectere lauri, commodius ab nobis facit Houuthi 
vicus de castro et recirculus ad circumstant.”   
Thank you, Adam. And thank you, Grace Lindsay. 
 
 
 
 
 
 
12 Adam Roberts. Pervigilium Finneganis: “Finnegans Wake” translated into Latin. Ancaster Books. 
Kindle Edition. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
15 
Five Good Books 
 
 
While I am no fan of the cliché that attributes all worthwhile ideas to Plato, I do think all 
ideas have influential predecessors. These five books are a useful set of predecessors in 
the speculative engineering approach to understanding minds. Note that von Neumann 
has quite a bit to say about the differences between digital and analog devices, while 
Powers proposes and analog model and Hays proposes a digital modal atop an analog 
one. 
 
(1) John von Neumann (1958). The Computer and the Brain. New Haven: Yale University 
Press. 
Von Neumann was a mathematician who made contributions in many fields. But he 
is best known for his work in computing. This slender volume (82 pages) is the last 
project he worked on and is incomplete. Brain cancer took him before he could finish.  
It is about how a computing process can be embodied or implemented in physical matter, 
and discusses two modes, the analog and the digital, and, among other things, addresses 
the limitations these modes impose. Real computation is a material process. Therefore a 
computational approach to literature is materialist. 
Though the book contains no math, it is quite abstract. Its details are at some remove 
from all the complex details about existing computers (then and now) or the messy 
wetware of the brain. That is to say, it is about the essential.  
Forget about the fact that computers are now quite different from those von 
Neumann knew, and forget about the fact that most of what we know about the brain 
was discovered since von Neumann’s death. In this book a first-class mind grapples 
with deep questions in simple, if abstract, terms. Reading it is a good workout. 
 
(2) Herbert A. Simon (1981). The Sciences of the Artificial, Second Edition. Cambridge, 
MA: MIT Press. 
Trained in political science, Simon became one of the founding fathers of the 
symbolic approach to artificial intelligence, computing, and cognitive science during the 
1950s and 60s. This is a relatively informal collection of essays that has been widely, and 
justly, influential. From the preface (xi): 
Engineering, medicine, business, architecture, and painting are concerned, not 
with how things are but with how they might be—in short, with design. . . . 
These essays then alempt to explain how a science of the artiﬁcial is possible 
and to illustrate its nature. I have taken as my main examples the ﬁelds of 
economics (chapter 2), the psychology of cognition (chapters 3 and 4), and 
planning and engineering design (chapters 5 and 6) [my emphasis, BB].  
Chapter 7 is entitled “The Architecture of Complexity” (originally published in 
1962) and takes up the problem of biological evolution. Chapter 3, “The Psychology of 
Thinking: Embedding Artifice in Nature,” contains a well-known passage about an ant 
walking on the beach. 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
16 
 
(3) William Powers (1973). Behavior: The Control of Perception. Aldine Transaction.  
Powers was trained as an engineer and very much concerned with building real 
things. Powers has two ideas, both from classical control theory, often known as 
cybernetics (and one of them is more or less given in his title), and both quite elegant. 
His is thus an analog account of the mind. This is how to go about theorizing, be as clear 
as you can and use elegant examples.  
His first idea is that the brain – for that, ultimately is what he is talking about, and 
he does provide some neural evidence – is organized as a stack of servomechanical 
systems, from sensory perceptors and motor effectors up through the most recently 
evolved systems of neocortical function. He discusses nine levels, with the most detailed 
discussion given to the first five. Each level in the stack is characterized by a specific set 
of reference levels (a term from control theory, you might want to think of them as akin 
to Bayesian priors). As you move up the stack the time-span of operation becomes 
longer. 
His second idea is that, while the tissue that constitutes the stack is specified by the 
genes, the values they take on, their reference level, are learned through a process of 
adaptive control, which he calls reorganization. This leads to interesting accounts of 
memory and imagination. 
 
(4) David G. Hays (1981). Cognitive Structures. HRAF Press. 
Hays was my teacher, and then my colleague, so I am biased.  
In this book he sets out to ground the symbolic systems he’d developed and learned 
as a computational linguist in the analog framework Powers laid out in his book. He 
accepts Powers’ first five levels and replaces the rest with a cognitive network, more or 
less of the symbolic kind. The two are linked through parameters of perception. The 
parameters regulate the operation of the analog servomechanisms while each node in 
the cognitive network is characterized by the parameter values of the servomechanical 
unit on which it is grounded.  
The model consists of four components: 1) the sensory-motor system (conceived as a 
Powers stack), 2) a systemic network (roughly comparable to a dictionary), and 3) an 
episodic system (comparable to an encyclopedia. Systems of the second and third kinds 
well-studied in the cognitive sciences. Hays called his fourth system the gnomic system, 
a concept he introduced. Roughly speaking, it is concerned with truth values: Was an 
episode experienced directly or known by hearsay? Did it occur in a dream or was it 
imagined?  
This is a difficult book, but deep. It is also hard to obtain. You are unlikely to find it 
in your favorite library. It can be ordered from the publisher: Human Relations Area 
Files (printed on demand): https://hraf.yale.edu/publications-archives/hraf-press-other-
publications/. 
 
 
 
Electronic copy available at: https://ssrn.com/abstract=4078246

 
17 
(5) Valentine Braitenberg (1999). Vehicles: Experiments in Synthetic Psychology. Cambridge, 
MA: MIT Press. 
This is a cumulative series of thought experiments, 14 of them in the first 83 pages. 
Braitenberg asks us to imagine a simple (artificial) creature in a simple environment. 
Here’s how he begins to describe the first one: “Vehicle 1 is equipped with one sensor 
and one motor. The connection is a very simple one. The more there is of the quality to 
which the sensor is tuned, the faster the motor goes” (p. 3). He then works out the 
consequences of this very simple creature, how it moves about.  
In the second chapter he gives the vehicle two sensors and two motors and from that 
constructs primitive fear and aggression. And so it goes for the rest of these 14 chapters. 
In each chapter he adds a little bit to the vehicle from the previous chapter and explores 
the behavior consequences, e.g.: love (vehicle 3), concepts (#7), getting ideas (#10), 
egotism and optimism (#14). The last 50 pages contain biological notes on the vehicles, 
thus relating to the real nervous systems of real animals. It conveys a sense of design, 
engineering, and construction that is important in developing a 21st century 
understanding of minds, whether natural or artificial. 
 
 
 
 
Electronic copy available at: https://ssrn.com/abstract=4078246

