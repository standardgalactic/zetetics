Basins of Attraction and the Density
Classiﬁcation Problem for Cellular Automata
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
Charles Sturt University
tbossomaier@csu.edu.au
Abstract. The density classiﬁcation problem has long been recognized
as a benchmark for exploring cellular automata rules with global prop-
erties. One of the major diﬃculties in evaluating rules is the enormous
number of conﬁgurations which have to be tested, particularly impor-
tant in any search process. We consider an approach to rule evaluation,
which is in some ways the reverse of the standard methods, and discuss
problems and opportunities raised by the approach.
1
Reformulation of the Density Classiﬁcation Problem
Cellular automata (CA) have excited interest over a long period as minimal
computational systems. The most popular example is a two-dimensional grid
world, the Game of Life, invented by John Conway [4], in which binary counters
live and die on a grid. Life or death is determined by a rule which speciﬁes,
for every possible pattern of nearest neighbours, whether the grid point in the
next generation will be occupied or not. The interesting thing about this very
simple system was the enormous range of diverse patterns and behaviour it
generated. This is just one example: CAs may be of arbitrary dimension from one
dimensional strings and upwards and have all sorts of diﬀerent rule structures.
But in their canonical form they are minimal in two respects: they have no
memory and they operate locally. Despite this CAs can achieve certain forms
of global computation. One well-known form is the ability of some CA rules to
partially solve the density classiﬁcation problem, in which a one-dimensional,
two-state CA (with circular boundary conditions) is required to relax to a state
of all 1s if the density of 1s in the initial conﬁguration (ie. at time-step zero)
is greater than some previously speciﬁed critical density ρc, and, conversely, to
relax to a state of all 0s if the initial density of 1s is below ρc. This task is
intrinsically global in nature and is therefore diﬃcult for a CA to solve, since
it has neither the ‘memory’ to keep a cumulative count nor the spatial range of
perception to make a correct assessment.
The density classiﬁcation problem has therefore attracted a great deal of
interest from those studying the emergence of global behaviour. All the work
to date on this problem has used the natural approach of determining a rule’s
performance for a given CA conﬁguration by taking the conﬁguration and moving
forward the speciﬁed number of time-steps in order to see if it has relaxed to the
required state.
J.-C. Heudin (Ed.): VW 2000, LNAI 1834, pp. 245–255, 2000.
c
⃝Springer-Verlag Berlin Heidelberg 2000

246
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
An alternative, however, is the use the algorithm developed by Andy Wuen-
sche [12] for running a CA rule backwards. In other words, given a CA conﬁg-
uration it is possible to determine all the precursor states. This opens up the
possibility of using a dynamical-systems approach to CA problems.
Such an approach is seemingly very well suited to the density classiﬁcation
problem, since it can be cleanly reformulated as follows: ﬁnd a CA rule such that
the whole of conﬁguration-space is divided into precisely two basins of attraction;
one of which consists of all rules with density greater than the critical density
and has as its attractor the state of all 1s, and the other consisting of all other
conﬁgurations and having as its attractor the state of all 0s.
This then raises the possibility that one can now avoid some of the diﬃculties,
both conceptual and computational, of the standard ‘forwards’ approach. For
example, one might be able to more eﬃciently estimate the performance of a
rule not by picking CA conﬁgurations at random and following each of them
forward to their attractor, but by starting with a single conﬁguration, following
it forward to its attractor (so as to determine which basin of attraction we are in),
and then working backwards from that conﬁguration, secure in the knowledge
that all precursors also belong to that same attractor. The percentage of these
precursors which are in the right density range would then give some sort of
measure of the rule’s performance.
2
Characteristics of Rule Evaluation by the Forwards
Method
In the forwards method of rule evaluation one takes a CA conﬁguration and uses
the CA rule to move forward a prescribed number of time-steps, then checks to
see if the CA has reached the desired ﬁnal state.
For the benchmark CA size of 149 cells, there are a total of 2149 ≈1045
diﬀerent CA conﬁgurations against which a rule could be tested to determine its
performance in density classiﬁcation. Obviously some form of sampling is needed.
The standard for majority classiﬁcation has become to generate a sample group
of CA conﬁgurations in which we randomly and independently assign a 1 or 0
to each cell giving rise to an equal probability for the two outcomes.
The density of 1s thus created obviously follows a binomial distribution about
0.5. Some authors have used CA conﬁgurations with densities uniformly dis-
tributed in [0, 1], but this is an inferior choice for a number of reasons. The
above binomial distribution is closely approximated by a normal distribution
with mean 0.5 and variance (4n)−1 where n is the number of cells in the CA.
For 149-cell CAs this gives a standard deviation of 0.04, equivalent to 6 cells.
The rule in question is assessed on that sample group of conﬁgurations, and
the percentage correct taken as an approximate measure of the ‘true percent-
age’. Such a measure obviously has a sampling error problem, an issue that is
frequently underplayed, both in the design and the reporting of experimental
work.

Basins of Attraction and the Density Classiﬁcation Problem
247
If we approximate the success of a given rule over such conﬁgurations as
being given by a Bernoulli distribution with probability p (typically p is known
to be in the range [0.8, 0.9]), and if we take N samples, then by the DeMoivre-
Laplace theorem the success rate determined by sampling has a distribution
which is approximately normal, with mean p and variance p(1 −p)/N. The
approximation is traditionally considered valid if Np > 5 and N(1 −p) > 5,
which is always the case for this problem.
So, if we wish to assign a ﬁtness value to a rule based on sampling which
will have three signiﬁcant ﬁgures to 95% conﬁdence, we require that .0005 be at
least two standard deviations. This means that 2σ = 2
p
p(1 −p)/N ≤0.0005,
or
N ≥4p(1 −p)/(0.0005)2 ≈2.5 × 106
since the above ﬁgure is not particularly sensitive to p.
Since most success measures are given in terms of percentage-correct we will
also express the uncertainty in those terms.
The above shows that, to ﬁnd a ﬁtness score with an uncertainty of ±10% one
needs approximately one thousand tests (this gives a 95% conﬁdence interval).
To reduce the uncertainty to ±1% one needs 104 tests, to ±0.1% one needs 106
tests, and to ±0.01% one needs 108 tests.
For the 149-cell CAs the best rules over a decade’s research all concentrate in
the low-to-mid 80% range as measured in the above way [7, 6, 8, 1, 9]. At present
we do not know – and have no way other than empirical search of knowing –
just what performance the best rules possible are capable of. Work by Land and
Belew [10] shows that for any rule there are some starting conditions which will
be misclassiﬁed, so perfect performance is impossible. However, the argument in
[10] shows failure on a statistically insigniﬁcant set of rules, and does not impose a
practical upper limit on performance. While there are undoubtedly other types
of conﬁgurations, for which similar arguments can be made, it remains likely
that there are rules still to be found which will perform far better than any yet
known. There may be rules whose performance is essentially indistinguishable
from 100%.
There are several known problems intrinsic to the forwards method. One is
that, with the exception of Cranny’s work on structured density classiﬁcation rule
[5], all good rules identiﬁed in the last decade have been found by evolutionary
search methods. Such methods (as with hill-climbing techniques) require one to
distinguish (at least in some probabilistic sense) between closely related rules
that have minor diﬀerences in peformance. However, as shown above, rules close
in performance require a vast number of tests to discriminate between them,
making it all the harder for the evolutionary process to make progress once
the early dramatic improvements have been made. This tends to frustrate the
evolutionary search just when the results were becoming interesting.
For some while the best available rule was the GKL rule [8], essentially de-
signed for stability in the presence of noise rather than the density problem per
se. Subsequent progress on the density classiﬁcation problem came through the
use of evolutionary methods, and in such techniques noise plays a complicating

248
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
role which is not yet understand properly. For example, the numerical, if not
conceptual, progress on the density classiﬁcation problem in the last decade has
often been hailed as a sign of increasing sophistication in the evolutionary tools
used (moving from simple GAs (Mitchell and others [7, 6]) to genetic program-
ming (ABK rule from Andr´e, Bennett and Koza [1]). Bossomaier and Cranny [2]
switched back to a simple evolutionary procedure, but using a restricted, struc-
tured search space where Cranny had discovered most of the good rules to lie.
Cranny also introduced a whole range of superior rules by enumeration based
on this approach [5]. The most recent signiﬁcant step was the introduction of
coevolution with resource sharing (the JP rule from Jordan and Pollack [9]).
A possible contributing factor to progress (which is less likely to be true
and which does not seem to have been properly addressed) is that evolutionary
methods of little increasing intrinsic merit have become more successful over
time because sampling noise has diminished over time through the use of larger
samples, as made feasible by increasing computational power.
Another problem speciﬁc to the forwards method is that many CA conﬁgu-
rations saturate long before the prescribed number of time-steps have elapsed.
The massive number of calculations needed for the forwards method means that
most researchers ﬁnd themselves using high-performance computers with vector-
or parallel-processing, but the constraints of such approaches make it almost im-
possible to exploit these ‘early saturators’.
3
Evaluating Attractor Basins
The above problems make it worthwhile to ask if a fundamentally diﬀerent ap-
proach to density classiﬁcation can be found. One alternative is the dynamical
systems approach mentioned in the Introduction, using the backwards algorithm
of Andy Wuensche [12]. Such an approach may have some qualitative advan-
tages as well as possibly oﬀering greater eﬃciency than the forwards approach.
For example, it is reasonable to hope for computational savings since we are
not following each conﬁguration all the way to the attractor, but merely moving
between conﬁgurations directly linked by the system dynamics.
The language of dynamical systems needs a little latitude of interpretation in
this context, since we are dealing with intrinsically ﬁnite and discrete systems,
but we will use words like ‘attractor’, ‘basin of attraction’ etc. in the appropriate
sense.
To measure a rule’s performance in this context one can start with a single
conﬁguration, map out the ‘backwards’ part of the basin of attraction, and take
as a measure of the rule’s performance the fraction of that following that back-
basin. We need to emphasise this ‘back-basin’ idea, since we cannot map the
entire basin: it is likely to have of the order of 1045 members. We therefore are
not free of sampling and sampling-error, but it reasonable to check to see if these
problems are reduced.
We implemented the Wuensche algorithm and ran it on some of the most
signiﬁcant rules for the density problem (GKL, ABK, Cranny and JP1). As is

Basins of Attraction and the Density Classiﬁcation Problem
249
typically the case, any conﬁguration generated at random is highly likely to be
a Garden of Eden state, i.e. a state which has no predecessor states. Wuensche’s
approach to developing the attractor is then to run the CA forward a few itera-
tions and work back from the resultant state. Since we are typically using fairly
large CAs even three or four steps forward will produce a back-basin with thou-
sands or tens-of-thousands of precursor states. There is no way of being more
precise as to what will happen, but some results of Wuensche (for example the
Z parameter [11]) does give some indication of how expansive the attractor will
be.
One might reasonably expect the density of Garden of Eden states to diminish
as one moves from critical-density to the extremes, but this was found not to
be the case: even when we generated populations of conﬁgurations with 120 of
14 cells set to 1, over 9.9% of conﬁgurations were Garden of Eden. We explain
below why such a result might be expected for good density-classiﬁcation rules.
The computational eﬃciency of the above process is hard to measure. In
practice the running time is strongly aﬀected by the eﬃciency of memory alloca-
tion/deallocation, cache and main memory size and the eﬀectiveness with which
the compiler/system libraries implement recursion. We have therefore adopted
a simple estimate. We count every lookup table access made in developing the
attractor, a total of Nl. Many of these will not lead to a precursor state at all.
Now if we imagine running the CA forwards, then each time-step requires L, the
length of the CA, lookups. So if the attractor has Na states, then we can deﬁne
the eﬀective number of iterations of the CA, Ieff as
Ieff =
Nl
NaL
Ideally Ieff should be smaller than the number of iterations one would run a
CA for to determine if it saturates. A typical value would be 300.
Table 1 shows results for the GKL rule on 37-cell CAs . To generate the
above table the following process was used: to avoid a Garden of Eden starting
conﬁguration a random conﬁguration with 19 out of 37 cells set at 1 was gen-
erated, and followed forward three time-steps to a non-Garden of Eden state.
(In fact it was followed to its attractor, to ensure that it was in the appropriate
basin of attraction). From there the Wuensche backwards algorithm was used to
ﬁnd all precursor states. These precursor states can be labelled ‘correct’ or ‘in-
correct’ based on whether or not they are on the right half of the density divide.
The ‘starting density’ column shows the number of 1s out of 37 after the three
time-steps forward (i.e. at the state from which the back-basin was calculated).
Several things are readily apparent from Table 1. One is that Ieff is no-
ticeably lower than 300, suggesting a computational gain. The other, and more
problematic, point is that the values in the ‘Percentage Correct’ column ﬂuctuate
quite noticeably. This is to some extent unavoidable, since we are still resorting
to sampling; it is just that here we are sampling regions of the attractor basin.
There is some correlation in the above table between the starting density (i.e.
the density of 1s in the CA after the 3 time-steps) and the percentage correct. As

250
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
Number Number of
Total
Percentage Ieff Starting
of Steps
Correct
Number of
Correct
Density
Back
Precursors
Precursors
5
1242
1314
94
102
22
5
827
1192
69
144
20
9
46907
79439
59
91
19
14
395905
740364
53
94
19
8
36092
39111
92
68
24
10
25067
31633
79
148
22
7
1150
1290
89
181
21
16
2545243
2763887
92
38
24
24
4526149
6875618
65
13
20
8
3759
4692
80
129
20
10
1472
1692
86
117
21
8
10728
12415
86
109
19
8
11426
13012
87
94
23
7
20620
20712
99
126
25
8
3661
3805
96
83
25
4
3070
3295
93
129
22
6
495
1493
33
105
19
11
63592
84649
75
79
22
9
3015
4107
73
121
23
6
935
1512
61
114
19
Table 1. 37-cell CA results for the GKL rule.
would be expected, the higher the density, the higher the percentage of correct
precursors the state has.
Since all good rules perform almost-perfectly on conﬁgurations suﬃciently
away from the critical density, it was thought that perhaps a better measure of
rule performance would come from only considering those conﬁgurations where
the density after the 3 time-steps remained at the critical level. The data col-
lected is given in Table 2.
This and other data showed that, as expected, the success percentage
recorded went down. Little other improvement was noted, and this avenue was
abandoned.
Data comparable to the above were collected for several other rules (the ABK,
JP1 and Cranny rules), and show the same phenomena. (The data is available
in [3].)
This raises another potential problem with the backwards scheme: by its very
nature it does not detect or measure any basins of attraction other than the one
it is actively studying. But this is less of a problem for good rules than poor, since
those rules typically have only the two major basins of attraction (as desired)
and a few minor limit cycles which do not contribute much to the failure rate of
the rule. The vast majority of failures by good rules come from conﬁgurations

Basins of Attraction and the Density Classiﬁcation Problem
251
Number Number of
Total
Percentage Ieff
of Steps
Correct
Number of
Correct
Back
Precursors
Precursors
9
9542
12218
78
78
8
683
2983
22
83
7
46543
135368
34
91
29
4347652
7906988
54
12
18
989078
1674155
59
21
Table 2. 37-cell CA results for the GKL rule with starting density constrained
to 19.
being drawn into the wrong one of the two main attractors, a failure-mode which
is easily detected by the above process.
At this point it is worth-while to introduce an image as to what we believe
is happening. Those rules which perform well at density classiﬁcation all seem
to do so by forming ‘domains’, whereby regions which are locally 1-heavy form
a solid block of 1s, and similarly for 0s. This can be seen in Figure 1, as can the
subsequent blending of the two regions when the white (or 0) domain is on the
left of the black (or 1) domain.
Fig. 1. Spacetime diagram for the GKL rule on a 149-cell CA.
In terms of what is happening in CA conﬁguration-space, this corresponds
to the movement from an arbitrary point in conﬁguration-space to a quite small
subspace (corresponding to the ‘domain’ conﬁgurations). The rule then processes
these domains in ways that are now well-understood (It is largely in the ﬁrst

252
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
few time-steps that our understanding is uncertain). The domain-conﬁgurations
therefore act as some sort of ‘royal road’ forward to the attractor: states typically
do not move independently towards the attractor, they ﬁrst converge on one
of these special conﬁgurations. This goes some way to explaining why almost
all conﬁgurations are Garden of Eden, even for extreme densities. The many
precursor states do not pass through these conﬁgurations on their way to the
attractor, but have already joined one of the royal roads.
We claim that in the above algorithm used to generate Table 1, the three time-
steps moved forward usually corresponding to the movement from the arbitrary
state to a domain state. This is in fact relatively easy to see from spacetime
diagrams such as in Figure 2. This means that we are in fact investigating the
back basin generated not from a ‘random’ conﬁguration, but from precisely those
conﬁgurations most important to the conﬁguration’s movement to the attractor.
Fig. 2. Two examples of substantial domain formation within three time-steps
for a density classiﬁcation rule (here, the GKL rule).
One useful side-eﬀect of this phenomenon is that it is reasonably easy to
generate the precursor states for domain conﬁgurations for a good density clas-
siﬁcation rule. If one has a local block of 1s, say, then any good rule will im-
mediately extinguish an isolated 0 in that block. One can therefore ‘seed’ the
existing domain structure with random 1s in the blocks of 0s, and/or random 0s
in the blocks of 1s. This will generate a large number of precursors (for larger
CAs, a vast number). One testable consequence of this argument is that from
elementary combinatorial arguments we would expect that one will get far more
precursors with approximately the same density as the ‘seed’ state than precur-
sors with signiﬁcantly diﬀerent densities. This is because while we can seed the
1s with 0s, lowering the density, or the 0s with 1s, raising it, there are far more
combinations where we do both, keeping the density approximately stable. This
would suggest a roughly binomial distribution of densities around the existing
density. This is precisely what can be observed, as in Figures 3 and 4.
4
Conclusions and Future Work
The backwards algorithm approach for the density classiﬁcation problem pro-
vides an alternative formulation which avoids many of the problems intrinsic to

Basins of Attraction and the Density Classiﬁcation Problem
253
10
15
20
25
30
0
0.5
1
1.5
2
2.5
x 10
6
Fig. 3. Histogram of precursor densities for a conﬁguration giving a high per-
centage correct (19 and above is super-critical density).
10
15
20
25
30
0
1
2
3
4
5
x 10
4
Fig. 4. Histogram of precursor densities for a conﬁguration giving a low percent-
age correct (19 and above is super-critical density).

254
Terry Bossomaier, Llewylyn Sibley-Punnett, and Tim Cranny
the standard forwards approach. At this early stage there remain several prob-
lems with the use of the backwards algorithm, but it has already provided a
number of valuable insights into good density classiﬁcation rules and the ﬂows
they create through conﬁguration space. It is a sign of the value of the approach
that the results obtained proved meaningful and consistent across diﬀerent rules
and CA sizes.
The main problem at this point is the wide variation in quantitative results
obtained when sampling diﬀerent parts of the attractor basin. This problem
may be amenable to more detailed statistical analysis in future work, and to
a deeper conceptual understanding of the ‘royal road’ paths through trajectory
space created through domain formation.
5
Acknowledgements
The authors would like to thank Andy Wuensche for a number of useful discus-
sions regarding his CA precursor algorithm.
References
[1] D. Andre, F. H. Bennett III, and J. R. Koza. Evolution of intricate long-distance
communication signals in cellular automata using genetic programming. In Ar-
tiﬁcial Life V: Proceedings of the Fifth International Workshop on the Synthesis
and Simulation of Living Systems. The MIT Press, 1996.
[2] T. R. J. Bossomaier, T. R. Cranny, and D. Schneider.
A new paradigm for
evolving cellular automata rules. In Proc. Congress on Evolutionary Computation,
Washington DC, pages 169–176, 1999.
[3] T.R.J. Bossomaier, L. Sibley-Punnet, and T.R. Cranny. Basins of attraction and
the density classiﬁcation problem for cellular automata. Technical Report RRAI-
01-00, Research Reports in Adaptive Informatics, Charles Sturt University, 2000.
[4] J. Conway. What is life? In E. Berlekamp, J. H. Conway, and R. Guy, editors,
Winning Ways for your Mathematical Plays, volume 2. Academic Press, New
York, 1982.
[5] T.R. Cranny.
On the role of symmetry, structure and simplicity in emergent
computation. Technical Report RRAI-99-02, Research Reports in Adaptive In-
formatics, Charles Sturt University, 1999.
[6] R. Das, J.P. Crutchﬁeld, M. Mitchell, and J.E. Hanson. Evolving globally syn-
cronized cellular automata.
In L.J. Eshelamn, editor, Proceedings of the Sixth
International Conference on Genetic Algorithms. Morgan-Kaufmann, 1995.
[7] R. Das, M. Mitchell, and J.P. Crutchﬁeld. A genetic algorithm discovers particle-
based computation in cellular automata.
In Y. Davidor, H-P. Schwefel, and
R. Maenner, editors, Parallel Problem Solving from Nature - PPSN III, volume
866 of Lecture Notes in Computer Science, pages 344–353. Springer-Verlag, 1994.
[8] P. Gacs, G.L. Kurdymunov, and L. Levin. One dimensional uniform arrays that
wash out ﬁnite islands. Problemy Peredachi Informatsii, 12:92–98, 1978.
[9] H. Juill´e and J. Pollack. Co-evolving the “ideal” trainer: application to the dis-
covery of cellular automata rules. In Proceedings of the Third Annual Genetic
Programming Conference, GP-98, 1998.

Basins of Attraction and the Density Classiﬁcation Problem
255
[10] M. Land and R.K. Belew. No two-state ca for density classiﬁcation exists. Physical
Review Letters, 74(25):5148–5150, 1995.
[11] A. Wuensche. Classifying cellular automata automatically. Technical Report 98-
02-018, Santa Fe Institute Working Paper, 1998.
[12] A. Wuensche and M. Lesser. The Global Dynamics of Cellular Automata. Addison-
Wesley, 1992.

