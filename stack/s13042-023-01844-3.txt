Vol.:(0123456789)
1 3
International Journal of Machine Learning and Cybernetics 
https://doi.org/10.1007/s13042-023-01844-3
ORIGINAL ARTICLE
OSAGGAN: one‑shot unsupervised image‑to‑image translation using 
attention‑guided generative adversarial networks
Xiaofei Huo1 · Bin Jiang1,2   · Haotian Hu1 · Xinjiao Zhou1 · Bolin Zhang1
Received: 23 August 2022 / Accepted: 16 April 2023 
© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2023
Abstract
This paper proposes a single-image translation method based on attention guidance to solve the problem of poor image 
quality in current single-image translation. The model uses a multi-scale pyramid architecture. First, the input image is 
downsampled, and then the downsampled image is input into the attention-guided generator to complete the translation of 
an image from the source domain X to the target domain Y. We introduce an attention module and a Scale-Add (SA) module, 
which can stabilize the training process of GAN and effectively improve the image quality. The attention module can retain 
the contour and detail of the object. In addition, the Scale-Add (SA) module can adjust the style of the image and add some 
low-scale detail information. Through extensive experimental verification and comparison with several baseline methods 
on benchmark datasets, we verify the effectiveness of the proposed framework.
Keywords  GANs · One-shot image-to-image translation · Attention-guided
1  Introduction
In recent years, image-to-image translation has been widely 
studied. Image-to-image translation can solve many prob-
lems in computer vision, such as image defogging [1], 
denoising [2], image stylization [3, 4], style transfer [5–9], 
image in-painting [10], super-resolution [11, 12] and so on 
(see Fig. 1 for an example). These problems all have a com-
mon goal: to learn the mapping from a source domain to a 
target domain while keeping the content characteristics of 
the source domain unchanged.
Due to the inherent differences between different tasks, 
these tasks are often handled separately. In the past few 
years, those frameworks that use Fully Convolutional Net-
works [13] and Conditional Generative Adversarial Nets 
[14] have been proposed so that these tasks can be handled 
uniformly. However, these methods require many paired data 
to participate in training, which is difficult to obtain in most 
cases. To date, many unsupervised image-to-image transla-
tion algorithms [5, 15–18] have been proposed, and they 
successfully transmit complex appearance changes between 
image classes. Zhu et al. [5], Kim et al. [15] and Yi et al. 
[16] used a cycle-consistent loss to reconstruct the translated 
image faithfully. However, these methods still need to collect 
a large number of unpaired images from the source domain 
and the target domain.
In real life, we may encounter extreme situations, such 
as: turning a daily photo into a photo of another style and 
the painter turning the actual scene into a painting. We can 
only obtain a few similar image construction datasets for 
training in these cases. Lin et al. [19] used a cycle-con-
sistent loss to capture the relationship between the source 
domain and target domain at different scales, but the image 
quality generated by this method is low, accompanied by 
 *	 Bin Jiang 
	
jiangbin@hnu.edu.cn
	
Xiaofei Huo 
	
hxfhnu@hnu.edu.cn
	
Haotian Hu 
	
huhaotian@hnu.edu.cn
	
Xinjiao Zhou 
	
zhouxinjiao@hnu.edu.cn
	
Bolin Zhang 
	
onlyou@hnu.edu.cn
1	
College of Computer Science and Electronic Engineering, 
Hunan University, Changsha 410082, Hunan, China
2	
Key Laboratory for Embedded and Network Computing 
of Hunan Province, Hunan University, Changsha 410082, 
Hunan, China

	
International Journal of Machine Learning and Cybernetics
1 3
some artifacts and noise. Therefore, how to ensure transla-
tion in case of an extreme shortage of samples.
Therefore, we propose a One-Shot Unsupervised 
Image-to-Image Translation using Attention-Guided 
Generative Adversarial Networks (OSAGGAN) to trans-
late images while preserving the content of the source 
domain. Inspired by [20–24], we introduced an attention 
module to retain the content and detail of the object. This 
module can guide the network to generate the attention 
mask that matches the content better to maintain the out-
line and details of the object. In addition, we demonstrate 
that unsupervised image-to-image translation with only 
two unpaired images can be achieved by combining an 
adversarial training scheme with a new network design. 
We compare our method with TuiGAN [19] to prove its 
superiority (see Fig. 2 for an example). The generated 
images by TuiGAN have some content loss and some 
apparent artifacts. Unlike this approach, our approach 
yields a result closer to the source domain style and better 
content retention.
The main contributions of this paper can be summarized 
as follows:
∙ To overcome the difficulty of insufficient data faced by 
the usual unsupervised translation methods, we propose an 
attention-guided generation adversarial network to solve the 
single-image translation problem.
∙ We introduce an attention module layer to guide the 
attention mask that matches the content and use a Scale-Add 
module to add low-scale details and promote the change of 
image style.
∙ The experiments demonstrate that our method is supe-
rior to the existing state-of-the-art methods.
2  Related work
2.1  Generative adversarial networks (GANs)
In recent years, image generation has been widely studied. 
The model based on variational autoencoder [25] aims to 
Fig. 1   Various image translation tasks

International Journal of Machine Learning and Cybernetics	
1 3
improve the quality and efficiency of the generated image 
by learning an inference network. Generative adversarial 
networks (GANs) [26] generate images from random vari-
ables through a two-player minimax game. The discrimina-
tor’s purpose is to distinguish the generated image from the 
real image, and the generator’s purpose is to generate the 
real image to fool the discriminator. Up to date, GANs have 
achieved great success in various image generation appli-
cations, including visual recognition [27], object detection 
[28], image generation [29], and so on. Our approach uses a 
cascade of generative adversarial networks within a pyramid 
framework to generate images in a coarse-to-fine manner.
2.2  Image‑to‑image translation
The basic idea of image-to-image translation differs from 
that of GAN. Instead of generating a new image from 
scratch, it learns a parameter conversion function to trans-
late the input image in the source domain into the output 
image in the target domain. The first method that can be 
used for various supervised image-to-image translation tasks 
is Pix2pix [14], in which Isola et al. used a regression loss. 
Then Wang et al. [30] used a coarse-to-fine generator to syn-
thesize high-resolution images. It can transform a semantic 
label image into a ground-truth image. However, in practical 
tasks, we may not be able to obtain a large number of paired 
data. When the paired training data cannot be obtained, 
image-to-image conversion becomes an ill-posed problem.
2.3  Unpaired image‑to‑image translation
In the past few years, many unsupervised methods have been 
proposed to overcome the limitation of data samples. The 
works by Zhu et al. [5], Kim et al. [15] and Yi et al. [16] had 
designed a cycle-consistency loss to achieve image transla-
tion. The works by Lee et al. [31] and by Huang et al. [32] 
were to encode the content and attributes of the global image 
and realize image translation by exchanging their attribute 
codes. Recently, there have been some unsupervised image-
to-image translation methods for few samples and single 
samples, such as FUNIT [33], OST [34], SinCut [35] and 
TuiGAN [19]. However, FUNIT requires a large number of 
images for pre-training. OST needs to use an image from 
the source domain and a group of images from the target 
domain. Although SinCUT [35] has achieved an excellent 
translation effect from high-resolution Claude Monet paint-
ings to natural photographs, it can not translate high-level 
semantic information well in more complex images, such 
as horse-to-zebra translation. In addition, his method can 
not convert images in the opposite direction. Tuigan [19] 
does not pay enough attention to the outline or details of 
the image, resulting in some fuzzy artifacts in the translated 
image or even failing to pay attention to the areas that need 
to be translated. In this article, we focus on solving the prob-
lem of single-image translation.
Fig. 2   Example of images translated result. (a) is translated by various methods (c)–(d), our method can get a generated image with a style con-
sistent with the target domain (b) and more complete content

	
International Journal of Machine Learning and Cybernetics
1 3
2.4  Attention learning
The attention mechanism can be understood as a system that 
mimics human vision and can quickly and efficiently focus 
on the most critical feature areas. The following formula can 
describe this process:
where g(x) represents the process of processing input image 
x and generating attention mask g(x), and f(g(x), x) describes 
the process of the processing input image in combination 
with attention mask. In recent years, due to the excellent 
effect of the attention mechanism, it has been introduced into 
the field of image-to-image translation. For example, Chen 
et al. [20] uses an additional attention network to generate 
attention maps to pay more attention to the objects of inter-
est. Alami et al. [21] tries to find the most distinctive region 
between the source and target domains to generate an atten-
tion map. Then, this map is applied to the generator input to 
constrain it to the relevant image region.
Instead of using an extra network to obtain attention 
masks, Tang et al. [23] proposes a new attention method, 
which disentangles the input image into foreground and 
background by generating multiple foreground attention 
masks, one background attention mask, and multiple content 
masks. Then, pay attention to the critical parts of the image, 
and convert the prominent object foreground to the target 
area while keeping everything else unchanged. In this paper, 
(1)
Attention = f(g(x), x),
we use a shared encoder, a content decoder, and a mask 
decoder to obtain a foreground attention mask, a background 
attention mask, and a content mask for the input image. In 
this way, our model can better focus on the essential areas of 
the object and improve the quality of the generated image.
3  Method
Figure 3a shows the overall architecture of our model, which 
is designed in a coarse-to-fine manner. Specifically, we learn 
the mapping relationship between domain X and domain 
Y and use only two unpaired images x ∈X and y ∈Y . In 
training, we downsample the input image x and y to n dif-
ferent scales and then start from the low scale to translate 
the image using a coarse-to-fine manner. In each scale, we 
adopt an encoder–decoder architecture for the attention-
guided generator. We divide the generator into an encoder 
module and a decoder module. The decoder module includes 
a content mask generator, an attention mask generator, and 
a Scale-Add (SA) module.
3.1  Model
Our goal is to translate x to ̂y ∈Y and y to ̂x ∈X . On the n-th 
scale, the translation process from x to ̂y is shown in Fig. 4. 
The process is expressed as follows:
Fig. 3   Overview of OSAGGAN (Here, we only show the architecture 
when translating from x to ̂y , and the architecture when translating 
from y to ̂x is the same). Our approach uses a cascade of generative 
adversarial networks within a pyramid framework to generate images 
in a coarse-to-fine manner. Each generator is an encoder–decoder 
architecture and contains our attention module. The discriminator 
takes the image obtained by x downsampling and the corresponding 
generated image as input

International Journal of Machine Learning and Cybernetics	
1 3
where Gn and Fn represent generators that convert from x to ̂y 
and from y to ̂x ; Gn
e and Fn
e are the parameter shared encoders 
of images xn and yn respectively; Gn
c and Fn
c are the content 
mask generators of images xn and yn respectively, Gn
a and Fn
a 
are the attention mask generators of images xn and yn respec-
tively; gn(xn) and f n(yn) are the generated images under the 
current scale, and Θ1(xn) and Θ2(yn) are the modules that add 
low-scale detail features to the generated images.
3.1.1  Generator
Attention model: Here, we collectively call an encoder 
with shared parameters, a content mask generator, and an 
attention mask generator as the attention module. The input 
of each attention module is a three-channel image, and 
the output of the attention model is a foreground attention 
mask, a background attention mask, and a content mask. 
Specifically, the input image is xn ∈RH×R×3 . The output is 
(2)
Gn ∶xn →Gn
e →[Gn
c, Gn
a] →gn(x) →Θn
1(x),
(3)
Fn ∶yn →Fn
e →[Fn
c, Fn
a] →f n(y) →Θn
2(y),
a foreground attention mask An
1(xn) ∈{0, ..., 1}H×R , a back-
ground attention mask An
2(xn) ∈{0, ..., 1}H×R , and a content 
mask Cn(xn) ∈RH×R×3 . This process can be described as,
where m is the feature map obtained by the shared encoder, 
conv() is a convolution operation.
For the convenience of subsequent operations, we copy 
the foreground mask An
1(xn) and background mask An
2(xn) to 
the three-channels respectively. The attention masks An
1(xn) 
and An
2(xn) define the intensity of each pixel and specify 
the contribution of each pixel of the content masks Cn(xn) 
to the final rendered image. In this way, the generator does 
not need to render the background but can only focus on 
the pixels that define the content movement of the domain, 
resulting in a clearer and more realistic composite image. 
Then, we fuse the input image xn , the generated attention 
masks An
1(xn) , An
2(xn) and the content mask Cn(xn) to obtain 
the generated image gn(xn):
(4)
Cn(xn) = Tanh(conv(m)),
(5)
An(xn) = Softmax(conv(m)),
for An
1(xn)andAn
2(xn)
∈An(xn)
Fig. 4   Generator structure in nth scale

	
International Journal of Machine Learning and Cybernetics
1 3
where ⊗ represents pixel-wise multiplication. In this way, 
we can focus on the most unique semantic part of the image.
Scale-Add module: The attention module generates an 
initial translation result gn(xn) , but the color is still very dif-
ferent from the target image (shown in Fig. 4). The Scale-
Add module can not only retain the current scale informa-
tion but also add some detailed information and play an 
important role in adjusting the image color. Precisely, the 
generated image obtained by the attention module is super-
imposed with the generated image of the previous scale, 
and the final translation result is generated by the Scale-Add 
module:
where concat() denotes the concatenation operation; conv() 
denotes the convolution operation.
3.1.2  Discrimtor
Previous studies found that the discriminator and genera-
tor have the same receptive field, which is very helpful in 
improving the quality of the generated image. Therefore, 
we use PatchGAN [36, 37] to ensure they have the same 
receptive field of 11*11 as the generator. As we can see in 
Fig. 3b, the discriminator consists of four 3*3 Conv-Batch-
Norm-LeakyRelu [38] blocks, a convolution block, and a 
Sigmod activation function.
3.2  Loss
The whole objective of our model comprises five loss func-
tions. Instead of using the vanilla GAN loss, we used the 
WGAN-GP [39] loss for stable training.
Adversarial loss. In the nth scale, there are two discrimi-
nators: Dn
1 , Dn
2 and two generators: Fn , Gn , which have the 
same network structure but different weight parameters. An 
adversarial loss is employed to match the distribution of the 
translated images to the target image distribution:
where we set 휆pen = 0.01 empirically. In addition, xn and 
yn represent the source domain and target domain images 
(6)
gn(xn) = An
1(xn) ⊗Cn(xn) + An
2(xn) ⊗xn,
(7)
Θn
1(x) = Tanh(conv(cancat(gn(xn), ̂yn−1))),
(8)
Ln
advx = Dn
1(xn) −Dn
1(Fn(yn, pn
2) −휆pen
(‖‖‖▽̂xnDn
1( ̂xn)‖‖‖2 −1
)2
,
(9)
Ln
advy = Dn
2(yn) −Dn
2(Gn(xn, pn
1) −휆pen
(‖‖‖▽̂ynDn
2( ̂yn)‖‖‖2 −1
)2
,
on the n-th scale respectively. pn
2 and pn
1 represent the out-
put result of the previous scale with xn−1 and yn−1 as inputs 
respectively.
Cycle loss. According to our proposed model, we need to 
push the generated image back to the source domain. To alle-
viate the mode collapse problem, we apply a cycle loss [5] to 
the generator:
where the reconstructed image Fn(Gn(xn, pn
1), cn
2) should be 
very close to the original one xn and the reconstructed image 
Gn(Fn(yn, pn
2), cn
1) should be very close to the original one yn . 
In addition, cn
2 and cn
1 represent the reconstructed result of 
the previous scale with yn−1 and xn−1 as inputs respectively.
Identity loss. We use identity loss to preserve the consist-
ency of the color distribution of the input image and the output 
image:
Attention loss. When training OSAGGAN, we don’t have 
a real annotation of the attention mask. The learning of 
the attention mask completely depends on the autonomous 
learning of the model, which leads to the saturation of the 
attention mask. Inspired by Mahendran et al. [40], we pro-
pose an attention loss:
where W represents image width and H represents image 
height, w ∈W , h ∈H . Let An
x
[i, j] denote the pixel located 
in the ith row and jth column of attention mask An
x , others 
are similar.
Content loss. In order to make the generated image 
smoother and reduce artifacts and noise, we propose a con-
tent loss:
(10)
Ln
cycx = ‖‖xn −Fn(Gn(xn, pn
1), cn
2)‖‖1,
(11)
Ln
cycx = ‖‖xn −Gn(Fn(yn, pn
2), cn
1)‖‖1,
(12)
Ln
recx = ‖‖xn −Fn(xn, zn
1)‖‖1,
(13)
Ln
recy = ‖‖yn −Gn(yn, zn
2)‖‖1,
(14)
Ln
attx =
W,H
∑
w,h=1
|An
x(w + 1, h, c) −An
x(w, h, c)|
+ |An
x(w, h + 1, c) −An
x(w, h, c)|,
(15)
Ln
atty =
W,H
∑
w,h=1
|An
y(w + 1, h, c) −An
y(w, h, c)|
+ |An
y(w, h + 1, c) −An
y(w, h, c)|,

International Journal of Machine Learning and Cybernetics	
1 3
At the nth scale, the optimization goal of our attention-
guided GAN can be expressed as:
where 휆1 , 휆2 , 휆3 , 휆4 , 휆5 are weight parameters. We set 휆1 = 1 , 
휆2 = 1 , 휆3 = 1 , 휆5 = 1 empirically and set 휆4 = 1 −rate . 
When the number of iterations is less than 1600, rate = 0.5 , 
otherwise rate = 0.01.
4  Experiments
Here, we compare our method with all baselines on different 
datasets. Further, we compare our algorithm with all base-
lines using the SIFID [41] score and UP score, showing that 
the performance of our method is better or on par with these 
methods. Finally, we verify the effectiveness and rationality 
of the attention module and Scale-Add(SA) module through 
ablation experiments. The training details, datasets, and 
evaluation metrics, along with all baselines and experiment 
results, are described as follows.
4.1  Experiments settings
Implement details. We mostly follow the setting of Tui-
GAN [19] to train our model. In this experiment, we use 
WGAN-GP [39] loss. More specifically, we use the Adam 
[42] optimiser with ltg = 0.0001 , lrd = 0.0004 . We use five 
different scales of images for training and our model requires 
5 h on a single 1080-Ti GPU with a pair of images.
Datasets. Because this paper proposes a one-shot transla-
tion from single-image to single-image, we randomly sample 
an image from the source domain and the other from the 
target domain and train in the two images. 
(16)
Ln
pixelx =
W,H
∑
w,h=1
|(Fn(yn, pn
2))(w + 1, h, c)
−(Fn(yn, pn
2))(w, h, c)|
+ |(Fn(yn, pn
2))(w, h + 1, c) −(Fn(yn, pn
2))(w, h, c)|,
(17)
Ln
pixely =
W,H
∑
w,h=1
|(Gn(xn, pn
1))(w + 1, h, c)
−(Gn(xn, pn
1))(w, h, c)|
+ |(Gn(xn, pn
1))(w, h + 1, c) −(Gn(xn, pn
1))(w, h, c)|,
(18)
Ln
total =휆1(Ln
advx + Ln
advy) + 휆2(Ln
cycx + Ln
cycy)
+ 휆3(Ln
recx + Ln
recy)
+ 휆4(Ln
attx + Ln
atty) + 휆5(Ln
pixelx + Ln
pixely),
1.	 Horse ↔ Zebra is introduced in CycleGAN [5], it con-
tains 1067 horse images, 1344 zebra images as the train-
ing set and 260 test images all collected from ImageNet.
2.	 Label ↔ Facade contains 400 paired training images and 
106 paired test images from the CMP Facade Database 
[43].
3.	 Map ↔ Aerial-Photo is also introduced in CycleGAN, 
it contains 1096 paired training images and 1098 paired 
test images.
Baselines. We compare our model with five baselines. 
1.	 CycleGAN [5], which leverages the full datasets without 
subsampling.
2.	 AttentionGAN [23], using full datasets and sampling 
images to 256*256
3.	 FUNIT [33], which is an unsupervised image-to-image 
method that requires a large number of images for train-
ing.
4.	 OST [34], which is an unsupervised image-to-image 
method that requires one image from the source domain 
and a set of images in the target domain for training.
5.	 TuiGAN [19], a pyramid conditional generation model, 
is trained on only two unpaired images.
6.	 SinCUT [35] adopts the model of comparative learning, 
does not use the traditional cycle-consistency loss, and 
only uses a pair of unpaired images for training.
Evaluation. Currently, there are no recognized evaluation 
metrics in single-image to single-image translation. In this 
paper, we use the SIFID [41] score proposed by SinGAN 
[41] to judge the difference in internal distribution between 
two images. We compute the SIFID score between translated 
image and the corresponding target image. In addition, we 
also use user preference to evaluate the quality of the gener-
ated image.
4.2  Visual results
The visible results of our proposed model and baselines are 
shown in Figs. 5, 6, and 7. Overall, FUNIT [33] has serious 
color difference problems, and the style does not match the 
target domain. the generated results of OST [34] contain 
many noticeable artifacts. Our method can produce more 
competitive results compared with CycleGAN [5] and Atten-
tionGAN [23]. For example, in Fig. 6, they cannot trans-
late the background of the image simultaneously, and the 
generated image’s zebra texture on the last line is partially 
missing. In Fig. 7, they fail to capture the distribution of the 
target domain, and the images translated by AttentionGAN 
are visually poor.
Our method can achieve comparable results compared 
with SinCUT [35] and TuiGAN [19] trained on only two 

	
International Journal of Machine Learning and Cybernetics
1 3
unpaired images. As we can see in Fig. 5, to some extent, 
SinCUT and TuiGAN can’t translate the corresponding 
semantic information (e.g., the first and second lines of 
SinCUT, the third and fourth lines of TuiGAN). In Fig. 6, 
Although SinCUT can effectively translate the background, 
it cannot successfully translate zebra texture information 
in the horse-to-zebra task. The image generated in Tui-
GAN has some strange artifacts, and the contour is not 
clear enough. In addition, SinCUT and TuiGAN do not 
achieve good translation results in Fig. 7. Compared with 
Fig. 5   Some results from different methods on Map ↔ Aerial-Photo 
datasets. We compare our method with CycleGAN (pre-trained), 
AttentionGAN (pre-trained), FUNIT (pre-trained), OST(trained 
with one image from the source domain and a set of images in the 
target domain), SinCUT(trained with two unpaired images) and 
TuiGAN(trained with two unpaired images)
Fig. 6   Some results from different methods on Horse ↔ Zebra 
datasets. We compare our method with CycleGAN (pre-trained), 
AttentionGAN (pre-trained), FUNIT (pre-trained), OST(trained 
with one image from the source domain and a set of images in the 
target domain), SinCUT(trained with two unpaired images) and 
TuiGAN(trained with two unpaired images)

International Journal of Machine Learning and Cybernetics	
1 3
the two types of baselines, it can be effectively explained 
that our method can capture the domain distribution of 
only two unpaired images.
4.3  Qualitative and quantitative study
The results of the average SIFID [41] score and the UP score 
are shown in Tables 1 and 2. We conduct a user study to 
evaluate the images generated in all models. Specifically, we 
randomly select 50 participants (Both men and women have 
received higher education and are over 18 years old.) for 
the experiment, show them different methods of generated 
images, source and target images, and ask them to choose 
the best image generated for the target domain.
We can see that our method obtains better SIFID score 
on all baselines, which shows that our method can capture 
the distribution of images in the target domain well. In 
Table 1, our method has similar user preferences compared 
with CycleGAN [5] and AttentionGAN [23]. In Table 2, our 
method gets a better user research score compared with the 
few sample and single sample methods.
4.4  Ablution study
We conduct a comprehensive ablation study to validate the 
importance and validity of the proposed attention model and 
the Scale-Add(SA) model. As mentioned earlier, we set up 
end-to-end baselines by discarding the attention module and 
the Scale-Add(SA) module Θ . Specifically, 
Fig. 7   Some results from different methods on Label ↔ Facade data-
sets. We compare our method with CycleGAN (pre-trained), Atten-
tionGAN (trained with full training dataset), FUNIT (pre-trained), 
OST(trained with one image from the source domain and a set of 
images in the target domain), SinCUT(trained with two unpaired 
images) and TuiGAN(trained with two unpaired images)
Table 1   The performance of the 
method compared with that of 
the training method with many 
unpaired images
Bold indicates minimum value. The lower the SIFID indicator value, the better the quality of the generated 
image
The percentage in the column of UP indicates the proportion of images that users prefer in the current 
method compared with our method
Metrics
Cycle GAN
Attention GAN
FUNIT
Our
mSIFID (∗10−2)
0.0275026
0.0425119
1484.309721
0.00225767
UP
57.4%
47.3%
1.1%
–
Table 2   Performance comparison between the proposed method and 
the few sample and single sample methods
Bold indicates minimum value. The lower the SIFID indicator value, 
the better the quality of the generated image
Metrics
OST
SinCUT​
TuiGAN
Our
mSIFID (∗10−2)
0.0233217
0.019964
0.00359426
0.00225767
UP
16.7%
44.4%
38.7%
–

	
International Journal of Machine Learning and Cybernetics
1 3
1.	 We delete the Scale-Add(SA) model Θ . To fairly com-
pare the SA model with the other baselines, we replace 
the SA module with a simple linear addition model 
(briefly denoted as w/o Θ).
2.	 We remove the attention model from the attention-
guided generator (briefly denoted as w/o A).
The qualitative results are shown in Fig. 8. When the 
generator has no attention module, the generated image 
will have varying degrees of missing content and strange 
artifacts. When the generator does not have SA module 
Θ , the background color seems very different from the 
target image, and some inaccurate colors also appear in 
the generated zebra.
We use the SIFID [41] score to quantitatively compare 
the results of ablation studies under different architecture 
settings. In Table 3, our method produces better perfor-
mance results, which shows that our model can generate 
images with the same style as the target image while pre-
serving the image’s content to the greatest extent, reveal-
ing the rationality and importance of the module design.
To validate the validity of the proposed attention mod-
ule, we conduct a comprehensive ablation study. We set up 
end-to-end baselines by using different attention models. 
Specifically, 
1.	 We use TuiGAN [19] to do the contrast experiment. Tui-
GAN uses the method of attention to fixing the details 
when processing the different scale images.
2.	 SEnet [44] proposes an attention method that imposes 
different attention values on channels. Inspired by this, 
we designed a structure to match the model in this paper. 
The input image passes through a shared encoder and 
reaches the corresponding content decoder and attention 
decoder. The attention decoder obtains a three-channel 
mask, giving each channel different attention weights 
(briefly denoted as A1).
3.	 AGGAN [22] proposes a new built-in attention module, 
which generates an attention mask and a content mask 
from the input image through the generator of shared 
parameters. We use this attention method to replace the 
attention module of this model to form an end-to-end 
baseline (briefly denoted as A2).
The ablation results of different attention methods are shown 
in Fig. 8. The results generated by TuiGAN [19] show some 
strange artifacts, and the legs of the generated horse are not 
as complete as our model. A1 also produces strange artifacts 
in the background. Although the content of the image gener-
ated by A2 is relatively complete, some details are not well 
preserved, such as the generated part of the horse leg is not 
clear enough, and there are some blurring phenomena.
We also apply the SIFID [41] score to perform the per-
formance comparison and find that our method achieves 
better performance in Table 4, indicating the rationality of 
our approach.
5  Conclusion
This paper proposes OSAGGAN to realize the translation 
process from single-image to single-image. Our approach 
uses a cascade of generative adversarial networks within a 
Fig. 8   Visual results of ablation study
Table 3   Quantitative comparison between different architectures 
under the same experiment settings, w/o indicates the absence of this 
module
Bold indicates minimum value. The lower the SIFID indicator value, 
the better the quality of the generated image
For this metric, lower is better
task
Metrics
Our
w/o A
w/o Θ
Zebra → 
Horse
SIFID 
(∗10−4)
0.007957347
0.021427593
0.43797347
Horse → 
Zebra
SIFID 
(∗10−4)
0.33847253
0.59240545
0.36621102

International Journal of Machine Learning and Cybernetics	
1 3
pyramid framework to generate images in a coarse-to-fine 
manner. In addition, an attention-guided generator is intro-
duced to learn which features should be paid more atten-
tion to. Finally, we combine the generated image with the 
output image of the previous scale to make up for the miss-
ing details of the current generated image. Experiments on 
several datasets show the effectiveness of this method. At 
the same time, the rationality of the model is verified by 
ablation experiments.
Acknowledgements  This work was supported in part by the National 
Natural Science Foundation of China under grant 62072169, and Natu-
ral Science Foundation of Hunan Province under grant 2021JJ30138.
References
	 1.	 Li R, Pan J, Li Z, Tang J (2018) Single image dehazing via con-
ditional generative adversarial network. In: Proceedings of the 
IEEE conference on computer vision and pattern recognition, pp 
8202–8211
	 2.	 Chen J, Chen J, Chao H, Yang M (2018) Image blind denoising 
with generative adversarial network based noise modeling. In: 
Proceedings of the IEEE conference on computer vision and pat-
tern recognition, pp 3155–3164
	 3.	 Huang X, Belongie S (2017) Arbitrary style transfer in real-time 
with adaptive instance normalization. In: Proceedings of the IEEE 
international conference on computer vision, pp 1501–1510
	 4.	 Jing Y, Liu X, Ding Y, Wang X, Ding E, Song M, Wen S (2020) 
Dynamic instance normalization for arbitrary style transfer. In: 
Proceedings of the AAAI conference on artificial intelligence, vol 
34, pp 4369–4376
	 5.	 Zhu J-Y, Park T, Isola P, Efros A.A (2017) Unpaired image-to-
image translation using cycle-consistent adversarial networks. In: 
Proceedings of the IEEE international conference on computer 
vision, pp 2223–2232
	 6.	 Huang X, Liu M-Y, Belongie S, Kautz J (2018) Multimodal unsu-
pervised image-to-image translation. In: Proceedings of the Euro-
pean conference on computer vision (ECCV), pp 172–189
	 7.	 Lee H-Y, Tseng H-Y, Huang J-B, Singh M, Yang M-H (2018) 
Diverse image-to-image translation via disentangled representa-
tions. In: Proceedings of the European conference on computer 
vision (ECCV), pp 35–51
	 8.	 Park T, Efros AA, Zhang R, Zhu J-Y (2020) Contrastive learning 
for unpaired image-to-image translation. In: European conference 
on computer vision. Springer, pp 319–345
	 9.	 Benaim S, Wolf L (2017) One-sided unsupervised domain map-
ping. In: Advances in neural information processing systems, vol 
30
	10.	 Pathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA (2016) 
Context encoders: Feature learning by inpainting. In: Proceedings 
of the IEEE conference on computer vision and pattern recogni-
tion, pp 2536–2544
	11.	 Yuan Y, Liu S, Zhang J, Zhang Y, Dong C, Lin L (2018) Unsu-
pervised image super-resolution using cycle-in-cycle generative 
adversarial networks. In: Proceedings of the IEEE conference on 
computer vision and pattern recognition workshops, pp 701–710
	12.	 Kim J, Lee JK, Lee KM (2016) Accurate image super-resolution 
using very deep convolutional networks. In: Proceedings of the 
IEEE conference on computer vision and pattern recognition, pp 
1646–1654
	13.	 Long J, Shelhamer E, Darrell T (2015) Fully convolutional net-
works for semantic segmentation. In: Proceedings of the IEEE 
conference on computer vision and pattern recognition, pp 
3431–3440
	14.	 Isola P, Zhu J-Y, Zhou T, Efros AA (2017) Image-to-image trans-
lation with conditional adversarial networks. In: Proceedings of 
the IEEE conference on computer vision and pattern recognition, 
pp 1125–1134
	15.	 Kim T, Cha M, Kim H, Lee JK, Kim J (2017) Learning to dis-
cover cross-domain relations with generative adversarial net-
works. In: International conference on machine learning. PMLR, 
pp 1857–1865
	16.	 Yi Z, Zhang H, Tan P, Gong M (2017) Dualgan: Unsupervised 
dual learning for image-to-image translation. In: Proceedings 
of the IEEE international conference on computer vision, pp 
2849–2857
	17.	 Choi Y, Uh Y, Yoo J, Ha J-W (2020) Stargan v2: Diverse image 
synthesis for multiple domains. In: Proceedings of the IEEE/
CVF conference on computer vision and pattern recognition, pp 
8188–8197
	18.	 Liu M-Y, Breuel T, Kautz J (2017) Unsupervised image-to-image 
translation networks. In: Advances in neural information process-
ing systems, vol 30
	19.	 Lin J, Pang Y, Xia Y, Chen Z, Luo J (2020) Tuigan: learning 
versatile image-to-image translation with two unpaired images. 
In: European conference on computer vision. Springer, pp 18–35
	20.	 Chen X, Xu C, Yang X, Tao D (2018) Attention-gan for object 
transfiguration in wild images. In: Proceedings of the European 
conference on computer vision (ECCV), pp 164–180
	21.	 Alami Mejjati Y, Richardt C, Tompkin J, Cosker D, Kim KI 
(2018) Unsupervised attention-guided image-to-image transla-
tion. In: Advances in neural information processing systems, vol 
31
	22.	 Tang H, Xu D, Sebe N, Yan Y (2019) Attention-guided genera-
tive adversarial networks for unsupervised image-to-image trans-
lation. In: 2019 International joint conference on neural networks 
(IJCNN). IEEE, pp 1–8
	23.	 Tang H, Liu H, Xu D, Torr PH, Sebe N (2021) Attentiongan: 
unpaired image-to-image translation using attention-guided gen-
erative adversarial networks. IEEE Trans Neural Netw Learn Syst
	24.	 Kim J, Kim M, Kang H, Lee K (2019) U-gat-it: unsupervised 
generative attentional networks with adaptive layer-instance nor-
malization for image-to-image translation. arXiv preprint arXiv:​
1907.​10830
	25.	 Kingma DP, Welling M (2013) Auto-encoding variational bayes. 
arXiv preprint arXiv:​1312.​6114
Table 4   Performance comparison using different attention approaches on the model. For this metric, lower is better
Bold indicates minimum value. The lower the SIFID indicator value, the better the quality of the generated image
task
Metrics
Our
TuiGAN
A1
A2
Zebra → Horse
SIFID (∗10−4)
0.007957347
0.032546598
0.015259689
0.019330166
Horse → Zebra
SIFID (∗10−4)
0.33847253
0.9454131
0.5873978
0.50720664

	
International Journal of Machine Learning and Cybernetics
1 3
	26.	 Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley 
D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial 
nets. In: Advances in neural information processing systems, vol 
27
	27.	 Hu T, Long C, Xiao C (2021) A novel visual representation on text 
using diverse conditional gan for visual recognition. IEEE Trans 
Image Process 30:3499–3512
	28.	 Islam A, Long C, Basharat A, Hoogs A (2020) Doa-gan: dual-
order attentive generative adversarial network for image copy-
move forgery detection and localization. In: Proceedings of the 
IEEE/CVF conference on computer vision and pattern recogni-
tion, pp 4676–4685
	29.	 Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville AC 
(2017) Improved training of wasserstein gans. In: Advances in 
neural information processing systems, vol 30
	30.	 Wang T-C, Liu M-Y, Zhu J-Y, Tao A, Kautz J, Catanzaro B (2018) 
High-resolution image synthesis and semantic manipulation with 
conditional gans. In: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp 8798–8807
	31.	 Lee H-Y, Tseng H-Y, Huang J-B, Singh M, Yang M-H (2018) 
Diverse image-to-image translation via disentangled representa-
tions. In: Proceedings of the European Conference on Computer 
Vision (ECCV), pp. 35–51
	32.	 Huang X, Liu M-Y, Belongie S, Kautz J (2018) Multimodal unsu-
pervised image-to-image translation. In: Proceedings of the Euro-
pean conference on computer vision (ECCV), pp 172–189
	33.	 Liu M-Y, Huang X, Mallya A, Karras T, Aila T, Lehtinen J, Kautz 
J (2019) Few-shot unsupervised image-to-image translation. In: 
Proceedings of the IEEE/CVF international conference on com-
puter vision, pp 10551–10560
	34.	 Benaim S, Wolf L (2018) One-shot unsupervised cross domain 
translation. In: Advances in neural information processing sys-
tems, vol 31
	35.	 Park T, Efros AA, Zhang R, Zhu J-Y (2020) Contrastive learning 
for unpaired image-to-image translation. In: European conference 
on computer vision. Springer, pp 319–345
	36.	 Li C, Wand M (2016) Precomputed real-time texture synthesis 
with Markovian generative adversarial networks. In: European 
conference on computer vision. Springer, pp 702–716
	37.	 Mathieu M, Couprie C, Le Cun Y (2015) Deep multi-scale video 
prediction beyond mean square error. arXiv preprint arXiv:​1511.​
05440
	38.	 Ioffe S, Szegedy C (2015) Batch normalization: accelerating deep 
network training by reducing internal covariate shift. In: Interna-
tional conference on machine learning. PMLR, pp 448–456
	39.	 Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville AC 
(2017) Improved training of Wasserstein gans. In: Advances in 
neural information processing systems, vol 30
	40.	 Mahendran A, Vedaldi A (2015) Understanding deep image repre-
sentations by inverting them. In: Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, pp 5188–5196
	41.	 Shaham TR, Dekel T, Michaeli T (2019) Singan: Learning a gen-
erative model from a single natural image. In: Proceedings of 
the IEEE/CVF international conference on computer vision, pp 
4570–4580
	42.	 Chen C, Carlson D, Gan Z, Li C, Carin L (2016) Bridging the gap 
between stochastic gradient mcmc and stochastic optimization. In: 
Artificial intelligence and statistics. PMLR, pp 1051–1060
	43.	 Tylecek R (2012) The cmp facade database. Technical report. 
Research Report CTU-CMP-2012-24, Czech Technical University
	44.	 Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. 
In: Proceedings of the IEEE conference on computer vision and 
pattern recognition, pp 7132–7141
Publisher's Note  Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with the 
author(s) or other rightsholder(s); author self-archiving of the accepted 
manuscript version of this article is solely governed by the terms of 
such publishing agreement and applicable law.

