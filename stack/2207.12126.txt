PirouNet: Creating Dance through
Artist-Centric Deep Learning
Mathilde Papillon1[0000−0003−1674−4218], Mariel Pettee2[0000−0001−9208−3218],
and Nina Miolane3[0000−0002−1200−9024]
1 University of California Santa Barbara, Department of Physics, Santa Barbara,
CA, USA
2 Lawrence Berkeley National Lab, Berkeley, CA, USA
3 University of California Santa Barbara, Department of Electrical and Computer
Engineering, Santa Barbara, CA, USA
Abstract. Using Artiﬁcial Intelligence (AI) to create dance choreogra-
phy is still at an early stage. Methods that conditionally generate dance
sequences remain limited in their ability to follow choreographer-speciﬁc
creative direction, often relying on external prompts or supervised learn-
ing. In the same vein, fully annotated dance datasets are rare and labor
intensive. To ﬁll this gap and help leverage deep learning as a meaning-
ful tool for choreographers, we propose “PirouNet”, a semi-supervised
conditional recurrent variational autoencoder together with a dance la-
beling web application. PirouNet allows dance professionals to annotate
data with their own subjective creative labels and subsequently generate
new bouts of choreography based on their aesthetic criteria. Thanks to
the proposed semi-supervised approach, PirouNet only requires a small
portion of the dataset to be labeled, typically on the order of 1%. We
demonstrate PirouNet’s capabilities as it generates original choreography
based on the “Laban Time Eﬀort”, an established dance notion describ-
ing a given intention for a movement’s time dynamics. We extensively
evaluate PirouNet’s dance creations through a series of qualitative and
quantitative metrics, validating its applicability as a tool for choreogra-
phers.
Keywords: Deep learning · Neural network · Semi-supervised · Gener-
ative · Recurrent · LSTM · VAE · Pose · Laban
1
Introduction
Recent years have witnessed the development of deep learning methods that
can generate original dance sequences. Yet, these methods have not been widely
adopted by communities of dancers and choreographers. A possible reason for
this lack of adoption is the fact that existing AI-powered dance generation tools
cannot create dance based on artistic criteria, or “user-inputs” that are mean-
ingful to choreographers, such as elements of Laban Movement Analysis [8,17].
arXiv:2207.12126v2  [cs.LG]  14 Oct 2022

2
M. Papillon et al.
Laban Movement Analysis (LMA) [8,17] stands as one of the most established
Western methods for describing and understanding human motion [33], often
used by dance professionals to transmit choreography, teach technique [56,13],
and rigorously assess performance [2]. LMA’s corresponding notation, Laban-
otation (LN), enables detailed and objective recording of the quantitative and
qualitative characteristics of movement [22]. As a result, LMA has become a
cross-disciplinary staple of technologically-driven movement analysis and has
been extensively assessed for reliability and eﬀectiveness [5,26,35]. LMA now
provides a valuable tool for many movement-centric ﬁelds beyond dance, such
as physical rehabilitation [8], theater [58], sports [19], and psychology [51,40,44].
Laban Axes and Laban Eﬀorts. LMA is divided into four axes which indepen-
dently contribute to its portrayal of movement [12]. “Body” outlines actions of
body parts and relationships between those parts, “Shape” addresses the body’s
more general evolution of shape, “Space” describes the geometrical and direc-
tional emphasis of movement, and “Eﬀort” describes the energy content and
inner intention of a movement. In the context of this paper, we use the Eﬀort
axis as a leading example of aesthetic criterion, depicted in Fig. 1, for AI-powered
dance creation. We describe choreographic intention as the intended Eﬀort of a
movement. Eﬀort is a multi-dimensional axis of LMA, described by four inde-
pendent Laban Eﬀorts. It truly sets LMA apart, as it describes an intention of
movement, rather than the resulting execution [15,29].
Fig. 1: LMA’s Eﬀort axis consists of a four-dimensional space spanned by four
orthogonal eﬀorts called Laban Eﬀorts. For every movement, there exists a 4-
vector of total eﬀort. For example, a movement that is described as bound, light,
sustained, and direct has a total eﬀort comprised of Low Flow Eﬀort, Low Weight
Eﬀort, Low Time Eﬀort, and High Space Eﬀort.
PirouNet: Creating Dance from User-Deﬁned Labels. Given the lack of user-
tailored creative dance AIs, we propose “PirouNet,” a semi-supervised genera-
tive recurrent deep learning model that creates new dance sequences from chore-
ographers’ aesthetic inputs. We illustrate the use of PirouNet with categorical
intensities of the Laban Time Eﬀort, a staple of LMA’s descriptive richness. Fu-
ture users can choose diﬀerent Laban-inspired labels, or deﬁne any subjective

PirouNet: Creating Dance through Artist-Centric Deep Learning
3
label they wish to use for creation, regardless of their knowledge of LMA. The
artist then categorically labels a very small portion of an input dance dataset,
typically of the order of 1%. We provide a label augmentation tool that smartly
increases this portion to about 25%, in our case. Once trained, PirouNet creates
new dances prompted by the artist’s desired style.
Contributions. Our contributions are three-fold.
1. We introduce the ﬁrst deep generative model for dance based on LMA’s
Eﬀort axis.
2. We provide a novel dance labeling web application with a label augmentation
toolkit, which distinguishes itself from currently available customizable an-
notation databases [14] as it may be used on the user’s own motion dataset.
Fig. 2 shows an annotated screenshot of the app.
3. We present a semi-supervised approach that, coupled with our web app, (i)
allows the use of our model for dance generation based on any class of the
choreographer’s choosing and (ii) limits manual annotation labor to 1% of
the dance dataset.
Fig. 2: Screen capture of web labeling app. User enters amount of poses per dance
sequence in (1), and the index of the ﬁrst sequence’s starting pose in (2). Upon
clicking “Get Dance,” an animation of the fully connected skeleton appears in
(3). The user can zoom and rotate the animation directly, and click into speciﬁc
frames with (4). User enters Laban Eﬀort (or any chosen label) in (5) and clicks
“Save label” to record the inputs to a CSV ﬁle. A spot is reserved in (6) for an
infographic with instructions to ensure consistent labeling.

4
M. Papillon et al.
Outline. The paper is organized as follows. Section 2 describes related work cou-
pling LMA, deep learning and automatic generation of human motion and dance.
Section 3 introduces the architecture and semi-supervised training of the pro-
posed model, PirouNet. Section 4 presents experimental results on dance creation
from Laban Eﬀorts, validated with a series of qualitative and quantitative met-
rics that demonstrate PirouNet’s potential for AI-powered and choreographer-
controlled dance generation.
2
Related Works
We review related works on LMA and machine learning, as well as on the auto-
matic creation of human motion and dance.
Predicting LMA from videos or pose sequences. A ﬁrst class of learning methods
focuses exclusively on predicting LMA from input dance videos. Researchers have
successfully leveraged motion sensors [48] and pose detection systems [6,23] to
automatically infer LMA labels from dance sequences, which subsequently allows
assessing and improving movement techniques [47]. Recent advances in the ﬁeld
have extended the ability of these systems to generate the analyzed movement’s
corresponding LN using feed-forward time-series neural networks [53] and bi-
directional RNNs [61].
Predicting high-level dance information from LMA. Another class of methods
uses existing LMA annotations (e.g. produced from the ﬁrst class of methods)
as inputs to perform downstream tasks, demonstrating the richness of LMA
notations. For example, [2] can predict the similarity of two dancing motions
from LMA. Supervised deep learning using LMA as inputs has also successfully
extended computational movement analysis to emotion recognition [59], often
relying on the complex temporal learning processes oﬀered by Long Short Term
Memory networks (LSTMs) [20,54] and Convolutional Neural Networks (CNNs)
[50].
Creating dance from LMA: non-deep AI and algorithmic approaches. LMA has
played a key role in the emergence of a myriad of tools for generating dance
directly from written scores [57] or vice versa [11]. This approach’s machine
learning component mostly relies on associative memory [10] or look-up tables
[60], with an associated retrieval criterion. The subsequent adaptation of the
dance motion is either implemented through hard-coded dynamic constraints or
through a combination of such constraints and direct user-inputs. While such
methods testify of the interest in generating dance from LMA, they are rather
restrictive in the creative context of dance generation. Carlson et al. [9] make
use of a genetic algorithm to generate static dance poses from Laban Eﬀorts,
meant to catalyze choreographic creation.

PirouNet: Creating Dance through Artist-Centric Deep Learning
5
Creating dance from non-LMA inputs: deep learning approaches. Outside of the
LMA community, researchers have introduced deep learning tools into the chore-
ographic creative process. Most of these tools generate sequences of movement
using variants of Recurrent Neural Networks (RNNs) [43] to represent the time-
component of the dance dynamical system, and (variational) autoencoders [55]
to transform abstract latent variables into dance movements. A ﬁrst class of algo-
rithms generates choreography from scratch: the generated dance is completely
new movement [3,62], without using any input from the choreographer. Other
instances of innovative techniques achieving this include self-organizing maps
[34] and autoencoders for reacting to live movement [4]. Recent research has
also introduced variational autoencoders [42] that encode sequences towards and
generate sequences from a lower dimensional latent space. This enables genera-
tion of variations on given sequences as well as sampling new sequences. Lastly,
other methods prompt sequences with music [32,30,1,31], which a choreographer
can use to inﬂuence dance outputs. In all of these cases, the choreographer’s cre-
ative control over the movement output is either limited to the choice of training
data or that of an external prompt. To the best of our knowledge, there exists
no model able to create dance sequences from choreographer-speciﬁc aesthetic
labels or Laban Eﬀorts.
Creating movement from non-LMA inputs: conditional deep learning approaches.
Recent methods, called label-conditioned movement generations, create human
motion based on a discrete set of labels. To our knowledge, only two label-
conditioned motion generation systems exist in the literature: Action2Motion
[18] and Actor [41]. Neither are adaptable to LMA-based motion generation nor
to choreographer-guided dance generation, as they require a large, fully labeled
dataset. Both Action2Motion and Actor train their methods from motion capture
(MoCap) datasets, including the NTU dataset (containing over 100k movements)
[45] or the UESTC dataset (containing over 25k movements) [24] labeled with
human actions such as walking or throwing. Such labeled MoCap datasets are
imbalanced in categories or noisy (inspiring [18] to produce their own action
dataset) and do not consider dance moves nor LMA labels. Certain annotated
dance databases exist [52,25,14], although they are limited to their producers’
speciﬁc genres, styles, and creative processes. We postulate that the lack of
creative, yet precise, dance generation algorithms come from a lack of large
dance datasets with labels meaningful to choreography as an artistic practice.
Coming up with such a dataset for fully supervised models is especially diﬃcult
due to the time-consuming nature of such annotations.
3
Methods
This section presents the proposed conditional dance generative model and the
core components of PirouNet’s architecture: a dance encoding and generative
model that uses (i) a variational autoencoder (VAE) [55] inspired from [42],
coupled with (ii) motion dynamics through a LSTM [20] network. In order to

6
M. Papillon et al.
minimize the need for manual labeling, PirouNet also leverages (iii) a semi-
supervised learning approach [28]. PirouNet’s source code implementing this
model is available at github.com/bioshape-lab/pirounet. As PirouNet accepts
both labeled and unlabeled input data for training, its VAE adopts two diﬀerent
forms, showcased in Fig. 3. The VAE makes use of a linear classiﬁer for the case
of unlabeled data.
Fig. 3: PirouNet’s supervised and unsupervised training. A dance pose at time t
is represented by an input xt, and a dance sequence is represented by an array
of such poses, x = [x1, ..., xT ]. The last hidden variable hT in the LSTM output
is encoded into a continuous latent variable z. The decoder’s LSTM takes in
a hidden variable hdec repeated T times. The Laban Eﬀort y associated to a
pose xt is a categorical latent variable that represents the input label for that
particular sequence. There are k = 3 distinct possible classes for the label y.
Importantly, the VAE in the unlabeled case reconstructs every dance sequence
k times, using a diﬀerent label value each time.
3.1
Conditional Dance Generative Model
We propose the following conditional dance generative model:
pπ(y) = Cat(y | π);
p(z) = N(z | 0, I);
pθ(x | y, z; θ) = N(fθ(y, z), σ2), (1)
where z is a continuous latent variable representing the dynamics - i.e. which
movement is performed - and y is a categorical variable denoting the choreog-
rapher speciﬁc or LMA-based label - i.e. how the movement is performed. If
no label is available, y is treated as a latent variable. The observed variable x
represents a dance sequence. The prior on y is modeled as a multinomial distri-
bution (chosen as a uniform categorical distribution in practice) Cat(y | π) with
parameter π, while the prior on z is represented by N(z | 0, I), the multivari-
ate standard Gaussian following the VAEs’ classical prior. The latent variables
y, z are assumed to be marginally independent. The forward generative model
pθ(x | y, z) is a Gaussian distribution parameterized by a mean fθ(y, z) deﬁned

PirouNet: Creating Dance through Artist-Centric Deep Learning
7
as a non-linear transformation of the latent variables y, z, and a variance σ2.
In practice, the non-linear transformation fθ(y, z) is implemented as a neural
network with weights θ called the decoder fθ.
The decoder further implements the dance dynamics through a series of tem-
poral updates on the hidden variable ht:
h(t+1) = Dynθ(h(t), z, y),
t = 1, ..., T −1
(2)
producing T poses that are concatenated to produce a dance sequence. In prac-
tice, PirouNet uses LSTM layers to represent the dynamic operator Dynθ [20].
3.2
Semi-Supervised Conditional Variational Autoencoder
Based on the conditional dance generative model of Eq. (1), PirouNet solves
the inverse problem of inferring the latent variables of movement and intention
(z, y) from a dance sequence (x). PirouNet is built with a conditional variational
autoencoder (VAE) architecture [46,55], modiﬁed following the semi-supervised
approach proposed in [28] in order to include the (only partially observed) labels
of movement intentions, as shown in Fig. 3 and described below.
Encoder: Posterior of the movement latent variable z (labeled and unlabeled
cases.) Inference in (conditional) VAEs involves computing an approximate pos-
terior qφ for the continuous latent variable z, which writes:
(
qφ(z|x, y)
when a label y is available,
qφ(z|x) = Pk
y=1 qφ(z, y|x) = Pk
y=1 qφ(z|y, x) · qφ(y|x)
otherwise.
(3)
Both labeled and unlabeled cases in Eq. 3 require the computation of the dis-
tribution qφ(z|y, x), which is modeled as a multi-variate diagonal Gaussian dis-
tribution, as is standard in VAEs [55]. The role of the encoder gφ is to output
qφ(z|y, x). In the unlabeled case, the encoder is used k times (see Fig. 3): once for
each possible value of the label y within the sum deﬁning the unlabeled case in
Eq. 3. Speciﬁcally, the encoder gφ takes a sequence x and a label y as inputs and
outputs the mean µφ(x, y) and variance σ2
φ(x, y) of the amortized approximate
posterior qφ(z|x, y), written as:
qφ(z|x, y) = N(gφ(x, y)) = N(µφ(x, y), σ2
φ(x, y)).
(4)
Importantly, the encoder extracts this posterior from the dynamics of the input
sequence x, which is represented as:
h(t+1) = Dynφ(h(t), x, y),
t = 1, ..., T −1,
(5)
where the last hidden variable hT is fed to fully-connected layers to give µφ(x, y)
and σ2
φ(x, y). In practice, just as for the decoder, this is implemented with LSTM
layers [20].

8
M. Papillon et al.
Classiﬁer: Posterior of the intention latent variable y (unlabeled case). The un-
labeled case requires the additional computation of the posterior qφ(y|x) of the
categorical latent variable y in Eq.(3). This is modeled as a multinomial distribu-
tion, as is standard in multi-class classiﬁcation. Compared to a traditional VAE,
we use an additional encoder-classiﬁer (called classiﬁer, for simplicity) whose
role is to output qφ(y|x). Speciﬁcally, the classiﬁer takes a sequence x as input
and outputs the k probabilities of the k possible values for the label y in the
probability vector πφ(x), i.e:
qφ(y|x) = Cat (y | πφ(x)) .
(6)
In practice, the classiﬁer is implemented with fully-connected layers (Fig. 3).
Reparametrization trick. The VAE reparameterization trick is used to generate a
sample z from the approximate posterior qφ(z|y, x), i.e. an element of the VAE’s
latent space that corresponds to a low-dimensional representation of the dance
sequence x. We emphasize that z represents a dynamic sequence, and not a static
dance pose — which is crucial for the encoding of Laban Eﬀorts that intrinsically
characterize dynamics. Speciﬁcally, z should be understood as a parameter of
dynamics (e.g. describing the movement of a jump or spin), which is fed to the
generative model’s LSTM layers.
Decoder/Generator: Conditional Dance Generative Model. The VAE’s decoder
corresponds to the implementation of the conditional dance generative model
from Eq. (1). The decoder takes the encoded dance sequence z (sampled with
the reparametrization trick) and an assigned label y as inputs. During unsuper-
vised training, the decoder is used k times (see Fig. 3): once for each possible
value of the label y. The decoder then outputs a reconstructed dance sequence
ˆx = fθ(z, y). To generate original dance choreography, a new random latent vari-
able z is sampled from an approximation of the marginal distribution qφ(z|y) -
corresponding to the body motion - where the label y is chosen by the user.
3.3
Semi-Supervised Training and Loss Function
The training uses the semi-supervised framework and loss function prescribed in
[28] which considers the labeled and unlabeled cases separately.
Labeled Case. In the labeled case, our objective is to maximize the log-likelihood
log pθ(x, y) to learn the parameters θ of the conditional dance model. This is
traditionally performed via the maximization of its lower bound −L(x, y):
log pθ(x, y) ≥Eqφ(z|x,y)[log pθ(x | z, y) + log pπ(y)] −KL(p(z)∥qφ(z | x, y))
= −L(x, y),
where KL denotes the Kullback-Leibler (KL) divergence. In our case, the prior
pπ(y) is uniform over all labels, and the distribution pθ(x | z, y) is modeled as a
Gaussian. Thus, the quantity L(x, y) resembles a regularized L2 reconstruction
loss of a VAE with a modiﬁed KL divergence. The L2 reconstruction loss is
averaged over each body joint in a pose, and over each pose per sequence.

PirouNet: Creating Dance through Artist-Centric Deep Learning
9
Unlabeled Case. In the unlabeled case, y is missing and treated as another latent
variable, in addition to z, over which we perform posterior inference. In this case,
the objective is to maximize the marginalized log-likelihood log pθ(x), via the
maximization of its lower bound −U(x) written as:
log pθ(x) ≥Eqφ(y,z|x) [log pθ(x | y, z) + log pθ(y) + log p(z) −log qφ(y, z | x)]
=
k
X
y=1
qφ(y | x)(−L(x, y)) + H (qφ(y | x)) = −U(x),
where −H denotes a loss term participating to the entropy:
H(qφ(y | x)) = −
k
X
y=1
qφ(y | x) log qφ(y | x) = −
k
X
y=1
qφ(y | x)H (qφ(y | x)) .
(7)
The loss −H (qφ(y | x)) and the regularized reconstruction loss, L(x, y), are
now weighted by the conﬁdence associated to each label y, qφ(y | x).
Loss Function. The training uses the loss function computed from a lower bound
that encompasses the labeled and unlabeled cases:
loss =
X
(x,y)∼epl
L(x, y) +
X
x∼epu
U(x) + α · Eepl(x,y) [−log qφ(y | x)] ,
(8)
where ˜pl(x, y) and ˜pu(x) are the empirical distributions over the labeled and
unlabeled subsets, and the third term is a classiﬁcation loss weighted by the
hyperparameter α which controls the relative weight between dance generation
and classiﬁcation objectives [28].
4
Results
We qualitatively and quantitatively evaluate PirouNet for semi-supervised classi-
ﬁcation and generation of choreography. During experiments, we ﬁnd that these
two goals are best achieved with diﬀerent hyperparameters and levels of supervi-
sion. Therefore, we present a model primarily trained for the purpose of creating
novel and conditional dance (PirouNetdance), as well as a model that achieves
better classiﬁcation accuracy (PirouNetwatch).
Like most deep learning tools for dance, we leverage motion data in key-
point format. This format, often included in large movement datasets [52,45,36],
represents the body as a cloud of 3D points, each representing a unique joint.
Available and high-performing pose-estimation software [49,38,16] makes key-
point format accessible to smaller, homemade datasets as well. We use half of
Pettee’s keypoint dataset [42], featuring a trained contemporary dancer in solo
improvisation. We choose this particular style of dance for its predisposition to
LMA classiﬁcation, a tool developed for Western styles of dance. Future work is
needed to validate PirouNet for other forms of dance and annotations. In this
spirit, we propose a Dash [21] app, pictured in Fig. 2, for easy manual labeling
of any keypoint dataset.

10
M. Papillon et al.
4.1
Datasets
The dataset [42] is comprised of 36,396 poses extracted from six uninterrupted
dances captured at a rate of 35 frames per second. This amounts to about 20
minutes of real-time movement of an experienced dancer. Each pose features 53
joints captured in three dimensions, normalized such that the dance ﬁts within
inside a unit box. The dancer’s barycenter is ﬁxed to one point on the (x,y)
plane. From the pose data, we extract 36,356 sliding sequences of 40 continuous
poses, and manually label 350 of these sequences (0.96% of the dataset) which
do not share any of the same poses. This takes an experienced dancer (the
principal author) about 3 hours, identifying if the movement’s Laban Time Eﬀort
is Low, Medium, or High. We apply two automated techniques to augment this
labeling to 9,167 labeled sequences (representing 25.2% of our unlabeled dataset)
in total, split between 45% Low, 34% Medium, and 21% High Eﬀorts. (i) We
automatically label all sequences between sequences that share a same Eﬀort.
For example, if two back-to-back sequences are deemed to have Low Time Eﬀort,
all sequences that are a combination of the poses in these two sequences are also
labeled with Low Time eﬀort. (ii) We extend every label to all sequences starting
within 6 frames (0.17 seconds) before or after its respective sequence.
4.2
Training
All experiments are built using the PyTorch library [39] and run on a server
with two Nvidia A30 GPUs and two CPUs, each with 16 cores. We train using
an Adam optimizer with standard hyperparameters [27]. We present results for
the PirouNet architecture resulting from a hyperparameter search using Wandb
[7] on batch size, learning rate, number of LSTM and dense layers, as well as
hidden variable sizes. PirouNet uses 5 LSTM layers with 100 nodes in both the
encoder and the decoder. The classiﬁer features 2 ReLU-activated [37] linear lay-
ers with 100 nodes. The latent space is 256-dimensional, which is approximately
25 times smaller than the 6360-dimensional initial space. We train for 500 epochs
with a learning rate of 3e−4 and a batch size of 80 sequences. We select diﬀer-
ent epochs for PirouNetdance and PirouNetwatch to minimize validation loss. For
unsupervised training, we use 35,538 40-pose sequences, with the remaining se-
quences being reserved for testing. For supervised training, PirouNetdance and
PirouNetwatch are trained on 79% (16.6% of entire training set) and 92% (18.8%
of entire training set) of the labeled sequences, respectively. We reserve 5% of
the labeled sequences for validation, and 3% for testing.
4.3
Semi-supervised Classiﬁcation of Laban Eﬀorts
While the primary purpose of this work is to generate Eﬀort-speciﬁc dance,
we examine classiﬁcation performance to investigate training: we evaluate the
accuracy of the classiﬁer in attributing the correct categorical Time Eﬀort to a
dance. Because evaluating Laban eﬀorts is subjective, the ground-truth labels
provided by the human labeler may be considered “noisy”. To account for this

PirouNet: Creating Dance through Artist-Centric Deep Learning
11
in our evaluation of PirouNetwatch, the human labeler also evaluates their own
self-accuracy by relabeling the validation and entire datasets (Fig. 4(b, c)).
When classifying the validation set (Fig. 4a), PirouNetwatch succeeds with
50.1% accuracy, which represents 75% of the labeler’s self-accuracy. In the case
of test data, PirouNetwatch is 72% as accurate as the labeler. This is a satis-
factory performance, considering the subjective nature of the task, the limited
access to labeled data, and the observation that classifying the validation set
was challenging even for the human labeler.
0.64
0.18
0.18
0.49 0.17
0.34
0.37
0.31 0.32
Low
Low
Med
Med
High
High
Ground truth
a. PirouNet predicts 
on valid dataset
b.  Labeler re-predicts
      on valid dataset
c.  Labeler re-predicts
      on entire dataset
0.0
0.2
0.4
0.6
0.8
1.0
0.77
0.01
0.22
0.77 0.11
0.12
0.75
0.01 0.24
0.72
0.00
0.28
0.70 0.20
0.10
0.00
0.00 1.00
Fig. 4: Confusion matrices for classiﬁcation of validation dance sequences’ Time
eﬀort as being “Low”, “Medium”, or “High”. a. PirouNetwatch classiﬁes the val-
idation dataset. b. The labeler reclassiﬁes a comparable validation dataset. c.
The labeler reclassiﬁes a comparable entire dataset (train, validation, and test).
We hypothesize that results could be further improved by increasing the
training dataset size, as well as optimizing the trade-oﬀbetween training the
VAE and the classiﬁer. This would include examining the impact of the loss’s
control term α during later training. A hyperparameter search limited to early
training shows α = 0.1· nunlabeled
nlabeled
is optimal, where nunlabeled and nlabeled are the
unlabeled and labeled dataset sizes. Another potential solution would be switch-
ing to a regression setting. Using continuous scalar variables as labels would
better reﬂect the continuous nature of the Eﬀort spectrum and be better suited
to deal with noise between Low and High Eﬀorts. Fig 4b shows an example of how
the discontinuous categories can result in strong self-disagreement for humans,
which aﬀects the performance of the classiﬁer on those same sequences (Fig 4a).
While selected for classiﬁcation, PirouNetwatch’s generation performance is also
satisfactory.
It is worth noting that the ground truth and relabeled sets of data are not
exactly identical in labeling procedure: while the dance sequences are the same, a
sequence that was manually classiﬁed in one dataset may have been classiﬁed via
label augmentation in the other. We do not expect this to signiﬁcantly impact
training, as the augmentation procedure was designed to agree with manual
labeling. However, this signiﬁes that a misclassiﬁcation of a small number of
sequences by the labeler can propagate through augmentation, and result in
100% misclassiﬁcation for a given label, as is the case in Fig. 4b. We note that
unlike PirouNetwatch, the labeler almost never re-predicts a sequence to be more
than one categorical bin away from the ground truth. This highlights the use of

12
M. Papillon et al.
regression classiﬁcation as a means of preserving information about the labels’
meanings. We leave the improvements discussed above for future iterations.
4.4
Reconstructions of Choreographies
Qualitative assessment. Artifacts obtained on validation and test datasets, two
of which are depicted in Fig. 5, show that PirouNetdance successfully encodes
and decodes a given sequence with very little variation. Reconstructions capture
complex movements that include rotations (“pirouettes”), changes in height,
and changes in velocity. The Laban Time Eﬀort is preserved, as indicated by the
identical time evolution of the reconstructions.
Fig. 5: Reconstructions of dance sequences. a. Sequence extracted from test data
featuring a fast pirouette. b. Sequence extracted from validation data featuring
a multi-axial rotation on the ﬂoor with gradual leg and arm extension.
Quantitative assessment. To assess PirouNetdance’s ability to reconstruct move-
ment, we use the root mean squared error representing the average joint distance
(AJD) between original and reconstructed sequences, pose per pose. For N se-
quences with T = 40 poses of J = 53 joints each, the AJD writes as:
¯D =
1
N · T · J
N
X
n=1
T
X
t=1
J
X
j=1
qxt,n
j
−ˆxt,n
j
2 +
yt,n
j
−ˆyt,n
j
2 +
zt,n
j
−ˆzt,n
j
2.
(9)
For the validation dataset (N = 455), the average joint diﬀerence ¯Dvalidation
is 2.8e−2. On the test dataset (N = 249), the average diﬀerence is ¯Dtest = 1.8e−2,
which, remarkably, matches ¯Dtrain (N = 7, 161). Since the joint coordinates are
scaled to ﬁt inside a unit box, ¯Dvalidation and ¯Dtest represent less than 2.8%
and 1.8% of the dancer’s height, 4.8 cm and 3.1 cm. While trained for clas-
siﬁcation purposes, PirouNetwatch has a satisfactory reconstruction error, with
¯Dvalidation = 3.6e−2 and ¯Dtest = 2.0e−2.
4.5
Creation of Choreographies based on Laban Eﬀorts
We propose conditionally generating dance by sampling from an approximation
of the marginal conditional distribution qφ(z|y) of the latent variable z, given an

PirouNet: Creating Dance through Artist-Centric Deep Learning
13
input Laban eﬀort y. Eﬀectively, this means sampling dance from neighborhoods
in the latent space featuring a high density of previously encoded same-Eﬀort
sequences (manually or automatically labeled). Future work will focus on disen-
tangling the latent space in order to enable conditional generation independently
from previously encoded data. Outside of the conditional framework, PirouNet
can generate dance in the general style of the training data by sampling random
latent variables.
Qualitative assessment. For each of the three labels (Low/Medium/High Eﬀort),
we conditionally generate 75 sequences from PirouNetdance’s marginal distribu-
tion. The assessment of this 225-sequence benchmark set is two-fold.
First, we examine the overall “danceability” of the generated material. We
determine danceable material as movement that (i) would be physically realiz-
able by the training set’s dancer in at least one physical environment and (ii) is
largely continuous in space and time. Within this broad category, we ﬁnd three
sub-categories, outlined in Fig. 6. Most of the danceable sequences belong to at
least two of these sub-categories, which shows that PirouNetdance has learned to
produce danceable materials.
Could happen in a 
dance studio
Continuity in space 
and time
Novel movement
Respects physical limits 
of the body
Poses follow 
continuously.  
Body parts are spatially 
invariant.
Respects gravity.  
Implies friction on 
the ground.  
Occurs on a leveled 
ﬂoor.
Not a reconstruction 
of training data.  
Not completely static.
Transgresses physical 
limits of the body
Limbs cross through each 
other.  
Articulations bend in 
impossible directions.  
Poses are highly 
discontinuous.  
Limb size is spatially 
variant.
 
D
a
n
c
e
a
b
l
e
 
m
a
t
e
ri
a
l
U
n
d
a
n
c
e
a
b
l
e
 
m
a
t
e
r
i
a
l
Fig. 6: Infographic of danceable versus non-danceable material, as generated by
PirouNetdance.
Second, we examine PirouNetdance’s display of Time Eﬀort in the dance-
able material. Fig. 7 depicts PirouNetdance originals grouped by intended Eﬀort.
Looking at the video format of such sequences, we remark that instances of dis-
continuity between poses, appearing as quick jumps, arise in a sequence’s ﬁrst
or last ﬁve frames. The (danceable) movement is otherwise realistic, and often

14
M. Papillon et al.
totally reproducible by an experienced dancer. We provide video examples at
github.com/bioshape-lab/pirounet.
Low Time Eﬀort
High Time Eﬀort
a
b
c
d
e
f
g
h
i
j
Fig. 7: Low/High Time Eﬀort dance sequences created by PirouNetdance. The
labeler describes the experience of Time Eﬀort as relating to the presence (or
absence) of impulse, sense of urgency, and explosive momentum. a. Slow and
continuous leg extension from V-sit. b. Continuous transition from crouched to
plank position. c. Smooth weight transfer through deep pli´e. d. Forward standing
weight transfer through small pli´e. e. Transition from standing to plank position
through double rond-de-jambe. f. Demi-pirouette lead by elbow, ending in quick
change in direction. g. Spontaneous drop to a crouched position. h. Leg swing
leads a sharp torso rotation. i. Pirouette with arm thrown up. j. Explosive ex-
tension of arms and torso from pli´e.
Quantitative assessment. We evaluate PirouNetdance’s Eﬀort-centric generative
model using the 225-sequence benchmark set. The labeler identiﬁes 96.0 % of
the dataset as danceable, which is 11.5% better than a benchmark set randomly
sampled from the latent space. Of the danceable portion, 98.1 % of the condition-
ally sampled set respects the constraints of the physical environment, compared
to 72.2% of the randomly sampled set. Only 0.46% of the former shows some
non-physical discontinuity, such as a noticeable change in limb size, versus 8.8%
for the latter. Therefore, the marginal conditional method proves to be advanta-
geous for the purposes of creating realizable and meaningful dance for the user,
outside of providing Eﬀort-speciﬁc movement. To determine PirouNetdance’s ac-
curacy in generating appropriate Eﬀorts, the labeler blindly labels the danceable
portion of the benchmark set without knowledge of PirouNetdance’s intended Ef-
fort. On average, the human labeler correctly classiﬁes 63% of PirouNetdance’s
sequences for a given Eﬀort (Fig. 8). This represents 83% of the labeler’s own
self-agreement on the validation and training data (Fig. 4c).

PirouNet: Creating Dance through Artist-Centric Deep Learning
15
0.77
0.01
0.22
0.61 0.03
0.36
0.52
0.24 0.24
Low
Low
Med
Med
High
High
PirouNet’s 
intended label
0.0
0.2
0.4
0.6
0.8
1.0
Labeler blindly predicts
Fig. 8: Confusion matrix on the danceable portion of the benchmark set (225
sequences). Generation is the least accurate for High Eﬀort, possibly due to the
smaller portion of High Eﬀort labels in the training data. Notably, the human
classiﬁer correctly recognizes the majority of the PirouNet’s intended Eﬀorts.
5
Conclusions
This paper presents PirouNet, the ﬁrst Laban Eﬀort based deep generative tool
for dance, made possible via a semi-supervised conditional dynamic variational
autoencoder. While illustrated on Laban Eﬀorts, our tool can be readily used
with any choreographer-speciﬁc categorical labels. Our keypoint labeling web-
app enables anyone to classify any keypoint data-set with any annotations, em-
powering artists to shape their own AI-tool. We demonstrate PirouNet’s ability
to classify the Laban Time Eﬀort and generate new choreography based on
this Eﬀort as comparable to that of an experienced dancer. Conditional gen-
eration from the marginal distribution qφ(z|y) oﬀers largely accurate, diverse,
and realizable choreography. Randomly sampling from the latent space at large
also produces realistic and novel dance, providing an exciting choreographic tool
outside of categorical generation. Future work will focus on improving classiﬁ-
cation accuracy and disentanglement of the latent space, as well as examining
PirouNet’s implementation and functionality for actual choreographic practices.
We hope this artist-centric, adaptable tool will act as a launching pad for engag-
ing with old repertoire and inspiring new choreography from a completely new
creative standpoint. While demonstrated on dance, the proposed method can be
extended to other forms of art creation, inspiring AI-based tools tailored to the
style and intuition of their artist.
References
1. Alemi, O., Fran¸coise, J., Pasquier, P.: GrooveNet: Real-time music-driven dance
movement generation using artiﬁcial neural networks. Networks 8(17), 26 (2017)
2. Aristidou, A., Stavrakis, E., Charalambous, P., Chrysanthou, Y., Himona, S.L.:
Folk Dance Evaluation Using Laban Movement Analysis. Journal on Computing
and Cultural Heritage 8(4), 1–19 (Aug 2015). https://doi.org/10.1145/2755566
3. Berman, A., James, V.: Kinetic imaginations: Exploring the possibilities of combin-
ing ai and dance. In: Proceedings of the 24th International Conference on Artiﬁcial
Intelligence. p. 2431–2437. IJCAI’15, AAAI Press (2015)
4. Berman, A., James, V.: Learning as Performance: Autoencoding and Generating
Dance Movements in Real Time. In: Computational Intelligence in Music, Sound,
Art and Design. pp. 256–266. Springer International Publishing, Cham (2018)

16
M. Papillon et al.
5. Bernardet, U., Fdili Alaoui, S., Studd, K., Bradley, K., Pasquier, P., Schiphorst,
T.: Assessing the reliability of the Laban Movement Analysis system. PloS one
14(6), e0218179 (2019)
6. Bernstein, R., Shaﬁr, T., Tsachor, R., Studd, K., Schuster, A.: Laban Movement
Analysis Using Kinect. International Journal of Computer and Information Engi-
neering 9(6), 1567–1571 (Aug 2015)
7. Biewald, L.: Experiment Tracking with Weights and Biases (2020), https://www.
wandb.com/, software available from wandb.com
8. Brooks, L.M.: Harmony in Space: A Perspective on the Work of Rudolf Laban.
Journal of Aesthetic Education 27(2), 29–41 (1993), publisher: University of Illinois
Press
9. Carlson, K., Schiphorst, T., Pasquier, P.: Scuddle: Generating Movement Catalysts
for Computer-Aided Choreography. In: ICCC. pp. 123–128 (2011)
10. Choensawat, W., Hachimura, K.: Generating Stylized Dance Motion from Laban-
otation by using an Autonomous Dance Avatar. In: GRAPP/IVAPP (2012)
11. Choensawat, W., Nakamura, M., Hachimura, K.: GenLaban: A tool for generat-
ing Labanotation from motion capture data. Multimedia Tools and Applications
74(23), 10823–10846 (2015)
12. Davies, E.: Beyond dance: Laban’s legacy of movement analysis. Routledge (2007)
13. Davis, J.: Laban Movement Analysis: A key to individualizing children’s dance.
Journal of Physical Education, Recreation & Dance 66(2), 31–33 (1995)
14. El Raheb, K., Ioannidis, Y.: Annotating the captured dance: Reﬂections on the
role of tool-creation. International Journal of Performance Arts and Digital Media
17(1), 118–137 (2021)
15. Ewan, V., Sagovsky, K.: Laban’s Eﬀorts in Action: A Movement Handbook for
Actors with Online Video Resources. Bloomsbury Publishing (2018)
16. Fang, H.S., Xie, S., Tai, Y.W., Lu, C.: RMPE: Regional multi-person pose esti-
mation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 2334–2343 (2017)
17. Groﬀ, E.: Laban Movement Analysis: Charting the ineﬀable domain of human
movement. Journal of Physical Education, Recreation & Dance 66(2), 27–30 (1995)
18. Guo, C., Zuo, X., Wang, S., Zou, S., Sun, Q., Deng, A., Gong, M., Cheng,
L.: Action2Motion: Conditioned Generation of 3D Human Motions. CoRR
abs/2007.15240 (2020)
19. Hamburg,
J.:
Coaching
Athletes
Using
Laban
Movement
Analysis.
Jour-
nal of Physical Education, Recreation & Dance 66(2), 34–37 (Feb 1995).
https://doi.org/10.1080/07303084.1995.10607040
20. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
9(8), 1735–1780 (1997)
21. Hossain, S., Calloway, C., Lippa, D., Niederhut, D., Shupe, D.: Visualization of
bioinformatics data with Dash bio. In: Proceedings of the 18th Python in Science
Conference. pp. 126–133 (2019)
22. Hutchinson, A.: Labanotation: the system of analyzing and recording movement.
No. 27, Taylor & Francis (1977)
23. Jang, M., Kim, D., Kim, Y., Kim, J.: Automated dance motion evaluation using
dynamic time warping and Laban Movement Analysis. In: 2017 IEEE International
Conference on Consumer Electronics (ICCE). pp. 141–142. IEEE (2017)
24. Ji, Y., Xu, F., Yang, Y., Shen, F., Shen, H.T., Zheng, W.S.: A large-scale RGB-D
database for arbitrary-view human action recognition. In: Proceedings of the 26th
ACM international Conference on Multimedia. pp. 1510–1518 (2018)

PirouNet: Creating Dance through Artist-Centric Deep Learning
17
25. Kim, D., Kim, D.H., Kwak, K.C.: Classiﬁcation of K-Pop dance movements based
on skeleton information obtained by a Kinect sensor. Sensors 17(6), 1261 (2017)
26. Kim, H.J., Neﬀ, M., Lee, S.H.: The Perceptual Consistency and Association
of the LMA Eﬀort Elements. ACM Trans. Appl. Percept. 19(1) (Jan 2022).
https://doi.org/10.1145/3473041
27. Kingma, D.P., Ba, J.: Adam: A method for Stochastic Optimization. In: 3rd In-
ternational Conference on Learning Representations, ICLR 2015 (2015)
28. Kingma, D.P., Mohamed, S., Jimenez Rezende, D., Welling, M.: Semi-supervised
learning with deep generative models. In: Ghahramani, Z., Welling, M., Cortes, C.,
Lawrence, N., Weinberger, K. (eds.) Advances in Neural Information Processing
Systems. vol. 27. Curran Associates, Inc. (2014)
29. Knight, H., Simmons, R.: Expressive motion with x, y and theta: Laban eﬀort
features for mobile robots. In: The 23rd IEEE International Symposium on Robot
and Human Interactive Communication. pp. 267–273. IEEE (2014)
30. Lee, H.Y., Yang, X., Liu, M.Y., Wang, T.C., Lu, Y.D., Yang, M.H., Kautz, J.:
Dancing to music. Advances in neural information processing systems 32 (2019)
31. Li, B., Zhao, Y., Zhelun, S., Sheng, L.: DanceFormer: Music conditioned 3D dance
generation with parametric motion transformer. In: Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence. vol. 36, pp. 1272–1279 (2022)
32. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: AI Choreographer: Music conditioned
3D dance generation with AIST++. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision. pp. 13401–13412 (2021)
33. Maletic, V.: Body-Space-Expression: The development of Rudolf Laban’s move-
ment and dance concepts, vol. 75. Walter de Gruyter (2011)
34. McCormick, J., Vincs, K., Nahavandi, S., Creighton, D.: Learning to dance with a
human. ISEA International (2013-01-01), http://hdl.handle.net/2123/9638
35. Mentis, H.M., Johansson, C.: Seeing Movement Qualities. In: Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems. p. 3375–3384.
CHI ’13, Association for Computing Machinery, New York, NY, USA (2013).
https://doi.org/10.1145/2470654.2466462
36. M¨uller, M., R¨oder, T., Clausen, M., Eberhardt, B., Kr¨uger, B., Weber, A.: Docu-
mentation MoCap Database HDM05 (2007)
37. Nair, V., Hinton, G.E.: Rectiﬁed Linear Units Improve Restricted Boltzmann Ma-
chines. In: ICML. pp. 807–814 (2010)
38. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-
mation. In: European conference on computer vision. pp. 483–499. Springer (2016)
39. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,
S.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In:
Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett,
R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024–8035.
Curran Associates, Inc. (2019)
40. Payne, H.: The psycho-neurology of embodiment with examples from authentic
movement and Laban Movement Analysis. American Journal of Dance Therapy
39(2), 163–178 (2017)
41. Petrovich, M., Black, M.J., Varol, G.: Action-Conditioned 3D Human Motion Syn-
thesis with Transformer VAE. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 10985–10995 (2021)

18
M. Papillon et al.
42. Pettee, M., Shimmin, C., Duhaime, D., Vidrin, I.: Beyond Imitation: Generative
and Variational Choreography via Machine Learning. In: Proceedings of the Tenth
International Conference on Computational Creativity. pp. 196–203 (2019)
43. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
by error propagation. Tech. rep., California Univ San Diego La Jolla Inst for Cog-
nitive Science (1985)
44. Santos, L., Dias, J.: Laban Movement Analysis towards behavior patterns. In:
Doctoral Conference on Computing, Electrical and Industrial Systems. pp. 187–
194. Springer (2010)
45. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+ D: A large scale dataset
for 3D human activity analysis. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. pp. 1010–1019 (2016)
46. Sohn, K., Yan, X., Lee, H.: Learning Structured Output Representation Using
Deep Conditional Generative Models. In: Proceedings of the 28th International
Conference on Neural Information Processing Systems - Volume 2. p. 3483–3491.
NIPS’15, MIT Press, Cambridge, MA, USA (2015)
47. Sun, G., Chen, W., Li, H., Sun, Q., Kyan, M., Muneesawang, P., Zhang, P.: A
Virtual Reality Dance Self-learning Framework using Laban Movement Analysis.
Journal of Engineering Science and Technology Review 10, 25–32 (Oct 2017).
https://doi.org/10.25103/jestr.105.03
48. Tan, F., Woo, G., Tsang, H.H.: CGLER: Laban Eﬀort Framework Analysis with
Conducting Gestures Using Neural Networks. In: 2020 IEEE Symposium Series on
Computational Intelligence (SSCI). pp. 1452–1459. IEEE (2020)
49. Toshev, A., Szegedy, C.: DeepPose: Human pose estimation via deep neural net-
works. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 1653–1660 (2014)
50. Truong, A., Boujut, H., Zaharia, T.: Laban descriptors for gesture recognition and
emotional analysis. The visual computer 32(1), 83–98 (2016)
51. Tsachor, R.P., Shaﬁr, T.: A somatic movement approach to fostering emotional
resiliency through Laban Movement Analysis. Frontiers in Human Neuroscience
11, 410 (2017)
52. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: AIST Dance Video Database:
Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information
Processing. In: ISMIR. p. 6. No. 5 (2019)
53. Wang, J., Miao, Z., Xie, N., Xu, W., Li, A.: Labanotation Generation From Mo-
tion Capture Data for Protection of Folk Dance. IEEE Access PP (Aug 2020).
https://doi.org/10.1109/ACCESS.2020.3014157
54. Wang, S., Li, J., Cao, T., Wang, H., Tu, P., Li, Y.: Dance emotion recognition based
on Laban Motion Analysis using convolutional neural network and long short-term
memory. IEEE Access 8, 124928–124938 (2020)
55. Welling, M., Kingma, D.P.: Auto-encoding Variational Bayes. ICLR (2014)
56. Whittier, C.: Laban Movement Analysis approach to classical ballet pedagogy.
Journal of dance education 6(4), 124–132 (2006)
57. Wilke, L., Calvert, T., Ryman, R., Fox, I.: From dance notation to human anima-
tion: The LabanDancer project. Journal of Visualization and Computer Animation
16, 201–211 (07 2005). https://doi.org/10.1002/cav.90
58. Young, J., Wood, L.L.: Laban: A guide ﬁgure between dance/movement therapy
and drama therapy. The Arts in Psychotherapy 57, 11–19 (2018)
59. Zacharatos, H., Gatzoulis, C., Chrysanthou, Y., Aristidou, A.: Emotion recogni-
tion for exergames using Laban Movement Analysis. In: Proceedings of Motion on
Games, pp. 61–66. Association for Computing Machinery (2013)

PirouNet: Creating Dance through Artist-Centric Deep Learning
19
60. Zhang, S., Li, Q., Yu, T., Shen, X., Geng, W., Wang, P.: Implementation of a
notation-based motion choreography system. In: International Conference on Vir-
tual Systems and Multimedia. pp. 495–503. Springer (2006)
61. Zhang, X., Miao, Z., Yang, X., Zhang, Q.: An Eﬃcient Method for Auto-
matic Generation of Labanotation Based on Bi-Directional LSTM. Journal of
Physics: Conference Series 1229(1) (May 2019). https://doi.org/10.1088/1742-
6596/1229/1/012031, publisher: IOP Publishing
62. Zhou, Y., Li, Z., Xiao, S., He, C., Huang, Z., Li, H.: Auto-Conditioned Recur-
rent Networks for Extended Complex Human Motion Synthesis. In: International
Conference on Learning Representations (2018)

