1
Ten Lessons We Have Learned in the New
“Sparseland”: A Short Handbook for Sparse
Neural Network Researchers
Shiwei Liu†, Zhangyang Wang‡
VITA Group, The University of Texas at Austin
This article does not propose any novel algorithm or new hardware for sparsity. Instead,
it aims to serve the “common good” for the increasingly prosperous Sparse Neural
Network (SNN) research community. We attempt to summarize some most common
confusions in SNNs, that one may come across in various scenarios such as paper
review/rebuttal and talks - many drawn from the authors’ own bittersweet
experiences! We feel that doing so is meaningful and timely, since the focus of SNN
research is notably shifting from traditional pruning to more diverse and profound forms
of sparsity before, during, and after training. The intricate relationships between their
scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even
experts in SNNs. In response, we summarize ten Q&As of SNNs from many key
aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs.
sparse training, dense-to-sparse training vs. sparse-to-sparse training, static sparsity vs.
dynamic sparsity, before-training/during-training vs. post-training sparsity, and many
more. We strive to provide proper and generically applicable answers to clarify those
confusions to the best extent possible. We hope our summary provides useful general
knowledge for people who want to enter and engage with this exciting community; and
also provides some “mind of ease” convenience for SNN researchers to explain their work
in the right contexts. At the very least (and perhaps as this article’s most insignificant
target functionality), if you are writing/planning to write a paper or rebuttal in the field
of SNNs, we hope some of our answers could help you!
1. Background on Sparsity in Neural Networks
Sparsity, one of the longest-standing concepts in machine learning, was introduced to
the neural network field as early as in the 1980s (Mozer & Smolensky 1989; Janowsky
1989; LeCun et al. 1990). It was picked up again for “modern” deep networks in the
late 2010s, first under the name of Pruning, with the primary goal to reduce inference
costs (Han et al. 2015a; Wen et al. 2016; Molchanov et al. 2016; Gale et al. 2019; Zhang
et al. 2021, 2022). In a few years, the research interest in sparsity was significantly
revamped, owing to the proposal of Lottery Ticket Hypothesis (LTH) (Frankle &
Carbin 2019), which revisits iterative magnitude pruning (IMP) (Han et al. 2015b) and
discovers that the sparse subnetworks from IMP can match the full performance of the
dense network when trained in isolation using original initializations. LTH has since been
empowered by weight/learning rate rewinding (Frankle et al. 2020; Renda et al. 2020),
and the existence of LTH has been verified in various applications, showing the almost
universal intrinsic sparsity in overparameterized networks (Chen et al. 2020, 2021c,b).
A large body of work has meanwhile emerged to pursue efficient training as the financial
† Email address for correspondence: shiwei.liu@austin.utexas.edu, atlaswang@utexas.edu
‡ The article will be continually updated and we plan to invite more contributors. Stay tuned.
arXiv:2302.02596v3  [cs.LG]  24 Jun 2023

2
Shiwei Liu, Zhangyang Wang
and environmental costs of model training grow exponentially (Strubell et al. 2019;
Patterson et al. 2021). Dynamic Sparse Training (DST) (Mocanu et al. 2018; Liu et al.
2021b; Evci et al. 2020a), stands out and receives upsurging interest due to its promise
in saving both training and inference phases. Distinguished from the conventional pre-
training and pruning, DST starts from a randomly initialized sparse neural network and
dynamically adjusts the sparse topology in one training run without the need for pre-
training, while maintaining moderate training costs by, for example, keeping the same
sparsity ratios across all varying masks (Jayakumar et al. 2020; Liu et al. 2021a). The
key crux of DST, “sparse-to-sparse”, should be contrasted with the more “old-fashioned”
training that gradually sparsifies a dense model (Lym et al. 2019; You et al. 2020) -
referred to as “dense-to-sparse” in this article. One should point out the recent ideas of
“pathway” (Barham et al. 2022) or “mixture of experts” (MoEs) essentially instantiate
DST too (Shazeer et al. 2017; Riquelme et al. 2021; Chowdhery et al. 2022).
As a naive special case of DST, Static Sparse Training (SST) (Mocanu et al.
2016; Evci et al. 2019; Liu et al. 2022b; Dao et al. 2022) sticks to a fixed sparse pattern
throughout training and the sparse pattern needs to be pre-chosen before training. This
is tantalizing too, since to date dynamic sparse masks during training lack practical
hardware system support. LTH could be considered as a sibling to this category: although
LTH utilizes costly pre-training and iterative pruning to discover “winning” sparse masks,
the main contribution it delivers is an “existence proof” of sparse and independently
trainable subnetworks although the construction using IMP compromises the efficiency
gain. As perhaps the most important subgroup of work underlying Static Sparse Training,
Pruning at Initialization (PaI) (Lee et al. 2018; Wang et al. 2020; Tanaka et al. 2020)
seeks to prune weights before training based on a class of gradient-based synaptic saliency
scores so that training over the remaining sparse weights can be more efficient.
While categorized as different sparse algorithms, LTH, DST, and PaI actually share
many similar flavors and approaches, i.e., finding sparse neural networks that can be
independently trained to match the dense ones’ performance, hence replacing the latter.
However, the shift of interest from conventional pruning to the emerging “sparseland”
(including but not limited to LTH, DST, and PaI) only occurred in the last few years, and
the relationships among different sparse algorithms in terms of scopes, assumptions, and
approaches are highly intricate and sometimes ambiguous, which together cause many
misunderstandings for non-experts or even experts. Readers may refer to an excellent
survey article (Hoefler et al. 2021) for a more comprehensive picture.
2. Overview of Sparse Neural Networks as Emerging Research Field
Seeing a new research community being rapidly formed (in part thanks to a few highly
visible tutorials†‡ and well-attended workshops¶∥), we hereby use a unified terminology,
Sparse Neural Networks (SNNs), to collectively refer to all research interests and
activities on the topics of sparsity in deep networks, including but not limited to:
• Sparsity in deep network weights, activations, inputs, or gradients
• Sparsity before, during, or after training
• Sparsity with or without statistical and/or structural group patterns
• Sparsity with constant or input-dynamic masks
† https://icml.cc/virtual/2021/tutorial/10845
‡ https://highdimdata-lowdimmodels-tutorial.github.io/
¶ https://www.sparseneural.net
∥https://slowdnn-workshop.github.io/

Ten Lessons We Have Learned in the New “Sparseland”
3
The research field of SNNs is incredibly deep and broad. We envision a (non-exhaustive
and incomplete) list of important research questions in the SNN field to encapsulate:
• Algorithm foundations of SNNs, including
◦weight sparsity, including classical pruning, LTH, PaI, DST/MoEs, and others
◦activation and gradient sparsity, which are commonly explored in efficient training,
transfer learning or distributed training algorithms
◦input data sparsity such as coreset selection, and in non-Euclidean domains such as
graph sub-sampling
• Learning theory foundations, rigorously relating sparsity to neural network repre-
sentation, architecture, optimization, or generalization
• Hardware and system foundations, supporting SNN training and inference algo-
rithm execution in the edge, the cloud, or the distributed network
• Theoretical neuroscience foundations of SNNs that call for more cross-disciplinary
collaborations (Ma et al. 2022)
Most questions addressed in this article would focus on the SNN algorithm foundations.
We should point out that due to historic research trends & convention, an SNN (in most
available papers nowadays) by default refers to a deep network with weight sparsity (e.g.,
most of its connection weights being zero - regardless of structured versus unstructured,
static versus dynamic). To be aligned with literature norms, this article may by default
use “SNN” to interchangeably refer to sparse-weight networks too. We do hope the scope
of SNNs continues to grow and the terminology will be quickly enriched and evolved.
In the following, we collect a series of the most typical confusions about SNNs and
provide proper answers to address/clarify them, in the hope of providing useful general
knowledge for the non-expert audiences who want to enter and engage with the SNN
community, as well as for the expert audience to better explain their work during a
variety of scenarios. At the very least (and perhaps as this article’s most insignificant
target functionality), if you are writing/planning to write a paper or rebuttal in the field
of SNNs, we hope some of our answers could help you!
3. Common Confusions in SNNs: Start from Ten Q&As
3.1. Why bother sparse neural networks, not just dense compact networks?
(Commonly appearing in reviewer comments like: “Why don’t just use a smaller dense
model, but rather (like an idiot) first start from a bigger model and then sparsifying it?”)
It is well-known that the performance of deep neural networks scales as a power law
w.r.t. model capacity and training data size (Hestness et al. 2017; Kaplan et al. 2020).
However, the memory and computation required to train and deploy these large models
also explode (García-Martín et al. 2019; Patterson et al. 2021). Sparse NNs keep the same
model architectures with those dense large models, but only activate a small fraction of
weights. Since state-of-the-art sparse NNs can match the utility performance of their
dense counterparts (through either pruning or DST/PaI), sparsity can potentially be a
free lunch to make training/inference cheaper while performing the same well.
For the sake of convenience, let us call the source (dense) deep network ‘large dense’,
and the SNN generated by sparsifying ‘large dense’ weights as ‘large sparse’. In general:
• ‘Large sparse’ should ≪‘large dense’ in terms of parameter counts. Meanwhile, ‘large
sparse’ should ≈‘large dense’ in terms of the utility performance such as accuracy.

4
Shiwei Liu, Zhangyang Wang
Meanwhile, since we steadily gain performance of the target utility by scaling up model
sizes appropriately, then if we construct another down-scaled dense model whose param-
eter count ≈‘large sparse’, and call it ‘small dense’, then we should expect that:
• The performance of ‘small dense’ ≪‘large dense’, and hence ≪‘large sparse’ too.
Hence, given the same parameter count, ideally, it is always preferable to choose ‘large
sparse’ since it is more performant than ‘small dense’ while more efficient than ‘large
dense’. The assumed benefits of ‘large sparse’ models over ‘small dense’ have been widely
observed in various SNN scenarios including neural network pruning (Li et al. 2020),
dynamic sparse training (Evci et al. 2020a), and static sparse training (Liu et al. 2022b).
3.2. What is the difference between unstructured and structured weight pruning? What
is the difference between weight pruning and activation pruning?
Unstructured pruning, as the most elementary pruning form, eliminates the least im-
portant weights based on the selected criteria, regardless of where they are. Unstructured
pruning leads to irregular sparse patterns, which typically only see limited speedups on
commodity hardware such as GPUs. On the other hand, structured pruning involves
the selective removal of an entire group of weights. The definition of ‘group’, which
makes those amenable to hardware speedup, could refer to weight blocks (Gray et al.
2017; Ding et al. 2017), neurons (Jiang et al. 2018), filters/channels (Li et al. 2016),
attention heads (Voita et al. 2019)), or other dedicated fine-grained sparse patterns (such
as N:M sparsity (nvi 2020; Zhou et al. 2021), block-diagonal sparsity (Dao et al. 2022),
or compiler-aware patterns (Ma et al. 2020)).
Activation pruning has lots of overlap with structured weight pruning in terms of
their outcomes: completely removing certain intermediate feature outputs. For example,
structural pruning by neurons in MLP layers, channels/filters in convolutional layers,
or heads in self-attention layers naturally yield activation pruning. However, one shall
notice that those are still two separate streams of research and their overlap is only partial
because: (1) on one hand, some structured weight pruning methods do not lead to any
reduction of output dimension. For example, applying N:M sparsity to convolutional
layers will change neither the output channel number nor the feature map size; (2) On
the other hand, some activation pruning methods do not come from removing weight: a
few methods sparsify activations after computing them fully (with full unpruned weights)
(Ardakani et al. 2017; Chakrabarti & Moseley 2019; Jiang et al. 2022b), and that often
happens when saving memory, instead of computation or latency, is the main focus.
3.3. Is structured pruning just channel pruning? Is channel pruning the only “practical
meaningful” sparse structure on hardware?
No, structured pruning is way more than just channel pruning - it encompasses various
more types of sparse patterns like block-wise, neuron-wise, channel/filter-wise, and fine-
grained sparsity (see above). Channel/filter pruning results in a sparse pattern that
is equivalent to a small dense network, that can be straightforwardly accelerated on
commodity hardware and platforms. Nevertheless, other forms of structured sparsity
(e.g., block-wise, vector-wise, N:M fine-grained) tend to deliver better accuracy at higher
levels of sparsity since their less restricted zero/nonzero weight positions. When combined
with the specialized hardware (nvi 2020; Toon 2020) and libraries (Gray et al. 2017), these
types often have stronger performance-speed trade-offs compared to channel pruning. So,

Ten Lessons We Have Learned in the New “Sparseland”
5
the answer to which sparsity pattern is more practically meaningful really depends on
the hardware customization and resource availability.
3.4. Does weight pruning simply produce a smaller/narrower network?
(Commonly appearing in reviewer comments like: “It makes no difference to specifically
study a sparse network, because that is just a normal dense network in reduced width!”)
Neuron Pruning
Weight Pruning
Neuron
Weight
Figure 1. Neuron Pruning v.s. Weight Pruning
on Fully-Connected Networks. The green color
indicates being pruned. Neuron pruning results
in width decrease while weight pruning results in
sparser connection but no change in width.
No in general
for
weight
pruning;
Yes only for the neuron/channel/filter
pruning case (equivalent to activation
pruning). We also create Figure 1 to
better illustrate their difference.
For simplicity, let us use a fully-
connected network with width m, and
“prune it” with a probability 1−α. Does
it result in another fully-connected
network with width m′ = mα? If we
are talking about pruning neurons (in
convolutional networks, channels), that
will indeed result in a fully-connected
network (‘small dense’ equivalent) with
just
a
decrease
of
width.
Pruning
weights, on the other hand, will result
in a network with the same width
but no longer being fully connected.
3.5. Why study unstructured sparsity if it can not be accelerated on common GPUs?
(Commonly appearing in reviewer comments like: “...the method is only demonstrated
on unstructured sparsity and hence has no practical value!”)
Unstructured sparsity is NOT a toy! It is extremely useful as both a mathematical pro-
totype and an empirical testbed for new SNN algorithms; it is also receiving increasingly
better support in practice. We strongly advise the community to pay more attention and
fair appreciation to unstructured sparsity research.
Firstly, as the finest-grained and most flexible sparsity level, unstructured sparsity
has performance superiority compared to other more structured forms of sparsity: (1)
it typically maintains the highest accuracy at high sparsity ratios (Mao et al. 2017).
One can commonly consider the achievable accuracy by unstructured sparsity as the
“ceiling” that other sparsity forms can hopefully match at the same sparsity ratio; (2) as
a regularizer, it is also found to boost many other performance aspects beyond efficiency,
such as adversarial robustness (Ye et al. 2019; Chen et al. 2022c), out-of-distribution
detection/generalization (Sun & Li 2022; Diffenderfer et al. 2021), uncertainty quantifi-
cation (Liu et al. 2022b), data efficiency (T et al. 2022; Chen et al. 2022b, 2021a), multi-
tasking and task transferability (Fan et al. 2022; Iofinova et al. 2022), interpretability
(Chen et al. 2022e), sometimes even a multi-pronged win (Chen et al. 2022d).
Secondly, unstructured sparsity has widely proven its practical relevance on non-
GPU hardware, such as CPUs or customized accelerators. For instance, in the range
of 70-90% high unstructured sparsity, XNNPACK (Elsen et al. 2020) has already shown
significant speedups over dense baselines on smartphone processors. For an unstructured

6
Shiwei Liu, Zhangyang Wang
sparse RNN, an FPGA accelerator in (Ashby et al. 2019) achieved high acceleration and
energy efficiency performance than commercial CPU/GPU, by maximizing the use of the
embedded multiply resource available on the FPGA. Another notable success was recently
demonstrated by DeepSparse (Kurtz et al. 2020; Kurtic et al. 2022) which successfully
deploys large-scale BERT-level sparse models on modern Intel CPUs, obtaining 10×
model size compression with < 1% accuracy drop, 10× CPU-inference speedup with <
2% drop, and 29× CPU-inference speedup with < 7.5% drop. Unstructured dynamic
sparse training has similarly shown some promise on CPUs (Liu et al. 2021b; Curci et al.
2021; Atashgahi et al. 2020). More recently, the S4 hardware platform introduced by
Moffett AI can support up to 32× acceleration (Yen et al. 2022).
Thirdly, the hardware support of unstructured sparsity may be relatively limited on
“off-the-shelf” commodity GPUs/TPUs, but it keeps improving quickly over the years. For
example, advanced GPU kernels such as NVIDIA cuSPARSE (Valero-Lara et al. 2018),
Sputnik (Gale et al. 2020), and NVIDIA Ampere Architecture (nvi 2020) have built the
momentum to better support finer-grained sparsity. Moreover, even the computational
latency and energy benefits of unstructured sparse matrices may not always be obvious,
their memory-saving effect compared to dense matrices is solid and readily available
(Evci et al. 2020a; Guo et al. 2020). Lastly, several algorithms support the conversion
from unstructured sparsity (at training time) to structured sparsity (at inference time),
e.g., (Chen et al. 2022a; Jiang et al. 2022a) - see the next Q & A.
3.6. Are unstructured and structured sparse algorithms connected, or potentially
“convertible” to each other?
Yes, although unstructured sparsity faces challenges in terms of hardware support,
researchers are finding ways to convert it into structured sparse patterns without in-
curring visible accuracy loss. Chen et al. (2022a) introduce weight refilling and weight
regrouping (Rumi et al. 2020) as post-training operations, converting unstructured LTH
solutions into fine-grained group-wise patterns, which are compatible with common GPU
devices for inference-time hardware acceleration. Similar grouping operations can be
utilized to gradually convert unstructured sparsity to structured sparsity, during dynamic
sparse training, as demonstrated by Jiang et al. (2022a). More recent works (Peste et al.
2021; Yin et al. 2022a; Dao et al. 2022) have all revealed the convertibility between
unstructured sparse, structured sparse, and dense networks.
3.7. What is the difference between dense-to-sparse training and sparse-to-sparse
training? When choosing the former, and when the latter?
Dense-to-sparse training is the more “classical” way: starting from training a dense
model, and (one-shot, or gradually) eliminating weights down to zero, ending up with a
sparse model. The standard post-training pruning could be viewed as a special case in this
category: typically involving fully pre-training a dense network as well as multiple cycles
of retraining (after pruning to higher sparsity) (Mozer & Smolensky 1989; Han et al.
2015a; LeCun et al. 1990; Molchanov et al. 2016). More recently, it has been revealed
that one can reap the fruit of mature sparse masks after completing just a small fraction of
dense training, such as Lym et al. (2019); Gale et al. (2019); You et al. (2020); Chen et al.
(2021d); Liu et al. (2021a) that attempted to prune the network to the desired sparsity
either “one-shot” in early training or gradually along with training. Their resultant sparse
models can often achieve comparably good accuracies with the dense models. The price

Ten Lessons We Have Learned in the New “Sparseland”
7
they have to pay, however, is the full or partial stage of dense (pre-training) that will
cost high memory usage besides energy/latency, i.e., the peak memory cost of a dense-
to-sparse training process will be as high as the full dense training.
On the contrary, sparse-to-sparse training never refers to a “dense model” at any stage
of training: it aims to train an intrinsically sparse neural network from scratch and
maintain the desired sparsity throughout training (Mocanu et al. 2016, 2018; Bellec et al.
2018; Liu et al. 2021b). Thus at any point, their memory cost, as well as the per-iteration
computational FLOPs (in theory), can be kept limited or constant (by just treating a
sparse subnetwork), and much smaller compared to the dense counterpart. That however
should not be directly interpreted as that sparse-to-sparse training necessarily reduces
the total training FLOPs, because many of those algorithms, particularly for supervised
learning†, could take significantly more time to achieve the dense model’s full accuracy,
compared to dense or dense-to-sparse training (Evci et al. 2020a).
The motivation behind studying sparse-to-sparse training is natural: as powerful
foundation models like GPT-3 (Brown et al. 2020), PaLM (Chowdhery et al. 2022),
and DALL.E 2 (Ramesh et al. 2022) become prevailing, the prohibitive training cost
of these big models is out of the reach for most academic researchers, and maybe even
most companies. Sparse-to-sparse training can amortize the per-iteration computation
and memory costs (“peak” resource usage) and allow for more affordable scaling of model
sizes (García-Martín et al. 2019; Patterson et al. 2021). For example, Mixture-of-Experts
(MoEs), a specific type of sparse-to-sparse training, has demonstrated impressive results
when models scale up (Shazeer et al. 2017; Lepikhin et al. 2020; Fedus et al. 2021; Chen
et al. 2023; Roller et al. 2021; Lewis et al. 2021; Chowdhery et al. 2022). At any time
of training or inference, the actual MoE model in use is always a small part of its full
network, adaptively activated on a per-example basis.
Long story short, the two most common confusions are clarified below:
• The main methodological difference between dense-to-sparse and sparse-to-sparse
training is whether or not a full-size dense model is trained at any point. The former
benefits from dense pre-training (when it is affordable) to yield more competitive
performance; but the latter scales up better with the larger model size.
• Comparing their training-time efficiency, sparse-to-sparse algorithms save the peak
memory usage, and (in theory) reduce the per-iteration computation. Meanwhile, if
required to match the dense training full accuracy, some sparse-to-sparse algorithms
might take more iterations to converge, hence not always “cheaper” in terms of the total
computation amount throughout training - but a few latest ones (Liu et al. 2021c,a;
Yuan et al. 2021; Schwarz et al. 2021) can indeed save the total FLOPs while matching
the full accuracy.
3.8. Now for sparse-to-sparse training: Is the sparse mask only static or only dynamic?
If newly activated weights of dynamic sparse training are initialized with zero, the
gradient will also be zero. Why does dynamic sparse training work?
Is Dropout a DST method or not?
Sparse-to-sparse training consists of both Dynamic Sparse Training (DST) and Static
Sparse Training (SST). Both start from a sparse neural network, and their main difference
is whether the sparse mask is dynamically adjusted or not during training.
† We do observe that in reinforcement learning, sparse training learns faster and better than
its dense counterpart Sokar et al. (2021); Graesser et al. (2022).

8
Shiwei Liu, Zhangyang Wang
DST starts from a sparse neural network and allows the sparse connectivity to evolve
dynamically during training. In essence, a DST algorithm needs to define a pruning
criterion and a re-growing criterion, to turn weights off and on, to ensure the sparsity
does not monotonically drop. It has been first introduced in Mocanu et al. (2018) by
proposing a simple prune-and-grow regime that randomly activates new weights during
training. Follow-up works further introduced weight redistribution (Bellec et al. 2018;
Mostafa & Wang 2019; Dettmers & Zettlemoyer 2019), gradient-based weight growth
(Dettmers & Zettlemoyer 2019; Evci et al. 2020b; Liu et al. 2021c), and extra weights
update (Jayakumar et al. 2020; Liu et al. 2021a; Yuan et al. 2021; Peste et al. 2021) to
improve its performance. The output of DST is most commonly a sparse subnetwork with
the final evolved mask, but sometimes could be a dense or semi-dense network as well,
e.g., through ensembling multiple snapshots in DST (Liu et al. 2022a; Yin et al. 2022b).
Note that if the dense pre-training is involved, then even at a later point it is reduced to
sparse training with dynamic sparse masks, we do not recommend calling such algorithm
DST, to be clear on the comparison fairness issue.
In contrast, SST (Mocanu et al. 2016; Evci et al. 2019; Liu et al. 2022b; Dao et al.
2022) sticks to a fixed sparse mask throughout training and the sparse pattern needs to be
pre-chosen before training. As perhaps the most important subgroup of work under the
umbrella of SST, Pruning at Initialization (PaI) (Lee et al. 2018; Wang et al. 2020; Tanaka
et al. 2020) seeks the sparse mask at initialization based on a class of gradient-based
synaptic saliency scores. Although precisely speaking, PaI approaches require calculating
dense gradients for at least one mini-batch/iteration, the overhead is negligible and hence
can be considered as sparse-to-sparse.
It is a common misconception that when weights are initialized to zero, their gradients
are also zero. However, this is not true. To illustrate this, let’s consider the Softmax
function as an example. Although the value of Softmax at zero is zero, its gradient at
zero is obviously nonzero. Therefore, after one iteration of gradient descent, the zero-
initialized weights will become meaningful as they are updated to non-zero values.
Dropout (Wan et al. 2013) is perhaps the most naive DST one can dream of: randomly
sampling and training a different sparse subnetwork every iteration (never training the full
dense weight); and its output, the dense network, could be considered as the “ensemble”
of all sparse subnetworks during training (Gal & Ghahramani 2016). So it fits in the
definition of DST, except for two differences to note: (1) Dropout does not exploit any
informative training signals to update the sparse mask, just randomly sampling; (2)
Dropout sticks to a moderate sparsity ratio (e.g., 50%), while modern DST methods
often operate with much higher sparsity (e.g., > 70% or even 90%).
3.9. Is Lottery Ticket Hypothesis also a sparse-to-sparse training approach, and should
PaI methods be asked to compare against it?
A common misunderstanding is to count LTH as one SST method: this is at least
imprecise. Although the final outcome of LTH is a sparse subnetwork that can be trained
like SST, the process to find that sparse mask requires resource-intensive dense pre-
training, as well as multiple rounds of pruning & re-training. Therefore, LTH provides
invaluable empirical evidence (we call “existence proof”) for powerful sparse-to-sparse
training, but the IMP-based LTH method itself (we call “construction”) should be counted
as dense-to-sparse training. From another angle, LTH could also be considered to set a
high accuracy bar, that would challenge most sparse-to-sparse training algorithms under

Ten Lessons We Have Learned in the New “Sparseland”
9
typical training time as dense training, especially at high sparsity (i.e., 90-100 epochs for
> 80% sparse ResNet-50 on ImageNet).
That said, if allowed to train with extended epoch lengths (e.g., 2-5×), several latest
DST approaches (Evci et al. 2020a; Liu et al. 2021c,a; Schwarz et al. 2021; Yuan et al.
2021) can already perform on par or better than LTH. Overall, we do not feel it necessary
to ask every new sparse-to-sparse algorithm, especially PaI methods, to outperform LTH:
but it would certainly make a highlight if they can.
3.10. How to draw fair and trusted comparisons among different sparse algorithms?
The recent surge of interest in SNNs is unquestionable, but the lack of standardized
evaluation protocols has led to notoriously unfair comparisons among SNN papers, as
highlighted in several persuasive studies (Gale et al. 2019; Blalock et al. 2020; Ma et al.
2021; Wang et al. 2023). Sparse models obtained by different settings and configurations
(e.g., training time, pre-trained models, training hyperparameters, and even training
phases) are often blindly mixed for comparisons to draw rushed conclusions. We see this
as a major roadblock that holds back the development of the SNN community.
In order to make fair comparisons, we suggest the following actions of sanity:
• Positioning: For fair comparison, it is utmostly important to accurately categorize the
proposed sparse algorithm: post-training, during-training, or before-training? Struc-
tured or unstructured? Dense-to-sparse, or sparse-to-sparse? If the latter, static or
dynamic? For example, comparing dense-to-sparse training pruning with PaI methods
cannot be fair since the former has dense model training at higher memory budgets.
Comparing any sparse-to-sparse training method to LTH is likewise disadvantageous.
• Tasks and architectures: One shall use a representative and diverse set of tasks and
architectures to evaluate different SNN algorithms, concretely:
◦We recommend against reporting SNN results, mainly or solely, on small “outdated”
datasets such as CIFAR-10/100 or MNIST, since all SNN algorithms will have decent
yet indistinguishable results. They could of course be used as a case study or ablation
subject, but no final conclusion shall be drawn from those.
◦Although not yet a popular convention in SNN papers, we advocate avoiding overfit-
ting one specific task or domain. It would be favorable to validate an SNN algorithm
beyond just image classification (most papers’ de facto choice) - but on multiple
computer vision tasks, or (even better) both vision and NLP tasks.
◦Lastly, we suggest the field move forwards to tackle some under-explored, signif-
icantly more sophisticated tasks and benchmarks, which will be likely to expose
new research challenges as well as opportunities: consider SparseGPT (Frantar &
Alistarh 2023) and SMC-Bench (Liu et al. 2023).
• Configurations: It is vital that we use as identical as possible training configurations
for all sparse baselines. Besides the pruning criterion itself, many other training
hyperparameters matter for the final performance, including but not limited to pre-
trained dense checkpoints, data augmentations, training/finetuning time, learning rate
schedule, which layer to prune, etc. Our recommendation is to keep strictly controlled
comparisons to avoid the (often surprisingly large) impact of confounding variables. It
is preferable to use the same pre-training dense network for all sparse algorithms.
• Metrics: Besides the widely used metrics like accuracy and theoretical speedups
(FLOPs) /compression ratio, other important evaluation metrics shall be brought into
the evaluation picture, such as realistic speedups (throughput/latency) on hardware,

10
Shiwei Liu, Zhangyang Wang
and other utility metrics of interest such as robustness. In addition, it is instrumental
to report results with the error bars plotted: be sure to include standard deviations
obtained from multiple (3-5 minimum) random seeds.
4. Acknowledgement
We would like to thank Decebal Constantin Mocanu and Huan Wang for their feedback
and support.
REFERENCES
2020 Nvidia a100 tensor core gpu architecture. https://www.nvidia.com/content/dam/en-
zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf .
Ardakani, Arash, Condo, Carlo & Gross, Warren J 2017 Activation pruning of deep
convolutional neural networks. In 2017 IEEE Global Conference on Signal and Information
Processing (GlobalSIP), pp. 1325–1329. IEEE.
Ashby, Mike, Baaij, Christiaan, Baldwin, Peter, Bastiaan, Martijn, Bunting,
Oliver, Cairncross, Aiken, Chalmers, Christopher, Corrigan, Liz, Davis,
Sam, van Doorn, Nathan & others 2019 Exploiting unstructured sparsity on next-
generation datacenter hardware.
Atashgahi, Zahra, Sokar, Ghada, van der Lee, Tim, Mocanu, Elena, Mocanu,
Decebal Constantin, Veldhuis, Raymond & Pechenizkiy, Mykola 2020 Quick
and robust feature selection: the strength of energy-efficient sparse training for
autoencoders. arXiv preprint arXiv:2012.00560 .
Barham, Paul, Chowdhery, Aakanksha, Dean, Jeff, Ghemawat, Sanjay, Hand,
Steven, Hurt, Daniel, Isard, Michael, Lim, Hyeontaek, Pang, Ruoming, Roy,
Sudip & others 2022 Pathways: Asynchronous distributed dataflow for ml. Proceedings
of Machine Learning and Systems 4, 430–449.
Bellec, Guillaume, Kappel, David, Maass, Wolfgang & Legenstein, Robert 2018
Deep rewiring: Training very sparse deep networks. In International Conference on
Learning Representations.
Blalock, Davis, Gonzalez Ortiz, Jose Javier, Frankle, Jonathan & Guttag, John
2020 What is the state of neural network pruning? Proceedings of machine learning and
systems 2, 129–146.
Brown,
Tom,
Mann,
Benjamin,
Ryder,
Nick,
Subbiah,
Melanie,
Kaplan,
Jared D, Dhariwal, Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry,
Girish, Askell, Amanda, Agarwal, Sandhini, Herbert-Voss, Ariel, Krueger,
Gretchen, Henighan, Tom, Child, Rewon, Ramesh, Aditya, Ziegler, Daniel,
Wu, Jeffrey, Winter, Clemens, Hesse, Chris, Chen, Mark, Sigler, Eric,
Litwin,
Mateusz,
Gray,
Scott,
Chess,
Benjamin,
Clark,
Jack,
Berner,
Christopher, McCandlish, Sam, Radford, Alec, Sutskever, Ilya & Amodei,
Dario 2020 Language models are few-shot learners. In Advances in Neural Information
Processing Systems (ed. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan & H. Lin),
, vol. 33, pp. 1877–1901. Curran Associates, Inc.
Chakrabarti, Ayan & Moseley, Benjamin 2019 Backprop with approximate activations for
memory-efficient network training. Advances in Neural Information Processing Systems .
Chen, Tianlong, Chen, Xuxi, Ma, Xiaolong, Wang, Yanzhi & Wang, Zhangyang
2022a
Coarsening the granularity: Towards structurally sparse lottery tickets. In
International Conference on Machine Learning, pp. 3025–3039. PMLR.
Chen, Tianlong, Cheng, Yu, Gan, Zhe, Liu, Jingjing & Wang, Zhangyang 2021a Data-
efficient gan training beyond (just) augmentations: A lottery ticket perspective. Advances
in Neural Information Processing Systems 34, 20941–20955.
Chen, Tianlong, Frankle, Jonathan, Chang, Shiyu, Liu, Sijia, Zhang, Yang, Carbin,
Michael & Wang, Zhangyang 2021b The lottery tickets hypothesis for supervised and
self-supervised pre-training in computer vision models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 16306–16316.

Ten Lessons We Have Learned in the New “Sparseland”
11
Chen, Tianlong, Frankle, Jonathan, Chang, Shiyu, Liu, Sijia, Zhang, Yang, Wang,
Zhangyang & Carbin, Michael 2020 The lottery ticket hypothesis for pre-trained
bert networks. In Advances in Neural Information Processing Systems (ed. H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan & H. Lin), , vol. 33, pp. 15834–15846. Curran
Associates, Inc.
Chen, Tianlong, Zhang, Zhenyu, JAISWAL, AJAY KUMAR, Liu, Shiwei & Wang,
Zhangyang 2023 Sparse moe with random routing as the new dropout: Training bigger
and self-scalable models. In International Conference on Learning Representations.
Chen, Tianlong, Zhang, Zhenyu, Liu, Sijia, Chang, Shiyu & Wang, Zhangyang 2021c
Long live the lottery: The existence of winning tickets in lifelong learning. In International
Conference on Learning Representations.
Chen, Tianlong, Zhang, Zhenyu, Liu, Sijia, Zhang, Yang, Chang, Shiyu & Wang,
Zhangyang 2022b Data-efficient double-win lottery tickets from robust pre-training. In
International Conference on Machine Learning, pp. 3747–3759. PMLR.
Chen, Tianlong, Zhang, Zhenyu, Wang, Pengjun, Balachandra, Santosh, Ma,
Haoyu, Wang, Zehao & Wang, Zhangyang 2022c Sparsity winning twice: Better
robust generalization from more efficient training. In International Conference on Learning
Representations.
Chen, Tianlong, Zhang, Zhenyu, Wu, Jun, Huang, Randy, Liu, Sijia, Chang, Shiyu
& Wang, Zhangyang 2022d Can you win everything with a lottery ticket? Transactions
on Machine Learning Research .
Chen, Xiaohan, Cheng, Yu, Wang, Shuohang, Gan, Zhe, Wang, Zhangyang & Liu,
Jingjing 2021d Earlybert: Efficient bert training via early-bird lottery tickets. ACL-
IJCNLP .
Chen,
Yubei,
Yun,
Zeyu,
Ma,
Yi,
Olshausen,
Bruno
&
LeCun,
Yann 2022e
Minimalistic unsupervised learning with the sparse manifold transform. arXiv preprint
arXiv:2209.15261 .
Chowdhery, Aakanksha, Narang, Sharan, Devlin, Jacob, Bosma, Maarten, Mishra,
Gaurav, Roberts, Adam, Barham, Paul, Chung, Hyung Won, Sutton, Charles,
Gehrmann, Sebastian & others 2022 Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 .
Curci, Selima, Mocanu, Decebal Constantin & Pechenizkiyi, Mykola 2021 Truly
sparse neural networks at scale. arXiv preprint arXiv:2102.01732 .
Dao, Tri, Chen, Beidi, Sohoni, Nimit S, Desai, Arjun, Poli, Michael, Grogan,
Jessica, Liu, Alexander, Rao, Aniruddh, Rudra, Atri & Ré, Christopher
2022 Monarch: Expressive structured matrices for efficient and accurate training. In
International Conference on Machine Learning, pp. 4690–4721. PMLR.
Dettmers, Tim & Zettlemoyer, Luke 2019 Sparse networks from scratch: Faster training
without losing performance. arXiv preprint arXiv:1907.04840 .
Diffenderfer,
James,
Bartoldson,
Brian,
Chaganti,
Shreya,
Zhang,
Jize
&
Kailkhura, Bhavya 2021 A winning hand: Compressing deep networks can improve
out-of-distribution robustness. Advances in Neural Information Processing Systems .
Ding, Caiwen, Liao, Siyu, Wang, Yanzhi, Li, Zhe, Liu, Ning, Zhuo, Youwei, Wang,
Chao, Qian, Xuehai, Bai, Yu, Yuan, Geng & others 2017 Circnn: accelerating and
compressing deep neural networks using block-circulant weight matrices. In Proceedings of
the 50th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 395–408.
Elsen, Erich, Dukhan, Marat, Gale, Trevor & Simonyan, Karen 2020 Fast sparse
convnets. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 14629–14638.
Evci, Utku, Gale, Trevor, Menick, Jacob, Castro, Pablo Samuel & Elsen, Erich
2020a Rigging the lottery: Making all tickets winners. In International Conference on
Machine Learning, pp. 2943–2952. PMLR.
Evci, Utku, Ioannou, Yani A, Keskin, Cem & Dauphin, Yann 2020b Gradient flow in
sparse neural networks and how lottery tickets win. arXiv preprint arXiv:2010.03533 .
Evci, Utku, Pedregosa, Fabian, Gomez, Aidan & Elsen, Erich 2019 The difficulty of
training sparse neural networks. arXiv preprint arXiv:1906.10732 .
Fan, Zhiwen, Sarkar, Rishov, Jiang, Ziyu, Chen, Tianlong, Zou, Kai, Cheng, Yu,
Hao, Cong, Wang, Zhangyang & others 2022 M3vit: Mixture-of-experts vision

12
Shiwei Liu, Zhangyang Wang
transformer for efficient multi-task learning with model-accelerator co-design. In Advances
in Neural Information Processing Systems.
Fedus, William, Zoph, Barret & Shazeer, Noam 2021 Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res 23, 1–40.
Frankle, Jonathan & Carbin, Michael 2019 The lottery ticket hypothesis: Finding sparse,
trainable neural networks. In International Conference on Learning Representations.
Frankle, Jonathan, Dziugaite, Gintare Karolina, Roy, Daniel & Carbin, Michael
2020 Linear mode connectivity and the lottery ticket hypothesis. In International
Conference on Machine Learning, pp. 3259–3269. PMLR.
Frantar, Elias & Alistarh, Dan 2023 Massive language models can be accurately pruned
in one-shot. arXiv preprint arXiv:2301.00774 .
Gal, Yarin & Ghahramani, Zoubin 2016 Dropout as a bayesian approximation: Representing
model uncertainty in deep learning. In international conference on machine learning, pp.
1050–1059. PMLR.
Gale, Trevor, Elsen, Erich & Hooker, Sara 2019 The state of sparsity in deep neural
networks. arXiv preprint arXiv:1902.09574 .
Gale, Trevor, Zaharia, Matei, Young, Cliff & Elsen, Erich 2020 Sparse gpu kernels
for deep learning. In SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis, pp. 1–14. IEEE.
García-Martín, Eva, Rodrigues, Crefeda Faviola, Riley, Graham & Grahn, Håkan
2019 Estimation of energy consumption in machine learning. Journal of Parallel and
Distributed Computing 134, 75–88.
Graesser, Laura, Evci, Utku, Elsen, Erich & Castro, Pablo Samuel 2022 The state
of sparse training in deep reinforcement learning. In International Conference on Machine
Learning, pp. 7766–7792. PMLR.
Gray, Scott, Radford, Alec & Kingma, Diederik P 2017 Gpu kernels for block-sparse
weights. arXiv preprint arXiv:1711.09224 3, 2.
Guo, Demi, Rush, Alexander M & Kim, Yoon 2020 Parameter-efficient transfer learning
with diff pruning. arXiv preprint arXiv:2012.07463 .
Han, Song, Mao, Huizi & Dally, William J 2015a Deep compression: Compressing deep
neural networks with pruning, trained quantization and huffman coding. International
Conference on Learning Representations .
Han, Song, Pool, Jeff, Tran, John & Dally, William 2015b Learning both weights and
connections for efficient neural network. In Advances in neural information processing
systems, pp. 1135–1143.
Hestness,
Joel,
Narang,
Sharan,
Ardalani,
Newsha,
Diamos,
Gregory,
Jun,
Heewoo, Kianinejad, Hassan, Patwary, Md, Ali, Mostofa, Yang, Yang &
Zhou, Yanqi 2017 Deep learning scaling is predictable, empirically. arXiv preprint
arXiv:1712.00409 .
Hoefler,
Torsten,
Alistarh,
Dan,
Ben-Nun,
Tal,
Dryden,
Nikoli
&
Peste,
Alexandra 2021 Sparsity in deep learning: Pruning and growth for efficient inference
and training in neural networks. Journal of Machine Learning Research 22 (241), 1–124.
Iofinova, Eugenia, Peste, Alexandra, Kurtz, Mark & Alistarh, Dan 2022 How well
do sparse imagenet models transfer? In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 12266–12276.
Janowsky, Steven A 1989 Pruning versus clipping in neural networks. Physical Review A
39 (12), 6600.
Jayakumar, Siddhant, Pascanu, Razvan, Rae, Jack, Osindero, Simon & Elsen, Erich
2020 Top-kast: Top-k always sparse training. Advances in Neural Information Processing
Systems 33, 20744–20754.
Jiang, Chunhui, Li, Guiying, Qian, Chao & Tang, Ke 2018 Efficient dnn neuron pruning
by minimizing layer-wise nonlinear reconstruction error. In IJCAI , , vol. 2018, pp. 2–2.
Jiang, Peng, Hu, Lihan & Song, Shihui 2022a Exposing and exploiting fine-grained block
structures for fast and accurate sparse training. In Advances in Neural Information
Processing Systems (ed. Alice H. Oh, Alekh Agarwal, Danielle Belgrave & Kyunghyun
Cho).
Jiang, Ziyu, Chen, Xuxi, Huang, Xueqin, Du, Xianzhi, Zhou, Denny & Wang,

Ten Lessons We Have Learned in the New “Sparseland”
13
Zhangyang 2022b Back razor: Memory-efficient transfer learning by self-sparsified
backpropagation. In Advances in Neural Information Processing Systems.
Kaplan, Jared, McCandlish, Sam, Henighan, Tom, Brown, Tom B, Chess, Benjamin,
Child, Rewon, Gray, Scott, Radford, Alec, Wu, Jeffrey & Amodei, Dario
2020 Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 .
Kurtic, Eldar, Campos, Daniel, Nguyen, Tuan, Frantar, Elias, Kurtz, Mark,
Fineran, Benjamin, Goin, Michael & Alistarh, Dan 2022 The optimal bert surgeon:
Scalable and accurate second-order pruning for large language models. arXiv preprint
arXiv:2203.07259 .
Kurtz, Mark, Kopinsky, Justin, Gelashvili, Rati, Matveev, Alexander, Carr, John,
Goin, Michael, Leiserson, William, Moore, Sage, Nell, Bill, Shavit, Nir &
Alistarh, Dan 2020 Inducing and exploiting activation sparsity for fast inference on
deep neural networks. In Proceedings of the 37th International Conference on Machine
Learning (ed. Hal Daumé III & Aarti Singh), Proceedings of Machine Learning Research,
vol. 119, pp. 5533–5543. Virtual: PMLR.
LeCun, Yann, Denker, John S & Solla, Sara A 1990 Optimal brain damage. In Advances
in neural information processing systems, pp. 598–605.
Lee, Namhoon, Ajanthan, Thalaiyasingam & Torr, Philip HS 2018 Snip: Single-shot
network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 .
Lepikhin, Dmitry, Lee, HyoukJoong, Xu, Yuanzhong, Chen, Dehao, Firat, Orhan,
Huang, Yanping, Krikun, Maxim, Shazeer, Noam & Chen, Zhifeng 2020 Gshard:
Scaling giant models with conditional computation and automatic sharding. arXiv preprint
arXiv:2006.16668 .
Lewis, Mike, Bhosale, Shruti, Dettmers, Tim, Goyal, Naman & Zettlemoyer, Luke
2021 Base layers: Simplifying training of large, sparse models. In International Conference
on Machine Learning, pp. 6265–6274. PMLR.
Li, Hao, Kadav, Asim, Durdanovic, Igor, Samet, Hanan & Graf, Hans Peter
2016 Pruning filters for efficient convnets. International Conference on Learning
Representations .
Li, Zhuohan, Wallace, Eric, Shen, Sheng, Lin, Kevin, Keutzer, Kurt, Klein, Dan
& Gonzalez, Joey 2020 Train big, then compress: Rethinking model size for efficient
training and inference of transformers. In International Conference on machine learning,
pp. 5958–5968. PMLR.
Liu, Shiwei, Chen, Tianlong, Atashgahi, Zahra, Chen, Xiaohan, Sokar, Ghada,
Mocanu,
Elena,
Pechenizkiy,
Mykola,
Wang,
Zhangyang
&
Mocanu,
Decebal Constantin 2022a Deep ensembling with no overhead for either training
or testing: The all-round blessings of dynamic sparsity. In International Conference on
Learning Representations.
Liu, Shiwei, Chen, Tianlong, Chen, Xiaohan, Atashgahi, Zahra, Yin, Lu, Kou,
Huanyu,
Shen,
Li,
Pechenizkiy,
Mykola,
Wang,
Zhangyang
&
Mocanu,
Decebal Constantin 2021a Sparse training via boosting pruning plasticity with
neuroregeneration. Advances in Neural Information Processing Systems 34, 9908–9922.
Liu, Shiwei, Chen, Tianlong, Chen, Xiaohan, Shen, Li, Mocanu, Decebal C, Wang,
Zhangyang & Pechenizkiy, Mykola 2022b The unreasonable effectiveness of random
pruning: Return of the most naive baseline for sparse training. In International Conference
on Learning Representations, ICLR 2022.
Liu,
Shiwei,
Chen,
Tianlong,
Zhang,
Zhenyu,
Chen,
Xuxi,
Huang,
Tianjin,
JAISWAL, AJAY KUMAR & Wang, Zhangyang 2023 Sparsity may cry: Let us
fail (current) sparse neural networks together! In International Conference on Learning
Representations.
Liu, Shiwei, Mocanu, Decebal Constantin, Matavalam, Amarsagar Reddy Ramapu-
ram, Pei, Yulong & Pechenizkiy, Mykola 2021b Sparse evolutionary deep learning
with over one million artificial neurons on commodity hardware. Neural Computing and
Applications 33 (7), 2589–2604.
Liu, Shiwei, Yin, Lu, Mocanu, Decebal Constantin & Pechenizkiy, Mykola 2021c Do
we actually need dense over-parameterization? in-time over-parameterization in sparse
training. In Proceedings of the 39th International Conference on Machine Learning, pp.
6989–7000. PMLR.

14
Shiwei Liu, Zhangyang Wang
Lym, Sangkug, Choukse, Esha, Zangeneh, Siavash, Wen, Wei, Sanghavi, Sujay &
Erez, Mattan 2019 Prunetrain: fast neural network training by dynamic sparse model
reconfiguration. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1–13.
Ma, Xiaolong, Niu, Wei, Zhang, Tianyun, Liu, Sijia, Lin, Sheng, Li, Hongjia, Wen,
Wujie, Chen, Xiang, Tang, Jian, Ma, Kaisheng & others 2020 An image enhancing
pattern-based sparsity for real-time inference on mobile devices. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
Part XIII 16, pp. 629–645. Springer.
Ma, Xiaolong, Yuan, Geng, Shen, Xuan, Chen, Tianlong, Chen, Xuxi, Chen,
Xiaohan, Liu, Ning, Qin, Minghai, Liu, Sijia, Wang, Zhangyang & others 2021
Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? Advances
in Neural Information Processing Systems 34, 12749–12760.
Ma, Yi, Tsao, Doris & Shum, Heung-Yeung 2022 On the principles of parsimony and
self-consistency for the emergence of intelligence. Frontiers of Information Technology &
Electronic Engineering 23 (9), 1298–1323.
Mao, Huizi, Han, Song, Pool, Jeff, Li, Wenshuo, Liu, Xingyu, Wang, Yu & Dally,
William J 2017 Exploring the granularity of sparsity in convolutional neural networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 13–20.
Mocanu, Decebal Constantin, Mocanu, Elena, Nguyen, Phuong H, Gibescu,
Madeleine & Liotta, Antonio 2016 A topological insight into restricted boltzmann
machines. Machine Learning 104, 243–270.
Mocanu, Decebal Constantin, Mocanu, Elena, Stone, Peter, Nguyen, Phuong H,
Gibescu, Madeleine & Liotta, Antonio 2018 Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by network science. arXiv:1707.04780.
Nature communications. 9 (1), 2383.
Molchanov, Pavlo, Tyree, Stephen, Karras, Tero, Aila, Timo & Kautz, Jan 2016
Pruning convolutional neural networks for resource efficient inference. arXiv preprint
arXiv:1611.06440 .
Mostafa, Hesham & Wang, Xin 2019 Parameter efficient training of deep convolutional neural
networks by dynamic sparse reparameterization. International Conference on Machine
Learning .
Mozer, Michael C & Smolensky, Paul 1989 Using relevance to reduce network size
automatically. Connection Science 1 (1), 3–16.
Patterson, David, Gonzalez, Joseph, Le, Quoc, Liang, Chen, Munguia, Lluis-
Miquel, Rothchild, Daniel, So, David, Texier, Maud & Dean, Jeff 2021 Carbon
emissions and large neural network training. arXiv preprint arXiv:2104.10350 .
Peste, Alexandra, Iofinova, Eugenia, Vladu, Adrian & Alistarh, Dan 2021 Ac/dc:
Alternating compressed/decompressed training of deep neural networks. Advances in
Neural Information Processing Systems 34, 8557–8570.
Ramesh, Aditya, Dhariwal, Prafulla, Nichol, Alex, Chu, Casey & Chen, Mark
2022 Hierarchical text-conditional image generation with clip latents. arXiv preprint
arXiv:2204.06125 .
Renda, Alex, Frankle, Jonathan & Carbin, Michael 2020 Comparing rewinding
and fine-tuning in neural network pruning. In International Conference on Learning
Representations.
Riquelme, Carlos, Puigcerver, Joan, Mustafa, Basil, Neumann, Maxim, Jenatton,
Rodolphe, Susano Pinto, André, Keysers, Daniel & Houlsby, Neil 2021 Scaling
vision with sparse mixture of experts. Advances in Neural Information Processing Systems
34, 8583–8595.
Roller, Stephen, Sukhbaatar, Sainbayar, Weston, Jason & others 2021 Hash layers
for large sparse models. Advances in Neural Information Processing Systems 34, 17555–
17566.
Rumi, Masuma Akter, Ma, Xiaolong, Wang, Yanzhi & Jiang, Peng 2020 Accelerating
sparse cnn inference on gpus with performance-aware weight pruning. In Proceedings of
the ACM International Conference on Parallel Architectures and Compilation Techniques,
pp. 267–278.

Ten Lessons We Have Learned in the New “Sparseland”
15
Schwarz, Jonathan, Jayakumar, Siddhant, Pascanu, Razvan, Latham, Peter E
& Teh, Yee 2021 Powerpropagation: A sparsity inducing weight reparameterisation.
Advances in Neural Information Processing Systems 34, 28889–28903.
Shazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof, Davis, Andy, Le, Quoc,
Hinton, Geoffrey & Dean, Jeff 2017 Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 .
Sokar, Ghada, Mocanu, Elena, Mocanu, Decebal Constantin, Pechenizkiy, Mykola
& Stone, Peter 2021 Dynamic sparse training for deep reinforcement learning. arXiv
preprint arXiv:2106.04217 .
Strubell, Emma, Ganesh, Ananya & McCallum, Andrew 2019 Energy and policy
considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243 .
Sun, Yiyou & Li, Yixuan 2022 Dice: Leveraging sparsification for out-of-distribution
detection. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XXIV , pp. 691–708. Springer.
T, Mukund Varma, Chen, Xuxi, Zhang, Zhenyu, Chen, Tianlong, Venugopalan,
Subhashini & Wang, Zhangyang 2022 Sparse winning tickets are data-efficient image
recognizers. In Advances in Neural Information Processing Systems (ed. Alice H. Oh,
Alekh Agarwal, Danielle Belgrave & Kyunghyun Cho).
Tanaka, Hidenori, Kunin, Daniel, Yamins, Daniel LK & Ganguli, Surya 2020 Pruning
neural networks without any data by iteratively conserving synaptic flow. Advances in
Neural Information Processing Systems. arXiv:2006.05467 .
Toon, Nigel 2020 Introducing 2nd generation ipu systems for ai at scale.
Valero-Lara, Pedro, Martínez-Pérez, Ivan, Sirvent, Raül, Martorell, Xavier
&
Pena,
Antonio
J
2018
Nvidia
gpus
scalability
to
solve
multiple
(batch)
tridiagonal systems implementation of cuthomasbatch. In Parallel Processing and Applied
Mathematics: 12th International Conference, PPAM 2017, Lublin, Poland, September 10-
13, 2017, Revised Selected Papers, Part I , pp. 243–253. Springer.
Voita, Elena, Talbot, David, Moiseev, Fedor, Sennrich, Rico & Titov, Ivan 2019
Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can
be pruned. arXiv preprint arXiv:1905.09418 .
Wan, Li, Zeiler, Matthew, Zhang, Sixin, Le Cun, Yann & Fergus, Rob 2013
Regularization of neural networks using dropconnect. In International conference on
machine learning, pp. 1058–1066. PMLR.
Wang, Chaoqi, Zhang, Guodong & Grosse, Roger 2020 Picking winning tickets
before training by preserving gradient flow. In International Conference on Learning
Representations.
Wang, Huan, Qin, Can, Bai, Yue & Fu, Yun 2023 Why is the state of neural network
pruning so confusing? on the fairness, comparison setup, and trainability in network
pruning. arXiv preprint arXiv:2301.05219 .
Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran & Li, Hai 2016 Learning
structured sparsity in deep neural networks. Advances in neural information processing
systems 29.
Ye, Shaokai, Xu, Kaidi, Liu, Sijia, Cheng, Hao, Lambrechts, Jan-Henrik, Zhang,
Huan, Zhou, Aojun, Ma, Kaisheng, Wang, Yanzhi & Lin, Xue 2019 Adversarial
robustness vs. model compression, or both? In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 111–120.
Yen, Ian En-Hsu, Xiao, Zhibin & Xu, Dongkuan 2022 S4: a high-sparsity, high-performance
ai accelerator. arXiv preprint arXiv:2207.08006 .
Yin, Lu, Liu, Shiwei, Meng, Fang, Huang, Tianjin, Menkovski, Vlado & Pechenizkiy,
Mykola 2022a Lottery pools: Winning more by interpolating tickets without increasing
training or inference cost. arXiv preprint arXiv:2208.10842 .
Yin, Lu, Menkovski, Vlado, Fang, Meng, Huang, Tianjin, Pei, Yulong, Pechenizkiy,
Mykola, Mocanu, Decebal Constantin & Liu, Shiwei 2022b Superposing many
tickets into one: A performance booster for sparse neural network training. arXiv preprint
arXiv:2205.15322 .
You, Haoran, Li, Chaojian, Xu, Pengfei, Fu, Yonggan, Wang, Yue, Chen, Xiaohan,
Baraniuk, Richard G, Wang, Zhangyang & Lin, Yingyan 2020 Drawing early-bird

16
Shiwei Liu, Zhangyang Wang
tickets: Toward more efficient training of deep networks. In International Conference on
Learning Representations.
Yuan, Geng, Ma, Xiaolong, Niu, Wei, Li, Zhengang, Kong, Zhenglun, Liu, Ning,
Gong, Yifan, Zhan, Zheng, He, Chaoyang, Jin, Qing & others 2021 Mest:
Accurate and fast memory-economic sparse training framework on the edge. Advances
in Neural Information Processing Systems 34, 20838–20850.
Zhang, Tianyun, Ye, Shaokai, Feng, Xiaoyu, Ma, Xiaolong, Zhang, Kaiqi, Li,
Zhengang, Tang, Jian, Liu, Sijia, Lin, Xue, Liu, Yongpan & others 2021
Structadmm: Achieving ultrahigh efficiency in structured pruning for dnns. IEEE
transactions on neural networks and learning systems 33 (5), 2259–2273.
Zhang, Yihua, Yao, Yuguang, Ram, Parikshit, Zhao, Pu, Chen, Tianlong, Hong,
Mingyi, Wang, Yanzhi & Liu, Sijia 2022 Advancing model pruning via bi-level
optimization. arXiv preprint arXiv:2210.04092 .
Zhou, Aojun, Ma, Yukun, Zhu, Junnan, Liu, Jianbo, Zhang, Zhijie, Yuan, Kun, Sun,
Wenxiu & Li, Hongsheng 2021 Learning n:m fine-grained structured sparse neural
networks from scratch. In International Conference on Learning Representations.

