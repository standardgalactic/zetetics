Masked Attention is All You Need for Graphs
David Buterez 1 Jon Paul Janet 2 Dino Oglic 3 Pietro Lio 1
Abstract
Graph neural networks (GNNs) and variations
of the message passing algorithm are the pre-
dominant means for learning on graphs, largely
due to their flexibility, speed, and satisfactory
performance. The design of powerful and gen-
eral purpose GNNs, however, requires significant
research efforts and often relies on handcrafted,
carefully-chosen message passing operators. Mo-
tivated by this, we propose a remarkably simple
alternative for learning on graphs that relies ex-
clusively on attention. Graphs are represented
as node or edge sets and their connectivity is en-
forced by masking the attention weight matrix,
effectively creating custom attention patterns for
each graph. Despite its simplicity, masked atten-
tion for graphs (MAG) has state-of-the-art per-
formance on long-range tasks and outperforms
strong message passing baselines and much more
involved attention-based methods on over 55 node
and graph-level tasks. We also show significantly
better transfer learning capabilities compared to
GNNs and comparable or better time and memory
scaling. MAG has sub-linear memory scaling in
the number of nodes or edges, enabling learning
on dense graphs and future-proofing the approach.
1. Introduction
The field of geometric deep learning (GDL) seeks to de-
scribe, understand, and even “unify” deep learning strategies
for data structures such as sets, grids, and graphs by leverag-
ing the fundamental concepts of symmetry and invariance.
A remarkably successful application of GDL is learning on
graphs, abstractions that represent relationships between
items of a set, and which naturally describe real-world phe-
nomena such as social, biological, or transportation net-
works, as well as objects like molecules. In particular, deep
1Department of Computer Science and Technology, University
of Cambridge, Cambridge, UK 2Molecular AI, BioPharmaceu-
ticals R&D, AstraZeneca, Gothenburg, Sweden 3Centre for AI,
BioPharmaceuticals R&D, AstraZeneca, Cambridge, UK.
learning for molecules has the potential to accelerate and
even revolutionise fields such as drug discovery and ma-
terials science, being one of the main factors behind the
accelerated development of GDL, as well as one of the ear-
liest (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer
et al., 2017). General purpose learning on graphs is typically
specified as an instance of message passing, an iterative al-
gorithm where one must define a message function which
aggregates information from a given node’s neighbourhood,
as well as a node (possibly also edge) update function to
incorporate the encoded messages.
Despite the overall success and wide adoption of GNNs,
several fundamental problems have been highlighted over
time. Firstly, although the message passing framework is
highly customisable through user-defined, learnable mes-
sage and node update functions, the design of novel layers
is a difficult research problem, where improvements take
years to achieve and often rely on hand-crafted operators.
This is particularly the case for general purpose GNNs that
do not exploit additional input modalities such as atomic
coordinates. For example, principal neighbourhood aggre-
gation (PNA) (Corso et al., 2020) is regarded as one of
the most powerful message passing layers, but it is built
using a collection of hand-picked neighbourhood aggrega-
tion functions, it requires a dataset degree histogram which
must be pre-computed prior to learning, and further uses
hand-picked degree scalers. Another example is given by
Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2018),
one of the most popular graph layers and one of the earliest
efforts to combine attention with GNNs. It has been shown,
afterwards, that the original formulation of GAT has lim-
ited expressive power, and that a simple reordering of the
operations can improve performance (Brody et al., 2022).
Secondly, the nature of message passing imposes certain
limitations which have shaped most of the GNN literature.
One of the most prominent examples is the readout function
used to combine node-level representations into a single
graph-level representation, and which is required to be per-
mutation invariant with respect to the node order. Thus,
the default choice for most GNNs remains a simple, non-
learnable function such as sum, mean, or max, despite the
potential limited expressivity. Recently, it has been shown
that breaking this constraint can lead to improved perfor-
mance, and might be acceptable in certain scenarios such as
1
arXiv:2402.10793v1  [cs.LG]  16 Feb 2024

Masked Attention is All You Need for Graphs
learning on molecules, where the inputs can be presented
in a canonical order (Buterez et al., 2022). Furthermore,
expressive readout functions that are based on Set Trans-
formers (Lee et al., 2019) have been shown to consistently
improve performance of GNNs regardless of the underlying
message passing algorithm (Buterez et al., 2022; 2023b).
Thirdly, the majority of GNN architectures are plagued by
the well-known oversmoothing and oversquashing prob-
lems that limit the depth of GNNs, as well as difficulties
in modelling long-range relationships, all of which are con-
sequences of aggregating information from exponentially
larger neighbourhoods (Alon & Yahav, 2021). The proposed
solutions typically take the form of message regularisation
schemes (Godwin et al., 2022; Zhao & Akoglu, 2020; Cai
et al., 2021). However, there is generally not a consensus on
the right architectural choices for deep GNNs. Separately,
standard GNNs have also shown limitations in terms of
transfer learning and strategies such as pre-training and fine-
tuning, as opposed to other families of neural networks such
as language models (Hu et al., 2020). For certain types of
data and tasks, non-standard GNNs that leverage attention-
based readouts are currently the only way to effectively
perform transfer learning (Buterez et al., 2024; 2023a).
Perhaps due to the progress that message passing neural
networks have enabled on a wide range of tasks, alternative
paradigms for learning on graphs are relatively underdevel-
oped. The attention mechanism (Vaswani et al., 2017) is
one of the main sources of innovation within graph learning,
either by directly incorporating attention within message
passing (Veliˇckovi´c et al., 2018; Brody et al., 2022), by for-
mulating graph learning as a language processing task (Ying
et al., 2021; Kim et al., 2022), or by combining standard
GNN layers with an attention mechanism (Rampasek et al.,
2022; Shirzad et al., 2023; Buterez et al., 2022; 2023b).
Here, we propose a novel graph learning framework charac-
terised by its simplicity and lack of message passing layers,
instead being based on a classical attention mechanism with
masking to create custom attention patterns. Compared
to some of the previous works that have formulated graph
learning as a language modelling task, we take a different
approach and consider graph learning as a learning task
on sets, where the graph connectivity (i.e., adjacency ma-
trix) is enforced by masking the pairwise attention weight
matrix and allowing only values that correspond to graph
connections. We term this architecture Masked Attention for
Graphs (MAG) and demonstrate that it can be customised
to propagate information across nodes (MAGN) or edges
(MAGE). It is general purpose, in the sense that it only
relies on the graph structure and possibly node and edge
features, and it is not restricted to a particular domain such
as chemistry. The simplicity of the architecture is further
demonstrated by the fact that MAG does not use positional
or similar encodings, it does not encode graph structures as
tokens or other language (sequence) specific concepts, and
it does not require any pre-computations.
Despite its simplicity, MAG generally outperforms strong
message passing baselines and much more involved
attention-based algorithms. Our empirical evaluation covers
long-range molecular benchmarks and over 55 tasks from
different domains such as quantum mechanics, molecular
docking, physical chemistry, biophysics, bioinformatics,
computer vision, social networks, functional call graphs,
and synthetic graphs. This emphasises the fact that the
carefully-selected and hand-crafted nature of most message
passing algorithms can be easily superseded by attention
itself, without the need to explicitly define any operator.
Beyond benchmarking, we also explore a recent research
direction that showed the transformative benefits of transfer
learning in drug discovery and quantum mechanics (Buterez
et al., 2024). Here, we leverage a newly-published, refined
version of the QM9 dataset at a higher level of theory (Fediai
et al., 2023), and show that MAG is a viable and well per-
forming transfer learning strategy, while GNNs are limited.
MAG is arguably one of the most straightforward ways to ap-
ply attention on graphs. Thanks to modern implementations
of exact attention, MAG scales sub-linearly in terms of mem-
ory with the number of nodes (MAGN) or edges (MAGE),
as it largely relies on (masked) self-attention. Although
current libraries are not optimised for masked attention and
many optimisations are possible (see Appendix A), both the
training time and the memory consumption are competitive
or even better than plain message passing.
2. Related Work
Graph neural networks with adaptive readouts – A re-
cent research trend consists of following standard GNN
layers with an attention-based pooling (readout) function,
for example using the Set Transformer (Buterez et al., 2022;
2024; 2023b), or standard Transformers (Jain et al., 2021).
The change to a more expressive readout function has pro-
vided consistent uplifts in most supervised learning tasks,
and has enabled easy and effective transfer learning for
molecules through a pre-training and fine-tuning workflow.
Transformers for graphs – One of the most popular ap-
proaches is the direct application of standard Transformers
designed for language modelling to graphs. Graphormer
(Ying et al., 2021) achieves this through an involved and
computation-heavy suite of pre-processing steps, involving
a centrality, spatial, and edge encoding. Graphormer models
the mean readout function from GNNs through a special
“virtual” node that is added to the graph and connected to
every other node. Another approach within this paradigm is
the Tokenized Graph Transformer (TokenGT) (Kim et al.,
2

Masked Attention is All You Need for Graphs
Node masking
n6
n1
n2
n3
n4
n5
n7
n8
n9
n1
n2
n3
n4
n5
n6
n7
n8
n9
n1
n2
n3
n4
n5
n6
n7
n8
n9
Node adjacency matrix
Edge masking
Edge adjacency matrix
e1
e2
e3
e5
e6
e7
e8
e4
e1
e2
e3
e4
e5
e6
e7
e8
e1
e2
e3
e4
e5
e6
e7
e8
or
or
n1
n2
nN
...
(
)
...
(
)
e1
e2
eM
Input set
Node features
Edge features
or
Self Attention Block
Masked Self Attention Block
Self Attention Block
Masked Self Attention Block
Encoder
Pooling by Multihead Attention
Self Attention Block
Decoder
Figure 1. Overview of the Masked Attention for Graphs (MAG) architecture. An important aspect is the choice of processing the node
feature set (MAGN) or the edge feature set (MAGE), and the appropriate masking algorithm. Conventional self attention blocks (S) can be
alternated with masked variants (M) as desired (here, a choice of SMSM is depicted). The decoder is only required for graph-level tasks.
2022), which treats all the nodes and edges as independent
tokens. To adapt sequence learning to the graph domain, To-
kenGT encodes the graph information using node identifiers
derived from orthogonal random features or Laplacian eigen-
vectors, and learnable type identifiers for nodes and edges.
Such models rely on embedding layers and are thus limited
to integer node or edge features. Moreover, the high number
of sequence-based architectures (e.g. Big Bird (Zaheer et al.,
2020), Performer (Choromanski et al., 2021), etc.) repre-
sents a large unexplored territory for graphs. Spectral Atten-
tion Networks (Kreuzer et al., 2021) use a computationally-
expensive positional encoding based on the Laplacian and
two complementary attention mechanisms over nodes. A
different philosophy is that of a message passing and Trans-
former hybrid. Such an approach is taken by the GraphGPS
framework (Ramp´aˇsek et al., 2022), which alternates mes-
sage passing layers with Transformer layers. Like previous
works, GraphGPS puts a large emphasis on different types of
encodings, proposing and analysing positional and structural
encodings, further divided into local, global, and relative
encodings. Exphormer (Shirzad et al., 2023) is an evolution
of GraphGPS which adds virtual global nodes and sparse
attention based on expander graphs. While effective, such
frameworks do still rely on message passing and are thus
not purely attention-based solutions. Limitations include de-
pendence on approximations (Performer for both, expander
graphs for Exphormer), decreased performance when encod-
ings (GraphGPS) or special nodes (Exphormer) are removed,
and scalability (GraphGPS). Notably, Exphormer is the first
work to consider custom attention patterns for graphs in the
form of node-level neighbourhoods.
3. Preliminaries
Graphs – A graph is a tuple G = (V, E) where V repre-
sents the set of nodes, E ⊆V × V is the set of edges, and
Nn = |V|, Ne = |E|. Nodes are associated with feature
vectors xu of dimension dn for all nodes u ∈V, and de-
dimensional edge features euv for all edges e ∈E. The node
features are collected as rows in a matrix X ∈RNn×dn, and
similarly for edge features into E ∈RNe×de. The graph
connectivity information can be represented as an adjacency
matrix A, where Auv = 1 if (u, v) ∈E and Auv = 0 other-
wise, although an edge list representation is often more prac-
tical. GNNs also define the neighbourhood of a node u ∈V
as Nu = {v | (u, v) ∈E ∨(v, u) ∈E}. Many GNNs can
be described as a form of message passing, which takes the
general form of mt+1
u
= P
v∈Nu messaget (xt
u, xt
v, euv)
and xt+1
u
= updatet
 xt
u, mt+1
u

(Gilmer et al., 2017),
where message and update are the message and node
update functions, respectively, and t is the current time step.
As described above, message passing is highly customis-
able, for example by novel definitions of the message and
update functions, their inputs, or even adding new update
steps such as the graph-level state embedding and update
proposed in Chen et al. (2021) for multi-fidelity applica-
tions. For graph-level prediction tasks, a readout or pooling
function L must be used, aggregating all the learnt node
representations: xG = L
u∈V (xu).
Set Transformer – The Set Transformer is an encoder-
decoder attention-based architecture for learning on sets. It
leverages the scaled dot product and multihead attention
3

Masked Attention is All You Need for Graphs
Algorithm 1: Node masking algorithm, where the
inputs follow PyTorch Geometric conventions.
1 from torch geometric import unbatch edge index
2 function node mask(b ei, b map, B, M)
3
# b ei is the batched edge index
4
# b map maps nodes to graphs
5
# B, M batch, respectively mask size
6
mask ←torch.full(size=(B, M, M), fill=False)
7
graph idx ←b map.index select(0, b ei[0, :])
8
eis ←unbatch edge index(b ei, b map)
9
ei ←torch.cat(eis, dim=1)
10
mask[graph idx, ei[0, :], ei[1, :]] ←True
11
return ∼mask
mechanisms proposed in Vaswani et al. (2017) to define
multihead attention blocks (MABs), self attention blocks
(SABs), and pooling by multihead attention (PMA) blocks.
We do not recapitulate the definition of multihead attention
and assume instead that a function Multihead (Q, K, V)
is available (for query, key, and value matrices, respec-
tively). The original Set Transformer is given by ST(X) =
Decoder(Encoder(X)) with:
MAB(X, Y) = H + Linearϕ(H)
(1)
and H = X + Multihead(X, Y, Y),
(2)
SAB(X) = MAB (X, X) ,
(3)
Encoder(X) = SABn (X) ,
(4)
PMAk(Z) = MAB (Sk, Linearϕ(Z)) ,
(5)
Decoder(Z) = Linearϕ (SABn (PMAk(Z))) .
(6)
and where Linearϕ is a linear layer followed by an activation
function ϕ, SABn(·) represents n subsequent applications
of a SAB, and Sk is a tensor of k learnable seed vectors that
are randomly initialised (PMAk outputs k vectors) (Buterez
et al., 2023b). Notably, the original Set Transformer is
designed exclusively for learning on sets, uses layer normal-
isation (LN) in a post-LN fashion (Xiong et al., 2020a), and
does not use any form of positional or structural encodings.
Efficient attention – Recently, Flash attention has enabled
exact attention with linear memory scaling and faster train-
ing and inference (up to x10) thanks to more efficiently util-
ising the architecture of modern graphics processing units
(GPUs) (Dao et al., 2022; Dao, 2023). For self-attention, an
exact attention implementation has also been developed that
scales with the square root of the sequence length in terms
of memory, and has comparable run time with standard
implementations (Rabe & Staats, 2021).
4. Masked attention for graphs
We formulate graph learning as a learning problem on sets.
The main learning mechanism consists of applying attention
directly to the node or edge feature matrix by means of
SABs. We propose masking the pairwise attention weight
matrix according to the node or edge adjacency information
in order to incorporate the graph structure. We call this
approach Masked Attention for Graphs (MAG).
Inputs – MAG supports two main ways of information
propagation: (1) on nodes (MAGN), more specifically on
the node feature matrix X, or (2) on edges (MAGE), i.e. on
the edge feature matrix E.
Masking – We extend the Set Transformer with masked
equivalents of the MAB and SAB (MSAB). Practically, this
means supplying the new blocks with a mask tensor of shape
B×Nd×Nd for MAGN and B×Ne×Ne for MAGE, where
B is the batch size. In a naive implementation, masking sim-
ply means replacing the targeted values of the scaled QKT
product with negative infinity (or a very large negative value
for stability) before softmax (Appendix F for the equation).
The correct mask for each batch is different and must thus
be computed on the fly. For MAGN, the mask allows only
adjacent nodes. In other words, the Nd × Nd portion of
the mask corresponds directly to the node adjacency matrix
(Algorithm 1). For MAGE, the mask operates on the set
of edges and must allow only edges that share a common
node. While this computation is not as trivial as the MAGN
mask, both kinds of masks can be efficiently computed using
exclusively tensor operations (Algorithm 2).
Architecture – At a high level, a complete MAG model
takes the form of an encoder where MSAB and SAB blocks
are alternated as desired, with a PMA-based decoder (Fig-
ure 1). Compared to the Set Transformer, we have also
adapted MAG to use a pre-LN architecture with layer or
batch normalisation and optionally include multi-layer per-
ceptrons (MLPs) after multihead attention. For graph-level
tasks, the PMA module of the Set Transformer acts as an
equivalent to the readout function in GNNs, but fully based
on attention, while for node-level tasks PMA is not required.
5. Experiments
Our evaluation encompasses: (1) an extensive suite of over
55 benchmarks from various domains, where the goal is
to compare MAG with representative message passing net-
works (GCN, GAT, GATv2, GIN, and PNA) and Trans-
formers for graphs (Graphormer, TokenGT). Due to space
limitations, we cannot present the results for all methods in
the main part of the paper, so we select the most represen-
tative ones based on the task, with the rest presented in the
Appendix. Graphormer and TokenGT are applicable only
to a subset of tasks due to limitations discussed in Related
Work. They also fail for many datasets due to very high
memory requirements (CPU and/or GPU). We cover long-
range tasks (Section 5.1), node-level tasks (Section 5.2) and
graph-level tasks (Section 5.3). (2) A transfer learning per-
formance evaluation (Section 5.4) based on a new, refined
4

Masked Attention is All You Need for Graphs
Algorithm 2: Edge masking algorithm, where the inputs follow PyTorch Geometric conventions. T is the transpose.
Helper functions are explained in Appendix B.
1 from mag import consecutive, first unique index
2 function edge adjacency(b ei)
3
E ←b ei.size(1)
4
source nodes ←b ei[0]
5
target nodes ←b ei[1]
6
7
# unsqueeze and expand
8
exp src ←source nodes.unsq(1).exp((-1, E))
9
exp trg ←target nodes.unsq(1).exp((-1, E))
10
11
src adj ←exp src == T(exp src)
12
trg adj ←exp trg == T(exp trg)
13
cross ←(exp src == T(exp trg)) logical or
14
(exp trg == T(exp src))
15
16
return (src adj logical or trg adj logical or cross)
17 function edge mask(b ei, b map, B, M)
18
mask ←torch.full(size=(B, M, M), fill=False)
19
edge to graph ←b map.index select(0, b ei[0, :])
20
21
edge adj ←edge adjacency(b ei)
22
ei to original ←consecutive(
23
first unique index(edge to graph), b ei.size(1))
24
25
edges ←edge adj.nonzero()
26
graph idx ←edge to graph.idx select(0, edges[:, 0])
27
coord 1 ←ei to original.idx select(0, edges[:, 0])
28
coord 2 ←ei to original.idx select(0, edges[:, 1])
29
30
mask[graph idx, coord 1, coord 2] ←True
31
return ∼mask
Table 1. Test set mean absolute error (MAE) or average precision (AP) for two long-range molecular benchmarks. All the results except
for MAGE are extracted from (T¨onshoff et al., 2023). The number of layers for PEPT-STRUCT, respectively PEPT-FUNC is given as (·/·).
Dataset
GCN (6/6)
GIN (10/8)
GraphGPS (8/6)
Exphormer (4/8)
MAGE (3/4)
PEPT-STRUCT (MAE ↓)
0.2460 ± 0.0007
0.2473 ± 0.0017
0.2509 ± 0.0014
0.2481 ± 0.0007
0.2453 ± 0.0003
PEPT-FUNC (AP ↑)
0.6860 ± 0.0050
0.6621 ± 0.0067
0.6534 ± 0.0091
0.6527 ± 0.0043
0.6863 ± 0.0044
Table 2. Test set Matthews correlation coefficient (MCC) for 3 node-level classification tasks, presented as mean ± standard deviation
from 5 different runs. The highest mean values are highlighted in bold.
Dataset
GCN
GAT
GATv2
GIN
PNA
MAGN
PPI ↑
0.47 ± 0.02
0.82 ± 0.03
0.72 ± 0.39
0.35 ± 0.02
0.83 ± 0.01
0.99 ± 0.00
CITESEER ↑
0.30 ± 0.04
0.19 ± 0.03
0.21 ± 0.05
0.23 ± 0.03
0.01 ± 0.02
0.54 ± 0.02
CORA ↑
0.48 ± 0.06
0.37 ± 0.01
0.43 ± 0.04
0.39 ± 0.02
0.04 ± 0.05
0.70 ± 0.02
variant of QM9 and following the recent framework pro-
posed by Buterez et al. (2024). (3) The time and memory
characteristics for all discussed methods (Section 5.5).
5.1. Long-range tasks
Graph learning with Transformers has traditionally been
evaluated on long-range graph benchmarks (LRGB)
(Dwivedi et al., 2022). However, it was recently shown
that simple GNNs outperform most attention-based meth-
ods (T¨onshoff et al., 2023). Nonetheless, we evaluated
MAGE on two long-range molecular tasks (Table 1) and
conclude that it outperforms GraphGPS, Exphormer, GCN,
and GIN. Despite using half the number of layers as other
methods or less, MAGE matches the 2nd model on the
PEPT-STRUCT leaderboard, and is within the top 5 for PEPT-
FUNC (as of January 2024). Remarkably, MAG is the only
top method that is exclusively based on attention (i.e. no
message passing or hybrid), does not use any positional or
similar encoding, and is general purpose (i.e. not specifically
built for molecules). We also did not use hyperparameter
optimisation or sophisticated schedulers (Appendix C).
5.2. Node-level tasks
Generally, node-level tasks take the form of citation net-
works of different sizes, and are not as varied as graph-level
problems. Nonetheless, they represent an interesting case
for MAG as PMA is not needed. Here, we selected 3 repre-
sentative datasets: PPI, CITESEER, and CORA. In particular,
MAGN is the most natural choice as it works over node
representations. Our results indicate that MAGN is the best
performing method by a large margin (Table 2). Graphormer
and TokenGT are not available for node-level classification,
and they would not work due to the large graph sizes.
5.3. Graph-level tasks
Graph-level tasks are generally more varied, as they often
originate from different domains and require a readout func-
tion for GNNs and a PMA module for MAG. Here, we
do not focus on the differences between readouts and in-
stead choose a reasonably strong baseline (4-layer GNNs
and mean readout; Appendix C for the experimental setup).
A more granular evaluation of readouts has been done by
(Buterez et al., 2022), concluding that the difference be-
tween readouts is small for most tasks.
5

Masked Attention is All You Need for Graphs
QM9 is a quantum mechanics dataset consisting of 133,885
small organic molecules and 19 regression targets given by
quantum properties (Ramakrishnan et al., 2014). We report
results for all 19 targets in Table 3, with GCN and TokenGT
separately in Appendix D due to limited space. We observe
that for 15 out of 19 properties, MAGE is the best perform-
ing method. For the dipole moment (µ), PNA is better by a
slight margin, while for HOMO, LUMO, and the HOMO-
LUMO gap (∆ϵ), Graphormer is stronger. We expect that
attention-based methods would be the best suited for inten-
sive and localised properties like the HOMO and LUMO,
which agrees with recent literature on this topic (Buterez
et al., 2023b). The fact that MAGE is not as competitive
on these tasks could be explained by MAGE converging
quicker than other methods and getting stuck in local min-
ima, especially for HOMO/LUMO which take a long time to
converge for most methods. We report the results of altering
the number and order of attention blocks in Appendix I.
DOCKSTRING (Garc´ıa-Orteg´on et al., 2022) is a recent drug
discovery data collection consisting of molecular docking
scores for 260,155 small molecules and 5 high-quality tar-
gets from different protein families that were selected as
a regression benchmark, with different levels of difficulty:
PARP1 (enzyme, easy), F2 (protease, easy to medium), KIT
(kinase, medium), ESR2 (nuclear receptor, hard), and PGR
(nuclear receptor, hard). The tasks are expected to be chal-
lenging, as the docking score depends on the 3D structure
of the ligand–target complex. Furthermore, a cluster split
into train (221,274 molecules) and test (38,881 molecules)
sets is provided, ensuring a more difficult and meaningful
benchmark. We have selected a random subset of 19,993
molecules from the original train set as a validation set (the
test set is not modified). We report results for the 5 targets in
Table 3 and observe that MAGE is the strongest method for
4 of the tasks. Remarkably, MAGE matches or outperforms
the strongest methods in the original manuscript (Attentive
FP, a GNN based on attention, Xiong et al., 2020b) despite
using 20,000 less training molecules. Moreover, while the
other methods are competitive for the 4 easy or medium
difficulty tasks, MAGE outperforms the others by a large
margin for the most difficult target (PGR).
We further extend our evaluation with a collection of
datasets that covers multiple domains such as bioinformat-
ics, physical chemistry, computer vision, social networks,
functional call graphs, and synthetic graphs (Tables 3 to 5).
We observe that MAGE and MAGN are competitive with
the other methods and in most cases better.
5.4. Transfer learning
We follow the recipe recently outlined for drug discovery
and quantum mechanics by Buterez et al. (2024). Here, we
leverage a recently-published refined version of the QM9
HOMO and LUMO energies (Fediai et al., 2023), which pro-
vides alternative DFT calculations based on the correlation-
consistent basis set aug-cc-DZVP and the PBE functional,
as well as calculations at the (more accurate) eigen-value-
self-consistent GW level of theory. The transfer learning
setup consists of randomly selected training, validation, and
test sets of 25K, 5K, and respectively 10K molecules with
GW calculations (from the total 133,885). As outlined by
Buterez et al. (2024), transfer learning can be performed in
transductive or inductive setups. In the transductive case,
test set DFT-level measurements are used for pre-training,
while in the inductive setting they are not. Here, we perform
transfer learning by pre-training a model on the DFT target
for a fixed number of epochs and then fine-tuning it on the
subset of 25K GW calculations. In the transductive case,
pre-training occurs on the full set of 133K DFT calculations,
while in the inductive case the DFT test set values are re-
moved (note that the evaluation is done on the test set GW
values). The results (Table 6) indicate that MAGE improves
thanks to transfer learning from DFT by 45% (HOMO) and
53% (LUMO) in the challenging inductive case, with 10
to 20-fold improvements for the transductive case, while
GNNs improve only by a modest amount.
5.5. Time and memory utilisation
In MAG, the most computation-intensive component is the
encoder. All encoder blocks perform (masked) self attention,
which can be efficiently implemented with O(
√
N) memory
complexity (Nn, Ne for MAGN, respectively MAGE). The
time complexity is as for standard attention. The decoder
PMA encodes cross-attention between the full set outputted
by the encoder and a set of learnable k vectors, benefitting
from Flash attention (linear memory and time scaling). A
decoder with a single PMA block and k = 1, as used here,
is even more efficient. The mask tensor requires B × N 2
memory (Appendix A for a solution). However, it does not
require gradients and MAGE runs with up to Ne ≈30, 000
edges on a consumer GPU with 24GB. Indeed, we report
competitive time and memory utilisation (Table 7).
6. Discussion
We presented an end-to-end approach that leverages atten-
tion in a novel way for learning on graphs and demonstrated
its effectiveness relative to message passing and more in-
volved attention-based methods. Our approach is end-to-
end in the sense that it replaces both message passing and
readout/pooling functions with attention mechanisms. The
former is facilitated by modularly stacking self and masked
attention blocks (node or edge-based masking). In the fu-
ture, node- and edge-masked blocks might be interspersed.
Both node and edge masked attention mechanisms can be
implemented using a few lines of code (Algorithms 1 and 2).
6

Masked Attention is All You Need for Graphs
Table 3. Test set root mean squared error (RMSE, standard for quantum mechanics) QM9, and R2 for the rest of tasks, presented as
mean ± standard deviation from 5 different runs. The lowest (QM9) and highest (rest) mean values are highlighted in bold. Only one of
Graphormer/TokenGT was chosen for spacing reasons, based on competitiveness and lack of out-of-memory errors (OOM). Any results
not displayed here (e.g. GCN, TokenGT) are available in Appendix D.
Property
GAT
GATv2
GIN
PNA
Graphormer
MAGE
QM9
µ ↓
0.61 ± 0.00
0.61 ± 0.01
0.61 ± 0.00
0.57 ± 0.00
0.63 ± 0.00
0.61 ± 0.02
α ↓
2.66 ± 0.25
1.86 ± 0.28
1.18 ± 0.10
1.00 ± 0.02
0.57 ± 0.16
0.48 ± 0.01
ϵHOMO ↓
0.12 ± 0.00
0.12 ± 0.00
0.12 ± 0.00
0.11 ± 0.00
0.10 ± 0.00
0.11 ± 0.00
ϵLUMO ↓
0.14 ± 0.00
0.13 ± 0.00
0.13 ± 0.00
0.11 ± 0.00
0.11 ± 0.00
0.13 ± 0.00
∆ϵ ↓
0.18 ± 0.00
0.17 ± 0.01
0.19 ± 0.00
0.16 ± 0.00
0.16 ± 0.01
0.19 ± 0.00
⟨R2⟩↓
53.03 ± 4.06
47.81 ± 3.50
36.14 ± 0.18
35.17 ± 0.35
31.65 ± 1.79
28.57 ± 0.26
ZPVE ↓
0.24 ± 0.01
0.14 ± 0.02
0.08 ± 0.01
0.06 ± 0.00
0.06 ± 0.02
0.03 ± 0.00
U0 ↓
609.25 ± 68.91
329.56 ± 30.08
143.71 ± 15.27
100.49 ± 5.43
31.06 ± 11.45
9.93 ± 2.90
U ↓
579.75 ± 78.19
319.37 ± 11.63
160.75 ± 46.21
100.41 ± 4.27
28.30 ± 18.81
10.05 ± 3.18
H ↓
558.89 ± 71.13
310.95 ± 20.79
166.10 ± 41.69
102.85 ± 4.13
32.18 ± 11.43
11.05 ± 6.37
G ↓
580.03 ± 68.41
320.53 ± 36.14
166.66 ± 54.19
102.43 ± 3.36
19.90 ± 11.13
10.69 ± 6.34
cV ↓
0.90 ± 0.16
0.64 ± 0.02
0.53 ± 0.04
0.39 ± 0.01
0.47 ± 0.08
0.17 ± 0.00
U ATOM
0
↓
3.96 ± 0.61
1.77 ± 0.22
1.19 ± 0.06
0.96 ± 0.03
0.45 ± 0.05
0.24 ± 0.01
U ATOM ↓
4.53 ± 0.33
1.92 ± 0.25
1.19 ± 0.05
0.97 ± 0.02
0.47 ± 0.09
0.24 ± 0.01
HATOM ↓
3.78 ± 0.70
1.81 ± 0.24
1.15 ± 0.03
0.96 ± 0.03
0.42 ± 0.03
0.25 ± 0.00
GATOM ↓
3.50 ± 0.67
1.73 ± 0.11
1.07 ± 0.04
0.85 ± 0.02
0.38 ± 0.03
0.22 ± 0.02
A ↓
13.45 ± 4.20
5.70 ± 5.79
3.83 ± 4.08
27.99 ± 38.02
1.61 ± 0.19
0.75 ± 0.11
B ↓
0.23 ± 0.01
0.25 ± 0.03
0.23 ± 0.02
0.26 ± 0.02
0.12 ± 0.03
0.08 ± 0.01
C ↓
0.18 ± 0.03
0.22 ± 0.03
0.22 ± 0.07
0.21 ± 0.02
0.12 ± 0.02
0.05 ± 0.01
MOLNET
FREESOLV ↑
0.95 ± 0.01
0.94 ± 0.03
0.72 ± 0.43
0.39 ± 0.51
0.92 ± 0.01
0.96 ± 0.00
LIPO ↑
0.78 ± 0.01
0.78 ± 0.01
0.78 ± 0.01
0.80 ± 0.01
OOM
0.71 ± 0.01
ESOL ↑
0.86 ± 0.01
0.85 ± 0.01
0.89 ± 0.01
0.88 ± 0.01
0.91 ± 0.01
0.93 ± 0.01
GAT
GATv2
GIN
PNA
TokenGT
MAGE
DOCKSTRING
ESR2 ↑
0.57 ± 0.01
0.58 ± 0.01
0.59 ± 0.01
0.61 ± 0.00
0.48 ± 0.01
0.63 ± 0.00
F2 ↑
0.79 ± 0.02
0.83 ± 0.01
0.85 ± 0.00
0.85 ± 0.00
0.77 ± 0.00
0.88 ± 0.00
KIT ↑
0.80 ± 0.00
0.80 ± 0.00
0.80 ± 0.01
0.82 ± 0.00
0.66 ± 0.02
0.80 ± 0.00
PARP1 ↑
0.79 ± 0.04
0.86 ± 0.01
0.88 ± 0.00
0.88 ± 0.00
0.79 ± 0.01
0.91 ± 0.00
PGR ↑
0.50 ± 0.02
0.50 ± 0.03
0.56 ± 0.02
0.50 ± 0.06
0.50 ± 0.00
0.68 ± 0.00
Table 4. Test set Matthews correlation coefficient (MCC) for 3 graph-level classification tasks from MoleculeNet, presented as mean ±
standard deviation from 5 different runs. The highest mean values are highlighted in bold. GCN and TokenGT are available in Appendix D.
Dataset
GAT
GATv2
GIN
PNA
Graphormer
MAGE
BBBP ↑
0.71 ± 0.04
0.72 ± 0.04
0.71 ± 0.02
0.72 ± 0.03
OOM
0.76 ± 0.05
BACE ↑
0.59 ± 0.02
0.60 ± 0.01
0.56 ± 0.05
0.45 ± 0.25
0.09 ± 0.07
0.65 ± 0.02
HIV ↑
0.38 ± 0.02
0.38 ± 0.07
0.35 ± 0.05
0.38 ± 0.05
OOM
0.43 ± 0.02
Despite its simplicity, MAG consistently outperforms other
message passing baselines and more involved Transformer-
based methods while supporting transfer learning through
pre-training and fine-tuning, which does not work well with
classical GNNs, and scales favourably in terms of time and
memory. Still, several software limitations impede a more
efficient MAG implementation (see Appendix A). Com-
pared to Transformers, we remarked several distinguishing
features. It performs as well as presented here without any
sophisticated learning rate scheduler (warm-up, cosine an-
nealing, etc.) or optimiser. Interestingly, for about half the
datasets MAG performed better with batch normalisation
instead of layer normalisation (more in Appendix G). Also,
MAG does not use any form of positional encoding, which
is against current trends for GNNs and Transformers. Future
research might investigate ways to combine node and edge
processing, devise efficient attention implementations for
graphs, study the importance of positional encodings, inte-
grate successful concepts from other domains such as sparse
expert models (Fedus et al., 2022), or study multi-modality.
7

Masked Attention is All You Need for Graphs
Table 5. Test set Matthews correlation coefficient (MCC) for graph-level classification tasks from various domains, presented as mean ±
standard deviation from 5 runs. All models use MAGE, except the last 6 (MAGN). The highest mean values are highlighted in bold.
Dataset
GCN
GAT
GATv2
GIN
PNA
MAG
MALNETTINY ↑
0.85 ± 0.01
0.87 ± 0.01
0.88 ± 0.01
0.90 ± 0.01
0.89 ± 0.01
0.91 ± 0.01
CV
MNIST ↑
0.85 ± 0.00
0.97 ± 0.00
0.97 ± 0.00
0.88 ± 0.01
0.98 ± 0.00
0.97 ± 0.00
CIFAR10 ↑
0.46 ± 0.00
0.63 ± 0.01
0.63 ± 0.01
0.44 ± 0.02
0.65 ± 0.02
0.63 ± 0.01
BIOINF.
ENZYMES ↑
0.61 ± 0.03
0.70 ± 0.04
0.72 ± 0.03
0.67 ± 0.04
0.73 ± 0.02
0.71 ± 0.03
PROTEINS ↑
0.21 ± 0.10
0.41 ± 0.04
0.47 ± 0.05
0.48 ± 0.08
0.44 ± 0.10
0.55 ± 0.06
DD ↑
0.49 ± 0.02
0.46 ± 0.05
0.43 ± 0.04
0.39 ± 0.07
0.40 ± 0.10
0.57 ± 0.04
SYNTHETIC
SYNTHETIC ↑
1.00 ± 0.00
1.00 ± 0.00
1.00 ± 0.00
0.99 ± 0.03
1.00 ± 0.00
1.00 ± 0.00
SYNTHETIC N. ↑
−0.01 ± 0.05
0.56 ± 0.12
0.79 ± 0.11
0.47 ± 0.14
0.91 ± 0.03
0.95 ± 0.03
SYNTHIE ↑
0.87 ± 0.04
0.26 ± 0.07
0.30 ± 0.07
0.60 ± 0.07
0.84 ± 0.08
0.92 ± 0.05
TRIANGLES ↑
0.17 ± 0.01
0.15 ± 0.02
0.14 ± 0.02
0.15 ± 0.01
0.07 ± 0.02
0.22 ± 0.05
COLORS-3 ↑
0.30 ± 0.01
0.22 ± 0.02
0.25 ± 0.01
0.29 ± 0.02
0.38 ± 0.01
0.75 ± 0.01
SOCIAL
IMDB-BINARY ↑
0.59 ± 0.04
0.52 ± 0.05
0.51 ± 0.05
0.54 ± 0.05
0.50 ± 0.05
0.62 ± 0.06
IMDB-MULTI ↑
0.18 ± 0.03
0.20 ± 0.02
0.17 ± 0.01
0.19 ± 0.02
0.20 ± 0.02
0.21 ± 0.03
REDDIT-BINARY ↑
0.54 ± 0.02
0.53 ± 0.05
0.35 ± 0.20
0.49 ± 0.04
0.53 ± 0.07
0.61 ± 0.04
REDDIT-M-5K ↑
0.34 ± 0.01
0.31 ± 0.01
0.29 ± 0.02
0.31 ± 0.01
0.34 ± 0.02
0.35 ± 0.01
REDDIT-M-12K ↑
0.34 ± 0.01
0.31 ± 0.01
0.26 ± 0.02
0.33 ± 0.00
0.35 ± 0.01
0.37 ± 0.03
TWITCH EGOS ↑
0.37 ± 0.00
0.38 ± 0.00
0.38 ± 0.00
0.38 ± 0.00
0.39 ± 0.00
0.39 ± 0.00
REDDIT THR. ↑
0.56 ± 0.00
0.57 ± 0.00
0.57 ± 0.00
0.57 ± 0.00
0.57 ± 0.00
0.57 ± 0.00
GITHUB STAR. ↑
0.27 ± 0.01
0.30 ± 0.01
0.21 ± 0.11
0.28 ± 0.03
0.36 ± 0.02
0.31 ± 0.02
Table 6. Transfer learning performance (RMSE) on QM9 for HOMO and LUMO, presented as mean ± standard deviation from 5 different
runs on test sets. The lowest mean values are highlighted in bold.
Task
Strategy
GCN
GAT
GATv2
GIN
PNA
MAGE
HOMO ↓
GW
0.23 ± 0.004
0.21 ± 0.002
0.21 ± 0.002
0.21 ± 0.001
0.19 ± 0.001
0.14 ± 0.001
Trans.
0.21 ± 0.001
0.16 ± 0.000
0.15 ± 0.000
0.15 ± 0.000
0.14 ± 0.000
0.01 ± 0.000
Ind.
0.21 ± 0.001
0.18 ± 0.000
0.18 ± 0.000
0.19 ± 0.000
0.17 ± 0.000
0.09 ± 0.000
LUMO ↓
GW
0.20 ± 0.001
0.19 ± 0.002
0.19 ± 0.002
0.19 ± 0.001
0.18 ± 0.003
0.12 ± 0.001
Trans.
0.21 ± 0.001
0.16 ± 0.000
0.17 ± 0.000
0.17 ± 0.000
0.16 ± 0.000
0.01 ± 0.000
Ind.
0.20 ± 0.000
0.17 ± 0.000
0.17 ± 0.000
0.17 ± 0.001
0.17 ± 0.001
0.08 ± 0.000
Table 7. Average training time per epoch (s) and used memory (GB) for all methods, presented as mean ± std. from 5 epochs and with the
number of parameters (#). For QM9 (103,542 train items), the maximum number of nodes/edges per graph is 29 (MAGN), respectively 56
(MAGE). DD (942 train items) has 5,748 maximum nodes and 28,534 maximum edges. All algorithms use bfloat16 mixed training. (*)
Parameters for masked blocks are counted as for normal self-attention, despite a very large number of connections being dropped.
Method
#
QM9
DD
Time (s)
Memory (GB)
Time (s)
Memory (GB)
GCN
158K
13.74 ± 0.47
0.10 ±
0.00
2.20 ± 0.11
0.32 ±
0.02
GAT
10.1M
25.60 ± 0.35
1.14 ±
0.07
4.05 ± 0.08
3.61 ±
0.55
GATv2
20.1M
28.82 ± 0.65
1.36 ±
0.09
5.31 ± 0.04
5.51 ±
0.67
GIN
433K
14.95 ± 1.24
0.12 ±
0.00
2.13 ± 0.03
0.28 ±
0.06
PNA
6.9M
66.62 ± 1.33
2.53 ±
0.20
12.05 ± 0.05
6.77 ±
0.97
Graphormer
22.3M
186.60 ± 4.50
1.95 ±
0.00
OOM
TokenGT
8.1M
30.76 ± 0.26
1.21 ±
0.00
OOM
MAGN (naive)
8.3M*
16.46 ± 0.71
0.31 ±
0.00
OOM
MAGN (mem-efficient)
15.38 ± 0.70
0.27 ±
0.00
13.53 ± 0.03
1.19 ±
0.00
MAGE (naive)
8.6M*
26.11 ± 0.31
0.61 ±
0.00
OOM
MAGE (mem-efficient)
22.58 ± 0.49
0.44 ±
0.00
348.22 ± 1.32 21.90 ±
1.19
8

Masked Attention is All You Need for Graphs
References
Alon, U. and Yahav, E. On the bottleneck of graph neural
networks and its practical implications. In International
Conference on Learning Representations, 2021.
Brody, S., Alon, U., and Yahav, E. How attentive are graph
attention networks?
In International Conference on
Learning Representations, 2022.
Buterez, D., Janet, J. P., Kiddle, S. J., Oglic, D., and Li`o,
P. Graph neural networks with adaptive readouts. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Ad-
vances in Neural Information Processing Systems, 2022.
Buterez, D., Janet, J. P., Kiddle, S. J., and Li`o, P. Mf-pcba:
Multifidelity high-throughput screening benchmarks for
drug discovery and machine learning. Journal of Chemi-
cal Information and Modeling, 63(9):2667–2678, 2023a.
doi: 10.1021/acs.jcim.2c01569. PMID: 37058588.
Buterez, D., Janet, J. P., Kiddle, S. J., Oglic, D., and Li`o, P.
Modelling local and general quantum mechanical prop-
erties with attention-based pooling. Communications
Chemistry, 6(1):262, Nov 2023b. ISSN 2399-3669. doi:
10.1038/s42004-023-01045-7.
Buterez, D., Janet, J. P., Kiddle, S., Oglic, D., and Li`o,
P.
Transfer learning with graph neural networks for
improved molecular property prediction in the multi-
fidelity setting.
ChemRxiv, 2024.
doi:
10.26434/
chemrxiv-2022-dsbm5-v3.
Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y., and Wang,
L. Graphnorm: A principled approach to accelerating
graph neural network training. In 2021 International
Conference on Machine Learning, May 2021.
Chen, C., Zuo, Y., Ye, W., Li, X., and Ong, S. P. Learning
properties of ordered and disordered materials from multi-
fidelity data. Nature Computational Science, 1(1):46–53,
01 2021. ISSN 2662-8457.
Choromanski, K. M., Likhosherstov, V., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mo-
hiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J.,
and Weller, A. Rethinking attention with performers. In
International Conference on Learning Representations,
2021.
Corso, G., Cavalleri, L., Beaini, D., Li`o, P., and Velickovic,
P. Principal neighbourhood aggregation for graph nets.
In Proceedings of the 34th International Conference on
Neural Information Processing Systems, NIPS’20, Red
Hook, NY, USA, 2020. Curran Associates Inc. ISBN
9781713829546.
Dao, T. Flashattention-2: Faster attention with better paral-
lelism and work partitioning, 2023.
Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R´e, C. FlashAt-
tention: Fast and memory-efficient exact attention with
IO-awareness. In Advances in Neural Information Pro-
cessing Systems, 2022.
Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J.,
G´omez-Bombarelli, R., Hirzel, T., Aspuru-Guzik, A.,
and Adams, R. P. Convolutional networks on graphs
for learning molecular fingerprints. In Proceedings of
the 28th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’15, pp. 2224–2232,
Cambridge, MA, USA, 2015. MIT Press.
Dwivedi, V. P., Ramp´aˇsek, L., Galkin, M., Parviz, A., Wolf,
G., Luu, A. T., and Beaini, D. Long range graph bench-
mark. In Thirty-sixth Conference on Neural Informa-
tion Processing Systems Datasets and Benchmarks Track,
2022.
Fediai, A., Reiser, P., Pe˜na, J. E. O., Friederich, P., and
Wenzel, W. Accurate gw frontier orbital energies of 134
kilo molecules. Scientific Data, 10(1):581, Sep 2023.
ISSN 2052-4463. doi: 10.1038/s41597-023-02486-4.
Fedus, W., Dean, J., and Zoph, B. A review of sparse expert
models in deep learning, 2022.
Garc´ıa-Orteg´on, M., Simm, G. N. C., Tripp, A. J.,
Hern´andez-Lobato, J. M., Bender, A., and Bacallado,
S. Dockstring: Easy molecular docking yields better
benchmarks for ligand design. Journal of Chemical In-
formation and Modeling, 62(15):3486–3502, 2022. doi:
10.1021/acs.jcim.1c01334. PMID: 35849793.
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and
Dahl, G. E. Neural message passing for quantum chem-
istry. In Proceedings of the 34th International Confer-
ence on Machine Learning - Volume 70, ICML’17, pp.
1263–1272. JMLR.org, 2017.
Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez-
Gonzalez, A., Rubanova, Y., Veliˇckovi´c, P., Kirkpatrick,
J., and Battaglia, P. Simple GNN regularisation for 3d
molecular property prediction and beyond. In Interna-
tional Conference on Learning Representations, 2022.
Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V.,
and Leskovec, J. Strategies for pre-training graph neu-
ral networks. In International Conference on Learning
Representations, 2020.
Jain, P., Wu, Z., Wright, M. A., Mirhoseini, A., Gonza-
lez, J. E., and Stoica, I. Representing long-range con-
text for graph neural networks with global attention. In
Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
J. W. (eds.), Advances in Neural Information Processing
Systems, 2021.
9

Masked Attention is All You Need for Graphs
Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and
Riley, P. Molecular graph convolutions: moving beyond
fingerprints. Journal of Computer-Aided Molecular De-
sign, 30(8):595–608, Aug 2016. ISSN 1573-4951. doi:
10.1007/s10822-016-9938-8.
Kim, J., Nguyen, D. T., Min, S., Cho, S., Lee, M., Lee, H.,
and Hong, S. Pure transformers are powerful graph learn-
ers. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
K. (eds.), Advances in Neural Information Processing
Systems, 2022.
Kreuzer, D., Beaini, D., Hamilton, W. L., L´etourneau, V.,
and Tossou, P. Rethinking graph transformers with spec-
tral attention. In Beygelzimer, A., Dauphin, Y., Liang, P.,
and Vaughan, J. W. (eds.), Advances in Neural Informa-
tion Processing Systems, 2021.
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh,
Y. W. Set transformer: A framework for attention-based
permutation-invariant neural networks. In Proceedings of
the 36th International Conference on Machine Learning,
pp. 3744–3753, 2019.
Lefaudeux, B., Massa, F., Liskovich, D., Xiong, W.,
Caggiano,
V.,
Naren,
S.,
Xu,
M.,
Hu,
J.,
Tin-
tore,
M.,
Zhang,
S.,
Labatut,
P.,
and
Haziza,
D.
xformers:
A modular and hackable trans-
former modelling library. https://github.com/
facebookresearch/xformers, 2022.
Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
ularization. In International Conference on Learning
Representations, 2019.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Advances
in Neural Information Processing Systems 32, pp. 8024–
8035. Curran Associates, Inc., 2019.
Rabe, M. N. and Staats, C. Self-attention does not need
o(n2) memory. CoRR, abs/2112.05682, 2021.
Ramakrishnan, R., Dral, P. O., Rupp, M., and von Lilienfeld,
O. A. Quantum chemistry structures and properties of
134 kilo molecules. Scientific Data, 1(1):140022, Aug
2014. ISSN 2052-4463. doi: 10.1038/sdata.2014.22.
Rampasek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf,
G., and Beaini, D. Recipe for a general, powerful, scal-
able graph transformer. In Oh, A. H., Agarwal, A., Bel-
grave, D., and Cho, K. (eds.), Advances in Neural Infor-
mation Processing Systems, 2022.
Ramp´aˇsek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf,
G., and Beaini, D. Recipe for a General, Powerful, Scal-
able Graph Transformer. Advances in Neural Information
Processing Systems, 35, 2022.
Shazeer, N. GLU variants improve transformer. CoRR,
abs/2002.05202, 2020.
Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland,
D. J., and Sinop, A. K. Exphormer: Scaling graph trans-
formers with expander graphs, 2023.
T¨onshoff, J., Ritzert, M., Rosenbluth, E., and Grohe, M.
Where did the gap go? reassessing the long-range graph
benchmark. In The Second Learning on Graphs Confer-
ence, 2023.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. At-
tention is all you need. In Guyon, I., Luxburg, U. V.,
Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc.,
2017.
Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Li`o, P., and Bengio, Y. Graph attention networks. In
International Conference on Learning Representations,
2018.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,
C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-
icz, M., Davison, J., Shleifer, S., von Platen, P., Ma,
C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,
S., Drame, M., Lhoest, Q., and Rush, A. Transform-
ers: State-of-the-art natural language processing. In Liu,
Q. and Schlangen, D. (eds.), Proceedings of the 2020
Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, pp. 38–45, Online,
October 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-demos.6.
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C.,
Zhang, H., Lan, Y., Wang, L., and Liu, T.-Y. On layer
normalization in the transformer architecture. In Proceed-
ings of the 37th International Conference on Machine
Learning, ICML’20. JMLR.org, 2020a.
Xiong, Z., Wang, D., Liu, X., Zhong, F., Wan, X., Li, X.,
Li, Z., Luo, X., Chen, K., Jiang, H., and Zheng, M. Push-
ing the boundaries of molecular representation for drug
discovery with the graph attention mechanism. Journal
of Medicinal Chemistry, 63(16):8749–8760, 2020b. doi:
10.1021/acs.jmedchem.9b00959. PMID: 31408336.
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen,
Y., and Liu, T.-Y. Do transformers really perform badly
10

Masked Attention is All You Need for Graphs
for graph representation? In Thirty-Fifth Conference on
Neural Information Processing Systems, 2021.
Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-
berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,
Yang, L., and Ahmed, A. Big bird: Transformers for
longer sequences. In Larochelle, H., Ranzato, M., Had-
sell, R., Balcan, M., and Lin, H. (eds.), Advances in
Neural Information Processing Systems, volume 33, pp.
17283–17297. Curran Associates, Inc., 2020.
Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth-
ing in gnns. In International Conference on Learning
Representations, 2020.
11

Masked Attention is All You Need for Graphs
A. Limitations
In terms of limitations, we highlight that the available li-
braries are not optimised for masking or custom attention
patterns. This is most evident for very dense graphs (tens of
thousands of edges or more). Memory efficient and Flash at-
tention are available natively in PyTorch (Paszke et al., 2019)
starting from version 2, as well as in the xFormers library
(Lefaudeux et al., 2022). More specifically, we have tested at
least 5 different implementations of MAG: (1) leveraging the
MultiheadAttention module from PyTorch, (2) lever-
aging the MultiHeadDispatch module from xFormers,
(3) a manual implementation of multihead attention, relying
on PyTorch’s scaled dot product attention func-
tion, (4) a manual implementation of multihead attention, re-
lying on xFormers’ memory efficient attention,
and (5) a naive implementation. Options (1) - (4) can all
make use of efficient and fast implementations. However,
we have observed performance differences between the 4
implementations, as well as compared to a naive implemen-
tation. This behaviour is likely due to the different low-level
kernel implementations. Moreover, Flash attention does not
support custom attention masks as there is little interest for
such functionality from a language modelling perspective.
Although the masks can be computed efficiently during
training, all frameworks require the last two dimensions of
the input mask tensor to be of shape (Nd, Nd) for nodes
or (Ne, Ne) for edges, effectively squaring the number of
nodes or edges. However, the mask tensors are very sparse
and a sparse tensor alternative could greatly reduce the
memory consumption for large and dense graphs. Such
an option exists for the PyTorch native attention, but it is
currently broken.
Another possible optimisation would be to use nested
(ragged) tensors to represent graphs, since padding is cur-
rently necessary to ensure identical dimensions for attention.
A prototype nested tensor attention is available in PyTorch;
however, not all the required operations are supported and
converting between normal and nested tensors is slow.
For all implementations, it is required that the mask ten-
sor is repeated by the number of attention heads (e.g. 8 or
16). However, a notable bottleneck is encountered for the
MultiheadAttention and MultiHeadDispatch
variants described above, which require that the repeats hap-
pen in the batch dimension, i.e. requiring 3D mask tensors
of shape (B × H, N, N), where H is the number of heads.
The other two efficient implementations require a 4D mask
instead, i.e. (B, H, N, N), where one can use PyTorch’s
expand function instead of repeat. The expand alter-
native does not use any additional memory, while repeat
requires ×H memory. Note that it is not possible to re-
shape the 4D tensor created using expand without using
additional memory.
B. Helper functions
consecutive is a helper function that generates consecu-
tive numbers starting from 0, with a length specified in
its tensor argument as the difference between adjacent
elements, and a second integer argument used for the
last length computation, e.g.
consecutive([1, 4, 6], 10)
= [0, 1, 2, 0, 1, 0, 1, 2, 3], and first unique index finds the
first occurrence of each unique element in the tensor (sorted),
e.g. first unique index([3, 2, 3, 4, 2]) = [1, 0, 3].
C. Experimental setup
We follow a simple and universal experimental protocol to
ensure that it is possible to compare the results of different
methods and to evaluate a large number of datasets with high
throughput. We chose a number of reasonable hyperparam-
eters and settings for all methods, regardless of their nature
(GNN or attention-based). This includes the AdamW opti-
miser (Loshchilov & Hutter, 2019), learning rate (0.0001),
batch size (128), 32-bit training (without mixed precision),
early stopping with a patience of 30 epochs (100 for the very
small datasets such as FREESOLV), and gradient clipping
(set to the default value of 0.5). Furthermore, we used a
simple learning rate scheduler that halved the learning rate
if no improvement was encountered for 15 epochs (half the
early stopping patience).
For GNNs, we used 4 graph layers for all algorithms (GCN,
GIN, GAT, GATv2, PNA) and the mean readout function, a
node dimension of 64, and a hidden dimension for the graph
layers of 256. All layers use batch normalisation. Settings
specific to some algorithms were also given by reasonable
defaults, such as 8 attention heads for GAT(v2), and 5 tow-
ers for PNA. In certain instances such as small datasets with
dense graphs, the defaults selected above can lead to out-
of-memory errors, even for GNNs, in particular the more
computationally-intensive algorithms such as GAT(v2) or
PNA. In such cases, we lower the settings that were the most
likely cause, such as the hidden dimension or batch size, to
the next (lower) power of two. In some cases, mixed preci-
sion training (using the bfloat16 type) was performed as
an alternative if Ampere-class GPUs were available.
For
Graphormer
and
TokenGT,
we
leverage
the
huggingface (Wolf et al., 2020) implementation.
Our selection of reasonable defaults include 3 layers, a
hidden and embedding size of 512, and 8 attention heads, in
addition to the defaults mentioned earlier such as learning
rate and batch size. Graphormer in particular can be difficult
to train, and in such cases we reduce the complexity of the
model using the same strategies as above. However, note
that many out-of-memory errors for this family of models
are not due to GPU memory, but RAM; we attempted to use
up to 256GB, but conceded if it did not work.
12

Masked Attention is All You Need for Graphs
For MAG, the same suite of “general” defaults such as
the learning rate and batch size apply. We also generally
follow the same configuration for all datasets. However,
since MAG is a new architecture with many unknowns, we
generally evaluate a small number of variations for each
dataset and select the best one according to the validation
metrics. The variations typically involve choosing MAGN
or MAGE, batch or layer normalisation, the number (3 or
4) and order of self-attention and masked self attention
blocks (e.g. SMM, MSMM, etc.), the hidden size (256 or 512)
and the number of attention heads (8 or 16). We generally
prefer MAGE in all situations and only consider MAGN for
datasets where MAGE would take a very long time to run or
requires heavy modifications to the default parameters. This
is because MAGE can naturally incorporate edge features
and almost always performs better. We have also found
SwiGLU to often be better than plain MLPs (Shazeer, 2020).
Graphormer (and TokenGT to a lower extent) have very
expensive pre-processing steps which require up to hun-
dreds of GBs of storage space to cache intermediary results.
The alternative would be to not use caching; however, this
means that everything must be stored in memory, resulting
in almost immediate crashing. As a further complication,
for large datasets suck as DOCKSTRING, Graphormer would
run for a few epochs but spontaneously crash, most likely
due to high memory utilisation during training. Combined
with the fact that one epoch took several hours, we have
included Graphormer results only for a minority of datasets.
Hyperparameter optimisation
Other than the basic filtering described above for MAG, we
did not use any techniques for tuning. In particular, we
did not perform hyperparameter optimisation and have not
tuned aspects of the networks such as the optimiser, learn-
ing rate, batch size, dropout, hidden dimensions, etc. We
acknowledge that we are not using optimal parameters for
the majority of models. However, this is also true for MAG,
and tuning every model presented here would dramatically
increase the time and resource utilisation (as well as the
financial costs associated with it), defeating the purpose of
presenting a simple yet effective alternative to GNNs.
Evaluation
Generally, for a self-contained evaluation we split all
datasets using a random 80%, 10%, 10% split for train,
validation, and test.
The same data splits are used for
the different evaluated algorithms. Some datasets, such
as MNIST or DOCKSTRING are provided with existing train,
test, and optionally validation splits. If such splits are avail-
able through PyTorch Geometric or from the authors (such
as DOCKSTRING), we use them and we do not perform our
own random splits. For all datasets and models, we provide
results from 5 different runs (seeds).
D. Additional results
The missing GCN, TokenGT, and Graphormer results for
some of the datasets presented in the main text are presented
below in Tables 8 and 9. These complete the results from
Tables 3 and 4.
Table 8. Test set root mean squared error (RMSE) for QM9 and
R2 for the others, for GCN and TokenGT, presented as mean ±
standard deviation from 5 different runs.
Property
GCN
TokenGT
QM9
µ
0.67 ± 0.01
1.00 ±
0.00
α
3.51 ± 0.18
2.14 ±
0.07
ϵHOMO
0.14 ± 0.00
0.26 ±
0.01
ϵLUMO
0.15 ± 0.00
0.42 ±
0.01
∆ϵ
0.21 ± 0.00
0.56 ±
0.01
⟨R2⟩
67.74 ± 5.72
177.70 ±
6.39
ZPVE
0.23 ± 0.00
0.14 ±
0.00
U0
573.32 ± 47.41
228.49 ± 198.10
U
594.55 ± 57.81
228.49 ± 198.10
H
575.10 ± 48.66
228.49 ± 198.10
G
594.07 ± 57.00
228.49 ± 198.10
cV
1.34 ± 0.26
1.05 ±
0.03
U ATOM
0
3.99 ± 0.04
2.02 ±
0.30
U ATOM
4.19 ± 0.33
2.22 ±
0.19
HATOM
4.05 ± 0.02
2.16 ±
0.10
GATOM
3.70 ± 0.02
1.91 ±
0.14
A
1.17 ± 0.11
0.01 ±
0.01
B
0.28 ± 0.02
0.40 ±
5.80
C
0.24 ± 0.02
0.35 ±
2.36
MOLNET
FREESOLV
0.34 ± 0.51
0.86 ±
0.02
LIPO
0.71 ± 0.01
OOM
ESOL
0.86 ± 0.01
0.78 ±
0.01
GCN
Graphormer
DOCKSTRING
ESR2
0.53 ± 0.01
OOM
F2
0.78 ± 0.00
OOM
KIT
0.76 ± 0.00
OOM
PARP1
0.81 ± 0.00
OOM
PGR
0.36 ± 0.01
OOM
Table 9. Test set Matthews correlation coefficient (MCC) for GCN
and TokenGT for 3 graph-level classification tasks from Molecu-
leNet, presented as mean ± standard deviation, over 5 runs.
Dataset
GCN
TokenGT
BACE
0.35 ± 0.31
0.19 ± 0.09
BBBP
0.68 ± 0.02
0.31 ± 0.09
HIV
0.39 ± 0.03
OOM
13

Masked Attention is All You Need for Graphs
E. Dataset statistics
We present a summary of all the used datasets, along with
their size and the maximum number of nodes and edges
encountered in a graph in the dataset (Table 10). The last two
are important as they determine the shape of the mask and of
the inputs for the attention blocks. Technically, we require
that the maximum number of nodes/edges is determined
per batch and the tensors to be padded accordingly. This
per-batch maximum is lower than the dataset maximum for
most batches. However, certain operations such as layer
norm., if performed over the last two dimensions, require a
constant value. To enable this, we use the dataset maximum.
Table 10. Summary of used datasets, their size, and the max. num-
ber of nodes (N) and edges (E) seen in a graph in the dataset.
Dataset
Size
N
E
LRGB
PEPT-STRUCT
15 535
444
928
PEPT-FUNC
15 535
444
928
NODE
PPI
24
3 480
106 754
CORA
1
2 708
10 556
CITESEER
1
3 327
9 104
QM9
133 885
29
56
MOLECULENET
FREESOLV
642
44
92
LIPO
4 200
216
438
ESOL
1 128
119
252
BBBP
2 039
269
562
BACE
1 513
184
376
HIV
41 127
438
882
DOCKSTRING
260 060
164
342
MALNETTINY
5 000
4 994
20 096
CV
MNIST
70 000
75
600
CIFAR10
60 000
150
1 200
BIOINFO
ENZYMES
600
126
298
PROTEINS
1 113
620
2 098
DD
1 178
5 748
28 534
SYNTHETIC
SYNTHETIC
300
100
392
SYNTHETIC NEW
300
100
396
SYNTHIE
400
100
424
TRIANGLES
45 000
100
396
COLORS-3
10 500
200
794
SOCIAL
IMDB-BINARY
1 000
136
2 498
IMDB-MULTI
1 500
89
2 934
REDDIT-BINARY
2 000
3 782
8 142
REDDIT-M-5K
4 999
3 648
9 566
REDDIT-M-12K
11 929
3 782
10 342
TWITCH EGOS
127 094
52
1 572
REDDIT THR.
203 088
97
370
GITHUB STAR.
12 725
957
9 336
F. Masked attention equation
Masked attention can be thought of as a custom attention pat-
tern, which for graphs was described succinctly by (Shirzad
et al., 2023). An adaptation for masking would be:
MaskedSelfAttention(X, M) =
(7)
h
X
j=1
Wj
O

Wj
V XM

σ

Wj
KXM
T
(Wj
QXM)

where h is the number of attention heads, X is the input fea-
tures matrix, M is the custom attention pattern/mask, which
restricts the attention to a subset of elements of X, denoted
by XM, Wj
Q, Wj
K, Wj
V , Wj
O are weight matrices corre-
sponding to queries, keys, values, and outputs, respectively,
and σ is the softmax function.
G. Layer vs batch normalisation
Contrary to standard Transformers and current trends, we
have found that simply replacing the layer normalisation
(LN) operation within MAG with batch normalisation (BN)
can dramatically improve performance on a number of
datasets. LN was our default initial choice and we have
found that it works better for the standard QM9 dataset and
its properties (Table 3). For the other datasets and tasks
(excluding the 19 QM9 properties but including the QM9
GW tasks from Table 6 and the PEPT-STRUCT and PEPT-FN
datasets), although we have not exhaustively tested LN vs
BN models, we have generally observed that LN is prefer-
able for 20 tasks and BN for 17. More specifically, BN was
preferable for the DOCKSTRING properties PARP1, ESR2,
and PGR, as well as for the datasets: CIFAR10, COLORS-
3, DD, ESOL, LIPO, PEPT-STRUCT, PEPT-FN, PROTEINS,
REDDIT THREADS, REDDIT-MULTI-12K, SYNTHIE, TRIAN-
GLES, ENZYMES, and TWITCH EGOS. Apart from the fact
that most molecular datasets tend to do better with LN, there
is no obvious indication of which normalisation technique
might be preferable for certain dataset types.
H. Experimental platform
Representative versions of the software used as part of
this paper include Python 3.11.6, PyTorch version 2.1.1
with CUDA 11.8, PyTorch Geometric 2.4.0, PyTorch Light-
ning 1.9.5, huggingface transformers version 4.35.2, and
xFormers version 0.0.23. We have also tested our code with
CUDA ≥12.0. It is worth noting that attention masking
and efficient implementations of attention are early fea-
tures that are advancing quickly. This means that their
behaviour might change unexpectedly and there might be
bugs. For example, PyTorch 2.1.1 recently fixed a bug that
concerned non-contiguous custom attention masks in the
scaled dot product attention function.
14

Masked Attention is All You Need for Graphs
In terms of hardware, the GPUs used include an NVIDIA
RTX 3090 with 24GB VRAM, NVIDIA V100 with 16GB or
32GB of VRAM, and NVIDIA A100 with 40GB of VRAM.
Recent, efficient implementations of attention are optimised
for the newest GPU architectures, generally starting from
Ampere (RTX 3090 and A100). However, while slower, it is
possible to run memory efficient attention on V100 GPUs.
I. Multiple configurations of MAGE
We summarise the results of running MAGE on the QM9
property α (alpha) in Table 11. Compared to the main results
of Table 3, we used an early stopping patience of 10 epochs
and trained for a maximum of 150 epochs.
Table 11. Test set RMSE for MAGE configurations for α (QM9),
presented as mean ± standard deviation from 5 different runs.
Configuration
MAGE
MMMSP
0.499 ± 0.012
MMSSP
0.501 ± 0.007
MMSP
0.503 ± 0.010
MMSMP
0.506 ± 0.003
MSMMP
0.508 ± 0.010
MMMMP
0.509 ± 0.007
MSSMP
0.509 ± 0.004
SMSSP
0.513 ± 0.012
SMMSP
0.515 ± 0.008
MSSSP
0.517 ± 0.014
MSMP
0.518 ± 0.008
MSMSP
0.519 ± 0.010
SMSMP
0.520 ± 0.010
SMMMP
0.522 ± 0.010
MMMP
0.522 ± 0.003
SSMMP
0.525 ± 0.012
MSP
0.525 ± 0.008
MSSP
0.527 ± 0.015
SMMP
0.527 ± 0.017
SSMSP
0.528 ± 0.009
SMSP
0.530 ± 0.005
SSSMP
0.539 ± 0.014
MMP
0.545 ± 0.017
SSMP
0.546 ± 0.010
MP
0.561 ± 0.007
SMP
0.567 ± 0.011
SSSSP
0.590 ± 0.014
SSSP
0.604 ± 0.013
SSP
0.642 ± 0.013
SP
0.665 ± 0.026
The configurations are given as a sequence of masked atten-
tion (M) or self-attention (S) blocks, followed by a pooling
by multihead attention block (P). While performance is
slightly lower than Table 3 due to the training changes, we
notice that models that use exclusively self-attention blocks
and no masked blocks are worse than models that predom-
inantly rely on masked attention, as expected. It is also
possible to use self-attention blocks after pooling by mul-
tihead attention. However, that would greatly increase the
number of possible configurations and we do not evaluate
this option here.
15

