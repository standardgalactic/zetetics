Published as a conference paper at ICLR 2020
ON THE VARIANCE OF THE ADAPTIVE LEARNING
RATE AND BEYOND
Liyuan Liu âˆ—
University of Illinois, Urbana-Champaign
ll2@illinois
Haoming Jiang â€ 
Georgia Tech
jianghm@gatech.edu
Pengcheng He, Weizhu Chen
Microsoft Dynamics 365 AI
{penhe,wzchen}@microsoft.com
Xiaodong Liu, Jianfeng Gao
Microsoft Research
{xiaodl,jfgao}@microsoft.com
Jiawei Han
University of Illinois, Urbana-Champaign
hanj@illinois
ABSTRACT
The learning rate warmup heuristic achieves remarkable success in stabilizing
training, accelerating convergence and improving generalization for adaptive
stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory
behind warmup, we identify a problem of the adaptive learning rate â€“ its vari-
ance is problematically large in the early stage, and presume warmup works as a
variance reduction technique. We provide both empirical and theoretical evidence
to verify our hypothesis. We further propose Rectiï¬ed Adam (RAdam), a novel
variant of Adam, by introducing a term to rectify the variance of the adaptive
learning rate. Experimental results on image classiï¬cation, language modeling,
and neural machine translation verify our intuition and demonstrate the efï¬cacy
and robustness of RAdam.1
1
INTRODUCTION
Adam-eps
Adam-2k
Adam-vanilla
RAdam
Adam-warmup
0
1
2
3
4
5
6
7
8
9
0
10k
20k
30k
40k
50k
60k
70k
Training loss
Overlapped
Figure 1: Training loss v.s. # of
iterations of Transformers on the
De-En IWSLTâ€™14 dataset.
Fast and stable optimization algorithms are what generations
of researchers have been pursuing (Gauss, 1823; Cauchy,
1847). Remarkably, stochastic gradient-based optimization,
such as stochastic gradient descent (SGD), has witnessed
tremendous success in many ï¬elds of science and engineering
despite its simplicity. Recently, many efforts have been made
to accelerate optimization by applying adaptive learning rate.
In particular, Adagrad (Duchi et al., 2010) and its variants, e.g.,
RMSprop (Hinton et al., 2012), Adam (Kingma & Ba, 2014),
Adadelta (Zeiler, 2012) and Nadam (Dozat, 2016), stand out
due to their fast convergence, and have been considered as the
optimizer of choice in many applications.
However, it has been observed that these optimization methods may converge to bad/suspicious
local optima, and have to resort to a warmup heuristic â€“ using a small learning rate in the ï¬rst
few epochs of training to mitigate such problem (Vaswani et al., 2017; Popel & Bojar, 2018). For
example, when training typical Transformers based neural machine translation models on the De-En
IWSLTâ€™14 dataset, removing the warmup stage increases the training loss from 3 to around 10, as
shown in Figure 1. Similar phenomena are observed in other scenarios like BERT (a bidirectional
transformer language model) pre-training (Devlin et al., 2019).
Due to the lack of the theoretical underpinnings, there is neither guarantee that warmup would bring
consistent improvements for various machine learning settings nor guidance on how we should
âˆ—Work was done during an internship at Microsoft Dynamics 365 AI.
â€ Work was done during an internship at Microsoft Dynamics 365 AI.
1All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.
1
arXiv:1908.03265v4  [cs.LG]  26 Oct 2021

Published as a conference paper at ICLR 2020
conduct warmup. Thus, researchers typically use different settings in different applications and
have to take a trial-and-error approach, which can be tedious and time-consuming.
In this paper, we conduct both empirical and theoretical analysis of the convergence issue to identify
its origin. We show that its root cause is: the adaptive learning rate has undesirably large variance in
the early stage of model training, due to the limited amount of training samples being used. Thus,
to reduce such variance, it is better to use smaller learning rates in the ï¬rst few epochs of training,
which justiï¬es the warmup heuristic.
Inspired by our analysis results, we propose a new variant of Adam, called Rectiï¬ed Adam (RAdam),
which explicitly rectiï¬es the variance of the adaptive learning rate based on derivations. We conduct
extensive experiments on language modeling, image classiï¬cation, and neural machine translation.
RAdam brings consistent improvement over the vanilla Adam, which veriï¬es the variance issue
generally exists on various tasks across different network architectures.
In summary, our main contributions are two-fold:
â€¢ We identify the variance issue of the adaptive learning rate and present a theoretical justiï¬cation
for the warmup heuristic. We show that the convergence issue is due to the undesirably large
variance of the adaptive learning rate in the early stage of model training.
â€¢ We propose a new variant of Adam (i.e., RAdam), which not only explicitly rectiï¬es the variance
and is theoretically sound, but also compares favorably with the heuristic warmup.
2
PRELIMINARIES AND MOTIVATIONS
Generic adaptive methods. Algorithm 1 is a generic framework (all operations are element-wise).
It describes various popular stochastic gradient descent algorithms (Reddi et al., 2018). Speciï¬cally,
different optimization algorithms can be speciï¬ed by different choices of Ï†(.) and Ïˆ(.), where Ï†(.)
speciï¬es how the momentum at time step t is calculated, and Ïˆ(.) how the adaptive learning rate at
t is calculated. For example, in the Adam algorithm, we have:
Ï†(g1, Â· Â· Â· , gt) = (1 âˆ’Î²1) Pt
i=1 Î²tâˆ’i
1
gi
1 âˆ’Î²t
1
and
Ïˆ(g1, Â· Â· Â· , gt) =
s
1 âˆ’Î²t
2
(1 âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i
. (1)
For numerical stability, the function Ïˆ(.) in Equation 1 is usually calculated as bÏˆ(g1, Â· Â· Â· , gt) =
âˆš
1âˆ’Î²t
2
Ïµ+âˆš
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i
, where Ïµ is a relatively small / negligible value (e.g., 1 Ã— 10âˆ’8).
Algorithm 1: Generic adaptive optimization method setup. All operations are element-wise.
Input: {Î±t}T
t=1: step size, {Ï†t, Ïˆt}T
t=1: function to calculate momentum and adaptive rate,
Î¸0: initial parameter, f(Î¸): stochastic objective function.
Output: Î¸T : resulting parameters
1 while t = 1 to T do
2
gt â†âˆ‡Î¸ft(Î¸tâˆ’1) (Calculate gradients w.r.t. stochastic objective at timestep t)
3
mt â†Ï†t(g1, Â· Â· Â· , gt) (Calculate momentum)
4
lt â†Ïˆt(g1, Â· Â· Â· , gt) (Calculate adaptive learning rate)
5
Î¸t â†Î¸tâˆ’1 âˆ’Î±tmtlt (Update parameters)
6 return Î¸T
Learning rate warmup. Instead of setting the learning rate Î±t as a constant or in a decreasing
order, a learning rate warmup strategy sets Î±t as smaller values in the ï¬rst few steps, thus not
satisfying âˆ€t Î±t+1 â‰¤Î±t. For example, linear warmup sets Î±t = t Î±0 when t < Tw. Warmup has
been demonstrated to be beneï¬cial in many deep learning applications. For example, in the NMT
experiments in Figure 1, the training loss convergences around 10 when warmup is not applied
(Adam-vanilla), and it surprisingly decreases to below 3 after applying warmup (Adam-warmup).
To further analyze this phenomenon, we visualize the histogram of the absolute value of gradients
on a log scale in Figure 2. We observe that, without applying warmup, the gradient distribution
is distorted to have a mass center in relatively small values within 10 updates. Such gradient dis-
tortion means that the vanilla Adam is trapped in bad/suspicious local optima after the ï¬rst few
2

Published as a conference paper at ICLR 2020
Iteration
Adam with warmup
Adam without warmup
6.76Ã—10'
9.38Ã—10'
Iteration
Iteration
4.08Ã—10'
4.08Ã—10'
Iteration
< -./0
-.1'
-.1/
-.2
< -./0
-.1'
-.1/
-.2
< -./0
-.1'
-.1/
-.2
< -./0
-.1'
-.1/
-.2
-.3
1
10
25
50
75
100
5
1
10
25
50
75
100
5
1
40K
70k
1
40K
70k
The distribution is distorted within 10 updates. 
Figure 2: The absolute gradient histogram of the Transformers on the De-En IWSLTâ€™ 14 dataset
during the training (stacked along the y-axis). X-axis is absolute value in the log scale and the
height is the frequency. Without warmup, the gradient distribution is distorted in the ï¬rst 10 steps.
   
   
   
  
   
   
   
  
   
   
   
  
Adam-2k
5.72 Ã— 106
RAdam
6.82 Ã— 106
Adam-eps
5.42 Ã— 106
10âˆ’20
ğ‘’âˆ’16
ğ‘’âˆ’12
ğ‘’âˆ’8
< ğ‘’âˆ’20
ğ‘’âˆ’16
ğ‘’âˆ’12
ğ‘’âˆ’8
< ğ‘’âˆ’20
ğ‘’âˆ’16
ğ‘’âˆ’12
ğ‘’âˆ’8
Iteration
Iteration
Iteration
< ğ‘’âˆ’20
1
40K
70k
1
40K
70k
1
40K
70k
Figure 3: The histogram of the absolute value of gradients (on a log scale) during the training of
Transformers on the De-En IWSLTâ€™ 14 dataset. using Adam-2k, RAdam and Adam-eps.
updates. Warmup essentially reduces the impact of these problematic updates to avoid the conver-
gence problem. In the following sections, we focus our analysis on learning rate warmup for the
Adam algorithm, while it can be applied to other algorithms that use similar adaptive learning rate
(Ïˆ(.)) designs, e.g., RMSprop (Hinton et al., 2012) and Nadam (Dozat, 2016).
3
VARIANCE OF THE ADAPTIVE LEARNING RATE
In this section, we ï¬rst introduce empirical evidence, then analyze the variance of the adaptive
learning rate to support our hypothesis â€“ Due to the lack of samples in the early stage, the adaptive
learning rate has an undesirably large variance, which leads to suspicious/bad local optima.
To convey our intuition, we begin with a special case. When t = 1, we have Ïˆ(g1) =
p
1/g2
1.
We view {g1, Â· Â· Â· , gt} as i.i.d. Gaussian random variables following N(0, Ïƒ2)2. Therefore, 1/g2
1
is subject to the scaled inverse chi-squared distribution, Scale-inv-X 2(1, 1/Ïƒ2), and Var[
p
1/g2
1]
is divergent. It means that the adaptive ratio can be undesirably large in the ï¬rst stage of learning.
Meanwhile, setting a small learning rate at the early stage can reduce the variance (Var[Î±x] =
Î±2 Var[x]), thus alleviating this problem. Therefore, we suggest it is the unbounded variance of the
adaptive learning rate in the early stage that causes the problematic updates.
3.1
WARMUP AS VARIANCE REDUCTION
In this section, we design a set of controlled experiments to verify our hypothesis. Particularly, we
design two variants of Adam that reducing the variance of the adaptive learning rate: Adam-2k and
Adam-eps. We compare them to vanilla Adam with and without warmup on the IWSLTâ€™14 German
to English translation dataset (Cettolo et al., 2014).
In order to reduce the variance of the adaptive learning rate (Ïˆ(.)), Adam-2k only updates Ïˆ(.) in the
ï¬rst two thousand iterations, while the momentum (Ï†(.)) and parameters (Î¸) are ï¬xed3; other than
this, it follows the original Adam algorithm. To make comparison with other methods, its iterations
are indexed from -1999 instead of 1. In Figure 1, we observe that, after getting these additional
two thousand samples for estimating the adaptive learning rate, Adam-2k avoids the convergence
problem of the vanilla-Adam. Also, comparing Figure 2 and Figure 3, getting large enough samples
prevents the gradient distribution from being distorted. These observations verify our hypothesis
that the lack of sufï¬cient data samples in the early stage is the root cause of the convergence issue.
2The mean zero normal assumption is valid at the beginning of the training, since weights are sampled from
normal distributions with mean zero (Balduzzi et al., 2017), further analysis is conducted in Section 5.3.
3Different from Gotmare et al. (2019), all parameters and ï¬rst moments are frozen in the ï¬rst 2000 iterations.
3

Published as a conference paper at ICLR 2020
Another straightforward way to reduce the variance is to increase the value of Ïµ in bÏˆ(g1, Â· Â· Â· , gt) =
âˆš
1âˆ’Î²t
2
Ïµ+âˆš
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i
. Actually, if we assume bÏˆ(.) is subject to the uniform distribution, its vari-
ance equals to
1
12Ïµ2 . Therefore, we design Adam-eps, which uses a non-negligibly large Ïµ = 10âˆ’4,
while Ïµ = 10âˆ’8 for vanilla Adam. Its performance is summarized in Figure 1. We observe that it
does not suffer from the serious convergence problem of vanilla-Adam. This further demonstrates
that the convergence problem can be alleviated by reducing the variance of the adaptive learning
rate, and also explains why tuning Ïµ is important in practice (Liu et al., 2019). Besides, similar to
Adam-2k, it prevents the gradient distribution from being distorted (as shown in Figure 3). However,
as in Figure 1, it produces a much worse performance comparing to Adam-2k and Adam-warmup.
We conjecture that this is because large Ïµ induces a large bias into the adaptive learning rate and
slows down the optimization process. Thus, we need a more principled and rigorous way to con-
trol the variance of the adaptive learning rate. In the next subsection, we will present a theoretical
analysis of the variance of the adaptive learning rate.
3.2
ANALYSIS OF ADAPTIVE LEARNING RATE VARIANCE
As mentioned before, Adam uses the exponential moving average to calculate the adaptive learning
rate. For gradients {g1, Â· Â· Â· , gt}, their exponential moving average has a larger variance than their
simple average. Also, in the early stage (t is small), the difference of the exponential weights of
{g1, Â· Â· Â· , gt} is relatively small (up to 1 âˆ’Î²tâˆ’1
2
). Therefore, for ease of analysis, we approximate
the distribution of the exponential moving average as the distribution of the simple average (Nau,
2014), i.e., p(Ïˆ(.)) = p(
r
1âˆ’Î²t
2
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i ) â‰ˆp(
q
t
Pt
i=1 g2
i ). Since gi âˆ¼N(0, Ïƒ2), we have
t
Pt
i=1 g2
i âˆ¼Scale-inv-X 2(t, 1
Ïƒ2 ). Therefore, we assume
1âˆ’Î²t
2
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i also subjects to a scaled
inverse chi-square distribution with Ï degrees of freedom (further analysis on this approximation is
conducted in Section 5.3). Based on this assumption, we can calculate Var[Ïˆ2(.)] and the PDF of
Ïˆ2(.). Now, we proceed to the analysis of its square root variance, i.e., Var[Ïˆ(.)], and show how the
variance changes with Ï (which corresponds to number of used training samples).
Theorem 1. If Ïˆ2(.) âˆ¼Scale-inv-X 2(Ï, 1
Ïƒ2 ), Var[Ïˆ(.)] monotonically decreases as Ï increases.
Proof. For âˆ€Ï > 4, we have:
Var[Ïˆ(.)] = E[Ïˆ2(.)] âˆ’E[Ïˆ(.)]2 = Ï„ 2(
Ï
Ï âˆ’2 âˆ’Ï 22Ïâˆ’5
Ï€
B(Ï âˆ’1
2
, Ï âˆ’1
2
)2),
(2)
where B(.) is the beta function. By analyzing the derivative of Var[Ïˆ(.)], we know it monotonically
decreases as Ï increases. The detailed derivation is elaborated in the Appendix A.
Theorem 1 gives a qualitative analysis of the variance of the adaptive learning rate. It shows that,
due to the lack of used training samples in the early stage, Var[Ïˆ(.)] is larger than the late stage
(Figure 8). To rigorously constraint the variance, we perform a quantiï¬ed analysis on Var[Ïˆ(.)] by
estimating the degree of freedoms Ï.
4
RECTIFIED ADAPTIVE LEARNING RATE
In the previous section, Equation 2 gives the analytic form of Var[Ïˆ(.)], where Ï is the degree of
freedoms. Here, we ï¬rst give an estimation of Ï based on t to conduct a quantiï¬ed analysis for
Var[Ïˆ(g1, Â· Â· Â· , gt)], then we describe the design of the learning rate rectiï¬cation, and compare it to
the heuristic warmup strategies.
4.1
ESTIMATION OF Ï
The exponential moving average (EMA) can be interpreted as an approximation to the simple mov-
ing average (SMA) in real application (Nau, 2014), i.e.,
p
 
(1 âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i
1 âˆ’Î²t
2
!
â‰ˆp
 Pf(t,Î²2)
i=1
g2
t+1âˆ’i
f(t, Î²2)
!
.
(3)
4

Published as a conference paper at ICLR 2020
Algorithm 2: Rectiï¬ed Adam. All operations are element-wise.
Input: {Î±t}T
t=1: step size, {Î²1, Î²2}: decay rate to calculate moving average and moving 2nd
moment, Î¸0: initial parameter, ft(Î¸): stochastic objective function.
Output: Î¸t: resulting parameters
1 m0, v0 â†0, 0 (Initialize moving 1st and 2nd moment)
2 Ïâˆâ†2/(1 âˆ’Î²2) âˆ’1 (Compute the maximum length of the approximated SMA)
3 while t = {1, Â· Â· Â· , T} do
4
gt â†âˆ‡Î¸ft(Î¸tâˆ’1) (Calculate gradients w.r.t. stochastic objective at timestep t)
5
vt â†Î²2vtâˆ’1 + (1 âˆ’Î²2)g2
t (Update exponential moving 2nd moment)
6
mt â†Î²1mtâˆ’1 + (1 âˆ’Î²1)gt (Update exponential moving 1st moment)
7
c
mt â†mt/(1 âˆ’Î²t
1) (Compute bias-corrected moving average)
8
Ït â†Ïâˆâˆ’2tÎ²t
2/(1 âˆ’Î²t
2)(Compute the length of the approximated SMA)
9
if the variance is tractable, i.e., Ït > 4 then
10
lt â†
p
(1 âˆ’Î²t
2)/vt (Compute adaptive learning rate)
11
rt â†
q
(Ïtâˆ’4)(Ïtâˆ’2)Ïâˆ
(Ïâˆâˆ’4)(Ïâˆâˆ’2)Ït (Compute the variance rectiï¬cation term)
12
Î¸t â†Î¸tâˆ’1 âˆ’Î±trt c
mtlt (Update parameters with adaptive momentum)
13
else
14
Î¸t â†Î¸tâˆ’1 âˆ’Î±t c
mt (Update parameters with un-adapted momentum)
15 return Î¸T
where f(t, Î²2) is the length of the SMA which allows the SMA to have the same â€œcenter of massâ€
with the EMA. In other words, f(t, Î²2) satisï¬es:
(1 âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
Â· i
1 âˆ’Î²t
2
=
Pf(t,Î²2)
i=1
(t + 1 âˆ’i)
f(t, Î²2)
.
(4)
By solving Equation 4, we have: f(t, Î²2) =
2
1âˆ’Î²2 âˆ’1 âˆ’
2tÎ²t
2
1âˆ’Î²t
2 .
In the previous section,
we assume:
1âˆ’Î²t
2
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i
âˆ¼Scale-inv-X 2(Ï, 1
Ïƒ2 ). Here, since gi âˆ¼N(0, Ïƒ2), we have
Pf(t,Î²2)
i=1
g2
t+1âˆ’i
f(t,Î²2)
âˆ¼Scale-inv-X 2(f(t, Î²2), 1
Ïƒ2 ). Thus, Equation 3 views Scale-inv-X 2(f(t, Î²2), 1
Ïƒ2 )
as an approximation to Scale-inv-X 2(Ï, 1
Ïƒ2 ). Therefore, we treat f(t, Î²2) as an estimation of Ï. For
ease of notation, we mark f(t, Î²2) as Ït. Also, we refer
2
1âˆ’Î²2 âˆ’1 as Ïâˆ(maximum length of the
approximated SMA), due to the inequality f(t, Î²2) â‰¤limtâ†’âˆf(t, Î²2) =
2
1âˆ’Î²2 âˆ’1.
4.2
VARIANCE ESTIMATION AND RECTIFICATION
Based on previous estimations, we have Var[Ïˆ(.)] = Ï„ 2(
Ït
Ïtâˆ’2 âˆ’Ït 22Ïtâˆ’5
Ï€
B( Ïtâˆ’1
2
, Ïtâˆ’1
2
)2). The
value of this function in the early stage is signiï¬cantly larger than the late stage (as analyzed later, it
decays roughly at the speed of O( 1
Ït )). For example, the variance at Ït = 5 is over 100 times larger
than the variance at Ït = 500. Additionally, based on Theorem 1, we know minÏt Var[Ïˆ(.)] =
Var[Ïˆ(.)]|Ït=Ïâˆand mark this minimal value as Cvar. In order to ensure that the adaptive learning
rate (Ïˆ(.)) has consistent variance, we rectify the variance at the t-th timestamp as below,
Var[rt Ïˆ(g1, Â· Â· Â· , gt)] = Cvar
where
rt =
p
Cvar/Var[Ïˆ(g1, Â· Â· Â· , gt)].
Although we have the analytic form of Var[Ïˆ(.)] (i.e., Equation 2), it is not numerically stable.
Therefore, we use the ï¬rst-order approximation to calculate the rectiï¬cation term. Speciï¬cally, by
approximating
p
Ïˆ2(.) to the ï¬rst order (Wolter, 2007),
p
Ïˆ2(.) â‰ˆ
p
E[Ïˆ2(.)] +
1
2
p
E[Ïˆ2(.)]
(Ïˆ2(.) âˆ’E[Ïˆ2(.)])
and
Var[Ïˆ(.)] â‰ˆVar[Ïˆ2(.)]
4 E[Ïˆ2(.)] .
Since Ïˆ2(.) âˆ¼Scale-inv-X 2(Ït, 1
Ïƒ2 ), we have:
Var[Ïˆ(.)] â‰ˆÏt/[2(Ït âˆ’2)(Ït âˆ’4)Ïƒ2].
(5)
In Section 5.3, we conduct simulation experiments to examine Equation 5 and ï¬nd that it is a reliable
approximation. Based on Equation 5, we know that Var[
p
Ïˆ(.)] decreases approximately at the
5

Published as a conference paper at ICLR 2020
40
42
44
46
48
50
52
54
56
58
60
0
0.5M
1M
1.5M
2M
2.5M
3M
3.5M
4M
34
36
38
40
42
44
46
48
50
52
54
0
1
2
3
4
5
6
7
8
9
10 11 12 13
110
120
130
140
150
160
170
10k 12k 14k 16k 18k 20k 22k
8k
Training PPL
Test PPL
Gradient updates
Iterations over training set
RAdam
Adam
36.92
35.70
Figure 4: Language modeling (LSTMs) on the One Billion Word.
Table 1: Image Classiï¬cation
Method
Acc.
CIFAR10
SGD
91.51
Adam
90.54
RAdam
91.38
ImageNet
SGD
69.86
Adam
66.54
RAdam
67.62
80
82
84
86
88
90
92
0
20
40
60
80
100
120
140
160
Adam
SGD
RAdam
Iteration over entire dataset
Test accuracy
46
48
50
52
54
56
58
60
62
64
66
68
70
0
10
20
30
40
50
60
70
80
90
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
2.1
2.2
2.3
2.4
0
10
20
30
40
50
60
70
80
90
0
20
40
60
80
100
120
140
160
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Training loss
Iteration over entire dataset
Iteration over entire dataset
Iteration over entire dataset
Test accuracy
Training loss
ImageNet
CIFAR10
Figure 5: Training of ResNet-18 on the ImageNet and ResNet-20 on the CIFAR10 dataset.
speed of O( 1
Ït ). With this approximation, we can calculate the rectiï¬cation term as:
rt =
s
(Ït âˆ’4)(Ït âˆ’2)Ïâˆ
(Ïâˆâˆ’4)(Ïâˆâˆ’2)Ït
.
Applying our rectiï¬cation term to Adam, we come up with a new variant of Adam, Rectiï¬ed Adam
(RAdam), as summarized in Algorithm 2. Speciï¬cally, when the length of the approximated SMA is
less or equal than 4, the variance of the adaptive learning rate is intractable and the adaptive learning
rate is inactivated. Otherwise, we calculate the variance rectiï¬cation term and update parameters
with the adaptive learning rate. It is worth mentioning that, if Î²2 â‰¤0.6, we have Ïâˆâ‰¤4 and
RAdam is degenerated to SGD with momentum.
4.3
IN COMPARISON WITH WARMUP AND OTHER STABILIZATION TECHNIQUES
Different from the analysis in this paper, warmup is originally proposed to handle training with very
large batches for SGD (Goyal et al., 2017; Gotmare et al., 2019; Bernstein et al., 2018; Xiao et al.,
2017). We notice that rt has a similar form to the heuristic linear warmup, which can be viewed as
setting the rectiï¬cation term as min(t,Tw)
Tw
. It veriï¬es our intuition that warmup works as a variance
reduction technique. RAdam deactivates the adaptive learning rate when its variance is divergent,
thus avoiding undesired instability in the ï¬rst few updates. Besides, our method does not require an
additional hyperparameter (i.e., Tw) and can automatically adapt to different moving average rules.
Here, we identify and address an underlying issue of adaptive optimization methods independent
of (neural) model architectures. Thus, the proposed rectiï¬cation term is orthogonal to other train-
ing stabilization techniques such as gradient clipping (Bengio et al., 2013), smoothing the adaptive
learning rate (i.e., increasing Ïµ, applying geometric mean ï¬lter (Chen et al., 2018), or adding range
constraints (Luo et al., 2019)), initialization (Balduzzi et al., 2017; Zhang et al., 2019) and normal-
ization (Ba et al., 2016; Ioffe & Szegedy, 2015). Indeed, these techniques can be combined with the
proposed variance rectiï¬cation method.
5
EXPERIMENTS
We evaluate RAdam on several benchmarks: One Billion Word for language modeling; Cifar10
and ImageNet for image classiï¬cation; IWSLTâ€™14 De-En/EN-DE and WMTâ€™16 EN-De for neural
machine translation. Following Loshchilov & Hutter (2018), we decouple weight decays in the
vanilla Adam, Adam with warmup and RAdam in our experiments. Details are in Appendix B.
6

Published as a conference paper at ICLR 2020
78
80
82
84
86
88
90
92
0
20
40
60
80
100
120
140
160
180
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0
20
40
60
80
100
120
140
160
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0
20
40
60
80
100
120
140
160
lr = 0.1
lr = 0.03
lr = 0.01
lr = 0.003
SGD
RAdam
Adam
Test accuracy
Training loss
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0
20
40
60
80
100
120
140
160
78
80
82
84
86
88
90
92
0
20
40
60
80
100
120
140
160
78
80
82
84
86
88
90
92
0
20
40
60
80
100
120
140
160
Diï¬€erent learning 
rates lead to similar 
performance.
Sensitive to the choice 
of the learning rate.
X-axis is the 
epoch #.  
Figure 6: Performance of RAdam, Adam and SGD with different learning rates on CIFAR10.
87
87.5
88
88.5
89
89.5
90
90.5
91
91.5
0
20
40
60
80
100
120
140
160
lr = 0.1
lr = 0.03
lr = 0.01
lr = 0.003
Test accuracy
Training loss
200
Comparing to RAdam, heuristic linear warmup needs to tune the warmup length to get the similar performance.
1000
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0
20
40
60
80
100
120
140
160
87
87.5
88
88.5
89
89.5
90
90.5
91
91.5
0
20
40
60
80
100
120
140
160
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0
20
40
60
80
100
120
140
160
87
87.5
88
88.5
89
89.5
90
90.5
91
91.5
0
20
40
60
80
100
120
140
160
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0
20
40
60
80
100
120
140
160
87
87.5
88
88.5
89
89.5
90
90.5
91
91.5
0
20
40
60
80
100
120
140
160
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0
20
40
60
80
100
120
140
160
87
87.5
88
88.5
89
89.5
90
90.5
91
91.5
0
20
40
60
80
100
120
140
160
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0
20
40
60
80
100
120
140
160
RAdam
length:     100
500
Adam with warmup
X-axis is the epoch #
Figure 7: Performance of RAdam, Adam with warmup on CIFAR10 with different learning rates.
5.1
COMPARING TO VANILLA ADAM
As analyzed before, the adaptive learning rate has undesirably large variance in the early stage
of training and leads to suspicious/bad local optima on NMT. One question we are interested in
is: whether such an issue widely exits in other similar tasks and applications. Thus, we conduct
a set of experiments with two classical tasks of NLP and CV, i.e., language modeling and image
classiï¬cation. RAdam not only results in consistent improvements over the vanilla Adam, but also
demonstrates its robustness to the change of learning rates. It veriï¬es that the variance issue exists
in various machine learning applications, and has a big impact on the model behavior.
Performance Comparison.
The performances on language modeling (i.e., One Billion
Word (Chelba et al., 2013)) and image classiï¬cation (i.e., CIFAR10 (Krizhevsky et al., 2009) and
ImageNet (Deng et al., 2009)) are presented in Figure 4, 5. The results show that RAdam out-
performs Adam in all three datasets. As shown in Figure 4, although the rectiï¬cation term makes
RAdam slower than the vanilla Adam in the ï¬rst few epochs, it allows RAdam to converge faster
after that. In other words, by reducing the variance of the adaptive learning rate in the early stage, it
gets both faster convergence and better performance, which veriï¬es the impact of the variance issue.
We also observe that RAdam obtains consistent improvements over Adam on image classiï¬cation.
It is worth noting that, on both ImageNet and CIFAR10, although RAdam fails to outperform SGD
in terms of test accuracy, it results in a better training performance (e.g., the training accuracy of
SGD, Adam, and RAdam on ImageNet are 69.57, 69.12 and 70.30 respectively).
Robustness to Learning Rate Change. Besides performance improvements, RAdam also improves
the robustness of model training. We use different initial learning rates, conduct experiments with
ResNet-20 on the CIFAR10 datasets, and summarize their performance in Figure 6. For learning
rates within a broad range (i.e., {0.1, 0.03, 0.01, 0.003}), RAdam achieves consistent model perfor-
mances (their test accuracy curves highly overlap with each other), while Adam and SGD are shown
to be more sensitive to the learning rate. The observation can be interpreted that by rectifying the
variance of the adaptive learning rate, RAdam improves the robustness of model training and can
adapt to different learning rates of a broader range.
7

Published as a conference paper at ICLR 2020
Table 2: BLEU score on Neural Machine Translation.
Method
IWSLTâ€™14 DE-EN
IWSLTâ€™14 EN-DE
WMTâ€™16 EN-DE
Adam with warmup
34.66 Â± 0.014
28.56 Â± 0.067
27.03
RAdam
34.76 Â± 0.003
28.48 Â± 0.054
27.27
5.2
COMPARING TO HEURISTIC WARMUP
To examine the effectiveness of RAdam, we ï¬rst conduct comparisons on neural machine transla-
tion, on which the state-of-the-art employs Adam with the linear warmup. Speciï¬cally, we conduct
experiments on three datasets, i.e., IWSLTâ€™14 De-En, IWSLTâ€™14 En-De, and WMTâ€™16 En-De. Due
to the limited size of the IWSLTâ€™14 dataset, we conduct experiments using 5 different random seeds
and report their mean and standard derivation. As discussed before, the vanilla Adam algorithm
leads to suspicious/bad local optima (i.e., converges to a training perplexity around 500), and needs
a learning rate warmup stage to stabilize the training.
We summarize the performance obtained with the heuristic warmup and our proposed rectiï¬cation
term in Table 2 and visualize the training curve of IWSLT De-En in Figure 1. With a consistent
adaptive learning rate variance, our proposed method achieves similar performance to that of previ-
ous state-of-the-art warmup heuristics. It veriï¬es our intuition that the problematic updates of Adam
are indeed caused by the undesirably large variance in the early stage.
Moreover, we applied Adam with warmup on the CIFAR10 dataset. Its best accuracy on the test
set is 91.29, which is similar to RAdam (91.38). However, we found that RAdam requires less hy-
perparameter tuning. Speciï¬cally, we visualize their learning curves in Figure 7. For some warmup
steps, Adam with warmup is relatively more sensitive to the choice of the learning rate. RAdam,
at the same time, is not only more robust, but also can automatically control the warmup behav-
ior (i.e., without requiring the length of warmup). For example, when setting the learning rate as
0.1, Adam with 100 steps of warmup fails to get satisfying performance and only results in an ac-
curacy of 90.13; RAdam successfully gets an accuracy of 91.06, with the original setting of the
moving average calculation (i.e., Î²1 = 0.9, Î²2 = 0.999). We conjecture the reason is due to the fact
that RAdam, which is based on a rigorous variance analysis, explicitly avoids the extreme situation
where the variance is divergent, and rectiï¬es the variance to be consistent in other situations.
5.3
SIMULATED VERIFICATION
In Sections 3 and 4, we approximate Var[
q
t/ Pt
i=1 g2
i ] to the ï¬rst order, and assume Ïˆ2(.) =
1âˆ’Î²t
2
(1âˆ’Î²2) Pt
i=1 Î²tâˆ’i
2
g2
i subjects to a scaled inverse chi-square distribution (this assumption covers the
approximation from EMA to SMA). Here, we examine these two approximations using simulations.
First Order Approximation of Var[
q
t/ Pt
i=1 g2
i ]. To compare Equations 5 and 2, we assume
Ï„ = 1 and plot their values and difference for Î½ = {5, Â· Â· Â· , 500} in Figure 8. The curve of the
analytic form and the ï¬rst-order approximation highly overlap, and their difference is much smaller
than their value. This result veriï¬es that our ï¬rst-order approximation is very accurate.
Scaled Inverse Chi-Square Distribution Assumption. In this paper, we assume gi accords to a
Normal distribution with a zero mean. We also assume Ïˆ2(.) accords to the scaled inverse chi-square
distribution to derive the variance of Var[Ïˆ(.)], based on the similarity between the exponential
moving average and simple moving average. Here, we empirically verify this assumption.
Speciï¬cally, since gi in the optimization problem may not be zero-mean, we assume its expectation
is Âµ and sample gi from N(Âµ, 1). Then, based on these samples, we calculate the variance of the
original adaptive learning rate and the proposed rectiï¬ed adaptive learning rate, i.e., Var[ 1
b
vt ] and
Var[ rt
b
vt ] respectively. We set Î²2 to 0.999, the number of sampled trajectories to 5000, the number
of iterations to 6000, and summarize the simulation results in Figure 9. Across all six settings with
different Âµ, the adaptive learning rate has a larger variance in the ï¬rst stage and the rectiï¬ed adaptive
learning rate has relative consistent variance. This veriï¬es the reliability of our assumption.
8

Published as a conference paper at ICLR 2020
0
200
400
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
Diï¬€erence
Analytic
First Order Approx.
Figure 8: The value of Equation 2,
Equation 5 and their difference (abso-
lute difference). The x-axis is Ï and
the y-axis is the variance (log scale).
0
2500
5000
10âˆ’3
10âˆ’2
10âˆ’1
0
2500
5000
10âˆ’3
10âˆ’2
10âˆ’1
0
2500
5000
10âˆ’3
10âˆ’2
10âˆ’1
Var[ 1
vt]
Var[ct
vt]
Âµ = 0
Âµ = 0.001
Âµ = 0.01
0
2500
5000
10âˆ’3
10âˆ’2
10âˆ’1
0
2500
5000
10âˆ’4
10âˆ’3
10âˆ’2
0
2500
5000
10âˆ’7
10âˆ’6
10âˆ’5
Âµ = 0.1
Âµ = 1
Âµ = 10
Figure 9: The simulation of Var[ 1
vt ] and Var[ ct
vt ]. The x-axis
is iteration # (from 5), the y-axis is the variance (log scale).
6
CONCLUSION
In this paper, we explore the underlying principle of the effectiveness of the warmup heuristic used
for adaptive optimization algorithms. Speciï¬cally, we identify that, due to the limited amount of
samples in the early stage of model training, the adaptive learning rate has an undesirably large
variance and can cause the model to converge to suspicious/bad local optima. We provide both
empirical and theoretical evidence to support our hypothesis, and further propose a new variant
of Adam, whose adaptive learning rate is rectiï¬ed so as to have a consistent variance. Empirical
results demonstrate the effectiveness of our proposed method. In future work, we plan to replace the
rectiï¬cation strategy by sharing the second moment estimation across similar parameters.
ACKNOWLEDGE
We thank Zeyuan Allen-Zhu for valuable discussions and comments, Microsoft Research Technol-
ogy Engineering team for setting up GPU machines. Research was sponsored in part by DARPA
No. W911NF-17-C-0099 and FA8750-19-2-1004, National Science Foundation IIS 16-18481, IIS
17-04532, and IIS-17-41317, and DTRA HDTRA11810026.
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question? In ICML,
2017.
Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing
recurrent networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 8624â€“8628. IEEE, 2013.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:
Compressed optimisation for non-convex problems. In ICML, 2018.
Augustin Cauchy.
MÂ´ethode gÂ´enÂ´erale pour la rÂ´esolution des systemes dâ€™Â´equations simultanÂ´ees.
Comp. Rend. Sci. Paris, 25(1847):536â€“538, 1847.
Mauro Cettolo, Jan Niehues, Sebastian StÂ¨uker, Luisa Bentivogli, and Marcello Federico. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on
Spoken Language Translation,, 2014.
Ciprian Chelba, Tomas Mikolov, Michael Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and
Tony Robinson. One billion word benchmark for measuring progress in statistical language mod-
eling. In INTERSPEECH, 2013.
9

Published as a conference paper at ICLR 2020
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
Closing the gener-
alization gap of adaptive gradient methods in training deep neural networks.
arXiv preprint
arXiv:1806.06763, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In ICML, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In COLT, 2010.
Carl-Friedrich Gauss. Theoria combinationis observationum erroribus minimis obnoxiae. Commen-
tationes Societatis Regiae Scientiarum Gottingensis Recentiores, 1823.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at
deep learning heuristics: Learning rate restarts, warmup and distillation. In ICLR, 2019.
Priya Goyal, Piotr DollÂ´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Cited on, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Liyuan Liu, Xiang Ren, Jingbo Shang, Jian Peng, and Jiawei Han. Efï¬cient contextualized repre-
sentation: Language model pruning for sequence labeling. EMNLP, 2018.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. In ICLR, 2018.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In ICLR, 2019.
Robert Nau. Forecasting with moving averages. 2014.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In NAACL, 2019.
Martin Popel and OndË‡rej Bojar. Training tips for the transformer model. The Prague Bulletin of
Mathematical Linguistics, 110(1):43â€“70, 2018.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
ICLR, 2018.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, 2016.
10

Published as a conference paper at ICLR 2020
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Kirk M Wolter. Taylor series methods. In Introduction to variance estimation. 2007.
Lin Xiao, Adams Wei Yu, Qihang Lin, and Weizhu Chen. Dscovr: Randomized primal-dual block
coordinate algorithms for asynchronous distributed optimization. J. Mach. Learn. Res., 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In ICLR, 2019.
11

Published as a conference paper at ICLR 2020
A
PROOF OF THEOREM 1
For ease of notation, we refer Ïˆ2(.) as x and
1
Ïƒ2 as Ï„ 2. Thus, x âˆ¼Scale-inv-X 2(Ï, Ï„ 2) and:
p(x) = (Ï„ 2Ï/2)Ï/2
Î“(Ï/2)
exp[ âˆ’ÏÏ„ 2
2x ]
x1+Ï/2
and
E[x] =
Ï
(Ï âˆ’2)Ïƒ2 (âˆ€Ï > 2)
(6)
where Î“(.) is the gamma function. Therefore, we have:
E[âˆšx] =
Z âˆ
0
âˆšx p(x) dx = Ï„âˆšÏ Î“((Ï âˆ’1)/2)
âˆš
2 Î“(Ï/2)
(âˆ€Ï > 4).
(7)
Based on Equation 6 and 7, for âˆ€Ï > 4, we have:
Var[Ïˆ(.)] = Var[âˆšx] = E[x] âˆ’E[âˆšx]2 = Ï„ 2(
Ï
Ï âˆ’2 âˆ’Ï 22Ïâˆ’5
Ï€
B(Ï âˆ’1
2
, Ï âˆ’1
2
)2),
(8)
where B(.) is the beta function. To prove the monotonic property of Var[Ïˆ(.)], we need to show:
Lemma 1. for t â‰¥4, âˆ‚
âˆ‚t(
t
tâˆ’2 âˆ’t 22tâˆ’5
Ï€
B( tâˆ’1
2 , tâˆ’1
2 )2) < 0
Proof. The target inequality can be re-wrote as
âˆ‚
âˆ‚t(
t
t âˆ’2 âˆ’t 22tâˆ’5
Ï€
B(t âˆ’1
2
, t âˆ’1
2
)2)
=
âˆ’2
(t âˆ’2)2 âˆ’22tâˆ’5
Ï€
B(t âˆ’1
2
, t âˆ’1
2
)2 âˆ’t 22tâˆ’5 ln 4
Ï€
B(t âˆ’1
2
, t âˆ’1
2
)2
âˆ’2t 22tâˆ’5
Ï€
B(t âˆ’1
2
, t âˆ’1
2
)2(Î¨(t âˆ’1
2
) âˆ’Î¨(t âˆ’1)),

Î¨(x) = Î“â€²(x)
Î“(x)

< 0
This inequality is equivalent to:
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 + 1 + t ln 4 + 2tÎ¨(t âˆ’1
2
)
> 2tÎ¨(t âˆ’1)
(i)
= t[Î¨(t âˆ’1
2
) + Î¨( t
2) + ln 4],
where (i) is derived from Legendre duplication formula. Simplify the above inequality, we get:
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 + 1 + tÎ¨(t âˆ’1
2
) âˆ’tÎ¨( t
2) > 0,
We only need to show
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 + 1 + tÎ¨(t âˆ’1
2
) âˆ’tÎ¨( t
2)
â‰¥
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 + 2 + t(ln(t/2) âˆ’1/(t/2 âˆ’0.5)) âˆ’t ln(t/2)
=
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 âˆ’
2
t âˆ’1
>
64Ï€
(t âˆ’2)24tB( tâˆ’1
2 , tâˆ’1
2 )2 âˆ’
2
t âˆ’2 â‰¥0,
where the ï¬rst inequality is from ln(x) âˆ’1/(2x) > Î¨(x) > ln(x + 0.5) âˆ’1/x.
Therefore, we only need to show
32Ï€ â‰¥(t âˆ’2)4tB(t âˆ’1
2
, t âˆ’1
2
)2,
which is equivalent to
(t âˆ’2)4tB(t âˆ’1
2
, t âˆ’1
2
)2 = (t âˆ’2)4t Î“( tâˆ’1
2 )4
Î“(t âˆ’1)2
(i)
= (t âˆ’2)4t Î“( tâˆ’1
2 )2
Î“(t/2)2 42âˆ’tÏ€ = 16Ï€(t âˆ’2)Î“( tâˆ’1
2 )2
Î“(t/2)2 â‰¤32Ï€,
12

Published as a conference paper at ICLR 2020
where (i) is from Legendre duplication formula.
So we only need to show
(t âˆ’2)Î“( tâˆ’1
2 )2
Î“(t/2)2 â‰¤2
(9)
Using Gautschiâ€™s inequality ( Î“(x+1)
Î“(x+s) < (x + 1)1âˆ’s), we have
(t âˆ’2)Î“( tâˆ’1
2 )2
Î“(t/2)2 â‰¤(t âˆ’2)(t âˆ’1
2
)âˆ’1 = 2(t âˆ’2)
t âˆ’1
< 2
(10)
B
IMPLEMENTATION DETAILS
B.1
LANGUAGE MODELING
Our implementation is based on the previous work (Liu et al., 2018). Speciï¬cally, we use two-layer
LSTMs with 2048 hidden states with adaptive softmax to conduct experiments on the one billion
words dataset. Word embedding (random initialized) of 300 dimensions is used as the input and the
adaptive softmax is incorporated with a default setting (cut-offs are set to [4000, 40000, 200000]).
Additionally, as pre-processing, we replace all tokens occurring equal or less than 3 times with as
UNK. Dropout is applied to each layer with a ratio of 0.1, gradients are clipped at 5.0. We use the
default hyper-parameters to update moving averages, i.e.Î²1 = 0.9 and Î²2 = 0.999. The learning
rate is set to start from 0.001, and decayed at the start of 10th epochs. LSTMs are unrolled for 20
steps without resetting the LSTM states and the batch size is set to 128. All models are trained on
one NVIDIA Tesla V100 GPU.
B.2
IMAGEINE CLASSIFICATION
We use the default ResNet architectures (He et al., 2016) in a public pytorch re-implementation4.
Speciï¬cally, we use 20-layer ResNet (9 Basic Blocks) for CIFAR-10 and 18-layer ResNet (8 Basic
Blocks) for ImageNet. Batch size is 128 for CIFAR-10 and 256 for ImageNet. The model is trained
for 186 epoches and the learning rate decays at the 81-th and the 122-th epoches by 0.1 on CIFAR-
10, while the model is trained for 90 epoches and the learning rate decays at the 31-th and the 61-th
epoch by 0.1 on ImageNet. For Adam and RAdam, we set Î²1 = 0.9, Î²2 = 0.999. For SGD, we
set the momentum factor as 0.9. The weight decay rate is 10âˆ’4. Random cropping and random
horizontal ï¬‚ipping are applied to training data.
B.3
NEURAL MACHINE TRANSLATION
Our experiments are based on the default Transformers (Vaswani et al., 2017) implementation from
the fairseq package (Ott et al., 2019). Speciï¬cally, we use word embedding with 512 dimensions
and 6-layer encoder / decoder with 4 head and 1024 feedforward dimensions on the IWSLT14â€™
dataset; use word embedding with 512 dimension and 6-layer encoder/decoder with 8 heads and
2048 feedforward dimensions on the WMT14â€™ dataset. Label smoothed cross entropy is used as
the objective function with an uncertainty = 0.1 (Szegedy et al., 2016). We use linear learning rate
decay starting from 3eâˆ’4, and the checkpoints of the last 20 epoches are averaged before evaluation.
As to the wamrup strategy, we use a linear warmup for Adam in the ï¬rst 4000 updates, and set Î²2
to satisfy Î½ = 4000 (Î²2 = 0.9995). In the IWSLTâ€™14 dataset, we conduct training on one NVIDIA
Tesla V100 GPU, set maximum batch size as 4000, apply dropout with a ratio 0.3, using weight
decay of 0.0001 and clip the gradient norm at 25. In the WMTâ€™16 dataset, we conduct training on
four NVIDIA Quadro R8000 GPUs and set maximum batch size as 8196.
C
DOWNGRADING TO SGDM
As a byproduct determined by math derivations, we degenerated RAdam to SGD with momentum
in the ï¬rst several updates. Although this stage only contains several gradient updates, these up-
4https://github.com/bearpaw/pytorch-classification
13

Published as a conference paper at ICLR 2020
dates could be quite damaging (e.g., in our Figure 2, the gradient distribution is distorted within 10
gradient updates). Intuitively, updates with divergent adaptive learning rate variance could be more
damaging than the ones with converged variance, as divergent variance implies more instability. As
a case study, we performed experiments on the CIFAR10 dataset. Five-run average results are sum-
marized in Table 3. The optimizer fails to get an equally reliably model when changing the ï¬rst
4 updates to Adam, yet the inï¬‚uence of switching is less deleterious when we change 5-8 updates
instead. This result veriï¬es our intuition and is in agreement with our theory â€” the ï¬rst few updates
could be more damaging than later updates. By saying that, we still want to emphasize that this part
(downgrading to SGDM) is only a minor part of our algorithm design whereas our main focus is on
the mechanism of warmup and the derivation of the rectiï¬cation term.
Table 3: Performance on CIFAR10 (lr = 0.1).
1-4 steps
5-8 steps
8+ steps
test
acc
train
loss
train
error
RAdam
RAdam
RAdam
91.08
0.021
0.74
Adam (w. divergent var.)
RAdam
RAdam
89.98
0.060
2.12
SGD
Adam (w. convergent var.)
RAdam
90.29
0.038
1.23
14

