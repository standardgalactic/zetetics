Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
Zitong Yang * 1 Yaodong Yu * 1 Chong You 1 Jacob Steinhardt 1 2 Yi Ma 1
Abstract
The classical bias-variance trade-off predicts that
bias decreases and variance increases with model
complexity, leading to a U-shaped risk curve.
Recent work calls this into question for neural
networks and other over-parameterized models,
for which it is often observed that larger models
generalize better. We provide a simple explana-
tion for this by measuring the bias and variance
of neural networks: while the bias is monoton-
ically decreasing as in the classical theory, the
variance is unimodal or bell-shaped: it increases
then decreases with the width of the network. We
vary the network architecture, loss function, and
choice of dataset and conﬁrm that variance uni-
modality occurs robustly for all models we con-
sidered. The risk curve is the sum of the bias
and variance curves and displays different qual-
itative shapes depending on the relative scale of
bias and variance, with the double descent curve
observed in recent literature as a special case. We
corroborate these empirical results with a theo-
retical analysis of two-layer linear networks with
random ﬁrst layer. Finally, evaluation on out-of-
distribution data shows that most of the drop in ac-
curacy comes from increased bias while variance
increases by a relatively small amount. More-
over, we ﬁnd that deeper models decrease bias
and increase variance for both in-distribution and
out-of-distribution data.
1. Introduction
Bias-variance trade-off is a fundamental principle for un-
derstanding the generalization of predictive learning models
(Hastie et al., 2001). The bias is an error term that stems
*Equal contribution
1Department of Electrical Engineering
and Computer Sciences, University of California, Berkeley.
2Department of Statistics, University of California, Berkeley. Cor-
respondence to: Zitong Yang <zitong@berkeley.edu>, Yaodong
Yu <yyu@eecs.berkeley.edu>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).
from a mismatch between the model class and the under-
lying data distribution, and is typically monotonically non-
increasing as a function of the complexity of the model. The
variance measures sensitivity to ﬂuctuations in the training
set and is often attributed to a large number of model pa-
rameters. Classical wisdom predicts that model variance
increases and bias decreases monotonically with model com-
plexity (Geman et al., 1992). Under this perspective, we
should seek a model that has neither too little nor too much
capacity and achieves the best trade-off between bias and
variance.
In contrast, modern practice for neural networks repeat-
edly demonstrates the beneﬁt of increasing the number of
neurons (Krizhevsky et al., 2012; Simonyan & Zisserman,
2015; Zhang et al., 2017), even up to the point of saturat-
ing available memory. The inconsistency between classical
theory and modern practices suggests that some arguments
in the classical theory can not be applied to modern neural
networks.
Geman et al. (1992) ﬁrst studied the bias and variance of
the neural networks and give experimental evidence that
the variance is indeed increasing as the width of the neu-
ral network increases. Since Geman et al. (1992), Neal
et al. (2019) ﬁrst experimentally measured the variance of
modern neural network architectures and shown that the
variance can actually be decreasing as the width increases to
a highly overparameterized regime. Recently, Belkin et al.
(2019a; 2018; 2019b) directly studied the risk of modern
machine learning models and proposed a double descent risk
curve, which has also been analytically characterized for cer-
tain regression and classiﬁcation models (Mei & Montanari,
2019; Hastie et al., 2019; Spigler et al., 2019; Deng et al.,
2019; Advani & Saxe, 2017; Bartlett et al., 2020; Chatterji
& Long, 2020). However, there exists two mysteries around
the double descent risk curve. First, the double descent
phenomenon can not be robustly observed (Nakkiran et al.,
2019; Ba et al., 2020). In particular, to observe it in modern
neural network architectures, we sometimes have to arti-
ﬁcially inject label noise (Nakkiran et al., 2019). Second,
there lacks an explanation for why the double descent risk
curve should occur. In this work, we offer an simple expla-
nation for these two mysteries by proposing an unexpected
unimodal variance curve.
arXiv:2002.11328v3  [cs.LG]  8 Dec 2020

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
(a) Case 1
(b) Case 2
(c) Case 3
Figure 1. Typical cases of expected risk curve (in black) in neural networks. Blue: squared bias curve. Red: variance curve.
Speciﬁcally, we measure the bias and variance of modern
deep neural networks trained on commonly used computer
vision datasets. Our main ﬁnding is that while the bias
is monotonically decreasing with network width as in the
classical theory, the variance curve is unimodal or bell-
shaped: it ﬁrst increases and then decreases (see Figure
2). Therefore, the unimodal variance is consistent with the
ﬁnding of Neal et al. (2019), who observed that the variance
eventually decreases in the overparameterized regime. In
particular, the unimodal variance curve can also be observed
in Neal et al. (2019, Figure 1, 2, 3). However, Neal et al.
(2019) did not point out the characteristic shape of the vari-
ance or connect it to double descent. More importantly, we
demonstrate that the unimodal variance phenomenon can
be robustly observed for varying network architecture and
dataset. Moreover, by using a generalized bias-variance
decomposition for Bregman divergences (Pfau, 2013), we
verify that it occurs for both squared loss and cross-entropy
loss.
This unimodal variance phenomenon initially appears to
contradict recent theoretical work suggesting that both bias
and variance are non-monotonic and exhibit a peak in some
regimes (Mei & Montanari, 2019; Hastie et al., 2019) . The
difference is that this previous work considered the ﬁxed-
design bias and variance, while we measure the random-
design bias and variance (we describe the differences in
detail in §2.1). Prior to our work, Nakkiran (2019) also
considered the variance of linear regression in the random-
design setting, and Rosset & Tibshirani (2017) discussed
additional ways to decompose risk into the bias and the
variance term.
A key ﬁnding of our work is that the complex behavior of
the risk curve arises due to the simple but non-classical vari-
ance unimodality phenomenon. Indeed, since the expected
risk (test loss) is the sum of bias and variance, monotonic
bias and unimodal variance can lead to three characteristic
behaviors, illustrated in Figure 1, depending on the relative
size of the bias and variance. If the bias completely domi-
nates, we obtain monotonically decreasing risk curve (see
Figure 1(a)). Meanwhile, if the variance dominates, we
obtain a bell-shaped risk curve that ﬁrst increases then de-
creases (see Figure 1(c)). The most complex behavior is
if bias and variance dominate in different regimes, leading
to the double-descent risk curve in Figure 1(b). All three
behaviors are well-aligned with the empirical observation
in deep learning that larger models typically perform bet-
ter. The most common behavior in our experiments is the
ﬁrst case (monotonically decreasing risk curve) as bias is
typically larger than variance. We can observe the double-
descent risk curve when label noise is added to the training
set (see §3.3), and can observe the unimodal risk curve
when we use the generalized bias-variance decomposition
for cross-entropy loss (see §3.2).
Further
Implications.
The investigations described
above characterize bias and variance as a function of net-
work width, but we can explore the dependence on other
quantities as well, such as model depth (§4.2). Indeed, we
ﬁnd that deeper models tend to have lower bias but higher
variance. Since bias is larger at current model sizes, this
conﬁrms the prevailing wisdom that we should generally
use deeper models when possible. On the other hand, it
suggests that this process may have a limit—eventually very
deep models may have low bias but high variance such that
increasing the depth further harms performance.
We also investigate the commonly observed drop in accu-
racy for models evaluated on out-of-distribution data, and at-
tribute it primarily to increased bias. Combined with the pre-
vious observation, this suggests that increasing model depth
may help combat the drop in out-of-distribution accuracy,
which is supported by experimental ﬁndings in Hendrycks
& Dietterich (2019).
Theoretical Analysis of A Two-Layer Neural Network.
Finally, we conduct a theoretical study of a two-layer lin-
ear network with a random Gaussian ﬁrst layer. While this
model is much simpler than those used in practice, we never-
theless observe the same characteristic behaviors for the bias
and variance. In particular, by working in the asymptotic
setting where the input data dimension, amount of training
data, and network width go to inﬁnity with ﬁxed ratios, we
show that the bias is monotonically decreasing while the

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
variance curve is unimodal. Our analysis also character-
izes the location of the variance peak as the point where
the number of hidden neurons is approximately half of the
dimension of the input data.
2. Preliminaries
In this section we present the bias-variance decomposition
for squared loss. We also present a generalized bias-variance
decomposition for cross-entropy loss in §2.2. The task is
to learn a function f : Rd →Rc, based on i.i.d. train-
ing samples T = {(xi, yi)}n
i=1 drawn from a joint dis-
tribution P on Rd × Rc, such that the mean squared er-
ror Ex,y

∥y −f(x, T )∥2
2

is minimal, where (x, y) ∼P.
Here we denote the learned function by f(x; T ) to make
the dependence on the training samples clear.
Note that the learned predictor f(x; T ) is a random
quantity depending on T .
We can assess its perfor-
mance in two different ways.
The ﬁrst way, random-
design, takes the expectation over T such that we con-
sider the expected error ET

∥y −f(x, T )∥2
2

.
The
second way, ﬁxed-design, holds the training covariates
{xi}n
i=1 ﬁxed and only takes expectation over {yi}n
i=1,
i.e., ET

∥y −f(x, T )∥2
2 | {xi}n
i=1

.
The choice of
random/ﬁxed-design leads to different bias-variance de-
compositions. Throughout the paper, we focus on random-
design, as opposed to ﬁxed-design studied in Mei & Monta-
nari (2019); Hastie et al. (2019); Ba et al. (2020).
2.1. Bias Variance Decomposition
Random Design.
In the random-design setting, decom-
posing the quantity ET

∥y −f(x, T )∥2
2

gives the usual
bias-variance trade-off from machine learning, e.g. Geman
et al. (1992); Hastie et al. (2001).
Ex,yET

∥y −f(x, T )∥2
2

=
Ex,y

∥y −¯f(x)∥2
2

|
{z
}
Bias2
+ ExET

∥f(x, T ) −¯f(x)∥2
2

|
{z
}
Variance
,
where ¯f(x) = ET f(x, T ). Here ET

∥(y −f(x, T )∥2
2

measures the average prediction error over different realiza-
tions of the training sample. In addition to take the expecta-
tion ET , we also average over Ex,y, as discussed in Bishop
(2006, §3.2). For future reference, we deﬁne
Bias2 = Ex,y

∥y −¯f(x)∥2
2

,
(1)
Variance = ExET

∥f(x, T ) −¯f(x)∥2
2

.
(2)
In §2.2, we present our estimator for bias and variance in
equation (1) and (2).
Fixed Design.
In ﬁxed-design setting, the covariates
{xi}n
i=1 are held be ﬁxed, and the only randomness in the
training set T comes from yi ∼P(Y | X = xi). As
presented in Mei & Montanari (2019); Hastie et al. (2019);
Ba et al. (2020), a more natural way to present the ﬁxed-
design assumption is to hold {xi}n
i=1 to be ﬁxed and let
yi = f0(x) + ϵi for i = 1, . . . , n, where f0(x) is a ground-
truth function and ϵi are random noises. Under this assump-
tion, the randomness in T all comes from the random noise
ϵi. To make this clear, we write T as Tϵi. Then, we obtain
the ﬁxed-design bias-variance decomposition
Eϵi

∥(y −f(x, Tϵi)∥2
2

=

∥(y −¯f(x)∥2
2

|
{z
}
Bias2
+ Eϵi

∥(f(x, Tϵi) −¯f(x)∥2
2

|
{z
}
Variance
,
where ¯f(x) = Eϵif(x, Tϵi). In most practical settings, the
expectation Eϵif(x, Tϵi) cannot be estimated from training
samples T = {(xi, yi)}n
i=1, because we do not have access
to independent copies of f(xi) + ϵi. In comparison to the
random-design setting, the ﬁxed-design setting tends to have
larger bias and smaller variance, since more “randomness”
is introduced into the variance term.
2.2. Estimating Bias and Variance
In this section, we present the estimator we use to estimate
the bias and variance as deﬁned in equation (1) and (2). The
high level idea is to approximate the expectation ET by
computing the sample average using multiple training sets
T1, . . . , TN. When evaluating the expectation ET , there is a
trade-off between having larger training sets (n) within each
training set and having larger number of splits (N), since
n × N = total number of training samples.
Mean Squared Error (MSE).
To estimate bias and vari-
ance in equation (1) and (2), we introduce an unbiased
estimator for variance, and obtain bias by subtracting the
variance from the risk. Let T = T1 ∪· · · ∪TN be a random
disjoint split of training samples. In our experiment, we
mainly take N = 2 (for CIFAR10 each Ti has 25k samples).
To estimate the variance, we use the unbiased estimator
c
var(x, T ) =
1
N −1
N
X
j=1
f(x, Tj) −
N
X
j=1
1
N f(x, Tj)

2
2,
where var depends on the test point x and on the random
training set T . While var is unbiased, its variance can be
reduced by using multiple random splits to obtain estimators
c
var1, . . . , c
vark and taking their average. This reduces the
variance of the variance estimator since:
VarT
1
k
k
X
i=1
c
vari

=
P
ij CovT ( c
vari, c
varj)
k2
≤VarT ( c
var1),
where the { c
vari}k
i=1 are identically distributed but not inde-
pendent, and we used the Cauchy-Schwarz inequality.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
Algorithm 1 Estimating Generalized Variance
Input: Test point x, Training set T .
for i = 1 to k do
Split the T into T (i)
1
, . . . , T (i)
N .
for j = 1 to N do
Train the model using T (i)
j
;
Evaluate the model at x; call the result π(i)
j ;
end for
end for
Compute bπ = exp
n
1
N·k
P
ij log

π(i)
j
o
(using element-wise log and exp; bπ estimates ¯π).
Normalize bπ to get a probability distribution.
Compute the variance
1
N·k
P
ij DKL

bπ∥π(i)
j

.
Cross-Entropy Loss (CE).
In addition to the classical
bias-variance decomposition for MSE loss, we also consider
a generalized bias-variance decomposition for cross-entropy
loss. Let π(x, T ) ∈Rc be the output of the neural network
(a probability distribution over the class labels). π(x, T ) is
a random variable since the training set T is random. Let
π0(x) ∈Rc be the one-hot encoding of the ground-truth
label. Then, omitting the dependence of π and π0 on x and
T , the cross entropy loss
H(π0, π) =
c
X
l=1
π0[l] log(π[l])
can be decomposed as
ET [H(π0, π)] = DKL(π0∥¯π)
|
{z
}
Bias2
+ ET [DKL(¯π∥π)]
|
{z
}
Variance
,
(3)
where π[l] is the l-th element of π, and ¯π is the average of
log-probability after normalization, i.e.,
¯π[l] ∝exp{ET log(π[l])} for l = 1, . . . , c.
This decomposition is a special case of the general decom-
position for Bregman divergence discussed in Pfau (2013).
We apply Algorithm 1 to estimate the generalized variance
in (3). Here we could not obtain an unbiased estimator, but
the estimate is better if we take more random splits (larger
k). In practice, we choose k to be large enough so that the
estimated variance stabilizes when we further increase k
(see §3.4). Similar to the case of squared loss, we estimate
the bias by subtracting the variance from the risk.
3. Measuring Bias and Variance for Neural
Networks
In this section, we study the bias and variance (equations (1)
and (2)) of deep neural networks. While the bias is mono-
tonically decreasing as folk wisdom would predict, the
variance is unimodal (ﬁrst increases to a peak and then
decreases). We conduct extensive experiments to verify
that this phenomenon appears robustly across architec-
tures, datasets, optimizer, and loss function. Our code
can be found at https://github.com/yaodongyu/
Rethink-BiasVariance-Tradeoff.
3.1. Mainline Experimental Setup
We ﬁrst describe our mainline experimental setup. In the
next subsection, we vary each design choice to check ro-
bustness of the phenomenon. More extensive experimental
results are given in the appendix.
For the mainline experiment, we trained a ResNet34 (He
et al., 2016) on the CIFAR10 dataset (Krizhevsky et al.,
2009). We trained using stochastic gradient descent (SGD)
with momentum 0.9. The initial learning rate is 0.1. We
applied stage-wise training (decay learning rate by a factor
of 10 every 200 epochs), and used weight decay 5 × 10−4.
To change the model complexity of the neural network, we
scale the number of ﬁlters (i.e., width) of the convolutional
layers. More speciﬁcally, with width = w, the number of
ﬁlters are [w, 2w, 4w, 8w]. We vary w from 2 to 64 (the
width w of a regular ResNet34 designed for CIFAR10 in He
et al. (2016) is 16).
Relative to the standard experimental setup (He et al., 2016),
there are two main differences. First, since bias-variance
is usually deﬁned for the squared loss (see (1) and (2)),
our loss function is the squared error (squared ℓ2 distance
between the softmax probabilities and the one-hot class
vector) rather than the log-loss. In the next section we
also consider models trained with the log-loss and estimate
the bias and variance by using a generalized bias-variance
decomposition, as described in §2.2. Second, to measure
the variance (and hence bias), we need two models trained
on independent subsets of the data as discussed in §2.2.
Therefore, the training dataset is split in half and each model
is trained on only n = 25, 000 = 50, 000/2 data points. We
estimate the variance by averaging over N = 3 such random
splits (i.e., we train 6 = 3 × 2 copies of each model).
In Figure 2, we can see that the variance as a function of the
width is unimodal and the bias is monotonically decreasing.
Since the scale of the variance is small relative to the bias,
the overall behavior of the risk is monotonically decreasing.
3.2. Varying Architectures, Loss Functions, Datasets
Architectures. We observe the same monotonically de-
screasing bias and unimodal variance phenomenon for
ResNext29 (Xie et al., 2017). To scale the “width” of the
ResNext29, we ﬁrst set the number of channels to 1 and
increase the cardinality, deﬁned in (Xie et al., 2017), from
2 to 4, and then ﬁx the cardinality at 4 and increase channel

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet34 Width
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Risk/Bias2/Variance
Risk
Bias2
Variance
101
ResNet34 Width
0.045
0.050
0.055
0.060
0.065
0.070
0.075
0.080
0.085
Variance
Variance
101
ResNet34 Width
0.00
0.05
0.10
0.15
0.20
0.25
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 2. Mainline experiment on ResNet34, CIFAR10 dataset (25,000 training samples). (Left) Risk, bias, and variance for ResNet34.
(Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.
101
102
ResNext29 Width
0.0
0.1
0.2
0.3
0.4
0.5
Risk/Bias2/Variance
Risk
Bias2
Variance
(a) ResNext29, MSE loss, CIFAR10
101
ResNet34 Width (5 Trials, CE)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Risk/Bias/Variance
Risk
Bias
Variance
(b) ResNet34, CE loss, CIFAR10
100
101
102
103
DNN Width
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Risk/Bias2/Variance
Risk
Bias2
Variance
(c) DNN, MSE loss, MNIST
Figure 3. Risk, bias, and variance with respect to different network architectures, training loss functions, and datasets. (a). ResNext29
trained by MSE loss on CIFAR10 dataset (25,000 training samples). (b). ResNet34 trained by CE loss (estimated by generalized
bias-variance decomposition using Bregman divergence) on CIFAR10 dataset (10,000 training samples). (c). Fully connected network
with one hidden layer and ReLU activation trained by MSE loss on MNIST dataset (10,000 training samples).
size from 1 to 32. Results are shown in Figure 3(a), where
the width on the x-axis is deﬁned as the cardinality times
the ﬁlter size.
Loss Function. In addition to the bias-variance decompo-
sition for MSE loss, we also considered a similar decom-
position for cross-entropy loss as described in §2.2. We
train with cross-entropy loss and use n = 10, 000 training
samples (5 splits), repeating N = 4 times with independent
random splits. As shown in Figure 3(b), the behavior of the
generalized bias and variance for cross entropy is consis-
tent with our earlier observations: the bias is monotonically
decreasing and the variance is unimodal. The risk ﬁrst in-
creases and then decreases, corresponding to the unimodal
risk pattern in Figure 1(c).
Datasets. In addition to CIFAR10, we study bias and vari-
ance on MNIST (LeCun, 1998) and Fashion-MNIST (Xiao
et al., 2017). For these two datasets, we use a fully con-
nected neural network with one hidden layer with ReLU ac-
tivation function. The “width” of the network is the number
of hidden nodes. We use 10,000 training samples (N = 5).
As seen in Figure 3(c) and 10 (in Appendix B), for both
MNIST and Fashion-MNIST, the variance is again unimodal
and the bias is monotonically decreasing.
In addition to the above experiments, we also conduct ex-
periments on the CIFAR100 dataset, the VGG network ar-
chitecture (Simonyan & Zisserman, 2015), various training
sample sizes, and different weight decay regularization and
present the results in Appendix B. We observe the same
monotonically descreasing bias and unimodal variance phe-
nomenon in all of these experiments.
3.3. Connection to Double-Descent Risk
When the relative scale of bias and variance changes, the risk
displays one of the three patterns, monotonically decreasing,
double descent, and unimodal, as presented in Figure 1(a),
1(b) and 1(c). In particular, the recent stream of observa-
tions on double descent risk (Belkin et al., 2019a) can be
explained by unimodal variance and monotonically decreas-
ing bias. In our experiments, including the experiments
in previous sections, we typically observe monotonically
decreasing risk; but with more label noise, the variance will
increase and we observe the double descent risk curve.
Label Noise.
Similar to the setup in Nakkiran (2019), for
each split, we sample training data from the whole training
dataset, and replace the label of each training example with a
uniform random class with independent probability p. Label
noise increases the variance of the model and hence leads
to double-descent risk as seen in Figure 4. If the variance
is small, the risk does not have the double-descent shape

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet34 Width
0.10
0.15
0.20
0.25
0.30
0.35
Bias2/Variance
Bias2 (Clean)
Variance (Clean)
Bias2 (Label Noise 10%)
Variance (Label Noise 10%)
Bias2 (Label Noise 20%)
Variance (Label Noise 20%)
101
ResNet34 Width
0
5
10
15
20
25
30
35
40
Train/Test Error (0-1 Loss)
Train Error (Clean)
Test Error (Clean)
Train Error (Label Noise 10%)
Test Error (Label Noise 10%)
Train Error (Label Noise 20%)
Test Error (Label Noise 20%)
Figure 4. Increasing label noise leads to double-descent. (Left) Bias and variance under different label noise percentage. (Right) Training
error and test error under different label noise percentage.
because the variance peak is not large enough to overwhelm
the bias, as observed in Figures 2, 3(a), 3(c) and 10.
3.4. Discussion of Possible Sources of Error
In this section, we brieﬂy describe the possible sources of
error in our estimator deﬁned in §2.2.
Mean Squared Error. As argued in §2.2, the variance
estimator is unbiased. To understand the variance of the
estimator, we ﬁrst split the data into two parts. For each part,
we compute the bias and variance for varying network width
by using our estimator. Averaging across different model
width, the relative difference between the two parts is 0.6%
for bias and 3% for variance, so our results for MSE are
minimally sensitive to ﬁnite-sample effects. The complete
experiments can be found in the appendix (see Figure 17).
Cross Entropy Loss. For cross entropy loss, we are cur-
rently unable to obtain an unbiased estimator. We can assess
the quality of our estimator using the following scheme.
We partition the dataset into ﬁve parts T1, . . . , T5, i.e., set
N = 5 in Algorithm 1. Then, we sequentially plot the esti-
mate of bias and variance using k = 1, 2, 3, 4 as described
in Algorithm 1. We observe that using larger k gives better
estimates. In Figure 18 of Appendix B.9, we observe that
as k increases, the bias curve systematically decreases and
the variance curve increases. Therefore our estimator over-
estimates the bias and under-estimates the variance, but the
overall behaviors of the curves remain consistent.
4. What Affects the Bias and Variance?
In this section, through the Bias-Variance decomposition
analyzed in §3, we investigate the role of depth for neural
networks and the robustness of neural networks on out-of-
distribution examples.
4.1. Bias-Variance Tradeoff for Out-of-Distribution
(OOD) Example
For many real-world computer vision applications, inputs
can be corrupted by random noise, blur, weather, etc.
These common occurring corruptions are shown to signiﬁ-
cantly decrease model performance (Azulay & Weiss, 2019;
Hendrycks & Dietterich, 2019). To better understand the
“generalization gap” between in-distribution test examples
and out-of-distribution test examples, we empirically evalu-
ate the bias and variance on the CIFAR10-C dataset devel-
oped by Hendrycks & Dietterich (2019), which is a common
corruption benchmark and includes 15 types of corruption.
By applying the models trained in the mainline experiment,
we are able to evaluate the bias and variance on CIFAR10-C
test dataset according to the deﬁnitions in (1) and (2). As we
can see from Figure 5(a), both the bias and variance increase
relative to the original CIFAR10 test set. Consistent with
the phenomenon observed in the mainline experiment, the
bias dominates the overall risk. The results indicate that
the “generalization gap” mainly comes from increased bias,
with relatively less contribution from variance as well.
4.2. Effect of Model Depth on Bias and Variance
In addition to the ResNet34 considered in the mainline
experiment, we also evaluate the bias and variance for
ResNet18 and ResNet50. Same as the mainline experi-
ment setup, we estimate the bias and variance for ResNet
using 25,000 training samples (N = 2) and three indepen-
dent random splits (k = 3). The standard building block of
ResNet50 architecture in He et al. (2016) is bottleneck block,
which is different from the basic block used in ResNet18 and
ResNet34. To ensure that depth is the only changing vari-
able across three architectures, we apply the basic block for
ResNet50. Same training epochs and learning rate decays

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet34 Width
0.1
0.2
0.3
0.4
0.5
0.6
Risk/Bias2/Variance
Risk (Clean)
Bias2 (Clean)
Variance (Clean)
Risk (OOD)
Bias2 (OOD)
Variance (OOD)
(a) OOD Example
101
ResNet Width
0.10
0.15
0.20
0.25
0.30
0.35
Bias2
Bias2 (ResNet18)
Bias2 (ResNet34)
Bias2 (ResNet50)
(b) Bias of model with different depth
101
ResNet Width
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Variance
Variance (ResNet18)
Variance (ResNet34)
Variance (ResNet50)
(c) Variance of model with different depth
Figure 5. (a). Risk, bias, and variance for ResNet34 on out-of-distribution examples (CIFAR10-C dataset). (b)-(c). Bias and variance for
ResNet with different depth trained by MSE loss on CIFAR10 (25,000 training samples).
are applied to three models.
From Figure 5(b) and 5(c), we observe that the bias de-
creases as the depth increases, while the variance increases
as the depth increases. For each model, the bias is monoton-
ically decreasing and the variance is unimodal. The differ-
ences in variance are small (around 0.01) compared with the
changes in bias. Overall, the risk typically decreases as the
depth increases. Our experimental results suggest that the
improved generalization for deeper models, with the same
network architecture, are mainly attributed to lower bias.
For completeness, we also include the bias and variance
versus depth when basic blocks in ResNet are replaced by
bottleneck blocks (see Figure 20 in the appendix). We
observe similar qualitative trend of bias and variance.
Note that at high width, the bias of ResNet50 is slightly
higher than the bias of ResNet18 and ResNet34.
We
attribute this inconsistency to difﬁculties when training
ResNet50 without bottleneck blocks at high width. Lastly,
we also include the bias and variance versus depth for out-
of-distribution test samples, in which case we also observed
decreased bias and increased variance as depth increases, as
shown in Figure 19 of Appendix B.10.
5. Theoretical Insights from a Two-layer
Linear Model
While the preceding experiments show that the bias and
variance robustly exhibit monotonic-unimodal behavior in
the random-design setting, existing theoretical analyses hold
instead for the ﬁxed-design setting, where the behavior of the
bias and variance are more complex, with both the bias and
variance exhibiting a peak and the risk exhibiting double
descent pattern (Mei & Montanari (2019, Figure 6)). In
general, while the risk should be the same (in expectation)
for the random and ﬁxed design setting, the ﬁxed-design
setting has lower bias and higher variance.
Motivated by the more natural behavior in the random-
design setting, we work to extend the existing ﬁxed-design
theory to the random-design case. Our starting point is Mei
& Montanari (2019), who consider two-layer non-linear
networks with random hidden layer weights. However, the
randomness in the design complicates the analysis, so we
make two points of departure to help simplify: ﬁrst, we
consider two-layer linear rather than non-linear networks,
and second, we consider a different scaling limit (n/d →∞
rather than n/d going to some constant). In this setting, we
rigorously show that the variance is indeed unimodal and
the bias is monotonically decreasing (Figure 6). Our precise
assumptions are given below.
5.1. Model Assumptions
We consider the task of learning a function y = f(x) that
maps each input vector x ∈Rd to an output (label) value
y ∈R. The input-output pair (x, y) is assumed to be drawn
from a distribution where x ∼N(0, Id/d) and
y = f0(x) := x⊤θ,
(4)
where θ ∈Rd is a weight vector. Given a training set T :=
{(xi, yi)}n
i=1 with training samples drawn independently
from the data distribution, we learn a two-layer linear neural
network parametrized by W ∈Rp×d and β ∈Rp as
f(x) = (W x)⊤β,
where p is the number of hidden units in the network. In
above, we take W as a parameter independent of the train-
ing data T whose entries are drawn from i.i.d. Gaussian
distribution N(0, 1/d). Given W , the parameter β is esti-
mated by solving the following ridge regression1 problem
βλ(T , W ) = arg min
β∈Rp
∥(W X)⊤β −y∥2
2 + λ∥β∥2
2, (5)
1ℓ2 regularization on weight parameters is arguably the most
widely used technique in training neural network, known for im-
proving generalization (Krogh & Hertz, 1992). Other regulariza-
tion such as ℓ1 can also be used and leads to qualitatively similar
behaviors.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
(a) Risk v.s. γ for different n
(b) Bias v.s. γ for different n
(c) Variance v.s. γ for different n
Figure 6. Risk, bias, and variance for a two-layer linear neural network.
where X
= [x1, . . . , xn] ∈Rd×n denotes a matrix
that contains training data vectors as its columns, y =
[y1, . . . , yn] ∈Rn denotes a vector containing training la-
bels as its entries, and λ ∈R+ is the regularization parame-
ter. By noting that the solution to (5) is given by
βλ(T , W ) = (W XX⊤W ⊤+ λI)−1W Xy,
our estimator f : Rd →R is given as
fλ(x; T , W ) = x⊤W ⊤βλ(T , W ).
(6)
5.2. Bias-Variance Analysis
We may now calculate the bias and variance of the model
described above via the following formulations:
Biasλ(θ)2 = Ex [ET ,W fλ(x; T , W ) −f0(x)]2 ,
Varianceλ(θ) = ExVarT ,W [fλ(x; T , W )] ,
where f0(x) and fλ(x; T , W ) are deﬁned in (4) and (6),
respectively. Note that the bias and variance are functions
of the model parameter θ. To simplify the analysis, we
introduce a prior θ ∼N(0, Id) and calculate the expected
bias and expected variance as
Bias2
λ := EθBiasλ(θ)2,
(7)
Varianceλ := EθVarianceλ(θ).
(8)
The precise formulas for the expected bias and the expected
variance are parametrized by the dimension of the input
feature d, the number of training points n, the number of
hidden units p and also λ.
Previous literatures (Mei & Montanari, 2019) suggests that
both the risk and the variance achieves a peak at the inter-
polation threshold (n = p). In the regime when n is very
large, the risk no longer exhibits a peak, but the unimodal
pattern of variance still holds. In the rest of the section, we
consider the regime where the n is large (monotonically de-
creasing risk), and derive the precise expression for the bias
and variance of the model. From our expression, we obtain
the location where the variance achieves the peak. For this
purpose, we consider the following asymptotic regime of
n, p and d:
Assumption 1. Let {(d, n(d), p(d))}∞
d=1 be a given se-
quence of triples. We assume that there exists a γ > 0
such that
lim
d→∞
p(d)
d
= γ,
and
lim
d→∞
n(d)
d
= ∞.
For simplicity, we will write n := n(d) and p := p(d).
With the assumption above, we have the expression of the
expected bias, variance and risk as a function of γ and λ.
Theorem 1. Given {(d, n(d), p(d))}∞
d=1 that satisﬁes As-
sumption 1, let λ =
n
d λ0 for some ﬁxed λ0 > 0. The
asymptotic expression of expected bias and variance are
given by
lim
d→∞Bias2
λ = 1
4Φ3(λ0, γ)2,
(9)
lim
d→∞Varianceλ =



Φ1(λ0, 1
γ )
2Φ2(λ0, 1
γ ) −(1−γ)(1−2γ)
2γ
−1
4Φ3(λ0, γ)2,
γ ≤1,
Φ1(λ0,γ)
2Φ2(λ0,γ) −γ−1
2
−1
4Φ3(λ0, γ)2,
γ > 1,
where
Φ1(λ0, γ) = λ0(γ + 1) + (γ −1)2,
Φ2(λ0, γ) =
p
(λ0 + 1)2 + 2(λ0 −1)γ + γ2,
Φ3(λ0, γ) = Φ2(λ0, γ) −λ0 −γ + 1.
The proof is given in Appendix C.
The risk can be obtained through Bias2
λ + Varianceλ.
The expression in Theorem 1 is plotted as the red curves
in Figure 6.
In addition to the case when n/d →∞,
we also plot the shape of bias, variance and risk when
n/d →{0.15, 0.25, 0.35, . . . , 1.00, 1.50}. We ﬁnd that the
risk of the model grows from unimodal to monotonically
decreasing as the number of samples increased (see Figure

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
6(a)). Moreover, the bias of the model is monotonically
decreasing (see Figure 6(b)) and the variance is unimodal
(see Figure 6(c)).
Corollary 1 (Monotonicity of Bias). The derivative of the
limiting expected Bias in (9) can be calculated as
−
p
2(γ + 1)λ0 + (γ −1)2 + λ2
0 −γ −λ0 + 1
2
2
p
γ2 + 2γ (λ0 −1) + (λ0 + 1) 2
.
(10)
When λ0 ≥0, the expression in (10) is strictly non-positive,
therefore the limiting expected bias is monotonically non-
increasing as a function of γ, as classical theories predicts.
To gain further insight into the above formulas, we also
consider the case when the ridge regularization amount λ0
is small. In particular, we consider the ﬁrst order effect of
λ0 on the bias and variance term, and compute the value of
γ where the variance attains the peak.
Corollary 2 (Unimodality of Variance – small λ0 limit).
Under the assumptions of Theorem 1, the ﬁrst order effect
of λ0 on variance is given by
lim
d→∞EVarianceλ =
(
O
 λ2
0

,
γ > 1,
−(γ −1)γ −2γλ0 + O
 λ2
0

, o.w.
and the risk is given by
lim
d→∞ERiskλ =
(
1 −γ + O
 λ2
0

,
γ ≤1,
O
 λ2
0

,
γ > 1.
Moreover, up to ﬁrst order, the peak in the variance is
Peak = 1
2 −λ0 + O
 λ2
0

.
Theorem 2 suggests that when λ0 is sufﬁciently small, the
variance of the model is maximized when p = d/2, and the
effect of λ0 is to shift the peak slightly to d/2 −λ0d.
From a technical perspective, to compute the variance in
the random-design setting, we need to compute the element-
wise expectation of certain random matrix. For this purpose,
we apply the combinatorics of counting non-cross partitions
to characterize the asymptotic expectation of products of
Wishart matrices.
6. Conclusion and Discussion
In this paper we re-examine the classical theory of bias and
variance trade-off as the width of a neural network increases.
Through extensive experimentation, our main ﬁnding is
that, while the bias is monotonically decreasing as classi-
cal theory would predict, the variance is unimodal. This
combination leads to three typical risk curve patterns, all ob-
served in practice. Theoretical analysis of a two-layer linear
network corroborates these experimental observations.
The seemingly varied and bafﬂing behaviors of modern neu-
ral networks are thus in fact consistent, and explainable
through classical bias-variance analysis. The main unex-
plained mystery is the unimodality of the variance. We
conjecture that as the model complexity approaches and
then goes beyond the data dimension, it is regularization in
model estimation (the ridge penalty in our theoretical exam-
ple) that helps bring down the variance. Under this account,
the decrease in variance for large dimension comes from
better conditioning of the empirical covariance, making it
better-aligned with the regularizer.
In the future, it would be interesting to see if phenomena
characterized by the simple two-layer model can be rigor-
ously generalized to deeper networks with nonlinear acti-
vation, probably revealing other interplays between model
complexity and regularization (explicit or implicit). Such
a study could also help explain another phenomenon we
(and others) have observed: bias decreases with more layers
as variance increases. We believe that the (classic) bias-
variance analysis remains a powerful and insightful frame-
work for understanding the behaviors of deep networks;
properly used, it can guide practitioners to design more
generalizable and robust networks in the future.
Acknowledgements.
We would like to thank Em-
manuel Cand´es for ﬁrst bringing the double-descent phe-
nomenon to our attention, Song Mei for helpful discussion
regarding random v.s. ﬁxed design regression, and Nikhil
Srivastava for pointing out to relevant references in random
matrix theory. We would also like to thank Preetum Nakki-
ran, Mihaela Curmei, and Chloe Hsu for valuable feedback
during preparation of this manuscript. The authors acknowl-
edge support from Tsinghua-Berkeley Shenzhen Institute
Research Fund and BAIR.
References
Advani, M. S. and Saxe, A. M. High-dimensional dynam-
ics of generalization error in neural networks. ArXiv,
abs/1710.03667, 2017.
Azulay, A. and Weiss, Y. Why do deep convolutional net-
works generalize so poorly to small image transforma-
tions? Journal of Machine Learning Research, 20:1–25,
2019.
Ba, J., Erdogdu, M., Suzuki, T., Wu, D., and Zhang, T.
Generalization of two-layer neural networks: An asymp-
totic viewpoint. In International Conference on Learning
Representations, 2020. URL https://openreview.
net/forum?id=H1gBsgBYwH.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
Bai, Z. and Silverstein, J. Spectral Analysis of Large Di-
mensional Random Matrices. Springer, 01 2010. doi:
10.1007/978-1-4419-0661-8.
Bartlett,
P.
L.,
Long,
P.
M.,
Lugosi,
G.,
and
Tsigler,
A.
Benign overﬁtting in linear regres-
sion.
Proceedings of the National Academy of Sci-
ences, 2020.
ISSN 0027-8424.
doi:
10.1073/
pnas.1907378117. URL https://www.pnas.org/
content/early/2020/04/22/1907378117.
Belkin, M., Ma, S., and Mandal, S. To understand deep
learning we need to understand kernel learning. In Inter-
national Conference on Machine Learning, pp. 541–549,
2018.
Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling
modern machine-learning practice and the classical bias–
variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019a.
Belkin, M., Rakhlin, A., and Tsybakov, A. B. Does data
interpolation contradict statistical optimality?
In The
22nd International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1611–1619, 2019b.
Bishop, A. N., Del Moral, P., and Niclas, A. An Introduction
to Wishart Matrix Moments. now, 2018. URL https://
ieeexplore.ieee.org/document/8572806.
Bishop, C. M. Pattern Recognition and Machine Learning.
Springer, 2006.
Chatterji, N. S. and Long, P. M. Finite-sample analysis of
interpolating linear classiﬁers in the overparameterized
regime, 2020.
Deng, Z., Kammoun, A., and Thrampoulidis, C. A model
of double descent for high-dimensional binary linear clas-
siﬁcation. arXiv preprint arXiv:1911.05822, 2019.
Dietterich, T. G. and Kong, E. B. Machine learning bias,
statistical bias, and statistical variance of decision tree al-
gorithms. Technical report, Technical report, Department
of Computer Science, Oregon State University, 1995.
Geman, S. A limit theorem for the norm of random matrices.
Ann. Probab., 8(2):252–261, 04 1980. doi: 10.1214/aop/
1176994775. URL https://doi.org/10.1214/
aop/1176994775.
Geman, S., Bienenstock, E., and Doursat, R. Neural net-
works and the bias/variance dilemma. Neural computa-
tion, 4(1):1–58, 1992.
Ghaoui, L. E.
Inversion error, condition number,
and
approximate
inverses
of
uncertain
matri-
ces.
Linear Algebra and its Applications, 343-
344:171 – 193, 2002.
ISSN 0024-3795.
doi:
https://doi.org/10.1016/S0024-3795(01)00273-7. URL
http://www.sciencedirect.com/science/
article/pii/S0024379501002737.
Special
Issue on Structured and Inﬁnite Systems of Linear
equations.
Hastie, T., Tibshirani, R., and Friedman, J. The Elements
of Statistical Learning.
Springer Series in Statistics.
Springer New York Inc., New York, NY, USA, 2001.
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J.
Surprises in High-Dimensional Ridgeless Least Squares
Interpolation. arXiv e-prints, art. arXiv:1903.08560, Mar
2019.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.
Hendrycks, D. and Dietterich, T. Benchmarking neural
network robustness to common corruptions and perturba-
tions. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/
forum?id=HJz6tiCqYm.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems,
pp. 1097–1105, 2012.
Krizhevsky, A. et al. Learning multiple layers of features
from tiny images, 2009.
Krogh, A. and Hertz, J. A. A simple weight decay can im-
prove generalization. In Advances in neural information
processing systems, pp. 950–957, 1992.
LeCun, Y.
The mnist database of handwritten digits.
http://yann. lecun. com/exdb/mnist/, 1998.
Mei, S. and Montanari, A. The generalization error of ran-
dom features regression: Precise asymptotics and double
descent curve. arXiv e-prints, art. arXiv:1908.05355, Aug
2019.
Nakkiran, P.
More data can hurt for linear regres-
sion:
Sample-wise double descent.
arXiv preprint
arXiv:1912.07242, 2019.
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak,
B., and Sutskever, I.
Deep double descent: Where
bigger models and more data hurt.
arXiv preprint
arXiv:1912.02292, 2019.
Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna,
M., Lacoste-Julien, S., and Mitliagkas, I. A modern
take on the bias-variance tradeoff in neural networks,

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
2019. URL https://openreview.net/forum?
id=HkgmzhC5F7.
Pfau, D. A generalized bias-variance decomposition for
bregman divergences, 2013.
Rosset, S. and Tibshirani, R. J. From Fixed-X to Random-X
Regression: Bias-Variance Decompositions, Covariance
Penalties, and Prediction Error Estimation. arXiv e-prints,
art. arXiv:1704.08160, April 2017.
Simonyan, K. and Zisserman, A. Very deep convolutional
networks for large-scale image recognition. In Interna-
tional Conference on Learning Representations, 2015.
Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli,
G., and Wyart, M. A jamming transition from under-to
over-parametrization affects generalization in deep learn-
ing. Journal of Physics A: Mathematical and Theoretical,
2019.
Wainwright, M. J. High-Dimensional Statistics: A Non-
Asymptotic Viewpoint. Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University
Press, 2019. doi: 10.1017/9781108627771.
Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a
novel image dataset for benchmarking machine learning
algorithms. arXiv preprint arXiv:1708.07747, 2017.
Xie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre-
gated residual transformations for deep neural networks.
In Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pp. 1492–1500, 2017.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,
O. Understanding deep learning requires rethinking gen-
eralization. In International Conference on Learning
Representations, 2017.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
A. Summary of Experiments
We summarize the experiments in Table 1, each row corresponds to one experiment, some include several independent splits,
in this paper. Every experiment is related to one or multiple ﬁgures, which is speciﬁed in the last column “Figure”.
Dataset
Architecture
Loss
Optimizer
Train Size
#Splits(k)
Label Noise
Figure
Comment
CIFAR10
ResNet34
MSE
SGD(wd=5e-4)
25000
3

2, 5
Mainline
CIFAR10
ResNext29
MSE
SGD(wd=5e-4)
25000
3

3(a), 7
Architecture
VGG11
MSE
SGD(wd=5e-4)
10000
1

8
CIFAR10
ResNet34
CE
SGD(wd=5e-4)
10000
4

3(b), 9
Loss
MNIST
DNN
MSE
SGD(wd=5e-4)
10000
1

3(c)
Dataset
Fashion-MNIST
DNN
MSE
SGD(wd=5e-4)
10000
1

10
CIFAR100
ResNet34
CE
SGD(wd=5e-4)
10000
1

11
CIFAR10
ResNet34
MSE
SGD(wd=5e-4)
10000
1
10%/20%
4
Label noise
CIFAR10
ResNet18
MSE
SGD(wd=5e-4)
25000
3

5
Depth
ResNet50
MSE
SGD(wd=5e-4)
25000
3

5
CIFAR10
ResNet34
MSE
SGD(wd=5e-4)
10000
1

12
Train size
ResNet34
MSE
SGD(wd=5e-4)
2500
1

13
CIFAR10
ResNet34
MSE
SGD(wd=1e-4)
10000
1

14
Weight decay
CIFAR10
ResNet26-B
MSE
SGD(wd=5e-4)
25000
3

20
Depth (with
bottleneck
block)
ResNet38-B
MSE
SGD(wd=5e-4)
25000
3

20
ResNet50-B
MSE
SGD(wd=5e-4)
25000
3

20
CIFAR10
VGG9
MSE
SGD(wd=5e-4)
25000
3

21
Depth
VGG11
MSE
SGD(wd=5e-4)
25000
3

21
Table 1. Summary of Experiments.
B. Additional Experiments
In this section, we provide additional experimental results, some of them are metioned in §3 and §4.
Network Architecture: The implementation of the deep neural networks used in this work is mainly adapted from
https://github.com/kuangliu/pytorch-cifar.
Training Details: For CIFAR10 dataset and CIFAR100 dataset, when training sample size is 25,000, we use 500 epochs for
training and decay by a factor of 10 the learning rate every 200 epoch. When training sample size is 10,000/5,000, we use
1000 epochs for training and decay by a factor of 10 the learning rate every 400 epoch. For MNIST dataset and FMNIST
dataset, we use 200 epochs for training and decay by a factor of 10 the learning rate every 100 epoch. For all the experiments
in this paper, we sampled data without replacement to train the models as described in §2.2.
B.1. Architecture
We provide additional results on ResNext29 presented in §3.2. The results are shown in Figure 7. We also study the behavior
of risk, bias, and variance of VGG network (Simonyan & Zisserman, 2015) on CIFAR10 dataset. Here we use VGG11 and
the number of ﬁlters are [k, 2k, 4k, 4k, 8k, 8k, 8k, 8k], where k is the width in Figure 8. The number of training samples of
each split is 10,000. We use the same optimization setup as the mainline experiment (ResNet34 in Figure2).
B.2. Loss
We provide additional results on cross-entropy loss presented in §3.2, the results are shown in Figure 9.
B.3. Dataset
We provide the results on Fashion-MNIST dataset in Figure 10, which is mentioned in §3.2. We study the behavior of
risk, bias, and variance of ResNet34 on CIFAR100 dataset. Because the number of class is large, we use cross-entropy

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
during training, and apply the classical Bias-Vairance decomposition for MSE in (1) and (2) to estimate the risk, bias, and
variance. As shown in Figure 11, we observe the bell-shaped variance curve and the monotonically decreasing bias curve on
CIFAR100 dataset.
B.4. Training Size
Appart from the 2 splits case in Figure 2, we also consider 5 splits (10,000 training samples) and 20 splits case (2,500
training samples). We present the 5 splits case (10,000 training samples) in Figure 12, which corresponds to the label 0%
case in Figure 4. We present the 20 splits (2,500 training samples) in Figure 13. With less number of training samples, both
the bias and the variance will increase.
B.5. Weight Decay
We study another different weight decay parameter, (wd=1e-4) for ResNet34 on CIFAR10 dataset (10,000 training samples).
The risk, bias, variance, and train/test error curves are shown in Figure 14. Compared with Figure 12, we observe that larger
weight decay can decrease the variance.
B.6. Label Noise
We provide the risk curve for ResNet34 under different label noise percentage as described in §3.3, and the results are shown
in Figure 15.
B.7. 0-1 Loss Bias-Variance Decomposition
We evaluated the bias and variance for 0-1 loss (deﬁned in Dietterich & Kong (1995)) on the CIFAR10 dataset with
10,000 training samples using ResNet34. The results are shown in Figure 16. We can consistently observe that the bias is
monotonically decreasing and the variance is unimodal.
101
102
ResNext29 Width
0.0
0.1
0.2
0.3
0.4
0.5
Risk/Bias2/Variance
Risk
Bias2
Variance
101
102
ResNext29 Width
0.025
0.030
0.035
0.040
0.045
0.050
Variance
Variance
101
102
ResNext29 Width
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 7. Risk, bias, variance, train/test error for ResNext29 trained by MSE loss on CIFAR10 dataset (25,000 training samples). (Left)
Risk, bias, and variance for ResNext29. (Middle) Variance for ResNext29. (Right) Train error and test error for ResNext29.
101
VGG11 Width
0.1
0.2
0.3
0.4
0.5
Risk/Bias2/Variance
Risk
Bias2
Variance
101
VGG11 Width
0.05
0.06
0.07
0.08
0.09
0.10
0.11
Variance
Variance
101
VGG11 Width
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 8. Risk, bias, variance, train/test error for VGG11 trained by MSE loss on CIFAR10 dataset (10,000 training samples). (Left) Risk,
bias, and variance for VGG11. (Middle) Variance for VGG11. (Right) Train error and test error for VGG11.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet34 Width (5 Trials, CE)
0.25
0.30
0.35
0.40
0.45
0.50
Variance
Variance
101
ResNet34 Width (5 Trials, CE)
0.00
0.05
0.10
0.15
0.20
0.25
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 9. Variance and train/test error for ResNet34 trained by cross-entropy loss (estimated by generalized bias-variance decomposition
using Bregman divergence) on CIFAR10 dataset (10,000 training samples). (Left) Variance for ResNet34. (Right) Train error and test
error for ResNet34.
100
101
102
103
DNN Width
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Risk/Bias2/Variance
Risk
Bias2
Variance
Figure 10. Fully connected network with one-hidden-layer and ReLU activation trained by MSE loss on Fashion-MNIST dataset (10,000
training samples).
101
ResNet34 Width
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Risk/Bias2/Variance
Risk
Bias2
Variance
101
ResNet34 Width
0.125
0.150
0.175
0.200
0.225
0.250
0.275
0.300
0.325
Variance
Variance
101
ResNet34 Width
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 11. Risk, bias, variance, and train/test error for ResNet34 trained by cross-entropy loss (estimated by MSE bias-variance decompo-
sition) on CIFAR100 (10,000 training samples). (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right)
Train error and test error for ResNet34.
101
ResNet34 Width
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Risk/Bias2/Variance
Risk
Bias2
Variance
101
ResNet34 Width
0.07
0.08
0.09
0.10
0.11
0.12
Variance
Variance
101
ResNet34 Width
0.00
0.05
0.10
0.15
0.20
0.25
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 12. Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (10,000 training samples). (Left)
Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet34 Width
0.2
0.3
0.4
0.5
Risk/Bias2/Variance
Risk
Bias2
Variance
101
ResNet34 Width
0.14
0.15
0.16
0.17
0.18
0.19
0.20
Variance
Variance
101
ResNet34 Width
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 13. Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (2,500 training samples). (Left)
Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.
101
ResNet34 Width
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Risk/Bias2/Variance
Risk
Bias2
Variance
101
ResNet34 Width
0.09
0.10
0.11
0.12
0.13
0.14
Variance
Variance
101
ResNet34 Width
0.00
0.05
0.10
0.15
0.20
0.25
Train/Test Error (0-1 Loss)
Train Error
Test Error
Figure 14. Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (10,000 training samples), the
weight decay parameter of SGD is 1e-4. (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train
error and test error for ResNet34.
101
ResNet34 Width
0.20
0.25
0.30
0.35
0.40
0.45
Risk
Risk (Clean)
Risk (Label Noise 10%)
Risk (Label Noise 20%)
Figure 15. Risk under different label noise percentage. Increasing label noise leads to double descent risk curve.
101
ResNet34 Width (0-1 loss bias-variance)
0.05
0.10
0.15
0.20
0.25
Test Error/Bias/Variance
Test Error
Bias
Variance
101
ResNet34 Width (0-1 loss bias-variance)
0.020
0.022
0.024
0.026
0.028
0.030
0.032
0.034
Variance
Variance
Figure 16. Bias-variance (0-1 loss), and test error for ResNet34 trained by MSE loss on CIFAR10 dataset (10,000 training samples).
(Left) Bias and variance (0-1 loss), and test error for ResNet34. (Right) Variance (0-1 loss) for ResNet34.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
B.8. Sources of Error for Mean Squared Error (MSE)
As argued in §2.2 the estimator for variance is unbiased estimator. To understand the variance of the estimator, we ﬁrst split
the data into two parts, A and B. For each part, we take multiple random splits (k) and estimate the variance by taking the
average of those estimators, and vary the number of random splits k. The results are shown in Figure 17. We can see that the
variation between to parts of data is small. Quantitatively, veraging across different model width, the relative difference
between two parts of data is 0.65% for bias and 3.15% for variance.
101
2 × 100
3 × 100
4 × 100
6 × 100
ResNet18 Width A/B
0.15
0.20
0.25
0.30
Bias2
Bias2 A k=1
Bias2 B k=1
Bias2 A k=2
Bias2 B k=2
Bias2 A k=3
Bias2 B k=3
Bias2 A k=4
Bias2 B k=4
Bias2 A k=5
Bias2 B k=5
101
2 × 100
3 × 100
4 × 100
6 × 100
ResNet18 Width A/B
0.065
0.070
0.075
0.080
0.085
0.090
0.095
0.100
Variance
Variance A k=1
Variance B k=1
Variance A k=2
Variance B k=2
Variance A k=3
Variance B k=3
Variance A k=4
Variance B k=4
Variance A k=5
Variance B k=5
Figure 17. Bias and variance for two portions of data with k from 1 to 5. (Left) Bias for ResNet18. (Right) Variance for ResNet18.
B.9. Sources of Error for Cross Entropy Loss (CE)
For cross entropy loss, we are currently unable to obtain an unbiased estimator. We can access the quality of our estimator
using the following scheme. We partition the dataset into ﬁve parts T1, . . . , T5, i.e., set N = 5 in Algorithm 1. Then,
we sequentially plot the estimate of bias and variance using k = 1, 2, 3, 4 as described in Algorithm 1. Using larger k
gives better estimate. As shown in Figure 18, when k is small, our estimator over-estimate the bias and under-estimate the
variance, but the overall behavior of the curves are consistent.
101
ResNet34 Width
0.25
0.30
0.35
0.40
0.45
0.50
0.55
Bias
Bias (Sample 1)
Bias (Sample 2)
Bias (Sample 3)
Bias (Sample 4)
101
ResNet34 Width
0.25
0.30
0.35
0.40
0.45
0.50
Variance
Variance (Sample 1)
Variance (Sample 2)
Variance (Sample 3)
Variance (Sample 4)
101
ResNet34 Width
0.5
0.6
0.7
0.8
0.9
Risk
Risk (Sample 1)
Risk (Sample 2)
Risk (Sample 3)
Risk (Sample 4)
Figure 18. Estimate of bias, variance, and risk using varying number of sample (k in Algorithm 1). (Left) Bias (CE) for ResNet34.
(Middle) Variance (CE) for ResNet34. (Right) Risk (CE) for ResNet34.
B.10. Effect of Depth on Bias and Variance for Out-Of-Distribution Data
We study the role of depth on out-of-distribution test data. In Figure 19, we observe that increasing the depth can decrease
the bias and increase the variance. Also, deeper ResNet can generalize better on CIFAR10-C dataset as shown in Figure 19.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
101
ResNet Width
0.35
0.40
0.45
0.50
0.55
Bias2
Bias2 (ResNet18)
Bias2 (ResNet34)
Bias2 (ResNet50)
101
ResNet Width
0.08
0.10
0.12
0.14
0.16
0.18
Variance
Variance (ResNet18)
Variance (ResNet34)
Variance (ResNet50)
101
ResNet Width
0.30
0.35
0.40
0.45
Test Error (0-1 Loss)
Test Error (ResNet18)
Test Error (ResNet34)
Test Error (ResNet50)
Figure 19. Bias, variance, and test error for ResNet with different depth (ResNet18, ResNet34 and ResNet50 trained by MSE loss on
25,000 CIFAR10 training samples) evaluated on out-of-distribution examples (CIFAR10-C dataset). (Left) Bias for ResNet18, ResNet34
and ResNet50. (Middle) Variance for ResNet18, ResNet34 and ResNet50. (Right) Test error for ResNet18, ResNet34 and ResNet50.
B.11. Effect of Depth on ResNet using Bottleneck Blocks
In order to study the role of depth for ResNet on bias and variance, we apply basic residual block for ResNet50. To better
investigate the depth of ResNet, we use Bottleneck block for ResNet26, ResNet38, and ResNet50. More speciﬁcally, the
number of 3-layer bottleneck blocks for ResNet26, ResNet38, and ResNet50 are [2, 2, 2, 2], [3, 3, 3, 3], and [3, 4, 6, 3]. As
shown in Figure 20, we observe that deeper ResNet with Bottleneck blocks has lower bias and higher variance.
101
ResNet Width
0.10
0.15
0.20
0.25
0.30
Bias2
Bias2 (ResNet26)
Bias2 (ResNet38)
Bias2 (ResNet50)
101
ResNet Width
0.030
0.035
0.040
0.045
0.050
0.055
0.060
0.065
Variance
Variance (ResNet26)
Variance (ResNet38)
Variance (ResNet50)
Figure 20. Bias and variance for ResNet (bottleneck block) with different depth. (Left) Bias for ResNet26, ResNet38 and ResNet50.
(Right) Variance for ResNet26, ResNet38 and ResNet50.
B.12. Effect of Depth on VGG
We study the role of depth for VGG network on bias and variance. As shown in Figure 21, we observe that deeper VGG has
lower bias and higher variance.
101
VGG Width
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
Bias2
Bias2 (VGG9)
Bias2 (VGG11)
Bias2 (VGG13)
101
VGG Width
0.05
0.06
0.07
0.08
0.09
0.10
0.11
0.12
Variance
Variance (VGG9)
Variance (VGG11)
Variance (VGG13)
Figure 21. Bias and variance for VGG with different depth. (Left) Bias for VGG9, VGG11 and VGG13. (Right) Variance for VGG9,
VGG11 and VGG13.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
B.13. Additional Synthetic Experiment
In Figure 22, we plot the result of performing regression on synthetic data using a two-layer linear fully connected linear
network with varying width. The data are generated as y = β⊤x, x ∼N(0, Id/d), where ∥β∥2 = 1 is randomly generated
and ﬁxed weight vector. The ﬁrst layer of the network is drawn from i.i.d. zero-mean Gaussian distribution with variance
1/
√
d, and the second layer is trained using gradient descent with weight decay 0.1. The horizontal axis is the number of
parameters of the hidden layer normalized by the dimension of the data (i.e., p/d). The dots indicate actual experimental
results, while the lines indicate theoretically predicted results. We can observe that they align well and the peak occurs at the
predicted value.
Figure 22. Bias, Variance, and Risk for two layer linear network with parameters n = 800 and d = 30.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
C. Proof of Theorems in §5
Throughout this section, we use ∥· ∥and ∥· ∥2 to denote the Frobenius norm and spectral norm of a matrix, respectively.
Recall that for any given θ, the training set T = (X, y) satisﬁes the relation y = X⊤θ. By plugging this relation into (6),
we get
fλ(x; T , W ) = x⊤M λ(T , W )θ,
(11)
where we deﬁne
M λ(T , W ) := W ⊤(W XX⊤W ⊤+ λI)−1W XX⊤.
(12)
To avoid cluttered notations, we omit the dependency of M on λ, T and W .
By using (11), the expected bias and expected variance in (7) and (8) can be written as functions on the statistics of M.
This is stated in the following proposition. To proceed, we introduce the change of variable
η := γ−1 = d
p
in order to be consistent with conventions in random matrix theory.
Proposition 1 (Expected Bias/Variance). The expected bias and expected variance are given by
EBias2
λ = 1
d∥EM −I∥2, and
EVarianceλ = 1
dE∥M −EM∥2,
where M is deﬁned in (12).
Proof. By plugging (11) into (7), and using the prior that x ∼N(0, Id/d) and θ ∼N(0, Id), we get
EBias2
λ = E{E(x⊤Mθ|x, θ) −x⊤θ}2
= E

x⊤(EM −I)θ
2
= Ex⊤(EM −I)θθ⊤(EM −I)x
= Etr
h
x⊤(EM −I)θθ⊤(EM −I)⊤x
i
= tr
h
(EM −I)E(xx⊤)(EM −I)⊤E(θθ⊤)
i
= 1
d∥EM −I∥2.
Similarly, by plugging (11) into (8) we get
EVarianceλ
= E
n
E

(x⊤Mθ −E(x⊤Mθ|x, θ))2|x, θ
o
= E
n
E

(x⊤Mθ −x⊤(EM)θ)2|x, θ
o
= E(x⊤Mθ −x⊤(EM)θ)2
= E
h
x⊤(M −EM)θ
i2
= 1
dE∥M −EM∥2.
The risk is given by
EBias2
λ + EVarianceλ = 1
dE∥M −I∥2 = 1
dEtr(M ⊤M) −2
dEtr(M) + 1.

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
First, we show that in the asymptotic setting deﬁned in Assumption 1, the expected Bias and expected Variance can be
calculated as functions on the statistics of the following matrix:
f
M λ0(W ) = W ⊤(W W ⊤+ λ0I)−1W .
(13)
In the following, we omit the dependency of f
M on λ0 and W .
Proposition 2 (Gap between M and f
M). Under Assumption 1 with λ −n
d λ0, we have
1
d∥EM −I∥2 = 1
d∥E f
M −I∥2, and
1
dE∥M −I∥2 = 1
dE∥f
M −I∥2.
Proof. It sufﬁces to show that ∥M −f
M∥2 = 0 almost surely. From (12) and (13), we have
M −f
M = W ⊤ΩW + W ⊤ΩW ∆+ W ⊤(W W ⊤+ λ0I)−1W ∆,
where ∆:= (d/n)XX⊤−I and Ω:= (W W ⊤+ λ0I + W ∆W ⊤)−1 −(W W ⊤+ λ0I)−1.
By using triangle inequality and the sub-multiplicative property of spectral norm, we have
∥M −f
M∥2 ≤∥W ∥2
2 · ∥Ω∥2 + ∥W ∥2
2 · ∥Ω∥2 · ∥∆∥2 + ∥f
M∥2 · ∥∆∥2.
(14)
Furthermore, by a classical result on the perturbation of matrix inverse (see e.g., Ghaoui (2002, equation (1.1))), we have
∥Ω∥2 ≤∥(W W ⊤+ λ0I)−1∥2
2∥W ∥2
2∥∆∥2 + O(∥∆∥2
2).
Combining this bound with (14) gives
∥M −f
M∥2 ≤∥W ∥4
2 · ∥(W W ⊤+ λ0I)−1∥2
2 · ∥∆∥2 + ∥f
M∥2 · ∥∆∥2 + O(∥∆∥2
2).
It remains to show that ∥∆∥2 = 0 and that ∥W ∥2, ∥(W W ⊤+ λ0I)−1∥2
2, and ∥f
M∥2 are bounded from above almost
surely. By Wainwright (2019, Example 6.2), ∀δ > 0 and n > d,
P

∥∆∥2 ≤2ϵ + ϵ2
≥1 −e−nδ2/2, where ϵ = δ +
r
d
n.
By letting δ =
p
d/n and taking the asymptotic limit as in Assumption 1, we have
∥∆∥2
a.s.
= 0.
From Geman (1980), the largest eigenvalue of W W ⊤is almost surely (1 + √η)2 < ∞. Therefore, we have
∥W ∥2
a.s.
= 1 + √η < ∞.
Finally, note that
∥(W W ⊤+ λ0I)−1∥2 ≤
1
λ0 + σmin(W )2 ≤1
λ0
< ∞,
∥f
M∥2 =
σmax(W )2
σmax(W )2 + λ0
≤1.
We therefore conclude that ∥M −f
M∥2 = 0 almost surely, as desired.
Proposition 3 (Asymptotic Risk). Given the expression for Bias and Variance in Proposition 1, under the asymptotic
assumptions from Assumption 1,
1
dE∥f
M −I∥2 =
(
(1 −1
η) + fλ−1
0 ( 1
η),
if d > p,
fλ−1
0 (η),
if d ≤p,
where η = d/p, and for any η, α ∈R,
fα(η) =
α + η(1 + η −2α + ηα)
2η
p
η2 + 2ηα(1 + η) + α2(1 −η)2 −1 −η
2η .

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
Proof. Recall that f
M = W ⊤(W W ⊤+ λ0I)−1W , by Sherman-Morrision,
f
M = I −(I + λ−1
0 W ⊤W )−1,
where (d/p)W ⊤W ∈Rd×d. Let λi ≥0, i = 1, . . . , d be the eigenvalues of (d/p)W ⊤W . For notational simplicity, let
α = λ−1
0 . Then
∥f
M −I∥2 = ∥[I + (α/η)(d/p)W ⊤W ]−1∥2 =
d
X
i=1
1
(1 + α
η λi)2 .
Let A = (d/p)W ⊤W , and µA be the spectral measure of A. Then
1
d∥f
M −I∥2 =
Z
R+
1
(1 + α
η x)2 dµA(dx).
According to Marchenko-Pastur Law (Bai & Silverstein, 2010), in the limit when d →∞when η ≤1,
1
d∥f
M −I∥2
F
a.s.
= 1
2π
Z η+
η−
p
(η+ −x)(x −η−)
ηx(1 + α
η x)2
dx,
where η+ = (1 + √η)2, and η−= (1 −√η)2. For convenience, deﬁne
fα(η) = 1
2π
Z η+
η−
p
(η+ −x)(x −η−)
ηx(1 + α
η x)2
dx =
α + η(1 + η −2α + ηα)
2η
p
η2 + 2ηα(1 + η) + α2(1 −η)2 −1 −η
2η .
When η > 1,
1
d∥f
M −I∥2
F =

1 −1
η

+ fα
1
η

.
Then, in the asymptotic regime,
1
d∥f
M −I∥2
F
a.s.
=
(
(1 −1
η) + fα( 1
η),
if d > p,
fα(η),
if d < p.
Proposition 4 (Asymptotic Bias). Given the expression for Bias in Proposition 1, under the asymptotic assumptions in
Assumption 1, the Bias for the model is given by
1
d∥EM −I∥2 =
h
1 −λ0η + (1 + η) −
p
λ2
0η2 + 2λ0η(1 + η) + (1 −η)2
2η
i2
.
Proof. Recall that
M = W ⊤(W XX⊤W ⊤+ λI)−1W XX⊤.
Recall that f
M = I −(I + λ−1
0 W ⊤W )−1. Thus
1
d∥E f
M −I∥2 = 1
d∥E(I + λ−1
0 W ⊤W )−1∥2.
By Neumann series,
E(I + λ−1
0 W ⊤W )−1 =
X
m≥0
E(−λ−1
0 W ⊤W )m = I +
X
m≥1
(−1)m(λ0η)−mEAm,
where η = d/p, A = (d/p)W ⊤W . According to Corollary 3.3 in Bishop et al. (2018) (recall we are considering the
asymptotic regime of d, p →∞),
EAm =
m
X
k=1
ηm−kNm,k · I,

Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
where
Nm,k = 1
k
m −1
k −1
 m
k −1

is the Narayana number. Therefore,
1
d∥E f
M −I∥2 =

1 + η−1
∞
X
m=1
k
X
k=1
(−λ−1
0 )m(η−1)k−1Nm,k
2
.
Observe that the double sum in the previous equation is just the generating series for the Narayana number,
∞
X
m=1
k
X
k=1
(−λ−1
0 )m(η−1)k−1Nm,k = −λ0η + (1 + η) −
p
λ2
0η2 + 2λ0η(1 + η) + (1 −η)2
2η
.
This completes the proof.
Finally, the statement of Theorem 1 follows directly from the above propositions.

