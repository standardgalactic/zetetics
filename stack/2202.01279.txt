PromptSource: An Integrated Development Environment
and Repository for Natural Language Prompts
Stephen H. Bach∗1,2 Victor Sanh∗3 Zheng-Xin Yong1 Albert Webson1 Colin Raffel3
Nihal V. Nayak1 Abheesht Sharma4 Taewoon Kim5 M Saiful Bari6 Thibault Fevry7
Zaid Alyafeai8 Manan Dey9 Andrea Santilli10 Zhiqing Sun11 Srulik Ben-David12
Canwen Xu13 Gunjan Chhablani7 Han Wang14 Jason Alan Fries15,2
Maged S. Al-shaibani8 Shanya Sharma16 Urmish Thakker17 Khalid Almubarak18
Xiangru Tang19 Dragomir Radev19 Mike Tian-Jian Jiang20 Alexander M. Rush3
1 Brown University
2 Snorkel AI
3 Hugging Face
4 BITS Pilani
5 VU Amsterdam
6 NTU
7 BigScience
8 KFUPM
9 SAP
10 University of Rome
11 CMU
12 Technion
13 UCSD
14 NYU
15 Stanford University
16 Walmart Labs
17 SambaNova Systems
18 PSAU
19 Yale University
20 ZEALS
∗Equal Contribution
Abstract
PromptSource
is
a
system
for
creating,
sharing, and using natural language prompts.
Prompts are functions that map an example
from a dataset to a natural language input
and target output.
Using prompts to train
and query language models is an emerging
area in NLP that requires new tools that
let users develop and reﬁne these prompts
collaboratively.
PromptSource
addresses
the emergent challenges in this new setting
with (1) a templating language for deﬁning
data-linked prompts, (2) an interface that
lets users quickly iterate on prompt develop-
ment by observing outputs of their prompts
on many examples, and (3) a community-
driven set of guidelines for contributing
new prompts to a common pool.
Over
2,000 prompts for roughly 170 datasets are
already available in PromptSource.
Prompt-
Source is available at https://github.
com/bigscience-workshop/
promptsource.
1
Introduction
Prompt engineering is emerging as a new focus in
NLP, particularly in zero- and few-shot learning
settings. Prompting is the practice of representing
a task as a natural language utterance in order to
query a language model for a response (Liu et al.,
2021). For example, if a language model is con-
ditioned on the text “She hit a home run. The
previous sentence is about ...”, then the model’s
subsequent generation would be interpreted as a
prediction of the topic of the preceding sentence,
e.g. by mapping a response such as “sports” to
a class label. In speciﬁc contexts, prompting has
been shown to have advantages over traditional
classiﬁcation, for example facilitating adaptation
of language models to ad-hoc tasks and improv-
ing sample efﬁciency in low-data settings (Brown
et al., 2020; Schick and Schütze, 2021b; Le Scao
and Rush, 2021; Gao et al., 2021). These advan-
tages motivate a practical challenge: How can we
enable users to create, reﬁne, and share prompts?
The process of prompt engineering is critical
for successful deployment as choices in prompt-
ing can affect downstream predictions signiﬁcantly,
particularly in the zero-shot setting (Perez et al.,
2021; Zhao et al., 2021; Webson and Pavlick, 2021).
Furthermore, training directly on collections of
prompts can enable large models to generalize to
new prompts more robustly (Sanh et al., 2021; Wei
et al., 2021; Min et al., 2021; Mishra et al., 2021).
There is therefore a growing need for tools that
support the creation of corpora of prompts.
PromptSource is an integrated development en-
vironment and repository for natural language
prompts to use in the context of zero-shot (or
gradient-based few-shot) learning. It provides a
Web-based GUI that enables developers to write
prompts in a templating language and immediately
view their outputs on different examples. The sys-
tem is integrated with the HuggingFace Datasets
library (Lhoest et al., 2021), so that users can load
any dataset automatically, browse existing prompts,
and create new ones. Through the course of writing
thousands of prompts, we converged on three key
arXiv:2202.01279v3  [cs.LG]  29 Mar 2022

aspects to the design of PromptSource:
• Flexible Templating Language. We adapt
a templating language to represent prompts.
Prompt authors can deﬁne prompts in terms of
dataset ﬁelds, hard-coded text, and simple con-
trol logic. This choice provides the ﬂexibility
of a programming environment without the
mental overhead of having to write and read
arbitrary code. Prompt templates can easily
be distributed and used in other systems.
• Tools for Prompt Management.
Prompt-
Source has multiple view to address the needs
of prompt authors at different stages of the
prompt engineering cycle. A global view lets
authors browse datasets and existing prompt
templates. A local view facilitates iteration
on prompt wording and metadata, as well as
testing on individual examples.
• Community-Driven
Quality
Standards.
PromptSource includes a set of guidelines
for prompting based on a large-scale prompt
writing pilot.
PromptSource’s collection
is meant to be useful for a wide range
of research, based on iterative reﬁnement
of a set of quality standards.
Prompts in
PromptSource are also annotated with various
pieces of metadata to make ﬁnding and using
prompts easier.
The PromptSource system includes over 2,000
open-source prompts for roughly 170 datasets,
which have all been reviewed to meet the quality
standards. This collection, which we call the Public
Pool of Prompts (P3), allows users to materialize
prompted forms of datasets for hundreds of differ-
ent tasks. The T0 series of models (Sanh et al.,
2021) for zero-shot inference were ﬁne-tuned on
a subset of P3. Since then, PromptSource and P3
have been extended for research on multi-lingual
prompting (Lin et al., 2021) and priming, i.e., in-
context few-shot learning (Min et al., 2021). The
PromptSource system and associated content is a
ﬁrst step in the study of systems for prompt engi-
neering, an area that is likely to continue to grow.
2
Background and Related Work
PromptSource builds on recent work in prompting
and prompt engineering. It is also related to work
on systems for other types of annotations.
Prompting
Recently, prompting has emerged
as a new focus within NLP as it can dramati-
cally improve language models’ few-shot and zero-
shot performance in a wide range of downstream
tasks (Brown et al., 2020; Schick and Schütze,
2021a; Sanh et al., 2021; Wei et al., 2021). Prompts
and prompt engineering come in several vari-
eties (Liu et al., 2021). PromptSource is focused on
facilitating research with human-written prompts,
in which natural language is the medium for de-
scribing tasks. This approach has the advantage
that prompts can be understood, modiﬁed, and ap-
plied without being tied to a speciﬁc model. In
contrast, past work has also aimed to automatically
construct prompts by framing the search for a good
prompt as a learning problem. These prompts can
either be expressed in natural language (Gao et al.,
2021; Shin et al., 2020) or as arbitrary vectors (a.k.a.
“continuous” or “soft” prompts) not corresponding
to words in the model’s original vocabulary (Lester
et al., 2021; Qin and Eisner, 2021)
When using human-written prompts, there are
several possible approaches to learning. One is a
zero-shot setting, where the goal is to generalize to
prompts for which no training examples are given.
Prompts can also be used in a few-shot setting, in
which a model is either (1) trained on prompted ex-
amples of the target task via gradient updates, or (2)
priming (i.e. in-context learning), in which labeled
examples are included in an input sequence in or-
der to prime models to make predictions without
gradient updates (Brown et al., 2020).
PromptSource was originally designed for zero-
shot learning, so it emphasizes explicit task instruc-
tions and no priming examples. If needed, users
can extend PromptSource for few-shot learning
(e.g., as done in Lin et al., 2021 and Min et al.,
2021, described in §7).
Systems for Annotating Data
Most work on
collecting annotations has focused on labels and
other annotations at the level of individual exam-
ples (Neves and Ševa, 2021). GATE (Cunningham
et al., 2002) was an early system for annotating
text, and includes support for many data types such
as labels and entity tags. Since then, many Web-
based systems for annotating text have been devel-
oped (Stenetorp et al., 2012; Salgado et al., 2012;
Wei et al., 2013; Yimam et al., 2013; Chen and
Styler, 2013; Eckart de Castilho et al., 2016; Putra
et al., 2020). Other systems support collaboration
among multiple annotators (Yang et al., 2018; Stew-
art et al., 2019). More recently, many annotation
systems have begun to incorporate learned models
to improve workﬂow, using techniques such as ac-

S1: Exploration
S2 + S3 + S4: Creation
S5: Review
Browse
SNLI
The SNLI corpus (version 1.0) is a 
collection of 570k human-written 
English sentence pairs manually 
labeled for the task of NLI…
{ premise:    “The kids…”, 
  hypothesis: “All kids…”,
  label:      2 }
{ premise:    “A person…”, 
  hypothesis: “A person…”,
  label:      1 }
Sourcing
SNLI
Browse
SNLI
The SNLI corpus (version 1.0) is a 
collection of 570k human-written 
English sentence pairs manually 
labeled for the task of NLI…
“The kids…” Based on the previous 
passage, is it true that “All kids…”? 
Yes, no, or maybe? |||
No
“A person…” Based on the previous 
passage, is it true that “A 
person…”? Yes, no, or maybe? |||
Maybe
Based…
based on the previous passage
{{premise}} Based on the 
previous passage, is it true 
that "{{hypothesis}}"?
Yes, no, or maybe? |||
{{ answer_choices[label] }}
Original Task
Choices in Prompt
Adapted from the BoolQ prompts in 
Schick & Schütze 2021.
Yes ||| No ||| Maybe
Accuracy
Figure 1: The ﬁve stages of creating prompts in PromptSource. The Browse view for Dataset Exploration (S1).
The Sourcing view for Prompt Writing (S2), Prompt Documentation (S3), and Iteration and Variation (S4). The
Browse view for performing a Global Review (S5).
tive learning (Lin et al., 2019; Li et al., 2021) and
example recommendation (Lee et al., 2020; Kiela
et al., 2021). These systems are possible because
the annotations to be collected are labels, for which
metrics like inter-annotator agreement and model
conﬁdence are available.
There has also been some work on collecting
annotations other than labels. AlvisAE (Papazian
et al., 2012) and TreeAnnotator (Helfrich et al.,
2018) support creating ontologies and other struc-
tured annotations. Prompts differ from these anno-
tations in that they are semi-structured functions,
requiring new tools for developers.
3
System Design and Workﬂow
Creating prompts differs from other types of data
collection and annotation. We focus on three chal-
lenging aspects on which prompting differs from
traditional NLP annotation:
• Functions, not Labels. A single prompt is a
function that maps dataset examples (dictio-
naries of arbitrary ﬁelds) to natural language
input/target pairs. Creating a prompt is there-
fore more like programming than typical data
annotation. How should a prompt format trade
off between expressivity and simplicity?
• Dataset-Level Choices. Prompts are associ-
ated with datasets, unlike label annotations
that are local to single examples. Prompt en-
gineering requires developers to evaluate their
choices across all examples. What interfaces
do authors need to inspect and debug their
prompts?
• Variation in Prompt Construction. Unlike
with labels, it is often desirable to have varia-
tion within prompt construction, as different
prompt choices may lead to different results.
However, variation complicates quality judg-
ment, and makes it impossible to apply simple
metrics like inter-annotator agreement. How
can multiple authors collaborate to build a
high-quality corpus of prompts and associated
metadata?
To illustrate these distinct aspects, we start with
a concrete overview of the prompt creation process
of PromptSource. For this example, we imagine
that a user of PromptSource is creating prompts for
a natural language inference dataset, speciﬁcally
SNLI (Bowman et al., 2015). The goal is to de-
sign a prompt query such that the answer can be
mapped onto the SNLI classes. A prompt author
can accomplish this goal with PromptSource via
the following ﬁve steps (Figure 1):
S1: Dataset Exploration The prompt author
starts in the Browse view to read the dataset de-
scription, including linked READMEs and papers,
and to browse through examples. In this case, they
would see that SNLI is a dataset for natural lan-
guage inference: assume a given premise sentence
is true, the goal is to determine whether a hypoth-
esis sentence is true (entailment), false (contradic-
tion), or undetermined (neutral).
S2: Prompt Writing The prompt author uses
the Sourcing view to try out a prompt wording, and
then adjusts it by observing prompted examples
(Figure 1 middle, full example in Figures 3 and 4).
S3: Prompt Documentation To facilitate using
the prompt, the author ﬁlls in various metadata in-
cluding possible metrics to evaluate the prompt,
valid outputs if applicable, whether the prompt ex-
presses the original intended task of the dataset,
and whether the template explicitly states the valid
outputs.
S4: Iteration and Variation The prompt author

then iterates through S2 and S3 to create multiple
prompts for the dataset. Authors are encouraged to
vary multiple factors such as the formulation of the
prompt and the targeted task (see Section 6).
S5: Global Review The author saves the draft
prompts in a structured ﬁle which are then veriﬁed
by other contributors through code reviews. New
prompts need to meet the quality standard with a
series of automatic tests and by validation through
prompted instances. Upon passing review, the new
prompts can be merged into a global prompts col-
lection.
Upon submission, prompts can be viewed
through PromptSource by other users. The full col-
lection is stored globally and can be used outside of
the tool, for instance to be applied on an example
from a dataset of the Datasets library (Lhoest et al.,
2021).
from promptsource.templates import DatasetTemplates
from datasets import load_dataset
prompts = DatasetTemplates("snli")
prompt_key = "based on the previous passage"
p = prompts[prompt_key]
dataset = load_dataset("snli", split="train")
example = dataset[0]
result = p.apply(example)
print("INPUT: ", result[0])
print("TARGET: ", result[1])
With this workﬂow in mind, we next describe the
key aspects of the PromptSource system in greater
detail.
4
Prompting Language
A key design decision is the format for prompts.
Previous works on prompting tended to use code
for specifying each prompt. We experimented with
this format and found a trade-off between expressiv-
ity and explicit structure. On one side, a maximally
expressive format such as pure Python code would
let users write complex programs to manipulate
the semi-structured examples into prompted exam-
ples. However, interpreting and analyzing these
programs becomes difﬁcult. This difﬁculty lim-
its downstream manipulation and analysis of the
prompts, for example for possible future work on
automatic prompt augmentation. On the other side,
a maximally structured format, such as rule-based
generation, limits the kinds of prompts that users
can create. We found it infeasible to enumerate
types of rules sufﬁcient for the wide range of tasks
and data formats for which we wanted prompts.
We therefore settled on a middle ground between
the two: a templating language.
Speciﬁcally,
we use the Jinja2 templating engine,1 originally
designed for producing web markup.
Users
write templates as prompts with placeholders,
such
as
If {{premise}} is true, is
it also true that {{hypothesis}}?
||| {{entailed}}.
The separator |||
denotes the break between the conditioning text
and the desired completion. Placeholders refer
to ﬁelds in the underlying example (represented
as a Python dict by Datasets (Lhoest et al.,
2021)). Users also have access to Jinja’s built-in
functions,
such as manipulating strings and
structured data.
For each prompt, prompted
examples are created by applying the prompt to
all examples in the corresponding dataset. While
Jinja is a complete programming language, our
review guidelines encourage simple functions with
minimal additional logic (see Figure 3 and 4 for
example).
During the development of PromptSource, we
found that a few idioms were particularly useful.
First, not all templates are applicable to all exam-
ples in a dataset. Users can wrap templates in
Jinja’s built-in conditional statements, and any ex-
ample that results in an empty prompted example
is simply skipped. Second, many examples can
be used to make multiple training instances, such
as a question that has multiple valid answers. We
therefore added a choice function that selects an
element from a list in a way that can be controlled
during dataset generation, such as picking a random
element using a seeded random number generator
or generating different prompts for each combina-
tion of elements in the template. Third, many tasks
such as classiﬁcation and binary question answer-
ing have a small set of possible valid completions,
and it is common to make predictions for these
tasks by scoring only the valid completions and
returning the highest one (Brown et al., 2020; Sanh
et al., 2021; Wei et al., 2021). Users therefore can
list the valid completions in a separate ﬁeld and
access them as a list in their prompts (displayed as
Answer choices in Figure 3). These comple-
tions are then explicitly available when evaluating
predictions for these prompted examples.
5
The PromptSource UI
The PromptSource system is designed to enable
prompt creators to view data (S1), write prompts
in a standard format (S2, S3, and S4), and ver-
1https://jinja.palletsprojects.com

Figure 2:
Prompt creators can browse through the
dataset examples (left-column) and their prompted
form (right column) using the Browse view.
ify that their templates work correctly (S5). We
implemented a lightweight interface for the tool in
Streamlit2 so that users could download, run locally
in a web browser, and then upload their results to
a central repository. Testing iterations of the inter-
face on pilot template-writing tasks, we converged
on three views for the interface.
V1: Browse This view (Figure 2) lets users in-
spect datasets before creating prompts (S1). Once
prompts are created, they can select prompts and
browse the prompted examples generated by them
(S5). The original example is viewed side-by-side
with the resulting prompted example, with the sub-
stituted text highlighted to distinguish from text
hard-coded in the template. Users can quickly
scroll through many examples, verify the behavior
of their prompt, and return to the sourcing view if
changes are needed.
V2: Sourcing This view (Figures 3 and 4) al-
lows users to select a dataset to prompt, browse
examples from that dataset in the form of tables,
and enter a prompt for that dataset. As the user
writes their template (S2, S3, and S4), every time
they save it, the output of the template applied to
the current example is displayed next to the edi-
tor. We also collect metadata like a name for the
template, and a reference for any bibliographic in-
formation or rationale for the template.
V3: Helicopter This view (Figure 5) allows
users to see what datasets are available for writ-
ing templates and how many are written for each,
to prioritize user attention. This view is particu-
larly useful for moving between datasets and for
the prompt reviewers (S5).
2https://streamlit.io/
6
Community Guidelines and Process
Due to the variety of existing NLP datasets, we
found it challenging to exhaustively describe the
characteristics of a good prompt: there are no
simple metrics like inter-annotator agreement on
example-level labels. Instead, over a few iterations,
we converged on community guidelines3 with three
objectives in mind: (a) provide a standardized vo-
cabulary for discussing prompts between prompt
authors, reviewers and users, and minimum require-
ments for a valid prompt, (b) highlight common
errors and best practices, (c) collect the necessary
information about the prompts to support current
and future research on prompt engineering. The
guidelines were enforced in the use of Prompt-
Source by a code review process in which each
prompt was reviewed before being committed to
the central repository.
Guidelines apply to the combination of a tem-
plate (a function that maps an example into an in-
put/target pair in natural language) and a set of
metadata about the template. The most impor-
tant constraint we imposed for a template to be
valid is that it is formulated in natural language
(both for the input and the target). We forbid the
use of non-natural language prompts such as pure
code. Each prompt should clearly state what task
should be solved, in a way a non-specialist adult
can understand. We found this guideline strikes a
good balance between freedom and expressivity in
the wording of the prompts on one side and short
generic prompts on the other side.
In early experiments, we found that user-written
prompts that did not explicitly state the possible
valid completions tended to perform worse in ex-
periments than their counterparts in which the pos-
sible valid completions were listed. We encouraged
prompt authors to explicitly state the valid outputs
in some of their prompts. In addition, when work-
ing with training prompts that include target text,
we found it useful to remove variations on the target
format that led to spurious ambiguity. For instance,
the target template should only contain the answer
to the task. It should not contain any extra text such
as “The answer is ...”, which can be equivalently
moved to the input template.
One of the research question we hope to enable
with PromptSource is whether the diversity of the
3Complete
guidelines
can
be
found
at
https:
//github.com/bigscience-workshop/
promptsource/blob/main/CONTRIBUTING.md.

Figure 3: With the Sourcing view, prompt authors can
write new prompts, ﬁll in the associated metadata, ob-
serve the result on examples, and iterate.
prompt formulation during training leads to mod-
els that are more robust to the prompt formulation
at test time. Therefore, we encouraged prompt
authors to create between 5 and 10 (or more)
prompts per dataset while varying the prompt for-
mulation. For a given dataset, authors produce
multiple prompts per example, sometimes for task
formulations that differed from the original dataset.
For instance, for question answering dataset, one
prompt can ask to extract the answer to a given
question from a given passage, while a second
prompt can ask to generate a potential question
given an answer and a passage.
As part of the community process and to facil-
itate future research, PromptSource asks prompt
authors to include additional metadata for each
prompt. Metadata ﬁelds include a name for the
prompt, a reference to the paper it was extracted
from (or any relevant explanation), whether the
prompt expresses the task originally intended by
the dataset, the valid outputs (if relevant), whether
the input template states the valid outputs, and pos-
sible metrics to evaluate the prompted examples.
These can be used in future systems to evaluate how
the style and structure of prompts leads to different
downstream results.
7
Case Studies
A system for creating, maintaining, and using
prompts is a key tool for supporting the emerg-
ing research area of prompting in a standardized
and reproducible manner. We highlight three recent
research projects for which PromptSource was a
key resource.
Figure 4: Another example of the the Sourcing view,
focusing on the editor. The templating language strikes
a balance between expressivity and explicit structure.
This prompt for QA-ZRE (Levy et al., 2017), a dataset
for zero-shot relation extraction, shows how to manipu-
late strings and do conditional statements with Jinja.
Massively multitask prompted training
Sanh
et al. (2021) study the question of zero-shot be-
haviors in large language models and ask whether
zero-shot generalization can be induced by train-
ing a language model on a massively multitask
mixture of prompts. To test this question, they
use PromptSource to create diverse prompts for a
large collection of NLP datasets. Their training and
evaluation prompts are a subset of P3. This work
demonstrates that PromptSource allows training a
language model on a massively multitask mixture
of prompted datasets and evaluating the ability of
models trained with such a procedure to perform
unseen tasks.
Multilingual prompting Lin et al. (2021) study
the zero- and few-shot learning abilities of an mul-
tilingual autoregressive language model trained on
30 languages. In particular, they are interested in
the cross-lingual generalization of such models and
benchmark a variety of tasks in multiple languages.
PromptSource allows using a massive set of high-
quality English prompts. Moreover, the English
prompts serve as support to create prompts in other
languages (through either machine or human trans-
lation).
Priming (in-context learning) Min et al. (2021)
study improving models’ few-shot priming perfor-
mance by ﬁrst fully training a model (with gradient
updates) on a multitask mixture formatted with
priming examples. They ﬁnd that incorporating
templates from P3 signiﬁcantly further improves
performance compared to training on priming ex-

Figure 5: The Helicopter view indicates what datasets
have prompts and how many prompts are available for
each dataset.
amples alone. Although PromptSource was not
originally designed for this speciﬁc form of prompt-
ing, users were able to easily use P3’s template col-
lection and the templating language for their own
priming methods.
8
Conclusion
PromptSource is an open-source system for creat-
ing, sharing, and using natural language prompts
and addresses the need for new collaborative and
centralized tools to support the emerging research
around prompting. The tool is designed to an-
swer three key needs: a ﬂexible template lan-
guage, a suite of tools for prompt management, and
community-driven quality standards. As of January
2022, PromptSource includes a growing collection
of 2,000 public prompts for roughly 170 datasets,
and has already been an instrumental resource for
multiple recent research projects.
Acknowledgements
This research was conducted under the BigScience
project for open research,4 a year-long initiative tar-
geting the study of large models and datasets. The
goal of the project is to research language models
in a public environment outside large technology
companies. The project has over 950 researchers
from over 65 countries and more than 250 insti-
tutions. The BigScience project was initiated by
Thomas Wolf at Hugging Face, and this collabora-
tion would not have been possible without his ef-
fort. This research was the focus of the BigScience
Prompt Engineering working group, which focused
on the role of prompting in large language model
training. Disclosure: Stephen Bach contributed to
this work as an advisor to Snorkel AI.
4https://bigscience.huggingface.co/
References
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual.
Wei-Te Chen and Will Styler. 2013. Anafora: A web-
based general purpose annotation tool. In Proceed-
ings of the 2013 NAACL HLT Demonstration Ses-
sion, pages 14–19, Atlanta, Georgia. Association for
Computational Linguistics.
Hamish
Cunningham,
Diana
Maynard,
Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: an
architecture for development of robust HLT applica-
tions. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
pages 168–175, Philadelphia, Pennsylvania, USA.
Association for Computational Linguistics.
Richard Eckart de Castilho, Éva Mújdricza-Maydt,
Seid Muhie Yimam,
Silvana Hartmann,
Iryna
Gurevych, Anette Frank, and Chris Biemann. 2016.
A web-based tool for the integrated annotation of se-
mantic and syntactic structures. In Proceedings of
the Workshop on Language Technology Resources
and Tools for Digital Humanities (LT4DH), pages
76–84, Osaka, Japan. The COLING 2016 Organiz-
ing Committee.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Philipp Helfrich, Elias Rieb, Giuseppe Abrami, Andy
Lücking, and Alexander Mehler. 2018. TreeAnno-
tator: Versatile visual annotation of hierarchical text
relations.
In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2018), Miyazaki, Japan. European
Language Resources Association (ELRA).

Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-
gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-
shia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,
Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mo-
hit Bansal, Christopher Potts, and Adina Williams.
2021.
Dynabench: Rethinking benchmarking in
NLP.
In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4110–4124, Online. Association for
Computational Linguistics.
Teven Le Scao and Alexander Rush. 2021. How many
data points is a prompt worth? In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 2627–2636, On-
line. Association for Computational Linguistics.
Dong-Ho Lee, Rahul Khanna, Bill Yuchen Lin, Seyeon
Lee, Qinyuan Ye, Elizabeth Boschee, Leonardo
Neves, and Xiang Ren. 2020.
LEAN-LIFE: A
label-efﬁcient annotation framework towards learn-
ing from explanation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics:
System Demonstrations, pages 372–
379, Online. Association for Computational Linguis-
tics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), pages 333–342, Vancou-
ver, Canada. Association for Computational Linguis-
tics.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 175–184, On-
line and Punta Cana, Dominican Republic. Associ-
ation for Computational Linguistics.
Yanzeng Li, Bowen Yu, Li Quangang, and Tingwen
Liu. 2021. FITAnnotator: A ﬂexible and intelligent
text annotation system. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies: Demonstrations, pages 35–
41, Online. Association for Computational Linguis-
tics.
Bill Yuchen Lin, Dong-Ho Lee, Frank F. Xu, Ouyu
Lan, and Xiang Ren. 2019. AlpacaTag: An active
learning-based crowd annotation framework for se-
quence tagging. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 58–63, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona T. Diab, Veselin
Stoyanov, and Xian Li. 2021.
Few-shot learn-
ing with multilingual language models.
CoRR,
abs/2112.10668.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
CoRR, abs/2107.13586.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2021. Metaicl: Learning to learn
in context. CoRR, abs/2110.15943.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773.
Mariana Neves and Jurica Ševa. 2021. An extensive
review of tools for manual annotation of documents.
Brieﬁngs in bioinformatics, 22(1):146–163.
Frédéric Papazian, Robert Bossy, and Claire Nédellec.
2012. AlvisAE: a collaborative web text annotation
editor for knowledge acquisition.
In Proceedings
of the Sixth Linguistic Annotation Workshop, pages
149–152, Jeju, Republic of Korea. Association for
Computational Linguistics.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.
2021. True few-shot learning with language models.
NeurIPS.
Jan Wira Gotama Putra, Simone Teufel, Kana Mat-
sumura, and Takenobu Tokunaga. 2020.
TIARA:
A tool for annotating discourse relations and sen-
tence reordering. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages
6912–6920, Marseille, France. European Language
Resources Association.

Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 5203–5212, Online. Association for Compu-
tational Linguistics.
David Salgado, Martin Krallinger, Marc Depaule,
Elodie Drula, Ashish V. Tendulkar, Florian Leitner,
Alfonso Valencia, and Christophe Marcelle. 2012.
MyMiner: a web application for computer-assisted
biocuration and text annotation.
Bioinformatics,
28(17):2285–2287.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak,
Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Stella Biderman, Leo
Gao, Tali Bers, Thomas Wolf, and Alexander M.
Rush. 2021.
Multitask prompted training enables
zero-shot task generalization.
Timo Schick and Hinrich Schütze. 2021a. Exploiting
cloze-questions for few-shot text classiﬁcation and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 255–269, Online. Association for Com-
putational Linguistics.
Timo Schick and Hinrich Schütze. 2021b. It’s not just
size that matters: Small language models are also
few-shot learners. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 2339–2352, Online. As-
sociation for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,
Eric Wallace, and Sameer Singh. 2020. AutoPrompt:
Eliciting Knowledge from Language Models with
Automatically Generated Prompts.
In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4222–4235, Online. Association for Computational
Linguistics.
Pontus Stenetorp,
Sampo Pyysalo,
Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102–107, Avignon, France. Association for
Computational Linguistics.
Michael Stewart, Wei Liu, and Rachel Cardell-Oliver.
2019.
Redcoat: A collaborative annotation tool
for hierarchical entity typing.
In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP): System Demonstrations, pages
193–198, Hong Kong, China. Association for Com-
putational Linguistics.
Albert Webson and Ellie Pavlick. 2021. Do prompt-
based models really understand the meaning of their
prompts? ArXiv, abs/2109.01247.
Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu.
2013.
PubTator:
a web-based text mining tool
for assisting biocuration. Nucleic Acids Research,
41(W1):W518–W522.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2021.
Finetuned
language models are zero-shot learners.
CoRR,
abs/2109.01652.
Jie Yang, Yue Zhang, Linwei Li, and Xingxuan Li.
2018.
YEDDA: A lightweight collaborative text
span annotation tool. In Proceedings of ACL 2018,
System Demonstrations, pages 31–36, Melbourne,
Australia. Association for Computational Linguis-
tics.
Seid
Muhie
Yimam,
Iryna
Gurevych,
Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno:
A ﬂexible, web-based and visually
supported system for distributed annotations.
In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics:
Sys-
tem Demonstrations, pages 1–6, Soﬁa, Bulgaria.
Association for Computational Linguistics.
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,
and Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language models.
CoRR, abs/2102.09690.

Figure 6: Most of the datasets have between 5 and 10
prompts.
A
Data and Statistics
P3 is the largest public collection of English
prompts and is actively growing. As of January
2022, it contains 2’052 English prompts for 170
English datasets (or 269 subsets, one dataset can
contain multiple subsets with different prompts).
There is an average of 7.6 prompts per data subset
and an average 5.6 original-task prompts per data
subset (see Figure 6).
P3 was developed as part of the BigScience
project for open research5.
There was a open
hackathon to collect prompts for as many English
NLP dataset (or English subsets of datasets) as pos-
sible. Almost 50 unique contributors afﬁliated with
more than 25 institutions in 10 countries partici-
pated.
B
Complete Views
We show higher resolution examples of the full
interfaces for the Browse (Figure 7), Sourcing (Fig-
ure 8), and Helicopter (Figure 9) views.
5https://bigscience.huggingface.co

Figure 7: Complete example of the Browse view.
Figure 8: Complete example of the Sourcing view.

Figure 9: Complete example of the Helicopter view.

