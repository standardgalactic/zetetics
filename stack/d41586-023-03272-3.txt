By Max Kozlov & Celeste Biever
S
cientists have created a neural network 
with the human-like ability to make 
generalizations about language. The 
artificial intelligence (AI) system 
performs about as well as humans at 
folding newly learnt words into an existing 
vocabulary and using them in fresh contexts, 
which is a key aspect of human cognition 
known as systematic generalization. 
The researchers gave the same task to the AI 
model that underlies the chatbot ChatGPT, and 
found that it performed much worse than did 
either the new neural net or people, despite 
the chatbot’s uncanny ability to converse in a 
human-like manner (B. M. Lake and M. Baroni 
Nature https://doi.org/k23d; 2023).
The work, published on 25 October in 
Nature, could lead to machines that interact 
with people more naturally than even the best 
AI systems do today. Although systems based 
on large language models, such as ChatGPT, 
are adept at conversation in many contexts, 
they display glaring gaps and inconsistencies 
in others.
The neural network’s human-like perfor-
mance suggests there has been a “breakthrough 
in the ability to train networks to be systematic”, 
says Paul Smolensky, a cognitive scientist who 
specializes in language at Johns Hopkins Uni-
versity in Baltimore, Maryland.
Systematic generalization is demonstrated 
by people’s ability to effortlessly use newly 
acquired words in fresh settings. For exam-
ple, once someone grasps the meaning of the 
A neural network outperforms ChatGPT in a key 
aspect of human intelligence.
AI HAS HUMAN‑LIKE 
ABILITY TO GENERALIZE 
LANGUAGE
A neural net has achieved a version of humans’ flexibility at using new vocabulary.
MARRIO31/GETTY
word ‘photobomb’, they can use it in a variety 
of situations, such as ‘photobomb twice’ or 
‘photobomb during a Zoom call’. Similarly, 
someone who understands the sentence ‘the 
cat chases the dog’ will also understand ‘the dog 
chases the cat’ without much extra thought.
Language lessons
But this ability does not come innately to 
neural networks, which emulate human cogni-
tion and have dominated artificial-intelligence 
research, says Brenden Lake, a cognitive com-
putational scientist at New York University and 
co-author of the study. Unlike people, neural 
nets struggle to use a new word until they have 
been trained on many sample texts that use 
that word. AI researchers have sparred for 
nearly 40 years over whether neural networks 
could ever be a plausible model of human cog-
nition if they cannot demonstrate this type of 
systematicity.
To attempt to settle this debate, the authors 
first tested 25 people on how well they use 
newly learnt words in different situations. 
The researchers ensured that the participants 
would be encountering these words for the first 
time by testing them on a pseudo-language 
consisting of two categories of nonsense 
words. ‘Primitive’ words such as ‘dax’ and ‘lug’ 
represented basic, concrete actions such as 
‘skip’ and ‘jump’. More abstract ‘function’ 
words such as ‘kiki’ and ’fep’ specified rules for 
using and combining the primitives, resulting 
in sequences such as ‘jump three times’ or ‘skip 
backwards’.
Participants were trained to link each prim-
itive word with a circle of a particular colour, 
with a red circle representing ‘dax’, and a blue 
circle representing ‘lug’. The researchers 
then showed the participants combinations 
of primitive and function words alongside the 
patterns of circles that would result when the 
functions were applied to the primitives. For 
example, the phrase ‘dax fep’ was shown with 
three red circles, and ‘lug fep’ with three blue 
circles, indicating that fep denotes an abstract 
rule to repeat a primitive three times.
Finally, the researchers tested participants’ 
ability to apply these abstract rules by giving 
them complex combinations of primitives and 
functions. The volunteers then had to select 
the correct colour and number of circles and 
put them in the appropriate order.
Cognitive benchmark
As predicted, people excelled at this task; they 
chose the correct combination of coloured cir-
cles about 80% of the time, on average. When 
they did make errors, the researchers noticed 
that these followed a pattern that reflected 
known human biases.
Next, the researchers trained a neural net-
work to do a task similar to the one presented 
to participants, by programming it to learn 
from its mistakes. This approach allowed the AI 
says when she was made a principal investi-
gator, she went from having no experience in 
running a lab, to having students depend on 
her for direction while also needing to meet 
her own research goals, without any profes-
sional support — an experience she describes 
as “overwhelming”. “The anxiety that came 
with it was not constructive for attempting 
long-term, high-impact research,” she says.
Igami says that seeing lab members strug-
gle with increased seniority might be putting 
off younger scientists from pursuing a career 
in research. He says that the number of PhD 
students has dropped by 21% in the past two 
decades. Attracting more PhD students to the 
lab, who have more research experience than 
undergraduates and master’s students, will be 
crucial for facilitating higher-impact research 
for Japan, he says.
“Japan’s research environment hasn’t pro-
gressed from the past, and career prospects 
in academia are only getting worse, as univer-
sities increasingly offer temporary positions 
for researchers,” he says.
16  |  Nature  |  Vol 623  |  2 November 2023
News in focus

By Davide Castelvecchi
A 
brain-inspired computer chip that 
could boost artificial intelligence (AI) 
by working faster than today’s devices 
with much less power has been devel-
oped by researchers at IBM in San 
Jose, California. Their massive NorthPole 
processor chip eliminates the need to access 
external memory frequently, and so performs 
tasks such as image recognition in a fraction 
of the time taken by existing architectures — 
while consuming less energy.
“Its energy efficiency is just mind-blowing,” 
says Damien Querlioz, a nanoelectronics 
researcher at the University of Paris-Saclay 
in Palaiseau. The work, published in Science1, 
shows that computing and memory can be 
integrated on a large scale, he says. “I feel 
the paper will shake the common thinking in 
computer architecture.”
NorthPole runs neural networks: multi- 
layered arrays of simple computational units 
programmed to recognize patterns in data. 
A bottom layer takes in data, such as the pix-
els in an image; each successive layer detects 
increasing complexity and passes information 
on to the next layer. The top layer’s output can 
then, for example, express how likely it is that 
an image contains a cat, a car or other objects.
Some computer chips can handle these cal-
culations efficiently, but they still need to use 
external memory called RAM each time they 
calculate a layer. Shuttling data between chips 
in this way slows things down — a phenome-
non known as the Von Neumann bottleneck, 
after mathematician John von Neumann, who 
Processor sidesteps need to access external memory, 
boosting computing power and saving energy.
‘MIND-BLOWING’  
IBM CHIP COULD 
SUPERCHARGE AI
The NorthPole chip brings memory and 
processing together.
IBM CORP.
first conceived the standard architecture of 
computers based on a processing unit and a 
separate memory unit. The Von Neumann bot-
tleneck is one of the most significant factors 
in slowing down computing — including AI. It 
also results in energy inefficiencies.
NorthPole is made of 256 computing units, 
or cores, each of which contains its own mem-
ory. “You’re mitigating the Von Neumann bot-
tleneck within a core,” says study co-author 
Dharmendra Modha, who is IBM’s chief scien-
tist for brain-inspired computing at the com-
pany’s Almaden research centre in San Jose.
The cores are wired together in a network 
inspired by the white-matter connections 
between parts of the human cerebral cortex, 
Modha says. This and other design principles 
enable NorthPole to beat existing AI machines 
by a substantial margin in standard bench-
mark tests of image recognition. NorthPole 
also uses one-fifth of the energy of state-of-
the-art AI chips. If the NorthPole design were 
implemented with the most up-to-date man-
ufacturing process, its efficiency would be 25 
times better than that of current designs, the 
authors estimate.
On the right road
But even NorthPole’s 224 megabytes of RAM 
are not enough for large language models, 
such as those used by the chatbot ChatGPT, 
which take up several thousand megabytes 
of data even in their most stripped-down 
versions. And the chip can run only pre-pro-
grammed neural networks that need to be 
‘trained’ in advance on a separate machine. 
But the paper’s authors say that the NorthPole 
architecture could be useful in speed-critical 
applications, such as self-driving cars.
NorthPole brings memory units as physically 
close as possible to the computing elements 
in the core. Elsewhere, researchers have been 
developing more-radical innovations using 
new materials and manufacturing processes. 
These enable the memory units themselves to 
perform calculations, which could, in principle, 
boost both speed and efficiency even further.
Another chip, described in September2, 
does in-memory calculations using memris-
tors, circuit elements that can switch between 
being a resistor and a conductor. “Both 
approaches, IBM’s and ours, hold promise in 
mitigating latency and reducing the energy 
costs associated with data transfers,” says Bin 
Gao at Tsinghua University in Beijing, who 
co-authored the memristor study.
Another approach, developed by several 
teams — including one at a separate IBM lab 
in Zurich, Switzerland3 — stores information 
by changing a circuit element’s crystal struc-
ture. It remains to be seen whether these newer 
approaches can be scaled up economically.
1.	 Modha, D. S. et al. Science 382, 329–335 (2023).
2.	 Zhang, W. et al. Science 381, 1205–1211 (2023).
3.	 Le Gallo, M. et al. Nature Electron. 6, 680–693 (2023).
to learn as it completed each task rather than 
using a static data set, which is the standard 
approach to training neural nets. To make the 
neural net human-like, the authors trained 
it to reproduce the patterns of errors they 
observed in humans’ test results. When the 
neural net was then tested on fresh puzzles, 
its answers corresponded almost exactly to 
those of the human volunteers, and in some 
cases exceeded their performance.
By contrast, the large language model GPT-4 
failed, on average, between 42% and 86% of the 
time, depending on how the researchers pre-
sented the task. “It’s not magic, it’s practice,” 
Lake says. “Much like a child also gets practice 
when learning their native language, the mod-
els improve their compositional skills through 
a series of compositional learning tasks.”
Melanie Mitchell, a computer and cognitive 
scientist at the Santa Fe Institute in New 
Mexico, says this study is an interesting 
proof of principle, but it remains to be seen 
whether this training method can be scaled up 
to generalize across a much larger data set or 
even to images. Elia Bruni, a specialist in nat-
ural language processing at the University 
of Osnabrück in Germany, says this research 
could make neural networks more-efficient 
learners. This would reduce the gargantuan 
amount of data needed to train systems such 
as ChatGPT and would minimize ‘hallucina-
tion’, which occurs when AI perceives patterns 
that are non-existent and creates inaccurate 
outputs. “Infusing systematicity into neural 
networks is a big deal,” Bruni says. “It could 
tackle both these issues at the same time.”
Nature  |  Vol 623  |  2 November 2023  |  17

