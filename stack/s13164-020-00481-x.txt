New Labels for Old Ideas: Predictive Processing
and the Interpretation of Neural Signals
Rosa Cao1
# Springer Nature B.V. 2020
Abstract
Philosophical proponents of predictive processing cast the novelty of predictive models
of perception in terms of differences in the functional role and information content of
neural signals. However, they fail to provide constraints on how the crucial semantic
mapping from signals to their informational contents is determined. Beyond a novel
interpretative gloss on neural signals, they have little new to say about the causal
structure of the system, or even what statistical information is carried by the signals.
That means that the predictive framework for perception can be relabeled in tradi-
tional, non-predictive terms, with no empirical consequences relevant to existing or
future data. To the extent that neuroscientific research based on predictive processing
is both innovative and productive, it will be due to the framework’s suggestive heuristic
effects, or perhaps auxiliary empirical claims about implementation, rather than a
difference in the information-processing structure that it describes.
1 Introduction
Predictive theories of brain function have seen a surge of philosophical interest over the
past few years. Their advocates claim that they constitute a radical advance over
previous theories, with Clark, for example, telling us that predictive theories are
“among the most promising ever to emerge from computational and cognitive
https://doi.org/10.1007/s13164-020-00481-x
“Prediction is in effect the conjectural anticipation of further sensory evidence for a foregone conclusion.
When a prediction comes out wrong, what we have is a divergent and troublesome sensory stimulation that
tends to inhibit that once foregone conclusion, and so to extinguish the sentence-to-sentence conditionings that
led to the prediction. Thus it is that theories wither when their predictions fail.” - WVO Quine (Word and
Object Ch 1)
* Rosa Cao
rosacao@stanford.edu
1
Rosa Cao Dept of Philosophy, Stanford University, Bldg 90 450 Jane Stanford Way, Stanford,
CA 94305, USA
Review of Philosophy and Psychology (2020) 11:517–546
Published online: 6 August 2020

neuroscience” (Clark 2013, 2015). Hohwy (2013) goes further, arguing that his
preferred version of predictive processing1 can explain everything from the brain’s
physical architecture to perceptual phenomenology,2 while providing “an entirely
general framework for uniting perception, action, and thinking.”3
Within theoretical neuroscience, predictive theories of perception are the intellectual
descendants of several families of models applied to non-perceptual systems with some
success: temporal difference (TD) models of learning involving reward prediction error
minimization,4 forward models for motor action inspired by work in control theory,5
and signal compression algorithms developed for telecommunications.6
To get an intuitive feel for some of the features that predictive theorists find attractive,
consider the idea behind a very simple video compression algorithm. We could either send a
full picture of each successive frame, or we could send only the changes between one frame
and the previous one. Since for most frames in most videos, the majority of the pixels stay
the same between successive frames, this results in a dramatic reduction in the bandwidth
required to transmit the video. There is now a sense in which the system that sends only the
changes has a “prediction” about what will likely happen next (i.e. nothing new), and then
transmits only the “error” in that prediction (i.e. the parts of the image that changed).
In perception, and in the visual system in particular, predictive theories are contrasted
with traditional bottom-up theories of visual processing. (See Fig. 1). According to the
classical story, visual stimuli set off a feedforward cascade of activations, with cells at earlier
“lower” stages of processing selectively responding to small local features (oriented edges in
primary visual cortex (V1) for example), and neurons at later “higher” stages firing in
response to more complex features in the scene (perhaps objects or faces in inferotemporal
cortex (IT) and the fusiform face area (FFA)).7 From the perspective of the predictive
1 Friston (2005, 2008, 2009, 2010), Adams et al. (2013).
2 Hohwy et al. (2008)
3 Part of the enthusiasm is due to the apparent unification of perception and action that predictive processing
theories provide, and this is indeed a novel contribution of the predictive framework. However, in this paper I
will focus just on the implications of the theory for perception, leaving aside prediction-based accounts of
action (e.g. Adams et al. 2013). I will also only be considering the relevant models as models of perceptual
inference on a short time scale, after model weights are already set, rather than as models of perceptual
learning over slower time scales. That said, I do think it is reasonable to highlight the distinctive role of
predictions and prediction error in learning (as in the reinforcement learning paradigm described in footnote 4
below).
4 The reward prediction error model maps firing of dopamine neurons in the ventral tegmental area (VTA), a
region that is active when an animal receives rewards, to the reward prediction error variable in a computa-
tional model of reinforcement learning. That model accurately predicts the firing rate of the VTA neurons in
stimulus conditions of expected, unexpected and less-than-expected reward. Sutton (1988); Schultz et al.
(1997)
5 Forward models for motor control were developed to solve the problem of controlling motor action given
noise in the system and tight timing constraints. Conant ad Ashby (1970); Wolpert and Miall (1996)
6 Linear predictive coding was originally a method of compressing audio and video signals for transmission –
a way to deliver the very same information in a signal, but using less bandwidth. Atal and Schroeder (1970).
7 For important contributions to this traditional story, see Hubel and Wiesel (1959), Marr (1982), and
Kanwisher et al. (1997). Marr in particular was very sensitive to the underdetermination of distal causes by
proximal sensory input; an important part of his framework involved the positing of implicit assumptions
regimenting visual processes (e.g. deriving surface information from 2D retinal images). See his discussion on
pp. 265–267 of Vision. Priors in the predictive theory, I take it, are explicit – they are identified with patterns of
top-down activity, rather than implicit in the functional architecture of the system (or in the “algorithms” that it
implements). As I’ll argue later, however, the distinction between implicit and explicit is going to depend on
the encoding scheme, and that, in turn, will depend on the eye of the beholder.
Cao R.
518

theorists, the defining feature of traditional theories is their focus on the bottom-up flow of
information.8 What exactly is meant by “information” depends on some further commit-
ments, which will be clarified in the next section.
8 The characterization of the flow of information as top-down/backwards or bottom-up/forwards immediately
raises the question of how those directions should be understood. Once we move away from the peripheral
sensory surfaces, the complex connectivity and organization of brain areas and cell populations can only be
shoehorned into a strictly hierarchical model with difficulty, if at all. Although there are broad projections from
population to population, from brain area to brain area, there is no clear pathway that all signals take that
would allow us to tag one area as definitively “above” or “below” another for all signals or all tasks. Still, for
the most well-studied portions of the visual system (for example), there is a canonical progression from the
retina to LGN, to V1, V2, V4, and IT that is taken to be the bottom-up sequence, and this is portrayed in Fig 1.
One more principled way to address the difficulty is to do as Shea (2015) proposes and replace the
distinction between top-down and bottom-up with the more flexible contrast between direct and indirect
processing. The idea is that the relevant distinction is between activity that depends primarily on current (or
more recent) input, vs. activity that depends primarily on previously stored information. So bottom-up activity
is generally activity based on current input, while top-down activity can be interpreted as the activation of
previously stored information. This characterization is compatible with both traditional and predictive
processing stories and is entirely neutral between them.
Fig. 1 Example (from Poggio and Serre 2013) of a “traditional” feedforward hierarchical model of visual
processing. IT is divided into posterior (PIT) and anterior (AIT) areas
New Labels for Old Ideas: Predictive Processing and the...
519

This traditional or classical view might be caricatured as taking perception to be passive –
impingements from the world flow in through our sensory surfaces and form impressions on a
blankly receptive system. The idea would be that perception is driven by external stimuli, so
that while there may be all kinds of modulation, the content of perception is determined by
incoming impingements on our sensory surfaces.
Predictive theories, by contrast, are supposed to take perception to be an internally driven
active capacity. The general idea is illustrated in Fig. 2. In Clark’s words, “Perception …
involves ‘explaining away’ the driving (incoming) sensory signal by matching it with a
cascade of predictions …. the key property of hierarchical predictive processing models … is
that the brain is in the business of active, ongoing, input prediction and does not (even in the
early sensory case) merely react to external stimuli … What gets “explained away” or
Fig. 2 The brain (indicated with the black box) must perform perceptual inference about the environmental
causes (S) of its sensory input (I). Higher level models or hypotheses (H) about the possible cause are used to
generate top–down predictions (dark arrows) about the evolving input, which in turn explain away bottom–up
sensory signal (light arrows), leaving only the prediction error as bottom–up signal to be explained away.
Subsequent updating of H should further minimise prediction error. (Adapted from Fig. 1. Hohwy et al. 2008)
Cao R.
520

cancelled out is the error signal, which (in these models) is depicted as computed by dedicated
“error units.” These are linked to, but distinct from, the so-called representation units meant to
encode the causes of sensory inputs.” (Clark 2013, p187–8).
With the slogan “perception is prediction”, predictive theorists argue that the contents of
perception are internally generated rather than externally imposed. For example, “it is
important to recognize the radical nature of full-fledged PEM [prediction error
minimization-based theories], in which top-down signals are taken to constitute perceptual
content, rather than merely modulating the activity of bottom-up sensory processing.” (Seth
2017) Stated as shockingly as possible: perception is akin to hallucination – albeit hallucina-
tion that is sensitive to feedback.9
So much for the advertising. The question is, are predictive models of perception really as
radical and novel as they sound? How different are they from extant models that do not use the
“prediction” or “error minimization” terminology? And where exactly does that difference lie?
If the key difference is a difference in the “flow of information”, what notion of information is
employed, and with what empirical implications?
To allay any suspense, my goal here is to raise some doubts about just how different
predictive models really are from traditional ones, and whether they merit the excitement
evoked by their more radical interpretations in the philosophical and broader scientific
community.
The argument is as follows:
&
In their broad causal commitments, predictive and traditional models appeal
to essentially the same resources.
Just as predictive theories allow for – and indeed, require – bottom-up feedback from the
outside world, traditional views also allow for top-down contributions to perception, whether
from memory, context, or attention. Moreover, the idea of starting with perceptual priors and
then updating them on the basis of incoming information is compatible with both predictive
and traditional theories, as is a conception of vision as an essentially active process involving
exploration. As Rescorla (2016) has pointed out, the virtues of predictive theories seem
primarily to be the same as the virtues of a Bayesian theory, and we already know that non-
predictive theories can be Bayesian.10
&
That is, predictive theorists start with the same empirical data but make
different claims about the content or import of the very same neural signals.
To wit: “What is most distinctive about this … proposal (and where much of the break from
tradition really occurs) is that it depicts the forward flow of information as solely conveying
error, and the backward flow as solely conveying predictions.” (Clark BBS pp187–188).
9 Proponents of predictive processing theories really do say this! Here’s Anil Seth saying that perception is just
hallucination (https://www.ted.com/talks/anil_seth_how_your_brain_hallucinates_your_conscious_reality),
and in print, Seth (2017). See also Frith (2007) “our perceptions are fantasies that coincide with reality,”
Hohwy (2013) “conscious experience is like a fantasy or virtual reality constructed to keep sensory input at
bay,” and Grush (BBS 2004 p.393) with the precursor to some of this work, quoting Ramesh Jain (in italics for
emphasis): “perception is a controlled hallucination process.”
10 The Bayesian/non-Bayesian distinction cross-cuts any divide between predictive and traditional theories.
See Rescorla (2015, 2016).
New Labels for Old Ideas: Predictive Processing and the...
521

Or Hohwy, in the Fig. 2 caption above: “… hypotheses (H) about the possible
[environmental] cause are used to generate top–down predictions (dark arrows) about
the evolving input.”
&
These claims are most obviously understood as claims about the information
content of neural signals, usually cashed out in neuroscience as a statistical notion.
Now, neuroscientists talk about neural signals carrying information all the time. These
assignments of contents to neural signals are generally justified by appealing to
statistical notions of information or Shannon information.
&
However, appealing to statistical information measures doesn’t help to distin-
guish predictive models, because given the same physical facts, signals carry
the same correlational information in both predictive and non-predictive
frameworks.
&
So any difference between predictive and non-predictive models will have to be
located either in a richer semantic notion of information or meaning carried by
signals, or in the function of the signals – that is, in what they are for.
&
Unfortunately for those counting on a definitive answer as to what neural
signals really mean, however, Quine’s arguments about indeterminacy can be
adapted to show that when behavior (in this case, neural data) is the same,
there is no way to privilege the predictive theorist’s content assignment over a
non-predictive interpretation of the same signals. Similar issues will dog
attempts to assign a determinate function to these signals.
These last three bullet points constitute the core of the argument – they may not yet
seem plausible, but they will be argued for in detail in Section 2.
In Section 3 I’ll run through several examples, before concluding in Section 4 that
we can label the same model of neural activity “predictive” or not, but that is a choice
that is unconstrained (and unconstrainable) by empirical data without additional as-
sumptions about the coding scheme or implementation relation connecting neural
contents to neural signals. And so, the exciting differences attributed to the predictive
framework are not empirical differences in the models, but rather differences in either
implementation or general outlook.
2 Assessing Predictive Models of Information Processing
2.1 The Distinctive Claims of Predictive Models Involve the Information Content
of Neural Signals
There are many versions of predictive models; I will consider the three most obvious
interpretations of what it takes for a model to count as appealing to predictive
processing: mechanistic, syntactic, and semantic.
These are logically independent interpretations, but compatible, and predictive
processing proponents may find one or more of these appealing, and sometimes slide
Cao R.
522

between different interpretations within the same paper (although evidence for one is
not directly evidence for the others).
Most straightforwardly, predictive models could be interpreted as making mechanistic
claims. That is, talk of “predictions” and “errors” are merely flowery ways of describing
physically-characterized signals that are directly measurable, and there is a literal physical
cancellation of top-down and bottom-up signals, as in noise-cancelling headphones.11
If this is all that predictive theories come to, then they might indeed be easy to test
empirically – we simply have to look for sites where signals seem to cancel each other
out. If this is all that is meant, then I have no objections (although I’m not sure that any of
the leading theorists endorse this interpretation).
A second alternative casts predictive processing as a claim about the format or encoding of
semantic content. That is, it doesn’t posit different contents, but does posit a different encoding
scheme that maps contents to (properties of) physical vehicles. I will come back to this
interpretation in the discussion at the end of Sections 2.3 and 3.4.
The third interpretation casts predictive processing claims as being about the representa-
tional content of neural signals. That is, it is assumed that the key terms – model, inputs, errors,
and predictions – are informational or representational. That is, the neural signals being
invoked are about something: they serve as predictions about either external stimuli or internal
activity. They might even be metarepresentational, in the sense of being about how accurate
some other internal representation is. These assumptions are manifest in the language used,
where terms such as “hypothesis”, “guessing”, “representing information about … causes”
feature prominently. It is this semantic interpretation that I think is most common, especially
among philosophers, and therefore it is the one that I will focus on for the rest of the paper.
On a semantic interpretation of the predictive framework, the following features
form a common core:
1.
An internal model (or many, one at each level in a hierarchically organized system,
each of) which generates predictions of distal causes, or of more proximal inputs.
2.
Inputs to the system which are to be modeled, and from which the system can
learn.
3.
Some minimization process aimed at reducing errors, understood as the difference
between what the model predicts and what actually happens (i.e. what the actual
inputs of the system are).
11 If at the level of neurons all this talk of representations, predictions, and encodings is merely rhetorical
flourish, then we should understand the predictive theory as essentially mechanistic and not content-involving
at all. That being so, “predictive” would mean nothing more than the attempt to physically match bottom-up
input, through top-down channels, with the polarity reversed such that the vehicles literally cancel each other
out. The setup would be something like that found in noise-cancelling headphones. As with those headphones,
the circuit can be constructed in such a way as to minimize any remaining noise, and even to “learn” the
ambient sound profile so that performance improves over time.
At that level, the theories could be understood to be making very specific claims about neural vehicles,
understood as spikes or average firing rates, and neural units, or the cells themselves. The subtraction process
at this level then would not be a subtraction of contents, but rather a cancelling out of physical vehicles
themselves. Now the top down activity will be a set of spikes that are supposed to cancel out an incoming set
of spikes by some physical mechanism. Spikes cannot be negative (or the picture would be somewhat cleaner),
but we could imagine inhibitory sub-threshold inputs arriving at the same time and cell as excitatory ones, and
only an excess of excitatory inputs will then result in spikes that travel up the hierarchy. In contrast to the
informational or semantic-level claims that I criticize, this is a very specific empirical claim that could in fact
be verified once the relevant populations were identified.
New Labels for Old Ideas: Predictive Processing and the...
523

4.
Predictions generated in higher level areas are sent to lower level areas, while the
minimization process involves sending error signals from lower level areas to
higher level areas.
Distinct predictive models might give different structures for the internal
models, pick out different inputs of interest, or use different algorithms to
perform the relevant minimization. Newer versions such as Friston’s free-
energy based models come apart from older predictive coding models such as
Rao and Ballard’s (1999). But what all these models have in common is that
they posit a flow of information (4, above) that is supposed to differ in an
interesting way from that posited by traditional theories.12 It is then crucial to
get clear on what that notion of information is.
There are other ambiguities to note. What exactly is an internal model? Is
any non-stimulus-evoked signal a prediction, or only some of them, and how
are they to be distinguished? How, physically, are predictions and inputs
compared, and error signals generated? For example, do prediction signals
literally inhibit bottom-up signals? Or is the comparison of prediction and input
an abstract characterization that is compatible with many different physical
implementations resulting in a bottom-up signal to be labeled “error”? Either
way, which cells are involved, and where are they located?
Rather than discussing the rest of these issues in detail, I would only note
that they are all rooted in the same concern about connecting information-
processing models to empirically testable hypotheses. At the very least, we will
need a specification of the mapping from informational contents to measurable
neural activities, so as to connect the variables in the model to signals in the
brain. The mapping introduces an additional degree of freedom into any
assessment of the model; with different mappings, the same model could make
very different predictions about brain activity. This looseness in interpretation
gives rise to my main worry: without pre-registration of which mappings are
allowed, predictive models and traditional models alike could accommodate
whatever brain data is found.
In practice, plausibility places some constraints on the mappings, and histor-
ically productive heuristics underlie mappings that are taken seriously. Perhaps
the most relevant standard is some kind of non-gerrymandering constraint: that
the code should be systematic and consistent enough that its deliverances are
projectible (and the predictions that it makes validated on new data). The most
simple cases of these involve codes that are not dynamic, or only very
limitedly so. In the next section (2.2) I’ll survey the standard assumptions
about neural coding and the information carried in neural signals, before
offering an equivalence argument (2.3) for why these constraints cannot help
us to distinguish between predictive and non-predictive models.
12 “What is most distinctive about this … proposal (and where much of the break from tradition really occurs)
is that it depicts the forward flow of information as solely conveying error, and the backward flow as solely
conveying predictions.” (Clark BBS pp187–188).
Cao R.
524

2.2 Neural Coding
What gets us from neural signals to what those signals are “about”? Aboutness can be a
loaded notion in philosophical discussions of representation and meaning. But most
neuroscientists seem to be working with a much thinner notion of information content,
where we are merely attempting to find the mapping from patterns of neural activity to
the information that they carry, information that the system might reasonably said to be
exploiting. The relevant notion of information is a little harder to pin down, but it is
presumed to be entirely naturalistic, objective, and most often cashed out in statistical
or information-theoretic terms. Sometimes the use of the word slips into richer semantic
territory, but it is the thinner notion that theorists will appeal to when pushed to justify
their usage. Perhaps the most common assumption employed in this enterprise is that
neurons code for that to which they are selectively responsive.
So for example, if a single cell in V1 preferentially responds to a contrast edge of a
particular orientation in its receptive field, it will be said to represent that visual
stimulus feature. If an area of the brain is preferentially responsive to faces, then it
will be said to represent faces. Preferential response requires only that the probability of
a neural response given the stimulus is higher than the baseline activity of the cell: i.e.
that P(firing|stimulus) > P(firing).
This is an intuitive measure for the scientist who is trying to infer whether a cell or
brain region is responsive to a particular stimulus, and thus (the thinking goes) likely to
be involved in its downstream processing. But presumably we are interested in what
information neurons carry for the brain itself, rather than some external observer. As
Stanley (2013) puts, “Despite the visceral effect that recreating or reconstructing
elements of sensory inputs or motor outputs from neural activity has on us, the results
are ultimately less than satisfying unless we use this to tell us something about the
physiological plausibility of the assumed codes. One way to attack this problem is to
force the observer to take the role of the downstream neurons to which the population
projects. In other words, who reads this information, and how do they do it?”
To get at that question, we need a slightly different measure: whether
P(stimulus|firing) > P(stimulus).13 In other words, the extent to which a signal carries
information depends on what the downstream receivers make of it – what were the
prior probabilities of the stimulus as far as they were concerned, and how does the
incoming signal change those priors? In Shannon’s terms, the signal reduces the
uncertainty of the receiver about the state of the source, and it is Shannon’s and related
notions of statistical information that continue to dominate contemporary analyses of
neural coding.
Another relevant factor for determining mappings from neural activities to their
import or content is specifying some kind of systematic relationship between how
much firing (or the particular pattern of firing) and properties of the stimulus. So for
13 See Bialek et al. (1991) for an early exposition of this point. In addition, we presumably also care about not
just how the probabilities change, but to what extent that matters for those making use of the signal. These
functional notions of information go beyond the merely statistical, and make explicit what is only implicit in
the purely statistical approach. However, they are too involved to discuss in detail here – see Cao (2012), Shea
(2014, 2018), Shea et al. (2017), Mann (2018), and Fresco and Ginsburg (2018) for more philosophical
perspectives on this issue.
New Labels for Old Ideas: Predictive Processing and the...
525

example, it is often presumed that more firing (than baseline) means a stronger stimulus
is present; less firing (than baseline) means that a weaker or opposing stimulus is
present, and that no (change in) firing means that nothing is happening. The general
idea is intuitive, but difficult to pin down precisely, since it’s not always clear more of
what (especially since more of one thing can be the same as less of something else, both
of which we can describe the neuron as being responsive to).
One common way of pinning things down in a particular case is to find an
apparently natural class of stimuli which does drive increased activity in the relevant
area, and then assume that that area codes for that stimulus class. As we’ll see, this
approach has some drawbacks if we want a determinate and unique identification of
what is being coded. In addition to disagreement about what counts as “natural”, we
might find variation in a signal to be driven by multiple distinct causal sources.
2.3 The Informational Equivalence of Traditional and Predictive Models
Recall that we are attempting to find empirical differences between claims like these:
Traditional model: Bottom-up signals14 carry sensory information about their
(distal) causes.
Predictive model: Bottom-up signals are error signals, conveying information
about the difference between predictions and actual input.
As we’ve just seen in Section 2.2, the standard way of assigning informational
contents to neural signals in order to get an estimate of the distal stimulus requires a
previously well-defined set of possible states for the signal to narrow down. But that
means that there is no way to decide between the two kinds of models, because in
informational terms the two are identical.15 Because the receiver’s prior on the distal
stimulus in a traditional model is essentially the same as the “prediction” in the
predictive model, the difference between P(stimulus|signal) and P(stimulus) will be
the core informational component of the upward signal in the traditional model as well
as in the predictive model.
The point is that given the prior, sending error and sending stimulus have the same
effects from the point of view of the system. And priors are, by definition, given – the
system has to have them in order for the central notion of uncertainty reduction in
information theory to make sense. Insofar as it makes sense to apply the framework at
all (i.e. to say that there are signals in the system playing the role of priors (or
posteriors) about the probability of a stimulus being present, as well as signals in the
system playing the role of incoming evidence, and representations of a stimulus), then
what one person calls the error signal carries the exact same information (in the sense of
reducing uncertainty about the signal source) as what someone else might call the
bottom-up input.
14 That is, signals propagating (relatively) directly from the sensory periphery. See footnote 7 for the
motivation for this gloss on “bottom-up”.
15 Relative to a particular convention that specifies the content of a signal independently of how much it
reduces a receiver’s uncertainty, the two may be distinguished (as in digital signal processing where engineers
stipulate the conventional encodings). However, no such independently stipulated convention exists for neural
signals, so we are left only with the appeal to what effect the signal has on a receiver.
Cao R.
526

In short, for predictive theorists to call that upward signal “error” seems to be just a
different way of describing the very same pattern of information flow as that ascribed to
the system by traditional models. Given these facts, the predictive and traditional
models seem to provide equivalent descriptions of the same informational facts. This
means that despite appearances they are not truly in competition with each other.
Several others have noted the equivalence of some prominent exemplars of predic-
tive models to existing traditional models (including hierarchical Bayesian models), but
it is somewhat surprising that this fact has not received more attention.16
One place this equivalence has been noted is in Spratling’s analysis of the biased
competition model of attention: “A simple variation of the standard biased competition
model is shown, via some trivial mathematical manipulations, to be identical to
predictive coding. Specifically, it is shown that a particular implementation of the
biased competition model, in which nodes compete via inhibition that targets the inputs
to a cortical region, is mathematically equivalent to the linear predictive coding model.
This observation demonstrates that these two important and influential rival theories of
cortical function are minor variations on the same underlying mathematical
model.”(Spratling 2008b).
Elsewhere, Summerfield and de Lange (2014), citing Bitzer et al. (2014), write: “At
first glance, predictive coding and sequential sampling models take quite different
approaches to understanding perceptual choices. Rather than a feedforward readout and
linear integration of noisy sensory information, predictive coding proposes a reciprocal
exchange of top-down and bottom-up information — ‘explaining away’ sensory inputs
… However, it is important to emphasize that under some assumptions, predictive
coding and evidence accumulation can be shown to be formally equivalent.”
In fact, the specific equivalence of these two interpretations of the bottom-up signal
is eventually acknowledged in Clark’s Embodying Prediction (2015). “Prediction error
signals are thus richly informative, and as such (I would argue) not radically different to
sensory information itself. This is unsurprising, since mathematically (as Karl Friston
[personal communication] has pointed out) sensory information and prediction error are
informationally identical, except that the latter are centred on the predictions. To see
this, reflect on the fact that prediction error is just the original information minus the
prediction. It follows that the original information is given by the prediction error plus
the prediction. Prediction error is simply error relative to some specific prediction and
as such it flags the sensory information that is as yet unexplained. The forward flow of
prediction error thus constitutes a forward flow of sensory information relative to
specific predictions.” Clark doesn’t say much beyond this, but on a natural interpreta-
tion of what he means by "centred on", it seems that his response to the challenge of
informational identity is to advert to a difference in content. A difference in content
with no change in the statistical information would have to depend on some other
factor, perhaps most likely a difference in the function of that information for the
system. But that just pushes the question back – how is the function of the signal
supposed to be ascertained? For further discussion see Sections 2.4 and Section 4.
How does this equivalence affect how we understand empirical claims by predictive
theorists? One point to note is that any predictive model that is supposed to predict or
16 See for example Spratling (2008a) and Issa et al. (2018). Everywhere in this paper, “traditional” means only
that signals are not interpreted as “predictions” or “errors”, not that there are no top-down signals.
New Labels for Old Ideas: Predictive Processing and the...
527

explain electrophysiological data (or some other measurement of brain activity) will
depend on further claims about which cells, layers, or regions in particular are involved
in the transmission of the relevant signals. Another point is that once we’ve fixed the
relevant entities and their locations, we also have to fix on how to interpret their
activities and signals in informational terms. The heuristics discussed in Section 2.2
provide only loose constraints – a great deal of freedom remains in how we interpret
any given spike train or pattern of spike trains from a particular population of cells.
Given significant variation in what these further claims are, there ends up being
substantial variation within the category of “predictive models”, variation that can end
up obscuring the putative contribution of the predictive component of the model. In
other words, a predictive model and a traditional model of the same area or phenom-
enon may well be empirically different, but not because one of them invokes prediction
and the other does not. Rather, they differ in the auxiliary assumptions made about
which cells are involved in the very same informational process, and perhaps how that
informational process is implemented at a neurophysiological level.
Let me briefly illustrate these points with an example. Clark tells us that “a common
conjecture in this literature depicts superficial pyramidal cells (a prime source of
forward neuro-anatomical connections) as playing the role of error units, passing
prediction error forward, while deep pyramidal cells play the role of representation
units, passing predictions (made on the basis of a complex generative model) down-
ward (see, e.g., Friston 2005, 2009; Mumford 1992).” (BBS p.188).
For the experimentalist interested in finding evidence for or against the predictive
theory, the first thing to fix is what mapping we intend to use to go from components of
the model to measurable physical features in the brain. Given this model-to-brain
mapping, we can then ask whether superficial pyramidal cells passing prediction error
forward look any different physically from superficial pyramidal cells passing sensory
information forward about their causes, as proposed by traditional models.
If the informational equivalence of traditional and predictive models holds, then,
relative to a given mapping from informational states and processes to brain states and
processes, there can be no empirical differences between them. So the answer to
Clark’s experimentalist must be “No”. Apparent differences will be due only to
differences in the model-to-brain mappings presumed by different modelers.
Now, what if that is just the difference being claimed? The predictive theorist might
agree that bottom-up signals have the same Shannon-information content on either a
predictive or a non-predictive framework, but still insist that they are still playing a
syntactically different role.
My suspicion is that this is not a distinction that is actually available when we are
talking about neural signals. What is syntactic in the signal is just that feature of the
signal that is responsible for its being typed to play some role (either functional or
semantic) in system. That typing is conventional, and makes sense for human and
artificial languages, but is much less clearly applicable when it comes to neural signals.
In order to specify a mapping between neural signals and what information they
carry that is independent of the priors held by downstream receivers of the signal, we
would have to stipulate some independent distribution over which the signals would
reduce uncertainty. But what distribution is that? It is certainly not explicitly specified
in a standard or consistent way by predictive theorists.
Cao R.
528

Instead, our best attempts to construct or discern some sort of typing for neural
signals depends on their informational content, so there is no obvious independent
syntactic feature of neural firing to appeal to that is not the same as their Shannon
information content for a receiver of the signal.
It’s an interesting question whether the real difference between predictive and
traditional modelers might be found in how they choose “plausible” model-to-brain
mappings, and in Section 3 I’ll discuss strategies for finding empirical support for
predictive models in more detail. But first I want to draw an analogy to a well-known
philosophical example that I think will help illuminate the situation here.
2.4 A Brief Quinean Interlude
In Word and Object, Quine gave a famous argument for the indeterminacy of linguistic
meaning. He argued that there were no determinate facts about the meanings of words
beyond their observable effects on behavior. Many philosophers are reluctant to accept
the thesis of the indeterminacy in its original linguistic context, where Quine’s argu-
ments seemed to rest on a behaviorism belied by our own experiences. Those who
don’t buy his thesis about the indeterminacy of linguistic reference can accept that
behavior underdetermines interpretation, but then appeal to non-behavioral facts such
as the intentions of the speaker (which we expect to be both determinate and episte-
mically accessible) to resolve facts about meaning. It might then be possible to rescue
the determinacy of meaning of linguistic expressions, if speaker intentions correspond
to in-principle accessible facts literally inside the head.
But Quine's problem recurs inside the head. What fixes the content or meaning of a
signal in the brain? Now the appeal to non-behavioral facts no longer seems available.
Functionalist facts about the causal roles of the signals are in the end no more than further
behavioral facts about the parts of the system. If the subpersonal users of signals in the
brain have intentions about what their signals mean, they are not intentions that are
scrutable to us (and that’s if the notion of subpersonal signalers in the brain is helpful in
the first place).
Neuroscientists attempting to decipher the neural code find themselves in a position
much like Quine’s field linguist – trying to discover mappings between signals and
their meanings with no access to anything but the stimulus that evoked the signal and
the response the signal elicits.
Quine tells us that “stimulus meaning, by whatever name, may be properly looked upon
still as the objective reality that the linguist has to probe when he undertakes radical
translation. For the stimulus meaning of an occasion sentence is by definition the native’s
total battery of present dispositions to be prompted to assent to or to dissent from the
sentence; and these dispositions are just what the linguist has to sample and estimate.”17
By analogy, we might say that knowing the set of causes is the “objective reality”
that the scientist has to probe when he undertakes to decode neural signals. Assent is
taken to be activation, and dissent is failure to respond. But whereas Quine’s linguist “is
much influenced by his natural expectation that any people in rabbit country would
have some brief expression that could in the long run be best translated simply as
17 Quine (1960) p.35 Ch 2 Word and Object
New Labels for Old Ideas: Predictive Processing and the...
529

‘Rabbit’,” our scientist cannot so obviously rely on anything of the sort. What kind of
“natural expectation” could we reasonably have with respect to neural signals? What
justifies the expectation in the native case is something like similarity of life or make-up
with ourselves. Why expect such similarity to be preserved when we go to subpersonal
systems? What are the brain’s primitives, and why should we expect them to be
anything like our linguistic or conceptual primitives?
We should be more worried about this if it turned out that something like determinate
meaning were required to make sense of neural activity. But arguably all that we really need
by way of neural meaning is that the nervous system’s primitives are sufficiently discrimi-
nating in combination with each other that their concerted activities can produce the thoughts
and behavior that we in fact enjoy. And all that that requires in turn is that they carry enough
natural information or Shannon information to get the job done, where those can be cashed out
in turn in terms of causal specificity. So while we can’t get more than this (if Quine is right),
we also don’t need more than this to make sense of neural-level information-processing.
Notice, however, that this Quinean indeterminacy does prevent us from ever deciding
between predictive and traditional models, at least without additional (non-empirical)
stipulations. Imagine the field linguist “realizing”: I was wrong to suppose that the
natives meant ‘Lo, a rabbit’ – I can predict their future behavior and explain their past
behavior by supposing that they in fact mean ‘Lo, an undetached-rabbit-part’! As Quine
made clear, no empirical facts prevent him from saying this; moreover, what he says is
genuinely and informatively (informationally) determined by correlational facts be-
tween the utterance and the state of the world; nonetheless, this particular interpretation
is arbitrary. Similarly, the choice of a predictive interpretation of neural data over a
traditional one is not – and cannot be – forced by the neural data alone.
More generally, if we look at correlational or statistical information alone, we will
find a huge number of correlations between events (or event-types) in the head and
events in the world, all of which have claim to being the informational content of a
neural signal. These might then be narrowed down by figuring out which features are
ecologically relevant for the creature, or play some causal role in producing its behavior.
There will still be a number of valid interpretations, forming an equivalence class with
respect to these constraints. My claim is that both predictive and traditional theories of
perceptual processing fall into this equivalence class, depending on whether we take the
upward signal to be carrying error information or stimulus information, with both
interpretations equally valid relative to these constraints.
3 Looking for Predictive Processing in the Brain
Now to apply these lessons to what we find in the brain.
On the anatomical side, proponents of predictive processing theories like to cite facts
about cortical connectivity – in particular the large number of top-down anatomical
connections.18 One way for predictive models to explain patterns of cortical connec-
tivity is by showing what the information-trafficking demands of the system are,
positing a physical means of information transmission, and then predicting how the
18 Clark (2013)
Cao R.
530

system should look. In much the same way, we might explain the locations and
capacities of physical highways by appealing to the volume of trade we expect between
different population centers. Then, in order for predictive processing to be a good
explanation of the large number of top-down or recurrent connections in the brain, it
has to be that predictive models require more top-down signals than traditional models.
On the functional side, proponents might look at some interesting perceptual or
neurophysiological phenomenon, and present a predictive model that can account for
that phenomenon. Rao and Ballard’s model was an early example, and some work on
binocular rivalry favored by Friston’s group also falls into this category.19
More directly, they might posit particular activity patterns as exemplifying predic-
tive processing, and then present fMRI or neurophysiological examples of those
patterns. Some common examples involve some kind of anticipatory activity (where
the cells that have responded to a stimulus in the past become activated even in the
absence of the stimulus in some repetitive stimulus paradigm), or some kind of extra
activity in response to novelty or the breaking of a pattern. Or there might be a change
in selectivity for some cell or some population of cells – where its responses change
over a short period of time, even though the stimulus has not, inviting the interpretation
that the responses correspond to some changing internal state, rather than the unchang-
ing external stimulus.20
However, while findings along these lines are suggestive, I am skeptical of the
empirical support that they provide to predictive models over traditional ones. First,
traditional accounts invoking attentional effects, entrainment, or adaptation can account
for the same data. Second, and perhaps relatedly, the predictive models have in several
cases been shown to be equivalent to non-predictive ones.
This is sometimes noted by the authors of the models themselves. For example, Issa
et al. (2018) present an error-coding model that successfully predicts patterns of neural
activity evoked by face-part stimuli in macaque face recognition areas. Interestingly
however, in an earlier version of their presentation, they also point out that the
“predictive” model they propose is computationally identical to a certain kind of
classical model21; only the “implementation” differs, so that error units are short range
in error backpropagation, while state units are short range in “predictive coding”. This
is a genuine difference, but one that makes explicit that the difference lies in how the
physical implementation of the model is specified. Moreover, among the hierarchical
“state-and-error” coding models that they consider, one predictive and one non-
predictive model account equally well for the neural data. Notice that this is exactly
the kind of Quinean indeterminacy described earlier: the very same units – with the
very same anatomical connections and causal role in the system – can be described
either as error units (in which case the model is “predictive”) or as state units – in which
case the model is classical. In either case, the model makes the same empirical
predictions.
These two points are related, and follow directly from the preceding discussion
about indeterminacy and informational equivalence. For further illustration, I’ll
19 Weilnhammer et al. (2017), Hohwy et al. (2008)
20 Issa et al. (2018)
21 The nomenclature is not standardized, so “classical” here does not mean feedforward-only. A feedforward-
only model would be genuinely different.
New Labels for Old Ideas: Predictive Processing and the...
531

examine in some detail the phenomenon of adaptation, and then the Rao and Ballard
model, before drawing some general lessons.
3.1 Adaptation
Adaptation, or the diminishing of responses to a stimulus over time, is one of the best-
characterized features neural responses. It is found at both the single cell (including
single neurons isolated from top-down influences) and population level, as well as in
indirect markers of neural activity, such as the fMRI BOLD signal.22 Adaptation and
repetition suppression effects are often claimed to be evidence in favor of predictive
coding theories, because they provide evidence that neurons preferentially respond to
novel signals, or changes in incoming signals. Because repetition suppression seems to
be a more complex phenomenon that is sensitive to context, including semantic and
task context, I will focus just on the simple case of neural adaptation here.
Lewis and Kristan provide an example of adaptation involving single neurons
(shown in Fig. 3) in a simple organism over very short periods of time. They follow
the standard tradition in interpreting those signals: what the pattern of neural activity
encodes is given by the most salient stimulus feature that they seem to the observer to
be selectively responsive to: T cells encode the velocity of a stimulus, while P cells
encode the magnitude.
So far this is not controversial, but for the purposes of distinguishing predictive and
traditional coding, we might then ask whether the T cell is sending an error signal,
indicating the difference between what the system was “expecting” over an ~10 ms
window (no change in stimulus) and what actually happened – namely an inward
deformation of a certain velocity at stimulus onset and an outward deformation at
stimulus offset. We can also ask whether the adaptation in the response of the P cell is a
case of fatigue, or rather expectation or priors influencing its response - or more to the
point, whether there is any real difference between these “alternative”
characterizations.23
Looking next at the T cell responses, we see that information about both stimulus
magnitude and onset/offset velocity is available in the spike train. How are we to rule in
favor of one rather than the other? Looking back at the P-cell response, we see a
decrease in the response over the stimulus period as well. With nothing else to go on,
multiple interpretations are available. Are we seeing “mere” adaptation, where the
spike-generating apparatus is becoming fatigued as resources are depleted over the
course of a spike train? Or rather a kind of efficient coding, where fewer spikes are now
necessary to produce the downstream effects allowing the sensory neuron to fulfill its
function in guiding behavior? If the earlier analysis of the information content of the
22 fMRI is only an indirect marker of neural activity because it measures changes in blood oxygenation level,
which is a time-lagged, spatially distorted metabolic concomitant of neural activity. The exact nature of its
relationship to action potential firing (vs. sub-threshold neural activity, for example) has also not been fully
characterized. Nonetheless, we often see similar patterns of activity in BOLD that we see in direct neural
recordings and other means of visualizing neural activity.
23 I’m tempted to say that if positing expectation or priors is helpful in explaining the system, then we can
reasonably say that the fatiguing process has been co-opted to serve this functional role. That is, the priors are
encoded in the disposition of the cell to fire less over time, and so fatigue - understood as depletion of
whatever physical resources the cell requires to fire - may be a mechanism by which the system’s priors come
to influence the cell’s response.
Cao R.
532

signal was right, then these questions are not posing contrasting options. Rather, we are
equally justified in giving either answer, or both, because they are saying the same
thing in different words.
Notice that there are at least two moving parts – both of which are open to
interpretation. One question is: what stimulus feature is the signal supposed to be
tracking (i.e. not changing with respect to)? If the signal is supposed to be tracking
velocity, then a constant velocity is no change, whereas if the signal is tracking
displacement, then a constant velocity corresponds to a change in displacement. This
is a question about the function of the signal in the system.
A different question is: what counts as a change in the signal itself? Is it a physical
change, or a change in its import downstream? Is the diminution of response over time
just a practical limitation (the system would work better if it were not present, or
perhaps has a way of compensating for it), or is it functionally important for the system,
conveying the fact that the stimulus is not changing, reflected in the fact that it is now
expected?24 That is a question about coding – about how we map physical features of a
signal to its informational import downstream. The answer to one will constrain the
other, but multiple packages will fit not just this particular data but all possible data.
One interpretation is that the code is static – we should always see the same physical
neural signal corresponding to the same representational target. Then any change in
spike rate must correspond to a change in the information content. On this interpreta-
tion, the T cell cannot be tracking the stimulus, but must be encoding some more
complex fact which is only partially determined by the stimulus. We might speculate
that one candidate for this complex fact is “expectation”, or “prediction”.
24 How long the stimulus has been on is inevitably confounded with how expected it is. Experimenters have
attempted to manipulate expectations independently (e.g. by giving some other cue), but still the effectiveness
of the cue is tied to past experience with the cue, and so produces some other confound. (See Summerfeld &
deLange 2014 for a review of some of these attempts).
Fig. 3 T cells have a low threshold and are most sensitive to the velocity of indentation of the body wall by
mechanical stimulation whereas P cells have a higher threshold and respond best to changes in the magnitude
of body wall indentation (Carlton and McVean 1995). [Thus] the T cells are thought to encode the velocity of
a moving mechanical stimulus, whereas the P cells are more likely to encode the magnitude of the stimulus.
(From Fig. 1, Lewis and Kristan Jr. 1998)
New Labels for Old Ideas: Predictive Processing and the...
533

On a different interpretation, the code is dynamic, so that a spike or pattern of spikes may
convey something different, depending on context. On this second interpretation, the T cell
may well be encoding the unchanging stimulus, even as it stops spiking 250 ms after the
stimulus onset. Given the changing relationship between spike rate and stimulus, we can
locate the change either in a change in “prediction”, or a change in coding. Notice also that a
stable code for error corresponds to a dynamic code for the stimulus, and a stable code for the
stimulus can be interpreted as a dynamic code for the error. Unless we have some independent
reason to suppose that the coding is static, we have no reason to pick the predictive
interpretation over the other.25
In other words, these cellular recordings seem to be as much (as little) evidence in favor of
predictive processing as they are evidence in favor of a dynamic code (one where the
encoding relationship between physical signal and evoking stimulus changes over time) with
no mention of prediction. The set of constraints considered thus far can only deliver contents
that are indeterminate between a predictive interpretation of neural signals and a more
traditional interpretation.
One might object that these single neuron examples are too simple. Perhaps their activities
(or dispositions to fire more or less in response to a given set of external conditions) don’t
deserve to be called priors or expectations at all in the absence of a top-down source for them.
Instead, perhaps we should focus on the kind of higher level repetition suppression effects that
Clark takes to be strong evidence for predictive processing.
Clark is particularly impressed by Summerfield and Egner (2009) finding that unexpected
changes produce greater responses than expected changes: “Repetition suppression thus
emerges as a direct effect of predictive processing in the brain, and as such its severity may
be expected to vary according to our local perceptual expectations.”26
However, the same considerations about indeterminacy and the exact coding relation
in the interpretation of adapted responses also apply in broad outline to cases of
repetition suppression, although there are further complications that would need to be
addressed to make the full case.
I’ll just note briefly that what made the changes “expected” in the repetition suppression
experiments is that they were indicated by another cue. If the cues are perceptually grouped,
then this consequence could also be explained by adaptation to the now-combined stimulus.
(We can see the earlier concern about what counts as a “natural” stimulus class or stimulus
characterization re-arising here). The results seem convincing because it is quite intuitive that
some of the stimuli are expected (that is, if we were the subjects in the paradigm, we would
experience expecting them). But it seems to me that our expectation is not obviously the same
as a “prediction” in the part of the visual system being investigated, and in fact, is quite likely
to be different. (This is the point from the Quinean interlude). Furthermore, as with the other
experiments in this area, saliency and attention are confounds – stimuli that are not-like-the-
others have a tendency to capture attention, and attention can boost neural responses.27
Moreover, as we’ll see next, similar indeterminacy issues arise when we look at
more complex models.
26 p.44, Clark (2016)
27 Moore and Zirnsak (2017). Of course, not everyone agrees with my diagnosis; see Summerfield et al.
(2014) for further discussion.
Cao R.
534

3.2 The Rao and Ballard Model
One of the earliest instances of a predictive coding model in theoretical neuroscience
came from Rao and Ballard (1999), and it is perhaps the most widely cited and most
fully-fleshed out predictive models to date. [Anyone who is not interested in the
technical details can skip this section]. It was constructed to predict (explain) V1 unit
responses, and then compared to actual neural responses for consistency. I’ll discuss
this model in some detail, because the predictive processing models that have excited
attention more recently share the same basic idea, but with variations (differences in
lateral connections within levels) and elaborations (presented not merely as models of
early sensory coding, but rather as a generalized story about perception and other
cognitive capacities including the control of action).
In the R&B model shown in Fig. 4, a succession of hierarchically organized
“predictive estimator” boxes generate “predictions”, which are then sent down the
hierarchy. The external input to the visual system, in this case an image, I, enters at the
first box. Within the predictive estimator boxes, there is a subtraction process – one that
compares the top-down signal received from the box at the next level up (rtd) with its
own r signal, and sends the difference back up the hierarchy as an error signal (r - rtd).
Taking a flat-footed interpretation of the system, let’s call r the system’s perceptual
representation. What is supposed to be novel here is that the r units send signals
downward (to the PE box on the next level down), and laterally (to the comparator).
Unlike in the traditional feedforward model, r units do not themselves send signals up
the hierarchy. What signals do go up the hierarchy are the “feedforward error signals”
(r minus rtd) generated by the comparator.
The representations, errors, and inputs in Rao and Ballard’s model are vectors, the
“weights” are matrices, and the model tells us how the quantities in these abstract
objects are calculated. Thus, although these theories specify particular computations as
the ones being performed by a perceptual system, the models by themselves are not
committed to any particular physical neural architecture, or particular patterns of firing
by particular cell populations, characterized physiologically. Rather, (and crucially for
my argument) they are committed to claims about information-processing – claims
about functional units and what they encode, compute, and signal to each other. (And
of course the actual implementation of the R&B model that produced their results was
on a computer).
Compare this to the traditional hierarchy of increasingly sophisticated feature
detectors, where the focus is on the forward flow of information. There too we might
have an input image I. The bottom-up signal driven by the input stimulus is then
filtered sequentially in a bottom-up fashion. (A characteristic example of this kind of
bottom-up model can be found in Fig. 1 where all the arrows in the diagram point up).
Near the sensory periphery, the filters are simple feature detectors, sensitive to low-
level and perspectival features of the environment – for example, whether an edge of a
certain orientation is within the detector’s receptive field. At each level, again we have
representations r, this time of these features. As we move up the hierarchy, the features
that are represented become more and more sophisticated, culminating as viewpoint-
invariant representations of distal manipulable objects in a spatially articulated envi-
ronment. Layered over this bottom-up backbone are sources of top-down modulation,
for example from attention, task, adaptation, and so forth (not shown in the figure).
New Labels for Old Ideas: Predictive Processing and the...
535

These feedback processes can help to disambiguate noisy inputs on the basis of context.
There can also be error processing, although this is not mentioned explicitly, and it is
often presumed to be coming not from the visual areas themselves, but rather from
other areas of the brain specialized for error processing (and not modality-specific).
3.3 Relabeling the Model
Now think about where r gets its information from in the R&B model. If we follow the
input arrows to r, we can see there are actually two sources: bottom-up “error signal”
input, and top-down “prediction”. But where did that top-down prediction come from?
Answer: even higher up the hierarchy. Here, again, is where the predictive model seems
counterintuitive. In the traditional case, it was completely clear that the bottom-up
representations had a source – the image or visual stimulus. But in the new story, the
further in we go, the farther we get from causal sources of information about the world.
But then how could the top down priors ever be more helpful in getting the r units to
accurately represent the world, when their input in turn comes from even deeper in the
cave?
Fig. 4 Rao and Ballard’s hierarchical predictive coding model of the visual cortex. (a) General architecture of
the hierarchical predictive coding model. Higher level units attempt to predict the responses of units in the next
lower level via feedback connections. Lower level units sent back the error between the higher level
predictions and the actual activity through feedforward connections. This residual error signal is then used
by the predictive estimator (PE) at each level to correct the higher level estimations of input signal. (b)
Components of a PE unit. Each unit consists of several kinds of neurons: feedforward neurons encoding the
synaptic weights UT, predictor-estimator neurons maintaining the current estimate r of the input signal,
feedback neurons encoding U and carrying the prediction f(Ur) to lower level, and error-detecting neurons
computing the discrepancy (r −rtd) between the current prediction r and its top-down prediction rtd from a yet
higher level. (Reprinted with permission from Rao and Ballard 1999)
Cao R.
536

Clark and Hohwy’s expositions sometimes make it sound as if the predictive
architecture requires that representation units receive all their input from higher areas.
That would create a regress – if at every level the activity of the representation units is
directed from above, where is the top level? But in fact, there is no mystery. Everyone
(including Clark and Hohwy) agrees that the predictive system’s priors ultimately
derive from a history of peripheral input.
The r units are ultimately driven by bottom-up input just as in the traditional model.
It’s just that that bottom-up input is labeled prediction error, and the r units also receive
top-down input, which are labeled as “predictions” or “priors”, but would in a
traditional model have been labeled “modulation”. (In a sophisticated Bayesian version
of the traditional view, that modulation could be the manifestation of expectations or
priors as well).
The prior might be quite simple – its exact form doesn’t matter, so long the prior
properly gets updated from the incoming error signals. In fact, one could start with a
completely blank prior, and by “error correction”, eventually converge on a represen-
tation that produces zero error signals when compared with the bottom-up signal. That
is, in the model, all the information informing the representations ultimately has to have
come from incoming signals – just via a more circuitous route, and perhaps integrated
over a longer period of time than was traditionally assumed.28
Suppose we initialize the Rao and Ballard model from a state where the rs at every
level are zeroed out, as well as the initial error. That is, no spikes are being sent. Then
we present the system with an image I. Then, since the estimate of input was 0, the
error sent up from the lowest level will be I (weighted by U1T) and since all of the r’s
are zero, the error will continue up the hierarchy as I*(U1T * U2T* … * UnT), updating
the estimates r as it goes. If the image is static, then once the error signal I reaches (say)
level n, the (n-1)th level r units will get updated by their top-down signal to predict I (or
more precisely, the patch of I that is within their receptive fields, increasing in size as
we go up the hierarchy).
So when the priors make no contribution, the numbers can be the same for either a
predictive or a feedforward model.29 I think in fact even when the numbers are
different, the information content of a predictive and traditional model will be the
same. This is because (Shannon) information reduces uncertainty for the receiving
system. Since the system in some sense “knows” (has access to, operates on) its own
priors/prediction, it can derive the stimulus input from the error, and vice versa. To
distinguish between these possibilities requires further constraints on how exactly the
signal is being processed, constraints which are not provided in the models.
Suppose we just looked at the fully-trained R&B model, with the labels taken away.
I contend that such a model gives us no extra reason to call the bottom-up signal “error”
28 In the actual brain, perhaps some of the priors are innate dispositions to certain patterns of activity. In
principle priors could be built into a Rao & Ballard style model as well – but that would undermine the part of
the explanatory success came from their not needing to build anything in – instead, units that exhibited activity
patterns similar to those of V1 cells just “fell out” from their optimization process.
29 There is a worry here (raised by Reviewer 2) that the weights are learned parameters of the model – and a
feedforward model would not necessarily learn the same parameters. This seems plausible, but I am not here
concerned with how the model is initially constructed or trained, but rather its responses to stimuli once
trained (and once all the weights have been set). That is, I am not taking the model to be a model of how the
brain learns, but rather how it infers or responds on the time scale of perception. So once the weights are fixed,
we can interpret the numbers in the very same model as either error signals or stimulus information signals.
New Labels for Old Ideas: Predictive Processing and the...
537

rather than “information about the stimulus”. Any difference between the two amounts
to an encoding difference of the very same information. And what I mean by encoding
difference is that there are different conventions (analogous to file formats) for how the
very same information is to be conveyed. (Please see Fig. 5
Another way of putting the same point is that the very same mathematical model can
have its various variables given labels that are drawn either from a traditional vocab-
ulary, or one from the predictive coding vocabulary. These labels have distinct
connotations, but do not change the predictions made by the model about information
flow, once the relationship between the labels and physical entities and causal structure
is fixed.
What about the operation being performed by the comparators? The original labels
suggest that what is happening there is inhibition – literal suppression of the physical
signals by signals of the opposite sign. But that isn’t mandatory – the mapping between
activity of error units to the errors they encode, and between the activity of represen-
tation units and what they encode is, again, not constrained by the computational model
itself. The model claims only that a comparison and subtraction of the signals’ contents
is being done somehow, and expresses no explicit commitment to how that process
manifests in numbers of spikes (subtracted or otherwise).
So once again, at an informational level – if indeed the theories are intended as
theories of the structure of information processing in the brain – there is no clear
difference between the model cast as predictive or cast as traditional. And without an
Fig. 5 Relabeling the model. Top: Rao and Ballard’s original labels. Bottom: Redescribing the information
content of the signals without appeals to prediction or error
Cao R.
538

informational difference (which, remember, Clark and others point to as the key difference
between traditional and predictive theories), it’s not clear what substantive difference there can
be.30
Recall that the evidence cited in favor of predictive theories over traditional ones depends
on there being some fact of the matter about whether signals indicate prediction error vs.
stimulus information. However, relative to the standard notion of informational content in this
area, there is no difference between the two – both reduce uncertainty about the stimulus and
the prediction error equally, and both can drive learning or updating of some internal
perceptual state.
However the signals are labeled semantically, they play the same causal role in the system,
and thus both sets of labelings (the predictive and the traditional) can be made to generate the
same predictions about observable measures of neural activity in all cases.31 What particular
predictions they make about neural activity will depend on what mapping the theorists assume
between the informational contents of the signals and the physical manifestation of those
signals.
It is true that different mappings will result in different predictions about what patterns of
neural activity will be observed. It is also true that different models might make different
assumptions about where the various components of the model are located (e.g. in which
cortical layer we might expect to find the comparators, vs. the producers of feedforward
signals, vs. the producers of top-down signals). However, those extra assumptions, ones that
might generate different empirical predictions from each other, are not strictly speaking part of
the computational (or algorithmic) level model being put forward. Rather, they are further
hypotheses about the physiological of the model in the brain. Particular assignments of
particular cell-types or anatomical connections to certain roles in the model are not proprietary
to the predictive model or the traditional one, but again, can be compatible with either. Any
differences in empirical predictions between traditional and predictive models will ultimately
comedown totheimplementational hypotheses, which are in no waytheoreticallydetermined
by whether the model is traditional or predictive.32
3.4 Assessing the Putative Empirical Evidence for Predictive Theories
Now let's return to the question of what predictive processing is supposed to look like
in the brain. Recall that empirically-based arguments offered in favor of predictive
processing tend to rely on some combination of the following three broad strategies.
30 Again (with apologies for the repetition…) “What is most distinctive about this … proposal (and where
much of the break from tradition really occurs) is that it depicts the forward flow of information as solely
conveying error, and the backward flow as solely conveying predictions.” (Clark BBS pp187–188). But once
we set the (content) labels aside, the two models are identical (i.e. not committed to any physical or causal
differences). Different people applying the model to different systems might want to posit different imple-
mentation relations, or different ways of picking out the relevant causal structure, but these relations are not
going to be part of the model itself.
31 The labels can be thought of as intentional glosses; see Egan (2014).
32 So it might just happen that those who prefer the predictive terminology also prefer one particular
implementation while those who prefer the traditional terminology prefer another, different, implementation.
But these implementation hypotheses are driven by considerations other than whether perception is predictive
or not (such as, for example, what the connectivity patterns of cells in layer V vs. layer VI of cortex are).
New Labels for Old Ideas: Predictive Processing and the...
539

3.4.1 Strategy 1: Show how some data can be accounted for on the basis
of a predictive model
This is what Rao and Ballard did, but this type of strategy only favors predictive models
if traditional models are less able to account for the same data. There are no clear cases
for this. (As we saw earlier, it’s not the case for the response properties of V1 cells that
R&B were concerned with).33
3.4.2 Strategy 2: Posit some anatomical or activity-based signature of predictive
processing, and then present examples of those signatures in brain data
The difference between this and the first strategy is only in the apparent order in which
they take things – in the first case, the data comes first, and begs for explanation, which
predictive theories provide; in the second, the predictive theory makes empirical
predictions, which are then to be substantiated. In practice the line between these two
is often blurred, but both strategies suppose that predictive processing requires or is
made easier (or more likely) by some particular physical set-up. Implicit in this is the
assignment of some implementation relation, which is usually not independently
justified. The adaptation and repetition suppression examples illustrate this strategy.
3.4.3 Strategy 3: Present an optimality argument: That predictive processing is more
efficient and thus better than traditional processing
The success of this strategy depends on the reliability of the adaptationist assumption
that the brain will do the better thing, all else equal.34 But even leaving aside qualms
about adaptationism (or optimality-based explanations), it is still a jump from infor-
mational efficiency to metabolic efficiency. At the informational level, predictive
coding does in some situations – namely those where not much is changing! – require
the transmission of fewer bits relative to a fixed coding scheme to produce the same
effects. But that only translates to metabolic efficiency if there is the right relationship
between sending and decoding bits, and metabolic costs to the system: namely one
where more spikes code for larger errors, and the costs of ‘decoding’ (i.e. responding
appropriately to) signals are either held constant or are smaller for predictive schemes.
33 This is not only because of the possibility of relabeling discussed, but perhaps more scientifically salient,
our best models of V1 responses now come from neural network models optimized for image recognition. See
Cadena et al. (2019).
34 “This difference signal obtained by predictive coding has a much smaller dynamic range than the raw
image, and is therefore more suited for transmission through neural fibres with a limited firing rate.” (Hosoya
et al. 2005). Strictly speaking this needn’t always be true; consider a case where values in a bitmap can go
from 0 to 100 (arbitrary units). Then the range of potential error values would go from −100 to +100, which in
fact comes to a greater dynamic range than the original signal required … but presumably we can make up for
this by not needing to send as many pixels. Or if we expect errors to be small on average. Or any number of
other small adjustments not specified. In general, which encoding scheme is more efficient depends on the
kinds of data that you expect to be transmitted on average, and also on the other requirements and constraints
on the system. Here is another typical statement of the efficiency claim: “Since top-down predictions suppress
expected sensory input (i.e., reduce prediction error), expected stimuli lead to relatively little neuronal firing.
Such a coding scheme has several advantages. First, it is metabolically efficient.” (Kok and de Lange 2015)
Cao R.
540

Moreover, looking only at the error stream, it would be more efficient only if errors are
usually low. So metabolic efficiency is a hard case to make.
Finally, I want to come back to a possibility set aside in Section 2. What if predictive
processing is a claim not about a difference in content, but in the way that content is
encoded? One might reasonably think that while predictive processing may just be an
alternative name for information processing, predictive coding is a substantive claim
about the relationship between the informational goings-on and the physical goings-on,
between spikes and their informational import. So the theories might say the same thing
both about what is being encoded (a set of correlated facts about distal stimulus
features and changes in internal state) and what is being done with that information
computationally speaking, but still say something different about the encoding scheme.
Wouldn’t that be empirically testable?
While such an interpretation seems initially appealing, I think that is due to a false
analogy. The distinction between an encoding scheme and a difference in content makes
sense in the context of, say, video transmission – when the task is clear, sender, receiver, and
protocols are well-defined, and so are the conventions dictating the relationships between
image pixels, numerical values, and bit-values (ones and zeros), and physical manifestations
of the transmission of those bits. In that context, the conventions distinguish between the
intended interpretation of a signal as representing errors vs. as directly representing
current stimulus features. We get to say whether a signal is meant to be metarepresentational
(in the sense of representing something about the accuracy of a prediction), or merely
representational (i.e. indicating something about the input) because we designed the system.
Moreover, we get to identify signals as ones and zeros independently of their role in a
particular transmission scheme. That is how we get to talk about a signal as compressed in
the first place – compression itself being a relative measure of how many bits need to be sent
in one scheme vs. another. When we talk about information transmission in the signal
processing sense, it is quite clear what counts as a bit, and it is also quite clear what the
receiver is going to do with the bits it receives. Any indeterminacy is resolved by our design
intentions.
The distinction is much fuzzier in these models of perception. We have a general idea of
the ultimate task to be explained (exploit sensory states to make information about the world
available for further cognitive use as well as action), but the subtasks that make this possible,
which entities are the receivers of the information, and which conventions those entities use to
“interpret” incoming signals, are all up for grabs.
Every signal is being sent somewhere, and what makes it the signal that it is – characterized
now in terms of its content – is the effect that it has and the functional role that it plays in the
system. There is no further fact about what a signal means, independent of its role in the
system. That means that there is no stable syntactic scheme to appeal to that is independent of
the semantic functional role posited for the signal.
4 Upshots (and some speculations)
To sum up: Where predictive and traditional theories disagree is not about the physical
anatomy of the brain. Nor is it about which behaviors or particular patterns of brain activity are
exhibited. So there can be no empirical evidence that would favor predictive over traditional
New Labels for Old Ideas: Predictive Processing and the...
541

theories of perception, when both are understood as computational models of information
processing.
Since there is no difference in information content (or physical form, or causal effects), any
difference in the putative content of neural signals in the model will have to be due to a
difference in interpretation of what the signal is for. A traditional theorist might think the
bottom-up signal has the function to inform the system about what is going on in the outside
world. A predictive theorist might prefer to say that the bottom-up signal has the function to
correctan existing hypothesis or representation of how the world is. But these are just different
ways of saying the same thing: they both seem to be compatible with however the world
might be.
It would seem that we have here a case of Putnam’s (1998) “conceptual relativity” or
theoretical equivalence. There is an apparent face-value disagreement between predictive and
non-predictive theories, but we can translate between them, and they are more like optional
choices of language than disagreements of fact.
On this way of thinking, there is no real difference between the prediction of
something in the future and the detection of something currently present. Both involve
pattern completion, priors, and so on. Depending on what we take to be the coding
scheme (i.e. the mapping from stimulus to import), we might call the same signal a
trigger for prediction of X, or instead an explicit representation of X. While this is
tricky to illustrate for the case of internal signals (and their representational import),
perhaps external representations can help us visualize what is going on by analogy.
For each of the shapes in Fig. 6, we might say that the visual system is “predicting”
contours and objects that are not actually there in the images. But in another sense, the
images are slightly unusual but nonetheless simple encodings of a triangle, a white
pole, a spiky ball, and the Loch Ness monster. That is, the visual system is picking out
only and exactly the objects that are depicted there.
Similarly, it is interpretation, emphasis, and a kind of conceptual connection be-
tween the cognitive vocabulary of “hypothesis testing” and the lower-level operations
of the perceptual system that account for the apparent novelty of predictive theories,
Fig. 6 Alternative encodings of familiar shapes (Image from Wikipedia entry on Gestalt psychology)
Cao R.
542

rather than any difference in predicted observations, or an ability to explain data that
the traditional theory cannot.
The terminology of predictive theories does have quite different connotations than that
of traditional theories. Prediction sounds more structured than the top-down modulation of
traditional theories. Prediction sounds more plausible as a source of perceptual content,
whereas modulation sounds secondary to the baseline processing of pre-existing content.
It might seem as though predictions have contents, while modulation merely has causes
and effects. Meanwhile, error has strong normative overtones, invoking the idea of falling
short of some standard, whereas mere stimulus information sounds relatively neutral. But
here we should be careful. It’s true that the world seems richer when we describe it in more
colorful language – just look at our readiness to anthropomorphize the simplest of systems.
But the color should be theoretically justified, and it should carry some empirical weight;
in this case, it’s not clear that it does.
In traditional theories, when the system is functioning as it should, percepts are formed
on the basis of incoming information, and when things are going well there need be no
errors at all. In that context, we speak of error only when something has gone wrong, so
that there is misrepresentation, for example. In predictive theories, by (apparent) contrast,
error seems to play a constitutive role in perceptual processing: normal perception
operating successfully involves the constant processing of “error” signals.
But again, this is only an apparent difference: Since error processing is now a normal part
of the successful functioning of the system, the presence of errors does not mean that anything
is not as it should be. While some internal expectation or prediction is “violated”, the system
runs on such violations, and there is no sense in which the system would do better if only such
errors never occurred.35
Error signals, then, are not really playing the same functional role in predictive models as
the things that we would call errors in traditional models. The actual functional role of error
signals in predictive models is just the same role that a signal carrying new information plays
in a traditional model. Representations of how the world is are updated on the basis of new
information. If the traditional model happens to be Bayesian, then they apply the update
35 Indeed, the “dark room” problem has been much discussed in the context of predictive theories. The best
way of eliminating error is to eliminate bottom-up signals, and one way to do that second thing (a way that
predictive and traditional theorists agree on!) is to sit quietly in a dark room leaving the perceptual system
unperturbed. On this description, it seems even more clear that the role of so-called predictive error is the same
role as the role of bottom-up sensory information in traditional models.
36 Part of this might be a reflected glow from the enthusiasm for Bayesian theories more generally – and it may
be that some promising Bayesian models of perception are cast as predictive, but the two terms are not the
same and should not be used interchangeably (as Rescorla argues, forcefully, in his 2017 review of Clark’s
Surfing Uncertainty).
37 A predictive theorist might double down, and argue that one contribution of the predictive framework is to
show how entities capable of the relatively bloodless activities characterized by traditional models are, under
an alternative labeling scheme, engaged in activities that seem much more interesting and closer to the ones
that we ourselves engage in. Or perhaps they point towards a reduction of predictions and errors to things that
we can understand in purely causal terms, such as modulatory activity and stimulus-evoked responses. If so,
this would be a radical claim, and one that is perhaps empirically testable by looking at neural correlates of
behavioral surprise, but I am dubious about this proposal, because the personal-level experience (and practice)
of prediction, expectation, and error seem very different from the subpersonal processes and signals sharing
the same names. If I were constantly predicting and correcting during perception, it seems to me that looking at
the world would feel much more like sight-reading music – a kind of frantic muddle to stay on top of what’s
coming – rather than the immersive and effortless process that I actually experience. (Perhaps if I were better at
sight-reading I would have different intuitions about this.)
New Labels for Old Ideas: Predictive Processing and the...
543

according to Bayes’ rule on the basis of new inputs, and this can be described as error
correction if we like, but needn’t be.
As I’ve said, the extravagance of the enthusiasm expressed for predictive theories makes
me uneasy.36 The casual use of the terms active, passive, error, contents, contributes to the
uneasiness – these terms do not have clear empirical import, and it seems very easy to slide
between personal-level versions of terms like “prediction” or “expectation” to the subpersonal
ones employed in theorizing.37
The remarkable flexibility of the predictive framework allows it to accommodate seem-
ingly any data. It casts all of cognition and perception as an optimization problem. But almost
any process can be cast as some optimization problem or another, if only we define the
outcome of the process as a maximum (or minimum) in some space of possible outcomes. So
something more needs to be said, both about the specific nature of the optimization problem,
and about how it is implemented in the brain, in order to say anything empirically substantive
about how cognition/perception works.38 Otherwise, predictive models can seem not just post
hoc but ad hoc.
In this way (good or bad), the predictive processing strategy looks a bit like the one
offered by an evolutionary framework. We can use evolutionary considerations to
explain why certain populations have the features that they do, given their history
and environment. It is harder to use the evolutionary framework to predict in advance
what traits creatures will exhibit in the future, perhaps impossible. And it can be
tempting to make up unsubstantiable just-so stories about how some trait came to be.
Many evolutionary explanations do make empirical predictions, given auxiliary as-
sumptions. The question is whether predictive theories can be cast in such a way that
they make testable empirical predictions, or whether they are being wielded primarily
as just-so stories about existing data – or if, indeed, auxiliary assumptions about
implementation are doing all the empirical work.
Perhaps the advantages of the predictive framework come from its heuristic contri-
butions to theorizing, arising from a suggestive halo. These heuristic contributions
seem somewhat elusive, although we can point to the theoretical virtues of a unifying
theory that emphasizes the similarities between perception, learning, and action, and
perhaps the practical advantages of a one-size-fits-many computational model.
There is also a more general worry that computational models that purport to
describe the information-processing structure of the system are empirically empty,
unless supplemented by a neural code that could connect claims made about informa-
tion content to empirically evaluable claims about observable activities of entities in the
brain.39 Only with such a mapping specified would any model, predictive or otherwise,
become empirically substantive. But these mappings themselves have uncertain status.
What justifies one mapping rather than another? Not more measurements – but rather,
38 Of course, the same issues can be raised with respect to understanding evolution as a maximizing process,
see e.g. Okasha (2019) for an overview of that debate. In our context, perception (traditionally) is supposed to
maximize (or satisfice with respect to) the accuracy of its depiction of the world.
39 See Grush (2001) and Cao (2018) for further discussion of this worry.
40 I am grateful to Colin Allen, Marc Artiga, Lindy Blackburn, Ned Block, Tian Yu Cao, David Chalmers,
Daniel Dennett, Shaul Druckmann, Gary Ebbs, Frankie Egan, Jon Gauthier, Peter Godfrey-Smith, Fred
Keijzer, Daniel Kraemer, Enoch Lambert, Kirk Ludwig, Tim Maudlin, Michael Rescorla, Nick Shea, Aaron
Sidford, Mark Sprevak, Michael Strevens, Jared Warren, John Han Wen, Adam White, Martha White, Daniel
Yamins, and audiences at KCL, MIT, Tufts, Antwerp, and Bochum for helpful discussions and comments. I
would also like to thank Reviewer 2 for careful and constructive criticism.
Cao R.
544

overarching considerations about what makes for a better explanation beyond accom-
modating and accurately predicting our measurements. Clark and Hohwy think that the
predictive theory provides better explanations overall, but I think the jury is still out.40
References
Adams, R.A., S. Shipp, and K.J. Friston. 2013. (2013) predictions not commands: Active inference in the
motor system. Brain Structure & Function 218: 611–643.
BS Atal, MR Schroeder. (1970) Adaptive predictive coding of speech signals. The bell system technical
journal (volume: 49, issue: 8).
Bialek, W., Rieke, F., de Ruyter van Steveninck, RR., and Warland, D. 1991. Reading a neural code. Science
252 (5014): 1854–1857
Bitzer, S., H. Park, F. Blankenburg, and S.J. Kiebel. 2014. Perceptual decision making: Drift-diffusion model
is equivalent to a Bayesian model. Frontiers in Human Neuroscience 8: 102.
Cadena, S.A., G.H. Denfield, E.Y. Walker, L.A. Gatys, A.S. Tolias, M. Bethge, and A.S. Ecker. 2019. Deep
convolutional models improve predictions of macaque V1 responses to natural images. PLoS
Computational Biology 15 (4): e1006897.
Cao, R. 2012. A Teleosemantic approach to information in the brain. Biology and Philosophy 27: 49–71.
Cao, R. 2018. Computational explanations and neural coding. In Routledge handbook of the computational
mind, ed. M. Sprevak and M. Colombo. London: Routledge.
Carlton, T., and A. McVean. 1995. The role of touch, pressure and nociceptive mechanoreceptors of the leech
in unrestrained behaviour. J. Comp. Physiol. [A] 177: 781–791.
Clark, A. 2013. Whatever next? Predictive brains, situated agents, and the future of cognitive science. The
Behavioral and Brain Sciences 36: 181–125.
Clark, A. 2015. Embodied prediction. In T. Metzinger & J. M. Windt (Eds). Open MIND: 7(T). Frankfurt am
Main: MIND group.
Clark, A. 2016. Surfing uncertainty: Prediction, action, and the embodied mind. Oxford University Press.
Conant, Roger C., and W. Ross Ashby. 1970. Every good regulator of a system must be a model of that
system. International Journal of Systems Science 1 (2): 89–97.
Egan, F. 2014. How to think about mental content. Philosophical Studies 170: 115–135.
Fresco, N., Ginsburg, S. & Jablonka (2018) Functional information: A graded taxonomy of difference-makers
E. Rev.Phil.Psych.
Friston, K. 2005. A theory of cortical responses. Philosophical Transactions of the Royal Society of London B:
Biological Sciences 360 (1456): 815–836.
Friston, K. 2008. Hierarchical models in the brain. PLoS Computational Biology 4 (11): e1000211.
Friston, K. 2009. The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences 13 (7):
293–301.
Friston, K.J. 2010. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience 11 (2):
127–138.
Friston, K., and C.J. Price. 2001. Dynamic representations and generative models of brain function. Brain
Research Bulletin 54 (3): 275–285.
Frith, C.D. 2007. Making up the mind: How the brain creates our mental world. Blackwell.
Grush, R. 2001. The semantic challenge to computational neuroscience, In Theory and Method in the
Neurosciences, Peter K. Machamer, Peter McLaughlin and Rick Grush, 155–172. Pittsburgh:
University of Pittsburgh Press.
Grush, R. 2004. The emulation theory of representation. Motor control, imagery, and perception Behavioral
and Brain Sciences 27 (3): 377–396.
Hohwy, J. 2013. The predictive mind. New York, NY: Oxford University Press.
Hohwy, J., A. Roepstorff, and K. Friston. 2008. Predictive coding explains binocular rivalry: An epistemo-
logical review. Cognition 108: 687–701.
Hosoya, T., S.A. Baccus, and M. Meister. 2005. Dynamic predictive coding by the retina. Nature 436 (7): 71–
77.
Hubel, D.H., and T.N. Wiesel. 1959. Receptive fields of single neurones in the cat's striate cortex. The Journal
of Physiology. 124 (3): 574–591.
Issa, E.B., C.F. Cadieu, and J.J. DiCarlo. 2018. Neural dynamics at successive stages of the ventral visual
stream are consistent with hierarchical error signals. eLife 2018;7:e42870.
New Labels for Old Ideas: Predictive Processing and the...
545

Kanwisher, N., J. McDermott, and M.M. Chun. 1997. The fusiform face area: A module in human extrastriate
cortex specialized for face perception. The Journal of Neuroscience 17 (11): 4302–4311.
Kok P, de Lange PF. (2015) Predictive coding in sensory cortex. Forstmann, UB and Wagenmakers, E-J,
editors, an introduction to model-based cognitive neuroscience, pages 221–44. Springer, New York, NY.
Lewis, J.E., and W.B. Kristan Jr. 1998. Representation of touch location by a population of leech sensory
neurons. Journal of Neurophysiology 80 (5): 2584–2592.
Mann, S.F. 2018. Consequences of a functional account of information rev. Phil.Psych. 2018.
Marr, David (1982) Vision: A Computational Investigation into the Human Representation and Processing of
Visual Information. Henry Holt and Co., Inc. New York, NY, USA.
Moore, T., and M. Zirnsak. 2017. Neural mechanisms of selective visual attention. Annual Review of
Psychology. 68: 47–72.
Mumford, D. 1992. On the computational architecture of the neocortex. II. The role of cortico-cortical loops.
Biological Cybernetics 66 (3): 241–251.
Serre, T., Oliva, A., and Poggio, T. 2007. A feedforward architecture accounts for rapid
categorization. Proceedings of the National Academy of Sciences 104 (15): 6424–6429
Poggio, T., and T. Serre. 2013. Models of visual cortex. Scholarpedia 8 (4): 3516.
Putnam, H. 1988. Representation and reality. Cambridge: MIT Press.
Quine, W.V.O. 1960. Word and object. Cambridge: Harvard University Press.
Rao, R., and D. Ballard. 1999. Predictive coding in the visual cortex: A functional interpretation of some extra-
classical receptive-field effects. Nature Neuroscience volume 2: 79–87.
Rescorla, M. 2015. Bayesian perceptual psychology. In The Oxford handbook of the philosophy of perception,
ed. M. Matthen: Oxford University Press.
Rescorla, M. 2016. Bayesian sensorimotor psychology. Mind and Language 31: 3–36.
Rescorla, M. 2017. Review of Andy Clark’s surfing uncertainty Notre Dame Philosophical Reviews.
Schultz, W., P. Dayan, and R.R. Montague. 1997. A neural substrate of prediction and reward. Science. 275:
1593–1599.
Seth, Anil, (2017) “From Unconscious Inference to The Beholder's Share: Predictive Perception and Human
Experience.” web PsyArXiv, (forthcoming, European Review).
Shea, N. 2014. Reward prediction error signals are meta-representational. Noûs 48: 314–341.
Shea, N. 2015. Distinguishing top-down from bottom-up effects. In Biggs, Matthen and stokes, eds, 73–91.
OUP: Perception and Its Modalities Oxford.
Shea, N. 2018. Representation in cognitive science. Oxford University Press.
Spratling, M.W. 2008a. Predictive coding as a model of biased competition in visual attention. Vision
Research 48: 1391–1408.
Spratling, M.W. 2008b. Reconciling predictive coding and biased competition models of cortical function.
Frontiers in Computational Neuroscience 2: 4.
Stanley, G. (2013) “Reading and Writing the Neural Code.” Nature Neuroscience, Vol 16, No 3.
Summerfield, C., and F.P. de Lange. 2014. Expectation in perceptual decision making: Neural and compu-
tational mechanisms. Nature Reviews Neuroscience volume 15: 745–756.
Summerfield, C., and T. Egner. 2009. Expectation (and attention) in visual cognition. Trends in Cognition
Science 13: 403–409.
Sutton, R.S. 1988. Learning to predict by the methods of temporal differences. Machine Learning 3: 9–44.
Sutton, Richard S., and Andrew G. Barto. 1998. Reinforcement learning: An introduction. MIT Press.
Weilnhammer, V., H. Stuke, G. Hesselmann, P. Sterzer, and K. Schmack. 2017. A predictive coding account
of bistable perception - a model-based fMRI study. PLoS Computational Biology 13 (5): e1005536.
Wolpert, M., and D.M. Miall. 1996. Forward models for physiological motor control. Neural Networks 9 (8):
1265–1279.
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.
Cao R.
546

