Topics in Bayesian
Inference for Causal Effects
The Harvard community has made this
article openly available.  Please share  how
this access benefits you. Your story matters
Citation
Garcia Horton, Viviana. 2015. Topics in Bayesian Inference for Causal
Effects. Doctoral dissertation, Harvard University, Graduate School
of Arts & Sciences.
Citable link
http://nrs.harvard.edu/urn-3:HUL.InstRepos:23845483
Terms of Use
This article was downloaded from Harvard University’s DASH
repository, and is made available under the terms and conditions
applicable to Other Posted Material, as set forth at http://
nrs.harvard.edu/urn-3:HUL.InstRepos:dash.current.terms-of-
use#LAA

Topics in Bayesian Inference for Causal Eﬀects
A dissertation presented
by
Viviana Garcia Horton
to
The Department of Statistics
in partial fulﬁllment of the requirements
for the degree of
Doctor of Philosophy
in the subject of
Statistics
Harvard University
Cambridge, Massachusetts
August 2015

c⃝2015 - Viviana Garcia Horton
All rights reserved.

Professors Donald Rubin and Tirthankar Dasgupta
Viviana Garcia Horton
Topics in Bayesian Inference for Causal Eﬀects
Abstract
This manuscript addresses two topics in Bayesian inference for causal eﬀects.
1) Treatment noncompliance is frequent in clinical trials, and because the treat-
ment actually received may be diﬀerent from that assigned, comparisons between
groups as randomized will no longer assess the eﬀect of the treatment received. To
address this complication, we create latent subgroups based on the potential out-
comes of treatment received and focus on the subgroup of compliers, where under
certain assumptions the estimands of causal eﬀects of assignment can be interpreted
as causal eﬀects of receipt of treatment. We propose estimands of causal eﬀects for
right-censored time-to event endpoints, and discuss a framework to estimate those
causal eﬀects that relies on modeling survival times as parametric functions of pre-
treatment variables. We demonstrate a Bayesian estimation strategy that multiply
imputes the missing data using posterior predictive distributions using a random-
ized clinical trial involving breast cancer patients. Finally, we establish a connection
with the commonly used parametric proportional hazards and accelerated failure time
models, and brieﬂy discuss the consequences of relaxing the assumption of indepen-
dent censoring.
2) Bayesian inference for causal eﬀects based on data obtained from ignorable
assignment mechanisms can be sensitive to the model speciﬁed for the data. Ignor-
ability is deﬁned with respect to speciﬁc models for an assignment mechanism and
iii

Abstract
data, which we call the “true” generating data models, generally unknown to the
statistician; these, in turn, determine a true posterior distribution for a causal esti-
mand of interest. On the other hand, the statistician poses a set of models to conduct
the analysis, which we call the “statistician’s” models; a posterior distribution for the
causal estimand can be obtained assuming these models. Let ∆M denote the dif-
ference between the true models and the statistician’s models, and let ∆D denote
the diﬀerence between the true posterior distribution and the statistician’s posterior
distribution (for a speciﬁc estimand). For ﬁxed ∆M and ﬁxed sample size, ∆D varies
more with data-dependent assignment mechanisms than with data-free assignment
mechanisms.
We illustrate this through a sequence of examples of ∆M, and under various ig-
norable assignment mechanisms, namely, complete randomization design, rerandom-
ization design, and the ﬁnite selection model design. In each case, we create the 95%
posterior interval for an estimand under a statistician’s model, and then compute
its coverage probability for the correct posterior distribution; this Bayesian coverage
probability is our choice of measure ∆D. The objective of these examples is to pro-
vide insights into the ranges of data models for which Bayesian inference for causal
eﬀects from datasets obtained through ignorable assignment mechanisms is approx-
imately valid from the Bayesian perspective, and how these validities are inﬂuenced
by data-dependent assignment mechanisms.
iv

Contents
Title Page . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
i
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iii
Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
viii
Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
1
Causal inference for time-to-event data with noncompliance
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Time-to-event data . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1.1
Censoring mechanisms . . . . . . . . . . . . . . . . .
2
1.1.1.2
Survival time distributions . . . . . . . . . . . . . . .
4
1.1.1.3
Standard estimation procedures . . . . . . . . . . . .
4
1.1.2
Noncompliance . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Case study: IBCSG trial 23-01 . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Rubin Causal Model Framework . . . . . . . . . . . . . . . . . . . . .
12
1.3.1
Potential outcomes . . . . . . . . . . . . . . . . . . . . . . . .
12
1.3.1.1
Survival potential outcomes . . . . . . . . . . . . . .
12
1.3.1.2
Noncompliance potential outcomes . . . . . . . . . .
14
1.3.2
Estimands . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.3.2.1
Compliance status proportions
. . . . . . . . . . . .
16
1.3.2.2
Survival curves . . . . . . . . . . . . . . . . . . . . .
17
1.3.2.3
Median lifetimes
. . . . . . . . . . . . . . . . . . . .
20
1.3.2.4
Rate ratios
. . . . . . . . . . . . . . . . . . . . . . .
20
1.3.3
Observed outcomes . . . . . . . . . . . . . . . . . . . . . . . .
23
1.4
Parametric model speciﬁcation . . . . . . . . . . . . . . . . . . . . . .
24
1.4.1
Models for complete data
. . . . . . . . . . . . . . . . . . . .
25
1.4.1.1
Treatment assignment model
. . . . . . . . . . . . .
27
1.4.1.2
Compliance status model
. . . . . . . . . . . . . . .
27
1.4.1.3
Survival endpoint model . . . . . . . . . . . . . . . .
28
1.4.2
Multiple imputation of missing potential outcomes
. . . . . .
30
1.4.2.1
Observed data likelihood . . . . . . . . . . . . . . . .
30
v

Contents
1.4.2.2
Prior distributions and posterior sampling . . . . . .
31
1.4.2.3
Superpopulation estimation . . . . . . . . . . . . . .
32
1.4.2.4
Finite population estimation . . . . . . . . . . . . . .
33
1.5
IBCSG trial 23-01 results . . . . . . . . . . . . . . . . . . . . . . . . .
36
1.6
Connection with PH and AFT formulations
. . . . . . . . . . . . . .
43
1.6.1
Proportional hazards models . . . . . . . . . . . . . . . . . . .
44
1.6.2
Accelerated failure time models . . . . . . . . . . . . . . . . .
47
1.7
Relaxing the assumption of independent censoring . . . . . . . . . . .
50
1.8
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2
Illustrating the inﬂuence of ignorable but covariate dependent ex-
perimental designs on the Bayesian validity of inference for causal
eﬀects
56
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.2
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.3
The assignment mechanism
. . . . . . . . . . . . . . . . . . . . . . .
62
2.3.1
General characteristics of assignment mechanisms . . . . . . .
62
2.3.2
Examples of random assignment mechanisms . . . . . . . . . .
63
2.3.2.1
Completely randomized design
. . . . . . . . . . . .
64
2.3.2.2
Rerandomization design . . . . . . . . . . . . . . . .
64
2.3.2.3
Finite Selection Model design . . . . . . . . . . . . .
65
2.4
Bayesian inference for causal eﬀects . . . . . . . . . . . . . . . . . . .
67
2.4.1
Models for underlying data . . . . . . . . . . . . . . . . . . . .
67
2.4.2
Model-based inference
. . . . . . . . . . . . . . . . . . . . . .
70
2.4.3
Sensitivity of inferences to prior speciﬁcations . . . . . . . . .
71
2.4.4
Contrast with classical Frequentist approaches . . . . . . . . .
72
2.5
Evaluation method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.5.1
True model, statistician’s model, and Bayesian coverage . . . .
74
2.5.2
Bayesian coverage in the context of estimation of causal eﬀects
under model misspeciﬁcation . . . . . . . . . . . . . . . . . . .
77
2.5.3
Alternative measures . . . . . . . . . . . . . . . . . . . . . . .
80
2.6
Illustrative examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
2.6.1
General setup . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
2.6.2
Example 1: correct likelihood, misspeciﬁed prior distributions
86
2.6.3
Example 2: misspeciﬁed functional relationship between Y and
X, correct prior distributions
. . . . . . . . . . . . . . . . . .
87
2.6.4
Example 3: Misspeciﬁed error distribution, correct prior distri-
butions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
2.6.5
Example 4: Misspeciﬁed error distribution and prior distributions 91
2.6.6
Example 5: Misspeciﬁed error distribution and relationship be-
tween Y and X . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.6.7
Example 6: Alternative covariate distribution speciﬁcation . .
95
vi

Contents
2.7
Simulation design for systematic study of
factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
2.7.1
Further extensions
. . . . . . . . . . . . . . . . . . . . . . . .
108
2.8
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
A Additional results from the analysis of IBCSG dataset
113
Bibliography
116
vii

Acknowledgments
I thank Tirthankar Dasgupta for his support, kindness, and help throughout my
entire PhD experience. I thank Dave Harrington for his constant encouragement,
guidance, and understanding, and for all the academic and life advice he provided. I
thank Don Rubin for introducing me to causal inference, for sharing his deep insights
into statistics, and for very valuable discussions. I thank Joe Blitzstein for making me
a part of the great Stat 110 teaching team on multiple ocassions and for his support.
I thank the three members of my committee, Tirthankar Dasgupta, David Har-
rington, and Donald Rubin, for helpful comments. I thank Karen Price and Meredith
Regan from the International Breast Cancer Study Group (IBCSG) Statistical Center
for supplying the clinical trial data for Chapter 1. I thank CONACyT and Fundaci´on
M´exico en Harvard for ﬁnancial support.
I am grateful for all the wonderful new friends I made during my time here; for Nik
and my “Mexican family”—especially Joel and Valeria. Thanks for all the good times
and for being my support system all these years. I thank my friends who are scattered
around the U.S. and Mexico for everything that we have experienced together.
Last, but not least, my heartfelt gratitude goes to my family. I thank my parents
for their unconditional support and love, for helping me through the diﬃcult times,
and for not letting me give up. I thank my brother and best friend, Alex, for always
believing in me and dreaming with me, and for inﬁnitely enriching my life.
viii

To Fernando, Leticia, and Alejandro.
ix

Chapter 1
Causal inference for time-to-event
data with noncompliance
1.1
Introduction
1.1.1
Time-to-event data
Time-to-event data arise when the outcome of interest in a study is the time until
the occurrence of some speciﬁed event. In medical studies, examples of such events
include death, recurrence of a disease, discharge from hospital, etc. This type of data,
also referred to as survival data – or alternatively, failure time data –, is ubiquitous
in many ﬁelds such as public health, economics, epidemiology, medicine, and engi-
neering, among others.
A distinctive feature of survival data is censoring. Here we focus on the case of
1

Chapter 1: Causal inference for time-to-event data with noncompliance
right censoring, that is, the times-to-event are known exactly only for a portion of
the individuals under study and the remainder are just known to be greater than or
equal to certain value. Right censoring is quite common in clinical trials and medical
settings because it is very likely that at least some individuals may not experience
the event of interest during their observation time frame. The implicit assumption is
that the event will occur for all individuals given suﬃcient follow-up time, and thus
the event will occur in the future for those for whom the time of event is not currently
known.
Time-to-event data consist of a pair of quantities associated with each individual:
a survival time (the length of time until a particular event occurs) and a censoring
time (the period of observation). The exact survival time of an individual will be
observed only if it is smaller than the censoring time. Therefore, the data available
to the analyst are the minimum between the survival and the censoring times, and
an indicator for whether the observation was censored, for each unit.
1.1.1.1
Censoring mechanisms
For any analysis involving time-to-event data, it is necessary to characterize the
censoring mechanism. One commonly made assumption is that of random censoring,
which holds when the censoring times are random processes that are stochastically in-
dependent of the survival times given the covariates (Cox and Oakes, 1984; Kalbﬂeisch
and Prentice, 2002).
2

Chapter 1: Causal inference for time-to-event data with noncompliance
The random censoring model includes two special types of right-censoring that
occur frequently in clinical trials:
1. Type I censoring. The study is stopped at a pre-speciﬁed time, and any individ-
uals who haven’t experienced the event of interest by then are right-censored.
This includes the case where the censoring time of each individual is ﬁxed in
advance (and is the same for all individuals), as well as the case where individ-
uals enter the study at random over time and the analysis is carried out at a
pre-speciﬁed time (censoring times may vary from individual to individual).
2. Type II censoring.
The study is stopped when a predetermined number of
individuals experience the event of interest, and the rest are right-censored.
A censoring mechanism is said to be independent if the failure rates that apply to
individuals on trial at each time t > 0 are the same as those that would have applied
had there been no censoring (Kalbﬂeisch and Prentice, 2002). Essentially, the censor-
ing procedure may depend on previous failure times, on previous censoring times, on
random mechanisms external to the study, or on values of covariates included in the
model. The censoring types discussed above are examples of independent censoring.
In other instances, however, individuals are censored selectively or decide to with-
draw from the study because they appear to be at an unusually high (or low) risk
of failure compared to others with the same covariates. In such cases, the censoring
scheme would not be independent: censoring time distributions will inﬂuence the
3

Chapter 1: Causal inference for time-to-event data with noncompliance
estimation of survival time distributions and treatment comparisons, and therefore
should be modeled.
1.1.1.2
Survival time distributions
Let T be a continuous nonnegative random variable representing the survival time
of an individual, and let f(t) and F(t) denote the probability density function (PDF)
and the cumulative distribution function (CDF) of T, respectively. The probability
of not experiencing an event up to time t is given by the survivor function:
S(t) = 1 −F(t) = P(T > t).
The hazard function gives the instantaneous rate of failure at time t, that is, the
potential that an event will occur (per time unit) given that an individual has survived
up to the speciﬁed time:
h(t) = lim
∆t→0
P(t < T ≤t + ∆t | T > t)
∆t
= f(t)
S(t).
1.1.1.3
Standard estimation procedures
It is not appropriate to analyze survival data using ordinary methods for con-
tinuous outcomes because the exact times-to-event are only known for a subset of
individuals; thus, censoring constitutes a particular type of missing data that has to
be taken into account.
4

Chapter 1: Causal inference for time-to-event data with noncompliance
A simple approach would be to dichotomize the outcomes according to survival or
non-survival at a ﬁxed time-point, and use techniques for binary data. Even though
this is often satisfactory, some information might be wasted, and – more importantly
– comparing proportions would make sense only when no individuals are censored
during the period of interest.
Conventional methods of analysis for survival data, which incorporate informa-
tion from both censored and uncensored observations, include non-parametric, semi-
parametric, and parametric approaches.
1. Non-parametric methods. The Kaplan-Meier estimate (Kaplan and Meier, 1958)
is a widely used estimate of the survival function; a closely-related alternative
is the Nelson-Aalen estimator of the cumulative hazard function. If there is
interest in comparing the estimated survival curves of two or more groups, the
log-rank test may be used. These approaches assume non-informative censoring.
2. Semi-parametric methods.
The Cox proportional hazards regression models
describe the relationship between the hazard function and covariates as non-
linear, while assuming that the hazard ratio comparing any two observations is
constant over time.
3. Parametric methods. These focus on the use of parametric models to describe
the underlying distribution of the survival times.
The exponential, gamma,
Weibull, and lognormal are some of the most frequently used probability distri-
butions. This approach allows for easy incorporation of covariates.
5

Chapter 1: Causal inference for time-to-event data with noncompliance
1.1.2
Noncompliance
Deviations from treatment assignment are common in randomized experiments
with human subjects, even when the trials are well-designed and executed. However,
a standard method for analysis such as intention-to-treat (ITT) will no longer assess
the true biological eﬀect of the treatment – that is, the eﬀect of treatment actually
received–, but rather the eﬀectiveness of the treatment assignment, because it ignores
the presence of noncompliance.
Some alternatives to ITT are frequently used to deal with this issue, but none of
them have any formal causal justiﬁcation. For example, a per-protocol analysis in-
cludes all patients who completed the full course of assigned treatment with no major
protocol violations; however, by discarding patients that did not comply with their
observed treatment assignment, it creates non-comparable groups.
An as-treated
analysis compares patients according to their treatment received, omitting the ran-
domization that gives validity to inferences obtained from the trial.
To address the problem of all-or-none treatment noncompliance, one approach is
based on the idea of instrumental variables (IV) and focuses on the ITT eﬀect for the
subpopulation of compliers, a partially unidentiﬁed group of units who always comply
with the treatment assigned through randomization (Angrist et al., 1996; Imbens and
Rubin, 1997; Frangakis and Rubin, 2002). Under this framework, the experimental
units are ﬁrst classiﬁed into latent subgroups based on a post-treatment variable:
the treatment received under both possible treatment assignments. Within the sub-
6

Chapter 1: Causal inference for time-to-event data with noncompliance
group of compliers, the eﬀect of the assignment can (at least in some situations) be
attributed to the eﬀect of receipt of treatment; this attribution is what is typically
done in randomized trials with full compliance.
Most of the literature on treatment noncompliance has focused on binary or con-
tinuous outcomes. A few authors have also addressed treatment noncompliance with
subsequent missing outcomes (see for example Frangakis and Rubin (1999), Mealli
et al. (2004), and Peng et al. (2004)). In particular, Frangakis and Rubin (1999) con-
struct a non-parametric estimation procedure for the intention-to-treat eﬀect under
nonignorable noncompliance and nonignorable missing-outcome conditions, and show
some connections with analysis of right censored time-to-event data.
For the estimation of the causal eﬀect of treatment received in the speciﬁc context
of right-censored survival data with noncompliance, structural nested accelerated fail-
ure time models were developed by Robins and Tsiatis (1991). They study a class
of rank-preserving models (i.e. individuals keep their order of failure time when they
receive the same treatment), which relate a patient’s observed survival time with a
baseline survival time that corresponds to never receiving the active treatment (that
may be unobservable). The models assume that the observed and baseline times diﬀer
by a factor that is the same for all patients, and the survival function of the baseline
life time is the same in the treated and the control group.
Frangakis and Rubin (1999) introduced the instrumental variables framework to
7

Chapter 1: Causal inference for time-to-event data with noncompliance
deal with both noncompliance and time-to-event endpoints, and proposed a nonpara-
metric estimator of the survival distribution of compliers under the assumption of
no access to active treatment in the control treatment arm. This approach relies on
exclusion restrictions, and does not allow for the incorporation of covariates.
Loeys and Goetghebeur (2003) also use the IV framework and propose a semipara-
metric estimator for the eﬀect of treatment actually received for compliers based on a
proportional hazards model. The method requires no access to the active treatment
on the control arm, exclusion restrictions, and noninformative censoring, and does
not incorporate baseline covariates (although the extension is possible).
We create latent subgroups based on the potential outcomes of treatment received
to address the problem of all-or-nothing noncompliance for time-to-event outcomes
from a two-arm randomized experiment that compares a control versus an active
treatment. We provide a framework that relies on parametric models that relate sur-
vival times to covariates, and propose a Bayesian estimation strategy that multiply
imputes the missing data using posterior predictive distributions.
The paper is organized as follows. In Section 1.2, we introduce the data from our
case study: a randomized clinical trial involving breast cancer patients, where the
intervention studied is a surgical procedure. We introduce our framework and esti-
mands of interest in Section 1.3. In Section 1.4, we introduce our model speciﬁcations
and estimation approach. Section 1.5 illustrates our method and provides the results
8

Chapter 1: Causal inference for time-to-event data with noncompliance
of the analysis of the case study. In Section 1.6 we make some parallels between
our modeling approach and the commonly used parametric proportional hazards and
accelerated failure time models. In Section 1.7, we brieﬂy discuss the consequences
of relaxing the assumption of independent censoring. We conclude and discuss future
work in Section 1.8.
1.2
Case study: IBCSG trial 23-01
Axillary dissection is a surgical procedure that incises the axilla (i.e., the armpit)
to examine or remove lymph nodes. It has been the standard technique used in the
staging and treatment of the axilla in breast cancer. However, it is an invasive pro-
cedure, and short-term and long-term side eﬀects such as swelling of the arm, pain,
and reduced arm movement have always been a concern.
We consider the International Breast Cancer Study Group (IBCSG) trial 23-01, a
randomized, non-inferiority phase 3 study of breast cancer patients with micrometas-
tases in the sentinel node (Galimberti et al., 2013).
Between April 1, 2001, and
February 28, 2010, 931 patients were randomly assigned to undergo either axillary
dissection or no axillary dissection, in a 1:1 allocation ratio. Masking was not done
in this surgical trial. Henceforth, we refer to axillary dissection as control treatment
(because it is the standard of care), and to no axillary dissection as active treatment
(because it is the new treatment that actively foregoes the use of the standard).
9

Chapter 1: Causal inference for time-to-event data with noncompliance
Patient and tumor characteristics related to disease severity measured before treat-
ment assignment include estrogen receptor status (positive vs. negative), progesterone
receptor status (positive vs. negative), menopausal status (premenopausal vs. post-
menopausal), tumor grade (grade I, II, or III), indicator for conservative (“less than
mastectomy”) surgery vs. mastectomy, and pathological tumor size group (< 2 cm,
2 −2.9 cm, or ≥3 cm). The covariates were well balanced between the treatment
groups, as shown by the summaries provided in Table 1.1.
Table 1.1: Averages of covariates and noncompliance outcomes by treatment group.
P-values for comparison of covariates and percentage of noncompliers come from
Fisher exact tests.
Covariate
Active
Control
Diﬀerence
Estrogen receptor status
0.913
0.887
0.026
Progesterone receptor status
0.750
0.762
-0.012
Menopausal status
0.557
0.561
-0.004
Tumor grade
2.100
2.031
0.069
LTM
0.909
0.907
0.001
Tumor size
1.361
1.382
-0.021
Outcome
Noncompliers
0.030
0.035
-0.005
The primary endpoint of interest is disease-free survival (DFS) time, deﬁned as
the number of months from randomization until ﬁrst evidence of invasive relapse at
any site, second primary tumor, or death. The length of time each patient was in
the study varied due to staggered entry, and many DFS times were subject to ad-
ministrative censoring. For this particular example, dichotomizing the outcome (e.g.
considering the DFS status at a speciﬁed time after randomization) is not very ap-
propriate, because the median follow-up was 5 years. Thus, much information would
be wasted by considering only patients with complete follow-up in a reduced time
10

Chapter 1: Causal inference for time-to-event data with noncompliance
frame. Additionally, in oncology clinical trials, long-term eﬀects – namely over a 5
year follow-up period, at least – are of main interest, so any eﬀort to obtain binary
data for “valid” analysis would prove unsatisfactory for the goals of the study.
The purpose of this trial was to determine if not undergoing axillary dissection
would provide an acceptable health outcome for patients with limited sentinel-node
involvement. As such, the objective was to compare 5-year disease-free survival be-
tween the treatment groups.
The trial suﬀered from two-sided all-or-nothing noncompliance: some patients al-
located to the axillary dissection group did not receive the surgery, whereas some
patients allocated to the no axillary dissection group did receive the surgery. Thus,
the indicator for treatment received is a well deﬁned outcome for each patient.
Figure 1.1 shows the Kaplan-Meier estimates of the disease-free survival curves
by treatment assignment for the ITT population, that is, disregarding information
on noncompliance.
11

Chapter 1: Causal inference for time-to-event data with noncompliance
0
1
2
3
4
5
6
0
20
40
60
80
100
ITT Kaplan−Meier Survival Curves
Time from randomization (years)
Disease free survival (%)
Treatment
No axillary dissection (z=1)
Axillary dissection (z=0)
Figure 1.1: ITT Kaplan-Meier estimates of disease-free survival curves, IBCSG trial.
1.3
Rubin Causal Model Framework
1.3.1
Potential outcomes
We work under the Rubin Causal Model (Holland, 1986) based on potential
outcomes, which extends Neyman’s approach beyond randomized experiments and
randomization-based inference (Neyman et al., 1990). Let N denote the total num-
ber of patients in the trial.
1.3.1.1
Survival potential outcomes
Throughout this manuscript, we make the following assumption for the censoring
mechanism.
Assumption 1.3.1. Independent censoring mechanism. As deﬁned in Section 1.1.1.1,
12

Chapter 1: Causal inference for time-to-event data with noncompliance
a censoring rule is said to be independent if, when applied at time t > 0, it depends
only on events that are in the history of the study up to (but not including) time t
and possibly on random mechanisms external to the survival process.
Together with the assumption that the set of parameters for the censoring distribu-
tion are distinct from those used in the modeling of the survival times, independence
of the censoring mechanism implies that it is ignorable for inference, in the sense that
it does not need to be modeled or included in the likelihood. In Section 1.7, we pro-
vide a brief discussion on the implications of relaxing the assumption of independent
censoring.
Let Emax denote the time at which the study ends (or the date of last follow-
up).
For patient i = 1, . . . , N, let Ei be the time of randomization, and deﬁne
Ci = Emax −Ei as the period of observation (or administrative censoring time). In
this scenario, the censoring times are constant (not a random variable) and are not
aﬀected by treatment assignment.
Let Ti(0) and Ti(1) denote the (uncensored) disease-free survival times, starting
from the time of randomization until the realization of the speciﬁed event, under
assignment to control treatment and active treatment, respectively. Then for z ∈
{0, 1}, let
Yi(z) = min(Ti(z), Ci)
(1.3.1)
13

Chapter 1: Causal inference for time-to-event data with noncompliance
and
Vi(z) = I(Ti(z) ≤Ci) =





1
if Ti(z) ≤Ci,
0
if Ti(z) > Ci
(1.3.2)
be the (possibly censored) DFS time and the indicator for censoring, respectively.
Writing the potential outcomes for unit i as a function of the indicator of assigned
treatment for that given unit only (instead of taking into account assignments for all
units), as above, requires the following assumption:
Assumption 1.3.2. Stable Unit Treatment Value Assumption (SUTVA, Rubin (1980)).
There is no interference between subjects and there are no hidden versions of the treat-
ments. In the context of a clinical trial, it is plausible to assume that the potential
outcomes for patient i are not aﬀected by the treatment assignments of other patients,
and we assume that there is only one version of the control treatment and one of the
active treatment.
1.3.1.2
Noncompliance potential outcomes
Let Di(0) and Di(1) denote the indicators of treatment received by patient i when
assigned to the control treatment and when assigned to the active treatment, respec-
tively. In the IBCSG 23-01 trial, Di(z) is 0 if patient i received the ancillary dissection
surgery (control treatment) and 1 otherwise, for z ∈{0, 1}.
Following the framework in Imbens and Rubin (1997), we classify patients into
groups according to the joint potential outcomes of the treatment received.
For
14

Chapter 1: Causal inference for time-to-event data with noncompliance
patient i, this partially missing variable is (Di(1), Di(0)), and it describes the com-
pliance behavior. The four possible combinations of values deﬁne four compliance
types, which we denote as Ui for patient i, as follows:
• Ui = c if Di(1) = 1 and Di(0) = 0. These are the compliers, who always take
the treatment they were assigned to.
• Ui = a if Di(1) = Di(0) = 1. These are the always-takers, who take the active
treatment regardless of their treatment assignment.
• Ui = n if Di(1) = Di(0) = 0. These are the never-takers, who take the control
regardless of their treatment assignment.
• Ui = d if Di(1) = 0 and Di(0) = 1. These are the deﬁers, who always take the
opposite treatment to that which they were assigned to.
We require the following assumptions:
Assumption 1.3.3. Unconfounded treatment assignment. This is guaranteed by the
randomization in the clinical trial.
Assumption 1.3.4. Correlation between treatment assignment and treatment re-
ceived. This is a veriﬁable assumption that reduces to checking that the eﬀect of the
randomized assignment on the receipt of treatment (i.e., the proportion of compliers)
is non-zero.
An additional assumption, although not necessary, is usually set on the compliance
behavior groups, and precludes the existence of deﬁers:
15

Chapter 1: Causal inference for time-to-event data with noncompliance
Assumption 1.3.5. Monotonicity. In terms of potential outcomes, this requires that
Di(1) ≥Di(0) for all patients i.
This is in general a sensible assumption to make, and it simpliﬁes the analysis. In
the context of the breast cancer clinical trial, the deﬁers are patients that wouldn’t
undergo ancillary dissection if assigned ancillary dissection, and would undergo an-
cillary dissection if assigned no ancillary dissection. We will make the monotonicity
assumption throughout this manuscript.
1.3.2
Estimands
Let N be the total number of patients in the trial.
In this section we deﬁne
some estimands of interest, including some that are causal eﬀects (comparisons of
the potential outcome under assignment to the active treatment versus the potential
outcome under assignment to control treatment for a well-deﬁned group of subjects).
1.3.2.1
Compliance status proportions
A ﬁrst estimand of interest is the proportion of patients within each compliance
behavior group u ∈{c,n,a}, which we denote by πu and deﬁne as
πu := 1
N
X
i:Ui=u
1.
(1.3.3)
16

Chapter 1: Causal inference for time-to-event data with noncompliance
1.3.2.2
Survival curves
The survival status of patient i until at least time t > 0 under assignment to
treatment z ∈{0, 1} is given by
I(Ti(z) > t).
A unit-level causal eﬀect for patient i can be deﬁned at any time t > 0 as, for
instance, the diﬀerence in survival status between active and control treatments:
I(Ti(1) > t) −I(Ti(0) > t).
A ﬁnite population1 estimand of interest might be the proportion of survivors
at time t > 0, out of N participants in the trial, under assignment to treatment
z ∈{0, 1}, namely
1
N
N
X
i=1
I(Ti(z) > t),
and thus a ﬁnite population causal eﬀect estimand can be deﬁned as the diﬀerence in
survival proportions between active and control treatments:
τ(t) = 1
N
N
X
i=1
I(Ti(1) > t) −1
N
N
X
i=1
I(Ti(0) > t).
The superpopulation causal eﬀect estimand is the expected diﬀerence in proportions
1Finite population refers to the patients in the study, not a ﬁnite population that contains the
patients.
17

Chapter 1: Causal inference for time-to-event data with noncompliance
of survivors
ESP [τ(t)]
=
ESP
 
1
N
N
X
i=1
I(Ti(1) > t) −1
N
N
X
i=1
I(Ti(0) > t)
!
=
1
N
N
X
i=1

P(Ti(1) > t) −P(Ti(0) > t)

=
S1(t) −S0(t)
where Sz(t) := P(Ti(z) > t | Zi = z) is the survival curve for time t > 0 for any
individual i in the population assigned to treatment z ∈{0, 1}. The expectation is
taken with respect to the distribution induced by the simple random sampling from
the target superpopulation.
The ﬁnite population causal eﬀect of receipt of active treatment as compared to
control treatment for compliers is
τc =
1
Nπc
X
i:Ui=c

I(Ti(1) > t) −I(Ti(0) > t)

,
where πc is the proportion of compliers (as deﬁned in Equation 1.3.3). The corre-
sponding superpopulation estimand is
S1,c(t) −S0,c(t)
where Sz,c(t) := P(Ti(z) > t | Zi = z, Ui = c), for z ∈{0, 1} and t > 0. The terms
above can be interpreted as the intention-to-treat eﬀects of assignment to active treat-
ment versus control for complying patients. If, additionally, we make an exclusion
18

Chapter 1: Causal inference for time-to-event data with noncompliance
restriction for compliers2, we can attribute these eﬀects to the causal eﬀects of the
receipt of treatment, rather than only to the assignment.
There are analogous deﬁnitions of the above quantities for the rest of the compli-
ance behavior groups u ∈{a, n}, which we denote in general by
τu =
1
Nπu
X
i:Ui=u

I(Ti(1) > t) −I(Ti(0) > t)

,
for the ﬁnite population, and
S1,u(t) −S0,u(t)
for the superpopulation, with Sz,u(t) = P(Ti(z) > t | Zi = z, Ui = u), for z ∈{0, 1}
and t > 0. Because never-takers and always-takers are exposed to the same treat-
ment under either assignment, any comparisons deﬁned within these groups are only
informative about the eﬀects of treatment assignment.
The overall population estimands can be rewritten as a weighted average of the
latent group estimands. For example, for the superpopulation,
Sz(t) = πcSz,c(t) + πaSz,a(t) + πnSz,n(t),
and the overall intention-to-treat estimand of treatment eﬀect reduces to
S1(t) −S0(t).
2If Ui =c, Ti(1) = Ti(0) when Di(1) = Di(0) = d, for both levels of d ∈{0, 1}.
19

Chapter 1: Causal inference for time-to-event data with noncompliance
An analogous expression for the ﬁnite population overall treatment eﬀect is
τ(t) = πcτc(t) + πaτa(t) + πnτn(t).
1.3.2.3
Median lifetimes
Alternatively, we can deﬁne causal estimands in terms of the median times at
which active and control treatment groups have reached the event of interest. This
type of metric is common in the medical literature because it gives an idea of the
relative speed until the endpoint is reached, whereas hazard ratios don’t give direct
time-to-event information.
An example of a ﬁnite population causal estimand based on median times until
event is the ratio of the control treatment group median value to the active treatment
group median value,
RM = Mediani{Ti(1)}
Mediani{Ti(0)}.
Analogous estimands can be deﬁned within the compliance behavior groups, e.g., for
u ∈{c,a,n},
RMu = Mediani{Ti(1), i : Ui = u}
Mediani{Ti(0), i : Ui = u}.
1.3.2.4
Rate ratios
We introduce a third set of estimands related to the notion of person-time in-
cidence rates in the epidemiology literature. These depend on the total number of
events occurring at a certain point in time, and person-time, deﬁned as the amount
20

Chapter 1: Causal inference for time-to-event data with noncompliance
of time an individual is observed while free of disease.
For z ∈{0, 1} and t ≥0, deﬁne the following rate
R(z; t) = total number of disease-free survival events up to time t under assignment to z
total person-time years of follow up if assigned to z
.
Taking t = Emax, that is, the time at which the study ends, we may simplify the
notation and deﬁnitions to obtain
R(z)
=
N
X
i=1
Vi(z)
N
X
i=1
Yi(z)
=
1
N
N
X
i=1
Vi(z)
1
N
N
X
i=1
Yi(z)
= V (z)
Y (z),
where Vi(z) is as deﬁned in Equation 1.3.2.
The estimand of interest is a rate ratio, RR, which compares the rate under
assignment to the trial treatment versus the rate under assignment to the control.
The overall ﬁnite population rate ratio, which is deﬁned for the entire sample and
disregards information on noncompliance, is
RR = R(1)
R(0).
21

Chapter 1: Causal inference for time-to-event data with noncompliance
Analogously, we deﬁne the latent group-level estimands for u ∈{c,n,a} and z ∈{0, 1}
as
RRu = Ru(1)
Ru(0),
where
Ru(z) =
N
X
i:Ui=u
Vi(z)
N
X
i:Ui=u
Yi(z)
.
The complier rate ratio, RRc, is of most interest, because it can be interpreted as
a causal eﬀect of receipt of trial treatment versus control. For patients outside of the
complier group, these causal eﬀects are comparisons of assignment to trial treatment
versus control.
The overall rate for z ∈{0, 1} can be rewritten as a function of the compliance
behavior group-level rates
R(z)
=
πcV c(z) + πaV a(z) + πnV n(z)
πcY c(z) + πaY a(z) + πnY n(z)
=
X
u∈{c,n,a}
πuV u(z)
X
u∈{c,n,a}
πuY u(z)
.
It is clear that the overall rate ratio causal eﬀect will not in general reﬂect any
compliance behavior-level rate ratio causal eﬀects.
An important assumption for these types of estimands is that the rates should
22

Chapter 1: Causal inference for time-to-event data with noncompliance
be constant for the time period during which they are calculated; therefore, they are
well-suited to examples where exponential (or exponential regression) models are used
to describe the data. Under those distributional assumptions, the maximum likeli-
hood estimator for the rate (and, in the exponential case, also the hazard) parameter
is of the form bR(z), and the rate ratio would give the ratio of the hazards between
the active and control treatment groups. However, we advocate that the choice of
estimands should be driven by quantities of interest ﬁrst, and not the choice of model.
1.3.3
Observed outcomes
Only one potential outcome per individual – the one corresponding to the assigned
treatment – can ever be realized and thus observed; this is the fundamental problem
faced by inference for causal eﬀects (Rubin, 1978). For patient i, let Zi represent the
treatment assignment: Zi = 1 for those assigned the active treatment, and Zi = 0 for
those assigned the control treatment.
The outcome Ti(Zi) := Zi Ti(1) + (1 −Zi) Ti(0) is the realized survival time,
which will only be observed if the speciﬁed event occurs before the censoring time,
that is, if Ti(Zi) ≤Ci.
Therefore the observed outcomes for patient i related to
survival can be summarized through the pair (Y obs
i
, V obs
i
) := (Yi(Zi), Vi(Zi)), where
Yi(Zi) = Zi Yi(1) + (1 −Zi) Yi(0) and Vi(Zi) = Zi Vi(1) + (1 −Zi) Vi(0)); Yi(z) and
Vi(z) for z ∈{0, 1} were deﬁned in Equations 1.3.1-1.3.2. Additionally, the observed
indicator of treatment received is Dobs
i
= Di(Zi).
23

Chapter 1: Causal inference for time-to-event data with noncompliance
Because we cannot observe both Di(0) and Di(1), there is not enough information
to determine the compliance behavior for each patient.
There are four observed
groups, determined by all possible combinations of treatment assignment and observed
treatment received. Denote by
O(z, d) = {i : Zi = z, Dobs
i
= d}
the group of patients assigned to treatment z ∈{0, 1} who received treatment
d ∈{0, 1}.
Each patient belongs to one of four observed groups, and also to at
least one latent group. The possible memberships to compliance behavior group im-
plied by membership to each of the four observed groups are displayed in Table 1.2.
Table 1.2: Observed and latent groups.
Assigned
Received
O(Zi, Dobs
i
)
Ui
Active treatment
Active treatment
O(1, 1)
c or a
Control treatment
O(1, 0)
n or d
Control treatment
Active treatment
O(0, 1)
a or d
Control treatment
O(0, 0)
n or c
1.4
Parametric model speciﬁcation
We model the potential outcomes for survival times using probability distribu-
tions, and incorporate noncompliance into the conventional survival parametric anal-
ysis techniques. Furthermore, we use Bayesian methods based on multiple imputation
24

Chapter 1: Causal inference for time-to-event data with noncompliance
for inference. Table 1.3 summarizes our notation.
Our preference to model the survival times parametrically is motivated by some
advantages of parametric models, such as the fact that they are more interpretable,
they allow for simple incorporation of covariates, and they can be used for forecast-
ing. The following conversation with David Cox, from Reid (1994), highlights that
the advantages of using parametric models may trump their main drawback, which
in general is argued to be model misspeciﬁcation.
Reid: How do you feel about the cottage industry that’s grown up around
[the Cox model]?
Cox: In the light of some of the further results one knows since, I think I
would normally want to tackle problems parametrically, so I would
take the underlying hazard to be a Weibull or something. I’m not
keen on nonparametric formulations usually.
Reid: So if you had a set of censored survival data today, you might rather
ﬁt a parametric model, even though there was a feeling among the
medical statisticians that that wasn’t quite right.
Cox: That’s right, but since then various people have shown that the an-
swers are very insensitive to the parametric formulation of the un-
derlying distribution.
1.4.1
Models for complete data
We treat the unobserved indicator of treatment received, Di(1 −Zi), and the un-
observed potential outcome of the clinical endpoint of interest, Ti(1 −Zi), as missing
data (as per the fundamental problem of causal inference). Our estimation strategy
multiply imputes the missing data using the posterior predictive distribution from a
25

Chapter 1: Causal inference for time-to-event data with noncompliance
Table 1.3: Notation.
Zi
treatment assignment (binary)
Xi
covariates (K-dimensional column vector)
Di(z)
treatment received (binary)
Ui
compliance type (complier, always taker, never taker)
Ti(z)
survival time (positive)
Ci(z)
censoring time (positive)
Yi(z) = min(Ti(z), Ci(z))
possibly censored survival time
Vi(z) = I{Ti(z) ≤Ci(z)}
indicator for censoring
f
PDF
F
CDF
S
survival function
β’s
parameters in compliance status model
γ’s
parameters in survival times model
Bayesian parametric model.
The complete data for each patient comprises (Zi, Ti(1), Ti(0), Ui), where Ui is the
latent compliance type. Given a vector of pre-treatment covariates Xi and the vector
of all parameters θ, as well as the censoring time Ci, the complete-data likelihood
factorizes as
p(Zi, Ti(0), Ti(1), Ui | Xi, Ci, θ) = p(Zi | Ti(0), Ti(1), Ui, Xi, Ci, θZ)
× p(Ti(0), Ti(1) | Ui, Xi, Ci, θT)
× p(Ui | Xi, Ci, θU),
where θ = (θZ, θU, θT), and θZ, θU, and θT are distinct. We consider each factor
individually.
26

Chapter 1: Causal inference for time-to-event data with noncompliance
Note that in our example, the censoring times are ﬁxed. In general, the assump-
tion of an independent censoring mechanism, in conjunction with a set of parameters
that drive it which are distinct from those of the survival times, make it unnecessary
to include the model for the censoring time distribution because it is ignorable for
inference.
1.4.1.1
Treatment assignment model
Because we are considering randomized trials, the treatment assignment is uncon-
founded and doesn’t depend on any unknown parameters, that is,
p(Zi | Ti(0), Ti(1), Ui, Xi, Ci, θZ) = p(Zi).
1.4.1.2
Compliance status model
Assuming monotonicity (no deﬁers), patients can have one of three compliance
types. We model the conditional distribution of the compliance variable Ui given the
pretreatment covariates Xi with a multinomial logistic regression:
πu,i = Pr(Ui = u | Xi, θU) =
exp(βu + X⊤
i βu,X)
X
u′∈{c,a,n}
exp(βu′ + X⊤
i βu′,X)
,
for u ∈{c,a,n}. The complier group is taken as the baseline; that is, βc = 0 and
βc,X = 0. We thus have θU = (βa, βn, βa,X, βn,X).
27

Chapter 1: Causal inference for time-to-event data with noncompliance
1.4.1.3
Survival endpoint model
The Weibull distribution with shape parameter α > 0 and scale parameter σ > 0
has PDF
f(t; α, σ) := Weibull(t; α, σ) = α
σ
 t
σ
α−1
e−(t/σ)α
(1.4.1)
and CDF
F(t; α, σ) := P(T < t; α, σ) = 1 −e−(t/σ)α.
(1.4.2)
Conditional on covariates and compliance group, we model the underlying poten-
tial times under assignment z ∈{0, 1} using a Weibull regression:
Ti(z) | Ui = u, Xi, θT ∼Weibull(αz,u,i, σz,u,i)
where we assume that the shape parameter αz,u,i ≡α is the same for all patients, and
we introduce covariates through the scale parameter:
−log(σz,u,i) = γz,u + X⊤
i γz,u,X.
Note that we take γz,u,X = γX for simplicity, for all assigned treatment and latent
compliance combinations. A more complex model might allow for interactions be-
tween covariates, treatment assignment, and possibly latent compliance types. Under
these speciﬁcations, θT = (α, γ0,c, γ0,a, γ0,n, γ1,c, γ1,a, γ1,n, γX).
We make additional assumptions about the survival potential outcomes.
Assumption 1.4.1. Stochastic exclusion restrictions for always-takers and for never-
28

Chapter 1: Causal inference for time-to-event data with noncompliance
takers. In terms of potential outcomes, these state that Ti(1) and Ti(0), for patients i
such that Ui ∈{a,n}, follow the same probability distribution. These assumptions rule
out an eﬀect of the treatment assignment on the outcome of interest for those patients
for whom the treatment received does not change with the change in assignment.
These assumptions are often controversial because they cannot be guaranteed by
physical randomization, and therefore their appropriateness for a speciﬁc problem
relies on subject-matter knowledge (Imbens and Rubin, 2015). It is reasonable to
justify them in double-blind trials, but in the IBCSG study the treatments are in-
vasive procedures and their nature cannot be hidden from the patients or doctors
(i.e. blinding is impossible). We might be suspicious of the exclusion restriction for
always-takers (never-takers) in the IBCSG trial if the mere assignment to axillary
dissection (no axillary dissection) would lead to special care not generally received in
the active (control) treatment arm. We have no reason to believe that this happened
or that they are cause for concern; otherwise, sensitivity analyses could be performed
to assess whether the assumptions seem appropriate.
The exclusion restrictions are not necessary for the Bayesian analysis, but they
do facilitate inference. In order to relax these assumptions, we could incorporate
diﬀerent restrictions through the prior distributions on the parameters; however, we
didn’t have access to adequate information to exploit in the prior speciﬁcation, and
therefore we use these exclusion restrictions.
We enforce the exclusion restrictions for always-takers and never-takers through
29

Chapter 1: Causal inference for time-to-event data with noncompliance
the parameters in the model, i.e., γa := γ1,a = γ0,a and γn := γ1,n = γ0,n.
1.4.2
Multiple imputation of missing potential outcomes
1.4.2.1
Observed data likelihood
Denote by f and F the PDF and CDF, respectively, of a survival time random
variable T. The observed data for individual i are the vector (Zi, Dobs
i
, Y obs
i
, V obs
i
).
With right censored data (and under Assumption 1.3.1, independent censoring), the
contribution to the likelihood associated with individual i is of the form f(Y obs
i
; α, σ)
if the exact survival time is observed, and 1 −F(Y obs
i
; α, σ) if the observation is
censored, i.e.
Li(α, σ | Y obs
i
, V obs
i
) = f(Y obs
i
; α, σ)V obs
i
· (1 −F(Y obs
i
; α, σ))1−V obs
i
.
For simplicity of notation, we deﬁne
g(y, v; α, σ) := f(y; α, σ)v · (1 −F(y; α, σ))1−v
as the individual likelihood contribution evaluated at (y, v) with a given shape α > 0
and scale σ > 0, for any y > 0 and v ∈{0, 1}.
Denote the observed data for all individuals by the pair of N-dimensional column
vectors (Yobs, Vobs). The observed data likelihood is decomposed into four combina-
tions of treatment assignment and observed treatment received (as describe in Table
1.2). Under monotonicity, it is possible to identify the patients assigned to the active
30

Chapter 1: Causal inference for time-to-event data with noncompliance
treatment but who actually received the control treatment as never-takers (because
we rule out the existence of deﬁers). Similarly, patients assigned to the control treat-
ment but who receive the active treatment can be identiﬁed as always-takers. In
contrast, the stratum membership for the rest of the patients cannot be uniquely
identiﬁed from the observed data, and they contribute a ﬁnite mixture probability
with two components to the likelihood. Therefore, the observed data likelihood is
given by the following expression:
L(θ | Z, Yobs, Vobs, Dobs, X) =
Y
i∈O(1,1)

πc,i · g(Y obs
i
, V obs
i
; α, σ1,c,i) + πa,i · g(Y obs
i
, V obs
i
; α, σ1,a,i)

×
Y
i∈O(1,0)

πn,i · g(Y obs
i
, V obs
i
; α, σ1,n,i)

×
Y
i∈O(0,1)

πa,i · g(Y obs
i
, V obs
i
; α, σ0,a,i)

×
Y
i∈O(0,0)

πn,i · g(Y obs
i
, V obs
i
; α, σ0,n,i) + πc,i · g(Y obs
i
, V obs
i
; α, σ0,c,i)

1.4.2.2
Prior distributions and posterior sampling
We used diﬀuse proper priors for all the parameters to ensure a proper posterior
distribution. Namely, we used standard Normal priors on all the multinomial logit
regression parameters θU. Likewise, we used standard Normal priors on the Weibull
regression parameters θT with the exception of the shape parameter, α, for which we
used an independent Gamma(1, 1) prior distribution. Varying these values did not
have an inﬂuence in the posteriors.
31

Chapter 1: Causal inference for time-to-event data with noncompliance
All posterior samples of the parameters were drawn using STAN software (Stan
Development Team, 2013). Four chains of 20,000 iterations were run simultaneously
in order to assess convergence; all potential scale reduction factors were below 1.001
for all parameters Gelman and Rubin (1992). In particular, the posterior sampling
for the θU vector of parameters is done via a weighted Bayesian multinomial logit re-
gression. Similarly, the parameters θT can be obtained via weighted Bayesian Weibull
regression (it is possible to implement a Gibbs sampler because the conditional pos-
terior distributions are log-concave).
1.4.2.3
Superpopulation estimation
We can obtain superpopulation estimates of the survival curves by plugging in
the posterior draws of the parameters into the explicit form of the survival function
of the Weibull distribution. We can then obtain the diﬀerences in proportions of
survivors (within compliance groups, and overall) by taking the diﬀerences between
the survival curves.
The median of a Weibull(α, σ) distribution is given by
t∗= σ · (log 2)1/α.
Again, superpopulation estimates are obtained by substituting the posterior draws of
the parameters into the expression above.
In the case of the rate ratios, if an Exponential model is appropriate (and therefore,
32

Chapter 1: Causal inference for time-to-event data with noncompliance
the assumption of constant rates), the rate ratio superpopulation estimator will be
the ratio of the rate parameters of the Exponential distributions corresponding to the
active treatment and control treatment.
1.4.2.4
Finite population estimation
We multiply impute the missing potential outcomes given the observed data and
a posterior sample of the parameters (Rubin, 1987). First, we impute the compli-
ance behavior (for those patients who could belong to two possible groups). Next,
given the imputed compliance status for each patient (and the posterior sample of
the parameters), we impute the missing potential outcome of survival time under
the alternative treatment. Once all missing data are imputed, we have a posterior
predictive sample of the complete data, from which it is straightforward to calculate
all estimands of interest. Each fully imputed dataset leads to one posterior draw of
the estimands given the observed data.
Details for the imputation process require the complete data likelihood. We take
the compliance behavior indicator U = (U1, . . . , UN) to be the missing latent data.
The complete data loglikelihood function is:
l(θ | Z, Yobs, Vobs, Dobs, X, U) =
X
i∈O(1,1)
I(Ui = c)

log(πc,i) + log g(Y obs
i
, V obs
i
; α, σ1,c,i)

+
X
i∈O(1,1)
I(Ui = a)

log(πa,i) + log g(Y obs
i
, V obs
i
; α, σ1,a,i)

33

Chapter 1: Causal inference for time-to-event data with noncompliance
+
X
i∈O(1,0)
log g(Y obs
i
, V obs
i
; α, σ1,n,i)
+
X
i∈O(0,1)
log g(Y obs
i
, V obs
i
; α, σ0,a,i)
+
X
i∈O(0,0)
I(Ui = c)

log(πc,i) + log g(Y obs
i
, V obs
i
; α, σ0,c,i)

+
X
i∈O(0,0)
I(Ui = n)

log(πn,i) + log g(Y obs
i
, V obs
i
; α, σ0,n,i)

Because given the parameter θ all patients’ compliance behaviors and missing
potential outcomes are independent of each other, we focus on imputing the missing
data for patient i, i ∈{1, . . . , N}. Given a posterior draw of the parameters, θ(j),
and the observed data, we ﬁrst draw the compliance status Ui.
We consider the four possible observed combinations of assigned treatment (Zi)
and observed treatment received (Dobs
i
). If i ∈O(1, 0), then
Pr(Ui = u | θ(j), Z, Yobs, Vobs, Dobs, X) =









1 if u = n
0 otherwise.
If i ∈O(0, 1), then
Pr(Ui = u | θ(j), Z, Yobs, Vobs, Dobs, X) =









1 if u = a
0 otherwise.
34

Chapter 1: Causal inference for time-to-event data with noncompliance
If i ∈O(1, 1), then
Pr(Ui = u | θ(j), Z, Yobs, Vobs, Dobs, X) ∝

















πc,i · g(Y obs
i
, V obs
i
; α, σ1,c,i) if u = c
πa,i · g(Y obs
i
, V obs
i
; α, σ1,a,i) if u = a
0
otherwise.
If i ∈O(0, 0), then
Pr(Ui = u | θ(j), Z, Yobs, Vobs, Dobs, X) ∝

















πc,i · g(Y obs
i
, V obs
i
; α, σ0,c,i) if u = c
πn,i · g(Y obs
i
, V obs
i
; α, σ0,n,i) if u = n
0
otherwise.
We then draw the missing potential outcome, Ti(1 −Zi), from a Weibull distri-
bution with shape parameter α and scale σ1−z,u,i. This is because given Ui = u and
Zi = z for u ∈{c, a, n} and z ∈{0, 1}, by our conditional independence assumption
on the joint distribution of the potential outcomes Ti(0) and Ti(1), the missing po-
tential outcome does not depend on the observed potential outcome, Ti(Zi).
In the case of Ti(Zi), it is only observed if it was not censored, and therefore for
individuals i such that V obs
i
= 0, we draw Ti(Zi) from its conditional distribution
given that Ti(Zi) > Ci. In the case of the Weibull distribution, the CDF of this
conditional distribution is given by
Ft|T>Ci(t | Ti(Zi) > Ci) = F(t) −F(Ci)
1 −F(Ci) ,
35

Chapter 1: Causal inference for time-to-event data with noncompliance
where F on the right-hand side is the CDF of a Weibull(α, σz,u,i) distribution. It is
straightforward to sample from this truncated Weibull distribution (a simple inverse
CDF method works well).
Finally, for each sample of the posterior distribution of the parameters, the esti-
mands of interest (e.g. the survival curves, median lifetimes, etc.) can be computed
as well because they are functions of the potential outcomes.
1.5
IBCSG trial 23-01 results
In this section, we present the results from the analysis of the IBCSG trial 23-01
of axillary dissection in breast cancer patients, introduced in Section 1.2, using the
modeling approach from Section 1.4. Summaries of the posterior distributions for all
parameters in the models can be found in Appendix A. Note that the shape param-
eter for the Weibull distributions is estimated to be 1, and therefore the model can
be simpliﬁed to an Exponential model.
Tables 1.4 and 1.5 show summaries of posterior distributions of selected ﬁnite pop-
ulation estimands. As reported in Table 1.4, we estimate that 92.1% of the sample
are compliers, 4.4% are always-takers, and the remaining 3.5% are never-takers.
The proportion of complier patients that haven’t experienced an invasive relapse,
second primary tumor, or death up to ﬁve years from randomization to active treat-
ment (i.e., no axillary dissection) is estimated to be 93%, whereas if randomized to
36

Chapter 1: Causal inference for time-to-event data with noncompliance
Table 1.4: Summaries of posterior distributions for selected estimands. The estimands
related to survival curves are evaluated at 5 years from randomization. IBCSG data.
Mean
SD
Median
2.5th perc.
97.5th perc.
πc
0.921
0.012
0.922
0.894
0.943
πa
0.044
0.009
0.043
0.027
0.065
πn
0.035
0.008
0.034
0.021
0.053
Surv1C(5)
0.930
0.013
0.931
0.904
0.953
Surv0C(5)
0.882
0.019
0.883
0.842
0.919
Surv1A(5)
0.886
0.056
0.889
0.763
0.977
Surv0A(5)
0.759
0.084
0.765
0.585
0.909
Surv1N(5)
0.883
0.071
0.889
0.730
1.000
Surv0N(5)
0.811
0.095
0.821
0.600
0.964
τc(5)
0.047
0.016
0.047
0.017
0.080
τa(5)
0.127
0.091
0.125
-0.056
0.304
τn(5)
0.072
0.107
0.067
-0.116
0.304
τ(5)
0.051
0.016
0.050
0.022
0.083
the control treatment (i.e., axillary dissection) that proportion is estimated to be
88.2%. Therefore, the diﬀerence in these survival proportions attributed to receiving
no axillary dissection is 4.7%. For always-takers and never-takers, the diﬀerences
in survival proportions at ﬁve years are due to assignment to no axillary dissection
(vs. dissection), but due to our exclusion restrictions they are estimated to be zero
(see posterior intervals). Finally, we obtain the overall ITT eﬀect of assignment to
active treatment on survival proportions at ﬁve years from randomization to be 5.1%.
Even though this estimate is statistically signiﬁcant and the active treatment appears
better, the fact that this trial had a non-inferiority design requires comparing it to
a non-inferiority margin (that should be established in advance for this type of esti-
mand) in order to interpret its practical signiﬁcance.
Figure 1.2 shows the ﬁnite population complier causal diﬀerence in survival pro-
37

Chapter 1: Causal inference for time-to-event data with noncompliance
portions, τc(t), and Figure 1.3 shows the overall ITT ﬁnite population diﬀerence in
survival proportions, τ(t), as functions of time, together with their corresponding 95%
posterior intervals. Numerical summaries associated to these ﬁgures can be found in
Tables A.3-A.4 in Appendix A. These two ﬁgures are very similar to each other, be-
cause the majority of patients belonged to the complier behavior group. We observe
that the CACE increases with time, as do the variance bounds; their numerical values
are quite small, indicative of small practical signiﬁcance.
0
1
2
3
4
5
6
0.00
0.02
0.04
0.06
0.08
Complier causal difference in survival proportions
time (years)
τc(t)
τc(t)
95% interval
Figure 1.2: Finite population complier causal eﬀect (diﬀerence in survival propor-
tions) as a function of time. Dotted lines represent the 95% posterior intervals. The
estimate provided in Table 1.4 corresponds to the vertical line at t = 5.
38

Chapter 1: Causal inference for time-to-event data with noncompliance
0
1
2
3
4
5
6
0.00
0.02
0.04
0.06
0.08
ITT difference in survival proportions
time (years)
τ(t)
τ(t)
95% interval
Figure 1.3: Finite population diﬀerence in survival proportions as a function of time,
as randomized (ITT). Dotted lines represent the 95% posterior intervals. The estimate
provided in Table 1.4 corresponds to the vertical line at t = 5.
The causal ratio of median lifetimes for compliers, RMc, was estimated to be 1.677
(see Table 1.5). We conclude that the (uncensored) survival times seem stochastically
larger for those patients assigned to the active treatment group of no axillary dissec-
tion than for those assigned to the control treatment group. As expected, the ratio
of medians for the entire ITT population was very similar to that for the compliers.
These results are consistent with our previous conclusions pertaining to survival curve
estimates.
39

Chapter 1: Causal inference for time-to-event data with noncompliance
The interpretation for the rate ratios works well in this particular example because
the model reduces to an exponential regression3; thus, the hazard rates are constant
and the required assumption from Section 1.3.2.4 is met. The rate ratio for compli-
ers, RRc, was estimated to be 0.456 (see Table 1.5). This means that the rate of
disease-related events within the compliers is larger for those assigned to the control
treatment group than those assigned to the active treatment group. Again, the rate
ratio estimate for the entire ﬁnite population was very close to the one for compliers,
and the conclusions are consistent with those obtained for the other estimands of
interest.
Table 1.5: Summaries of posterior distributions for selected estimands: ratios of
median lifetimes and rate ratios (for compliers and for ITT population).
IBCSG
data.
Mean
SD
Median
2.5th perc.
97.5th perc.
RMc
1.677
0.086
1.673
1.513
1.837
RM
1.717
0.078
1.722
1.570
1.864
RRc
0.456
0.020
0.456
0.417
0.497
RR
0.451
0.017
0.450
0.419
0.484
Table 1.6 summarizes the posterior distributions of the covariates within compli-
ance behavior groups. We did not ﬁnd very large diﬀerences in the covariate distri-
butions; however, some features do stand out. For example, the always-takers (those
who never undergo axillary dissection regardless of assignment), have lower levels of
estrogen receptors on average. Having a lower estrogen receptor status is indicative of
a worse prognosis: having a positive status means that it is possible for the patients
to receive hormone therapy treatment that inhibits the estrogen receptors implicated
3The estimated shape parameter for the Weibull distribution, α, is 1.
40

Chapter 1: Causal inference for time-to-event data with noncompliance
in the growth of breast cancer. Always-takers also have a larger tumor grade, which
is usually associated with lower survival rates. One could also arguably say that they
have a smaller tumor size as compared to compliers (and axillary dissection is gener-
ally recommended for larger tumors, in general larger than 2.5 cm). So overall, this is
indicative of the always-takers generally having a worse prognosis. The never-takers
undergo a mastectomy more often than the other two groups, which is also indicative
(to a less conclusive extent) of a worse prognosis. This also suggests that it is more
common for patients to undergo an axillary dissection if they have already had a
mastectomy.
Table 1.6: Summaries of the posterior distribution of each covariate within compliance
group. IBCSG data.
Covariate
Status
Med
2.5Q
97.5Q
Estrogen receptor status
Compliers
0.902
0.898
0.909
Always takers
0.818
0.692
0.897
Never takers
1.000
0.841
1.000
Progesterone receptor status
Compliers
0.760
0.753
0.770
Always takers
0.727
0.571
0.842
Never takers
0.667
0.500
0.806
Menopausal status
Compliers
0.560
0.551
0.570
Always takers
0.500
0.344
0.659
Never takers
0.585
0.406
0.750
Tumor grade
Compliers
2.051
2.037
2.064
Always takers
2.400
2.194
2.587
Never takers
2.100
1.846
2.353
LTM
Compliers
0.910
0.905
0.917
Always takers
1.000
0.932
1.000
Never takers
0.739
0.567
0.857
Tumor size
Compliers
1.383
1.373
1.392
Always takers
1.148
1.057
1.313
Never takers
1.304
1.160
1.559
We additionally perform an analysis without covariates, in order to provide a
41

Chapter 1: Causal inference for time-to-event data with noncompliance
comparison with the original results obtained from the Kaplan-Meier estimates of the
survival curves for the intention-to-treat population (i.e., as randomized), originally
presented in Figure 1.1. After obtaining the posterior samples for the parameters in
the model, we calculate the superpopulation estimates of the survival curves under
control and active treatment assignment by using the deﬁnition of the Weibull sur-
vival function. Figure 1.4 shows the aforementioned comparison of the Kaplan-Meier
estimates vs. the curves estimated with the Weibull modeling approach. As expected,
the curves estimated through the model are smoother, but similar conclusions can be
reached.
0
1
2
3
4
5
6
0
20
40
60
80
100
ITT Kaplan−Meier Survival Curves
Time from randomization (years)
Disease free survival (%)
Treatment
No axillary dissection (z=1)
Axillary dissection (z=0)
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
Estimated ITT survival curves
time (years)
% DFS
Treatment assignment
No axillary dissection (z=1)
Axillary dissection (z=0)
Figure 1.4: Kaplan-Meier (left) vs Weibull model (right) estimated superpopulation
survival curves for the ITT population. IBCSG trial.
Figure 1.5 shows the superpopulation estimates of the survival curves by compli-
ance group. Note that there is only one estimated curve for always-takers and never
42

Chapter 1: Causal inference for time-to-event data with noncompliance
takers, respectively, because of the exclusion restrictions. This plot supports our pre-
vious statements suggesting that the always-takers and never-takers tend to have, in
general, worse prognoses than the compliers.
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
Estimated survival curves
time (years)
% DFS
Compliance group
compliers (active)
compliers (control)
always-takers
never-takers
Figure 1.5: Superpopulation survival curves within compliance behavior types, esti-
mated using the Weibull model. IBCSG trial.
1.6
Connection with PH and AFT formulations
The Weibull distribution is very ﬂexible for many patterns of data because the
associated hazard function changes with the shape parameter. It also has simple,
closed forms for the PDF, survival function, and hazard function. Furthermore, it is
the only parametric family that has an interpretation as both a proportional hazards
43

Chapter 1: Causal inference for time-to-event data with noncompliance
(PH) regression model and an accelerated failure time (AFT) model: therefore one
can estimate the model in terms of the hazards or the survival times, and reproduce
equivalent results from diﬀerent parameterizations.
The PH and AFT are two fundamental models commonly used to describe the
way that some factor might aﬀect a time to event outcome. In this section, we brieﬂy
introduce the PH and AFT formulations and relate them to our proposed modeling
approach.
1.6.1
Proportional hazards models
Proportional hazards models are a class of survival models that consider the in-
stantaneous risk of failure at each time, or hazard, among subjects who have not
failed. In particular, the “proportional hazards” assumption requires that the ratio
of the hazards for two individuals be constant over time, and it is conventional to as-
sume that the eﬀect of covariates is multiplicative with respect to the baseline hazard
function. Thus, the general formulation speciﬁes a hazard function at time t for an
individual with observed covariate vector x, as
h(t | x) = h0(t) exp{x′β}
where h0(t) is the underlying hazard function that describes how the risk of event
per time unit changes over time at baseline levels of covariates, and the second term
describes how the hazard varies in response to explanatory covariates (Ibrahim et al.,
2001).
44

Chapter 1: Causal inference for time-to-event data with noncompliance
By specifying the baseline hazard function to follow a particular parametric form,
parametric proportional hazards models are obtained.
The hazard function of a Weibull distribution with shape parameter α and scale
σ, under the parameterization introduced in 1.4.1, is given by
h(t; α, σ) =
f(t; α, σ)
1 −F(t; α, σ) = α
σ
 t
σ
α−1
.
In this same general context, if the times to event for subjects in groups z ∈{0, 1}
follow a Weibull(α, σz) distribution, the ratio of hazard functions is then
h1(t)
h0(t) =
σ0
σ1
α
.
To make the connection with our formulation from Section 1.4, recall the setup for
the time-to-event potential outcomes, given the latent compliance type and observed
covariates:
Ti(z) | Ui = u, Xi, θT ∼Weibull(α, σz,u,i)
where σz,u,i = exp{−γz,u −X⊤
i γX}.
For any speciﬁc patient, and given her compliance type u ∈{c, a, n}, the ratio of
the hazard at time t under assignment to active treatment vs. assignment to control
45

Chapter 1: Causal inference for time-to-event data with noncompliance
treatment can be deﬁned as
h1u(t)
h0u(t)
=
σ0,u,i
σ1,u,i
α
= (exp{γ1,u −γ0,u})α.
This complies with the proportional hazards assumption (the hazard ratio doesn’t
depend on time) because we assumed a common shape parameter α for the diﬀerent
groups. Because of the parameterization that we used to introduce the covariates, we
obtain this simple expression; however, more complicated parameterizations can be
used that, for example, allow for interactions between compliance type and covariates,
and similar conclusions can be attained.
In the proportional hazards modeling approach, the hazard ratio is a function of
a parameter of interest in the model:
h1(t) = h0(t) · eψ.
Under our proposed (simple) model, we can obtain compliance group hazard ratios,
by exponentiating the parameter ψu = α · (γ1,u −γ0,u) for u ∈{c,a,n}. Of course
under the exclusion restrictions, the only one that is non-zero is the complier hazard
ratio. Therefore, under our modeling approach one can still give a proportional haz-
ards interpretation and hazard ratios can be estimated, if of interest.
For example, we estimate the hazard ratio for compliers for the IBCSG data by
using the posterior distributions of the parameters (see Appendix A). Because the
46

Chapter 1: Causal inference for time-to-event data with noncompliance
estimand of interest is just a function of parameters in the model, we obtain its
posterior distribution as well. The posterior mean of the complier hazard ratio is
given by
[
HRc = \
exp ψc = 0.919,
with 95% posterior interval (0.755, 1.102). Since the interval contains the value 1,
we can conclude that there is no signiﬁcant diﬀerence in terms of the hazards across
treatment assignment groups for the compliers.
1.6.2
Accelerated failure time models
Accelerated failure time models are a parametric class of survival models in which
the covariates are assumed to have a multiplicative eﬀect on the survival times. These
include the parametric exponential, Weibull, logistic, lognormal, and loglogistic mod-
els, for example. The general formulation speciﬁes that the hazard function at time
t for an individual with observed covariate vector x, relates to an underlying hazard
function h0(t) through the following expression:
h(t | x) = h0(t exp{−x′β}) exp{−x′β}.
An equivalent formulation can be posed in terms of the survival times themselves
through the relationship
T = exp{−x′β}T0,
where T0 has a baseline hazard function h0(t) independent of β, and ζ = exp{−x′β}
is the so-called acceleration factor (Kalbﬂeisch and Prentice, 2002).
47

Chapter 1: Causal inference for time-to-event data with noncompliance
To draw an analogy to our original model speciﬁcation, we use the convolution
property of Weibull distributions:
Property 1.6.1. If T = T0eζ with T0 ∼Weibull(α0, σ0), then T ∼Weibull(α0, σ0eζ).
Proof.
The survival function for T is given by
ST(t) = P(T > t) = P(T0eζ > t) = P(T0 > te−ζ) = exp

−
te−ζ
σ0
α0
,
which corresponds to the survival function of a Weibull(α0, σ0eζ) distribution.
We now translate the AFT formulation into our potential outcomes framework,
and obtain an equivalence with the model posed in Section 1.4. Conditional on Ui = u,
for u ∈{c,a,n}, we interpret the strong assumption from AFT models as the following
relationship between the time-to-event potential outcomes under assignment to active
treatment and control:
Ti(1)
Ti(0) = e−ζu.
By combining this restriction with our setup from Section 1.4, and using Property
1.6.1, we obtain
Ti(0)
∼
Weibull(α, σ0,u,i)
Ti(1)
=
Ti(0) · e−ζu ∼Weibull(α, σ0,u,i · e−ζu).
48

Chapter 1: Causal inference for time-to-event data with noncompliance
Because by deﬁnition Ti(1) ∼Weibull(α, σ1,u,i), through some simple algebraic manip-
ulations we obtain that ζu = γ1,u −γ0,u, and thus the acceleration factor is exp{−ζu}.
Again, with diﬀerent parameterizations that might allow an interaction between co-
variates and treatment assignment group, for example, we can obtain a diﬀerent form
of the acceleration factor that involves the covariates.
Even though we originally modeled the potential outcomes for a given patient
as independent from each other (given Ui and their own parameters), the likelihood
doesn’t change because we don’t observe information on the correlation between po-
tential outcomes. Therefore, we can achieve an AFT interpretation by performing
the reparameterization described above, and use the same inferential procedure that
we proposed in Section 1.4 to estimate the acceleration factor, if of interest.
For example, we estimate the acceleration factor for compliers for the IBCSG data
by using its posterior distribution. The posterior mean of the acceleration factor for
compliers is
\
exp{−ζc} = 1.095,
with 95% posterior interval (0.909, 1.316). Since the acceleration factor is larger than
1, the endpoint would be reached more slowly for compliers in the active treatment
group than in the control treatment group; however, since the interval contains the
value 1, this diﬀerence is not statistically signiﬁcant.
49

Chapter 1: Causal inference for time-to-event data with noncompliance
1.7
Relaxing the assumption of independent cen-
soring
Up to this point, we have assumed an independent censoring mechanism which,
as brieﬂy discussed in Section 1.3.1.1, is ignorable for inference. However, if censoring
is created by loss to follow-up, or other potentially selective reasons, such as sicker
patients withdrawing from the study more often, additional considerations have to
be made regarding the censoring mechanism.
A ﬁrst approach could be to assume independence between the censoring and the
survival times conditionally on observed covariates and observed treatment received,
i.e.,
Ti ⊥⊥Ci | Zi, Xi, Dobs
i
.
This statement is analogous to the missing at random (MAR, Little and Rubin (2002))
assumption introduced in the context of missing data mechanisms.
However, it might seem more plausible to assume independence between the cen-
soring and survival times only conditionally on observed covariates and latent com-
pliance status. This assumption was introduced by Frangakis and Rubin (1999) with
the name of latent ignorability, and can be summarized as:
Ti ⊥⊥Ci | Zi, Xi, Ui
50

Chapter 1: Causal inference for time-to-event data with noncompliance
i.e., Pr(Ti, Ci | Zi, Xi, Ui) = Pr(Ti | Zi, Xi, Ui) · Pr(Ci | Zi, Xi, Ui).
Under this second scenario, the censoring mechanism is no longer ignorable: censoring
and survival times are correlated because the compliance status is only partially
observed. This formulation also warrants additional assumptions for the censoring
and survival times, such as compound exclusion restrictions for always-takers and
never-takers:



Ti(0)
Ci(0)


=



Ti(1)
Ci(1)



if i ∈{a,n}.
With a parametric approach, the next question would be how to model the cen-
soring mechanism, since it is now non-ignorable for inference. Even though there is
no consensus on how to model non-ignorable censoring mechanisms, it is straightfor-
ward to incorporate a model into our framework. For example, because the complete
data for patient i is now given by (Zi, Ti(0), Ti(1), Ci(0), Ci(1), Ui), the complete data
likelihood can be written as the following product:
p(Zi, Ti(0), Ti(1), Ci(0), Ci(1), Ui | Xi, θ)
=
p(Zi)
×
p(Ti(0), Ti(1) | Ui, Xi, θT)
×
p(Ci(0), Ci(1) | Ui, Xi, θC)
×
p(Ui | Xi, θU),
where we assume that θ = (θU, θT, θC), and θU, θT, and θC are distinct.
Denote by d and D the PDF and CDF of censoring time Ci; f and F denote
51

Chapter 1: Causal inference for time-to-event data with noncompliance
the PDF and CDF of survival time Ti, as before. The contribution to the observed
likelihood associated with individual i under non-ignorable censoring is:
(i) f(Y obs
i
; θt) · (1 −D(Y obs
i
; θc)) if the exact survival time is observed, and
(ii) (1 −F(Y obs
i
; θt)) · d(Y obs
i
; θc) if the observation is censored.
That gives the observed likelihood below:
Li(θt, θc | Y obs
i
, V obs
i
)
=

f(Y obs
i
; θt)(1 −D(Y obs
i
; θc))
V obs
i
×

(1 −F(Y obs
i
; θt))d(Y obs
i
; θc)
1−V obs
i
A Weibull regression might be a reasonable parametric model for the censoring
times. However, because of the lack of consensus about the modeling, and because
with the observed data it is impossible to verify whether the assumption of indepen-
dent censoring (even conditionally) is true, there is a need for more sensitivity studies
to validate the plausibility of the assumptions and adequacy of the parametric models
used.
1.8
Conclusions
We described the problem of noncompliance in the speciﬁc context of right-
censored survival data. The approach inspired on the idea of instrumental variables
clariﬁes the diﬃculties for analysis; it focuses on the subpopulation of compliers,
for whom the desired comparison of receipt of active treatment and receipt of con-
trol treatment is appropriate. A fundamental diﬀerence between our approach and
52

Chapter 1: Causal inference for time-to-event data with noncompliance
classical methods is that the estimand is not forced to be a parameter in some super-
population model; to illustrate this advantage, we deﬁned new estimands relevant for
this type of data that could be of interest for practitioners.
We proposed a ﬂexible parametric modeling approach which easily incorporates co-
variates, presented a Bayesian multiple imputation inferential procedure, and showed
connections with the commonly used proportional hazards and accelerated failure
time models. We used simple models to relate the survival times to the covariates
and the compliance behavior groups, but some additional complexity can be added
by allowing interactions between covariates, treatment assignment, and compliance,
which would just translate into additional parameters to be estimated. In our exam-
ple, we found that making this simpliﬁcation did not modify our conclusions; however,
this might not be the case for diﬀerent studies. In addition, it is generally possible
to relax the exclusion restrictions if relevant data that could be incorporated into the
deﬁnition of the prior distributions were available. We did not have additional infor-
mation from previous trials that could have been used for this objective, and therefore
we chose to use diﬀuse proper priors instead of more informative distributions for our
analysis.
One drawback of our approach, shared by any parametric modeling approach, is
that in practice it may not be easy to identify an adequate model for the data, and
therefore some future work should include sensitivity analyses to address possible
model misspeciﬁcation. We used a Weibull distribution for its ﬂexibility, and for its
53

Chapter 1: Causal inference for time-to-event data with noncompliance
interpretation as both a proportional hazards and accelerated failure time model, but
an advantage of our framework is that we can easily use a diﬀerent parametric distri-
bution for the time-to-event endpoints; the estimation procedure might diﬀer slightly,
but the overall framework and the estimand deﬁnitions will not change. Instead of a
Weibull speciﬁcation, one could use alternative distributions such as the lognormal,
log-logistic (which does not follow the proportional hazards assumption), or gamma.
The conclusions from our model-based ITT analysis are consistent with those pub-
lished in Galimberti et al. (2013), whose approach did not account for noncompliance
and provided ITT and per-protocol estimates, as well as a hazard ratio estimate. We
also expected that the estimates that we would obtain for the subgroup of compli-
ers might be very similar to those obtained without accounting for noncompliance,
because of the small percentage of observed noncompliance in the study (3% in the
active treatment group and 3.5% in the control treatment group). However, there
was still some uncertainty on whether the use of parametric models would be appro-
priate for this type of problem, or whether there might be variability over time that
wasn’t accounted for with our modeling, to mention a few issues. The fact that our
conclusions were fairly robust to the modeling assumptions, and were also consistent
with non-parametric viewpoints, provide some evidence that this approach may be
justiﬁed for a broader set of trials, and that it makes sense to attribute the causal
eﬀect of the treatment to the complier ITT.
Future work could include exploring the use of more ﬂexible modeling approaches
54

Chapter 1: Causal inference for time-to-event data with noncompliance
for the time-to-event endpoints, such as piecewise exponential models or splines to
determine the survival function.
Another extension would be to explore in more
depth the consequences of relaxing the independent censoring assumption, as well
as the viability of diﬀerent modeling approaches for the censoring mechanism. How-
ever, because these assumptions cannot be corroborated from the data, any progress
on this aspect would require the development of sensitivity analyses tailored to the
characteristics of survival data.
55

Chapter 2
Illustrating the inﬂuence of
ignorable but covariate dependent
experimental designs on the
Bayesian validity of inference for
causal eﬀects
2.1
Introduction
Suppose we wish to estimate the eﬀect of an active treatment relative to a control
treatment in a population. Each unit in the population has a potential outcome asso-
ciated with each of the two treatments, and the causal eﬀect involves the comparison
of these potential outcomes. However, only one of these outcomes is observed: the one
56

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
corresponding to the treatment actually taken. Thus, causal inference is fundamen-
tally a missing data problem (Rubin, 1978), with a key role played by the assignment
mechanism, which describes the process by which treatments are assigned to units
(i.e., why certain potential outcomes were realized and not others). The estimation
of causal eﬀects through any mode of inference therefore requires the speciﬁcation
of an assignment mechanism, and causal answers generally change as the posited
assignment mechanism is changed. In this manuscript, we focus solely on Bayesian
predictive inference (full probability modeling) for causal eﬀects.
Under certain conditions, the assignment mechanism is ignorable, in the sense
that standard Bayesian inferential procedures may be applied without the need to
model the assignment mechanism explicitly. However, Bayesian inference for causal
eﬀects based on data obtained from ignorable assignment mechanisms may not nec-
essarily be insensitive to the model speciﬁed for analysis. Ignorability is deﬁned with
respect to speciﬁc models for an assignment mechanism and data, which we call the
“true” data generating models, and therefore the validity of the ignorability assump-
tion in practice relies on having the correct data model for analysis. That is, the
statistician may still be led astray if the data model being used is not approximately
correct, and the extent of this error can depend not only on the true and posited data
models but also on which ignorable assignment mechanism is used to collect the data.
In a previous example of this approach, Rosenbaum and Rubin (1984a) inves-
tigated the robustness of Bayesian inference with respect to prior assumptions un-
57

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
der data-dependent, but ignorable, stopping rules, and non-data dependent stopping
rules, and found heightened sensitivity when wrong prior distributions were used for
analysis. This leads to the conclusion that, even though the stopping rules consid-
ered were ignorable, they have subtle implications for the robustness of the resulting
inference.
The objective of this paper is to explore, through a sequence of examples, the
impact of model misspeciﬁcation on Bayesian estimation of treatment eﬀects in ex-
perimental settings, when the assignment to treatment is ignorable. The “true” gener-
ating data models, generally unknown to the statistician, determine a true posterior
distribution for a causal estimand of interest. On the other hand, the statistician
poses a set of models to conduct the analysis, which we call the “statistician’s” mod-
els; a posterior distribution for the causal estimand can be obtained assuming these
models. Let ∆M denote the diﬀerence between the true models and the statistician’s
models, and let ∆D denote the diﬀerence between the true posterior distribution (for
a speciﬁc estimand) and the statistician’s posterior distribution. In our examples,
we ﬁx ∆M and the sample size, and investigate the behavior of ∆D under under
various ignorable assignment mechanisms, namely, complete randomization design,
rerandomization design, and the ﬁnite selection model design. The measure ∆D we
choose is the Bayesian probability coverage of approximate posterior interval esti-
mates over the true posterior distribution for an estimand of interest, calculated as
follows. We ﬁrst summarize the information about the estimand by deriving its cor-
rect posterior distribution using the true data generating process. Then, we compute
58

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
a posterior interval using a model speciﬁed for analysis (which could be misspeciﬁed),
and calculate the area of the true posterior distribution that this interval covers.
This is a Bayesian evaluation of a Bayesian procedure, because the performance of
the procedure is evaluated for a speciﬁc class of problems in repeated practice (i.e.,
across multiple populations), and the the coverages computed provide a measure of
discrepancy that is, on its own, a probability distribution (we can make real proba-
bility statements).
To the best of our knowledge, there are no other references, other than Rosen-
baum and Rubin (1984a), that explore Bayesian evaluation procedures. For example,
Rao et al. (2003) investigates coverage probabilities of Normal-approximation-based
conﬁdence intervals for regression estimators where the coverage being computed is
the usual Frequentist coverage (i.e., the “hit-rate” deﬁned as the number of times
that the intervals cover the true point estimand).
Through our evaluations, we attempt to explore the advantages and limitations of
two alternatives to complete randomization, and in particular, to give initial guidance
about the eﬀect of diﬀerent assignment mechanisms on the range of models for which
Bayesian inference is approximately Bayesianly calibrated in the sense that the eﬀects
of misspeciﬁcation in the posited data models are not too drastic.
In Section 2.2, we introduce notation. In Section 2.3, we introduce characteristics
of assignment mechanisms, and describe complete randomization design, rerandom-
59

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
ization design, and the ﬁnite selection model design.
In Section 2.4 we describe
Bayesian estimation of causal eﬀects, and how the data model of the response as a
function of the covariate is more crucial for the sensitivity of the inference to prior
speciﬁcations when rerandomization or the ﬁnite selection model are used rather than
complete randomization. In Section 2.5 we introduce the speciﬁc Bayesian evaluation
method that will be used to assess the sensitivity of inference to prior speciﬁcations.
In Section 2.6 we present a sequence of examples to illustrate the inﬂuence of ignor-
able but covariate dependent designs and of misspeciﬁed data models on the Bayesian
validity of inference for causal eﬀects. In Section 2.7 we propose a future simulation
design that would allow for a systematic investigation of all the factors that are likely
to inﬂuence the Bayesian validity of inference for causal eﬀects. We conclude with an
overall discussion in Section 2.8.
2.2
Notation
Consider a ﬁnite population of N experimental units, indexed by i ∈{1, . . . , N}.
The N ×K matrix of K covariates, which we assume to be fully observed and recorded
for all units before the treatments are assigned, is denoted X, where Xi denotes the
K−component row vector of covariates corresponding to unit i. Let Y (1), Y (0) de-
note the N−component column vectors of potential outcomes of the response measure
of interest under assignment to active treatment and control treatment, respectively.
For notational simplicity, we assume that the response is univariate. The pair of
potential outcomes for unit i is denoted (Yi(1), Yi(0)); this notation tacitly makes the
Stable Unit Treatment Value Assumption (SUTVA, Rubin (1980)), which states there
60

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
is no interference between units and there are no hidden versions of the treatments.
Lastly, a causal eﬀect for a particular experimental unit is a comparison between the
potential outcome under assignment to the active treatment and the potential out-
come under assignment to the control treatment (Rubin, 1974).
Let W be the N−dimensional column vector of treatment assignments for all
units in the ﬁnite population, where Wi = 1 if unit i is assigned to the active treat-
ment, and Wi = 0 if unit i is assigned to the control treatment. Of course, only one
potential outcome is ever observed because each unit can be assigned to only one
treatment at a speciﬁc moment in time. The observed outcome for unit i is denoted
by Y obs
i
, where Y obs
i
= WiYi(1) + (1 −Wi)Yi(0) = Yi(Wi); analogously, the missing
potential outcome is Y mis
i
= (1−Wi)Yi(1)+WiYi(0) = Yi(1−Wi); Y obs and Y mis are
the corresponding N−dimensional column vectors of observed and missing outcomes.
Finally, let Nt =
N
X
i=1
Wi and Nc =
N
X
i=1
(1 −Wi) denote the number of units assigned
to the active treatment and control treatment, respectively.
Here, we assume that W is obtained through an ignorable assignment mechanism.
Once the experiment is conducted and an outcome Y obs
i
is observed for each unit, a
Bayesian analysis is performed to assess the causal eﬀect of the treatment on this
response.
61

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.3
The assignment mechanism
Because causal inference is basically a missing data problem with at least half
of the potential outcomes not observed, the assignment mechanism is fundamental.
Without understanding or at least positing the process that creates the missing data,
formal causal inference using probabilistic statements is impossible.
2.3.1
General characteristics of assignment mechanisms
The assignment mechanism gives the conditional probability of each vector of
assignments given the covariates and the potential outcomes. An assignment mecha-
nism is individualistic if the assignment probability for a given unit does not depend
on the covariates, potential outcomes, or assignments of the other units. It is prob-
abilistic if, for each possible Y (1), Y (0) and X, the probability of assignment to
treatment for unit i is strictly between zero and one, and it is unconfounded if it is
free of dependence on the potential outcomes. Under unconfoundedness, at each set
of values Xi that has a distinct probability of Wi = 1, there is eﬀectively a completely
randomized experiment.
Completely randomized designs, which we discuss further in Section 2.3.2.1, have
an individualistic, probabilistic, and unconfounded assignment mechanism that allows
straightforward randomization-based estimation of treatment causal eﬀects (this com-
bination of characteristics is referred to as strongly ignorable treatment assignment in
Rosenbaum and Rubin (1984b)). Therefore, they form the basis for randomization-
based inference for causal eﬀects in more complicated situations, such as when as-
62

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
signment probabilities depend on covariates.
Bayesian inference considers the observed values W , X, and Y obs to be realiza-
tions of random variables, and the missing values Y mis to be unobserved random
variables.
We discuss the notion of ignorability of the assignment mechanism for
Bayesian inference in Section 2.4.2.
2.3.2
Examples of random assignment mechanisms
Randomization involves randomly allocating the experimental units across the
treatment groups, thereby creating balance in the covariate distributions. By bal-
ance, we mean that the treatment groups should not diﬀer in any systematic way in
terms of their covariate values.
We consider three examples of (strongly) ignorable random assignment mecha-
nisms: completely randomized design (CR), rerandomization design (RR), and the
Finite Selection Model design (FSM). These methods produce balance in diﬀerent
ways. Complete randomization balances all potential confounding factors on average
across all possible randomizations. However, when covariates are available that are
thought to be highly associated with the potential outcomes, completely randomized
experiments might have the drawback of creating, by chance, groups that are notably
imbalanced on a covariate. In contrast, both rerandomization and the ﬁnite selection
model design have the advantage of providing a certain degree of balance in the co-
63

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
variates for the speciﬁc experiment at hand, in accordance with a chosen criterion.
2.3.2.1
Completely randomized design
Given a population of size N and assuming a binary treatment, in a completely
randomized experiment a ﬁxed number of units, denoted by Nt (where 1 ≤Nt < N),
are drawn at random and assigned to receive the active treatment. In the simplest
case, an even number of units N are divided at random into two groups, with one
half of the sample, N/2, assigned to the active treatment and the remaining units
assigned to the control treatment (Imbens and Rubin, 2015, Ch. 4, p. 50).
Because all possible assignment vectors are equally likely, the assignment mecha-
nism that governs a completely randomized experiment is
P(W | X, Y (0), Y (1)) =
N
Nt
−1
if
N
X
i=1
Wi = Nt,
and zero otherwise.
This assignment mechanism does not depend on covariates,
potential outcomes, or unknown parameters. Additionally, the unit level probabilities
are Nt/N, common for all units in the experiment.
2.3.2.2
Rerandomization design
Rerandomization was proposed in Morgan and Rubin (2012), and is an assign-
ment mechanism that directly achieves balance in many covariates. This procedure
requires the speciﬁcation of a balance criterion before conducting any randomizations,
64

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
which depends on the recorded covariates and determines when a randomization is
acceptable. Once this criterion is determined, the experimental units are assigned to
the active and control treatment groups via complete randomization: if the resulting
randomization satisﬁes the balance criterion, then the experiment is conducted using
it; otherwise, the units are randomized again until the criterion is met and an accept-
able randomization is obtained, before proceeding to conduct the experiment.
In the examples presented in Section 2.6 we consider only one covariate. Thus,
we use a balance criterion that is based on the standardized diﬀerence in covariate
means between the active and control treatment groups; a randomization is considered
acceptable whenever this measure falls below a certain threshold. Alternative balance
criteria can be deﬁned, and can include higher moments or transformations of the
covariate. We postpone our discussion of the case of multiple covariates to Section
2.7.
2.3.2.3
Finite Selection Model design
The ﬁnite selection model design (FSM) was introduced in Morris (1979) and
further developed in Morris and Hill (2000). It is a sequential allocation technique
that balances covariates across multiple treatment groups; it is also ignorable, but
not individualistic. An intuitive explanation of this method can be given through a
sports draft analogy: ﬁrst, a random team (here, treatment group) starts—and then
the teams take turns—choosing their team members (here, experimental units) from
the pool of remaining participants according to a speciﬁed selection function that
65

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
depends on the covariates to be balanced. The “teams” take turns choosing and, as a
result of using the same criterion, the treatment groups will be balanced with respect
to the covariates.
The algorithm is sequential, because the decision to allocate a certain unit into
a treatment group depends on the values of the covariates of those units previously
allocated to that group.
Because of this sequential nature, the selection function
generally involves the covariate matrices for units assigned to the treatments up to
the selection round j. Each selection function will induce certain balance criteria for
the covariates. The randomness in this allocation method comes from the fact that
at each step, the order in which the diﬀerent treatment groups are allowed to choose
their next unit is random.
In the case where there is only one covariate, FSM reduces to selecting, from the
pool of available experimental units at each step, the one that would maximize the
sample variance of the covariate in the treatment group to which it would be assigned.
Again, we postpone the discussion of criteria for the case of multivariate X to Section
2.7.
66

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.4
Bayesian inference for causal eﬀects
2.4.1
Models for underlying data
In Bayesian inference, (Y (1), Y (0), X, W ) the array of quantities is considered
to be a random variable, where the joint distribution of (Y (1), Y (0), X, W ) can be
written as the product of the marginal distribution of potentially observable data and
the assignment mechanism (conditional on the covariates and potential outcomes).
In general, it is diﬃcult to formulate a sensible model for the entire joint distribu-
tion of the underlying data, p(Y (1), Y (0), X), but under (row) exchangeability and
by appealing to de Finetti’s theorem, we can model it as the integral over the product
of independent and identically distributed (i.i.d.) unit-level distributions:
p(Y (1), Y (0), X) =
Z "
N
Y
i=1
p(Yi(1), Yi(0), Xi | θ)
#
p(θ)dθ,
where θ is an unknown parameter with marginal distribution p(θ). Additionally, it is
always possible to factor the conditional distribution of the triplets (Yi(1), Yi(0), Xi), i =
1, . . . , N given θ into two components:
p(Yi(1), Yi(0), Xi | θ) = p(Yi(1), Yi(0) | Xi, θY |X)p(Xi | θX),
where θY |X and θX) are functions of θ; we often assume that θY |X and θX are a pri-
ori independent. Specifying the conditional distribution (Yi(1), Yi(0) | Xi, θ) requires
subject matter knowledge. Much of this manuscript will deal with the issue of spec-
67

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
iﬁcation of this model for the data, as well as the prior distribution p(θ), which are
the two essential inputs into the model-based inferential approach when the third
input, the conditional distribution of W given the covariates and potential outcomes,
p(W | Y (1), Y (0), X), can be ignored. For example, we gave an expression for a
completely randomized design in Section 2.3.2.1: the distribution does not depend on
covariates, potential outcomes, or any unknown parameters, and the unit-level values
Wi are exchangeable but not i.i.d. The rerandomization design and the FSM design
depend on covariates but are free of the potential outcomes, and all are fully known.
The model p(Yi(1), Yi(0), Xi | θ) is never entirely under the experimenter’s con-
trol because the conditional distribution of (Yi(1), Yi(0)) given Xi reﬂects the state
of nature; the marginal distribution of X may be to some extent under the exper-
imenters’ control when the population is deﬁned by the choice of the experimental
units. Finally, the assignment mechanism p(W | Y (1), Y (0), X) can be under the
experimenters’ control when they can assign treatments to experimental units.
The joint distribution of all quantities is
p(Y (1), Y (0), X, W )
=
Z "
N
Y
i=1
p(Yi(1), Yi(0), Xi | θ)
#
p(θ)dθ
×
[p(W | Y (1), Y (0), X)]
When the second factor in brackets is fully known for observed data, it is a con-
68

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
stant with known distribution and so, for ﬁxed speciﬁcations p(· | θ) and p(θ), it
can be ignored for Bayesian inference (Rubin, 1978; Little and Rubin, 2002); that is,
standard Bayesian inferential procedures may be applied without the need to model
the assignment mechanism explicitly.
In more generality, consider an assignment mechanism for which the following two
conditions hold:
(a) The probability of the treatment assignments depends only on the observed
data, and not on any of the missing values, i.e.,
p(W | Y obs, Y mis, X) = p(W | Y obs, Y mis
∗, X)
for all possible Y mis and Y mis
∗. When this requirement holds, the missing data
are called missing at random (MAR).
(b) The parameters that govern the distribution of the potentially observable data
and those that govern the assignment mechanism are a priori independent.
As a consequence of these two conditions, that assignment mechanism is ignorable
for Bayesian inference (Rubin, 1978; Little and Rubin, 2002).
If the treatment assignment depends on recorded covariates, then the dependence
of the outcome variable on the covariates, p(Y obs | X, θ), must be modeled. Note
that if the treatment assignment depends on unrecorded covariates, then it is not ig-
norable, and the resulting selection eﬀects must be modeled, which is generally more
69

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
demanding than just specifying a model for the marginal distribution of the potential
outcomes, as noted in Rubin (1978).
2.4.2
Model-based inference
The primary goal is to derive the posterior predictive distribution of the missing
potential outcomes given the observed data, p(Y mis | Y obs, X, W ), which is used to
derive the distribution for the estimand of interest. Therefore, the Bayesian approach
requires the speciﬁcation of the joint distribution p(Yi(1), Yi(0), Xi | θ) and the prior
distribution p(θ), both of which can be approached as discussed in Section 2.4.1, and
of the assignment mechanism, p(W | Y (1), Y (0), X).
To go from these inputs to the distribution of the estimand, we follow these steps
(Imbens and Rubin, 2015, Ch. 8, p.154-155):
1. Derive p(Y mis | Y obs, X, W , θ).
2. Derive the posterior distribution for θ, namely p(θ | Y obs, X, W ).
3. Combine the previous two distributions to obtain p(Y mis | Y obs, X, W ).
4. Because any estimand τ is just a function of the potential outcomes and co-
variates, we use the conditional distribution from step 3 to derive the posterior
distribution of τ given the observed data, p(τ | Y obs, X, W ).
70

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.4.3
Sensitivity of inferences to prior speciﬁcations
The fact that the assignment mechanism is ignorable (for Bayesian inference)
does not mean that it leads to data from which causal inferences are insensitive to
the model speciﬁcation for the data. For example, inference can be sensitive to prior
speciﬁcations if the ignorable assignment yields data poorly balanced with respect to
recorded covariates.
In general, inferences for causal eﬀects will be relatively insensitive to prior spec-
iﬁcations of the conditional distribution p(Yi(1), Yi(0) | Xi, θ) and of the parameter
p(θ) if the assignment mechanism is ignorable and, for each distinct value of recorded
covariates, there are experimental units with recorded outcome values within each
treatment condition (Rubin, 1978).
When blocking is used in classical randomized designs, randomization will balance
the distribution of the covariates that were used for blocking. Here, there is no need
to record those individual covariates for data analysis; only the values of the block
indicators need to be recorded in order for the assignment mechanism to be ignor-
able. Additionally, for the data analysis, only an acceptable model for the randomized
block experiment (with no covariates recorded) needs to be speciﬁed, because within
each block we have a completely randomized experiment. Thus, the complication
of modeling the conditional distribution of the outcomes given covariates other than
block indicators is essentially avoided, resulting in analyses for causal eﬀects that are
relatively insensitive to prior speciﬁcations.
71

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
However, when the treatment assignment is done using rerandomization or the
ﬁnite selection model design, no explicit partitions or blocks in terms of the covariates
are obtained, hence, there are no block indicators immediately available to simplify
the analysis as described above.
Here, all the covariates used in the assignment
need to be recorded and included in the model speciﬁcation for analysis to make it
ignorable, but modeling the conditional distribution of (Y (1), Y (0)) given X might
not be trivial. Thus, the Bayesian analysis is less straightforward and can be more
sensitive to prior speciﬁcations. We explore this hypothesis further in the rest of
this manuscript and, in particular, the notion that ignorability in practice eﬀectively
depends, to some extent, on the assignment mechanism used or having the correct
model speciﬁcation for analysis.
2.4.4
Contrast with classical Frequentist approaches
In the Fisher randomization-based p-value approach, a speciﬁcation for the distri-
bution of the potentially observable data is irrelevant. If the assignment mechanism
depends on covariates, the randomization test should just duplicate that mechanism,
allowing for calculation of exact p-values (see Morgan and Rubin (2012) for details).
Therefore, a randomization test will produce a valid randomization-based analysis
for randomized experiments whether the experiment used complete randomization,
rerandomization, or the ﬁnite selection method.
From the Neymanian perspective, na¨ıve estimators, that is, the diﬀerence in ob-
72

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
served outcome means and the usual asymptotic conﬁdence intervals that ignore the
covariates, tend to be too “conservative” for rerandomization, in the sense that the
asymptotic normal-based versions of these tests result in p-values that are too large
and there is (frequentist) higher than nominal coverage for the conﬁdence intervals.
Morgan and Rubin (2012) suggest that using regression adjustments would provide
better results, because the results will more accurately capture the true sampling
variance of the estimated treatment eﬀect.
Any complication that creates problems for one form of inference creates problems
for all forms of inference, just in diﬀerent ways. The need in randomization inference
to deal directly with the actually used p(W | X, ψ) and to specify null hypotheses,
statistics, and notions of extremeness is replaced in Bayesian inference with the need
to specify p(Y (1), Y (0) | X, θ) (Rubin, 1991). In Bayesian inference, even though
there is no need to model the assignment mechanism explicitly if it is ignorable, the
complication arises from having to propose a model for the conditional distribution
of the potential outcomes given the covariates.
2.5
Evaluation method
From a frequentist perspective, evaluating the performance of a procedure is based
upon repeated sampling from a ﬁxed population. One common example of such an
evaluation procedure is to obtain the frequentist coverage of interval estimates, which
is calculated by counting the fraction of times that nominal (e.g., 95%) intervals cover
the truth (i.e., the value of the estimand that has a ﬁxed value). One then checks
73

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
whether the calculated coverage is at least as large as the nominal coverage. In the
case where a Bayesian inferential method is used to obtain the intervals, this evalua-
tion method is also an example of frequentist calibration of a Bayesian procedure, in
the sense that a (1 −α)% calibrated posterior interval should cover the truth about
(1 −α)% of the time over repeated sampling. Such evaluation methods are relevant
for recommending procedures for general consumption, and for a quantiﬁcation of the
accuracy of estimation.
From a Bayesian perspective, the evaluation of the performance of a procedure
should be done for a speciﬁc class of problems in repeated practice (i.e., across multi-
ple populations, all obtained from the same data generating model including the prior
distributions), as contrasted with simply repeated sampling. We propose a Bayesian
simulation-based approach for evaluating procedures, which is based on Bayesian
probability coverage of posterior interval estimates for the estimand of interest. We
exemplify this approach through an evaluation of the sensitivity of Bayesian inference
to model misspeciﬁcation when ignorable, but possibly covariate-dependent, assign-
ment mechanisms are used.
2.5.1
True model, statistician’s model, and Bayesian cover-
age
We call the collection of speciﬁcations used in the process of generating the data
the “true” model, that is, the model that the statistician would want to use to draw
inferences; however, this is unknown to the statistician in real life examples. Let
74

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
g(Y | θ) denote the likelihood, and q(θ) denote the prior distribution of the parameters
in the true model; in this section, we use Y and θ generically to denote all the available
data and all parameters of interest, respectively. The correct posterior distribution
of θ is thus given by:
g(Y | θ)q(θ)
R
θ g(Y | θ)q(θ)dθ.
We call the collection of models assumed by the statistician in order to conduct
the analysis the “statistician’s” model. The statistician’s model is chosen, to some
extent, for computational simplicity and convenience. Let f(Y | θ)p(θ) denote the
statistician’s model. Then an interval I(Y ) such that
R
θ∈I(Y ) f(Y | θ)p(θ)dθ
R
θ f(Y | θ)p(θ)dθ
= 1 −α,
for α ∈(0, 1),
is a (1−α)×100% posterior interval for θ constructed under this model. For given Y ,
the probability coverage of I(Y ) over the true (correct) posterior distribution (Rubin,
1984) is given by
PC(Y ) =
R
θ∈I(Y ) g(Y | θ)q(θ)
R
θ g(Y | θ)q(θ)dθ .
That is, PC(Y ) measures how much of the true posterior distribution is covered by
the constructed interval I(Y ) (i.e., the area of the true posterior distribution that is
contained within the bounds of I(Y )). Clearly, if g(Y | θ) = f(Y | θ) and q(θ) = p(θ),
then PC(Y ) = 1 −α for all Y .
Using the above deﬁnitions, Bayesian coverage can be calculated as depicted in
Figure 2.1.
One ﬁrst summarizes the information about the estimand of interest
75

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
by deriving its posterior distribution from the true model; its entire true posterior
distribution will be used to evaluate the posterior intervals obtained from the anal-
ysis.
Then, for each dataset obtained from the model {g(Y | θ), q(θ)}, one con-
structs a posterior interval for the estimand of interest using the statistician’s model,
{f(Y | θ), p(θ)}, and one computes the posterior probability of the true distribution
that it covers, PC(Y ). Figure 2.2 illustrates the notion of Bayesian coverage for a
single dataset.
Data generation
(true model)
g(Y | θ)q(θ)
Dataset Y
Statistician’s
model
f(Y | θ)p(θ)
Interval I(Y )
Coverage
probability
of I(Y )
over g(Y | θ)q(θ)
PC(Y )
Figure 2.1: Procedure for calculation of a Bayesian coverage probability for a single
dataset.
The coverage probability of I(Y ), PC(Y ), has a distribution over the examples
to which I(Y ) will be applied—the distribution of Y generated by g(Y | θ)q(θ).
Ideally, the statistician would choose the model f(Y | θ)p(θ) in such a way that the
distribution of PC(Y ) is tightly concentrated about the nominal level (e.g., 0.95) for
a reasonably broad range of plausible g(Y | θ)q(θ).
76

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
estimand
density
True distribution
I(Y)
PC(Y)
Figure 2.2: Coverage probability of the interval I(Y ) under the true model, for a
single dataset.
2.5.2
Bayesian coverage in the context of estimation of causal
eﬀects under model misspeciﬁcation
In the context of a causal inference problem, the “true” model used to generate
the Science encompasses:
• the assignment mechanism,
• the underlying model for the potential outcomes conditional on the covariates,
and
77

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
• the prior distributions for any parameters used.
The “statistician’s” model encompasses, in this context, a model on the potential
outcomes conditional on the covariates, as well as all prior distributions that will be
used in the analysis, and the assumptions made about the assignment mechanism. If
the statistician’s model corresponds exactly to the one used to generate the Science,
then the analysis would produce the true posterior distribution; otherwise, it is a mis-
speciﬁed model. For the purpose of our evaluations, we assume that the statistician’s
diﬀers from the one used to generate the Science, and thus is misspeciﬁed.
The process to construct the distribution of Bayesian coverage probabilities is as
follows. First, we generate the data from the true model1:
1. Produce a sample of the Science from the true model:
(a) Draw θX ∼q(θX), and obtain a set of covariates X ∼q(X | θX).
(b) Draw θY |X ∼q(θY |X), and then draw the potential outcomes given the
covariates from q(Y (1), Y (0) | X, θY |X).
Let τ denote the estimand of interest (in this case, the average treatment eﬀect).
2. Obtain one assignment vector W ∗under the mechanism considered (‘∗’ stands
for one of CR, RR, FSM).
3. Obtain the corresponding Y obs
∗.
1In this context, we use q(·) indistinctly to denote all prior distributions and likelihoods associated
with the true model speciﬁcations, and distinguish them from the statistician’s model speciﬁcations
which we denote by p(·).
78

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
One dataset consists of the collection (Y obs
∗, X, W ∗). Next, for each data set
generated, the coverage probability of a posterior interval with respect to the true
model is obtained as follows:
1. Obtain the posterior distribution for the estimand τ under the true model,
q∗(τ | Y obs
∗, X, W ∗, true model).
Note that a diﬀerent posterior distribution is obtained under each assignment
method because, even though the form of the posterior distribution is the same
under ignorability, the datasets that are observed under diﬀerent assignment
mechanisms diﬀer2. Again, the objective is to evaluate a procedure; therefore,
this step can generally be done either through simulation or through exact
calculation (depending on the complexity of the modeling), because the correct
data generating model is known.
2. Propose a model for analysis and obtain the posterior distribution of the es-
timand as described in Section 2.4.2, assuming ignorability of the assignment
mechanism. Produce the 95% posterior interval I∗≡I∗(Y obs
∗, X, W ∗) under
the proposed statistician’s model.
3. Obtain the probability coverage of the interval I∗under the true model speciﬁ-
cation,
PC(Y obs
∗, X, W ∗) ≡P(τ ∈I∗| Y obs
∗, X, W ∗, true model).
2RR and FSM have diﬀerent sets of possible acceptable randomizations.
79

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
For each generated dataset (Y obs
∗, X, W ∗), we obtain one coverage probability.
When the procedure is repeated for a sequence of datasets generated from the same
true model, a distribution of coverage probabilities is obtained. Ideally, the resulting
distribution would be close to a point mass centered at the interval’s nominal value
(e.g., 0.95).
2.5.3
Alternative measures
For Bayesian statisticians, the objective should be to assess the Bayesian validity
of a procedure, that is, to determine the consequences of using the wrong models for
data analysis. To this end, we compare the true posterior distribution of the estimand
τ of interest to the posterior distribution of the estimand that is obtained using the
statistician’s model.
Here, we choose to perform this comparison using PC(Y ), the Bayesian coverage
probability of a nominal (1 −α) × 100% posterior interval obtained using the pro-
posed statistician’s model, calculated over the true posterior distribution of τ. The
advantage of considering Bayesian coverage probabilities of posterior intervals is that
they have some intuitive appeal for practitioners, who are used to working with, and
interpreting, intervals. Nevertheless, the interpretation of frequentist and Bayesian
coverage is quite diﬀerent. For a particular dataset, frequentist coverage provides a
binary decision on whether the statistician’s interval contains the true ﬁxed value of
the estimand, whereas Bayesian coverage provides a probability reﬂecting how much
of the estimand’s true distribution is covered by the statistician’s interval. Further-
80

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
more, these summaries address diﬀerent questions: the focus of frequentist coverage
is on the calibration of a procedure over repeated sampling, and the focus of Bayesian
coverage is on calibration in repeated practice across a certain range of models.
Alternative measures can be used to quantify the diﬀerence between the true
and the statistician’s posterior distribution of τ. For example, the Kullback-Leibler
divergence between two (continuous) probability distributions G and F (with densities
g(x)dx and f(x)dx, respectively), deﬁned as
DKL(G, F) ≡
Z ∞
−∞
g(x) log g(x)
f(x)dx,
would be a natural choice because, typically, G represents a “true” or theoretical
distribution and F represents a model or approximation of G. Similarly, one could
use the Jensen-Shannon divergence between G and F, a symmetrized and smoothed
variant of Kullback-Leibler divergence, deﬁned as
JSD(G, F) ≡1
2DKL(G, M) + 1
2DKL(F, M),
where M = 1
2(G + F). The Hellinger distance between G and F, deﬁned as
H(G, F) ≡
1
2
Z p
g(x) −
p
f(x)
2
dx
1/2
,
is another commonly used metric for the discrepancy between probability distribu-
tions.
81

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
This is by no means an exhaustive list: other examples of measures could be
used and might focus on diﬀerent aspects of the distributions. Here we use Bayesian
coverage probabilities for ease of interpretation, but this type of Bayesian evaluation
procedure can be applied in many diﬀerent settings and the ﬁnal choice of measure
can be left to the practitioner.
2.6
Illustrative examples
In this section, we illustrate in a sequence of examples the inﬂuence of ignorable
but covariate dependent experimental designs on the Bayesian validity of inference for
causal eﬀects when the data models for analysis are misspeciﬁed. In these examples,
we focus speciﬁcally on comparing results produced when the designs are complete
randomization, rerandomization, and the FSM, under diﬀerent speciﬁcations of the
following true probability distributions:
q(θX), q(X | θX), q(θY |X | θX), and q(Y | X, θY |X),
and under diﬀerent speciﬁcations of the statistician’s probability distributions:
p(θX), p(X | θX), p(θY |X | θX), and p(Y | X, θY |X).
82

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.6.1
General setup
For the purposes of these examples, we assume that X is actually a scalar; this
can be either because there is only one covariate, or because there are speciﬁcations
for estimating a propensity score, which can be a function of multiple covariates.
Additionally, we choose θX such that the covariate is standardized. Unless otherwise
stated, q(X | θX) = N(0, 1). As mentioned in Section 2.3.2.3, for the case of one co-
variate the FSM design reduces to selecting, from the pool of available experimental
units at each step, the one that maximizes the sample variance of the covariate in
the treatment group to which it would be assigned. Major diﬀerences between com-
plete randomization, rerandomization and FSM might be observed when multiple
covariates are considered; however, the simpliﬁcation is made here in order to avoid
complicating the joint models for the outcomes and covariates, thereby allowing for
a cleaner interpretation of the initial results. We address the case for multivariate X
further in Section 2.7.
The procedure in each of the examples is the following. For the N(0, 1) covari-
ate distribution, we generate nsim = 1000 covariate N-component vectors, where we
set N = 400 experimental units. We then generate nsim = 1000 assignment 400-
component vectors for a completely randomized scheme; for rerandomization and
FSM, each covariate vector produces one assignment vector (for a total of nsim =
1000 assignment vectors per method). This results in 1000 (X, W ∗) pairs of 400-
component vectors, for each of the three assignment mechanisms. We assume a 1:1 ra-
tio of units assigned to active treatment and control treatment (i.e., Nt = Nc = N/2).
83

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
For each example, there is a ﬁxed speciﬁcation for ∆M (which we use to denote
the diﬀerence between the true models and the statistician’s models) and sample
size. Each (X, W ∗) pair of N-vectors is, in turn, used to generate the corresponding
nsim = 1000 total Y obs
∗
vectors through the true model speciﬁcation. For each of the
nsim datasets (X, W ∗, Y obs
∗), we use the statistician’s model to produce the posterior
interval for the estimand of interest (here, the average treatment eﬀect for the super-
population). Finally, we perform the procedure described in Section 2.5.2 to obtain
the Bayesian coverage probability PC(X, W ∗, Y obs
∗); this probability quantiﬁes the
diﬀerence between the true posterior distribution and the statistician’s posterior dis-
tribution, which we have denoted ∆D. The simulation distribution of the Bayesian
coverage probabilities consists of nsim = 1000 samples, which we summarize via per-
centiles.
For the true model speciﬁcation for the data, we assume for simplicity a lin-
ear relationship between the potential outcomes and the covariates, and an additive
treatment eﬀect:
Yi(Wi) = β0 + βXXi + βWWi + εi,
with εi ∼[0, σ2
y].
(2.6.1)
The probability distribution of the errors ε varies with the examples.
We set an
inverse gamma prior distribution for the residual variance,
σ2
y ∼Inv −gamma(v0, v0 · s2
0),
84

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
and ﬁx the shape v0 = 10 and the scale s2
0 = 10. We set Normal prior distributions
for the regression parameters β0 and βX, and the causal eﬀect βW,
(β0, βX, βW) ∼N((β(0)
0 , β(0)
X , β(0)
W ), 0.0025I3),
where the values of the hyperparameters are set as β(0)
0
= 2, β(0)
X = 3 (chosen to reﬂect
strong dependence on the covariate), and β(0)
W = 4 (to reﬂect a large treatment eﬀect),
and I3 is the identity matrix of dimension 3. Thus, θY |X = (β0, βX, βW, σ2
y).
The statistician’s models for data analysis all assume Normal distributions for the
outcomes, but vary with the examples. We use simple models with additive Normal
errors and straightforward choices of prior distributions that are commonly used by
statisticians. Again, we assume a priori independence of θY |X and θX, and therefore
Bayesian inferences for causal eﬀects are determined simply by the observed values
(X, W ∗, Y obs
∗), the speciﬁcations for the conditional distribution of the outcome given
the covariate, and the prior distribution of θY |X.
The summary measure we use is the posterior distribution of the Bayesian coverage
probabilities PC(X, W ∗, Y obs
∗). Because the distributions of coverage probabilities
we obtained are usually left-skewed, we choose to summarize the results using the
10th and 25th percentiles.
85

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.6.2
Example 1: correct likelihood, misspeciﬁed prior dis-
tributions
As a ﬁrst example, we consider the case where the statistician’s likelihood, p(Y |
X, θY |X), is correctly speciﬁed, but the statistician’s prior distribution, p(θY |X), is
misspeciﬁed. In particular, the true conditional error distribution (given the covari-
ate) in Equation 2.6.1 is Normal, and the statistician’s model is a Normal linear
regression with “noninformative” improper prior distributions on all parameters, i.e.,
Yi(Wi) = γ0 + γXXi + γWWi + εi,
with εi ∼N(0, σ2
y),
where θY |X = (γ0, γX, γW, σ2
y), and
p(γ0, γX, γW, σ2
y | Y obs) ∝1/σ2
y.
In short, ∆M can be described as having diﬀerent prior distributions.
Using this model for data analysis, we obtain similar and valid results under all as-
signment mechanisms considered (see Table 2.1). The 10th percentile of the Bayesian
coverage probability distribution is about 0.84, and the 25th percentile is about 0.96.
As expected, there is slight overcoverage because the analysis distribution of the es-
timand (which is based on improper prior distributions) has more variability than its
true distribution (which uses proper prior distributions).
86

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Table 2.1: Summaries of posterior distributions of Bayesian coverage probabilities:
10th and 25th percentiles. With β(0)
X = 3, β(0)
W = 4. Covariate distribution X ∼N(0, 1).
Assignment mechanism
Example no.
True
CR
RR
FSM
ε distr.
10 %ile
25 %ile
10 %ile
25 %ile
10 %ile
25 %ile
Example 1
Normal
0.84
0.96
0.84
0.96
0.85
0.97
Example 2
Normal
0.92
0.98
0.99
1.00
0.99
1.00
Example 3
t7
0.93
0.95
0.93
0.95
0.93
0.95
t15
0.94
0.95
0.94
0.95
0.94
0.95
Example 4
t7
0.79
0.97
0.00
0.00
0.00
0.00
t15
0.84
0.97
0.00
0.00
0.00
0.00
Example 5
t7
0.94
0.99
0.00
0.00
0.00
0.00
t15
0.95
0.99
0.00
0.00
0.00
0.00
2.6.3
Example 2: misspeciﬁed functional relationship between
Y and X, correct prior distributions
The second example involves specifying the error distribution correctly, but slightly
misspecifying the likelihood through a wrong conditional expectation of Y given X,
and using the correct prior distributions. This is the case, for instance, when the
true error distribution in Equation 2.6.1 is Normal, and the statistician’s model ex-
cludes the covariate from the conditional expectation of Y , but still assumes that the
potential outcomes follow a Normal distribution, i.e.,
Yi(Wi) = µ + τWi + εi,
with εi ∼N(0, σ2
y).
Here, θY |X = (µ, τ, σ2
y). We use the true proper prior distributions for µ and τ, namely,
(µ, τ) ∼N((β(0)
0 , β(0)
W ), 0.0025I2), with β(0)
0
and β(0)
W as in Section 2.6.1, and the true
inverse-gamma distribution for σ2
y. Thus, ∆M in this example is the discrepancy in
the speciﬁcation of the conditional expectation of Y given X.
87

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Even with strong dependence of the outcome on the covariate in the true model (as
in our setup), inference under complete randomization always produces a valid answer.
There is slight overcoverage, as expected, because the posterior distribution of the
estimand obtained under the statistician’s model has a larger variance than its true
posterior distribution (it does not take the covariate into account). This overcoverage
is reﬂected by the 10th percentile of the Bayesian coverage probabilities’ distribution
being about 0.92, and the 25th percentile being about 0.98 (see Table 2.1). However,
RR and FSM produce unusually large overcoverage, as demonstrated by the 10th
percentile of the Bayesian coverage probability distribution being about 0.99. The
explanation for this behavior is that the covariates used in the assignment are not
included in the model for the analysis, which makes these assignment mechanisms
to be no longer be apposite for Bayesian inference, resulting in incorrect conclusions
due to the lack of adjustment for the conditional variance that would be controlled
through the use of covariates.
Figure 2.3 shows the distributions of Bayesian coverage probabilities obtained from
an analysis excluding the covariates under the setup described above. One interesting
feature to note is that under complete randomization, it is possible to obtain very
small Bayesian coverage probabilities (close to zero), whereas under rerandomization,
the minimum Bayesian coverage probability obtained was 0.35 and under FSM the
minimum was 0.84. The insight is that, under rerandomization and FSM, for every
dataset the covariate balance is always good by design, and therefore the statistician’s
posterior distribution for the estimand will be centered at the right place, but will have
88

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
CR
coverages
Frequency
0.0
0.2
0.4
0.6
0.8
1.0
0
200
400
600
800
1000
RR
coverages
Frequency
0.0
0.2
0.4
0.6
0.8
1.0
0
200
400
600
800
1000
FSM
coverages
Frequency
0.0
0.2
0.4
0.6
0.8
1.0
0
200
400
600
800
1000
Figure 2.3: Distributions of the Bayesian coverage probabilities under diﬀerent as-
signment mechanisms. The true conditional model is Normal, with β(0)
X
= 3, and
β(0)
W = 4; statistician’s model excludes the covariate. Red vertical line at 0.95 indi-
cates the nominal center of the distributions.
a larger posterior variance (as expected); thus, overcoverage is produced. The small
coverage probabilities that appear in the CR scenario are related to the occurrence
of assignment vectors that produce slightly bad balance in the covariate (which can
occur by chance under this assignment mechanism), as illustrated in Figure 2.4. This
reveals potential advantages of rerandomization and FSM as alternatives to complete
randomization, even when leaving out the covariates used to create the extra balance.
89

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
Complete Randomization
Difference in means
Coverage
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
Rerandomization
Difference in means
Coverage
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
FSM
Difference in means
Coverage
Figure 2.4: Bayesian coverage probabilities vs balance (measured through standard-
ized diﬀerence in means) under the assignment mechanisms considered. The true
conditional model is Normal, with β(0)
X
= 3, and β(0)
W = 4; statistician’s model ex-
cludes the covariate. Blue horizontal line at 0.95 indicates the nominal coverage.
2.6.4
Example 3: Misspeciﬁed error distribution, correct prior
distributions
Now we illustrate a case where the error distribution is misspeciﬁed but the con-
ditional expectation of the outcome given the covariate is correctly speciﬁed, and the
prior distributions the correct priors distributions.
Here, we consider two possibilities for the true conditional error distribution in
Equation 2.6.1: either a t distribution with 7 degrees of freedom (t7, which is close
to a logistic distribution), or a t distribution with 15 degrees of freedom (t15). The
statistician’s model states that the potential outcomes are a linear function of the
covariates, but with Normal errors (instead of t, the truth), and with the true proper
90

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
prior distributions on the regression parameters, i.e.,
Yi(Wi) = γ0 + γXXi + γWWi + εi,
with εi ∼N(0, σ2
y),
where (γ0, γX, γW) ∼N((β(0)
0 , β(0)
X , β(0)
W ), 0.0025I3) and σ2
y ∼Inv −gamma(v0, v0 ·s2
0).
We use the true values of the hyperparameters as speciﬁed in Section 2.6.1. Here,
∆M is the discrepancy in error distributions between the true and statistician’s model.
In this case, inference under all the assignment mechanisms considered gives sim-
ilar and approximately valid results (see Table 2.1). There is a consistent very slight
undercoverage (the 10th percentile of the distribution of PC(X, W ∗, Y obs
∗) is around
0.936 for the t7 speciﬁcation, and 0.946 for the t15 speciﬁcation), which is expected
because the Normal distribution used for analysis has lighter tails than the true t
distributions. Of course, the larger the degrees of freedom, the closer the t distri-
bution is to a Normal, and the better the ﬁt of the statistician’s model. Not much
problem is observed under this setting because even though there is misspeciﬁcation
of the conditional error distribution for Y given X, both the true and statistician’s
distributions are still symmetric and the linear relationship is correctly speciﬁed.
2.6.5
Example 4: Misspeciﬁed error distribution and prior
distributions
Here, we work under similar conditions as the previous example, where the error
probability distribution is misspeciﬁed but the functional relationship between out-
91

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
come and covariate is still correctly speciﬁed, with the added complication that the
prior distributions are now also misspeciﬁed.
We again consider two possibilities for the true conditional error distribution in
Equation 2.6.1: either a t7 or a t15.
The statistician’s model poses the potential
outcomes as a linear function of the covariates but with Normal errors; however,
“noninformative” prior distributions are used on the regression parameters, i.e.,
Yi(Wi) = γ0 + γXXi + γWWi + εi,
with εi ∼N(0, σ2
y),
where θY |X = (γ0, γX, γW, σ2
y), and
p(γ0, γX, γW, σ2
y | Y obs) ∝1/σ2
y.
In summary, ∆M includes the discrepancies in error distributions and in prior distri-
butions.
Under complete randomization, inference still provides approximately valid re-
sults when the analysis is done using the statistician’s model described above, for
both of the true t distribution speciﬁcations. The posterior distribution of the esti-
mand obtained through the statistician’s model is centered at the right place, and
has a larger variance than the true posterior distributions, as expected from the use
of noninformative priors. However, for the covariate-dependent assignment mecha-
nisms, the analyses result in huge undercoverage (to the extreme of even producing
92

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Bayesian coverage probabilities of zero), as shown in Table 2.1. This result occurs be-
cause the statistician’s posterior distributions for the treatment eﬀect under RR and
FSM are not centered at the right value (they are actually oﬀset towards zero), even
though their posterior variance is comparable to the one observed under a completely
randomized design. This behavior is illustrated in Figure 2.5 for a representative
example dataset. This behavior seems to be exacerbated in this example by the mis-
speciﬁcation of the error distribution, as compared to Example 1 where the error
distribution was correct, suggesting that there might be an interaction between the
choice of assignment mechanism and the likelihood misspeciﬁcation.
2.6.6
Example 5: Misspeciﬁed error distribution and rela-
tionship between Y and X
For this example, both the error distribution and the functional relationship be-
tween Y and X are misspeciﬁed, but we use the correct prior distributions. As in the
previous two examples, we consider two possibilities for the true error distribution in
Equation 2.6.1: either a t7 or a t15. The statistician’s model for the potential outcomes
is misspeciﬁed on one hand, because it assumes that the potential outcomes follow a
Normal distribution, and on the other hand, because it excludes the covariate, i.e.,
Yi(Wi) = µ + τWi + εi,
with εi ∼N(0, σ2
y).
Again, we use the true proper prior distribution (µ, τ) ∼N((β(0)
0 , β(0)
W ), 0.0025I2),
with β(0)
0
and β(0)
W as in Section 2.6.1, and the true inverse-gamma distribution for σ2
y.
93

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
CR (true t7)
treatment effect
Frequency
2.5
3.0
3.5
4.0
4.5
5.0
0
20
40
60
80
100
120
CR (noninf. priors)
treatment effect
Frequency
2.5
3.0
3.5
4.0
4.5
5.0
0
50
100
150
200
RR (true t7)
treatment effect
Frequency
-1
0
1
2
3
0
20
40
60
80
100
RR (noninf. priors)
treatment effect
Frequency
-1
0
1
2
3
0
100
200
300
FSM (true t7)
treatment effect
Frequency
-1
0
1
2
3
0
20
40
60
80
100
FSM (noninf. priors)
treatment effect
Frequency
-1
0
1
2
3
0
100
200
300
400
Figure 2.5: For Example 4: true and statistician’s posterior distributions of the average treatment eﬀect for a represen-
tative dataset, for the three assignment mechanisms considered, with β(0)
X = 3, and β(0)
W = 4. The true error distribution
shown is t7. The statistician’s model is a Normal linear regression with correct conditional expectation of Y given X
and “noninformative” priors.
94

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
In summary, ∆M includes discrepancies in error distributions and in the form of the
relationship of Y and X.
The results obtained in this scenario, with a non-null true treatment eﬀect, are
similar to those from Example 4 and are displayed in Table 2.1: complete random-
ization still provides valid results (with slight, but expected, overcoverage), and for
the rest of the assignment mechanisms, the results of the inference show a large pro-
portion of coverage probabilities being zero (the 25th percentile of the distribution is
zero). Again, the statistician’s posterior distributions for the treatment eﬀect under
RR and FSM are not centered at the right value, as illustrated in Figure 2.6 for a
representative example dataset. The oﬀsetting of the statistician’s posterior distribu-
tion of the estimand towards zero is not as extreme as in Example 4 because there
are proper prior distributions on some of the parameters, but it is still suﬃciently
away from the true value that it produces tiny Bayesian coverage probabilities.
2.6.7
Example 6: Alternative covariate distribution speciﬁ-
cation
Here, we reproduced the setup of Examples 1-5 above, with the only diﬀerence
being that the true distribution q(X | θX) is now a skew-Normal(0,1,4) distribution
(see Azzalini (1985) for full description of the skew-Normal distribution) instead of a
standard Normal. Table 2.2 shows the results for the setups described in Examples
1-5, but with this alternative covariate distribution speciﬁcation.
95

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
CR (true t7)
treatment effect
Frequency
3.5
4.0
4.5
5.0
5.5
0
20
40
60
80
100
120
CR (no covs.)
treatment effect
Frequency
3.5
4.0
4.5
5.0
5.5
0
50
100
150
200
RR (true t7)
treatment effect
Frequency
1.0
1.5
2.0
2.5
3.0
3.5
4.0
0
20
40
60
80
100
RR (no covs.)
treatment effect
Frequency
1.0
1.5
2.0
2.5
3.0
3.5
4.0
0
50
100
150
200
FSM (true t7)
treatment effect
Frequency
1.5
2.0
2.5
3.0
3.5
4.0
0
20
40
60
80
100
FSM (no covs.)
treatment effect
Frequency
1.5
2.0
2.5
3.0
3.5
4.0
0
50
100
150
Figure 2.6: For Example 5: true and statistician’s posterior distributions of the average treatment eﬀect for a represen-
tative dataset, for the three assignment mechanisms considered, with β(0)
X = 3, and β(0)
W = 4. The true error distribution
shown is t7. The statistician’s model is a Normal linear regression that excludes the covariate.
96

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Overall, the results presented in this section show a similar behavior to those ob-
tained through a standard Normal true covariate distribution. The only noticeable
diﬀerence was for Example 2, where we do not observe the pronounced overcoverage
under RR and FSM as we did for the N(0,1) case. The reason why there is not much
change in behavior as compared to the standard Normal true covariate distribution
is because we have made an independence assumption between θY |X and θX for the
true models throughout; therefore, we do not require the distribution of the covariate
to be modeled in the analysis. Thus, only the correct speciﬁcation of the likelihood
of Y given X and the prior distributions for θY |X play an important role.
Table 2.2: Summaries of posterior distributions of Bayesian coverage probabilities:
10th and 25th percentiles.
With β(0)
X
= 3, β(0)
W
= 4.
Covariate distribution X ∼
skew −Normal(0, 1, 4).
Assignment mechanism
Example no.
True
CR
RR
FSM
ε distr.
10 %ile
25 %ile
10 %ile
25 %ile
10 %ile
25 %ile
Example 1
Normal
0.86
0.97
0.87
0.97
0.85
0.97
Example 2
Normal
0.90
0.96
0.90
0.96
0.90
0.96
Example 3
t7
0.93
0.94
0.94
0.95
0.94
0.95
t15
0.94
0.95
0.94
0.95
0.94
0.95
Example 4
t7
0.85
0.98
0.00
0.00
0.00
0.00
t15
0.87
0.98
0.00
0.00
0.00
0.00
Example 5
t7
0.92
0.98
0.00
0.00
0.00
0.00
t15
0.90
0.97
0.00
0.00
0.00
0.00
97

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2.7
Simulation design for systematic study of
factors
In this section, we describe a simulation that would allow for a systematic initial
investigation of the factors that inﬂuence the Bayesian validity of inference for causal
eﬀects, when ignorable but covariate dependent experimental designs are used. The
proposed study can be compactly described as a (possibly fractional) factorial de-
sign. One example of a summary measure per cell is PC(X, W , Y obs), the Bayesian
coverage probability of a nominal 95% posterior interval for the estimand of interest
obtained using the statistician’s model, where the probability is calculated over the
true posterior distribution of the estimand; transformations of this measure can also
be considered.
Alternative measures may also include any of those introduced in
Section 2.5.3.
We now describe the 10 factors to consider in the study.
Factor 1: The assignment mechanism. We consider situations where the experi-
menter has complete control over the assignment mechanism. This factor is known to
the analyst, either through access to the design protocols or through communication
with the person who designed the experiment. Some examples of the levels that may
be chosen for this factor are described below.
1. Complete randomization: a ubiquitous assignment mechanism to which we wish
to compare the rest.
98

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
2. Randomized blocks design: commonly used when covariates that might be
highly correlated with the outcomes are available, although it is most useful
when there are only a few discrete covariates.
3. Rerandomization with Mahalanobis distance criterion: this is the balance cri-
terion proposed (and thoroughly explored) by Morgan and Rubin (2012), and
is based on the Mahalanobis distance between the covariate mean in the active
treatment group ( ¯Xt) and the covariate mean in the control treatment group
( ¯Xc), that is,
M ≡( ¯Xt −¯Xc)′[Cov( ¯Xt −¯Xc)]−1( ¯Xt −¯Xc),
where Cov( ¯Xt −¯Xc) is the sample covariance matrix of the diﬀerence in co-
variate means between the treatment and control groups. A randomization is
considered acceptable whenever M falls below a certain threshold, denoted by
a. Let pa denote the proportion of randomizations that are deemed acceptable,
which here we specify to be pa = 0.001. The acceptance threshold is then set
as the critical value such that P(M ≤a) = pa. We refer the reader to Morgan
and Rubin (2012) for speciﬁcs on determining the acceptance threshold given
pa, and for properties and implementation of the rerandomization algorithm.
An advantage of Mahalanobis distance is that it is aﬃnely invariant, that is, it
is unaﬀected by aﬃne transformations of the covariates.
4. Rerandomization with an expanded covariate matrix: higher moments or non-
linear functions of input covariates, interactions, and other transformations can
99

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
be balanced as well by including them in the matrix X. The decision to include
these should be based on subject-matter knowledge.
5. Finite selection model design with a selection function that minimizes the trace
of the matrix (X′(j)
w X(j)
w )−1 (where X(j)
w is the covariate matrix for units assigned
to treatment w up to selection round j) at each selection round j = 1, . . . , Nw
for each treatment group w (here, w ∈{0, 1}). This is a simpliﬁed case of the
function suggested by Morris (1979), with equal costs for all units and equal
weights for all standardized covariates, and ignores the correlations between
covariates. This criterion is aﬃnely invariant.
6. Finite selection model design with a selection function that minimizes the de-
terminant of the matrix (X′(j)
w X(j)
w )−1 at each selection round j = 1, . . . , Nw for
each treatment group w. This criterion is also aﬃnely invariant.
The choice of selection functions for FSM is based on their close link to optimal
design of experiments theory; in addition, FSM (with either the trace or determinant
criteria) is a “greedy” version of RR with Mahalanobis distance. For the case of one
covariate, the criteria for FSM described above are equivalent and result in selecting,
from the pool of available experimental units at each step, the one that maximizes
the sample variance of the covariate in the treatment group to which it would be
assigned. However, for multiple covariates the diﬀerent FSM criteria are no longer
equivalent.
Rerandomization and FSM are simple methods that provide the advantage of bal-
ancing the covariates for the speciﬁc experiment, but we need to understand their
100

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
consequences on Bayesian validity for inference in order to advocate their general-
ized implementation in practice.
We hope these methods will gain more traction
among practitioners, especially with the recent publication of theoretical results for
randomization-based inference under rerandomization in Morgan and Rubin (2012).
In a follow-up experiment, we could investigate additional assignment mechanisms
such as multivariate matching (e.g., see Greevy et al. (2004)) or methods of sequen-
tial treatment assignment.
Factor 2: Estimand. Some examples of levels that may be chosen for this factor
are:
1. average causal eﬀect for the superpopulation (i.e., the parameter governing the
distribution);
2. average causal eﬀect for the ﬁnite population;
3. median diﬀerence between active and control treatment potential outcomes in
the superpopulation;
4. diﬀerence in median potential outcomes between active and control treatment
for the ﬁnite population.
The motivation behind these choices is their simplicity and ease of interpretation, as
well as their potential use in practical applications. The average causal eﬀect is widely
used in practice, and we want to contrast with non-linear estimands. Additionally,
superpopulation estimands are a very common choice because they correspond to
101

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
parameters in models, but Bayesian methods are also ideally suited for ﬁnite popu-
lation inference. If the results show that this factor is inﬂuential, other possibilities
such as ratios or diﬀerent percentiles (or odds ratios if the outcome is binary) may
be explored in a follow-up experiment.
The next two factors are completely known to the investigator in practice.
Factor 3: Number of experimental units (N). It is of interest to investigate
whether there are important diﬀerences when dealing with a small or a large sample
size. We expect N to aﬀect the spread of the posterior distribution of the estimand of
interest, both in the true model and the statistician’s model. For large N, these pos-
terior distributions would tend to resemble point masses; therefore, we would expect
small to no overlap (and thus, small Bayesian probability coverages). For small N,
we might expect a big overlap between the true and the statistician’s posterior distri-
bution because of their larger spread; however, with small sample sizes there might
also be a larger dependence of the posterior distributions on the prior distributions
used.
Additionally, we should consider the allocation ratio for active treatment and con-
trol treatment (Nt/Nc). In our examples in Section 2.6, we assumed the allocation
ratio to be 1:1 (i.e., Nt = Nc). This ratio is likely to aﬀect the spread of the pos-
terior distributions of the estimands, because the sample size of a treatment group
will aﬀect the precision of the estimation within that group. It will also aﬀect how
102

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
the covariate-dependent treatment assignment mechanisms should be speciﬁed, if the
treatment groups do not have the same number of units. For example for the RR de-
sign, some suggestions for obtaining treatment groups of unequal sizes are discussed
in Morgan and Rubin (2012), including randomizing units into multiple (more than
2) equally sized treatment groups, and then merging groups after the allocation has
been established.
Factor 4: Number of covariates (K). We expect some diﬀerences to arise be-
tween the cases with univariate and multivariate X. For instance, the criteria for
FSM are equivalent in the univariate case, but not in the multivariate case. Also, in
the case of multivariate X one could take into account as an additional factor the
K × (K −1)/2 correlations between the covariates; this allows us to investigate their
eﬀect on the standard errors for the estimates (both super and ﬁnite population), un-
der the diﬀerent assignment mechanisms. Additional considerations for multivariate
X are brieﬂy discussed in the description of the next factor and in Section 2.7.1.
The next four factors specify the probability distributions used in the true data-
generating process.
Factor 5: q(X | θX). We may consider diﬀerent choices for the marginal distribution
of the covariates X, for example:
1. (multivariate) Normal;
2. (multivariate) skew-Normal (see Azzalini (1985));
103

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
3. binary;
4. a combination of binary and continuous.
The covariate distributions in practice are only to some extent under the statistician’s
control: they have to be estimated from real data, but he can compute transforma-
tions of X such that the (yet unobserved) outcome Y is approximately linear in this
version of X. Some simpliﬁcations can be made for this factor. For example, we
could ﬁrst assume that the covariates denoted X that are used for assignment (and
later on, for analysis) already include all covariates and transformations that are cor-
related with the outcome and such that Y is approximately linear in X; that is, we
could assume that the experimenter has some prior knowledge about what covariates
are important, thus disregarding the need for variable selection in this initial simu-
lation study. Second, we could consider the simple case where, if there are multiple
covariates, they are uncorrelated and of equal importance (therefore, they undergo a
standardization process); an extension to this is brieﬂy discussed in Section 2.7.1.
Factor 6: q(θX). The next choice is the true prior distribution of the parameter
of the marginal distribution of X, θX. This factor is interesting in order to investi-
gate potential invariances with respect to the covariance distribution speciﬁcations.
For example, Mahalanobis distance is aﬃnely invariant, thus the RR design using
this balance criterion will not be aﬀected by aﬃne transformations of the covariates.
From properties of the determinant and trace, the minimization problem that needs
to be sequentially solved as part of the FSM design procedure also will not be altered.
104

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
For the case of a continuous covariate, some examples of levels for this factor
could include choosing the prior distribution such that the covariates are either in
a standardized form (e.g., mean zero and variance one), or not standardized. For
a binary covariate, we would need to specify the probability of “success”, and we
can consider for example symmetric distributions (such as Bern(1/2)), or asymmetric
distributions (e.g., Bern(1/3)). If a simpliﬁcation is made in q(X | θX) that involves
assuming uncorrelated covariates, then these speciﬁcations are still valid in the case
of multivariate X.
Factor 7: q(Y | X, θY |X). Next is the speciﬁcation of the likelihood, or the con-
ditional distribution of Y given X. As discussed in Section 2.5.1, we care about
this factor, together with Factor 8, in order to investigate the plausible range of true
models for which the distribution of Bayesian coverage probabilities is tightly con-
centrated about the nominal level.
We could, for example, focus on the simple case of parallel response surfaces.
In particular, we would assume a simple linear relationship between the potential
outcomes and the covariates with additive errors,
Yi(Wi) = β0 + β′
XXi + βWWi + εi,
with εi ∼[0, σ2
y].
This is the setup that we adopted for our examples in Section 2.6. Under this
particular speciﬁcation, the deﬁnition of this factor reduces to determining the dis-
tribution of ε; in Section 2.6, we chose a Normal distribution, a t distribution with
105

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
7 degrees of freedom (t7), and a t distribution with 15 degrees of freedom (t15). We
chose the Normal speciﬁcation because in many cases the data can be approximated
reasonably well through a Normal distribution, and the t distributions as alternatives
for cases where we observe heavier tails.
Factor 8: q(θY |X | θX). One example for how to approach this factor is through a
commonly made simpliﬁcation: assuming a priori independence between the param-
eters that govern the conditional distribution of Y given X, and those that govern
the marginal distribution of X, i.e.,
q(θY |X | θX) = q(θY |X).
If this simpliﬁcation is made, we may focus solely on the prior distribution of θY |X;
under the example speciﬁcation of the conditional distribution of Y given X given
in Factor 7, we would have θY |X = (β0, βX, βW, σ2
y). Continuing with the same ex-
ample, we could propose the use of conjugate prior distributions for these regression
parameters, i.e.,
(β0, βX, βW) ∼N((β(0)
0 , β(0)
X , β(0)
W ), aI3) for some a > 0,
σ2
y ∼Inv −gamma(v0, v0 · s2
0).
This factor can be better described as a smaller full factorial design for the six factors
consisting of the hyperprior means and variance of the regression coeﬃcients, and the
parameters for the residual variance: β(0)
0 , β(0)
X , β(0)
W , a, v0, s2
0.
106

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
The last four factors specify the probability distributions proposed by the statis-
tician for data analysis. Factors 9 and 10 govern the distribution of the covariates,
and Factors 11 and 12 govern the conditional distribution of Y given X.
Factor 9: p(X | θX). The marginal distribution of the covariates X, used by the
statistician. Choices for these are analogous to those described above for q(X | θX).
Factor 10: p(θX). The prior distribution of the parameter of the marginal distribu-
tion of X, assumed by the statistician.
Factor 11: p(Y | X, θY |X). Some examples that can be chosen for the statistician’s
conditional distribution of Y given X are:
1. normal linear regression that includes all covariates in the functional relation-
ship;
2. normal linear regression only on treatment indicator (excludes the rest of the
covariates from the analysis).
Normal linear regression is a technique that is widely used in practice. Considering
the examples above would allow for the evaluation of two types of misspeciﬁcation:
the error distribution and the functional relationship between Y and X. When X is
multivariate, there may be many choices of which and how many covariates to exclude
(especially if they vary in importance).
107

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Factor 12: p(θY |X | θX). For the choice of prior distributions for the statistician’s
model, some examples that we propose are:
1. proper prior distributions;
2. “noninformative” prior distributions (e.g., Jeﬀreys’ priors).
These are standard choices in practice: proper distributions for those cases where
there is some knowledge about the parameters (e.g., from previous experiments) and
the additional information can be justiﬁed, and noninformative priors otherwise.
If for the analysis we assume a priori independence of θY |X and θX, then this
condition, together with an ignorable assignment mechanism and fully recorded co-
variates, eliminates the need to model X in the analysis, and it is not necessary to
include Factors 9 and 10. In that particular case, we need only consider Factors 11
and 12 in order to specify the probability distributions used in the analysis.
2.7.1
Further extensions
One extension to the simulation design presented above would be to address ad-
ditional complications that may arise in the case of multivariate X. For example, one
could relax the assumption that all the covariates are of equal importance, and allow
some of them to be more highly correlated with the outcome than others. This will,
on one hand, aﬀect how the assignment to treatment is to be conducted. For the
case of rerandomization, Morgan and Rubin (2016) developed theory for using tiers
of covariates of varying importance. For FSM, these conditions can be incorporated
108

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
into the selection criteria through the use of weights. On the other hand, the factor
that deals with the speciﬁcation of the true prior distribution q(θY |X) will now have
to include distinct hyperparameters for the prior means of the regression coeﬃcients
(i.e., β(0)
X,k, k = 1, . . . , K) to capture their varying importance.
An additional consideration regarding the covariates is that there might be a
discrepancy between having the theoretical distributions of X (in analytic form) as
compared to the empirical distributions that have to be estimated from the real data.
Further investigation might shed light on whether substitution of the theoretical dis-
tributions with estimates has any impact on the analysis.
A second extension would be to include alternative conditional distributions for
the outcome. For example, taking outcomes that are strictly positive or binary out-
comes would entail consideration of transformations of the outcome and possibly the
use of Bayesian generalized linear models to model the functional relationship be-
tween Y and X for both the data generation process and analysis.
Finally, it is possible to extend the simulation to consider multiple (more than
two) treatment groups. A choice of multivariate balance measure needs to be made
for RR; an extension of FSM is straightforward because all groups would just need
to follow the same decision criteria to choose the units. The estimands in this setting
may change: we could consider, as an example, contrasts involving the group mean
potential outcomes. However, we are not limited by one-dimensional summaries, and
109

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
the true and statistician’s distributions might now be ellipsoids. The idea of Bayesian
coverage probabilities of posterior intervals can be extended under these conditions
to coverage of posterior regions.
2.8
Conclusions
We explored, through a sequence of examples, the impact of misspecifying the
data models used for analysis (when the assignment to treatment is ignorable but
covariate-dependent) on Bayesian estimation of treatment eﬀects in experimental set-
tings. We did this through a Bayesian evaluation procedure, in which the measure
of discrepancy between the true and the statistician’s posterior distributions of the
estimand of interest is given by the Bayesian coverage probability of the statistician’s
posterior interval over the true distribution.
A Bayesian statistician’s concern is not, in general, to evaluate the simple fre-
quency properties of the Bayesian procedures; rather, he assumes a model, ﬁts it, and
then checks the ﬁt to existing and future data. The assumption is, in the famous
words of G.E.P. Box, that “all models are wrong, but some are useful”; therefore,
a Bayesian statistician should care more about the consequences of using the wrong
models, or about what range of models provide valid answers when used with a partic-
ular procedure, than about simple frequency properties. Here, we choose to perform
this type of evaluation using Bayesian coverage probabilities, and it allows us to in-
vestigate the need for correct speciﬁcation of models for the data.
110

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
Covariate-dependent assignment mechanisms such as rerandomization and FSM
are particularly useful when strong correlations exist between the covariates and the
outcome, and may appeal to statisticians because they guarantee balance in the co-
variates for the dataset at hand (as contrasted to balance in expectation that is
provided by complete randomization). However, even though they are ignorable for
Bayesian inference, the datasets that are observed under diﬀerent assignment mech-
anisms diﬀer and can make the analysis sensitive to the model speciﬁed for the data.
We exempliﬁed the important role that the speciﬁcation of the models for data
analysis plays on the Bayesian validity of causal inferences under ignorable assign-
ments. In particular, we demonstrated that ignorability in practice eﬀectively de-
pends, to some extent, on having the correct model speciﬁcation for analysis. Not
surprisingly, complete randomization seems to be more robust to model misspeciﬁca-
tions than the covariate-dependent assignment mechanisms. This perhaps suggests
a trade-oﬀbetween the advantages of procuring better balance (which leads to more
comparable treatment groups and therefore better justiﬁcation for our estimands as
causal eﬀects) and more sensitivity to the correct models for the data in the analysis.
Our results do not necessarily mean that covariate-dependent assignment mechanisms
should not be used, but rather that more care might be needed in assessing models.
Additionally, Bayesian evaluations of the type used here have rarely been done, and
because they are very strict, they often give poor results; this might justify the need
for frequency calibrations of Bayesian procedures to some extent. Finally, we outlined
the issues arising in a more comprehensive simulation design to investigate the factors
111

Chapter 2: Illustrating the inﬂuence of ignorable but covariate dependent
experimental designs on the Bayesian validity of inference for causal eﬀects
that are likely to inﬂuence the Bayesian validity of inference for causal eﬀects.
112

Appendix A
Additional results from the
analysis of IBCSG dataset
Table A.1: Summaries of posterior draws for all parameters in the full model (without
covariates). The parameterization used is λ = 1/σ.
Parameter
Mean
SD
2.5th perc.
97.5th perc.
α
1.074
0.067
0.944
1.208
λ1,c
0.050
0.004
0.043
0.059
λ0,c
0.053
0.005
0.045
0.064
λa
0.076
0.025
0.037
0.134
λn
0.074
0.035
0.028
0.163
πc
0.935
0.012
0.910
0.956
πa
0.036
0.009
0.021
0.055
πn
0.029
0.008
0.016
0.046
113

Appendix A: Additional results from the analysis of IBCSG dataset
Table A.2: Summaries of posterior draws for all parameters in the full model (with
covariates).
Parameter
Mean
SD
2.5th perc.
97.5th perc.
βa
-3.534
0.306
-4.170
-2.973
βn
-3.676
0.302
-4.295
-3.109
β(1)
a,X
-0.155
0.267
-0.702
0.354
β(2)
a,X
0.179
0.327
-0.399
0.869
β(3)
a,X
-0.113
0.238
-0.576
0.362
β(4)
a,X
0.507
0.264
-0.003
1.030
β(5)
a,X
0.602
0.480
-0.152
1.721
β(6)
a,X
-0.563
0.322
-1.270
0.004
β(1)
n,X
0.689
0.460
-0.076
1.724
β(2)
n,X
-0.339
0.246
-0.806
0.160
β(3)
n,X
0.032
0.258
-0.478
0.543
β(4)
n,X
0.179
0.265
-0.344
0.698
β(5)
n,X
-0.412
0.191
-0.767
-0.011
β(6)
n,X
-0.308
0.278
-0.890
0.211
α
1.031
0.068
0.900
1.167
γ1,c
-2.953
0.089
-3.117
-2.768
γ0,c
-2.866
0.097
-3.047
-2.666
γa
-2.265
0.325
-2.923
-1.646
γn
-2.385
0.412
-3.143
-1.526
γ(1)
X
-0.006
0.055
-0.116
0.102
γ(2)
X
0.051
0.058
-0.063
0.166
γ(3)
X
-0.013
0.047
-0.109
0.078
γ(4)
X
-0.087
0.050
-0.184
0.011
γ(5)
X
-0.091
0.053
-0.195
0.015
γ(6)
X
0.021
0.048
-0.073
0.115
114

Appendix A: Additional results from the analysis of IBCSG dataset
Table A.3: Summaries of posterior distributions for ﬁnite population complier diﬀer-
ence in survival proportions, τc(t), at diﬀerent time points. IBCSG data.
Time (years)
Mean
SD
Median
2.5th perc.
97.5th perc.
1
0.010
0.007
0.009
-0.004
0.025
2
0.019
0.011
0.019
0.000
0.041
3
0.028
0.013
0.027
0.005
0.055
4
0.037
0.015
0.036
0.009
0.067
5
0.047
0.016
0.047
0.017
0.080
6
0.057
0.017
0.056
0.024
0.092
Table A.4: Summaries of posterior distributions for ITT overall ﬁnite population
diﬀerence in survival proportions, τ(t), at diﬀerent time points. IBCSG data.
Time (years)
Mean
SD
Median
2.5th perc.
97.5th perc.
1
0.010
0.007
0.010
-0.003
0.026
2
0.021
0.010
0.020
0.001
0.042
3
0.030
0.013
0.030
0.007
0.056
4
0.039
0.014
0.038
0.013
0.068
5
0.051
0.016
0.050
0.022
0.083
6
0.061
0.017
0.060
0.029
0.094
115

Bibliography
Angrist, J. D., G. W. Imbens, and D. B. Rubin (1996).
Identiﬁcation of causal
eﬀects using instrumental variables. Journal of the American Statistical Associa-
tion 91(434), 444–455.
Azzalini, A. (1985). A class of distributions which includes the normal ones. Scandi-
navian journal of statistics, 171–178.
Cox, D. R. and D. Oakes (1984). Analysis of survival data. Chapman and Hall.
Frangakis, C. and D. B. Rubin (1999).
Addressing complications of intention-to-
treat analysis in the combined presence of all-or-none treatment-noncompliance
and subsequent missing outcomes. Biometrika 86(2), 365–379.
Frangakis, C. and D. B. Rubin (2002). Principal stratiﬁcation in causal inference.
Biometrics 58, 21–29.
Galimberti, V., B. F. Cole, S. Zurrida, G. Viale, A. Luini, P. Veronesi, P. Baratella,
C. Chifu, M. Sargenti, M. Intra, et al. (2013). Axillary dissection versus no axillary
dissection in patients with sentinel-node micrometastases (IBCSG 23–01): a phase
3 randomised controlled trial. The Lancet Oncology.
Gelman, A. and D. Rubin (1992). Inference from iterative simulation using multiple
sequences. Statistical Science, 457–472.
Greevy, R., B. Lu, J. H. Silber, and P. Rosenbaum (2004). Optimal multivariate
matching before randomization. Biostatistics 5(2), 263–275.
Holland, P. W. (1986).
Statistics and causal inference.
Journal of the American
Statistical Association 81(396), 945–960.
Ibrahim, J. G., M.-H. Chen, and D. Sinha (2001).
Bayesian Survival Analysis.
Springer.
Imbens and Rubin (1997). Bayesian inference for causal eﬀects in randomized exper-
iments with noncompliance. The Annals of Statistics 25(1), 305–327.
116

Bibliography
Imbens, G. and D. B. Rubin (2015).
Causal Inference for Statistics, Social, and
Biomedical Sciences. An Introduction. Cambridge University Press.
Kalbﬂeisch, J. D. and R. L. Prentice (2002). The statistical analysis of failure time
data, Volume 360. John Wiley & Sons.
Kaplan, E. and P. Meier (1958). Nonparametric estimation from incomplete obser-
vations. Journal of the American Statistical Association 53, 457–481.
Little, R. and D. B. Rubin (2002). Statistical Analysis with Missing Data. Wiley.
Loeys, T. and E. Goetghebeur (2003). A causal proportional hazards estimator for
the eﬀect of treatment actually received in a randomized trial with all-or-nothing
compliance. Biometrics 59(1), 100–105.
Mealli, F., G. W. Imbens, S. Ferro, and A. Biggeri (2004). Analyzing a random-
ized trial on breast self-examination with noncompliance and missing outcomes.
Biostatistics 5(2), 207–222.
Morgan, K. L. and D. B. Rubin (2012). Rerandomization to improve covariate balance
in experiments. The Annals of Statistics 40(2), 1263–1282.
Morgan, K. L. and D. B. Rubin (2016). Rerandomization to balance tiers of covariates.
Journal of the American Statistical Association.
Morris, C. N. (1979). A ﬁnite selection model for experimental design of the health
insurance study. Journal of Econometrics 11, 43–61.
Morris, C. N. and J. L. Hill (2000). The Health Insurance Experiment: design using
the ﬁnite selection model. Public policy and statistics: case studies from RAND,
29–53.
Neyman, J., D. Dabrowska, and T. Speed (1990). On the application of probabil-
ity theory to agricultural experiments. Essay on principles. Section 9. Statistical
Science 5(4), 465–472. Translated from Polish to English.
Peng, Y., R. J. Little, and T. E. Raghunathan (2004). An extended general location
model for causal inferences from data subject to noncompliance and missing values.
Biometrics 60(3), 598–607.
Rao, J., W. Jocelyn, and M. Hidiroglou (2003). Conﬁdence interval coverage prop-
erties for regression estimators in uni-phase and two-phase sampling. Journal of
oﬃcial statistics - Stockholm 19(1), 17–30.
Reid, N. (1994). A conversation with Sir David Cox. Statistical Science, 439–455.
117

Bibliography
Robins, J. M. and A. A. Tsiatis (1991). Correcting for non-compliance in randomized
trials using rank preserving structural failure time models. Communications in
Statistics-Theory and Methods 20(8), 2609–2631.
Rosenbaum, P. and D. B. Rubin (1984a). Sensitivity of bayes inference with data-
dependent stopping rules. The American Statistician 38(2), 106–109.
Rosenbaum, P. R. and D. B. Rubin (1984b). Reducing bias in observational studies
using subclassiﬁcation on the propensity score. Journal of the American Statistical
Association 79(387), 516–524.
Rubin, D. B. (1974).
Estimating causal eﬀects of treatments in randomized and
nonrandomized studies. Journal of Educational Psychology 66, 668–701.
Rubin, D. B. (1978). Bayesian inference for causal eﬀects: The role of randomization.
The Annals of Statistics 6(1), 34–58.
Rubin, D. B. (1980). Discussion of “Randomization analysis of experimental data:
The Fisher randomization test” by D. Basu. Journal of the American Statistical
Association 75, 591–593.
Rubin, D. B. (1984). Bayesianly justiﬁable and relevant frequency calculations for
the applied statistician. The Annals of Statistics 12(4), 1151–1172.
Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. New York,
NY: Wiley.
Rubin, D. B. (1991). Practical implications of modes of statistical inference for causal
eﬀects and the critical role of the assignment mechanism. Biometrics, 1213–1234.
Stan Development Team (2013). Stan: A C++ library for probability and sampling,
version 2.0.
118

