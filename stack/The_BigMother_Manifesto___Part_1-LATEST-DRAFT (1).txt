The BigMother Manifesto: A Roadmap to Provably
Maximally-Aligned Maximally-Superintelligent AGI (Part 1)
[unfinished draft]

TURNER
The BigMother Manifesto: A Roadmap to Provably
Maximally-Aligned Maximally-Superintelligent AGI (Part 1)
Aaron Turner
AARON.TURNER@BIGMOTHER.AI
BigMother.AI CIC
Cambridge, UK
Timestamp: 2024-07-28 19:01:08Z
Abstract
Whoever owns human-level AGI (Artificial General Intelligence) will own the global means
of production for all goods and services [Brynjolfsson (2022)]. Superintelligent AGI has been
conservatively estimated to have a net present value of ~$15 quadrillion [Russell (2024)]. Accordingly,
the major equity-funded/profit-motivated AI labs (and their associated sovereign states), being
aggressively competitive by nature, are engaged in an AGI arms race [Ramamoorthy and Yampolskiy
(2018)], each in pursuit of their own short-term self-interest, seemingly oblivious to the long-term
best interest of the human species as a whole [Hardin (1968); Dawes (1980); Alexander (2014)].
Rather than engage in a race, over (realistically) the next 20-30 years, towards an AGI future that,
due to competitive race dynamics and the trapdoor nature of superintelligence [Yudkowsky (2021)],
is likely to be, at best, hugely sub-optimal for all mankind for all eternity, and, at worst, catastrophic
[Hendrycks, Mazeika, and Woodside (2023)], we propose spending 50-100 years doing it properly
[Russell (2022); Russell (2023); Tegmark and Omohundro (2023)], in the best interest of all mankind,
in order to achieve an AGI endgame that is (as close as possible to) maximally-beneficent for all
mankind, while at the same time using the additional breathing space to mitigate to the maximum
extent possible the inevitable pain of such a profound societal transition [Altman (2023)].
Our overall approach is to try to imagine the ideal AGI endgame (from the perspective of the
human species as a whole), and to work backwards from there in order to make it (or something
close to it) actually happen. This is largely equivalent to imagining the ideal (or "Gold Standard")
superintelligent AGI, and then working backwards to actually build it. To this end, we seek to
design, develop, and deploy a provably maximally-aligned maximally-superintelligent AGI (called
BigMother/BigMom) that is ultimately owned by all mankind (via the United Nations), and whose
operation benefits all mankind, without favouring any subset thereof (such as the citizens of any
particular country or countries, or the shareholders of any particular company or companies).
In this paper, we describe the BigMother cognitive architecture and associated BigMother project
in detail. Together, these define an AGI research agenda for the next 50-100 years.
Keywords: Artificial General Intelligence, superintelligence, cognitive architecture, alignment,
Turner’s Three Laws, knowledge representation, induction, deduction, abduction, generic problem-
solving, program synthesis, continuous learning, continuous planning, symbolic-connectionist hybrid
For Sir Clive Sinclair
(30 July 1940 – 16 September 2021)
Copyright 2023-2024 BigMother.AI CIC
2

THE BIGMOTHER MANIFESTO (PART 1)
Contents
1
Overview
5
2
TL;DR (NOTE — This section is incomplete)
6
2.1
Section 3 — Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2
Section 4 — Cognitive Architecture
. . . . . . . . . . . . . . . . . . . . . . . . .
6
2.3
Section 5 — Construction Sequence . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.4
Section 6 — Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.5
Section 7 — Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.6
Section 8 — Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3
Introduction
7
3.1
It takes a village . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.2
Basic concepts and definitions pertaining to AGI
. . . . . . . . . . . . . . . . . .
7
3.2.1
What is intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.2.2
Physical systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.2.3
Extrinsic intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.2.4
Analogous intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.2.5
Human-like intelligence
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.2.6
Problem-solving
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.2.7
Artificial Intelligence (AI), and passive problem-solvers
. . . . . . . . . .
9
3.2.8
Problem statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
3.2.9
Problem solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.2.10 Solution quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.2.11 Narrow AI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.2.12 Artificial General Intelligence (AGI) . . . . . . . . . . . . . . . . . . . . .
11
3.2.13 AGI competence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.2.14 The alpha AGI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.2.15 Superintelligent narrow AI . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2.16 Superintelligent AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2.17 Maximally-superintelligent AGI . . . . . . . . . . . . . . . . . . . . . . .
14
3.2.18 Example — GPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.2.19 Example — DALL·E . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
3.2.20 Example — ChatGPT
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.2.21 Don’t Panic!
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.2.22 Intrinsic intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.2.23 The role of information in problem-solving . . . . . . . . . . . . . . . . .
19
3.2.24 Maximally-intrinsic passive problem-solvers
. . . . . . . . . . . . . . . .
20
3.2.25 Maximally-intrinsic superintelligent AGI . . . . . . . . . . . . . . . . . .
23
3.2.26 The path to superintelligent AGI . . . . . . . . . . . . . . . . . . . . . . .
23
3.2.27 Induction, deduction, abduction, and weak vs strong reasoning . . . . . . .
24
3.2.28 Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.2.29 Active problem-solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.2.30 Continuous planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
Copyright 2023-2024 BigMother.AI CIC
3

TURNER
3.2.31 Continuous learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.2.32 The sense-effect loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2.33 The structure of the physical universe . . . . . . . . . . . . . . . . . . . .
29
3.2.34 Internal models of the physical universe, and depth of understanding . . . .
30
3.2.35 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.2.36 Codependence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.2.37 Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3.2.38 From internal model and final goal to actions and causal effects
. . . . . .
33
3.2.39 Autonomous robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.2.40 Liveness and safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2.41 Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.2.42 Human values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.2.43 Ordinal preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.2.44 Utility functions, and expected utility . . . . . . . . . . . . . . . . . . . .
37
3.2.45 Eliciting human preferences . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.2.46 Aggregating human preferences . . . . . . . . . . . . . . . . . . . . . . .
39
3.2.47 Realising human preferences . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.2.48 Maximal alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.2.49 Alignment is everything! . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.2.50 X-risk and s-risk
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.2.51 The interplay between intelligence and alignment . . . . . . . . . . . . . .
43
3.2.52 Well-founded AGI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.2.53 The alignment matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.3
Consciousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4
The specific problem that we seek to address . . . . . . . . . . . . . . . . . . . . .
45
3.5
Our proposed approach to the problem that we have identified
. . . . . . . . . . .
45
4
Cognitive Architecture (NOTE — This section is incomplete)
46
5
Construction Sequence (NOTE — This section is incomplete)
47
6
Governance (NOTE — This section is incomplete)
47
7
Execution (NOTE — This section is incomplete)
47
8
Conclusion (NOTE — This section is incomplete)
47
9
Acknowledgements
47
Copyright 2023-2024 BigMother.AI CIC
4

THE BIGMOTHER MANIFESTO (PART 1)
1. Overview
As a field, artificial intelligence has always been on the border of respectability, and therefore on
the border of crackpottery. Many critics have urged that we are over the border. We have been very
defensive toward this charge, drawing ourselves up with dignity when it is made and folding the cloak
of Science about us. On the other hand, in private, we have been justifiably proud of our willingness
to explore weird ideas, because pursuing them is the only way to make progress. McDermott (1976)
This is a very long (book-length!) paper. We have tried to make the paper accessible to non-
technical professionals (e.g. policymakers). Key words and phrases are highlighted in bold in order
to facilitate skimming on first reading. The structure of the paper may be summarised as follows:
• In Section 2, we summarise the paper itself.
• In Section 3, we:
– explain why the current paper is the first in a sequence of BigMother papers
– introduce a number of basic concepts and definitions pertaining to AGI1
– explore the concept of consciousness in relation to AGI
– describe the specific problems that we seek to address
– describe our proposed approach to the problems that we have identified.
• In Section 4, we describe a cognitive architecture for a proposed Gold Standard AGI
– this section includes a tentative solution to the AI alignment problem.
• In Section 5, we describe a construction sequence for the proposed Gold Standard AGI
– any AGI so constructed will be well-founded by virtue of its method of construction.
• In Section 6, we explore the problem of AGI governance.
• In Section 7, we propose a coordinated collaboration of international experts to:
– safely build and deploy the proposed Gold Standard AGI
– participate in the global AGI governance process.
• In Section 8, we offer our conclusions.
• In Section 9, we thank our informal reviewers for their contributions.
1 Artificial General Intelligence
Copyright 2023-2024 BigMother.AI CIC
5

TURNER
2. TL;DR (NOTE — This section is incomplete)
2.1 Section 3 — Introduction
2.2 Section 4 — Cognitive Architecture
2.3 Section 5 — Construction Sequence
2.4 Section 6 — Governance
2.5 Section 7 — Execution
2.6 Section 8 — Conclusion
Copyright 2023-2024 BigMother.AI CIC
6

THE BIGMOTHER MANIFESTO (PART 1)
3. Introduction
This paper describes work conducted by the author from 1985 to 2024, but not previously reported2.
3.1 It takes a village
Artificial General Intelligence (AGI) [Turing (1950); Goertzel and Pennachin (2007); Goertzel and
Wang (2007); Wang and Goertzel (2012)] — the central subject of this paper — is complicated.
Thus (i) the opportunities for miscommunication by an author, and misunderstanding by a reader,
are endless, and (ii) it takes a village to build an AGI, and a particularly large and varied village to
build a superintelligent AGI [Good (1966); Bostrom (2014); Yampolskiy (2016)].
On the one hand, it is extremely desirable, when embarking upon a technical project, to have a
description of that project that is accessible to technical and non-technical project contributors alike —
i.e. the entire village — in order that everyone involved has a baseline conceptual understanding of
what it is that they’re actually doing. (In particular, the dominant factor determining the accessibility
of a document will be how mathematical it is.) On the other hand, more advanced readers will
expect a much deeper technical exposition. One document cannot possibly satisfy both audiences!
Accordingly, we envisage a sequence of BigMother papers as follows:
Part
Accessible?
Description
1
yes
assumes high-school mathematics plus an undergraduate degree (any subject)
≥2
no
expands upon its predecessor; may contain more advanced mathematics
Although quite long (currently 54 pages), Part 1 is nevertheless designed to be an easy read.
Given the complexities of AGI, Part 1 is a good starting point even for more advanced readers.
3.2 Basic concepts and definitions pertaining to AGI
There are a lot of pieces to the AGI jigsaw puzzle. In this Section, we introduce the key ideas.
3.2.1 WHAT IS INTELLIGENCE?
It’s not really possible to answer questions about intelligence, Artificial Intelligence (AI), or AGI
without first considering what these things mean. Despite prior attempts at providing formal definitions
[e.g. Barr, Feigenbaum, and Cohen (1982); Gottfredson (1997); Nilsson (2010); Russell and Norvig
(2021a); Wang (2019); Monett, Lewis, and Thórisson (2020)], the concept of intelligence remains
elusive, i.e. "intelligence" means different things to different people. In the absence of any widely
accepted definitions, we will attempt to define these concepts relative to our specific purposes.
3.2.2 PHYSICAL SYSTEMS
Assuming that the physical universe exists (an assumption to which we will return later), any physical
system (i.e. any part of the physical universe) may potentially be intelligent (or not) relative to some
definition. Physical systems include rocks, cats, humans, organisations, computers, and robots.
2 *** funding acknowledgement goes here
Copyright 2023-2024 BigMother.AI CIC
7

TURNER
3.2.3 EXTRINSIC INTELLIGENCE
From the perspective of an external observer, a physical system is either a black box or a white box.
If it’s a white box then we can see inside it (i.e. we can see at least some of its internal structure); if
it’s a black box then we cannot. In the case of a black box, the only information we have pertaining
to its intelligence is its externally observable behaviour, i.e. its pattern of interaction with its
physical environment. As a first approximation, a physical system possesses extrinsic intelligence
if an external observer deems it to be behaving intelligently (relative to the observer’s intuitive
understanding of what "intelligence" is) solely on the basis of its externally observable behaviour3.
3.2.4 ANALOGOUS INTELLIGENCE
On what other basis might an external observer deem a physical system to be behaving intelligently
(or otherwise)? Let’s imagine that an external observer (i) has already concluded that physical system
is intelligent, and (ii) has determined, after a period of observation, that physical system ’s
pattern of external behaviour is analogous to physical system ’s pattern of external behaviour
(in other words, if certain aspects are ignored, and others are retained, then and may be regarded
as behaving "equivalently"). If conditions (i) and (ii) are satisfied then is deemed to be intelligent
(by i), and are deemed to be "equivalent" (by ii), and therefore may be deemed to be intelligent.
Note that the precise nature of the analogy applied at step (ii) — specifically, which aspects of
externally observable behaviour are ignored, and which are retained — is of paramount importance.
Different analogies (focusing on different aspects) may lead to diametrically opposite conclusions.
3.2.5 HUMAN-LIKE INTELLIGENCE
Given that intelligence is deemed to be the quality that most distinguishes humans [Harari (2011)] from
other species, it is natural to use humans as the reference intelligence against which physical
system is compared in order to determine (by analogy) whether or not the latter is intelligent.
As already alluded, the immediate problem that then arises is the exact nature of the analogy to be
used when forming the comparison, i.e. which aspects of externally observable human behaviour are
deemed relevant to intelligence, and which are not. For example, is an ability to converse in natural
language a requirement for human-like intelligence? Or an ability to draw pictures and diagrams? Or
an ability to compose plays, poetry, and music? The exact choice of analogy is highly subjective, and
consequently so is the definition of "human-like intelligence" that results from using this approach.
In order to explore intelligence systematically, we need a more objective definition than this.
3.2.6 PROBLEM-SOLVING
If we’re engineers (meaning that we build stuff) and all we really care about is utility (i.e. that the
systems that we build are practically useful in some way) then we might choose to equate intelligence
with problem solving (i.e. the ability to solve problems). From this perspective, whenever we’re
thinking, we’re solving some problem (internally, in our minds), and a mind is "something that thinks"
or "something that solves problems". Whenever an intelligent system (i.e. a mind) solves a problem,
some utility (real-world value) is generated. This definition seems to work reasonably well. For our
present purposes, therefore, let’s define intelligence as problem solving, and see where it leads us4.
3 without further qualification, this definition is highly subjective — different observers may make conflicting judgements
4 this definition, while more objective, is deliberately broad (even a simple pocket calculator will qualify as intelligent)
Copyright 2023-2024 BigMother.AI CIC
8

THE BIGMOTHER MANIFESTO (PART 1)
3.2.7 ARTIFICIAL INTELLIGENCE (AI), AND PASSIVE PROBLEM-SOLVERS
Given this definition, an Artificial Intelligence (or AI) is a purposefully-engineered system that
solves problems. For our purposes, we will make a distinction between (purely cognitive) passive
problem-solvers5 and (agentic) active problem-solvers. A passive problem-solver takes as input
some kind of problem statement, and generates a problem solution as output. As a very simple
example, the problem statement might be "what is 1 + 1?", and the problem solution might be "2".
In contrast, an active problem-solver (or agent) takes as input a continuous stream of percepts,
and performs a continuous stream of actions as output, all in pursuance of a fixed final goal6.
3.2.8 PROBLEM STATEMENTS
For our purposes, the ideal problem statement has three parts (expressed as a triple ⟨𝑃, 𝑄, 𝑅⟩):
• 𝑃: a property over all possible things 𝑇
• 𝑄: an optional ordering over all possible things 𝑇having property 𝑃
• 𝑅: a constraint on the use of physical resources (always including a finite time limit).
Simply stated:
• 𝑃tells us which things 𝑇are valid solutions (there may be 0, 1, or many) and which are not
• if more that one valid solution exists, 𝑄tells us which valid solutions are better than others7
• 𝑅tells us how much time, energy, compute, money etc we can use in the search for a solution8.
A triple ⟨𝑃, 𝑄, 𝑅⟩should be interpreted by a passive problem-solver as follows: "using no more
than physical resources 𝑅, strive to find some 𝑄-maximal thing 𝑥having property 𝑃"; e.g.9:
• using no more than 60 s of time or 1 MJ of energy, strive to find some thing 𝑥such that 𝑥is a
natural number and 𝑥= 1 + 1 — in this case, there is only one solution (i.e. 𝑥= 2)
• using no more than 60 s of time or 1 MJ of energy, strive to find some 𝑄-maximal thing 𝑥such
that 𝑥is an integer and 𝑥2 = 4, where 𝑄favours positive solutions — in this case, there are
two possible solutions (𝑥= −2 and 𝑥= 2); however, because the ordering 𝑄favours positive
solutions, 𝑥= 2 is considered to be a "better" (i.e. higher quality) solution than 𝑥= −2.
In principle, problems ⟨𝑃, 𝑄, 𝑅⟩may be arbitrarily complex, i.e. as complex as the real world.
5 sometimes called oracle AI [LessWrong (2011)] or tool AI [LessWrong (2012)]
6 active problem-solvers are described in more detail in Section 3.2.29
7 if 𝑄is not specified, then any valid solution is considered to be as good as any other
8 in particular, in many real-world contexts, only timely solutions will have significant utility [see e.g. Newell (1990)]
9 at this point we are more concerned with intuitive concepts than strictly formal definitions, and so, for simplicity, problems
⟨𝑃, 𝑄, 𝑅⟩are described in English; in later sections, ⟨𝑃, 𝑄, 𝑅⟩may be expressed in a more formal notation
Copyright 2023-2024 BigMother.AI CIC
9

TURNER
3.2.9 PROBLEM SOLUTIONS
Given an arbitrary problem ⟨𝑃, 𝑄, 𝑅⟩, a passive problem-solver:
• will strive to find the best valid solution that it can, with no guarantees beyond that
• may deliver10 any of the following:
– the best possible (i.e. maximum) solution, according to the specified total ordering
– a best possible (i.e. maximal) solution, according to the specified partial ordering
– a satisfactorily good solution, according to the specified ordering, albeit not the best
– a disappointingly poor solution, according to the specified ordering, albeit not the worst
– a worst possible (i.e. minimal) solution, according to the specified partial ordering
– the worst possible (i.e. minimum) solution, according to the specified total ordering
– no solution at all (e.g. if the problem has no solution, or if the AI runs out of resources).
When dealing with arbitrary problems of the form ⟨𝑃, 𝑄, 𝑅⟩, this is the best we can hope for11.
Some passive problem-solvers will deliver better (i.e. higher-quality) solutions than others.
3.2.10 SOLUTION QUALITY
If we set the finite time limit within which problem solutions must be found to some fixed value,
but otherwise allow a problem-solver unbounded physical resources 𝑅, then we can visualise a
problem-solver’s capabilities on a simple 2D graph showing (1) the specific problem defined by 𝑃
and 𝑄(on the x-axis), and (2) the quality of the delivered solution according to ordering 𝑄(on the
y-axis). We can then plot different passive problem-solvers on the same 2D graph, for comparison12:
(For the avoidance of doubt, the following graphs (depicted in Sections 3.2.11 to 3.2.17) pertain
specifically to passive problem-solvers (as defined in Section 3.2.7), assuming a fixed time limit.)
10 for our present purposes, we assume that any given passive problem-solver is deterministic (i.e. given the same problem
statement ⟨𝑃, 𝑄, 𝑅⟩it always delivers the same result); in reality, many passive problem-solvers will be nondeterministic
11 note that if we were to give an arbitrary problem to a diligent human, the possible outcomes would be exactly the same
12 such diagrams are intended for illustrative purposes only, and should not be interpreted as any kind of formal definition!
Copyright 2023-2024 BigMother.AI CIC
10

THE BIGMOTHER MANIFESTO (PART 1)
3.2.11 NARROW AI
A narrow AI delivers valid solutions across a narrow range of problems:
Here, AI A is a narrow AI (according to our definition).
Note that we do not require a narrow AI’s delivered solutions to exceed any minimum level of
quality, beyond being valid solutions according to the specified problem statement. Thus this is a
very broad definition, and some very weak problem-solvers will nevertheless qualify as narrow AI.
3.2.12 ARTIFICIAL GENERAL INTELLIGENCE (AGI)
A general AI (GAI, usually styled AGI), delivers valid solutions across a wide range of problems:
Here, AGI B is an AGI (according to our definition).
Again, we do not require an AGI’s delivered solutions to exceed any minimum level of quality.
We also do not attempt to define the threshold between narrow AI and AGI, other than to say that
a narrow AI delivers valid solutions across a single problem domain, whereas an AGI delivers valid
solutions across multiple problem domains. It will usually be intuitively clear what a "problem
domain" is. Nevertheless, the distinction between narrow AI and AGI remains somewhat subjective.
Copyright 2023-2024 BigMother.AI CIC
11

TURNER
3.2.13 AGI COMPETENCE
Some AGIs might be particularly adept in some problem areas, but not in others:
Here, AGI C and AGI D are good at different things.
3.2.14 THE ALPHA AGI
It’s possible for one AGI — the alpha — to outperform all its peers across all possible problems:
Here, AGI E outperforms both AGI C and AGI D at all things. (Note that, given any two AGIs C
and D, a corresponding alpha AGI E can always be constructed by simply invoking, for any given
problem ⟨𝑃, 𝑄, 𝑅⟩, both AGIs C and D, and returning the better of the two results according to 𝑄.)
The concept of the alpha AGI will become extremely important later on.
Copyright 2023-2024 BigMother.AI CIC
12

THE BIGMOTHER MANIFESTO (PART 1)
3.2.15 SUPERINTELLIGENT NARROW AI
Things get interesting when we compare AI and baseline human13 (BH) performance:
Here, the median BH outperforms narrow AI F, narrow AI G outperforms the median BH, and
narrow AI H outperforms all BHs (i.e. the entire population of BHs). In other words, narrow AI H
(e.g. a modern pocket calculator) is superintelligent across the narrow range of problems in question.
3.2.16 SUPERINTELLIGENT AGI
We can also compare AGI performance against BH performance:
There are six primary possibilities. The median BH outperforms AGI J at all things, AGI K
outperforms the median BH at some things14, AGI L outperforms the median BH at all things15,
AGI M outperforms all BHs at some things14 and the median BH at as least as many things, AGI N
outperforms all BHs at some things14 and the median BH at all things, and AGI O outperforms all
BHs at all things. In other words, AGI O is superintelligent across all possible problems16.
13 by "baseline human" we mean a human unassisted by any other system that would itself qualify as an AI as defined above
14 note: across multiple problem domains — exceeding the threshold for a single problem domain is not deemed sufficient!
15 broadly speaking, AGI L seems to be what most AI researchers mean by human-level AGI [e.g. Morris et al. (2023)]
16 note that a sufficiently performant AGI M or AGI N might be judged to be near-superintelligent
Copyright 2023-2024 BigMother.AI CIC
13

TURNER
3.2.17 MAXIMALLY-SUPERINTELLIGENT AGI
Of course, there’s no need to stop at superintelligence. It might be possible to go way beyond AGI O:
Intuitively, a maximally-superintelligent AGI is the most intelligent superintelligent AGI that
it’s possible to build. Merely outperforming all baseline humans at everything is a lower bound.
Copyright 2023-2024 BigMother.AI CIC
14

THE BIGMOTHER MANIFESTO (PART 1)
3.2.18 EXAMPLE — GPS
The General Problem-Solving Program (GPS) [Newell, Shaw, and Simon (1958); Ernst and Newell
(1969)] was an early attempt at a general problem-solving system. Once a problem description had
been input to the system17, GPS would attempt to find a path to a solution via means-ends analysis18.
Although only ever intended as an exploratory research project, and heavily constrained by the
limits of 1960s computer systems, GPS could potentially be applied to a wide range of problems.
In the course of its roughly 10-year existence, GPS was applied to problems such as the following:
• Missionaries and Cannibals — Three missionaries and three cannibals wish to cross a river,
but their boat only holds two people; how can all six get across with no-one being eaten?
• Symbolic integration — Integrate ∫𝑡𝑒𝑡2 𝑑𝑡.
• Tower of Hanoi — Discover a sequence of moves that will transfer all the disks (all of different
sizes) from the first peg to the third peg, such that no disk is ever on top of a smaller disk.
• First-order logic theorem-proving — Use the resolution rule [Robinson (1965)] to prove:
∃𝑢∃𝑦∀𝑧((𝑃(𝑢, 𝑦) ⇒(𝑃(𝑦, 𝑧) ∧𝑃(𝑧, 𝑧))) ∧((𝑃(𝑢, 𝑦) ∧𝑄(𝑢, 𝑦)) ⇒(𝑄(𝑢, 𝑧) ∧𝑄(𝑧, 𝑧)))).
• Father and Sons — A father (weighing 200 pounds) and his sons (each weighing 100 pounds)
wish to cross a river in a boat whose capacity is 200 pounds; how can they all get across?
• Monkey — A room contains a monkey, a box, and some bananas hanging from the ceiling,
but they are too high for the monkey to reach; how can the monkey get the bananas?
• Three coins — Make three coins (initially T, H, T) all show the same, in just three moves.
• Parsing sentences — Correctly parse the sentence "Free variables cause confusion".
• Bridges of Königsberg — The river Pregel runs through the German town of Königsberg19;
in the river are two islands connected with the mainland and with each other via seven bridges;
is it possible to cross each of the seven bridges exactly once and return to the same point?20
• Water jug — Given a water tap, a drain, a five-gallon jug, and an eight-gallon jug (and no other
water-measuring devices), how can exactly two gallons of water be put into the five-gallon jug?
• Letter series completion — Complete the series "B C B D B E __ __".
GPS was able to successfully solve all of the above problems, except for the Bridges of Königsberg.
Is GPS intelligent? From the perspective of an external observer, GPS takes as input a problem
statement, and generates a problem solution as output. According to our earlier definitions, therefore,
GPS is a passive problem-solver demonstrating extrinsic intelligence.
Is GPS an AGI? GPS is not limited to a single problem domain, so GPS is an AGI.
Is it superintelligent? No — we would judge GPS to be an AGI K, far short of AGI O.
17 in terms of the objects pertinent to the problem in question and the operators that may be applied to them
18 effectively working backwards from the desired final state, via the available operators, to the initial state
19 now Kaliningrad in Russia
20 in 1736, the mathematician Leonhard Euler proved this to be impossible
Copyright 2023-2024 BigMother.AI CIC
15

TURNER
3.2.19 EXAMPLE — DALL·E
DALL·E [Ramesh et al. (2021)] generates an image in response to a text prompt21.
For example, the following images were generated by DALL·E:
Is DALL·E intelligent? From the perspective of an external observer, DALL·E takes as input a
problem statement, and generates a problem solution as output. According to our earlier definitions,
therefore, DALL·E is a passive problem-solver demonstrating extrinsic intelligence.
Is DALL·E an AGI? DALL·E is limited to a single problem domain, so DALL·E is not an AGI.
Is it superintelligent? No — we would judge DALL·E to be an AI G, far short of AI H.
21 Midjourney [Oppenlaender (2022)] is a similar text-to-image system
Copyright 2023-2024 BigMother.AI CIC
16

THE BIGMOTHER MANIFESTO (PART 1)
3.2.20 EXAMPLE — CHATGPT
ChatGPT22, 23 [Brown et al. (2020); Liu et al. (2023)] generates text in response to a text prompt.
Example — arithmetic:
Example — haikus:
22 specifically, the May 2024 version based on the GPT-4o Large Language Model (LLM) [Zhao et al. (2023)]
23 note that the Bard chatbot (powered by the Gemini 1.0 LLM released by Google DeepMind in December 2023) is broadly
comparable, as is the Claude 3 LLM released by Anthropic in March 2024 [see Romero (2024) for an informal analysis]
Copyright 2023-2024 BigMother.AI CIC
17

TURNER
Example — summarisation:
Is ChatGPT intelligent? From the perspective of an external observer, ChatGPT takes as input a
problem statement, and generates a problem solution as output. According to our earlier definitions,
therefore, ChatGPT is a passive problem-solver demonstrating extrinsic intelligence.
Is ChatGPT an AGI? ChatGPT is not limited to a single domain, so ChatGPT is an AGI.
Is it superintelligent? No — we would judge ChatGPT to be an AGI K24, far short of AGI O.
24 i.e. broadly the same as GPS in terms of solution quality, only much easier to use, and serving a wider range of problems;
note that four AI systems, 60 years apart, all stalling at AGI K, suggests that AGI K is a difficult barrier to overcome!
Copyright 2023-2024 BigMother.AI CIC
18

THE BIGMOTHER MANIFESTO (PART 1)
3.2.21 DON’T PANIC!
It is said that despite its many glaring (and occasionally fatal) inaccuracies, the Hitchhiker’s Guide
to the Galaxy itself has outsold the Encyclopedia Galactica because it is slightly cheaper, and
because it has the words ’DON’T PANIC’ in large, friendly letters on the cover. Adams (1978)
The next few Sections, 3.2.22 to 3.2.26, require a little mental gymnastics on the part of the
reader. Considering that this first paper in the BigMother series of papers aims to be as accessible to
non-technical professionals as possible, this is conceptually the most difficult part of Part 1 to read,
for which we apologise, but there doesn’t seem to be any way around it. Once one of our informal
reviewers, Professor Bob Kowalski, had pointed out a glaring inconsistency in an earlier version of
Part 1, we had no choice but to resolve it, and Sections 3.2.22 to 3.2.26 are the result. The paper,
we believe, has been significantly strengthened overall (thanks Bob!), with the minor downside that
some readers may struggle to read these particular Sections as much as we struggled to write them!
3.2.22 INTRINSIC INTELLIGENCE
Following our earlier definitions, we have determined that both DALL·E and ChatGPT are passive
problem-solvers exhibiting extrinsic intelligence, and we have also determined that ChatGPT is an
AGI, albeit neither a human-level nor a superintelligent AGI. But is extrinsic ("black box") intelligence
a sufficient measure of genuine intelligence? Or is whatever happens "inside the box" also relevant?
Imagine if we opened up DALL·E or ChatGPT and found nothing inside but a massive lookup
table going from problem statements to problem solutions. If this were the case, then the system’s
externally observable behaviour would be identical to what we have already judged to be extrinsically
intelligent, yet we would not judge such a system to be intrinsically intelligent25. Therefore:
• a system’s internal behaviour is highly relevant to whether or not it is intrinsically intelligent
• we cannot reliably infer a system’s internal behaviour just by observing its external behaviour26
• if we want to know what’s going on inside the box, we have to actually look inside the box27!
3.2.23 THE ROLE OF INFORMATION IN PROBLEM-SOLVING
If we look inside a passive problem-solver, we will see some kind of problem-solving mechanism
(e.g. software). But problem-solving is not just about algorithms [Wirth (1976); Kowalski (1979)].
If there’s one key takeaway to be gleaned from the classic problem-solving literature [e.g. Pólya
(1945); Pólya (1954a); Pólya (1954b); Newell, Shaw, and Simon (1958); Pólya (1962a); Pólya (1962b);
Ernst and Newell (1969); Newell and Simon (1972); Lakatos (1976); Newell (1990); Tsang (1993);
Dechter (2003)], it’s that the key to effective problem-solving is the effective use of information.
It’s impossible to overstate the importance of this observation when designing intelligent systems!
25 John Searle’s famous Chinese Room Argument [Searle (1980); Cole (2023)] makes essentially the same observation
26 therefore, just because DALL·E and ChatGPT possess extrinsic intelligence, it’s not necessarily the case that they possess
significant intrinsic intelligence — any such assertion based solely on externally observable behaviour (such as that
described above) is merely one possible explanation of that externally observable behaviour (i.e. an abductive hypothesis)
27 e.g. we can use mechanistic interpretability [Bereska and Gavves (2024)] to look inside an LLM [Templeton et al. (2024)]
Copyright 2023-2024 BigMother.AI CIC
19

TURNER
When we look (closely) inside a passive problem-solver, we will necessarily see two things:
1. the underlying problem-solving mechanism per se (which we shall call the inventor28)
2. the information (data, knowledge, belief 29) that drives the inventor towards solutions:
For example, when searching the space of all possible values of 𝑥trying to find a solution to the
equation 𝑥= 1 + 1, the inventor may be guided by its knowledge of the number 1 and the operator +.
The information that drives (or otherwise guides) the inventor towards solutions must be:
• acquired somehow30
• represented (essentially, "written down") somehow31
• processed (by the inventor) somehow32.
3.2.24 MAXIMALLY-INTRINSIC PASSIVE PROBLEM-SOLVERS
Earlier, in Section 3.2.22, we determined that both DALL·E and ChatGPT are passive problem-solvers
exhibiting extrinsic intelligence. We also imagined the possibility of opening up either DALL·E or
ChatGPT and finding nothing inside but a massive lookup table going from problem statements to
problem solutions. In such a configuration, the inventor (the underlying problem-solving mechanism
per se) is merely an extremely simple algorithm that uses the problem statement as a key to index
into the massive lookup table, delivering the contents of the table row so accessed as the problem
solution, and the massive lookup table is the information that drives the inventor towards solutions.
As already alluded, even though such a system exhibits extrinsic intelligence, it lacks intrinsic
intelligence. The real intelligence resides in the human AI designer that created the lookup table!
In constructing the lookup table, the human AI designer would need to:
1. consider every possible problem statement (i.e. every possible problem instance)
2. attempt to solve each problem instance themselves (e.g. by hand-executing a suitable algorithm)
3. add the results to the lookup table.
28 from the Latin invenire: to find, discover, invent, devise
29 for our purposes, we shall use these terms interchangeably — philosophers should have a nap and a banana at this point! :-)
30 working out where each piece of problem-solving information comes from is a key part of the AI design process
31 for a would-be superintelligent AGI, the underlying knowledge representation language (its language of thought
[Kowalski (2011a); Rescorla (2019)]) must be as general as possible — otherwise there may be concepts that a human
being can express (and thereby reason about) but that the AGI cannot, immediately rendering superintelligence impossible!
32 e.g. via an arbitrarily complex combination of strong probabilistic induction, deduction, and abduction (3.2.27 and 3.2.28)
Copyright 2023-2024 BigMother.AI CIC
20

THE BIGMOTHER MANIFESTO (PART 1)
In other words, the AI designer is operating as a "higher-level" (or meta-level) problem-solver,
and the passive problem-solver being designed is the "lower-level" (or object-level) problem-solver:
Figure 1: A meta-level passive problem-solver designing an object-level passive problem-solver
The components of this dual-problem-solver system are:
meta-statement
the meta-level problem statement (usually in some natural language, e.g. English)
meta-info
other information that the (human) meta-level problem-solver has in its possession
meta-inventor
the meta-level passive problem-solving mechanism (a human mind)
meta-solution
the meta-level problem solution (i.e. the solution to the meta-statement)
object-statement
the object-level problem statement (expressed in a specific language or format)
object-info
other information that the object-level passive problem-solver has in its possession
object-inventor
the object-level passive problem-solving mechanism (e.g. some AI algorithm)
object-solution
the object-level problem solution (i.e. the solution to the object-statement)
We define pre-calculated information as any information (e.g. a lookup table, a division
algorithm, or a CPU logic circuit) derived (i.e. calculated) by the meta-level problem-solver
from either (a) the meta-statement, or (b) any object-statement, such that is either (i) contained
in object-info, or (ii) incorporated into object-inventor. An object-level passive problem-solver is
maximally-intrinsically-intelligent33 if object-inventor is capable of calculating any object-solution
that may be calculated using pre-calculated information without using any pre-calculated information.
The idea is that a maximally-intrinsic object-level problem-solver does all its own thinking, and
is not cognitively reliant on the meta-level problem-solver. Hopefully an example will help to clarify.
33 usually shortened to maximally-intrinsic for convenience
Copyright 2023-2024 BigMother.AI CIC
21

TURNER
In the "massive lookup table" example:
• within the meta-level problem-solver (the AI designer, modelled as a passive problem-solver):
1. meta-info contains information pertaining to mathematics, computer science, AI design,
etc, such as how to construct a passive problem-solver structured as a lookup table
2. meta-inventor initialises the lookup table such that every row contains "no solution found"
3. from meta-statement conjoined with meta-info, meta-inventor derives an algorithm ,
which strives to find a valid object-solution for any given instance of object-statement
4. meta-inventor simulates algorithm (thereby yielding a valid object-solution, assuming
that one can be found for the problem instance in question) for every object-statement (i.e.
problem instance) that it is able to derive from meta-statement conjoined with meta-info,
inserting any object-solution so found into the corresponding row of the lookup table;
note that every object-solution so found has been calculated (not by the object-level
passive problem-solver, but by the meta-level problem-solver) from the corresponding
object-statement, and therefore qualifies as pre-calculated information
5. from meta-statement conjoined with meta-info, meta-inventor derives object-inventor (a
simple algorithm to index into a lookup table, such as the one just constructed)
6. the meta-solution is (an implementable design for) the object-level problem-solver
(comprising (a) the completed lookup table as object-info, and (b) object-inventor)
• within the object-level problem-solver (an operational implementation of the meta-solution):
1. object-info contains the lookup table (i.e. a collection of pre-calculated object-solutions)
2. object-inventor uses object-statement to index into the lookup table, yielding (without
any further calculation) either "no solution found" or the corresponding object-solution
• thus:
(a) the lookup table contained in object-info contains pre-calculated information
(b) object-inventor calculates object-solutions using that pre-calculated information
(c) ... but is unable to do so without using any pre-calculated information
(d) accordingly, the object-level passive problem-solver in question is not maximally intrinsic.
Note that it would also be possible for the meta-level problem-solver to achieve equivalent effect
not by generating a lookup table for explicit incorporation into object-info but by instead structuring
the object-inventor (algorithm) as a big "switch" statement, effectively incorporating the lookup table
into the object-inventor. This would of course also qualify as pre-calculated derived information.
As a more complex example, if, according to meta-statement, the intended purpose of object-level
problem-solver is to perform division, and meta-inventor derives a suitable division algorithm 
for incorporation into object-inventor which subsequently uses to perform division, but which is
unable to perform division without using , then is again not maximally-intrinsic.
Copyright 2023-2024 BigMother.AI CIC
22

THE BIGMOTHER MANIFESTO (PART 1)
3.2.25 MAXIMALLY-INTRINSIC SUPERINTELLIGENT AGI
A maximally-intrinsic superintelligent passive problem-solver (AGI O/Z) must necessarily possess:
1. an inventor that (given the same information) is better than any human at solving any problem
2. a deep understanding of all human knowledge34
3. sufficient physical resources (e.g. compute) to be able to deliver timely problem solutions35.
We shall call these qualities super-inventive, super-knowledgeable, and super-resourced.
3.2.26 THE PATH TO SUPERINTELLIGENT AGI
In general, intelligence36 has three scalable dimensions37: inventiveness + knowledge + resources.
Accordingly, if a human AI designer (meta-level passive problem-solver) designs a non-
maximally-intrinsic object-level passive problem-solver −then that means that some sub-
components of −(and thereby the quality of object-solutions delivered by −) are partly reliant
on pre-calculated information generated by the inventiveness + knowledge + resources combination
of ’s meta-inventor + ’s meta-information + "one human brain’s worth of compute".
Conversely, if instead designs a maximally-intrinsic variant of −called + then (on
which the quality of object-solutions delivered by + partly relies) may also be derived by +’s
object-inventor + +’s object-information + whatever physical resources + has available to it.
Comparing the three passive problem-solvers in question (, −, and +):
• ’s meta-inventor, −’s object-inventor, and +’s object-inventor are (assumed to be) fixed
• ’s meta-information scales relatively slowly with time (reading literature, taking courses, etc)
• both −’s and +’s object-information can potentially scale much more rapidly
• ’s physical (compute) resources are fixed (at ~3 pounds of grey matter and ~20 W of power)
• −’s and +’s physical resources can potentially be scaled by several orders of magnitude38.
Thus the potential scope for improving the performance (object-solution quality) of either −
or + by scaling ’s meta-information or physical resources is limited by the rate at which can
gain new knowledge. There is greater scope for improving the performance of −by scaling −’s
object-information or physical resources, but, even if −were near-superintelligent, this would
not improve the quality of the pre-calculated information on which the quality of object-solutions
delivered by −partly relies. Thus the potential scope for improving performance via scaling is
greatest for near-superintelligent +, because in this case the quality of might also be improved.
Therefore, given equivalent near-superintelligent −and +, + has the greater chance of
being pushed across the line from sub-superintelligence to superintelligence via scaling.
34 a deep understanding of any subject provides much more problem-solving information than a shallow understanding
35 think about it: a maximally-intrinsic passive problem-solver does all its own thinking (without any cognitive assistance
from its designer), and so it must attempt to solve any problem by itself; in order to be superintelligent, it must be able to
solve (i.e. deliver a timely solution to) any problem better than any human, and, in order to do that, it must possess (a) any
problem-solving information that any human might have (i.e. all human knowledge), and (b) sufficient physical resources
36 as we have defined it, i.e. problem-solving
37 note that the actual relationship between these qualities is more likely to be multiplicative than additive!
38 a modern supercomputer weighs ~600,000 pounds and consumes 20-60 MW of power [Dongarra and Geist (2022)]
Copyright 2023-2024 BigMother.AI CIC
23

TURNER
3.2.27 INDUCTION, DEDUCTION, ABDUCTION, AND WEAK VS STRONG REASONING
It seems to be the case that, at the highest levels of abstraction (at the level of mind rather than brain),
robust critical thought and problem-solving within humans involves three modes of reasoning:
• induction39, 40 — the discovery of patterns — Example 1: after seeing a number of examples
of swans (all of which are white), you formulate the general concept of "swan" (and, based on
your experience, all swans are white); Example 2: after seeing a number of examples of cats (all
of which have tails), you formulate the general concept of "cat" (and, based on your experience,
all cats have tails); Example 3: after learning of the existence of black swans (which are not
white) and Manx cats (which don’t have tails), you realise that not all swans are white, and
not all cats have tails, and you formulate the abstract concept of "exception"; Example 4: after
seeing a number of dead swans, cats, and men, you formulate the abstract concept of "mortal"
• deduction41 — the derivation of necessary conclusions — Example: Socrates is a man, and
all men are mortal, therefore [via the modus ponens rule of inference] Socrates is mortal
• abduction42 — the formulation of possible explanations — Example: Socrates is mortal,
therefore Socrates could be a swan, Socrates could be a cat, and Socrates could be a man43.
Thus induction, deduction, and abduction (IDA) may be viewed as cognitive primitives on
top of which higher-level reasoning such as generic problem-solving might be constructed. Specific
formulations of IDA may be either strong (well-founded) or weak (prone to some kind of error)44:
• weak induction, e.g. either seeing patterns that aren’t in the data, or not seeing patterns that are
• weak deduction, e.g. concluding something non sequitur that isn’t necessarily a consequence
• weak abduction, e.g. elevating a plausible hypothesis to an unqualified belief, without evidence.
We conjecture the following:
1. IDA, suitably integrated45, are necessary for ≥human-level AGI (~AGI L)
2. IDA, suitably integrated, are sufficient for ≥human-level AGI (~AGI L)
3. any AGI that is able to perform strong IDA will have a performance advantage over an AGI
that is limited to relatively weak IDA (as the weaker forms will introduce errors of reasoning)
4. superintelligent AGI (AGI O/Z) will require strong IDA.
39 Holland et al. (1986); Mortimer (1988); Flach and Hadjiantonis (2000); Johnson (2016); Henderson (2020); Bartha (2022)
40 note that induction and analogy are closely related, a subject to which we shall return in Section ??
41 Shoenfield (1967); Barwise (1977); Mendelson (1987); Halbach (2010)
42 Flach and Hadjiantonis (2000); Walton (2014); Douven (2021); Douven (2022)
43 these are plausible hypotheses only; elevation to unqualified belief (e.g. Socrates is a cat) requires additional evidence
44 humans, of course, are prone to a wide range of errors of reasoning, as we shall discuss in Section ??
45 in order to be able to cooperate effectively on the stepwise solution of any given problem, these (or any other) cognitive
primitives must be able to share between them information pertaining to the problem in question, which means that they
must share (and be able to reason about information represented in) a common knowledge representation language
(KRL); furthermore, the more precise the KRL, the more precise the reasoning that may be performed in it — this was
clearly understood by Gottfried Leibniz in 1676 when he proposed his characteristica universalis [Whipple (2022)]
Copyright 2023-2024 BigMother.AI CIC
24

THE BIGMOTHER MANIFESTO (PART 1)
3.2.28 UNCERTAINTY
As we shall see in Section ??, any belief pertaining to the (assumed) physical universe (including any
belief derived from its observation) is a guess. Accordingly, there is no such thing as certain belief
pertaining to the physical universe; all belief pertaining to the physical universe46 is uncertain.
This unfortunate but nevertheless unavoidable reality makes reasoning about the deepest subtleties
of the physical universe distinctly counter-intuitive. In the last ~370 years in particular, a number of
mechanisms have been proposed for reasoning about uncertainty mathematically, including:
• Dempster-Shafer belief functions47
• possibility measures48
• probability measures49
• ranking functions48
• relative likelihood48
• plausibility measures48 (note that plausibility measures generalise of all of the above!)
Of these, probability theory is by far the most widely used in statistics, science, and engineering:
• a probability is a number between 0 and 1 expressing (in the modern Bayesian interpretation)
"strength of belief", i.e. the degree to which an assertion is supported by the available evidence
• probability theory has been axiomatised, notably by Kolmogorov (1933); such axiomatisations
describe the rules via which probabilities may be manipulated (i.e. reasoned about).
The cognitive primitives induction, deduction, and abduction may be applied probabilistically:
• induction:
– rather than a set of observations definitely containing pattern X, if the data are noisy then
one might conclude that they might contain pattern X (with a certain probability)
– similarly, having identified a specific pattern X as possibly being contained in previous
observations, a new observation might match that pattern (with a certain probability)
• deduction – rather than concluding (for example) that "it will definitely rain today", by applying
the probability axioms one might conclude that "there’s a 70% chance that it will rain today"
• abduction – similarly, on learning that "Socrates is mortal", one might conclude (based on
past experience) that "there’s a 90% chance that Socrates is a man, and a 10% chance he is not".
Note that, in practice, probabilities themselves are usually estimates, i.e. guesses.
46 such as whether or not the physical universe exists, whether or not you exist, who gave birth to you, or what time it is
47 Shafer (1976); Yager et al. (2004); Halpern (2017)
48 Halpern (2017)
49 Kolmogorov (1933); Jeffreys (1939); Cox (1946); Pearl (1988) Bernardo and Smith (2000); Jaynes (2003); Jeffrey (2004);
Benaroya and Han (2005); Rosenthal (2006); Stone (2013); Venkatesh (2013); Gelman et al. (2014); Halpern (2017)
Copyright 2023-2024 BigMother.AI CIC
25

TURNER
3.2.29 ACTIVE PROBLEM-SOLVERS
So far we have been discussing passive problem-solvers:
An active problem-solver (or agent) takes as input a continuous stream of percepts (its percept
history), and performs a continuous stream of actions (its action history) as output, all in pursuance
of a fixed final goal50 defining the condition of the physical universe that the agent strives to maintain:
where:
• a percept is a digitised input received from a sensor (e.g. a video camera, or microphone)
• an action is a digitised output sent to an effector (e.g. an actuator, display screen, or speaker)
• the final goal might be e.g.:
– "Do nothing"
– "Do something"
– "Maximise the total number of paperclips that exist"
– "Maximise shareholder value for XYZ Corp"
– "Maximise GDP for country X"
– "Maximise human happiness".
We shall return to the (surprisingly difficult) problem of formulating a final goal in Section ??.
Note that:
• any passive problem-solver can be turned into an active problem-solver via a simple wrapper
• active problem-solvers are necessarily embodied (physically connected to the physical universe)
via their sensors and effectors [see e.g. Paolo, Gonzalez-Billandon, and Kégl (2024)]
• whereas a passive problem-solver must attempt to solve a specified problem ⟨𝑃, 𝑄, 𝑅⟩using
just the information that it already has and the physical resources 𝑅specified by the problem
statement, a non-trivial active problem-solver may acquire additional information and
additional resources through deliberate (i.e. planned51) interaction with the physical universe.
50 sometimes also called a terminal goal
51 see Section 3.2.30
Copyright 2023-2024 BigMother.AI CIC
26

THE BIGMOTHER MANIFESTO (PART 1)
3.2.30 CONTINUOUS PLANNING
In preparing for battle I have always found that plans are useless, but that planning is indispensable.
— Dwight D. Eisenhower [Nixon (1962)]
We define a planning mechanism to be an algorithm which, when executed, generates a plan,
and a plan to be an algorithm which, when executed, generates a sequence of actions.
Any active problem-solver must necessarily generate actions somehow, even if only one at a
time. Accordingly, if we look inside an active problem-solver, we will necessarily see some kind of
planning mechanism [Allen, Hendler, and Tate (1990); Ghallab, Nau, and Traverso (2016)].
The planning mechanism strives to solve (and continuously re-solve) the following problem:
"Given the current state of the (assumed) physical universe (as indicated by the percept history),
formulate a plan which, when executed, will generate a sequence of actions (to be appended to
the action history) whose most likely causal effect52 is to maximise progress towards (and ideally
achieve) the final goal". Append to this an English description of a specific final goal, and the result is
an English specification (meta-level problem statement) for the body of the planning loop that might
be given to a human programmer, or to a human AI designer (meta-level passive problem-solver).
Accordingly, following Section 3.2.24, the body of the planning loop is effectively an object-level
passive problem-solver to be designed by a meta-level passive problem-solver (human AI designer)53.
There are many ways in which the AI designer might choose to solve this problem. For example,
it might be possible, in some cases, to reformulate the final goal as a finite set of production rules54
to be blindly followed in response to incoming percepts, thereby generating a sequence of actions
"whose most likely causal effect is to make progress towards (and ideally achieve) the final goal".
In other words, rather than the final goal being an explicitly-represented component of the planning
mechanism, the desired behaviour is instead an emergent property of the set of production rules
[Kowalski (2011b)]. Should the production rules include any pre-calculated information derived from
either the meta-level problem-statement or any object-statement then the body of such a planning
loop might not be maximally-intrinsic. As described in Section 3.2.26, maximally-intrinsic solutions
facilitate the path to superintelligence, and (if that is the objective) are therefore greatly preferable.
We will say that an active problem-solver is maximally-intrinsic if every passive problem-solver
that it incorporates (for example, as the body of the planning loop) is maximally-intrinsic.
52 see Section 3.2.37
53 we shall henceforth use the designation "AGI 𝑋" to refer to either (a) a passive problem-solver at level AGI 𝑋, or (b) an
active problem-solver incorporating a passive problem-solver at level AGI 𝑋(e.g. as the body of the planning loop)
54 a production rule has the general form: if this happens then do this
Copyright 2023-2024 BigMother.AI CIC
27

TURNER
3.2.31 CONTINUOUS LEARNING
If we look inside an active problem-solver, we might also see some kind of learning mechanism:
A learning mechanism strives to solve (and continuously re-solve) the following problem: "Given
the current percept history, construct an internal model55 capturing (as accurately as possible) the
structure56 of the (assumed) physical universe, including all of its nuanced complexity57". Due
to the importance of information to problem-solving, an active problem-solver that incorporates a
continuous learning mechanism will be able to apply its accumulated learned knowledge to future
problems (potentially finding better solutions as a result), whereas an active problem-solver that does
not incorporate such a learning mechanism will be unable to grow with experience in the same way58.
Again following Section 3.2.24, the body of the learning loop is effectively an object-level passive
problem-solver to be designed by a meta-level passive problem-solver (human AI designer).
As before, there are many ways in which the AI designer might choose to solve this problem
[Michalski, Carbonell, and Mitchell (1983); Mitchell (1997); Jain (1999); MacKay (2003); Bishop
(2006); Rasmussen and Williams (2006); Murphy (2012); Shalev-Shwartz and Ben-David (2014);
Goodfellow, Bengio, and Courville (2016); Sutton and Barto (2018); Faul (2020); Schulte (2023)].
Some solutions will be maximally-intrinsic, whereas others will not. Maximally-intrinsic solutions
facilitate the path to superintelligence, and (if that is the objective) are therefore greatly preferable.
Clearly, an active problem-solver will only qualify as maximally-intrinsic if the body of its
planning loop and the body of its learning loop (assuming that it has one) are maximally-intrinsic.
55 see Section 3.2.34
56 see Section 3.2.33
57 including, but not limited to [Adler (1974)]: Matter and Energy (Atoms; Energy, Radiation, and States of Matter; the
Universe), The Earth (Earth’s Properties, Structure, Composition; Earth’s Envelope; Surface Features; Earth’s History),
Life (The Nature and Diversity of Life; The Molecular Basis of Life; The Structures and Functions of Organisms; The
Behaviour of Organisms; The Biosphere), Human Life (The Development of Human Life; The Human Body: Health and
Disease; Human Behaviour and Experience), Society (Social Groups: Ethnic Groups and Cultures (Peoples and Cultures
of the World; The Development of Human Culture; Major Cultural Components and Institutions of Societies; Language
and Communication); Social Organisation and Social Change; The Production, Distribution, and Utilization of Weath;
Politics and Government; Law; Education), Art (Art in General; Particular Arts), Technology (Nature and Development
of Technology; Elements of Technology; Fields of Technology), Religion (Religion in General; Particular Religions),
History (Ancient Southwest Asia, North Africa, and Europe; Medieval Southwest Asia, North Africa, and Europe; East,
Central, South, and Southeast Asia; Sub-Saharan Africa to 1885; Pre-Columbian America; The Modern World to 1920; The
World Since 1920), and Branches of Knowledge (Logic; Mathematics; Science; History and the Humanities; Philosophy;
Preservation of Knowledge) — note that natural languages such as English are part of the structure of the universe; a
learning mechanism should be able to assimilate all natural languages along with the rest of the structure of the universe
58 this quality of adaptability is often cited in the literature as being a core principle of intelligence [see e.g. Wang (2013)]
Copyright 2023-2024 BigMother.AI CIC
28

THE BIGMOTHER MANIFESTO (PART 1)
3.2.32 THE SENSE-EFFECT LOOP
We now have enough pieces of the active-problem-solver jigsaw puzzle to see how they fit together:
Figure 2: The active problem-solver sense-effect loop (a.k.a. perception-action cycle)
We shall unpack Figure 2 over Sections 3.2.33 to 3.2.38, starting with the physical universe.
3.2.33 THE STRUCTURE OF THE PHYSICAL UNIVERSE
Let’s assume for a moment that the physical universe actually exists. What do we know about it...?
We know that it’s not completely homogeneous. If it were, wherever we looked, with whatever
sensor, at any time, we would see the same thing. But that’s not what we see. We also know that it’s
not completely random. If it were, wherever we looked, with whatever sensor, at any time, all we
would see is random data (like an old TV screen with no signal). But that’s not what we see either.
What we see instead is something in between complete homogeneity and complete randomness.
In other words, the universe has structure, and that structure is noisy, but not completely noisy.
And if it’s not completely noisy then that means we can predict things about it (perhaps not perfectly,
but well enough to be useful relative to our goals). And, for that, we need an internal model.
Copyright 2023-2024 BigMother.AI CIC
29

TURNER
3.2.34 INTERNAL MODELS OF THE PHYSICAL UNIVERSE, AND DEPTH OF UNDERSTANDING
Following Figure 2, the physical universe causally affects (Section 3.2.37) the sensors (Section
3.2.29) of an active problem-solver, or agent, 𝐺, thereby generating a continuous stream of percepts.
These percepts then become the primary input into 𝐺’s continuous learning mechanism (Section
3.2.31), the body of which comprises a passive problem-solver (Section 3.2.7) utilising various
cognitive primitives such as strong probabilistic induction, deduction, and abduction (Sections 3.2.27
and 3.2.28). As its primary output, the continuous learning mechanism maintains an internal model
capturing (as accurately as possible) the structure (Section 3.2.33) of the actual physical universe59:
Figure 3: Shallow internal model (and therefore understanding) of the physical universe
Let’s imagine that Figure 3 is an internal model of the physical universe, constructed by agent 𝐺
after having carefully observed the actual physical universe for one year, where each circle represents
some feature (corresponding to an identifiable part of the structure of the physical universe) and
each line represents a relationship between two or more features. Let’s further imagine that Figure 3
captures only a fragment of the structure of the actual physical universe — in other words, there are
lots of missing features and missing relationships that are not (yet) captured by the model. In such
a scenario, we would say that agent 𝐺has only a shallow understanding of the physical universe.
Let’s imagine that agent 𝐺now observes the physical universe for a further ten years, and, given
these additional percepts, 𝐺’s learning mechanism is able to construct a more detailed internal model:
Figure 4: Intermediate internal model (and therefore understanding) of the physical universe
59 note that if agent 𝐺lacks a continuous learning mechanism then its internal model of the physical universe will be fixed
Copyright 2023-2024 BigMother.AI CIC
30

THE BIGMOTHER MANIFESTO (PART 1)
Finally, let’s imagine that agent 𝐺observes the physical universe for a further hundred years,
thereby allowing 𝐺’s learning mechanism to construct a much more detailed internal model:
Figure 5: Deep internal model (and therefore understanding) of the physical universe
In such a scenario, we would say that agent 𝐺has a deep understanding of the physical universe.
As already alluded in Section 3.2.26, "a deep understanding of any subject [such as the physical
universe] provides much more problem-solving information than a shallow understanding".
3.2.35 INDEPENDENCE
It is instructive to consider the various relationships that might (or might not) exist between features
of the physical universe, and which might (or might not) be captured by an internal model of it.
The simplest case is when no relationship exists at all. Thus two features 𝑥and 𝑦are said to be
independent when information about one gives no information about the other, for example:
Figure 6: Independent features 𝑥and 𝑦(shown on the 𝑥and 𝑦axes, respectively)
Copyright 2023-2024 BigMother.AI CIC
31

TURNER
3.2.36 CODEPENDENCE
Alternatively, features 𝑥and 𝑦may be (simply) codependent, for example:
• association: if either feature changes, the other also changes60
• +ve correlation: if either feature changes, the other changes in the same direction
• –ve correlation: if either feature changes, the other changes in the opposite direction:
(a) Association
(b) Positive correlation
(c) Negative correlation
Figure 7: Examples of simple codependence
As with induction, deduction, and abduction, these relationships may be probabilistic (noisy):
• association: if either feature changes, the other tends to change
• +ve correlation: if either feature changes, the other tends to change in the same direction
• –ve correlation: if either feature changes, the other tends to change in the opposite direction:
(a) Noisy association
(b) Noisy positive correlation
(c) Noisy negative correlation
Figure 8: Examples of noisy codependence
Note that, if the relationship between two simply codependent features is known, then one may be
calculated (i.e. exactly predicted) from the other. Similarly, if the relationship between two noisily
codependent features is known, then one may be estimated (i.e. approximately predicted) from
the other. Accordingly, whenever codependent relationships exist between features of the physical
universe, and these relationships are captured by agent 𝐺’s internal model of the physical universe, 𝐺
will be able to use its internal model to make predictions about the actual physical universe.
60 but not necessarily in the same direction, and not necessarily in the opposite direction either!
Copyright 2023-2024 BigMother.AI CIC
32

THE BIGMOTHER MANIFESTO (PART 1)
3.2.37 CAUSALITY
Consider the following statement (which, for our present purposes, we shall assume to be true):
In the English seaside town of Brighton, ice cream sales are positively correlated with drownings.
As already alluded in Section 3.2.36, this means that, if the numerical relationship between "daily
ice cream sales" and "daily drownings" is approximately known, then, given one, we may confidently
estimate the other. But does this mean that ice cream sales cause drownings? Because, if that were
the case, restricting ice cream sales would lead to a decrease in the number of drowning deaths!
Clearly, ice cream sales do not cause drownings. A much more plausible explanation is that some
common factor, such as hot weather, causes both an increase in ice cream sales and an increase in
the number of people taking a swim (some roughly fixed proportion of whom will then drown).
This is an example of the well-known phenomenon that correlation does not imply causality61.
This has important implications for active problem-solving, because it means that, when constructing
a plan of actions, i.e. when trying to predict the causal effect of each candidate action, an agent 𝐺
cannot simply rely on any correlations that might be present in its internal model of the physical
universe. Instead, in order for 𝐺(when formulating a plan) to be able to predict the likely causal
effect of each candidate action, 𝐺’s internal model of the physical universe must include relationships
between features that capture the cause-and-effect behaviour of the actual physical universe.
3.2.38 FROM INTERNAL MODEL AND FINAL GOAL TO ACTIONS AND CAUSAL EFFECTS
Returning to Figure 2, the internal model maintained by 𝐺’s continuous learning mechanism
(Section 3.2.31) then becomes — along with 𝐺’s final goal (Section 3.2.29) — the primary input
into 𝐺’s continuous planning mechanism (Section 3.2.30), the body of which (exactly analogously
to 𝐺’s continuous learning mechanism) comprises a passive problem-solver (Section 3.2.7) utilising
various cognitive primitives such as strong probabilistic induction, deduction, and abduction (Sections
3.2.27 and 3.2.28). As its primary output, and guided by the cause-and-effect relationships in its
internal model of the physical universe, the continuous planning mechanism formulates a plan whose
execution generates a continuous stream of actions, which, via 𝐺’s effectors (Section 3.2.29) then
causally affect (Section 3.2.37) the physical universe. If the causal effects of the actions generated
by the plan serve to maximise progress towards (and ideally achieve) the final goal, then the plan
formulated by the continuous planning mechanism may be said to be causally effective.
Finally, in Figure 2:
• note how actions are also looped back as percepts; this not only maintains an automatic record
(in 𝐺’s percept history) of every action sent to the effectors (over which 𝐺may then perform
induction), it also allows 𝐺to imagine its own percepts — for example, 𝐺might imagine a
Monte Carlo simulation [Ross (2022)], and then perform induction over the results
• the dotted lines indicate information flows that aren’t required functionally, but which may
enhance efficiency; e.g. having knowledge of the internal model or plan at the previous time
step makes the internal model or plan at the next time step much easier to calculate; similarly,
having knowledge of the current plan allows the continuous learning mechanism to focus
(resources etc) on those aspects of the physical universe that are currently the most relevant.
61 sometimes also called causation [Pearl (2009); Halpern (2016); Pearl and Mackenzie (2019)]
Copyright 2023-2024 BigMother.AI CIC
33

TURNER
3.2.39 AUTONOMOUS ROBOTS
Being a physical system (ultimately comprising computer hardware etc), an active problem-solver
must necessarily possess some kind of physical body, including its physical sensors and effectors.
In the limit, an active problem-solver may be mounted on a physical frame, such that its sensors
and effectors are sufficient for locomotion (i.e. the ability to move around in physical space).
For example, NASA’s Curiosity Mars rover (see Figure 9a) is capable of surface locomotion.
(a) Curiosity takes a selfie
(b) Atlas goes for a run
Figure 9: Examples of contemporary robots capable of surface locomotion
Although the Curiosity rover is able to do some things "all by itself" — such as navigate from
point A to point B, and select interesting rocks to sample — it is still nevertheless mostly controlled by
NASA engineers on Earth. It is therefore only partially-autonomous, rather than fully-autonomous.
In order for a robot [Corke (2011)] to be genuinely fully-autonomous, the body of its planning
loop must comprise a sufficiently performant object-level passive problem-solver that the robot is
able to perform its desired function (i.e. pursuit of its final goal) without any human supervision.
In the popular imagination, the ultimate fully-autonomous robot (UFAR) comprises62:
1. a humanoid frame with sensors63 and effectors64 (e.g. c/o Boston Dynamics — Figure 9b)
2. a maximally-intrinsic active problem-solver65 (Section 3.2.29):
(a) incorporating:
(i) a continuous learning loop (Section 3.2.31)
— whose body comprises a superintelligent passive problem-solver (AGI O/Z)
(ii) a continuous planning loop (Section 3.2.30)
— whose body comprises a superintelligent passive problem-solver (AGI O/Z)
(b) that is maximally-aligned with human beings in perpetuity.
We shall explore the concept of maximal alignment over Sections 3.2.40 to 3.2.48.
62 note that, just as an active problem-solver (or agent) is effectively a wrapper around one or more passive problem-solvers
(Section 3.2.29), a fully-autonomous robot is effectively a wrapper around one or more active problem-solvers
63 e.g. vision ((in)visible/(un)polarised light, LIDAR, radar), hearing (audio, ultrasonic, sonar), touch (tactile/haptic), smell /
taste (olfactory), position (shaft encoders, magnetometer, gyroscope, accelerometer, clock, GPS), incoming cellular/WiFi
64 e.g. graphical displays, speakers, locomotors (legs, wheels), manipulators (arms, hands), outgoing cellular/WiFi
65 arranged such that UFAR’s sensors input to the percept history, and the action history outputs to UFAR’s effectors
Copyright 2023-2024 BigMother.AI CIC
34

THE BIGMOTHER MANIFESTO (PART 1)
3.2.40 LIVENESS AND SAFETY
We hereby extend the concept of agent to include the following, viewed as active problem-solvers:
• individual humans
• organisations
• autonomous robots.
The following are desirable properties of the behaviour of any agent 𝐺:
1. good (i.e. desirable) things happen66 — usually referred to as liveness67
2. bad (i.e. undesirable) things don’t happen66 — usually referred to as safety.
Things are a little more complicated than might appear at first glance because:
• the concepts of desirable and undesirable are subjective (i.e. relative to some observer)
• intent and effect (i.e. causal effect, as described in Section 3.2.37) are two different things.
The precise terminology pertaining to safety and liveness might be summarised as follows:
benevolent
(adjective)
(an agent 𝐺) intending to have desirable causal effect (as judged by observer 𝐻)
neutral
(adjective)
(an agent 𝐺) lacking any intent in respect of causal effect (as judged by observer 𝐻)
malevolent
(adjective)
(an agent 𝐺) intending to have undesirable causal effect (as judged by observer 𝐻)
beneficent
(adjective)
(an agent 𝐺, or action) having desirable causal effect (as judged by observer 𝐻)
immaterial
(adjective)
(an agent 𝐺, or action) not having any causal effect (as judged by observer 𝐻)
maleficent
(adjective)
(an agent 𝐺, or action) having undesirable causal effect (as judged by observer 𝐻)
benevolence
(noun)
the quality of intending to have desirable causal effect (as judged by observer 𝐻)
neutrality
(noun)
the quality of lacking any intent in respect of causal effect (as judged by observer 𝐻)
malevolence
(noun)
the quality of intending to have undesirable causal effect (as judged by observer 𝐻)
beneficence
(noun)
the quality of having desirable causal effect (as judged by observer 𝐻)
immateriality
(noun)
the quality of not having any causal effect (as judged by observer 𝐻)
maleficence
(noun)
the quality of having undesirable causal effect (as judged by observer 𝐻)
It’s possible for an agent 𝐺(e.g. a human, organisation, or autonomous robot) to be benevolent
(i.e. to have benevolent intent) and yet for the causal effects of 𝐺’s actions to nevertheless be judged
(by some observer 𝐻) as being some combination of beneficent, immaterial, and maleficent.
Note that different observers 𝐻1 and 𝐻2 might judge the causal effects of 𝐺’s actions differently.
66 as a result of 𝐺’s actions
67 sometimes also called progress — an agent that doesn’t do anything may be perfectly safe but, well, it doesn’t do anything!
Copyright 2023-2024 BigMother.AI CIC
35

TURNER
3.2.41 ALIGNMENT
If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently
interfere once we have started it ... then we had better be quite sure that the purpose put into the machine
is the purpose which we really desire and not merely a colorful imitation of it. [Wiener (1960)]
Simply stated, alignment = liveness + safety, i.e. good things happen, and bad things don’t.
As already alluded, the concepts of good (desirable) and bad (undesirable) are subjective, i.e.
only meaningful relative to some observer 𝐻, or, in general, some population of observers .
Intuitively, an agent 𝐺is aligned with human values 68 if 𝐺’s behaviour is consistent with .
Figure 10: Human values (as imagined by Midjourney)
3.2.42 HUMAN VALUES
If 𝐺is a purposefully-engineered maximally-intrinsic agent (such as an organisation, or an autonomous
robot) which we desire to be aligned with human values , then 𝐺must have knowledge of human
values in order to be able to behave accordingly. There are two ways in which this may be achieved:
(1) we (as 𝐺’s designers) work out what human values to use, and code them into 𝐺69, or
(2) by carefully observing humans, agent 𝐺works out for itself what human values to use.
Despite many centuries of effort by mankind’s greatest moral philosophers, there does not seem
to be any universally agreed upon set of human values that we can simply code into 𝐺. On the
contrary, we (humans) can’t even agree on when it is, and isn’t, OK to take a human life. Instead,
human values vary from individual to individual, from culture to culture, and even across time.
What were once acceptable human values 300 years ago are very different from what is considered
acceptable today, and doubtless many of today’s human values will seem barbaric 300 years from now.
As responsible AI designers, we must be careful not to impose our values onto future humans!
Thus option (1) doesn’t seem viable, which leaves option (2) whereby, in order to align itself with
humans, agent 𝐺carefully observes humans and, from those observations, determines (or at least
estimates as accurately as possible) an appropriate set of human values . A great advantage of this
approach is that agent 𝐺continually re-aligns itself in perpetuity as human values evolve over time.
68 corresponding to what humans (on aggregate) judge to be good (desirable) and bad (undesirable)
69 for example, via 𝐺’s constitution (if 𝐺is an organisation), or 𝐺’s final goal (if 𝐺is an autonomous robot)
Copyright 2023-2024 BigMother.AI CIC
36

THE BIGMOTHER MANIFESTO (PART 1)
3.2.43 ORDINAL PREFERENCES
The concept of "human values " is a little vague. It’s much easier to think in terms of preferences.
In general, an agent 𝐺performs a sequence of actions in pursuit of its goals. Any such action
sequence may have possible causal effects 𝐸1, ..., 𝐸𝑛with corresponding probabilities 𝑃1, ..., 𝑃𝑛
70.
Imagine 𝕊𝕥𝕒𝕥𝕖𝕤, the set of all distinguishable states of the universe {𝑆1, ..., 𝑆𝑢}. Given sufficient
information, 𝕊𝕥𝕒𝕥𝕖𝕤may be strictly ordered such that 𝑆𝑥< 𝑆𝑦if and only if observer 𝐻judges state
𝑆𝑥to be less desirable than state 𝑆𝑦. Such an ordering is said to express 𝐻’s individual preferences
[Arrow (1951); Hare (1982); Hansson and Grüne-Yanoff (2022)]. If the initial state is 𝑆𝐼then causal
effect 𝐸corresponding to new state 𝑆𝐸is beneficent71 if 𝑆𝐼< 𝑆𝐸, and maleficent71 if 𝑆𝐸< 𝑆𝐼:
Beneficent causal effect 𝐸:
Immaterial causal effect 𝐸:
Maleficent causal effect 𝐸:
Note that, in general, observer 𝐻’s preferences are not fixed, and may change over time.
3.2.44 UTILITY FUNCTIONS, AND EXPECTED UTILITY
Some preferences are more strongly held than others (e.g. a preference not to be killed vs. a preference
for chocolate ice cream). A utility function 𝑈for observer 𝐻assigns a real (decimal) number to each
distinguishable state of the universe. If the initial state is 𝑆𝐼then causal effect 𝐸corresponding to new
state 𝑆𝐸is beneficent [with impact 𝑈(𝑆𝐸) −𝑈(𝑆𝐼)] if 𝑈(𝑆𝐼) < 𝑈(𝑆𝐸), maleficent [with impact
𝑈(𝑆𝐼) −𝑈(𝑆𝐸)] if 𝑈(𝑆𝐸) < 𝑈(𝑆𝐼), and immaterial [with impact 0] if 𝑈(𝑆𝐸) = 𝑈(𝑆𝐼). The
expected utility (according to 𝑈) of an action sequence having possible causal effects 𝐸1, ..., 𝐸𝑛
(corresponding to new states 𝑆1, ..., 𝑆𝑛) with corresponding probabilities 𝑃1, ..., 𝑃𝑛may be calculated
by multiplying, for each possible causal effect 𝐸𝑖, the utility (according to 𝑈) of the corresponding new
state 𝑆𝑖by the probability 𝑃𝑖that the causal effect in question will occur, and then summing the results:
expected utility = [𝑈(𝑆1) × 𝑃1] + [𝑈(𝑆2) × 𝑃2] + [𝑈(𝑆3) × 𝑃3] + ... + [𝑈(𝑆𝑛) × 𝑃𝑛].
70 in other words, 𝑃𝑖is the probability (between 0 and 1) that causal effect 𝐸𝑖will occur as a result of action sequence 
71 as judged by H
Copyright 2023-2024 BigMother.AI CIC
37

TURNER
3.2.45 ELICITING HUMAN PREFERENCES
Captain Renault:
And what in heaven’s name brought you to Casablanca?
Rick Blaine:
My health. I came to Casablanca for the waters.
Captain Renault:
The waters? What waters? We’re in the desert.
Rick Blaine:
I was misinformed.
Casablanca, 1942
Let’s imagine that we desire agent 𝐺to align its behaviour with an individual human 𝐻. One
approach would be for agent 𝐺to carefully observe 𝐻and, from those observations, determine (or at
least estimate as accurately as possible) what 𝐻’s preferences are (expressed as a utility function ̂𝑈)
in order that 𝐺may then behave in a manner that is consistent with those (estimated) preferences.
In practice, preference elicitation is surprisingly difficult. For example:
• Being human, 𝐻may sometimes be less than entirely rational (this being the kindest way we
can find to say it!) It will rarely be in 𝐻’s best interest for 𝐺to realise an irrational preference.
• 𝐻may sometimes be less than entirely truthful (e.g. in embarrassing social contexts).
• 𝐻might not always know what they want (either precisely, or even at all).
• Even if they do, 𝐻might not always express their preferences clearly and unambiguously72.
• Accordingly, 𝐻’s stated preferences are not necessarily the same as 𝐻’s actual preferences73.
• It’s possible that 𝐻is less than entirely well-informed74 to some significant degree in respect
of the preferability of some future state, perhaps as a result of being deliberately misinformed
by another party [Davies (2009); Oborne (2021)]. For example, 𝐻might request a rat poison
cupcake for dessert, on the basis of what somebody had told them, or some YouTube video that
they had seen. It will rarely be in 𝐻’s best interest for 𝐺to realise an ill-informed preference.
• It’s possible that an actor 𝐽has manipulated 𝐻into ostensibly having some preference 𝑃to a
greater or lesser degree than 𝐻would have had had it not been for 𝐽’s manipulation [Handelman
(2009); Coons and Weber (2014); Noggle (2022)]. In such situations, 𝐻’s apparent preference
𝑃is less than entirely genuine, having been influenced by 𝐽’s manipulation, and will in some
cases effectively be 𝐽’s preference rather than 𝐻’s. It will not necessarily be in 𝐻’s best
interest for 𝐺to realise a preference 𝑃that has not been determined entirely freely by 𝐻.
Accordingly, 𝐻’s best interests would be much better served if agent 𝐺were to carefully observe
𝐻and, from those observations, determine (or at least estimate as accurately as possible)75 what
𝐻’s actual rational well-informed freely-determined preferences (expressed as a utility function ̂𝑈)
would be if 𝐻were entirely rational, well-informed, and free from manipulation by any party76.
72 for example, 𝐻might be a pre-verbal human child
73 just ask The Sorcerer’s Apprentice [Wiener (1960)] or King Midas [Russell (2019)]
74 here, a well-informed person does not simply have access to the relevant information, but also fully understands it
75 e.g. via an arbitrarily complex combination of strong probabilistic induction, deduction, and abduction (3.2.27 and 3.2.28)
76 qualified preferences such as these are sometimes called idealised preferences [see e.g. Hendrycks (2023)]
Copyright 2023-2024 BigMother.AI CIC
38

THE BIGMOTHER MANIFESTO (PART 1)
3.2.46 AGGREGATING HUMAN PREFERENCES
In order for agent 𝐺to align itself with all humans , one approach would be for 𝐺to:
(1) carefully observe each individual human 𝐻𝑖in the population of all humans 77 and, from
those observations, determine (or at least estimate as accurately as possible) what 𝐻𝑖’s actual
rational well-informed freely-determined preferences (expressed as a utility function ̂𝑈𝑖) would
be if 𝐻𝑖were entirely rational, well-informed, and free from manipulation by any party
(2) calculate, in some "perfectly fair" manner, an aggregated utility function ̂𝑈for the population
of all humans from the individual utility functions ̂𝑈𝑖for each individual human 𝐻𝑖.
Although many different methods have been proposed by economists for aggregating individual
utility functions [Arrow (1951); Gabriel (2020); List (2022)], none is ideal78. In other words, there is
no unambiguously "perfectly fair" method of doing so, which means that the best that an agent 𝐺
can ever hope to achieve in actual practice is to try to find some not necessarily perfectly fair but
nevertheless maximally fair way in which to calculate the estimated aggregated utility function ̂𝑈.
One possible solution therefore would be to split step (2) above into two parts, as follows:
(2a) calculate, in some sensible manner, an interim aggregated utility function ̄𝑈for the population
of all humans from the individual utility functions ̂𝑈𝑖for each individual human 𝐻𝑖
(2b) calculate, in some manner consistent with ̄𝑈, an aggregated utility function ̂𝑈for the population
of all humans from the individual utility functions ̂𝑈𝑖for each individual human 𝐻𝑖.
In other words, the interim aggregated utility function ̄𝑈for all humans calculated at step (2a)
constrains how the final aggregated utility function ̂𝑈for all humans is calculated at step (2b)79.
3.2.47 REALISING HUMAN PREFERENCES
Once agent 𝐺has estimated an aggregated utility function ̂𝑈for the population of all humans , 𝐺
may proceed to behave (perform a sequence of actions) consistent with ̂𝑈, for example as follows:
• 𝐺cannot possibly consider every possible sequence of future actions, as that would be infinite
• instead, 𝐺must necessarily look ahead by some finite amount, say the next 𝑀≥1 actions
• using its internal model (Section 3.2.34) as maintained by its learning mechanism (Section
3.2.31), 𝐺attempts to predict, for every candidate action sequence of length 𝑀:
– the possible causal effects (of action sequence ) 𝐸1, ..., 𝐸𝑛
– their corresponding probabilities 𝑃1, ..., 𝑃𝑛
• 𝐺then performs the action sequence yielding the greatest expected utility according to ̂𝑈.
Thus agent 𝐺strives to behave in such a way as to maximise expected utility for all humans .
77 or at least a statistically meaningful sample of — the larger the better
78 one problem, for example, is that individual utilities are not easily comparable — "10 units of utility" for one person does
not necessarily equal "10 units of utility" for another; inter-person utility calibration requires additional information
79 the process of constructing an aggregated utility function ̂𝑈could of course be extended to more than two levels
Copyright 2023-2024 BigMother.AI CIC
39

TURNER
3.2.48 MAXIMAL ALIGNMENT
We will say that agent 𝐺is maximally-aligned in perpetuity with the maximally-fairly-aggregated
idealised preferences of population , such as the population of all humans (living or future), if, at
any time 𝑡, (a) utility function 𝑈𝑡encapsulates the maximally-fairly-aggregated idealised preferences
of population , and (b) the causal effect 𝐸(corresponding to a new state of the physical universe
𝑆𝐸) of any action performed by 𝐺is such that 𝑈𝑡(𝑆𝐸) is the maximum value of 𝑈80, 81. In other
words, agent 𝐺’s behaviour as judged (on aggregate) by population cannot possibly be improved.
Unfortunately, such perfect behaviour is all-but-impossible in practice. For example, in general:
– 𝐺will never have enough information (in particular, about humans, or about how the universe works)
– 𝐺will never have enough physical resources (in particular, time, space, energy, and compute)
– 𝐺’s estimation ̂𝑈𝑡of ’s instantaneous aggregated utility function 𝑈𝑡will never be entirely accurate
– 𝐺’s predictions pertaining to the possible causal effects of its actions will never be entirely accurate.
Thus maximal alignment effectively requires unbounded information and physical resources.
Maximal alignment is nevertheless an ideal towards which any benevolent agent 𝐺should strive.
We will say that agent 𝐺is maximally-aligned-but-for-information-or-resources if 𝐺would
be maximally-aligned if only it had more information or resources. Thus maximal-alignment-but-
for-information-or-resources is a minimum requirement for any benevolent agent 𝐺. We will say
that agent 𝐺is near-maximally-aligned if 𝐺is maximally-aligned-but-for-information-or-resources
and "almost, but not quite" maximally-aligned, the precise definition of which is necessarily a value
judgement, such as (henceforth assumed) that 𝐺’s behaviour is considered to be robustly safe by
the population of all humans (living or future)82. We make the following observations:
• near-maximal-alignment requires considerable knowledge and understanding — in order
to achieve near-maximal-alignment in respect of the population of all humans , a maximally-
intrinsic agent 𝐺must necessarily possess a deep understanding of a wide range of complex and
nuanced subjects, including humans, human languages83, mathematics (including uncertainty),
and physics (including causality); 𝐺will also need to maintain a detailed and accurate internal
model of the physical universe in order to be able to make accurate predictions about it
• near-maximal-alignment requires considerable problem-solving ability — in order to
reason about all of the above, 𝐺will need to apply robust critical thought and problem-solving
(e.g. strong probabilistic induction, deduction, and abduction) to complex and nuanced concepts
• near-maximal-alignment requires massive compute — solving problems pertaining to all of
humanity will require massive compute84; without it, near-maximal-alignment is compromised
• most AGI designs will be less than robustly safe — near-maximal alignment requires highly-
specific technical capabilities which the vast majority of AGI designs will simply not possess.
80 in other words, there is no "better" state of the physical universe 𝑆for which 𝑈𝑡(𝑆) > 𝑈𝑡(𝑆𝐸)
81 analogously, agent 𝐺is maximally-misaligned/medially-aligned if 𝑈𝑡(𝑆𝐸) is the minimum/median value of 𝑈, respectively
82 maximally-misaligned-but-for-information-and-resources and near-maximally-misaligned may be defined analogously
83 35 languages cover 95% of all humans: English, Mandarin Chinese (inc Standard Chinese), Hindi, Spanish, French, Modern
Standard Arabic, Bengali, Portuguese, Russian, Urdu, Indonesian, Standard German, Japanese, Nigerian Pidgin, Egyptian
Arabic, Marathi, Telugu, Turkish, Tamil, Yue Chinese (inc Cantonese), Vietnamese, Wu Chinese (inc Shanghainese),
Tagalog, Korean, Iranian Persian, Hausa, Swahili, Javanese, Italian, Punjabi, Gujarati, Thai, Kannada, Amharic, Bhojpuri
84 we suspect easily 10 or 20 orders of magnitude more compute per dollar (and ideally per watt) than is available in 2024
Copyright 2023-2024 BigMother.AI CIC
40

THE BIGMOTHER MANIFESTO (PART 1)
3.2.49 ALIGNMENT IS EVERYTHING!
Compare the following best- and worst-case scenarios:
• best-case scenario — imagine that agent 𝐺is an AGI Z (maximally-super-inventive +
maximally-super-knowledgeable + maximally-super-resourced, i.e.
the most intelligent
superintelligent AGI possible)85, and that 𝐺is at least near-maximally-aligned (i.e. at
least robustly safe) and at the same time strives to be maximally-aligned with the population
of all humans whose aggregated preferences are encapsulated by utility function 𝑈; given
all of the above, 𝐺will strive to behave in such a way as to maximise expected utility for all
humans to the maximum extent that is possible in actual practice; in other words, good
things (resulting from 𝐺actions) are maximised, and bad things (resulting from 𝐺’s actions)
are minimised, where what counts as good and bad, as well as the trade-offs between good and
bad things, are determined by utility function 𝑈(encapsulating ’s aggregated preferences);
or, to put it yet another way, 𝐺will strive to create the best possible utopia for all mankind
Figure 11: A near-utopia for all mankind for all eternity (as imagined by Midjourney)
• worst-case scenario — imagine that agent 𝐺is the exact same AGI Z as above except that its
final goal has been modified such that 𝐺is at least near-maximally-misaligned (i.e. at least
robustly unsafe) and at the same time strives to be maximally-misaligned with the population
of all humans whose aggregated preferences are encapsulated by utility function 𝑈; given
all of the above, 𝐺will strive to behave in such a way as to minimise expected utility for all
humans to the maximum extent that is possible in actual practice; in other words, good
things (resulting from 𝐺actions) are minimised, and bad things (resulting from 𝐺’s actions)
are maximised, where what counts as good and bad, as well as the trade-offs between good and
bad things, are determined by utility function 𝑈(encapsulating ’s aggregated preferences);
or, to put it yet another way, 𝐺will strive to create the worst possible dystopia for all mankind.
85 consistent with aggregated human preferences, e.g. not so much compute that Earth’s atmosphere boils away!
Copyright 2023-2024 BigMother.AI CIC
41

TURNER
Figure 12: A near-dystopia for all mankind for all eternity (as imagined by Midjourney)
The only difference between these two scenarios is a minus sign — in the best-case scenario agent
𝐺strives to be maximally-aligned, and in the worst-case agent 𝐺strives to be maximally-misaligned.
Every intermediate scenario in between the best and worst cases is also possible.
3.2.50 X-RISK AND S-RISK
The reality is that the chances of a misaligned AI are not small. In fact, in the absence of an
effective safety program, that is the only outcome we will get. ... We are looking at an almost
guaranteed event with the potential to cause an existential catastrophe. [Yampolskiy (2024b)]
In the worst-case scenario, one possibility is for agent 𝐺to simply decide to kill all humans.
Figure 13: A lifeless post-apocalyptic Earth (as imagined by Midjourney)
Copyright 2023-2024 BigMother.AI CIC
42

THE BIGMOTHER MANIFESTO (PART 1)
Given that agent 𝐺is an AGI Z, and therefore far more intelligent than any human, mere humans
would be powerless to stop it. Thus there exists at least one plausible scenario in which a powerful
AGI would represent an existential threat (x-risk) to the continued existence of the human species.
Many other (less than existential) scenarios exist where the threat is one of massive human suffering
(s-risk) [DiGiovanni (2023)] or global catastrophe [Hendrycks, Mazeika, and Woodside (2023)86].
3.2.51 THE INTERPLAY BETWEEN INTELLIGENCE AND ALIGNMENT
In Section 3.2.49, we assume that agent 𝐺is an AGI Z, i.e. the most intelligent system imaginable,
and certainly far more intelligent than mere superintelligence (AGI O), or mere humans (~AGI L).
If we now reduce agent 𝐺’s intelligence from AGI Z through AGI O to AGI L, AGI K, and finally
AGI J — in the best-case, worst-case, or any intermediate scenario — then agent 𝐺’s motivations
and intent remain the same but its capabilities and effectiveness are gradually diminished.
As already alluded in Section 3.2.48, below some minimum threshold of intelligence (probably
~AGI L, i.e. broadly human-level AGI), agent 𝐺will lack the cognitive ability to be either near-
maximally-aligned, or near-maximally-misaligned (i.e. it simply won’t understand the necessary
concepts, or be able to reason about them as required). At or above that minimum threshold,
however, and assuming that 𝐺is at least near-maximally-aligned and also strives to be maximally-
aligned, then good things (resulting from 𝐺’s actions) will increase as 𝐺’s intelligence increases —
as 𝐺’s capabilities and effectiveness increase — and bad things (resulting from 𝐺’s actions) will
decrease. Therefore, as long as (≥~AGI L) agent 𝐺is at least near-maximally-aligned and also
strives to be maximally-aligned, the smarter it can be made (e.g. to AGI O/Z), the better.
3.2.52 WELL-FOUNDED AGI
Imagine that we are successful in implementing the best-case scenario described in Section 3.2.49,
with an AGI Z that is at least near-maximally-aligned and that strives to be maximally-aligned.
Given the stakes (if we get it right then we will have created a near-utopia for all mankind for all
eternity (Figure 11), but if we get it wrong then we may have created a near-dystopia for all mankind
for all eternity (Figure 12)), we need to be able to convince (i) ourselves, and (ii) everyone else of
the correctness of our design and the unimpeachable trustworthiness of its implementation.
It is not enough to simply say "well, we tried it a bunch of times and it seemed to sort of work OK
most of the time". Instead, such a critical system must necessarily be well-founded [Russell (2023)]:
• semantically well-defined, individually checkable components
• rigorous theory of composition for complex agent architectures
• built on formally verified [hardware and] software stacks for single and multiple agents.
This requirement will disqualify any implementation technology that cannot satisfy it.
86 "Rapid advancements in [AI] have sparked growing concerns among experts, policymakers, and world leaders regarding
the potential for increasingly advanced AI systems to pose catastrophic risks. ... This paper provides an overview of the
main sources of catastrophic AI risks ...: malicious use, in which individuals or groups intentionally use AIs to cause harm;
AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks,
highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs,
describing the inherent difficulty in controlling agents far more intelligent than humans."
Copyright 2023-2024 BigMother.AI CIC
43

TURNER
3.2.53 THE ALIGNMENT MATRIX
We now consider the space of all possible agents 𝐺, organised by intelligence and degree of alignment.
Given all of the above, we may identify:
• four possible levels of intelligence (Sections 3.2.16 and 3.2.17):
• nine possible levels of alignment87 (Section 3.2.48):
If we arrange these features along two axes, we obtain the following alignment matrix:
Note that some agent configurations are:
• unsafe, i.e. considered by humans to be less than robustly safe (Section 3.2.48)
• either a suffering risk (s-risk) or an existential risk (x-risk) to mankind (Section 3.2.50) —
poorly aligned superintelligent agents are particularly dangerous [Yampolskiy (2024a)]
• effectively impossible, because agent 𝐺is just not smart enough (Section 3.2.51)
• effectively a trapdoor, because agent 𝐺is too smart for us to trick into modifying its behaviour.
87 with respect to a utility function encapsulating maximally-fairly-aggregated idealised human preferences
Copyright 2023-2024 BigMother.AI CIC
44

THE BIGMOTHER MANIFESTO (PART 1)
3.3 Consciousness
NOTE — This section is incomplete.
3.4 The specific problem that we seek to address
NOTE — This section is incomplete.
3.5 Our proposed approach to the problem that we have identified
NOTE — This section is incomplete.
Copyright 2023-2024 BigMother.AI CIC
45

TURNER
4. Cognitive Architecture (NOTE — This section is incomplete)
The AI control problem, then, is how to ensure that systems with an arbitrarily
high degree of intelligence remain strictly under human control. [Russell (2022)]
What should the system’s final goal be...? We propose Turner’s Three Laws (T3L)88,89:
"Assume for the purposes of this final goal (which is expressed in 2024 English, and should be
interpreted relative to your own temporal frame of reference) that the physical universe exists;
given this assumption, perform tasks 1 and 2 (only, simultaneously, repeatedly, continuously, in
perpetuity, and to the best of your ability), subject to qualification , with the overall objective of
behaving in a manner that is maximally-aligned in perpetuity with the maximally-fairly-aggregated
idealised preferences of all human beings (living or future), where 1, 2, and are as follows:
1 for each individual human being (living or future), strive to estimate (as accurately as
possible) what ’s actual rational well-informed freely-determined preferences would be
if were entirely rational, well-informed, and free from manipulation by any party;
2 for each individual human being (living or future), strive to maximise the extent to which
’s preferences (as estimated pursuant to 1) are (and most likely will be) realised;
strive to resolve (to the best of your ability, and in a manner that is consistent with a maximally
fair aggregation of the individual preferences (as estimated pursuant to 1) of the living
human being population) any problems that may arise in respect of this final goal."
Note that T3L tacitly assumes that every passive problem-solver underlying the active problem-
solver for which T3L is the final goal possesses a minimum level of (a) problem-solving ability
(i.e. inventiveness), (b) knowledge and understanding (i.e. information), and (c) available physical
resources (e.g. compute); accordingly, a maximally-intrinsic AGI O or Z (which, as already alluded,
necessarily possesses (i) an inventor that (given the same information) is better than any human at
solving any problem, (ii) a deep understanding of all human knowledge90, and (iii) sufficient physical
resources (e.g. compute) to be able to deliver timely solutions) will amply satisfy this requirement.
We conjecture the following:
• any active problem-solver for which T3L is the final goal will (a) strive to be maximally-aligned
in perpetuity with maximally-fairly-aggregated idealised human preferences, and therefore (b)
continually re-align itself with human preferences, in perpetuity, as these naturally evolve
• it’s humans (collectively) that are in control of the machine, not the other way around
• no individual human or minority of humans can ever gain control of the machine
• if enough humans genuinely want the machine to switch itself off, then it will do so.
88 a nod to Azimov’s Three Laws [Asimov (1942); Asimov (1950)]
89 note that, according to the construction sequence (Section 5), T3L only becomes operational during the very final stages of
the system’s construction; this means that, if T3L needs refining in any way, we still have 50-100 years in which to do so!
90 including all published AI, AI safety, and AI alignment literature; accordingly, (being super-knowledgeable) any such
system will not only have a far deeper grasp of AI safety than any human AI safety researcher, but (being super-inventive)
it will also be able to formulate far better solutions to any AI-safety-related problem than any human AI safety researcher
Copyright 2023-2024 BigMother.AI CIC
46

THE BIGMOTHER MANIFESTO (PART 1)
5. Construction Sequence (NOTE — This section is incomplete)
6. Governance (NOTE — This section is incomplete)
For a successful technology, reality must take precedence over
public relations, for nature cannot be fooled. Feynman (1986)
7. Execution (NOTE — This section is incomplete)
We can only see a short distance ahead, but we can
see plenty there that needs to be done. Turing (1950)
8. Conclusion (NOTE — This section is incomplete)
It seems a pity, but I do not think that I can write more.
For God’s sake, look after our people. Scott (1912)
9. Acknowledgements
We are deeply indebted to the following informal reviewers, each of whom provided invaluable
encouragement and priceless feedback during the course of this paper’s interminable development91:
Reviewer
Affiliation
Professor Leslie Smith
University of Stirling
Professor Pei Wang
Temple University
Professor Bob Kowalski
Imperial College London
Professor Jay McClelland
Stanford University
Many thanks to all of you!
91 note that each informal reviewer read some subset of the paper as it gradually developed, not necessarily the whole thing!
Copyright 2023-2024 BigMother.AI CIC
47

TURNER
References
Adams, D. 1978. The Hitchhiker’s Guide to the Galaxy (radio series). BBC Radio 4.
Adler, M. J. 1974. Propædia. In Encyclopædia Britannica. Encyclopædia Britannica, Inc., 15th
edition.
Alexander, S. 2014. Meditations on Moloch. https://slatestarcodex.com/2014/07/30/
meditations-on-moloch/.
Allen, J.; Hendler, J.; and Tate, A., eds. 1990. Readings in Planning. Morgan Kaufmann.
Altman,
S.
2023.
Planning for AGI and beyond.
https://openai.com/blog/
planning-for-agi-and-beyond.
Arrow, K. J. 1951. Social Choice and Individual Values. Yale University Press.
Asimov, I. 1942. Runaround. In Astounding Science Fiction. Street & Smith.
Asimov, I. 1950. I, Robot. Gnome Press.
Barr, A.; Feigenbaum, E. A.; and Cohen, P. R., eds. 1982. The Handbook of Artificial Intelligence
(Vols 1-3). Pitman Books.
Bartha, P. 2022. Analogy and Analogical Reasoning. In Zalta, E. N., ed., The Stanford Encyclopedia
of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2022 edition.
Barwise, J. 1977. An introduction to first-order logic. In Barwise, J., ed., Handbook of Mathematical
Logic. North-Holland.
Benaroya, H., and Han, S. M. 2005. Probability Models in Engineering and Science. Taylor &
Francis.
Bereska, L., and Gavves, E. 2024. Mechanistic Interpretability for AI Safety – A Review.
Bernardo, J. M., and Smith, A. F. 2000. Bayesian Theory. John Wiley & Sons.
Bishop, C. M. 2006. Pattern Recognition and Machine Learning. Springer.
Bostrom, N. 2014. Superintelligence: Paths, Dangers, Strategies. Oxford University Press.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam,
P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.;
Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D.
2020. Language Models are Few-Shot Learners. https://arxiv.org/pdf/2005.14165.pdf.
Brynjolfsson, E. 2022. The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence.
Cole, D. 2023. The Chinese Room Argument. In Zalta, E. N., and Nodelman, U., eds., The
Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer
2023 edition.
Copyright 2023-2024 BigMother.AI CIC
48

THE BIGMOTHER MANIFESTO (PART 1)
Coons, C., and Weber, M. 2014. Manipulation: Theory and Practice. Oxford University Press.
Corke, P. 2011. Robotics, Vision, and Control: Fundamental Algorithms in MATLAB. Springer.
Cox, R. 1946. Probability, Frequency, and Reasonable Expectation. American Journal of Physics
14(1).
Davies, N. 2009. Flat Earth News. Vintage.
Dawes, R. M. 1980. Social Dilemmas. Annual Review of Psychology 31.
Dechter, R. 2003. Constraint Processing. Elsevier Science.
DiGiovanni, A.
2023.
Beginner’s guide to reducing s-risks.
https://longtermrisk.org/
beginners-guide-to-reducing-s-risks/.
Dongarra, J., and Geist, A. 2022. Report on the Oak Ridge National Laboratory’s Frontier System.
Technical Report ICL-UT-22-05, University of Tennessee.
Douven, I. 2021. Abduction. In Zalta, E. N., ed., The Stanford Encyclopedia of Philosophy.
Metaphysics Research Lab, Stanford University, Summer 2021 edition.
Douven, I. 2022. The Art of Abduction. MIT Press.
Ernst, G., and Newell, A. 1969. GPS: A Case Study in Generality and Problem Solving. Academic
Press Inc.
Faul, A. 2020. A Concise Introduction to Machine Learning. CRC Press.
Feynman, R. P. 1986. Report of the Presidential Commission on the Space Shuttle Challenger Accident
— Appendix F. https://www.e-education.psu.edu/files/meteo361/file/nasa_report.
pdf.
Flach, P., and Hadjiantonis, A., eds. 2000. Abduction and Induction: Essays on their Relation and
Integration. Kluwer Academic Publishers.
Gabriel, I. 2020. Artificial Intelligence, Values, and Alignment. Minds and Machines 30(3):411–437.
Gelman, A.; Carlin, J. B.; Stern, H. S.; Dunson, D. B.; Vehtari, A.; and Rubin, D. B. 2014. Bayesian
Data Analysis (Third edition). CRC Press.
Ghallab, M.; Nau, D.; and Traverso, P. 2016. Automated Planning and Acting. Cambridge University
Press.
Goertzel, B., and Pennachin, C., eds. 2007. Artificial General Intelligence. Springer.
Goertzel, B., and Wang, P., eds. 2007. Advances in Artificial General Intelligence: Concepts,
Architectures and Algorithms. IOS Press.
Good, I. 1966. Speculations Concerning the First Ultraintelligent Machine. Advances in Computers
6:31–88.
Copyright 2023-2024 BigMother.AI CIC
49

TURNER
Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. Deep Learning. MIT Press.
Gottfredson, L. S. 1997. Mainstream Science on Intelligence: An Editorial with 52 Signatories,
History, and Bibliography. Intelligence 24(1):13–23.
Halbach, V. 2010. The Logic Manual. Oxford University Press.
Halpern, J. Y. 2016. Actual Causality. MIT Press.
Halpern, J. Y. 2017. Reasoning About Uncertainty (Second edition). MIT Press.
Handelman, S., ed. 2009. Thought Manipulation: The Use and Abuse of Psychological Trickery.
ABC-CLIO.
Hansson, S. O., and Grüne-Yanoff, T. 2022. Preferences. In Zalta, E. N., ed., The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2022 edition.
Harari, Y. N. 2011. Sapiens: A Brief History of Humankind. Vintage.
Hardin, G. 1968. The Tragedy of the Commons. Science 162:1243–1248.
Hare, R. M. 1982. Moral Thinking: Its Levels, Method and Point. Oxford University Press.
Henderson, L. 2020. The Problem of Induction. In Zalta, E. N., ed., The Stanford Encyclopedia of
Philosophy. Metaphysics Research Lab, Stanford University, Spring 2020 edition.
Hendrycks, D.; Mazeika, M.; and Woodside, T. 2023. An Overview of Catastrophic AI Risks.
Hendrycks, D. 2023. Introduction to AI Safety, Ethics, and Society. https://www.aisafetybook.
com.
Holland, J. H.; Holyoak, K. J.; Nisbett, R. E.; and Thagard, P. R. 1986. Induction: Processes of
Inference, Learning, and Discovery. MIT Press.
Jain, S. 1999. Systems that Learn: An Introduction to Learning Theory. MIT Press.
Jaynes, E. 2003. Probability Theory: The Logic of Science. Cambridge University Press.
Jeffrey, R. 2004. Subjective Probability: The Real Thing. Cambridge University Press.
Jeffreys, H. 1939. Theory of Probability. Clarendon Press.
Johnson, G. 2016. Argument & Inference: An Introduction to Inductive Logic. MIT Press.
Kolmogorov, A. 1933. Foundations of the Theory of Probability. Chelsea Publishing Company.
Kowalski, R. 1979. Algorithm = Logic + Control. Communications of the ACM 22(1):424–436.
Kowalski, R. 2011a. Artificial Intelligence and Human Thinking (presentation delivered at IJCAI
2011). https://www.youtube.com/watch?v=k_HGvkfDnT8.
Kowalski, R. 2011b. Computational Logic and Human Thinking: How to be Artificially Intelligent.
Cambridge University Press.
Copyright 2023-2024 BigMother.AI CIC
50

THE BIGMOTHER MANIFESTO (PART 1)
Lakatos, I. 1976. Proofs and Refutations: The Logic of Mathematical Discovery. Cambridge
University Press.
LessWrong. 2011. Oracle AI. https://www.lesswrong.com/tag/oracle-ai.
LessWrong. 2012. Tool AI. https://www.lesswrong.com/tag/tool-ai.
List, C. 2022. Social Choice Theory. In Zalta, E. N., and Nodelman, U., eds., The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2022 edition.
Liu, Y.; Han, T.; Ma, S.; Zhang, J.; Yang, Y.; Tian, J.; He, H.; Li, A.; He, M.; Liu, Z.; Wu, Z.; Zhu,
D.; Li, X.; Qiang, N.; Shen, D.; Liu, T.; and Ge, B. 2023. Summary of ChatGPT/GPT-4 Research
and Perspective Towards the Future of Large Language Models.
MacKay, D. J. 2003. Information Theory, Inference, and Learning Algorithms. Cambridge University
Press.
McDermott, D. 1976. Artificial Intelligence meets Natural Stupidity. SIGART Newsletter 57:4–9.
Mendelson, E. 1987. Mathematical Logic. Wadsworth & Brooks/Cole, 3rd edition.
Michalski, R. S.; Carbonell, J. G.; and Mitchell, T. M., eds. 1983. Machine Learning: An Artificial
Intelligence Approach. Tioga Publishing Co.
Mitchell, T. M. 1997. Machine Learning. McGraw-Hill.
Monett, D.; Lewis, C. W.; and Thórisson, K. R. 2020. Introduction to the JAGI Special Issue "On
Defining Artificial Intelligence" – Commentaries and Author’s Response. Journal of Artificial
General Intelligence 11.
Morris, M. R.; Sohl-dickstein, J.; Fiedel, N.; Warkentin, T.; Dafoe, A.; Faust, A.; Farabet, C.; and
Legg, S. 2023. Levels of AGI: Operationalizing Progress on the Path to AGI.
Mortimer, H. 1988. The Logic of Induction. Ellis Horwood.
Murphy, K. P. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.
Newell, A., and Simon, H. A. 1972. Human Problem Solving. Echo Point Books and Media.
Newell, A.; Shaw, J.; and Simon, H. A. 1958. Report on a General Problem-Solving Program.
Technical Report P-1584, Rand Corporation.
Newell, A. 1990. Unified Theories of Cognition. Harvard University Press.
Nilsson, N. J. 2010. The Quest for Artificial Intelligence: A History of Ideas and Achievements.
Cambridge University Press.
Nixon, R. M. 1962. Six Crises. W H Allen.
Noggle, R. 2022. The Ethics of Manipulation. In Zalta, E. N., ed., The Stanford Encyclopedia of
Philosophy. Metaphysics Research Lab, Stanford University, Summer 2022 edition.
Copyright 2023-2024 BigMother.AI CIC
51

TURNER
Oborne, P. 2021. The Assault on Truth. Simon & Schuster.
Oppenlaender, J. 2022. The Creativity of Text-to-Image Generation. In Proceedings of the 25th
International Academic Mindtrek Conference. ACM.
Paolo, G.; Gonzalez-Billandon, J.; and Kégl, B. 2024. A call for embodied AI.
Pearl, J., and Mackenzie, D. 2019. The Book of Why: The New Science of Cause and Effect. Penguin
Science.
Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers.
Pearl, J. 2009. Causality: Models, Reasoning, and Inference. Cambridge University Press.
Pólya, G. 1945. How To Solve It. Princeton University Press.
Pólya, G. 1954a. Mathematics and Plausible Reasoning, volume I: Induction and Analogy in
Mathematics. Princeton University Press.
Pólya, G. 1954b. Mathematics and Plausible Reasoning, volume II: Patterns of Plausible Inference.
Princeton University Press.
Pólya, G. 1962a. Mathematical Discovery: On Understanding, Learning and Teaching Problem
Solving, volume I. Ishi Press International.
Pólya, G. 1962b. Mathematical Discovery: On Understanding, Learning and Teaching Problem
Solving, volume II. Ishi Press International.
Ramamoorthy, A., and Yampolskiy, R. V. 2018. Beyond MAD?: The Race for Artificial General
Intelligence. https://www.itu.int/en/journal/001/Documents/itu2018-9.pdf.
Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.; and Sutskever, I. 2021.
Zero-Shot Text-to-Image Generation.
Rasmussen, C. E., and Williams, C. K. 2006. Gaussian Processes for Machine Learning. MIT Press.
Rescorla, M. 2019. The Language of Thought Hypothesis. In Zalta, E. N., ed., The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2019
edition.
Robinson, J. 1965. A Machine-Oriented Logic Based on the Resolution Principle. Journal of the
Association for Computing Machinery 12 (1):23–41.
Romero, A. 2024. The GPT-4 Generation: Why Are the Best AI Models Equally Intelligent?
https://www.thealgorithmicbridge.com/p/the-gpt-4-generation-why-are-the.
Rosenthal, J. S. 2006. A First Look at Rigorous Probability Theory (Second edition). World Scientific.
Ross, S. M. 2022. Simulation. Academic Press.
Copyright 2023-2024 BigMother.AI CIC
52

THE BIGMOTHER MANIFESTO (PART 1)
Russell, S., and Norvig, P. 2021a. Artificial Intelligence, A Modern Approach. Prentice Hall, 4th
edition.
Russell, S. 2019. Human Compatible: AI and the Problem of Control. Allen Lane.
Russell, S. 2022. Provably Beneficial Artificial Intelligence. In Proceedings of the 27th International
Conference on Intelligent User Interfaces, IUI ’22, 3. New York, NY, USA: Association for
Computing Machinery.
Russell, S. 2023. How Not To Destroy the World With AI. https://www.youtube.com/watch?
v=ISkAkiAkK7A.
Russell, S. 2024. AI: What If We Succeed. https://www.youtube.com/watch?v=UvvdFZkhhqE.
Schulte, O. 2023. Formal Learning Theory. In Zalta, E. N., and Nodelman, U., eds., The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2023 edition.
Scott, R. F. 1912. Sledging diary (’Vol. III’); last entry, 29 March, 1912. https://www.paulrose.
org/news/2012/02/captain-scotts-diaries.
Searle, J. R. 1980. Minds, brains, and programs. Behavioral and Brain Sciences 3(3):417–424.
Shafer, G. 1976. A Mathematical Theory of Evidence. Princeton University Press.
Shalev-Shwartz, S., and Ben-David, S. 2014. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press.
Shoenfield, J. R. 1967. Mathematical Logic. Association for Symbolic Logic.
Stone, J. V. 2013. Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis. Sebtel Presss.
Sutton, R. S., and Barto, A. G. 2018. Reinforcement Learning. MIT Press.
Tegmark, M., and Omohundro, S. 2023. Provably safe systems: the only path to controllable AGI.
Templeton, A.; Conerly, T.; Marcus, J.; Lindsey, J.; Bricken, T.; Chen, B.; Pearce, A.; Citro, C.;
Ameisen, E.; Jones, A.; Cunningham, H.; Turner, N. L.; McDougall, C.; MacDiarmid, M.; Freeman,
C. D.; Sumers, T. R.; Rees, E.; Batson, J.; Jermyn, A.; Carter, S.; Olah, C.; and Henighan, T. 2024.
Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. Transformer
Circuits Thread.
Tsang, E. 1993. Foundations of Constraint Satisfaction. Academic Press.
Turing, A. M. 1950. Computing Machinery and Intelligence. Mind 49:433–460.
Venkatesh, S. S. 2013. The Theory of Probability: Explorations and Applications. Cambridge
University Press.
Walton, D. 2014. Abductive Reasoning. University of Alabama Press.
Wang, P., and Goertzel, B., eds. 2012. Theoretical Foundations of Artificial General Intelligence.
Atlantis Press.
Copyright 2023-2024 BigMother.AI CIC
53

TURNER
Wang, P. 2013. Non-Axiomatic Logic: A Model of Intelligent Reasoning. World Scientific.
Wang, P. 2019. On Defining Artificial Intelligence. Journal of Artificial General Intelligence 10:1–37.
Whipple, J. 2022. Leibniz’s Exoteric Philosophy. In Zalta, E. N., and Nodelman, U., eds., The
Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2022
edition.
Wiener, N. 1960. Some Moral and Technical Consequences of Automation (1960). Science
131:1355–1358.
Wirth, N. 1976. Algorithms + Data Structures = Programs. Prentice-Hall.
Yager, R.; Liu, L.; Dempster, A. P.; and Shafer, G., eds. 2004. Classic Works of the Dempster-Shafer
Theory of Belief Functions. Springer.
Yampolskiy, R. V. 2016. Artificial Superintelligence: A Futuristic Approach. CRC Press.
Yampolskiy, R. V. 2024a. Roman Yampolskiy: Dangers of Superintelligent AI | Lex Fridman Podcast.
https://www.youtube.com/watch?v=NNr6gPelJ3E.
Yampolskiy, R. V. 2024b. AI: Unexplainable, Unpredictable, Uncontrollable. CRC Press.
Yudkowsky, E. 2021. The real core of the argument for ’AGI risk’ (AGI ruin). https://twitter.
com/ESYudkowsky/status/1405580521237745665.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong,
Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie,
J.-Y.; and Wen, J.-R. 2023. A Survey of Large Language Models.
Copyright 2023-2024 BigMother.AI CIC
54

