 
1
There’s Plenty of Room Right Here: 
Biological Systems as Evolved, Overloaded, Multi-scale Machines 
 
Joshua Bongard1,3† and Michael Levin2,3†* 
 
 
1 Department of Computer Science, University of Vermont, Burlington, VT USA 
2 Allen Discovery Center at Tufts University, Medford MA, USA. 
3 Institute for Computer Designed Organisms 
 
†Both authors contributed equally to this work. 
 
* Author for correspondence 
200 Boston Ave., Suite 4600 
 
Medford, MA 02155 
 
Email: michael.levin@tufts.edu  
 
Tel. (617) 627-6161 
 
 
Running title:  Overloaded living machines 
 
Keywords: 
biology, computer science, robot, artificial life, artificial intelligence, machine 
learning, evolution 
 
 
 

 
2
Abstract 
 
 
The applicability of computational models to the biological world is an active topic of 
debate. We argue that a useful path forward results from abandoning hard boundaries between 
categories and adopting an observer-dependent, pragmatic view. Such a view dissolves the 
contingent dichotomies driven by human cognitive biases (e.g., tendency to oversimplify) and 
prior technological limitations in favor of a more continuous, gradualist view necessitated by the 
study of evolution, developmental biology, and intelligent machines. Form and function are 
tightly entwined in nature, and in some cases, in robotics as well. Thus, efforts to re-shape living 
systems for biomedical or bioengineering purposes require prediction and control of their 
function at multiple scales. This is challenging for many reasons, one of which is that living 
systems perform multiple functions in the same place at the same time. We refer to this as 
“polycomputing” – the ability of the same substrate to simultaneously compute different things. 
This ability is an important way in which living things are a kind of computer, but not the familiar, 
linear, deterministic kind; rather, living things are computers in the broad sense of computational 
materials as reported in the rapidly-growing physical computing literature. We argue that an 
observer-centered framework for the computations performed by evolved and designed systems 
will improve the understanding of meso-scale events, as it has already done at quantum and 
relativistic scales. To develop our understanding of how life performs polycomputing, and how it 
can be convinced to alter one or more of those functions, we can create technologies that 
polycompute, and learn how to alter their functions first. Here, we review examples of biological 
and technological polycomputing, and develop the idea that overloading of different functions 
on the same hardware is an important design principle that helps understand and build both 
evolved and designed systems. Learning to hack existing polycomputing substrates, as well as 
evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and 
computer engineering. 
 
 
 

 
3
1. Introduction 
 
In Feynman’s famous lecture titled “There’s Plenty of Room at the Bottom” [1], he argued 
that vast technological progress could be achieved by learning to manipulate matter and energy 
at ever smaller scales. Such potential presumably could be exploited by natural selection as well. 
How does biology expand the adaptive function of an existing system? It can’t go down, since 
there’s already something there, exhibiting functional competencies, at every level [2]. Instead, 
it squeezes more action out of each level by overloading mechanisms with multiple functions – 
which we term polycomputing. We argue that the most effective lens on a wide range of natural 
and engineered systems must enable a multiple-observers view where the same set of events 
can be interpreted as different computations (Figure 1 illustrates how artists have recognized this 
feature). 
Herein we review remarkable examples of biological polycomputing, such as spider webs 
that serve as auditory sensors and prey capture devices [3], and holographic memory storage in 
the brain [4,5]. We will also review emerging examples in computer and materials engineering [6]. 
We provisionally define polycomputing as the ability of a material to provide the results of more 
than one computation in the same place at the same time. To distinguish this from complex 
materials that necessarily produce complex results in the same place at the same time (like the 
multiple peaks in the frequency spectrum of a vibrating material), polycomputing must be 
embodied in a material that has been evolved, or can be designed to produce particular results 
– such as the results of particular mathematical transformations like digital logic – and must be 
readable by other parts of the material or other devices. That is, the computation, to be a 
computation, must be useful. These advances in understanding polycomputing in biology suggest 
ways to improve synthetic polycomputing systems, the latter of which in turn shed light on the 
nature of computation, evolution, and control. Biological systems that polycompute also 
contributes to an ongoing conceptual debate in interdisciplinary science --- the applicability of 
computer frameworks and metaphors to living systems [7] --- in three ways. First: if 
polycomputing changes our understanding of what computation is, that might change whether 
we consider a living system to be a computer (Sect. 1.1). Second: a living system (or inorganic 
material) may be considered to be polycomputing depending on one’s point of view, suggesting 
observer dependence is unavoidable when considering whether or what a living or engineered 
computes (Sect 1.2). Third: increasingly intricate admixtures of technological and biological 
components that compute are forcing a redefinition of life itself (Sect 1.3). 
 

 
4
 
Figure 1: Polycomputing concepts in art. 
(A) Sculpture by Shigeo Fukuda, “Lunch with a helmet on”, 1987 – appears as a random pile of 
knives and forks but when observed in just the right way, light moving through the sculpture 
reveals another pattern (a motorcycle) present at the same time in the same structure. 
(B) A well-known bistable (ambiguous) image, “My Wife and my Mother-in-Law” by British 
cartoonist William Ely Hill in 1915, reveals how our nervous system is not suited to taking in 
multiple meanings – it prefers to squash down to a single interpretation, even if it then has to 
vacillate back and forth. 
 
1.1 What constitutes a computer? The notion of “computer” needs to be expanded: it no longer 
only refers to the sequential, deterministic, silicon-embodied, human-programmed, Von 
Neumann/Turing architectures with which biologists are familiar. Those are indeed not similar to 
living systems. There is now a widening array of computational substrates and robots that are 
often massively parallel (such as GPUs and computational metamaterials [8]), stochastic (hard to 
predict) [9], exploit non-obvious (and potentially not-yet-understood) properties of the exotic 
substrates they are built from [10], emergent, produced by evolutionary techniques [11], and built 
by other machines [12] or programmed by other algorithms [13-15]. The benefit of considering 
biological systems as members of this broader class is that it avails powerful conceptual 
frameworks from computer science to be deployed in biology in a deep way – to understand life 
– far beyond the current limited use in computational biology. Moreover, exploring this powerful 
invariant between natural and synthetic systems can enrich intervention techniques in biology 
and improve the capabilities of engineered devices, and reveal gaps in our understanding and 
capabilities in both computer science and biology. Polycomputing is a powerful but as yet under-
appreciated example of the many ways in which the wider class of computer devices can help 
revolutionize the life sciences. In the same way that organic and inorganic materials acting as 
computers increasingly challenges the claim that living materials are not computers, we have 

 
5
argued elsewhere [13] that the widening array of materials that can now be viewed, or 
engineered with, as machines is corroding the classic claim that living systems are not machines, 
and forcing an improved definition of “machine” that escapes the narrow definitions of past 
decades which are no longer appropriate [14-16]. 
 
1.2 Observer dependency. In the statement “living things are (or are not) computers”, “are” 
implies the existence of an objective, privileged view of both computers and biology that allows 
an unambiguous, universal decision as to whether they are related. This binary view is untenable 
and gives rise to numerous pseudoproblems. We argue instead for an observer-dependent, 
gradualist view in which computational formalisms are just metaphors, but indeed, all scientific 
concepts are just metaphors, with varying degrees of utility (not binary). Once we come to grips 
with the fact that “all models are wrong but some are useful” [16], it is possible to adopt a 
pragmatic approach [17] in which anything is a computer, in a given context, to the degree to 
which it enables some observer to predict and control that thing better than competing 
metaphors allow us to do. On this view, whether something is computing is not a philosophical 
question, but one to be settled experimentally by specifying a computational framework and 
showing empirically what new levels of capability, and what new experiments and research, are 
enabled by adopting that framework. Of course, it is expected that future progress will uncover 
even better ones, so the answer is never final but always provisional and relative to a specific 
perspective. This view is akin both to the intentional stance in philosophy of mind [18], driving the 
development of frameworks and tools from cognitive science deployed broadly across biology 
and the biomedical sciences [2,19,20]. 
 
1.3 What things are alive? Finally, the question of what constitutes a “living thing” is itself 
undergoing a renaissance due to the new chimeric, synthetic, and bioengineering techniques 
being developed [21]. Active matter, synthetic biology, and biohybrids [22-28] are blurring the line 
between evolved and designed systems, and dissolving distinctions between “life” and 
“machine” [29-31], which were easy to maintain when our capabilities did not permit the 
construction and analysis of the full option space of agents [32,33]. At this point, the life sciences 
have expanded well beyond the N=1 example of phylogenetic history here on Earth, to a clear 
mandate to understand life-as-it-can be via synthetic and exobiological exploration [34-40]. 
 
1.4. From a philosophy to a science of how life (poly)computes. We propose that the way to side-
step philosophical debates about whether biological systems “are” computers is to adopt an 
observer-centered, gradualist view of computational formalisms in biology. Polycomputing is an 
ideal example of a linking concept that will enrich both fields, and which enables a number of 
fascinating questions with many fundamental and practical implications. What are key functional 
and control properties of polycomputing systems? How does evolution create systems where 
multiple functions reside in the same hardware, and what does this design principle mean for 
evolvability?  How can we derive intervention policies to make rational changes in existing 
polycomputing systems, and what are efficient paths to the design of novel polycomputing 
materials, control algorithms, and device architectures? 
Regardless of whether or not a living system is distally observed, it still polycomputes 
because life itself adopts the same operator-dependent approach. In other words, a biological 

 
6
mechanism is polycomputing because its functionality and signaling are interpreted in different 
ways by other components of that same living system. Each level and component of a living 
system is simultaneously an observer and hacker, interpreting and taking advantage of different 
aspects of mechanisms in its microenvironment, in parallel. Life polycomputes because it is a set 
of overlapping, competing, cooperating nested dolls each of which is doing the best it can to 
predict and exploit its microenvironment [41-47]. 
 
1.5. Why “life as computation” matters. Transfer of knowledge between the disciplines of biology 
and computation forms a positive feedback loop for increasing insight in both. Biological 
examples help widen the range of implementations for computing devices and provide novel 
ideas for architectures [48-52]; unconventional computing platforms include fungal networks, ant 
colonies, and DNA. In complement, computer science and its ideas of functionalist substrate 
independence (multiple realizability) help biologists focus on essential, not contingent, design 
principles – expanding biology beyond zoology and botany. This has been most prevalent in 
neuroscience [53-55], but more recently has been extended far beyond, in recognition of the fact 
that neural dynamics are only an extension of far older biological problem-solving architectures 
[20,56-58].  
A key conceptual insight from computer science that informs biology concerns the nature 
of computation. For example, the field of physical reservoir computing [59], in which a neural 
network is trained to map dynamics occurring within an inorganic, biological or technological 
system (the “reservoir”) into some output desired by a human observer, helps us see the 
observer-dependent aspect of biology. This offers ways to think about biology as nested societies 
of elements which are exploiting the information-processing capacities of their living 
environment. Cells, parasites, conspecifics, commensal organisms, and evolution itself, are all 
hackers in the sense of using their parts and their neighbors as affordances in whatever way they 
can, rather than in some single, unique, privileged objective way that reflects “true” functionality. 
The concepts of superposition in quantum mechanics and primacy of observer frames in 
relativity transformed understanding of phenomena at, respectively, very small and very large 
scales. Polycomputing challenges us to apply the same concepts to computation and life at meso-
scales. Here, we overview the concepts of superposition and observer frame as they apply to 
meso-scales and argue that the polycomputing lens, like the agential matter lens [60,61], helps us 
understand, predict, and control new classes of evolved and designed materials, with numerous 
applications ranging from regenerative medicine to engineering. 
 
2. Current Debates: dissolving dichotomous thinking 
Whenever technological progress in a particular domain begins to slow, researchers often 
look to nature for fresh inspiration. Examples include photosynthesis for new energy capture 
devices [62] and flapping wings for new drone designs [63]. Following in this tradition, the 
increasing difficulty of packing more compute into microchips [64] challenges us to seek new 
paths forward by considering how computation is embedded in living systems. Comparing how 
organisms and machines compute requires one to view an organism as a kind of machine; 
otherwise, no comparison is possible. The debate about how or whether organisms are machines 
has a long history, and has become more intense in recent years [29-31,56,65-67] as various 
disciplines not only compare life to machines, but attempt to merge the two (reviewed in [32]). 

 
7
Our usage of the term “machine” in what follows will denote that subset of machines 
capable of computation. Such machines include robots and physical computers but exclude 
simple mechanical devices such as combustion engines and flywheels, for which no way to 
stimulate or exploit them to produce computation has yet been invented. (If such interventions 
are discovered, these technologies can then be considered as belonging more to the class of 
computational machines.) In the spirit of our thesis, we acknowledge that there is no clear 
dividing line between these two “types” of machines, as circuitry-free machines like physical 
reservoir computers [59] and computational metamaterials [91] can still compute to some 
degree. As always, there is a continuum: in this case, across machines capable of more or less 
computation. The usage of the term “machine” rather than “computer” in what follows is meant 
to remind the reader that we are considering organisms vis-a-vis human-made things that 
compute, rather than just comparing them to traditional computers. 
 
2.1. Structure function mapping and polycomputing. An obvious starting point for comparing 
organisms and computers, or organisms and machines, is to assume a 1-to-1 mapping between 
structure and function. A comparison can then be attempted between the organism’s and 
machine’s structures, and then between their functions. Finally, one can compare the structure-
to-function mappings of organisms and machines. However, teasing apart structure and function 
for such comparisons is difficult. Genetics [68] and neuroscience [69] both provide historical 
examples of how 1-to-1 structure/function mappings were rapidly replaced by models with 
increasingly dense and non-intuitive interactions between structural and functional units. Even 
harder than making predictions based on this nontrivial structure-to-function mapping is the 
inferring of interventions to make rational changes at the system level, as is needed for example 
for regenerative medicine – replacing complex organs such as hands and eyes [19,20]. Advances 
in other areas where biology and computer science meet are similarly demolishing long held 
dichotomies (Table 1).  
Indeed, an understanding of the wide range of implementations (materials, including 
organic components) and origin stories (e.g., evolutionary design techniques [70]) for machines 
makes it clear that in many cases, a modern machine lens on life facilitates progress. The machine 
metaphor is a functional approach that seeks to develop possible efficient ways to predict, 
control, communicate with, and relate to a system and its reliable behavior modes. However, 
one aspect has lagged, in both engineering and biology. It is relatively easy to see that 
technological or living components can support different functions at the same time but at 
different spatial scales: myosin, for example, supports muscle fiber contraction and legged 
locomotion simultaneously. It is also easy to see how components can support different functions 
at the same spatial scale but at different times: myosin can support legged locomotion and then 
tree climbing. But it can be difficult to see how a component can provide multiple uses to multiple 
beneficiaries (or compute different functions from the vantage point of different observers) at 
the same spatial scale and at the same time. Investigating this last phenomenon---
polycomputing---enables not only a new set of questions for biology, but also a quest for 
engineers to understand how to pack more functionality into the same machine. 
 
 
 

 
8
Assumed distinction Counterexamples 
Software/Hardware 
Physical materials that compute [59] and learn [71]. 
Tape/Machine 
Tape-less von Neumann self replicators [72]. 
Digital/analog 
Evolved digital circuits can exploit electromagnetic properties of 
the circuit’s substrate [11]. 
Machine/Life form 
AI-designed organisms [72,73]. 
Automaton/Free agent 
The intentional stance [18]. 
Brain/Body 
Computational metamaterials [8]. 
Body/Environment 
Other cells are the environment for a cell in a multicellular body. 
Intelligent/Faking it 
AI technologies that seem to pass verbal [74], visual [75], or 
physical [76] Turing tests. 
Created/Evolved 
Artefacts designed by human-created evolutionary algorithms. 
Table 1: Some common assumed distinctions in biology and technology, and recent advances 
that serve as counterexamples, suggesting a spectrum of complementarity. 
 
2.2. Dichotomous thinking in the life sciences. 
 
Biology does not really support dichotomous categories. While it is sometimes convenient 
for biologists to adopt discrete criteria for specific characters, evolution and developmental 
biology both exhibit remarkable gradualism. Neither process supports any kind of clean bright 
line that separates the cognitive human being from the “just physics” of a quiescent oocyte or 
“true grounded knowledge” from the statistically-driven speech behavior of babies and some AIs, 
etc. (Table 1). All of these, like the process of slowly changing a being from a caterpillar to a 
butterfly [41], show that familiar categories in fact represent poles of a spectrum of highly diverse 
mixed properties. The interoperability of life [41,77-79] enables chimeras at all levels of 
organization, which provide a continuum of every possible combination of features from 
supposedly distinct categories (Table 1), making it impossible to objectively classify either natural 
and artificial chimeras [32,80]. It is becoming increasingly apparent that the departmental, 
funding, and publication distinctions between disciplines (e.g., neuroscience and cell biology), 
are much more a practical consequence of our cognitive and logistical limitations than the 
reflection of a deep underlying distinction. In fact, these divisions obscure important invariants – 
symmetries across categories that enable unifications such as the use of cognitive neuroscience 
techniques to understand the collective intelligence of cells during morphogenesis [19,20,81,82] or 
indeed, of physics itself [83,84]. 
 
2.3. Dichotomous thinking in computer science. 
Advances in the computational sciences also increasingly disrespect human-devised 
categorical boundaries. One such boundary under attack is that between body and brain. One 
set of technologies eating away at this distinction is physical computing; a conceptual advance 
doing similarly caustic work is that of morphological computation. In mechanical computing, 
computation is performed without recourse to electronics and instead relies on optical [85], 
mechanical [86], or quantum [87] phenomena. Recent advances in mechanical computing show 
how inert bulk materials can be induced to perform non-trivial calculations including error 
backpropagation, the algorithmic cornerstone of modern AI [71]. The recent demonstration by 

 
9
one of the authors (Bongard) that propagation of acoustic waves through granular metamaterials 
can be exploited to perform multiple Boolean operations in the same place at the same time [8], 
can be considered the first example of mechanical polycomputing. Mechanical computing thus 
challenges the assumption that the brain (or the circuitry) is the thing that computes (or 
polycomputes); while the body (or the robot body) is a separate thing that does not, or cannot, 
compute. 
Morphological computation, a concept originating in the robotics literature, upholds that 
the body of an animal or robot can indeed compute and, moreover, it can “take over” some of 
the computation performed by a nervous system or robot control policy [88,89]. Although 
mechanical computing and morphological computing are similar in spirit, in mechanical 
computing, the bulk material passively accepts whatever computation is forced upon it. In 
contrast, in morphological computation, the animal or robot body is active: it is capable of taking 
on computational responsibilities via evolution or learning. This flow of computation back and 
forth between body and brain (or between digital circuitry and bulk materials) suggests that the 
two human-devised categories of “body” and “brain” should not be as distinct as once thought. 
 
2.4.  Polycomputing in bodies and brains.  
If polycomputing is to be considered a kind of computation, one can then ask whether 
polycomputation can be shuttled back and forth between biological bodies and brains, or can be 
made to do so between machine bodies and brains. For this to work, polycomputation must be 
implementable in different kinds of substrates. Traditional computation is assumed to be 
substrate agnostic: if configured appropriately, any physical material can compute. In contrast, 
only vibrational materials currently seem capable of polycomputing, as polycomputation requires 
storage of the results of multiple computations at the same place and at the same time, but at 
different peaks in the frequency spectrum. This would seem to preclude some materials, like 
digital circuitry and biological nervous systems, from performing polycomputation, since digital 
circuitry traffics in electrons, and nervous systems traffic in chemicals and ions; neither seems to 
traffic in vibration. At first glance, this seems poised to rescue the brain/body distinction via the 
surprising route of suggesting that bodies and brains are different things because bodies 
polycompute but brains do not.  
However, this odd-seeming distinction may be short-lived. It has been shown that 
neurons may communicate mechanically [90] in addition to electrically and chemically. If so, such 
mechanical neural communication may contain vibrational components, suggesting nervous 
systems may be polycomputing as well. If this turns out to be the case, it in turn opens the 
possibility that nervous tissues may have evolved incremental enrichments of non-neural cells’ 
already proven ability to polycompute. This would once again frustrate our attempts to cleave 
body from brain, in this case by the claim that one polycomputes, while the other does not. 
Mechanical computing and morphological computation are closely related to another 
way in which computer science provides useful viewpoints for biology. In CS, the view that an 
algorithm drives (functionally determines) outcomes, even though implemented by the 
microphysics of electron flows through a CPU, is accepted and indeed essential to perform the 
useful and powerful activity of programming. This is in stark contrast to debates in biology and 
neuroscience about whether higher levels of description are merely epiphenomenal [91-95], 
supervening on biochemical microstates (reductionism). Computer science clearly shows how 

 
10
taking a causal stance at higher levels enables progress. Indeed, the recent advances in 
information theory around quantifying causal emergence [95,96] show how the same Boolean 
network can be computing different functions simultaneously (Figure 2), depending on the level 
of analysis chosen by an observer [95]. This has interesting biological implications, since such 
networks are a popular model for understanding functional relationships between genes [97-99]. 
 
Biological nervous systems - the human brain in particular - have attracted increasingly 
computational metaphors throughout the industrial revolution and information age. The 
application of computational concepts to brains has had unintended consequences, most of all 
the implicit assumption that tissues, cells, and other biological systems that are not brains do not 
compute. However, the brain:body dichotomy is increasingly being dismantled by studies of basal 
cognition (i.e., intelligence in unfamiliar embodiments) in plants [100,101], single somatic cells [102-
104], microbes [105-108], and at the tissue level in organisms [109-112]. Indeed, the bioelectric and 
neurotransmitter dynamics that implement predictive processing and other computations in 
brains are speed-optimized versions of extremely ancient bioelectrical computations that 
navigated spaces (such as anatomical morphospace, physiological space, etc.) long before brains 
and muscles appeared [58,113,114]. Indeed, the tools of neuroscience – from conceptual 
approaches such as active inference [20,82] to the molecular tools like optogenetics [115-118] – do 
not distinguish between neurons and non-neural contexts, being applicable broadly across 
biology. 
The benefit of dissolving these arbitrary distinctions is that commonalities and 
fundamental design principles across substrates are beginning to emerge across evolved and 
designed constructs at all scales [32,103,119].  Frameworks that are to survive the next decades, in 
which technological advancement will further enmesh biology and technology, must facilitate 
experimental progress at the expense of philosophical preconceptions. More than that, they 
must provide unifying insight by identifying symmetry and deep order across fields, to combat 
the ever-growing problems of big data and the interpretability crisis [120,121]. Here, we delve into 
one emerging principle: polycomputing, which places front and center the fascinating issues of 
form, function, control, interpretation, and the role of the observer. 
 

 
11
 
Figure 2: Spatial causal emergence (counteracting degeneracy).  
(A) A degenerate network with deterministic AND gates.  
(B) The cycle of AND gates is mapped onto a cycle of COPY gates at the macro level.  
(C) The deterministic but degenerate micro TPM.  
(D) The deterministic macro TPM with zero degeneracy.  
By eliminating degeneracy and achieving perfect effectiveness, the macro scale beats the micro 
scale in terms of actionable information content (causal emergence = 0.57 bits). This system is 
simultaneously computing a set of ANDs and, at a macroscale, a set of COPY gates as well, 
depending on the level of observation. Taken with permission from [95]. 
 
3. Learning from superposed systems in engineering 
The ability to compute and store multiple results in the same place at the same time is of 
obvious engineering interest as it could greatly increase computational density. Various 
technologies are now being built that roll back the assumption that such superposition is 
impossible. These technologies, reviewed below, suggest ways to look for similar phenomena in 
natural systems. 
Quantum computing has made clear that multiple computations can be performed 
simultaneously. But practical and general-purpose quantum computing remains a distant 
prospect. Recently, one of the authors (Bongard) has shown that quantum effects are not 
necessary for polycomputing [8]: Even relatively simple materials composed of only 30 parts are 
capable of computing two logical functions in the same place at the same time. This non-quantum 
form of computational superposition suggests not only that more computation may be packed 

 
12
into smaller spaces, but also that the fundamental way in which computation arises in 
technological and biological materials may need to be rethought. 
 
Holographic data storage (HDS; [122]) is another set of related technologies that do not 
assume only one datum or computational result is stored locally. HDS stores and reads data that 
has been dispersed across the storage medium. It does so by etching a distributed representation 
of a datum across the storage medium, for example with laser light, from a particular direction. 
That datum can then be retrieved by capturing the reflection of light cast from the same 
direction. By storing data in this way from multiple directions, parts of multiple pieces of data are 
stored in the same place, but accessed at different times. Exactly how this can be achieved in 
hardware such that it affords appreciable increases in storage density over current traditional 
approaches has yet to be resolved. 
A third technology relaxing the assumption of data/compute locality is physical reservoir 
computing (PRC). PRC, inspired by HDS, attempts to retrieve the results of desired computations 
by exciting inert bulk materials, such as metal plates or photonic crystals, and capturing the 
resulting vibrations or refracted light, respectively. Different computations can be extracted from 
the same material by exciting it in different ways. An attempt to “program” PRCs, thus easing the 
ability to extract desired computation from them, has also been reported [123]. Notably, this 
method has been used to create “deep physical neural networks” [71]: the input, and parameters 
describing an artificial neural network, are combined into forces supplied to the material. Forces 
captured back from the material are interpreted as if the input had been passed through a neural 
network with those parameters. Errors in the output can then be used to modulate the input, 
and the process repeats until a set of input forces has been found that produces the desired 
output. Importantly, the internal structure of the bulk material is not changed during this training 
process. This means that the same material can embody different computations. Just how 
distributed or localized these computations are within these materials remains to be seen. 
Other materials are not only changed by forces acting on them, but retain an imprint of 
those forces even after they cease; they are capable of memory. Efforts are also underway to 
design such materials to maximize the number of overlapping memories that they can store 
[124,125]. The ability of designed materials to absorb forces, compute with them, and output 
transformed forces encoding the results of those computations holds great promise for robotics. 
If future robots can be built from such materials, force propagation within them could 
simultaneously produce external behavior and internal cogitation, without requiring distinct 
behavior-generating components (the body) and computation-generating components (the 
brain). Indeed, soft robots are already demonstrating how exotic materials enable the 
traditionally distinct functions of sensation, actuation, computation, power storage, and power 
generation to be performed simultaneously by the same parts of the robot’s body [126]. 
 
4. Biology is massively overloaded: polycomputing 
Analyzing natural systems to determine whether or how they perform polycomputation 
is particularly challenging, as most analytic approaches are reductionist: they “reduce” to 
characterizing one phenomenon that arises in one place, at one time, under one set of 
circumstances. Synthesis is also difficult: polycomputable technologies seem, to date, resistant 
to traditional engineering design principles such as hierarchy and modularity. A fundamental 
problem is that typical human designs are highly constrained, such that changes made to 

 
13
optimize one function often interfere with another. Although humans struggle to manually 
design polycomputing technologies, it turns out that AI methods can do so, at least in one 
domain. We have recently applied an evolutionary algorithm—a type of AI search method—to 
automatically design a granular metamaterial that polycomputes. It does so by combining 
vibrations at different frequencies at its inputs, and providing different computations in the same 
place, at the same time, at different frequencies. Figure 3 illustrates this process. 
 
Many biological functions have been usefully analyzed as computations [56] (Table 2). 
These include molecular pathways [124,127], individual protein molecules [57], cytoskeletal 
elements [125,128,129], calcium signaling [130], and many others. Although differing from the most 
familiar, 
traditional 
algorithms, 
the 
massively 
parallel, 
stochastic 
(indeterministic), 
evolutionarily-shaped information processing of life is well within the broad umbrella of 
computations familiar to workers in the information sciences. Indeed, information science tools 
have been used to understand cell- and tissue-level decision-making, including estimations of 
uncertainty  [104,131-141], analog/digital dynamics [142], and distributed computation [100]. 
Bioelectric networks within non-neural tissues, just like their neural counterparts, have shown 
properties very amenable to polycomputation, including ability to store diverse pattern 
memories that help execute morphogenesis on multiple scales simultaneously [20,22,112,143-149], 
and enables the same genome to produce multiple diverse outcomes [150].  
A key aspect to recognizing unconventional computing in biology is that the notion of 
“what is this system really computing” has to be dropped (because of the multiple observers 
issue described above). Once we do this, biology is rife with polycomputing at all scales.  Examples 
include the storage of a (possibly uncountable) number of memories in the same neuronal real-
estate of the brain [151,152], and many others summarized in Table 2. We do not yet know 
whether the prevalence of polycomputing is because of efficiency, robustness, or other gains that 
override the evolutionary difficulty of finding such solutions. Or, perhaps we overestimate the 
difficulty, and evolution has no problem identifying such solutions – they may indeed be the 
default. If so, it may be because of the generative, problem-solving nature of developmental 
physiology as the layer between the genotype and the phenotype [32,153]. For these reasons, 
polycomputing may join degeneracy and redundancy [154] as one of the organizing principles that 
underlies the open-ended, robust nature of living systems. 
 
 

 
14
 
Figure 3: Engineering polycomputing materials.  
(A) A granular metamaterial can be assembled by embedding stiff (dark gray) and soft (light gray) 
particles into a sheet and wrapping it into a tub. If a particle collides with soft particles, it only 
affects their motion a bit.  
(B) if it hits rigid particles, their motion is affected more.  
(D) An evolutionary algorithm (EA) can be created that evolves populations of metamaterials, 
where each one has a unique combination of stiff and soft particles. The EA can then delete those 
metamaterials that perform poorly at some desired task, such as performing a computation, and 
make randomly modified copies of those that do a better job.  
(D) This can result in the evolution of a material that acts as an AND gates, a building block of 
computers: some designated ‘output’ particle (red) should only vibrate if two other ‘input’ 
particles are vibrated from outside the system (green and blue).  
(E) An evolutionary algorithm can be asked to evolve a metamaterial that acts as an AND at one 
frequency, but to act as another computational building block, an XOR gate, at a higher 
frequency: the output particle should only vibrate if one of the input particles is vibrated.  
(F) This process results in the evolution of a polycomputing material: if inputs are supplied at two 
different frequencies, the evolved material acts as an AND and XOR gate simultaneously: it 
provides the results of these two computations at the same place at the same time (the output 
particle), but at different frequencies. Details can be found in [8]. 
 
 
 
 

 
15
Table 2:  Examples of biological polycomputing at diverse scales 
Multiple Computations in the Same Biological Hardware 
Reference 
Mitochondria also act as micro-lenses in photoreceptors 
[155] 
Proteins acting in multiple (fluctuating) conformations 
[156] 
Gene regulatory networks with multiple memories/behaviors 
[157-160] 
Chemical networks performing neural network tasks 
[161,162] 
RNA encoding enzyme and protein functions 
[163-166] 
DNA with more than one active reading frame (overlapping genes) 
[167,168] 
Ion channels that are also transcription factors 
[169] 
DNA transcription factors working in DNA replication machinery 
[170] 
Polysemanticity and superposition of meaning in neural networks and language 
understanding 
[171-173] 
Cytoskeleton performing computations via simultaneous biomechanical, 
bioelectrical, and quantum-mechanical dynamics 
[174-183] 
Electrophysiological networks performing memory functions while regulating 
heartbeat 
[184] 
[185,186] 
Bioelectric networks performing physiological functions while also regulating 
morphogenesis 
[113] 
Spider webs as auditory sensors and structural elements 
[3] 
Pleiotropy – most genes have multiple functions 
[68] 
Holographic memory in the brain 
[187] 
Multiple behaviors in the same neuronal circuit 
[188] 
Multiple personalities in the same brain (dissociative identity disorder and split 
brain studies) 
[189,190] 
Calcium dynamics performing as a hub in a huge bowtie network of diverse 
simultaneous processes 
[191,192] 
 
4.1. Evolutionary pivots: origins of polycomputing? Evolution is remarkably good at finding new 
uses for existing hardware because of its fundamental ability to generate novelty to exploit new 
niches while being conservative in terms of building upon what already exists. This ability to 
simultaneously innovate and conserve plays out across structural, regulatory, and computational 
domains. Moreover, re-use of the same conserved mechanisms in situ enabled evolution to pivot 
successful algorithms (policies) from solving problems in metabolic space to solving them in 
physiological, transcriptional, and anatomical (morphospace) spaces, and finally, once muscles 
and nerves came on the scene, to 3D behavioral spaces [193]). For example [58], the same ion 
channels that are used for physiological control of cell homeostasis and metabolism are used 
simultaneously in large-scale bioelectric circuits that compute the direction of adaptive changes 
in growth and form in embryogenesis, metamorphosis, regeneration, and cancer suppression 
[58,108,113,194-198]. Indeed, in some animals such as planaria and axolotl, this is all happening at 
the same time as these exact same mechanisms in neural cells are guiding behavior [199]. Biology 
is rife with polycomputing because it uses a multi-scale competency architecture – every level of 
organization is competent to solve certain problems in its own space and is doing so at the same 
time via the same physical medium [2,200] (Figure 4). 

 
16
 
Figure 4: Polycomputing architectures in biology 
(A) Gael McGill’s computer model of the inside of a cell, combining all of the known components 
that occupy the same space. This illustrates the pressure on biology to have each component do 
multiple duty (there is not much room to add additional components). Used with permission. 
(B) Multiscale competency architecture of life consists of molecular networks which make up 
cells which make up tissues which make up organs which make up organisms within swarms. 
Each layer is performing specific functions simultaneously; for example, the tissue layer is 
attempting to compute the correct attractor for the collective morphogenetic behavior of 
planarian fragment cells, which can build one of several head shapes). Each layer deforms the 
action landscape for the layer below it, providing incentives and shaping geodesics that force the 
lower-level components to use their behaviors in service of the higher level’s goals. Taken with 
permission from [200]. Images by Jeremy Guay of Peregrine Creative Inc. and Alexis Pietak. 

 
17
Polycomputing is seen even at the lowest scale of molecular biological information. It has 
long been known that genomes are massively over-loaded, providing polycomputing not only 
because of multiple reading frames (overlapping genes) for some loci [167,168], but also because 
the question of “what is this gene for?” may have a clear answer at the molecular scale of a 
protein, but often has no unique answer at the phenotypic scale because complex traits are 
implemented by many genes, and many (or most? [68]) genes contribute to multiple mesoscale 
capabilities. Moreover, epigenetics enables the same genomic information to facilitate the 
embodied computation that results in multiple different anatomical, physiological, and 
behavioral forms [201,202].  
 
4.2. Polycomputing and the range of phenotypic possibility. How much actionable information on 
how to assemble an adaptive, behaving organism can be packed into the same genomic and 
physiological information medium? A recent example of massive phenotypic plasticity are the 
Xenobots – proto-organisms that result from a rebooting of multicellularity by frog skin cells 
[73,203]. The Xenobots self-assemble as spheroids that are self-motile, exhibiting a range of 
autonomous behaviors, including highly novel ones such as kinematic self-replication: the ability, 
of which Von Neumann famously dreamed, to assemble copies of themselves from material 
found in their environment [72]; in the case of Xenobots, dissociated cells introduced into their 
surroundings. A key point is that Xenobots are not genetically modified – their novel functionality 
is implemented by perfectly standard frog cells. So, what did evolution learn [204-208] in crafting 
the Xenopus laevis genome and the frog egg’s cytoplasmic complement? It was not just how to 
make a frog; it is how to make a system in which cells allow themselves to be coerced (by other 
cells) into a boring, 2-dimensional life as the animal’s outer skin surface, or, when on their own, 
to assemble into a basal 3-dmensional creature that autonomously explores its environment and 
has many other capabilities (Figure 5). 
 This capacity to do things that were not specifically selected for [209] and do not exist 
elsewhere in their (or others’) phylogenetic history reveals that evolution can not only create 
seeds for machines that do multiple things, but ones that can do novel things. This is because 
genomic information is overloaded by physiological interpretation machinery (internal observer 
modules): the exact same DNA sequence can be used to build a tadpole or a Xenobot (and the 
same planarian genome can build the heads of several different species [210,211]).  Thus, 
evolution teaches us about powerful polycomputing strategies because it does not make 
solutions for specific problems – it creates generic problem-solving machines, in which the 
competition and cooperation of overlapping, nested computational agents at all levels exploit 
the ability of existing hardware to carry out numerous functions simultaneously. This is closely 
tied to recent advances at the interface of the fields of developmental biology and primitive 
cognition, with the generation of models in which larger-scale Selves (in psychological, 
anatomical, etc. spaces) arise as composite systems made of smaller Selves, all of which are 
pursuing diverse agendas [199,212,213]. 
 
As surprising as these examples are, we should have already seen this coming – biology 
had to work like that, and it could not work otherwise. First, the “sim-to-real gap” (the difference 
an agent experiences when trained in virtual environments, built as a robot, and deployed into a 
real environment [214,215]) is as real for biology as it is for robotics: prior evolutionary experience 
in a past environment is not a reliable guide to the novel challenges each generation experiences  

 
18
 
Figure 5: What does the Xenopus laevis genome specify? 
(A) The standard Xenopus laevis genome (in a frog egg) typically causes the construction of a set 
of standard embryonic stages (Path A) which results in tadpoles with specific behaviors. However, 
in a different context, the skin cells can autonomously create a Xenobot (Path B) – a spherical 
construct with autonomous motion, a different morphogenetic sequence, and behaviors such as 
kinematic self-replication. The same genomic information contains simultaneously seeds of 
emergent tadpoles or Xenobots.  
(B) Similar to the iconic cover image of the classic book Godel, Escher Bach [245], this image 
illustrates how the same hardware (the standard frog egg in the middle) can be used to generate 
diverse forms of living constructs. Different environments, external signals, and physiological 
events can coax diverse morphogenetic outcomes out of a constant biological information string 
(DNA). Images in A courtesy of Xenbase and Douglas Blackiston, Levin lab. Image in B by Jeremy 
Guay of Peregrine Creative. 

 
19
in new environments. Thus evolution does not overtrain on prior examples, but generalizes, 
producing substrates that can compute different functions for different needs (Figure 6). Second, 
the evolutionary process is not working with a blank slate – the canvas of embryogenesis is made 
of cells, which used to be independent organisms. Evolution exploits the competencies of cells in 
numerous problem spaces as a toolkit of affordances to be exploited. Development is akin to 
behavior-shaping, where evolution finds signals that cells can send to other cells to push them 
into specific actions. This is a strong start for polycomputing as a design principle – working with 
an agential material [60] requires strategies that don't establish a single, privileged new way of 
doing things but instead drive adaptive outcomes by managing the many things that the material 
is already doing. Third, evolution simply wouldn’t work well with an architecture that didn’t 
support polycomputing, because each new evolutionary experiment would wreck prior gains, 
even if it itself was an advance.  
 
4.3. Evolving polycomputing. The rate of evolution would be much slower without this multiscale 
competency architecture – the ability of parts to get their job accomplished even if circumstances 
change [216]. In one remarkable example, the tadpole eye, placed in the wrong position on the 
head or even on the tail [217] still provides vision, because the eye primordia cells can make an 
eye in aberrant locations, move it if possible [218] and if not, connect to the spinal cord (rather 
than directly to the brain), providing visual signals that way [219,220]. This competency of the 
substrate in regulative development and remodeling [32,200] can neutralize the lethal side effects 
of many mutations, enabling exploration of other possibly beneficial effects. For example, 
consider a mutation that causes displacement of the mouth and also another effect, E, elsewhere 
in the body. The potential benefits of E might never be explored in a monocomputational 
developmental architecture because the mouth defect would prevent the animal from eating 
and drive fitness to 0. Exploration of the effect of E would have to wait for another mutation to 
appear that produces the same effect without untoward side effects elsewhere – a very long 
wait, and often altogether impossible. In contrast, in a polycomputing architecture, structures 
solve morphological and physiological problems simultaneously: the mouth will move to the right 
location on its own [218], in parallel to all of the other developmental events, enabling evolution 
to explore the consequences of E. Thus, the overloaded competencies of cells and tissues allow 
evolution to simultaneously explore other effects of those mutations on phenotype (pleiotropy).  
In this case, these competencies create the “hidden layer” of developmental physiology 
that sits between genomic inputs and phenotypic outputs and provides problem-solving capacity: 
getting an adaptive task completed despite changes in the microenvironment or in their own 
parts [2,200]. This occurs simultaneously at all scales of organization (Figure 4) and thus each level 
computes specific functions not only in its own problem space, but also participates in the higher 
level’s space (as a component) and has an influence that deforms the action space of its lower 
levels’ components [200]. By behavior-shaping competent subunits as agential materials [221], 
evolution produces modules that make use of each other’s outputs in parallel, virtually 
guaranteeing that the same processes are exploited as different “functions” by other 
components of the cell, the body, and the swarm. 
 
The evolutionary pressure to make existing materials do multiple duty is immense. 
However, much remains to be learned about how such pressure brings about polycomputing: 
how some materials can be overloaded with new functions without negatively impacting existing  

 
20
 
Figure 6: Morphogenesis plays the hand it is dealt 
The essence of biological morphogenesis is that it does not assume much about current 
circumstances and attempts to create a viable organism with whatever is at-hand. Thus, frog 
embryos in which eye primordia cells are moved from the head to the tail still make good eyes 
(A), try to connect to the spinal cord (A’, red stain), and enable the organism to exhibit behavioral 
vision (A”) despite a completely novel visual system-brain architecture which had no evolutionary 
prep time to get used to the new arrangement – nothing needed to be changed (at the DNA level) 
to make this new configuration workable. Similarly, tadpoles (B) which must rearrange their face 
to turn into a frog (B’) can still do so even if everything is artificially placed in a scrambled 
configuration (B”) because each organ is able to move as needed to get its job done (reach a 
specific region of morphospace). Finally, the cross-level nature of this overloading of basic 
mechanisms is seen in newt kidney tubules schematized here in cross-section. While they 
normally consist of 8-10 cells that communicate to make a tubule, the cells can be made 
experimentally very large – in that case, fewer cells will work together to make the same size 
tubule (C’). In the case of enormous cells, a completely different mechanism (cytoskeletal 
bending) will be used by a single cell to create a lumen – showing how the same machine 
(genome+cell) can enable high-level anatomical goals to trigger diverse low-level molecular 
mechanisms as needed. Panels A,A’,A”,B courtesy of Douglas Blackiston, used with permission 
after [217] and [219]. Panel B’ courtesy of Erin Switzer, B” taken with permission from [116]. Panels 
C,C’,C” by Jeremy Guay of Peregrine Creative. 

 
21
functions. In parallel to such biological investigations, in the computer science domain, much 
work remains to be done to devise optimization pressures that create polycomputing substrates, 
and then create new programming strategies suitable for polycomputing. For example, no 
programming language has yet been devised to truly take advantage of the polycomputational 
metamaterials described above. Despite our ignorance about how evolutionary or optimization 
pressures can create polycomputational systems, what is clear is that evolution would not work 
at all if living things were not machines – predictable, tractable systems. The key aspects of 
machines are that they harness the laws of physics, computation, etc. in a reliable, rational 
manner to produce specific, useful outcomes. Evolution exploits the fact that life is a machine by 
making changes to the material, the control algorithm, and indirectly, to their environment, in a 
way that gives rise to predictable, adaptive outcomes. Cells could not influence each other during 
development to reliably achieve the needed target morphologies if they could not be efficiently 
controlled. Life shows us the true power of the “machine”: a powerful multi-scale polycomputing 
architecture, in which machines control and comprise other machines, all working at the same 
time in different spaces, produces massive amounts of plasticity, robustness, and novelty.  
 
4.4 A new approach to identifying and harnessing computational capabilities in vivo and in silico. 
One way to exploit this property is to use protocols that examine a particular mechanism for 
novel things it can do, and for the best way to get it to execute some of its capabilities. At the 
molecular level, an example is gene regulatory networks (GRNs) – a formalism whereby a set of 
genes up- and down-regulate each other’s function [222,223]. While GRNs and protein pathways 
are normally studied for ways to explain a particular aspect of biology (e.g., neural crest tissue 
formation or axial patterning in development [224,225]), we asked whether existing neural 
network models could have novel computational functions, specifically learning. Our algorithm 
took biological GRN models and for each one, examined each possible choice of triplets of nodes 
as candidates for conditioned and unconditioned stimuli and response, as per Pavlovian classical 
associative learning [226]. We found numerous examples of learning capacity in biological 
networks and many fewer in control random networks, suggesting that evolution enriches for 
this property [157]. Most strikingly, the same networks offered multiple different types of memory 
and computations, depending on which of the network’s nodes the observer took as their control 
knobs and salient readout in the training paradigm. This approach is an example of searching not 
for ways to re-wire the causal architecture of the system for a desired function, but searching 
instead for a functional perspective from which an un-modified system already embodies novel 
functions.  
This illustrates an important principle of biological polycomputing: evolution prepared a 
computational affordance (the GRN) with multiple interfaces (different gene targets) through 
which engineers, neighboring cells, or parasites can manipulate the system to benefit from its 
computational capabilities. We suggest that this kind of approach may be an important way to 
understand biological evolution: as a search for ways in which body components can adaptively 
exploit other body components as features of their environment – a search for optimal 
perspectives and ways to use existing interfaces. At the organism level, an excellent example is 
the brain, in which an immense number of functions are occurring simultaneously. Interestingly, 
it has been suggested that the ability to store multiple memories in the same neuronal real-estate 
is implemented by phase [5]. 

 
22
The results of our probing neural networks for novel functions also suggest that alongside 
tools for predicting ways to rewire living systems [227-229], we should be developing tools to 
identify optimal perspectives with which to view and exploit existing polycomputing capacities. 
 
5. Conceptual transitions 
 
 
To develop such tools, we will need to overcome human cognitive bias and resist the 
temptation to cleave phenomena apart in ways that feel comfortable. One approach is to look 
for particularly non-intuitive phenomena that defy our attempted categories. Better yet is to seek 
gradients along which we can move from “obvious” approximations of phenomena to 
increasingly “non-obvious”, but more accurate, reflections of reality. 
 
5.1. Directions of conceptual travel. 
 
One such gradient is the one that leads from serial to parallel to superposed processes. 
The industrial revolution demonstrated the advantage of performing tasks in parallel rather than 
serially; the computer age similarly demonstrated the power of parallel over serial computation. 
One reason for these slow transitions may be cognitive limitations: Despite the massive 
parallelism in the human brain, human thinking seems to proceed mostly, or perhaps completely 
[230], in a serial fashion. “Traditional” parallelism, as it is usually understood, assumes that 
multiple processes are coincident in time but not in space. Even more difficult a concept to grasp 
is that of superposition: the performance of multiple functions in the same place at the same 
time. 
 
Another conceptual direction that leads from obvious into non-obvious territories is that 
which leads from modular processes into non-modular ones. The cardinal rule in engineering in 
general, and software engineering in particular, is modular design. But, this is a concession to 
human cognitive limits, not necessarily “the best way to do things”: many natural phenomena 
are continua. Taking another step, if we consider biological or technological polycomputing 
systems, we might ask whether they are modular. But if a system polycomputes, different 
observers may see different subsets of functions and, some may be more modular than others. 
In that case, the question of whether a given polycomputing biological system (or bioinspired 
technology) is more or less modular becomes ill-defined.  We argue that to facilitate future 
research, these classical distinctions must now be abandoned (at least in their original forms). 
 
5.2. Practical implications for AI/robotics. Learning how biological systems polycompute, and 
building that learning into technology, is worth doing for several practical reasons. First, creating 
more computationally dense AI technologies or robots may enable them to act intelligently and 
thus do useful, complex work using fewer physical materials and thus creating less waste. Second, 
technological components that polycompute may be more compatible with naturally 
polycomputing biological components, facilitating the creation of biohybrids.  Third, creating 
machines that perform multiple computations in the same place at the same time may lead to 
machines that perform different functions in different domains – sensing, acting, computing, 
storing energy, and releasing energy – in the same place at the same time, leading to new kinds 
of robots. Fourth, polycomputing may provide a new solution to catastrophic interference, a 
ubiquitous problem in AI and robotics in which an agent can only learn something new at the 
cost of forgetting something it has already learned. A polycomputing agent might learn and store 

 
23
a new behavior at an underutilized place on the frequency spectrum of its metamaterial “brain” 
better than a polycomputing-incapable agent that must learn and incorporate the same behavior 
into its already-trained neural network controller. Such ability would be the neural network 
analogue of cognitive radio technologies, which constantly seek underutilized frequency bands 
from which to broadcast [231]. 
 
6. Gradual computing in biology: when does the (digital) soul enter the (analog) body? 
 
The importance of continuous models (and the futility of some binary categories) is 
readily apparent when tracking the gradual emergence of specific features that we normally 
identify in their completed state.  Examples include pseudoproblems like “when does a human 
baby become sentient during embryogenesis”, “when does a cyborg become a machine vs. 
organism?”, and “when does a machine become a robot?”; all of these force arbitrary lines to be 
chosen that are not backed up by discrete transitions.  Developmental biology and evolution both 
force us to consider gradual, slow changes as essential to the nature of important aspects of 
structure and function.  This biological gradualism has strong parallels in computer science. An 
unfertilized human oocyte, mostly amenable to the “chemistry and physics” lens, eventually 
transforms into a complex being for whom behavioral and cognitive (and psychotherapeutic) 
lenses are required.  What does the boot-up of a biologically-embodied intelligence consist of? 
What are the first thoughts of a slowly developing nervous system? One key aspect of this 
transition process is that it involves polycomputing, as structural and physiological functions 
become progressively harnessed toward new, additional tasks for navigating behavioral spaces 
in addition to their prior roles in metabolic, physiological, and other spaces [200]. 
Similarly, one can zoom into the boot-up process when a dynamical system consisting of 
electrical components becomes a computer. During the first few microseconds when the power 
is first turned on, the system becomes increasingly more amenable to computational formalisms 
in addition to the electrodynamics lens. The maturation of the process consists of a dynamical 
mode which can profitably be modeled as “following an algorithm” (taking instructions off a stack 
and executing them). Similarly, one could observe externally supplied vibrations spreading 
through a metamaterial and consider when it makes sense to interpret the material’s response 
as a computation, or the running of an algorithm. In essence, the transition from an analog device 
to a computer is really just a shift in the relative payoffs for two different formalisms from the 
perspective of the observer. These are readily missed, and an observer that failed to catch the 
ripening of the computational lens during this process would be a poor coder indeed, relegated 
to interacting with the machine via Maxwell’s laws guiding electron motion and atomic force 
microscopy, vs. exploiting the incredibly rich set of higher-level interfaces that computers afford. 
 
6.1. Agency and persuadability: implication for polycomputing. One of the most important next 
steps, beyond recognizing the degree to which certain dynamical systems or physical materials 
can be profitably seen as computational systems, is to observe and exploit the right degree of 
agency. Systems vary widely along a spectrum of persuadability [2] – the range of techniques 
suitable for interacting with them, including physical rewiring, setpoint modification, training, 
and language-based reasoning. Animals are often good at detecting agency in their environment, 
and for humans, theory of mind is an essential aspect of individual behavior and social culture. 
Consistent with the obvious utility of recognizing agency in potential interaction partners, 

 
24
evolution primed our cognitive systems to attribute the intentional stance quite readily [232,233]. 
Crucially, making mistakes by overestimating agency (anthropomorphizing) is no worse than 
underestimating agency – both reduce the effectiveness of adaptive interactions with the agent’s 
world. 
 
6.2. The impact of observer frames. So far we have considered a single human observer of a 
biological or technological system, how much agency she detects in the system from her 
perspective, and how she uses that knowledge to choose how to persuade it to do something. 
However, a biological system may have many observers (neighboring cells, tissues, conspecifics, 
parasites) trying to “persuade” it to do different things, at the same time. (Scare quotes here 
remind us that we must in turn decide to adopt the intentional stance for each of the observers.) 
A polycomputing system may be capable of acceding to all of these requests simultaneously. As 
a simple example, an organism may provide a computational result to one observer while also 
providing waste heat produced by that computation to a cold parasite. Traditional computers are 
not capable of this, or at least are not designed to do so; but future polycomputational machines 
might be. 
 
6.3. Becoming a computer 
For many outside the computational sciences, “computer” denotes the typical physical 
machines in our everyday lives, such as laptops and smartphones. Turing, however, provided a 
formal definition for computers that is device-independent: in summary, a system is a computer 
if it has an internal state, if it can read information from the environment in some way, update 
its behavior based on what it read and its current state, and (optionally) write information back 
out into the world. This theoretical construct has become known as a Turing Machine; any 
physical system that embodies it, including organisms, is formally referred to as a computer. This 
broad definition admits a wide range of actors that do not seem like computers, including 
consortia of crabs [234], slime molds [235], fluids [236], and even algorithms running inside other 
computers [237]. For all of these unconventional computers, as well as for the novel mechanical 
computing substrates discussed above, it is difficult to tell at which point they transition from 
“just physical materials” into computers. With continuous dynamical systems such as these, 
observers may choose different views from which the system appears to be acting more or less 
like a physical instantiation of a Turing Machine. 
 
Even if an observed system seems to be behaving as if it is a Turing Machine, identifying 
the components of that machine, such as the tape or the read/write head, can be difficult. This 
is a common reason why it is often claimed that organisms are not machines/computers 
[67,238,239]. Consider an example from the authors’ own recent work [72]. We found that motile 
multicellular assemblies can “build” other motile assemblies from loose cells. This looks very 
much like von Neumann machines: theoretical machines that can construct copies of themselves 
from materials in their environment. von Neumann initially proved the possibility of such 
machines by constructing, mathematically, Turing Machines that built copies of themselves by 
referring to and altering an internal tape. However, in the biological Turing Machines we 
observed, there seems to be no tape. If there is one, it is not likely localized in space and time. 
 
This difficulty in identifying whether something is a computer, or at what point it becomes 
one, is further frustrated by the fact that biological and non-biological systems change over time: 

 
25
even if one holds one view constant, the system, as it changes, may seem to act more or less like 
a computer. Finally, a polycomputing system, because it can provide different computational 
results to different observers simultaneously, may at the same time present as different 
computers --- better or worse ones, more general or more specialized ones --- to those observers. 
Such behavior would not only foil the question “Is that a computer?”, but would even foil any 
attempts to determine the time at which a system becomes a computer, or begins to act more 
like a computer. Zooming out, it seems that as more advanced technology is created, and our 
understanding of biological systems progresses, attempts to attribute any singular cognitive self 
to a given system will become increasingly untenable. Instead, we will be forced, by our own 
engineering and science, to admit that many systems of interest house multiple selves, with more 
or less computational and agential potential, not just at different size scales, but also 
superimposed upon one another within any localized part of the system. 
 
 As Hoel points out about the ad-hoc status of claiming one single privileged Perspective 
within a system according to the Integrated Information Theory (IIT) account of consciousness 
[240]: 
“…There are so many viable scales of description, including computations, and all 
have some degree of integrated information. So, the exclusion postulate is 
necessary to get a definite singular consciousness. This ends up being the most 
controversial postulate within IIT, however.” [94] 
 
We hold that as our understanding of polycomputing biological and technological systems 
increases, it will eventually exclude the exclusion postulate from any attempt to explain human 
consciousness as some mental module operating within the brain.  
 
7. Conclusion 
Prior skeptical debates about whether biological systems are computers reflect both an 
outdated view of computation and a mistaken belief that there is a single, objective answer. 
Instead, we suggest a view in which computational models are not simply lenses through which 
diverse observers can all understand a given system in the same way, but indeed that several 
computational models can be true of a biological system at the same time. It is now seen that 
there is no one-to-one mapping between biological form and function: the high conservation of 
biological form and function across evolutionary instances implements a kind of multiple 
realizability. At the same time, biological components are massively overloaded toward 
polycomputing. Indeed, their competency, plasticity, and autonomy [2,241-243] may enable a kind 
of second-order polycomputing where various body components attempt to model each other’s 
computational behavior (in effect serving as observers) and act based on their expected reward. 
Thus, modern computer engineering offers metaphors much more suited to understand and 
predict life than prior (linear, absolute) computational frameworks. Not only are biological 
systems a kind of computer (an extremely powerful one), but they are amazing polycomputing 
devices of a depth which has not yet been achieved by technology. In this sense, biological 
systems are indeed different than today’s computers, although there is no reason why future 
efforts at deep, multiscale, highly plastic synthetic devices cannot take advantage of the 
principles of biologic polycomputing. A key implication of our view is that that blanket 
pronouncements about what living, or non-living, machines can do are worthless: we are 

 
26
guaranteed to be surprised by outcomes that can only be achieved by formulating and testing 
hypotheses. It is already clear that synthetic, evolved, and hybrid systems far outstrip our ability 
to predict limits on their adaptive behavior; abandoning absolutist categories and objective views 
of computation is a first step towards expanding our predictive capabilities. 
 
At stake are numerous practical outcomes in addition to fundamental questions. For 
example, to make transformative advances in our ability to improve health in biomedical settings 
[244], we must be able to control multiple scales of biological organization which are heavily 
polycomputing - from cellular pathways to patient psychological state. It is essential to begin to 
develop computational frameworks to facilitate that kind of control. The ability to construct and 
model a kind of computational superposition, in which diverse observers (scientists, users, the 
agent itself, and its various components) have their own model of their dynamic environment 
and optimize their behavior accordingly, will also dovetail with and advance efforts in synthetic 
bioengineering, biorobotics, smart materials, and AI. 
 
 
Funding 
M.L. gratefully acknowledges the support of the Templeton World Charity Foundation 
(grant TWCF0606) and the John Templeton Foundation (grant 62212). J. B. gratefully 
acknowledges the support of the National Science Foundation (NAIRI award 2020247; DMREF 
award 2118988). 
 
 
Acknowledgements 
 
We thank Oded Rechavi and Aimer G. Diaz for useful pointers to relevant biological 
phenomena, and Julia Poirier and Susan Lewis for assistance with the manuscript.  
 
 
Author Contributions 
 
J.B. and M.L. contributed equally to this work. 
 
 
Conflict of Interest 
 
M.L. and J.B. are co-founders of Fauna Systems, an AI-biorobotics company; we declare 
no other competing interests. 
 

 
27
References 
 
1. 
Feynman, R. There’s plenty of room at the bottom. Engineering and Science 1959, 23. 
2. 
Levin, M. Technological Approach to Mind Everywhere: An Experimentally-Grounded 
Framework for Understanding Diverse Bodies and Minds. Front Syst Neurosci 2022, 16, 
768201, doi:10.3389/fnsys.2022.768201. 
3. 
Zhou, J.; Lai, J.; Menda, G.; Stafstrom, J.A.; Miles, C.I.; Hoy, R.R.; Miles, R.N. Outsourced 
hearing in an orb-weaving spider that uses its web as an auditory sensor. Proc Natl Acad 
Sci U S A 2022, 119, e2122789119, doi:10.1073/pnas.2122789119. 
4. 
Cariani, P.; Baker, J.M. Time Is of the Essence: Neural Codes, Synchronies, Oscillations, 
Architectures. Front Comput Neurosci 2022, 16, 898829, 
doi:10.3389/fncom.2022.898829. 
5. 
Pietsch, P. Shufflebrain; Houghton Mifflin: Boston, 1981; p. 273 p. 
6. 
Yasuda, H.; Buskohl, P.R.; Gillman, A.; Murphey, T.D.; Stepney, S.; Vaia, R.A.; Raney, J.R. 
Mechanical computing. Nature 2021, 598, 39-48, doi:10.1038/s41586-021-03623-y. 
7. 
Wood, C.C. The computational stance in biology. Philos Trans R Soc Lond B Biol Sci 2019, 
374, 20180380, doi:10.1098/rstb.2018.0380. 
8. 
Parsa, A.; Wang, D.; O'Hern, C.S.; Shattuck, M.D.; Kramer-Bottiglio, R.; Bongard, J. 
Evolving Programmable Computational Metamaterials. In Proceedings of the Genetic 
and Evolutionary Computation Conference (GECCO ’22), Boston, MA, 2022; pp. 122-129. 
9. 
Li, S.; Batra, R.; Brown, D.; Chang, H.D.; Ranganathan, N.; Hoberman, C.; Rus, D.; Lipson, 
H. Particle robotics based on statistical mechanics of loosely coupled components. 
Nature 2019, 567, 361-365, doi:10.1038/s41586-019-1022-9. 
10. 
Gondarenko, A.; Preble, S.; Robinson, J.; Chen, L.; Lipson, H.; Lipson, M. Spontaneous 
emergence of periodic patterns in a biologically inspired simulation of photonic 
structures. Phys Rev Lett 2006, 96, 143904, doi:10.1103/PhysRevLett.96.143904. 
11. 
Thompson, A. An evolved circuit, intrinsic in silicon, entwined with physics. In 
Proceedings of the International Conference on Evolvable Systems, Tsukuba, Japan, 
1996; pp. 390-405. 
12. 
Brodbeck, L.; Hauser, S.; Iida, F. Morphological Evolution of Physical Robots through 
Model-Free Phenotype Development. PLoS One 2015, 10, e0128444, 
doi:10.1371/journal.pone.0128444. 
13. 
Abolafia, D.A.; Norouzi, M.; Shen, J.; Zhao, R.; Le, Q.V. Neural program synthesis with 
priority queue training. arXiv 2018, doi:10.48550/arXiv.1801.03526. 
14. 
Sobania, D.; Briesch, M.; Rothlauf, F. Choose your programming copilot: a comparison of 
the program synthesis performance of github copilot and genetic programming. In 
Proceedings of the Proceedings of the Genetic and Evolutionary Computation 
Conference 2022; pp. 1019-1027. 
15. 
Karmaker, S.K.; Hassan, M.M.; Smith, M.J.; Xu, L.; Zhai, C.X.; Veeramachaneni, K. AutoML 
to Date and Beyond: Challenges and Opportunities. Acm Comput Surv 2021, 54, 1-36, 
doi:10.1145/3470918. 
16. 
Box, G.E.P. Robustness in the Strategy of Scientific Model Building. In Robustness in 
Statistics, Launer, R.L., Wilkinson, G.N., Eds.; Academic Press: New York; London, 1979; 
pp. 201-236. 

 
28
17. 
James, W. Pragmatism, a New Name for Some Old Ways of Thinking; Longmans, Green, 
and Co.: New York, 1907. 
18. 
Dennett, D.C. The intentional stance; MIT Press: Cambridge, Mass., 1987; pp. xi, 388 p. 
19. 
Pezzulo, G.; Levin, M. Top-down models in biology: explanation and control of complex 
living systems above the molecular level. J R Soc Interface 2016, 13, 
doi:10.1098/rsif.2016.0555. 
20. 
Pezzulo, G.; Levin, M. Re-membering the body: applications of computational 
neuroscience to the top-down control of regeneration of limbs and other complex 
organs. Integr Biol (Camb) 2015, 7, 1487-1517, doi:10.1039/c5ib00221d. 
21. 
Nanos, V.; Levin, M. Multi-scale Chimerism: An experimental window on the algorithms 
of anatomical control. Cells Dev 2022, 169, 203764, doi:10.1016/j.cdev.2021.203764. 
22. 
Manicka, S.; Levin, M. Modeling somatic computation with non-neural bioelectric 
networks. Sci Rep 2019, 9, 18612, doi:10.1038/s41598-019-54859-8. 
23. 
Grozinger, L.; Amos, M.; Gorochowski, T.E.; Carbonell, P.; Oyarzun, D.A.; Stoof, R.; 
Fellermann, H.; Zuliani, P.; Tas, H.; Goni-Moreno, A. Pathways to cellular supremacy in 
biocomputing. Nat Commun 2019, 10, 5250, doi:10.1038/s41467-019-13232-z. 
24. 
Sole, R.V.; Macia, J. Expanding the landscape of biological computation with synthetic 
multicellular consortia. Nat Comput 2013, 12, 485-497, doi:Doi 10.1007/S11047-013-
9380-Y. 
25. 
Macia, J.; Posas, F.; Sole, R.V. Distributed computation: the new wave of synthetic 
biology devices. Trends Biotechnol 2012, 30, 342-349, doi:S0167-7799(12)00038-8 [pii] 
10.1016/j.tibtech.2012.03.006. 
26. 
Auslander, S.; Auslander, D.; Muller, M.; Wieland, M.; Fussenegger, M. Programmable 
single-cell mammalian biocomputers. Nature 2012, 487, 123-127, 
doi:10.1038/nature11149. 
27. 
Regot, S.; Macia, J.; Conde, N.; Furukawa, K.; Kjellen, J.; Peeters, T.; Hohmann, S.; de 
Nadal, E.; Posas, F.; Sole, R. Distributed biological computation with multicellular 
engineered networks. Nature 2011, 469, 207-211, doi:10.1038/nature09679. 
28. 
Qian, L.; Winfree, E.; Bruck, J. Neural network computation with DNA strand 
displacement cascades. Nature 2011, 475, 368-372, doi:10.1038/nature10262. 
29. 
Bongard, J.; Levin, M. Living Things Are Not (20th Century) Machines: Updating 
Mechanism Metaphors in Light of the Modern Science of Machine Behavior. Frontiers in 
Ecology and Evolution 2021, 9, doi:10.3389/fevo.2021.650726. 
30. 
Nicholson, D.J. The machine conception of the organism in development and evolution: 
a critical analysis. Stud Hist Philos Biol Biomed Sci 2014, 48 Pt B, 162-174, 
doi:10.1016/j.shpsc.2014.08.003. 
31. 
Nicholson, D.J. Organisms not equal Machines. Stud Hist Philos Biol Biomed Sci 2013, 44, 
669-678, doi:10.1016/j.shpsc.2013.05.014. 
32. 
Clawson, W.P.; Levin, M. Endless Forms Most Beautiful: teleonomy and the 
bioengineering of chimeric and synthetic organisms. Biological Journal of the Linnean 
Society 2022, in press. 
33. 
Olle-Vila, A.; Duran-Nebreda, S.; Conde-Pueyo, N.; Montanez, R.; Sole, R. A 
morphospace for synthetic organs and organoids: the possible and the actual. Integr Biol 
(Camb) 2016, 8, 485-503, doi:10.1039/c5ib00324e. 

 
29
34. 
Langton, C.G. Artificial life : an overview; MIT Press: Cambridge, Mass., 1995; pp. xi, 
340 , [346] of plates. 
35. 
Doursat, R.; Sanchez, C. Growing fine-grained multicellular robots. Soft Robotics 2014, 1, 
110-121. 
36. 
Doursat, R.; Sayama, H.; Michel, O. A review of morphogenetic engineering. Nat Comput 
2013, 12, 517-535, doi:Doi 10.1007/S11047-013-9398-1. 
37. 
Ebrahimkhani, M.R.; Levin, M. Synthetic living machines: A new window on life. iScience 
2021, 24, 102505, doi:10.1016/j.isci.2021.102505. 
38. 
Ebrahimkhani, M.R.; Ebisuya, M. Synthetic developmental biology: build and control 
multicellular systems. Curr Opin Chem Biol 2019, 52, 9-15, 
doi:10.1016/j.cbpa.2019.04.006. 
39. 
Kamm, R.D.; Bashir, R.; Arora, N.; Dar, R.D.; Gillette, M.U.; Griffith, L.G.; Kemp, M.L.; 
Kinlaw, K.; Levin, M.; Martin, A.C.; et al. Perspective: The promise of multi-cellular 
engineered living systems. Apl Bioeng 2018, 2, 040901, doi:10.1063/1.5038337. 
40. 
Kamm, R.D.; Bashir, R. Creating living cellular machines. Ann Biomed Eng 2014, 42, 445-
459, doi:10.1007/s10439-013-0902-7. 
41. 
Levin, M. Life, death, and self: Fundamental questions of primitive cognition viewed 
through the lens of body plasticity and synthetic organisms. Biochem Biophys Res 
Commun 2021, 564, 114-133, doi:10.1016/j.bbrc.2020.10.077. 
42. 
Levin, M. The Computational Boundary of a "Self": Developmental Bioelectricity Drives 
Multicellularity and Scale-Free Cognition. Front Psychol 2019, 10, 2688, 
doi:10.3389/fpsyg.2019.02688. 
43. 
Levin, M. Bioelectrical approaches to cancer as a problem of the scaling of the cellular 
self. Prog Biophys Mol Biol 2021, 165, 102-113, doi:10.1016/j.pbiomolbio.2021.04.007. 
44. 
Friston, K. Life as we know it. J R Soc Interface 2013, 10, 20130475, 
doi:10.1098/rsif.2013.0475. 
45. 
Ramstead, M.J.D.; Badcock, P.B.; Friston, K.J. Answering Schrodinger's question: A free-
energy formulation. Phys Life Rev 2018, 24, 1-16, doi:10.1016/j.plrev.2017.09.001. 
46. 
Constant, A.; Ramstead, M.J.D.; Veissiere, S.P.L.; Campbell, J.O.; Friston, K.J. A 
variational approach to niche construction. J R Soc Interface 2018, 15, 
doi:10.1098/rsif.2017.0685. 
47. 
Allen, M.; Friston, K.J. From cognitivism to autopoiesis: towards a computational 
framework for the embodied mind. Synthese 2018, 195, 2459-2482, 
doi:10.1007/s11229-016-1288-5. 
48. 
Resnick, M. Turtles, termites, and traffic jams : explorations in massively parallel 
microworlds, 1st MIT Press pbk. ed.; MIT Press: Cambridge, Mass., 1997; pp. xviii, 163 p. 
49. 
Adamatzky, A. Towards fungal computer. Interface Focus 2018, 8, 20180029, doi:ARTN 
20180029 10.1098/rsfs.2018.0029. 
50. 
Siccardi, S.; Adamatzky, A. Models of Computing on Actin Filaments. In Advances in 
Unconventional Computing: Volume 2: Prototypes, Models and Algorithms, Adamatzky, 
A., Ed.; Springer International Publishing: Cham, 2017; pp. 309-346. 
51. 
Maley, C.C. DNA computation: theory, practice, and prospects. Evol. Comput. 1998, 6, 
201-229. 

 
30
52. 
Dorigo, M.; Di Caro, G. Ant colony optimization: a new meta-heuristic. In Proceedings of 
the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), 1999; pp. 
1470-1477  
53. 
Churchland, P.S.; Sejnowski, T.J. The computational brain, 25th Anniversary edition. ed.; 
The MIT Press: Cambridge, MA, 2017; pp. xix, 544 pages. 
54. 
Friston, K.J.; Stephan, K.E.; Montague, R.; Dolan, R.J. Computational psychiatry: the 
brain as a phantastic organ. Lancet Psychiatry 2014, 1, 148-158, doi:10.1016/S2215-
0366(14)70275-5. 
55. 
Corlett, P.R.; Fletcher, P.C. Computational psychiatry: a Rosetta Stone linking the brain 
to mental illness. Lancet Psychiatry 2014, 1, 399-402, doi:10.1016/S2215-
0366(14)70298-6. 
56. 
Bray, D. Wetware : a computer in every living cell; Yale University Press: New Haven, 
2009; pp. xii, 267 p. 
57. 
Bray, D. Protein molecules as computational elements in living cells. Nature 1995, 376, 
307-312, doi:10.1038/376307a0. 
58. 
Fields, C.; Bischof, J.; Levin, M. Morphological Coordination: A Common Ancestral 
Function Unifying Neural and Non-Neural Signaling. Physiology 2020, 35, 16-30, 
doi:10.1152/physiol.00027.2019. 
59. 
Tanaka, G.; Yamane, T.; Heroux, J.B.; Nakane, R.; Kanazawa, N.; Takeda, S.; Numata, H.; 
Nakano, D.; Hirose, A. Recent advances in physical reservoir computing: A review. 
Neural Netw 2019, 115, 100-123, doi:10.1016/j.neunet.2019.03.005. 
60. 
Davies, J.; Levin, M. Synthetic morphology via active and agential matter. preprint 2022, 
doi:10.31219/osf.io/xrv8h. 
61. 
Manicka, S.; Levin, M. The Cognitive Lens: a primer on conceptual tools for analysing 
information processing in developmental and regenerative morphogenesis. Philos Trans 
R Soc Lond B Biol Sci 2019, 374, 20180369, doi:10.1098/rstb.2018.0369. 
62. 
Nocera, D.G. The artificial leaf. Acc Chem Res 2012, 45, 767-776, 
doi:10.1021/ar2003013. 
63. 
Chin, Y.W.; Kok, J.M.; Zhu, Y.Q.; Chan, W.L.; Chahl, J.S.; Khoo, B.C.; Lau, G.K. Efficient 
flapping wing drone arrests high-speed flight using post-stall soaring. Sci Robot 2020, 5, 
eaba2386, doi:10.1126/scirobotics.aba2386. 
64. 
Leiserson, C.E.; Thompson, N.C.; Emer, J.S.; Kuszmaul, B.C.; Lampson, B.W.; Sanchez, D.; 
Schardl, T.B. There's plenty of room at the Top: What will drive computer performance 
after Moore's law? Science 2020, 368, eaam9744, doi:10.1126/science.aam9744. 
65. 
Rahwan, I.; Cebrian, M.; Obradovich, N.; Bongard, J.; Bonnefon, J.F.; Breazeal, C.; 
Crandall, J.W.; Christakis, N.A.; Couzin, I.D.; Jackson, M.O.; et al. Machine behaviour. 
Nature 2019, 568, 477-486, doi:10.1038/s41586-019-1138-y. 
66. 
Nicholson, D.J. Is the cell really a machine? J Theor Biol 2019, 477, 108-126, 
doi:10.1016/j.jtbi.2019.06.002. 
67. 
Boldt, J. Machine metaphors and ethics in synthetic biology. Life Sci Soc Policy 2018, 14, 
12, doi:10.1186/s40504-018-0077-y. 
68. 
Boyle, E.A.; Li, Y.I.; Pritchard, J.K. An Expanded View of Complex Traits: From Polygenic 
to Omnigenic. Cell 2017, 169, 1177-1186, doi:10.1016/j.cell.2017.05.038. 

 
31
69. 
Plaut, D.C.; McClelland, J.L. Locating object knowledge in the brain: comment on 
Bowers's (2009) attempt to revive the grandmother cell hypothesis. Psychol Rev 2010, 
117, 284-288, doi:10.1037/a0017101. 
70. 
Bongard, J.C. Evolutionary robotics. Communications of the ACM 2013, 56, 74-83, 
doi:10.1145/2493883. 
71. 
Wright, L.G.; Onodera, T.; Stein, M.M.; Wang, T.; Schachter, D.T.; Hu, Z.; McMahon, P.L. 
Deep physical neural networks trained with backpropagation. Nature 2022, 601, 549-
555, doi:10.1038/s41586-021-04223-6. 
72. 
Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. Kinematic self-replication in 
reconfigurable organisms. Proc Natl Acad Sci U S A 2021, 118, 
doi:10.1073/pnas.2112672118. 
73. 
Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. A scalable pipeline for designing 
reconfigurable organisms. Proc Natl Acad Sci U S A 2020, 117, 1853-1859, 
doi:10.1073/pnas.1910837117. 
74. 
Floridi, L.; Chiriatti, M. GPT-3: Its Nature, Scope, Limits, and Consequences. Mind Mach 
2020, 30, 681-694, doi:10.1007/s11023-020-09548-1. 
75. 
Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; Chen, M. Hierarchical text-conditional 
image generation with clip latents arXiv 2022, doi:10.48550/arXiv.2204.06125. 
76. 
Ortiz Jr, C.L. Why we need a physically embodied Turing test and what it might look like. 
AI magazine 2016, 37, 55-62. 
77. 
Giselbrecht, S.; Rapp, B.E.; Niemeyer, C.M. The chemistry of cyborgs--interfacing 
technical devices with organisms. Angew Chem Int Ed Engl 2013, 52, 13942-13957, 
doi:10.1002/anie.201307495. 
78. 
Nam, Y.; Wheeler, B.C. In vitro microelectrode array technology and neural recordings. 
Crit. Rev. Biomed. Eng. 2011, 39, 45-61. 
79. 
Rothschild, R.M. Neuroengineering tools/applications for bidirectional interfaces, brain-
computer interfaces, and neuroprosthetic implants - a review of recent progress. Front 
Neuroeng 2010, 3, 112, doi:10.3389/fneng.2010.00112. 
80. 
Nanos, V.; Levin, M. Multi-scale Chimerism: An experimental window on the algorithms 
of anatomical control. Cells Dev 2021, 169, 203764, doi:10.1016/j.cdev.2021.203764. 
81. 
Kuchling, F.; Friston, K.; Georgiev, G.; Levin, M. Morphogenesis as Bayesian inference: A 
variational approach to pattern formation and control in complex biological systems. 
Phys Life Rev 2020, 33, 88-108, doi:10.1016/j.plrev.2019.06.001. 
82. 
Friston, K.; Levin, M.; Sengupta, B.; Pezzulo, G. Knowing one's place: a free-energy 
approach to pattern regulation. J R Soc Interface 2015, 12, doi:10.1098/rsif.2014.1383. 
83. 
Fields, C.; Friston, K.; Glazebrook, J.F.; Levin, M. A free energy principle for generic 
quantum systems. 2021, arXiv:2112.15242. 
84. 
Friston, K. A free energy principle for a particular physics arXiv 2019, 
doi:10.48550/arXiv.1906.10184. 
85. 
Genty, G.; Salmela, L.; Dudley, J.M.; Brunner, D.; Kokhanovskiy, A.; Kobtsev, S.; Turitsyn, 
S.K. Machine learning and applications in ultrafast photonics. Nat Photonics 2021, 15, 
91-101. 
86. 
Nakajima, K. Physical reservoir computing—an introductory perspective. Japanese 
Journal of Applied Physics 2020, 59.6, 060501. 

 
32
87. 
Gyongyosi, L.; Imre, S. A Survey on quantum computing technology. Computer Science 
Review 2019, 31, 51-71, doi:10.1016/j.cosrev.2018.11.002. 
88. 
Paul, C. Morphological computation - A basis for the analysis of morphology and control 
requirements. Robot Auton Syst 2006, 54, 619-630, doi:Doi 
10.1016/J.Robot.2006.03.003. 
89. 
Zahedi, K.; Ay, N. Quantifying Morphological Computation. Entropy-Switz 2013, 15, 
1887-1915, doi:10.3390/e15051887. 
90. 
Heimburg, T.; Jackson, A.D. On soliton propagation in biomembranes and nerves. Proc 
Natl Acad Sci U S A 2005, 102, 9790-9795, doi:10.1073/pnas.0503823102. 
91. 
Hoel, E.P. Agent Above, Atom Below: How Agents Causally Emerge from Their 
Underlying Microphysics. In Wandering Towards a Goal: How Can Mindless 
Mathematical Laws Give Rise to Aims and Intention?, Aguirre, A., Foster, B., Merali, Z., 
Eds.; Springer International Publishing: Cham, 2018; pp. 63-76. 
92. 
Hoel, E.P. When the Map Is Better Than the Territory. Entropy-Switz 2017, 19, 
doi:10.3390/e19050188. 
93. 
Albantakis, L.; Marshall, W.; Hoel, E.P.; Tononi, G. What caused what? An irreducible 
account of actual causation. arXiv 2017, arXiv:1708.06716. 
94. 
Hoel, E.P.; Albantakis, L.; Marshall, W.; Tononi, G. Can the macro beat the micro? 
Integrated information across spatiotemporal scales. Neurosci Conscious 2016, 2016, 
niw012, doi:10.1093/nc/niw012. 
95. 
Hoel, E.P.; Albantakis, L.; Tononi, G. Quantifying causal emergence shows that macro 
can beat micro. Proceedings of the National Academy of Sciences of the United States of 
America 2013, 110, 19790-19795, doi:10.1073/pnas.1314922110. 
96. 
Klein, B.; Hoel, E. Uncertainty and causal emergence in complex networks. arXiv e-prints 
2019. 
97. 
Kim, H.; Sayama, H. How Criticality of Gene Regulatory Networks Affects the Resulting 
Morphogenesis under Genetic Perturbations. Artif. Life 2018, 24, 85-105, 
doi:10.1162/artl_a_00262. 
98. 
Graudenzi, A.; Serra, R.; Villani, M.; Colacci, A.; Kauffman, S.A. Robustness analysis of a 
Boolean model of gene regulatory network with memory. J. Comput. Biol. 2011, 18, 559-
577, doi:10.1089/cmb.2010.0224. 
99. 
Darabos, C.; Di Cunto, F.; Tomassini, M.; Moore, J.H.; Provero, P.; Giacobini, M. Additive 
functions in boolean models of gene regulatory network modules. PLoS One 2011, 6, 
e25110, doi:10.1371/journal.pone.0025110. 
100. 
Bassel, G.W. Information Processing and Distributed Computation in Plant Organs. 
Trends Plant Sci 2018, 23, 994-1005, doi:10.1016/j.tplants.2018.08.006. 
101. 
Calvo, P.; Friston, K. Predicting green: really radical (plant) predictive processing. J R Soc 
Interface 2017, 14, doi:10.1098/rsif.2017.0096. 
102. 
Zhu, L.; Kim, S.J.; Hara, M.; Aono, M. Remarkable problem-solving ability of unicellular 
amoeboid organism and its mechanism. R Soc Open Sci 2018, 5, 180396, 
doi:10.1098/rsos.180396. 
103. 
Sole, R.; Amor, D.R.; Duran-Nebreda, S.; Conde-Pueyo, N.; Carbonell-Ballestero, M.; 
Montanez, R. Synthetic collective intelligence. Biosystems 2016, 148, 47-61, 
doi:10.1016/j.biosystems.2016.01.002. 

 
33
104. 
Reid, C.R.; MacDonald, H.; Mann, R.P.; Marshall, J.A.; Latty, T.; Garnier, S. Decision-
making without a brain: how an amoeboid organism solves the two-armed bandit. J R 
Soc Interface 2016, 13, doi:10.1098/rsif.2016.0030. 
105. 
Vallverdu, J.; Castro, O.; Mayne, R.; Talanov, M.; Levin, M.; Baluska, F.; Gunji, Y.; 
Dussutour, A.; Zenil, H.; Adamatzky, A. Slime mould: The fundamental mechanisms of 
biological cognition. Biosystems 2018, 165, 57-70, 
doi:10.1016/j.biosystems.2017.12.011. 
106. 
Mugler, A.; Kittisopikul, M.; Hayden, L.; Liu, J.; Wiggins, C.H.; Suel, G.M.; Walczak, A.M. 
Noise Expands the Response Range of the Bacillus subtilis Competence Circuit. PLoS 
computational biology 2016, 12, e1004793, doi:10.1371/journal.pcbi.1004793. 
107. 
Prindle, A.; Liu, J.; Asally, M.; Ly, S.; Garcia-Ojalvo, J.; Suel, G.M. Ion channels enable 
electrical communication in bacterial communities. Nature 2015, 527, 59-63, 
doi:10.1038/nature15709. 
108. 
Larkin, J.W.; Zhai, X.; Kikuchi, K.; Redford, S.E.; Prindle, A.; Liu, J.; Greenfield, S.; Walczak, 
A.M.; Garcia-Ojalvo, J.; Mugler, A.; et al. Signal Percolation within a Bacterial 
Community. Cell Syst 2018, 7, 137-145 e133, doi:10.1016/j.cels.2018.06.005. 
109. 
Baluska, F.; Reber, A.S.; Miller, W.B., Jr. Cellular sentience as the primary source of 
biological order and evolution. Biosystems 2022, 218, 104694, 
doi:10.1016/j.biosystems.2022.104694. 
110. 
Baluska, F.; Miller, W.B.; Reber, A.S. Cellular and evolutionary perspectives on 
organismal cognition: from unicellular to multicellular organisms. Biological Journal of 
the Linnean Society 2022, doi:10.1093/biolinnean/blac005. 
111. 
Reber, A.S.; Baluska, F. Cognition in some surprising places. Biochem Biophys Res 
Commun 2021, 564, 150-157, doi:10.1016/j.bbrc.2020.08.115. 
112. 
Baluška, F.; Levin, M. On Having No Head: Cognition throughout Biological Systems. 
Front Psychol 2016, 7, 902, doi:10.3389/fpsyg.2016.00902. 
113. 
Levin, M. Bioelectric signaling: Reprogrammable circuits underlying embryogenesis, 
regeneration, and cancer. Cell 2021, 184, 1971-1989, doi:10.1016/j.cell.2021.02.034. 
114. 
Harris, M.P. Bioelectric signaling as a unique regulator of development and 
regeneration. Development 2021, 148, doi:10.1242/dev.180794. 
115. 
Bonzanni, M.; Rouleau, N.; Levin, M.; Kaplan, D.L. Optogenetically induced cellular 
habituation in non-neuronal cells. PLoS One 2020, 15, e0227230, 
doi:10.1371/journal.pone.0227230. 
116. 
Adams, D.S.; Uzel, S.G.; Akagi, J.; Wlodkowic, D.; Andreeva, V.; Yelick, P.C.; Devitt-Lee, 
A.; Pare, J.F.; Levin, M. Bioelectric signalling via potassium channels: a mechanism for 
craniofacial dysmorphogenesis in KCNJ2-associated Andersen-Tawil Syndrome. J Physiol 
2016, 594, 3245-3270, doi:10.1113/JP271930. 
117. 
Adams, D.S.; Lemire, J.M.; Kramer, R.H.; Levin, M. Optogenetics in Developmental 
Biology: using light to control ion flux-dependent signals in Xenopus embryos. The 
International journal of developmental biology 2014, 58, 851-861, 
doi:10.1387/ijdb.140207ml. 
118. 
Adams, D.S.; Tseng, A.S.; Levin, M. Light-activation of the Archaerhodopsin H(+)-pump 
reverses age-dependent loss of vertebrate regeneration: sparking system-level controls 
in vivo. Biology open 2013, 2, 306-313, doi:10.1242/bio.20133665. 

 
34
119. 
Fields, C.; Levin, M. Scale-Free Biology: Integrating Evolutionary and Developmental 
Thinking. BioEssays 2020, 42, e1900228, doi:10.1002/bies.201900228. 
120. 
Bizzari, M.; Brash, D.E.; Briscoe, J.; Grieneisen, V.A.; Stern, C.D.; Levin, M. A call for a 
better understanding of causation in cell biology. Nature Reviews Molecular Cell Biology 
2019, 20, 261-262, doi:10.1038/s41580-019-0127-1. 
121. 
Stern, C.D. Reflections on the past, present and future of developmental biology. Dev 
Biol 2022, 488, 30-34, doi:10.1016/j.ydbio.2022.05.001. 
122. 
Xiao, L.; Jianying, H.; Mingjie, Z.; Tiangui, D.; Hui, L.; Yuhong, R. Optical holographic data 
storage—The time for new development. Opto-Electronic Engineering 2019, 46, 180642, 
doi:10.12086/oee.2019.180642 
. 
123. 
Kim, J.Z.; Bassett, D.S. A Neural Programming Language for the Reservoir Computer. 
arXiv 2022, doi:10.48550/arXiv.2203.05032. 
124. 
Cardelli, L.; Csikasz-Nagy, A. The cell cycle switch computes approximate majority. Sci 
Rep 2012, 2, 656, doi:10.1038/srep00656. 
125. 
Bray, D.; Gilbert, D. Cytoskeletal elements in neurons. Annu Rev Neurosci 1981, 4, 505-
523, doi:10.1146/annurev.ne.04.030181.002445. 
126. 
Aubin, C.A.; Gorissen, B.; Milana, E.; Buskohl, P.R.; Lazarus, N.; Slipher, G.A.; Keplinger, 
C.; Bongard, J.; Iida, F.; Lewis, J.A.; et al. Towards enduring autonomous robots via 
embodied energy. Nature 2022, 602, 393-402, doi:10.1038/s41586-021-04138-2. 
127. 
Sanford, E.M.; Emert, B.L.; Cote, A.; Raj, A. Gene regulation gravitates toward either 
addition or multiplication when combining the effects of two signals. Elife 2020, 9, 
doi:10.7554/eLife.59388. 
128. 
Hameroff, S.R.; Craddock, T.J.; Tuszynski, J.A. "Memory bytes" - molecular match for 
CaMKII phosphorylation encoding of microtubule lattices. J Integr Neurosci 2010, 9, 253-
267. 
129. 
Craddock, T.J.; Beauchemin, C.; Tuszynski, J.A. Information processing mechanisms in 
microtubules at physiological temperature: Model predictions for experimental tests. 
Biosystems 2009, 97, 28-34. 
130. 
Hentschel, H.G.; Fine, A.; Pencea, C.S. Biological computing with diffusion and excitable 
calcium stores. Math Biosci Eng 2004, 1, 147-159, doi:10.3934/mbe.2004.1.147. 
131. 
Habibi, I.; Cheong, R.; Lipniacki, T.; Levchenko, A.; Emamian, E.S.; Abdi, A. Computation 
and measurement of cell decision making errors using single cell data. PLoS 
computational biology 2017, 13, e1005436, doi:10.1371/journal.pcbi.1005436. 
132. 
Timsit, Y.; Gregoire, S.P. Towards the Idea of Molecular Brains. Int J Mol Sci 2021, 22, 
doi:10.3390/ijms222111868. 
133. 
Johnston, I.G.; Bassel, G.W. Identification of a bet-hedging network motif generating 
noise in hormone concentrations and germination propensity in Arabidopsis. J R Soc 
Interface 2018, 15, doi:10.1098/rsif.2018.0042. 
134. 
Ray, S.K.; Valentini, G.; Shah, P.; Haque, A.; Reid, C.R.; Weber, G.F.; Garnier, S. 
Information Transfer During Food Choice in the Slime Mold Physarum polycephalum. 
Frontiers in Ecology and Evolution 2019, 7, doi:10.3389/fevo.2019.00067. 
135. 
Iwayama, K.; Zhu, L.P.; Hirata, Y.; Aono, M.; Hara, M.; Aihara, K. Decision-making ability 
of Physarum polycephalum enhanced by its coordinated spatiotemporal oscillatory 
dynamics. Bioinspiration & Biomimetics 2016, 11, 036001, doi:Artn 036001 

 
35
10.1088/1748-3190/11/3/036001. 
136. 
Tarabella, G.; D'Angelo, P.; Cifarelli, A.; Dimonte, A.; Romeo, A.; Berzina, T.; Erokhin, V.; 
Iannotta, S. A hybrid living/organic electrochemical transistor based on the Physarum 
polycephalum cell endowed with both sensing and memristive properties. Chem Sci 
2015, 6, 2859-2868, doi:10.1039/c4sc03425b. 
137. 
Adamatzky, A. Slime mould processors, logic gates and sensors. Philos Trans A Math 
Phys Eng Sci 2015, 373, doi:10.1098/rsta.2014.0216. 
138. 
Jones, J.; Adamatzky, A. Towards Physarum binary adders. Bio Systems 2010, 101, 51-58, 
doi:10.1016/j.biosystems.2010.04.005. 
139. 
Katz, Y.; Goodman, N.D.; Kersting, K.; Kemp, C.; Tenenbaum, J.B. Modeling Semantic 
Cognition as Logical Dimensionality Reduction. ? 2018, ? 
140. 
Katz, Y. Embodying probabilistic inference in biochemical circuits. ArXiv 2018. 
141. 
Katz, Y.; Springer, M. Probabilistic adaptation in changing microbial environments. PeerJ 
2016, 4, e2716, doi:10.7717/peerj.2716. 
142. 
Tay, S.; Hughey, J.J.; Lee, T.K.; Lipniacki, T.; Quake, S.R.; Covert, M.W. Single-cell NF-
kappaB dynamics reveal digital activation and analogue information processing. Nature 
2010, 466, 267-271, doi:10.1038/nature09145. 
143. 
Pezzulo, G.; LaPalme, J.; Durant, F.; Levin, M. Bistability of somatic pattern memories: 
stochastic outcomes in bioelectric circuits underlying regeneration. Philos Trans R Soc 
Lond B Biol Sci 2021, 376, 20190765, doi:10.1098/rstb.2019.0765. 
144. 
Levin, M.; Pietak, A.M.; Bischof, J. Planarian regeneration as a model of anatomical 
homeostasis: Recent progress in biophysical and computational approaches. Semin Cell 
Dev Biol 2018, 87, 125-144, doi:10.1016/j.semcdb.2018.04.003. 
145. 
Pietak, A.; Levin, M. Bioelectric gene and reaction networks: computational modelling of 
genetic, biochemical and bioelectrical dynamics in pattern regulation. J R Soc Interface 
2017, 14, doi:10.1098/rsif.2017.0425. 
146. 
Fields, C.; Levin, M. Multiscale memory and bioelectric error correction in the 
cytoplasm–cytoskeleton-membrane system. Wiley Interdisciplinary Reviews: Systems 
Biology and Medicine 2017, 10, e1410-n/a, doi:10.1002/wsbm.1410. 
147. 
Law, R.; Levin, M. Bioelectric memory: modeling resting potential bistability in 
amphibian embryos and mammalian cells. Theor Biol Med Model 2015, 12, 22, 
doi:10.1186/s12976-015-0019-9. 
148. 
Raman, K.; Wagner, A. The evolvability of programmable hardware. J R Soc Interface 
2011, 8, 269-281, doi:10.1098/rsif.2010.0212. 
149. 
Manicka, S.; Levin, M. Minimal Developmental Computation: A Causal Network 
Approach to Understand Morphogenetic Pattern Formation. Entropy-Switz 2022, 24, 
107, doi:10.3390/e24010107. 
150. 
Durant, F.; Bischof, J.; Fields, C.; Morokuma, J.; LaPalme, J.; Hoi, A.; Levin, M. The Role of 
Early Bioelectric Signals in the Regeneration of Planarian Anterior/Posterior Polarity. 
Biophys J 2019, 116, 948-961, doi:10.1016/j.bpj.2019.01.029. 
151. 
Berend, D.; Dolev, S.; Frenkel, S.; Hanemann, A. Towards holographic "brain" memory 
based on randomization and Walsh-Hadamard transformation. Neural Netw 2016, 77, 
87-94, doi:10.1016/j.neunet.2016.02.001. 

 
36
152. 
Wess, O.; Roder, U. A holographic model for associative memory chains. Biol Cybern 
1977, 27, 89-98, doi:10.1007/BF00337260. 
153. 
Levin, M. Collective Intelligence of Morphogenesis as a Teleonomic Process. in review 
2022. 
154. 
Sajid, N.; Parr, T.; Hope, T.M.; Price, C.J.; Friston, K.J. Degeneracy and Redundancy in 
Active Inference. Cereb Cortex 2020, 30, 5750-5766, doi:10.1093/cercor/bhaa148. 
155. 
Ball, J.M.; Chen, S.; Li, W. Mitochondria in cone photoreceptors act as microlenses to 
enhance photon delivery and confer directional sensitivity to light. Sci Adv 2022, 8, 
eabn2070, doi:10.1126/sciadv.abn2070. 
156. 
Wrabl, J.O.; Gu, J.; Liu, T.; Schrank, T.P.; Whitten, S.T.; Hilser, V.J. The role of protein 
conformational fluctuations in allostery, function, and evolution. Biophys. Chem. 2011, 
159, 129-141, doi:10.1016/j.bpc.2011.05.020. 
157. 
Biswas, S.; Manicka, S.; Hoel, E.; Levin, M. Gene Regulatory Networks Exhibit Several 
Kinds of Memory: Quantification of Memory in Biological and Random Transcriptional 
Networks. iScience 2021, 24, 102131, doi:https://doi.org/10.1016/j.isci.2021.102131. 
158. 
Szabó, Á.; Vattay, G.; Kondor, D. A cell signaling model as a trainable neural 
nanonetwork. Nano Communication Networks 2012, 3, 57-64. 
159. 
Herrera-Delgado, E.; Perez-Carrasco, R.; Briscoe, J.; Sollich, P. Memory functions reveal 
structural properties of gene regulatory networks. PLoS computational biology 2018, 14, 
e1006003, doi:10.1371/journal.pcbi.1006003. 
160. 
Watson, R.A.; Buckley, C.L.; Mills, R.; Davies, A. Associative memory in gene regulation 
networks. In Proceedings of the Artificial Life Conference XII, Odense, Denmark, 2010; 
pp. 194-201. 
161. 
Chen, Z.; Linton, J.M.; Zhu, R.; Elowitz, M.B. A synthetic protein-level neural network in 
mammalian cells. bioRxiv 2022, 2022.2007.2010.499405, 
doi:10.1101/2022.07.10.499405. 
162. 
McGregor, S.; Vasas, V.; Husbands, P.; Fernando, C. Evolution of associative learning in 
chemical networks. PLoS computational biology 2012, 8, e1002739, 
doi:10.1371/journal.pcbi.1002739. 
163. 
Lau, M.W.; Ferre-D'Amare, A.R. Many Activities, One Structure: Functional Plasticity of 
Ribozyme Folds. Molecules 2016, 21, doi:10.3390/molecules21111570. 
164. 
Muller, F.; Escobar, L.; Xu, F.; Wegrzyn, E.; Nainyte, M.; Amatov, T.; Chan, C.Y.; Pichler, 
A.; Carell, T. A prebiotically plausible scenario of an RNA-peptide world. Nature 2022, 
605, 279-284, doi:10.1038/s41586-022-04676-3. 
165. 
Couzigou, J.M.; Lauressergues, D.; Becard, G.; Combier, J.P. miRNA-encoded peptides 
(miPEPs): A new tool to analyze the roles of miRNAs in plant biology. RNA Biol 2015, 12, 
1178-1180, doi:10.1080/15476286.2015.1094601. 
166. 
Raina, M.; King, A.; Bianco, C.; Vanderpool, C.K. Dual-Function RNAs. Microbiol Spectr 
2018, 6, doi:10.1128/microbiolspec.RWR-0032-2018. 
167. 
Makalowska, I.; Lin, C.F.; Makalowski, W. Overlapping genes in vertebrate genomes. 
Comput Biol Chem 2005, 29, 1-12, doi:10.1016/j.compbiolchem.2004.12.006. 
168. 
Wright, B.W.; Molloy, M.P.; Jaschke, P.R. Overlapping genes in natural and engineered 
genomes. Nat Rev Genet 2022, 23, 154-168, doi:10.1038/s41576-021-00417-w. 

 
37
169. 
Tosteson, M.T.; Kim, J.B.; Goldstein, D.J.; Tosteson, D.C. Ion channels formed by 
transcription factors recognize consensus DNA sequences. Biochim Biophys Acta 2001, 
1510, 209-218, doi:10.1016/s0005-2736(00)00351-5. 
170. 
Melendez Garcia, R.; Haccard, O.; Chesneau, A.; Narassimprakash, H.; Roger, J.; Perron, 
M.; Marheineke, K.; Bronchain, O. A non-transcriptional function of Yap regulates the 
DNA replication program in Xenopus laevis. Elife 2022, 11, doi:10.7554/eLife.75741. 
171. 
Elhage, N.; Hume, T.; Olsson, C.; Schiefer, N.; Henighan, T.; Kravec, S.; Hatfield-Dodds, Z.; 
Lasenby, R.; Drain, D.; Chen, C.; et al. Toy Models of Superposition. Available online: 
https://transformer-circuits.pub/2022/toy_model/index.html (accessed on  
172. 
Solé R; LF, S. Polysemy and power: Ambiguity in language networks. Linguist. Rev. 2015, 
32, 5-35. 
173. 
Noguchi, W.; Iizuka, H.; Yamamoto, M.; Taguchi, S. Superposition mechanism as a neural 
basis for understanding others. Sci Rep 2022, 12, 2859, doi:10.1038/s41598-022-06717-
3. 
174. 
Lahoz-Beltra, R.; Hameroff, S.R.; Dayhoff, J.E. Cytoskeletal logic: a model for molecular 
computation via Boolean operations in microtubules and microtubule-associated 
proteins. Biosystems 1993, 29, 1-23. 
175. 
Craddock, T.J.; Tuszynski, J.A.; Hameroff, S. Cytoskeletal signaling: is memory encoded in 
microtubule lattices by CaMKII phosphorylation? PLoS computational biology 2012, 8, 
e1002421, doi:10.1371/journal.pcbi.1002421. 
176. 
Larson, B.T.; Garbus, J.; Pollack, J.B.; Marshall, W.F. A unicellular walker controlled by a 
microtubule-based finite-state machine. Curr Biol 2022, 32, 3745-3757 e3747, 
doi:10.1016/j.cub.2022.07.034. 
177. 
Tuszynski, J.A.; Friesen, D.; Freedman, H.; Sbitnev, V.I.; Kim, H.; Santelices, I.; Kalra, A.P.; 
Patel, S.D.; Shankar, K.; Chua, L.O. Microtubules as Sub-Cellular Memristors. Sci Rep 
2020, 10, 2108, doi:10.1038/s41598-020-58820-y. 
178. 
Priel, A.; Tuszynski, J.A.; Woolf, N.J. Neural cytoskeleton capabilities for learning and 
memory. J Biol Phys 2010, 36, 3-21, doi:10.1007/s10867-009-9153-0. 
179. 
Cantero, M.D.R.; Villa Etchegoyen, C.; Perez, P.L.; Scarinci, N.; Cantiello, H.F. Bundles of 
Brain Microtubules Generate Electrical Oscillations. Sci Rep 2018, 8, 11899, 
doi:10.1038/s41598-018-30453-2. 
180. 
Priel, A.; Ramos, A.J.; Tuszynski, J.A.; Cantiello, H.F. A biopolymer transistor: electrical 
amplification by microtubules. Biophysical journal 2006, 90, 4639-4643, 
doi:10.1529/biophysj.105.078915. 
181. 
Tuszynski, J.A.; Portet, S.; Dixon, J.M.; Luxford, C.; Cantiello, H.F. Ionic wave propagation 
along actin filaments. Biophys J 2004, 86, 1890-1903, doi:S0006-3495(04)74255-1 [pii] 
10.1016/S0006-3495(04)74255-1. 
182. 
Hameroff, S.; Nip, A.; Porter, M.; Tuszynski, J. Conduction pathways in microtubules, 
biological quantum computation, and consciousness. Biosystems 2002, 64, 149-168. 
183. 
Jibu, M.; Hagan, S.; Hameroff, S.R.; Pribram, K.H.; Yasue, K. Quantum optical coherence 
in cytoskeletal microtubules: implications for brain function. Biosystems 1994, 32, 195-
209. 

 
38
184. 
Rosen, M.R.; Binah, O.; Marom, S. Cardiac memory and cortical memory: do learning 
patterns in neural networks impact on cardiac arrhythmias? Circulation 2003, 108, 1784-
1789, doi:10.1161/01.CIR.0000091402.34219.6C. 
185. 
Zoghi, M. Cardiac memory: do the heart and the brain remember the same? J Interv 
Card Electrophysiol 2004, 11, 177-182. 
186. 
Chakravarthy, S.V.; Ghosh, J. On Hebbian-like adaptation in heart muscle: a proposal for 
'cardiac memory'. Biol Cybern 1997, 76, 207-215. 
187. 
Fusi, S.; Miller, E.K.; Rigotti, M. Why neurons mix: high dimensionality for higher 
cognition. Curr Opin Neurobiol 2016, 37, 66-74, doi:10.1016/j.conb.2016.01.010. 
188. 
Quach, K.T.; Chalasani, S.H. Flexible reprogramming of Pristionchus pacificus motivation 
for attacking Caenorhabditis elegans in predator-prey competition. Curr Biol 2022, 32, 
1675-1688 e1677, doi:10.1016/j.cub.2022.02.033. 
189. 
Baars, B.J.; Franklin, S. An architectural model of conscious and unconscious brain 
functions: Global Workspace Theory and IDA. Neural Netw 2007, 20, 955-961, 
doi:10.1016/j.neunet.2007.09.013. 
190. 
Dorahy, M.J.; Brand, B.L.; Şar, V.; Krüger, C.; Stavropoulos, P.; Martínez-Taboas, A.; 
Lewis-Fernández, R.; Middleton, W. Dissociative identity disorder: An empirical 
overview. Australian & New Zealand Journal of Psychiatry 2014, 48, 402-417, 
doi:10.1177/0004867414527523. 
191. 
Dodd, A.N.; Kudla, J.; Sanders, D. The language of calcium signaling. Annu Rev Plant Biol 
2010, 61, 593-620, doi:10.1146/annurev-arplant-070109-104628. 
192. 
De Pitta, M.; Volman, V.; Levine, H.; Ben-Jacob, E. Multimodal encoding in a simplified 
model of intracellular calcium signaling. Cogn Process 2009, 10 Suppl 1, S55-70, 
doi:10.1007/s10339-008-0242-y. 
193. 
Chakraborty, A.; Alam, M.; Dey, V.; Chattopadhyay, A.; Mukhopadhyay, D. A survey on 
adversarial attacks and defences. Caai T Intell Techno 2021, 6, 25-45, 
doi:10.1049/cit2.12028. 
194. 
Martinez-Corral, R.; Liu, J.; Prindle, A.; Suel, G.M.; Garcia-Ojalvo, J. Metabolic basis of 
brain-like electrical signalling in bacterial communities. Philos Trans R Soc Lond B Biol Sci 
2019, 374, 20180382, doi:10.1098/rstb.2018.0382. 
195. 
Lee, D.D.; Galera-Laporta, L.; Bialecka-Fornal, M.; Moon, E.C.; Shen, Z.; Briggs, S.P.; 
Garcia-Ojalvo, J.; Suel, G.M. Magnesium Flux Modulates Ribosomes to Increase Bacterial 
Survival. Cell 2019, 177, 352-360 e313, doi:10.1016/j.cell.2019.01.042. 
196. 
Martinez-Corral, R.; Liu, J.; Suel, G.M.; Garcia-Ojalvo, J. Bistable emergence of 
oscillations in growing Bacillus subtilis biofilms. Proc Natl Acad Sci U S A 2018, 115, 
E8333-E8340, doi:10.1073/pnas.1805004115. 
197. 
Liu, J.; Martinez-Corral, R.; Prindle, A.; Lee, D.D.; Larkin, J.; Gabalda-Sagarra, M.; Garcia-
Ojalvo, J.; Suel, G.M. Coupling between distant biofilms and emergence of nutrient time-
sharing. Science 2017, 356, 638-642, doi:10.1126/science.aah4204. 
198. 
Lee, D.D.; Prindle, A.; Liu, J.; Suel, G.M. SnapShot: Electrochemical Communication in 
Biofilms. Cell 2017, 170, 214-214 e211, doi:10.1016/j.cell.2017.06.026. 
199. 
Levin, M. Life, death, and self: Fundamental questions of primitive cognition viewed 
through the lens of body plasticity and synthetic organisms. Biochemical and Biophysical 
Research Communications 2020, 564, 114-133, doi:10.1016/j.bbrc.2020.10.077. 

 
39
200. 
Fields, C.; Levin, M. Competency in Navigating Arbitrary Spaces as an Invariant for 
Analyzing Cognition in Diverse Embodiments. Entropy (Basel) 2022, 24, 
doi:10.3390/e24060819. 
201. 
Jablonka, E.; Lamb, M.J.; Zeligowski, A. Evolution in four dimensions : genetic, epigenetic, 
behavioral, and symbolic variation in the history of life, Revised edition ed.; A Bradford 
Book, The MIT Press: Cambridge, Massachusetts ; London, England, 2014; pp. xii, 563 
pages. 
202. 
Jablonka, E.; Lamb, M.J.; Avital, E. 'Lamarckian' mechanisms in darwinian evolution. 
Trends Ecol. Evol. 1998, 13, 206-210, doi:10.1016/S0169-5347(98)01344-5. 
203. 
Blackiston, D.; Lederer, E.K.; Kriegman, S.; Garnier, S.; Bongard, J.; Levin, M. A cellular 
platform for the development of synthetic living machines. Sci Robot 2021, 6, eabf1571, 
doi:10.1126/scirobotics.abf1571. 
204. 
Kouvaris, K.; Clune, J.; Kounios, L.; Brede, M.; Watson, R.A. How evolution learns to 
generalise: Using the principles of learning theory to understand the evolution of 
developmental organisation. PLoS computational biology 2017, 13, e1005358, 
doi:10.1371/journal.pcbi.1005358. 
205. 
Zliobaite, I.; Stenseth, N.C. Improving Adaptation through Evolution and Learning: A 
Response to Watson and Szathmary. Trends Ecol Evol 2016, 31, 892-893, 
doi:10.1016/j.tree.2016.10.007. 
206. 
Watson, R.A.; Szathmary, E. How Can Evolution Learn? Trends Ecol Evol 2016, 31, 147-
157, doi:10.1016/j.tree.2015.11.009. 
207. 
Watson, R.A.; Mills, R.; Buckley, C.L.; Kouvaris, K.; Jackson, A.; Powers, S.T.; Cox, C.; 
Tudge, S.; Davies, A.; Kounios, L.; et al. Evolutionary Connectionism: Algorithmic 
Principles Underlying the Evolution of Biological Organisation in Evo-Devo, Evo-Eco and 
Evolutionary Transitions. Evol Biol 2016, 43, 553-581, doi:10.1007/s11692-015-9358-z. 
208. 
Livnat, A.; Papadimitriou, C. Evolution and Learning: Used Together, Fused Together. A 
Response to Watson and Szathmary. Trends Ecol Evol 2016, 31, 894-896, 
doi:10.1016/j.tree.2016.10.004. 
209. 
Blackiston, D.; Kriegman, S.; Bongard, J.; Levin, M. Biological Robots: Perspectives on an 
Emerging Interdisciplinary Field. arXiv 2022, doi:10.48550/arXiv.2207.00880. 
210. 
Sullivan, K.G.; Emmons-Bell, M.; Levin, M. Physiological inputs regulate species-specific 
anatomy during embryogenesis and regeneration. Commun Integr Biol 2016, 9, 
e1192733, doi:10.1080/19420889.2016.1192733. 
211. 
Emmons-Bell, M.; Durant, F.; Hammelman, J.; Bessonov, N.; Volpert, V.; Morokuma, J.; 
Pinet, K.; Adams, D.S.; Pietak, A.; Lobo, D.; et al. Gap Junctional Blockade Stochastically 
Induces Different Species-Specific Head Anatomies in Genetically Wild-Type Girardia 
dorotocephala Flatworms. Int J Mol Sci 2015, 16, 27865-27896, 
doi:10.3390/ijms161126065. 
212. 
Levin, M.; Dennett, D.C. Cognition all the way down. Aeon 2020. 
213. 
Levin, M. The Computational Boundary of a “Self”: Developmental Bioelectricity Drives 
Multicellularity and Scale-Free Cognition. Front Psychol 2019, 10, 2688, 
doi:10.3389/fpsyg.2019.02688. 
214. 
Jakobi, N.; Husbands, P.; Harvey, I. Noise and the reality gap: The use of simulation in 
evolutionary robotics. In Advances in Artificial Life. ECAL 1995, Morán, F., Moreno, A., 

 
40
Merelo, J.J., Chacón, P., Eds.; Lecture Notes in Computer Science; Springer: Berlin, 
Heidelberg, 1995; pp. 704-720. 
215. 
Tobin, J.; Fong, R.; Ray, A.; Schneider, J.; Zaremba, W.; Abbeel, P. Domain randomization 
for transferring deep neural networks from simulation to the real world. In Proceedings 
of the 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), 
Vancouver, BC, Canada, 2017; pp. 23-30. 
216. 
Levin, M. Collective Intelligence of Morphogenesis as a Teleonomic Process. In 
Teleology, Corning, P.A., Ed.; MIT Press: Cambridge, 2023. 
217. 
Blackiston, D.J.; Levin, M. Ectopic eyes outside the head in Xenopus tadpoles provide 
sensory data for light-mediated learning. The Journal of experimental biology 2013, 216, 
1031-1040, doi:10.1242/jeb.074963. 
218. 
Vandenberg, L.N.; Adams, D.S.; Levin, M. Normalized shape and location of perturbed 
craniofacial structures in the Xenopus tadpole reveal an innate ability to achieve correct 
morphology. Developmental Dynamics 2012, 241, 863-878, doi:10.1002/dvdy.23770. 
219. 
Blackiston, D.J.; Vien, K.; Levin, M. Serotonergic stimulation induces nerve growth and 
promotes visual learning via posterior eye grafts in a vertebrate model of induced 
sensory plasticity. npj Regenerative Medicine 2017, 2, 8, doi:10.1038/s41536-017-0012-
5. 
220. 
Blackiston, D.J.; Anderson, G.M.; Rahman, N.; Bieck, C.; Levin, M. A novel method for 
inducing nerve growth via modulation of host resting potential: gap junction-mediated 
and serotonergic signaling mechanisms. Neurotherapeutics 2015, 12, 170-184, 
doi:10.1007/s13311-014-0317-7. 
221. 
Davies, J.; Levin, M. Synthetic morphology via active and agential matter. Nature 
Bioengineering 2022, doi:10.31219/osf.io/xrv8h. 
222. 
Kauffman, S.A. The origins of order : self organization and selection in evolution; Oxford 
University Press: New York, 1993; pp. xviii, 709. 
223. 
Alvarez-Buylla, E.R.; Balleza, E.; Benitez, M.; Espinosa-Soto, C.; Padilla-Longoria, P. Gene 
regulatory network models: a dynamic and integrative approach to development. SEB 
Exp Biol Ser 2008, 61, 113-139. 
224. 
Sauka-Spengler, T.; Bronner-Fraser, M. A gene regulatory network orchestrates neural 
crest formation. Nat Rev Mol Cell Biol 2008, 9, 557-568. 
225. 
Peter, I.S.; Davidson, E.H. Evolution of gene regulatory networks controlling body plan 
development. Cell 2011, 144, 970-985, doi:10.1016/j.cell.2011.02.017. 
226. 
Abramson, C.I.; Levin, M. Behaviorist approaches to investigating memory and learning: 
A primer for synthetic biology and bioengineering. Commun Integr Biol 2021, 14, 230-
247, doi:10.1080/19420889.2021.2005863. 
227. 
Suel, G.M.; Garcia-Ojalvo, J.; Liberman, L.M.; Elowitz, M.B. An excitable gene regulatory 
circuit induces transient cellular differentiation. Nature 2006, 440, 545-550, 
doi:10.1038/nature04588. 
228. 
Ho, C.; Morsut, L. Novel synthetic biology approaches for developmental systems. Stem 
Cell Rep 2021, 16, 1051-1064, doi:10.1016/j.stemcr.2021.04.007. 
229. 
Toda, S.; Blauch, L.R.; Tang, S.K.Y.; Morsut, L.; Lim, W.A. Programming self-organizing 
multicellular structures with synthetic cell-cell signaling. Science 2018, 361, 156-162, 
doi:10.1126/science.aat0271. 

 
41
230. 
Chater, N. The Mind is Flat: The Illusion of Mental Depth and The Improvised Mind; Yale 
University Press: New Haven; London, 2018. 
231. 
Nasser, A.; Al Haj Hassan, H.; Abou Chaaya, J.; Mansour, A.; Yao, K.C. Spectrum Sensing 
for Cognitive Radio: Recent Advances and Future Challenge. Sensors (Basel) 2021, 21, 
2408, doi:10.3390/s21072408. 
232. 
Scholl, B.J.; Tremoulet, P.D. Perceptual causality and animacy. Trends Cogn Sci 2000, 4, 
299-309, doi:10.1016/s1364-6613(00)01506-0. 
233. 
Bloom, P.; Veres, C. The perceived intentionality of groups. Cognition 1999, 71, B1-9, 
doi:10.1016/s0010-0277(99)00014-1. 
234. 
Gunji, Y.P.; Nishiyama, Y.; Adamatzky, A. Robust soldier crab ball gate. arXiv 2012, 
doi:10.1063/1.3637777. 
235. 
Adamatzky, A.; Erokhin, V.; Grube, M.; Schubert, T.; Schumann, A. Physarum Chip 
Project: Growing Computers from Slime Mould. International Journal of Unconventional 
Computing 2012, 8, 319-323. 
236. 
Adamatzky, A. A brief history of liquid computers. Philos Trans R Soc Lond B Biol Sci 
2019, 374, 20180372, doi:10.1098/rstb.2018.0372. 
237. 
Rendell, P. Turing Universality of the Game of Life. In Collision-Based Computing, 
Adamatzky, A., Ed.; Springer: London, 2002; pp. 513-539. 
238. 
Boudry, M.; Pigliucci, M. The mismeasure of machine: Synthetic biology and the trouble 
with engineering metaphors. Stud Hist Philos Biol Biomed Sci 2013, 44, 660-668, 
doi:10.1016/j.shpsc.2013.05.013. 
239. 
Witzany, G.; Baluska, F. Life's code script does not code itself. The machine metaphor for 
living organisms is outdated. EMBO reports 2012, 13, 1054-1056, 
doi:10.1038/embor.2012.166. 
240. 
Bayne, T. On the axiomatic foundations of the integrated information theory of 
consciousness. Neurosci Conscious 2018, 2018, niy007, doi:10.1093/nc/niy007. 
241. 
Kaspar, C.; Ravoo, B.J.; van der Wiel, W.G.; Wegner, S.V.; Pernice, W.H.P. The rise of 
intelligent matter. Nature 2021, 594, 345-355, doi:10.1038/s41586-021-03453-y. 
242. 
Fields, C.; Levin, M. Competency in Navigating Arbitrary Spaces: Intelligence as an 
Invariant for Analyzing Cognition in Diverse Embodiments. Entropy-Switz 2022, in 
review. 
243. 
Koseska, A.; Bastiaens, P.I. Cell signaling as a cognitive process. EMBO J 2017, 36, 568-
582, doi:10.15252/embj.201695383. 
244. 
McNerney, M.P.; Doiron, K.E.; Ng, T.L.; Chang, T.Z.; Silver, P.A. Theranostic cells: 
emerging clinical applications of synthetic biology. Nat Rev Genet 2021, 22, 730-746, 
doi:10.1038/s41576-021-00383-3. 
245. 
Hofstadter, D.R. Godel, Escher, Bach : an eternal golden braid; Basic Books: New York, 
1979; pp. xxi, 777 p. : ill. ; 724 cm. 
246. 
Vandenberg, L.N.; Morrie, R.D.; Adams, D.S. V-ATPase-dependent ectodermal voltage 
and pH regionalization are required for craniofacial morphogenesis. Dev Dyn 2011, 240, 
1889-1904, doi:10.1002/dvdy.22685. 
 

