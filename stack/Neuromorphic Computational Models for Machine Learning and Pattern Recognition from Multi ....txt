Design and Creative Technologies
School of Engineering, Computer and Mathematical Sciences
Knowledge Engineering and Discovery Research Institute (KEDRI)
Neuromorphic Computational Models for
Machine Learning and Pattern
Recognition from Multi-modal Time-series
Data
Neelava Sengupta
A thesis submitted to Auckland University of Technology in fulﬁllment of the
requirements for the degree of Doctor of Philosophy
Supervisors
Nikola Kasabov, Denise Taylor and Grace Wang

Neelava Sengupta
Neuromorphic Computational Models for Machine Learning and Pattern Recognition from
Multi-modal Time-series Data
Computer and Information Sciences, July 4, 2018
Supervisors: Nikola Kasabov, Denise Taylor and Grace Wang
Auckland University of Technology
Knowledge Engineering and Discovery Research Institute (KEDRI)
School of Engineering, Computer and Mathematical Sciences
Design and Creative Technologies
55 Wellesley St. E
1010 Auckland

Abstract
The ﬁelds of neuroscience and artiﬁcial intelligence have a long and entwined
history. In recent times, however, communication and collaboration between
the two ﬁelds has become a rarity as they have evolved. Written in the era when
artiﬁcial intelligence and deep learning is revolutionising the world, this thesis
revisits and searches for inspiration from biological intelligence. The efﬁciency
and accuracy with which the human brain processes incoming stimulus (data) in
millisecond resolution using remarkably low power is unprecedented. Motivated
by this very capability in the generic sense, this thesis has focused on developing
neurobiologically inspired computational models known as spiking neural net-
works to tackle multi-modal time-series data. In a more deﬁnitive formalisation,
this work has aimed to answer three research questions:
1. How to optimally design an implementation of neuromorphic architecture
which is capable of processing large volumes of spatio-temporal data? To
answer this research question, the unsupervised SNNc algorithm (as part
of NeuCube architecture) were studied and numerous designs of the SNNc
graph was analysed in regards to storage and execution time complexities.
Further, the study was extended to include an analysis of the software
design principles for achieving modularity and heterogeneity. The design
principles formalised here are implemented in the NeuCube software pub-
licly available from www.kedri.aut.ac.nz/neucube. The design principles
proposed in the study are also utilised in other parts of this work.
2. How to perform neural encoding on real-world data to represent informa-
tion as spike-timings? This topic has been analysed from the viewpoint of
data compression and information theory. To answer this research ques-
iii

tion, the focus was on using temporal encoding as a framework for concise
representation of large volumes of data. Apart from comparing the state
of the art temporal encoding algorithms from literature, a novel a priori
knowledge driven temporal encoding framework was formalised and an al-
gorithmic realisation of it for fMRI data, called GAGamma, was proposed.
The temporal encoding algorithms on benchmark fMRI data was experi-
mentally evaluated to demonstrate its superiority to succinctly represent
the discriminatory information in the data without appreciable informa-
tion loss. It was also demonstrated that the proposed encoding framework
provides enhanced ﬂexibility to include a priori knowledge of the data
source and thus, provide the compression/encoding algorithms sufﬁcient
redundancy to compress large datasets in an optimally concise manner.
3. How to recognise patterns from multi-modal data with spatial, temporal
and orientation information using neuromorphic architectures? To answer
this research question, a novel, unsupervised learning algorithm, namely
oiSTDP learning algorithm, was proposed for fusing temporal, spatial and
orientation information in a spiking neural network architecture. Further-
more, a case study is presented on building a computational model that
discriminates between people with schizophrenia who respond or do not
respond to mono-therapy with the anti-psychotic clozapine. The perfor-
mance of the proposed algorithm as part of the modiﬁed NeuCube frame-
work has demonstrated superiority against the state of the art deep learn-
ing algorithm not only in prediction performance but also degree of inter-
pretability.
Overall, through this work, the researcher has presented several novel approaches
of recognising patterns in time series data using spike-timings as the unit of data
representation in the neuromorphic computation framework. The several soft-
ware design challenges that arose due to the nature of neuromorphic computa-
tion framework has also been addressed in this thesis.
iv

Contents
1
Introduction
1
1.1
Rationale and Motivation
. . . . . . . . . . . . . . . . . . . . . .
2
1.2
Aims of this Thesis and Research Questions
. . . . . . . . . . . .
4
1.3
Thesis Structure, Navigation and List of Peer-reviewed Publications
5
2
Literature Review on Brain Data Modelling and Machine Learning
9
2.1
A Review on Modelling Brain Data
. . . . . . . . . . . . . . . . .
9
2.2
A Review of Machine Learning . . . . . . . . . . . . . . . . . . . .
14
3
From the Human Brain to Artiﬁcial Evolving Spiking Neural Networks 22
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3.2
The Human Brain and the Morphology of a Neuron . . . . . . . .
22
3.2.1
Neural Encoding and Information Communication . . . . .
24
3.3
Generations of Artiﬁcial Neuron . . . . . . . . . . . . . . . . . . .
28
3.3.1
First Generation Neurons: . . . . . . . . . . . . . . . . . .
29
3.3.2
Second Generation Neurons: . . . . . . . . . . . . . . . . .
30
3.3.3
Third Generation Neurons . . . . . . . . . . . . . . . . . .
31
3.4
Spiking Neuron Models . . . . . . . . . . . . . . . . . . . . . . . .
31
3.4.1
Integrate and Fire . . . . . . . . . . . . . . . . . . . . . . .
34
3.4.2
Hodgkin-Huxley . . . . . . . . . . . . . . . . . . . . . . . .
36
3.4.3
Izhikevich . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.5
Spiking Neural Networks . . . . . . . . . . . . . . . . . . . . . . .
39
3.6
Neuromorphic Computing Beyond von Neumann Hardware Archi-
tecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.7
Brief Review of SNN Software Implementations . . . . . . . . . .
46
3.8
Evolving Connectionist System
. . . . . . . . . . . . . . . . . . .
48
3.8.1
Principles of ECOS
. . . . . . . . . . . . . . . . . . . . . .
49
v

3.8.2
Application of eSNN in Forecasting Stock Price Movement
50
3.8.3
Application of eSNN in Cyber Fraud Detection . . . . . . .
55
3.9
Contributions and Publications
. . . . . . . . . . . . . . . . . . .
58
4
A Review of NeuCube: An Evolving Spatio-Temporal Data Machine
Framework
60
4.1
Recurrent Neural Networks
. . . . . . . . . . . . . . . . . . . . .
62
4.1.1
What Makes RNN Effective? . . . . . . . . . . . . . . . . .
63
4.2
Formalisation of the Temporal Learning Problem . . . . . . . . . .
64
4.3
Reservoir Computing and Liquid State Machines . . . . . . . . . .
65
4.3.1
Liquid State Machines
. . . . . . . . . . . . . . . . . . . .
66
4.4
NeuCube Evolving Spatio-temporal Data Machine . . . . . . . . .
67
4.4.1
Encoding
. . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.4.2
SNNc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.4.3
Readout . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.5
Departure of NeuCube from LSM . . . . . . . . . . . . . . . . . .
71
4.6
Software Design Architecture of the NeuCube Framework
. . . .
72
4.6.1
NeuCube for Generic Prototyping and Testing of Applications 75
4.6.2
NeuCube Based Spatio-temporal Data Research and Appli-
cations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.7
Contributions and Publications
. . . . . . . . . . . . . . . . . . .
82
5
Data Structure Optimisation and Software Architecture Design for
the Implementation of Large-Scale Spiking Neural Networks
83
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
5.2
Brain vs. Digital Computers
. . . . . . . . . . . . . . . . . . . . .
84
5.3
NeuCube SNNc . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.3.1
SNNc Architecture, Mapping and Initialisation Scheme . .
86
5.3.2
Neuron Dynamics Model . . . . . . . . . . . . . . . . . . .
87
5.3.3
Unsupervised Weight Adaptation in SNNc
. . . . . . . . .
91
5.3.4
Formal Description of SNNc Unsupervised Learning Algo-
rithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.4
Analysis of the Data Structure Representations of SNNc . . . . . .
98
vi

5.4.1
Large-scale Unsupervised Learning of SNNc Using the Ad-
jacency Forward-backward List
. . . . . . . . . . . . . . . 104
5.5
Considerations for Modularity and Heterogeneity: Towards Graph
Based Software Design of SNNc . . . . . . . . . . . . . . . . . . . 106
5.6
Summary and Conclusion
. . . . . . . . . . . . . . . . . . . . . . 110
5.7
Contributions and Publications
. . . . . . . . . . . . . . . . . . . 111
6
A Novel a priori Knowledge Driven Temporal Encoding Framework
for Compressing and Recognising Pattern on Temporal Data
112
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.2
Data Compression and Inspirations from Neural Coding . . . . . . 113
6.2.1
Information Theory and Data Compression . . . . . . . . . 114
6.3
Literature Review on Analog to Digital Data Transformation Algo-
rithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.4
A General Framework of Spike-time Encoding and Compression
for Temporal Data Sequences
. . . . . . . . . . . . . . . . . . . . 117
6.4.1
Formalisation of the a priori Knowledge Driven Optimisa-
tion Problem for Data Encoding . . . . . . . . . . . . . . . 117
6.5
GAGamma: A Spike-time Encoding and Compression Method for
fMRI Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.5.1
fMRI As a Linear Time Invariant System
. . . . . . . . . . 122
6.5.2
GAGamma Optimisation Problem for fMRI . . . . . . . . . 123
6.5.3
Distinction of GAGamma from HSA and BSA . . . . . . . . 124
6.6
Experiments and Evaluation . . . . . . . . . . . . . . . . . . . . . 125
6.6.1
Description of Dataset
. . . . . . . . . . . . . . . . . . . . 125
6.6.2
Evaluation Metrics
. . . . . . . . . . . . . . . . . . . . . . 126
6.6.3
Design of Experiments . . . . . . . . . . . . . . . . . . . . 129
6.6.4
Baseline Encoding Algorithms . . . . . . . . . . . . . . . . 130
6.6.5
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
6.7
Summary and Conclusion
. . . . . . . . . . . . . . . . . . . . . . 143
6.8
Contributions and Publications
. . . . . . . . . . . . . . . . . . . 144
vii

7
Orientation Inﬂuence Driven Spike-Time Dependent Plasticity Learn-
ing: A Novel Unsupervised Learning Algorithm for Integrating Spa-
tial, Temporal and Orientation Information
145
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.2
fMRI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.3
DTI and Tractography
. . . . . . . . . . . . . . . . . . . . . . . . 149
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Ori-
entation Information . . . . . . . . . . . . . . . . . . . . . . . . . 152
7.4.1
Formalisation of the Machine Learning Problem . . . . . . 153
7.4.2
NeuCube Personalised SNNc Architecture . . . . . . . . . . 153
7.4.3
Multi-modal Information Integration in SNNc Using Orien-
tation and Spike-time Data . . . . . . . . . . . . . . . . . . 158
7.4.4
Orientation Inﬂuence Driven STDP (oiSTDP) Learning in
SNNc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
7.5
Analysis of oiSTDP Algorithm Using Synthetic Data . . . . . . . . 171
7.5.1
Effect of the Orientation Information on SNNc . . . . . . . 173
7.5.2
Effect of the spike synchronicity on SNNc . . . . . . . . . . 173
7.6
Personalised Predictive Modelling of Treatment Outcome using
NeuCube with oiSTDP Learning . . . . . . . . . . . . . . . . . . . 175
7.6.1
Schizophrenia and Antipsychotics . . . . . . . . . . . . . . 175
7.6.2
Treatment Outcome in People with Schizophrenia . . . . . 177
7.6.3
Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
7.7
Summary and Conclusion
. . . . . . . . . . . . . . . . . . . . . . 190
7.8
Contributions and Publications
. . . . . . . . . . . . . . . . . . . 192
8
Conclusion
193
8.1
Research Questions and Contributions
. . . . . . . . . . . . . . . 193
8.1.1
How to Design Architectures of SNN that are Capable of
Digesting and Processing Large Volumes and Variety of
Spatio-temporal Data? . . . . . . . . . . . . . . . . . . . . 193
8.1.2
How to Perform Neural Encoding on Real-world Data to
Represent Information as Timings of Spikes? . . . . . . . . 195
viii

8.1.3
How to Integrate Spatial, Temporal and Orientation Infor-
mation Present in Multi-modal Brain Data using SNN Ar-
chitecture?
. . . . . . . . . . . . . . . . . . . . . . . . . . 197
8.2
Limitations of the Thesis . . . . . . . . . . . . . . . . . . . . . . . 198
8.3
Future Direction and Closing remarks . . . . . . . . . . . . . . . . 199
Bibliography
201
A Sequential eSNN Architectures for Cyber Fraud Detection
222
A.1 Hyperparameter Selection of eSNN Classiﬁer . . . . . . . . . . . . 227
A.2 Testing Method of eSNN Classiﬁer . . . . . . . . . . . . . . . . . . 228
B eSNN Architectures for Stock Price Movement Forecast
230
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock
Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
B.1.1
Overall Architecture
. . . . . . . . . . . . . . . . . . . . . 231
B.1.2
Neural Encoding
. . . . . . . . . . . . . . . . . . . . . . . 232
B.1.3
Neural Model . . . . . . . . . . . . . . . . . . . . . . . . . 234
B.1.4
Learning in the Output Neurons . . . . . . . . . . . . . . . 234
B.1.5
Algorithm for eSNN Training . . . . . . . . . . . . . . . . . 235
B.1.6
Testing (Recall of the Model on New Data) . . . . . . . . . 236
B.2
The CUDA-SI-eSNN Model: A Parallel eSNN Model for GPU Ma-
chines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
B.3
Sliding Window (SW)-eSNN for Incremental Learning and Stock
Movement Prediction . . . . . . . . . . . . . . . . . . . . . . . . . 237
C Miscellaneous Code Snippets
241
C.1 Matlab Code for the SNNc Unsupervised Learning Using Forward
Adjacency Lists for Connection C and Weight W . . . . . . . . . . 241
C.2 Matlab Code for the SNNc Unsupervised Learning Using Forward-
backward Adjacency Lists for Connection C and Weight W . . . . 243
C.3 Python Code for the Modular Implementation of the SNNc . . . . 245
D Third Party Copyright Materials
256
E
Ethical Approval of TRS Study
265
ix

List of Figures
1.1
Bird’s eye view of thesis components and their inter-dependence.
.
6
2.1
A decision tree model built on the Iris dataset (Fisher, 1936) (source
Girones (n.d.)). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
The ADALINE single layer perceptron model and an example of lin-
early separable problem (source Widrow and Lehr (1990)).
. . . .
18
2.3
A fully connected feed-forward multi layer perceptron model (source
Widrow and Lehr (1990)). . . . . . . . . . . . . . . . . . . . . . . .
19
2.4
Maximum-margin hyperplane and margins for an SVM trained with
samples having two features from two classes (source Cortes and
Vapnik (1995)). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.1
Morphology of a biological neuron (source Carlson (1967)). . . . .
23
3.2
Dynamics of the membrane potential of a typical neuron (source
“Afterhyperpolarization” (2007)). . . . . . . . . . . . . . . . . . . .
25
3.3
Experimental results obtained by Hubel and Wiesel (1962). The
orientation of light bars presented at various orientations are shown
on the left and the spike responses are shown on the right (source
Hubel and Wiesel (1959)). . . . . . . . . . . . . . . . . . . . . . . .
27
3.4
Block diagram showing components of an artiﬁcial neuron. . . . . .
29
3.5
Electrical properties of the passive membrane of an IF neuron. (A)
A cell membrane enclosed neuron receives a positive input current
I(t) resulting in increase of electrical charge inside the cell. The
cell membrane acts as a capacitor in parallel with a resistor which
is in line with a battery of potential vrest. (B) The reaction of the
cell membrane to a step current (top) with a smooth voltage trace
(source Gerstner, Kistler, Naud and Paninski (2014)). . . . . . . . .
35
x

3.6
Comparison of spiking neuron models in the evaluation landscape
of biological plausibility and implementation cost (source Izhike-
vich (2004)).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
4.1
Node level architectures of popular neural networks (source van
Veen (2016)). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.2
Unrolled RNN (source Olah (2015)). . . . . . . . . . . . . . . . . .
63
4.3
Mapping capabilities of RNN (source Karpathy (2015)). . . . . . . .
63
4.4
Schematic diagram of a feed forward neural network vs. a reservoir
computing system (source Abel and Fompeyrin (n.d.)). . . . . . . .
65
4.5
Principle of reservoir computing: The input states are transformed
into a high-dimensional feature space where classiﬁcation can be
performed with linear operation (source Abel and Fompeyrin (n.d.)). 66
4.6
Schematic architecture diagram of NeuCube. . . . . . . . . . . . . .
68
4.7
Block diagram of the multi-modular NeuCube software architecture.
73
4.8
Standard conﬁguration of the NeuCube software architecture con-
sisting of the prototyping and testing module, immersive visualisa-
tion module and the SpiNNaker neuromorphic hardware chip. . . .
74
4.9
NeuCube generic prototyping and testing software user interface
and panel descriptions. . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.10
Algorithmic control UI panels. . . . . . . . . . . . . . . . . . . . . .
77
4.11
Neuron cluster analysis by network analysis toolbox (A)Neuron clus-
tering based on connection weights of the network (B)average one
to one interaction between the input nodes. The thicker lines signi-
ﬁes more interaction. . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.1
A typical connectivity conﬁguration of a spiking neuron i.
. . . . .
88
5.2
Plot of the PSP trace ϵ0 as a function of t −tf. This ﬁgure plots the
PSP simulation for different τm. . . . . . . . . . . . . . . . . . . . .
89
xi

5.3
Plot of the membrane potential traces(vi) of a neuron i simulated
over T = 200 time points using the SRM model.
For the simu-
lations, 3 predecessor neurons were connected to a spiking neuron.
The spike data from the predecessor neurons are sampled randomly
from uniform random distribution. The ηthr for the spiking neuron
was set to 10. Each of the three vi traces correspond to a preset vthr
mentioned in the label. . . . . . . . . . . . . . . . . . . . . . . . . .
91
5.4
Plot showing the functional dependence of the spike-time depen-
dent plasticity learning rule. The STDP function shows the change
of synaptic weights ∆w as a function of difference in post and pre-
synaptic spike-time difference. . . . . . . . . . . . . . . . . . . . . .
92
5.5
Plot of modiﬁed spike-time dependent plasticity learning rule im-
plemented in SNNc learning algorithm. The STDP function shows
the change of synaptic weights ∆w as a function of difference in last
and present spike-time difference. . . . . . . . . . . . . . . . . . . .
95
5.6
Example of the adjacency matrix representation. . . . . . . . . . . .
99
5.7
Example of the edge-weight table representation. . . . . . . . . . . 100
5.8
Example of the adjacency forward list representation. . . . . . . . . 101
5.9
Example of the adjacency forward-backward list representation. . . 102
5.10
Storage space comparison of different data structures with increas-
ing number of neurons. . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.11
Execution time comparison of different data structures with increas-
ing number of neurons. . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.12
Snapshot of the spike activity at a time instant inside the SNNc. . . 104
5.13
Strongest connections learned by the SNNc at the end of SNNc sim-
ulation.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.14
UML Class diagram of the template based object-oriented architec-
ture of the SNNc. This architecture should be considered an ex-
ample architecture for incorporating modularity. A python imple-
mentation of the SNNc following this architecture is presented in
Appendix C. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.1
Flowchart depicting the evaluation criteria and experimental proto-
col used in this research. . . . . . . . . . . . . . . . . . . . . . . . . 129
xii

6.2
Plot illustrating the comparison of the quality of the encoding meth-
ods with respect to the mean classiﬁcation accuracy and bit com-
pression ratio across the two subjects. The horizontal and the verti-
cal error bars represent the standard deviation of accuracy and BCR
across experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
6.3
Comparison of signal reconstruction (ˆs) from a spike sequence by
GAGamma-16 decoding algorithm (Equation 6.3), BSA decoding al-
gorithm (Algorithm 6.2) and Temporal contrast decoding algorithm
(Algorithm 6.4). The true signal is randomly selected from subject
04847 (10th trial and 23rd voxel). . . . . . . . . . . . . . . . . . . . 137
6.4
Comparison of RMSE of reconstruction between GAGamma, BSA
and Temporal Contrast across 100 voxels of second trial in subject
04847. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5
Comparison of haemodynamic response function learned by GAGamma
encoding method for voxel 8 of subject 04847 across 7 different trials.138
6.6
Comparative analysis of spike frequencies of the subject 04847 see-
ing picture vs. reading a sentence. . . . . . . . . . . . . . . . . . . . 141
6.7
Comparative analysis of spike frequencies of the subject 07510 see-
ing picture vs. reading a sentence. . . . . . . . . . . . . . . . . . . . 142
7.1
fMRI scanning device (source NIBIB (2017)).
. . . . . . . . . . . . 147
7.2
Example of fMRI data represented as a sequence of images (source
Quarantelli et al. (2013)). . . . . . . . . . . . . . . . . . . . . . . . 149
7.3
Visual intuition of the ellipsoid representation of a diffusion tensor
voxel.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7.4
Orientation information from DTI image. Left image shows an axial
slice of a single subject’s DTI data, registered to structural and MNI
standard space. The Right image shows a close-up of the right pos-
terior corpus callosum. Directions corresponding to each colour are
as follows: Red - left to right/right to left, green - anterior to poste-
rior/posterior to anterior and blue - superior to inferior/inferior to
superior (source “Medical image computing” (n.d.)). . . . . . . . . 152
7.5
Proposed NeuCube personalised SNNc architecture. . . . . . . . . . 157
xiii

7.6
Plot of the STDP weight update as a function of the relative time
difference of the pre and post synaptic spikes. The data for this plot
was generated using code snippet 7.2 with hyperparameter κ+ =
κ−= 0.5.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
7.7
Example of a pre-synaptic neuron j connected to two post synaptic
neurons i1 and i2. Each neurons spatial location is deﬁned by the
polar coordinates (r, α). . . . . . . . . . . . . . . . . . . . . . . . . 165
7.8
Plot of the elevation inﬂuence ψα as a function of the radial distance
αji −αdti
j
and γ = 8. . . . . . . . . . . . . . . . . . . . . . . . . . . 168
7.9
Graph showing the relationship of oiSTDP weight update ∆w with
post and pre synaptic ﬁring time difference ti −tj and orientation
distance rji. As the temporal difference between neuronal spikes
decreases, the effect on weight updating increases, so that spikes
timed closely together lead to greater increases in weight updating
than spikes timed further apart. The order of spikes also affects
weight updating. If neuron j ﬁres before neuron i consistently, then
the synaptic weight between them continues to increase; however,
if the order switches, the weight is reduced. . . . . . . . . . . . . . 171
7.10
The columns show the 3D, horizontal and coronal view of the strongest
connection weights in the 3D SNNc at the end of the unsupervised
learning. Each row corresponds to a different input orientation data.
Every neuron of the SNN in the ﬁrst row were simulated with ori-
entation data (α = 0°, β = 0°). This resulted in the majority of
the strong connections being oriented in the direction (alpha = 0°,
β = 0°). This shows the systematic bias towards orientation infor-
mation in absence of any dynamic bias. The second row shows simi-
lar systematic bias towards the input orientation (α = 45°, β = 45°).
The third row is the baseline with no orientation inﬂuence where
no systematic orientation information bias can be observed.
. . . . 172
7.11
Comparison of the effect of synchronous input spikes on the SNNc
map generated by Algorithm 7.1.
The blue dots show the syn-
chronous input channels. . . . . . . . . . . . . . . . . . . . . . . . . 174
7.12
Voxel selection using absolute mean standard deviation.
. . . . . . 184
xiv

7.13
A visual comparison of the strongest connections (mean weight
across subjects within a group) formed in the SNNc of the TRS and
the UTRS group. The top row shows the connections in the TRS
group and the bottom row corresponds to the UTRS group. The
yellow coloured cluster represents the input neurons and the green
neurons are the computational spiking neurons. . . . . . . . . . . . 189
A.1
Proposed eSNN architecture for phishing website classiﬁcation prob-
lem.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
A.2
Input neuron model
. . . . . . . . . . . . . . . . . . . . . . . . . . 223
A.3
Intermediate neuron model
. . . . . . . . . . . . . . . . . . . . . . 223
B.1
Architecture of the proposed technical stock indicator SI-eSNN model
for stock price direction prediction
. . . . . . . . . . . . . . . . . . 231
xv

List of Tables
1.1
List of peer-reviewed publications . . . . . . . . . . . . . . . . . . .
8
3.1
A comparison of the key contrasts between von Neumann and neu-
romorphic computing paradigm. . . . . . . . . . . . . . . . . . . . .
42
3.2
Comparison of the key features of the popular neuromorphic sys-
tems and human brain. The details are adapted from (Scott, 2015;
S. Furber, 2016). . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.3
Accuracy and AUC of CUDA-eSNN model with Gaussian Distribu-
tion for different stock indices. . . . . . . . . . . . . . . . . . . . . .
52
3.4
Accuracy and AUC of CUDA-eSNN model with Gaussian Distribu-
tion for different stock indices (Continued).
. . . . . . . . . . . . .
52
3.5
Comparisons of SI-eSNN and CUDA eSNN for the best value of num-
ber of Gaussian receptive ﬁelds. . . . . . . . . . . . . . . . . . . . .
52
3.6
Accuracy and AUC of CUDA-SI-eSNN model using Logistic Distribu-
tion for different stock indices. . . . . . . . . . . . . . . . . . . . . .
53
3.7
Accuracy and AUC of CUDA-SI-eSNN model using Logistic Distribu-
tion for different stock indices (Continued).
. . . . . . . . . . . . .
53
3.8
Comparisons of SI- eSNN and CUDA-SI-eSNN for the best value of
number of Logistic receptive ﬁelds. . . . . . . . . . . . . . . . . . .
53
3.9
p values of McNemar test for the pairwise comparison of perfor-
mance of CUDA-SI-eSNN with Logistic and Gaussian distribution. .
54
3.10
Average AUC score of SW-eSNN Incremental approach using Logis-
tic and Gaussian distributions. . . . . . . . . . . . . . . . . . . . . .
55
3.11
Comparison of performance of eSNN and other iterative and non
iterative machine learning algorithms on the phishing website data.
56
3.12
t-test based model comparison.
. . . . . . . . . . . . . . . . . . . .
56
4.1
Supported ﬁle format for descriptors. . . . . . . . . . . . . . . . . .
76
xvi

5.1
Comparison of time and storage complexity for different data struc-
tures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.1
Comparative evaluation of the data encoding algorithms applied to
subject 04847 and 07510 in the Starplus fMRI dataset. . . . . . . . 133
6.2
Average pairwise asynchronicity of three different voxels at the end
of ten independent runs of GAGamma encoding. . . . . . . . . . . . 140
7.1
Experimental results showing saturation characteristics of canonical
NeuCube SNNc. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
7.2
Frequency table of ROI’s of the selected voxels.
. . . . . . . . . . . 184
7.3
Comparison of the pattern recognition methods on the binary clas-
siﬁcation task. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
B.1
Prediction Techniques and Stock Market Indices for Stock Price Di-
rection Prediction used in Literature
. . . . . . . . . . . . . . . . . 239
B.2
Selected technical indicators and their formulas . . . . . . . . . . . 240
B.3
The number of instances of UP and DOWN categories in Stock Mar-
ket Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
B.4
GPU Speciﬁcations
. . . . . . . . . . . . . . . . . . . . . . . . . . . 240
xvii

List of Algorithms
5.1
STDP based SNNc unsupervised learning algorithm . . . . . . . .
96
5.2
Modiﬁed STDP based SNNc unsupervised learning algorithm . . .
97
6.1
BSA encoding algorithm . . . . . . . . . . . . . . . . . . . . . . . 131
6.2
BSA decoding algorithm . . . . . . . . . . . . . . . . . . . . . . . 131
6.3
Temporal contrast encoding algorithm . . . . . . . . . . . . . . . 133
6.4
Temporal contrast decoding algorithm . . . . . . . . . . . . . . . 133
7.1
oiSTDP-based SNNc learning algorithm . . . . . . . . . . . . . . . 169
B.1
eSNN training algorithm . . . . . . . . . . . . . . . . . . . . . . . 235
B.2
CUDA-SI-eSNN algorithm
. . . . . . . . . . . . . . . . . . . . . . 237
B.3
SW-eSNN learning algorithm
. . . . . . . . . . . . . . . . . . . . 238
xviii

List of code snippets
7.1
Python code for NeuCube modiﬁed SRM neuron model . . . . . . 159
7.2
Python code for calculating dynamic inﬂuence . . . . . . . . . . . 162
7.3
Python code for calculating static orientation inﬂuence . . . . . . 166
C.1
Matlab code for SNNc unsupervised learning algorithm using for-
ward adjacency list data structure for storing connection C and
weight W
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
C.2
Matlab code for SNNc unsupervised learning algorithm using forward-
backward adjacency list data structure for storing connection C
and weight W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
C.3
Python code for modular implementation of SNNc
. . . . . . . . 245
xix

Declaration
I, Neelava Sengupta, declare that this thesis titled, Neuromorphic Computational
Models for Machine Learning and Pattern Recognition from Multi-modal Time-
series Data and the work presented in it are my own. I conﬁrm that:
• This work was done wholly while in candidature for a research degree at
this University.
• Where I have consulted the published work of others, this is always clearly
attributed.
• Where I have quoted from the work of others, the source is always given.
With the exception of such quotations, this thesis is entirely my own work.
• I have acknowledged all main sources of help.
Auckland, July 4, 2018
Neelava Sengupta

Acknowledgement
I would like to take this opportunity to thank all the people without whom this
study would never have been possible. Although it is just my name on the cover,
it has been a collaborative effort as many have in their own special way helped
and for that I want to give them special congratulations.
First, my supervisors Prof. Nikola Kasabov, Prof. Denise Taylor and Dr. Grace
Wang. Nik, you have created the invaluable space for me to do this research
and develop myself as a researcher in the best possible way. I greatly appreciate
the freedom you have given me to ﬁnd my own path, and the guidance and
support you offered when needed. I highly appreciate you continuously pushing
me to make this piece of work better. Denise and Grace, thank you for being
an incredibly valuable part of this journey of mine and providing the guidance
when required. I truly hope that you will give me the opportunity to work even
closer together with you in the future. A special thanks also goes out to my
examiners, Prof. Chrisina Jayne and Prof. Giacomo Indiveri for taking time to
read and examine my thesis.
I would also want to extend my gratitude towards Mrs.
Joyce D’mello, the
administrative manager of KEDRI. Joyce, you are the lifeline of KEDRI and I,
among many, am indebted to you for all that you have done to make this journey
worth. Thank you for helping me keep it all together in difﬁcult times.
Next, some people of outstanding importance for my research. Carolyn, Nathan
and Anne, thank you for accepting to work alongside me, and for having numer-
ous valuable conversations, all of which have shaken parts of my grey matter
xxi

and resulted in this work. Our discussions were always lively, and although gen-
erally longer than planned, have always ended too soon. I hope to continue our
collaborations in the future. I would also like to extend my acknowledgement
to all the KEDRI members, especially Vivienne, Israel, Reggio, Fahad, Muhaini,
Kaushalya, Clarence, Elisa, Maryam, Zohreh, Akshay, and everyone else I have
had an opportunity to interact with. I also wish all of you all the very best for
your future.
I would not be sitting here today writing an acknowledgement for my thesis
without the selﬂess support and encouragement of my family. Shumi, my dear
wife, thank you for supporting me day in and day out throughout the ups and
downs of this journey. This work is more yours than mine. Also, I thank you for
your innumerable iterations of reading and proofreading this text. I am sure your
PhD journey will much more enjoyable and you will always have my support on
yours. Maa and baba, no words can ever describe my gratefulness to you and
all the contributions you have made, quietly, to make this happen. Thank you
for being my parents. I hope this work makes you proud. And of course to my
precious little dogs, Burﬁand Golu, who have always kept me going with all the
love and cuddles. This writing is incomplete without a mention of all my family
and friends in New Zealand, India and around the world who have continuously
inﬂuenced me directly or indirectly over the years.
And ﬁnally, to my late maa and baba. This is for you. This is a gift that you
deserve. May god bless you and may you rest in peace together. You are always
and forever will be in my thoughts and prayers.
xxii

1
Introduction
In the recent past, signiﬁcant progress has been made in neuroscience and arti-
ﬁcial intelligence (AI). With the inception of computers in the early nineteenth
century, progress in AI was inextricably intertwined with neuroscience and psy-
chology, and many of the early pioneers straddled both ﬁelds, with collabora-
tions between these disciplines proving highly productive (Turing, 1950; Hinton,
Mcclelland & Rumelhart, 1986; McCulloch & Pitts, 1943; Hebb, 1949; Church-
land & Sejnowski, 1988). Nevertheless, progress in recent times has seen in-
teractions and collaborations becoming less commonplace, as both ﬁelds have
evolved massively in complexity, and disciplinary boundaries have solidiﬁed.
The development of AI began with the premise that creating human like gen-
eral purpose artiﬁcial intelligence (or Turing-powerful intelligent systems) is an
intimidating task, due to the massive search space of sparsely populated solu-
tions. This underscored the utility of examining the principles of the human
brain which is the only existing proof that such an intelligence is even possi-
ble. As described by Hassabis, Kumaran, Summerﬁeld and Botvinick (2017),
the beneﬁts of developing AI by close scrutinisation of biological intelligence are
two-fold. First, neuroscience provides a rich source of inspiration for new algo-
rithms and architectures, independent of and complementary to the mathemati-
cal and logic-based methods, and ideas that have largely dominated traditional
approaches to AI. For example, where a new facet of biological computation
found to be critical to supporting a cognitive function, then it has been consid-
ered as an excellent candidate for incorporation into artiﬁcial systems. Second,
neuroscience can provide validation of artiﬁcial intelligence techniques that al-
ready exist. If a known algorithm is subsequently found to be implemented in
the brain, then that is strong support for its plausibility as an integral component
of an overall general intelligence system.
1

1.1 Rationale and Motivation
It is without a doubt that in the era of ‘smart’ everything, availability and utility
of massive volumes of digital data plays a substantial role. Historically, data was
used as a part of business and gathered to serve speciﬁc needs. For example,
retailers recorded sales for accounting, manufacturers recorded raw materials
for quality management and the number of mouse clicks on advertising banners
was collected for calculating advertisement revenue. But as the demand for big
data analytics emerged, data no longer served only its initial purpose. Organi-
sations were able to access huge amounts of data and possess a valuable asset
that when combined with the ability to analyse it, has given rise to a whole new
industry.
This thesis began its journey in the middle of a technological revolution in the
year 2014 when the industry was booming with the following technological rev-
olutions:
• Big data: The total volume of digital data has grown into Zetabytes and is
projected to be reaching 180 Zetabytes by the year 2025. This enormous
growth in digital data has brought on an era of "Big data". Big data is a
term applied to data sets whose size or type is beyond the capabilities of
traditional relational databases to capture, manage, and process the data
with low-latency. Also, it has one or more of the following characteristics:
high volume, high velocity, or high variety. Big data traditionally originates
from Internet of Things (IoT) enabled sensors, devices, video/audio, net-
works, log ﬁles, transactional applications, and social media - much of it
generated in real time and on a very large scale.
• Deep learning: Deep learning is a specialised stream of AI which has come
into prominence in the past decade. Deep learning is considered to be
an extension of machine learning, which consists of a set of technologies
around neural networks that empowers computers to learn, evolve and
1.1
Rationale and Motivation
2

improve upon their own learning by reiterating and consistently updat-
ing the data bank through recursive experiments and human intervention.
Deep learning has already revolutionised AI and pushed the boundaries
of intelligent systems through technologies such as autonomous cars, fully
automated language translation and speech transcription systems.
• High performance computing (HPC): Experts have described deep learning
as analogous to a rocketship that needs a really big engine (a model) and a
lot of fuel (the data) in order to go anywhere interesting. A big model that
learns from big data, of course, requires large scale computational capabil-
ities, or, in other words, high performance computing for it to be efﬁcient
and effective. The need for HPC has also meant a massive progress in the
form of cloud computing and infrastructure as a service providing capa-
bility to scale infrastructure requirements (computation, storage, network
speed) on a demand basis.
The state-of-the-art of AI is composed of big data, big models and big infrastruc-
ture where solutions are traditionally developed by stacking up resources in the
form of multiple instances of computers (physical and/or logical). The main mo-
tivation and/or question that drives the present research in this domain is the
following: "Is stacking up computers an ideal solution for the future direction
of AI?" This question led to the revisiting of the ﬁeld of neuroscience seeking
inspiration from biological intelligence for efﬁcient and accurate computing.
An important distinction of the human brain from AI is its ability to operate
at full capacity using orders of magnitude lower power. This is a rather subtle
hint that the human brain does not scale by stacking up computational units.
Rather, the efﬁciency is due to the ability of the human brain to represent sen-
sory information as electrical impulses. The idea of information representation
as electrical impulses is at the base of this thesis. The present work concen-
trates on developing algorithms for pattern recognition, not by stacking layers
of computational units and infrastructure, rather by developing algorithms for
1.1
Rationale and Motivation
3

representing, processing and recognising patterns in big data, especially time-
series data, efﬁciently and accurately.
1.2 Aims of this Thesis and Research
Questions
On the basis of the rationale and motivations discussed previously, this work
constrains its focus on the development of neuromorphic computational models
for recognising patterns using multi-modal time-series data.
1. Neuromorphic models: This thesis concentrates on developing neuro-biology
inspired computational models known as spiking neural networks for recog-
nising patterns in data. The most important characteristic ensuring neuro-
morphic behaviour is the ability to represent, transform and process spike-
time data.
2. Multi-modal time-series data: As it has been brieﬂy described earlier, the
concept of big data is characterised not only by volume and velocity, but
also very importantly, by variety. The data sources in the era of IoT are di-
verse, leading to multi-modal data with a variety of spatio-temporal prop-
erties. The aim is to develop technologies that has the capacities to deal
with such multi-modal data sources.
3. Time-series, especially brain data: The studies in this thesis are also lim-
ited to experiments performed on time-series data, especially, non-invasive
brain data, such as functional Magnetic Resonance Imaging (fMRI), Elec-
troencephalography (EEG), and Diffusion Tensor Imaging (DTI). The brain
activity data is particularly useful as data sources of multi-modal nature
with prominent impact on healthcare for several brain related diseases.
This will be discussed in detail later.
1.2
Aims of this Thesis and Research Questions
4

Keeping the above mentioned constraints in mind, this research aims to answer
the following more in-depth research questions:
1. Research question 1: How to design architectures of spiking neural net-
works that are capable of digesting and processing large volumes of spatio-
temporal data? This research question focuses on software design prin-
ciples, including the data structure design, modularity and scalability as-
pects of spiking neural network architectures.
2. Research question 2: How to perform neural encoding on real-world data
to represent information as timings of spikes? This research question can
be observed in the light of information theory and theories of lossy data
compression.
3. Research question 3: How to integrate spatial, temporal and orientation
information present in multi-modal brain data using spiking neural net-
work architecture?
1.3 Thesis Structure, Navigation and List of
Peer-reviewed Publications
This thesis is presented in eight Chapters. Figure 1.1 depicts a self-explanatory
bird’s eye view of the relationships between the different components of interest
of this thesis. It is highly recommended to review this ﬁgure before proceeding
further. The components are categorised as Chapters, topics, research questions,
tangible outcomes (software implementations or algorithms) and publications
(journals, conferences or book chapters). There are three categories of relation-
ships (arrows in the diagram) drawn in the ﬁgure: (1) Contains: These are
one-to-many relationships. A relationship of this kind can be observed between
a chapter and multiple topics. (2) Dependence: These are directed one-to-one
relationships. The one-to-one directed relationships are drawn between chap-
1.3
Thesis Structure, Navigation and List of Peer-reviewed Publications
5

Figure 1.1.: Bird’s eye view of thesis components and their inter-dependence.
1.3
Thesis Structure, Navigation and List of Peer-reviewed Publications
6

ters and/or topics. Following the dependence graph, the reader can navigate
through the prerequisites and/or ﬂow of topics. (3) Associated with: These are
undirected one-to-one relationships associating a pair of components (such as a
Chapter and a research question, or a Chapter and a publication). An example
of a possible navigation through the diagram is as follows: To read Chapter 7
for example (which is associated with research question 3 and resulted in one
journal publication and a tangible outcome in the form of oiSTDP learning al-
gorithm), a reader is recommended to read Chapter 2, especially the topics on
brain data modelling and Chapter 4, especially the topics related to NeuCube
(which also has certain prerequisites).
This thesis generally follows a linear and continuous ﬂow. This means that ef-
forts have been made throughout the thesis to cross-reference across chapters
and sections in order to minimise repetitions. Chapter 5, 6 and 7 are presented
as the main contributions of this thesis and directly relates to the research ques-
tions discussed in section 1.2. Chapters 3 and 4 on the other hand forms the
basis on which the main chapters are written and many of the concepts are in-
troduced in these chapters and reused thereafter in the main contributions. The
following table lists the contributions that have been made in the form of peer
reviewed publications through the work of this thesis.
1.3
Thesis Structure, Navigation and List of Peer-reviewed Publications
7

Table 1.1.: List of peer-reviewed publications
Type
Publication
Year
Journal
Sengupta, N., McNabb, C. B., Kasabov, N. & Russell, B. (2018),
integrating space, time and orientation in spiking Neural Net-
works: A case study on multi-modal brain data modelling, IEEE
Transactions on Neural Networks and Learning System.
DOI:
https://doi.org/10.1109/TNNLS.2018.2796023.
2018
Journal
Sengupta, N., & Kasabov, N. (2017). Spike-time encoding as a
data compression technique for pattern recognition of temporal
data. Information Sciences, 406, 133-145.
2017
Journal
Kasabov, N., Scott, N. M., Tu, E., Marks, S., Sengupta, N., Capecci,
E., ... & Espinosa-Ramos, J. I. (2016). Evolving spatio-temporal
data machines based on the NeuCube neuromorphic framework:
design methodology and selected applications. Neural Networks,
78, 1-14.
2016
Conference
Sengupta, N., Scott, N., & Kasabov, N. (2015). Framework for
knowledge driven optimisation based data encoding for brain
data modelling using spiking neural network architecture.
In
Proceedings of the Fifth International Conference on Fuzzy and
Neuro Computing (FANCCO-2015) (pp.
109-118).
Springer,
Cham.
Conference
Abbott, A., Sengupta, N. & Kasabov, N. (2016). Which method to
use for optimal structure and function representation of large spik-
ing neural networks: A case study on the Neucube architecture. In
2016 international joint conference on neural networks(ijcnn)(pp.
1367-1372). IEEE
2016
Conference
Kasabov, N., Sengupta, N., & Scott, N. (2016, September). From
von neumann, John Atanasoff and ABC to Neuromorphic compu-
tation and the NeuCube spatio-temporal data machine. In IEEE
8th International Conference on Intelligent Systems (IS), 2016
(pp. 15-21). IEEE.
2016
Conference
Arya, A. S., Ravi, V., Tejasviram, V., Sengupta, N., and Kasabov, N.
(2018, January) Cyber fraud detection using evolving spiking neu-
ral network, in IEEE International conference on industrial and
information systems (ICIIS), 2016 (pp. 263-268). IEEE.
2016
Chapter
Sengupta, N., Ramos, J.I.E., Tu, E., Marks, S., Scott, N.,...
&
Abbott, A. (2018). From von Neumann architecture and Atana-
soffs ABC to Neuromorphic Computation and Kasabovs NeuCube:
Principles and Implementations, Learning Sytems: From Theory
to Practice. Springer.
2018
1.3
Thesis Structure, Navigation and List of Peer-reviewed Publications
8

2
Literature Review on Brain
Data Modelling and Machine
Learning
2.1 A Review on Modelling Brain Data
The human curiosity to understand the neural activity associated with cognition
and perception is as old as the ﬁeld of neuroscience itself. During its incep-
tion, neuroscience studies were focused on invasive techniques for modelling
the brain. This strand of research suffered from two major disadvantages:
1. The studies were non-functional studies due to their invasive nature (Geschwind,
1974; Shallice, 1988).
2. These studies needed to be implemented on non-human subjects (Felle-
man & Van Essen, 1991; Pandya & Yeterian, 1985) due to the risk associ-
ated with invasive procedures.
Due to these limitations, therefore, for many years, neuroscientists faced great
difﬁculties in measuring the neural activity in the mammalian brain, while they
performed tasks or simply rested. The advent of non-invasive techniques, such
as electrical activity (Electroencephalography, Electromyography, Electronystag-
mography), haemodynamic activity (fMRI), water diffusion (DTI), and others,
allowed for indirect measurements of neural activity and connectivity, leading
to the affordance of a new departure of computational neuroscience.
9

• Functional Magnetic Resonance Imaging (fMRI): Magnetic resonance imag-
ing (MRI) applies an external magnetic ﬁeld to align the magnetic mo-
ments of hydrogen ions in the body and then applies a radio frequency
pulse, which ﬂips the ions in the opposite direction. The recovery of these
hydrogen ions back to the direction of the external magnetic ﬁeld produces
a current, which generates the black and white image in an MRI photo.
Resting state functional magnetic resonance imaging (rs-fMRI) measures
changes in blood oxygenation within the brain during rest. When neurons
are activated in the brain, they expend energy, which needs to be replaced.
This is achieved via an increase in blood ﬂow to the active area (occur-
ring between 6 and 10 seconds after activity is initiated). This increase
in blood ﬂow produces a net decrease in deoxygenated haemoglobin lev-
els, changing the magnetic properties of the blood. Areas of high activity
are consequently associated with higher signal in the MRI and this can be
measured across time.
• Electroencephalography (EEG): EEG is a technique for measuring the elec-
trical activity of the brain. Silver chloride electrodes placed on the scalp
record the integrated activity of a large number of neurons within the
brain. Only neural structures with a speciﬁc spatial organisation (i.e., lay-
ers of neurons with all cell bodies/dendrites facing in the same direction,
such as the cortex, thalamus and cerebellum) can generate scalp potentials.
Consequently, non-invasive EEG can only measure a subset of the overall
activity of the brain. Despite this limitation, however, EEG is a valuable
tool for investigating abnormalities in brain function and provides a use-
ful data source for classifying patient populations with known changes in
their EEG output.
• Diffusion Tensor Imaging (DTI): DTI measures the net movement of water
within the brain. Water molecules outside of the cell have equal probability
of movement in every direction. However, when water is trapped within
a neuronal cell (within an axon or dendrite), its diffusion is restricted to
movement along the direction of the axon or dendrite. The diffusion direc-
2.1
A Review on Modelling Brain Data
10

tion of these trapped water molecules can be measured using DTI, provid-
ing a map of neuronal tracts (white matter) within the brain.
Numerous studies have focused on analysing and modelling neural activities re-
lated to cognitive or non-cognitive tasks, using single modalities of non-invasive
techniques, such as EEG, fMRI or DTI. Functional Magnetic Resonance (fMRI)
was used by Boksman et al. (2005) to study brain connectivity related to word
ﬂuency during the ﬁrst episode of schizophrenia. Supekar, Menon, Rubin, Musen
and Greicius (2008) used fMRI to study brain connectivity in Alzheimers disease.
Diffusion Tensor imaging (DTI) was also employed in the works of Eluvathingal
et al. (2006), Price et al. (2007) for cognitive studies. Ravan, Hasey, Reilly,
MacCrimmon and Khodayari-Rostamabad (2015) used EEG data for studying
the effect of Clozapine therapy in schizophrenia. Cabeza and Nyberg (2000)
studied fMRI and PET, two modalities of neuroimaging to explore the func-
tional anatomy of different cognitive functions, such as, attention, perception,
language and so on. McClure et al. (2007) studied a predictive model for treat-
ment of anxiety disorder in young children. DTI is used by Hamstra, Rehemtulla
and Ross (2007), as a biomarker for treatment response in oncology. Gordon
(2007) discussed using neuromarkers like fMRI and DTI for brain-related per-
sonalised medicine and treatment. Apart from these, applications, such as, Brain
Computer Interfaces (BCI), neuroprosthetics, neuro-rehabilitation are a few of
many other areas, which can potentially gain from brain data modelling and
analysis. All of the research work discussed above show the trend of modelling
a single modality of brain data. This is primarily due to the technological con-
straints associated with simultaneous acquisition of multiple modalities of brain
data. Concurrent recording suffers from several technical difﬁculties, such as,
ballistocardiographic artefact, MRI pulse artefact and others, leading to high
noise to signal ratio. Current technical advances in MRI and EEG recordings,
though, have overcome the aforementioned difﬁculties and allow for simulta-
neous recording of mixed modalities of brain data (Menon & Crottaz-Herbette,
2005; Horovitz, Skudlarski & Gore, 2002). This is ground breaking, as fused
analysis of multiple data types is potentially more informative about the com-
plex brain activity during the measured task. Until recently, the most commonly
2.1
A Review on Modelling Brain Data
11

used method of integrated data analysis for this kind of problem was by converg-
ing evidence (Horwitz & Poeppel, 2002). Typically, each data type is analysed
separately, and the results from other analyses that support one’s ﬁnding are dis-
cussed in the discussion Section. A standard sentence looks like "the activation
that was found in region X is consistent with studies in patients with focal lesion
in region X and in ERP studies, where a latency in the Y component on task Z
has been observed during.." (Horwitz & Poeppel, 2002). Horwitz and Poeppel
(2002) also discussed an alternative data fusion analysis called computational
neural modelling. This is done by creating biologically realistic neural network
models where each network simulates data of a certain type and is compared
with observed data. One major setback of this paradigm of data analysis is that
the hypothesis-driven neural network model is built under several assumptions
for simulated data generation. Hence, it is difﬁcult to know, whether any lack of
agreement between observed and simulated data is due to the assumptions in
the model, or whether it is simply wrong.
There is a third alternative for multi-modal data integration, known as direct
data fusion (George et al., 1995). Direct data fusion can be loosely deﬁned as,
the technique for directly fusing multiple datasets using statistical and machine
learning algorithms. BieSSmann, Plis, Meinecke, Eichele and Muller (2011) pre-
sented a detailed description and analysis of this approach. In this approach, if
one is interested in the neural potentials induced by a certain stimulus event,
one averages epochs of EEG time series aligned to the presentation of that stim-
ulus. If one is not interested in the exact phase of the neural response, one
often extracts the amplitude modulations of neural oscillations in a certain fre-
quency band (e.g. (Laufs et al., 2006)). For fMRI data, one typically extracts
patterns of activity that are correlated with the time course of the experimental
stimulus. Several exploratory unsupervised methods are also proposed for fused
data analysis. In (Eichele et al., 2008), temporal ICA is applied to EEG, and
spatial ICA is applied to fMRI data. Other unsupervised feature extraction tech-
niques include microstate analyses (Brandeis & Lehmann, 1989; Patil, Carmena,
Nicolelis & Turner, 2004), which is based on clustering to ﬁnd quasi-stable to-
pographical EEG scalp maps whose time courses can be compared with fMRI
2.1
A Review on Modelling Brain Data
12

signals (Muthukumaraswamy & Singh, 2008). This thesis intends to emphasize
the broad technique of direct data fusion. Next, a brief review of the literature
on statistical and machine learning treatment of brain data will be presented.
From a methodological perspective, there exists several statistical techniques for
brain data analysis. Conceptually, they can be divided into two different groups
according to Horwitz, Tagamets and McIntosh (1999):
• Subtraction paradigm: This paradigm hypothesizes the functional speciali-
sation, that different brain regions are associated with different brain func-
tions (Fristen, 1997; Posner, Petersen, Fox & Raichle, 1988). It compares
the signals between sets of scans, where each set represents a different
experimental condition. The locations of large differences in signals be-
tween the two presumably delineated brain regions differentially involved
in the two conditions. This paradigm is implemented using different forms
of univariate (Woolrich, Ripley, Brady & Smith, 2001) feature by feature
approaches, and does not take into account the inﬂuence of the spatial
neighbourhood, considering each feature activity as statistically indepen-
dent.
• Covariance paradigm: The assumption in covariance paradigm is that any
experimental condition is mediated by a network of interacting regions of
interest (ROI), and different functional tasks relate to different functional
networks (Horwitz, Soncrant & Haxby, 1992; Mesulam et al., 1990). This
paradigm focuses on correlation (Horwitz, Grady et al., 1992), covariance
and regression (Friston et al., 1997) to analyse the relationships between
brain regions and producing activity maps of the ROIs.
Until recently,
the General Linear Model (GLM), a form of statistical linear model was
used to perform multivariate statistical modelling of neuroimaging data
(Beckmann, Jenkinson & Smith, 2003; Calhoun, Stevens, Pearlson & Kiehl,
2004), and it was integrated in dedicated neuroimage analysis tools like
SPM (Friston, 1994) and AFNI (Cox, 1996).
2.1
A Review on Modelling Brain Data
13

Recently, the machine learning community collaborated with computational neu-
roscience, which led to the application of several machine learning techniques
in cognitive pattern recognition. The ability of pattern recognition algorithms
to map neural activity to mental states led to the practical realisation of mind
reading (Norman, Polyn, Detre & Haxby, 2006). A study by Haxby et al. (2001)
illustrated how multi voxel pattern analysis can be used to distinguish cognitive
states. Similar methods were also applied to discriminate the viewing of differ-
ent orientation of stripes (Haynes & Rees, 2005; Kamitani & Tong, 2005), move-
ment direction of a ﬁeld of dots (Kamitani & Tong, 2006), determine whether
a subject is viewing a picture or a sentence, and whether a subject was reading
an ambiguous or an unambiguous word, and the semantic category of a viewed
object (Mitchell et al., 2004). The University of Pittsburg organised brain com-
petitions in the years 2006 and 2007 that aimed to predict dynamic experience
in a virtual reality environment. In 2009, a brain connectivity challenge was
organised by them that aimed to map the cables of human brain mapping ap-
proximately 300, 000 ﬁbres streamline into 20 −50 cables or tracts. In all of the
above mentioned studies, advanced pattern classiﬁcation algorithms like Gaus-
sian Nave Bayes (Mitchell et al., 2004), Multilayer Perceptron (MLP) and Kernel
Ridge Regression (Chu, Ni, Tan, Saunders & Ashburner, 2011) were used as
discriminative models and achieved signiﬁcant classiﬁcation accuracy.
2.2 A Review of Machine Learning
The problem of searching and recognising patterns in data is a fundamental
form of science which has a long and fruitful existence in the history of the
human race. One of the earliest instances of the successful pattern recognition
endeavour was the discovery of empirical laws of planetary motion by Johannes
Kepler in the 16th century AD. The area of pattern recognition aims at automated
discovery of regularities in data through the usage of computational algorithms,
and with the use of these regularities, to take action on the task in hand (Bishop,
2006). Over the years, several approaches of pattern recognition have come
into prominence, the most primitive of which is hand crafted heuristics based on
2.2
A Review of Machine Learning
14

known knowledge, as used by Kepler during the discovery of empirical laws of
planetary motion using radio astronomy data.
In this work, however, the interest lies in a more sophisticated and automated
approach called machine learning. A machine in the form of a computer pro-
gram is formally said to "learn from experience E with respect to some task T
and performance measure P if its performance at task in T, as measured by P,
improves with experience E" (Mitchell et al., 1997). The machine learning way
of pattern recognition approaches the problem as a three step process. Each
step is accompanied by a subset of data. In the ﬁrst step, known as the train-
ing phase, a learning algorithm trains a mathematical model y(x) using a set of
training data xtrain := {x1, x2, · · · xn} (experience E). It must be noted, that
in the majority of cases, the data lies in a multidimensional feature space. The
training data xtrain can be labelled or unlabelled according to the class of the
problem. In the second phase, known as the validation phase, the model is val-
idated on its performance P on the set xvalid. The validation step is performed
to estimate the best model parameters and properties. At the end of the training
and validation phase, the best model y(x) is applied on new unseen data sample
xnew to recognise the pattern.
If one considers the pattern recognition programs as intelligent agents, based
on the type of feedback from which it learns the patterns, the area of machine
learning can be broadly divided into three main categories (Russell, 2003):
• Unsupervised learning: This class of learning tasks involve no explicit feed-
back from the environment. The most common unsupervised learning task
is clustering, which aims to create distinct groups of data in multidimen-
sional space, where there is signiﬁcant homogeneity within a group and
heterogeneity between groups.
• Supervised learning: The supervised learning task is to learn a mapping
between input and output pairs. In this class of machine learning, the
2.2
A Review of Machine Learning
15

agent receives explicit feedback from the environment, which acts as the
supervisor.
• Reinforcement learning: This type of learning is concerned with how an
agent ought to take actions in an environment with an aim to maximise
certain short and long term rewards. In this form of learning, the agent
receives indirect weak feedback from the environment in the form of the
reward (Sutton & Barto, 1998).
The focus in this research is on supervised and unsupervised learning paradigms.
Traditionally, the supervised learning paradigm deals with static data, where a
dataset is deﬁned by D := {X, y|X ∈Rn×m}. The input data X is multisample
(n) and multivariate (m) in nature. The output label is a vector y. In a classi-
ﬁcation task, y is sampled from a nominal set C := {c1, c2, · · · , ck}n and in a
regression task, y is continuous Rn. A simple example of a classiﬁcation task is
that it is an intelligent agent being able to discriminate between a ‘spam’ and a
‘non-spam’ email given an email. In this task, the output y ∈{spam, no spam} is
nominal in nature. On the contrary, prediction of temperature from multivariate
weather information is a good example of a regression/prediction task, where
the output y ∈R is a real continuous number.
A brief review of the classical supervised ML, as presented in Kotsiantis, Za-
harakis and Pintelas (2007), classiﬁed the research directions in the following
categories:
1. Logic based algorithms: These type of algorithms are generally divided
into the decision trees and rule based classiﬁers.
Decision trees These trees perform instance classiﬁcation by data parti-
tioning based on feature values (Murthy, 1998). Figure 2.1 shows an
example of decision tree. Each node in the tree represents a feature.
A new sample is classiﬁed based on the corresponding feature value
beginning from the root node. Numerous techniques are found in the
2.2
A Review of Machine Learning
16

Figure 2.1.: A decision tree model built on the Iris dataset (Fisher, 1936) (source
Girones (n.d.)).
literature for data partitioning including information gain (Quinlan,
1986), gini index and others. The best known algorithms for creating
decision trees are the C4.5 (Quinlan, 2014) and ID3 (Quinlan, 1986).
The major stand out aspect of the decision tree is the representation
of the model as a knowledge extraction system which makes it com-
prehensible for the user.
Rule induction Apart from decision trees, various algorithms for induc-
ing rules from training data have been proposed in the literature. The
rule induction algorithms aim to minimise the set of rules consisting
of training data. Minimising the rule-set ensures generalisation and
avoids overﬁtting (Kotsiantis et al., 2007). RIPPER (Cohen, 1995),
AQ family (Michalski, 1980) and CN2 (Clark & Niblett, 1989) are
some of the more popular rule based learning algorithms.
2. Perceptron based: Perceptron oriented algorithms are one of the most re-
searched and powerful class of machine learning algorithms. These ML
algorithms, as described in the seminal article (Rosenblatt, 1958), are
inspired by the working principles of the brain, and can be categorised
2.2
A Review of Machine Learning
17

(a) Adaptive Linear Elements (ADALINE)
(b) Example of a linearly separable prob-
lem.
Figure 2.2.: The ADALINE single layer perceptron model and an example of linearly
separable problem (source Widrow and Lehr (1990)).
loosely as neuromorphic in nature. Historically, the perceptron based algo-
rithms are divided into two sub categories:
Single layer perceptron A single layer perceptron model consists of a set
of computational neurons (perceptrons) arranged in a single layer
and fully connected to the input data x by connectors with weight w.
These networks were equipped to solve a binary classiﬁcation prob-
lem (y := −1, 1). The neurons compute weighed sum
∑
i
wi · xi of
the input data, and is passed through an adjustable threshold gate to
output −1 or 1. In the ADALINE model the weighed sum is further
passed through an activation function (Widrow & Lehr, 1990). Fig-
ure 2.2a shows the architecture of the ADALINE network. The most
prevalent method of learning the patterns is by running multiple it-
erations of the training data and adjusting the connection weights w
until the output matches ground truth. Some of the well known learn-
ing algorithms for single layer perceptron are described in (Littlestone
& Warmuth, 1994; Freund & Schapire, 1999). Despite its computa-
tional ability to solve linearly separable binary output problems as
shown in Figure 2.2b, these ML algorithms cannot solve problems of
non-linearly separable variety, such as, the XOR problem (Minsky &
Papert, 1969).
2.2
A Review of Machine Learning
18

Figure 2.3.: A fully connected feed-forward multi layer perceptron model (source
Widrow and Lehr (1990)).
Multi layer perceptron MLP models are extended from the earlier percep-
trons. It is deﬁned as a fully connected feed-forward ANN having mul-
tiple layers of nodes arranged as a directed graph. The layers are seg-
regated into three classes: input units, hidden and output units. The
hidden elements of MLP possess non-linear activation functions and
can distinguish non-linearly separable data. Figure 2.3 shows an ex-
ample of an MLP model. The feed-forward MLP networks are trained
usually using some variant of the gradient based backpropagation al-
gorithm (Rumelhart, Hinton & Williams, 1988), which optimise the
output by minimising the network prediction error by backpropaga-
tion of the error. It must be noted that due to the gradient descent-
based approach of the learning algorithm, these networks require the
activation functions to be fully differentiable and the input data to
be in a continuous space. Genetic algorithms have also been used to
train the weights of neural networks (Siddique & Tokhi, 2001) and
to ﬁnd the architecture of neural networks (Yen & Lu, 2000). There
are also Bayesian methods in existence which attempt to train neural
networks (Vivarelli & Williams, 2001).
3. Support vector machines: SVM is one of the most recent and widely used
supervised ML algorithm introduced by Cortes and Vapnik (1995). It was
developed to solve binary classiﬁcation problems using linear hyperplanes.
SVM revolves around the idea of a margin that maximally separates two
2.2
A Review of Machine Learning
19

Figure 2.4.: Maximum-margin hyperplane and margins for an SVM trained with sam-
ples having two features from two classes (source Cortes and Vapnik
(1995)).
data classes around a hyperplane (see Figure 2.4). For linearly separable
training data, the tuple (w, b) exists such that:
wTxi + b ≥1 ∀xi ∈P
(2.1)
wTxi + b ≤−1 ∀xi ∈N
(2.2)
The black and white dots represent the positive class P and negative class
N, respectively. The decision rule is given by:
fw,b(x) := sgn(wTxi + b)
(2.3)
An optimum separating hyperplane can be found by minimising the squared
norm of the separating hyperplane. The minimisation can be set up as a
convex quadratic programming (QP) problem:
minw,b
Φ(w) := 1
2||w||2
s.t.
yi(wTxi + b) ≥1, i = 1, · · · , l
(2.4)
For linearly separable data, at the end of the optimally separating hyper-
plane search, data points lying on its margin are known as support vector
points. This hard margin solution is later modiﬁed (Veropoulos, Campbell,
2.2
A Review of Machine Learning
20

Cristianini et al., 1999) to accommodate misclassiﬁcation of the training
samples.
To overcome the issue of non-linearly separable problems, non-linear SVM
is introduced by applying the kernel approach to ﬁnd maximum margin
hyperplanes. The kernel function transforms the given data space in a
higher dimensional space in the hope of achieving linear hyperplane in
a higher dimension that can separate the classes. There are numerous
transformation kernels existing in the literature including:
• linear: K(xi, xj) = xixj
• polynomial: K(xi, xj) = (γxixj + c)d
• RBF: K(xi, xj) = exp(−γ|xixj|2)
• sigmoid: K(xi, xj) = tanh(γxixj + c)
The kernel function represents the dot product of the input data points
mapped into higher dimensional feature space. Some of the known disad-
vantages of SVM are the high computational complexity of the quadratic
programming based training phase (Horvath, 2003) and sensitivity to over-
ﬁtting the model selection criteria in kernel models (Cawley & Talbot,
2010).
The brief review presented in this chapter are very general and touches upon the
two broad strands of research the studies in this thesis belongs to. Chapters 3
and 4 further reviews and delves into the nitty-gritties of computational neuron
and network of such neurons as a basis of the main studies presented in Chapters
5, 6 and 7. Further, literature reviews are also performed locally in the context
of the topics.
2.2
A Review of Machine Learning
21

3
From the Human Brain to
Artiﬁcial Evolving Spiking
Neural Networks
3.1 Introduction
This Chapter introduces the biophysiology of a human brain cell known as a
neuron and proceeds through the historical evolution of artiﬁcial neurons and
network of such neurons as computational models. Further, a brief review and
the work on the evolving connectionist systems (ECOS) will be presented. This is
an evolving neural network framework developed for recognising patterns from
data.
3.2 The Human Brain and the Morphology of
a Neuron
The human brain is the central processing unit of the human nervous system.
It is responsible for processing, integrating, and coordinating the information it
receives from the sensory organs, and making decisions as to the instructions
sent to the rest of the body. The human brain is made up of a network of approx-
imately ten billion interconnected neurons. A neuron is the basic information
processing unit which receives, processes and transmits information using bio-
chemical reactions. A typical neuron, as shown in Figure 3.1, comprises three
functional units, namely, dendrites, soma and axon. Dendrites are tree-like struc-
tural ﬁbres which emanate from the cell body and provide the receptive zones
for receiving the signals from other neurons. The morphology of the dendritic
22

Figure 3.1.: Morphology of a biological neuron (source Carlson (1967)).
tree plays an important role in the integration of the synaptic inputs and inﬂu-
ences the way the neuron processes information (Mel, 1994). The integration
process is performed through a non-linear process of spatio-temporal summation
(Koch, 1999). The resulting activation ﬂows to the soma and suffers voltage at-
tenuation, so that about half of the total charge injected into the distal dendritic
site reaches the soma. The primary function of the soma is to perform the con-
tinuous maintenance required to keep the neuron functional (Kandel, Schwartz,
Jessell, Siegelbaum, Hudspeth et al., 2000). The part of the soma that performs
the important nonlinear processing of the input is known as axon hillock. If the
total input produces a depolarisation up to the neural threshold, the axon hillock
ﬁres an action potential. The output signal is transmitted down the axon, which
delivers it to other neurons. Some of the important characteristics of of neuron
are enumerated below:
• Synapse: The contact point between a pre-synaptic axon and a post-synaptic
dendrite or soma is known as a synapse. In a chemical synapse, arrival of
action potential triggers a complex chain of biochemical reactions leading
to the release of neurotransmitters. This process generates a post-synaptic
potential (PSP) in the post-synaptic neuron. The internal state of a post-
synaptic neuron is driven by complex integration of PSP arising from multi-
ple such synapses. When the state of a post-synaptic neuron is sufﬁciently
stimulated, it gives rise to an action potential or a spike which is then trans-
mitted through the axon. Synapses are believed to be the locus of learning
3.2
The Human Brain and the Morphology of a Neuron
23

and memory in the brain (Squire & Kandel, 1999). This is where the brain
is the most ﬂexible and the most vulnerable (Marian, 2002).
• Action potential and neuron dynamics: The output generated by a biolog-
ical neuron is referred to as the action potential. The action potential is
an all or none electrical voltage based impulse response (Aur & Jog, 2010)
also known as a spike. A neuronal action potential is generated when the
membrane potential of the neuron reaches a threshold limit. Figure 3.2
plots a typical example of membrane potential (in mv) with time (in ms).
The membrane potential starts with a resting potential of −70mV . On
application of a stimulus at t = 1ms, the membrane potential is raised
above −55mV which is the threshold limit. After the stimulus is applied,
the membrane potential rises to +40 mV. This phase is known as the de-
polarisation phase. The potential then drops down to −90mV at time =
3ms (the repolarisation phase), and ﬁnally rises to the resting potential of
−70mV is at time = 5ms. The period between repolarisation and resting
state is known as the refractory period during which the neuron goes into
a hyperpolarisation state. During the hyperpolarised state, the neuron is
incapable of ﬁring any action potential. This mechanism stops a neuron
ﬁring spikes immediately after ﬁring a spike.
3.2.1 Neural Encoding and Information
Communication
Neurons are thought to convey signals mainly if not exclusively through the in-
formation content of their spike-trains. A spike-train consists of a series of times
at which the neuron has ﬁred. It is possible to record spike trains from individual
neurons using various electro-physiological methods in-vivo and in-vitro. Such
methods have generated a good number of datasets, which, in turn, have re-
vealed many properties of the neural computation. These properties constitute
the main body of results in the rapidly growing neuroscience literature.
3.2
The Human Brain and the Morphology of a Neuron
24

Figure 3.2.: Dynamics of the membrane potential of a typical neuron (source “Afterhy-
perpolarization” (2007)).
Neural encoding refers to the study of how neurons perceive and process infor-
mation received from other neurons or through incoming stimuli. As mentioned
earlier, information in a biological neuron is represented by action potentials.
Even though minor variations exist in the morphology of an action potential,
they are generally treated as identical stereotyped events in time. The study
of neural encoding and decoding has taken massive leaps over the past decade
with the availability of sophisticated invasive and non-invasive recording mech-
anisms from the brain cells. The curiosity regarding the nature of the neural
code has been around from the beginning of neuroscience and till now, there
has been no single answer towards it. Several large scale projects, such as the
brain decoding project, (Tsien et al., 2013) have emerged due to progress in this
area. The scientiﬁc fraternity has presented evidence over the years pointing to
the existence of two major categories of neural encoding schemes:
1. Rate coding: The most classical view of the neural encoding is the rate
coding and is originally shown in Adrian (1926) through experiments con-
ducted by hanging different weights from a muscle. They found the in-
crease of stimulus weight increases the number of spikes recorded from
the nerve. The frequency based approach of rate coding states that the ﬁr-
3.2
The Human Brain and the Morphology of a Neuron
25

ing rate of the neuron changes monotonically with the intensity or salience
of the input. This also means that the randomness in the inter spike inter-
val (ISI) in a spike train is considered as noise. This coding hypothesis can
be explained by Poisson’s model of random process (Pachitariu, Brody, Jun
& Holmes, n.d.). Out of the abundant examples of this coding scheme, the
experiments conducted by (Hubel & Wiesel, 1959, 1962) in their ground
breaking articles are the most interesting. During the experiments, light
stimuli was presented to a cat and the action potentials of the primary vi-
sual cortex receptive ﬁelds were recorded over time. Figure 3.3 shows how
different orientation of light bars generates response of varying spike rates
in the receptive ﬁelds of cat eyes. Such observations formed the basis of
the rate coding scheme.
Even today, neuro-scientists often assume that useful theories can be learned
about neural coding by summarising the post-stimulus time histogram
(PSTH) that plots the rate of ﬁring as a function of time.
There is no
doubt that rate coding seems to be the most obvious explanation of how
neurons encode information. However, it is also quite evident that the ﬁr-
ing rate as a coding scheme is highly inefﬁcient for information processing
(S. Thorpe, Delorme & Van Rullen, 2001; Gautrais & Thorpe, 1998; Van
Rullen & Thorpe, 2001). S. J. Thorpe and Imbert (1989) argued that the
primate visual cortex brain cells need to operate in less than 10 ms resolu-
tion. On the other hand, the upper bound of the ﬁring rate is ≈100 Hz,
which implies that such processing can be accomplished if the individual
neurons only get to ﬁre none or one spike. The inefﬁciency in rate coding
is caused by the requirement of precise calculation of the ﬁring rate. Gau-
trais and Thorpe (1998) showed within a mathematical framework that
during the ﬁrst 10 ms of computation, n neurons can transmit log2(n + 1)
bits of information in a rate coding scheme and log2(n!) bits of information
with the alternate temporal coding scheme. The necessity of approximat-
ing ﬁring rate from a large volume of spikes is a signiﬁcant hindrance in the
acceptance of this theory, especially considering the extreme efﬁciency of
the human brain. Additionally, the refractory period of the neuron dynam-
3.2
The Human Brain and the Morphology of a Neuron
26

Figure 3.3.: Experimental results obtained by Hubel and Wiesel (1962). The orienta-
tion of light bars presented at various orientations are shown on the left
and the spike responses are shown on the right (source Hubel and Wiesel
(1959)).
ics also prevents neurons from generating large numbers of spiking events
within a short period.
Olshausen and Field (2006) empirically demon-
strated from the electro-physiological data that rate coding V1 consumes
ten times more energy to operate.
2. Temporal coding: Temporal encoding hypothesizes that the information
is encoded in the relative timings of the spikes. This is in complete con-
trast to the rate coding scheme where irregularities in ISI do not arise from
stochastic forces and thus are not a random process. Temporal codes em-
ploy the features of the spiking activity that cannot be described by the
ﬁring rate. For example, the time to the ﬁrst spike after the stimulus onset,
characteristics based on the second and higher statistical moments of the
ISI probability distribution, spike randomness, or precisely timed groups
of spikes (temporal patterns) are candidates of temporal coding schemes
(Kostal, Lansky & Rospars, 2007). Several studies have also suggested the
existence of the temporal coding scheme across the animal kingdom. For
fast encoding of visual stimuli in the retina cells, latency time between
the stimulus onset and time to ﬁrst spike is used for encoding (Gollisch &
Meister, 2008). This is also known as the rank order coding (Van Rullen
& Thorpe, 2001). As with the visual system, in mitral/tufted cells in the
3.2
The Human Brain and the Morphology of a Neuron
27

olfactory bulb of mice, ﬁrst-spike latency relative to the start of a snifﬁng
action seem to encode much of the information about an odour. This strat-
egy of using spike latency allows for rapid identiﬁcation of and reaction
to an odourant. In addition, some mitral/tufted cells have speciﬁc ﬁring
patterns for given odourants. This type of extra information could help in
recognising a certain odour, but is not completely necessary, as the average
spike count over the course of the animal’s snifﬁng was also a good iden-
tiﬁer (R. I. Wilson, 2008). Along the same lines, experiments done with
the olfactory system of rabbits showed distinct patterns which correlated
with different subsets of odourants, and a similar result was obtained in
experiments with the locust olfactory system (Theunissen & Miller, 1995).
3.3 Generations of Artiﬁcial Neuron
It is a common understanding that primate intelligence has formed the base of
artiﬁcial intelligence. The idea of mimicking or translating human intelligence
into machines has been a signiﬁcant ambition for the scientiﬁc community in
the past centuries.
The evolution of artiﬁcial neural networks has massively
contributed towards realisation of that ambition. ANN is a mathematical real-
isation of a network of neurons or processing units that can perform complex
functional mappings. Although ANNs have progressed through various stages
of evolution, attempts were made only very recently to classify them into gen-
erations of neural network (Maass, 1997). Due to the multiple branching of
ANN research and development, this has been a challenging task.
Addition-
ally, such categorisation is subjective and dependent on what is considered as
achievement. However, one such identiﬁable conceptual progress has been the
development of mathematically-deﬁned activation function as the information
processing mechanism of an artiﬁcial neuron (Maass, 1997).
An artiﬁcial neuron has been conceived as a mathematical model of a biological
neuron. Figure 3.4 shows a block diagram of the components of an artiﬁcial
neuron. Starting from the left, an ANN consists of multiple input channels. The
3.3
Generations of Artiﬁcial Neuron
28

x2
w2
Σ
f
Activate
function
y
Output
x1
w1
x3
w3
Weights
Bias
b
Inputs
Figure 3.4.: Block diagram showing components of an artiﬁcial neuron.
input channels resemble the properties of dendrites in a biological neuron. These
channels feed input signals (originating from pre-synaptic neurons) {x1, x2, · · · }
to the processing unit of the neuron.
The synaptic weights {w1, w2, · · · } de-
ﬁne the strengths of the synapses and allows for strengthening or weakening of
synapses through synaptic plasticity. The input along with the synaptic strengths
are fed into the processing units which consists of the summation and activation
units. These units together are responsible for mapping of the inputs x to the
output y. Next, The evolution and properties of an artiﬁcial neuron is well sum-
marised by Ghosh-Dastidar and Adeli (2009), however, for the sake of continuity,
I present below, a brief overview of the three generations of computational neu-
ron:
3.3.1 First Generation Neurons:
The perceptron model described by Rosenblatt (1958) was the beginning of the
ﬁrst generation of artiﬁcial neuron. The variations of this model used the per-
ceptrons as integrate and ﬁre units, which integrate the inputs and ﬁred if the
internal state (synaptic weighed sum of inputs) reached a threshold. Mathe-
matically, the activation can be described by a Heavyside step function (range
{0, 1}).
f(x) =





1,
if w · x + b > 0
0,
otherwise
(3.1)
Equation 3.1 describes the activation of a simple perceptron. It produces binary
output y = 1 or y = 0, if the weighed sum w · x goes beyond b or otherwise.
3.3
Generations of Artiﬁcial Neuron
29

Unlike the biological neuron, the inputs of the perceptron were real and con-
tinuous, i.e. the magnitude of inputs contribute to the activation of the neuron.
This is a primitive realisation of the rate coding scheme, i.e. higher input causes
the neuron to ﬁre. However, the outputs were binary spikes and did not cater
to the rate coding scheme. The perceptrons were also time agnostic. The inputs
are always considered to be synchronous in time and it did not matter when
the threshold is reached, i.e., as they contribute to the internal state at the same
time, and hence can be directly integrated. Any events of the past do not affect
the activation at the present time.
3.3.2 Second Generation Neurons:
The second generation of neurons were developed in the 1960s as an extension
of the ﬁrst generation neurons. In the second generation, non linear smooth
activation functions were introduced. Hence, instead of a ﬁxed threshold value
for the output determination, the outputs were proportionate within range to in-
put signals. Tanh (Equation 3.3 range (−1, 1)) or Sigmoid (Equation 3.2 range
(0, 1)) functions were used most often as activation functions. With this develop-
ment, the outputs became real and continuous, and contrary to the ﬁrst gener-
ation, the post-synaptic neurons could generate rate coded information. These
neurons became extremely popular in the AI community with the introduction
of feed-forward neural networks and back-propagation (BP) algorithm (Rumel-
hart et al., 1988), which enabled supervised learning. Since the BP algorithm
was constrained by its requirement of a continuous and differentiable activation
function, a signiﬁcant portion of the ensuing research became focused on ﬁnding
more appropriate continuous and differentiable activation functions. This model
was signiﬁcantly more powerful than the one based on ﬁrst generation neurons
and could solve complex pattern recognition problems (the most notable early
example was the XOR problem). However, the computational power of the neu-
ron still did not reach its full potential because the temporal information about
individual spikes was not represented.
3.3
Generations of Artiﬁcial Neuron
30

f(x) =
1
1 + e−(w·x+b)
(3.2)
f(x) = tanh(w · x + b)
(3.3)
3.3.3 Third Generation Neurons
In the past decade, to overcome the shortcomings of the ﬁrst and second gen-
eration neurons, neurons that can communicate via the precise timing of spikes
or a sequence of spikes have been developed and adapted for ANNs. These neu-
rons are known as spiking neurons. In the literature, these spiking neurons have
been referred to as third generation neurons (Maass & Bishop, 2001). Similar to
the ﬁrst generation neurons, spiking neurons act as integrate-and-ﬁre units and
have an all or none response. The spiking neuron, however, has an inherent dy-
namic nature characterised by an internal state which changes with time. Each
post-synaptic neuron ﬁres an action potential or spike at the time instance that
its internal state exceeds the neuron threshold. Similar to biological neurons,
the magnitude of the spikes (input or output) contains no information. Rather,
all information is encoded in the timing of the spikes.
3.4 Spiking Neuron Models
The ﬁrst and second generation neurons discussed in the previous Section use
rather simplistic activation of the artiﬁcial neurons. The biophysical neuron mod-
els, however, are developed to quantitatively characterise neuronal behaviour
based on neuron membrane potential and ion channel conductance models (Ghosh-
Dastidar & Adeli, 2009). Marian (2002) distinguished between the two spiking
neuron models by the extent of detail included in the neuron model.
3.4
Spiking Neuron Models
31

1. Detailed neuron models: In this approach, researchers aimed at creating
detailed and complex models of the neurons.
De Schutter and Bower
(1994), Segev, Burke and Hines (1998) described the chemical process
at the sub-cellular level.
This included ion channel and dendritic tree
biophysics, synaptic interplay between excitation and inhibition, and volt-
age dependent events in the active dendrites (Mainen & Sejnowski, 1998).
Among all the detailed neuron models, the most prominent is the con-
ductance based Hodgkin-Huxley model (Hodgkin & Huxley, 1952). The
Hodgkin-Huxley model describes the generation of action potentials at the
level of ion channels and ion current ﬂow.
It is the starting point for
detailed biophysical neuron models which in general include more than
the three types of currents considered by Hodgkin and Huxley. Electro-
physiologists have described an overwhelming richness of different ion
channels.
The set of ion channels is different from one neuron to the
next. The precise channel conﬁguration in each individual neuron deter-
mines a good deal of its overall electrical properties. The model provides
a detailed description of the biophysics of ionic mechanisms underlying
the initiation and propagation of the neural spike. By doing this, it offers
an accurate quantitative model of the physiological data. But, complex
frameworks, such as this, which account for numerous ions-channels and
different types of synapses are difﬁcult to construct and analyse. An impor-
tant conceptual drawback of this family of models is that their numerical
complexity (e.g., solve a large number of non-linear differential equations)
can prevent one from understanding which features are responsible for a
particular phenomenon and which are irrelevant (Koch, 1999).
The Hodgkin-Huxley like models are useful in modelling point neurons.
For the purpose of building detailed neuron models that can take into con-
sideration extreme cell complexities, such as branched cable structure, the
standard way is to divide a neuron into a ﬁxed number of compartments
where each compartment acts like a resistance-capacitance (RC) circuit.
The RC circuits are modelled by a system of differential equations. Numer-
ous biological neuron simulators, such as Genesis (M. A. Wilson, Bhalla,
3.4
Spiking Neuron Models
32

Uhley & Bower, 1989) and Neuron (M. L. Hines & Carnevale, 2006), imple-
mented the compartmental models. Nonetheless, there is always a trade-
off between computational cost and biological realism.
2. Formal spiking neuron models: The second direction focuses on the spik-
ing nature of the neurons and retaining the essential elements of the be-
haviour being modelled, while trying to simplify the complexity of the
resulting description. The main motivation for the creation of simpliﬁed
models is that they allow studying more easily the computational and func-
tional principles of neural systems.
The reduction of the detailed neuron models to formal models requires sim-
pliﬁcations in at least two respects. First, the nonlinear dynamics of spike
generation must be reduced to a single ordinary differential equation and
second, the spatial structure of the neuron (i.e., the dendritic tree) is ne-
glected and reduced to an input (Gerstner & Kistler, 2002). To support the
validity of the former simpliﬁcation, Kistler, Gerstner and van Hemmen
(1997) demonstrated that spike generation in the Hodgkin-Huxley model
can be reproduced to a high degree of accuracy (up to 90%) by a sin-
gle variable model. Several simpliﬁed neural models have been proposed
in the last decades. The leaky-integrate-and-ﬁre (LIF) neuron is proba-
bly the best-known example of a formal neural model. It simulates the
dynamics of the neuron membrane potential in response to a synaptic cur-
rent by implementing an equivalent electrical circuit. The function of the
integrate-and-ﬁre circuit is to accumulate the input currents, and, when
the membrane potential reaches the threshold value, to generate a spike.
Immediately after emitting a pulse, the potential is reset and maintained
there for an absolute refractory period.
The simpliﬁed mathematical models for spiking neurons cannot account
for the entire range of computational functions of the biological neuron.
Rather, they try to abstract a number of essential computational aspects
of the real cell function. The essential features implemented can differ
3.4
Spiking Neuron Models
33

between models, as a function of what the modeller considers to be rele-
vant and crucial for its domain study. Thus, the integrate-and-ﬁre model
focuses upon the temporal summation function of the neuron (Bugmann,
Christodoulou & Taylor, 1997). The spike response model proposed by
Gerstner (1998) simpliﬁes the action potential generation to a threshold
process. The resonate-and-ﬁre model (Izhikevich, 2001) focuses upon the
operation of the neuron in a resonating regime. By contrast with the de-
tailed neural models, the computational strength of the spiking neurons
arises from the way they interact with each other, when they work cooper-
atively in large networks.
Next, I will outline some of the most prominent spiking neural network models
present in the literature:
3.4.1 Integrate and Fire
The description of the Integrate and Fire (IF) and Leaky Integrate and Fire (LIF)
model is adapted from (Gerstner, Kistler, Naud & Paninski, 2014) which serves
as a comprehensive review on this topic. An IF model is described by a single
variable known as membrane potential.
The effect of an incoming spike on
a post-synaptic neuron can be recorded using intra-cellular electrodes which
measure the potential difference v(t) at time t between the inner and outer wall
of a cell membrane. This is known as the membrane potential. In absence of any
input, the neuron is said to be at rest with a constant resting membrane potential
vrest. The arrival of a spike changes the membrane potential and ﬁnally decays
back to vrest. For excitatory synapses the change is positive and for inhibitory,
it is negative. The neuronal dynamics of an IF neuron can be conceived as an
integration process combined with a mechanism that triggers action potential
above a threshold voltage Vthr. From hereon, post-synaptic and pre-synaptic
neurons will be indexed by symbols i and j respectively. A simple IF model can
be described using: (i) A linear differential equation to describe the evolution of
the membrane potential vi(t). (ii) A threshold for spike ﬁring.
3.4
Spiking Neuron Models
34

Figure 3.5.: Electrical properties of the passive membrane of an IF neuron. (A) A cell
membrane enclosed neuron receives a positive input current I(t) resulting
in increase of electrical charge inside the cell. The cell membrane acts as
a capacitor in parallel with a resistor which is in line with a battery of
potential vrest. (B) The reaction of the cell membrane to a step current
(top) with a smooth voltage trace (source Gerstner, Kistler, Naud and
Paninski (2014)).
Gerstner et al. (2014) modelled an IF neuron as a single RC circuit (see Figure
3.5) following the laws of electricity. The cell membrane (insulation to the cell
body) acts as a capacity of a capacitor C. Also, because the insulation is imper-
fect, the charge over time, slowly leaks through the cell membrane. The cell
membrane hence can be characterised by a ﬁnite leak resistance R. Due to the
leaky nature of the resistance, the model is also known as leaky integrate-and-
ﬁre (LIF) model.
I(t) = IR + IC
(3.4)
I(t) = v(t) −vrest
R
+ C dv
dt
(3.5)
τm
dv
dt = RI(t) −[v(t) −vrest]
(3.6)
3.4
Spiking Neuron Models
35

v(t) = vrest + RI0[1 −exp(−t
τm
)]
(3.7)
The circuit shown in Figure 3.5 can be analysed from the law of current con-
servation as per Equation 3.4. This can be further rewritten as Equation 3.5 by
calculating IR = vR/R as per Ohm’s law, where vR = v −vrest. The IC charges
capacitor C. The capacitive current can be written as IC = dq/dt = Cdv/dt. Eq.
Equation 3.6 is the linear differential equation describing a LIF neuron’s passive
membrane. The trajectory of membrane potential (Equation 3.7) for a constant
input current I(t) = I0 starting and ending at t = 0 and t = ∆can be derived by
solving the differential Equation 3.6. The LIF neuron emits a spike denoted as
t(f)
i
generates a spike when the membrane potential v(t) reaches threshold vthr,
i.e.,
t(f) : v(t(f)) = vthr
(3.8)
3.4.2 Hodgkin-Huxley
One of the earliest proposed (1952) models for spiking neuron is the Hodgkin-
Huxley (HH) model. This model describes the inﬂuence of ion channel conduc-
tance on the spike responses of axon and was empirically studied on the axons
of a Giant Squid. The choice of squid was due to its non-microscopic size. This
large size was necessary as electrodes had to be inserted into the axon, in order
to record the changes in electrical state experienced when neurons are active.
The HH model is described by a RC circuit connected in a parallel scheme resem-
bling the IF model. Following Equation 3.4, the HH model rewrites it as:
C dv
dt = I(t) −IR
(3.9)
3.4
Spiking Neuron Models
36

HH model describes the resistive current as the sum of three different ion cur-
rents due to the presence of movement of sodium, potassium and leakage. The
resistive current is described by Equation 3.10.
IR = GNam3h(u −VNa) + GKn4(u −VK) + GLeak(u −VLeak)
(3.10)
Where VNa, VK and VLeak are known as reversible potential constants. GNa and
Gk are the maximum conductance of the sodium and potassium channels, while
the voltage independent leak channel is represented by GLeak. The variables
m, n and h are gating variables described by Equations 3.11, 3.12 and 3.13.
m
dt = αm(u)(1 −m) −βm(u)m
(3.11)
n
dt = αn(u)(1 −n) −βn(u)n
(3.12)
h
dt = αh(u)(1 −h) −βh(u)h
(3.13)
where m and h control the sodium channel and n controls the potassium chan-
nels.
The function αx(.) and βx(.) represent empirically determined voltage
dynamics across capacitor v, are adjusted to simulate different neuron types.
3.4.3 Izhikevich
The Izhikevich model claims to combine biological plausibility of the HH model
with the lower computational complexity of the IF and LIF models (Izhikevich,
2004). Based on the theory of dynamic systems, the dynamics of this model are
governed by two equations:
3.4
Spiking Neuron Models
37

dv
dt = 0.04v2 + 5v + 140 −u + I
(3.14)
where u is a membrane recovery variable providing negative feedback for v;
variables u and v, and parameters a, b, c and d are dimensionless. The input
stimuli in the form of input current is represented by I. When the membrane
potential reaches the (ﬁxed) threshold vthr = 30mV , the neuron spikes and
resets u and v as:
ifv ≥30mV





v ←c,
u ←u + d,
(3.15)
Dependent on parameters a (decay rate of membrane potential), b (sensitivity
of membrane recovery), c and d (reset values of v and u respectively), a huge
variety of neuronal types can be modelled with relative ease.
It can be observed that the mathematical representation of the biological neu-
rons are extremely complex and focus on mimicking the biological properties
accurately. This direction of work focuses on discoveries of biophysical prop-
erties of neurons and the human brain through simulations. The work of this
thesis is, however, not intended to follow that direction. This work focuses on
using the spiking properties of the biological neurons, interwoven together in
a network for solving pattern recognition problems. Therefore, it is necessary
to compare the capabilities of the spiking neuron models in regards to the bi-
ological plausibility and complexity vs computational cost. Izhikevich (2004)
presented an insightful comparison of over twenty spiking neuron models in
this regard. Izhikevich, in this article, compared the presence or absence of 22
categories of biological properties, such as tonic spiking, phasic spiking, spike
latency and chaos in the spiking neuron models. Figure 3.6 shows the compar-
ison of spiking neuron models presented in (Izhikevich, 2004). The biological
plausibility on the Y axis is measured by the total number of biological proper-
ties (out of 22 different categories) present in the neuron model. The X axis
plots the implementation cost of the neuron model by the number of ﬂoating
3.4
Spiking Neuron Models
38

Figure 3.6.: Comparison of spiking neuron models in the evaluation landscape of bio-
logical plausibility and implementation cost (source Izhikevich (2004)).
point operations (FLOPS). It can be observed that out of many models proposed
in the literature, Hodgkin-Huxley and integrate-and-ﬁre neurons, reside at the
extremity of the evaluation landscape. The choice of neuron models, as argued
in (Izhikevich, 2004) really depends on the goal. If the goal is to simulate a large
number of neurons interacting within a network, efﬁciency plays an important
role. Variations of IF models are suitable for this purpose. For the rest of this
thesis, variations of the IF model are used and discussed.
3.5 Spiking Neural Networks
The phenomenological models of spiking neurons described earlier can be simpli-
ﬁed to lesser realistic models where spikes are not modelled which leads to the
ﬁrst two generations of the neurons. Of course, the computational load reduces
drastically along with biological plausibility. For decades, the modelling of the
neurons was limited by the available computing power because the hardware
was unable to support large ANNs based on detailed neuronal models. This
limitation dictated the design of the learning algorithms. Subsequently, even
when advances were made in computing power, proportionate advances were
not made in the complexity of the neuronal models because the existing learn-
ing algorithms were not compatible with the detailed models.
3.5
Spiking Neural Networks
39

Accordingly, two distinct research areas emerged. The ﬁeld of Artiﬁcial Neu-
ral Networks concentrated on the behaviour of large networks of neuron-like
processing units (i.e., the second generation neurons), which were primitive
and oversimpliﬁed formulations of biological neurons. However, it was demon-
strated that even such networks were capable of learning using pseudo-realistic
learning algorithms, such as backpropagation. ANNs were applied with great
success to pattern recognition, classiﬁcation, and pattern completion tasks in a
wide variety of areas. The other ﬁeld became known as Computational Neuro-
science. Within this broad interdisciplinary ﬁeld, the detailed biophysical and
phenomenological models were primarily used in relatively smaller networks to
study electro-physiological processes, pattern generation, and the dynamic be-
haviour of small groups of neurons. There have also been studies involving very
large numbers of interconnected biophysical neuron models. However, it has
not been possible to use such networks of detailed neurons in a manner similar
to ANNs for large real-world pattern recognition and classiﬁcation tasks.
Modern advances and the accessibility of computing resources have increased
the overlap between the two ﬁelds. On one hand, the processing units, net-
works, and learning algorithms for ANNs have become biologically more realis-
tic. On the other hand, networks of biophysical neurons have become increas-
ingly larger in size and the biophysical models, more detailed. The available
computing power still limits the use of the detailed models in large biophysical
neural networks for pattern recognition and classiﬁcation tasks. As the comput-
ing power becomes more readily available, suitable learning algorithms are also
being developed for such models. The development of Spiking Neural Networks
(SNN) was the next logical step towards achieving this goal.
Simply stated, SNNs are networks of spiking neurons. The SNN is architecturally
similar to that of a traditional ANN. The processing units, however, are spiking
neurons, typically modelled by a phenomenological model, such as a Spike Re-
sponse Model. As discussed earlier, the use of the biophysical models in certain
applications of SNNs is less common due to the computational burden.
3.5
Spiking Neural Networks
40

3.6 Neuromorphic Computing Beyond von
Neumann Hardware Architecture
This Section is an adaptation of the article (N. Kasabov, Sengupta & Scott, 2016)
published in 2016. The tremendous push of AI towards emulation of real intel-
ligence has been sustained by the realisation of Moore’s law (Schaller, 1997),
which states that the processing power of central processing units (CPU) will
double every couple of years. The scalable computer architecture proposed by
John von Neumann in 1945 as part of the draft of EDVAC computer (Randell,
2013) had to play a substantial role in accomplishing the continuous miniaturi-
sation of the CPU chips. In more recent years, CPU chip manufacturing compa-
nies have spent billions of dollars in CMOS technology to shrink the transistor
size to a minuscule (≈14 nanometres) and thus keep Moore’s law alive. It is
evident that this is non-sustainable and as per well-supported predictions will
reach its boundary in the next three to ﬁve years (Toumey, 2016). The literature
suggests that the ongoing effort to enhance the traditional von Neumann archi-
tectures are unlikely to lower simulation runtimes signiﬁcantly, as single and
multi-core architectures are reaching a state of saturation in terms of transistor
size (Thompson & Parthasarathy, 2006), energy consumption (Esmaeilzadeh,
Blem, St Amant, Sankaralingam & Burger, 2011) and communication (Perrin,
2011).
The Von Neumann architecture is a multi-modular design based on rigid phys-
ically separate functional units. It speciﬁcally consists of three different enti-
ties:
• Processing unit: The processing unit can be broken down into several sub-
units, the arithmetic and logical unit (ALU), the processing control unit
and the program counter. The ALU computes the arithmetic logic needed
to run programs.
The control unit is used to control the ﬂow of data
through the processor.
3.6
Neuromorphic Computing Beyond von Neumann Hardware Architecture
41

Table 3.1.: A comparison of the key contrasts between von Neumann and neuromor-
phic computing paradigm.
Properties
von Neumann
Neuromorphic
Representation of the data
Sequence of binary numbers
Spike (event) timings
Memory
Volatile and non-volatile
Long and short term memory
Plasticity (Learning)
No
Long, short term potentiation and depression
Processing
Deterministic, centralised and sequential
Stochastic, decentralised and parallel
• I/O unit: The i/o unit essentially encompasses all I/O the computer could
possibly do (printing to a monitor, to paper, inputs from a mouse or key-
board, and others.).
• Storage unit: The storage unit stores anything the computer would need
to store and retrieve. This includes both volatile and non volatile memory.
These units are connected over different buses like data bus, address bus and
control bus. The bus allows for the communication between the various logical
units. Though very robust, this architecture inherently suffers from the bottle-
neck created due to the constant shufﬂing of the data between the memory unit
and the central processing unit. This bottleneck leads to rigidity in the architec-
ture as the data needs to pass through the bottleneck in a sequential order. An al-
ternate solution of parallelising the computers has been proposed where millions
of processors are interconnected. Despite the increase in processing power, this
solution is still limited by the bottleneck in its core elements (Schuler, 2015).
The saturation in the scalability of the von Neumann architecture led to develop-
ments in computer and computing architectures. Neuromorphic computing was
coined by Carver Mead (Mead, 1990) in the 1980s and further developed in a
new paradigm of computing. As the name ‘neuromorphic’ suggests, this is in-
spired by the human brain. Moreover, as the existence of AI is complimented by
computing architectures, having a real neuromorphic computer architecture ori-
ented processing unit is a step towards the development of highly neuromorphic
AI. The focus of neuromorphic hardware systems are to model the behaviour
of the biological neurons in digital or analog circuits. It also draws great in-
spiration from our brain’s ability to manage tens of billions of processing units
connected by the hundreds of trillions of synapses using tens of watts of power
3.6
Neuromorphic Computing Beyond von Neumann Hardware Architecture
42

Table 3.2.: Comparison of the key features of the popular neuromorphic systems and
human brain. The details are adapted from (Scott, 2015; S. Furber, 2016).
Feature
Human brain
SpiNNaker (S. B. Furber, Galluppi, Temple & Plana, 2014)
Zheijang FPGA (J.-l. Li, Wang, Li, Han & Zhang, 2005)
TrueNorth (Hsu, 2014)
Type
Biological
Programmable digital
FPGA
Fixed Digital
Neuron model
Diverse
Programmable
LIF
LIF
Synapse model
Diverse
Programmable
Programmable
Binary
Max # neurons
100B
16K
2048
1M
Max # synapses
1015
14M
4.2M
256M
Runtime plasticity
yes
Programmable
No
No
Energy per connection
10fJ
10nJ
Unknown
25pJ
Biological speed up
1x
1x
1x
1x
on average. The large network of the processing units (neurons) in the brain are
in a true sense a mesh. The data is transmitted over the network via the mesh
of synapses seamlessly. Architecturally, the presence of the memory and the
processing unit as a single abstraction is uniquely advantageous leading to dy-
namic, self-programmable behaviour in complex environments (Schuler, 2015).
The highly stochastic nature of computation in our brain is a very signiﬁcant
divergence from the bit-precise processing of the traditional CPU. The neuro-
morphic computing hence aspires to move away from the bit-precise computing
paradigm towards the probabilistic models of simple, reliable, power and data
efﬁcient computing (Calimera, Macii & Poncino, 2013) by implementing neuro-
morphic principles, such as spiking, plasticity, dynamic learning and adaptabil-
ity. This architecture morphs the biological neurons, where the memory and the
processing units are present as part of the cell body leading to the decentralised
presence of memory and processing power over the network. There is a signif-
icant interest in such hardware implementations of SNN due to many factors.
The primary factor being the massive power efﬁciency -somewhere in the order
of 20 watts- for its ability to learn and operate in a stochastic environment. Such
systems can be broadly categorised in one of the families of Application-Speciﬁc
Integrated Circuit (ASIC), Field Programmable Gate Array (FPGA), or digital sys-
tems. Table 3.1 lists some fundamental characteristics of the von Neumann and
neuromorphic architecture.
A number of large-scale neuromorphic systems have emerged over recent years
taking advantage of the enormous transistor resources now available on a sin-
gle microchip. The enhanced capabilities of the neuromorphic systems enable
modellers to contemplate building models of complete brains of animals from in-
sects up to smaller mammals, or substantial sub-areas of the human brain, and
the same systems also offer platforms capable of supporting new scales of cogni-
3.6
Neuromorphic Computing Beyond von Neumann Hardware Architecture
43

tive architecture (S. Furber, 2016). A tabular comparison of the neuromorphic
hardware platforms are adapted from (Scott, 2015) and shown in Table 3.2.
SpiNNaker: The SpiNNaker hardware, developed as part of the Human Brain
Project is a massively parallel digital computer whose communication infras-
tructure is motivated by the the objective of modelling massively scalable SNN
with the brain like connectivity proﬁle in biological real time. In many respects,
SpiNNaker resembles conventional supercomputers, with the following differ-
ences:
• The processors in SpiNNaker are mobile IC chips.
• The communication protocol in SpiNNaker is brain inspired and is opti-
mised for broadcasting large quantities of small data packets to destina-
tions in a stochastic manner.
The SpiNNaker system is designed around a plastic ball grid array package which
incorporates a custom processing chip and a 128 Mbyte SDRAM memory chip.
The processing chip contains 18 ARM968 processing cores, each with 23 Kbytes
of instruction memory and 64 Kbytes of data memory, a multicast packet router
and sundry support components (S. Furber, 2016). The SpiNNaker communi-
cation fabric is based on a 2D triangular mesh with each node formed from a
processor layer and a memory layer. The routing is based upon packet-switched
Address Event Representation and relies on the fact that the connections from a
particular neuron are static, or at most slowly changing. Each neuron can route
through a unique tree, though in practice routing is based on populations of
neurons rather than individual neurons, and the restricted size of each routing
table makes this optimisation necessary in most cases. In addition to the hard-
ware system, the project also developed numerous high level neural description
language, such as PyNN (Davison et al., 2008) and Nengo (Bekolay et al., 2013)
for application development on SpiNNaker.
3.6
Neuromorphic Computing Beyond von Neumann Hardware Architecture
44

IBM TrueNorth: The IBM TrueNorth chip is the hardware developed under the
DARPA SYNAPSE programme aimed at developing dense, power-efﬁcient hard-
ware for cognitive applications. This hardware consists of a 5.4 million transistor
28 nm CMOS chip with 4096 cores, where each core is made up of 256 neurons
each having 256 synaptic inputs (Cassidy et al., 2013).
The design of the TrueNorth core is a 256 × 256 cross-bar which selectively con-
nects incoming neural spike events to outgoing neurons. The cross-bar inputs
are coupled via buffers that can insert axonal delays.
The outputs from the
cross-bar couple into the digital neuron model, which implements a form of IF
algorithm with 23 conﬁgurable parameters that can be adjusted to yield a range
of different behaviours, and digital pseudo-random sources are used to generate
stochastic behaviours through modulating the synaptic connections, the neuron
threshold and the neuron leakage. Neuron spike event outputs from each core
follow individually-conﬁgurable point-to-point routes to the input to another
core, which can be on the same or another TrueNorth chip. Where a neuron
output is required to connect to two or more neurosynaptic cores, the neuron is
simply replicated within the same core. The TrueNorth hardware is supported by
a software emulator, which, exploiting the deterministic nature of the hardware,
can be relied upon to predict the performance of the hardware exactly.
Apart from these two systems that rose to prominence, numerous other neuro-
morphic hardware systems are being constantly developed. Some of the systems
worth a mention include Neurogrid (Stanford University) (Boahen, 2006), Brain-
ScaleS (University of Heidelberg) (Markram et al., 2011), and multi-PWM pulse
generator (Zheijiang University) (J.-l. Li, Wang, Li, Han & Zhang, 2005).
3.6
Neuromorphic Computing Beyond von Neumann Hardware Architecture
45

3.7 Brief Review of SNN Software
Implementations
The number of software implementations that have appeared, as a result of on-
going research in the area of ANN and SNN, is ever growing. These software
packages are developed for two main purposes:
• Data analysis: They are aimed at analysing real-world data derived from
practical applications. These software use a relatively simple static archi-
tecture, hence are easily conﬁgurable and easy to use. A few examples
of such software are: multi-layer perceptron (MLP) (Baum, 1988), RBF
network (Park & Sandberg, 1991), Probabilistic network (PNN) (Specht,
1990), Self organising maps (SOM) (Kohonen, 1998), Evolving connec-
tionist systems, such as DENFIS and EFuNN (N. Kasabov, 2007). These
software are either available as independent open source APIs, such as
NeuCom (“NeuCom”, n.d.), PyBrain (python) (Schaul et al., 2010), Fast
Artiﬁcial Neural Network (C++) (Nissen & Nemerson, 2000), or as part
of a data analytics suite like Weka (Hall et al., 2009), Knime (Berthold
et al., 2008), Orange (Demar, Zupan, Leban & Curk, 2004) and so on.
• Research and development systems: As opposed to data analysis software,
they are complex in behaviour, and require expert knowledge for usage
and conﬁguration. The majority of the existing SNN software belong to
this class of ANN.
NEURON (M. L. Hines & Carnevale, 1997): Neuron is aimed at simulating a
network of detailed neurological models. Its ability to simulate biophysical prop-
erties, such as multiple channel types, channel distributions, ionic accumulation
and so on, renders it well suited for biological modelling (Brette et al., 2007). It
also supports parallel simulation environment through: (1) distributing multiple
simulations over multiple processors, and (2) distributing models of individual
cells over multiple processors.
3.7
Brief Review of SNN Software Implementations
46

PyNEST (Eppler, Helias, Muller, Diesmann & Gewaltig, 2008): The neural simu-
lation tool (NEST) is primarily developed in C++ to simulate a heterogeneous
network of spiking neurons. NEST is implemented to ideally model neurons
in the order of 104 and synapses in the order of 107 to 109 on a range of de-
vices from single core architectures to supercomputers. NEST interfaces with
python via implementation of PyNEST. PyNEST allows for greater ﬂexibility in
simulation setup, stimuli generation and simulation result analysis.
A ‘node’
and a ‘connection’ comprise the core elements of the heterogeneous architec-
ture. The ﬂexibility to simulate ‘a neuron’, ‘a device’ or ‘a subnetwork (which
can be arranged hierarchically)’ as a node, provides a major improvement over
(Pecevski, Natschläger & Schuch, 2009). Due to the bottom-up approach of net-
work simulation, the software allows for individually conﬁgurable neuron states
and connection setup.
Circuit Simulator (Natschläger, Markram & Maass, 2003): The circuit simulator
is a software developed in C++ for simulation of heterogeneous networks with
major emphasis on high-level network modelling and analysis, as opposed to
(M. L. Hines & Carnevale, 1997). The C++ core of the software is integrated
with Matlab based GUI, for ease of use and analysis. CSIM enables the user to
operate both spiking and analog neuron models along with mechanisms of spike
and analog signal transmission through its synapse. It also performs dynamic
synaptic behaviour by using short and long-term plasticity. In 2009, circuit sim-
ulator was further extended to parallel circuit simulator (PCSIM) software with
the major extension being implementation on a distributed simulation engine in
C++, interfacing with Python based GUI.
Neocortical Simulator (Drewes, 2005): NCS or Neocortical Simulator is a SNN
simulation software developed for simulating mammalian neocortex (Brette et
al., 2007). During its initial development, NCS was a serial implementation in
Matlab but later rewritten in C++ to integrate distributed modelling capability
(E. C. Wilson, 2001). As reported in (Brette et al., 2007), NCS could simulate
in the order of 106 single compartment neurons and 1012 synapses using STP,
LTP and STDP dynamics. Due to the considerable setup overhead of the ASCII-
3.7
Brief Review of SNN Software Implementations
47

based ﬁles used for the I/O, a Python-based GUI scripting tool called BRAINLAB
(Drewes, 2005) was later developed to process I/O speciﬁcations for large scale
modelling.
Oger Toolbox (Pecevski, n.d.): Oger toolbox is a Python-based toolbox, which
implements modular learning architecture on large datasets. Apart from tradi-
tional machine learning methods such as PCA and ICA, it also implements SNN
based reservoir computing paradigm for learning from sequential data. This soft-
ware uses a single neuron as its building block, similar to the implementation in
(Eppler et al., 2008). A major highlight of this software includes the ability to
customise the network with several non-linear functions and weight topologies,
and a GPU optimised reservoir using CUDA.
BRIAN (D. F. Goodman & Brette, 2009; D. F. Goodman, 2010): Brian is a SNN
simulator application programming interface written in Python. The purpose
of developing this API is to provide users with the ability to write quick and
easy simulation code (D. F. Goodman, 2010), including custom neuron models
and architecture. The model deﬁnition equations are separated from the imple-
mentation for better readability and reproducibility. D. F. Goodman and Brette
(2009) also emphasise the use of this software in teaching a neuroinformatics
course (Diesmann, Gewaltig & Aertsen, 1999). A major limitation of BRIAN is,
however, the requirement of Python knowledge to run the simulation, and the
lack of GUI for the non-technical user community.
3.8 Evolving Connectionist System
It is beyond any speculation that AI and neuromorphic hardware systems be-
long to a uniﬁed symbiotic ecosystem, where a neuromorphic hardware is truly
neuromorphic if and only if it has dynamic learning, adaptation and creativ-
ity incorporated in the framework. On the other hand neuromorphic learning
systems are paradigm shifting in machine learning where it is not only about
3.8
Evolving Connectionist System
48

accurate performance, but also about performing it economically using limited
resources.
Subsequently, I will concentrate on the principle and evolution of the Evolving
Connectionist System (ECOS) with emphasis on our work in evolving spiking
neural network (eSNN). The rest of the Chapter is an adaptation of our work
presented in (Arya, Vadlamani, Valadi, Sengupta & Kasabov, 2016) along with
an article under review.
3.8.1 Principles of ECOS
The human brain’s unique ability to combine low level learning within an in-
terconnected framework and high level rule abstraction leads to the learning of
abstract concepts. ECOS is inspired by this very concept and aims at training
neural networks for deriving abstract knowledge representations that explains
data and can be used as an interpretable knowledge-based system. The ECOS
was developed as a trend in neural networks and computational intelligence by
N. Kasabov (2001) and incorporated in many novel computational methods over
the last few decade across different application domains. Some classical exam-
ple of ECOS systems are EFuNN (N. Kasabov, 1998) and DENFIS (N. Kasabov
& Song, 2002), where classical McCulloch and Pitts neuron models are used to
perform pattern recognition in single dimensional scalar data. ECOS principle
was propagated further to evolving spiking neural networks (eSNN) that used
spiking neuron models and temporal spike sequences as data representation.
The eSNN was architected primarily as a visual pattern recognition system. The
ﬁrst eSNNs were based on the Thorpes ‘time to ﬁrst spike’ rule (S. Thorpe et al.,
2001), in which the importance of early spikes (after the onset of a certain stim-
ulus) is boosted, called rank-order encoding and learning. N. Kasabov, Dhoble,
Nuntalid and Indiveri, 2013 further proposed dynamic eSNN (deSNN), which
combines rank order encoding of eSNN with dynamic Hebbian spike-time based
learning. The main advantage of the eSNN when compared with other super-
vised or unsupervised SNN models is that it is computationally inexpensive due
3.8
Evolving Connectionist System
49

to the one-pass nature of the learning algorithm and boosts the importance of
the order in which input spikes arrive, thus making the eSNN based algorithms
suitable for on-line learning with a range of applications. For a comprehensive
study of ECOS and eSNN see (Watts, 2009; Wysoski, Benuskova & Kasabov,
2010). I, in collaboration with The Institute of Development and Research in
Banking Technology, India, have used eSNN in applications of automatic cyber
fraud detection and stock price movement prediction case studies. In the follow-
ing two Sections, the results that have been obtained will be summarised from
these two studies. The detail of each of the studies can be found in Appendix A
and B
3.8.2 Application of eSNN in Forecasting Stock Price
Movement
Prediction of the stock price index and its trend are considered challenging tasks
due to their complexity, non-linearity, dynamic and chaotic nature. In addition,
the stock market behaviour is also inﬂuenced by socio-political movements, in-
vestor psychology, and so on. Although many researchers have explored several
forecasting techniques, it is for the ﬁrst instance that eSNN has been employed to
develop a computational model for the stock market movement trend prediction.
In this study, two computational models are proposed, namely, CUDA-SI-eSNN,
which is a parallel CUDA implementation of the standard eSNN architecture.
The second algorithm, known as SW-eSNN, is the incremental learning of eSNN
using a sliding window of data. Appendix B describes the motivation behind
this study along with descriptions of the proposed SI-eSNN, CUDA-SI-eSNN and
SW-eSNN architectures along with the formal description of the algorithms.
3.8
Evolving Connectionist System
50

Dataset Description and Experiments with the SI-eSNN and the
CUDA-eSNN Models
The datasets used in this study are obtained from QUANDL (“Quandl Financial,
Economic and Alternative Data.” n.d.), (“Historical - Indices.” n.d.), and (“NSE -
National Stock Exchange of India Ltd.” n.d.). These datasets cover stock market
indices of different countries: BSE, Nikkie-225, NIFTY-50, S&P-500, Dow-Jones,
NYSE-Amex, DAX, NASDAQ and Shanghai stock exchange. In this study, thir-
teen technical indicators have been selected as input variables based on earlier
research done in (K.-j. Kim, 2003; J. Patel, Shah, Thakkar & Kotecha, 2015;
Kara, Boyacioglu & Baykan, 2011). The direction of daily stock price index is
categorised as ‘UP’ or ‘DOWN’. If the stock price index at time t is higher than that
at time t + 1, then the trend is ‘DOWN’. If the stock price index at time t is lower
than that at time t + 1, then the trend is ‘UP’. The number of instances of each of
the stock index data is given in Table B.3. The details about selected indicators
is given in Table B.2 and the summary statistics of selected technical indicators
for each stock indices are provided in the supplementary information.
In the experiments, the ﬁrst 70% of the temporal stock data represented by the
13 indicators on a daily basis are used as input variables for training the SI-eSNN
and the CUDA-SI-eSNN models and the future 30% of the time series stock data
is used to test the model accuracy. Experiments in the present study were carried
on systems with 32 GB RAM and eight cores. The GPU device used for all of our
experiment is GeForce GT730. The details of the GPU device is given in Table
B.4.
Classiﬁcation accuracy and AUC scores were used to evaluate the performance
of both eSNN and CUDA-SI-eSNN models. Sensitivity and speciﬁcity are used
to assess the AUC score. In addition to the sensitivity, speciﬁcity and accuracy
equations described earlier, the AUC score can be deﬁned as:
AUC score = Sensitivity + Specificity
2
(3.16)
3.8
Evolving Connectionist System
51

Table 3.3.: Accuracy and AUC of CUDA-eSNN model with Gaussian Distribution for
different stock indices.
Datasets
BSE
Nikkie-225
NASDAQ
NIFTY-50
S&P-500
No. of Gaussian Receptive Fields
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
3
82.17
0.84
0.82
81.57
3.3
0.81
84.46
0.89
0.84
76.85
0.61
0.76
82.96
8.51
0.82
4
85.71
1.06
0.86
82.53
4.67
0.82
85.43
1.1
0.85
79.96
0.7
0.79
83.74
9.77
0.83
5
83.88
1.25
0.83
80.33
5.96
0.8
84.22
1.23
0.84
80.13
0.69
0.8
81.16
11.59
0.81
6
85.59
1.36
0.85
84.41
7.15
0.84
84.22
1.24
0.83
77.37
0.79
0.77
82.72
13.78
0.82
7
87.05
1.39
0.87
80.88
8.06
0.80
85.92
1.53
0.86
82.55
0.81
0.82
82.53
18.95
0.82
8
85.47
1.55
0.85
86.15
8.91
0.85
81.79
1.55
0.81
82.03
0.84
0.81
84.03
22.05
0.84
9
88.76
1.65
0.88
82.12
9.68
0.82
84.7
1.67
0.84
81.69
0.93
0.81
82.16
29.05
0.82
10
86.08
1.75
0.86
87.12
10.6
0.87
85.92
1.84
0.85
83.93
0.99
0.83
84.13
35.7
0.84
11
88.64
1.78
0.88
83.73
11.58
0.83
84.7
1.91
0.84
80.31
1.05
0.8
84.5
39.36
0.84
12
87.91
1.88
0.88
86.11
12.41
0.86
83.49
2.04
0.83
82.38
1.08
0.82
81.26
46.94
0.81
13
88.4
2
0.88
83.68
13.15
0.83
84.22
2.19
0.84
84.45
1.18
0.84
84.74
51.51
0.84
14
87.91
2.25
0.88
86.57
14.04
0.86
83.73
2.19
0.83
81
1.22
0.8
84.13
51.88
0.84
∗Acc.=Accuracy, ∗T.T = Training Time
Table 3.4.: Accuracy and AUC of CUDA-eSNN model with Gaussian Distribution for
different stock indices (Continued).
Sanghai Stock Exchange
Dow-Jones
NYSE-Amex
DAX-Index
No. of Gaussian Receptive Fields
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
3
77.48
1.68
0.77
75.31
0.83
0.74
78.37
2.01
0.77
79.23
2.67
0.79
4
78.14
2.08
0.78
76.59
0.93
0.76
84.04
2.5
0.83
77.64
4
0.77
5
80.74
3.13
0.8
75.31
0.96
0.74
82.75
3.67
0.81
81.55
5.12
0.81
6
80.37
3.3
0.8
78.38
1.05
0.78
80.56
3.96
0.8
82.07
5.79
0.81
7
79.92
3.7
0.79
79.79
1.23
0.79
82.62
4.4
0.82
78.41
6.43
0.78
8
79.18
3.73
0.79
79.28
1.46
0.79
77.79
4.7
0.77
82.22
7.22
0.81
9
79.48
3.76
0.79
78.51
1.49
0.78
82.78
5.1
0.83
82.53
7.76
0.82
10
80.37
4.8
0.8
77.74
1.59
0.77
83.65
5.51
0.82
82.07
8.48
0.81
11
82.66
5.2
0.82
77.62
1.69
0.77
81.78
6.05
0.81
83.41
9.11
0.83
12
79.77
5.33
0.79
79.28
1.77
0.79
86.29
6.45
0.85
84.9
9.66
0.84
13
79.7
5.9
0.79
81.2
1.84
0.8
83.78
6.87
0.83
82.22
10.68
0.81
14
80.44
5.44
0.8
80.17
2.03
0.79
86.42
7.42
0.86
84.75
11.23
0.84
∗Acc.=Accuracy, ∗T.T = Training Time
Tables 3.3 and 3.4 reports the accuracy and AUC of CUDA-SI-eSNN model for a
different number of Gaussian receptive ﬁelds on various stock indices.
In Table 3.5, the performance of both SI-eSNN and CUDA-SI-eSNN models have
been included for the best value of some Gaussian receptive ﬁelds. The accuracy
of both these models is very high across all stock indexes (between 80% and
90%) and only slightly different between the two models. Table 3.5 also reports
the time for training performed by both the SI-eSNN and the CUDA-SI-eSNN on
the same data. While the latter is 2 to 5 times faster, the SI-eSNN training was
very fast as well. For example, it took only 40 seconds to train the SI-eSNN on
5000 samples for the S&P-500, while it took 20 seconds on the CUDA-SI-eSNN.
Table 3.5.: Comparisons of SI-eSNN and CUDA eSNN for the best value of number of
Gaussian receptive ﬁelds.
eSNN
CUDA-eSNN
Dataset
Size of training data
No. of Gaussian Receptive Fields (Best)
Accuracy (%)
Training Time (Sec)
AUC
Accuracy (%)
Training Time (Sec)
AUC
BSE
1912
9
88.15
5.91
0.88
88.76
1.65
0.88
Nikkie-225
5092
10
87.12
23.67
0.87
87.12
10.6
0.87
NASDAQ
1923
7
86.04
5.22
0.86
85.92
1.53
0.86
NIFTY-50
1353
13
83.76
5.98
0.84
84.45
1.18
0.83
S&P-500
9591
13
84.71
82.56
0.84
84.74
51.51
0.84
Shanghai Stock Exchange
3149
11
83.18
12.87
0.82
82.66
5.2
0.83
Dow-Jones
1824
13
81.2
8.53
0.8
81.2
1.84
0.8
NYSE-Amex
3625
14
86.16
20.09
0.86
86.42
7.42
0.86
DAX-Index
4527
12
84.16
24.05
0.84
84.9
9.66
0.84
3.8
Evolving Connectionist System
52

Table 3.6.: Accuracy and AUC of CUDA-SI-eSNN model using Logistic Distribution for
different stock indices.
Datasets
BSE
Nikkie-225
NASDAQ
NIFTY-50
S&P-500
No. of Logistic Receptive Fields
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
3
78.14
0.85
0.78
84.87
3.05
0.84
70.87
0.87
0.71
72.02
0.57
0.72
70.85
8.24
0.7
4
77.53
1.01
0.75
86.98
3.32
0.86
83.98
1.12
0.83
78.23
0.64
0.78
79.68
9.16
0.79
5
79.36
1.12
0.78
81.57
3.7
0.81
83.85
1.11
0.83
82.21
0.68
0.82
84.47
11.01
0.84
6
84.37
1.32
0.84
86.29
4.29
0.86
85.19
1.27
0.84
80.65
0.72
0.8
80.58
12.23
0.8
7
86.81
1.38
0.86
81.57
5.07
0.81
87.37
1.81
0.87
83.24
0.76
0.83
83.09
15.37
0.82
8
85.34
1.55
0.85
86.34
6.45
0.86
86.89
1.52
0.86
81.69
0.79
0.81
85.86
19.97
0.85
9
90.47
1.63
0.9
84.55
7.67
0.84
86.77
1.65
0.86
82.03
0.89
0.82
83.81
24.53
0.83
10
86.81
1.74
0.86
86.66
9.73
0.86
85.8
1.74
0.85
83.93
0.96
0.83
84.28
30.96
0.84
11
89.01
1.78
0.89
85.24
10.59
0.85
87.5
1.85
0.87
81.69
1.02
0.81
85.71
35.78
0.85
12
88.03
1.89
0.88
86.38
11.58
0.86
87.01
1.99
0.86
84.8
1.09
0.84
83.5
41.02
0.83
13
89.13
2.03
0.89
86.52
12.87
0.86
86.16
2.05
0.86
85.14
1.15
0.85
84.54
45.49
0.84
14
87.05
2.08
0.87
86.61
13.87
0.86
86.28
2.19
0.86
84.8
1.18
0.84
85.76
49.69
0.85
Table 3.7.: Accuracy and AUC of CUDA-SI-eSNN model using Logistic Distribution for
different stock indices (Continued).
Sanghai Stock Exchange
Dow-Jones
NYSE-Amex
DAX-Index
No. of Logistic Receptive Fields
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
Acc. (%)
T.T (Sec)
AUC
3
78.74
1.6
0.78
73.91
0.79
0.73
69.75
2.07
0.69
75.32
2.65
0.74
4
78.88
1.95
0.78
74.8
0.81
0.74
81.59
2.43
0.8
77.94
3.63
0.77
5
77.85
2.54
0.77
78.9
0.86
0.78
81.14
3.14
0.8
85
4.78
0.84
6
81.11
3.03
0.81
78
0.92
0.78
81.85
3.63
0.81
81.04
5.59
0.8
7
80
3.23
0.79
79.66
1.05
0.79
86.16
4.15
0.85
80.47
6.24
0.8
8
79.7
3.49
0.79
81.45
1.27
0.81
82.11
4.59
0.81
82.27
6.82
0.81
9
80.29
3.78
0.8
79.79
1.34
0.79
84.42
4.91
0.84
83.51
7.57
0.83
10
81.92
4.15
0.81
79.79
1.46
0.79
84.23
5.34
0.83
82.63
8.11
0.82
11
83.03
4.53
0.83
82.09
1.63
0.81
84.1
5.79
0.83
84.49
8.83
0.84
12
81.55
4.73
0.81
80.56
1.74
0.8
86.62
6.23
0.85
85.67
9.57
0.85
13
81.03
5.1
0.81
82.99
1.81
0.83
83.91
6.87
0.83
84.23
10.25
0.83
14
82.48
5.41
0.81
81.84
1.97
0.81
85.97
7.14
0.85
84.75
11.07
0.84
Table 3.8.: Comparisons of SI- eSNN and CUDA-SI-eSNN for the best value of number
of Logistic receptive ﬁelds.
eSNN
CUDA-SI-eSNN
Dataset
Size of training data
No. of Logistic Receptive Fields (Best)
Accuracy (%)
Training Time (Sec)
AUC
Accuracy (%)
Training Time (Sec)
AUC
BSE
1912
9
90.04
5.78
0.89
90.47
1.63
0.9
Nikkie-225
5092
10
86.66
22.17
0.86
86.66
9.73
0.86
NASDAQ
1923
11
88.14
6.14
0.87
87.5
1.85
0.87
NIFTY-50
1353
13
84.74
5.65
0.84
85.14
1.15
0.85
S&P-500
9591
8
85.23
40.83
0.85
85.86
19.97
0.85
Sanghai Stock Exchange
3149
11
83.85
11.83
0.83
83.03
4.53
0.83
Dow-Jones
1824
13
82.87
5.43
0.83
82.99
1.81
0.83
NYSE-Amex
3625
12
85.94
19.23
0.85
86.62
6.23
0.85
DAX-Index
4527
12
85.16
23.02
0.85
85.67
9.57
0.85
3.8
Evolving Connectionist System
53

Table 3.9.: p values of McNemar test for the pairwise comparison of performance of
CUDA-SI-eSNN with Logistic and Gaussian distribution.
Stock Indices
p Values
BSE
0.648
Nikkie-225
0.804
NASDAQ
0.794
NIFTY-50
0.470
S&P-500
0.269
SSE
0.044
DJUS
0.007
NYSE-Amex
0.001
DAX-Index
0.125
The results of CUDA-SI-eSNN with Logistic Distribution is presented in Tables 3.6
and 3.7. Logistic distribution was employed in place of Gaussian in (Farquad,
Ravi & Raju, 2012) too, where better results were obtained with the former.
The experimental results showed that CUDA-SI-eSNN with Logistic distribution
outperforms the CUDA-SI-eSNN model with Gaussian distribution on all stock
indices except on Nikkie-225 stock index. The accuracy of CUDA-SI-eSNN with
Logistic distribution varies between 82.99% and 90.47%. The result of the CUDA-
SI-eSNN with the best value of a number of logistic receptive ﬁelds presented
in Table 3.8. The training time of the CUDA-SI-eSNN with logistic receptive
ﬁelds is different from the training time of the CUDA-SI-eSNN with Gaussian
receptive ﬁelds for the same number of receptive ﬁelds is due to running of
other background processes. The CUDA-SI-eSNN model with logistic receptive
ﬁelds outperformed the CUDA-SI-eSNN model with Gaussian receptive ﬁelds in
all stock indices since logistic distribution has higher kurtosis than the Gaussian
distribution.
To test whether CUDA-SI-eSNN with Logistic Distribution signiﬁcantly outper-
forms another model, the McNemar test was used. The result of McNemar test
is given in Table 3.9. The McNemar test showed that CUDA-SI-eSNN with Lo-
gistic Distribution signiﬁcantly outperforms other model on DJUS, NYSE-Amex,
and SSE stock indices at 5% statistical signiﬁcance level.
3.8
Evolving Connectionist System
54

Table 3.10.: Average AUC score of SW-eSNN Incremental approach using Logistic and
Gaussian distributions.
Dataset
eSNN+Logistic
eSNN+Gaussian
BSE
0.77
0.71
Nikkie-225
0.72
0.69
NASDAQ
0.76
0.77
NIFTY-50
0.69
0.67
S&P-500
0.75
0.73
Sanghai Stock Exchange
0.67
0.65
Dow-Jones
0.73
0.7
NYSE-Amex
0.73
0.69
DAX-Index
0.72
0.7
Appendix B also presented the SW-eSNN model. In this approach, a model is ﬁrst
trained with one-year data and then tested for the prediction on the subsequent
month. Subsequently, the window was slid by one-month for next one-month
prediction. The experimental results show that the average AUC score of the
models over all the benchmark data sets vary from 65% to 80% and for some
months reaches 100%. The manifested ﬂuctuation across monthly prediction
accuracy is expected with this model due to the short window length used and
external factors affecting the stock price movement.
The average AUC score of SW-eSNN incremental approach for both logistic distri-
bution and Gaussian distribution on all stock indices is presented in Table 3.10.
This result shows that SW-eSNN incremental approach with logistic distribution
outperformed SW-eSNN incremental approach with Gaussian distribution on all
stock indices except on NASDAQ stock index.
3.8.3 Application of eSNN in Cyber Fraud Detection
In the present work (Arya et al., 2016) published in 2016, an application of
eSNN based system for detecting cyber frauds in phishing websites was pre-
sented. This study has applied the eSNN algorithm to learn a model for detect-
ing phishing websites from URL and web page source information. The detail of
the eSNN learning algorithm used is described in Appendix A.
3.8
Evolving Connectionist System
55

Table 3.11.: Comparison of performance of eSNN and other iterative and non iterative
machine learning algorithms on the phishing website data.
Classiﬁers
Type
Sensitivity (%)
Speciﬁcity (%)
Accuracy (%)
Gaussian process
Iterative
99
100
99.5
Logistic Regression
91
88
89.5
MLP
89
80
84.5
CART
94
90
92
Gaussian Process+CART
94
87
90.5
Probabilistic NN
One pass
93
92
92.5
eSNN
89
90
89.5
Table 3.12.: t-test based model comparison.
t-statistic value (accuracy)
eSNN vs.
Gaussian Process
4.33
Logistic Regression
1.12
PNN
1.17
MLP
3.04
CART
0.2
Dataset Description
For the purpose of experiments on phishing website detection, a web phishing
dataset was used for phishing website detection. 200 phishing websites were cho-
sen for analysis, where 50% URL’s were phishing website and the rest were legit-
imate websites URLs. The URLs are collected from PhishTank (www.phishtank.
com). The dataset was created by extracting 17 features based on t-statistics
values from web pages source code and URLs.
Results
The result of the experiments performed on the web phishing data sets are pre-
sented in Tables 3.11 and 3.12. All the reported results are 10-fold cross vali-
dated. The details of eSNN model testing method and hyperparameter selection
protocol are described further in Appendix A. The performances of the various
models were compared with respect to sensitivity, speciﬁcity and overall accu-
racy. These measures are deﬁned as the following:
3.8
Evolving Connectionist System
56

Specificity =
TN
TN + FP
(3.17)
Sensitivity =
TP
TP + FN
(3.18)
Accuracy =
TP + TN
TP + FP + TN + FN
(3.19)
Where TP = True Positive, TN = True Negative, FP = False Positive, FN = False
Negative, where a phishing website is considered to be a positive class and non-
phishing as negative class.
In Table 3.11, the proposed eSNN algorithm for cyber fraud detection is com-
pared with the iterative and one-pass learning algorithms. The result shows that
the proposed eSNN model achieves better performance with respect to overall
accuracy and sensitivity/speciﬁcity in comparison to the one pass PNN algorithm.
Table 3.12 compares the eSNN model performance with the benchmark models
based on the t-statistic value. The t-test value is found to be less than 2.83 (t-
table value concerning 18 degrees of freedom, i.e., 10 + 10 −2 = 18) in all the
classiﬁer except GP and MLP. For all the cases, eSNN performance is statistically
equivalent.
3.8
Evolving Connectionist System
57

3.9 Contributions and Publications
Contributions
1. Historical evolution of artiﬁcial neuron and neural networks.
2. Review and motivation of the hardware and software implementa-
tion of spiking neural networks.
3. Application of evolving spiking neural network on cyber fraud detec-
tion.
• Implementation of a novel sequential eSNN classiﬁcation algo-
rithm.
• Comparison of the proposed algorithm with iterative and one-
pass learning algorithms on cyber fraud detection problem.
4. Application of evolving spiking neural network on stock movement
forecast.
• GPU implementation of the evolving spiking neural network al-
gorithm.
• Implementation of the sliding window approach for evolving
spiking neural network.
• Evaluation of one day ahead stock index forecast across 9 dif-
ferent stock indices.
3.9
Contributions and Publications
58

Publications
1. Kasabov, N., Sengupta, N., & Scott, N. (2016, September). From
von neumann, John Atanasoff and ABC to Neuromorphic computa-
tion and the NeuCube spatio-temporal data machine. In IEEE 8th
International Conference on Intelligent Systems (IS), 2016 (pp. 15-
21). IEEE.
2. Arya, A. S., Ravi, V., Tejasviram, V., Sengupta, N., and Kasabov, N.
(2018, January) Cyber fraud detection using evolving spiking neural
network, in IEEE International conference on industrial and informa-
tion systems (ICIIS), 2016 (pp. 263-268). IEEE.
3.9
Contributions and Publications
59

4
A Review of NeuCube: An
Evolving Spatio-Temporal
Data Machine Framework
In the last decade or so, with the advent of deep neural network, a signiﬁcant
proportion of research in artiﬁcial intelligence and machine learning has been
conducted in the area of neural networks. This has led to a massive diversiﬁ-
cation in the architecture and functionality of the neural networks. This diver-
sity across the domain of neural networks makes it extremely difﬁcult to review
the area exhaustively or even perform systematic taxonomy as such. van Veen
(2016) has visually demonstrated (see Figure 4.1) the node level block archi-
tectures of several popular neural networks. This is a very useful comparative
diagram to provide broad understanding of different neural networks. However,
care must be taken in interpreting the networks from the ﬁgure as the node level
diagram hides several degrees of abstractions and similar looking architectures
(e.g. VAE and DAE), which may be greatly different with respect to training
protocol and application areas.
In this Chapter, the focus will be shifted towards discussing the novel evolv-
ing spatio-temporal data machine, NeuCube, proposed by Nikola Kasabov in (N.
Kasabov, 2012). This framework is especially designed to take advantage of het-
erogeneous properties present in the data, especially in the form of spatial and
temporal information. NeuCube draws its inspiration from the of recurrent neu-
ral network based reservoir computing paradigm and this topic will be discussed
further.
60

Figure 4.1.: Node level architectures of popular neural networks (source van Veen
(2016)).
61

4.1 Recurrent Neural Networks
Rather than diverting into describing all the networks (see (van Veen, 2016) for
a brief description of all the architectures shown in Figure 4.1), I will concen-
trate on the recurrent neural networks (RNN). RNNs can be described simply as
feed forward neural networks (FFNN) with a time twist. RNNs consist of a set
of processing elements (neurons) which are interconnected by abstractions of
synaptic connections. The interconnected mesh enables activations to be prop-
agated through the network. The characteristic feature that differentiates RNN
from FFNN is the connection topology. RNNs possess cyclic connections form-
ing intra and inter-layer loops. The existence of cyclic connection has profound
impact:
1. Even in the absence of input signals, RNNs possess the ability to develop a
self-sustained activation dynamics along the cyclic connections. This prop-
erty allows RNNs to act as a dynamic system as opposed to the function
like behaviour of FFNN.
2. RNNs possess dynamic memory and are able to make use of temporal con-
textual information.
The philosophy of the recurrent neural network is rooted in the fact that humans
do not start thinking from scratch every second. When an individual reads or
speaks, their understanding of a concept at any point in time is contextually
dependent on what they understood earlier. Thoughts have persistence. Tra-
ditional neural networks are not capable of using information in a contextual
manner, which is a major shortcoming. For example, to classify events in a video
over time, it is unclear how a traditional neural network could use its reasoning
about previous events in the video to inform future events.
RNNs are built to address this exact issue. The network retains information
by including loops in them. On the left hand side of Figure 4.2, a RNN block is
4.1
Recurrent Neural Networks
62

Figure 4.2.: Unrolled RNN (source Olah (2015)).
Figure 4.3.: Mapping capabilities of RNN (source Karpathy (2015)).
shown. Block A, receives some input xt and outputs a value ht. A loop passes the
information from one step of the network to the next step (Olah, 2015). An RNN
can be intuitively described as multiple copies of the same network unrolled
across the time dimension (see Figure 4.2). This chain-like nature reveals that
recurrent neural networks are intimately related to sequences and lists. They
are the natural architecture of neural networks to use for such data.
4.1.1 What Makes RNN Eﬀective?
Andrej Karpathy in his blog post (Karpathy, 2015) addresses this question with
great detail. In his post, he argued that the ability of the RNNs to operate over
sequences of vectors, input, output or both makes it more powerful. Figure 4.3
taken from (Karpathy, 2015) describes the effectiveness with respect to map-
ping capabilities of RNN. Each rectangle in the ﬁgure represents a vector and
the arrows are functions (such as matrix multiplication). The input vectors are
coloured in red, output vectors in blue and RNN hidden states in green. From
left to right, it shows: (1) The typical FFNN style vanilla processing with ﬁxed
size input and output; (2) Sequence of outputs (Image captioning. One image
as input and a sequence of words as output); (3) Sequence of inputs (Sentiment
analysis. Sequence of word as input and sentiment as output); (4)Sequence
4.1
Recurrent Neural Networks
63

input and delayed sequence output (Machine translation); and, (5) Synced se-
quence input and output (Video frame labelling).
From a dynamic system viewpoint, two major classes of RNNs exist. The ﬁrst
class of models are characterised by symmetric connections and energy min-
imising stochastic dynamics (Lukoeviius & Jaeger, 2009). The best known ar-
chitectures of this category are Hopﬁeld Network (Hopﬁeld, 1982), Boltzmann
Machine (Hinton & Sejnowski, 1986), Deep Belief Network (Bengio, Lamblin,
Popovici & Larochelle, 2007) and Long Short Term memory (Hochreiter & Schmid-
huber, 1997a). These networks are mostly trained in some unsupervised learn-
ing scheme. Typical targeted network functionalities in this ﬁeld are associative
memories, data compression, the unsupervised modelling of data distributions,
and static pattern classiﬁcation, where the model is run for multiple time steps
per single input instance to reach some type of convergence. In contrast, the sec-
ond big class of RNN models typically features a deterministic update dynamics
and directed connections. Systems from this class implement non-linear ﬁlters,
which transform an input time series into an output time series. The mathe-
matical background here is non-linear dynamical systems (Lukoeviius & Jaeger,
2009). I will focus on the second category of RNNs as part of my work.
4.2 Formalisation of the Temporal Learning
Problem
A traditional machine learning task can be expressed mathematically as the
learning of functional dependence given an input x(n) ∈RNx and ground truth
y(n) ∈RN, where n = 1, · · · , T, and T is the number of samples in the training
dataset {x(n), y(n)}. In a static data scenario, there is no temporal dependence
between the samples, and the objective is to learn a function ˆy = f(x), such that
the error or loss function E( ˆy, y) is minimised. On the contrary, in a temporal
task, x and y are signals in a discrete time domain n = 1, · · · , T, and the goal
is to learn ˆy = f(x(1), · · · , x(n −1), x(n)), such that that E( ˆy, y) is minimised.
4.2
Formalisation of the Temporal Learning Problem
64

Figure 4.4.: Schematic diagram of a feed forward neural network vs. a reservoir com-
puting system (source Abel and Fompeyrin (n.d.)).
This formalism clearly shows that in the temporal learning scenario, the function
learned is stateful or memory driven as opposed to the stateless non-temporal
task. In a dynamic ﬁlter approach of RNN, it typically implements the non-linear
expansion of memory as a state vector of the form described in the equation
below:
h(n) = f(Winx(n) + Wx(n −1), · · · ), n = 1, · · · , T
(4.1)
where h is the activation function of a computational unit.
4.3 Reservoir Computing and Liquid State
Machines
Figure 4.4 compares the node level diagram of a typical FFNN and the reser-
voir computing approach. Reservoir computing as described by Schrauwen, Ver-
straeten and Van Campenhout (2007) is a dynamic ﬁlter, where an input signal
is fed into a ﬁxed and random dynamic system called reservoir, and the dynam-
ics of the reservoir performs non-linear expansion of the input into a higher di-
mensional space. A recurrence-free readout mechanism then takes the data and
maps it to the desired output. Figure 4.5 shows the intuition of reservoir comput-
4.3
Reservoir Computing and Liquid State Machines
65

Figure 4.5.: Principle of reservoir computing: The input states are transformed into
a high-dimensional feature space where classiﬁcation can be performed
with linear operation (source Abel and Fompeyrin (n.d.)).
ing paradigm in a schematic diagram. Since the expansion and the readout serve
different purposes, training/generating them separately and even with different
goal functions makes sense. The blue nodes in FFNN acts as non-dynamic and
non-linear transfer functions. On the other hand, the blue nodes in a reservoir
computing paradigm acts as a reservoir of recurrently connected neurons which
possess the ability to act stateful and transform data into a higher dimensional
space for the readout layer (green) to map into output with relative ease. The
"traditional" RNN training methods do not make the conceptual separation of a
reservoir vs. a readout, and train both reservoir-internal and output weights in
technically the same fashion. For a detailed review of reservoir computing, see
(Lukoeviius & Jaeger, 2009).
4.3.1 Liquid State Machines
Liquid State Machines (LSM) are a special kind of RNN architecture within the
reservoir computing paradigm proposed by Maass, Natschläger and Markram
(2002). LSMs were primarily intended for elucidating computational proper-
ties of microcircuits from a computational neuroscience perspective. Therefore,
4.3
Reservoir Computing and Liquid State Machines
66

LSMs possess great sophistication and biological realism with respect to the be-
haviour of the neurons within the reservoir. The reservoir of LSM is often de-
scribed as a liquid state. Input data feeds to the reservoir resembles "throwing
pebbles into a pond" creating ripples in the ﬂuid in the reservoir. The interac-
tion of the ripples creates useful patterns over space and time. The neurons of
LSM are spiking in nature and due to the biological plausibility are extremely
complex and parameter-heavy in nature. For this reason, the LSMs have proven
to be not only computationally expensive but also difﬁcult to tune. However, a
major advantage of using LSM is its ability to perform complex information pro-
cessing through temporal encoding in a highly efﬁcient manner. I will provide
discussion on this topic in Chapter 6. The main theoretical contributions of the
LSM brand to reservoir computing consist in analytical characterisations of the
computational power of such systems.
4.4 NeuCube Evolving Spatio-temporal Data
Machine
The brain is a complex integrated spatio-temporal information processing ma-
chine.
The mammalian brain is made up of spatially distributed structural
and functional areas constrained within a three dimensional space. External
stimuli and/or inner processes are of a varying nature, such as visual, audi-
tory, somatosensory, olfactory and so on, from which emanate a complex spatio-
temporal activity path within the brain leading to highly efﬁcient and accurate
recognition of patterns.
For example Benuskova and Kasabov (2010) provide the following example:
"· · · the language task involves transfer of information from the inner ear through
the auditory nucleus in thalamus to the primary auditory cortex (Brodmanns
area 41), then to the higher-order auditory cortex (area 42), before it is relayed
to the angular gyrus (area 39). Angular gyrus is a speciﬁc region of the parietal-
4.4
NeuCube Evolving Spatio-temporal Data Machine
67

Figure 4.6.: Schematic architecture diagram of NeuCube.
temporal-occipital association cortex, which is thought to be concerned with the
association of incoming auditory, visual and tactile information. From here, the
information is projected to Wernickes area (area 22) and then, by means of the
arcuate fasciculus, to Brocas area (44, 45), where the perception of language
is translated into the grammatical structure of a phrase and where the memory
for word articulation is stored. This information about the sound pattern of the
phrase is then relayed to the facial area of the motor cortex that controls artic-
ulation so that the word can be spoken. It turns out that a similar pathway is
involved in naming an object that has been visually recognised. This time, the
input proceeds from the retina and LGN (lateral geniculate nucleus) to the pri-
mary visual cortex, then to area 18, before it arrives at the angular gyrus, from
where it is relayed by a particular component of arcuate fasciculus directly to
Brocas area, bypassing Wernickes area."
The example above shows that the brain processes information through the ac-
tivation of complex spatiotemporal trajectories involving multiple functional ar-
eas. The NeuCube evolving Spatio-Temporal Data Machines are derived exactly
from this very philosophy.
The generic principles of the NeuCube architecture was introduced in (N. Kasabov,
2012) and was further elaborated in (N. Kasabov et al., 2014; N. K. Kasabov,
Doborjeh & Doborjeh, 2017; Sengupta, Ramos et al., 2018). Figure 4.6 shows
a schematic diagram of the NeuCube evolving Spatio-Temporal Data Machine
(eSTDM) architecture. This framework is designed to use the spatial and tempo-
ral relationships in the data and therefore expects a spatio-temporal data source
4.4
NeuCube Evolving Spatio-temporal Data Machine
68

to feed spatio or spectro temporal data (SSTD) to the system. The NeuCube
framework is a tiered architecture of three layers.
4.4.1 Encoding
The encoding layer is the ﬁrst layer of NeuCube. This layer is responsible for con-
verting real world continuous SSTD into a sequence of events or binary spikes.
Formally, the encoding layer transforms real continuous data RN×T (N is the fea-
ture count or the spatial component and T is the temporal component) into spike
trains {0, 1}N×T. Numerous temporal encoding algorithms like BSA (Schrauwen
& Van Campenhout, 2003), temporal contrast, GAGamma (Sengupta, Scott &
Kasabov, 2015) are proposed and used in an application speciﬁc manner. The
data encoding layer in NeuCube can also be observed as a data compression
layer which has the unique property of compressing data in temporal domain
by representing important events by spike-timings. In the temporal encoding
scheme, the timings of the spikes are considered to be useful rather than the
quantity of the spike. This is in contrast to the traditional data compression algo-
rithms like auto-encoder and PCA, as the compression in the data is performed
in a temporal dimension of the data. Sengupta and Kasabov (2017) discusses
the temporal encoding by spike-time representation in the light of data compres-
sion and information theory, and compares the capabilities of different temporal
encoding algorithms.
4.4.2 SNNc
The SNNc layer is claimed to be the most complex and novel component of this
architecture. It is an unsupervised learning layer composed of a 3D grid of spa-
tially arranged spiking and input neurons. Each neuron inside the grid has a
spatial location and resides within a neighbourhood of other neurons. This grid
is known as the spiking neural network cube (SNNc) in the NeuCube architec-
ture. The purpose of the SNNc layer is to transform the compressed spike rep-
resentation of input data into a higher-dimensional space through unsupervised
4.4
NeuCube Evolving Spatio-temporal Data Machine
69

learning (g : {0, 1}N×T →{0, 1}M×T|M >> N) inside an SNNc, using a mod-
iﬁed Hebbian spike-time dependent plasticity (STDP) learning (Song, Miller &
Abbott, 2000). The purpose of learning in the SNNc is to dynamically update the
synaptic strengths within the network to mimic spatio-temporal synchronicity in
the data.
STDP Learning
STDP is a temporally asymmetric form of Hebbian learning induced by temporal
correlations between the spikes generated by the pre and post-synaptic neurons.
With STDP, repeated pre-synaptic spike arrival earlier than post-synaptic spike
release leads to strengthening of the synapse known as long-term potentiation
(LTP), and, in contrast, repeated spike arrival after post-synaptic spikes leads to
weakening of the synapses known as long-term depression (LTD). The change of
the synapse plotted as a function of the relative timing of pre- and post-synaptic
action potentials is called the STDP function or learning window and varies
between synapse types. I will formalise and discuss on the STDP learning rule
and its variations as part of Chapter 5.
4.4.3 Readout
The readout layer is the last layer of the sequence in the NeuCube framework.
This layer digests the spike-time data generated by the SNNc and maps it to
relevant pattern labels (classiﬁcation) or values (regression). KNN-based models
(N. Kasabov et al., 2013) are the typical choice of supervised learning in almost
all of the work done until now. However, the architecture is ﬂexible enough
to include other supervised learning methods that can perform spike pattern
associations, such as SPAN (Mohemmed, Schliebs, Matsuda & Kasabov, 2012),
SpikeProp (Schrauwen & Van Campenhout, 2004), ReSuMe (Ponulak & Kasiski,
2010) etc.
4.4
NeuCube Evolving Spatio-temporal Data Machine
70

4.5 Departure of NeuCube from LSM
The NeuCube framework described in Section 4.4, in many respects, is inspired
from the LSM architecture. In fact, the NeuCube framework can be considered
as the next generation evolution of LSM. From Figure 4.6, it is quite clear that
the second and the third layer of NeuCube architecture is the same as that of the
LSM shown in Figure 4.4.
Prior to discussing the novelty that NeuCube provided in the last couple of lay-
ers, it must be realised that the encoding layer of NeuCube is unique to the
framework. One major disadvantage of the applicability of LSM is its inability
to perform computation in continuous valued data. The encoding layer of Neu-
Cube addresses this disadvantage of LSM by adding the encoding layer. Useful
encoding of high density data is of paramount importance for efﬁcient and effec-
tive recognition of patterns in the spatio-temporal domain. The encoding layer
typically aims to design algorithms to encode data following the temporal encod-
ing paradigm discussed earlier. The objective of these encoding methods is to
minimise the spike density while maximising the information retention in the en-
coded data. Realisation of such an objective ensures that the data encoder acts
as a data compression machine. The data compression by encoding allows for:
(1) Reduction of systematic noise in the data; and (2) Improvement in variability
between patterns, and thus makes it much more recognisable.
The NeuCube SNNc shares numerous conceptual similarities to the LSM, pri-
marily among which is the fading memory of the neurons, ability to transform
otherwise non-linearly separable data to higher-dimensional space. Some of the
limitations of LSM architecture are:
• In contrast to the contentions in (Natschläger, Maass & Markram, 2002),
LSM alone is not sufﬁcient to model human brain functionality.
4.5
Departure of NeuCube from LSM
71

• The effectiveness of the reservoir network in LSM is heavily dependent on
a random selection of parameters.
• Due to the random connections within the reservoir, LSMs are spatially
irrelevant and non-analysable. This makes the LSM behave very much like
a black box.
The NeuCube SNNc seeks to address these issues through creating a meaning-
ful structure of the network. The random connectome of LSM is replaced with
a meaningful spatially organised connectome structure in NeuCube. This con-
nectome is designed to physically encode a priori knowledge of the data being
processed. This allows inspection of the evolution of clusters of the model over
space and time. The spatio-temporal knowledge discovery and analysis through
inspection resembles the visualisation and analysis capability of Kohonen’s self
organising maps (Kohonen, 1990). However, the information represented in the
SNNc model is distinctly different from the Kohonen’s map. Contrary to the static
information representation in the synaptic strengths of the SOM, the SNNc can
capture the spatio-temporal dynamics. This feature in conjunction with the com-
mon neural network analysis techniques allows knowledge extraction from the
structure of the network. This means that general patterns, aberrant behaviours,
and insights not otherwise comprehensible can be surfaced by analysis.
4.6 Software Design Architecture of the
NeuCube Framework
Any theoretical and conceptual design of pattern recognition system is invari-
ably followed by implementation and design considerations. The design con-
siderations necessary for applying the generalised NeuCube architecture in an
application speciﬁc manner is well elaborated in (Scott, 2015) and provides an
useful insight on how to use a priori knowledge in designing the NeuCube archi-
tecture. The multi-modular and rather minimally-constrained description of the
4.6
Software Design Architecture of the NeuCube Framework
72

Figure 4.7.: Block diagram of the multi-modular NeuCube software architecture.
NeuCube framework allows high ﬂexibility in plugging in variety of algorithms
and innovations as part of the framework. This ﬂexibility also poses a great chal-
lenge in deﬁning a clear ground truth of singularity in NeuCube. The evolution
of NeuCube from a tool to perform neuro-informatic data analysis to a general
purpose pattern recognition machine has also constrained the software develop-
ment process to be uniﬁed in a single direction. Figure 4.7 introduces a block
level diagram of the NeuCube software implementation strategy. This thesis di-
rectly contributes towards the development of the blocks highlighted in black.
This architecture uses the core pattern recognition framework described earlier
as the core component, and wraps a set of functionalities around the framework.
There are three conﬁguration abstractions present in the implementation strat-
egy:
• Basic conﬁguration: The basic conﬁguration includes a prototyping and
testing module integrated with the I/O module. The main intention of
this conﬁguration is to use the NeuCube framework for general purpose
pattern recognition, prototype model development and experimentation.
Apart from model development capabilities, this module possesses addi-
tional integrated capabilities, such as dynamic visualisation, knowledge
discovery and model analysis, and parameter optimisation modules. Due
to the requirement of general purpose usage, this implementation is fo-
cused more towards usability and hence graphical user interface driven.
4.6
Software Design Architecture of the NeuCube Framework
73

Figure 4.8.: Standard conﬁguration of the NeuCube software architecture consisting
of the prototyping and testing module, immersive visualisation module
and the SpiNNaker neuromorphic hardware chip.
The basic conﬁguration is developed using Matlab and is freely available
(licensed) for public usage. I will describe the design of the prototyping
and testing software later in this Chapter.
• Standard conﬁguration: The standard conﬁguration is an extension of
the basic conﬁguration and includes development of scalable software for
large scale experiments, and general purpose and neuromorphic hardware
implementations. Since the prime focus is on scalability, the software im-
plementations for large scale simulation are API driven and much more
consideration is given to efﬁciency of implementation rather than usability.
Additionally, the standard conﬁguration also includes immersive visualisa-
tion of the NeuCube model for deeper analysis and knowledge discovery.
These implementations generally follow strict software patterns and uses
higher level languages like Java and Python. The contribution of this the-
sis towards the large scale implementation and simulation is presented in
Chapter 5.
• Full conﬁguration: In addition to standard conﬁguration, the full conﬁg-
uration includes all the application speciﬁc implementations of the Neu-
Cube, such as personalised modelling, multi-modal brain data modelling,
seismic event forecasting and so on. These application speciﬁc modules
4.6
Software Design Architecture of the NeuCube Framework
74

Figure 4.9.: NeuCube generic prototyping and testing software user interface and
panel descriptions.
are developed for speciﬁc application purpose and extends the NeuCube
architecture by adding application speciﬁc algorithms or functionality. This
thesis contributes towards the development of methods and functionalities
for multi-modal brain data. Chapters 6 and 7 are dedicated to this topic.
4.6.1 NeuCube for Generic Prototyping and Testing of
Applications
The generic prototyping and testing tool is a GUI based Matlab implementation
for rapid development of SNN application prototype systems for temporal or
SSTD. The software is implemented as a set of continuous signal processing
steps as shown in Figure 4.6. The user interface of NeuCube, as shown in Figure
4.9, is built as a set of functional components. An exhaustive description of the
components can be found in the manual distributed along with the software.
Here, I will brieﬂy describe some of them:
4.6
Software Design Architecture of the NeuCube Framework
75

Table 4.1.: Supported ﬁle format for descriptors.
Descriptor type
Mat
JSON
CSV
Dataset



SNNc



Parameter



Result



I/O and Information Exchange
The information exchange component is used to import or export user deﬁned
information to and from the software, which includes temporal or SSTD data,
NeuCube models, parameters and results. This module interacts with the exter-
nal environment using four data descriptors. They are the following:
• Dataset descriptor: The Dataset descriptor consists of the data (and the
meta data), that is to be learned and analysed. In the majority of cases, a
dataset consists of a set of time series samples and the output label/value
for the sample set. It is also possible to add miscellaneous information
like feature name, encoding method and other meta information in the
dataset.
• SNNc descriptor: The SNNc descriptor is essentially the model descriptor
containing all information related to the structure and learning of the SNN.
Some of the most important information stored in this descriptor are the
spatial information of the input and reservoir neurons, structural informa-
tion of the SNNc and the state of the SNNc during learning.
• Parameter descriptor: The parameter descriptor stores all the user deﬁned
parameters including hyperparameters of data encoding algorithms, the
unsupervised learning algorithm and the supervised learning algorithm.
• Result descriptor: The result descriptor stores information about the exper-
imental results produced by NeuCube.
4.6
Software Design Architecture of the NeuCube Framework
76

(a) data encoding.
(b) SNNcube initialisation.
(c) Unsupervised learning.
(d) Supervised learning.
Figure 4.10.: Algorithmic control UI panels.
This module supports three different ﬁle formats, Mat (binary), JSON (struc-
tured text) and CSV (comma separated plain text). Table 4.1 describes the sup-
ported ﬁle formats for each of the descriptor type. As a heuristic, the Mat format
is recommended for achieving faster I/O. The CSV ﬁles are the recommended
choice for import/export of dataset and results for later analysis. The JSON
format is recommended for inter modular communication.
Algorithm Interactions
This module allow users to interact with the pattern recognition and signal pro-
cessing algorithms via the algorithm controls panel, shown in Figure 4.9. The
algorithm control panel includes a set of buttons for conﬁguring and running
4.6
Software Design Architecture of the NeuCube Framework
77

the step by step process of data encoding, network initialisation, unsupervised
learning (by training the SNNc) and supervised learning. The software uses a
guided approach for performing the algorithmic steps by enabling or disabling
buttons after every operation. Figures 4.10a, 4.10b, 4.10c and 4.10d shows the
individual user interaction panels for encoding, initialisation, unsupervised and
supervised learning respectively. Each panel allows users to choose from a set
of algorithms and corresponding hyperparameters. For example, the data en-
coding panel (Figure 4.10a) that encodes the real-valued signal to spike trains
also provides the option of choosing from a set of encoding algorithms and its
hyperparameters from the drop down menu. The algorithmic control interacts
with the visualisation panels, for visualisation and analysis of the data and the
models.
Integrated Visualisation and Network Analysis
Visualisation and model analysis is an integral feature of this module. Due to
NeuCube’s ability to create an interpretable and analysable model, the visuali-
sation and analysis features plays an important part. Visualisation capabilities
includes: comparative display of real and encoded data; online dynamic visual-
isation of the SNNc learning; Visualisation of the SNNc model and the output
readout layer.
Analysis of an SNNc network can be performed using the network analysis tool-
box. This toolbox includes a bunch of graph analysis capabilities for measuring
and visualising interactions between nodes and edges the SNNc at different lev-
els. The network analysis consists of two major functionalities: (1) Neuron clus-
ter analysis; and, (2) Information route analysis. An example of neuron cluster
analysis is shown in Figure 4.11
Parameter optimisation
Parameter optimisation is developed to allow users to
search for the optimal set of hyperparameters for the NeuCube prototype sys-
tem (model). The computational time for parameter optimisation depends on
4.6
Software Design Architecture of the NeuCube Framework
78

Figure 4.11.: Neuron cluster analysis by network analysis toolbox (A)Neuron cluster-
ing based on connection weights of the network (B)average one to one
interaction between the input nodes. The thicker lines signiﬁes more
interaction.
the number of parameters to be optimised and the size of the NeuCube model.
Parameter optimisation, can be performed using various methods, such as: Grid
search; Genetic Algorithm; Differential Evolution; Quantum Inspired Evolution-
ary Algorithms, and PSO. The current release of NeuCube includes two methods
(Grid search and Genetic algorithm).
4.6.2 NeuCube Based Spatio-temporal Data
Research and Applications
Application of NeuCube in Brain Data Modelling
A number of recent studies have applied NeuCube architecture to brain data
modelling, especially using ElectroEncephalography (EEG) data and functional
Magnetic Resonance Images (fMRI) brain data. The spatio-temporal informa-
tion contained in EEG and fMRI data poses challenges to standard statistical or
machine learning techniques. Though, often standard machine learning tech-
niques are used to process spatio-temporal brain data, they lack the ability: (1)
to recognise differences in neurological dynamics that occur over the time; (2)
identify the functional brain area involved; and, (3) to quantify the information
involved. The RNN architecture of NeuCube, however, improves such capabil-
4.6
Software Design Architecture of the NeuCube Framework
79

ities (Taylor et al., 2014; Hu, Hou, Chen, Kasabov & Scott, 2014; N. Kasabov
et al., 2013). In (Capecci, Kasabov & Wang, 2015) for example, NeuCube was
used to study mental tasks using data collected by a 19-channel EEG recorder.
This research showed the ability of NeuCube to classify and analyse changes in
functional brain activities. This study was quite signiﬁcant for identiﬁcation of
the appearance of mild cognitive impairment (MCI) and the staging of its de-
generation toward Alzheimers Disease (AD). Recently, there has been a huge
interest in using functional magnetic resonance imaging (fMRI) to understand,
analyse and predict behaviour and cognition. The ability of fMRI to sample high
resolution spatial information over time has been successfully used in correlat-
ing high resolution neural activity with behaviour. NeuCube has been used in
several studies (Doborjeh, Capecci & Kasabov, 2014; N. K. Kasabov et al., 2017)
involving fMRI data.
NeuCube for BCI
The feasibility of using NeuCube with EEG data to develop a functional electrical
stimulation BCI/BMI system that is able to assist in the rehabilitation of complex
upper limb movements was shown in (Taylor et al., 2014). In order to provide
an effective tool for this purpose, a NeuCube model was trained on EEG data for
a series of relatively complex muscle movements. The preliminary experiments
suggest that NeuCube is much more efﬁcient for this task than standard machine
learning techniques, resulting in high recognition accuracy, a better adaptability
to new data, and a better interpretation of the models, leading to a better under-
standing of the brain data and the processes that generated it.
NeuCube for Neuro-rehabilitation
Neuro-rehabilitation is another area of research NeuCube was applied as feasibil-
ity analysis study. Biomimetic learning and information processing time-scales
of NeuCube provides appropriate technology for integration with mental tasks.
4.6
Software Design Architecture of the NeuCube Framework
80

In addition, the fast and incremental learning offered by the framework is capa-
ble of adapting to the users’ changing abilities as their rehabilitation progresses.
Repetitive activities of daily living (ADL) and robotic active training are com-
monly practised in the rehabilitation of paralysed patients, both of which have
been proven effective in the recovery of locomotor function in impaired limbs.
Classiﬁcation of ADL from EEG is of interest for the active robotic rehabilita-
tion of patients with spinal cord injuries (SCI). This classiﬁcation is a signiﬁcant
challenge with classical techniques, as these cannot manage the high noise, vari-
ability, and gradual change (due to the subject learning or rehabilitating the
task) in the EEG signals effectively. Hu et al. (2014) performed an experiment
using the NeuCube eSTDM to identify the upper-limb ADL of three classes with
14-channel EEG data. Classiﬁcation accuracy using this technique is shown to
be promising despite the highly noisy, low resolution EEG data (Hu et al., 2014).
This experiment indicates strong potential for further exploration of the Neu-
Cube for neuro-rehabilitation tasks.
NeuCube for SSTD Modelling in Other Applications
Several applications of NeuCube in the above areas are described in (N. Kasabov,
Scott et al., 2016; Doborjeh et al., 2018), including: individual risk of stroke
prediction based on personal, static data and temporal climate data; early exper-
imental results on earthquake prediction; predicting establishment of harmful
species in ecology; and stock index prediction.
4.6
Software Design Architecture of the NeuCube Framework
81

4.7 Contributions and Publications
Contributions
1. Introduction to recurrent neural networks and evolution of reservoir
computing and liquid state machines.
2. Overview of the NeuCube evolving spatio-temporal data machine archi-
tecture.
3. Description of the Software design framework of NeuCube and the Neu-
Cube prototyping and testing environment.
Publications
1. Kasabov, N., Scott, N. M., Tu, E., Marks, S., Sengupta, N., Capecci,
E., ... & Espinosa-Ramos, J. I. (2016). Evolving spatio-temporal data
machines based on the NeuCube neuromorphic framework: design
methodology and selected applications. Neural Networks, 78, 1-14.
2. Sengupta, N., Ramos, J.I.E., Tu, E., Marks, S., Scott, N.,... & Abbott,
A. (2018).
From von Neumann architecture and Atanasoffs ABC
to Neuromorphic Computation and Kasabovs NeuCube: Principles
and Implementations, Learning Sytems: From Theory to Practice
Springer.
4.7
Contributions and Publications
82

5
Data Structure Optimisation
and Software Architecture
Design for the
Implementation of
Large-Scale Spiking Neural
Networks
5.1 Introduction
Chapters 3 and 4 have, on numerous occasions, discussed the importance of ﬁnd-
ing a trade-off between the biological plausibility and the computational com-
plexity of computational models. The discussions have, especially, been focused
on the spiking neuron. In this Chapter, the concentration will be on implementa-
tion level design considerations for simulating scalable spiking neural network
architectures. The push towards big data analytics is driving the development of
fast, scalable and real-time pattern recognition systems all over the world. Scala-
bility of the machine learning framework plays a crucial role in realising the big
data dream. Interestingly, this issue of scalability was seldom solved using actual
scaling in machine learning, at least not in the big data sense. Part of the reason
is certainly that multi-core processors did not yet exist on the scale they do today
and that the idea of just scaling out was not as pervasive as it is today. Instead,
‘scalable’ machine learning is almost always based on ﬁnding more efﬁcient al-
gorithms, and most often, approximations to the original algorithm which can
be computed much more efﬁciently. According to Vowpal Wabbit (Langford, Li
& Strehl, 2007):
83

"There are two ways to have a fast learning algorithm: (a) start with a slow algo-
rithm and speed it up, or (b) build an intrinsically fast learning algorithm. This
project is about approach (b), and its reached a state where it may be useful to
others as a platform for research and experimentation."
This Chapter will be taking the ﬁrst approach and will use the second layer or
the SNNc component of the NeuCube framework (Read Chapter 4 for overview)
to drive the discussion. Before moving into the crux of this Chapter, a broad
picture of the contrasting properties of a digital computer will be painted. There
will also be discussions of a biological brain which poses signiﬁcant challenges
in integrating both in a single framework.
5.2 Brain vs. Digital Computers
Although, the brain-computer metaphor has served cognitive psychology well,
research in cognitive neuroscience has revealed many important differences be-
tween brains and computers.
1. Memory and storage: In computers, data stored in memory is accessed by
looking at a precise memory location. This is known as byte-address mem-
ory. On the contrary, the brain uses content-address memory, such that
data can be addressed in memory through activation spread from closely
related concepts.
2. Parallelism: The brain is naturally a massively parallel machine. Commu-
nication and synchronisations are automatically dealt with by the neuronal
mesh of the brain. On the other hand, von Neumann architecture driven
computer systems are modular and serial. Parallelism cannot be integrated
into such an architecture intuitively.
3. Processing speed and system clock: The speed of neural information pro-
cessing is subject to a variety of constraints, including the time for electro-
5.2
Brain vs. Digital Computers
84

chemical signals to traverse axons and dendrites, axonal myelination, the
diffusion time of neurotransmitters across the synaptic cleft, differences in
synaptic efﬁcacy, the coherence of neural ﬁring, the current availability of
neurotransmitters, and the prior history of neuronal ﬁring. Although there
are individual differences in processing speed, this does not reﬂect a mono-
lithic or unitary construct, and certainly nothing as concrete as the speed
of a microprocessor. Instead, psychometric processing speed probably in-
dexes a heterogeneous combination of all the speed constraints mentioned
above. Similarly, there does not appear to be any central clock in the brain,
and there is debate as to how clock-like the brains time-keeping devices
actually are.
4. Processing unit and memory interactions: Computers process information
from memory using CPUs, and then write the results of that processing
back to memory. No such distinction exists in the brain. As neurons process
information they are also modifying their synapses which are themselves
the substrate of memory.
5. Heterogeneity and size: The brain in its full maturity is a gigantic net-
work of over 86 billion brain cells (or neurons) with approximately 1750
synapses per neuron. It is estimated that a truly biological model of the
brain would have to include nearly 225 million billion interactions be-
tween cell types, neuro-transmitters, neuro-modulators, axonal branches
and dendritic spines. This means layers of hierarchy and heterogeneity is
present in the brain with complex interactions among them.
5.3 NeuCube SNNc
The second layer or the SNNc layer of the NeuCube (see Figure 4.6) is considered
to be the most complex layer of the NeuCube architecture and it is computation-
ally most expensive. For example, the NeuCube implementation distributed as
part of the prototyping and testing module requires gigabyte order memory for
5.3
NeuCube SNNc
85

running the unsupervised learning for an SNNc of size less than 1000 neurons.
Hence, it is necessary to concentrate on this layer to improve scalability of the
NeuCube system.
5.3.1 SNNc Architecture, Mapping and Initialisation
Scheme
The SNNc architecture of NeuCube consists of a spatially arranged (in three di-
mensions) set of neurons (computational units), denoted by set M (with |M|
deﬁning the cardinality of the set). The neurons are partially connected by re-
current synapses forming a directed incomplete and acyclic graph. The neurons
and synapses form the vertices and the edges of the graph. The SNNc architec-
ture is thus formalised as a directed graph G = {M, C, W}, consisting of the set
of neurons M, the set of directed synaptic connections, C and the corresponding
strengths or weights of the connections W. The network consists of two types of
neuron:
• Input neurons: The input neurons, denoted by set N ⊂M (with |N| deﬁn-
ing the cardinality of the set), feed the input spike data (generated by the
encoding layer) to the SNNc. These neurons do not have any activations
and do not perform any computations. The input neurons form a similar
level of abstraction as does the input layer in traditional neural networks.
It is apparent that the input neurons do not have pre-synaptic connections,
i.e., a synapse can only originate from such a neuron.
• Spiking neurons: The spiking neurons, denoted by set Q ⊂M (with |Q|
deﬁning the cardinality of the set), are the computational units, and are
leaky integrate and ﬁre (LIF) in nature. These neurons are responsible for
performing computations on input data. The details of the neuron model
is described later. These neurons can act both as post and pre-synaptic
(connection) neurons, i.e. if one considers a pair of neurons connected by
5.3
NeuCube SNNc
86

a directed synapse, the synapse can both originate and end at a spiking
neuron.
The neurons in the SNNc are arranged spatially following the natural spatial ar-
rangements in the data or by using automated mapping algorithms. Tu, Kasabov
and Yang (2017) maps feature covariance in the data to Euclidean distance in
the SNNc. The spatial arrangement of the neurons are typically done in two or
three dimensional space. The synaptic connectivity of the SNNc graph is created
using the small world connectivity (SWC) algorithm (N. Kasabov et al., 2014; Tu
et al., 2014). The SWC algorithm connects a neuron to its spatial neighbourhood
(controlled by the hyperparameter radial distance rswc) of neurons.
5.3.2 Neuron Dynamics Model
The activation of the spiking neurons present in the SNNc is modelled by the
spike response model (SRM), which is a kernel based simpliﬁed realisation of
the LIF model. The SRM model generalises the differential equation based dy-
namics of the LIF model (see Equation 3.7) by replacing them with arbitrary
kernels. SRM is a powerful computational framework for temporal integration
with elegant mathematical formulation.
Figure 5.1 shows a typical example of a spiking neuron’s conﬁguration. A spik-
ing neuron has a multi-input, multi-output conﬁguration. A pair of neurons are
connected by synapses. The synaptic strengths are represented by wi. A spiking
neuron receives spike data at different time instances from the pre-synaptic neu-
rons and emit spike data when sufﬁciently stimulated. The activation state of a
spiking neuron i is described by the membrane potential vi. In a non-stimulated
state, the membrane potential is said to be in a resting state vrest = 0. The SRM
model in the present setup consists of multiple components:
5.3
NeuCube SNNc
87

i
1
2
n
wni
w2i
w1i
Figure 5.1.: A typical connectivity conﬁguration of a spiking neuron i.
Post-synaptic Potential Kernel
Firing of pre-synaptic neuron j at time tf
j , evokes a post-synaptic potential (PSP)
in neuron i and is modelled by the kernel response function ϵ0.
ϵ0 = exp(−t −tf
j
τm
)H(t −tf
j )
(5.1)
where
H(t −tf
j ) =





1,
if t −tf
j ≥0
0,
otherwise
(5.2)
The PSP kernel is a function of t −tf
j , representing the PSP trace over time
generated by the ﬁring of neuron j over time. Figure 5.2 plots the PSP kernel
as the function of t −tf. τm (Equation 5.1) is the membrane constant which
controls the decay rate of the PSP.
Temporal Integration of PSP Kernels and Conditions for Spike Emission
Under the SRM model, the PSPs evoked by the pre-synaptic neurons are tempo-
rally integrated to activate the spiking neuron. The overall contribution of the
pre-synaptic spikes elicited by the pre-synaptic neurons j at any time t is given
as Equation 5.3 describing the SRM model:
5.3
NeuCube SNNc
88

0.00
0.25
0.50
0.75
1.00
0
25
50
75
100
t −tf
ε0
linetype
tau=0.5
tau=5
tau=50
Figure 5.2.: Plot of the PSP trace ϵ0 as a function of t −tf. This ﬁgure plots the PSP
simulation for different τm.
vi(t) = vrest +
∑
j∈Ti
wji
∑
tf
j ∈Fj
ϵ0(t −tf
j )
(5.3)
The temporal summation is a double summation operation. The inner sum adds
up the PSP contributions due to the ﬁrings tf
j ∈Fj of one pre-synaptic neuron.
The outer sum adds up the PSP contributions of all the pre-synaptic neurons
j ∈Ti connected to neuron i. Equation 5.3 describes the membrane potential
(activation state) vi of a spiking neuron i can be calculated by adding the rest-
ing potential term and the temporal PSP sum. Each incoming spike perturbs
the value of vi and if, after the summation of the inputs, the membrane po-
tential reaches the threshold vthr then an output spike is generated. The ﬁring
time is given by the condition vi(tf
i ) >= vthr. After a neuron ﬁres the neurons’
membrane potential is reset to vrest. In standard NeuCube implementations, the
inner sum is generally set to 1. By setting the inner sum to 1, NeuCube only
uses the information of present instance and forgets the inﬂuence of historical
spike-trains, thus demonstrating minimal memory in the neuron model. How-
ever, any instance of the usage of NeuCube based architecture in this thesis uses
the historical information with a preset hyperparameter controlling the memory
horizon.
5.3
NeuCube SNNc
89

Refractory Period
After emitting the spike, a spiking neuron enters a period of quietness known
as the refractory period. During this period, the membrane potential remains
unaffected by incoming spikes. The refractory behaviour can be mathematically
achieved by setting the membrane potential to a inﬁnitely low value. In the SRM
model the neuron behaviour under the inﬂuence of refractoriness depends only
on the last ﬁring moment leading to a short-term memory in the neuron. In the
literature, the refractory period is described by absolute and relative refractory
period. During the absolute refractory period, the neurons do not accumulate
membrane potential and hence cannot ﬁre. During the relative refractory pe-
riod, it can be relatively difﬁcult but not impossible to ﬁre the neuron. In this
current implementation, an absolute refractory period for the sake of simplicity
has been used here. The absolute refractory period of a neuron is speciﬁed by
the hyperparameter ηthr.
The modiﬁed SRM neuronal dynamics of NeuCube is formalised by Equation
5.4. The modiﬁcations of the canonical SRM model can be observed in: (1) the
implementation of the PSP kernel which outputs a unit pulse at the time neuron i
receives a spike from neuron j; (2) the implementation of refractoriness, where
the membrane potential is set to negative inﬁnity during the period ηthr after
neuron i ﬁres a spike at time tf
i .
vi(t) = vrest +
∑
j∈Ti
wji
∑
tf
j ∈Fj
ϵ0(t −tf
j ) + η(t −tf
i )
ϵ0(t −tf
j ) =





1,
if t −tf
j = 0
0,
otherwise
η(t −ti) =





−∞,
if t −tf
i < ηthr
0,
otherwise
(5.4)
Figure 5.3 shows a plot of three simulations of a spiking neuron for 200 discrete
times with random spike inputs. Each simulation uses a preset vthr. At the be-
ginning of the simulation, the neuron is in a resting state vt=0 = vrest. With the
5.3
NeuCube SNNc
90

0
100
200
300
400
500
0
50
100
150
200
t
vi
vthr
1000
20
200
Figure 5.3.: Plot of the membrane potential traces(vi) of a neuron i simulated over
T = 200 time points using the SRM model. For the simulations, 3 prede-
cessor neurons were connected to a spiking neuron. The spike data from
the predecessor neurons are sampled randomly from uniform random dis-
tribution. The ηthr for the spiking neuron was set to 10. Each of the three
vi traces correspond to a preset vthr mentioned in the label.
arrival of spikes, the membrane potential increases in a linear fashion and when
sufﬁciently stimulated (sufﬁciency is determined by vthr), the neuron spikes, and
then goes back to the resting state. At this point, the neuron is said to be in a
refractory state. The neuron stays in this state for a predetermined period ηthr
and then goes back to a non-refractory state.
5.3.3 Unsupervised Weight Adaptation in SNNc
The unsupervised weight adaptation mechanism in the SNNc is an extremely
important aspect of the dynamics. In a neural network paradigm, learning or
plasticity is achieved through the synaptic strength updates of the network. The
learning behaviour of the SNNc can be explained using the learning model of
a single spiking neuron. Considering the single neuron architecture in Figure
5.1, the unsupervised learning problem is to formalise a scheme of updating the
weights W of the network by ∆W(t) over the simulation time T. The NeuCube
5.3
NeuCube SNNc
91

−0.50
−0.25
0.00
0.25
0.50
−50
−25
0
25
50
ti −tj

ti
t
ti
tj
j
i
Figure 5.4.: Plot showing the functional dependence of the spike-time dependent plas-
ticity learning rule.
The STDP function shows the change of synaptic
weights ∆w as a function of difference in post and pre-synaptic spike-time
difference.
SNNc has employed numerous variations of temporally asynchronous forms of
Hebbian learning in different implementations.
STDP
STDP is a temporally asymmetric form of Hebbian learning induced by temporal
correlations between the spike-timings of pre- and post-synaptic neurons (Song
et al., 2000). As with other forms of synaptic plasticity, it is thought to underlie
learning and memory in the brain, as well as the development and reﬁnement of
neuronal circuits during the development of the brain. With STDP, repeated pre-
synaptic spike arrival, a few discrete times earlier to post-synaptic ﬁring leads to
Long Term Potentiation (LTP) of a synapse, whereas repeated spike arrival after
post-synaptic spike generation leads to Long Term Depression (LTD) of the same
synapse. The synaptic weight changes as a function the relative ﬁring-time of
5.3
NeuCube SNNc
92

the pre- and post-synaptic neurons, also known as the STDP learning window
(Gerstner & Kistler, 2002). The overall signiﬁcance of STDP lies in the ability
of a spiking neuron to discriminate between, and then integrate, temporally sig-
niﬁcant inputs and transforming that to meaningful output, even though the
actual meaning is not strictly known by the neuron (Markram et al., 2011). Net-
works that employ STDP operates as palimpsests, i.e. older stimuli are forgotten
gradually to make room for new ones.
Gerstner, Kempter, van Hemmen and Wagner (1996), Song et al. (2000) for-
malised the mathematical model of STDP learning as per Equations 5.5 and 5.6.
Symbols j and i are used to indicate pre- and post-synaptic neurons. In STDP
learning, the dynamic change of weight ∆w is estimated using a learning win-
dow function W(·). The learning window takes historical pre-synaptic ﬁring
times {t1
j · · · tf
j } and post-synaptic ﬁring times {t1
i · · · tg
i } as input and calculates
the LTP and LTD traces. These historical ﬁring times are nothing but indices of
a historical spike sequence. For example, a historical spike sequence [01001011]
can be rewritten as sequence of spike-time indices tf := {1, 4, 6, 7}. Exponential
decay functions are a popular choice for the learning window and Equation 5.6
is a good choice of the learning window function. The κ+ and κ−parameters
control the maximum LTP and LTD update respectively and κ−= κ+ = 1 is a
good choice to keep the bounds of weight update between [−1, 1]. From Equa-
tion 5.6, it can be observed that the polarity of (tg
i −tf
j ) deﬁnes the polarity of
∆wji. This is Hebbian model of causal relationship where synapses are rewarded
positively (strengthened) for causal ﬁring (i ﬁres later than j i.e. ﬁring of i is
caused by ﬁring of j) and penalised (weakened) for non-causal ﬁring. However,
Equations 5.5 and 5.6 describes a batch update scheme and requires modiﬁca-
tion for on-line learning in the SNNc. Sjöström and Gerstner (2010a) proposed
a modiﬁed on-line STDP update rule. In the on-line setting, ∆wji is calculated
every time neuron i ﬁres a spike or receives a spike from neuron j. Equation
5.7 formalises the weight update rule for on-line mode. The ﬁrst term in the
right hand side of Equation 5.7 corresponds to the LTP update and is calculated
when neuron i ﬁres a spike at time t. The second term is the LTD update and
is calculated when neuron i receives a spike from neuron j at time t. Both the
5.3
NeuCube SNNc
93

batch and on-line formalisations of STDP learning are extended from (Sjöström
& Gerstner, 2010b) which discusses the properties of the STDP learning model
extensively. Figure 5.4 shows the plot of the STDP learning function where the
∆w in quadrants I and III correspond to LTP and LTD respectively.
∆wji :=
∑
f
∑
g
W(tg
i −tf
j )
(5.5)
W(s) :=





κ+ exp(−s)
if s > 0
−κ−exp(−s)
if s < 0
(5.6)
∆wji(t) :=
∑
f
κ+ exp(−(t −tf
j )) −
∑
g
κ−exp(−(t −tg
i ))
(5.7)
It is evident from the discussion above that the STDP learning rule enhances or
depletes the synaptic strength of the connections, based on the relative coinci-
dence of the spikes. This behaviour mimics the ability of the biological neurons
to encode information by detecting the occurrence of temporally close but spa-
tially distributed input signals and thus incorporating spatio-temporal informa-
tion in the model.
Modiﬁed STDP in NeuCube
The standard NeuCube implementation uses a modiﬁed form of the STDP learn-
ing algorithm. The modiﬁcation mainly relates to when and which neurons are
updated. The contrast between STDP and modiﬁed STDP are:
1. As opposed to the STDP learning, in modiﬁed STDP learning, synaptic
updates are only performed when a neuron i, ﬁres a spike, and not when
it receives a spike.
5.3
NeuCube SNNc
94

−1.0
−0.5
0.0
0.5
1.0
−100
−50
0
50
100
t −tls
δw
Figure 5.5.: Plot of modiﬁed spike-time dependent plasticity learning rule imple-
mented in SNNc learning algorithm. The STDP function shows the change
of synaptic weights ∆w as a function of difference in last and present spike-
time difference.
2. When a neuron i ﬁres, both pre- and post-synaptic strengths are updated.
The pre-synaptic connections are strengthened and post-synaptic connec-
tions are depleted.
The weight update rule can be formalised by Equations 5.8 and 5.9, where tls
is the last spike time of a neuron and t is the current time instance. It is quite
evident that the smaller the difference between t and tls the more signiﬁcant the
weight update is. This weight update rule also implies that enhanced importance
is given to faster ﬁring rate.
κ+ and κ−, referred to as learning rate in the
NeuCube literature, control the upper and lower bound of ∆w similar to STDP.
Figure 5.5 shows the functional dependence plot of the modiﬁed STDP learning.
It is quite evident from the plot that the weight update curve is very similar to
the STDP weight update rule. The difference, however, lies in the functional
dependence as discussed.
∆wik(t) =
κ+
t −tls
j + 1
(5.8)
5.3
NeuCube SNNc
95

∆wji(t) = −
κ−
t −tls
k + 1
(5.9)
Algorithm 5.1 STDP based SNNc unsupervised learning algorithm
1: input:
G =
{M, C, W}, Dseq
∈
{0, 1}|N|×|T|, {hyperparameters
:=
vthr, ηthr, κ}
2: output: Oseq ∈{0, 1}|M|×|T|
3: for all t ∈T do
4:
initialise Clearn ←{}
5:
for all i ∈Q do
6:
ﬁnd ﬁring (at t −1) pre-synaptic neurons, Jspk(t−1)
i
7:
set Cltd
i
←(Jspk
i
, i)
8:
set Clearn+ ←Cltd
i
9:
simulate neuron i as per Equation 5.4
10:
if i ﬁres then
11:
set Oseq[i, t + 1] ←1
12:
ﬁnd pre-synaptic neurons, Jspk(t)
i
13:
set Cltp
i
←(Ji, i)
14:
set Clearn+ ←Cltp
i
15:
end if
16:
end for
17:
for cji ∈Clearn do
18:
calculate ∆wji(t) ←∑
f κ+ exp(−(t −tf
j )) −∑
g κ−exp(−(t −tg
i ))
19:
update wji+ ←∆wji
20:
end for
21: end for
5.3.4 Formal Description of SNNc Unsupervised
Learning Algorithm
A classical implementation of the SNNc unsupervised learning algorithm is for-
mally presented in Algorithm 5.2. This algorithm uses the modiﬁed STDP learn-
ing described in Section 5.3.3. The goal of the learning algorithm is to continu-
ously input data sequence Dseq in the form of spikes and simulate the network
G in a way that the synaptic strengths W of the network are updated over time
T to learn poly-synchronous relationships across space and time. The hyperpa-
rameters for neuron simulation (vthr, ηthr) and learning (κ) are also input into
the learning algorithm. At the end of the simulation, the algorithm outputs the
spike sequence Oseq. The simulation of G is performed at every time instance
5.3
NeuCube SNNc
96

Algorithm 5.2 Modiﬁed STDP based SNNc unsupervised learning algorithm
1: input:
G =
{M, C, W}, Dseq
∈
{0, 1}|N|×|T|, {hyperparameters
:=
vthr, ηthr, κ}
2: output: Oseq ∈{0, 1}|M|×|T|
3: for all t ∈T do
4:
initialise Clearn ←{}
5:
for all i ∈Q do
6:
ﬁnd ﬁring (at t −1) pre-synaptic neurons, Jspk(t−1)
i
7:
simulate neuron i as per Equation 5.4
8:
if i ﬁres then
9:
set Oseq[i, t + 1] ←1
10:
ﬁnd pre- and post-synaptic neurons Jspk(t)
i
and Kspk(t)
i
11:
set Cltp
i
←(Jspk(t)
i
, i) and Cltd
i
←(i, Kspk
i
)
12:
set Clearn+ ←Cltp
i
and Clearn+ ←Cltd
i
13:
end if
14:
end for
15:
for {cji, cik} ∈Clearn do
16:
calculate ∆wji ←
κ
t−tls
j +1 and ∆wik ←−
κ
t−tls
k +1
17:
update wji+ ←∆wji and wik+ ←∆wik
18:
end for
19: end for
t ∈T and is described within the loop between line 3 and 19 in Algorithm 5.2. At
every time instance, an empty variable Clearn is initialised, which stores over the
subsequent steps, a subset of connection identities (Clearn ⊂C) for the synaptic
strength updates. The simulation is then performed in two subsequent phases.
In the ﬁrst phase (line 5 to line 14), all the spiking neurons Q are simulated based
on the spiking neuron model. In NeuCube, the neuron simulations are done fol-
lowing Equation 5.4. In NeuCube, pre- (Jspk(t)
i
, i) and post-synaptic (i, Kspk(t)
i
)
connections of the spiking neurons that ﬁre at time instance t are candidates
for weight evolution. These connections are stored in Clearn for update. The
second step (line 15 to line 18) is the learning stage. During the learning stage,
the connections Clearn are updated according to the learning rule, which in case
of Algorithm 5.2 is the modiﬁed STDP learning rule. In addition, Algorithm 5.1
also presented the SNNc unsupervised learning algorithm using the canonical
STDP learning rule described in Section 5.3.3. A careful comparison between
the two algorithms reveal that the synaptic strength update in modiﬁed STDP is
drastically different from canonical STDP in regards to when and what synapses
are updated.
5.3
NeuCube SNNc
97

Considerations for Parallelisation
Neural networks are generally considered as a massively parallel problem, i.e.,
the computations are simultaneous rather than sequential in layers of a typi-
cal neural network. Therefore, the divide and conquer type of parallelisation
construct can be very easily achieved in neural networks by using the map and
reduce paradigm of functional programming. However, by observing the char-
acteristics of the SNNc learning algorithm, there does not seem to be a clear
parallelisation approach. This is caused by the recurrence present as the out-
put of the neurons in the SNNc layer, in the form of spike sequences which are
recurrently fed back to other neurons creating a neuron level dependency over
space and time. Therefore, although it is very tempting to merge the learning
step and the neuron simulation step across the spiking neurons together, the
asynchronous nature of the updates deems it an extremely hard parallelisation
problem. The system clock driven computational simulation at the moment is
clearly different from the clock precise parallelised scheme in the brain.
5.4 Analysis of the Data Structure
Representations of SNNc
In this Section, focus of attention will be on the SNNc network structure repre-
sentation G in light of Algorithm 5.2. The overall objective of the network struc-
ture representation analysis is guided by the objective to improve the computa-
tion and storage load of the algorithm as the unsupervised learning mechanism
evolves the network over time. This work speciﬁcally looks into the storage and
time complexity of the algorithm with increasing numbers of neurons in the net-
work. During the iterative simulation process, the connections and the weights
of the network are accessed very frequently. Parts of the SNNc network are ac-
cessed speciﬁcally in lines 6, 10 −12 and 15 −17. The access to the network is
theoretically a search operation within the search space C for information on
speciﬁc neuron identities. In particular, the algorithm needs to access the im-
5.4
Analysis of the Data Structure Representations of SNNc
98

Figure 5.6.: Example of the adjacency matrix representation.
mediate neighbours of a given neuron i. These are the pre- and post-synaptic
neurons Jspk
i
and Kspk
i
. A data structure that is used for representing the net-
work must, therefore, provide fast accessibility of neighbouring nodes. As an
additional constraint, the present work focuses on the algorithmic implementa-
tion of a general purpose von-Neumann architecture computer (as opposed to
the implementation on a neuromorphic hardware (Scott, 2015) setup), designed
for commodity consumption. Therefore, storage space and computing capacity
are of course constrained and an optimum data structure representation should
be storage- and time-efﬁcient. For the current experiments, a general purpose
PC running a 64 bit Windows 7 enterprise operating system was used; one that
had 16GB RAM, Intel Core i5 processor with 3.20 GHz clock speed. The imple-
mentations of Algorithm 5.2 was done in Matlab version R2014b.
In the Matlab based prototype and testing version of NeuCube, an adjacency
matrix was used to store the connection structure C. According to graph theory,
an adjacency matrix is deﬁned as a square matrix C of order M (number of neu-
rons) where 1 represents an existing edge between the two vertex indices. The
edges correspond to the connections, and the vertices are the neuronal unique
identiﬁers. In the present implementation, a second adjacency matrix is needed
to store the weights W in order to avoid confusion between the existence of an
edge and the weight values themselves. Figure 5.6 shows an example of an adja-
cency matrix. Since all values in an adjacency matrix can be directly accessed by
using the corresponding neuron IDs as indices, this data structure is extremely
fast. However, due to the nature of storing all relations between vertices, the
storage had grown at a squared rate, and it also showed to be the most storage
demanding option when compared to other data structures.
5.4
Analysis of the Data Structure Representations of SNNc
99

Figure 5.7.: Example of the edge-weight table representation.
The second option that was investigated was an edge-weight table. An edge-
weight table stores the connection and their weights in a simple look-up table
with each row containing a pair of neuron indices (identities) i and j, and the
weight of the connection between them, as shown in Figure 5.7. This edge-
weight table used far less storage than the adjacency matrix; however, it was
also considerably slower, because in order to access a connection between a pair
of neurons, the whole table had to be searched, which meant an average time
complexity of O(1
2c). Ordering the table by neuron i to use it as an index could
to some extent alleviate the problem for ﬁnding post-synaptic neurons, but not
for pre-synaptic neurons, since in that case only neuron j was given. Therefore,
this data structure is sub-optimal in regards to computation time especially in
case of larger networks.
The third data structure was an adjacency forward list. In graph theory, this is
a list of vertices in which each entry contains a sub-list of neighbouring vertices.
An example for an adjacency forward list is depicted in Figure 5.8. In terms
of storage space, the adjacency list, like the edge-weight table, is considerably
smaller than the adjacency matrix as it only stores the connections. Compared
to the edge-weight table, the adjacency forward list saves space by listing all
neurons connected to neuron i in an indexed list (the index being neuron i) in-
stead of repeating the index for every connection. However, for Algorithm 5.2, a
second adjacency forward list was needed to store the weight values of the con-
nections, which is why this data structure uses slightly more storage space than
the edge-weight table in the experiments. In terms of temporal performance, the
adjacency forward list was also very similar to the edge-weight table in that it
was faster to access post-synaptic connections due to indexing than looking up
5.4
Analysis of the Data Structure Representations of SNNc
100

Figure 5.8.: Example of the adjacency forward list representation.
pre-synaptic connections where all sub-lists had to be searched. However, the
indexing mechanism of the adjacency forward list provides a signiﬁcantly faster
look-up of post-synaptic connections than the edge-weight table. For these rea-
sons, the adjacency forward list is overall expected to scale up best for a larger
number of neurons, compared to the edge-weight table and the adjacency ma-
trix.
Taking into consideration that the adjacency forward list is a very storage-efﬁcient
data structure and that its main bottleneck for temporal performance is the look-
up of pre-synaptic indices, a decision was made to add a second adjacency list
called an adjacency backward list to the structure to represent the connections
from the opposite perspective, thus making the neuron at the end of the con-
nection (neuron j) the index of this second list. A schema of this approach is
shown in Figure 5.9. This backwards indexing mechanism caused a signiﬁcant
decrease of the algorithm’s execution time, because it effectively reduced the
time complexity of the data structure to O(1) for ﬁnding the right pre-synaptic
and post-synaptic indices. In comparison with the other data structures, the
adjacency forward-backward list was now the best alternative for representing
the SNNc network. Table 5.1 gives a comparative overview of the theoretical
complexities in regards to time and storage with the different data structures
discussed here. The complexities are measured by n and c referring to the neu-
ron count and connection count respectively. The relationship between n and c
can be represented by c = α × n2, where α ∈[0, 1] is the degree of sparseness.
All of the present experiments have used α = 0.02.
The ﬁndings from the theoretical analysis of the structures’ complexity could be
veriﬁed through the present experimental results. For the experiments, a bench-
5.4
Analysis of the Data Structure Representations of SNNc
101

Figure 5.9.: Example of the adjacency forward-backward list representation.
Table 5.1.: Comparison of time and storage complexity for different data structures.
data structure
connection type
time
storage
worst
average
best
adjacency matrix
all
O(1)
S(n2)
edge-weight table
all
O(c)
O(1
2c)
O(1)
S(c)
adjacency forward list
pre-synaptic
O(c)
O(1
2c)
O(1)
S(2 × c)
post-synaptic
O(1)
adjacency forward-backward list
all
O(1)
S(3 × c)
mark EEG dataset was used. The dataset consisted of EEG data measuring brain
signals during a task of wrist movement. The wrist movements were categorised
into upward, downward, and central directions. This task was performed on a
single subject and EEG data was sampled from 14 channels at a sampling rate of
128 Hz while the subject performed the task. 20 independent trials of one second
duration were collected while the subject performed each movement task. 14 of
the 1485 neurons in the reservoir were randomly chosen as input neurons for the
EEG channels. A network with a highly sparse connectivity (≈2% of all possible
connections) was initialised randomly for every experiment. The value of two
per cent was chosen because this was the average amount of connections used
typically during experiments.
Figure 5.10 shows the comparison of storage required in megabytes by each of
the data structures. The graph itself is dependent on the number of neurons in
the SNNc (between 0 and 70, 000 for the storage, and between 500 and 3, 500 for
the execution time). When it was decided that the number of neurons should
be increased further, all values above 80, 000 neurons for the adjacency matrix
and above 120, 000 neurons for the sparse matrix had to be excluded, due to
the above discussed technical restrictions in the experimental setup. In addition,
5.4
Analysis of the Data Structure Representations of SNNc
102

 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 0
 10000 20000 30000 40000 50000 60000 70000
storage (MegaBytes)
number of neurons
Adjacency matrix
Adjacency forward list
Adjacency forward-backward list
Edge-weight table
Figure 5.10.: Storage space comparison of different data structures with increasing
number of neurons.
it became difﬁcult to distinguish between the curves, especially in the lower
regions of the graph.
It is clearly visible that the adjacency matrix has the steepest increase of stor-
age space needed. The other three data structures are relatively similar in their
development, with the edge-weight table growing slowest. It was interesting to
see that the curve of the sparse matrix was very close to the one of the adjacency
list, which indicates that the internal representation of a sparse matrix in Matlab
is similar to an adjacency list. Figure 5.11 shows the results of comparing the
execution time of the data structures. For the comparison of execution times,
the curve of the edge-weight table showed its disadvantage as it increases near
exponentially. Thus, another graph representation without the edge-weight ta-
ble was created. This showed that the adjacency list was considerably slower
than the matrices and the adjacency backwards list.
These experimental results verify the previous theoretical analysis. The most
promising data structure for representing an SNNc network having a large num-
ber of neurons in the NeuCube SNN is the adjacency list with backwards index-
ing due to its constant complexity in terms of execution time of the algorithm,
and linear storage growth with increasing connection density.
5.4
Analysis of the Data Structure Representations of SNNc
103

 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 500
 1000  1500  2000  2500  3000  3500
time (seconds)
number of neurons
Adjacency matrix
Adjacency forward list
Adjacency forward-backward list
Figure 5.11.: Execution time comparison of different data structures with increasing
number of neurons.
5.4.1 Large-scale Unsupervised Learning of SNNc
Using the Adjacency Forward-backward List
As a conclusion of the current experimental results, it seemed evident that the
adjacency forward-backward list data structure representation is the most eco-
nomic in regards to storage and temporal complexity. To further demonstrate
this, the NeuCube SNNc learning algorithm in Matlab capable of using the ad-
Figure 5.12.: Snapshot of the spike activity at a time instant inside the SNNc.
5.4
Analysis of the Data Structure Representations of SNNc
104

Figure 5.13.: Strongest connections learned by the SNNc at the end of SNNc simula-
tion.
jacency forward-backward list data structure was implemented (see Appendix
C for the Matlab programme). In order to test the efﬁciency of the implemen-
tation on a much larger scale, Algorithm 5.2 was run on a SNNc consisting of
241, 606 neurons taking the same EEG input data described earlier. The SNNc
was simulated to mimic a brain with neural cells in the order of 106 and connec-
tions in the order of 1010. The spatial coordinates of the neurons were obtained
from the xjView (Cui, Li & Song, 2011) software and resembled the spatial dis-
tribution of the human brain based on the MNI coordinate system. Figure 5.12
shows a snapshot of spiking patterns at a certain time instance during the unsu-
pervised learning simulation. The ripple like behaviour of the liquid state within
the SNNc can be clearly observed in the network. The ripple effect in the SNNc
is of course clearly visible in a dynamic simulation environment. Figure 5.13
shows the strongest connections formed in the brain-like network as a result of
the unsupervised learning.
5.4
Analysis of the Data Structure Representations of SNNc
105

5.5 Considerations for Modularity and
Heterogeneity: Towards Graph Based
Software Design of SNNc
In the last Section, the considerations of scalability in regards to the size of the
SNNc was discussed. At this point, it is critical to mention that apart from the
large size, biological brains are considerably modular and heterogeneous. The
theory of modularity suggests that there are functionally specialised regions in
the brain that are domain speciﬁc for different cognitive processes. The brain
is often represented as a network of interconnected, dynamically interacting
elements. Cognitive processes are thought to result from the integration of neu-
ronal processing distributed across these complex networks at different temporal
and spatial scales. Several graph theory based methods (Sporns & Betzel, 2016;
Nicolini & Bifone, 2016) have been proposed in analysing the modularity and
heterogeneity in the brain.
The idea of modularity and heterogeneity is of major importance for the neuro-
morphic inspirations of the SNNc architecture design. Heterogeneity and mod-
ularity is observed not only in the brain but is quite prevalent in a broad range
of networks, such as groups in social networks, ensembles of interacting pro-
teins or coregulated genes in cellular network. These clusters or groups of items
have homogeneous property behaviours within the group, and vary considerably
between the groups across the network.
The objective of this Section was to design algorithmic or implementation im-
provements to facilitate heterogeneity and modularity in the SNNc. Let us revisit
the implementation of unsupervised learning algorithms (see Appendix C). The
functional style of implementation constrains the ability to inject heterogeneity
into the network, especially if one considers designing networks with varieties
of neurons, synapses, learning behaviour etc.
5.5
Considerations for Modularity and Heterogeneity: Towards Graph Based Software Design of
SNNc
106

The basic concept behind the template method design pattern is relatively simple.
Generally abstract classes are created representing necessary steps for a general
algorithm operation. An instantiation of the template (class) then implements
these steps with necessary extension. In Algorithm 5.2, the network G is repre-
sented by the tuple < M, C, W >. M is a list of neuron identiﬁers and serves
no purpose other than storing the identiﬁers. The code for neuron simulation
is integrated within the network simulation programme and is independently
treated compared to M. Additionally, weights are modelled independently of
the connections in Algorithm 5.2.
In order to overcome these shortcomings, the SNNc network in the graph based
design is constructed as a directed graph data structure. The graph based ar-
chitecture that was designed is summarised in the UML class diagram shown in
Figure 5.14. The graph G =< V , E > is made of vertices V = {v1, v2, · · · , vm}
and edges E = {e1, e2, · · · , ec}. The class SNNc is designed at the network level
of abstraction, where behaviours and operations are performed on the whole or
part of the network. Some operations on the network are initialiseNetwork():
used to initialise the SNNc network. Several algorithms can be implemented
as part of the SNNc class; learnNetwork(): Method to perform unsupervised
learning on the SNNc network. This method should be used to handle the syn-
chronisation and broadcasting of the data in the form of spikes across the net-
work; visualiseNetwork(): is used to visualise the dynamic and static states of
the network. The output of this method are similar to the ones shown in Figures
5.13 and 5.12. The vertices and edges forming the SNNc graph are designed
to be modelled individually. Personalised models of vertices and edges ensures
the ﬂexibility of implementing varying degrees of heterogeneity in the network
through encapsulation and polymorphism of the objects.
A vertex in graph G is modelled as an object with the following properties:
1. ID: Stores the unique identiﬁcation of a vertex.
2. Location: Stores the spatial location of the vertex in the 2D/3D space.
5.5
Considerations for Modularity and Heterogeneity: Towards Graph Based Software Design of
SNNc
107

3. Neuron: Refers to an instance of a suitable neuron model. The Neuron
object is modelled as an instantiation of the GenericNeuron class which
can morph into either input or spiking neurons. A couple of implementa-
tions of spiking neuron models that inherits from the GenericNeuron are
shown as LIFNeuron and IzhikevichNeuron. Each of the spiking neu-
rons are initialised by the init() method and simulated over time using the
simulate() method. It is evident that the realisation of any neuron models
or even newer behaviours in the existing neuron models can be achieved
without much effort through this modular design.
An edge in graph G consists of the following properties:
1. ID: Stores the unique identiﬁcation of an edge.
2. fromVertexID: Stores the source vertex ID of the edge.
3. toVertexID: Stores the destination vertex ID of the edge.
4. synapse: Refers to an instance of a suitable synapse model implemen-
tation.
An example synapse model is described in class Synapse.
The
primary behaviour of the synapse is controlled updateSynapse() method.
This method modiﬁes the synaptic strength (weight) using learning rules
implemented as static methods.
Overall to inject modularity and variety in the SNNc, the architecture has been
designed in hierarchical layers of abstraction in a top-down manner.
At the
highest layer of abstraction, the SNNc network has been designed only at net-
work level, keeping the vertex and edge level implementations abstract. In the
next layer of hierarchy, the individual vertices and edges are modelled and drills
down further into individual neuron models, learning behaviour and so on. Im-
plementing the architecture this way also allows a user to conﬁgure the SNNc at
varying degrees of generality.
5.5
Considerations for Modularity and Heterogeneity: Towards Graph Based Software Design of
SNNc
108

Figure 5.14.: UML Class diagram of the template based object-oriented architecture of the SNNc. This architecture should be considered an example
architecture for incorporating modularity. A python implementation of the SNNc following this architecture is presented in Appendix C.
5.5
Considerations for Modularity and Heterogeneity: Towards Graph Based Software Design of
SNNc
109

5.6 Summary and Conclusion
In this Chapter, the discussions have primarily revolved around the implementa-
tion aspects of the SNNc. This Chapter began by contemplating the discrepancy
between a biological brain and a computer. The discrepancies formed the ba-
sis of the challenges that arises in developing human brain-like computation
algorithms in computers. Then, there was a presentation of an in-depth for-
malisation, discussion and analysis of the several components of the SNNc ar-
chitecture and unsupervised learning algorithms in NeuCube. This discussion
paves the pathway for analysing the data structure representations of the SNNc
network with respect to scalability and heterogeneity. During this discussion, it
was shown how the network representation plays a decisive role in large scale
implementations, especially balancing the storage and execution time. The im-
portance of modularity and heterogeneity in SNNc was further discussed as well
as a proposal put forward for a hierarchical template pattern-based software
architecture for realising such a goal.
5.6
Summary and Conclusion
110

5.7 Contributions and Publications
Contributions
1. There were qualitative comparisons of the contrasting characteristics of
a biological brain and a computer.
2. In-depth descriptions about the SNNc layer of the NeuCube architecture
were made. This included SNNc network initialisation, simulation and
learning with formalisation of the unsupervised learning algorithm.
3. An analyses of the SNNc network storage architecture and data struc-
ture was presented in the light of large scale simulation of the SNNc.
4. A template pattern based software design architecture of SNNc to in-
corporate modularity and heterogeneity in the SNNc was proposed and
implemented.
Publications
1. Abbott, A., Sengupta, N., & Kasabov, N. (2016, July). Which method
to use for optimal structure and function representation of large spik-
ing neural networks: A case study on the NeuCube architecture. In
Neural Networks (IJCNN), 2016 International Joint Conference on
(pp. 1367-1372). IEEE.
5.7
Contributions and Publications
111

6
A Novel a priori Knowledge
Driven Temporal Encoding
Framework for Compressing
and Recognising Pattern on
Temporal Data
6.1 Introduction
Traditionally, data analytics, especially, predictive analytics and machine learn-
ing have focused on computer intensive approaches that are intended to be ap-
plied directly to the raw data present in the continuous space (Bishop, 2006).
These methods aim to take advantage of the ﬂexibility entailed by continuous
mathematics (uncountable sets) to build complex learning theories that can per-
form pattern recognition in data. However, with the rise of big data, the machine
learning technologies are dealing with new challenges with respect to real time
processing of massive volumes of data. Although the machine learning commu-
nity has continuously striven to learn from massive volumes of data, the devel-
opment has been grounded on the assumption that the computation ﬁts into the
memory seamlessly. In contrast, the current data size has grown to such a scale
that the data are becoming harder to store.
112

6.2 Data Compression and Inspirations from
Neural Coding
In this Chapter, the focus will be on the inspirations from the human brain that
allows us to see the problem of data processing and predictive analytics prob-
lem under the big data environment from an alternate perspective. It can be
observed that the continuous incoming stimuli in various forms and frequencies
processed by the human brain can indeed be characterised by the properties of
big data, i.e., volume, variety and velocity. The human brain is considered to be
the most resourceful and efﬁcient system which can recognise distinct patterns
in the streaming continuous stimuli (volume) captured by multiple sensory or-
gans (variety) in millisecond resolution (velocity). It is also observed that human
brain cells, when presented with external stimuli, propagates the signal econom-
ically, over long distances using electrical impulses or spikes via the synaptic
action potentials. Hence, it is imperative that there exists an efﬁcient system,
which converts the massive volume of continuous signal to discrete events or
spikes. In neurobiology, the process of such analog to digital signal transforma-
tion is known as neural encoding (Brown, Kass & Mitra, 2004). It is intriguing
that the process of neural encoding not only converts the big streaming con-
tinuous data space into a compressed space of spikes, but the brain cells also
recognise patterns in such a compressed space. The biological organisation of
the brain tends to create signals with a very speciﬁc class of distributions, and
it is from the perspective of evolutionary understanding that these distributions
are optimised for fast analysis. The most popular hypothesis states that the sig-
nal strengths are encoded by the mean ﬁring rate, i.e. stronger input signal
produces larger volumes of neuronal ﬁring on an average in the brain. A range
of studies (Mainen & Sejnowski, 1995; Maunsell & Gibson, 1992) across multi-
ple species in the sensory and motor-neuronal system supports the validity of the
mean ﬁring rate hypothesis. A major drawback of this theory, however, lies in
the association of information density with spike density. Determining the spike
density in millisecond resolution from a large volume of spikes leads to a level
of computational inefﬁciency. As per an alternate theory on neural encoding,
6.2
Data Compression and Inspirations from Neural Coding
113

neurons carry information in the precise timing of the spikes. This is known as
the temporal encoding or spike-time encoding. Numerous research (Gollisch &
Meister, 2008; Hallock & Di Lorenzo, 2006) has shown the presence of temporal
encoding in different parts of the human brain. Temporal encoding supports the
efﬁcient representation of information that is required for very fast processing
(in millisecond scale) of the stimulus presented to the human brain. As opposed
to the rate coding scheme, high ﬂuctuations in mean ﬁring rate, also known as
inter-spike interval (ISI) probability distribution is considered to be informative
rather than noise in this scheme. The temporal spike-time representation of the
data acts as a lossy compression of information. Most forms of learning, though,
could be seen as forms of data compression. In fact, one can, in terms of pattern
recognition, only learn something from data when there is redundancy in the
data. In many data analysis tasks, the data is preprocessed or recoded in a way
that could be seen as a form of data compression. If such preprocessing does
not destroy the patterns of interest, it results in a comparative performance of
the learning algorithms. The motivation of the temporal encoding, thus, in this
context is to reduce large volumes of data into a compressed state with minimal
loss and the maximal presence of discriminable information. Examples of data
sources where such encoding is useful are high-frequency streaming data, such
as the pulsar data in radio astronomy, and seismic activity data.
6.2.1 Information Theory and Data Compression
From the viewpoint of computational theory, the data encoding problem relates
to the concepts of information theory. In the seminal work on information the-
ory, Shannon (1948) proposed a mathematically complete theory to quantify
transmission of information in a communication channel. A conclusive ﬁnding
that the amount of information in any object can be estimated as the description
length of the object continues to set the stage for the development of commu-
nications and data processing. Shannon’s information theory is built on a pre-
supposition that the computable information in an object is the characteristic of
a random source with known probability distribution of which the object is a
6.2
Data Compression and Inspirations from Neural Coding
114

part. To realise this idea, Shannon derived the ‘entropy’ from the ﬁrst principle
of the theory, which is the measure of average information emitted by an object
when observed. It can be described as the functional mapping of the random
variable to a real number. Kolmogorov (1965), later proposed an alternate and
more generalisable notion of information measurement known as algorithmic
information theory. Contrary to Shannon’s theory, Kolmogorov’s theory of com-
plexity (Kolmogorov, 1965; Chaitin, 1966) considers information as the property
of an object in isolation, irrespective of the way the object came into existence
(Grunwald & Vitányi, 2004). It describes information as the minimum number
of bits from which a message or a ﬁle can effectively be reconstructed, i.e. the
minimum number of bits sufﬁce to store a reproducible ﬁle. A computational
neuron responsible for emitting spikes from sensory data can be regarded as
a logical transmission medium responsible for broadcasting continuous informa-
tion received from the data source. The two neural coding hypotheses hence can
be seen and described in the light of information theory. It can be observed that
the rate coding scheme adheres to Shannon’s interpretation of encoding. The
inherent assumption of the presence of a random source with a known probabil-
ity distribution in Shannon’s theory is much apposite to the mean ﬁring rate as
it relates to the frequency of spikes over time. However, the interest in efﬁcient
compression of a large volume of data by a sequence of spike-timings and using
it for the purpose of pattern recognition is much more in line with Kolmogorov’s
notion of object representation by minimal description length using computer
programs.
6.3 Literature Review on Analog to Digital
Data Transformation Algorithms
A signiﬁcant amount of research has focused on using the biological realism
of the SNN for information processing applications akin to traditional neural
networks (Maass, 1997). Under the broad umbrella of SNN, the area of data
encoding has been relatively unexplored compared to neuronal dynamics, net-
6.3
Literature Review on Analog to Digital Data Transformation Algorithms
115

work learning behaviours and so on. Human Information Processing Research
Laboratory’s (Advanced Telecommunication Research Institute) artiﬁcial brain
(Cellular Automata Machine Brain) project (De Garis, 1994) used data encod-
ing as a part of its large-scale brain-like neural architecture. Hardware acceler-
ated implementation of spike encoding for image and video processing was per-
formed by Iakymchuk et al. (2014). The literature on the application of spike
encoding on the information processing task in data science is restricted to a
few algorithms, such as temporal-contrast (TC) (N. Kasabov, Scott et al., 2016),
Hough spiker algorithm (HSA) (Hough, De Garis, Korkin, Gers & Nawa, 1999)
and Bens spiker algorithm (BSA) (Schrauwen & Van Campenhout, 2003). HSA
and BSA algorithms are event-driven in nature and can be classiﬁed under the
temporal encoding paradigm where the time of occurrence of an event (spike)
is considered as a unit of information. The TC algorithm, also known as AER
encoding, is inspired from the human visual cochlea. The TC algorithm uses a
threshold-based method to detect signal contrasts or changes (N. Kasabov, Scott
et al., 2016). A user-deﬁned or auto-generated contrast threshold determines
the spike events in the TC algorithm. The HSA and BSA algorithm, however,
determine a spike event using a deconvolution operation between the observed
signal and a predeﬁned ﬁlter. The HSA method which is based on convolution
function produces a biased converted signal where it always stays below the orig-
inal waveform and this would yield an error. The BSA method on the other hand
uses the Finite Impulse Response (FIR) reconstruction ﬁlter. Even though BSA
reduces the error in the HSA method and has less optimal threshold sensitivity,
this method like HSA, needed a suitable ﬁlter for every type of input. Find-
ing this ﬁlter automatically for each image would require a tremendous amount
of work and time. There are some LIF neural network modelling approaches
that for analog to digital transformation applied to computer vision (Van Rullen,
Gautrais, Delorme & Thorpe, 1998). These methods, however, require a large
number of input neurons.
6.3
Literature Review on Analog to Digital Data Transformation Algorithms
116

6.4 A General Framework of Spike-time
Encoding and Compression for Temporal
Data Sequences
The temporal encoding problem for pattern recognition can be formalised as a
data compression problem. An encoder is hence deﬁned as the map E : RT →tf,
where the encoder E(·) release spikes at ﬁring times tf := {tf
1, tf
2, · · · tf
n|ti ∈I+}.
The temporal encoding algorithm primarily assumes that the discriminatory in-
formation is encoded by the sequence of discrete spike-timings rather than the
magnitude and/or the spike density. As a consequence of this assumption, the
temporal encoding aims at joint maximisation of information representation and
minimisation of the spike density. Thus, it is in sharp contrast to the rate coding
hypothesis. Next, the proposed a priori knowledge driven generalised frame-
work for temporal encoding will be presented. This framework will be further
extended to formalise a temporal encoding algorithm for fMRI data.
6.4.1 Formalisation of the a priori Knowledge Driven
Optimisation Problem for Data Encoding
If one assumes a continuous source signal is represented by s ∈RT represent-
ing a vector of continuous values, an encoded data or a spike-train is repre-
sented by b ∈{0, 1}T as a ﬁxed-length binary sequence. This formalisation
is slightly modiﬁed from the variable length sequence formalisation of spike-
timings {tf
1, tf
2, · · · tf
n|ti ∈I+} deﬁned earlier without any loss of generality. For
example, a historical spike sequence [01001011] can be rewritten as a sequence
of spike-time indices tf := {1, 4, 6, 7}. T denotes the length of the temporal sig-
nal to be encoded into spikes. The a priori knowledge driven optimisation based
encoding framework is built on the premise that
• The universal data encoder is non-existent.
6.4
A General Framework of Spike-time Encoding and Compression for Temporal Data
Sequences
117

• a priori knowledge about the data generation process or in other words,
prior knowledge of the properties of the data generation source can be
injected into a predictive system that can generate a predicted signal ˆs.
For example, the fMRI data generation process behaves like a linear time in-
variant system, where a spike in the brain cell gives rise to a signal mimicking
the gamma distribution function (Ashby, 2011), whereas the process of EEG
data generation can be modelled as a phase varying mixture model of sinusoidal
waves or multi-source Gaussian noise model (Nunez, Nunez & Srinivasan, 2016).
The notion of knowledge injection is further elaborated in Section 6.5 using fMRI
as an example. If it is possible to formalise a decompression function ˆs from the
spike sequence b, the optimal encoding of data can be formulated as an optimi-
sation problem that minimises the discrepancy between the predicted and the
original signal. One way of realising such a discrepancy is by minimising the
root mean squared error (RMSE) of decompression between the observed signal
s and the predicted signal ˆs := f(b, Θ), Θ being the set of additional parame-
ters required along with b to describe the prediction function. The optimisation
problem can be formulated as:
min
b,Θ
√∑
t(st −ˆs(bt, Θ))2
t
s.t.
bt := I+
0 ≤bt ≤1
∑
t
bt ≤α
β ≤Θ < γ
(6.1)
The above optimiser solves for the RMSE, subject to the following constraints:
1. Binary constraints for spikes: The binary constraint for the spikes are im-
plemented by forcing Bt to be an integer and within a range of [0, 1].
2. Constraint on the number of spikes: The ∑
t Bt ≤α constraint enforces the
maximum number of spikes to be limited to a. This constraint is of major
6.4
A General Framework of Spike-time Encoding and Compression for Temporal Data
Sequences
118

importance from a biological plausibility perspective. Since the encoding
scheme discussed here, aims to mimic the temporal coding behaviour of
the human brain, it is always preferable to encode maximal information
with the minimal number of spike.
3. Bounds can be set on the other parameters Θ to be optimised as part of
the prediction model f(B, Θ).
Mixed Integer Optimisation and Genetic Algorithms
The aforementioned optimisation problem is one related to mixed-integer pro-
gramming optimisation. A mixed integer programming problem is an optimisa-
tion problem, linear or nonlinear, with or without constraints, in which some
or all decision variables are restricted to have integer values. Such problems
frequently arise in various application ﬁelds such as process industry, ﬁnance,
engineering design, management science and others.
Several classical computational techniques (such as, branch and bound tech-
nique, cutting planes technique, and outer approximation technique), which are
reasonably efﬁcient, have been proposed in the literature for solving mixed inte-
ger programming problems (Cooper, 1981; Floudas, 1995; Grossmann, 2002).
Over the last couple of decades, several stochastic algorithms have been de-
veloped and appropriately updated for problems related to mixed integer pro-
gramming, such as simulated annealing, differential evolution and ant colony
optimisation (Dorigo, Maniezzo & Colorni, 1996; Babu & Jehan, 2003; Yiqing,
Xigang & Yongjian, 2007). However, algorithms of this class generally harbour
the capacity to provide near global optimal solutions, although the quality of the
obtained solution is unstable and requires large amount of computation time.
Genetic algorithms (GA) are general purpose population based stochastic search
methods inspired by Charles Darwin’s principles of natural selection and genet-
ics. Holland (1992) introduced the concept of GA, and it was used by Jong
6.4
A General Framework of Spike-time Encoding and Compression for Temporal Data
Sequences
119

(1975) to solve the optimisation problem. Goldberg (1989) presents a detailed
implementation of GA. To describe it in a simple manner, GA searches for sets
of better solutions in the global search space. Potential solutions termed as chro-
mosomes (individuals) are evolved iteratively over generations using a set of
genetic operators such as selection, crossover and mutation. The quality of a
solution or a population is measured by a ﬁtness function, which is equivalent to
a loss function in the ﬁeld of machine learning and objective function in optimi-
sation. The ﬁtness function is responsible for evaluating how ‘ﬁt’ a chromosome
is for reproduction. The selection operator chooses the best ‘ﬁt’ chromosomes
for reproduction. In the reproduction process, new chromosomes are created by
crossover and mutation operations. The Crossover operator blends the genetic
information between chromosomes to explore the search space, whereas the
mutation operator is used to maintain adequate diversity in the population of
chromosomes to avoid premature convergence. The way the variables are coded
into the chromosome is clearly essential for GAs’ efﬁciency. Real coded genetic al-
gorithms (RCGAs), which use real numbers for encoding, have fast convergence
towards optima as compared to binary and Gray coded GAs (Deb, 2001). Also,
‘RCGAs’ overcome the difﬁculty of Hamming Cliff as in binary coded GAs. In the
case of integer and mixed integer programming problems, many applications of
GAs are available in the literature, some of which use binary coded representa-
tion (Cheung, Langevin & Delmaire, 1997; Luo, Guignard & Chen, 2001; Hua &
Huang, 2006) and some use real coded representations (Y.-X. Li & Gen, 1996;
Yokota, Gen & Li, 1996; Maiti, Bhunia & Maiti, 2006).
Integer programming with GA modiﬁes the vanilla GA algorithm in several ways:
1. It requires custom creation, crossover and mutation function in order to
enforce the variables to be integers (see (Deep, Singh, Kansal & Mohan,
2009) for detail).
2. The genetic algorithm attempts to minimise a penalty function, not the
ﬁtness function. The penalty function includes a term for infeasibility. This
penalty function is combined with binary tournament selection to select
6.4
A General Framework of Spike-time Encoding and Compression for Temporal Data
Sequences
120

individuals for subsequent generations. The penalty function value of a
member of a population is:
• If the member is feasible, the penalty function is the ﬁtness function.
• If the member is infeasible, the penalty function is the maximum ﬁt-
ness function among feasible members of the population, plus a sum
of the constraint violations of the (infeasible) point. For details of the
penalty function, see (Deb, 2000).
3. GA does not enforce linear constraints when there are integer constraints.
Instead, it incorporates linear constraint violations into the penalty func-
tion.
In the present implementation, the mixed integer genetic algorithm solver (Deb,
2000; Deep et al., 2009) was used. The constraints in Equation 6.1 are imposed
on the parameters of ˆs. The ﬁrst and second constraints are used to reduce the
search space of the possible values of bt to {0, 1}. The hyperparameter α is used
to control the maximum number of spikes and hence the spike density in the
optimal solution. The other sets of hyper-parameters {β, γ} are used to control
the upper and lower bounds of the model parameter Θ.
The formulation above for the proposed framework for data encoding is generic,
ﬂexible and is driven by knowledge-injection from the data source. The knowledge-
injection component allows the further inclusion of systematic noise as part of ˆs,
if present. Examples of the inclusion of noise models, such as acoustic noise as
part of linear time invariant models of fMRI are treated in (Sierra, Versluis, Hoog-
duin & Duifhuis, 2008; Z. Cho et al., 1997). The hypothesis is that a sufﬁciently
good choice of ˆs(b, Θ) preserves, in some cases, enhances the discriminative
property of the data in a greatly compressed space. It must also be noted that
this formulation adheres to the concept of the non-existence of a universal com-
pression algorithm for all the data sources. The general framework described
above can be used to derive speciﬁc methods for encoding of special types of
6.4
A General Framework of Spike-time Encoding and Compression for Temporal Data
Sequences
121

data for which a priori knowledge is available. One such case is fMRI data based
on blood-oxygen level dependent response (BOLD). This is further introduced
and illustrated in Section 6.5.
6.5 GAGamma: A Spike-time Encoding and
Compression Method for fMRI Data
This Section will formalise a sample prediction model f(B, Θ) for functional
Magnetic Resonance Imaging (fMRI) data, and will present experimental results
and evaluation of data encoding by solving Equation 6.1.
6.5.1 fMRI As a Linear Time Invariant System
Functional magnetic resonance imaging (fMRI) is a form of magnetic resonance
imaging that takes advantage of magnetic susceptibility artefacts caused by the
deoxygenated haemoglobin in the brain. Magnetic susceptibility measures the
magnetic properties of the interaction between a tissue or other substance and
the in-scanner magnetic ﬁeld strength. Magnetically susceptible materials distort
the homogeneity of a magnetic ﬁeld: materials with negative magnetic suscepti-
bility are known as diamagnetic, and those with positive magnetic susceptibility
are referred to as paramagnetic. Introduction of a paramagnetic substance such
as deoxyhaemoglobin into the scanner magnetic ﬁeld causes variability in ﬁeld
strength, spin dephasing, geometric distortion and loss of signal; fMRI exploits
this property by measuring changes in the relative ratio of oxygenated (diamag-
netic) to deoxygenated (paramagnetic) haemoglobin in the blood.
Functional Magnetic Resonance Imaging (fMRI) is most commonly acquired us-
ing Blood Oxygen Level Dependent (BOLD) response. The BOLD response is
measured by the changes in deoxyhaemoglobin at time t, and is caused by neu-
ral activation in the brain. The neural activations are caused by some sequence
6.5
GAGamma: A Spike-time Encoding and Compression Method for fMRI Data
122

of events driven by the task performed by the subject (Friston et al., 1995). The
BOLD response is mathematically described as a time invariant system, i.e. a
system whose output does not depend explicitly on time. Under the appropriate
experimental protocol, BOLD response also pertain to the superposition principle
and henceforth can be designed as a linear time invariant (LTI) (Vazquez & Noll,
1998). According to C.-T. Chen (1995), a LTI system is said to be completely
characterised by convolution integral functions. The fMRI BOLD is described by
the convolution of the spikes b and the haemodynamic response function (HRF),
h(Θ). This operation is characterised by Equations 6.2 and 6.3.
ˆs :=
∫t
0 b(τ)h(t −τ)dτ
(6.2)
ˆs(b, Θ) := b ∗h(Θ)
(6.3)
h(θ1, θ2) :=
1
θθ1
2 T (θ1)
tθ1−1e−t
θ2
(6.4)
6.5.2 GAGamma Optimisation Problem for fMRI
Numerous models for HRF have been proposed in the literature (Boynton, Engel,
Glover & Heeger, 1996; Friston, Josephs, Rees & Turner, 1998; Glover, 1999).
The majority of mathematical models for the canonical HRF are found to be
some variant of the gamma function. In all the experiments performed here,
the gamma distribution function has been used as the HRF model (Equation
6.4). This function is characterised by the parameter set Θ := {θ1, θ2}, where
θ1 ∈R+ and θ2 ∈R+ controls the shape and the scale of the gamma function
respectively. By substituting Equations 6.3 and 6.4 in Equation 6.1, the encoding
problem can be reduced to solving Equation 6.5 and is referred to as GAGamma
encoding algorithm hereafter.
6.5
GAGamma: A Spike-time Encoding and Compression Method for fMRI Data
123

min
b,θ1,θ2
√∑
t(st −ˆs(bt, θ1, θ2))2
t
s.t.
bt := I+
0 ≤bt ≤1
∑
t
bt ≤α
β1 ≤θ1 ≤γ1
β2 ≤θ2 ≤γ2
where
ˆst(bt, θ1, θ2) := bt ∗
1
θθ1
2 T (θ1)
tθ1−1e−t
θ2
(6.5)
6.5.3 Distinction of GAGamma from HSA and BSA
At this point, it is imperative to make the distinction between the GAGamma
and the existing HSA and BSA algorithms. Apart from exhibiting similarities
in convolution framework, HSA and BSA also resemble GAGamma as methods
of stimulus estimation using FIR. Nevertheless, the data encoding approach in
HSA and BSA use a deconvolution (of Equation 6.2) approach contrary to the
optimisation approach in GAGamma. The knowledge-injection component of
GAGamma, as part of formalisation of ˆs and the optimisation approach, has two
distinct beneﬁts over the deconvolution-based methods:
• A generic Gamma function has been used as the knowledge-injection com-
ponent to ˆs in GAGamma, which is driven by the existing knowledge about
the fMRI data as opposed to the generic sinusoidal function used as the
FIR in BSA. It is also argued that this formulation allows the inclusion of
additional knowledge about the data source (such as systematic noise) if
present, providing greater ﬂexibility in the formulation of the encoding
algorithm.
• The optimisation problem formulation in GAGamma jointly optimises for
the parameter set Θ and b. This formulation thus includes the parameter
set Θ of the prediction model ˆs and the spike sequence b for each individ-
ual voxel or feature. In HSA and BSA, the equivalent ﬁlter parameters are
6.5
GAGamma: A Spike-time Encoding and Compression Method for fMRI Data
124

predetermined for the whole set of voxels and are not learned from the
data.
• The constraint ∑
t bt ≤α in GAGamma ensures the ﬂexibility in choosing
the desired spike density, hence the ability to control the compression and
quality of signal reconstruction. The BSA or HSA algorithm, on the con-
trary, accommodates no such control in the encoding framework.
6.6 Experiments and Evaluation
6.6.1 Description of Dataset
The experiments described in this Chapter were performed on the publicly avail-
able benchmark Starplus fMRI dataset (Mitchell & Wang, 2001) collected by The
Centre for Cognitive Brain Imaging, Carnegie Mellon University. The Starplus
experiment was conducted on a set of 7 subjects. Each subject had undergone
multiple trials of exactly the same cognitive experiment. At every trial lasting for
27 seconds, a set of stimuli were presented to a subject in the following order:
1. The ﬁrst stimulus (picture or sentence) was presented at the beginning for
4 seconds.
2. A blank screen was presented during the interval of 5 −8 seconds.
3. The second stimulus (sentence or picture) was presented during the inter-
val of 9 −12 seconds.
4. A rest period of 15 seconds was added after the presentation of the second
stimulus.
6.6
Experiments and Evaluation
125

While the subject performed the cognitive tasks, fMRI images from speciﬁc re-
gions of interest (ROI) of the brain were collected at every 500ms interval. The
preprocessed fMRI dataset has been used in a number of pattern recognition
studies (Mitchell et al., 2003; Mitchell et al., 2008; Shinkareva et al., 2008). In
this study, this benchmark dataset was chosen to build pattern recognition sys-
tems that can predict and discriminate between the binary cognitive states of a
subject ‘seeing a picture’ and ‘reading a sentence’. Two subjects were chosen (id:
04847 and 07510) randomly and two spatial ROIs; Calcarine Sulcus (‘CALC’)
and Left Intra-Parietal Sulcus (‘LIPL’) were used for the experiments. The choice
of the ROI is based on previous work (Do & Yang, 2014) that found these ROIs
to be amongst the most discriminatory in the raw continuous data space. The
dataset for each subject is composed of 40 samples (trials) of each class, and
each sample is made up of 452 and 483 voxels in subject 04847 and 07510 re-
spectively. Each cognitive task lasted for a total of 8 seconds emitting 16 fMRI
images for each class within a trial.
6.6.2 Evaluation Metrics
Three metrics have been used to evaluate and compare the performance of
the encoding techniques and the traditional ‘no-encoding’ (raw data) approach.
The evaluation criteria and the baseline encoding techniques are elaborated be-
low:
Bit Compression Ratio
The Bit Compression Ratio (BCR) is deﬁned as the ratio between the average
number of bits required to store an encoded dataset and the number of bits
required to store a raw dataset, respectively. BCR is directly associated with
the relative description lengths (DL) and data type of the datasets. The DL of a
dataset is described by the length of the dataset represented by the number of
values in the dataset. If one assumes a dataset intended for pattern recognition
6.6
Experiments and Evaluation
126

is represented by Draw := {x1, x2, · · · xn|type(xi) = R} which is transformed by
an encoding algorithm to Dencoded := {y1, y2, · · · ym|type(yj) = I+}, where m
and n are the DL of the raw and the encoded data respectively. The BCR is then
estimated as:
BCR := m × sizeof(I+)
n × sizeof(R)
(6.6)
The notion of BCR (Equation 6.6) can be analysed from the viewpoint of the
Kolmogorov complexity. As described earlier, Kolmogorov’s descriptional com-
plexity aims at a simpler object representation and simplicity is measured by the
DL of the object. Here, the object being a pattern recognition dataset, the objec-
tive is to achieve simpler representation of the dataset by performing the encod-
ing operation. This is achieved by minimising the numerator m × sizeof(I+).
A compression is said to be achieved, if 0 < BCR < 1 is satisﬁed. It is also
quite evident from Equation 6.6 that the data type of the objects present in the
dataset contributes signiﬁcantly to the BCR metric. In this case, the encoded
data being represented by positive integers (spike-timings) as opposed to the
ﬂoating-point numbers in the raw data, already contribute signiﬁcantly to BCR.
Additionally, the temporal encoding algorithms aspire to minimise the DL of the
object (m << n), thus achieving a lower BCR.
Decoding Error
The decoding error metric is the measure of the decompression reliability, i.e. the
ability to recover the original signal from the compressed spike-timings reliably.
RMSE of signal reconstruction between the original signal s, and the predicted
signal ˆs has been used as a measure of decompression reliability in this study.
The RMSE is given by:
RMSE :=
√∑
t(s −ˆs(bt, Θ))2
t
(6.7)
A low RMSE of the signal reconstruction indicates higher preservation of the orig-
inal data in the spike-timings. However, low RMSE is not necessarily indicative
of a better encoding for pattern recognition. For example, an encoder producing
6.6
Experiments and Evaluation
127

better reconstruction error for noisy data may indicate inefﬁcient noise ﬁltering.
It must also be noted that the prediction models are built on the spike-time data
and have no knowledge of the mapping s →b being performed beforehand.
Hence, although this metric plays an important role in evaluating the robustness
of the encoding algorithm with respect to the reconstruction of the raw data, the
effect on the quality of pattern recognition performance is unaffected.
Classiﬁcation Performance
From the pattern recognition viewpoint, due to the balanced nature of the dataset,
the classiﬁcation accuracy is the most important and relevant measure of success
and this has been used as a measure of classiﬁcation performance. The mean
accuracy is estimated from thirty independent runs of 50/50 train/test split of
the binary classiﬁcation data described previously.
As the data that was encoded was intended to be used for pattern recognition
problems, conservation and possible enhancement of the discriminatory infor-
mation in the spike-timings is as crucial as efﬁcient compression of the data.
This is a distinctly different approach from the existing ones in pattern recog-
nition. In the traditional pattern recognition approach, the volume of the data
plays a crucial role in the performance of the pattern recognition algorithms to
produce accurate predictions. In the temporal encoding approach, by keeping
both compressibility and classiﬁcation performance as the criteria of evaluation,
the aim is to beneﬁt from the efﬁcient representation of information in the data
along with the classiﬁcation performance. It is, thus, important to have a bal-
ance between compression and conservation of discriminatory information in
the encoded data.
6.6
Experiments and Evaluation
128

Figure 6.1.: Flowchart depicting the evaluation criteria and experimental protocol
used in this research.
6.6.3 Design of Experiments
Figure 6.1 shows a ﬂowchart of the experimental design used in this Chapter.
The experimental protocol begins with the raw temporal data s. At the ﬁrst step,
the encoding operation is performed on s to generate the encoded spike-time
data b. In the second step, a K-NN based prediction model is learned using a
fraction of b. The rest of b is used to test the performance of the model emitting
the prediction performance. The next evaluation criteria relate to the compress-
ibility of the encoding algorithms. To evaluate the compressibility, BCR is calcu-
lated by comparing b and s. Finally, the lossiness of the encoding operation is
evaluated by comparing predicted signal ˆs produced by the decoding algorithm
(corresponding to the encoding algorithm) and the ground truth s to emit the
decoding error.
K-Nearest Neighbour (K-NN) Algorithm with Custom Distance Function
As discussed earlier, the non-parametric K-NN algorithm for building the classiﬁ-
cation model from the data is used. The class label prediction of a new sample
6.6
Experiments and Evaluation
129

(in this case a spike-train) in K-NN is a majority vote between the neighbours of
the new sample, where the sample is assigned to the class label most common
among its K nearest neighbours. To evaluate the neighbourhood of a sample,
it is hence necessary to calculate pairwise distances between the sample to be
predicted and the training samples. As the key interest is in learning about a
K-NN model for both raw (in continuous space) and spike-time (discrete space)
data, there have been two different distance functions used: raw and encoded
data. For the raw data in the continuous space, the standard Euclidean distance
or the L2 norm as the distance function was used. On the other hand, for the
spike-time dataset, the spike asynchronicity based distance function was used,
and is described below:
Spike asynchronicity based distance function:
Here, I propose a distance func-
tion that can capture the relative distance between a pair of the spike-train sam-
ples. Since the concern is with using spike-timings as a carrier of information,
a useful way to capture similarity between a pair of spike-train samples is to
record if the two samples have spiked at the same time instances. Therefore,
mean absolute asynchronicity has been used as the distance function. The mean
absolute asynchronicity based distance function between two spike-train sam-
ples b1 ∈{0, 1}T×M and b2 ∈{0, 1}T×M is formalised as the mean pairwise
Hamming distance between all feature-wise pairs bm
1 and bm
2 , where M is the
feature count. As the spike-time data lies in the binary space, the mean pairwise
Hamming distance is equivalent to the mean XOR distance between the pairs of
spike sequences.
6.6.4 Baseline Encoding Algorithms
In this study, three different encoding methods have been compared and evalu-
ated. It must be noted that for each encoding or compression algorithm, there
is also a decoding algorithm which decompresses the spike-trains into the recon-
structed signal ˆs.
6.6
Experiments and Evaluation
130

Algorithm 6.1 BSA encoding algorithm
1: input: s, filter, thresholdBSA
2: output: b
3: b ⇐0
4: L = length(s)
5: F = length(filter)
6: for t = 1 : (L −F + 1) do
7:
e1 ←0
8:
e2 ←0
9:
for k = 1 : F do
10:
e1 += |s(t + k) −filter(k)|
11:
e2 += |s(t + k −1)|
12:
end for
13:
if e1 ≤(e2 −thresholdBSA) then
14:
b(t) ←1
15:
for k = 1 : F do
16:
s(i + j −1) −= filter(k)
17:
end for
18:
end if
19: end for
Algorithm 6.2 BSA decoding algorithm
1: input: b, filter
2: output: ˆs
3: L = length(b)
4: F = length(filter)
5: for t=1:L-F+1 do
6:
if b(t) == 1 then
7:
for k = 1 : F do
8:
ˆs(t + k −1) += filter(k)
9:
end for
10:
end if
11: end for
6.6
Experiments and Evaluation
131

• GAGamma: The GAGamma encoding method is outlined and described in
Section 6.5. The encoding and decoding principles are given by Equations
6.3 and 6.5.
• BSA: The BSA encoding and decoding algorithms (Schrauwen & Van Camp-
enhout, 2003) are formalised in Algorithms 6.1 and 6.2, respectively. The
BSA encoding algorithm takes a ﬁlter function and a threshold value as
input along with the signal s. The deconvolution approach of BSA begins
with a FIR ﬁlter, and at every time instant τ calculates two error metrics:
∑P
k=0 abs(s(k + τ) −h(k)) and ∑P
k=0 abs(s(k + τ), where P is the num-
ber of ﬁlter taps. If the ﬁrst error is less than the second error minus the
threshold, then the BSA encoder ﬁres a spike and subtracts the ﬁlter from
the input (Schrauwen & Van Campenhout, 2003).
• Temporal contrast: The temporal contrast algorithm captures the greater
than average changes in the data as spikes. Algorithms 6.3 and 6.4 presents
the temporal contrast encoding and decoding algorithms respectively. One
major characteristic and deviation of temporal contrast algorithm from the
temporal encoding framework is its ability to generate spikes with positive
and negative polarity. Since the main interest is in the spike-timings, dur-
ing the classiﬁcation, the polarity of the spikes have been ignored. The
algorithm takes the factor ∈{0, 1} parameter as input. This parameter
controls the estimate of the thresholdTC variable, which is responsible for
determining the spike-timings.
6.6.5 Results
For the comparative evaluation of the encoding methods with the classical ‘no-
encoding’ (raw data) method, identical experiments were performed for the
subjects 04847 and 07510.
For the GAGamma encoding, two sets of hyper-
parameters have been used to demonstrate the tuning capability of the algo-
rithm. In the GAGamma-16 method, the hyperparameter values [α = 16, β =
6.6
Experiments and Evaluation
132

Algorithm 6.3 Temporal contrast encoding algorithm
1: input: s, factor
2: output:b, thresholdTC
3: L ←length(s)
4: for t = 1 : L −1 do
5:
diff ←|s(t + 1) −s(t)|
6: end for
7: thresholdTC ←mean(diff) + factor · std(diff)
8: diff ⇐[0, diff]
9: for t = 1 : L do
10:
if diff(t) > thresholdTC then
11:
b(t) ←1
12:
else if diff(t) < −thresholdTC then
13:
b(t) ←−1
14:
else
15:
b(t) ←0
16:
end if
17: end for
Algorithm 6.4 Temporal contrast decoding algorithm
1: input: b, thresholdTC
2: output: ˆs
3: ˆs ←0
4: L ←length(b)
5: for t = 2 : L do
6:
if ˆs(t) > 0 then
7:
ˆs(t) ←ˆs(t −1) + thresholdTC
8:
else if ˆs(t) < 0 then
9:
ˆs(t) ←ˆs(t −1) −thresholdTC
10:
else
11:
ˆs(t) ←ˆs(t −1)
12:
end if
13: end for
Table 6.1.: Comparative evaluation of the data encoding algorithms applied to subject
04847 and 07510 in the Starplus fMRI dataset.
subject id
method
data type 1
BCR
decoding error
classiﬁcation accuracy (K2)
04847
GAGamma-16
int
0.15
0.07
87.41 ± 4.80(16)
GAGamma-3
int
0.04
0.29
85.02 ± 4.76(11)
BSA
int
0.08
0.15
84.5 ± 4.47(3)
TC
int
0.06
0.23
54.16 ± 5.47(1)
No-encoding
decimal
1
-
89.55 ± 4.60(1)
07510
GAGamma-16
int
0.15
0.06
76.00 ± 5.89(8)
GAGamma-3
int
0.04
0.27
81.16 ± 7.50(2)
BSA
int
0.04
0.15
74.08 ± 6.71(8)
TC
int
0.05
0.26
52.75 ± 5.84(2)
No-encoding
decimal
1
-
79.11 ± 3.99(5)
Random
int
0.11
-
52.58 ± 4.79(1)
6.6
Experiments and Evaluation
133

0, γ = 10] (see Equation 6.5) were used, and in the GAGamma-3 method, [α =
3, β = 0, γ = 10] were used. The BSA encoding algorithm takes a ﬁnite impulse
response filter and a thresholdBSA as input. In the current experiments, the low
pass FIR ﬁlter of size 10 and the thresholdBSA = 0.95 have been used. These
values are guided by the existing literature on the application of BSA on brain
data (Nuntalid, Dhoble & Kasabov, 2011). For the temporal contrast encoding,
the hyper-parameter factor = 0.6 has been used. As a baseline, a randomly
generated spike-train dataset has also been included. The random spike-time
dataset was created using a Poisson’s distribution function (λ = 0.6). Varying
the λ parameter affects the BCR directly for random spike generation. It must be
noted that the presented results are non-exhaustive in the hyperparameter space
of different encoding methods. In the ‘no-encoding’ method, the raw dataset
was created by transforming each multi-dimensional time series (set of images)
within a trial into a single static observation by concatenating the feature values
across the 16 time intervals similar to the approach employed by Mitchell et al.
(2003).
Comparison Between ‘Encoding’ and ‘No-encoding’
The advantages of performing encoding as opposed to using the raw data (‘no-
encoding’) is well established in Table 6.1. This table shows the experiments
replicated across subjects 04847 and 07510. For every subject, three temporal
encoding methods were evaluated. They are the proposed GAGamma, BSA and
the temporal contrast encoding. In the GAGamma-X method, X corresponds to
the value of the maximum number of allowed spikes, α (Equation 6.5). The tem-
poral encoding methods are compared against the raw data or the ‘no-encoding’
method along with a random spike-train as a baseline. As discussed earlier, the
encoding methods are evaluated using BCR, decoding error and classiﬁcation
accuracy as the measures of success. The decoding error metric is not applicable
for the ‘random’ and ‘no-encoding’ method. This is because in ‘no-encoding’ and
‘random’ method, the raw data and a random spike generator were used, respec-
1An integer is assumed to take 8 bits and ﬂoating-point number 32 bits.
2K: Number of nearest neighbours used in K-NN algorithm
6.6
Experiments and Evaluation
134

0.00
0.05
0.10
0.15
0.20
40
50
60
70
80
90
100
BCR
Accuracy
GAGamma-16
GAGamma-3
BSA
Temporal contrast
Random
Figure 6.2.: Plot illustrating the comparison of the quality of the encoding methods
with respect to the mean classiﬁcation accuracy and bit compression ratio
across the two subjects. The horizontal and the vertical error bars repre-
sent the standard deviation of accuracy and BCR across experiments.
tively, for pattern recognition and thus, encoding principles are not applied in
these two cases.
It is observed that the encoding operations could compress the data dramatically
and thus attaining BCR values between 0.04 and 0.15 i.e. an approximate com-
pression of 6 to 25 times compared to the raw data. The most important metric
of evaluation, the classiﬁcation accuracy column in Table 6.1, shows that the
GAGamma and BSA methods achieve comparable classiﬁcation performances
with respect to the ‘no-encoding’ method, and hence can capture the discrimi-
natory information in the spike-time data well. It should also be noted that for
subject 07510, the GAGamma-3 method achieved a classiﬁcation performance
of 81.16 ± 7.50% as opposed to the 79.11 ± 3.99% by the raw data and thus out-
performing the ‘no-encoding’ method. This, it can be argued is due to the ability
of the encoding algorithms to represent the information in the raw data into the
spike-time and thus concisely present to the classiﬁer.
6.6
Experiments and Evaluation
135

Comparison of the Temporal Encoding Algorithms
Figure 6.2 graphically depicts the quality comparison of the different encoding
techniques. The plot shows the mean BCR and the classiﬁcation accuracy of the
encoding techniques across the two subjects. The horizontal and vertical error
bars are the standard deviations of the BCR and accuracy respectively. It can
be initially observed that the GAGamma-16 and GAGamma-3 encodings show
superior mean performance in the pattern recognition task compared to other
methods. However, the aim is to simultaneously achieve high classiﬁcation per-
formance and a highly compressed information representation, and thus, the
GAGamma-3 and BSA data points residing on the top left quadrant of the plot
fare better overall in both respects. From the error bar representations, it can
also be seen that the GAGamma method has a negligible deviation on BCR. This
is due to the ﬂexibility that the GAGamma method provides to control the spike
density by the constraint ∑
t bt ≤α (Equation 6.5) without sacriﬁcing much pat-
tern recognition performance. This can be of signiﬁcant importance, especially,
for the storage and transmission of large volumes of streaming data within lim-
ited resources, where the encoding operation can precisely tune the compression
rate and hence the storage. It is also recognised from the plot that the temporal
contrast encoding method fares poorly in this experiment and is no better than
a random spike generator.
Table 6.1 also shows an inverse relationship between the BCR and the decoding
error. This is because it requires signiﬁcantly more effort to accurately represent
the seasonal variations in the data using fewer spikes. However, if decoding
robustness is of major importance, the GAGamma method can be tuned to max-
imise spike density and thus will have minimal loss in the signal reconstruction
and thus sacriﬁcing the compression. Figure 6.3 shows an example of the signal
reconstruction done by the GAGamma-16, BSA and temporal contrast decoding
algorithms. In Figure 6.4, comparisons were also performed on the RMSE of
signal reconstruction across 100 different voxels using spikes encoded by the
encoding methods. It can be clearly observed that the signal reconstruction by
GAGamma (smaller RMSE means better reconstruction) is superior to the others
6.6
Experiments and Evaluation
136

0
20
40
60
0.0
0.5
1.0
data point
Signal value
True signal
GAGamma
BSA
Temporal Contrast
Figure 6.3.: Comparison of signal reconstruction (ˆs) from a spike sequence by
GAGamma-16 decoding algorithm (Equation 6.3), BSA decoding algo-
rithm (Algorithm 6.2) and Temporal contrast decoding algorithm (Algo-
rithm 6.4). The true signal is randomly selected from subject 04847 (10th
trial and 23rd voxel).
Figure 6.4.: Comparison of RMSE of reconstruction between GAGamma, BSA and Tem-
poral Contrast across 100 voxels of second trial in subject 04847.
6.6
Experiments and Evaluation
137

Figure 6.5.: Comparison of haemodynamic response function learned by GAGamma
encoding method for voxel 8 of subject 04847 across 7 different trials.
as not only can it reconstruct the bigger trends in the signal but also the seasonal
variations.
As part of the optimisation, the GA-gamma encoding method also optimises for
the parameters of the response ﬁlter H. Figure 6.5 shows the gamma haemo-
dynamic response ﬁlters learned by the model for a single voxel across 7 trials.
It can be seen from the ﬁgure, that for a single voxel across trials, the shape
of the HRF is nearly consistent, but varies in the scale. This result is consistent
with the notion of the existence of minor variations of HRF across voxel and/or
subject.
Analysis of the Spike-train Encoded by GAGamma-16 Method
Additionally, GAGamma-16 encoded spikes for the ‘seeing a picture’, and the
‘reading a sentence’ stimuli were independently analysed for interpreting the
discriminating spatio-temporal inﬂuence of the spikes. As described earlier in
the experimental protocol, the presentation of a certain stimulus within a trial
6.6
Experiments and Evaluation
138

follows an order, i.e. for each stimuli class there exists subclasses of ‘presented
ﬁrst’ or ‘presented second’. To analyse the effect of the ﬁrst or second presen-
tation of stimuli, the encoded dataset was separated into four classes, ‘picture
presented ﬁrst’, ‘picture presented second’, ‘sentence presented ﬁrst’ and ‘sen-
tence presented second’. Figures 6.6 and 6.7 shows the comparison of the mean
spike percentage across the trials for the four subclasses in subject 04847 and
subject 07510. The points in the 3D plot correspond to the spatial location of
the voxels. Each voxel belongs to two physiologically deﬁned clusters or regions
of interest, namely ‘CALC’ and ‘LIPL.’ The top row plots are the ‘picture’ trials,
and the bottom row trials are the ‘sentence’ plots. The columns correspond to
the stimulus (‘picture’ or ‘sentence’) being presented ﬁrst or second. The two
clusters in each of the 3D plots relate to the two ROI’s (top left is ‘LIPL’ and
bottom right is ‘CALC’) of the brain structure. Functionally, the ‘CALC’ region is
responsible for central and peripheral vision whereas the ‘LIPL’ region is related
to visual attention. In both the subjects, ‘reading a sentence second’ after ‘seeing
a picture ﬁrst’ has more spike activity on average across the trials than the other
way around, especially in the ‘LIPL’ region. The mean spike activity in the ‘LIPL’
is observed to be relatively higher (0.59 and 0.57) when the subjects were ‘read-
ing a sentence’ than when the subjects were ‘seeing a picture’ (0.54 and 0.55). A
two-sample T-test was conducted between the ‘seeing a picture’ and the ‘reading
a sentence’ class in the ‘LIPL’ region for the subjects to validate the previous re-
sult. The null hypothesis for the test conducted was the following, H0: ‘there is
no difference between the picture spike activity and sentence spike activity’. The
null hypothesis was rejected at 5% signiﬁcance level with p = 5.27 × 10−18 for
subject 04847 and with p = 7.05 × 10−12 for subject 07510. Hence, as per the
T-test, the average spike activity across the trials over time for ‘seeing a picture’
is signiﬁcantly different from the average spike activity across trials over time for
‘reading a sentence’. Further, it must also be noted the sentences shown as part
of the experiment are highly visual in nature (e.g. ‘It is not true that the dollar
is below the plus.’) and requires a high image comprehension ability. This result
is, therefore, consistent with the experimental results (Just, Newman, Keller,
McEleney & Carpenter, 2004) obtained earlier which shows a greater degree
of activation and functional connectivity in the ‘LIPL’ region during cognitive
6.6
Experiments and Evaluation
139

Table 6.2.: Average pairwise asynchronicity of three different voxels at the end of ten
independent runs of GAGamma encoding.
voxel ID
dp
dvp
30
24.18 ± 10.15
0.23 ± 0.09
468
27.78 ± 11.96
0.26 ± 0.10
3429
28.03 ± 11.31
0.28 ± 0.11
tasks associated with high imagery sentence comprehension. This, in fact, also
validates the ability of the proposed encoding algorithm to preserve the useful
discriminatory information in the compressed encoded space of data.
Table 6.2 relates to the reproducibility of the spike-timings produced by the
mixed integer genetic algorithm solver for the GAGamma encoding. The genetic
algorithm being an evolutionary optimisation solver produces a non-reproducible
result when on multiple iterations. Nevertheless, a pareto-optimal ﬁtness value
is guaranteed on each iteration. To validate the reliability of the GAGamma opti-
misation, in this instance, ten independent runs of GAGamma encoding was ap-
plied on three random voxels (30468 and 3429) from trial 12 of subject 04847.
Table 6.2 compares the similarity of the spike-trains produced by the GAGamma
encoding using two spike-asynchronicity measures.
They are the percentage
asynchronicity dp and Victor Purpura distance dvp respectively. The Victor Pur-
pura distance (dvp) (Victor & Purpura, 1997) metric is a cost based distance
metric. The distance is deﬁned by the minimum cost of converting one spike-
train into the other using three operations: insertion (cost 1); deletion (cost 1);
and shifting a spike by an interval δt (cost q|δt|). For the smaller value of q
the distance metric approximates the spike count difference and hence supports
rate coding. A higher penalty value of q, on the contrary, supports the number
of non-coincidental spikes and hence temporal encoding. The comparison of the
spike synchronicity using dp and dvp in Table 6.2 shows that the spike-timings
are correctly reproduced approximately 75% of times.
6.6
Experiments and Evaluation
140

50
45
40
X
35
30
25
45
50
55
Y
60
7
8
1
2
3
4
5
6
65
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(a) ‘seeing a picture’ ﬁrst during trial
50
45
40
X
35
30
25
45
50
55
Y
60
7
8
1
2
3
4
5
6
65
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(b) ‘seeing a picture’ second during trial
50
45
40
X
35
30
25
45
50
55
Y
60
7
8
1
2
3
4
5
6
65
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(c) ‘reading a sentence’ ﬁrst during trial
50
45
40
X
35
30
25
45
50
55
Y
60
7
8
1
2
3
4
5
6
65
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(d) ‘reading a sentence’ second during trial
Figure 6.6.: Comparative analysis of spike frequencies of the subject 04847 seeing picture vs. reading a sentence.
6.6
Experiments and Evaluation
141

50
45
40
X
35
30
25
35
40
45
Y
50
5
6
4
3
2
1
55
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(a) ‘seeing a picture’ ﬁrst during trial
50
45
40
X
35
30
25
35
40
45
Y
50
5
6
4
3
2
1
55
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(b) ‘seeing a picture’ second during trial
50
45
40
X
35
30
25
35
40
45
Y
50
5
6
4
3
2
1
55
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(c) ‘reading a sentence’ ﬁrst during trial
50
45
40
X
35
30
25
35
40
45
Y
50
5
6
4
3
2
1
55
Z
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(d) ‘reading a sentence’ second during trial
Figure 6.7.: Comparative analysis of spike frequencies of the subject 07510 seeing picture vs. reading a sentence.
6.6
Experiments and Evaluation
142

6.7 Summary and Conclusion
In this Chapter, the focus was on using temporal encoding as a framework to
concisely represent large volumes of data by spike-timings. By doing so, the ex-
isting discriminatory spatio-temporal information was preserved. In this regard,
apart from using the existing temporal encoding techniques, a novel temporal
encoding framework was formalised and a speciﬁc encoding algorithm for fMRI
data, called GAGamma, was proposed. The experimental result on benchmark
fMRI dataset shows the superiority of the temporal encoding algorithms, such
as GAGamma and BSA, to succinctly represent the discriminatory information in
the compressed encoded spike space without losing any appreciable amount of
information. Thus, it achieves comparable or superior pattern recognition perfor-
mance. It can be argued that the ﬂexibility of the proposed encoding framework
lies in its ability to inject known structure information about the data source
and thus, provide the compression/encoding algorithms sufﬁcient redundancy
to represent the large dataset in an optimally concise manner.
6.7
Summary and Conclusion
143

6.8 Contributions and Publications
Contributions
1. A generalised a priori knowledge driven optimisation framework for
spike-time encoding of continuous data was formalised.
2. To elaborate the characteristics of this framework, a realisation of
the proposed framework, namely GAGamma, was proposed for the
purpose of encoding fMRI data using a priori knowledge of the data
source.
3. The proposed encoding algorithm was applied and compared with
state of the art encoding algorithms on a benchmark pattern recog-
nition problem involving fMRI data. The results showed, in general,
the uniqueness of the proposed temporal encoding framework not
only lies in its ability to signiﬁcantly compress the data, but also in
keeping the discriminatory information intact, which is extremely
useful for pattern recognition tasks.
Publications
1. Sengupta, N., & Kasabov, N. (2017). Spike-time encoding as a data
compression technique for pattern recognition of temporal data. In-
formation Sciences, 406, 133-145.
2. Sengupta, N., Scott, N., & Kasabov, N. (2015).
Framework for
knowledge driven optimisation based data encoding for brain data
modelling using spiking neural network architecture. In Proceedings
of the Fifth International Conference on Fuzzy and Neuro Comput-
ing (FANCCO-2015) (pp. 109-118). Springer, Cham.
6.8
Contributions and Publications
144

7
Orientation Inﬂuence Driven
Spike-Time Dependent
Plasticity Learning: A Novel
Unsupervised Learning
Algorithm for Integrating
Spatial, Temporal and
Orientation Information
7.1 Introduction
In the recent past, non-invasive brain data collection techniques such as func-
tional magnetic resonance imaging (fMRI), electroencephalography (EEG), dif-
fusion tensor imaging (DTI) and others have made signiﬁcant contributions to
understanding various structural and functional properties of the human brain.
There has been consistent development in data sampling technology over the
past few years which has enabled simultaneous sampling of multiple modalities
of brain data while a subject performs or does not perform a task. This pro-
vided an opportunity to perform pattern recognition using large quantities of
such data. It is evident that each data modality provides a unique but limited
perspective of the brain. In the past, these data modalities were used indepen-
dently for pattern recognition and overlooked the joint information present in
the data (Sui, Adali, Yu, Chen & Calhoun, 2012). Algorithms with the ability
to extract and integrate relevant information from various data sources into a
145

single model are crucial not only for predictive modelling but also in terms of
understanding the spatio-temporal relationships within the data.
Structural dysconnectivity, as measured by DTI, has been demonstrated in sev-
eral psychiatric disorders and has been shown to reﬂect functional dysconnec-
tivity in some cases (Greicius, Supekar, Menon & Dougherty, 2009; Stephan,
Friston & Frith, 2009). In accordance with these theories, it would be appealing
to incorporate dysconnectivity information into any algorithm that is designed
to classify or predict outcomes in people with psychiatric disorders. This Chap-
ter discusses the steps that have been undertaken to develop a new algorithm
that can incorporate orientation information from DTI along with the EEG/fMRI
activity data as a data fusion approach. The most commonly used method of inte-
grated data analysis for this kind of problem is by converging evidence (Horwitz
& Poeppel, 2002). Typically, each data type is analysed separately, and the re-
sults from other analysis that support one’s ﬁndings are discussed. Horwitz and
Poeppel (2002) also put forth discussions about an alternative data fusion anal-
ysis called computation neural modelling. This is done by creating biologically
realistic neural network models, where each network simulates data of a certain
type and is compared with observed data. One major setback of this paradigm of
data analysis is that the hypothesis driven neural network model is built under
several assumptions for simulated data generation. Hence, it would be difﬁcult
to know whether a disagreement between observed and simulated data is due
to the assumptions in the model, or simply wrong. A comprehensive review of
the research in the direction of multi-modal brain data analysis is summarised
by Sui et al. (2012). Some prominent work includes integration of fMRI/EEG
(Valdes-Sosa et al., 2009), fMRI/MEG (Plis et al., 2011), fMRI/DTI (Stämpﬂi
et al., 2008; Kleiser, Staempﬂi, Valavanis, Boesiger & Kollias, 2010) and fMRI/-
gene expression (Yang, Liu, Sui, Pearlson & Calhoun, 2010). There is a third
alternative for multi-modal data integration known as data fusion, which is a
more data driven approach. Direct data fusion encompasses the technique of di-
rectly fusing multiple datasets using statistical and machine learning algorithms.
The data-driven methods span across the domain of the combined blind source
separation techniques such as Independent Component Analysis (Calhoun, Liu &
7.1
Introduction
146

Figure 7.1.: fMRI scanning device (source NIBIB (2017)).
Adal, 2009; Teipel et al., 2010) and its variants, multi-modal Cross-Correlation
Analysis (N. M. Correa, Li, Adali & Calhoun, 2008; N. M. Correa, Eichele, Adal,
Li & Calhoun, 2010; N. Correa, Li, Adali & Calhoun, 2008), Partial Least Squares
(K. Chen et al., 2009), and others.
7.2 fMRI
Functional magnetic resonance imaging (fMRI) is a form of magnetic resonance
imaging that takes advantage of magnetic susceptibility artefacts caused by the
deoxygenated haemoglobin in the brain. Magnetic susceptibility measures the
magnetic properties of the interaction between a tissue or other substance and
the in-scanner magnetic ﬁeld strength. Magnetically susceptible materials distort
the homogeneity of a magnetic ﬁeld: materials with negative magnetic suscepti-
bility are known as diamagnetic, and those with positive magnetic susceptibility
are referred to as paramagnetic. Introduction of a paramagnetic substance, such
as deoxyhaemoglobin, into the scanner magnetic ﬁeld causes variability in ﬁeld
strength, spin dephasing, geometric distortion and loss of signal; fMRI exploits
this property by measuring changes in the relative ratio of oxygenated (diamag-
netic) to deoxygenated (paramagnetic) haemoglobin in the blood.
7.2
fMRI
147

fMRI measures the haemodynamic response to neuronal excitation and is there-
fore a secondary measure of neuronal activity. As the metabolic demands of neu-
rons increase (as observed during task performance), astrocytes are signalled to
produce prostaglandin E2 and epoxyeicosotrienoic acids, which diffuse to arteri-
olar smooth muscle and cause vasodilation (Hamilton, Attwell & Hall, 2010). In-
dependently, adenosine, a breakdown product of adenosine triphosphate (ATP)
produced during periods of high metabolic demand signals pericytes (contractile
cells surrounding capillaries) to relax, permitting increased blood ﬂow through
capillaries (Hamilton et al., 2010). This increased blood ﬂow delivers high con-
centrations of oxygenated haemoglobin and glucose to the activated region, in-
creasing the ratio of oxygenated to deoxygenated haemoglobin. As discussed
above, deoxyhaemoglobin is paramagnetic and causes dephasing and signal loss
in the MR image. Increasing the ratio of oxyhaemoglobin to deoxyhaemoglobin
reduces signal loss, because oxyhaemoglobin is diamagnetic. It is this decreased
signal loss, corresponding to the peak of the haemodynamic response function
(HRF), that is measured during fMRI experiments. This is referred to as the
blood oxygen-level dependent (BOLD) signal. Importantly, the HRF produces
only a 1 −2% change in signal following a single stimulus. For this reason, it
is required that data be collected over a long period of time so that the signal
to noise ratio can be improved. fMRI may be acquired during performance of a
cognitive task or during rest. During rest, the brain exhibits patterns of sponta-
neous activity that coincide with those present during task performance (S. M.
Smith et al., 2009), making resting-state fMRI (rs-fMRI) an excellent tool for
investigating functional brain connectivity. In a standard setup, fMRI data is
collected using an MRI device such as the one shown in Figure 7.1. Each fMRI
scan is visualised as sequence of 2D slices of a 3D image. The pixel colours
represents the indirect measure of neural activity. An example of fMRI data vi-
sualisation is shown in Figure 7.2. Each image in Figure 7.2 is showing a single
slice horizontal view of the 3D image.
7.2
fMRI
148

Figure 7.2.: Example of fMRI data represented as a sequence of images (source Quar-
antelli et al. (2013)).
7.3 DTI and Tractography
DTI is a magnetic resonance imaging technique that uses immense gradient am-
plitudes together with spin-echo or gradient-echo EPI sequences to provide a
measure of the relative diffusion of molecules in tissues. In DTI, each voxel of
the MR image has one or more pairs of parameters: a rate of diffusion and a
preferred direction of diffusion described in terms of three-dimensional space
for which that parameter is valid.
The diffusion of hydrogen within a voxel is described by the diffusion tensor D
as represented by Equation 7.1. D is a reciprocal matrix, with six independent
scalar elements (Dxx, Dyy, Dzz, Dxz, Dxy, Dyz). The six unique elements of the
tensor are the coefﬁcients of the ellipsoid equation given by Dxxx2 + Dyyy2 +
Dzzz2 + Dyxyx + Dzxzx + Dxyxy = 1. This equation can also be expressed in
terms of orthogonal Eigenvectors σ and their respective eigenvalues λ. These
7.3
DTI and Tractography
149

variables describe the elements of the diffusion ellipsoid as shown in Figure
7.3.
D =







Dxx
Dxy
Dxz
Dxy
Dyy
Dyz
Dxz
Dyz
Dzz







=







σ1
σ2
σ3














λ1
0
0
0
λ2
0
0
0
λ3







(
σ1
σ2
σ3
)
(7.1)
Isotropic diffusion within a voxel causes the diffusion ellipsoid to take a spher-
ical shape with λ1 = λ2 = λ3. In contrast, restriction of diffusion in certain
directions leads to an elevated eigenvalue coupled with the principal diffusion
direction as opposed to those corresponding to the secondary and tertiary diffu-
sion direction. From the tensor equation depicted in Equation 7.1, a number of
metrics corresponding to the properties of diffusion in the voxel can be derived.
λ1 corresponds to the primary direction of diffusion (principle eigenvector; σ1)
is referred to as the axial diffusivity (AD). Radial diffusivity (RD) is the mean of
λ2 and λ3 and reﬂects the diffusion behaviour transverse to the axonal path.
From the raw data, it is possible to determine fractional anisotropy but not the
ﬁbre orientation. Diffusion tensor information can also be used to reconstruct
white matter bundles in the brain. This technique is termed tractography. Fi-
bre tractography is a very elegant method for delineating individual ﬁbre tracts
from diffusion images. Tractography uses diffusion orientation information from
tensor imaging to calculate the direction of ﬁbre bundles in-vivo. In determinis-
tic, or streamline tractography, the local tract direction is deﬁned by the major
eigenvector of the diffusion tensor (Alexander, 2010). This causes issues in vox-
els with crossing, kissing or splitting ﬁbres. However, as the algorithm is only
capable of estimating one ﬁbre orientation (Alexander, 2010), probabilistic trac-
tography addresses these issues by estimating the orientations of two or three
different ﬁbre populations within a single voxel (Behrens, Berg, Jbabdi, Rush-
worth & Woolrich, 2007). Thereafter, at every voxel, the algorithm estimates
the most probable ﬁbre orientation and also provides a distribution representing
the probability that every other orientation lies along that ﬁbre. This process is
repeated many times, each time using a slightly different orientation (according
7.3
DTI and Tractography
150

σ2
σ1
σ3
(a) The primary, secondary and tertiary diffusion directions de-
scribed by the Eigenvectors of Equation 7.1.
λ2
λ1
λ3
(b) The primary, secondary and tertiary Eigen values described in
Equation 7.1.
Figure 7.3.: Visual intuition of the ellipsoid representation of a diffusion tensor voxel.
7.3
DTI and Tractography
151

Figure 7.4.: Orientation information from DTI image. Left image shows an axial slice
of a single subject’s DTI data, registered to structural and MNI standard
space. The Right image shows a close-up of the right posterior corpus
callosum. Directions corresponding to each colour are as follows: Red -
left to right/right to left, green - anterior to posterior/posterior to anterior
and blue - superior to inferior/inferior to superior (source “Medical image
computing” (n.d.)).
to its likelihood). The integration of all estimates provides a collective measure
of connection probability along each tract (Behrens et al., 2007; Behrens et al.,
2003). Figure 7.4 shows an example of a diffusion MRI image of the human
brain.
7.4 NeuCube Architecture for Integrating
Spatial, Temporal and Orientation
Information
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
152

7.4.1 Formalisation of the Machine Learning
Problem
The machine learning problem here is to learn a functional mapping of f(Dseq, Dstat) →
C, given a set of training samples < Dseq, Dstat, C >, so that the spatial tempo-
ral and orientation information present in Dseq and Dstat are used to not only
increase prediction performance, but also impart robustness to the model. Here
Dseq and Dstat represents the fMRI/EEG and DTI data respectively.
7.4.2 NeuCube Personalised SNNc Architecture
Here, a personalised SNNc based NeuCube architecture is proposed for the pur-
pose of learning from multi-modal information. The personalised SNNc based
NeuCube architecture is a modiﬁcation of the NeuCube architecture described
in Section 4.4 and depicted in Figure 4.6. Before proposing the modiﬁed ar-
chitecture, it is necessary to elaborate certain characteristics of the SNNc in a
canonical setting.
Saturation behaviour of canonical NeuCube SNNc:
The classical NeuCube ar-
chitecture shown in Figure 4.6 is made up of a single instance of encoder layer,
SNNc layer and supervised readout layer. The instances of these layers are ini-
tialised before the training process and data is passed through sequentially over
time for: (1) Encoding layer to continuously encode the data into spike-timings;
(2) SNNc layer to digest the data and update its state; and (3) supervised read-
out layer to digest the transformed spike-train and create instances in the data
space for the K-NN algorithm to act on. NeuCube is a pattern recognition sys-
tem that learns continuously from data. During the course of development of
NeuCube, it is observed that in a vanilla setting, learning continuously from
streaming input data has some undesirable effects on the run-time efﬁciency
and performance over time. Upon analysis, it was found this behaviour relates
to the single instantiation of the NeuCube SNNc layer.
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
153

To further describe this behaviour, the spike density of SNNc can be deﬁned as
the number of neurons that ﬁre at a certain time instance. It is observed that
as input data is fed into the SNNc over time, the probability of the neurons in
the SNNc ﬁring increases. The spiking neuron of course has a mechanism, such
as refractoriness, to avoid continuous spiking; however, over time, periodicity
in spike density cannot be avoided. To demonstrate this statement, a set of
experiments were conducted using synthetically generated spike data.
In the experimental setup, a small SNNc was initialised having 27 neurons. The
neurons were spatially distributed in a 3 × 3 grid. The network of neurons were
connected following the SWC algorithm (rthr = 0.3). For input data, random
input spikes (with spiking probability 0.3) data Dseq = {0, 1}200×4 were syn-
thetically generated for each sample. Table 7.1 enlists three instances of the
experiment wherein the canonical SNNc unsupervised learning algorithm was
run using varying sample sizes and hyperparameter values. At the end of the
SNNc simulation, the spike-rate over time (or samples as samples are presented
over time) was plotted. It can be observed that irrespective of the hyperparam-
eter settings, a periodicity/total saturation in spike-rate is seen in the SNNc. As
a consequence, the spiking patterns after a certain number of sample presenta-
tions becomes predictable and does not relate to the variabilities in the sample
any more. This fact can be veriﬁed from the ‘sample distance matrix’ column
where the pairwise hamming distance matrix was plotted across the sample out-
put of the SNNc. The colours (red is high and blue is low) in the plot represents
the hamming distance between two samples. It must be noted that the ham-
ming distance between samples not only considers how many times SNNc has
spiked for a sample, but also considers when the neurons have spiked. It can
be observed there is a lot of variability in the similarity values initially, but as
more samples are presented, the spike patterns become more and more similar
leading to very small values of dissimilarity.
The point of including the SNNc layer is to improve pattern formation by expand-
ing it into higher dimensional space. However, the behaviour described above
does have a signiﬁcant impact on pattern generation capabilities of the canon-
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
154

ical NeuCube SNNc beyond a certain time. This can be deﬁned as saturation
threshold time. There are two directions where research can traipse in order to
resolve or further analyse this issue:
• Improve the SNNc mechanisms and principles to keep it within sub-threshold
limit. In this setting, a single instance of the SNNc can be simulated life-
long without reduction in pattern generation abilities.
• Minimise the impact of saturation by using multiple instances of the SNNc
and control the sub-threshold state using the hyperparameter values. This
approach was used in this work.
The personalised SNNc based NeuCube architecture is depicted in Figure 7.5.
In the personalised SNNc approach, usage of multiple instances of SNNc was
proposed. In this approach one instance of SNNc is simulated per sample along
with a single instance of the encoding layer and supervised readout layer. In
this setup, each sample of input data is fed into a unique pre-initialised personal
SNNc. The personal SNNc acts as a ﬁlter which evolves over time to capture
the spatio-temporal relationships within a sample in the synaptic strengths. The
knowledge of ﬁnite time horizon means the SNNc instance can be kept in a
sub-saturation state by controlling the hyperparameter ranges. The other advan-
tage of the personalised SNNc architecture is the ability to avoid the sequential
sample presentation bias that may exist if all the samples are fed into a single
instance of the SNNc. This means the sequence of the sample presentation does
not have any impact on spike patterns generated by the SNNc. The other im-
portant aspect of the proposed personalised SNNc architecture is the ability to
incorporate multi-modal nature of the input data which will be discussed fur-
ther.
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
155

Table 7.1.: Experimental results showing saturation characteristics of canonical NeuCube SNNc.
# samples
vthr
ηthr
κ
sample vs spike rate
Hamming distance matrix
saturation type
50
0.5
0
0.1
total
500
10
0
0.1
periodic
200
1
15
0.1
periodic
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
156

Encoder
Supervised 
Readout
fMRI
fMRI
DTI
DTI
.
.
.
.
.
.
.
.
.
.
.
.
Sample 1
Sample n
Figure 7.5.: Proposed NeuCube personalised SNNc architecture.
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
157

7.4.3 Multi-modal Information Integration in SNNc
Using Orientation and Spike-time Data
In this Section, the proposed adaptations of the canonical SNNc unsupervised
learning algorithm mentioned earlier in Section 5.3.4, for the purpose of fusing
dynamic spatio-temporal and static orientation information from brain data, will
be discussed.
SNNc Architecture, Mapping and Initialisation
The SNNc architecture, as described elaborately in Section 5.3.1, are spatially
arranged (in three dimensions) set of neurons, partially connected by recur-
rent synapses forming a directed incomplete and acyclic graph G = {M, C, W}.
The symbols and formalisations are consistent with Chapter 5 unless otherwise
stated. The neurons within the network are input or spiking types. The spatial
arrangement of the neurons follow a natural ordering and in the case of brain
data, integration takes a brain-like shape. The connections of the network are
established following the standard SWC (N. Kasabov et al., 2014) algorithm.
Neuron Dynamics
The SRM model has been used for implementing the leaky integrate and ﬁre like
behaviour of a neuron. The SRM neuron model is used to adapt the membrane
potential v(t) of a neuron over time. This neuron model speciﬁes the membrane
potential as the sum of: (1) temporal integration of the PSPs and (2) the refrac-
toriness. The PSP kernel ϵ0 is a function of t −tf, representing the PSP trace
over time generated by the ﬁring of neuron j over time. τm is known as the
membrane constant which controls the decay rate of the PSP. In the present ex-
periments, a constant τm = 0.5 has been used. This means the inﬂuence of a
pre-synaptic spike diminishes from maximum to minimum within approximately
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
158

ﬁve discrete time intervals. For a more detailed explanation and behaviour of
the neuron dynamics, see Section 5.3.2.
vi(t) = vrest +
∑
j∈Ti
wji
∑
tf
j ∈Fj
ϵ0(t −tf
j ) + η(t −tf
i )
ϵ0(t −tf
j ) = exp(−
t−tf
j
τm )H(t −tf
j )
H(t −tf
j ) =





1,
if t −tf
j ≥0
0,
otherwise
η(t −ti) =





−∞,
if t −tf
i < ηthr
0,
otherwise
(7.2)
Python implementation:
The neuron model dynamics has been implemented
in python v3.5. The neuron model dynamics were implemented as part of the
LeakyIntegrateAndFire class shown below. The simulate() method is imple-
mented to simulate the neuron dynamics using the input spike_train and pre-
synaptic connection weights weight and hyperparameter tau.



1 class LeakyIntegrateAndFireNeuron:
2
def __init__(self, predecessor_count, potential_threshold,
refractory_threshold, potential_resting,
spike_history_length):
3
self.predecessor_count = predecessor_count
4
self.refractory_threshold = refractory_threshold
5
self.potential_threshold = potential_threshold
6
self.potential_resting = potential_resting
7
self.spike_history_length = spike_history_length
8
self.time = 0
9
# initialise neuron state
10
self.potential = self.potential_resting
11
self.refractory_state = 0
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
159

12
self.potential_state_history = [self.potential]
13
self.spike_history = np.zeros((self.spike_history_length,
self.predecessor_count), dtype=int)
14
15
@staticmethod
16
def spike_response_kernel(current_time, spike_time, tau):
17
time_diff = current_time - spike_time
18
potential = np.exp(-time_diff / tau)
19
return potential
20
21
def simulate(self, spike_train, weight, tau):
22
self.time += 1
23
self.spike_history = np.delete(self.spike_history, 0,
axis=0)
24
self.spike_history = np.append(self.spike_history, [
spike_train], axis=0)
25
current_time = self.spike_history_length
26
potential = self.potential
27
spike = 0
28
if self.refractory_state == 0:
29
for k in np.arange(current_time):
30
count = np.count_nonzero(self.spike_history[k, :])
31
indices = np.nonzero(self.spike_history[k, :])
32
for i in range(count):
33
w = weight[indices[0][i]]
34
potential += w * self.spike_response_kernel(
current_time=current_time - 1, spike_time=k, tau=tau)
35
self.potential = potential
36
self.potential_state_history.append(self.potential)
37
if self.potential > self.potential_threshold:
38
self.refractory_state = self.refractory_threshold
39
self.potential = self.potential_resting
40
spike = 1
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
160

41
else:
42
self.refractory_state = max(0, self.refractory_state -
1)
43
self.potential = max(self.potential_resting, self.
potential)
44
self.potential_state_history.append(self.potential)
45
return spike


code snippet 7.1: Python code for NeuCube modiﬁed SRM neuron model
Adaptation of Synaptic Strengths of the SNNc
The unsupervised learning algorithm in the SNNc is the most important aspect of
the proposed architecture for integrating multi-modal information. In a neural
network paradigm, learning is achieved through the synaptic strength updates
of the network. The learning behaviour of the SNNc can be explained using
the learning model of a single spiking neuron. Considering the single neuron
architecture, as shown in Figure 5.1, the unsupervised learning problem is the
a scheme of updating the wji’s by ∆wji(t) over the simulation time T. In a
recurrent SNNc layer, the aim is to learn dynamic inﬂuence from dynamic data
(fMRI) and static orientation inﬂuence from static data (DTI).
Dynamic Inﬂuence from fMRI/EEG
In the majority of the machine learning applications, models are trained on static
data, where a sample is represented by a vector of numbers d = {d1, d2, · · · },
where each of the elements within the tuple d represents the scalar value of a par-
ticular feature. However, in this case with fMRI or EEG data, a sample is repre-
sented by a matrix Dseq = {d1, d2, · · · , dn}, where di = {di(1), di(2), · · · di(t)}.
The sample representation is not only multidimensional but also is ordered in
time (sequence).
Learning from these types of data in machine learning is
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
161

known as sequence learning and techniques, like the hidden Markov model and
ﬂavours of recurrent neural network have shown promise in learning from such
sequences. In this instance, an unsupervised sequence learning algorithm will be
described that is within the NeuCube SNNc layer and utilises the sequential in-
formation as part of its learning mechanism. The sequential information within
the SNNc architecture is named as the dynamic inﬂuence and is denoted by ϕ.
The dynamic inﬂuence from the spike-time data using the STDP learning algo-
rithm was modelled. As discussed elaborately in Section 5.3.3, STDP is a tem-
porally asynchronous form of Hebbian learning ("neurons wire together, if they
ﬁre together") (Hebb, 1949) induced by the temporal correlation of the spikes.
Due to the online nature of the learning within the SNNc layer, the canonical
online formulation of the STDP learning rule have been used. Sjöström and Ger-
stner (2010a) proposed the on-line STDP update rule by modifying the canonical
STDP rule. In the on-line setting, the ϕji is calculated every time a neuron i ﬁres
a spike or receives a spike from neuron j. Equation 7.3 formalises the weight up-
date rule that was used to calculate the dynamic inﬂuence. The ﬁrst term in the
right hand side of Equation 5.7 corresponds to the LTP update and is calculated
when neuron i ﬁres a spike at time t. The second term is the LTD update and is
calculated when neuron i receives a spike from neuron j at time t.
ϕji(t) =
∑
f
κ+ exp(−(t −tf
j )) −
∑
g
κ−exp(−(t −tg
i ))
(7.3)
Python implementation:
The python implementation of Equation 7.3 is given
below. The STDP() method takes the pre- and post-synaptic spike history tf
j s
and tg
i s along with the learning rate hyperparameter κ to generate the dynamic
inﬂuence phi.



1 def STDP(pre_synaptic_spike_history,
post_synaptic_spike_history, kappa):
2
assert isinstance(pre_synaptic_spike_history, list)
3
assert isinstance(post_synaptic_spike_history, list)
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
162

4
if len(pre_synaptic_spike_history) != len(
post_synaptic_spike_history):
5
raise ValueError("Length mismatch of pre and post synaptic
spike history!!")
6
pre_synaptic_spike_energy = 0
7
post_synaptic_spike_energy = 0
8
importance_of_LTP = 0.3
9
importance_of_LTD = 0.3
10
time_history = len(pre_synaptic_spike_history)
11
pre_synaptic_spike_history = np.asarray(
pre_synaptic_spike_history)
12
post_synaptic_spike_history = np.asarray(
post_synaptic_spike_history)
13
"""
14
calculation of LTD
15
"""
16
if pre_synaptic_spike_history[time_history - 1] == 1:
17
post_spike_indices = np.nonzero(
post_synaptic_spike_history)[0]
18
k = kappa * np.exp(-(1 - importance_of_LTD) * ((
time_history - 1) - post_spike_indices))
19
pre_synaptic_spike_energy = np.sum(k)
20
21
"""
22
calculation of LTP
23
"""
24
if post_synaptic_spike_history[time_history - 1] == 1:
25
pre_spike_indices = np.nonzero(pre_synaptic_spike_history
)[0]
26
k = kappa * np.exp(-(1 - importance_of_LTP) * ((
time_history - 1) - pre_spike_indices))
27
post_synaptic_spike_energy = np.sum(k)
28
phi = post_synaptic_spike_energy - pre_synaptic_spike_energy
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
163

−0.50
−0.25
0.00
0.25
0.50
−50
−25
0
25
50
ti −tj
φji
Figure 7.6.: Plot of the STDP weight update as a function of the relative time difference
of the pre and post synaptic spikes. The data for this plot was generated
using code snippet 7.2 with hyperparameter κ+ = κ−= 0.5.
29
return phi


code snippet 7.2: Python code for calculating dynamic inﬂuence
Static Orientation Inﬂuence from DTI Tractography Data
The present study has used the DTI data in the form of orientation vectors rep-
resenting mean orientation of the ﬁbre tract at different voxel locations. The
process of generating the orientation data from a DTI image is described later in
Section 7.6.3. The orientation vector of a sample DTI image is represented by a
matrix Dstat ∈R|N|×3, where each feature/voxel is made up of a 3D vector de-
scribing the primary orientation of the ﬁbre in the Cartesian coordinate system.
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
164

Figure 7.7.: Example of a pre-synaptic neuron j connected to two post synaptic neu-
rons i1 and i2. Each neurons spatial location is deﬁned by the polar coor-
dinates (r, α).
Here, the interest lies in establishing a learning rule that does not only utilises
dynamic data inﬂuence as described in Section 7.4.3, but can also make use of
the static orientation inﬂuence from the DTI data. The intuition behind the orien-
tation inﬂuence can be explained again by a small SNNc architecture consisting
of three neurons as shown in Figure 7.7. The ﬁgure shows a single pre-synaptic
neuron j connected to two post synaptic neurons i1 and i2. The important as-
pect to note here is that the neurons in this diagram have spatial allocations
contrary to the one in Figure 5.1. The location of the neurons are deﬁned by
the radial and the angular coordinates in the polar coordinate system. Now, the
intent is to measure the orientation inﬂuence ψ of neuron j on neurons {i1, i2}.
The neuron j will be referred to as the pivot neuron. The orientation vector of
the pivot neuron (from DTI data) is be represented by (rj, αj). The orientation
inﬂuence of the pivot neuron on the post-synaptic neurons {i1, i2} is deﬁned by
the angular proximity of the pivot neuron’s DTI orientation vector to the synap-
tic orientations of the neuron pairs. In that way, as per the hypothesis, the pivot
neuron wields a stronger angular inﬂuence on the neurons that lie in closer an-
gular proximity to the orientation vector of the pivot neuron. Therefore in Figure
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
165

7.7, the orientation inﬂuence of neuron j can be arranged as i1 > i2 due to the
angular proximity of i1 and j being greater than i2 and j.
Even though a 2D vector space has been used for explaining the intuition of
angular inﬂuence, the SNNc neurons reside in a 3D space. The intuition can be
extended to the 3D vector space by adding another dimension in the coordinate
system. In 3D, the spherical coordinates of a point are given by (r, α, β), where
r is the scalar distance of the point from the centre, α and β are the elevation
and azimuth angle from the centre. A Gaussian radial basis (GRB) kernel has
been utilised to realise the elevation and azimuth orientation inﬂuences given
the elevation and azimuth data of the neurons. ψ is calculated as the mean of
azimuth and elevation inﬂuences between pre- and post-synaptic neurons j and
i according to Equation 7.4.
ψji =
ψα
ji+ψβ
ji
2
ψα
ji = exp(
||αji−αdti
j
||2
2γ2
)
ψβ
ji = exp(
||βji−βdti
j
||2
2γ2
)
(7.4)
Python implementation:
The python implementation of Equation 7.4 is given
below. The angular_influence() method takes the coordinates of two neurons
and the orientation information, and outputs the orientation inﬂuence psi.



1 def angular_influence(pivot_coordinate, sphere_coordinate,
elevation_angle, azimuth_angle):
2
assert isinstance(pivot_coordinate, np.ndarray)
3
assert isinstance(sphere_coordinate, np.ndarray)
4
assert isinstance(elevation_angle, float)
5
assert isinstance(azimuth_angle, float)
6
7
standard_deviation = 8
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
166

8
relative_sphere_coordinate = np.subtract(sphere_coordinate,
pivot_coordinate)
9
radius, elevation, azimuth = cart2sph(
relative_sphere_coordinate[0], relative_sphere_coordinate
[1], relative_sphere_coordinate[2])
10
el_prob = radial_decay(elevation, elevation_angle,
standard_deviation)
11
az_prob = radial_decay(azimuth, azimuth_angle,
standard_deviation)
12
psi = (el_prob + az_prob) / 2
13
return psi
14
15 def cart2sph(x, y, z):
16
radius = math.sqrt(x ** 2 + y ** 2 + z ** 2)
17
elevation = math.atan2(math.sqrt(x ** 2 + y ** 2), z)
18
azimuth = math.atan2(y, z)
19
elevation *= 180 / math.pi
20
azimuth *= 180 / math.pi
21
return (radius, elevation, azimuth)
22
23 def radial_decay(x, mu, sigma):
24
return math.exp(-1 * (x - mu) ** 2 / (2 * sigma ** 2))


code snippet 7.3: Python code for calculating static orientation inﬂuence
The GRB kernels exponentially decay the orientation inﬂuence as the Euclidean
norm ||αji −αj|| and ||βji −βj|| increases. The variance hyperparameter γ2 con-
trols the speed with which the orientation inﬂuence decays with increasing ra-
dial distance (see Figure 7.8). The overall orientation inﬂuence is calculated as
the mean of the elevation and azimuth inﬂuence, as shown in Equation 7.4.
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
167

0.00
0.25
0.50
0.75
1.00
−50
−25
0
25
50
αji −αj
dti
ψα
Figure 7.8.: Plot of the elevation inﬂuence ψα as a function of the radial distance αji −
αdti
j
and γ = 8.
7.4.4 Orientation Inﬂuence Driven STDP (oiSTDP)
Learning in SNNc
Algorithm 7.1 describes the oiSTDP learning algorithm step by step. The unsu-
pervised learning is executed on a preinitialised SNNc. The algorithm takes: (1)
the dynamic data Dseq (fMRI/EEG) in the form of spikes; (2) the static orienta-
tion data Dstat as 3D orientation vectors; and (3) the coordinates of the SNNc
neurons as the input. The output of the learning algorithm are the spikes in
the higher dimensional space in the form of Oseq. Over simulation time T, the
execution of the algorithm can be divided into two sequential blocks:
1. Block 1 (line 6 to 17): First, each spiking neuron in set Q is queried for
the pre-synaptic neurons Jspk
i
, that has ﬁred a spike at time t . These
connections are stored in Clearn for later updating the weights as per the
LTD rule. Then the spiking neuron is simulated to update the membrane
potential. If the neuron spikes as a result of neuron simulation (line 10),
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
168

Algorithm 7.1 oiSTDP-based SNNc learning algorithm
1: input:
G = {M, C, W}, Dseq ∈{0, 1}|N|×|T|, Dstat ∈R|N|×3, loc ∈
R|M|×3,{hyperparameters = vthr, ηthr, κ}
2: output: Oseq ∈{0, 1}|M|×|T|
3: Oseq[N, T] ←Dseq
4: for t ∈T do
5:
initialise Clearn ←{}
6:
for i in spiking neurons Q do
7:
ﬁnd ﬁring(at time t −1) pre-synaptic neurons, Jspk(t)
i
8:
set Cltd
i
←(Jspk
i
, i)
9:
Clearn+ ←Cltd
i
10:
simulate i as per Equation 5.4
11:
if i ﬁres a spike then
12:
Oseq[i, t + 1] ←1
13:
ﬁnd pre-synaptic neurons, Ji
14:
set Cltp
i
←(Ji, i)
15:
set Clearn+ ←Cltp
i
16:
end if
17:
end for
18:
for cji in Clearn do
19:
initialise ψji ←1
20:
if j in N then
21:
(rdti
j , αdti
j , βdti
j ) ←cart2sph(Dstat[j])
22:
(rji, αji, βji) ←cart2sph(loc[i] −loc[j])
23:
calculate ψji as per Equation 7.4
24:
end if
25:
calculate ϕji as per Eq. 5.7
26:
set ∆wji ←ψji · ϕji
27:
update wji ←wji + ∆w
28:
end for
29: end for
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
169

a spike is added to Oseq and the pre-synaptic connections are again added
to the variable Clearn for later weight update as per the LTP rule.
2. Block 2 (line 18 to 28): This block implements the weight update rule.
At every time iteration t, synaptic strengths are updated for all the con-
nections stored in Clearn. Lines 20 to 24 are the steps for calculating the
orientation inﬂuence ψ. The function cart2sph(.) takes a 3D Cartesian co-
ordinate (x, y, z) as input and outputs the spherical coordinate (r, α, β).
The following formulae are used for this conversion:
r =
√
x2 + y2 + z2
α = tan−1
√
x2+y2
z
β = tan−1 y
z
The weight update ∆wji for connection Cji is the product of ψji and ϕji
(line 27). In this way, the orientation inﬂuence has been used as a modula-
tion factor of the dynamic inﬂuence. phi and psi are bound to [−1, 1] and
[0, 1] respectively. Henceforth, Deltaw is bound between [−1, 1]. The most
important characteristic of this learning rule is, of course, the inclusion of
the orientation inﬂuence along with the dynamic inﬂuence in the weight
update rule formulation. The rationale behind this formulation is to bind
the coincidence (STDP) and tract information together in a way such that
the strongest weight update occurs between a neuron pair when (1) there
is maximal coincidence between the pre-synaptic and post-synaptic ﬁring,
and (2) the orientation of a neuron pair matches the DTI orientation data
of the pre-synaptic neuron. Hence, the relations observed in the synap-
tic strengths of the SNNc network are representative of both the spatio-
temporal coincidence generated from the encoded spike sequence and the
orientation information produced by the DTI data. In this way, the spatial,
temporal and orientation information in the synaptic strengths are able to
be captured. Figure 7.9 shows the behaviour of the oiSTDP update rule.
The X and Y axes are the radial orientation distance between the neuron
pair and the temporal difference between the pre and post-synaptic ﬁring
times, respectively. The ﬁgure shows a mirrored inverted half Mexican hat
behaviour. It is visible that every slice across rji axis mimics the STDP
7.4
NeuCube Architecture for Integrating Spatial, Temporal and Orientation Information
170

rji
0
ti-tj 
0
-1
0
1
∆ w
-1
-0.5
0
0.5
1
Figure 7.9.: Graph showing the relationship of oiSTDP weight update ∆w with post
and pre synaptic ﬁring time difference ti −tj and orientation distance rji.
As the temporal difference between neuronal spikes decreases, the effect
on weight updating increases, so that spikes timed closely together lead
to greater increases in weight updating than spikes timed further apart.
The order of spikes also affects weight updating. If neuron j ﬁres before
neuron i consistently, then the synaptic weight between them continues
to increase; however, if the order switches, the weight is reduced.
behaviour shown in Figure 7.6. The top half of the Mexican hat relates
to the positive LTP weight update, which peaks at the minimum angular
distance and decays with increasing angular distance. The bottom inverse
half, on the contrary, relates to negative LTD weight update which achieves
a negative peak at the minimum angular distance. In this way, the angular
proximity of the neurons plays a role in modulating the spike synchronicity
driven dynamic inﬂuence.
7.5 Analysis of oiSTDP Algorithm Using
Synthetic Data
To analyse the behaviours of the oiSTDP learning algorithm, synthetically gen-
erated dynamic spatio-temporal data Dseq and static orientation data Dstat have
been used. The input spike data Dseq is of size 128 × 14, and was generated in a
way that it mimics a random one second sample of a 128Hz 14 channel EEG de-
vice. The current experiment included M = 1485 neurons with sparse recurrent
connections. The neurons in the SNNc are spatially distributed to resemble the
7.5
Analysis of oiSTDP Algorithm Using Synthetic Data
171

60
40
20
0
left->right
-20
-40
-60
-80
-60
-40
-20
anterior->posterior
0
20
40
60
40
20
0
-20
-40
60
inferior->superior
(a) 3D view
left->right
-60
-40
-20
0
20
40
60
posterior->anterior
-80
-60
-40
-20
0
20
40
60
(b) Horizontal view (X Y plane)
inferior->superior
-40
-30
-20
-10
0
10
20
30
40
50
60
left->right
-60
-40
-20
0
20
40
60
(c) Coronal view (X Z plane)
60
40
20
0
left->right
-20
-40
-60
-50
anterior->posterior
0
50
-40
-20
0
20
40
60
inferior->superior
(d) 3D view
left->right
-60
-40
-20
0
20
40
60
posterior->anterior
-80
-60
-40
-20
0
20
40
60
(e) Horizontal view
inferior->superior
-40
-30
-20
-10
0
10
20
30
40
50
60
-60
-40
-20
0
20
40
60
left->right
(f) Coronal view
60
40
20
0
left->right
-20
-40
-60
-50
anterior->posterior
0
50
20
60
0
-20
-40
40
inferior->superior
(g) 3D view
left->right
-60
-40
-20
0
20
40
60
posterior->anterior
-80
-60
-40
-20
0
20
40
60
(h) Horizontal view
inferior->superior
-40
-30
-20
-10
0
10
20
30
40
50
60
left->right
-60
-40
-20
0
20
40
60
(i) Coronal view
Figure 7.10.: The columns show the 3D, horizontal and coronal view of the strongest connection weights in the 3D SNNc at the end of the unsupervised
learning. Each row corresponds to a different input orientation data. Every neuron of the SNN in the ﬁrst row were simulated with
orientation data (α = 0°, β = 0°). This resulted in the majority of the strong connections being oriented in the direction (alpha = 0°,
β = 0°). This shows the systematic bias towards orientation information in absence of any dynamic bias. The second row shows similar
systematic bias towards the input orientation (α = 45°, β = 45°). The third row is the baseline with no orientation inﬂuence where no
systematic orientation information bias can be observed.
7.5
Analysis of oiSTDP Algorithm Using Synthetic Data
172

shape of the brain (N. K. Kasabov, 2014). The location of the input neurons in
the SNNc are resolved as per the natural spatial ordering of EEG channels-AF3,
F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4 and F8. For connection generation,
rswc = 0.02 (meaning connect neurons within 2% of the maximum distance),
and W0 = 0.05, have been used. The spiking neurons are simulated using the
parameters (ηthr = 4, vthr = 0.1, κ−= κ+ = 0.01).
7.5.1 Eﬀect of the Orientation Information on SNNc
This segment demonstrates the systematic effect of orientation information in
the SNNc map via the orientation inﬂuence ψ. The oiSTDP learning rule repre-
sents orientation information in conjunction with the spatio-temporal informa-
tion in the connection strengths. The sample Dseq was taken from a Poissons’
distribution to keep minimal spike synchronicity in the data. This is done to
minimise the inﬂuence of synchronicity to the maximum. As per Algorithm 7.1,
absence of synchronicity will mean that the spike data has minimal inﬂuence in
the SNNc weight update. Figure 7.10 shows the systematic effect of different
input orientation information on the ﬁnal 3D SNNc map. In ﬁrst of three ex-
periments, all the SNNc neurons were fed with input orientation data (α = 0°,
β = 0°), i.e. parallel to the X axis and perpendicular to the Z axis. It is clearly
visible from Figures 7.10a, 7.10b and 7.10c that the strongest connections in
the SNNc are formed in the direction of the orientation information provided.
The second and third experiment use (α = 45°, β = 45°), and no orientation
information respectively. It is evident from Figure 7.10 that in absence of the
temporal information (synchronicity), the orientation information is reﬂected in
the strongest connections of the SNNc, and, as such, in simple cases, they are
visually discriminatory.
7.5.2 Eﬀect of the spike synchronicity on SNNc
The aim of this experiment is to show the effect of spike synchronicity, i.e. the
effect of STDP learning (Equation 5.7) on the SNNc map for different spatio-
7.5
Analysis of oiSTDP Algorithm Using Synthetic Data
173

X
-60
-40
-20
0
20
40
60
Y
-80
-60
-40
-20
0
20
40
60
(a) Synchronous input spike train at locations
AF3, F7, F3, FC5, FC6, F4, F8 and AF4 (Hor-
izontal view)
X
-60
-40
-20
0
20
40
60
Y
-80
-60
-40
-20
0
20
40
60
(b) Synchronous input spike train at locations
P7, O1, O2 and P8 (Horizontal view)
Figure 7.11.: Comparison of the effect of synchronous input spikes on the SNNc map
generated by Algorithm 7.1. The blue dots show the synchronous input
channels.
temporal patterns. Per the STDP learning rule, greater synchronicity leads to
stronger connections through LTP. To demonstrate the effect of the spatio-temporal
synchronicity, two samples of the input spike data have been created. In the ﬁrst
sample, the spike sequences corresponding to the channels in the frontal lobe of
the brain are kept the same (mimicking 100% synchronicity) and in the second
sample, 100% spike synchronicity is kept at the occipital and parietal lobe. Figure
7.11 shows the comparison between the two SNNc maps created by the oiSTDP
learning algorithm when fed with these two samples. The ‘strongest connection’
density is clearly more prominent in the frontal lobe in Figure 7.11a due to the
greater input spike synchronicity in that region. Similar clusters (Figure 7.11b)
at the parietal and occipital lobe can be seen with when the second sample is
used. Through these analyses, it has been demonstrated how different temporal
patterns and the spatial arrangement of such patterns can affect the visual map
of SNNc through the oiSTDP learning.
7.5
Analysis of oiSTDP Algorithm Using Synthetic Data
174

7.6 Personalised Predictive Modelling of
Treatment Outcome using NeuCube with
oiSTDP Learning
This study was conducted as part of a large cross-sectional TRS study investigat-
ing Clozapine (CLZ) response in people with treatment resistant schizophrenia
using EEG, MRI and genetic information. This study is a collaborative work with
the school of pharmacy, University of Auckland. The University of Auckland
conducted the participant recruitment, data collection and preprocessing. The
current study’s concentration and contribution lies in the data preprocessing,
computational method development and data analysis. Strong efforts have been
made to include all the relevant information in this study. However, for in-depth
detail about the study, participants, data acquisitions and so on, the PhD thesis
of McNabb (2017) is a highly recommended read. This study was approved by
the Health and Disability Ethics Committee and received locality approval from
Auckland and Counties Manukau, District Health Boards of New Zealand. The
ethical approval was obtained by the University of Auckland and a copy of the
approval is attached in Appendix E.
7.6.1 Schizophrenia and Antipsychotics
The Global Burden of Disease Study, in 2013, estimated that 23 million peo-
ple, across the globe, were living with schizophrenia (Vos et al., 2015). Taking
into account fewer than 0.01% of the total population (though it was previously
reported by McGrath, Saha, Chant and Welham (2008) the prevalence rates be-
tween 0.3% and 0.7%), schizophrenia was found as a leading cause of years lived
with disability and contributed to 3.7% of the global burden of disease (Vos et
al., 2015). The annual national expenditure of schizophrenia, in a systematic
review, was estimated to range between USD 92 billion and USD 102 billion,
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
175

with indirect costs accounting for 50% to 85% of the overall cost (Chong et al.,
2016).
Schizophrenia most commonly emerges during adolescence or early adulthood
(Spitzer & Williams, 1980). This disease is a leading cause of disability in indi-
viduals aged between 15 and 44 years (Rössler, Salize, van Os & Riecher-Rössler,
2005). Schizophrenia affects its targets through hallucination, reduced emo-
tional expression and relapsing episodes of delusions, along with persistent cog-
nitive dysfunction and several other disruptive symptoms that hinder an individ-
uals capacity to be an active and engaged member of society, thereby greatly
diminishing their quality of life (Spitzer & Williams, 1980). Pharmacological
treatment is one of the most successful tools for managing the symptoms afﬁli-
ated with schizophrenia and has been shown to be effective in the majority of in-
stances (Matheson, Shepherd & Carr, 2014). Nevertheless, despite best-practice,
there are a small number of individuals who remain symptomatic. It has been
estimated that as few as 5% to as much as 60% of the population inﬂicted with
schizophrenia are resistant to ﬁrst-line treatment (Elkis, 2007; Lehman et al.,
2004; Essock et al., 1996; Juarez-Reyes et al., 1995). Contributing to some of
the highest rates of hospitalisation and diminished functioning in mental health
(Iasevoli et al., 2016; Lieberman & Murray, 2012), resistance to ﬁrst-line antipsy-
chotic intervention is estimated to cost USD 34 billion per annum in healthcare
expenditure within the US alone (Kennedy, Altar, Taylor, Degtiar & Hornberger,
2014). Relieving patients of their most incapacitating symptoms, the atypical an-
tipsychotic clozapine (CLZ) has been shown to be effective for treating between
30% and 70% of individuals who fail ﬁrst-line treatment (Elkis, 2007; Essali, Al-
Haj Haasan, Li & Rathbone, 2009; J. M. Kane & Correll, 2016). The severe and
potentially life-threatening side-effects linked to CLZ, however, restrict its use in
the clinic. There are no current means for identifying those who will respond to
treatment with CLZ, or alternatively, for identifying who will fail to respond to
ﬁrst-line therapy.
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
176

7.6.2 Treatment Outcome in People with
Schizophrenia
Since the advent of chlorpromazine during the middle of the twentieth century,
pharmacological interventions have been recognised as an integral component
of treatment of people with schizophrenia. Nevertheless, treatment using ﬁrst-
line antipsychotics has been shown to be effective in only 60% to 80% of individ-
uals. Fewer options for treatment exist for those who do not respond to ﬁrst-line
therapy, thereby leaving many patients without an effective means for managing
their symptoms. Research suggests that CLZ can successfully treat treatment-
resistant schizophrenia (TRS) (Meltzer, 2010; J. Kane, Honigfeld, Singer &
Meltzer, 1988), resulting in signiﬁcant enhancements towards quality of life,
as well as positive long-term functional outcomes in some individuals (Essali et
al., 2009). However, CLZ also has the potential to cause serious, life-threatening
side effects, such as agranulocytosis, myocarditis and cardiomyopathy, and has
therefore, been restricted in its use, leading to delayed access for many individ-
uals (Wheeler, 2008). Due to its noteworthy potential to cause improvement,
psychiatrists are in need of reliable tools that can predict whether CLZ will be
both an effective and a safe option for clients with schizophrenia.
Evidence suggests that schizophrenia is linked to disruptions to structural and
functional connectivity (Yu et al., 2012; Fitzsimmons, Kubicki & Shenton, 2013).
Work from previous research also suggests that functional connectivity may be
different between individuals who respond to CLZ and those who are ultra-
treatment-resistant (UTRS) (Creese, Burt & Snyder, 1976; Knott, Labelle, Jones
& Mahoney, 2000). (Rodrguez, Andreé, Castejón, Garca et al., 1996) found
that lower perfusion in the thalamus, left basal ganglia and right prefrontal
regions are poor responders to CLZ prior to treatment, and that individuals
who have high metabolic activity in the dorsolateral prefrontal cortex were
more likely to experience improvements in negative symptoms after administrat-
ing CLZ. Another study found improvements in the Positive and Negative Syn-
drome Scale (PANSS) correlated with pretreatment inter- and intra-hemispheric
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
177

spectral power asymmetry, which was measured using electroencephalography
(EEG) (Knott et al., 2000).
Pattern recognition algorithms can provide a novel and practical means to un-
derstand the differences between patients and healthy controls and predict in-
dividual patients’ responses to treatment. Within psychiatric research, in par-
ticular, machine learning has gained considerable momentum as a useful tool
for developing predictive models of treatment response. Incorporation of multi-
ple imaging modalities into these algorithms could provide increased reliability,
especially in disorders where clinical diagnosis does not necessarily guide treat-
ment. M. J. Patel et al. (2015) recently applied machine learning algorithms to
predict treatment response in late-life depression using a combination of clinical
and imaging data. Comparing several algorithms, they determined that alternat-
ing decision trees could most accurately predict treatment outcome in this cohort
using a combination of structural and functional connectivity data (M. J. Patel
et al., 2015). Khodayari-Rostamabad, Reilly, Hasey, de Bruin and MacCrimmon
(2013) have used EEG data to predict the response to selective serotonin reup-
take inhibitors in major depressive disorder and to CLZ in people with treatment-
resistant schizophrenia (Khodayari-Rostamabad, Hasey, MacCrimmon, Reilly &
de Bruin, 2010). C.-C. Lin et al. (2008) also employed machine learning algo-
rithms to predict the response to CLZ, instead of using a combination of clinical
and pharmacogenetic data as input. Doehrmann et al. (2013) employed ma-
chine learning techniques to predict treatment outcome in social anxiety disor-
der. Using task-based fMRI, they accounted for 40% of the variance in treatment
response (Doehrmann et al., 2013). The challenge now is to create an algorithm
that can incorporate brain data from different modalities.
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
178

7.6.3 Method
Participants
In accordance with the Diagnostic and Statistical Manual of Mental Disorders
(DSM-IV), persons with a diagnosis of schizophrenia were recruited from mental
health clinics within the Waitemata and Counties Manukau districts of Auckland,
New Zealand. Participants were required to be: between the ages of 18 and 45;
and clinically stable for at least six weeks prior to their inclusion in the study and
also receiving atypical antipsychotics for the treatment of schizophrenia. Approx-
imately twenty psychiatrically healthy control participants were also recruited as
part of the research. The exclusion criteria involved current or previous diagno-
sis of any other axis I disorder, history of traumatic brain injury producing loss of
consciousness of longer than three minutes, along with other signiﬁcant physical
disorders that were uncontrolled and may have affected brain structure, active
substance dependence and contraindications for MRI. It is also noteworthy that
participants were excluded from analysis if their functional image or supporting
ﬁeldmap image was overwhelmingly disrupted by motion.
As per the history of treatment and current antipsychotic regimen, participants
were assigned into one of three studies: ﬁrst-line responder (FLR); treatment
resistant (TRS); and ultra-treatment resistant (UTRS). The FLR group involved
those responding well to ﬁrst-line atypical antipsychotic mono-therapy. The TRS
group consisted of participants who failed at least two previous six-to-eight-week
trials of atypical antipsychotics and were now receiving CLZ. Lastly, those who
failed at least two previous six-to-eight-week trials of atypical antipsychotics and
had also failed a trial of CLZ monotherapy were enrolled in the UTRS group.
Participants in the UTRS group all received a mixture of two antipsychotic drugs;
79% received CLZ as part of an augmented treatment method.
The demographics of the participants were compared across cohorts using IBM
SPSS Statistics Version 24. The Students t-test was used to analyse the variables
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
179

that satisﬁed assumptions of homoscedasticity (Brown-Forsythe test for equality
of variances) and normality (Shapiro-Wilk test for normality). Variables that
violated assumptions of homoscedasticity and/or normality were analysed using
the Mann-Whitney U tests or Z-score.
Data Acquisition
The Siemens Mangetom Skyra 3T scanner at the Centre for Advanced MRI (Uni-
versity of Auckland, New Zealand) was used to acquire the diffusion and resting-
state (rs) functional magnetic resonance images. Magnetisation prepared 180-
degrees radio-frequency pulses and rapid gradient-echo (MPRAGE) sequence
were used to obtain Structural T1-weighted images. These were the acquisition
parameters: Repetition time (TR) 1900 ms; echo time (TE) 2.39 ms; inversion
time (TI) 900 ms; ﬂip angle 9°; repetition 1; acceleration (GRAPPA) factor of 2;
ﬁeld of view (FOV) 230 mm; matrix 256 × 256; voxel size 0.9 × 0.9 × 0.8 mm.
Diffusion-weighted (DWI) images were obtained through an echo planar imag-
ing (EPI) sequence with the following parameters: TR 8900 ms, TE 95 ms,
FOV 240 mm, 122 × 122 matrix, 2.0 mm slice thickness, isotropic voxel size
2.0 × 2.0 × 2.0 mm. An acceleration factor of 2 was used. 67 slices were acquired
parallel to the anterior commissure-posterior commissure (A >> P) direction
with diffusion-weighting factor b = 1000 s/mm2 in 64 gradient directions. There
were a further 8 scans without diffusion weighting (b = 0 s/mm2) which were
also acquired.
Gradient distortion (ﬁeldmap) images for diffusion data were acquired using a
gradient echo pulse sequence with the following parameters: TR 655 ms; TE1
4.92 ms; TE2 7.38 ms; voxel size 3.8x3.8x2.0 mm; phase encode direction R >>
L; FOV 240 mm.
Rs functional images were acquired using EPI with the following parameters: TR
3000 ms, TE 30 ms; echo spacing 0.65 ms (0.62 ms for last 4 participants, follow-
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
180

ing software upgrade); phase-encode direction A >> P; slices 54; volumes 160;
FOV 192 mm; acceleration factor of 2; matrix 64 × 64; voxel size 3.0 × 3.0 × 3.0
mm. Participants were requested to lie immobile with eyes open and to focus
their attention on a ﬁxation cross that was presented on a screen in front of the
scanner. Participants were given the instruction to keep a blank state of mind by
not thinking of anything in particular.
Gradient distortion images for functional data were acquired using a gradient
echo pulse sequence with the following parameters: TR 655 ms; TE1 4.92 ms;
TE2 7.38 ms; voxel size 3.4 × 3.4 × 2.4 mm; phase-encode direction A >> P;
FOV 220 mm.
fMRI Image Preprocessing
fMRI image preprocessing was performed using FSL version 5.0.7 (S. M. Smith
et al., 2004). Brain tissue was extracted from raw magnitude ﬁles using FSLs
BET (S. M. Smith, 2002), after which brain-extracted images were eroded to
ensure that no voxels containing non-brain tissue remained. Fieldmaps were
then created using the fsl_prepare_ﬁeldmap function. Functional image regis-
tration to high resolution structural and MNI152 standard space was performed
using FMRIB’s Expert Analysis Tool (FEAT). Preprocessing parameters in FEAT
were as follows: motion correction = MCFLIRT; b0 unwarping = on; echo spac-
ing = 0.325 (0.31 for last 5 participants); TE = 30; spatial smoothing = 5mm;
global intensity normalisation = on; temporal ﬁltering = off; MELODIC = off;
registration to structural image = boundary-based registration; registration to
MNI152_2mm = non-linear; warp resolution = 10 mm.
ICA-based Automatic Removal Of Motion Artefacts (ICA-AROMA) (Pruim et al.,
2015) was used to remove motion artefacts from the fMRI data utilising FSLs
FEAT output as input. White matter and cerebro-spinal ﬂuid (CSF) maps were
segmented from high resolution structural images using FSLs FAST (Yongyue
Zhang, Brady & Smith, 2001) and warped to functional space using linear reg-
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
181

istration to FEAT output (FSLs FLIRT (Jenkinson & Smith, 2001)). Nuisance
time-series were generated from ICA-AROMA output (denoised functional data)
using CSF and white matter maps as input. A general linear model (GLM) of
residual activity was then generated from the denoised functional data and nui-
sance time-series using FSLs GLM. A temporal mean ﬁle of denoised functional
data was created, to which high-pass temporal ﬁltering (sigma=16.7) was ap-
plied in addition to removal of residual activity attributed to CSF and white
matter. Filtered, de-noised functional data was then warped to MNI standard
space for use in further analysis.
DTI Image Preprocessing
DTI image preprocessing was performed using FSL version 5.0.7 (S. M. Smith
et al., 2004). Structural images were reoriented to a standard template and
brain tissue was extracted from raw image ﬁles using FSLs brain extraction tool
(BET) (S. M. Smith, 2002). If automatic brain extraction failed to eliminate all
non-brain tissue, the excess was removed manually. Magnitude images were
subjected to the same process, after which brain-extracted images were eroded
to ensure that no voxels containing non-brain tissue remained. Fieldmaps were
then created using the fsl_prepare_ﬁeldmap function. The gradient-free image
was used to create a binary mask with BET. Gradient distortions were corrected
using FSLs fugue function and output registered to gradient-free images using
the linear registration function (FLIRT) (Jenkinson & Smith, 2001). Data were
then corrected for head movement and eddy current distortions using FSLs eddy
tool (Andersson & Sotiropoulos, 2016). Slices with average intensity at least
four standard deviations lower than the expected intensity were interpolated
with predictions made by the Gaussian Process (Andersson, Graham, Zsoldos
& Sotiropoulos, 2016). DTIﬁt was used to independently ﬁt diffusion tensors
to each voxel, limited to brain space using the binary brain mask. Crossing ﬁ-
bres were modelled using Bayesian Estimation of Diffusion Parameters Obtained
using Sampling Techniques (BEDPOSTX) (Behrens et al., 2003). BEDPOSTX es-
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
182

timates of primary ﬁbre orientations (dyads1) were then warped to a standard
MNI template for use in the initial NeuCube construction.
The second stage of data processing focused on selecting a set of voxels from the
fMRI and DTI to be used to build the multi-modal NeuCube model. As discussed
in Section 7.4.3, since a major component of this model captures temporal vari-
ations in data and the noise reduction capabilities of SNN architectures through
encoding (N. Kasabov, Scott et al., 2016), it was hypothesised that the discrim-
inatory information is hidden in the voxels with signiﬁcant variability in the
activity over time. A set of voxels with an absolute mean standard deviation of
greater than 105 were selected for the experiments. Figure 7.12 shows the 3D
atlas locations of the selected voxels in the MNI coordinate system. The selected
voxels are found to be predominantly (> 67%) located in the cerebellum area
of the brain. The second and third column of the ROI frequency table (see Ta-
ble 7.2) also lists to the number and the percentage of voxels belonging to the
different ROIs.
In this study, the aim was to build a model for discriminating CLZ mono-therapy
respondent and non-respondent individuals from multi-modal fMRI and DTI
brain data. For this investigation, a subset of data was collected from the TRS
study with the intention of classifying subjects into groups with either TRS or
UTRS using resting-state fMRI and DTI data. A total of 30 subjects were chosen
for the study. Sixteen subjects belonged to the TRS group and fourteen to the
UTRS group.
Experimental Results
The ﬁnal preprocessed dataset consists of dynamic fMRI trials Dseq ∈R30×2318×80
and static DTI orientation vector data Dstat ∈R30×2318×3 of 30 subjects and 2318
voxels. Each fMRI voxel is sampled over 80 time points within a trial. The voxels
of the DTI orientation data are represented by 3D vectors signifying the primary
orientation of the ﬁbre tract at the voxel location.
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
183

Table 7.2.: Frequency table of ROI’s of the selected voxels.
ROI
# voxel
%
Frontal Lobe
177
7.64
Insula
16
0.69
Temporal Lobe
138
5.95
Cerebellum
1557
67.17
Occipital Lobe
25
1.08
Parietal Lobe
134
5.78
Thalamus
187
8.07
Caudate
84
3.62
0
10
100
20
30
40
inferior->superior
50
60
80
70
anterior->posterior
60
80
70
60
right->left
50
40
40
30
20
20
10
Thalamus
Temporal Lobe
Parietal Lobe
Cerebellum
Frontal Lobe
Occipital Lobe
Figure 7.12.: Voxel selection using absolute mean standard deviation.
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
184

The NeuCube personalised architecture consists of three modules as described
in Section 7.4.3. The ﬁrst step of the process was to compress or encode the
dynamic fMRI data from continuous real space to binary spike space. For the
encoding step, the BSA (Schrauwen & Van Campenhout, 2003) algorithm was
chosen due to its ability to represent brain data as important spike event and
has shown promising results in (Sengupta & Kasabov, 2017; Nuntalid et al.,
2011). For the second step, subject-speciﬁc SNNc’s were initialised with Q =
1000 spiking neurons, N = 2318 input neurons and synapses were initialised
using the SWC algorithm within the radial neighbourhood of rswc = 0.05 and
wij = 0.05. The spiking neurons were set with the hyperparameters {vrest =
0, vthr = 0.1, ηthr = 8}. These along with other hyperparameters were chosen
by a grid based hyperparameter search strategy. The encoded fMRI data and
the DTI orientation vector data for each subject were then passed through the
initialised SNNc, generating Oseq for each subject using Algorithm 7.1. In the
ﬁnal step, a lazy K-NN binary classiﬁcation model using 50% of the randomly
chosen subjects was learned. It is to be noted that a custom distance function as
part of the K-NN algorithm for learning binary spike data was used. A custom
spike asynchronicity-based distance function as described in Section 6.6.3 and
in (Sengupta & Kasabov, 2017) has been utilised. The K-NN learning algorithm
can of course be replaced by any supervised learning module that can learn from
discrete or binary input data.
Due to the multi-modular and rather ﬂexible nature of the NeuCube architecture,
selecting baselines for comparison is a challenging task. In this work, the Neu-
Cube architecture was used as a combination of temporal feature compressor,
spatial expander and classiﬁer. The compressor and the SNNc module together
are used as a spatio-temporal feature extraction module. The supervised read-
out layer then learns a model from the transformed feature transformed data.
Hence it is appropriate to compare the BSA+oiSTDP feature extraction mod-
ule against other feature extraction methods in continuous data domain. The
following feature extraction algorithms have been compared:
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
185

1. Sparse autoencoder (Ng, 2011): Autoencoders are shallow single hidden
layer neural networks that can perform identity mapping of the input. The
hidden layer of the autoencoder learns non linear lower dimensional data
representations. The sparse autoencoders are an extension of regular au-
toencoders that impose sparsity regularisation constraints on the loss func-
tion. In these present experiments, the fMRI data was used to learn a
sparse autoencoder (with 1000 relu units in the hidden layer and L1 regu-
larisation constraint of 10−5) that encodes the data into 1000 dimensional
feature space using the python keras API (Chollet et al., 2015).
2. Principal component analysis (PCA): PCA is a standard orthogonal linear
feature transformation technique that transforms features into principal
components. Using the scikit-learn API, 1000 principal components were
ﬁtted and transformed on the fMRI data.
3. Independent component analysis (ICA): ICA is another statistical feature
transformation technique used to decompose feature space to statistically
independent component space by maximising statistical independence of
the estimated components. Once again, scikit-learn (Pedregosa et al., 2011)
API’s FastICA algorithm was used to ﬁt and transform the fMRI data to 1000
independent components.
4. Restricted Boltzman’s machine (RBM) (Hinton & Salakhutdinov, 2006):
RBM is an unsupervised nonlinear feature learner based on a probabilistic
model that has gained much popularity in the deep neural network do-
main. The scikit-learn API (Pedregosa et al., 2011) was used to learn a
Bernoulli RBM network, with 1000 components using stochastic Maximum
likelihood (Tieleman, 2008) learning.
Table 7.3 presents the experimental results. The rows of the table compares
the methods for the classiﬁcation task. (C) and (E) in the method names cor-
1TFC=Temporal feature compressor, NTFC=No-temporal feature compressor, SE=Spatial ex-
pander, C=classiﬁer
2The performance metrics are computed as mean ± standard deviation of 10 independent
train/test runs of the classiﬁcation module
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
186

Table 7.3.: Comparison of the pattern recognition methods on the binary classiﬁcation
task.
Method
Framework1
Data
Relation learning capabilities
Performance2
Temporal
Multi-
modal
Accuracy (%)
Cohen’s κ
BSA+oiSTDP+K-NN(C)
TFC+SE+C
fMRI+DTI


72.3 ± 12.3
0.44 ± 0.25
BSA+STDP+K-NN(C)
TFC+SE+C
fMRI


69.4 ± 13.9
0.38 ± 0.28
BSA+K-NN(C)
TFC+C
fMRI


64.2 ± 12.4
0.22 ± 0.26
Sparse Autoencoder+K-NN(E)
NTFC+C
fMRI


56.1 ± 7.2
0.01 ± 0.11
PCA+K-NN(E)
NTFC+C
fMRI


56.1 ± 11.3
0.13 ± 0.18
ICA+K-NN(E)
NTFC+C
fMRI


62.8 ± 12.3
0.26 ± 0.23
RBM+K-NN(E)
NTFC+C
fMRI


36.2 ± 4.9
−0.23 ± 0.11
LSTM
C
fMRI


45.7 ± 9.6
−0.15 ± 0.14
GRU
C
fMRI


45.2 ± 7.5
−0.018 ± 0.13
respond to the custom and euclidean distance function used as part of K-NN
respectively. The framework column speciﬁes the role of each component in the
method names. For example, the proposed BSA+oiSTDP+K-NN is a combina-
tion of temporal feature compressor (TFC), spatial expander (SE) and classiﬁer
(C). The Performance of the binary classiﬁcation task is measured by overall ac-
curacy and Cohen’s κ statistic. The ﬁrst 3 rows of the table compare the different
NeuCube architectures. The BSA+oiSTDP+K-NN is the proposed architecture
for fMRI and DTI integrated learning. The next two methods systematically re-
moves (1) orientation inﬂuence from SNNc learning (STDP) and (2) the SNNc
module to show the effect of inclusion of these artefacts on the performance.
The best performance across the different methods is achieved by the proposed
BSA+oiSTDP+K-NN architecture with overall accuracy of 72.4 ± 12.3% and Co-
hen’s kappa of 0.44 ± 0.25. The classiﬁcation accuracy increases by ≈8% and
doubles the mean Cohen’s κ statistic when oiSTDP-based SNNc learning is per-
formed in the middle using fMRI and DTI data. Rows 4 to 7 are the non temporal
feature extraction baselines described earlier. Due to the non temporal nature of
the baseline feature compressors, the fMRI data for each subject is input to these
feature extractors as a single vector (created by concatenating the temporal di-
mension) leading to a massive feature vector space. K-NN (K = 1) classiﬁer was
used for the classiﬁcation task to keep the comparisons as fair as possible. The
disadvantage of the large feature space is quite imperative as it leads to potential
over ﬁtting of the data. The DTI data was not added to the already large feature
space to avoid further over ﬁtting. As the SNNc of NeuCube is a spiking recurrent
neural network framework with temporal or sequential learning capabilities, the
binary classiﬁcation task with other single hidden layer recurrent neural network
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
187

framework, such as long short term memory (LSTM) (Hochreiter & Schmidhu-
ber, 1997b) and gated recurrent units (GRU) (K. Cho et al., 2014) was also
brought to light. Both LSTM and GRU networks were designed as shallow single
hidden layered neural networks having 50 LSTM and GRU units. These networks
were implemented in keras API (Chollet et al., 2015) and learned by optimising
the binary crossentropy loss function using the adaptive momentum optimiser.
The results for the baselines show that K-NN performs best on ICA-based feature
representation. PCA and sparse autoencoders are similar in accuracy, but PCA
achieves a stronger κ statistic. On the other hand, the baseline recurrent neural
networks fail to learn the task, leading to poor performance statistics.
Furthermore, connection weights have been individually scrutinised for the TRS
and the UTRS group, generated by the oiSTDP learning algorithm. Figure 7.13
shows a comparison of the strongest mean connection weights of the TRS and
the UTRS groups. Most of the strong connections are shown to be created in
the lower cerebellum and thalamus across UTRS and TRS group. It has been
shown that by connections via the thalamus, the cerebellum innervates with mo-
tor cortical, pre-frontal and parietal lobes (W. Ou, Cameron & Thomas, 1992).
Following cerebellar damage, neuro-cognitive symptoms and a cognitive affec-
tive syndrome including blunted affect and inappropriate behaviour have been
shown (Baillieux, Verslegers, Paquier, De Deyn & Mariën, 2008). Recent fMRI
and PET studies have also demonstrated the involvement of cerebellum and
thalamus in sensory discrimination (Gao, Parsons, Bower, Xiong et al., 1996),
attention (Courchesne, Akshoomoff, Townsend & Saitoh, 1994), and complex
problem solving (S. Kim, Ugurbil, Strick et al., 1994). All of which may be im-
paired in people with schizophrenia (Yeganeh-Doost, Gruber, Falkai & Schmitt,
2011). A recent study also has shown that the administration of CLZ in people
with schizophrenia can restore cerebellar functions by altering the glutamater-
gic system and neuro-plasticity (Yeganeh-Doost et al., 2011). The present study
has additionally shown (Figure 7.13) the presence of a large density of strong
connections in the cerebellum region of the model in the UTRS group compared
to the TRS group. Similarly, a large number of strong connections are present in
the thalamic region of the TRS as opposed to UTRS. As the oiSTDP learning reg-
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
188

20
30
40
50
60
70
left->right
30
40
50
60
70
80
90
posterior->anterior
(a) Horizontal view
20
30
40
50
60
70
left->right
10
20
30
40
50
60
inferior->superior
(b) Coronal view
30
40
50
60
70
80
90
anterior->posterior
10
20
30
40
50
60
inferior->superior
(c) Sagittal view
(d) Horizontal view
(e) Coronal view
(f) Sagittal view
Figure 7.13.: A visual comparison of the strongest connections (mean weight across subjects within a group) formed in the SNNc of the TRS and the
UTRS group. The top row shows the connections in the TRS group and the bottom row corresponds to the UTRS group. The yellow
coloured cluster represents the input neurons and the green neurons are the computational spiking neurons.
7.6
Personalised Predictive Modelling of Treatment Outcome using NeuCube with oiSTDP
Learning
189

ulates the connection strength based on the joint information extracted from the
spike activity and angular information, it can be stated that the distinctive con-
nection density in the cerebellum and thalamus regions of the two groups in the
Figure 7.13 is due to distinctive fMRI activity and DTI orientation information
in the input data.
7.7 Summary and Conclusion
This is the ﬁrst attempt to integrate multiple modalities of information in a spik-
ing neural network architecture. The novelty of this approach lies in the pro-
posed personalised SNNc-based architecture of NeuCube, and most importantly
the proposed oiSTDP learning algorithm, which can integrate multiple modal-
ities of information including time, space and orientation from data. Despite
some assumptions being made on multi-modal brain data, the proposed algo-
rithm is not limited to brain data and can be extended to data having spatial, tem-
poral and orientation information. Examples of such data are weather (change
in temperature, wind movement, and cloud movement) and trafﬁc data.
The experiments shown here were conducted to demonstrate the ability of the
algorithm to capture discriminative joint information present in the data and rep-
resent this information within its connection strengths. The current design has
incorporated DTI and fMRI from individuals initiating antipsychotic therapy to
create a personalised classiﬁer of treatment response in people with schizophre-
nia. Interrogation of the SNNc network revealed increased network connectivity
in the cerebellar region of the model, potentially implicating activity in this area
of the brain as a bio-marker of treatment response in schizophrenia. Inclusion of
more participants and studies using speciﬁc task-based designs may expose other
markers not currently identiﬁed in the literature and provide novel hypotheses
regarding why some individuals respond to CLZ mono-therapy while others do
not. Additional applications of the algorithm may include other disorders where
treatment or clinical outcome is poorly understood.
7.7
Summary and Conclusion
190

The ability to incorporate data from multiple imaging modalities simultaneously
could increase the reliability of the model to predict treatment outcomes in fu-
ture. To date, studies have achieved high rates of accuracy in patient samples
combining single imaging techniques alongside clinical and pharmaco-genetic
data (M. J. Patel et al., 2015; Khodayari-Rostamabad et al., 2013), though none
have led to changes in clinical practice. This could potentially be a result of over
ﬁtting during training, which the algorithm proposed would be less likely to do.
The learning algorithm is formulated in a way that it favours joint information
over information that is contradictory. In this way the predictive outcomes are
robust and trustworthy.
In the future, apart from algorithmic reﬁnement to further improve the perfor-
mance, the aim is also to include EEG data as part of the integrated brain data
modelling using the NeuCube personalised SNNc. Further improvement of the
classiﬁcation performance through the addition of EEG data to the model. This
strand of work will lead to new methods for integration of multi-modal data
with heterogeneous spatial and temporal resolutions.
7.7
Summary and Conclusion
191

7.8 Contributions and Publications
Contributions
1. This Chapter has put forth a proposal for an unsupervised SNN learning
algorithm for learning and recognising patterns in the form of spatial,
temporal and orientation information from data.
2. A novel personalised SNNc based architecture of NeuCube has also been
proposed for achieving sub-criticality in the SNNc network.
3. The proposed algorithm has been applied to a case study of predict-
ing treatment response in people with schizophrenia. The results have
shown the superior ability of the proposed SNNc architecture to incor-
porate multiple dimensions of information and generate a better per-
forming model when compared to the other state-of-the-art technique.
Publications
1. Sengupta, N., McNabb, C. B., Kasabov, N. & Russell, B. (2018),
integrating space, time and orientation in spiking Neural Net-
works: A case study on multi-modal brain data modelling, IEEE
Transactions on Neural Networks and Learning System.
DOI:
https://doi.org/10.1109/TNNLS.2018.2796023.
7.8
Contributions and Publications
192

8
Conclusion
This thesis has delved into numerous areas relating to both the theory and prac-
tice in neuromorphic pattern recognition systems and advanced the current sta-
tus quo in various ways. This ﬁnal Chapter discusses the major contributions
this present thesis has made towards the literature and further articulates the
ways in which the research questions that were posed at the beginning in Sec-
tion 1.2 have been answered. The key caveats and limitations of this work are
then examined along with the review of future research and open questions
identiﬁed.
8.1 Research Questions and Contributions
From Chapter 3 onwards, at the end end of each chapter, a list of contributions
has been enumerated. Additionally, a list of peer-reviewed publications that has
resulted from that Chapter has also been mentioned. Here, I summarise the
key-contributions towards answering the research questions. The novel contri-
butions on the basis of the research questions are as follows:
8.1.1 How to Design Architectures of SNN that are
Capable of Digesting and Processing Large Volumes
and Variety of Spatio-temporal Data?
It is no doubt that in the era of big data, a multitude of research is ongoing
which delves into the subject of handling large volumes of data. Many of the
research in this area concentrate on improving the processing capacities of the
infrastructures, such as using GPUs or building elastic cloud infrastructures. This
193

research, however, investigates the large volume data processing problem in the
light of efﬁcient data representation in the form of binary spikes, and therefore,
aims to build SNN pattern recognition systems that can recognise patterns from
such binary spikes. Through the research performed in this thesis, I have delved
into various aspects of the ways in which such scalable SNN architectures can be
built.
Neuromorphic Computing Beyond von Neumann Architecture
Chapter, 3 has followed the development of brain-inspired spiking neural net-
works and presented the basis of the present research as being paradigm-shifting
in regards to computation architecture. This Chapter presented the neuromor-
phic thinking and design that has the potential of creating a novel, efﬁcient and
low power echo-system between neuromorphic hardware (such as SpiNNaker)
and neuromorphic pattern recognition systems (such as, the NeuCube) architec-
ture.
Software Design Principles of NeuCube SNN architecture
Through the work in Chapter 4 and 5, an in-depth overview of NeuCube, an
SNN architecture was presented at ﬁrst. The focus was especially on the Neu-
Cube generic prototyping and testing tool as a case of implementation frame-
work. Recognising the NeuCube SNNc layer as the scalability bottleneck of the
system, the focus was centered around the scalability of the SNNc layer.
It
has been elaborated through experimentation and analysis that the adjacency
forward-backward list serves as the most optimal data structure for represent-
ing the SNNc graph in relation to optimal storage (S(3 × C) complexity), and
execution time (O(1)) of the SNNc unsupervised learning algorithm. Through
simulation in a commodity hardware, it was demonstrated that using the adja-
cency forward-backward list data structure, it is possible to run neuromorphic
SNNc with neurons in the order of 106 and connections in the order of 1010.
8.1
Research Questions and Contributions
194

Beyond volume, the human brain is extremely good at processing a variety of
data, which relates to the modularity (functional specialisation of the parts of
the brain) and heterogeneity (variety in the components such as learning mech-
anisms, synapses, neuron dynamics and so on) inside the brain. Incorporating
modularity and heterogeneity, thus, is an important aspect of any SNN system.
The design principles with which one can achieve such modular and heteroge-
neous architectures was demonstrated in these Chapters.
8.1.2 How to Perform Neural Encoding on Real-world
Data to Represent Information as Timings of Spikes?
At the start of the research, incorporation of neural, and especially the temporal
encoding, was envisioned to be a critical piece of the puzzle towards solving neu-
romorphic pattern recognition in large volume data. Chapter 6 was dedicated
towards exploring this idea.
Neural Encoding from the Perspective of Data Compression and
Information Theory
This research began with the very notion that, in terms of pattern recognition,
one can in fact, only learn from data when there exists redundancy. In many data
analysis tasks, the data is preprocessed or re-coded in a way that could be seen as
a form of data compression. If such a preprocessing does not destroy the patterns
of interest, it results in comparative performance of the learning algorithms.
The motivation of the temporal encoding, thus, was to reduce large volumes
of data into a compressed state with minimal loss and the maximal presence
of discriminable information. Then, a qualitative comparison of temporal and
rate encoding schemes was made, as per Shannon and Kolmogorov’s information
theory principles. It was found the main interest, the temporal encoding scheme,
to be adherent to Kolmogorov’s algorithmic information theory.
8.1
Research Questions and Contributions
195

Framework for a priori Knowledge Driven Optimisation Based Temporal
Encoding
The a priori knowledge driven encoding framework was proposed on the premise
that (1) a universal data encoder does not exist; and (2) a priori-knowledge of
a data source can be injected into a prediction system that can predict the data
generation process. Temporal data encoding was formalised as a generalised
constrained optimisation problem (Equation 6.6), where a priori-knowledge of
data generation process is injected into the problem formulation.
GAGamma Encoding Algorithm and Case Study on Benchmark Data
To illustrate a concrete example, an encoding algorithm based on the a priori-
knowledge driven optimisation framework, namely GAGamma, was proposed,
and applied as part of a pattern recognition framework (encoding and classiﬁ-
cation) on the benchmark StarPlus fMRI dataset. A comparison of the proposed
GAGamma algorithm against the state-of-the-art temporal encoding algorithms
such as BSA and TC not only demonstrated its superior data compression quality
in regards to decoding error and bit compression ratio, but also achieved supe-
rior classiﬁcation performance (Table 6.1). Additionally, it was observed that
on the benchmark data, applying temporal encoding operation compressed the
data dramatically, between 6 to 25 times compared to the raw data, still keeping
the classiﬁcation performance high and thus could capture the discriminatory
information well.
8.1
Research Questions and Contributions
196

8.1.3 How to Integrate Spatial, Temporal and
Orientation Information Present in Multi-modal Brain
Data using SNN Architecture?
This research question relates to Chapter 7. This research question was envis-
aged as a direction towards the data fusion approach for pattern recognition
in multi-source multi-modal data. In order to keep the research focused and
constrained, this research question was directed towards speciﬁc use-case, in
this case, recognising patterns in multi-modal brain data. The rationale behind
fusing multiple modality of brain data revolved around not only a hypothesised
improved performance, but superior reliability of the model as well.
Personalised SNNc Architecture of NeuCube
From a methodological point-of-view, the current research stayed within the
NeuCube framework and proposed modiﬁcation of the NeuCube architecture
for dealing with large volumes of multi-modal data. Sub-criticality and satu-
ration behaviours in the NeuCube SNNc unsupervised learning algorithm were
analysed, and discussed how such criticality can be minimised by using the per-
sonalised SNNc architecture depicted in Figure 7.5.
Orientation Inﬂuence Driven Spike-time Dependent Plasticity (oiSTDP)
Learning for NeuCube SNNc
A novel online unsupervised learning algorithm for the SNNc layer of NeuCube
was proposed, namely oiSTDP learning algorithm (see Algorithm 7.1), that can
jointly fuse and learn from the spatial, temporal and orientation information
from multi-source brain data.
8.1
Research Questions and Contributions
197

Case Study on Predicting Treatment Outcomes of Clozapine on People
with Schizophrenia
A case study was performed in collaboration with the University of Auckland on
predicting treatment outcomes of clozapine in schizophrenia patients. The re-
sults presented in Table 7.3 summarise the comparative performances and capa-
bilities of the proposed method against numerous other state-of-the-art methods,
including deep learning algorithms. The proposed method of BSA+oiSTDP+KNN
has shown best performance in regards to accuracy and Cohen’s kappa statistic
for the classiﬁcation task. Further, interrogation of the SNNc network revealed
increased network connectivity in the cerebellar region of the model, potentially
implicating activity in this area of the brain as a bio-marker of treatment re-
sponse in schizophrenia.
8.2 Limitations of the Thesis
Generally, the limitations of the individual pieces of work in this thesis have
been discussed in context in all of the Chapters. Here, thus, only the overall
limitations of this work will be discussed.
The studies performed as part of this thesis are proof-of-concept, rather than
comprehensive studies. These works provide certain empirical support towards
the systems introduced. It was never the researcher’s intention to perform large-
scale comprehensive experiments, instead, the intention was to provide the sys-
tems and methodologies to support the empirical studies, which are in turn,
handled in other literature.
8.2
Limitations of the Thesis
198

8.3 Future Direction and Closing remarks
Concluding a research is almost always the most difﬁcult part because a signif-
icant piece of research typically asks more than what it answers. During the
process of conducting this research as well, the same was found to be true. It
is rather an impossible task to exhaustively list the open-ended questions and
future directions. Throughout the Chapters of this thesis, possible future direc-
tions of the research have been discussed. Therefore, here, without elaborating
too much, some potentially interesting research directions this thesis could open
up in the future are mentioned:
• Towards more efﬁcient representation of real world data through data
encoding. The efﬁciency (both time and power consumption) with which
the brain can recognise patterns is second to none. The current state-of-the-
art in pattern recognition and artiﬁcial intelligence is signiﬁcantly lagging
in this domain.
The inherent sequential processing architecture of the
von Neumann computer architectures is a signiﬁcant bottleneck towards
achieving efﬁciency. Through the present work, there has potentially been
a paradigm shift in research towards more efﬁcient and accurate pattern
recognition algorithms made through highly compressed representation
of data using data encoding. This, in conjunction with developments in
neuromorphic hardware systems over the next decade, can push AI to be
more neuromorphic.
• How to fuse multi-modal data with heterogeneous spatio-temporal
resolution?
A signiﬁcant future direction towards pattern recognition
by fusing multi-modal brain data would be to focus on methods for fus-
ing data with heterogeneous spatio-temporal resolutions such as fMRI and
EEG.
• Towards brain-like multi-modular heterogeneous architecture for pat-
tern recognition? This direction in research relates to the domain of neu-
8.3
Future Direction and Closing remarks
199

ral networks. Through the work in Chapter 5 possibilities and opportuni-
ties (from a software design perspective) have been discussed to create het-
erogeneous and modular neural network consisting of spatially distributed
components of different types that mimics the human brain. However, the
open question that remains is the type of scenario where such architecture
could be used, and then, the ways in which it could be built in a way that
is useful to the ﬁeld of AI.
8.3
Future Direction and Closing remarks
200

Bibliography
Abbott, A., Sengupta, N. & Kasabov, N. (2016). Which method to use for optimal struc-
ture and function representation of large spiking neural networks: A case study on
the neucube architecture. In 2016 international joint conference on neural networks
(ijcnn) (pp. 1367–1372). IEEE.
Abel, S. & Fompeyrin, J. (n.d.). Neuromorphic devices & systems.
Adrian, E. D. (1926). The impulses produced by sensory nerve endings. The Journal of
physiology, 61(1), 49–72.
Afterhyperpolarization. (2007). https://en.wikipedia.org/wiki/Afterhyperpolarization.
Accessed: 2018-02-25.
Alexander, A. L. (2010). Deterministic white matter tractography. Diffusion MRI: Theory,
methods, and applications, 383–395.
Andersson, J. L., Graham, M. S., Zsoldos, E. & Sotiropoulos, S. N. (2016). Incorporating
outlier detection and replacement into a non-parametric framework for movement
and distortion correction of diffusion mr images. NeuroImage, 141, 556–572.
Andersson, J. L. & Sotiropoulos, S. N. (2016). An integrated approach to correction for
off-resonance effects and subject movement in diffusion mr imaging. Neuroimage,
125, 1063–1078.
Arya, A. S., Vadlamani, R., Valadi, T., Sengupta, N. & Kasabov, N. (2016). Cyber fraud
detection using evolving spiking neural network. In 2016 11th international con-
ference on industrial and information systems (iciis) (pp. 263–268). doi:10.1109/
ICIINFS.2016.8262948
Ashby, F. G. (2011). Statistical analysis of fmri data. MIT press.
Aur, D. & Jog, M. S. (2010). Neuroelectrodynamics: Understanding the brain language.
IOS Press.
Babu, B. & Jehan, M. (2003). Differential evolution for multi-objective optimization. In
Evolutionary computation, 2003. cec’03. the 2003 congress on (Vol. 4, pp. 2696–
2703). IEEE.
Baillieux, H., Verslegers, W., Paquier, P., De Deyn, P. P. & Mariën, P. (2008). Cerebellar
cognitive affective syndrome associated with topiramate. Clinical neurology and
neurosurgery, 110(5), 496–499.
Ballings, M., Van den Poel, D., Hespeels, N. & Gryp, R. (2015). Evaluating multiple
classiﬁers for stock price direction prediction. Expert Systems with Applications,
42(20), 7046–7056.
201

Baum, E. B. (1988). On the capabilities of multilayer perceptrons. Journal of complexity,
4(3), 193–215.
Beckmann, C. F., Jenkinson, M. & Smith, S. M. (2003). General multilevel linear model-
ing for group analysis in fmri. Neuroimage, 20(2), 1052–1063.
Behrens, T. E., Berg, H. J., Jbabdi, S., Rushworth, M. F. & Woolrich, M. W. (2007).
Probabilistic diffusion tractography with multiple ﬁbre orientations: What can we
gain? Neuroimage, 34(1), 144–155.
Behrens, T. E., Woolrich, M., Jenkinson, M., Johansen-Berg, H., Nunes, R., Clare, S., . . .
Smith, S. M. (2003). Characterization and propagation of uncertainty in diffusion-
weighted mr imaging. Magnetic resonance in medicine, 50(5), 1077–1088.
Bekolay, T., Bergstra, J., Hunsberger, E., DeWolf, T., Stewart, T. C., Rasmussen, D., . . .
Eliasmith, C. (2013). Nengo: A python tool for building large-scale functional
brain models. Frontiers in neuroinformatics, 7.
Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. (2007). Greedy layer-wise training
of deep networks. In Advances in neural information processing systems (pp. 153–
160).
Benuskova, L. & Kasabov, N. (2010). Computational neurogenetic modeling. Springer
Science & Business Media.
Berthold, M. R., Cebron, N., Dill, F., Gabriel, T. R., Kötter, T., Meinl, T., . . . Wiswedel,
B. (2008). Knime: The konstanz information miner. In Data analysis, machine
learning and applications (pp. 319–326). Springer.
BieSSmann, F., Plis, S., Meinecke, F. C., Eichele, T. & Muller, K.-R. (2011). Analysis of
multimodal neuroimaging data. IEEE Reviews in Biomedical Engineering, 4, 26–58.
Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
Bisoi, R. & Dash, P. K. (2014). A hybrid evolutionary dynamic neural network for stock
market trend analysis and prediction using unscented kalman ﬁlter. Applied Soft
Computing, 19, 41–56.
Boahen, K. (2006). Neurogrid: Emulating a million neurons in the cortex. In Conf. proc.
ieee eng. med. biol. soc (p. 6702).
Bohte, S. M., Kok, J. N. & La Poutre, H. (2002). Error-backpropagation in temporally
encoded networks of spiking neurons. Neurocomputing, 48(1), 17–37.
Boksman, K., Théberge, J., Williamson, P., Drost, D. J., Malla, A., Densmore, M., . . .
Neufeld, R. W. (2005). A 4.0-t fmri study of brain connectivity during word ﬂu-
ency in ﬁrst-episode schizophrenia. Schizophrenia research, 75(2), 247–263.
Boynton, G. M., Engel, S. A., Glover, G. H. & Heeger, D. J. (1996). Linear systems anal-
ysis of functional magnetic resonance imaging in human v1. The journal of neuro-
science, 16(13), 4207–4221.
Brandeis, D. & Lehmann, D. (1989). Segments of event-related potential map series
reveal landscape changes with visual attention and subjective contours. Electroen-
cephalography and clinical neurophysiology, 73(6), 507–519.
Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., . . . Harris Jr,
F. C. et al. (2007). Simulation of networks of spiking neurons: A review of tools
and strategies. Journal of computational neuroscience, 23(3), 349–398.
Bibliography
202

Brown, E. N., Kass, R. E. & Mitra, P. P. (2004). Multiple neural spike train data analysis:
State-of-the-art and future challenges. Nature neuroscience, 7(5), 456–461.
Bugmann, G., Christodoulou, C. & Taylor, J. G. (1997). Role of temporal integration and
ﬂuctuation detection in the highly irregular ﬁring of a leaky integrator neuron
model with partial reset. Neural Computation, 9(5), 985–1000.
Cabeza, R. & Nyberg, L. (2000). Imaging cognition ii: An empirical review of 275 pet
and fmri studies. Journal of cognitive neuroscience, 12(1), 1–47.
Calhoun, V. D., Liu, J. & Adal, T. (2009). A review of group ICA for fMRI data and ICA
for joint inference of imaging, genetic, and erp data. Neuroimage, 45(1), S163–
S172.
Calhoun, V. D., Stevens, M., Pearlson, G. D. & Kiehl, K. (2004). Fmri analysis with the
general linear model: Removal of latency-induced amplitude bias by incorporation
of hemodynamic derivative terms. Neuroimage, 22(1), 252–257.
Calimera, A., Macii, E. & Poncino, M. (2013). The human brain project and neuromor-
phic computing. Functional neurology, 28(3), 191–196.
Capecci, E., Kasabov, N. & Wang, G. Y. (2015). Analysis of connectivity in neucube spik-
ing neural network models trained on eeg data for the understanding of functional
changes in the brain: A case study on opiate dependence treatment. Neural Net-
works, 68, 62–77.
Carlson, N. R. (1967). Foundations of physiological psychology. WW Publisher.
Cassidy, A. S., Merolla, P., Arthur, J. V., Esser, S. K., Jackson, B., Alvarez-Icaza, R., . . .
Feldman, V. et al. (2013). Cognitive computing building block: A versatile and
efﬁcient digital neuron model for neurosynaptic cores. In Neural networks (ijcnn),
the 2013 international joint conference on (pp. 1–10). IEEE.
Cawley, G. C. & Talbot, N. L. (2010). On over-ﬁtting in model selection and subsequent
selection bias in performance evaluation. Journal of Machine Learning Research,
11(Jul), 2079–2107.
Chaitin, G. J. (1966). On the length of programs for computing ﬁnite binary sequences.
Journal of the ACM (JACM), 13(4), 547–569.
Chang, P.-C. et al. (2012). A novel model by evolving partially connected neural network
for stock price trend forecasting. Expert Systems with Applications, 39(1), 611–
620.
Chen, C.-T. (1995). Linear system theory and design. Oxford University Press, Inc.
Chen, K., Reiman, E. M., Huan, Z., Caselli, R. J., Bandy, D., Ayutyanont, N. & Alexander,
G. E. (2009). Linking functional and structural brain images with multivariate net-
work analyses: A novel application of the partial least square method. Neuroimage,
47(2), 602–610.
Cheung, B.-S., Langevin, A. & Delmaire, H. (1997). Coupling genetic algorithm with
a grid search method to solve mixed integer nonlinear programming problems.
Computers & Mathematics with Applications, 34(12), 13–23.
Chiou, Y., Liu, S. & Tsaih, R. (1996). Applying reasoning neural networks to the analy-
sis and forecast of taiwans stock index variation. Taipei Economics Inquiry, Taipei,
34(2), 171–200.
Bibliography
203

Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H. &
Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078.
Cho, Z., Park, S., Kim, J., Chung, S., Chung, S., Chung, J., . . . Wong, E. (1997). Analysis
of acoustic noise in mri. Magnetic resonance imaging, 15(7), 815–822.
Chollet, F. et al. (2015). Keras. https://github.com/fchollet/keras. GitHub.
Chong, H. Y., Teoh, S. L., Wu, D. B.-C., Kotirum, S., Chiou, C.-F. & Chaiyakunapruk, N.
(2016). Global economic burden of schizophrenia: A systematic review. Neuropsy-
chiatric disease and treatment, 12, 357.
Chu, C., Ni, Y., Tan, G., Saunders, C. J. & Ashburner, J. (2011). Kernel regression for
fmri pattern prediction. NeuroImage, 56(2), 662–673.
Churchland, P. S. & Sejnowski, T. J. (1988). Perspectives on cognitive neuroscience.
Science, 242(4879), 741.
Clark, P. & Niblett, T. (1989). The cn2 induction algorithm. Machine learning, 3(4), 261–
283.
Cohen, W. W. (1995). Fast effective rule induction. In Proceedings of the twelfth interna-
tional conference on machine learning (pp. 115–123).
Cooper, M. W. (1981). A survey of methods for pure nonlinear integer programming.
Management Science, 27(3), 353–361.
Correa, N. M., Eichele, T., Adal, T., Li, Y.-O. & Calhoun, V. D. (2010). Multi-set canonical
correlation analysis for the fusion of concurrent single trial ERP and functional
MRI. Neuroimage, 50(4), 1438–1445.
Correa, N. M., Li, Y.-O., Adali, T. & Calhoun, V. D. (2008). Canonical correlation analysis
for feature-based fusion of biomedical imaging modalities and its application to
detection of associative networks in schizophrenia. Selected Topics in Signal Pro-
cessing, IEEE Journal of, 2(6), 998–1007.
Correa, N., Li, Y.-O., Adali, T. & Calhoun, V. D. (2008). Examining associations be-
tween fMRI and EEG data using canonical correlation analysis. In Biomedical
imaging: From nano to macro, 2008. isbi 2008. 5th ieee international symposium
on (pp. 1251–1254). IEEE.
Cortes, C. & Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273–
297.
Courchesne, E., Akshoomoff, N., Townsend, J. & Saitoh, O. (1994). A model system for
the study of attention and the cerebellum: Infantile autism. Electroencephalogra-
phy and clinical neurophysiology. Supplement, 44, 315–325.
Cox, R. W. (1996). Afni: Software for analysis and visualization of functional magnetic
resonance neuroimages. Computers and Biomedical research, 29(3), 162–173.
Creese, I., Burt, D. R. & Snyder, S. H. (1976). Dopamine receptor binding predicts clini-
cal and pharmacological potencies of antischizophrenic drugs. Science, 192(4238),
481–483.
Cui, X., Li, J. & Song, X. (2011). Xjview: A viewing program for spm.
Davison, A. P., Brüderle, D., Eppler, J., Kremkow, J., Muller, E., Pecevski, D., . . . Yger, P.
(2008). Pynn: A common interface for neuronal network simulators. Frontiers in
neuroinformatics, 2.
Bibliography
204

De Garis, H. (1994). An artiﬁcial brain ATR’s CAM-Brain project aims to build/evolve
an artiﬁcial brain with a million neural net modules inside a trillion cell cellular
automata machine. New Generation Computing, 12(2), 215–221.
De Schutter, E. & Bower, J. M. (1994). An active membrane model of the cerebellar
purkinje cell. i. simulation of current clamps in slice. Journal of neurophysiology,
71(1), 375–400.
de Oliveira, F. A., Nobre, C. N. & Zárate, L. E. (2013). Applying artiﬁcial neural networks
to prediction of stock price and improvement of the directional prediction index–
case study of petr4, petrobras, brazil. Expert Systems with Applications, 40(18),
7596–7606.
Deb, K. (2000). An efﬁcient constraint handling method for genetic algorithms. Com-
puter methods in applied mechanics and engineering, 186(2), 311–338.
Deb, K. (2001). Multi-objective optimization using evolutionary algorithms. John Wiley &
Sons.
Deep, K., Singh, K. P., Kansal, M. L. & Mohan, C. (2009). A real coded genetic algorithm
for solving integer and mixed integer optimization problems. Applied Mathematics
and Computation, 212(2), 505–518.
Demar, J., Zupan, B., Leban, G. & Curk, T. (2004). Orange: From experimental machine
learning to interactive data mining. Springer.
Diesmann, M., Gewaltig, M.-O. & Aertsen, A. (1999). Stable propagation of synchronous
spiking in cortical neural networks. Nature, 402(6761), 529–533.
Do, L.-N. & Yang, H.-J. (2014). A robust feature selection method for classiﬁcation of
cognitive states with fmri data. In Advances in computer science and its applications
(pp. 71–76). Springer.
Doborjeh, M. G., Capecci, E. & Kasabov, N. (2014). Classiﬁcation and segmentation of
fmri spatio-temporal brain data with a neucube evolving spiking neural network
model. In Evolving and autonomous learning systems (eals), 2014 ieee symposium
on (pp. 73–80). IEEE.
Doborjeh, M. G., Gholami, Z., Gollahalli, A. R., Kumarasinghe, K., Breen, V., Sengupta,
N., . . . Ge, C. (2018). From von neumann architecture and atanasoffs abc to neuro-
morphic computation and kasabovs neucube. part ii: Applications. Springer.
Doehrmann, O., Ghosh, S. S., Polli, F. E., Reynolds, G. O., Horn, F., Keshavan, A., . . . Hof-
mann, S. G. et al. (2013). Predicting treatment response in social anxiety disorder
from functional magnetic resonance imaging. JAMA psychiatry, 70(1), 87–97.
Dora, S., Suresh, S. & Sundararajan, N. (2015). A sequential learning algorithm for a
spiking neural classiﬁer. Applied Soft Computing, 36, 255–268.
Dorigo, M., Maniezzo, V. & Colorni, A. (1996). Ant system: Optimization by a colony
of cooperating agents. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE
Transactions on, 26(1), 29–41.
Drewes, R. (2005). Brainlab: A toolkit to aid in the design, simulation, and analysis of
spiking neural networks with the ncs environment (Doctoral dissertation, University
of Nevada Reno).
Eichele, T., Calhoun, V. D., Moosmann, M., Specht, K., Jongsma, M. L., Quiroga, R. Q.,
. . . Hugdahl, K. (2008). Unmixing concurrent eeg-fmri with parallel independent
component analysis. International Journal of Psychophysiology, 67(3), 222–234.
Bibliography
205

Elkis, H. (2007). Treatment-resistant schizophrenia. Psychiatric Clinics of North America,
30(3), 511–533.
Eluvathingal, T. J., Chugani, H. T., Behen, M. E., Juhász, C., Muzik, O., Maqbool, M.,
. . . Makki, M. (2006). Abnormal brain connectivity in children after early severe
socioemotional deprivation: A diffusion tensor imaging study. Pediatrics, 117(6),
2093–2100.
Eppler, J. M., Helias, M., Muller, E., Diesmann, M. & Gewaltig, M.-O. (2008). Pynest: A
convenient interface to the nest simulator. Frontiers in neuroinformatics, 2.
Esmaeilzadeh, H., Blem, E., St Amant, R., Sankaralingam, K. & Burger, D. (2011). Dark
silicon and the end of multicore scaling. In Acm sigarch computer architecture news
(Vol. 39, 3, pp. 365–376). ACM.
Essali, A., Al-Haj Haasan, N., Li, C. & Rathbone, J. (2009). Clozapine versus typical
neuroleptic medication for schizophrenia. The Cochrane Library.
Essock, S. M., Hargreaves, W. A., Dohm, F.-A., Goethe, J., Carver, L. & Hipshman, L.
(1996). Clozapine eligibility among state hospital patients. Schizophrenia bulletin,
22(1), 15–25.
Farquad, M., Ravi, V. & Raju, S. B. (2012). Analytical crm in banking and ﬁnance us-
ing svm: A modiﬁed active learning-based rule extraction approach. International
Journal of Electronic Customer Relationship Management, 6(1), 48–73.
Felleman, D. J. & Van Essen, D. C. (1991). Distributed hierarchical processing in the
primate cerebral cortex. Cerebral cortex (New York, NY: 1991), 1(1), 1–47.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals
of human genetics, 7(2), 179–188.
Fitzsimmons, J., Kubicki, M. & Shenton, M. E. (2013). Review of functional and anatom-
ical brain connectivity ﬁndings in schizophrenia. Current opinion in psychiatry,
26(2), 172–187.
Floudas, C. A. (1995). Nonlinear and mixed-integer optimization: Fundamentals and ap-
plications. Oxford University Press.
Freund, Y. & Schapire, R. E. (1999). Large margin classiﬁcation using the perceptron
algorithm. Machine learning, 37(3), 277–296.
Fristen, K. J. (1997). Imaging cognitive anatomy. Trends in cognitive sciences, 1(1), 21–
27.
Friston, K. J. (1994). Statistical parametric mapping.
Friston, K. J., Buechel, C., Fink, G., Morris, J., Rolls, E. & Dolan, R. (1997). Psychophys-
iological and modulatory interactions in neuroimaging. Neuroimage, 6(3), 218–
229.
Friston, K. J., Holmes, A. P., Poline, J., Grasby, P., Williams, S., Frackowiak, R. S. &
Turner, R. (1995). Analysis of fmri time-series revisited. Neuroimage, 2(1), 45–53.
Friston, K. J., Josephs, O., Rees, G. & Turner, R. (1998). Nonlinear event-related re-
sponses in fmri. Magnetic resonance in medicine, 39(1), 41–52.
Furber, S. (2016). Large-scale neuromorphic computing systems. Journal of neural engi-
neering, 13(5), 051001.
Bibliography
206

Furber, S. B., Galluppi, F., Temple, S. & Plana, L. A. (2014). The spinnaker project.
Proceedings of the IEEE, 102(5), 652–665.
Gao, J.-H., Parsons, L. M., Bower, J. M., Xiong, J. et al. (1996). Cerebellum impli-
cated in sensory acquisition and discrimination rather than motor control. Science,
272(5261), 545.
Gautrais, J. & Thorpe, S. (1998). Rate coding versus temporal order coding: A theoreti-
cal approach. Biosystems, 48(1), 57–65.
George, J. S., Aine, C., Mosher, J., Schmidt, D., Ranken, D., Schlitt, H., . . . Belliveau,
J. (1995). Mapping function in the human brain with magnetoencephalography,
anatomical magnetic resonance imaging, and functional magnetic resonance imag-
ing. LWW.
Gerstner, W. (1998). Spiking neurons. MIT-press.
Gerstner, W., Kempter, R., van Hemmen, J. L. & Wagner, H. (1996). A neuronal learning
rule for sub-millisecond temporal coding. Nature, 383(LCN-ARTICLE-1996-002),
76–78.
Gerstner, W. & Kistler, W. M. (2002). Spiking neuron models: Single neurons, populations,
plasticity. Cambridge University Press.
Gerstner, W., Kistler, W. M., Naud, R. & Paninski, L. (2014). Neuronal dynamics: From
single neurons to networks and models of cognition. Cambridge University Press.
Geschwind, N. (1974). Disconnexion syndromes in animals and man. In Selected papers
on language and the brain (pp. 105–236). Springer.
Ghosh-Dastidar, S. & Adeli, H. (2009). Spiking neural networks. International journal of
neural systems, 19(04), 295–308.
Girones, J. (n.d.). J48 decision tree.
Glover, G. H. (1999). Deconvolution of impulse response in event-related bold fmri 1.
Neuroimage, 9(4), 416–429.
Goldberg, D. E. (1989). Genetic algorithms in search, optimization, and machine learn-
ing, 1989. Reading: Addison-Wesley.
Gollisch, T. & Meister, M. (2008). Rapid neural coding in the retina with relative spike
latencies. science, 319(5866), 1108–1111.
Goodman, D. F. (2010). Code generation: A strategy for neural network simulators.
Neuroinformatics, 8(3), 183–196.
Goodman, D. F. & Brette, R. (2009). The brian simulator. Frontiers in neuroscience, 3(2),
192.
Gordon, E. (2007). Integrating genomics and neuromarkers for the era of brain-related
personalized medicine. Personalized Medicine, 4(2), 201–215.
Greicius, M. D., Supekar, K., Menon, V. & Dougherty, R. F. (2009). Resting-state func-
tional connectivity reﬂects structural connectivity in the default mode network.
Cerebral cortex, 19(1), 72–78.
Grossmann, I. E. (2002). Review of nonlinear mixed-integer and disjunctive program-
ming techniques. Optimization and engineering, 3(3), 227–252.
Grunwald, P. & Vitányi, P. (2004). Shannon information and kolmogorov complexity.
arXiv preprint cs/0410002.
Bibliography
207

Hafezi, R., Shahrabi, J. & Hadavandi, E. (2015). A bat-neural network multi-agent sys-
tem (bnnmas) for stock price prediction: Case study of dax stock price. Applied
Soft Computing, 29, 196–210.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P. & Witten, I. H. (2009).
The weka data mining software: An update. ACM SIGKDD explorations newsletter,
11(1), 10–18.
Hallock, R. M. & Di Lorenzo, P. M. (2006). Temporal coding in the gustatory system.
Neuroscience & Biobehavioral Reviews, 30(8), 1145–1160.
Hamilton, N. B., Attwell, D. & Hall, C. N. (2010). Pericyte-mediated regulation of cap-
illary diameter: A component of neurovascular coupling in health and disease.
Frontiers in neuroenergetics, 2.
Hamstra, D. A., Rehemtulla, A. & Ross, B. D. (2007). Diffusion magnetic resonance imag-
ing: A biomarker for treatment response in oncology. Journal of clinical oncology,
25(26), 4104–4109.
Hassabis, D., Kumaran, D., Summerﬁeld, C. & Botvinick, M. (2017). Neuroscience-inspired
artiﬁcial intelligence. Neuron, 95(2), 245–258.
Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L. & Pietrini, P. (2001).
Distributed and overlapping representations of faces and objects in ventral tempo-
ral cortex. Science, 293(5539), 2425–2430.
Haynes, J.-D. & Rees, G. (2005). Predicting the orientation of invisible stimuli from
activity in human primary visual cortex. Nature neuroscience, 8(5), 686–691.
Hebb, D. O. (1949). The organization of behavior: A neuropsychological approach. John
Wiley & Sons.
Hines, M. L. & Carnevale, N. T. (1997). The neuron simulation environment. Neural
computation, 9(6), 1179–1209.
Hines, M. L. & Carnevale, N. T. (2006). The neuron simulation environment. NEURON,
9(6).
Hinton, G. E., Mcclelland, J. L. & Rumelhart, D. E. (1986). Distributed representations,
parallel distributed processing: Explorations in the microstructure of cognition,
vol. 1: Foundations. MIT Press, Cambridge, MA.
Hinton, G. E. & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with
neural networks. science, 313(5786), 504–507.
Hinton, G. E. & Sejnowski, T. J. (1986). Learning and releaming in boltzmann machines.
Parallel Distrilmted Processing, 1.
Historical - Indices. (n.d.). http://www.bseindia.com/indices/IndexArchiveData.aspx.
Hochreiter, S. & Schmidhuber, J. (1997a). Long short-term memory. Neural computation,
9(8), 1735–1780.
Hochreiter, S. & Schmidhuber, J. (1997b). Lstm can solve hard long time lag problems.
In Advances in neural information processing systems (pp. 473–479).
Hodgkin, A. L. & Huxley, A. F. (1952). A quantitative description of membrane current
and its application to conduction and excitation in nerve. The Journal of physiol-
ogy, 117(4), 500–544.
Bibliography
208

Holland, J. H. (1992). Adaptation in natural and artiﬁcial systems: An introductory anal-
ysis with applications to biology, control, and artiﬁcial intelligence. MIT press.
Hopﬁeld, J. J. (1982). Neural networks and physical systems with emergent collec-
tive computational abilities. Proceedings of the national academy of sciences, 79(8),
2554–2558.
Horovitz, S. G., Skudlarski, P. & Gore, J. C. (2002). Correlations and dissociations be-
tween bold signal and p300 amplitude in an auditory oddball task: A parametric
approach to combining fmri and erp. Magnetic resonance imaging, 20(4), 319–
325.
Horvath, G. (2003). Cmac neural network as an svm with b-spline kernel functions. In In-
strumentation and measurement technology conference, 2003. imtc’03. proceedings
of the 20th ieee (Vol. 2, pp. 1108–1113). IEEE.
Horwitz, B., Grady, C. L., Haxby, J. V., Schapiro, M. B., Rapoport, S. I., Ungerleider,
L. G. & Mishkin, M. (1992). Functional associations among human posterior ex-
trastriate brain regions during object and spatial vision. Journal of Cognitive Neu-
roscience, 4(4), 311–322.
Horwitz, B. & Poeppel, D. (2002). How can eeg/meg and fmri/pet data be combined?
Human brain mapping, 17(1), 1–3.
Horwitz, B., Soncrant, T. T. & Haxby, J. V. (1992). Covariance analysis of functional
interactions in the brain using metabolic and blood ﬂow data. In Advances in
metabolic mapping techniques for brain imaging of behavioral and learning func-
tions (pp. 189–217). Springer.
Horwitz, B., Tagamets, M. & McIntosh, A. R. (1999). Neural modeling, functional brain
imaging, and cognition. Trends in cognitive sciences, 3(3), 91–98.
Hough, M., De Garis, H., Korkin, M., Gers, F. & Nawa, N. E. (1999). SPIKER: Ana-
log waveform to digital spiketrain conversion in atrs artiﬁcial brain (cam-brain)
project. In International conference on robotics and artiﬁcial life. Citeseer.
Hsu, J. (2014). Ibm’s new brain [news]. IEEE spectrum, 51(10), 17–19.
Hu, J., Hou, Z.-G., Chen, Y.-X., Kasabov, N. & Scott, N. (2014). Eeg-based classiﬁca-
tion of upper-limb adl using snn for active robotic rehabilitation. In Biomedical
robotics and biomechatronics (2014 5th ieee ras & embs international conference on
(pp. 409–414). IEEE.
Hua, Z. & Huang, F. (2006). An effective genetic algorithm approach to large scale mixed
integer programming problems. Applied Mathematics and computation, 174(2),
897–909.
Huang, C.-J., Yang, D.-X. & Chuang, Y.-T. (2008). Application of wrapper approach and
composite classiﬁer to the stock trend prediction. Expert Systems with Applications,
34(4), 2870–2878.
Huang, W., Nakamori, Y. & Wang, S.-Y. (2005). Forecasting stock market movement
direction with support vector machine. Computers & Operations Research, 32(10),
2513–2522.
Hubel, D. H. & Wiesel, T. N. (1959). Receptive ﬁelds of single neurones in the cat’s
striate cortex. The Journal of physiology, 148(3), 574–591.
Bibliography
209

Hubel, D. H. & Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and func-
tional architecture in the cat’s visual cortex. The Journal of physiology, 160(1),
106–154.
Iakymchuk, T., Rosado-Munoz, A., Bataller-Mompean, M., Guerrero-Martinez, J., Frances-
Villora, J., Wegrzyn, M. & Adamski, M. (2014). Hardware-accelerated spike train
generation for neuromorphic image and video processing. In Programmable logic
(spl), 2014 ix southern conference on (pp. 1–6). IEEE.
Iasevoli, F., Giordano, S., Balletta, R., Latte, G., Formato, M. V., Prinzivalli, E., . . . de
Bartolomeis, A. (2016). Treatment resistant schizophrenia is associated with the
worst community functioning among severely-ill highly-disabling psychiatric con-
ditions and is the most relevant predictor of poorer achievements in functional
milestones. Progress in Neuro-Psychopharmacology and Biological Psychiatry, 65,
34–48.
Izhikevich, E. M. (2001). Resonate-and-ﬁre neurons. Neural networks, 14(6), 883–894.
Izhikevich, E. M. (2004). Which model to use for cortical spiking neurons? IEEE transac-
tions on neural networks, 15(5), 1063–1070.
Jenkinson, M. & Smith, S. (2001). A global optimisation method for robust afﬁne regis-
tration of brain images. Medical image analysis, 5(2), 143–156.
Jong, K. A. D. (1975). An analysis of the behavior of a class of genetic adaptive systems
(Doctoral dissertation).
Juarez-Reyes, M. G., Shumway, M., Battle, C., Bacchetti, P., Hansen, M. S. & Hargreaves,
W. A. (1995). Effects of stringent criteria on eligibility for clozapine among public
mental health clients. Psychiatric services, 46(8), 801–806.
Just, M. A., Newman, S. D., Keller, T. A., McEleney, A. & Carpenter, P. A. (2004). Imagery
in sentence comprehension: An fmri study. Neuroimage, 21(1), 112–124.
Kamitani, Y. & Tong, F. (2005). Decoding the visual and subjective contents of the hu-
man brain. Nature neuroscience, 8(5), 679–685.
Kamitani, Y. & Tong, F. (2006). Decoding seen and attended motion directions from
activity in the human visual cortex. Current biology, 16(11), 1096–1102.
Kandel, E. R. [Eric R], Schwartz, J. H., Jessell, T. M., Siegelbaum, S. A., Hudspeth, A. J.
et al. (2000). Principles of neural science. McGraw-hill New York.
Kane, J. M. & Correll, C. U. (2016). The role of clozapine in treatment-resistant schizophre-
nia. JAMA psychiatry, 73(3), 187–188.
Kane, J., Honigfeld, G., Singer, J. & Meltzer, H. (1988). Clozapine for the treatment-
resistant schizophrenic: A double-blind comparison with chlorpromazine. Archives
of general psychiatry, 45(9), 789–796.
Kara, Y., Boyacioglu, M. A. & Baykan, Ö. K. (2011). Predicting direction of stock price
index movement using artiﬁcial neural networks and support vector machines:
The sample of the istanbul stock exchange. Expert systems with Applications, 38(5),
5311–5319.
Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks. http:
//karpathy.github.io/2015/05/21/rnn-effectiveness/.
Bibliography
210

Kasabov, N. (1998). Evolving fuzzy neural networks-algorithms, applications and bio-
logical motivation. Methodologies for the conception, design and application of soft
computing, World Scientiﬁc, 1, 271–274.
Kasabov, N. (2001). Evolving fuzzy neural networks for supervised/unsupervised online
knowledge-based learning. IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics), 31(6), 902–918.
Kasabov, N. (2007). Evolving connectionist systems: The knowledge engineering approach.
Springer Science & Business Media.
Kasabov, N. (2012). NeuCube evospike architecture for spatio-temporal modelling and
pattern recognition of brain signals. In Iapr workshop on artiﬁcial neural networks
in pattern recognition (pp. 225–243). Springer.
Kasabov, N. K. (2014). NeuCube: A spiking neural network architecture for mapping,
learning and understanding of spatio-temporal brain data. Neural Networks, 52,
62–76.
Kasabov, N. K., Doborjeh, M. G. & Doborjeh, Z. G. (2017). Mapping, learning, visualiza-
tion, classiﬁcation, and understanding of fmri data in the neucube evolving spa-
tiotemporal data machine of spiking neural networks. IEEE transactions on neural
networks and learning systems, 28(4), 887–899.
Kasabov, N., Dhoble, K., Nuntalid, N. & Indiveri, G. (2013). Dynamic evolving spiking
neural networks for on-line spatio-and spectro-temporal pattern recognition. Neu-
ral Networks, 41, 188–201.
Kasabov, N., Feigin, V., Hou, Z.-G., Chen, Y., Liang, L., Krishnamurthi, R., . . . Parmar,
P. (2014). Evolving spiking neural networks for personalised modelling, classi-
ﬁcation and prediction of spatio-temporal patterns with a case study on stroke.
Neurocomputing, 134, 269–279.
Kasabov, N., Scott, N. M., Tu, E., Marks, S., Sengupta, N., Capecci, E., . . . Hartono,
R. et al. (2016). Evolving spatio-temporal data machines based on the NeuCube
neuromorphic framework: Design methodology and selected applications. Neural
Networks, 78, 1–14.
Kasabov, N., Sengupta, N. & Scott, N. (2016). From von neumann, john atanasoff and
abc to neuromorphic computation and the neucube spatio-temporal data machine.
In Ieee 8th international conference on intelligent systems (is) (pp. 15–21). IEEE.
Kasabov, N. & Song, Q. (2002). Denﬁs: Dynamic evolving neural-fuzzy inference system
and its application for time-series prediction. IEEE transactions on Fuzzy Systems,
10(2), 144–154.
Kennedy, J. L., Altar, C. A., Taylor, D. L., Degtiar, I. & Hornberger, J. C. (2014). The
social and economic burden of treatment-resistant schizophrenia: A systematic
literature review. International clinical psychopharmacology, 29(2), 63–76.
Khodayari-Rostamabad, A., Hasey, G. M., MacCrimmon, D. J., Reilly, J. P. & de Bruin, H.
(2010). A pilot study to determine whether machine learning methodologies using
pre-treatment electroencephalography can predict the symptomatic response to
clozapine therapy. Clinical Neurophysiology, 121(12), 1998–2006.
Khodayari-Rostamabad, A., Reilly, J. P., Hasey, G. M., de Bruin, H. & MacCrimmon, D. J.
(2013). A machine learning approach using EEG data to predict response to ssri
treatment for major depressive disorder. Clinical Neurophysiology, 124(10), 1975–
1985.
Bibliography
211

Kim, K.-j. (2003). Financial time series forecasting using support vector machines. Neu-
rocomputing, 55(1), 307–319.
Kim, K.-j. & Han, I. (2000). Genetic algorithms approach to feature discretization in
artiﬁcial neural networks for the prediction of stock price index. Expert systems
with Applications, 19(2), 125–132.
Kim, K.-j. & Lee, W. B. (2004). Stock market prediction using artiﬁcial neural networks
with optimal feature transformation. Neural computing & applications, 13(3), 255–
260.
Kim, S., Ugurbil, K., Strick, P. et al. (1994). Activation of a cerebellar output nucleus dur-
ing cognitive processing. SCIENCE-NEW YORK THEN WASHINGTON-, 949–949.
Kistler, W. M., Gerstner, W. & van Hemmen, J. L. (1997). Reduction of the hodgkin-
huxley equations to a single-variable threshold model. Neural computation, 9(5),
1015–1045.
Kleiser, R., Staempﬂi, P., Valavanis, A., Boesiger, P. & Kollias, S. (2010). Impact of fmri-
guided advanced dti ﬁber tracking techniques on their clinical applications in pa-
tients with brain tumors. Neuroradiology, 52(1), 37.
Knott, V., Labelle, A., Jones, B. & Mahoney, C. (2000). Eeg hemispheric asymmetry
as a predictor and correlate of short-term response to clozapine treatment in
schizophrenia. Clinical Electroencephalography, 31(3), 145–152.
Koch, M. (1999). The neurobiology of startle. Progress in neurobiology, 59(2), 107–128.
Kohonen, T. (1990). The self-organizing map. Proceedings of the IEEE, 78(9), 1464–
1480.
Kohonen, T. (1998). The self-organizing map. Neurocomputing, 21(1), 1–6.
Kolmogorov, A. N. (1965). Three approaches to the quantitative deﬁnition oﬁnforma-
tion’. Problems of information transmission, 1(1), 1–7.
Kostal, L., Lansky, P. & Rospars, J.-P. (2007). Neuronal coding and spiking randomness.
European Journal of Neuroscience, 26(10), 2693–2701.
Kotsiantis, S. B., Zaharakis, I. & Pintelas, P. (2007). Supervised machine learning: A
review of classiﬁcation techniques.
Kumar, M. & Thenmozhi, M. (2006). Forecasting stock index movement: A comparison
of support vector machines and random forest.
Langford, J., Li, L. & Strehl, A. (2007). Vowpal wabbit online learning project. Technical
report, http://hunch. net.
Laufs, H., Holt, J. L., Elfont, R., Krams, M., Paul, J. S., Krakow, K. & Kleinschmidt, A.
(2006). Where the bold signal goes when alpha eeg leaves. Neuroimage, 31(4),
1408–1418.
Lehman, A. F., Lieberman, J. A., Dixon, L. B., McGlashan, T. H., Miller, A. L., Perkins,
D. O., . . . Altshuler, K. et al. (2004). Practice guideline for the treatment of par-
tients with schizophrenia. American Journal of Psychiatry, 161(2 SUPPL.).
Li, J.-l., Wang, L.-q., Li, C.-x., Han, B. & Zhang, Z.-c. (2005). Multi-pwm pulse genera-
tor based fpga. PROCEEDINGS-CHINESE SOCIETY OF ELECTRICAL ENGINEERING,
25(10), 55.
Bibliography
212

Li, Y.-X. & Gen, M. (1996). Nonlinear mixed integer programming problems using ge-
netic algorithm and penalty function. In Systems, man, and cybernetics, 1996., ieee
international conference on (Vol. 4, pp. 2677–2682). IEEE.
Lieberman, J. A. & Murray, R. M. (2012). Comprehensive care of schizophrenia: A textbook
of clinical management. Oxford University Press.
Lin, C.-C., Wang, Y.-C., Chen, J.-Y., Liou, Y.-J., Bai, Y.-M., Lai, I.-C., . . . Li, Y.-C. (2008).
Artiﬁcial neural network prediction of clozapine response with combined pharma-
cogenetic and clinical data. Computer methods and programs in biomedicine, 91(2),
91–99.
Lin, Y., Guo, H. & Hu, J. (2013). An svm-based approach for stock market trend predic-
tion. In Neural networks (ijcnn), the 2013 international joint conference on (pp. 1–
7). IEEE.
Littlestone, N. & Warmuth, M. K. (1994). The weighted majority algorithm. Information
and computation, 108(2), 212–261.
Lukoeviius, M. & Jaeger, H. (2009). Reservoir computing approaches to recurrent neural
network training. Computer Science Review, 3(3), 127–149.
Luo, Y.-C., Guignard, M. & Chen, C.-H. (2001). A hybrid approach for integer program-
ming combining genetic algorithms, linear programming and ordinal optimiza-
tion. Journal of Intelligent Manufacturing, 12(5), 509–519.
Maass, W. (1997). Networks of spiking neurons: The third generation of neural network
models. Neural networks, 10(9), 1659–1671.
Maass, W. & Bishop, C. M. (2001). Pulsed neural networks. MIT press.
Maass, W., Natschläger, T. & Markram, H. (2002). Real-time computing without stable
states: A new framework for neural computation based on perturbations. Neural
computation, 14(11), 2531–2560.
Mainen, Z. F. & Sejnowski, T. J. (1995). Reliability of spike timing in neocortical neurons.
Science, 268(5216), 1503–1506.
Mainen, Z. F. & Sejnowski, T. J. (1998). Modeling active dendritic processes in pyrami-
dal neurons. Methods in neuronal modeling, 171–210.
Maiti, A., Bhunia, A. & Maiti, M. (2006). An application of real-coded genetic algorithm
(rcga) for mixed integer non-linear programming in two-storage multi-item inven-
tory model with discount policy. Applied Mathematics and computation, 183(2),
903–915.
Marian, I. (2002). A biologically inspired model of motor control of direction. Master’s
thesis, University College Dublin.
Markram, H., Meier, K., Lippert, T., Grillner, S., Frackowiak, R., Dehaene, S., . . . De-
Felipe, J. et al. (2011). Introducing the human brain project. Procedia Computer
Science, 7, 39–42.
Matheson, S., Shepherd, A. & Carr, V. (2014). How much do we know about schizophre-
nia and how well do we know it? evidence from the schizophrenia library. Psycho-
logical medicine, 44(16), 3387–3405.
Maunsell, J. H. & Gibson, J. R. (1992). Visual response latencies in striate cortex of the
macaque monkey. Journal of neurophysiology, 68(4), 1332–1344.
Bibliography
213

McClure, E. B., Adler, A., Monk, C. S., Cameron, J., Smith, S., Nelson, E. E., . . . Pine,
D. S. (2007). Fmri predictors of treatment outcome in pediatric anxiety disorders.
Psychopharmacology, 191(1), 97–105.
McCulloch, W. S. & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous
activity. The bulletin of mathematical biophysics, 5(4), 115–133.
McGrath, J., Saha, S., Chant, D. & Welham, J. (2008). Schizophrenia: A concise overview
of incidence, prevalence, and mortality. Epidemiologic reviews, 30(1), 67–76.
McNabb, C. B. (2017). Brain dysconnectivity as a biomarker of treatment resistance in
schizophrenia (Doctoral dissertation, The University of Auckland).
Mead, C. (1990). Neuromorphic electronic systems. Proceedings of the IEEE, 78(10),
1629–1636.
Medical image computing. (n.d.). https://en.wikipedia.org/wiki/Medical_image_
computing.
Mel, B. W. (1994). Information processing in dendritic trees. Neural computation, 6(6),
1031–1085.
Meltzer, H. Y. (2010). Role of clozapine in treatment-resistant schizophrenia. In Therapy-
resistant schizophrenia (Vol. 26, pp. 114–128). Karger Publishers.
Menon, V. & Crottaz-Herbette, S. (2005). Combined eeg and fmri studies of human
brain function. Int Rev Neurobiol, 66, 291–321.
Mesulam, M. et al. (1990). Large-scale neurocognitive networks and distributed process-
ing for attention, language, and memory. Annals of neurology, 28(5), 597–613.
Michalski, R. S. (1980). Learning by being told and learning by examples: An experi-
mental comparison of the two methods of knowledge acquisition in the context of
developing an expert system for soybean disease analysis. Int. J. of Policy Analysis
and Information Systems, 4(2).
Minsky, M. & Papert, S. (1969). Perceptrons.
Mitchell, T. M. et al. (1997). Machine learning. wcb. McGraw-Hill Boston, MA:
Mitchell, T. M., Hutchinson, R., Just, M. A., Niculescu, R. S., Pereira, F. & Wang, X.
(2003). Classifying instantaneous cognitive states from fmri data. In American
medical informatics association annual symposium.
Mitchell, T. M., Hutchinson, R., Niculescu, R. S., Pereira, F., Wang, X., Just, M. & New-
man, S. (2004). Learning to decode cognitive states from brain images. Machine
learning, 57(1), 145–175.
Mitchell, T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A. &
Just, M. A. (2008). Predicting human brain activity associated with the meanings
of nouns. science, 320(5880), 1191–1195.
Mitchell, T. M. & Wang, W. (2001). Starplus fmri data.
Mohemmed, A., Schliebs, S., Matsuda, S. & Kasabov, N. (2012). Span: Spike pattern as-
sociation neuron for learning spatio-temporal spike patterns. International Journal
of Neural Systems, 22(04), 1250012.
Mostafa, M. M. (2010). Forecasting stock exchange movements using neural networks:
Empirical evidence from kuwait. Expert Systems with Applications, 37(9), 6302–
6309.
Bibliography
214

Murthy, S. K. (1998). Automatic construction of decision trees from data: A multi-
disciplinary survey. Data mining and knowledge discovery, 2(4), 345–389.
Muthukumaraswamy, S. D. & Singh, K. D. (2008). Spatiotemporal frequency tuning of
bold and gamma band meg responses compared in primary visual cortex. Neu-
roimage, 40(4), 1552–1560.
Nair, B. B., Mohandas, V. & Sakthivel, N. (2010). A decision treerough set hybrid system
for stock market trend prediction. International Journal of Computer Applications,
6(9), 1–6.
Natschläger, T., Maass, W. & Markram, H. (2002). The" liquid computer": A novel strat-
egy for real-time computing on time series. Special issue on Foundations of Infor-
mation Processing of TELEMATIK, 8(LNMC-ARTICLE-2002-005), 39–43.
Natschläger, T., Markram, H. & Maass, W. (2003). Computer models and analysis tools
for neural microcircuits. In Neuroscience databases (pp. 123–138). Springer.
NeuCom. (n.d.). http://www.theneucom.com. Accessed: 2015-08-15.
Ng, A. (2011). Sparse autoencoder. CS294A Lecture notes, 72(2011), 1–19.
NIBIB. (2017). Fmri scanner. http://www.advancedimagingsouthbay.com/services-mri-
scan.html.
Nicolini, C. & Bifone, A. (2016). Modular structure of brain functional networks: Break-
ing the resolution limit by surprise. Scientiﬁc reports, 6.
Nissen, S. & Nemerson, E. (2000). Fast artiﬁcial neural network library. Available at
leenissen. dk/fann/html/ﬁles/fann-h. html.
Norman, K. A., Polyn, S. M., Detre, G. J. & Haxby, J. V. (2006). Beyond mind-reading:
Multi-voxel pattern analysis of fmri data. Trends in cognitive sciences, 10(9), 424–
430.
NSE - National Stock Exchange of India Ltd. (n.d.). https : / / www. nseindia . com /
products/content/equities/indices/historical_index_data.htm.
Nunez, M. D., Nunez, P. L. & Srinivasan, R. (2016). Handbook of neuroimaging data
analysis. (Chap. Electroencephalography (EEG): neurophysics, experimental meth-
ods, and signal processing). Chapman & Hall/CRC.
Nuntalid, N., Dhoble, K. & Kasabov, N. (2011). Eeg classiﬁcation with bsa spike encod-
ing algorithm and evolving probabilistic spiking neural network. In International
conference on neural information processing (pp. 451–460). Springer.
Olah, C. (2015). Understanding lstm. http://colah.github.io/posts/2015-08-Understanding-
LSTMs/.
Olshausen, B. A. & Field, D. J. (2006). What is the other 85 percent of v1 doing. L. van
Hemmen, & T. Sejnowski (Eds.) 23, 182–211.
Ou, P. & Wang, H. (2009). Prediction of stock market index movement by ten data
mining techniques. Modern Applied Science, 3(12), 28.
Ou, W., Cameron, P. & Thomas, D. (1992). Anatomical evidence for cerebellar and basal
ganglia involvement in higher cognitive function. Biol, 2, 227.
Pachitariu, M., Brody, C. D., Jun, P. J. K. & Holmes, P. J. (n.d.). Probabilistic models for
spike trains of single neurons.
Bibliography
215

Pandya, D. N. & Yeterian, E. H. (1985). Architecture and connections of cortical associa-
tion areas. In Association and auditory cortices (pp. 3–61). Springer.
Park, J. & Sandberg, I. W. (1991). Universal approximation using radial-basis-function
networks. Neural computation, 3(2), 246–257.
Patel, J., Shah, S., Thakkar, P. & Kotecha, K. (2015). Predicting stock and stock price
index movement using trend deterministic data preparation and machine learning
techniques. Expert Systems with Applications, 42(1), 259–268.
Patel, M. J., Andreescu, C., Price, J. C., Edelman, K. L., Reynolds, C. F. & Aizenstein, H. J.
(2015). Machine learning approaches for integrating clinical and imaging features
in late-life depression classiﬁcation and response prediction. International journal
of geriatric psychiatry, 30(10), 1056–1067.
Patil, P. G., Carmena, J. M., Nicolelis, M. A. & Turner, D. A. (2004). Ensemble recordings
of human subcortical neurons as a source of motor control signals for a brain-
machine interface. Neurosurgery, 55(1), 27–38.
Pecevski, D. (n.d.). Oger: Modular learning architectures for large-scale sequential pro-
cessing.
Pecevski, D., Natschläger, T. & Schuch, K. (2009). Pcsim: A parallel simulation environ-
ment for neural circuits fully integrated with python. Frontiers in neuroinformatics,
3.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . Duch-
esnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12, 2825–2830.
Perrin, D. (2011). Complexity and high-end computing in biology and medicine. In Soft-
ware tools and algorithms for biological systems (pp. 377–384). Springer.
Plis, S. M., Weisend, M. P., Damaraju, E., Eichele, T., Mayer, A., Clark, V. P., . . . Calhoun,
V. D. (2011). Effective connectivity analysis of fMRI and MEG data collected under
identical paradigms. Computers in biology and medicine, 41(12), 1156–1165.
Ponulak, F. & Kasiski, A. (2010). Supervised learning in spiking neural networks with
resume: Sequence learning, classiﬁcation, and spike shifting. Neural Computation,
22(2), 467–510.
Posner, M. I., Petersen, S., Fox, P. & Raichle, M. (1988). Localization of cognitive opera-
tions in the human brain. Science, 240(4859), 1627–1631.
Price, G., Cercignani, M., Parker, G. J., Altmann, D. R., Barnes, T. R., Barker, G. J., . . .
Ron, M. A. (2007). Abnormal brain connectivity in ﬁrst-episode psychosis: A dif-
fusion mri tractography study of the corpus callosum. Neuroimage, 35(2), 458–
466.
Pruim, R. H., Mennes, M., van Rooij, D., Llera, A., Buitelaar, J. K. & Beckmann, C. F.
(2015). Ica-aroma: A robust ica-based strategy for removing motion artifacts from
fmri data. Neuroimage, 112, 267–277.
Quandl Financial, Economic and Alternative Data. (n.d.). https://www.quandl.com/.
Quarantelli, M., Salvatore, E., Giorgio, S. M. D. A., Filla, A., Cervo, A., Russo, C. V., . . .
De Michele, G. (2013). Default-mode network changes in huntingtons disease: An
integrated mri study of functional connectivity and morphometry. PloS one, 8(8),
e72159.
Bibliography
216

Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81–106.
Quinlan, J. R. (2014). C4. 5: Programs for machine learning. Elsevier.
Randell, B. (2013). The origins of digital computers: Selected papers. Springer.
Ravan, M., Hasey, G., Reilly, J. P., MacCrimmon, D. & Khodayari-Rostamabad, A. (2015).
A machine learning approach using auditory odd-ball responses to investigate the
effect of clozapine therapy. Clinical Neurophysiology, 126(4), 721–730.
Rodrguez, V. M., Andreé, R. M., Castejón, M., Garca, E. C. et al. (1996). Spect study
of regional cerebral perfusion in neuroleptic-resistant schizophrenic patients who
responded or did not respond to clozapine. The American Journal of Psychiatry.
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage
and organization in the brain. Psychological review, 65(6), 386.
Rössler, W., Salize, H. J., van Os, J. & Riecher-Rössler, A. (2005). Size of burden of
schizophrenia and psychotic disorders. European Neuropsychopharmacology, 15(4),
399–409.
Rumelhart, D. E., Hinton, G. E. & Williams, R. J. (1988). Learning representations by
back-propagating errors. Cognitive modeling, 5(3), 1.
Russell, S. (2003). Norvig (2003). Artiﬁcial intelligence: a modern approach, 25–26.
Saad, E. W., Prokhorov, D. V. & Wunsch, D. C. (1998). Comparative study of stock
trend prediction using time delay, recurrent and probabilistic neural networks.
IEEE Transactions on neural networks, 9(6), 1456–1470.
Schaller, R. R. (1997). Moore’s law: Past, present and future. IEEE spectrum, 34(6), 52–
59.
Schaul, T., Bayer, J., Wierstra, D., Sun, Y., Felder, M., Sehnke, F., . . . Schmidhuber, J.
(2010). Pybrain. The Journal of Machine Learning Research, 11, 743–746.
Schliebs, S., Defoin-Platel, M., Worner, S. & Kasabov, N. (2009). Integrated feature and
parameter optimization for an evolving spiking neural network: Exploring hetero-
geneous probabilistic models. Neural Networks, 22(5), 623–632.
Schrauwen, B. & Van Campenhout, J. (2003). Bsa, a fast and accurate spike train encod-
ing scheme. In Proceedings of the international joint conference on neural networks
(Vol. 4, pp. 2825–2830). IEEE Piscataway, NJ.
Schrauwen, B. & Van Campenhout, J. (2004). Extending spikeprop. In Neural networks,
2004. proceedings. 2004 ieee international joint conference on (Vol. 1, pp. 471–475).
IEEE.
Schrauwen, B., Verstraeten, D. & Van Campenhout, J. (2007). An overview of reservoir
computing: Theory, applications and implementations. In Proceedings of the 15th
european symposium on artiﬁcial neural networks. p. 471-482 2007 (pp. 471–482).
Schuler, I. (2015). Neuromorphic computing: From materials to systems architecture.
Accessed: 2016-07-16.
Scott, N. (2015). Evolving spiking neural networks for spatio- and spectro- temporal data
analysis: Models, implementations, applications (Doctoral dissertation, Auckland
University of Technology).
Segev, I., Burke, R. E. & Hines, M. (1998). Compartmental models of complex neurons.
Methods in neuronal modeling: From ions to networks, 2.
Bibliography
217

Sengupta, N. & Kasabov, N. (2017). Spike-time encoding as a data compression tech-
nique for pattern recognition of temporal data. Information Sciences, 406-407,
133–145. doi:https://doi.org/10.1016/j.ins.2017.04.017
Sengupta, N., McNabb, C. B., Kasabov, N. & Russell, B. R. (2018). Integrating space,
time, and orientation in spiking neural networks: A case study on multimodal
brain data modeling. IEEE Transactions on Neural Networks and Learning Systems,
PP(99), 1–15. doi:10.1109/TNNLS.2018.2796023
Sengupta, N., Ramos, J. I. E., Tu, E., Marks, S., Scott, N., Weclawski, J., . . . Abbott,
A. (2018). From von neumann architecture and atanasoffs abc to neuromorphic
computation and kasabovs neucube: Principles and implementations. Springer.
Sengupta, N., Scott, N. & Kasabov, N. (2015). Framework for knowledge driven optimi-
sation based data encoding for brain data modelling using spiking neural network
architecture. In Proceedings of the ﬁfth international conference on fuzzy and neuro
computing (fancco - 2015) (pp. 109–118). Cham: Springer International Publish-
ing. doi:10.1007/978-3-319-27212-2_9
Shallice, T. (1988). From neuropsychology to mental structure. Cambridge University
Press.
Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical
Journal, 27(3), 379–423.
Shinkareva, S. V., Mason, R. A., Malave, V. L., Wang, W., Mitchell, T. M. & Just, M. A.
(2008). Using fmri brain activation to identify cognitive states associated with
perception of tools and dwellings. PLoS One, 3(1), e1394.
Siddique, M. & Tokhi, M. (2001). Training neural networks: Backpropagation vs. genetic
algorithms. In Neural networks, 2001. proceedings. ijcnn’01. international joint con-
ference on (Vol. 4, pp. 2673–2678). IEEE.
Sierra, C. V. R., Versluis, M. J., Hoogduin, J. M. & Duifhuis, H. (2008). Acoustic fmri
noise: Linear time-invariant system model. IEEE Transactions on Biomedical Engi-
neering, 55(9), 2115–2123.
Sjöström, J. & Gerstner, W. (2010a). Spike-timing dependent plasticity. Spike-timing
dependent plasticity, 35.
Sjöström, J. & Gerstner, W. (2010b). Spike-timing dependent plasticity. Scholarpedia,
5(2), 1362. revision #151671. doi:10.4249/scholarpedia.1362
Smith, S. M. (2002). Fast robust automated brain extraction. Human brain mapping,
17(3), 143–155.
Smith, S. M., Fox, P. T., Miller, K. L., Glahn, D. C., Fox, P. M., Mackay, C. E., . . . Laird,
A. R. et al. (2009). Correspondence of the brain’s functional architecture dur-
ing activation and rest. Proceedings of the National Academy of Sciences, 106(31),
13040–13045.
Smith, S. M., Jenkinson, M., Woolrich, M. W., Beckmann, C. F., Behrens, T. E., Johansen-
Berg, H., . . . Flitney, D. E. et al. (2004). Advances in functional and structural mr
image analysis and implementation as fsl. Neuroimage, 23, S208–S219.
Song, S., Miller, K. D. & Abbott, L. F. (2000). Competitive hebbian learning through
spike-timing-dependent synaptic plasticity. Nature neuroscience, 3(9), 919–926.
Specht, D. F. (1990). Probabilistic neural networks. Neural networks, 3(1), 109–118.
Bibliography
218

Spitzer, R. L. & Williams, J. B. (1980). Diagnostic and statistical manual of mental dis-
orders. In American psychiatric association. Citeseer.
Sporns, O. & Betzel, R. F. (2016). Modular brain networks. Annual review of psychology,
67, 613–640.
Squire, L. & Kandel, E. (1999). Memory: From molecules to memory: Scientiﬁc american
library. New York, Freeman Press.
Stämpﬂi, P., Reischauer, C., Jaermann, T., Valavanis, A., Kollias, S. & Boesiger, P. (2008).
Combining fmri and dti: A framework for exploring the limits of fmri-guided dti
ﬁber tracking and for verifying dti-based ﬁber tractography results. Neuroimage,
39(1), 119–126.
Stephan, K. E., Friston, K. J. & Frith, C. D. (2009). Dysconnection in schizophrenia: From
abnormal synaptic plasticity to failures of self-monitoring. Schizophrenia bulletin,
sbn176.
Sui, J., Adali, T., Yu, Q., Chen, J. & Calhoun, V. D. (2012). A review of multivariate meth-
ods for multimodal fusion of brain imaging data. Journal of neuroscience methods,
204(1), 68–81.
Supekar, K., Menon, V., Rubin, D., Musen, M. & Greicius, M. D. (2008). Network analy-
sis of intrinsic functional brain connectivity in alzheimer’s disease. PLoS computa-
tional biology, 4(6), e1000100.
Sutton, R. S. & Barto, A. G. (1998). Reinforcement learning: An introduction. MIT press
Cambridge.
Taylor, D., Scott, N., Kasabov, N., Capecci, E., Tu, E., Saywell, N., . . . Hou, Z.-G. (2014).
Feasibility of neucube snn architecture for detecting motor execution and motor
intention for use in bciapplications. In Neural networks (ijcnn), 2014 international
joint conference on (pp. 3221–3225). IEEE.
Teipel, S. J., Bokde, A. L., Meindl, T., Amaro, E., Soldner, J., Reiser, M. F., . . . Hampel, H.
(2010). White matter microstructure underlying default mode network connectiv-
ity in the human brain. Neuroimage, 49(3), 2021–2032.
Theunissen, F. & Miller, J. P. (1995). Temporal encoding in nervous systems: A rigorous
deﬁnition. Journal of computational neuroscience, 2(2), 149–162.
Thompson, S. E. & Parthasarathy, S. (2006). Moore’s law: The future of si microelectron-
ics. Materials today, 9(6), 20–25.
Thorpe, S. J. & Gautrais, J. (1997). Rapid visual processing using spike asynchrony.
Advances in neural information processing systems, 901–907.
Thorpe, S. J. & Imbert, M. (1989). Biological constraints on connectionist modelling.
Connectionism in perspective, 63–92.
Thorpe, S., Delorme, A. & Van Rullen, R. (2001). Spike-based strategies for rapid pro-
cessing. Neural networks, 14(6), 715–725.
Tieleman, T. (2008). Training restricted boltzmann machines using approximations to
the likelihood gradient. In Proceedings of the 25th international conference on ma-
chine learning (pp. 1064–1071). ACM.
Toumey, C. (2016). Less is moore. Nature Nanotechnology, 11, 2–3.
Tsaih, R., Hsu, Y. & Lai, C. C. (1998). Forecasting s&p 500 stock index futures with a
hybrid ai system. Decision Support Systems, 23(2), 161–174.
Bibliography
219

Tsien, J. Z., Li, M., Osan, R., Chen, G., Lin, L., Wang, P. L., . . . Liu, T. et al. (2013). On
brain activity mapping: Insights and lessons from brain decoding project to map
memory patterns in the hippocampus. Science China. Life Sciences, 56(9), 767.
Tu, E., Kasabov, N., Othman, M., Li, Y., Worner, S., Yang, J. & Jia, Z. (2014). Neucube
(st) for spatio-temporal data predictive modelling with a case study on ecological
data. In Neural networks (ijcnn), 2014 international joint conference on (pp. 638–
645). IEEE.
Tu, E., Kasabov, N. & Yang, J. (2017). Mapping temporal variables into the neucube for
improved pattern recognition, predictive modeling, and understanding of stream
data. IEEE transactions on neural networks and learning systems, 28(6), 1305–
1317.
Turing, A. M. (1950). Computing machinery and intelligence. Mind, 59(236), 433–460.
Valdes-Sosa, P. A., Sanchez-Bornot, J. M., Sotero, R. C., Iturria-Medina, Y., Aleman-
Gomez, Y., Bosch-Bayard, J., . . . Ozaki, T. (2009). Model driven EEG/fMRI fusion
of brain oscillations. Human brain mapping, 30(9), 2701–2721.
Van Rullen, R., Gautrais, J., Delorme, A. & Thorpe, S. (1998). Face processing using one
spike per neurone. Biosystems, 48(1), 229–239.
Van Rullen, R. & Thorpe, S. J. (2001). Rate coding versus temporal order coding: What
the retinal ganglion cells tell the visual cortex. Neural computation, 13(6), 1255–
1283.
van Veen, F. (2016). The neural network zoo. http://www.asimovinstitute.org/neural-
network-zoo/.
Vazquez, A. L. & Noll, D. C. (1998). Nonlinear aspects of the bold response in functional
mri. Neuroimage, 7(2), 108–118.
Veropoulos, K., Campbell, C., Cristianini, N. et al. (1999). Controlling the sensitivity of
support vector machines. In Proceedings of the international joint conference on ai
(pp. 55–60).
Victor, J. D. & Purpura, K. P. (1997). Metric-space analysis of spike trains: Theory, algo-
rithms and application. Network: computation in neural systems, 8(2), 127–164.
Vivarelli, F. & Williams, C. K. (2001). Comparing bayesian neural network algorithms
for classifying segmented outdoor images. Neural Networks, 14(4), 427–437.
Vos, T., Barber, R. M., Bell, B., Bertozzi-Villa, A., Biryukov, S., Bolliger, I., . . . Dicker, D.
et al. (2015). Global, regional, and national incidence, prevalence, and years lived
with disability for 301 acute and chronic diseases and injuries in 188 countries,
1990–2013: A systematic analysis for the global burden of disease study 2013. The
Lancet, 386(9995), 743–800.
Wang, Y. (2014). Stock price direction prediction by directly using prices data: An em-
pirical study on the kospi and hsi. International Journal of Business Intelligence and
Data Mining, 9(2), 145–160.
Watts, M. J. (2009). A decade of kasabov’s evolving connectionist systems: A review.
IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Re-
views), 39(3), 253–269.
Wheeler, A. J. (2008). Treatment pathway and patterns of clozapine prescribing for
schizophrenia in new zealand. Annals of Pharmacotherapy, 42(6), 852–860.
Bibliography
220

Widrow, B. & Lehr, M. A. (1990). 30 years of adaptive neural networks: Perceptron,
madaline, and backpropagation. Proceedings of the IEEE, 78(9), 1415–1442.
Wilson, E. C. (2001). Parallel implementation of a large scale biologically realistic neocor-
tical neural network simulator (Doctoral dissertation, University of Nevada Reno).
Wilson, M. A., Bhalla, U. S., Uhley, J. D. & Bower, J. M. (1989). Genesis: A system for
simulating neural networks. In Advances in neural information processing systems
(pp. 485–492).
Wilson, R. I. (2008). Neural and behavioral mechanisms of olfactory perception. Current
opinion in neurobiology, 18(4), 408–412.
Woolrich, M. W., Ripley, B. D., Brady, M. & Smith, S. M. (2001). Temporal autocorrela-
tion in univariate linear modeling of fmri data. Neuroimage, 14(6), 1370–1386.
Wysoski, S. G., Benuskova, L. & Kasabov, N. (2006). Adaptive learning procedure for a
network of spiking neurons and visual pattern recognition. In International confer-
ence on advanced concepts for intelligent vision systems (pp. 1133–1142). Springer.
Wysoski, S. G., Benuskova, L. & Kasabov, N. (2010). Evolving spiking neural networks
for audiovisual information processing. Neural Networks, 23(7), 819–835.
Yang, H., Liu, J., Sui, J., Pearlson, G. & Calhoun, V. D. (2010). A hybrid machine learning
method for fusing fMRI and genetic data: Combining both improves classiﬁcation
of schizophrenia. Front Hum Neurosci, 4(192.10), 3389.
Yeganeh-Doost, P., Gruber, O., Falkai, P. & Schmitt, A. (2011). The role of the cerebellum
in schizophrenia: From cognition to molecular pathways. Clinics, 66, 71–77.
Yen, G. G. & Lu, H. (2000). Hierarchical genetic algorithm based neural network de-
sign. In Combinations of evolutionary computation and neural networks, 2000 ieee
symposium on (pp. 168–175). IEEE.
Yiqing, L., Xigang, Y. & Yongjian, L. (2007). An improved pso algorithm for solving
non-convex nlp/minlp problems with equality constraints. Computers & chemical
engineering, 31(3), 153–162.
Yokota, T., Gen, M. & Li, Y.-X. (1996). Genetic algorithm for non-linear mixed integer
programming problems and its applications. Computers & industrial engineering,
30(4), 905–917.
Yu, Q., A Allen, E., Sui, J., R Arbabshirani, M., Pearlson, G. & D Calhoun, V. (2012).
Brain connectivity networks in schizophrenia underlying resting state functional
magnetic resonance imaging. Current topics in medicinal chemistry, 12(21), 2415–
2425.
Zhang, X.-d., Li, A. & Pan, R. (2016). Stock trend prediction based on a new status box
method and adaboost probabilistic support vector machine. Applied Soft Comput-
ing, 49, 385–398.
Zhang, Y. [Yongyue], Brady, M. & Smith, S. (2001). Segmentation of brain mr images
through a hidden markov random ﬁeld model and the expectation-maximization
algorithm. IEEE transactions on medical imaging, 20(1), 45–57.
Zhang, Y. [Yudong] & Wu, L. (2009). Stock market prediction of s&p 500 via com-
bination of improved bco approach and bp neural network. Expert systems with
applications, 36(5), 8849–8854.
Bibliography
221

A
Sequential eSNN
Architectures for Cyber
Fraud Detection
The sequential eSNN classiﬁer is inspired by the architecture proposed in (Dora,
Suresh & Sundararajan, 2015). The proposed architecture is a two layer fully
connected feed-forward network as shown in Figure A.1. The input layer con-
sists of m input neurons that convert real-valued inputs to spike-patterns (Figure
A.2) and the output layer consist of spiking neurons (Figure A.3), The network
consists of a decision block which monitors the output of the intermediate neu-
rons to determine the predicted class for the presented sample. The intermedi-
ate neurons are modelled as IF neurons. These neurons can generate multiple
spikes. The intermediate neurons process the input spike patterns from the in-
put neurons. Each intermediate neuron is associated with a particular class and
this association is stored in the decision block. The decision block identiﬁes the
intermediate neuron that ﬁres ﬁrst and returns its associated class label as the
predicted class label. It should be noted here that although, an intermediate
neuron can generate multiple spikes,the decision block uses only the ﬁrst spike
to determine the predicted class.
The proposed method is based on the evolving spiking neural network model for
classiﬁcation. It builds on the Thorpes model (S. Thorpe et al., 2001), in which
early spikes are given more importance. Thorpe’s model is highly inﬂuenced by
the visual pattern recognition system. This method has fast supervised one pass
learning. The eSNN have two layers:(1) an input layer and (2) an output layer.
Initially, the output layer is empty. The output neurons are added to the output
layer depending on the input samples during the training phase.
222

Figure A.1.: Proposed eSNN architecture for phishing website classiﬁcation problem.
Figure A.2.: Input neuron model
Figure A.3.: Intermediate neuron model
To deal with real-valued data sets, each data sample needs to map with the
sequence of spikes using a precise neural encoding technique. Rank order pop-
ulation encoding is used for this purpose. The population encoding uses the
Gaussian receptive ﬁelds (GRF) to encode the real-valued data. In this method,
each input goes through a ﬁxed number of GRF, and it generates a peak at the
certain point of time. Following steps are performed during the classiﬁcation
process:
In rank order coding, every input is encoded individually using a set of P respon-
ders (i.e. receptive ﬁelds). For ith input neuron with P receptive ﬁelds (P > 2)
223

whose input feature varies from Ii
min to Ii
max. The centre and width of the hth
receptive ﬁeld is given by:
µh
i = Ii
min + (2h −3)
2
(Ii
max −Ii
min)
(P −2)
(A.1)
and
σh
i = 1
γ
(Ii
max −Ii
min)
(P −2)
(A.2)
where γ directly controls the width of the receptive ﬁeld. The width controls the
overlap between the two receptive ﬁelds. The τh
i is the ﬁring time of the neuron
calculated using:
τh
i =
⌊
T(1 −ϕh
i )
⌋
(A.3)
Where T is the simulation interval and ϕh
i is the GRF output deﬁned as follows:
ϕh
i = exp(−(xi
r −µh
i )2
2(σh
i )2
)
(A.4)
The output of the hth responder of the ith input neuron is given by:
uh
i (t, xi
r) = fh
i (xi
r)δh
i (t −τh
i )
(A.5)
Where fh
i (·) is the spike amplitude function and δh
i (.) is the dirac delta function
or ﬁring time function.
The amplitude of the spikes
fh
i (xi
r) −
λrh
i
1 + |xir −µh
i |
(A.6)
224

Where rh
i is the rank of the hth responder of ith neuron and λ is the slope of
the amplitude function. The rank of the spikes is determined using the ranking
function as follows:
FR(x, y) =





1,
if ϕx
i ≥ϕy
i
0,
otherwise
(A.7)
Where x and y are the indices of any two receptive ﬁelds of the ith neuron is
given by:
rh
i = 1 +
P
∑
y=1,y̸=h
FR(h, y)
(A.8)
The equations above describes the spike generation process using the population
encoding framework and forms the ﬁrst layer. The second layer also known
as the output layer consists of another set of neurons and next, we describe
the rules for the establishment of the synaptic connection between the input
and output neuron and synaptic weight initialisation scheme. It is a sequential
learning architecture that starts with no output neuron. The algorithm either
chooses to add the new neuron at output layer or update the synaptic weight for
training sample.
• Addition of output neuron: output neuron addition strategy to evolve the
neuron if the current sample satisﬁes the following condition:
ˆc = {ϕ} OR (c /∈coverall) OR (c ̸= ˆc AND ||f −wnrs|| > βa)
(A.9)
Where nrs is nearest output neuron of same class, βa is distance threshold
constant. f is the set of current sample spike-amplitude-response and wnrs
is the existing weight of synapse of the same class in the network. ϕ rep-
resents for the current sample none of the output neurons ﬁred. coverall
is the class label associated with the current output layer neurons and c
225

is the actual class label input training sample. The nearest output neuron
is evaluated using the Euclidean distance between the current sample am-
plitude response and existing synaptic weight of all the output neurons of
the same class. If the new sample satisﬁes the above condition then new
output (k + 1) neuron is added its synaptic weight and threshold is given
as:
wk+1 = f
(A.10)
θk+1 = αwT
k+1wk+1
(A.11)
• In this strategy, the training sample associated class is not the same as the
class associated with the ﬁred output neuron. It means the different class
label neuron ﬁre ﬁrst (nrl) and there exists another nearest output neuron
of the same class (nrs). So the weight vector of different classes associated
with output neurons is conﬂicting in nature. To detect this conﬂict problem
following condition should be satisﬁed:
c ̸= ˆc AND ||f −wnrs|| < βa
(A.12)
To resolve this conﬂict, the nearest neuron of the same class goes into long-
term potentiation and the output neuron goes into different class goes into
long-term depression. The synaptic weight update is as follows:
wnrs = (1 −ηnrs)wnrs + ηnrsf
(A.13)
θnrs = (1 −ηnrs)θnrs + ηnrsαfTf
(A.14)
Where ηnrs is self-adaptive learning factor. It is also called self-adaptive
potentiation factor of output neuron. The factor ηnrs for output neuron
given as:
ηnrs =
ηnrs
1 + ηnrs
(A.15)
226

The output neuron of the other class goes to long-term depression. Due
to the effect of this, the output neuron will not ﬁre for the similar samples
and the weights are updated as follows:
wnrl = (1 + k)wnrl −kf
(A.16)
Where k is the depression factor, which controls the synaptic weight de-
pression factor. It is close to zero because the higher value of k results in
a massive shift in the synaptic weight. Resulting information loss is stored
in the network.
• Synaptic weight update approach: If the actual class label c is same as
the class label associated with the ﬁred output neuron then the synaptic
weight of connection and threshold of the output neuron are updated as
following:
wnrs = (1 −ηnrs)wnrs + ηnrsf
(A.17)
θnrs = (1 −ηnrs)θnrs + ηnrsαfTf
(A.18)
The voltage of the output neuron at any time calculated as follows:
vj(t1) =
t+1
∑
t=0
P
∑
h=1,i=1
uh
i (t, xi
r)wh
ij
(A.19)
A.1 Hyperparameter Selection of eSNN
Classiﬁer
There are a number of hyperparameters that needs to be set for the algorithm
described above. The ﬁrst parameter is the number of receptive ﬁelds P. During
the experiment, we observed that the number of receptive ﬁeld increases as
the ﬁring time decreases. The number of receptive ﬁelds also depends on the
A.1
Hyperparameter Selection of eSNN Classiﬁer
227

dataset. P also controls the amplitude of the input neuron. For our dataset, the
number of the receptive ﬁeld is set to 8. The second parameter is the overlap
factor γ, which controls the overlap between receptive ﬁelds and regulates the
width of the receptive ﬁeld. γ the width of the receptive ﬁeld and thus impact
only the ﬁring time function. In the experiment, the γ value is set to 3, which
means 30% overlap exists between two subsequent receptive ﬁelds. It controls
the range of the of receptive ﬁeld and useful for temporal coding. The third
factor is λ. It represent the slope of the amplitude function f(·). There are other
parameters as well. The parameter α is the threshold fraction which controls
the ﬁring time of the output neuron and is set between 0.5 and 0.9. γ is the self-
adaptive potential factor. Its value is initialised to 0.5 and decayed to 0. β is a
constant that controls the addition of the neuron at the output layer. The range
of the parameter β depends on the feature count and has an inverse relationship
with values ranging between 0.5 and 0.8. The depression factor k controls if the
neuron ﬁred wrongly during training, the corresponding weights goes into long-
term depression. The value of k is typically set to a very small value between
0.01 to 0.35. Large value of k leads to information loss in the network.
A.2 Testing Method of eSNN Classiﬁer
At the end of the training phase, the knowledge is stored in the network. For test-
ing, ﬁrst, need to convert the real-valued input data into the spikes. Each output
neuron associated with certain threshold value are chosen during the training.
If the incoming potential (signal) summation crosses the speciﬁc threshold value
of the particular neuron, then the neuron is ﬁred, and the corresponding class
label is predicted. It is often the case that many output neurons ﬁre for the given
input. To resolve this issue, k-nearest neighbour is applied to predict the class
label. To get the optimal accuracy we check the different values of k to predict
the class label, the value of k are 3, 5, 7, 9, 11, 13, 15 and so on. The k-nearest
neighbour is performed using the Euclidean distance between the amplitude of
spikes of the current input testing sample and the synaptic weight correspond-
A.2
Testing Method of eSNN Classiﬁer
228

ing to the ﬁred neuron which is stored by the network at the time of training i.e.
||f −w|| with weights corresponding to each ﬁred output neuron.
A.2
Testing Method of eSNN Classiﬁer
229

B
eSNN Architectures for
Stock Price Movement
Forecast
In the last few years, there have been a large number of studies performed on
predicting the stock market trend. Both researchers and practitioners have used
numerous approaches to predict the stock market trend. Due to the chaotic
nature and the complexity of the stock market indices, researchers are still strug-
gling to design techniques that can accurately model the behaviour of their
trends.
In Table B.1, we have given an overview of the different prediction
techniques and stock market indices used in the literature to predict the stock
price direction by taking technical indicators as input variables.
Table B.1 presents a concise literature review on machine learning techniques
applied on stock price movement prediction. It is clear that many traditional
machine learning techniques have been explored to predict stock price direction
and also most of the algorithms have been applied to a single stock index to mea-
sure the performance of the model. In the third generation of neural networks
the spiking neural networks (SNN) now offers a new perspective to explore for
the solution of the problem. SNN uses spike information representation and
spike-time learning rules to capture temporal associations between a large num-
ber of temporal variables in streaming data and to predict future events. One
of the successful SNN models is the eSNN model, where the number of spik-
ing neurons evolve incrementally in time to capture temporal prototypes from
data.
230

Figure B.1.: Architecture of the proposed technical stock indicator SI-eSNN model for
stock price direction prediction
B.1 The SI-eSNN Model for Stock Trend
Prediction Based on Stock Indicators
B.1.1 Overall Architecture
The architecture of the SI-eSNN model is presented in Figure B.1. The ﬁrst layer
is the set of inputs to the model, each of them representing a technical stock
indicator. The research so far has demonstrated that using technical indicators
can lead to better results than using real stock values as time series and also
that there is a lot of research done on selecting the most appropriate technical
indicators. In the model presented in Figure B.1 the input technical indicators
have been selected from (J. Patel et al., 2015; K.-j. Kim, 2003; Kara et al., 2011)
and explained in Table B.2, but these indicators can vary across stock prediction
applications.
0Ct is the closing price, Lt is the low price and Ht is the high price at time t, DIFFt =
EMA(12)t −EMA(26)t, EMA is the moving average, EMA(k)t = EMA(k)t−1 + α ×
(Ct −EMA(k)t−1), α is the smoothing factor which is equal to
2
k+1, k is the time period of
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock Indicators
231

.
Layer 2 is the encoding layer, where the real value of each input variable (tech-
nical indicator) is encoded as trains of spikes generated by several encoding
spiking neurons (or also, pre-synaptic neurons), each of them having a receptive
ﬁeld. The receptive ﬁelds of neighbouring neurons are overlapping as Gaussian
or Logistic functions and all of them covering the whole range of the values of
this variable. The number of these encoding neurons (receptive ﬁelds) can vary,
and this is a user-deﬁned parameter that is optimised for a better performance
of the model.
Layer 3 is the output evolving layer, which evolves output spiking neurons that
represent clusters (prototypes) of input vectors that belong to the same class, in
this case class UP and class DOWN. Each output neuron is connected to all the
input neurons, and the connection weights are subject to learning from data.
The architecture of the SI-eSNN model for stock price direction prediction allows
for incremental learning. It is adaptive to new data when it becomes available.
Hence, it can learn new samples without retraining the model on old data. The
details of the functioning of the SI-eSNN model is presented below.
B.1.2 Neural Encoding
To learn real-valued data, each instance or sample (input vector) is encoded
in the form of spikes over time using a neural encoding technique such as
rank order population coding (S. J. Thorpe & Gautrais, 1997; Bohte, Kok &
La Poutre, 2002). In our study, we have used rank order population encoding
as per (Schliebs, Defoin-Platel, Worner & Kasabov, 2009). Population encoding
maps the input value into a series of spikes over time using an array of Gaussian
receptive ﬁelds that describe pre-synaptic neurons. The center (Cj) and width
k-day exponential moving average, LLt and HHt implies lowest low and highest high in the
last t days respectively. Mt = Ht+Lt+Ct
3
, SMt =
∑n
i=1 Mt−i+1
n
, Dt =
∑n
i=1 |Mt−i+1−SMt|
n
, UPt
means upward price change while DWt is the downward price change at time t
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock Indicators
232

(Wj) of each of the Gaussian or Logistic receptive ﬁeld of pre-synaptic neurons
j are deﬁned as:
Cj = In
min + 2j −3
2
× In
max −In
min
N −2
(B.1)
and
Wj = 1
β × In
max −In
min
N −2
, 1 ≤β ≤2
(B.2)
Where N is the number of receptive ﬁelds; n is the range of input variable n =
[In
min, In
max] , the parameter β deﬁnes the width of each receptive ﬁeld. Output
of each of the pre-synaptic neuron j using Gaussian receptive ﬁeld is deﬁned
as:
outputj = exp(−(x −Cj)2
2 · W 2
j
)
(B.3)
Output of each of the pre-synaptic neuron j using Logistic receptive ﬁeld is de-
ﬁned as:
outputj =
exp(−(x−Cj)
Wj
)
(1 + exp(−(x−Cj)
Wj
))2
(B.4)
The ﬁring time of each of the pre-synaptic neurons is deﬁned as:
τj = ⌊T(1 −outputj)⌋
(B.5)
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock Indicators
233

where: T is the simulation or spike time interval.
When a real value input is presented to the N pre-synaptic spiking neurons, the
ﬁrst spike is generated by this neuron to which receptive ﬁeld the input value
belongs to the highest degree, etc.
B.1.3 Neural Model
For the context of SI-eSNN, Thorpes neuron model (S. J. Thorpe & Gautrais,
1997) has been used since it is simple and effective. The Thorpes model is based
on the timing of each spike, that is, earlier spike deﬁnes stronger weight as
compared to the later spike. Each neuron in this model can spike at most once.
A neuron in this model ﬁres when its post-synaptic potential (PSP) reaches the
threshold value. The PSP of neuron i is deﬁned as:
PSPi =







0,
if fired
∑wji ∗modorder(j),
otherwise
(B.6)
Where wji represents the weight of the synaptic connection between pre-synaptic
neuron j to the output neuron i; mod represents modulation factor with a range
in between 0 to 1; order(j) deﬁnes the rank of pre-synaptic neurons spike. The
ﬁrst rank will be assigned as 0 and subsequently, rank will be increased by 1
based on ﬁring time of each pre-synaptic neurons.
B.1.4 Learning in the Output Neurons
The eSNN algorithm was ﬁrst introduced in (Wysoski, Benuskova & Kasabov,
2006; N. Kasabov, 2007). The learning techniques used by the eSNN model is
one-pass learning, that is, the model requires one-time presentation of a sample
in a feed-forward manner. It will create an output neuron for each input sample.
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock Indicators
234

The weight vector and a threshold value for each of the output neuron generated
towards the training pattern are learned and stored in the repository. However,
if this weight vector is similar to the weight vector of the already trained neuron
in the repository with some similarity threshold, then it will merge with the most
similar one. Merging here means updating the weights and the threshold value
of the merged neurons. The weight vector and threshold value of the merged
neurons update their values by taking the average value of new output neuron
weight vector and merged neuron weight vector and the average value of new
output neuron threshold and merged neuron threshold respectively.
B.1.5 Algorithm for eSNN Training
Algorithm B.1 eSNN training algorithm
1: Initialize neuron repository, R = {}
2: Set eSNN parameter mod = [0, 1], C = [0, 1], sim = [0, 1]
3: for ∀input pattern i that belongs to the same class do
4:
Encode input pattern into ﬁring time of multiple pre-synaptic neurons j
5:
Create a new output neuron i for this class and calculate the connection
weights as wji = modorder(j)
6:
Calculate PSPmax(i) = ∑
j wji × modorder(j)
7:
Get PSP threshold value γi = PSPmax(i) × C
8:
if The new neuron weight vector ≤sim of trained output neuron weight
vector in R then
9:
Update the weight vector and threshold of the most similar neuron in
the same output class group
10:
w = wnew+w∗N
N+1
11:
γ = γnew+γ∗N
N+1
12:
where N is the number of previous merges of the most similar neuron
13:
else
14:
Add the weight vector and threshold of the new neuron to the neuron
repository R
15:
end if
16: end for
17: Repeat above for all input patterns of other output classes
The eSNN algorithm creates a repository of output neurons for the training pat-
terns. For each training pattern that belongs to the same class, a new output
neuron is created and connected to all the pre-synaptic neurons in the previ-
ous layer. The weight for each of the connection from pre-synaptic neuron j to
output neuron i is denoted as wji. The value of wji is calculated based on the
B.1
The SI-eSNN Model for Stock Trend Prediction Based on Stock Indicators
235

spike order through a synapse j, which is given in line 5 of Algorithm B.1 as
wji = modorder(j), ∀j where j is the pre-synaptic neuron of output neuron i.
The threshold γi of newly created output neuron would be deﬁned as the fraction
C ∈R, 0 < C < 1, of the maximum post-synaptic potential PSPmax(i): γi =
PSPmax(i) × C
The weight vector of newly created output neuron is then compared with the
already trained output neurons in the repository. If the newly created output
neuron weight vector is less than sim of trained neuron weight vectors, then
the threshold and weight vector of newly created output neuron is merged with
most similar neuron according to w = wnew + w ∗N
N + 1
and γ = γnew + γ ∗N
N + 1
.
Here, N is the number of previous merges of the most similar neurons. After the
merge operation, the newly created output neuron weight vector is discarded,
and the new pattern is presented to model. If none of the trained neurons in the
repository is similar, the new output neuron is added to the repository.
B.1.6 Testing (Recall of the Model on New Data)
The testing process is done by propagating the spikes that encode the test vector
(sample) to all trained output neurons. The class label for the testing sample
will be deﬁned based on the output class label of the output neuron which ﬁres
ﬁrst.
B.2 The CUDA-SI-eSNN Model: A Parallel
eSNN Model for GPU Machines
The training and recall procedures for the SI-eSNN model consists of two op-
erations. In the ﬁrst stage, encoding of the input patterns, calculations of the
B.2
The CUDA-SI-eSNN Model: A Parallel eSNN Model for GPU Machines
236

Algorithm B.2 CUDA-SI-eSNN algorithm
1: Initialize input patterns array in device memory with size is equal to N × M,
Where N is number of input patterns and M is number of input variable.
2: Declare weights matrix in device memory with size is equal to N × P Where
P is number of pre-synaptic neurons which is equal to the number of input
variable multiplied by number of Gaussian receptive ﬁelds.
3: Transfer the input patterns data into the GPU memory.
4: Encode input patterns into ﬁring time of multiple pre-synaptic neurons j. In
this step, each of the threads in the GPU device will calculate the ﬁring-time
of each of the pre-synaptic neurons independently. The number of threads
is equal to N × P.
5: Calculate the connection weights as wji = modorder(j). In this step, each
tread in the GPU device will calculate the weight independently.
6: Transfer the resulted weight matrix into CPU memory.
7: Calculation of maximum post-synaptic potential PSPmax(i), threshold value
γi and the merging operation are similar to the equivalent operations of
eSNN, which are performed on CPU sequentially.
weights and the thresholds are performed, which is from step-4 to step-7 of
the eSNN algorithm.
In the second stage, merging of the output neurons is
performed, which is from step-8 to step-14 of the eSNN algorithm. Since the
encoding and learning of weights of one input pattern is independent of others
except for the merging operation, it is possible to perform both encoding and
weight learning operations parallelly in GPU device using multiple threads. The
procedure for the GPU-based eSNN algorithm is described in Algorithm B.2.
B.3 Sliding Window (SW)-eSNN for
Incremental Learning and Stock Movement
Prediction
While learning in the SI-eSNN and CUDA-SI-eSNN models were vector by vector
and each vector was learned separately from the others even though the vectors
of consecutive days would have some temporal associations. Here we present
an SW-eSNN model where a window based block of the time series of technical
indicator data is used for training a model and a future section to test it. Then
B.3
Sliding Window (SW)-eSNN for Incremental Learning and Stock Movement
Prediction
237

Algorithm B.3 SW-eSNN learning algorithm
1: Train an eSNN model on the whole existing historical data of a stock till time
t (as per Algorithm B.1)
2: Recall the model to predict the next month (t+1) stock movement.
3: When the next month results are known, train the model incrementally on
this data.
4: Aggregate the output neurons if necessary using the aggregation operator
and the sim parameter.
5: Evaluate the classiﬁcation error and the AUC so far.
6: Optimize parameters to improve future time accuracy.
the window moves in time, to select the next section for training and testing,
etc.
B.3
Sliding Window (SW)-eSNN for Incremental Learning and Stock Movement
Prediction
238

Table B.1.: Prediction Techniques and Stock Market Indices for Stock Price Direction Prediction used in Literature
Techniques
Stock Market Indices
GPU-Based
(Chiou, Liu & Tsaih, 1996)
RNN
TSI
No
(Tsaih, Hsu & Lai, 1998)
RBS + RNN
S & P 500
No
(Saad, Prokhorov & Wunsch, 1998)
TDN, RN, PNN
AAPL, IBM, MOT, MSFT
No
(K.-j. Kim & Han, 2000)
GA + ANN
KOSPI
No
(K.-j. Kim, 2003)
SVM
KOSPI
No
(K.-j. Kim & Lee, 2004)
FS + ANN
KOSPI
No
(W. Huang, Nakamori & Wang, 2005)
SVM, LDA, QDA, EBNN
Nikkie-225
No
(Kumar & Thenmozhi, 2006)
RF , SVM
NIFTY, CNX, S & P 500
No
(C.-J. Huang, Yang & Chuang, 2008)
Wrapper, LR, SVM, C 4.5 DT, BP, KNN, Voting
KOSPI, TSE
No
(P. Ou & Wang, 2009)
LDA, QDA, KNN, NB, SVM, LS-SVM, ANN, TBC, BC, Logit
HSI
No
(Yudong Zhang & Wu, 2009)
IBCO + BPNN
S & P 500
No
(Mostafa, 2010)
MLP, GRNN
KSE
No
(Nair, Mohandas & Sakthivel, 2010)
DT + RS
BSE
No
(Kara, Boyacioglu & Baykan, 2011)
SVM, ANN
ISE
No
(Chang et al., 2012)
EPCNNs
S & P 500
No
(Y. Lin, Guo & Hu, 2013)
SVM
TW50
No
(de Oliveira, Nobre & Zárate, 2013)
ANN
PETR4
No
(Wang, 2014)
PCA + SVM
KOSPI, HSI
No
(Bisoi & Dash, 2014)
ANN
BSE, IBM, RIL, OSI
No
(J. Patel, Shah, Thakkar & Kotecha, 2015)
ANN, SVM, NB, RF
NIFTY, BSE-Sensex, Infosys, Reliance
No
(Hafezi, Shahrabi & Hadavandi, 2015)
BNNMAS
DAX
No
(Ballings, Van den Poel, Hespeels & Gryp, 2015)
LR, ANN, KNN, SVM, AB, RF, KF
European Stock Index
No
(X.-d. Zhang, Li & Pan, 2016)
AB + PSVM +GA
SZSE, NASDAQ
No
Our study
eSNN, CUDA-eSNN
BSE, NIFTY-50, NASDAQ, Nikkie-225, DAX, NYSE-Amex,S & P-500, SSE
Yes
B.3
Sliding Window (SW)-eSNN for Incremental Learning and Stock Movement
Prediction
239

Table B.2.: Selected technical indicators and their formulas
Name of the technical indicators (SI)
Formulas 1
Simple 10-day moving average
Ct + Ct−1 + · · · + Ct−9
10
Weighted 10-day moving average
(n) × Ct + (n −1) × Ct−1 + · · · + Ct−9
(n + (n −1) + · · · + 1)
Stochastic K%
Ct −LLt−(n−1)
HHt−(n−1) −LLt−(n−1)
× 100
Stochastic D%
∑n−1
t=0 Kt−i
10
%
Disparity 5
Ct
MA5
× 100
Disparity 10
Ct
MA10
× 100
OSCP
MA5 −MA10
MA10
Momentum
Ct −Ct−9
RSI (Relative Strength Index)
100 −
100
1 +
∑n−1
i=0 UPt−i/n
∑n−1
i=0 DWt−i/n
Larry William R%
Hn −Ct
Hn −Ln
× 100
A/D (Accumulation/Distribution)
Ht−Ct−1
Ht−Lt
CCI (Commodity Channel Index)
Mt−SMt
0.015×Dt
MACD (moving average convergence divergence)
MACD(n)t−1 +
2
n+1 × (DIFFt −MACD(n)t−1)
Table B.3.: The number of instances of UP and DOWN categories in Stock Market
Indices
Datasets (No. of instances)
Years Covered
Training (70%)
Testing (30%)
UP
Down
Total
UP
DOWN
Total
BSE
Jan 2005- Dec 2015
1020
892
1912
437
382
819
Nikkie-225
Jan 1987 - Jul 2016
2609
2483
5092
1115
1067
2182
NASDAQ
Jan 2005 - Dec 2015
1058
865
1923
446
378
824
NIFTY-50
Jan 2008 - Dec 2015
709
644
1353
300
279
579
S&P-500
Jan 1962 - July 2016
5118
4473
9591
2146
1964
4110
Sanghai Stock Exchange
Jan 1998 - July 2016
1679
1470
3149
685
665
1350
DJUS
Apr 2005 - July 2016
997
827
1824
438
344
782
DAX Index
Jan 1991 - July 2016
2413
2114
4527
1044
897
1941
NYSE-Amex
Jan 1996 to July 2016
1957
1668
3625
850
704
1554
Table B.4.: GPU Speciﬁcations
GeForce GT 730 Speciﬁcations
Version
GT 730 DDR3, 64-bit
CUDA Cores
384
Base Clock (MHz)
902
Memory Clock
1.8 Gbps
Standard Memory Conﬁg
2048 MB
Memory Interface
DDR3
Memory Interface Width
64-bit
Memory Bandwidth (GB/sec)
14.4
B.3
Sliding Window (SW)-eSNN for Incremental Learning and Stock Movement
Prediction
240

C
Miscellaneous Code
Snippets
C.1 Matlab Code for the SNNc Unsupervised
Learning Using Forward Adjacency Lists for
Connection C and Weight W



1 function [O] =
neucube_unsupervised_learning_forward_adjacency_list( W,C,D,
indices_of_input_neuron, V_THR, ETA_THR, KAPPA)
2
N=128;
3
Q=1485;
4
T=size(D,1);
5
O= int16(zeros(T,Q));
6
for t=1:T
7
if(mod(t,N)==1)
8
firing_neuron=int16(zeros(1,Q));
9
last_spike_time=uint16(zeros(1,Q));
10
eta=int16(zeros(1,Q));
11
v=single(zeros(1,Q));
12
end
13
last_spike_time(firing_neuron==1)=t;
14
firing_neuron_indices=find(firing_neuron==1);
15
for i=1:size(firing_neuron_indices,2)
16
index_i=firing_neuron_indices(i);
17
post_synaptic_neuron_indices=C{index_i};
18
if(~isempty(post_synaptic_neuron_indices))
19
for j=1:size(post_synaptic_neuron_indices,2)
241

20
index_j=post_synaptic_neuron_indices(j);
21
if(eta(index_j)==0)
22
v(index_j)=v(index_j)+W{index_i}(1,j);
23
end
24
end
25
end
26
firing_neuron(index_i)=0;
27
end
28
a=find(v>=V_THR);
29
not_a=find(v<V_THR);
30
v(a)=0;
31
eta(a)=ETA_THR;
32
firing_neuron(a)=1;
33
eta(not_a)=max(0,eta(not_a)-1);
34
v(not_a)=max(0,v(not_a)-0.002);
35
firing_neuron_indices=find(firing_neuron==1);
36
if(~isempty(firing_neuron_indices))
37
for i=1:size(firing_neuron_indices,2)
38
index_firing=firing_neuron_indices(i);
39
for k=1:Q
40
if(find(C{index_firing}==k))
41
index_j=find(C{index_firing}==k);
42
deltaw=KAPPA/single(t-last_spike_time(k)+1);
43
W{index_firing}(1,index_j)=W{index_firing}(1,
index_j)-deltaw;
44
end
45
if(find([C{k}]==index_firing))
46
index_k=find([C{k}]==index_firing);
47
deltaw=KAPPA/single(t-last_spike_time(k)+1);
48
W{k}(1,index_k)= W{k}(1,index_k)+deltaw;
49
end
50
end
51
end
C.1
Matlab Code for the SNNc Unsupervised Learning Using Forward Adjacency Lists for Connection C
and Weight W
242

52
end
53
firing_neuron(indices_of_input_neuron)=D(t,1:14);
54
firing_neuron(end-14+1:end)=D(t,14+1:end);
55
O(t,:)=firing_neuron;
56
end
57 end


code snippet C.1: Matlab code for SNNc unsupervised learning algorithm using
forward adjacency list data structure for storing connection C
and weight W
C.2 Matlab Code for the SNNc Unsupervised
Learning Using Forward-backward Adjacency
Lists for Connection C and Weight W



1 function [O] =
neucube_unsupervised_forward_backward_adjacency_list( W,C,D,
indices_of_input_neuron, V_THR, ETA_THR, KAPPA )
2
N=128;
3
Q=1485;
4
T=size(D,1);
5
O= int16(zeros(T,Q));
6
for t=1:T
7
if(mod(t,N)==1)
8
firing_neuron=int16(zeros(1,Q));
9
last_spike_time=uint16(zeros(1,Q));
10
eta=int16(zeros(1,Q));
11
v=single(zeros(1,Q));
12
end
13
last_spike_time(firing_neuron==1)=t;
14
firing_neuron_indices=find(firing_neuron==1);
15
for i=1:size(firing_neuron_indices,2)
C.2
Matlab Code for the SNNc Unsupervised Learning Using Forward-backward Adjacency Lists for
Connection C and Weight W
243

16
index_i=firing_neuron_indices(i);
17
post_synaptic_neuron_indices=find(C(index_i,:));
18
if(~isempty(post_synaptic_neuron_indices))
19
for j=1:size(post_synaptic_neuron_indices,2)
20
index_j=post_synaptic_neuron_indices(j);
21
if(eta(index_j)==0)
22
v(index_j)=v(index_j)+W(index_i,index_j);
23
end
24
end
25
end
26
firing_neuron(index_i)=0;
27
end
28
a=find(v>=V_THR);
29
not_a=find(v<V_THR);
30
v(a)=0;
31
eta(a)=ETA_THR;
32
firing_neuron(a)=1;
33
eta(not_a)=max(0,eta(not_a)-1);
34
v(not_a)=max(0,v(not_a)-0.002);
35
firing_neuron_indices=find(firing_neuron==1);
36
if(~isempty(firing_neuron_indices))
37
for i=1:size(firing_neuron_indices,2)
38
index_firing=firing_neuron_indices(i);
39
post_neuron_indices=find(C(index_firing,:)==1);
40
for k=1:size(post_neuron_indices,2)
41
index_j=post_neuron_indices(1,k);
42
deltaw=KAPPA/single(t-last_spike_time(index_j)
+1);
43
W(index_firing,index_j)=W(index_firing,index_j)-
deltaw;
44
end
45
pre_neuron_indices=find(C(:,index_firing)==1);
46
for k=1:size(pre_neuron_indices,1)
C.2
Matlab Code for the SNNc Unsupervised Learning Using Forward-backward Adjacency Lists for
Connection C and Weight W
244

47
index_j=pre_neuron_indices(k,1);
48
deltaw=KAPPA/single(t-last_spike_time(index_j)
+1);
49
W(index_j,index_firing)=W(index_j,index_firing)+
deltaw;
50
end
51
end
52
end
53
firing_neuron(indices_of_input_neuron)=D(t,1:14);
54
firing_neuron(end-14+1:end)=D(t,14+1:end);
55
O(t,:)=firing_neuron;
56
end
57 end


code snippet C.2: Matlab code for SNNc unsupervised learning algorithm using
forward-backward adjacency list data structure for storing
connection C and weight W
C.3 Python Code for the Modular
Implementation of the SNNc



1 class SNNc:
2
def __init__(self, spiking_neuron_coordinates,
input_neuron_coordinates, SNNc_max_distance=None,
connection_algorithm="swc", radial_distance_coverage=0.1):
3
assert isinstance(connection_algorithm, str)
4
assert isinstance(radial_distance_coverage, float)
5
assert isinstance(spiking_neuron_coordinates, np.ndarray)
6
assert isinstance(input_neuron_coordinates, np.ndarray)
7
8
if connection_algorithm not in "swc":
C.3
Python Code for the Modular Implementation of the SNNc
245

9
raise ValueError(’see doc string for possible values
of connection algorithm parameter!!’)
10
if not 0 < radial_distance_coverage <= 1:
11
raise ValueError(’The radial distance coverage
parameter should be between (0,1]!!’)
12
self.spiking_neuron_coordinates =
spiking_neuron_coordinates
13
self.input_neuron_coordinates = input_neuron_coordinates
14
self.SNNc_coordinates = np.vstack((
input_neuron_coordinates, spiking_neuron_coordinates))
15
self.spiking_neuron_count = len(self.
spiking_neuron_coordinates)
16
self.input_neuron_count = len(self.
input_neuron_coordinates)
17
self.SNNc_neuron_count = self.input_neuron_count + self.
spiking_neuron_count
18
19
self.input_coordinate_indices = np.arange(self.
input_neuron_count)
20
self.spiking_coordinate_indices = np.arange(self.
input_neuron_count, self.SNNc_neuron_count)
21
"""
22
Calculate maximum distance between the SNNc neurons if
SNNc_max_distance=None. Uses a brute force approach due
23
to the irregularity in the shape. Consider updating it to
a non bruteforce approach later.
24
"""
25
if SNNc_max_distance is None:
26
self.SNNc_MAX_DISTANCE = 0
27
for ind1, neuron1 in enumerate(self.SNNc_coordinates):
28
for ind2, neuron2 in enumerate(self.
SNNc_coordinates):
29
distance = np.linalg.norm(neuron1 - neuron2)
C.3
Python Code for the Modular Implementation of the SNNc
246

30
if distance > self.SNNc_MAX_DISTANCE:
31
self.SNNc_MAX_DISTANCE = distance
32
else:
33
self.SNNc_MAX_DISTANCE = SNNc_max_distance
34
35
self.network = None
36
self.create_network(connection_algorithm,
radial_distance_coverage)
37
38
def create_network(self, connection_algorithm,
radial_distance_coverage):
39
self.network = nx.DiGraph()
40
nodes_list = []
41
for neuron in np.arange(self.SNNc_neuron_count):
42
if neuron in self.input_coordinate_indices:
43
attributes = dict(type="i", location=self.
SNNc_coordinates[neuron, :])
44
attributes["neuron"] = None
45
nodes_list.append((neuron, attributes))
46
else:
47
attributes = dict(type="s", location=self.
SNNc_coordinates[neuron, :])
48
nodes_list.append((neuron, attributes))
49
self.network.add_nodes_from(nodes=nodes_list)
50
if connection_algorithm is "swc":
51
self.swc(radial_distance_coverage=
radial_distance_coverage)
52
self.network = nx.freeze(self.network)
53
54
def swc(self, radial_distance_coverage):
55
abs_radial_distance_coverage = radial_distance_coverage *
self.SNNc_MAX_DISTANCE
56
edge_list = []
C.3
Python Code for the Modular Implementation of the SNNc
247

57
points = self.SNNc_coordinates
58
points_tree = scipy.spatial.cKDTree(points)
59
input_neuron_indices = self.input_coordinate_indices
60
for query_index in np.arange(self.SNNc_neuron_count):
61
neighbour_indices = points_tree.query_ball_point(
points[query_index], abs_radial_distance_coverage)
62
neighbour_indices_without_input = [index for index in
neighbour_indices if index not in input_neuron_indices]
63
for index in neighbour_indices_without_input:
64
edge_list.append((query_index, index))
65
if (index, query_index) not in edge_list:
66
self.network.add_edge(query_index, index, w
=0.05)
67
68
def create_spiking_neurons(self, neuron_type=’lif’, **
parameters):
69
"""
70
71
Parameters
72
----------
73
neuron_type:str
74
current options: ’LIF’(Leaky integrate and fire).
75
potential_threshold: float
76
’lif’ parameter(optional, default=1.0). The firing
threshold potential
77
refractory_threshold: int
78
’lif’ parameter(optional, default=1.0). The number of
discrete times
79
the neuron stays quiet after it fires a spike
80
leak_rate: float
81
’lif’ parameter(optional, default=0.000). The absolute
leak of
82
potential when the neuron is refractory
C.3
Python Code for the Modular Implementation of the SNNc
248

83
potential_resting: float
84
’lif’ parameter(optional, default=0.0). The resting
potential of the neuron.
85
spike_history_length:int
86
’lif’ parameter(optional, default=10. The historical
spike information that
87
the neuron remembers and calculates the states upon
88
89
90
Returns
91
-------
92
None
93
94
"""
95
if neuron_type not in ("lif"):
96
raise ValueError("See docstring for possible values of
neuron_type!!")
97
if neuron_type is "lif":
98
for neuron_index in self.spiking_coordinate_indices:
99
predecessor_count = len(self.network.predecessors(
neuron_index))
100
self.network.node[neuron_index]["neuron"] =
LeakyIntegrateAndFireNeuron(
101
predecessor_count=predecessor_count, **parameters)
102
103
def learn_stdp(self, sample, learning_rate):
104
"""
105
106
Parameters
107
----------
108
sample: numpy.ndarray
109
input samples consisting of spike data(time x feature)
110
input_learns: boolean(optional, default:True)
C.3
Python Code for the Modular Implementation of the SNNc
249

111
Input connections weights are not changed during learning
if set to False
112
learning_rate: float(optional, default:0.5)
113
The learning rate of the learning algorithm
114
115
Returns
116
-------
117
numpy.ndarray
118
The array of spikes generated by the SNNc for the given
input sample. The dimension of the array is
119
time x SNNc neuron count
120
121
122
"""
123
124
assert isinstance(sample, np.ndarray)
125
126
input_data = sample
127
(simulation_time, _) = input_data.shape
128
spike_data = np.zeros(shape=(simulation_time, self.
SNNc_neuron_count), dtype=int)
129
spike_data[:, self.input_coordinate_indices] = input_data
130
131
for time in np.arange(simulation_time):
132
connections_of_neuron_receiving_spike = []
133
connections_of_neuron_emitting_spike = []
134
"""
135
block for network simulation using spike information
present in spike data
136
"""
137
for neuron_id in self.spiking_coordinate_indices:
138
predecessor_indices = self.network.predecessors(
neuron_id)
C.3
Python Code for the Modular Implementation of the SNNc
250

139
predecessor_spike_data = spike_data[time,
predecessor_indices]
140
141
"""
142
Block to compute values to be used during learning
143
"""
144
s_ind = np.nonzero(spike_data[time,
predecessor_indices])[0]
145
predecessor_spike_indices = []
146
if len(s_ind) != 0:
147
predecessor_spike_indices = list(np.array(
predecessor_indices)[s_ind])
148
for identifier in predecessor_spike_indices:
149
connections_of_neuron_receiving_spike.append((
identifier, neuron_id))
150
predecessor_weight_data = []
151
for k in self.network.predecessors(neuron_id):
152
predecessor_weight_data.append(self.network[k][
neuron_id]["w"])
153
spk = self.network.node[neuron_id]["neuron"].
simulate(spike_train=predecessor_spike_dat,
154
weight=
predecessor_weight_data)
155
"""
156
Block to compute values to be used during learning
157
"""
158
if spk == 1:
159
predecessor_indices = self.network.predecessors(
neuron_id)
160
for identifier in predecessor_indices:
161
connections_of_neuron_emitting_spike.append((
identifier, neuron_id))
162
C.3
Python Code for the Modular Implementation of the SNNc
251

163
if time < simulation_time - 1:
164
spike_data[time + 1, neuron_id] = spk
165
"""
166
block for learning
167
"""
168
for connection in
connections_of_neuron_receiving_spike:
169
pre_spike = spike_data[0:time + 1, connection[0]]
170
post_spike = spike_data[0:time + 1, connection[1]]
171
deltaw = self.stdp(pre_synaptic_spike_history=np.
abs(pre_spike).tolist(),
172
post_synaptic_spike_history=np.abs(
post_spike).tolist(),
173
max_weight_update=learning_rate)
174
self.network[connection[0]][connection[1]][’w’] +=
deltaw
175
for connection in connections_of_neuron_emitting_spike
:
176
pre_spike = spike_data[0:time + 2, connection[0]]
177
post_spike = spike_data[0:time + 2, connection[1]]
178
deltaw = self.stdp(pre_synaptic_spike_history=np.
abs(pre_spike).tolist(),
179
post_synaptic_spike_history=np.abs(
post_spike).tolist(),
180
max_weight_update=learning_rate)
181
self.network[connection[0]][connection[1]][’w’] +=
deltaw
182
183
return spike_data
184
185
@staticmethod
186
def stdp(pre_synaptic_spike_history,
post_synaptic_spike_history, max_weight_update):
C.3
Python Code for the Modular Implementation of the SNNc
252

187
"""
188
189
Parameters
190
----------
191
pre_synaptic_spike_history: list
192
It is the binary vector of size n specifying the
presynaptic spike history of the neuron.
193
The last element signifies a spike/no spike at current
timepoint t, second last element
194
signifies a spike/no spike at t-1 and so on.
195
196
post_synaptic_spike_history: list
197
It is a binary vector of size n specifying the post
synaptic spike history.
198
The last element signifies a spike/no spike at current
timepoint t, second
199
last element signifies a spike/no spike at t-1 and so on.
200
201
max_weight_update: float
202
hyperparameter to control the upper bound of deltaw.
203
204
205
Returns
206
-------
207
float
208
209
"""
210
211
assert isinstance(pre_synaptic_spike_history, list)
212
assert isinstance(post_synaptic_spike_history, list)
213
if not 0 < max_weight_update <= 1:
214
raise ValueError("See docstring for value range of
learning_rate")
C.3
Python Code for the Modular Implementation of the SNNc
253

215
if len(pre_synaptic_spike_history) != len(
post_synaptic_spike_history):
216
raise ValueError("Length mismatch of pre and post
synaptic spike history!!")
217
pre_synaptic_spike_energy = 0
218
post_synaptic_spike_energy = 0
219
importance_of_LTP = 0.3
220
importance_of_LTD = 0.3
221
time_history = len(pre_synaptic_spike_history)
222
pre_synaptic_spike_history = np.asarray(
pre_synaptic_spike_history)
223
post_synaptic_spike_history = np.asarray(
post_synaptic_spike_history)
224
"""
225
calculation of LTD
226
"""
227
if pre_synaptic_spike_history[time_history - 1] == 1:
228
post_spike_indices = np.nonzero(
post_synaptic_spike_history)[0]
229
k = max_weight_update * np.exp(
230
-(1 - importance_of_LTD) * ((time_history - 1) -
post_spike_indices))
231
pre_synaptic_spike_energy = np.sum(k)
232
233
"""
234
calculation of LTP
235
"""
236
if post_synaptic_spike_history[time_history - 1] == 1:
237
pre_spike_indices = np.nonzero(
pre_synaptic_spike_history)[0]
238
k = max_weight_update * np.exp(
239
-(1 - importance_of_LTP) * ((time_history - 1) -
pre_spike_indices))
C.3
Python Code for the Modular Implementation of the SNNc
254

240
post_synaptic_spike_energy = np.sum(k)
241
242
deltaw = post_synaptic_spike_energy -
pre_synaptic_spike_energy
243
return deltaw


code snippet C.3: Python code for modular implementation of SNNc
C.3
Python Code for the Modular Implementation of the SNNc
255

D
Third Party Copyright
Materials
256

Figure D.1.: Copyright permission of Sengupta and Kasabov (2017)
Figure D.2.: Copyright permission of Sengupta, McNabb, Kasabov and Russell (2018)
257

Figure D.3.: Copyright permission of N. Kasabov, Scott et al. (2016)
Figure D.4.: Copyright permission of Abbott, Sengupta and Kasabov (2016)
258

Figure D.5.: Copyright permission of N. Kasabov, Sengupta and Scott (2016)
259

Figure D.6.: Copyright permission of Sengupta, Scott and Kasabov (2015)
260

Figure D.7.: Copyright permission of Arya, Vadlamani, Valadi, Sengupta and Kasabov
(2016)
Figure D.8.: Copyright permission of Widrow and Lehr (1990)
261

Figure D.9.: Copyright permission of Cortes and Vapnik (1995)
262

Figure D.10.: Copyright permission of Izhikevich (2004)
263

Figure D.11.: Copyright permission of Gerstner, Kistler, Naud and Paninski (2014)
264

E
Ethical Approval of TRS
Study
265

266

267

268

