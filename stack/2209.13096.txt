BayesNetCNN: incorporating uncertainty in
neural networks for image-based classiﬁcation
tasks
Matteo Ferrante1, Tommaso Boccato1, and Nicola Toschi1,2
1 Department of Biomedicine and Prevention, University of Rome Tor Vergata (IT)
2 Martinos Center For Biomedical Imaging, MGH and Harvard Medical School (USA)
3
{matteo.ferrante,tommaso.boccato nicola.toschi@uniroma2.it
Abstract. The willingness to trust predictions formulated by automatic
algorithms is key in a vast number of domains. However, a vast number
of deep architectures are only able to formulate predictions without an
associated uncertainty. In this paper, we propose a method to convert
a standard neural network into a Bayesian neural network and estimate
the variability of predictions by sampling diﬀerent networks similar to
the original one at each forward pass. We couple our methods with a
tunable rejection-based approach that employs only the fraction of the
dataset that the model is able to classify with an uncertainty below a
user-set threshold. We test our model in a large cohort of brain images
from Alzheimer’s Disease patients, where we tackle discrimination of
patients from healthy controls based on morphometric images only. We
demonstrate how combining the estimated uncertainty with a rejection-
based approach increases classiﬁcation accuracy from 0.86 to 0.95 while
retaining 75% of the test set. In addition, the model can select cases to
be recommended for manual evaluation based on excessive uncertainty.
We believe that being able to estimate the uncertainty of a prediction,
along with tools that can modulate the behavior of the network to a
degree of conﬁdence that the user is informed about (and comfortable
with) can represent a crucial step in the direction of user compliance and
easier integration of deep learning tools into everyday tasks currently
performed by human operators.
Keywords: Bayesian Neural Networks · Alzheimer · Morphometric ·
Explainability · Trust in AI
1
Introduction
The willingness to trust predictions formulated by automatic algorithms is key
in a vast number of domains. In addition to questions of ethics and responsi-
bility, it is important to note that, whilst extremely powerful, a vast number
of deep architectures are only able to formulate predictions without an associ-
ated uncertainty. This shortcoming critically reduces user compliance even when
arXiv:2209.13096v1  [eess.IV]  27 Sep 2022

2
Ferrante et al.
explainability techniques are used, and this issue is particularly sensitive when
deep learning techniques are employed e.g. in the medical diagnosis ﬁeld. Being
able to produce a measure of system conﬁdence in its prediction can really im-
prove the trustability in the deep learning tool as a recommendation machine
able to improve the workﬂow of physicians. Alzheimer’s disease (AD) is one
of the most critical public health concerns of our time. Due to life expectancy
increases and better professional care more and more people reach older ages
but are often aﬀected by degenerative brain disorders like AD, which is a se-
vere form of dementia [12]. Principal symptoms are progressive memory loss,
diﬃculties in normal-life activities, language disorders, disorientation, and, in
general, a decrease in cognitive functions. One of the most important risk fac-
tors is age, while in some cases speciﬁc genetic mutations are responsible for
pathology onset, which however can also be related to comorbidities. As a pro-
gressive degenerative pathology, AD is usually preceded by a diﬀerent condition
called mild cognitive impairment (MCI), with less intense symptoms that often,
but not always, evolve into AD, which has no cure. Many theories about the
etiopathogenesis of AD exist, several of which are linked to an alteration in the
metabolism of the precursor protein of beta-amyloid. The latter’s metabolism
slowly changes over the course of the years, leading to the formation of neuro-
toxic substances which slowly accumulate in the brain. The causal relationship
between beta-myeloid metabolism and clinical AD presentation is the object of
intense research [19,22,6,5]. In clinical practice, AD diagnosis is based on the
symptoms and commonly conﬁrmed using magnetic resonance imaging (MRI)
or positron emission tomography (PET), which however leaves the clinician with
a great deal of subjectivity and uncertainty to deal with when positioning a pa-
tient in the AD continuum. For this reason, there is great interest in models
able to detect and predict AD-related structural and functional changes. Deep
learning models are able to usefully extract local and global features through
convolutional layers and learn how to predict interesting outcomes, such as dis-
tinguishing healthy controls from AD patients or even MCI patients which will
remain stable for those who will progress to AD [21,19,18,9]. In this context,
diﬃculties in accessing large-scale curated datasets and the need to work with
multimodal high-dimensional data, call for particular attention to avoiding over-
ﬁtting and increasing the reliability of automatic models, possibly including the
output of uncertainty estimates which can be evaluated by neuroscientists and
physicians. For those reasons, we propose a Hybrid Bayesian Neural Network in
a framework where predicted probabilities are coupled with their uncertainties.
To reduce the number of parameters, we propose a convolutional neural network
based on depthwise separable convolutions. We trained our model on a subset of
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset using Jacobian
Determinant images, that is, images where each voxel describes the change in
the volume element resulting from nonlinear coregistration of the patient’s brain
MRI into a standard space such as e.g. the Montreal Neurological Institute (MNI)
T1-weighted template. This choice was made in order to isolate morphometric
changes (such as e.g. cortical atrophy) from image intensity variations [8]. Once

Bayesian NN for Alzheimer prediction on JD images
3
trained, we turned the last linear layer into a Bayesian neural network, replac-
ing optimal values w∗with narrow parameter distribution N(w∗, s). This means
that instead of having a single weight value for each connection between two
neurons from last layer to ﬁnal output, a Gaussian distribution centered on the
optimal value w∗is used. Every time the network does an inference the weights
of the last layer are sampled from those distributions. In this way is possible to
obtain N slightly diﬀerent networks which, in turn, allows to perform ensembling
and hence provide an uncertainty estimate. The latter can also be thresholded
in order to subset the data and increases prediction performance.
2
Material and methods
Fig. 1. Architecture overview: Our model is a classiﬁer based on residual convolutional
blocks. Each block is composed of two depthwise separable convolutional layers to keep
the number of parameters as low as possible to process 3D images. In correspondence
with the gaussian distribution symbol we sample the network classiﬁer weights w ∼
N(w∗, s).
In this section, we describe the dataset and brieﬂy revisit the theory behind
Bayesian neural networks that justiﬁes our approach.
2.1
Dataset
We selected a subset of 376 cases from the ADNI [17] dataset, composed of
cases labeled as both healthy and AD and employed the Magnetization Pre-
pared RApid Gradient Echo (MPRAGE), T1-weighted image only.T1-weighted
(T1w) images were coregistered to the MNI template using linear initialization

4
Ferrante et al.
and a nonlinear warp, after which the Jacobian Determinant (JD) maps were
computed by isolating the nonlinear part of the deformational ﬁeld which takes
the images from native space to standard space. We ﬁnally masked the defor-
mation maps using the standard MNI brain mask. Registration procedures were
performed using the ANTs package [1]. The high-dimensional nonlinear trans-
formation (symmetric diﬀeomorphic normalization transformation) model was
initialized through a generic linear transformation that consisted of the center
of mass alignment, rigid, similarity, and fully aﬃne transformations followed by
nonlinear warps (metric: neighborhood cross-correlation, sampling: regular, gra-
dient step size: 0.12, four multiresolution levels, smoothing sigmas: 3, 2, 1, 0
voxels in the reference image space, shrink factors: 6, 4, 2, 1 voxels. We also used
histogram matching of images before registration and data winsorization with
quantiles 0.001 and 0.999. The convergence criterion was set to be as follows:
the slope of the normalized energy proﬁle over the last 10 iterations ¡ 10-8).
Coregistration of all scans required approximately 19200 hours of CPU time on
a high-performance parallel computing cluster. Our ﬁnal dataset consisted of
376 JD images, evenly distributed between AD and healthy cases. The dataset
was split in an 80(train)/20(test) fashion, normalized globally, and cropped to a
(96,96,96) size.
Fig. 2. Example of slices for one random case in the test set. Local Jacobian Determi-
nant images are normalized in the range [0,1].

Bayesian NN for Alzheimer prediction on JD images
5
2.2
Bayesian Neural Network
We brieﬂy recap the theory behind Bayesian neural networks and then describe
the architecture of our model and the training procedure.
The idea is that instead of estimating w∗which minimizes the cost func-
tion, we learn a weight distribution. This is equivalent to an inﬁnite ensemble
approach, which allows us to estimate the variance of the prediction, sampling
a slightly diﬀerent neural network each time we perform inference. Instead of
learning w∗we learn the posterior p(w|D), where D represents the incoming
data. Our aim here is to perform inference as the average of diﬀerent neural
networks as described in Equation 1.
p(ˆy|D) =
Z
w
p(y|w)p(w|D)dw = Ep(w|D)(p(y|w))
(1)
Where p(ˆy|D) is probability of get the prediction y given the data D, p(y|w)
the conditioned probability of get y given network’s weights w and p(w|D) the
posterior probability of having weight w given the data D.
To perform this computation we need the posterior p(w|D) which can be
rewritten using the Bayes theorem. So p(w|D) can be expressed by the likeli-
hood p(D|w) and the prior p(w) but we also need the normalization term at
the denominator which is computationally intractable. At this point, several
approaches exist to overcome this issue. One popular approach is variational
inference, trying to estimate p(w|D) approximating this distribution with a
parametrized distribution qφ(z) that minimizes the Kullback-Leibler (KL) di-
vergence with the target distribution. Monte Carlo approaches are also possible,
sampling points matching the required distribution as described in [2,13,11,4].
This latter approach is anyway computationally intensive while using varia-
tional inference requires changes in the objective function that accomplish a new
task described by a modiﬁcation in the loss function. In this case, the standard
loss function is augmented with the KL divergence between the distribution of
the weights and the chosen prior which can make training unstable and longer.
Instead, we opted for a hybrid approach, ﬁrst training a standard convo-
lutional network and then turning the last layer weights into narrow Gaussian
distributions centered with the optimal values w∗. Assuming that optimal values
can serve as the center of Gaussian Distribution and little deviation around these
optimal values can represent similar networks this approach turns a standard
neural network into a Bayesian one without the need for any added complexity
during training time.
p(w|D) =
p(D|w)p(w)
R
w′ p(D|w′)p(w′)dw′
(2)
2.3
Neural network architecture
Our base model is a residual convolutional neural network based on depthwise
separable convolutions, which we implemented to reduce the number of param-
eters and the risk of overﬁtting. 3D depthwise separable convolutions are based

6
Ferrante et al.
on an hoc PyTorch implementation, using grouped convolutions with the group
number set to the same value as the number of input channels, followed by a
pointwise convolution with output channels. In other words, convolutions are
ﬁrst learned channelwise, and then information about the interaction between
channels is taken into account by the second depthwise convolution for each
point. This reduces the number of parameters from COK3 to C(K3 + O). Here
C is the number of input channels, K is the 3D kernel size, O is the number of
output channels. Our model is composed of three residual blocks, and each block
is composed of two depthwise separable convolutions with a PReLU activation
function [7]. Each block halves the side dimension of the images. A ﬂattening
layer is followed by a linear layer for the ﬁrst part of the training and then
turned into a Bayesian linear layer replacing optimal values with Gaussian dis-
tributions. We used the Adam optimizer [10] with a learning rate of 3e −4 and
trained our model for 5 epochs. All implementation was built in python, using
Pytorch, Monai [3] and Torchbnn libraries [13].
After training the last layer weights were replaced by a set of Gaussians w∗→
N(w∗, s) with s chosen to be small. We set s = 0.01 in our experiments. At each
forward pass, the network processes information the standard way until reaching
the last layer. Here, a set of weights is sampled from ∼N(w∗, s) generating a
slightly diﬀerent neural network. Sampling N networks in inference produce
diﬀerent estimation and let us the chance to estimate uncertainty about the
output.
2.4
Experiment
Our model is trained to classify AD and healthy cases on JD images. We ran in-
ference on our test set with N = 100, each time sampling the weights from their
distributions. We computed the softmax of the output to obtain the probabilities
and aggregate the results by means. We also computed the standard deviation
for each outcome probability. Then, we set a set of thresholds on standard devi-
ation to study the variation of performances. Each estimation with a standard
deviation is rejected. The idea is that we can set the threshold according to our
needs. If we need higher accuracy and avoid uncertain estimation we can set a
small value for the threshold t. In this case, we’ll have fewer data accepted for es-
timation and more ”rejected” cases to be reviewed manually. On the other side,
we can retain most or all of the test dataset if we can accept more misclassiﬁed
cases.
Algorithm 1 describes the whole procedure.

Bayesian NN for Alzheimer prediction on JD images
7
Algorithm 1 Inference
input x, N JD images, number of inference
for i in range(N) do
Sample NN weights w ∼N(w∗, s)
Estimate output probabilities pi = fw(x)
Average prediction pmean =
1
N
P
i pi
Compute pstd
Algorithm 2 Reject procedure
input pmean, pstd for test set
keep=[]
for sample in test set do
if pstd < threshold then
keep.append(pmean)
else
Reject pmean
Evaluate keep
2.5
Explainability
In order to visualize the portion of the images which were weighed most by our
model, we used the trulens library [14] implementation of integrated gradients
in [20]. A baseline x0 image - usually a tensor of zeros - is generated and a set of
interpolated images are computed according to the formula xi = x0 + α(x −x0)
where x is the actual image that we are trying to explain, α is a set linearly
spaced of coeﬃcients in [0, 1]. All those images are passed to the network and
the gradients along the path to the chosen class are collected and integrated.
Then, we smoothed the images with a 3D Gaussian kernel with σ = 4 to reduce
noise in the procedure and keep the values above the 95 percentile to get a mask
for the most important regions. We repeated the procedure 10 times sampling
each time a slightly diﬀerent neural network and then averaging the attributions
masks.
3
Results
As a ﬁrst experiment, we tested the standard neural network. In this case, on the
100% of the test set we obtained an accuracy of 0.86, F1-score of 0.87, precision
0.86 and recall 0.86 with an AUC of 0.938.
Successively, our approach was tested as a function of t (i.e. the maximum
standard deviation accepted for the class with the highest probability, see above).
WE compured area under the receiver operating characteristic curve, and the
fraction of the retained test dataset for each threshold. In Fig 3, the results are

8
Ferrante et al.
reported as a function of the threshold value. We can clearly see two opposite
trends, where reducing the threshold the accuracy and AUC increase while the
fraction of the remaining test dataset naturally decreases since the model is
rejecting the predictions whose associated uncertainty exceeds the threshold.
In our use case, we observed the best results with a threshold of 0.002, which
retains 75% of the dataset and reaches an accuracy of 0.95 and AUC of 0.96.
Figure 4 shows the ﬁnal explainability masks generated by averaging integrated
gradients for randomly chosen AD and healthy cases in the test set. It appears
that the model focuses on diﬀerent areas of the lower brain and, in particular,
the ventricular spaces, whose deformation is known to be correlated with AD
[16,15].
ThresholdAccuracy
AUC
fraction
0.002
0.947
0.959
0.750
0.005
0.916
0.955
0.789
0.01
0.904
0.951
0.829
0.02
0.898
0.939
0.907
0.05
0.876
0.939
0.960
0.10
0.868
0.940
1.0
0.15
0.868
0.940,
1.0
0.2
0.868
0.940
1.0
Table 1. Results: Accuracy, AUC, and the fraction of the test dataset with uncertainty
behind the threshold.
4
Discussion
The willingness to trust predictions by automatic devices, especially those based
on black-box algorithms like neural networks, is critical in a vast number of ap-
plication domains, such as e.g. the medical ﬁeld. Ethics and responsibility pose
an upper bound on the contribution of such techniques in medical diagnosis,
screening, and triaging. We believe that being able to estimate the uncertainty
of a prediction, along with tools that can modulate the behavior of the network
to the degree of conﬁdence that the user is informed about (and comfortable
with) can represent a crucial step in this direction. Such features can improve
the translation from research to clinical predictive models. Rather than com-
pletely replacing humans in evaluation, AI can support extremely useful recom-
mendation systems and powerful tools to reduce workload in an eﬃcient way for
e.g. medical professionals. We proposed a method that turns a classical neural
network into a Bayesian neural network, hence endowing the model with the

Bayesian NN for Alzheimer prediction on JD images
9
Fig. 3. Results: In the left ﬁgure accuracy (blue), AUC (orange), and the fraction of
the test dataset (green) are shown as functions of the threshold. Right ﬁgure: AUC for
the standard model and a selected bayesian model with a threshold of 0.002.
Fig. 4. Results: Integrated Gradients. This is an interoperability method to look at the
most inﬂuent areas for prediction. In this case, the model focus is on the ventriculi of
the brain, an area which is involved in neurodegeneration.
ability to estimate the uncertainty associated with predictions. We also incor-
porate a rejection method based on a threshold based on thresholding the esti-
mated uncertainty, which has resulted in a global performance increase (which
amounts to reducing probably misclassiﬁed cases as they are associated with
higher uncertainty). Additionally, by exclusion, this system can select cases to

10
Ferrante et al.
be recommended for expert human evaluation when the uncertainty is above the
threshold.
5
Conclusion
We built a Bayesian-based neural network method able to estimate variability
in predictions by simulating sampling from an inﬁnite neural network ensemble.
We used the estimated variability combined with a rejection method to retain
only the fraction of the dataset that the model is able to classify with an under-
threshold uncertainty, and showed that this procedure can improve the accuracy
from 0.86 to 0.95 (while retaining 75% of the test) when discriminating for AD
from healthy cases based on brain morphometry only. Using integrated gradients,
we also found that our model focuses on brain areas that are consistent with the
clinical presentation of AD, in addition to highlighting previously unexplored
areas in the lower part of the brain.
Acknowledgements
Part of this work is supported by the EXPERIENCE project (European Union’s Hori-
zon 2020 research and innovation program under grant agreement No. 101017727)
Matteo Ferrante is a PhD student enrolled in the National PhD in Artiﬁcial In-
telligence, XXXVII cycle, course on Health and life sciences, organized by Universit`a
Campus Bio-Medico di Roma.
References
1. Avants, B., Tustison, N., Song, G.: Advanced normalization tools (ants). Insight J
1–35 (11 2008). https://doi.org/10.54294/uvnhin
2. Blundell, C., Cornebise, J., Kavukcuoglu, K., Wierstra, D.: Weight uncertainty
in neural networks (2015). https://doi.org/10.48550/ARXIV.1505.05424, https:
//arxiv.org/abs/1505.05424
3. Consortium,
T.M.:
Project
monai
(Dec
2020).
https://doi.org/10.5281/zenodo.4323059,
https://doi.org/10.5281/zenodo.
4323059
4. Gal,
Y.,
Ghahramani,
Z.:
Dropout
as
a
bayesian
approxima-
tion:
Representing
model
uncertainty
in
deep
learning
(2015).
https://doi.org/10.48550/ARXIV.1506.02142,
https://arxiv.org/abs/1506.
02142
5. Hampel, H., Caraci, F., Cuello, A.C., Caruso, G., Nistic`o, R., Corbo, M., Baldacci,
F., Toschi, N., Garaci, F., Chiesa, P.A., Verdooner, S.R., Akman-Anderson, L.,
Hern´andez, F., ´Avila, J., Emanuele, E., Valenzuela, P.L., Luc´ıa, A., Watling, M.,
Imbimbo, B.P., Vergallo, A., Lista, S.: A path toward precision medicine for neu-
roinﬂammatory mechanisms in alzheimer’s disease. Front. Immunol. 11, 456 (Mar
2020)

Bayesian NN for Alzheimer prediction on JD images
11
6. Hampel, H., Toschi, N., Babiloni, C., Baldacci, F., Black, K.L., Bokde, A.L.W.,
Bun, R.S., Cacciola, F., Cavedo, E., Chiesa, P.A., Colliot, O., Coman, C.M.,
Dubois, B., Duggento, A., Durrleman, S., Ferretti, M.T., George, N., Genthon,
R., Habert, M.O., Herholz, K., Koronyo, Y., Koronyo-Hamaoui, M., Lamari, F.,
Langevin, T., Leh´ericy, S., Lorenceau, J., Neri, C., Nistic`o, R., Nyasse-Messene, F.,
Ritchie, C., Rossi, S., Santarnecchi, E., Sporns, O., Verdooner, S.R., Vergallo, A.,
Villain, N., Younesi, E., Garaci, F., Lista, S., Alzheimer Precision Medicine Initia-
tive (APMI): Revolution of alzheimer precision neurology. passageway of systems
biology and neurophysiology. J. Alzheimers. Dis. 64(s1), S47–S105 (2018)
7. He,
K.,
Zhang,
X.,
Ren,
S.,
Sun,
J.:
Delving
deep
into
rectiﬁers:
Sur-
passing
human-level
performance
on
imagenet
classiﬁcation
(2015).
https://doi.org/10.48550/ARXIV.1502.01852,
https://arxiv.org/abs/1502.
01852
8. Hua, X., Leow, A.D., Parikshak, N., Lee, S., Chiang, M.C., Toga, A.W., Jack,
Jr, C.R., Weiner, M.W., Thompson, P.M., Alzheimer’s Disease Neuroimaging Ini-
tiative: Tensor-based morphometry as a neuroimaging biomarker for alzheimer’s
disease: an MRI study of 676 AD, MCI, and normal subjects. Neuroimage 43(3),
458–469 (Nov 2008)
9. Jo, T., Nho, K., Saykin, A.J.: Deep learning in alzheimer’s disease: Diagnos-
tic classiﬁcation and prognostic prediction using neuroimaging data. Frontiers in
Aging Neuroscience 11 (2019). https://doi.org/10.3389/fnagi.2019.00220, https:
//www.frontiersin.org/articles/10.3389/fnagi.2019.00220
10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2014).
https://doi.org/10.48550/ARXIV.1412.6980, https://arxiv.org/abs/1412.6980
11. Kingma,
D.P.,
Welling,
M.:
Auto-encoding
variational
bayes
(2013).
https://doi.org/10.48550/ARXIV.1312.6114, https://arxiv.org/abs/1312.6114
12. Knopman, D.S., Amieva, H., Petersen, R.C., Ch´etelat, G., Holtzman, D.M., Hy-
man, B.T., Nixon, R.A., Jones, D.T.: Alzheimer disease. Nature Reviews Dis-
ease Primers 7(1),
33 (May 2021). https://doi.org/10.1038/s41572-021-00269-y,
https://doi.org/10.1038/s41572-021-00269-y
13. Lee, S., Kim, H., Lee, J.: Graddiv: Adversarial robustness of randomized neu-
ral networks via gradient diversity regularization. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2022)
14. Leino,
K.,
Sen,
S.,
Datta,
A.,
Fredrikson,
M.,
Li,
L.:
Inﬂuence-
directed
explanations
for
deep
convolutional
networks
(2018).
https://doi.org/10.48550/ARXIV.1802.03788,
https://arxiv.org/abs/1802.
03788
15. Nestor, S.M., Rupsingh, R., Borrie, M., Smith, M., Accomazzi, V., Wells, J.L.,
Fogarty, J., Bartha, R., Alzheimer’s Disease Neuroimaging Initiative: Ventricular
enlargement as a possible measure of alzheimer’s disease progression validated
using the alzheimer’s disease neuroimaging initiative database. Brain 131(Pt 9),
2443–2454 (Sep 2008)
16. Ott, B.R., Cohen, R.A., Gongvatana, A., Okonkwo, O.C., Johanson, C.E., Stopa,
E.G., Donahue, J.E., Silverberg, G.D., Alzheimer’s Disease Neuroimaging Initia-
tive: Brain ventricular volume and cerebrospinal ﬂuid biomarkers of alzheimer’s
disease. J. Alzheimers. Dis. 20(2), 647–657 (2010)
17. Petersen, R.C., Aisen, P.S., Beckett, L.A., Donohue, M.C., Gamst, A.C., Harvey,
D.J., Jack, Jr, C.R., Jagust, W.J., Shaw, L.M., Toga, A.W., Trojanowski, J.Q.,
Weiner, M.W.: Alzheimer’s disease neuroimaging initiative (ADNI): clinical char-
acterization. Neurology 74(3), 201–209 (Jan 2010)

12
Ferrante et al.
18. Sethi, M., Rani, S., Singh, A., Maz´on, J.L.V.: A CAD system for alzheimer’s disease
classiﬁcation using neuroimaging MRI 2D slices. Comput. Math. Methods Med.
2022, 8680737 (Aug 2022)
19. Spasov, S., Passamonti, L., Duggento, A., Li`o, P., Toschi, N., Alzheimer’s Disease
Neuroimaging Initiative: A parameter-eﬃcient deep learning approach to predict
conversion from mild cognitive impairment to alzheimer’s disease. Neuroimage 189,
276–287 (Apr 2019)
20. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep net-
works.
https://doi.org/10.48550/ARXIV.1703.01365,
https://arxiv.org/abs/
1703.01365
21. Termine, A., Fabrizio, C., Caltagirone, C., Petrosini, L., On Behalf Of The Fron-
totemporal Lobar Degeneration Neuroimaging Initiative: A reproducible Deep-
Learning-based Computer-Aided diagnosis tool for frontotemporal dementia using
MONAI and clinica frameworks. Life (Basel) 12(7), 947 (Jun 2022)
22. Toschi, N., Lista, S., Baldacci, F., Cavedo, E., Zetterberg, H., Blennow, K., Kili-
mann, I., Teipel, S.J., Melo Dos Santos, A., Epelbaum, S., Lamari, F., Genthon,
R., Habert, M.O., Dubois, B., Floris, R., Garaci, F., Vergallo, A., Hampel, H.,
INSIGHT-preAD study group, Alzheimer Precision Medicine Initiative (APMI):
Biomarker-guided clustering of alzheimer’s disease clinical syndromes. Neurobiol.
Aging 83, 42–53 (Nov 2019)

