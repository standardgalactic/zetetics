1
Distribution Matching for Machine Teaching
Xiaofeng Cao and Ivor W. Tsang
Australian Artiﬁcial Intelligence Institute, University of Technology Sydney, Australia.
Email: xiaofeng.cao@uts.edu.au, ivor.tsang@uts.edu.au
You may need to know the following questions before reading our manuscript.
Q1: What is machine teaching?
Machine teaching is an inverse problem of machine learning that aims at steering the
student learner towards its target hypothesis, in which the teacher has already known
the student’s learning parameters. In simple terms, machine teaching can help to ﬁnd
the best training data for student learners automatically.
Q2: What is our research problem?
Previous machine teaching studies on machine teaching focused on balancing the teach-
ing risk and cost to ﬁnd those best teaching examples deriving the student model. This
optimization solver is in general ineffective when the student learner does not disclose
any cue (i.e. a black-box) of the learning parameters. To supervise such a teaching
scenario, this paper presents a distribution matching-based machine teaching strategy.
Q3: Why it is signiﬁcant?
Our study will help machine learning to ﬁnd the best teaching examples on a black-
box setting, then the training examples will be closed-form. In this paper, we present a
distribution matching perspective to resolve this issue from optimization.
Q4: What is our novelty?
Technically, our strategy can be expressed as a cost-controlled optimization process
that ﬁnds the optimal teaching examples without further exploring in the parameter
distribution of the student learner.
Q5: What is the related study?
The most relevant machine learning study to machine teaching is active learning. Its key
assumption is that a student learner who frequently interacts with a teacher (annotator)
would do better or no worse than other passive student learners who randomly solicit
the training examples. Generally, active learning forwardly updates the current training
model into its target, while the target is always agnostic.
arXiv:2105.13809v1  [cs.LG]  6 May 2021

Distribution Matching for Machine Teaching
Xiaofeng Cao and Ivor W. Tsang
Australian Artiﬁcial Intelligence Institute, University of Technology Sydney, Australia.
Email: xiaofeng.cao@uts.edu.au, ivor.tsang@uts.edu.au
Abstract
Machine teaching is an inverse problem of machine learning that aims at steer-
ing the student learner towards its target hypothesis, in which the teacher has
already known the student’s learning parameters. Previous studies on machine
teaching focused on balancing the teaching risk and cost to ﬁnd those best teach-
ing examples deriving the student model. This optimization solver is in general
ineffective when the student learner does not disclose any cue of the learning pa-
rameters. To supervise such a teaching scenario, this paper presents a distribution
matching-based machine teaching strategy. Speciﬁcally, this strategy backwardly
and iteratively performs the halving operation on the teaching cost to ﬁnd a desired
teaching set. Technically, our strategy can be expressed as a cost-controlled opti-
mization process that ﬁnds the optimal teaching examples without further explor-
ing in the parameter distribution of the student learner. Then, given any a limited
teaching cost, the training examples will be closed-form. Theoretical analysis and
experiment results demonstrate this strategy.
Keywords: Machine teaching, teaching risk, teaching cost, learning parameters, surro-
gate.
1
Introduction
Machine learning (Mitchell et al., 1997) is the study of artiﬁcial intelligent algorithms
that improve automatically through model construction and parameter experience. Highly
informative or representative training examples accelerate the convergence of the learn-
ing model. However, how to control the machine learning paradigm if there is a teacher
who has already known the learning parameters of the student and wants better training
examples to improve its generalization? This inverse question of machine learning was
studied by machine teaching (X. Zhu et al., 2018), which explores the optimal train-
ing data via driving the student learner to its target hypothesis. It has been shown many
promising paradigms ranging from a teaching scheme to a student learner (W. Liu et al.,
2018) such as a curriculum to human education system in curriculum learning (Matiisen
et al., 2019), an iterative query algorithm to an annotator in active learning (Dasgupta
et al., 2019), etc. The key assumption is that the teacher knows the target parameters of
the student model e.g. a speciﬁed hyperplane in SVM classiﬁer, geometric properties
of clustering centers, etc. Typically, a machine teacher interacts with its student learner
by exploring those teaching examples that minimize the parameter disagreement (dif-
ference) of the current training model and its desired (Gao et al., 2017) (Shinohara &
Miyano, 1991).
2

0
1
Figure 1: Label complexity of a student learner. Given an error ϵ, the passive learner
needs O(1/ϵ) data, and the active learner needs O(log1/ϵ); the machine teacher only
needs two symmetrical examples a and b (black points) around θ∗to minimize the
parameter disagreements: ∥ˆθ−θ∗∥, where the target parameter ˆθ is denoted by ˆθ = a+b
2 ,
where ∥· ∥denotes an effective metric over the parameter space.
In simple terms, machine teaching can help to ﬁnd the best training data for student
learners automatically. With machine teaching, those teaching examples can be closed-
form to supervise the training model of a student learner. As far as we know, the most
relevant machine learning study to machine teaching is active learning (Dasgupta et
al., 2008). Its key assumption is that a student learner who frequently interacts with a
teacher (annotator) would do better or no worse than other passive student learners who
randomly solicit the training examples. Generally, active learning forwardly updates
the current training model into its target, while the target is always agnostic. We next
employ a threshold classiﬁer to further explain active learning and machine teaching
following the survey of (X. Zhu et al., 2018).
Considering an interval [-1,1] with positive and negative labels over a uniform dis-
tribution P (see Figure 1), the parameterized threshold classiﬁer at θ∗= 0 stipulates
the classiﬁcation function f for any (x, y) ∼P: y = f(x) = +1, s.t. x ≥θ∗,
y = f(x) = −1, s.t. x ≤θ∗, where y denotes the label of x. Drawing i.i.d. n sam-
ples from P, a passive (random) learner will make n times of querying from f(x) that
yields a generalization error of ∥θ −θ∗∥= O(1/n) due to an average uniform spac-
ing 1/n, where θ yields a passive learner. Given an error ϵ, a passive learner needs
O(1/ϵ) data sent to the classiﬁer. Speciﬁcally, the passive learner at least receives
10,000 samples from P to obtain a desired error 0.0001. For an active learner who
usually employs binary search, the learner halves the remaining interval and removes
the data over it, thereby receives around O(log1/ϵ) samples to obtain an error ϵ because
∥θ −θ∗∥= O(1/2n). Speciﬁcally, the active learner at least receives 13 samples from
P to obtain a desired error 0.0001. However, a machine teacher who knows θ∗only
needs two teaching examples to obtain ϵ: (θ∗−ϵ/2, −1), and (θ∗+ ϵ/2, +1). Then,
for any student learner, they will easily achieve better performance using those teaching
examples.
From a machine learning perspective, Eq. (1) ﬁrstly presents a general strategy on
active learning: optimizing a model parameter θ with current training data D0 ⊂D,
subsequently updated by queries ˆDq:
min
θ∈Θ R(θ, D0 ∪ˆDq) + Ω(θ),
s.t. ˆDq ∈argmax
Dq⊂D\D0
R(θ, D0) −R(θ, D0 ∪Dq)
,
(1)
3

where R(·, ·) denotes the empirical risk function, Ω(·) denotes the regularization con-
straint, D denotes the full training data, Dq denotes the queries from D\D0 using active
learning which maximizes the risk disagreement, and Θ denotes the parameter space.
With the minimizer of the regulated learning risk, model parameter θ derives the update
on D0 via merging ˆDq.
Given a teacher who has already known the learning parameters, machine teach-
ing inversely optimizes the training data by estimating parameter disagreements over
the generalization models. X. Zhu (2015) proposed a more general machine teaching
formula based on Eq. (1),
min
D⊂D L(ˆθ, θ∗) + ηM(D),
s.t. ˆθ ∈arg minθ∈ΘR(θ, D) + Ω(θ),
(2)
where D denotes the complete or whole search space of D that covers all candidate
training subsets, L(·, ·) w.r.t. parameter disagreement denotes the teaching risk, M(·)
denotes the teaching cost of the input set, and η denotes a balance coefﬁcient. Typically,
L(ˆθ, θ∗) can be simply deﬁned as the indication disagreement Ex∼P1(ˆθ(x)̸=θ∗(x)), where
1 denotes the indication function, and ˆθ denotes the desired parameter. Teaching cost
can be simply deﬁned as the cardinality of D: M(D) = ∥D∥0, in which M(D) = 2∥D∥0
(∥· ∥0 denotes L0 norm)1.
In this paper, we consider one challenging problem: how to teach a student learner
which does not disclose any cue (also termed as a black-box learner) from the distribu-
tion of the parameters, i.e. estimating L(ˆθ, θ∗) is inefﬁcient due to improper parameter
disagreement or inestimable parameter space e.g. huge amounts of parameters in a large
neural network. Our main proposal to solving Eq. (2) in such scenario is optimizing an
approximated distribution for teaching. Speciﬁcally, we 1) approximate R(θ, D) into a
surrogate R(θ, D′) by shrinking D into its surrogate D′ with smooth boundary, and 2)
then transform L(ˆθ, θ∗) as a distribution metric optimized in D′ that assumes D is w.r.t.
ˆθ and D is w.r.t. θ∗.
Our analysis based on importance sampling (Beygelzimer et al., 2008) shows that
approximating the teaching risk by its surrogate is still guaranteed safely to yield a
tighter learning bound on label complexity (number of sampled data to achieve a desired
error). To implement the optimization of distribution matching-based machine teach-
ing, we employ the hyperbolic metric (Ganea et al., 2018) that hierarchically ranks the
transformed teaching risks. A Poincar´e measure h(·, ·) (Sarkar, 2011) is then utilized
due to its effectiveness in scattering those ranked features (Tay et al., 2018) (Tran et al.,
2020)
L(ˆθ, θ∗) ≈h(D, D).
(3)
The main technical question of this paper is as follows: how to generalize Eq. (3)
with surrogate R(θ, D′). A distribution matching-based machine teaching algorithm
12∥D∥0 =
 ∥D∥0
1

+
 ∥D∥0
2

+ ... +
 ∥D∥0
∥D∥0

, where
 ∥D∥0
k

denotes the operation of selecting k teaching
examples from the full training set D.
4

that transfers the disagreement estimation of parameters into hypotheses, thereby ap-
proximating the hypothesis to distribution, is then presented by narrowing the assump-
tion of Eq. (3). Assume θ∗is generalized from the optimal hypothesis h∗, i.e. R(θ∗, D) =
R(h∗, D), ˆθ is generalized from the hypothesis ˆh, i.e. R(ˆθ, D) = R(ˆh, D), let D′ ⊂D
be the optimal surrogate with respect to h∗, let ˆD ⊂D be the desired training set with
respect to ˆh, with the proposal of Eq. (3), we further have
min
ˆθ∈Θ
∥ˆθ −θ∗∥= min
ˆh∈H
∥ˆh −h∗∥≈min
ˆD∈D ∥ˆD −D′∥h,
(4)
where ∥ˆD −D′∥h := h( ˆD, D′). In optimization of distribution matching-based ma-
chine teaching, minimizing ∥ˆD −D′∥h is solved by controlling the teaching cost, i.e.
backwardly and iteratively halving M(D′). Then, the ﬁnal update on D′ is deﬁned as
the teaching set ˆD.
Typical machine teaching mainly studies the optimal teaching set when the student
learner is a white-box whose underlying parameter distributions are known, such as lin-
ear learners (J. Liu & Zhu, 2016), sequential learner (Lessard et al., 2019), Bayesian
learners of the exponential family (J. Zhu, 2013), etc. Theoretical discussion was the
main trend in past decades. A complete teaching theory such as teaching dimension
and teaching complexity were proposed (Khan et al., 2011). This paper focuses more
on a black-box learner which further leads to ineffective estimation on parameter dis-
agreement for teaching risk. We thus simplify the introduction of theoretical progress,
and present related active learning work. Finally, we contribute this work from the
following highlights.
• We introduce the idea of distribution matching for machine teaching a student
learner who does not disclose any cue of the learning parameters, i.e. a black-
box.
• We present a cost-controlled optimization process that ﬁnds the optimal teaching
examples without further exploring in the parameter distribution.
• We presents a distribution matching-based machine teaching strategy by back-
wardly and iteratively performing the halving operation on the teaching cost.
The organization of this paper is outlined as follows. The related work is listed in
Section 2. Section 3 presents the main theoretical results. Section 4 presents distribu-
tion matching-based machine teaching. Section 5 presents experiments and Section 6
discusses the classiﬁer perturbations to the assumption of this work, followed by the
conclusion in Section 7.
2
Related Work
2.1
Machine Teaching
When the training set of a model consists of plenty data from an underlying distribution,
a teacher always desires to pick up some teaching examples from the original data set to
5

supervise its student learner (X. Zhu et al., 2018). In this teaching process, the teacher
can be generalized as a human expert, a learning algorithm or system. Two factors are
studied in the teaching process: how to ﬁx the size of the teaching set and whether the
teacher knows the full knowledge of the parameter distribution of the student model?
Early works answered the ﬁrst problem by studying the teaching complexity. Intro-
ducing a novel concept named teaching dimension (Khan et al., 2011), the complexity
of teaching is characterized by the minimum number of teaching examples must reveal
to uniquely identify any hypothesis chosen from the hypothesis class. If the teaching
examples are selected independently from the original data pool, the learner can coop-
erate with a teacher who supervises a teaching set. Recently, machine teaching (Mei &
Zhu, 2015) studied the teaching model when the learner was a convex minimizer, such
as least square regression, and simple support vector machine. In (W. Liu et al., 2018),
they presented a theoretical interpretation against multiple-teacher teaching rather than
a single teacher. Under the joint teaching for conjugate Bayesian learners, Zhu et al.
(J. Zhu, 2013) proposed a new concept called class teaching dimension. In this deﬁni-
tion, the teacher independently picks up some learners as a representation for the whole
class of learners. Indeed, it shrinks the original teaching dimension by the representa-
tion features of the learner class.
In real-world applications, a variety of learning tasks involved with machine teach-
ing were studied. For instance, in curriculum learning (Bengio et al., 2009), human can
intelligently annotate or recognize examples when the teaching examples are not ran-
domly presented but organized in a meaningful order. Those orders help the teachers to
optimize a group of teaching set for the subsequent learning tasks. To improve the gen-
eralization of an active learner, Dasgupta et al. (2019) proposed a black-box teaching
scheme to shrink training sets for any family of classiﬁer by merely serving up the rele-
vant examples beforehand, and does not need to observe the feedback from the learner,
where “shrink” can be deemed as a typical distribution matching-based machine teach-
ing strategy.
2.2
Active Learning
Active learning (Cohn et al., 1994) adopts the same sampling goal as machine teaching
to ﬁnd the optimal training data, but forwardly updates the models. In this task, the
learners are given access to interactively query the labels of a group of unlabeled data.
The learning goal is that the queries can substantially improve the performance of a
learning model within a given annotation budget. Theoretically, the learners always
try to maintain a version space (Beygelzimer et al., 2010) which covers a series of
candidate hypotheses and shrinks its size via querying as few as possible data. However,
the version space-based learning theory unfortunately has drawbacks of computational
intractability (Dasgupta et al., 2008), i.e., guaranteeing that only hypotheses from this
space are returned is intractable for nonlinear classiﬁers.
To develop a new strategy which addresses the above limitations, Langley (2006)
and Hanneke (2007) constructed learning algorithms to predict which data may signif-
icantly affect the subsequent hypothesis, thereby giving different weight coefﬁcients.
6

The convergence guarantees, adopted from a PAC-style2, is rigorous and tighter than
the generalized bounds of any supervised learning algorithms. The other technique,
termed as importance-weighted active learning (Beygelzimer et al., 2008), provides an
unbiased sampling approach with the loss-weighting and has more practical use in ob-
serving the error and label complexity change.
In practical tasks, traditional active learning methods such as pool-based AL (Tong
& Koller, 2001) samples the data that reduces the error rate in a descried change by
repeatedly visiting the unlabeled data pool. Usually, the learner is given access to the
hypothesis class easily, i.e. can favorably observe the hypothesis updates by estimating
the error disagreements (differences). However, when supervising a black-box learner,
obtaining precise details of the prior labels and list of classiﬁer parameters are not avail-
able, and only the queries on labels are accessible. This makes many of the traditional
strategies, which estimate the error disagreements, are not applicable, or at the least can
not work well (Rubens et al., 2011). To reduce the dependence on a single classiﬁer,
query by committee algorithm (Seung et al., 1992) uses a set of classiﬁers to evaluate
the error rate changes and selects the data which maximize the disagreement among the
committee members. However, estimating the error rate changes in settings of single or
multiple classiﬁers will cost expensively on time and space complexities.
3
Main Theoretical Results
The main purpose of our theoretical study is to reconstruct the original distribution by
its surrogate with a smooth boundary. Section 3.1 introduces importance sampling that
used for approximating R(θ, D) and Section 3.2 provides the safety guarantee for its
surrogate. Section 3.3 presents the label complexity bounds of minimizing R(θ, D′).
Section 3.4 generalizes the approximation of D to D′ in hyperbolic geometry. Sec-
tion 3.5 presents a case study of the approximation. Proof sketches of Theorems 1 and
2 are presented in Appendix.
3.1
Importance Sampling
Importance sampling (Beygelzimer et al., 2008) uses importance weighting to correct
sampling bias and rigorously observe the on-line error change for a machine learning
model. In this section, we ﬁnd a surrogate D′ to approximate R(θ, D) by employing
the importance sampling algorithm, which further eliminates the noisy perturbations
around the boundary of the distribution.
In importance sampling, the machine learning algorithm assigns an unlabeled data
xt ∈D with a probability pt to query its label yt. The underlying rule is: if xt is
selected for querying at t-time of sampling, its weight is set to 1
pt. Let f(·) denote the
mapping loss function from D to Y, given a classiﬁcation hypothesis h : D × Y →R,
where Y denotes the label space of D, let errT(h) be the expected error loss over D of
2PAC: Probably approximately correct. In computational learning theory, the learner must select a gen-
eralization function i.e. the hypothesis from a certain class of possible functions (also called hypothesis
class). The goal is that, with a high Probability, the selected function will have low generalization error
to be Approximately Correct.
7

a hypothesis h at query time T, the learning risk with T times of sampling from D is
deﬁned as
R(h, D, T) := errT(h) = 1
T
T
X
t=1
qt
pt
f(h(xt), yt),
(5)
where qt denotes a Bernoulli distribution with qt ∈{0, 1}, yt ∈Y, and it denotes
the true label of xt. Importance sampling uses the probability weights to eliminate
the sampling bias with rigorous label complexity bounds to accelerate the convergence
of the minimization on R(h, D), where label complexity denotes the number of the
sampled data to achieve a desired error. We next exploit the idea of importance sampling
to present the label complexity for the approximated surrogate and its safety guarantee.
3.2
Safety Guarantee for Surrogate
In importance sampling, learning in surrogate D′ can keep consistent properties for the
machine learning model but eliminates the noisy perturbations from the boundary of the
distribution i.e. its conceptual version space. Theoretically, a desired safety guarantee
(Beygelzimer et al., 2008) expects that the performance of a machine learning algorithm
keeps a provably consistency on its inherent optimal hypothesis.
Given an agnostic distribution P maintaining a training set D for sampling, we
deﬁne a subset D′ with a smooth boundary as its surrogate.
Deﬁnition 1. Surrogate D′ of D. Given a ﬁnite hypothesis class H with ﬁnite VC di-
mension 3 bound d that is uniquely associated with D. Let D′ be a surrogate of D and
H′ be the shrunk hypothesis class over D′. Assume that dH(D) and dH′(D′) are the
hypothesis diameters (maximum hypothesis disagreement) of H and H′, respectively,
for any probability δ, surrogate D′ is one subset from D with a smooth boundary that
satisﬁes
dH(D) −dH′(D′) ≤
r
2

In d + In2
δ

,
s.t. D′ ⊂D, ∥D′∥0 = n′ < ∥D∥0 = n,
dH(D) := max
(errD(h+, x+
t ) −errD(h−, x−
t )

H
)
,
dH′(D′) := max
(errD′(h+, x+
t ) −errD′(h−, x−
t )

H′
)
,
(6)
where errD(h+, x+
t ) denotes the error of a subsequent hypothesis h+ over D after
adding and annotating xt with a positive label, errD(h−, x−
t ) follows the annotation
assumption of a negative label, errD′(h+, x+
t ) and errD′(h−, x−
t ) also follow a surrogate
3Vapnik–Chervonenkis dimension. It is a measure of the capacity such as complexity of a space of
functions that can be learned by a classiﬁcation learning algorithm. In VC theory (Vapnik, 2013), the
VC bound is deﬁned as the cardinality of the largest set of input training data that a learning algorithm
can shatter.
8

D′, and xt is the sampled data from D at t-time.
Deﬁnition 2. Safety guarantee. Given Eθ∈Θ R(θ, D′) be the expected empirical risk
over surrogate D′, let d be the ﬁnite VC dimension bound that is uniquely associated
with D, for any probability δ, if
Pr
(
|R(θ, D) −Eθ∈ΘR(θ, D′)|≤
r
2

In d + In2
δ
)
≈1,
(7)
any machine learning model that minimizes R(θ, D) is guaranteed safely on minimizing
R(θ, D′).
Assumption 1. With importance sampling, assume that θ ∈Θ is respected to h ∈H,
recalling Eq. (4), approximating R(θ, D) into R(θ, D′) is equivalent to approximating
R(h, D) into R(h, D, T), where R(h, D, T) denotes error risk of T times of impor-
tance sampling w.r.t. Eq. (5) and R(h, D) = E(x,y)∼Df(h(x), y) without importance
sampling.
Theorem 1 observes the ground-truth risk disagreement and its expectation, where
the risk disagreement is over the full training data and its surrogate.
Theorem 1. With Assumption 1, given the training set D, for all ﬁnite hypothesis
class H with a VC dimension bound d, for any probability δ > 0 and pt > φ, if a
learning algorithm samples T times to obtain a surrogate of D, let R be the ground-
truth risk disagreement of the surrogate and its full training data that stipulates R =
|R(h, D, T) −R(h, D)|, ˆR be the expected risk disagreement that stipulates ˆR =
Eh∈H|R(h, D, T) −R(h, D)|, with Deﬁnition 2, the generalization probability bound
of achieving a safe surrogate is
Pr
(
|R −ˆR|≤
r
2

In d + In2
δ
)
≤exp
 
−4(In d + In2
δ)
Tφ−2
!
.
(8)
In brief, Theorem 1 shows that there exists nearly consistent hypothesis diameters
between the full training data and its surrogate, where the diameter of surrogate is over
its expectation.
Corollary 1.1. In Theorem 1, R and ˆR denote the maximum and expected risk dis-
agreement of the T times importance sampling and full training data, respectively. For
a given hypothesis class H which covers all feasible hypotheses, the maximum error
disagreement is close to the hypothesis diameter (Tosh & Dasgupta, 2017) of H. If
the expected hypothesis distance of a sub hypothesis class over D′ is close to it, we
say sampling in D′ yields consistency as sampling in D. Therefore, with Deﬁnition 1,
Theorem 1 has another equivalent form
Pr
(
|dH(D) −ED′⊂DdH′(D′)|≤
r
2

In d + In2
δ
)
≈1.
(9)
Speciﬁcally, the probability bound of Eq. (8) approximates 1. With Assumption 1,
approximating R(θ, D) into R(θ, D′) achieves safety guarantee for any θ ∈Θ over D′.
9

3.3
Label Complexity Bound for Minimizing R(θ, D′)
We follow (Langley, 2006) to present the label complexity of minimizing R(θ, D′).
Assumption 2. Let n denote the sample amount in D, its VC dimension bound d ap-
proximates to 2n. By using importance sampling, D′ is with a VC bound 2T.
With Assumption 2, an upper bound of the label complexity of minimizing R(θ, D′)
is presented.
Theorem 2. Given the slope asymmetry Kf that bounds the loss function f(h(x), y)
w.r.t. Eq. (5) for any hypothesis h over D′: Kf =
sup
x′
t,xt∈D′

max errD′(h(xt),Y)−errD′(h(x′
t),Y)
min errD′(h(xt),Y)−errD′(h(x′
t),Y)
,
considering a disagreement coefﬁcient ϑ = Ext∈D
sup
h∈B(h∗,r)
n
ℓ(h(xt),Y)−ℓ(h∗(xt),Y)
r
o
, if the
learning algorithm uses ϑ to smooth those data of D with smaller hypothesis disagree-
ments than r, with a probability 1 −δ, at t-time, minimizing R(θ, D′) into R(θ∗, D′),
i.e. updating the current hypothesis h into the optimal hypothesis h∗in surrogate D′,
costs at most 4ϑ × Kf ×
 
R(h∗, D′) + 2
r
8
t−1In
 2(t2−t)
d
2n−T
δ
!
.
Note that ϑ is an error disagreement parameter that used to perform the importance
sampling. Any hypothesis h holding a hypothesis disagreement smaller than r in terms
of ϑ, will be considered as a null hypothesis which presents insigniﬁcant inﬂuence
for updating the current model, thereby being smoothed from the candidate hypothesis
class. More related analysis based on this class of error disagreement parameters can
refer to Hanneke’s work e.g. (Hanneke, 2007) (Hanneke et al., 2014).
Note that Kf is a constant that satisﬁes Kf ≥1. Based on the importance sampling
of (Beygelzimer et al., 2008), Kf affects the label complexity bound due to its “sensi-
tivity”. For example, given a 0-1 loss for f(h(x), y), Kf will be 1. However, for a hinge
loss, Kf will be ∞. Therefore, for a sensitive loss function, the learning algorithm will
require a large number of importance sampling times to obtain a desired hypothesis,
then may lead to many ineffective queries. In other words, the sensitive loss function
usually presents a coarse estimation on hypothesis disagreements. We here present a
lemma to improve the generalization of Kf.
Lemma 1. Let h be generalized as a logistic hypothesis that stipulates f(h(x), y) :=
In(1 + exy), assume that the label space Y ∈[−1, +1], if x ∈[−M, M], Kf can be as
large as (1 + eM).
3.4
Approximating D into D′ using Poincar´e Distance
Poincar´e distance (Ganea et al., 2018) of hyperbolic geometry has presented an effective
improvement in latent hierarchical tasks compared to Euclidean distance (∥· ∥2) such
as ranking features (Tay et al., 2018) (Tran et al., 2020), embeddings (Nickel & Kiela,
2018), non-linear gradient descending (Nitta & Kuroe, 2017), etc. To approximate D
into D′, we need to rank one property of all its members based speciﬁed estimations
10

such as clustering property, density characteristics, geometric structure, etc. Poincar´e
distance thus is introduced to implement the ranking of the approximation progress.
Let Bd = {x ∈Rd, ∥x∥2 < 1} be an open d-dimensional unit Poincar´e sphere
(∥· ∥2 denotes the L2 norm), u and v be any two vectors in the sphere, i.e. u, v ∈Bd,
the Poincar´e distance between them is deﬁned as
h(u, v) = arccosh
 
1 + 2
∥u −v∥2
(1 −∥u∥2)(1 −∥v∥2)
!
.
(10)
Based on the work of Cao et al. (2018), noisy perturbations around the boundary
usually are characterized with low density observations. We thus estimate the density
of the data constrained within a ﬁxed hypersphere
S := {v, ∀v ∈D, s.t. ψ(u, v) = 1},
s.t. ψ(u, v) =
 0,
h(u, v) > r,
1,
0 ≤h(u, v) ≤r,
(11)
where r denotes the radius of the hypersphere S centered with u. A more general
equation that applies hypersphere S to observe the density on u is presented
fS(u) =
1
∥S∥0
X
v∈S
1
√
2π
drdexp
"
−1
2
h(u, v)
r
2 #
.
(12)
3.5
Case Study: Improving Clustering on Surrogate
This case study collects three real-world data sets and then compares the clustering per-
formance of three typical clustering baselines in D′, where fS(u) is used to approximate
D into D′. To show the advantages of Poincar´e distance in ranking, fS(u) is general-
ized into FS(u) characterized with Euclidean distance, which is further used to compare
Eq. (12):
FS(u) =
1
∥S∥0
X
v∈S
1
√
2π
mrdexp
"
−1
2
∥u −v∥2)
r
2 #
.
(13)
Datasets of the case study are digit, USPS, and FashionMnist, where all the features
of the data are scaled within an numerical unit of 10−5 to satisfy ∥xi∥2 < 1, ∀xi ∈D.
The sizes of these data sets are 3823×65, 9, 298×257, and 70, 000×784, respectively.
R(θ, D) is generalized as (1-adjusted rand index (ARI))(Hubert & Arabie, 1985) and
(1-mutual information (MI))(Vinh et al., 2010) coefﬁcients.
Figure 2 presents the R(θ, D) values that yield 1) minimizing R(h, D) by clustering
baselines, 2) minimizing R(h, D′) by clustering baselines with FS(u), and minimizing
R(h, D′) by clustering baselines with fS(u), where the clustering baselines are gener-
alized as k-means, hierarchical, and spectral clustering algorithms, the parameters r of
Eqs. (11) and (12) are deﬁned as 0.4, and ∥D∥0 −∥D′∥0 = 0.05n, i.e. approximate
D into D′ by eliminating perturbations from 0.05n boundary examples. Speciﬁcally,
11

D
D'~F
D'~f
0.05
0.1
0.15
0.2
0.25
0.3
0.35
(a) digit
D
D'~F
D'~f
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(b) USPS
D
D'~F
D'~f
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
(c) FashionMnist
Figure 2: Case study of clustering on surrogate with a smooth boundary to improve
clustering. R(θ, D) is generalized as (1-ARI) or (1-MI) coefﬁcients.
kernel function of spectral clustering is set as RBF, driving a kernel parameter as 0.1 to
construct an afﬁnity matrix, where a k-means clustering is used to assign labels in the
embedding space of the kernel.
As the shown in Figure 2, the solid lines yield (1-ARI), and dash lines yield (1-MI).
h ∼D denotes performing clustering in D, referring R(θ, D) values on y-axis. h ∼D′
includes D′ ∼F and D′ ∼f, where D′ ∼F denotes that D′ is approximated by FS(u)
w.r.t. Eq. (13), and D′ ∼f denotes that D′ is approximated by fS(u) w.r.t. Eq. (12). It is
intuitively that clustering in D′ yields R(h, D′) ≤R(h, D). Moreover, approximating
D into D′ using fS(u) (D′ ∼f) achieves lower empirical risks than that of FS(u)
(D′ ∼F) due to its hierarchical metric on ranking. This further demonstrates that
density observations employing Poincar´e distance can yield more accurate surrogate
than Euclidean distance on eliminating the noisy perturbations around the boundary.
12

4
Distribution Matching-based Machine Teaching
Section 4.1 presents the assumption of the distribution matching-based machine teach-
ing. Section 4.2 presents the detailed optimization scheme by generalizing h(D, D) in
hyperbolic geometry. Section 4.3 describes the distribution matching-based machine
teaching algorithm.
4.1
Assumption
With the effectiveness of approximating R(θ, D) in hyperbolic geometry, the general-
ization of h(D, D) also follows this non-Euclidean structure. Recalling Eq. (3), we
here present a more formal assumption against teaching a black-box learner: transfer
the disagreement estimation of parameters into hypotheses, thereby approximating the
hypothesis to distribution.
Assumption 3. Assume that θ∗is generalized from the optimal hypothesis h∗, i.e.
R(θ∗, D) = R(h∗, D), ˆθ is generalized from the hypothesis ˆh, i.e. R(ˆθ, D) = R(ˆh, D),
let D′ ⊂D be the optimal surrogate with respect to h∗, let ˆD ⊂D be the desired
training set with respect to ˆh, with the proposal of Eq. (3), we further have
min
ˆθ∈Θ
∥ˆθ −θ∗∥= min
ˆh∈H
∥ˆh −h∗∥≈min
ˆD∈D ∥ˆD −D′∥h,
(14)
where ∥ˆD −D′∥h := h( ˆD, D′).
Another expression of Eq. (14) is L(ˆθ, θ∗) = minˆθ∈Θ ∥ˆθ −θ∗∥≈h(D, D). We thus
have the following remark.
Remark 1. With Assumption 3, the optimization of distribution matching-based ma-
chine teaching over h(D, D) is generalized into min ˆD∈D ∥ˆD −D′∥h. We thus itera-
tively halve M(D′) i.e. ∥D′∥0, which linearly reduces the teaching cost. With iterative
halving, M(D′) varies from n′/2 into n′/4, n′/8, n′/16, ..., n′/2l, where l denotes the
halving frequency, n′/2i = ⌈n′/2i⌉, and the remaining examples after the lth halving
are the ﬁnal teaching test if the learner does not control the output M(D′). Speciﬁcally,
the halving process is implemented with the Poincar´e distance of hyperbolic geometry.
With iterative halving on D′, the ﬁnal update on D′ is deﬁned as the teaching set ˆD.
4.2
Cost-controlled Optimization
With Remark 1, the optimization of distribution matching-based machine teaching over
generalized min ˆD∈D ∥ˆD −D′∥h is solved by controlling the teaching cost of D′, that is
performing a continuous algorithmic halving on D′, where the ﬁnal update on D′ is the
desired target ˆD.
The algorithm begins by generalizing R(θ, D). Let D′ = {D1, D2, ..., Dn′} ∼P,
R(θ, Di) := θDi + εi, where εi denotes a constant constraint on Di, machine teaching
13

with a black-box is to optimize θ∗
θ∗= argmin
θ∈Θ
(
∥θ −θ∗∥:=
n′
X
i=1
θDi −R(θ, Di)

2
)
.
(15)
Recalling Eq. (1), we add a regularization constraint Ω(θ) := η∥θ∥2 to Eq. (15)
θ∗= argmin
θ∈Θ
(
∥θ −θ∗∥+ Ω(θ) :=
n′
X
i=1
θDi −R(θ, Di)

2
+ η∥θ∥2
)
.
(16)
Based on Assumption 3, estimating the parameter disagreement can be transferred into
distribution disagreement. We next introduce the Poincar´e distance h(·, ·) that stipulates
R(θ, Di) := Pn′/2
j=1 αjh(Dj, Di), then Eq. (16) is equivalent to
min
α
n′/2
X
i=1

n′/2
X
j=1
αjh(Dj, Di)−θDi)

2
+
n′/2
X
i=1
n′/2
X
j=1
αiαjh(Di, Dj),
(17)
where α = [α1, α2, ..., αn′/2]. Let Hij := h(Di, Dj), [H ˆ
D′D′]ij := h( ˆDi, Dj), where
ˆD′ ⊂D′, s.t. ∥ˆD′∥0 = n′/2, with Deﬁnition 3.1 in (Yu et al., 2006), Eq. (17) is
transferred as h( ˆD′, D′) that can be solved by transductive optimization
h( ˆD′, D′) := min
ˆ
D′⊂D′ Tr
"
H ˆ
D′D′(HD′D′ + ηC)−1HD′ ˆ
D′
#
,
(18)
where C ∝D′D′T. To optimize ˆD′, let D be the last selected teaching example, ˆD′i
subsequently is obtained by
ˆD′i := argmin
D∈D′ Tr
"
H ˆ
D′D(HDD + ηC)−1HD ˆ
D′
#
,
(19)
where H = H −H ˆ
D′D(HDD + ηC)−1HD ˆ
D′.
4.3
Distribution Matching-based Machine Teaching Algorithm
Our distribution matching-based machine teaching algorithm is presented in Algorithm 1.
Here, l denotes the frequency of performing the halving operation on D′ with a de-
fault constraint of M(D′) = n′ = 0.95∥D∥0. Lines 2 to 4 approximate R(θ, D) into
R(θ, D′) by shrinking D into its surrogate D′. Lines 5 to 12 perform the iterative halv-
ing process on D′. The ﬁnal update on D′ after l times halving is the machine teaching
set ˆD′. If the student learner controls the output M( ˆD′), k-medoids is performed on
the ﬁnal update of ˆD′ to satisfy the student learner’s request.
14

Algorithm 1: Distribution Matching-based Machine Teaching Algorithm
1 Input: Full training set D, 0 ←j.
2 Estimating fS(Di) for any Di:
3 F(i) ←fS(Di) =
1
∥S∥0
P
v∈S
1
√
2π
drdexp
"
−1
2

h(Di,v)
r
2
#
.
4 Keep top n′ examples with large ∥F∥1 to obtain D′.
5 while j ≤l do
6
for i = 1, 2, ..., n′
2l do
7
ˆD′i = argminD∈D′ Tr
"
H ˆ
D′D(HDD + ηC)−1HD ˆ
D′
#
,
8
s.t. H = H −H ˆ
D′D(HDD + ηC)−1HD ˆ
D′.
9
end
10
D′ ←ˆD′.
11
j ←j + 1.
12 end
13 Output: the ﬁnal teaching set M( ˆD′); if the student learner controls the out
M( ˆD′), performing k-medoids on ˆD′ to satisfy the request.
5
Experiments
Typical machine teaching algorithms estimate the parameter disagreement of models to
generalize the teaching risk, where the teacher knows the desired parameter, i.e. the
learner is a white-box. When teaching a black-box learner, parameter estimations may
be inefﬁcient due to improper parameter disagreement or inestimable parameter space.
We thus select a series of supervised and unsupervised machine learning baselines,
which can be generalized as white-box teaching, to compare our distribution-based ma-
chine teaching algorithm.
To solve Eq. (2) of general machine teaching, there exists three conditions which
can simply its optimization process: 1) control M(D) with Eq. (1)’s solver of active
learning, 2) reduce the search space for limited risk minimization, and 3) ﬁx M(D)
with unsupervised machine learning. To realize these conditions, three groups of exper-
iments are presented:
• regulating M(D) to minimize the risk disagreement of ∥R(θ, D) −R(θ∗, D)∥,
i.e. supervised way;
• reducing the search space of D to observe the perturbations to typical machine
learning and our machine teaching algorithms;
• minimizing ∥R(θ, D) −R(θ∗, D)∥with quantitative M(D), i.e. unsupervised
way.
Data sets. The data sets used in the ﬁrst two groups of experiments are the full
training data of Adult, Phishing, Satimage, and MNIST data sets, where ∥R(θ, D) −
15

R(θ∗, D)∥is over those training data. The sizes of these data sets are 11,055× 68,
1,605 × 14, 4,435×36, and 60,000× 780, respectively. The data sets used in the third
group of experiment are CIFAR10 and CIFAR100, where ∥R(θ, D)−R(θ∗, D)∥is over
their test data. The sizes of the two data sets are all 60, 000 with 32×32 pixels.
Baselines. Four supervised learning algorithms that regulate M(D) to minimize
∥R(θ, D) −R(θ∗, D)∥are selected including expected error reduction (ERR) (Roy &
McCallum, 2001), Pre-clustering (Dasgupta & Hsu, 2008), transductive experimental
design (TED) (Yu et al., 2006) and self-paced active learning (SPAL) (Tang & Huang,
2019). Speciﬁcally, they are active learning algorithms. Three typical unsupervised ma-
chine learning algorithms that minimize ∥R(θ, D)−R(θ∗, D)∥with quantitative M(D)
are selected: k-medoids, hierarchical, and spectral clustering. Those baselines are ﬁ-
nally used in experiment of teaching a deep neural network. A case study of teaching on
Gaussian data is ﬁrstly presented before the experiments. Note that distribution-based
machine teaching is denoted as DM-based machine teaching in all experimental ﬁgures.
5.1
Case Study: Teaching on a 2D Gaussian Dataset
Figure 3 presents a case study of distribution matching-based machine teaching on a
2D Gaussian dataset. The 2D visualizations dynamically show the iterative halving
process on surrogate D′: 1) Figure 3(a) draws the full training Gaussian data D, where
M(D) = 1400; 2) Figure 3(b) draws the surrogate of D, where the circled 170 data
are boundary examples, the remaining blue points are the data of D′, and the parameter
settings are r = 0.4, η = 1.0e −4; 3) Figures 3(c) to 3(h) show the iterative halving
process, where M(D′) is continuously halved. The presented teaching sets with dif-
ferent M(D′) properly match the distribution of D without noisy perturbations around
boundary.
Speciﬁcally, all the teaching examples are distributed inside the clusters with high
densities due to the smooth boundary of the surrogate D′ (w.r.t. Lines 1 to 4 of Algo-
rithm 1). The iterative halving (w.r.t. Lines 5 to 12) is performed on the last update
of D′, which keeps consistent distribution properties as its previous. Therefore, all the
teaching sets with different M(D′) yield consistent distributions as the original distri-
bution of D.
5.2
Regulating M(D) to Minimize ∥R(θ, D) −R(θ∗, D)∥
Regulating M(D) is important for both machine teacher and student learners due to
over-ﬁtting or computational overhead. With Assumption 3, the goal of machine teach-
ing is to minimize ∥R(θ, D) −R(θ∗, D)∥, where θ∗is with respect to D. The exper-
imental datasets are Adult, Phishing, Satimage, and the MNIST. We assume that h is
generated from a SVM classiﬁer with a RBF kernel. That means, R(θ∗, D) = 0.1200
on Adult, R(θ∗, D) = 0.1141 on Phishing, R(θ∗, D) = 0.0875 on Satimage, and
R(θ∗, D) = 0.0009 on MNIST, where each h∗is over the full training data.
The compared machine learning baselines are typical active learning algorithms in-
cluding ERR, Pre-clustering, TED and SPAL, where ERR maximizes the expected error
reduction over a SVM classiﬁer, Pre-clustering employs the Hierarchical clustering and
16

-8
-6
-4
-2
0
2
4
6
8
-4
-2
0
2
4
6
8
(a) Gaussian dataset D
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(b) Surrogate D′ (blue examples) of D
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(c) M(D′) = 615
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(d) M(D′) = 307
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(e) M(D′) = 153
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(f) M(D′) = 76
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(g) M(D′) = 38
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(h) M(D′) = 19
Figure 3:
2D visualizations of distribution matching-based machine teaching on a
Gaussian dataset D. In (b), circled data are boundary examples. In (c) to (h), circled
data with ‘+’ are teaching sets with different M(D′)
the pruning budget is set from 100 to 1000 with a step of 100, TED uses a hyperpa-
rameter σ=1.8 (kernel bandwidth parameter) to generate the kernel matrix and vary λ
(kernel ridge regression) from 0.01 to 1 with a step of 0.01, and SPAL sets the paced
learning parameter from 0.01 to 1 with a step of 0.01, etc. Before running those ma-
chine learning baselines, we randomly select 10 data from each dataset to train an initial
hypothesis for them. For our distribution matching-based machine teaching algorithm,
M(D′) = n′, n′/n ∈[0.85, 0.95], r = [0.1, 0.5] η = 10e −4, and l is constrained by
M(D′) that satisﬁes n′
2l ≥M(D′).
Figure 4 draws the learning curves of regulating M(D) to minimize ∥R(θ, D) −
17

100
200
300
400
500
600
700 800 900
10 -4
10 -3
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(a) Adult
100
200
300
400
500
600 700 800 900
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(b) Phishing
100
200
300
400
500
600
700 800 900
10 -2
10 -1
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(c) Satimage
100
200
300
400
500
600 700 800 900
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(d) MNIST
Figure 4: Regulating M(D) to minimize ∥R(θ, D) −R(θ∗, D)∥on Adult, Phishing,
Satimage, and MNIST data sets.
R(θ∗, D)∥into an expected risk across the best parameter candidates of each baseline.
From the test results in Figure 4, we ﬁnd that machine teaching algorithm can regulate
M(D) better than the machine learning baselines, i.e. spend fewer training data to
obtain an expected learning risk. Especially at the beginning of those curves, M(D) of
machine teaching is much smaller than that of the machine learning baselines.
5.3
Reducing the Search Space of D
Reducing the search space of D can relieve the minimization costs of Eqs. (1) and (2)
because estimating one data whether can be picked up as a teaching example needs to
access the whole unlabeled data pool. If the teacher needs to give feedback for the
learner in a limited budge cost e.g. time and space, the teaching algorithm must help
the teacher make a decision on which example should be selected.
Given an access budget of ⌈n/2⌉to the unlabeled data pool one time, the machine
learning and teaching algorithms have to return one best candidate teaching example.
18

100
200
300
400
500
600
700 800 900
10 -4
10 -3
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(a) Adult
100
200
300
400
500
600
700 800 900
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(b) Phishing
100
200
300
400
500
600
700 800 900
10 -2
10 -1
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(c) Satimage
100
200
300
400
500
600
700 800 900
10 -2
10 -1
10 0
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based  Machine Teaching
(d) MNIST
Figure 5: Reducing the search space of D to minimize ∥R(θ, D) −R(θ∗, D)∥on Adult,
Phishing, Satimage, and MNIST data sets.
Figure 5 draws the M(D) curves by progressively minimizing ∥R(θ, D) −R(θ∗, D)∥.
Compared to the results in Figure 4 with a full access budget, the M(D) of machine
learning baselines arise rapidly due to the greedy updates on θ to ˆθ can not always be the
optimal. This forces the learning algorithm to request more data to reach an expected
learning risk. However, our machine teaching algorithm backward and iteratively halves
θ∗to ˆθ without greedy search in D, thereby lower perturbations to the limited access
budget are presented. Therefore, the generalized distribution matching-based machine
teaching algorithm could trust a black-box student learner with inestimable teaching
loss in real teaching tasks.
5.4
Minimizing ∥R(θ, D) −R(θ∗, D)∥with Quantitative M(D)
Optimizing Eqs. (1) and (2) with a quantitative M(D) is also a possible condition to
simplify the minimization process. Then, Eq. (2) can be solved by an unsupervised way.
Therefore, unsupervised machine learning algorithms such as clustering can be deemed
19

100
200
300
400
500
600
700 800 900
10 -4
10 -3
10 -2
10 -1
10 0
 k-medoids of Machine Learning
Hierarchical of Machine Learning
Spectral of Machine Learning
DM-based  Machine Teaching
(a) Adult
100
200
300
400
500
600
700 800 900
10 -2
10 -1
10 0
 k-medoids of Machine Learning
Hierarchical of Machine Learning
Spectral of Machine Learning
DM-based  Machine Teaching
(b) Phishing
100
200
300
400
500
600
700 800 900
10 -2
10 -1
10 0
 k-medoids of Machine Learning
Hierarchical of Machine Learning
Spectral of Machine Learning
DM-based  Machine Teaching
(c) Satimage
100
200
300
400
500
600
700 800 900
10 -2
10 -1
10 0
 k-medoids of Machine Learning
Hierarchical of Machine Learning
Spectral of Machine Learning
DM-based  Machine Teaching
(d) MNIST
Figure 6: Minimizing R(θ∗, D) −R(θ, D) with quantitative M(D), i.e. unsupervised
machine learning.
as a special class of candidate teaching methods with quantitative M(D).
In this group of experiments, we collect the learning risk change of ∥R(θ, D) −
R(θ∗, D)∥with quantitative M(D) settings. The compared three unsupervised algo-
rithms are k-medoids, hierarchical, and spectral clustering, where M(D) is set as the
clustering numbers. Speciﬁcally, kernel function of spectral clustering is set as RBF,
driving a kernel parameter as 0.1 to construct an afﬁnity matrix, where a k-means clus-
tering is used to assign labels in the embedding space of the kernel. The whole col-
lected teaching results of the three baselines are drawn in Figure 6. We intuitively ﬁnd
the performance of all the clustering algorithms are very unstable. They show sensitive
change on ∥R(θ, D) −R(θ∗, D)∥in term of the test set of Adult and Phishing since
they are binary classiﬁcation data sets without strong clustering structures. We also ﬁnd
Hierarchical clustering algorithm cannot decrease ∥R(θ, D) −R(θ∗, D)∥when setting
M(D) be lower than 700. This is because that the two data sets have no intuitive
tree structures. For the Satimage and MNIST data sets with clear clustering structure,
unsupervised machine learning algorithms achieve better performance on minimizing
20

0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
12
3
4
5
6
7
89
0
9
5
5
6
5
0
9
8
9
8
41
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3 6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2 3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
41
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
90
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
(a) M(D′)=500
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
22
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
25
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
48
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2 1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0 1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
56
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
54
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
78
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
91
7
6
8
4
3
1 4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
65
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4 9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
76
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
90
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
(b) M(D′)=1,000
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
95
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
95
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9 8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
25
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4 8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
10
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
00
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
01
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6 9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
56
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
17
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
98
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
34
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9 8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
28
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
82
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
84
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
54
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5 0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
33
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
78
2
0
1
2
6
3
3
7
3
3
46
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
56
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
23
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
10
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3 1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1 7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
78
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0 0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7 2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
23
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
28
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
(c) M(D′)=2,000
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9 5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
54
4
7
2
8
2
2
5
7
9
5
48
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1 0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
00
1
7
6
3
2
1
7
4
6
31
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
54
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6 9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
17
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
89
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
34
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
98
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
82
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2 3
4
5
6
7
89
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
91
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
89
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
95
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9 0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
35
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
91
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
31
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
95
5
6
5
0
9
8
9
8
4
1
77
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
35
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9 0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1 7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
56
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
12
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
09
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6 6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
89
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
77
3
5
1
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6 9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
17
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
90
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
21
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
75
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
09
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
2
5
7
9
5
4
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
88
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9 0
8
9
3
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
1
7
6
8
4
3
1
4
0
5
3
6
9
6
1
7
5
4
4
7
2
8
2
2
5
7
9
5
4
8
8
4
9
0
8
9
8
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
01
2
3
4
5
6
7
8
9
0
9
5
5
6
5
0
9
8
9
8
4
1
7
7
3
5
1
0
0
2
2
7
8
2
0
1
2
6
3
3
7
3
3
4
6
6
6
4
9
1
5
0
9
5
2
8
2
0
0
1
7
6
3
2
1
7
4
6
3
1
3
9
(d) M(D′)=3,000
Figure 7: 2D embeddings of distribution matching-based machine teaching sets on
MNIST, which properly draws the 10 classes.
∥R(θ, D) −R(θ∗, D)∥, even better than machine teaching on Satimage.
Overall, the unsupervised machine learning approaches can be applied in teaching
a black-box learner, but show very unstable performance on minimizing learning risk
due to their local convergence conditions. A global strategy should be considered to
minimize ∥R(θ, D)−R(θ∗, D)∥with quantitative M(D). This also is the inherent rea-
son why our proposed distribution-based algorithm can be adopted in machine teaching
with inestimable teaching risk.
To visualize the distribution of the output teaching set of distribution matching-
based machine teaching algorithm, Figure 7 presents the 2D embeddings of teaching
sets of distribution matching-based machine teaching on MNIST with different M(D′).
The results show those teaching sets can properly draw the 10 separable classes.
21

10 2
10 3
10 4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based Machine Teaching
(a) CIFAR10
10 3
10 4
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Random of Passive Learning
ERR of Machine Learning
Pre-clustering of Machine Learning
TED of Machine Learning
SPAL of Machine Learning
DM-based Machine Teaching
(b) CIFAR100
Figure 8: Regulating M(D) to minimize ∥R(θ, D) −R(θ∗, D)∥on data sets CIFAR10
and CIFAR100.
5.5
Teaching a Deep Neural Network
We compare the deep learning performance of our distribution matching-based ma-
chine teaching algorithm to the supervised and unsupervised machine learning models.
Figure 8 presents the learning curves of regulating M(D) to minimize ∥R(θ, D) −
R(θ∗, D)∥following the experiments of Sections V.A and V.B. The deep neural net-
work is ResNet20 and the tested datasets are CIFAR10 and CIFAR100. The hyper-
parameters of the network architecture are batch size=32, epochs=200, depth =20,
learning rate=0.001, ﬁlter number=16, etc. The network architecture was implemented
by Keras 2.2.3. The results show our machine teaching algorithm can still minimize
R(θ∗, D) −R(θ, D) faster than the compared supervised and unsupervised machine
learning baselines, where R(θ∗, D) = 0.9200 over CIFAR10 and R(θ∗, D) = 0.6729
over CIFAR100. Figure 9 presents the 2D embeddings of distribution matching-based
machine teaching sets on CIFAR10 with different M(D′).
6
Discussion on Our Assumption
This study is based on Assumption 3, which stipulates R(θ∗, D) ≈R(h∗, D). There-
fore, how to derive a more general hypothesis h may have perturbations to this assump-
tion. In read-world, h is usually generalized from different classiﬁers. In this section,
we collect different classiﬁers to test the main technical steps of the iterative halving in
distribution matching-based machine teaching.
With this goal, we apply our machine teaching algorithm to derive teaching sets
as the labeled data for the subsequent supervised classiﬁcation. The candidate classi-
ﬁers are k Neighbors Regressor (KNR), Random Forest (RandomForest), Multi-layer
Perceptron classiﬁer (MLPClassiﬁer), and Support Vector Machine (SVM). Figure 10
presents the learning curves of regulating M(D) to minimize ∥R(θ, D) −R(θ∗, D)∥
on digit, USPS, and FashionMnist data sets, where θ∗is respect to h∗generalized from
22

[9]
[4]
[3]
[2]
[4]
[4]
[1]
[4]
[2]
[2]
[3]
[8]
[0]
[6]
[8]
[5]
[6]
[6]
[4]
[7]
[1]
[1]
[4]
[6]
[2]
[3]
[9]
[6]
[9]
[1]
[3]
[6]
[8]
[5]
[9]
[6]
[8]
[1]
[6]
[0]
[2]
[3]
[7]
[9]
[0]
[9]
[7]
[6]
[3]
[9]
[2]
[6]
[1]
[6]
[7]
[3]
[8]
[3]
[8]
[3]
[8]
[5]
[9]
[6]
[1][2]
[5]
[2]
[1]
[4]
[3]
[7]
[5]
[9]
[3]
[9]
[3]
[2]
[9]
[1]
[8]
[5]
[9]
[7]
[2]
[6]
[0]
[8]
[5]
[7]
[1]
[5]
[8]
[5]
[7]
[1]
[5]
[0]
[3]
[9]
[3]
[6]
[9]
[1]
[3]
[9]
[8]
[2]
[2]
[3]
[2]
[5]
[9]
[7]
[9]
[9]
[8]
[9]
[7]
[0]
[3]
[3]
[2]
[0]
[3]
[7]
[6]
[3]
[3]
[2]
[0]
[6]
[6]
[5]
[5]
[7]
[5]
[9]
[8]
[2]
[9]
[8]
[0]
[4]
[0]
[1]
[2]
[0]
[4]
[7]
[3]
[8]
[5]
[1]
[6]
[6]
[5]
[5]
[4]
[6]
[3]
[6]
[8]
[2]
[3]
[7]
[0]
[7]
[0]
[4]
[1]
[9]
[5]
[7]
[8]
[6]
[6]
[8]
[0]
[7]
[2]
[8]
[4]
[8]
[2]
[0]
[9]
[0]
[0]
[2]
[9]
[6]
[6][5]
[6]
[0]
[3]
[7]
[5]
[5]
[7]
[9]
[3]
[4]
[5]
[0]
[5]
[2]
[3]
[2]
[6]
[0]
[4]
[9]
[0]
[7]
[0]
[9]
[7]
[2]
[6]
[4]
[6]
[9]
[5]
[4]
[7]
[0]
[6]
[8]
[8]
[9]
[9]
[9]
[0]
[9]
[8]
[6]
[4]
[8]
[1]
[9]
[1]
[0]
[5]
[8]
[6]
[9]
[6]
[0]
[8]
[1]
[3]
[9]
[4]
[8]
[4]
[3]
[2]
[6]
[0]
[8]
[9]
[9]
[4]
[3]
[0]
[2]
[4]
[4]
[0]
[3]
[5]
[7]
[5]
[7]
[7]
[9]
[0]
[9]
[5]
[3]
[8]
[2]
[4]
[2]
[3]
[1]
[2]
[8]
[9]
[2]
[8]
[1]
[4]
[2]
[0]
[4]
[5]
[4]
[8]
[1]
[7]
[4]
[1]
[1]
[0]
[2]
[7]
[7]
[4]
[4]
[4]
[4]
[8]
[4]
[3]
[6]
[6]
[0]
[1]
[3]
[9]
[8]
[4]
[8]
[9]
[6]
[2]
[0]
[5]
[5]
[9]
[4]
[2]
[0]
[8]
[8]
[0]
[4]
[0]
[7]
[6]
[9]
[5]
[3]
[5]
[4]
[4]
[4]
[4]
[3]
[7]
[9]
[2]
[5]
[1]
[8]
[3]
[2]
[6]
[9]
[6]
[3]
[1]
[7]
[4]
[6]
[3]
[7]
[8]
[6]
[2]
[4]
[6]
[8]
[0]
[1]
[9]
[9]
[1]
[0]
[0]
[8]
[9]
[4]
[7]
[4]
[4]
[1]
[9]
[8]
[8]
[6]
[1]
[7]
[4]
[8]
[8][8]
[0]
[5]
[6]
[6]
[8]
[3]
[4]
[4]
[1]
[2]
[1]
[5]
[7]
[1]
[7]
[2]
[8]
[5]
[9]
[5]
[6]
[1]
[9]
[5]
[0]
[4]
[3]
[3]
[0]
[8]
[2]
[8]
[0]
[9]
[0]
[4]
[6]
[9]
[2]
[8]
[2]
[7]
[7]
[2]
[2]
[7]
[1]
[6]
[1]
[3]
[4]
[4]
[8]
[6]
[0]
[1]
[9]
[4]
[2]
[7]
[5]
[3]
[9]
[0]
[1]
[9]
[0]
[9]
[8]
[7]
[0]
[4]
[9]
[0]
[5]
[2]
[1]
[0]
[2]
[8]
[8]
[0]
[5]
[7]
[6][6]
[5]
[3]
[7]
[7]
[7]
[4]
[2]
[0]
[1]
[7]
[3]
[2]
[7]
[3]
[9]
[4]
[3]
[2]
[4]
[4]
[1]
[4]
[2]
[2]
[3]
[8]
[0]
[6]
[8]
[5]
[6]
[6]
[4]
[7]
[1]
[1]
[4]
[6]
[2]
[3]
[9]
[6]
[9]
[1]
[3]
[6]
[8]
[5]
[9]
[6]
[8]
[1]
[6]
[0]
[2]
[3]
[7]
[9]
[0]
[9]
[7]
[6]
[3]
[9]
[2]
[6]
[1]
[6]
[7]
[3]
[8]
[3]
[8]
[3]
[8]
[5]
[9]
[6]
[1]
[2]
[5]
[2]
[1]
[4]
[3]
[7]
[5]
[9]
[3]
[9]
[3]
[2]
[9]
[1]
[8]
[5]
[9]
[7]
[2]
[6]
[0]
[8]
[5]
[7]
[1]
[5]
[8]
[5]
[7]
[1]
[5]
[0]
[3]
[9]
[3]
[6]
[9]
[1]
[3]
[9]
[8]
[2]
[2]
[3]
[2]
[5]
[9]
[7]
[9]
[9]
[8]
[9]
[7]
[0]
[3]
[3]
[2]
[0]
[3]
[7]
[6]
[3][3]
[2]
[0]
[6]
[6]
[5]
[5]
[7]
[5]
[9]
[8]
[2]
[9]
[8]
[0]
[4]
[0]
[1]
[2]
[0]
[4]
[7]
[3]
[8]
[5]
[1]
[6]
[6]
[5]
[5]
[4]
[6]
[3]
[6]
[8]
[2]
[3]
[7]
[0]
[7]
[0]
[4]
[1]
[9]
[5]
[7]
[8]
[6]
[6]
[8]
[0]
[7]
[2]
[8]
[4]
[8]
[2]
[0]
[9]
[0]
[0]
[2]
[9]
[6]
[6]
[5]
[6]
[0]
[3]
[7]
[5]
[5]
[7] [9]
[3]
[4]
[5]
[0]
[5]
[2]
[3]
[2]
[6]
[0]
[4]
[9]
[0]
[7]
[0]
[9]
[7]
[2]
[6]
[4]
[6]
[9]
[5]
[4]
[7]
[0]
[6]
[8]
[8]
[9]
[9]
[9]
[0]
[9]
[8]
[6]
[4]
[8]
[1]
[9]
[1]
[0]
[5]
[8]
[6]
[9]
[6]
[0]
[8]
[1]
[3]
[9]
[4]
[8]
[4]
[3]
[2]
[6]
[0]
[8]
[9]
[9]
[4]
[3]
[0]
[2]
[4]
[4]
[0]
[3]
[5]
[7]
[5]
[7]
[7]
[9]
[0]
[9]
[5]
[3]
[8]
[2]
[4]
[2]
[3]
[1]
[2]
[8]
[9]
[2]
[8]
[1]
[4]
[2]
[0]
[4]
[5]
[4]
[8]
[1]
[7]
[4]
[1]
[1]
[0]
[2]
[7]
[7]
[4]
[4]
[4]
[4]
[8]
[4]
[3]
[6]
[6]
[0]
[1]
[3]
[9]
[8]
[4]
[8]
[9]
[6]
[2]
[0]
[5]
[5]
[9]
[4]
[2]
[0]
[8]
[8]
[0]
[4]
[0]
[7]
[6]
[9]
[5]
[3]
[5]
[4]
[4]
[4]
[4]
[3]
[7]
[9]
[2]
[5]
[1]
[8]
[3]
[2]
[6]
[9]
[6]
[3]
[1]
[7]
[4]
[6]
[3]
[7]
[8]
[6]
[2]
[4]
[6]
[8]
[0]
[1]
[9]
[9]
[1]
[0]
[0]
[8]
[9]
[4]
[7] [4]
[4]
[1]
[9]
[8]
[8]
[6]
[1]
[7]
[4]
[8]
[8]
[8]
[0]
[5]
[6]
[6]
[8]
[3]
[4]
[4]
[1]
[2]
[1]
[5]
[7]
[1]
[7]
[2]
[8]
[5]
[9]
[5]
[6]
[1]
[9]
[5]
[0]
[4]
[3]
[3]
[0]
[8]
[2]
[8]
[0]
[9]
[0]
[4]
[6]
[9]
[2]
[8]
[2]
[7]
[7]
[2]
[2]
[7]
[1]
[6]
[1]
[3]
[4]
[4]
[8]
[6]
[0]
[1]
[9]
[4]
[2]
[7]
[5]
[3]
[9]
[0] [1]
[9]
[0]
[9]
[8]
[7]
[0]
[4]
[9]
[0]
[5]
[2]
[1]
[0]
[2]
[8]
[8]
[0]
[5]
[7]
[6][6]
[5]
[3]
[7]
[7]
[7]
[4]
[2]
[0]
[1]
[7]
[3]
[2]
[7]
[3]
(a) M(D′)=500
[9]
[4]
[3]
[2]
[4]
[4]
[1]
[4]
[2]
[2]
[3]
[8]
[0]
[6]
[8]
[5]
[6]
[6]
[4]
[7]
[1]
[1]
[4]
[6]
[2]
[3]
[9]
[6]
[9]
[1]
[3]
[6]
[8]
[5]
[9]
[6]
[8]
[1]
[6]
[0]
[2]
[3]
[7]
[9]
[0]
[9]
[7]
[6]
[3]
[9]
[2]
[6]
[1]
[6]
[7]
[3]
[8]
[3]
[8]
[3]
[8]
[5]
[9]
[6]
[1]
[2]
[5]
[2]
[1]
[4]
[3]
[7]
[5]
[9]
[3]
[9]
[3]
[2]
[9]
[1]
[8]
[5]
[9]
[7]
[2]
[6]
[0]
[8]
[5]
[7]
[1]
[5]
[8]
[5]
[7]
[1]
[5]
[0]
[3]
[9]
[3]
[6]
[9]
[1]
[3]
[9]
[8]
[2]
[2]
[3]
[2]
[5]
[9]
[7] [9]
[9]
[8]
[9]
[7]
[0]
[3]
[3]
[2]
[0]
[3]
[7]
[6]
[3]
[3]
[2]
[0]
[6]
[6]
[5]
[5]
[7]
[5]
[9]
[8]
[2]
[9]
[8]
[0]
[4]
[0]
[1]
[2]
[0]
[4]
[7]
[3]
[8]
[5]
[1]
[6]
[6]
[5]
[5]
[4]
[6]
[3]
[6]
[8]
[2]
[3]
[7]
[0]
[7]
[0]
[4]
[1]
[9]
[5]
[7]
[8]
[6]
[6]
[8]
[0]
[7]
[2]
[8]
[4]
[8]
[2]
[0]
[9]
[0]
[0]
[2]
[9]
[6]
[6]
[5]
[6]
[0]
[3]
[7]
[5]
[5]
[7]
[9]
[3]
[4]
[5]
[0]
[5]
[2]
[3]
[2]
[6]
[0]
[4]
[9]
[0]
[7]
[0]
[9]
[7]
[2]
[6]
[4]
[6]
[9]
[5]
[4]
[7]
[0]
[6]
[8]
[8]
[9]
[9]
[9]
[0]
[9]
[8]
[6]
[4]
[8]
[1]
[9]
[1]
[0]
[5]
[8]
[6]
[9]
[6]
[0]
[8]
[1]
[3]
[9]
[4]
[8]
[4]
[3]
[2]
[6]
[0]
[8]
[9]
[9]
[4]
[3]
[0]
[2]
[4]
[4]
[0]
[3]
[5]
[7]
[5]
[7]
[7]
[9]
[0]
[9]
[5]
[3]
[8]
[2]
[4]
[2]
[3]
[1]
[2]
[8]
[9]
[2]
[8]
[1]
[4]
[2]
[0]
[4]
[5]
[4]
[8]
[1]
[7]
[4]
[1]
[1]
[0]
[2]
[7]
[7]
[4]
[4]
[4]
[4]
[8]
[4]
[3]
[6]
[6]
[0]
[1]
[3]
[9]
[8]
[4]
[8]
[9]
[6]
[2]
[0]
[5]
[5]
[9]
[4]
[2]
[0]
[8]
[8]
[0]
[4]
[0]
[7]
[6]
[9]
[5]
[3]
[5]
[4]
[4]
[4]
[4]
[3]
[7]
[9]
[2]
[5]
[1]
[8]
[3]
[2]
[6]
[9]
[6]
[3]
[1]
[7]
[4]
[6]
[3]
[7]
[8]
[6]
[2]
[4]
[6]
[8]
[0]
[1]
[9]
[9]
[1]
[0]
[0]
[8]
[9]
[4]
[7]
[4]
[4]
[1]
[9]
[8]
[8]
[6]
[1]
[7]
[4]
[8]
[8][8]
[0]
[5]
[6]
[6]
[8]
[3]
[4]
[4]
[1]
[2]
[1]
[5]
[7]
[1]
[7]
[2]
[8]
[5]
[9]
[5]
[6]
[1]
[9]
[5]
[0]
[4]
[3]
[3]
[0]
[8]
[2]
[8]
[0]
[9]
[0]
[4]
[6]
[9]
[2]
[8]
[2]
[7]
[7]
[2]
[2]
[7]
[1]
[6]
[1]
[3]
[4]
[4]
[8]
[6]
[0]
[1]
[9]
[4]
[2]
[7]
[5]
[3]
[9]
[0]
[1]
[9]
[0]
[9]
[8]
[7]
[0]
[4]
[9]
[0]
[5]
[2]
[1]
[0]
[2]
[8]
[8]
[0]
[5]
[7]
[6][6]
[5]
[3]
[7]
[7]
[7]
[4]
[2]
[0]
[1]
[7]
[3]
[2]
[7]
[3]
[2]
[2]
[1]
[5]
[9]
[9]
[8]
[0]
[8]
[4]
[3]
[3]
[8]
[5]
[0]
[8]
[4]
[5]
[7]
[5]
[1]
[5]
[0]
[2]
[0]
[5]
[4]
[2]
[3]
[6]
[2]
[6]
[2]
[2]
[3]
[4]
[6]
[6]
[5]
[3]
[0]
[1]
[2]
[7]
[7]
[5]
[3]
[1]
[1]
[7]
[6]
[1]
[6]
[3]
[3]
[3]
[3]
[4]
[8]
[1]
[0]
[7]
[7]
[6]
[8]
[1]
[2]
[5]
[3]
[4]
[1]
[1]
[9]
[3]
[1]
[4]
[2]
[7]
[1]
[6]
[5]
[7]
[7]
[7]
[6]
[6]
[1]
[9]
[5]
[0]
[9]
[5]
[7]
[0]
[0]
[4]
[6]
[8]
[0]
[1]
[5]
[0]
[9]
[3]
[1]
[1]
[2]
[0]
[3]
[2]
[9]
[1]
[9]
[6]
[2]
[4]
[8]
[8]
[5]
[9]
[2]
[1]
[9]
[4]
[4]
[3]
[0]
[6]
[6]
[0]
[4]
[1]
[0]
[9]
[5]
[6]
[5]
[6]
[9]
[4]
[4]
[2]
[6]
[8]
[4]
[7]
[6]
[5]
[9]
[8]
[7]
[1]
[9]
[5]
[4]
[3]
[5]
[4]
[3]
[4]
[1]
[5]
[5]
[4]
[0]
[8]
[4]
[4]
[0]
[9]
[2]
[8]
[9]
[8]
[0]
[2]
[2]
[2]
[6]
[7]
[8]
[1]
[9]
[8]
[0]
[3]
[8]
[6]
[8]
[1]
[6]
[5]
[4]
[2]
[1]
[4]
[3]
[9]
[7]
[8]
[3]
[0]
[8]
[3]
[4]
[2]
[9]
[1]
[0]
[0]
[3]
[0]
[4]
[5]
[9]
[0]
[7]
[5]
[9]
[5]
[8]
[8]
[6]
[3]
[1]
[9]
[5]
[2]
[4]
[7]
[6]
[1]
[8]
[6]
[9]
[3]
[1]
[3]
[7]
[4]
[0]
[6]
[7]
[6]
[9]
[2]
[4]
[1]
[9]
[8]
[5]
[8]
[2]
[2]
[5]
[0]
[2]
[0]
[7]
[0]
[6]
[6]
[4]
[8]
[7]
[9]
[6]
[9]
[2]
[3]
[8]
[8]
[3]
[9]
[9]
[8]
[7]
[2]
[3]
[5]
[5]
[1]
[8]
[7]
[4]
[3]
[5]
[2]
[2]
[2]
[1]
[4]
[2]
[2]
[8]
[4]
[9]
[8]
[2]
[1]
[2]
[5]
[6]
[3]
[4]
[5]
[7]
[6]
[7]
[6]
[5]
[0]
[2]
[4]
[4]
[3]
[0]
[4]
[2]
[6]
[1]
[8]
[8]
[3]
[0]
[7]
[4]
[9]
[7]
[9]
[2]
[0]
[4]
[7]
[3]
[7]
[6]
[6]
[2]
[3]
[7]
[3]
[6]
[8]
[2]
[3]
[3]
[5]
[5]
[5]
[2]
[4]
[2]
[8]
[7]
[4]
[3]
[7]
[7]
[8]
[5]
[2]
[8]
[4]
[3]
[5]
[4]
[2]
[9]
[1]
[4]
[0]
[0]
[5]
[6]
[5]
[6]
[8]
[0]
[3]
[4]
[4]
[4]
[2]
[2]
[0]
[5]
[0]
[3]
[4]
[7]
[7]
[3]
[9]
[7]
[3]
[7]
[3]
[7]
[7]
[7]
[1]
[7]
[4]
[4]
[7]
[9]
[1]
[7]
[7]
[4]
[5]
[9]
[0]
[8]
[7]
[3]
[6]
[2]
[3]
[8]
[2]
[5]
[4]
[8]
[4]
[0]
[7]
[5]
[2]
[7]
[7]
[2]
[6]
[4]
[0]
[2]
[4]
[3]
[8]
[9]
[4]
[5]
[5]
[6]
[7]
[1]
[9]
[6]
[5]
[0]
[3]
[4]
[4]
[0]
[6]
[6]
[8]
[3]
[6]
[0]
[3]
[3]
[3]
[8]
[3]
[3]
[8]
[4]
[3]
[8]
[2]
[9]
[1]
[4]
[9]
[5]
[0]
[6]
[5]
[0]
[2]
[6]
[5]
[4]
[1]
[5]
[9]
[0]
[6]
[2]
[5]
[4]
[5]
[8]
[2]
[8]
[7]
[5]
[0] [9]
[4]
[3]
[2]
[4]
[4]
[1]
[4]
[2]
[2]
[3]
[8][0]
[6]
[8]
[5]
[6]
[6]
[4]
[7]
[1]
[1]
[4]
[6]
[2]
[3]
[9]
[6]
[9]
[1]
[3]
[6]
[8]
[5]
[9]
[6]
[8]
[1]
[6]
[0]
[2]
[3]
[7]
[9]
[0]
[9]
[7]
[6]
[3]
[9]
[2]
[6]
[1]
[6]
[7]
[3]
[8]
[3]
[8]
[3]
[8]
[5]
[9]
[6]
[1]
[2]
[5]
[2]
[1]
[4]
[3]
[7]
[5]
[9]
[3]
[9]
[3]
[2]
[9]
[1]
[8]
[5]
[9]
[7]
[2]
[6]
[0]
[8]
[5]
[7]
[1]
[5]
[8]
[5]
[7]
[1]
[5]
[0]
[3]
[9]
[3]
[6]
[9]
[1]
[3]
[9]
[8]
[2]
[2]
[3]
[2]
[5]
[9]
[7]
[9]
[9]
[8]
[9]
[7]
[0]
[3]
[3]
[2]
[0]
[3]
[7]
[6]
[3][3]
[2]
[0]
[6]
[6]
[5]
[5]
[7]
[5]
[9]
[8]
[2]
[9]
[8]
[0]
[4]
[0]
[1]
[2]
[0]
[4]
[7]
[3]
[8]
[5]
[1]
[6]
[6]
[5]
[5]
[4]
[6]
[3]
[6]
[8]
[2]
[3]
[7]
[0]
[7]
[0]
[4]
[1]
[9]
[5]
[7]
[8]
[6]
[6]
[8]
[0]
[7]
[2]
[8]
[4]
[8]
[2]
[0]
[9]
[0]
[0]
[2]
[9]
[6]
[6][5]
[6]
[0]
[3]
[7]
[5]
[5]
[7]
[9]
[3]
[4]
[5]
[0]
[5]
[2]
[3]
[2]
[6]
[0]
[4]
[9]
[0]
[7]
[0]
[9]
[7]
[2]
[6]
[4]
[6]
[9]
[5]
[4]
[7]
[0]
[6]
[8]
[8]
[9]
[9]
[9]
[0]
[9]
[8]
[6]
[4]
[8]
[1]
[9]
[1]
[0]
[5]
[8]
[6]
[9]
[6]
[0]
[8]
[1]
[3]
[9]
[4]
[8]
[4]
[3]
[2]
[6]
[0]
[8]
[9]
[9]
[4]
[3]
[0]
[2]
[4]
[4]
[0]
[3]
[5]
[7]
[5]
[7]
[7]
[9]
[0]
[9]
[5]
[3]
[8]
[2]
[4]
[2]
[3]
[1]
[2]
[8]
[9]
[2]
[8]
[1]
[4]
[2]
[0]
[4]
[5]
[4]
[8]
[1]
[7]
[4]
[1]
[1]
[0]
[2]
[7]
[7]
[4]
[4]
[4]
[4]
[8]
[4]
[3]
[6]
[6]
[0]
[1]
[3]
[9]
[8]
[4]
[8]
[9]
[6]
[2]
[0]
[5]
[5]
[9]
[4]
[2]
[0]
[8]
[8]
[0]
[4]
[0]
[7]
[6]
[9]
[5]
[3]
[5]
[4]
[4]
[4]
[4]
[3]
[7]
[9]
[2]
[5]
[1]
[8]
[3]
[2]
[6]
[9]
[6]
[3]
[1]
[7]
[4]
[6]
[3]
[7]
[8]
[6]
[2]
[4]
[6]
[8]
[0]
[1]
[9]
[9]
[1]
[0]
[0]
[8]
[9]
[4]
[7]
[4]
[4]
[1]
[9]
[8]
[8]
[6]
[1]
[7]
[4]
[8]
[8]
[8]
[0]
[5]
[6]
[6]
[8]
[3]
[4]
[4]
[1]
[2]
[1]
[5]
[7]
[1]
[7]
[2]
[8]
[5]
[9]
[5]
[6]
[1]
[9]
[5]
[0]
[4]
[3]
[3]
[0]
[8]
[2]
[8]
[0]
[9]
[0]
[4]
[6]
[9]
[2]
[8]
[2]
[7]
[7]
[2]
[2]
[7]
[1]
[6]
[1]
[3]
[4]
[4]
[8]
[6]
[0]
[1]
[9]
[4]
[2]
[7]
[5] [3]
[9]
[0]
[1]
[9]
[0]
[9]
[8]
[7]
[0]
[4]
[9]
[0]
[5]
[2]
[1]
[0]
[2]
[8]
[8]
[0]
[5]
[7]
[6] [6]
[5]
[3]
[7]
[7]
[7]
[4]
[2]
[0]
[1]
[7]
[3]
[2]
[7]
[3]
[2]
[2]
[1]
[5]
[9]
[9]
[8]
[0]
[8]
[4]
[3]
[3]
[8]
[5]
[0]
[8]
[4]
[5]
[7]
[5]
[1]
[5]
[0]
[2]
[0]
[5]
[4]
[2]
[3]
[6]
[2]
[6]
[2]
[2]
[3]
[4]
[6]
[6]
[5]
[3]
[0]
[1]
[2]
[7]
[7]
[5]
[3]
[1]
[1]
[7]
[6]
[1]
[6]
[3]
[3]
[3]
[3]
[4]
[8]
[1]
[0]
[7]
[7]
[6]
[8]
[1]
[2]
[5]
[3]
[4]
[1]
[1]
[9]
[3]
[1]
[4]
[2]
[7]
[1]
[6]
[5]
[7]
[7]
[7]
[6]
[6]
[1]
[9]
[5]
[0]
[9]
[5]
[7]
[0]
[0]
[4]
[6]
[8]
[0] [1]
[5]
[0]
[9]
[3]
[1]
[1]
[2]
[0]
[3]
[2]
[9]
[1]
[9]
[6]
[2]
[4]
[8]
[8]
[5]
[9]
[2]
[1]
[9]
[4]
[4]
[3]
[0]
[6]
[6]
[0]
[4]
[1]
[0]
[9]
[5]
[6]
[5]
[6]
[9]
[4]
[4]
[2]
[6]
[8]
[4]
[7]
[6]
[5]
[9]
[8]
[7]
[1]
[9]
[5]
[4]
[3]
[5]
[4]
[3]
[4]
[1]
[5]
[5]
[4]
[0]
[8]
[4]
[4]
[0]
[9]
[2]
[8]
[9]
[8]
[0]
[2]
[2]
[2]
[6]
[7]
[8]
[1]
[9]
[8]
[0]
[3]
[8]
[6]
[8]
[1]
[6]
[5]
[4]
[2][1]
[4]
[3]
[9]
[7]
[8]
[3]
[0]
[8]
[3]
[4]
[2]
[9]
[1]
[0]
[0]
[3]
[0]
[4]
[5]
[9]
[0]
[7]
[5]
[9]
[5]
[8]
[8]
[6]
[3]
[1]
[9][5]
[2]
[4]
[7]
[6]
[1]
[8]
[6]
[9]
[3]
[1]
[3]
[7]
[4]
[0]
[6]
[7]
[6]
[9]
[2][4]
[1]
[9]
[8]
[5]
[8]
[2]
[2]
[5]
[0]
[2]
[0]
[7]
[0]
[6]
[6]
[4]
[8]
[7]
[9]
[6]
[9]
[2]
[3]
[8]
[8]
[3]
[9]
[9]
[8]
[7]
[2]
[3]
[5]
[5]
[1]
[8]
[7]
[4]
[3]
[5]
[2]
[2]
[2]
[1]
[4]
[2]
[2]
[8]
[4]
[9]
[8]
[2]
[1]
[2]
[5]
[6]
[3]
[4]
[5]
[7]
[6]
[7]
[6]
[5]
[0]
[2]
[4]
[4]
[3]
[0]
[4]
[2]
[6]
[1]
[8]
[8]
[3]
[0]
[7]
[4][9]
[7]
[9]
[2]
[0]
[4]
[7]
[3]
[7]
[6]
[6]
[2]
[3]
[7]
[3]
[6]
[8]
[2]
[3]
[3]
[5]
[5]
[5]
[2]
[4]
[2]
[8]
[7]
[4]
[3]
[7]
[7]
[8]
[5]
[2]
[8]
[4]
[3]
[5]
[4]
[2]
[9]
[1]
[4]
[0]
[0]
[5]
[6]
[5]
[6]
[8]
[0]
[3]
[4]
[4]
[4]
[2]
[2]
[0]
[5]
[0]
[3]
[4]
[7]
[7]
[3]
[9]
[7]
[3]
[7]
[3]
[7]
[7]
[7]
[1]
[7]
[4]
[4]
[7]
[9]
[1]
[7]
[7]
[4]
[5]
[9]
[0]
[8]
[7]
[3]
[6]
[2]
[3]
[8]
[2]
[5]
[4]
[8]
[4]
[0]
[7]
[5]
[2]
[7]
[7]
[2]
[6]
[4]
[0]
[2]
[4]
[3]
[8]
[9]
[4]
[5]
[5]
[6]
[7]
[1]
[9]
[6]
[5]
[0]
[3]
[4]
[4]
[0]
[6]
[6]
[8]
[3]
[6]
[0]
[3]
[3]
[3]
[8]
[3]
[3]
[8]
[4]
[3]
[8]
[2]
[9]
[1]
[4]
[9]
[5]
[0]
[6]
[5]
[0]
[2]
[6]
[5]
[4]
[1]
[5]
[9]
[0]
[6]
[2]
[5]
[4]
[5]
[8]
[2]
[8]
[7]
[5]
[0]
(b) M(D′)=1000
Figure 9: 2D embeddings of distribution matching-based machine teaching sets on CI-
FAR10, which properly draws the 10 classes.
different classiﬁers.
The training parameters of the four classiﬁers are described as follows: (1) we set
the k nearest number as 10 for KNR; (2) we set the number of trees in the RandomForest
as 600; (3) for MLPClassiﬁer, we set the size of the hidden layers as 100, the maximum
iteration number as 1000, the L2 penalty (regularization term) parameter as 0.0004, the
optimization strategy as stochastic gradient descent, and the learning rate as 0.001; (4)
for the SVM classiﬁer, we set the penalty parameter as 1.0, the kernel type as RBF, the
degree of the kernel function as 3, and the tolerance for stopping criterion as 0.003.
Distribution matching-based machine teaching signiﬁcantly reduces the expected
learning risks with the increase of M(D). In the reported results of the four classiﬁers,
perturbations of ∥R(θ, D) −R(θ∗, D)∥, i.e.

max R(θ∗, D) −R(θ, D)) −(min R(θ∗, D) −R(θ, D)

,
which yields an interval of [0.06, 0.14], where M(D)/n yields an interval of [0.1, 0.5].
Particularly, a part of learning curves don not keep consistent decreasing with the in-
crease of M(D)/n such as MLPClassiﬁer on digit, RandomForest on FashionMnist,
etc. This explores that the iterative halving may delay the decrease of the expected
learning risks, which further reduces the perturbations of having. Therefore, our ma-
chine teaching algorithm keeps an uniform decrease on expected learning risks, which
also may delay its decrease to a lower loss. Besides this, SVM achieves the lowest
∥R(θ, D) −R(θ∗, D)∥. Cooperating with a solid classiﬁer may further delay the de-
crease of the learning risks.
7
Conclusion
In this work, we proposed a distribution matching-based machine teaching algorithm
with regard to estimating a teaching risk on distributions against a black-box learner.
23

0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.11
0.12
0.13
(a) KNR on digit
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
(b) RandomForest on digit
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.11
(c) MLPClassiﬁer on digit
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
(d) SVM on digit
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
(e) KNR on USPS
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
(f) RandomForest on USPS
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.04
0.06
0.08
0.1
0.12
0.14
0.16
(g) MLPClassiﬁer on USPS
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.11
(h) SVM on USPS
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
(i) KNR on FashionMnist
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.17
0.18
0.19
0.2
0.21
0.22
0.23
0.24
0.25
0.26
(j) RandomForest on FashionMnist
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.12
0.14
0.16
0.18
0.2
0.22
0.24
(k) MLPClassiﬁer on FashionM-
nist
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.08
0.09
0.1
0.11
0.12
0.13
0.14
0.15
0.16
0.17
0.18
(l) SVM on FashionMnist
Figure 10: Learning curves of regulating M(D) to minimize R(θ∗, D) −R(θ, D) on
digit, USPS, and FashionMnist data sets, where θ∗is respect to h∗generalized from
different classiﬁers.
The analysis proved that the approximated surrogate had safety guarantee. Case study
further presented support for this theoretical view and demonstrated that Poincar´e dis-
tance of hyperbolic geometry could yield a smoother boundary for learning a surrogate
24

than Euclidean distance. We thus projected the subsequent iterative halving in this ge-
ometry. Experiments demonstrated distribution matching-based machine teaching out-
performed the supervised and unsupervised machine learning algorithms on minimizing
expected learning risk disagreement. Finally, this work leads to an open question: can
we co-teach the disagreement estimations on the distribution and model parameters?
Acknowledgments
This work was supported by Australian Research Council under Grant DP180100106
and DP200101328.
Appendix
Proof of Theorem 1.
Proof. Fix the input training data D, for any hypothesis hi ∈H over D, we use passive
sampling to weight the learning process of the importance sampling algorithm. Let
errT(h) denote the important weighted error at T-time of sampling, we deﬁne the risk
of T times of importance sampling as R(h, D, T) :=errT(h) = 1
T
PT
t=1
qt
ptf(h(xt), yt),
and the risk of full training on D R(h, D) := errD(h) = E(x,y)∼Df(h(x), y).
Let Y be the label set of D, f(·) be a class of mapping function involved with er-
ror measure from D to Y, such as the best-in-class error or all-in-class error. Given
any sampled example xt from D which leads to a biased error at any t-th time of sam-
pling, the upper bound of errors of the T times of sampling satisﬁes R(h, D, T) >
R(h, D, t), ∀t < T. That is to say, R(h, D, T) > R(h, D, 1), R(h, D, T) > R(h, D, 2),
..., R(h, D, T) > R(h, D, T-1). Therefore, the disagreement of the surrogate and its
full training data in the T times of sampling satisﬁes
R = |R(h, D, T) −R(h, D)| ≤R(h, D, T)
= 1
T
T
X
t=1
qt
φ f (h(xt), yt) ,
(20)
then we have the following inequality about the upper bound of risk R
R ≤1
T
T
X
t=1
qt
φ f (h(xt), yt) ,
(21)
where qt < φ for any t ≤T.To produce a generalization on the above risk disagreement,
we need to estimate a more general upper bound on 1
T
PT
t=1
qt
φ f (h(xt), yt). Following
importance sampling, we deﬁne f(·) be a 0-1 loss. Then, a more general upper bound
of PT
t=1
qt
φ f (h(xt), yt) is PT
t=1
qt
φ × 1. We here update Eq. (21) into
R ≤1
φ.
(22)
25

Let h1, h2, ..., hT be T independent hypotheses over D at different sampling times,
where ht is the hypothesis generated at the t-time of sampling. We follow the result of
Eq. (22) and know
|R(h, D, T) −R(h, D)| ≤1
φ, ∀t < T.
(23)
Simply to say,
1
φ can be a hypothesis diameter that covers the hypothesis class of
{h1, h2, ..., hT}. With this diameter constraint, for any pair hypotheses {ht, ht′}, we
conclude
errT(ht′) −errD(ht)
 ≤1
φ, ∀t, t′ < T.
(24)
After applying the McDiarmid’s Inequality for Eq. (24), with deﬁnition 2, the gen-
eralization probability bound of achieving a safety surrogate in the T times of sampling
is
Pr
(|errT(h) −errD(h)| −E[errT(h) −errD(h)]
 ≤λ
)
≤exp
 
−2λ2
PT
i=1 φ−2
!
= exp
−2λ2
Tφ−2

.
(25)
If we set λ proportional to φ−1, then there exists a maximum possible martingale
value of
|errT(h) −errD(h)| −E[errT(h) −errD(h)]
. Following the choice of slack
variable in (Beygelzimer et al., 2009), we set λ =
p
2(In d + In(2/δ)) and ˆR =
Eh∈H|R(h, D, T) −R(h, D)|, then the bound is as stated.
Proof of Theorem 2.
Proof. Fix θ, assume that D is over a ball B with radius r. Let h(x) ̸= h∗(x), h∗=
arg inf
h∈H errD(h), we deﬁne ϑ (Beygelzimer et al., 2010) (Beygelzimer et al., 2008) as
ϑ = Ext∈Dsuph∈B(h∗,r)
ℓ(h(xt), Y) −ℓ(h∗(xt), Y)
r

,
(26)
where ℓ(h, Y) denotes the classiﬁcation loss of a hypothesis h regarded with the label
space Y, and r denotes the hypothesis radius of H.
To estimate the hypothesis radius r, we have ℓ(h, h∗) ≤r. For any hypothesis
h, we deﬁne the hypothesis distance ℓ(h, h∗) as: ℓ(h, h∗) = Ex∈Dmax|ℓ(h(xt), Y) −
26

ℓ(h∗(xt), Y)|. Then we have
ℓ(h, h∗) = Ex∈Dmax|ℓ(h(xt), Y) −ℓ(h∗(xt), Y)|
≤
sup
x′
t,xt∈D′

max ℓ(h(xt), Y) −ℓ(h(x′
t), Y)
min ℓ(h(xt), Y) −ℓ(h(x′
t), Y)
 × |Ex∈D|ℓ(h(xt), Y) −ℓ(h∗(xt), Y)|
≤Kf |Ex∈Dℓ(h(xt), Y)| + |Ex∈Dℓ(h∗(xt), Y)|
≤2Kfℓ(h).
(27)
By Theorem 2 of (Beygelzimer et al., 2008), we know that the risk disagreement of h
and h∗satisﬁes ℓ(h) ≤ℓ(h∗) + 2
s
8
t−1In

2(t2−t)
d
2n−T
δ

. Thus,
ℓ(h, h∗) ≤2Kfℓ(h)
≤2Kf

ℓ(h∗) + 2
v
u
u
t
8
t −1In
 
2(t2 −t)
d
2n−T
δ
! 
.
(28)
For any two hypotheses hi, hj, ℓ(hi, hj) ≤2ℓ(h, h∗) over B(h∗, r). Therefore, updating
the hypothesis h into the optimal hypothesis h∗over D′ costs at most 2ℓ(h, h∗) at t-time.
Here, we give a bound on 2ℓ(h, h∗) as follows:
2ℓ(h, h∗)
≤2ϑr ≤4ϑKf

ℓ(h∗) + 2
v
u
u
t
8
t −1In
 
2(t2 −t)
d
2n−T
δ
! 
.
(29)
When we associate the loss function ℓ(h, Y) with err′
D(h) as a generalization, ℓ(h∗)
equals R(h∗, D′). Theorem 2 then holds.
References
Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning (pp.
41–48).
Beygelzimer, A., Dasgupta, S., & Langford, J. (2008). Importance weighted active
learning. Proceedings of the 25th international conference on Machine learning.
Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active
learning. In Proceedings of the 26th annual international conference on machine
learning (pp. 49–56).
27

Beygelzimer, A., Hsu, D. J., Langford, J., & Zhang, T. (2010). Agnostic active learning
without constraints. In Advances in neural information processing systems (pp. 199–
207).
Cao, X., Qiu, B., Li, X., Shi, Z., Xu, G., & Xu, J. (2018). Multidimensional balance-
based cluster boundary detection for high-dimensional data. IEEE transactions on
neural networks and learning systems, 30(6), 1867–1880.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine learning, 15(2), 201–221.
Dasgupta, S., & Hsu, D. (2008). Hierarchical sampling for active learning. In Proceed-
ings of the 25th international conference on machine learning (pp. 208–215).
Dasgupta, S., Hsu, D., Poulis, S., & Zhu, X. (2019). Teaching a black-box learner. In
Proceedings of the 36th international conference on machine learning, ICML 2019,
9-15 june 2019, long beach, california, USA (pp. 1547–1555).
Dasgupta, S., Hsu, D. J., & Monteleoni, C. (2008). A general agnostic active learning
algorithm. In Advances in neural information processing systems (pp. 353–360).
Ganea, O.-E., B´ecigneul, G., & Hofmann, T. (2018). Hyperbolic entailment cones for
learning hierarchical embeddings. Thirty-ﬁfth International Conference on Machine
Learning (ICML 2018).
Gao, Z., Ries, C., Simon, H. U., & Zilles, S. (2017). Preference-based teaching. The
Journal of Machine Learning Research, 18(1), 1012–1043.
Hanneke, S. (2007). A bound on the label complexity of agnostic active learning.
In Proceedings of the 24th international conference on machine learning (pp. 353–
360).
Hanneke, S., et al. (2014). Theory of disagreement-based active learning. Foundations
and Trends® in Machine Learning, 7(2-3), 131–309.
Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of classiﬁcation, 2(1),
193–218.
Khan, F., Mutlu, B., & Zhu, J. (2011). How do humans teach: On curriculum learning
and teaching dimension. In Advances in neural information processing systems (pp.
1449–1457).
Langley, P. (2006). Agnostic active learning. ICML.
Lessard, L., Zhang, X., & Zhu, X. (2019). An optimal control approach to sequential
machine teaching. In The 22nd international conference on artiﬁcial intelligence and
statistics (pp. 2495–2503).
Liu, J., & Zhu, X. (2016). The teaching dimension of linear learners. The Journal of
Machine Learning Research, 17(1), 5631–5655.
28

Liu, W., Dai, B., Li, X., Liu, Z., Rehg, J. M., & Song, L. (2018). Towards black-box
iterative machine teaching. In Proceedings of the 35th international conference on
machine learning, ICML 2018, stockholmsm¨assan, stockholm, sweden, july 10-15,
2018 (pp. 3147–3155).
Matiisen, T., Oliver, A., Cohen, T., & Schulman, J. (2019). Teacher-student curriculum
learning. IEEE Transactions on Neural Networks and Learning Systems.
Mei, S., & Zhu, X. (2015). Using machine teaching to identify optimal training-set at-
tacks on machine learners. In Twenty-ninth aaai conference on artiﬁcial intelligence.
Mitchell, T. M., et al. (1997). Machine learning. 1997. Burr Ridge, IL: McGraw Hill,
45(37), 870–877.
Nickel, M., & Kiela, D. (2018). Learning continuous hierarchies in the lorentz model
of hyperbolic geometry. Thirty-ﬁfth International Conference on Machine Learning
(ICML 2018).
Nitta, T., & Kuroe, Y. (2017). Hyperbolic gradient operator and hyperbolic back-
propagation learning algorithms. IEEE transactions on neural networks and learning
systems, 29(5), 1689–1702.
Roy, N., & McCallum, A. (2001). Toward optimal active learning through sampling
estimation of error reduction. int. conf. on machine learning. Morgan Kaufmann.
Rubens, N., Sheinman, V., Tomioka, R., & Sugiyama, M. (2011). Active learning in
black-box settings. Austrian Journal of Statistics, 40(1-2), 125–135.
Sarkar, R. (2011). Low distortion delaunay embedding of trees in hyperbolic plane. In
International symposium on graph drawing (pp. 355–366).
Seung, H. S., Opper, M., & Sompolinsky, H. (1992). Query by committee. In Proceed-
ings of the ﬁfth annual workshop on computational learning theory (pp. 287–294).
Shinohara, A., & Miyano, S. (1991). Teachability in computational learning. New
Generation Computing, 8(4), 337–347.
Tang, Y.-P., & Huang, S.-J. (2019). Self-paced active learning: Query the right thing
at the right time. In Proceedings of the aaai conference on artiﬁcial intelligence
(Vol. 33, pp. 5117–5124).
Tay, Y., Tuan, L. A., & Hui, S. C. (2018). Hyperbolic representation learning for fast and
efﬁcient neural question answering. In Proceedings of the eleventh acm international
conference on web search and data mining (pp. 583–591).
Tong, S., & Koller, D. (2001). Support vector machine active learning with applications
to text classiﬁcation. Journal of machine learning research, 2(Nov), 45–66.
Tosh, C., & Dasgupta, S. (2017). Diameter-based active learning. In International
conference on machine learning (pp. 3444–3452).
29

Tran, L. V., Tay, Y., Zhang, S., Cong, G., & Li, X. (2020). Hyperml: A boosting
metric learning approach in hyperbolic space for recommender systems. In Wsdm
(pp. 609–617).
Vapnik, V. (2013). The nature of statistical learning theory. Springer science & business
media.
Vinh, N. X., Epps, J., & Bailey, J. (2010). Information theoretic measures for cluster-
ings comparison: Variants, properties, normalization and correction for chance. The
Journal of Machine Learning Research, 11, 2837–2854.
Yu, K., Bi, J., & Tresp, V. (2006). Active learning via transductive experimental design.
In Proceedings of the 23rd international conference on machine learning (pp. 1081–
1088).
Zhu, J. (2013). Machine teaching for bayesian learners in the exponential family. In
Advances in neural information processing systems (pp. 1905–1913).
Zhu, X. (2015). Machine teaching: An inverse problem to machine learning and an
approach toward optimal education. In Twenty-ninth aaai conference on artiﬁcial
intelligence.
Zhu, X., Singla, A., Zilles, S., & Rafferty, A. N. (2018). An overview of machine
teaching. arXiv preprint arXiv:1801.05927.
30

