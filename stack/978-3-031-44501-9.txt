Emergence, Complexity and Computation ECC
Vincenzo Manca
Vincenzo Bonnici
Infogenomics
The Informational Analysis of Genomes

Emergence, Complexity and Computation 
Volume 48 
Series Editors 
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic 
Andrew Adamatzky, University of the West of England, Bristol, UK 
Guanrong Chen, City University of Hong Kong, Hong Kong, China 
Editorial Board 
Ajith Abraham, MirLabs, USA 
Ana Lucia, Universidade Federal do Rio Grande do Sul, Porto Alegre, Rio Grande 
do Sul, Brazil 
Juan C. Burguillo, University of Vigo, Spain 
Sergej ˇCelikovský, Academy of Sciences of the Czech Republic, Czech Republic 
Mohammed Chadli, University of Jules Verne, France 
Emilio Corchado, University of Salamanca, Spain 
Donald Davendra, Technical University of Ostrava, Czech Republic 
Andrew Ilachinski, Center for Naval Analyses, USA 
Jouni Lampinen, University of Vaasa, Finland 
Martin Middendorf, University of Leipzig, Germany 
Edward Ott, University of Maryland, USA 
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China 
Gheorghe P˘aun, Romanian Academy, Bucharest, Romania 
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany 
Juan A. Rodriguez-Aguilar 
, IIIA-CSIC, Spain 
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany 
Yaroslav D. Sergeyev, Dipartimento di Ingegneria Informatica, University of 
Calabria, Rende, Italy 
Vaclav Snasel, Technical University of Ostrava, Ostrava, Czech Republic 
Ivo Vondrák, Technical University of Ostrava, Ostrava, Czech Republic 
Hector Zenil, Karolinska Institute, Solna, Sweden

The Emergence, Complexity and Computation (ECC) series publishes new devel-
opments, advancements and selected topics in the ﬁelds of complexity, computa-
tion and emergence. The series focuses on all aspects of reality-based computation 
approaches from an interdisciplinary point of view especially from applied sciences, 
biology, physics, or chemistry. It presents new ideas and interdisciplinary insight on 
the mutual intersection of subareas of computation, complexity and emergence and 
its impact and limits to any computing based on physical limits (thermodynamic and 
quantum limits, Bremermann’s limit, Seth Lloyd limits…) as well as algorithmic 
limits (Gödel’s proof and its impact on calculation, algorithmic complexity, the 
Chaitin’s Omega number and Kolmogorov complexity, non-traditional calculations 
like Turing machine process and its consequences,…) and limitations arising in arti-
ﬁcial intelligence. The topics are (but not limited to) membrane computing, DNA 
computing, immune computing, quantum computing, swarm computing, analogic 
computing, chaos computing and computing on the edge of chaos, computational 
aspects of dynamics of complex systems (systems with self-organization, multiagent 
systems, cellular automata, artiﬁcial life,…), emergence of complex systems and its 
computational aspects, and agent based computation. The main aim of this series is 
to discuss the above mentioned topics from an interdisciplinary point of view and 
present new ideas coming from mutual intersection of classical as well as modern 
methods of computation. Within the scope of the series are monographs, lecture 
notes, selected contributions from specialized conferences and workshops, special 
contribution from international experts. 
Indexed by zbMATH.

Vincenzo Manca · Vincenzo Bonnici 
Infogenomics 
The Informational Analysis of Genomes

Vincenzo Manca 
Department of Computer Science 
University of Verona 
Pisa, Italy 
Vincenzo Bonnici 
Department of Mathematics, Physics 
and Computer Science 
University of Parma 
Parma, Italy 
ISSN 2194-7287
ISSN 2194-7295 (electronic) 
Emergence, Complexity and Computation 
ISBN 978-3-031-44500-2
ISBN 978-3-031-44501-9 (eBook) 
https://doi.org/10.1007/978-3-031-44501-9 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2023 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Paper in this product is recyclable.

We dedicate this book to our nearest nodes in 
the tree of life to which we belong: our 
parents, our children, and grandchildren (of 
the oldest author).

Preface 
This book originates in the courses that authors took in the last ten years at the 
universities of Verona and Parma, and before at the University of Udine and Pisa. 
They have been on very different subjects centered on information, in many scien-
tiﬁc ﬁelds: computation, formal representation, unconventional computing, molec-
ular biology, artiﬁcial intelligence. The names of courses were: Information Theory, 
Languages and Automata, Unconventional Computing, Discrete biological Models, 
Programming Laboratory, Data Structures, Computational Biology. The book does 
not collect all the topics of these courses, but surely inﬂuenced by them, it is focused 
on genomes. 
The conviction that we maturated during our teaching and related research, during 
last years, was the centrality of information as the common factor of all subjects of 
our interests. Information is the most recent scientiﬁc concept. It emerged in the 
last century, by revolutionizing the whole science. The reason of this is evident. 
Namely, all scientiﬁc theories develop by collecting data from observed phenomena, 
and these data support theories which explain phenomena and provide new scientiﬁc 
knowledge. 
Information is inside data, but it does not coincide with them. It assumes and 
requires data, in order to be represented, but is independent from any speciﬁc data 
representation, because it can be always equivalently recovered by encoding data 
into other data. 
This is the reason of its deep almost evanescent nature, but also of its ﬂuidity, ﬂexi-
bility, adaptability, and universality. Information can always be linearized into strings. 
Genomes, which are the texts of life, are strings. But these linear forms of repre-
sentation are managed, elaborated, transformed, and become meaningful, through 
networks, made of elements connected by relations, at different levels of complexity. 
Our brains are a particular case of such networks which develops through sensory, 
linguistic, social, and cultural information. In the dynamics of the communication, 
the ethereal essence of information ﬂows and determines the various kinds of reality 
of our knowledge universe.
vii

viii
Preface
This book tells small fragments of this big story, that we collected and partially 
understood during our scientiﬁc adventure. 
Pisa, Italy 
Parma, Italy 
Vincenzo Manca 
Vincenzo Bonnici 
Acknowledgments We express our gratitude to the students of the universities of Verona, Pisa, 
and Parma, which in the last ten years followed our courses in the curricula of Computer Science, 
Biological Sciences, Computational Biology, and Mathematics, at different levels. Their comments 
and suggestions improved the quality of the notes from which this book originates.We thank also 
our colleagues: Giuditta Franco, Giuseppe Scollo, Luca Marchetti, Alberto Castellini, Roberto 
Pagliarini, Rosario Lombardo, of the University of Verona, who collaborated with us in writing 
many papers in the research area of the book.

Contents 
1 
The Infogenomics Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4 
2 
Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7 
2.1
Sets and Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7 
2.2
Strings and Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12 
2.3
Variables and Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16 
2.4
Events and Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22 
3 
Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23 
3.1
From Physical to Informational Entropy . . . . . . . . . . . . . . . . . . . . . .
24 
3.2
Entropy and Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27 
3.3
Entropic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31 
3.4
Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38 
3.5
Huffman Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43 
3.6
First Shannon Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47 
3.7
Typical Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50 
3.7.1 
AEP (Asymptotic Equipartition Property) Theorem . . . . . .
50 
3.8
Second Shannon Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52 
3.9
Signals and Continuous Distributions . . . . . . . . . . . . . . . . . . . . . . . . .
55 
3.9.1 
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57 
3.10 
Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61 
3.11 
Sampling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62 
3.12 
Third Shannon Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65 
4 
Informational Genomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67 
4.1
DNA Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68 
4.2
Genome Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74 
4.3
Genome Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77 
4.4
Genome Informational Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
ix

x
Contents
4.5
Genome Information Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82 
4.6
Genome Spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86 
4.7
Elongation and Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91 
4.8
Genome Informational Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97 
4.9
Genome Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98 
4.10 
Genome Divergences and Similarities . . . . . . . . . . . . . . . . . . . . . . . .
100 
4.11 
Lexicographic Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105 
4.12 
Sufﬁx Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111 
5 
Information and Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113 
5.1
Topics in Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113 
5.2
Informational Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133 
5.3
Information in Physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140 
5.4
The Informational Nature of Quantum Mechanics . . . . . . . . . . . . . .
151 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156 
6 
Life Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
6.1
Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162 
6.2
Swarm Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164 
6.3
Artiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165 
6.4
Artiﬁcial Versus Human Intelligence . . . . . . . . . . . . . . . . . . . . . . . . .
175 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186 
7 
Introduction to Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189 
7.1
The Python Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189 
7.2
The Python Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190 
7.3
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194 
7.4
Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195 
7.5
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197 
7.6
Collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200 
7.7
Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209 
7.8
Classes and Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210 
7.9
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211 
7.10 
Some Notes on Efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215 
7.11 
Iterators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216 
7.12 
Itertools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219 
8 
Laboratory in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223 
8.1
Extraction of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223 
8.2
Extraction of Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225 
8.3
Word Multiplicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226 
8.4
Counting Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228 
8.5
Searching for Nullomers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230 
8.6
Dictionary Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233 
8.7
Reading FASTA Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235 
8.8
Informational Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236

Contents
xi
8.9
Genomic Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239 
8.10 
Genomic Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249 
8.11 
Recurrence Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270 
8.12 
Generation of Random Genomes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
278 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283 
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285

Acronyms 
2LE
Binary Logarithmic Entropy 
AC
Anti-entropic Component 
AEP
Asymptotic Equipartition Property 
AI
Artiﬁcial Intelligence 
AMD
Average Multiplicity Distribution 
ANN
Artiﬁcial Neural Network 
BB
Bio Bit 
CSO
Constraint Optimization Problem 
EC
Entropic Component 
ECP
Entropic Circular Principle 
EED
Empirical Entropy Distribution 
EP
Equipartition Property 
ESA
Enhanced Sufﬁx Arrays 
ghl
Greatest Minimal Hapax Length 
GPT
Generative Pre-trained Transformer 
LCP
Longest Common Preﬁx 
LE
Logarithmic Entropy 
LG
Logarithmic Length 
LX
Lexical Index 
mcl
Maximal Complete Length 
mﬂ
Minimum Forbidden Length 
mhl
Minimum Hapax Length 
mrl
Maximum Repeat Length 
nhl
No Hapax Length 
nrl
No Repeat Length 
PSO
Particle Swarm Optimization 
RDD
Recurrence Distance Distribution 
WCMD
Word Co-Multiplicity Distribution 
WLD
Word Length Distribution 
WMD
Word Multiplicity Distribution
xiii

Chapter 1 
The Infogenomics Perspective 
Prologue 
This book presents a conceptual and methodological basis for the mathematical and 
computational analysis of genomes, as developed in publications [ 1– 13]. However, 
the presentation of the following chapters is, in many aspects, independent from their 
original motivation and could be of wider interest. 
Genomes are containers of biological information, which direct the cell func-
tions and the evolution of organisms. Combinatorial, probabilistic, and informational 
aspects are fundamental ingredients of any mathematical investigation of genomes 
aimed at providing mathematical principles for extracting the information that they 
contain. 
Basic mathematical notions are assumed, which are recalled in the next chapter. 
Moreover, basic concepts of Chemistry, Biology, Genomics [ 14], Calculus and Prob-
ability theory [ 15– 17] are useful for a deeper comprehension of several discussions 
and for a better feeling of the motivations that underlie many subjects considered in 
the book. 
A key concept of this book is that of distribution, a very general notion occurring 
in many forms. The most elementary notion of distribution consists of a set and a 
“multiplicity” function that associates a number of occurrences to each element of 
the set. Probability density functions or distributions in the more general perspective 
of measure theory generalize this idea. Distributions as generalized functions are 
central in the theory of partial differential equations. The intuition common to any 
notion of distribution is that of a “quantity” spread over a “space”, or parts, of a whole 
quantity, associated to locations. A probability distribution is a way of distributing 
possibilities to events. Probability and Information are for many aspects two faces 
of the same coin, and the notion of “Information Source”, which is the starting point 
of Shannon’s Information Theory, is essentially a probability distribution on a set of 
data. 
The topics presented in the book include research themes developed by authors 
in the last ten years, and in many aspects, the book continues a preceding volume 
published by Vincenzo Manca, Infobiotics: Information in biotic systems, Springer, 
2013 [ 4]. We could shortly say that, abstractly and concisely, the book [ 4] investigates 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_1 
1

2
1
The Infogenomics Perspective
the relationship between rewriting and metabolism (metabolic P systems are the 
formal counterpart of metabolic systems), while the present book investigates the 
relationship between distributions and genomes. 
The main inspiring idea of the present book is an informational perspective to 
Genomics. Information, is the most recent, among the fundamental mathematical 
and physical concepts developed in the last two centuries [ 18, 19]. It has revolu-
tionized the whole science, and continues, in this direction, to dominate the trends 
of contemporary science. In fact, any discipline collects data from observations, by 
providing theories able to explain, predict, and dominate natural phenomena. But 
data are containers of information, whence information is essential in any scientiﬁc 
elaboration. 
The informational viewpoint in science becomes even more evident for the sci-
ences of life, which after molecular biology, have revealed as living organisms elab-
orate and transmit information, organized in molecular structures. Reproduction is 
the way by means of which information is transmitted along generations, but, in the 
same moment that biological information is transmitted, random variations alter a 
small percentage of the message passed to the reproduced organism. 
Any reproductive system is reﬂexive, in the sense that it includes a proper part 
containing the information for reconstructing the whole organism. Thus, reﬂexivity is 
a purely informational concept, and DNA molecules are the basilar forms of reﬂexive 
structures, where reﬂexivity is realized by pairing two strands of biopolymers that 
can be recovered, by a template-driven copying process, from only one of them. 
Genomes evolved from very short primordial biopolymers, proto-genomes, that 
assumed the function of biological memories. Enzymes, which direct, as catalysts, 
metabolic reactions, are the ﬁrst molecular machines, but as any physical com-
plex structure, they are subjected to destructive forces acting over time. The only 
way of keeping their speciﬁc abilities is the possibility of maintaining traces of 
them. Thus, proto-genomes probably originate as enzymatic memories, sequences 
including “copies” of the enzymes of proto-cells (membrane hosting substances and 
reactions among them). These memories, passing along generations, allowed life 
to develop and evolve, without losing the biological information cumulated along 
generations. 
Genomes are “long” strings, or texts, of millions or billions of letters. These texts 
describe forms of life and even the history of life that produced them. Therefore, 
genomes are biologically meaningful texts. This means that these texts postulate 
languages. The main challenge for the biology of the next centuries is just discover-
ing these languages. Speciﬁc steps in this direction are the following: (1) to identify 
the minimal units conveying information and their categories; (2) to discover the 
syntax expressing the correct mechanisms of aggregation of genomic units, at their 
different levels of articulation; (3) to understand the ways and the contexts activating 
statements that direct the (molecular) agents performing speciﬁc cellular tasks; (4) 
to determine the variations admitted in the class of individual genomes of a given 
species; (5) to reconstruct the relationships between genome structures and the cor-
responding languages that these structures realize in the communication processes 
inside cells.

1
The Infogenomics Perspective
3
Genomes are texts, but in a very speciﬁc sense, which is hardly comparable with 
the usual texts telling stories or describing phenomena. Maybe, a close comparison 
could be done with a legal code, prescribing rules of behaviour, with a manual of the 
use of a machine, or with an operative system of a computer. However, this special 
kind of manual/program is a sort of “living manual”, cabled in the machine, as the 
interpreter of a programming language, but able to generate new machines. This fact 
is directly related to the cruciality of reproduction. Genomes generate statements 
assimilable to linguistic forms of usual languages, which instruct cell agents and, in 
some particular contexts, can change just parts of the same genomes directing the 
play, but somewhat paradoxically, they maintain a memory of their past history, but 
also of the mechanisms allowing their future transformations. 
In fact, genomes structures and related languages provide an intrinsic instabil-
ity promoting a small rate of change pushing toward new structures. This means 
that there are general genome built-in rules of internal variation and reorganization. 
Genealogies of life are ﬁrstly genealogies of genomes, very similar to those of nat-
ural languages. In this way, from a biological species new species derive, through 
passages called speciations. A new species arises when individual variations, in 
restricted classes of individuals, cumulate in such a way that some internal criteria of 
coherence require a new organization incompatible with the genome of the ancestor 
organism. 
These dynamics require two kinds of interactions, one between species and indi-
viduals, and another between syntax and semantics of the genome languages. The 
genome of a species is an abstract notion, because, in the concrete biological reality, 
only individual genomes exist, and the genome of a species is the set of genomes of 
all individuals of that species. If some speciﬁc attributes have a range of variability, in 
the genome of the species these attributes become variables. Such a kind of abstract 
genome is a pattern of individual genomes. Fisher’s theorem on natural selection (due 
to Ronald Fisher, 1890–1962) establishes that the time variation rate of genomes, of 
a given species, is proportional to the genome variability among the individuals of 
that species. In other words, the more genomes are variegate in the individuals living 
at some time, the more, on average, they change along generations. 
Variations in individual genomes promote new interactions between syntax and 
semantics of genome languages, where semantics “follows” syntax. In other words, 
syntax admits some statements that even if well-formed can be meaningless. How-
ever, under the pressure of some circumstances, certain statements, casually and 
temporarily, assume new meanings that in cell interactions “work”, becoming seeds 
of new semantic rules. If these rules are reinforced by other situations of success in 
cellular communication, they eventually stabilize. Speciation occurs when the num-
ber of genome variations overcomes a given threshold so that the language underlying 
the genomes results differently with respect to those of ancestor genomes. At that 
moment, the resulting organisms represent a new branch in the tree of life. 
This “linguistic” scenario is only evocative of possible investigations, however, 
it is a good argument for explaining why considering genomes as “texts of life” is 
fascinating and could be one of the most promising approaches to the comprehension 
of the evolutionary mechanisms. In this regard, a clue about life intelligence resides

4
1
The Infogenomics Perspective
in the relationship between chance and purpose. Randomness provides a powerful 
generator of possibilities, but the navigation in the space of possibilities selects tra-
jectories that result in more convenience to realize some ﬁnalities. Probably, this form 
of intelligence, is spread evenly in the primordial genomes (as rules of syntactical 
coherence?). This possibility could explain the efﬁciency of evolution in ﬁnding the 
best solutions for life development. This theme is present since Wiener’s pioneering 
research in Cybernetics (“Purpose, Finality and Teleology” is the title of Wiener’s 
paper in 1943, with Arturo Rosenblueth and Julian Bigelow, anticipating his famous 
book published in 1948). Random genomes, which will be considered in the chapter 
“Information and Randomness” of this book, show a purely informational property 
of random texts. 
This short introduction wants to emphasize the role that information plays in 
biological systems. The awareness of this fact persuaded us that a deep understanding 
of life’s fundamental mechanisms has to be based on the informational analysis of 
biological processes. In this spirit, the following chapters outline general principles 
and applications of information theory to genomes. 
Finally, a general remark on information concerns its relationship with knowl-
edge. It is important to distinguish between the two notions. Knowledge is surely 
based on information, but a collection of data is something completely different from 
knowledge. Knowledge appears when data are interpreted within theories where they 
ﬁt, by revealing an internal logic connecting them. But theories do not emerge auto-
matically from data, conversely, data are explained when they are incorporated within 
theories, which remain scientiﬁc creations of human (or artiﬁcial?) intelligence. 
References 
1. Manca, V.: On the logic and geometry of bilinear forms. Fundamenta Informaticae 64, 261– 
273 (2005) 
2. Manca, V., Franco, G.: Computing by polymerase chain reaction. Math. Biosci. 211, 282–298 
(2008) 
3. Castellini, A., Franco, G., Manca, V.: A dictionary based informational genome analysis. 
BMC Genomics 13, 485 (2012) 
4. Manca, V.: Infobiotics: Information in Biotic Systems. Springer, Berlin (2013) 
5. Manca, V.: Infogenomics: Genomes as Information Sources. Chapter 21, pp. 317–324. Else-
vier, Morgan Kauffman (2016) 
6. Bonnici, V., Manca, V.: Infogenomics tools: a computational suite for informational analysis 
of genomes. J. Bioinform. Proteomics Rev. 1, 8–14 (2015) 
7. Bonnici, V., Manca, V.: Recurrence distance distributions in computational genomics. Am. J. 
Bioinform. Comput. Biol. 3, 5–23 (2015) 
8. Manca, V.: Information theory in genome analysis. In: Membrane Computing, LNCS, vol. 
9504, pp. 3–18. Springer, Berlin (2016) 
9. Bonnici, V.: Informational and Relational Analysis of Biological Data. Ph.D. Thesis, Dipar-
timento di Informatica Università di Verona (2016) 
10. Bonnici, V., Manca, V.: Informational laws of genome structures. Sci. Rep. 6, 28840 (2016). 
http://www.nature.com/articles/srep28840. Updated in February 2023 
11. Manca, V.: The principles of informational genomics. Theor. Comput. Sci. (2017)

References
5
12. Manca, V., Scollo, G.: Explaining DNA structure. Theor. Comput. Sci. 894, 152–171 (2021) 
13. Bonnici, V., Franco, G., Manca, V.: Spectrality in genome informational analysis. Theor. 
Comput. Sci. (2020) 
14. Lynch, M.: The Origin of Genome Architecture. Sinauer Associate. Inc., Publisher (2007) 
15. Aczel, A.D.: Chance. Thunder’s Mouth Press, New York (2004) 
16. Feller, W.: An Introduction to Probability Theory and Its Applications. Wiley, New York 
(1968) 
17. Schervish, M.J.: Theory of Statistics. Springer, New-York (1995) 
18. Brillouin, L.: Scienze and Information Theory. Academic, New York (1956) 
19. Brillouin, L.: The negentropy principle of information. J. Appl. Phys. 24, 1152–1163 (1953)

Chapter 2 
Basic Concepts 
2.1 
Sets and Relations 
A set is a collection of elements considered as a whole entity characterized only and 
completely by the elements belonging to it. Braces are used for enclosing a ﬁnite list 
of elements of a set, or for enclosing a variable xx and (after a bar vertical bar|) a property upper P left parenthesis x right parenthesisP(x)
true when xx assumes values that are elements of the set: StartSet x vertical bar upper P left parenthesis x right parenthesis EndSet{x | P(x)}. 
Two basic relations element of∈and subset of or equal to⊆denote when an element belongs to a set, and when 
a set is included in another set (all elements of the ﬁrst set belong also to the second 
set). A special set is the empty set denoted by normal empty set∅that does not contain any element. 
On sets are deﬁned the usual boolean operations of union union∪, intersection intersection∩and 
difference–. The powerset double struck upper P left parenthesis upper A right parenthesisP(A) is the set of all subsets of upper AA. 
The pair set StartSet a comma b EndSet{a, b} has only aa and bb as elements. 
The ordered pairleft parenthesis a comma b right parenthesis(a, b) is a set with two elements, but where an order is identiﬁed. 
A classical way to express ordered pair is the following deﬁnition due to Kuratowski: 
left parenthesis a comma b right parenthesis equals StartSet a comma StartSet a comma b EndSet EndSet(a, b) = {a, {a, b}}
where the ﬁrst elementaa is the element that is a member of the set and of the innermost 
set included in it, whereas the second element bb belongs only to the innermost set. 
The cartesian productupper A times upper BA × B of two sets is the set of ordered pairs where the ﬁrst 
element is in upper AA and the second is in upper BB. 
The notion of ordered pair can be iteratively applied for deﬁning nn-uples for any 
nn in the set double struck upper NN of natural numbers. In fact, an nn-uple is deﬁned in terms of pairs: 
left parenthesis a 1 comma a 2 comma a 3 comma ellipsis comma a Subscript n Baseline right parenthesis equals left parenthesis a 1 comma left parenthesis a 2 comma left parenthesis a 3 comma ellipsis a Subscript n Baseline right parenthesis right parenthesis right parenthesis period(a1, a2, a3, . . . , an) = (a1, (a2, (a3, . . . an))).
Logical symbols for all comma there exists comma or comma and comma normal not sign comma right arrow∀, ∃, ∨, ∧, ¬, →abbreviate for all, there exists, or, and, not, 
if-then, respectively. 
Akk-ary relationupper RR over a setupper AA is a set ofkk-uples of elements ofupper AA.upper R left parenthesis a 1 comma a 2 comma ellipsis comma a Subscript k Baseline right parenthesisR(a1, a2, . . . , ak)
is an abbreviation for left parenthesis a 1 comma a 2 comma ellipsis comma a Subscript k Baseline right parenthesis element of upper R(a1, a2, . . . , ak) ∈R. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_2 
7

8
2
Basic Concepts
Binary relations are of particular importance and among them equivalence and 
ordering relations, with all the concepts related to them. Oftenupper R left parenthesis a comma b right parenthesisR(a, b) is abbreviated 
by a upper R baRb. 
A binary relation over a set upper AA is an equivalence relation over upper AA if the following 
conditions hold: 
a upper R a for all a element of upper AaRa
∀a ∈A (Refexivity) 
a upper R b right double arrow b upper R a for all a comma b element of upper AaRb ⇒bRa
∀a, b ∈A (Symmetry) 
a upper R b and b upper R c right double arrow a upper R c for all a comma b comma c element of upper AaRb ∧bRc ⇒aRc ∀
a, b, c ∈A (Transitivity) 
Given an equivalence class over a setupper AA and an elementaa ofupper AA the setleft bracket a right bracket Subscript upper R Baseline equals StartSet x element of upper A vertical bar a upper R x EndSet[a]R = {x ∈
A | aRx} is the equivalence class of aa. The  set StartSet left bracket x right bracket Subscript upper R Baseline vertical bar x element of upper A EndSet{[x]R | x ∈A} is the quotient set of 
upper AA with respect to upper RR, denoted by upper A slash Subscript upper R BaselineA/R. This set is a partition of upper AA, that is, the union 
of all its sets coincides with upper AA (it is a covering of upper AA) and these sets are non-empty 
and disjoint (with no common element). 
A binary relation over a set upper AA is an ordering relation over upper AA if the following 
conditions hold: 
a upper R a for all a element of upper AaRa
∀a ∈A (Reﬂexivity) 
a upper R b and b upper R a right double arrow a equals b for all a comma b element of upper AaRb ∧bRa ⇒a = b
∀a, b ∈A (Antisymmetry) 
a upper R b and b upper R c right double arrow a upper R c for all a comma b comma c element of upper AaRb ∧bRc ⇒aRc
∀a, b, c ∈A (Transitivity) 
An ordering relation is partial if two elements exist such that both a upper R baRb and b upper R abRa
do not hold. In this case, a comma ba, b are incomparable with respect toupper RR. An order is linear 
or total if for all two elements a comma b element of upper Aa, b ∈A at least one of the two cases a upper R baRb and b upper R abRa
holds. Given a subsetupper BB ofupper AA, the elementm element of upper Bm ∈B is the minimum ofupper BB forupper RR ifm upper R bmRb
for allb element of upper Bb ∈B, whilemm is minimal inupper BB if nob element of upper Bb ∈B exist such thatb upper R mbRm. Analogously 
upper M element of upper BM ∈B is the maximum ofupper BB forupper RR ifb upper R upper MbRM for allb element of upper Bb ∈B, andupper MM is maximal inupper BB if 
no b element of upper Bb ∈B exists such that upper M upper R bM Rb. 
Given a subsetupper BB ofupper AA the elementm element of upper Am ∈A is a lower bound ofupper BB, with respect toupper RR, 
ifm upper R bmRb for allb element of upper Bb ∈B. Analogouslyupper M element of upper BM ∈B is an upper bound ofupper BB with respect toupper RR
ifb upper R upper MbRM for allb element of upper Bb ∈B. In these cases,mm is the greatest lower bound ofupper BB with respect 
to upper RR if mm is the maximum in the set of the lower bounds of upper BB, and analogously upper MM
is the lowest upper bound ofupper BB, with respect toupper RR, ifupper MM is the minimum in the set of 
the upper bounds of upper BB. 
A kk-operation omegaω over a set upper AA is a left parenthesis k plus 1 right parenthesis(k + 1)-ary relation over upper AA where the last 
argument depends completely on the others. In other words, if two left parenthesis k plus 1 right parenthesis(k + 1)-uples 
of the relation coincide on the ﬁrst kk components, then they coincide on the last 
argument too. The last argument is also called the result of the operation omegaω on the 
ﬁrst kk arguments a 1 comma a 2 comma ellipsis comma a Subscript k Baselinea1, a2, . . . , ak and is denoted by omega left parenthesis a 1 comma a 2 comma ellipsis comma a Subscript k Baseline right parenthesisω(a1, a2, . . . , ak). 
A function of domain upper AA and codomain upper BB
f colon upper A right arrow upper B f : A →B

2.1 Sets and Relations
9
(from upper AA to upper BB) is identiﬁed by the two sets upper A comma upper BA, B and by an operation f f such that 
whenf f is applied to an elementaa ofupper AA, then the resultf left parenthesis a right parenthesis f (a) is an element ofupper BB, also  
called the image of aa according to f f and if f left parenthesis a right parenthesis equals b f (a) = b, element aa is also called the 
counter-image or inverse image of bb. A function is injective when distinct elements 
of the domain have always distinct images in the codomain. A function is surjective 
when all the elements of upper BB are images of elements of upper AA, A function is 1-to-1 or 
bijective when it is injective and surjective. Given a subsetupper C subset of or equal to upper BC ⊆B the set of inverse 
images StartSet x element of upper A vertical bar f left parenthesis x right parenthesis element of upper C EndSet{x ∈A | f (x) ∈C} is denoted by f Superscript negative 1 Baseline left parenthesis upper C right parenthesis f −1(C). 
Set-theoretic concepts and notation are the basic language of mathematics, devel-
oped in the last two centuries [ 1, 2]. Within the set theory, Cantor developed a rigor-
ous mathematical analysis of inﬁnity, by showing the rich articulation of this concept 
when it is mathematically formalized (transﬁnite, ordinal and cardinal numbers are 
sets). Set theory is also the basis of general topology, where general notions of space 
can be developed. Nowadays, almost all mathematical theories can be expressed in 
the set-theoretic language (see [ 3, 4] for introductory texts to set theory). For the 
needs of our discourse, the basilar concepts given here are enough. The foundational 
power of sets will not appear explicitly in the chapters of this book. However, it is 
important to realize that the expressiveness and universality of set-theoretic language 
is mainly due to the conceptual strength of the set theory, which in its advanced topics 
remains one of the most complex and fascinating mathematical theories. 
A ﬁnite sequence of length nn over a set upper AA is a function from (positions) 
StartSet 1 comma 2 comma ellipsis comma n EndSet{1, 2, . . . , n} to upper AA. 
A ﬁnite multiset of size nn over a set upper AA can be considered as a function from upper AA to 
natural numbers assigning a multiplicity to any element of the set upper AA, such that the 
sum of all multiplicities is equal to nn. 
An inﬁnite sequence, denoted by left parenthesis a Subscript n Baseline vertical bar n element of double struck upper N right parenthesis(an | n ∈N) has the set of natural numbers as 
a domain. The same notation with a generic setupper II of indexes denotes a family over a 
setupper AA (indexed byupper II). An alphabetupper AA is a ﬁnite set of elements, called symbols. Finite 
sequences of symbols are also called strings. 
A variable is an elementxx which is associated with a setupper AA, called the range ofxx, 
where the variable takes values in correspondence to some contexts of evaluation. The 
notion of variable is strictly related to that of function, but in a variable is stressed its 
range rather than the correspondence between contexts of evaluation (corresponding 
to the domain of a function) and the values assumed in the range (corresponding to 
the codomain of a function). In fact, for a variable, often, the mechanism determining 
the values is unknown or is not relevant. 
If a variable xx takes values in correspondence to the values of another variable, 
for example, a variable tt of time, then the value assumed by xx are denoted by x left parenthesis t right parenthesisx(t), 
and the correspondence, 
t right arrow from bar x left parenthesis t right parenthesist |→x(t)
determines a function from the range of tt to the range of xx. 
When we apply operations to elements, in a composite way, for example: 
omega left parenthesis x comma eta left parenthesis y right parenthesis right parenthesis right parenthesisω(x, η(y)))

10
2
Basic Concepts
where omegaω is a binary operation and etaη is a unary operation, then the expression above 
denotes a value in correspondence to the values of variables x comma yx, y. Parentheses are 
essential for denoting the order of applications of operations. In the case of the 
expression above the innermost parentheses are related to the application of etaη and 
after determining the value of eta left parenthesis y right parenthesis right parenthesisη(y)) the operation omegaω is applied to its arguments. 
In this case, a function can be deﬁned as having as domain the cartesian product 
of the ranges of xx and yy and, as codomain, the values assumed by omega left parenthesis x comma eta left parenthesis y right parenthesis right parenthesis right parenthesisω(x, η(y))) in 
correspondence to the values of xx and yy: 
left parenthesis x comma y right parenthesis right arrow from bar omega left parenthesis x comma eta left parenthesis y right parenthesis right parenthesis right parenthesis period(x, y) |→ω(x, η(y))).
The deep understanding of the relationship among arithmetic, logic, and com-
putability is one of the most exciting successes of mathematical logic in the 20th 
century, explaining the pervasive role of symbolic sequences in computation pro-
cesses [ 5, 6]. In a sense, molecular biology discovered the same kind of pervasive-
ness for biopolymers, which are representations of symbolic sequences at a molecular 
level. 
The notion of expression can be mathematically generalized by the notion of 
a rooted tree, which expresses the abstract notion underlying a genealogical tree. 
A rooted tree is essentially obtained by starting from an element, called root, by 
applying to it a generation operation providing a set of nodes, that are the sons of the 
root, and then by iteratively applying to generated nodes the generation operation. 
When no generation operation is applied to a node, then it is a leaf. An element that 
is not a leaf is an internal node. A sequence of elements where any element (apart 
from the ﬁrst) is the son of the previous one provides a branch of the tree, and a tree 
is inﬁnite if it has an inﬁnite branch. 
Trees can be further generalized by graphs. A graph is given by a setupper AA of elements, 
called nodes, and a set upper EE of edges such that each edge is associated with a source 
node and a target node. A path in a graph is a sequence of nodes where for each node 
aa of the sequence, apart from the last element of the sequence, exists another node 
bb such that aa and bb are the source and the target of an edge of the graph. A path is 
inﬁnite if no last element is present in the sequence, and it is a cycle if the last node 
of the path is equal to its ﬁrst node. A graph is connected if, for any two nodes of 
the graph, there exists a path starting from one node and ending in the other one. A 
graph that is connected and without cycles is called a non-rooted tree. 
Basic combinatorial schemata 
Combinatorics deals with problems of counting ﬁnite structures of interest in many 
mathematical contexts [ 7]. These structures are usually deﬁnable in terms of sets, 
sequences, functions, and distributions over ﬁnite sets. Counting them is often not 
only mathematically interesting in itself, but very important from the point of view 
of a lot of applications. Combinatorics was the starting point of probability because

2.1 Sets and Relations
11
some events correspond to the occurrence of structures of a given subclass within a 
wider class of possible structures. In many cases, a ﬁnite structure of interest is given 
by a distribution of objects into cells. 
The number of functions fromnn objects tomm cells isn Superscript nnn (any object can be allocated 
in one of the cells), whereas the number of bijective functions from nn objects to nn, 
called also nn-permutations, is obtained by allocating the ﬁrst object in nn ways, the 
second one inn minus 1n −1 ways (because cannot be allocated in the ﬁrst cell), and so forth, 
with the last object allocated in only one possible way (in the last remained cell). 
Therefore: 
StartAbsoluteValue n left right arrow n EndAbsoluteValue equals n factorial period|n ↔n| = n!.
where n factorialn! is the factorial of nn, that is, the product 1 times 2 times midline horizontal ellipsis times left parenthesis n minus 1 right parenthesis times n1 × 2 × · · · × (n −1) × n. 
Reasoning as in the previous cases the number of injective functions StartAbsoluteValue n Subscript subset of Baseline right arrow m EndAbsoluteValue|n ⊂→m|
from nn to mm objects (n greater than or equals mn ≥m) is given by: 
StartAbsoluteValue n Subscript subset of Baseline right arrow m EndAbsoluteValue equals n factorial Subscript m Baseline|n ⊂→m| = n!m
where n factorial Subscript m Baselinen!m is the falling factorial of mm decreasing factors starting from nn: 
n factorial Subscript m Baseline equals n left parenthesis n minus 1 right parenthesis left parenthesis n minus 2 right parenthesis ellipsis left parenthesis n minus m plus 1 right parenthesis equals product Underscript k equals 1 Overscript m Endscripts left parenthesis n minus k plus 1 right parenthesisn!m = n(n −1)(n −2) . . . (n −m + 1) =
m
∏
k=1
(n −k + 1)
Counting the number of mm-subsets of nn elements can be obtained by counting 
all the injective functions n right arrow Subscript superset of Baseline mn →⊃m and then ignoring the distinguishability of cells, 
by dividing by m factorialm!, which corresponds to all the possible ways of ordering cells. In 
conclusion, the number StartAbsoluteValue m subset of or equal to n EndAbsoluteValue|m ⊆n| of mm-sets is given by: 
StartAbsoluteValue m subset of or equal to n EndAbsoluteValue equals n factorial Subscript m Baseline slash m factorial period|m ⊆n| = n!m/m!.
Themm-sets overnn objects are called combinations ofmm overnn, and their numbers 
n factorial Subscript m Baseline slash m factorialn!m/m! are called binomial coefﬁcients, and denoted by: 
StartBinomialOrMatrix n Choose m EndBinomialOrMatrix equals n factorial Subscript m Baseline slash m factorial
(n
m
)
= n!m/m!
also given by the following formula: 
StartBinomialOrMatrix n Choose m EndBinomialOrMatrix equals StartFraction n factorial Over m factorial left parenthesis n minus m right parenthesis factorial EndFraction period
(n
m
)
=
n!
m!(n −m)!.
The number of partitions of nn undistinguishable objects in kk distinct classes, 
possibly empty, can be obtained by considering boolean sequences of nn zeros and 
k minus 1k −1 ones. A boolean sequence of n plus k minus 1n + k −1 positions is completely determined 
when we chose the nn positions where put zeros, therefore:

12
2
Basic Concepts
StartBinomialOrMatrix n plus k minus 1 Choose n EndBinomialOrMatrix equals left parenthesis n plus k minus 1 right parenthesis Subscript factorial n Baseline divided by n factorial
(n + k −1
n
)
= (n + k −1)!n/n!
If we use the rising factorial notationk factorial Superscript n Baselinek!n (the product ofnn increasing factors starting 
from kk) it is easy to realize that: 
StartBinomialOrMatrix n plus k minus 1 Choose n EndBinomialOrMatrix equals k factorial Superscript n Baseline slash n factorial
(n + k −1
n
)
= k!n/n!
If we choose kk objects, over nn distinct objects, with the possibility that an object 
is chosen many times, the choice can be represented by a boolean sequence of kk
zeros and n minus 1n −1 ones, where zeros between two consecutive positions ii and i plus 1i + 1
(also before the ﬁrst one, or after the last one) express the multiplicity of the object 
of typeii. A boolean sequence ofn plus k minus 1n + k −1 positions is completely determined when 
we chose the kk positions where put zeros, therefore the number of these choices is 
given by (the roles of objects and cells are exchanged, with respect to partitions): 
StartBinomialOrMatrix n plus k minus 1 Choose k EndBinomialOrMatrix equals n factorial Superscript k Baseline slash k factorial
(n + k −1
k
)
= n!k/k!
The following proposition is an important characterization of binomial coefﬁ-
cients. Its proof follows a general schema, which is common to a great number of 
combinatorial formulas where the cases to count are partitioned into two distinct 
subsets. 
Proposition 2.1 The binomial coefﬁcient can be computed by means of the Tartaglia-
Pascal equation: 
StartBinomialOrMatrix n plus 1 Choose k plus 1 EndBinomialOrMatrix equals StartBinomialOrMatrix n Choose k EndBinomialOrMatrix plus StartBinomialOrMatrix n Choose k plus 1 EndBinomialOrMatrix
(n + 1
k + 1
)
=
(n
k
)
+
(
n
k + 1
)
Proof Let us consider an object, denoted by a 0a0, among the given n plus 1n + 1 objects. 
Then k plus 1k + 1-sets of n plus 1n + 1 elements can include a 0a0 or not. Therefore, we can count 
separately the numberupper C Subscript k plus 1 comma n Baseline left parenthesis a 0 right parenthesisCk+1,n(a0) ofk plus 1k + 1-sets ofn plus 1n + 1 elements includinga 0a0 and the 
number upper C Subscript k plus 1 comma n Baseline left parenthesis normal not sign a 0 right parenthesisCk+1,n(¬a0) of k plus 1k + 1-sets of n plus 1n + 1 elements not including a 0a0. The number 
upper C Subscript k plus 1 comma n Baseline left parenthesis a 0 right parenthesisCk+1,n(a0) is given by StartBinomialOrMatrix n Choose k EndBinomialOrMatrix
(n
k
)
, because being a 0a0 included, any k plus 1k + 1-set corresponds to 
the choice of a kk-set over nn elements. The number upper C Subscript k plus 1 comma n Baseline left parenthesis normal not sign a 0 right parenthesisCk+1,n(¬a0) is equal to StartBinomialOrMatrix n Choose k plus 1 EndBinomialOrMatrix
( n
k+1
)
, 
because being a 0a0 not included, it can be removed from the set of n plus 1n + 1 objects, 
which becomes annn set, giving thatupper C Subscript k plus 1 comma n Baseline left parenthesis normal not sign a 0 right parenthesis equals StartBinomialOrMatrix n Choose k plus 1 EndBinomialOrMatrixCk+1,n(¬a0) =
( n
k+1
)
. In conclusion, the number 
of StartBinomialOrMatrix n plus 1 Choose k plus 1 EndBinomialOrMatrix
(n+1
k+1
)
is the sum of StartBinomialOrMatrix n Choose k EndBinomialOrMatrix
(n
k
)
and StartBinomialOrMatrix n Choose k plus 1 EndBinomialOrMatrix
( n
k+1
)
. 
2.2 
Strings and Rewriting 
Strings are the mathematical concept corresponding to the notion of words as linear 
arrangements of symbols. Dictionaries are ﬁnite sets of strings. These two notions 
are very important in the analysis of strings, and present a very rich catalogue of 
concepts and relations, in a great variety of situations.

2.2 Strings and Rewriting
13
Let us recall basic concepts and notation on strings (for classical textbooks, see 
for example [ 8]). Strings are ﬁnite sequences over an alphabet. 
Often strings are written as words, that is, without parentheses and commas or 
spaces between symbols:a 1 a 2 ellipsis a Subscript n Baseline perioda1a2 . . . an. When single elements are not relevant, generic 
strings are often denoted by Greek letters (possibly with subscripts). In particular, 
lamdaλ denotes the empty string. The length of a string alphaα is denoted by StartAbsoluteValue alpha EndAbsoluteValue|α| (StartAbsoluteValue lamda EndAbsoluteValue equals 0|λ| = 0), 
whereasalpha left bracket i right bracketα[i] denotes the symbol occurring at positionii ofalphaα, andalpha left bracket i comma j right bracketα[i, j] denotes the 
substring of alphaα starting at position ii and ending at position j j (the symbols between 
these positions are in the order they have in alphaα). 
A string can be also seen as a distribution when any symbol of the alphabet is 
associated with all the positions where it occurs in the string. Given an ordering 
a less than b less than midline horizontal ellipsis less than za < b < · · · < z over the symbols of a string, the ordered normalization of a string 
alphaα is the string 
a Superscript StartAbsoluteValue alpha EndAbsoluteValue Super Subscript a Superscript Baseline b Superscript StartAbsoluteValue alpha EndAbsoluteValue Super Subscript b Superscript Baseline ellipsis z Superscript StartAbsoluteValue alpha EndAbsoluteValue Super Subscript z Superscripta|α|ab|α|b . . . z|α|z
whereStartAbsoluteValue alpha EndAbsoluteValue Subscript x|α|x is the number of times symbolxx occurs in stringalphaα and exponents denote 
the concatenation of copies of the symbol (a number of copies equal to the value of 
the exponent). 
The set of all substrings of a given string alphaα is deﬁned by: 
sub left parenthesis alpha right parenthesis equals left brace alpha left bracket i comma j right bracket StartAbsoluteValue 1 less than or equals i less than or equals j less than or equals EndAbsoluteValue alpha vertical bar right brace periodsub(α) = {α[i, j]|1 ≤i ≤j ≤|α|}.
and preﬁxes and sufﬁxes are given by: 
pref left parenthesis alpha right parenthesis equals left brace alpha left bracket 1 comma j right bracket StartAbsoluteValue 1 less than or equals j less than or equals EndAbsoluteValue alpha vertical bar right bracepref(α) = {α[1, j]|1 ≤j ≤|α|}
suff left parenthesis alpha right parenthesis equals left brace alpha left bracket i comma StartAbsoluteValue alpha EndAbsoluteValue right bracket StartAbsoluteValue 1 less than or equals i less than or equals EndAbsoluteValue alpha vertical bar right brace periodsuff(α) = {α[i, |α|]|1 ≤i ≤|α|}.
The most important operation over strings are: 
1. concatenation of alpha comma betaα, β, usually denoted by the juxtaposition alpha betaαβ The overlap 
concatenation of two stringsalpha gamma comma gamma betaαγ, γβ, isalpha gamma betaαγβ wheregammaγ is the maximum substring 
that is the sufﬁx of the ﬁrst string and the preﬁx of the second one; 
2. length usually denoted by the absolute value notation StartAbsoluteValue alpha EndAbsoluteValue|α|; 
3. preﬁx alpha left bracket 1 comma j right bracketα[1, j], with 1 less than or equals j less than or equals StartAbsoluteValue alpha EndAbsoluteValue1 ≤j ≤|α|; 
4. sufﬁx alpha left bracket j comma StartAbsoluteValue alpha EndAbsoluteValue right bracketα[ j, |α|], with 1 less than or equals j less than or equals StartAbsoluteValue alpha EndAbsoluteValue1 ≤j ≤|α|; 
5. substitutions u b s t Subscript fsubst f with respect to a functionf f from the alphabet ofalphaα to another 
(possibly the same) alphabet, where s u b s t Subscript f Baseline left parenthesis alpha right parenthesis left bracket j right bracket equals f left parenthesis alpha left bracket j right bracket right parenthesissubst f (α)[ j] = f (α[ j]) for 0 less than j less than or equals StartAbsoluteValue alpha EndAbsoluteValue0 < j ≤|α|; 
6. reverse r e v left parenthesis alpha right parenthesisrev(α), such that r e v left parenthesis alpha right parenthesis left bracket j right bracket equals alpha left bracket n minus j plus 1 right bracketrev(α)[ j] = α[n −j + 1]; 
7. empty string lamdaλ, such that, for every string alphaα: lamda alpha equals alpha lamda equals alphaλα = αλ = α. This string is very 
useful in expressing properties of strings and can be seen as a constant or nullary 
operation.

14
2
Basic Concepts
We remark the difference between substring and subsequence. Both notions 
select symbols occurring in a sequence in the order they are, but a subsequence does 
not assume that the chosen symbols are contiguous, while a substring ofalphaα is always 
of type alpha left bracket i comma j right bracketα[i, j] and selects all the symbols occurring in alphaα between the two speciﬁed 
positions (included). The number of all possible substrings of a string of length nn
has a quadratic order, whereas the number of all possible subsequences is 2 Superscript n2n. 
A class of strings over an alphabet upper AA is called a (formal) language (over the 
alphabet). Many operations are deﬁned over languages: the set-theoretic operations 
(union, intersection, difference, cartesian product) and more speciﬁc operations as 
concatenation upper L 1 upper L 2L1L2 (of two languages), iteration upper L Superscript nLn (n element of double struck upper Nn ∈N), and Kleene star upper L Superscript asteriskL∗: 
upper L 1 upper L 2 equals left brace alpha beta vertical bar alpha element of upper L 1 comma beta element of upper L 2 right braceL1L2 = {αβ | α ∈L1, β ∈L2}
upper L Superscript 0 Baseline equals StartSet lamda EndSetL0 = {λ}
upper L Superscript n plus 1 Baseline equals upper L upper L Superscript nLn+1 = LLn
upper L Superscript asterisk Baseline equals union Underscript i element of double struck upper N Endscripts upper L Superscript i Baseline periodL∗=
∪
i∈N
Li.
Algorithms generating languages are called grammars. Automata are algorithms 
recognizing languages or computing functions over strings. Grammars are intrin-
sically non-deterministic while recognizing automata can be deterministic or non-
deterministic. In the second case at each step many different consecutive steps are 
possible, and one of them is non-deterministically chosen. A string is recognized by 
such automata if for one of the possible computations the string is recognized. 
We recall that a Turing Machineupper MM [ 9] can be completely described by means of 
rewriting rules that specify as a tape conﬁguration can be transformed into another 
tape conﬁguration. A Turing conﬁguration is a string of type: 
alpha q x betaαqxβ
where alphaα is the string on the left of current position of upper MM tape, betaβ is the string on the 
right of this position, qq is the current state, and xx is the symbol of the tape cell that 
upper MM is reading. In this way, an instruction of upper MM such as q x y q prime upper Rqxyq'R (in the state qq and 
reading xx rewrite xx as yy and pass to state q primeq' moving on the cell to the right of the 
current cell) can be expressed by the following rewriting rule on conﬁgurations: 
alpha q x beta right arrow alpha y q prime betaαqxβ →αyq'β
whereas an instruction of upper MM such as q x y q prime upper Lqxyq'L (in the state qq and reading xx rewrite 
xx as yy and pass to state q primeq' moving on the cell to the left of the current cell) can be 
expressed by the following two rewriting rules on conﬁgurations: 
alpha q x beta right arrow alpha q prime Subscript asterisk Baseline y betaαqxβ →αq'
∗yβ

2.2 Strings and Rewriting
15
alpha x q prime Subscript asterisk Baseline beta right arrow alpha q prime x beta periodαxq'
∗β →αq'xβ.
The representation above of a Turing machine implies a very important property 
of the string rewriting mechanism: any process of string rewriting can be expressed 
by means of rules involving at most two consecutive symbols. In fact, the rules of 
the representation above involve the symbol of the state and only one symbol to 
the right or to the left of it. Moreover, according to the Church-Turing thesis, any 
computation can be realized by a suitable Turing machine, therefore any computation 
can be described by 2-symbol rewriting rules. This is a well-known result in formal 
language theory (Kuroda normal form representation). 
Universal Turing machinesupper UU exist that verify the following universality equation: 
upper U left parenthesis less than upper M comma alpha greater than right parenthesis equals upper M left parenthesis alpha right parenthesisU(< M, α >) = M(α)
where upper M left parenthesis alpha right parenthesisM(α) denotes the computation of Turing machine upper MM with the input string alphaα. 
The angular parentheses less than greater than< > denote an encoding of the pair given by machine upper MM
(its instructions) and input stringalphaα. Equality means that the left computation halts if 
and only if the right computation halts, and moreover, their ﬁnal conﬁgurations are 
essentially equal (being in a 1-to-1 correspondence). In 1956 [ 10] Shannon found a 
universal Turing machine [ 9, 11] working with only two symbols, say 11 and upper BB (the 
symbol upper BB for blank). 
An important class of grammars are the Chomsky Grammars essentially given 
by a set of rewriting rulesalpha right arrow betaα →β, wherealpha element of upper A Superscript asterisk Baseline minus upper T Superscript asterisk Baseline comma beta element of upper A Superscript asterisk Baseline comma upper S element of upper A minus upper T comma upper T subset of upper Aα ∈A∗−T ∗, β ∈A∗, S ∈A −T, T ⊂A
(upper TT is called the terminal alphabet), andright double arrow Subscript asterisk Baseline⇒∗is the rewriting relation between a string 
and any other string obtained by a chain of substring replacements where a left side 
of a rule of a grammar upper GG is replaced by its corresponding right side. The language 
determined by upper GG is given by: 
upper L left parenthesis upper G right parenthesis equals StartSet gamma element of upper T Superscript asterisk Baseline vertical bar upper S right double arrow Subscript asterisk Baseline gamma EndSet periodL(G) = {γ ∈T ∗|S ⇒∗γ }.
A Chomsky grammar is of type 0 if no restriction is given to its rules. 
Chomsky grammars of type 0 generate all the recursively enumerable (or semi-
decidable) languages, which belong to the class upper R upper ERE. For any language upper L element of upper R upper EL ∈RE an 
algorithm exists that provides a positive answer to the question alpha element of upper Lα ∈L if and only if 
alpha element of upper Lα ∈L. The class upper R upper E upper CREC of recursive (or decidable) languages is a proper subclass of 
upper R upper ERE such that, for anyupper L element of upper R upper E upper CL ∈REC an algorithm exists that provides a positive answer 
to the question alpha element of upper Lα ∈L if alpha element of upper Lα ∈L and a negative answer if alpha not an element of upper Lα /∈L (proper inclusion 
upper R upper E upper C subset of upper R upper EREC ⊂RE was one of the most important achievements of the epochal paper of 
Alan Turing, in 1936 [ 9]). 
Recursive enumerable languages coincide with the domains or codomains of 
Turing’s computable functions.

16
2
Basic Concepts
2.3 
Variables and Distributions 
Given a variableupper XX, we denote byModifyingAbove upper X With caret ̂X its set of variability or its range. It is important to 
remark that the setModifyingAbove upper X With caret ̂X of elements whereupper XX takes values can be completely unknown, 
or not relevant. 
When a probability is associated to a set of values assumed by variable upper XX, then 
upper XX is a random variable, which identiﬁes a probability distribution, where the whole 
unitary probability is distributed among the subsets of ModifyingAbove upper X With caret ̂X. 
The notion of distribution is very general and is a key concept in mathematical 
analysis, measure theory, probability, statistics, and information theory. Intuitively, 
a distribution speciﬁes as a quantity is distributed in parts among the points or the 
regions of a space. A very simple case of a ﬁnite distribution is a multiset of objects 
where a number mm of occurrences is distributed among kk objects where n 1n1 are the 
occurrences of the ﬁrst object, n 2n2 those of the second one, and so on, up to n Subscript knk, the  
occurrences of the last object, and: 
sigma summation Underscript i equals 1 ellipsis k Endscripts n Subscript i Baseline equals m period
∑
i=1...k
ni = m.
A discrete probability distribution is a family of real numbers p Subscript ipi, indexed in a set upper II
such that: 
sigma summation Underscript i element of upper I Endscripts p Subscript i Baseline equals 1 period
∑
i∈I
pi = 1.
2.4 
Events and Probability 
The theory of probability emerged in the 17th century, with some anticipations by 
the Italian mathematician Girolamo Cardano (1501–1576) (Liber de ludo aleae), 
Galileo (1564–1642) (Sopra le scoperte dei dadi, that is, About discoveries on dice), 
and Christian Huygens (1629–1695) (De ratiociniis in ludo aleae). The basic rules for 
computing probabilities were developed by Pascal (1623–1662) and Fermat (1601– 
1665). The ﬁrst treatise on this subject was the book Ars Conjectandi by Jacob 
Bernoulli (1654–1705), where binomial coefﬁcients appear in the probability of urn 
extractions, and the ﬁrst law of large numbers is enunciated (Theorema Aureus: 
frequencies approximate to probabilities in a large space of events). The idea of 
events to which degrees of possibility are assigned is a change of perspective where 
facts, as they are (modus essendi), are replaced by facts as they are judged (modus 
conjectandi). 
After Bernoulli’s work, De Moivre (1667–1754) and Laplace (1749–1827), found 
the normal distribution as a curve for approximating the binomial distribution. 
Bayes (1702–1752) discovered the theorem named by his name (independently 
discovered also by Laplace) ruling the inversion of conditional probabilities. Gauss

2.4 Events and Probability
17
(1777–1855) recognized the normal distribution as the law of error distribution, and 
Poisson (1781–1840) introduced his distribution ruling the law of rare events. 
An event can be expressed by means of a proposition asserting that a variableupper XX
assumes a value belonging to a given subset. This means that the usual propositional 
operationsnormal not sign comma and comma or¬, ∧, ∨(not, and, or) are deﬁned on events. A space of events is a special 
boolean algebra (with 0, 1, sum, product, and negation) 
A probability measure can be easily assigned to an event, as a value in the real 
interval [0, 1], which can be seen as a sort of evaluation of the possibility that the 
event has of happening. 
The theory of probability concerns two different aspects: 
(i) the study of probabilistic laws held in spaces of events, 
(ii) the determination of the space of events which is more appropriate in a given 
context. 
The ﬁrst aspect constitutes a conceptual framework which is often independent 
of the speciﬁc spaces of events. A comparison may help to distinguish the two 
aspects. Calculus and the theory of differential equations provide rules and methods 
for solving and analyzing differential equations, but the choice of the right equation 
which is the best description of a physical phenomenon is a different thing, which 
pertains to the ability to correctly model phenomena of a certain type. 
The axiomatic approach in probability theory was initiated by Andrej Nikolaeviˇc 
Kolmogorov (1903–1987) and was developed by the Russian mathematical school 
(Andrey Markov, Pafnuty Chebyshev, Aleksandr Yakovlevich Khinchin). It is com-
parable to the axiomatic approach in geometry, and it is important for understanding 
the logical basis of probability. From the mathematical point of view, probability 
theory is part of a general ﬁeld of mathematics, referred to as Measure theory, ini-
tiated by French mathematicians of the last century (Émile Borel, Henri Lebesgue, 
Nikolai Luzin, Johann Radon, Constantin Carathéodory, and Maurice Fréchet). 
Basic rules of probability 
The conditional probability of an eventupper AA, given an eventupper BB, is denoted byupper P left parenthesis upper A vertical bar upper B right parenthesisP(A|B). 
It expresses the probability of upper AA under the assumption that event upper BB has occurred. 
Formally (propositional connectives or set-theoretic operations are used on events): 
upper P left parenthesis upper A vertical bar upper B right parenthesis equals StartFraction upper P left parenthesis upper A and upper B right parenthesis Over upper P left parenthesis upper B right parenthesis EndFraction periodP(A|B) = P(A ∧B)
P(B)
.
Events upper AA and upper BB are said to be  independent, and we write upper A StartAbsoluteValue EndAbsoluteValue upper BA||B, if  upper P left parenthesis upper A vertical bar upper B right parenthesis equals upper P left parenthesis upper A right parenthesisP(A|B) =
P(A). Events upper AA and upper BB are disjoint if upper P left parenthesis upper A and upper B right parenthesis equals 0P(A ∧B) = 0. The following rules connect 
propositional operations to probabilities. Proposition normal not sign upper A¬A has to be considered in 
terms of complementary set, that is if upper A equals left parenthesis upper X element of upper S right parenthesisA = (X ∈S), then normal not sign upper A equals left parenthesis upper X element of ModifyingAbove upper X With caret minus upper S right parenthesis¬A = (X ∈ ̂X −S).

18
2
Basic Concepts
1. upper P left parenthesis upper A right parenthesis greater than or equals 0P(A) ≥0
2. upper P left parenthesis upper A or normal not sign upper A right parenthesis equals 1P(A ∨¬A) = 1
3. upper P left parenthesis upper A and normal not sign upper A right parenthesis equals 0P(A ∧¬A) = 0
4. upper P left parenthesis normal not sign upper A right parenthesis equals 1 minus upper P left parenthesis upper A right parenthesisP(¬A) = 1 −P(A)
5. upper P left parenthesis upper A or upper B right parenthesis equals upper P left parenthesis upper A right parenthesis plus upper P left parenthesis upper B right parenthesis minus upper P left parenthesis upper A and upper B right parenthesisP(A ∨B) = P(A) + P(B) −P(A ∧B)
6. upper P left parenthesis upper A vertical bar upper B right parenthesis equals upper P left parenthesis upper A and upper B right parenthesis divided by upper P left parenthesis upper B right parenthesisP(A|B) = P(A ∧B)/P(B)
7. upper A StartAbsoluteValue EndAbsoluteValue upper B left right double arrow upper P left parenthesis upper A and upper B right parenthesis equals upper P left parenthesis upper A right parenthesis upper P left parenthesis upper B right parenthesisA||B ⇔P(A ∧B) = P(A)P(B). 
The theory of probability is the ﬁeld where even professional mathematicians can 
be easily wrong, and very often reasoning under probabilistic hypotheses is very 
slippery. Let us consider the following examples from [ 12]. A military pilot has a 
2 percent sign2% chance of being shot down in each military ﬂight. What is the probability to die 
in ﬁfty ﬂights? We might guess that he is almost sure to die. But this is completely 
wrong. The probability of dying is the sum of the probabilities of dying at the ﬁrst, 
at the second, and so on, up to the probability of dying at the last mission. Let us 
denote by pp the probability of dying, then the sum of probabilities in all the ﬂights 
is (the ﬂight ii is possible only if the pilot survives in the preceding ﬂights): 
p plus left parenthesis 1 minus p right parenthesis p plus left parenthesis 1 minus p right parenthesis squared p plus ellipsis plus left parenthesis 1 minus p right parenthesis Superscript 49 Baseline pp + (1 −p)p + (1 −p)2 p + . . . + (1 −p)49 p
A shorter evaluation of this probability (completely equivalent to the equation 
above) is the probability of surviving0.98 Superscript 500.9850 subtracted to 1. Therefore, the probability 
of dying is 1 minus 0.98 Superscript 50 Baseline equals 0.641 −0.9850 = 0.64, which is very different from 11. 
A similar error was the origin of the problem posed by the Chevalier de Méré 
to Pascal, about a popular dice game: Why the probability of one ace, rolling one 
die 4 times, is greater than that of both aces, rolling two dice 24 times? In fact, 
the probability of one ace is 1 divided by 61/6, and 4 divided by 6 equals 2 divided by 34/6 = 2/3, analogously the probability of 2 
aces is 1 divided by 361/36, and 24 divided by 36 equals 2 divided by 324/36 = 2/3, so we may suppose that the two events: “1 ace in 
4 rolls”, and “2 aces in 24 double rolls” are equiprobable. But the empirical evi-
dence reported by Chevalier de Méré was against this conclusion. Namely, Pascal 
(discussing with Fermat) solved the apparent paradox. In the ﬁrst game, P(no-ace-
in-4-tolls) = left parenthesis 5 divided by 6 right parenthesis Superscript 4(5/6)4, therefore P(ace-in-4-rolls)= 1 minus left parenthesis 5 divided by 6 right parenthesis Superscript 4 Baseline equals 0.51771 −(5/6)4 = 0.5177. In the sec-
ond game, P(no-2-aces-in-24-double-rolls)= left parenthesis 35 divided by 36 right parenthesis Superscript 24(35/36)24, therefore P(2-aces-in-24-
double-rolls) = 1 minus left parenthesis 35 divided by 36 right parenthesis Superscript 24 Baseline equals 0.49141 −(35/36)24 = 0.4914. The simple mistake, as in the case of the 
military pilot, is due to the sum of probabilities that are not disjoint (by ignoring rule 
5 given above). 
Bayes’ theorem 
Cases of wrong probability evaluations are very frequent in contexts where condi-
tional probabilities are present. Bayes’ theorem, explains the nature of this difﬁculty 
because it establishes the rule for inverting conditional probabilities. 
In a simpliﬁed form, Bayes’ theorem asserts the following equation:

2.4 Events and Probability
19
Proposition 2.2 
upper P left parenthesis upper A vertical bar upper B right parenthesis equals upper P left parenthesis upper A right parenthesis upper P left parenthesis upper B vertical bar upper A right parenthesis divided by upper P left parenthesis upper B right parenthesis periodP(A|B) = P(A)P(B|A)/P(B).
Proof By the deﬁnition of conditional probability, we have: 
upper P left parenthesis upper A and upper B right parenthesis equals upper P left parenthesis upper A vertical bar upper B right parenthesis upper P left parenthesis upper B right parenthesisP(A ∧B) = P(A|B)P(B)
analogously: 
upper P left parenthesis upper A and upper B right parenthesis equals upper P left parenthesis upper B and upper A right parenthesis equals upper P left parenthesis upper B vertical bar upper A right parenthesis upper P left parenthesis upper A right parenthesisP(A ∧B) = P(B ∧A) = P(B|A)P(A)
therefore, by equating the right members of the two equations above we obtain: 
upper P left parenthesis upper A vertical bar upper B right parenthesis upper P left parenthesis upper B right parenthesis equals upper P left parenthesis upper B vertical bar upper A right parenthesis upper P left parenthesis upper A right parenthesisP(A|B)P(B) = P(B|A)P(A)
from which the statement claimed by the theorem easily follows.
□
Despite the simplicity of this proof, what the theorem asserts is an inversion of 
conditions. In fact, upper P left parenthesis upper A vertical bar upper B right parenthesisP(A|B) can be computed by means of upper P left parenthesis upper B vertical bar upper A right parenthesisP(B|A). Assume that a 
test upper TT for a disease upper DD is wrong only in one out of 1000 cases. When a person is 
positive on this test, what is the probability of having D? Is it 1 minus 0.001 equals 0.9991 −0.001 = 0.999? 
No. In fact, this value confuses upper P left parenthesis upper D vertical bar upper T right parenthesisP(D|T ) with upper P left parenthesis upper T vertical bar upper D right parenthesisP(T |D). The right way to reason is 
the application of Bayes’ theorem; let us assume to know that upper DD affects one out of 
10000 persons, soupper P left parenthesis upper D right parenthesis equals 1 divided by 10000P(D) = 1/10000, and thatupper TT is very reliable, withupper P left parenthesis upper T vertical bar upper D right parenthesis equals 1P(T |D) = 1. 
The probability upper P left parenthesis upper T right parenthesisP(T ) is 11 divided by 1000011/10000 because upper TT is wrong from 1 to 1000, and in only 
one out of 10000 persons with the disease D, test upper TT is positive. In conclusion, the 
probability of having upper DD is less than 10%. In fact: 
upper P left parenthesis upper D vertical bar upper T right parenthesis equals upper P left parenthesis upper D right parenthesis upper P left parenthesis upper T vertical bar upper D right parenthesis divided by upper P left parenthesis upper T right parenthesis equals 1 divided by 10000 times 1 divided by left parenthesis 11 divided by 10000 right parenthesis equals 1 divided by 11 periodP(D|T ) = P(D)P(T |D)/P(T ) = 1/10000 × 1/(11/10000) = 1/11.
Statistical indexes and Chebicev’s inequality 
Let muμ the mean of a random variable upper XX assuming real values x 1 comma x 2 comma ellipsis x Subscript n Baselinex1, x2, . . . xn with 
probabilitiesp 1 comma p 2 comma ellipsis p Subscript n Baselinep1, p2, . . . pn respectively, thenmu μ, denoted also byupper E left parenthesis upper X right parenthesisE(X), is deﬁned by 
the following equation: 
upper E left parenthesis upper X right parenthesis equals sigma summation Underscript i Endscripts p Subscript i Baseline x Subscript iE(X) =
∑
i
pixi
the second order moment of upper XX is given by: 
upper E left parenthesis upper X squared right parenthesis equals sigma summation Underscript i Endscripts p Subscript i Baseline x Subscript i Superscript 2E(X2) =
∑
i
pix2
i

20
2
Basic Concepts
and in an analogous way, moments of higher orders can be deﬁned. the variance of 
upper XX is given by the second-order momentum of the deviation from the mean, that is: 
upper V a r left parenthesis upper X right parenthesis equals upper E left bracket left parenthesis upper X minus mu right parenthesis squared right parenthesis right bracket equals sigma summation Underscript i Endscripts p Subscript i Baseline left parenthesis x Subscript i Baseline minus mu right parenthesis squared periodVar(X) = E[(X −μ)2)] =
∑
i
pi(xi −μ)2.
The following equation holds: 
v a r left parenthesis upper X right parenthesis equals upper E left parenthesis upper X squared right parenthesis minus left bracket upper E left parenthesis upper X right parenthesis right bracket squared periodvar(X) = E(X2) −[E(X)]2.
In fact, from deﬁnitions, we obtain that: 
StartLayout 1st Row 1st Column sigma summation Underscript i Endscripts p Subscript i Baseline left parenthesis x Subscript i Baseline minus mu right parenthesis squared 2nd Column equals sigma summation Underscript i Endscripts p Subscript i Baseline x Subscript i Superscript 2 Baseline plus sigma summation Underscript i Endscripts p Subscript i Baseline mu squared minus 2 mu sigma summation Underscript i Endscripts p Subscript i Baseline x Subscript i Baseline 2nd Row 1st Column Blank 2nd Column equals upper E left parenthesis upper X squared right parenthesis plus mu squared minus 2 mu squared equals upper E left parenthesis upper X squared right parenthesis minus left bracket upper E left parenthesis upper X right parenthesis right bracket squared period EndLayout
∑
i
pi(xi −μ)2 =
∑
i
pix2
i +
∑
i
piμ2 −2μ
∑
i
pixi
= E(X2) + μ2 −2μ2 = E(X2) −[E(X)]2.
The standard deviation of upper XX, s d left parenthesis upper X right parenthesissd(X) is deﬁned by: 
s d left parenthesis upper X right parenthesis equals StartRoot upper V a r left parenthesis upper X right parenthesis EndRoot periodsd(X) =
√
Var(X).
The following theorem states a basic constraint of any statistical distribution: given 
a random variable upper XX, the probability that upper XX assume a value differing, in absolute 
value, more than tt from the mean mu μ is less than t Superscript negative 2t−2 times the variance of upper XX. 
Proposition 2.3 (Chebichev Theorem) 
upper P left parenthesis StartAbsoluteValue upper X minus mu EndAbsoluteValue greater than t right parenthesis less than or equals t Superscript negative 2 Baseline upper E left bracket left parenthesis upper X minus mu right parenthesis squared right bracketP(|X −μ| > t) ≤t−2E[(X −μ)2]
Proof 
upper P left parenthesis StartAbsoluteValue upper X EndAbsoluteValue greater than t right parenthesis less than or equals t Superscript negative 2 Baseline upper E left parenthesis upper X squared right parenthesisP(|X| > t) ≤t−2E(X2)
In fact: 
sigma summation Underscript StartAbsoluteValue x EndAbsoluteValue greater than or equals t Endscripts p left parenthesis x right parenthesis less than or equals sigma summation Underscript StartAbsoluteValue x EndAbsoluteValue greater than or equals t Endscripts p left parenthesis x right parenthesis StartFraction x squared Over t squared EndFraction less than or equals t Superscript negative 2 Baseline upper E left parenthesis upper X squared right parenthesis
∑
|x|≥t
p(x) ≤
∑
|x|≥t
p(x)x2
t2 ≤t−2E(X2)
whence, replacing upper XX by left parenthesis upper X minus mu right parenthesis(X −μ) the thesis holds.
□
A direct consequence of the theorem above is the so-called Law of large numbers 
in its weak form, originally proved by Bernoulli, stating that in a boolean sequence 
of successes and failures, the frequency of successes rends to the success probability 
according to which the sequence is generated. In formal terms. letupper S left parenthesis alpha Subscript n Baseline right parenthesisS(αn) be a boolean 
sequence generated by a nn Bernoulli boolean variables upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n BaselineX1, X2, . . . , Xn all with a 
probability of success (that is of having value 1) equal to pp. Let  upper S left parenthesis alpha Subscript n Baseline right parenthesis equals upper S Subscript nS(αn) = Sn the

2.4 Events and Probability
21
number of successes inalpha Subscript nαn,upper S Subscript n Baseline equals upper X 1 plus upper X 2 comma ellipsis comma plus upper X Subscript n BaselineSn = X1 + X2, . . . , +Xn andnormal upper Omega Subscript nΩn the set of all sequences 
of length nn. The following limit holds. 
Proposition 2.4 
for all epsilon element of double struck upper R Superscript plus Baseline colon limit Underscript n right arrow normal infinity Endscripts upper P left brace alpha Subscript n Baseline element of normal upper Omega Subscript n Baseline colon StartAbsoluteValue upper S Subscript n Baseline divided by n minus p EndAbsoluteValue greater than epsilon right brace equals 0∀ε ∈R+ : lim
n→∞P{αn ∈Ωn : |Sn/n −p| > ε} = 0
Proof The mean of a Bernoulli boolean variable upper XX is upper E left parenthesis upper X right parenthesis equals 1 times p plus 0 times left parenthesis 1 minus p right parenthesis equals pE(X) = 1 × p + 0 × (1 −
p) = p. Therefore being upper S Subscript nSn the sum of nn independent variables upper E left parenthesis upper S Subscript n Baseline right parenthesis equals n pE(Sn) = np. Anal-
ogously, the square deviation of any boolean random variable upper XX is (q equals 1 minus pq = 1 −p): 
left parenthesis 1 minus p right parenthesis squared p plus left parenthesis 0 minus p right parenthesis squared q equals q squared p plus p squared q equals q p left parenthesis q plus p right parenthesis equals q p(1 −p)2 p + (0 −p)2q = q2 p + p2q = qp(q + p) = qp
therefore the sum ofnn independent boolean variables with success probabilitypp has 
square deviation n p q equals n p left parenthesis 1 minus p right parenthesisnpq = np(1 −p). Fix epsilonε, then by Chebichev’s inequality: 
upper P left brace alpha Subscript n Baseline element of normal upper Omega Subscript n Baseline colon StartAbsoluteValue StartFraction upper S Subscript n Baseline Over n EndFraction minus upper E left parenthesis upper S Subscript n Baseline right parenthesis EndAbsoluteValue greater than epsilon right brace less than or equals StartFraction upper V a r left parenthesis upper S Subscript n Baseline right parenthesis Over epsilon squared EndFraction periodP{αn ∈Ωn : | Sn
n −E(Sn)| > ε} ≤Var(Sn)
ε2
.
The mean value and the variance of StartFraction upper S Subscript n Baseline Over n EndFraction Sn
n are: 
upper E left parenthesis StartFraction upper S Subscript n Baseline Over n EndFraction right parenthesis equals StartFraction upper E left parenthesis upper S Subscript n Baseline right parenthesis Over n EndFraction equals pE
( Sn
n
)
= E(Sn)
n
= p
upper V a r left parenthesis StartFraction upper S Subscript n Baseline Over n EndFraction right parenthesis equals StartFraction upper V a r left parenthesis upper S Subscript n Baseline right parenthesis Over n squared EndFraction equals StartFraction n p left parenthesis 1 minus p right parenthesis Over n squared EndFraction equals StartFraction p left parenthesis 1 minus p right parenthesis Over n EndFractionVar
( Sn
n
)
= Var(Sn)
n2
= np(1 −p)
n2
= p(1 −p)
n
thus, we obtain: 
upper P left brace alpha Subscript n Baseline element of normal upper Omega Subscript n Baseline colon StartAbsoluteValue StartFraction upper S Subscript n Baseline Over n EndFraction minus p EndAbsoluteValue greater than epsilon right brace less than or equals StartFraction p left parenthesis 1 minus p right parenthesis Over n epsilon squared EndFractionP{αn ∈Ωn : | Sn
n −p| > ε} ≤p(1 −p)
nε2
so, passing to the limit: 
limit Underscript n right arrow normal infinity Endscripts upper P left brace alpha Subscript n Baseline element of normal upper Omega Subscript n Baseline colon StartAbsoluteValue StartFraction upper S Subscript n Baseline Over n EndFraction minus p EndAbsoluteValue greater than epsilon right brace less than or equals 0 lim
n→∞P{αn ∈Ωn : | Sn
n −p| > ε} ≤0
but the probability cannot be negative, therefore: 
limit Underscript n right arrow normal infinity Endscripts upper P left brace alpha Subscript n Baseline element of normal upper Omega Subscript n Baseline colon StartAbsoluteValue StartFraction upper S Subscript n Baseline Over n EndFraction minus p EndAbsoluteValue greater than epsilon right brace equals 0 period lim
n→∞P{αn ∈Ωn : | Sn
n −p| > ε} = 0.
□
Besides, mean, momenta, variance, and standard deviation, the correlation coefﬁ-
cient is an important index for expressing the degree of reciprocal inﬂuence between 
two variables, on the basis of the distributions of their values. Correlation is based on 
the notion of covariance that is expressed by the mean of the product of deviations 
(mu μ and nu ν are the means of upper XX and upper YY, respectively):

22
2
Basic Concepts
c o v left parenthesis upper X comma upper Y right parenthesis equals upper E left bracket left parenthesis upper X minus mu right parenthesis left bracket upper Y minus nu right parenthesis right bracket periodcov(X, Y) = E[(X −μ)[Y −ν)].
Then, the correlation coefﬁcient rho left parenthesis upper X comma upper Y right parenthesisρ(X, Y) is given by (assuming a ﬁnite variance 
for upper XX and upper YY): 
rho left parenthesis upper X comma upper Y right parenthesis equals StartFraction c o v left parenthesis upper X comma upper Y right parenthesis Over upper V a r left parenthesis upper X right parenthesis upper V a r left parenthesis upper Y right parenthesis EndFraction periodρ(X, Y) =
cov(X, Y)
Var(X)Var(Y).
The intuition concerning covariance is very simple. If two variables deviate from 
their means in a coherent way they are related, and the product of the deviations 
is positive when their deviations agree and is negative otherwise. Moreover, the 
agreements and disagreements are reasonably weighted by the probabilities with 
which they occur. The denominator in the correlation ratio is introduced to normalize 
the covariance with respect to the variances of the two variables. 
References 
1. Manca, V.: Il Paradiso di Cantor. La costruzione del linguaggio matematico, Edizioni Nuova 
Cultura (2022) 
2. Manca, V., Arithmoi. Racconto di numeri e concetti matematici fondamentali (to appear) 
3. Fränkel, A.A.: Set Theory and Logic. Addison-Wesley Publishing Company (1966) 
4. Halmos, P.: Axiomatic Set Theory. North-Holland (1964) 
5. Manca, V.: Formal Logic. In: Webster, J.G. (ed.) Encyclopedia of Electrical and Electronics 
Engineering, vol. 7, pp. 675–687. Wiley, New York (1999) 
6. Manca, V.: Logical string rewriting. Theor. Comput. Sci. 264, 25–51 (2001) 
7. Aigner, M.: Discrete Mathematics. American Mathematical Society, Providence, Rhode Island 
(2007) 
8. Rozenberg, G., Salomaa, A.: Handbook of Formal Languages: Beyonds words, vol. 3. Springer, 
Berlin (1997) 
9. Turing, A.M.: On computable numbers, with an application to the Entscheidungsproblem. Proc. 
Lond. Math. Soc. 42(1), 230–265 (1936) 
10. Shannon, C.E.: A universal Turing machine with two internal states. In: Automata Studies, 
Annals of Mathematics Studies, vol. 34, pp. 157–165. Princeton University Press (1956) 
11. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice-Hall Inc. Englewood Cliffs, 
N. J. (1967) 
12. Aczel, A.D.: Chance. Thunder’s Mouth Press, New York (2004)

Chapter 3 
Information Theory 
Introduction 
Information theory “ofﬁcially” begins with Shannon’s booklet [ 1] published in 1948. 
The main idea of Shannon is linking information with probability. In fact, the starting 
deﬁnition of this seminal work is that of information source as a pairleft parenthesis upper X comma p right parenthesis(X, p), whereupper XX
is a ﬁnite set of objects (data, signals, words) andpp is a probability function assigning 
to everyx element of upper Xx ∈X the probabilityp left parenthesis x right parenthesisp(x) of occurrence (emission, reception, production). 
The perspective of this approach is the mathematical analysis of communication 
processes, but its impact is completely general and expresses the probabilistic nature 
of the information. Information is an inverse function of probability. because it is a 
sort of a posteriori counterpart of the a priori uncertainty represented by probability, 
measuring the gain of knowledge when an event occurs. For this reason, the more an 
event is rare, the more it is informative. However, if event upper EE has probability p Subscript upper EpE, for  
several technical reason it is better to deﬁne i n f left parenthesis upper E right parenthesisinf (E) as log base 10 left parenthesis 1 divided by p Subscript upper E Baseline right parenthesis equals minus log base 10 left parenthesis p Subscript upper E Baseline right parenthesislg(1/pE) = −lg(pE) rather 
than 1 divided by p Subscript upper E1/pE. In fact, the logarithm guarantees the information additivity for a joint 
event left parenthesis upper E comma upper E prime right parenthesis(E, E') where components are independent, giving i n f left parenthesis upper E comma upper E Superscript prime Baseline right parenthesis equals i n f left parenthesis upper E right parenthesis plus i n f left parenthesis upper E prime right parenthesisinf (E, E') = inf (E) +
inf (E'). However, it is important to remark that the relationship between information 
and probability is in both verses because as Bayesian approaches make evident, 
information can change the probability evaluation (conditional probability, on which 
Bayes theorem is based, deﬁnes how probability changes when we know that an event 
occurred). A famous example of this phenomenon is the three-doors (or Monty Hall) 
dilemma [ 2], which can be fully explained by using the Bayes theorem. 
In this chapter, we give a quick overview of the basic concepts in Information 
Theory. The reader is advised to refer to [ 1, 3, 4] for more details on Information 
and Probability theories. Some speciﬁc concepts and facts about probability will be 
more exhaustively considered in the next chapter. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_3 
23

24
3
Information Theory
3.1 
From Physical to Informational Entropy 
Shannon founded Information Theory [ 1, 3] on the notion of Information Source, 
a pair left parenthesis upper A comma p right parenthesis(A, p), given by a set of data upper AA and a probability distribution pp deﬁned 
on upper AA, or even, a pair left parenthesis upper X comma p Subscript upper X Baseline right parenthesis(X, pX) of a variable upper XX assuming values with an associated 
probability distribution p Subscript upper XpX (upper AA is the set on which upper XX assumes values). In this case, 
only the variableupper XX can be indicated ifp Subscript upper XpX is implicitly understood. In this framework, 
Shannon deﬁned a measure of information quantity of the eventupper X equals aX = a (upper XX assumes 
the value aa): 
upper I n f left parenthesis upper X equals a right parenthesis equals minus log base 10 Subscript 2 Baseline left parenthesis p Subscript upper X Baseline left parenthesis a right parenthesis right parenthesisInf (X = a) = −lg2(pX(a))
wherep Subscript upper X Baseline left parenthesis a right parenthesispX(a) is the probability of the eventleft parenthesis upper X equals a right parenthesis(X = a). The intuitive motivation for the 
equation above is that the information quantity associated with an event is inversely 
proportional to the probability of the event, and moreover, the information quantity 
has to be additive for pairs of independent events (p left parenthesis a 1 comma a 2 right parenthesis equals p left parenthesis a 1 right parenthesis p left parenthesis a 2 right parenthesisp(a1, a2) = p(a1)p(a2)): 
upper I n f left parenthesis upper E 1 comma upper E 2 right parenthesis equals upper I n f left parenthesis upper E 1 right parenthesis plus upper I n f left parenthesis upper E 2 right parenthesis periodInf (E1, E2) = Inf (E1) + Inf (E2).
On the basis of this deﬁnition, ifModifyingAbove upper X With caret ̂X denotes the range of the variableupper XX, the  entropy 
of the information source left parenthesis upper X comma p Subscript upper X Baseline right parenthesis(X, pX) is deﬁned by: 
upper H left parenthesis upper X comma p Subscript upper X Baseline right parenthesis equals sigma summation Underscript a element of ModifyingAbove upper X With caret Endscripts p Subscript upper X Baseline left parenthesis a right parenthesis upper I n f left parenthesis upper X equals a right parenthesis equals minus sigma summation Underscript a element of ModifyingAbove upper X With caret Endscripts p Subscript upper X Baseline left parenthesis a right parenthesis log base 10 Subscript 2 Baseline p Subscript upper X Baseline left parenthesis a right parenthesis periodH(X, pX) =
∑
a∈ ̂X
pX(a)Inf (X = a) = −
∑
a∈ ̂X
pX(a) lg2 pX(a).
Therefore, the entropy of an information source is the mean (in a probabilistic sense) 
of the information quantity of the events generated by the information sourceleft parenthesis upper X comma p Subscript upper X Baseline right parenthesis(X, pX). 
One crucial result about entropy is the Equipartition Property (proved by Shan-
non in an appendix to his booklet): in the class of variables assuming the same value 
of upper XX the value of the entropy reaches its maximum with a source left parenthesis upper Y comma q Subscript upper Y Baseline right parenthesis(Y, qY) where q Subscript upper YqY
is the uniform probability distribution (all the values of variableupper YY are assumed with 
the same values). 
Let us recall that Shannon’s idea has very ancient historical roots (probably 
unknown to Shannon). The encoding method “Caesar” (used by Julius Caesar) is a 
very simple way of encoding messages for hiding their contents so that only whoever 
knows a deciphering key can access to them. This method is based on a one-to-one 
function assigning a letterf left parenthesis upper X right parenthesis f (X) to a letterupper XX. Given a text, if you replace each letter 
by using this function, then the message becomes unreadable unless you use the 
inverse of f f for recovering the original message. In the eightieth century, the Ara-
bic mathematician Al-Kindi discovered a method for cracking “Caesar” encoding, 
entirely based on frequencies. Let us assume to know the language of the original 
messages, and to collect a great number of encrypted messages. In any language, 
the frequency of each letter is almost constant (especially if it is evaluated for long 
texts). Then, if we compute the frequencies of letters in the encrypted messages, it 
results that, with a very good approximation, we can guess, for every letter of the

3.1 From Physical to Informational Entropy
25
alphabet which is the letter in the encrypted texts having a similar frequency. This 
letter is, with a good probability the letter corresponding to the letter of plain texts, 
in the encrypted messages. In this way, the deciphering keyf f can be discovered and 
messages can be disclosed. 
The consequence of this deciphering method is that the essence of any symbol in 
a symbolic system is intrinsically related to its frequency, which is a special case of 
probability (an empirical probability). For this reason, information is directly related 
to probability. 
The notion of informational entropy is strongly related to thermodynamic entropy 
upper SS, which emerged in physics since Carnot’s analysis of heat-work conversion [ 5] and 
was named by Clausius “entropy” (from Greek en-tropos meaning “internal verse”). 
Physical entropy is subjected to a famous inequality stating the Second Principle of 
Thermodynamics for isolated systems (systems that do not exchange energy with 
their environment), entropy cannot decrease in time: 
Delta upper S greater than or equals 0 periodΔS ≥0.
In years 1870s, Ludwig Boltzmann started a rigorous mathematical analysis of a 
thermodynamic system consisting of ideal gas within a volume, aimed at explaining 
the apparent contradiction of the inequality above with respect to the equations of 
Newtonian mechanics, underlying the microscopic reality on which heat phenomena 
are based on. More precisely, Boltzmann’s question was: “From where the inequality 
comes from, if molecules colliding in a gas follow mechanics equational laws with 
no intrinsic time arrow?” 
For answering the above question, Boltzmann introduced, in a systematic way, a 
probabilistic perspective in the microscopic analysis of physical phenomena. For its 
importance, his approach transcends the particular ﬁeld of his investigation. In fact, 
after Boltzmann, probability and statistics [ 4, 6, 7] became crucial aspects of any 
modern physical theory. Boltzmann deﬁned a function upper HH, which in discrete terms 
can be expressed by: 
upper H equals sigma summation Underscript i equals 1 comma n Endscripts n Subscript i Baseline log base 10 Subscript 2 Baseline n Subscript iH =
∑
i=1,n
ni lg2 ni
wheren Subscript ini are the number of gas molecules having velocities in theii-class of velocities 
(velocities are partitioned in nn disjoint intervals). 
By simple algebraic passages, it turns out (details are given in Section 5.4) that 
the upper HH function coincides, apart from additive and multiplicative constants, with 
Shannon’s entropy (Shannon quotes Boltzmann’s work). On the basis ofupper HH function 
Boltzmann proved the microscopic representation of Clausius entropy upper SS [ 8]. 
upper S equals k log Subscript e Baseline upper WS = k loge W
(3.1) 
where upper WW is the number of distinguishable micro-states associated with the thermo-
dynamic macro-state of a given system (two micro-states are indistinguishable if

26
3
Information Theory
the same velocity distribution is realized apart from the speciﬁc identities of single 
molecules) and kk is the so-called Boltzmann constant. 
However, despite this remarkable result, Boltzmann was not able to deduce that 
Delta upper S greater than or equals 0ΔS ≥0, by using  H function. The so-called Theorem H statingupper H Subscript t plus 1 Baseline less than or equals upper H Subscript tHt+1 ≤Ht, orDelta upper H less than or equals 0ΔH ≤
0, was never proved by him in a satisfactory way (fromDelta upper H less than or equals 0ΔH ≤0 inequalityDelta upper S greater than or equals 0ΔS ≥0
follows as an easy consequence of the above equation upper S equals k log Subscript e Baseline upper WS = k loge W, where the 
opposite verses of inequalities are due to the different signs of Boltzmann’s H and 
Shannon’s H) [  5, 9– 12]. 
The microscopic interpretation of thermodynamic entropy given by Boltzmann 
has a very general nature expressed by the Entropy Circular Principle ECP, cor-
responding to Boltzmann’s “Wahrscheinlichkeit” Principle. In terms of Shannon 
information sources, ECP can be formulated by the following statement. 
The entropy of a sourceleft parenthesis upper X comma p right parenthesis(X, p) is proportional to the logarithm of the numberupper WW of 
different sources generating the same probability distribution pp, that is, the entropy 
of a given source is proportional to the number of sources having that entropy. 
Entropy Circular Principle (ECP) 
upper H left parenthesis upper X comma p Subscript upper X Baseline right parenthesis equals c log base 10 Subscript 2 Baseline StartAbsoluteValue double struck upper X EndAbsoluteValueH(X, pX) = c lg2 |X|
for some constant cc, where: 
double struck upper X equals left brace left parenthesis upper Y comma p Subscript upper Y Baseline right parenthesis vertical bar ModifyingAbove upper X With caret equals ModifyingAbove upper Y With caret comma p Subscript upper X Baseline equals p Subscript upper Y Baseline right braceX = {(Y, pY) |  ̂X =  ̂Y , pX = pY}
From this principle will emerge an interesting link between probabilistic and dig-
ital aspects of information. In fact, it can be proved thatlog base 10 Subscript 2 Baseline nlg2 n corresponds essentially 
to the minimum average number of binary digits necessary for encoding nn differ-
ent values (the exact value d i g i t 2 left parenthesis n right parenthesisdigit2(n) is bounded by: left parenthesis left floor l g 2 left parenthesis n right parenthesis right floor minus 2 right parenthesis less than d i g i t 2 left parenthesis n right parenthesis less than n left parenthesis left floor l g 2 left parenthesis n right parenthesis right floor right parenthesis([lg2(n)] −2) < digit2(n) <
n([lg2(n)])). Therefore, entropy corresponds (apart from multiplicative constants) 
to the average length of codewords encoding all the sources with the same entropy. 
This digital reading of the entropy is also related to the First Shannon Theorem, 
stating that upper H left parenthesis upper X comma p Subscript upper X Baseline right parenthesisH(X, pX) is a lower bound for the (probabilistic) mean of yes-no ques-
tions necessary to guess a value a element of ModifyingAbove upper X With careta ∈ ̂X by asking a less than or equals b question marka ≤b?, for suitable values bb in 
ModifyingAbove upper X With caret ̂X. This explains in a very intuitive manner the meaning of upper H left parenthesis upper X comma p right parenthesisH(X, p), as the average 
uncertainty of the source left parenthesis upper X comma p right parenthesis(X, p). 
In this sense, entropy is, at the same time, the average uncertainty of the events 
generated by the source, and at the sane time, the average uncertainty in identifying 
the source, in the class of those with the same probability distribution. The uncertainty 
internal to the space of events coincides with the external uncertainty of the event 
space, among all possible event spaces. 
In this perspective, the ECP principle unveils the intrinsic circularity of entropy, 
becauseupper H left parenthesis upper X comma p Subscript upper X Baseline right parenthesisH(X, pX) is completely determined byp Subscript upper XpX but, at the same time, corresponds 
to the (logarithm of the number of) ways of realizing distribution p Subscript upper XpX by random 
variables assuming the same values of upper XX. 
The reason for Boltzmann’s failure in correctly proving upper HH theorem is due to 
the fact that this theorem is, in its essence, an information Theory theorem (see [ 8,

3.2 Entropy and Computation
27
12] for proofs of upper HH theorem). In fact, in simple words, the irreversible increase of 
entropy in time, results from a collective effect of the large number of reversible 
elementary events, according to the casual nature of molecular collisions. Molecules 
that collide, exchanging information, produce, in time, an increasing uniformity of 
molecular velocities. The global energy does not change (collisions are elastic), 
but differences between velocities decrease and this provides a greater value of the 
entropy, according to the entropy equipartition property. 
3.2 
Entropy and Computation 
We showed how information entropy is related to physics and how time in physics is 
related to entropy. In computations the informational and physical components are 
intertwined, because any computational device, even when abstractly considered, is 
based on a physical process, where states, symbols, or conﬁgurations are transformed 
along a dynamics starting with some initial conﬁgurations, and eventually ending, 
if computation reaches a result, with a ﬁnal conﬁguration (the initial conﬁguration 
encodes input data, while the ﬁnal conﬁguration encodes the corresponding output 
data). 
In this perspective, computation is a trajectory in space of events, where events 
can be conceived as suitable sets of states (according to micro-state/macro-state 
Boltzmann’s distinction, any macro-state consists of a collection of micro-states). 
On the other side, if computation is a process intended to acquire information, this 
representation suggests that the conﬁgurations along a computation have to reduce 
the number of internal micro-states. In fact, the uncertainty of a conﬁguration corre-
sponds with the number of states it contains, and a computation tends to reduce the 
initial uncertainty by reaching more certain conﬁgurations, which provide a solution, 
by satisfying the constraints of the given problem. 
In terms of entropy, if the number of possible states reduces along a computation, 
then the entropy log base 10 upper Wlg W (upper WW number of states) decreases along the computation. In 
conclusion, according to the outlined perspective, the computation results to be an 
anti-entropic process. For this reason, from a physical point of view, a computation 
is a dissipative process releasing energy (heat) in the environment. A computational 
device is comparable with a living organism: it increases the environment entropy 
for maintaining his low internal entropy [ 11], analogously, a computational device 
increases the environment entropy for proceeding in the computation and providing 
a ﬁnal result. 
An intuitive way for realizing that “information is physical” is a simple device 
called Centrifugal Governor, invented by James Watt in 1788 for controlling his 
steam engine. The principle on which it is based is that of a “conical pendulum”. The 
axis of a rotating engine is supplied with an attached rigid arm terminating with a mass 
that can assume an angle with respect to the axis (two symmetrical arms in Watt’s 
formulation, with a negligible mass at end of any arm). When the axis is rotating, in 
proportion to the rotation speed, the mass at end of the arm is subjected to a centrifugal

28
3
Information Theory
Fig. 3.1 The schema of 
Centrifugal Governor 
force rising the  arm (see Fig.  3.1). This rising opens a valve, in proportion to the 
vertical rising of the arm, which diminishes a physical parameter related to the speed 
(pressure in the case of a steam engine), and consequently, decreases the rotation 
speed. This phenomenon is a negative feedback that, according to the length of the 
arm, stabilizes the rotation speed to a ﬁxed velocity (negative means that control 
acts in the opposite verse of the controlled action). It realizes a kind of homeostasis 
(keeping some variables within a ﬁxed sub-range of variability), a property that is 
crucial for any complex system and thus typical of living organisms. 
The term “cybernetics” introduced by Norbert Wiener [ 13] comes from a Greek 
root expressing the action of guiding or controlling, and is essentially based on the 
image of a Centrifugal Governor, where the arm length encodes a piece of information 
capable of controlling the rotation speed (Wiener’s book title is: Cybernetics, or 
Control and Communication in the Animal and the Machine). Information directs 
processes, but it is realized by a physical process too. And in the centrifugal governor, 
m g hmgh is the energetic cost of the control exerted by the arm on the rotation (see Fig. 
3.1:mm is the mass at the end of the arm,gg the gravity acceleration, andhh is the vertical 
rising of the arm). We remark that from equation m v squared divided by 2 equals m g hmv2/2 = mgh (kinetic energy = 
potential energy) it follows that rising h equals v squared divided by 2 gh = v2/2g does not depend on the value of 
the mass, but only on the speed (that has to be sufﬁcient to provide a centrifugal force 
able to open the valve). 
The investigation of the relationship between information and physics has a long 
history, going back to the famous Maxwell demon, introduced by the great physicist 
in addressing the problem of how an intelligent agent can interfere with physical 
principles. His demon was apparently violating the second principle of thermody-
namics, for his ability to get information about the molecules’ velocity (but also this 
acquisition of information requires an energetic cost). A very intensive and active line 
of research was developed in this regard (see [ 14] for a historical account of these 
researches) and continues to be developed, even in recent years, especially under 
the pressure of the new results in quantum information and in quantum computing, 
which show how much it is essential the informational perspective for a deep analysis 
of quantum phenomena [ 15]. 
It is becoming always more evident that any mathematical model of physical phe-
nomena is based on data coming from information sources generated by measurement 
processes. This fact is not of secondary relevance, with respect to the construction

3.2 Entropy and Computation
29
Fig. 3.2 Landauer’s 
minimal erasure as a 
compression of the state 
space: in a the ball can be 
located in any of the two 
parts 0, 1 of the volume; in b 
it is conﬁned in part 1, 
therefore in (b) the state 
space is reduced from two 
possibilities to only one of 
them 
of the model, especially when the data acquisition processes are very sophisticated 
and cannot ingenuously be considered as mirrors of reality, rather, the only reality 
on which reconstructions can be based is just the information source resulting from 
the interactions between the observer and the observed phenomena. 
In a celebrated paper [ 16] Rolf Landauer asserts the Erasure Principle, according 
to which the erasure of a bit during a computation has an energetic cost of k upper T ln 2kT ln 2
(kk Boltzmann constant, upper TT absolute temperature) [ 17]. This means that, if the com-
putation entropy diminishes (for the reduction of computation conﬁguration states), 
then the environment entropy has to compensate this decrease by releasing a corre-
sponding quantity of heat upper S upper TST in the environment. 
The erasure principle is nothing else than a direct derivation of Boltzmann Equa-
tion 3.1, when from two possible states one of them is chosen, by passing fromupper W equals 2W = 2
to upper W equals 1W = 1. However, it is important to remark that “erasing” has to be considered in 
the wide sense of decreasing states of a computation conﬁguration, as indicated in 
Fig. 3.2 (the term erasure could be misleading, because it means writing a special 
symbol on a Turing machine tape). 
A continuation of Landauer’s research has been developed by Bennet and others 
[ 14, 18– 20] (in  [  14] a historical account of the theory of reversible computation is 
given). Computation steps can be reversible and irreversible. The ﬁrst kind of step 
arise when information is not lost after the step and it is possible to go back by 
returning to the data previously available. The irreversible steps are those that do 
not satisfy the condition of reversibility because some data are lost after the step. In 
this framework, it is shown that any computation can be performed in a reversible 
way, by using suitable strategies, where all the steps of a computation ending in a 
given conﬁguration are copied in a suitable zone of memory such that from the ﬁnal 
state it is possible to go back in a reverse way. Of course, in this way computation is 
reversible, but we are perplexed about the effective meaning of this result. In fact, if 
the information is gained by reducing the set of states of the initial conﬁguration, then 
in the case of a reversible computation no new information is obtained at end of the 
computation. This would require a different way of considering computations, where 
the evaluation of the amount of the gained information, at the end of computation, 
has to be deﬁned in other terms. Otherwise, if a computation does not generate 
information, for what reason it is performed? What is the advantage of obtaining a

30
3
Information Theory
result when computation halts? If a result is not new information, what gain is given 
by it? The theory of reversible computation, even if correct, is surely incomplete, as 
far as it does not answer these open questions [ 21]. 
Physical states representing data are a proper subset of all physical states of a 
system. Those that represent data are selected by the observer of the system who is 
using it for computing. But computation is not only a physical activity, it is mainly 
a coding-decoding activity according to a code chosen by another system coupled 
with the computation. This means that in any computation we need to distinguish 
between a passive and an active component. Forgetting this coupling and analyzing 
computations only on the side of the operations applied to the tape (of a Turing 
machine) can be useful, but does not tell the whole story of the computing process. 
This partiality is the source of some conclusions drawn in the theory of reversible 
computation. If even all the logical gates are reversible, the internal states of the 
“interpreter” processor could forget an essential aspect of the computation. Therefore, 
is not so obvious that we can always get rid of erasing operations, when also the 
program determination is taken into account, rather than only data transformations. 
The hidden trap, in analyzing a computation, is that of considering only the entropy 
of data, by forgetting that an agent provides the input data and a program, and he/she 
decodes the data obtained at end of the computation (if it halts). In other words, 
no computation can forget the agent performing it. Computation is not only in the 
computing device, but also in the interaction between an agent and a computation 
device. For this reason, also the entropic/energetic evaluation of the process that in 
the agent is coupled with the computing device has to be taken into account. 
For example, in reversible computation theory, a comparison is often presented 
with the RNA Polymerase transcription from DNA to RNA, by arguing that this 
copy phenomenon is reversible. But in this comparison, no mention is done about 
the anti-entropic character of RNA Polymerase that, in order to be maintained “alive”, 
requires some corresponding anti-entropic processes. 
In conclusion, the theory of reversible computation is sure of great interest, by dis-
closing subtle relations among time, space, and energy [ 14], however, what it claims 
raises several controversial aspects that need further clariﬁcations and experiments. 
An anecdote reports that when Shannon asked John von Neumann to suggest him 
a name for the quantityupper SS, then von Neumann promptly answered: “Entropy. This is 
just entropy”, by adding that this name would have been successful because only a 
few men knew exactly what entropy was. 
Entropy has a paradoxical aspect, due to its intrinsic probabilistic nature. The 
paradox is apparent at the beginning of Shannon’s paper [ 1]. In fact, Shannon says 
that he is searching for a measure of information, or uncertainty. But how can we 
consider these notions as equivalent? It would be something like searching for a 
measure of knowledge or ignorance. Can we reasonably deﬁne the same measures 
for opposite concepts? The answer to these questions can be found in the intrinsic 
orientation of events in time. When an event ee happens with a probability pp, we  
can measure its information by a function of its a priori uncertainty pp, but after 
it happens, we gain, a posteriori, the same amount of information associated to 
pp, therefore a priory uncertainty is transformed into information. The same kind

3.3 Entropic Concepts
31
of opposite orientation is always present in all informational concepts and it is 
often a source of confusion when the perspective of considering them is not clearly 
deﬁned [ 7]. 
As we will show, data can be always expressed by strings, that is, data can be 
linearized over a given alphabet of symbols. This assumption is a prerequisite of 
Shannon’s analysis that can be summarized by his three fundamental theorems. 
First Shannon theorem provides a lower bound to the mean length of strings 
representing the data emitted by an information source (mean calculated with respect 
to the probability distribution of the source). More precisely, the entropy of the source, 
which is independent of any method of data representation, turns out to provide this 
lower bound. 
Second Shannon theorem is based on mutual information: even if data of an 
information source are transmitted with some noise along a channel, it is possible 
to encode them in such a way that transmission could become error-free, because 
there exist transmission codes such that, the longer are the transmission encodings, 
the more error probability approaches to zero. 
In more precise terms, the theorem establishes quantitative notions giving the 
possibility of avoiding error transmission when the transmission rate is lower than 
the channel capacity of transmission, where these notions are formally deﬁned in 
suitable terms by using mutual information (between the transmitter information 
source and the bf receiver information source). 
Third Shannon theorem concerns with signals. To this end, the entropic notions 
are extended to the case of continuous information sources, and then, by using these 
continuous notions, quantitative evaluations are proven about safe communications 
by means of continuous signals. 
3.3 
Entropic Concepts 
Entropy allows us to deﬁne other informational concepts further developing the 
probabilistic approach. 
Let us remark that the notion of information source is completely equivalent to 
that of an aleatory or random variable, that is, a variable upper XX to which is associated a 
probability p Subscript upper XpX, where p Subscript upper X Baseline left parenthesis x right parenthesispX(x) is the probability that upper XX assumes the value xx. In other 
words, the random variableupper XX identiﬁes the information sourceleft parenthesis upper A comma p Subscript upper X Baseline right parenthesis(A, pX), whereupper AA is 
the set of values assumed by the variableupper XX. Viceversa, an information sourceleft parenthesis upper A comma p right parenthesis(A, p)
identiﬁes a random variable upper XX that assumes the values of upper AA with the probability 
distribution pp. For this reason, very often, entropy and other entropic concepts, are 
equivalently given for information sources, random variables, and/or probability 
distributions (discrete or continuous). In the following, according to the speciﬁc 
context, we adopt one of these alternatives.

32
3
Information Theory
Conditional entropy 
Given two random variables upper XX e upper YY of corresponding distributions p Subscript upper XpX and p Subscript upper YpY, the  
joint variableleft parenthesis upper X comma upper Y right parenthesis(X, Y) is that assuming as values the pairsleft parenthesis x comma y right parenthesis(x, y) of values assumed by 
upper XX andupper YY respectively, wherep Subscript upper X upper YpXY is the joint distributionp Subscript upper X upper Y Baseline left parenthesis x comma y right parenthesispXY(x, y), shortly indicated 
by p left parenthesis x comma y right parenthesisp(x, y), giving the probability of the composite event upper X equals xX = x and upper Y equals yY = y (when 
arguable subscripts in p Subscript upper X Baseline comma p Subscript upper Y Baseline comma p Subscript upper X upper Y BaselinepX, pY, pXY will be avoided). 
The joint entropy upper H left parenthesis upper X comma upper Y right parenthesisH(X, Y) of left parenthesis upper X comma upper Y right parenthesis(X, Y) is deﬁned by: 
upper H left parenthesis upper X comma upper Y right parenthesis equals minus sigma summation Underscript x element of upper A Endscripts sigma summation Underscript y element of upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x comma y right parenthesisH(X, Y) = −
∑
x∈A
∑
y∈B
p(x, y) log p(x, y)
or: 
upper H left parenthesis upper X comma upper Y right parenthesis equals minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x comma y right parenthesis periodH(X, Y) = −
∑
x,y∈A×B
p(x, y) log p(x, y).
By deﬁnition of conditioned probability, we have that p left parenthesis y vertical bar x right parenthesis equals StartFraction p left parenthesis x comma y right parenthesis Over p left parenthesis x right parenthesis EndFractionp(y|x) = p(x,y)
p(x) . Given  two  
random variables upper XX e upper YY, conditional entropy upper H left parenthesis upper Y vertical bar upper X right parenthesisH(Y|X) is deﬁned by the following 
equation, where upper E left bracket ellipsis right bracketE[. . .] denotes the expected value with respect to the distribution 
probability of the random variable inside brackets: 
StartLayout 1st Row 1st Column upper H left parenthesis upper Y vertical bar upper X right parenthesis 2nd Column equals 3rd Column minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis y vertical bar x right parenthesis 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 3rd Row 1st Column upper H left parenthesis upper Y vertical bar upper X right parenthesis 2nd Column equals 3rd Column minus sigma summation Underscript x comma y element of upper A times upper B Endscripts left bracket p left parenthesis x right parenthesis p left parenthesis y vertical bar x right parenthesis right bracket log p left parenthesis y vertical bar x right parenthesis 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 5th Row 1st Column Blank 2nd Column equals 3rd Column minus sigma summation Underscript x element of upper A Endscripts p left parenthesis x right parenthesis sigma summation Underscript y element of upper B Endscripts p left parenthesis y vertical bar x right parenthesis log p left parenthesis y vertical bar x right parenthesis 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 7th Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript x element of upper A Endscripts p left parenthesis x right parenthesis upper H left parenthesis upper Y vertical bar upper X equals x right parenthesis period EndLayout
H(Y|X) = −
∑
x,y∈A×B
p(x, y) log p(y|x)
H(Y|X) = −
∑
x,y∈A×B
[p(x)p(y|x)] log p(y|x)
= −
∑
x∈A
p(x)
∑
y∈B
p(y|x) log p(y|x)
=
∑
x∈A
p(x)H(Y|X = x).
In the above deﬁnition, the joint probability is multiplied by the logarithm of the 
conditional entropy (rather than, conditioned probability multiplied by its logarithm). 
This apparent asymmetry is motivated by the following equation, which follows 
from the deﬁnition above, where for any value xx of upper XX, upper H Subscript xHx denotes the entropy of upper XX
conditioned by the event upper X equals xX = x: 
upper H left parenthesis upper Y vertical bar upper X right parenthesis equals sigma summation Underscript x element of upper A Endscripts p left parenthesis x right parenthesis upper H Subscript x Baseline periodH(Y|X) =
∑
x∈A
p(x)Hx.
The following proposition corresponds to the well-known relation between joint 
and conditional probabilities: p left parenthesis x comma y right parenthesis equals p left parenthesis x right parenthesis p left parenthesis y vertical bar x right parenthesis periodp(x, y) = p(x)p(y|x).

3.3 Entropic Concepts
33
Proposition 3.1 
upper H left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis periodH(X, Y) = H(X) + H(Y|X).
Proof 
StartLayout 1st Row 1st Column upper H left parenthesis upper X comma upper Y right parenthesis 2nd Column equals minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x comma y right parenthesis 3rd Column Blank 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log left parenthesis p left parenthesis x right parenthesis p left parenthesis y vertical bar x right parenthesis right parenthesis 3rd Column Blank 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x right parenthesis minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis y vertical bar x right parenthesis 6th Row 1st Column Blank 7th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x comma y element of upper A times upper B Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis 8th Row 1st Column Blank 9th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x element of upper A Endscripts dot sigma summation Underscript y element of upper B Endscripts left parenthesis p left parenthesis x comma y right parenthesis log p left parenthesis x right parenthesis right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis 10th Row 1st Column Blank 11th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x element of upper A Endscripts left parenthesis sigma summation Underscript y element of upper B Endscripts p left parenthesis x comma y right parenthesis right parenthesis log p left parenthesis x right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis 12th Row 1st Column Blank 13th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript x element of upper A Endscripts p left parenthesis x right parenthesis log p left parenthesis x right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis 14th Row 1st Column Blank 15th Row 1st Column upper H left parenthesis upper X comma upper Y right parenthesis 2nd Column equals upper H left parenthesis upper X right parenthesis plus upper H left parenthesis upper Y vertical bar upper X right parenthesis EndLayout
H(X, Y) = −
∑
x,y∈A×B
p(x, y) log p(x, y)
= −
∑
x,y∈A×B
p(x, y) log(p(x)p(y|x))
= −
∑
x,y∈A×B
p(x, y) log p(x) −
∑
x,y∈A×B
p(x, y) log p(y|x)
= −
∑
x,y∈A×B
p(x, y) log p(x) + H (Y|X)
= −
∑
x∈A
·
∑
y∈B
(p (x, y) log p(x)) + H (Y|X)
= −
∑
x∈A
⎛
⎝∑
y∈B
p (x, y)
⎞
⎠log p(x) + H (Y|X)
= −
∑
x∈A
p(x) log p(x) + H (Y|X)
H(X, Y) = H(X) + H (Y|X)
Entropic divergence 
Given two probability distributions p comma qp, q their entropic divergence, upper D left parenthesis p comma q right parenthesisD(p, q), also  
denoted byupper K upper L left parenthesis p comma q right parenthesisK L(p, q) after Kullback and Leibler [ 3], is the probabilistic mean, with 
respect to distribution pp, of the differences of information quantities associated to 
distribution pp and qq respectively: 
upper D left parenthesis p comma q right parenthesis equals sigma summation p left parenthesis x right parenthesis left bracket log base 10 p left parenthesis x right parenthesis minus log base 10 q left parenthesis x right parenthesis right bracketD(p, q) =
∑
p(x)[lg p(x) −lg q(x)]
(3.2) 
or, indicating by upper E Subscript pE p the mean with respect to the probability distribution pp: 
upper D left parenthesis p comma q right parenthesis equals upper E Subscript p Baseline left bracket log base 10 p left parenthesis x right parenthesis minus log base 10 q left parenthesis x right parenthesis right bracketD(p, q) = E p[lg p(x) −lg q(x)]
(3.3) 
or, by using a property of logarithm:

34
3
Information Theory
upper D left parenthesis p comma q right parenthesis equals upper E Subscript p Baseline left bracket log StartFraction p left parenthesis x right parenthesis Over q left parenthesis x right parenthesis EndFraction right bracketD(p, q) = E p
[
log p(x)
q(x)
]
Whenq equals pq = p,upper D left parenthesis p comma p right parenthesis equals 0D(p, p) = 0. Moreover, equations 0 log StartFraction 0 Over q EndFraction equals 00 log 0
q = 0 ep log StartFraction p Over 0 EndFraction equals normal infinityp log p
0 = ∞are 
usually assumed. 
Mutual Information 
Letpp be a probability distribution deﬁned overupper X times upper YX × Y, wherep Subscript upper X Baseline comma p Subscript upper Y BaselinepX, pY are the proba-
bilities ofupper X comma upper YX, Y respectively, also called marginal probabilities. The  mutual infor-
mation upper I left parenthesis upper X comma upper Y right parenthesisI (X, Y) is given by the entropic divergence between distributions p Subscript left parenthesis upper X comma upper Y right parenthesisp(X,Y)
and p Subscript upper X Baseline times p Subscript upper YpX × pY, where p Subscript upper X upper YpXY is the joint probability assigning to each pair left parenthesis x comma y right parenthesis(x, y) the 
probability p left parenthesis x comma y right parenthesisp(x, y) of the joint event upper X equals x comma upper Y equals yX = x, Y = y, while p Subscript upper X Baseline times p Subscript upper YpX × pY is the product 
of the marginal probabilities, assigning to each pair left parenthesis x comma y right parenthesis(x, y) the product p left parenthesis x right parenthesis p left parenthesis y right parenthesisp(x)p(y) of 
the two probabilities p Subscript upper X Baseline comma p Subscript upper Y BaselinepX, pY: 
upper I left parenthesis upper X comma upper Y right parenthesis equals upper D left parenthesis p Subscript upper X upper Y Baseline comma p Subscript upper X Baseline times p Subscript upper Y Baseline right parenthesis periodI (X, Y) = D(pXY, pX × pY).
(3.4) 
By the deﬁnition of upper I left parenthesis upper X comma upper Y right parenthesisI (X, Y) we have: 
upper I left parenthesis upper X comma upper Y right parenthesis equals sigma summation Underscript x element of upper X comma y element of upper Y Endscripts p left parenthesis x comma y right parenthesis log base 10 left bracket p left parenthesis x comma y right parenthesis divided by p left parenthesis x right parenthesis p left parenthesis y right parenthesis right bracket periodI (X, Y) =
∑
x∈X,y∈Y
p(x, y) lg[p(x, y)/p(x)p(y)].
(3.5) 
therefore: 
upper I left parenthesis upper X comma upper Y right parenthesis equals sigma summation Underscript x element of upper X comma y element of upper Y Endscripts p left parenthesis x comma y right parenthesis left bracket log base 10 p left parenthesis x comma y right parenthesis divided by p left parenthesis x right parenthesis right bracket minus sigma summation Underscript x element of upper X comma y element of upper Y Endscripts p left parenthesis x comma y right parenthesis log base 10 p left parenthesis y right parenthesis right bracketI (X, Y) =
∑
x∈X,y∈Y
p(x, y)[lg p(x, y)/p(x)] −
∑
x∈X,y∈Y
p(x, y) lg p(y)]
(3.6) 
that is: 
upper I left parenthesis upper X comma upper Y right parenthesis equals left parenthesis sigma summation Underscript x element of upper X comma y element of upper Y Endscripts p left parenthesis x comma y right parenthesis left bracket log base 10 p left parenthesis y vertical bar x right parenthesis right bracket right parenthesis minus sigma summation Underscript y element of upper Y Endscripts p left parenthesis y right parenthesis log base 10 p left parenthesis y right parenthesis right bracketI (X, Y) =
⎛
⎝∑
x∈X,y∈Y
p(x, y)[lg p(y|x)]
⎞
⎠−
∑
y∈Y
p(y) lg p(y)]
(3.7) 
that can be written as: 
upper I left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper Y right parenthesis minus upper H left parenthesis upper Y vertical bar upper X right parenthesis periodI (X, Y) = H(Y) −H(Y|X).
(3.8) 
If we consider the conditional entropy upper H left parenthesis upper Y vertical bar upper X right parenthesisH(Y|X) as the mean conditional infor-
mation of upper YY given upper XX, then  Eq. (3.8) tells us that the mutual information between upper XX
and upper YY is the mean information of upper YY minus the mean conditional information of upper YY
given upper XX.

3.3 Entropic Concepts
35
Proposition 3.2 upper D left parenthesis p comma q right parenthesis greater than or equals 0D(p, q) ≥0
Proof The following is a very simple inequality resulting from the graphic of loga-
rithm function: 
ln x less than or equals x minus 1 ln x ≤x −1
therefore: 
ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction minus 1 ln qi
pi
≤qi
pi
−1
multiplying by p Subscript ipi and summing over ii on both members: 
sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals sigma summation Underscript i Endscripts p Subscript i Baseline left parenthesis StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction minus 1 right parenthesis
∑
i
pi ln qi
pi
≤
∑
i
pi
( qi
pi
−1
)
sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals sigma summation Underscript i Endscripts q Subscript i Baseline minus sigma summation Underscript i Endscripts p Subscript i
∑
i
pi ln qi
pi
≤
∑
i
qi −
∑
i
pi
because sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline equals sigma summation Underscript i equals 1 Overscript m Endscripts q Subscript i Baseline equals 1
m
∑
i=1
pi =
m
∑
i=1
qi = 1 (pp e qq are probability distributions): 
sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals 0
∑
i
pi ln qi
pi
≤0
or, by passing to logarithms in base kk: 
sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals 0
∑
i
pi logk
qi
pi
≤0
then, reversing the logarithm fraction with the consequent change of sign, the opposite 
inequality is obtained, that is: 
StartLayout 1st Row 1st Column sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline StartFraction p Subscript i Baseline Over q Subscript i Baseline EndFraction 2nd Column greater than or equals 3rd Column 0 2nd Row 1st Column upper D left parenthesis p comma q right parenthesis 2nd Column greater than or equals 3rd Column 0 EndLayout
∑
i
pi logk
pi
qi
≥0
D(p, q) ≥0
□
Proposition 3.3 • upper I left parenthesis upper X comma upper Y right parenthesis equals upper I left parenthesis upper X comma upper Y right parenthesisI (X, Y) = I (X, Y) (symmetry) 
• upper I left parenthesis upper X comma upper Y right parenthesis greater than or equals 0I (X, Y) ≥0 (non-negativity) 
• upper I left parenthesis upper Y comma upper X right parenthesis equals upper H left parenthesis upper X right parenthesis plus upper H left parenthesis upper Y right parenthesis minus upper H left parenthesis upper X comma upper Y right parenthesisI (Y, X) = H(X) + H(Y) −H(X, Y)
Proof The ﬁrst equation follows directly from the deﬁnition of mutual informa-
tion, while the second one follows from non-negativity of Kullback-Leibler diver-
gence. The third equation follows from equations upper I left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesisI (X, Y) = H(X) −H(X|Y) and

36
3
Information Theory
upper I left parenthesis upper Y comma upper X right parenthesis equals upper H left parenthesis upper Y right parenthesis minus upper H left parenthesis upper Y vertical bar upper X right parenthesisI (Y, X) = H(Y) −H(Y|X), by replacing in them the conditional entropies as they 
result from upper H left parenthesis upper X vertical bar upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X comma upper Y right parenthesisH(X|Y) = H(X) −H(X, Y) and upper H left parenthesis upper Y vertical bar upper X right parenthesis equals upper H left parenthesis upper Y right parenthesis minus upper H left parenthesis upper Y comma upper X right parenthesisH(Y|X) = H(Y) −H(Y, X).
□
Entropic divergence and mutual information provide two important properties of 
entropy. 
Proposition 3.4 The entropy of a random variableupper XX reaches its maximum whenupper XX
is uniformly distributed. 
Proof If nn is the number of values assumed by upper XX (discrete and ﬁnite case), then: 
upper H left parenthesis upper X right parenthesis less than or equals log n periodH(X) ≤log n.
In fact, if upper XX is uniformly distributed we have: 
upper H left parenthesis upper X right parenthesis equals minus sigma summation Underscript 1 Overscript n Endscripts StartFraction 1 Over n EndFraction log StartFraction 1 Over n EndFraction equals minus StartFraction n Over n EndFraction log StartFraction 1 Over n EndFraction equals log nH(X) = −
n
∑
1
1
n log 1
n = −n
n log 1
n = log n
Ifu  u denotes the uniform probability distribution: 
StartLayout 1st Row 1st Column upper D left parenthesis p comma u right parenthesis 2nd Column equals sigma summation Underscript x Endscripts p left parenthesis x right parenthesis log StartFraction p left parenthesis x right parenthesis Over u left parenthesis x right parenthesis EndFraction 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript x Endscripts p left parenthesis x right parenthesis left parenthesis log p left parenthesis x right parenthesis minus log u left parenthesis x right parenthesis right parenthesis 3rd Row 1st Column Blank 2nd Column equals sigma summation Underscript x Endscripts p left parenthesis x right parenthesis log p left parenthesis x right parenthesis minus sigma summation Underscript x Endscripts p left parenthesis x right parenthesis log u left parenthesis x right parenthesis 4th Row 1st Column Blank 2nd Column equals minus upper H left parenthesis upper X right parenthesis plus log n EndLayout
D(p, u) =
∑
x
p(x) log p(x)
u(x)
=
∑
x
p(x) (log p(x) −log u(x))
=
∑
x
p(x) log p(x) −
∑
x
p(x) log u (x)
= −H(X) + log n
therefore: 
minus upper H left parenthesis upper X right parenthesis minus log 1 divided by n greater than or equals 0 −H(X) −log 1/n ≥0
that is: 
upper H left parenthesis upper X right parenthesis less than or equals log nH(X) ≤log n
□
Proposition 3.5 
upper H left parenthesis upper X vertical bar upper Y right parenthesis less than or equals upper H left parenthesis upper X right parenthesis periodH(X|Y) ≤H(X).
Proof 
upper I left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesisI (X, Y) = H(X) −H(X|Y)
and: 
upper D left parenthesis p comma q right parenthesis greater than or equals 0D(p, q) ≥0
therefore: upper I left parenthesis upper X comma upper Y right parenthesis greater than or equals 0I (X, Y) ≥0 that is:

3.3 Entropic Concepts
37
upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesis greater than or equals 0H(X) −H(X|Y) ≥0
that gives the thesis.
□
Letupper H left parenthesis upper X right parenthesisH(X) be the uncertainty ofupper XX, andupper H left parenthesis upper X vertical bar upper Y right parenthesisH(X|Y) be the conditional uncertainty of 
upper XX givenupper YY. Mutual information results to be equal to the decrease of the uncertainty 
of upper H left parenthesis upper X right parenthesisH(X) produced by upper H left parenthesis upper X vertical bar upper Y right parenthesisH(X|Y): 
Proposition 3.6 
upper I left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesis periodI (X, Y) = H(X) −H(X|Y).
Proof 
upper I left parenthesis upper X comma upper Y right parenthesis equals sigma summation Underscript x comma y Endscripts p left parenthesis x comma y right parenthesis log StartFraction p left parenthesis x comma y right parenthesis Over p left parenthesis x right parenthesis p left parenthesis y right parenthesis EndFractionI (X, Y) =
∑
x,y
p(x, y) log
p(x, y)
p(x)p(y)
but p left parenthesis x vertical bar y right parenthesis equals StartFraction p left parenthesis x comma y right parenthesis Over p left parenthesis y right parenthesis EndFractionp(x|y) = p(x,y)
p(y) , then: 
upper I left parenthesis upper X comma upper Y right parenthesis equals sigma summation Underscript x comma y Endscripts p left parenthesis x comma y right parenthesis log StartFraction p left parenthesis x vertical bar y right parenthesis Over p left parenthesis x right parenthesis EndFractionI (X, Y) =
∑
x,y
p(x, y) log p(x|y)
p(x)
from which the following equations hold: 
StartLayout 1st Row 1st Column upper I left parenthesis upper X comma upper Y right parenthesis 2nd Column equals sigma summation Underscript x comma y Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x vertical bar y right parenthesis minus sigma summation Underscript x comma y Endscripts p left parenthesis x comma y right parenthesis log p left parenthesis x right parenthesis 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals minus upper H left parenthesis upper X vertical bar upper Y right parenthesis minus sigma summation Underscript x element of upper A Endscripts left parenthesis sigma summation Underscript y element of upper B Endscripts p left parenthesis x comma y right parenthesis right parenthesis log p left parenthesis x right parenthesis 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals minus upper H left parenthesis upper X vertical bar upper Y right parenthesis minus sigma summation Underscript x element of upper A Endscripts p left parenthesis x right parenthesis log p left parenthesis x right parenthesis 6th Row 1st Column Blank 7th Row 1st Column Blank 2nd Column equals minus upper H left parenthesis upper X vertical bar upper Y right parenthesis plus upper H left parenthesis upper X right parenthesis 8th Row 1st Column Blank 9th Row 1st Column Blank 2nd Column equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesis EndLayout
I (X, Y) =
∑
x,y
p(x, y) log p (x|y) −
∑
x,y
p(x, y) log p (x)
= −H (X|Y) −
∑
x∈A
(∑
y∈B p (x, y)
)
log p(x)
= −H(X|Y) −
∑
x∈A
p(x) log p(x)
= −H(X|Y) + H(X)
= H(X) −H(X|Y)
□
According to the previous proposition, mutual information upper I left parenthesis upper X comma upper Y right parenthesisI (X, Y) can be seen 
as the uncertainty ofupper XX reduced by the uncertainty ofupper XX whenupper YY is known. The term 
upper H left parenthesis upper X vertical bar upper Y right parenthesisH(X|Y) is called equivocation, and corresponds to the uncertainty of upper XX given upper YY. 
Mutual information allows us to give a mathematical formulation of the commu-
nication process between a sender and a receiver connected by means of a channel of 
communication where noise is acting disturbing the communication. In this picture 
sender and receiver are two information sources, or random variables upper X comma upper YX, Y, respec-
tively, and a quantity, called channel capacityupper CC measures the maximum amount of

38
3
Information Theory
information that can be transmitted through the channel in the time unity, where the 
maximum is considered in the class of the probability distributions pp associated to 
the sender upper XX: 
upper C equals max Underscript p Endscripts upper I left parenthesis upper X comma upper Y right parenthesisC = max
p
I (X, Y)
The second Shannon theorem will show that upper CC results to be equivalent to the max-
imum transmission rate, that is, the maximum quantity of information transmitted 
by the sender, in the time unity, that can be correctly received by the receiver. 
3.4 
Codes 
A code is a function from a ﬁnite set upper CC of codewords or encodings, which are 
strings over a ﬁnite alphabet, to a set upper DD of data. In the following, we often will 
identify a code with the set upper CC of its codewords, by using the term encoding for 
referring to the function from upper CC to upper DD. When encoding is not injective, the code 
is said to be redundant, because different codewords can encode the same datum. 
When encoding is not surjective, the code is said to be degenerate. When encoding 
is not deﬁnite for some codewords, the code is said to be partial. In the following 
discussion, we will often assume 1–1 codes, with 1-to-1 encoding functions. 
The genetic code is a code that is redundant and partial. It encodes 20 amino acids 
with terns of symbols, called codons, over the alphabet upper U comma upper C comma upper A comma upper GU, C, A, G. Many codons 
may encode the same amino acid, and there are 3 of the 64 codons (UGA. UAA, 
UAG) that do not encode any amino acid and are stop signals in the translation from 
sequences of codons to sequences of amino acids (Table 3.1). 
The Standard ASCII has only 7 bits (Table 3.2). Extended ASCII (ISO-Latin-
8859) has 8 bits. 
In the ASCII code, a character is encoded by summing the column value with 
the row value. For example: upper N right arrow 40 plus upper E equals 4 upper EN →40 + E = 4E. Then hexadecimal encodings are 
transformed into binary values. 
The digital information of datumdd, with respect to a codeupper CC, is the length of the 
string alpha element of upper Cα ∈C encoding dd (the arithmetic mean of the codewords encoding dd, if they  
are more than 1). Digital information is not a good measure of the information of 
data. In fact, as we will see, given a code, another code can exist encoding the same 
data with shorter codewords. This means that a measure of information, independent 
from codes, should require the search for codewords of minimum lengths. One of 
the main results of Shannon’s probabilistic approach is the ﬁrst Shannon theorem, 
which explains as the entropy of an information source provides a minimal bound to 
any digital measure of information. 
A code upper CC is preﬁx free, or  instantaneous, if no codeword of upper CC is a preﬁx of 
another codeword ofupper CC. When a code is preﬁx-free, it can be represented by a rooted 
tree, called encoding tree, where codewords are the leaves of the tree. Starting from 
the root, a number of edges equal to the symbols of the alphabet spring from the root,

3.4 Codes
39
Table 3.1 The amino-acids and the corresponding codons 
Amino-acid
Name
Letter
Codons 
Arg
Arginine
R
StartSet upper C upper G upper U comma upper C upper G upper C comma upper C upper G upper A comma upper C upper G upper G comma upper A upper G upper A comma upper A upper G upper G EndSet{CGU, CGC, CG A, CGG, AG A, AGG}
Leu
Leucine
L
StartSet upper U upper U upper A comma upper U upper U upper G comma upper C upper U upper U comma upper C upper U upper C comma upper C upper U upper A comma upper C upper U upper G EndSet{UU A,UUG, CUU, CUC, CU A, CUG}
Ser
Serine
S
StartSet upper U upper C upper C comma upper U upper C upper U comma upper U upper C upper A comma upper U upper C upper G comma upper A upper G upper U comma upper A upper G upper C EndSet{UCC,UCU,UC A,UCG, AGU, AGC}
Ala
Alanine
A
StartSet upper G upper C upper U comma upper G upper C upper C comma upper G upper C upper A comma upper G upper C upper G EndSet{GCU, GCC, GC A, GCG}
Gly
Glycine
G
StartSet upper G upper G upper U comma upper G upper G upper C comma upper G upper G upper A comma upper G upper G upper G EndSet{GGU, GGC, GG A, GGG}
Pro
Proline
P
StartSet upper C upper C upper U comma upper C upper C upper C comma upper C upper C upper A comma upper C upper C upper G EndSet{CCU, CCC, CC A, CCG}
Thr
Threonine
T
StartSet upper A upper C upper U comma upper A upper C upper C comma upper A upper C upper A comma upper A upper C upper G EndSet{ACU, ACC, AC A, ACG}
Val
Valine
V
StartSet upper G upper U upper U comma upper G upper U upper C comma upper G upper U upper A comma upper G upper U upper G EndSet{GUU, GUC, GU A, GUG}
Ile
Isoleucine
I
StartSet upper A upper U upper U comma upper A upper U upper C comma upper A upper U upper A EndSet{AUU, AUC, AU A}
Asn
Aspargine
N
StartSet upper A upper A upper U comma upper A upper A upper C EndSet{AAU, AAC}
Asp
Aspartate
D
StartSet upper G upper A upper U comma upper G upper A upper C EndSet{G AU, G AC}
Cys
Cysteine
C
StartSet upper U upper G upper U comma upper U upper G upper C EndSet{UGU,UGC}
His
Hystidine
H
StartSet upper C upper A upper U comma upper C upper A upper C EndSet{C AU, C AC}
Gln
Glutamine
Q
StartSet upper G upper A upper A comma upper G upper A upper G EndSet{G AA, G AG}
Glu
Glutamate
E
StartSet upper C upper A upper A comma upper C upper A upper G EndSet{C AA, C AG}
Lys
Lysine
K
StartSet upper A upper A upper A comma upper A upper A upper G EndSet{AAA, AAG}
Phe
Phenilananine 
F
StartSet upper U upper U upper U comma upper U upper U upper C EndSet{UUU,UUC}
Tyr
Tyrosine
Y
StartSet upper U upper A upper U comma upper U upper A upper C EndSet{U AU, U AC}
Met
Methionine
M
StartSet upper A upper U upper G EndSet{AUG}
Trp
Tryptophan
W
StartSet upper U upper G upper G EndSet{UGG}
Table 3.2 The Table of ASCII standard code 
0
1
2
3
4
5
6
7
8
9
A
B
C
D
E
F 
00
NULL
SOH 
STX 
ETX 
EOT 
ENQ ACK 
BEL 
BS
TAB 
LF
VT
FF
CR
SO
SI 
10
DLE
DC1 
DC2 
DC3 
DC4 
NAK SYN 
ETB 
CAN EM
SUB 
ESC 
FS
GS
RS
US 
20
SPACE
!
¨
#
$
%
&
’
(
)
*
+
,
–
.
/ 
30
0
1
2
3
4
5
6
7
8
9
:
;
less than<
=
greater than>
? 
40
@
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O 
50
P
Q
R
S
T
U
V
W
X
Y
Z
[
set minus\
]
ê
_ 
60
‘
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o 
70
p
q
r
s
t
u
v
w
x
y
z
{
|
}
˜
DEL 
each edge labelled by a symbol of the alphabet of upper CC. The same situation is repeated 
for all the nodes of the tree that are not leaves. In such a way, each path from the root 
to a leaf identiﬁes the sequence of the edges of the path, and the sequence of labels is a 
codeword ofupper CC. Therefore, by construction, codewords cannot be substrings of other 
codewords. The encoding tree of Fig. 3.4 shows that that, passing from a node to 
their sons, a partition of data is realized, and a leaf is obtained when a single datum is

40
3
Information Theory
reached. If the partitions are uniformly distributed, the tree reaches a minimal depth, 
which results to be logarithmic with respect to the size of the set of data. 
✡ 
✡ 
✡
✡✢ 
❏ 
❏ 
❏❏❫ 
✡ 
✡ 
✡
✡✢ 
❏ 
❏ 
❏❏❫ 
✁ 
✁ 
✁☛ 
✁ 
✁ 
✁☛ 
✁ 
✁ 
✁☛ 
❆ 
❆ ❆❯ 
❆ 
❆ ❆❯ 
❆ 
❆ ❆❯ 
✁ 
✁ 
✁☛ 
❆ 
❆ ❆❯ 
✁ 
✁ 
✁☛ 
❆ 
❆ ❆❯ 
✚ 
✚ 
✚ 
✚ 
✚ 
✚
✚
❂ 
◗ ◗ ◗ ◗ ◗ ◗ ◗
◗
 
{a1, a2, a3, a4, a5, a6, a7, a8, a9}
{a1, a2, a3, a4}
{a5, a6, a7, a8, a9} 
{a5, a6, a7} {a8, a9} 
{a5}
a9}
{a8}
{a6, a7} 
{a6} 
{a7} 
0 
0 
0 
0
0 
0 
1 
1
1 
1 
1 
1 
100 
1010 
1011 
110
111 
{a1, a2, a3} 
0
1 
{a1, a2} 
{a4} 
{a3} 
{a1} {a2} 
01 
001 
0001
000 
0
1 
{a1, a2, a3, a4}
{a5, a6, a7, a8, a9} 
{a5, a6, a7} 
An encoding tree 
 
 
 
 
 
 
 
 
 
0
{
The above encoding tree shows a very general and universal aspect of strings, 
which we emphasize with the following proposition. 
Proposition 3.7 (Linearization of data) Any ﬁnite set upper DD of data can be represented 
by a ﬁnite sets of strings, called codewords of the elements of upper DD. 
McMillan and Kraft Theorems 
A codeupper CC is univocal if any concatenation of codewords ofupper CC gives, in a unique way, 
the sequence of codewords of that concatenation. This means that do not exist two 
different sequences of codewords ofupper CC that when are concatenated provide the same 
string over the alphabet of upper CC. 
A 1-1 code upper CC, for a set of data upper DD, where alpha Subscript dαd is the codeword of dd, is  optimal, 
with respect to an information source left parenthesis upper D comma p right parenthesis(D, p), if no other code for upper DD exists having a 
smaller average length upper L Subscript upper CLC, with respect to the probability distribution pp: 
upper L Subscript upper C Baseline equals sigma summation Underscript d element of upper D Endscripts StartAbsoluteValue alpha Subscript d Baseline EndAbsoluteValue p left parenthesis d right parenthesis periodLC =
∑
d∈D
|αd|p(d).
The Kraft normStartAbsoluteValue EndAbsoluteValue upper C StartAbsoluteValue EndAbsoluteValue||C|| of a codeupper CC is deﬁned by (kk is the cardinality of the alphabet 
of upper CC, and StartAbsoluteValue alpha EndAbsoluteValue|α| is the length of alphaα):

3.4 Codes
41
StartAbsoluteValue EndAbsoluteValue upper C StartAbsoluteValue EndAbsoluteValue equals sigma summation Underscript alpha element of upper C Endscripts k Superscript minus StartAbsoluteValue alpha EndAbsoluteValue Baseline period||C|| =
∑
α∈C
k−|α|.
(3.9) 
Proposition 3.8 (Kraft inverse theorem) Ifdouble vertical bar upper C double vertical bar less than or equals 1∥C∥≤1 thenthere exists∃upper C primeC' such thatupper C primeC' ìs preﬁx-
free and double vertical bar upper C prime double vertical bar equals double vertical bar upper C double vertical bar
∥∥C'∥∥= ∥C∥(the number of encodings of a given length is the same in the 
two codes). 
Proof Let upper CC be a binary code with the following encoding lengths: script l 1 comma script l 2 comma midline horizontal ellipsis comma script l Subscript m Baselinel1, l2, · · · , lm. 
and let double vertical bar upper C double vertical bar less than or equals 1∥C∥≤1 and script l 1 less than script l 2 less than ellipsis less than script l Subscript ml1 < l2 < . . . < lm. 
First, we observe that, by deﬁnition of Kraft norm, if two codes upper C comma upper C primeC, C' have the 
same encoding lengths and the same number of encodings for each length, then 
StartAbsoluteValue EndAbsoluteValue upper C prime StartAbsoluteValue EndAbsoluteValue equals StartAbsoluteValue EndAbsoluteValue upper C StartAbsoluteValue EndAbsoluteValue||C'|| = ||C||. Let us constructupper C primeC' by composing trees. The ﬁrst treeupper T 1T1 has depthscript l 1l1
and2 Superscript script l 12l1 leaves. Letalpha 1 ellipsis alpha Subscript n Baselineα1 . . . αn be the encodings ofupper CC of lengthscript l 1l1, of coursen less than or equals 2 Superscript script l 1 Baseline periodn ≤2l1. In 
our hypothesis we have also encodings of lengthsscript l 2 comma ellipsis comma script l Subscript m Baselinel2, . . . , lm. This means that some 
leaves of upper T 1T1 are not associated to encodings. Therefore we can construct the trees 
upper T 2T2 for allocating the encodings of length script l 2l2. Without loss of generality, we assume 
that the root of upper T 2T2 is the leftmost leaf of upper T 1T1 following all the leaves encoding the 
codewords of length script l 1l1. Then, allocate in the ﬁrst leaves of upper T 2T2 all the codewords of 
length script l 2l2. The third tree is added with the same principle, and analogously we can 
continue until the codewords of all lengths are encoded. This process can continue, 
because if, at some stage of the process, no leaf would be available, this should 
contradict the hypothesis that Kraft norm ofupper CC is less than or equal to 1. The tree that 
we obtain at end is, by construction, a code upper C primeC' with the same norm of the original 
univocal code upper CC. Moreover, upper C primeC' is a preﬁx-free code, because the codewords of upper C primeC'
are associated to the leaves of an encoding tree.
□
Example 3.1 The codes of the following table have the same norm, code upper CC is 
univocal but it is not preﬁx-free, because (alpha 3α3 is a preﬁx ofalpha 4α4), whileupper C primeC' is preﬁx-free 
(Table 3.3). 
The norm of Kraft is for both codes: 
double vertical bar upper C double vertical bar equals double vertical bar upper C Superscript prime Baseline double vertical bar equals seven eighths less than 1 ∥C∥=
∥∥C'∥∥= 7
8 < 1
Table 3.4 gives an example of non-univocal code, where Kraft norm is less 
than 1. 
Table 3.3 Codeupper CC is not preﬁx-free whileupper C primeC' preﬁx-free. 
Code C
Code Cprime'
alpha bold 1α1
10
00 
alpha bold 2α2
00
01 
alpha bold 3α3
11
10 
alpha bold 4α4
110
110

42
3
Information Theory
Table 3.4 An example of code that is not univocal, but having Kraft norm less than 1 
alpha 1α1
1 
alpha 2α2
11 
alpha 3α3
111 
alpha 4α4
1111 
Proposition 3.9 (McMillan Theorem) 
upper I f upper C i s u n i v o c a l t h e n double vertical bar upper C double vertical bar less than or equals 1I f C is univocal then ∥C∥≤1
Let us introduce some concepts in order to develop the proof. 
The concatenation of two codes upper L 1 comma upper L 2L1, L2 is deﬁned by: 
upper L 1 dot upper L 2 equals left brace alpha beta vertical bar alpha element of upper L 1 comma beta element of upper L 2 right braceL1 · L2 = {αβ|α ∈L1, β ∈L2}
The codeupper C Superscript jC j withj greater than 1 j > 1, is thej j-power ofupper CC obtained by concatenatingj j copies ofupper CC: 
upper C Superscript j Baseline equals ModifyingBelow upper C dot upper C dot upper C dot ellipsis dot upper C With bottom braceC j = C · C · C · . . . · C
 
  
 
Lemma 3.1 If upper CC is univocal then double vertical bar upper C Superscript j Baseline double vertical bar equals double vertical bar upper C double vertical bar Superscript j
∥∥C j∥∥= ∥C∥j. 
Proof In fact: 
StartLayout 1st Row 1st Column double vertical bar upper C double vertical bar Superscript j 2nd Column equals left parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts 2 Superscript minus StartAbsoluteValue alpha Super Subscript i Superscript EndAbsoluteValue Baseline right parenthesis Superscript j Baseline 2nd Row 1st Column Blank 2nd Column equals product Underscript k equals 1 Overscript j Endscripts left parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts 2 Superscript minus StartAbsoluteValue alpha Super Subscript i Superscript EndAbsoluteValue Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column Blank 4th Row 1st Column Blank 2nd Column equals sigma summation Underscript i 1 equals 1 Overscript n Endscripts dot sigma summation Underscript i 2 equals 1 Overscript n Endscripts dot midline horizontal ellipsis dot sigma summation Underscript i Subscript j Baseline equals 1 Overscript n Endscripts 2 Superscript minus left parenthesis StartAbsoluteValue alpha Super Subscript i 1 Superscript EndAbsoluteValue plus StartAbsoluteValue alpha Super Subscript i 2 Superscript EndAbsoluteValue plus midline horizontal ellipsis plus StartAbsoluteValue alpha Super Subscript i Super Sub Subscript j Super Subscript Superscript EndAbsoluteValue right parenthesis Baseline 5th Row 1st Column Blank 2nd Column Blank 6th Row 1st Column Blank 2nd Column equals sigma summation Underscript i 1 midline horizontal ellipsis i Subscript j Baseline element of left brace 1 midline horizontal ellipsis n right brace Endscripts 2 Superscript minus left parenthesis StartAbsoluteValue alpha Super Subscript i 1 Superscript dot alpha Super Subscript i 2 Superscript midline horizontal ellipsis alpha Super Subscript i Super Sub Subscript j Super Subscript Superscript EndAbsoluteValue right parenthesis Baseline 7th Row 1st Column Blank 2nd Column Blank 8th Row 1st Column Blank 2nd Column equals double vertical bar bold upper C Superscript bold j Baseline double vertical bar 3rd Column Blank EndLayout
∥C∥j =
( n
∑
i=1
2−|αi |
) j
=
j∏
k=1
( n
∑
i=1
2−|αi |
)
=
n
∑
i1=1
·
n
∑
i2=1
· . . . ·
n
∑
i j =1
2
−(|αi1|+|αi2 |+···+|αi j |)
=
∑
i1···i j ∈{1···n}
2
−(|αi1·αi2 ···αi j |)
=
∥∥∥Cj∥∥∥
□
because when upper CC is univocal: 
StartAbsoluteValue alpha Subscript i 1 Baseline EndAbsoluteValue plus StartAbsoluteValue alpha Subscript i 2 Baseline EndAbsoluteValue plus midline horizontal ellipsis plus StartAbsoluteValue alpha Subscript i Sub Subscript j Subscript Baseline EndAbsoluteValue equals StartAbsoluteValue alpha Subscript i 1 Baseline dot alpha Subscript i 2 Baseline midline horizontal ellipsis alpha Subscript i Sub Subscript j Subscript Baseline EndAbsoluteValue|αi1| + |αi2| + · · · + |αi j| = |αi1 · αi2 · · · αi j|
by the uniqueness of the sequence of codewords of upper CC with a given concatenation. 
Let m a xmax and m i nmin be the maximum and minimum of the length of codewords of 
upper CC, then we have that the maximum number of different lengths of codewords of upper C Superscript jC j
is less than or equals bold j bold left parenthesis bold max minus bold min bold right parenthesis bold plus bold 1≤j(max −min) + 1.

3.5 Huffman Encoding
43
Proof (Proof of the theorem of McMillan [ 3]) Letupper CC be a univocal binary code with 
a maximum length of codewords denoted by max and minimum length min (in the 
case of a code over an alphabet with more than 2 symbols the proof is analogous). 
Let upper C Superscript jC j, for any j greater than 1 j > 1, a power of  upper CC, then upper C Superscript jC j has a number of different lengths of 
codewords script l 1 ellipsis script l Subscript m Baselinel1 . . . lm that is at most given by j left parenthesis m a x minus m i n right parenthesis plus 1 j(max −min) + 1. In the Kraft norm 
codewords of a given length give a contribution of less than 11. In fact if in upper CC there 
are n 1n1 codewords long l 1l1, n 2n2 long l 2l2, up to n Subscript mnm long l Subscript mlm, the norm (in base 2) is: 
n 1 2 Superscript minus l 1 plus n 2 2 Superscript minus l 2 plus midline horizontal ellipsis plus n Subscript m Baseline 2 Superscript minus l Super Subscript mn12−l1 + n22−l2 + · · · + nm2−lm
but n 1 less than or equals 2 Superscript minus l 1 Baseline comma n 2 less than or equals 2 Superscript minus l 2 Baseline comma ellipsis comma n Subscript m Baseline less than or equals 2 Superscript minus l Super Subscript m Superscriptn1 ≤2−l1, n2 ≤2−l2, . . . , nm ≤2−lm, whence 
n 1 2 Superscript minus l 1 Baseline plus n 2 2 Superscript minus l 2 Baseline plus midline horizontal ellipsis plus n Subscript m Baseline 2 Superscript minus l Super Subscript m Baseline less than or equals 2 Superscript l 1 Baseline 2 Superscript minus l 1 Baseline plus 2 Superscript l 2 Baseline 2 Superscript minus l 2 Baseline plus midline horizontal ellipsis plus 2 Superscript l Super Subscript m Baseline 2 Superscript minus l Super Subscript m Baseline less than or equals mn12−l1 + n22−l2 + · · · + nm2−lm ≤2l12−l1 + 2l22−l2 + · · · + 2lm2−lm ≤m
Therefore, the norm of upper CC is at most equal to the maximum number of its lengths: 
double vertical bar upper C Superscript j Baseline double vertical bar less than or equals m less than or equals j left parenthesis m a x minus m i n right parenthesis plus 1
∥∥C j∥∥≤m ≤j(max −min) + 1
If upper CC is univocal, then 
double vertical bar upper C Superscript j Baseline double vertical bar equals double vertical bar upper C double vertical bar Superscript j
∥∥C j∥∥= ∥C∥j
whence 
double vertical bar upper C double vertical bar Superscript j Baseline less than or equals j left parenthesis m a x minus m i n right parenthesis plus 1 ∥C∥j ≤j(max −min) + 1
but in this inequality we have on the left j j as an exponent, while on the right it is a 
linear term, therefore it can hold for any j greater than 1 j > 1 only if: 
double vertical bar upper C double vertical bar less than or equals 1 ∥C∥≤1
□
From McMillan’s theorem and the inverse Kraft theorem, we get the following 
corollary. 
Corollary 3.1 If upper CC is univocal, then there exists a preﬁx-free code upper C primeC' such that 
parallel to upper C parallel to equals parallel to upper C prime parallel to∥C∥= ∥C'∥. 
3.5 
Huffman Encoding 
Of course, codes having all codewords of the same length are univocal. However, 
this requirement is often too strong in many cases. Assume that we want to optimize 
the average length of strings encoding data emitted by an information source. In this 
case, it is reasonable to have shorter codewords for data more frequently, allowing 
longer codewords for data seldom appearing in the source generation.

44
3
Information Theory
A code is optimal, with respect to an information source, when its probabilistic 
average length sigma summation Underscript alpha element of upper C Endscripts StartAbsoluteValue alpha EndAbsoluteValue p left parenthesis alpha right parenthesis∑
α∈C |α|p(α) is minimal. Simple algorithms exist providing codes 
that are optimal for a given probability distribution. 
Assume an information source: 
left parenthesis upper A comma p right parenthesis equals Start 2 By 3 Matrix 1st Row 1st Column d 1 2nd Column ellipsis 3rd Column d Subscript n Baseline 2nd Row 1st Column p 1 2nd Column ellipsis 3rd Column p Subscript n EndMatrix (A, p) =
( d1 . . . dn
p1 . . . pn
)
Huffman Encoding is based on the cardinalitykk of the alphabet (here we assume 
a binary code). 
1. Let us order the data according to the increasing order of their probabilities. 
p 1 less than or equals p 2 less than or equals midline horizontal ellipsis less than or equals p Subscript n minus 1 Baseline less than or equals p Subscript np1 ≤p2 ≤· · · ≤pn−1 ≤pn
2. Replace the two data with the lowest probabilities with only one datum having 
the sum of their probabilities. 
3. An edge is labeled by 00 and the other with 11. Go to step 1.  
4. Stop when only one datum is available. The encodings are the sequences of 
labels of each path from the leaves to the root.

3.5 Huffman Encoding
45
Example 3.2 Consider the probabilities given in the following table, an Huffman 
code resulting from these probabilities is given in the following ﬁgure. 
Probability 
a
0.46 
b
0.18 
c
0.09 
d
0.09 
r
0.18

46
3
Information Theory
Codeupper C primeC'
a
0 
b
111 
c
1100 
d
1101 
r
10 
The Huffman method is not deterministic (probabilities can be aggregated, and 
edge labels can be used, in different ways). However, It can be shown that Huffman 
codes are optimal codes [ 3]. 
Huffman’s method gives evidence of the intrinsic relationship between informa-
tion and probability. Consider two playersupper A comma upper BA, B. The playerupper AA knows the probability 
distribution of the source left parenthesis upper X comma p right parenthesis(X, p) and the second player does not know it but can ask 
questions to A of type x less than or equals a question markx ≤a?. The minimum number of questions necessary, on 
average, to guess the most approximate values of the probabilities ofpp hasupper H left parenthesis upper X comma p right parenthesisH(X, p)
as a lower bound. If we organize the questions ofupper BB toupper AA in an encoding tree. It can be 
shown that the lengths of the tree with the minimum number of questions necessary, 
on average, to guess the probabilities ofpp is a Huffman binary encoding of the values 
of upper XX, with respect to the probability pp. The sum of these lengths weighted by the 
corresponding probabilities is an optimal upper approximation to upper H left parenthesis upper X comma p right parenthesisH(X, p). 
Compression 
Optimal encoding is related with the compression of strings. In general terms, a 
compression algorithm is a way of encoding a string alphaα by a string betaβ shorten than alphaα
(let us assume betaβ to belong to the same alphabet of alphaα) such that the original string alphaα
can be completely and univocally recovered from betaβ by a decompression algorithm 
(partial forms of compression, where some loss of information is tolerated, are useful 
in some cases). The compression ratio is given by the fractionStartAbsoluteValue beta EndAbsoluteValue divided by StartAbsoluteValue alpha EndAbsoluteValue|β|/|α|, and the smaller 
it is, the more the compression algorithm reduces the length of the original string. 
Of course, this reduction is paid by a computational cost of the compression and/or 
decompression algorithm. It is easy to realize that a compression algorithm cannot 
be universal because it can give a small ratio only for some strings, but this cannot 
happen for all the strings over the alphabet, otherwise, the compression algorithm 
could encode all the strings of a given length by shorter strings, but this is impossible 
because longer strings are much more than the shorter strings. Nevertheless, it is

3.6 First Shannon Theorem
47
often interesting that compression could really compress some classes of strings for 
which a more compact way is required. Methods of compression are usually based 
on three different principles (in many aspects related). The ﬁrst is based on a known 
distribution of the probabilities of some words occurring in the string. In this case, it 
is enough to deﬁne short encodings for words with high occurrence probabilities and 
longer encodings for those with low probabilities. The second method is based on 
dictionaries. If we know a dictionary of words occurring in a string, then we memorize 
this dictionary and replace the words with the corresponding encodings within the 
text. This provides a compression when we have, for example, one thousand binary 
words of length 20. In fact, one thousand objects need only binary strings of length 
10, therefore we get a reduction of 10 bits for each word occurrence. The third 
method is based on a rearrangement of a string from which the original string can 
uniquely be recovered, in such a way that in the rearranged string similar substrings 
are contiguous. If this happens, an element of type alpha Superscript nαn can be encoded by encoding 
alphaα andnn, that needStartAbsoluteValue alpha EndAbsoluteValue|α| symbols plusupper O left parenthesis log base 10 n right parenthesisO(lg n) symbols instead thann StartAbsoluteValue alpha EndAbsoluteValuen|α| symbols (the 
n copies of alphaα scattered in the original sequence). 
3.6 
First Shannon Theorem 
Now we show that the notion of entropy allows us to determine a lower bound to the 
compression. Let us give a preliminary lemma. 
Lemma 3.2 (Logarithm Lemma) 
Let p 1 comma ellipsis comma p Subscript m Baselinep1, . . . , pm and q 1 comma ellipsis comma q Subscript m Baselineq1, . . . , qm be two probability distributions deﬁned on the 
same set of m data: 
sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline equals sigma summation Underscript i equals 1 Overscript m Endscripts q Subscript i Baseline equals 1 period
m
∑
i=1
pi =
m
∑
i=1
qi = 1.
For any base kk of the logarithms we have: 
minus sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline log Subscript k Baseline p Subscript i Baseline less than or equals minus sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline log Subscript k Baseline q Subscript i −
m
∑
i=1
pi logk pi ≤−
m
∑
i=1
pi logk qi
Proof As it is shown in the following diagram the logarithm is always under the line 
y equals x minus 1y = x −1, therefore: 
ln x less than or equals x minus 1 for all x element of upper R ln x ≤x −1 ∀x ∈R

48
3
Information Theory
If we consider the value x equals StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFractionx = qi
pi
, then we obtain: 
ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction minus 1 ln qi
pi
≤qi
pi
−1
and multiplying both members by p Subscript ipi, and summing we have: 
sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals sigma summation Underscript i Endscripts p Subscript i Baseline left parenthesis StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction minus 1 right parenthesis
∑
i
pi ln qi
pi
≤
∑
i
pi
( qi
pi
−1
)
But, according to the hypothesis: sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline equals sigma summation Underscript i equals 1 Overscript m Endscripts q Subscript i Baseline equals 1
m
∑
i=1
pi =
m
∑
i=1
qi = 1, it holds: 
StartLayout 1st Row 1st Column sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction 2nd Column less than or equals ModifyingAbove sigma summation Underscript i Endscripts q Subscript i Baseline With top brace Overscript 1 Endscripts minus ModifyingAbove sigma summation Underscript i Endscripts p Subscript i Baseline With top brace Overscript 1 Endscripts 2nd Row 1st Column Blank 3rd Row 1st Column sigma summation Underscript i Endscripts p Subscript i Baseline ln StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction 2nd Column less than or equals 0 EndLayout
∑
i
pi ln qi
pi
≤
1
    
∑
i
qi −
1
    
∑
i
pi
∑
i
pi ln qi
pi
≤0
whence: 
sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline StartFraction q Subscript i Baseline Over p Subscript i Baseline EndFraction less than or equals 0
∑
i
pi logk
qi
pi
≤0
(we can pass to any other base k prime greater than 1k' > 1 by multiplying for the constant log base 10 Subscript k prime Baseline klgk' k) 
by the logarithm properties: 
minus sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline p Subscript i Baseline plus sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline q Subscript i Baseline less than or equals 0 −
∑
i
pi logk pi +
∑
i
pi logk qi ≤0

3.6 First Shannon Theorem
49
hence: 
minus sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline p Subscript i Baseline less than or equals minus sigma summation Underscript i Endscripts p Subscript i Baseline log Subscript k Baseline q Subscript i −
∑
i
pi logk pi ≤−
∑
i
pi logk qi
□
Given a codeupper CC encoding data of an information source, the probability distribution 
of data is naturally associated with the codewords of upper CC. We deﬁne upper L Subscript upper CLC as the mean 
length of upper CC by: 
upper L Subscript upper C Baseline equals sigma summation Underscript alpha element of upper C Endscripts StartAbsoluteValue alpha EndAbsoluteValue p left parenthesis alpha right parenthesis periodLC =
∑
α∈C
|α|p(α).
Of course, the shorter is upper L Subscript upper CLC, the most compressed is the encoding of upper CC. The  
following theorem establishes a lower bound to the mean length upper L Subscript upper CLC. 
Theorem 3.1 (First Shannon Theorem) 
Given an information sourceleft parenthesis upper A comma p right parenthesis(A, p) and a preﬁx-free codeupper CC overupper AA, the following 
inequality holds: 
upper L Subscript upper C Baseline greater than or equals upper H left parenthesis upper A comma p right parenthesisLC ≥H(A, p)
Proof Let us consider the following distributionqq (whereaa ranges in the set of data): 
q left parenthesis a right parenthesis equals StartFraction k Superscript minus StartAbsoluteValue c left parenthesis a right parenthesis EndAbsoluteValue Baseline Over double vertical bar upper C double vertical bar EndFractionq(a) = k−|c(a)|
∥C∥
let alsoc left parenthesis a right parenthesisc(a) be the encoding ofaa anddouble vertical bar upper C double vertical bar∥C∥the Kraft norm ofupper CC. From the deﬁnition 
of double vertical bar upper C double vertical bar∥C∥it follows that qq is a probability distribution. 
If kk is the cardinality of the alphabet upper AA, we have (the inequality follows from the 
logarithm lemma): 
StartLayout 1st Row 1st Column upper H left parenthesis upper A comma p right parenthesis 2nd Column equals minus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis log Subscript k Baseline p left parenthesis a right parenthesis 3rd Column Blank 2nd Row 1st Column Blank 2nd Column less than or equals minus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis log Subscript k Baseline q left parenthesis a right parenthesis 3rd Column Blank 3rd Row 1st Column Blank 2nd Column equals minus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis log Subscript k Baseline StartFraction k Superscript minus StartAbsoluteValue c left parenthesis a right parenthesis EndAbsoluteValue Baseline Over double vertical bar upper C double vertical bar EndFraction 3rd Column Blank 4th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis left parenthesis log Subscript k Baseline k Superscript minus StartAbsoluteValue c left parenthesis a right parenthesis EndAbsoluteValue Baseline minus log Subscript k Baseline double vertical bar upper C double vertical bar right parenthesis 3rd Column Blank 5th Row 1st Column Blank 2nd Column equals minus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis log Subscript k Baseline k Superscript minus StartAbsoluteValue c left parenthesis a right parenthesis EndAbsoluteValue Baseline plus sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis log Subscript k Baseline double vertical bar upper C double vertical bar 3rd Column Blank 6th Row 1st Column Blank 7th Row 1st Column Blank 2nd Column equals sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis StartAbsoluteValue c left parenthesis a right parenthesis EndAbsoluteValue plus log Subscript k Baseline double vertical bar upper C double vertical bar left parenthesis sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis right parenthesis period 3rd Column Blank EndLayout
H(A, p) = −
∑
a∈A
p(a) logk p(a)
≤−
∑
a∈A
p(a) logk q(a)
= −
∑
a∈A
p(a) logk
k−|c(a)|
∥C∥
= −
∑
a∈A
p(a)(logk k−|c(a)| −logk ∥C∥)
= −
∑
a∈A
p(a) logk k−|c(a)| +
∑
a∈A
p(a) logk ∥C∥
=
∑
a∈A
p(a) |c(a)| + logk ∥C∥
(∑
a∈A
p(a)
)
.
Being sigma summation Underscript a element of upper A Endscripts p left parenthesis a right parenthesis equals 1
∑
a∈A
p(a) = 1, we obtain: 
upper H left parenthesis upper A comma p right parenthesis less than or equals upper L Subscript upper C Baseline plus log Subscript k Baseline double vertical bar upper C double vertical barH(A, p) ≤LC + logk ∥C∥

50
3
Information Theory
But upper CC is preﬁx-free, so double vertical bar upper C double vertical bar less than or equals 1∥C∥≤1, whence log Subscript k Baseline double vertical bar upper C double vertical bar less than or equals 0logk ∥C∥≤0, that is, the thesis 
follows.
□
3.7 
Typical Sequences 
Given an information sourceleft parenthesis upper A comma p right parenthesis(A, p), we say  typical a sequence where the frequency 
of every datum coincides with the probability of the information source. 
3.7.1 
AEP (Asymptotic Equipartition Property) Theorem 
Let us consider an information source left parenthesis upper A comma p right parenthesis(A, p) emitting data at steps: 1 comma 2 comma ellipsis n comma ellipsis1, 2, . . . n, . . .. 
We ask what is the probability of a typical sequence of lengthnn emitted by the source 
left parenthesis upper A comma p right parenthesis(A, p). We will show that this probability is directly related to the entropy of the 
source. The following proposition holds. 
Proposition 3.10 
1. The probability of each typical sequence of lengthupper NN is2 Superscript minus upper N upper H2−N H; 
2. If upper NN tends to the inﬁnite, then the probability of a sequence long upper NN approaches 
the probability of a typical sequence with the same length; 
3. For upper NN tending to the inﬁnite the cardinality of typical sequences approaches 
2 Superscript upper N upper H2N H. 
Proof Let us consider sequences of length upper NN over data upper A equals left brace a 1 ellipsis a Subscript m Baseline right braceA = {a1 . . . am}. Let  upper N Subscript iNi be 
the number of occurrences a Subscript iai. 
First point. 
In any typical sequencealphaα, for everyi element of StartSet 1 comma ellipsis comma m EndSeti ∈{1, . . . , m},upper N Subscript i Baseline equals upper N p Subscript iNi = Npi, so, its probability is: 
p left parenthesis alpha right parenthesis equals p 1 Superscript upper N p 1 Baseline dot p 2 Superscript upper N p 2 Baseline ellipsis dot p Subscript m Superscript upper N p Super Subscript m Superscript Baseline periodp(α) = pNp1
1
· pNp2
2
. . . · pNpm
m
.
Therefore (for a binary base) we have: 
StartLayout 1st Row 1st Column log p left parenthesis alpha right parenthesis 2nd Column equals log left parenthesis p 1 Superscript upper N p 1 Baseline dot p 2 Superscript upper N p 2 Baseline dot midline horizontal ellipsis dot p Subscript m Superscript upper N p Super Subscript m Superscript Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals log p 1 Superscript upper N p 1 Baseline plus log p 2 Superscript upper N p 2 Baseline plus midline horizontal ellipsis plus log p Subscript m Superscript upper N p Super Subscript m Baseline 3rd Row 1st Column Blank 2nd Column equals sigma summation Underscript i equals 1 Overscript m Endscripts log p Subscript i Superscript upper N p Super Subscript i Baseline 4th Row 1st Column Blank 2nd Column equals sigma summation Underscript i equals 1 Overscript m Endscripts upper N p Subscript i Baseline log p Subscript i Baseline 5th Row 1st Column Blank 2nd Column equals upper N dot sigma summation Underscript i equals 1 Overscript m Endscripts p Subscript i Baseline log p Subscript i Baseline 6th Row 1st Column Blank 2nd Column equals upper N left parenthesis negative upper H right parenthesis 7th Row 1st Column Blank 2nd Column equals minus upper N upper H EndLayout
log p(α) = log(pNp1
1
· pNp2
2
· · · · · pNpm
m
)
= log pNp1
1
+ log pNp2
2
+ · · · + log pNpm
m
=
m
∑
i=1
log pNpi
i
=
m
∑
i=1
Npi log pi
= N ·
m
∑
i=1
pi log pi
= N(−H)
= −N H

3.7 Typical Sequences
51
whence 
p left parenthesis alpha right parenthesis equals 2 Superscript minus upper N upper H Baseline periodp(α) = 2−N H.
Second point. 
From the law of large numbers with the increasing of upper NN we have with probability 
tending to one: 
StartFraction upper N Subscript i Baseline Over upper N EndFraction right arrow p Subscript i Baseline right double arrow upper N Subscript i Baseline right arrow p Subscript i Baseline upper N Ni
N →pi
⇒
Ni →pi N
therefore, increasing upper NN any sequence approaches to a typical sequence of the same 
length. 
Third point. 
The probability of any typical sequence of length upper NN is 2 Superscript minus upper N upper H2−N H, therefore from point 
(2) according to which forupper N right arrow normal infinityN →∞any sequence approaches a typical sequence, the 
number of typical sequences is the inverse 2 Superscript minus upper N upper H2−N H, that is, 2 Superscript upper N upper H2N H.
□
Example 3.3 Let us consider binary sequences where: 
p equals StartSet p 0 comma p 1 EndSet comma p 0 equals 0.89 comma p 1 equals 0.11p = {p0, p1} , p0 = 0.89, p1 = 0.11
where p 0p0 e p 1p1 are the probabilities of 0 and 1 respectively. 
Let us consider sequences of length 100. 
• In 100 bits right double arrow⇒we can assume 89 times 0 and 11 times 1; 
• In 200 bits right double arrow⇒we can assume 178 times 0 and 22 times 1 . 
Let us evaluate the sequences that differ at most in two bits from the ratio 89/11. 
According to the binomial distribution upper B left parenthesis n comma p right parenthesisB(n, p) (nn bits, pp probability of 1): 
upper P left parenthesis upper X equals k right parenthesis equals p Subscript x Baseline left parenthesis k right parenthesis equals left parenthesis Subscript k Superscript n Baseline right parenthesis asterisk p Superscript k Baseline left parenthesis 1 minus p right parenthesis Superscript n minus kP(X = k) = px(k) =
(n
k
)
∗pk(1 −p)n−k
StartLayout 1st Row 1st Column 91 minus 9 2nd Column right double arrow p left parenthesis alpha right parenthesis equals left parenthesis Subscript 9 Superscript 100 Baseline right parenthesis asterisk 0.91 Superscript 91 Baseline asterisk 0.09 Superscript 9 Baseline equals 0.138 2nd Row 1st Column 90 minus 10 2nd Column right double arrow p left parenthesis alpha right parenthesis equals left parenthesis Subscript 10 Superscript 100 Baseline right parenthesis asterisk 0.90 Superscript 90 Baseline asterisk 0.10 Superscript 10 Baseline equals 0.132 3rd Row 1st Column 89 minus 11 2nd Column right double arrow p left parenthesis alpha right parenthesis equals left parenthesis Subscript 11 Superscript 100 Baseline right parenthesis asterisk 0.89 Superscript 89 Baseline asterisk 0.11 Superscript 11 Baseline equals 0.127 4th Row 1st Column 88 minus 12 2nd Column right double arrow p left parenthesis alpha right parenthesis equals left parenthesis Subscript 12 Superscript 100 Baseline right parenthesis asterisk 0.88 Superscript 88 Baseline asterisk 0.12 Superscript 12 Baseline equals 0.122 5th Row 1st Column 87 minus 13 2nd Column right double arrow p left parenthesis alpha right parenthesis equals left parenthesis Subscript 13 Superscript 100 Baseline right parenthesis asterisk 0.87 Superscript 87 Baseline asterisk 0.13 Superscript 13 Baseline equals 0.118 EndLayout
91 −9
⇒
p(α) =
(100
9
)
∗0.9191 ∗0.099 = 0.138
90 −10
⇒
p(α) =
(100
10
)
∗0.9090 ∗0.1010 = 0.132
89 −11
⇒
p(α) =
(100
11
)
∗0.8989 ∗0.1111 = 0.127
88 −12
⇒
p(α) =
(100
12
)
∗0.8888 ∗0.1212 = 0.122
87 −13
⇒
p(α) =
(100
13
)
∗0.8787 ∗0.1313 = 0.118
Summing these probabilities we have a value of 0.637. Therefore, with a proba-
bility greater than 60% we have sequences that are about 10 Superscript negative 1510−15 of the total number 
of possible sequences, which are about 10 Superscript 301030. 
Typical sequences provide a further characterization of entropy as the limit (with 
respect to the length) of the ratio between the logarithm of the number of typical 
sequences and the logarithm of the number of all the possible sequences of a given 
length (logarithms are with respect to a base equal to the number of symbols of the 
alphabet).

52
3
Information Theory
3.8 
Second Shannon Theorem 
The second Theorem of Shannon concerns the transmission of information along a 
channel. Shannon shows that even if the channel is affected by an error, it is possible, 
under suitable conditions, to transmit information by reaching an error probability 
that approaches zero, as the redundancy of encoding increases. In other words, a 
suitable kind of redundancy can compensate for error transmission, by ensuring 
methods of reliable communication. This theorem is strongly based on the notion of 
a typical sequence for an information source. 
The capacity C of a communication channel is the maximum value of mutual 
information between the random variables associated with the information sources 
of the sender and of the receiver, when the probability distribution associated with 
the sender source ranges on the set of all possible probability distributions (this 
maximum can be proved to exist): 
upper C equals max Underscript upper P Endscripts left brace upper I left parenthesis upper X StartAbsoluteValue EndAbsoluteValue upper Y right parenthesis right brace periodC = max
P {I (X||Y)}.
Channel Encoding 
A transmission code is a code used for transmitting, with redundancy that introduces 
more symbols than those necessary to represent all the data, in such a way that it is 
possible to contrast the loss of information due to the noise affecting the channel. The 
Shannon second theorem will explain that in principle this is a possible solution for 
ensuring an error-free transmission even in presence of noise. The following devel-
opments of auto-correcting codes show that this possibility is effectively realizable. 
This is one of the most exciting validations of the correctness of Shannon’s analysis 
of communication in terms of entropic concepts. 
The transmission error is the number of times a message is decoded by the 
receiver with a message different from the message that was sent. 
If alpha equals a 1 comma a 2 ellipsis comma a Subscript n Baselineα = a1, a2 . . . , an is sent but beta equals a 1 comma b 2 comma a 3 ellipsis comma a Subscript n Baselineβ = a1, b2, a3 . . . , an with a 2 not equals b 2a2 /= b2 is received, 
then the channel introduced an error. 
Let’s consider a binary code upper CC encoding 10 messages. Of course, the length of 
each encoding has to be greater than 3. In fact, strings of length 3 can at most encode 
8 messages. A transmission code for a number upper MM of messages uses blocks, that is 
strings, of kk symbols of an alphabet upper AA having a length nn such that k Superscript n Baseline greater than or equals upper Mkn ≥M. 
A transmission code of type left parenthesis upper M comma n right parenthesis(M, n) is a code encoding a number upper MM of possible 
messages, each of them by a sequence of lengthnn (over a ﬁnite alphabet), called also 
a block (all codewords have the same length). 
Let us assume to transmit with a code of type left parenthesis upper M comma n right parenthesis(M, n) and that the alphabet upper AA of 
the code has kk symbols, then k Superscript nkn is the number of possible messages of length nn that 
can be formed over the alphabet upper AA, therefore the inequality upper M less than or equals k Superscript nM ≤kn has to hold.

3.8 Second Shannon Theorem
53
Given a transmission code of type left parenthesis upper M comma n right parenthesis(M, n), its Transmission Rate upper RR is given by: 
upper R equals StartFraction l o g Subscript k Baseline upper M Over n EndFractionR = logk M
n
upper M less than or equals k Superscript nM ≤kn
log Subscript k Baseline upper M less than or equals log Subscript k Baseline k Superscript n logk M ≤logk kn
therefore: 
log Subscript k Baseline upper M less than or equals n logk M ≤n
that is: 
StartFraction log Subscript k Baseline upper M Over n EndFraction less than or equals 1logk M
n
≤1
upper R less than or equals 1R ≤1
upper RR represents the percentage of the used transmission band. Whenupper R less than 1R < 1, then some 
redundancy is introduced. This means that in the case of a binary alphabet, more bits 
than those necessary for encoding the messages are used. 
The rate upper RR is reachable by a family of transmission codes StartSet left parenthesis upper M comma n right parenthesis vertical bar n element of double struck upper N EndSet{(M, n) | n ∈N} if a 
succession of codes of this family exists such that the transmission error approaches 
to zero as nn increases. 
When a code left parenthesis 2 Superscript n upper R Baseline comma n right parenthesis(2nR, n) is used, the transmission rate is given by: 
StartFraction log Subscript 2 Baseline 2 Superscript n upper R Baseline Over n EndFraction equals StartFraction n upper R Over n EndFraction equals upper Rlog2 2nR
n
= nR
n
= R
Let us consider a sender information source upper XX, a channel of capacity upper CC, and a 
transmission process toward a receiver represented by a random variable upper YY, with a 
transmission rate upper RR. The following theorem holds. 
Theorem 3.2 (Second Shannon Theorem) 
Let upper R less than upper CR < C, then there exists a sequence of codes left parenthesis 2 Superscript n upper R Baseline comma n right parenthesis(2nR, n) for which the rate R is 
reachable. 
Proof (Informal Proof) In a transmission process, let upper XX be the random variable of 
the sender and upper YY that one of the receivers. According to the AEP theorem, we can 
consider only typical sequences. The average number of X-typical sequences, given 
one Y-typical sequence, received along the transmission, is expressed by: 
2 Superscript n upper H left parenthesis upper X vertical bar upper Y right parenthesis Baseline period2nH(X|Y).

54
3
Information Theory
The following picture visualizes this fact, where sequences are identiﬁed by points. 
On the left there are X-typical sequences, while on the right there are the correspond-
ing received Y-typical sequences. 
The average probability that one X-typical is chosen as the origin of one Y-typical is 
the ratio between: (i) the number of X-typical given a Y-typical sequences, and (ii) 
the number X-typical. This ratio can be expressed, in terms of conditional entropy, 
in the following way: 
StartFraction 2 Superscript n upper H left parenthesis upper X vertical bar upper Y right parenthesis Baseline Over 2 Superscript n upper H left parenthesis upper X right parenthesis Baseline EndFraction equals 2 Superscript n upper H left parenthesis upper X vertical bar upper Y right parenthesis minus upper H left parenthesis upper X right parenthesis Baseline period2nH(X|Y)
2nH(X) = 2nH(X|Y)−H(X).
From a property of mutual information we have: 
upper I left parenthesis upper X comma upper Y right parenthesis equals upper H left parenthesis upper X right parenthesis minus upper H left parenthesis upper X vertical bar upper Y right parenthesisI (X, Y) = H(X) −H(X|Y)
therefore, the average probability of determining one X-typical sequence as origin 
of one Y-typical is: 
2 Superscript minus n upper I left parenthesis upper X comma upper Y right parenthesis2−nI (X,Y)
From this probability, we can evaluate the probability of error. In fact, the number 
of wrong messages is equal to the number of possible messages minus 1 (the only 
right one), that is: 
2 Superscript n upper R Baseline minus 1 almost equals 2 Superscript n upper R2nR −1 ≈2nR
therefore, the error probability upper EE is given by: 
StartLayout 1st Row 1st Column upper E 2nd Column equals 2 Superscript n upper R Baseline asterisk 2 Superscript minus n upper I left parenthesis upper X comma upper Y right parenthesis Baseline 2nd Row 1st Column Blank 3rd Row 1st Column upper E 2nd Column equals 2 Superscript n upper R minus n upper C Baseline 4th Row 1st Column Blank 5th Row 1st Column upper E 2nd Column equals 2 Superscript minus n left parenthesis upper C minus upper R right parenthesis EndLayout
E = 2nR ∗2−nI (X,Y)
E = 2nR−nC
E = 2−n(C−R)
whence if upper R less than upper CR < C the error probability upper EE approaches to zero.
□

3.9 Signals and Continuous Distributions
55
3.9 
Signals and Continuous Distributions 
Waves are periodical functions that propagate in space. For this reason, they can 
transport data and therefore can be used as channels for transmitting the information. 
However, their nature is continuous. Hence, if we want to apply to waves the entropic 
concepts, we need to extend them to the case of continuous distributions. This can 
be done in a very natural manner, by replacing sums with integrals. Namely, iff left parenthesis x right parenthesis f (x)
is a continuous distribution deﬁned over a set upper AA of values, then its entropy upper H left parenthesis f right parenthesisH( f ) is 
given by: 
upper H left parenthesis f right parenthesis equals integral Underscript upper A Endscripts f left parenthesis x right parenthesis log base 10 f left parenthesis x right parenthesis d x periodH( f ) =
∫
A
f (x) lg f (x)dx.
Analogously, if f comma g f, g are distributions over the same set of values upper AA, then the 
entropic divergence of f f with respect to gg is given by: 
upper D left parenthesis f comma g right parenthesis equals integral Underscript upper A Endscripts f left parenthesis x right parenthesis left bracket log base 10 f left parenthesis x right parenthesis minus log base 10 g left parenthesis x right parenthesis right bracket d x periodD( f, g) =
∫
A
f (x)[lg f (x) −lg g(x)]dx.
and the mutual information of random variable upper XX with respect to a random variable 
upper YY, on the same range of values, is given by: 
upper I left parenthesis f comma g right parenthesis equals integral Underscript upper A Endscripts f left parenthesis x right parenthesis left bracket log base 10 f Subscript upper X comma upper Y Baseline left parenthesis x comma y right parenthesis minus log base 10 left parenthesis f Subscript upper X Baseline left parenthesis x right parenthesis f Subscript upper Y Baseline left parenthesis y right parenthesis right parenthesis right bracket d x periodI ( f, g) =
∫
A
f (x)[lg fX,Y(x, y) −lg( fX(x) fY(y))]dx.
where f Subscript upper X comma upper Y Baseline left parenthesis x comma y right parenthesis fX,Y(x, y) is the distribution of the joint events, while f Subscript upper X fX and f Subscript upper Y fY are the 
(marginal) probabilities of the same events. 
Let upper N left parenthesis x right parenthesisN(x) denote the normal probability distribution upper N left parenthesis x right parenthesis equals StartFraction 1 Over StartRoot 2 pi EndRoot sigma EndFraction e Superscript minus StartFraction x squared Over 2 sigma squared EndFractionN(x) =
1
√
2πσ e−x2
2σ2 , and 
upper H left parenthesis f right parenthesisH( f ) be the Shannon entropy of a probability distribution f f (here upper D left parenthesis f comma upper N right parenthesisD ( f, N) is the 
continuous version of Kullback-Leibler divergence). The following propositions are 
proven in [ 3]. 
Proposition 3.11 
upper H left parenthesis upper N right parenthesis equals one half ln left parenthesis 2 pi e sigma squared right parenthesisH(N) = 1
2 ln(2πeσ 2)
(3.10) 
Proof ([ 3]). 
upper H left parenthesis upper N right parenthesis equals minus integral Subscript negative normal infinity Superscript plus normal infinity Baseline upper N left parenthesis x right parenthesis ln upper N left parenthesis x right parenthesis d xH(N) = −
+∞
∫
−∞
N(x) ln N(x)dx
equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline minus upper N left parenthesis x right parenthesis ln StartFraction e Superscript minus StartFraction x squared Over 2 sigma squared EndFraction Baseline Over StartRoot 2 pi sigma squared EndRoot EndFraction d x=
+∞
∫
−∞
−N(x) ln e−x2
2σ2
√
2πσ 2 dx

56
3
Information Theory
equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline minus upper N left parenthesis x right parenthesis left bracket minus StartFraction x squared Over 2 sigma squared EndFraction minus ln StartRoot 2 pi sigma squared EndRoot right bracket d x=
+∞
∫
−∞
−N(x)[−x2
2σ 2 −ln
√
2πσ 2]dx
equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline StartFraction upper N left parenthesis x right parenthesis x squared Over 2 sigma squared EndFraction d x plus ln StartRoot 2 pi sigma squared EndRoot integral Subscript negative normal infinity Superscript plus normal infinity Baseline upper N left parenthesis x right parenthesis d x=
+∞
∫
−∞
N(x)x2
2σ 2
dx + ln
√
2πσ 2
+∞
∫
−∞
N(x)dx
equals StartFraction upper E left parenthesis x squared right parenthesis Over 2 sigma squared EndFraction plus ln StartRoot 2 pi sigma squared EndRoot dot 1= E(x2)
2σ 2
+ ln
√
2πσ 2 · 1
equals one half plus one half ln 2 pi sigma squared= 1
2 + 1
2 ln 2πσ 2
equals one half ln 2 pi e sigma squared= 1
2 ln 2πeσ 2.
□
Proposition 3.12 Normal distributions are those for which entropy reaches the max-
imum value in the class of probability distributions having the same variance. 
Proof ([ 3]) Let f f denote any probability distribution of variance upper V a r left parenthesis f right parenthesis equals sigma squaredVar( f ) = σ 2. 
upper D left parenthesis f comma upper N right parenthesis equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln StartFraction f left parenthesis x right parenthesis Over upper N left parenthesis x right parenthesis EndFraction d xD ( f, N) =
+∞
∫
−∞
f (x) ln f (x)
N (x)dx
upper D left parenthesis f comma upper N right parenthesis equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln f left parenthesis x right parenthesis d x minus integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln upper N left parenthesis x right parenthesis d xD ( f, N) =
+∞
∫
−∞
f (x) ln f (x)dx −
+∞
∫
−∞
f (x) ln N(x)dx
upper D left parenthesis f comma upper N right parenthesis equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln f left parenthesis x right parenthesis d x minus integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln StartFraction e Superscript minus StartFraction x squared Over 2 sigma squared EndFraction Baseline Over StartRoot 2 pi sigma squared EndRoot EndFraction d xD ( f, N) =
+∞
∫
−∞
f (x) ln f (x)dx −
+∞
∫
−∞
f (x) ln e−x2
2σ2
√
2πσ 2 dx
upper D left parenthesis f comma upper N right parenthesis equals minus upper S left parenthesis f right parenthesis minus integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis ln e Superscript minus StartFraction x squared Over 2 sigma squared EndFraction Baseline d x plus ln StartRoot 2 pi sigma squared EndRoot integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis d xD ( f, N) = −S( f ) −
+∞
∫
−∞
f (x) ln e−x2
2σ2 dx + ln
√
2πσ 2
+∞
∫
−∞
f (x)dx
upper D left parenthesis f comma upper N right parenthesis equals minus upper S left parenthesis f right parenthesis plus StartFraction 1 Over 2 sigma squared EndFraction integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis x right parenthesis x squared d x plus one half ln 2 pi sigma squared dot 1D ( f, N) = −S( f ) +
1
2σ 2
+∞
∫
−∞
f (x)x2dx + 1
2 ln 2πσ 2 · 1
upper D left parenthesis f comma upper N right parenthesis equals minus upper S left parenthesis f right parenthesis plus StartFraction 1 Over 2 sigma squared EndFraction upper V a r left parenthesis f right parenthesis plus one half ln left parenthesis 2 pi sigma squared right parenthesisD ( f, N) = −S( f ) +
1
2σ 2 Var( f ) + 1
2 ln(2πσ 2) (upper V a r left parenthesis f right parenthesis less than or equals sigma squaredVar( f ) ≤σ 2) 
upper D left parenthesis f comma upper N right parenthesis less than or equals minus upper S left parenthesis f right parenthesis plus one half plus one half ln left parenthesis 2 pi sigma squared right parenthesisD ( f, N) ≤−S( f ) + 1
2 + 1
2 ln(2πσ 2)
upper D left parenthesis f comma upper N right parenthesis equals minus upper S left parenthesis f right parenthesis plus one half left parenthesis ln e plus ln left parenthesis 2 pi sigma squared right parenthesis right parenthesisD ( f, N) = −S( f ) + 1
2(ln e + ln(2πσ 2))
upper D left parenthesis f parallel to upper N right parenthesis less than or equals minus upper S left parenthesis f right parenthesis plus one half ln left parenthesis 2 pi e sigma squared right parenthesisD( f ∥N) ≤−S( f ) + 1
2 ln(2πeσ 2) (upper D left parenthesis f parallel to upper N right parenthesis greater than or equals 0D( f ∥N) ≥0) 
therefore upper H left parenthesis f right parenthesis less than or equals upper H left parenthesis upper N right parenthesis periodH( f ) ≤H(N).
□
The notion of a channel is very broad, it is any medium through which data, 
which are always physical objects, move in the space from one place where the 
sender is located to another place where the receiver is located. If data are sounds 
they propagate in the air, if are molecules they may ﬂoat in the water. The most 
interesting case in many applications is propagation through electromagnetic waves.

3.9 Signals and Continuous Distributions
57
These waves propagate in space without no other medium. In this way, data can 
be transmitted by modulating waves by altering their forms, in such a way that the 
alterations correspond, according to speciﬁc criteria, to “write” on them codewords. 
When the signal is received by the receiver, the alterations are “read” and codewords 
are recovered by obtaining the data that were transmitted by means of the signals. 
From a mathematical point of view, a signal is a periodic, in time, function that 
propagates in space. The signal band is the quantity of information (number of bits) 
that the signal can transmit in the unit of time. In this section, we will show that this 
quantity is related to the frequency of the signal (sampling theorem). 
a
a
MOD
DEM 
This is the general schema of a communication: 
StartLayout 1st Row 1st Column Blank 2nd Column Source 3rd Column data hyphen encoding 2nd Row 1st Column Blank 2nd Column Sender 3rd Column message hyphen encoding 3rd Row 1st Column Blank 2nd Column Emittent 3rd Column channel hyphen encoding 4th Row 1st Column Blank 2nd Column Signal 3rd Column modulation 5th Row 1st Column Blank 2nd Column Channel 3rd Column transmission 6th Row 1st Column Blank 2nd Column Recipient 3rd Column demodulation 7th Row 1st Column Blank 2nd Column Receiver 3rd Column message hyphen decoding 8th Row 1st Column Blank 2nd Column Destination 3rd Column data hyphen decoding EndLayout
Source
data-encoding
Sender
message-encoding
Emittent
channel-encoding
Signal
modulation
Channel
transmission
Recipient
demodulation
Receiver
message-decoding
Destination data-decoding
3.9.1 
Fourier Series 
A periodic continuous (or piecewise continuous) function f left parenthesis t right parenthesis f (t) (f left parenthesis t right parenthesis equals f left parenthesis t plus n asterisk upper T right parenthesis f (t) = f (t + n ∗
T )), can be completely represented as an inﬁnite sum of sine and cosine functions. 
StartLayout 1st Row 1st Column f left parenthesis t right parenthesis equals 2nd Column a 0 3rd Column plus ModifyingBelow a 1 sine left parenthesis omega t right parenthesis plus b 1 cosine left parenthesis omega t right parenthesis With bottom brace 4th Column plus 5th Column ModifyingBelow sigma summation Underscript n equals 2 Overscript normal infinity Endscripts a Subscript n Baseline sine left parenthesis n omega t right parenthesis plus sigma summation Underscript n equals 2 Overscript normal infinity Endscripts b Subscript n Baseline cosine left parenthesis n omega t right parenthesis With bottom brace 2nd Row 1st Column Blank 2nd Column c o n s t a n t 3rd Column f u n d a m e n t a l h a r m o n i c 4th Column Blank 5th Column o t h e r h a r m o n i c s EndLayout
f (t) =
a0
+ a1 sin(ωt) + b1 cos(ωt)
 
  
 +
∞
∑
n=2
an sin(nωt) +
∞
∑
n=2
bn cos(nωt)
 
  
 
constant
f undamental harmonic
other harmonics
where: 
• upper TT is the period and nu equals StartFraction 1 Over upper T EndFractionν = 1
T is the frequency (number of periods in a second) 
• omega equals 2 pi nuω = 2πν is called pulsation 
• the coefﬁcients of Fourier series are given by: 
StartLayout 1st Row 1st Column a 0 2nd Column equals 3rd Column StartFraction 1 Over upper T EndFraction integral Subscript 0 Superscript upper T Baseline f left parenthesis t right parenthesis d t 2nd Row 1st Column a Subscript n 2nd Column equals 3rd Column StartFraction 2 Over upper T EndFraction integral Subscript 0 Superscript upper T Baseline f left parenthesis t right parenthesis sine left parenthesis n omega t right parenthesis d t 3rd Row 1st Column b Subscript n 2nd Column equals 3rd Column StartFraction 2 Over upper T EndFraction integral Subscript 0 Superscript upper T Baseline f left parenthesis t right parenthesis cosine left parenthesis n omega t right parenthesis d t EndLayout
a0 = 1
T
∫T
0 f (t)dt
an = 2
T
∫T
0 f (t) sin(nωt)dt
bn = 2
T
∫T
0 f (t) cos(nωt)dt

58
3
Information Theory
For example, let f left parenthesis t right parenthesis f (t) be given by: 
StartLayout 1st Row 1st Column f left parenthesis t right parenthesis 2nd Column equals 3rd Column Blank 4th Column Blank 5th Column ModifyingBelow 4 asterisk sine left parenthesis StartFraction 2 pi Over 50 EndFraction t right parenthesis plus 6 asterisk cosine left parenthesis StartFraction 2 pi Over 50 EndFraction t right parenthesis With bottom brace 6th Column plus 7th Column ModifyingBelow 3 asterisk sine left parenthesis 2 asterisk StartFraction 2 pi Over 20 EndFraction t right parenthesis plus 8 asterisk cosine left parenthesis 2 asterisk StartFraction 2 pi Over 20 EndFraction t right parenthesis With bottom brace EndLayout f (t) =
4 ∗sin(2π
50 t) + 6 ∗cos(2π
50 t)
 
  
 
+ 3 ∗sin(2 ∗2π
20 t) + 8 ∗cos(2 ∗2π
20 t)
 
  
 
In a Fourier trigonometric representation (with1 comma sine x comma cosine x comma sine 2 x comma cosine 2 x period period period1, sin x, cos x, sin 2x, cos 2x...). The 
sum converges to the values of f f and, in the case of discontinuity points, the sum 
converges to the average of left and right limits. 
The signal above can be composed by the following components. 
StartLayout 1st Row 1st Column g left parenthesis t right parenthesis 2nd Column equals 3rd Column ModifyingBelow 4 asterisk sine left parenthesis StartFraction 2 pi Over 50 EndFraction t right parenthesis plus 6 asterisk cosine left parenthesis StartFraction 2 pi Over 50 EndFraction t right parenthesis With bottom brace 2nd Row 1st Column Blank 2nd Column Blank 3rd Column fundamental harmonic EndLayout
g(t) = 4 ∗sin(2π
50 t) + 6 ∗cos(2π
50 t)
 
  
 
fundamental harmonic
StartLayout 1st Row 1st Column h left parenthesis t right parenthesis 2nd Column equals 3rd Column ModifyingBelow 3 asterisk sine left parenthesis 2 asterisk StartFraction 2 pi Over 20 EndFraction t right parenthesis plus 8 asterisk cosine left parenthesis 2 asterisk StartFraction 2 pi Over 20 EndFraction t right parenthesis With bottom brace 2nd Row 1st Column Blank 2nd Column Blank 3rd Column second harmonic EndLayout
h(t) = 3 ∗sin(2 ∗2π
20 t) + 8 ∗cos(2 ∗2π
20 t)
 
  
 
second harmonic
The two harmonics are:

3.9 Signals and Continuous Distributions
59
summing them in all the points we obtain the composite signal. 
Let us recall Euler formulae for complex numbers, where thetaϑ is real and ii is the 
imaginary unit, are: 
e Superscript i theta Baseline equals cosine theta plus i sine thetaeiϑ = cos ϑ + i sin ϑ
e Superscript minus i theta Baseline equals cosine theta minus i sine thetae−iϑ = cos ϑ −i sin ϑ
sine theta equals StartFraction e Superscript i theta Baseline minus e Superscript minus i theta Baseline Over 2 i EndFraction sin ϑ = eiϑ −e−iϑ
2i
cosine theta equals StartFraction e Superscript i theta Baseline plus e Superscript minus i theta Baseline Over 2 EndFraction cos ϑ = eiϑ + e−iϑ
2
Fourier summation in the ﬁeld of complex numbers, by using Euler formulae, gives: 
StartLayout 1st Row 1st Column Blank 2nd Column f left parenthesis t right parenthesis 3rd Column equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts c Subscript n Baseline e Superscript i n omega t Baseline 2nd Row 1st Column Blank 2nd Column c Subscript n 3rd Column equals StartFraction 1 Over 2 upper T EndFraction integral Subscript negative upper T Superscript upper T Baseline f left parenthesis t right parenthesis e Superscript minus i n omega t Baseline d t EndLayout
f (t) =
+∞
∑
n=−∞
cneinωt
cn
=
1
2T
T
∫
−T
f (t)e−inωtdt
Now we explain as Fourier representations are naturally deﬁnable in the general 
setting of Hilbert spaces. A vector space over an algebraic ﬁeld is a structure that 
axiomatically expresses the nature ofbold upper R Superscript nRn (nn-tuples of real numbers) with the sum (of 
each component), a product by a scalar value (multiplying all the components for the 
same value) and the zero vector with all components zero. A basis for such a space 
is a set of vectors such that any other vector can be obtained by sum and product 
by scalars from some vectors of the basis. A Hilbert space is a vector space that is 
complete, that is, any Cauchy sequence of vectors converges to some vector of the 
space, and moreover, a scalar product is deﬁned, that associates to any pair of vectors 
u comma vu, v a real value less than u vertical bar v greater than< u|v >. A special case of Hilbert space is the functions upper L squared left bracket a comma b right bracketL2[a, b]
deﬁned on real numbers such that their square is integrable in some closed interval 
left bracket a comma b right bracket[a, b]. This space is extended to the complex functions with the scalar product, where 
f Superscript asterisk f ∗is the complex conjugate of the function f f : 
less than f vertical bar g greater than equals integral Subscript a Superscript b Baseline f left parenthesis x right parenthesis f Superscript asterisk Baseline left parenthesis x right parenthesis < f |g >=
b
∫
a
f (x) f ∗(x)
A basis of vectors is orthonormal if the scalar product is null between any two 
vectors of the basis, and moreover less than v vertical bar v greater than equals 1< v|v >= 1 for any vv of the basis. 
In the case of a ﬁnite basis, we have:

60
3
Information Theory
v equals sigma summation Underscript n equals negative 1 Overscript upper N Endscripts c Subscript n Baseline e Subscript nv =
N
∑
n=−1
cnen
and for every vv in the basis and n equals 1 comma ellipsis upper Nn = 1, . . . N: 
c Subscript n Baseline equals less than v vertical bar e Subscript n Baseline greater thancn =< v|en >
In any Hilbert space, the following propositions hold. 
Proposition 3.13 Given a orthonormal basis of a Hilbert space, for every vector vv
of a basis upper BB we have: 
v equals sigma summation Underscript e element of upper B Endscripts less than v vertical bar e greater than ev =
∑
e∈B
< v|e > e
Proposition 3.14 In the spaceupper L squared left bracket l comma negative l right bracketL2[l, −l] of complex function square-integrable com-
plex functions the following functions are a basis for that space: 
StartFraction 1 Over StartRoot 2 l EndRoot EndFraction StartSet e Superscript i StartFraction pi Over l EndFraction n x Baseline vertical bar n element of bold upper N EndSet period 1
√
2l
{ei π
l nx | n ∈N}.
Proof Let n not equals mn /= m and n minus m equals kn −m = k. 
less than f vertical bar g greater than equals StartFraction 1 Over 2 l EndFraction integral Subscript negative l Superscript l Baseline e Superscript minus i StartFraction pi Over l EndFraction n t Baseline e Superscript i StartFraction pi Over l EndFraction n t Baseline d t equals StartFraction 1 Over 2 l EndFraction integral Subscript negative l Superscript l Baseline d t equals 1 < f |g >= 1
2l
l∫
−l
e−i π
l ntei π
l ntdt = 1
2l
l∫
−l
dt = 1
less than f vertical bar f greater than equals StartFraction 1 Over 2 l EndFraction integral Subscript negative l Superscript l Baseline e Superscript minus i StartFraction pi Over l EndFraction n t Baseline e Superscript i StartFraction pi Over l EndFraction m t Baseline d t equals< f | f >= 1
2l
∫l
−l e−i π
l ntei π
l mtdt =
StartFraction 1 Over 2 l EndFraction integral Subscript negative l Superscript l Baseline e Superscript i StartFraction pi Over l EndFraction k t Baseline d t equals 1
2l
∫l
−l ei π
l ktdt =
StartFraction l Over 2 i l k pi EndFraction left bracket e Superscript i StartFraction k pi t Over l EndFraction Baseline right bracket Subscript negative l Superscript l Baseline equals
l
2ilkπ
[
ei kπt
l
]l
−l =
StartFraction e Superscript i k pi Baseline minus e Superscript minus i k pi Baseline Over 2 i k pi EndFraction equals StartFraction sine k pi Over k pi EndFraction equals 0 eikπ−e−ikπ
2ikπ
= sin kπ
kπ
= 0.
□
From the two propositions above the Fourier’s representation follows as a conse-
quence of the Hilbert space structure of circular functions (ll instead of upper TT is used in 
order to be coherent with the following notation). 
Proposition 3.15 Every function in upper L squared left bracket negative l comma l right bracketL2[−l,l] can be expressed as a linear combi-
nation (with complex coefﬁcients) of circular functions. 
f left parenthesis x right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts c Subscript n Baseline e Superscript i StartFraction pi Over l EndFraction n x f (x) =
+∞
∑
n=−∞
cnei π
l nx
(3.11) 
c Subscript n Baseline equals StartFraction 1 Over 2 l EndFraction integral Subscript l Superscript l Baseline f left parenthesis x right parenthesis e Superscript minus i StartFraction pi Over l EndFraction n x Baseline d xcn = 1
2l
∫l
l
f (x)e−i π
l nxdx
(3.12)

3.10 Fourier Transform
61
Proof Using the basis of upper L squared left bracket negative l comma l right bracketL2[−l,l]: 
f left parenthesis x right parenthesis equals sigma summation Underscript e element of upper B Endscripts less than f left parenthesis x right parenthesis vertical bar StartFraction 1 Over StartRoot 2 l EndRoot EndFraction e Superscript i StartFraction pi Over l EndFraction n x Baseline greater than StartFraction 1 Over StartRoot 2 l EndRoot EndFraction e Superscript i StartFraction pi Over l EndFraction n x Baseline period f (x) =
∑
e∈B
< f (x)| 1
√
2l
ei π
l nx >
1
√
2l
ei π
l nx.
□
We remark that for technical reasons, which we do not explicit more precisely, in 
the context of Hilbert space a notion of integral more general than Riemann integral 
is used, which was introduced by Lebesgue in the context of the general measure 
theory. 
3.10 
Fourier Transform 
The Fourier Transform is obtained with a process that generalizes the coefﬁcients in 
the Fourier representation of functions. 
Let 
omega Subscript n Baseline equals n StartFraction pi Over l EndFractionωn = n π
l
then: 
c Subscript n Baseline equals StartFraction 1 Over 2 l EndFraction integral Subscript negative l Superscript l Baseline f left parenthesis x right parenthesis e Superscript minus i omega Super Subscript n Superscript x Baseline d xcn = 1
2l
l∫
−l
f (x)e−iωnxdx
Let assume that whenll goes to the inﬁnite the integral approaches to a ﬁnite value, 
and replace coefﬁcients c Subscript ncn by c Subscript n Baseline StartFraction l Over pi EndFractioncn l
π , by obtaining: 
c Subscript n Baseline StartFraction l Over pi EndFraction equals StartFraction 1 Over 2 pi EndFraction integral Subscript negative l Superscript l Baseline f left parenthesis x right parenthesis e Superscript minus i omega Super Subscript n Superscript x Baseline d xcn
l
π = 1
2π
l∫
−l
f (x)e−iωnxdx
now replacing c Subscript ncn by c left parenthesis omega right parenthesisc(ω) we obtain: 
c left parenthesis omega right parenthesis equals StartFraction 1 Over 2 pi EndFraction integral Subscript negative normal infinity Superscript normal infinity Baseline f left parenthesis x right parenthesis e Superscript minus i omega x Baseline d xc(ω) = 1
2π
∞
∫
−∞
f (x)e−iωxdx
f left parenthesis x right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts c Subscript n Baseline StartFraction l Over pi EndFraction e Superscript i omega Super Subscript n Superscript x Baseline StartFraction pi Over l EndFraction f (x) =
+∞
∑
n=−∞
cn
l
π eiωnx π
l
where replacing c Subscript n Baseline l divided by picnl/π by c left parenthesis omega Subscript n Baseline right parenthesisc(ωn) we have:

62
3
Information Theory
f left parenthesis x right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts c left parenthesis omega Subscript n Baseline right parenthesis e Superscript i omega Super Subscript n Superscript x Baseline StartFraction pi Over l EndFraction f (x) =
+∞
∑
n=−∞
c(ωn)eiωnx π
l
when in this sum ll goes to the inﬁnite we obtain the following integral: 
f left parenthesis x right parenthesis equals integral Subscript negative normal infinity Superscript normal infinity Baseline c left parenthesis omega right parenthesis e Superscript i omega x Baseline d x period f (x) =
∞
∫
−∞
c(ω)eiωxdx.
In the end, we obtain the following pair where the complex valueupper F left parenthesis omega right parenthesisF(ω) generalizes 
the coefﬁcients of Fourier representation. The next theorem shows the advantage of 
such a generalization. 
ModifyingBelow StartLayout 1st Row 1st Column f left parenthesis t right parenthesis 2nd Column equals 3rd Column integral Subscript negative normal infinity Superscript plus normal infinity Baseline upper F left parenthesis omega right parenthesis e Superscript i omega t Baseline d omega left parenthesis 1 right parenthesis 2nd Row 1st Column upper F left parenthesis omega right parenthesis 2nd Column equals 3rd Column StartFraction 1 Over 2 pi EndFraction integral Subscript negative normal infinity Superscript plus normal infinity Baseline f left parenthesis t right parenthesis e Superscript minus i omega t Baseline d t left parenthesis 2 right parenthesis EndLayout With bottom brace
f (t) =
+∞
∫
−∞
F(ω)eiωtdω (1)
F(ω) =
1
2π
+∞
∫
−∞
f (t)e−iωtdt
(2)
 
  
 
Pair of Fourier Transforms 
upper F left parenthesis omega right parenthesisF(ω) is the Fourier Transform of f left parenthesis t right parenthesis right parenthesis f (t)) and f left parenthesis t right parenthesis f (t) is the anti-transform of upper F left parenthesis omega right parenthesisF(ω). 
It can be shown that the pair (f f ,upper FF) is completely determined by each of the two 
components. 
3.11 
Sampling Theorem 
The following theorem highlights the role of signals in transporting information. 
Proposition 3.16 (Sampling Theorem (Nyquist-Shannon)) Let f left parenthesis t right parenthesis f (t) be a periodic 
function with integrable square, in its period, and with limited bandupper WW (components 
of frequencies outside left bracket negative upper W comma plus upper W right bracket[−W, +W] are zero), then f left parenthesis t right parenthesis f (t) is completely determined by 
the values assumed at time intervals with distance StartFraction 1 Over 2 upper W EndFraction 1
2W : left bracket 0 comma StartFraction 1 Over 2 upper W EndFraction comma 2 StartFraction 1 Over 2 upper W EndFraction comma 3 StartFraction 1 Over 2 upper W EndFraction comma period period period right bracket
[
0,
1
2W , 2 1
2W , 3 1
2W , ...
]
. 
T 
t=1/2W 
Proof Using the Fourier transform we have: 
f left parenthesis t right parenthesis equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline upper F left parenthesis omega right parenthesis e Superscript i omega t Baseline d omega f (t) =
+∞
∫
−∞
F(ω)eiωtdω

3.11 Sampling Theorem
63
If we consider frequencies in the intervalleft bracket negative upper W comma plus upper W right bracket[−W, +W], then we integrate between 
negative a equals minus 2 pi upper W−a = −2πW ad a equals plus 2 pi upper Wa = +2πW: 
f left parenthesis t right parenthesis equals integral Subscript negative a Superscript plus a Baseline upper F left parenthesis omega right parenthesis e Superscript i omega t Baseline d omega f (t) =
+a
∫
−a
F(ω)eiωtdω
(3.13) 
the function f left parenthesis t right parenthesis f (t) in the points minus StartFraction pi Over a EndFraction n−π
a n is: 
f left parenthesis minus StartFraction pi Over a EndFraction n right parenthesis equals integral Subscript negative a Superscript plus a Baseline upper F left parenthesis omega right parenthesis e Superscript minus i omega StartFraction pi Over a EndFraction n Baseline d omega period f (−π
a n) =
+a
∫
−a
F(ω)e−iω π
a ndω.
(3.14) 
Now let us consider the anti-transform upper F left parenthesis omega right parenthesisF(ω) within left bracket negative a comma a right bracket[−a, a]: 
upper F left parenthesis omega right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts d Subscript n Baseline e Superscript i StartFraction pi omega Over a EndFraction nF(ω) =
+∞
∑
n=−∞
dnei πω
a n
(3.15) 
with coefﬁcients d Subscript ndn given by Eq. (3.12): 
d Subscript n Baseline equals StartFraction 1 Over 2 a EndFraction integral Subscript negative a Superscript a Baseline upper F left parenthesis omega right parenthesis e Superscript minus i StartFraction pi omega Over a EndFraction n Baseline d omegadn = 1
2a
a
∫
−a
F(ω)e−i πω
a ndω
(3.16) 
but from Eqs. (3.13) and (3.14) we have:  
d Subscript n Baseline equals StartFraction 1 Over 2 a EndFraction f left parenthesis minus StartFraction pi Over a EndFraction n right parenthesis perioddn = 1
2a f
(
−π
a n
)
.
(3.17) 
Therefore, the values of f f in minus StartFraction pi Over a EndFraction n−π
a n coincide with the coefﬁcients of the Fourier 
Transform of f f . But this transform completely determines f f . In conclusion, the 
values of f f in pi divided by a equals 1 divided by 2 upper Wπ/a = 1/2W allow us to recover the whole function f f . 
If we substitute the right member of (3.17) in (3.15): 
upper F left parenthesis omega right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts StartFraction 1 Over 2 a EndFraction f left parenthesis minus StartFraction pi Over a EndFraction n right parenthesis e Superscript i StartFraction pi omega Over a EndFraction nF(ω) =
+∞
∑
n=−∞
1
2a f (−π
a n)ei πω
a n
(3.18) 
and then the right member of (3.18) in (3.13): 
f left parenthesis t right parenthesis equals integral Subscript negative a Superscript plus a Baseline left parenthesis StartFraction 1 Over 2 a EndFraction sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts f left parenthesis StartFraction pi Over a EndFraction n right parenthesis e Superscript minus i StartFraction pi omega Over a EndFraction n Baseline right parenthesis e Superscript i omega t Baseline d omega f (t) =
+a
∫
−a
(
1
2a
+∞
∑
n=−∞
f (π
a n)e−i πω
a n
)
eiωtdω
(3.19) 
therefore:

64
3
Information Theory
f left parenthesis t right parenthesis equals StartFraction 1 Over 2 a EndFraction sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts f left parenthesis StartFraction pi Over a EndFraction n right parenthesis integral Subscript negative a Superscript plus a Baseline e Superscript i omega left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis Baseline d omega f (t) = 1
2a
+∞
∑
n=−∞
f (π
a n)
+a
∫
−a
eiω(t−π
a n)dω
(3.20) 
giving: 
f left parenthesis t right parenthesis equals StartFraction 1 Over 2 a EndFraction sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts f left parenthesis StartFraction pi Over a EndFraction n right parenthesis StartFraction 1 Over i EndFraction left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis left bracket e Superscript i a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis Baseline minus e Superscript minus i a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis Baseline right bracket f (t) = 1
2a
+∞
∑
n=−∞
f
(π
a n
)1
i (t −π
a n)
[
eia(t−π
a n) −e−ia(t−π
a n)]
(3.21) 
that is: 
f left parenthesis t right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts f left parenthesis StartFraction pi Over a EndFraction n right parenthesis StartStartFraction 1 OverOver a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis EndEndFraction left bracket StartFraction e Superscript i a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis Baseline minus e Superscript minus i a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis Baseline Over 2 i EndFraction right bracket f (t) =
+∞
∑
n=−∞
f
(π
a n
)
1
a(t −π
a n)
[eia(t−π
a n) −e−ia(t−π
a n)
2i
]
(3.22) 
whence, by Euler formulae: 
f left parenthesis t right parenthesis equals sigma summation Underscript n equals negative normal infinity Overscript plus normal infinity Endscripts f left parenthesis StartFraction pi Over a EndFraction n right parenthesis StartStartFraction sine a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis OverOver a left parenthesis t minus StartFraction pi Over a EndFraction n right parenthesis EndEndFraction f (t) =
+∞
∑
n=−∞
f
(π
a n
)sin a(t −π
a n)
a(t −π
a n)
(3.23) 
that concludes the proof.
□
The third theorem of Shannon concerns the information capacity of signals. Shan-
non proves that this capacity is directly proportional to the bandupper WW of the signal and 
is inversely proportional to the noise, according to a speciﬁc form. 
3.12 
Third Shannon Theorem 
This theorem gives a limit to the capacity of signals. 
Theorem 3.3 (Third Shannon Theorem) Given signalupper XX of limited bandupper WW, affected 
by a noise described by a random variableupper ZZ with gaussian distribution (providing a 
global signal upper Y equals upper X plus upper ZY = X + Z). The limit of the capacity upper CC that this signal can support, 
as an information channel, is bounded by: 
upper C equals upper W log base 10 left parenthesis StartFraction upper P plus upper N Over upper N EndFraction right parenthesisC = W lg
( P + N
N
)
where upper P equals v a r left parenthesis upper X right parenthesisP = var(X) and upper N equals v a r left parenthesis upper Z right parenthesisN = var(Z) are the powers of the signal and of the noise.

References
65
References 
1. Shannon, C.E.: A mathematical theory of communication. Bell. Syst. Tech. J. 27, 623–656 
(1948) 
2. Rosenhouse, J.: Monty Hall Problem. Oxford University Press (2009) 
3. Cover, T., Thomas, C.: Information Theory. Wiley, New York (1991) 
4. Feller, W.: An Introduction to Probability Theory and Its Applications. Wiley, New York (1968) 
5. Carnot, S.: Reﬂections on the Motive Power of Heat (English translation from French edition 
of 1824, with introduction by Lord Kelvin). Wiley, New York (1890) 
6. Schervish, M.J.: Theory of Statistics. Springer, New York (1995) 
7. Brillouin, L.: Scienze and Information Theory. Academic, New York (1956) 
8. Manca, V.: Infobiotics: Information in Biotic Systems. Springer, Berlin (2013) 
9. Sharp, K., Matschinsky, F.: Translation of Ludwig Boltzmann’s paper “On the Relationship 
between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability 
Calculations Regarding the Conditions for Thermal Equilibrium.” Entropy 17, 1971–2009 
(2015) 
10. Brush, S.G., Hall, N.S. (eds.): The Kinetic Theory of Gases. An Anthology of Classical Papers 
with Historical Commentary. Imperial College Press, London (2003) 
11. Schrödinger, E.: What Is Life? the Physical Aspect of the Living Cell and Mind. Cambridge 
University Press (1944) 
12. Manca, V.: An informational proof of H-Theorem. Open Access Lib. (Modern Physics) 4, 
e3396 (2017) 
13. Wiener, N.: Cybernetics or Control and Communication in the Animal and the Machine. Her-
mann, Paris (1948) 
14. Bennett, C.H.: Time/space trade-offs for reversible computation. SIAM J. Comput. 18, 766–776 
(1989) 
15. Nielsen M.A., Chuang I.L.: Quantum Computation and Quantum Information. Cambridge 
University Press (2000) 
16. Landauer, R.: Irreversibility and heat generation in the computing process. IBM J. Res. Dev. 
5, 183–191 (1961) 
17. Bérut, A.: Experimental veriﬁcation of Landauer’s principle linking information and thermo-
dynamics. Nature 483, 187–189 (2012) 
18. Bennett, C.H.: Logical reversibility of computation. IBM J. Res. Dev. 17, 525–532 (1973) 
19. Bennett, C.H.: Notes on the history of reversible computation. IBM J. Res. Dev. 32(1) (1988) 
20. Bennett, C.H.: Notes on Landauer?s principle, reversible computation, and Maxwell’s Demon. 
Stud. Hist. Philos. Mod. Phys. 34, 501–510 (2003) 
21. Manca, V.: A note on the entropy of computation. In: Graciani, C., Riscos-Núñez, A., Pun, G., 
Rozenberg, G., Salomaa, A. (eds.) Enjoying Natural Computing. Lecture Notes in Computer 
Science, vol. 11270. Springer, Cham (2018)

Chapter 4 
Informational Genomics 
Introduction 
In this chapter, we consider many aspects of the mathematical and computational 
analysis of genomes. The presentation is mainly based on books and research papers 
developed by the authors and collaborators in the last ten years. In particular, we 
mainly refer to [ 1– 12]. 
The genomic alphabet of characters for nucleotides or monomers is upper Gamma equals StartSet a comma c comma g comma t EndSetΓ =
{a, c, g, t}, where upper Gamma Superscript starΓ * is the set of all possible strings over upper GammaΓ . A genome double struck upper GG is a 
“long” string overupper GammaΓ of length usually greater than10 Superscript 5105 (symbols are written in a lin-
ear order, from left to right, according to the chemical orientation 5 prime minus 3 prime5' −3' of Carbon 
bonds in the pentose of DNA molecules). 
The double string structure of genomes has three main reasons: 
(1) it guarantees the efﬁciency of the template-driven duplication in linear time with 
respect to the length of replicated string; 
(2) the transcription (in a single RNA strand) of a genome segment is controlled 
and requires that a portion of the double strand could be opened, by avoiding 
free access to the information contained in the genome, which is accessible only 
in speciﬁc situations. At the same time, the double strand provides a structure 
where better conservation of information is realized; 
(3) ﬁnally, the double linear structure is the basis of helix DNA conformation, which 
is essential for its capability to be packed in a very small volume [ 1, 9]. 
Virus genomes have lengths from some thousands of monomers to several tens of 
thousands. Bacterial genomes are long from ﬁve hundred thousand (Mycoplasma) 
to some millions (Escherichia). The genome lengths of multicellular organisms are 
from hundreds of millions to billions. The human genome is around 3 billion long. 
Genomes are often organized in parts, called chromosomes. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_4 
67

68
4
Informational Genomics
For the analyses of genome organization, we can forget double strings, realized 
by paired DNA strands, because, according to the complementarity principle one 
strand is entirely implied by the other one. 
4.1 DNA Structure 
As already noticed, the geometry of the DNA helix allows that very long DNA 
double sequences could be packed in small volumes (in eukaryotes they are within 
the nucleus, typically a spherical space of radius around one micron (10 Superscript negative 610−6 m). The 
global length of DNA molecules inside one human cell is around two meters. If we 
consider that an average estimation of the number of cells in a human body is around 
10 Superscript 141014, we deduce that the total length of DNA inside a human body is 600 times the 
distance from the Earth to the Sun. This means that the problem of DNA compression 
is crucial for the existence of life. 
The essence of this structure can be explained in terms of monomeric triangles. 
In fact, any nucleotide is a monomer A of a polymer, concatenated with the next 
monomer B of the concatenation line to which it belongs and paired with the cor-
responding complementary monomer C of the paired strand. Abstractly this pro-
vides a triangle ABC. In order to reach a coherent and stable structure the angles of 
monomeric triangles need to remain almost stable along the whole structure (phosfo-
diester bonds along the concatenation line and hydrogen bonds along the pairing line). 
This means that the wrapping of DNA double strands must guarantee the uniformity 
of monomeric triangles. For this reason, any spiral where concatenated triangles are 
arranged along the same plane is forbidden, because in that case, the concatenation 
angles should vary according to the growth of the spiral radius. In DNA helix, con-
catenation is arranged within an ideal cylinder (Fig. 4.1) of a radius of 1 nanometer 
and a height of 3.4 nm, corresponding in the most typical DNA form, called B-form, 
with a 2 pi2π rotation along the axis realized by 10, 5 paired nucleotides (Fig. 4.5) and 
a concatenation rotation angle of about pi divided by 5π/5, around the cylinder axis (Fig. 4.3). 
Fig. 4.1 Spiral arrangements in an abstract setting

4.1 DNA Structure
69
Fig. 4.2 A single monomeric triangle and two paired triangles 
Fig. 4.3 A double line of paired monomeric triangles 
Fig. 4.4 Paired monomeric 
triangles 
Double helix arrangement of monomeric triangles guarantees constant concate-
nation angles and provides a very stable structure (pairing bonds are internally closed 
and protected (they are very weak and hydrophobic), being the basis of other wrap-
ping levels that provide an incredible compression of the bilinear structure (Figs. 4.2, 
4.3, 4.4 and 4.5). 
The helix height has a reduction, with respect to the linear arrangement, which 
can be evaluated by placing the cylinder’s lateral surface on a plane. In this way, the 
line of concatenated nucleotides results to be placed along the diagonal of a triangle 
having an edge of2 pi2π nanometers and the other edge of 3, 4 nm (Fig. 4.6). The length 
compression ratiorhoρ of helix arrangement with respect to the concatenation line of a 
complete rotation around the cylinder axis is thus: 
rho equals 3.4 divided by StartRoot 3.4 squared plus 6.28 squared EndRoot equals 0 comma 429 ellipsisρ = 3.4/
√
3.42 + 6.282 = 0, 429 . . .
Double helix structure gives a reduction of about the 40% of the original length, 
but, most important, the cohesion of the helix arrangement permits the other four 
folds of the DNA sequence [ 13] (nucleosomes, loops, ﬁbres, chromatids). 
Monomers are chiral objects, that is, 3D structures where no plane can divide 
them into two symmetric parts (each one a mirror image of the other one). The 
notion of chirality was introduced by Lord Kelvin (William Thomson the physicist 
who gave importantly contributes to thermodynamics) is essential for DNA structure 
and expresses a property of hands (chiros in Greek means hand). An example of an

70
4
Informational Genomics
Fig. 4.5 The double helix of monomeric triangles 
Fig. 4.6 The helix cylinder 
placed on a plane 
object that is not chiral (achiral) is a bi-glove, which a right hand, or indifferently, 
a left hand can wear, because is identical to itself after a rotation around the axis 
passing through the middle ﬁnger. 
A more general deﬁnition of chirality is based on group theory and permits to 
extend chirality in metric spaces which are not Euclidean. However, the usual deﬁni-
tion in 3D Euclidean space is adequate for our present discussion. Two objects formed 
by parts that are in a reciprocal correspondence (head, body, tail, …) are homochiral 
when the cartesian coordinate systems assigned to them by means of the spatial rela-
tions among their parts result to have the same handedness: left-hand or right-hand. 
This means that, if x and y are two axes of the related cartesian coordinate system, an 
x-to-y rotation (through the convex xy angle) results anti-clockwise, with respect to 
the z-axis, in a left-handed system, while results clockwise in a right-handed system. 
As we will explain monomers are homochiral (with the same chirality), but also 
paired helices, forming the DNA double helix, must be homochiral, otherwise, they 
would cross repeatedly on the surface of the cylinder. The most stable form of DNA 
is the B form, which is a right-handed double helix. 
DNA helix follows precise logical and geometric principles making this structure 
a sort of necessary consequence of them. The basic principles are given by the 
following items. 
A wider system of principles will be given at the end of this section [ 1].

4.1 DNA Structure
71
Fig. 4.7 The two possible arrangements of cartesian systems of nucleotides in a double strand. 
In the arrangement (a) nucleotides are concatenated in the same direction. In the arrangement 
(b) nucleotides are arranged in an anti-parallel way. In case of a paired monomers have different 
chiralities, while in case b all monomers have the same chirality 
Fig. 4.8 Two vistas of a 
double helix of monomeric 
triangles produced with 3D 
printer [ 1]. Angles conform 
with those of B-DNA 
• Double linearity: a double linear structure provides a template driven sequential 
copy process in a time that is linear with respect to the length of the duplicated 
sequence. 
• Complementarity: In order to obtain two single linear templates from a double 
strand, the chemical pairing bonds are weak (hydrogen bonds). This possibility 
is better obtained by different (complementary) molecules because bonds between 
molecules of the same type are covalent bonds that are strong. 
• Anti-parallelism: The chirality of a monomer is given by: the x-axis, or concatena-
tion axis, going from the tail to the head; the y-axis, or pairing axis, going from the 
middle point of the tail-head line to the corresponding point of the paired monomer; 
z-axis orthogonal to the other axes, with one of two possible orientations, depend-
ing on the handedness of chirality. Monomers are homohiral, hence, in both 
strands, they have the same chirality, otherwise, for each nucleotide, we would 
have the left-hand and the right-hand version, with a complication of the double 
strand structure and of the related process of duplication. Pairing is specular, so  
that monomers are oppositely oriented with respect to the pairing axis. In such 
a way, each monomer can pair with only one monomer. Concatenation goes in 
opposite directions in the paired strands, otherwise, it is easy to realize (see Fig. 
4.7) that any monomer in one strand would have a chirality different from that of 
its paired monomer, against the assumption of homochirality (Fig. 4.8). 
The following propositions emphasize the role of the chirality of monomers.

72
4
Informational Genomics
Proposition 4.1 Chirality of the 3D-structure of monomers is necessary for the 
correct formation of bilinear DNA strands. 
Proof If monomers were achiral, any of them could be brought to coincide with 
its mirror image by roto-translations in 3D space, with respect to any plane. Since 
template-driven duplication is a process where pairing bonds are formed before 
concatenation bonds, the free motion of monomers getting close to the template 
could lead to pairing a monomer with different head-tail directions without affecting 
the orientedness of the pairing direction. However, neither head-to-head nor tail-to-
tail concatenation bonds are allowed to form the DNA double strands.
∎
Proposition 4.2 Paired monomers are homochiral if and only if their concatenation 
directions are antiparallel in the paired strands. 
Proof The requirement of specular pairing (see Fig. 4.7) entails that, if paired 
monomers would concatenate in the same direction, then they would be mirror 
images of each other, whence they would have different chirality. Conversely, spec-
ular pairing and anti-parallelism together imply homochirality, because it is easy to 
realize, that a roto-translation of the paired monomer makes chiralities of two paired 
monomers coincident.
∎
Let us give ten structural principles of DNA structure, putting in evidence the 
logic motivating them. 
1. Directed linearity (of concatenation) 
2. Specularity of pairing (no more than two monomers may pair) 
3. Complementarity of pairing (weak pairing) 
4. Monomeric chirality (unique pairing direction) 
5. Homogeneity of chirality (Monomer can belong to both paired strands) 
6. Antiparallelism of concatenation (implied by conditions 2 and 4) 
7. Helix folding (uniformity concatenation angles) 
8. Asymmetry of grooves (phase angle less than pi< π) 
9. Bistability of helix (winding-unwinding) 
10. Silencing of portions (hiding substrands) 
Let us summarize all together the motivations of these principles (by repeating 
some already given explanations): 
Directed linearity is necessary to realize a structure that is a text, composed of 
letters, with a reading direction. 
Specularity of pairing ensures a pairing that “closes” the text making information 
inaccessible, and more protected. 
Complementarity of pairing is related to three main aspects: (a) it is the basis 
of the template-driven copy of DNA strands: (b) it permits the closure of DNA texts 
in a reversible way because complementary bonds are weak (with respect to the 
concatenation bonds), and closed text can be opened (enzymes, called helicases, are 
devoted to this operation) and closed again; (c) pairing bonds are placed in the middle

4.1 DNA Structure
73
between the two paired strands, whence inaccessibility and protection are realized 
at the same time. 
Monomeric chirality ensures that pairing happens correctly with respect to the 
concatenation direction. 
Homogeneity of chirality ensures that all monomers have the same chirality, 
and can be placed indifferently in any strand. If two chiralities for each type of 
monomer were possible, according to the specularity, paired monomers would have 
different chirality and the same concatenation direction in both paired strands, by 
having a greater complexity in the overall elaboration of DNA texts. Therefore, 
Antiparallelism of concatenation is a consequence of homogeneity and chirality. 
Helix folding guarantees equal concatenation angles in different positions of a 
strand that needs to be folded for reducing its spatial volume. 
Asymmetry of grooves refers to the agents elaborating DNA helix. In fact, the 
major grove is the part where agents enter for performing their actions, as is shown 
in the ﬁgures, the asymmetry of groves establishes a preferential way of accessing 
to the helix. 
Bistability of helix is a crucial aspect for the correct use of information. In fact, 
information needs two states: (a) the inactive state, when it is conserved, and (b) 
the active state when it is operative and expresses statements executed by speciﬁc 
agents in some cellular contexts. The mechanism of closing and opening portions of 
paired strands is further controlled by the helix conformation. In fact, Helix formation 
consists of a winding of the double strand according to a rotational direction, deter-
mining a screw tightening, usually clockwise oriented, with respect to the cylinder 
axis of the helix. When a portion of the helix is unwinded, then strands are sepa-
rated for access to their information. Therefore, the helix form becomes a further 
mechanism for controlling the bistability between the states (a) and (b). This aspect 
is essential because information becomes useful in directing dynamics only if it is 
used at the right moment, otherwise, it could be useless or even dangerous to the 
system. In this concern, DNA appears as a mere container of biological information. 
The activation of information is realized by means of DNA transcription, where a 
portion of one DNA strand is transformed, by means of suitable enzymes, into an 
RNA strand (U monomer replaces T). Then, biological actions are performed by 
RNA, which is a single-strand polymer with a short life, sufﬁcient to realize spe-
ciﬁc actions inside the cell. RNA, probably, was the polymer at the origin of life, 
and by a process of translation, an RNA strand is transformed, according to the 
genetic code (deﬁned on triplets of RNA letters A, U, C, G) into a sequence over 
an alphabet of 20 letters (amino-acid molecules). These sequences, when formed, 
assume speciﬁc spatial conﬁgurations, giving them the possibility of realizing spe-
ciﬁc biological functions. These biomolecules are called proteins, and the process of 
their realization is called protein synthesis. Thus, proteins are  conformational and 
functional biomolecules, and constitute fundamental biological agents. However, 
RNA single strands, being single-stranded, are subjected to the attraction between 
different portions of the same strand, based on complementarity. For this reason, they 
have the ability to fold, assuming conﬁgurations and consequent capacities of real-
izing speciﬁc functions. In this sense, RNA strands are informational and functional

74
4
Informational Genomics
molecules, at the same time. This is a natural condition for RNA strands being most 
probably the ﬁrst biopolymers, at the beginning of life, and also a natural condition 
for being the intermediaries between DNA and proteins. 
Silencing of portions is a radical way to close some portions of DNA texts. 
It is the genomic basis of multicellularity. This phenomenon corresponds to the 
idea of a super-genome, that is, a genome that originates many genomes (one for 
any kind of cell, in an integrated system of different cells). This permits to the 
maintenance of an original genome, by diversifying it in several forms. A toy example 
can clarify the main idea. Let upper A upper B upper C upper D upper E upper F upper GABC DE FG a partition of a string in 7 parts, then 
let indicate inside brackets silenced parts: upper A left bracket upper B right bracket upper C upper D left bracket upper E right bracket upper F left bracket upper G right bracketA[B]C D[E]F[G], left bracket upper A right bracket upper B upper C left bracket upper D right bracket upper E left bracket upper F right bracket upper G[A]BC[D]E[F]G, 
upper A upper B left bracket upper C upper D right bracket upper E left bracket upper F right bracket upper GAB[C D]E[F]G, that are equivalent respectively toupper A upper C upper D upper FAC DF,upper B upper C upper E upper GBC EG,upper A upper B upper E upper GABEG. In this  
way germinal cells, those responsible for the reproduction of the whole organism, 
have the global genome without silenced parts but, in the course of the formation 
and development of a new organism, cells of different kinds (depending on their 
relative localization in an initial population of equal amorphous cells) receive the 
genomes with silenced parts, each of them developing functions that are proper of 
their speciﬁc type (around 250 main types in the human body). 
4.2 Genome Texts 
From the point of view of genome functional organization, we can distinguish seven 
kinds of regions (see, for example, [ 14] for speciﬁc analyses): 
• Transcriptional-Translational regions (genes); 
• Pure Transcriptional regions related to different types of RNA modulation activi-
ties; 
• Regions of promotion/activation where factors and co-factors (proteins) adhere; 
• Regulatory regions related to protein coordination functions (homeobox); 
• Regions with topological functions related to wrapping/unwrapping activities; 
• Regions hosting DNA recombination hotspot; 
• Regions related to evolutionary remains, without no direct current functional mean-
ing, repositories in a standby situation. 
The classiﬁcation above does not pretend any completeness, it wants only to 
point to a very general functional organization based on the main aspects of genome 
functions. Moreover, only large regions are considered. Of course, small pieces, from 
some hundreds to some thousands of nucleotides (introns, exons, transcription starts, 
and so on) surely provide a great number of categories. 
Let us consider a very simple numerical argument showing that a complex and 
articulated mechanism of categorization underlies the assembly of small strings. 
All the possible strings of length 100 over four letters are 4 Superscript 1004100, that is, more than 
10 Superscript 601060. This number is thousands of times greater than the number of atoms on Earth. 
Therefore, the DNA strands of length 100 that are present in real genomes provide an

4.2 Genome Texts
75
inﬁnitesimal fraction of all the possible strands of that length. This means that these 
strands are selected according to very speciﬁc constraints. The discovery of some of 
the main principles underlying this selection would be a great step in understanding 
genome structures. It is very reasonable to postulate that these principles exist and 
that mathematical and computational analyses are possible ways for ﬁnding them. 
The next section about genomic dictionaries is based on this assumption, and in that 
context, some formal categories of genomic words will be considered. 
Genomes direct the basic activity of cell metabolism and all the major cell func-
tions are mediated by genes that encode proteins. Proteins are effectors of functions 
encoded by genes. However, this simple schema works with a complex network 
of functions, with different levels, that involve, genome segments, generalizing the 
notion of genes, RNA strands of different types, and proteins. The interactions among 
them determine a circularity, because proteins that are produced by genes are at the 
same time, in many phases, crucial for directing genome activities (for example, 
by activating, promoting, regulating, or inhibiting transcriptions). This circularity is 
a sort of chicken-egg paradox, which can be solved only at the evolutionary level, 
postulating protein-like molecules (short amino-acid chains) before the existence of 
the protein synthesis process, and proto-genomes (most probably based on RNA) 
before DNA standard genomes [ 15]. 
Genomes play a crucial role in transmitting information along life generations. 
In this sense, they have a double nature: (i) they are the operating systems of cells, 
(ii) they summarize the evolutionary passages of the biological information pro-
ducing the species, that is, the history of the lineages from the origin of life up 
to the formation of organisms hosting them. The analogy with computer operating 
systems is very attractive, but of course, can be misleading for several reasons. In 
fact, genomes include pieces that activate and regulate processes, but the way these 
processes are integrated is not comparable, in many respects, with those acting on 
electronic computers. First, genomes are self-assembly products, and the processes 
that they promote are autonomous. For this reason, cooperation is dictated by no cen-
tral unit but emerges as a result of a network of interactions where the only common 
attitude is the reproduction of the system hosting the whole dynamics. 
We could say that life is a process that wants to keep living. This implies two 
auto-referential strategies: maintaining life processes in time and, at the same time, 
propagating them in space. Reproduction is the solution to both these requirements 
because given the spatial and temporal ﬁniteness of any organism, required by its 
physical realization, reproduction is a way for overcoming individual limitations in 
space and in time. Therefore, genomes are intrinsically replicable and probably, in 
the history of life, organism reproduction is a consequence of searching for cor-
rect genome replication. In any case, genomes are individual structures that can be 
fully analyzed in the context of their evolutionary nature, where individuals are only 
instances of genealogical lines from which species arose. 
Genomes have a double nature, directing the life of individuals, and propagating 
along evolutionary lines through species. For this reason, they memorize the species’ 
genealogy but determine also the changes aimed at exploring new life possibilities. 
In fact, just at the moment of their transmission, they include variability mechanisms.

76
4
Informational Genomics
These are intrinsic to their nature, in the sense that even if they search for the best 
ﬁtting of individuals with the environment, they in any case change even without 
any external pressure, but just for a free intrinsic mechanism of casual exploration. 
They are subjected to computational rules for their functioning inside cells, but they 
are subjected to random events on which natural selection acts for ﬁxing the most 
advantageous for better adaptability to the environment where organisms are placed. 
For this reason, a mathematical and computational analysis of genomes has to 
cope with their computational ability (see [ 16, 17] for analyses in a DNA computing 
perspective), and at the same time with random processes. A clear and rigorous 
account of the way genomes balance and dominate both aspects is surely one of 
the most difﬁcult challenge to a deep understanding of the nature and dynamics of 
genomes. 
Three were the factors of Darwin’s theory of evolution: heredity, variability, and 
selection. The discovery of DNA and genomes not only do not contradict Darwin’s 
perspective but give a new comprehension of evolutionary dynamics at a molecular 
level. 
Heredity is the mechanism of passing biological information by duplicating the 
genome of the parent cell and leaving that the copy becomes the genome of the 
generated cell. However, in the passage, some variability is included due to casual 
mutations, and copy errors, but even speciﬁc mechanisms of variability in sexual 
reproduction reach the level of a very speciﬁc recombination machine: from the two 
genomes of parents a new recombined genome is obtained with parts coming from 
the two parental cells. The key point of Darwin’s formulation is that information 
does not pass from individuals to the species, because only their genomes matter, 
nothing else about the individual history of the parents. 
As already noticed, genomes vary for a free tendency of exploring new life possi-
bilities. This capability of genomes is the main mechanism for realizing the ﬁtness of 
the organisms to the environment where they live. Genome changes, in their extreme 
details, are representable by mechanisms of string rewriting. Each genotype is asso-
ciated with a phenotype. Natural selection favourites phenotypes that increase ﬁtness 
to the environment. This selection improves the average quality of life of individuals, 
because the better they live, the greater their probability of reproduction, and their 
descendants maintain alive the genomes that they received from their parents. In this 
way, the genomes of organisms more adapted become dominant and the correspond-
ing genomes have a greater probability of passing to the species of their evolutionary 
path. In this way, information passes with no direct intervention in the individual life. 
Therefore, individual genomes are containers of biological information. a substance 
ﬂowing through them, and mixing their contents, by recombination mechanisms. 
The only intervention of the individual in this process is dying or surviving. Death 
is a necessity of evolution. 
A classical example of evolution is that of a giraffe. This animal obtained his 
long neck only by chance, and surely not for the attempts to eat leaves of high trees. 
On the contrary, the advantage of a casual long neck became an advantage in an 
environment with high trees of good leaves on the top. In general, species evolve, 
toward increasing ﬁtness, because some individuals change, casually, in a direction

4.3 Genome Dictionaries
77
that is advantageous with respect to their environment, Therefore, organisms evolve 
because they change, while they do not change for evolving. 
In this evolutionary scenario, randomness is the king of the process, and para-
doxically it is a necessity. Deep mathematical properties of randomness ensure great 
efﬁciency in exploring spaces of possibilities. In particular, a property, called ergod-
icity, can be intuitively visualized by a dynamics visiting, homogeneously in time, 
all the regions of a space of possibilities. In this way, if optimal solutions to a given 
problem exist, then they are surely found. 
However, the mechanism of ﬁxing some advantageous steps, and integrating the 
single steps toward a coherent structure which, in the end, appears as a sort of per-
formed project remains a mystery, in terms of our actual knowledge about genomes. 
About species formation, how can we measure the steps necessary for passing to 
a new species from a previous one? In fact, changes cumulate along generations, but 
at some point, speciations become a jump, where the genomic differences result in 
a radical change of the play, and the resulting genome “speaks” a different language 
with respect to those of genomes of its generative lineage. This process is very 
similar to the passage from one linguistic system to another one. Also, in this case, 
a new language is generated that results incompatible, for communication, with the 
language from which it stems. When and why this incompatibility arises? In the 
sexual species, incompatibility means the impossibility of procreation from sexual 
mating among individuals of different species. But again, when and why does an 
individual represent the birth of a different species with respect to that his/her parents? 
The actual impossibility of rigorous answers to the previous question is almost 
centred on the absence of quantitative laws describing the evolutionary dynamics. 
These laws are the core of any deep future comprehension of the basic mechanisms 
of life. 
4.3 Genome Dictionaries 
Given a genome double struck upper GG its complete dictionary upper D left parenthesis double struck upper G right parenthesisD(G) is given by s u b left parenthesis double struck upper G right parenthesissub(G), that is, by all 
the strings occurring in double struck upper GG between two positions: 
upper D left parenthesis double struck upper G right parenthesis equals left brace double struck upper G left bracket i comma j right bracket StartAbsoluteValue 1 less than or equals i less than or equals EndAbsoluteValue double struck upper G vertical bar right braceD(G) = {G[i, j] | 1 ≤i ≤|G|}
and moreover: 
upper D Subscript k Baseline left parenthesis upper G right parenthesis equals upper D left parenthesis double struck upper G right parenthesis intersection upper Gamma Superscript k Baseline periodDk(G) = D(G) ∩Γ k.
where normal upper Gamma Superscript kΓ k is the set of all possible words of length kk over the alphabet normal upper GammaΓ . 
Any subset ofupper D left parenthesis double struck upper G right parenthesisD(G) is called a dictionary ofdouble struck upper GG. Elements ofupper D left parenthesis double struck upper G right parenthesisD(G) are also called 
words, factors, bold kk-mers, bold kk-grams of double struck upper GG. A preﬁx kk. generic or of a given value, is 
used when we want explicitly mention the length of strings. We will writealpha element of double struck upper Gα ∈G as 
an abbreviation of alpha element of upper D Subscript double struck upper Gα ∈DG.

78
4
Informational Genomics
The set of positions where (the ﬁrst symbol of) a string alphaα occurs in double struck upper GG is denoted 
byp o s Subscript double struck upper G Baseline left parenthesis alpha right parenthesisposG(α), and the multiplicity ofalphaα indouble struck upper GG, that is, the number of times it occurs in 
double struck upper GG, is indicated by m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesismultG(α). 
Let us deﬁne some classes of strings that are very useful in the analysis of genomes: 
1. Repeats are words with multiplicity greater than 1. 
2. Hapaxes are elements with multiplicity exactly equal to 1. 
3. Duplexes are elements with multiplicity exactly equal to 2. 
4. Nullomers are the shortest strings that do not occur in the genome. 
The term hapax comes from philology and means a word occurring in upper GG once 
(from a Greek root for once). A repeat of double struck upper GG is a word occurring in double struck upper GG at least twice. 
A maximal repeat is a repeat that is not a substring of another repeat. A minimal 
hapax is an hapax where removing its ﬁrst symbol, or its last symbol, produces a 
repeat. Equivalently, elongating a maximal repeat alphaα with a letter x element of upper Gammax ∈Γ , in strings 
alpha xαx or x alphaxα, provides minimal hapaxes. 
The maximality of repeats can be considered in two different senses. Let alpha comma betaα, β
be repeats of double struck upper GG while alphaα is a preﬁx of betaβ and betaβ is a maximal repeat, then alphaα is not a 
maximal repeat. But, ifalphaα occurs indouble struck upper GG in some positions wherebetaβ does not occur, and 
in these positions,alphaα is not a substring of any repeat ofdouble struck upper GG, thenalphaα is a maximal repeat, 
with respect to these positions. Then, it is useful to consider the notion of positional 
maximal repeat as a repeat that is maximal with respect to some non-empty set upper PP
of positions, called the set of maximal occurrences of alphaα in double struck upper GG. 
If we consider texts in natural languages, words such as mean, meaning, meaning-
ful, meaningfulness result to be, in some English texts, cases of positional maximal 
repeats. 
The notion of positional maximal repeat is very close to the usual notion of word 
in natural languages. Conversely repeats that are never positional maximal repeats, 
can occur only internally to positional maximal repeats corresponding to roots (in a 
similar sense as mean, ing, ful, ness are English roots). The notion of repeat provides a 
hierarchy of units: letters, roots, words, syntagmas, statements, composite statements. 
Of course, an hapax of double struck upper GG, does never correspond to a word, because it occurs 
once indouble struck upper GG. But, if we consider a class of genomes, or in general a class of texts, then 
the hapaxes of these texts can be repeats in the union of all the texts of the class. 
If the length ofdouble struck upper GG isnn, that is,StartAbsoluteValue double struck upper G EndAbsoluteValue equals n|G| = n, then fork greater than log Subscript k Baseline nk > logk n surely existkk-mers that 
are nullomers of double struck upper GG, and increasing kk, the most part of kk-mers surely are nullomers, 
because in this case 4 Superscript k Baseline greater than n4k > n, therefore surely there are kk-mers that cannot occur in 
double struck upper GG. This means that forkk around the logarithmic length ofdouble struck upper GG is realized the selection 
of kk-mers occurring in double struck upper GG, and in this selection is expressed the biological meaning 
of kk-mers and of the more complex syntactical units where they occur. 
All the set-theoretic operations and relations of formal language theory can be 
applied to dictionaries. In particular, given a dictionary upper LL, then StartAbsoluteValue upper L EndAbsoluteValue|L| is the number of 
its elements (with notation overloading because StartAbsoluteValue alpha EndAbsoluteValue|α| also denotes the length of the 
string alphaα). 
Interesting dictionaries ofdouble struck upper GG are those that coversdouble struck upper GG but consisting of a relatively 
small numbers of words with respect to the length of double struck upper GG.

4.4 Genome Informational Indexes
79
We say absolute dictionary ofdouble struck upper GG, denoted bya b s left parenthesis double struck upper G right parenthesisabs(G), the dictionary constituted 
by the positional maximal repeats and minimal hapaxes ofdouble struck upper GG. This notion of a genome 
dictionary is not related to a ﬁxed length of words, and in many aspects seems to 
reﬂect intrinsic characters of a genome. 
Proposition 4.3 For any kk, StartAbsoluteValue upper D Subscript k Baseline left parenthesis double struck upper G right parenthesis EndAbsoluteValue less than or equals StartAbsoluteValue double struck upper G EndAbsoluteValue minus k plus 1|Dk(G)| ≤|G| −k + 1 and there exist genomes for 
which equality holds. 
Proof In fact if we move forward a window of length kk from the ﬁrst position 
of double struck upper GG, then the rightmost position where the window can be placed is the position 
StartAbsoluteValue double struck upper G EndAbsoluteValue minus k plus 1|G| −k + 1. The existence of genomes having the maximum number of possible 
different kk-mers follows from the existence of de Bruijn genomes, which will be 
presented in the next section.
∎
Proposition 4.4 StartAbsoluteValue upper D left parenthesis double struck upper G right parenthesis EndAbsoluteValue less than StartAbsoluteValue double struck upper G EndAbsoluteValue left parenthesis StartAbsoluteValue double struck upper G EndAbsoluteValue plus 1 right parenthesis divided by 2|D(G)| < |G|(|G| + 1)/2. 
Proof There is one word of lengthStartAbsoluteValue double struck upper G EndAbsoluteValue|G|, at most two different words of lengthStartAbsoluteValue double struck upper G EndAbsoluteValue minus 1|G| −1, 
at most three different words of length StartAbsoluteValue double struck upper G EndAbsoluteValue minus 2|G| −2, and so on. But words of length 1 equals StartAbsoluteValue double struck upper G EndAbsoluteValue minus left parenthesis StartAbsoluteValue double struck upper G EndAbsoluteValue minus 1 right parenthesis1 =
|G| −(|G| −1) are less than StartAbsoluteValue double struck upper G EndAbsoluteValue|G| (unless StartAbsoluteValue double struck upper G EndAbsoluteValue equals 4|G| = 4). Therefore according to Gauss’ 
summation (sigma summation Underscript i equals 1 Overscript n Endscripts equals StartFraction n left parenthesis n plus 1 right parenthesis Over 2 EndFraction∑n
i=1 = n(n+1)
2
), we get the given upper bound.
∎
4.4 Genome Informational Indexes 
Informational indexes of a genome are parameters related to the global proper-
ties of genomes and are deﬁned by means of particular genomic distributions and 
dictionaries. 
The kk-mer distribution of double struck upper GG is given by the frequency in double struck upper GG of each kk-mer of 
dictionary upper D Subscript k Baseline left parenthesis double struck upper G right parenthesisDk(G). 
The entropy deﬁned by the genomic kk-mer distribution is called the kk-entropy of 
double struck upper GG and is denoted by upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G). 
The following indexes are of particular importance. 
1. upper L upper G left parenthesis double struck upper G right parenthesis equals log base 10 Subscript 4 Baseline left parenthesis StartAbsoluteValue double struck upper G EndAbsoluteValue right parenthesisLG(G) = lg4(|G|), the  logarithmic length of double struck upper GG. 
2. m r l left parenthesis double struck upper G right parenthesismrl(G) the maximum repeat length, that is, the maximum length of maximal 
repeats. 
3. m c l left parenthesis double struck upper G right parenthesismcl(G) the maximum complete length such that all possible strings of that 
length occur in double struck upper GG. 
4. m f l left parenthesis double struck upper G right parenthesism f l(G) the minimal forbidden length such that surely strings of that length 
exist that do not occur in double struck upper GG. 
5. m h l left parenthesis double struck upper G right parenthesismhl(G) the minimum hapax length, that is, the minimum length of minimal 
hapaxes. 
6. m g l left parenthesis double struck upper G right parenthesismgl(G) the minimal hapax greatest length, that is, the value of the maximum 
among the lengths of minimal hapaxes. 
7. nabla left parenthesis double struck upper G right parenthesis∇(G) the no-repeat length, that is, m r l left parenthesis double struck upper G right parenthesis plus 1mrl(G) + 1. 
8. Delta left parenthesis double struck upper G right parenthesisΔ(G) the no-hapax length, that is, m h l left parenthesis double struck upper G right parenthesis minus 1mhl(G) −1.

80
4
Informational Genomics
9. upper L upper X Subscript k Baseline left parenthesis double struck upper G right parenthesisL Xk(G), the  average multiplicity of kk-mers, called kk-lexical index 
10. upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G), the kk-entropy 
11. upper L upper E left parenthesis double struck upper G right parenthesisLE(G), the  logarithmic entropy, that is, kk-entropy for k equals log Subscript 4 Baseline nk = log4 n, where nn is 
the length of double struck upper GG. 
12. 2 upper L upper E left parenthesis double struck upper G right parenthesis2LE(G), the  binary logarithmic entropy, that is,kk-entropy fork equals log Subscript 2 Baseline nk = log2 n (the 
interest of this value will be clariﬁed later on). 
Other indexes that could be interesting in some contexts are Repeat indexes giving 
the lengths and the numbers of maximal repeats or the lengths and the numbers of 
maximal positional repeats (minima, maxima and average values). 
Any genomic dictionary determines indexes given by the maximum, minimum, 
and average multiplicities of its kk-mers. 
Some simple facts follow from the deﬁnitions given above (when the genome 
is understood indexes will be used without explicit indication of the genome. For 
example upper L upper GLG abbreviates upper L upper G left parenthesis double struck upper G right parenthesisLG(G)). 
Proposition 4.5 In any genome, a string including an hapax string is an hapax too. 
Proposition 4.6 In any genome, a string included in a repeat string is a repeat too. 
The right elongation of a stringalphaα is any string of whichalphaα is a preﬁx, analogously 
a left elongation of alphaα is a string of which alphaα is a sufﬁx. A string that is the right or 
left elongation of alphaα is an elongation of alphaα. 
Proposition 4.7 Nullomers are elongations of minimal hapaxes. 
Proof In fact, if an hapax alphaα occurs in a genome, then all the strings of the genome 
includingalphaα are hapaxes too. Letbeta alpha gammaβαγ be any string includingalphaα in the given genome. 
Then, all the strings where betaβ and gammaγ are replaced, respectively, by strings different 
from them are surely strings that cannot occur in the genome.
∎
Proposition 4.8 In any genome, a string longer thanm r lmrl is an hapax, and any string 
shorter than the m h lmhl is a repeat. 
The interval left bracket Delta left parenthesis double struck upper G right parenthesis comma nabla left parenthesis double struck upper G right bracket right parenthesis[Δ(G), ∇(G]) is called hapax-repeat interval of a given genome. 
All the strings longer than nabla left parenthesis double struck upper G right parenthesis∇(G) are hapaxes, those shorter than Delta left parenthesis double struck upper G right parenthesisΔ(G) are repeats. 
Proposition 4.9 In any genome m c l less than upper L upper Gmcl < LG. 
Proof In fact, by the deﬁnition of m c lmcl it follows that: 
4 Superscript m c l Baseline less than or equals n minus k plus 14mcl ≤n −k + 1
therefore: 
m c l less than or equals log base 10 Subscript 4 Baseline left parenthesis n minus k plus 1 right parenthesis less than log base 10 Subscript 4 Baseline left parenthesis n right parenthesis periodmcl ≤lg4(n −k + 1) < lg4(n).
Proposition 4.10 In any genome m c l equals m f l minus 1mcl = m f l −1. 
Proposition 4.11 In any genome m c l less than or equals m h lmcl ≤mhl.

4.4 Genome Informational Indexes
81
Proof In fact, Ifk equals m c l greater than m h lk = mcl > mhl, then an hapaxalphaα of lengthk minus 1k −1 should exist. This 
means that three among the string alpha aαa, alpha cαc, alpha gαg, alpha tαt cannot occur in the genome. But, 
this means that k not equals m c lk /= mcl, which is a contradiction.
∎
Proposition 4.12 In all genomes m f l less than or equals m h l plus 1m f l ≤mhl + 1. 
Proof Let k equals m f lk = m f l, then a kk-mer alphaα has to be an hapax, this means that the letter 
following it is only of one kind. This implies that, for some letter xx the left parenthesis k plus 1 right parenthesis(k + 1)-mer 
alpha xαx is a nullomer. Therefore, k equals m f l less than or equals m h l plus 1k = m f l ≤mhl + 1. 
The following proposition gives a chain of inequalities among m c l comma m f l comma m h lmcl, m f l, mhl
indexes. 
Proposition 4.13 
m c l equals m f l minus 1 less than or equals m h lmcl = m f l −1 ≤mhl
Proposition 4.14 Maximal repeats have at most multiplicity 5. 
Proof In fact, if alphaα is a maximal repeat no its elongation alpha xαx with x element of upper Gammax ∈Γ can occur 
twice, otherwise alphaα would not be maximal. This means that symbols after the occur-
rences ofalphaα must be different. But, ifalphaα occurs more than four times, in the occurrences 
where a symbol occurs after alphaα, this condition cannot be veriﬁed. Therefore alphaα has to 
occur at most four times in the middle of the genome and only once at the end of the 
whole genome.
∎
A genome double struck upper GG is a kk-hapax genome when its dictionary upper D Subscript k Baseline left parenthesis double struck upper G right parenthesisDk(G) consists only of 
hapax, moreover double struck upper GG is also a complete hapax genome if upper D Subscript k Baseline left parenthesis double struck upper G right parenthesisDk(G) consists of all 4 Superscript k4k kk-
mers. In this case the length ofdouble struck upper GG is4 Superscript k Baseline plus k minus 14k + k −1. The following genome is a22-hapax 
genome: 
a a c c g g t t a g a t c t gaaccggttagatctg
The following is a 22-hapax complete genome. It is complete because all pairs occur 
once, and its length is17 equals 2 Superscript 4 Baseline plus 2 minus 117 = 24 + 2 −1, capable of containing all the possible pairs: 
a a c c g g t t a g a t c t g c aaaccggttagatctgca
The following genome, where the last two symbols of the previous genome were 
permuted, is no more an hapax genome: 
a a c c g g t t a g a t c t g a c periodaaccggttagatctgac.
A general method for generating hapax genomes will be given in a next section where 
de Bruijn graphs are introduced. According to the equipartition property of entropy, 
we have the following proposition. 
Proposition 4.15 Any kk-hapax genome double struck upper GG of length nn has the maximum value of 
upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G) among all genomes of the same length.

82
4
Informational Genomics
In real genomes usually m h l less than or equals m r lmhl ≤mrl, but this inequality does not hold in general. 
In fact, in the following genome: 
a a t t a a c caattaacc
the shortest hapaxes are long 3, but the longest repeats are long 2, therefore m h l greater than m r lmhl >
mrl. 
If a kk-mer is an hapax, it is univocally elongated, but the converse does not hold, 
because a kk-mer can elongate in only one way, without being hapax. 
4.5 Genome Information Sources 
The most important notion of informational genomics is that of genomic distribu-
tion. It is deﬁned by a variable upper X Subscript double struck upper GXG taking values in a set of components/features 
of double struck upper GG (positions, k-mers, substrings, distances, substring occurrences, …). Counting 
the number of times that an eventupper X Subscript double struck upper G Baseline equals xXG = x occurs provides a multiplicity, and conse-
quently, a frequency associated to the value xx, whence a probability distribution for 
upper X Subscript double struck upper GXG. A genomic distribution is given by a pairleft parenthesis upper X comma p right parenthesis(X, p), whereupper XX is a variable, deﬁned 
over a genome or over a class of genomes, andpp is a function assigning a probability 
p left parenthesis x right parenthesisp(x) to each value xx assumed by upper XX. The pair left parenthesis upper X comma p right parenthesis(X, p) is also an information source 
in the sense of Shannon. 
Given a genomic sequence double struck upper GG, several types of functions and distributions can be 
deﬁned on it, depending on the different perspectives from which genomic sequences 
are investigated. 
The position function p o s Subscript double struck upper GposG assigns to each word alpha element of upper D left parenthesis double struck upper G right parenthesisα ∈D(G) the set of positions 
where alphaα occurs in double struck upper GG. Formally: 
p o s Subscript double struck upper G Baseline left parenthesis alpha right parenthesis equals StartSet i colon double struck upper G left bracket i comma i plus StartAbsoluteValue alpha EndAbsoluteValue right bracket equals alpha EndSet periodposG(α) = {i : G[i, i + |α|] = α}.
The word multiplicity function m u l t Subscript double struck upper GmultG maps each word alphaα into its multiplicity 
m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesismultG(α), within the given genome: 
m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis equals StartAbsoluteValue p o s Subscript double struck upper G Baseline left parenthesis alpha right parenthesis EndAbsoluteValue periodmultG(α) = |posG(α)|.
The word length distribution assigns to each positive integer k element of left bracket 1 ellipsis StartAbsoluteValue double struck upper G EndAbsoluteValue right bracketk ∈[1 . . . |G|] the 
number of distinct words of StartAbsoluteValue upper D Subscript k Baseline left parenthesis double struck upper G right parenthesis EndAbsoluteValue|Dk(G)| that occur in the genome double struck upper GG. 
The value l c l left parenthesis double struck upper G right parenthesislcl(G), called the limit cardinality length, is the shortest length for 
which words of double struck upper GG of length l c l left parenthesis double struck upper G right parenthesislcl(G) are, in number, more than words of double struck upper GG shorter 
than l c l left parenthesis double struck upper G right parenthesislcl(G), but at the same time are, in number, not less than words of double struck upper GG longer 
than l c l left parenthesis double struck upper G right parenthesislcl(G). 
The hapax length distribution assigns to each word length kk the number of 
hapaxes of such a length: 
StartAbsoluteValue StartSet alpha element of upper D Subscript k Baseline left parenthesis double struck upper G right parenthesis vertical bar m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis equals 1 EndSet EndAbsoluteValue period|{α ∈Dk(G) | multG(α) = 1}|.

4.5 Genome Information Sources
83
Analogously, the repeat length distribution assigns to each word length kk the 
number of repeats of double struck upper GG having that speciﬁc length: 
StartAbsoluteValue StartSet alpha element of upper D Subscript k Baseline left parenthesis double struck upper G right parenthesis vertical bar m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis greater than 1 EndSet EndAbsoluteValue period|{α ∈Dk(G) | multG(α) > 1}|.
Distributions can be converted as percentages by dividing the number of words of a 
given lengthkk by the total number ofkk-mers occurring in the given genome, that is: 
f r e q Subscript double struck upper G Baseline left parenthesis alpha right parenthesis equals m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis divided by left parenthesis StartAbsoluteValue double struck upper G EndAbsoluteValue minus StartAbsoluteValue alpha EndAbsoluteValue plus 1 right parenthesis f reqG(α) = multG(α)/(|G| −|α| + 1)
because StartAbsoluteValue double struck upper G EndAbsoluteValue minus StartAbsoluteValue alpha EndAbsoluteValue plus 1|G| −|α| + 1 is the number of all the occurrences in double struck upper GG of the words with 
the same length of alphaα. 
Given a distribution over a set upper AA of elements, the associated Zipf’s curve is 
obtained by ordering (in increasing or decreasing order) the elements of upper AA with 
respect to their values of frequency, where elements with the same frequency are 
also said to be of the same rank, and then by associating to the elements of a given 
rank the corresponding value of frequency. 
The co-multiplicity distribution assigns to each valuemm of multiplicity the set of 
words (or the number of words) of double struck upper GG having that multiplicity: 
StartSet alpha element of upper D left parenthesis double struck upper G right parenthesis vertical bar m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis equals m EndSet vertical bar period{α ∈D(G) | multG(α) = m}|.
Multiplicity and co-multiplicity distributions are shown to be distinctive features 
for genomic sequences belonging to different species and phyla [ 18]. 
The entropy length distribution assigns to each word length kk the empirical 
entropy calculated on the frequencies of the words inupper D Subscript k Baseline left parenthesis double struck upper G right parenthesisDk(G). This distribution shows 
different trends that depend on the speciﬁc genomic sequence. This characterization 
is more evident for values of kk in the hapax-repeat interval. 
Figure 4.9 shows the distributional trends for a random sequence of 10.000 
monomers. 
Other interesting distributions, are based on the notions of coverage. There are two 
main notions of coverage relating genomes and dictionaries: (i) sequence coverage 
and (ii) position coverage. Informally, they denote, respectively, (i) the set of positions 
of a genome where are located the words of a given dictionary, and (ii) the set of 
words of a dictionary such that their occurrences, in a genome, pass trough a given 
position of it. 
Given a dictionaryupper DD, the  sequence coverage ofupper DD, for a genomedouble struck upper GG, is deﬁned as 
the set of positions ofdouble struck upper GG for which at least one word inupper DD is placed in them. Formally: 
p o s c o v Subscript double struck upper G comma p Baseline equals StartAbsoluteValue left brace p vertical bar there exists i comma j left parenthesis i less than or equals p less than or equals j right parenthesis and left parenthesis double struck upper G left bracket i comma j right bracket element of upper D right parenthesis right brace EndAbsoluteValue periodposcovG,p = |{p | ∃i, j(i ≤p ≤j) ∧(G[i, j] ∈D)}|.
Starting from the notion of sequence coverage, one can deﬁne genomic distributions 
assigning to a set of words of double struck upper GG all the positions of double struck upper GG covered by the considered 
set. Thus, given the setsupper H Subscript k Baseline left parenthesis double struck upper G right parenthesisHk(G) andupper R Subscript k Baseline left parenthesis double struck upper G right parenthesisRk(G), of hapaxes and repeats of lengthkk indouble struck upper GG,

84
4
Informational Genomics
Fig. 4.9 Distributional values of StartAbsoluteValue upper D Subscript k Baseline EndAbsoluteValue|Dk|, StartAbsoluteValue upper H Subscript k Baseline EndAbsoluteValue|Hk| and StartAbsoluteValue upper R Subscript k Baseline EndAbsoluteValue|Rk| for a random sequence of 10,000 positions 
generated on an alphabet of 4 symbols having equal probability. Them h lmhl value equals 6, the maximal 
repeat length (mrl) is 11 and the minimal forbidden length (mﬂ) is 6. Moreover,l g 4 left parenthesis 10 comma 000 right parenthesis equals 6.64lg4(10, 000) = 6.64
the hapax coverage distribution and repeat coverage distribution are deﬁned as 
k right arrow from bar StartAbsoluteValue c o v Subscript double struck upper G Baseline left parenthesis upper H Subscript k Baseline left parenthesis double struck upper G right parenthesis right parenthesis EndAbsoluteValuek {→|covG(Hk(G))| and k right arrow from bar StartAbsoluteValue c o v Subscript double struck upper G Baseline left parenthesis upper R Subscript k Baseline left parenthesis double struck upper G right parenthesis right parenthesis EndAbsoluteValuek {→|covG(Rk(G))|, respectively. 
In modern sequencing techniques, the notion of sequencing depth refers to the 
average number of reads that cover a given position of the sequenced genome. The 
set of reads produced by the sequencing operation can be seen as a set of words, and 
the sequencing depth corresponds to the notion of position coverage. Namely, given 
a genome double struck upper GG, a position pp of it, and dictionary upper DD, the position coverage of pp in upper DD is 
deﬁned as the number of words of upper DD which cover pp. Formally: 
p o s c o v Subscript double struck upper G comma p Baseline equals StartAbsoluteValue left brace alpha element of upper D vertical bar there exists i comma j left parenthesis i less than or equals p less than or equals j right parenthesis and left parenthesis double struck upper G left bracket i comma j right bracket equals alpha right parenthesis right brace EndAbsoluteValue periodposcovG,p = |{α ∈D | ∃i, j(i ≤p ≤j) ∧(G[i, j] = α)}|.
The position coverage distribution assigns to a value cc the number of positions on 
double struck upper GG having such a speciﬁc value of position coverage. Formally: 
c right arrow from bar StartAbsoluteValue StartSet p colon p o s c o v Subscript double struck upper G comma p Baseline left parenthesis upper L right parenthesis equals c EndSet EndAbsoluteValue periodc {→|{p : poscovG,p(L) = c}|.
The Recurrence Distance Distribution upper R upper D upper D Subscript double struck upper G Baseline left parenthesis alpha right parenthesisRDDG(α) is the function assigning to 
each value dd of a distance the number of times two consecutive occurrences of alphaα in 
double struck upper GG are separated by a distance dd. 
It is important to remark thatupper R upper D upper D Subscript double struck upper G Baseline left parenthesis alpha right parenthesisRDDG(α) is often related to the function ofalphaα within 
the given genome. Namely, an alphaα such that upper R upper D upper D Subscript double struck upper G Baseline left parenthesis alpha right parenthesisRDDG(α) assumes a very characteristic 
pattern, with respect to the other strings, seems to be reasonably considered as an 
element representing a “genomic word” (see [ 7] for a detailed analysis of several 
aspects related to distance recurrence).

4.5 Genome Information Sources
85
Fig. 4.10 On the left side, recurrence distance distribution (up to distance200200) of the  wordsupper A upper T upper GAT G
from the chromosome 22 of Homo sapiens. Besides the distribution curve, there are4 comma 0814, 081 colored 
rows, in lexicographic order, of the8484-strings enclosed between the (minimal) recurrences ofupper A upper T upper GAT G
at distance8181. On the right side, the recurrence distance distribution of the wordupper A upper T upper GAT G in Escherichia 
coli. Besides the curve, there are 416416 strings upper A upper T upper G minus 78 upper N minus upper A upper T upper GAT G −78N −AT G denoted by coloured rows in 
lexicographic order, that is, the strings enclosed between the recurrences of upper A upper T upper GAT G at distance 8181. 
The enclosed strings of Escherichia coli show low similarity. Contrarily, a great part of the enclosed 
strings in Homo sapiens are similar. Moreover, approximately, the percentage of strings showing 
similarity corresponds to the portion of the peak emerging from the basic curve 
For example, by using upper R upper D upper DRDD it is possible to distinguish regions of genomes 
where this distribution presents regular peaks that result in strongly correlated with 
the coding (versus non-coding) property of these regions, as it is shown in Fig. 4.10. 
In general, the main criterium for discriminating between strings that are puta-
tive “genomic words” with respect to strings that do not express such conditions 
is based on the degree of anti-randomness of upper R upper D upper DRDD. In any long string a substring 
that occurs randomly has anupper R upper D upper DRDD distribution that approximates an exponential law 
(is the probability distribution associated with a Poisson process, and usually called 
waiting time, that is, the number of steps of the process between two occurrences of a 
rare event). Therefore, any kind of “distance” betweenupper R upper D upper D left parenthesis alpha right parenthesisRDD(α) and a suitable expo-
nential distribution e x pexp provides a reasonable criterium for characterizing genomic 
words. The recurrence pattern of akk-mer in a genome is an important aspect related 
to speciﬁc genome features. Entropic concepts applied toupper R upper D upper DRDD distributions can be 
applied for extracting informational regularities and peculiarities of given genomes 
or classes of genomes [ 3– 8]. 
For any genomic distribution, we can immediately deﬁne the usual statistical 
indexes: mean, standard deviation, max, min, and so on. 
Many different genomic distributions can be deﬁned for a given genome. However, 
it is crucial the choice the right variables by means of which genomic information 
sources are identiﬁed that are signiﬁcant for speciﬁc analyses of genomes.

86
4
Informational Genomics
4.6 Genome Spectra 
The k-Spectrum of a genome double struck upper GG [ 19], denoted by s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G), is deﬁned in terms of 
multiset, that is, a set of pairs (monomer, multiplicity): 
s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesis equals StartSet left parenthesis alpha comma m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis right parenthesis vertical bar alpha element of upper D Subscript k Baseline left parenthesis double struck upper G right parenthesis EndSet periodspeck(G) = {(α, multG(α)) | α ∈Dk(G)}.
In the following, the usual operation of multiset-sum and multiset-difference are 
assumed forkk-spectra. In particular, the elements of a multiset-sumupper A plus upper BA + B are those 
of upper AA or upper BB with the multiplicities given by the sum of their multiplicities in upper AA and 
upper BB, while those of upper A minus upper BA −B have as multiplicities the differences of multiplicities, by 
setting zero the negative values. Moreover, any multiset does not change if a new 
element with zero multiplicity is added o removed, therefore a multiset having only 
zero multiplicities can be considered equivalent to the empty set. 
Assuming all thekk-mers in the lexicographic order, anykk-spectrum is completely 
represented by a multiplicity vectorupper WW, where theii-component gives the multiplicity, 
in the kk-spectrum, of the kk-mer of lexicographic position ii. 
Now we introduce a number of concepts related to kk-spectra that provide very 
powerful representations of genomes [ 19]. 
A genomedouble struck upper GG is said to be k-univocal (in spectrum) if no genome different fromdouble struck upper GG
has the samekk-spectrum ofdouble struck upper GG. The sum of the multiplicities associated tos p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G)
coincides with StartAbsoluteValue double struck upper G EndAbsoluteValue minus k plus 1|G| −k + 1. 
The following toy example of spectra helps to explain the concept. Let us consider 
the string a cubed b squared a Superscript 5a3b2a5. Its 2-spectrum is the following: 
s p e c 2 left parenthesis a cubed b squared a Superscript 5 Baseline right parenthesis equals StartSet left parenthesis a a comma 6 right parenthesis comma left parenthesis a b comma 1 right parenthesis comma left parenthesis b b comma 1 right parenthesis comma left parenthesis b a comma 1 right parenthesis EndSetspec2(a3b2a5) = {(aa, 6), (ab, 1), (bb, 1), (ba, 1)}
moreover: 
s p e c 2 left parenthesis a Superscript 5 Baseline b squared a cubed right parenthesis equals s p e c 5 left parenthesis a cubed b squared a Superscript 5 Baseline right parenthesisspec2(a5b2a3) = spec5(a3b2a5)
Therefore, a cubed b squared a Superscript 5a3b2a5 is and a Superscript 5 Baseline b squared a cubeda5b2a3 are not 22-univocal. However, it is easy to realize 
that for k equals 5k = 5 we have: 
s p e c 5 left parenthesis a cubed b squared a Superscript 5 Baseline right parenthesis not equals s p e c 5 left parenthesis a Superscript 5 Baseline b squared a cubed right parenthesis periodspec5(a3b2a5) /= spec5(a5b2a3).
Proposition 4.16 A kk-hapax genome is sure kk-univocal in the spectrum, but the 
converse implication does not hold. 
Proof If a genome iskk-hapax, then itskk-mers can be concatenated in only one way, 
because they appear only once in the genome. However, the converse implication 
does not hold. We can show this fact with an example, which for a better reading we 
give in the English alphabet: 
c a n e g a t t o g a l l o g a t t o n e periodcanegattogallogattone.

4.6 Genome Spectra
87
All the 44-mers occurring in it are hapaxes, apart from g a t tgatt and a t t oatto, which have 
multiplicity 22. Therefore, the sequence is not 44-hapax. Nevertheless, this sequence 
is 44-univocal, as can be easily veriﬁed.
∎
Of course, given a genome double struck upper GG, some k greater than 1k > 1 has to exist such that double struck upper GG is kk-univocal 
(surelydouble struck upper G primeG' is univocal fork equals StartAbsoluteValue double struck upper G EndAbsoluteValuek = |G|). We call spectrality ofdouble struck upper GG, denoted bys p l left parenthesis double struck upper G right parenthesisspl(G), the  
minimum kk for which double struck upper GG is kk-univocal: 
s p l left parenthesis double struck upper G right parenthesis equals m i n left brace k vertical bar double struck upper G i s k minus u n i v o c a l right brace periodspl(G) = min{k | G is k−univocal}.
If double struck upper GG is not kk-univocal there is at least one different genome double struck upper G primeG' having the same 
spectrum of double struck upper GG. This means that all the kk-mers occurrences of the spectrum occur in 
the two genomes double struck upper GG and double struck upper G primeG' with different orders. A natural question is: which are 
the longest substrings occurring in both genomes? 
In the extreme case, these arrangements coincide with thekk-mers, but in general, 
these portions can be longer than kk and it would be interesting to ﬁnd such com-
mon components of genomes having the same kk-spectrum. Let us introduce some 
deﬁnitions for answering this question in a general setting. 
We say that a kk-spectrum upper HH admits double struck upper GG when 
s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesis subset of or equal to upper Hspeck(G) ⊆H
where the inclusion has to be intended as multiset inclusion, that is, all kk-mers 
occurring in the left part occur in the right one with an equal or greater multiplicity. 
Of course, if double struck upper G primeG' is a proper segment of double struck upper GG (preﬁx, inﬁx or sufﬁx of double struck upper GG), then: 
s p e c Subscript k Baseline left parenthesis double struck upper G Superscript prime Baseline right parenthesis subset of s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesis periodspeck(G') ⊂speck(G).
A left parenthesis k minus 1 right parenthesis(k −1)-concatenation of two strings alpha gamma comma gamma betaαγ, γβ, also called concatenation with 
left parenthesis k minus 1 right parenthesis(k −1) overlap, is the string alpha gamma betaαγβ where StartAbsoluteValue gamma EndAbsoluteValue equals k minus 1|γ | = k −1. A string alphaα is univocally kk-
assembled in a given kk-spectrum upper HH, when the kk-preﬁx betaβ of alphaα is in upper HH, and for any 
proper preﬁx gammaγ of alphaα, there is only one kk-mer of upper HH that can be left parenthesis k minus 1 right parenthesis(k −1)-concatenated 
to it, by right elongating gammaγ of one symbol. 
We saykk-spectral segment ofdouble struck upper GG any string that is maximal among the univocally 
kk-assembled strings ins p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G). Akk-spectral reverse segment ofdouble struck upper GG is akk-spectral 
segment of the reverse of double struck upper GG, and an kk-spectral symmetric segment of double struck upper GG is any 
string that is a kk-spectral segment and a kk-spectral reverse segment of double struck upper GG. 
From the previous deﬁnition of kk-spectral segment, the following proposition 
follows. 
Proposition 4.17 A kk-spectral segment of double struck upper GG is also a kk-spectral segment of any 
genome having the same spectrum of double struck upper GG. 
The following proposition easily follows from the previous one.

88
4
Informational Genomics
Fig. 4.11 Preﬁxes and sufﬁxes having the same form denote equal strings longk minus 1k −1
Proposition 4.18 Two different genomes double struck upper GG and double struck upper G primeG' have the same kk-spectrum if, 
and only if, they have the samekk-spectral segments, possibly occurring in them with 
different relative orders. 
We remark that when the condition of univocal elongation does not hold, there 
are strings that are admitted by thekk-spectrum, but that do not occur in the genome. 
For example, c a n e g a t t ocanegatto and a t t o g a l l o g a t t o n eattogallogattone are the 44-segments of the 44-
spectral segmentation of 
c a n e g a t t o g a l l o g a t t o n ecanegattogallogattone
but c a n e g a t t o n ecanegattone is not a 44-spectral segment of that string, even if it is admitted by 
its44-spectrum, becausec a n e g a t t ocanegatto is a proper substring of both stringsc a n e g a t t o gcanegattog
and c a n e g a t t o ncanegatton that occur in the string. 
In conclusion, the kk-mers of the spectrum of double struck upper GG occur, in different orders, in all 
genomes with the same kk-spectrum of double struck upper GG, the  kk-spectral segments of double struck upper GG extend this 
property ofkk-mers ofs p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G), in the sense thatkk-spectral segments are the maximal 
strings, occurring with different orders, in all genomes having spectrum s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G). 
Two strings, occurring in a genome double struck upper GG, are spectrally kk-permutable when, after 
they are swapped indouble struck upper GG thekk-spectrum remain equal tos p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G). Spectral segments 
determine genomes apart from the swapping of kk-spectral segments. It is easy to 
realize that two strings are spectral kk-permutable if they have the same k minus 1k −1 preﬁx 
and the same k minus 1k −1 sufﬁx. Figure 4.11 shows spectrally kk-permutable strings. 
If a genome double struck upper GG is kk-univocal, then its kk-spectral segments can be concatenated, 
withleft parenthesis k minus 1 right parenthesis(k −1)-overlap, in only one way. However, a genome double struck upper GG can be kk-univocal but 
overlap concatenations of kk-mers of its spectrum may be obtained that do not occur 
in the genome. 
A kk-spectral segmentation of s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G) (or of double struck upper GG), is the multiset of kk-spectral 
segments of double struck upper GG, each with its multiplicity in double struck upper GG. 
The following procedures provide two ways of computing spectral segmentations 
in two cases: from a given spectrum, and from a given genome. 
Algorithm 4.1 (computing a kk-segmentation from a kk spectrum) 
Letupper HH be akk-spectrum. The procedure starts by assigning to the current stringww one 
kk-mer chosen out of upper HH and, at each step, searches in upper HH for the kk-mer that uniquely 
elongates ww, on the right, or on the left, by updating upper HH after removing the kk-mer

4.6 Genome Spectra
89
occurrence used in the elongation. If more than one possible kk-mers of upper HH can be 
concatenated to both the extremities of the current string, then the concatenation 
process halts, and the string obtained so far is produced in output, as a kk-spectral 
segment. Then, the procedure restarts with a similar process with another kk-mer of 
upper HH. The procedure halts when upper HH becomes empty.
∎
In the procedure above it is not essential to establish the choice criterion for the 
initial string ww (the ﬁrst kk-mer in the lexicographic order could be one possibility). 
Some of the kk-spectral maximal segments produced in output during the whole 
process above can be generated with a multiplicity greater than 1. The multiset 
of these segments with their respective multiplicities is the kk-segmentation of H. 
Clearly, in the genomes withkk-spectrumupper HH, thekk-spectral segments of the obtained 
kk-segmentation of upper HH occur with different possible orders that are speciﬁc to each 
genome. 
If we change perspective and assume to start from the genomic sequence itself, 
we may compute its spectral segmentation in a more efﬁcient way. We used this 
procedure in order to compute the kk-segmentations of real genomes [ 19]. 
Algorithm 4.2 (computing a kk-segmentation from a genome) 
A kk-mer alpha element of upper D Subscript k Baseline left parenthesis double struck upper G right parenthesisα ∈Dk(G) is univocally elongated in (the kk-spectrum of) upper GG if StartAbsoluteValue StartSet beta element of upper D Subscript k Baseline left parenthesis upper G right parenthesis colon alpha left bracket 2 period period period k right bracket equals beta left bracket 1 period period period k minus 1 right bracket EndSet EndAbsoluteValue equals 1|{β ∈
Dk(G) : α[2...k] = β[1...k −1]}| = 1. A Boolean array upper AA, such that StartAbsoluteValue upper A EndAbsoluteValue equals StartAbsoluteValue double struck upper G EndAbsoluteValue|A| = |G|, is  
initialized to be false in every position. 
For eachalpha element of upper D Subscript k Baseline left parenthesis upper G right parenthesisα ∈Dk(G), such thatalphaα is univocally elongated indouble struck upper GG, the algorithm sets as 
t r u etrue all the positions ofupper AA of the setp o s left parenthesis alpha comma double struck upper G right parenthesis equals StartSet i colon double struck upper G left bracket i comma period period period comma i plus k minus 1 right bracket equals alpha EndSetpos(α, G) = {i : G[i, ..., i + k −1] = α}. Then, 
thekk-segmentation is retrieved by scanning for consecutive runs oft r u etrue assignments 
in upper AA. A  kk-spectral segment is a substring double struck upper G left bracket i period period period j right bracketG[i... j] of double struck upper GG such that upper A left bracket l right bracket equals t r u eA[l] = true for all l colon i less than or equals l less than or equals j∀l :
i ≤l ≤j.
∎
All the notions based on genome spectra can be extended with respect to a dictio-
naryupper DD, by replacings p e c Subscript k Baseline left parenthesis double struck upper G right parenthesisspeck(G) bys p e c Subscript upper D Baseline left parenthesis double struck upper G right parenthesisspecD(G), the multiset of strings ofupper DD with their 
occurrence multiplicities in double struck upper GG: 
s p e c Subscript upper D Baseline left parenthesis double struck upper G right parenthesis equals StartSet left parenthesis alpha comma m u l t Subscript double struck upper G Baseline left parenthesis alpha right parenthesis right parenthesis vertical bar alpha element of upper D EndSet periodspecD(G) = {(α, multG(α)) | α ∈D}.
Of course: 
s p e c Subscript k Baseline left parenthesis double struck upper G right parenthesis equals s p e c Subscript upper D Sub Subscript k Subscript left parenthesis double struck upper G right parenthesis Baseline left parenthesis double struck upper G right parenthesis periodspeck(G) = specDk(G)(G).
We call kk-spectral ambiguity of double struck upper GG the number of different genomes having the 
same spectrum of double struck upper GG. A more practical index of ambiguity, we call kk-permutation 
ambiguity is the average number of kk-spectral segments that are spectrally kk-
permutable. 
We deﬁne the absolute spectrum of a genomedouble struck upper GG, denoted bys p e c Subscript upper A b s Baseline left parenthesis double struck upper G right parenthesisspecAbs(G), as the  
spectrum of double struck upper GG relative to its absolute dictionary upper A b s left parenthesis double struck upper G right parenthesisAbs(G). 
Spectra, when visualized by means of heat maps, highlight the graphical sim-
ilarity of the human chromosomes, and their dissimilarity with the mitochondrial 
chromosomes, as it is shown in Fig. 4.12. The ﬁgure shows 6-mers spectrum of each

90
4
Informational Genomics
Fig. 4.12 6-mer multiplicity heat map of all human chromosomes. The bottom four colors ruler 
indicates the 3-mer preﬁx of each column block. The original image was created using40964096 (equals 
to4 Superscript 646) pixels  
chromosome by mapping multiplicities to a given palette. The correspondence to the 
black colour is performed by mapping such colour to the minimum multiplicity of 
each chromosome, and the maximum multiplicity is mapped to the red colour.

4.7 Elongation and Segmentation
91
4.7 Elongation and Segmentation 
Elongation and segmentation are two crucial aspects of genomes. 
A segmentation of a genome is a multiset of strings covering the genome, where 
each string is equipped with its multiplicity of occurring in the genome. 
We have already met elongation and segmentation in the analysis of genome 
spectra. 
If we consider that genomes are products of a long evolutive process where longer 
and longer sequences were assembled from very short fragments, then we easily 
realize how much elongation and segmentation are important in selecting strings 
that compose genomes. 
For value of k greater than or equals 20k ≥20 the set of all possible strings, become gigantic, and those 
really occurring in genomes are only an inﬁnitesimal part. This means that after this 
value elongation of kk-mers is a very selective operation. 
A right elongation of a non-empty string is a string of whichalphaα is a preﬁx, while 
a left elongation is a string of which it is a sufﬁx. String alphaα is right elongated by betaβ
in a genome double struck upper GG if alphaα is preﬁx of betaβ that occurs in double struck upper GG. Moreover alphaα is right univocally 
elongated bybetaβ if there is no stringgammaγ different frombetaβ such thatalphaα is right elongated 
bygammaγ indouble struck upper GG. Finally,alphaα is right maximally elongated bybetaβ indouble struck upper GG ifalphaα is right elongated 
bybetaβ butbetaβ is not univocally right elongated by any string indouble struck upper GG. Analogous deﬁnitions 
can be given for left elongation. 
Figure 4.13 is the representation of an elongation tree. Starting from a small 
number of “seeds” an entire genome can be expressed by showing all the elongations 
of the seeds covering the given genome. Figures 4.16, 4.15 and 4.14 are examples 
of representations of small genomes that are based on elongation trees (using circles 
and colors). What is surprising in these representations is the strong symmetry of the 
Fig. 4.13 An elongation tree 
(from [ 20])

92
4
Informational Genomics
Fig. 4.14 An elongation tree represented by means of circles (from [ 20]) 
structures and the self-similarity between parts. Similarly, Figs. 4.17, 4.18 and 4.19 
show the multiplicity of k-mers for kk from 1 to 6 mapped to the gradients that are 
shown in the bottom part of the image. A multiplicity value of 0 is mapped to black, 
instead, the maximum multiplicity, for each value of k, is mapped to red. Each row 
of the image represents a different value of kk. Within each row, k-mers are sorted 
according to the lexicographic order. The images also contain coloured rules that 
help the reader to identify k-mer by means of their preﬁx. The top ruler reports the 
4 1-mers, lexicographically sorted and coloured with blue, red, green and yellow, 
respectively from A to T. The bottom ruler, displaced at the centre of the image, 
reports the 3-mers from upper A upper A upper AAAA to upper T upper T upper TT T T from the left to the right. Such a ruler can be 
used to identify the k-mers (for k greater than 3, the 256 columns rising from a given 
3-mer cover all the kk-mers with that common 3-preﬁx). Although, the resolution 
of the reported heat maps is poor, nevertheless they give an overall view, which 
highlights, in a graphical way, differences among species, such as Homo sapiens, 
Caenorhabditis elegans and Caenorhabditis elegans. 
Proposition 4.19 Hapaxes of a genome are, right and left, univocally elongated in 
the genome.

4.7 Elongation and Segmentation
93
Fig. 4.15 Genome of Coliphage Phi-X174 represented by 103 seeds of maximum elongation length 
15 (from [ 20]) 
Fig. 4.16 Genome of Carsonella Ruddii (from [ 20])

94
4
Informational Genomics
Fig. 4.17 Word branching heat map of the human chromosome 1. Values ofkk areleft parenthesis 1 comma 2 comma 3 comma 4 comma 5 comma 6 right parenthesis(1, 2, 3, 4, 5, 6)
and displayed in the same order, from top to bottom side 
Fig. 4.18 Word branching heat map of Escherichia coli 
Fig. 4.19 Word branching heat map of Caenorhabditis elegans 
Genome Sequencing consists of the correct identiﬁcation of the nucleotide 
sequence of a given genome. The methods used for sequencing are based on many dif-
ferent biochemistry and biophysics techniques (everywhere changing and improving 
their efﬁciency, cost, and reliability, according to technical progress in nanotechnol-
ogy). However, the basic principle of sequencing remains the same in all different 
technologies, which essentially are algorithms of a sequence reconstruction from a 
dictionary. Namely, a given genome is always fragmented in pieces, called reads, 
smaller than a given length. The length goes from some tenths to some thousand 
bases, according to the speciﬁc methodologies, which continuously increase both 
length and efﬁciency. These fragments are read by means of mechanisms of bio-
chemical of physical nature. A sequencing technique deduces the most probable 
sequence containing all the reads. Usually, reads need to be ampliﬁed, by providing 
many copies of each one. Crucial factors of the process are not only the average 
length of reads but also their redundancy, that is, the average number of reads asso-
ciated with any genome position. It is experimentally proved that the shorter reads 
are, the greater their redundancy has to be, in order to obtain good sequencing. Reads 
are assembled, that is, they are concatenated with overlaps, and the longer and more 
precise are the overlaps, the more, their concatenations are reliable. By applying iter-

4.7 Elongation and Segmentation
95
atively, and hierarchically, the process of overlap concatenation of longer substrings 
of the original genome is generated, up to a unique sequence that corresponds to the 
putative genome from which the initial reads were generated (probability and preci-
sion can be quantiﬁed by using reasonable hypotheses about the errors introduced in 
the assembly process). 
Two main kinds of sequencing can be distinguished: “de novo sequencing” and re-
sequencing. In the second case, the genome sequence is already known for a genome 
of the same species, therefore reads are aligned, or mapped, (with some possible 
mismatches) to a whole sequence of reference. On the contrary, in the ﬁrst case, this 
reference is missing, therefore the greater number of possible solutions have to be 
considered, evaluated and discriminated. 
Here we want to point out an aspect that, in many cases, could be a methodological 
limitation of the sequencing approach. In fact, since the ﬁrst approaches to human 
sequencing project, the genome to which the process above outlined was applied did 
not correspond to the genome of a single individual but was a sort of mixing derived 
from a small population of individual genomes. This choice was motivated by the 
purpose of obtaining, in such a way, something more general and representative of 
the biological species of selected individuals. 
In our opinion, now we are in a position where the new technological efﬁciency 
allows us to reverse this kind of approach. Namely, if a big number of different 
individual genomes are sequenced, from the analysis of their differences and of their 
common structure, we can deduce, in a more correct way, what can be considered 
the essence of all genomes of a given species. This new perspective could provide 
new ideas and new applications in many ﬁelds, from medical science to a better 
understanding of evolutionary mechanisms. 
The following proposition gives an interesting result that characterizes the orga-
nization of genomes in terms of particular kinds of spectra. 
Given a genome double struck upper GG and a kk-mer alphaα and x element of StartSet upper A comma upper T comma upper C comma upper G EndSetx ∈{A, T, C, G}, we denote by 
e Subscript double struck upper G comma x Baseline left parenthesis alpha right parenthesiseG,x(α)
the number of times alphaα is right elongated with x element of double struck upper Gx ∈G. Of course, it holds that: 
m u l t left parenthesis alpha right parenthesis equals e Subscript double struck upper G comma upper A Baseline left parenthesis alpha right parenthesis plus e Subscript double struck upper G comma upper T Baseline left parenthesis alpha right parenthesis plus e Subscript double struck upper G comma upper C Baseline left parenthesis alpha right parenthesis plus e Subscript double struck upper G comma upper G Baseline left parenthesis alpha right parenthesis periodmult(α) = eG,A(α) + eG,T (α) + eG,C(α) + eG,G(α).
From this equation, it easily follow that when two genomes have the same left parenthesis n plus 1 right parenthesis(n + 1)-
spectrum, then, necessarily, they have also the same nn-spectrum. Vice versa, from 
the knowledge of e Subscript double struck upper G comma x Baseline left parenthesis alpha right parenthesiseG,x(α) for x element of double struck upper Gx ∈G, it is easy to compute the left parenthesis n plus 1 right parenthesis(n + 1)-spectrum of 
double struck upper GG from its nn-spectrum. 
We denote by f e l left parenthesis double struck upper G right parenthesis f el(G) the fully elongated length of double struck upper GG, that is the length kk such 
that e Subscript double struck upper G comma x Baseline left parenthesis alpha right parenthesis greater than 0eG,x(α) > 0 for any x element of StartSet upper A comma upper T comma upper C comma upper G EndSetx ∈{A, T, C, G}, and for any possible kk-mers alphaα. 
Proposition 4.20 Let k equals m g l left parenthesis double struck upper G right parenthesis plus 1k = mgl(G) + 1 (the minimal hapax greatest length incre-
mented by 1), then if a genome double struck upper GG has the same kk-spectrum of double struck upper G 1G1, both genomes 
double struck upper GG and double struck upper G 1G1 have the same minimal hapaxes that occur in both genome in the sane

96
4
Informational Genomics
relative order. Moreover, the two genomes may differ for possible swaps of strings 
located between their common minimal hapaxes. 
Proof First, having the two genomes the sameleft parenthesis n plus 1 right parenthesis(n + 1)-spectrum the multiplicity of 
any nn-mer alphaα is the same in both genomes. 
Now, if double struck upper GG and double struck upper G 1G1 have the same kk-spectrum, then they have the same stings of 
lengthkk with multiplicity 1. But for the deﬁnition ofkk, all the minimal hapaxes ofdouble struck upper GG
are substrings of thekk-mers with multiplicity 1, and moreover, the two genomes have 
the same spectra for values smaller than kk, therefore, they have the same minimal 
hapaxes. Moreover, in both genomes hapaxes of lengthkk, including minimal hapaxes, 
are notkk-permutable, because two of them cannot have the sameleft parenthesis k minus 1 right parenthesis(k −1)-preﬁx and 
left parenthesis k minus 1 right parenthesis(k −1)-sufﬁx, being some of these strings hapaxes too. 
In conclusion, repeats of a genome have to occur in the regions comprised between 
(non kk-permutable) consecutive kk-hapaxes. In particular, being hapaxes elongation 
of minimal hapaxes, repeats surely occur between consecutive minimal hapaxes. ∎
Given a genome double struck upper GG, for any kk, we denote by 
c a r d Subscript k Baseline left parenthesis double struck upper G right parenthesiscardk(G)
the number or different kk-mers that occur in double struck upper GG. The value: 
log Subscript 2 Baseline c a r d Subscript m g l Baseline left parenthesis double struck upper G right parenthesis log2 cardmgl(G)
is the number of bits necessary for representing all the minimal hapaxes ofdouble struck upper GG, while 
2 m g l left parenthesis double struck upper G right parenthesis2mgl(G) is the number of bits necessary for representing all possible m g lmgl-mers. 
Therefore, the following fraction is inversely proportional to the “informational selec-
tivity” of double struck upper GG, because the smaller it is, the greater the information that the genome 
selects: 
StartFraction log Subscript 2 Baseline c a r d Subscript m g l Baseline left parenthesis double struck upper G right parenthesis Over 2 m g l left parenthesis double struck upper G right parenthesis EndFraction periodlog2 cardmgl(G)
2mgl(G)
.
This is only a partial indication, because does not count the selection related to the 
repeats between minimal hapaxes (see later on). 
Letupper D Subscript m h Baseline left parenthesis double struck upper G right parenthesisDmh(G) be the dictionary of the minimal hapaxes ofdouble struck upper GG. For anyalpha element of upper D Subscript m h Baseline left parenthesis double struck upper G right parenthesisα ∈Dmh(G), 
we denote by e Subscript double struck upper G Baseline left parenthesis alpha right parenthesiseG(α) the unique right elongation of alpha element of double struck upper Gα ∈G, that does not include the 
minimal hapax following alphaα in double struck upper GG, then the set: 
upper E Subscript m h Baseline equals StartSet e Subscript double struck upper G Baseline left parenthesis alpha right parenthesis vertical bar alpha element of upper D Subscript m h Baseline left parenthesis double struck upper G right parenthesis EndSetEmh = {eG(α)|α ∈Dmh(G)}
determines a segmentation of double struck upper GG where all the segments of upper E Subscript m hEmh are hapaxes. 
The setupper D Subscript m h r Baseline left parenthesis double struck upper G right parenthesisDmhr(G) of maximal repeats occurring between two consecutive minimal 
hapaxes of double struck upper GG constitutes a dictionary such that the whole genome double struck upper GG is a sequence 
having the following form (h Subscript ihi is the minimal hapax of occurrence order ii): 
double struck upper G equals h prime 1 upper R 1 h prime 2 upper R 2 ellipsis h prime Subscript m Baseline upper R Subscript m BaselineG = h'
1R1h'
2R2 . . . h'
m Rm

4.8 Genome Informational Laws
97
where, for i less than or equals mi ≤m, h prime Subscript ih'
i is a hapax including (possibly non strictly) the minimal hapax 
h Subscript ihi, while upper R Subscript i Baseline element of upper D Subscript m h r Baseline left parenthesis double struck upper G right parenthesisRi ∈Dmhr(G), and for different values of i less than j less than mi < j < m it may be upper R Subscript i Baseline equals upper R Subscript jRi = R j. 
The same form holds for all genomes having the same left parenthesis m g l plus 1 right parenthesis(mgl + 1)-spectrum as double struck upper GG, 
apart from a possible ordering of the inter-hapax repeats. 
4.8 Genome Informational Laws 
Random Genomes are generated by a Bernoulli random process extracting at each 
step one of the letters inupper GammaΓ , having each letter the same probability of being extracted. 
In a next chapter we will study such kinds of genomes, and we will show that the 
empiricalkk-entropies of random genomes of lengthnn, for anyk greater than or equals 1k ≥1, are bounded by 
2 log Subscript 2 Baseline n equals 2 upper L upper G2 log2 n = 2LG. Moreover, entropyupper E Subscript 2 upper L upper G Baseline left parenthesis double struck upper G right parenthesisE2LG(G), for any genomedouble struck upper GG, is bounded by the 
value of upper E Subscript 2 upper L upper GE2LG in random genomes of the same length as double struck upper GG, therefore the following 
inequalities hold: 
StartEnclose box bold First Law upper L upper G left parenthesis double struck upper G right parenthesis less than upper E Subscript 2 upper L upper G Baseline left parenthesis double struck upper G right parenthesis less than 2 upper L upper G left parenthesis double struck upper G right parenthesis EndEnclose First Law
LG(G) < E2LG(G) < 2LG(G)
By recalling the meaning of entropy of an information source, as mean information 
of the data emitted by the source, the relation above expresses a very general property 
of genomes of a given length, saying that forkk-mers of binary logarithmic length, the 
mean information quantity emitted by a genome, as an information source ofkk.mers, 
is between the logarithmic length and the double logarithmic length of the genome. 
This fact has been veriﬁed for a wide class of genomes of different types [ 4]. 
Given a genome double struck upper GG, two values are associated to it: the entropic component 
upper E upper C left parenthesis double struck upper G right parenthesisEC(G) and the anti-entropic component upper A upper C left parenthesis double struck upper G right parenthesisAC(G), deﬁned by the following equa-
tions: 
upper E upper C left parenthesis double struck upper G right parenthesis equals upper E Subscript 2 upper L upper G Baseline left parenthesis double struck upper G right parenthesis minus upper L upper G left parenthesis double struck upper G right parenthesisEC(G) = E2LG(G) −LG(G)
upper A upper C left parenthesis double struck upper G right parenthesis equals 2 upper L upper G left parenthesis double struck upper G right parenthesis minus upper E Subscript 2 upper L upper G Baseline left parenthesis double struck upper G right parenthesisAC(G) = 2LG(G) −E2LG(G)
shortly denoted byupper E upper CEC andupper A upper CAC, when the genome is understood. From these deﬁni-
tions follows that: 
upper L upper G equals upper A upper C plus upper E upper C periodLG = AC + EC.
The anti-entropic fraction upper A upper F left parenthesis double struck upper G right parenthesisAF(G) is given by: 
upper A upper F left parenthesis double struck upper G right parenthesis equals upper A upper C left parenthesis double struck upper G right parenthesis divided by upper L upper G left parenthesis double struck upper G right parenthesisAF(G) = AC(G)/LG(G)
while the entropic fraction upper E upper F left parenthesis double struck upper G right parenthesisE F(G) is given by: 
upper E upper H left parenthesis double struck upper G right parenthesis equals left bracket upper E upper C left parenthesis double struck upper G right parenthesis minus upper A upper C left parenthesis double struck upper G right parenthesis right bracket divided by upper L upper GE H(G) = [EC(G) −AC(G)]/LG
and from the deﬁnitions above it follows that upper E upper H equals 1 minus 2 upper A upper FE H = 1 −2AF.

98
4
Informational Genomics
Let upper L upper XL X be the index upper L upper X Subscript k Baseline equals StartAbsoluteValue upper D Subscript k Baseline left parenthesis upper G right parenthesis EndAbsoluteValue divided by StartAbsoluteValue upper G EndAbsoluteValueL Xk = |Dk(G)|/|G| with k equals 2 upper L upper G left parenthesis double struck upper G right parenthesisk = 2LG(G) (upper D Subscript kDk the dictionary 
of kk-mers). In [ 4] some informational laws have been identiﬁed and expressed, in 
terms of the entropic indexes introduced above, which were tested for 70 genomes of 
different types (from bacteria to primates). Here we add to the previously given law, 
a second law relating the entropic components to the lexical index upper L upper XL X, associated 
to the length upper L upper GLG: 
StartEnclose box bold upper S bold e bold c bold o bold n bold d bold l bold a bold w upper E upper C greater than upper L upper X dot upper A upper C EndEnclose Second law
EC > L X · AC
Apart from the speciﬁc contents of these informational laws, it is worthwhile to notice 
the interest of such kinds of laws for a deep comprehension of the informational nature 
of genomes. 
A third law, given in the next section, will put in evidence the role of the two 
entropic components of genomes in expressing their evolutionary complexity. 
4.9 Genome Complexity 
Letdouble struck upper GG a genome of lengthnn. A complexity measure is deﬁned in [ 4], which is based on 
the entropy and anti-entropy of genomes deﬁned in the previous section. For genomes 
of 70 different biological species such a measure agrees in a very surprising manner 
with the evolutionary complexity of species, in the usual phylogenetic sense. The 
entropic component is responsible for the chaotic component pushing genomes to 
change, while the anti-entropic component refers to the structure and to biological 
functions of genomes. 
The index upper B upper BBB, called also Bio-Bit, is given by: 
upper B upper B equals StartRoot upper L upper G EndRoot StartRoot upper A upper F EndRoot left parenthesis 1 minus upper A upper F right parenthesis cubedBB =
√
LG
√
AF(1 −AF)3
it corresponds to the measure of evolutionary complexity. Its deﬁnition, apart from 
the factor StartRoot upper L upper G EndRoot
√
LG, is a sort of logistic map x left parenthesis 1 minus x right parenthesisx(1 −x), where factors are weighted by 
exponents: x Superscript one half Baseline left parenthesis 1 minus x right parenthesis cubedx
1
2 (1 −x)3, according to experimental evaluations over a great number 
of genomes, at different levels in the evolutionary scale. 
The equilibrium between the two entropic components expresses an essential 
aspect of evolution, as a consequence of deep abstract informational dynamics that 
is intrinsic to genome texts. The complexity measure that we deﬁne increases in 
time by disclosing a tendency opposite to that one driven by the second principle 
of thermodynamics. In fact, physical entropy concerns the individual genomes, as 
physical molecular structures. Species genomes, which are abstractions of a purely 
symbolic nature, increase their complexity as texts that in time exploit richer forms of 
life. In this perspective, death is a necessity of life, which refers only to the physical 
entities supporting individual instances of purely symbolic texts. In this perspective, 
the expression “purely symbolic text” is somewhat paradoxical, because intends 
something like an “unwritten text”. In other words, the written texts are of physical

4.9 Genome Complexity
99
Fig. 4.20 Genome Biobit ordering. In the abscissa the logarithmic length and in the ordinate the 
biobit value 
Fig. 4.21 Biobit ordering. 
The up-right corner of Fig. 
4.20 enlarged 
nature, subjected to perish, while what is common to a set of such texts is their form, 
which is independent from any individual physical text. This abstract form is what 
supports life development along with evolution. 
The last law given in [ 4] is here formulated, as a third informational law on 
genomes. This law claims thatupper B upper B left parenthesis double struck upper G right parenthesisBB(G) expresses the genomic complexity of genome 
double struck upper GG according to the following genomic complexity orderingprecedes≺(Figs. 4.20 and 4.21). 
StartEnclose box bold Third Law upper B upper B left parenthesis double struck upper G 1 right parenthesis less than upper B upper B left parenthesis double struck upper G 2 right parenthesis right double arrow double struck upper G 1 precedes double struck upper G 2 EndEnclose Third Law
BB(G1) < BB(G2) ⇒G1 ≺G2

100
4
Informational Genomics
4.10 Genome Divergences and Similarities 
Kullback and Leibler’s entropic divergence upper D left parenthesis p comma q right parenthesisD(p, q) is a measure of dissimilarity 
between two probability distributions p comma qp, q over the same variable upper XX: 
upper D left parenthesis p comma q right parenthesis equals sigma summation Underscript x element of ModifyingAbove upper X With caret Endscripts p left parenthesis x right parenthesis log base 10 left parenthesis p left parenthesis x right parenthesis divided by q left parenthesis x right parenthesis right parenthesis periodD(p, q) =
∑
x∈⌃X
p(x) lg(p(x)/q(x)).
The natural way of adapting this notion to a genomic dictionary is to consider the 
set of kk-mers of two genomes. If upper D Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesisDk(G1, G2) is the set of kk-mers that are common 
to two genomes, then the two distributions p comma qp, q are given by the frequencies of the 
elements of upper D Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesisDk(G1, G2) in double struck upper G 1 comma double struck upper G 2G1, G2, respectively: 
Delta Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesis equals sigma summation Underscript x element of upper D Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesis Endscripts p left parenthesis x right parenthesis log base 10 left parenthesis p left parenthesis x right parenthesis divided by q left parenthesis x right parenthesis right parenthesisΔk(G1, G2) =
∑
x∈Dk(G1,G2)
p(x) lg(p(x)/q(x))
The value Delta Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesisΔk(G1, G2) can be also multiplied by 
StartAbsoluteValue upper Gamma Subscript k Baseline EndAbsoluteValue divided by StartAbsoluteValue upper D Subscript k Baseline left parenthesis double struck upper G 1 comma double struck upper G 2 right parenthesis EndAbsoluteValue|Γk|/|Dk(G1, G2)|
for considering the inaccuracy due to missing frequencies of kk-mers that are not 
common to both genomes. 
Divergence is surely a useful concept in measuring the disagreement between 
random variables or distributions. But other measures of similarity or dissimilarity 
can be deﬁned. One of them is Jaccard distance that is based on a dictionary. In its 
original formulation, given two strings, the dictionaries ofkk-mers (for some value of 
kk) for both sequences are considered and then the ratio between the cardinality of the 
intersection with the cardinality of the union is considered as a degree of similarity 
between the two sequences, expressed as a value between 0 and 1: 
upper J Subscript k Baseline left parenthesis alpha comma beta right parenthesis equals StartFraction StartAbsoluteValue upper D Subscript k Baseline left parenthesis alpha right parenthesis intersection upper D Subscript k Baseline left parenthesis beta right parenthesis EndAbsoluteValue Over StartAbsoluteValue upper D Subscript k Baseline left parenthesis alpha right parenthesis union upper D Subscript k Baseline left parenthesis beta right parenthesis EndAbsoluteValue EndFractionJk(α, β) = |Dk(α) ∩Dk(β)|
|Dk(α) ∪Dk(β)|
A variant of this deﬁnition takes into account also the multiplicities of kk-mers, 
by considering the union of two dictionaries and taking as a similarity measure the 
ratio between a numerator, given by the sum of the minimum of the multiplicities 
of kk-mers in the union dictionary, and a denominator, given by the maximum of the 
multiplicities of kk-mers in the union dictionary. 
upper J prime Subscript k Baseline left parenthesis alpha comma beta right parenthesis equals StartFraction sigma summation Underscript w element of upper D Subscript k Baseline left parenthesis alpha right parenthesis union upper D Subscript k Baseline left parenthesis beta right parenthesis Endscripts m i n left brace m u l t Subscript alpha Baseline left parenthesis w right parenthesis comma m u l t Subscript beta Baseline left parenthesis w right parenthesis right brace Over sigma summation Underscript w element of upper D Subscript k Baseline left parenthesis alpha right parenthesis union upper D Subscript k Baseline left parenthesis beta right parenthesis Endscripts m a x left brace m u l t Subscript alpha Baseline left parenthesis w right parenthesis comma m u l t Subscript beta Baseline left parenthesis w right parenthesis right brace EndFractionJ '
k(α, β) =
∑
w∈Dk(α)∪Dk(β) min{multα(w), multβ(w)}
∑
w∈Dk(α)∪Dk(β) max{multα(w), multβ(w)}
Jaccard distance is 1 minus upper J Subscript k Baseline left parenthesis alpha comma beta right parenthesis1 −Jk(α, β), which can also be deﬁned by considering the 
symmetric difference of kk-mer dictionaries:

4.10 Genome Divergences and Similarities
101
StartFraction StartAbsoluteValue upper D Subscript k Baseline left parenthesis alpha right parenthesis union upper D Subscript k Baseline left parenthesis beta right parenthesis EndAbsoluteValue minus StartAbsoluteValue upper D Subscript k Baseline left parenthesis alpha right parenthesis intersection upper D Subscript k Baseline left parenthesis beta right parenthesis EndAbsoluteValue Over StartAbsoluteValue upper D Subscript k Baseline left parenthesis alpha right parenthesis union upper D Subscript k Baseline left parenthesis beta right parenthesis EndAbsoluteValue EndFraction period|Dk(α) ∪Dk(β)| −|Dk(α) ∩Dk(β)|
|Dk(α) ∪Dk(β)|
.
Another way of considering similarity is given by attribute vectors. An attribute 
vector is a sequence of real values giving a measure to any attribute of a list of nn
attributes. This means that any stringalphaα is associated with a real vector ofupper V left parenthesis alpha right parenthesis element of double struck upper R Superscript nV (α) ∈Rn. 
For example, if we chosenn kk-mers as attributes, the corresponding vector for a string 
consists of the frequencies that the chosen kk-mers have in the string. In this case the 
similarity between two stringsalpha comma betaα, β is given by the cosine of the angle between their 
vectors upper V left parenthesis alpha right parenthesis comma upper V left parenthesis beta right parenthesisV (α), V (β), where less than dot vertical bar dot greater than< ·|· > represents the usual scalar product in double struck upper R Superscript nRn: 
StartFraction less than upper V left parenthesis alpha right parenthesis vertical bar upper V left parenthesis beta right parenthesis greater than Over less than upper V left parenthesis alpha right parenthesis StartAbsoluteValue upper V left parenthesis alpha right parenthesis greater than less than upper V left parenthesis beta right parenthesis EndAbsoluteValue upper V left parenthesis beta right parenthesis greater than EndFraction period
< V (α)|V (β) >
< V (α)|V (α) >< V (β)|V (β) >.
Distance is a classical concept of mathematics, and a similarity index is a sort 
of opposite notion because expresses a vicinity with respect to some parameters of 
interest. If distancesupper DD and similaritiesupper SS are considered to range in theleft bracket 0 comma 1 right bracket[0, 1] interval, 
then upper S equals 1 minus upper DS = 1 −D. 
The generalization of Euclidean distance is the Minkowski distanced Subscript kdk of param-
eter k element of double struck upper Nk ∈N and k greater than 0k > 0 deﬁned between vectors of double struck upper R Superscript nRn: 
d Subscript k Baseline left parenthesis upper X comma upper Y right parenthesis equals RootIndex Superscript k Baseline StartRoot sigma summation Underscript 1 less than or equals i less than or equals n Endscripts StartAbsoluteValue upper X left parenthesis i right parenthesis minus upper Y left parenthesis i right parenthesis EndAbsoluteValue Superscript k Baseline EndRoot perioddk(X, Y) =
k
√∑
1≤i≤n
|X(i) −Y(i)|k.
Fork equals 2k = 2 we obtain the usual Euclidean distance, while fork equals 1k = 1 we obtain the sum 
of distances in the different cartesian dimensions of the space (this distance is also 
called Manhattan distance). 
Minkowski distances can be also normalized in several ways by dividing the terms 
by suitable values, depending on the kind of application (average values of standard 
deviation within some population of vectors). 
Cebychevdistance between two vectorsupper X comma upper YX, Y is the maximum among the absolute 
value of differences StartAbsoluteValue upper X left parenthesis i right parenthesis minus upper Y left parenthesis i right parenthesis EndAbsoluteValue|X(i) −Y(i)|. 
A distance, frequently used between binary sequences, is the Hamming distance, 
which is a special case of Manhattan distance between two boolean vectors (of the 
same length), that is, the number of positions where they have different values: 
h left parenthesis upper X comma upper Y right parenthesis equals sigma summation Underscript 1 less than or equals i less than or equals n Endscripts StartAbsoluteValue upper X left parenthesis i right parenthesis minus upper Y left parenthesis i right parenthesis EndAbsoluteValue periodh(X, Y) =
∑
1≤i≤n
|X(i) −Y(i)|.
Another kind of dissimilarity for genomic applications is obtained by considering 
the dictionary ofkk-mers, for some valuekk. then ifmu 1 left parenthesis w right parenthesis comma mu 2 left parenthesis w right parenthesisμ1(w), μ2(w) are the multiplicities 
of the kk-mer ww, in two genomes double struck upper G 1 comma double struck upper G 2G1, G2, then when we set mu Subscript 1 comma 2 Baseline left parenthesis w right parenthesis equals StartAbsoluteValue mu 1 left parenthesis w right parenthesis minus mu 2 left parenthesis w right parenthesis EndAbsoluteValueμ1,2(w) = |μ1(w) −
μ2(w)| and a dissimilarity measure given by:

102
4
Informational Genomics
sigma summation Underscript w element of upper Gamma Superscript k Baseline Endscripts mu Subscript 1 comma 2 Baseline left parenthesis w right parenthesis period
∑
w∈Γ k
μ1,2(w).
If mu 0 equals m a x left brace mu Subscript 1 comma 2 Baseline left parenthesis w right parenthesis vertical bar w element of upper Gamma Superscript k Baseline right braceμ0 = max{μ1,2(w) | w ∈Γ k}, then the following sum is a normalized way for 
expressing the multiplicity differences of kk-mers between two genomes: 
sigma summation Underscript w element of upper Gamma Superscript k Baseline Endscripts StartFraction mu Subscript 1 comma 2 Baseline left parenthesis w right parenthesis Over mu 0 EndFraction period
∑
w∈Γ k
μ1,2(w)
μ0
.
The same approach can be extended by replacing multiplicities with probabilities 
and by normalizing with the sums of probability differences: 
p Subscript 1 comma 2 Baseline left parenthesis w right parenthesis equals StartAbsoluteValue p 1 left parenthesis w right parenthesis minus p 2 left parenthesis w right parenthesis EndAbsoluteValuep1,2(w) = |p1(w) −p2(w)|
p Subscript asterisk Baseline equals sigma summation Underscript w element of upper Gamma Superscript k Baseline Endscripts p Subscript 1 comma 2 Baseline left parenthesis w right parenthesisp∗=
∑
w∈Γ k
p1,2(w)
sigma summation Underscript w element of upper Gamma Superscript k Baseline Endscripts StartFraction p Subscript 1 comma 2 Baseline left parenthesis w right parenthesis Over p Subscript asterisk Baseline EndFraction period
∑
w∈Γ k
p1,2(w)
p∗
.
The notions of distance, proximity, similarity, dissimilarity, and divergences, 
focusing on different speciﬁc aspects, are of particular importance in the analy-
ses of genomes and have a crucial role in the comparisons of genomes, and in the 
identiﬁcation of common structures and evolutionary or functional differences. 
Let us say that two discrete variables are joint when they assume values that we 
can arrange in two sequences where values in the same position are considered each 
other corresponding. In this case, the covariance between two variables is deﬁned 
by: 
c o v left parenthesis upper X comma upper Y right parenthesis equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis x Subscript i Baseline minus upper E left bracket upper X right bracket right parenthesis left parenthesis y Subscript i Baseline minus upper E left bracket upper Y right bracket right parenthesis equals upper E left bracket left parenthesis upper X minus upper E left bracket upper X right bracket right parenthesis left parenthesis upper Y minus upper E left bracket upper Y right bracket right parenthesis right bracketcov(X, Y) = 1
n
n
∑
i=1
(xi −E[X])(yi −E[Y]) = E[(X −E[X])(Y −E[Y])]
whereupper E left parenthesis upper X right parenthesis comma upper E left parenthesis upper Y right parenthesisE(X), E(Y) are the mean expected values of random variablesupper X comma upper YX, Y. Pearson’s 
correlation is deﬁned by means of covariance by: 
upper P Subscript upper X comma upper Y Baseline equals StartFraction c o v left parenthesis upper X comma upper Y right parenthesis Over sigma Subscript upper X Baseline sigma Subscript upper Y Baseline EndFractionPX,Y = cov(X, Y)
σXσY
where sigma Subscript upper XσX and sigma Subscript upper YσY are the standard deviations of upper X comma upper YX, Y respectively, that is. the root 
square of the corresponding variances sigma Subscript upper X Superscript 2σ 2
X and sigma Subscript upper Y Superscript 2σ 2
Y, that is, by denoting also by mu μ the 
mean value upper E left bracket upper X right bracketE[X]:

4.10 Genome Divergences and Similarities
103
sigma Subscript upper X Superscript 2 Baseline equals upper E left bracket left parenthesis upper X minus upper E left bracket upper X right bracket right parenthesis squared right bracket equals StartRoot StartFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis x Subscript i Baseline minus mu right parenthesis squared Over n EndFraction EndRootσ 2
X = E[(X −E[X])2] =
√∑n
i=1(xi −μ)2
n
Whenupper P Subscript upper X comma upper Y Baseline greater than 0PX,Y > 0 variablesupper X comma upper YX, Y are positively correlated, while whenupper P Subscript upper X comma upper Y Baseline less than 0PX,Y < 0, vari-
ables upper X comma upper YX, Y are negatively correlated, or anti-correlated. Another way of measuring 
the degree of agreement, between two discrete random variables, is the following 
Kendal ratio (after Maurice Kendall, a statistician who introduced it in 1938): 
StartStartFraction upper C left parenthesis upper X comma upper Y right parenthesis minus upper D left parenthesis upper X comma upper Y right parenthesis OverOver StartFraction n left parenthesis n minus 1 right parenthesis Over 2 EndFraction EndEndFractionC(X, Y) −D(X, Y)
n(n−1)
2
where upper C left parenthesis upper X comma upper Y right parenthesisC(X, Y) is the number of values u comma vu, v of the two variables upper X comma upper YX, Y such that 
StartFraction u minus upper E left bracket upper X right bracket Over v minus upper E left bracket upper Y right bracket EndFraction greater than 0 u−E[X]
v−E[Y] > 0, while upper D left parenthesis upper X comma upper Y right parenthesisD(X, Y) is the number of values u comma vu, v such that StartFraction u minus upper E left bracket upper X right bracket Over v minus upper E left bracket upper Y right bracket EndFraction less than 0 u−E[X]
v−E[Y] < 0, and 
StartFraction n left parenthesis n minus 1 right parenthesis Over 2 EndFraction n(n−1)
2
is the number of all possible pairs of values assumed by the two variables. 
Correlation indexes are very often useful in random variables analysis and can 
be deﬁned in many different ways according to the kinds of comparisons more 
appropriate to the contexts of interest. 
Spearman correlation is deﬁned by ordering the values of two variablesupper X comma upper YX, Y and 
by comparing values according to pairs: left parenthesis x 1 comma y 1 right parenthesis comma left parenthesis x 2 comma y 2 right parenthesis comma ellipsis comma left parenthesis x Subscript n Baseline comma y Subscript n Baseline right parenthesis(x1, y1), (x2, y2), . . . , (xn, yn). Then, the 
values of two variables are ordered in ranks. In other words, the values of the ﬁrst 
variable are ordered from the smallest value to the greatest one, and also the second 
variable values are ordered in the same manner (for simplicity it is assumed that 
values do not repeat). The rank of a value is its position in these orderings. The rank 
differences of two corresponding pairs are summed giving a correlation between the 
two variables (possibly normalized with the standard deviations of the rank variables). 
Other versions of this correlation count the numbers of rank inversions between 
contiguous values of the ﬁrst variable and between contiguous pairs of the second 
variable, by comparing these numbers in some way. 
Other indexes, from economics or statistics, could be relevant also for many kinds 
of sequences. In fact, when a real variable provides values related to the individuals 
of a population, it is interesting to analyze how much these values differ or agree, 
on average. The origin of this kind of analysis was concerned, for example, with the 
richness distribution in a society, or with the water abundance in the rivers of the 
world. As the Italian economist Wilfred Pareto showed, the most typical situations 
follow the so-called 20/80 rule: the 20% of the population has 80% of all resources of 
the whole population. If we plot the correspondences between the two percentages, 
that is, for each population percentage the corresponding percentage of corresponding 
resources, then we get a curve and its integral area gives a measure of the fairness 
level of the resource distribution among individuals. In this context, Gini’s coefﬁcient 
(after the Italian statistician Corrado Gini, at beginning of ’900) is deﬁned (for all 
possible pairs) as: 
g equals StartFraction sigma summation Underscript i not equals j Endscripts StartAbsoluteValue x Subscript i Baseline minus x Subscript j Baseline EndAbsoluteValue Over n left parenthesis n minus 1 right parenthesis EndFractiong =
∑
i/= j |xi −x j|
n(n −1)

104
4
Informational Genomics
possibly, by normalizing this value with the average of all values. 
Any notion of information is naturally related to a kind “distance”, in a wide sense. 
Information is uncertainty, that is, a sort of difference w. r. t. the certainty of neces-
sary events, mutual information is a divergence between two particular probability 
distributions, and in general, meaningfulness can be considered a sort of distance 
from randomness. Therefore, this discussion highlights the crucial role of “general” 
notions of distance between probability distributions. The attribute “general” wants 
to stress the possibility that some properties of classical metric distances can be 
relaxed. A necessary property is the positivity of distance and its null value for pairs 
on the diagonal, but symmetry and transitivity are generally too strong. In fact, in 
some non-homogeneous spaces the distance from two points can differ in the two 
verses (the top of a hill and its basis are two positions with asymmetric distances), 
and for the same reason, the distance along two edges of a triangle can be shorter 
that the direct distance between two extremities of one edge. 
A genomic distance directly involving entropy is the following that we call Dis-
tance Entropy. Given two genomes upper G comma upper G primeG, G', of the same length that are segmented in 
mm disjoint and adjacent parts, called segments: upper G 1 comma upper G 2 comma ellipsis upper G Subscript m BaselineG1, G2, . . . Gm and upper G prime 1 comma upper G prime 2 comma ellipsis upper G prime Subscript mG'
1, G'
2, . . . G'
m, 
and given a distance dd between strings, if d Subscript idi is the value of dd between segments 
at position ii, and p Subscript ipi the probability of occurring of the value d Subscript idi (within a preﬁxed 
approximation range), then the Distance Entropy between the two genomes is given 
by: 
upper D upper E left parenthesis upper G comma upper G Superscript prime Baseline right parenthesis equals minus sigma summation Underscript 1 Overscript m Endscripts p Subscript i Baseline log base 10 p Subscript i Baseline periodDE(G, G') = −
m
∑
1
pi lg pi.
A different notion of information strictly based on random variables is Fisher’s 
information. It expresses the role that a parameterthetaθ has in determining the probability 
distribution of a given random variable (depending on thetaθ). 
Technically, the likelihood of a probability distribution p left parenthesis upper X comma theta right parenthesisp(X, θ) is a sort “inverse 
probability”, that is, a function giving the probability of observingupper X equals xX = x for a given 
value of the parameter thetaθ; in terms of conditional probability we can write: 
p left parenthesis upper X comma theta right parenthesis equals p r o b left parenthesis upper X vertical bar theta right parenthesis periodp(X, θ) = prob(X|θ).
Fisher information associated with a random variable upper XX is the variance of the log-
arithmic derivative (w.r.t. thetaθ), also called score, of p left parenthesis upper X comma theta right parenthesisp(X, θ) computed for the true 
parameter theta 0θ0, that is, the value of thetaθ corresponding to the probability distributionp Subscript upper XpX
ofupper XX. The mean score ofp left parenthesis upper X comma theta right parenthesisp(X, θ) attheta 0θ0 is zero. In fact, let us denote it byupper E Subscript theta 0 Baseline p left parenthesis x comma theta right parenthesisEθ0 p(x, θ), 
abbreviated as upper E Subscript thetaEθ: 
upper E Subscript theta Baseline equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline left parenthesis StartFraction partial differential Over partial differential theta EndFraction ln p left parenthesis x comma theta right parenthesis right parenthesis p left parenthesis x comma theta 0 right parenthesis d x equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline StartFraction p prime left parenthesis x comma theta right parenthesis Over p left parenthesis x comma theta right parenthesis EndFraction p left parenthesis x comma theta 0 right parenthesis d xEθ =
+∞
∫
−∞
( ∂
∂θ ln p(x, θ)
)
p(x, θ0)dx =
+∞
∫
−∞
p'(x, θ)
p(x, θ) p(x, θ0)dx

4.11 Lexicographic Ordering
105
whence 
upper E Subscript theta 0 Baseline equals integral Subscript negative normal infinity Superscript plus normal infinity Baseline StartFraction p prime left parenthesis x comma theta 0 right parenthesis Over p left parenthesis x comma theta 0 right parenthesis EndFraction p left parenthesis x comma theta 0 right parenthesis d xEθ0 =
+∞
∫
−∞
p'(x, θ0)
p(x, θ0) p(x, θ0)dx
that is: 
upper E Subscript theta 0 Baseline equals left bracket StartFraction partial differential Over partial differential theta EndFraction integral Subscript negative normal infinity Superscript plus normal infinity Baseline p left parenthesis x comma theta right parenthesis d x right bracket Subscript theta 0 Baseline equals 0 periodEθ0 =
⎡
⎣∂
∂θ
+∞
∫
−∞
p(x, θ)dx
⎤
⎦
θ0
= 0.
Therefore, being upper E Subscript theta 0 Baseline equals 0Eθ0 = 0, the variance of p left parenthesis upper X comma theta right parenthesisp(X, θ) corresponds to: 
upper F left parenthesis p left parenthesis upper X comma theta right parenthesis right parenthesis equals left parenthesis StartFraction partial differential p Over partial differential theta EndFraction ln p left parenthesis upper X comma theta right parenthesis right parenthesis Subscript theta 0 Superscript 2 Baseline periodF(p(X, θ)) =
(∂p
∂θ ln p(X, θ)
)2
θ0
.
We do not give more details on Fisher’s information, but we want to remark that 
in both notions of information, due to Shannon and Fisher respectively, information 
is related to probability, and in both cases, the logarithm is used in the functional 
dependence from probability. 
Finally, an issue related to the signiﬁcance of information is the notion of p-value. 
In a very wide sense, the p-value associated with a datum is the inverse of its infor-
mational relevance, that is a value expressing its level of casualty. In a technical 
sense, this value corresponds to the probability that the datum was generated by 
chance/error in the process of generating a series of observed values. In more pre-
cise terms, given a random variable upper XX, the p-value of x 0 element of ModifyingAbove upper X With caretx0 ∈⌃X is the probability of 
observing a value equal or more extremal than x 0x0, if the observed value was gen-
erated according to a random variable upper ZZ that is assumed as reference for casualty. 
In this formulation “more extremal” can be speciﬁed in many ways, depending on 
the shape of upper ZZ: (left-tail, right-tail, or doubly-tailed). The most natural choice of 
upper ZZ is a normally distributed variable (casual errors distribute normally) with mean 
and standard deviation evaluated from the elements of ModifyingAbove upper X With caret⌃X. But other choices can be 
determined by speciﬁc features of the given variable upper XX. 
A thresholdalphaα (usually 5% or 2%) is associated with a p-valuep left parenthesis x 0 right parenthesisp(x0) forx 0x0, such 
that, ifp left parenthesis x 0 right parenthesis less than alphap(x0) < α, then the value is considered signiﬁcant, otherwisex 0x0 is considered 
as due to chance. 
4.11 Lexicographic Ordering 
Lexicographic ordering is an essential and intrinsic characteristic of strings. Assume 
an ordering less than< over the symbols of the alphabet. This order is extended to all the 
strings over the alphabet by the following conditions. Given two stringsalpha comma betaα, β we set:

106
4
Informational Genomics
Fig. 4.22 A tree of a  
lexicographic order of a 
quaternary alphabet. Any 
path is a string. Ordering is 
left-right and top-down. Two 
levels are full and the ﬁrst 
four strings of the third level 
are given 
(i) alpha less than betaα < β if StartAbsoluteValue alpha EndAbsoluteValue less than StartAbsoluteValue beta EndAbsoluteValue|α| < |β|
(ii) if StartAbsoluteValue alpha EndAbsoluteValue equals StartAbsoluteValue beta EndAbsoluteValue|α| = |β| let ii be the ﬁrst position from the left where alphaα and betaβ differ, then 
alpha less than betaα < β if alpha left bracket i right bracket less than beta left bracket i right bracketα[i] < β[i]. 
It is easy to represent the lexicographic ordering by a tree where the empty string is 
put as a label on the root and a son node is added, from the left to the right, to the root 
for every symbol of the alphabet, which is associated as a label of the node. Then, 
iteratively the same method is applied to every node added at the last level, in such 
a way that every node identiﬁes the string concatenating all the labels of the path 
from the root to the node. In this way, a string alphaα precedes lexicographically another 
string betaβ if the node to which alphaα is associated is at the previous level or on the left of 
the node to which betaβ is associated (Fig. 4.22). 
The general rule of generating strings in the lexicographic order follows directly 
from the structure of the tree given above. First, the symbols of the alphabet are 
generated in their order (consider four symbols) in consecutive orders and periods: 
upper A comma upper B comma upper C comma upper DA, B, C, D (First Order) 
upper A left parenthesis upper A comma upper B comma upper C comma upper D right parenthesis comma upper B left parenthesis upper A comma upper B comma upper C comma upper D right parenthesis comma upper C left parenthesis upper A comma upper B comma upper C comma upper D right parenthesis comma upper D left parenthesis upper A comma upper B comma upper C comma upper D right parenthesis equalsA(A, B, C, D), B(A, B, C, D), C(A, B, C, D), D(A, B, C, D) =
upper A upper A comma upper A upper B comma upper A upper C comma upper A upper D comma upper B upper A comma upper B upper B comma upper B upper C comma upper B upper D comma upper C upper A comma upper C upper B comma upper C upper C comma upper C upper D comma upper D upper A comma upper D upper B comma upper D upper C comma upper D upper DAA, AB, AC, AD, B A, BB, BC, BD, C A, C B, CC, C D, DA, DB, DC,
DD (First Period) 
In general, if: 
alpha comma beta comma ellipsis etaα, β, . . . η
are the string of the period of nn (in their lexicographic order), then: 
upper A alpha comma upper A beta comma ellipsis upper A eta comma upper B alpha comma upper B beta comma ellipsis upper B eta comma upper C alpha comma upper C beta comma ellipsis upper C eta comma upper D alpha comma upper D beta comma ellipsis upper D etaAα, Aβ, . . . Aη, Bα, Bβ, . . . Bη, Cα, Cβ, . . . Cη, Dα, Dβ, . . . Dη
are the strings of period n plus 1n + 1. 
Essentially, this generation schema is that one, based on orders and periods, intro-
duced by the great Greek mathematician Archimedes of Syracuse (213–212 BC) 
[ 10, 21], for generating big numbers. Starting from kk digits upper A 1 comma upper A 2 ellipsis upper A Subscript k BaselineA1, A2 . . . Ak periods 
are generated iteratively. First period has kk orders: 
upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript k BaselineA1, A2, . . . , Ak
upper A 1 upper A 1 comma upper A 1 upper A 2 comma ellipsis comma upper A 1 upper A Subscript k BaselineA1A1, A1 A2, . . . , A1Ak
upper A 2 upper A 1 comma upper A 2 upper A 2 comma ellipsis comma upper A 2 upper A Subscript k BaselineA2 A1, A2 A2, . . . , A2 Ak

4.11 Lexicographic Ordering
107
……… 
upper A Subscript k Baseline upper A 1 comma upper A Subscript k Baseline upper A 2 comma ellipsis comma upper A Subscript k Baseline upper A Subscript k BaselineAk A1, Ak A2, . . . , Ak Ak
In general, from a period upper PP the next period is obtained by producing an order 
for each element x element of upper Px ∈P, in such a way that if x less than yx < y all the elements of the order 
produced by xx precede all the elements of the order produced by yy. 
The interpretation of Archimedes was multiplicative, this means that using deci-
mal symbols the expression375375 is intended to denote5 times 10 Superscript 375 × 1037 (Archimedes deﬁned 
exponential numbers, by using geometric progressions, where the ﬁnal value of an 
order coincides with the ratio of the following order). Whereas, as we are going to 
show, the number expressing the position of375375 in the lexicographic ordering (over 
10 symbols) corresponds to an additive interpretation of digits, analogous to that one 
of the usual decimal system, which, differently from the lexicographic representation, 
includes a digit for denoting zero [ 21– 23]. 
Lexicographic ordering can easily be deﬁned by induction. Let’s assume a ﬁnite 
ordered alphabet (for example the usual letters). Letalpha comma betaα, β be any two strings over the 
alphabet and x comma yx, y any two elements of this alphabet, then: 
lamda less than a less than b less than midline horizontal ellipsis less than zλ < a < b < · · · < z
alpha less than alpha xα < αx
alpha less than beta right double arrow alpha x less than beta yα < β ⇒αx < βy
alpha less than beta right double arrow x alpha less than x beta periodα < β ⇒xα < xβ.
(this deﬁnition is correct and complete). 
Lemma 4.1 (Lexicographic recurrence) Let upper SS be an (ordered) ﬁnite alphabet of kk
symbols where, for anyx element of upper Sx ∈S a numberrho Subscript upper S Baseline left parenthesis x right parenthesisρS(x) is assigned, with1 less than or equals rho Subscript upper S Baseline left parenthesis x right parenthesis less than or equals k1 ≤ρS(x) ≤k, then 
the lexicographic enumeration numberrho Subscript upper S Baseline left parenthesis alpha x right parenthesisρS(αx),alpha element of upper S Superscript asterisk Baseline comma x element of upper Sα ∈S∗, x ∈S satisﬁes the following 
equation: 
rho left parenthesis alpha x right parenthesis equals k rho left parenthesis alpha right parenthesis plus rho left parenthesis x right parenthesis periodρ(αx) = kρ(α) + ρ(x).
(4.1) 
Proof Let us identify upper SS with StartSet 1 comma 2 comma ellipsis k EndSet{1, 2, . . . k}, and let us consider the lexicographic 
enumeration kk-tree having StartSet 1 comma 2 comma ellipsis k EndSet{1, 2, . . . k} ordered (from left) sons of the root, then 
StartSet 11 comma 12 comma ellipsis 1 k EndSet{11, 12, . . . 1k} as ordered sons of 1 and so on for 2 comma 3 comma ellipsis2, 3, . . .. The tree grows by iter-
ating the same procedure at each level, with kk ordered sons StartSet alpha Baseline 1 comma alpha Baseline 2 comma ellipsis alpha k EndSet{α1, α2, . . . αk} for any 
node alphaα. Now, if  beta precedes alphaβ ≺α denotes that betaβ precedes lexicographically alphaα, then it easy to 
realize that (StartAbsoluteValue EndAbsoluteValue| | denotes the cardinality of the set, the string lamdaλ is included): 
rho left parenthesis alpha right parenthesis equals StartAbsoluteValue StartSet beta element of upper S Superscript asterisk Baseline vertical bar beta precedes alpha EndSet EndAbsoluteValueρ(α) = |{β ∈S∗| β ≺α}|
therefore, according to the way lexicographic ordering is deﬁned, beingStartAbsoluteValue upper S EndAbsoluteValue equals k|S| = k, the  
strings preceding alpha xαx consist of two disjoint sets: 
(1) left brace beta y element of upper S Superscript asterisk Baseline vertical bar beta precedes alpha comma y element of upper S right brace union StartSet lamda EndSet{βy ∈S∗| β ≺α, y ∈S} ∪{λ}

108
4
Informational Genomics
(2) left brace alpha y element of upper S Superscript asterisk Baseline vertical bar y element of upper S comma y precedes x right brace{αy ∈S∗| y ∈S, y ≺x}
The ﬁrst set is obtained by the set StartSet beta element of upper S Superscript asterisk Baseline vertical bar beta precedes alpha EndSet{β ∈S∗| β ≺α} of rho left parenthesis alpha right parenthesisρ(α) elements elongated 
with all possible symbols of upper SS. These strings are of course k rho left parenthesis alpha right parenthesiskρ(α), but to them the 
string lamdaλ has to be added, giving in the overall k rho left parenthesis alpha right parenthesis plus 1kρ(α) + 1 elements. The second set 
hasrho left parenthesis x right parenthesis minus 1ρ(x) −1 elements, therefore, being disjoint, the number of strings precedingalpha xαx
are: 
rho left parenthesis alpha x right parenthesis equals k rho left parenthesis alpha right parenthesis plus 1 plus rho left parenthesis x right parenthesis minus 1 periodρ(αx) = kρ(α) + 1 + ρ(x) −1.
(4.2) 
according to the formula asserted by the lemma. 
Proposition 4.21 (The lexicographic Theorem) Letupper SS be an (ordered) ﬁnite alphabet 
ofkk symbols where, for anyx element of upper Sx ∈S a numberrho Subscript upper S Baseline left parenthesis x right parenthesisρS(x) is assigned, with1 less than or equals rho Subscript upper S Baseline left parenthesis x right parenthesis less than or equals k1 ≤ρS(x) ≤k, 
then the lexicographic enumeration number rho Subscript upper S Baseline left parenthesis alpha right parenthesisρS(α) of string alpha element of upper S Superscript asteriskα ∈S∗is given by (for 
empty string lamdaλ, rho left parenthesis lamda right parenthesis equals 0ρ(λ) = 0). 
rho left parenthesis alpha right parenthesis equals sigma summation Underscript i equals 1 Overscript StartAbsoluteValue alpha EndAbsoluteValue Endscripts rho left parenthesis alpha left bracket i right bracket right parenthesis k Superscript StartAbsoluteValue alpha EndAbsoluteValue minus iρ(α) =
|α|
∑
i=1
ρ(α[i])k|α|−i
(4.3) 
Proof By iterating Eq. 4.2 we get Eq. 4.3 asserted by the theorem. Let alpha equals a 1 a 2 a 3 ellipsis a Subscript n Baselineα =
a1a2a3 . . . an: 
rho left parenthesis alpha right parenthesis equals k rho left parenthesis a 1 a 2 a 3 ellipsis a Subscript n minus 1 Baseline right parenthesis plus rho left parenthesis a Subscript n Baseline right parenthesis equalsρ(α) = kρ(a1a2a3 . . . an−1) + ρ(an) =
k left bracket k rho left parenthesis a 1 a 2 a 3 ellipsis a Subscript n minus 2 Baseline right parenthesis plus rho left parenthesis a Subscript n minus 1 Baseline right parenthesis right bracket plus rho left parenthesis a Subscript n Baseline right parenthesis equalsk[kρ(a1a2a3 . . . an−2) + ρ(an−1)] + ρ(an) =
k squared rho left parenthesis a 1 a 2 a 3 ellipsis a Subscript n minus 2 Baseline right parenthesis plus k rho left parenthesis a Subscript n minus 1 Baseline right parenthesis plus rho left parenthesis a Subscript n Baseline right parenthesis equalsk2ρ(a1a2a3 . . . an−2) + kρ(an−1) + ρ(an) =
k squared left bracket k rho left parenthesis a 1 a 2 a 3 ellipsis a Subscript n minus 3 Baseline right parenthesis plus rho left parenthesis a Subscript n minus 2 Baseline right parenthesis right bracket plus k rho left parenthesis a Subscript n minus 1 Baseline right parenthesis right bracket rho left parenthesis a Subscript n Baseline right parenthesis equalsk2[kρ(a1a2a3 . . . an−3) + ρ(an−2)] + kρ(an−1)]ρ(an) =
k cubed rho left parenthesis a 1 a 2 a 3 ellipsis a Subscript n minus 3 Baseline right parenthesis plus k squared rho left parenthesis a Subscript n minus 2 Baseline right parenthesis plus k rho left parenthesis a Subscript n minus 1 Baseline right parenthesis right bracket rho left parenthesis a Subscript n Baseline right parenthesis periodk3ρ(a1a2a3 . . . an−3) + k2ρ(an−2) + kρ(an−1)]ρ(an).
Therefore, going on up to the end, whenrhoρ is applied only to symbolsa 1 comma a 2 comma a 3 comma ellipsis a Subscript n Baselinea1, a2, a3, . . . an, 
we get exactly what the theorem asserts.
∎
Corollary 4.1 Given a number k greater than 0k > 0, any non-null natural number nn, such that 
k Superscript m minus 1 Baseline less than or equals n less than or equals k Superscript mkm−1 ≤n ≤km, is univocally representable by a linear combination of powers k Superscript iki
with all coefﬁcients belonging to StartSet 1 comma 2 comma ellipsis k EndSet{1, 2, . . . k}: 
n equals sigma summation Underscript i equals 1 Overscript m Endscripts c Subscript i Baseline k Superscript m minus i Baseline periodn =
m
∑
i=1
cikm−i.
We remark that the equation above is the same expressing the number repre-
sentation with respect to a base k greater than 1k > 1, with the difference that in the case of usual 
positional representation digit00 is allowed, and for all the coefﬁcientsc Subscript ici the following 
inequalities hold:

4.11 Lexicographic Ordering
109
Table 4.1 Lexicographic sum 
+
1
2
3
4 
1
2
3
4
11 
2
3
4
11
12 
3
4
11
12
13 
4
11
12
13
14 
Table 4.2 Lexicographic difference, where * means decreasing one unit of the leftmost digit 
minus−
1
2
3
4 
1
4*
1
2
3 
2
3*
4*
1
2 
3
2*
3*
4*
1 
4
1*
2*
3*
4* 
Table 4.3 Lexicographic product 
times×
1
2
3
4 
1
1
2
3
4 
2
2
4
12
14 
3
3
12
21
24 
4
4
12
24
34 
Table 4.4 Lexicographic division, where * means one-decreasing of the leftmost digit and subscript 
denotes the remainder 
slash/
1
2
3
4 
1
1
2
3
4 
2
2Subscript 11*
1
1Subscript 11
1Subscript 22
3
1Subscript 22*
2
2Subscript 11*
2Subscript 22* 
4
1Subscript 11*
1Subscript 22*
1Subscript 33*
2 
0 less than or equals c Subscript i Baseline less than k period0 ≤ci < k.
Arithmetic operations for the lexicographic number representation [ 10] are based 
on the operation tables expressed by symbols 1, 2, 3, 4. It is important to remark 
that this number representation is zeroless, but at the same time it is positional [ 24] 
(Tables 4.1, 4.2, 4.3 and 4.4).

110
4
Informational Genomics
4.12 Sufﬁx Arrays 
A problem occurring very often with strings is the search for a given substring, that 
is, answering the question: 
beta element of s u b left parenthesis alpha right parenthesis periodβ ∈sub(α).
Sufﬁx trees were introduced in years 1970, to efﬁciently solve the question above 
and a great number of other related problems. The main intuition behind the notion 
of a sufﬁx tree is very simple. In fact, any substring of a given string is a preﬁx of 
some sufﬁx. Then, given a string ww let us consider the set s u f f left parenthesis w right parenthesissuff(w) of its sufﬁxes and 
aggregate iteratively them by means of common preﬁxes. Sufﬁxes are easily deﬁned 
by induction:s u f f left parenthesis lamda right parenthesis equals StartSet lamda EndSetsuff(λ) = {λ} ands u f f left parenthesis alpha x right parenthesis equals s u f f left parenthesis alpha right parenthesis union StartSet alpha x EndSetsuff(αx) = suff(α) ∪{αx}. Let us consider a rootlamdaλ
with the elements of s u f f left parenthesis w right parenthesissuff(w) labelling edges exiting from the root. If gamma alpha element of s u f f left parenthesis w right parenthesisγ α ∈suff(w)
andgamma beta element of s u f f left parenthesis w right parenthesisγβ ∈suff(w), then we replace the edge with an edge labelled bygammaγ and connected 
to two edges labelled by alphaα and betaβ respectively. 
This rule is applied again when other common preﬁxes are found between two 
strings. It was proven that the sufﬁx tree can be constructed in linear time (with 
respect to the length of ww). However, the main result of the sufﬁx tree data structure 
is that the substring problem can be solved in a time that is linear with respect to the 
length of the substring, independently from the length of the superstring where it is 
searched. The following ﬁgure shows a sufﬁx tree, where an extra symbol $ was used 
in order to have all the sufﬁxes as leaves. From a sufﬁx tree, it is possible to deduce 
the occurrence multiplicity of any substring and the positions where it occurs. We 
do not give further details on the sufﬁx tree, preferring to focus on sufﬁx arrays (Fig. 
4.23). 
Sufﬁx arrays [ 25] are structures derived from the sufﬁx tree. The sufﬁx array of a 
given stringalphaα is based on the sequence of all sufﬁxes of the string ordered according 
to their alphabetic ordering, that is, the lexicographic order where the conditions 
about the string length are removed: a string alphaα precedes a string betaβ when in the ﬁrst 
position j j where they differ alphaα has a symbol preceding the symbol at j j position of 
betaβ. The main intuition of the sufﬁx array is the fact that any substring is a preﬁx of 
some sufﬁx of the string. Hence, a sufﬁx array is expressed by two vectors: 
(i) vector upper S upper A Subscript alphaSAα such that upper S upper A Subscript alpha Baseline left parenthesis i right parenthesisSAα(i) is the position of alphaα where the sufﬁx of alphabetic 
order ii occurs in alphaα; 
Fig. 4.23 The sufﬁx tree of 
BANANA

References
111
Table 4.5 Sufﬁx array of BANANA 
Index
Sufﬁx
Position
LCP 
1
A
6
0 
2
ANA
4
1 
3
ANANA
2
3 
4
BANANA
1
0 
5
NA
5
0 
6
NANA
3
2 
(ii) vector upper L upper C upper P Subscript alphaLC Pα such that upper L upper C upper P Subscript alpha Baseline left parenthesis i right parenthesisLC Pα(i) is the length of the longest common preﬁx 
between sufﬁx upper S upper A Subscript alpha Baseline left bracket 1 comma i right bracketSAα[1, i] and sufﬁx upper S upper A Subscript alpha Baseline left bracket 1 comma i minus 1 right bracketSAα[1, i −1] (upper L upper C upper P Subscript alpha Baseline left bracket 1 comma 1 right bracketLC Pα[1, 1] is set to 00). 
Akk-interval ofupper L upper C upper P Subscript alphaLC Pα is a contiguous rangeleft bracket i comma j right bracket[i, j] of indexes of vectorupper L upper C upper P Subscript alphaLC Pα such 
that: 
upper L upper C upper P Subscript alpha Baseline left bracket i right bracket less than kLC Pα[i] < k
upper L upper C upper P Subscript alpha Baseline left bracket l right bracket greater than or equals k commaLC Pα[l] ≥k , for i less than l less than or equals ji < l ≤j
In other words, kk has to be less than the value of upper L upper C upper P Subscript alphaLC Pα in correspondence to the 
left index of the interval, but cannot be less than upper L upper C upper P Subscript alphaLC Pα on the other indexes of the 
interval. 
Sufﬁxes belonging to the same kk-interval, for some positive integer kk, share the 
same kk-preﬁx, also called the preﬁx of the kk-interval (Table 4.5). 
In the sufﬁx array above interval left bracket 1 comma 3 right bracket[1, 3] is a 1-interval, and interval left bracket 2 comma 3 right bracket[2, 3] is a 3-
interval. Analogously, interval left bracket 5 comma 6 right bracket[5, 6] is a 2-interval. Of course, any kk-interval is a 
hh-interval for any h less than kh < k. What is important is that all the sufﬁxes of a kk-interval 
share their kk-preﬁxes. Therefore if alphaα is the kk-preﬁx of the ﬁrst sufﬁx of an interval, 
then it is the preﬁx of all the sufﬁxes of that interval. 
The computational cost of realization of such structures is linear with respect to 
the length of the string to which are applied. However, when they are constructed the 
substring search becomes logarithmic with respect to the length of the overall string. 
Software IGTools [ 8] is entirely based on a suitable extension of sufﬁx arrays and 
makes possible the efﬁcient computation of many genomic indexes based on very big 
genomic dictionaries that would be intractable without a so powerful data structure. 
References 
1. Manca, V., Scollo, G.: Explaining DNA structure. Theor. Comput. Sci. 894, 152–171 (2021) 
2. Bonnici, V., Manca, V.: An informational test for random ﬁnite strings. Entropy 20(12), 934 
(2018) 
3. Manca, V.: The principles of informational genomics. Theor. Comput. Sci. (2017) 
4. Bonnici, V., Manca, V.: Informational laws of genome structures. Sci. Rep. 6, 28840 (2016). 
http://www.nature.com/articles/srep28840. Updated in February 2023

112
4
Informational Genomics
5. Manca, V.: Information theory in genome analysis. In: Membrane Computing, LNCS 9504, 
pp. 3–18. Springer, Berlin (2016) 
6. Manca, V.: Infogenomics: genomes as information sources. Chapter 21, pp. 317–324. Elsevier, 
Morgan Kauffman (2016) 
7. Bonnici, V., Manca, V.: Recurrence distance distributions in computational genomics. Am. J. 
Bioinform. Comput. Biol. 3, 5–23 (2015) 
8. Bonnici, V., Manca, V.: Infogenomics tools: a computational suite for informational analysis 
of genomes. J. Bioinform. Proteomics Rev. 1, 8–14 (2015) 
9. Manca, V.: On the logic and geometry of bilinear forms. Fundamenta Informaticae 64, 261–273 
(2005) 
10. Manca, V.: On the Lexicographic Representation of Numbers, pp. 1–15. Cornell University 
Library (2015). ArXiv.org 
11. Manca, V.: Infobiotics: Information in Biotic Systems. Springer, Berlin (2013) 
12. Manca, V.: From biopolymer duplication to membrane duplication and beyond. J. Membr. 
Comput. (2019). https://doi.org/10.1007/s41965-019-00018-x 
13. Annunziato, A.: DNA packaging: nucleosomes and chromatin. Nat. Educ. 1(1), 26 (2008) 
14. Lynch, M.: The Origin of Genome Architecture. Sinauer Associate. Inc., Publisher (2007) 
15. Manca, V.: A marvelous accident: the birth of life. J. Proteomics Bioinform. 11, 135–137 
(2018) 
16. Franco, G.: Biomolecular Computing—Combinatorial Algorithms and Laboratory Experi-
ments. Ph.D. Thesis, Dipartimento di Informatica Università di Verona (2006) 
17. Manca, V., Franco, G.: Computing by polymerase chain reaction. Math. Biosci. 211, 282–298 
(2008) 
18. Castellini, A., Franco, G., Manca, V.: A dictionary based informational genome analysis. BMC 
Genomics 13, 485 (2012) 
19. Bonnici, V., Franco, G., Manca, V.: Spectrality in genome informational analysis. Theor. Com-
put. Sci. (2020) 
20. Lombardo, R.: UnconventionalComputations and Genome Representations, Ph.D. Thesis, 
Dipartimento di Informatica Università di Verona (2013) 
21. Manca, V.: Lezioni Archimedee. Quaderni della Biblioteca Alagoniana, Siracusa, N. 6 (2020) 
22. Manca, V.: Il Paradiso di Cantor. La costruzione del linguaggio matematico, Edizioni Nuova 
Cultura (2022) 
23. Manca, V.: Arithmoi. Racconto di numeri e concetti matematici fondamentali (to appear) 
24. Boute, R.T.: Zeroless positional number representation and string ordering. Am. Math. Mon. 
107(5), 437–444 (2000) 
25. Abouelhoda, M.I., Kurtz, S., Ohlebusch, E.: Replacing sufﬁx trees with enhanced sufﬁx arrays. 
J. Discret. Algorithms 2, 53–86 (2004)

Chapter 5 
Information and Randomness 
Probability, Randomness, Information 
We have already seen that information is intrinsically related to probability, analo-
gously randomness is the basis of probability. In fact, we will show as any random 
distribution can be obtained from a purely random sequence of values. In this chapter, 
the mutual relationships between the notions of probability, information, and ran-
domness will be considered in more details, by emphasizing some important aspects 
that directly link information with randomness. 
5.1 
Topics in Probability Theory 
In this section, we will analyze some important aspects of probability theory that 
help to understand subtle points of crucial probabilistic notions. 
Monty Hall’s Problem 
A famous debate about probability is the three doors quiz also known as the Monty 
Hall problem, by the name of the TV conductor in a popular American show. There 
are three doors upper A comma upper BA, B, and upper CC. A treasure is behind only one of them, and you are 
asked to bet on the lucky door. Let’s assume that the player chooses the door upper CC, but  
another chance is given to him/her, and the conductor opens the doorupper BB showing that 
the treasure is not there. The player is free of conﬁrming the preceding choice of upper CC
or change by betting on upper AA. The question is: What is the more rational choice? 
Many famous mathematicians concluded that there is no reason for changing 
(Paul Erdös, the number one of 20th mathematicians, was among them). However, 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_5 
113

114
5
Information and Randomness
by using Bayes’ theorem it can be shown that changing the door is the better choice. 
This fact was even conﬁrmed by computer simulations of the game. 
Proposition 5.1 In the three doors quiz, after that, the player bets on upper CC and the 
conductor opens the door upper BB showing that the treasure is not behind upper BB, the  more  
rational choice of the player is changing the choice betting on upper AA. 
Proof Let denote byupper DD the event: The player choosesupper CC and after that the doorupper BB is 
opened (showing no treasure behind upper BB). 
Let denote by upper A Subscript upper TAT the event: The treasure is behind the door upper AA. 
Let us apply Bayes Rule for computing upper P left parenthesis upper A Subscript upper T Baseline vertical bar upper D right parenthesisP(AT |D): 
upper P left parenthesis upper A Subscript upper T Baseline vertical bar upper D right parenthesis equals upper P left parenthesis upper D vertical bar upper A Subscript upper T Baseline right parenthesis upper P left parenthesis upper A Subscript upper T Baseline right parenthesis divided by upper P left parenthesis upper D right parenthesisP(AT |D) = P(D|AT )P(AT )/P(D)
upper P left parenthesis upper D vertical bar upper A Subscript upper T Baseline right parenthesis equals 1P(D|AT ) = 1, because givenupper A Subscript upper TAT , after the player choice ofupper CC, the conductor can open 
onlyupper BB without stopping the game. Of courseupper P left parenthesis upper A Subscript upper T Baseline right parenthesis equals 1 divided by 3P(AT ) = 1/3. Moreover,upper P left parenthesis upper D right parenthesis equals 1 divided by 2P(D) = 1/2, 
because after the choice of upper CC two alternative cases are possible; if the treasure 
is behind upper AA (with probability 1 divided by 31/3), the conductor can open only upper BB, while if the 
treasure is behind upper CC (with probability 1 divided by 31/3) the conductor can indifferently open upper AA
or upper BB, therefore the probability of opening upper BB is 1 divided by 3 plus 1 divided by 6 equals 1 divided by 21/3 + 1/6 = 1/2. In conclusion, by 
substituting the values of probabilities in the formula above we obtain: 
upper P left parenthesis upper A Subscript upper T Baseline vertical bar upper D right parenthesis equals left parenthesis 1 times 1 divided by 3 right parenthesis divided by 1 divided by 2 equals 2 divided by 3 periodP(AT |D) = (1 × 1/3)/1/2 = 2/3.
Therefore, after player’s choice of upper CC, when the conductor opens upper BB the probability 
upper P left parenthesis upper A Subscript upper T Baseline right parenthesisP(AT ) changes from its previous value of 1 divided by 31/3 to the new value of 2 divided by 32/3.
∎
The solution to the Monty-Hall problem given above tells us something of very 
general about the power of Bayes’ approach to probability. People sustaining that 
the more rational behaviour is maintaining the initial bet support their position by 
observing that the world has not changed by the opening of door B. In fact, this is true. 
But what results from our proof is that even if the world is unchanged, our information 
about the world changes when the door is open (and no treasure is found). We can 
make more evident the increase of information after this event if we generalize the 
quiz to 100 doors. Let’s assume that the player guesses that the treasure is behind the 
second door. Moreover, let us assume that the conductor opens doors 1, 3, 4, 5, … 
69, 71, 72, …100 (all doors apart 2 and 70). In end, all the doors, apart from doors 2 
and 70, are open showing no treasure. Is it more rational to maintain the initial bet, 
or to change by betting on door 70? In this case, is evident that if door 70 is closed, 
while door 2 maintains the initial probability of having the treasure (the conductor 
cannot open it), on the contrary, door 70 cumulates all the information of the events 
opening doors different from 70. 
In other words, information affects probability and changes our knowledge of the 
world. But the knowledge that only our rational investigation of the world can obtain 
does not coincide with reality. No scientiﬁc theory tells us what reality exactly is, 
but can only increase our knowledge about reality. In this sense, Bayes’ theorem

5.1 Topics in Probability Theory
115
intercepts one of the crucial points about the essence of science, by giving new 
evidence that probability and information concern with the scientiﬁc attitude that in 
Latin is expressed by “Modus conjectandi” (“as we judge that the things are”, the 
title of Bernoulli’s treatise was Ars Conjectandi), which distinguishes from Modus 
Essendi (as the things are). We can only construct reasonable (and approximate) 
models about reality, but no deﬁnitive and absolute certainty can be reached by any 
scientiﬁc investigation. For this reason, science, since the 19th century, is based on 
probability and its progress is strongly based on probabilistic models. Moreover, 
these models are not due to temporary states of ignorance, but as modern physics 
tells us, they are intrinsically related to the essence of the scientiﬁc statements about 
reality (see the section Information in Physics of this chapter). 
de Moivre-Laplace Theorem 
Given an urn having white balls with percentage pp (1 minus p1 −p the percentage of black 
balls), the probability p left parenthesis n comma k right parenthesisp(n, k) of having kk withe balls in extracting nn balls is: 
p left parenthesis n comma k right parenthesis equals StartBinomialOrMatrix n Choose k EndBinomialOrMatrix p Superscript k Baseline q Superscript n minus k Baseline periodp(n, k) =
(n
k
)
pkqn−k.
(5.1) 
The mean value of kk, in the extractions of nn balls, is given by n pnp because it is 
the mean of the sum of nn independent boolean random variable upper XX (1 white and 0 
black) each having the mean upper E left parenthesis upper X right parenthesis equals 1 times p plus 0 times left parenthesis 1 minus p right parenthesis equals pE(X) = 1 × p + 0 × (1 −p) = p. Analogously, the 
standard deviation can be evaluated. In fact, the square deviation of any boolean 
random variable upper XX is (q equals 1 minus pq = 1 −p): 
left parenthesis 1 minus p right parenthesis squared p plus left parenthesis 0 minus p right parenthesis squared q equals q squared p plus p squared q equals q p left parenthesis q plus p right parenthesis equals q p(1 −p)2 p + (0 −p)2q = q2 p + p2q = qp(q + p) = qp
therefore the sum ofnn independent variableupper XX has square deviationn p qnpq, and standard 
deviation StartRoot n p q EndRoot√npq. 
Let us denote by a Subscript n comma kan,k the probability of having n plus kn + k white balls when 2 n2n balls 
are extracted in an urn containing the same number of white and black balls. In the 
following, we make explicit in asymptotically equals≃the variable with respect to which the asymptotic 
equivalence holds by writing asymptotically equals Subscript n Baseline≃n. 
Proposition 5.2 (De Moivre-Laplace Theorem) a Subscript n comma k Baseline asymptotically equals Subscript n Baseline StartFraction 1 Over StartRoot pi n EndRoot EndFraction e Superscript minus k squared divided by n Baseline periodan,k ≃n
1
√πn e−k2/n.
Proof According to Bernoulli’s binomial distribution: 
a Subscript n comma k Baseline equals left parenthesis 2 n right parenthesis Subscript left parenthesis n plus k right parenthesis factorial Baseline divided by left parenthesis n plus k right parenthesis factorial 2 Superscript minus 2 nan,k = (2n)(n+k)!/(n + k)!2−2n

116
5
Information and Randomness
which can be rewritten as: 
a Subscript n comma k Baseline equals left bracket left parenthesis 2 n right parenthesis Subscript n factorial Baseline left parenthesis n right parenthesis Subscript k factorial Baseline divided by left parenthesis n plus k right parenthesis factorial right bracket 2 Superscript minus 2 n Baseline equals 2 Superscript minus 2 n Baseline StartFraction left parenthesis 2 n right parenthesis factorial Over n factorial n factorial EndFraction StartFraction n factorial left parenthesis n right parenthesis Subscript k factorial Baseline Over left parenthesis n plus k right parenthesis factorial EndFraction periodan,k = [(2n)n!(n)k!/(n + k)!] 2−2n = 2−2n (2n)!
n!n!
n!(n)k!
(n + k)!.
therefore a Subscript n comma kan,k is the product of two terms; a ﬁrst term: 
2 Superscript minus 2 n Baseline StartFraction left parenthesis 2 n right parenthesis factorial Over n factorial n factorial EndFraction left parenthesis upper I right parenthesis2−2n (2n)!
n!n!
(I)
and a second term: 
StartFraction left parenthesis n right parenthesis Subscript k factorial Baseline n factorial Over left parenthesis n plus k right parenthesis factorial EndFraction left parenthesis upper I upper I right parenthesis period (n)k!n!
(n + k)!
(I I).
Using Stirling approximation: 
n factorial asymptotically equals Subscript n Baseline StartFraction n Superscript n Baseline Over e Superscript n Baseline EndFraction StartRoot 2 pi n EndRootn! ≃n nn
en
√
2πn
for the term left parenthesis upper I right parenthesis(I) we have: 
2 Superscript minus 2 n Baseline StartFraction left parenthesis 2 n right parenthesis factorial Over n factorial n factorial EndFraction asymptotically equals 2 Superscript minus 2 n Baseline StartFraction left parenthesis 2 n right parenthesis Superscript 2 n Baseline Over e Superscript 2 n Baseline EndFraction StartFraction e Superscript n Baseline Over n Superscript n Baseline EndFraction StartFraction e Superscript n Baseline Over n Superscript n Baseline EndFraction StartFraction StartRoot 2 pi Baseline 2 n EndRoot Over StartRoot 2 pi n EndRoot StartRoot 2 pi n EndRoot EndFraction2−2n (2n)!
n!n! ≃2−2n (2n)2n
e2n
en
nn en
nn
√
2π2n
√
2πn
√
2πn
asymptotically equals 2 Superscript minus 2 n Baseline StartFraction 2 Superscript 2 n Baseline n Superscript 2 n Baseline Over e Superscript 2 n Baseline EndFraction StartFraction e Superscript n Baseline Over n Superscript n Baseline EndFraction StartFraction e Superscript n Baseline Over n Superscript n Baseline EndFraction StartFraction StartRoot 2 pi Baseline 2 n EndRoot Over StartRoot 2 pi n EndRoot StartRoot 2 pi n EndRoot EndFraction asymptotically equals StartFraction StartRoot 2 pi n EndRoot StartRoot 2 EndRoot Over StartRoot 2 pi n EndRoot StartRoot 2 pi n EndRoot EndFraction asymptotically equals StartFraction 1 Over StartRoot pi n EndRoot EndFraction period ≃2−2n 22n n2n
e2n
en
nn en
nn
√
2π2n
√
2πn
√
2πn ≃
√
2πn
√
2
√
2πn
√
2πn ≃
1
√πn .
Let us rewrite the second term (II): 
StartFraction left parenthesis n right parenthesis Subscript k factorial Baseline n factorial Over left parenthesis n plus k right parenthesis factorial EndFraction equals StartFraction n Over n plus 1 EndFraction StartFraction n minus 1 Over n plus 2 EndFraction StartFraction n minus 2 Over n plus 3 EndFraction ellipsis StartFraction n minus k plus 1 Over n plus k EndFraction (n)k!n!
(n + k)! =
n
n + 1
n −1
n + 2
n −2
n + 3 . . . n −k + 1
n + k
which can again be rewritten as: 
left parenthesis 1 minus StartFraction 1 Over n plus 1 EndFraction right parenthesis left parenthesis 1 minus StartFraction 3 Over n plus 2 EndFraction right parenthesis left parenthesis 1 minus StartFraction 5 Over n plus 3 EndFraction right parenthesis period period period equals
(
1 −
1
n + 1
)(
1 −
3
n + 2
)(
1 −
5
n + 3
)
... =
left parenthesis 1 minus StartFraction 1 Over n plus 1 EndFraction right parenthesis Superscript StartFraction n plus 1 Over n plus 1 EndFraction Baseline left parenthesis 1 minus StartFraction 3 Over n plus 2 EndFraction right parenthesis Superscript StartFraction n plus 2 Over n plus 2 EndFraction Baseline left parenthesis 1 minus StartFraction 5 Over n plus 3 EndFraction right parenthesis Superscript StartFraction n plus 3 Over n plus 3 EndFraction Baseline ellipsis asymptotically equals Subscript n Baseline
(
1 −
1
n + 1
) n+1
n+1 (
1 −
3
n + 2
) n+2
n+2 (
1 −
5
n + 3
) n+3
n+3
. . . ≃n
e Superscript minus StartFraction 1 Over n plus 1 EndFraction Baseline e Superscript minus StartFraction 3 Over n plus 2 EndFraction Baseline e Superscript minus StartFraction 5 Over n plus 3 EndFraction Baseline ellipsis asymptotically equals Subscript n Baselinee−
1
n+1 e−
3
n+2 e−
5
n+3 . . . ≃n
e Superscript minus left parenthesis 1 plus 3 plus 5 period period period right parenthesis divided by n Baseline asymptotically equals Subscript n Baselinee−(1+3+5...)/n ≃n

5.1 Topics in Probability Theory
117
e Superscript minus k squared divided by n Baseline periode−k2/n.
In conclusion we have: 
a Subscript n comma k Baseline asymptotically equals Subscript n Baseline StartFraction 1 Over StartRoot pi n EndRoot EndFraction e Superscript minus k squared divided by nan,k ≃n
1
√πn e−k2/n
(5.2)
∎
Now, let us consider the following probability distribution (the integral of e Superscript minus one half StartFraction x Over sigma EndFraction squarede−1
2
x
σ
2
over the whole real line is well-known to be sigma StartRoot 2 pi EndRootσ
√
2π): 
StartFraction 1 Over sigma StartRoot 2 pi EndRoot EndFraction e Superscript minus one half left parenthesis StartFraction x Over sigma EndFraction right parenthesis squared
1
σ
√
2π e−1
2 ( x
σ )2
and replace xx and sigmaσ with the deviation from the mean muμ and with the standard 
deviation, respectively, of the Bernoulli variable considered above, which has mean 
nn (nn white balls and nn black balls), deviation from the mean n plus k minus n equals kn + k −n = k, and 
standard deviation StartRoot 2 n divided by 4 EndRoot equals StartRoot n divided by 2 EndRoot√2n/4 = √n/2: 
StartFraction 1 Over StartRoot 2 pi n divided by 2 EndRoot EndFraction e Superscript minus one half left parenthesis StartFraction k Over StartRoot n divided by 2 EndRoot EndFraction right parenthesis squared Baseline equals StartFraction 1 Over StartRoot pi n EndRoot EndFraction e Superscript minus k squared divided by n
1
√2πn/2e
−1
2 (
k
√n/2 )2
=
1
√πn e−k2/n
which coincides with the equation of the De Moivre-Laplace theorem. This means 
that this theorem is a particular case of a more general distribution, as we will show 
in the next section. 
Normal, Poisson and Exponential Distributions 
De Moivre-Laplace theorem can be extended to the case of a probability p left parenthesis n comma k right parenthesisp(n, k) of 
kk successes innn extractions withpp andqq the percentage of white and black balls, by 
giving the following equation: 
p left parenthesis n comma k right parenthesis equals StartFraction 1 Over StartRoot n p q EndRoot StartRoot 2 pi EndRoot EndFraction e Superscript minus left parenthesis one half left bracket StartFraction left parenthesis k minus n p right parenthesis Over StartRoot n p q EndRoot EndFraction right bracket squared right parenthesisp(n, k) =
1
√npq
√
2π
e
−
(
1
2
[
(k−np)
√npq
]2)
wheren pnp is the mean value andStartRoot n p q EndRoot√npq the mean standard deviation. If these two values 
are replaced by muμ and sigmaσ, respectively, we obtain the general Normal or Gaussian 
distribution n Subscript mu comma sigmanμ,σ of mean muμ and standard deviation sigmaσ: 
n Subscript mu comma sigma Baseline left parenthesis x right parenthesis equals StartFraction 1 Over sigma StartRoot 2 pi EndRoot EndFraction e Superscript minus left parenthesis one half left bracket StartFraction left parenthesis x minus mu right parenthesis Over sigma EndFraction right bracket squared right parenthesis Baseline periodnμ,σ(x) =
1
σ
√
2π
e
−
(
1
2
[
(x−μ)
σ
]2)
.

118
5
Information and Randomness
This law can be found in many random phenomena, going from the distribution 
of the height in a population of individuals to the distribution of the errors in the 
measures of a physical quantity. 
Poisson law can be derived from Bernoulli’s law under suitable hypotheses, 
typical of rare events: a very big number nn of possible events and a probability 
pp very small of having kk successful events. In this case, if we call lamdaλ the product 
n pnp, the probability is derived with the following reasoning. Let us replacepp byStartFraction lamda Over n EndFraction λ
n in 
Bernoulli formula of urn extraction, then the probability upper P Subscript n comma kPn,k of kk successes (white 
ball) in nn extractions is: 
upper P Subscript n comma k Baseline equals StartBinomialOrMatrix n Choose k EndBinomialOrMatrix left parenthesis StartFraction lamda Over n EndFraction right parenthesis Superscript k Baseline left parenthesis 1 minus StartFraction lamda Over n EndFraction right parenthesis Superscript n minus kPn,k =
(n
k
)(λ
n
)k(
1 −λ
n
)n−k
(5.3) 
upper P Subscript n comma k Baseline asymptotically equals StartBinomialOrMatrix n Choose k EndBinomialOrMatrix left parenthesis StartFraction lamda Over n EndFraction right parenthesis Superscript k Baseline e Superscript negative lamdaPn,k ≃
(n
k
)(λ
n
)k
e−λ
(5.4) 
upper P Subscript n comma k Baseline asymptotically equals StartFraction n Subscript k factorial Baseline Over k factorial EndFraction StartFraction lamda Superscript k Baseline Over n Superscript k Baseline EndFraction e Superscript negative lamdaPn,k ≃nk!
k!
λk
nk e−λ
(5.5) 
upper P Subscript n comma k Baseline asymptotically equals StartFraction n Subscript k factorial Baseline Over n Superscript k Baseline EndFraction StartFraction lamda Superscript k Baseline Over k factorial EndFraction e Superscript negative lamdaPn,k ≃nk!
nk
λk
k! e−λ
(5.6) 
upper P Subscript n period k Baseline asymptotically equals StartFraction lamda Superscript k Baseline Over k factorial EndFraction e Superscript negative lamdaPn.k ≃λk
k! e−λ
(5.7) 
The right member of Eq. (5.7) is called Poisson distribution, after the mathematician 
who introduced it. 
Usually, a time real variablett is introduced in such a way thatlamda equals n pλ = np is the mean 
of successes in the unit time, whereaslamda tλt is the mean of successes in the time interval 
tt, thus the Poisson distribution upper P Subscript lamda Baseline left parenthesis k right parenthesisPλ(k) (of parameter lamdaλ) is set by: 
upper P Subscript lamda Baseline left parenthesis k right parenthesis equals e Superscript minus lamda t Baseline StartFraction left parenthesis lamda t right parenthesis Superscript k Baseline Over k factorial EndFractionPλ(k) = e−λt (λt)k
k!
(5.8) 
If we set k equals 0k = 0 in the Poisson distribution (5.8) we obtain: 
upper P Subscript lamda Baseline left parenthesis 0 right parenthesis equals e Superscript minus lamda tPλ(0) = e−λt
(5.9) 
therefore 1 minus upper P Subscript lamda Baseline left parenthesis 0 right parenthesis1 −Pλ(0) is the probability of a success at end of the tt interval, that 
is, tt is the waiting time for the ﬁrst success. However, this is a continuous distribu-
tion (tt is real), thus we consider the density distribution of the waiting time in a 
process following Poisson distribution, by taking the derivative of 1 minus upper P Subscript lamda Baseline left parenthesis 0 right parenthesis equals 1 minus e Superscript minus lamda t1 −Pλ(0) =
1 −e−λt. In conclusion, this waiting time is given by the exponential distribution 
(of parameter lamdaλ):

5.1 Topics in Probability Theory
119
upper E Subscript lamda Baseline left parenthesis t right parenthesis equals lamda e Superscript minus lamda t Baseline periodEλ(t) = λe−λt.
(5.10) 
The geometric distribution of probability pp gives the probability of a success 
extraction exactly at step kk. It is the discrete version of exponential: 
left parenthesis 1 minus p right parenthesis Superscript k minus 1 Baseline p period(1 −p)k−1 p.
The hypergeometric distribution refers to urn extractions of balls without inserting 
the extracted balls. What is the probability of havingkk successes innn extractions? If 
upper NN is the total number of balls, andmm the number of white balls (success balls), then 
the probability is given by: 
StartBinomialOrMatrix m Choose k EndBinomialOrMatrix StartBinomialOrMatrix upper N minus m Choose n minus k EndBinomialOrMatrix divided by StartBinomialOrMatrix upper N Choose n EndBinomialOrMatrix
(m
k
)(N −m
n −k
)
/
(N
n
)
where the denominator counts all the possible ways of extracting nn balls, while the 
numerator counts the ways of choosing kk white balls among the all mm white balls 
multiplied by the number of ways of choosing the remaining n minus kn −k balls among all 
the remaining upper N minus mN −m black balls. 
The negative binomial, also called Pascal’s distribution, gives the probability of 
waiting for nn extractions for having kk successes. Let pp be the probability of success 
and qq the probability of failure. Of course, when the kkth success occurs exactly at 
extractionnn we hadn minus 1n −1 extractions withk minus 1k −1 successes, which have a probability 
StartBinomialOrMatrix n minus 1 Choose k EndBinomialOrMatrix p Superscript k minus 1 Baseline q Superscript n minus k
(n−1
k
)
pk−1qn−k. If we multiply this value by the probability of having success with 
the following extraction we obtain: 
StartBinomialOrMatrix n minus 1 Choose k EndBinomialOrMatrix p Superscript k Baseline q Superscript n minus k Baseline period
(n −1
k
)
pkqn−k.
The probability that after kk extraction other m greater than or equals 0m ≥0 extractions need for kk successes 
results, with easy calculations, to be (this is the reason for the term “negative bino-
mial”): 
StartBinomialOrMatrix negative k Choose m EndBinomialOrMatrix p Superscript k Baseline left parenthesis negative q right parenthesis Superscript m Baseline period
(−k
m
)
pk(−q)m.
Multinomial distribution refers to the probability of extractingnn balls ofkk types 
wheren Subscript ini are of typeii andp Subscript ipi are the corresponding probabilities fori equals 1 comma 2 comma ellipsis comma ki = 1, 2, . . . , k: 
StartBinomialOrMatrix n Choose n 1 comma n 2 comma ellipsis comma n Subscript k Baseline EndBinomialOrMatrix p Superscript n 1 Baseline p Superscript n 2 Baseline ellipsis p Superscript n Super Subscript k Superscript Baseline period
(
n
n1, n2, . . . , nk
)
pn1 pn2 . . . pnk.

120
5
Information and Randomness
Laws of large numbers 
The ﬁrst law of large numbers was found by Jakob Bernoulli and can be informally 
expressed by saying that when an event with probability pp occurs within a set of 
nn events, then the number mm of times it occurs tends to p npn as nn increases, or also 
p asymptotically equals m divided by np ≃m/n. Bernoulli was so impressed by this result that he named it “Theorema 
Aureus”. A deep meaning of this asymptotic behaviour is the intrinsic link between 
time and probability. In time, events occur with a multiplicity corresponding to their 
probabilities. In other words, when we know their probabilities, then we can reason-
ably predict their occurrence, and vice versa, their probabilities can be reasonably 
inferred from their occurrences in time. In the following, we present a stronger form 
of Bernoulli’s golden theorem, which is due to Cantelli (further generalized by Emil 
Borel in the context of Measure Theory). 
The following Lemma puts in evidence the main property used in the proof of 
strong law of large numbers. 
Lemma 5.1 (Cantelli 1917) Let left parenthesis upper E Subscript k Baseline vertical bar k element of double struck upper N right parenthesis(Ek|k ∈N) be a sequence of events such that 
upper P left parenthesis upper E Subscript k Baseline right parenthesis equals a Subscript kP(Ek) = ak andsigma summation Underscript k equals 0 Overscript normal infinity Endscripts a Subscript k element of double struck upper R∑∞
k=0 ak ∈R, then with probability 1 only a ﬁnite number of events 
upper E Subscript kEk can occur. 
Proof From the hypothesis sigma summation Underscript k equals 0 Overscript normal infinity Endscripts element of double struck upper R∑∞
k=0 ∈R we have that for all epsilon greater than 0∀ε > 0 there exists an n greater than 0n > 0
such that; 
sigma summation Underscript k greater than n Overscript normal infinity Endscripts a Subscript k Baseline less than epsilon period
∞
∑
k>n
ak < ε.
If we assume that inﬁnite events upper E Subscript kEk can occur with a probability delta greater than 0δ > 0, then, by 
the equation above, such a deltaδ has to be less than any real value, therefore deltaδ cannot 
be different from 00. Whence, with probability 1 only a ﬁnite number of events can 
occur.
∎
From the previous lemma, the following proposition follows. 
Proposition 5.3 (Strong Law of Large Numbers) Given a Bernoulli boolean 
sequence where a success 1 occurs with probability pp, a failure 0 with probabil-
ityqq, andupper S Subscript nSn the sum of sequence up tonn, then for every positive realepsilonε, the following 
inequality 
StartAbsoluteValue StartFraction upper S Subscript n Baseline Over n EndFraction minus p EndAbsoluteValue greater than epsilon
||||
Sn
n −p
|||| > ε
(5.11) 
with probability 1 holds only a ﬁnite number of times. 
(Feller [ 1])

5.1 Topics in Probability Theory
121
Random walks 
A random walk (in a discrete plane) can be represented as a sequence in StartSet 0 comma 1 EndSet{0, 1}
by encoding by 00 a horizontal step and by 11 a vertical step. An equivalent way of 
representing a random walk is a walkplus or minus 1±1, which is a sequence inStartSet 1 comma negative 1 EndSet{1, −1} (diagonal, 
anti-diagonal instead of horizontal, vertical). In this case, starting from the origin of 
the plane left parenthesis 0 comma 0 right parenthesis(0, 0) the sum of steps without sign gives, at any time, the value of the 
ordinate of the current position, whereas the algebraic sum of steps gives the abscissa 
of the current position. Therefore, if11 steps arepp, whilenegative 1−1 steps areqq, respectively, 
then r equals p minus qr = p −q, and n equals p plus qn = p + q and are the coordinates of the position reached at 
end of steps plus or minus 1±1. Of course, left parenthesis n plus r right parenthesis divided by 2 equals left parenthesis p plus q minus q plus p right parenthesis divided by 2 equals p(n + r)/2 = (p + q −q + p)/2 = p. The number of 
walks plus or minus 1±1 from the origin to left parenthesis n comma r right parenthesis(n,r), with pp steps 11 and qq steps negative 1−1 is given by: 
upper N Subscript n comma r Baseline equals StartBinomialOrMatrix p plus q Choose p EndBinomialOrMatrix equals StartBinomialOrMatrix n Choose left parenthesis n plus r right parenthesis divided by 2 EndBinomialOrMatrix periodNn,r =
(p + q
p
)
=
(
n
(n + r)/2
)
.
(5.12) 
Reﬂexion Principle (André) Let ww be a walk  plus or minus 1±1 from the point upper A equals left parenthesis x comma y right parenthesisA = (x, y) to 
the point upper BB of the discrete plane. Let us assume that this walk touches or crosses 
the abscissa in a point upper C equals left parenthesis c comma 0 right parenthesisC = (c, 0). Then, a w primew' walks plus or minus 1±1 going from the point left parenthesis upper A prime equals left parenthesis x comma negative y right parenthesis(A' =
(x, −y) to upper BB is uniquely associated to ww, where all the steps of w primew' before reaching 
the point upper CC pass along the symmetric points of walkww, whereas from upper CC toupper BB ww and 
w primew' coincide.
∎
The following proposition is a consequence of the Reﬂexion Principle. 
Proposition 5.4 (Bertrand) The numberupper N Subscript n comma rNn,r of walksplus or minus 1±1 fromleft parenthesis 0 comma 0 right parenthesis(0, 0) toleft parenthesis n comma r right parenthesis(n,r) that, 
apart from the ﬁrst position, do not touch or cross the abscissa is StartFraction r Over n EndFraction upper N Subscript n comma r r
n Nn,r. 
The Ballot Problem (Bertrand) Given a ballot between two competitors upper A comma upper BA, B
who at the end cumulatepp andqq votes, respectively (p greater than qp > q). What is the probability 
that the winner has at any time of the ballot a greater number of votes? 
The solution to the problem is obtained by applying Bertrand’s Theorem. 
In fact, any counting of votes is a random walk plus or minus 1±1 with pp steps 11 and qq steps 
negative 1−1. All these walks are upper N Subscript n comma rNn,r (n equals p plus q comma r equals p minus qn = p + q,r = p −q). Moreover, ballots, where the 
winner is always in advantage, correspond to walks plus or minus 1±1 that do not touch or 
cross the abscissa, whence, according to Bertrand’s Theorem, their number is: 
left parenthesis p minus q right parenthesis divided by left parenthesis p plus q right parenthesis upper N Subscript n comma r(p −q)/(p + q)Nn,r. Therefore, the ratio between always advantageous ballots and 
all the possible ballots is: 
left parenthesis p minus q right parenthesis divided by left parenthesis p plus q right parenthesis upper N Subscript n comma r Baseline divided by upper N Subscript n comma r Baseline equals left parenthesis p minus q right parenthesis divided by left parenthesis p plus q right parenthesis period(p −q)/(p + q)Nn,r/Nn,r = (p −q)/(p + q).
which is the probability of having always advantageous ballots.

122
5
Information and Randomness
Random processes and Arcsine Law 
A random process is a family of random variables: 
left brace chi Subscript i Baseline right brace Subscript i element of upper I{χi}i∈I
therefore, random values are generated in correspondence to the valuesii in the setupper II. 
Usually ii corresponds to time (real number), which can be discrete if upper II is restricted 
to sets of natural or integer numbers. 
Therefore, in a random process not only the probability of taking values is con-
sidered, but the probability of taking values at a given time. A random walk is a 
random process, and a process of Bernoulli extractions is a random process too. In 
both cases, at each step, a value of a Boolean random variable is generated. In this 
sense, a genome can be seen as a random process at several levels, the level of single 
letters generation, or the levels of string generation (for all possible segmentations 
of the genome). 
Given a random process, we can deﬁne many random variables associated with 
the process, that depends on the random variables underlying the process. However, 
even when the process is purely random, as the deMoivre-Laplace theorem shows, 
nevertheless such derived random variables are not purely random but follow speciﬁc 
probability distributions, such as the normal, Poisson, or exponential distributions. 
In this sense, these distributions are often called laws of chance. Let us give some 
examples based on a process of Bernoulli extractions. 
1. the probability that (exactly/at least/at most) kk successes occur in nn steps; 
2. the probability that (exactly/at least/at most) kk failures occur between two 
successes; 
3. the probability of (exactly/at least/at most)kk advantages of one type are cumulated 
in nn steps; 
4. the probability that the ﬁrst parity (abscissa touching) occurs at step 2 n2n; 
5. the probability that the last parity (abscissa touching) occurs at step 2 n2n; 
6. the probability of (exactly/at least/at most)kk consecutive successes/failures (runs) 
occur in 2 n2n step; 
7. the probability that (exactly/at least/at most) kk successes occur when exactly nn
failures occur (Pascal or negative binomial distribution). 
Let us consider a particular case, that is, a process of Bernoulli extractions with 
success probability 1/2. In2 n2n extractions, for the law of large numbers, when all2 n2n
extractions are concluded, the probability of having a parity, withnn successes andnn
failures, is surely very close to 1. But, for k less than nk < n what is the probability that the last 
situation of parity happens at extraction 2 k2k? 
The previous question is equivalent to evaluating, in a random walk, the number 
of paths touching the abscissa at step .2k that do not touch again this axis up to step 
2 n2n (included). This number is given by: 
alpha Subscript 2 k comma 2 n Baseline equals u Subscript 2 k Baseline u Subscript 2 n minus 2 kα2k,2n = u2ku2n−2k

5.1 Topics in Probability Theory
123
where, by Stirling approximation: 
u Subscript 2 n Baseline equals StartBinomialOrMatrix 2 n Choose n EndBinomialOrMatrix 2 Superscript minus 2 n Baseline asymptotically equals 1 divided by StartRoot pi n EndRoot periodu2n =
(2n
n
)
2−2n ≃1/√πn.
Therefore, 
alpha Subscript 2 k comma 2 n Baseline equals u Subscript 2 n minus 2 k Baseline equals StartFraction 1 Over StartRoot pi k EndRoot EndFraction StartFraction 1 Over StartRoot pi left parenthesis n minus k right parenthesis EndRoot EndFraction equals StartFraction 1 Over pi StartRoot k left parenthesis n minus k right parenthesis EndRoot EndFraction periodα2k,2n = u2n−2k =
1
√
πk
1
√π(n −k) =
1
π√k(n −k).
Then if we set: 
x Subscript k Baseline equals StartFraction k Over n EndFractionxk = k
n
and 
f left parenthesis x Subscript k Baseline right parenthesis equals StartFraction 1 Over pi StartRoot x Subscript k Baseline left parenthesis 1 minus x Subscript k Baseline right parenthesis EndRoot EndFraction f (xk) =
1
π√xk(1 −xk)
we obtain: 
alpha Subscript 2 k comma 2 n Baseline equals StartFraction f left parenthesis x Subscript k Baseline right parenthesis Over n EndFraction periodα2k,2n = f (xk)
n
.
Now, we recall that the derivative of arc sine left parenthesis x right parenthesisarcsin(x) function is given by: 
arc sine prime left parenthesis x right parenthesis equals StartFraction 1 Over StartRoot 1 minus x squared EndRoot EndFraction arcsin'(x) =
1
√
1 −x2
whence: 
StartFraction 2 Over pi EndFraction arc sine prime left parenthesis StartRoot x EndRoot right parenthesis equals StartFraction 1 Over pi StartRoot x left parenthesis 1 minus x right parenthesis EndRoot EndFraction equals f left parenthesis x right parenthesis 2
π arcsin'(√x) =
1
π√x(1 −x) = f (x)
therefore, for 0 less than x less than 10 < x < 1: 
sigma summation Underscript k equals 1 Overscript n x Endscripts alpha Subscript 2 k comma 2 n Baseline almost equals integral Subscript 0 Superscript x Baseline f left parenthesis x right parenthesis d x equals StartFraction 2 Over pi EndFraction arc sine left parenthesis StartRoot x EndRoot right parenthesis
nx
∑
k=1
α2k,2n ≈
x
∫
0
f (x)dx = 2
π arcsin(√x)
this means thatalpha Subscript 2 k comma 2 nα2k,2n corresponds to the probability densityStartFraction 2 Over pi EndFraction arc sine left parenthesis StartRoot x EndRoot right parenthesis 2
π arcsin(√x) (which is 
a catenary-like curve). 
If you extract a ball (from a Bernoulli urn) every second for one year, you have 
more than.10% probability that before 9 days you get a parity of white and black balls 
and, after that, always one type (white or black) will be the winning colour during 
the following 356 days. Moreover, with a5 percent sign5% probability last parity is reached within 
54 h, and one type will always lead up to the end. 
This situation shows that random symmetries are very often very particular, or 
not symmetric in a trivial way. In this case, the last parity follows a kind of symmetry 
with extremal preferences.

124
5
Information and Randomness
Randomness and Montecarlo methods 
Probability is deeply linked with randomness. As we already argued in [ 2], the lack 
of information is one of the most typical features of any random process. Therefore, 
it is not surprising that informational analysis of purely random strings, or of prob-
abilistic distributed strings, can beneﬁt from informational indexes. The notion of 
randomness appears at the beginning of Probability Theory (Jakob Bernoulli’s trea-
tise“Ars Conjectandi sive Stochastice” can be translated as “The art of conjectures 
and casualty”). 
An important distinction has to be done, in order to understand the nature of 
randomness. As for probability and information, we need to distinguish between a 
priori and a posteriori randomness. The a priori notion of random string of length 
nn over mm symbols is very simple: it is obtained by choosing for any position of the 
string one of the symbols of the alphabet with probability 1 divided by m1/m. 
However, it is not easy to say in a rigorous and general way if a given string 
is random or not. This means that the a posteriori notion of randomness is very 
difﬁcult to be deﬁned in a general and in rigorous mathematical way. Intuitively, any 
string with no apparent rule in the arrangement of its symbols is random, but this 
is a negative deﬁnition. What appears with no rule can really follow some internal 
mechanism perfectly regulated, but in a way that it is almost impossible to discover, 
or if possible, only with large consumption of time and computational resources. 
Whence, a difﬁculty is evident in ﬁnding good deﬁnitions of a posteriori randomness. 
In practice, the randomness of a process, and any string generated by it, is judged 
empirically, by statistical tests [ 3] trying to verify that some parameters follow prob-
ability distributions of typical natural random processes (dice, play cards, urn extrac-
tions, coin tossing, measurement errors). 
We use the expression “purely random” for expressing a process of generation of 
values where at each step values can occur with a uniform probability distribution, 
whereas, in probability theory, random is used for any kind of probability distribution. 
Purely Random sequences are not easy to be generated, and in nature, they can 
be observed in particular cases. A famous example that had an important historical 
role in physics is that of Brownian motions, observed by the botanist Brown at end 
of the 19th century and fully explained by Albert Einstein at beginning of the 20th 
century. 
In physics, many purely random phenomena have been discovered. Randomness 
is intrinsic in the measurements of quantum systems, where physical parameters 
are generated by processes indistinguishable from random walks or Bernoulli pro-
cesses. Therefore, randomness is a fundamental component of natural processes at 
the deepest level of microscopic reality. 
Surprisingly, in the middle of the 20th century, another strong connection between 
randomness and computation was discovered: there exist mathematical functions 
generating, deterministically, sequences of values that have the same appearance as 
purely random processes of non-deterministic casualty (dice, random walks) or of 
physical non-determinism (Brownian motions, quantum measurements). This kind

5.1 Topics in Probability Theory
125
of process was called deterministic chaos. Both the notions of non-deterministic 
casualty and deterministic chaos refer to the lack of knowledge of rules generating 
some processes. For this reason, in practice, it is very difﬁcult to establish if some 
underlying rules exist but is unknown, or if they do not exist. In both two cases, what 
is missing is the information linking an observed process with the causes generating 
it. Therefore, randomness and pseudo-randomness are hardly distinguishable. 
Already at beginning of 1900, the French mathematician Henry Poincare claimed 
that casualty does not express the absence of causes/rules, but only our ignorance of 
them, or better, the impossibility of discriminating among a great number of causes 
where none of them is dominating over the others. 
Deterministic chaos is a subtle phenomenon, because although the cause can be 
also very simple and due to precise deterministic rules, their effect may provide 
dynamics with an apparent lack of any regularity. Surprisingly, even quadratic func-
tions provide deterministic chaos; the simplest one is the discrete map having the 
following recurrent form: 
x Subscript n plus 1 Baseline equals alpha x Subscript n Baseline left parenthesis 1 minus x Subscript n Baseline right parenthesisxn+1 = αxn(1 −xn)
when alphaα has values near 3.83.8 the sequence has a purely random aspect. 
Linear congruential generators are deﬁned by recurrent equations of the fol-
lowing form where: left bracket n right bracket Subscript m[n]m denotes the value of nn modulus mm (the remainder of the 
division n divided by mn/m), alphaα is the multiplier, cc is the increment, x 0x0 is the initial value of the 
sequence, also called the seed of generator. Usually alphaα and cc are relatively prime 
positive integers: 
x Subscript n plus 1 Baseline equals left bracket alpha x Subscript n Baseline plus c right bracket Subscript mxn+1 = [αxn + c]m
m equals 2 Superscript 31 Baseline comma alpha equals 1103515245 comma c equals 12345m = 231, α = 1103515245, c = 12345 are examples of values used in available gen-
erators. A theory has been developed for deﬁning suitable values ensuring conditions 
for “good” pseudo-random linear congruential generators having also efﬁcient com-
putational performances. In these cases generated sequences pass all usual statistical 
tests of randomness. 
A great number of algorithms are available that generate pseudo-casual sequences. 
These sequences have a huge of important applications, especially in cryptography 
and security protocols. Namely, the ignorance of “secret” parameters of pseudo-
casual generators makes them a powerful source of algorithmic methods based on 
randomness. 
The power of purely random dynamics, over a given space, is its ability to visit 
the space in a uniform and unbiased way. This property is mathematically expressed 
as ergodicity, and is typical of purely random, but also of deterministic chaotic pro-
cesses. In simple words, ergodicity means statistical homogeneity, as a coincidence 
between time and probability: the development of a process in time is coherent with 
the probability space of the events where the process is realized. A simple applica-
tion of chaos can help to give an intuition. In fact, for evaluating the size of a closed 
subspace in a given space, we can move along a random walk visiting the space 
and, after a number of steps, we count the number of points inside the subspace; 
the ratio m divided by nm/n between the mm internal points and the total number nn of points is a

126
5
Information and Randomness
good evaluation of the relative size of the subspace. In such evaluation the more the 
dynamics are chaotic, and the longer the trajectory is, the more the evaluation result 
is accurate. 
Montecarlo methods refer to a class of algorithms that use pseudo-casual sequences 
for exploring spaces of events. Here we want only to explain the intuition of a 
method for obtaining a sequence following a given random distribution, starting 
from a purely random sequence. Let us consider the simple case of a ﬁnite number 
of valuesx 1 comma x 2 comma ellipsis comma x Subscript k Baselinex1, x2, . . . , xk that are distributed with probabilitiesp 1 comma p 2 comma ellipsis comma p Subscript k Baselinep1, p2, . . . , pk respec-
tively. Then consider a pseudo-random generator providing values in the interval 
left bracket 0 comma 1 right bracket[0, 1], and partition this interval into kk contiguous subintervals of lengths equal to 
p 1 comma p 2 comma ellipsis comma p Subscript k Baselinep1, p2, . . . , pk. At any step generate a pseudo-random number inleft bracket 0 comma 1 right bracket[0, 1]. If the  value  
falls in the interval j j, then the corresponding value x Subscript jx j is provided as output. In 
this way, the values x 1 comma x 2 comma ellipsis comma x Subscript k Baselinex1, x2, . . . , xk are surely generated according to the distribution 
p 1 comma p 2 comma ellipsis comma p Subscript k Baselinep1, p2, . . . , pk. Of course, this simple idea can be extended and adapted to many 
possible cases, but it explains the importance of purely random processes and their 
centrality in the theory and applications of random variables. 
Given a continuous probability densityf f , then the method of the inverse cumu-
lative distribution can be used, for obtaining a sequence of random values generated 
according tof f , from a purely random sequence. Namely, ifupper FF is the cumulative dis-
tribution associated with f f , then upper FF is a 1-to-1 correspondence of the interval left parenthesis 0 comma 1 right parenthesis(0, 1)
in itself, and the following proposition holds. 
Proposition 5.5 Given a continuous probability densityf f having cumulative prob-
ability distribution upper FF, and a purely random sequence of values left parenthesis x 1 comma x 2 comma ellipsis x Subscript n Baseline right parenthesis(x1, x2, . . . xn) gen-
erated by a uniform random variable upper XX, then the sequence: 
left parenthesis upper F Superscript negative 1 Baseline left parenthesis x 1 right parenthesis comma upper F Superscript negative 1 Baseline left parenthesis x 2 right parenthesis comma ellipsis upper F Superscript negative 1 Baseline left parenthesis x Subscript n Baseline right parenthesis right parenthesis(F−1(x1), F−1(x2), . . . F−1(xn))
is generated according to the probability distribution f f . 
Proof Let us consider the variable upper F Superscript negative 1 Baseline left parenthesis upper X right parenthesisF−1(X). Given an interval left bracket a comma b right bracket[a, b], being upper FF a 
1-to-1 continuous function, the probability that upper F Superscript negative 1 Baseline left parenthesis upper X right parenthesis element of left bracket a comma b right bracketF−1(X) ∈[a, b] is the same of 
upper F left parenthesis upper F Superscript negative 1 Baseline left parenthesis upper X right parenthesis right parenthesis element of left bracket upper F left parenthesis a right parenthesis comma upper F left parenthesis b right parenthesis right bracketF(F−1(X)) ∈[F(a), F(b)], which equals the probability ofupper X element of left bracket upper F left parenthesis a right parenthesis comma upper F left parenthesis b right parenthesis right bracketX ∈[F(a), F(b)]. But  
upper XX is uniform, whence this probability is upper F left parenthesis b right parenthesis minus upper F left parenthesis a right parenthesisF(b) −F(a). In conclusion, the probabil-
ity of upper F Superscript negative 1 Baseline left parenthesis upper X right parenthesis element of left bracket a comma b right bracketF−1(X) ∈[a, b] is upper F left parenthesis b right parenthesis minus upper F left parenthesis a right parenthesisF(b) −F(a), which implies that upper F Superscript negative 1 Baseline left parenthesis upper X right parenthesisF−1(X) is distributed 
according to the cumulative probability density upper FF, that is, according ton the proba-
bility density f f .
∎
Other methods exist, which are very efﬁcient, one of them is the Metropolis-
Hastings algorithm (based on Markov chains, see later on) [ 4]. 
First digit phenomena and Benford’s Law 
In 1881, a paper appeared in the American Journal of Mathematics, the ﬁrst-digit 
phenomenon was reported, according to which logarithmic tables are used with a

5.1 Topics in Probability Theory
127
probability that is not uniform, but seems to follow the empirical probability distri-
bution p Subscript d Baseline equals log Subscript 10 Baseline left parenthesis 1 plus 1 divided by d right parenthesispd = log10(1 + 1/d), where dd is the ﬁrst digit of a decimal logarithm and 
p Subscript dpd is the probability of occurring this number in the context of logarithmic tables 
applications. 
The same kind of observation was conﬁrmed in 1938 by the physicist Frank 
Benford, and the appearance of this strange phenomenon was observed to hap-
pen in other contexts (even with different systems of number representations). In 
1961 Roger Pinkham proved that this phenomenon is due to an interesting property 
of some random variables: their scale invariance. Let us show brieﬂy this fact 
according to [ 5]. 
A random variable upper XX is scale-invariant if the probability of a upper XX value, within a 
given interval, is the same after multiplying the interval by a scale factor: 
upper P left parenthesis y less than upper X less than x right parenthesis equals upper P left parenthesis a y less than upper X less than a x right parenthesisP(y < X < x) = P(ay < X < ax)
in terms of probability density phi left parenthesis x right parenthesisϕ(x), by differentiating we obtain: 
phi left parenthesis x right parenthesis equals a phi left parenthesis a x right parenthesisϕ(x) = aϕ(ax)
therefore: 
phi left parenthesis a x right parenthesis equals StartFraction 1 Over a EndFraction phi left parenthesis x right parenthesis periodϕ(ax) = 1
a ϕ(x).
Let us assume that upper XX is scale-invariant and let upper YY be the random variable upper Y equals log Subscript b Baseline left parenthesis upper X right parenthesisY =
logb(X), then the following proposition holds. 
Proposition 5.6 The logarithm of a scale-invariant random variable has a constant 
probability density function. 
Proof Let y equals log Subscript b Baseline xy = logb x, then by denoting with phi left parenthesis x right parenthesisϕ(x) and phi left parenthesis y right parenthesis equals phi left parenthesis log Subscript b Baseline left parenthesis x right parenthesisϕ(y) = ϕ(logb(x) the prob-
ability density functions of upper XX and upper Y Y, we have:  
phi left parenthesis log Subscript b Baseline x right parenthesis equals phi left parenthesis x right parenthesis times StartFraction d x Over d y EndFraction equals phi left parenthesis x right parenthesis times StartFraction d b Superscript y Baseline Over d y EndFraction equals phi left parenthesis x right parenthesis times b Superscript y Baseline log base 10 b equals x phi left parenthesis x right parenthesis log base 10 bϕ(logb x) = ϕ(x) × dx
dy = ϕ(x) × dby
dy = ϕ(x) × by lg b = xϕ(x) lg b
therefore, by using the scale invariance, we obtain: 
phi left parenthesis log Subscript b Baseline a x right parenthesis equals a x phi left parenthesis a x right parenthesis log base 10 b equals a x StartFraction 1 Over a EndFraction phi left parenthesis x right parenthesis log base 10 b equals x phi left parenthesis x right parenthesis log base 10 b equals phi left parenthesis log Subscript b Baseline x right parenthesisϕ(logb ax) = axϕ(ax) lg b = ax 1
a ϕ(x) lg b = xϕ(x) lg b = ϕ(logb x)
but at the  same  time:  
phi left parenthesis log Subscript b Baseline a x right parenthesis equals phi left parenthesis log Subscript b Baseline x plus log Subscript b Baseline a right parenthesisϕ(logb ax) = ϕ(logb x + logb a)
and equating the right members of the two last equations above: 
phi left parenthesis log Subscript b Baseline x right parenthesis equals phi left parenthesis log Subscript b Baseline x plus log Subscript b Baseline a right parenthesis periodϕ(logb x) = ϕ(logb x + logb a).

128
5
Information and Randomness
Since aa can be any value, the equation above implies that phi left parenthesis log Subscript b Baseline x right parenthesisϕ(logb x) is constant.
∎
Now we prove that Benford’s law is a consequence of the scale invariance, by 
using the proposition above. In fact, the probability of a number x times 10 Superscript kx × 10k of having 
the ﬁrst digit equal tonn, forn element of StartSet 1 comma 2 comma ellipsis comma 9 EndSetn ∈{1, 2, . . . , 9}, isupper P left parenthesis n less than or equals x less than n plus 1 right parenthesisP(n ≤x < n + 1), then according 
to the constancy of probability density, proved by the proposition above, we have: 
upper P left parenthesis n less than or equals x less than or equals n plus 1 right parenthesis equalsP(n ≤x ≤n + 1) =
upper P left parenthesis log Subscript 10 Baseline n less than or equals log Subscript 10 Baseline x less than or equals log Subscript 10 Baseline left parenthesis n plus 1 right parenthesis right parenthesis equalsP(log10 n ≤log10 x ≤log10(n + 1)) =
log Subscript 10 Baseline left parenthesis n plus 1 right parenthesis minus log Subscript 10 Baseline n equals log10(n + 1) −log10 n =
log Subscript 10 Baseline left parenthesis StartFraction n plus 1 Over n EndFraction right parenthesis equals log10
(n + 1
n
)
=
log Subscript 10 Baseline left parenthesis 1 plus StartFraction 1 Over n EndFraction right parenthesis period log10
(
1 + 1
n
)
.
The last equation is just Benford’s law. 
We conclude by observing the distribution of the ﬁrst digit in the powers of 2 
(expressed in decimal notation), where a clear phenomenon of the predominance of 
1 and 2 is evident. The powers of 2 from 0 to 9 are: 
1 comma 2 comma 4 comma 8 comma 16 comma 32 comma 64 comma 128 comma 256 comma 5121, 2, 4, 8, 16, 32, 64, 128, 256, 512
whereas ﬁrst digit 1 occurs 3 times, 2 occurs 2 times, 3, 4, 5, 6, 8 once, and 7 and 9 
do not occur. It is easy to realize that a quasi-periodical behaviour is evident because 
after 1024 the ﬁrst digits follow a similar pattern. Namely, the dominance of digit 1, 
followed by digit 2 is conﬁrmed: 
1024 comma 2048 comma 4096 comma 8192 comma 16384 comma 32768 comma 65536 comma 131072 comma 262144 comma 5242881024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288
and again, the same type of distribution holds in the next group of powers: 
104 ellipsis comma 209 ellipsis comma 419 ellipsis comma 838 ellipsis comma 167 ellipsis comma 335 ellipsis comma 671 ellipsis comma 134 ellipsis comma 268 ellipsis comma 536 ellipsis period104 . . . , 209 . . . , 419 . . . , 838 . . . , 167 . . . , 335 . . . , 671 . . . , 134 . . . , 268 . . . , 536 . . . .
Going on, we realize that at some step 7 occurs as the ﬁrst digit and then the 
power of the next group starts with 111 ellipsis111 . . . and digit 9 appears (instead of 8) as the 
ﬁrst digit. In this case, the dominance of the ﬁrst digit 1 is related to the fact that 
doubling numbers starting with digits 5, 6, 7, 8, and 9 always 1 occurs as the ﬁrst 
digit (59 times 2 equals 118 comma 69 times 2 equals 138 comma 79 backslash time 2 equals 158 comma 89 times 2 equals 178 comma 99 times 2 equals 19859 × 2 = 118, 69 × 2 = 138, 79 = 158, 89 × 2 = 178, 99 × 2 = 198).

5.1 Topics in Probability Theory
129
Markov chains 
Discrete Markov chains, introduced by Andrej Markov (1856–1922) are discrete 
stochastic processes, that is, sequences of states generated by starting from an initial 
state, where at each step a state is generated according to a probability that depends 
exclusively on the state of the current step. If the probability depends only on the 
state of the current step but does not depend on the value of the step, then the chain 
is homogeneous, and if the set of possible states is ﬁnite, then the Markov chain is 
ﬁnite. 
In this section, we consider only homogeneous and ﬁnite Markov chains. In this 
case the probability that state ii is followed by state j j is given by the transition 
probability p Subscript i comma jpi, j and the stochastic process of a Markov chain is completely deﬁned 
by an initial probability distributionleft brace p Subscript i Baseline right brace Subscript i less than or equals n{pi}i≤n giving the probability that the initial state 
is one of thenn possible states and by a stochastic square matrix of non-negative real 
numbers left brace p Subscript i comma j Baseline right brace Subscript i comma j less than or equals n{pi, j}i, j≤n, called transition matrix, giving the transition probabilities for 
each pair left parenthesis i comma j right parenthesis(i, j) of states. 
Another way of describing a transition matrix is by means of a graph where nodes 
are states and edges connecting nodes are the probabilities of transition. 
Markov chains have wide applicability in many ﬁelds. In several cases, they 
revolutionized the methods of solving problems. In terms of Markov chains, many 
important dynamical and probabilistic concepts can be formalized and analyzed. 
We refer to [ 1] for a good introduction to the general theory of Markov chains. 
One of the main properties of some Markov chains is their ergodicity, that is, the 
property of reaching a limiting probability distribution, independently from the 
initial distribution of the chain. In this section, we prove this property, for a class of 
Markov chains, as a consequence of the Perron-Frobenius Theorem. This Theorem 
is a result of linear algebra, obtained at beginning of 1900, with a lot of important 
implications in several ﬁelds. Here we consider its relevance for Markov processes. 
The following is its usual formulation. 
Proposition 5.7 (Perron-Frobenius Theorem) Letupper AA ann times nn × n matrix of real values 
that are positive, that is,upper A equals left parenthesis a Subscript i comma j Baseline greater than 0 right parenthesis Subscript 1 less than or equals i comma j less than or equals nA = (ai, j > 0)1≤i, j≤n, writing shortlyupper A greater than 0A > 0. The following 
conditions hold: 
1. there exists lamda 0 greater than 0 element of double struck upper R∃λ0 > 0 ∈R that is eigenvalue of upper AA, with u 0u0 the corresponding eigenvector: 
lamda 0 u 0 equals upper A u 0 u 0 greater than 0λ0u0 = Au0
u0 > 0; 
2. every eigenvector of upper AA is multiple of u 0u0; 
3. every eigenvalue of upper AA, possibly in the ﬁeld of complex numbers, has an absolute 
value inferior to u 0u0. 
Proof We will prove only the ﬁrst of the above conditions, which is relevant for 
our further discussion. Let lamda 0 equals m a x left brace lamda greater than 0 element of double struck upper R vertical bar lamda u less than or equals upper A u comma u element of double struck upper R Superscript n Baseline right braceλ0 = max{λ > 0 ∈R | λu ≤Au, u ∈Rn}. Surely this 
maximum value exists because the set is closed and bounded (upper A u less than or equals upper M uAu ≤Mu for each 
vectoruu whenupper MM is then times nn × n matrix such thatm a x left parenthesis a Subscript i comma j Baseline right parenthesis Subscript 1 less than or equals i comma j greater than or equals nmax(ai, j)1≤i, j≥n is put in all positions). 
Of course lamda 0 u 0 less than or equals upper A u 0λ0u0 ≤Au0. We will show that lamda 0 u 0 equals upper A u 0λ0u0 = Au0. In fact, let us assume that 
equality does not hold: lamda 0 u 0 less than upper A u 0λ0u0 < Au0, then by the assumption upper A greater than 0A > 0 we obtain:

130
5
Information and Randomness
upper A left parenthesis upper A u 0 minus lamda 0 u 0 right parenthesis greater than 0A(Au0 −λ0u0) > 0
whence by setting y 0 equals upper A u 0y0 = Au0: 
upper A y 0 greater than lamda 0 u 0 periodAy0 > λ0u0.
Therefore for some lamda prime greater than lamda 0λ' > λ0: 
upper A y 0 greater than lamda prime u 0Ay0 > λ'u0
that is an absurd because by deﬁnitionlamda 0λ0 is the maximum value satisfying the inequal-
ity. In conclusionupper A y 0 equals lamda 0 u 0Ay0 = λ0u0. Moreover,upper A greater than 0A > 0 implies that whenupper A u 0 equals lamda 0 u 0 greater than 0Au0 = λ0u0 > 0, 
being lamda 0 greater than 0λ0 > 0, necessarily u 0 greater than 0u0 > 0.
∎
A matrix of real values is left stochastic when the sum of its values over any 
column is equal to one. 
Corollary 5.1 If upper BB is a left stochastic real matrix, then lamda 0 equals 1λ0 = 1. 
Proof Let upper B equals left parenthesis b Subscript 1 comma j Baseline right parenthesis Subscript 1 less than or equals i comma j less than or equals nB = (b1, j)1≤i, j≤n. We have that for an eigenvalue lamda 0λ0: 
sigma summation Underscript j Endscripts b Subscript j comma 1 Baseline u Subscript j Baseline equals lamda 0 u 1∑
j b j,1u j = λ0u1
sigma summation Underscript j Endscripts b Subscript j comma 2 Baseline u Subscript j Baseline equals lamda 0 u 2∑
j b j,2u j = λ0u2
……… 
sigma summation Underscript j Endscripts b Subscript j comma n Baseline u Subscript j Baseline equals lamda 0 u Subscript n∑
j b j,nu j = λ0un
therefore, by summing, for 1 less than or equals j less than or equals n1 ≤j ≤n: 
sigma summation Underscript j Endscripts left parenthesis b Subscript j comma 1 Baseline plus b Subscript j comma 2 Baseline plus ellipsis plus b Subscript j comma n Baseline right parenthesis u Subscript j Baseline equals lamda 0 u Subscript j∑
j(b j,1 + b j,2 + . . . + b j,n)u j = λ0u j
but, from the left stochasticity ofupper BB it followsleft parenthesis b Subscript j comma 1 Baseline plus b Subscript j comma 2 Baseline plus ellipsis plus b Subscript j comma n Baseline right parenthesis equals 1(b j,1 + b j,2 + . . . + b j,n) = 1 therefore 
lamda 0 equals 1λ0 = 1.
∎
A real matrixupper AA is primitive if for somemm the power matrixupper A Superscript mAm has all components 
greater than 0> 0. Therefore, for a primitive stochastic matrix upper BB, for all k greater than mk > m matrices upper B Superscript kBk have 
all components less than 1< 1, because if some value of the upper BB were equal to 11, then all the 
remaining values of the column have to be null, but this contradicts the fact that in 
upper B Superscript nBn all the components are strictly positive. 
Corollary 5.2 Given two stochastic vectorsu comma v element of double struck upper R Superscript nu, v ∈Rn (with components giving sum 
11) and a primitive stochastic matrix upper AA, then for some m greater than or equals 1m ≥1 and for any k greater than mk > m
StartAbsoluteValue upper A Superscript k Baseline left parenthesis u minus v right parenthesis EndAbsoluteValue less than 1|Ak(u −v)| < 1. 
Proof If for both u comma vu, v, u left bracket i right bracket less than 1u[i] < 1 and v left bracket i right bracket less than 1v[i] < 1 for 1 less than or equals i less than or equals n1 ≤i ≤n, then being also u greater than 0u > 0
andv greater than 0v > 0, the asserted inequality surely holds becauseupper AA is stochastic and primitive. 
The same situation holds even if one component of uu or vv equals 11 (at most one 
component can have this value in each vector). In the case that for two values i not equals ji /= j
both equations u left bracket i right bracket equals 1u[i] = 1 and v left bracket j right bracket equals 1v[ j] = 1 hold, then left parenthesis u minus v right parenthesis left bracket k right bracket equals 0(u −v)[k] = 0 for k not equals i comma k not equals jk /= i, k /= j
whereasleft parenthesis u minus v right parenthesis left bracket i right bracket equals 1(u −v)[i] = 1 andleft parenthesis u minus v right parenthesis left bracket j right bracket equals negative 1(u −v)[ j] = −1 or vice versa. In this case, whenupper A Superscript kAk is 
multiplied byleft parenthesis u minus v right parenthesis(u −v), we get as result two columns ofupper A Superscript kAk, that are stochastic vectors 
with all components less than 11, therefore we are in the same situation as the ﬁrst 
case. In conclusion, in all cases the inequality StartAbsoluteValue upper A Superscript k Baseline left parenthesis u minus v right parenthesis EndAbsoluteValue less than 1|Ak(u −v)| < 1 holds.
∎

5.1 Topics in Probability Theory
131
Now let u 0u0 be the eigenvector postulated by the Perron-Frobenius Theorem of a 
primitive stochastic matrix upper B Superscript mBm. According to Corollary 5.1, the eigenvalue of u 0u0 is 
11, whenceupper B u 0 equals u 0Bu0 = u0. Thus, let us consider any stochastic vectorvv different fromu 0u0, 
we claim that: 
limit Underscript m right arrow normal infinity Endscripts upper B Superscript m Baseline left parenthesis v minus u 0 right parenthesis right arrow 0 period lim
m→∞Bm(v −u0) →0.
In fact, 
upper B Superscript m Baseline v equals upper B Superscript m Baseline left parenthesis u 0 plus left parenthesis v minus u 0 right parenthesis right parenthesis equals upper B Superscript m Baseline u 0 plus upper B Superscript m Baseline left parenthesis v minus u 0 right parenthesisBmv = Bm(u0 + (v −u0)) = Bmu0 + Bm(v −u0)
but upper B u 0 equals u 0Bu0 = u0 implies that upper B Superscript m Baseline u 0 equals u 0Bmu0 = u0, while for m right arrow normal infinitym →∞the value upper B Superscript n Baseline left parenthesis v minus u 0 right parenthesisBn(v −u0)
approaches to zero, as mm increases, because all the components of upper B Superscript mBm are less than 1< 1 and, 
from Corollary 5.2, for  some kk and any m greater than km > k also StartAbsoluteValue upper B Superscript n Baseline left parenthesis v minus u 0 right parenthesis EndAbsoluteValue less than 1|Bn(v −u0)| < 1. 
The discussion above has a deep consequence in the theory of Markov chains. In 
fact, the conclusion we obtained above, as a consequence of the Perron-Frobenius 
Theorem, tells us that starting from any stochastic vector, if the stochastic matrix is 
primitive, we reach a limit state probability that does not depend on the initial state 
probability and corresponds to the eigenvector of the matrix. This possibility for 
Markov chains is the basis of an important algorithm due to Metropolis-Hastings [ 4] 
and based on pseudo-random numbers, the MonteCarloMarkovChain algorithm, by 
means of which is possible to evaluate probabilities even when the analytical form 
of density distribution is difﬁcult to express or to integrate analytically. 
Google Page Rank Algorithm 
Here we present an elegant application of the Perron-Frobenius Theorem to the 
Google Page Rank Algorithm [ 6]. Let us consider a Google query. The way the links 
of relevant pages are listed is a crucial aspect of Google’s research engine. Firstly, all 
the web pages containing the words of the query are determined, and then these pages 
are organized as a graph where pages are the nodes and two pages are connected 
with an oriented edge if the ﬁrst page contains an HTML link to the second page. 
Let us include the initial pageupper P 0P0 as a page having an edge toward any of the selected 
pages. Let upper B equals left parenthesis b Subscript i comma j Baseline right parenthesisB = (bi, j) the boolean matrix associated with the graph we constructed, 
and let us denote by upper O left parenthesis i right parenthesisO(i) the number of edges exiting from the edge ii. 
Now from the matrix of the graph, of nodes upper P 0 comma upper P 1 comma upper P 2 comma ellipsis comma upper P Subscript n BaselineP0, P1, P2, . . . , Pn, we deﬁne a 
stochastic matrix upper CC, by ﬁxing a threshold value p less than 1p < 1, usually set as 0.750.75, which 
we call search probability. The components of matrix upper C equals left parenthesis c Subscript i comma j Baseline right parenthesisC = (ci, j) are deﬁned in the 
following way. 
c Subscript 0 comma i Baseline equals 1 divided by nc0,i = 1/n
c Subscript i comma 0 Baseline equals 1ci,0 = 1 if upper O left parenthesis i right parenthesis equals 1O(i) = 1
c Subscript i comma 0 Baseline equals 1 minus pci,0 = 1 −p if upper O left parenthesis i right parenthesis not equals 1O(i) /= 1
This means that from the initial page, we can move with the same probability to any 
of thenn pages, whereas ifupper P Subscript iPi is not connected with no page different fromupper P 0P0, then we

132
5
Information and Randomness
necessarily go to upper P 0P0, and if upper P Subscript iPi is connected with upper O left parenthesis i right parenthesisO(i) pages, we have a probability 
1 minus p1 −p of starting again the search from upper P 0P0. 
Moreover, from each page we can go to any connected page with the same probability, 
the search probability divided by the number of connected pages: 
c Subscript i comma j Baseline equals 0ci, j = 0 if b Subscript i comma j Baseline equals 0bi, j = 0
c Subscript i comma j Baseline equals p divided by upper O left parenthesis i right parenthesisci, j = p/O(i) if b Subscript i comma j Baseline equals 1bi, j = 1. 
The matrix upper CC is stochastic and it is also primitive because two nodes are always 
connected by a path, that is, the probability of going from one node to any other 
node, in some number of steps, is different from zero (possibly passing throughupper P 0P0). 
This means that there existsmm such thatupper C Superscript m Baseline greater than 0Cm > 0. Therefore, we can deduce, according 
to Perron-Frobenius Theorem, that if u 0u0 is the eigenvector of upper CC, then starting from 
any stochastic vectorvv, by applying iterativelyupper CC we approach to the limit vectoru 0u0. 
This vector approximates the probability of reaching a given page along a very long 
path in the graph. For this reason, the Brin-Page algorithm of Page-Rank lists the 
pages connected toupper P 0P0 according to the decreasing order of the probabilities assigned 
by u 0u0. 
Topologies, Measures, Distributions 
The general mathematical setting of probability is based on the mathematical theory 
of measure that was developed since the seminal research of Lebesgue, Borel, and 
other mathematicians, especially of the French school, at beginning of the twentieth 
century. In general, the notion of measure is formulated within a topological setting, 
and probability is a special case of measure. 
A topological space over a set upper SS is given by associating to any element xx of 
upper SS a family upper U left parenthesis x right parenthesisU(x) of non-empty subsets of upper SS that are called neighborhoods of xx. 
These neighbourhoods express the intuition of localization ofxx inupper SS by means of an 
approaching “telescopic” sequence of sets includingxx in an increasingly closer way. 
In fact, it is assumed that upper S element of upper U left parenthesis x right parenthesisS ∈U(x) for every x element of upper Sx ∈S, and if upper V element of upper U left parenthesis x right parenthesisV ∈U(x) then x element of upper Vx ∈V
and exists upper V prime element of upper U left parenthesis x right parenthesisV ' ∈U(x) such that upper V superset of upper V primeV ⊃V '; if both upper V 1 element of upper U left parenthesis x right parenthesisV1 ∈U(x) and upper V 2 element of upper U left parenthesis x right parenthesisV2 ∈U(x) then 
upper V 1 intersection upper V 2 element of upper U left parenthesis x right parenthesisV1 ∩V2 ∈U(x); and ﬁnally, if upper V element of upper U left parenthesis x right parenthesisV ∈U(x) there is upper W element of upper U left parenthesis x right parenthesisW ∈U(x) such that upper V element of upper U left parenthesis y right parenthesisV ∈U(y)
for everyy element of upper Wy ∈W. An  open set of a topology is any set that is a neighbourhood of all 
its elements, while a closed set is complementary of an open set. 
The notion of limit and continuous function can be deﬁned in general topological 
terms without assuming any notion of distance. A functionf f between two topologies 
is continuous when the f f -inverse image of any open set is an open set too. For this 
reason, a topology allows us to deﬁne the classical notions of mathematical analysis in 
more general and abstract terms, by individuating the essence of many crucial spatial 
concepts. We remark that the deﬁnition of continuity by means of inverse images 
is stronger than a possible “direct” deﬁnition. In fact, in inverse terms, continuity 
requires that points that are close together cannot be images of points that are not

5.2 Informational Randomness
133
close together, that is not only “vicinity” is conserved by a continuous function, but 
“non-vicinity” in the domain cannot be transformed into vicinity in the codomain. 
A collection of subsets of a set upper SS that is closed under taking complements and 
ﬁnite unions is called a ﬁeld (or an algebra of sets). A ﬁeld that is closed under taking 
countable unions is called a sigmaσ-ﬁeld. A Borel sigmaσ-ﬁeld is a sigmaσ-ﬁeld generated by the 
open sets of a topology (the smallest sigmaσ-ﬁeld containing all the open sets). 
A measurable space is given byleft parenthesis upper S comma upper A right parenthesis(S, A) whereupper AA is asigmaσ-ﬁeld of subsets of the set 
upper SS including normal empty set∅. 
Given a measurable spaceleft parenthesis upper S comma upper A right parenthesis(S, A), then a measuremuμ on S is a function from A to 
the non-negative real numbers extended (with the inﬁnite value normal infinity∞) that satisﬁes: 
(1) mu left parenthesis normal empty set right parenthesis equals 0μ(∅) = 0, 
(2) the countable union of mutually disjoint measurable sets is the (countable) sum 
of the measures of the sets. 
The triple left parenthesis upper S comma upper A comma mu right parenthesis(S, A, μ) is called a measure space. If left parenthesis upper S comma upper A comma mu right parenthesis(S, A, μ) is a measure space 
and mu left parenthesis upper S right parenthesis equals 1μ(S) = 1, then muμ is called a probability and left parenthesis upper S comma upper A comma mu right parenthesis(S, A, μ) is called a probability 
space. 
Suppose that left parenthesis upper S comma upper A right parenthesis(S, A) is a measurable space, and letleft parenthesis upper T comma upper C right parenthesis(T, C) be another measurable 
space. A measurable function f colon upper S right arrow upper T f : S →T is a function such that for every subset 
upper BB that belongs to upper CC the inverse image f Superscript negative 1 Baseline left parenthesis upper B right parenthesis f −1(B) belongs to upper AA. 
Given a probability space left parenthesis upper S comma upper A comma mu right parenthesis(S, A, μ), any element of upper AA is called an event. A  
measurable function upper XX from upper SS to the set upper CC of some other measurable space left parenthesis upper C comma upper B right parenthesis(C, B)
is called a random quantity. A random quantity upper XX is a random variable when upper CC
is the set double struck upper RR of reals and upper BB the Borel sigmaσ-ﬁeld over double struck upper RR. 
The probability measuremu Subscript upper XμX induced onleft parenthesis upper C comma upper B right parenthesis(C, B) by the random quantityupper XX frommuμ
is called the distribution ofupper XX. In other words, a distribution is a measure determined 
by means of inverse images from a previously assumed measure. However, apart from 
this very general setting, the intuition behind the concept of distribution is that of a 
quantity that is distributed among the individuals of a population. In this sense, it is 
a truly statistical concept. 
5.2 
Informational Randomness 
In a previous section, we deﬁned random strings, as those generated by Bernoulli 
processes with the uniform probability of extracting the different values of a ﬁnite 
variable. This is a good deﬁnition for generating random strings. But when a string 
is given how can we establish if it is random, that is, if it was generated by a sort of 
Bernoulli process or not? 
In this section we will give a very simple rule for answering to the question above. 
Given a string alphaα of length nn over mm symbols, let us consider the shortest length nabla∇
such that all the strings of length nabla∇that occur in alphaα occur only once in it. If 
nabla equals 2 log Subscript m Baseline n∇= 2 logm n

134
5
Information and Randomness
or it is very close to this value, then alphaα is almost surely a string randomly generated 
over its mm symbols. 
The results that we present in this section were developed in relation to genomes 
and random genomes, which are very important for discriminating meaningful bio-
logical aspects from those that have a purely casual nature. However, their relevance 
is very general, therefore we present them in a chapter devoted to general concepts of 
information and probability. Often generic strings will be indicated bydouble struck upper GG, even when 
they are strings over alphabets that are not genome alphabets. The results given in 
this section improve those presented in [ 2, 7, 8]. 
A deep link between information and randomness was discovered within Algo-
rithmic Information Theory [ 9, 10] developed, by Andrej Nikolaeviˇc Kolmogorov 
(1902–1987) and Ray Solomonoff (1926–2009), and by Gregory Chaitin (1947–), in 
a more computer science oriented setting. According to this theory the algorithmic 
information of a string is given by the length of the shortest program able to gener-
ate the string (in a ﬁxed programming language). Then, strings having algorithmic 
information greater than or equal to their lengths are algorithmically incompress-
ible and for this, they are considered casual or random (no efﬁcient deterministic 
rule exists for generating them). 
This perspective is illuminating in many aspects. It sheds new light on the con-
nection between comprehension and compression, which have the same Latin root, 
because the comprehension of a phenomenon is a form of reduction of some data 
observed about the phenomenon to a few principles of a theory within which the 
phenomenon is interpreted. Moreover, in the algorithmic perspective, a clear link is 
established between information, randomness, and computation. 
From a practical viewpoint, the algorithmic notion of casualty cannot be recog-
nized in a complete exact way, because the property of a string of being algorith-
mically casual is not computable. Thus, you can deﬁne the property, but you cannot 
show, in formal terms, that the property holds for a given string. 
This limitative result is of a very general nature. In fact, other mathematical formal-
izations of randomness for ﬁnite, or inﬁnite, strings, outside the probability theory, 
suffer a sort of incompatibility between a rigorous deﬁnition of randomness, based 
on intrinsic properties of strings, and the possibility of verifying these properties in 
a computable way. 
In a paper by Martin Löf [ 11] a sequence is casual when it is generic, that is, 
when all Turing machine tests for checking speciﬁc properties fail. This implies 
that casualty is not a decidable property. In fact, assuming its decidability, then a 
Turing machine would be able to decide if a given sequence is casual, therefore the 
sequence does not fail for a particular computable test, whence the sequence could 
not be casual, thus reaching a contradiction. 
The only way to approach a characterization of randomness can be obtained only 
in probabilistic terms or within some degree of approximation. The probabilistic 
approach is already well-established and it is the basis for the actual statistical tests, 
organized in suitable packages, used in the majority of applications. In the sequel, 
we will explain our informational, approximate criterion of randomness for strings.

5.2 Informational Randomness
135
A randomness index based on hapaxes 
Let us indicate by upper R upper N upper D Subscript m comma nRN Dm,n random strings, of length nn, over an alphabet of mm
symbols, that is, a sequence of length nn generated by a random process where at 
each step one of mm symbol is generated with a probability of 1 divided by m1/m. 
The strings in upper R upper N upper D Subscript m comma nRN Dm,n obey to an Homogeneity Principle very similar to nor-
mality introduced by Emil Borel for positional representations of irrational numbers 
(inﬁnite sequence of digits) [ 12, 13]. According to Borel, an irrational number is 
said to be normal if, for every positive integer nn, all strings of nn digits have the 
same probability of occurring in the representation of the number in a given base. 
It can be shown that any irrational number is normal, and normality is assumed as 
a form of randomness. Analogously, we assume that strings of upper R upper N upper D Subscript m comma nRN Dm,n obey to an 
internal homogeneity, according to which, in any random string, all portions of a 
given length have the same probabilistic properties (properties expressed by means of 
conditions on probabilities). Moreover, strings ofupper R upper N upper D Subscript m comma nRN Dm,n obey also to an external 
homogeneity, according to which all random strings, singularly considered, satisfy 
the same probabilistic properties. 
We recall thatnabla left parenthesis double struck upper G right parenthesis∇(G) is the no-repeat of a the stringdouble struck upper GG. The following proposition 
tells us that if k equals nabla left parenthesis double struck upper G right parenthesisk = ∇(G), then the upper E Subscript kEk-entropy of double struck upper GG is the maximum among the 
upper E Subscript hEh-entropies of double struck upper GG for values grater than kk. 
Proposition 5.8 Given a string double struck upper GG of length nn over mm symbols, if k equals nabla left parenthesis double struck upper G right parenthesisk = ∇(G), then 
2 log base 10 Subscript m Baseline left parenthesis n minus k plus 1 right parenthesis2 lgm(n −k + 1) is the maximum value that upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G) reaches in the class of values 
StartSet upper E Subscript h Baseline left parenthesis double struck upper G right parenthesis vertical bar n greater than or equals h greater than or equals k EndSet{Eh(G)|n ≥h ≥k}. 
Proof Minimum value of kk such that all kk-mers are hapaxes of double struck upper GG is nabla left parenthesis double struck upper G right parenthesis equals m r l left parenthesis double struck upper G right parenthesis plus 1∇(G) =
mrl(G) + 1 (m r lmrl is the maximum length of repeats). Hence, fork equals nabla left parenthesis double struck upper G right parenthesisk = ∇(G) empirical 
entropyupper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G) is equal tolog Subscript m Baseline left parenthesis n minus k plus 1 right parenthesislogm(n −k + 1), in factleft parenthesis n minus k plus 1 right parenthesis(n −k + 1) is the number of dis-
tinct k-mers indouble struck upper GG. The same expression holds for anyh less than or equals nh ≤n andh greater than or equals kh ≥k, because strings 
longer thankk are hapaxes too. But ifh greater than kh > k thenlog Subscript m Baseline left parenthesis n minus k plus 1 right parenthesis greater than log Subscript m Baseline left parenthesis n minus h plus 1 right parenthesislogm(n −k + 1) > logm(n −h + 1), 
therefore upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G) is the maximum of upper E Subscript h Baseline left parenthesis double struck upper G right parenthesisEh(G) in the interval n greater than or equals h greater than or equals kn ≥h ≥k.
∎
In the ﬁrst half of 1900, Nicholas de Brujin (1918–2012) introduced a class of 
graphs generating, for any k greater than 0k > 0, cyclic sequences of length m Superscript k Baseline minus k minus 1mk −k −1 over mm
symbols that are kk-hapax, called de Brujin sequences of order kk, where all possible 
kk-mers occur. Such sequences are cyclic because they are arranged on a circle, where 
thekk-meromegaω at the end of a de Bruijn sequence is followed byk minus 1k −1 otherkk-mers that 
begin at a position ii of omegaω, with 1 less than i less than or equals k1 < i ≤k, and continue at positions 1 comma 2 comma ellipsis comma i minus 11, 2, . . . , i −1
of the sequence. 
A de Bruijn graph has all the possible strings of length kk as nodes, where an 
oriented edge from node alphaα to node betaβ is present if the left parenthesis k minus 1 right parenthesis(k −1)-sufﬁx of alphaα is equal to 
the left parenthesis k minus 1 right parenthesis(k −1)-preﬁx of betaβ. Therefore, each node has mm exiting edges and mm entering 
edges. In this way, starting from a node, any path ofn minus 1n −1 edges connectsn minus k plus 1n −k + 1
nodes that, when concatenated with left parenthesis k minus 1 right parenthesis(k −1) overlap, provide a string of length nn. 
All the nodes of a de Bruijn graph have even degree (2 m2m if strings are over mm
symbols). For this reason, it can be shown that there exists a path passing for all the 
nodes once only once (Hamiltonian path), and also passing once only once through

136
5
Information and Randomness
all edges (Eulerian path). The existence of this path ensures the existence of a de 
Bruijn sequence where all possiblekk-mers occur and where each of them occurs only 
once in the sequence. 
The following proposition generalizes a result obtained for random genomes 
[ 7, 8]. It gives an evaluation of the no-repeat bound in random strings. 
Let us recall thatnabla Subscript m comma n Baseline∇m,n denotes the no-repeat length for any string of lengthnn over 
mm symbols. Namely, according to the homogeneity of random strings, this value has 
to be the same for all the strings of upper R upper N upper D Subscript m comma nRN Dm,n. 
Proposition 5.9 Let nabla Subscript m comma n Baseline∇m,n be the no-repeat length for strings of upper R upper N upper D Subscript m comma nRN Dm,n. The fol-
lowing logarithm randomness condition holds: 
nabla Subscript m comma n Baseline equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor∇m,n = [2 logm(n)]
Proof There is inupper RR a string of lengthnabla Subscript m comma n Baseline∇m,n which is an hapax ofupper RR. Therefore, from 
the homogeneity of random strings, all nabla Subscript m comma n Baseline∇m,n-mers of upper RR are hapaxes of upper RR. In other 
words, any upper R element of upper R upper A upper N upper D Subscript m comma nR ∈RAN Dm,n is a nabla Subscript m comma n Baseline∇m,n-hapax string. 
According to de Bruijn graph theory, we know that we can arrange, for anyk greater than 0k > 0, 
allm Superscript kmk possible k-mers in a circular sequencealphaα (the last symbol ofalphaα is followed by the 
ﬁrst symbol ofalphaα) where eachkk-mer occurs exactly once. Of course, any contiguous 
portion long nn of alphaα contains left parenthesis n minus k plus 1 right parenthesis(n −k + 1) consecutive kk-mers and corresponds to a 
string, which we call ann-portion ofalphaα, where all substrings ofalphaα with lengths less than 
or equal to kk are equi-probable, and this probabilistic homogeneity holds along all 
positions ofalphaα, in the sense that, going forward (circularly) other be Brujin sequences, 
with the same equi-probability property are obtained. 
Now, let alphaα be a de Bruijn sequence arranging all kk-mers where k equals nabla Subscript m comma n Baselinek = ∇m,n. Let us 
consider the disjoint nn-portions (with no common kk-mer) that are concatenated in 
alphaα. Their number rr has to be equal to m Superscript k Baseline divided by left parenthesis n minus k plus 1 right parenthesismk/(n −k + 1), and any nn-portion belongs to 
upper R upper N upper D Subscript m comma nRN Dm,n, given its homogeneous probabilistic structure. But alphaα is a circular string 
arranging allkk-mers, where anykk-mer occurs once inalphaα, therefore, anykk-mer has to 
belong to somenn-portion ofalphaα and only to one of them. Moreover, eachkk-mer which 
belongs to ann-portion occurs only in one of itsleft parenthesis n minus k plus 1 right parenthesis(n −k + 1) positions. Therefore, the 
maximum probability homogeneity is realized when r equals left parenthesis n minus k plus 1 right parenthesisr = (n −k + 1), that is, when 
the numberrr ofnn-portions ofalphaα is equal to the number of positions of anynn-portion. 
In fact, this implies that any kk-mer of mm symbols occurs in some random nn-portion 
ofalphaα with the same probability of occurring in any position of such random portions. 
In conclusion, if k equals nabla Subscript m comma n Baselinek = ∇m,n, then: 
m Superscript k Baseline divided by left parenthesis n minus k plus 1 right parenthesis equals left parenthesis n minus k plus 1 right parenthesismk/(n −k + 1) = (n −k + 1)
that is: 
m Superscript k Baseline equals left parenthesis n minus k plus 1 right parenthesis squaredmk = (n −k + 1)2
giving the following equations and inequalities:

5.2 Informational Randomness
137
k equals 2 log Subscript m Baseline left parenthesis n minus k plus 1 right parenthesisk = 2 logm(n −k + 1)
k equals 2 log Subscript m Baseline left parenthesis n minus 2 log Subscript m Baseline left parenthesis n minus k plus 1 right parenthesis plus 1 right parenthesisk = 2 logm(n −2 logm(n −k + 1) + 1)
2 log Subscript m Baseline left parenthesis n minus 2 log Subscript m Baseline left parenthesis n right parenthesis right parenthesis less than k less than 2 log Subscript m Baseline left parenthesis n right parenthesis2 logm(n −2 logm(n)) < k < 2 logm(n)
2 log Subscript m Baseline left parenthesis n minus StartFraction m minus 1 Over m EndFraction n right parenthesis less than k less than or equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor2 logm(n −m −1
m
n) < k ≤[2 logm(n)]
2 log Subscript m Baseline left parenthesis n divided by m right parenthesis less than k less than or equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor2 logm(n/m) < k ≤[2 logm(n)]
2 left parenthesis log Subscript m Baseline left parenthesis n right parenthesis minus log Subscript m Baseline left parenthesis m right parenthesis right parenthesis less than k less than or equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor2(logm(n) −logm(m)) < k ≤[2 logm(n)]
2 log Subscript m Baseline left parenthesis n minus 1 right parenthesis less than k less than or equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor2 logm(n −1) < k ≤[2 logm(n)]
left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor less than or equals k less than or equals left floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor[2 logm(n)] ≤k ≤[2 logm(n)]
thus, the last inequality implies the equation asserted by the proposition.
∎
The proposition above suggests the following hypothesis. 
Hypothesis of informational randomness The classupper R upper N upper D Subscript n comma dRN Dn,d coincides with the 
class of strings of length nn over mm symbols where nabla Subscript m comma n Baseline equals left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor∇m,n = [2 lgm(n)]. 
Letbetaβ a string of lengthnn overmm symbols that differs only in one position from a 
string alpha upper R upper A upper N upper D Subscript m comma nαRAN Dm,n. Is betaβ a random string or not? And, what about its randomness, if 
it differs from alphaα in two, or three, or more positions? 
Intuitively, it seems reasonable that for a string of millions or billions of occur-
rences, very small differences do not alter the random character of the whole string. 
This kind of situation arises in many contexts. For example, what is a “pile of stones”? 
If from a pile of stones we remove a stone, does remain it a pile of stones? These 
questions cannot have precise answers, because are based on intrinsically impre-
cise concepts. Therefore, the only way to answer adequately to them is according to 
some approximation degree. In this sense, there is an agreement with the probabilis-
tic notion of randomness, deﬁned in terms of the statistical tests passed by a given 
string, and on the corresponding degrees of fulﬁlment of these tests. 
In conclusion, randomness is not a 0–1 property, but what is also called, after Lotﬁ 
Aliasker Zadeh (1921–2017), a fuzzy property, in the sense of fuzzy logic, which 
holds with a value between 0 and 1. Therefore, a string is informational random in 
a degree that depends on the closeness of its nabla∇-index to the value left floor log Subscript m Baseline left parenthesis n right parenthesis right floor[logm(n)]. Of  
course, several criteria for measuring such a degree can be given, suitable to speciﬁc 
contexts of application. 
Table 5.1 shows the results of an experiment based on Proposition 5.9, conﬁrming 
2 log base 10 Subscript 4 Baseline left parenthesis n right parenthesis2 lg4(n) as a good approximation of the average value of nabla Subscript n comma 4 Baseline∇n,4 in upper R upper N upper D Subscript n comma 4RN Dn,4. 
Results similar to those given in the table above were obtained with random 
sequences generated by quantum phenomena, games, irrational numbers, pseudo-
random generators [ 3, 14– 16], by obtaining an agreement, within a very good level 
of approximation, between randomness, pseudo-randomness and informational ran-

138
5
Information and Randomness
Table 5.1 Pseudo-random strings over 4 symbols of different lengthsnn have been generated (100 
strings for each length); their nabla Subscript 4 comma n Baseline∇4,n values have been evaluated, by reporting in the table, for each 
length, the minimum nabla Subscript n comma 4 Baseline∇n,4 value (m i nmin), the maximum nabla Subscript 4 comma n Baseline∇4,n value (m a xmax), standard deviation (s dsd), 
and the averagenabla Subscript 4 comma n Baseline∇4,n value (a v gavg). With a good approximation it results that2 log base 10 Subscript 4 Baseline left parenthesis n right parenthesis equals log Subscript 2 Baseline n almost equals a v g2 lg4(n) = log2 n ≈avg
Length
Min.
Max.
Sd.
Avg.
l g 2 left parenthesis n right parenthesislg2(n)
1,000
9
15
1.07
10.2
9.97 
100,000
15
20
0.95
16.67
16.61 
200,000
16
21
0.86
17.78
17.61 
500,000
18
23
0.91
19.09
18.93 
1,000,000
18
24
0.96
20.14
19.93 
10,000,000
22
26
0.97
23.49
23.25 
20,000,000
23
27
0.93
24.31
24.25 
30,000,000
24
30
1.14
25.08
24.84 
50,000,000
24
31
1.17
25.86
25.58 
75,000,000
25
29
0.85
26.44
26.16 
100,000,000
25
30
1.02
26.89
26.58 
domness [ 2]. In conclusion, the deﬁnition of informationally random string, given 
above, as 2 log Subscript m Baseline left parenthesis n right parenthesis2 logm(n)-hapax string, seems to be quite satisfactory, and very useful in 
many contexts. 
In the perspective of Proposition 5.9, de Bruijn graphs can be considered a class 
of generators of pseudo-random strings of given lengths. 
The following proposition shows thatleft floor 2 log Subscript m Baseline left parenthesis n right parenthesis right floor[2 logm(n)] corresponds to an upper bound 
to the entropy of strings of length nn over mm symbols, and 2 log Subscript m Baseline n2 logm n is essentially a 
ﬁx-point for the function that associates, to any valuekk, the maximum value reached 
by kk-entropy. 
Proposition 5.10 In the class of strings double struck upper GG of length nn over mm symbols, for any kk
such that nabla left parenthesis double struck upper G right parenthesis less than or equals k less than n∇(G) ≤k < n the following equation holds: 
upper E Subscript k Baseline left parenthesis double struck upper G right parenthesis less than or equals left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor periodEk(G) ≤[2 lgm(n)].
(5.13) 
moreover, random stringsupper R upper A upper N upper D Subscript m comma nRAN Dm,n havenabla Subscript m comma n Baseline∇m,n-entropies that differs from the upper 
bound: left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor[2 lgm(n)] less than di 2 log base 10 Subscript m Baseline left parenthesis n divided by left parenthesis n minus left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor right parenthesis right parenthesis2 lgm(n/(n −[2 lgm(n)])), which approximates to 
zero for increasing values of nn. 
Proof From Proposition 5.8, for  nabla left parenthesis double struck upper G right parenthesis less than or equals k less than n∇(G) ≤k < n, entropy upper E Subscript k Baseline left parenthesis double struck upper G right parenthesisEk(G) reaches its maxi-
mum value when k equals nabla left parenthesis double struck upper G right parenthesis equals m r l left parenthesis double struck upper G right parenthesis plus 1k = ∇(G) = mrl(G) + 1. In such a case: 
upper E Subscript k Baseline left parenthesis double struck upper G right parenthesis equals 2 log base 10 Subscript m Baseline left parenthesis n minus k plus 1 right parenthesis equals 2 log base 10 Subscript m Baseline left parenthesis n minus m r l left parenthesis double struck upper G right parenthesis right parenthesisEk(G) = 2 lgm(n −k + 1) = 2 lgm(n −mrl(G))
(5.14) 
hence, difference 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis minus upper E Subscript k Baseline left parenthesis double struck upper G right parenthesis2 lgm(n) −Ek(G) is given by:

5.2 Informational Randomness
139
2 log base 10 Subscript m Baseline left parenthesis n right parenthesis minus 2 log base 10 Subscript m Baseline left parenthesis n minus m r l left parenthesis double struck upper G right parenthesis right parenthesis equals 2 log base 10 Subscript m Baseline left parenthesis n divided by left parenthesis n minus m r l left parenthesis double struck upper G right parenthesis right parenthesis right parenthesis period2 lgm(n) −2 lgm(n −mrl(G)) = 2 lgm(n/(n −mrl(G))).
(5.15) 
According to Proposition 5.9, when double struck upper G element of upper R upper A upper N upper D Subscript m comma nG ∈RAN Dm,n m r l left parenthesis double struck upper G right parenthesis equals left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor minus 1mrl(G) = [2 lgm(n)] −1, 
whence, by replacing in Eq. (5.15) m r l left parenthesis double struck upper G right parenthesismrl(G) by left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor minus 1[2 lgm(n)] −1 we get: 
2 log base 10 Subscript 2 Baseline left parenthesis n divided by left parenthesis n minus m r l left parenthesis double struck upper G right parenthesis right parenthesis right parenthesis less than 2 log base 10 Subscript 2 Baseline left parenthesis n divided by left parenthesis n minus left floor 2 log base 10 Subscript m Baseline left parenthesis n right parenthesis right floor right parenthesis right parenthesis period2 lg2(n/(n −mrl(G))) < 2 lg2(n/(n −[2 lgm(n)])).
∎
The following proposition gives an evaluation of the (enormous) number of ran-
dom strings, by using the number of de Bruijn sequences. 
Proposition 5.11 
StartAbsoluteValue upper R upper N upper D Subscript m comma m Sub Superscript k Subscript Baseline EndAbsoluteValue equals left parenthesis m factorial right parenthesis Superscript left parenthesis m Super Superscript 2 k Superscript minus 1 right parenthesis|RN Dm,mk| = (m!)(m2k−1)
Proof From de Brujin theory we know that the number of de Bruijn sequences of 
order kk over mm symbols is given by the following formula: 
StartFraction left parenthesis m factorial right parenthesis Superscript m Super Superscript left parenthesis k minus 1 right parenthesis Superscript Baseline Over m Superscript k Baseline EndFraction(m!)m(k−1)
mk
whence, from the hypothesis of informational randomness, the number of random 
strings of length nn over mm symbols is easily obtained by replacing kk by 2 k2k and 
considering that, at any position of a de Bruijn sequence of order 2 k2k, a contiguous 
portion ofm Superscript kmk positions starts, which is a random string. Whence, by easy calculation 
the asserted formula follows.
∎
Randomness is crucial for life, as we have already seen in the chapter on genomes. 
It is also crucial for Physics. Quantum physics can be properly developed only 
within a framework where random processes are intrinsic to particle behaviours. 
Measurements are operators that when applied to quantum systems get values, that 
is, measures of quantum variables, by inducing systems to reach some speciﬁc states. 
However, without such interventions systems assume states that are random combi-
nations of a huge of possible states. In this perspective, uncertainty results to be a way 
of parallel elaboration for a whole class of possible solutions, among which the right 
ones emerge. The computational power of quantum computing resides essentially in 
this possibility. Hence, randomness is a new frontier of computing [ 17– 19]. Random 
algorithms, already discovered in computer science, are formidable instruments of 
problem solving, and pseudo-random numbers are ubiquitous [ 4, 14, 15] in many  
application ﬁelds of Computer Science. In recent approaches of machine learning 
and deep neural nets, the combination of calculus and pseudo-random processes dis-
closes new perspectives for Artiﬁcial Intelligence, where the essence of intelligence 
appears just in the way randomness is employed within strategies able to elaborate 
goals and resolution criteria. These perspectives open new scenarios for many disci-
plines, by overcoming intrinsic epistemological limitations of the present scientiﬁc 
conceptual apparatus.

140
5
Information and Randomness
5.3 
Information in Physics 
In this section, three fundamental passages will be considered, which were cru-
cial in the development of physics, and where information results to be, implic-
itly or explicitly, the essence for novel comprehensions of physical phenomena 
[ 20– 25]. An “informational” line links Boltzmann’s entropy with Planck’s discov-
ery, which extends Boltzmann’s approach, and with Schrödinger’s equation, which 
uses Planck’s relation between frequency and energy and de Broglie’s wave-particle 
duality. 
Boltzmann’s Equation 
In 1824 Sadi Carnot wrote a book about the power of ﬁre in producing energy [ 26]. 
It was known that heat can provide mechanical energy. Moreover, as it was experi-
mentally later shown by Joule, mechanical energy can be entirely transformed into 
heat (ﬁrst law of thermodynamics). However, it is impossible entirely transform heat 
into mechanical energy. In fact, this is a way to formulate the second law of ther-
modynamics, which is also equivalent to the impossibility of spontaneous processes 
where heat passes from a colder body to a hotter body. Carnot introduced some ideal 
heat engines, called reversible engines, reaching the optimal efﬁciency in heat-work 
transformation. In this way, he proved a theorem giving an evaluation of the heat 
quantity that necessarily cannot be transformed into mechanical energy. Namely, 
when an engineupper MM takes a quantityupper Q 2Q2 of heat from a heat source (boiler) at constant 
temperatureupper T 2T2, then a quantityupper Q 1Q1 has to be released to a colder body (condenser) at 
temperatureupper T 1T1 (upper T 2 greater than upper T 1T2 > T1), and only the differenceupper Q 2 minus upper Q 1Q2 −Q1 can be transformed into 
mechanical work. For reversible engines working between temperaturesupper T 2 greater than upper T 1T2 > T1, the  
released heat quantity upper Q 1Q1 reaches the minimum (positive) value, and the following 
equation holds: 
StartFraction upper Q 1 Over upper T 1 EndFraction equals StartFraction upper Q 2 Over upper T 2 EndFraction Q1
T1
= Q2
T2
(5.16) 
therefore, if we denote byupper SS the heat quantityupper Q 1Q1 whenupper T 1T1 is the unitary temperature 
and upper T 2 equals upper TT2 = T we obtain: 
upper S equals upper Q divided by upper T periodS = Q/T.
(5.17) 
upper SS corresponds to a thermodynamical quantity, later called by Clausius [ 20] the  
entropy (of upper MM), corresponding to the minimum heat quantity that necessarily a heat 
engine, working between temperatures upper TT and 11 has to release (to the unitary con-
denser) and that cannot be transformed into mechanical work.

5.3 Information in Physics
141
The famous formulation of the second law of thermodynamics, by means of 
entropy upper SS, asserts that in a closed system (that does not exchange energy with the 
external environment) the entropy cannot decrease in time. 
In this scenario, in the 1870s years, Ludwig Boltzmann started research aimed 
at explaining the second law of thermodynamics in terms of Newtonian mechanics 
[ 21]. The main question was: “Where does the time arrow come from?”. In fact, in 
mechanics, all the laws are symmetric with respect to time and the same equations 
tell us what happens in the future, but also what happened in the past. In no equation, 
there is an explicit indication of the direction of events in time (the ﬁrst Chapter of 
Wiener “Cybernetics” [ 24] is devoted to the different notions of time in Newtonian 
mechanics and in biology). 
The ﬁrst step of Boltzmann’s project was a mechanical formulation of entropy 
(the argument is presented according to [ 27]). This formulation starts from the fun-
damental law of ideal gases, where upper PP is the pressure, upper VV the volume, upper TT the absolute 
(Kelvin) temperature, upper NN the number of gas moles, and upper RR is the gas constant: 
upper P upper V equals upper N upper R upper T periodPV = N RT.
(5.18) 
If we pass from the gas moles upper NN to the number of molecules nn in the gas (by the 
relationupper N a equals nNa = n whereaa is the Avogadro constant), we get an equivalent formulation, 
where k equals upper R ak = Ra is now called the Boltzmann constant: 
upper P upper V equals n k upper T periodPV = nkT.
(5.19) 
Now, let us assume that the gas takes some heat by expanding from a volume upper V 1V1
to a volume upper V 2V2. Then, the quantity upper QQ of this heat is given by: 
upper Q equals integral Subscript upper V 1 Superscript upper V 2 Baseline upper P d vQ =
V2
∫
V1
Pdv
(5.20) 
and by expressing upper PP according to Eq. (5.18), we get: 
upper Q equals integral Subscript upper V 1 Superscript upper V 2 Baseline n k upper T divided by upper V d v equals n k upper T integral Subscript upper V 1 Superscript upper V 2 Baseline 1 divided by upper V d v equals n k upper T left parenthesis ln upper V 2 minus ln upper V 1 right parenthesis periodQ =
V2
∫
V1
nkT/V dv = nkT
V2
∫
V1
1/V dv = nkT (ln V2 −ln V1).
(5.21) 
Let’s assume to start from a unitary volume upper V 0 equals 1V0 = 1. If in Eq.  (5.21) upper V 1 equals upper V 0V1 = V0, 
upper V equals upper V 2V = V2 and upper TT are moved to the left member, then we obtain: 
upper Q divided by upper T equals n k ln upper VQ/T = nk ln V
(5.22) 
that, according to Carnot’s equation (5.17), gives:

142
5
Information and Randomness
upper S equals n k ln upper VS = nk ln V
(5.23) 
that is: 
upper S equals k ln upper V Superscript nS = k ln V n
(5.24) 
where upper V Superscript nV n expresses the number of possible ways of allocating the nn molecules in 
upper VV volume cells. The passage from constant upper RR to constant kk and from upper NN moles to nn
molecules, accordingly, is crucial to the microscopic reading of the formula (very 
often this is not adequately stressed when Boltzmann’s argument is analyzed). 
We can assume that the gas is spatially homogeneous, that is, the volume cell 
positions do not matter and molecules are indiscernible when they have the same 
velocities. Moreover, only mm velocity intervals can be distinguished, at a discrete-
ness level DeltaΔ, that is, intervals v 1 plus or minus Delta comma v 2 plus or minus Delta comma ellipsis comma v Subscript m Baseline plus or minus Deltav1 ± Δ, v2 ± Δ, . . . , vm ± Δ. Hence, a distribution 
of nn molecules in the volume upper VV is given, apart from multiplicative constants, by 
the number upper WW of different ways in which nn molecules can be distributed into mm
different velocity classes where n 1 comma n 2 comma ellipsis n Subscript m Baselinen1, n2, . . . nm are the number of molecules in each 
velocity classes (upper WW is the number of different micro-states associated to a given 
thermodynamic macro-state). Thus upper SS is given by: 
upper S equals k ln upper WS = k ln W
(5.25) 
hence: 
log base 10 upper W equals k ln StartFraction n factorial Over n 1 factorial n 2 factorial ellipsis n Subscript m Baseline factorial EndFraction lg W = k ln
n!
n1!n2! . . . nm!
(5.26) 
whence, by using Stirling’s approximation: 
log base 10 upper W equals k n ln n minus k left bracket n 1 ln n 1 plus n 2 ln n 2 plus midline horizontal ellipsis plus n Subscript m Baseline ln n Subscript m Baseline right bracket lg W = kn ln n −k[n1 ln n1 + n2 ln n2 + · · · + nm ln nm]
(5.27) 
that, apart from an additive constant and the sign, is theupper HH function for which Boltz-
mann tried to prove theupper HH Theorem asserting that is not increasing in time (in isolated 
systems). 
Now if in the above equation we replace n 1 comma n 2 comma ellipsis comma n Subscript m Baselinen1, n2, . . . , nm by n p 1 comma n p 2 comma ellipsis comma n p Subscript m Baselinenp1, np2, . . . , npm
(p 1 comma p 2 comma ellipsis comma p Subscript m Baselinep1, p2, . . . , pm the frequencies n Subscript i Baseline divided by nni/n) we obtain: 
log base 10 upper W equals k n ln n minus k left bracket n p 1 ln n p 1 plus n p 2 ln n p 2 plus midline horizontal ellipsis plus n p Subscript m Baseline ln n p Subscript m Baseline right bracket lg W = kn ln n −k[np1 ln np1 + np2 ln np2 + · · · + npm ln npm]
(5.28) 
whence: 
log base 10 upper W equals k n ln n minus k n left bracket p 1 left parenthesis ln n plus ln p 1 right parenthesis plus p 2 left parenthesis ln n plus ln p 2 right parenthesis plus midline horizontal ellipsis plus p Subscript m Baseline left parenthesis ln n plus ln p Subscript m Baseline right parenthesis right bracket lg W = kn ln n −kn[p1(ln n + ln p1) + p2(ln n + ln p2) + · · · + pm(ln n + ln pm)]
(5.29)

5.3 Information in Physics
143
that is: 
log base 10 upper W equals k n ln n minus k n ln n plus minus k n left bracket p 1 ln p 1 plus p 2 ln p 2 plus midline horizontal ellipsis plus p Subscript m Baseline ln p Subscript m Baseline right bracket lg W = kn ln n −kn ln n + −kn[p1 ln p1 + p2 ln p2 + · · · + pm ln pm]
(5.30) 
or: 
log base 10 upper W equals minus k n left bracket p 1 ln p 1 plus p 2 ln p 2 plus midline horizontal ellipsis plus p Subscript m Baseline ln p Subscript m Baseline right bracket lg W = −kn[p1 ln p1 + p2 ln p2 + · · · + pm ln pm]
(5.31) 
showing the equivalence (apart from multiplicative constants) between the physical 
entropyupper SS and the informational entropyupper H Subscript upper SHS (subscriptupper SS stands for Shannon initial): 
log base 10 upper W equals minus k n upper H Subscript upper S Baseline period lg W = −knHS.
(5.32) 
Equation (5.25) is reported in Boltzmann’s tomb in Wien. In this form, the equation 
was later given by Max Planck, who followed Boltzmann’s approach in his famous 
conference on December 14, 1900, from which Quantum Theory emerged [ 22]. The 
importance of this microscopic formulation of Entropy is the statistical approach 
that stemmed from it and that became crucial in the whole physics of the twentieth 
century. The computation of upper WW was obtained from Boltzmann by means of the so-
called Maxwell-Boltzmann statistics, that is the multinomial representation of upper WW, 
expressed by Boltzmann as the “Wahrscheinlichkeit” principle. 
From the statistical formulation of entropy, Boltzmann tried to prove the statisti-
cal nature of time arrow in thermodynamics. The question he posed was: Why does 
entropy increase (or better does not decrease) in time? We know that molecules col-
liding in a gas follow Newtonian mechanics, where time does not show a preferential 
direction because collisions are ruled by equations. Therefore time irreversibility has 
to emerge from a population reversible elementary processes. How can happen such 
a kind of phenomenon? Boltzmann proved in 1972 his H Theorem, showing that the 
function H, in an isolated system (which does not exchange energy with its envi-
ronment) does not increase in time (entropy which is essentially given by -H does 
not decrease). In this way, the second principle of thermodynamics is a consequence 
of the microscopic representation of entropy. However, Boltzmann’s proof was not 
accepted by many scientists of that time (some technical inadequacies were present 
in the proof). Nevertheless, Boltzmann was right, and today many proofs are avail-
able of H Theorem. Here we present a numerical experiment, which, in a very simple 
manner, explains why time direction is a consequence of the laws of large numbers 
in random processes and of Shannon entropy properties [ 27, 28]. 
Let us consider a sort of two-dimensional gas, that is, a population of numbers 
randomly chosen in a numerical interval (any number can occur in many copies). 
Let us assume speciﬁc approximation levels, by considering equal numerical values 
differing within a ﬁxed error interval. Then start a process of collisions, where these 
numbers, seen as velocities of particles in two dimensions, evolve, step by step, by 
exchanging in each collision theirx comma yx, y-components (according to a randomly chosen 
collision direction). In this way, both velocity distributions of xx-component and yy-

144
5
Information and Randomness
Fig. 5.1 Initial distribution, 
Interval 100–200, population 
size 10000 (from [ 28]) 
component tend, in time, to normal distributions. From Information theory, we know 
that a normal distribution is one having the maximum value of Shannon entropy 
among all distributions with a ﬁxed variance. But in the considered case, collisions 
exchangingx comma yx, y-components (along a collision direction) do not change the variance 
ofx comma yx, y-distributions (collisions are elastic, without loss of kinetic energy). Therefore, 
tending this distribution to normal laws, and being the numbers of the formStartRoot v Subscript x Superscript 2 Baseline plus v Subscript y Superscript 2 Baseline EndRoot
√
v2x + v2y
their population tends to achiχ distribution (a distribution that is the square root of the 
sum of the squares of two variables following normal laws). Finally,x comma yx, y components 
of numbers obtained after many collisions reach the maximum values of Shannon 
entropy (and the minimum of Boltzmann’s H function). 
The following table reports from [ 28] the rule of collisions, called Pythagorean 
recombination game. Diagrams are also reported in [28], at several collisions, which 
show clearly the emergence of an evidentchiχ-distributions and the minimum of H func-
tion (the last diagram of the experiment is given in Fig. 5.2). In conclusion, according 
to Boltzmann’s claim, the time arrow, that is, time-orientedness, is a consequence 
of the collective behaviour of a huge of elementary events following newtonian 
mechanics with no time orientation. Synthetically, time is a statistical phenomenon 
of complex systems (Fig. 5.1 and Table 5.2). 
Table 5.2 The Pythagorean recombination game 
Randomly choose two numbersa comma ba, b of the given number population; 
Randomly choose a numbera 1a1, sucha 1 less than or equals aa1 ≤a, and splitaa intoa 1a1 anda 2 equals StartRoot a squared minus a 1 squared EndRoota2 =
√
a2 −a2
1; 
Randomly choose a numberb 1b1, suchb 1 less than or equals bb1 ≤b, and splitbb intob 1b1 andb 2 equals StartRoot b squared minus b 1 squared EndRootb2 =
√
b2 −b2
1; 
Replace the paira comma ba, b with the paira prime equals StartRoot a 1 squared plus b 2 squared EndRoota' =
√
a2
1 + b2
2,b prime equals StartRoot b 1 squared plus a 2 squared EndRootb' =
√
b2
1 + a2
2.

5.3 Information in Physics
145
Fig. 5.2 4000 steps with 
200 collisions per step, 
applied to the initial random 
distribution of velocities 
given in Fig. 5.1, with a 
speciﬁc approximation level, 
computed by a MathLab 
function (from [ 28]) 
Planck’s Quantum Discovery 
On December 14th of 1900, Max Planck gave an epochal conference where he pre-
sented a solution to a physical problem showing a theoretical limitation of classical 
physics: the explanation of the observed distribution of irradiation frequencies with 
respect to the temperature of the irradiating body (usually referred as the black body 
irradiation problem). The solution presented by Planck was based on the assump-
tion that energy can be exchanged only in discrete quantities called quanta. The 
experimental formula giving the average energy upper U Subscript nuUν irradiated at frequency nuν and 
at temperature upper TT is the following, but all the attempts to explain this formula with 
classical arguments had been unsuccessful: 
upper U Subscript nu Baseline equals StartFraction h nu Over e Superscript StartFraction h nu Over k upper T EndFraction Baseline minus 1 EndFractionUν =
hν
e
hν
kT −1
(5.33) 
The starting point of Planck’s argument for deriving the formula above was the 
notion of entropy upper SS as developed by Ludwig Boltzmann, in terms of mechanical 
statistics, as the logarithm of the numberupper WW of microscopical conﬁgurations provid-
ing the observed macroscopic state: 
upper S equals k log base 10 upper W periodS = k lg W.
(5.34) 
Let us consider a body that we assume as a population ofupper NN microscopical elementary 
irradiating bodies and let upper PP be the number of elementary irradiators having a given 
frequency nuν. We determine the number of possible conﬁgurations giving the same 
amount of energy irradiated at the given frequency. This number corresponds to the 
following combinatorial value: 
StartFraction left parenthesis upper N plus upper P minus 1 right parenthesis factorial Over upper N factorial left parenthesis upper P minus 1 right parenthesis factorial EndFraction almost equals StartFraction left parenthesis upper N plus upper P right parenthesis factorial Over upper N factorial upper P factorial EndFraction(N + P −1)!
N!(P −1)!
≈(N + P)!
N!P!
(5.35)

146
5
Information and Randomness
whence the entropy computed according to Boltzmann’s formula yields for a constant 
kk, called Planck constant: 
upper S equals k left bracket log base 10 left parenthesis upper N plus upper P right parenthesis factorial minus log base 10 upper N factorial minus log base 10 upper P factorial right bracketS = k
[
lg(N + P)! −lg N! −lg P!
]
(5.36) 
and by applying Stirling approximation: 
upper S equals k left bracket left parenthesis upper N plus upper P right parenthesis log base 10 left parenthesis upper N plus upper P right parenthesis minus upper N log base 10 upper N minus upper P log base 10 upper P right bracket periodS = k
[
(N + P) lg(N + P) −N lg N −P lg P
]
.
(5.37) 
Let upper UU be the average energy irradiated by the upper NN elementary irradiators, then if epsilonε is 
the minimum energy irradiated at frequency nuν we have: 
upper N upper U Subscript nu Baseline equals upper P epsilonNUν = Pε
(5.38) 
or, dropping the subscript nuν in the passages for the sake of simpliﬁcation: 
upper P equals StartFraction upper N upper U Over epsilon EndFractionP = NU
ε
(5.39) 
when we replace the value of upper PP of the equation above in Stirling approximation 
(5.37) we obtain: 
upper S equals k left bracket left parenthesis upper N plus StartFraction upper N upper U Over epsilon EndFraction right parenthesis log base 10 left parenthesis upper N plus StartFraction upper N upper U Over epsilon EndFraction right parenthesis minus StartFraction upper N upper U Over epsilon EndFraction log base 10 StartFraction upper N upper U Over epsilon EndFraction minus upper N log base 10 upper N right bracket equalsS = k
[(
N + NU
ε
)
lg
(
N + NU
ε
)
−NU
ε
lg NU
ε
−N lg N
]
=
(5.40) 
upper S equals k left bracket upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis log base 10 upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis minus StartFraction upper N upper U Over epsilon EndFraction log base 10 StartFraction upper N upper U Over epsilon EndFraction minus upper N log base 10 upper N right bracket periodS = k
[
N
(
1 + U
ε
)
lg N
(
1 + U
ε
)
−NU
ε
lg NU
ε
−N lg N
]
.
(5.41) 
Let us derive with respect toupper U Subscript t o t Baseline equals upper N upper UUtot = NU. We observe that the derivative of a function 
y log yy log y, wherey primey' the derivative ofyy, isy prime log y plus y primey' log y + y', and that derivatives with respect 
to upper UU of upper N left parenthesis StartFraction 1 plus upper U Over epsilon EndFraction right parenthesisN( 1+U
ε ) and StartFraction upper N upper U Over epsilon EndFraction NU
ε
are equal, therefore: 
StartFraction d upper S Over d upper U Subscript t o t Baseline EndFraction equals StartFraction d upper S Over upper N d upper U EndFraction equals StartFraction k Over upper N EndFraction left bracket StartFraction upper N Over epsilon EndFraction log base 10 upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis minus StartFraction upper N Over epsilon EndFraction log base 10 StartFraction upper N upper U Over epsilon EndFraction right bracket equals dS
dUtot
=
dS
NdU = k
N
[ N
ε lg N
(
1 + U
ε
)
−N
ε lg NU
ε
]
=
(5.42) 
StartFraction k Over epsilon EndFraction left bracket log base 10 upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis minus log base 10 StartFraction upper N upper U Over epsilon EndFraction right bracket equalsk
ε
[
lg N
(
1 + U
ε
)
−lg NU
ε
]
=
(5.43) 
but StartFraction d upper S Over d upper U Subscript t o t Baseline EndFraction equals StartFraction 1 Over upper T EndFraction dS
dUtot = 1
T , therefore: 
StartFraction d upper S Over d upper U Subscript t o t Baseline EndFraction equals StartFraction k Over epsilon EndFraction left bracket log base 10 upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis minus log base 10 StartFraction upper N upper U Over epsilon EndFraction right bracket equals StartFraction 1 Over upper T EndFraction dS
dUtot
= k
ε
[
lg N
(
1 + U
ε
)
−lg NU
ε
]
= 1
T
(5.44)

5.3 Information in Physics
147
that is: 
log base 10 StartStartFraction upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis OverOver StartFraction upper N upper U Over epsilon EndFraction EndEndFraction equals StartFraction epsilon Over k upper T EndFraction lg N(1 + U
ε )
NU
ε
= ε
kT
(5.45) 
whence: 
StartStartFraction upper N left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis OverOver StartFraction upper N upper U Over epsilon EndFraction EndEndFraction equals e Superscript StartFraction epsilon Over k upper T EndFraction N(1 + U
ε )
NU
ε
= e
ε
kT
(5.46) 
that, with the following simple passages, provides a ﬁnal equation: 
StartStartFraction left parenthesis 1 plus StartFraction upper U Over epsilon EndFraction right parenthesis OverOver StartFraction upper U Over epsilon EndFraction EndEndFraction equals e Superscript StartFraction epsilon Over k upper T EndFraction(1 + U
ε )
U
ε
= e
ε
kT
(5.47) 
StartFraction epsilon Over upper U EndFraction equals e Superscript StartFraction epsilon Over k upper T EndFraction Baseline minus 1 ε
U = e
ε
kT −1
(5.48) 
upper U equals StartFraction epsilon Over e Superscript StartFraction epsilon Over k upper T EndFraction Baseline minus 1 EndFractionU =
ε
e
ε
kT −1
(5.49) 
now setting epsilon equals h nuε = hν as the energy quantum we obtain a formula coinciding with 
the experimental formula of irradiation given at beginning of this analysis, where 
subscript nuν is explicit: 
upper U Subscript nu Baseline equals StartFraction h nu Over e Superscript StartFraction h nu Over k upper T EndFraction Baseline minus 1 EndFraction periodUν =
hν
e
hν
kT −1
.
(5.50) 
Schrödinger Equation 
In 1926 the physicist Erwin Schrödinger published a paper [ 23] where he presented 
an equation describing the state psiψ of a quantum system, by assuming that it is 
described by a wave. The derivation is based on a particle of massmm and velocityvv, 
to which, according to de Broglie’s complementarity principle, a wave is associated 
with lengthh divided by m vh/mv, wherehh is the Planck constant, and for which the wave energyupper EE
veriﬁes the Planck equation upper E equals h nuE = hν, where nuν is the wave frequency. 
The following are the main passages of Schrödinger’s equation. 
A wave of length lamdaλ and frequency nuν has a velocity uu given by: 
u equals lamda nuu = λν
(5.51) 
and D’Alembert wave equation states that: 
Delta psi minus psi With two dots slash u squared equals 0Δψ −¨ψ/u2 = 0
(5.52)

148
5
Information and Randomness
Moreover, according to Planck, the wave frequency nuν gives a quantum of energy 
upper EE divided by the Planck constant hh: 
nu equals upper E divided by hν = E/h
(5.53) 
Equations (5.51) and (5.53) provide: 
u equals lamda upper E divided by hu = λE/h
(5.54) 
moreover, according to De Brogile, a wave length lamdaλ is associated to a particle of 
mass mm moving at velocity vv that satisﬁes the following equation: 
lamda equals h divided by m vλ = h/mv
(5.55) 
and from Eqs. (5.54) and (5.55) we obtain: 
u equals upper E divided by m vu = E/mv
(5.56) 
but considering the total energyupper EE of a particle at velocityvv, if the particle is subjected 
to a conservative ﬁeld with a potentialupper VV , thenupper EE is the sum of kinetic energy and of 
potential energy upper VV , that is, upper E equals one half m v squared plus upper VE = 1
2mv2 + V , thus we can write: 
m v equals left bracket 2 m left parenthesis upper E minus upper V right parenthesis right bracket Superscript one halfmv = [2m(E −V )]
1
2
(5.57) 
and putting together Equations (5.56) and (5.57) we obtain: 
u squared equals upper E squared divided by 2 m left parenthesis upper E minus upper V right parenthesisu2 = E2/2m(E −V )
(5.58) 
where we remark that the velocity vv of a particle is different from the velocity uu
associated of its corresponding wave. 
Schrödinger assumes for psiψ a typical form of wave function (kk a constant): 
psi equals e Superscript minus i left parenthesis omega t plus k right parenthesisψ = e−i(ωt+k)
(5.59) 
whereomega equals 2 pi nuω = 2πν. But, according to Planck,nu equals upper E divided by hν = E/h thereforeomega equals 2 pi upper E divided by hω = 2π E/h thus, by 
deriving twice with respect to tt we have: 
psi With two dots equals minus 4 pi squared upper E squared psi divided by h squared period ¨ψ = −4π2E2ψ/h2.
(5.60) 
By replacing the right member of Equations (5.60) into (5.52) where u squaredu2 is replaced 
according to Equation (5.58) we get exactly the original form of Schrödinger 
Equation (5.16) in his paper of 1926: 
Delta psi plus 8 pi squared m left parenthesis upper E minus upper V right parenthesis psi divided by h squared equals 0Δψ + 8π2m(E −V )ψ/h2 = 0
(5.61)

5.3 Information in Physics
149
now by setting italic h over two pi equals h divided by 2 pih = h/2π and by multiplying by italic h over two pi squared divided by 2 mh2/2m Equation (5.61) we have:  
italic h over two pi squared Delta psi divided by 2 m plus left parenthesis upper E minus upper V right parenthesis psi equals 0h2Δψ/2m + (E −V )ψ = 0
(5.62) 
or 
minus italic h over two pi squared Delta psi divided by 2 m plus upper V psi equals upper E psi −h2Δψ/2m + V ψ = Eψ
(5.63) 
which is the standard form of Schrödinger Equation without time dependency. 
If we introduce the Hamiltonian operatorscript upper HH (an analogous of that one of classical 
mechanics) such that: 
script upper H left parenthesis psi right parenthesis equals minus StartFraction italic h over two pi squared Over 2 m EndFraction Delta psi plus upper V psiH (ψ) = −h2
2m Δψ + V ψ
then, we obtain the following very synthetic form: 
script upper H left parenthesis psi right parenthesis equals upper E psi periodH (ψ) = Eψ.
(5.64) 
In order to introduce explicit time dependency in the equation, we derive psi equals e Superscript minus i left parenthesis omega t plus k right parenthesisψ =
e−i(ωt+k) with respect to tt: 
StartFraction d psi Over d t EndFraction equals minus i omega psi equals minus i Baseline 2 pi nu psi equals minus i StartFraction 2 pi upper E Over h EndFraction psidψ
dt = −iωψ = −i2πνψ = −i 2π E
h
ψ
(5.65) 
whence: 
upper E psi equals StartFraction d psi Over d t EndFraction italic h over two pi iEψ = dψ
dt hi
(5.66) 
thus, from Eq. (5.63) the time dependent Schrödinger Equation follows: 
minus StartFraction italic h over two pi squared Over 2 m EndFraction Delta psi plus upper V psi equals italic h over two pi i StartFraction d psi Over d t EndFraction period −h2
2m Δψ + V ψ = hi dψ
dt .
(5.67) 
or, more synthetically: 
script upper H left parenthesis psi right parenthesis equals italic h over two pi i StartFraction d psi Over d t EndFraction periodH (ψ) = hi dψ
dt .
(5.68) 
The formal derivation of Schrödinger Equation can be easily followed, but what really 
does it mean? What intuition does it express? This is a very difﬁcult question, and it 
is very impressive that Schrödinger seems guided by a sort of unconscious intuition, 
which is a miraculous mixing of physical classical principles (d’Alembert) with new 
ideas introduced by quantum perspective (Planck’s Energy-frequency relation and 
de Broglie’s particle-wave complementarity). 
First of all, what psiψ is? Certainly, it is a wave, but what kind of wave? Surely, 
Schrödinger refers to a subatomic particle, but the implicit assumption was that the 
approach extends to systems of particles, up to a whole atom. These systems can

150
5
Information and Randomness
be correctly described only by introducing new principles with respect to classi-
cal mechanics and electromagnetism. For example, the Hydrogen atom cannot be 
explained by using classical electrodynamics. In fact, an electron rotating around 
a proton, in its motion, should consume a quantity of energy that would produce a 
collapse of its rotation toward the proton. 
Schrödinger’s approach assumes that psiψ describes the state of a quantum system. 
If a number of variables identify a quantum system, at any moment, these variables 
“oscillate”, as a wave, within speciﬁc ranges of values, and each value is assumed 
with a given probability of occurring. This interpretation is not present in the 1926 
original paper of Schrödinger, who believed in a real physical wave (he uses the term 
“parcel of waves”). Namely, the probabilistic interpretation emerged later, especially 
for the contributions of Max Born. In this sense, a state is a system of probability 
distributions, that is, a system of information sources, in the sense of Shannon. 
The state psiψ of a quantum system is a wave, but a “probability wave”, that is, an 
abstract wave, very different from the usual ones that propagate through a medium 
(the empty space in the case of electromagnetic waves). We cannot observepsiψ without 
disturbing it, and a datum is generated as the output of an observation that contem-
poraneously changes the original setting of the observed state. 
This mechanism is strictly regulated by a mathematical theory initiated by John 
von Neumann, based on Hilbert spaces over complex numbers, with several kinds 
of operators acting on these spaces. Of course, the story is much more complex and 
the interested reader can ﬁnd many beautiful presentations of quantum principles in 
many classical and new presentations of quantum mechanics [ 29– 32]. 
What is really surprising is the fact that Schrödinger Equation, which is the kernel 
of quantum mechanics, captures the internal structure of a quantum state. However, 
it was discovered by Schrödinger by means of passages where psiψ is considered as a 
physical wave, by applying classical physical principles integrated with Planck’supper E equals h nuE =
hν, and de Broglie’slamda equals h divided by m vλ = h/mv. The situation is resembling Colombo’s discovery of 
America, while he was searching for a way to India. In this case, the explanation is 
due to the ignorance about the existence of a country between the Atlantic and Paciﬁc 
oceans. In the case of Schrödinger’s discovery, no clear explanation is available up 
to now. This fact suggests us the presence of a mystery strictly related to the role of 
information in physical models. In other words, we can read Schrödinger’s derivation 
as a sort of “unreasonable passage” from a physical reality to an “informational 
essence”, hence a crucial open problem remains for the explanation of this passage. 
Information results to be a sort of red thread of the most important physical 
revolutions of the last centuries: Boltzmann’s entropy is an analogue of Shannon 
entropy; quanta emerge from a discretization of irradiating oscillators, by considering 
Boltzmann’s entropy of the whole oscillating system; ﬁnally, quantum systems are 
information sources of physical variables. 
This analysis shows as the informational perspective involves, mathematics, 
physics, biology, and of course neurology, being neurons, of natural or artiﬁcial sys-
tems, computing agents integrated into networks and communicating with signals of 
continuous and/or discrete nature. All the levels of reality, and of our knowledge of

5.4 The Informational Nature of Quantum Mechanics
151
reality, are dominated by information sources, and many secrets of this reality can 
be deciphered by deep analyses of their informational mechanisms. 
Since Boltzmann’s discovery of theupper HH function, an intrinsic relationship emerges 
between mass-energy and data-information. Symbols need physical support in order 
to be realized, therefore information postulates physics. Conversely, from quantum 
physics, we discover that physics requires information sources for expressing the 
physical states of microscopic quantum systems. This means that the physical and 
informational perspectives seem to be different aspects of the same reality. The 
bounds and the passages from these two faces of the same coin are again the miss-
ing points that the science of the future is called to analyze and understand. This 
theoretical necessity has been raised by many important physicists, along different 
lines, by reconsidering physical principles of quantum mechanics [ 25, 33– 37], and 
a famous physicist, as John Archibald Wheeler, in his paper “Information, physics, 
quantum: The search for links” [ 25] writes: […] Every physical quantity, every it, 
derives its ultimate signiﬁcance from bits, binary yes-no indications, a conclusion 
which we epitomize in the phrase it from bit. 
We are waiting for the right theoretical framework for being this link adequately 
developed. 
In conclusion, it is worthwhile to mention here the famous Schrödinger’s booklet 
[ 38] entitled What is Life? published in 1944. The great physicist, following a very 
general reasoning, prophesies the presence, inside cells, of some kind of structure 
encoding biological information. Only a few years before the DNA discovery, he 
speaks of “aperiodical crystals”. Thus, the scientist who introduced the quantum 
wave equation postulates a chemical structure supporting biological information. 
Science is very often compartmentalized in disciplines, but deep concepts do not 
follow the academic partitions of scientiﬁc ﬁelds. 
5.4 
The Informational Nature of Quantum Mechanics 
Let us brieﬂy outline the principles of quantum mechanics (QM), in a way that puts 
in evidence the crucial role of information as a founding concept. 
QM, for its nature, is on the border of knowledge of physical reality. However, 
it seems that the lack of a rigorous and coherent explanation of its principles, at the 
same time ﬁrmly anchored to physical intuitions, is mostly due to an inadequacy of 
language. In fact, great difﬁculties arise when classical concepts, which were elab-
orated for macroscopic phenomena, are used in QM. The mental images evoked by 
terms such as particle, wave, measure,ellipsis. . . become a barrier to the comprehension of 
microscopic realities. In the following, we give a formulation, which surely requires 
further speculation, where opposite terms are conciliated by distinguishing between 
epiphenomenon and ipophenomenon, a sort of reinterpretation of hidden variables, 
often invoked in the long debate on QM. Just this distinction allows us to introduce the 
notion of localization wave, which could make possible to link the original physical 
intuition of Schrödinger’s wave with Born’s probabilistic interpretation.

152
5
Information and Randomness
Quantum Phenomenon 
A quantum phenomenon consists of the interaction between two systems Q and O. 
The ﬁrst one is a quantum system, that is, a microscopic system at the atomic or 
subatomic level (one or more particles, an atom, an electron, a photon beam, a laser 
ray, …). The second one is a macroscopic observation instrument (a Geiger counter, 
a Stern-Gerlach apparatus, a polarization ﬁlter, …). A microscopic system cannot be 
observed without an observation instrument, that being a macroscopic object, fol-
lows a logic assuming the principles of classical mechanics and electromagnetism. 
Therefore, even if the laws of microscopic systems are different from those of clas-
sical physics, an observation assumes these laws. 
Epiphenomenon, Ipophenomenon 
A quantum system Q is characterized by some physical variables divided into two 
disjoint groups. The ﬁrst group are the internal variables of the system, the remain-
ing are the variables observed during the interaction with the observation system 
O. The observed variables assume, after the interaction, values that determine the 
epiphenomenon of the interaction. The values of the internal variables determine 
the ipophenomenon of the interaction. In particular, at any state ss of Q, a complex 
numberpsi left parenthesis s right parenthesisψ(s) is associated, called probability amplitude ofss. This amplitude is not 
observed, but it is linked to some observable valueq left parenthesis s right parenthesisq(s) by the fundamental relation: 
upper P r o b left parenthesis q left parenthesis s right parenthesis right parenthesis equals StartAbsoluteValue psi left parenthesis s right parenthesis EndAbsoluteValue squaredProb(q(s)) = |ψ(s)|2
according to which the probability of observing q left parenthesis s right parenthesisq(s) is given by the square of the 
absolute values of psi left parenthesis s right parenthesisψ(s). 
Localization Wave 
A quantum particle (photon, electron, proton, neutron, neutrin, positron, …) is the 
space localization of a quantum wave, and conversely, a quantum wave is the poten-
tiality of space localization of a particle. This means that the probability wave usu-
ally associated with Schrödinger equation, according to Born’s interpretation, cor-
responds to a true physical wave, a space vibration, which we know as a probability 
wave. In other words, Planck’s principle claiming that energy is present in packets 
h nuhν, with hh Planck constant and nuν a frequency of oscillations, has to be properly 
intended so that energy can be transferred between systems in discrete packets. 
Then, in this perspective, a particle is the quantum packet of a wave that transfers its 
energy, in a speciﬁc microregion, to an observation instrument, revealing the inter-
action between the wave and the instrument. Therefore Schrödinger wave and Born 
probability wave are the two faces of the same coin. This interpretation can explain 
something incredible: the fact that Schrödinger, starting from a physical intuition, 
expressed by d’Alembert wave equation, and inserting in that equation Planck and de 
Broglie equations, by using principles of classical mechanics, was able to discover

5.4 The Informational Nature of Quantum Mechanics
153
his famous psiψ-equation, later interpreted as a probability wave. The duality wave-
particle is the essence of quantum phenomena, where the wave is the ipophenomenon 
related to a particle that is the epiphenomenon revealing it. 
Another aspect of a localization wave is the intrinsic random nature of its vibration 
around positions or centres of localization. This randomness is the basis of random-
ness observed in quantum phenomena, resembling random walks, and which consti-
tutes the basis for true random variables based on physical phenomena (observed in 
Brownian motions, or in the arrival time of photons in detectors). 
Many books on QM start from the interference phenomenon of Young revealing 
the wave nature of light, against Newtonian particle viewpoint about light rays. In 
1801, Young presented a famous paper to the Royal Society entitled “On the Theory 
of Light and Colours’. In Young’s experiment, light passes through two slits and 
reaches a screen, where light appears divided by dark stripes, which become wider 
as they are more remote from the slits. This phenomenon is completely explained 
by assuming that light consists of waves, which are subjected to interference when 
they meet in opposite phases that destroy each other by producing darkness. 
The particle nature of light was again accredited after the discovery of the photo-
electric phenomenon, exhibiting a clear particle behaviour of photons, seen as light 
quanta. 
However, reconsidering Young’s experiment, where single photons are sent to two 
slits, for example, one photon per second, the same interference is obtained, while 
interference disappears (no stripes, but only two light regions on the screen, more 
luminous in correspondence to the slits) when an observation apparatus is located 
near to one of the two slits, for knowing through which one any photon will pass. 
This situation seems paradoxical because it results that photons behave in com-
pletely different ways when they are observed, with respect to when they are not 
observed. Moreover, the same situation arises even when electrons, instead of pho-
tons, are considered. Wave interference is the core of QM, where the dualism between 
waves and particle is a crucial aspect of quantum phenomena. 
Non determinism 
Laws that describe quantum phenomena, differently from those of classical physics, 
do not tell us what happens to a system when it is in a given state. Quantum laws tell 
us what is the probability of observing one value of an observed variable upper XX, among 
some possible values that the variable can assume., If q 0q0 is the state of the quantum 
system that we observe anda 1 comma a 2 comma ellipsis a Subscript j Baseline comma ellipsisa1, a2, . . . a j, . . . are the possible values of the observed 
variableupper XX, then what we can know from a quantum description are the probabilities 
p 1 comma p 2 comma ellipsis comma p Subscript j Baseline comma ellipsisp1, p2, . . . , p j, . . . of observing the corresponding values ofupper XX. Ifq 0 left parenthesis upper X right parenthesisq0(X) is the value 
of upper XX observed in the sate q 0q0, we can write: 
p Subscript i Baseline equals upper P r o b left parenthesis q 0 left parenthesis upper X right parenthesis equals a Subscript i Baseline right parenthesis periodpi = Prob(q0(X) = ai).
this means that a quantum description provides a Shannon source of information 
associated with the observed state.

154
5
Information and Randomness
Let us consider a short presentation of some quantum principles, in light of the 
concepts previously outlined. The presented perspective is aimed at giving possible 
solutions to two crucial conundrums of QM: the wave interference experiment and 
Schrödinger’s equation derived from the intuition of a physical wave, but interpreted 
as an abstract probability wave. Here, the wave-particle dualism is based on the 
epiphenomenon-ipophenomenon distinction and on the concept of localization wave. 
1. Discreteness (Planck) 1900 
Energy is exchanged among physical systems in discrete packets. upper E equals h nuE = hν (h 
Planck constant, minimum action quantum, of order 10 Superscript negative 3410−34 Joule x sec, nuν wave 
frequency). 
2. Duality (de Broglie) 1923 
A wave of length h divided by m vh/mv is associated with any particle of mass mm and velocity 
vv. This principle is the kernel of all the following principles and, in particular, 
the basis of Schrödinher’s equation, which is the epicentre of QM. 
3. Exclusion (Pauli) 1925 
Two identical particles cannot be in the same quantum state. This simple property 
allows for explaining the logic of Mendelejev’s table of chemical elements, and 
many important properties of the different forms that matter assumes in the 
universe. 
4. Superposition (Schrödinger) 1926 
An event is given by the assumption of values from the variables deﬁning the 
state of a quantum system. We writex 1 equals a 1 comma x 2 equals a 2 comma ellipsisx1 = a1, x2 = a2, . . . for denoting an event 
of variables x 1 comma x 2 comma ellipsisx1, x2, . . .. An event can be deﬁned apart from any possibility of 
observing the values of the considered variables. For example, x 1x1 could be an 
internal variable and then a 1a1 is never observable, while an experiment is an 
event where the values of some variables are observed. 
A quantum system is described by a periodical functionpsiψ (wave) taking values 
in the complex plane, or in general, in vectors of complex numbers. The value 
psi left parenthesis s right parenthesisψ(s) for a statess of the system Q is called wave amplitude associated with the 
state ss. This function veriﬁes the following Schrödinger’s equation: 
script upper H left bracket psi left parenthesis s right parenthesis right bracket equals upper E psi left parenthesis s right parenthesisH [ψ(s)] = Eψ(s)
according to which the Hamiltonian operator script upper HH applied to psi left parenthesis s right parenthesisψ(s) provides the 
value psi left parenthesis s right parenthesisψ(s) multiplied by the value upper EE of the energy of the system, where the 
Hamiltonian script upper HH generalizes an analogous operator of classical mechanics. In 
thepsiψ-equation given above, three variables occur: the state variabless, the energy 
variable upper EE, and the amplitude variable psi left parenthesis s right parenthesisψ(s). We easily realize that two of them 
are non-observable variables (ss, psi left parenthesis s right parenthesisψ(s)), while upper EE is an observable variable. The 
equation expresses an intrinsic connection among them, which is the fundamental 
relation of quantum systems. 
5. Indetermination (Heisenberg) 1927 
Delta x Delta p greater than or equals hΔxΔp ≥h

5.4 The Informational Nature of Quantum Mechanics
155
where, for any variableyy, notationDelta yΔy denotes the approximation of determina-
tion, that is the error interval in the determination ofyy. If the productDelta x Delta p greater than or equals hΔxΔp ≥h, 
a small error in determining the position xx of a particle corresponds to a very 
large error in determining its moment pp, and vice versa. 
6. Complementarity (Bohr) 1927 
The description of a quantum phenomenon postulates an interaction between 
a quantum system Q and a macroscopic observation system O (instrument). 
These two physical systems are complementary and their interaction produces a 
perturbation of the observed system, which in its essence remains unknowable. 
Therefore, if we observe a quantum system, then we disturb it. At the same time, 
we cannot observe it without disturbing it. This means that the description of a 
quantum system is a description of its perturbations. This principle is the basis 
of other speciﬁc complementary aspects that are essential in QM. 
7. Probability (Born) 1927 
The probability of realizing a statess is given by the square ofStartAbsoluteValue psi left parenthesis s right parenthesis EndAbsoluteValue|ψ(s)|, the absolute 
value of psi left parenthesis s right parenthesisψ(s): 
upper P r o b left parenthesis s right parenthesis equals StartAbsoluteValue psi left parenthesis s right parenthesis EndAbsoluteValue squared periodProb(s) = |ψ(s)|2.
If an experiment can discriminate a value of an observed variablexx among many 
different possible values, then the probability of the experiment is the sum of the 
probabilities of all single possible alternatives. For example, if xx is an observed 
variable that can assume only two possible values a 1 comma a 2a1, a2, then: 
upper P r o b left parenthesis x equals a 1 o r x equals a 2 right parenthesis equals upper P r o b left parenthesis s left parenthesis x right parenthesis equals a 1 right parenthesis plus upper P r o b left parenthesis s left parenthesis x right parenthesis equals a 2 right parenthesis periodProb(x = a1 or x = a2) = Prob(s(x) = a1) + Prob(s(x) = a2).
Conversely, let xx be a variable that is not observed, and either s left parenthesis x right parenthesis equals a 1s(x) = a1 or 
s left parenthesis x right parenthesis equals a 2s(x) = a2, then, according to superposition: 
psi left parenthesis s right parenthesis equals psi left parenthesis s left parenthesis x right parenthesis equals a 1 right parenthesis plus psi left parenthesis s left parenthesis x right parenthesis equals a 2 right parenthesisψ(s) = ψ(s(x) = a1) + ψ(s(x) = a2)
therefore: 
upper P r o b left parenthesis s right parenthesis equals StartAbsoluteValue psi left parenthesis s left parenthesis x right parenthesis equals a 1 right parenthesis plus psi left parenthesis s left parenthesis x right parenthesis equals a 2 right parenthesis EndAbsoluteValue squaredProb(s) = |ψ(s(x) = a1) + ψ(s(x) = a2)|2
but the square of a sum is different from the sum of squares, this means that there is 
interference between the two alternatives. This situation is that one of Young’s 
interference experiments revealing the wave nature of light (Wave mechanics 
is another way of denoting Shrödinger’s approach). The amplitude/probability 
interference is a crucial aspect in discriminating between observed and non-
observed variables. Namely, an event involving variables that are not observed 
follows the rule of summing amplitudes, which produces superposition, whereas, 
in an experiment observing variables the probability of more alternatives is the 
sum of the single probabilities.

156
5
Information and Randomness
8. Representation (Dirac-von Neumann) 1925–1932 
A quantum state is a vector in a multidimensional Hilbert space over com-
plex numbers satisfying Dirac-von Neumann’s Axioms (on the scalar product, 
orthogonality, linear, hermitian, and unitary operators). 
9. Pilot Wave (Bohm-deBroglie) 1952–1957 
The probability wave psiψ corresponds to a physical wave (that in the case of a 
particle guides its movement). This principle is a natural consequence of the 
wave approach in QM, and corresponds to the “localization wave” of the present 
formulation. However, it requires further theoretical and experimental develop-
ments. Moreover, the relationship between the localization wave and probability 
wave is linked to the role that randomness plays in QM. This aspect is surely a key 
to understanding the informational nature of QM. This research line is probably 
related to other ﬁelds of physical research (String Theory, Quantum Gravity), 
where the submicroscopic structure of space (called plenum by David Bohm) 
could reveal fundamental informational aspects of physical reality, summarized 
by the famous John Wheeler’s motto it from bit. 
10. Non-Locality (Entanglement) (Bell) 1964–1982 
The principle of locality in physics establishes that the state of a system depends 
only from what is spatially “close” to it. In 1964, the British physicist John Bell, 
developed an analysis, originating from a version of EPR paradox due to David 
Bohm. This paradox was formulated by Einstein, Podolsky, Rosen for showing 
that QM has something wrong or incomplete. Bell found an inequality that has to 
be satisﬁed by any physical theory that is in agreement with the locality principle. 
From 1972 up to 1982, the physicists Alain Aspect, John F. Clauser, and Anton 
Zeilinger conducted experiments conﬁrming that Bell’s inequality does not hold 
in some quantum systems. In these systems, it is possible that two particles, 
even at a very large distance, behave as if they were a unique reality, because 
any change in one of them, instantaneously, produces a corresponding change in 
the other one. This phenomenon called entanglement is a speciﬁc feature of QM. 
In 2023 the physicists who experimentally proved quantum non-locality received 
the Nobel prize for Physics. Namely, quantum entanglement represents a frontier 
for theoretical and applicative developments of QM (quantum computing and 
quantum cryptography). 
References 
1. Feller, W.: An Introduction to Probability Theory and Its Applications. Wiley, New York (1968) 
2. Bonnici, V., Manca, V.: An informational test for random ﬁnite strings. Entropy 20(12), 934 
(2018) 
3. NIST, National Institute of Standards and Technologies. A Statistical Test Suite for Random 
and Pseudorandom Number Generators for Cryptographic Applications. Gaithersburg, MD 
20899-8930, Revision 1a (2010)

References
157
4. Hastings, W.K.: Monte Carlo sampling methods using Markov chains and their applications. 
Biometrika 57(1), 97–109 (1970) 
5. Havil, J.: Gamma: Exploring Euler’s Constant. Princeton University Press, Princeton (2003) 
6. Brin, M., Stuck, G.: Introduction to Dynamical Systems. Cambridge University Press, Cam-
bridge (2002) 
7. Bonnici, V., Manca, V.: Informational laws of genome structures. Sci. Rep. 6, 28840. http:// 
www.nature.com/articles/srep28840 (2016) Updated in February 2023 
8. Bonnici, V., Manca, V.: Author correction: informational laws of genome structures. Sci. Rep. 
13, 3422 (2023) 
9. Chaitin G.J.: Algorithmic Information Theory. Cambridge University Press, Cambridge (1987) 
10. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information. Probl. Inf. 
Transm. 1, 1–7 (1965) 
11. Martin-Löf, P.: The deﬁnition of random sequences. Inf. Control 9, 602–619 (1966) 
12. Borel, E.: Les probabilities denomerable et leurs applications arithmetiques. Rend. Circ. Mat. 
Palermo 27, 247–271 (1909) 
13. Borel, E.: Leçons sur la théorie des functions. Gauthier-Villars (1914) 
14. L’Ecuyer, P.: History of uniform random number generation. In: Proceedings of the 2017 Winter 
Simulation Conference, pp. 202–230 (2017) 
15. L’Ecuyer, P.: Random number generation with multiple streams for sequential and parallel 
computers. In: Proceedings of the 2015 Winter Simulation Conference, pp. 31–44. IEEE Press 
(2015) 
16. Marsaglia, G.: DIEHARD: A Battery of Tests of Randomness (1996). http://stat.fsuedu/~geo/ 
diehard.html 
17. Nielsen, M.A., Chuang, I.L.: Quantum Computation and Quantum Information. Cambridge 
University Press, Cambridge (2000) 
18. Nies, A.: Computability and Randomness. Oxford University Press, Oxford (2009) 
19. Volkenstein, M.V.: Entropy and Information. Springer, Berlin (2009) 
20. Brush, S.G., Hall, N.S. (eds.): The Kinetic Theory of Gases. An Anthology of Classical Papers 
with Historical Commentary. Imperial College Press, London (2003) 
21. Sharp, K., Matschinsky, F.: Translation of Ludwig Boltzmann’s Paper “On the relationship 
between the second fundamental theorem of the mechanical theory of heat and probability 
calculations regarding the conditions for thermal equilibrium.” Entropy 17, 1971–2009 (2015) 
22. Planck, M.: Planck’s Original Papers in Quantum Physics. Annotated by Kangro, H., translated 
by Haarter D., Brush, S.G. Taylor & Francis, London (1972) 
23. Schrödinger, E.: An undulatory theory of the mechanics of atoms and molecules. Phys. Rev. 
28, 6 (1926) 
24. Wiener, N.: Cybernetics or Control and Communication in the Animal and the Machine. Her-
mann, Paris (1948) 
25. Wheeler, J.A.: Information, physics, quantum: the search for links. In: Zurek, W.H. (ed.) Com-
plexity, Entropy, and the Physics of Information. Addison-Wesley, Redwood City, California 
(1990) 
26. Carnot, S.: Reﬂections on the motive power of heat (English translation from French edition 
of 1824, with introduction by Lord Kelvin). Wiley, New York (1890) 
27. Manca, V.: An informational proof of H-Theorem. Open Access Lib. (Modern Physics) 4, 
e3396 (2017) 
28. Manca, V.: Infobiotics: Information in Biotic Systems. Springer, Berlin (2013) 
29. Bohm, D.: Quantum Theory. Dover (1989) 
30. Dirac, P.: The Principles of Quantum Mechanics. Oxford University Press, Oxford (1958) 
31. Susskind, L., Friedman, A.: Quantum Mechanics. Basic Books, The Theoretical Minimum 
(2014) 
32. Lederman, L.M.: Quantum Physics for Poets. Prometheus Books (2011) 
33. Bohm, D.: Causality and Chance in Modern Physics. Routledge & Kegan Paul, London (1957) 
34. Bell, J.S.: Speakable and Unspeakable in Quantum Mechanics. Collected Papers on Quantum 
Philosophy. Cambridge University Press, Cambridge (2004)

158
5
Information and Randomness
35. Fagin, F.: Consciousness comes ﬁrst. In: Consciousness Unbound. Rowman& Littleﬁeld, Lan-
ham (MD) (2021) 
36. D’Ariano, G.M., Perinotti, P.: Derivation of the dirac equation from principles of information 
processing. Phys. Rev. A 062106 (2014) 
37. D’Ariano, G.M., Chiribella, G., Perinotti, P.: Quantum Theory from First Principles. An Infor-
mational Approach. Cambridge University Press, Cambridge (2017) 
38. Schrödinger, E., What Is Life? the Physical Aspect of the Living Cell and Mind. Cambridge 
University Press, Cambridge (1944)

Chapter 6 
Life Intelligence 
Introduction 
Let us introduce the theme of the relationship between life and intelligence by report-
ing the recent success of AI (Artiﬁcial Intelligence). Protein folding is one of the 
most challenging problems of biology. In fact, when a protein is synthesized from an 
RNA transcript to a sequence of amino acids, according to the genetic code deﬁned 
over codons (by means of ribosomes and RNA-transfer elements), in a very short 
time, it assumes a spatial form due to many kinds of repulsion and afﬁnity forces 
between the different amino-acids of the sequence. This form determines the protein 
function. 
Guessing the form associated with a given sequence of amino acids is a very com-
plex task that has remained unsolved for decades. Recently, a solution was found, 
which was based on Alpha-go, a machine learning program used in designing a 
machine able to play go, at a level of competence able to win against a world 
champion. The same program, suitably instructed along years of training, solved 
the protein folding, by obtaining an epochal success of artiﬁcial intelligence. In this 
way, science solved the problem, but without knowing exactly how the solution was 
obtained. This is a peculiar aspect of human intelligence, giving us behavioural com-
petencies, acquired by training, but often conﬁned at the level of our unconscious 
neural activity, such that we are able to do something, but we do not know how we 
do it. In the same manner, an AI algorithm is something internalised in a structure, 
as a path in a suitable space, directing from a target toward a corresponding solution, 
where it is unknown the relationship between the path and the solution and between 
the training and the emergence of the path. However, we know the meta-algorithm, 
that is, the rules that allow a suitable structure to evolve, along the training, toward 
a conﬁguration where the solving path can emerge. 
Let us give a preliminary approximate deﬁnition of intelligence. This is, of course, 
a philosophical question but it has a huge impact on deﬁning intelligent theoretical 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_6 
159

160
6
Life Intelligence
frameworks and on implementing them. A past deﬁnition of intelligence was related 
to the emulation, or similarity, of human activities. An entity was considered intel-
ligent if it was able to act as a human brain. It is today clear that human intelligence 
is not the only form of intelligence on our planet. Many other animals show similar 
forms of intelligence. In animals, intelligence is often a property of an individual 
which is associated with the brain, and thus with neurons. For this reason, many 
modern computational models of intelligence try to emulate the brain. In fact, artiﬁ-
cial neural networks (ANN) are inspired by animal brains, and in particular, they try 
to emulate the complex network of neurons of which a brain is composed. However, 
many different forms of intelligence exist which do not (as far as our knowledge) rely 
on neurons. Plants are intelligent organisms that interact with the environment and 
are also able to communicate, but they do not have neural cells. Similarly, microbes 
are unicellular organisms showing the same capabilities, even if they are made of 
a single cell. Thus, intelligence is something more abstract that the results of the 
connections of neurons. A modern deﬁnition of intelligence deﬁnes a (complex) 
system as intelligent if such a system is able to: (i) retrieve information from the 
environment; (ii) solve a speciﬁc task/problem by building a strategy by exploiting 
the retrieved information; (iii) store the information and the strategy in order to face 
up to the same problem in future times, promptly; (iv) exploit the learned strategy 
to solve similar problems. An intelligent system may be formed by more than one 
entity, such that the entities interact to obtain the wanted capabilities. 
All forms of intelligence present some fundamental aspects that are indepen-
dent of their speciﬁc realizations. Typical structures of intelligent systems are net-
works, as in animal nervous systems consisting of neurons and synapses among 
them. However, substances connected by biochemical transformations of molecules 
(reactants) into other molecules (reagents) provide other kinds of networks. Anal-
ogously, genes connected among them and with proteins (factors and co-factors) 
by activation and inhibition relations provide other very rich and sophisticated net-
works. In this sense, life is based on networks, starting from the metabolic level of 
biochemicals, up to the genetic and cellular levels, and beyond them, with species 
and ecosystem networks. All these systems exhibit very big connectivity, which 
can be deﬁned as hyper-connectivity. Moreover, very often “intelligent” structures 
are hyper-heterogeneous (with many different categories of components), hyper-
dimensional (dependent on several parameters), hyper-stratiﬁed and hyper-modular 
(organised in functional units). Finally, random events very often affect their dynam-
ics. 
For what concerns modern computational models, emerging in the context of 
artiﬁcial intelligence, the retrieving of speciﬁc information from the environment is 
today a mostly solved problem. There are plenty of sensor technologies that scan the 
environment and produce digital data. Starting from the acquired data, the creation 
of data-driven strategies and their exploitation is the main focus of the so-called 
machine learning models, which have shown incredible performance in recent years. 
In particular, modern ANNs are very good at solving problems of classifying objects, 
namely, assigning an object to a speciﬁc predeﬁned class by observing the features 
of such an object. These solutions postulate that a problem can be solved by learning

6
Life Intelligence
161
the parameters of a speciﬁc computational model. The learning process is based on a 
learn-by-example approach in which example data are used to learn the parameters. 
This implies that examples must be available before the learning phase and, often, 
the number of available examples affects the performance of the learned model. In 
fact, in general, the more input example is available the higher the performance of 
the model is. Unfortunately, it is very uncommon to observe machines with ﬂexible 
and wide-spectrum learning strategies. In fact, it is usual that models are trained on 
data that regard a speciﬁc problem, thus they become able to solve just that speciﬁc 
problem. It has to be noticed that, instead, the capability of re-purposing a strategy 
developed for a different problem to another problem, as well as the reusing of 
information previously acquired for a different task, remains a capability peculiar to 
living organisms. 
Organisms aggregate to form communities, where particular forms of intelligence 
arise from such aggregation. In this context, swarm intelligence and, more in general, 
forms of collective intelligence are well-known to scientists and are sometimes used 
as inspiration for computational models. At the bottom level of life intelligence, we 
may ﬁnd genomes. They are surely the directors of living cells, thus they may seem 
intelligent entities, but something more interesting regards them. In fact, if we see 
genomes not as single entities but as a population, which evolves over time, then 
a different kind of intelligence emerges. This intelligence differs from collective 
intelligence because the entities that are interested do not collaborate in an explicit 
way, and often they do not belong to the same temporal frame. Thus, the intelligent 
genomic system is not a single genome, nor a single lineage of genomes, but it is the 
set of genomes which has evolved to adapt to a given environment. In this perspec-
tive, genomes constantly interact with the environment, which deﬁnes the constraints 
for their survival. They acquire information by sensing the environment, but this is 
not the primary form with which they learn. In fact, their learning process is more 
related to a try-and-survive-or-die approach. They made random choices which lead 
to characteristics of the organisms they implement which advantageous or disad-
vantageous property is determined a posteriori by the environment. However, such 
choices are not uniformly (purely) random, but they take into account the information 
that the system has previously acquired. Once a genome learns that a given choice is 
advantageous, it tries to keep such a choice for future generations and to protect it. 
In this perspective, genomes have developed strategies to recover from DNA reading 
errors and to reduce the variation level of certain regions of the DNA molecules. 
Once a certain choice, namely, a variation in the genomic sequence, has been made, 
it is used to implement a potential set of functions of the living cell/organism. Thus, 
the function that appears to be advantageous is only one with such a set of potential-
ities. This behaviour gives genomes, and more in general to all biological systems, 
the capability of being able to solve new previously-unseen problems by exploit-
ing solutions to old already-seen problems. Scientists had exploited the knowledge 
regarding genomes and biological systems to produce computational models for 
solving problems, which are brieﬂy described below in this chapter. A very general 
aspect emerges for this knowledge, that is the exploitation of “informed” random

162
6
Life Intelligence
variables. Such a random engine is used to make choices that bring to potentially 
unexpected functions, but, at the same time, it is built by a learning process which 
preserves advantageous past choices. Such advantageous choices represent a piece 
of information that the system acquires by testing and sensing the environment. 
Two important aspects seem to be necessary for the emergence of intelligent 
behaviours, within a given structure, as it happens in biological networks: (1) mech-
anisms of memorisation, (2) mechanisms of reﬂexivity, where some parts become 
able to control the whole structure, showing a sort of conscience. Genomes, probably, 
emerged as sorts of memories of complex enzymatic activities, responsible for cell 
competencies. Moreover, any genome includes a kernel that is able to control the 
preservation and a coherent unfolding of biological knowledge accumulated along 
its evolution. 
In what follows, we describe the three main types of bio-inspired computational 
models by putting a special focus on how they exploit random variables to reach their 
goals. The ﬁrst two types, which are genetic algorithms and swam intelligence, are 
usually applied to solve combinatorial or optimisation problems. Instead, the third 
type, namely the artiﬁcial neural networks, is generally applied to solve classiﬁcation 
problems, but they ﬁnd applicability to other types of problems. 
6.1 
Genetic Algorithms 
The resolution of combinatorial problems, and in particular constraint satisfaction 
problems, are within the range of applicability of genetic algorithms [ 1]. A constraint 
satisfaction problem (CSP) consists in ﬁnding a speciﬁc assignment of values to a 
given set of variables such that a predeﬁned set of constraints is satisﬁed [ 2]. For each 
variable, a set of possible values is allowed, and different variables may share possible 
values. Because of their combinatorial nature, many CSP problems are NP-Complete 
problems and no polynomial algorithm exists for their resolution. A well-known CSP 
problem is the graph colouring problem in which we want to assign a colour to the 
vertices of a graph such that the same colour is never assigned to two neighbouring 
vertices [ 3]. Formally, a graph is a pair left parenthesis upper V comma upper E right parenthesis(V, E) such that upper V equals StartSet v 1 comma ellipsis comma v Subscript n Baseline EndSetV = {v1, . . . , vn} is the set 
of vertices and upper E element of upper V times upper VE ∈V × V is the set of edges between such vertices. Given a set 
of colours upper CC, the goal is to ﬁnd a function m colon upper V right arrow from bar upper Cm : V {→C such that for all left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis element of upper E right double arrow m left parenthesis v Subscript i Baseline right parenthesis not equals m left parenthesis v Subscript j Baseline right parenthesis∀(vi, v j) ∈E ⇒
m(vi) /= m(v j). The CSP problem is modelled such thatupper VV is the set of variables,upper CC
is the set of values and upper EE deﬁnes the constraints to be veriﬁed. 
A genetic algorithm aims at ﬁnding mm by iterative steps which emulate the evo-
lution of genomes and their natural selection due to environmental constraints. Each 
individual genome represents an assignment of values, one for each variable, which 
partially satisﬁes the set constraints. Thus a valid assignmentmm is obtained by evolv-
ing and reproducing a population of genomes. Thus, genomes are strings of length 
StartAbsoluteValue upper V EndAbsoluteValue|V | over the alphabetupper CC. Each positionii of a string relates to a speciﬁc vertex/variable 
v Subscript ivi. Such a relationship is the same for every genome in the population. The steps of

6.1 Genetic Algorithms
163
a genetic algorithm are initialisation, evaluation, selection, reproduction and muta-
tion. Except for initialisation, the other steps are repeated in this exact order for a 
given number of iterations. The algorithm has two main parameters: the population 
size and the number of iterations. The algorithm starts by initialising a population 
double struck upper P equals StartSet upper G 1 comma upper G 2 comma ellipsis comma upper G Subscript m Baseline EndSetP = {G1, G2, . . . , Gm} of genomes. Initial genomes are usually created using a ran-
dom process which picks up symbols in upper CC with a uniform random distribution. 
However, more sophisticated rules can be used for their creation. For example, we 
may extract two different symbols in a single extraction and enqueue them to cre-
ate the solution. Namely, we can extract 2-mers. Step 2 regards the evaluation of 
the current population with respect to their agreement to the constraints. A ﬁtness 
function is deﬁned such that the higher (lower) the value of the function for a given 
genome, the more suitable the genome is to solve the problem. For our example, 
we can deﬁne the ﬁtness function as phi colon double struck upper P right arrow from bar double struck upper Nφ : P {→N such that it represents the number 
of constraints (edges of the graph) which are violated. A genome upper G Subscript xGx representing 
a valid complete solution has phi left parenthesis upper G Subscript x Baseline right parenthesis equals 0φ(Gx) = 0 , and the maximum value for phiφ is StartAbsoluteValue upper E EndAbsoluteValue|E|. 
Thus, it is deﬁned as phi left parenthesis upper G Subscript x Baseline right parenthesis equals vertical bar left brace left parenthesis i comma j right parenthesis colon left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis element of upper Eφ(Gx) = |{(i, j) : (vi, v j) ∈E AND upper G Subscript x Baseline left bracket i right bracket equals upper G Subscript x Baseline left bracket j right bracket right brace vertical barGx[i] = Gx[ j]}| with 
1 less than i comma j less than StartAbsoluteValue upper G Subscript x Baseline EndAbsoluteValue comma i not equals j1 < i, j < |Gx|, i /= j. However, the ﬁtness function does not necessarily have to 
be deﬁned directly on the constraints. For example, the ﬁtness function may eval-
uate the number of positions of upper G Subscript xGx (namely the number of vertices of the graph) 
which contribute to violating the constraints. In this case, the function is deﬁned as 
phi left parenthesis upper G Subscript x Baseline right parenthesis equals vertical bar left brace i colon there exists 1 less than j less than StartAbsoluteValue upper G EndAbsoluteValue vertical bar left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis element of upper Eφ(Gx) = |{i : ∃1 < j < |G||(vi, v j) ∈E AND upper G Subscript upper X Baseline left bracket i right bracket equals upper G Subscript x Baseline left bracket j right bracket right brace vertical barG X[i] = Gx[ j]}|. Subsequently, a 
selection phase is run according to the evaluation results. The most suitable genomes 
are chosen to be reproduced, while less attractive genomes are discarded. Such a 
selection can be performed by ranking the genomes according to phiφ a by select-
ing the top-kk individuals. However, such a procedure may drive the entire search 
procedure to get stuck in local minima which never lead to an optimal solution. 
For this reason, more sophisticated approaches allow non-top-kk to be selected by 
deﬁning speciﬁc rules that deﬁne the probability of an unattractive solution being 
selected. Local minima are critical issues for computational algorithms aiming at 
solving maximisation (minimisation). Algorithms, such as Tabu Search and simu-
lated annealing, exploit randomness in a similar way to genetic algorithms do in 
the selection step. They allow apparently-unfeasible partial solutions to be selected 
with a (sometimes informed) random probability distribution. After the selection 
phase, a reproduction phase recombines the genetic material of survived genomes. 
A crossover operation between pairs of genomes is performed. Such a crossover can 
be made in several ways. A simple approach is to randomly pick up a position ii
of the genome and use such a position for a cut-and-paste operation. Thus, given 
two parent genomes,upper G Subscript xGx andupper G Subscript yG y, two new genomes,upper G Subscript zGz andupper G Subscript hGh, are generated such 
that upper G Subscript z Baseline equals upper G Subscript x Baseline left bracket 1 comma i right bracket dot upper G Subscript y Baseline left bracket i plus 1 comma n right bracketGz = Gx[1, i] · G y[i + 1, n] and upper G Subscript h Baseline equals upper G Subscript y Baseline left bracket 1 comma i right bracket dot upper G Subscript x Baseline left bracket i plus 1 comma n right bracketGh = G y[1, i] · Gx[i + 1, n], where nn is the 
length of the genomes. Of course, such a reproduction process can be as sophisti-
cated as wanted. More complex crossover operations can be deﬁned, and uni-sexual 
or multi-sexual reproduction can act. Once the new progeny is created, it is added todouble struck upper PP. 
Because we usually want double struck upper PP to not increase in size, the parent genomes are removed 
from the population after their reproduction. Subsequently, the new genomes are

164
6
Life Intelligence
mutated. The process was originally intended to emulate single nucleotide polymor-
phism. In general, it uses a uniform probability distribution to pick the position to be 
mutated and to pick a symbol to be assigned to such a position, but of course, it can 
be a more sophisticated process. Mutations allow the search process to visit a wider 
range of areas of the searching space, with respect to simply crossing the genomes. It 
is another powerful strategy for escaping from local minima. Evaluation, selection, 
reproduction and mutation are repeated until a solution is found. 
It has to be noticed that there is an essential difference between how genomes 
actually act and how they are emulated to solve computational problems. In compu-
tational approaches, such as the one described above for the resolution of the graph 
colouring problem, constraints are constants, namely, they do not change over time. 
In real life, genomes have to face an environment that may mutate. For this reason, 
convergent evolution acts between species of different continents. However, local 
casual facts append in speciﬁc places, and genomes have to adapt to these short-term 
events. In conclusion, the evolutionary process partially ﬁts a global ﬁxed ﬁtness 
function, thus it preserves past advantageous mutations. However, it is equipped to 
potentially react to temporary environmental changes which are faced by exploiting 
such an informed random process of adaptation. 
6.2 
Swarm Intelligence 
Social behaviours within swarms of animals (birds, or insects) have been used 
as a source of inspiration to implement artiﬁcial intelligence. In a swarm intelli-
gence algorithm, there are several entities, which correspond to elements of swarms, 
which simultaneously search for a solution. The algorithm has two main phases. An 
exploitative phase in which each entity, independently from the swarm, explores 
the proximal space in search of a better partial solution. Then, the information 
independently retrieved by each entity is shared with the whole swarm to combine 
such knowledge for driving the global search process in promising directions. Such 
approaches are then usually used in solving optimisation problems. One example is 
the pigeon-inspired optimisation (PIO) [ 4] which emulates strategies the pigeons use 
when ﬂying as a swarm during migration. Each pigeon acquires information from 
the environment, for example, the sun’s location, landmark markers and measure-
ments of the magnetic ﬁeld. Because such information is acquired individually by 
the pigeons, each individual has a personalised perspective of the environment which 
leads the pigeon to take a speciﬁc direction. However, other pigeons may have more 
useful data from the environment that can help the whole swarm to drive in a better 
trajectory. Moreover, a pigeon swarm is hierarchically organised into clusters, such 
that in each cluster a leader takes decisions and the other pigeons only share their 
information with the leader and follow the leader’s decision.

6.3 Artiﬁcial Neural Networks
165
Similar to a pigeon swarm, but without the hierarchical structure, there is the 
particle swarm optimisation algorithm (PSO) [ 5, 6]. A set of particles scan simulta-
neously the space in search of an optimal solution, for example, the minimum of a 
function with multiple parameters. Thus, each particle represents a position within 
the nn-dimensional search space that is a speciﬁc parameters assignment. Moreover, 
each particle has a speciﬁc direction within thenn-dimensional search space. At each 
step, the particles optimise their direction through an optimal (unknown) solution 
by taking into account the best position visited by the particle and the best position 
visited by any particle in the swarm. These two positions may produce contrasting 
directions, thus the resultant direction is a mediation of them by suitable weights. 
Particles are initially located in random (possibly equally spaced) positions of the 
search space, but this behaviour may be not enough to avoid the algorithm to trap 
in local minima. For this reason, often informed random choices are taken by the 
particles. Thus, the PSO algorithm is enhanced with techniques coming from the 
tabu search algorithm [ 7] or simulated annealing optimisation [ 8]. 
Is a living entity that always acts with the same pattern, by denying chances of 
random divergences, truly intelligent? Is informed randomness the key feature for 
intelligence, namely for adapting to a continuously changing environment? 
6.3 
Artiﬁcial Neural Networks 
The ﬁrst discrete mathematical model of nervous systems was Neural Networks 
(NN), deﬁned in 1943 by Warren McCulloch (neuroscientist) and Walter Pitts (logi-
cian) [ 9– 11]. A NN is given by “neurons” assuming Boolean values and having enter-
ing and exiting edges. Input neurons take input signals from outside in their entering 
edges, while output neurons send externally along their exiting edges response sig-
nals as output. Internal neurons are connected with other neurons by internal edges 
(synapses). At each step, any neuron receives values along its entering edges, applies 
to them a transfer function, called also activation function, and sends the result 
to the connected neurons through its exiting edges (that are entering edges of the 
receiving neurons). In McCulloch and Pitts’s original version, if the sum of entering 
values is greater or equal to a given threshold, the neuron sends result 1 along its 
exiting edges, otherwise, it sends result 0. Some entering edges can be inhibitory and 
if at least one inhibitory edge enters value 1 into a neuron, it sends 0 as its result. 
McCulloch and Pitts’s model was shown to be computationally equivalent to 
ﬁnite-state machines [ 11]. It is interesting to remark that the ﬁrst project of a modern 
electronic machine (program driven), elaborated in 1945 by John von Neumann, 
and called EDVAC (Electronic Digital Variable Automatic Calculator) was based on 
circuits directly related to McCullough and Pitts neural networks. 
The initial NN model, conceived within the classical paradigm of computing 
machines, evolved in time through different variants, usually called ANN (Artiﬁcial 
Neural Networks), for their perspective, related to artiﬁcial intelligence, in solving

166
6
Life Intelligence
problems typical of intelligent behaviours: classiﬁcation, recognition, and constraint 
satisfaction. 
Further versions of neural networks extended the values of neurons from Boolean 
to real values, and edges were provided by parameters (negative parameters play a 
role in inhibition). Transfer functions became real functions of many arguments and 
of different types (sigmoid-like functions are very usual). In these ANN, transfer 
functions apply to the values arriving along entering edges, after weighting them by 
the parameters of the edges, and the result is sent along the exiting edges [ 12]. 
In more than ﬁfty years of research in ANN, many structural and algorithmic 
principles were elaborated [ 13– 22] that have provided a surprising efﬁciency and 
a wide gamma of applications. Texts [ 12, 23] are recommended for ﬁrst-level and 
second-level introductions on ANN, respectively. Outlines of the history of AI can 
be found in [ 24]). 
More powerful ANNs exhibit intelligent competencies, including abilities in per-
forming: complex forms of: (1) association, (2) accommodation, (3) assimilation, 
(4) integration, (5) selection, (6) memorisation, (7) abstraction. 
Association is the ability underlying any activity of classiﬁcation or recognition. 
Accommodation is the ability to adapt an acquired knowledge for coping with a new 
situation, while assimilation expresses the ability to reconstruct an internal model of 
an external reality for elaborating an adequate reaction to it. Integration is the ability 
to combine abilities already developed. Selection refers to the search for values that 
satisfy some given constraints. Memorisation is the ability to retain traces of all the 
previous abilities in order to access them, as directly available when the environment 
requires them. Abstraction is the ability to eliminate details while keeping the main 
features of a given pattern according to the needs of a situation of interaction. 
The following ﬁgures want to support the neurological intuition on which ANNs 
are based, but also the consideration that their nature is similar to many different 
kinds of biological networks: metabolic networks, gene activation networks, or social 
interaction networks. All these structures have a common mathematical basis, even if 
with many possible variants: a graph formed by nodes (of many possible types) and 
edges (of many possible types) connecting nodes. It is important to remark that in 
the picture of Fig. 6.5, for a neuron there are many entering edges and many exiting 
edges, but exiting edges send all the same output, therefore the information elaborated 
by each neuron has a ﬂow, directed from inputs to outputs, which corresponds to a 
function of many arguments providing one result (Figs. 6.1, 6.2 and 6.3). 
In the network of Fig. 6.6, the substances associated to two circles provide oscil-
lating values, obtained by using null initial values and suitable linear functions as 
“regulators” (values of two regulators acting on the same reaction are summed) [ 25]. 
Moreover, the graph of Fig. 6.6 is a second-order graph, because meta-edges connect 
nodes with edges. Graphs and multi-graphs of any ﬁnite order can be easily deﬁned. 
It is easy to show that these kinds of graphs can be completely encoded in simple 
(ﬁrst-order) graphs (nodes-edges, or objects-arrows) by using suitable labeling of 
nodes and edges. For example, the multi-edge of Fig. 6.4 can be seen as a simple

6.3 Artiﬁcial Neural Networks
167
Fig. 6.1 Cajal’s model of neurons 
Fig. 6.2 The schema of an internal neuron. The ﬂow of elaboration goes from left (dendrites) to 
right (synapses), along the axon that sent its signal to axon terminals (Wikipedia) 
graph of eight nodes where the central node is of a different type with respect to the 
others. 
Metabolic networks can compute all functions computed by Turing machines 
[ 27, 28]. But, it is interesting that they can represent an algorithm by spreading the 
whole structure, by means of suitable initial values of substances and by the func-
tional forms of their regulators. Regulators are functions that establish the quantity 
of matter passing through edges. They correspond to the transfer functions of ANN. 
MP grammars, or MP systems, are a class of metabolic networks with great expres-
sive power in describing many kinds of biological dynamic [ 29– 34], which can be

168
6
Life Intelligence
Fig. 6.3 Dendrite-Synapse connections (Wikipedia) 
Fig. 6.4 Neuron abstract schema as a multi-edge. The central base arrow is the neuron. White 
circles are “synapses”. The edges connecting the arrow (axon) with synapses are “dendrites” 
Fig. 6.5 Schema 
neurons-synapses.This 
representation is simpler 
than that of Fig. 6.4, but  
conceptually equivalent to it: 
circles are neurons and 
synapses are edges

6.3 Artiﬁcial Neural Networks
169
Fig. 6.6 A graph modelling an oscillating metabolism. Substances are triangles (input and output 
substances) and circles. Edges are reactions and wavy edges are “meta-edges” expressing regulators. 
Any regulator is a function of its source nodes giving, in each state, the quantity of substance 
transformed by its target reaction 
Fig. 6.7 A metabolic 
network. Substances are 
triangles and rectangles, 
reactions are ellipses, and 
regulators are stars 
translated into suitable ANNs and vice versa. Moreover, any gene activation network 
can be expressed by a suitable metabolic network [ 35] (the metabolic network of Fig. 
6.7 was derived by a phenomenon of gene activation [ 36]). This means that the three 
main types of biological networks show a common deep nature in their mathemati-
cal structures. essentially represented by systems of equations, as it is shown in the 
simple case of the ANN in Fig. 6.9 (Fig. 6.8). 
In the general schema of an ANN, nodes are divided into three groups: input 
neurons, internal neurons and output neurons. The ﬁrst group receives input external 
signals, while the last group sends their values as external signals. 
Output neurons are responsible for producing the desired outcome of the model. 
For example, if the ANN is developed for a classiﬁcation process required to assign 
one among two classes, then the output neurons codify inputs in one of two classes. 
Internal neurons are the key elements of an ANN. They are responsible for pro-
cessing the input signal into the output values by means of intermediate evaluations. 
In general, an ANN implements a mathematical function obtained by combining 
the transfer functions of its neurons, according to the parameters associated with 
their edges. 
A radical change in ANN research was due to the Machine Learning approach 
(ML), which introduced a sort of inversion in the link between computation and 
function. In classical models of computation, devices’ computing functions start from

170
6
Life Intelligence
Fig. 6.8 A metabolic network, where metabolic ﬂuxes are controlled by ANNs (from [ 26]) 
data by providing, at the end of computation, some associated results. Differently, a 
machine learning system does not compute the results associated with some data, but 
infers a function that ﬁts, within some approximation threshold, a given set of data-
results pairs. In this way, the function is not the knowledge internal to the machine 
but it is the result of its computation that is derived according to a suitable strategy 
from a data set of examples. Of course, this approach naturally introduces viewpoints 
based on approximation and probability. 
The term “Machine Learning” was coined in 1952 by Arthur Samuel, a computer 
scientist at IBM and a pioneer in AI and computer gaming [ 37] (see [  38], for a 
classical textbook on ML). The inversion of the Turing computational paradigm 
is intertwined with statistical and probabilistic aspects. In a sense, this it is rooted 
in Shannon’s approach to information, based on the intrinsic relationship between 
digital and probabilistic information [ 39, 40]. 
The training phase of a learning ANN aims at minimising the error that the ANN 
produces by its output neurons, with respect to a function that the network is required 
to compute for acquiring a given competence. Therefore, an error function is deﬁned 
for expressing the gap between expected and computed functions. Such an error is 
globally minimised by successive training iterations in a conceptual learning pro-
cedure (elaboration of rules and strategies from examples). The main algorithm for

6.3 Artiﬁcial Neural Networks
171
realizing such a procedure is backward propagation, or simply  backpropagation 
[ 23, 41]. The basic idea of such a procedure is that of propagating errors on output 
neurons to neurons preceding them in the ANN structure. In this way, the parameters 
of the mathematical functions implemented by the neurons are iteratively optimised. 
Backward propagation produces high-performance ANN but comes with a very high 
computational cost, therefore many variants have been proposed, which usually gain 
more efﬁciency by using probabilistic strategies. 
Backpropagation, is an old idea, based on the gradient of functions and related to 
least square approximation, going back to Augustine Cauchy, Jacques Hadamard, and 
Richard Courant. It is called “method of steepest descent (for non-linear) minimiza-
tion problems)” in a paper of 1944 by Haskell Curry (the inventor of Combinatory 
Logic) cited in [ 42], where a stepwise version of the method is outlined. Paul Werbos 
proposed in 1974, in his dissertation, the process of training artiﬁcial neural networks 
through backpropagation of errors. The idea was applied, in 1986 in a paper [ 19], 
which had an enormous impact in the community of ANNs. It was the algorithmic 
ingredient that catalyzed a key passage from “computing machines” to “learning 
machines”. If we consider that the least square approximation goes back to Adien-
Marie Legenddre and Friedrich Gauss, we can realize as backpropagation is rooted 
in classical methods of mathematical tradition. 
The backpropagation algorithm can be applied to any systems of equations involv-
ing derivable functions, therefore its ability in learning parameters can be applied to 
ANN but also to a wide class of systems, for example, metabolic or gene activation 
networks [ 43]. 
The machine learning approach has promoted a richer relationship between com-
putation theory and the wide area of continuous mathematics related to calculus, prob-
ability, approximation, and optimization. Discrete aspects of data representation and 
programs interact with typical concepts of continuous mathematics. Namely, back-
propagation is based on the computation of gradients, and Jabobian matrices, which 
are basic notions of differential calculus, and the automatic computation of gradi-
ents was an essential ingredient of backpropagation [ 44]. This change of perspective 
can be expressed by the passage from the universality of Turing machines (Turing-
Church thesis) to the universality theorem, telling that any continuous functions from 
real vectors to real vectors can be approximated by some ANN with internal layers 
(see [ 41] for a very intuitive proof), The ﬁrst universality result which is a conse-
quence of backpropagation. Functions, which are the abstract mathematical notion 
behind computations, are learned from data by ANN, and any complex activity, going 
from recognition to translation, can be expressed by a function approximating some 
suitable continuous many-valued real function. 
Recently, machine learning techniques have improved by using deep neural net-
works, in which neurons are structured into layers, and the number of such layers 
is relatively high. Deep neural networks, often provided by probabilistic mecha-
nisms, improve in accuracy and efﬁciency ANN capability of approximating con-
tinuous functions [ 23, 45]. This approach was possible through the introduction of 
hardware, in particular, General-Purpose Graphic Processing Units (GP-GPU) with 
computation capabilities that were previously unavailable.

172
6
Life Intelligence
Deep ANNs have revolutionised the ﬁeld of machine learning. However, their best 
contribution has probably been given by their integration with search algorithms. In 
fact, their application to learning random distributions, integrated into the Monte-
Carlo Tree Search approach, constitutes the core strategy of AlphaFold [ 46], the 
system which solved the 50-year-old problem of protein folding. It can predict the 
spatial form of a protein from the sequence of its amino acids. The related algorithm 
was developed by DeepMind, soon after its AlphaGo, which in 2016 beat world Go 
champion Lee Sedol at the game. AlphaGo represents the state space as a tree and 
explores it partially. Given a state, the next state is chosen according to a heuristic 
which aims at predicting, in a very efﬁcient way, the gain or the loss in choosing 
such a move for future states. The heuristic is driven by a simulation which results 
in a suitable probability distribution. 
Neurology can suggest further developments of actual ANN, which, at the same 
time, can clarify neurological mechanisms in the path toward consciousness. For 
example, Eric Kandel [ 47] (Nobel laureate) developed in the last century a theory, 
supported by many experiments, on a primordial organism, called Aplysia, by show-
ing, in a basic circuit of only four neurons, a phenomenon of general relevance, found 
in other animals. Namely, the repeated activation of a neuronal circuit, under some 
speciﬁc stimuli, provides the development of a new synapse that in a certain sense 
encodes the activated circuit promoting its rising. Therefore it results in a memory 
of acquired competence. The process involves a complex interaction of many subtle 
biological aspects: the activation of some genes in a neuron, the synthesis of a pro-
tein promoting the growth of the synapse, through its peripheral localisation, and as 
a consequence of the speciﬁc nature of this protein, being a prion, the assumption 
of a stable form that propagates to all proteins of the same kind, which enforces a 
perpetual form associated to the event causing the circuit activation. 
ANNs can be seen as systems of equations. In fact, let us associate a variable 
upper XX to each neuron, and let us denote by upper X left parenthesis i right parenthesisX(i) the value that the neuron assumes 
in correspondence to inputs ii. If  f f is the transfer function of the neuron, and 
w 1 comma w 2 comma ellipsis comma w Subscript n Baselinew1, w2, . . . , wn the weights of its entering wedges, which are also exiting edges 
of neurons upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n BaselineX1, X2, . . . , Xn, then the value upper X left parenthesis i right parenthesisX(i) is given by: 
upper X l ef t parenthesis i right p a r e nth esis equals f left parenthesis w 1 upper X 1 left parenthesis i right parenthesis comma w 2 upper X 2 left parenthesis i right parenthesis comma ellipsis comma w Subscript n Baseline upper X Subscript n Baseline left parenthesis i right parenthesis right parenthesis
while input neurons directly take value from outside and output neurons send their 
value externally. By putting together the equations associated with all neurons, a 
system of equations is generated that completely expresses the behaviour of the 
ANN. The number of levels of internal neurons gives the degree of connectivity 
of the ANN. Assuming that transfer functions are the sum of weighted inputs, the 
following system of equations corresponds to the ANN of Fig. 6.9 with inputs a comma ba, b. 
upper U left parenthesis a comma b right parenthesis equals w 1 upper X left parenthesis a comma b right parenthesis plus w 2 upper Y left parenthesis a comma b right parenthesisU(a, b) = w1X(a, b) + w2Y(a, b)
upper V left parenthesis a comma b right parenthesis equals w 3 upper X left parenthesis a comma b right parenthesis plus w 4 upper Y left parenthesis a comma b right parenthesisV (a, b) = w3X(a, b) + w4Y(a, b)
upper W left parenthesis a comma b right parenthesis equals w 1 upper X left parenthesis a comma b right parenthesis plus w 6 upper Y left parenthesis a comma b right parenthesisW(a, b) = w1X(a, b) + w6Y(a, b)
upper Z left parenthesis a comma b right parenthesis equals w 7 upper U left parenthesis a comma b right parenthesis plus w 8 upper V left parenthesis a comma b right parenthesis plus w 9 upper W left parenthesis a comma b right parenthesisZ(a, b) = w7U(a, b) + w8V (a, b) + w9W(a, b)

6.3 Artiﬁcial Neural Networks
173
Fig. 6.9 ANN with 6 neurons 12 edges 2 input neurons and 1 output neuron. The system of 
equations associated with it is given above. Input neurons correspond to variables that occur only 
in the left members of equations, while output neurons correspond to variables that occur only in 
the right members 
In this representation, an ANN is completely described by a text, or equivalently, 
by a piece of code of a programming language based on equations (and even training 
procedures producing the ANN can be suitably encoded by a text). 
When the mathematical form of the transfer functions is deﬁned, all the informa-
tion of the ANN is encoded by the weights occurring in the system of equations. This 
means that if the ANN is trained to learn a speciﬁc functionupper FF, relating input neurons 
with output neurons, this function is entirely given by the values of the weights of the 
ANN, which memorise the whole competence acquired by the ANN in the learning 
process. 
In the equation representation, it appears, even more clearly, that if a transfer 
function expresses an elementary mechanism of stimuli-response, the whole ANN is 
a suitable composition of a great number of simple stimuli-response mechanisms that 
in the ANN composition become able to develop very complex functionalities, and 
the ﬁnal weights record the history of their determination along the whole process 
of successive adjustments to the data received in the learning process. 
Figure 6.10 shows a multi-layer representation of ANN as a network of vectors 
(layers). It can be applied to represent also metabolic networks, and by means of 
it, any ANN can be easily transformed into a metabolic network and vice versa, 
This means that the two kinds of structures are equivalent. Figure 6.11 is a pictorial 
representation of the information ﬂow in a multi-layer representation. 
In order to better understand the logic of the learning mechanism via backprop-
agation, it is useful to consider the metabolic network given in Fig. 6.6. Let us 
associate the variablesupper X comma upper YX, Y (left, right) to the substances represented by the circles, 
and assume that regulators are linear functions. We obtain the following system of 
equations, for i equals 1 comma 2 comma ellipsis 1000i = 1, 2, . . . 1000 and tau equals 2 pi 10 Superscript negative 3τ = 2π10−3 (upper X left parenthesis 0 right parenthesis equals 1 comma upper Y left parenthesis 0 right parenthesis equals 0X(0) = 1, Y(0) = 0): 
StartLayout 1st Row upper X left parenthesis i tau right parenthesis minus upper X left parenthesis left parenthesis i minus 1 right parenthesis tau right parenthesis equals w 1 plus w 2 upper X left parenthesis i tau right parenthesis minus w 3 upper Y left parenthesis i tau right parenthesis 2nd Row upper Y left parenthesis i tau right parenthesis minus upper Y left parenthesis left parenthesis i minus 1 right parenthesis tau right parenthesis equals w 4 upper X left parenthesis i tau right parenthesis plus w 5 upper Y left parenthesis i tau right parenthesis minus w 6 period EndLayoutX(iτ) −X((i −1)τ) = w1 + w2X(iτ) −w3Y(iτ)
Y(iτ) −Y((i −1)τ) = w4X(iτ) + w5Y(iτ) −w6.
(6.1)

174
6
Life Intelligence
Fig. 6.10 A multi-layer representation of an ANN with one input vector, one internal and one 
output vector (all of 8 components). Any element of a vector is labelled by a variable that depends 
(via some basic transfer function) on the components of the previous (left) vector, and the value of 
the variable, at any step, gives the component of the vector (Playground free image) 
Fig. 6.11 A visualization of 
the ﬂow of information in 
ANN multi-layer 
representation 
In [ 25], in the context of a class of metabolic networks, called MP grammars 
(also metabolic grammars or MP-systems [ 48]) the values of weights are found such 
that upper XX and upper YY approximate the cosine and sine functions, respectively, by means a 
regression algorithm based on Least Square Evaluation (see [ 25, 49, 50] Chap. 3, 
[ 27, 39, 51] for the use of MP grammars in the approximation of real functions). 
The entire time series of the pairs left parenthesis i tau comma left parenthesis upper X left parenthesis i tau right parenthesis comma upper Y left parenthesis i tau right parenthesis right parenthesis(iτ, (X(iτ), Y(iτ)), for  i equals 1 comma 1000i = 1, 1000 is con-
sidered in such a way that one thousand instances of equations (6.1) are generated, 
where the right members are equated to cosine and sine values (fortau i comma i equals 1 comma 1000τi, i = 1, 1000). 
The best values of the six weights, according to the least square evaluation, are deter-
mined that minimize errors between right members of equations (6.1) and cosine-sine 
corresponding values. 
The performance of this method is very good, even if compared with available 
ANN learning the same functions, reaching an approximation order of 10 Superscript negative 1410−14. 
In the backpropagation methods, an analogous determination is obtained, but not 
at once, rather, in an incremental way of iterative adjustments (driven by the gradient 
of an error function with respect to the weights). Whence, the right weights are 
the result of an incremental accommodation to all the pairs arguments-result given 
during the training of the network. However, the sequential nature of the training,

6.4 Artiﬁcial Versus Human Intelligence
175
developed along a sequence of single steps, entails that the whole process takes into 
account, in a cumulative way, the data provided during all the steps. In the case 
of backpropagation, We could exemplify that instances of equations like (6.1) are  
solved, with respect to weights, by adding at each step a new instance, where new 
weights are determined by evaluating the error that previous instances produce with 
the old values of weights. 
The example of the sine-cosine metabolic network also shows, in a very simple 
setting, other two important aspects common to the “inverse methods” in function 
determination: (1) an unknown function is determined by combining basic functions 
(linear, in this case); (2) the six constants determined by the method result to be a 
sort of compression of all information of the thousand data given for obtaining their 
determination. 
Deep ANNs show us the importance of complex levels of connectivity in order 
to reach good approximation in the approximation of real functions, related to spe-
ciﬁc functionalities. This aspect is an a posteriori evidence of the complexity of 
metabolic transformations that we found in nature. In fact, the more a metabolic 
network is complex, the more it is efﬁcient, stable, and adaptive. The most part of 
the complexity of the stepwise regression algorithm given in [ 25] is the choice of 
the best “regressors”, that is, the basic functions, to put as regulators. This task was 
realized for metabolic networks [ 25, 50] by a kind of F-test driving the algorithm to 
the best choice of regulators. However, as deep ANN clearly indicate, it is better to 
increase the network, by leaving simple functions acting at the single neurons, rather 
than having more complex basic functions in smaller networks. 
6.4 
Artiﬁcial Versus Human Intelligence 
Recent successes of artiﬁcial intelligence (Machine Learning, Neural Networks, Evo-
lutionary and Distributed Computation, Computer Vision, Robotics, Expert Systems, 
Speech Processing, and Natural Language Processing) in many ﬁelds of application 
raise many kinds of debates about limits, perspectives, and dangers that can arise in 
the interaction between human and artiﬁcial intelligence, in the next future, when 
cognitive and social involvements of these interactions will be more complex and 
invasive. 
In particular, a new class of systems, called chatbots (chat + robots) conquered 
the attention of mass media for recent decisions of stopping and controlling their 
diffusion and use in public contexts. These systems can generate texts, from suitable 
lists of topics of interest (and lines of expansion), by a competence extracted from 
large moles of data, and exhibit composition/conversation competencies that in some 
cases seem indistinguishable from those of texts produced by well-educated people 
with an average competence and knowledge in the speciﬁc topics under consideration. 
ChatGPT, a very popular chatbot, when requested of deﬁning itself, answers in 
this way:

176
6
Life Intelligence
GPT (short for “Generative Pre-training Transformer”) is a type of language 
model developed by OpenAI. It is a neural network trained to generate human-like 
text by predicting the next word in a sequence given a large dataset of examples. 
Chat GPT refers to the use of this model in chat applications, where it can be used to 
generate responses to user messages in real-time. It is designed to be able to carry 
on a conversation by understanding the context of the conversation and generating 
appropriate responses. 
The last versions of GPT (GPT-4, 2022) are based on neural networks with a 
size comparable to the human brain (trillions of neurons and around 100 trillion 
parameters). The emergence of the ability shown by these systems was a real surprise 
for many people, and up to now many aspects are not fully understood, hence the 
comparison with the size of the human brain is very interesting, but more advanced 
analyses need to be developed in this regard: a human brain has a massively parallel 
conﬁguration of10 Superscript 121012 neurons and10 Superscript 151015 synapses in less than two litres, with a weight 
of 1 kg and operate at low power of less than 20 W. However biological neurons have 
structure extremely complex with respect to artiﬁcial neurons. 
A chatbot can write essays, solve coding problems, translate texts, and even dis-
cuss topics of interest. For it (her/him) producing coding in Python is very easy, and 
even if the ﬁrst answer can be incorrect, reformulating the question or reporting the 
error messages of some generated coding, in the end, very often, produces correct 
programming texts useful to solve the problem posed to it. In fact, in a continu-
ous conversation with it, previous interactions are remembered by it, hence from a 
sequence of follow-up questions a ﬁnal answer can be obtained that gives a working 
piece of code. 
However, in some cases, errors (due, for example, to homonymy) may generate 
products that can be inappropriate or offend the privacy of mentioned persons. For this 
reason, some national authorities took actions aimed at contrasting the free diffusion 
of some chatbots. These episodes suggest general speculations about the essence of 
comprehension and intelligence. 
In a sense, chatbots are generalizations of an old idea introduced by Alan Turing, 
about a behavioural approach to the deﬁnition of comprehension. Turing’s Test was 
realized by a program chatting with a human, through some interface by means of 
which two agents develop a dialogue. One of them is human and he is requested to 
manifest, along the conversation, when he got the impression of speaking with an 
artiﬁcial system misunderstanding the terms of their dialogue. In a sense, the average 
length of conversations where the human does not receive any misunderstanding 
impression can be considered a measure of the “conversational ability” of the artiﬁcial 
system. The natural question in this regard is that of establishing if a system able to 
develop a safe dialogue can be considered able to comprehend, and in this case, in 
which terms his/her comprehension could reasonably be deﬁned. 
Presently, chatbots are not equipped with any active memory of the whole con-
versational processes that they are able to develop. However, this does not mean that 
future versions of these systems cannot acquire such kind of ability. The main crit-
icism of the cognitive competence of chatbots is the consideration that they realize 
only a syntactic play, learned through a training process based on a huge quantity

6.4 Artiﬁcial Versus Human Intelligence
177
of data, but with no real semantic acquisition. Even if this is true, it is also true 
that in complex systems languages emerge by syntactic games that develop and 
acquire meanings in the interaction process among communicating agents. In for-
mal languages, semantics is deﬁned independently from syntax, and interpretation 
rules assign meaning to syntactic forms (well-formed formulae) deﬁned indepen-
dently from semantics. In natural languages, the interaction between syntax and 
semantics is constructed during the emergence and use of languages, and usually, 
semantics follows syntax. Child experience in developing linguistic activities gives 
many examples of mechanisms acquired without any full understanding of related 
meanings. According to this perspective, a chatbot who associates his/her interaction 
with the development of a neural system tracing functions and correlations of his/her 
activities could be an agent where the dialogue play generates primordial forms of 
comprehension. In any case, memorization is a necessary acquisition of any com-
prehension. Individual identity and consciousness are strongly based on memory, as 
recent acquisitions of neurological research deﬁnitively conﬁrm. 
In the previously outlined basic model of “comprehension machine”, another 
essential ingredient has to be introduced which is “the fuel of the motor”. In fact, no 
comprehension can be formed without a will to push toward it. In the case of many 
learning processes, what moves the attention of the learning agent is some emotional 
gratiﬁcation based on affective interactions. This means that a chatbot learning the 
meanings of what he/she says, through a suitable neural network (making connections 
and selecting functional parameters), needs to be able to develop an emotional system 
supporting the coupling between syntax and semantics (with primitive forms of 
memory and consciousness). Pain, joy, hope, and love are necessary for any real 
understanding. In conclusion, a body with all its sensory and perceptive activities 
is necessary for full comprehension, in the absence of such a corporal part, any AI 
agent can provide only an artiﬁcial, abstract, or assisted comprehension. Namely, 
it is well known that, in child educational activity, affective deﬁcits preclude even 
logical and mathematical abilities because the attention requested for dominating 
abstract ideas needs emotional support to attract and ﬁx attention on the topics of a 
learning activity. 
It is too early to say if and when some future “very intelligent” possible sce-
narios of AI could be, even partially, realized. In principle, it seems only a ques-
tion of increasing and integrating competencies that singularly are, in some forms, 
already available in the actual information technology. However, realizations could 
be blocked by quantitative limits in computational abilities, or by problems arising 
with the scale-up in complexity as far as many intelligent activities are integrated 
into autonomous individual systems. 
A remarkable aspect of the relationship between humans and artiﬁcial intelligence 
is the impact that artiﬁcial intelligence systems have in producing new forms of 
human intelligence, and, possibly, in helping to reach more accurate models of human 
intelligence. 
The educational processes with which human knowledge is transmitted and human 
minds are formed, by acquiring the individual and collective consciousness of a

178
6
Life Intelligence
culture, can dramatically change by a massive presence of artiﬁcial systems assisting 
humans in many kinds of tasks. 
In particular, the classical paradigm of mathematical proof, where passages are 
completely dominated by human intelligence, changes radically in situations where 
solutions are found by AI agents (Artiﬁcial Intelligence agents), trained by humans, 
where no complete knowledge is available for the single steps performed by the 
machine for reaching a solution. Along this way, the human approach to problems 
surely is inﬂuenced and even the meaning of “comprehension” assumes new char-
acters, when human knowledge includes pieces that are not directly accessible. 
Let us report some fragments of recent conversations we had with ChatGPT. From 
the OpenAI site (What is ChatGPT?) we report the following statement. “Subject 
to the Content Policy and Terms, you own the output you create with ChatGPT, 
including the right to reprint, sell, and merchandise “regardless of whether the output 
was generated through a free or paid plan.” 
A conversation with ChatGPT 
User 
what is ChatGPT? 
ChatGPT 
Hello! I am ChatGPT, a large language model developed by OpenAI. I am based on 
the GPT (Generative Pre-trained Transformer) architecture, speciﬁcally the GPT-3.5 
version. I have been trained on a massive amount of text data and can understand 
natural language inputs, generate responses, and perform various language-related 
tasks. Essentially, I am an AI-powered chatbot designed to communicate with humans 
in a way that feels natural and conversational. 
User 
What is a Transformer architecture? 
ChatGPT 
Transformer is a neural network architecture that was introduced in a paper titled 
“Attention Is All You Need” by Vaswani et al. in 2017. The Transformer architecture 
is based solely on the attention mechanism, which allows the network to selectively 
focus on different parts of the input sequence during processing. 
Unlike previous neural network architectures, such as recurrent neural networks 
(RNNs) or convolutional neural networks (CNNs), the Transformer architecture does 
not rely on sequential processing. Instead, it processes the entire sequence of input 
tokens at once, enabling it to parallelize training and inference and making it more 
efﬁcient than traditional RNN or CNN models. 
The Transformer architecture has been widely adopted in various natural language 
processing (NLP) tasks, such as machine translation, language modeling, and text 
classiﬁcation. In fact, the GPT-3, one of the largest and most powerful language 
models to date, is based on the Transformer architecture.

6.4 Artiﬁcial Versus Human Intelligence
179
User 
What is an “attention mechanism”? 
ChatGPT 
An attention mechanism is a key component of many neural network architectures, 
particularly those used for natural language processing tasks. It enables the net-
work to selectively focus on different parts of the input sequence during processing, 
allowing it to process the most relevant information while ignoring irrelevant or noisy 
information. 
In the context of neural networks, attention can be thought of as a way of assigning 
weights to different parts of the input sequence based on their relative importance to 
the task at hand. These weights are learned during training and are used to weight 
the contribution of each input token to the ﬁnal output of the network. 
One of the most commonly used attention mechanisms is called “self-attention” 
or “scaled dot-product attention”, which is used in the Transformer architecture. In 
self-attention, each input token is transformed into a query, a key, and a value vector. 
The query vector is then used to compute a weighted sum of the value vectors, where 
the weights are determined by the similarity between the query vector and the key 
vectors. This weighted sum is then used as the output of the self-attention mechanism. 
Overall, attention mechanisms have proven to be a powerful tool for modelling 
complex, sequential data such as natural language text, and have enabled signiﬁcant 
improvements in the accuracy of many NLP tasks. 
User 
What is a token? 
ChatGPT 
In natural language processing (NLP), a token refers to a sequence of characters 
that represents a single unit of meaning. Typically, tokens are words, but they can 
also be phrases, numbers, punctuation marks, or other types of elements that convey 
meaning in a language. 
Tokenization is the process of splitting a text into individual tokens or words. 
This is an important step in many NLP tasks, as it allows the text to be processed 
by algorithms that can only operate on structured data. For example, in a machine 
learning model that predicts the sentiment of a text, the text must ﬁrst be tokenized 
into individual words or phrases before it can be fed into the model. 
In some cases, tokenization can be a non-trivial task, particularly in languages 
where words are not separated by spaces (such as Chinese or Japanese) or where com-
pound words are common (such as German). In these cases, specialized algorithms 
may be needed to accurately tokenize the text. 
User 
Does this mean that a sentence is transformed into a sequence of tokens corresponding 
to the words of the sentence? 
ChatGPT 
Yes, that’s correct! When a sentence is tokenized, it is typically transformed into a 
sequence of tokens, where each token represents a word in the sentence. For example, 
the sentence “The quick brown fox jumps over the lazy dog” might be tokenized into 
the following sequence of tokens:

180
6
Life Intelligence
[“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”] 
Note that tokenization can sometimes involve additional processing steps, such 
as lowercasing all the tokens or removing punctuation marks, depending on the 
speciﬁc task and requirements. However, the basic idea is to break down the input 
text into smaller, more manageable units that can be processed by machine learning 
algorithms or other NLP tools. 
User 
Is each token a vector of features? 
ChatGPT 
Yes, typically each token is represented as a vector of features in natural language 
processing (NLP) tasks. These features can be thought of as numerical representations 
of various aspects of the token, such as its spelling, part of speech, or context. 
The speciﬁc set of features used to represent a token can vary depending on the 
task and the speciﬁc NLP tool being used. For example, in a sentiment analysis task, 
the features might include the token’s frequency in a corpus of positive or negative 
reviews, while in a named entity recognition task, the features might include the 
token’s capitalization and the presence of certain keywords. 
One common way to represent a token as a feature vector is to use a one-hot 
encoding, where each feature corresponds to a speciﬁc attribute of the token and is 
represented as a binary value (0 or 1). Another approach is to use word embeddings, 
which are dense, low-dimensional vectors that capture the semantic relationships 
between words based on their usage in context. 
Regardless of the speciﬁc representation used, the goal is to transform the text 
into a format that can be processed by machine learning algorithms or other NLP 
tools, allowing us to extract useful information and insights from the text. 
User 
How long is the vector associated with a word, and are all vectors of the same length? 
ChatGPT 
The length of the feature vector associated with a word can vary depending on the 
speciﬁc NLP tool being used and the representation being used for the word. 
For example, in a one-hot encoding representation, the feature vector for a word is 
typically a vector of 0’s and 1’s, with a length equal to the size of the vocabulary being 
used. Each element of the vector corresponds to a speciﬁc word in the vocabulary, 
and the element corresponding to the word being represented is set to 1, while all 
other elements are set to 0. So, in this case, the length of the vector would be equal 
to the size of the vocabulary. 
In contrast, in a word embedding representation, the feature vector for a word 
is typically a dense vector of ﬂoating-point numbers, with a ﬁxed length that is 
chosen by the user or determined by the NLP tool being used. Commonly used word 
embedding models such as Word2Vec or GloVe typically produce word vectors of 
length 100, 200, 300, or 400. 
Regardless of the speciﬁc representation used, it is important that the vectors are 
of the same length in order to enable the data to be processed by machine learning 
algorithms or other NLP tools. In order to achieve this, some NLP tools may pad

6.4 Artiﬁcial Versus Human Intelligence
181
shorter vectors with zeros or truncate longer vectors to a ﬁxed length, depending on 
the requirements of the speciﬁc task. 
User 
How many languages can GPT speak? 
ChatGPT 
GPT models are trained on large amounts of text data from a wide variety of sources 
and are designed to be able to process and generate text in many different languages. 
However, the extent to which a GPT model can "speak" or generate text in different 
languages depends on the speciﬁc language(s) that the model has been trained on. 
The original GPT model was trained on English text data, but subsequent versions 
of the model, including GPT-2 and GPT-3, have been trained on much larger and more 
diverse datasets that include text in many different languages. GPT-3, for example, 
was trained on a dataset of over 570GB of text data in multiple languages, including 
English, Spanish, French, German, Chinese, and many others. 
While GPT models are not explicitly designed to be multilingual, they are able to 
generate text in multiple languages based on the patterns and relationships they have 
learned from the training data. However, the quality and ﬂuency of the generated 
text may vary depending on the speciﬁc language and the amount of training data 
available for that language. 
User 
When you say that GPT understands a sentence, what do you mean in more precise 
terms? 
ChatGPT 
When we say that GPT “understands” a sentence, we mean that it has the ability to 
process and represent the meaning of that sentence in a way that can be used for 
downstream natural language processing tasks. 
More speciﬁcally, when a sentence is input to a GPT model, it is ﬁrst tokenized 
into a sequence of word embeddings, each of which represents a speciﬁc word in 
the sentence. The model then processes these embeddings through multiple layers 
of self-attention and feedforward networks, gradually building up a more abstract 
representation of the sentence that captures its meaning and context. 
At each layer, the model learns to attend to different parts of the input sequence and 
to integrate information from previous layers to form a higher-level representation. 
This allows the model to capture both local and global dependencies between words 
and phrases in the sentence, and to extract patterns and relationships that can be used 
to represent its meaning. 
Once the input sentence has been processed through the GPT model, the output 
is a ﬁnal hidden state vector that represents the entire sentence. This vector can be 
used as input to downstream tasks such as language modelling, question answering, 
or text classiﬁcation, where it can be used to generate predictions or make decisions 
based on the meaning of the input text. 
While we often use the term “understands” colloquially to describe this process, 
it is important to note that GPT and other language models do not truly “understand” 
language in the way that humans do. Instead, they learn to recognize patterns and

182
6
Life Intelligence
relationships in large amounts of text data and to use this knowledge to perform 
speciﬁc NLP tasks. 
User 
Ok, you say “hidden state vector”, please, could you explain better the attribute 
“hidden”? 
ChatGPT 
The term “hidden” in “hidden state vector” refers to the fact that this vector represents 
the internal or hidden state of the GPT model after it has processed the input sentence. 
The vector is not directly visible or accessible to the user, but it contains valuable 
information about the meaning and context of the input text. 
In the case of GPT models, the hidden state vector is typically a high-dimensional 
vector with several hundred or thousand dimensions, depending on the speciﬁc archi-
tecture and hyperparameters of the model. This vector is computed through a series of 
computations and transformations that take place as the input sentence is processed 
through the model. 
Despite being “hidden”, the information contained in the hidden state vector is 
extremely useful for downstream NLP tasks, such as text classiﬁcation or language 
modelling. By using this vector as input to another model or algorithm, we can 
leverage the contextual information captured by the GPT model to improve the 
accuracy and performance of our task. 
It’s worth noting that the term “hidden” can be a bit misleading, as the hidden 
state vector is not actually a secret or mysterious part of the model. Rather, it is 
simply a term used to describe the fact that this vector is not directly observable or 
interpretable in the same way that the input tokens or output predictions are. 
User 
Can we approximately say that the hidden state vector associated with a statement 
understanding is constituted by the weights of GPT’s ANN, or of a part of it, at the 
moment that the representation of this understanding is generated? 
ChatGPT 
It’s not quite accurate to say that the hidden state vector associated with a statement 
understanding is constituted by the weights of GPT’s ANN (artiﬁcial neural network), 
or a part of it, at the moment that the representation of this understanding is generated. 
The hidden state vector is actually a computational output of the GPT model that 
represents the internal state of the model after it has processed the input sentence. 
It is computed by applying a sequence of mathematical transformations to the input 
sentence, using the model’s weights and parameters, and does not directly correspond 
to the weights of any speciﬁc layer or component of the model. 
That being said, the weights and parameters of the ANN do play a crucial role in 
determining the structure and behaviour of the GPT model and are carefully tuned 
during the training process to optimize the model’s performance on a given task. 
So while the hidden state vector is not directly constituted by the model’s weights, 
it is ultimately derived from the interactions between these weights and the input 
sentence and can be seen as a reﬂection of the model’s learned knowledge and 
patterns of representation.

6.4 Artiﬁcial Versus Human Intelligence
183
User 
Ok, but when you say “a computational output” what do you exactly mean? Where 
this output can be localized? 
ChatGPT 
By “computational output”, I mean that the hidden state vector is a result of com-
putations performed by the GPT model during the processing of the input sentence. 
More speciﬁcally, the hidden state vector is produced by the ﬁnal layer of the GPT 
model, which is typically a fully connected layer that takes as input the output of the 
preceding layer and produces a high-dimensional vector as output. 
In the case of GPT models, the hidden state vector is often associated with the 
output of the ﬁnal layer of the model, which is a fully connected layer that takes 
as input the sequence of token embeddings produced by the preceding layers. The 
weights and biases of this ﬁnal layer are learned during the training process and are 
optimized to produce an output vector that captures the contextual information and 
meaning of the input sentence. 
Once the input sentence has been processed by the GPT model, the hidden state 
vector is typically stored in memory or passed to downstream algorithms or models 
for further processing. The location and format of this output will depend on the spe-
ciﬁc implementation of the GPT model and the application being used. For example, 
in the case of the original GPT model, the hidden state vector is output by the ﬁnal 
layer of the model as a tensor with dimensions (batch size, sequence length, hidden 
size). 
User 
In the multi-head approach of the transformer GPT structure, which is the criterion 
for establishing how many heads to consider in a statement? 
ChatGPT 
The number of attention heads used in the multi-head approach of the Transformer-
based GPT model is typically a hyperparameter that is set before training the model. 
The choice of the number of heads depends on several factors, including the size 
of the training data, the complexity of the task being addressed, and the available 
computational resources. 
In general, a larger number of attention heads can help the model to capture more 
ﬁne-grained patterns and dependencies in the input sequence, at the cost of increased 
computational complexity and potentially longer training times. However, there are 
diminishing returns to increasing the number of attention heads, and there may be a 
point at which further increases provide little beneﬁt. 
Empirically, the number of attention heads used in GPT models has varied across 
different versions of the model, with some models using as few as 12 heads and 
others using up to 48 heads. The original GPT model used 16 attention heads, while 
the subsequent GPT-2 and GPT-3 models used up to 48 and 96 heads, respectively. 
Ultimately, the choice of the number of attention heads is a trade-off between 
model complexity, computational efﬁciency, and task performance, and is often deter-
mined through experimentation and hyperparameter tuning. 
User 
At the end of the representation process of a statement, is a ﬁnal vector associated

184
6
Life Intelligence
with the sentence? 
ChatGPT 
Yes, in many natural language processing (NLP) tasks, a ﬁnal vector is associated with 
a sentence after the representation process is complete. This vector is often referred 
to as a sentence embedding or a sentence vector, and it represents a compressed and 
abstracted version of the original sentence that can be used for downstream tasks like 
sentiment analysis, text classiﬁcation, or machine translation. 
There are many different methods for generating sentence embeddings, but most 
of them involve some form of pooling or aggregation of the word embeddings or 
feature vectors associated with the individual words in the sentence. For example, 
one common approach is to take the average of the word embeddings or feature 
vectors, while another approach is to use a more sophisticated pooling technique like 
max pooling or self-attention. 
The length and dimensionality of the ﬁnal sentence vector can vary depending on 
the speciﬁc method used and the requirements of the task. In some cases, the sentence 
vector may be a ﬁxed-length vector with a pre-deﬁned dimensionality, while in other 
cases it may have a variable length or a dimensionality that depends on the length of 
the input sentence. 
Overall, the goal of generating a sentence vector is to provide a compact, high-
level representation of the sentence that can be used by machine learning algorithms 
or other NLP tools to make predictions or extract insights from the text. 
User 
How is realized the training of GPT? 
ChatGPT 
The training of GPT, or any other large language model based on the transformer 
architecture, involves several steps: 
Corpus Collection: The ﬁrst step is to collect a large corpus of text data that the 
model will be trained on. This can be done using a web crawler to scrape text from 
the internet, or by using existing datasets such as Wikipedia or news articles. 
Preprocessing: The text data is then preprocessed to clean and normalize the text, 
and to tokenize it into individual words or subwords. The tokens are typically mapped 
to integer IDs for efﬁcient processing. 
Architecture Design: The transformer architecture used by GPT is designed to 
process sequences of tokens in a parallel and hierarchical manner, using multi-
head attention and feedforward layers. The architecture is often pre-trained on large 
amounts of text data using an unsupervised learning objective, such as predicting the 
next word in a sequence or ﬁlling in a masked token. 
Training: During training, the model is fed batches of sequences of tokens, and 
the weights of the model are updated using backpropagation and stochastic gradient 
descent. The goal is to minimize a loss function that measures the difference between 
the model’s predictions and the true target values (e.g., the next word in the sequence). 
Fine-tuning: Once the model has been pre-trained on a large corpus of text, it can 
be ﬁne-tuned on a smaller dataset for a speciﬁc task, such as sentiment analysis or 
text classiﬁcation. During ﬁne-tuning, the weights of the model are updated using 
supervised learning, with the goal of minimizing a task-speciﬁc loss function.

6.4 Artiﬁcial Versus Human Intelligence
185
Evaluation: The ﬁnal step is to evaluate the performance of the model on a held-out 
test set, using metrics such as accuracy, F1 score, or perplexity. 
Overall, the training of GPT involves a combination of unsupervised pre-training 
on a large corpus of text data, followed by supervised ﬁne-tuning on a smaller dataset 
for a speciﬁc task. This approach has been shown to be effective for a wide range of 
NLP tasks and has led to signiﬁcant improvements in state-of-the-art performance 
on many benchmarks. 
End of the conversation 
The entire conversation was a source of basic understanding of the main ideas on the 
structure and of the functions of the chatbot. Namely, after reading many introductory 
articles about ChatGPT, the self-presentation was surely the most useful font on the 
topic. We had other conversations, on different scientiﬁc arguments. In one of them, 
the chatbot was requested to prove Nichomachus’ theorem (the square of the sum of 
numbers from 1 to n is equal to the sum of corresponding cubes) and the proof was 
correctly stated by using mathematical induction. However, when a Turing machine 
was requested to duplicate a string given in input, the program exhibited by ChatGPT 
was wrong, even with some suggestions from the user. Other errors were found in 
other programming tests, and in technical questions of formal language theory, at an 
elementary level. However, when the discussion was about the meaning of Turing-
Church Thesis, the adequacy of the answers of ChatGPT was very impressive, and 
an analogous impression was reported when the dialogue was about the universality 
theorem of ANN’s in approximating continuous real functions. 
Three considerations are in order. First, AI agents are produced by humans and 
trained by humans. This means that their knowledge is, in a sense, human knowledge 
of the second level, even when it realizes activities beyond the direct control of 
humans. In other words, AI agents can be deﬁned as intellectual prostheses of human 
intelligence. 
Secondly, in a sense from the reported conversation, a sort of model of chatbot 
comprehension emerges. In fact, a vector of very large dimension can be associated, 
which is an internal representation of the statement used by the machine in further 
elaborations for dialogue development. This vector derives from the token vectors 
associated with the components of the statement, by using dictionaries and attention 
matrices learned during the learning process. 
Thirdly, the knowledge of a chatbot is usually internalized during the training 
phase, where he/she learns to react correctly to some given inputs within a speciﬁc 
class of data. Extending the training, with a larger or updated set of data, determines 
a greater competence of the agent. This means that knowledge acquisition is not an 
autonomous process, but remains a human-driven activity, where a crucial element 
is missing, that is, an intrinsic internal autonomous and free dynamics aimed at 
continuously acquiring new knowledge: just the kind of activity, that in a collective 
manner, pushed humans to realize chatbots. 
Finally, our brain is a network of neurons, in some way, written inside our genomes, 
because all functions of living organisms are encoded in genomes, as any kind of 
information can be expressed by strings, This information passes from its potential

186
6
Life Intelligence
state, at the genome level, to its active state of directing actions, or from syntax 
to semantics, by means of networks: metabolic, genetic, hormonal, immunological, 
neurological. The last kind of network provides support to the more complex activities 
of living behaviours. 
Maybe it is now the time that artiﬁcial intelligence provides us with new forms of 
comprehension, by using suitable kinds of ANN for helping humans to understand 
genomes. 
A ﬁrst suggestion could be the following. Evolution is the essence of life, but its 
internal logic remains in many aspects unknown. What is really difﬁcult to grasp is 
its ability in combining casualty with ﬁnality. It seems unbelievable that its search in 
the space of possibilities could be so effective and efﬁcient. In some computational 
experiments on evolutionary systems [ 52] it emerges that when these systems have 
learning abilities, then evolution is much more efﬁcient. Therefore, are there learn-
ing mechanisms internal to genomes? In other words, is there some sort of Genome 
Learning that drives evolution? Can machine learning help humans in discovering 
such genome learning? A very basic observation is that genomes memorize a quan-
tity of information that exceeds the needs of life functions within the organisms. 
Only a very small portion of the human genome encodes proteins or RNA strands 
with regulatory or informational roles. Therefore, it is reasonable to infer that the 
remaining part of genomes can be explained in terms of evolutionary dynamics. 
Probably, the corresponding information drives learning mechanisms able to ﬁnalize 
the evolutionary steps of genomes, by selecting speciﬁc paths. If this assumption is 
correct, in which way this happens? 
References 
1. Katoch, S., Chauhan, S.S., Kumar, V.: A review on genetic algorithm: past, present, and future. 
Multimed. Tools Appl. 80, 8091–8126 (2021) 
2. Lecoutre, C.: Constraint Networks: Targeting Simplicity for Techniques and Algorithms. 
Wiley, New York (2013) 
3. Hell, P.: From graph colouring to constraint satisfaction: there and back again. Algorithms 
Comb. 26, 407 (2006) 
4. Duan, H., Qiao, P.: Pigeon-inspired optimization: a new swarm intelligence optimizer for air 
robot path planning. Int. J. Intell. Comput. Cybern. 7(1), 24–37 (2014) 
5. Eberhart, R., Kennedy, J.: A new optimizer using particle swarm theory. In: MHS’95. Pro-
ceedings of the Sixth International Symposium on Micro Machine and Human Science. IEEE 
(1995) 
6. Kennedy, J., Eberhart, R.: Particle swarm optimization. In: Proceedings of ICNN’95-
International Conference on Neural Networks, vol. 4. IEEE (1995) 
7. Glover, F., et al.: Tabu search principles. Tabu Search 125–151 (19970 
8. Kirkpatrick, S.C., Gelatt, D., Jr, Vecchi, M.P.: Optimization by simulated annealing. Science 
220(4598), 671–680 (1983) 
9. McCulloch, W., Pitts, W.: A logical calculus of ideas immanent in nervous activity. Bull. 
Math. Biophys. 5(4), 115–133 (1943) 
10. Kleene, S.C.: Representation of events in nerve nets and ﬁnite automata. Ann. Math. Stud. 
34, 3–41 (1956). Princeton University Press

References
187
11. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice-Hall Inc., Englewood 
Cliffs, N. J. (1967) 
12. Nielsen M.A., Chuang I.L.: Quantum Computation and Quantum Information. Cambridge 
University Press (2000) 
13. Hebb, D.: The Organization of Behavior. Wiley, New York (1949) 
14. Rosenblatt, F.: The Perceptron-a perceiving and recognizing automaton. Cornell Aeronaut. 
Lab., Report 85-460-1 (1957) 
15. Rosenblatt, F.: The perceptron: a probabilistic model for information storage and organization 
in the brain. Psychol. Rev. 65(6), 386–408 (1958) 
16. Minsky, M., Papert, S.: Perceptrons: An Introduction to Computational Geometry. MIT Press. 
(1969) 
17. Hopﬁeld, J.J.: Neural networks and physical systems with emergent collective computational 
abilities. Proc. Natl. Acad. Sci. 79(8), 2554–2558 (1982) 
18. Hopﬁeld, J.J., Tank, D.W.: Neural computation of decisions in optimization problems. Biol. 
Cybern. 52(3), 141–6 (1985) 
19. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representation by backpropagation 
errors. Nature 323, 523–536 (1986) 
20. Werbos, P.: Backpropagation through time: what it does and how to do it. Proc. IEEE 78(10), 
1550–1560 (1990) 
21. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document 
recognition. Proc. IEEE 86(11), 2278–2324 (1998) 
22. Bahdanau, D., Bengio, Y. Neural machine translation by jointly learning to align and translate 
(2016). arXiv:1409.0473v7 [cs.CL] 
23. Goodfellow, I., Bengio, Y. Courville A.: Deep Learning. MIT Press (2016) 
24. Russell, S.J., Norvig, P.: Artiﬁcial Intelligence A Modern Approach, 3rd edn. Pearson Edu-
cation Limited (2016) 
25. Manca, V.: Infobiotics: Information in Biotic Systems. Springer, Berlin (2013) 
26. Castellini, A., Manca, V.: Learning Regulation Functions of Metabolic Systems by Artiﬁcial 
Neural Networks. GECCO’09, Montréal, Québec, Canada. ACM 978-1-60558-325 (2009) 
27. Manca, V.: Metabolic computing. J. Membr. Comput. 1, 223 (2019) 
28. Lombardo, R., Manca, V.: Arithmetical metabolic p systems. In: Foundations on Natural , 
Artiﬁcial Computation, vol. 6686/2011, Springer, Berlin (2011) 
29. Castellini, A.: Algorithms and Software for biological MP Modeling by Statistical and Opti-
mization Techniques. Dipartimento di Informatica University di Verona. Ph.D. Thesis (2010) 
30. Pagliarini, R.: Modeling and Reverse-Engineering Biological Phenomena by means of 
Metabolic P Systems. Ph.D. Thesis, Dipartimento di Informatica University di Verona (2011) 
31. Marchetti, L.: MP Representations of Biological Structures and Dynamics. Ph.D. Thesis, 
Dipartimento di Informatica University di Verona (2012) 
32. Lombardo, R.: UnconventionalComputations and Genome Representations, Ph.D. Thesis, 
Dipartimento di Informatica University di Verona (2013) 
33. Castellini, A., Manca, V.: Metaplab: a computational framework for metabolic p systems. In: 
Membrane Computing, WMC9 2008, LNCS 5391, pp. 157–168. Springer, Berlin (2009) 
34. Castellini, A., Paltrinieri, D., Manca, V.: MP-GeneticSynth: Inferring biological network 
regulations from time series. Bioinformatics 31, 785–787 (2015) 
35. Marchetti, L., Manca, V.: A methodology based on MP theory for gene expression analysis. 
In: Membrane Computing, CMC 2011, vol. 7184, pp. 300–313. Springer, Berlin (2012) 
36. Bollig-Fischer, A., Marchetti, L., Mitrea, C., Wu, J., Kruger, A., Manca, V., Draghici, S.: 
Modeling time-dependent transcription effects of HER2 oncogene and discovery of a role for 
E2F2 in breast cancer cell-matrix adhesion. Bioinformatics 30(21), 3036–3043 (2014) 
37. Samuel, A.: Some studies in machine learning using the game of checkers. IBM J. Res. Dev. 
3(3), 210–229 (1959) 
38. Mitchell, T.: Machine Learning. McGraw Hill (1997) 
39. Manca, V.: Grammars for discrete dynamics. In: Holzinger, A. (ed.) Machine Learning for 
Health Informatics. Lecture Notes in Artiﬁcial Intelligence, LNAI 9605, pp. 37–58. Springer, 
Berlin (2016)

188
6
Life Intelligence
40. Manca, V.: A brief philosophical note on information. In: Holzinger, A., et al. (eds.) Integrative 
Machine Learning, LNAI 10344, pp. 146–149 (2017) 
41. Nielsen,
M.:
Neural
Networks
and
Deep
Learning
(2013).
http:// 
neuralnetworksanddeeplearning.com/ 
42. Kelley, H.J.: Gradient theory of optimal ﬂight paths. ARS J. 30(10), 947–954 (1960) 
43. Castellini, A., Manca, V., Suzuki, Y.: Metabolic P system ﬂux regulation by artiﬁcial neural 
networks. In: Membrane Computing, WMC 2009, vol. LNCS 5957, pp. 196-209. Springer, 
Berlin (2010) 
44. Linnainmaa, S.: The Representation of the Cumulative Rounding Error of An Algorithm as a 
Taylor Expansion of the Local Rounding Errors (Masters) (in Finnish). University of Helsinki 
(1970) 
45. Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are universal 
approximators. Neural Netw. 2(5), 359–366 (1989) 
46. Jumper, J., Evans, R., Pritzel, A., et al.: Highly accurate protein structure prediction with 
AlphaFold. Nature 596, 583–589 (2021) 
47. Kandel, E.R.: In Search of Memory. The Emergence of a New Science of Mind. W. W. Norton 
& Company, Inc. (2006) 
48. Manca, V.: Metabolic P systems. Scholarpedia 5(3), 9273–9273 (2010) 
49. Manca, V., Marchetti, L.: Metabolic approximation of real periodical functions. J. Log. Algebr. 
Program. 79, 363–373 (2010) 
50. Manca, V., Marchetti, L.: Solving dynamical inverse problems by means of metabolic p 
systems. BioSystems 109, 78–86 (2012) 
51. Marchetti, L. Manca, V.: MpTheory Java library: a multi-platform Java library for systems 
biology based on the Metabolic P theory. Bioinformatics 15; 31(8), 1328–1330 (2015) 
52. Kelly, K.: Out of Control, the New Biology of Machines, Social Systems and the Economic 
World. Copyright 1994 by Kevin Kelly (2008)

Chapter 7 
Introduction to Python 
7.1 
The Python Language 
Python 3 is used as a programming language for this laboratory. The choice of Python 
is due to three main factors. Firstly, the Python syntax is very intuitive and similar 
to a spoken natural language, making it among the easiest to learn programming 
languages. Secondly, it is portable and freely available, thus a Python script can 
be run on any operating system for which a Python interpreter has been released. 
Thirdly, as a result of the previously listed factors, Python is one of the most currently 
used programming languages, especially in machine learning and bioinformatics. In 
fact, several frameworks and computational instruments are developed on top of 
the python environment. For all of these reasons, Python results in a very powerful 
instrument for didactic scopes. If fact, it allows the reader to focus attention on the 
algorithmic aspect regarding the solution to a computational problem, by ignoring 
technical details that are speciﬁc to a given programming language, such as the 
management of the memory. 
Python has been designed to be a scripting language, thus a Python script is inter-
preted by an interpreter who reads the script ﬁle and executes the instructions line 
by line. This approach affects the way in which the programmer can write a Python 
program. The resultant philosophy is that everything that needs a declaration and 
that has not been deﬁned in a previous line can not be used in the current line of code. 
Typical examples are Python functions, they can not be called by an instruction that 
precedes the deﬁnition of the given called function (by following the reading order 
of the interpreter). This is one of the main differences between an interpreted and a 
compiled programming language. Python has been inﬂuenced by other programming 
languages, such as Modula-3, C, C++, Perl, Java and Unix shell programming lan-
guage. The result is a powerful programming language which presents a very intuitive 
syntax that hides the technical aspects of more sophisticated languages. The mix-
ture results in a multi-paradigm language which supports procedural programming 
(which is mainly based on functions and procedures), object-oriented programming 
(by allowing basic notions regarding classes, such as inheritance, operator over-
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_7 
189

190
7
Introduction to Python
loading and duck typing) and some aspects of the functional programming (such as 
iterators and generators). However, the interpreting philosophy described above is 
maintained. Moreover, some built-in data structures and procedures are developed 
in C, in order to increase performance, and Python just offers an interface to them. 
7.2 
The Python Environment 
The source code shown below gives a ﬁrst example of a Python script. 
1 v = input( ‘please input a value’) 
2 print ( ‘the input value is ’ , v) 
The script reads a value from the standard input, by means of the input function, 
and assigns such a value to the variable v. The assigned value is printed on the screen 
by means of the function print. 
The Python 3 interpreter can be called by typing python3 in a console environ-
ment, after that the Python 3 interpreter has been installed in the system. If no input 
ﬁle is given to the python3 command, an interactive python console is open and the 
user is allowed to input the instructions manually line by line. Each line, or block of 
lines, is immediately executed after its insertion. The following listing shows as the 
previous example runs in the interactive console. Lines that are the source code of the 
program are preﬁxed by monospace greater than monospace greater than monospace greater than>>>, and lines that correspond to the interaction between 
the environment and the user are listed without a particular preﬁx. 
1 >>>v = input( ‘please input a value’) 
2 5 
3 >>>print ( ‘the input value is ’ , v) 
4 the input value is 5 
Of course, the use of an interactive console may result helpful for on-the-ﬂy 
calculations or simple analyses that use a pre-programmed procedure. However, 
developers will ﬁnd the possibility to write scripts, save them on ﬁles and run those 
ﬁles a second time, which is more practical and reusable. Thus, source ﬁles can 
be passed as input to the Python interpreter by specifying them as arguments to 
the python3 command. For example, if the source code is stored in a ﬁle named 
myscript.py, it can be executed by typing python3 myscript.py in the 
same directory where the ﬁle resides. This emphasizes the fact that what the Python 
interpreter does on reading an input script is exactly the same as manually typing the 
lines of the script in the interactive console, in the order in which they are written 
into the ﬁle.

7.2 The Python Environment
191
Built-in types and variables 
The built-in data types that are available in Python 3 are listed in the following table. 
Name
Description 
int
Integer number 
float
Floating point number 
bool
Boolean value 
complex
Complex number with real and imaginary parts 
str
String 
byte
Representation of a binary encoding of a string 
list
A mutable ordered list of values 
tuple
An immutable tuple of values 
set
A mutable set of values 
frozenset A immutable set of values 
dict
A mutable key-value map 
Primitive types are int, float, bool, complex, str and byte, while the 
other built-in types are structures of data. Some languages, such as Java, deﬁne a 
clear difference between primitive types and structured objects, and only the latter 
can be equipped with functions that do not just concern with the storage of the 
values. In this sense, every Python data is an object which provides both storage and 
additional functions. For example, int is a basic object which also provided the 
function bit_length for calculating the number of bits necessary to represent the 
speciﬁc integer value in binary. 
Built-in data types are also divided into immutable and mutable types. Primitive 
types, tuple and frozenset are immutable, while types list, set, and dict 
are mutable. In Python, there is no general notion of array or matrix, which is usually 
implemented via lists and lists of lists. Lists are mutable, thus they are inefﬁcient with 
respect to a classical array data structure, but tuple and frozenset overcome 
efﬁciency limitations at the cost of immutability. 
There are other data types that are not built-in but they are available via mod-
ules of the standard Python library. Decimal that is provided within the decimal 
module, and gives a more accurate representation of decimal numbers that over-
comes some limitations of the ﬂoating point numbers. fraction, within the mod-
ule Fraction, allows to represent fractions and provides some operators for them. 
The module collection provides several data structures for collecting objects 
with speciﬁc aims, such as namedtuple for representing tuples in which the ele-
ments are indexed by a key that can be not only an integer (elements in simple 
tuples can only be accessed by integer keys that are their position within the tuple), 
OrderedDict that are maps in which keys are stored in an ordered way (the simple 
key-value structure is implemented via hash tables thus it does not ensure an order), 
defaultdict that are maps in which a default value is speciﬁed (simple maps

192
7
Introduction to Python
raise an error in case of a missing key) and Counter which are maps where the 
values are the corresponding number of times the keys are inserted. 
A variable is an object assuming values that can change according to situations 
inﬂuencing the object. A name, or label, is associated to a variable along the source 
code of a program. This notion can be further extended, by considering a second 
level of variability, with the concept of reference variable, where a variable, often 
called identiﬁer, assumes as value a variable having its own value. In other words, two 
associations are combined: identiﬁerright arrow from bar˫→variableright arrow from bar˫→value, where not only value, but  
also variable can vary. The term variable is usually reserved for a one-level variable, 
and non-reference variable is used for remarking a simple mechanism of variabil-
ity. Another way of expressing the difference between non-reference and reference 
variables is by means of the expression direct variability for non-reference variables, 
and the expression indirect variability for reference variables. The advantages of 
indirect variability are very important and provide computational power that can be 
appreciated in the development of complex programming. 
A physical model of non-reference and reference variables is given by an address-
able memory, that is, a set of locations, each of them identiﬁed by a number, called 
address. Locations can contain values, possibly of different types and aggregated in 
some way. In particular, a location upper LL can contain the address ii of a location upper L primeL'. In  
this case, we say thatii is a pointer toupper L primeL'. This mechanism makes it possible to realise 
a two-level variability: identiﬁer right arrow from bar˫→variable right arrow from bar˫→value. In fact, upper LL plays the role of 
the identiﬁer, the location upper L primeL', to which upper LL points by means of ii, plays the role of the 
variable, and the content upper L prime overbar ¯L' of upper L primeL' the role of the  value of  upper L primeL' and of the referenced 
value ofupper LL (actually the value ofupper LL isupper L primeL'). In the association chain:upper L right arrow from bar upper L prime right arrow from bar upper L prime overbarL ˫→L' ˫→¯L' the 
pairupper L right arrow from bar upper L primeL ˫→L' is also called an environment, whileupper L prime right arrow from bar upper L prime overbarL' ˫→¯L' is called a state, moreover, 
upper L primeL' is the reference of upper LL, while upper L prime overbar ¯L', the  direct value of upper L primeL', is the  indirect (referenced) 
value of upper LL. 
In general terms, the global state of any computation is given by the associa-
tions that, at many possible levels, are established between variables and values. The 
execution of a computation is essentially a transformation of this global state. This 
crucial role of variables in computations is the basis of the Object Oriented Program-
ming (OOP) paradigm, being objects a generalization and abstraction of variables. 
Python treats variables similarly to Java. Thus, a variable of a primitive data type 
is a non-reference variable, while a variable of a non-primitive type is variable by 
reference. In this sense, ifvv andww are variables of a primitive type, then an equation 
such as v equals wv = w is asymmetric because it has to be read from the right to the left, as 
expressing that the value of ww is associated to variable vv, we say this equation an 
assignment (in some languages an asymmetric symbol like colon equals:= is used in this case). 
However, an equation v equals wv = w over non-primitive types means that the two reference 
variables point to the same object. In this case, when the value of vv is modiﬁed also 
the value of ww is modiﬁed too (and vice versa). 
Differently from Java, a Python variable does not need to be declared and its type 
is inferred when a value is assigned to it. The actual type of an object can be obtained 
by the type function.

7.2 The Python Environment
193
1 x = 5  
2 print( type(5) ) 
1 <class ‘int ’> 
The following example shows how to create variables that represent primitive 
data types. Comment lines start with the character #, and can also be inserted at the 
end of an instruction line. 
1 # this is an integer value 
2 i =  −516 
3 # this is a float value 
4 f = 3.14 
5 # this is a boolean value 
6 b1 = True 
7 b2 = False # this is another boolean value 
8 # this is a sequence of byte 
9 y = b‘\x00\x01’ 
10 z = b‘Python’ # this is another sequence of byte 
11 # this is a string 
12 s1 = ‘my string 1’ 
13 s2 = “my string 2” # this is another string 
The following two examples show the difference between a reference and a non-
reference variable. 
Variables representing primitive data types directly correspond to the data itself 
and are non-reference variables. Thus, an assignment i = j  
determines the value 
of the variable i as the value of variable j. In this way, a change to the value of j 
does not alter the value of i. 
1 i = 512 
2 j = i  
3 j = 516 
4 print( i )  
5 print( j )  
1 512 
2 516 
In the case of non-primitive data types, such as a list, variables are reference 
variables, and an assignment of a reference variable such as l2 = l1, does not 
assign a list value to l2, but instead, l2 is a new reference to the value of l1. In this  
way, any modiﬁcation made to one of the two variables affects the value to which 
both refer. 
1 l1 = l i s t ([1 ,2 ,3]) 
2 l2 = l1 
3 l2 .remove(1) 
4 print(l1) 
5 print(l2) 
1 [2 , 3] 
2 [2 , 3]

194
7
Introduction to Python
7.3 
Operators 
Operators provide results associated, by some speciﬁc rule, with some elements taken 
as arguments. For example, arithmetic operators correspond to the usual operations 
on numbers (in the standard inﬁx notation with the operator between the arguments): 
addition +, subtraction -, multiplication *, division / and modulus %, exponentiation 
**, and ﬂoor division // (which returns only the integer part of a division). 
Bitwise operators express operations on the bits that compose the numbers. Binary 
AND &, binary OR | and binary XOR + return a number where each bit is set 
according to the results of the binary operator. The NOT operator inverts all the bits 
of a number (x = y). The bit shift operators push zeros from one side of the series 
of bits of a number and shift them on the other side, backslash ctexttt less than less than« and backslash ctexttt greater than greater than». 
Arithmetic and bitwise operators can be combined with assignment statements, 
such as x += 1  
or x &= y. Python does not provide increment or decrement 
operators such as ++ and –. 
Logical operators are provided via the keywords and, or and not. They can 
operate on heterogeneous operands by automatically converting the value of the 
operands. Zero values and empty strings are considered False and any other value 
or string is True 
Classical comparison operators are provided for working with numbers that return 
a boolean value (True or False), such as equality ==, inequality !=, greater than 
backslash ctexttt greater than>, less than backslash ctexttt less than<, greater than or equal to backslash ctexttt greater than equals>= and less than or equal to backslash ctexttt less than equals<=. Comparison 
operators can also be applied to strings. In this case, the comparison takes into account 
the lexicographic ordering of the two string operand for calculating the result of the 
comparison. 
String operators are the concatenation + (‘a’ + ‘b’ = ‘ab’) and the repe-
tition * (‘ab’ * 2 = ‘abab’). The len built-it function returns the length of 
a string (len(‘abc’)). In addition, strings are supported by the slice operator for 
extracting subsequences. It is provided in the form [start:end:step], where 
the start is the (inclusive) position from which to start the slicing, end is the (non-
inclusive) position at which the slicing is stopped, and step deﬁnes the number of 
position shifts to extract the next character. 
Indentation 
Standards and guidelines for what concerns good programming practices have been 
released by the Python Software Foundation in a document called PEP (Python 
Enhanced Proposal). One of the main differences between Python and any other 
programming language regards code indentation. In fact, contrary to the most used 
syntax for dividing sections of source code that are based on brackets or do-done 
statements, the Python language uses indentation levels to group sections of the 
source code. For example, the body of a function is deﬁned by an increased indenta-

7.4 Statements
195
tion level after the declaration of the function. Both spaces and tabulation characters 
can be used for the indentation. The PEP 8 documents suggest the use of 4 spaces 
for each indentation level. Moreover, it discourages the use of tabulation characters 
and the mixing of spaces and tabulations. 
7.4 
Statements 
Python provides most of the classical control ﬂow statements of programming lan-
guages, such as conditional and looping statements. For what concerns conditional 
statements, Python provides the if-elif-else structure. The if-elif-else statement if 
deﬁned as follow, where the elif and else parts are optional: 
1 if <boolean condition>: 
2
<code block> 
3 elif <boolean condition>: 
4
<code block> 
5 else : 
6
<code block> 
The if, elif and else keywords must be at the same indentation level, as well 
as the internal lines of the code blocks. Of course, internal code blocks may in turn 
be structured and contain nested statements. 
1 x = 5  
2 if x >  0:  
3
print( ‘the number is positive ’) 
4
if x > 100: 
5
print ( ‘and i t is greater than 100’) 
6 elif x <  0:  
7
print( ‘the number is negative ’) 
8 else : 
9
print ( ‘the number is exactly zero ’) 
Python does not provide the switch-case statement, but it can be implemented via 
a series of  elif conditions. 
1 x = input( ‘choose a single character ’) 
2 if x == ‘a’ : 
3
print( ‘the character is a’) 
4 elif x == ‘b’ : 
5
print( ‘the character is b’) 
6 elif x == ‘c’ : 
7
print( ‘the character is c’) 
8 elif x == ‘d’ : 
9
print( ‘the character is d’) 
10 else : 
11
print ( ‘the character is something else ’) 
For what concerns the looping statements, Python provides the while and the for 
statements. The while statement is deﬁned as follows:

196
7
Introduction to Python
1 while <positive boolean condition>: 
2
<code block> 
3 else : 
4
<code block> 
The else statement is optional and it is executed as soon as the while condition 
becomes False. The following example shows the iteration obtained b increasing 
the variable i from zero until it reaches the value 3. 
1 i =  0  
2 while i <  3:  
3
print( i )  
4
i = i +  1  
5 else : 
6
print( ’  finish  ’)  
1 0 
2 1 
3 2 
4 finish 
Python also provides the break command for breaking looping statements. As 
in any other programming language, the break affects the nearest looping statement. 
1 i =  0  
2 while i <  3:  
3
if i ==  2:  
4
break 
5
print( i )  
6
i = i +  1  
7 else : 
8
print(‘ finish ’) 
1 0 
2 1 
Lastly, the for statements is deﬁned as follows, where the else part is optional: 
1 for <object> in <iterable objects >: 
2
<code block> 
3 else : 
4
<code block> 
An iterable object is any kind of object for which its content can be iterated in a 
certain way. Built-in collections are iterable objects but also strings and sequences 
are iterable. The following example shows how to iterate and print the characters of 
the string Python from left to right. 
1 for c in ‘Python’ : 
2
print (c)

7.5 Functions
197
Strictly related to the for statement is the concept of generator. A generator is 
determined by a set of values obtained as a result of a for statement. A typical 
example of generators is provided by the built-in range operator which is used to 
generate numbers in a given range of values, from an initial value to a ﬁnal value, 
according to an increment/decrement step. In the most simple form, it generates 
values from zero to a given number (excluded). 
1 for i in range(3): 
2
print( i )  
1 0 
2 1 
3 2 
The following generator starts from the value 1 and generates the successive values 
by increasing the current value by 2. It stops when the generated value is greater than 
the maximum wanted value of 10. 
1 # generates number from 1 to 10 by increasing of a step of 2 
2 for i in range(1 ,10 ,2): 
3
print( i )  
1 1 
2 3 
3 5 
4 7 
5 9 
The concept of iterator, which is an extension of that one of the generator, will be 
treated in more details1 in Sect. 7.11. 
7.5 
Functions 
A function identiﬁes a sequence of statements. Any function has a name. When the 
name of the function is given to the Python interpreter, that is, when the function is 
called, then all the statements of the function are executed. Some variables occurring 
in the statements of the function can be identiﬁed as parameters of the function 
(they are listed between parentheses after the name of the function). In this case, 
when the function is called with some values of its parameters as arguments, then 
the statements of the function are executed with the parameters having the values 
corresponding to the arguments of the called function. 
Python functions are deﬁned via the def keyword. It is required to specify the 
name of the function and the parameters that it takes as input between parentheses. 
The deﬁnition of the body of the function follows the indentation rules described in 
Sect. 7.3, thus the instructions that are at the initial level of the body must have the 
same level of indentation.

198
7
Introduction to Python
1 def print_hello (): 
2
print( ‘Hello! ’) 
3 
4 print_hello () 
1 Hello! 
Parameters are deﬁned by only specifying their names, and not their types. 
1 def print_value(x): 
2
print(‘This is the value : ’ , x) 
3 
4 v =  ‘my  value’  
5 print_value(v) 
1 This is the value : my value 
The scope of parameters and of any variable declared within the body of the 
function is locally limited to the function itself. However, the function can access 
variables occurring in an upper level of the source code of which the function is part. 
The deﬁnition of the function does not include any information regarding the 
return value. If a return statement is reached by an instruction of the function, then 
the speciﬁed value is returned and the function is terminated. 
1 def increase (x): 
2
return x + 1  
3 
4 v = 0  
5 v = increase (v) 
6 print(v) 
1 1 
None, one or multiple values can be returned and they are assigned to variables 
by following the order in which they are speciﬁed in the return statement. 
1 def divide(x, y): 
2
return x /  / y  ,  x  %  2  
3 
4 quotient , remainder = divide(15, 2) 
5 print(quotient , ‘remainder’ , remainder) 
1 7 remainder 1 
Python allows the declaration of a default value for input parameters, by specifying 
it in the function declaration. If a value for the parameter is not speciﬁed during the call 
to the function, the default value is used. Otherwise, the default value is overwritten 
by the speciﬁed one. 
1 def pow(x, exponent = 1): 
2
return x ∗∗ exponent 
3 
4 print( pow(2) ) 
5 print( pow(2 ,2) )

7.5 Functions
199
1 1 
2 4 
As a consequence, parameters for which a default value is declared become 
optional. Thus, in some circumstances, it is necessary to specify which value is 
assigned to which parameters during the call of the function. 
1 def do_something(x = 1, y = 2, z = 3): 
2
return (x + y) ∗ z
# by default (1 + 2) ∗ 3 = 9  
3 
4 print( do_something(4) ) 
# ?  
5 print( do_something(y = 4) ) 
1 18
# the f i r s t optional value is overwritten 
2 15
# x = 1, z = 3 by default values 
It is a good programming practice to document functions by explaining what 
the input parameters are, what the function computes, or generally does, and what 
the function returns. Comment lines, which are generally read by users who are 
developing the speciﬁc function, are useful for understanding the functioning of a 
piece of code but they do not provide insights to end users. It is allowed to insert 
special comments, called docstrings, to provide end-user documentation. Docstrings 
are multi-line comments that start and end with three consecutive double quotes. 
When applied to functions they must be inserted immediately after the function 
deﬁnition and with the same indentation level as the function body. Once speciﬁed, 
the docstring is registered by the python environment as a documentation of the 
function that can be retrieved by the help built-in method. 
1 def divide(x, y): 
2
“““ 
3
This function computes the quotient and remainder of a 
4
division . 
5
””” 
6
return x /  / y  ,  x  %  2  
7 
8 help(divide) 
1 Help on function divide in module _ _main_ _: 
2 
3 divide(x, y) 
4
This function computes the quotient and remainder of a 
5
division . 
A best practice is to specify, within the docstring of a function, a summary of the 
function, the type and the meaning of the function parameters and of the returned 
values. 
1 def divide(x, y): 
2
“““ 
3
This function computes the quotient and the remainder of the 
4
division x / y. 
5

200
7
Introduction to Python
6
−−−−−−−−−− 
7
Parameters 
8
x (numeric) : the dividend 
9
y (numeric) : the divisor 
10 
11
−−−−−−−−−− 
12
Returns 
13
int : the quotient 
14
int : the remainder 
15
””” 
16
return x /  / y  ,  x  %  2  
7.6 
Collections 
The Python built-in data collections are list, set and dict. They can contain 
several primitive data of heterogeneous types, and they can even contain collections 
themselves. The following source code shows how to deﬁne collections for a pre-
deﬁned set of objects. Lists are deﬁned by means of square brackets, while sets are 
deﬁned with curly brackets, by listing the objects that are contained in the collec-
tion and separating them with commas. For avoiding confusion with the genomic 
dictionaries, we call “maps” the objects of type dict, usually called dictionaries. 
Maps are also deﬁned with curly brackets, but their deﬁnition must contain key-value 
associations that are listed in the form key : value. 
1 mylist = [1 , ‘s ’ , ‘a string ’] 
2 myset = {1, ‘s ’ , ‘a string ’} 
3 mydict = {0: 1, 1: ‘s ’ , ‘x’ : ‘a string ’} 
4 
5 print (mylist) 
6 print (myset) 
7 print (mydict) 
1 [1 , ‘s ’ , ‘a string ’] 
2 {1, ‘a string ’ , ‘s ’} 
3 {0: 1, ‘x’ : ‘a string ’ , 1: 1} 
Since, sets and maps do not ensure the ordering of the elements that they store, 
the print of such data structures does not respect the order in which their elements 
have been declared. 
Collections can also be declared as empty containers, and items can be added 
successively. Empty list is constructed by list() or []. 
The following code shows the main functions for adding items to a list, where 
append(2), append(3), append(4) are methods because are applied, writing 
them on the right after a dot, to the variable mylist by changing its value. The 
notion of method extends to any object as it will be explained in the section devoted 
to classes and objects.

7.6 Collections
201
1 # initialize an empty l i s t 
2 mylist = l i s t () 
3 
4 # append items to the tail of the l i s t 
5 mylist .append(2) 
6 mylist .append(3) 
7 mylist .append(4) 
8 print(mylist) 
1 [2 , 3, 4] 
1 # insert an item at a specific position , 
2 # the f i r s t position is indexed by 0 
3 mylist . insert (‘1 ’ , 0) 
4 print (mylist) 
1 [1 , 2, 3, 4] 
1 # append all the items of a l i s t to the tail of my l i s t 
2 mylist .append( [5 ,6 ,7] ) 
3 print(mylist) 
1 [1  ,  2, 3, 4, 5, 6,  7]  
Two main methods provide the removal of items from a list and one for entirely 
cleaning the collection. 
1 # initialize a l i s t 
2 mylist = [1 , 2, 2, 3] 
3 
4 # remove the first occurrence of a given item 
5 mylist .remove(2) 
6 print(mylist) 
1 [1 ,2 , 3] 
1 # remove the item at a specific position , and return i t 
2 e = mylist .pop(1) 
3 print(mylist) 
4 print(e) 
1 [1 , 3] 
2 2 
1 # i f no index is given , that i t removes the last item 
2 mylist .pop() 
3 print(mylist) 
4 print(e) 
1 [1]

202
7
Introduction to Python
1 # clear the list 
2 l i s t . clear () 
1 3 
Also, operators are deﬁned for lists. Similarly to strings, the concatenation oper-
ator +, and the repetition operator * can be applied. Their usage is intuitive as it is 
shown in the following example. 
1 # initialize a l i s t by concatenating two l i s t s 
2 mylist = [1 , 2, 3] + [4 , 5] 
3 print(mylist) 
4 
5 # create a l i s t that repeats 3 times a initial l i s t 
6 mylist = [1 , 2] ∗ 3 
7 print(mylist) 
1 [1 , 2, 3, 4, 5] 
2 [1  ,  2, 1, 2, 1,  2]  
The slicing operator is deﬁned in order to retrieve speciﬁc portions of lists. It is 
deﬁned in the form [start : end : step] such that the elements going from 
the starting position (included) to the ending position (excluded) are retrieved by 
applying a speciﬁc step. If the step argument is not speciﬁed then simply the slice of 
the list from the given positions is retrieved. Moreover, default values for the start 
and the end of the slicing are the head and the tail of the list, respectively. 
1 l  =  [0  ,  1, 2, 3, 4, 5, 6, 7, 8,  9]  
2 
3 # slicing from a star to and end position 
4 print( l  [1:5]  )  
5 
6 # slicing to and end position 
7 print( l  [:5])  
8 
9 # slicing from a start position 
10 print( l  [5:]  )  
11 
12 # slicing the entire l i s t by a step of 2 
13 print( l [::2] ) 
1 [1 , 2, 3, 4, 5] 
2 [0 , 1, 2, 3, 4] 
3 [5 , 6, 7, 8, 9] 
4 [0 , 2, 4, 6, 8] 
The square bracket operator is also used to retrieve the item at a given position 
of the list by considering the ﬁrst position as having index 0. Moreover, negative 
values can be speciﬁed for the argument of the operator. When negative values are 
speciﬁed for the start and end positions, the list is treated as a circular list and the 
corresponding positions are calculated by going backwards from the default position. 
A negative value of the step corresponds to going backwards from the end to the start 
position by skipping the speciﬁed number of positions at each step.

7.6 Collections
203
1 mylist  =  [0  ,  1, 2, 3, 4, 5, 6, 7, 8,  9]  
2 
3 print( mylist[−1:] ) 
4 
5 print( mylist[:−1] ) 
6 
7 print( mylist[::−1] ) 
1 [9] 
2 [0  ,  1, 2, 3, 4, 5, 6, 7,  8]  
3 [9  ,  8, 7, 6, 5, 4, 3, 2, 1,  0]  
Lastly, the slicing operator can be combined with the del operator to remove 
from the list the elements selected with the slice. 
1 mylist  =  [0  ,  1, 2, 3, 4, 5, 6, 7, 8,  9]  
2 
3 # remove the f i r s t element 
4 del mylist[0] 
5 print(mylist) 
6 
7 # remove a slice of elements 
8 del mylist[1:2] 
9 print(mylist) 
10 
11 # remove a slice of elements with a step of 2 
12 del mylist [1::2] 
13 print(mylist) 
1 [1  ,  2, 3, 4, 5, 6, 7, 8,  9]  
2 [1  ,  3, 4, 5, 6, 7, 8,  9]  
3 [1 , 4, 6, 8] 
Similarly to lists, sets provides a method for adding elements, that is add, and 
methods for removing items or clearing the collections, namely remove and clear. 
However, sets do not prove any functionality that is based on an indexing concept, 
such as the position of an element. 
1 # initialize an empty set 
2 myset = set () 
3 
4 # add an element to a set 
5 myset.add(1) 
6 
7 # adding a duplicated element to a set 
8 # does not affects the content of the set 
9 myset.add(1) 
10 
11 # remove an  element  form  a  set  
12 myset.remove(1) 
Special operators that are deﬁned to be applied to sets are set-theoretic operators, 
such as union

204
7
Introduction to Python
|, intersection &, difference - and symmetric difference (elements that are in both 
sets both not in their intersection). 
1 # initialize two sets 
2 s1 = {1, 2} 
3 s2 = {2, 3} 
4 
5 # union 
6 print( s1  |  s2  )  
7 
8 # intersection 
9 print( s1  &  s2  )  
10 
11 # difference 
12 print( s1  − s2 ) 
13 
14 # symmetric difference 
15 print( s1  ^  s2)  
1 {1, 2, 3} 
2 {2} 
3 {1} 
4 {1, 3} 
Moreover, two operators are deﬁned for checking the inclusion of a set within 
another set, that are backslash ctexttt greater than equals>= and backslash ctexttt less than equals<=. 
1 # initialize two sets 
2 s1 = {1, 2} 
3 s2 = {1, 2, 3} 
4 
5 # check is s1 is subset of s2 
6 print( s1  <=  s2 )  
7 
8 # check is s1 is superset of s2 
9 print( s1  >=  s2 )  
1 True 
2 False 
Python provides an easy way to convert lists to sets and vice versa such that the 
speciﬁc methods of both collections can be applied in several situations. For example, 
sets can be used to remove duplicated elements from a list, by simply converting a 
list to a set. 
1 mylist = [1 , 2, 2, 3] 
2 
3 # covert to a set fro removing duplicates 
4 myset = set (mylist) 
5 
6 # convert back to a l i s t 
7 print( l i s t (myset) ) 
1 [1 , 2, 3]

7.6 Collections
205
Lists and sets are iterable objects, thus content can be iterated in a for statement. 
1 mylist = [1 , 2, 2, 3] 
2 
3 for v in mylist : 
4
print( str  (v)  +  ’  ,  ’  ,  end=”) 
5 # end=” prevents the new line at the end of the print 
6 
7 print() # make a new line 
8 
9 for v in set (mylist ): 
10
print( str  (v)  +  ’  ,  ’  ,  end=”) 
1 1, 2, 2, 3, 
2 3, 1, 2, 
As introduced before, maps are collections of key-value pairs. Keys are unique 
and each key has only one assigned value. There is no limitation on the type of 
keys and values, and they can be of different types within the same map. The square 
bracket operator allows one to assign a value to a key and retrieve the value of a 
given key. 
1 # initialize an empty map 
2 mydict = dict () 
3 
4 # assign ‘one’ to the integer key 1 
5 mydict[1] = ‘one’ 
6 
7 # assign 2 to the string key ’two’ 
8 mydict[ ‘two’] = 2 
9 
10 # print the entire map 
11 print( mydict ) 
12 
13 # print the value associated to the key 1 
14 print( mydict[1] ) 
1 {1:‘one’ , ‘two’:2} 
2 ’one’ 
Maps are iterable objects, thus their content can be iterated within a for loop. 
The iteration of a map can be performed in three different ways. The entire set of 
keys can be iterated, as well as the entire list of values, or the maps can be iterated 
as a set of key-value items. None of these three methodologies provided an ordered 
iteration. The following example shows how to use the method keys to retrieve the 
complete set of keys of a map and to print the entire content of the map with a for 
loop. 
1 mydict = {1:‘one’ , ‘two’ :2 , 3:‘three ’} 
2 
3 # iterating by the set of keys 
4 for k in mydict.keys (): 
5
print ( k, mydict[k])

206
7
Introduction to Python
1 1
‘one’ 
2 ‘two’
2 
3 3
‘three
 
’
 
Alternatively, the method items can be used to retrieve the complete set of key-
value pairs contained in a map. These kinds of items are represented as tuples, thus 
the iterating object is a tuple with two values, the ﬁrst is the key and the second is 
the assigned value. 
1 mydict = {1:‘one’ , ‘two’ :2 , 3:‘three ’} 
2 
3 # iterating by the set of key−value pairs 
4 for t in mydict.keys (): 
5
print ( t , t [0] , t [1]) 
1 (1 , ‘one’) 
1
‘one’ 
2 ( ‘two’ ,2)
‘two’
2 
3 (3 , ‘ three ’) 
3
‘three ’ 
Python provides a special behaviour for decomposing tuples into distinct values. 
The iteration of a map is a typical example. The item tuple is decomposed into two 
values k and v, which name is deﬁned by the developer. 
1 mydict = {1:‘one’ , ‘two’ :2 , 3:‘three ’} 
2 
3 # iterating by the set of key−value pairs 
4 for k,v in mydict. items (): 
5
print ( k, v ) 
1 1
‘one’ 
2 ‘two’
2 
3 3
‘three
 
’
 
For what concerns the retrieving operation, if the key is not present in the map 
then an error arises. Thus, the existence of the key must be veriﬁed before every 
access to the map that is not an assignment. The existence can be veriﬁed with the 
in operator which returns true in case the item is an existing key of the map. 
1 # initialize an empty map 
2 mydict = {1:‘one’ , ‘two’:2} 
3 
4 if 3 in mydict: 
5
print( mydict[3] ) 
6 else : 
7
print(‘key not found’) 
1 key not found 
The in operator can also be combined with the logical not operator. 
1 # initialize an empty map 
2 mydict = {1:‘one’ , ‘two’:2} 
3 
4 if 3 not in mydict:

7.6 Collections
207
5
mydict[3] = ‘three ’ 
6 else : 
7
print ( ‘a value already exists for the key 3’) 
8 
9 print(mydict) 
1 {1:‘one’ , ‘two’ :2 , 3:‘three ’} 
The method get provided by the dict class is an alternative to the use of the 
in operator. The method has the same behaviour as the square bracket operator for 
retrieving the value assigned to a key, however, a default value can be speciﬁed in 
case of a missing key such that the default value is returned and no error arises. 
1 # initialize an empty map 
2 mydict = {1:‘one’ , ‘two’:2} 
3 
4 print( mydict. get(3 , ‘not found’) ) 
1 not found 
The in operator can also be used for lists and sets to check the existence of an 
item within the collection. 
1 mylist = [1 , 2, 3] 
2 print( 4  in mylist ) 
1 False 
Comprehension is an instrument that allows the creation of collections in a concise 
way by starting from some generated values. Comprehension also allows us ﬁltering 
and transforming the given element before it is inserted into the collection. 
For what concerns lists, the comprehension is in the form 
1 [ <transformation> <generative loops> <filters > ] 
Initial values are generated in the middle block of the form, which consists of one 
or more generative for loops. Filters are applied to generated values, and transfor-
mations are ﬁnally applied in order to obtain the resultant values. The transformed 
values compose the created list. 
For example, a list which contains the numbers from 1 to 10 is created in the 
following way: 
1 [ i  for i in range(1,11) ] 
1 [1  ,  2, 3, 4, 5, 6, 7, 8, 9,  10]  
In this case, no ﬁlters and transformations are applied. The for statement gener-
ated the values iteratively, and the iteration variable i is used to insert values within 
the collection. 
Comprehension with ﬁltering can be used to generate numbers multiples of 2 
form 2 to 10:

208
7
Introduction to Python
1 [ i  for i in range(2,10) if ( i  %  2)  ==  0  ]  
1 [2 , 4, 6, 8, 10] 
In this case, the ﬁlter if (i % 2) == 0  
is applied to the iteration variable, 
and the values of the iteration that pass the ﬁlter are inserted into the resultant list. 
The next example shows how to apply a transformation in order to obtain the 
powers of 2 from 2 Superscript 020 up to 2 Superscript 626 (we recall that ** is the power operator). 
1 [ 2∗∗i for i in range(0 ,7) ] 
1 [1 , 2, 4, 8, 16, 32, 64] 
In the same way, comprehension can be used to generate sets and maps. Compre-
hension for sets is equal in almost everything to the comprehension for lists except 
that it is encapsulated within curly brackets instead of square brackets. The following 
examples show how to create a set that contains the squares of numbers from 1 squared12 to 
10 squared102 put together with the number themselves in a pair: 
1 { (  i  ,  i∗i )  for i in range(1,11) } 
2 { (  i  ,  i∗i )  for i in range(1,11)} 
1 {(1, 1) ,
(2, 4) , (3, 9) , (4, 16), (5, 25), (6, 36), (7, 49), 
2 
(8 , 64), (9 , 81), (10, 100)} 
In this case, the generated value is not just a number but it is a more complex 
item, such as a tuple. 
Comprehension can also be applied to generating maps. The previous example is 
converted into a map where keys are the numbers from 1 to 10 and values are their 
square: 
1 {  i : i∗i for i in range(1,11) } 
1 {1:  1, 2: 4, 3: 9, 4:  16,  5:  25,  6:  36,  7:  49,  8:  64,  9:  81,  
2 10: 100} 
Lastly, the function len is applied to any type of collection in order to retrieve 
the number of elements that are stored in the data structure. In the case of maps, the 
built-in function returns the number of key-value items that are currently inside the 
map. 
1 mylist = [ i for i in range(1,11)] 
2 print( len(mylist) ) 
1 10

7.7 Sorting
209
7.7 
Sorting 
The built-in operator sorted sorts input lists of sets and returns an ordered list. 
1 myset = {2,1,3} 
2 print( sorted (myset) ) 
1 [1 ,2 ,3] 
It is required that the elements of the collection must be comparable to each other. 
Thus, the sorting function is not able to compare a numeric with a non-numeric 
object, for example, an integer with a string. An error of type TypeError is thrown 
if incomparable types are sorted. 
1 myset = {2,1,‘a string ’} 
2 print (myset) 
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 ValueError
Traceback (most recent call last ) 
3 <ipython−input−27−45c0c9bb2cec> in <module>() 
4 −−−−> 1  print( sorted (myset)) 
5 
6 TypeError: unorderable types : str () < int () 
Boolean values are exceptions because they can be interpreted also as numbers 
or strings. In fact, the integer values of True and False are 1 and 0, respectively, 
and their string representation simply reports their keywords. 
1 print( int (True) , int (False )) 
2 print( str (True) , str (False )) 
1 1
0
 
2 True
False 
Notably, the None constant can not be converted to a numeric value but only to 
literal and boolean values. 
The sort function receives two optional arguments, reverse and key. The  
reverse argument speciﬁes that the sorting must be performed in a reverse order-
ing, namely from the highest to the lowest value. 
1 sorted ( [4,2,1,3] ) 
2 sorted ( [4,2,1,3] , reverse = True ) 
1 [1 , 2, 3, 4] 
2 [4 , 3, 2, 1] 
The key argument speciﬁes a function (function will be treated in Sect. 7.5) that 
is applied to the elements of the collection and is used to give a representation of the 
elements that will be used for the sorting. For example, the comparison between two 
tuples iteratively takes into account the values of the tuples from the left-most to the 
right-most. The ﬁrst comparison that is not an equality result gives the ordering of 
the two tuples. In the following example, the two tuples are (1,3) and (2,0) and 
the resultant order is computed by only taking into account the values 1 and 2.

210
7
Introduction to Python
1 sorted ( [(1 ,3) , (2 ,0)] ) 
1 [(1 ,3) , (2 ,0)] 
The sorting of the two tuples by their second values can be obtained by using the 
key argument in the following way: 
1 def get_second( t ): 
2
return t [1]  
3 
4 sorted ( [(1 ,3) , (2 ,0)] , key=get_second ) 
1 [(2 ,0) , (1 ,3)] 
In this case, the function get_second is applied to each element of the list 
in order to extract an alternative value that is representative of the element for its 
comparison with the others. 
The default toolkit of Python provides the function itemgetter available by 
the operator module. that is an alternative to the writing of a new function for the 
developer. The function takes as a parameter the index of the value within the tuple 
that must be used for the comparison. 
1 from operator import itemgetter 
2 
3 sorted ( [(1 ,3) , (2 ,0)] , key=itemgetter (1)) 
1 [(2 ,0) , (1 ,3)] 
7.8 
Classes and Objects 
This section gives a brief introduction to object-oriented programming (OOP) in 
Python by focusing on how to deﬁne classes. We recall that in the OOP paradigm, an 
object is an instance of a given class. A class deﬁnition speciﬁes how to construct and 
modify the objects of the class. The concept of class has a hierarchical organization 
of classes and subclasses. 
An object of a class is constructed by a constructor identiﬁed by a reserved name 
given by __init__. Any constructor must be declared with an initial parameter 
that is called self. This parameter is a reference to the object and it is used to 
modify the object by obtaining another instance of the object. Therefore, different 
objects of a class correspond to instances of the class, while different instances of an 
object correspond to different states of the object, conceived as a dynamical entity 
changing during computations. In the following example, the deﬁnition of the class 
Coordinate describes objects that represent Cartesian coordinates. 
1 class Coordinate : 
2
“““ 
3
A class to represent Cartesian coordinates

7.9 Methods
211
4
””” 
5 
6
x = 0  
7
# The x coordinate . By default i t is 0 
8 
9
y = 0  
10
# The y coordinate . By default i t is 0 
11 
12
def _ _init_ _( self , x_coord, y_coord): 
13
self .x = x_coord 
14
self .y = y_coord 
15 
16 c1 = Coordinate(2 ,3) 
17 c2 = Coordinate(2 ,4) 
The example shows the deﬁnition of the class and the creation of two instances, 
c1 and c2, that represent two distinct points in the Cartesian space. 
Parameters inside parentheses of __init__ are called attributes of the objects. 
The attributes are by default publicly accessible. They can be retrieved by the dot . 
operator applied to an instance of the class. In the following example, the coordinates 
of one point are read. 
1 c1 = Coordinate(2 ,3) 
2 print( c1.x,  c2.y  )  
Since the access is public, the internal attributes of the class can also be written 
by other entities. 
1 c1 = Coordinate(2 ,3) 
2 c1.x = 1 
3 print( c1.x,  c2.y  )  
1 1 3  
However, the access to the internal state of an object by means of its attributes 
violates good programming practices in OOP, a principle called encapsulation. 
7.9 
Methods 
In order to implement the encapsulation, access to the class attributes must be for-
bidden and speciﬁc functions for reading and writing the values of attributes must 
be equipped within the class deﬁnition. These functions are called methods of the 
class and are applied to the objects of the class by means of the standard dot-sufﬁx 
notation. In Python syntax, any attribute whose name starts with a double underscore 
is a private variable, thus it cannot be externally accessed. 
1 class Coordinate : 
2
“““ 
3
A class to represent Cartesian coordinates 
4
”””

212
7
Introduction to Python
5 
6
_ _x  =  0  
7
# The x coordinate . By default i t is 0 
8 
9
_ _y  =  0  
10
# The y coordinate . By default i t is 0 
11 
12
def _ _init_ _( self , x_coord, y_coord): 
13
self . set_x(x_coord) 
14
self . set_y(y_coord) 
15 
16
def get_x( self ): 
17
return self ._ _x 
18 
19
def get_y( self ): 
20
return self ._ _y 
21 
22
def set_x( self , x): 
23
if x >=  0:  
24
self ._ _x = x 
25 
26
def set_y( self , y): 
27
if y >=  0:  
28
self ._ _y = y 
29 
30 c = Coordinate(2 ,3) 
31 
32 # print ( c ._ _x )
# _ _x is private , this will throw an error 
33 
34 print( c  .  get_x()  )  
35 
36 c. set_x(1) 
As standard guidelines for OOP, get and set methods are implemented for access-
ing the internal state in a controlled way. In fact, the encapsulation allows the class 
to control how the internal state is modiﬁed, and this helps the object to ensure a 
consistent form of its state. Thus, only internal mechanisms that are relevant for 
using the object are made available to external entities. In the example, the class is 
implemented such that it works only with positive coordinates, and if negative values 
are speciﬁed then they are ignored. 
In modern languages, objects are equipped with special functions for obtaining 
a printable description of them. Python does not make an exception to this practice. 
The method is called __repr__, and it is used by the Python environment every 
time it tries to cast an instance of the class to a string. 
1 class Coordinate : 
2
_ _x  =  0  
3
_ _y  =  0  
4
#class_body 
5
def _ _repr_ _(  self  ):  
6
return ‘( ’+str ( self ._ _x)+’ , ’+str ( self ._ _y)+’) ’ 
7

7.9 Methods
213
8 c = Coordinate(2 ,3) 
9 print (c) 
1 (2 , 3) 
More in general, Python reserves some special names to be assigned to functions 
for implementing speciﬁc behaviours. These names are recognizable since they start 
and end with a double underscore. 
Other interesting special methods are those reserved for deﬁning how Python 
operators work on custom objects. This concept is usually called operator overload-
ing. Each operator has a corresponding function name. For example, the + operator is 
deﬁned via the __add__ function. Special methods that implement operators must 
ensure that the operand belongs to the speciﬁc class. This check is performed by 
the statement assert in conjunction with the built-in function isinstance. The  
built-in function returns a positive value if the object is an instance of a given class. 
The assertion will throw an error in case of a negative result. The addition operator 
works on two operands. The left operand is identiﬁed by the self instance, and the 
right operand is identiﬁed by the additional function parameter. 
1 class Coordinate : 
2
_ _x  =  0  
3
_ _y  =  0  
4
#class_body 
5
def _ _add_ _( self , other ): 
6
assert isinstance (other , Coordinate) 
7
return new Coordinate( self_ _x + other ._ _x, 
8
self_ _y + other ._ _y ) 
9 
10
def _ _sub_ _( self , other ): 
11
assert isinstance (other , Coordinate) 
12
return new Coordinate( self_ _x − other ._ _x, 
13
self_ _y − other ._ _y ) 
14 
15 c1 = Coordinate(2 ,4) 
16 c2 = Coordinate(2 ,3) 
17 
18 print( c1  +  c2  )  
19 print( c1  −− c2 ) 
1 (4 ,7) 
2 (0 ,1) 
A set of interesting operator overloading functions are those regarding comparison 
operators. In the following example, a function to compare coordinates is ﬁrst devel-
oped, and then it is used for implementing the comparison operator. The function, 
that in the following example is called compare, and follows some good practices 
in programming. It takes as input the current instance and an object with which the 
instance must be compared. If the two entities are considered equal, then the result of 
the comparison is zero. Otherwise, the result is a negative integer value if the instance 
is considered less than the compared object, otherwise, it is a positive integer value.

214
7
Introduction to Python
This practice implies the deﬁnition of an ordering between objects of the given class. 
Moreover, it allows the speciﬁcation of the amount of difference between the objects. 
For example, the Hamming distance can be used for comparing strings. The distance 
counts in how many positions the two strings have a different character. Thus, the 
ordering function adds a sign to the distance for deciding the order between two 
objects. 
In the following example, a comparison between coordinates is deﬁned such that 
they are ﬁrstly compared by the xx coordinate, and in the case of equality they are 
compared by the yy coordinate. 
1 import math 
2 
3 class Coordinate : 
4
. . .  
5
def compare( self , other ): 
6
assert isinstance (other , Coordinate) 
7
if self ._ _x == other ._ _x: 
8
return self ._ _y − other ._ _y 
9
return self ._ _x − other ._ _x 
10 
11
# Overloading of comparison operators # 
12
def _ _lt_ _( self , other ): 
13
return self .compare(other) < 0 
14
def _ _gt_ _( self , other ): 
15
return self .compare(other) > 0 
16
def _ _eq_ _(  self  ,  other  ):  
17
return self .compare(other) == 0 
18
def _ _le_ _( self , other ): 
19
return self .compare(other) <= 0 
20
def _ _ge_ _( self , other ): 
21
return self .compare(other) >= 0 
22
def _ _ne_ _( self , other ): 
23
return self .compare(other) != 0 
24 
25 c1 = Coordinate(2 ,3) 
26 c2 = Coordinate(2 ,4) 
27 
28 print( c1  <  c2  )  
29 print( c1  >=  c2 )  
1 True 
2 False 
Python is in many of his aspects a structure-less language. Variables do not need 
to be declared, for example. Functions are overwritten when multiple declarations 
with the same function name are given. 
1 def foo() 
2
print(‘foo’) 
3 
4 foo() 
5 
6 def foo(x = 0):

7.10 Some Notes on Efﬁciency
215
7
print ( ‘foo’ ,0) 
8 
9 foo() 
1 foo 
2 foo 0 
Objects are not an exception to this kind of ﬂuidity that is in Python. As said 
before, objects are in their essence containers of properties. The list of the properties 
associated with a given object can vary over time. This means that the class declaration 
is a template that speciﬁes the guidelines for building an initial structure of an object 
that instantiates such a class. Such a structure can further be modiﬁed. The following 
example shows how it is possible to add an internal variable to an object after its 
instantiation. 
1 class Myclass: 
2
x = 0  
3
def _ _init_ _( self , x ): 
4
self .x = x 
5 
6 o = Myclass(1) 
7 o.y = 10 
8 print(o.y) 
1 10 
7.10 
Some Notes on Efﬁciency 
Python has changed during the time its internal behaviours, especially from version 
2 to version 3. The changes do not only regard the syntax of the language but include 
modiﬁcations to the internal behaviour of built-in functions. Python is a very high-
level programming language, and this feature gives to it a series of advantages. 
However, a legitimate developer who takes into account the underlying functioning 
of the Python environment will take beneﬁts from developing software of good 
quality and increased performance. The choice of using a speciﬁc coding strategy is 
of main importance. 
In this perspective, generators provide a typical case study to understand the 
importance of good coding behaviour. As an example, the developer is asked to 
write a source code for retrieving all the prime numbers that are smaller than a given 
number. A trivial solution is to iterate over the number from 1 to the maximum 
required value and to check for each number if it is a prime number. The following 
example uses an already developed function, is_prime, to perform the check, 
which implementation is not the focus of this example and thus it is ignored. 
1 max_value = 100 
2 prime_numbers = set () 
3

216
7
Introduction to Python
4 for i in range(1 , max_value): 
5
if is_prime( i ): 
6
prime_numbers.add( i ) 
The solution is implemented by using the function range, which creates a gen-
erator for navigating through the numbers that have to be tested. An important fact is 
that the generator does not create a complete list of numbers from 1 to the maximum 
value. On the contrary, one number at a time is stored in memory. An alternative 
would be the use of list comprehension as it is shown in the following source code: 
1 max_value = 100 
2 prime_numbers = set () 
3 numbers = [ i for in in range(1 ,max_value)] 
4 
5 for i in numbers: 
6
if is_prime( i ): 
7
prime_numbers.add( i ) 
The choice of using this strategy has a main pitfall, that is, the creation of a list 
that is completely stored in memory. If the maximum value is relatively small, such 
as 100, the issue has a low impact on the performance of the software, but it may be 
disruptive for high values. 
Another example of good programming regards the sorting of a list. In this toy 
case, the developer has to order a given list of numbers. A non-expert developer will 
probably sort the list via the built-in function sorted which creates a copy of the 
input list. Such a copy reports elements in an ordered way. Then, the original list 
must be overwritten with the sorted copy. 
1 ilist = [2 ,4 ,1 ,3] 
2 ilist = sorted ( ilist ) 
If there is no need for creating a copy of the list in the original ordering, then 
it should be preferable to not create such a copy. This is of high impact on the 
performance, especially if the list is relatively large: It takes twice the memory, but 
it also requires additional time to create the copy to be sorted. Thus, in this situation, 
it is preferable to use the method sort provided by the list data type. The method 
acts on the original data structure without the need for a creation of a copy. 
1 ilist = [2 ,4 ,1 ,3] 
2 ilist . sort () 
7.11
Iterators 
Generators described in the previous section are special kinds of iterators, that is, 
objects identifying a collection, but without storing all the elements of the collection, 
but only one of them at each time. In this way, a gain of space is realized at the cost of a

7.11 Iterators
217
computational mechanism of updating objects along the computation, by elaborating 
on each of them, at any step of an iterative process. 
Two important steps of an iterator are the initialization and the passing from one 
item to another. In this perspective, an iterator is an entity which maintains an internal 
state. The state is initialised in a given way, and special procedures deﬁne how to 
pass from one state to another. If the iterator is in a consistent state, then it can be 
asked for the value that corresponds to its internal state. 
In Python, iterators can be implemented in several ways. We focus on deﬁn-
ing iterators as objects. Iterators are special types of objects which implement the 
__iter__ and __next__ methods. 
The following example illustrates how to build an iterator that iterates over the 
integer numbers starting from 1. The internal state is represented by the internal 
variable __i. Each time the method next is called on the iterator, the internal state 
is updated with a forward operation and the element resulting from such an operation 
is returned. 
1 class my_iterator : 
2
_ _i  =  0  
3 
4
def _ _init_ _( self ): 
5
self ._ _i = 0 
6 
7
def _ _next_ _( self ): 
8
self ._ _i += 1 
9
return self ._ _i 
10 
11 i t = my_iterator () 
12 print( next(  i  t  )  )  
13 print(next( i t )) 
1 1 
2 2 
One important thing to remember is that the method next is called in order to 
retrieve the elements of the iteration. This means that, after the creation of the iterator, 
the ﬁrst element is retrieved only after a call of the next method. For this reason, 
in order to start the iteration from the value 1, the initial state is set to 0. 
The given class is still not suitable to provide objects that can be used in for 
statements. To this end, an explicit deﬁnition of __iter__ method is necessary 
that makes explicit the iterable nature of the deﬁned objects, as it is shown in the 
following class deﬁnition. 
1 class my_iterator : 
2
_ _i  =  0  
3 
4
def _ _init_ _( self ): 
5
self ._ _i = 0 
6 
7
def _ _iter_ _( self ): 
8
return self

218
7
Introduction to Python
9 
10
def _ _next_ _( self ): 
11
self ._ _i += 1 
12
return self ._ _i 
13 
14 i t = my_iterator () 
15 for n in it : 
16
print(n) 
17
if n > 10: 
18
break 
1 1 
2 2 
3 3 
4 4 
5 5 
6 6 
7 7 
8 8 
9 9 
10 10 
11 11 
The object provides an iteration over integer numbers without any limit. In fact, it 
can iterate up to inﬁnite. The method __next__ has to raise a StopIteration 
exception in order to stop the interaction at a speciﬁc state. For example, an iterator 
to enumerate integers from 1 to 10 is the following one. 
1 class my_iterator : 
2
_ _i  =  0  
3
_ _limit = 10 
4 
5
def _ _init_ _( self ): 
6
self ._ _i = 0 
7 
8
def _ _iter_ _( self ): 
9
return self 
10 
11
def _ _next_ _( self ): 
12
self ._ _i += 1 
13
if self ._ _i <= self ._ _limit : 
14
return self ._ _i 
15
else : 
16
raise StopIteration 
17 
18 i t = my_iterator () 
19 for n in it : 
20
print(n) 
1 1 
2 2 
3 3 
4 4

7.12 Itertools
219
5 5 
6 6 
7 7 
8 8 
9 9 
10 10 
The raise of a StopIteraction exception is a special event that, in the case 
of iterator objects, is not threaded like a common exception raise. In fact, it is not 
cached by the try-except statement. 
1 i t = my_iterator () 
2 try : 
3
for n in it : 
4
print(n) 
5 except: 
6
print( ’an exception has been arised ’) 
1 1 
2 2 
3 3 
4 4 
5 5 
6 6 
7 7 
8 8 
9 9 
10 10 
1 def m_function (): 
2
raise StopIteration 
3 
4 try : 
5
my_function() 
6 except: 
7
print( ‘an exception has been arised ’) 
1 an exception has been arised 
7.12 
Itertools 
Python provides a set of built-in functions to build combinatorial structures. The 
library is module itertool. and its documentation is given at https://docs.python. 
org/3/library/itertools.html. It can be used, for example, to perform Cartesian prod-
ucts. 
1 import itertools 
2 
3 alphabet = ‘acgt ’

220
7
Introduction to Python
4 
5 # cartesian product , equivalent to a nested for−loop 
6 itertools . product(alphabet , repeat = 2) 
7 
8 for i in itertools . product(alphabet , repeat = 2): 
9
print ( i ) 
1 (‘a’ , ‘a’) 
2 (‘a’ , ‘c’) 
3 (‘a’ , ‘g’) 
4 (‘a’ , ‘ t ’) 
5 (‘c’ , ‘a’) 
6 (‘c’ , ‘c’) 
7 (‘c’ , ‘g’) 
8 (‘c’ , ‘ t ’) 
9 (‘g’ , ‘a’) 
10 (‘g’ , ‘c’) 
11 ( ‘g’ , ‘g’) 
12 (‘g’ , ‘ t ’) 
13 (‘ t ’ , ‘a’) 
14 (‘ t ’ , ‘c’) 
15 (‘ t ’ , ‘g’) 
16 (‘ t ’ , ‘ t ’) 
The conversion to usual Python strings can be obtained with the following for 
statement. 
1 for i in itertools . product(alphabet , repeat = 2): 
2
print( ”. join  (  i  )  )  
1 aa 
2 ac 
3 ag 
4 at 
5 ca 
6 cc 
7 cg 
8 ct 
9 ga 
10 gc 
11 gg 
12 gt 
13 ta 
14 tc 
15 tg 
16 tt 
And many other combinatorial structures can be generated. 
1 for i in itertools . product(alphabet , repeat = 2): 
2
print(”. join  (  i  )  ,  end=’  ’)  
3 print() 
4 
5 # r−length tuples , no repeated elements

7.12 Itertools
221
6 for i in itertools . permutations(alphabet , 2): 
7
print(”. join  (  i  )  ,  end=’  ’)  
8 print() 
9 
10 # r−length tuples , in sorted order , no repeated elements 
11 for i in itertools . combinations(alphabet , 2): 
12
print(”. join  (  i  )  ,  end=’  ’)  
13 print () 
14 
15 # r−length tuples , in sorted order , with repeated elements 
16 for i in itertools . combinations_with_replacement(alphabet ,2): 
17
print(”. join  (  i  )  ,  end=’  ’)  
18 print () 
1 aa ac ag at ca cc cg ct ga gc gg gt ta tc tg t t 
2 ac ag at ca cg ct ga gc gt ta tc tg 
3 ac ag at cg ct gt 
4 aa ac ag at cc cg ct gg gt t t

Chapter 8 
Laboratory in Python 
8.1 
Extraction of Symbols 
Given a string ss we would like to know if it is contained in the genome. We can 
use the set data structure, which is a modiﬁable collection of elements with no 
repetitions. We iterate over the positions of ss and extract the characters at the given 
position. 
1 s = ’agctaggaggatcgccagat ’ 
2 
3 alphabet = set () 
4 for i in range( len(s) ): 
5
alphabet .add( s[ i ] ) 
6 
7 print(“ your alphabet is : ” , alphabet) 
The result is: 
1 your alphabet is :
{’g’ ,  ’c’ ,  ’a’ ,  ’  t  ’}  
Alternatively, since a string is an iterable object, we can iterate over the characters 
of ss with no indication of their position. 
1 alphabet = set () 
2 for c in s: 
3
alphabet .add( c ) 
4 
5 print(“ your alphabet is : ” , alphabet) 
1 your alphabet is :
{’  t  ’ ,  ’a’ ,  ’c’ ,  ’g’}  
The set data structure does not assure any speciﬁc order regarding the elements 
contained in it. Thus multiple runnings of the same source code can generate the 
same set but print the symbols in a different order. The sorted function returns a 
list of sorted elements. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9_8 
223

224
8
Laboratory in Python
1 print(“ your sorted alphabet is : ” , sorted(alphabet) ) 
1 your sorted alphabet is :
[  ‘a’ ,  ‘c’ ,  ‘g’ ,  ‘  t  ’]  
The sorted function requires that there must exist an ordering relation between 
the elements of the input collection. The following instructions generate an error 
because there is no deﬁned ordering relation between strings (or simple characters) 
and integers. 
1 myset = {‘a’ , 0} 
2 print ( sorted (myset)) 
However, the last instruction throws an error: 
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 TypeError
Traceback (most recent call last ) 
3 <ipython−input−25−d74947341104> in <module>() 
4
1 myset  =  {‘a’  ,  0}  
5 −−−−> 2  print  (  sorted  (myset))  
6 
7 TypeError: unorderable types : str () < int () 
The way lists and sets (but more in general any built-in data structure) are printed 
is deﬁned by Python. Lists are delimited by square brackets, while sets are delimited 
by curly brackets, and elements are separated by commas. However, we would like 
to personalize the print of our alphabet. A ﬁrst approach could be the creation of an 
empty string that will be concatenated to the symbols of the alphabet such that they 
are interleaved by a separator character. 
1 alpha_string = ” 
2 
3 # sets are iterable objects , thus they can be used for cycling 
4 for a in sorted(alphabet ): 
5
alpha_string = alpha_string + ‘ , ’ + a 
6 
7 print ( ‘your sorted alphabet is : ’ + alpha_string) 
1 your sorted alphabet is : ,a,c,g, t 
Since the ﬁrst comma is unwanted, we can extract the created string without the 
ﬁrst character. 
1 print( ‘your sorted alphabet is : ’ + alpha_string [1:]) 
1 your sorted alphabet is : a  ,c  ,g,  t  
The same string can be obtained by using a function deﬁned for the string class, 
called join. Given a string and a collection of elements, the joinfunction con-
catenate the element by putting a copy of the string between them. 
1 print( ‘ , ’ . join (alphabet) ) 
1 t ,a,c,g

8.2 Extraction of Words
225
We can use the join function on lists generated by comprehension as well. 
1 ‘ , ’ . join ( [ i for i in range(0,11)] ) 
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 TypeError
Traceback (most recent call last ) 
3 <ipython−input−16−475d059feeaf> in <module>() 
4 −−−−> 1 ‘ , ’ . join ( [ i for i in range(0,11)] ) 
5 
6 TypeError: sequence item 0: expected str instance , int 
7 found 
However, we need to remember that the concatenation operation is not deﬁned 
in python between elements that are not strings. Thus, if we want to concatenate 
elements that are not strings, we need to explicitly cast the elements to be strings. 
1 ‘ , ’ . join ( [ str ( i ) for i in range(0,11)] ) 
1 
‘0,1,2,3,4,5,6,7,8,9,10 ’ 
1 print(“ your sorted alphabet is : ” + 
2
‘ , ’ . join ( [ str (a) for a in sorted (alphabet )] ) ) 
1 your sorted alphabet is : a  ,c  ,g,  t  
8.2 
Extraction of Words 
The extraction of the alphabet of a string corresponds to the extraction of the 1-mers 
contained in the string. The procedure can be generalized to extract 2-mers in the 
following manner: 
1 words = set () 
2 for i in range(len(s )): 
3
words.add( s[ i : i+2] ) 
4 print( words ) 
1 {‘at  ’ ,  ‘gg’  ,  ‘ca’ ,  ‘ga’  ,  ‘tc  ’ ,  ‘cc’  ,  ‘  t  ’  ,  ‘ta  ’ ,  ‘ag’  ,  ‘cg’ ,  ‘ct  ’  ,  
2 ‘gc’} 
A procedure giving the same result can be represented as a sliding window, having 
a size equal to two, and that starts from the ﬁrst position of the string, then it extracts 
the ﬁrst 2-mer and, subsequently, extracts the other 2-mers by moving forward of 
one step. An easy way to visualize all the extracted windows is to append them to a 
list and then print the list: 
1 words = list () 
2 for i in range(len(s )): 
3
words.append( s[ i : i+2] ) 
4 print(s) 
5 print( words )

226
8
Laboratory in Python
1 agctaggaggatcgccagat 
2 [  ‘ag’  ,  ‘gc’  , ‘ct  ’  ,  ‘ta  ’  , ‘ag’  ,  ‘gg’  , ‘ga’  ,  ‘ag’  , ‘gg’  ,  ‘ga’  , ‘at  ’  ,  
3 ‘tc  ’  , ‘cg’  ,  ‘gc’  , ‘cc’  ,  ‘ca’  , ‘ag’  ,  ‘ga’  , ‘at  ’  ,  ‘  t  ’]  
For avoiding the extraction of the last tt, which is not a 2-mer, a general function 
get_kmers is deﬁned that takes as input the string ss and the word length kk, and it 
returns all and only the k-mers occurring in ss. 
1 def get_kmers(s , k): 
2
“““ This function returns the set of k−mers that occur in a 
3
given string ””” 
4 
5
kmers = set () 
6
for i in range(len(s) − k +1):  
7
kmers.add(s[ i : i+k]) 
8
return kmers 
9 
10 kmers = get_kmers(s ,3) 
11 print(kmers) 
1 {‘gga’ ,  ‘gag’ ,  ‘cag’ ,  ‘tag  ’ ,  ‘cgc’ ,  ‘cta  ’ ,  ‘gcc’ ,  ‘aga’ ,  ‘tcg  ’ ,  
2 ‘agg’ ,  ‘cca’ ,  ‘atc  ’ ,  ‘gct  ’ ,  ‘agc’ ,  ‘gat  ’}  
8.3 
Word Multiplicity 
In what follows, a function that counts the number of characters cc within a string ss
is given. The function scans every position of ss and updates the counter in case a 
cc symbol occurs in the position. The procedure equals the built-in count function 
deﬁned for Python strings. 
1 def count_C(s ): 
2
count_c = 0 
3
for a in s: 
4
if a ==  ’c’  :  
5
count_c += 1 
6
return count_c 
7 
8 nof_c = count_C(s) 
9 
10 print(s) 
11 print(nof_c) 
12 print(s . count( ’c’ )) 
1 agctaggaggatcgccagat 
2 4 
3 4 
Python functions can return multiple values. For example, the following function 
counts the number of cc and gg, and returns the two separated values.

8.3 Word Multiplicity
227
1 def count_CG(s ): 
2
"""Count the number of c and g in a given string and return the 
3 counts 
4
−−−−−−−− 
5
Parameters: 
6
s ( str ) : the input string 
7
−−−−−−−− 
8
Returns: 
9
int : the count of c 
10
int : the count of g 
11
""" 
12
count_c , count_g = 0,0 
13
for a in s: 
14
if a ==  ’c’  :  
15
count_c += 1 
16
elif a ==  ’g’  :  
17
count_g += 1 
18
return count_c , count_g 
19 
20 nof_c , nof_g = count_CG(s) 
21 
22 print(s) 
23 print(nof_c , nof_g) 
24 print(s . count( ’c’ ) , s . count( ’g’ )) 
1 agctaggaggatcgccagat 
2 4 7  
3 4 7  
Python functions receive multiple input parameters and return multiple values, 
and moreover, Python is not a typed programming language, therefore, understanding 
the correct value to be passed to a function, as well as the meaning and the type of 
the returned values may result difﬁcult for the ﬁnal developer who wants to use the 
function. Thus, documentation of the function plays a crucial role in the usability 
of Python. The code shown below gives an example of how to document input 
parameters and return values of a function. Return values do not have names, but the 
order in which they are returned is crucial for correct comprehension. 
1 help(count_CG) 
1 Help on function count_CG in module __main__: 
2 
3 count_CG(s) 
4
Count the number of c and g in a given  string  and return the 
5 counts 
6
−−−−−−−− 
7
Parameters: 
8
s (  str ) :  the  input string 
9
−−−−−−−− 
10
Returns: 
11
int : the count of c 
12
int : the count of g

228
8
Laboratory in Python
The CG content of a string corresponds to the sum of the number of cc and gg
characters divided by the total length of the string. The following Python function 
computes the CG content. 
1 print(“ the CG content of ”+s+ is ”+str ((nof_c+nof_g)/ len(s ))) 
2 print( str ((nof_c + nof_g))+“ of ” +str (len(s )) +“ positions”) 
1 the CG content of agctaggaggatcgccagat is 0.55 
2 11 of 20 positions 
8.4 
Counting Words 
The previous example showed how to count for speciﬁc single characters, how-
ever, counting the occurrences of longer words (k-mers) is often required. The 
count_occurrences function counts the number of occurrences of a word ww
in a string ss. For each position of ss it veriﬁes if an occurrence of ww starts from that 
position. The veriﬁcation is made by comparing one by one the characters ofww with 
those occurring in the sibling positions of ss. 
The built-in python library already provides a string function to count the occur-
rences of a substring. However, this exercise wants to emphasize the complexity of 
such a searching approach. It requires two nested for loops, which result in a time 
that is StartAbsoluteValue s EndAbsoluteValue|s| times StartAbsoluteValue w EndAbsoluteValue|w|. 
More generally, the reader is encouraged to understand the complexity of the built-
in functions when they are used, because they can conceal an unexpected complexity 
in time or space. The function given in the next example is the most trivial way to 
count substring occurrences, but more powerful methods exist, available in the Python 
library. 
1 def count_occurrences(s ,w): 
2
“““ 
3
Count the number of occurrences of w in s 
4
−−−−−−−− 
5
Parameters: 
6
s (  str ) 
7
w (  str ) 
8
””” 
9
count = 0 
10
for i in range(len(s) −len(w) +1): 
11
for j in range(len(w)): 
12
if s[ i+j ] != w[ j ]: 
13
break 
14
else : 
15
count += 1 
16
#i f s[ i : i+len(w)] == w: 
17
#
count += 1 
18
return count 
19

8.4 Counting Words
229
20 print(count_occurrences(s , ‘ga’ )) 
21 print (s . count( ‘ga’ )) 
1 3 
2 3 
When a function to count the number of occurrences of a single word has been 
deﬁned, the multiplicity of all k-mers occurring in a string can be obtained. The fol-
lowing example uses the multiplicity information to inform about the repetitiveness 
of all 2-mers (and more in general k-mers) of the string ss. The k-mers are distin-
guished into hapaxes and repeats, and special repeats are duplexes that occur exactly 
twice. 
1 k = 2  
2 kmers = get_kmers(s ,k) 
3 for kmer in kmers: 
4
m = count_occurrences(s ,kmer) 
5
if m ==  1:  
6
print(kmer + “ is an hapax”) 
7
else : 
8
print(kmer + “ is a repeat”) 
9
if m ==  2:  
10
print(kmer + “ is a duplex”) 
1 cc is an hapax 
2 gc is a repeat 
3 gc is a duplex 
4 ag is a repeat 
5 at is a repeat 
6 at is a duplex 
7 ca is an hapax 
8 tc is an hapax 
9 cg is an hapax 
10 ga is a repeat 
11 gg is a repeat 
12 gg is a duplex 
13 ta is an hapax 
14 ct is an hapax 
The procedure is modiﬁed such that the multiplicity information is not just printed, 
but words are bucketed depending on their occurrence property. The operation is 
performed by representing buckets as sets. 
1 k = 2  
2 kmers = get_kmers(s ,k) 
3 
4 hapaxes = set () 
5 repeats= set () 
6 duplexes = set () 
7 
8 for kmer in kmers: 
9
m = count_occurrences(s ,kmer) 
10
if m ==  1:

230
8
Laboratory in Python
11
hapaxes.add(kmer) 
12
else : 
13
repeats .add(kmer) 
14
if m ==  2:  
15
duplexes .add(kmer) 
16 
17 print( ’hapaxes’ , len(hapaxes) , sorted(hapaxes)) 
18 print( ’repeats ’ , len( repeats ) , sorted( repeats )) 
19 print( ’duplexes’ , len(duplexes) , sorted(duplexes)) 
1 hapaxes 6 [ ‘ca’ , ‘cc’ , ‘cg’ , ‘ct ’ , ‘ta ’ , ‘tc ’] 
2 repeats  5  [  ‘ag’ ,  ‘at  ’  ,  ‘ga’ ,  ‘gc’  ,  ‘gg’]  
3 duplexes 3 [ ‘ at ’ , ‘gc’ , ‘gg’] 
By deﬁnition, the set of duplexes is contained within the complete set of repeats. 
A way to retrieve the set of repeats that occur more than twice is to use the built-
in python set-theoretic operations, in this case, the set difference is denoted by the 
symbol minus−. 
1 nodupl = repeats − duplexes 
2 print(“ repeats not duplexes” , len(nodupl) , sorted(nodupl)) 
1 repeats not duplexes 2 [ ‘ag’ , ‘ga’] 
Unfortunatelly, the built-in method count does not count overlapping occur-
rences. 
1 s = ‘aaa’ 
2 count_occurrences(s , ‘aa’) 
3 s . count( ‘aa’) 
1 1 
In fact, the expected number of aa in the string aaa is three, however, the built-in 
function only takes into account the ﬁrst occurrence of the pattern and ignores the 
overlapped one. 
8.5 
Searching for Nullomers 
Nullomers are k-mers over a speciﬁc alphabet that do not appear in a given string. 
Thus, to search for nullomer, ﬁrstly a way to generate the complete set of words is 
necessary. Then, the presence of the k-mers can be veriﬁed. 
The complete set can be computed via a recursive function. It extends words of 
lengthxx to lengthx plus 1x + 1 in a combinatorial way by elongating them with all the sym-
bols of the alphabet. This means that, given a word alphaα, it generates StartSet alpha a comma alpha c comma alpha g comma alpha t EndSet{αa, αc, αg, αt}, 
then it recursively extends such elongations until words of lengthkk are formed. The 
initial string is set to be the empty string. The stop condition of the recursion is 
reached when the formed word has a length equal to kk.

8.5 Searching for Nullomers
231
1 def list_words( prefix , k, alphabet ): 
2
if len( prefix ) == k: 
3
print( prefix ) 
4
else : 
5
for a in alphabet : 
6
list_words( prefix + a , k, alphabet) 
7 
8 nuc_alphabet = [ ‘a’ , ‘c’ , ‘g’ , ‘ t ’] 
9 
10 list_words(” , 1, nuc_alphabet) 
1 a 
2 c 
3 g 
4 
t 
1 list_words(” , 2, nuc_alphabet) 
1 aa 
2 ac 
3 ag 
4 at 
5 ca 
6 cc 
7 cg 
8 ct 
9 ga 
10 gc 
11 gg 
12 gt 
13 ta 
14 tc 
15 tg 
16 
tt 
This function can be modiﬁed in such a way that k-mers are not printed, but added 
to an output set. 
1 def list_words_2( prefix , k, nuc_alphabet , words): 
2
if len( prefix ) == k: 
3
words.add( prefix ) 
4
else : 
5
for a in nuc_alphabet: 
6
list_words_2( prefix + a , k, nuc_alphabet , words) 
7 
8 kmers = set () 
9 list_words_2(”,2 , nuc_alphabet , kmers) 
10 print(kmers) 
1 {‘cc’  ,  ‘gc’  , ‘gt  ’  ,  ‘tg  ’  , ‘ct  ’  ,  ‘ag’  , ‘ac’  ,  ‘at  ’  , ‘ca’  ,  ‘cg’  , ‘tc  ’  ,  
2 ‘  t  t  ’  , ‘ga’  ,  ‘gg’  , ‘ta  ’  ,  ‘aa’}  
For each k-mer, if its multiplicity within the string is equal to zero it means that 
it is not occurring in the string, thus it is a nullomer.

232
8
Laboratory in Python
1 k = 2  
2 kmers = set () 
3 list_words_2(” ,k, nuc_alphabet ,kmers) 
4 
5 nullomers = set () 
6 for kmer in kmers: 
7
if count_occurrences(s ,kmer) == 0: 
8
nullomers .add(kmer) 
9 
10 print(s) 
11 print(nullomers) 
1 agctaggaggatcgccagat 
2 {‘ac’ ,  ‘tg  ’  ,  ‘  t  t  ’ ,  ‘gt  ’  ,  ‘aa’}  
The mere presence of a single occurrence can be searched for. If on average k-mer 
appear more than twice, the just-presence method reduces the time of the procedure 
since it stops to the ﬁrst occurrence and does not search for other occurrences. 
Python provides a built-in operator, that is inclusion operator in, to check for 
presence of an elements within a set. The operator can also be combined with the 
logical not operator. 
1 A = {‘a’ ,‘b’ ,‘c’} 
2 print ( ‘a’ in A) 
3 print( ‘a’ not in A) 
1 True 
2 False 
When the in operator is applied to strings, it searches whenever a string is contained 
in another. 
1 k = 2  
2 kmers = set () 
3 list_words_2(” ,k, nuc_alphabet ,kmers) 
4 
5 nullomers = set () 
6 for kmer in kmers: 
7
if kmer not in s: 
8
nullomers .add(kmer) 
9 
10 print(s) 
11 print( ‘nullomers: ’ , nullomers) 
1 agctaggaggatcgccagat 
2 nullomers  :  {‘ac’  , ‘tg  ’  ,  ‘  t  t  ’  , ‘gt  ’  ,  ‘aa’}

8.6 Dictionary Coverage
233
8.6 
Dictionary Coverage 
The sequence coverage of a dictionaryupper DD over a genomess is deﬁned as the number 
of positions of the genome that are involved in at least one occurrence of a word of 
the dictionary. 
For each word in the dictionary, we ﬁrst need to ﬁnd its occurrences within the 
genome, and then we need to keep track of the positions that they cover. 
The function get_positions returns the list of starting positions of the occur-
rences of a word ww in a string ss. 
An array of boolean values is used to keep track of the coverage of the occurrence. 
Initially, the array is ﬁlled with ‘False’ values, subsequently, it is ﬁlled with ‘True’ 
values for each position covered by the word of the given dictionary. 
A dictionary may contain words of different lengths, but this property does not 
inﬂuence the deﬁnition of sequence coverage. 
1 def get_positions(s ,w): 
2
“““ 
3
Return the starting positions in a reference string s where 
4
a word w occurs 
5
−−−− 
6
Parameters: 
7
s (  str ) : the reference string 
8
w (  str ) : the searched word 
9
−−−− 
10
Returns: 
11
list [ int ] : the positions 
12
””” 
13
positions = list () 
14
for i in range(len(s )): 
15
if s[ i : i+len(w)] == w: 
16
positions .append( i ) 
17
return positions 
18 
19 # s = ‘agctaggaggatcgccagat’ 
20 dictionary = [ ‘ga’ , ‘ag’ , ‘ca’ , ‘ t ’ , ‘aaaa’] 
21 
22 coverage = [False for _ in range(len(s ))] 
23 
24 # coverage = [False] ∗ len(s) 
25 
26 for w in dictionary : 
27
for pos in get_positions(s ,w): 
28
for i in range(len(w)): 
29
coverage[pos + i ] = True 
30 
31 print (s) 
32 print (coverage) 
33 
34 print ( ‘sequence coverage: ’ , coverage . count(True) , ‘covered positions 
35 
over a total of ’ , len(s),‘= ’ , coverage . count(True)/ len(s ))

234
8
Laboratory in Python
1 agctaggaggatcgccagat 
2 [True, True, False , True, True, True, True, True, True, True, True, 
3 True, False  , False  , False  ,  True,  True,  True,  True,  True]  
4 sequence coverage: 16 covered positions over a total of 20 = 0.8 
When a kk-mer occurs in a given genome, it covers kk positions, the position of 
its ﬁrst character, which is considered the position of its occurrence, and theleft parenthesis k minus 1 right parenthesis(k −1)
following positions of the kk-mer. The position coverage of a given genome, with 
respect to a given dictionary, is deﬁned as the number of words of the dictionary that 
cover a given position of the genome. 
The computation of the position coverage is obtained by modiﬁcation of the 
implementation of sequence coverage. The trace array is converted from a boolean 
vector to an array of integers. The array is initially ﬁlled with zero values. For each 
word occurrence, the corresponding positions in the array are increased by a value 
equal to 11. 
1 def get_positions(s ,w): 
2
“““ 
3
Return the starting positions in a reference string s where 
4
a word w occurs 
5
−−−− 
6
Paramters: 
7
s (  str ) : the reference string 
8
w (  str ) : the searched word 
9
−−−− 
10
Returns: 
11
list [ int ] : the positions 
12
””” 
13
positions = list () 
14
for i in range(len(s )): 
15
if s[ i : i+len(w)] == w: 
16
positions .append( i ) 
17
return positions 
18 
19 print(s) 
20 
21 dictionary = [ ‘ga’ , ‘ag’ , ‘ca’ , ‘ t ’] 
22 
23 coverage = [0 for _ in range(len(s )) ] 
24 for w in dictionary : 
25
for pos in get_positions(s ,w): 
26
for i in range(len(w)): 
27
coverage[pos + i ] += 1 
28 
29 print(coverage) 
1 agctaggaggatcgccagat 
2 [1  ,  1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 2, 2, 1,  1]  
The sequence coverage can also be retrieved from the integer array by counting 
the number of cells with a value that differs from zero.

8.7 Reading FASTA Files
235
1 print( ‘sequence coverage’ ,( len(coverage) − coverage . count(0)) , ‘/ ’ , 
2
len(coverage),‘= ’ , 
3
(len(coverage) − coverage . count(0)) / len(coverage)) 
1 sequence coverage 16 / 20 = 0.8 
Statistics concerning the positional coverage for all the positions of the genome 
can be computed by the following script. 
1 from statistics import mean, stdev 
2 
3 print( ‘average positional coverage’ , mean(coverage)) 
4 print ( ‘standard deviation of positional coverage’ , stdev(coverage)) 
1 average positional coverage 0.95 
2 standard deviation of positional coverage 0.6048053188292994 
List comprehension can be used to calculate the mean positional coverage of only 
the covered positions. 
1 print( ’average positional coverage of covered positions ’ , 
2
mean([ i for i in coverage if i > 0])) 
3 print( ’standard deviation of positional coverage of covered 
4 positions ’ , stdev ([ i for i in coverage if i > 0])) 
1 average positional coverage of covered positions 1.1875 
2 standard deviation of positional coverage of covered positions 
3
0.4031128874149275 
8.7 
Reading FASTA Files 
FASTA ﬁles are text ﬁles containing genomic sequences written in the FASTA format. 
Multiple sequences can be stored in a single ﬁle. Each sequence is preceded by a 
description line that starts with agreater than> character. Sequences are split into multiple lines, 
usually having at most 80 characters per each. 
In python, ﬁle pointers are provided by the built-in open function ( in reading 
or writing mode). This function returns an object that technically is a pointer to the 
ﬁle but it can also be used as an iterable object. The iteration is performed along the 
lines of the ﬁle. 
1 i f i l e = ’mycoplasma_genitalium_G37. fna ’ 
2 
3 s =  ” 
4 
5 for line in open( ifile , ’r ’ ): 
6
if line . strip ()[0] != ’>’ : 
7
s += line . strip () 
8 
9 print( len(s) )

236
8
Laboratory in Python
1 580076 
The split function removes multiple blank characters (spaces, tabulations and 
any other printable symbol that results in a blank print) from the left and right of the 
string. 
The ﬁrst use of strip ensures that the greater than> symbol is checked even if spaces are 
inserted before. The second one is used to remove the new line character, namelyset minus monospace n\n
from the read line. If the strip is not performed, the resultant genomic sequence 
will be a string of lines concatenated by set minus monospace n\n character. This means that the alphabet 
of the string will contain also the set minus monospace n\n character. 
8.8 
Informational Indexes 
The maximum repeat length (mrl) is deﬁned as the length of the longest repeat. 
Starting from k greater than 1k > 1, the search for the mrl checks for the existence of a repeat of 
length kk, and in a positive case it scans forward to k plus 1k + 1. If no repeat is found at 
length k plus 1k + 1, then the procedure stops and the returned mrl value is kk. 
1 def mrl(s ): 
2
“““ Calculate the maximal repeat length of a string s ””” 
3
k = 0  
4
mrl = 0 
5
next_k = True 
6
while next_k: 
7
k += 1  
8
next_k = False 
9
for kmer in get_kmers(s ,k): 
10
mult = count_occurrences(s ,kmer) 
11
if mult > 1: 
12
mrl = k 
13
next_k = True 
14
return mrl 
15 
16 mrl_s = mrl(s[:1000]) 
17 # redefining mrl as a variable will overwrite the mrl function 
18 # definition , thus the name of the variable is mrl_s 
19 print(‘mrl’ , mrl_s) 
1 mrl 14 
A modiﬁed version of the function returns the mrl value together with one of the 
repeats having such a length. The multiplicity value is also returned. 
1 def mrl(s ): 
2
“““ Calculate the maximal repeat length of a string s ””” 
3
k = 0  
4
mrl = 0 
5
kmer_mrl = ” 
6
mult_mrl = 0

8.8 Informational Indexes
237
7
next_k = True 
8
while next_k: 
9
#print (k , end=” , sep=’  ’)  
10
k += 1  
11
next_k = False 
12
for kmer in get_kmers(s ,k): 
13
mult = count_occurrences(s ,kmer) 
14
if mult > 1: 
15
mrl = k 
16
kmer_mrl = kmer 
17
mult_mrl = mult 
18
next_k = True 
19
return mrl, kmer_mrl, mult_mrl 
20 
21 
22 mrl_s , kmer_mrl, mult_mrl = mrl(s[:1000]) 
23 # redefining mrl as a variable will overwrite the mrl function 
24 # definition , thus the name of the variable is mrl_s 
25 
26 
27 print( ‘mrl’ , mrl_s , ’ , kmer ’ , kmer_mrl, ’ , multiplicity ’ , mult_mrl) 
1 mrl 14 , kmer TAACAATATTATTA , multiplicity 2 
The search for the minimum hapax length (mhl) is similar to the search for mrl. 
Starting from k equals 1k = 1, the multiplicity of kk-mers is examined and the value of kk is 
increased until the ﬁrst hapax has not been found. 
1 def mhl(s ): 
2
“““ Calculate the minimal hapax length of a string s ””” 
3
k = 0  
4
mhl = 0 
5
kmer_mhl = ” 
6
mult_mhl = 0 
7
next_k = True 
8
while next_k: 
9
k += 1  
10
for kmer in get_kmers(s ,k): 
11
mult = count_occurrences(s ,kmer) 
12
if mult == 1: 
13
mhl = k 
14
kmer_mhl = kmer 
15
mult_mhl = mult 
16
next_k = False 
17
return mhl, kmer_mhl, mult_mhl 
18 
19 
20 mhl_s, kmer_mhl, mult_mhl = mhl(s[:1000]) 
21 
22 print( ‘mhl’ , mhl_s, ’ , kmer ’ , kmer_mhl, ’ , multiplicity ’ , mult_mhl) 
1 mhl 3 , kmer CGA , multiplicity 1

238
8
Laboratory in Python
The minimum forbidden length is calculated by comparing the size of the effective 
set of k-mers occurring in a sequence with 4 Superscript k4k. The smallest value of kk at which the 
two sizes differ is the wanted value. 
1 def get_alphabet(s ): 
2
al = set () 
3
for c in s: 
4
al .add(c) 
5
return al 
6 
7 def mfl(s , alphabet = None): 
8
“““ Calculate the minimal forbidden length of a string s ””” 
9
if alphabet == None: 
10
a =  len(get_alphabet(s )) 
11
else : 
12
a =  len(alphabet) 
13 
14
k = 0  
15
while True: 
16
k += 1  
17
kmers = get_kmers(s ,k) 
18
if len(kmers) != a∗∗k: 
19
return k 
20 
21 
22 mfl_s = mfl(s) 
23 
24 
25 print(‘mfl’ ,mfl_s , ’ , mcl’ , mfl_s − 1) 
1 mfl 6 , mcl 5 
The reader may notice that in the previous examples, the variable used to store 
the calculated indexes never has the same name as the function used to calculate 
it. This is due to the fact that if a variable with the same name is used, this fact 
overwrites the declaration of the function, therefore calling the function produces an 
error exception, as it is shown in the following example. 
1 def my_funct(): 
2
return 0 
3 
4 my_funct = my_funct() 
5 # declaring a variable having the same name of a function 
6 # declared before 
7 print(my_funct) 
1 0 
1 my_funct() 
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 TypeError
Traceback (most recent call last ) 
3 <ipython−input−13−50af49a3d99d> in <module>()

8.9 Genomic Distributions
239
4 my_funct() 
5 TypeError: ‘int ’ object is not callable 
Another useful informational index is also the empirical kk-entropy. In order to 
compute (empirical) kk-entropy, multiplicities of words are converted into probabili-
ties (frequencies) and then classical Shannon’s formula is used: 
1 import math 
2 
3 def k_entropy(s ,k): 
4
“““ Calculate the empirical entropy at word length k of a 
5
string s ””” 
6 
7
t =  0.0  
8
for kmer in get_kmers(s ,k): 
9
t += count_occurrences(s ,kmer) 
10 
11
e =  0.0  
12
for kmer in get_kmers(s ,k): 
13
e += math. log(count_occurrences(s ,kmer) / t , 2) 
14
return −e 
15 
16 k = 2  
17 print( str (k)+‘−entropy ’ , k_entropy(s ,k)) 
1 2−entropy 68.1465951013701 
8.9 
Genomic Distributions 
In this section, Python programs are presented that extract the most important distri-
butions of informational genomics. 
Word Multiplicity Distribution (WMD) 
The word multiplicity distribution (WMD) is a function that assigns to any word 
its corresponding multiplicity in a given string. In the following example, to each 
symbol of the alphabet, the number of its occurrences within a string is assigned. 
1 nuc_alphabet = [ ‘a’ , ‘c’ , ‘g’ , ‘ t ’] 
2 for n in nuc_alphabet: 
3
print(n, count_occurrences(s ,n)) 
1 a 6  
2 c 4  
3 g 7  
4 
t 3

240
8
Laboratory in Python
Multiplicities for 2-mers occurring in a string s equals prime a g c t a g g a g g a t c g c c a g a t primes =' agctaggaggatcgccagat' are 
given below. 
1 kmers = get_kmers(s , k = 2) 
2 for kmer in kmers: 
3
print(kmer, count_occurrences(s ,kmer)) 
1 tc 1 
2 cg 1 
3 ca 1 
4 ta 1 
5 ct 1 
6 gc 2 
7 ga 3 
8 gg 2 
9 at 2 
10 cc 1 
11 ag 4 
WMD as key-value collection 
The word multiplicity distribution provides a python dictionary which assigns mul-
tiplicity values (integers) to a set of key k-mers (strings). 
1 WMD= dict () # multiplicity distribution 
2 kmers = get_kmers(s ,2) 
3 for kmer in kmers: 
4
WMD[kmer] 
= count_occurrences(s ,kmer) 
5 
6 for kmer in WMD.keys (): 
7
print(kmer, WMD[kmer]) 
1 tc 1 
2 cg 1 
3 ga 3 
4 gg 2 
5 ta 1 
6 ct 1 
7 at 2 
8 ca 1 
9 cc 1 
10 ag 4 
11 gc 2 
There is no assumption about the order in which keys, or pairs, are iterated because 
it depends on the internal data structure used by the speciﬁc python version to imple-
ment the dictionary. Thus, in case the user wants to output the distribution in the 
lexicographical order of the k-mers, explicit sorting must be performed. 
1 for kmer in sorted(WMD.keys ()): 
2
print(kmer, WMD[kmer])

8.9 Genomic Distributions
241
1 ag 4 
2 at 2 
3 ca 1 
4 cc 1 
5 cg 1 
6 ct 1 
7 ga 3 
8 gc 2 
9 gg 2 
10 ta 1 
11 tc 1 
Alternatively, the dict’s iterator returns a list of pairs (key,value) that can 
be automatically decomposed into the variables kmer and mult. 
1 for kmer,mult in sorted(WMD. items ()): 
2
print(kmer, mult) 
1 ag 4 
2 at 2 
3 ca 1 
4 cc 1 
5 cg 1 
6 ct 1 
7 ga 3 
8 gc 2 
9 gg 2 
10 ta 1 
11 tc 1 
Retrieving the set of k-mers and then searching for their multiplicity results to be a 
redundant combination of steps. Namely, the distribution can be built while scanning 
for k-mers in the string and, at the same time, by updating their multiplicity values. 
1 WMD= dict () 
2 k = 2  
3 for i in range(len(s) −k +1):  
4
# extraction of the k−mer at position i 
5
w =  s[  i  :  i+k]  
6
# update of the occurrence of the k−mer w 
7
WMD[w] += 1 
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 KeyError
Traceback (most recent call last ) 
3 <ipython−input−38−8cbf6223371f> in <module>() 
4
3 for i in range(len(s) −k +1):  
5
4
# extraction of the k−mer at position i 
6
5
w
 
=
 
s[
 
i
 
:
 
i+k]
 
7
6
# update of the occurrence of the k−mer w 
8 −−−−> 7
WMD[w] += 1 
9 
10 KeyError: ‘ag’

242
8
Laboratory in Python
The single-slice operator of dict can only be used to access pairs that have been 
previously inserted in the collection. Thus, using it to access a key not yet present 
will cause an error. 
A possible way to solve the problem is to ﬁrst check for the presence of the key, 
and in case of absence make a ﬁrst insertion of it. The operator ‘in‘ allows one to 
check for the existence of a key in a dictionary. 
1 WMD= dict () 
2 k = 2  
3 for i in range(len(s) −k +1):  
4
w =  s[  i  :  i+k]  
5
if w in WMD: 
6
WMD[w] += 1 
7
else : 
8
WMD[w] = 1 
9 
10 for kmer in WMD.keys (): 
11
print(kmer, WMD[kmer]) 
1 tc 1 
2 cg 1 
3 ga 3 
4 gg 2 
5 ta 1 
6 ag 4 
7 at 2 
8 ca 1 
9 cc 1 
10 gc 2 
11 ct 1 
Alternatively, the built-in get function can be used, which can use a default value 
that is returned in case of a missing key, without causing any error. 
In the next example, the default value of zero is used. Ifww has not yet been inserted 
as a key, the get operation returns 00 and the ﬁnal counter is set to 11. Conversely, if 
ww already exists within the collection, the operation returns its current counter that 
will be updated by 1. 
1 WMD= dict () 
2 k = 2  
3 for i in range(len(s) −k +1):  
4
w =  s[  i  :  i+k]  
5
WMD[w] = WMD. get(w, 0) + 1 
6 
7 for kmer in WMD.keys (): 
8
print(kmer, WMD[kmer]) 
1 tc 1 
2 cg 1 
3 ga 3 
4 gg 2 
5 ta 1

8.9 Genomic Distributions
243
6 ag 4 
7 at 2 
8 ca 1 
9 cc 1 
10 gc 2 
11 ct 1 
Once a working code has been written, it can be also encapsulated in a function. 
1 def get_multiplicity_distribution (s ,k): 
2
“““ 
3
Return the word multiplciity distribution of k−mers 
4
occurring in the string s 
5
−−−−−− 
6
Parameters: 
7
s (  str ) :  the  input string 
8
k (  int ) : the length of the k−mers 
9
−−−−−−− 
10
Returns: 
11
dict [str , int ] : a dictionary which associates 
12
multiplicity values to the k−mers in s 
13
””” 
14
WMD= dict () 
15
for i in range(len(s) −k +1):  
16
w =  s[  i  :  i+k]  
17
WMD[w] = WMD. get(w,0) + 1 
18
return WMD 
19 
20 WMD = get_multiplicity_distribution (s ,2) 
21 
22 for k,v in sorted(WMD. items ()): 
23
print(k,v) 
1 ag 4 
2 at 2 
3 ca 1 
4 cc 1 
5 cg 1 
6 ct 1 
7 ga 3 
8 gc 2 
9 gg 2 
10 ta 1 
11 tc 1 
The given WMD implementation speeds the calculation of some of the infor-
mational indexes up. For example, in the calculation of k-entropies, which in the 
following example scales down from more than 9 seconds to less than one second. 
1 import math 
2 
3 def k_entropy(s ,k): 
4
“““ 
5
Calculate the empirical k−entrpy on k−mers in s

244
8
Laboratory in Python
6
””” 
7
t =  0.0  
8
for kmer in get_kmers(s ,k): 
9
t += count_occurrences(s ,kmer) 
10 
11
e =  0.0  
12
for kmer in get_kmers(s ,k): 
13
e += math. log(count_occurrences(s ,kmer) / t , 2) 
14
return −e 
15 
16 
17 def fast_k_entropy(s ,k): 
18
“““ 
19
Calculate the empirical k−entrpy on k−mers in s 
20
””” 
21
distr = get_multiplicity_distribution (s ,k) 
22
t =  sum( distr . values ()) 
23
e =  0.0  
24
for v in distr . values (): 
25
e +=  math.  log(v  /  t  ,  2)  
26
return −e 
27 
28 k = 2  
29 
30 import time 
31 
32 start_time = time . time() 
33 print( str (k)+‘−entropy ’ , k_entropy(mg37,k)) 
34 print ( ‘seconds: ’ , time . time() − start_time ) 
35 
36 print () 
37 
38 start_time = time . time() 
39 print( str (k)+‘−entropy ’ , fast_k_entropy(mg37,k)) 
40 print ( ‘seconds: ’ , time . time() − start_time ) 
1 2−entropy 68.14659510137011 
2 seconds: 9.379900455474854 
3 
4 2−entropy 68.14659510137011 
5 seconds: 0.2494194507598877 
Word length distribution 
The word length distribution (WLD) assigns to each word length kk the number of 
kk-mers ofupper GG, namely the size ofupper D Subscript k Baseline left parenthesis upper G right parenthesisDk(G). It can be calculated by a function that, for a 
range of word lengths, uses the predeclared get_kmers procedure. The following 
source code shows also how to graphically visualize the distribution as a histogram by

8.9 Genomic Distributions
245
means of the matplotlib library. The histogram reports for each word lengthkk, on  
the x-axis, the cardinality of the corresponding dictionary in the analysed sequence. 
1 def wld(s , k_start , k_end): 
2
“““ 
3
Calculate the word length distribution of the string s for 
4
the given range of values of word length k 
5
−−−− 
6
Parameters: 
7
s (  str ) :  the  input string 
8
k_start ( int ) : the i nitial word length 
9
k_end ( int ) : the final word length 
10
−−−− 
11
Returns: 
12
dict [int , int ] : a dictionary which associates world 
13
lengths (key) to the number of k−mers at each length 
14
(value) 
15
””” 
16
wld = dict () 
17
for k in range( k_start ,k_end): 
18
wld[k] = len(get_kmers(s ,k)) 
19
return wld 
20 
21 k_start = 1 
22 k_end = 20 
23 wld = wld(mg37, k_start , k_end) 
24 
25 import matplotlib . pyplot as plt 
26 bar_values = [v for k,v in sorted(wld. items ())] 
27 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
28 plt . bar(range( k_start ,k_end) , bar_values) 
29 plt . xticks (range( k_start ,k_end) , range( k_start ,k_end)) 
30 plt . ylabel ( ‘|D_k(G) | ’) 
31 plt . xlabel ( ‘k’) 
32 plt . title (‘Word length distribution ’) 
33 plt .show()

246
8
Laboratory in Python
Average multiplicity distribution 
The average multiplicity distribution (AMD) assigns to each word lengthkk the aver-
age multiplicity of the k-mers of upper GG. 
1 import statistics 
2 
3 def amd(s , k_start , k_end): 
4
“““ 
5
Calculate the average multiplicity distribution of the 
6
string s for the given range of values of word length k 
7
−−−− 
8
Paramters: 
9
s (  str ) :  the  input string 
10
k_start ( int ) : the i nitial word length 
11
k_end ( int ) : the final word length 
12
−−−− 
13
Returns: 
14
dict [int , int ] : a dictionary which associates word 
15
lengths (key) to the average multiplicity at the 
16
specific word length (value) 
17
””” 
18
amd = dict () 
19
for k in range( k_start ,k_end): 
20
amd[k] = statistics .mean( 
21
get_multiplicity_distribution (s ,k). values () ) 
22
return amd 
23 
24 k_start = 1 
25 k_end = 20 
26 amd = amd(mg37, k_start , k_end) 
27 
28 #for k , v in amd. items (): 
29 #
amd[k]
 
=
 
math.
 
log(v)
 
30 
31 #k_start = 12 
32 
33 import matplotlib . pyplot as plt 
34 bar_values = [v for k,v in sorted(amd. items ())] 
35 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
36 plt . bar(range( k_start ,k_end) , bar_values) 
37 plt . xticks (range( k_start ,k_end) , range( k_start ,k_end)) 
38 plt . ylabel ( ‘Average multiplicity ’) 
39 plt . xlabel ( ‘k’) 
40 plt . title (‘Average multiplicity distribution ’) 
41 plt .show()

8.9 Genomic Distributions
247
Empirical entropy distribution 
The empirical entropy distribution (EED) assigns to each word lengthkk the value of 
the k-entropy. 
1 def eed(s , k_start , k_end): 
2
“““ 
3
Calculate the empirical entropy distribution of the string 
4
s for the given range of values of word length k 
5
−−−− 
6
Paramters: 
7
s (  str ) :  the  input string 
8
k_start ( int ) : the i nitial word length 
9
k_end ( int ) : the final word length 
10
−−−− 
11
Returns: 
12
dict [int , int ] : a dictionary which associates word 
13
lengths (key) to the empirical entropy at the specific 
14
word length (value) 
15
””” 
16
eed = dict () 
17
for k in range( k_start ,k_end): 
18
eed[k] = fast_k_entropy(s ,k) 
19
return eed 
20 
21 k_start = 1 
22 k_end = 20 
23 eed = eed(mg37[:10000], k_start , k_end) 
24 
25 #for k , v in amd. items (): 
26 #
amd[k]
 
=
 
math.
 
log(v)
 
27 
28 #k_start = 12 
29 
30 import matplotlib . pyplot as plt 
31 bar_values = [v for k,v in sorted(eed. items ())] 
32 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
33 plt . bar(range( k_start ,k_end) , bar_values) 
34 plt . xticks (range( k_start ,k_end) , range( k_start ,k_end))

248
8
Laboratory in Python
35 plt . ylabel ( ‘k−entropy ’) 
36 plt . xlabel ( ‘k’) 
37 plt . title (‘Empirical entropy distribution ’) 
38 plt .show() 
Word co-multiplicity distribution 
Given a value of kk, the word co-multiplicity distribution (WCMD) reports for each 
value of multiplicity the number of words having such multiplicity. In the following 
example, it is calculated for the ﬁrst 100,000 bases of the m. genitalium G37 genome. 
The source code exploits the predeﬁned get_multiplicity_distribution 
function in order to extract the required information. In fact, given the multiplicity 
distribution, which assigns to each word presented in the genome its multiplicity, the 
co-multiplicity distribution is calculated by counting the number of times a given 
multiplicity appears in the multiplicity distribution. 
1 def wcmd(s ,k): 
2
“““ 
3
Calculate the word co−multiplicity distribution of the 
4
string s for the given value of word length k 
5
−−−− 
6
Paramters: 
7
s (  str ) :  the  input string 
8
k (  int ) : the word length 
9
−−−− 
10
Returns: 
11
dict [int , int ] : a dictionary which associates a 
12
multiplicity value (key) to the number of k−mers 
13
having such multiplicity (value) 
14
””” 
15
distr = dict () 
16
mdistr = get_multiplicity_distribution (s ,k) 
17
for m in mdistr . values (): 
18
distr [m]= distr . get(m,0) + 1 
19
return distr 
20 
21 k = 6  
22 wcmd = wcmd(mg37[:100000],k)

8.10 Genomic Data Structures
249
23 # add missing multiplicity values 
24 for i in range(1 ,max(wcmd.keys ())): 
25
wcmd[ i ] = wcmd. get( i ,0) + 0 
26 
27 import matplotlib . pyplot as plt 
28 bar_values = [v for k,v in sorted(wcmd. items ())] 
29 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
30 plt . bar( sorted (wcmd.keys()) , bar_values , width=1.0) 
31 plt . ylabel ( ‘Number of words’) 
32 plt . xlabel ( ‘ Multiplicity ’) 
33 plt . title (‘Word co−multiplicity distribution ’) 
34 plt .show() 
8.10 
Genomic Data Structures 
The computation of some informational indexes would be prohibitive for real 
long genomes, by using ingenious algorithms, while if suitable data structures are 
employed, computations become possible. One of the most important data struc-
tures, in the ﬁeld of informational genomics, is the sufﬁx array, which we present 
in the following subsection. Additional data structures, such as the longest common 
preﬁx (LCP) array, are added up to the sufﬁx array in order to allow the extraction 
of genomic dictionaries in linear time. Moreover, it has to be pointed out that several 
state-of-the-art algorithms are able to compute both the sufﬁx array and the LCP 
array in linear time, however, since they are not the focus of this book, the user is 
referred to external readings. 
Sufﬁx array 
The sufﬁx array is the most-used data structure to index texts (strings over any 
alphabet) such that operations like the search of a substring are made more efﬁciently 
with respect to online approaches that do not use indexing structures. 
Given a string ss and the set of all the sufﬁxes of ss, the sufﬁx array reports the 
starting position of the ith sufﬁx in the lexicographical order.

250
8
Laboratory in Python
The lexicographic order over strings has been deﬁned in the Chap. 4, Informa-
tional Genomics. However, many variants are used in mathematics, linguistics, and 
computer science. The standard version, we call strong lexicographic order, gives  a  
preliminary order condition where any string precedes the longer ones, while strings 
of the same length are compared according to the ﬁrst position, from the left, where 
they disagree, and the order of their symbols in that position gives the order of 
strings. A second variant, also called alphabetic order, is the lexicographic order 
used in the context of sufﬁx arrays. In this version, the preliminary order by length 
is avoided, and strings are directly compared according to their ﬁrst disagreement 
position. However, in this case, inﬁnite chains between two strings are possible, for 
example, if 0 less than 10 < 1, for any natural number nn strings 0 Superscript n0n precede 1. 
Given a stringss, the set of its sufﬁxes can be extracted in the following manner: 
1 s = ’agctagctagctagtttagct ’ 
2 
3 suffixes = list () 
4 
5 for i in range(len(s )): 
6
suffixes .append(s[ i :]) 
7 
8 for suff in suffixes : 
9
print(suff) 
1 agctagctagctagtttagct 
2 gctagctagctagtttagct 
3 ctagctagctagtttagct 
4 tagctagctagtttagct 
5 agctagctagtttagct 
6 gctagctagtttagct 
7 ctagctagtttagct 
8 tagctagtttagct 
9 agctagtttagct 
10 gctagtttagct 
11 ctagtttagct 
12 tagtttagct 
13 agtttagct 
14 gtttagct 
15 
tttagct 
16 ttagct 
17 tagct 
18 agct 
19 gct 
20 ct 
21 t 
Then, the sufﬁxes can be lexicographically sorted via the python function sorted. 
1 for suff in sorted( suffixes ): 
2
print(suff) 
1 agct

8.10 Genomic Data Structures
251
2 agctagctagctagtttagct 
3 agctagctagtttagct 
4 agctagtttagct 
5 agtttagct 
6 ct 
7 ctagctagctagtttagct 
8 ctagctagtttagct 
9 ctagtttagct 
10 gct 
11 gctagctagctagtttagct 
12 gctagctagtttagct 
13 gctagtttagct 
14 gtttagct 
15 t 
16 tagct 
17 tagctagctagtttagct 
18 tagctagtttagct 
19 tagtttagct 
20 ttagct 
21 
tttagct 
NOTE: the strong lexicographical order can be obtained by using the parameter 
key of the function sorted. In this manner, the words are sorted by following the 
order driven by the function speciﬁed in the parameter, and in case of equality the 
lexicographical order is used. 
1 qllist = [ ‘a’ , ‘b’ , ‘ab’ , ‘ac’ , ‘bb’ , ‘abc’ , ‘bba’ , ‘bbc’] 
2 
3 for suff in sorted( qllist , key = len ): 
4
print(suff) 
1 a 
2 b 
3 ab 
4 ac 
5 bb 
6 abc 
7 bba 
8 bbc 
A trivial and inefﬁcient way to construct the sufﬁx array is to build a list of pairs, 
where the ﬁrst element of the pair is one of the sufﬁxes and the second element is its 
starting position. Because starting positions are all different, the sorted function 
sorts the pairs by the lexicographical order of the sufﬁxes. 
1 s = ’agctagctagctagtttagct ’ 
2 
3 pairs = list () 
4 
5 for i in range(len(s )): 
6
pairs .append( (s[ i :] , i ) ) 
7 
8 for p in sorted(pairs ): 
9
print(p)

252
8
Laboratory in Python
1 ( ’agct ’ , 17) 
2 ( ’agctagctagctagtttagct ’ , 0) 
3 ( ’agctagctagtttagct ’ , 4) 
4 ( ’agctagtttagct ’ , 8) 
5 ( ’ agtttagct ’ , 12) 
6 ( ’ct ’ , 19) 
7 ( ’ctagctagctagtttagct ’ , 2) 
8 ( ’ ctagctagtttagct ’ , 6) 
9 ( ’ ctagtttagct ’ , 10) 
10 ( ’gct ’ , 18) 
11 ( ’gctagctagctagtttagct ’ , 1) 
12 ( ’gctagctagtttagct ’ , 5) 
13 ( ’gctagtttagct ’ , 9) 
14 ( ’ gtttagct ’ , 13) 
15 ( ’ t ’ , 20) 
16 ( ’tagct ’ , 16) 
17 ( ’tagctagctagtttagct ’ , 3) 
18 ( ’ tagctagtttagct ’ , 7) 
19 ( ’ tagtttagct ’ , 11) 
20 ( ’ ttagct ’ , 15) 
21 ( ’ tttagct ’ , 14) 
Once the pairs are sorted, the sufﬁx array is built by iterating the sorted pairs and 
by extracting their starting position (namely the second element of the pair). 
1 print( ’suffixes ’+ ’ ’∗(len(s) − len( ’suffixes ’ )) + ’suffix_array ’) 
2 
3 for p in sorted(pairs ): 
4
print(p[0] + ’ ’∗(p[1]) ,p[1]) 
5
# a given number of space characters ( ’ ’∗(p[1])) is added 
6
# such that the elements of the suffix array are print on the 
7
# same column 
1 suffixes
suffix_array 
2 agct
17 
3 agctagctagctagtttagct 0 
4 agctagctagtttagct
4 
5 agctagtttagct
8 
6 agtttagct
12 
7 ct
19 
8 ctagctagctagtttagct
2 
9 ctagctagtttagct
6 
10 ctagtttagct
10 
11 gct
18 
12 gctagctagctagtttagct 
1 
13 gctagctagtttagct
5 
14 gctagtttagct
9 
15 gtttagct
13 
16 t
20
 
17 tagct
16 
18 tagctagctagtttagct
3 
19 tagctagtttagct
7 
20 tagtttagct
11 
21 ttagct
15 
22 
tttagct
14

8.10 Genomic Data Structures
253
Actually, we are not interested in storing the ordered set of sufﬁxes, but only in 
storing the resultant sufﬁx array. Thus, the function get_suffix_array receives 
in input a string ss and returns only the array. 
1 def get_suffix_array (s ): 
2
“““ 
3
Construct the suffix array of the string s . 
4
””” 
5
pairs = list () 
6
for i in range(len(s )): 
7
pairs .append( (s[ i :] , i ) ) 
8
sa = list () 
9
for p in sorted(pairs ): 
10
sa .append(p[1]) 
11
return sa 
12 
13 sa = get_suffix_array (s) 
The list of sorted sufﬁxes can be obtained by iterating over the sufﬁx array and 
by extracting on the ﬂy the sufﬁx corresponding to the given starting position given 
by the array. Let s asa be the sufﬁx array of a string ss, then the iith ordered sufﬁx 
starts in positions a left bracket i right bracketsa[i], and the sufﬁx corresponds tos left bracket i colon right brackets[i :]. A padding of extra space 
characters ’ ’*(sa[i]) is added to the sufﬁx for a well-print format. 
1 def print_suffix_array (s , sa ): 
2
for i in range(len(sa )): 
3
print(s[sa[ i ]:] + ’ ’∗(sa[ i ]) , sa[ i ]) 
4 
5 print_suffix_array (s , sa) 
1 agct
17 
2 agctagctagctagtttagct 0 
3 agctagctagtttagct
4 
4 agctagtttagct
8 
5 agtttagct
12 
6 ct
19 
7 ctagctagctagtttagct
2 
8 ctagctagtttagct
6 
9 ctagtttagct
10 
10 gct
18 
11 gctagctagctagtttagct 
1 
12 gctagctagtttagct
5 
13 gctagtttagct
9 
14 gtttagct
13 
15 t
20
 
16 tagct
16 
17 tagctagctagtttagct
3 
18 tagctagtttagct
7 
19 tagtttagct
11 
20 ttagct
15 
21 
tttagct
14

254
8
Laboratory in Python
LCP intervals and enhanced sufﬁx arrays (ESA) 
Indexing structures based on sufﬁx arrays are enhanced by means of a further array 
reporting the length of the Longest Common Preﬁx (LCP) between two consecutive 
sufﬁxes in the lexicographical order. The LCP value of the ﬁrst sufﬁx is set to be 0. 
The length of the common preﬁx between the sufﬁxes can be computed in a triv-
ial way as it is shown in the function longest_prefix_length. The function 
computes the length for the two sufﬁxes starting at positionii andj j of the input string 
ss. On top of this function, the get_lcp procedure is deﬁned. 
1 def longest_prefix_length (s , i , j ): 
2
“““ 
3
Calculate the length of the longest common prefix between 
4
two suffixes , the one in position i and the other in 
5
position j , of s 
6
””” 
7
l =  0  
8
while ( i+l  <  len(s )) and ( j+l  <  len(s )): 
9
if s[ i+l ] != s[ j+l ]: 
10
break 
11
l +=  1  
12
return l 
13 
14 
15 def get_lcp(s , sa ): 
16
“““ 
17
Construct the LCP array associated to the suffix array 
18
(sa)of the string s . 
19
The LCP value of the first suffix is set to be 0. 
20
””” 
21
lcp = list () 
22
lcp .append(0) 
23
for i in range(1 ,len(sa )): 
24
lcp .append( longest_prefix_length (s , sa[ i ] , sa[ i−1]) ) 
25
return lcp 
26 
27 
28 lcp = get_lcp(s , sa) 
The print of the ordered sufﬁxes together with the sufﬁx array is extended to report 
the lcp values. 
1 def print_sa_lcp(s , sa , lcp ): 
2
print( ’index’ , ’suffixes ’ + ’ ’∗(len(s)−len( ’suffixes ’ )) , ’SA’ , 
3
’LCP’ , sep=’ \ t ’) 
4
print( ’−’∗45) 
5
for i in range(len(sa )): 
6
print(i , s[sa[ i ]:] + ’ ’∗(sa[ i ]) , sa[ i ] , lcp[ i ] , sep=’ \ t ’) 
7 
8 print_sa_lcp(s , sa , lcp)

8.10 Genomic Data Structures
255
1 index
suffixes
SA LCP
2 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
3 0
agct
17
0
4 1
agctagctagctagtttagct
0
4
5 2
agctagctagtttagct
4
10
6 3
agctagtttagct
8
6
7 4
agtttagct
12
2
8 5
ct
19
0
9 6
ctagctagctagtttagct
2
2
10 7
ctagctagtttagct
6
8
11 8
ctagtttagct
10
4
12 9
gct
18
0
13 10
gctagctagctagtttagct
1
3
14 11
gctagctagtttagct
5
9
15 12
gctagtttagct
9
5
16 13
gtttagct
13
1
17 14
t
20
0
18 15
tagct
16
1
19 16
tagctagctagtttagct
3
5
20 17
tagctagtttagct
7
7
21 18
tagtttagct
11
3
22 19
ttagct
15
1
23 20
tttagct
14
2
An LCPkk-interval is deﬁned as a contiguous region of the LCP array, deﬁned by
two indexes ii and j j (with 1 less than or equals i less than or equals j less than or equals StartAbsoluteValue s EndAbsoluteValue1 ≤i ≤j ≤|s|) such that for i less than x less than or equals ji < x ≤j, upper L upper C upper P left bracket x right bracket greater than or equals kLC P[x] ≥k.
Moreover, upper L upper C upper P left bracket i minus 1 right bracket less than kLC P[i −1] < k and upper L upper C upper P left bracket j plus 1 right bracket less than kLC P[ j + 1] < k.
LCP intervals are useful in enumerating the k-mers occurring in a given string. In
fact, fork equals 1k = 1 all and only the sufﬁxes starting with the symbol ‘a’ are contiguously
listed in the ﬁrst positions of the sufﬁx array. Moreover, such sufﬁxes have a lcp value
greater than 1, except for the ﬁrst one. For the second sufﬁx, the ﬁrst position where
the lcp value is less than 1 is the one identiﬁed by the sufﬁx ‘ct‘. More in general
an lcp value of 0 occurs each time the corresponding sufﬁx has the ﬁrst character
different from the previous sufﬁx.
In the previous example, the four nucleobasesupper A comma upper C comma upper G comma upper TA, C, G, T are recognized by 4 lcp
1-intervals that are [0, 4], [5, 8], [9, 13] and [14, 20]. Within each interval, the lcp
value i is greater or equal to 1, except for the ﬁrst element that has an lcp value of 0
and that identiﬁes the end of the previous interval and the start of the current one.
An algorithm to retrieve LCP k-intervals uses two indexes, ii and j j, to identify
the bounds of the current interval. Theii index is the start, and thej j index is the end
(not included).
Thealgorithmstartswithi equals 0i = 0 andj equals 1 j = 1,increasesthevalueofj j untill c p left bracket j right bracket greater than or equals klcp[ j] ≥k
(and until j j is a valid index, namely it is less than the length of the array). When
l c p left bracket j right bracket less than klcp[ j] < k, the algorithm stops and return the intervalleft bracket i comma j right bracket[i, j]. Subsequently,ii is set to
be the current value ofj j,j j is increased by 1 and the search is repeated. The algorithm
continues until ii is less than the length of the array.
1 def print_sa_lcp_region(s ,sa ,lcp , i , j ):
2
print( ’−’∗40)
3
print( ’interval ’)

256
8
Laboratory in Python
4
for x in range(i , j ): 
5
print(x, s[sa[x]:] +’ ’∗(sa[x]) , sa[x] , lcp[x] , sep=’ \ t ’) 
6
#print (’.’∗40) 
7 
8 
9 k = 1  
10 i =  0  
11 while i <  len(s ): 
12
j = i +  1  
13
while ( j  <  len(s )) and (lcp[ j ] >= k): 
14
j +=  1  
15 
16
print_sa_lcp_region (s , sa , lcp , i , j ) 
17
print( ’k−mer: ’ , s[ sa[ i ]: sa[ i]+k] ) 
18 
19
i = j  
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 interval 
3 0
agct
17
 0
 
4 1
agctagctagctagtttagct
0
4 
5 2
agctagctagtttagct
4
10 
6 3
agctagtttagct
8
6 
7 4
agtttagct
12 
2 
8 k−mer: a 
9 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
10 interval 
11 5
ct
19
 0
 
12 6
ctagctagctagtttagct
2
2 
13 7
ctagctagtttagct
6
8 
14 8
ctagtttagct
10 
4 
15 k−mer: c 
16 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
17 interval 
18 9
gct
18
 0
 
19 10 
gctagctagctagtttagct
1
3 
20 11 
gctagctagtttagct
5
9 
21 12 
gctagtttagct
9
5 
22 13 
gtttagct
13 
1 
23 k−mer: g 
24 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
25 interval 
26 14
t
20 
0 
27 15 
tagct
16 
1 
28 16 
tagctagctagtttagct
3
5 
29 17 
tagctagtttagct
7
7 
30 18 
tagtttagct
11 
3 
31 19 
ttagct
15 
1 
32 20
tttagct
14 2 
33 k−mer: t 
The previous algorithm results to be not suitable for values ofkk greater than 1, in 
fact when it is applied to extract 2-mers unwanted intervals, thus unwanted words,

8.10 Genomic Data Structures
257
are retrieved. In particular, the 1-mer tt is produced, which is due to the fact that the 
algorithm extracts preﬁxes that are shorter than the given value of kk. 
1 k = 2  
2 i =  0  
3 while i <  len(s ): 
4
j = i +  1  
5
while ( j  <  len(s )) and (lcp[ j ] >= k): 
6
j +=  1  
7 
8
print_sa_lcp_region (s , sa , lcp , i , j ) 
9
print( ’k−mer: ’ , s[ sa[ i ]: sa[ i]+k] ) 
10 
11
i = j  
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 interval 
3 0
agct
17
 0
 
4 1
agctagctagctagtttagct
0
4 
5 2
agctagctagtttagct
4
10 
6 3
agctagtttagct
8
6 
7 4
agtttagct
12 
2 
8 k−mer: ag 
9 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
10 interval 
11 5
ct
19
 0
 
12 6
ctagctagctagtttagct
2
2 
13 7
ctagctagtttagct
6
8 
14 8
ctagtttagct
10 
4 
15 k−mer: ct 
16 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
17 interval 
18 9
gct
18
 0
 
19 10 
gctagctagctagtttagct
1
3 
20 11 
gctagctagtttagct
5
9 
21 12 
gctagtttagct
9
5 
22 k−mer: gc 
23 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
24 interval 
25 13 
gtttagct
13 
1 
26 k−mer: gt 
27 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
28 interval 
29 14
t
20 
0 
30 k−mer: t 
31 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
32 interval 
33 15 
tagct
16 
1 
34 16 
tagctagctagtttagct
3
5 
35 17 
tagctagtttagct
7
7 
36 18 
tagtttagct
11 
3 
37 k−mer: ta

258
8
Laboratory in Python
38 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
39 interval 
40 19 
ttagct
15 
1 
41 20
tttagct
14 2 
42 k−mer: t t 
In order to avoid the extraction of such extra intervals, the algorithm has to take 
into account that right intervals are not contiguous but are interleaved with intervals 
that must be discarded. The condition for discarding such intervals is given by the 
check of their proximity with the end of the string. 
1 k = 2  
2 i =  0  
3 while i <  len(s ): 
4
while ( i  <  len(s )) and 
(sa[ i ] > len(s) − k − 1): # check 
5
i +=  1  
6
if i ==  len(s ): # there are no more valid intervals 
7
break 
8 
9
j = i +  1  
10
while ( j  <  len(s )) and (lcp[ j ] >= k): 
11
j +=  1  
12 
13
print_sa_lcp_region (s , sa , lcp , i , j ) 
14
print(‘k−mer: ’ , s[ sa[ i ]: sa[ i]+k] ) 
15 
16
i = j  
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 interval 
3 0
agct
17
 0
 
4 1
agctagctagctagtttagct
0
4 
5 2
agctagctagtttagct
4
10 
6 3
agctagtttagct
8
6 
7 4
agtttagct
12 
2 
8 k−mer: ag 
9 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
10 interval 
11 6
ctagctagctagtttagct
2
2 
12 7
ctagctagtttagct
6
8 
13 8
ctagtttagct
10 
4 
14 k−mer: ct 
15 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
16 interval 
17 9
gct
18
 0
 
18 10 
gctagctagctagtttagct
1
3 
19 11 
gctagctagtttagct
5
9 
20 12 
gctagtttagct
9
5 
21 k−mer: gc 
22 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
23 interval 
24 13 
gtttagct
13 
1 
25 k−mer: gt

8.10 Genomic Data Structures
259
26 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
27 interval 
28 15 
tagct
16 
1 
29 16 
tagctagctagtttagct
3
5 
30 17 
tagctagtttagct
7
7 
31 18 
tagtttagct
11 
3 
32 k−mer: ta 
33 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
34 interval 
35 19 
ttagct
15 
1 
36 20
tttagct
14 2 
37 k−mer: t t 
Some notes on the enhanced sufﬁx array 
1. Due to the lexicographical construction of the sufﬁx array, the enumeration of 
k-mer via lcp k-intervals implicitly follows the lexicographical order of enumer-
ating the k-mers. 
2. The enhanced sufﬁx array implicitly contains the complete set upper D left parenthesis s right parenthesisD(s), which 
includes every kk-mer occurring in ss for 1 less than or equals k less than or equals StartAbsoluteValue s EndAbsoluteValue1 ≤k ≤|s|. 
Informational genomics via ESA (and NESA) 
Usually, sequenced genomes present an extra symbol, coded with an upper NN, which rep-
resents an ambiguity in determining a speciﬁc nucleotide at a given position of the 
genome. The execution of the previous approach on such sequences produces k-mers 
that may contain upper NN symbols. However, it is desirable to skip such k-mers from the 
enumeration. 
The issue is solved with an approach similar to the one used to discard k-mers 
shorter than the desired word lengthkk. A modiﬁed algorithm discards sufﬁxes which 
have a symbol upper NN within the initial kk characters. 
In order to increase the efﬁciency in time complexity, a further array is computed. 
The array, called upper N upper SN S, takes a trace of the distance from the starting position of the 
sufﬁx to the closest upper NN character on the right of such position. 
1 def distance_to_n(s , i ): 
2
j = i  
3
while (j<len(s )) and (s[ j ] != ’N’ ): 
4
j +=  1  
5
return j − i 
6 
7 def get_ns_array(s , sa ): 
8
return [
distance_to_n(s , sa[ i ]) for i in range(len(s )) ] 
9

260
8
Laboratory in Python
10 
11 def print_sa_lcp_ns(s , sa , lcp , ns ): 
12
print( ’index’ , ’suffixes ’ + ’ ’∗(len(s)−len( ’suffixes ’ )) , ’SA’ , 
13
’LCP’ , ’NS’ , sep=’ \ t ’) 
14
print( ’−’∗60) 
15
for i in range(len(sa )): 
16
print(i , s[sa[ i ]:] + ’ ’∗(sa[ i ]) , sa[ i ] , lcp[ i ] , ns[ i ] , 
17
sep=’ \ t ’) 
18 
19 
20 
21 s = ’agctagNctagctagNtttagctN’ 
22 
23 
24 sa = get_suffix_array (s) 
25 lcp = get_lcp(s , sa) 
26 ns = get_ns_array(s , sa) 
27 
28 
29 print_sa_lcp_ns(s , sa , lcp , ns) 
30 print( ’−’∗60) 
1 index
suffixes
SA LCP NS 
2 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
3 0
N
23
 0
0
 
4 1
NctagctagNtttagctN
6
1
0 
5 2
NtttagctN
15 
1
0 
6 3
agNctagctagNtttagctN
4
0
2 
7 4
agNtttagctN
13 
3
2 
8 5
agctN
19
 2
4
 
9 6
agctagNctagctagNtttagctN
0
4
6 
10 7
agctagNtttagctN
9
7
6 
11 8
ctN
21
 0
2
 
12 9
ctagNctagctagNtttagctN
2
2
4 
13 10 
ctagNtttagctN
11 
5
4 
14 11 
ctagctagNtttagctN
7
4
8 
15 12 
gNctagctagNtttagctN
5
0
1 
16 13 
gNtttagctN
14 
2
1 
17 14 
gctN
20 
1
3 
18 15 
gctagNctagctagNtttagctN
1
3
5 
19 16 
gctagNtttagctN
10 
6
5 
20 17 
tN
22 
0
1 
21 18 
tagNctagctagNtttagctN
3
1
3 
22 19 
tagNtttagctN
12 
4
3 
23 20 
tagctN
18 
3
5 
24 21 
tagctagNtttagctN
8
5
7 
25 22 
ttagctN
17 
1
6 
26 23 
tttagctN
16 
2
7 
27 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
Two further conditions are included on enumerating k-mers.

8.10 Genomic Data Structures
261
The ﬁrst one is due to a modiﬁcation of the deﬁnition of the lcp k-interval such 
that it is extended by the condition upper L upper C upper P left bracket x right bracket greater than or equals kLC P[x] ≥k. This ensures that interval do not 
contains k-mers having a upper NN character in the initial kk positions. 
The second condition is applied to discard unwanted intervals that interleave valid 
ones. 
1 def print_sa_lcp_ns_region(s , sa , lcp , ns , i , j ): 
2
print( ’−’∗60) 
3
print( ’ interval ’) 
4
for x in range(i , j ): 
5
print(x, s[sa[x]:] +’ ’∗(sa[x]) , sa[x] , lcp[x] , ns[x] , 
6
sep=’ \ t ’) 
7 
8 
9 k = 3  
10 i =  0  
11 while i <  len(s ): 
12
while
( i  <  len(s )) and 
( (sa[  i  ]  >  len(s) − k − 1) 
13
or (ns[ i ] < k) ): # second further condition 
14
i +=  1  
15
if i ==  len(s ): 
16
break 
17 
18
j = i+1  
19
while ( j  <  len(s )) and (lcp[ j ] >= k) 
20
and (ns[ i ] >= k): # f i r s t further condition 
21
j +=  1  
22 
23
print_sa_lcp_ns_region(s , sa , lcp , ns , i , j ) 
24
print( ’k−mer: ’ , s[ sa[ i ]: sa[ i]+k] ) 
25
i = j  
1 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
2 interval 
3 5
agctN
19
 2
4
 
4 6
agctagNctagctagNtttagctN
0
4
6 
5 7
agctagNtttagctN
9
7
6 
6 k−mer: agc 
7 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
8 interval 
9 9
ctagNctagctagNtttagctN
2
2
4 
10 10 
ctagNtttagctN
11 
5
4 
11 11 
ctagctagNtttagctN
7
4
8 
12 k−mer: cta 
13 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
14 interval 
15 14 
gctN
20 
1
3 
16 15 
gctagNctagctagNtttagctN
1
3
5 
17 16 
gctagNtttagctN
10 
6
5 
18 k−mer: gct 
19 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
20 interval 
21 18 
tagNctagctagNtttagctN
3
1
3

262
8
Laboratory in Python
22 19 
tagNtttagctN
12 
4
3 
23 20 
tagctN
18 
3
5 
24 21 
tagctagNtttagctN
8
5
7 
25 k−mer: tag 
26 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
27 interval 
28 22 
ttagctN
17 
1
6 
29 k−mer: tta 
30 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
31 interval 
32 23 
tttagctN
16 
2
7 
33 k−mer: ttt 
The following code provides a faster implementation of the construction of the N 
array. It constructs an inverse sufﬁx array, such that for each position iiin the string, 
the corresponding position of theii-th sufﬁx in the sufﬁx array is obtained in constant 
time. Then, starting from the end of the string, it keeps track of the last position, on 
the right, where the symbol N has been found, thus it assigns the distance between 
the current position and the last right occurrence of the N. 
1 def fast_get_ns_array (s , sa ): 
2
inv_sa = [0 for _ in range(len(sa ))] 
3
for i in range(len(sa )): 
4
inv_sa[ sa[ i ] ] = i 
5 
6
ns = [0 for _ in range(len(sa ))] 
7
lastn = len(s) 
8
for i in range(len(s)−1,−1,−1): 
9
if s[ i ] == ’N’ : 
10
lastn = i 
11
ns[ inv_sa[ i ] ] = lastn − i 
12
return ns 
13 
14 
15 fns = fast_get_ns_array (s , sa) 
16 print(len(fns ) , fns) 
17 
18 ns = get_ns_array(s , sa) 
19 print(len(ns) , ns) 
20 
21 assert ns == fns 
1 24  [0  ,  0, 0, 2, 2, 4, 6, 6, 2, 4, 4, 8, 1, 1, 3, 5, 5, 1, 3, 3, 5,  
2 7, 6, 7] 
3 24  [0  ,  0, 0, 2, 2, 4, 6, 6, 2, 4, 4, 8, 1, 1, 3, 5, 5, 1, 3, 3, 5,  
4 7, 6, 7]

8.10 Genomic Data Structures
263
Implementing a k-mer iterator 
According to the method shown before for the enumeration of k-mers based on 
enhanced sufﬁx arrays, a k-mer iterator has four internal ﬁxed parameters that are 
the indexed string, the sufﬁx array, the LCP array and the word lengthkk. The variables 
of the internal state are the start and the end of the current LPC k-interval, identiﬁed 
by the private variables ii and j j respectively. Each time the next method is called 
on the iterator, the search for the successive interval starts. If two valid limits for 
the next interval are found, the iterator pauses the iteration and returns the k-mer 
corresponding to the found interval. If no more intervals are found, the iterator rises 
a StopIteration exception. 
In addition, the iterator informs about the multiplicity of the current k-mer and 
the starting position of its occurrences along the string. The information is provided 
b the two methods multiplicity and positions, respectively. We recall, that 
the multiplicity is given by the size of the k-interval on the LCP array, and the set 
of occurring positions is the corresponding slice on the sufﬁx array. Since the sufﬁx 
array only ensures the lexicographical order of the sufﬁx, the positions are not sorted. 
Its deﬁnition follows the directives that are given in Sect. 7.11. 
1 class ESAIterator : 
2
""" 
3
An interator for extracting all the k−mers of a genomic 
4
sequence by means of the NESA data structure . 
5
""" 
6
__s = None 
7
__k = 0 
8
__sa = None 
9
__lcp = None 
10
__i = 0 
11
__j = 0 
12 
13
def __init__( self , s , k, sa = None, lcp = None): 
14
""" 
15
If the suffix array (sa) and LCP array are not provided , 
16
then they will be computed by the constructor . 
17
""" 
18
self .__s = s 
19
self .__k = k 
20 
21
if sa == None: 
22
self . build_sa () 
23
else : 
24
self .__sa = sa 
25 
26
if lcp == None: 
27
self . build_lcp () 
28
else : 
29
self . __lcp = lcp 
30 
31
def build_sa( self ):

264
8
Laboratory in Python
32
print("building suffix array . . . ") 
33
suffixes = list () 
34
for i in range(len( self .__s )): 
35
suffixes .append( ( self .__s[ i :] + self .__s[: i ] , i ) ) 
36
self .__sa = list () 
37
for suff in sorted( suffixes ): 
38
self .__sa.append( suff [1]) 
39
print( ’done’) 
40
def longest_prefix_length (s , i , j ): 
41
l =  0  
42
while ( i+l  <  len(s )) and ( j+l  <  len(s )): 
43
if s[ i+l ] != s[ j+l ]: 
44
break 
45
l +=  1  
46
return l 
47 
48
def build_lcp( self ): 
49
print( ’building lcp array . . . ’) 
50
self . __lcp = list () 
51
self . __lcp .append(0) 
52
for i in range(1 ,len( self .__sa )): 
53
self . __lcp .append( 
54
ESAIterator . longest_prefix_length ( self .__s, 
55
self .__sa[ i ] , self .__sa[ i−1]) ) 
56
print( ’done’) 
57 
58
def get_sa( self ): 
59
return self .__sa 
60 
61
def get_lcp( self ): 
62
return self . __lcp 
63 
64
def __iter__( self ): 
65
return self 
66 
67
def __next__( self ): 
68
if self . __i < len( self .__s): 
69
self . __i = self . __j 
70 
71
while ( self . __i < len( self .__s)) and 
72
( self .__sa[ self . __i] > 
73
len( self .__s) − self .__k − 1): 
74
self . __i += 1 
75
if self . __i == len( self .__s): 
76
raise StopIteration 
77
self . __j = self . __i+1 
78
while ( self . __j < len( self .__s) ) and 
79
( self . __lcp[ self . __j] >= self .__k): 
80
self . __j += 1 
81
ret = self .__s[ self .__sa[ self . __i] : 
82
self .__sa[ self . __i] + self .__k ] 
83
return ret 
84
else :

8.10 Genomic Data Structures
265
85
raise StopIteration 
86 
87
def multiplicity ( self ): 
88
return self . __j − self . __i 
89 
90
def positions ( self ): 
91
return self .__sa[ self . __i : self . __j] 
92 
93 
94 i t = ESAIterator( ’agctagctagctagtttagct ’ , 3, sa=None, lcp=None) 
95 
96 for kmer in it : 
97
print(kmer, i t . multiplicity () , i t . positions ()) 
1 building suffix array . . . 
2 done 
3 building lcp array . . . 
4 done 
5 agc 4 [17, 0, 4, 8] 
6 agt 1 [12] 
7 cta 3 [2 , 6, 10] 
8 gct 3 [1 , 5, 9] 
9 gtt 1 [13] 
10 tag 1 [16] 
11 tag 3 [3 , 7, 11] 
12 
tta 1 [15] 
13 
ttt 1 [14] 
In addition, the iterator builds by it self the sufﬁx and the LCP arrays if they are 
not proved by the user. However, the construction of the arrays is an expensive step, 
thus, this is not a good practice if multiple iterators have to be launched on the same 
strings, even with different values of kk. It is preferable to build the necessary data 
structure once and then, successively, to instantiate as iterators as needed on top of 
the same data structures. 
1 s = ’agctagctagctagtttagct ’ 
2 
3 k = 2  
4 i t = ESAIterator(s , k, sa=None, lcp=None) 
5 
6 sa = i t . get_sa () 
7 lcp = i t . get_lcp () 
8 
9 print( str (k)+"−mers") 
10 for kmer in it : 
11
print(kmer, i t . multiplicity () , i t . positions ()) 
12 
13 
14 k = 3  
15 i t = ESAIterator(s , k, sa , lcp) 
16 
17 print( str (k)+"−mers") 
18 for kmer in it :

266
8
Laboratory in Python
19
print(kmer, i t . multiplicity () , i t . positions ()) 
1 building suffix array . . . 
2 done 
3 building lcp array . . . 
4 done 
5 2−mers 
6 ag 5 [17, 0, 4, 8, 12] 
7 ct 3 [2 , 6, 10] 
8 gc 4 [18, 1, 5, 9] 
9 gt 1 [13] 
10 ta 1 [16] 
11 ta 3 [3 , 7, 11] 
12 
t t 2 [15, 14] 
13 3−mers 
14 agc 4 [17, 0, 4, 8] 
15 agt 1 [12] 
16 cta 3 [2 , 6, 10] 
17 gct 3 [1 , 5, 9] 
18 gtt 1 [13] 
19 tag 1 [16] 
20 tag 3 [3 , 7, 11] 
21 
tta 1 [15] 
22 
ttt 1 [14] 
There are many more efﬁcient ways to build the sufﬁx array and the LCP array. 
One of them is provided by the module pysuffixarray, which can be installed 
by the command pip install pysuffixarray. An example of how to build 
the data structures by means of this library is given in what follows. 
1 from pysuffixarray . core import SuffixArray 
2 sa_obj = SuffixArray(s) 
3 sa_sa = sa_obj . suffix_array () 
4 lcp_sa = sa_obj .longest_common_prefix() 
5 
6 print_sa_lcp(s , sa_sa , lcp_sa) 
1 index
suffixes
SA LCP 
2 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 
3 0
21
 
0
 
4 1
agct
17
 0
 
5 2
agctagctagctagtttagct
0
4 
6 3
agctagctagtttagct
4
10 
7 4
agctagtttagct
8
6 
8 5
agtttagct
12 
2 
9 6
ct
19
 0
 
10 7
ctagctagctagtttagct
2
2 
11 8
ctagctagtttagct
6
8 
12 9
ctagtttagct
10 
4 
13 10 
gct
18 
0 
14 11 
gctagctagctagtttagct
1
3 
15 12 
gctagctagtttagct
5
9 
16 13 
gctagtttagct
9
5

8.10 Genomic Data Structures
267
17 14 
gtttagct
13 
1 
18 15
t
20 
0 
19 16 
tagct
16 
1 
20 17 
tagctagctagtttagct
3
5 
21 18 
tagctagtttagct
7
7 
22 19 
tagtttagct
11 
3 
23 20 
ttagct
15 
1 
24 21
tttagct
14 2 
Because of technical details in the construction of the data structures by means of 
the library, the ﬁrst position of the sufﬁx array and the LCP array must be discarded 
if they are used in a k-mer iterator deﬁned in the previous sections. 
1 from pysuffixarray . core import SuffixArray 
2 sa_obj = SuffixArray(s) 
3 sa_sa = sa_obj . suffix_array ()[1:] 
4 lcp_sa = sa_obj .longest_common_prefix()[1:] 
5 
6 k = 3  
7 i t = ESAIterator(s , k, sa_sa , lcp_sa) 
8 
9 print( str (k)+“−mers”) 
10 for kmer in it : 
11
print(kmer, i t . multiplicity () , i t . positions ()) 
1 3−mers 
2 agc 4 [17, 0, 4, 8] 
3 agt 1 [12] 
4 cta 3 [2 , 6, 10] 
5 gct 3 [1 , 5, 9] 
6 gtt 1 [13] 
7 tag 4 [16, 3, 7, 11] 
8 
tta 1 [15] 
9 
ttt 1 [14] 
Similarly to the iterator for the ESA data structure, an iterator based on the NESA 
data structure is implemented in the following source code. Differently from the 
previous version, this new iterator also implements the reset method that allows 
the iterator to reset to the initial state. 
1 class NESAIterator: 
2
__s = None 
3
__k = 0 
4
__sa = None 
5
__lcp = None 
6
__ns = None 
7
__i = 0 
8
__j = 0 
9 
10
def __init__( self , s , k, sa = None, lcp = None, ns = None): 
11
self .__s = s 
12
self .__k = k 
13 
14
if sa == None:

268
8
Laboratory in Python
15
self . build_sa () 
16
else : 
17
self .__sa = sa 
18 
19
if lcp == None: 
20
self . build_lcp () 
21
else : 
22
self . __lcp = lcp 
23 
24
if ns == None: 
25
self . build_ns () 
26
else : 
27
self .__ns = ns 
28 
29
def get_k( self ): 
30
return self .__k 
31 
32
def reset ( self ):#<−−−−−− 
33
self . __i = 0 
34
self . __j = 0 
35 
36
def build_sa( self ): 
37
print("building suffix array . . . ") 
38
suffixes = list () 
39
for i in range(len( self .__s )): 
40
suffixes .append( ( self .__s[ i :] + self .__s[: i ] , i ) ) 
41
self .__sa = list () 
42
for suff in sorted( suffixes ): 
43
self .__sa.append( suff [1]) 
44
print( ’done’) 
45 
46 
47
def longest_prefix_length (s , i , j ): 
48
l =  0  
49
while ( i+l  <  len(s )) and ( j+l  <  len(s )): 
50
if s[ i+l ] != s[ j+l ]: 
51
break 
52
l +=  1  
53
return l 
54 
55
def build_lcp( self ): 
56
print( ’building lcp array . . . ’) 
57
self . __lcp = list () 
58
self . __lcp .append(0) 
59
for i in range(1 ,len( self .__sa )): 
60
self . __lcp .append( 
61
NESAIterator. longest_prefix_length ( 
62
self .__s, self .__sa[ i ] , self .__sa[ i−1]) ) 
63
print( ’done’) 
64 
65
def build_ns( self ): 
66
print( ’building ns array . . . ’) 
67
inv_sa = [0 for _ in range(len( self .__sa))]

8.10 Genomic Data Structures
269
68
for i in range(len( self .__sa )): 
69
inv_sa[
self .__sa[ i ] ] = i 
70 
71
self .__ns = [0 for _ in range(len( self .__sa))] 
72
lastn = len( self .__s) 
73
for i in range(len( self .__s)−1,−1,−1): 
74
if self .__s[ i ] == ’N’ : 
75
lastn = i 
76
self .__ns[ inv_sa[ i ] ] = lastn − i 
77
print( ’done’) 
78 
79
def get_sa( self ): 
80
return self .__sa 
81 
82
def get_lcp( self ): 
83
return self . __lcp 
84 
85
def get_ns( self ): 
86
return self .__ns 
87 
88 
89
def __iter__( self ): 
90
return self 
91
def __next__( self ): 
92
if self . __i < len( self .__s): 
93
self . __i = self . __j 
94 
95
while ( self . __i < len( self .__s)) 
96
and 
( ( self .__sa[ self . __i] > 
97
len( self .__s) − self .__k − 1) 
98
or ( self .__ns[ self . __i] < self .__k) ): 
99
self . __i += 1 
100
if self . __i == len( self .__s): 
101
raise StopIteration 
102
self . __j = self . __i+1 
103
while ( self . __j < len( self .__s) ) 
104
and ( self . __lcp[ self . __j] >= self .__k) 
105
and ( self .__ns[ self . __i] >= self .__k) : 
106
self . __j += 1 
107
ret = self .__s[ self .__sa[ self . __i] : 
108
self .__sa[ self . __i] + self .__k ] 
109
#self . __i = self . __j #!!!!!! 
110
return ret 
111
else : 
112
raise StopIteration 
113 
114
def multiplicity ( self ): 
115
return self . __j − self . __i 
116 
117
def positions ( self ): 
118
return self .__sa[ self . __i : self . __j] 
119 
120

270
8
Laboratory in Python
121 i t = NESAIterator( ’agctagctagNctagtttagctN’ , 3) 
122 
123 print( ’ iterating ’+str ( i t . get_k())+ ’−mers  . . .  ’)  
124 for kmer in it : 
125
print(kmer, i t . multiplicity () , i t . positions ()) 
1 building suffix array . . . 
2 done 
3 building lcp array . . . 
4 done 
5 building ns array . . . 
6 done 
7 iterating 3−mers  . . .  
8 agc 3 [18, 4, 0] 
9 agt 1 [13] 
10 cta 3 [6 , 2, 11] 
11 gct 3 [19, 5, 1] 
12 gtt 1 [14] 
13 tag 4 [7 , 17, 3, 12] 
14 
tta 1 [16] 
15 
ttt 1 [15] 
1 print( ‘ reiterating ’+str ( i t . get_k())+‘−mers  . . .  ’)  
2 i t . reset () 
3 kmer = next( i  t )  
4 print(“ first : ” ,kmer, i t . multiplicity () , i t . positions ()) 
5 
6 for kmer in it : 
7
print(kmer, i t . multiplicity () , i t . positions ()) 
1 reiterating 3−mers  . . .  
2 
first :
agc 3 [18, 4, 0] 
3 agt 1 [13] 
4 cta 3 [6 , 2, 11] 
5 gct 3 [19, 5, 1] 
6 gtt 1 [14] 
7 tag 4 [7 , 17, 3, 12] 
8 
tta 1 [16] 
9 
ttt 1 [15] 
8.11 
Recurrence Patterns 
Given a word  ww, the recurrence distance distribution (RDD) informs about the dis-
tances at whichww recurs in a reference stringss. A recurrence distance is the distance 
between two consecutive occurrences ofww. The RDD distribution reports how many 
times two consecutive occurrences recur at a given distance. 
1 def RDD(s ,w): 
2
“““

8.11 Recurrence Patterns
271
3
Extract the recurrence distance ditribution (RDD) 
4
of the word w in s. 
5
Given the starting positions of two occurences of w, 
6
p1 and p2, the reucrrence distance is calculated as 
7
p1 − p2 such that consecutive occurrences are 
8
at distance 1. 
9
−−−− 
10
Parameters: 
11
s (  str ) : the reference string 
12
w (  str ) : the searched substring 
13
−−−− 
14
Returns: 
15
dict [int , int ] : a dictionary mapping recurrence 
16
distances to the number of times they occur 
17
””” 
18
pos = sorted( get_positions(s ,w)) 
19
rdd = dict () 
20
for i in range(2 ,len(pos )): 
21
l = pos[ i ] − pos[ i−1] 
22
rdd[ l ] = rdd . get( l ,0) + 1 
23
return rdd 
24 
25 print(s+‘\n’) 
26 print ( ‘RDD(a ): ’ , RDD(s , ‘a’ )) 
27 print ( ‘RDD( t ): ’ ,RDD(s , ‘ t ’ )) 
28 print ( ‘RDD(c ): ’ ,RDD(s , ‘c’ )) 
29 print ( ‘RDD(g): ’ ,RDD(s , ‘g’ )) 
1 agctaggaggatcgccagat 
2 
3 RDD(a ): {2: 1, 3: 2, 6: 1} 
4 RDD( t ): {8: 1} 
5 RDD(c ): {1: 1, 2: 1} 
6 RDD(g): {1: 2, 2: 1, 4: 2} 
In real sequences, RDD distributions are usually deﬁned on several values of 
recurrence distance. Thus, simple prints result in too much verbose output. The 
module matlibplot provide several functionalities for plotting charts in python. 
In this case, a bar plot is used to visualize the content of an RDD distribution. 
If a recurrence distance between the minimum and the maximum retrieved values 
is missing, it is preferable to add such information by adding to the RDD a value 
equal to zero for the corresponding distances. In this way, the plot is more readable 
since the values on the x-axis are contiguous and no hidden gaps are produced. 
In what follows, the RDD of the word upper G upper CGC is computed along the genome of the 
M. genitalium species. 
1 w =  ’GC’  
2 rdd = RDD(mg37,w) 
3 
4 def plot_RDD(rdd , title ): 
5
""" 
6
Plot an RDD distribution adding missing recurring

272
8
Laboratory in Python
7
distances , between 1 and the original maximum distance , 
8
by adding a value of zero in correspondence of them. 
9
""" 
10
# se a value equal to zero for the missing recurrence distances 
11
for d in range(0 ,max(rdd .keys ())): 
12
rdd[d] = rdd . get(d,0) + 0 
13 
14
# module can be imported by uring aliases to refer them 
15
import matplotlib . pyplot as plt 
16 
17
# set the figure size 
18
plt .rcParams[ ’figure . figsize ’] = [20, 6] 
19
# assign height of bars 
20
bar_values = [v for k,v in sorted(rdd . items ())] 
21
# plot with specific values on the x−axis that are 
22
# associated to the height 
23
plt . bar(sorted(rdd .keys()) , bar_values , width=1.0) 
24
# set the label on the y−axis 
25
plt . ylabel ( ’Number of pairs ’) 
26
# set the label on the x−axis 
27
plt . xlabel ( ’Recurrence distance ’) 
28
# set a title for the chart 
29
plt . title ( title ) 
30
# plot the chart 
31
plt .show() 
32 
33 plot_RDD(rdd , ’Recurrence distance distribution of ’+ w) 
Average recurrence distance distribution 
The average recurrence distribution (aRDD) reports an average value of recurrence 
regarding the RDDs of several words belonging to a dictionary. Usually, it is com-
puted for the complete set of kk-mers occurring in a string. 
1 def aRDD(s ,k): 
2
“““ 
3
Computer the average recurrence distance distribution of 
4
the complete set of k−mers occuring in s. 
5
−−−− 
6
Parameters:

8.11 Recurrence Patterns
273
7
s (  str ) : the string from which extract the RDDs 
8
k (  int ) : the word length of the k−mers for which 
9
extract the RDD 
10
−−−− 
11
Returns: 
12
dict [int , float ] : a dictionary mapping recurrence 
13
distances to the average number of 
14
times they occur 
15
””” 
16
ardd = dict () 
17
kmers = get_kmers(s ,k) 
18
for kmer in kmers: 
19
rdd = RDD(s ,kmer) 
20
for distance , value in rdd . items (): 
21
ardd[ distance ] = ardd . get(distance ,0) + value 
22
for d,v in ardd . items (): 
23
ardd[d] = ardd[d] / len(kmers) 
24
return ardd 
The following code shows the aRDD between the recurrence distances 1 and 60. 
1 nardd = dict () 
2 for d,v in ardd . items (): 
3
if d <= 
60: 
4
nardd[d] = v 
5 ardd = nardd 
6 
7 plot_RDD(ardd , ‘Average recurrence distance distribution for k ’+  
8
str (k)) 
Reading GFF3 ﬁles 
The GFF3 (general feature format, version 3) ﬁle format is sued to specify a list of 
annotations along a genome. An annotation is a contiguous region of the genomic 
sequences which show some particular properties. Common annotations are the 
genes. Each gene is identiﬁed by the starting and the ending coordinate, the strand 
and some supplementary non-physical descriptions.

274
8
Laboratory in Python
GFF3 ﬁles are comma-separated values (CSV) ﬁles, which means that they are 
textual ﬁles in which a sort of column content is represented. Columns are sepa-
rated with a speciﬁc character, usually a comma o a tabulation character. Additional 
comment lines are identiﬁed with an initial # character. 
GFF3 have at least 6 tab-separated columns that are: 
• 0 : annotation identiﬁer 
• 1 : source (database, organization, etc...) 
• 2 : feature type (region, gene, exon, tRNA, etc...) 
• 3 : start coordinate 
• 4 : end coordinate 
• 5 : score (gives an indication regarding the conﬁdence of the sources) 
• 6 : strand (+ or –) 
• 7 : phase (phase o- CDS features) 
• 8 : additional attributes 
Independently from the strand, coordinates are given as absolute values from the 
ﬁrst nucleotide on the (+) 5’–3’ strand. 
The following code reads the GFF3 annotation ﬁle of the mycoplasma genitalium 
genome in order to extract the parts of it that are covered by gene annotations. Since 
genes may reside on both strands and they also may overlap, thus the sequence 
coverage is intended as the number of positions in the 5’–3’ strand that are covered 
by at least one gene independently of the strand. 
We also recall that GFF3 ﬁle usually only reports annotation coordinates, thus the 
actual sequence must be retrieved from a corresponding FASTA ﬁle. 
1 i f i l e = ’mycoplasma_genitalium_G37. fna ’ 
2 mg37 = ” 
3 for line in open( ifile , ’r ’ ): 
4
if line . strip ()[0] != ’>’ : 
5
mg37 += line . strip () 
6 
7 coverage = [0 for i in range(len(mg37))] 
8 
9 i f i l e = ’mycoplasma_genitalium_G37. gff3 ’ 
10 for line in open( ifile , ’r ’ ): 
11
if line [0] != "#" : 
12
cc = line . split ( ’ \ t ’) 
13
if len(cc) >= 6: 
14
if (cc[2] == ’gene’ ):# and (cc[6] == ’+’): # we 
15 calculate the coverage of both strands as a single strand 
16
start = int (cc[3]) 
17
end = int (cc[4]) 
18
for i in range( start  ,  end):  
19
coverage[ i ] += 1 
20 
21 print( ’sequence coverage’ , 
22
(len(coverage) − coverage . count(0)) / len(coverage)) 
23 
24 # sequence of non−coding portion of the genome

8.11 Recurrence Patterns
275
25 ncseq = ”. join  (  
26
[ mg37[ i ] for i in range(len(mg37)) if coverage[ i ] == 0 ]) 
27 
28 # sequence of coding portion of the genome 
29 cseq = ”. join  (  
30
[ mg37[ i ] for i in range(len(mg37)) if coverage[ i ] > 0 ]) 
31 
32 print( ’ total length ’ , len(mg37) , ’ , non−coding length ’ ,len(ncseq) , 
33
’ , protein−coding length ’ , len(cseq)) 
1 sequence coverage 0.9502547941993807 
2 total length 580076, non−coding length 28856, protein−coding 
3 length 551220 
The sequence coverage indicates that 95% of the mycolpasma genitalium is cov-
ered by coding genes. This has crucial implications for the composition of the 
genome. In fact, it means that it is almost composed of nucleotides that code for 
codons. Of course, the k-mer content is expected to be slightly different from the 
non-coding portions. 
The following is the multiplicity distribution of all the theoretical 2-mers com-
puted on the non-coding part. 
1 def list_words_2( prefix , k, nuc_alphabet , words): 
2
if len( prefix ) == k: 
3
words.add( prefix ) 
4
else : 
5
for a in nuc_alphabet: 
6
list_words_2( prefix + a , k, nuc_alphabet , words) 
7 
8 k = 2  
9 kmers = set () 
10 list_words_2(“ ,k,[  ‘A’ ,  ‘T’ ,  ‘C’ ,  ‘G’  ]  ,  kmers)  
11 kmers = sorted(kmers) 
12 
13 ncseq_wmd = dict () 
14 for kmer in kmers: 
15
ncseq_wmd[kmer] = count_occurrences(ncseq , kmer) 
16 
17 import matplotlib . pyplot as plt 
18 bar_values = [v for k,v in sorted(ncseq_wmd. items ())] 
19 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
20 plt . bar(kmers, bar_values) 
21 plt . xticks (kmers, kmers) 
22 plt . ylabel ( ‘ multiplicity ’) 
23 plt . xlabel ( ‘words’) 
24 plt . title (‘Word multiplicity distribution ’) 
25 plt .show()

276
8
Laboratory in Python
The following is the multiplicity distribution of all the theoretical 2-mers com-
puted on the coding part. 
1 cseq_wmd = dict () 
2 for kmer in kmers: 
3
cseq_wmd[kmer] = count_occurrences(cseq ,kmer) 
4 
5 import matplotlib . pyplot as plt 
6 bar_values = [v for k,v in sorted(cseq_wmd. items ())] 
7 plt .rcParams[ ‘ figure . figsize ’] = [20, 6] 
8 plt . bar(kmers, bar_values) 
9 plt . xticks (kmers, kmers) 
10 plt . ylabel ( ‘ multiplicity ’) 
11 plt . xlabel ( ‘words’) 
12 plt . title (‘Word multiplicity distribution ’) 
13 plt .show() 
There are differences between the two charts, however, there is not a strong signal 
that indicates the presence of the coding language. On the contrary, there is strong 
evidence of the codon language via the analysis of the RDD composition. In the next 
sections, a crucial aspect of RDDs extracted from coding regions is distinctive of 
such parts w.r.t. to non-coding regions. 
Differences between coding and non-coding parts 
The following chart is the average RDD calculated for the overall mycoplasma geni-
talium genome, as has been shown before. The distribution seems to follow an expo-
nential distribution, as expected, but some distinctive peaks on recurrence distance 
multiple of 3 appear. Such peaks seem to follow a parallel exponential distribution.

8.11 Recurrence Patterns
277
1 k = 2  
2 ardd = aRDD(mg37,k) 
3 
4 nardd = dict () 
5 for d,v in ardd . items (): 
6
if d < 
100: 
7
nardd[d] = v 
8 ardd = nardd 
9 
10 plot_RDD(ardd , ‘Average recurrence distance distribution for k ’  
11 
+ str  (k))  
The following chart is the average RDD calculated over only the non-coding part 
of the genome. The parallel 3-peak distribution disappears. 
1 k = 2  
2 ardd = aRDD(ncseq ,k) 
3 
4 nardd = dict () 
5 for d,v in ardd . items (): 
6
if d < 100: 
7
nardd[d] = v 
8 ardd = nardd 
9 
10 plot_RDD(ardd , ‘Average recurrence distance distribution on 
11 non−coding parts for k ’+  str  (k))  
When the average RDD is computed on only the coding part of the genome, 
the parallel 3-peak distribution appears back. The phenomenon is given by the 3-
periodicity of the codon language that is reﬂected in the 3-peak distribution of the 
RDDs of almost all the 2-mers. The implications on the overall sequence lead to the 
fact that the genome is almost covered by coding sequences

278
8
Laboratory in Python
1 k = 2  
2 ardd = aRDD(cseq ,k) 
3 
4 nardd = dict () 
5 for d,v in ardd . items (): 
6
if d < 100: 
7
nardd[d] = v 
8 ardd = nardd 
9 
10 plot_RDD(ardd , ‘Average recurrence distance distribution on 
11 coding parts for k ’+  str  (k))  
8.12 
Generation of Random Genomes 
This section shows how to produce random strings in python3. The basic approach 
uses the pseudo-random number generator provided by the built-in library of python 
called random. The library implements several methods for generating random 
integers which can also be exploited for surrogate operations such as shufﬂing a 
sequence of elements or picking up a random element from kk elements from of 
a given alphabet of allowed elements symbols, by generating a casual sequence. 
However, one of the most important functions of this library does not strictly concern 
the output of the random generator, but rather its internal conﬁguration. In fact, 
random generators are very deterministic processes which start from a given internal 
conﬁguration to generate the ﬁrst pseudo-random number, and then by switching to 
the next internal conﬁguration generate the successive number, and so forth. Thus, the 
parameters of the initial conﬁguration are of key importance for the entire sequence of 
generated numbers. In some cases, depending on the internal process of the generator, 
a bad initial conﬁguration can lead to a non-random sequence of numbers. One 
famous example is the linear congruential generator which is based on the formula 
upper X Subscript n plus 1 Baseline equals left parenthesis a upper X Subscript n Baseline plus c right parenthesis mod mXn+1 = (aXn + c) mod m, where upper X Subscript n plus 1Xn+1 is the next number to be generated, upper X Subscript nXn is 
the number generated at the previous step and a comma ca, c and mm are the parameters of 
the generator. It can be shown that for upper X 0 equals 1 comma a equals 3 comma c equals 1 comma m equals 5X0 = 1, a = 3, c = 1, m = 5 the generator 
produces a sequence with period 4. Modern generators, such as the Mersenne Twister 
generator, are much more robust and safe, but still, the initial conﬁguration plays 
an important role in the quality of randomness level of their generations. Random 
sequences as well as random modiﬁcations of a biological sequence are used to

8.12 Generation of Random Genomes
279
evaluate the divergence from the randomness of the real biological sequences. Often, 
empirical p-values are calculated by randomly shufﬂing sequences or by generating 
random instances. If different sequences are generated by different executions of the 
same script, then very likely different results will be obtained. thus, it is of primary 
importance to ensure the reproducibility of the results, which means that multiple 
executions of the same script must return the same result. If the script contains the 
generation of random sequences, or in general random elements, these elements 
must be the same in every execution of the script. In this context, reproducibility is 
ensured by setting the initial conﬁguration of the random generator via the function 
random.seed, which takes as input a number, called seed, to set up the initial 
conﬁguration of the generator. 
A ﬁrst approach to generate random sequences of nucleotides, thus genomes, is 
by emulating a Bernoullian process in which the four nucleobases have the same 
probability to be extracted. This is obtained by taking into account that the basis 
random generator works by generating numbers in the range 0 to 2 Superscript b2b, where bb is the 
number of bits of the architecture that is running the code, with uniform distribution. 
It is trivial to convert numbers generated in the interval left bracket 0 comma 2 Superscript b Baseline right bracket[0, 2b] to numbers in the 
intervalleft bracket x comma y right bracket[x, y], wherexx andyy are user-speciﬁed limits of the interval. Thus, one can 
ask to generate numbers in left bracket 0 comma 3 right bracket[0, 3] and then convert them into the 4 nucleobases by 
means of a bijective function between left bracket 0 comma 3 right bracket[0, 3] and StartSet upper A comma upper C comma upper G comma upper T EndSet{A, C, G, T }, as it is shown  in  the  
following example. 
1 import random 
2 random. seed(123456) 
3 
4 map = {0:‘A’ , 1:‘C’ , 2:‘G’ , 3:‘T’} 
5 
6 s =  ” #an empty string to be filled 
7 for i in range(100): 
8
s +=  map[ random. int(0 ,3) ] 
9 
10 print(s) 
The script uses 123456 as seed and always generates the same sequence of 
100 nucleotides. It has to be pointed out that such a sequence is constant between 
multiple executions of the scripts in the same environment. This means that different 
versions of pyhton, as well as different versions of the random library, may generate 
different results. For this reason, it is crucial to make the script publicly available 
when publishing the results of a study. and it is of the same importance to report 
any detail regarding the environment, especially the version of each library that was 
used. 
An alternative way to generate the same sequence is given by using the function 
random.choice which randomly picks up an element of the input list. 
1 random. seed(123456) 
2 
3 alphabet = [ ‘A’ ,‘C’ ,‘G’ ,‘T’] 
4 
5 s =  ”

280
8
Laboratory in Python
6 for i in range(100): 
7
s += random. choice(alphabet) 
8 
9 print(s) 
Since the two scripts generate the same sequence, it is intuitive to think that the 
internal functioning of random.choice is very close to the ﬁrst example, but two 
important aspects must be considered. Firstly, both scripts set the same seed for the 
random generator. Secondly, the order in which the symbols are declared in the list 
alphabet follows the mapping given by the dictionary map of the ﬁrst script. 
In real cases, the Bernoullian uniform process can not be the most suitable model 
for generating random strings. In particular, if the frequencies of the nucleobases are 
taken into account. In real genomes, the nucleobases do have not the same frequency, 
and one may want to generate a random string which at least respects such frequen-
cies. A possible approach is to implement a Bernoullian process in which a different 
number of balls are used to ﬁll the urn. Thus, if the probabilities of nucleobases 
are expressed in cents, then an alphabet/list of 100 symbols can be passed to the 
random.choice. Such a list contains a number ofupper AA elements that is proportional 
to the probability of upper AA, as well as for the other 3 symbols. 
1 import math 
2 import random 
3 
4 random. seed(123456) 
5 
6 resolution = 100 
7 
8 probabilities = 
{‘A’ :0.2 , ‘C’ :0.3 , ‘G’ :0.3 , ‘T’:0.2} 
9 
10 alphabet = [] 
11 for k,v in probabilities . items (): 
12
alphabet += k∗math. ceil ( resolution ∗ v) 
13 
14 s =  ” 
15 
16 for i in range(100): 
17
s += random. choice(alphabet) 
The example can be used to generate sequences of any length by keeping the same 
resolution for the probabilities. 
It has to be noticed that such an approach does not ensure that the real and 
the random strings have the same multiplicity distribution. In fact, given two kk-
mers, for example alpha equals a 1 a 2α = a1a2 and beta equals b 1 b 2β = b1b2, with a 1 comma a 2 comma b 1 comma b 2 element of StartSet upper A comma upper C comma upper G comma upper T EndSeta1, a2, b1, b2 ∈{A, C, G, T }, their 
consecutive extraction produces the substringalpha beta equals a 1 a 2 b 1 b 2αβ = a1a2b1b2 and thus the unwanted 
a 2 b 1a2b1 2-mer. Thus, it can be better to model the process with a Markov chain such that 
it assigns a probability of extracting a symbol from the alphabet by looking at the last 
generated nucleotide. In any case, none of the two approaches can guarantee the exact 
correspondence between the observed frequency of the kk-mers in the real genome 
and the random string. Such approaches are then helpful in studying the random 
composition ofkk-mers starting from frequencies ofk minus 1k −1-mers. An alternative way,

8.12 Generation of Random Genomes
281
which preserves kk-multiciplicities, is to build a de Bruijn graph of order k minus 1k −1 as a 
multigraph in which the number of edges between two vertices equals the multiplicity 
of the kk-mer formed by the junction of the two left parenthesis k minus 1 right parenthesis(k −1)-mers represented by the 
vertices. Thus, the solution is obtained by ﬁnding all the Eulerian paths which start 
from the ﬁrst kk-mer of the real genome, and then randomly selecting one of them. 
This approach could be employed in the recurrences of words rather than the overall 
multiplicity distribution.

References 
1. Aardenne, T., de Bruijn, N.G.: Circuits and trees in oriented linear graphs. Simon Stevin 28, 
203–217 (1951) 
2. Bonnici, G.R., Manca, P.: A dictionary-based method for pan-genome content discovery. BMC 
Bioinform. 19(Suppl 15), 437 (2018) 
3. Chaitin, G.J.: On the simplicity and speed of programs for computing inﬁnite sets of natural 
numbers. JACM 16(3), 407–422 (1969) 
4. Chaitin, G.J.: Algorithmic Information Theory. Cambridge University Press (1987) 
5. Church, A.: On the concept of a random sequence. Bull. Am. Math. Soc. 46, 130–135 (1940) 
6. Devroye, L.: Non-Uniform Random Variate Generation. Springer, Berlin (1986) 
7. Downey, R.G., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity. Springer, Berlin 
(2010) 
8. Holland, J., Mallot, H.: Emergence: from chaos to order. Nature 395, 342–342 (1998) 
9. Knuth, D.: The Art of Computer Programming Volume 1 (Fundamental algorithms) and Volume 
2 (Semi-numerical algorithms). Addison-Wesley (1997) 
10. Kolmogorov, A.N.: Logical basis for information theory and probability theory. IEEE Trans. 
Inf. Theory 14, 662–664 (1968) 
11. Kolmogorov, A.N.: On tables of random numbers. Theor. Comput. Sci. 207(2), 387–395 (1998) 
12. Kullback, S., Leibler, R.A.: On information and sufﬁciency. Ann. Math. Stat. 79–86 (1951) 
13. Landauer, R.: Dissipation and noise immunity in computation and communication. Nature 335, 
779–784 (1988) 
14. Li M., Vitanyi, P.: An Introduction to Kolmogorov Complexity and Its Applications. Springer, 
Berlin (2008) 
15. Manca, V.: Logica Matematica. Bollati-Boringhieri (2005) 
16. Manca, V.: Linguaggi e Calcoli. Principi matematici del “coding”. Bollati Boringhieri (2019) 
17. Manca, V.: Formule e Metafore. Immaginazione e Concettualizzaaione scientiﬁca, Edizioni 
Nuova Cultura (2021) 
18. Manca, V., Santagata, M.: Un meraviglioso accidente. La nascita della vita, Mondadori (2018) 
19. Manca, V., Bianco, L.: Biological networks in metabolic P systems. BioSystems 91, 489–498 
(2008) 
20. May, R.M.: Simple mathematical models with very complicated dynamics. Nature 261, 459–67 
(1976) 
21. McEliece, R.J.: The Theory of Information and Coding. Cambridge University Press (2003) 
22. Pincus, S.M.: Approximate entropy as a measure of system complexity. P. Nat. Acad. Sci. 88, 
2297–2301 (1991) 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer 
Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9 
283 

284
References 
23. Rosenblueth, A., Wiener, N., Bigelow, J.: Behavior, purpose and teleology. Philos. Sci. 10(1), 
18–24 (1943) 
24. Solomonov, R.: An exact method for the computation of the connectivity of random nets. Bull. 
Math. Biophys. 14, 153 (1952) 
25. Soto, J.: Statistical Testing of Random Number Generators (1999). http://csrc.nist.gov/rng/ 

Index 
A 
Absolute dictionary, 79 
AI agents, 178 
Al-kindi, 24 
Anti-entropic component, 97 
Anti-parallelism, 71 
Archimedes of Syracuse, 106 
Arcsine law, 122 
Artiﬁcial neural networks, 165 
ASCII code, 38 
Automa, 14 
Average multiplicity distribution, 246 
B 
Bayes, 16 
Bayes’ theorem, 18 
Benford’s law, 126 
Bernoulli, 16 
Bernoulli distribution, 115 
Bilinear structure, 69 
Binary logarithmic entropy, 80 
Binomial coefﬁcients, 12 
Binomial distribution, 115 
Bio-bit, 98 
Bistabilty, 72 
Boltzmann, 25 
Boltzmann’s equation, 25, 140 
C 
Carnot, 25 
Cebychev distance, 101 
Centrifugal governor, 27 
Chatbot, 176 
Chebicev’s inequality, 19 
Chevalier de Méré, 18 
Chirality, 69 
Chomsky, 15 
Chomsky grammar, 15 
Clausius, 25 
Code, 38 
Codewords, 38 
Combinatorial schemata, 10 
Complementarity, 71 
Computation, 27 
Conditional entropy, 32 
Conditional probability, 17 
Cybernetics, 28 
D 
Darwin, 76 
Data, 31 
Data types, 191 
de Bruijn graph, 135 
De Moivre, 16 
de Moivre-Laplace theorem, 115 
Digital information, 38 
Double helix, 69 
Duplex, 78 
E 
Elongation, 80 
Empirical entropy distribution, 247 
Encoding, 38 
Enhanced sufﬁx arrays, 254, 259 
Entropic component, 97 
Entropic divergence, 33 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer 
Nature Switzerland AG 2023 
V. Manca and V. Bonnici, Infogenomics, Emergence, Complexity and Computation 48, 
https://doi.org/10.1007/978-3-031-44501-9 
285 

286
Index 
Entropy, 24 
Entropy and computation, 27 
Entropy circular principle, 26 
Equipartition property, 24, 27, 36 
Erasure principle, 29 
Ergodicity, 77 
Exponential distribution, 117 
F 
First Shannon theorem, 26, 47, 49 
Fisher information, 105 
Fourier series, 57 
Fourier transform, 61 
Fully elongated length, 95 
G 
Gauss, 16 
Generative pre-training transformer, 176 
Generator, 197 
Genetic algorithm, 162 
Genetic code, 38 
Genome, 67 
Genome dictionary, 77 
Genome sequencing, 94 
Genomic complexity, 99 
Grammar, 14 
H 
Hapax, 78 
Hapax-repeat interval, 80 
Heredity, 76 
Hilbert space, 59 
Homeostasis, 28 
Homochirality, 70 
Huffman encoding, 43 
I 
Indentation, 194 
Information, 23, 24 
Instantaneous code, 38 
Iterable object, 196 
J 
Jaccard distance, 100 
Joint entropy, 32 
K 
Kolmogorov, 17 
k-permutable, 88 
Kraft norm, 40 
k-spectral segment, 87 
k-spectral segmentation, 88 
k-spectrum, 86 
Kullback-Leibler divergence, 33 
k-univocal, 86, 88 
L 
Laplace, 16 
Laws of large numbers, 120 
Lexical index, 80 
Lexicographic recurrence, 107 
Logarithmic length, 79 
Logarithm lemma, 47 
Logarithm randomness condition, 136 
Longest common preﬁx, 254 
M 
Markov chain, 129 
Maximal repeat, 78 
Maximum Repeat Length (MRL), 79, 236 
McMillan theorem, 42 
Mean, 21 
Minimal forbidden length, 79 
Minimal hapax, 78 
Minimal hapax greatest length, 79 
Minimal hapax length, 79 
Minimum Forbidden Length (MFL), 238 
Minimum Hapex Length (MHL), 237 
Minkowski distance, 101 
Monomer, 67 
Monomeric triangle, 68 
Montecarlo methods, 124 
Monty Hall’s problem, 113 
Multinomial distribution, 119 
Mutual information, 34 
N 
No-hapax length, 79 
No-repeat length length, 79 
Normal distribution, 117 
Nucleotide, 67 
Nullomer, 78 
O 
Object, 192 
P 
Pascal, 18 
Planck constant, 146 

Index
287 
Poisson, 16 
Poisson distribution, 117, 118 
Positional maximal repeat, 78 
Position coverage, 83, 84 
Preﬁx free code, 38 
Probability: basic rules, 17 
p-Value, 105 
Pythagorean recombination game, 144 
Python class, 210 
Python function, 197 
Python iterator, 197 
Python method, 211 
Q 
Quantum mechanics, 151 
R 
Random sequence, 124 
Random string, 135 
Random walk, 121 
Repeat, 78 
Reversible computation, 30 
S 
Sampling theorem, 62 
Schrödinger equation, 149 
Second principle of thermodynamics, 25 
Second Shannon theorem, 53 
Selection, 76 
Sequence coverage, 83 
Shannon, 23, 24 
Shannon’s theorems, 31 
Specularity, 72 
Standard deviation, 21 
String, 13 
Substring, 13 
Sufﬁx array, 249 
Swarm intelligence, 164 
T 
Theorem H, 26 
Thermodynamic entropy, 25 
Third Shannon theorem, 64 
Topology, 132 
Transmission code, 52 
Turing, 14 
Turing machine, 14 
Turing test, 176 
Typical sequence, 50 
U 
Univocal code, 40 
V 
Variability, 76 
Variables, 191 
Variance, 21 
von Neumann, 30 
W 
Watt, 27 
Wiener, 28 
Word co-multiplicity distribution, 248 
Word length distribution, 244 
Word multiplicity distribution, 239 
Z 
Zeroless positional representation, 109 

