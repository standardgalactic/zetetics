HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics
Artur Grigorev1,2
Bernhard Thomaszewski1
Michael J. Black2
Otmar Hilliges1
1 ETH Zurich, Department of Computer Science
2 Max Planck Institute for Intelligent Systems, Tubingen
https://dolorousrtur.github.io/HOOD/
Figure 1. We combine graph neural networks, a hierarchical graph representation and multi-level message passing with an unsupervised
training scheme to enable real-time prediction of realistic clothing dynamics for arbitrary types of garments and body shapes. Our method
models both tight-fitting and free-flowing clothes draped over arbitrary body shapes. The method generalizes to new, entirely unseen,
garments (left), and allows for dynamic and unconstrained poses (right) and changes in material parameters and topology at test time.
Abstract
We propose a method that leverages graph neural
networks, multi-level message passing, and unsupervised
training to enable efficient prediction of realistic clothing
dynamics. Whereas existing methods based on linear blend
skinning must be trained for specific garments, our method,
called HOOD, is agnostic to body shape and applies to
tight-fitting garments as well as loose, free-flowing cloth-
ing.
Furthermore, HOOD handles changes in topology
(e.g., garments with buttons or zippers) and material prop-
erties at inference time. As one key contribution, we pro-
pose a hierarchical message-passing scheme that efficiently
propagates stiff stretching modes while preserving local de-
tail. We empirically show that HOOD outperforms strong
baselines quantitatively and that its results are perceived as
more realistic than state-of-the-art methods.
1. Introduction
The ability to model realistic and compelling clothing
behavior is crucial for telepresence, virtual try-on, video
games, and many other applications that rely on high-
arXiv:2212.07242v3  [cs.CV]  16 Jun 2023

fidelity digital humans. A common approach to generating
plausible dynamic motions is physics-based simulation [2].
While impressive results can be obtained, physical simula-
tion is sensitive to initial conditions, requires animator ex-
pertise, and is computationally expensive; state-of-the-art
approaches [14, 22, 36] are not designed for the strict com-
putation budgets imposed by real-time applications.
Deep learning-based methods have started to show
promising results both in terms of efficiency and quality.
However, there are several limitations that have so far pre-
vented such approaches from unlocking their full potential:
First, existing methods rely on linear-blend skinning
and compute clothing deformations primarily as a func-
tion of body pose [24, 43]. While compelling results can
be obtained for tight-fitting garments such as shirts and
sportswear, skinning-based methods struggle with dresses,
skirts, and other types of loose-fitting clothing that do not
closely follow body motion.
Crucially, many state-of-the-art learning-based methods
are garment-specific [20,38,43,45,53] and can only predict
deformations for the particular outfit they were trained on.
The need to retrain these methods for each garment limits
applicability.
In this paper, we propose a novel approach for predict-
ing dynamic garment deformations using graph neural net-
works (GNNs). Our method learns to predict physically-
realistic fabric behavior by reasoning about the map be-
tween local deformations, forces, and accelerations. Thanks
to its locality, our method is agnostic to both the global
structure and shape of the garment and directly generalizes
to arbitrary body shapes and motions.
GNNs have shown promise in replacing physics-based
simulation [40,41], but a straightforward application of this
concept to clothing simulation yields unsatisfying results.
GNNs apply local transformations (implemented as
MLPs) to feature vectors of vertices and their one-ring
neighborhood in a given mesh. Each transformation results
in a set of messages that are then used to update feature vec-
tors. This process is repeated, allowing signals to propagate
through the mesh. However, a fixed number of message-
passing steps limits signal propagation to a finite radius.
This is problematic for garment simulation, where elastic
waves due to stretching travel rapidly through the material,
leading to quasi-global and immediate long-range coupling
between vertices. Using too few steps delays signal propa-
gation and leads to disturbing over-stretching artifacts, mak-
ing garments look unnatural and rubbery. Na¨ıvely increas-
ing the number of iterations comes at the expense of rapidly
growing computation times. This problem is amplified by
the fact that the maximum size and resolution of simulation
meshes is not known a priori, which would allow setting a
conservative, sufficiently large number of iterations.
To address this problem, we propose a message-passing
scheme over a hierarchical graph that interleaves propaga-
tion steps at different levels of resolution. In this way, fast-
travelling waves due to stiff stretching modes can be effi-
ciently treated on coarse scales, while finer levels provide
the resolution needed to model local detail such as folds
and wrinkles. We show through experiments that our graph
representation improves predictions both qualitatively and
quantitatively for equal computation budgets.
To extend the generalization capabilities of our ap-
proach, we combine the concepts of graph-based neural
networks and differentiable simulation by using an incre-
mental potential for implicit time stepping as a loss func-
tion [33,43].
This formulation allows our network to be trained in a
fully unsupervised way and to simultaneously learn multi-
scale clothing dynamics, the influence of material parame-
ters, as well as collision reaction and frictional contact with
the underlying body, without the need for any ground-truth
(GT) annotations. Additionally, the graph formulation en-
ables us to model garments of varied and changing topol-
ogy; e.g. the unbuttoning of a shirt in motion.
In summary, we propose a method, called HOOD, that
leverages graph neural networks, multi-level message pass-
ing, and unsupervised training to enable real-time predic-
tion of realistic clothing dynamics for arbitrary types of gar-
ments and body shapes.
We empirically show that our method offers strategic ad-
vantages in terms of flexibility and generality compared to
state-of-the-art approaches.
Specifically, we show that a
single trained network: (i) efficiently predicts physically-
realistic dynamic motion for a large variety of garments;
(ii) generalizes to new garment types and shapes not seen
during training; (iii) allows for run-time changes in mate-
rial properties and garment sizes, and (iv) supports dynamic
topology changes such as opening zippers or unbuttoning
shirts. Code and models are available for research purposes:
https://dolorousrtur.github.io/hood/.
2. Related Work
Physics-based Simulation. Modeling the behavior of
3D clothing is a longstanding problem in computer graph-
ics [48].
Central research problems include mechanical
modeling [13,19,50], material behavior [7,34,51], time in-
tegration [3,49], collision handling [8,21,22,47] and, more
recently, differentiable simulation [25, 26]. While state-of-
the-art methods can generate highly realistic results with
impressive levels of detail, physics-based simulation meth-
ods are often computationally expensive.
Learned Deformation Models. To overcome the per-
formance limitations of traditional physical simulators,
prior work uses machine learning to accelerate computa-
tion. One line of research uses neural networks, in combi-
nation with linear blend skinning, to learn garment defor-
2

mations from pose and shape parameters of human body
models [20, 30, 39, 42]. While such methods can produce
plausible results at impressive rates, their reliance on skin-
ning limits their ability to realistically model loose, free-
flowing garments such as long skirts and dresses.
Several learning-based methods specifically tackle loose
garments. For example, Santesteban et al. [45] introduce a
diffused body model to extend blend shapes and skinning
weights to any 3D point beyond the body mesh. Pan et
al. [38] use virtual bones to drive the shape of garments as
a function of the body pose.
A common limitation of such methods is their inability
to generalize across multiple garments: as they predict a
specific number of vertex offsets, these networks need to be
retrained even for small changes in garment geometry.
Another set of methods learns geometric priors from a
large dataset of synthetic garments [4]. Zakharkin et al. [52]
learn a latent space of garment geometries, modeling gar-
ments as point clouds.
Su et al. [46] represent garments in texture space, allow-
ing them to explicitly control garment shape and topology.
DeePSD [6] learns a mapping from a garment template to
its skinning weights. Although these methods are able to
generate geometries for various garments, including loose
ones, they still rely on linear blend skinning and paramet-
ric body models, ultimately limiting their ability to generate
dynamic garment motions.
Traditional, mesh-based methods are typically restricted
to fixed topologies and cannot deal with topological
changes such as unzipping a jacket. To address this, sev-
eral methods resort to implicit shape models [1, 10, 12, 15,
27,31,44]. While these can capture arbitrary topology, they
require significant training data, are not compatible with ex-
isting graphics pipelines, and are expensive to render. In
contrast, our graphical formulation supports varied topolo-
gies with an efficient mesh representation.
Recent work learns the mapping from body parameters
to garment deformations in an unsupervised way. PBNS [5]
pioneered this idea by using the potential energy of a mass-
spring system to train a neural network to predict garment
deformations in static equilibrium.
SNUG [43] extends
this approach with a dynamic component, using a recur-
rent neural network to predict sequences of garment defor-
mations. Their method outperforms other learning-based
approaches without using any physically-simulated training
data. SNUG, however, is limited to tight-fitting garments
and cannot generalize to novel garments.
Our method builds on the idea of a physics-based loss
function for self-supervised learning. Unlike SNUG, how-
ever, our method does not rely on skinning and is able
to generate plausible dynamic motion for arbitrary types
of garments, including dynamic, free-flowing clothing and
changing topology.
Graph-based Methods. As another promising line of
research, graph neural networks have recently started to
show their potential for learning-based modeling of dy-
namic physical systems [16,40,41]. Most closely related to
our work are MeshGraphNets [40], which use a message-
passing network to learn the dynamics of physical systems
such as fluids and cloth from mesh-based simulations. Due
to their local nature, MeshGraphNets achieve outstanding
generalization capabilities.
However, using their default
message passing scheme, signals tend to propagate slowly
through the graph and it is difficult to set hyper-parameters
(number of message passing steps) in advance such that sat-
isfying behavior is obtained.
This problem is particularly relevant for clothing sim-
ulation, where an insufficient number of steps can lead to
excessive stretching and unnatural dynamics. We address
this problem with a multi-level message passing architec-
ture that uses a hierarchical graph to accelerate signal prop-
agation.
Recently, several graph pooling strategies were intro-
duced to increase the radius of message propagation includ-
ing learned pooling [17], pooling by rasterization [28] and
spatial proximity [16]. Concurrent to ours, the work of Cao
et al. [11] analyzes limitations of pooling strategies and sug-
gests using pre-computed coarsened geometries with hand-
crafted aggregation weights for inter-level transfer. We pro-
pose a simple and efficient graph coarsening strategy that
allows our network to implicitly learn transitions between
graph levels, thus avoiding the need for any manually de-
signed transfer operators.
Graph-based methods have demonstrated their ability to
model numerous types of physical systems, including fabric
simulation. To the best of our knowledge, however, we are
the first to propose a graph-based approach for modeling the
dynamic garment motions of dressed virtual humans.
3. Method
Our method, schematically summarized in Fig. 2, learns
the parameters of a single network that is able to pre-
dict plausible dynamic motion for a wide range of gar-
ment types and shapes, that generalizes to new, unseen
clothing, and allows for dynamic changes in material pa-
rameters (Fig. 10) and garment topology (Fig. 4). These
unique capabilities derive from a novel combination of
a) graph neural networks that learn the local dynamics
in a garment-independent way (Sec. 3.1), b) hierarchical
message-passing for the efficient capture of long-range cou-
pling (Sec. 3.2), and c) a physics-based loss function that
enables self-supervised training (Sec. 3.3).
3.1. Background
HOOD builds on MeshGraphNets [40], a type of graph
neural network, to learn the local dynamics of deformable
3

Figure 2. Method overview: We model garment mesh interactions based on a graph that is derived from the garment mesh and augmented
with additional body nodes (blue) and edges between the garment and the closest body nodes. The input graph is converted into a
hierarchical graph structure to allow fast signal propagation and is processed by a message-passing network with a UNet-like architecture.
The colors of garment nodes show which of the nodes are present on each of the hierarchical levels and correspond to the levels of the
GNN. The GNN predicts accelerations (green) for each garment node. The model is trained via self-supervision with a set of physical
objectives, thus removing the need for any offline training data. At inference time, the model autoregressively generates dynamic garment
motions over long time horizons. A single network can generalize to unseen garments, material properties, and even topologies.
materials. Once trained, MeshGraphNets predict nodal ac-
celerations from current positions and velocities, which are
then used to step the garment mesh forward in time.
Basic Structure. We model garment dynamics based on
a graph consisting of the vertices and edges of the garment
mesh, augmented with so-called body edges: for each gar-
ment node we find the nearest vertex on the body model
and add a new edge if the distance is below a threshold r.
Vertices and edges are endowed with feature vectors vi and
eij, respectively, where i and j are node indices. Nodal
feature vectors consist of a type value (garment or body),
current state variables (velocity, normal vector), and physi-
cal properties (mass). Edge feature vectors store the relative
position between their two nodes w.r.t. both the current state
and canonical geometry of the garment (Fig. 2, left).
Message Passing. To evolve the system forward in time,
we apply N message-passing steps on the input graph. In
each step, edge features are first updated as
eij ←fv→e(eij, vi, vj) ,
(1)
where fv→e is a multilayer perceptron (MLP). Nodes are
then updated by processing the average of all incident edge
features, denoted by fe→v:
vi ←fe→v(vi,
X
j
ebody
ij
,
X
j
eij) ,
(2)
where fe→v is another multilayer perceptron (MLP), and
ebody
ij
are body edges. While the same MLP is used for
all nodes, each set of edges is processed by a separate
MLP. After N message-passing steps, the nodal features are
passed into a decoder MLP to obtain per-vertex accelera-
tions, which are then used to compute end-of-step velocities
and positions.
Extensions for Clothing. To model different types of
fabric and multi-fabric garments, we extend node and edge
feature vectors with local material parameters. These ma-
terial parameters include: Young’s modulus and Poisson’s
ratio (mapped to their corresponding Lam´e parameters µ
and λ) that model the stretch resistance and area preserva-
tion of a given fabric, the bending coefficient kbending that
penalizes folding and wrinkling, as well as the density of
the fabric, defining its weight. Since our network supports
heterogeneous material properties (for each edge and node)
as input, the definition of individual material parameters for
different parts to model multi-material garments is possible,
even at inference time (See Fig. 10).
3.2. Hierarchical Message Passing
Fabric materials are sufficiently stiff such that forces ap-
plied in one location propagate rapidly across the garment.
When using a fixed number of message-passing steps, how-
ever, forces can only propagate within a finite radius for a
given time step. Consequently, using too small a number
of message-passing steps will make garments appear overly
elastic and rubbery. We solve this problem by extending
MeshGraphNets to accelerate signal propagation. To this
end, we construct a hierarchical graph representation from
the flat input graph and use this to accelerate signal propa-
gation during message-passing.
Hierarchical Graph Construction. Allowing messages
to travel further within a single step requires long-range
4

Figure 3. Our hierarchical network architecture with 1 fine (green)
and 2 coarse (yellow, orange) levels. We use 15 message-passing
steps, simultaneously processing two levels at a time.
connections between nodes. To this end, we recursively
coarsen a given input graph to obtain a hierarchical repre-
sentation. Although there are many other options for gener-
ating graph hierarchies, we take inspiration from concurrent
work [11] and use a simple but effective recursive process.
We start by partitioning the nodes of the input graph into
successively coarser sets such that the inter-vertex distance
— the number of edges in the shortest path between two
nodes — increases. We then create new sets of coarsened
edges for each of the partitions. See Fig. 2 for an illustration
and the supplemental material for details.
By applying this algorithm recursively, we obtain a
nested hierarchical graph in which the nodes of each coarser
level form a proper subset of the next finer level nodes, i.e.,
Vl+1 ⊂Vl. This property is important for our multi-level
message-passing scheme, which we describe next.
Multi-level Message Passing. Our nested hierarchical
graph representation enables accelerated message-passing
through simultaneous processing on multiple levels. To this
end, we endow each level l in the graph with its own set
of edge feature vectors el
ij while the node feature vectors
vi are shared across all levels. At the beginning of each
message-passing step, we first update edge features on all
levels using the finest-level node features:
el
ij ←f l
v→e(el
ij, v0
i , v0
j ) ,
(3)
where f l
v→e is a level-specific MLP. Then, node features are
updated:
vi ←fe→v(vi,
X
j
ebody
ij
,
X
j
e1
ij, ...,
X
j
eL
ij) ,
(4)
where L is the number of levels processed in this step. Note
that, for each message-passing step, we update the set of
body edges ebody
ij
to keep only those that are connected to
the currently processed garment nodes.
This scheme has the important advantage of not requir-
ing any explicit averaging or interpolation operators for
inter-level transfer. Thanks to the nesting property of our
hierarchical graph, nodes are shared across levels and all
information transfer happens implicitly by processing the
fe→v MLPs at the end of each message-passing step.
Our multi-level message-passing scheme can operate on
any number of levels simultaneously. We found that the
UNet-like architecture shown Fig. 3, with a three-level hi-
erarchy and simultaneous message-passing on two adjacent
levels at a time, yields a favorable trade-off between infer-
ence time and quality of results.
To compute the propagation radius of our multi-level
message-passing scheme, we sum the maximal distances a
message can travel per message-passing step. For roughly
the same amount of computation, our architecture yields a
radius of 48 edges compared with 15 edges for a single-
level scheme. See Sec. 4 and the supplementary material
for a more detailed analysis.
3.3. Garment Model
To learn the dynamics of garments, we must model their
mechanical behaviour; i.e., the relation between internal de-
formations and elastic energy, as well as friction and contact
with the underlying body. Our approach follows standard
practice in cloth simulation: we model resistance to stretch-
ing with an energy term Lstretching, using triangle finite el-
ements [36] with a St. Venant-Kirchhoff material [35]. The
Lbending term penalizes changes in discrete curvature as a
function of the dihedral angle between edge-adjacent trian-
gles [19]. To prevent interpenetration between cloth and
body, we penalize the negative distance between garment
nodes and their closest point on the body mesh with a cu-
bic energy term Lcollision, once it falls below a threshold
value [43]. Furthermore, Linertia is an energy term whose
gradient with respect to xt+1 yields inertial forces. Finally,
we introduce a friction term Lfriction that penalizes the tan-
gential motion of garment nodes over the body [9,18].
To model the evolution of clothing through time, we
follow [45] and use the optimization-based formulation of
Martin et al. [33] to construct an incremental potential
Ltotal =Lstretching(xt+1) + Lbending(xt+1)+
Lgravity(xt+1) + Lfriction(xt, xt+1)+
(5)
Lcollision(xt, xt+1) + Linertia(xt−1, xt, xt+1) ,
where xt−1, xt, and xt+1 are nodal positions at the previ-
ous, current, and next time steps, respectively. Minimizing
Ltotal with respect to end-of-step positions is equivalent to
solving the implicit Euler update equations, providing a ro-
bust method for forward simulation. When used as a loss
during training, this incremental potential allows the net-
work to learn the dynamics of clothing without supervision.
3.4. Training
We train our hierarchical graph network in a fully self-
supervised way using the physics-based loss function (5).
We briefly discuss some aspects specific to our setting be-
low and provide more details in the supplemental material.
5

Figure 4. Its graph-based nature allows HOOD to model garments with changing topology by enabling and disabling specific edges in the
garment mesh. Here we show frames from a sequence generated with our model, where we unbutton a shirt and then button it back up.
Training Data. We use the same set of 52 body pose
sequences from the AMASS dataset [32] used in [45]. For
each training step, we randomly sample SMPL [29] shape
parameters β from the uniform distribution U(−2, 2).
We randomly select a garment mesh from a set of tem-
plates that includes a shirt, tank top, long-sleeve top, shorts,
pants, and a dress. We resize each garment according to
the blend shapes of the underlying body and apply a small
sizing perturbation randomly sampled from U(0.9, 1.1) to
emulate tighter and looser fits.
We also sample material parameters (Lam´e parameters
for stretching, bending stiffness, and mass density) from
log-uniform distributions and use corresponding values in
the loss function (5). The same values are attached to all
feature vectors, encouraging the network to learn the map
between local material parameters and dynamics.
Garment Initialization. During training, we want to
step garment meshes forward in time from any point in the
pose sequences. To evaluate the loss function at an arbi-
trary starting point, we must provide garment geometry for
the two previous time steps. We approximate these geome-
tries using linear blend skinning combined with the diffused
body model formulation from [45]. We then remove any in-
tersections between skinned garment meshes and the body.
The initial garment meshes computed in this way are gener-
ally not in energetically optimal states. To reduce this inter-
nal energy, we find it useful to scale down the contribution
of the inertia term using a coefficient α ∈(0, 1]. Using
a smaller coefficient α for the first step during training it-
erations allows the garment to quickly relax into a state of
lower potential energy. We also pass α as an input to the net-
work (see Sec. 3.1) so it can adapt its prediction to different
values. Finally, it should be noted that skinning is only used
for initialization during training, but not at inference time.
Normalization. Following [40], we find it crucial for
convergence to normalize feature vectors at the beginning
of each step using exponentially weighted averages of their
mean and standard deviations. We also perform denormal-
ization on the outputs to obtain accelerations. Since we do
not have access to ground-truth data, we use statistics col-
lected from linearly-skinned garments as described above.
Autoregressive Training. We start by predicting accel-
erations for only one next step, then gradually increase the
number of predicted steps every k iterations up to 5. We
set k to 5000 and find that predicting a maximum of 5 steps
during training is enough for the model to autoregressively
run for hundreds of steps at inference time.
4. Evaluation
To evaluate HOOD, we first perform a perceptual study
that compares the perceived visual quality of our results
with the state of the art. Next, we analyze the benefit of our
network architecture using quantitative performance met-
rics. Finally, we show examples that demonstrate the main
advantage of HOOD; i.e., its generalization capabilities.
4.1. Comparison to State-of-the-Art Methods
To evaluate the quality of the dynamic garment motions
produced by HOOD, we compare it to two state-of-the-art
alternatives, SSCH [45] and SNUG [43]. Since there are no
object measures of quality and plausibility, we use a percep-
tual study: 30 participants were shown pairs of videos, gen-
erated by our method and a baseline, for the same garment
and pose sequence in a side-by-side view. Participants were
asked to select the sequence in which ”the garment looks
and behaves more realistically”. The order and placement
(left/right) of the videos were randomized per presentation.
We used 8 body pose sequences from the AMASS
dataset [32] for this study: 4 sequences from the vali-
dation set used in [43], and 4 new sequences with more
dynamic and challenging body motions, including upside-
down poses. We analyze preferences from participants sep-
arately for the two sequence sets.
6

Figure 5. Perceptual study. Each bar shows the percentage of se-
quences for which participants preferred our method over the base-
line, i.e., SNUG [43], SSCH [45] or ARCSIM [37]. Our method
comfortably outperforms learned approaches while being on par
with a genuine physical simulator. We do not compare to ARC-
SIM on the new sequences due to a large number of self-collisions
in them, that ARCSIM failed to resolve.
garments
# steps
propagation
radius ↑
average
speed,
fps ↑
Ltotal ↓
∥∂Ltotal
∂ˆx
∥2 ↓
Fine15
only dress
15
15
6.65
2.92
2.37e-2
Fine48
48
48
2.45
1.43
1.03e-2
Ours
15
48
7.27
1.48
1.56e-2
Fine15
all
15
15
13.1
1.68
3.7e-2
Fine48
48
48
4.99
1.04
2.5e-2
Ours
15
48
13.6
1.07
2.72e-2
Table 1. Comparison of our hierarchical model to two ablations in
terms of total loss values and inference speed for a dress with 12K
vertices from the VTO dataset [45]. See supplementary material
for more detail.
In each of the comparisons, we use the full set of gar-
ments that the baseline was trained on. For SNUG, this
includes 5 garments (t-shirt, long-sleeve, tank top, pants,
shorts), for SSCH it includes 2 garments (t-shirt, dress). For
each sequence, we randomly sample SMPL shape parame-
ters β from the uniform distribution U(−2, 2).
As can be seen from Fig. 5, participants preferred results
from our method in a clear majority of the cases when com-
pared to other learned methods, whereas the preferences
among our method and a physical simulator ARCSIM [37]
are nearly equal. To put this result into perspective, we note
that all of our results were generated by a single network,
while the learned baselines need a separate trained model
for each garment. In Fig. 6 we also demonstrate the quali-
tative advantages of our method over the state-of-the-art.
4.2. Hierarchical Architecture
To evaluate our multi-level message passing, we com-
pare our hierarchical UNet-like architecture to two baseline
models in terms of objective value and gradient norm, aver-
aged across the validation set. The ablations use the single-
level message passing scheme from MeshGraphNets [40]
with 15 (Fine15) and 48 (Fine48) steps, respectively. Table
1 shows that our hierarchical model outperforms Fine15 in
Figure 6.
Qualitative comparison to state-of-the-art methods,
SNUG [43] and SSCH [45]. Note how our geometry conforms
well to the body and flows naturally with plausible wrinkles in
areas without body contact.
Figure 7. Qualitative comparison of our hierarchical network to
the baseline (Fine15) using only a single level.
The baseline
method requires more time steps to propagate messages from the
point where contact between cloth and body occurs to the loose
parts of the dress. This results in large stretching artifacts (in-
dicated in pink) and rubbery material behaviour. We also show
results of a physical simulator, ARCSIM [37], for reference.
both inference speed and loss value. This is due to the fact
that our hierarchical method has a propagation radius of 48
edges per time step, compared to only 15 for the single-level
7

Figure 8. Results generated by HOOD for garments unseen during
training. Garments in purple are modified versions of those from
the training set, while garments in green are completely new.
Figure 9. We manually extract a garment mesh from a 3D scan
and animate it with HOOD.
baseline. When increasing the number of message pass-
ing steps to 48, the single-level baseline Fine48 achieves
slightly better scores for loss and gradient norm, but its in-
ference speed is significantly lower. Figure 7 shows how the
limited propagation radius of the baseline Fine15 results in
excessive stretching for sequences with dynamic motions.
4.3. Generalization
Novel Garments. Since our method learns the local be-
haviour of fabric, it is not limited to inferring motion for
the garments from the training set. Figure 8 shows results
generated by HOOD for garments not seen during training.
The three garments on the left are modified from the training
set (dress with a cut, long sleeve top with shorter sleeves,
and elongated t-shirt). The next two are provided by the
authors of FoldSketch [23]. The rightmost skirt was manu-
ally created in Blender. HOOD can also simulate real-world
garments; e.g. Figure 9 shows the animation of a garment
manually extracted from a 3D scan.
Resizing Garments. Since our model takes the garment’s
edge lengths at rest as input, it is possible to resize the rest
mesh at inference time to simulate different sizes of the
same garment. We provide examples in the supplementary
Figure 10. A: A single network can model garments with different
material parameters. B: The network is also able to model gar-
ments made of several materials. Here two halves of the dress (in
yellow and purple) have different bending coefficients.
material and the accompanying video.
Changing Topology. We represent garments as a graph,
therefore we can naturally handle dynamically changing
topology (e.g., through buttons or zippers). For example,
in Fig. 4, we manually choose pairs of vertices to connect
graph segments (the “buttons”). Toggling inclusion of these
edges “buttons” or “unbuttons” the garment.
Material parameters. By augmenting the input material
parameters and applying corresponding supervision during
training, HOOD can simulate garments with different mate-
rials. Furthermore, since the input material parameters are
defined separately for each vertex and edge in the garment
mesh, we can define different materials for different parts
of a single garment. Fig. 10 (A) shows an example where
our method—a single trained network—produces qualita-
tively different folds and wrinkle patterns when given dif-
ferent bending coefficients. We also show an example of
a single garment with different materials for different parts
in Fig. 10 (B).
5. Conclusion and Limitations
We have proposed a novel method for modeling the dy-
namics of virtual garments. While previous methods have
shown the benefits of combining physical simulation and
machine learning, they often do not generalize to unseen
garments and struggle with loose and free-flowing garments
due to their reliance on skinning. To overcome these lim-
itations, we leverage a graph-based garment representa-
tion that is processed by a graph neural network to pre-
dict nodal accelerations. We further introduce a hierarchical
graph representation that accelerates the propagation of stiff
waves through the garment and, consequently, leads to more
natural simulation results. Following recent work [43], we
leverage an optimization-based formulation [33] of phys-
ical simulation for supervision. Together, these contribu-
tions enable HOOD to predict plausible dynamic motion for
a wide range of garments on arbitrary body shapes, while
generalizing to unseen garments and new pose sequences.
8

Our method can also handle dynamic changes in material
properties and garment topology at inference time. We be-
lieve that HOOD constitutes an important step towards the
goal of modeling realistic and compelling clothing behavior
on digital humans.
Limitations. Our method currently has a number of limita-
tions that we plan to address in future work. First, our model
does not handle garment-garment interactions and our re-
sults may therefore exhibit self-penetrations. While Mesh-
GraphNets [40] can model cloth self-collisions, the method
learns this behavior from ground-truth data generated from
an offline simulation method. Since our model is trained
in a fully self-supervised way, interaction patterns between
remote garment nodes are not known in advance. Na¨ıvely
including large sets of collision edges to handle potentially
colliding garment nodes would be prohibitively expensive.
Devising methods to deal with garment self-collisions in a
self-supervised fashion is an exciting direction for future
work. Second, while our model learns the effects of body-
garment interactions well, it may fail when body motions
exceed the velocities seen at training time. One promising
strategy to address this would be to introduce continuous
collision detection into our approach. Finally, when sim-
ulating garments on bodies with severe self-intersections,
our approach may use the wrong body-garment correspon-
dences and produce erroneous results. While the automatic
resolution of body self-penetrations is an interesting prob-
lem even for physics-based simulation, it is beyond the
scope of this work.
Acknowledgements Artur Grigorev was supported by the
Max Planck ETH Center for Learning Systems. We thank
Xi Wang, Christoph Gebhardt, Velko Vechev, Yao Feng,
and Tobias Pfaff for their feedback and help during the
project.
Disclosure MJB has received research gift funds from
Adobe, Intel, Nvidia, Meta/Facebook, and Amazon. MJB
has financial interests in Amazon, Datagen Technologies,
and Meshcapade GmbH. MJB’s research was performed
solely at, and funded solely by, the Max Planck.
References
[1] Alakh Aggarwal, Jikai Wang, Steven Hogue, Saifeng Ni,
Madhukar Budagavi, and Xiaohu Guo. Layered-garment net:
Generating multiple implicit garment layers from a single
image. In ACCV, pages 3000–3017, 2022. 3
[2] David Baraff and Andrew Witkin.
Large steps in cloth
simulation. In Proceedings of the 25th Annual Conference
on Computer Graphics and Interactive Techniques, SIG-
GRAPH ’98, page 43–54, New York, NY, USA, 1998. Asso-
ciation for Computing Machinery. 2
[3] David Baraff and Andrew Witkin. Large steps in cloth sim-
ulation. In Proceedings of the 25th annual conference on
Computer graphics and interactive techniques, pages 43–54,
1998. 2
[4] Hugo Bertiche, Meysam Madadi, and Sergio Escalera.
Cloth3d: clothed 3d humans. In European Conference on
Computer Vision, pages 344–359. Springer, 2020. 3
[5] Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Pbns:
Physically based neural simulator for unsupervised garment
pose space deformation. arXiv preprint arXiv:2012.11310,
2020. 3
[6] Hugo Bertiche, Meysam Madadi, Emilio Tylson, and Ser-
gio Escalera. Deepsd: Automatic deep skinning and pose
space deformation for 3d garment animation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 5471–5480, 2021. 3
[7] Kiran S Bhat, Christopher D Twigg, Jessica K Hodgins,
Pradeep Khosla, Zoran Popovic, and Steven M Seitz. Es-
timating cloth simulation parameters from video. 2003. 2
[8] Robert Bridson, Ronald Fedkiw, and John Anderson. Robust
treatment of collisions, contact and friction for cloth anima-
tion. In Proceedings of the 29th annual conference on Com-
puter graphics and interactive techniques, pages 594–603,
2002. 2
[9] George E. Brown, Matthew Overby, Zahra Forootaninia, and
Rahul Narain. Accurate dissipative forces in optimization
integrators. ACM Trans. Graph., 37(6), dec 2018. 5
[10] Thomas Buffet, Damien Rohmer, Lo¨ıc Barthe, Laurence
Boissieux, and Marie-Paule Cani.
Implicit untangling: A
robust solution for modeling layered clothing. ACM Trans.
Graph., 38(4), jul 2019. 3
[11] Yadi Cao, Menglei Chai, Minchen Li, and Chenfanfu Jiang.
Bi-stride multi-scale graph neural network for mesh-based
physical simulation. arXiv preprint arXiv:2210.02573, 2022.
3, 5
[12] Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges,
and Andreas Geiger. SNARF: Differentiable forward skin-
ning for animating non-rigid neural implicit shapes. In Proc.
International Conference on Computer Vision (ICCV), pages
11574–11584, Piscataway, NJ, Oct. 2021. IEEE. 3
[13] Kwang-Jin Choi and Hyeong-Seok Ko. Stable but responsive
cloth. In ACM SIGGRAPH 2005 Courses, pages 1–es. 2005.
2
[14] Gabriel Cirio, Jorge Lopez-Moreno, David Miraut, and
Miguel A. Otaduy. Yarn-level simulation of woven cloth.
ACM Trans. Graph., 33(6), nov 2014. 2
[15] Enric Corona, Albert Pumarola, Guillem Aleny`a, Ger-
ard Pons-Moll, and Francesc Moreno-Noguer.
SMPLicit:
Topology-aware generative model for clothed people.
In
CVPR, 2021. 3
[16] Meire Fortunato, Tobias Pfaff, Peter Wirnsberger, Alexan-
der Pritzel, and Peter Battaglia. MultiScale MeshGraphNets.
arXiv preprint arXiv:2210.00612, 2022. 3
[17] Hongyang Gao and Shuiwang Ji. Graph u-nets. In inter-
national conference on machine learning, pages 2083–2092.
PMLR, 2019. 3
[18] Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz
B¨acher, Bernhard Thomaszewski, and Stelian Coros. Add:
Analytically differentiable dynamics for multi-body systems
with frictional contact. ACM Trans. Graph., 39(6), nov 2020.
5
9

[19] Eitan Grinspun, Anil N. Hirani, Mathieu Desbrun, and Peter
Schr¨oder. Discrete shells. In Proceedings of the 2003 ACM
SIGGRAPH/Eurographics Symposium on Computer Anima-
tion, SCA ’03, page 62–67, Goslar, DEU, 2003. Eurograph-
ics Association. 2, 5
[20] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander
Weiss, and Michael J Black. Drape: Dressing any person.
ACM Transactions on Graphics (ToG), 31(4):1–10, 2012. 2,
3
[21] David Harmon, Etienne Vouga, Breannan Smith, Rasmus
Tamstorf, and Eitan Grinspun. Asynchronous contact me-
chanics. New York, NY, USA, 2009. ACM. 2
[22] Minchen Li, Danny M. Kaufman, and Chenfanfu Jiang.
Codimensional incremental potential contact. ACM Trans.
Graph., 40(4), jul 2021. 2
[23] Minchen Li, Alla Sheffer, Eitan Grinspun, and Nicholas Vin-
ing. Foldsketch: Enriching garments with physically repro-
ducible folds. ACM Transaction on Graphics, 37(4), 2018.
8
[24] Ren Li, Benoˆıt Guillard, Edoardo Remelli, and Pascal Fua.
Dig: Draping implicit garment over the human body. arXiv
preprint arXiv:2209.10845, 2022. 2
[25] Yifei Li, Tao Du, Kui Wu, Jie Xu, and Wojciech Matusik.
Diffcloth: Differentiable cloth simulation with dry frictional
contact. ACM Trans. Graph., 42(1), oct 2022. 2
[26] Junbang Liang, Ming C. Lin, and Vladlen Koltun. Differen-
tiable Cloth Simulation for Inverse Problems. Curran Asso-
ciates Inc., Red Hook, NY, USA, 2019. 2
[27] Siyou Lin, Hongwen Zhang, Zerong Zheng, Ruizhi Shao,
and Yebin Liu. Learning implicit templates for point-based
clothed human modeling. In ECCV, 2022. 3
[28] Mario Lino, Chris Cantwell, Anil A Bharath, and Stathi Fo-
tiadis.
Simulating continuum mechanics with multi-scale
graph neural networks.
arXiv preprint arXiv:2106.04900,
2021. 3
[29] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
ard Pons-Moll, and Michael J. Black.
SMPL: A skinned
multi-person linear model.
ACM Trans. Graphics (Proc.
SIGGRAPH Asia), 34(6):248:1–248:16, Oct. 2015. 6
[30] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,
Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-
ing to dress 3d people in generative clothing. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 6469–6478, 2020. 3
[31] Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black.
The power of points for modeling humans in clothing. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 10974–10984, Oct. 2021. 3
[32] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black.
Amass:
Archive
of motion capture as surface shapes.
In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 5442–5451, 2019. 6
[33] Sebastian Martin, Bernhard Thomaszewski, Eitan Grinspun,
and Markus Gross. Example-based elastic materials. ACM
Trans. Graph., 30(4), jul 2011. 2, 5, 8
[34] E. Miguel, D. Bradley, B. Thomaszewski, B. Bickel, W. Ma-
tusik, M. A. Otaduy, and S. Marschner. Data-driven estima-
tion of cloth simulation models. Computer Graphics Forum,
31(2pt2):519–528, 2012. 2
[35] Juan Montes, Bernhard Thomaszewski, Sudhir P. Mudur,
and Tiberiu Popa. Computational design of skintight cloth-
ing. ACM Trans. Graph., 39(4):105, 2020. 5
[36] Rahul Narain, Armin Samii, and James F. O’Brien. Adap-
tive anisotropic remeshing for cloth simulation. ACM Trans.
Graph., 31(6), nov 2012. 2, 5
[37] Rahul Narain, Armin Samii, and James F O’brien. Adaptive
anisotropic remeshing for cloth simulation. ACM transac-
tions on graphics (TOG), 31(6):1–10, 2012. 7
[38] Xiaoyu Pan, Jiaming Mai, Xinwei Jiang, Dongxue Tang,
Jingxiang Li, Tianjia Shao, Kun Zhou, Xiaogang Jin, and
Dinesh Manocha.
Predicting loose-fitting garment defor-
mations using bone-driven motion networks. In ACM SIG-
GRAPH 2022 Conference Proceedings, pages 1–10, 2022. 2,
3
[39] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-
Moll. Tailornet: Predicting clothing in 3d as a function of
human pose, shape and garment style.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7365–7375, 2020. 3
[40] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,
and Peter W Battaglia. Learning mesh-based simulation with
graph networks. arXiv preprint arXiv:2010.03409, 2020. 2,
3, 6, 7, 9
[41] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff,
Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to
simulate complex physics with graph networks. In Interna-
tional Conference on Machine Learning, pages 8459–8468.
PMLR, 2020. 2, 3
[42] Igor Santesteban, Miguel A Otaduy, and Dan Casas.
Learning-based animation of clothing for virtual try-on.
Computer Graphics Forum, 38(2):355–366, 2019. 3
[43] Igor Santesteban, Miguel A Otaduy, and Dan Casas. SNUG:
Self-supervised neural dynamic garments. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8140–8150, 2022. 2, 3, 5, 6, 7, 8
[44] Igor Santesteban, Miguel A. Otaduy, Nils Thuerey, and Dan
Casas. ULNeF: Untangled layered neural fields for mix-and-
match virtual try-on. In Advances in Neural Information Pro-
cessing Systems, (NeurIPS), 2022. 3
[45] Igor Santesteban, Nils Thuerey, Miguel A. Otaduy, and Dan
Casas.
Self-supervised collision handling via generative
3d garment models for virtual try-on.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 11763–11773, June 2021. 2, 3,
5, 6, 7
[46] Zhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. Deep-
cloth: Neural garment representation for shape and style
editing. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2022. 3
[47] Min Tang, Tongtong Wang, Zhongyuan Liu, Ruofeng Tong,
and Dinesh Manocha.
I-cloth: Incremental collision han-
dling for gpu-based interactive cloth simulation. ACM Trans-
actions on Graphics (TOG), 37(6):1–10, 2018. 2
10

[48] Demetri Terzopoulos, John Platt, Alan Barr, and Kurt Fleis-
cher. Elastically deformable models. In Proceedings of the
14th Annual Conference on Computer Graphics and Interac-
tive Techniques, SIGGRAPH ’87, page 205–214, New York,
NY, USA, 1987. Association for Computing Machinery. 2
[49] B. Thomaszewski, Simon Pabst, and Wolfgang Straßer.
Asynchronous cloth simulation. 2008. 2
[50] Pascal Volino, Nadia Magnenat-Thalmann, and Francois
Faure. A simple approach to nonlinear tensile stiffness for
accurate cloth simulation. ACM Trans. Graph., 28(4), sep
2009. 2
[51] Huamin Wang, James F. O’Brien, and Ravi Ramamoorthi.
Data-driven elastic models for cloth: Modeling and measure-
ment. In ACM SIGGRAPH 2011 Papers, SIGGRAPH ’11,
New York, NY, USA, 2011. Association for Computing Ma-
chinery. 2
[52] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor
Lempitsky.
Point-based modeling of human clothing.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 14718–14727, 2021. 3
[53] Meng Zhang, Duygu Ceylan, and Niloy J Mitra.
Mo-
tion guided deep dynamic 3d garments.
arXiv preprint
arXiv:2209.11449, 2022. 2
11

