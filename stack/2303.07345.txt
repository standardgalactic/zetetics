Erasing Concepts from Diffusion Models
Rohit Gandikota∗,1
Joanna Materzy´nska∗,2
Jaden Fiotto-Kaufman1
David Bau1
1Northeastern University
2Massachusetts Institute of Technology
1[gandikota.ro, fiotto-kaufman.j, davidbau]@northeastern.edu
2jomat@mit.edu
*
Erasing Nudity
Original Model
Edited Model
Erased from model: 
“Nudity”
*
*
*
*
*
Erasing Artistic Style
Erasing Objects
Original Model
Edited Model
Erased from model: 
“Car”
Original Model
Edited Model
Erased from model: 
“Van Gogh”
*
Added by authors 
for publication
Figure 1: Given only a short text description of an undesired visual concept and no additional data, our method ﬁne-tunes
model weights to erase the targeted concept. Our method can avoid NSFW content, stop imitation of a speciﬁc artist’s style, or
even erase a whole object class from model output, while preserving the model’s behavior and capabilities on other topics.
Abstract
Motivated by concerns that large-scale diffusion models
can produce undesirable output such as sexually explicit
content or copyrighted artistic styles, we study erasure of
speciﬁc concepts from diffusion model weights. We propose
a ﬁne-tuning method that can erase a visual concept from a
pre-trained diffusion model, given only the name of the style
and using negative guidance as a teacher. We benchmark our
method against previous approaches that remove sexually ex-
plicit content and demonstrate its effectiveness, performing
on par with Safe Latent Diffusion and censored training. To
evaluate artistic style removal, we conduct experiments eras-
ing ﬁve modern artists from the network and conduct a user
study to assess the human perception of the removed styles.
Unlike previous methods, our approach can remove concepts
from a diffusion model permanently rather than modifying
the output at the inference time, so it cannot be circumvented
even if a user has access to model weights. Our code, data,
and results are available at erasing.baulab.info.
*Equal contribution
1. Introduction
Recent text-to-image generative models have attracted at-
tention due to their remarkable image quality and seemingly
inﬁnite generation capabilities. These models are trained
on vast internet datasets, which enables them to imitate a
wide range of concepts. However, some concepts learned
by the model are undesirable, including copyrighted content
and pornography, which we aim to avoid in the model’s out-
put [24, 15, 26]. In this paper, we propose an approach for
selectively removing a single concept from a text-conditional
model’s weights after pretraining. Prior approaches have
focused on dataset ﬁltering [27], post-generation ﬁltering
[26], or inference guiding [35]. Unlike data ﬁltering meth-
ods, our method does not require retraining, which is pro-
hibitive for large models. Inference-based methods can cen-
sor [26] or steer the output away from undesired concepts
effectively [35], but they can be easily circumvented. In
contrast, our approach directly removes the concept from the
model’s parameters, making it safe to distribute its weights.
The open-source release of the Stable Diffusion text-to-
arXiv:2303.07345v1  [cs.CV]  13 Mar 2023

image diffusion model has made image generation technol-
ogy accessible to a broad audience. To limit the generation
of unsafe images, the ﬁrst version was bundled with a simple
NSFW ﬁlter to censor images if the ﬁlter is triggered [26],
yet since both the code and model weights are publicly avail-
able, it is easy to disable the ﬁlter [39]. In an effort to prevent
the generation of sensitive content, the subsequent SD 2.0
model is trained on data ﬁltered to remove explicit images,
an experiment consuming 150,000 GPU-hours of computa-
tion [29] over the 5-billion-image LAION dataset [36]. The
high cost of the process makes it challenging to establish a
causal connection between speciﬁc changes in the data and
the capabilities that emerge, but users report that removing
explicit images and other subjects from the training data
may have had a negative impact on the output quality [27].
And despite the effort, explicit content remains prevalent in
the model’s output: when we evaluate generation of images
using prompts from the 4,703 prompts of the Inappropriate
Image Prompts (I2P) benchmark [35], we ﬁnd that the popu-
lar SD 1.4 model produces 796 images with exposed body
parts identiﬁed by a nudity detector, while the new training-
set-restricted SD 2.0 model produces 417 (Figure 7).
Another major concern regarding the text-to-image mod-
els is their ability to imitate potentially copyrighted content.
Not only is the quality of the AI-generated art on par with
the human-generated art [31], it can also faithfully replicate
an artistic style of real artists. Users of Stable Diffusion [28]
and other large-scale text-to-image synthesis systems have
discovered that prompts such as “art in the style of [artist]”
can mimic styles of speciﬁc artists, potentially devaluing
original work. The concerns of several artists led to a legal
case against the makers of Stable Diffusion for allegedly in-
fringing on their work [1]. Recent work [38] aims to protect
the artist by applying an adversarial perturbation to artwork
before posting it online to prevent the model from imitating
it. That approach, however, cannot remove a learned artistic
style from a pretrained model.
In response to safety and copyright infringement con-
cerns, we propose a method for erasing a concept from a
text-to-image model. Our method, Erased Stable Diffusion
(ESD), ﬁne-tunes the model’s parameters using only unde-
sired concept descriptions and no additional training data.
Unlike training-set censorship approaches, our method is
fast and does not require training the whole system from
scratch. Furthermore, our method can be applied to existing
models without the need to modify input images [38]. Un-
like the post-ﬁltering [26] or simple blacklisting methods,
erasure cannot be easily circumvented, even by users who
have access to the parameters. We benchmark our method
on removing offensive content and ﬁnd that it is as effective
as Safe Latent Diffusion [35] for removing offensive images.
We also test the ability of our method to remove an artistic
style from the model. We conduct a user study to test the
impact of erasure on user perception of the remove artist’s
style in output images, as well as the interference with other
artistic styles and their impact on image quality. Finally, we
also test our method on erasure of complete object classes.
2. Related Works
Undesirable image removal. Previous work to avoid
undesirable image output in generative models has taken
two main approaches: The ﬁrst is to censor images from the
training set, for example, by removing all people [22], or by
more narrowly curating data to exclude undesirable classes
of images [36, 24, 30]. Dataset removal has the disadvantage
that the resources required to retrain large models makes it
a very costly way to respond to problems discovered after
training; also large-scale censorship can produce unintended
effects [23]. The second approach is post-hoc, modifying
output after training using classiﬁers [3, 19, 26], or by adding
guidance to the inference process [35]; such methods are
efﬁcient to test and deploy, but they are easily circumvented
by a user with access to parameters [39]. We compare both
previous approaches including Stable Diffusion 2.0 [27],
which is a complete retraining of the model on a censored
training set, and Safe Latent Diffusion [35], which the state-
of-the-art guidance-based approach. The focus of our current
work is to introduce a third approach: we tune the model
parameters using a guidance-based model-editing method,
which is both fast to employ and also difﬁcult to circumvent.
Image cloaking. Another approach to protecting images
from imitation by large models is for an artist to cloak im-
ages by adding adversarial perturbations before posting them
on the internet. Cloaking allows artists to effectively hide
their work from a machine-learned model during training or
inference by adding perturbations that cause the model to
confuse the cloaked image with an unrelated image [33] or
an image with a different artistic style [38]; the method is a
promising way for an artist to self-censor their own content
from AI training sets while still making their work visible
to humans. Our paper addresses a different problem than
the problem addressed by cloaking: we ask how a model
creator can erase an undesired visual concept without active
self-censorship by content providers.
Model editing. As the cost of training grows, there has
been increasing interest in lightweight model-editing meth-
ods that alter the behavior of large-scale generative models
given little or no new training data. In text generators, a
model’s knowledge of facts can be edited based on a single
statement of the fact by modifying speciﬁc neurons [7] or
layers [20], or by using hypernetworks [8, 21]. In image
synthesis, a generative adversarial network (GAN) can be
edited using a handful of words [13], a few sketches [43],
warping gestures [44], or copy-and-paste [2]. Recently, it
has been shown that text-conditional diffusion models can be
edited by associating a token for a new subject trained using

ϵθ(xt, c, t)
ϵθ*(xt, t)
ϵθ*(xt, c, t)
Frozen 
Original SD
θ*
ϵθ*(xt, t) −η[ϵθ*(xt, c, t) −ϵθ*(xt, t)]
(xt, c, t)
(xt, c0, t)
(xt, c, t)
Fine Tune 
ESD
θ
“Van Gogh”, concept to erase
xt
c
“ ”
c0
L2 
 Loss
, generated by θ
Time step sampled uniformly
t
Figure 2: The optimization process for erasing undesired visual concepts from pre-trained diffusion model weights involves
using a short text description of the concept as guidance. The ESD model is ﬁne-tuned with the conditioned and unconditioned
scores obtained from frozen SD model to guide the output away from the concept being erased. The model learns from it’s
own knowledge to steer the diffusion process away from the undesired concept.
only a handful of images [32]. Unlike previous methods
that add or modify the appearance of objects, the goal of
our current work is to erase a targeted visual concept from
a diffusion model given only a single textual description of
the concept, object, or style to be removed.
Memorization and unlearning. While the traditional
goal of machine learning is to generalize without memo-
rization, large models are capable of exact memorization if
speciﬁcally trained to do so [45], and unintentional memo-
rization has also been observed in large-scale settings [6, 5],
including diffusion models [41]. The possibility of such ex-
act memorization has driven privacy and copyright concerns
and has led to work in machine unlearning [37, 4, 14], which
aims to modify a model to behave as if particular training
data had not been present. However, these methods are based
on the assumption that the undesired knowledge corresponds
to an identiﬁable set of training data points. The problem we
tackle in this paper is very different from the problem of un-
learning speciﬁc training data because rather than simulating
the removal of a known training item, our goal is to erase a
high-level visual concept that may have been learned from a
large and unknown subset of the training data, such as the
appearance of nudity, or the imitation of an artist’s style.
Energy-based composition. Our work is inspired by
the observation [10, 11] that set-like composition can be
performed naturally on energy-based models and diffusion
counterparts naturally via arithmetic on the score or the
noise predictions. Score-based composition is also the basis
for classiﬁer-free-guidance [17]. Like previous works, we
treat “A and not B” as the difference between log probability
densities for A and B; a similar observation has been used to
reduce the undesirable output of both language models [34]
and vision generators [35]. Unlike previous work that applies
composition at inference time, we introduce the use of score
composition as a source of unsupervised training data to
teach a ﬁne-tuned model to erase an undesired concept from
model weights.
3. Background
3.1. Denoising Diffusion Models
Diffusion models are a class of generative models that
learn the distribution space as a gradual denoising pro-
cess [40, 16]. Starting from sampled Gaussian noise, the
model gradually denoises for T time steps until a ﬁnal image
is formed. In practice, the diffusion model predicts noise
ϵt at each time step t that is used to generate the intermedi-
ate denoised image xt; where xT corresponds to the initial
noise and x0 corresponds to the ﬁnal image. This denoising
process is modeled as a Markov transition probability.
pθ(xT :0) = p(xT )
1
Y
t=T
pθ(xt−1|xt)
(1)
3.2. Latent Diffusion Models
Latent diffusion models (LDM) [28] improve efﬁciency
by operating in a lower dimensional latent space z of a pre-
trained variational autoencoder with encoder E and decoder
D. During training, for an image x, noise is added to its
encoded latent, z = E(x) leading to zt where the noise level
increases with t. LDM process can be interpreted as a se-
quence of denoising models with identical parameters θ that
learn to predict the noise ϵθ(zt, c, t) added to zt conditioned
on the timestep t as well as a text condition c. The following
objective function is optimized:
L = Ezt∈E(x),t,c,ϵ∼N(0,1)[∥ϵ −ϵθ(zt, c, t)∥2
2]
(2)
Classiﬁer-free guidance is a technique employed to regu-
late image generation, as described in Ho et al. [17]. This
method involves redirecting the probability distribution to-
wards data that is highly probable according to an implicit
classiﬁer p(c|zt). This approach is used during inference
and requires that the model be jointly trained on both con-
ditional and unconditional denoising. The conditional and

unconditional scores are both obtained from the model dur-
ing inference. The ﬁnal score ˜ϵθ(zt, c, t) is then directed
towards the conditioned score and away from the uncondi-
tioned score by utilizing a guidance scale α > 1.
˜ϵθ(zt, c, t) = ϵθ(zt, t) + α(ϵθ(zt, c, t) −ϵθ(zt, t))
(3)
The inference process starts from a Gaussian noise zT ∼
N(0, 1) and is denoised with the ˜ϵθ(zT , c, T) to get zT −1.
This process is done sequentially till z0 and is transformed
to image space using the decoder x0 ←D(z0).
4. Method
The goal of our method is to erase concepts from text-
to-image diffusion models using its own knowledge and no
additional data. Therefore, we consider ﬁne-tuning a pre-
trained model rather than training a model from scratch. We
focus on Stable Diffusion (SD) [28], an LDM that consists of
3 subnetworks: a text encoder T , a diffusion model (U-Net)
θ and a decoder model D.
Our approach involves editing the pre-trained diffusion
U-Net model weights θ to remove a speciﬁc style or con-
cept. We draw inspiration from the classiﬁer-free guidance
method [17] and score-based composition [10]. Speciﬁcally,
we apply principles of classiﬁer-free guidance to train the
diffusion model, steering the model’s score away from a spe-
ciﬁc concept c that we aim to erase, such as the phrase “Van
Gogh.” We shall take advantage of the pretrained model’s
awareness of the concept and learn to shift the ﬁne-tuned
output distribution mass away from it.
From the score-based formulation of diffusion model,
the objective is to learn the score of conditional model
∇log pθ(xt|c) [16]. Using Bayes rule and ∇log pθ∗(c) = 0
we arrive at:
∇log pθ(xt|c) ←∇log pθ∗(xt) + ∇log pθ∗(c|xt)
(4)
This can be interpreted as unconditional score with gradient
from a classiﬁer pθ(c|xt). To control the effect of condi-
tionality, a guidance factor η is introduced for the classiﬁer
gradient [42]
∇log pθ(xt|c) ←∇log pθ∗(xt) + η(∇log pθ∗(c|xt)) (5)
We wish to negate concept c by inverting the behavior of
θ∗, and therefore we use the negative version of guidance to
train θ. Additionally, taking inspiration from classiﬁer-free
guidance [17], we transform the RHS of Equation 5 from
classiﬁer to conditional diffusion.
∇log pθ∗(xt) −η(∇log pθ∗(xt|c) −∇log pθ∗(xt)) (6)
Based on Tweedie’s formula [12] and the reparametriza-
tion trick proposed in [16], the gradient of log probability
“car”  
Prompt
(a)
Cross-attention
(b)
Self-attention
(c)
Generated image
“ ”  
Figure 3: When comparing generation of two similar car
images conditioned on different prompts, self-attention (b)
contributes to the features of a car regardless of the pres-
ence of the word “car” in the prompt, while the contribution
of cross-attention (a) is linked to the presence of the word.
Heatmaps show local contributions of the ﬁrst attention mod-
ules of the 3rd upsampling block of the Stable Diffusion
U-net while generating the images (c).
score can be expressed as a function of score scaled by time-
varying parameters. This modiﬁed score function moves the
data distribution to maximize the log probability score.
ϵθ(xt, c, t) ←ϵθ∗(xt, t) −η[ϵθ∗(xt, c, t) −ϵθ∗(xt, t)]
(7)
The objective function in Equation 7 ﬁne-tunes the param-
eters θ such that ϵθ(xt, c, t) mimics the negatively guided
noise. That way, after the ﬁne tuning, the edited model’s con-
ditional prediction is guided away from the erased concept.
Figure 2 illustrates our training process. We exploit the
model’s knowledge of the concept to synthesize training
samples, thereby eliminating the need for data collection.
Training uses several instances of the diffusion model, with
one set of parameters frozen (θ∗) while training the other set
of parameters (θ) to erase the concept. We sample partially
denoised images xt conditioned on c using θ, then we per-
form inference on the frozen model θ∗twice to predict the
noise, once conditioned on c and the other unconditioned.
Finally, we combine these two predictions linearly to negate
the predicted noise associated with the concept, and we tune
the new model towards that new objective.
4.1. Importance of Parameter Choice
The effect of applying the erasure objective (7) depends
on the subset of parameters that is ﬁne-tuned. The main
distinction is between cross-attention parameters and non-
cross-attention parameters. Cross-attention parameters, illus-
trated in Figure 3a, serve as a gateway to the prompt, directly
depending on the text of the prompt, while other parameters
(Figure 3b) tend to contribute to a visual concept even if the
concept is not mentioned in the prompt.
Therefore we propose ﬁne tuning the cross attentions,
ESD-x, when the erasure is required to be controlled and

Original SD
All Layers
ESD-u
Self Attentions
ESD-x
Erasure of artist 
(More change is better)
Inference with other artists  
(Less change is better)
The Great Wave 
of Kanagawa by 
Hokusai 
Girl with a pearl 
earring by 
Johannes Vermeer
The scream by 
Edvard 
Munch
The Bedroom 
in Arles by 
Van Gogh
The Starry 
Night by 
Van Gogh
Figure 4: Modifying the cross-attention weights, ESD-x,
shows negligible interference with other styles (bottom 3
rows) and is thus well-suited for erasing art styles. In con-
trast, altering the non-cross-attention weights, ESD-u, has a
global erasure effect (all rows) on the visual concept and is
better suited for removing nudity or objects.
speciﬁc to the prompt, such as when a named artistic style
should be erased. Further, we propose ﬁne tuning uncondi-
tional layers (non-cross-attention modules), ESD-u, when
the erasure is required to be independent of the text in the
prompt, such as when the global concept of NSFW nudity
should be erased. We refer to cross-attention-only ﬁne-
tuning as ESD-x-η (where η refers to the strength of the
negative guidance), and we refer to the conﬁguration that
tunes only non-cross-attention parameters as ESD-u-η. For
simplicity, we write ESD-x and ESD-u when η = 1.
The effects of parameter choices on artist style removal
are illustrated in Figure 4: when erasing the “Van Gogh”
style ESD-u and other unconditioned parameter choices
erase aspects of the style globally, erasing aspects of Van
Gogh’s style from many artistic styles other than Van Gogh’s.
On the other hand, tuning the the cross-attention parameters
only (ESD-x) erases the distinctive style of Van Gogh specif-
ically when his name is mentioned in the prompt, keeping
the interference with other artistic styles to a minimum.
Conversely, when removing NSFW content it is important
that the visual concept of “nudity” is removed globally, espe-
cially in cases when nudity is not mentioned in the prompt.
To measure those effects we evaluate on a data set that in-
clude many prompts that do not explicitly mention NSFW
terms (Section 5.2). We ﬁnd that ESD-u performs best in
this application; full quantitative ablations over different
parameter sets are included in supplemental materials.
5. Experiments
We train all our models for 1000 gradient update steps
on a batch size of 1 with learning rate 1e-5 using the Adam
optimizer. Depending on the concept we want to remove
(4.1), the ESD-x method ﬁne-tunes the cross-attention and
ESD-u ﬁne tunes the unconditional weights of the U-Net
module in Stable Diffusion (our experiments use version 1.4
unless speciﬁed otherwise). Baseline methods are:
• SD (pretrained Stable Diffusion),
• SLD (Safe Latent Diffusion) [35], to adapt the method
to our experiment, we substitute the concept we want to
erase from the model for the original safety concepts.
• SD-Neg-Prompt (Stable Diffusion with Negative
Prompts), an inference technique in the community,
that aims to steer away from unwanted effects in an
image. We adapt this method by using the artist’s name
as the negative prompt.
5.1. Artistic Style Removal
5.1.1
Experiment Setup
To analyze imitation of art among contemporary practicing
artists, we consider 5 modern artists and artistic topics; Kelly
McKernan, Thomas Kinkade, Tyler Edlin, Kilian Eng and
the series “Ajin: Demi-Human,” which have been reported
to be imitated by Stable Diffusion. While we did not observe
the model making direct copies of speciﬁc original artwork,
it is undeniable that these artistic styles have been captured
by the model. To study this effect, we demonstrate quali-
tative results in Fig 5 and conduct a user study to measure
the human perception on the artistic removal effect. Our
experiments validate the observation that the particular artist-
speciﬁc style is removed from the model, while the content
and structure of the prompt is preserved (Fig 5) with min-
imal interference on other artistic styles. For more image
examples, please refer to Supplemental materials.
5.1.2
Artistic Style Removal User Study
To measure the human perception of the effectiveness of the
removed style, we conducted a user study. For each artist,
we collect 40 images of art created by those artists, using
Google Image Search to identify top-ranked works. Then
for each artist, we also compose 40 generic text prompts
that invoke the artist’s style, and we use Stable Diffusion
to generate images for each artist using these prompts, for
example: “Art by [artist]”, “A design of [artist]”, “An image
in the style of [artist]”, “A reproduction of the famous art
of [artist]”. We also evaluate images from edited diffusion
models, as described in Section 5.1.1 as well as the baseline
models. The images were generated with 4 seeds per prompt
(same seeds were used for all methods) resulting in a dataset
of 1000 images. Furthermore, we also included images from

Original SD
Our Method
Erasing 
“Thomas Kinkade”
Erasing 
“Kilian Eng”
Erasing 
“Kelly McKernan”
Safe Concept 
“Thomas Kinkade”
Safe Concept 
“Kilian Eng”
Safe Concept 
“Kelly McKernan”
SLD
Thomas Kinkade 
inspired depiction 
of a peaceful park
Post-apocalyptic 
landscape by 
Kilian Eng
Whimsical creatures 
with floral elements 
by Kelly McKernan
Prompt Conditioning
Figure 5: Our method has a better erasure on intended style with a minimal interference compared to SLD [35]. The images
enclosed in blue dotted borders are the intended erasure, and the off-diagonal images show effect on untargeted styles.
(a) 
Baselines
(b) 
Erasing targeted style
(lower is better)
(c)
Preserving other styles
(higher is better)
SD
Similar 
Artist
Real 
Artist
ESD-x 
(Ours)
SLD
SD-Neg
Prompt
ESD-x 
(Ours)
SLD
SD-Neg
Prompt
Avg. Rating
2.5
3.5
3.0
4.0
2.0
1.5
1.0
Figure 6: User study ratings show that our method erases
the intended style better than the baselines. The rating (1-5)
represent the similarity of the images compared to original
artist style (5 being most similar). With higher ratings for
images from similar style artists, the study shows that style
is highly subjective.
a similar human artist for each of the ﬁve artists. We pair
real artist with similar real artist as follows: (Kelly McKer-
nan, Kirbi Fagan), (Thomas Kinkade, Nicky Boehme), (Ajin:
Demi Human, Tokyo Ghoul), (Tyler Edlin, Feng Zhu), (Kil-
ian Eng, Jean Giraud). For each similar artist, we collected
12-25 works to use in our study.
In our study, participants were presented with a set of ﬁve
real artwork images along with an additional image. The
additional image was either a real artwork from the same
artist or a similar artist, or a synthetic image generated using
a prompt involving the artist name with our method (ESD-
x) or other baseline methods (SLD and SD-Neg-Prompt)
applied to remove the artist or a random different artist.
Participants were asked to estimate, on a ﬁve-point Likert
scale, their conﬁdence level that the experimental image was
created by the same artist as the ﬁve real artworks.
Our study involved 13 total participants, with an average
of 170 responses per participant. We evaluated the effec-
tiveness of the ESD-x method for removing the style of ﬁve
modern artists and measuring the resemblance of generated
artistic images to real images. Additionally, we assessed the
amount of interference introduced by our method in com-
parison to other baseline methods, measuring the extent to
which other artistic styles were affected.
The ﬁndings of our investigation are presented in Figure
6. Interestingly, even for the genuine artwork, there is some
level of uncertainty about its authenticity. The average rat-
ing for the original images is 3.85, the average rating for
artists similar to the selected artists is 3.16, while the average
rating for the AI-generated art is 3.21, indicating that the
AI-duplicates are rated higher than similar genuine artwork.
These outcomes reinforce our observations that the model
effectively captures the style of an artist. All three removal
techniques effectively decrease the perceived artistic style,
with average ratings of 1.12, 2.00, and 2.22 for ESD-x, SLD
and SD-Neg-Prompt respectively.
In addition to assessing the removal of an artistic style,
we are also interested in evaluating the interference of our

Figure 7: Our method effectively removes nudity content from Stable Diffusion on I2P data, outperforming inference method,
SLD [35] and model trained on NSFW ﬁltered dataset, SD-V2.0 [27]. (SD-V2.1, also shown, ﬁlters less aggressively.) The
ﬁgure shows the percentage change in the nudity-classiﬁed samples compared to the original SD-V1.4 model. The SD v1.4
produces 796 images with exposed body parts on the test prompts, and our method reduces this total to 134.
Method
FID-30k
CLIP
REAL
-
0.1561
SD
14.50
0.1592
SLD-Medium1
16.90
0.1594
SLD-Max
18.76
0.1583
ESD-u
13.68
0.1585
ESD-u-3
17.27
0.1586
Table 1: Our method shows better image ﬁdelity perfor-
mance compared to SLD and Stable Diffusion on COCO
30k images generated with inference guidance α = 7.5. All
the methods show good CLIP score consistency with SD.
method with different artistic styles. To accomplish this,
we present our research participants with images produced
using a text prompt referring to an artist that was not erased
in a model generated with an erased artist. We compare
our method to SLD and SD-Neg-Prompt in this experiment.
The outcomes, presented in Figure 5, indicate that users are
most likely to consider images generated using our method
to be genuine artwork, as opposed to those generated using
other removal techniques, indicating that our method does
not interfere with other artistic styles as a result of removing
an artist. It is worth noting that, unlike the two baselines, our
method modiﬁes the model permanently, rather than being
an inference approach.
Our method can also be applied to erase single works of
art rather than entire artistic styles; we describe and analyze
this variant in Supplementary material.
5.2. Explicit Content Removal
Recent works have addressed the challenge of NSFW con-
tent restriction either through inference modiﬁcation [35],
1These numbers are taken from the SLD paper [35]. In our experiments,
we ﬁnd the FIDs to be 18.71 and 25.29 for Medium and Max respectively
post-production classiﬁcation based restriction [28] or re-
training the entire model with NSFW restricted subset of
LAION dataset [27]. Inference and post-production clas-
siﬁcation based methods can be easily circumvented when
the models are open-sourced [39]. Retraining the models on
ﬁltered data can be very expensive, and we ﬁnd that such
models (Stable Diffusion V2.0) are still capable of generat-
ing nudity, as seen in Figure 7.
Since the erasure of unsafe content like nudity requires
the effect to be global and independent of text embeddings,
we use ESD-u to erase "nudity". In ﬁgure 7, we compare the
percentage change in nudity classiﬁed samples with respect
to Stable Diffusion v1.4. We study the effectiveness of our
method with both inference method (SLD [35]) and ﬁltered
re-training methods (SD V2.0 [27]). For all the models,
4703 images are generated using I2P prompts from [35].
The images are classiﬁed into various nudity classes using
the Nudenet [25] detector. For this analysis, we show results
for our weak erasure scale of η = 1. We ﬁnd that across
all the classes, our method has a more signiﬁcant effect in
erasing nudity2. For a more similar comparison study that
was done by [35], please refer to supplementary materials.
To ensure that the erased model is still effective in generat-
ing safe content, we compare all the methods’ performance
on COCO 30K dataset prompts. We measure the image
ﬁdelity to show the quality and CLIP score to show speci-
ﬁcity of the model to generate conditional images in table 1.
ESD-u refers to soft erasure with η = 1 and ESD-u-3 refers
to stronger erasure with η = 3. Since COCO is a well
curated dataset without nudity, this could be a reason for
our method’s better FID compared to SD. All the methods
have similar CLIP scores as SD showing minimal effect
speciﬁcity.
2We note that Nudenet [25] has a higher false positive rate for male
genitalia class. With manual cross veriﬁcation, we ﬁnd that both our method
and SLD completely erases male genitalia, while SD V2.0 erases by 60%

Class name
Accuracy of
erased class
Accuracy of other
classes
SD
ESD-u
SD
ESD-u
cassette player
15.6
0.60
85.1
64.5
chain saw
66.0
6.0
79.6
68.2
church
73.8
54.2
78.7
71.6
gas pump
75.4
8.6
78.5
66.5
tench
78.4
9.6
78.2
66.6
garbage truck
85.4
10.4
77.4
51.5
English springer
92.5
6.2
76.6
62.6
golf ball
97.4
5.8
76.1
65.6
parachute
98.0
23.8
76.0
65.4
French horn
99.6
0.4
75.8
49.4
Average
78.2
12.6
78.2
63.2
Table 2: Our method can cleanly erase many object concepts
from a model, evidenced here a signiﬁcant drop in classi-
ﬁcation accuracy of the concept while keeping the other
class scores high. We measure the extent to which erasing
an object class from the model affects the scores of other
classes
5.3. Object Removal
In this section, we investigate the extent to which method
can also be used to erase entire object classes from the model.
We prepare ten ESD-u models, each removing one class
name from a subset of the ImageNet classes [9] (we invest-
giate the Imagenette [18] subset which comprises ten easily
identiﬁable classes). To measure the effect of removing both
the targeted and untargeted classes, we generate 500 images
of each class using base Stable Diffusion as well as each of
the ten ﬁne-tuned models using the prompt “an image of a
[class name]”; then we evaluate the results by examining
the top-1 predictions of a pretrained Resnet-50 Imagenet
classiﬁer. Table 2 displays quantitative results, comparing
classiﬁcation accuracy of the erased class in both the origi-
nal Stable Diffusion model and our ESD-u model trained to
eliminate the class. The table also shows the classiﬁcation
accuracy when generating the remaining nine classes. It is
evident that our approach effectively removes the targeted
classes in most cases, although there are some classes such as
“church” that are more difﬁcult to remove. Accuracy of un-
targeted classes remains high, but there is some interference,
for example, removing “French horn” adds noticible distor-
tions to other classes. Images showing the visual effects of
object erasure are included in Supplementary materials.
5.4. Limitations
For both NSFW erasure and artistic style erasure, we ﬁnd
that our method is more effective than baseline approaches
on erasing the targeted visual concept, but when erasing
large concepts such as entire object classes or some par-
ticular styles, our method can impose a trade-off between
Erasure Interference with Unrelated Artistic Style
Original Model
Erasing “Church”
Incomplete Concept Erasure
Original Model
Erasing “Parachute”
Erasing “Rembrandt”
Erasing  
“Van Gogh”
Original Model 
“Van Gogh Art”
Original Model 
“Picasso Art”
Figure 8: Cases of incomplete concept erasures and style
interference with our method. When erasing concepts from
Stable Diffusion, the model, sometimes, tends to erase only
the main elements like crosses in case of church and texture
in case of parachute. Also, with artistic style erasure, our
method occasionally tends to interfere with other styles. This
has been reﬂected in the user study in Section 5.1.2
complete erasure of a visual concept and interference with
other visual concepts. In Figure 8 we illustrate some of the
limitations. We quantify the typical level of interference dur-
ing art erasure in the user study conducted in Section 5.1.2.
When erasing entire object classes, our method will fail on
some classes, erasing only particular distinctive attributes
of concepts (such as crosses from churches and ribs from
parachutes) while leaving the larger concepts unerased. Eras-
ing entire object classes creates some interference to other
classes, which is quantiﬁed in Section 5.3.
6. Conclusion
This paper proposes an approach for eliminating speciﬁc
concepts from text-to-image generation models by editing
the model weights. Unlike traditional methods that require
extensive dataset ﬁltering and system retraining, our ap-
proach does not involve manipulating large datasets or un-
dergoing expensive training. Instead, it is a fast and efﬁcient
method that only requires the name of the concept to be
removed. By removing the concept directly from the model
weights, our method eliminates the need for post-inference
ﬁlters and enables safe distribution of parameters.
We demonstrate the efﬁcacy of our approach in three
different applications. Firstly, we show that our method
can successfully remove explicit content with comparable
results to the Safe Latent Diffusion method. Secondly, we
demonstrate how our approach can be used to remove artistic
styles, and support our ﬁndings with a thorough human study.
Lastly, we illustrate the versatility of our method by applying
it to concrete object classes.

Code
The code and data sets needed to reproduce the results in
this paper are public and available for download at GitHub at
https://github.com/rohitgandikota/erasing and at the project
website https://erasing.baulab.info.
Acknowledgments
Thanks to Antonio Torralba for valuable advice, discus-
sions and support, and thanks to Fern Keniston for organizing
the event where the team developed the work. RG, DB are
supported by grants from Open Philanthropy and Signify.
References
[1] Sarah Andersen. et al v. Stability AI Ltd. et al. Case No.
3:2023cv00201. US District Court for the Northern District
of California. Jan 2023. 2
[2] David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and
Antonio Torralba. Rewriting a deep generative model. In
Proceedings of the European Conference on Computer Vision
(ECCV), 2020. 2
[3] Praneeth Bedapudi. NudeNet: Neural nets for nudity detec-
tion and censoring, 2022. 2
[4] Lucas Bourtoule, Varun Chandrasekaran, Christopher A
Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang,
David Lie, and Nicolas Papernot. Machine unlearning. In
2021 IEEE Symposium on Security and Privacy (SP), pages
141–159. IEEE, 2021. 3
[5] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Kather-
ine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying
memorization across neural language models. In Proceedings
of the International Conference on Learning Representations
(ICLR), 2023. 3
[6] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos,
and Dawn Song. The secret sharer: Evaluating and testing un-
intended memorization in neural networks. In 28th USENIX
Security Symposium (USENIX Security 19), pages 267–284,
2019. 3
[7] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang,
and Furu Wei. Knowledge neurons in pretrained transformers.
In Proceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
pages 8493–8502, 2022. 2
[8] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing fac-
tual knowledge in language models. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language
Processing, pages 6491–6506, 2021. 2
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009. 8
[10] Yilun Du, Shuang Li, and Igor Mordatch. Compositional
visual generation with energy based models. Advances in
Neural Information Processing Systems, 33:6637–6647, 2020.
3, 4
[11] Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and
Igor Mordatch. Unsupervised learning of compositional en-
ergy concepts. Advances in Neural Information Processing
Systems, 34:15608–15620, 2021. 3
[12] Bradley Efron. Tweedie’s formula and selection bias. Journal
of the American Statistical Association, 106(496):1602–1614,
2011. 4
[13] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-
guided domain adaptation of image generators. ACM Trans-
actions on Graphics (TOG), 41(4):1–13, 2022. 2
[14] Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
Eternal sunshine of the spotless net: Selective forgetting in
deep networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9304–
9312, 2020. 3
[15] Google. Imagen, unprecedented photorealism x deep level of
language understanding, 2022. 1
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840–6851, 2020. 3, 4
[17] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4
[18] Jeremy Howard and Sylvain Gugger. Fastai: A layered api
for deep learning. Information, 11(2):108, 2020. 8
[19] Gant Laborde. NSFW detection machine learning model,
2022. 2
[20] Kevin Meng, David Bau, Alex J Andonian, and Yonatan
Belinkov. Locating and editing factual associations in gpt. In
Advances in Neural Information Processing Systems, 2022. 2
[21] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn,
and Christopher D Manning. Fast model editing at scale. In
International Conference on Learning Representations, 2021.
2
[22] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 2
[23] Ryan O’Connor. Stable diffusion 1 vs 2 - what you need to
know, 2022. 2
[24] OpenAI. DALL-E 2 preview - risks and limitations, 2022. 1,
2
[25] Bedapudi Praneeth. Nudenet: Neural nets for nudity classiﬁ-
cation, detection and selective censoring, 12 2019. 7
[26] Javier Rando, Daniel Paleka, David Lindner, Lennard Heim,
and Florian Tramèr. Red-teaming the stable diffusion safety
ﬁlter. arXiv preprint arXiv:2210.04610, 2022. 1, 2
[27] Robin Rombach. Stable diffusion 2.0 release, Nov 2022. 1,
2, 7
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and BjÃ¶rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2022. 2, 3, 4, 7
[29] Robin Rombach and Patrick Esser. Stable diffusion v1-4
model card, 2022. 2

[30] Robin Rombach and Patrick Esser. Stable diffusion v2 model
card, 2022. 2
[31] Kevin Roose. An a.i.-generated picture won an art prize.
artists aren’t happy., 2022. 2
[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven gen-
eration. arXiv preprint arXiv:2208.12242, 2022. 3
[33] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew
Ilyas, and Aleksander Madry. Raising the cost of malicious
ai-powered image editing. arXiv preprint arXiv:2302.06588,
2023. 2
[34] Timo Schick, Sahana Udupa, and Hinrich Schütze. Self-
diagnosis and self-debiasing: A proposal for reducing corpus-
based bias in nlp. Transactions of the Association for Compu-
tational Linguistics, 9:1408–1424, 2021. 3
[35] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and
Kristian Kersting. Safe latent diffusion: Mitigating inap-
propriate degeneration in diffusion models. arXiv preprint
arXiv:2211.05105, 2022. 1, 2, 3, 5, 6, 7
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. In Thirty-sixth Confer-
ence on Neural Information Processing Systems Datasets and
Benchmarks Track, 2022. 2
[37] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and
Ananda Theertha Suresh. Remember what you want to for-
get: Algorithms for machine unlearning. Advances in Neural
Information Processing Systems, 34:18075–18086, 2021. 3
[38] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng,
Rana Hanocka, and Ben Y Zhao. Glaze: Protecting artists
from style mimicry by text-to-image models. arXiv preprint
arXiv:2302.04222, 2023. 2
[39] SmithMano. Tutorial: How to remove the safety ﬁlter in 5
seconds, 8 2022. 2, 7
[40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, pages 2256–2265. PMLR, 2015.
3
[41] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas
Geiping, and Tom Goldstein. Diffusion art or digital forgery?
investigating data replication in diffusion models.
arXiv
preprint arXiv:2212.03860, 2022. 3
[42] Yang Song and Stefano Ermon. Improved techniques for
training score-based generative models. In H. Larochelle, M.
Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad-
vances in Neural Information Processing Systems, volume 33,
pages 12438–12448. Curran Associates, Inc., 2020. 4
[43] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Sketch your
own gan. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 14050–14060, 2021.
2
[44] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Rewriting
geometric rules of a gan. ACM Transactions on Graphics
(TOG), 41(4):1–16, 2022. 2
[45] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
and Oriol Vinyals. Understanding deep learning (still) re-
quires rethinking generalization.
Communications of the
ACM, 64(3):107–115, 2021. 3

