Semantic Scene Manipulation Based on 3D Spatial
Object Relations and Language Instructions
Rainer Kartmann, Danqing Liu and Tamim Asfour
Abstractâ€” Robot understanding of spatial object relations is
key for a symbiotic human-robot interaction. Understanding the
meaning of such relations between objects in a current scene
and target relations speciï¬ed in natural language commands is
essential for the generation of robot manipulation action goals
to change the scene by relocating objects relative to each other
to fulï¬ll the desired spatial relations. This ability requires a
representation of spatial relations, which maps spatial relation
symbols extracted from language instructions to subsymbolic
object goal locations in the world. We present a generative
model of static and dynamic 3D spatial relations between
multiple reference objects. The model is based on a parametric
probability distribution deï¬ned in cylindrical coordinates and
is learned from examples provided by humans manipulating
a scene in the real world. We demonstrate the ability of
our representation to generate suitable object goal positions
for a pick-and-place task on a humanoid robot, where object
relations speciï¬ed in natural language commands are extracted,
object goal positions are determined and used for parametrizing
the actions needed to transfer a given scene into a new one that
fulï¬lls the speciï¬ed relations.
I. INTRODUCTION
Natural language is a powerful interface for instructing robots
to perform tasks such as manipulating objects or navigating
a building. Spatial object relations play an important role
in communicating the goals or parameters of actions in a
human understandable way such as in natural language to
the robot, e. g. where to go, which object to pick or where to
put it. A key challenge in understanding natural language
commands is grounding them in concepts of the robotâ€™s
scene model, i. e. mapping words and phrases to physical
entities and locations perceived by the robot [1]. In language
understanding, spatial relations are frequently used to resolve
referring expressions used in object descriptions, such as in
â€œPick up the box in front of the tableâ€ [2], [3], [4], [5] and are
used to ï¬nd and disambiguate discrete action parameters, such
as the object to pick or the landmark to place it at.
Our aim is to endow a robot with the ability to transfer a
given scene with a set of objects with certain spatial relations
into a new scene fulï¬lling new spatial relations speciï¬ed by a
natural language command. The overall idea is depicted in
Fig. 1. However, when giving instructions such as â€œPut the
apple tea on top of the mint teaâ€ and â€œPlace the plate between
the fork and the knife,â€ the set of possible goal locations is
not limited to a ï¬nite set of landmarks or objects, but could
be any position in the workspace of the robot. Therefore, it
This work has been supported by the German Federal Ministry of Education
and Research (BMBF) under the project OML (01IS18040A).
The authors are with the Institute for Anthropomatics and Robotics, Karl-
sruhe Institute of Technology, Karlsruhe, Germany. {rainer.kartmann,
asfour}@kit.edu
Fig. 1: Semantic scene manipulation based on language commands
specifying 3D spatial object relations.
is not only necessary to ground language phrases referring
to objects in the robotâ€™s scene model and the spatial relation
phrase to a symbol, but also to map this symbol to an arbitrary,
i. e. subsymbolic, goal position.
To address this problem, we introduce a parametric and
probabilistic generative model for representing 3D spatial
relations between multiple objects in a scene. Given an initial
scene and a desired spatial relation, the model generates
object locations where the object can be placed to fulï¬ll the
relation. Thus, the model grounds a spatial relation extracted
from a language command in a subsymbolic object goal
position. Such a model should have only few parameters which
can be intuitively interpreted. A generative model has the
beneï¬t that appropriate goal locations (and thus, actions) can
directly be sampled from the model instead of sampling and
classifying the whole workspace [6]. Following a learning
from demonstration paradigm [7], [8], our model is learned
from pairwise scene examples that are generated by a human
demonstrator manipulating a scene in the real world.
We build on our previous work on 2D spatial relations [9]
by extending the model to deal with 3D positions and relations
and with two or more reference objects (e. g. A between B and
C). We consider dynamic spatial relations, which depend on
the relative position of objects in the initial scene as well as
static relations, which are independent of the relative object
positions.

II. RELATED WORK
We review work related to spatial relations in the following
domains: language understanding, scene understanding including
human action recognition, and scene manipulation.
A. Spatial Relations in Language Understanding
In a recent survey, Tellex et al. [1] reviewed the ï¬eld of using
language in robotics. The authors classify works according to
the technical approach and the addressed problem. As to the
technical approach, our work falls into the category of lexically-
grounded methods, as we extract a formal representation of the
task (the spatial relations to fulï¬ll and the involved objects)
from a language command, which is then grounded to physical
placing locations in the scene for execution.
With respect to the addressed problem, Tellex et al. distin-
guish between (1) human-to-robot communication (mainly
concerned with language understanding) and (2) robot-to-human
communication (dealing with language generation). Our work
falls into (1), as we extract goal information from a language
command to manipulate the current scene. Furthermore, this
class contains works on using language to (1a) give robots
instructions and (1b) to inform robots about the world. Even
when using spatial relations in commands, their main purpose
is currently (1b), i. e. describing objects or known landmarks.
Tan et al. [10] use a simple grammar and spatial relations
represented as fuzzy memberships to nine image regions
around an object to identify a referred object in a command.
In [11], spatial relations in daily human instructions are
parsed using a syntactic parser and used to learn attributes of
new objects. These works do not involve a robot executing
actions. Fasola et al. [6] represent 2D spatial prepositions by
semantic ï¬elds to resolve ambiguous noun phrase groundings
in language navigation instructions. Hatori et al. [12] jointly
train object recognition and language understanding modules to
resolve unconstrained referring expressions to objects and query
additional expressions in case of ambiguities. The probabilistic
graphical model proposed by Tellex et al. [2] maps constituents
of a language command to objects, places and robot actions.
Forbes et al. [13] ï¬nd the most suitable parameters for a set
of primitive actions by using language generation to resolve
referring expressions given the language command and current
context in a programming by demonstration setup. Prepositions
from natural language commands are incorporated as action
parameters in a task representation based on part-of-speech
tagging in [14]. However, in these works action parameters
are limited to a ï¬nite set of values and position offsets are
ï¬xed. Paul et al. [4], [15] build on extensions of [2] to ground
abstract spatial concepts such as columns or groups of objects
for language understanding. Shridhar et al. present in [5] and
[16] a framework to ground referring expressions to objects
in an image, using spatial relations to resolve ambiguities
when referring to visually similar objects. Placement positions
are found based on spatial prepositions in [3] by training a
multi-class logistic regression to classify random positions on
a table. These works employ discriminative models of spatial
relations to identify referred objects, actions or parameters
from a ï¬nite set of options. In contrast, the problem addressed
in this work requires a generative model of spatial relations
allowing to place objects at any location without limitation to
known entities.
B. Spatial Relations for Scene Understanding
Spatial relations have been extracted to build semantic scene
understanding by classifying the displacement between two
objects to distinguish between on and adjacent [17], learning to
classify functional relations from geometric object features in
simulation [18], or extracting spatial relations from 3D vision
using histograms [19]. Yan et al. [20] employ a neural-logic
network trained with loss functions representing logic rules to
predict fundamental spatial relations and infer complex spatial
relations from them.
For human activity recognition, Lee et al. [21] extract spatial
relations from an RGB-D video according to three calculi
modeling contact, relative movement and static distance by
applying a Dynamic Bayesian Network (DBN) to hand-deï¬ned
key metrics computed from point cloud masks. While their
focus is on the relation extraction and the human activity
recognition itself uses simple rules, other authors have employed
simpler relation extraction models but more sophisticated action
recognition models. (Enriched) Semantic Event Chains encode
changes in spatial relations between objects in RGB-D videos
and have been used to segment and recognize actions based on
distance measures in [22], [23], [24], [25], [26]. Dreher et al.
[27] used the same spatial relations model but employed a
graph neural network to perform bimanual action recognition.
C. Spatial Relations for Scene Manipulation
The following works focus on changing a scene with the
goal of fulï¬lling given spatial relations. Fasola et al. [6]
represent 2D paths as combinations of static spatial prepositions
modeled by semantic ï¬elds. Although they focus on navigation,
their representation could be extended to object manipulation.
Mees et al. [28] use metric learning to realize spatial relations
from known scene examples with potentially different objects.
Jund et al. [29] perform gradient descend on object poses in
a scene to create the same relation as in a reference scene.
In our work, we derive generative models of spatial relations
which abstract from scene examples. Furthermore, the works
above only consider static relations. In [30], Mees et al. train
a neural network to predict pixel-wise probability maps of
placement positions given an input image of the scene and an
object to be placed according to a spatial relation in a language
command. In contrast to a neural network, our representations
based on parametric probability distributions can be intuitively
interpreted and estimated from few examples. In addition, we
are not restricted to an input image.
III. PROBLEM FORMULATION
Given a scene with a set of objects and their spatial relations
encoded in a semantic scene model together with a language
command specifying new spatial relations between these objects,
a robot must transfer the initial scene into a new scene fulï¬lling
the speciï¬ed relations between the same objects by executing
actions (see Fig. 2). To this end, the robot must extract the

Objects, 
Relations
ğ’ª, â„›ğ‘¡
Semantic Scene Model ğ‘†(ğ‘¡)
â€¢ Objects ğ’ª
â€¢ Names
â€¢ Geometries
â€¢ Configuration ğ’«(ğ‘¡)
â€¢ Relations â„›(ğ‘¡)
Language Command ğ¶
â€¢ Speech to Text
â€¢ Named Entity 
Recognition
Robot 
Vision
Speech /
Text
Prior 
Knowledge
Object &  
Relation
Phrases
ğ‘ğœ, ğ‘ğ‘¢, ğ‘ğ’±
Grounding 
Language Phrases 
in Semantic 
Scene Model
Target 
Relation & 
Objects
ğ‘…âˆ—
Generative 
Model of 
Spatial Relations
Target Object 
Configuration
ğ’«(ğ‘¡+1)
VMP-based 
Motion 
Generation
Pick-&-
Place
Actions
Initial 
Scene
Target 
Scene
ğ’ª, ğ’«(ğ‘¡)
Fig. 2: We pose the problem of transferring a scene S(t0) to a new scene S(t1) fulï¬lling spatial relations Râˆ—between objects which are
speciï¬ed by a language command C.
speciï¬ed relations and objects from the language command,
then ï¬nd and execute appropriate actions leading to the desired
spatial relations.
The semantic scene model contains the conï¬guration of
n objects as well as prior knowledge. The conï¬guration
P = {P1, . . . , Pn} consists of the pose Pi âˆˆSE(3) of each
object i with position pi and orientation qi (as quaternion).
The prior knowledge Oi = (gi, Î·i) of object i contains
the object geometry gi and a human-readable name Î·i. In
addition, the scene model contains the spatial relations R =
{R1, . . . , Rm} in the scene. Each relation Rj = (Ïƒj, uj, Vj)
has a symbol Ïƒj âˆˆÎ£, a subject object uj and a set of reference
objects Vj (uj /âˆˆVj). Î£ is the set of known spatial relation
symbols. Therefore, a scene
S = (O, P, R)
(1)
is deï¬ned by the prior object information O = {O1, . . . , On},
conï¬guration P and spatial relations R.
A language command is an imperative text sentence C
(potentially derived from speech via a speech recognition
system) containing object phrases referring to the subject and
reference objects as well as the spatial relations between them.
In lexically-grounded language understanding [1], the relation
and object phrases cÏƒ, cu, cV are parsed from the command,
(cÏƒ, cu, cV) â†parse(C) ,
{cvk}K
k=1
(2)
where K is the number of reference objects. Then, the extracted
phrases (which are still text) must be grounded in the objects O
in the current scene S(t0) and the known relation symbols Î£,
specifying the desired relation Râˆ—,
Râˆ—= (Ïƒâˆ—, uâˆ—, Vâˆ—) â†ground

S(t0), cÏƒ, cu, {cvk}K
k=1

.
We will refer to uâˆ—as the target object. The robotâ€™s task is to
transfer the current scene S(t0) to a target scene
S(t1) =

O, P(t1), R(t1)
(3)
with Râˆ—âˆˆR(t1). To achieve this, the robot must manipulate
the initial object conï¬guration P(t0) by executing actions A,
P(t0)
A
âˆ’â†’P(t1)
such that Râˆ—âˆˆR(t1).
(4)
In this work, we only manipulate the target object by executing
pick&place actions. Therefore, the problem is reduced to ï¬nd
a suitable target pose P (t1)
uâˆ—
to place uâˆ—at.
IV. APPROACH
We approach the problem formulated in the last section
as follows. Based on prior knowledge and visual data of the
scene, we build a 3D semantic scene model S(t0) of the initial
scene using prior knowledge and state-of-the-art methods for
visual pose estimation of known objects [31], [32] to localize
objects in the current scene. We parse the language command
with a Named Entity Recognition model (Section IV-A) and
ground the extracted phrases by performing substring matching
with names of known objects and relations (Section IV-B).
Using a generative model of the spatial relation (Section IV-C),
we generate a suitable target object pose P (t1)
uâˆ—
fulï¬lling the
desired relation Râˆ—(Section V).
A. Extracting the Target Relation from Language Command
Given a speech command such as â€œPlace the apple tea
between the juice and the milk,â€ we apply an existing speech
recognition system used on our ARMAR-6 robot [33] to obtain
the command text C. Following the formulation in Section III,
we then extract the object and relation phrases using a state-of-
the-art Named Entity Recognition (NER) model. NER classiï¬es
parts of a sentence into several class labels, allowing multiple
occurrences of the same class label. We deï¬ne three class
labels, REL, TRG, and REF, for spatial relation symbol Ïƒâˆ—,
target object uâˆ—, and reference object(s) Vâˆ—, respectively.
To train the NER model, we generate language commands
based on sentence templates. For object phrases (TRG and
REF), we use the names of all objects from the KIT and
YCB object data bases ([34], [35]) after processing them to be
human readable and unambiguous. For relation phrases (REL),
we generate phrases for the relations left, right, front, behind,
close to, inside, on top, above, under, closer, farther, other
side, between and among which we manually deï¬ned.
To generate a command, a spatial relation phrase, a target
object and reference objects are selected at random and inserted
into a suitable sentence template, resulting in commands such
as â€œPlace the orange juice on top of the potato starchâ€ and
â€œPut apple tea between the coffee ï¬lters and the orange juice.â€
The commands are labeled by the selected template inputs.
Thus, we generated labeled training data and used them to train
a standard NER model from the natural language processing
library spaCy (https://spacy.io/).
Given the input command sentence, we apply the trained
NER model and get a list of sub-phrases for each class label.

We assume to retrieve exactly one phrase each for REL and
TRG and one or more for REF. For instance, for the previous
example sentence, â€œPlace the apple tea between the juice and the
milk,â€ we obtain the phrases cÏƒ = â€œbetweenâ€, cu = â€œapple teaâ€
and cV = {â€œjuiceâ€, â€œmilkâ€}.
B. Grounding Command Phrases in Semantic Scene Model
Having obtained the object and relation phrases from the
language command, we ground it to the objects in the semantic
scene model and the known relations using substring matching.
For the target object, we ï¬nd the longest common substring si
of cu and each object name Î·i,
si â†arg max
sâˆˆcuâˆ©Î·i
|s| ,
(i = 1, . . . , n)
(5)
where cu âˆ©Î·i denotes all commons substrings of cu and Î·i. We
sort the matched substrings {s1, . . . , sn} decreasingly by their
length and discard those which are shorter than a threshold
(e. g. 3 characters). The result is a list of object hypotheses
in decreasing order of their namesâ€™ conformity with target
object phrase. The same is done for each cvk âˆˆcV as well as
the relation phrase cÏƒ (based on the relation phrases used in
command generation).
Despite its simplicity, this kind of substring matching has
two beneï¬ts. First, it implicitly handles minor errors of the NER
model such as â€œtheâ€ being part of an object phrase â€œthe teaâ€,
as the phrase will still match the object names â€œapple teaâ€ and
â€œpeppermint teaâ€. Second, it allows using a general term such as
â€œjuiceâ€ with speciï¬c names, such as â€œorange juiceâ€ and â€œapple
juiceâ€. In general, we choose the relation and objects with the
longest matches. If multiple objects match equally well, we
pick one at random. Although substring matching could be
applied to the whole command, the NER model focuses it on
relevant phrases and provides the label for each phrase.
C. Generative Model of 3D Spatial Relations
After obtaining Râˆ—= (Ïƒâˆ—, uâˆ—, Vâˆ—) from the language un-
derstanding components, the next step is ï¬nding an appropriate
object pose P (t1)
uâˆ—
to place the target object uâˆ—at. To this end,
we propose a generative model G of spatial relations, which
can directly generate suitable target object poses fulï¬lling a
relation R = (Ïƒ, u, V) based on the current scene
Pu âˆ¼GÏƒ

u, V, O, P(t0)
.
(6)
With a generative model G, appropriate target poses (and thus,
actions) can directly be sampled from G instead of sampling
and classifying the whole workspace. In addition, it still allows
to discard hypotheses, which are infeasible due to other external
constraints, such as collisions or reachability. In addition, our
model can be learned from examples in form of pairs of scenes
before and after a change induced by humans.
In order to prevent object uâˆ—from tipping after it was
placed, we keep its initial orientation in the target scene,
q(t1)
uâˆ—
= q(t0)
uâˆ—. Therefore, the problem of generating P (t1)
uâˆ—
is
reduced to generating a suitable target position p(t1)
uâˆ—.
In the following, we describe the methods for representing
static and dynamic 3D spatial relations and how they are
extended to multiple reference objects.
(a)
ğ›ğ‘£
ğ‘¥
â„= ğ‘§ğ‘£+ âˆ’ğ‘§ğ‘£âˆ’
ğ‘§
ğ›ğ‘¢
ğ‘¦
(b)
ğ‘¥
ğ‘§
ğ‘¦
â„= 0
ğ›ğ‘£
ğ›ğ‘¢
(c)
Fig. 3: (a) Visualization of a cylindrical distribution and its parameters
(best viewed in color). The orange arrow runs from the origin to the
distribution mean. The cylinder surface is a cross section at the mean
radius Âµr (mantle) and mean height Âµh (top), color indicates PDF
ranging from 0 (purple) to the maximum PDF at the mean (yellow).
Red arrows represent the radius variance, blue arrows represent height
variance (radius and height are not covariant in this example), green
arrows represent azimuth concentration. (b, c) Using the bottom-
projected centroid of objects as reference positions helps to generalize
to reference objects of different size.
1) 3D Spatial Relations as Cylindrical Distributions: In
our previous work [9], we proposed to represent 2D spatial
relations in a horizontal plane (e. g. a tabletop surface) as
parametric probability distributions deï¬ned in a polar coordinate
system (PCS). A polar distributionâ€™s probability density function
has been deï¬ned as
pÎ¸(r, Ï†) = pÎ¸r(r) Â· pÎ¸Ï†(Ï†) ,
Î¸ = (Î¸r, Î¸Ï†)
(7)
over radial distance r âˆˆRâ‰¥0 (denoted as distance d in [9])
and azimuthal angle Ï† âˆˆ[âˆ’Ï€, Ï€], where
r âˆ¼N(Î¸r) ,
Î¸r =
 Âµr, Ïƒ2
r

,
(8)
Ï† âˆ¼M(Î¸Ï†) ,
Î¸Ï† = (ÂµÏ†, ÎºÏ†)
(9)
with mean radius Âµr and radius variance Ïƒ2
r as well as mean
azimuth ÂµÏ† and azimuth concentration ÎºÏ†. N(Â·) denotes a
Gaussian while M(Â·) denotes a von Mises distribution, which
is a circular distribution wrapping over [âˆ’Ï€, Ï€]. The reference
PCS is centered at the reference objectâ€™s geometric center,
so the polar coordinate (r, Ï†) represents the target objectâ€™s
position relative to the reference object.
To extend the previous model to 3D space, we introduce the
height h âˆˆR as an additional dimension of the probability
distributionâ€™s deï¬nition space. Effectively, this transforms the
PCS over (r, Ï†) to a cylindrical coordinate system (CCS) over
(r, Ï†, h). As both r and h are distances, we assume that they
follow a joint Gaussian distribution
(r, h) âˆ¼N(Î¸rh) ,
Î¸rh = (Âµrh, Î£rh)
(10)
with means Âµrh = (Âµr, Âµh)âŠ¤âˆˆR2 and covariance matrix
Î£rh âˆˆR2Ã—2. For the azimuth, we adopt its von Mises
distribution from eq. (9). We also adopt the assumption that
(r, h) are independent of Ï†. This assumption is motivated by
the observation that spatial relations are typically either a
statement about direction (e. g. left, behind, above) or about
distance (e. g. close to, far away from) [36]. Hence, we deï¬ne
a cylindrical distribution as joint probability distribution
(r, Ï†, h) âˆ¼C(Î¸) ,
Î¸ = (Î¸rh, Î¸Ï†)
(11)

over radius r âˆˆRâ‰¥0, azimuth Ï† âˆˆ[âˆ’Ï€, Ï€] and height h âˆˆR
with the joint probability density function (PDF)
pÎ¸(r, Ï†, h) = pÎ¸rh(r, h) Â· pÎ¸Ï†(Ï†)
(12)
with pÎ¸Ï†(Ï†) and pÎ¸rh(r, h) according to eq. (9) and (10). An
example distribution and its parameters is visualized in Fig. 3a.
To sample and evaluate positions in Cartesian world
coordinates, they need to be transformed from and to the
relationâ€™s CCS. The Cartesian world coordinate system (WCS)
is aligned to the agent such that +x points to the right and
+z points up (i. e. opposite of the vector of gravity), while
the relationâ€™s CCS is aligned to the reference objects V. For
simplicity, we ï¬rst consider the case of a single reference
object v = v1. The alignment of the CCS is based on the
reference objectâ€™s geometry gv in the WCS. Let
B(t0)
v
= [x
âˆ’
v , x
+
v ] Ã— [y
âˆ’
v , y
+
v ] Ã— [z
âˆ’
v , z
+
v ] âŠ‚R3.
(13)
be the axis-aligned bounding box (AABB) enclosing gv at its
position p(t0)
v
in the WCS. The bottom-projected centroid
b(t0)
v
=
xâˆ’
v + x+
v
2
, yâˆ’
v + y+
v
2
, z
âˆ’
v
âŠ¤
âˆˆR3
(14)
of B(t0)
v
represents the reference position of v. At the same time,
it is the origin of the relationâ€™s CCS. Likewise, the reference
position of the subject object u is the bottom-projected centroid
b(t1)
gu
of its own AABB B(t1)
u
at a potential target position p(t1)
u
.
This choice complies with our previous formulation of 2D
spatial relations, with the origin at the height of the surface
the objects are standing on. A useful property is that two
objects standing on the same horizontal surface have the same
z-coordinate (height), independent of their respective size.
As in [9], we distinguish between static and dynamic spatial
relations. The geometric meaning of static relations such as
left of or on top of mainly depends on the reference objects,
while the meaning of dynamic relations such as closer to and
on the other side of depends on the initial conï¬guration of
both the subject and reference objects.
In our prior work, we have shown that it is effective to
apply different transformations depending the relationâ€™s type to
incorporate these dependencies into the CCS. Here, we extend
these transformations to 3D. We will refer to
p(t1)
loc (u, v) =

x(t1)
loc , y(t1)
loc , z(t1)
loc
âŠ¤
= b(t1)
u
âˆ’b(t0)
v
(15)
as uâ€™s target position in the local WCS, i. e. relative to v.
2) Static Spatial Relations: The local target position is
scaled proportionally to the reference objectâ€™s size before
transforming it to cylindrical coordinates:
p(t1)
sta (u, v) =
 
2 x(t1)
loc
x
+v âˆ’x
âˆ’v
, 2 y(t1)
loc
y
+v âˆ’y
âˆ’
v
,
z(t1)
loc
z
+v âˆ’z
âˆ’
v
!âŠ¤
(16)
This way, if the target object is standing on top of the
reference object, zsta â‰ˆ1, independent of the respective object
sizes (Fig. 3b). If they are standing on the same horizontal
surface, such as a tabletop or shelf, zsta â‰ˆ0 (Fig. 3c). This
design helps to generalize spatial relations over objects of
different size. Finally, the local scaled world coordinate is
converted to cylindrical coordinates
c(t1)
sta (u, v) = cylindrical

p(t1)
sta (u, v)

(17)
using
cylindrical
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
r
Ï†
h
ï£¶
ï£¸=
ï£«
ï£­
p
x2 + y2
atan2(y, x)
z
ï£¶
ï£¸.
(18)
3) Dynamic Spatial Relations: Let

r(t0), Ï†(t0), h(t0)âŠ¤
= cylindrical

p(t0)
loc (u, v)

(19)
be uâ€™s initial local position in cylindrical coordinates. For the
dynamic cylindrical coordinate
c(t1)
dyn(u, v) =
 rdyn, Ï†dyn, hdyn
âŠ¤,
(20)
we want to obtain three properties: (i) rdyn = 1 shall
correspond to the current radial distance r(t0) between v and u,
(ii) Ï†dyn = 0 shall correspond to the current direction Ï†(t0)
from v to u, and (iii) hdyn = 1 shall correspond to the current
height difference h(t0) in units of the reference objectâ€™s height.
These properties help generalizing dynamic spatial relations
to different initial object conï¬gurations [9]. To achieve them,
we adopt the alignment of the polar part of the cylindrical
coordinates (r, Ï†) from [9] and extend it to the height parameter
h by aligning a potential target cylindrical coordinate

r(t1), Ï†(t1), h(t1)âŠ¤
= cylindrical

p(t1)
loc (u, v)

(21)
to the initial conï¬guration as follows:
ï£«
ï£­
rdyn
Ï†dyn
hdyn
ï£¶
ï£¸=
ï£«
ï£­
r(t1)/r(t0)
Ï†(t1) âˆ’Ï†(t0)
(h(t1) âˆ’h(t0))/ (z+
v âˆ’zâˆ’
v )
ï£¶
ï£¸
(22)
4) Extension to Multiple Reference Objects: To extend
to multiple reference objects (|V| > 1), we adopt the notion
of abstract objects from [4]. In their work, the authors have
used abstract concepts such as â€œrowâ€, â€œcolumnâ€ and â€œgroupâ€
in order to ground referential expressions in natural language
instructions such as â€œpick up the middle block from this row
of blocksâ€ to colored cubes in an image. Here, we conï¬‚ate
all reference objects into one abstract object and apply the
logic for a single object explained above. As the formulations
above are based on the AABB B(t0)
v
, we can easily extend
them from v to V by computing the AABB B(t0)
V
enclosing
all reference objects vk âˆˆV. This way, we can apply the same
spatial relations to single and multiple reference objects without
signiï¬cantly changing the mathematical formulation.
V. LEARNING GENERATIVE MODELS
OF 3D SPATIAL RELATIONS
A. Data Collection in Real World Setup
To learn generative models of spatial relations, we collected
a few samples for each spatial relation. As we are using
spatial relations to change an initial scene to a target scene, a
sample minimally consists of the initial scene S(t0), the target
scene S(t1) and the relation Râˆ—which has generated the scene

stereo camera
known objects
scene model
generated 
language 
command
Fig. 4: Real world data collection setup for data collection.
change (which includes the involved objects). In contrast to
our previous work, where we collected data on a 2D monitor,
for this work, we collected data in a real world demonstration
setup shown in Fig. 4. The setup consists of a tabletop scene
with multiple known objects from the KIT and YCB object
databases [34], [35], a stereo camera and a monitor displaying
generated language instructions, which the demonstrator should
follow. We calibrated the camera externally to an agent-aligned
world coordinate frame as described above. We localize the
objects in the scene based on the camera images. The current
(left) camera image and scene model are displayed to the
human demonstrator on the monitor.
We generate language commands in the same way as to
train the NER model (Section IV-A). Here, the object phrases
are limited to the currently localized objects. As this work
focuses on pick-and-place tasks, we limit the REF phrases
to the spatial relations, which can be created by relocating a
single object in a scene (e. g. we do not generate sentences like
â€œPlace the tea under the coffee,â€ which would require lifting
the coffee ï¬rst). Reference objects, a target object and a spatial
relation are selected randomly and inserted into a suitable
sentence template, which is then shown to the demonstrator on
the monitor. The demonstrator can then record the initial scene,
perform the manipulation, and record the changed scene. To
create diverse target scenes, the demonstrator can change and
record the scene multiple times (all target scenes are relative
to the same initial scene). Eventually, the demonstrator can
request the next command. The result is a collection of samples
DÏƒ for each spatial relation Ïƒ âˆˆÎ£, where each sample consists
of an initial and a target scene:
DÏƒ =
n
S(t0)
k
, S(t1)
k
oKÏƒ
k=1
(Ïƒ âˆˆÎ£)
(23)
B. Estimation of Cylindrical Distributions
In our model, each 3D spatial relation Ïƒ is represented by
a single cylindrical distribution CÏƒ. Therefore, we estimate
one cylindrical distribution based on the samples DÏƒ of each
Ïƒ âˆˆÎ£. For each target scene sample k, we transform the target
objectâ€™s pose to cylindrical coordinates according to its type as
deï¬ned in eq. (17) and (22).
 rk, Ï†k, hkâŠ¤â†
(
c(t1)
sta (u, v) ,
static
c(t1)
dyn(u, v) ,
dynamic
(24)
The relationsâ€™ types (static or dynamic) are predeï¬ned. We then
perform Maximum Likelihood Estimation (MLE) separately
for (1) radius and height and (2) the azimuth according to the
cylindrical distributionâ€™s structure in eq. (12)
Î¸âˆ—
rh â†MLEN

rk, hk	KÏƒ
k=1

,
(25)
Î¸âˆ—
Ï† â†MLEM

Ï†k	KÏƒ
k=1

.
(26)
With Î¸ = (Î¸rh, Î¸Ï†) as in eq. (11), this deï¬nes the cylindrical
distribution.
C. Sampling and Selection of Target Position
Given the desired relation Râˆ—= (Ïƒâˆ—, uâˆ—, Vâˆ—), the previously
estimated cylindrical distribution CÏƒâˆ—(Î¸) representing Ïƒâˆ—as
well as the initial scene S(t0), we ï¬nd a target position p(t1)
uâˆ—
by
sampling from CÏƒâˆ—(Î¸) and selecting the best feasible hypotheses.
First, we sample a predeï¬ned number of cylindrical coordinates
ck = (rk, Ï†k, hk) âˆ¼CÏƒâˆ—(Î¸) ,
(k = 1, . . . , K).
(27)
Using the initial scene S(t0) and type of Ïƒâˆ—, we transform the
cylindrical coordinates back to Cartesian space by inverting the
steps described above
pk = (xk, yk, zk) â†cartesian

ck, S(t0), Ïƒâˆ—
.
(28)
These are our initial hypotheses for p(t1)
uâˆ—. To select a ï¬nal
hypothesis for execution, we use a similar approach as
in [9]. To ensure that the resulting scene is collision-free
and statically stable, we ï¬lter the hypotheses using collision
and stability checks [37]. Let Hfeas âŠ†{1, . . . , K} be the
feasible hypotheses. To ï¬nd the best action, we evaluate the
PDF of each hypothesis and keep those with the highest values:
Hbest =

k âˆˆHfeas
 pÎ¸(ck) â‰¥0.9 Â·max
lâˆˆIfeas pÎ¸(cl)

(29)
To avoid unnecessary movement, we select the position pkâˆ—
from the remaining hypotheses which is closest to the target
objectâ€™s current position
kâˆ—= arg min
kâˆˆHbest
pk âˆ’p(t0)
uâˆ—
 ,
thus p(t1)
uâˆ—
= pkâˆ—.
(30)
VI. EVALUATION
A. Qualitative Analysis of 3D Spatial Relation Representations
Table I shows examples using the learned representations of
different spatial relations to select a placing position for an
object in simulated scenes. The ï¬rst observation is that we
inherit the representation power of the polar distribution from
[9], as â€œï¬‚atâ€ relations such as left or closer to can easily be
represented as cylindrical distribution with a mean height close
to zero. This is a result of using the bottom-projected centroids
of objects as reference positions. Second, as can be seen in the
examples for the relations in front of and behind, collision-
freeness and static stability can be achieved by discarding
samples violating any external constraints. Third, the relation
on top can now be represented as well (with Âµh = 0.939 â‰ˆ1),
corresponding roughly to the vertical size of reference object(s).
Due to the natural fuzziness of our representation, the relation
among can also generate positions on top of another object if
the tabletop surface is obstructed.

TABLE I
ESTIMATED CYLINDRICAL DISTRIBUTIONS USED TO SELECT PLACING POSITIONS IN SIMULATED SCENES.
The number of samples used to estimate each distribution is given in parentheses. The target object is marked green, reference objects are
yellow. The target object is drawn transparently at a selected placing position. In the ï¬rst row, one reference object is speciï¬ed, while two are
used in the second row (for among, two and three have been speciï¬ed, respectively). The distributions are visualized similarly to Figure 3a,
with the cylinder tops (colored disks) spanning 1.5Âµr to completely show the distributionsâ€™ high-likelihood areas.
|V|
left (9),
right (10)
in front of (9),
behind (9)
on top of (13)
close to (3),
far from (6)
between (10)
among (8)
(|V| âˆˆ{2, 3})
closer to (19),
further from (10)
on the other
side of (15)
1
2
(a) Initial scene
(b) Grasp
(c) VMP placing motion
(d) Place and release
(e) Retreat
Fig. 5: Changing a scene according to the command â€œPlace the mustard on the rusk.â€ on the humanoid robot ARMAR-6.
Finally, we can specify more than one reference object as
easily as one for all relations. For instance, on the other side
of generates a position on the opposite side of the mid point
between the two reference objects. Similarly, on top of yields
similar results when generating positions relative to a stack of
objects or just the highest object of the stack. A very interesting
observation is that using between with a container as single
reference object (which never occurred in the demonstrated
data) results in a behavior expected from an inside relation
(which was demonstrated as containers were not used in data
collection). This is plausible, as between behaves like an inside
for the abstract object comprising all reference objects.
Overall, the results demonstrate that our previous representa-
tion of 2D spatial relations have successfully been extended to
3D and multiple reference objects.
B. Validation Experiments on Real Robot
We demonstrate the usefulness of the learned representations
of 3D spatial relations in a robot pick-and-place task (Fig. 5)
on the humanoid robot ARMAR-6 [33]. We place several
known objects on a table, give a language command to the
robot and follow the steps described in Section IV to parse the
command, ground the relation and object phrases in the robotâ€™s
scene model and retrieve the learned cylindrical distribution,
sample and select a target position and execute the grasping
and placing motions.
For the generation of robot motions, we leverage our previous
work on Via-Point Movement Primitives (VMP) [38] that can
be learned from kinesthetic teaching and adapted to new start
and goal positions as well as arbitrary intermediate via-points.
Once the target position p(t1)
uâˆ—
has been selected, the robot has
to grasp the target object uâˆ—and place it at the new position.
Several VMPs (approach, move, place, retreat) are used to
approach and grasp the target object, lift it and place it at
the target position. Vision-based localization of the involved
objects on the table as well as in the robotâ€™s hand is used to
determine goal positions for the adaptation of the VMPs. The
experiments show that the learned cylindrical distributions can
generate suitable placing positions.
VII. CONCLUSION AND FUTURE WORK
We presented a generative model for learning 3D spatial
relations with multiple reference objects that extends our
previous work on 2D spatial relations. The model is based
on a parametric cylindrical distribution and enables a robot to
manipulate objects in the current scene to fulï¬ll spatial relations
speciï¬ed in a language command by generating suitable placing
positions of the manipulated object. To understand and ground
the language command, we have leveraged a Named Entity
Recognition model and substring matching with the names of
objects and relations in the robotâ€™s semantic scene model.
Future work and extensions of this work will address the
questions of whether the type of a spatial relation (static or
dynamic) can be derived automatically based on the respective
likelihood of the demonstration data. Further, we will investigate
the use of ambiguities in grounding object phrases as entry
points for a dialog for disambiguation. In addition, we will
extend the spatial relation models to encode more complex
manipulation actions.

REFERENCES
[1] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek, â€œRobots That
Use Language,â€ Annual Review of Control, Robotics, and Autonomous
Systems, vol. 3, no. 1, pp. 25â€“55, 2020.
[2] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. Teller,
and N. Roy, â€œUnderstanding Natural Language Commands for Robotic
Navigation and Mobile Manipulation,â€ in Proceedings of the AAAI
Conference on Artiï¬cial Intelligence, 2011.
[3] S. Guadarrama, L. Riano, D. Golland, D. GÂ¨ohring, Y. Jia, D. Klein,
P. Abbeel, and T. Darrell, â€œGrounding Spatial Relations for Human-Robot
Interaction,â€ in IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), Nov. 2013, pp. 1640â€“1647.
[4] R. Paul, J. Arkin, N. Roy, and T. M. Howard, â€œEfï¬cient Grounding of
Abstract Spatial Concepts for Natural Language Interaction with Robot
Manipulators,â€ in Robotics: Science and Systems (RSS), vol. 12, 2016.
[5] M. Shridhar and D. Hsu, â€œInteractive Visual Grounding of Referring
Expressions for Human-Robot Interaction,â€ in Robotics: Science &
Systems (RSS), 2018.
[6] J. Fasola and M. J. MatariÂ´c, â€œUsing semantic ï¬elds to model dynamic
spatial relations in a robot architecture for natural language instruction
of service robots,â€ in IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2013, pp. 143â€“150.
[7] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, â€œRobot Programming
by Demonstration,â€ in Handbook of Robotics.
Springer, 2008, pp.
1371â€“1394.
[8] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, â€œA survey of
robot learning from demonstration,â€ Robotics and Autonomous Systems,
vol. 57, no. 5, pp. 469â€“483, 2009.
[9] R. Kartmann, Y. Zhou, D. Liu, F. Paus, and T. Asfour, â€œRepresenting
Spatial Object Relations as Parametric Polar Distribution for Scene
Manipulation Based on Verbal Commands,â€ in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2020, pp. 8373â€“
8380.
[10] J. Tan, Z. Ju, and H. Liu, â€œGrounding Spatial Relations in Natural
Language by Fuzzy Representation for Human-Robot Interaction,â€ in
IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), July
2014, pp. 1743â€“1750.
[11] J. Bao, Z. Hong, H. Tang, Y. Cheng, Y. Jia, and N. Xi, â€œTeach robots
understanding new object types and attributes through natural language
instructions,â€ in International Conference on Sensing Technology (ICST),
vol. 10, Nov. 2016, pp. 1â€“6.
[12] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno,
W. Ko, and J. Tan, â€œInteractively Picking Real-World Objects with
Unconstrained Spoken Language Instructions,â€ in 2018 IEEE International
Conference on Robotics and Automation (ICRA), 2018, pp. 3774â€“3781.
[13] M. Forbes, R. P. N. Rao, L. Zettlemoyer, and M. Cakmak, â€œRobot
Programming by Demonstration with Situated Spatial Language Under-
standing,â€ in IEEE International Conference on Robotics and Automation
(ICRA), May 2015, pp. 2014â€“2020.
[14] M. Nicolescu, N. Arnold, J. Blankenburg, D. Feil-Seifer, S. B. Banisetty,
M. Nicolescu, A. Palmer, and T. Monteverde, â€œLearning of Complex-
Structured Tasks from Verbal Instruction,â€ in IEEE/RAS International
Conference on Humanoid Robots (Humanoids), Oct. 2019, p. 8.
[15] R. Paul, J. Arkin, D. Aksaray, N. Roy, and T. M. Howard, â€œEfï¬cient
grounding of abstract spatial concepts for natural language interaction
with robot platforms,â€ International Journal of Robotics Research, vol. 37,
no. 10, pp. 1269â€“1299, 2018.
[16] M. Shridhar, D. Mittal, and D. Hsu, â€œINGRESS: Interactive visual
grounding of referring expressions,â€ International Journal of Robotics
Research, vol. 39, no. 2-3, pp. 217â€“232, 2020.
[17] B. Rosman and S. Ramamoorthy, â€œLearning spatial relationships between
objects,â€ International Journal of Robotics Research, vol. 30, no. 11, pp.
1328â€“1342, 2011.
[18] K. SjÂ¨oÂ¨o and P. Jensfelt, â€œLearning spatial relations from functional
simulation,â€ in IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), 2011, pp. 1513â€“1519.
[19] S. Fichtl, A. McManus, W. Mustafa, D. Kraft, N. KrÂ¨uger, and F. Guerin,
â€œLearning spatial relationships from 3D vision using histograms,â€ in
IEEE International Conference on Robotics and Automation (ICRA),
May 2014, pp. 501â€“508.
[20] F. Yan, D. Wang, and H. He, â€œRobotic Understanding of Spatial
Relationships Using Neural-Logic Learning,â€ in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2020, pp. 8358â€“
8365.
[21] S. U. Lee, S. Hong, A. Hofmann, and B. Williams, â€œQSRNet: Estimating
Qualitative Spatial Representations from RGB-D Images,â€ in IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), 2020,
pp. 8057â€“8064.
[22] K. Zampogiannis, Y. Yang, C. Fermuller, and Y. Aloimonos, â€œLearning
the Spatial Semantics of Manipulation Actions Through Preposition
Grounding,â€ in IEEE International Conference on Robotics and
Automation (ICRA), 2015, pp. 1389â€“1396.
[23] E. E. Aksoy, Y. Zhou, M. WÂ¨achter, and T. Asfour, â€œEnriched Manipulation
Action Semantics for Robot Execution of Time Constrained Tasks,â€ in
IEEE/RAS International Conference on Humanoid Robots (Humanoids),
Nov. 2016, pp. 109â€“116.
[24] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. WÂ¨orgÂ¨otter, â€œPrediction
of Manipulation Action Classes Using Semantic Spatial Reasoning,â€ in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Oct. 2018, pp. 3350â€“3357.
[25] â€”â€”, â€œRecognition and Prediction of Manipulation Actions Using
Enriched Semantic Event Chains,â€ Robotics and Autonomous Systems
(RAS), vol. 110, pp. 173â€“188, Dec. 2018.
[26] T. R. Savarimuthu, A. G. Buch, C. Schlette, N. Wantia, J. RoÃŸmann,
D. MartÂ´Ä±nez, G. Aleny`a, C. Torras, A. Ude, B. Nemec, A. Kramberger,
F. WÂ¨orgÂ¨otter, E. E. Aksoy, J. Papon, S. Haller, J. Piater, and N. KrÂ¨uger,
â€œTeaching a Robot the Semantics of Assembly Tasks,â€ IEEE Transactions
on Systems, Man, and Cybernetics: Systems, vol. 48, no. 5, pp. 670â€“692,
2018.
[27] C. R. G. Dreher, M. WÂ¨achter, and T. Asfour, â€œLearning Object-Action
Relations from Bimanual Human Demonstration Using Graph Networks,â€
IEEE Robotics and Automation Letters (RA-L), vol. 5, no. 1, pp. 187â€“194,
2020.
[28] O. Mees, N. Abdo, M. Mazuran, and W. Burgard, â€œMetric Learning for
Generalizing Spatial Relations to New Objects,â€ in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2017, pp. 3175â€“
3182.
[29] P. Jund, A. Eitel, N. Abdo, and W. Burgard, â€œOptimization Beyond the
Convolution: Generalizing Spatial Relations with End-to-End Metric
Learning,â€ in IEEE International Conference on Robotics and Automation
(ICRA), May 2018, pp. 4510â€“4516.
[30] O. Mees, A. Emek, J. Vertens, and W. Burgard, â€œLearning Object Place-
ments For Relational Instructions by Hallucinating Scene Representations,â€
in IEEE International Conference on Robotics and Automation (ICRA),
2020.
[31] P. Azad, T. Asfour, and R. Dillmann, â€œCombining Harris Interest Points
and the SIFT Descriptor for Fast Scale-Invariant Object Recognition,â€ in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Oct. 2009, pp. 4275â€“4280.
[32] K. Pauwels and D. Kragic, â€œSimTrack: A Simulation-Based Framework
for Scalable Real-Time Object Pose Detection and Tracking,â€ in IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), Sept.
2015, pp. 1300â€“1307.
[33] T. Asfour, M. WÂ¨achter, L. Kaul, S. Rader, P. Weiner, S. Ottenhaus,
R. Grimm, Y. Zhou, M. Grotz, and F. Paus, â€œARMAR-6: A High-
Performance Humanoid for Human-Robot Collaboration in Real-World
Scenarios,â€ IEEE Robotics Automation Magazine, vol. 26, no. 4, pp.
108â€“121, 2019.
[34] A. Kasper, Z. Xue, and R. Dillmann, â€œThe KIT object models database:
An object model database for object recognition, localization and
manipulation in service robotics,â€ The International Journal of Robotics
Research, vol. 31, no. 8, pp. 927â€“934, 2012. [Online]. Available:
https://doi.org/10.1177/0278364912445831
[35] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa,
P. Abbeel, and A. M. Dollar, â€œYale-CMU-Berkeley dataset for robotic
manipulation research,â€ International Journal of Robotics Research,
vol. 36, no. 3, pp. 261â€“268, Mar. 2017.
[36] J. Oâ€™Keefe, â€œVector Grammar, Places, and the Functional Role of the
Spatial Prepositions in English,â€ in Representing Direction in Language
and Space.
Oxford University Press, 2003, pp. 69â€“85.
[37] R. Kartmann, F. Paus, M. Grotz, and T. Asfour, â€œExtraction of Physically
Plausible Support Relations to Predict and Validate Manipulation Action
Effects,â€ IEEE Robotics and Automation Letters (RA-L), vol. 3, no. 4, pp.
3991â€“3998, Oct. 2018.
[38] Y. Zhou, J. Gao, and T. Asfour, â€œLearning Via-Point Movement Primitives
with Inter- and Extrapolation Capabilities,â€ in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2019, pp. 4301â€“
4308.

