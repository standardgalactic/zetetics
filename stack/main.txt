Diss. ETH No. 24584
Bayesian Uncertainty QuantiÔ¨Åcation
for Data-Driven Applications in
Engineering and Life Sciences
A thesis submitted to attain degree of
DOCTOR OF SCIENCES of ETH ZURICH
(Dr. sc. ETH Zurich)
presented by
LINA KULAKOVA
Specialist in Mathematics, Lomonosov Moscow State University
born on December 1, 1990
citizen of Russia
accepted on the recommendation of
Prof. Dr. Petros Koumoutsakos, examiner
Prof. Dr. Costas Papadimitriou, co-examiner
2017


Abstract
The world is quickly moving towards using computer modeling and simulations in
various branches of science and engineering.
The computational methods found
their way to biology, chemistry, physics, social sciences and many other Ô¨Åelds.
Simulations allow the researchers to study phenomena which can not be directly
observed in the experiments.
In order to be trustworthy, the simulations must
go through a thorough process of data-driven validation.
Often, an adjustment
of the free parameters of the computational model is required for it to match the
experimental observations.
One of the possibilities to Ô¨Ånd the right model parameters is an optimization with
respect to the quantities of interest which one wishes to reproduce in the simulation.
The amount of information which can be extracted from this deterministic approach
is, however, very limited. The parameters sensitivity or their mutual correlations,
which are crucial for making robust predictions, are not available in this approach.
The solution to this problem lies in the area of probability, speciÔ¨Åcally in the
Bayesian methodology. The value of the Bayesian approach in the modern com-
putational science can hardly be overestimated. It updates the prior beliefs about
the model parameters, expressed in the form of a probability distribution, using the
available experimental data. This approach takes into account all the uncertainties
arising in the inference process: the experimental, the modeling and the simulation
ones, thus providing a reliable result. Blending together the simulation and the ex-
periment outcomes, it constructs a conditional distribution of the model parameters
given the observational data. From this distribution, one can extract the uncertainty
bounds, the correlations, the sensitivities etc. for each of the parameters, as well as
for any quantity of interest which can be computed using the calibrated model.
All these useful features, however, come with a price of an excessive computa-
iii

tional cost, since the Bayesian inference requires a large number of model evalua-
tions to construct the parameters distribution. There exist two main approaches
to deal with the high cost issue while keeping the simulation itself untouched. The
Ô¨Årst option is to use a cheap replacement of the simulation ‚Äì a surrogate model ‚Äì
constructed via interpolation or similar methods. The second (non-exclusive) pos-
sibility is to apply high performance computing techniques to eÔ¨Éciently exploit the
available hardware architectures and reduce the time to solution.
Both options require a Ô¨Çexible and robust computational platform. A framework
Œ†4U, which meets both requirements, was recently developed in our lab.
Œ†4U
contains state of the art high performance computing implementations of algorithms
for optimization, Bayesian uncertainty quantiÔ¨Åcation and propagation.
In this thesis, we extend the Œ†4U framework to include the lacking inference
tools, such as an algorithm for the approximate Bayesian inference, surrogate mod-
els, visualization tools and a sensitivity tool.
We then use the extended frame-
work to perform the Bayesian uncertainty quantiÔ¨Åcation and propagation in Ô¨Åne-
and coarse-grained molecular dynamics and dissipative particle dynamics. The sys-
tematic Bayesian approach allows us to enhance the predictive capabilities of the
particle-based models and gain new insights into the century-old force-Ô¨Åelds.
iv

R¬¥esum¬¥e
Le monde se bouget rapidement `a l‚Äôutilisation de la mod¬¥elisation d‚Äôordinateur et des
simulation automatis¬¥ee dans diverses branches de la science et de l‚Äôing¬¥enierie. Les
m¬¥ethodes de calcul ont trouv¬¥e leur chemin vers la biologie, la chimie, la physique,
les sciences sociales et vers des nombreux autres domaines.
Les simulations permettent aux chercheurs d‚Äô¬¥etudier des ph¬¥enom`enes qui ne peu-
vent pas ÀÜetre observ¬¥es directement dans les exp¬¥eriences. Pour ÀÜetre Ô¨Åable, les simula-
tions doivent passer par un processus fastidieux, ax¬¥e sur les donn¬¥ees, de validation.
Un ajustement des param`etres libres du mod`ele de calcul est souvent n¬¥ecessaire
pour qu‚Äôil corresponde aux donn¬¥ees d‚Äôobservation. Le processus de validation est
un jeu interactif de donn¬¥ees et de mod`eles: apr`es s‚Äôassurer que le mod`ele reproduit
bien les ph¬¥enom`enes qui peuvent ÀÜetre mesur¬¥es exp¬¥erimentalement, on peut l‚Äôutiliser
pour pr¬¥edire de nouvelles quantit¬¥es et eÔ¨Äets.
Une des possibilit¬¥es pour trouver les bons param`etres de mod`ele est une opti-
misation avec en ce qui concerne les quantit¬¥es d‚Äôint`erÀÜet. La quantit`e d‚Äôinformation
qui peut ÀÜetre d¬¥eriv¬¥ee de cette approche d¬¥eterministe est, cependant, tr`es limit¬¥e. La
sensibilit¬¥e des param`etres ou leurs corr¬¥elations mutuelles, qui sont cruciales pour
faire des pr¬¥edictions robustes, ne sont pas disponibles dans cette approche.
La solution `a ce probl`eme r¬¥eside dans le domaine de la probabilit¬¥e, en parti-
culier dans le m¬¥ethodologie bay¬¥esienne. La valeur de l‚Äôapproche bay¬¥esienne dans
la science computationnelle moderne peut diÔ¨Écilement ÀÜetre surestim¬¥ee. Elle met `a
jour les croyances ant¬¥erieures sur les param`etres du mod`ele, exprim¬¥es sous la forme
d‚Äôune distribution de probabilit¬¥e, en utilisant des donn¬¥ees exp¬¥erimentales. Cette ap-
proche prend en compte toutes les incertitudes d¬¥ecoulant du processus d‚Äôinf¬¥erence:
l‚Äôexp¬¥erimental, la mod¬¥elisation et la simulation ceux, fournissant ainsi un r¬¥esultat
Ô¨Åable. En m¬¥elangeant la simulation et les r¬¥esultats de l‚Äôexp¬¥erience, elle construit
v

une distribution conditionnelle des param`etres du mod`ele. A partir de cette dis-
tribution, on peut extraire l‚Äôincertitude limites, les corr¬¥elations, les sensibilit¬¥es etc.
pour chacun des param`etres, ainsi que pour chaque quantit d‚Äôint¬¥erÀÜet qui peut ÀÜetre
calcul¬¥ee en utilisant le mod`ele calibr¬¥e.
Cependant, toutes ces caract¬¥eristiques utiles ont un coÀÜut de calcul excessif, car
l‚Äôinf¬¥erence bay¬¥esienne n¬¥ecessite un grand nombre d‚Äô¬¥evaluations de mod`ele, pour
construire la distribution des param`etres. Il existe deux approches principales pour
faire face `a la question de coÀÜut ¬¥elev¬¥e, en gardant la simulation elle-mÀÜeme intacte.
Le premi`ere option est `a utiliser un remplacement bon march¬¥e de la simulation ‚Äì un
mod`ele de substitution ‚Äì construit par interpolation ou par des m¬¥ethodes similaires.
La deuxi`eme possibilit¬¥e (non exclusive) est d‚Äôexploiter eÔ¨Écacement architectures
mat¬¥erielles disponibles et r¬¥eduire le temps de solution.
Les deux options n¬¥ecessitent une plateforme de calcul Ô¨Çexible et robuste. Un
cadre Œ†4U, qui r¬¥epond aux deux exigences, a ¬¥et¬¥e r¬¥ecemment d¬¥evelopp¬¥e dans notre
laboratoire. Œ†4U contient des impl¬¥ementations informatiques hautes performances
d‚Äôalgorithmes pour l‚Äôoptimisation, la quantiÔ¨Åcation et la propagation des incerti-
tudes bay¬¥esiennes. Dans cette th`ese, nous ¬¥etendons le cadre Œ†4U pour inclure des
outils d‚Äôinf¬¥erence manquantes, tels qu‚Äôun algorithme pour l‚Äôinf¬¥erence bay¬¥esienne ap-
proximative, des mod`eles de substitution, des outils de visualisation et un outil de
sensibilit¬¥e. Nous utilisons ensuite le cadre ¬¥etendu pour eÔ¨Äectuer la quantiÔ¨Åcation
et la propagation de l‚Äôincertitude bay¬¥esienne dans la dynamique mol¬¥eculaire et la
dynamique des particules dissipatives.
L‚Äôapproche syst¬¥ematique bay¬¥esienne nous
permet d‚Äôam¬¥eliorer les capacit¬¥es pr¬¥edictives des mod`eles `a base de particules de tae
et d‚Äôacqu¬¥erir de nouvelles connaissances sur les champs de force tr`es vieux.
vi

Acknowledgments
I wish to thank my supervisor, Petros Koumoutsakos, for the inspirational ideas
he shared with me and the possibility to work in a place full of amazing people,
with an atmosphere which was always encouraging me to pursue my scientiÔ¨Åc goals.
I would also like to thank my co-supervisor, Costas Papadimitriou, for providing
great advice throughout my PhD studies.
Special thanks to Sergey, for all the cool and enlightening discussions on science,
programming, philosophy and many other topics.
I am also thankful to all of the lab members, present ans past, for their passion
to computational science, which was always a source of inspiration for me. I am
especially thankful to Panos A. and Panos H. as they endorsed my interest in the
Bayesian statistics and High Performance Computing which eventually became the
main theme of this thesis.
I wish to thank my friends Pesho, Yury, Diana and Olek for their moral support
during my academic journey. I am also grateful to Chakri and Athena for brightening
up the days in the lab.
I thank all my tango friends for the great joy of dancing they shared with me.
I am endlessly grateful to my parents and the whole family for their unconditional
trust and support.
vii


Notations
Abbreviations
ABC
Approximate Bayesian Computation
ABC-SubSim
Approximate Bayesian Computation Subset Simulation
BASIS
Bayesian Annealed Sequential Importance Sampling
BMU
Bayesian Model Updating
CG
Coarse-Grained
CMA-ES
Covariance Matrix Adaptation ‚Äì Evolution Strategy
CSE
Computational Science and Engineering
CoV
CoeÔ¨Écient of Variation
DEM
Discrete Element Method
DPD
Dissipative Particle Dynamics
ELM
Extreme Learning Machine
FD
Force-Displacement
HB
Hierarchical Bayesian
LJ
Lennard-Jones
MCMC
Markov Chain Monte Carlo
MD
Molecular Dynamics
MH
Metropolis-Hastings
MLP
Multi-Layer Perceptron
MPI
Message-Passing Interface
NC
Nascent (protein) Chain
NN
Neural Network
PDF
Probability Density Function
PMF
Potential Mean Force
QoI
Quantity of Interest
ix

RDF
Radial Distribution Function
SubSim
Subset Simulation
TMCMC
Transitional Markov Chain Monte Carlo
TTF
Tank-Treading Frequency
UQ
Uncertainty QuantiÔ¨Åcation
UQ+P
Uncertainty QuantiÔ¨Åcation and Propagation
Assumptions
All the probability distributions have PDFs.
Conventions
We denote scalars with small regular letters, vectors with small bold letters,
matrices with capital bold letters.
The length of the vector v is denoted as v.
Each of the sample plots is created in the same way. Diagonal: histograms of
marginal distributions of parameters. Above the diagonal: projection of posterior
samples on all pairs of 2-d parameter spaces colored by discrepancy/log-likelihood.
Below the diagonal: 2-d probability densities constructed via a bivariate kernel
estimate.
Common symbols
œë
vector of stochastic model parameters
œà
vector of stochastic model hyper-parameters
P(¬∑)
probability
p(¬∑)
PDF
x

Contents
1
Introduction
1
1.1
Bayesian Uncertainty QuantiÔ¨Åcation and Its Challenges
. . . . . . .
4
1.2
Large-Scale Models and High Performance Computing . . . . . . . .
6
1.3
This Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
I
Theory
11
2
Bayesian Inference
13
2.1
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2
Stochastic Model Selection . . . . . . . . . . . . . . . . . . . . . . . .
15
2.3
Forward Uncertainty Propagation . . . . . . . . . . . . . . . . . . . .
15
2.4
Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3
Approximate Bayesian Inference
19
3.1
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
19
3.2
Stochastic Model Selection . . . . . . . . . . . . . . . . . . . . . . . .
20
3.3
Forward Uncertainty Propagation . . . . . . . . . . . . . . . . . . . .
20
3.4
Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4
Hierarchical Bayesian Inference
25
4.1
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
26
4.2
Stochastic Model Selection . . . . . . . . . . . . . . . . . . . . . . . .
29
4.3
Forward Uncertainty Propagation . . . . . . . . . . . . . . . . . . . .
29
4.3.1
Propagation for the same QoI . . . . . . . . . . . . . . . . . .
29
xi

4.3.2
Propagation for a new QoI
. . . . . . . . . . . . . . . . . . .
30
4.4
Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
5
Implementation
33
5.1
Œ†4U Framework
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.2
Structure of Œ†4U algorithms
. . . . . . . . . . . . . . . . . . . . . .
35
5.3
Improving MH Sampling . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.4
Surrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.4.1
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
5.4.2
Surrogates with Bayesian inference algorithms
. . . . . . . .
41
5.4.3
Available surrogates in Œ†4U . . . . . . . . . . . . . . . . . . .
43
5.5
HPC Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
5.5.1
ABC-SubSim . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
5.5.2
Surrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
5.6
Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
II
Applications
49
6
Helium Modeling
51
6.1
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
52
6.2
Discrepancy Model Selection
. . . . . . . . . . . . . . . . . . . . . .
56
6.3
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
7
Argon Modeling
59
7.1
Computational Model Description
. . . . . . . . . . . . . . . . . . .
60
7.2
Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
7.3
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
62
7.3.1
Calibration of LJ 6-12 . . . . . . . . . . . . . . . . . . . . . .
62
7.3.2
Calibration of LJ 6-p . . . . . . . . . . . . . . . . . . . . . . .
65
7.3.3
Comparison of LJ 6-12 and LJ 6-p . . . . . . . . . . . . . . .
69
7.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
8
Translocon Modeling
77
8.1
Computational Model
. . . . . . . . . . . . . . . . . . . . . . . . . .
77
8.2
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . . . . . .
78
xii

8.3
Forward Uncertainty Propagation . . . . . . . . . . . . . . . . . . . .
83
8.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
9
Blood Modeling
87
9.1
Computational Model
. . . . . . . . . . . . . . . . . . . . . . . . . .
87
9.1.1
Solvent
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
9.1.2
RBC model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
9.1.3
Simulation Setup . . . . . . . . . . . . . . . . . . . . . . . . .
92
9.2
RBC Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
9.3
Red Blood Cell Stretching . . . . . . . . . . . . . . . . . . . . . . . .
92
9.3.1
Parameters Calibration
. . . . . . . . . . . . . . . . . . . . .
94
9.3.2
Forward Uncertainty Propagation
. . . . . . . . . . . . . . .
98
9.4
Red Blood Cell in Shear Flow . . . . . . . . . . . . . . . . . . . . . .
99
9.4.1
Parameter Calibration . . . . . . . . . . . . . . . . . . . . . .
100
9.4.2
Forward Uncertainty Propagation
. . . . . . . . . . . . . . .
103
9.5
High performance computing
. . . . . . . . . . . . . . . . . . . . . .
106
III
Conclusions and Outlook
109
10 Conclusions and Outlook
111
10.1 Œ†4U Framework
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
10.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Appendix
119
A ABC for Likelihood-Driven Bayesian Inference
119
A.1 Computational Model
. . . . . . . . . . . . . . . . . . . . . . . . . .
119
A.2 Parameter Calibration . . . . . . . . . . . . . . . . . . . . . . . . . .
121
B RBC Parameters Sensitivity
125
C Argon LJ Parameters
127
C.1
Hyper-parameter models for hierarchical inference
. . . . . . . . . .
127
C.2
Posterior PDFs for LJ 6-12 and LJ 6-p . . . . . . . . . . . . . . . . .
129
xiii

D LAMMPS Scripts for the MD Simulations
137
D.1 Helium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
D.2 Argon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
xiv

Chapter 1
Introduction
Computational science is a rapidly growing multidisciplinary Ô¨Åeld. It deals with
complex physical problems using advanced computing techniques. In the core of
this area of science lies the development of computational models and computer
simulations techniques to understand natural systems by making their complexity
tractable.
Here and further, the computational model is a representation of the
mathematical description of a given system in a computer (see Fig. 1.1).
One of the sub-branches of the computational science is computational science
and engineering (CSE), a relatively new discipline that aims at developing and ap-
plying computational models and simulations representing physical reality of engi-
neering and life sciences. The development process is often coupled with the usage
of high performance computing, which allows to simulate large and complex sys-
tems. CSE is nowadays the third method of the scientiÔ¨Åc discovery, together with
the theory and the experiments. Computer simulations provides the capability to
obtain results and observe processes that are either inaccessible in the experiments
or exhibit an extremely high Ô¨Ånancial cost. Its importance in many Ô¨Åelds of study
can hardly be overestimated, while constant progress in the hardware technologies
and software algorithms makes the computer simulations spread wider every day.
The way the computational models are constructed, however, allows for a num-
ber of uncertainties, such as the modeling, the experimental, the parameter and the
numerical ones (see Fig. 1.1). The modeling uncertainty accounts for our imper-
fect understanding of the physical laws, meaning that the equations we use may
1

2
 
 
Physical reality
Predicted value
and uncertainty
Experimental
data
Mathematical
model
Computational
model
Computational
model
parameters
experimental errors
modeling errors
numerical errors
parameter errors
VALIDATION
VERIFICATION
CALIBRATION
Figure 1.1: Schematic representation of the inference process. Red arrows show the
origins of errors. Blue dashed arrows show the information and errors Ô¨Çow.

3
not account for certain eÔ¨Äects, making the model simpler, but less accurate. The
experimental uncertainty is arising in the process of data collection during experi-
mental studies. It appears due to the Ô¨Ånite precision of the measurement devices or
due to some uncontrollable environmental conditions. The parameter uncertainty
is attributed to those parameters of the computational model which are not known
exactly, an example being material properties or the local free-fall acceleration. Fi-
nally, the numerical uncertainty appears when the physical laws are translated into
a computer.
An example of this kind of uncertainty is the discretization of the
partial diÔ¨Äerential equations or numerical integration.
Uncertainties can be classiÔ¨Åed into two categories: aleatoric and epistemic un-
certainty. Aleatoric, or statistical, uncertainty corresponds to unknowns that diÔ¨Äer
each time the experiment is run. An example of such an uncertainty is the uncer-
tainty in the Ô¨Ånal position of an arrow shot with a mechanical bow. Each time we
shoot the same arrow with the exactly same initial conditions: the same accelera-
tion, altitude, direction. However, the random vibrations of the arrow shaft prevent
it from hitting the same point in the end of its Ô¨Çights. These vibrations cannot
be determined suÔ¨Éciently accurately to eliminate the resulting uncertainty in the
Ô¨Ånal arrow position. It should be noticed that the speculations above rely on the
Ô¨Ånite precision of the modern measurement devices, and the current inability to
eliminate the uncertainty in the arrow vibrations does not prove the existence of
the uncertainty which cannot be eliminated with good enough measurements. The
other type of uncertainty is epistemic, or systematic, uncertainty. It appears due to
properties which could be learned, but are unknown in practice. An example of the
situation when such an uncertainty is present is a simulation with a model which
neglects certain eÔ¨Äects, such as measuring the acceleration of gravity without taking
into account the air resistance.
The majority of applications contain both types of uncertainties.
Reduction
in the epistemic uncertainty can be achieved by gaining a better understanding
of the system, process or mechanism. The uncertainty quantiÔ¨Åcation (UQ) aims
at reducing epistemic uncertainties to the aleatoric ones, which can be treated by
statistical tools. The present work focuses on the Bayesian approach to quantifying
the uncertainties.

4
1.1
Bayesian Uncertainty QuantiÔ¨Åcation and Its Challenges
Bayesian Uncertainty QuantiÔ¨Åcation (UQ) is a method of statistical inference. Here,
‚Äúinference‚Äù is the process of deducing properties of the probability distribution of the
parameters œë of a computational model M by analysis of the available experimental
data d. Bayesian UQ uses Bayes‚Äô theorem to update beliefs about the underlying
parameter distribution in the presence of new information:
p(œë | d, M) = p(d | œë, M) p(œë | M)
p(d | M)
.
(1.1)
The term on the left is called ‚Äúposterior distribution‚Äù. It describes the updated
beliefs about the parameter distribution after observing the data. The Bayes‚Äô theo-
rem states that, in order to compute the posterior distribution, one should ‚Äúreweigh‚Äù
the prior beliefs p(œë | M) with the likelihood p(d | œë, M). The prior distribution
summarizes all the information available to the researcher before observing any data.
This information usually includes expert opinions or physical constraints. The like-
lihood, on the other hand, speciÔ¨Åes the probability of observing the experimental
data d given a set of parameters œë. The likelihood incorporates the assumptions
made about the data collection process and the feasibility of the model, such as:
what kind of uncertainties were present in the experiment and simulation or how
well does the model with the best parameters Ô¨Åt the data. The choice of the prior
and the formulation of the likelihood is discussed in detail in Part I.
The last term of the Bayes‚Äô theorem is the denominator on the right hand side.
This term is called ‚Äúmodel evidence‚Äù and is computed as a normalisation constant:
p(d | M) =
Z
p(d | œë, M) p(œë | M) dœë .
(1.2)
The model evidence is used to select the most probable model M among the com-
peting ones.
The Bayes‚Äô theorem was Ô¨Årst stated by Thomas Bayes (1701-1761) in 1763 in
his treatise ‚ÄúAn Essay towards solving a Problem in the Doctrine of Chances‚Äù. The
modern formulation of the Bayes rule appeared in 1812 in ‚ÄúTh¬¥eorie analytique des
probabilit¬¥es‚Äù of Pierre-Simon Laplace.
Although the theorem was formulated a long time ago, it is only recently, after
almost 200 years, that it is gaining an attention of numerous scientists and engineers.
The reason for that lies in the model evidence.

1.1. Bayesian Uncertainty QuantiÔ¨Åcation and Its Challenges
5
As was discussed above, the evidence is computed as an integral over the param-
eter space. The dimensionality of this space depends on the problem and in general
can be quite high. Integration in high-dimensional spaces is a challenging problem,
since the quadrature schemes need a large number of points to work with a suÔ¨Écient
accuracy. Although there exist methods, which avoid the curse of dimensionality,
such as techniques of the Monte Carlo family [73], they still require a signiÔ¨Åcant
amount of points to converge.
A solution is oÔ¨Äered by the Markov Chain Monte Carlo (MCMC) method [53, 49].
It is able to sample probability distributions known up to a normalizing constant,
which is precisely the case in the Bayesian inference. The advantage of this method,
as compared to the integration approach, is that it only focuses on the area of
non-zero probability, thus not wasting precious points.
However, MCMC method is also not able to completely eliminate the problem of
high computational cost associated with the Bayesian inference. Each point in the
parameter space used by the MCMC algorithm required evaluating the likelihood,
which can be an expensive computation by itself, an example being a full molecular
dynamics simulation. The ways of eliminating the high computational cost of the
Bayesian inference are discussed in Chapter 5.
Another problem with the MCMC method arises in case of complex posterior
distributions: multi-modal, peaked or the ones with unidentiÔ¨Åable manifolds. The
algorithm in this case can get trapped in one of the peaks or fail to sample the
whole manifold. Hence, the modern MCMC-based sampling algorithms went quite
far away from the original version [22, 16], focusing on various population-based
annealing schemes [29, 26, 23].
Nevertheless, none of the algorithms acts perfectly in each situation, and the
researchers try to target diÔ¨Äerent sampling problems separately. One part of the
research addresses the sampling of the complex PDFs [21], including multi-modal
distributions [90, 108], cases with unidentiÔ¨Åable manifolds [14, 74], high-dimensional
distributions [24, 120]. Another part of the research is concentrated on developing
algorithms for special types of models (e.g. stochastic models [115, 13, 104, 124])
and data (e.g. multiple data sets [110, 75, 54, 129]).
The state-of-the-art algorithms for Bayesian inference will be further discussed
in Chapter 2.
Even more diÔ¨Éculties arise when one starts challenging the assumptions of the

6
classical Bayesian inference. For example, what happens if the model under consid-
eration is stochastic? Or what is a meaningful way of using several diÔ¨Äerent datasets
in one calibration? The answers to these and other questions will be discussed in
Part I.
1.2
Large-Scale Models and High Performance Computing
An increasing number of areas are now using computer-aided modeling. The soft-
ware serves diÔ¨Äerent purposes and runs on various architectures. The simulations
are written in virtually all the existing programming languages. The hardware ar-
chitectures also vary: single-core and multi-core CPUs, GPUs, and hybrid clusters.
The parallelisation and communication methods are numerous: Message Passing
Interface (MPI), CUDA, OpenMP, POSIX threads, and many libraries.
In such a complex world, it becomes a necessity to have a uniÔ¨Åed framework
for Bayesian inference. A non-exhaustive list of options includes packages devel-
oped for a speciÔ¨Åc Ô¨Åeld: BEAST [34], BAli-Phy [114], DIYABC [27] ‚Äì phylogeny,
BioBayes [122] ‚Äì systems biology, MultiNest [44] ‚Äì cosmology, BayesSDT [81] ‚Äì sig-
nal detection theory, and general purpose packages: MCMCpack [87], Stan [51],
BUGS [89], NIMBLE [28].
The special purpose packages lack generality, and the mentioned general purpose
packages target computationally inexpensive models (running up to several minutes
on one CPU) and are not suitable for heavy parallel computations. That is why we
use a general-purpose High Performance Computing (HPC) framework Œ†4U [60, 59],
which meets both requirements. This framework can be compiled and run on any
architecture from a laptop to a hybrid-cluster in a platform-agnostic way. With a
help of task-based parallel library TORC [58], Œ†4U performs job scheduling and
adaptive load balancing. A transparent C-based interface allows the user to call any
kind of a simulation engine on any architecture.
In this thesis we shall give examples of using Œ†4U coupled with LAMMPS [1, 93]
simulation which uses MPI parallelisation and a single-core python script.
The
applications which are studied in this thesis run up to several hours on 8 CPUs
(argon simulations) or 2 hours on a GPU (blood cells simulations).

1.3. This Thesis
7
1.3
This Thesis
Throughout this thesis we present a consistent approach to Bayesian inference for
various large-scale models. We discuss both the algorithmic and the implementation
challenges and present ways to address them.
This thesis contains two main achievements.
The Ô¨Årst one is extending our
in-house open-source high performance computing framework Œ†4U with sampling
algorithms for approximate Bayesian computations, surrogate models and additional
auxiliary tools.
The second one is enhancing the predictive capabilities of four
particle-based computational models from engineering and life sciences with the
help of Œ†4U.
This thesis consists of three main parts, brieÔ¨Çy introduced here.
Part I: Theory
In this part, we present the general Bayesian inference theory and its implemen-
tation inside the Œ†4U library. We consider the Bayesian Model Updating (BMU),
the Approximate Bayesian Computation (ABC) and the Hierarchical Bayesian (HB)
inference. For each of these cases we present the necessary background for the pa-
rameter calibration, model selection and forward uncertainty propagation, including
the algorithms needed for each of the inferences. We then present an overview of
Œ†4U, which incorporates all the algorithms needed for the Bayesian inference. We
continue by discussing methods for improving sampling and reducing the compu-
tational cost of the Bayesian inference. This is when the surrogate models come
into play. After giving a general context of the surrogate usage within the Bayesian
inference, we demonstrate two types of surrogate models recently added to the Œ†4U
framework: Gaussian process-based and neural network-based. Finally, we give the
speciÔ¨Åcations of the computational platforms used in the calculations throughout
this thesis.
Part II: Applications
This part gives a description of the four particle-based models from life sciences
and engineering, which we improved using various types of the Bayesian inference.
The models are the following: molecular dynamics simulations of helium and argon,

8
coarse-grained modeling of proteins integration/translocation into a cell membrane,
dissipative particle dynamics simulations of a red blood cell.
Helium Modeling
This chapter addresses a problem of calibrating MD force-
Ô¨Åelds using stochastic quantities with non-Gaussian Ô¨Çuctuations as data. We infer
the Lennard-Jones (LJ) potential well depth and the well location parameters for
helium using the data on Boltzmann factor distribution over time. It is known from
the statistical physics, that the Boltzmann factor exhibits non-Gaussian Ô¨Çuctuations,
therefore we are not able to formulate the likelihood, and we instead apply the ABC.
Using three diÔ¨Äerent functions describing the discrepancy between the data and the
simulation outcome, we conclude that the mean and variance-based discrepancy,
which mimics the likelihood-driven Bayesian inference, is not able to reduce the prior
uncertainty in the LJ parameters. In accordance with the work of Shell [109], we
observe that for the MD quantities of interest with unknown Ô¨Çuctuations, a natural
choice of the discrepancy measure is the relative entropy. The result of this chapter
is thus an approach to MD calibrations using data with arbitrary Ô¨Çuctuations: we
suggest applying ABC with relative entropy as discrepancy function.
Argon Modeling
In this chapter, we focus on identifying parameters of an MD
force-Ô¨Åeld using many sets of data.
This time we choose argon as a target and
consider data on its Radial Distribution Function (RDF) for six diÔ¨Äerent thermody-
namic conditions. We use a modiÔ¨Åed LJ 6-p potential and infer its well depth and
well location parameters, as well as the repulsion exponent p. This modiÔ¨Åcation is
inspired by the fact that, unlike the value 6 for the attraction exponent, the power
12 for the repulsive term has no theoretical justiÔ¨Åcation. Following the methodol-
ogy from Ref. [129], we apply the Hierarchical Bayesian (HB) techniques to treat
the distinct datasets appropriately. Our results demonstrate that the adjusted re-
pulsive exponent of 6.5 improves the prediction of the RDF, density and diÔ¨Äusion
coeÔ¨Écient for liquid argon and saturated argon vapour. At the same time, we show
that the diÔ¨Äerent thermodynamic states, such as liquid, saturated vapour or gas,
should be simulated with diÔ¨Äerent parameters. The result of this chapter is thus
the improvement of the LJ potential for argon with the use of the HB inference.
Translocon Modeling
This chapter contains an example of a Bayesian inference
in a relatively high-dimensional parameter space.
We calibrate a coarse-grained

1.3. This Thesis
9
model of a transmembrane channel, called ‚Äútranslocon‚Äù. The model describes the
process of protein translation by a ribosome and its further movement through the
translocon. There are four model parameters describing two diÔ¨Äerent LJ integrations
and Ô¨Åve geometrical parameters.
Due to a relatively high dimensionality of the
problem, we develop a sequence of structural model which assess diÔ¨Äerent sets of
the translocon parameters. Using Bayesian model selection, we choose the most
plausible structural model.
With this model, we predict the probability for the
newly produced protein to be integrated into the cell membrane (versus it being
translocated inside the cell). The translocon model with the calibrated parameters
shows a considerably better agreement with the experiments.
The result of this
chapter is a demonstration of an approach to calibrating models with a number of
parameters challenging for the Bayesian inference. With this approach, we have
signiÔ¨Åcantly improved the coarse-grained translocon model from Ref. [91].
Red Blood Cell Modeling
In this chapter, an inference based purely on sur-
rogates constructed oÔ¨Ä-line is considered. We calibrate a coarse-grained dissipative
particle dynamics-based model of red blood cell from Ref. [97]. We employ exper-
imental data on the cell behavior when stretched with optical tweezers and when
placed in a simple shear Ô¨Çow. Using the sensitivity analysis, we discover which pa-
rameters are active in each of the experiments. Pre-computing the results of the
simulations on a grid formed from the sensitive parameters, we construct polynomial
approximations of the results for all the parameter space. In the further inference,
only these surrogates are used. The inference outcome demonstrates a good agree-
ment with the experiments for the considered red blood cell model. The result of
this chapter is an example of a very accurate Bayesian inference performed with sur-
rogates only. SpeciÔ¨Åcally, we have calibrated a red blood cell model from Ref. [97]
and showed that it reproduces well the considered experimental data.
Part III: Conclusions and Outlook
This chapter presents the main results of the thesis for both implementation and
applications. We conclude with discussing the open issues and possible directions
of the future work.

10
Appendix
The Appendix consists of four chapters. The Ô¨Årst chapter presents the use of ABC
for the likelihood-driven case. The second chapter displays the parameter sensitivity
plots for the blood cell. The third chapter provides the hyper-prior models used for
the hierarchical inference of argon and gives the sampling results of the individual
Bayesian inference for each of the thermodynamic conditions. The fourth chapter
contains the LAMMPS scripts used for helium and argon computations.

Part I
Theory
11


Chapter 2
Bayesian Inference
Building computational models in various branches of science goes through many
stages: conducting an experiment demonstrating some phenomenon, proposing its
mathematical explanation (mathematical model), simulating the phenomenon in a
computer (computational model), which includes adjustment of the computational
model in order for it to best reproduce the experiment. Each of the stages introduces
an uncertainty in the resulting computational model. The experimental uncertainty
comes from the conditions of the experiment which cannot be controlled: human
factor, environmental factor, intrinsic stochasticity of the system etc. The model
uncertainty accounts for our imperfect understanding of the physical world, point-
ing out that the best existing mathematical model is often, if not always, only an
approximation of the physical reality. The computational uncertainty comes from
the simulations and incorporates the numerical errors introduced into the mathe-
matical model. The sampling uncertainty appears in the model adjustment process
and is related to the fact that many parameters may have an equal probability of
producing an outcome equal to the experimental one.
The Bayesian approach aims at calibrating all the uncertainties appearing in the
computational model adjustment process for making robust predictions with this
model. The approach is probability-based and provides a distribution over a space
of the possible parameters of the computational model. Using a uniÔ¨Åed theory, it
can answer all kinds of probability-related questions concerning the computational
model, its parameters and predictions, including selecting the most probable param-
13

14
Chapter 2. Bayesian Inference
eters for a given model or the most probable model out of a set of the competing
ones.
This chapter contains the theory needed for the likelihood-driven Bayesian un-
certainty quantiÔ¨Åcation of the parameters œë of a computational model M given the
data d.
2.1
Parameters Calibration
Bayesian inference allows to calibrate the computational model parameters given
data on some QoI. This is accomplished using Bayes‚Äô theorem and a prediction
error equation Eq. (2.1) used to formulate the likelihood function:
Ô£±
Ô£≤
Ô£≥
y
= f(œë) + Œ∂e + Œ∂m + Œ∂c ,
Œ∂Œæ
‚àºN(0, Œ£Œæ),
Œæ ‚àà{e, m, c} ,
(2.1)
where f(œë) is the outcome of the computational model with parameters œë, y is
the stochastic model prediction, Œ∂e is the experimental (measurement) error, Œ∂m is
the model error, Œ∂c is the computational (simulation) error, Œ£Œæ are the covariance
matrices of the multivariate Gaussian distributions N(Œ∂Œæ | 0, Œ£Œæ) , assumed for each
of the error terms Œ∂Œæ. The graph corresponding to the Eq. (2.1) is referred to as
stochastic model and is denoted by M. Using the prediction error equation Eq. (2.1)
and the experimental data d, one can compute the likelihood p(y = d | œë):
p(d | œë, M) = N(d | f(œë), Œ£e + Œ£m + Œ£c, M) .
(2.2)
After having computed the likelihood function, Bayes‚Äô theorem allows to obtain
the posterior distribution of the model parameters œë:
p(œë | d, M) = p(d | œë, M) p(œë | M)
p(d | M)
,
(2.3)
where p(œë | M) is the prior probability density function (PDF) of the parameters œë
and p(d | M) is the normalization constant called model evidence, given by
p(d | M) =
Z
p(d | œë, M) p(œë | M)dœë .
(2.4)

2.2. Stochastic Model Selection
15
2.2
Stochastic Model Selection
The Bayesian framework allows one to select from a set of competing stochastic
models M(k), k = 1, . . . , K a model M(j) which Ô¨Åts the data best. The criterion for
the model selection comes from the Bayes‚Äô theorem, which computes the probability
of a stochastic model M(k) as
P(M(k) | d) = p(d | M(k)) P(M(k))
p(d)
,
(2.5)
where p(M(k)) is the prior PDF of the model M(k) and p(d | M(k)) is computed
by Eq. (2.4).
According to Eq. (2.5), the best model of the competing models
M(k), k = 1, . . . , K is the one with the highest probability.
2.3
Forward Uncertainty Propagation
The uncertainty in the model parameters can be further propagated into a QoI q(œë)
using the following formula [6, 60]:
p(q | d, M) =
Z
p(q, œë | d, M)dœë
=
Z
p(q | œë, M) p(œë | d, M)dœë
‚âà1
Ns
Ns
X
k=1
p(q | œëk, M) , where œëk ‚àºp(œë | d, M) .
(2.6)
According to the prediction error equation (Eq. (2.1)), the conditional distribu-
tion p(q | œëk, M) is Gaussian with mean q(œë) and covariance matrix Œ£m+Œ£c, where
the matrix Œ£m is known only for the QoI that was used in the inference process,
i.e. for q(œë) ‚â°f(œë). For other QoIs this matrix is usually set to zero, such that
only the parameter uncertainty and the computational uncertainty contribute to the
uncertainty of q(œë).
2.4
Algorithms
In order to compute the posterior distribution p(œë | d, M) (see Eq. (2.3)), one needs
to deal with the evidence p(d | M), which requires evaluating a high-dimensional
integral (Eq. (2.4)).
To overcome this problem, available Markov Chain Monte

16
Chapter 2. Bayesian Inference
Carlo (MCMC) methods can be used to eÔ¨Éciently generate samples from the pos-
terior distribution [65, 53, 49, 52, 95].
In our work we use the unbiased version
of the Transitional Markov Chain Monte Carlo (TMCMC) algorithm [26], called
BASIS [106]. The pseudocode for both algorithms is given in Algorithm 1 (the con-
ditioning on model M is omitted). An additional helpful feature of the TMCMC
(and, consequently, BASIS) is that the model evidence is computed as a by-product
during the run.
Algorithm 1 TMCMC/BASIS
// Initialization
Select the population size N
Set the model evidence: E = 1
Sample œën from the prior p(œë), n = 1, . . . , N
// Main loop
for (Œ± = 0; Œ± ‚â§1; Œ± = Œ± + Œ¥Œ±) do
1. Evaluate plausibility weights wn = p(d | œën)Œ¥Œ±, where Œ¥Œ± > 0 is chosen such
that the CoV of the weights equals to the prescribed value (usually 1)
2. Select distinct samples s‚Ñì= œën‚Ñì, with probability Àúw‚Ñì= wn‚Ñì/ PN
n=1 wn as
leaders for the next stage. The selection process continues until PL
‚Ñì=1 c‚Ñì= N,
where c‚Ñìis the number of times the sample œën‚Ñìgets selected and L is the
number of samples s‚Ñì.
3. Simulate Markov chains:
for (‚Ñì= 1; ‚Ñì‚â§L; ‚Ñì= ‚Ñì+ 1) do
TMCMC: Run MH starting from s‚Ñìto produce c‚Ñìsamples.
BASIS: Run c‚ÑìMHs starting from s‚Ñìto produce 1 sample each. The proposal
covariance Œ£‚Ñìis user-deÔ¨Åned (see Section 5.3 for details). The probability of
acceptance for the sample œë‚Ä≤ obtained from the sample œë is
r(œë, œë‚Ä≤) = min

1, p(œë‚Ä≤) p(d | œë‚Ä≤)Œ¥Œ±
p(œë) p(d | œë)Œ¥Œ±

.
(2.7)
end for
4. Replace œën, n = 1, . . . , N with these newly produced samples
5. Update the evidence: E = E PN
n=1 wn/N
end for

2.4. Algorithms
17
BASIS hinges on accurate tuning of its parameters, especially of the proposal
covariance inside the Metropolis-Hastings. This issue is addressed in Chapter 5.


Chapter 3
Approximate Bayesian Inference
This chapter contains the theory needed for the likelihood-free Bayesian uncertainty
quantiÔ¨Åcation of the parameters œë of a computational model M given the data d.
In many cases the likelihood function can be theoretically or computationally
intractable, as in the case of stochastic reaction networks [85] and stochastic models
in general. To remedy this, the Approximate Bayesian Computation (ABC) [85, 99]
framework was introduced as a likelihood-free counterpart of the classical Bayesian
inference methodology. Examples of its widespread usage include stochastic popu-
lation models in ecology [107, 117, 12], biology [123, 101] and chemistry [83].
The inference performed in ABC relaxes the need to formulate a likelihood for
a probabilistic prediction error model [15], so that one can use summary statistics
of the QoI to calibrate the model parameters. This is achieved as the likelihood
is replaced with a discrepancy measure that compares predictions of a stochastic
model to observed data [119].
3.1
Parameters Calibration
The main idea of ABC is to estimate the joint PDF p(œë, y | d, M) of the parameter
set œë and the stochastic model prediction y. This can be done by applying Bayes‚Äô
Theorem and the chain rule:
p(œë, y | d, M) = p(œë | M) p(y | œë, M) p(d | œë, y, M)
p(d | M)
,
(3.1)
19

20
Chapter 3.
Approximate Bayesian Inference
where p(d | œë, y, M) is approximated as
p(d | œë, y, M) ‚âàP(œÅ(y, d) ‚â§Œ¥ | y, M)
(3.2)
for an arbitrary value of Œ¥ ‚â•0 and some discrepancy function œÅ(¬∑, ¬∑) assigning smaller
values for pairs (y, d) in which the arguments are closer to each other. Note that
P(œÅ(y, d) ‚â§Œ¥ | y, M), as a function of y, can only take value 0 or 1. For Œ¥ ‚Üí‚àû, the
approximate posterior equals the prior since the approximate likelihood contribution
is Ô¨Çat. As Œ¥ ‚Üí0 the approximate posterior tends to the exact one.
Using Eq. (3.2), the approximate joint posterior PDF (3.1) reads:
pŒ¥(œë, y | d, M) = p(œë | M) p(y | œë, M) P(œÅ(y, d) ‚â§Œ¥ | y, M)
p(d | M)
‚àùp(œë | M) p(y | œë, M) P(œÅ(y, d) ‚â§Œ¥ | y, M) .
(3.3)
Finally, the approximate posterior PDF pŒ¥(œë | d, M) can be obtained by marginal-
ization of Eq. (3.3) with respect to y.
It is worth noticing that ABC can be also used with the likelihood-based stochas-
tic forward model (Eq. (2.1)) introducing however an additional uniformly dis-
tributed error term which depends on the discrepancy function and the tolerance
Œ¥ [127]. An example of such a use and its comparison with TMCMC is given in
Appendix A.
3.2
Stochastic Model Selection
The model selection for the exact Bayesian inference makes use of the model evidence
(see Eq. (3.4)). In case of ABC, the model evidence can only be approximated:
p(d | M) ‚âàP((œë, y) | œÅ(y, d) ‚â§Œ¥, M)
=
Z
P((œë, y) | œÅ(y, d) ‚â§Œ¥, M) p(y | œë, M) p(œë | M) dœëdy ,
(3.4)
The bigger the Œ¥, the larger the error of the model evidence in ABC, which is a
well-known limitation of this approach [30, 103].
3.3
Forward Uncertainty Propagation
The forward uncertainty propagation is done in the same way as for the exact
Bayesian inference (see Eq. (2.6)) with a replacement of the posterior p(œë | d, M)
with the approximate posterior pŒ¥(œë | d, M).

3.4. Algorithms
21
3.4
Algorithms
A wide variety of stochastic methods exist for the posterior inference in ABC. A
non-exhaustive list can be found in [12, 25, 113].
In order to deal with complex posterior PDF supports and peaked distributions,
an extension has been proposed to the established algorithm Subset Simulation (Sub-
Sim) [10] used for estimating rare events. For the ABC framework, the rare event is
to Ô¨Ånd samples in the parameter space, such that the corresponding computational
model outcome is close enough to the experimental measurements. The correspond-
ing algorithm, named ABC-SubSim [25], can be used for posterior sampling within
the ABC framework. Its pseudocode is given in Algorithm 2 (the conditioning on
model M is omitted). Note that in order to obtain the marginal approximate pos-
terior PDF pŒ¥(œë | d), the algorithm simply needs to take the œë-components of the
generated samples.
As in the case of TMCMC/BASIS, the model evidence is computed as a side-
product of the ABC-SubSim run.
Additionally, ABC-SubSim allows for perfect
sampling parallelization (assuming that the run time of the simulation does not de-
pend on the input parameters), as all the Markov chains have the same length.
The
parameters of choice for ABC-SubSim are the spread of the proposal covariance and
the length of Markov chains. The former can be chosen as in the TMCMC/BASIS
(see Chapter 5), the latter is recommended to be set to 5 [25].
A new problem, as compared to TMCMC/BASIS, is the stopping criterion.
There is no rigid convergence criterion for the termination of ABC-SubSim, thus
leaving us with empirical and computational insights. Some of the heuristics which
can be used are:
1) reaching a speciÔ¨Åed change in the tolerance,
2) reaching a speciÔ¨Åed change in some QoI,
3) reaching a speciÔ¨Åed acceptance rate in the MH.
If the QoI is the stochastic model prediction y, the criterion 1) provides an estimate
on how much the approximation of the QoI improved, which coincides with the
criterion 2). The stopping criterion 3) comes into play when samples get rejected
not because of getting in the area of big discrepancy values in the œë-space, but
because of the natural variability of y. In that case tuning the proposal covariance

22
Chapter 3.
Approximate Bayesian Inference
Algorithm 2 ABC-SubSim
// Initialization
Select the population size N and the Markov chain length c, s.t. L = N/c is
integer
Set the model evidence: E = 1
Sample œën from the prior p(œë) and yn from p(y | œë), n = 1, . . . , N
// Main loop
while stopping conditions are not met do
1. Evaluate discrepancies œÅn, n = 1, . . . , N and renumber the samples s.t. their
discrepancies are sorted in the ascending order; set Œ¥ = 1
2 (œÅL + œÅL+1).
2. Select samples s‚Ñì= (œë‚Ñì, y‚Ñì), ‚Ñì= 1, . . . , L as leaders for the next stage
3. Simulate Markov chains:
for (‚Ñì= 1; ‚Ñì‚â§L; i = i + 1) do
Run MH starting from s‚Ñìto produce c samples. The proposal covariance Œ£‚Ñì
is user-deÔ¨Åned (see Section 5.3 for details). The probability of acceptance for
the sample (œë‚Ä≤, y‚Ä≤) obtained from the sample (œë, y) is
r(œë, œë‚Ä≤) = min

1, p(œë‚Ä≤)
p(œë)

P(œÅ(y‚Ä≤, d) ‚â§Œ¥ | y‚Ä≤) .
(3.5)
end for
4. Replace (œën, yn), n = 1, . . . , N with these newly produced samples
5. Update the evidence: E = E/c
end while
Report the Ô¨Ånal tolerance Œ¥

3.4. Algorithms
23
matrix does not help since even for a Ô¨Åxed œë from a good region some samples y
can have a too high discrepancy to be accepted at the current stage.


Chapter 4
Hierarchical Bayesian Inference
This chapter contains the theory needed for the likelihood-driven hierarchical Bayesian
uncertainty quantiÔ¨Åcation of the parameters œë of a computational model M given
a collection of datasets #¬ªd.
In certain cases the data may come split in several datasets. One example of
such a situation is when many research groups conduct the same experiment. Then
the observed dependency can have the same functional form, but the underlying pa-
rameters may be diÔ¨Äerent (see Fig. 4.1 for an example). Another possibility occurs
in personalized medicine, where the parameter variability is attributed to the indi-
vidual characteristics of a patient. Finally, the datasets may correspond to diÔ¨Äerent
input variables x of the model, one of the examples being pressure and temperature
of a system in molecular dynamics. In such cases it is desirable to distinguish the
uncertainty which is due to the variability between diÔ¨Äerent datasets (groups, ther-
modynamic conditions, patients) from the uncertainty encoded in each individual
dataset. This diÔ¨Äerentiation is useful for making predictions for the future, having
observed some data in the beginning of the time line. An example can be predicting
the slope of the linear model in Fig. 4.1 if we know points up to x = 1. Not taking
into account the variability of the slopes between diÔ¨Äerent datasets, we will make
a prediction with wide errorbars from the biggest slope to the smallest, instead of
noticing that we can predict the linear Ô¨Åt almost perfectly from some initial points.
The methodology, which distinguishes the individual variability of parameters
from the collective one, is called ‚ÄúHierarchical Bayesian (HB) inference‚Äù. In this
25

26
Chapter 4.
Hierarchical Bayesian Inference
chapter, we describe an approach to the HB inference proposed in Ref. [129].








Figure 4.1: A synthetic example of a hierarchical data structure. All the datasets
exhibit a linear behavior y = œëx with diÔ¨Äerent slopes œë. The hyper-parameter œà
governs the prior distribution of the slopes: p(œë | œà) = U(œë; œà1, œà2).
4.1
Parameters Calibration
We consider a case when data for the QoI f(œë) comes split in K diÔ¨Äerent datasets:
#¬ªd = {d(1), . . . , d(K)}. The likelihood in the probabilistic model M(k) corresponding
to the dataset k is assumed to be N(d(k) | f(œë(k)), Œ£(k)), where Œ£(k) is user-deÔ¨Åned.
We also assume that the probability of œë(k) depends on a hyper-parameter œà ‚ààRNœà
and is given by a PDF p(œë | œà, M), where M corresponds to the graph describing
the relations between œà, œë(k) and d(k), see Fig. 4.2.
Our goal is to obtain samples from the posterior distributions p(œë(k) | #¬ªd, M):
p(œë(k) | #¬ªd, M) =
Z
p(œë(k) | œà, #¬ªd, M) p(œà | #¬ªd, M) dœà .
(4.1)
The dependency assumptions from Fig. 4.2 allow to simplify:
p(œë(k) | œà, #¬ªd, M) = p(œë(k) | œà, d(k), M) ,
(4.2)
and Eq. (4.1) can be rewritten using the Bayes‚Äô theorem:
p(œë(k) | #¬ªd, M) =
Z p(d(k) | œë(k), œà, M) p(œë(k) | œà, M)
p(d(k) | œà, M)
p(œà | #¬ªd, M) dœà .
(4.3)

4.1. Parameters Calibration
27
œà
œë(1)
œë(k)
œë(K)
d(K)
d(k)
d(1)
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
œënew
ynew
y(k)
new
Mnew
M(1)
M(k)
M(K)
Figure 4.2: Hierarchical Bayesian network.
Since, from Fig. 4.2 we have
p(d(k) | œë(k), œà, M) = p(d(k) | œë(k), M) ,
(4.4)
Eq. (4.3) simpliÔ¨Åes to
p(œë(k) | #¬ªd, M) =
p(d(k) | œë(k), M)
Z p(œë(k) | œà, M)
p(d(k) | œà, M) p(œà | #¬ªd, M) dœà .
(4.5)
Finally, as suggested in Ref. [129], the posterior distribution Eq. (4.1) can be ap-
proximated as
p(œë(k) | #¬ªd, M) ‚âàp(d(k) | œë(k), M)
Nsœà
Nsœà
X
i=1
p(œë(k) | œài, M)
p(d(k) | œài, M) ,
(4.6)
where Nsœà is suÔ¨Éciently large, œài ‚àºp(œà | #¬ªd, M) and p(œë(k) | œài, M) is a user-
deÔ¨Åned PDF called ‚Äúhyper-prior‚Äù.
We see that, in order to obtain samples œë(k), we Ô¨Årst have to sample the proba-
bility distribution p(œà | #¬ªd, M). According to Bayes‚Äô theorem, we write
p(œà | #¬ªd, M) = p(#¬ªd | œà, M) p(œà | M)
p(#¬ªd | M)
,
(4.7)

28
Chapter 4.
Hierarchical Bayesian Inference
where p(œà | M) is the prior PDF on œà and p(#¬ªd | M) is the normalizing constant.
Exploiting the dependency assumption of Fig. 4.2, we see that
p(#¬ªd | œà, M) =
K
Y
k=1
p(d(k) | œà, M) .
(4.8)
The likelihood of k-th dataset is expressed according to the total probability theorem
as
p(d(k) | œà, M) =
Z
p(d(k), œë(k) | œà, M) dœë(k) ,
(4.9)
which can be written, using the chain rule, as
p(d(k) | œà, M) =
Z
p(d(k) | œë(k), œà, M) p(œë(k) | œà, M) dœë(k) .
(4.10)
According to Eq. (4.4), one can simplify Eq. (4.13) to
p(d(k) | œà, M) =
Z
p(d(k) | œë(k), M) p(œë(k) | œà, M) dœë(k) .
(4.11)
Here we introduce the model M(k) described in Fig. 4.2. The posterior distribution
of this model will be used as instrumental density for importance sampling. Under
the modeling assumption p(d(k) | œë(k), M) = p(d(k) | œë(k), M(k)) (see [129]) and the
use of the Bayes‚Äô theorem, Eq. (4.9) is written as
p(d(k) | œà, M) =
Z p(œë(k) | d(k), M(k)) p(d(k) | M(k))
p(œë(k) | M(k))
p(œë(k) | œà, M) dœë(k) , (4.12)
or, equivalently, as
p(d(k) | œà, M) = p(d(k) | M(k))
Z p(œë(k) | œà, M)
p(œë(k) | M(k)) p(œë(k) | d(k), M(k)) dœë(k) . (4.13)
Referring again to Ref. [129], one approximates Eq. (4.9) as
p(d(k) | œà, M) ‚âàp(d(k) | M(k))
Nsœë
Nsœë
X
i=1
p(œë(k)
i
| œà, M)
p(œë(k)
i
| M(k))
,
(4.14)
where œë(k)
i
‚àºp(œë(k) | d(k), M(k)), Nsœë is suÔ¨Éciently large and can be diÔ¨Äerent
for each dataset d(k).
The advantage of this approach is that the likelihoods
p(d(k) | œë(k), M(k)), k = 1, . . . , K, which are the most expensive part of the compu-
tations, are not re-evaluated for each œà.

4.2. Stochastic Model Selection
29
The Ô¨Ånal formula for the posterior probability of the hyper-parameters will thus
look as follows:
p(œà | #¬ªd, M) ‚âàp(œà | M)
p(#¬ªd | M)
K
Y
k=1
p(d(k) | M(k))
Nsœë
Nsœë
X
i=1
p(œë(k)
i
| œà, M)
p(œë(k)
i
| M(k))
,
(4.15)
where œë(k)
i
‚àºp(œë(k) | d(k), M(k)), Nsœë is suÔ¨Éciently large and can be diÔ¨Äerent for
each dataset d(k).
4.2
Stochastic Model Selection
In the case of hierarchical inference, one can select the model M, which reduces
to selecting the most probable model for the hyper-prior PDF p(œë | œà, M) in the
hyper-parameter inference Eq. (4.7). This selection is performed in the same way
as for the non-hierarchical Bayesian inference in Chapter 2.
4.3
Forward Uncertainty Propagation
The uncertainty propagation for HB case has two cases:
1) propagation for the same QoI ‚Äì case y(k)
new on Fig. 4.2,
2) propagation for a new QoI ‚Äì case ynew on Fig. 4.2.
4.3.1
Propagation for the same QoI
The predictive distribution of the QoI that was used in the calibration process (y(k)
new
on Fig. 4.2), can be computed in the following way:
p(y(k)
new | #¬ªd, M) =
Z
p(y(k)
new, œë(k) | #¬ªd, M) dœë(k)
=
Z
p(y(k)
new | œë(k), #¬ªd, M) p(œë(k) | #¬ªd, M) dœë(k) .
(4.16)
Since the knowledge of œë(k) breaks the dependence between the data #¬ªd and the
prediction y(k)
new, Eq. (4.16) simpliÔ¨Åes to
p(y(k)
new | #¬ªd, M) =
Z
p(y(k)
new | œë(k), M) p(œë(k) | #¬ªd, M) dœë(k) ,
(4.17)

30
Chapter 4.
Hierarchical Bayesian Inference
which, in turn, can be approximated by
p(y(k)
new | #¬ªd, M) ‚âà
Npo
X
i=1
p(y(k)
new | œë(k)
i
, M) ,
(4.18)
where Npo is suÔ¨Éciently large and œë(k)
i
‚àºp(œë(k) | #¬ªd, M), which can be computed
from Eq. (4.6).
Note that the hyper-parameter vector œà enters into Eq. (4.18)
indirectly through the full dataset #¬ªd.
4.3.2
Propagation for a new QoI
The predictive distribution of a QoI that was not used in the calibration process
(ynew on Fig. 4.2), can be computed in the following way:
p(ynew | #¬ªd, M) =
Z
p(ynew, œënew | #¬ªd, M) dœënew ,
(4.19)
which, according to the chain rule, is equivalent to
p(ynew | #¬ªd, M) =
Z
p(ynew | œënew, #¬ªd, M) p(œënew | #¬ªd, M) dœënew .
(4.20)
Knowing œënew makes the data #¬ªd and the prediction ynew independent, simplifying
Eq. (4.20) to
p(ynew | #¬ªd, M) =
Z
p(ynew | œënew, M) p(œënew | #¬ªd, M) dœënew ,
(4.21)
which can be Ô¨Ånally approximated as
p(ynew | #¬ªd, M) ‚âà
Npn
X
i=1
p(ynew | œënew,i, M) ,
(4.22)
where œënew,i ‚àºp(œënew | #¬ªd, M) and Npn is suÔ¨Éciently large.
The last distribution we need, p(œënew | #¬ªd, M), can be computed as
p(œënew | #¬ªd, M) =
Z
p(œënew, œà | #¬ªd, M) dœà ,
(4.23)
which, by the chain rule, is equivalent to
p(œënew | #¬ªd, M) =
Z
p(œënew | œà, #¬ªd, M) p(œà | #¬ªd, M) dœà .
(4.24)

4.4. Algorithms
31
If œà is known, #¬ªd is independent of œënew (see Fig. 4.2)), and we get
p(œënew | #¬ªd, M) =
Z
p(œënew | œà, M) p(œà | #¬ªd, M) dœà .
(4.25)
Finally, the right hand side of Eq. (4.25) gets approximated in the same way as all
the other integrals:
p(œënew | #¬ªd, M) ‚âà
Npœë
X
i=1
p(œënew | œài, M) ,
(4.26)
where Npœë is suÔ¨Éciently large and œài ‚àºp(œà | #¬ªd, M), which is computed according to
Eq. (4.7). In this case, the hyper-parameter vector œà enters into the approximation
directly.
4.4
Algorithms
The overall procedure for performing the HB inference can now be summarized as
follows.
1. Sample the parameters distributions p(œë(k) | d(k), M(k)), k = 1, . . . , K. This
step is an inference procedure discussed in detail in Chapter 2, we use BA-
SIS [106] to sample from the distributions involving one set of data d(k):
p(œë(k) | d(k), M(k)), k = 1, . . . , K. We save the evidences p(d(k) | œë(k), M(k))
of the models M(k) computed by BASIS as a by-product of the run to use
them at the step 2.
2. Sample the hyper-parameter distribution p(œà | #¬ªd, M) using BASIS and Bayes‚Äô
theorem formulation from Eq. (4.7). Recall that the likelihood p(#¬ªd | œà, M) is
computed from Eq. (4.8) and further approximated using Eq. (4.14).
The
latter requires computing a sum for each hyper-parameter œà, which involves
two priors (hierarchical and non-hierarchical) ‚Äì analytical functions ‚Äì and the
evidences, which were evaluated at step 1.
3. Sample the updated parameters distributions p(œë(k) | #¬ªd, M), k = 1, . . . , K
using BASIS and the approximation from Eq. (4.6). In this approximation,
the multiplier outside the sum p(d(k) | œë(k), M) is a standard non-hierarchical
likelihood. The sum together with a multiplier 1/Nsœà can be considered as an

32
Chapter 4.
Hierarchical Bayesian Inference
improper prior (i.e. such that it does not integrate to 1), which is not a problem
for MCMC methods. The sum itself involves computing the hyper-prior ‚Äì an
analytical function ‚Äì and the term p(d(k) | œài, M), which is computed using
Eq. (4.14), in the same way as discussed in step 2.
The hyper-parameter
œài is generated from the distribution p(œà | #¬ªd, M) sampled in step 1. Since
the terms p(d(k) | œài, M) do not involve œë, they can be precomputed for a
set of samples œài, i = 1, . . . , Nsœë before sampling the updated parameters
distributions p(œë(k) | #¬ªd, M(k)) to reduce the computational eÔ¨Äort.
4. Propagate the uncertainty into a QoI. The propagation for the same QoI y(k)
new
(see Eq. (4.18)) requires sampling from the distribution p(œë(k) | #¬ªd, M), which
was computed at step 3. The propagation for the new QoI ynew (see Eq. (4.22))
requires sampling from the distribution p(œënew | #¬ªd, M), which is approximated
by a sum of hyper-priors ‚Äì analytical functions ‚Äì in Eq. (4.26).
The com-
ponents of the sum require, in turn, sampling from p(œà | #¬ªd, M), which was
discussed in step 2.
Note that BASIS in the above procedure can be replaced by any other sampling
algorithm. The only additional requirement is that the algorithm, which is used at
step 1., should be able to compute the evidence of a model.

Chapter 5
Implementation
5.1
Œ†4U Framework
For the Bayesian Uncertainty QuantiÔ¨Åcation and Propagation, we employ our in-
house open-source high performance computing framework Œ†4U [60]. Œ†4U is platform-
agnostic and can eÔ¨Éciently exploit massively parallel computer architectures. With
a help of the task-parallel library for clusters TORC [58], Œ†4U extracts and sched-
ules the parallelism of the Bayesian inference algorithms. Œ†4U exhibits both ease of
programming and transparent load balancing through the task stealing mechanism.
To support this mechanism, TORC maintains a pool of available work chunks and
automatically manages the access of tasks to this pool. Additionally, Œ†4U enables
multiple levels of parallelisation, which come in play naturally in the Bayesian infer-
ence: 1) diÔ¨Äerent Markov chains are independent and can be processed in parallel, 2)
each likelihood evaluation may require multiple simulation runs, 2) each simulation
run can also be a parallel program.
Œ†4U has been used for inferences in molecular dynamics, granular materials and
structural dynamics [60, 59, 80] and has shown excellent performance and scalability,
which is discussed more in detail further in this chapter.
From the user prospective, the framework is easy to work with. The user pro-
vides a C function which returns the log-likelihood value (‚ÄúÔ¨Åtfun‚Äù) and adjusts the
conÔ¨Åguration Ô¨Åle (the population size, the prior information, the dimensionality
etc.). The ‚ÄúÔ¨Åtfun‚Äù function can call any external simulation engine inside, parallel
or sequential. If one wishes to use this function for the forward uncertainty propa-
33

34
Chapter 5.
Implementation
gation, it should also store the simulation output in a special array. After this, Œ†4U
is ready to run.
Currently, Œ†4U has Ô¨Åve algorithms for the main problems of the Bayesian UQ:
1) TMCMC [26] and BASIS [106] algorithms for the exact Bayesian inference,
2) ABC-SubSim [25] algorithm for the approximate Bayesian inference,
3) Subset Simulation [10] algorithm for rare events sampling,
4) CMA-ES [63] algorithm for optimization,
three surrogate models:
1) kriging surrogate,
2) multilayer perceptron surrogate,
3) extreme learning machine surrogate,
and a number of helper tools:
1) sensitivity tool,
2) uncertainty propagation tool,
3) samples visualization tool,
4) propagation visualization tool.
The surrogates are described more in detail later in this chapter.
Sensitivity tool
For this tool, the user provides a point œër, at which he wishes
to check the sensitivity, selects the number of steps in each direction (M) and the
size of the step (Œ¥k, in percents). The sensitivity tool spawns parallel jobs in all
the directions from a reference point, i.e. each of the submitted simulations will be
evaluated at a point
œë :
Ô£±
Ô£≤
Ô£≥
œëi = œër,i,
if i Ã∏= k ,
œëk = œër,k + mŒ¥k,
otherwise ,
(5.1)
where i, k = 1, . . . , Nœë, m = ‚àíM, ‚àíM +1, . . . , 0, . . . , M ‚àí1, M. The quantity under
sensitivity analysis can be either the log-likelihood, or a user-deÔ¨Åned QoI.

5.2. Structure of Œ†4U algorithms
35
Propagation tool
This tool is able to run parallel simulations using each line
from a given Ô¨Åle as a parameter vector. The points are samples from the posterior
distribution. The tool utilizes the output array of the ‚ÄúÔ¨Åtfun‚Äù to obtain the result
of the simulation. Thus, in order for the tool to run properly, the user should save
the outcome of the simulation.
Samples visualization tool
The samples visualization tool takes an putput Ô¨Åle
of an inference algorithm (TMCMC, BASIS or ABC-SubSim) to produce an image
of samples projected onto 2-d subspaces of the parameter space colored by the log-
likelihood. The Ô¨Ågure is amended with 1-d histograms of the marginal posteriors
and the 2-d kernel histograms of each pair or parameters. The tool is written in R
and can be adjusted to generate diÔ¨Äerent sizes of Ô¨Ågures, colors, fonts etc.
Propagation visualization tool
This tool generates a Ô¨Ågure containing quan-
tiles of the propagated QoI. The tool is written in MATLAB, which allows users to
modify the Ô¨Ågure properties.
5.2
Structure of Œ†4U algorithms
All of the algorithms currently implemented inside Œ†4U follow the same scheme,
shown in Algorithm 3.
Such a similarity allows for modular implementation,
planned for the future. The next four paragraphs give speciÔ¨Åcations for each of the
algorithms with respect to the general scheme.
TMCMC (BASIS)
The algorithm-speciÔ¨Åc settings for TMCMC (BASIS) are
the following:
I-1. Œ± = 0, E = 1
M-1. Evaluate plausibility weights wn = p(d | œën)Œ¥Œ±, where Œ¥Œ± > 0 is chosen such
that the CoV of the weights equals to the prescribed value (usually 1)
M-2. Àúw‚Ñì= wn‚Ñì/ PN
n=1 wn and c‚Ñìis the number of times that œën‚Ñìwas selected
M-3. r(œë, œë‚Ä≤) = min
n
1, p(œë‚Ä≤)/p(œë) (p(d | œë‚Ä≤)/p(d | œë))Œ¥Œ±o
and Œ£‚Ñìis discussed be-
low
M-5. Œ± = Œ± + Œ¥Œ±, E = E PN
n=1 wn/N

36
Chapter 5.
Implementation
Algorithm 3 Typical Œ†4U algorithm
// Initialization
I-1. Select the population size N and algorithm-speciÔ¨Åc constants and variables
I-2. Sample œën from the prior p(œë), n = 1, . . . , N
// Main loop
while stopping conditions S are not met do
M-1. Evaluate auxiliary quantities
M-2. Select samples s‚Ñì= œën‚Ñìas leaders for the next stage with algorithm-
speciÔ¨Åc probability Àúw‚Ñì.
Each of the leaders will generate c‚Ñìsamples (also
algorithm-speciÔ¨Åc).
M-3. Simulate Markov chains:
for (‚Ñì= 1; ‚Ñì‚â§L; i = i + 1) do
Do c‚Ñìsteps of MH starting from s‚Ñì. MH proposal covariance Œ£‚Ñìand proba-
bility of acceptance r(œë, œë‚Ä≤) are algorithm-speciÔ¨Åc.
end for
M-4. Replace œën, n = 1, . . . , N with these newly produced samples
M-5. Update variables
end while

5.2. Structure of Œ†4U algorithms
37
BASIS
The algorithm-speciÔ¨Åc settings for BASIS are the same as for TMCMC,
except that we run c‚ÑìMHs with the chain length 1 from each of the leaders.
ABC-SubSim
The algorithm-speciÔ¨Åc settings for ABC-SubSim are the following:
œë is augmented with the random variable describing the output QoI: œë ‚Üê(œë, y) ,
and
I-1. E = 1, choose c such that L = N/c is integer
M-1. Evaluate discrepancies œÅn, n = 1, . . . , N and renumber the samples s.t. their
discrepancies are sorted in the ascending order.
Set Œ¥ = (œÅL + œÅL+1) /2 .
M-2. Àúwn = I{œÅn ‚â§Œ¥} and c‚Ñì= c
M-3. r(œë, œë‚Ä≤) = min {1, p(œë‚Ä≤)/p(œë)} I{œÅ(y‚Ä≤, d) ‚â§Œ¥} and Œ£‚Ñìis discussed below
M-5. E = E/c
SubSim
The algorithm-speciÔ¨Åc settings for SubSim are the following:
I-1. Choose the current level b, choose c such that L = N/c is integer
M-2. Àúwn = I{yn ‚â•b} and c‚Ñì= c
M-3. r(œë, œë‚Ä≤) = I{y‚Ä≤ ‚â•b} and Œ£‚Ñìis discussed below
M-5. Update b (see [10])
CMA-ES
The algorithm-speciÔ¨Åc settings for CMA-ES are the following:
I-1. Initialize pc = pœÉ = 0, Œ£ = I, ‚ü®x‚ü©, œÉ (see [64]).
M-2. Select ¬µ best points, c‚Ñì= 1
M-3. 1 (accept everything)
M-5. Update pc = pœÉ = 0, Œ£ = I, ‚ü®x‚ü©, œÉ (see [64]).

38
Chapter 5.
Implementation
5.3
Improving MH Sampling
The three out of four algorithms in Œ†4U hinge on accurate tuning of the MH proposal
covariance. A matrix which allows for big jumps leads to the low sample acceptance
rate, while a matrix which forces small steps leads to the ineÔ¨Écient exploration of
the parameter space by the algorithm and might lead to stagnation.
The original TMCMC uses the covariance matrix from the previous stage scaled
with some factor. We have observed that for some distributions with unidentiÔ¨Åable
manifolds this approach leads to low acceptance rates and requires many samples
to converge.
One approach to resolve this issue is to adjust the scaling factor of the covariance
matrix based on the MH acceptance rate calculated on a Markov chain of a small
length, as proposed for ABC-SubSim in Ref. [25]. This method, however, fails to
generate a good covariance matrix for skewed posterior distributions, such as the
Rosenbrock function.
Thus, we have decided to focus on another way of creating the proposal co-
variance distribution, which makes use of the local geometry in the neighborhood
of each chain leader. Our approach is to compute the sample covariance matrix
in a rectangular region around the leader. The size of the region is adjusted au-
tomatically, starting from a very small neighborhood, until the calculated sample
covariance matrix of this region is positive deÔ¨Ånite.
The local covariance idea was tested on the 2-d Rosenbrock function using BASIS
with 8000 samples per stage. As it can be seen in Fig. 5.1, the sampling of the BASIS
is greatly improved with the local covariance. Taking into account these promising
results, we now employ the local covariance for all the MH-based algorithms in Œ†4U.
5.4
Surrogates
Large-scale models often exhibit exceedingly high computational cost, which makes
it nearly impossible to perform Bayesian calibration requiring thousands of simu-
lation runs. In such a situation, lightweight surrogate models can be used instead
of the real function evaluations. Although they introduce an additional layer of ap-
proximation, surrogates allow to use much more samples in the inference and thus
decrease the sampling error, as compared to the true model evaluations. This, to-
gether with a careful control of the approximation error, makes surrogates a useful

5.4. Surrogates
39
(a) Global covariance (scale = 0.2 [26]).
(b) Local covariance.














	
	
(c) Analytical plot.
Figure 5.1: 8000 BASIS samples from the Rosenbrock function.

40
Chapter 5.
Implementation
tool which can provide a reasonable estimate of the posterior PDF.
5.4.1
Methodology
The surrogate can be built oÔ¨Ä-line, when one Ô¨Årst obtains a low-order model and
then runs inference with it, or on-line, when one re-builds the surrogate on the
Ô¨Çy during the inference. Another choice to be made is building a local surrogate
for each point based on the neighborhood information or building a global one
for all the points. The Markov chains in the TMCMC, BASIS and ABC-SubSim
are of a local nature, which suggests that one should choose a local surrogate. In
addition, the local approximation is cheaper and more accurate around the chain
leader than the global one. We choose to approximate the log-likelihood of a given
sample instead of approximating the result of the simulation, as the latter may be
high-dimensional and thus more troublesome to handle. Having all this in mind, we
follow the methodology proposed in Ref. [8] to implement the on-line local surrogates
approximating the log-likelihood in Œ†4U.
The current implementation of the surrogates in Œ†4U proceeds for each Markov
chain leader as follows:
1. Find the leader‚Äôs neighbors in a predeÔ¨Åned domain
2. Build a surrogate for the current chain using the neighbors found at step 1
3. For each sample produced by the Markov chain, evaluate the surrogate
4. Decide whether to use or not the surrogate outcome. If not, then evaluate the
full model and use its outcome.
In our implementation we look for neighbors in a rectangular region equal to a user-
deÔ¨Åned percentage of the full domain. If the speciÔ¨Åed percentage is larger or equal
than 100%, the algorithm optimizes the computations by building the surrogate
only once per algorithm stage, as the interpolating surface will be the same for all
the leaders.
To make a decision about using or not the value proposed by the surrogate model,
we again refer to Ref. [8]. We apply two criteria: 1) the relative error of the surrogate
approximation is smaller than 1% (when available), 2) the value predicted by the
surrogate lies within the 5%-95% quantile range of all the real model evaluations.

5.4. Surrogates
41
5.4.2
Surrogates with Bayesian inference algorithms
Three of the algorithms implemented in Œ†4U, ABC-SubSim, BASIS and TMCMC,
share algorithmic similarities that encourage the usage of surrogates. More precisely,
in all the three algorithms, the newly generated samples lie inside (or very close
to) the convex hull of the samples from the previous stages, which is schematically
illustrated in Fig. 5.2a. The two samples domains are nested, with the samples drawn
at stage k populating the region inside the blue boundary, and the samples generated
at stage (k + 1) populating the domain inside the red curve. Chains generated at
stage (k + 1) from the leader samples located in the red domain produce samples
aiming at populating this domain. Note that there exist blue samples (generated up
to stage k) that contain in their convex hull the red chain. This property, however,
can be broken if the ABC-SubSim/TMCMC/BASIS annealing goes too fast or if
the number of samples per stage is too small.
This algorithmic feature was exploited in Ref. [9] to introduce kriging surrogates
within TMCMC. The full model evaluations from previous TMCMC stages (blue
samples in Fig. 5.2a) were gathered at the neighborhood of each leader (red points
with blue boundary in Fig. 5.2a) to become kriging support points. After that, the
inference was continued, but the log-likelihood value in each of the points proposed
by MH was replaced by a surrogate prediction, which was used or not according to
the rules discussed in the previous section. As a result, the authors observed up to
one order of magnitude reduction of the number of full model evaluations required.
We thus conclude that the same concept can be readily applied to ABC-SubSim and
TMCMC/BASIS with any kind of an interpolation algorithm supporting irregular
grids.
The property discussed above breaks however for the SubSim and CMA-ES,
as these algorithms try to explore regions of low probability/low Ô¨Åtness function
values, as shown in Fig. 5.2b. Each chain generated using SubSim/CMA-ES, drawn
with arrows, can be located outside of the convex hull of the samples generated
from SubSim/CMA-ES up to stage k. This prevents any interpolation method from
building accurate estimates, since there is no enough support points from previous
stages in the region of interest. We thus are limited to building oÔ¨Ä-line surrogate
models on a Ô¨Åne enough grid to cover the whole region of interest in the parameter
space.

42
Chapter 5.
Implementation
k
k + 1
(a) ABC-SubSim and TMCMC. For BASIS the chains have lengths 1 and may start from
the same leader.
k
k + 1
(b) SubSim and CMA-ES.
Figure 5.2: Schematic representation of intermediate domains at stages k (dashed
blue line) and (k + 1) (dashed red line), samples up to stage k (blue dots and red
dots with blue border), chains generated from the leader samples at the stage k
(red dots and red dots with blue border) for various algorithms. Black dots indicate
samples which were proposed, but rejected by the algorithm.

5.5. HPC Aspects
43
5.4.3
Available surrogates in Œ†4U
Having assessed the necessity of the surrogates in Œ†4U, we have implemented them
for TMCMC and BASIS. Their implementation with ABC-SubSim is scheduled for
the future. Œ†4U now supports three surrogate models: kriging [78, 112], Multi-Layer
Perceptron (MLP) [66] and Extreme Learning Machine (ELM) [71].
The kriging surrogate is implemented using the open-source gplib library [2].
The multilayer perceptron and the extreme learning machine implementation use
the open-source OpenANN library [3].
We have used the surrogate-aided Bayesian inference for a two-dimensional Him-
melblau function using each of these surrogates. Fig. 5.3 presents the posterior PDFs
of four runs: a run with real model evaluations only, a run with kriging as a surro-
gate, a run with MLP (1 layer, 32 neurons) as a surrogate, and a run with ELM (1
layer, 512 neurons) as a surrogate.
There is no visible diÔ¨Äerence in the surrogate-aided and surrogate-free inferences.
For more in-depth investigation of the eÔ¨Äect of the surrogates on the performance
of the BASIS, we have performed a series of runs with diÔ¨Äerent support size of the
surrogate and diÔ¨Äerent surrogate models. Table 5.1 presents the results of the BASIS
runs using surrogates. We observe that the error of the ELM decreases with the
increase of the percentage D of the domain used to build the surrogate. The best
result in terms of accuracy-time trade-oÔ¨Äis achieved with a global interpolation.
The error of MLP decreases for D < 8% and increases when D is bigger than 8%,
which suggests the overÔ¨Åtting issues. The error of the kriging surrogate consistently
decreases with the increase of D. This series of experiments suggests that the global
ELM surrogate is the best option in terms of both time and accuracy. However, more
tests are required to address the high-dimensional distributions and those with the
unidentiÔ¨Åable manifolds.
5.5
HPC Aspects
In this section we discuss the performance of the newly implemented ABC-SubSim
algorithm and surrogate techniques.

44
Chapter 5.
Implementation
(a) No surrogates.
(b) Kriging surrogate.
(c) MLP surrogate (1 layer, 32 neurons).
(d) ELM surrogate (1 layer, 512 neurons).
Figure 5.3: Posterior PDFs of the 2-d Himmelblau function. The population size is
8000. Each of the surrogates uses 8% of the domain to build the approximation.

5.5. HPC Aspects
45
Table 5.1: Results of the BASIS inference of the Himmelblau function using the
surrogates: percentage D of the domain used to build surrogate, posterior mean
value m(œë) of the parameter œë, 5%-95% quantiles q(œë) of the posterior PDF of
the parameter œë, percentage S of surrogate points, average surrogate evaluation
time T in seconds, average error E computed as the absolute diÔ¨Äerence between
the surrogate and the real model evaluation. The population size was chosen to be
2000.
D m(œë1) q(œë1)
m(œë2) q(œë2)
S
T
E
No surr.
‚Äì 1.113
[-3.796, 3.795] 0.376
[-3.277, 3.414] ‚Äì
‚Äì
‚Äì
ELM
4% 0.930
[-3.907, 3.963] 0.245
[-3.344, 3.461] 64% 0.11
280.00%
ELM
8% 1.102
[-3.818, 3.960] 0.235
[-3.435, 3.409] 65% 0.25
102.00%
ELM
16% 1.158
[-3.906, 3.795] 0.126
[-3.380, 3.376] 64% 0.55
25.00%
ELM
32% 1.242
[-3.818, 3.796] 0.238
[-3.329, 3.332] 64% 1.3
0.86%
ELM
100% 0.852
[-3.898, 3.833] 0.320
[-3.402, 3.335] 71% 0.024
0.43%
MLP
4% 1.130
[-3.990, 4.153] 0.056
[-3.633, 3.450] 90% 0.047
100.0%
MLP
8% 0.792
[-3.932, 3.861] 0.107
[-3.460, 3.397] 48% 1.2
1.7%
MLP
16% 1.021
[-3.944, 3.910] 0.196
[-3.465, 3.223] 48% 2.8
2.4%
MLP
32% 1.203
[-3.847, 3.945] 0.102
[-3.325, 3.300] 48% 5.6
2.8%
MLP
100% 1.247
[-3.921, 3.897] 0.154
[-3.277, 3.224] 64% 0.018
188.0%
Kriging
4% 1.150
[-3.828, 3.828] 0.183
[-3.321, 3.293] 30% 0.10
4.400%
Kriging
8% 1.189
[-3.897, 3.806] 0.261
[-3.321, 3.308] 42% 0.64
2.800%
Kriging
16% 1.589
[-3.816, 3.913] 0.219
[-3.307, 3.302] 45% 0.45
0.410%
Kriging
32% 1.549
[-3.687, 3.908] 0.324
[-3.297, 3.344] 36% 3.89
0.013%
Kriging 100% 1.326
[-3.833, 3.933] 0.140
[-3.376, 3.286] 39% 0.75
0.049%

46
Chapter 5.
Implementation
5.5.1
ABC-SubSim
As a test case for the ABC-SubSim, we consider an MD simulation of helium,
described in detail in Chapter 6.
The inference in this case uses MPI-parallel LAMMPS and runs on 512 compute
nodes of the Piz Daint. TORC is initialized with two MPI workers per node resulting
in 1024 workers in total. Each LAMMPS simulation utilizes 4 cores, making the full
use of each compute node.
The three discrepancies considered for this inference problem give similar out-
comes, and we thus only present results for one of them ‚Äì the quantile-based dis-
crepancy. Table 5.2 summarizes the parallel performance of ABC-SubSim. We see
that the scheduling mechanism of TORC is working extremely well, as the parallel
eÔ¨Éciency reaches 95-96%, despite the high variance of the simulation run times. The
eÔ¨Éciency of the initialization phase (stage 0) is 95% as 15360 function evaluations
are evenly distributed among the 1024 workers. The considerably lower eÔ¨Éciency of
63% at the stage 1 is explained by the existence of chains with high total running
times, which is due to the small number of chains available to each worker (3072
chains in total, 3 chains per worker). For the stage 2, the eÔ¨Éciency increases to 89%,
and further reaches the plateau of 95% on stages 3-7. The average load imbalance
on this plateau is approximately 8% computed as the mean over the stages 3-7 of
(Tmax ‚àíTavg)/Tmax, where Tmax and Tavg are the maximum and average time (in
seconds) workers spent during a given stage.
5.5.2
Surrogates
We have applied the on-line local kriging surrogate to the argon inference problem
described in detail in Chapter 7. The interpolation was constructed in the neigh-
borhood of each chain leader deÔ¨Åned as 16% of the domain size in each direction. A
total of 28% of the log-likelihood evaluations were accepted surrogate runs resulting
in a speedup of 28% computed as:
1 ‚àíTs ¬∑ Ns + Tr ¬∑ Nr
Tr ¬∑ N
= 1 ‚àí1.3 ¬∑ 10000 + 797.1 ¬∑ 8596
797.1 ¬∑ 12000
,
(5.2)
where Ts is the average runtime of the surrogate, Ns is the total number of the
surrogate evaluations (both accepted and rejected), Tr is the average runtime of the
real model evaluation, Nr is the total number of the real model evaluations, N is

5.6. Clusters
47
Table 5.2: Detailed per-stage performance results of ABC-SubSim with 1024 workers
on the Piz Daint cluster: Tm ‚Äì mean simulation time, Ts ‚Äì standard deviation of the
simulation times, Tt ‚Äì sum of the execution times of all simulations, Tw ‚Äì wall-clock
time per stage. All the times are measured in seconds.
Stage Tm Ts Tt
Tw
Speedup EÔ¨Éciency Load imbalance
0
83
84 1267610 1307 970
95%
5%
1
132 95 2025660 3147 644
63%
37%
2
66
6
1014480 1114 911
89%
28%
3
64
3
988523
1012 977
95%
7%
4
64
3
987557
1006 981
96%
8%
5
64
3
989781
1012 978
95%
9%
6
65
3
991197
1016 975
95%
9%
7
65
3
996400
1021 976
95%
8%
the total number of samples per stage equal to the number of accepted surrogate
evaluations plus the number of real model evaluations. All the times are given in
seconds.
5.6
Clusters
All the inferences described in this thesis were performed with the open-source high
performance computing library Œ†4U compiled on Brutus cluster of the Swiss Federal
Institute of Technology (ETH) Zurich and Piz Daint cluster of the Swiss National
Supercomputing Center (CSCS), Lugano.
On Brutus, we use nodes with four 12-core AMD Opteron 6174 CPUs of the
Magny-Cours family. The operating frequency of each of the 12 cores is 2.2 GHz.
On Piz Daint, we use 8-core Intel Xeon E5-2670 CPUs operating at 2.6 GHz and
NVIDIA Tesla P100 GPUs.


Part II
Applications
49


Chapter 6
Helium Modeling
In this chapter we apply the Approximate Bayesian Inference to Molecular Dynamics
(MD). The results described here were published in Ref. [80].
MD is a popular method for investigating properties of materials at atomistic
scale. MD simulations integrate Newton‚Äôs equation of motion for the atoms in the
system, and use their trajectories and forces to estimate QoIs. In this chapter, we
focus on QoIs in the form of moments of a statistical distribution averaged over time
and atoms. Before MD simulations can be used, one must assign a functional form
to the interatomic interactions. These interactions are commonly referred to as a
Force-Field (FF).
There have been several recent works examining the usage of Bayesian inference
in MD-FF calibration [5, 7, 20, 19, 102, 84, 39].
The underlying assumption in
these works is Gaussianity of the Ô¨Çuctuations of the target properties (diÔ¨Äusion
coeÔ¨Écients, radial distribution functions, etc.) [5, 39]. However, it is often the case
in MD that some thermodynamical properties of interest do not follow a Gaussian
distribution, and the Ô¨Årst two moments are then not suÔ¨Écient for the full QoI
description. In this case a natural choice is to apply ABC with a more involved
discrepancy, than the mean and variance-based one. Shell [109] suggests that the
most relevant target for calibrating molecular models is the relative entropy of the
two MD ensembles. We employ the relative entropy as discrepancy in the ABC
inference and demonstrate its increased capability of inferring the LJ parameters,
as compared to the discrepancy based on the Ô¨Årst two moments.
51

52
Chapter 6. Helium Modeling
6.1
Parameters Calibration
We consider the calibration of the Lennard-Jones potential parameters of helium
using the likelihood-free Bayesian inference. The Lennard-Jones potential is given
by
V (r; œÉ, Œµ) = 4Œµ
œÉ
r
12
‚àí
œÉ
r
6
,
(6.1)
where Œµ is the depth of the potential well, œÉ is the Ô¨Ånite distance at which the
inter-particle potential is zero, and r is the distance between the particles.
The
parameters Œµ and œÉ are material-speciÔ¨Åc.
Œµ is measured in 10‚àí21 J, œÉ and r are measured in nanometers (10‚àí9 m). Boltz-
mann factor fB is measured in 1015 kg‚àí1.
We are interested in calibrating the LJ parameters of helium using the data on
the Boltzmann factor distribution over time:
fB =

exp

‚àíH
kBT

,
(6.2)
where H is the enthalpy of the system of helium atoms, T is the temperature of the
system, kB is the Boltzmann constant, and ‚ü®¬∑‚ü©denotes the ensemble average. The
ensemble average indicates that H is measured for all interacting atoms at each time
instance. We generated synthetic Boltzmann factor data using LAMMPS [98, 1] for
a system of 1000 atoms equilibrated for 2 ns and simulated for 20 ns in the NPT
ensemble [111] with a time step of 2 fs and Œµ = 0.141, œÉ = 0.2556 (see Fig. 6.1),
which are the LJ parameters values taken from [121]. The system used for calibration
consists of 1000 atoms and is equilibrated for 2 ns, following a production run in the
NPT ensemble for another 5 ns with a 2 fs time step. The LAMMPS script used
in the inference is given in Appendix D.2. We note that the model cannot replicate
the target data exactly due to the smaller sampling time. This is a typical situation
in MD, where the calibration is done with limited resources.
The distribution of the Boltzmann factor is non-Gaussian due to its statistical
thermodynamics deÔ¨Ånition, which results in a heavy tail in Fig. 6.1. Exploiting the
Ô¨Çexibility of ABC, we employ 3 diÔ¨Äerent discrepancy functions to capture higher
order moments of the distribution, and compare the results.

6.1. Parameters Calibration
53









	





Figure 6.1: Data: distribution of the Boltzmann factor fB over time of a helium
system with 1000 atoms, œÉ = 0.2556, Œµ = 0.141.
The Ô¨Årst discrepancy function makes use of the Ô¨Årst two moments of the Boltz-
mann factor distribution:
œÅ(y, z) =
  ¬Øy ‚àí¬Øz
¬Øy
2
+
sy ‚àísz
sy
2!1/2
,
(6.3)
where ¬Ø
X denotes a sample mean and sX denotes the sample standard deviation
of the quantity X. Since the mean and the variance form a suÔ¨Écient statistic of
the Gaussian distribution, this discrepancy mimics the likelihood-driven approach
which usually assumes Gaussianity of the Ô¨Çuctuations.
The second choice for the discrepancy is based on the quintiles (5-quantiles) q
of the data sets:
œÅ(y, z) =
 
4
X
k=1
qk(y) ‚àíqk(z)
qk(y)
2!1/2
.
(6.4)
The Ô¨Ånal option is:
œÅ(y, z) =
Z
pz(t) log pz(t)
py(t)dt,
(6.5)

54
Chapter 6. Helium Modeling
which is a relative entropy (also called Kullback-Leibler divergence) of the PDF
pz(¬∑) of z and the PDF py(¬∑) of y. The involved PDFs are computed by a kernel
density estimate.
Note that the Kullback-Leibler divergence depends on the order of arguments,
and one should take care to not commute the target data with the simulations result.
The motivation behind the use of the relative entropy as a discrepancy measure
comes from the work of Shell [109]. He treats it as a natural way of comparing the
two MD ensembles. It is worth noticing that in the case of Gaussian distributed
QoI, the relative entropy relaxes to accurate estimation of its mean and variance.
We ran the calibrations using 15360 samples per stage.
In every case ABC-
SubSim stopped because of the acceptance-based criterion (the acceptance rate
dropped below 5%). A full MD simulation was run for every sample in the parame-
ters space requiring a signiÔ¨Åcant amount of computational work. For this inference
problem, an additional load imbalance is introduced to the algorithm, as simulations
with smaller values of œÉ require considerably more time, as compared to those with
larger values of œÉ. Since the relative expected run time is known a priori, we help
Œ†4U by applying the Longest Processing Time algorithm [56], which means that
the tasks with higher execution time are processed Ô¨Årst. More precisely, we sort
the samples according to the values of œÉ before giving the simulation runs to the
workers. Our large scale parallel runs on Piz Daint supercomputer show an excellent
parallelization eÔ¨Éciency of up to 95% with the load imbalance of approximately 8%
(see Table 5.2 for details).
Analyzing the results, we observe that the outcome of calibration for each dis-
crepancy function diÔ¨Äers from the two others both in terms of the most probable
parameters and their associated uncertainty, see Table 6.1 and Fig. 6.2.
Under the stochastic model class MG which assumes the Gaussianity of the
Boltzmann factor distribution, the parameter Œµ is unidentiÔ¨Åable, as the approximate
posterior PDF spans across all the prior range (Fig. 6.2a). To explain this eÔ¨Äect, we
checked the contributions of the fractions (¬Øy ‚àí¬Øz)/¬Øy and (sy ‚àísz)/sz from Eq. (6.3)
to the discrepancy. We observed that a typical contribution of the mean diÔ¨Äerence
is around 3%, while for the variance diÔ¨Äerence it is around 97%. Together with
the observation that the standard deviation of the Boltzmann factor distribution
appears to be insensitive to changes in Œµ, we obtain a reasonable explanation of the
unidentiÔ¨Åability of Œµ.

6.1. Parameters Calibration
55
(a) Gaussian discrepancy.
(b) Quantile discrepancy.
(c) Relative entropy discrepancy.
Figure 6.2: Calibration results for the LJ parameters of helium using ABC-SubSim
with diÔ¨Äerent discrepancies (see Notations). Green circles on the bottom-left sub-
plots indicate parameters for which the data were created.

56
Chapter 6. Helium Modeling
Table 6.1: Values of the LJ parameters œë ‚àà{œÉ, Œµ} of helium: prior bounds p(œë), pos-
terior MPVs b(œë), posterior 5%-95% quantiles q(œë), number of stages S, achieved tol-
erances Œ¥ for three discrepancy models: MG ‚Äì Gaussian (Eq. (6.3)), MQ ‚Äì quantile-
based (Eq. (6.4)), MKL ‚Äì Kullback-Leibler (Eq. (6.5)).
Model p(œÉ)
b(œÉ)
q(œÉ)
p(Œµ)
b(Œµ)
q(Œµ)
S Œ¥
MG
[0.1,0.8] 0.245 [0.203, 0.300] [0.01, 1] 0.423 [0.049, 0.899] 4 3.4√ó10‚àí3
MQ
[0.1,0.8] 0.259 [0.247, 0.269] [0.01, 1] 0.138 [0.123, 0.159] 7 9.6√ó10‚àí6
MKL
[0.1,0.8] 0.279 [0.251, 0.304] [0.01, 1] 0.117 [0.087, 0.156] 6 6.7√ó10‚àí2
Inspection of Fig. 6.1 reveals the skewed nature of the Boltzmann factor distribu-
tion, which is neglected when modeled with a Gaussian distribution. The diÔ¨Äerence
in the posterior distributions obtained using the other two discrepancies, as com-
pared to the Gaussian one, lies mainly in their ability to reduce the uncertainty of
both Œµ and œÉ (see Fig. 6.2b, Fig. 6.2c). There is no signiÔ¨Åcant diÔ¨Äerence between the
quantile and the relative entropy results, except that the quantile setting is arguably
more robust.
6.2
Discrepancy Model Selection
The inference with discrepancy models MQ and MKL identiÔ¨Åed the force-Ô¨Åeld pa-
rameters, employing a full MD run per sample, while MG resulted in a wide posterior
distribution that spans across the entire prior range. This happened because, in the
Gaussian case, we have reduced the information regarding the ensemble PDF of
the QoI into 2 numbers, the mean and variance, which are not enough to uniquely
identify the LJ potential values. This is a typical approach however in potential
calibration and hierarchical coarse graining strategies [96] in MD, where the values
of the QoI (e.g. intermolecular forces) are taken to be the ensemble averages of
the coarse and Ô¨Åne molecular models to be matched.
On the other hand, ABC
equipped with a relative entropy discrepancy provides a natural framework [109]
for the development of eÔ¨Écient parallel sampling algorithms. ABC-SubSim imple-
mented herein provided accurate identiÔ¨Åcation of the force Ô¨Åeld parameters when
using thermodynamic ensemble target data in MD. This is achieved by exploiting
the full non-Gaussian description of the QoI, contrary to Ref. [5, 20, 39].

6.3. Discussion
57
Based on the ABC model selection criterion (Eq. (3.4)) applied to ABC-SubSim,
the Gaussian model should be chosen as the best one, as it required the least number
of stages to converge. However, the Gaussian setting resulted in the unidentiÔ¨Åable
manifold for Œµ, while the two other settings were able to reduce its prior uncertainty.
Recalling that the evidence computed in ABC, and in particular by ABC-SubSim, is
only an approximation of the true evidence, we conclude that in this case the error
of the formula is too big to make any conclusions. The issues of the model selection
in the context of ABC are discussed in detail in Ref. [50, 104].
6.3
Discussion
We have addressed a problem of calibrating the LJ potential of helium using data
with non-Gaussian Ô¨Çuctuations.
Using the ABC inference, we have assessed the
eÔ¨Äect of changing the discrepancy between the data and the simulation outcome on
the resulting LJ parameters distribution. Our results show that assuming the Gaus-
sianity of the Ô¨Çuctuations leads to an unidentiÔ¨Åable manifold in Œµ, while relaxing this
assumption allows to reduce the prior uncertainty in both LJ parameters. Following
the works of Shell [109], we made one of the discrepancies to be the Kullback-Leibler
divergence (the relative entropy), which he considers as a natural way of comparing
two thermodynamical ensembles. Our results conÔ¨Årm that this setting allows for
quite precise calibration of the LJ potential.


Chapter 7
Argon Modeling
The Lennard-Jones (LJ) potential is one of the most popular force-Ô¨Åelds in Molec-
ular Dynamics (MD) simulations, a widespread computational method for studying
atomistic phenomena. Although it has been subject to numerous calibrations, its
repulsion exponent often does not get involved into these studies. In this chapter
we investigate the inÔ¨Çuence of the repulsion exponent on the behavior of argon and
demonstrate that this parameter has a power to enhance the predictive capabilities
of MD simulations.
LJ potential force depends on the inter-atomic distance r and consists of an
attractive and a repulsive parts.
The attractive term ‚àír‚àíq models the Van der
Waals interactions and the repulsive term r‚àíp models the Pauli repulsion at short
ranges due to overlapping electron orbitals. There exists a theoretical justiÔ¨Åcation
for setting the attractive exponent q = 6 [72], however there is none for the repulsive
exponent p = 12. The latter was chosen to be the square of the attractive term to
ease the computations. In the past, several (discrete) values of the exponent p of
the LJ 6-p potential, ranging from 10 to 20, have been considered [48]. The values
of pressure and viscosity for various thermodynamic conditions were studied with a
condition that p = 12 is the best choice for this type of data.
Two more parameters control the shape of the potential: œÉ is related to the
location of the potential well and Œµ controls the well depth.
These parameters
have been the subject of numerous calibration studies [11, 100, 105, 126], including
Bayesian ones [20, 6].
59

60
Chapter 7. Argon Modeling
Here we employ a hierarchical Bayesian (HB) technique to infer the parameters
(Œµ, œÉ, p) of the LJ 6-p potential in a systematic way for a number of thermodynamic
conditions, following Ref. [128, 129]. HB inference allows information to Ô¨Çow be-
tween the diÔ¨Äerent data sets leading to more robust and accurate predictions for the
model parameters. In our studies, we use experimental data from Radial Distribu-
tion Function (RDF) and data from quantum simulations of argon. The RDFs are
measured for liquid argon at Ô¨Åve diÔ¨Äerent temperature-pressure pairs and one case
of a saturated argon vapor.
We perform the HB calibration using the RDFs for the classical LJ 6-12 and
the modiÔ¨Åed LJ 6-p potentials and present rigorous model selection, which prefers
LJ 6-p. After that, robust posterior predictions for the RDF, diÔ¨Äusion coeÔ¨Écient
and density are made using both potentials, where the LJ 6-p demonstrate superior
quality of results. The value of the repulsion exponent found for LJ 6-p is close to
6.5 for most of the thermodynamic conditions, largely deviating from the widely
used 12.
Finally, we calibrate the LJ 6-p using data on quantum dimer scans of argon,
which describe the behavior of the argon gas. This calibration suggests a repulsion
exponent p = 12.7, which is much closer to the conventional value of p = 12.
However, the predictions of this potential for all the quantities considered (in the
liquid and saturated vapor states) are inferior to those obtained from the LJ 6-6.5.
We conclude with the two main results: 1) adjusting the repulsive exponent of LJ
6-p can boost its predictive capabilities, 2) the calibration has to be done, however,
for each thermodynamic state separately. We remark that our results have been
obtained in the case of a very simple system, potentially suggesting even greater
improvements for more complex simulations.
7.1
Computational Model Description
We perform MD simulations of argon using LAMMPS package [98, 1]. The argon
atoms are modeled as spheres which interact with LJ 6-p potential:
VLJ(r; Œµ, œÉ, p) = 4Œµ
œÉ
r
p
‚àí
œÉ
r
6
,
(7.1)
where r is the distance between the interacting atoms and p is the repulsion exponent
usually taken to be 12. The parameters Œµ, œÉ and p are to be chosen according to

7.2. Notations
61
the available measurements. As the Lennard-Jones interactions quickly decay with
the distance, an additional computational parameter rc is usually introduced. This
parameter deÔ¨Ånes a cut-oÔ¨Ädistance at which the potential is set to zero. Here, we
set rc = 2.5œÉ. The thermodynamic state of the system is deÔ¨Åned by the temperature
and the pressure of the argon atoms. We ensure that argon is in the liquid/vapor
state by checking the self-diÔ¨Äusion coeÔ¨Écient and the density. The simulation starts
with energy minimization followed by 5 √ó 106 steps, of 2 fs, in an NPT ensemble.
Then the RDF is computed in the production run consisting of 105 NVE integration
steps of 2 fs each.
The boundary conditions are periodic in each direction, the
domain contains 666 argon atoms. The self-diÔ¨Äusion coeÔ¨Écient is calculated via the
mean-squared displacement of the atoms, the RDF is discretized using 100 bins.
The LAMMPS script used in the inference is given in Appendix D.2. The units
used in the current work are given in Table 7.1.
Table 7.1: Units used in this chapter.
name
notation real
Temperature
T
K
Pressure
P
atm
Distance
r
ÀöA
LJ well depth
Œµ
kcal/mol
LJ well location
œÉ
ÀöA
LJ repulsion exponent p
‚Äì
RDF model error
œÉn
‚Äì
Density
œÅ
g/cm3
DiÔ¨Äusion coeÔ¨Écient
D
cm2/s
7.2
Notations
The calibration of LJ 6-12 using RDF data is denoted as B12,R. The inference using
RDF data with LJ 6-p is denoted as Bp,R. The corresponding HB inferences are
denoted as HB12,R and HBp,R. The RDFs are computed for 6 temperature/pressure
pairs (T, P).
We denote the pairs as: L1 = (84.4, 0.8), L2 = (91.8, 1.8), L3 =
(126.7, 18.3), L4 = (144.1, 37.7), L5 = (149.3, 46.8), V = (149.3, 43.8), where L

62
Chapter 7. Argon Modeling
stands for ‚Äúliquid‚Äù and V stands for ‚Äúvapor‚Äù. The corresponding datasets (RDFs)
are denoted as RLi for liquid and RV for vapor.
The calibration using quantum dimer calculations with LJ 6-p is denoted as
Bp,Q. The corresponding dataset is denoted as Q.
The experimentally measured RDFs are taken from Ref. [36].
The quantum
dimer energy calculations come from Ref. [62]. For the forward predictions, we use
data on density and diÔ¨Äusion coeÔ¨Écient. The reference values for density (œÅ) are
experimental measurements taken from Ref. [36]. The reference values for diÔ¨Äusion
coeÔ¨Écient (D) are computed analytically using the equations from Ref. [76].
7.3
Parameters Calibration
This section describes the inferences we performed and their results. We always use
BASIS algorithm [106] with 2000 samples per stage for LJ 6-12 and 4000 samples
per stage for LJ 6-p. The parallelisation is made with MPI and internal worker
threads of the Œ†4U library. Each LAMMPS simulation is also run in parallel with 8
MPI processes. In order to reduce the computational cost of the simulations, we use
the kriging surrogate inside Œ†4U with 16% of the domain used for the interpolation
surface construction.
All the data we use comes without errorbars, and the simulation also gives
an exact result, which allows to specify the prediction error equation Eq. (2.1)
as following:
Œ£e = Œ£c = 0, Œ£m = œÉ2
nI ,
(7.2)
where the model error œÉn is to be inferred.
7.3.1
Calibration of LJ 6-12
Bayesian inference
We present results of parameter calibration for Œµ, œÉ, œÉn, with
p Ô¨Åxed to 12. We use uniform prior for each of the parameters and each of the
datasets RLi, RV : œë ‚àà[0.05, 3] √ó [3, 4] √ó [0, 1].
Fig. 7.1 presents the most probable values (MPVs) of the inferred parameters
along with their 5%-95% quantiles. The results are only presented for four out of six
datasets as the LJ 6-12 potential failed to simulate the liquid argon for conditions
L1 and L2. We observe a large diÔ¨Äerence in the values of Œµ for liquid and vapor,
which implies that one cannot perform the simulations using the same parameters

7.3. Parameters Calibration
63
L3
L4
L5
V
0.1
0.2
0.3
L3
L4
L5
V
3
3.2
3.4
3.6
Figure 7.1: Posterior parameter values: MPVs along with 5%-95% quantiles ob-
tained in B12,R (light red), HB12,R (dark red). Horizontal lines indicate the refer-
ence values: Ref. [11] (yellow), Ref. [126] (red), Ref. [100, 105] (blue).

64
Chapter 7. Argon Modeling
for the two phases. We deÔ¨Åne the uncertainty in a parameter as the ratio of the
5%-95% quantile spread to the MPV. Thus deÔ¨Åned uncertainty varies from 14% to
20% for Œµ depending on the dataset, while œÉ is identiÔ¨Åed more precisely with an
uncertainty of 2%-6%. Such a diÔ¨Äerence in the uncertainties is attributed to the
type of data used in the inference process. The location of the RDF peak, which
gives a substantial contribution to the sum of squared errors in the log-likelihood,
is more sensitive to œÉ. On the other hand, Œµ aÔ¨Äects the height of the RDF peak,
which has a smaller contribution to the log-likelihood.
We note that the values which we obtained in the calibration process are close
to those found in literature (see Table 7.2, Fig. 7.1).
Table 7.2: LJ parameters for argon used in literature. The last row shows the data
used for Ô¨Åtting. Notations: T (temperature), P (pressure), œÅ (density), B (second
virial coeÔ¨Écient), E (energy), TP (gas-liquid transition pressure), L (latent heat of
evaporation).
Ref. [100] Ref. [11]
Ref. [105]
Ref. [126]
Œµ
0.2385
0.2824
0.2381
0.2498
œÉ
3.4000
3.3605
3.4050
3.3450
T
94.4
86.64 - 168.86
137.77
88 - 127
œÅ
1.374
0.435 - 1.479
0.156, 0.972 0.283 - 3.897
Phase liquid
gas, liquid, solid gas + liquid gas + liquid
Data
RDF
P, E
TP, œÅ, L
P, B
The full set of the MPVs and distribution quantiles for each dataset RLi, RV is
given in Table 7.3. The full posterior distributions are shown in Figs. C.1 and C.2.
HB inference
Next, we infer the parameters of LJ 6-12 using the HB approach
described in Chapter 4.
We choose the hyper-prior p(œëi | œà) using Bayesian model selection, according to
which the most probable prior model is uniform (see Appendix C.1 for details). The
values of the LJ parameters are presented in Fig. 7.1 (dark red). The MPVs and the
quantiles of the parameters are almost the same as in the B12,R, which means that
for each dataset yi ‚àà{RL1, . . . , RL5, RV } no information about the parameters can
be extracted from the other datasets.

7.3. Parameters Calibration
65
The full set of the MPVs and distribution quantiles for each dataset RLi, RV is
given in Table 7.3.
Table 7.3: Posterior values of each parameter œë ‚àà{Œµ, œÉ, œÉn} of LJ 6-12: MPV b(œë)
and 5%-95% quantiles q(œë).
b(Œµ)
q(Œµ)
b(œÉ)
q(œÉ)
b(œÉn) q(œÉn)
B12,R
L3 0.286 [0.284, 0.323] 3.305 [3.250, 3.332] 0.168 [0.158, 0.314]
L4 0.255 [0.254, 0.266] 3.314 [3.301, 3.353] 0.089 [0.080, 0.140]
L5 0.263 [0.255, 0.309] 3.266 [3.110, 3.536] 0.317 [0.292, 0.586]
V
0.144 [0.083, 0.184] 3.109 [3.029, 3.213] 0.147 [0.119, 0.304]
HB12,R
L3 0.283 [0.283, 0.303] 3.300 [3.226, 3.327] 0.177 [0.156, 0.310]
L4 0.253 [0.254, 0.267] 3.333 [3.301, 3.367] 0.073 [0.073, 0.147]
L5 0.262 [0.256, 0.296] 3.269 [3.108, 3.523] 0.337 [0.301, 0.579]
V
0.190 [0.140, 0.242] 3.075 [3.001, 3.229] 0.180 [0.192, 0.385]
7.3.2
Calibration of LJ 6-p
Bayesian inference using RDF
We now include the LJ exponent p into the
parameter set œë. As in the LJ 6-12 case, we choose a uniform prior with wide enough
bounds: [0.05, 10] √ó [3, 4] √ó [6.01, 15] √ó [0, 1]. The bounds for Œµ were increased, as
compared to LJ 6-12 case, after a test run of the sampling algorithm. As will be
seen later, this is due to a strong negative correlation between Œµ and p.
Being a more Ô¨Çexible potential, LJ 6-p can simulate a wider range of thermody-
namic conditions, including L1 and L2, which result in the values of LJ parameters
similar to those obtained for the other three liquid conditions. The 95% quantile
of p, as well as its MPV, is for four out of six RDF datasets below 7.5 and for all
the datasets below 10, which is much smaller than the conventional 12. This can
be explained by the fact that the repulsion energies predicted by the standard 6-12
LJ potential are very high for the liquids. The conÔ¨Ågurations with such energies
happen with probability close to zero, and the MD simulation is not able to sample
them.
As in the LJ 6-12 case, the parameter Œµ exhibits signiÔ¨Åcant variation within
each dataset RLi, RV (uncertainty 110%-216%, computed the same way as for LJ

66
Chapter 7. Argon Modeling
6-12), while p and œÉ are well-deÔ¨Åned with the uncertainty of 5%-30% and 1%-
6%, respectively. In addition, Œµ diÔ¨Äers substantially among the RDF datasets, but
always in accordance with p: the higher the p, the lower the Œµ (see Fig. 7.3). Notably,
parameters inferred using L3 ‚àíL5 and Q (see below) lie in the same manifold, while
those calibrated using L1 ‚àíL2 sample a diÔ¨Äerent manifold and those inferred from
V lie in yet another manifold. This result gives grounds for future studies of other
thermodynamic conditions and their eÔ¨Äect on p ‚àíŒµ relation.
Once again, we observe the non-transferability of the LJ parameters from liquid
to vapor simulations: the values of œÉ lie in disjoint domains for Li and V (see
Fig. 7.2).
HB inference using RDF
We proceed by calibrating the LJ 6-p parameters
using the HB approach from Chapter 4. Details for the selection of the hyper-prior
can be found in Appendix C.1, according to which the most probable hyper-prior
model is the uniform one.
We observe that the uncertainty in Œµ gets signiÔ¨Åcantly reduced for conditions
L1, L2, L5 and V (see Fig. 7.2) indicating that the inference beneÔ¨Åted from the
information contained in the two remaining datasets RL3 and RL4 with narrow
posterior distributions of Œµ (Figs. C.3 to C.5). On the other hand, the uncertainty
in Œµ for L3 and L4 increases adjusting to the wide ranges in the other four cases.
A similar situation can be seen for p, where narrow distributions for L1, L3, L5,
V shift the posterior values for L2 and L4. The RDF is, as discussed above, very
sensitive to the changes in œÉ, which controls the location of the LJ potential well,
and therefore œÉ is well determined for each of the datasets RLi, RV and extracts
almost no information from the other data sets.
The full set of MPVs and distribution quantiles of the LJ parameters for RLi,
RV is given in Table 7.4.
Bayesian inference using quantum calculations
We examine the suitability
of the repulsion exponent 12 for gaseous argon by performing a calibration using
the calculated quantum dimer scans of argon as data.
We infer the parameters
of the LJ 6-p potential by Ô¨Åtting it to the binding energy of the quantum dimer.
The resulting MPV 12.7 of p is much closer to the conventional 12 (see Fig. 7.2,
Table 7.4), suggesting that for the gaseous argon, unlike for the liquid one, LJ 6-12
is a reasonable choice. This drastic diÔ¨Äerence in p once again shows that the same

7.3. Parameters Calibration
67
L1
L2
L3
L4
L5
V
0
2
4
6
8
L1
L2
L3
L4
L5
V
3
3.1
3.2
3.3
3.4
3.5
L1
L2
L3
L4
L5
V
6
8
10
12
p
Figure 7.2: Posterior parameter values: MPVs along with 5%-95% quantiles ob-
tained in Bp,R (light blue), HBp,R (dark blue). Horizontal line indicates the MPVs
for Bp,Q.

68
Chapter 7. Argon Modeling
Table 7.4: Posterior values of each parameter œë ‚àà{Œµ, œÉ, p, œÉn} of LJ 6-p: MPV b(œë)
and 5%-95% quantiles q(œë).
b(Œµ)
q(Œµ)
b(œÉ) q(œÉ)
b(p) q(p)
b(œÉn) q(œÉn)
Bp,R
L1 2.286 [2.193, 4.79] 3.43 [3.42, 3.46] 6.64 [6.29, 6.65] 0.157 [0.185, 0.302]
L2 3.134 [0.712, 4.42] 3.37 [3.32, 3.43] 6.37 [6.29, 8.31] 0.222 [0.215, 0.518]
L3 1.250 [0.914, 2.70] 3.32 [3.30, 3.40] 6.72 [6.33, 7.07] 0.098 [0.080, 0.159]
L4 0.337 [0.322, 1.06] 3.33 [3.33, 3.40] 9.50 [6.90, 9.84] 0.057 [0.066, 0.153]
L5 5.928 [0.794, 7.32] 3.33 [3.19, 3.40] 6.12 [6.10, 7.13] 0.164 [0.168, 0.326]
V
1.065 [0.625, 3.61] 3.12 [3.04, 3.12] 6.60 [6.15, 6.92] 0.100 [0.099, 0.190]
HBp,R
L1 4.561 [3.890, 4.63] 3.45 [3.41, 3.47] 6.30 [6.30, 6.37] 0.422 [0.450, 0.602]
L2 2.081 [1.333, 3.52] 3.39 [3.34, 3.42] 6.57 [6.34, 7.05] 0.211 [0.178, 0.351]
L3 2.506 [0.941, 4.33] 3.35 [3.30, 3.38] 6.32 [6.20, 7.00] 0.093 [0.081, 0.186]
L4 2.588 [0.892, 3.68] 3.40 [3.36, 3.43] 6.34 [6.23, 7.06] 0.082 [0.075, 0.146]
L5 2.055 [0.992, 4.28] 3.25 [3.19, 3.38] 6.36 [6.17, 6.84] 0.183 [0.159, 0.316]
V
1.371 [0.582, 1.90] 3.13 [3.08, 3.19] 6.42 [6.28, 7.03] 0.111 [0.103, 0.233]
Bp,Q
0.252 [0.239, 0.26] 3.37 [3.37, 3.38] 12.7 [12.3, 13.3] 0.006 [0.006, 0.010]
LJ parameters cannot be used to simulate diÔ¨Äerent thermodynamic states, such as
liquid, saturated vapor, or gas.
The diÔ¨Äerence between the Ri and Q datasets shows up in the region of the
subspace which gets populated. The quantum dimer-based calibration prefers high
values of p, which correspond to the tails of the distributions inferred using the RDF
data. We performed a calibration with L3 and narrow prior bounds (p ‚àà[12, 14])
to see whether this is indeed a tail of the full posterior distribution (Fig. C.7). The
narrow posterior values of Œµ are below 0.34, while the values of the full posterior
start from Œµ = 0.90, which explains why the tails of the full distributions for Li and
V have a negligible number of samples in the region p ‚àà[12, 14] preferred by the
Q-based inference.
The full set of MPVs and distribution quantiles of the LJ parameters for Q is
given in Table 7.4 and the full posterior distribution is plotted in Fig. C.6.

7.3. Parameters Calibration
69
7.3.3
Comparison of LJ 6-12 and LJ 6-p
Model selection
We select between the LJ 6-12 and LJ 6-p potentials by applying
the Bayes selection procedure described in Chapter 2.
The model selection (Table 7.5) shows that LJ 6-p is signiÔ¨Åcantly better than the
LJ 6-12 for L3 and L5. Since LJ 6-12 is not able to produce a liquid for L1 and L2,
we conclude that LJ 6-p is preferred for four RDF datasets out of six. In the case
of L4 the two potentials provide results that are indistinguishable by the Bayesian
model selection. The only dataset on which the LJ 6-12 potential produces better
results (3 times more probable than LJ 6-p) is V , the vapor case. Thus, LJ 6-p is
better (or, at least, not worse) than LJ 6-12 for all the liquid cases considered. For
the vapor case, the LJ 6-p is over parametrized, as compared to LJ 6-12.
Table 7.5: Log-evidences E12,R (Ep,R) for B12,R (Bp,R).
E12,R
Ep,R
eEp,R‚àíE12,R
L1
‚Äì
-7.05
‚Äì
L2
‚Äì
-14.8
‚Äì
L3
-9.72
2.81
2.74√ó105
L4
5.10
5.18
1.09
L5
-15.8
-8.76
1.18√ó103
V
-3.83
-4.94
3.31√ó10‚àí1
LJ potentials
Studying the reasons for LJ 6-p being more plausible than LJ 6-
12, we take a closer look at the inferred shapes of the potentials.
The LJ 6-12
parameters can alter the potential shape in two ways: Œµ scales the whole curve, and
œÉ moves the potential well. LJ 6-p has one additional parameter p, which scales
the repulsive part of the potential only, making this force-Ô¨Åeld more Ô¨Çexible. We
observe a very stable correlation in the (p, Œµ) subspace (Fig. 7.3) for all the datasets
used. This result is expected as p regulates the strength of the repulsion and Œµ alters
the strength of both repulsion and attraction simultaneously. Since the parameters
Œµ and p are highly correlated, one could expect that the inference will be able to
recover values of Œµ for LJ 6-12 such that the resulting potential is close to the inferred
LJ 6-p. However, the eÔ¨Äect that p and Œµ have on the LJ potential is not entirely the

70
Chapter 7. Argon Modeling
6
8
10
12
14
p
0.3
1
3
10
Figure 7.3: Posterior samples of Bp,R projected onto (p, Œµ) subspace.
Red: V .
Shades of blue: Li in the temperature increasing order from the lightest to the
darkest color. Green: Q. Grid lines indicate the log-scale for Œµ.

7.3. Parameters Calibration
71
same. As Œµ acts as a scaling factor for the whole potential, it is not able to make the
potential less deep and at the same time Ô¨Çat enough to avoid switching to the gas
phase (compare simulations with MPVs for L5, V in Fig. 7.4). The same reasoning
can be applied to explain the inability of LJ 6-12 to drive L1 and L2 to the liquid
phase: the potential is too repulsive, frustrating the liquid packing, and the system
behaves either like a gas or like a solid (note that L1 is close to the argon triple
point).
The full set of the inferred LJ 6-12 and LJ 6-p potentials is given in Fig. 7.4.
3
4
5
6
r
-0.4
-0.2
0
0.2
VLJ
L3
3
4
5
6
r
-0.4
-0.2
0
0.2
VLJ
L5
3
4
5
6
r
-0.4
-0.2
0
0.2
VLJ
L4
3
4
5
6
r
-0.4
-0.2
0
0.2
VLJ
V
Figure 7.4: Posterior LJ potentials comparison. Blue: MPVs with 5%-95% quan-
tiles from HBp,R.
Red: MPVs from HB12,R.
Green: MPVs from Bp,Q (when
applicable). Black: quantum dimer calculations (when applicable).

72
Chapter 7. Argon Modeling
Robust posterior prediction
The quality of the predictions, made for a QoI
diÔ¨Äerent than the one used for the inference, quantiÔ¨Åes the predictive power of
the model (see Section 4.3). We obtain robust predictions of the RDF, density œÅ
and diÔ¨Äusion coeÔ¨Écient D of argon by propagating the posterior LJ parameters
uncertainty into these quantities.
We measure the error ‚àÜg of the prediction of the scalar quantity g as
‚àÜg = 1
N
N
X
k=1
gk ‚àírk
rk
2
,
(7.3)
where N ‚â§6 is the number of thermodynamic conditions for which the prediction
can be made, gk is the prediction made using the MPV and rk is the reference
value. The accuracy of the Ô¨Åt for these computations is 0.7%. The error of the
RDF is computed as an average over all the thermodynamic conditions of the mean
squared error between the computed and the experimental RDF. The predictions
are compared on three diÔ¨Äerent sets of conditions: 1) the conditions which can be
simulated using MPVs obtained in all the three inferences HB12,R, HBp,R, and
Bp,Q (L4, L5), 2) the conditions which can be simulated using MPVs obtained in
the inferences HB12,R and HBp,R (L3 ‚àíL5, V ), 3) the conditions which can be
simulated using MPVs obtained in the inference HBp,R (L1 ‚àíL5, V ).
The predictions made using the results of HBp,R are the most accurate for all
the QoIs considered and for all but one case where HB12,R gives a better result (see
Table 7.6). On the other hand, the predictions made using the results of Bp,Q are
the least accurate for all the QoIs. Additionally, the inferences HB12,R and Bp,Q
result in LJ potentials which cannot be used to simulate all the thermodynamic
conditions. We conclude that the HBp,R produces a better LJ model than HB12,R,
and that Bp,Q does not result in a good model for liquid argon or saturated argon
vapor.
We note that the values of D diÔ¨Äer by an order of magnitude for liquid and vapor
which explains the huge deterioration of the predictions on the sets of conditions
that include V .
We observe that the dataset RL4 appears to be the most democratic one, allowing
any kind of LJ potential to Ô¨Åt it: the classical LJ 6-12, the modiÔ¨Åed LJ 6-12.7 (from
Q), LJ 6-9.5 (from Bp,R) or LJ 6-6.3 (from HBp,R).
The MPVs of D and œÅ along with the corresponding quantiles are presented in
Fig. 7.6. The same values for RDF are given in Fig. 7.5.

7.3. Parameters Calibration
73
3
4
5
6
7
8
r
0
1
2
3
4
RDF
L1
3
4
5
6
7
8
r
0
1
2
3
4
RDF
L4
3
4
5
6
7
8
r
0
1
2
3
4
RDF
L2
3
4
5
6
7
8
r
0
1
2
3
4
RDF
L5
3
4
5
6
7
8
r
0
1
2
3
4
RDF
L3
3
4
5
6
7
8
r
0
1
2
3
4
RDF
V
Figure 7.5: Robust posterior predictions of RDF. Blue: MPVs with 5%-95% quan-
tiles from HBp,R. Red: MPVs from HB12,R (when applicable). Green: MPVs from
Bp,Q (when applicable). Black: experimental data.

74
Chapter 7. Argon Modeling
L1
L2
L3
L4
L5
V
0
0.5
1
1.5
L1
L2
L3
L4
L5
V
10-5
10-4
10-3
D
Figure 7.6: Robust posterior predictions of density œÅ and diÔ¨Äusion coeÔ¨Écient D.
Blue: MPVs with 5%-95% quantiles from HBp,R. Red: MPVs with 5%-95% quan-
tiles from HB12,R (when applicable). Green: MPVs from Bp,Q (when applicable).
Grey: experimental data for œÅ, analytically computed values for D.
Grid for D
shows the logarithmic scale.

7.4. Discussion
75
Table 7.6: Errors of robust posterior predictions of RDF, density and diÔ¨Äusion
coeÔ¨Écient using LJ 6-12 and LJ 6-p.
We denote S1 = {L4, L5} (all inferences
produce the correct argon phase), S2 = {L3 ‚àíL5, V } (Bp,Q produces wrong phase),
S3 = {L1 ‚àíL5, V } (Bp,Q and HB12,R produce wrong phase).
‚àÜRDF
‚àÜœÅ
‚àÜD
S1
S2
S3
S1
S2
S3
S1
S2
S3
Bp,Q
0.087
‚Äì
‚Äì
0.118
‚Äì
‚Äì
0.136
‚Äì
‚Äì
HB12,R
0.071
0.050
‚Äì
0.029
0.108
‚Äì
0.009
1.573
‚Äì
HBp,R
0.016
0.024
0.027
0.011
0.068
0.049
0.043
1.230
0.898
7.4
Discussion
We have performed a systematic study of the modiÔ¨Åed 6-p Lennard-Jones potential
for liquid and gaseous argon using Hierarchical Bayesian inference with data from
experiments and quantum mechanics simulations.
Our results show that the value p = 12 of the repulsive exponent needs to be
calibrated together with the other LJ parameters. In the case of liquid argon we
obtain much better predictions with a smaller value p = 6.5, while for the gaseous
argon the classical p = 12 or slightly bigger p = 12.7 results in a better agreement
with the data.
Our results contradict the conclusion of Ref. [48], where LJ potentials with p =
10, 12, 14, 16, 18, 20 were Ô¨Åt to viscosity and pressure data, and the potential with
p = 12 showed better agreement for diÔ¨Äerent thermodynamic conditions.
This
mismatch can be explained by the fact that diÔ¨Äerent data was used and also that
the exponents below 10, which appear to be the best according to the results of the
current study, were not tested in Ref. [48].
Taking into account the diÔ¨Äerences in Œµ and œÉ (Fig. 7.2), as well as in p, we con-
clude that one cannot use the same values of the LJ parameters for liquid, saturated
vapor and gas.
Smaller values of the repulsion exponent (p ‚àà(6, 9)) in the Lennard-Jones po-
tential provide better predictions for RDF (Fig. 7.5), density and diÔ¨Äusion data
(Fig. 7.6) than the conventional p = 12 or p = 12.7 inferred from the quantum cal-
ibrations. Additionally, these new LJ exponents allow to simulate a larger variety

76
Chapter 7. Argon Modeling
of thermodynamic conditions, as compared to the classical 12.
We have also examined whether the smaller exponent allows for bigger time steps
in MD simulations. However, it appears that the exponent is not a critical factor for
the stability of the system. We observed similar execution times for the simulations
with MPVs of LJ 6-12 and LJ 6-p.
From the computational point of view, usage of the kriging surrogates inside
BASIS in Œ†4U resulted in a speed-up of 28%.

Chapter 8
Translocon Modeling
The work described in this chapter is done in collaboration with the group of Prof.
Dr. T. F. Miller from the California Institute of Technology.
The transmembrane channel Sec translocon is an essential piece of molecular ma-
chinery for protein biosynthesis. It has two main functions: the secretion of newly
synthesized proteins across the lipid membrane, and the integration of proteins into
the membrane. Understanding the mechanisms in the translocon is necessary for
gaining insights into the early stages of the protein folding and targeting in the cell.
Available computational techniques for simulating the interactions of the translo-
con with the newly synthesized proteins, were, until recently, unable to reach the
minute-long timescales, which are of the crucial relevance for protein biosynthesis.
However, with a newly developed coarse-grained approach of Niesen et al. [91], ex-
cessively long simulations of protein biosynthesis can now be done. In this work, we
improve the proposed coarse-grained model of the translocon, enhancing its predic-
tive capabilities.
8.1
Computational Model
The coarse-grained model of interest [91] is used for simulating the protein translo-
cation and membrane integration via the Sec translocon. It features an accurate
geometry of the ribosome and Sec translocon, obtained directly from the experi-
mental observations. The model has the CG beads corresponding to the following
77

78
Chapter 8.
Translocon Modeling
elements: the translocon, the lateral gate, and the ribosome (Fig. 8.1a). The lateral
gate can be either open, or closed.
The model is fully described in Ref. [91], and in this work we only consider the
parameters of interest. These parameters can be divided into two groups: 1) pa-
rameters describing the interactions of the nascent polypeptide chain (NC) with the
translocon, 2) parameters describing the interactions of the NC with the membrane.
The interactions between the NC beads and translocon beads are modeled using the
Lennard-Jones (LJ) potential. The parameters of this interaction are diÔ¨Äerent for
the inner and outer part of the translocon (see Fig. 8.1b). The outer beads interact
with NC with the LJ parameters Œµ1, œÉ1, and the inner beads (called ‚ÄúconÔ¨Åned‚Äù)
use Œµ2, œÉ2. The size of the conÔ¨Åned bead region is controlled by the parameters xm
(start of the region in the vertical direction), xr (thickness of the region), and r (ra-
dius of the region in the horizontal direction), see Fig. 8.1b. The interactions of NC
with the membrane are governed by the membrane radius mr and the membrane
thickness mx (see Fig. 8.1b).
8.2
Parameters Calibration
The CG translocon model has 9 parameters: œÉ1, œÉ2, Œµ1, Œµ2, xm, xr, r, mx, mr.
In the simulations, we measure the potentials of mean force (PMF) for a bead
located at diÔ¨Äerent positions in the lateral gate. We then match the resulting force
proÔ¨Åle to that obtained using the MARTINI [86] CG force-Ô¨Åeld.
Translocation
PMFs are calculated for homogeneous leucine (L) and aspartate (D) tripeptides in
two channel conÔ¨Ågurations, open O and closed C. This makes four cases in total:
DC, DO, LC, LO. The data used for calibration is presented on Fig. 8.2a.
In order to construct the model which best represents the experimental data and
provides reasonable predictions, we build a sequence of models M1‚àíM5 of diÔ¨Äerent
complexity and select the best of them given the data. The best model is then used
in predictive simulations. The unchangeable parameters of models M1 ‚àíM4 are
obtained from physical considerations. All the parameters of M5, which are not used
in the inference, are set to their posterior MPVs obtained in optimization. Leaving
the membrane parameters free allows to pick consistent values for the location of the
lipid bilayer for all four target proÔ¨Åles in the hierarchical Bayesian inference. The
parameters of all the models and their prior values are summarized in Table 8.1,
Table 8.2.

8.2. Parameters Calibration
79
y
Lateral gate
Translocon
Membrane
Nascent chain
Ribosome
x
(a) Nascent polypeptide chain, produced by the ribosome, goes through the open lateral
gate.
xm
xr
r
x
y
mr
mx
(b) ConÔ¨Åned bead region and geometrical parameters of the model.
Figure 8.1: Schematic representation of the coarse-grained model of the Sec translo-
con.

80
Chapter 8.
Translocon Modeling











	



	
(a) Data: PMF proÔ¨Åles obtained using MARTINI interactions.











	



	
(b) Posterior MPVs of M5 compared to the data.
Figure 8.2: Bayesian inference using PMF proÔ¨Åles.

8.2. Parameters Calibration
81
Table 8.1: Parameters of M1 ‚àíM4. Parameters under inference are given with
their uniform prior bounds. For the parameters not used in the inference, the values
are given. The values on the left side of the slash (‚Äô/‚Äô) are for the D substrate, while
the values on the right side correspond to the L substrate.
œÉ1
œÉ2
Œµ1
Œµ2
xm
xr
r
mx
mr
M1 [0.8, 1.5] [0.8, 1.5] [0.3, 10] [0.3, 10] -0.1/0.3 1.2/0.7 2.2/1.5
2
1.5
M2 [0.8, 1.5] [0.8, 1.5] [0.3, 10] [0.3, 10]
[-2, 2]
[0, 4]
[0, 4]
2
1.5
M3 [0.8, 1.5] [0.8, 1.5] [0.3, 10] [0.3, 10] -0.1/0.3 1.2/0.7 2.2/1.5 [1.5, 2.5] [1.5, 2.5]
M4 [0.8, 1.5] [0.8, 1.5] [0.3, 10] [0.3, 10]
[-2, 2]
[0, 4]
[0, 4]
[1.5, 2.5] [1.5, 2.5]
Table 8.2: Parameters of M5.
Parameters under inference are given with their
uniform prior bounds. For the parameters not used in the inference, the values are
given.
œÉ1
œÉ2
Œµ1
Œµ2
xm
xr
r
mx
mr
DC
1.13
1.19
0.54
0.68
-0.06
2.05
2.31
[1.5, 2.5]
[1.5, 2.5]
DO
0.91
1.13
0.66
0.30
-0.23
3.61
1.83
[1.5, 2.5]
[1.5, 2.5]
LC
0.80
0.82
0.42
1.93
0.57
0.38
3.39
[1.5, 2.5]
[1.5, 2.5]
LO
1.00
1.18
0.38
0.85
-0.40
1.41
2.81
[1.5, 2.5]
[1.5, 2.5]

82
Chapter 8.
Translocon Modeling
We use BASIS algorithm [106] and the HB approach from Ref. [129] to perform
all the inferences in this chapter.
The optimization is done with the CMA-ES
algorithm [64, 63]. All of the algorithms are implemented in our framework Œ†4U [60].
In the Bayesian inference, we assume the following covariance matrices in the
prediction error equation Eq. (2.1):
Œ£c = 0, Œ£e = diag(œÉ2
m,i), Œ£m = œÉ2I ,
(8.1)
where œÉm,i are the error bars of the data (Fig. 8.2a) and œÉ is to be inferred. Ac-
cording to the results of the Bayesian inference, the M5 greatly outperforms all the
other models in all the cases (see Table 8.3).
Table 8.3: Log-likelihood values of all the models.
DC
DO
LC
LO
M1
-120.12
-87.76
-75.74
-69.35
M2
-71.82
-65.88
-80.28
-58.49
M3
-115.33
-75.23
-66.90
-67.83
M4
-71.62
-59.62
-66.36
-51.28
M5
-45.54
-38.88
-38.51
-22.66
In the HB inference, we use the uniform hyper-prior model with wide enough
bounds.
The hierarchical calibration has identiÔ¨Åed the most probable posterior
values and the quantiles of the parameters of this model (Table 8.4). The PMF
proÔ¨Åle obtained using MPVs Ô¨Åts the data well (see Fig. 8.2b). The most probable
parameters of M5 improve the original Ô¨Åt in all the cases (compare to Fig. 2B from
Ref. [91], especially for x < 0 in the DC setting and ‚àí1 < x < 1 in the LO setting).
We note that the model error parameter œÉ is of the order of 25% of the typical value
of the PMF.
Table 8.4: Posterior information obtained in the HB inference for each of the pa-
rameters œë ‚àà{mx, mr, œÉ} of M5: MPV b(œë), 5%-95% quantiles q(œë).
b(mx)
q(mx)
b(mr)
q(mr)
b(œÉ)
q(œÉ)
M5
1.99
[1.64, 2.49]
2.03
[1.79, 2.49]
0.55
[0.21, 0.90]

8.3. Forward Uncertainty Propagation
83
8.3
Forward Uncertainty Propagation
The proteins generated by the ribosome can behave in two diÔ¨Äerent ways: they
either translocate across the lipid membrane by passing the translocon, or integrate
into the membrane via the translocon lateral gate. The probability of integration
of polypeptide segments is, to some extent, governed by the dynamics of protein
synthesis [77, 55]. We use M5 to predict the probability of protein integration into
the lipid membrane. The experimental data comes from the work of Hessa et al. [68].
We observe that the result of M5 (Fig. 8.3a) is almost indistinguishable from
the one of the original model in Ref. [91], Fig.4B.
In order to improve the Ô¨Åt, we have decided to scale the MARTINI PMF proÔ¨Åles.
Fig. 8.3b presents the results obtained using the force proÔ¨Åles scaled by 50% and
80%. We found that the higher scaling factors lead to very long trajectories due to
high barriers for the translocation of charged residues. Against the expectations,
the results remain almost the same, as compared to the unscaled proÔ¨Åles.
As a next step, we have tried to scale only the MARTINI proÔ¨Åles corresponding
to the D substrate. This allowed us to assess the eÔ¨Äect of changing the barrier for
translocation of a aspartate, while leaving the hydrophobic leucine (L substrate) un-
aÔ¨Äected. Now, the computed probability of integration shows an excellent match to
the experimental one for the MARTINI PMF of aspartate scaled by 50% (Fig. 8.3c).
The parameters of M5 used in this setting are reported in Table 8.5.
We see (Fig. 8.4) that the diÔ¨Äerence in the old M5 and the new M5 parameters
lies in the Œµ2 and the geometrical parameters (the thickness of the conÔ¨Åned bead
region xr and its radius r, the radius and thickness of the membrane mr and mx).
8.4
Discussion
We have considered a coarse-grained model of a transmembrane channel Sec translo-
con. By applying the optimization and sampling algorithms from the Œ†4U frame-
work, we have considerably improved the model performance in terms of its resem-
blance of the MARTINI PMF proÔ¨Åles. We have applied the updated model to the
simulation of protein integration/translocation, however the diÔ¨Äerence with the ex-
perimental observations was unsatisfactory. We have further investigated the model
and discovered that changing the barrier for translocation of the hydrophilic D sub-
strate (aspartate), while leaving the hydrophobic L substrate (leucine) unaÔ¨Äected,

84
Chapter 8.
Translocon Modeling






	

	
		


	
(a) Original PMF.






	

	
		







(b) Scaled PMF.






	

	
		







(c) Scaled D in PMF.
Figure 8.3: Predictions of probability of integration using M5 and diÔ¨Äerent MAR-
TINI PMF datasets. The simulation results are given in the order of increasing the
scaling coeÔ¨Écient from the lightest to the darkest color. All the datasets are Ô¨Åtted
with a hyperbolic tangent.

8.4. Discussion
85
Table 8.5: Information about the parameters of M5 obtained using the data with
D proÔ¨Åles scaled by 50%.
(a) Values of the parameters of M5 not used in the inference.
œÉ1
œÉ2
Œµ1
Œµ2
xm
xr
r
DC
1.03
1.06
0.45
0.91
-0.16
1.66
2.57
DO
0.94
0.90
0.44
0.78
-0.21
3.88
1.82
LC
0.87
0.82
0.43
2.24
0.51
0.37
1.64
LO
0.98
1.18
0.39
0.86
-0.30
1.22
2.86
(b) Posterior information gathered in the HB inference for each of the parameters œë ‚àà
{mx, mr, œÉ} of M5: MPV b(œë), 5%-95% quantiles q(œë).
b(mx)
q(mx)
b(mr)
q(mr)
b(œÉ)
q(œÉ)
M5
1.55
[1.54, 2.43]
1.81
[1.65, 2.42]
0.42
[0.2, 1]













	







Figure 8.4: Parameters of M5 for the original MARTINI PMF datasets (Ô¨Ålled cir-
cles) and D-scaled datasets (empty circles).

86
Chapter 8.
Translocon Modeling
provides an extremely good match to the experimentally observed probabilities of
integration. In terms of the model parameters, this result was obtained by adjust-
ing the conÔ¨Åned bead region size and the interactions of its atoms with the nascent
protein chain.

Chapter 9
Blood Modeling
This chapter aims at studying the behavior of one red blood cell (RBC) under
various conditions. We investigate the Dissipative Particle Dynamics-based RBC
model of Li et al. [82] using experimental data on RBC in stretching and in a simple
shear Ô¨Çow. The Ô¨Åne-grain particle-based RBC model was Ô¨Årst proposed by Discher
et al. [31] and further developed by Li et al. [82]. In this thesis we use a coarse-grain
representation of this model developed by Pivkin et al. [97] and further studied by
Fedosov et al. [41, 40].
9.1
Computational Model
Our method of choice for RBC modeling is Dissipative Particle Dynamics (DPD), a
powerful mesoscale simulation method allowing the investigation of various atomistic
systems on a coarse-grained level. Because of the coarse representation, it is possible
to study larger and more complex systems with DPD, as compared to Molecular
Dynamics (MD).
The chosen blood model consists of two essential parts: the solvent and the
RBCs. The solvent is modeled using DPD described in Section 9.1.1. The RBCs
are modeled as a mesh of DPD particles connected with springs which interact with
potentials given in Section 9.1.2.
87

88
Chapter 9. Blood Modeling
9.1.1
Solvent
Dissipative particle dynamics (DPD) is a multi-particle model. A pairwise force acts
on each particle Fij [57, 70, 37] and change its velocity (vi) and position (ri):
dri
dt = vi,
dvi
dt = 1
m
X
jÃ∏=i
Fij .
(9.1)
The force Fij consists of three parts: conservative force F C
ij , dissipative force F D
ij
and random force F R
ij
Fij = F C
ij + F D
ij + F R
ij .
(9.2)
The forces are
F D
ij = ‚àímŒ≥swD(rij)(eij ¬∑ vij)eij,
(9.3)
F R
ij = mœÉswR(rij)Œ∏seij,
(9.4)
F C
ij = asœÉswC(rij)eij. ,
(9.5)
where the weighting functions wD(rij) and wR(rij) as well as dissipation coeÔ¨É-
cient Œ≥s and random coeÔ¨Écient œÉs are related so that they satisfy the Ô¨Çuctuation-
dissipation balance [37]
wD(rij) = (wR(rij))2,
Œ≥s =
œÉ2
s
2kBT .
(9.6)
We deÔ¨Åne rij = ri ‚àírj, rij = ||rij|| and eij = rij/rij as relative position,
distance and unit vector between two particles i and j. kB is the Boltzmann constant
and T is the temperature. vij = vi ‚àívj is the relative velocity, and Œ∏s is a Gaussian
random variable with the properties [57]
Œ∏s(t) = 0,
Œ∏s(t)Œ∏kl(t‚Ä≤) = (Œ¥ikŒ¥jl + Œ¥ilŒ¥jk)Œ¥(t ‚àít‚Ä≤) .
(9.7)
We choose standard weighting functions wD(rij), wR(rij) and wC(rij) [57]
wD(rij) = (wR(rij))2 = (wC(rij))2 =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

1 ‚àírij
rc
2
, rij < rc
0, rij ‚â•rc
.
(9.8)

9.1. Computational Model
89
9.1.2
RBC model
We use the RBC model proposed in [82]. Within this model the RBC membrane, as
well as the internal and external Ô¨Çuids, are modeled with DPD particles. An RBC
is represented by a set of Nv vertexes that form a 2-d triangulated network with Ns
edges and Nt triangles (see Fig. 9.1).
Figure 9.1: RBC membrane.
The conditions imposed on the cell are: volume and area conservation, bend-
ing constraints, and prescribed spring behavior. Thus, the forces acting on each
membrane particle from the other membrane particles are given by the following
sum [92]:
F cell =
N
X
n=1
F dihedral,1
n
+ F dihedral,2
n
+ F triangle
n
+ F bond
n
,
(9.9)
where n is the index of a neighboring vertex, N is the number of neighbors of the
vertex under consideration.
The force components are [40]:
F dihedral,1
n
= Œ≤n,n+1
Œæn √ó an+1 + Œæn+1 √ó an
ŒænŒæn+1
‚àícos Œ∏n,n+1
Œæn √ó an
Œæ2n
+ Œæn+1 √ó an+1
Œæ2
n+1

,
(9.10)
representing the bending force due to the angle between the triangles with vertexes
(0, n, n + 1) and (0, n + 1, n + 2).

90
Chapter 9. Blood Modeling
F dihedral,2
n
= Œ≤n,n+N
Œæn+N √ó an
ŒænŒæn+N
‚àícos Œ∏n,n+N Œæn √ó an
Œæ2n

,
(9.11)
representing the bending force due to the angle between the triangles with vertexes
(0, n, n + 1) and (n, n + N, n + 1).
Here, ri is the radius-vector of the particle i, ai = ri ‚àíri+1, Œæi is the outer
normal of the triangle (0, i, i + 1) for i <= N and of the triangle (i ‚àíN, i, i ‚àíN + 1)
for i >= N, Œ∏s is the instantaneous angle between Œæi and Œæj (see Fig. 9.2), and
Œ≤s = kb sin Œ∏s cos Œ∏0 ‚àícos Œ∏s sin Œ∏0
‚àö
1 ‚àícos2 Œ∏s
,
(9.12)
where
Œ∏0 = arccos
‚àö
3(Nv ‚àí2) ‚àí5œÄ
‚àö
3(Nv ‚àí2) ‚àí3œÄ

(9.13)
is the equilibrium angle and the bending constant kb is user-deÔ¨Åned.
4
r3
(0, 0, 0)
3
5
6
2
7
8
0
1
a4
b5
Œ∏1,7
9
10
11
12
A6
Œæ2
Figure 9.2: RBC: Interactions of a single vertex.

9.1. Computational Model
91
The component
F triangle
n
= kd A0 ‚àíAn
A0
Œæn √ó an
4An
(9.14)
+ ka Atot
0
‚àíAtot
Atot
0
Œæn √ó an
4An
(9.15)
+ kv V tot
0
‚àíV tot
V tot
0
Œæn + (r0 + rn + rn+1) √ó an
18
(9.16)
consists of the area conservation force for the triangle (0, n, n + 1), the global area
conservation force and the volume conservation force, respectively. Here, Atot
0
(V tot
0
)
is the target RBC area (volume) deÔ¨Åned from experiments, Atot (V tot) is the instan-
taneous RBC area (volume). A0 = Atot
0 /Nt is the target area of each triangle, An is
the instantaneous area of the triangle (0, n, n + 1), Nt = 2(Nv ‚àí2). The constants
kd, ka, kv are user-deÔ¨Åned.
Finally, the bond force of the spring (0, n) consists of the worm-like chain force,
dissipative force and random force, given by
F bond
n
= ‚àíkBT
p

1
4(1 ‚àíxn)2 ‚àí1
4 + xn

+ kp
b2n
(9.17)
‚àíŒ≥C(3vij + (vij ¬∑ eij) eij)
(9.18)
+ 2
p
3kBTŒ≥CdW S
ij eij ,
(9.19)
where kB is the Boltzmann constant, T is the system temperature, bn = ||rn ‚àír0||.
The matrix in the random component is dW S
ij = dW S
ij ‚àítr[dW S
ij]1/3, where dWij
is a random matrix of independent Wiener increments and dW S
ij is its symmetric
part: dW S
ij = (dWij +dW T
ij )/2. Other parameters are computed from the following
relations:
xn = bnx0/l0,
(9.20)
kp = kBT(4x2
0 ‚àí9x0 + 6)l2
0
4(x0 ‚àí1)2p
,
(9.21)
where l0 =
q
4A0/
‚àö
3 is the spring equilibrium length. The parameters x0, p and
Œ≥C are user-deÔ¨Åned.

92
Chapter 9. Blood Modeling
9.1.3
Simulation Setup
The default parameter values are the following: Nv = 498, as = 5, Œ≥s = 15,
kBT = 0.1, rc = 1.5, x0 = 0.45, p = 2 √ó 10‚àí3, Œ≥C = 10, ka = 5000, kd = 200,
kb = 100, kv = 5000. Unlike in MD, the DPD units are not directly connected to
the physical units. We thus have to match the simulation and the experiment in
some other way, e.g. through characteristic non-dimensional numbers.
9.2
RBC Parameters
The parameters of the RBC model to be calibrated are the inverse maximum spring
extension x0, the spring persistence length (p), the membrane viscosity (Œ≥C) and
the membrane bending rigidity (kb). The area and volume conservation constants
should be big enough for the membrane and the RBC itself to be incompressible.
There have been several studies of the membrane parameters of the chosen RBC
model [41, 82]. In Ref. [82] the authors adjust one of the derived parameters of
the model to Ô¨Åt the stretching data. In Refs. [41, 40] the authors study various
experiments, including stretching, placing RBC in a shear Ô¨Çow, and attaching a
micro-bead to it. The obtained parameters provide a good Ô¨Åt for the experimental
data, however no uncertainty bounds are provided.
We consider two experiments: stretching of an RBC and its rotation in a simple
shear, which allow us to infer all the RBC parameters, except for the bending
constant kb.
9.3
Red Blood Cell Stretching
We analyze the elongation of the RBC subject to various stretching forces. The
experimental data comes from Ref. [116] and is depicted in Fig. 9.3. The stretching
force is measured in pN.
We choose 2% of the membrane vertexes from two opposite sides of the blood cell
to apply the stretching force. To measure the RBC diameters, we bring the cell to
its principal axes and determine the maximum and minimum coordinates in the two
directions of interest. For obtaining the maximum (minimum) coordinate, we take
an average value over the 7 vertexes with the maximum (minimum) coordinates.
The number is chosen to be the typical degree of a vertex in the triangulation plus

9.3. Red Blood Cell Stretching
93
0
50
100
150
200
stretching force
4
6
8
10
12
14
16
18
20
RBC diameters
Figure 9.3: Experimental values of the axial and transverse diameters of an RBC
stretched with optical tweezers [116].
one (the vertex itself).
Before doing the parameter calibration, we did the resolution study and the
sensitivity analysis of the RBC parameters. We observe that 500 vertexes is enough
to resolve the stretching experiment (see Fig. 9.4), which is in agreement with the
results of Ref. [41].
Figure 9.4: Stretching of RBCs at diÔ¨Äerent resolutions. Yellow: Nv ‚àº500, red:
Nv ‚àº2000, blue: Nv ‚àº8000.
Investigating the eÔ¨Äect of the various RBC parameters onto the axial and trans-
verse RBC diameters, we found that only the spring parameters p and x0 change
the model behavior, see Fig. B.1, Fig. B.2.

94
Chapter 9. Blood Modeling
To relate the DPD simulation to the physical reality, one needs to choose unit
scaling. The time scale does not play a role in this setting. The length scale is also
of no importance, as one can normalize the data (axial and transverse diameters)
to the RBC size under zero stretching force. According to the form of the bond
force (Eq. (9.17)), setting the force scale is equivalent to changing p. We thus set
an arbitrary force (mass) scale to 0.025 pN, meaning that one DPD unit of force
corresponds to 0.025 pN.
9.3.1
Parameters Calibration
In order to reduce the computational eÔ¨Äort, we will construct and use a surrogate
model. The sensitivity analysis has shown that only the bond force parameters aÔ¨Äect
the result of the stretching procedure, thus the surrogate should only depend on p
and x0. As was noticed above, the persistence spring length p linearly rescales the
force acting on the spring. For this reason, one can compute the axial and transverse
diameters for one value of p only (and multiple forces and values of x0) and obtain
the diameters for a diÔ¨Äerent p by linear scaling. Now, the surrogate only depends
on one parameter and can be constructed by a simple polynomial interpolation.
We ran the stretching experiment for the values of x0 ranging from 0.3 to 0.6
and Ô¨Åtted the diameters for each of the cases using the following functional form:
f(x) = a + (1 ‚àía)e‚àítx ,
(9.22)
where x takes the values of force and a and t are the Ô¨Åtting parameters. The result
of one of the Ô¨Åtting is presented on Fig. 9.5.
Now, we build an approximation for the parameters of the Ô¨Åtting function for
diÔ¨Äerent values of x0. We use polynomials up to degree 5, their coeÔ¨Écients are given
in Table 9.1. The plots of the approximations are given on Fig. 9.6. These approx-
imations are only intended to be used for interpolation. They prescribe a relatively
simple functional form which provides an almost linear interpolation between the
data points. From the Eq. (9.22), it is evident that for p‚Ä≤, which is diÔ¨Äerent from p
for which the approximation was computed, one needs to rescale t as t‚Ä≤ = t ¬∑ p/p‚Ä≤.

9.3. Red Blood Cell Stretching
95
















	

	


	


Figure 9.5: Example of Ô¨Åtting the RBC diameters under stretching with the function
from Eq. (9.22).
Table 9.1: CoeÔ¨Écients of the polynomial approximation of parameters a, t from
Eq. (9.22) as a function of x0. CoeÔ¨Écient ci corresponds to degree i.
c0
c1
c2
c3
c4
c5
a (axial)
5.6
-11.07
7.38
0.0
0.0
0.0
t (axial, x0 < 0.325)
0.01448
-0.00222
0.0
0.0
0.0
0.0
t (axial, x0 ‚â•0.325)
0.0212
-0.0268
0.0123
0.0
0.0
0.0
a (transverse)
-16.465
208.7
-1010.7
2387.8
-2746.0
1233.7
t (transverse)
0.339
-2.378
6.456
-7.72
3.393
0.0

96
Chapter 9. Blood Modeling










	
(a) Parameter a for the axial diameter.







	



(b) Parameter t for the axial diameter.









(c) Parameter a for the transverse diameter.









	
(d) Parameter t for the transverse diameter.
Figure 9.6: Approximation of the Ô¨Åtting function (Eq. (9.22)) parameters.
We assume the following terms in the prediction error equation Eq. (2.1):
Œ£e,Œ∑ = diag(œÉ2
e,Œ∑,i),
(9.23)
Œ£m,Œ∑ = œÉ2
Œ∑I,
(9.24)
Œ£c,Œ∑ = diag(œÉ2
c,Œ∑,i) ,
(9.25)
where Œ∑ ‚àà[a, t] indicates the axial and transverse diameters. The index i corresponds
to the diÔ¨Äerent stretching forces. The values of œÉa,e,i and œÉt,e,i are the errorbars
from Fig. 9.3. The computational errors are negligibly small, as compared to the
experimental ones, and we take œÉa,c,i = œÉt,c,i = 0. The model errors œÉa and œÉt are
to be inferred.
Not possessing any information about the model parameters, we assume the
uniform prior with wide enough bounds (see Table 9.2). We perform the calibration
using TMCMC with 10‚Äô000 samples per stage. The algorithm converges in 5 stages,

9.3. Red Blood Cell Stretching
97
taking about a minute for a serial run.
After running the inference, we observe an unidentiÔ¨Åable manifold in the (x0, p)
space, Fig. 9.7. This shows that the spring parameters should be changed simulta-
neously in order to obtain the correct model behavior. This is an expected result,
since p is inversely proportional to the strength of the spring, while increasing x0,
on the contrary, makes the spring stronger. Both p and x0 are well-deÔ¨Åned given
the stretching data, with a preference for medium values of x0 and small values of p.
We note that the model errors œÉa, œÉt are small compared to the experimental error
(0.4¬±0.3 vs 1.2¬±0.4 for the axial diameter and 0.3¬±0.2 vs 0.7¬±0.1 for the transverse
diameter).
Figure 9.7: Results of parameter calibration of the RBC membrane parameters using
TMCMC (see Notations).

98
Chapter 9. Blood Modeling
Table 9.2: Prior and posterior information on the spring parameters of the RBC
membrane model.
Prior bounds
Posterior MPV
Posterior 5%-95% quantiles
x0
[0.3, 0.6]
0.457
[0.332, 0.484]
p
[0.001, 5]√ó10‚àí2
0.136√ó10‚àí2
[0.101, 0.391]√ó10‚àí2
œÉa
[10‚àí6, 1]
0.019
[0.015, 0.554]
œÉt
[10‚àí6, 1]
0.052
[0.037, 0.675]
9.3.2
Forward Uncertainty Propagation
We propagate the sampling and model uncertainty into the diameters of the RBC
under stretching and into the frequency of the RBC rotation in the shear Ô¨Çow using
Eq. (2.6).
The quantiles of the robust posterior distribution for the diameters are given on
Fig. 9.9.
0
50
100
150
200
stretching force
4
6
8
10
12
14
16
18
20
RBC diameters
Figure 9.8: Posterior predictions of RBC diameters under stretching.
Red line:
experimental data, blue line: posterior MPV, blue area: 5%-95% quantiles.
There exist three diÔ¨Äerent behaviors of RBCs in shear Ô¨Çow. For low shear rates
RBCs tumble [35], for medium shear rate they swing [4], and for high shear rates
cells tank-tread [118]. As the experimental data is available for tumbling and tank-

9.4. Red Blood Cell in Shear Flow
99
treading, we propagate the uncertainty in the RBC parameters to these two modes.
The quantiles of the robust posterior distribution are given on the Fig. 9.9.
0
5
10
15
20
normalized shear rate
0
2
4
6
8
10
12
14
16
18
20
B  (A + A-1)
(a) Tumbling mode.
0
1000
2000
3000
4000
5000
6000
7000
normalized shear rate
0
500
1000
1500
normalized TTF
(b) Tank-treading mode.
Figure 9.9: Posterior predictions of the frequency of RBC in shear Ô¨Çow. Calibration
using the data on stretching. Original model. Red line/crosses: experimental data,
blue line: mean and 5%-95% quantiles of the predictions. Data from [35] and [118].
Predictions made by the model are very accurate for tumbling. This good Ô¨Åt
can be explained by the simplicity of the motion: in this case an RBC rotates as a
solid body. On the other hand, the predictions for a more complex movement, such
as tank-treading, are inferior. The slope of the line is not predicted correctly and
the point at ‚âà140s‚àí1 does not lie close to the line. The next section investigates
this issue.
9.4
Red Blood Cell in Shear Flow
There exist several experimental datasets describing tank-treading of a single RBC [118,
45, 46, 47]. The data consists of the RBC tank-treading frequency (TTF) measured
for various shear rates, see Fig. 9.10. To put all the data on one plot, we rescale the
reported shear rate multiplying it by viscosity. The shear rate is measured in 1/s.

100
Chapter 9. Blood Modeling
The TTF is measured in rad/s. A linear Ô¨Åt describes well all of the experiments,
although for certain cases the exponential Ô¨Åt provides slightly better results [47]. We
see a signiÔ¨Åcant variation in the slopes of the linear Ô¨Åts of TTF (Table 9.3). There-
fore, we have decided to focus on the dataset of Fischer (1980), since it exhibits the
smallest error of the linear Ô¨Åt (0.5%).







	

	












Figure 9.10: RBC tank-treading frequency in the shear Ô¨Çow along with the linear
Ô¨Åt. The shear rate is normalized with the viscosity of the solvent. The experiments
are listed in the order of viscosity increase (from blue to red).
We adjust the solvent parameters to match the viscosity contrast of the chosen
dataset. We set Œ≥i = 5, Œ≥o = 11.5, where the subscript o corresponds to the solvent
outside the RBC, and i corresponds to the solvent inside the RBC.
9.4.1
Parameter Calibration
Following the same approach as for the stretching, we perform a local sensitivity
analysis to determine which parameters aÔ¨Äect the TTF. We found that 3 parameters
are sensitive: the two spring parameters x0 and p and the membrane viscosity
parameter Œ≥C (see Fig. B.3, Fig. B.4). Since the spring parameters were already
calibrated in the stretching experiment, we proceed with determining the membrane
viscosity.
Before doing the calibration, we found it necessary to increase the local area
conservation parameter, as otherwise the triangles were collapsing at high shear

9.4. Red Blood Cell in Shear Flow
101
Table 9.3: Linear Ô¨Åts and their errors for RBC tank-treading frequency in the shear
Ô¨Çow.
Reference
Viscosity (cP)
Slope
Error
Fischer (1978)
11.0
0.140
8.15%
Fischer (1978)
18.0
0.153
4.02%
Fischer (1980)
23.0
0.185
0.50%
Fischer (2007)
28.9
0.175
3.27%
Fischer (1978)
31.0
0.152
2.51%
Tran-Son-Tay (1984)
35.0
0.217
0.88%
Fischer (1978)
59.0
0.161
3.29%
rates (see Fig. 9.11). This appeared to also be the reason of the non-linear behavior
in Fig. 9.9b.
Figure 9.11: Collapse of the triangles at shear rate 12 in DPD units. Left: kd = 200,
right: kd = 5000.
After that, we have performed a resolution study, which suggested that, as for
the stretching, 500 points is enough to resolve the RBC rotation (see Fig. 9.12).
In order to speed up the computations, we construct a surrogate model which re-
places the expensive runs of the shear Ô¨Çow simulation. Since we have only one param-
eter, we constructed a simple interpolating polynomial (referred to as ‚Äúsurrogate‚Äù),
see Fig. 9.13. The coeÔ¨Écients of the surrogate are c0 = 0.196, c1 = ‚àí6.88 √ó 10‚àí4,
c2 = 3.96 √ó 10‚àí6. As before, ci corresponds to the degree i. The variance of the
residuals of the Ô¨Åt is 8.46√ó10‚àí3.
Since there is only one measurement, the prediction error equation covariances,

102
Chapter 9. Blood Modeling
Figure 9.12: Resolution study of RBC in shear Ô¨Çow.
Yellow: Nv ‚àº500, red:
Nv ‚àº2000.








	






	
Figure 9.13: Approximation of the TTF as a function of Œ≥C.

9.4. Red Blood Cell in Shear Flow
103
Table 9.4: Prior and posterior information about the parameters of the RBC mem-
brane in the shear Ô¨Çow experiment.
Prior bounds
Posterior MPV
Posterior 5%-95% quantiles
Œ≥C
[0, 60]
17.8
[3.62, 53.5]
œÉ
[0.01, 100]√ó10‚àí4
7.66√ó10‚àí4
[5.94, 95.3]√ó10‚àí4
as deÔ¨Åned in Eq. (2.1), simplify to:
Œ£e = œÉ2
e, Œ£m = œÉ2, Œ£c = œÉ2
c ,
(9.26)
The experimental error in this case (measured as the error of the TTF Ô¨Åt) is
negligibly small, and thus œÉe = 0.
The computational error is taken from the
surrogate Ô¨Åt, œÉc = 8.46 √ó 10‚àí3. The model error œÉ is to be inferred.
Using this surrogate, we run TMCMC with 10‚Äô000 samples per stage. In this
case, we have decided to limit the model error to 0.01, so that it is close to the
other error terms. We observe that the prior range for Œ≥C does not get signiÔ¨Åcantly
reduced, but the shape of the distribution changes (see Fig. 9.14). The inference has
identiÔ¨Åed the posterior most probable value of the membrane viscosity parameter
(see Table 9.4), which can be used in the further RBC model investigation.
9.4.2
Forward Uncertainty Propagation
We now propagate the posterior uncertainty in Œ≥C into the tank-treading motion of
a single RBC, see Fig. 9.15. The prediction is now linear, unlike in Fig. 9.9b. We
observe that the spread of the uncertainty increases for high shear rates. This is a
natural consequence of the uncertainty in the TTF slope.
To further investigate the predictive capabilities of the considered RBC model,
we study its behavior in the shear Ô¨Çow at low shear rates. This setup was fully
studied by Dupire et al. [35], who observed the following qualitative behavior. At
very low shear rates, the cell performs a Ô¨Çipping motion (Fl), characterized by the
rotation of its axes in the direction of the Ô¨Çow and swinging of the axis orthogonal
to the Ô¨Çow (see Fig. 9.16).
When the shear rate is increased, the cell selects from the wheel-like (W) and the
frisbee-like (Fr) motions, which are followed by the tank-treading (TT) as the shear

104
Chapter 9. Blood Modeling
Figure 9.14: Posterior distribution of the RBC membrane viscosity parameter Œ≥C
obtained with TMCMC (see Notations).
0
2000
4000
6000
8000
10000
12000
normalized shear rate
0
500
1000
1500
2000
2500
normalized TTF
Figure 9.15: Robust posterior quantiles of TTF obtained after calibrating the mem-
brane viscosity parameter. Red line: experimental data, blue line: posterior MPV,
blue area: 5%-95% quantiles of the predictions.

9.4. Red Blood Cell in Shear Flow
105
(a) Flipping.
(b) Wheel-like motion.
(c) Frisbee-like motion.
Figure 9.16: Schematic representation of RBC motion in various shear rates.
rate is further increased. In the W and Fr types of motions one of the cell‚Äôs axes
orthogonal to the Ô¨Çow direction is Ô¨Åxed and the other two are rotating in a plane
(see Fig. 9.16). In all the types of motion, except for the TT, the RBC behaves like
a solid object, meaning that it‚Äôs almost not deformed.
If the cell is placed in a high shear rate subject to a further decrease of the shear
rate, the sequence of transformations is slightly diÔ¨Äerent. First, the cell tank-treads,
after which it enters the intermittent regime of Fl-TT, followed by W and Ô¨Ånally
pure Fl.
It turns out that the paths which the cell takes while deforming from Fl to TT
with the increase of the shear rate and from TT to Fl with the decrease of the shear
rate, diÔ¨Äer not only qualitatively, but also quantitatively. The authors in Ref. [35]
have also measured the critical stresses, required to trigger the solid-to-Ô¨Çuid and
Ô¨Çuid-to-solid RBC transformations. The values found are 0.099 Pa and 0.297 Pa,
which is in agreement with the observations of Abkarian et al. [4], where is critical
stress was found to be of the order of 0.1 Pa.
We use the most probable posterior parameters, calibrated using the stretching
diameters and the TTF, to run the RBC model in the shear rate varying from low
to high and vice versa. Our model reproduces all the qualitative features of the
RBC behavior. It gives the correct sequence of transformations for both directions
of shear rate changes and correctly identiÔ¨Åes the presence of hysteresis. However,
the predicted critical stress is approximately 3 times higher than that observed in
the experiments ( 0.3 Pa instead of 0.1 Pa).

106
Chapter 9. Blood Modeling
On the other hand, the UQ-based calibration gives us a whole distribution of
the possible model parameters. The less likely parameters in the area of low x0
values result in a critical stress equal or even smaller than the experimental one,
while keeping the right sequence of transformations and the hysteresis. This eÔ¨Äect
can be explained by the changes in the membrane shear modulus. The area of small
values of x0 in the RBC parameter distribution corresponds to the low values of
the membrane shear modulus, which means a highly deformable cell. As a result,
such a cell needs less stress to become Ô¨Çuid-like. At the same time, a more soft cell
stretches easier, which means that its axial and transverse diameters (see Fig. 9.9)
are not optimal.
This shows that optimizing the parameters with respect to a given experiment
is sometimes too restrictive and not suÔ¨Écient to make a consistent model, while a
more relaxed Bayesian methodology allows to Ô¨Ånd a compromise between diÔ¨Äerent
experimental data sets.
9.5
High performance computing
This section presents estimates on the speed-up gained with a help of the surrogates.
We performed our computations on Piz Daint cluster of the Swiss National Su-
percomputing Center (CSCS). The BASIS algorithm was run inside the Œ†4U frame-
work [60]. The blood cell was simulated using the uDeviceX code [?], implemented
with CUDA.
An average time needed for one stretching run on one GPU is 1 minute. We
have performed 13 runs to build the surrogates, spending in total 13 GPU-minutes.
The run-time of BASIS with 10‚Äô000 samples and these surrogates was 1.5 minutes
(6 stages) on a single CPU. Its run-time with full model evaluations with the same
number of samples would have been 10‚Äô000 minutes per stage, making the total run-
time to be 60‚Äô000 minutes, or 1‚Äô000 hours on one GPU. The approximate speed-up
is thus 60000/(13 + 1.5) ‚âà4138 times.
A similar situation arises for shear Ô¨Çow. An average time needed for one shear
Ô¨Çow run on one GPU is 1.4 hours. We have performed 31 runs to build the sur-
rogates, spending in total 43.4 GPU-hours. The run-time of BASIS with 10‚Äô000
samples and these surrogates was 15 seconds (2 stages) on a single CPU, which is
negligibly small, as compared to the surrogate construction time. Its run-time with
full model evaluations with the same number of samples would have been 14‚Äô000

9.5. High performance computing
107
hours per stage, making the total run-time to be 28‚Äô000 hours on one GPU. The
approximate speed-up is thus 28000/43.4 ‚âà645 times.


Part III
Conclusions and Outlook
109


Chapter 10
Conclusions and Outlook
10.1
Œ†4U Framework
This thesis presents the following extensions of the Œ†4U framework:
1) ABC-SubSim algorithm,
2) kriging- and neural network-based surrogates,
3) visualization tools,
4) sensitivity tool.
The Œ†4U framework is a great tool for running parameter calibration and un-
certainty propagation in cases where performance is a concern. Its adaptive load
balancing and easy coupling with any external software makes Œ†4U the key frame-
work in the area of HPC-aware Bayesian computations.
The availability of the visualization tools gives a possibility to immediately re-
view and analyze the results. Additional inference tools, such as optimization, sen-
sitivity tool or surrogates, help to get insights into the parameter space geometry
before running the full inference.
The Œ†4U framework is currently at an intensive development stage. The recent
contributions include: python and MATLAB bindings, multi-objective optimization,
surrogates. Optimal experimental design algorithms are now under development.
However, there is plenty of work to be done, mainly in bringing the framework and
the potential user together.
111

112
Chapter 10.
Conclusions and Outlook
There now exist several branches containing diÔ¨Äerent features (e.g. the code for
surrogates is separated from the main branch) which in the future are to be merged
into one. Since all the algorithms use the same target function (the likelihood),
we would ideally like to have a tool to conÔ¨Ågure the run by specifying the options,
such as which algorithm to use or which surrogate to use. Based on the algorithmic
similarities outlined in Chapter 5, such a modiÔ¨Åcation should be possible. At the
moment the conÔ¨Åguration Ô¨Åle only exists for TMCMC and BASIS.
We have recently added a user manual on Œ†4U, and we are planning to create
a full documentation and the developer guide.
The best possibility to promote
the Œ†4U framework would probably be to create a website for it and make a user
interface. At the moment, the description of the code and a latest stable code release
can be obtained at cse-lab.ethz.ch/software/Pi4U.
Within this thesis, we have demonstrated the use of the various tools of the Œ†4U
framework in applications to large-scale problems in Life Sciences and Engineering
discussed in the next section.
The tools we have used are: the sensitivity tool,
the sampling algorithms (TMCMC, BASIS and ABC-SubSim), the optimization
algorithm (CMA-ES), the forward uncertainty propagation tool, the visualization
tools (for samples and model predictions).
10.2
Applications
We have considered four applications of the Bayesian uncertainty quantiÔ¨Åcation:
blood, helium, argon and translocon.
Helium Modeling
We have illustrated the application of Subset Simulation for
Bayesian inference in the context of ABC by calibrating the potential well depth
and the well location parameters of the Lennard-Jones potential of helium. The
data was chosen to be the Boltzmann factor distribution over time. Since the data
follows a non-Gaussian distribution, the likelihood can not be formulated, and we
instead applied the ABC.
The ABC inference requires formulating a discrepancy function. To check the
eÔ¨Äect of the Gaussian assumption, usual for the likelihood-driven Bayesian uncer-
tainty quantiÔ¨Åcation, we have formulated three diÔ¨Äerent discrepancies, one of which
is based on the suÔ¨Écient statistics of the Gaussian distribution (mean and variance).
We found that the mean and variance-based discrepancy is not able to reduce the

10.2. Applications
113
uncertainty in the LJ well depth parameter. This suggests that the exact Bayesian
calibration can not replace the ABC, which deÔ¨Ånitely has its place among the other
inferences.
When the Gaussian discrepancy was not able to identify the LJ parameters,
we looked into the other two discrepancies: the quantile-based and the relative
entropy. Both of them were able to correctly identify the most probable values and
to reduce the prior uncertainty.
Shell [109] stated that for the MD ensembles a
relative entropy is a natural choice of the discrepancy measure, which is conÔ¨Årmed
by our computations.
The ABC calibration approach is very intuitive, as it only utilizes the comparison
between the data and the simulation outcome. Equipped with a Kullback-Leibler
(relative entropy), ABC provides a direct link to the MD ensemble averaging of
thermodynamic QoIs, resulting in a natural framework for calibration of any types
of force-Ô¨Åelds.
In all of the inferences, we used the full MD simulations. All the inferences were
made on the Piz Daint cluster of CSCS. The LAMMPS script used in the inference
is given in Appendix D.1.
Argon Modeling
We have addressed the problem of calibrating not only the
potential well depth and the well location LJ parameters, but also the repulsion
exponent, which is usually taken to be 12. The target substance was argon and
the calibration data was the radial distribution function (RDF) at six diÔ¨Äerent
thermodynamic conditions, among which Ô¨Åve described a liquid argon and one an
argon vapor.
We performed two types of inferences: one with the repulsion exponent Ô¨Åxed to
12, and the other with the free exponent. To assess whether including the exponent
into the calibration set improves the argon model, we have performed several tests:
Bayesian model selection for the cases with a free and a Ô¨Åxed exponent, forward
predictions for the RDF, density and diÔ¨Äusion coeÔ¨Écient.
Our results show that including the repulsive exponent into the calibration set
improves the predictions for all the tests and most of the thermodynamic conditions
considered. This outcome is as well supported by the model selection: the structural
model with a free exponent is in four out of six cases better than the Ô¨Åxed exponent
model, in one case they are equally good and in one more case (the only vapor case)
the model with a free exponent is over parameterized.

114
Chapter 10.
Conclusions and Outlook
Since there are several datasets, we perform a hierarchical Bayesian inference
which allows us to split the uncertainty due to parameter variability inside one set
and the uncertainty due to parameter variability among diÔ¨Äerent datasets. After ap-
plying the Bayesian inference, we observe the uncertainty reduction in the majority
of the predictions.
In all of the inferences, we used the kriging surrogates to reduce the computa-
tional cost, resulting in a speed-up of 28%. All the inferences were computed on
the Brutus cluster of ETHZ. The LAMMPS script used in the inference is given
in Appendix D.2.
Translocon Modeling
The last example of a Bayesian inference is quantifying
the uncertainties in the coarse-grained translocon model from Ref. [91].
The model contains three main parts: the ribosome, the translocon, and the
protein chain produced by the ribosome. The translocon, in turn, is built from two
types of particles: the normal and the conÔ¨Åned one, having diÔ¨Äerent LJ interactions
with the nascent protein chain.
The data used for calibration comes from another coarse-grained simulation
which uses the MARTINI force Ô¨Åeld [86]. This simulation computes the force ex-
erted by the translocon onto a bead moved through the channel. Two types of beads
are possible: D, the aspartate, and L, the leucine, and two types of the translocon
conÔ¨Ågurations: open and closed, making it four simulations in total.
There are nine parameters of interest in this model: the two sets of LJ parameters
and Ô¨Åve parameters describing the geometry of the translocon and its conÔ¨Åned bead
region. Due to a relatively high dimensionality of the problem, we have considered a
number of structural models which assess diÔ¨Äerent sets of the translocon parameters.
Using Bayesian model selection, we chose the most plausible structural model. This
model has two parameters: the cell membrane thickness and the translocon width,
while the values of all the other parameters are set to their posterior most probable
values for each of the four MARTINI cases.
The best model was further used for the forward predictions. We have obtained
a closer match for all the four MARTINI force proÔ¨Åles, as compared to the original
parameter values from Ref. [91].
We further predicted the probability for the nascent protein chain to be inte-
grated into the cell membrane (versus it being translocated inside the cell), but got
the same result as the original model. This gave us an idea to try diÔ¨Äerent scal-

10.2. Applications
115
ings of the MARTINI force proÔ¨Åles, allowing for lower barriers for translocation of
either aspartate (D) only, or both substances simultaneously Finally, we found that
scaling the proÔ¨Åle for the D substrate by 50% gives an excellent match with the
experimental results.
The potential use of the model includes investigating phenomena, such as po-
sition dependence of the biological hydrophobicity scale [69] and the dependence
of the hydrophobicity on the amino-acid residues Ô¨Çanking the transmembrane do-
mains [67]. Considering these new experiments may aÔ¨Äect the already calibrated pa-
rameters and activate the currently insensitive ones, changing the translocon model.
All the inferences were computed on the Piz Daint cluster of CSCS.
Blood Modeling
We have studied a coarse-grained, particle-based model of a
red blood cell from Ref. [40]. The RBC in this model is represented by 500 DPD
particles connected with springs.
Using experimental data on the cell behavior under stretching and in a simple
shear Ô¨Çow, we have calibrated three out of four RBC parameters. The Ô¨Årst two
parameter, the inverse maximum spring extension and the persistence length of
the spring, were calibrated in the stretching experiment.
The third parameter,
the membrane viscosity, was calibrated in the shear Ô¨Çow experiment. The fourth
parameter, the bending rigidity of the cell membrane, did not aÔ¨Äect the results of
either of the two experiments.
To calibrate all the parameters, we used an exact non-hierarchical Bayesian infer-
ence with the BASIS algorithm. In all of the inferences, we constructed polynomial-
based surrogates to reduce the computational cost. All the inferences were made on
the Piz Daint cluster of CSCS. The code used for the simulations is called uDeviceX
and is available at github.com/udevicex.
Our uncertainty propagation results show that the RBC model of Pivkin et
al. [97] provides good agreement with the experiments. The Bayesian UQ method-
ology helped us to identify the RBC membrane parameters which provide a com-
promise between matching the stretching data and the shear Ô¨Çow data.
The future work has several direction:
1) Ô¨Ånish the calibration of the RBC model using an experiment sensitive to
changes in the membrane bending rigidity kb (e.g. micropipette aspiration [38,
32, 31]),

116
Chapter 10.
Conclusions and Outlook
2) calibrate the interactions between the blood cells using measurements of the
whole blood viscosity as a function of the shear rate [125, 88, 43],
3) simulate RBCs behavior in realistic blood vessel geometries,
4) include other elements in the blood model (such as white blood cells [42],
platelets [130] and circulating tumor cells).
Another possible extension of the current blood cell model is introduction of a
cytoskeleton [94], which better represents the actual geometry of an RBC.

Appendix
117


Appendix A
ABC for Likelihood-Driven Bayesian
Inference
This chapter presents a comparison of ABC-SubSim and TMCMC used on the same
problem from the likelihood-driven Bayesian inference.
A.1
Computational Model
The application is calibration of a Force-Displacement (FD) model for steel particles.
We use a 2-d Discrete Element Method (DEM) simulation [17] of the collision of a
particle with a wall, both made of steel. The particle and the wall are represented
by 2-d disks.
The steel properties used in the simulation are the following: Young‚Äôs modulus
E = 2.10 √ó 1010 Pa, Poisson ratio ŒΩ = 0.3, disk radius Ri = 0.02225 m, mass
m = 0.3538 kg. The wall disk radius is taken big enough for the curvature of the
wall to be neglected.
At the heart of DEM lies the FD model of the steel particles.
This model
updates the rotational and translational components of a particle movement taking
into account pairwise collisions with other particles and obstacles. According to the
results of Ref. [61], the best model for the normal component of the collision force
is the extended non-linear model [79]:
F n = ‚àíknŒæ3/2
ij
‚àíŒ≥n ÀôŒæijŒæŒ±n
ij ,
(A.1)
119

120
Appendix A. ABC for Likelihood-Driven Bayesian Inference
where Œæij = Ri + Rj ‚àí||ri ‚àírj|| is the mutual compression between particles i
and j, the damping coeÔ¨Écient Œ≥n and the power Œ±n are model parameters, and the
material property spring constant kn is given by
kn = 2
3
E
(1 ‚àíŒΩ)2
s
RiRj
Ri + Rj .
(A.2)
The tangential component of the force is computed according to a modiÔ¨Åcation of
the original Cundall and Strack model [18]:
F t = ‚àí|ktŒæt
ij|sign(Œæt
ij) .
(A.3)
Œæt
ij is the elongation of an imaginary tangential spring from time œÑ0 (beginning of
the collision) to time œÑ1 (force evaluation time):
Œæt
ij =
œÑ1
Z
œÑ0
vt
ij(œÑ) Œò
Ô£´
Ô£≠¬µ|F n|
kt
‚àí
œÑ1
Z
œÑ0
vt
ij(œÑ) dœÑ
Ô£∂
Ô£∏dœÑ ,
(A.4)
where Œò(¬∑) is the Heaviside step function, vt
ij is the relative tangential velocity
between the two particles at their contact point, the imaginary tangential spring
stiÔ¨Äness kt and the friction coeÔ¨Écient ¬µ are model parameters. Further details on
the simulation setup can be found in [61].
ri
rj
Rj
Ri
‚á†ij
i
j
Figure A.1: Schematic representation of a collision between particles i and j.

A.2. Parameter Calibration
121
A.2
Parameter Calibration
Summarizing the discussion above, the model under consideration has 4 parameters
to be calibrated: the friction coeÔ¨Écient ¬µ, the power Œ±n, the damping coeÔ¨Écient
Œ≥n, and the stiÔ¨Äness of the imaginary tangential spring kt.
The calibration data comes from Ref. [33] and consists of the normal and the
tangential coeÔ¨Écients of restitution cn
j , ct
j, j = 1, . . . , 6 along with their uncertainties
u(cn
j ), u(ct
j) at diÔ¨Äerent impact angles, see Table A.1.
Table A.1: Experimentally measured normal and tangential coeÔ¨Écients of restitu-
tion cn
j , ct
j and their uncertainties u(cn
j ), u(ct
j), j = 1, . . . , 6 depending on the impact
angle œÜ. Data from [33].
œÜ (deg)
ct
j
u(ct
j)
cn
j
u(cn
j )
10
0.523
0.010
0.890
0.020
20
0.524
0.014
0.892
0.015
30
0.637
0.044
0.918
0.014
40
0.779
0.013
0.896
0.005
50
0.846
0.024
0.918
0.013
60
0.886
0.024
0.874
0.033
We specialize the components of the prediction error equation Eq. (2.1) as fol-
lows:
Œ£c = 0 ,
(A.5)
Œ£e =
 
diag
 u(cn
j )

0
0
diag
 u(ct
j)

!2
,
(A.6)
Œ£m = œÉ2
 
diag
 cn
j

0
0
diag
 ct
j

!2
.
(A.7)
In the approximate Bayesian inference, we select a Euclidean discrepancy: œÅ(y, d) =
||y ‚àíd||2, which resembles the log-likelihood of the likelihood-driven setup. The
model error œÉ2 is unknown, and we extend the parameter vector to include it:
œë = (¬µ, Œ±n, Œ≥n, kt, œÉ2).
We set the prior PDF to be uniform with large enough
bounds so that they do not aÔ¨Äect the inference.

122
Appendix A. ABC for Likelihood-Driven Bayesian Inference
We compare the results of the Bayesian inference using TMCMC and ABC-
SubSim with 50‚Äô000 samples per stage each in Table A.2 and Fig. A.2, Fig. A.3.
ABC-SubSim halts with a tolerance equal to 0.06.
Table
A.2:
Prior
and
posterior
information
about
the
parameters
œë
‚àà
{¬µ, Œ±n, Œ≥n, kt, œÉ2} for ABC-SubSim (A) and TMCMC (T): uniform prior bounds
p(œë), posterior MPVs b(œë), 5%-95% posterior quantiles q(œë), scale s(œë).
p(¬µ)
b(¬µ)
q(¬µ)
s(¬µ)
p(Œ±n)
b(Œ±n) q(Œ±n)
s(Œ±n)
A [5, 20]
10.6
[10.2, 11.1]
√ó10‚àí2 [1.5, 5.5] 3.99
[2.40, 5.00]
√ó10‚àí1
T
[5, 20]
10.5
[9.78, 11.2]
√ó10‚àí2 [1.5, 5.5] 4.24
[2.53, 5.12]
√ó10‚àí1
p(Œ≥n)
b(Œ≥n) q(Œ≥n)
s(Œ≥n)
p(kt)
b(kt)
q(kt)
s(kt)
A [0.1, 10] 4.26
[0.757, 8.54] √ó104
[0.1, 1.5] 0.987 [0.335, 1.19] √ó100
T
[0.1, 10] 5.09
[0.843, 9.35] √ó104
[0.1, 1.5] 0.983 [0.334, 1.19] √ó100
p(œÉ2)
b(œÉ2) q(œÉ2)
s(œÉ2)
A [0, 10]
4.73
[0.431, 9.39] √ó10‚àí5
T
[0, 10]
5.06
[0.725, 9.30] √ó10‚àí5
The DEM calibration problem turns out to be extremely challenging for the
sampling algorithms, as it contains both an unidentiÔ¨Åable manifold (in (Œ±n, Œ≥n)
subspace) and a bi-modality (in kt parameter). Nevertheless, TMCMC and ABC-
SubSim algorithms capture both features and give almost identical results. In ac-
cordance with the results of Wilkinson [127], an additional uniformly distributed
prediction error arises in the domain œÅ(y, d) ‚â§Œ¥ = 0.06 for ABC-SubSim. How-
ever, this error appears to be small enough for the quantitative discrepancies in the
inference results of the two algorithms to be negligible.
We thus conclude that ABC-SubSim provides a reasonable alternative to TM-
CMC for Bayesian likelihood-driven inference. Its usage however appears to be more
suitable in the cases when the likelihood is intractable.

A.2. Parameter Calibration
123
Figure A.2: Results of parameter calibration of the DEM problem using 50000 ABC-
SubSim samples (see Notations), Œ¥ = 0.06. The scaling factors for the parameters
are given in Table A.2.

124
Appendix A. ABC for Likelihood-Driven Bayesian Inference
Figure A.3: Results of parameter calibration of the DEM problem using 50000
TMCMC samples (see Notations). The scaling factors for the parameters are given
in Table A.2.

Appendix B
RBC Parameters Sensitivity
This appendix presents the results of the sensitivity analysis of the RBC model in
stretching and shear Ô¨Çow. Every Ô¨Ågure displays a set of values of one parameter of
the RBC membrane model and the corresponding values of the QoI along with 95%
conÔ¨Ådence intervals. The reference point conÔ¨Ådence intervals are shown as dashed
lines. The 1% deviations from the reference values are shown as solid lines. We
consider a given parameter sensitive if there is a visible trend in the QoI.
14
15
16
17
axial
0.43
0.44
0.45
0.46
0.47
x0
4.4
4.6
4.8
5
transverse
15.2
15.4
15.6
axial
1.9
1.95
2
2.05
2.1
p
√ó10-3
4.55
4.6
4.65
4.7
transverse
Figure B.1: EÔ¨Äect of the spring parameters in the stretching experiment.
125

126
Appendix B. RBC Parameters Sensitivity
15.2
15.4
15.6
axial
38
39
40
41
42
kb
4.55
4.6
4.65
4.7
transverse
15.2
15.4
15.6
axial
28.5
29
29.5
30
30.5
31
31.5
Œ≥C
4.55
4.6
4.65
4.7
transverse
Figure B.2: EÔ¨Äect of the bending rigidity parameter kb and of the membrane vis-
cosity parameter Œ≥C in the stretching experiment.
0.43
0.44
0.45
0.46
0.47
x0
0.18
0.185
0.19
0.195
0.2
0.205
frequency
0.48
0.49
0.5
0.51
0.52
p
0.186
0.188
0.19
0.192
0.194
0.196
0.198
0.2
0.202
frequency
x 10-2
Figure B.3: EÔ¨Äect of the spring parameters in the shear Ô¨Çow experiment.
38
39
40
41
42
kb
0.188
0.19
0.192
0.194
0.196
0.198
0.2
0.202
frequency
28.5
29
29.5
30
30.5
31
31.5
Œ≥C
0.19
0.195
0.2
frequency
Figure B.4: EÔ¨Äect of the bending rigidity parameter kb and of the membrane vis-
cosity parameter Œ≥C in the shear Ô¨Çow experiment.

Appendix C
Argon LJ Parameters
C.1
Hyper-parameter models for hierarchical inference
Inference for LJ 6-12
We assume that,
p(œë | œà, M) =
3
Y
j=1
p(œëj | œà, M) ,
(C.1)
and we consider the following two models:
1) uniform: p(œëj | œà, M) = U(œëj | œà2j‚àí1, œà2j‚àí1 + œà2j), where U(Œæ|a, b) is the uni-
form distribution of Œæ with parameters a, b and M is set to U,
2) log-normal: p(œëj | œà, M) = L(œëj | œà2j‚àí1, œà2j), where L(Œæ|a, b) is the log-normal
distribution of Œæ with parameters a, b and M is set to L.
The prior distribution on the hyper-parameters is modeled as independent uniform,
p(œà | M) =
6
Y
j=1
U(œàj | aM
j , bM
j ) ,
(C.2)
where M ‚àà{U, L} and the constants aM
j , bM
j
are given in Table C.1, along with
the values of the log-evidences for the two models. The model U is according to the
Bayesian model selection criterion, an order of magnitude more plausible and thus
will be used for the further inference.
127

128
Appendix C. Argon LJ Parameters
Table C.1:
HB12,R inference:
lower bound and width for each of the hyper-
parameters deÔ¨Åned in Eq. (C.2). Log-evidences for each hyper-prior model at the
last row.
M = U
M = L
[aM
1 , bM
1 ] [0.0, 3.0] [-1.000, 2.0]
[aM
2 , bM
2 ] [0.0, 7.0] [ 0.001, 2.5]
[aM
3 , bM
3 ] [3.0, 3.4] [-3.000, 0.5]
[aM
4 , bM
4 ] [0.0, 2.0] [ 0.001, 4.0]
[aM
5 , bM
5 ] [0.0, 0.2] [-3.500, 0.5]
[aM
6 , bM
6 ] [0.0, 1.0] [ 0.001, 2.5]
Log-ev.
-19.1401 -22.0198
Inference for LJ 6-p
We assume
p(œë | œà, M) =
4
Y
j=1
p(œëj | œà, M)
(C.3)
and consider the following three models:
1) uniform: p(œëj | œà, U) = U(œëj | œà2j‚àí1, œà2j‚àí1 + œà2j), where U(Œæ|a, b) is the uni-
form distribution of Œæ with parameters a, b and M is set to U,
2) log-normal: p(œëj | œà, M) = L(œëj | œà2j‚àí1, œà2j), where L(Œæ|a, b) is the log-normal
distribution of Œæ with parameters a, b and M is set to L,
3) truncated normal: p(œëj | œà, M) = T (œëj | œà2j‚àí1, œà2j), where T (Œæ|a, b) is the
truncated normal distribution of Œæ with parameters a, b and M is set to T .
The prior distribution on the hyper-parameters is modeled as independent uniform,
p(œà | M) =
8
Y
j=1
p(œàj | M) =
8
Y
j=1
U(œàj | aM
j , bM
j ) ,
(C.4)
where M ‚àà{U, L, T } and the constants aM
j , bM
j
are given in Table C.2, along with
the values of the log-evidences for the three models. As it can be seen, the uniform
model is the most plausible one.

C.2. Posterior PDFs for LJ 6-12 and LJ 6-p
129
Table C.2:
HBp,R inference:
lower bound and width for each of the hyper-
parameters deÔ¨Åned in Eq. (C.4). Log-evidences for each hyper-prior model at the
last row.
M = U
M = L
M = T
[aM
1 , bM
1 ] [0.0, 3.00] [-1.000, 2.0] [0.0500, 10.0]
[aM
2 , bM
2 ] [0.0, 7.00] [ 0.001, 2.5] [0.0010, 3.30]
[aM
3 , bM
3 ] [3.0, 3.40] [-3.000, 0.5] [3.0000, 4.00]
[aM
4 , bM
4 ] [0.0, 2.00] [ 0.001, 4.0] [0.0010, 0.30]
[aM
5 , bM
5 ] [6.0, 7.00] [-2.500, 1.5] [6.0000, 12.0]
[aM
6 , bM
6 ] [0.0, 10.0] [ 0.001, 2.5] [0.0010, 2.00]
[aM
7 , bM
7 ] [0.0, 0.20] [-3.500, 0.5] [0.0001, 1.00]
[aM
8 , bM
8 ] [0.0, 1.00] [ 0.001, 2.5] [0.0010, 0.30]
Log-ev.
-24.0202
-27.3889
-25.3927
C.2
Posterior PDFs for LJ 6-12 and LJ 6-p
This section presents the posterior distributions for LJ 6-12 and LJ 6-p potentials
obtained in non-hierarchical BASIS runs for each thermodynamic condition, as well
as, the distribution for the quantum dimer-based Bayesian inference. Each plot con-
tains all the BASIS samples of the last stage and it consists of the following parts:
histograms of marginal distributions of parameters are shown on the diagonal, pro-
jections of the samples to all pair 2-d subspaces in the parameter space colored by
the log-likelihood values are given above the diagonal, the corresponding densities
constructed via a bivariate kernel estimate are depicted below the diagonal. Green
circle shows the parameters from Ref. [100, 105], green square indicates the param-
eters from Ref. [11], and green triangle marks the parameters from Ref. [126].

130
Appendix C. Argon LJ Parameters
Figure C.1: LJ parameters distributions obtained in B12,R for L3 (top) and L4
(bottom).

C.2. Posterior PDFs for LJ 6-12 and LJ 6-p
131
Figure C.2: LJ parameters distributions obtained in B12,R for L5 (top) and V
(bottom).

132
Appendix C. Argon LJ Parameters
Figure C.3: LJ parameters distributions obtained in Bp,R for L1 (top) and L2
(bottom).

C.2. Posterior PDFs for LJ 6-12 and LJ 6-p
133
Figure C.4: LJ parameters distributions obtained in Bp,R for L3 (top) and L4
(bottom).

134
Appendix C. Argon LJ Parameters
Figure C.5: LJ parameters distributions obtained in Bp,R for L5 (top) and V (bot-
tom).

C.2. Posterior PDFs for LJ 6-12 and LJ 6-p
135
Figure C.6: LJ parameters distribution obtained in Bp,Q.

136
Appendix C. Argon LJ Parameters
Figure C.7: LJ parameters distribution obtained in Bp,R for L3 with the prior for p
restricted to [12, 14].

Appendix D
LAMMPS Scripts for the MD
Simulations
D.1
Helium
This section presents a script used for running the ABC inference for helium.
1
# Variables
2
variable
m
equal 6.646476e-6
# mass
3
variable
R
equal 2077.1488
# gas constant
4
variable
P
equal 101325e-6
# pressure
5
variable
T
equal 300
# temperature
6
variable
rho
equal 2*($P/($T*$R))
# initial density
7
variable
cut
equal 2.5*${sigma}
# LJ cutoff
8
variable
nc
equal 10
# cells per dimension
9
variable
dL
equal ($m/${rho})^(1./3.) # length of one cell
10
variable
L
equal ${dL}*${nc}
# domain length
11
variable
seed equal 1
# rnd seed for velocity
12
variable
dt
equal 2e-6
# time step
13
variable
nt1
equal 1e6
# steps (equilibration)
14
variable
nt2
equal 2.5e6
# steps (production)
15
variable
td
equal 1e3*${dt}
# temperature damping
137

138
Appendix D. LAMMPS Scripts for the MD Simulations
16
variable
pd
equal 1e4*${dt}
# pressure damping
17
variable
kb
equal 0.0138
# Boltzmann constant
18
variable
fb
equal exp(-enthalpy/(${kb}*temp))
19
# Boltzmann factor
20
21
# Initialization:
22
# 1) general
23
units
nano
24
atom_style
atomic
# interatomic forces
25
# 2) geometry
26
boundary
p p p
# periodic in xyz
27
lattice
sc ${dL}
28
region
box block 0 $L 0 $L 0 $L units box
29
create_box
1 box
30
# 3) create atoms
31
create_atoms
1 box
32
mass
* $m
33
velocity
all create $T ${seed} mom yes rot yes dist gaussian
34
# 4) LJ interactions
35
pair_style
lj/cut ${cut}
36
pair_coeff
* * ${eps} ${sigma}
37
# 5) relaxation process
38
neighbor
0.1 bin
39
neigh_modify
delay 0 every 1 check yes
40
minimize
0.0 1.0e-8 1000 100000
41
42
# Equilibration run
43
reset_timestep 0
44
timestep
${dt}
45
fix
1 all npt temp $T $T ${td} iso $P $P ${pd}
46
thermo_style
custom temp press vol pe etotal density
47
thermo
1000
48
run
${nt1}
49

D.2. Argon
139
50
# Production run
51
reset_timestep 0
52
timestep
${dt}
53
fix
2 all npt temp $T $T ${td} iso $P $P ${pd}
54
thermo_style
custom temp press vol pe etotal v_fb
55
thermo_modify
norm yes
56
thermo
1000
57
run
${nt2}
D.2
Argon
This section presents a script used for running the HB inference for argon.
1
# Variables (‚Äòsigma‚Äò, ‚Äòeps‚Äò, ‚ÄòD‚Äò [density], ‚ÄòP‚Äò [pressure],
2
# ‚ÄòT‚Äò [temperature] are input parameters)
3
variable
m
equal 39.95
# mass
4
variable
rho equal 25
# lattice density
5
variable
L
equal "40.0 * v_sigma" # domain length
6
variable
nt1 equal 100000
# steps (NPT, 1)
7
variable
nt2 equal 100000
# steps (NPT, 2) [loop 50]
8
variable
nt3 equal 100000
# steps (NVE)
9
variable
nlj equal 1000
# LJ interpolation points
10
11
# Initialization
12
# 1) general
13
units
real
14
atom_style
atomic
# interatomic forces
15
# 2) geometry
16
boundary
p p p
# periodic in xyz
17
lattice
fcc ${rho}
18
region box
block
0 $L
0 $L
0 $L
19
create_box
1 box # number of atom types
20
# 3) create atoms
21
create_atoms 1 box
22
mass
* $m

140
Appendix D. LAMMPS Scripts for the MD Simulations
23
velocity
all create $T ${seed} rot yes dist gaussian
24
# 4) LJ interactions
25
pair_style
table linear ${nlj}
26
pair_coeff
* *
lj.tab.txt LJ
# tabulated LJ in file
27
# 5) relaxation process
28
neighbor
${sigma}
bin
29
atom_modify
map array sort 10 6
30
minimize
0.0 1.0e-8 1000 100000
31
32
# NPT integration
33
neigh_modify
delay 1
34
reset_timestep 0
35
timestep
1
36
thermo
1000
37
thermo_style
custom step temp press etotal vol density
38
fix
1 all npt temp ${T} ${T} 20 iso ${P} ${P} 20
39
run
${nt1}
40
41
# NPT integration-2 (bigger time step)
42
neigh_modify
delay 5
43
reset_timestep 0
44
timestep
2
45
thermo
1000
46
thermo_style
custom step temp press etotal vol density
47
fix
2 all npt temp ${T} ${T} 100 iso ${P} ${P} 1000
48
fix
LMOM all momentum 100 linear 1 1 1
49
50
# Conditional run
51
variable
d equal density
52
label
loop
53
variable
a loop 50
54
if
"$d > $D" then "jump SELF break"
55
run
${nt2}
56
next
a

D.2. Argon
141
57
jump
SELF loop
58
label
break
59
run
10000
# finalize
60
61
# Production run (NVE)
62
reset_timestep 0
63
timestep
2
64
fix
3 all nve
65
compute
RDF all rdf 100
66
fix
RDF all ave/time 1 ${nt3} ${nt3} c_RDF file rdf.txt
67
mode vector
68
compute
MSD all msd com yes
69
fix
MSD all vector 10 c_MSD[4]
70
variable
dc equal slope(f_MSD)/6/(10*dt) # diffusion coefficient
71
thermo
1000
72
thermo_style
custom step temp press etotal vol density v_dc c_MSD[4]
73
run
${nt3}
In this script, we conditionally run the NPT integration (lines 41-59) until argon
reaches the desired phase (liquid or vapor). For some parameter sets this may never
happen, and the run should be rejected.
The tabulated version of the LJ potential is precomputed with an external script,
after that LAMMPS reads it (line 26) and constructs a linear interpolation (line 25).
The diÔ¨Äusion coeÔ¨Écient is computed using the mean square displacement (lines
67-69).


List of Figures
1.1
Schematic representation of the inference process. Red arrows show
the origins of errors. Blue dashed arrows show the information and
errors Ô¨Çow.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
4.1
A synthetic example of a hierarchical data structure. All the datasets
exhibit a linear behavior y = œëx with diÔ¨Äerent slopes œë. The hyper-
parameter œà governs the prior distribution of the slopes: p(œë | œà) =
U(œë; œà1, œà2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.2
Hierarchical Bayesian network.
. . . . . . . . . . . . . . . . . . . . .
27
5.1
8000 BASIS samples from the Rosenbrock function. . . . . . . . . . .
39
5.2
Schematic representation of intermediate domains at stages k (dashed
blue line) and (k + 1) (dashed red line), samples up to stage k (blue
dots and red dots with blue border), chains generated from the leader
samples at the stage k (red dots and red dots with blue border) for
various algorithms. Black dots indicate samples which were proposed,
but rejected by the algorithm. . . . . . . . . . . . . . . . . . . . . . .
42
5.3
Posterior PDFs of the 2-d Himmelblau function. The population size
is 8000. Each of the surrogates uses 8% of the domain to build the
approximation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
6.1
Data: distribution of the Boltzmann factor fB over time of a helium
system with 1000 atoms, œÉ = 0.2556, Œµ = 0.141. . . . . . . . . . . . .
53
143

144
List of Figures
6.2
Calibration results for the LJ parameters of helium using ABC-SubSim
with diÔ¨Äerent discrepancies (see Notations).
Green circles on the
bottom-left subplots indicate parameters for which the data were cre-
ated. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
7.1
Posterior parameter values: MPVs along with 5%-95% quantiles ob-
tained in B12,R (light red), HB12,R (dark red).
Horizontal lines
indicate the reference values:
Ref. [11] (yellow), Ref. [126] (red),
Ref. [100, 105] (blue).
. . . . . . . . . . . . . . . . . . . . . . . . . .
63
7.2
Posterior parameter values: MPVs along with 5%-95% quantiles ob-
tained in Bp,R (light blue), HBp,R (dark blue). Horizontal line indi-
cates the MPVs for Bp,Q.
. . . . . . . . . . . . . . . . . . . . . . . .
67
7.3
Posterior samples of Bp,R projected onto (p, Œµ) subspace. Red: V .
Shades of blue: Li in the temperature increasing order from the light-
est to the darkest color. Green: Q. Grid lines indicate the log-scale
for Œµ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
7.4
Posterior LJ potentials comparison. Blue: MPVs with 5%-95% quan-
tiles from HBp,R. Red: MPVs from HB12,R. Green: MPVs from
Bp,Q (when applicable). Black: quantum dimer calculations (when
applicable). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
7.5
Robust posterior predictions of RDF. Blue:
MPVs with 5%-95%
quantiles from HBp,R. Red: MPVs from HB12,R (when applicable).
Green: MPVs from Bp,Q (when applicable). Black: experimental data. 73
7.6
Robust posterior predictions of density œÅ and diÔ¨Äusion coeÔ¨Écient D.
Blue: MPVs with 5%-95% quantiles from HBp,R. Red: MPVs with
5%-95% quantiles from HB12,R (when applicable).
Green: MPVs
from Bp,Q (when applicable). Grey: experimental data for œÅ, ana-
lytically computed values for D. Grid for D shows the logarithmic
scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
8.1
Schematic representation of the coarse-grained model of the Sec translo-
con.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
8.2
Bayesian inference using PMF proÔ¨Åles. . . . . . . . . . . . . . . . . .
80

List of Figures
145
8.3
Predictions of probability of integration using M5 and diÔ¨Äerent MAR-
TINI PMF datasets. The simulation results are given in the order
of increasing the scaling coeÔ¨Écient from the lightest to the darkest
color. All the datasets are Ô¨Åtted with a hyperbolic tangent. . . . . .
84
8.4
Parameters of M5 for the original MARTINI PMF datasets (Ô¨Ålled
circles) and D-scaled datasets (empty circles). . . . . . . . . . . . . .
85
9.1
RBC membrane.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
9.2
RBC: Interactions of a single vertex. . . . . . . . . . . . . . . . . . .
90
9.3
Experimental values of the axial and transverse diameters of an RBC
stretched with optical tweezers [116]. . . . . . . . . . . . . . . . . . .
93
9.4
Stretching of RBCs at diÔ¨Äerent resolutions. Yellow: Nv ‚àº500, red:
Nv ‚àº2000, blue: Nv ‚àº8000.
. . . . . . . . . . . . . . . . . . . . . .
93
9.5
Example of Ô¨Åtting the RBC diameters under stretching with the func-
tion from Eq. (9.22). . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
9.6
Approximation of the Ô¨Åtting function (Eq. (9.22)) parameters. . . . .
96
9.7
Results of parameter calibration of the RBC membrane parameters
using TMCMC (see Notations). . . . . . . . . . . . . . . . . . . . . .
97
9.8
Posterior predictions of RBC diameters under stretching. Red line:
experimental data, blue line: posterior MPV, blue area: 5%-95%
quantiles.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
9.9
Posterior predictions of the frequency of RBC in shear Ô¨Çow. Calibra-
tion using the data on stretching. Original model. Red line/crosses:
experimental data, blue line: mean and 5%-95% quantiles of the pre-
dictions. Data from [35] and [118]. . . . . . . . . . . . . . . . . . . .
99
9.10 RBC tank-treading frequency in the shear Ô¨Çow along with the linear
Ô¨Åt. The shear rate is normalized with the viscosity of the solvent.
The experiments are listed in the order of viscosity increase (from
blue to red). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
9.11 Collapse of the triangles at shear rate 12 in DPD units. Left: kd =
200, right: kd = 5000.
. . . . . . . . . . . . . . . . . . . . . . . . . .
101
9.12 Resolution study of RBC in shear Ô¨Çow.
Yellow: Nv ‚àº500, red:
Nv ‚àº2000.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
9.13 Approximation of the TTF as a function of Œ≥C. . . . . . . . . . . . .
102

146
List of Figures
9.14 Posterior distribution of the RBC membrane viscosity parameter Œ≥C
obtained with TMCMC (see Notations). . . . . . . . . . . . . . . . .
104
9.15 Robust posterior quantiles of TTF obtained after calibrating the
membrane viscosity parameter.
Red line: experimental data, blue
line: posterior MPV, blue area: 5%-95% quantiles of the predictions.
104
9.16 Schematic representation of RBC motion in various shear rates. . . .
105
A.1 Schematic representation of a collision between particles i and j.
. .
120
A.2 Results of parameter calibration of the DEM problem using 50000
ABC-SubSim samples (see Notations), Œ¥ = 0.06. The scaling factors
for the parameters are given in Table A.2. . . . . . . . . . . . . . . .
123
A.3 Results of parameter calibration of the DEM problem using 50000
TMCMC samples (see Notations). The scaling factors for the param-
eters are given in Table A.2. . . . . . . . . . . . . . . . . . . . . . . .
124
B.1
EÔ¨Äect of the spring parameters in the stretching experiment.
. . . .
125
B.2
EÔ¨Äect of the bending rigidity parameter kb and of the membrane
viscosity parameter Œ≥C in the stretching experiment. . . . . . . . . .
126
B.3
EÔ¨Äect of the spring parameters in the shear Ô¨Çow experiment.
. . . .
126
B.4
EÔ¨Äect of the bending rigidity parameter kb and of the membrane
viscosity parameter Œ≥C in the shear Ô¨Çow experiment. . . . . . . . . .
126
C.1
LJ parameters distributions obtained in B12,R for L3 (top) and L4
(bottom). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
C.2
LJ parameters distributions obtained in B12,R for L5 (top) and V
(bottom). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
C.3
LJ parameters distributions obtained in Bp,R for L1 (top) and L2
(bottom). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
C.4
LJ parameters distributions obtained in Bp,R for L3 (top) and L4
(bottom). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
C.5
LJ parameters distributions obtained in Bp,R for L5 (top) and V
(bottom). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
C.6
LJ parameters distribution obtained in Bp,Q.
. . . . . . . . . . . . .
135
C.7
LJ parameters distribution obtained in Bp,R for L3 with the prior for
p restricted to [12, 14]. . . . . . . . . . . . . . . . . . . . . . . . . . .
136

List of Tables
5.1
Results of the BASIS inference of the Himmelblau function using
the surrogates: percentage D of the domain used to build surrogate,
posterior mean value m(œë) of the parameter œë, 5%-95% quantiles q(œë)
of the posterior PDF of the parameter œë, percentage S of surrogate
points, average surrogate evaluation time T in seconds, average error
E computed as the absolute diÔ¨Äerence between the surrogate and the
real model evaluation. The population size was chosen to be 2000.
.
45
5.2
Detailed per-stage performance results of ABC-SubSim with 1024
workers on the Piz Daint cluster: Tm ‚Äì mean simulation time, Ts ‚Äì
standard deviation of the simulation times, Tt ‚Äì sum of the execution
times of all simulations, Tw ‚Äì wall-clock time per stage. All the times
are measured in seconds. . . . . . . . . . . . . . . . . . . . . . . . . .
47
6.1
Values of the LJ parameters œë ‚àà{œÉ, Œµ} of helium: prior bounds p(œë),
posterior MPVs b(œë), posterior 5%-95% quantiles q(œë), number of
stages S, achieved tolerances Œ¥ for three discrepancy models: MG
‚Äì Gaussian (Eq. (6.3)), MQ ‚Äì quantile-based (Eq. (6.4)), MKL ‚Äì
Kullback-Leibler (Eq. (6.5)). . . . . . . . . . . . . . . . . . . . . . . .
56
7.1
Units used in this chapter. . . . . . . . . . . . . . . . . . . . . . . . .
61
7.2
LJ parameters for argon used in literature. The last row shows the
data used for Ô¨Åtting. Notations: T (temperature), P (pressure), œÅ
(density), B (second virial coeÔ¨Écient), E (energy), TP (gas-liquid
transition pressure), L (latent heat of evaporation). . . . . . . . . . .
64
147

148
List of Tables
7.3
Posterior values of each parameter œë ‚àà{Œµ, œÉ, œÉn} of LJ 6-12: MPV
b(œë) and 5%-95% quantiles q(œë).
. . . . . . . . . . . . . . . . . . . .
65
7.4
Posterior values of each parameter œë ‚àà{Œµ, œÉ, p, œÉn} of LJ 6-p: MPV
b(œë) and 5%-95% quantiles q(œë).
. . . . . . . . . . . . . . . . . . . .
68
7.5
Log-evidences E12,R (Ep,R) for B12,R (Bp,R).
. . . . . . . . . . . . .
69
7.6
Errors of robust posterior predictions of RDF, density and diÔ¨Äusion
coeÔ¨Écient using LJ 6-12 and LJ 6-p. We denote S1 = {L4, L5} (all
inferences produce the correct argon phase), S2 = {L3 ‚àíL5, V } (Bp,Q
produces wrong phase), S3 = {L1‚àíL5, V } (Bp,Q and HB12,R produce
wrong phase). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
8.1
Parameters of M1 ‚àíM4. Parameters under inference are given with
their uniform prior bounds. For the parameters not used in the infer-
ence, the values are given. The values on the left side of the slash (‚Äô/‚Äô)
are for the D substrate, while the values on the right side correspond
to the L substrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
8.2
Parameters of M5. Parameters under inference are given with their
uniform prior bounds. For the parameters not used in the inference,
the values are given. . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
8.3
Log-likelihood values of all the models. . . . . . . . . . . . . . . . . .
82
8.4
Posterior information obtained in the HB inference for each of the
parameters œë ‚àà{mx, mr, œÉ} of M5: MPV b(œë), 5%-95% quantiles q(œë). 82
8.5
Information about the parameters of M5 obtained using the data
with D proÔ¨Åles scaled by 50%.
. . . . . . . . . . . . . . . . . . . . .
85
9.1
CoeÔ¨Écients of the polynomial approximation of parameters a, t from
Eq. (9.22) as a function of x0. CoeÔ¨Écient ci corresponds to degree i.
95
9.2
Prior and posterior information on the spring parameters of the RBC
membrane model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
9.3
Linear Ô¨Åts and their errors for RBC tank-treading frequency in the
shear Ô¨Çow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
9.4
Prior and posterior information about the parameters of the RBC
membrane in the shear Ô¨Çow experiment. . . . . . . . . . . . . . . . .
103

List of Tables
149
A.1 Experimentally measured normal and tangential coeÔ¨Écients of resti-
tution cn
j , ct
j and their uncertainties u(cn
j ), u(ct
j), j = 1, . . . , 6 de-
pending on the impact angle œÜ. Data from [33]. . . . . . . . . . . . .
121
A.2 Prior and posterior information about the parameters œë ‚àà{¬µ, Œ±n, Œ≥n, kt, œÉ2}
for ABC-SubSim (A) and TMCMC (T): uniform prior bounds p(œë),
posterior MPVs b(œë), 5%-95% posterior quantiles q(œë), scale s(œë).
.
122
C.1
HB12,R inference: lower bound and width for each of the hyper-
parameters deÔ¨Åned in Eq. (C.2). Log-evidences for each hyper-prior
model at the last row. . . . . . . . . . . . . . . . . . . . . . . . . . .
128
C.2
HBp,R inference:
lower bound and width for each of the hyper-
parameters deÔ¨Åned in Eq. (C.4). Log-evidences for each hyper-prior
model at the last row. . . . . . . . . . . . . . . . . . . . . . . . . . .
129


Bibliography
[1] http://lammps.sandia.gov.
[2] https://github.com/pin3da/gplib.
[3] https://github.com/OpenANN/OpenANN.
[4] M. Abkarian, M. Faivre, and A. Viallat. Swinging of red blood cells under
shear Ô¨Çow. Phys. Rev. Lett., 98:188302, 2007.
[5] P. Angelikopoulos, C. Papadimitriou, and P. Koumoutsakos.
Bayesian un-
certainty quantication and propagation in molecular dynamics simulations:
a high performance computing framework. J. Chem. Phys., 137(14):144103,
2012.
[6] P. Angelikopoulos, C. Papadimitriou, and P. Koumoutsakos. Bayesian uncer-
tainty quantiÔ¨Åcation and propagation in molecular dynamics simulations: A
high performance computing framework. J. Chem. Phys., 137:144103, 2012.
[7] P. Angelikopoulos, C. Papadimitriou, and P. Koumoutsakos. Data driven, pre-
dictive molecular dynamics for nanoscale Ô¨Çow simulations under uncertainty.
J. Phys. Chem. B, 117(47):14808‚Äì14816, 2014/12/01 2013.
[8] P. Angelikopoulos, C. Papadimitriou, and P. Koumoutsakos. X-tmcmc: Adap-
tive kriging for bayesian inverse modeling. Comp. Meth. Appl. Mech. Eng.,
289:409‚Äì428, 2015.
[9] P. Angelikopoulos, C. Papadimitriou, and P. Koumoutsakos.
X-TMCMC:
Adaptive kriging for Bayesian inverse modeling. Comp. Meth. Appl. Mech.
Eng., 289:409‚Äì428, 6 2015.
151

152
Bibliography
[10] S.-K. Au and J. L. Beck.
Estimation of small failure probabilities in high
dimensions by subset simulation. Prob. Eng. Mech., 16(4):263‚Äì277, 2001.
[11] J. A. Barker, R. A. Fisher, and R. O. Watts. Liquid argon: Monte Carlo and
molecular dynamics calculations. Mol. Phys., 21:657‚Äì673, 1971.
[12] M. A. Beaumont. Approximate Bayesian computation in evolution and ecol-
ogy. Annu. Rev. Ecol. Evol. Syst., 41(1):379‚Äì406, 2014/12/01 2010.
[13] M. A. Beaumont, J. M. Cornuet, J. M. Marin, and C. P. Robert. Adaptive
approximate Bayesian computation. Biometrika, 96(4):983‚Äì990, 2009.
[14] J. L. Beck and S. K. Au. Probabilistic system identiÔ¨Åcation with unidentiÔ¨Å-
able models. In Proceedings of the 8th International Conference on Structural
Safety and Reliability, Balkema, 2001.
[15] J. L. Beck and L. S. Katafygiotis. Updating models and their uncertainties. I:
Bayesian statistical framework. J. Eng. Mech.-ASCE, 124(4):455‚Äì461, April
1998.
[16] W. Betz, I. Papaioannou, and D. Straub. Transitional Markov chain Monte
Carlo: observations and improvements. Journal of Engineering Mechanics,
142(5):04016016, 2016.
[17] N. Bicanic. Discrete Element Methods, volume 1 of Encyclopedia of Compu-
tational Mechanics. Wiley, 2004.
[18] L. Brendel and S. Dippel. Lasting contacts in molecular dynamics simulations,
pages 31‚Äì33. Physics of Dry Granular Media. Kluwer Academic Publishers,
1998.
[19] F. Cailliez, A. Bourasseau, and P. Pernot. Calibration of forceÔ¨Åelds for molec-
ular simulation: Sequential design of computer experiments for building cost-
eÔ¨Écient kriging metamodels. J. Comp. Chem., 35(2):130‚Äì149, 2014.
[20] F. Cailliez and P. Pernot.
Statistical approaches to forceÔ¨Åeld calibra-
tion and prediction uncertainty in molecular simulation.
J. Chem. Phys.,
134(5):054124, 2011.

Bibliography
153
[21] B. Calderhead and M. Girolami. Statistical analysis of nonlinear dynamical
systems using diÔ¨Äerential geometric sampling methods. Interface focus, page
rsfs20110051, 2011.
[22] O. Capp¬¥e, S. J. Godsill, and E. Moulines. An overview of existing methods
and recent advances in sequential Monte Carlo. In Proceedings of the IEEE
95.5, pages 899‚Äì924, 2007.
[23] O. Capp¬¥e, A. Guillin, J. M. Marin, and C. P. Robert. Population Monte Carlo.
J. Comp. Graph. Stat., 13(4):907‚Äì929, 2004.
[24] S. H. Cheung and J. L. Beck. Bayesian model updating using hybrid Monte
Carlo simulation with application to structural dynamic models with many
uncertain parameters. J. Eng. Mech., 135(4):243‚Äì255, 2009.
[25] M. Chiachio, J. L. Beck, J. Chiachio, and G. Rus. Approximate Bayesian com-
putation by subset simulation. SIAM J. Sci. Comput., 36(3):A1339‚ÄìA1358,
2014/10/03 2014.
[26] J. Ching and Y. Chen. Transitional Markov chain Monte Carlo method for
Bayesian model updating, model class selection, and model averaging. J. Eng.
Mech., 133(7):816‚Äì832, 2007.
[27] J.-M. Cornuet, P. Pudlo, J. Veyssier, A. Dehne-Garcia, M. Gautier, R. Leblois,
J.-M. Marin, and A. Estoup. DIYABC v2. 0: a software to make approxi-
mate Bayesian computation inferences about population history using single
nucleotide polymorphism, DNA sequence and microsatellite data. Bioinfor-
matics 30, 8:1187‚Äì1189, 2014.
[28] P. de Valpine, D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. T. Lang,
and R. Bodik. Programming with models: writing statistical algorithms for
general model structures with NIMBLE. Journal of Computational and Graph-
ical Statistics, 26(2):403‚Äì413, 2017.
[29] P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. J.
R. Stat. Soc. Series B Stat. Methodol., 68(3):411‚Äì436, 2006.
[30] X. Didelot, R. G. Everitt, A. M. Johansen, and D. J. Lawson. Likelihood-free
estimation of model evidence. Bayesian analysis, 6(1):49‚Äì76, 2011.

154
Bibliography
[31] D. E. Discher, D. H. Boal, and S. K. Boey. Simulations of the erythrocyte
cytoskeleton at large deformation. ii. Micropipette aspiration. Biophys. J.,
75(3):1584‚Äì1597, 1998.
[32] D. E. Discher, N. Mohandas, and E. A. Evans. Molecular maps of red cell
deformation: hidden elasticity and in situ connectivity. Science, New Series,
266(5187):1032‚Äì1035, 1994.
[33] H. Dong and M. H. Moys. Experimental study of oblique impacts with initial
spin. Powd. Technol., 161(1):22‚Äì31, 2006.
[34] A. J. Drummond and A. Rambaut. BEAST: Bayesian evolutionary analysis
by sampling trees. BMC evolutionary biology, 7(1):214, 2007.
[35] J. Dupire, M. Socol, and A. Viallat. Full dynamics of a red blood cell in shear
Ô¨Çow. PNAS, 109(51):20808‚Äì20813, 2012.
[36] A. Eisenstein and N. S. Gingrich. The diÔ¨Äraction of X-rays by Argon in the
liquid, vapor, and critical regions. Phys. Rev., 62:261‚Äì270, 1942.
[37] P. EspaÀúnol and P. Warren. Statistical Mechanics of Dissipative Particle Dy-
namics. EPL (Europhysics Letters), 30(4):191, May 1995.
[38] E. A. EVANS. Bending elastic modulus of red blood cell membrane derived
from buckling instability in micropipet aspiration tests. Biophys. J., 43:27‚Äì30,
July 1983.
[39] K. Farrell, J. T. Oden, and D. Faghihi. A bayesian framework for adaptive
selection, calibration, and validation of coarse-grained models of atomistic
systems. J. Comput. Phys., 295(0):189‚Äì208, 8 2015.
[40] D. A. Fedosov. Multiscale modeling of blood Ô¨Çow and soft matter. PhD thesis,
Brown University, 2010.
[41] D. A. Fedosov, B. Caswell, and G. E. Karniadakis. A multiscale red blood cell
model with accurate mechanics, rheology, and dynamics. Biophysical Journal,
98:2215‚Äì2225, May 2010.
[42] D. A. Fedosov and G. Gompper. White blood cell margination in microcircu-
lation. Soft Matter, 10(17):2961‚Äì2970, 2014.

Bibliography
155
[43] D. A. Fedosov, W. Pan, B. Caswell, G. Gompper, and G. E. Karniadakis.
Predicting human blood viscosity in silico. PNAS, 108(29):11772‚Äì11777, 2011.
[44] F. Feroz, M. P. Hobson, and M. Bridges. MultiNest: an eÔ¨Écient and robust
Bayesian inference tool for cosmology and particle physics. Monthly Notices
of the Royal Astronomical Society, 398(4):1601‚Äì1614, 2009.
[45] T. M. Fischer.
The red cellas a Ô¨Çuid droplet: Tank tread-like motion of
the human erythrocyte membrane in shear Ô¨Çow. Science, 202(4370):894‚Äì896,
November 1978.
[46] T. M. Fischer. On the energy dissipation in a tank-treading human red blood
cell. Biophys. J., 32:863‚Äì868, November 1980.
[47] T. M. Fischer. Tank-tread frequency of the red cell membrane: Dependence on
the viscosity of the suspending medium. Biophys. J., 93:2553‚Äì2561, October
2007.
[48] G. Galli¬¥ero, C. Boned, A. Baylaucq, and F. Montel. Molecular dynamics com-
parative study of lennard-jones a-6 and exponential Œ±-6 potentials: Applica-
tion to real simple Ô¨Çuids (viscosity and pressure). Phys. Rev. E, 73:061201‚Äì1,
2006.
[49] D. Gamerman and H. F. Lopes. Markov chain Monte Carlo: stochastic sim-
ulation for Bayesian inference. CRC Press, 2006.
[50] A. E. Gelfand and D. K. Dey. Bayesian model choice: asymptotics and exact
calculations. J. R. Stat. Soc. Series B Stat. Methodol., 56(3):501‚Äì514, 1994.
[51] A. Gelman, D. Lee, and J. Guo. Stan: A probabilistic programming language
for Bayesian inference and optimization. J. Edu. Behav. Stat., 40(5):530‚Äì543,
2015.
[52] C. J. Geyer. Practical Markov chain Monte Carlo. Statistical Science, pages
473‚Äì483, 1992.
[53] W. R. Gilks. Markov chain Monte Carlo. John Wiley and Sons, Ltd., 2005.
[54] W. R. Gilks, S. Richardson, and D. Spiegelhalter, editors. Markov chain Monte
Carlo in practice. CRC press, 1995.

156
Bibliography
[55] V. Goder and S. M. Molecular mechanism of signal sequence orientation in
the endoplasmic reticulum. EMBO J., 22(14):3645‚Äì3653, 2003.
[56] R. Graham.
Bounds on multiprocessing timing anomalies.
SIAM J. App.
Math., 17(2):416‚Äì429, 1969.
[57] R. D. Groot and P. B. Warren. Dissipative particle dynamics: Bridging the
gap between atomistic and mesoscopic simulation. The Journal of Chemical
Physics, 107(11):4423‚Äì4435, Sept. 1997.
[58] P. Hadjidoukas, E. Lappas, and V. Dimakopoulos.
A runtime library for
platform-independent task parallelism ,. In 20th Euromicro International Con-
ference on Parallel, Distributed and Network-Based Processing (PDP), pages
229‚Äì236. IEEE Computer Society, Munich, Germany, 2012.
[59] P. E. Hadjidoukas, P. Angelikopoulos, L. Kulakova, C. Papadimitriou, and
P. Koumoutsakos. Exploiting Task-Based Parallelism in Bayesian Uncertainty
QuantiÔ¨Åcation, volume 9233 of LNCS, pages 532‚Äì544. Springer Berlin Heidel-
berg, 2015.
[60] P. E. Hadjidoukas, P. Angelikopoulos, C. Papadimitriou, and P. Koumout-
sakos. Œ†4U: A high performance computing framework for Bayesian uncer-
tainty quantiÔ¨Åcation of complex models. J. Comput. Phys., 284(1):1‚Äì21, 2015.
[61] P. E. Hadjidoukas, P. Angelikopoulos, D. Rossinelli, D. Alexeev, C. Papadim-
itriou, and P. Koumoutsakos. Bayesian uncertainty quantiÔ¨Åcation and prop-
agation for discrete element simulations of granular materials. Comp. Meth.
Appl. Mech. Eng., 282:218‚Äì238, 12 2014.
[62] A. M. Halpern and T. Haute. Structural and thermodynamic properties of
the argon dimer a computational chemistry exercise in quantum and statistical
mechanics. J. Chem. Educ., 87(2), 2010.
[63] N. Hansen, S. D. M¬®uller, and P. Koumoutsakos. Reducing the time complexity
of the derandomized evolution strategy with covariance matrix adaptation
(CMA-ES). Evolutionary computation, 11(1):1‚Äì18, 2003.
[64] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in
evolution strategies. Evolutionary computation, 9(2), 2001.

Bibliography
157
[65] W. K. Hastings. Monte Carlo sampling methods using Markov chains and
their applications. Biometrika, 57(1):97‚Äì109, 1970.
[66] S. Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR,
1994.
[67] L. Hedin, K. ¬®Ojemalm, A. Bernsel, A. Hennerdal, K. IllergÀöard, K. Enquist, and
et al. Membrane insertion of marginally hydrophobic transmembrane helices
depends on sequence context. Journal of Molecular Biology, 396(1):221‚Äì229,
2010.
[68] T. Hessa, H. Kim, K. Bihlmaier, C. Lundin, J. Boekel, H. Andersson, I. Nils-
son, S. H. White, and G. von Heijne. Recognition of transmembrane helices
by the endoplasmic reticulum translocon. Nature, 433(7024):377‚Äì381, 2005.
[69] T. Hessa, N. M. Meindl-Beinker, A. Bernsel, H. Kim, Y. Sato, M. Lerch-Bader,
and et al. Molecular code for transmembrane-helix recognition by the Sec61
translocon. Nature, 450(7172):1026‚Äì1030, 2007.
[70] P. J. Hoogerbrugge and J. M. V. A. Koelman. Simulating Microscopic Hydro-
dynamic Phenomena with Dissipative Particle Dynamics. EPL (Europhysics
Letters), 19(3):155, June 1992.
[71] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew. Extreme learning machine: theory
and applications. Neurocomputing, 70(1):489‚Äì501, 2006.
[72] J. E. Jones. On the determination of molecular Ô¨Åelds. ii. from the equation of
state of a gas. Proceedings of the Royal Society of London A: Mathematical,
Physical and Engineering Sciences, 106(738):463‚Äì477, 1924.
[73] M. H. Kalos and P. A. Whitlock. Monte Carlo Methods. Wiley-VCH, 2008.
[74] L. S. Katafygiotis and H. Lam.
Tangential projection algorithm for mani-
fold representation in unidentiÔ¨Åable model updating problems.
Earthquake
engineering & structural dynamics, 31(4):791‚Äì812, 2002.
[75] C. Kemp, A. Perfors, and J. B. Tenenbaum. Learning overhypotheses with
hierarchical Bayesian models. Developmental science, 10(3):307‚Äì321, 2007.

158
Bibliography
[76] J. Kestin, K. Knierim, E. A. Mason, B. NajaÔ¨Å, S. T. Ro, and M. Waldman.
Equilibrium and transport properties of the noble gases and their mixtures at
low density. J. Phys. Chem. Ref. Data, 13:229‚Äì303, 1984.
[77] L. Kocik, T. Junne, and M. Spiess.
Orientation of internal signal-anchor
sequences at the sec61 translocon. J. Mol. Biol., 424(5):368‚Äì378, 2012.
[78] D. Krige. A statistical approach to some mine valuations and allied problems
at the witwatersrand. Master‚Äôs thesis, University of Witwatersrand, 1951.
[79] H. Kruggel-Emden, S. Wirtz, and V. Scherer. A study on tangential force
laws applicable to the discrete element method (DEM) for materials with
viscoelastic or plastic behavior. Chem. Eng. Sci., 63(6):1523‚Äì1541, 2008.
[80] L. Kulakova, P. Angelikopoulos, P. E. Hadjidoukas, C. Papadimitriou, and
P. Koumoutsakos. Approximate Bayesian computation for granular and molec-
ular dynamics simulations. In Proceedings of the Platform for Advanced Sci-
entiÔ¨Åc Computing Conference, page 4. ACM, 2016.
[81] M. D. Lee. BayesSDT: Software for Bayesian inference with signal detection
theory. Behavior Research Methods, 40(2):450‚Äì456, 2008.
[82] J. Li, M. Dao, C. Lim, and S. Suresh.
Spectrin-level modeling of the cy-
toskeleton and optical tweezers stretching of the erythrocyte.
Biophys. J.,
88(5):3707‚Äì3719, 2005.
[83] J. S. Lopes, M. Arenas, D. Posada, and M. A. Beaumont. Coestimation of
recombination, substitution and molecular adaptation rates by approximate
Bayesian computation. Heredity, 112(3):255‚Äì264, March 2014.
[84] P. Marepalli, J. Y. Murthy, B. Qiu, and X. Ruan. Quantifying uncertainty
in multiscale heat conduction calculations. J. Heat Transfer, 136(11):111301‚Äì
111301, 08 2014.
[85] P. Marjoram, J. Molitor, V. Plagnol, and S. Tavar¬¥e. Markov chain Monte Carlo
without likelihoods.
Proc. Natl. Ac. Am., 100(26):15324‚Äì15328, December
2003.

Bibliography
159
[86] S. J. Marrink, A. H. de Vries, and A. E. Mark. Coarse grained model for
semiquantitative lipid simulations.
The Journal of Physical Chemistry B,
108(2):750‚Äì760, 2004.
[87] A. D. Martin, K. M. Quinn, and J. H. Park.
MCMCpack: Markov chain
Monte Carlo in R. J. Stat. Soft., 42(9):1‚Äì21, 2011.
[88] E. W. Merrill, E. R. Gilliland, G. Cokelet, H. Shin, A. Britten, and R. E. Wells.
Rheology of human blood, near and at zero Ô¨Çow: eÔ¨Äects of temperature and
hematocrit level. Biophys. J., 3(3):199‚Äì213, 1963.
[89] R. Meyer and J. Yu. BUGS for a bayesian analysis of stochastic volatility
models. The Econometrics Journal, 3(2):198‚Äì215, 2000.
[90] R. M. Neal. Sampling from multimodal distributions using tempered transi-
tions. Statistics and computing, 6(4):353‚Äì366, 1996.
[91] M. J. Niesen,
C. Y. Wang,
R. C. Van Lehn,
and T. F. Miller III.
Structurally detailed coarse-grained model for Sec-facilitated co-translational
protein translocation and membrane integration.
PLoS Computat. Biol.,
13(3):e1005427, 2017.
[92] W. Pan, I. V. Pivkin, and G. E. Karniadakis. Single-particle hydrodynamics
in DPD: A new formulation. Europhys. Lett., 84(1):10012, 2008.
[93] M. L. Parks, P. Seleson, S. J. Plimpton, S. A. Silling, and R. B. Lehoucq.
Peridynamics with lammps: A user guide, v0. 3 beta. Sandia Report, pages
2011‚Äì8253, 2011.
[94] Z. Peng, X. Li, I. V. Pivkin, M. Dao, G. E. Karniadakis, and S. Suresh. Lipid
bilayer and cytoskeletal interactions in a red blood cell. PNAS, 110(33):13356‚Äì
13361, 2013.
[95] P. H. Peskun.
Optimum Monte Carlo sampling using Markov chains.
Biometrika, 60(3):607‚Äì612, 1973.
[96] C. Peter and K. Kremer. Multiscale simulation of soft matter systems. Faraday
Discuss., 144:9‚Äì24, 2010.

160
Bibliography
[97] I. V. Pivkin and G. E. Karniadakis. Accurate coarse-grained modeling of red
blood cells. Phys. Rev. Lett., 101(11):118105, 2008.
[98] S. Plimpton. Fast parallel algorithms for short-range molecular dynamics. J.
Comput. Phys., 117(1):1‚Äì19, March 1995.
[99] J. K. Pritchard, M. T. Seielstad, A. Perez-Lezaun, and M. W. Feldman. Popu-
lation growth of human y chromosomes: A study of y chromosome microsatel-
lites. Mol. Biol. Evol., 16(12):1791‚Äì1798, December 1999.
[100] A. Rahman. Correlations in the motion of atoms in liquid argon. Phys. Rev.,
136:A405‚ÄìA411, 1964.
[101] A. Rau, F. JaÔ¨Ärezic, J.-L. Foulley, and R. W. Doerge. Reverse engineering gene
regulatory networks using approximate Bayesian computation. Stat. Comput.,
22(6):1257‚Äì1271, November 2012.
[102] F. Rizzi, H. Najm, B. Debusschere, K. Sargsyan, M. Salloum, H. Adal-
steinsson, and O. Knio. Uncertainty quantiÔ¨Åcation in MD simulations. part
II: Bayesian inference of force-Ô¨Åeld parameters.
SIAM Multi. Mod. Sim.,
10(4):1460‚Äì1492, 2012.
[103] C. P. Robert and G. Casella.
Monte Carlo Statistical Methods Springer.
Springer, New York, 2004.
[104] C. P. Robert, J. M. Cornuet, J. M. Marin, and N. S. Pillai. Lack of conÔ¨Ådence
in approximate Bayesian computation model choice. PNAS, 108(37):15112‚Äì
15117, 2011.
[105] L. A. Rowley, D. Nicholson, and N. G. Parsonage. Monte Carlo grand canon-
ical ensemble calculation in a gas-liquid transition region for 12-6 argon. J.
Comp. Phys., 17:401‚Äì414, 1975.
[106] W. S., A. P., P. C., and K. P.
Bayesian Annealed Sequential Importance
Sampling (BASIS): an unbiased version of Transitional Markov Chain Monte
Carlo. ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems,
Part B: Mechanical Engineering, 2017.

Bibliography
161
[107] K. Scranton, J. Knape, and P. de Valpine. An approximate Bayesian com-
putation approach to parameter estimation in a stochastic stage-structured
population model. Ecology, 95(5):1418‚Äì1428, May 2014.
[108] J. R. Shaw, M. Bridges, and M. P. Hobson. EÔ¨Écient Bayesian inference for
multimodal problems in cosmology. Monthly Notices of the Royal Astronomical
Society, 378(4):1365‚Äì1370, 2007.
[109] M. S. Shell. The relative entropy is fundamental to multiscale and inverse
thermodynamic problems. J. Chem. Phys., 129(14):144108, 2008.
[110] L. Shi and T. L. GriÔ¨Éths. Neural implementation of hierarchical Bayesian
inference by importance sampling. Advances in neural information processing
systems, pages 1669‚Äì1677, 2009.
[111] W. Shinoda, M. Shiga, and M. Mikami. Rapid estimation of elastic constants
by molecular dynamics simulation under constant stress. Phys. Rev. B, 69(13),
April 2004.
[112] M. L. Stein. Statistical Interpolation of Spatial Data: Some Theory for Kriging.
Springer, New York, 1999.
[113] D. Straub and I. Papaioannou. Bayesian updating with structural reliability
methods. J. Eng. Mech.-ASCE, 141(3):04014134, 2015/06/03 2014.
[114] M. A. Suchard and B. D. Redelings. BAli-Phy: simultaneous Bayesian infer-
ence of alignment and phylogeny. Bioinformatics, 22(16):2047‚Äì2048, 2006.
[115] M. SunnÀöaker, A. G. Busetto, E. Numminen, J. Corander, M. Foll, and
C. Dessimoz. Approximate Bayesian computation. PLoS computational bi-
ology, 9(1):e1002803, 2013.
[116] S. Suresh, J. Spatz, J. P. Mills, A. Micoulet, M. Dao, C. T. Lim, M. Beil,
and T. SeuÔ¨Äerlein. Connections between single-cell biomechanics and human
disease states: gastrointestinal cancer and malaria. Acta Biomater., 1:15‚Äì30,
2005.
[117] D. A. Tallmon, G. Luikart, and M. A. Beaumont. Comparative evaluation
of a new eÔ¨Äective population size estimator based on approximate Bayesian
computation. Genetics, 167(2):977‚Äì988, June 2004.

162
Bibliography
[118] R. Tran-Son-Tay, S. P. Sutera, and P. R. Rao. Determination of red blood cell
membrane viscosity from rheoscopic observations of tank-threading motion.
Biophys. J., 46:65‚Äì72, 1984.
[119] B. M. Turner and T. Van Zandt. A tutorial on approximate Bayesian compu-
tation. J. Math. Psychol., 56(2):69‚Äì85, April 2012.
[120] M. K. Vakilzadeh, V. Yaghoubi, A. T. Johansson, and T. Abrahamsson. Man-
ifold Metropolis adjusted Langevin algorithm for high-dimensional Bayesian
FE model updating in structural dynamics. In Proceedings of the 9th interna-
tional conference on structural dynamics, EURODYN, 2014.
[121] S. W. Van Sciver. Helium cryogenics, page 63. Springer Science & Business
Media, 2012.
[122] V. Vyshemirsky and M. Girolami. BioBayes: a software package for Bayesian
inference in systems biology. Bioinformatics, 24(17):1933‚Äì1934, 2008.
[123] K. Walters. Parameter estimation for an immortal model of colonic stem cell
division using approximate Bayesian computation. J. Theor. Biol., 306:104‚Äì
114, August 2012.
[124] D. Wegmann, C. Leuenberger, and L. ExcoÔ¨Éer.
EÔ¨Écient approximate
Bayesian computation coupled with Markov chain Monte Carlo without like-
lihood. Genetics, 182(4):1207‚Äì1218, 2009.
[125] R. E. Wells Jr and E. W. Merrill.
InÔ¨Çuence of Ô¨Çow properties of blood
upon viscosity-hematocrit relationships.
Journal of Clinical Investigation,
41(8):1591, 1962.
[126] J. A. White. Lennard-jones as a model for argon and test of extended renor-
malization group calculations. J. Chem. Phys., 111:9352‚Äì9356, 1999.
[127] R. D. D. Wilkinson.
Approximate Bayesian computation (ABC) gives ex-
act results under the assumption of model error.
Stat. Appl. Genet. Mol.,
12(2):129‚Äì141, Jan. 2013.
[128] S. Wu, P. Angelikopoulos, C. Papadimitriou, R. Moser, and P. Koumout-
sakos. A hierarchical Bayesian framework for force Ô¨Åeld selection in molecular
dynamics simulations. Phil. Trans. R. Soc. A, 374:20150032, 2015.

Bibliography
163
[129] S. Wu, P. Angelikopoulos, G. Tauriello, C. Papadimitriou, and P. Koumout-
sakos. Fusing heterogeneous data for the calibration of molecular dynamics
force Ô¨Åelds using hierarchical Bayesian models. J. Chem. Phys., 145:244112,
2016.
[130] H. Zhao and E. S. Shaqfeh. Shear-induced platelet margination in a microchan-
nel. Physical Review E, 83(6):061924, 2011.


Own Publications
1. Kulakova L., Arampatzis G., Angelikopoulos P., Hadjidoukas P. E., Papadim-
itriou C., Koumoutsakos P. Experimental Data over Quantum Mechanics Sim-
ulations for Inferring the Repulsive Exponent of the Lennard-Jones Potential
in Molecular Dynamics. Accepted for publication in ScientiÔ¨Åc Reports (2017)
2. Kulakova L., Angelikopoulos P., Hadjidoukas P. E., Papadimitriou C., Koumout-
sakos P. Approximate Bayesian Computation for Granular and Molecular Dy-
namics Simulations. Proceedings of the Platform for Advanced ScientiÔ¨Åc Com-
puting Conference PASC‚Äô16 (2016)
3. Hadjidoukas P.E., Angelikopoulos P., Kulakova L., Papadimitriou C., Koumout-
sakos P. Exploiting Task-Based Parallelism in Bayesian Uncertainty QuantiÔ¨Å-
cation. Lecture Notes in Computer Science, 9233, 532 (2015)
165


Curriculum Vitae
Name: Lina Kulakova
Citizen of: Nizhny Novgorod, Russia
Born: December 1, 1990, Nizhny Novgorod, USSR
2013-2017: Researcher and teaching assistant at the Computational Science
and Engineering Lab, ETH Zurich, under the supervision of Prof.
Dr. Petros Koumoutsakos
2008-2013 Diploma in Moscow State University, Russia
167

