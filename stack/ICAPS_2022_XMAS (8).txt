Explaining Preference-Driven Schedules: The EXPRES Framework
Alberto Pozanco1, Francesca Mosca1, Parisa Zehtabi1, Daniele Magazzeni1, Sarit Kraus2
1J.P. Morgan AI Research
2Department of Computer Science, Bar-Ilan University
{alberto.pozancolancho, francesca.mosca, parisa.zehtabi, daniele.magazzeni}@jpmorgan.com, sarit@cs.biu.ac.il
Abstract
Scheduling is the task of assigning a set of scarce resources
distributed over time to a set of agents, who typically have
preferences about the assignments they would like to get.
Due to the constrained nature of these problems, satisfying
all agents’ preferences is often infeasible, which might lead to
some agents not being happy with the resulting schedule. Pro-
viding explanations has been shown to increase satisfaction
and trust in solutions produced by AI tools. However, it is par-
ticularly challenging to explain solutions that are inﬂuenced
by and impact on multiple agents. In this paper we intro-
duce the EXPRES framework, which can explain why a given
preference was unsatisﬁed in a given optimal schedule. The
EXPRES framework consists of: (i) an explanation generator
that, based on a Mixed-Integer Linear Programming model,
ﬁnds the best set of reasons that can explain an unsatisﬁed
preference; and (ii) an explanation parser, which translates
the generated explanations into human interpretable ones.
Through simulations, we show that the explanation generator
can efﬁciently scale to large instances. Finally, through a set
of user studies within J.P. Morgan, we show that employees
preferred the explanations generated by EXPRES over human-
generated ones when considering workforce scheduling sce-
narios.
Introduction
Scheduling is the task of assigning a set of scarce resources
distributed over time to a set of agents. This is the case of
many well-known problems, such as assigning jobs to ma-
chines (Watson et al. 2003), nurses to work shifts (Legrain,
Bouarab, and Lahrichi 2015), or teachers to courses (Gu-
nawan, Ng, and Poh 2007), among others. Another appli-
cation that has become very relevant to organisations (in-
cluding J.P. Morgan) due to COVID-19 restrictions is that of
scheduling or assigning employees to a limited number of
desks (less than in normal situations in order to guarantee so-
cial distancing) over a ﬁxed time period. In this context, em-
ployees may have speciﬁc preferences regarding their sched-
ules, such as the dates or the number of days a week that they
want to be at the workplace, or the peers they would like
to meet more often, etc. However, the limited availability
of desks may preclude the fulﬁllment of all of the employ-
ees’ preferences, and this may hinder their satisfaction with
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
PRES
solver
constraints
preferences
explanation
generator
explanation
parser
raw explanations
EXPRES
usable
explanations
optimal
solution
unsatisfied
preference
Figure 1: Overview of the EXPRES framework.
the schedule. Presenting a contrastive explanation of the rea-
sons why the employees could not be scheduled in any other
way may promote the acceptance of the schedule (Bradley
and Sparks 2009). Furthermore, beside the intrinsic difﬁ-
culty of explaining a schedule for an individual, more chal-
lenges arise when considering the automated explanation of
decisions regarding multiple agents. Among the features that
(Kraus et al. 2020) argue for, an explainable system should
(i) allow its users to understand its decision-making, (ii) be
able to generate different types of explanations, so to pro-
vide tailored outputs for its users, (iii) while preserving the
agents’ privacy.
In this paper we present the EXPRES framework (see
Fig. 1), a novel approach to providing EXplanations for
PREference-driven Scheduling problems (PRES). Our for-
malisation is general, allowing it to be applied in different
scenarios; however, for the sake of clarity, we discuss and
empirically evaluate a complete example of EXPRES in the
context of workforce scheduling at J.P. Morgan.
We ﬁrst deﬁne PRES problems as scheduling tasks where
an optimal solution is identiﬁed not only by a set of con-
straints, but also by a totally ordered set of preferences. We
then formalise the problem of explaining PRES solutions
(EXPRES) as an optimisation task where, given a schedule
and an agent’s unsatisﬁed preference, we ﬁnd the best set of
reasons that can justify it. On one hand, EXPRES explana-
tions simplify the task, frequently manually performed with
notable effort and time-consumption, of providing justiﬁca-
tion for a speciﬁc unsatisﬁed preference in a given sched-
ule. On the other hand, these explanations aim to support

the individual’s understanding of the other factors, beyond
their own preferences, that inﬂuenced the schedule. We pro-
pose to model EXPRES as a Mixed-integer Linear Program-
ming (MILP) problem. After that, we show how to group
and translate the computer generated explanations to natural
language in order for them to be easily interpreted by hu-
mans. After discussing how to convey the explanations to
end users, we show through software simulations that EX-
PRES is able to (i) scale to large instances, in terms of num-
ber of employees and preferences; and (ii) provide different
explanations, in terms of reasons to justify the unsatisﬁed
preference. Later, we also present the results of a user study
within J.P. Morgan that shows how EXPRES explanations are
better appreciated than human-generated ones. Finally, we
draw our main conclusions and outline future work.
PRES: Preference-driven Scheduling
In this section we formalise a scheduling problem where a ﬁ-
nite set of resources R distributed across different time slots
T needs to be assigned to a set of agents Ag. Each agent
might have a set of constraints as well as preferences of dif-
ferent types. An external actor, namely the Principal, spec-
iﬁes a set of global constraints and a total order over the
agents’ preference types1. We refer to this set of scheduling
problems as PRES, and formally deﬁne them as follows:
Deﬁnition 1. A
PRES problem is a tuple
PRES
=
⟨R, Ag, T, C, P, O⟩where:
• R is a set of resource types
• Ag is a set of agents
• T is a set of time slots
• C = S
ag∈Ag Cag ∪CP rinc is the set of all the con-
straints, where Cag are the constraints of the agent ag,
and CP rinc are the constraints imposed by the Principal
• P = S
ag∈Ag Pag is the set of all agents’ preferences
• O = {τ1 ≺. . . ≺τ|O|} is a totally ordered set of pref-
erence types τi deﬁned by the Principal; τi ≺τj means
that τi precedes (is more important than) τj in the order.
Given a preference p ∈P, we refer to the agent having the
preference, to its type and to the order of its type as ag(p),
τ(p) and O(p) respectively. The solution to a PRES problem
is a schedule S, which consists of a set of assignments as =
⟨ag, r, t⟩∈{Ag×R×T} that assign agents to resources and
time slots. Again, ag(as) and r(as) refer to the agent and
the resource of the assignment respectively. We refer as S ⊆
{Ag × R × T} to the set of all feasible schedules subject to
the constraints in C. The optimal solution to a PRES problem
is deﬁned according to the totally ordered set of preference
types O.
Deﬁnition 2. Given a set of solutions ˜S and a preference
type τ, a preference ﬁlter function fτ : ˜S −→˜Sτ returns
the set of solutions that maximises the number of preferences
p ∈P of type τ that are satisﬁed.
1This problem setting was considered in a real-world scenario,
namely the return to the ofﬁce at J.P. Morgan, as we discuss later.
We formally deﬁne an optimal schedule given a totally
ordered set of preference types and the set of feasible sched-
ules as follows:
Deﬁnition 3. A set of optimal schedules S∗is the output of
a composition of preference ﬁlter functions, where the order
of composition is given by the total order O:
S∗= fτ|O| ◦· · · ◦fτ1(S)
where τ1 is the ﬁrst (most important) preference type.
In the rest of the paper we assume that the computed so-
lutions are optimal and that the agents’ preferences do not
contradict their own constraints or the constraints deﬁned
by the Principal (e.g., an agent cannot ask for a number of
resources higher than their own maximum number or higher
than the number of available resources).
EXPRES: Explaining PRES Solutions
Given the constrained nature of the PRES problems, it may
happen that in a schedule S some agents’ preferences are
not accommodated. We refer to these as unsatisﬁed pref-
erences UNSAT ⊆P, using SAT ⊆P to denote the set
of satisﬁed preferences. Note that UNSAT ∩SAT = ∅. In
such scenarios we aim to provide agents with informative
explanations that clarify why some of their preferences were
unsatisﬁed. Particularly, we explain unsatisﬁed preferences
by reporting the reasons why the preferred assignments had
to be assigned in a different way. We refer to this task as ex-
plaining PRES solutions (EXPRES); before formalising it, we
deﬁne some concepts.
In order to provide an explanation for why preference u ∈
UNSAT is unsatisﬁed, we ﬁrst need to identify the set of
assignments that were responsible for or involved in u being
unsatisﬁed.
Deﬁnition 4. INVOLVED(S, u) is a domain-speciﬁc func-
tion that computes the set of assignments S′ ⊆S involved
in u ∈UNSAT being unsatisﬁed.
For example, assuming an unsatisﬁed preference u, an as-
signment as is involved in u if t(as) = t(u), i.e., if there is
another agent assigned to the time slot to which agent ag(u)
preferred to be assigned. The INVOLVED function relates as-
signments to unsatisﬁed preferences, returning the set of as-
signments to be justiﬁed or explained.
We also need a way of relating satisﬁed preferences to
assignments, i.e., if a given assignment is affected or not by
a preference.
Deﬁnition 5. AFFECTED(p, as) is a domain-speciﬁc func-
tion that returns a binary output indicating whether assign-
ment as ∈S is affected by satisﬁed preference p ∈SAT.
For example, assuming a satisﬁed preference p, an assign-
ment as is affected by p if ag(p) = ag(as), i.e., the agent
ag had a preference regarding that assignment.
Explanations in our setting are formed by reasons that jus-
tify why the preferred assignments had to be assigned in a
different way. We formally deﬁne a reason as follows:
Deﬁnition 6. Given a PRES problem and a schedule S that
solves it, a reason is a tuple R = ⟨p, as, u⟩, where p ∈

SAT ⊆P is a satisﬁed preference, as ∈S is an assignment,
and u ∈UNSAT ⊆P is an unsatisﬁed preference.
Reasons can be read as “preference u about assignment
as was unsatisﬁed due to preference p being satisﬁed”.
Deﬁnition 7. A reason R = ⟨p, as, u⟩is well deﬁned iff (1)
AFFECTED(p, as) = 1; (2) as ∈INVOLVED(S, u); and (3)
RANK(p, ≤, u).
Here we assume a RANK function that returns 1 (True)
if O(p) ≤O(u). A well deﬁned reason employs a more
important preference over an assignment to explain why a
lower (or equally) important preference over that same as-
signment was unsatisﬁed. If both preferences have the same
rank (O(p) = O(u)), the reason suggests that there exists
another optimal schedule in which u was satisﬁed, because
they are equally important. With these deﬁnitions at hand we
are ready to formalise an EXPRES problem as follows:
Deﬁnition 8. An EXPRES problem is a tuple EXPRES =
⟨PRES, S, u⟩where:
• PRES = ⟨R, Ag, T, C, P, O⟩is a PRES problem
• S is a set of assignments that optimally solve PRES
• u ∈UNSAT is an unsatisﬁed preference
The solution to an EXPRES problem is an explanation ES,u,
which is a set of reasons that describe why a preference u
was unsatisﬁed in a schedule S that optimally solves a PRES
problem.
Explanations have different properties depending on the
reasons they contain. We say that an explanation is complete
if it provides a reason for each of the assignments involved
in an unsatisﬁed preference.
Deﬁnition 9. An explanation ES,u that solves an EXPRES
problem is complete iff ∀as ∈INVOLVED(S, u), ∃R ∈
ES,u | as = as(R).
We also say that an explanation is sound if all of the rea-
sons in the explanation are well-deﬁned.
Deﬁnition 10. An explanation ES,u that solves an EXPRES
problem is sound iff R is well-deﬁned ∀R ∈ES,u.
Finally, we say that an explanation is optimal if it mini-
mizes the order of the satisﬁed preferences used by its rea-
sons.
Deﬁnition 11. An explanation ES,u that solves an EXPRES
problem is optimal iff minO(p)
P
R∈ES,u O(p(R)).
For an example of well deﬁned reasons, and complete,
sound and optimal explanations, see the later section “Re-
turn to the Ofﬁce at J.P. Morgan”.
Solving EXPRES Tasks using MILP
We propose to use MILP to solve EXPRES problems, i.e., to
compute the set of reasons that explain why a given pref-
erence was unsatisﬁed. The use of MILP to solve EXPRES
problems follows naturally, since we want to compute a set
of reasons that optimise a given metric (the order of the
preferences used in the reasons), subject to some constraints
(well-deﬁned reasons). Given an EXPRES problem, we for-
mulate it as a MILP as follows.2
minimize
X
p∈SAT,as∈INVOLVED(S,u)
xp,as,u ∗O(p)
(1)
subject to the following constraints:
X
p∈SAT
xp,as,u = 1, as ∈INVOLVED(S, u)
(2)
xp,as,u ≤AFFECTED(p, as) , p ∈SAT, as ∈INVOLVED(S, u)
(3)
xp,as,u ≤RANK(p, u) , p ∈SAT, as ∈INVOLVED(S, u)
(4)
There is only one type of decision variable, xp,as,u, which
represents all the possible reasons ⟨p, as, u⟩in the EX-
PRES problem: p ∈SAT, as ∈INVOLVED(S, u), and
u ∈UNSAT which is the unsatisﬁed preference we want
to explain. The variable xp,as,u gets a value of 1 if reason
⟨p, as, u⟩is used in the explanation ES,u that solves the EX-
PRES problem, and a value of 0 otherwise. Therefore, the
computational complexity of our approach depends on the
number of satisﬁed preferences and the number of assign-
ments to be explained returned by the INVOLVED function.
Expr. (1) models the objective function of our MILP: to
minimise the rank of the satisﬁed preferences used in the
explanation’s reasons. This means that the MILP tries to
use more important satisﬁed preferences to explain unsat-
isﬁed preferences. This objective function ensures that if the
MILP ﬁnds an optimal solution, the explanation ES,u ex-
tracted from such a solution is optimal (Def. 11).
Constr. (2) ensures that we provide exactly one reason for
each of the assignments returned by INVOLVED(S, u), i.e.,
for each assignment involved in u being unsatisﬁed in solu-
tion S. Therefore, if the MILP ﬁnds an optimal solution, the
explanation is complete (Def. 9).
Constr. (3) ensures that we only select reasons where
the given assignment is affected by the preference,
AFFECTED(p, as) = 1. Finally, Constr. (4) ensures that we
only select reasons where more (or equally) important pref-
erences are used to justify less (or equally) important un-
satisﬁed preferences. These two constraints ensure that, if
the MILP ﬁnds an optimal solution, the explanation is sound
(Def. 10).
In some cases, we might be interested in computing more
than one explanation for a given EXPRES task. This could be
the case when different users have a subjective order over
the preference types that is different from the Principal’s or-
der (O), making them prefer some reasons/explanations over
others. To compute multiple explanations, we can iteratively
run the MILP, forcing the previously found solutions to not
be accepted. In this way, we get explanations with equal or
higher cost (lower quality) at each iteration.
2Without loss of generality, we model EXPRES to generate ex-
planations for a single unsatisﬁed preference, but the problem can
be generalised to generate explanations for all the unsatisﬁed pref-
erences in a PRES problem.

Note that this MILP formulation is general and can be
used to generate explanations for any EXPRES task regard-
less of the preferences in the associated PRES problem. De-
pending on the preferences in the PRES problem and its in-
teractions, one just needs to appropriately deﬁne the IN-
VOLVED and AFFECTED functions that describe when as-
signments are involved in or are affected by unsatisﬁed and
satisﬁed preferences, respectively. Then the MILP automat-
ically generates the set of reasons that better explain an un-
satisﬁed preference.
Parsing and Clustering EXPRES Solutions
Explanations are the output of a cognitive process, meant to
identify the necessary information to justify an event, and a
social process, meant to convey that information to an ex-
plainee (Miller 2019). So, once MILP provides us with a
solution to the EXPRES problem, i.e., it identiﬁes the rea-
sons why u was unsatisﬁed in the schedule S, it is crucial
to make that solution understandable for any user, not only
for expert ones. In this section we present a possible ap-
proach, which we validated with users, as described later, to
parse the MILP explanations in natural language. Convey-
ing MILP solutions can be challenging, especially (i) when
the INVOLVED function returns many assignments that need
to be explained; and (ii) because the variables in the MILP
solution contain information about the agents and their pref-
erences: to include them in an explanation may be beneﬁcial
in some cases, but harmful in others where the privacy of the
agents needs to be preserved (Kraus et al. 2020).
According to the application context, we recommend the
deﬁnition of natural language templates describing con-
straints, preferences and reasons. For instance, a reason R =
⟨p, as, u⟩can be parsed with the following template: “ag(p)
was assigned r(as) on t(as) instead of ag(u) because to
satisfy τ(p) is more important than to satisfy τ(u)”. As we
mentioned, the solution returned by the MILP contains very
granular information (for each assignment), even after pars-
ing it in natural language. This can be useful for Principals,
as it allows them to understand all the details, but might
yield explanations that are tedious and difﬁcult to interpret
for general users. For this reason, we suggest to aggregate
the reasons according to time slot and preference type be-
fore parsing: e.g., if there are n reasons ⟨p, as, u⟩justifying
the assignments of n agents having satisﬁed preferences of
the same type on the time slot t, we could have “ag(p1), ...,
ag(pn) were assigned on t(as) instead of ag(u) because to
satisfy τ(p) is more important than to satisfy τ(u)”.
Finally, given an explanation ES,u of why the preference u
was not satisﬁed by the solution S to the PRES problem, we
parse it as follows: “u could not be satisﬁed because [list
of constraints C] [list of reasons in ES,u]”. Lastly and if
the scenario requires it, we recommend to remove any iden-
tifying reference to the agents whose satisﬁed preferences
are mentioned in the explanation, in order to better preserve
their privacy. For a complete example of parsed explana-
tions, see the next section, where we discuss an EXPRES
problem and solution in the context of workforce schedul-
ing, an application of interest at J.P. Morgan.
Employee Min-Max Days Preferred Group
Meetings
OOO
Edith
1 - 2
Th
1
17/11
George
4 - 5
1
15/11
Han
3 - 4
W,Th,F
1
17/11
Bob
5 - 5
2
18/11
Charlie
4 - 4
F
2
17/11
Daphne
2 - 3
M,Tu,Th
2
15/11
Alice
2 - 4
M,Tu
3
18/11
Fei
3 - 4
W,Th,F
3
15/11,17/11
Table 1: Scheduling preferences of the employees in the
working example.
Figure 2: Solution to the PRES problem in the working ex-
ample.
Return to the Ofﬁce at J.P. Morgan
In this section we exemplify the EXPRES problem formula-
tion and solution through MILP that we have thus far intro-
duced in a general manner. In order to do this, we refer to a
real-world scenario, namely return to the ofﬁce at J.P. Mor-
gan. As outlined in the introduction, in this PRES problem a
set of employees (Ag) needs to be assigned to a limited num-
ber of desks R (that is the only resource type) over a ﬁxed
time period (T). In the example we discuss, summarised by
Tab. 1 and Fig. 2, we consider 8 employees and a time pe-
riod of one working week (the 15th to the 20th of November,
2021). Coloured cells mark the assignments of employees to
the ofﬁce, with different colours representing different work-
ing groups (a set of agents that need to be co-assigned). The
company, which acts as Principal, deﬁnes the following set
of constraints C (in the depicted example, ndesks = 5). For
simplicity, here we assume assignments to be binary vari-
ables, i.e., yag,r,t = 1 if ⟨ag, r, t⟩∈S, and 0 otherwise.
X
ag∈Ag
yag,r,t = ndesks, r ∈R, t ∈T
(5)
X
t∈T
yag,r,t ≤maxDays(ag), r ∈R, ag ∈Ag
(6)
yag,r,t = 0
if dayOut(ag, t), ag ∈Ag, r ∈R, t ∈T
(7)
Constr. (5) ensures that the sum of the employees assigned
to a desk on the same day is equal to the number of avail-
able desks deﬁned by the Principal. Constr. (6) ensures that
employees are not assigned to the ofﬁce more days than the
maximum they requested. Constr. (7) ensures that employ-
ees are not assigned on the days they are out of ofﬁce, for
example due to vacations or personal matters.

Regarding the employees’ preferences, we consider the
following totally ordered set of preferences types O =
⟨τmin ≺τmeet ≺τgroup ≺τpref⟩, as imposed by the Prin-
cipal in J.P. Morgan, where:
• τmin represents a preference type where an employee
asks to have a desk at least n ∈N days over the time
period T (instantiated as p = ⟨min, ag, n⟩).
• τmeet represents a preference type where an employee
asks to have a desk on a given day because he/she has an
important meeting (instantiated as p = ⟨meet, ag, t⟩).
• τgroup represents a preference type where an employee
asks to have a desk together with another employee on a
given day because they need to collaborate (instantiated
as p = ⟨group, ag1, ag2, t⟩).
• τpref represents a preference type where an employee
asks to have a desk on a given weekday, e.g., for personal
convenience (instantiated as p = ⟨pref, ag, t⟩).
We
can
explain
any
unsatisﬁed
preference,
but
in
our
example
we
focus
on
explaining
bu
=
⟨pref, Edith, Thursday⟩,
i.e.,
why
Edith’s
preferred
day on Thursday could not be satisﬁed.
We instantiate the INVOLVED and AFFECTED functions
as follows. INVOLVED(S, u) returns the set of assignments
S′ ⊆S involved in an unsatisﬁed preference u depending
on its type:
• If τ(u) = min, INVOLVED returns all of the assignments
in as ∈S where ag(u) ̸= ag(as), in order to provide a
reason why the rest of the desks were better assigned in
that way and could not be assigned to ag(u).
• If u is of any other type, INVOLVED returns all of the
assignments in S where t(as) = t(u), in order to pro-
vide a reason why each of the desks on day t were bet-
ter assigned to other agents and could not be assigned to
ag(u).
For example, when considering bu, INVOLVED(S, bu) re-
turns the ﬁve assignments of agents to desks on Thursday:
{⟨Bob, Thursday⟩, ⟨Charlie, Thursday⟩, . . .}.
Likewise, AFFECTED(p, as) checks if a satisﬁed prefer-
ence p is related to an assignment as depending on the pref-
erence type:
• If τ(p) = min, AFFECTED returns 1 if ag(p) = ag(as)
and |T| −P
t∈T dayOut(ag(p), t) = n(p). That is, a sat-
isﬁed preference of this type affects an assignment if the
number of days that the employee is available over the
time period is equal to his/her minimum, i.e., his/her days
at the ofﬁce cannot be reduced; and if the employee in the
preference is the same as the employee in the assignment.
• If τ(p) = meet or τ(p) = pref, AFFECTED returns 1
if ag(p) = ag(as) and t(p) = t(as). That is, a satisﬁed
preference of these types affects an assignment if the em-
ployee was assigned a desk on the day he/she requested.
• If τ(p) = group, AFFECTED returns 1 if ag1(p) =
ag(as), t(p) = t(as), and ∃as2 ∈S | ag2(p) =
ag(as2), t(p) = t(as2). That is, a satisﬁed preference
of this type affects an assignment if both employees are
assigned a desk on the same day.
In the running example, considering a satisﬁed prefer-
ence p′
=
⟨group, Edith, George, Wednesday⟩, we
have AFFECTED(p′, ⟨Edith, desk, Wednesday⟩) = 1 be-
cause the preference is related to the assignment; and
AFFECTED(p′, ⟨Han, desk, Tuesday⟩) = 0 because the
preference is not related to that assignment.
When explaining why bu was unsatisﬁed in the schedule
S∗, to say that Edith could not be assigned on Thursday be-
cause Daphne was assigned on Monday is an ill-deﬁned rea-
son, but to refer to Bob being assigned on Thursday due to a
meeting is a well-deﬁned one (cf. Def. 7). An example of a
complete, sound and optimal explanation (cf. Defs. 9,10,11)
is E: “The preference could not be satisﬁed because the 5
available desks were assigned to other people with more im-
portant preferences: George, Bob and Charlie due to a min-
imum number of days per week; Alice due to meetings; and
Fei due to 1 working group.” EXPRES can generate other
explanations, not optimal but still sound and complete, that
may be of interest according to the circumstances. For exam-
ple, Alice’s assignment could be justiﬁed due to her working
group, without mentioning her meeting as in E, in case that
meeting was conﬁdential, or if Edith considers more con-
vincing explanations regarding working groups (if her sub-
jective order over the preference types is different from the
Principal’s one).
Evaluation through Simulation
We evaluate our approach by providing explanations in sim-
ulated scenarios of our return to the ofﬁce domain.
Experimental Setting
We generate problems in three conﬁgurations of increasing
size: 10, 30 and 50 employees over a ﬁxed period of a ﬁve-
day week. For each conﬁguration, we generate 100 PRES
problems with random preferences for each agent. Each
agent randomly has 1 or 2 meetings, 1 or 2 preferred days
and 1 to 4 working group preferences. The number of days
on which an agent has a preference deﬁnes his/her prefer-
ence for minimum number of days; as a maximum number
of days, they have a random number between their mini-
mum and 5. For example, if an agent has preferences over
2 different days, its minimum number of days is 2 and its
maximum is a random number between 2 and 5. Agents also
have a 20% probability of having dates out (1 or 2, randomly
picked). We set the number of available desks each day to be
50% of the total number of employees, in line with the pol-
icy followed in the actual return to the ofﬁce at J.P. Morgan.
We optimally solve all of these PRES problems and auto-
matically compute all of the satisﬁed and unsatisﬁed prefer-
ences. We have an average of 31.4 satisﬁed and 28.7 unsat-
isﬁed preferences in the problems with 10 employees; 89.4
and 141.7 in the problems with 30 employees; and 147.5 and
280.1 in the problems with 50 employees. For each problem,
we randomly pick one unsatisﬁed preference of each type
(if one exists) to build the EXPRES task to be solved by the
MILP. This gives us a total of 1200 (3 agent conﬁgurations,
100 problems on each conﬁguration, and 4 unsatisﬁed pref-
erences on each problem) EXPRES tasks to solve. We run

the MILP on each task with a timeout of 30 seconds, or after
1000 explanations are produced. We use CPLEX3 to solve
the MILP. Experiments were run in Intel(R) Xeon(R) CPU
E3-1585L v5 @ 3.00GHz machines with 64GB of RAM.
Scalability Evaluation
First, we evaluate the scalability of our approach, by mea-
suring the time needed to compute the ﬁrst (optimal) ex-
planation and the number of explanations provided for each
scenario (see left plot of Figure 3). EXPRES tasks can be
solved in a reasonable time for all the conﬁgurations: even
with 50 agents, the solver returns the optimal explanation
on average in less that 0.5 seconds. Regarding the number
of generated explanations, we expect it to increase as we in-
crease the number of agents, because with more agents there
are more satisﬁed preferences and more ways of combining
them to justify unsatisﬁed preferences. However, when in-
creasing the number of agents, the complexity of the prob-
lems also increases, allowing less problems to be solved
within the time bound. With 10 agents, we are able to gen-
erate all of the sound explanations in less than 1 second.
For these problems, we can compute an average of 19.7 dif-
ferent explanations, with some EXPRES tasks where we can
compute more than 250 explanations. With 30 and 50 agents
we cannot generate all of the sound explanations within the
given time bound. Without timing out, the solver generates
an average of 157.2 and 402.3 different explanations with
30 and 50 agents, respectively. We conclude that EXPRES is
scalable because, even though not all the sound explanations
are found, the user will ultimately be interested in only one:
how to identify and learn the preferred explanation for a user
is part of our future work.
Explanations per Unsatisﬁed Preference Type
Next, we analyse how the number of explanations is inﬂu-
enced by the type of unsatisﬁed preference that needs to be
explained (see the central plot of Figure 3). We consider only
the problems with 10 agents given that it is the only conﬁg-
uration for which we can compute all of the explanations
in all of the problems. As expected, we can compute more
explanations if we explain less important, unsatisﬁed pref-
erences such as those of type τpref, where we can produce
an average of 44.5 explanations for each unsatisﬁed pref-
erence. This happens because there is a larger number of
more important preferences that can be used to explain these
unsatisﬁed preferences. Coherently, there are fewer ways of
explaining more important preferences such as preferences
of type τmeet being unsatisﬁed, since they can only be ex-
plained by using preferences of type τmeet or τmin, that are
equally or more important. In fact, there are approximately
35% of problems involving an unsatisﬁed preference in τmin
where we cannot produce any explanation. This is because
these problems were extremely overconstrained and the so-
lution could not satisfy many requests for a minimum num-
ber of days. Since our reasons require satisﬁed preferences,
there were some cases where we could not produce any ex-
planation.
3https://www.ibm.com/analytics/cplex-optimizer
Understanding Explanation Structure
Finally, given an EXPRES problem and the set of generated
explanations, we investigate how different the explanations
within that set are. We focus on the 100 problems with 10
agents where the selected unsatisﬁed preference is of type
τpref, since these are the cases where we can generate more
explanations. Then, given two explanations E1 and E2, we
measure their distance as |E1 \ E2|, i.e., the number of dif-
ferent reasons they contain. Given the 5 available desks, the
distance is bounded between 0, if both explanations are the
same, and 5, if the two explanations have no common rea-
son. We compute this pairwise distance for all of the pairs of
explanations produced in an EXPRES problem (see the right-
hand plot of Figure 3). As we can see, the standard devia-
tion of the explanations’ pairwise distance in each problem
is close to 1, meaning that explanations tend to vary for one
out of ﬁve reasons. The average distance between explana-
tions in a problem is 2.3, and the average maximum distance
is 3.8, with more than 75% of the problems having a maxi-
mum distance between explanations higher than 2.7. These
results suggest that many of the explanations we provide for
a problem only differ in 1−2 reasons (20−40% of the expla-
nation). However, most of the time our set of explanations
contains at least two explanations that are really different,
differing in 3 −4 reasons (60 −80% of the explanation).
Evaluation through User Studies
We have designed and implemented two user studies in order
to understand (i) how humans solve EXPRES tasks, (ii) how
automated generated explanations compare to human gener-
ated ones, and (iii) what type of explanations are preferred
by users of the tool. In particular, we wanted to validate the
following hypotheses:
• Hp1: The EXPRES framework produces explanations
faster than humans.
• Hp2: Humans ﬁnd automatically generated explanations
at least as satisfying as the human generated ones.
The ﬁrst user study (US1), where we collected human-
generated explanations, aimed to discuss Hp1; the second
user study (US2), where we compared human-generated and
EXPRES-generated explanations, aimed to discuss Hp2. We
deﬁned two PRES scenarios with different complexities by
considering 8 employees in the ﬁrst scenario and 20 em-
ployees in the second scenario, to be scheduled over a week.
In US1 we showed all the preferences and the entire team’s
weekly schedule; in US2 we showed only one individual set
of preferences and one individual weekly schedule.
User study 1
US1 consists of individual interviews with people (N=10)
who have previously actively interacted with the workforce
scheduling tool deployed at J.P. Morgan, i.e., team managers
or assistants who generated schedules through the tool. The
interviews took place virtually. After enquiring about the
participant’s familiarity with the tool (on a 5-point Likert
scale, avg=4.9), the two scenarios were fully disclosed. The
participants were asked to justify to one selected ﬁctitious

Figure 3: The ﬁgure on the left shows the number of explanations generated within 30 seconds as we increase the number of
agents. The ﬁgure in the middle shows the number of explanations we can compute depending on the unsatisﬁed preference
type in problems with 10 agents. The ﬁgure on the right shows different statistical measures over the distance between the
generated explanations. For each violin plot, the central horizontal line depicts the mean of the distribution, while the other two
lines depict the minimum and maximum values. A wider shadow indicates that more points have that value.
Scenario 1
Scenario 2
avg
std dev
EXPRES
avg
std dev
EXPRES
time (s)
195.9
80.8
≪1
186.3
96.2
0.8
difﬁculty
2.4
1.4
–
2.7
1.1
–
Table 2: Results of user study 1. Time is expressed in
seconds, difﬁculty is expressed on a 5-point Likert scale
(1=very easy, 5=very difﬁcult).
employee why their preference could not be satisﬁed in the
team’s schedule (e.g., “Why wasn’t Edith assigned to the
ofﬁce on Thursday 18th November, as she requested?”). In
the meeting chat, the participants wrote an explanation for
the unsatisﬁed preference and then evaluated the difﬁculty
of providing that explanation. We tracked the time that each
participant required to provide each an explanation.
Tab. 2 shows a quantitative overview of the results. All of
the participants spent considerably more time than EXPRES
to provide an explanation (Hp1 is conﬁrmed): despite sce-
nario 2 being more complex in terms of quantity and density
of information and being reported more challenging to ex-
plain (avg difﬁculty=2.7 vs 2.4 in scenario 1), 6 participants
were quicker to provide the second explanation than the ﬁrst
one. We suppose this could be due to the participants being
more familiar with the task by the time they faced the sec-
ond scenario. Regarding the quality of explanations, in each
scenario we gathered very different justiﬁcations (see Tab. 3
for a sample, and an extended version of this work4 for a
complete list of the collected explanations), sometimes more
explicit and detailed, sometimes more implicit and general.
However, no explanation can be considered complete, sound
or optimal, according to Defs. 9, 10 and 11.
User study 2
US2 consists of an online questionnaire with people (N=28)
who have passively interacted with the tool, i.e., employ-
ees whose schedule was generated by the tool, within J.P.
4https://arxiv.org/abs/2203.08895
Scenario 1
H4: Too many of the team members have requested to be in the
ofﬁce on a Thursday.
H5: Edith wasn’t assigned into the ofﬁce because she hadnt re-
quesred the dates in on the 18th or because since her minimum
was 1 and max was 2 and it maxed out the days she can come in
Scenario 2
H4: Due to being a large group and many of the employees
requesting the same day, not everyone could get their desired
day in the ofﬁce.
H5: 3 people had dates in, another 2 had 5 days min, the rest
are perhaps a combination of all the other factors, meaning they
had to be in instead of Ivan
H6: He was limited by the seat availability that day , members
of two other groups has selected the 1th speciﬁcally to be in so
they took preference
Table 3: Details of some of the human-generated explana-
tions selected from US1 and included in US2.
Morgan. Each participant was shown, sequentially, the pref-
erences and the schedule of one ﬁctitious employee (e.g.,
Edith’s) from the two scenarios previously deﬁned. For each
scenario, 6 explanations were listed in a random order: 3
were generated by EXPRES (E1-E3, see Tab. 4; explanations
are anonymised and parsed as shown at the end of “Return
to the Ofﬁce at J.P. Morgan”) and 3 were selected from the
human generated ones in US1 (H4-H6, see Tab. 3). When
selecting the explanations to include in this study, we aimed
to represent the diversity, in terms of structure and reasons
mentioned, of the pool of explanations that were available,
in order to explore the participants’ appreciation of different
combinations of reasons. In both scenarios, E1 is the opti-
mal explanation (cf. Def. 11). Each participant was asked to
ﬁrst select and then rank the three explanations that were the
most satisfying. We report the results in Tab. 5.
Participants showed a strong preference in both scenarios
for the EXPRES explanations, which were selected signiﬁ-
cantly (t-test with pvalue=0.01) more often than the human-
generated ones (Hp2 is conﬁrmed). Looking at the results
in greater detail, we see that E1 has been the most selected

Reason type
Scenario 1 (n=5 desks)
Scenario 2 (n=12 desks)
E1
E2
E3
E1
E2
E3
Rmin
3
1
1
3
0
3
Rmeet
1
0
2
2
0
0
Rgroup
1
4
1
7
12
8
Rpref
0
0
1
0
0
1
Table 4: Details of the reasons part of the explanations gen-
erated by EXPRES and included in US2. Reason type refers
to the preference-type mentioned as a justiﬁcation for the
unsatisﬁed preference.
E1
E2
E3
tot.E
H4
H5
H6
tot.H
s1
# selections
22
19
14
55
17
5
7
29
rank score
47
36
24
107
37
10
14
61
s2
# selections
23
17
11
51
14
8
11
33
rank score
55
28
17
100
34
12
22
68
Table 5: Results of US2. The rank score is 3x1 + 2x2 + x3
(xi is the n. of times an explanation has been ranked i).
explanation in both scenarios (by 78.6% and 82.1% of the
participants). We interpret this as a predilection for explana-
tions that are sound, complete and as consistent as possible
with the Principal’s total order of preferences. Regarding the
human-generated explanations, most people appreciated H4,
which in both scenarios was the most general and vague ex-
planation. This suggests that brief and simple explanations
can satisfy the general user who does not look for detailed
justiﬁcations.
Related Work
Explanations are essential for humans to understand the out-
puts and decisions made by AI systems (Core et al. 2006;
Miller 2019). There exist many works that provide expla-
nations for different AI use cases ranging from automated
planning (Fox, Long, and Magazzeni 2017; Chakraborti
et al. 2019) to machine learning (Carvalho, Pereira, and Car-
doso 2019) or deep learning (Samek, Wiegand, and M¨uller
2017). Explanations are also crucial in multi-agent environ-
ments where some extra challenges arise, such as privacy
preservation or fairness (Kraus et al. 2020). Scheduling of
multiple agents is one of these problems, and explaining the
resulting schedules is not a trivial task.
In (Agrawal, Yelamanchili, and Chien 2020), the authors
propose CROSSCHECK, a tool that (i) diagnoses scheduling
failures in the context of a Mars Rover mission, and (ii)
guides users about which constraints need to be altered in
order for the activity to be successfully scheduled. Their tool
focuses on visualisation and improving user experience with
the scheduler, but could hardly be adapted to provide expla-
nations for multiple users having competing preferences.
In (Zahedi, Sengupta, and Kambhampati 2020), the au-
thors propose AITA, a centralised Artiﬁcial Intelligence Task
Allocation that simulates a negotiation between the agents.
If unhappy with the recommended allocation, an agent
can question it using counterfactuals; these will be refuted
by AITA, which explains, using negotiation trees, how the
agent’s proposal would entail a worse-off allocation than the
recommended one. Despite not formally allowing counter-
factual queries, in this paper we still enable users to get ex-
planations (i) that are speciﬁcally targeted for preferences
that are unsatiﬁed in the recommended schedule, and (ii)
that contain more interpretable information (AITA refers to
the overall allocation cost, while we include other agents’
satisﬁed preferences). Finally, the authors discuss the length
of the explanations when 2-4 agents are involved in the
scheduling process but do not share any results on the scala-
bility of their approach (more agents and more tasks).
In (Cyras et al. 2019), the authors explain schedules us-
ing argumentation frameworks. They can explain (i) why a
solution is not feasible or suboptimal (we assume feasible
and optimal solutions are given); or (ii) why a preference
was not satisﬁed in the solution, as in our EXPRES prob-
lem formulation. In order to provide the explanations, they
need to manually generate the attack graphs, i.e., the rela-
tionship between the preferences and the assignments. This
is similar to the effort needed to deﬁne the rules inside our
INVOLVED and AFFECTED functions. A key difference be-
tween both works is that they are restricted to makespan
scheduling problems with a very limited number of prefer-
ences. EXPRES can be used to generate explanations in any
scheduling problem where there is a totally ordered set of
preferences. On the evaluation side, they do not report any
experiment. An interactive tool was presented ( ˇCyras, Lee,
and Letsios 2021), but its validation was left for future work.
Conclusion and Future Work
In this paper, we introduced the EXPRES framework, an ap-
proach to explain why a given preference was unsatisﬁed
in a schedule. We framed this problem as an optimisation
task that aims to ﬁnd the best set of reasons that can ex-
plain an unsatisﬁed preference, and we solved it using MILP
techniques. Then we showed how to group and translate the
raw explanations in order to be easily interpreted by hu-
mans as well as to preserve agents’ privacy. Experimental
results through simulations showed that EXPRES can efﬁ-
ciently scale. Finally, a set of user studies within J.P. Mor-
gan showed how employees interacting with a workforce
scheduling tool preferred our automatically-generated ex-
planations over human-generated ones.
Currently, we assume a totally ordered set of preferences
that is always respected in any optimal solution. This sim-
pliﬁes the deﬁnition of the INVOLVED and AFFECTED func-
tions, but that assumption does not hold in all scheduling
problems. We will explore how to treat problems where
only partial orders over the preferences exist, and how to
deﬁne reasons that justify unsatisﬁed preferences through a
chain of satisﬁed ones, without limiting to only one. Also,
we would like to investigate (i) whether providing explana-
tions improves the users’ satisfaction with their schedules
(Bradley and Sparks 2009), and (ii) how to learn the subjec-
tive preference order of a user and the type of explanations
they prefer, in order to generate even more tailored explana-
tions (Soni, Sreedharan, and Kambhampati 2021).

Acknowledgements
This paper was prepared for informational purposes in part
by the Artiﬁcial Intelligence Research group of JPMorgan
Chase & Co. and its afﬁliates (“JP Morgan”), and is not a
product of the Research Department of JP Morgan. JP Mor-
gan makes no representation and warranty whatsoever and
disclaims all liability, for the completeness, accuracy or re-
liability of the information contained herein. This document
is not intended as investment research or investment advice,
or a recommendation, offer or solicitation for the purchase
or sale of any security, ﬁnancial instrument, ﬁnancial prod-
uct or service, or to be used in any way for evaluating the
merits of participating in any transaction, and shall not con-
stitute a solicitation under any jurisdiction or to any person,
if such solicitation under such jurisdiction or to such person
would be unlawful.
References
Agrawal, J.; Yelamanchili, A.; and Chien, S. A. 2020. Using
Explainable Scheduling for the Mars 2020 Rover Mission.
CoRR, abs/2011.08733.
Bradley, G. L.; and Sparks, B. A. 2009. Dealing with Ser-
vice Failures: The Use of Explanations. Journal of Travel &
Tourism Marketing, 26(2): 129–143.
Carvalho, D. V.; Pereira, E. M.; and Cardoso, J. S. 2019. Ma-
chine Learning Interpretability: A Survey on Methods and
Metrics. Electronics, 8(8): 832.
Chakraborti, T.; Kulkarni, A.; Sreedharan, S.; Smith, D. E.;
and Kambhampati, S. 2019. Explicability? Legibility? Pre-
dictability? Transparency? Privacy? Security? The Emerg-
ing Landscape of Interpretable Agent Behavior. In Proceed-
ings of the Twenty-Ninth International Conference on Auto-
mated Planning and Scheduling (ICAPS’19), Berkeley, CA,
USA, July 11-15, 2019, 86–96. AAAI Press.
Core, M. G.; Lane, H. C.; van Lent, M.; Gomboc, D.;
Solomon, S.; and Rosenberg, M. 2006. Building Explain-
able Artiﬁcial Intelligence Systems.
In Proceedings of
the Twenty-First AAAI Conference on Artiﬁcial Intelligence
(AAAI’06) July 16-20, 2006, Boston, Massachusetts, USA,
1766–1773. AAAI Press.
ˇCyras, K.; Lee, M.; and Letsios, D. 2021.
Schedule Ex-
plainer: An Argumentation-Supported Tool for Interactive
Explanations in Makespan Scheduling.
In International
Workshop on Explainable, Transparent Autonomous Agents
and Multi-Agent Systems, 243–259. Springer.
Cyras, K.; Letsios, D.; Misener, R.; and Toni, F. 2019. Ar-
gumentation for Explainable Scheduling. In Proceedings of
the Thirty-Third AAAI Conference on Artiﬁcial Intelligence,
AAAI’19, Honolulu, Hawaii, USA, January 27 - February 1,
2019, 2752–2759. AAAI Press.
Fox, M.; Long, D.; and Magazzeni, D. 2017. Explainable
Planning. CoRR, abs/1709.10256.
Gunawan, A.; Ng, K. M.; and Poh, K. L. 2007. Solving the
Teacher Assignment-course Scheduling Problem by a Hy-
brid Algorithm. International Journal of Computer, Infor-
mation, and Systems Science, and Engineering, 1(2): 136.
Kraus, S.; Azaria, A.; Fiosina, J.; Greve, M.; Hazon, N.;
Kolbe, L.; Lembcke, T.-B.; Muller, J. P.; Schleibaum, S.; and
Vollrath, M. 2020. AI for Explaining Decisions in Multi-
agent Environments. In Proceedings of the Thirty-Fourth
AAAI Conference on Artiﬁcial Intelligence (AAAI’20), vol-
ume 34, 13534–13538.
Legrain, A.; Bouarab, H.; and Lahrichi, N. 2015. The Nurse
Scheduling Problem in Real-life. Journal of medical sys-
tems, 39(1): 1–11.
Miller, T. 2019. Explanation in Artiﬁcial Intelligence: In-
sights from the Social Sciences. Artif. Intell., 267: 1–38.
Samek, W.; Wiegand, T.; and M¨uller, K.-R. 2017.
Ex-
plainable Artiﬁcial Intelligence: Understanding, Visualizing
and Interpreting Deep Learning Models.
arXiv preprint
arXiv:1708.08296.
Soni, U.; Sreedharan, S.; and Kambhampati, S. 2021. Not
all Users are the Same: Providing Personalized Explana-
tions for Sequential Decision Making Problems.
CoRR,
abs/2106.12207.
Watson, J.; Beck, J. C.; Howe, A. E.; and Whitley, L. D.
2003.
Problem Difﬁculty for Tabu Search in Job-shop
Scheduling. Artif. Intell., 143(2): 189–217.
Zahedi, Z.; Sengupta, S.; and Kambhampati, S. 2020. ’Why
not give this work to them?’ Explaining AI-Moderated
Task-Allocation Outcomes using Negotiation Trees. CoRR,
abs/2002.01640.

