AGI “Understanding” Survey Report
Mike Archbold
Northwest AGI Forum
October 2021
SUMMARY
Use of the word “understanding” is common in AI, as in “our AI understands x.”
This type of statement can be both positive and negative, as in “our AI understands how to x”
yet “our AI does not understand y.”
A problem is that it’s quite possible that the claimant of such machine “understanding” has no
rigorous definition of understanding, and perhaps has no definition at all, making it challenging
for a listener to judge what is being claimed, particularly if no such definition is forthcoming --
and it usually isn’t. Making matters worse is that defining human understanding itself is difficult,
let alone how a machine “understands”! The result is a confused conflation in need of attention.
It seems clear that there is no single definition of understanding. A significant problem is that a
machine’s understanding seems to be crucial, since whether or not any machine purporting to
have AI can be trusted to complete a given task depends upon its understanding of the task in
question.
Given this state of affairs, a series on machine understanding definitions in two parts was
envisioned by the Northwest AGI Forum. The first part was a survey:  members and others in
the AGI community at large (mostly independent developers) were asked for their definition of
understanding, and also a few definitions of understanding were gathered from existing sources.
The results were compiled and published, available here →
https://drive.google.com/file/d/1XfRZudUomqTuuSmMQDBhlcTmZRrezBl2/view?usp=sharing
The second part of the series on understanding was originally envisioned as a presentation and
discussion of formal accounts, however due to the paucity of applicable formal material, and the
fact that such formal accounts are not materially different than the results of the survey
(although better organized and further developed) it was decided just to integrate the community
survey along with any formal accounts available, circulate for review, and publish the results in
this white paper, and reach any such conclusions deemed warranted.
The following sections discuss 1) the issues that were commonly found in the constellation of
definitions, 2) a summary of formal accounts, and 3) conclusions.

ISSUES
The results:
There is no universally accepted definition of understanding.
Certain issues, some of which are described below, seem to appear
repeatedly.
1) LEVELS
Most people seem to agree that there are grades of understanding, from the simplest,
eg., prediction, to the higher levels, eg., the ability to explain in detail. This “levels” issue
is arguably the most widely held, seemingly implicit therefore in our understanding of
understanding. This is mostly what makes claims of AI “understanding” inherently
troublesome, since it is usually not clear what level(s) is defined and claimed. If you get
nothing else from reading this paper, remember it’s best to try to qualify what you mean
by understanding -- that is basically the same as identifying its level. Otherwise you are
vague.
2) COMPRESSION, RECOGNITION, AND PREDICTION BASED ON PATTERNS
Proponents of the algorithmic information theory school of intelligence (AIT) and
proponents of the explosion of what can be labeled generally as “patternism” within the
last ten years tend to favor a minimalist definition of understanding.  They limit machine
understanding within the boundaries of the facility of pattern recognition. Advocates
basically claim that mere prediction alone, along with the ability to act appropriately given
correct recognition, is sufficient to claim adequate machine understanding.
3) EXPLANATION
Most people seem to require that a machine can be said to understand if it possesses
some means of explanation.
4) UNDERSTANDING (or not) WITHOUT BEING ABLE TO EXPLAIN
Whereas explanation is a sign of machine understanding, humans are often able to act
without actually understanding. As in drama, they may follow a script without knowledge
of the underlying causes. Secondly, we may be able to understand performing activities
but simply lack a means of adequate explanation to others. It seems then that the
explanation ability is not always needed.
5) SENSORY INPUT CRUCIAL OR NOT?

Some argue that without actually being embedded in the world it is impossible to
understand it. For example, the world of text alone would never pass for sensing the
human-experienced world of sights, sounds, tastes. Others argue from a “brain in a vat”
standpoint, that understanding can be achieved anyway, eg., by text alone.
6) CAPACITY TO APPLY UNDERSTOOD KNOWLEDGE TO REACH GOALS
Most seem to agree that a machine can be said to understand if it can apply its
knowledge to reach its goals. Mere knowledge alone is worthless, then, if it isn’t actually
understood. Knowledge can also be aligned with common sense. Common sense
without the understanding of it is worthless. In simple terms there is 1) the data the
machine has and 2) its ability to use the data, ie., to understand it. The true test of
understanding is whether or not new goals can be met using understood, existing
knowledge.
7) UNDERSTANDING REQUIRES A MODEL
Most people seem to argue that a machine requires an inner model that effectively
represents what is the case outside of it in order to understand. The internal model
should capture the structure of what it represents, eg:  relationships, causes, spatial and
temporal properties. The model might be a fairly static world-level view, or it may be
generated dynamically, applicable to a specific domain.
8) UNDERSTANDING AS A PROCESS REACHING A STATE
Some think that understanding is a constant process proceeding in time. The machine
attempts to understand by harmonizing its inner model with the state of the external
world, with the problem at hand. An “aha” effect is possible. Truth is implicated: the
machine can decide truth or falsity if the inner model and outer circumstances align. If
the inner and outer don’t harmonize well enough, then the machine “doesn’t
understand.”
9) CAUSE AND EFFECT
The sensitivity to causality in a machine is one of the most cited signs of understanding.
To a great extent this seems tied to the current frustration in that AI is often limited to
mere correlation. Most people seem to think that the ability to distinguish cause from
correlation is needed in understanding. (Lately there appears to be increased interest in
modelling causality.)
10) REPHRASING, RECREATING, TEACHING, etc
Some claim that a test of understanding is the ability to apply knowledge in sophisticated
ways, such as having the ability to use what is understood to recreate, teach, apply,
manage, modify etc.
11) RECOGNIZING WHAT IS RELEVANT AND USEFUL IN CONTEXT TO REACH
GOALS/MISSION/MOTIVATION

Some claim that it takes an “I” to “understand.” This seems to follow from common sense
since humans have a mental structure which so heavily is dependent upon the self. The
self experiences motivations and pursues its good and avoids the bad. Understanding in
this sense means that the I/self needs to ascertain what is important in context to reach
goals.
12) UNDERSTANDING INCLUDES A FEELING
Some claim that understanding is ineffable and will not yield easily to definitions and
implementation in a machine, if at all. We also have feelings about feelings and other
complexities related to understanding that appear difficult to mimic.
13) NIHILISTS
Some believe there is no such thing as understanding at all, no “I” in a human nor
machine, and variations of this view. This might be a sensible approach, since it’s a valid
question how a machine could actually understand anything ever:  it’s just a machine
after all.

FORMAL TREATMENT OF UNDERSTANDING
The works of Kristinn Thorisson et al are most applicable, easily readable, and recommended.
They (Thorisson and colleagues) repeatedly cite four levels in order of difficulty as a measure of
understanding:
1) prediction (crude understanding),
2) utilization of understanding to reach goals,
3) explanation ability,
4) ability to recreate.
In the paper titled “About Understanding” (2016) they state “no concrete theory of understanding
has been fielded as of yet in artificial intelligence (AI), and references on this subject are far
from abundant in the research literature.”  They write:
While understanding as a phenomenon has received more attention in the philo-
sophical than the AI literature [8,7], even there it has nevertheless been claimed to
have “virtually escaped investigation in English-speaking philosophy” ([5]: 307); this
dearth of interest in the subject is evidenced not only there but also in the fields of AI and
cognitive science.4 A few books have been published with the word pair “understanding
understanding” in the title [15,4]. Interesting as they may be, one of these contains selected
writings by cybernetics pioneer Heinz von Forrester, which, in spite of its promising title, is not
about understanding at all (as evidenced by the word “understanding” not appearing the index);
the other gives a cursory (albeit a decent) summary of the subject in the context of
epistemological philosophy.
From their paper “Do Machines Understand? A Short Review of Understanding & Common
Sense” (emphasis mine):
(in AI) “For the most part, understanding has simply been ignored, or discussed in a very
specific sense within particular domains. If mentioned at all in the AI literature, understanding is
generally discussed within the context of one specific domain, such as language or scene
understanding, or equated with common sense.”
In spite of this state of neglect they nevertheless regard understanding as crucial (from
“Evaluating Understanding” emphasis mine):
While results in automation can be achieved without it, we argue that understanding is
especially important for general-purpose systems. It is doubtful that we could classify systems
as having general intelligence if they don’t really understand their tasks, environment, and
world, and thus it is important for us to verify the level of understanding of any system intended
to strive for generality and autonomy.

Thorisson et al’s goal (as gathered from his papers and talks) is to present a useful and
pragmatic theory of understanding.
Basically, his theory rests on the accuracy of the relationship a machine can build between the
phenomena it experiences and its internal models. The more a machine’s models are complete
and accurate with respect to its phenomena and context (basically the ~world) the more a
machine “understands.” He also proposes a means of formalizing meaning given context:
depending upon context something may be more or less meaningful to the machine. I refer the
reader to these papers for more details.

CONCLUSIONS
I recommend not saying things like “my AI understands X” without qualification. It’s probably
best to follow a variable form like this:
“My AI understands X <specify one of the levels listed above> but cannot understand Y
<specify the levels above not applicable>”
For example: developer Z should say “my AI can understand how to predict stock price changes
but the AI cannot explain its reasoning” rather than a grandiose claim like “my AI understands
the stock market.”
REFERENCES
Archive of the “Understanding Survey” event →
https://drive.google.com/drive/folders/1_96Noo_s4BpES2K88etHpY4-aAqEusgt?usp=sharing
Thorisson et al papers →http://alumni.media.mit.edu/~kris/select_publ.html
Video of event https://www.youtube.com/watch?v=MbDCqQ0oFyY

