PRIORS FOR BAYESIAN SHRINKAGE AND HIGH-DIMENSIONAL MODEL
SELECTION
A Dissertation
by
MINSUK SHIN
Submitted to the Ofﬁce of Graduate and Professional Studies of
Texas A&M University
in partial fulﬁllment of the requirements for the degree of
DOCTOR OF PHILOSOPHY
Chair of Committee,
Valen E. Johnson
Co-Chair of Committee
Anirban Bhattacharya
Committee Members,
Jianhua Huang
Byung-Jun Yoon
Head of Department,
Valen E. Johnson
August 2017
Major Subject: Statistics
Copyright 2017 Minsuk Shin

ABSTRACT
This dissertation focuses on the choice of priors in Bayesian model selection and their
applied, theoretical and computational aspects. As George Box famously said “all models are
wrong, but some are useful"; many statisticians and scientists are aware of the importance of
model selection. In a Bayesian perspective, however, it is challenging to choose the prior on the
parameters involved in model selection or how to evaluate the criterion on the prior, especially
when the number of models to be compared is massive or when a nonparametric model is
considered.
For high-dimensional Bayesian model selection for linear models, my dissertation studies
theoretical perspectives of the choice of the prior on the regression coefﬁcient. Especially, I
consider the nonlocal prior densities that assign zero density around the null value, which is
typically 0 in model selection settings. When certain regularity conditions apply, I demonstrate
that the model selection procedure based on the nonlocal priors is consistent for linear models
even when the number of covariates p increases sub-exponentially with the sample size n. I
investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and
show that it attains a unique penalty term that adapts to the strength of signal corresponding
variable in the model, and I remark that this term cannot be attained from local priors such as
Gaussian prior densities.
Another topic of my dissertation is about computational aspects of Bayesian model selec-
tion under high-dimensional settings. A full posterior sampling using existing Markov chain
Monte Carlo (MCMC) algorithms to explore high-dimensional model space is highly inefﬁ-
cient and often not feasible from a practical perspective. To overcome this issue, I propose a
scalable stochastic search algorithm called Simpliﬁed Shotgun Stochastic Search with Screen-
ing (S5), which efﬁciently explores the model space. The S5 algorithm dramatically reduces
ii

the computational burden to search the neighborhood of a model by considering a screening
step within the algorithm. Its empirical performance is examined in several examples, and it
outperforms existing algorithms in the sense that S5 is computationally fast while it efﬁciently
searches the model space. S5 is used to implement the model selection procedures introduced
in this dissertation, including linear and nonparametric model selection. The computing func-
tions are provided in the R package BayesS5 in CRAN (https://cran.r-project.org).
For nonparametric regression models, I introduce a new shrinkage prior on function spaces,
the functional horseshoe prior, that encourages shrinkage towards parametric classes of func-
tions. When the true underlying function is in the parametric class, improved estimation per-
formance is obtained relative to classical nonparametric procedures. The proposed prior also
provides a natural penalization interpretation, and casts light on a new class of penalized like-
lihood methods for function estimation. I theoretically exhibit the efﬁcacy of the proposed
approach by showing an adaptive posterior concentration property.
The last topic of the dissertation is about a novel extension of the nonlocal idea to functional
spaces, called the nonlocal functional prior, which is suitable for nonparametric Bayesian hy-
pothesis testing (model selection) problems. I illustrate the asymptotic rate of the Bayes factor
deﬁned by the proposed prior for nonparametric hypothesis testing problems. I apply the pro-
posed prior densities for high-dimensional model selection of nonparametric additive models,
and investigate the model selection consistency of the resulting model selection procedure. I
provide some simulation studies and real data examples that show that the proposed model
selection procedure outperforms state-of-the-art methods in ﬁnite samples.
iii

DEDICATION
To my lovely wife Mirae, my adorable daughter Jane and son Sungmin (Daniel).
iv

ACKNOWLEDGMENTS
First of all, I would like to express my deep gratitude to my advisors Valen E. Johnson
and Anirban Bhattacharya. Because of their thoughtful advice, I was able to complete my
dissertation, and they helped me a lot to grow academically. Val taught me the right attitude and
passion towards science and he has alway given me a piece of warm hearted advice. Anirban
is not only a good friend, but he is also a good partner to discuss ideas. Val and Anirban, I
have been incredibly inspired by you, and no word of thanks is enough for your wonderful
mentorship.
I would also like to extend my gratitude to the members of my dissertation committee, Dr.
Jianhua Huang and Dr. Byung-Jun Yoon for their support and encouragement. Thanks also
to Dr. Irina Gaynanova and Dr. Xianyang Zhang for giving me helpful advice to apply for
academic jobs and to improve my presentation skills. Dr. David Rossell has been extremely
encouraging and I have thoroughly enjoyed the discussions with him. It has been a great
pleasure working with Dr. Naveen N. Narisetty, even though our work has been delayed so
long. I believe that we can ﬁnish the project soon. Amir Nikooienejad is also a good friend
of mine, and I really appreciate his constant encouragement. I am also grateful to my friend
Sangyoon Yi for buying me Starbucks coffee many times.
Dr. Ersen Arseven, who has worked in the clinical trial industry for 30 years, has been a
good mentor. He gave me a lot of supports and warm-hearted advice. In particular, his advice
for presentation and job interview was really useful and practical.
Finally, I devote my dissertation to my family: my beloved wife Mirae, my daughter Jane,
my son Sungmin (Daniel), my parents and my parents-in-law. Especially, my mother Eulsun
Kim and my mother-in-law Soonja Lee crossed the Paciﬁc Ocean from South Korea and came
to America to help us when my daughter and son were newborn. I would also like to thank my
v

grandmother and grandfather for their dedicated support. Without supports from my family,
none of this work would have been possible. I love you!
vi

CONTRIBUTORS AND FUNDING SOURCES
Contributors
This work was supported by a dissertation committee consisting of Dr. Valen E. Johnson
(co-advisor), Dr. Anirban Bhattacharya (co-advisor) and Dr. Jianhua Huang of the Department
of Statistics and Dr. Byung-Jun Yoon of the Department of Electrical Engineering.
All other work conducted for the dissertation was completed by the student independently.
Funding Sources
Graduate study was supported by a teaching assistantship from Texas A&M University and
from the National Institute of Health (NIH) R01 CA.
vii

TABLE OF CONTENTS
Page
ABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ii
DEDICATION
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
ACKNOWLEDGMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
CONTRIBUTORS AND FUNDING SOURCES
. . . . . . . . . . . . . . . . . . . . .
vii
TABLE OF CONTENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii
LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii
LIST OF TABLES  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
A Brief Review of Bayesian Model Selection
. . . . . . . . . . . . . . . . . .
1
1.1.1
Bayesian Model Selection for the Linear Regression Model . . . . . . .
3
1.1.2
Bayesian Model Selection in the Nonparametric Regression
. . . . . .
5
1.2
Research Challenges and Main Contributions
. . . . . . . . . . . . . . . . . .
9
1.2.1
Linear Model Selection in High-dimensional Settings . . . . . . . . . .
9
1.2.2
Nonparametric Model Selection in High-dimensional Settings
. . . . .
13
1.3
Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2. NONLOCAL PRIOR DENSITIES FOR HIGH-DIMENSIONAL LINEAR MODEL
SELECTION
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Nonlocal Prior Densities for Regression Coefﬁcients
. . . . . . . . . . . . . .
21
2.3
Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.3.1
Simulation Studies Using Precision-Recall Curves
. . . . . . . . . . .
24
2.3.2
Further Comparison with Zellner’s g-prior . . . . . . . . . . . . . . . .
27
viii

2.4
Model Selection Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.5
Connections Between Nonlocal Priors and Reciprocal Lasso
. . . . . . . . . .
33
2.6
An Adaptive Form of Asymptotic Marginal Likelihoods Based on Nonlocal
Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.7
Real Data Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.7.1
Analysis of Polymerase Chain Reaction (PCR) data . . . . . . . . . . .
36
2.7.2
A Simulation Study Based on the Boston Housing Data . . . . . . . . .
39
2.8
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3. SIMPLIFIED SHOTGUN STOCHASTIC SEARCH WITH SCREENING ALGO-
RITHM FOR HIGH-DIMENSIONAL BAYESIAN MODEL SELECTION
. . . . .
44
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
Shotgun Stochastic Search Algorithm (SSS) . . . . . . . . . . . . . . . . . . .
44
3.3
Simpliﬁed Shotgun Stochastic Search Algorithm with Screening (S5) . . . . . .
45
3.4
Performance Comparisons Between S5 and SSS . . . . . . . . . . . . . . . . .
47
3.4.1
Application to Real Data Examples
. . . . . . . . . . . . . . . . . . .
49
3.5
R Package: BayesS5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.5.1
S5 Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.5.2
S5_parallel Function for Parallel Computing Environments . . . .
55
4. FUNCTIONAL HORSESHOE PRIOR FOR NONPARAMETRIC SUBSPACE 
AGE  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.3
Functional Horseshoe Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.3.1
Posterior Concentration Rate . . . . . . . . . . . . . . . . . . . . . . .
65
4.4
Simulation Studies for Univariate Examples . . . . . . . . . . . . . . . . . . .
68
4.5
Applications to Additive Models . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.5.1
A Comparison to the Standard Horseshoe Prior . . . . . . . . . . . . .
74
4.5.2
Simulation Studies
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.5.3
Real Data Analysis: Boston Housing Data and Ozone Data . . . . . . .
79
4.6
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
ix
SHRINK

5. NONLOCAL FUNCTIONAL PRIORS FOR NONPARAMETRIC HYPOTHESIS
TESTING AND HIGH-DIMENSIONAL MODEL SELECTION . . . . . . . . . . .
83
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
5.2
Bayesian Nonparametric Hypothesis Testing Procedures
. . . . . . . . . . . .
86
5.3
Convergence Rates of Bayes Factor . . . . . . . . . . . . . . . . . . . . . . . .
90
5.3.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
5.3.2
Local Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
5.3.3
Moment Functional Prior Densities . . . . . . . . . . . . . . . . . . . .
92
5.3.4
Inverse Moment Functional Prior Densities
. . . . . . . . . . . . . . .
94
5.3.5
The Choice of Kn . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.4
Examples of Bayesian Hypothesis Tests Using Nonlocal Functional Priors . . .
95
5.5
Nonparametric Additive Model Selection Using Nonlocal Functional Priors . .
99
5.5.1
Additive Model Selection Consistency for High-dimensional Settings
. 102
5.5.2
Asymptotic Rates of Marginal Likelihood for Additive Models . . . . . 104
5.5.3
Computational Strategy Using S5
. . . . . . . . . . . . . . . . . . . . 106
5.5.4
Simulation Studies
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.5.5
Practical Selection of Hyperparameter Values . . . . . . . . . . . . . . 114
5.6
Applications to Real Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.6.1
Bardet-Biedl Syndrome Gene Expression Data
. . . . . . . . . . . . . 116
5.6.2
Near Infrared Spectroscopy Data . . . . . . . . . . . . . . . . . . . . . 116
5.6.3
Technical Details and Results . . . . . . . . . . . . . . . . . . . . . . . 116
5.7
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
APPENDIX A. PROOFS OF THEORETICAL RESULTS . . . . . . . . . . . . . . . . 130
A.1 Nonlocal Prior Densities for High-dimensional Linear Model Selection . . . . . 130
A.2 Functional Horseshoe Prior for Nonparametric Subspace Shrinkage . . . . . . . 149
A.3 Nonlocal 
Functional 
Priors 
for 
Nonparametric 
Hypothesis 
Testing 
and High-dimensional Model Selection  . . . . . . . . . . . . . . . . . . . . . 160
APPENDIX B. DETAILS OF COMPUTATION
. . . . . . . . . . . . . . . . . . . . . 176
x

B.1
Nonlocal Prior Densities for High-dimensional Linear Model Selection . . . . . 176
B.2
Functional Horseshoe Prior for Nonparametric Subspace Shrinkage . . . . . . . 177
B.3 Nonlocal 
Functional 
Priors 
for 
Nonparametric 
Hypothesis 
Testing 
and High-dimensional Model Selection  . . . . . . . . . . . . . . . . . . . . . 179
B.3.1
Modiﬁed Simpliﬁed Shotgun Stochastic Search with Screening (S5) for
Additive Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
B.3.2
Laplace Approximations of Marginal Likelihoods Based on Nonlocal
Functional Prior Densities
. . . . . . . . . . . . . . . . . . . . . . . . 180
xi

LIST OF FIGURES
2.1
Nonlocal prior density functions for a single regression coefﬁcient
. . . .
22
2.2
Plot of the mean precision-recall curves over 100 datasets.
. . . . . . . . . . .
28
2.3
Averaged posterior true model probability and the number of models which
attain the posterior odds ratio, with respect to the maximum a posteriori model,
larger than 0.001. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.1
Performance comparison between S5 and SSS. (a) Average computation time
to ﬁrst ﬁnd the MAP model; (b) Average number of models searched before
hitting the MAP model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2
Correlation between the top 10 posterior model probabilities estimated from
SSS and S5 with different screening set sizes. . . . . . . . . . . . . . . . . . .
49
3.3
Marginal inclusion probabilities approximated by S5 for the synthesized Boston
housing data set.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.1
A description of the prior density of !. The ﬁrst two columns illustrate the
prior density function of ! with different hyperparameters (a, b)
. . . . . . . .
64
4.2
Examples when the underlying true functions are parametric. . . . . . . . . . .
71
4.3
Examples when the underlying true functions are nonparametric. . . . . . . . .
72
4.4
Performance comparison with simulated data sets. . . . . . . . . . . . . . . . .
78
5.1
The convergence rate of Bayes factor under a true null.
. . . . . . . . . . . . .
97
5.2
Performance of additive model selection (1): the results of Scenario 1 and
Scenario 2.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.3
Performance of additive model selection (2): the results of Scenario 3 and
Scenario 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
5.4
A description of the hyperparameter selection procedure. The black line and
the blue line are the density functions of the null and the prior distribution of
F T(I −Q0)F/bσ2, respectively, for a given ⌧n and bσ2
. . . . . . . . . . . . . . 115
xii
ate
-
sedan

LIST OF TABLES
2.1
Optimal hyperparameters for Bayesian model selection methods
. . . . . . . .
30
2.2
Analysis of the PCR data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.3
The Boston Housing data set. . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.1
Comparisons between S5 and SSS using the Bardet-Biedl syndrome data and
the Boston housing data.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
4.1
Results of univariate examples . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.2
Results of real data examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.1
The convergence rate of Bayes factor under true alternative hypotheses.
. . . .
98
5.2
Optimal MSE and MSPE of each method for the considered settings.
. . . . . 110
5.3
Real data examples for additive model selection. . . . . . . . . . . . . . . . . . 117
xiii
-
.
theorem
-

1.
INTRODUCTION
1.1
A Brief Review of Bayesian Model Selection
Suppose that a set of H models M = {M1, . . . , MH} is considered for observed data y.
Under a model Mh 2 M, the density function of y is L(y | ✓h, Mh), where ✓h is a vector of
unknown parameters under model Mh. For Bayesian inference, priors should be fully speciﬁed
by assigning a prior distribution ⇡(✓m | Mh) to the parameters of each model and a model prior
⇡(Mh) to each model. The posterior probability of model Mh conditionally on the observed y
can be expressed as
⇡(Mh | y) =
mMh(y)⇡(Mh)
P
h0 mMh0(y)⇡(Mh0),
(1.1)
where
mMh(y) =
Z
L(y | ✓h, Mh)⇡(✓h | Mh)d✓h
is the marginal likelihood of a model Mh. Based on these posterior model probabilities, pair-
wise comparison of models M1 and M2 is conducted by the posterior odds that can be expressed
as a product between the ratio of marginal likelihoods and the model prior odds; i.e.,
⇡(M1 | y)
⇡(M2 | y) = mM1(y)
mM2(y) ⇥⇡(M1)
⇡(M2).
In particular, the ratio of marginal likelihoods mM1(y)/mM2(y) is called the Bayes factor
and it determines the decision rules for Bayesian hypothesis testing problems as discussed in
Kass and Raftery (1995) and Jeffreys (1961). The higher the Bayes factor value supports, the
more evidence in favor of M1. In Kass and Raftery (1995), a rough descriptive statement of
1

some decision rules regarding Bayes factors was empirically addressed as
log BF10
Evidence against H0
0 to 1
Not worth more than a bare mention
1 to 3
Positive
3 to 5
Strong
> 5
Very strong.
More discussions and empirical examples regarding Bayes factor are provided in Kass and
Raftery (1995).
Throughout this dissertation, I assume that one of considered models is the true model that
represents the data-generating process, which is a setting called the “M-closed" framework as
proposed in Bernardo and Smith (1994). This in itself is somewhat controversial, because the
true model might not exist or it might not be one of those under consideration. However, it is
a helpful viewpoint for at least thinking through the consequences of a true Bayesian model
selection procedure and desirable qualities.
I now introduce a desirable asymptotic property for Bayesian model selection procedures
called model selection consistency that can be deﬁned as follows.
Deﬁnition 1. (Model Selection Consistency) Suppose that t is the true data-generating model.
Then, if
⇡(t | y)
p! 1,
as n ! 1, the Bayesian model selection procedure is called “consistent".
From a theoretical point of view, when the number of models is ﬁxed regardless of the
sample size n, Schwarz (1978) showed that the model selection procedures deﬁned by some
general classes of priors; e.g. Gaussian priors achieve the model selection consistency. How-
2

ever, when I allow the number of models to increase at a certain rate of n, the asymptotic
behavior of the posterior probability of the true model is not clear. This situation is commonly
faced in high-dimensional variable selection problems for regression models due to the fact
that the total number of models for variable selection is 2p, where p(≫n) is the total number
of variables .
For the Bayesian framework, the uncertainty of the model space can be represented by
the posterior model distribution ⇡(M1 | y), . . . , ⇡(MH | y). By considering ⇡(Mh | y) as
a measure of the "truth" of model Mh, a natural strategy for model selection is to choose the
model that attains the largest posterior model probability. This model is called the maximum
a posteriori (MAP) model, i.e. c
M MAP = argmaxh ⇡(Mh | y). These posterior probabilities
are also important for full posterior inference in prediction using Bayesian model averaging
(Raftery et al., 1997), which is quantiﬁed by the posterior predictive distribution as p(ypred |
y) = P
h0 ⇡(ypred | Mh0, y)⇡(Mh0 | y) for a future observation ypred.
1.1.1
Bayesian Model Selection for the Linear Regression Model
Consider the standard setup of a Gaussian linear regression model with a univariate re-
sponse and p candidate predictors. Let y = {y1, . . . , yn}T denote a vector of responses for n
individuals and X an n ⇥p matrix of covariates. Let β = {β1, . . . , βp}T denote the regression
coefﬁcients. The linear regression model for the data is given by
y = Xβ + ✏,
(1.2)
where ✏⇠Nn(0, σ2In). However, in high-dimensional settings (n ⌧p), the unique MLE does
not exist and a MLE fails to achieve consistency of estimation. To overcome this issue, from
a Bayesian perspective, one can consider a sparsity inducing prior (Castillo et al., 2015) that
restricts the size of a given model k and puts zero prior probability on other parameters that are
3

not in model k. More precisely, for a given model k, the prior is
⇡(β | k) / ⇡k(βk)δ0(βkc)
(1.3)
where the term δ0(βkc) implies the coordinates βkc = {βj : j 2 kc} being zero and ⇡k(βk)
is a prior on the nonzero regression coefﬁcients βk = {βj : j 2 k}. This class of priors
includes many instances such as Zellner’s g-prior (Zellner, 1986), mixtures of g-priors (Liang
et al., 2008) and discrete mixtures of spike and slab priors (Ishwaran and Rao, 2005). With
a slight abuse of the notation, I denote the prior on the model space as ⇡(k) for a model k.
By following the deﬁnition of the posterior model probability in (1.1), the resulting posterior
probability of model k is deﬁned as
⇡(k | y) =
mk(y)⇡(k)
P
l ml(y)⇡(l),
where the marginal likelihood of a model k is given by
mk(y) =
Z
L(y | β, k)⇡(β | k)dβ,
for the likelihood function for model k, L(y | β, k). More practically, when σ2 is unknown,
a prior on σ2 can be deployed and the corresponding marginal likelihood can be deﬁned by
integrating with respect to the prior on σ2. The posterior model probability can be used to select
variables that are associated with the response. The simplest approach to the best model is to
consider the MAP model that maximizes the posterior model probability. An other option is to
utilize the marginal inclusion probabilities {qj : j = 1, . . . , p}, where qj = P
k:j2k ⇡(k | y) for
j = 1, . . . , p. The median probability model is deﬁned as the set of variables whose marginal
inclusion probability is larger than 0.5. Barbieri and Berger (2004) showed that the median
probability model is optimal in a predictive sense when only a single model is considered .
4

Some desirable theoretical properties of the posterior inference with some choices of the
prior speciﬁcation (1.3) have been discussed in high-dimensional settings. Johnson and Rossell
(2010) proposed a class of prior densities called nonlocal prior densities for ⇡k(βk | k). Nonlo-
cal prior densities are density functions that are identically zero whenever a model parameter is
equal to its null value, which is typically 0 in model selection settings. More formal deﬁnition
is as follows:
Deﬁnition 2. Suppose that ✓is a parameter supported in ⇥and ✓0 is the null value. Consider
a hypothesis test H0 : ✓= ✓0
versus
H1 : ✓6= ✓0. Under the alternative hypothesis, a
prior density ⇡is nonlocal, if for every ✏> 0, there is δ > 0 such that ⇡(✓) < ✏for all ✓2 ⇥
such that |✓−✓0| < δ.
Conversely, local prior densities are positive at the null parameter value. In Johnson and
Rossell (2012), it was shown that Bayesian model selection procedures based on nonlocal
priors achieve model selection consistency when p = O(n). However, when p increases at
a sub-exponential rate of n, i.e. log p = O(nc) for some 0 c < 1, its posterior model
consistency has not been derived.
Also, under increasing p at a sub-exponential rate of n, Narisetty and He (2014) investigated
the asymptotic behavior of model selection procedure based on a Gaussian prior with diverging
variance as n grows. Castillo et al. (2015) discussed some general conditions on priors on
the coefﬁcient and models for the optimal rate of posterior contraction and model selection
consistency. All priors considered in the literature were local priors.
1.1.2
Bayesian Model Selection in the Nonparametric Regression
Consider a simple nonparametric regression model deﬁned according to a response y =
{y1, . . . , yn} and a univariate predictor X = {x1 . . . , xn}
y = F + ✏,
(1.4)
5

where ✏⇠N(0, σ2In) and F = {f(x1), . . . , f(xn)} with the unknown regression function f.
From a practical point of view, a practitioner should decide whether the shape of f is speciﬁed
by a parametric from such as linear or quadratic function, or a nonparametric representation
using splines, wavelet, Gaussian processes etc. This argument can be formalized as a model
selection problem by writing
H0 : F 2 L
versus
H1 : F 62 L,
(1.5)
where L is a class of the parametric functions that are speciﬁed in advance. For example, if a
practitioner is interested in whether F is linear or not, L can be deﬁned as L = {β0 + β1X :
β0, β1 2 R}. Under H0, the resulting model is simply a univariate linear regression model.
Under H1, one can model the unknown function f as spanned by a set of pre-speciﬁed basis
functions {φj}1jKn, where Kn is the number of basis functions, as follows:
f(x) =
Kn
X
k=1
βkφk(x).
(1.6)
I shall work with the B-spline basis (De Boor, 1978) in the sequel, although the method-
ology generalizes to a larger class of basis functions. The B-splines basis functions can be
constructed in a recursive way. Let the positive integer q denote the degree of the B-spline
basis functions satisfying Kn > q + 1. Without loss of generality, assume that xi 2 [0, 1] for
i = 1, . . . , n. Deﬁne a sequence of knots 0 = t0 < t1 < · · · < tKn−q = 1. In addition, deﬁne
q knots t−q = · · · = t−1 = t0 and another set of q knots tKn−q = · · · = tKn. As in De Boor
6

(1978), the B-spline basis functions are deﬁned as
φk,1(x)
=
8
>
>
<
>
>
:
1, tk x < tk+1,
0, otherwise,
φk,q+1(x)
=
x −tk
tk+q −tk
φk,q(x) +
tk+q+1 −x
tk+q+1 −tk+1
φk+1,q(x),
for k = −q, . . . , Kn−q−1. I reindex k = −q, . . . , Kn−q−1 to k = 1, . . . , Kn and the number
of basis functions is Kn. Letting β = (β1, . . . , βKn)T denote the vector of basis coefﬁcients
and Φ = {φk(xi)}1in,1kKn denote the n ⇥Kn matrix of basis functions evaluated at the
observed covariates, I model F = Φβ.
Even though parametric models might fail to capture important features of the data when
they do not ﬁt into the parametric form, the asymptotic behavior of the parametric model is
superior to the nonparametric counterparts when the data-generating model is in the class of
the parametric models or it is close enough to the class. In Ghosal and van der Vaart (2007), it
was shown that the contraction rate of the posterior distribution deﬁned by isotropic Gaussian
priors for the nonparametric regression model in (1.4) is n−↵/(1+2↵), where ↵> 0 quantiﬁes the
smoothness of the function, which is slower than the parametric optimal rate n−1/2. In practice,
nonparametric models also require extra steps to choose the tuning parameter that controls
the smoothness of the estimated function, and they are usually challenging in computational
and practical senses. Furthermore, in many cases, parametric shapes of F have advantages
for interpretation of the regression function. For example, the slope parameter of the linear
regression model represents the linear association between the response and the covariate.
In the Bayesian paradigm, the evidence in favor of each model in (1.5) is naturally quanti-
ﬁed by Bayes factor that was introduced in Jeffreys (1961) and deﬁned as
BF10 = m1(y)
m0(y),
7

where m1(y) and m0(y) are the marginal likelihoods under H1 and H0; i.e. m1(y) =
R
L(y |
β, H1)d⇡NP(β) and m0(y) =
R
L(y | ✓, H0)d⇡P(✓), where ⇡NP is a prior on the B-spline
coefﬁcient β and ⇡P is a prior on the parameter ✓2 Rd0 for the parametric model in L.
For the hypothesis test in (1.5), Choi et al. (2009) considered a semiparametric model
that has an additive form between a parametric function and a nonparametric function. They
investigated the asymptotic behavior of the Bayes factor deﬁned by Gaussian priors on the
coefﬁcients of the basis functions. More general theoretical results regarding Bayes factor
were provided in Choi and Rousseau (2015), which showed that the resulting Bayes factor
achieves consistency in the sense that BF10 converges to zero in probability when the true
data-generating process supports H0 and BF10 diverges to inﬁnity in probability, otherwise.
When multiple predictors are considered, the nonparametric additive model (Hastie and
Tibshirani, 1986) can be considered, which is expressible as
y =
p
X
j=1
fj(Xj) + ✏,
(1.7)
where ✏⇠N(0, σ2In) and fj is the j-th marginal regression function. Also, Xj is the j-th
covariate among p covariates. The setting for (1.4) can be naturally extended to the additive
model by modeling each component function as a linear combination of the B-spline basis
functions, i.e. fj(Xj) = PKn
k=1 φk(Xj)βjk = Φjβj, where βj = {βj1, . . . , βjKn} and Φj =
{φ1(Xj), . . . , φKn(Xj)} for 1 j p. Similar to the model selection procedures discussed in
Section 1.1.1 for linear models, I am interested in selecting variables that are associated with
the response y, and the uncertainty identiﬁcation of the model space is also my concern.
From a frequentist perspective, there have been several studies regarding the additive model
selection in high-dimensional settings, including Ravikumar et al. (2009), Meier et al. (2009),
and Huang et al. (2010). Many procedures use the group Lasso penalty proposed in Yuan and
Lin (2006) to induce the sparse representation of the component function. Theoretical proper-
8

ties of associated estimation and model selection properties have been investigated in Raskutti
et al. (2012) and Yuan and Zhou (2016). In Bayesian frameworks, Shang and Li (2014) consid-
ered the Bayesian additive model in high-dimensional settings and provided some conditions
on the prior on the basis coefﬁcient necessary to achieve model selection consistency. How-
ever, in Shang and Li (2014), the practical guideline regarding the choice of prior on the spline
coefﬁcients is unclear, and the computational challenges are not resolved, since the proposed
algorithm is based on an MCMC algorithm that is inefﬁcient in high-dimensional settings.
1.2
Research Challenges and Main Contributions
1.2.1
Linear Model Selection in High-dimensional Settings
The Choice of Priors
For high-dimensional linear model selection problems, there is a rich literature regarding
the choice of prior on the regression coefﬁcient and the model space. In Castillo et al. (2015),
a class of model priors called complexity priors was deﬁned as
⇡(k) /
✓p
|k|
◆−1
a−|k|p−b|k|,
for some constants a, b > 0. I note that the Bernoulli-uniform prior discussed in Scott and
Berger (2010), ⇡(k) /
- p
|k|
.−1, is a special case of the complexity prior with a = 1 and b = 0.
Castillo et al. (2015) provided tail conditions on the prior on the coefﬁcients and sufﬁcient
conditions on the hyperparameter of a class of model priors to guarantee the optimal posterior
concentration rate and model selection ocnsistency. Narisetty and He (2014) investigated the
asymptotic behavior of model selection procedures based on the Bernoulli-uniform model prior
and a Gaussian prior with variance that increases faster than p2+✏for any small ✏> 0.
For linear models, the posterior model probability based on priors discussed in Castillo et al.
(2015) and Narisetty and He (2014) (or Zellner’s g-prior Zellner (1986)) can be asymptotically
9

expressed as
log ⇡(k | y) ⇡lk(bβk) −|k|cn,p + C,
(1.8)
where lk is the logarithm of the likelihood function under a model k and bβk is the maximum
likelihood estimator of βk under model k for some sequence cn,p > 0 and a constant C. For ex-
ample, as shown in Narisetty and He (2014), if βk | k ⇠N(0, pc) for some constant c > 0 and
the Bernoulli-uniform prior is imposed on the model space, then the logarithm of the resulting
posterior model probability is asymptotically equivalent to lk(bβk) −(1 + c/2)|k| log p + C. It
is interesting to note that this expression is exactly the same as penalized likelihood procedures
with a L0 penalty (e.g., Zhang et al. (2010), Chen and Chen (2008), Kim et al. (2012)).
The main property of the form in (1.8) is that the penalty strength on model k is determined
solely by its size |k|, regardless of the marginal strength of the regression coefﬁcient in the
model. For example, suppose that two different models with the same model size are consid-
ered. One model consists of predictors that are strongly associated with the response, and the
other model contains only some of strongly associated variables and the rest of variables in the
model are weakly associated with the response. Under objective function in (1.8), two models
would be penalized by the same amount, because the model size is the same. Even though the
model with weakly associated variables will be strongly penalized by the log-likelihood func-
tion, the model selection criterion with the penalty that only depends on the model size might
not be able to select important variables and might fail to control the multiplicity in a practical
sense since there are too many models to be compared in the model space in high-dimensional
settings.
In this dissertation, certain sufﬁcient conditions on the nonlocal priors deﬁned in Deﬁni-
tion 2 will be provided to allow the resulting model selection procedure to achieve the model
selection consistency when log p = O(nc) for some 0 c < 1. Also, the asymptotic form
10

of the posterior model probability deﬁned by the nonlocal priors will be discussed, and I will
point out that the asymptotic form of the posterior model probability contains a unique form
of penalty on the regression coefﬁcient. The form of penalty cannot be attained by local priors
that have been used previously in the literature.
The asymptotic form of the logarithm of the posterior model probability deﬁned by the
nonlocal prior on the regression coefﬁcients can be expressed as
log ⇡(k | y) ⇡lk(bβk) −
|k|
X
j=1
⌧
eβ2
k,j
+ log ⇡(k) + C0,
(1.9)
where eβk,j = bβk,j + Op
-
(n/⌧)−1/4.
and ⌧is the hyperparameter for the nonlocal prior, which
controls the parsimony of model selection. Also, bβk,j denotes the j-th element of bβk.
While model selection procedures deﬁned by local priors penalize a model only by the
size of the model, the penalty P|k|
j=1 ⌧/eβ2
k,j in the objective function in (1.9) is adaptively
determined by the strength of the marginal signal that is measured by the eβ2
k,j term and imposes
different penalties on each predictor in the given model. This adaptive term encourages the
model selection procedure to select variables with strong signals. This property has not been
previously discussed in the original literature (Johnson and Rossell, 2010), and it explains why
the nonlocal prior shows empirically outstanding performance in model selection.
A Scalable Computation
Even though sparsity inducing priors in (1.3) enjoy desirable theoretical properties, the
practical implementation of Bayesian model selection procedures based on these priors is
computationally challenging due to the discrete nature of the prior. Since the total number
of possible models is enormous (2p) even for a moderate dimension p, it is not computationally
practical to calculate all possible marginal likelihoods to evaluate the exact posterior model
probabilities. Thus, algorithms to efﬁciently explore the model space to reduce the computa-
11

tional burden are needed. One might consider reversible jump Markov chain Monte Carlo pro-
posed in Green (1995) for posterior inference, but that algorithm is inefﬁcient and impractical,
especially in high-dimensional settings. A Gibbs sampling based algorithm called the Stochas-
tic Search Variable Selection (SSVS) was proposed in George and McCulloch (1993), but its
computational efﬁciency decreases as the number of predictors increases. Besides Markov
Chain Monte Carlo (MCMC) approaches, Hans et al. (2007) introduced Shotgun Stochastic
Search (SSS) to efﬁciently search the model space and approximate posterior model probabili-
ties. However, the computational demands of SSS signiﬁcantly increases as dimension grows.
More recently, some deterministic approaches, such as Rockova and George (2014) and Car-
bonetto and Stephens (2012), were used to ﬁnd the MAP model. Those algorithms only ﬁnd a
single model and do not provide posterior model probabilities, so it is challenging to quantify
the uncertainty on the model space.
To ameliorate these computational issues, several continuous shrinkage priors have been
proposed, including the Bayesian Lasso (Hans, 2009; Park and Casella, 2008), the horseshoe
prior (Carvalho et al., 2010), the generalized double Pareto shrinkage prior (Armagan et al.,
2013) and the Dirichlet-Laplace prior (Bhattacharya et al., 2015). Those priors can be ex-
pressed as scale mixtures of Gaussian distributions, so the resulting marginal priors are con-
tinuous. By avoiding the structure of the discrete mixtures, those continuous shrinkage priors
provide a computational advantage, and efﬁcient MCMC algorithms are available for sampling
from the corresponding posterior distribution; e.g. Bhattacharya et al. (2016). However, pos-
terior inferences obtained under these continuous priors do not induce any posterior model
probabilities. Nor is it straightforward to choose a model or select variables.
In this dissertation, a scalable stochastic model search algorithm called Simpliﬁed Shotgun
Stochastic Search with Screening (S5) is proposed, and its empirical performance is examined.
S5 is a simpliﬁed version of SSS and it utilizes a screening step embedded in the algorithm
to reduce the model space to be searched. Even though S5 is motivated by SSS, its efﬁciency
12

in searching interesting regions in the model space is remarkably improved by adopting a
screening algorithm. For linear model selection in high-dimensional settings, the S5 algorithm
often ﬁnds the MAP model hundreds of times faster than SSS does, but it identiﬁes the same
MAP model as SSS in all data sets examined in this dissertation. Furthermore, S5 accurately
approximates posterior model probabilities and approximated posterior model probabilities
are almost identical to those obtained from SSS. This algorithm is applicable to any variable
selection procedures as long as a sound screening procedure is available. That is, it can be used
for logistic regression models and nonparametric additive models. I extend the S5 algorithm to
search the space of nonparametric additive models by adding a nonparametric screening step
in the algorithm, and used it to implement the nonparametric model selection procedure that is
described in the following sections.
An R functions that implements S5 is available in the R package BayesS5 in CRAN
(https://cran.r-project.org). This package includes a parallelized version of the
code, which lets multiple independent chains search the model space simultaneously. This al-
lows the algorithm explore a wider range of interesting regions in the model space. Simple
tutorials about the package are also provided in this dissertation.
1.2.2
Nonparametric Model Selection in High-dimensional Settings
A Novel Shrinkage Prior for Nonparametric Regression
Frequently, practitioners face the problem of choosing between a parametric model and
a nonparametric model, where the parametric model is nested within a more general class
of functions. For example, a simple linear regression model or a nonparametric regression
model might be considered for a data set, and the linear regression model is a special case of
the nonparametric model. However, sometimes building a reasonable criterion for the choice
between the parametric form and nonparametric form of the function is not evident, especially
when multiple functions are involved in the model.
13

From a frequentist perspective, there has been a surge of interest in solving this problem us-
ing various forms of penalized estimation via the group Lasso (Yuan and Lin, 2006). Variable
selection based on the group Lasso for partially linear additive models was studied in Zhang
et al. (2011), where it was shown that the resulting procedure identiﬁes the underlying true
model structure correctly and at the same time estimates the multivariate regression function
consistently. For variable selection problems in high-dimensional additive models, several pe-
nalized likelihood approaches using the group Lasso penalty have been proposed in Ravikumar
et al. (2009); Meier et al. (2009); Huang et al. (2010). These approaches force the objective
function to shrink only towards the zero function, and cannot impose shrinkage towards a
more general class of functions, which is useful in many practical examples. For example,
in log-density estimation problems, when the logarithm of a density function is quadratic, the
resulting density function is Gaussian. This means that if we let the log-density function shrink
towards a class of quadratic functions, the resulting estimated density can converge a Gaussian
density. However, shrinkage procedures that accomplish this more general form of shrinkage
have not been investigated in either Bayesian and frequentist frameworks.
In this dissertation, I propose a new shrinkage prior called functional horseshoe prior (fHS)
that encourages shrinkage of the function towards a general class of functions including zero,
constant, linear and quadratic functions. The resulting posterior mean of the function ob-
tained from the fHS prior is expressed as a mixture of nonparametric and parametric estimators.
Hence, by using the fHS prior, when the true function is in a class of parametric functions that
are speciﬁed in advance, the posterior distribution of the function behaves as if the parametric
model is used, and when the true function is strictly separated form the class of parametric
functions, the resulting posterior distribution holds its nonparametric properties.
To construct the fHS prior, I introduce a novel semi-norm that measures the discrepancy
between a function and a class of parametric functions. For the nonparametric regression model
in (1.4), the semi-norm is F T(I−Q0)F, where Q0 is the projection matrix of the null covariates
14

that span the class of parametric functions. For example, the semi-norm can be interpreted as
F is linear () F
T(I −Q0)F = 0,
(1.10)
by setting Q0 to be the projection matrix of {1, X}. The above relation is natural, because any
linear function that is expressed as a + bX for some a, b 2 R must have zero sum of square
residuals from a linear model, which is F T(I −Q0)F = 0. Unlike existing shrinkage priors
for shrinkage on a parameter towards zero such as the horseshoe prior (Carvalho et al., 2010),
the shrinkage of the fHS prior acts on this semi-norm of the function and compels shrinkage
towards the class of parametric functions (linear functions in the above example). Further, the
proposed prior provides a natural connection to a new class of penalized likelihood methods
which can be interpreted from a frequentist perspective.
Theoretical properties of the fHS priors are studied, and it is shown that under some mild
conditions the posterior contraction rate achieves the parametric optimal rate n−1/2 under the
L2 norm when the true function lies on the class of pre-speciﬁed parametric functions. That is,
resulting inferences maintain the optimal nonparametric rate up to a logarithm term of n when
the underlying function is not parametric. This result suggests that the use of the fHS prior can
improve the estimation performance when the underlying function is parametric, and it does
not degrade the estimation when the underlying model is nonparametric. The product of the
fHS priors can be applied to the additive models in (1.7) to select variables by letting each
component function shrink towards the zero function (Q0 = 0). I evaluate the performance
of this methodology through multiple real and simulated data sets. In terms of estimation
and model selection, the proposed prior outperforms the state-of-the-art alternative methods
including the standard horseshoe prior and the penalized likelihood procedure using the group
Lasso.
15

A Novel Nonlocal Prior for Nonparametric Model Selection
As brieﬂy discussed in Section 1.1.2, for nonparametric hypothesis testing problems in
(1.5), Choi et al. (2009) and Choi and Rousseau (2015) have shown that Bayes factors deﬁned
by certain classes of priors achieve consistency. Even though these approaches showed that the
convergence rate of Bayes factors in favor of alternative hypotheses increases at exponential
rate of n under a true alternative hypothesis, they did not address the asymptotic behavior of
the Bayes factor under true null hypothesis. This asymptotic study of Bayes factors under true
null hypothesis is important, especially when the number of functions to be tested increases as
sample size n grows.
I show that local prior densities, which assign positive probability around a null function
in nonparametric Bayesian hypothesis tests, provide exponential accumulation of evidence in
favor of an alternative hypothesis under a true alternative hypothesis, but only a polynomial
rate of accumulation in favor of null hypothesis under true null. This imbalanced behavior has
been noted also in parametric hypothesis testing problems as discussed in Johnson and Rossell
(2010).
For parametric hypothesis testing problems (Johnson and Rossell, 2010), the nonlocal prior
densities deﬁned in Deﬁnition 2 were proposed to improve the convergence rate of the Bayes
factor under a true null. These priors ameliorates the imbalanced behavior of the convergence
rate of Bayes factor. Johnson and Rossell (2012) showed that the application of nonlocal
priors to linear model selection procedures resulted in consistency in high-dimensional settings,
whereas procedures based on local priors failed to be consistent.
To improve the convergence rate of nonparametric Bayes factor and pursue a consistent
model selection procedure for nonparametric models in high-dimensional settings, the same
strategy as nonlocal priors seems compelling in nonparametric settings. However, the appli-
cation of the nonlocal idea to nonparametric models has been challenging. Unlike the null
16

hypothesis for scalar valued parameters, the nonparametric null hypothesis in (1.5) is compos-
ite. This means that the null hypothesis does not deﬁne a unique density for generating the
data. Thus, the null space of functions is difﬁcult to be parameterized, and this has hindered a
consideration of an extension of nonlocal prior densities to nonparametric models.
In this dissertation, by using a novel semi-norm F T(I −Q0)F introduced in (1.10), I deﬁne
the null space of functions in (1.5) as {F : F T(I −Q0)F = 0}. I then construct a new class of
nonlocal priors called nonlocal functional prior densities for nonparametric hypothesis testing
and model selection problems. I provide the convergence rate of Bayes factors based on the
nonlocal functional priors. When the true data-generating process is from the null model, I
show that the convergencerate is much faster than that obtained from local priors. Finally,
I apply the nonlocal functional prior to variable selection problems for the additive model
in (1.7). Under mild regularity conditions, the consistency of the resulting model selection
procedure is shown in high-dimensional settings where the number of predictors p increases
at sub-exponential rate of n. A wide range of simulated and real data sets are considered to
examine the model selection performance of the nonlocal functional prior, showing that it has
better or comparable performance compared to all of its current competitors.
1.3
Outline
In Chapter 2, I consider model selection consistency for nonlocal prior densities in high-
dimensional settings where the dimensionality p is allowed to increase at sub-exponential rate
in n. Under suitable regularity conditions, the asymptotic form of the logarithm of the posterior
model probability based on the nonlocal prior is illustrated. I show that it contains a unique
form of adaptive penalty that cannot be derived from local priors.
In Chapter 3, I provide a detailed description of the S5 algorithm. Its efﬁciency is examined
by using simulated and real data sets. Also, I provide examples of the implementation of the
S5 algorithm using the R package BayesS5.
17

In Chapter 4, I propose a new class of shrinkage densities called the fHS prior for non-
parametric models. These shrinkage priors bridge the gap between parametric functions and
nonparametric functions. Under mild conditions, I show that when the true underlying func-
tion has a parametric form that is pre-speciﬁed in advance, the resulting posterior distribution
contracts at the parametric optimal rate n−1/2 under the L2 norm, and that it achieves the opti-
mal nonparametric rate when the true function is strictly separated from the class of parametric
functions. I apply the fHS prior to additive models to improve estimation and select variables.
For several real and simulated data sets, it shows outstanding performance in both estimation
and model selection.
In Chapter 5, the nonlocal functional prior is proposed for nonparametric hypothesis testing
(or model selection). I show that local prior densities, which assign positive probability around
a null function in nonparametric Bayesian tests, provide exponential accumulation of evidence
in favor of an alternative hypothesis under a true alternative hypothesis, but only a polynomial
rate of accumulation in favor of a null hypothesis under a true null. This imbalanced behavior
of the convergence rate of Bayes factor can be ameliorated by nonlocal I functional priors, and
the resulting hypothesis testing procedures strongly penalize cases where the null hypotheses
are rejected. I apply the proposed prior densities for high-dimensional model selection of
nonparametric additive models and investigate model selection consistency of the resulting
model selection procedures. I provide simulation studies and real data examples wher the
proposed model selection procedure outperforms state-of-the-art methods.
The proofs of theoretical results in this dissertation appear in Appendix A, while the tech-
nical details of computation are presented in Appendix B.
18

2.
NONLOCAL PRIOR DENSITIES FOR HIGH-DIMENSIONAL LINEAR MODEL
SELECTION
2.1
Introduction
In the context of hypothesis testing, Johnson and Rossell (2010) deﬁned nonlocal (alterna-
tive) priors as densities that are exactly zero whenever a model parameter equals its null value.
Nonlocal priors were extended to model selection problems in Johnson and Rossell (2012),
where product moment (pMoM) prior and product inverse moment (piMoM) prior densities
were introduced as priors on a vector of regression coefﬁcients. In p n settings, model
selection procedures based on these priors were demonstrated to have a model selection prop-
erty: the posterior probability of the true model converges to 1 as the sample size n increases.
More recently, Rossell et al. (2013) and Rossell and Telesca (2017) proposed product expo-
nential moment (peMoM) prior densities that have similar behavior to piMoM densities near
the origin. However, the behavior of nonlocal priors in p ≫n settings remains understudied
to date (particularly in comparison to other commonly used variables selection procedures),
which serves as the motivation for this dissertation.
I undertook a detailed simulation study to compare the performance of nonlocal priors in
p ≫n settings under sparsity with a host of penalization methods including the least abso-
lute shrinkage and selection operator (Lasso; Tibshirani (1996)), smoothly clipped absolute
deviation (Scad; Fan and Li (2001)), adaptive Lasso (Zou, 2006), minimum convex penalty
(MCP; Zhang (2010)), and the reciprocal Lasso (rLasso), recently proposed by Song and Liang
(2015). The penalty function of the rLasso is equivalent to the negative log-kernel of nonlocal
prior densities; further connections are described in Section 2.5. As a natural Bayesian com-
petitor, I also considered the widely used g-prior (Zellner, 1986; Liang et al., 2008), which is
a local prior in the sense of Johnson and Rossell (2010). I used precision-recall curves (Davis
19

and Goadrich, 2006) as a basis for comparison between methods. These curves eliminate the
effect of the choice of tuning parameters for each method so that the comparison across differ-
ent methods is transparent. In cases where only a tiny proportion of variables are signiﬁcant,
precision-recall curves are more appropriate tools for comparison than are the more widely
used receiver operating characteristic curves (Davis and Goadrich, 2006). While ROC curves
present a trade-off between the type I error and the power of a decision procedure, precision-
recall curves examine the trade-off between the power and the false discovery rate.
My studies indicate that Bayesian procedures based on nonlocal priors and the g-prior per-
form better than penalized likelihood approaches in the sense that they achieve a lower false
discovery rate while maintaining a given level of statistical power. Furthermore, I ﬁnd that pos-
terior distributions on the model space based on nonlocal priors are more tightly concentrated
around the maximum a posteriori model than the posterior based on g-priors, implying that
they have a faster rate of posterior concentration. I also identiﬁed the oracle hyperparameter
that maximizes the posterior probability of the true model for the Bayesian procedures. The
growth-rate of these oracle hyperparameters with p also offers an interesting contrast between
nonlocal and local priors. In the case of g-priors, the oracle value of g varied between 7.83⇥108
and 4.29 ⇥1013 as p ranged between 1000 and 20000 in a variety of simulation settings. For
the same range of p, the oracle value of ⌧varied between 1.97 and 3.60, where ⌧is the tuning
parameter for nonlocal priors described in Section 2. George and Foster (2000) argued from
a minimax perspective that the g parameter should satisfy g ⇣p2, which explains the large
values of the optimal g. However, using asymptotic arguments to obtain default hyperparame-
ters is difﬁcult because the constant of proportionality is typically unknown. Moreover, when
g is very large, the g-prior assigns negligible prior mass at the origin, essentially resulting in a
nonlocal like prior. A similar point can be made about the recently proposed Bayesian shrink-
ing and diffusing (BASAD) priors Narisetty and He (2014). On the other hand, the optimal
hyperparameter value for the nonlocal priors is stable with increasing p, growing at a very slow
20

rate.
Motivated by this empirical ﬁnding, I studied properties of two classes of nonlocal priors
allowing the hyperparameter ⌧to scale with p. Using a ﬁxed value of ⌧, it seems that model
selection consistency is possible only when p n (Johnson and Rossell (2012)). In this
article, I establish that nonlocal priors can achieve model selection consistency even when
the number of variables p increases sub-exponentially in the sample size n, provided that the
hyperparameter ⌧is asymptotically larger than log p. This theoretical result is consistent with
my empirical ﬁnding.
2.2
Nonlocal Prior Densities for Regression Coefﬁcients
I consider the standard setup of a Gaussian linear regression model with a univariate re-
sponse and p candidate predictors. Let y = {y1, . . . , yn}T denote a vector of responses for
n individuals and X an n ⇥p matrix of covariates. I denote a model by an index set of
variables k = {k1, . . . , k|k|}, with 1 k1 < . . . < k|k| p. Given a model k, let Xk
denote the design matrix formed from the columns of X corresponding to the model k and
βk = {βk,1, . . . , βk,|k|}T the regression coefﬁcient for the model k. Under each model k, the
linear regression model for the data is
y = Xkβk + ✏,
(2.1)
where ✏⇠Nn(0, σ2In). Let t denote the true, or data-generating model and let β0
t be the true
regression coefﬁcient under model t. I assume that the true model is ﬁxed but unknown.
Given a model k, the product exponential moment (peMoM) prior density (Rossell et al.,
2013; Rossell and Telesca, 2017) for the vector of regression coefﬁcients βk is deﬁned as
⇡(βk | σ2, ⌧, k) = C−|k|
|k|
Y
j=1
exp{−β2
k,j/(2σ2⌧) −⌧/β2
k,j}.
(2.2)
21

−10
−5
0
5
10
0.00
0.05
0.10
0.15
parameter
density
peMoM
piMoM
Figure 2.1: Nonlocal prior density functions for a single regression coefﬁcient with ⌧= 5; for
the piMoM prior, r = 1.
The normalizing constant C can be explicitly calculated as
C =
Z 1
−1
exp{−t2/(2σ2⌧) −⌧/t2}dt = (2⇡σ2⌧)1/2 exp{−(2/σ2)1/2},
(2.3)
since
R
exp{−µ/t2 −⇣t2}dt = (⇡/⇣)1/2 exp{−2(µ⇣)1/2}.
Second, for a ﬁxed positive integer r, the product inverse-moment (piMoM) prior density
(Johnson and Rossell, 2012) for βk is given by
⇡(βk | σ2, ⌧, k) = C⇤−|k|
|k|
Y
j=1
[(βk,j)−2r exp{−⌧/β2
k,j}],
(2.4)
where C⇤= ⌧−r+1/2Γ(r −1/2) for r > 1/2 and Γ(·) is the gamma function.
The piMoM and peMoM prior densities are nonlocal in the sense that the density value
at the origin is exactly zero. This feature of the densities for a single regression coefﬁcient
is illustrated in Figure 2.1. Since the piMoM prior densities and the peMoM prior densities
have the same term exp{−⌧/β2} that controls the behavior of the density function around the
origin, they attain almost the same shape of the density function at the origin, which yields the
22

similar properties. Further details regarding this point are discussed in Section 2.4.
I focus on these two classes of nonlocal priors in the sequel. Note that in both (2.2) and
(2.4), ⇡(βk) = 0 when βk = 0; a deﬁning feature of nonlocal priors. The distinction between
the peMoM and the piMoM priors mainly involves their tail behavior. Whereas peMoM priors
possess Gaussian tails, the piMoM prior densities have inverse polynomial tails. For example,
piMoM densities with r = 1 have Cauchy-like tails, which has implications for their ﬁnite
sample consistency and asymptotic bias in posterior mean estimates of regression coefﬁcients.
Because similar constraints are imposed on the hyperparameter ⌧appearing in both (2.2) and
(2.4), at the risk of some ambiguity I use the same symbol for the two hyperparameters in these
equations.
In addition to imposing priors on the regression parameters given a model, I need to place
a prior on the space of models to complete the prior speciﬁcation. I consider a uniform prior
on the model space restricted to models having size less than or equal to qn, with qn < n, i.e.,
⇡(k) / I(|k| qn),
(2.5)
where I(·) denotes the indicator function and with a slight abuse of notation, I denote the prior
on the space of models by ⇡as well. Similar priors have been considered in the literature by
Jiang (2007) and Liang et al. (2013). Since the peMoM and piMoM priors already induce a
strong penalty on the size of the model space (see Section 2.4), I do not need to additionally
penalize larger models using, for example, model space priors of the type discussed in Scott
and Berger (2010).
Under a peMoM prior (2.2) on the regression coefﬁcients, the marginal likelihood mk(y)
under model k given σ2 can be obtained by integrating out βk, resulting in
mk(y) = (2⇡σ2)−n
2 C−|k| Qk exp{−eRk/(2σ2)},
23

where
eRk
=
yT(In −ePk)y,
ePk = Xk(X
T
kXk + 1/⌧Ik)−1X
T
k,
Qk
=
Z
exp{−(βk −eβk)
Te⌃−1
k (βk −eβk)/(2σ2) −
|k|
X
j=1
⌧/β2
k,j}dβk,
(2.6)
eβk
=
(X
T
kXk + 1/⌧Ik)−1X
T
ky,
e⌃k = (X
T
kXk + 1/⌧Ik)−1.
Similarly, the marginal likelihood using the piMoM prior densities (2.4) can be expressed
as mk(y) = (2⇡σ2)−n
2 C⇤−|k| Q⇤
k exp{−R⇤
k/(2σ2)}, where
R⇤
k
=
y
T(In −Pk)y,
Pk = Xk (X
T
kXk)−1 X
T
k,
Q⇤
k
=
Z
|k|
Y
j=1
β−2r
k,j exp{−(βk −bβk)
T⌃⇤−1
k
(βk −bβk)/(2σ2) −
|k|
X
j=1
⌧/β2
k,j}dβk,
(2.7)
bβk
=
(X
T
kXk)−1X
T
ky,
⌃⇤
k = (X
T
kXk)−1.
The integrals for Qk and Q⇤
k cannot be obtained in closed form, so for computational purposes
I make Laplace approximations to mk(y). The expressions for the marginal likelihood derived
here is nevertheless important for theoretical studies in Section 2.4.
2.3
Numerical Results
2.3.1
Simulation Studies Using Precision-Recall Curves
To illustrate the performance of nonlocal priors in ultrahigh-dimensional settings and to
compare their performance with other methods, I calculated precision-recall curves Davis and
Goadrich (2006) for all selection procedures. A precision-recall curve plots the precision =
TP/(TP + FP) versus recall (or sensitivity) = TP/(TP + FN), where TP, FP and FN respectively
denote the number of true positives, false positives, and false negatives, as the tuning parameter
is varied. The efﬁcacy of a procedure can be measured by the area under the precision-recall
24

curve; the greater the area, the more accurate the method. Since both precision and recall take
values in [0, 1], the area under the curve for an ideal precision-recall curve is 1. I used two
(n, p) combinations, namely (n, p) = (400, 10000) and (n, p) = (400, 20000), and plotted the
average of the precision-recall curves obtained from 100 independent replicates of each pro-
cedure. To evaluate the marginal likelihood of each model, I used the Laplace approximation
method.
I compared the performance of peMoM and piMoM priors to a number of frequentist penal-
ized likelihood methods: Lasso (Tibshirani, 1996), adaptive Lasso (Zou, 2006), Scad (Fan and
Li, 2001), and Minimax Concave Penalty (MCP) (Zhang, 2010). I used the R package ncvreg
to ﬁt these penalized likelihood methods. I also included the reciprocal Lasso in the simulation
studies. However, due to computational constraints involved in implementing the full rLasso
procedure, I followed the recommendation in Song and Liang (2015) and instead implemented
the reduced rLasso. The reduced rLasso procedure is a simpliﬁed version of rLasso that uses
the least square estimators of β when minimizing the rLasso objective function.
I considered Zellner’s g-prior Zellner (1986); Liang et al. (2008) as a competing Bayesian
method, with βk | k, σ2 ⇠N(0, gσ2(X T
kXk)−1) and g a tuning parameter. With the prior
⇡(σ2) / 1/σ2, the marginal likelihood mk(y) / (1 + g)−|k|/2{1 + g(1 −D2
k)}−(n−1)/2 can
be obtained in a closed form; see for example, (Liang et al., 2008, pp 412), where D2
k is the
ordinary coefﬁcient of determination for the model k.
A uniform model prior (2.5) was considered for all Bayesian procedures. This prior was
chosen for several reasons. First, construction of the PR curves requires maximization over
model hyperparameters, which is most easily achieved if there is only one unknown hyperpa-
rameter. I also wished to avoid providing an advantage to the Bayesian methods by introducing
additional tuning parameters into these methods that were not present in the penalized likeli-
hood methods. Furthermore, the use of non-uniform priors on the model space introduces (at
least) one more degree of freedom into the comparisons between methods, and my intent was
25

to compare the effects of the penalties imposed on regression coefﬁcients by both penalized
likelihood and Bayesian methods. At ﬁrst blush, this might appear to put Bayesian methods
like those based on the g-prior at a disadvantage, since such methods do not yield consistent
variable selection even in p < n settings without prior sparsity penalties on the model space
(when g is held ﬁxed as n increases). However, in the construction of PR curves, I allowed prior
hyperparameters to increase with n, which effectively allowed the Bayesian methods to impose
additional sparseness penalties through the introduction of large hyperparameter values.
I arbitrarily ﬁxed r = 1 for the piMoM prior (2.4) and used an inverse-gamma prior on σ2
with parameters (0.1, 0.1) for the peMoM, piMoM priors, and g-priors. Posterior computations
for the peMoM, piMoM and g-priors were implemented using the Simpliﬁed Shotgun Stochas-
tic Search with Screening (S5) algorithm described in Chapter 3. The maximum a posteriori
model was used in each case to summarize the model selection performance. The precision-
recall curves are drawn by varying the hyperparameters (⌧for the nonlocal priors and g for the
g-priors) so the comparison between the model selection based on the nonlocal priors and the
g-prior is free of the choice of hyperparameters. Because of their high computational burden, I
could not include BASAD Narisetty and He (2014) in the comparisons.
For each simulation setting, I simulated data according to a Gaussian linear model as in
(2.1) with the ﬁxed true model t = {1, 2, 3, 4, 5} with the true regression coefﬁcient β0
t =
(0.50, 0.75, 1.00, 1.25, 1.50)T and σ = 1.5. Also, the signs of the true regression coefﬁcients
were randomly determined with probability one-half. Each row of X was independently gen-
erated from a N(0, ⌃) distribution with one of the following covariance structures:
Case (1): compound symmetry design; ⌃jj0 = 0.5, if j 6= j0 and ⌃jj = 1, 1 j, j0 p.
Case (2): autoregressive correlated design; ⌃jj0 = 0.5|j−j0|, 1 j, j0 p.
Case (3): isotropic design; ⌃= Ip.
Figure 2.2 plots the precision-recall curves averaged over 100 simulation replicates for the
26

different methods across the two (n,p) pairs and the three covariate designs. From Figure 2.2,
it is evident that the precision-recall curves for the peMoM and piMoM priors have an overall
better performance than the penalized likelihood methods Lasso, adaptive Lasso, Scad, and
MCP. For decision procedures having the same power, this implies that the nonlocal priors
achieve lower false discovery rates. As discussed in Section 2.5, since the reduced rLasso
shares the same nonlocal kernel as the nonlocal priors, it has a similar selection performance.
The ﬁgure also shows that Zellner’s g-prior attains comparable performance with the nonlocal
priors in terms of the precision-recall curves.
2.3.2
Further Comparison with Zellner’s g-prior
The similarity of the performances of the g-prior and the nonlocal priors in terms of precision-
recall curves begs for closer comparisons of these procedures. For this reason, I also investi-
gated the concentration of the posterior densities around their maximum models. To this end,
I ﬁxed p = 20, 000 and varied n from 150 to 400; the data generating mechanism was exactly
the same as in Section 2.3.1. The left column of Figure 2.3 displays the posterior probability
of the true model under the peMoM, piMoM and g-prior models versus n for the three covari-
ate designs in Section 2.3.1. The plot shows that the posterior probability of the true model
increases with n for all three methods, with the peMoM and piMoM priors almost uniformly
dominating the g-prior, implying a higher concentration of the posterior around the true model
for the nonlocal priors.
This tendency is conﬁrmed in the right panel of Figure 2.3, where I plot the number of
models k which achieve a posterior odds ratio ⇡(k | y)/⇡(bk | y) > 0.001, where bk is the
maximum a posteriori model. This plot clearly shows that the posterior distribution on the
model space from the g-priors is more diffuse than those obtained using the nonlocal prior
methods. These comparisons were based on ﬁtting the hyperparameters g and ⌧at their oracle
value, i.e., the value which maximized the posterior probability of the true model for a given
27

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
peMoM
piMoM
g−prior
rrLASSO
AdapLASSO
LASSO
SCAD
MCP
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
Figure 2.2: Plot of the mean precision-precision curves over 100 datasets with (n, p) =
(400, 10000)(ﬁrst column) and (n, p) = (400, 20000)(second column). Top: case (1); mid-
dle: case (2); bottom: case (3).
28

n
posterior probability
150
200
250
300
400
0.0
0.2
0.4
0.6
0.8
1.0
peMoM
piMoM
g−prior
n
# of models
150
200
250
300
400
0
20
40
60
80
100
n
posterior probability
150
200
250
300
400
0.0
0.2
0.4
0.6
0.8
1.0
n
# of models
150
200
250
300
400
0
20
40
60
80
100
n
posterior probability
150
200
250
300
400
0.0
0.2
0.4
0.6
0.8
1.0
n
# of models
150
200
250
300
400
0
20
40
60
80
100
Figure 2.3: Averaged posterior true model probability and the number of models which attain
the posterior odds ratio, with respect to the maximum a posteriori model, larger than 0.001
with the ﬁxed p = 20000 and varying n. Top: case (1); middle: case (2); bottom: case (3).
29

Table 2.1: Optimal hyperparameters for Bayesian model selection methods
The number of predictors
Case
p = 1000
p = 2000
p = 5000
p = 10000
p = 20000
(1)
peMoM
2.24
2.72
2.88
3.32
3.60
piMoM
2.16
2.59
2.70
3.04
3.26
g-prior
7.83 ⇥108
2.87 ⇥109
3.05 ⇥109
9.66 ⇥109
1.70 ⇥1010
(2)
peMoM
1.97
2.29
2.34
2.75
3.00
piMoM
1.97
2.20
2.32
2.66
2.86
g-prior
8.56 ⇥109
2.55 ⇥1010
2.62 ⇥1010
6.58 ⇥1010
1.25 ⇥1011
(3)
peMoM
2.66
3.00
3.00
3.10
3.60
piMoM
2.61
2.94
2.94
2.94
3.46
g-prior
1.26 ⇥1012
8.84 ⇥1012
9.67 ⇥1012
6.81 ⇥1012
4.29 ⇥1013
value of n.
The magnitudes of the oracle hyperparameters under each model also present an interesting
contrast between the local and nonlocal priors. I observed that the oracle value of g increased
rapidly with p, whereas the oracle value of ⌧was much more stable. This phenomenon is illus-
trated in Table 2.1, which shows the oracle hyperparameter value averaged over 100 replicates
for the three different covariate designs in Section 2.3.1. For this comparison, I ﬁxed n = 400
and varied p between 1000 and 20, 000; ﬁve representative values are displayed. The oracle
values for g are on a completely different scale from the oracle values of ⌧, and they vary
more with p. This table conﬁrms the recommendations in George and Foster (2000) for set-
ting g = p2 based on minimax arguments. However, the ﬁnite sample behavior of the optimal
choice of g is unclear, which means that the large variance of the optimal hyperparameter value
is likely to hinder the selection of g in real applications. Finally, I note that such large values
of g effectively convert the local g-priors into nonlocal priors by collapsing the g-prior density
to 0 at the origin.
30

2.4
Model Selection Consistency
The empirical performance of the peMoM and piMoM priors suggests that the hyperpa-
rameter ⌧should be increased slowly with p. While Johnson and Rossell (2012) were able to
show strong selection consistency with a ﬁxed value of ⌧, it is not clear whether their proof can
be extended to p ≫n cases. Motivated by the empirical ﬁndings of the last section, I next in-
vestigated the strong consistency properties of peMoM and piMoM priors when ⌧was allowed
to grow at a logarithmic rate in p. I found that in such cases, both peMoM and piMoM priors
achieve model selection consistency under standard regularity assumptions when p increases
sub-exponentially with n, i.e., log p = O(n↵) for ↵2 (0, 1).
Henceforth, I use ⌧n,p instead of ⌧to denote the hyperparameter in the peMoM and piMoM
priors in (2.2) and (2.4) respectively. The normalizing constants for these priors are now de-
noted by Cn,p and C⇤
n,p, respectively. Before providing my theoretical results, I ﬁrst state a
number of regularity conditions. Let ⌫j(A) denote the j-th largest nonzero eigenvalue of an
arbitrary matrix A, and let
⌫k⇤=
min
1jmin(n,|k|) ⌫j(X
T
kXk/n),
⌫⇤
k =
max
1jmin(n,|k|) ⌫j(X
T
kXk/n).
(2.8)
For sequences an and bn, an ⌫bn indicates bn = O(an), and an ≻bn indicates bn = o(an).
With this notation, I assume that the following regularity conditions apply.
Assumption 1. There exists ↵2 (0, 1) such that log p = O(n↵).
Assumption 2. log p ≺⌧n,p ≺n.
Assumption 3. |k| qn, where qn ≺⌧n,p
log p.
Assumption 4.
min
k:|k|qn⌫k⇤≻⌧n,p
n .
31

Assumption 5. C1 < ⌫t⇤⌫⇤
t < C2 for some positive constants C1 and C2.
Several comments regarding these conditions are worth making. Assumption 1 allows
p to grow sub-exponentially with n. My theoretical results continue to hold when p grows
polynomially in n, i.e., at the rate O(nγ) for some γ > 1. Assumption 2 reﬂects the empirical
ﬁndings about the oracle ⌧⌘⌧n,p in Section 2.3.1, which was observed to grow slowly with
p. I need the bound on qn in Assumption 3 to ensure that the least square estimator of a
model is consistent when a model contains the true model. In the p n setting, Johnson and
Rossell (2012) assumed that all eigenvalues of the Gram matrix (X T
kXk)/n are bounded above
and below by global constants for all k. However, this assumption is no longer viable when
p ≫n and I replace that by Assumption 4, where the minimum of the minimum eigenvalue
of (X T
kXk)/n over all submodels k with |k| qn is allowed to decrease with increasing n
and p. Assumption 4 is called the sparse Riesz condition and is also used in Chen and Chen
(2008) and Kim et al. (2012). Narisetty and He (2014) showed that Assumption 4 holds with
overwhelmingly large probability when the rows of the design matrix are independent with an
isotropic sub-Gaussian distribution. Even though the assumption of sub-Gaussian tails on the
covariates is difﬁcult to verify, the results in Narisetty and He (2014) show that Assumption 4
can be satisﬁed for some sequence of design matrices.
I now state a Theorem that demonstrates that model selection procedures based on the
peMoM and piMoM nonlocal prior densities achieve strong consistency under the proposed
regularity conditions. A proof of the Theorem is provided in the Appendix.
Theorem 1. Suppose σ2 is known and that Assumptions 1 – 5 hold. Let ⇡(t | y) denote the
posterior probability of the true model obtained under a peMoM prior (2.2). Also, assume a
uniform prior on all models of size less than or equal to qn, i.e., ⇡(k) / I(|k| qn). Then,
⇡(t | y) converges to one in probability as n goes to 1.
32

Corollary 2. Assume the conditions of the preceding Theorem apply. Let ⇡(t | y) denote the
posterior probability of the true model obtained under a piMoM prior density (2.4). Then,
⇡(t | y) converges to one in probability as n goes to 1.
I note that these results apply also if a beta-Bernoulli prior is imposed on the model space
as in Scott and Berger (2010), because the effect of that prior is asymptotically negligible when
|k| qn ≺n.
In most applications, σ2 is unknown, and it is thus necessary to specify a prior density on it.
By imposing a proper inverse gamma prior density on σ2, I can obtain the model consistency
result stated in the Theorem below. The proof is again deferred to the Appendix.
Theorem 3. Suppose σ2 is unknown and a proper inverse gamma density with parameters
(a0, b0) is assumed for σ2. Also, let ⇡(t | y) denote the posterior probability of the true model
evaluated using peMoM priors. Then if Assumptions 1 – 5 are satisﬁed, ⇡(t | y) converges to
one in probability as n goes to 1.
Corollary 4. Suppose the conditions of the preceding Theorem apply, but that ⇡(t | y) now
denotes the posterior probability of the true model obtained under a piMoM prior density.
Then ⇡(t | y) converges to one in probability as n goes to 1.
2.5
Connections Between Nonlocal Priors and Reciprocal Lasso
In this section, I highlight the connection between the rLasso in Song and Liang (2015)
and Bayesian variable selection procedures based on nonlocal priors. I begin by noting that the
objective function g(βk; k) of rLasso on a model k can be expressed as follows:
g(βk; k) =
11y −Xkβk
112
2 +
|k|
X
j=1
⌧n,p/|βk,j|.
(2.9)
The optimal model is selected by minimizing this objective function with respect to βk and
k. It is clear that the penalty function P|k|
j=1 ⌧n,p/|βk,j| in (2.9) is similar to the negative log-
33

density of piMoM nonlocal priors as proposed in (Johnson and Rossell, 2012, pp 659) and
(Johnson and Rossell, 2010, pp 149). The main difference between the nonlocal prior version
of rLasso and the piMoM-type prior densities proposed in the previous section is the power
of β in the exponential kernels. For the rLasso penalty this power is 1, while for piMoM-
type prior densities it is 2. The implications of this difference are apparent from the following
proposition.
Proposition 5. For a given model k, suppose that eβ⇤
k is the minimizer of the objective function
(2.9), and again let bβk denote the least square estimator of β under model k. Assume that
⌧n,p ≺n, and there exist strictly positive contants CL and CU such that CL < ⌫k⇤⌫⇤
k < CU.
Then, for any ✏⇤
n ≻(⌧n,p/n)1/3,
P
h
eβ⇤
k /2 R
-bβk; ✏⇤
n
.i
! 0,
where R(u; ✏) = {x 2 R|k| : |xj −uj| ✏, j = 1, . . . , |k|}.
This proposition shows that under standard conditions on the eigenvalues of the Gram ma-
trix XT
k Xk/n, the estimator derived from (2.9) is asymptotically within (⌧n,p/n)1/3 distance of
the least squares estimator bβk. On the other hand, results cited in the previous section show
that maximum a posteriori estimators obtained from the piMoM-type prior densities reside at
an asymptotic distance of (⌧n,p/n)1/4 from the least squares estimator. Variable selection pro-
cedures based on both forms of piMoM priors thus achieve adaptive penalties on the regression
coefﬁcients in the sense described in Song and Liang (2015).
Although rLasso is proposed as a penalized likelihood approach, the computational pro-
cedure to optimize its objective function is quite different from the other penalized likelihood
methods. The resulting computational complexity of this optimization procedure, which con-
tains a discontinuous penalty function, is NP-hard. This suggests that the formulation of this
nonlocal penalty in a penalized likelihood framework is unlikely to provide signiﬁcant compu-
34

tational advantages over related Bayesian model selection procedures, even though the infer-
ential advantages of the Bayesian framework are lost.
2.6
An Adaptive Form of Asymptotic Marginal Likelihoods Based on Nonlocal Priors
From Lemma A.1.1 in the Appendix, it follows that the asymptotic log-marginal likelihood
of a model k based on a peMoM or piMoM prior density can be expressed as
log ⇡(k | y)
=
l(bβk) + log Qk −|k| log Cn,p
⇡
l(bβk) −
|k|
X
j=1
p⌧n,p
-bβk,j
.
+ C,
for some constant C, where bβk is the maximum likelihood estimator under model k, i.e. bβk =
(XT
k Xk)−1XT
k y, and
p⌧n,p
-bβk,j
.
⇡
8
>
>
<
>
>
:
(n⌧n,puk)1/2,
if |bβk,j| < c
- nuk
⌧n,p
.−1/4
⌧n,p/bβ2
k,j,
if |bβk,j| ≥c
- nuk
⌧n,p
.−1/4,
(2.10)
for some constant c and some arbitrary sequence uk with ⌫k⇤uk ⌫⇤
k. I note that the
strength of the correlation between the variables in model k affects the behavior of uk, and
(nuk/⌧n,p)−1/4 converges to zero as n tends to inﬁnity due to Assumption 4 described in Sec-
tion 2.4.
On the other hand, the penalty term in the other Bayesian model selection approaches is
quite different from that of the nonlocal priors as in (2.10). The marginal likelihood based on
the g-prior when σ2 is known can be expressed as
l(bβk) −|k| log(1 + g)/2.
Narisetty and He(2014) demonstrated that BASAD achieves model selection consistency.
35

This consistency follows from that the fact that the BASAD “penalty” is asymptotically equiv-
alent to
l(bβk) −c|k| log(p),
(2.11)
where c is some constant. Yang et al. (2016) and Castillo and van der Vaart (2012) also con-
sidered a similar penalty term on the model space, which implies that the posterior probability
for their procedures can be expressed in the same form as (2.11). When g = p2c, the marginal
likelihood based on a g-prior is asymptotically equivalent to (2.11).
This asymptotic term of the marginal likelihoods is quite different from that of the nonlo-
cal priors, since the penalty terms in the other Bayesian approaches only focus on the model
size without considering the different weights on variables in the model. The marginal like-
lihoods based on nonlocal priors, however, impose different penalties on each predictor in
the given model. When the MLE of the regression coefﬁcient in the model is asymptotically
close to zero (|bβk,j| < c(nuk/⌧n,p)−1/4), the model that contains the corresponding variable
would be strongly penalized by (n⌧n,puk)1/2. In contrast, when the MLE is asymptotically
signiﬁcant (|bβk,j| ≥c(nuk/⌧n,p)−1/4), the penalty attains a different weight based on the MLE
(p⌧n,p(bβk,j) ⇡⌧n,p/bβ2
k,j).
This analysis highlights the fact that the nonlocal priors are able to adapt their penalty for
the inclusion of covariates based on the observed data, whereas local priors must instead rely
on a prior penalty that encourages non-sparse models.
2.7
Real Data Analysis
2.7.1
Analysis of Polymerase Chain Reaction (PCR) data
Lan et al. (2006) studied coordinated regulation of gene expression levels on 31 female
and 29 male mice (n = 60). A number of psychological phenotypes, including numbers of
36

stearoyl-CoA desaturase 1 (SCD1), glycerol-3-phosphate acyltransferase (GPAT) and phos-
phoenopyruvate carboxykinase (PEPCK), were measured by quantitative real-time RT-PCR,
along with 22,575 gene expression values. The resulting data set is publicly available at http:
//www.ncbi.nlm.nih.gov/geo (accession number GSE3330).
Zhang et al. (2009) used penalized orthogonal components regression to predict the three
phenotypes mentioned above based on the high-dimensional gene expression data. Bondell and
Reich (2012) also used the same data set to examine their model selection procedure based on
penalizing regression coefﬁcients within a (marginal or joint) credible interval obtained from a
ridge-type prior. For brevity, I restrict attention here to SCD1 as the response variable.
Since the ground truth regarding the true signiﬁcant variables is not known for this data, I
compared my approach with a host of competitors on predictive accuracy and parsimony of the
selected model.
Prior to analyses, I standardized the covariates and randomly split the data set into 5 test
samples and 55 training samples to evaluate the out-of-sample mean square prediction error
(MSPE)
MSPE =
X
i2Ttest
(yi −XT
i bβtr
bk )2/|Ttest|,
where Ttest is the index set of the test samples and bβtr
bk is the least square estimator under the
estimated model bk based on the training samples. To avoid sensitivity to a particular split, I
considered 100 replications of the training and test sample generation. To measure the stability
of model selection, I considered the number of variables that were (i) selected at least 95 times,
and (ii) at least once, out of the 100 replicates.
Due to the high-computational burden of the penalized credible interval approach (Bondell
and Reich, 2012), I followed the pre-processing step suggested in that article to marginally
screen variables to reduce to 2000 variables (1999 genes and gender). For all the other ap-
proaches, all 22,575 genes were used. For the nonlocal prior method, I considered both the
37

MAP estimator and the least squares (LS) estimator from the MAP model. For the g-prior, I
set g = p2 as recommended in George and Foster (2000). For the penalized likelihood proce-
dures, I used ten-fold cross validation to choose the tuning parameter.
To choose the hyperparameter ⌧n,p for the nonlocal priors, I used a procedure proposed by
Nikooienejad et al. (2016). That procedure sets the hyperparameter so that the L1 distance
between the posterior distribution on the regression parameters under the null distribution (i.e.,
β = 0) and the nonlocal prior distributions on these parameters is constrained to be less than
a speciﬁed value (e.g., p−1/2). The average value of the hyperparameter values chosen by this
procedure were ⌧n,p = 1.12 and ⌧n,p = 1.16 for piMoM and peMoM priors, respectively.
To make the comparison between the nonlocal priors and the g-prior more transparent, I
used the same beta-binomial prior on the model space in both models, rather than the uniform
prior on the model space described previously. The form of the beta-binomial prior was given
by
⇡(k) / ⇢|k|(1 −⇢)p−|k|I(|k| qn),
(2.12)
with a uniform prior on ⇢and qn = 40. I note that this prior does not strongly induce sparsity as
does, for example, the prior obtained by imposing a Beta(1, pu), u > 1 prior on ⇢, as suggested
in Castillo et al. (2015).
Table 2.2 summarizes the results from the analysis of the gene expression data set. On
average, the nonlocal priors simultaneously produced the lowest MSPE and the most parsimo-
nious model. The other model selection methods selected a wide array of different variables
for different splits of the data set. In particular, Lasso and the penalized credible region ap-
proach selected more than 180 different variables from 100 repeated splits, while the average
size of the selected model was less than 20 and the number of frequently selected variables was
only zero or one, indicating a potentially large number of false positives picked up by these
38

Method
MSPE
MS
FS
TS
piMoM(MAP)
0.283 (0.17)
1.00 (0.00)
1
1
piMoM(LS)
0.282 (0.17)
1.00 (0.00)
1
1
peMoM(MAP)
0.291 (0.18)
1.02 (0.14)
1
2
peMoM(LS)
0.287 (0.17)
1.02 (0.14)
1
2
g-prior
0.368 (0.20)
4.07 (0.56)
1
133
Lasso
0.542 (0.39)
17.97 (8.62)
1
211
Scad
0.308 (0.23)
12.66 (7.62)
2
163
MCP
0.308 (0.21)
2.20 (0.94)
0
29
Marginal(p = 2000)
0.456 (0.40)
17.47 (11.16)
0
273
Joint(p = 2000)
0.440 (0.40)
16.42 (11.06)
1
185
Table 2.2: Analysis of the PCR data. Marginal and Joint refer to the variable selection proce-
dures Bondell and Reich (2012) based on Bayesian marginal credible set and Bayesian joint
credible set, respectively. MS is the average size of the selected model. FS is the number of
frequently selected variables, i.e., that were selected at least 95 times in 100 repetitions. TS
refers to the total number of variables selected at least once from 100 repetitions. Standard
errors are provided in parenthesis.
methods.
2.7.2
A Simulation Study Based on the Boston Housing Data
I next examined the Boston housing data set that contains the median value of owner-
occupied homes in the Boston area, together with several variables that might be associated
with their median value. There were n = 506 median values in the data set, and I considered
10 continuous variables as the predictor variables: crim, indus, nox, rm, age, dis, tax,
ptratio, b, and lstat. This data set has been used to validate a variety of variable selection
methods; some recent examples include Radchenko and James (2011), Yuan and Lin (2005),
and Rockova and George (2014).
To examine the model selection performance in high-dimensional settings, I added 1,000
noise variables that were generated independently from a standard Gaussian distribution (p =
1, 010). The same competitors from the previous subsection were used with the aforementioned
choice of hyperparameters. For nonlocal priors, the hyperparameter value was chosen by the
39

aforementioned procedure Nikooienejad et al. (2016); the average of the chosen hyperparame-
ter values were ⌧n,p = 2.01 and ⌧n,p = 0.47 for piMoM and peMoM priors, respectively. Prior
to analyses, I standardized the covariates and considered a simulation test size of 100 samples.
Methods
MSPE
MS-O
MS-N
FS-O
TS-O
piMoM(MAP)
24.281 (9.01)
5.05 (0.22)
0.01 (0.10)
5
6
piMoM(LS)
24.265 (9.04)
5.05 (0.22)
0.01 (0.10)
5
6
peMoM(MAP)
24.156 (9.02)
5.02 (0.14)
0.00 (0.00)
5
6
peMoM(LS)
24.165 (9.00)
5.02 (0.14)
0.00 (0.00)
5
6
g-prior
26.314 (9.87)
3.10 (0.44)
0.00 (0.00)
3
5
Lasso
30.243 (11.82)
5.07 (0.87)
7.77 (11.16)
4
8
Scad
33.993 (10.66)
5.39 (0.57)
31.60 (28.28)
5
7
MCP
26.191 (9.87)
4.66 (0.74)
0.54 (1.04)
3
6
Marginal
26.612 (10.16)
3.74 (0.88)
0.41 (0.72)
3
7
Joint
26.385 (10.25)
3.77 (0.94)
0.02 (0.20)
3
6
Table 2.3: The Boston Housing data set: MS-O and MS-N refer to the average number of
selected original variables and selected noise variables, respectively. FS-O is the number of
original variables that are frequently selected at least 95 times out of 100 repetitions. TS-O
refers to the number of original variables selected at least once from 100 repetitions.
The results of are analysis are summarized in Table 2.3. The conclusions are similar to those
reported in Section 8.1; the nonlocal priors consistently choose more parsimonious models and
had better predictive performance. The model selection procedure resulting from the nonlocal
prior selects almost the same variables across the 100 repetitions. The average number of the
original variables selected more than 95 times over 100 repetitions is 5, which is close to the
average model size. It is also reliable in the sense that the average number of the original
variables that are selected at least once across the repetitions is only 6. This means that model
selection based on the nonlocal prior selects the same model in most data splits. On the other
hand, penalized likelihood methods such as Lasso and Scad tend to select a large number of
noise variables.
40

2.8
Conclusion
This dissertation described theoretical properties of peMoM and piMoM priors for variable
selection in ultrahigh-dimensional linear model settings. In terms of identifying a “true” model,
selection procedures based on peMoM priors are asymptotically equivalent to piMoM priors
in Johnson and Rossell (2012) because they share the same kernel, exp{−⌧n,p/β2}. I demon-
strated that model selection procedures based on peMoM priors and piMoM priors achieve
model selection consistency in p ≫n settings.
In Section 2.3.1, precision-recall curves were used to show that the model selection pro-
cedure based on a g-prior can achieve nearly the same performance in identifying the MAP
model as nonlocal priors when an optimal value for the hyperparameter g is chosen. However,
as shown in Section 2.3.2, the value of the hyperparameter that maximizes the posterior prob-
ability of the true model is very large and has high variability, which may limit the practical
application of this method. To overcome this problem, one can consider mixtures of g-prior as
in Liang et al. (2008), but the asymptotic behavior of Bayes factor and model selection con-
sistency in ultrahigh-dimensional settings have not been examined for hyper-g priors, and they
are difﬁcult to implement computationally.
In Section 5.5.3, I proposed an efﬁcient and scalable model selection algorithm called S5.
By incorporating the SSS with a screening idea and a temperature control, S5 was able to
accelerate the computation speed without losing the capacity to explore the interesting region
in the model space. Under some simulation settings, it outperformed the SSS in a sense that
not only did S5 search the MAP model much faster than the SSS, but it also found exactly the
same MAP model that was identiﬁed by the SSS.
Because the explicit form of the marginal likelihood of the nonlocal priors is not available,
I used the Laplace approximation throughout this chapter. Barber et al. (2016) studied the ac-
curacy of the approximation in Bayesian high-dimensional variable selection, especially when
41

the dimension of the approximation (which is qn) and n are both increasing. However, their
results do not apply to the case of the nonlocal priors, since the nonlocal priors violate their
regularity condition (nonzero density at the origin). While empirical results in this chapter
and Johnson and Rossell (2012) suggest that the use of the Laplace approximation is reason-
able, in future work it is still worth paying attention to the approximation error of the Laplace
approximation to the marginal likelihood of the nonlocal priors.
The close connection between my methods and the reduced rLasso procedures provides
a useful contrast between Bayesian and penalized likelihood methods for variable selection
procedures. According to the evaluation criteria proposed in Section 2.5, the two classes of
methods appear to perform quite similarly. A potential advantage of the reduced rLasso proce-
dure, and to the lesser extent the rLasso procedure, is reduced computation cost. This advantage
accrues primarily because the reduced rLasso can be computed from the least squares estimate
of each model’s regression parameter, whereas the Bayesian procedures require numerical op-
timization to obtain the maximum a posteriori estimate used in the evaluation of the Laplace
approximation to the marginal density of each model visited. However, the procedures used to
search the model space, given the value of a marginal density or objective function, are approx-
imately equally complex for both classes of procedures. There are also potential advantages
of the Bayesian methods. For example, it is possible to approximate the normalizing constant
of the posterior model probability from the models visited by S5 algorithm, and to use this
normalizing constant to obtain an approximation to the posterior probability assigned to each
model. In so doing, the Bayesian procedures provide a natural estimate of uncertainty associ-
ated with model selection. These posterior model probabilities can also be used in Bayesian
modeling averaging procedures, which have been demonstrated to improve prediction accu-
racy (e.g., Raftery et al. (1997)) over prediction procedures based on maximum a posteriori
estimates. Finally, the availability of prior densities may prove useful in setting model hyper-
parameters (i.e., ⌧n,p) in actual applications, where scientiﬁc knowledge is typically available
42

to guide the deﬁnition of the magnitude of substantively important regression parameters.
I also developed an R package
BayesS5 that provides all computational functions used
in this dissertation, including a support of parallel computing environments. It is available on
the author’s website and on CRAN (https://cran.r-project.org).
43

3.
SIMPLIFIED SHOTGUN STOCHASTIC SEARCH WITH SCREENING
ALGORITHM FOR HIGH-DIMENSIONAL BAYESIAN MODEL SELECTION
3.1
Introduction
In p ≫n settings, full posterior sampling using existing Markov chain Monte Carlo
(MCMC) algorithms is highly inefﬁcient and often not feasible from a practical perspec-
tive. Due to this limitation, several deterministic approaches to ﬁnd the maximum a poste-
riori (MAP) model have been proposed, e.g. Carbonetto and Stephens (2012), Liu and Ihler
(2013) and Rockova and George (2014). However, those procedures only provide a single
model without considering uncertainty on the model space. This lack of assessment of model
uncertainty can be problematic, particularly if one wishes to average over models to improve
prediction performance (Raftery et al., 1997). To overcome this issue and approximate full
posterior model probabilities, I propose a scalable stochastic search algorithm aimed at rapidly
identifying regions of high posterior probability and ﬁnding the MAP) model for linear model
selection problems. My main innovation is to develop a stochastic search algorithm combining
isis-like screening techniques (Fan and Lv, 2008) and temperature control procedure similar to
those used in global optimization algorithms like simulated annealing (Kirkpatrick and Vecchi,
1983).
To describe my proposed algorithm, note that the MAP model bk that can be expressed as
bk = argmax
k2Γ⇤{⇡(k | y)},
(3.1)
where Γ⇤is the set of all models assigned non-zero prior probability.
3.2
Shotgun Stochastic Search Algorithm (SSS)
Hans et al. (2007) proposed the shotgun stochastic search (SSS) algorithm in an attempt
44

to efﬁciently navigate through very large model spaces and identify global maxima. Letting
nbd(k) = {Γ+, Γ−, Γ0}, where Γ+ = {k [ {j} : j 2 kc}, Γ−= {k \ {j} : j 2 k}, and
Γ0 = {[k \ {j}] [ {l} : l 2 kc, j 2 k}, the SSS procedure is described in Algorithm 1.
Algorithm 1 Shotgun Stochastic Search (SSS)
Choose an initial model k(1)
For i = 1 to i = N −1
Compute ⇡(k | y) for all k 2 nbd(k(i))
Sample k+, k−, and k0, from Γ+, Γ−, and Γ0, with probabilities proportional to ⇡(k | y)
Sample k(i+1) from {k+, k−, k0}, with probability proportional to
{⇡(k+ | y), ⇡(k−| y), ⇡(k0 | y)}
The estimated MAP model is deﬁned as the model that achieves the largest posterior prob-
ability among those searched models only.
3.3
Simpliﬁed Shotgun Stochastic Search Algorithm with Screening (S5)
SSS is effective in exploring regions of high posterior model probability, but its compu-
tational cost is still expensive because it requires the evaluation of marginal probabilities for
models in Γ+, Γ−, and Γ0 at each iteration. The largest computational burden occurs for the
evaluation of marginal likelihood for models in Γ0, since |Γ0| = |k|(p −|k|). To improve the
computational efﬁciency of SSS, I propose a modiﬁed version which only examines models in
Γ+ and Γ−. These sets have cardinality p −|k| and |k|, respectively. However, ignoring Γ0
in the sampling updates can make the algorithm less likely to explore “interesting” regions of
high posterior model probability. The algorithm would therefore be more likely to get stuck
in local maxima. To counter this problem, I introduce a “temperature parameter” analogous to
simulated annealing that allows the algorithm to explore a broader spectrum of models.
45

Even though ignoring models in Γ0 reduces the computational burden of the SSS algorithm,
the calculation of p posterior model probabilities in every iteration is still computationally
prohibitive when p is very large. To further reduce the computational burden, I borrow ideas
from the Iterative Sure Independence Screening (isis; Fan and Lv (2008)) and consider only
those variables that have a large correlation with the residuals of the current model. More
precisely, I examine the products |rT
kXj|, where rk is the residual of the model k, for j =
1, . . . , p, after every iteration of the modiﬁed shotgun stochastic search algorithm, and then
restrict attention to variables for which {|rT
kXj| : j = 1, . . . , p} is large (I assume that the
columns of X have been standardized). This yields a scalable algorithm even when the number
of variables p is large.
With these ingredients, I propose a new stochastic model search algorithm called Simpliﬁed
Shotgun Stochastic Search with Screening (S5). This algorithm is described in Algorithm 2.
Algorithm 2 Simpliﬁed Shotgun Stochastic Search with Screening (S5)
Set a temperature schedule t1 > t2 > . . . > tL > 0
Choose an initial model k(1,1) and a set of variables after screening Sk(1,1) based on k(1,1)
For l = 1 in l = L
For i in 1, . . . , J −1
Compute all ⇡(k | y) for all k 2 nbdscr(k(i,l))
Sample k+ and k−, from Γ+
scr and Γ−, with probabilities proportional to ⇡(k | y)1/tl
Sample k(i+1,l) from {k+, k−}, with probability proportional to
{⇡(k+ | y)1/tl, ⇡(k−| y)1/tl}
Update the set of considered variables Sk(i+1,l) to be the union of variables in k(i+1,l) and
the top Mn variables according to {|rT
k(i+1,l)Xj| : j = 1, . . . , p}
In S5, Sk is the union of variables in k and the top Mn variables obtained by screening
using the residuals from model k. The screened neighborhood of model k can be deﬁned as
nbdscr(k) = {Γ+
scr, Γ−}, where Γ+
scr = {k [ {j} : j 2 kc \ Sk}.
46

Even though this algorithm is designed to identify the MAP model, it also provides an ap-
proximation to the posterior model probability of each sampled model. The uncertainty of the
model space can be measured by approximating the normalizing constant from the (unnormal-
ized) posterior probabilities of the models explored by the algorithm.
Denoting the computational complexity of the evaluation of the unnormalized posterior
model probability of the largest model among searched models by En, the computational com-
plexity of the SSS algorithm can be expressed as the product of the number of models explored
by the algorithm and En, which is [O{Np} + O{Nqn} + O{N(p −qn)qn}] ⇥En, where qn is
the maximum size of model among searched models and qn < n ⌧p.
S5 only considers Mn variables after the screening step in each iteration, which dramati-
cally reduces the number of models to be considered in constructing the neighborhood,
O{JL(Mn −qn)} + O(JLMn). Therefore, the resulting computational complexity is
[O{JL(Mn −qn)} + O(JLMn)] ⇥En + O(JLnp),
where qn < Mn. When the computational complexity for screening steps, O(JLnp), is domi-
nated by the other terms, the computational complexity is almost independent of p. As a result,
the proposed algorithm is scalable in the sense that the resulting computational complexity is
typically robust to the size of p.
3.4
Performance Comparisons Between S5 and SSS
I examined the computational efﬁciency of S5 to SSS in identifying the MAP model under
a piMOM prior with ⌧n,p = log n log p and r = 1. I generated data according to Case (1) in
Section 2.3 with a ﬁxed sample size (n = 200) and a varying number of covariates p. I set
Mn = 20, L = 20, and J = 20 for S5. To match the total number of iterations between
S5 and SSS, I set N = 400 for SSS. All computations were implemented in R on a machine
containing 16 CPU cores (Intel(R) Xeon(R) CPU E5-2690 @ 2.90GHz with 64GB of DDR3
47

p
log (seconds)
100
200
500
1000
2000
1
2
3
4
5
6
7
3.9
9
60.3
148.4
403.4
1359.7
seconds
S5
SSS
(a)
p
log (# of models)
100
200
500
1000
2000
5
6
7
8
9
10
11
181.3
1636
4447.1
38561.1
# of models
(b)
Figure 3.1: (a) Average computation time to ﬁrst ﬁnd the MAP model; (b) Average number of
models searched before hitting the MAP model. The left y-axis is on a logarithmic scale and
the right y-axis is on the raw scale.
@ 1600Mhz).
Figure 3.1 shows the average computation time and the number of models searched before
hitting the MAP model for the ﬁrst time for the S5 and SSS algorithms. All averages were
based on 100 simulated datasets, and both algorithms found the same MAP model in all data
sets. Panel (a) shows that the computation time of SSS increases roughly at a p2 rate, but that
the computation time for S5 was nearly independent of the number of covariates p (about 4
seconds). For example, when p = 2, 000, SSS ﬁrst found the MAP model in an average of
1,360 seconds (about 23 minutes), whereas S5 hit the MAP model after about only 4 seconds.
Interestingly, panel (b) of Figure 3.1 shows that the S5 algorithms explored only 181 models
on average before ﬁnding the MAP model, whereas SSS typically visited slightly more than
38,000 models. Thus, not only is S5 much faster than SSS in identifying the MAP model, but
it also visited far fewer models before visiting the MAP model.
To see how sensitive the efﬁciency of the S5 algorithm is to the choice of the screening set
48

screening size
correlation
0.99
5
10
20
50
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3.2: Correlation between the top 10 posterior model probabilities estimated from SSS
and S5 with different screening set sizes.
size, Figure 3.2 reports the average correlation between the top 10 posterior model probabilities
approximated from S5 and SSS with varying screening set sizes. This ﬁgure shows that even
when the screening set size is small in comparison to the true model size (|t| = 5), the correla-
tion of the top 10 posterior model probabilities from S5 and SSS is at least 0.99 (the horizontal
green line in the ﬁgure is located on 0.99). Thus, the resulting posterior model probabilities are
almost same as those of SSS. At least in this example, S5 is not sensitive to the choice of the
screening set size. However, for real data sets, I recommend examining output from multiple
screening set sizes.
3.4.1
Application to Real Data Examples
In this subsection, I apply the S5 algorithm to Bardet-Biedl syndrome gene expression data
that was ﬁrst reported in Scheetz et al. (2006). The data set contains microarrays expression
values from eye tissue of 120 twelve-week old male rats. a total of 31,042 different probe sets
were used to analyze the RNA values from the tissue. The intensity values were normalized
using the robust multi-chip averaging method (Irizarry et al., 2003). This microarray data set
49

has been considered in multiple papers, including Huang et al. (2008), Kim et al. (2008) and
Fan et al. (2011). As in those papers I am interested in ﬁnding a subset of the probe sets that
are associated with the probe set is 1389163_at, which corresponds to the expression of gene
TRIM32. This gene is related to Bardet-Biedl syndrome, a hereditary disease of the retina.
The data set was ﬁrst ranked all other probes according to the absolute value of the marginal
correlation to 1389163_at and selected the top 200 probes (n = 120 and p = 200). The
screened data set is available in the R package flare.
I also considered a simulated data set based on the Boston housing data set that was used
in Section 2.7.2 by adding 1000 spurious variables to the original Boston housing data set
(n = 506 and p = 1010).
For S5 and SSS, the settings usedin the previous simulation study section were again used.
I repeatedly ran S5 and SSS for 30 replicates starting from different initial models. Table
3.1 reports the average time of the computation (Time) over 30 replicates, the logarithm of
the (unnormalized) posterior probability of the MAP model found by each algorithm (Log-
post), and the average number of models searched to ﬁnd the MAP model by each algorithm
(Avg.#models). S5 found exactly the same MAP model searched by SSS for each data set, and
the computation time of S5 is much shorter than SSS. For the Bardet-Biedl syndrome data, S5
is 29 times faster than SSS and 84 times faster for the Boston housing data. Moreover, S5 ﬁnds
the MAP model after visiting far fewer models than SSS. This shows that S5 very efﬁciently
explores the model space.
Bardet-Biedl Syndrome Data
Boston Housing Data
Method
Time (sec)
Log-post
Avg.#models
Time (sec)
Log-post
Avg.#models
S5
61.4
199.746
1198.0
54.6
-1115.34
442.9
SSS
1767.4
199.746
21423.8
4569.8
-1115.34
40406.9
Table 3.1: Comparisons between S5 and SSS using the Bardet-Biedl syndrome data and the
Boston housing data.
50

3.5
R Package: BayesS5
In this section, I provide tutorials about how to use an R package called BayesS5 (Shin
and Tian, 2017) to implement the S5 algorithm for high-dimensional Bayesian model selection
problems. This package is available from CRAN (https://cran.r-project.org). In
the following subsections, I provide examples that demonstrate how to use the package in
several model selection problems.
To illustrate the use of the package, I consider the Boston data set that was used in Section
2.7.2. To examine high-dimensional settings, I added 500 spurious variables to the original
data set, so that p = 510 and n = 506. The following code imports the data set in R.
R> library(BayesS5); library(MASS)
R> data(Boston); attach(Boston)
R> X = cbind(crim,indus,nox,rm,age,dis,tax,ptratio,black,lstat)
R> X = scale(X)
R> y = medv; y = y-mean(y)
R> n = nrow(X)
R> set.seed(291287)
R> X = cbind(X,matrix(rnorm(500*n),n,500)); X = scale(X)
R> p = ncol(X)
3.5.1
S5 Function
Without specifying the priors, the simplest implementation with the default setting can be
conducted with the following command:
R> fit_default = S5(X,y)
The default setting is the piMOM prior in (2.4) for regression coefﬁcients and the beta-
uniform prior in (2.12). The hyperparameter value of the piMOM prior is automatically chosen
51

by the procedure proposed in Nikooienejad et al. (2016). The default setting for iterations is
Mn = 20, L = 20, and J = 20, and the default choice of temperature schedule is the square
inverse of the equi-spaced sequence from 0.4 to 1 with size 20. For every transition between
temperatures, the function prints out the current status of the model selection. An example of
this output looks like this:
[1] "#################################"
[1] "Inverse Temperature"
[1] 0.16
[1] "The Selected Variables in the Searched MAP Model"
[1]
3
4
6
8 10
[1] "The Evaluated Object Value at the Searched MAP Model"
[1] -1111.231
[1] "Current Model"
[1]
4
8 10
[1] "The Evaluated Object Value at the Current Model"
[1] -1118.82
[1] "The Number of Total Searched Models"
[1] 341
During the run, the S5 function outputs the inverse of the current temperature used in
the algorithm, and it provides the MAP model and its (unnormalized) log-posterior model
probability, log mk(y) + log ⇡(k). For example, in the above output, the model of 3, 4, 6, 8
and 10 indicates the model deﬁned by the third, fourth, sixth, eighth and tenth covariates, and
−1111.231 is the (unnormalized) log-posterior model probability of the model {3, 4, 6, 8, 10}.
The "Current Model" is the model that the algorithm is currently visiting. Its log-posterior
model probability is also listed. The ﬁnal line of the output is the number of models that have
been searched by the algorithm.
52

The object of S5 (e.g. fit_default in the above code) contains the list of the searched
models that are identiﬁed by binary vectors and their (unnormalized) log-posterior model prob-
abilities. To approximate the full posterior model probabilities of each model, one extra step is
required:
R> res_default = result(fit_default)
[1] "# of Searched Models by S5"
[1] 1291
[1] "The MAP model is "
[1]
3
4
6
8 10
[1] "with posterior probability 0.739"
The result shows that the MAP model is {3, 4, 6, 8, 10} and its posterior model probability
is 73.9%. In this case, the selected model does not include any of the 500 spurious variables that
are generated independently from the response variable. The total number of models explored
by S5 was 1,291. The result function also provides the marginal inclusion probabilities for
each variable. These probabilities are deﬁned as
qj =
X
k:j2k
⇡(k | y),
for j = 1, . . . , p. The command to generate these values is given by
R> mar_default = res_default$marg.prob
R> print(which(mar_default>0.5))
[1]
3
4
6
8 10
R> plot(mar_default, ylim=c(0,1), xlab="covariate index",
ylab="marginal inclusion prob", pch=3)
Figure 3.3 is the output of the above command presenting the marginal inclusion probabil-
53

0
100
200
300
400
500
0.0
0.2
0.4
0.6
0.8
1.0
covariate index
marginal inclusion prob
Figure 3.3: Marginal inclusion probabilities approximated by S5 for the synthesized Boston
housing data set.
ities.
I note that full posterior model probabilities searched by the S5 algorithm can be calculated
by using the result function. The below code provides the top three models that have highest
posterior model probabilities.
R> gam_default = res_default$gam
R> post_default = res_default$post
R> round(post_default[1:3], 3)
[1] 0.739 0.192 0.012
R> which(gam_default[,1] == 1)
[1]
3
4
6
8 10
R> which(gam_default[,2] == 1)
[1]
3
4
6
8
9
10
R> which(gam_default[,3] == 1)
[1]
3
4
6
8
9
10
179
54

In the above code, gam_default contains the binary vectors that identify the correspond-
ing models (1 indicates the corresponding variable is in the model, and 0 means it is not in
the model). The object post_default stores the posterior probabilities of models identi-
ﬁed by binary vectors in gam_default. As shown in the previous code, the MAP model
is {3, 4, 6, 8, 10} with 73.9% posterior probability, and the second most signiﬁcant model is
{3, 4, 6, 8, 9, 10} with probability 19.2%. The third highest posterior model probability model
is {3, 4, 6, 8, 10, 179} with 1.2% posterior probability. This model includes a spurious variable
X179. The S5 algorithm searched a total 1, 291 models; the posterior probabilities of models
not visited are approximate by 0.
The S5 package also provides other priors for Bayesian model selection procedure. These
includes the peMOM priors in (2.2) and Zellner’s g-prior. For example, the g-prior can be
applied to S5 by the following code:
R> tuning = p^2 # tuning parameter g for g-prior
R> ind_fun = ind_fun_g # choose g-prior for the regression coef
R> model = Uniform #choose the uniform model prior
R> fit_g = S5(X,y,ind_fun=ind_fun,model=model,tuning=tuning)
3.5.2
S5_parallel Function for Parallel Computing Environments
The S5 algorithm is efﬁcient and fast in exploring the model space. However, it may not
be fast enough to implement in practice when the data set is high-dimensional and variables
are highly correlated. To overcome this problem, it is reasonable to use multiple independent
chains to search the model space. The S5 parallel function permits S5 to be ran in parallel com-
puting environments. The following command can be used to implement the parallel version
of S5 using 20 cores.
55

R> NC = 20 # the number of cores that will be used
R> fit_parallel = S5_parallel(NC=NC, X, y)
R Version:
R version 3.3.3 (2017-03-06)
snowfall 1.84-6.1 initialized (using snow 0.4-2):
parallel execution on 20 CPUs.
Library Matrix loaded.
Library Matrix loaded in cluster.
user
system elapsed
0.090
0.004 153.129
Stopping cluster
R> res_parallel = result(fit_parallel)
[1] "# of Searched Models by S5"
[1] 6840
[1] "The MAP model is "
[1]
3
4
6
8 10
[1] "with posterior probability 0.736"
In the single processor version of S5, 1, 291 models were visited. In this 20 CPU appli-
cation, 6, 840 models were visited. This is more than ﬁve times the number of visited models
using the same amount of real time. The MAP model found by the parallel version is exactly
the same with the MAP model by the standard S5, and its posterior probability is 73.6%; this
is slightly smaller than the 73.9% that was estimated from the single chain. The code to extract
these results follows:
R> gam_parallel = res_parallel$gam
R> post_parallel = res_parallel$post
R> round(post_parallel[1:3], 3)
[1] 0.736 0.191 0.012
56

R> which(gam_parallel[,1] == 1)
[1]
3
4
6
8 10
R> which(gam_parallel[,2] == 1)
[1]
3
4
6
8
9
10
R> which(gam_parallel[,3] == 1)
[1]
3
4
6
8
9
10
179
57

4.
FUNCTIONAL HORSESHOE PRIOR FOR NONPARAMETRIC SUBSPACE
SHRINKAGE
4.1
Introduction
Since the seminal work of James and Stein (1961), shrinkage estimation has been im-
mensely successful in various statistical disciplines and continues to enjoy widespread atten-
tion. Many shrinkage estimators have a natural Bayesian ﬂavor. For example, one obtains the
ridge regression estimator as the posterior mean arising from an isotropic Gaussian prior on
the vector of regression coefﬁcients (Jeffreys, 1961; Hoerl and Kennard, 1970). Along similar
lines, an empirical Bayes interpretation of the (positive part) James–Stein estimator can be ob-
tained (Efron and Morris, 1973). Such connections have been extended to the semiparametric
regression context, with applications to smoothing splines and penalized splines (Wahba, 1990;
Ruppert et al., 2003). Over the past decade and a half, a number of second-generation shrink-
age priors have appeared in the literature for application in high-dimensional sparse estimation
problems. Such priors can be almost exclusively expressed as global-local scale mixtures of
Gaussians (Polson and Scott, 2010a); examples include the relevance vector machine (Tip-
ping, 2001), normal/Jeffrey’s prior (Bae and Mallick, 2004), the Bayesian Lasso (Park and
Casella, 2008; Hans, 2009), the horseshoe priors (Carvalho et al., 2010), normal/gamma and
normal/inverse-Gaussian priors (Caron and Doucet, 2008; Grifﬁn and Brown, 2010), general-
ized double Pareto priors (Armagan et al., 2013) and Dirichlet–Laplace priors (Bhattacharya
et al., 2015). These priors typically have a large spike near zero with heavy tails, thereby pro-
viding an approximation to the operating characteristics of sparsity inducing discrete mixture
priors (George and McCulloch, 1997; Johnson and Rossell, 2012). For more on connections
between Bayesian model averaging and shrinkage, refer to Polson and Scott (2010a).
A key distinction between ridge-type shrinkage priors and the global-local priors is that
58

while ridge-type priors typically shrink towards a ﬁxed point–most commonly the origin– the
global-local priors shrink towards the union of subspaces consisting of sparse vectors. The
degree of shrinkage to sparse models is controlled by certain hyperparameters (Bhattacharya
et al., 2015). In this dissertation, I further enlarge the scope of shrinkage prior by imposing a
class of functional shrinkage priors, called the functional horseshoe priors (fHS). fHS priors
facilitate shrinkage towards pre-speciﬁed subspaces. The shrinkage factor (deﬁned in Section
3) is assigned a Beta(a, b) prior with a, b < 1, which has the shape of a horseshoe prior (Car-
valho et al., 2010). While the horseshoe prior shrinks towards sparse vectors, the proposed fHS
prior enforces functions to shrink towards arbitrary subspaces.
To illustrate the proposed methodology, consider a nonparametric regression model with
unknown regression function f : X ! R given by
Y = F + ",
" ⇠N(0, σ2In),
(4.1)
where Y = {y1, . . . , yn}T, F = {f(x1), . . . , f(xn)}T = E(Y | x), and the covariates xi 2
X ⇢R.
In (4.1), one can either make parametric assumptions (e.g., linear or quadratic dependence
on x) regarding the shape of f, or one may model it nonparametrically using splines, wavelets,
Gaussian processes, etc. Scatter plots or goodness of ﬁt tests can be used to ascertain the
validity of a linear or quadratic model in (4.1), but such procedures are only feasible in rela-
tively simple settings. In complex and/or high dimensional problems, there is clearly a need
for an automatic data-driven procedure to adapt between models of varying complexity. With
this motivation, the fHS priors encourage shrinkage towards a parametric class of models em-
bedded inside a larger semiparametric model, as long as a suitable projection operator can be
deﬁned. For example, in (4.1), f will be shrunk towards a linear or quadratic function if such
parametric assumptions are supported by the data, and will remain unshrunk otherwise. As
59

noted already, my approach is not limited to the univariate regression context and can be ex-
tended to the varying coefﬁcient model (Hastie and Tibshirani, 1993), density estimation via
log-spline models (Kooperberg and Stone, 1991) and additive models (Hastie and Tibshirani,
1986), among others. Further details are provided in Section 4.4. In the additive regression
context, the proposed approach performs well compared to state-of-the-art procedures like
Sparse Additive Model (SpAM) of Ravikumar et al. (2009) and High-dimensional Generalized
Additive Model (HGAM) by Meier et al. (2009).
I provide theoretical justiﬁcation for the method by showing an adaptive property of the ap-
proach in the context of (4.1). Speciﬁcally, I show that the posterior contracts at the parametric
rate if the true function belongs to the pre-designated subspace, and contracts at the optimal
rate for ↵-smooth functions otherwise. In other words, my approach adapts to the paramet-
ric shape of the unknown function while allowing deviations from the parametric shape in a
nonparametric fashion.
4.2
Preliminaries
I begin by introducing some notation. For ↵> 0, let b↵c denote the largest integer smaller
than or equal to ↵and d↵e denote the smallest integer larger than or equal to ↵. Let C↵[0, 1]
denote the Hölder class of ↵smooth functions on [0, 1] that have continuously differentiable
derivatives up to order b↵c, with the b↵cth order derivative being Lipschitz continuous of
order ↵−b↵c. For a vector x 2 Rd, let
11x
11 denote its Euclidean norm. For a function
g : [0, 1] ! R and points x1, . . . , xn 2 [0, 1], let
11g
112
2,n = n−1 Pn
i=1 g2(xi); I shall refer to
11 ·
11
2,n as the empirical L2 norm. For an m ⇥d matrix A with m > d and rk(A) = d, let
L(A) = {Aβ : β 2 Rd} denote the column space of A, which is a d-dimensional subspace of
Rm. Let QA = A(ATA)−1AT denote the projection matrix on L(A).
60

4.3
Functional Horseshoe Prior
In the nonparametric regression model in (4.1), I model the unknown function f as spanned
by a set of pre-speciﬁed basis functions {φj}1jKn as follows:
f(x) =
Kn
X
j=1
βjφj(x).
(4.2)
I work with B-spline basis functions (De Boor, 1978) for illustrative purpose here. However,
the methodology generalizes to a larger class of basis functions. The details about B-spline
basis functions were described in Section 1.1.2. Letting β = {β1, . . . , βKn}T denote the vector
of basis coefﬁcients and Φ = {φj(Xi)}1in,1jKn denote the n ⇥Kn matrix of B-spline
basis functions evaluated at the observed covariates. Model (4.1) can then be expressed as
Y | β ⇠N(Φβ, σ2In).
(4.3)
A standard choice for a prior on β is a g-prior β ⇠N(0, g(ΦTΦ)−1) (Zellner, 1986). The
g-priors have been commonly used in linear models since they incorporate the correlation
structure of the covariates inside the prior variance. The posterior mean of β with a g-prior
can be expressed as {1−1/(1+g)}bβ, where bβ = QΦY is the maximum likelihood estimate of
β. Thus, the posterior mean shrinks the maximum likelihood estimator towards zero, with the
amount of shrinkage controlled by the parameter g. Bontemps (2011) studied asymptotic prop-
erties of the resulting posterior by providing bounds on the total variation distance between the
posterior distribution and a Gaussian distribution centered at the maximum likelihood estima-
tor with the inverse Fisher information matrix as covariance. In his work, the g parameter was
ﬁxed a priori depending on the sample size n and the error variance σ2. The results in par-
ticular imply minimax optimal posterior convergence for ↵-smooth functions. Among related
work, Ghosal and van der Vaart (2007) established minimax optimality with isotropic Gaussian
61

priors on β.
My goal is to deﬁne a broader class of shrinkage priors on β that facilitate shrinkage towards
a null subspace that is ﬁxed in advance, rather than shrinkage towards the origin or any other
ﬁxed a priori guess β0. For example, if I have a priori belief that the function is likely to attain
a linear shape, then I would like to impose shrinkage towards the class of linear functions. In
general, my methodology allows shrinkage towards any null subspace spanned by the columns
of a null regressor matrix Φ0, with d0 = rank(Φ0) equal to the dimension of the null space.
For example in the linear case, I deﬁne the null space as L(Φ0) with Φ0 = {1, x} 2 Rn⇥2,
where 1 is a n ⇥1 vector of ones and d0 = 2. Shrinkage towards quadratic, or more generally
polynomial, regression models are achieved similarly.
With the above ingredients, I propose the fHS prior through the following conditional spec-
iﬁcation:
⇡(β | ⌧)
/
(⌧2)−(Kn−d0)/2 exp
⇢
−
1
2σ2⌧2β
TΦ
T(I −Q0)Φβ
5
,
(4.4)
⇡(⌧)
/
(⌧2)b−1/2
(1 + ⌧2)(a+b)1(0,1)(⌧),
(4.5)
where a, b > 0. Recall that Q0 = Φ0(ΦT
0Φ0)−1ΦT
0 denotes the projection matrix of Φ0.
When Φ0 = 0, (4.4) is equivalent to a g-prior with g = ⌧2. The key additional feature in
my proposed prior is he introduction of the quantity (I −Q0) in the exponent, which enables
shrinkage towards subspaces rather than a single point. Although the proposed prior may be
singular, it follows from subsequent results that the joint posterior of (β, ⌧2) is proper. Note that
the prior on the scale parameter ⌧follows a half-Cauchy distribution when a = b = 1/2. Half-
Cauchy priors have been recommended as a default prior choice for global scale parameters
in the linear regression framework (Polson and Scott, 2012). Using the reparameterization
! = 1/(1 + ⌧2), the prior in (4.5) can be interpreted as the prior induced on ⌧through a
Beta(a, b) prior on !. I work in the ! parameterization for reasons to be evident shortly.
62

Exploiting the conditional Gaussian speciﬁcation, the conditional posterior of β is also
Gaussian, and can be expressed as
β | Y, ! ⇠N(eβ!, e⌃!),
(4.6)
where
eβ! =
✓
Φ
TΦ +
!
1 −!Φ
T(I −Q0)Φ
◆−1
Φ
TY,
e⌃! = σ2
✓
Φ
TΦ +
!
1 −!Φ
T(I −Q0)Φ
◆−1
.
(4.7)
I now state a lemma which delineates the role of ! as the parameter controlling the shrinkage.
Lemma 1. Suppose that L(Φ0) ( L(Φ). Then,
E [Φβ | Y, !] = Φeβ! = (1 −!)QΦY + !Q0Y,
where QΦ is the projection matrix of Φ.
The above lemma shows that the conditional posterior mean of the regression function
given ! is a convex combination of the classical B-spline estimator QΦY and the parametric
estimator Q0Y . The parameter ! 2 (0, 1) controls the shrinkage effect; the closer ! is to 1, the
greater the shrinkage towards the parametric estimator. I learn the parameter ! from the data
with a Beta(a, b) prior on !. The hyperparameter b < 1 controls the amount of prior mass near
one.
Figure 4.1 illustrates the connection between the choice of the hyperparameters a and b and
the shrinkage behavior of the prior. The ﬁrst and the second column in Figure 4.1, with a ﬁxed
at 1/2 shows that the prior density of ! increasingly concentrates near 1 as b decreases from
1/2 to 10−1. The third column in Figure 4.1 depicts the prior probability that ! > 0.95 and
63

0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
ω
density
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
ω
density
0.5
0.4
0.3
0.2
0.1
0.0
0.0
0.2
0.4
0.6
0.8
1.0
b
probability
Figure 4.1: The ﬁrst two columns illustrate the prior density function of ! with different hyper-
parameters (a, b): (1/2, 1/2) for the ﬁrst column and (1/2, 10−1) for the second column. The
third column shows the prior probability that ! > 0.95 (solid line) and ! < 0.05 (dotted line)
for varying b and a ﬁxed a = 1/2.
! < 0.05. Clearly, as b decreases, the amount of prior mass around one increases, which results
in stronger shrinkage towards the parametric estimator. In particular, when a = b = 1/2, the
resulting “horseshoe" prior density derives its name from the shape of the prior on ! (Carvalho
et al., 2010).
When L(Φ0) ( L(Φ), one can orthogonally decompose QΦ = Q1+Q0, where the columns
of Q1 are orthogonal to Q0, i.e., QT
1Q0 = 0. For L(Φ0) ( L(Φ), this follows because we can
use Gram-Schmidt orthogonalization to create eΦ = [Φ0; Φ1] of the same dimension as Φ with
ΦT
1Φ0 = 0 and L(Φ) = L(eΦ). Let Q1 denote the projection matrix on L(Φ1). Simple algebra
shows that
⇡(! | Y ) =
Z
⇡(!, β | Y )dβ = ⇡(!)
m(Y )
Z
f(Y | β, !)⇡(β | !)dβ
= !a+(Kn−d0)/2−1(1 −!)b−1 exp{−Hn!}/m(Y ),
(4.8)
where Hn = Y TQ1Y/(2σ2) and m(Y ) =
R 1
0 !a+(Kn−d0)/2−1(1 −!)b−1 exp {−Hn!} d!.
To investigate the asymptotic behavior of the resulting posterior, it is crucial to ﬁnd tight
two-sided bounds on m(Y ). Such bounds are speciﬁed in Lemma 2.
64

Lemma 2. (Bounds on the normalizing constant) Let An and Bn be arbitrary sequences satis-
fying An ! 1 as n ! 1 and Bn = O(1).
Let tn =
R 1
0 !An−1(1 −!)Bn−1 exp{−Hn!}d!. Then,
Γ(An)Γ(Bn)
Γ(An + Bn) exp{−Hn}(1 + QL
n) tn Γ(An)Γ(Bn)
Γ(An + Bn) exp{−Hn}(1 + QU
n ),
where,
QU
n
=
Bn
An + Bn
exp(Hn),
QL
n
=
BnHn
An + Bn
+ DBn(Bn + Tn)−An
(An + Bn)3/2
-
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+ ,
Tn = max{A2
n, 3 dHne} and D is some positive constant.
By setting An = a + Kn/2 and Bn = b, Lemma 2 shows that the magnitude of the normal-
izing constant m(Y ) in (4.8) is determined by an interplay between the relative sizes of b and
exp(Hn). When b is small enough to dominate exp(Hn), m(Y ) ⇡Be(a+Kn/2, b) exp(−Hn),
where Be(·, ·) denotes the beta function. Otherwise, ignoring polynomial terms, m(Y ) ⇡
Be(a + Kn/2, b)b. This asymptotic behavior of m(Y ) is the key ingredient to identify the
posterior contraction rate of the fHS prior. I also note that the magnitude of a does not affect
the strength of shrinkage for large n as long as a is a ﬁxed constant, since the prior contribution
!a−1 is dominated by the likelihood contribution !Kn/2.
4.3.1
Posterior Concentration Rate
I ﬁrst state a set of assumptions that have been used by others (Zhou et al., 1998; Claeskens
et al., 2009) to prove minimax optimality of B-spline estimators. Assume that the following
conditions hold:
65

(A1). Let u = max1j(Kn−1)(tj+1 −tj). There exists a constant C > 0, such that
u/ min1j(Kn−1)(tj+1 −tj) C and u = o(K−1
n ).
(A2). There exists some distribution function G with a positive continuous density such
that
sup
x2[0,1]
|Gn(x) −G(x)| = o(K−1
n ),
where Gn is the empirical distribution of the covariates {xi}1in, which are assumed to be
ﬁxed by design.
Under (A1) and (A2), Zhou et al. (1998) showed that the mean square error of the B-spline
estimator QΦY achieves the minimax optimal rate. If the true function f0 2 C↵[0, 1] is ↵-
smooth and the number of basis functions Kn ⇣n1/(2↵+1), then Zhou et al. (1998) shows
that
E0
h11QΦY −F0
112
2,n
i
= O
-
n−2↵/(1+2↵).
,
(4.9)
where E0(·) represents an expectation with respect to the true data generating distribution of
Y .
I now state main results on the posterior contraction rate of the functional horseshoe prior.
Theorem 6. Consider the model (4.1) equipped with the functional horseshoe prior (4.4)-(4.5).
Assume (A1) and (A2) hold and L(Φ0) ( L(Φ). Further assume that for some integer ↵≥1,
the true regression function f0 2 C↵[0, 1] and the B-spline basis functions Φ are constructed
with Kn −b↵c knots and b↵c −1 degree, where Kn ⇣n1/(1+2↵). Suppose that the prior
hyperparameters a and b in (4.5) satisfy a 2 (δ, 1 −δ) for some constant δ 2 (0, 1/2), and
66

Kn log Kn ≺−log b ≺(nKn)1/2. Then,
E0
h
P
n11Φβ −F0
11
2,n > Mn(f0)1/2 | Y
oi
= o(1),
(4.10)
where
Mn(f0) =
8
>
>
<
>
>
:
⇣nn−1, if F0 2 L(Φ0),
⇣nn−2↵/(1+2↵) log n, if F T
0 (I −Q0)F0 ⇣n,
and ⇣n can be any arbitrary sequence that diverges to inﬁnity as n tends to 1.
Theorem 6 exhibits an adaptive property of the fHS prior.
If the true function is ↵-
smooth, then the posterior contracts around the true function at the near minimax rate of
n−↵/(2↵+1) log n. However, if the true function F0 belongs to the ﬁnite dimensional subspace
L(Φ0), then the posterior contracts around F0 in the empirical L2 norm at the parametric
1/pn rate. I note that the bound Kn log Kn ≺−log b ≺(nKn)1/2 is key to the adaptiv-
ity of the posterior, since the strength of the shrinkage towards L(Φ0) is controlled by b. If
−log b ≺Kn log Kn, then the shrinkage towards L(Φ0) is too weak to achieve the paramet-
ric rate when F0 2 L(Φ0). On the other hand, if −log b ≻(nKn)1/2, the resulting posterior
distribution would strongly concentrate around L(Φ0), and it would fail to attain the optimal
nonparametric rate of posterior contraction when F0 62 L(Φ0).
I ignore the subspace of functions such that {F 2 Rn : F T(I−Q0)F = o(n), F 62 L(Φ0)}.
I only focus on the function space that can be strictly separated from the null space L(Φ0).
However, I acknowledge that it would be meaningful to illustrate the shrinkage behavior when
the regression function f approaches the null space in the sense that F T(I −Q0)F/n ! 0 as
n ! 1.
67

4.4
Simulation Studies for Univariate Examples
In this section, I examine the performance of the functional horseshoe prior on various
simulated data sets. I consider three models as follows:
(i) simple regression model:
Yi = f(xi) + ✏i
(4.11)
(ii) varying coefﬁcient model:
Yi = wif(xi) + ✏i
(4.12)
(iii) density function estimation:
p(Yi) =
exp{f(Yi)}
R
exp{f(t)}dt.
(4.13)
In scenario (i) and (ii), ✏i
i.i.d
⇠N(0, σ2) for i = 1, . . . , n. In (iii), p(·) is the density function of Y .
The varying coefﬁcient model (Hastie and Tibshirani, 1993) in (4.12) reduces to a linear model
when the coefﬁcient function f is constant, and the density function p is Gaussian when the
log-density function f is quadratic in the log-spline model in (4.13); (Kooperberg and Stone,
1991). These facts motivate the use of the fHS prior in these examples to shrink towards the
respective parametric alternatives. For each setting, I considered the case corresponding to the
relevant parametric model.
For (i) and (ii), I generated the covariates independently from a uniform distribution be-
tween −⇡and ⇡and set the error variance σ2 = 1. For each scenario (i) - (iii), I considered
three parametric choices for f. For scenario (i), I considered f to be linear, quadratic, and sinu-
soidal. For (ii), I considered constant, quadratic and sinusoidal functions. For (iii), I considered
normal, log-normal and mixture of normal distributions. For the ﬁrst two cases, I standardized
the true function so as to obtain a signal-to-noise ratio of 1.0.
I used the B-spline basis with Kn = 8 in (4.2) to model the function f in each setting.
To shrink the regression function in (4.11) towards linear subspaces, I set Φ0 = {1, x} in
the fHS prior (4.4). For the varying coefﬁcient model (4.12), I set Φ0 = {1} to shrink f
towards constant functions, whence the resulting model reduces to a linear regression model.
68

Table 4.1: Results of univariate examples
True function
Method
n = 200
n = 500
n = 1000
Linear
fHS
0.93 (0.81)
0.44 (0.45)
0.17 (0.17)
B-spline
3.57 (1.60)
1.54 (0.74)
0.76 (0.38)
Quadratic
fHS
3.63 (1.73)
1.55 (0.74)
0.77 (0.37)
B-spline
3.59 (1.60)
1.56 (0.74)
0.78 (0.38)
Sine
fHS
3.64 (1.58)
1.50 (0.74)
0.75 (0.36)
B-spline
3.57 (1.60)
1.53 (0.74)
0.76 (0.38)
Constant
fHS
0.13 (0.15)
0.06 (0.08)
0.03 (0.04)
B-spline
1.33 (0.63)
0.48 (0.26)
0.25 (0.13)
Quadratic
fHS
1.35 (0.62)
0.51 (0.27)
0.27 (0.13)
B-spline
1.36 (0.64)
0.51 (0.26)
0.27 (0.13)
Sine
fHS
1.35 (0.63)
0.48 (0.26)
0.25 (0.13)
B-spline
1.33 (0.63)
0.48 (0.26)
0.25 (0.13)
Normal
fHS
1.34 (1.35)
0.59 (0.52)
0.35 (0.31)
B-spline
10.30 (5.00)
3.68 (1.42)
1.96 (0.77)
Log-normal
fHS
5.15 (2.70)
3.35 (1.14)
2.91 (0.98)
B-spline
6.37 (4.21)
3.27 (1.86)
2.83 (1.14)
Mixture
fHS
4.42 (2.18)
1.79 (0.85)
1.04 (0.39)
B-spline
5.31 (3.61)
1.85 (0.93)
1.04 (0.39)
Finally, I set Φ0 = {1, Y, Y 2} to shrink f towards the space of quadratic functions in (4.13),
which results in the density p being shrunk towards the class of Gaussian distributions. I
note that the prior for p in (4.13) is data-dependent. An inverse-gamma prior with parameters
(1/100, 1/100) was imposed on σ2 for the fHS prior in (i) and (ii). In all three examples, I
set b = exp{−Kn log n/2} to satisfy the conditions of Theorem 6 and arbitrarily set a = 1/2.
Although Theorem 6 only applies to the regression model (4.11), the empirical results for these
hyperparameter choices are promising for the varying coefﬁcient model and the log-density
model as well.
I imposed Jeffrey’s prior, ⇡(β, σ2) / 1/σ2, on the B-spline coefﬁcients for the simple
regression model and the varying coefﬁcient model as a competitor to the fHS prior. Following
Ghosal et al. (2008), I assigned independent U(−⇡, ⇡) priors on the B-spline coefﬁcients,
69

which are known to guarantee the minimax rate of posterior convergence rate for the log-
density model. For each prior, I used the posterior mean ˆf as a point estimate for f, and report
the empirical Mean Square Error (MSE), i.e.
11 bf −f
112
n,2.
In Table 4.1, I report 100 times MSE of the posterior mean estimator and its standard
deviation over 100 replicates in estimating the unknown function f for all three models, for
sample sizes n = 200, 500, and 1000. The ﬁrst top three rows are for the simple regression
model; the second three rows for the varying coefﬁcient model; the last three rows for the
density estimation. “Mixture" in the last row indicates a mixture of Gaussian densities as
0.3N(2, 1) + 0.7N(−1, 0.5). In all three settings, when the true function f belongs to the
nominal parametric class, the posterior mean function resulting from the functional horseshoe
prior clearly outperforms the B-spline prior. When the true function does not belong to the
parametric model, the functional horseshoe prior performs comparably to the B-spline prior.
Figure 4.2 depicts the point estimate (posterior mean) and pointwise 95% credible bands
for the unknown function f for a single data set for each of the three examples when the true
function belongs to the parametric class; that is, a linear function in (4.11), a constant function
in (4.12), and a quadratic function in (4.13). Figure 4.3 depicts the corresponding estimates
when the data generating function does not fall into the assumed parametric class. It is evident
from Figure 4.2 that when the parametric assumptions are met, the fHS prior performs similarly
to the parametric model. THis fact empirically corroborates my ﬁndings in Theorem 6 that the
posterior contracts at a near parametric rate when the parametric assumptions are met. It is also
evident that the fHS procedure automatically adapts to deviations from the parametric assump-
tions in Figure 4.3, again conﬁrming the conclusion of Theorem 6 that when the true function is
well-separated from the parametric class, the posterior concentrates at a near optimal minimax
rate. I reiterate that the same hyperparameters a = 1/2 and b = exp{−Kn log n/2} for the
fHS prior were used in the examples in Figure 4.2 and Figure 4.3.
70

−3
−2
−1
0
1
2
3
−2
0
2
4
X
y
−3
−2
−1
0
1
2
3
−2
0
2
4
X
y
−3
−2
−1
0
1
2
3
−2
0
2
4
X
y
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
y
Density
−3
−2
−1
0
1
2
3
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
y
Density
−3
−2
−1
0
1
2
3
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
y
Density
−3
−2
−1
0
1
2
3
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Figure 4.2: Examples when the underlying true functions are parametric. Posterior mean of
each procedure (red solid), its 95% pointwise credible bands (red dashed), and the true function
(black solid) from a single example with n = 200 for each model. The top row is for the
simple regression model; the second row is for the varying coefﬁcient model; the last row is
for the density estimation. The Bayesian B-spline procedure, the Bayesian parametric model
procedure, and functional horseshoe priors are illustrated in the ﬁrst, second, and third columns,
respectively.
71

−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
4
X
y
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
4
X
y
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
4
X
y
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x
y/w
y
Density
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
y
Density
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
y
Density
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 4.3: Examples when the underlying true functions are nonparametric. Posterior mean
of each procedure (red solid), its 95% pointwise credible bands (red dashed), and the true
function (black solid) from a single example with n = 200 for each model. The top row is
for the simple regression model; the second row is for the varying coefﬁcient model; the last
row is for the density estimation. The Bayesian B-spline procedure, the Bayesian parametric
model procedure, and functional horseshoe priors are illustrated in the ﬁrst, second, and third
columns, respectively.
72

4.5
Applications to Additive Models
My regression examples in the previous subsection involved one predictor variable. In
the case of multiple predictors, a popular modeling framework is the class of additive models
(Hastie and Tibshirani, 1986), where the unknown function relating p candidate predictors to a
univariate response is modeled as the sum of p univariate functions, with the jth function only
dependent on the jth predictor Xj = {x1j, . . . , xnj}. In this section, I apply the fHS prior to
additive models and compare results obtained under this prior to several alternative methods.
To be consistent with my previous notation, I express additive models as
Y =
p
X
j=1
Fj + ✏,
(4.14)
where Fj = {fj(x1j), . . . , fj(xnj)} for j = 1, . . . , p and ✏⇠N(0, σ2In). I let Φj denote the
spline basis matrix for Xj and let βj = {βj1, . . . , βjKn} denote the corresponding coefﬁcient.
In general, each component function can be modeled nonparametrically, for example, using the
B-spline basis functions as described in the previous section, fj(x) = PKn
l=1 βjlφl(x) = Φjβj
for j = 1, . . . , p. However, if there are many candidate predictors, then nonparametrically
estimating p functions may be statistically difﬁcult and in addition, may result in a loss of
precision if only a small subset of the variables are signiﬁcant. With this motivation, I extend
the fHS framework to additive models, where I assign independent fHS priors to the fj’s with
Q0 = 0 in (4.4) to facilitate shrinkage of each of these functions towards the null function.
Therefore, the resulting prior speciﬁcation can be expressed as
⇡(β | ⌧2, σ2)
/
exp
(
−1
σ2
p
X
j=1
βT
j ΦT
j Φjβj
⌧2
j
)
⇡(⌧j)
/
(⌧2
j )b−1/2
(1 + ⌧2
j )(a+b)1(0,1)(⌧j),
73

for j = 1, . . . , p. This prior imposes a shrinkage effect on each
11fj
112
2,n = βT
j ΦT
j Φjβj towards
the null function. Thus, the resulting posterior distribution of Fj concentrates on the zero
function when the marginal effect of Fj is negligible.
4.5.1
A Comparison to the Standard Horseshoe Prior
For the additive model in (4.14), one can impose a product of standard horseshoe (HS)
priors (Carvalho et al., 2010) on the spline coefﬁcients as
⇡(β | λ, w, σ2)
/
exp
(
−
1
σ2λ2
p
X
j=1
Kn
X
l=1
β2
jl
 2
jl
)
λ
⇠
C+(0, 1)
 jl
⇠
C+(0, 1),
(4.15)
where C+(0, 1) is the half-Cauchy distribution and βjl is the l-th spline coefﬁcient for the com-
ponent function of the j-th covariate for j = 1, . . . , p and l = 1, . . . , Kn. Polson and Scott
(2010b) states that this prior imposes global-local shrinkage rules. The parameter λ serves a
global shrinkage parameter controlling the concentration near zero, while the  jl’s are local
shrinkage parameters that control the tail heaviness of the individual coefﬁcients. The use of
the standard horseshoe prior would impose strong shrinkage effects towards zero on each co-
efﬁcient, but it does not take into account the grouping structure in the spline expansions of the
components. From a frequentist perspective, this issue was addressed in Huang et al. (2010).
There, the authors stated that the standard Lasso (Tibshirani, 1996) with sparsity constraints
on individual marginal coefﬁcients is not appropriate for additive models. The group Lasso
penalties (Yuan and Lin, 2006) on the spline coefﬁcients achieve much better performance in
prediction and model selection compared to the standard Lasso in such settings. The same
illustration can be applied to the Bayesian additive model. The use of the standard horseshoe
prior might degrade the estimation performance due to the ignorance of the grouping structure
74

in the spline coefﬁcients. In the following sections, I provide simulated and real examples
where the standard horseshoe prior does not perform well for additive models, but the proce-
dure based on the fHS prior shows excellent performances compared to other state-of-the-art
methods.
4.5.2
Simulation Studies
For additive models, Ravikumar et al. (2009) proposed penalized likelihood procedures
called Sparse Additive Models (SpAM) that combine ideas from model selection and additive
nonparametric regression. The penalty term of SpAM can be described as a weighted group
Lasso penalty (Yuan and Lin, 2006) in which the coefﬁcients for each component function fj
for j = 1, . . . , p are forced to simultaneously shrink towards zero. Meier et al. (2009) proposed
High-dimensional Generalized Additive Model (HGAM) that differs from SpAM because its
penalty term imposes both shrinkage towards zero and regularization on the smoothness of
the function. Huang et al. (2010) introduced a two step procedure of adaptive group Lasso
(AdapGL) for additive models. The ﬁrst step estimates the weight of the group penalty, and the
second step applies it to the adaptive group Lasso penalty. Since the performance of penalized
likelihood methods is sensitive to the choice of the tuning parameter, in the simulation studies
that follow I considered two criterion for tuning parameter selection: AIC and BIC. R packages
SAM, hgam, and grpLasso were used to implement SpAM, HGAM, and AdapGL, respec-
tively. I also considered the standard HS prior. Its computation was implemented by the R
package monomv. For the fHS prior and the HS prior, I imposed a prior on ⇡(σ2) proportional
to 1/σ2. I used 20, 000 samples from the MCMC algorithms after 10, 000 burn-in iterations to
estimate the posterior mean estimator.
I deﬁne the signal-to-noise ratio as SNR = Var(f(X))/V ar(✏), where f is the true under-
lying regression function, and I examine the same simulation scenarios that were considered in
Meier et al. (2009) as follows:
75

Scenario 1: (p = 200, SNR ⇡15). This is the same as Example 1 in Meier et al. (2009). A
similar scenario was also considered in Härdle et al. (2012) and Ravikumar et al. (2009). The
true model is
Yi = f1(xi1) + f2(xi2) + f3(xi3) + f4(xi4) + ✏i,
where ✏i
i.i.d
⇠N(0, 1) for i = 1, . . . , n, with
f1(x)
=
−sin(2x), f2(x) = x2 −25/12, f3(x) = x,
f4(x)
=
exp{−x} −2/5 · sinh(5/2).
The covariates are independently generated from a uniform distribution between −2.5 to 2.5.
Scenario 2: (p = 80, SNR ⇡7.9). This is equivalent to Example 3 in Meier et al. (2009) and
similar to an example in Lin and Zhang (2006). The true model is
Yi = 5f1(xi1) + 3f2(xi2) + 4f3(xi3) + 6f4(xi4) + ✏i,
where ✏i
i.i.d
⇠N(0, 1.74) for i = 1, . . . , n, with
f1(x)
=
x, f2(x) = (2x −1)2, f3(x) =
sin(2⇡x)
2 −sin(2⇡x),
f4(x)
=
0.1 sin(2⇡x) + 0.2 cos(2⇡x) + 0.3 sin2(2⇡x)
+0.4 cos3(2⇡x) + 0.5 sin3(2⇡x).
The covariate xj = {x1j, . . . , xnj}T for j = 1, . . . , p is generated by xj = (Wj + U)/2, where
W1, . . . , Wp and U are independently simulated from U(0, 1) distributions.
76

Scenario 3 (p = 60, SNR ⇡11.25). This scenario is equivalent to Example 4 in Meier
et al. (2009), and a similar example was also considered in Lin and Zhang (2006). The same
functions and the same process to generate the covariates used as in Setting 2 were used in this
scenario. The true model is
Yi
=
f1(xi1) + f2(xi2) + f3(xi3) + f4(xi4)
+1.5f1(xi5) + 1.5f2(xi6) + 1.5f3(xi7) + 1.5f4(xi8)
+2.5f1(xi9) + 2.5f2(xi10) + 2.5f3(xi11) + 2.5f4(xi12) + ✏i,
where ✏i
i.i.d
⇠N(0, 0.5184) for i = 1, . . . , n.
To evaluate the estimation performance of the fHS prior, I report the MSE for each method.
To measure the performance of variable selection, I examined the proportion of times the true
model was selected, as well as Matthews correlation coefﬁcient (MCC; Matthews (1975)),
deﬁned as,
MCC =
TP · TN −FP · FN
(TP + FP)(TP + FN)(TN + FP)(TN + FN),
where TP, TN, FP, and FN denote the number of true positive, true negatives, false positives,
false negatives, respectively. MCC is generally regarded as a balanced measure of the perfor-
mance of classiﬁcation methods, which simultaneously takes into account TP, TN, FP, and FN.
I note that MCC is bounded by 1, and the closer MCC is to 1, the better the model selection
performance is.
For variable selection using the fHS prior and the HS prior, I used 95% pointwise credible
bands for each component function to exclude component functions whose credible bands
uniformly contained the zero function on the entire support of the corresponding covariate. To
77

n
log MSE
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
−2
−1
0
1
2
n
MCC
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
n
Proportion
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
n
log MSE
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
−2
−1
0
1
2
n
MCC
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
n
Proportion
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
n
log MSE
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
−2
−1
0
1
2
n
MCC
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
n
Proportion
●
●
●
●
●
●
●
●
●
●
200
300
400
500
600
0.0
0.2
0.4
0.6
0.8
1.0
●
Partial−Oracle
fHS
HS
SpAM
HGAM
AdapGL
Figure 4.4: The ﬁrst column illustrates the logarithm of the MSE of each method; the second
column displays the MCC; the third column is the proportion of times the each procedure se-
lected the true model. The top row, the middle row, and the bottom row represent the Scenario
1, Scenario 2, and Scenario 3, respectively. For penalized likelihood methods, AIC (black) and
BIC (grey) were used to choose the tuning parameter.
78

investigate the performance achieved by the proposed method, I compared it to a “partial oracle
estimator". The partial oracle estimator refers to the B-spline least squares estimator when the
variables in the true model are given, but the true component functions in the additive model
are not provided.
Results from a simulation study to compare these methods are depicted in Figure 4.4. In
all three settings,the procedure based on the fHS prior has smaller MSE than the estimator
based on the horsehoe prior and the penalized likelihood estimators. The proposed procedure
also provides comparable or better variable selection than the other methods. I note that the
SpAM procedure with tuning parameter selected by BIC provides comparable variable selec-
tion performance to the fHS prior in Scenario 1, yet its MSE is at least 8 times larger than that
of the procedure based on the fHS prior (note that the reported results are on the logarithmic
scale). The results suggest that the fHS prior provides improvement over the penalized like-
lihood methods in terms of both MSE and model selection performance in these simulation
scenarios.
4.5.3
Real Data Analysis: Boston Housing Data and Ozone Data
In this section, I apply the functional horseshoe prior to two well known data sets: the ﬁrst
concerns ozone levels and the second considers housing prices in Boston. Both data sets are
available in the R package mlbench. These two data sets have been previously analyzed by
various researchers, including Buja et al. (1989), Breiman (1995), Lin and Zhang (2006) and
Xue (2009). Following the pre-processing step in Xue (2009), I standardized both the response
and independent variables prior to my analyses.
I ﬁrst consider the Boston housing data set that contains the median value of 506 owner-
occupied homes in the Boston area, together with several variables that might be associated
with the median value. To examine the performance of my method in eliminating extraneous
predictors, I added 40 spurious variables generated as i.i.d. standard Gaussian deviates. Using
79

the standard notation for the variable in this data set, I then assumed a model of the following
form:
medv
=
β0 + f1(crim) + f2(indus) + f3(nox) + f4(rm) + f5(age) + f6(dis)
+f7(tax) + f8(ptratio) + f9(b) + f10(lstat) + ✏,
where ✏⇠N(0, σ2In). Each component function was modeled by the B-spline bases with
Kn = 8. Fifty test data points were randomly selected to estimate the out-of-sample prediction
error. Five hundreds simulations of each procedure were used to generate the plots in Table
4.2.
I also modeled the ozone data set using each of the procedures that were applied the housing
data. The ozone data consists of the daily maximum one-hour-average ozone readings and nine
meteorological variables for 330 days in the Los Angeles basin in 1976. The model applied to
these data can be expressed as follows:
ozone
=
β0 + f1(height) + f2(wind) + f3(humidity) + f4(temp1)
+f5(temp2) + f6(inv height) + f7(gradient) + f8(inv temp)
+f9(visibility) + ✏.
Like the Boston Housing data case, I added 40 spurious variables generated as i.i.d. standard
Gaussian deviates. I used B-spline bases with Kn = 5 to model the component functions. I
performed a cross-validation experiment to assess the predictive performance of the compet-
ing methods. In each of 500 simulated data sets, I held out 30 data values as the test set and
used the remaining observations to estimate the model. The parameter settings described in
Section 4.5.2 were again used for the functional horseshoe prior. Also, for each training data
set I generated 30, 000 posterior samples by following the MCMC algorithm described in the
80

Table 4.2: Results of real data examples
Boston Housing Data
Method
Test Error
NN
Selected Model
Original
0.156(0.065)
fHS
0.154(0.067)
0.00
crim, nox, rm, dis, ptratio, lstat
HS
0.180(0.081)
0.00
crim, nox, rm, dis, ptratio, lstat
SpAM(AIC)
0.224(0.072)
21.06
All
SpAM(BIC)
0.344(0.093)
2.00
crim, nox, rm, dis, ptratio, lstat
HGAM(AIC)
0.212(0.095)
37.49
All
HGAM(BIC)
0.222(0.115)
1.06
indus, nox, age, dis, tax, ptratio
AdaptGL(AIC)
0.579(0.214)
40.00
All
AdaptGL(BIC)
0.218(0.144)
4.17
nox, rm, dis, tax, ptratio, lstat
Ozone Data
Original
0.311(0.085)
fHS
0.278(0.092)
0.02
temp2, gradient
HS
0.294(0.296)
0.00
temp2
SpAM(AIC)
0.427(0.156)
20.67
All but height and inv temp
SpAM(BIC)
0.624(0.213)
0.07
temp1, temp2, gradient
HGAM(AIC)
0.298(0.109)
23.12
All but gradient
HGAM(BIC)
0.631(0.260)
0.208
humidity, temp1
AdaptGL(AIC)
0.359(0.131)
21.91
All but height and inv temp
AdaptGL(BIC)
0.341(0.142)
2.252
humidity, temp1, temp2, inv height,
gradient, visibility
Appendix, and only the last 20, 000 samples were used in the analysis. I compared the perfor-
mance of the procedure based on the proposed priors with that of SpAM, HGAM, AdapGL and
the classical B-spline estimator. The classical B-spline estimator was ﬁt without the spurious
noise variables. For the penalized likelihood methods, AIC and BIC were used to choose tun-
ing parameters. Table 4.2 displays the average of test set errors, the average number of selected
noise variables, and the most frequently selected model for each method.
In Table 2, “Test Error" refers to the average of empirical L2 test errors, and “NN" rep-
resents the averaged number of selected spurious variables. “Original" indicates the B-spline
least square estimator from the original model without spurious variables. Table 2 shows that
81

for both data sets the procedure based on the fHS prior achieved the smallest test errors, and it
also selected the minimum number of spurious variables. Moreover, its test error was smaller
than that of the original estimator that was estimated without the spurious variables. The HS
prior also selects parsimonious models in the sense that the average number of selected spuri-
ous variables is close to zero, but its prediction error is much larger than the fHS prior. For both
data sets, the model selected by the fHS prior was similar to that chosen by SpAM with BIC.
However, the test error of the SpAM procedure was roughly twice that of fHS. More generally,
the fHS procedure outperformed all of the other procedures in these examples.
4.6
Conclusion
I have proposed a class of shrinkage priors which I call the functional horseshoe priors.
When appropriate, these priors imposes strong shrinkage towards a pre-speciﬁed class of func-
tions. The shrinkage term in this prior is new. It directly allows the nonparametric function to
shrink towards parametric functions. By so doing, it preserves the minimax optimal parametric
rate of posterior convergence n−1/2 when the true underlying function is parametric, and it also
comes within O(log n) of achieving the minimax nonparametric rate when the true function is
strictly separated from the class of parametric functions.
The novel shrinkage term contained in the proposed prior, F T(I −Q0)F (i.e., (4.4)), can
be naturally applied to a new class of penalized likelihood methods having a general form
expressible as
−l(Y | F) + pλ
-
F
T(I −Q0)F
.
,
where l(Y | F) is the logarithm of a nonparametric likelihood function and pλ is the penalty
term. In contrast to other penalized likelihood methods, this form of penalty allows shrinkage
towards the space spanned by a projection matrix Q0, rather than simply a zero function.
82

5.
NONLOCAL FUNCTIONAL PRIORS FOR NONPARAMETRIC HYPOTHESIS
TESTING AND HIGH-DIMENSIONAL MODEL SELECTION
5.1
Introduction
Consider the following nonparametric additive model (Hastie and Tibshirani, 1986) that
was discussed in Chapter 4; i.e., for a response variable y = {y1, . . . , yn} and the covariates
X = {X1, . . . , Xp},
y =
p
X
j=1
fj(Xj) + ✏,
where ✏⇠N(0, σ2In), and fj is the j-th marginal regression function. Also, Xj is the j-th
covariate for j = 1, . . . , p. I assume that some of the functions fj are nonzero and the rest are
zero functions.
For additive models, signiﬁcant progress in selecting a subset of variables has been made
over the past decades under high-dimensional settings. From a frequentist perspective, this
problem has been examined in Ravikumar et al. (2009), Meier et al. (2009), and Huang et al.
(2010). Theoretical properties of associated estimation properties have been investigated in
Raskutti et al. (2012) and Yuan and Zhou (2016). In a Bayesian framework, Shang and Li
(2014) investigated asymptotic properties of high-dimensional model selection procedures de-
ﬁned by Gaussian priors.
From a Bayesian perspective, Choi et al. (2009) investigated the asymptotic property of
nonparametric Bayesian testing procedure. More recently, Choi and Rousseau (2015) studied a
Bayesian hypothesis test on the regression function in partially linear models using a Gaussian
process prior and showed its consistency. However, neither Choi et al. (2009) or Choi and
Rousseau (2015) provided the exact convergence rate of the evidence when the null hypothesis
83

is true.
In this dissertation, I propose new classes of prior densities called nonlocal functional prior
densities for nonparametric Bayesian hypothesis testing problems, and I apply the proposed
prior to the model selection procedure for additive models under high-dimensional settings. I
investigate theoretical properties of the resulting Bayesian model selection procedure and show
its model selection consistency under high-dimensional settings when the number of covariates
p increases at an sub-exponential rate of n.
The proposed prior densities are a novel extension of nonlocal priors (Johnson and Rossell,
2010) to nonparametric settings. For parametric models, a family of nonlocal prior densities as-
signs negligible density around the null value of the parameter, and Johnson and Rossell (2010)
showed that the Bayes factor based on the nonlocal priors penalizes the alternative hypothesis
at a faster rate than that of local prior density functions that are strictly positive at the null value,
when the data-generating process is consistent with the null hypothesis. For high-dimensional
linear model selection problems, Johnson and Rossell (2012) and Shin et al. (2017) provided
desirable theoretical properties of model selection procedures based on nonlocal priors. How-
ever, their extension to nonparametric models has been hindered because it is not clear how to
deﬁne the null space to construct nonlocal prior densities on the space of functions due to the
fact that the null hypothesis is composite for nonparametric hypothesis tests.
I ﬁrst introduce a novel discrepancy quantity between the null space of functions and the
function objective to be inferred, and construct nonlocal functional prior densities based on the
null space deﬁned by the discrepancy. I then show the theoretical properties of the Bayesian
hypothesis test (model selection) based on the proposed priors for univariate nonparametric
regression models. I derive the asymptotic rate of the Bayes factor in favor of the alternative
based on the local prior densities, e.g. g-priors Zellner (1986), and show that it diminishes
only at a polynomial rate under a true null, but increases at an exponential rate of the sample
size under a true alternative. On the other hand, when data sets are generated from the null
84

hypothesis, the Bayes factor based on the proposed prior densities not only achieve a faster
rate than that from existing local priors, but also can attain a sub-exponential rate, rather than
a polynomial rate, under some conditions. I provide description of these properties detailed in
Section 5.3. I also discuss some applications of the proposed priors and test their ﬁnite sample
behavior by simulation studies in Section 5.4.
I apply the proposed nonlocal functional prior density to additive model selection problems
under high-dimensional settings. In Section 5.5.1 I show that the resulting model selection pro-
cedure is consistent in the sense that the posterior model probability of the true data-generating
model converges to one in probability under mild regularity conditions. In Section 5.5.2, I
also provide a convergence rate of the logarithm of posterior model probabilities deﬁned by
the nonlocal functional priors, and I show that this rate can be decomposed as a sum of the
logarithm of posterior model probabilities from local priors (e.g. Gaussian priors) and an ad-
dtional penalty term on the model. It is shown that the additional penalty term is adaptively
determined by the marginal effect of the B-spline estimator, and this property explains why
the model selection procedure based on the proposed priors outperforms the other methods in
simulation studies and real data examples.
Choosing an appropriate hyperparameter for the prior densities is important when imple-
menting a Bayesian models selection (hypothesis testing). In Section 5.5.5, I propose a practi-
cal procedure to choose the hyperparameter of the nonlocal functional priors by comparing the
null distribution and the prior density of the discrepancy measure. For computation, I describe
a scalable algorithm that is a modiﬁed version of the Simpliﬁed Shotgun Stochastic Search
with Screening (S5) (Shin et al., 2017). Originally, S5 was designed to efﬁciently explore the
space of linear models. Here, I modify the S5 to be suitable for nonparametric additive model
selection. All computational functions used in this dissertation are available in the R package
BayesS5.
85

5.2
Bayesian Nonparametric Hypothesis Testing Procedures
To illustrate the idea of nonlocal functional priors, I assume the nonparametric univariate
regression model with the regression function f can be expressed as
yi = f(xi) + ✏i,
(5.1)
where ✏i
i.i.d
⇠N(0, σ2) for i = 1, . . . , n. I suppose that the predictor is compactly supported,
and assume without loss of generality that xi 2 [0, 1] for each i. I denote y = {yi}i=1,...,n,
x = {xi}T
i=1,...,n and F = f(x) = {f(x1), . . . , f(xn)}T. For simplicity, I assume that σ2 is
known. By using B-spline basis functions, I model F = PKn
j=1 βjφj(x) = Φβ, where φj denote
the j-th B-spline basis for j = 1, · · · , Kn, β = {β1, . . . , βKn}, and Φ is the n ⇥Kn matrix of
the B-spline bases.
I aim to test if the regression function F belongs to a certain class of parametric functions
that can be linearly spanned by a design matrix Φ0 with a dimension d0. So, the null space of the
corresponding hypothesis test can be deﬁned as L(Φ0) = {F 2 Rn : F = Φ0↵for some ↵2
Rd0}. For example, any linear function of x can be expressed as a1 + bx for some a, b 2 R,
where 1 indicates the vector with all entries of 1, which means that the null space of a linearity
test of F is deﬁned by L(Φ0) = {F : F = Φ0↵for some ↵2 R2} with Φ0 = {1, x}.
A general class of hypothesis tests on F to examine if F belongs to a pre-speciﬁed class of
parametric functions can be deﬁned as
H0 : F 2 L(Φ0)
vs.
H1 : F /2 L(Φ0).
(5.2)
A common Bayesian hypothesis testing procedure is based on Bayes factor (Jeffreys, 1961)
which measures the evidence in favor of the alternative hypothesis. Given the hypotheses in
(5.2), the Bayes factor in favor of H1, denoted by B10(y), is deﬁned by the ratio of the marginal
86

likelihoods of the null and the alternative, m0(y) and m1(y), respectively. This is expressible
as
B10(y) = m1(y)
m0(y) =
R
L(y | β, H1)⇡1(β)dβ
R
L(y | ↵, H0)⇡0(↵)d↵,
where L(y|·) is the likelihood function resulting from each hypothesis, and ⇡0 and ⇡1 denote
the prior densities under the null and the alternative hypothesis. Note that the large value
of B10(y) indicates a strong evidence in support of H1 while values closer to zero indicate
evidence in favor of the null hypothesis.
I note that F T(I−Q0)F = 0 if and only if F 2 L(Φ0), where Q0 is the projection matrix of
the null regressor Φ0, i.e., Q0 = Φ0(ΦT
0Φ0)−1ΦT
0. By using this fact, I redeﬁne the hypothesis
tests in (5.2) as
H0 : F
T(I −Q0)F = 0
vs.
H1 : F
T(I −Q0)F 6= 0.
Under H0, the resulting model is thus the parametric regression model with F = Φ0↵for
some
↵2 Rd0. By using the semi-norm F T(I −Q0)F, I consider the null space of the
hypothesis as {F 2 Rn : F T(I −Q0)F = 0}.
In testing a point null hypothesis of a scalar-valued parameter, i.e., H0 : ✓= 0 versus
H1 : ✓6= 0 for some parameter ✓2 R, Johnson and Rossell (2010) pointed out that the Bayes
factor based on local prior densities in favor of H0 is Op(n−1/2) when the true parameter is the
null value. However, when data are generated from alternative hypotheses, the Bayes factor in
favor of H1 increases at an exponential rate of n as discussed in Walker (2004). Johnson and
Rossell (2010) showed that the convergence rate of the Bayes factor based on the nonlocal prior
densities can be more equitably balanced under the true null and true alternative hypotheses. In
Section 5.3, I observed the similar imbalance of the asymptotic rate of the Bayes factor from
87

the local prior even in nonparametric Bayesian testing. To ameliorate the asymmetry of the
convergence rate, I extend the original idea of nonlocal prior densities to functional spaces and
introduce nonlocal functional prior densities on the coefﬁcient β deﬁned as follows:.
Deﬁnition 3. Let Q0 be the projection matrix of the null regressor Φ0. If for any ✏> 0, there
exists δ > 0 such that ⇡(β) < ✏for any β with βTΦT(I −Q0)Φβ < δ, then I deﬁne ⇡(β) to be
a nonlocal functional prior density.
In contrast, local prior densities have strictly positive values of βTΦT(I −Q0)Φβ even on
the null space {β 2 RKn : βTΦT(I −Q0)Φβ = 0}. I propose a nonlocal functional prior
density ⇡NL(β) as the product of a nonlocal kernel h(β) and a local prior density ⇡L(β) as
⇡NL(β) = E⇡L{h(β)}−1h(β)⇡L(β),
(5.3)
where E⇡L(·) denotes the expectation with respect to the local prior density ⇡L. Also, the
nonlocal kernel h(β) satisﬁes the condition that for any ✏> 0, there exists δ > 0 such that
h(β) < ✏for any β with βTΦT(I −Q0)Φβ < δ, so the resulting prior ⇡NL attains the nonlocal
property.
In this dissertation, I consider Gaussian priors as the local base priors to deﬁne nonlocal
prior densities as in (5.3), which can be expressed as
⇡L(β) ⇠N
-
µ, σ2⌃n
.
,
(5.4)
where µ 2 RKn and ⌃n are the mean and the covariance of the Gaussian distribution, respec-
tively.
Under the alternative hypothesis, a natural choice of the local prior is Zellner’s g-prior
Zellner (1986) that is a special case of (5.4) with µ = 0 and ⌃n = gn(ΦTΦ)−1 with a hy-
perparameter gn. For the null hypothesis, I also impose a g-prior, N{0, σ2gn(ΦT
0Φ0)−1} on
88

the regression parameters in (5.1). The resulting marginal likelihood is m0(y) =
R
L(y |
↵, σ2, H0)⇡0(↵)d↵. In the same way, the marginal likelihood of the alternative hypothesis is
deﬁned as m1(y) =
R
L(y | β, σ2, H1)⇡1(β)dβ, where ⇡1 is the prior on the B-spline coefﬁ-
cients. I denote the marginal density as mL
1 (y) under the alternative hypothesis if the prior on
coefﬁcients is local, and mNL
1
(y) if the prior is nonlocal.
I propose two classes of nonlocal functional prior densities deﬁned from the following
nonlocal kernels in (5.3).
First, r-th moment functional prior densities can be deﬁned by the nonlocal kernel
hr
M(β | r) = {β
TΦ
T(I −Q0)Φβ}r,
(5.5)
so the resulting nonlocal functional prior density is
⇡Mr(β | σ2, µ, ⌃n) / {β
TΦ
T(I −Q0)Φβ}r exp
⇢
−1
2σ2(β −µ)
T⌃−1
n (β −µ)
5
.
(5.6)
Second, I introduce inverse moment functional prior densities by applying the nonlocal
kernel
hI(β | ⌧n) = exp
⇥
−σ2⌧n{β
TΦ
T(I −Q0)Φβ}−1⇤
,
(5.7)
to obtain the nonlocal density function
⇡I(β | σ2, ⌧n, µ, ⌃n) / exp
⇢
−1
2σ2(β −µ)
T⌃−1
n (β −µ) −
σ2⌧n
βTΦT(I −Q0)Φβ
5
.
(5.8)
89

5.3
Convergence Rates of Bayes Factor
5.3.1
Preliminaries
I deﬁne some notation that will be used in the following sections. For sequences an and bn,
an ⪯bn and an ≺bn indicate bn = O(an) and bn = o(an), respectively, and an ⇣bn means
that an = O(bn) and bn = O(an).
I deﬁne the functional space C↵[0, 1] to be the space of ↵0 times continuously differentiable
functions f with ||f||↵< 1, where ↵0 is the greatest integer less than ↵and the semi-norm
|| · ||↵is deﬁned by
kfk↵=
sup
{(x,w):x6=w}
|f (↵0)(x) −f (↵0)(w)|
|x −w|↵−↵0
.
I also deﬁne the empirical L2 norm as ||f||2
n,2 = Pn
i=1 f 2(xi)/n for some function f.
Let P0 denote the probability measure that generates data y under the null hypothesis,
having true regression function f0 and F0 = {f0(x1), . . . , f0(xn)}T. Let Eβ|y(·) denote the
expectation operator with respect to the posterior distribution of β induced by the local prior
⇡L. Let E⇡L(·) indicate the expectation operator with respect to the local prior ⇡L.
5.3.2
Local Priors
I now state a theorem that demonstrates the convergence rate of Bayes factor based on local
priors (5.4).
Theorem 7. Consider the nonparametric regression model (5.1) and a hypothesis test on the
regression function in (5.2). Suppose that the prior on the B-spline coefﬁcients is the local
prior in (5.4) with µ = 0 and ⌃n = gn(ΦTΦ)−1 under the alternative hypothesis, and consider
a g-prior on the coefﬁcients with a hyper parameter gn for the null hypothesis. Assume that σ2
is known and L(Φ0) ( L(Φ). Then for any diverging sequence vn ! 1,
P0
====log
⇢mL
1 (y)
m0(y)
5
−Tn
==== > {2F
T
0 (QΦ −Q0)F0/σ2 + Kn −d0}1/2vn
>
= o(1),
90

where mL
1 (y) is the marginal likelihood based on the local prior under the alternative hypoth-
esis and
Tn = −Kn −d0
2
log(1 + gn) +
gn
2(1 + gn)
?
F
T
0 (QΦ −Q0)F0/σ2 + Kn −d0
 
,
where d0 = rank(Q0).
Theorem 7 states that asymptotic behavior of the Bayes factor that is derived from the local
prior densities is determined by the interplay between F T
0 (QΦ −Q0)F0, Kn and gn. Under H0,
the fact that F T
0 (QΦ −Q0)F0 = 0 implies that the logarithm of the Bayes factor approximately
concentrates on −(Kn −d0) log(1 + gn)/2 + gnKn/{2(1 + gn)}, and this quantity would be
dominated by the ﬁrst term −(Kn−d0) log(1+gn)/2 when gn ≻Kn. On the other hand, under
H1 there exists a constant δ such that F T
0 (QΦ −Q0)F0/n > δ for any n, so the convergence
rate of the Bayes factor is dominated by F T
0 (QΦ −Q0)F0/(2σ2) ⇣n when Kn log gn ≺n.
When gn = O(n) as recommended in Zellner (1986) and George and Foster (2000), this
means that the asymptotic behavior of the Bayes factor in favor of alternative hypotheses can
be summarized as follows.
• For a true null hypothesis, the Bayes factor in favor of the alternative hypothesis de-
creases only at rate Op(n−(Kn−d0)/2)
• For a true alternative hypothesis, the Bayes factor in favor of the alternative hypothesis
increases at rate Op(exp{cn}) for some constant c.
Because Kn should be chosen to be much smaller than n, these Bayes factor rates imply
that the resulting hypothesis testing procedure highly tends to provide stronger evidence in
favor of a true alternative hypothesis than it does for a true null hypothesis.
These asymptotic results are similar to those obtained from the Bayesian parametric hy-
pothesis tests using local prior densities on a scalar-valued parameter as described in Bahadur
91

and Bickel (1967), Walker (1969), and Johnson and Rossell (2010). The Bayes factor in favor
of alternative hypotheses, when data are generated under an alternative hypothesis, increases
exponentially fast. The Bayes factor, when the null hypothesis is true, decreases only at a
polynomial rate of n. In nonparametric hypothesis testing, Choi et al. (2009) and Choi and
Rousseau (2015) provided similar results of consistency of Bayes factor for semiparametric
regression model and partially linear models. Yet neither article discussed the convergence
rate of Bayes factor under the true null. Scott and Walker (2015) also derived a similar rate of
Bayes factor for a monotonicity test for regression function.
As discussed in Rossell and Telesca (2017), the Bayes factor based on nonlocal priors
can be decomposed into a product between the Bayes factor deﬁned by a local prior and the
ratio between the posterior expectation and the prior expectation of a nonlocal kernel h. In
other words, the Bayes factor BF NL
10 (y) based on the nonlocal prior derived from (5.3) can be
expressed as
BF NL
10 (y) = mNL
1
(y)
m0(y) = mL
1 (y)Dn(h; y)
m0(y)
= BF L
10(y)Dn(h; y),
(5.9)
where BF L
10(y) be the Bayes factor resulting from the local prior in (5.4), i.e., BF L
10(y) =
mL
1 (y)/m0(y), and Dn(h; y) = Eβ|y{h(β)}/E⇡L{h(β)}. Recall that two classes of the nonlo-
cal kernel h are introduced in (5.5) and (5.7). The decomposition in (5.9) means that the Bayes
factor BF NL
10 (y) based on the nonlocal prior with the nonlocal kernel h only differs from the
Bayes factor based on the local prior by a product of Dn(h; y). Thus, the asymptotic proper-
ties of the Bayes factors derived from the nonlocal priors can be identiﬁed by the asymptotic
behavior of Dn(h; y) that will be discussed in the following subsections.
5.3.3
Moment Functional Prior Densities
Theorem 8. Assume that the conditions of Proposition 7 apply. Consider the moment nonlocal
function prior ⇡Mr in (5.6) with the nonlocal kernel hr
M in (5.5), with µ = 0 and ⌃n =
92

gn(ΦTΦ)−1. Suppose gn ! 1 as n ! 1. Then, for any diverging sequence vn ! 1,
P0
⇥==Dn(hr
M; y) −T r
n,M
== > {gn(Kn −d0)}−r/2(T r
n,M)1/2vn
⇤
= o(1),
where
T r
n,M =
⇢F T
0 (QΦ −Q0)F0 + Kn −d0
gn(Kn −d0)
5r
,
for r = 1, 2.
Theorem 8 states that the rate of Dn(hr
M; y) is determined by the interplay between F T
0 (QΦ−
Q0)F0 and gn. Under H0, F T
0 (QΦ −Q0)F0 = 0 so that the rate of T r
n,M is g−r
n . On the
other hand, under H1, if F T
0 (QΦ −Q0)F0 ≻Kn, the rate of Bayes factor is governed by
F T
0 (QΦ −Q0)F0. The condition F T
0 (QΦ −Q0)F0 ≻Kn is reasonable in the sense that the scale
of F T
0 (QΦ −Q0)F0 is in the order of n(≻Kn), when the true function f0 is ﬁxed.
Corollary 9. Assume that the conditions of Theorem 8 apply. Under H1, assume that F0 and
the B-spline basis function satisfy that F T
0 (QΦ−Q0)F0 ⇣n. Suppose that gn ⇣n and Kn ≺n.
Then, under H1, Dn(hr
M; y) = Op(1), and under H0, Dn(hr
M; y) = Op(n−r) for r = 1, 2.
Corollary 9 considers a simple setting that F T
0 (QΦ −Q0)F0 ⇣n under the alternative
hypothesis with the choice of the hyperparameter gn ⇣n. For this setting, the hypothesis test
procedures based on moment functional prior densities enjoys the extra penalty Op(n−r) on
the Bayes factor in favor of the alternative compared to that from the local prior densities. On
the other hand, Corollary 9 states that the convergence rate of Bayes factor is asymptotically
invariant compared to that based on the local priors, under a true alternative hypothesis. This
indicates that using nonlocal functional prior densities in nonparametric hypothesis tests not
only improves the convergence rate of the Bayes factor by a polynomial rate when the null is
true, but also does not attenuate the rate when the alternative is true, at least in an asymptotic
sense.
93

5.3.4
Inverse Moment Functional Prior Densities
Theorem 10. Assume that the conditions of Theorem 8 apply, but now consider the inverse
moment functional prior densities in (5.8) with the nonlocal kernel hI in (5.7), with µ = 0 and
⌃n = ⌧n(ΦTΦ)−1. Then, for any diverging sequence vn,
P0

Mn,I
−log Dn(hI; y) > vn
>
= o(1),
where Mn,I = ⌧n(dn + σ2(ndn)1/2)−1 for dn = F T
0 (QΦ −Q0)F0 + σ2(Kn −d0).
This theorem shows that the under H0 with F T
0 (QΦ −Q0)F0 = 0, the rate of
{−log Dn(hI; y)}−1 is asymptotically bounded by (nKn)1/2⌧−1
n
in probability, while un-
der H0 with F T
0 (QΦ −Q0)F0 = 0 ≻Kn, it is asymptotically bounded by (nF T
0 (QΦ −
Q0)F0)1/2⌧−1
n . The following corollary provides a simpler setting to evaluate the convergence
rate of Dn(hI; y).
Corollary 11. Assume that the conditions of Theorem 10 apply. Suppose that under H1,
F T
0 (QΦ −Q0)F0 ⇣n. Assume that ⌧n ⇣n and Kn ≺n. Then, under H1, Dn(hI; y) = Op (1),
and under H0, Dn(hI; y) = Op(exp{−cn1/2K−1/2
n
}) for some positive constant c.
Corollary 11 shows that under H0, the Bayes factor based on the inverse moment func-
tional prior achieves an exponentially faster convergence rate Op(exp{−cn1/2K−1/2
n
}) than
does the Bayes factor based on the local priors. Moreover, the use of the inverse moment
functional prior does not degrade the convergence rate of the Bayes factor in an asymptotic
sense when the alternative is true because Dn(hI; y) = Op (1). Therefore, we can expect sig-
niﬁcant improvement in the convergence rate of the Bayes factor under true null hypotheses
and asymptotically the same convergence rate as when the local prior is deployed under true
alternative hypotheses.
94

5.3.5
The Choice of Kn
The asymptotic behavior of Bayesian nonparametric inference based on B-spline basis
functions was well-studied in Bontemps (2011) and Ghosal and van der Vaart (2007). In
particular, Ghosal and van der Vaart (2007) showed that the minimax rate of n−↵/(1+2↵) for
posterior concentration under the L2 norm can be achieved by setting Kn ⇣n1/(1+2↵) when
f0 2 C↵[0, 1]. Similar results were obtained in a frequentist perspective in Zhou et al. (1998);
Claeskens et al. (2009). However, the asymptotic results regarding the Bayes factors that are
discussed in this section do not require the optimal condition on Kn (⇣n1/(1+2↵)). The follow-
ing proposition illustrates why this is so.
Proposition 12. Suppose that f0 2 C↵[0, 1] and Kn ! 1 as n tends to 1. Then, F T
0 (QΦ −
Q0)F0 ⇣F T
0 (I −Q0)F0.
This proposition shows that the asymptotic behavior of F T
0 (QΦ −Q0)F0 is solely deter-
mined by F T
0 (I −Q0)F0, without any dependence on the rate of Kn. Even though the asymp-
totic estimation performance would be sub-optimal when the rate of Kn is misspeciﬁed, the
convergence rates of the Bayes factors discussed in the previous theorems are still valid. When
the data are generated under true alternative hypotheses (i.e. F T
0 (I −Q0)F0 ⇣n), Proposi-
tion 12 guarantees that the condition F T
0 (QΦ −Q0)F0 ⇣n in Corollary 9 and Corollary 11 is
satisﬁed. Therefore, the results of these corollaries hold with any diverging Kn.
5.4
Examples of Bayesian Hypothesis Tests Using Nonlocal Functional Priors
In this section, I examine the behavior of Bayes factors based on different priors in ﬁnite
samples. I ﬁrst consider a simple hypothesis setting to test the sparsity of the regression func-
tion in (5.1).
H0 : F = 0
vs.
H1 : F 6= 0.
(5.10)
95

Here, the projection matrix on the null space Q0 is equivalent to a matrix with all zero entries,
resulting in βTΦT(I −Q0)Φβ = βTΦTΦβ. Another example is a hypothesis test for linearity
speciﬁed as
H0 : F is linear
vs.
H1 : F is not linear.
(5.11)
In this case, the null regressors Φ0 can be deﬁned as {1, x} and Q0 is the projection matrix
derived from Φ0.
I also consider a hypothesis test on the coefﬁcient function in a varying coefﬁcient model
as introduced in Hastie and Tibshirani (1993). This model can be deﬁned as yi = tif(xi) + ✏i,
where ✏i is i.i.d N(0, σ2). I also assume that x = {xi}1in and t = {ti}1in are independent
variables. Some practitioners might be interested in testing if F is constant so that the resulting
model is equivalent to a simple linear regression model. To test this hypothesis, I construct the
contrasting hypotheses on the coefﬁcient function F.
H0 : F is constant
vs.
H1 : F is not constant.
(5.12)
The resulting nonlocal prior densities can be generated by setting Φ0 = {1}.
To compare the performance of the local and nonlocal functional prior densities, I consid-
ered several functions for the hypothesis tests as follows:
fSp(u)
=
0,
fQ(u) = u2
fC(u)
=
1,
fs(u) = sin(u)
fL(u)
=
u,
fpL(u) = (u −⇡/3)+ −(−⇡/3 −u)+,
(5.13)
where u 2 [−⇡, ⇡] and (·)+ is the truncation function on negative values by zero.
The null
96

Sparsity Test
n
log Bayes Factor
0
100
200
300
400
−50
−40
−30
−20
−10
0
Positive
Strong
Very Strong
Local prior
MoM (r=1)
MoM (r=2)
iMoM
Linearity Test
n
log Bayes Factor
0
100
200
300
400
−80
−60
−40
−20
0
Varying Coefficient Test
n
log Bayes Factor
0
100
200
300
400
−150
−100
−50
0
Figure 5.1: When the null hypothesis is true, the averaged logarithm of Bayes factor in favor of
the alternative hypothesis based on the nonlocal prior and the local prior densities for sparsity,
linearity, and varying coefﬁcient test with varying sample size n. “MOM" and “iMOM" indi-
cate the moment functional prior and the inverse moment functional prior. The two horizontal
green lines are on -3 and -5 of Bayes factor.
functions of the hypothesis tests for sparsity, linearity, and varying coefﬁcient are fSp, fL, and
fC, respectively. I generated independent variables x and t from a uniform distribution on
(−⇡, ⇡). Given a regression function f, the corresponding dependent variable is generated
from yi = Af(xi) + ✏i, where ✏i follows a N(0, σ2) with σ = 1/2 for i = 1, · · · , n and A is
a constant that models V ar{Af(x)} = 1. This speciﬁcation controls the signal-to-noise ratio
for different regression functions. For varying coefﬁcient models, I simulated the dependent
variable by setting yi = Rxif(ti) + ✏i, where R is chosen by solving V ar{Rxf(t)} = 1. One
hundreds replicated data sets were used for each simulation setting with sample sizes varying
from 10 to 400. I also set Kn = 4, gn = n/Kn and ⌧n = n. I use a simple Monte Carlo
simulation to evaluate the Bayes factors. Because the posterior distribution of β based on the
local prior has a closed form (Gaussian), the posterior expectation of the nonlocal kernels can
be easily evaluated from the Gaussian samples of the posterior distribution.
The performance comparison between the nonlocal functional priors and the local priors
is illustrated in Figure 5.1 for data generated under the null hypothesis. As Figure 5.1 shows,
evidence in favor of the alternative hypothesis resulting from the local prior density in (5.4)
97

Test
Sparsity
Linearity
VC
True function
fL
fs
fpL
fQ
fs
fL
fs
(n = 50)
log BF L
48.27
43.02
0.49
35.87
1.40
38.22
37.10
MOM r=1
1.18
1.15
-0.60
1.64
0.79
0.21
0.16
MOM r=2
1.99
1.92
-1.62
2.62
0.96
-0.05
-0.16
iMOM
1.54
1.41
-3.67
2.28
0.55
-0.36
-0.53
(n = 100)
log BF L
193.18
187.94
11.18
176.85
66.80
181.67
177.99
MOM r=1
1.12
1.10
-0.68
1.72
0.80
0.17
0.18
MOM r=2
1.85
1.80
-1.96
2.76
0.93
-0.16
-0.16
iMOM
1.59
1.56
-5.62
2.66
0.95
-0.33
0.31
(n = 400)
log BF L
789.47
780.95
55.07
778.07
293.74
775.11
733.18
MOM r=1
1.10
1.10
-0.75
1.78
0.82
0.18
0.14
MOM r=2
1.80
1.78
-2.16
2.87
0.95
-0.15
-0.24
iMOM
1.62
1.61
-7.63
2.77
1.12
-0.24
-0.38
Table 5.1: Under alternative hypotheses, the expectation of logarithm of Bayes factor and the
Dn(h; y) for nonparametric Bayesian hypothesis tests: Sparsity test in (5.10), Linearity test in
(5.11), and Varying Coefﬁcient test (VC) in (5.12). log BF⇡L denotes the averaged logarithm
of Bayes factor in favor of the alternative hypothesis based on the local prior. MOM r=1,
MOM r=2, and iMOM indicates the average of the logarithm of Dn(hr=1
M ; y), Dn(hr=2
M ; y),
and Dn(hI; y), respectively, for each alternative function over 100 replicates, and the data-
generating true function fL, fs, fpL and fQ are deﬁned in (5.13).
98

decreases much slower than that of the moment functional prior or the inverse moment func-
tional prior densities when the data were generated from a model that is consistent with the null
hypothesis. Even with small size of samples the moment or inverse moment functional prior
provided “very strong" support in favor of the null hypothesis. The logarithm of the Bayes fac-
tor in favor of the alternative was less than −5. On the other hand, the local prior (5.4) requires
a relatively large sample size to attain the same strength of evidence of the null hypothesis as
the nonlocal functional priors do. Moreover, as discussed by Johnson and Rossell (2012) in
parametric model selection, the local prior densities provide evidence in favor of the null which
is not strong enough so that it fails to achieve desirable model selection consistency when a
diverging number of models or hypotheses is considered.
I note that nonlocal functional prior densities often provide stronger evidence in favor of
the null hypothesis, especially when the discrepancy of the true regression function and the null
space, F T
0 (I−Q0)F0/n, is expected to be small. In Table 1, most considered alternative models
showed negligible differences between the Bayes factors based on the nonlocal prior and the
local prior densities. However, when the piece-wise linear function fpL was adopted as an
alternative hypothesis for the linearity test, the Bayes factor in favor of alternative hypotheses
based on the inverse moment nonlocal priors is signiﬁcantly attenuated compared to that of
the local prior densities. This result stems from the fact that the shape of the piece-wise linear
function is quite similar to that of a linear function. So, the discrepancy measure fpL(x)T(I −
Q0)fpL(x)/n is expected to be much smaller than that from the other alternative functions
considered, resulting in non-negligible log Dn(hI; y).
5.5
Nonparametric Additive Model Selection Using Nonlocal Functional Priors
In this section, I apply the proposed nonlocal functional priors to model selection problems
for high-dimensional nonparametric additive models as in (4.14). I denote the true model,
which is the index set of variables involved in the data-generating process, by t. I denote the
99

true marginal regression function of Xj for j 2 t by f0,j and the true regression function by
f0 = P
j2t f0,j. I also denote the empirical realization of the true regression function and the
true marginal regression functions by F0 and F0,j, respectively.
I model each marginal regression function Fj by a linear combination of B-spline basis
functions of the term Fj = Φjβj for βj 2 RKn. Here, Φj = {φl(xj)}l=1,...,Kn, and φl is the
l-th B-spline basis function for l = 1, . . . , Kn and j = 1, . . . , p. I deﬁne Φk to be a set of basis
function for the covariates in model k, i.e., Φk = {Φj}j2k.
For a given model k, let
bβk = (Φ
T
kΦk)−1Φ
T
ky,
bFk = Φk bβk,
and
Pk = Φk(Φ
T
kΦk)−1Φ
T
k.
(5.14)
For 1 j p, deﬁne
bβj = (Φ
T
j Φj)−1Φ
T
j y,
bFj = Φj bβj,
and
Pj = Φj(Φ
T
j Φj)−1Φ
T
j .
(5.15)
Given a model k, I consider a nonlocal functional prior for the additive model that is a
product of independent inverse moment functional priors (5.8) as
⇡NL(βk | k, σ2, ⌧n) /
Y
j2k
exp
⇢
−βT
j βj
2σ2⌧n
−
σ2⌧n
βT
j ΦT
j (I −Q0)Φjβj
5
.
(5.16)
Here, βj is a Kn-dimensional coefﬁcient vector for the B-splines basis functions corresponding
to xj for j = 1, . . . , p and Q0 is the projection matrix of an n-dimensional one vector. This
prior assigns zero density to the space of constant functions since F T(I −Q0)F = 0 when F
is constant. A constant marginal regression function implies that the corresponding covariate
is not associated with the response. Thus, the proposed prior for the additive model induces a
nonlocal functional prior for model selection. For high-dimensional additive model selection,
I only focus on the inverse moment prior. Even though model selection procedures based on
100

the moment functional priors are computationally efﬁcient to implement, the convergence rates
resulting Bayes factor are not strong enough to control for multiplicity. The resulting model
selection procedures thus fail to achieve model selection consistency in high-dimensional set-
tings.
In addition to imposing priors on the B-spline coefﬁcients given a model, I place a prior on
the model space to complete the prior speciﬁcation. I consider a uniform prior on the model
space restricted to models having size less than or equal to qn, with qn < n. That is, the prior
on the model space can be written as
⇡(k) / I(|k| qn),
(5.17)
where I(·) denote the indicator function. With a slight abuse of notation, I denote the prior on
the space of models by ⇡as well.
One might also consider nonuniform model priors on the model space. For instance, the
following model prior is introduced by Castillo et al. (2015) in high-dimensional model selec-
tion for linear models:
⇡(k) /
✓p
|k|
◆−1
a−|k|
1
p−a2|k|, a1, a2 > 0.
(5.18)
This prior strongly penalizes large-sized models when p is large. Castillo et al. (2015) de-
rived the posterior contraction rate and model selection consistency for linear model selection
problems based on this model prior. However, for nonparameteric additive model selection in
high-dimensional settings, the asymptotic properties of the procedure have not been investi-
gated. In contrast, I show that the model selection procedure based on the nonlocal functional
priors can achieve model selection consistency without the stronger prior on the model space.
101

Based on the deﬁned priors, the posterior distribution is deﬁned by
⇡(k | y) =
mk(y)⇡(k)
P
l ml(y)⇡(l),
where mk(y) =
R
L(y | k, βk, σ2)⇡(βk | k, σ2)dβk.
In the following sections, I illustrate some desirable theoretical properties of the model
selection procedure based on the proposed prior in (5.16).
5.5.1
Additive Model Selection Consistency for High-dimensional Settings
I ﬁrst state the regularity conditions that are assumed.
(A1) The true model t is ﬁxed regardless of n and p.
(A2) For any k with |k| qn, where qn is deﬁned in (5.17), there exist positive sequences ⇣n⇤
and ⇣⇤
n such that
⇣n⇤
|k|
X
j=1
β
T
j Φ
T
j Φjβj β
T
kΦ
T
kΦkβk ⇣⇤
n
|k|
X
j=1
β
T
j Φ
T
j Φjβj.
for any βk = {βj}j2k.
(A3) There exist positive constants λ⇤and λ⇤such that,
nλ⇤
Kn
β
Tβ min
j=1,...,p β
TΦ
T
j Φjβ max
j=1,...,p β
TΦ
T
j Φjβ nλ⇤
Kn
β
Tβ,
for any β 2 RKn.
(A4) For any k with |k| qn and j 62 k, βTΦT
j PkΦjβ βTΦT
j (I −Pk)Φjβ for any β 2 RKn.
(A5) For j 2 t, F T
0 PjF0/n converges to some constant cj as n tends to 1, and for j 62 t,
F T
0 PjF0 ≺log p. Also, mink:t6⇢k,|k|qn {F T
0 (Pk[t −Pk)F0} ≻n/ log n and
maxk:|k|qn {F T
0 (Pk[t −Pt)F0} ≺qn log p.
The condition (A2) is essential for model identiﬁability. When some basis functions evalu-
102

ated at the observed covariates are extremely correlated, any model selection procedure for the
additive models would fail to distinguish the corresponding variables with the highly correlated
basis functions. This results in identiﬁability issues between marginal functions. The condition
(A3) uniformly controls the maximum and minimum eigenvalues of marginal basis matrices
Φj, for j = 1, . . . , p. As stressed in Ghosal and van der Vaart (2007), the B-spline basis matrix
for a single covariate is asymptotically isotropic, i.e., there exist constants C1 and C2 such that,
as n increases for any β 2 RKn,
C1
n
Kn
β
Tβ β
TΦ
TΦβ C2
n
Kn
β
Tβ,
where Φ is the B-spline basis matrix of a single variable. However, this result does not as-
sure that the isotropic property holds uniformly over all basis matrices under high-dimensional
settings. Since the marginal likelihoods contains the determinant of the basis matrix corre-
sponding to a model, it is necessary to set (A3) as a regularity condition on the basis matrices
to evaluate the convergence rate of the marginal likelihoods.
Let ⇡(t | y) denote the posterior model probability of the true model obtained under the
product of inverse moment functional prior densities in (5.16) on the B-spline coefﬁcients and
a truncated uniform prior on all models of size less than or equal to qn in (5.17). I now illustrate
that model selection procedures based on the product of inverse moment functional priors is
consistent in the sense that the posterior true model probability converges to one in probability
as n increases, even when the number of predictors increases at sub-exponential rate.
Theorem 13. Suppose (A2)–(A5) hold with Kn ≺min{n1/2, log p} and qn ≺log n. Assume
σ2 is known. Suppose that there exists ⌘2 (0, 1) such that log p = O(n⌘). Assume that
⇣n⇤≻log p/{n(log n)2} and ⇣⇤
n = O(1). If ⇣−1/2
n⇤
u1/2
n (⇣−1
n⇤+ Kn + log p)1/2n1/2 log p ≺⌧n ≺
103

⇣−1/2
n⇤
(⇣−1
n⇤+ Kn + log p)1/2n3/2/ log n, where un = q2
n(log n)2,
⇡(t | y)
p! 1.
Theorem 13 imposes constraints on the hyperparameter ⌧n that are determined by the di-
mension p, Kn and ⇣n⇤. In particular, when some basis functions evaluated at the observed
covariates are highly correlated in the sense that ⇣n⇤decreases at a faster rate than those of Kn
and log p, the rate of ⌧n resulting model selection consistency is asymptotically determined by
the rate of ⇣n⇤.
5.5.2
Asymptotic Rates of Marginal Likelihood for Additive Models
In this section, I discuss a unique property of nonlocal functional priors that distinguishes
it from the local priors for additive models. I shall show that the marginal likelihood from the
nonlocal functional prior attains a very different form from that of local priors (5.21).
For a given model k, the convergence rate of the logarithm of the marginal likelihood
mL
k(y) based on the local prior with a hyper parameter gn in (5.21) can be expressed as
log mL
k(y) ⇡log L(y | bβk, k) −|k|Kn
2
log gn,
(5.19)
where βk is deﬁned in (5.14) and ⇡(k) is a model prior. An important point is that the penalty
on the model k depends solely on the model size |k|. In other words, adding one extra variable
to a model penalizes the marginal likelihood by (Kn log gn)/2, regardless of the strength of the
signal from the estimated marginal regression function.
On the other hand, the the asymptotic rate of the logarithm of the marginal likelihood based
on the nonlocal function prior involves a totally different penalty on model k. This penalty is
adaptively determined by the estimated marginal function bFj for j 2 k. Under the regularity
conditions considered in Section 5.5.1, the asymptotic marginal likelihood mNL
k (y) from the
104

nonlocal functional priors can be written as
log mNL
k (y) ⇡
(A)
z
}|
{
log L(y | bβk, k) −|k|Kn
2
log ⌧n
(B)
z
}|
{
−
X
j2k
cσ2⌧n
bF T
j (I −Q0) bFj + ✏n
,
(5.20)
where bFj is deﬁned in (5.15) and ✏n = {n bF T
j (I −Q0) bFj}1/2. This can be shown by us-
ing Lemma A.3.1 in Appendix. The term (A) in (5.20) is exactly the same as the rate of
marginal likelihoods deﬁned by local priors. Additional to (A), the rate of the marginal like-
lihood based on the nonlocal prior attains an extra penalty term (B). This penalty term adapts
to the semi-norm bF T
j (I −Q0) bFj for eah j 2 k. When j 2 t, the assumption (A5) im-
plies that the marginal penalty term for the j-th variable is Op(⌧n/n). On the other hand,
when j 62 t, the marginal term is Op(⌧n/(n min{Kn, log p, ⇣−1
n⇤})1/2) since bF T
j (I −Q0) bFj fol-
lows a noncentral chi square distribution χ2
Kn(Rn), where the noncentral parameter is Rn =
F T
0 Pj(I −Q0)PjF0 ≺log p. The latter property again follows from the assumption (A5. This
property of nonlocal functional priors shows that a model containing variables in the true model
will be weakly penalized, and a model with any spurious variables will be strongly penalized,
since min{Kn, log p, ⇣−1
n⇤} ≺n. This property results in a promising performance of model
selection by nonlocal functional priors with ﬁnite samples. Simulated and real data sets are
provided to examine the model selection performance of my procedure in Section 5.5.4 and
Section 5.6.
The posterior model probability ⇡(k | y) of a model k is proportional to the product of
the marginal likelihood and the model prior. In the above cases, under a uniform model prior,
i.e. ⇡(k) / 1, the logarithm of the posterior model prior is asymptotically equivalent to the
logarithm of the marginal likelihood. One can also consider a nonuniform model prior in (5.18)
on the model space. By using Stirling’s approximation, one can show that that model prior is
asymptotically equivalent to a−|k|
1
p−(1+a2)|k|. The rate of marginal likelihood can be matched
105

to the rate of the model prior. For example, the logarithm of the model prior for a1 = 1 and
a2 = 1, plus the log-marginal likelihood with the local prior for gn = n, is asymptotically
equivalent to the logarithm of the marginal likelihood of the local prior for gn = np4/Kn. Since
the logarithm of the marginal likelihood of the nonlocal functional prior can be expressed as the
sum of the logarithm of the marginal likelihood of the local prior and the extra penalty term,
the model prior in (5.18) is embedded in the marginal likelihood of the nonlocal functional
prior.
5.5.3
Computational Strategy Using S5
In the previous sections, I have discussed desirable theoretical properties of nonlocal func-
tional priors. From a computational viewpoint, implementing Bayesian variable selection pro-
cedures for high-dimensional additive models is a challenging problem. For high-dimensional
model selection, full posterior sampling using Markov Chain Monte Carlo (MCMC) algo-
rithms, such as the reversible jump MCMC (Green, 1995), is highly inefﬁcient and often not
feasible from a practical perspective. Recently, Shin et al. (2017) proposed a scalable algo-
rithm called the Simpliﬁed Shogun Stochastic Search with Screening (S5) that is optimized
to efﬁciently explore high-dimensional model spaces in Bayesian variable selection for lin-
ear models. The S5 algorithm is a simpliﬁed version of an existing search algorithm, Shogun
Stochastic Search (SSS) by Hans et al. (2007) and utilizes a screening step. The screening
step is embedded in the algorithm to reduce the model space to be searched. Shin et al. (2017)
empirically showed that S5 efﬁciently searches the model space and dramatically reduces the
computational burden in exploring linear models.
Here, I modify S5 to be suitable for high-dimensional additive models by adding a non-
parametric screening step called the Iterative Nonparametric Independence Screening (INIS;
Fan et al. (2011)). For a given model k, the ﬁrst step in INIS is to calculate the residual rk of
the additive model using its B-spline least square estimator. Second, the residuals are used to
106

evaluate the nonparametric screened set SINIS
k
(M) with a screening size M deﬁned as
SINIS
k
(M) = {j 2 {1, . . . , p} : rank(k bFk,jk2
n,2) M},
where bFk,j = Pjrk for j = 1, . . . , p. The function rank(aj) for some aj in {al}1lp is a
rank function that evaluates the decent order of aj in {al}1lp. This screened set SINIS
k
(M)
includes the M top variables that have the large empirical L2 norm of teh estimated marginal
function. I then restrict the S5 algorithm to the screened set of variables SINIS
k
(M) whose
cardinality is M (⌧p), so that the target model space can be signiﬁcantly reduced and the
computation can be highly accelerated. The screening step is performed and the screened set is
updated in every iteration in S5, so even when some signiﬁcant variables are ignored in early
iterations, they can re-enter the model in subsequent iterative screening. The formal statement
of the algorithm is provided in Appendix. For more details and discussions about S5, see
Section 3.
To evaluate the marginal likelihood for each model, I used the Laplace approximation. The
derivation of this procedure is described in Appendix. All computational functions used in this
dissertation are provided in the R package BayesS5.
5.5.4
Simulation Studies
To examine the performance of the model selection procedure based on the nonlocal func-
tional priors, I considered several simulations settings that were perviously proposed by others.
However, I used a different sample size n and dimension p for each setting to examine high-
dimensional settings. Let SNR = Var(f0(x))/V ar(✏) denote the signal-to-noise ratio, where
f0 is the true underlying function; i.e. f0 = P
j2t fj(Xj).
107

Scenario 1: (n = 150, p = 3000, SNR ⇡15) The true model is
yi = f1(xi1) + f2(xi2) + f3(xi3) + f4(xi4) + ✏i,
where ✏i
i.i.d
⇠N(0, 1) for i = 1, . . . , n, with
f1(x)
=
−sin(2x), f2(x) = x2 −25/12, f3(x) = x,
f4(x)
=
exp{−x} −2/5 · sinh(5/2).
The covariates are independently generated from a uniform distribution between −2.5 to 2.5.
Similar settings with this were considered in multiple articles such as Härdle et al. (2012),
Meier et al. (2009) and Ravikumar et al. (2009).
Scenario 2: (n = 150, p = 3000, SNR ⇡6.7) The same scenario was considered in Meier
et al. (2009), but the dimension and the sample size in that paper are different from this sce-
nario (n = 100 and p = 1000 in Meier et al. (2009)). This scenario is similar to Scenario 1,
but the covariates are instead generated from a Gaussian distribution with a covariance matrix
⌃, where ⌃k,j = 0.5|k−j| for 1 k, j p; i.e., xi ⇠N(0, ⌃) for i = 1, . . . , n.
Scenario 3: (n = 200, p = 3000, SNR ⇡3.11). I consider a scenario that was examined in
Huang et al. (2010). The true model is
yi = 5f5(xi1) + 3f6(xi2) + 4f7(xi3) + 6f8(xi4) + ✏i,
108

where
f5(x)
=
x, f6(x) = (2x −1)2, f7(x) = sin(2⇡x)/(2 −sin(2⇡x)),
f8(x)
=
0.1 sin(2⇡x) + 0.2 cos(2⇡x) + 0.3 sin(2⇡x)2 + 0.4 cos(2⇡x)3 + 0.5 sin(2⇡x)3.
The covariates are generated as follows. First, I generate Wij, Vi, and Ui independently from
s N(0, 1) truncated to the interval, [0, 1] for i = 1, . . . , n and p = 1, . . . , p. Then I set xij =
(Wij + Ui)/2 for j = 1, . . . , 4 and xij = (Wij + Vi)/2 for j = 5, . . . , p. This guarantees that
the variables in the true model and the spurious variables are independent.
Scenario 4: (n = 400, p = 1000, SNR ⇡11.25) This scenario was considered in Meier et al.
(2009), and a similar example was also considered in Lin and Zhang (2006). The same func-
tions are used as in Scenario 3 and the covariates are independently and uniformly generated
on the interval (0,1). The model is
yi
=
f1(xi1) + f2(xi2) + f3(xi3) + f4(xi4)
+1.5f1(xi5) + 1.5f2(xi6) + 1.5f3(xi7) + 1.5f4(xi8)
+2.5f1(xi9) + 2.5f2(xi10) + 2.5f3(xi11) + 2.5f4(xi12) + ✏i,
where ✏i
i.i.d
⇠N(0, 0.5184) for i = 1, . . . , n.
For comparisons with a classical local prior, I consider a simple local prior deﬁned by a
product of g-priors, expressible as
⇡L(βk | k, σ2) /
Y
j2k
exp
⇢
−βT
j ΦT
j Φjβj
2σ2gn
5
.
(5.21)
To my best knowledge there do not exist references regarding theoretical properties about
the model selection procedure using the prior in (5.21) for high-dimensional additive mod-
109

Scenario 1
Scenario 2
Scenario 3
Scenario 4
Methods
MSE
MSPE
MSE
MSPE
MSE
MSPE
MSE
MSPE
NLfP
0.147
1.270
0.162
1.649
0.891
3.127
0.271
0.927
g-prior
0.148
1.270
0.157
1.649
0.916
3.052
0.268
0.924
HGAM
2.128
6.501
1.706
5.905
1.236
3.544
0.656
1.452
SpAM
0.496
2.189
0.575
3.347
0.740
3.294
0.271
1.161
AdapGL
0.492
1.989
0.563
2.873
0.902
3.939
0.305
1.293
Table 5.2: Optimal MSE and MSPE of each method for the considered settings.
els. However, the use of the prior is natural for the variable selection in additive models since
each component function is modeled by a linear combination of B-spline basis functions. For
the Bayesian procedures, I used a noninformative prior on σ2 proportional to 1/σ2. I set the
number of the basis functions Kn = 5 for all simulation scenarios.
I compared results from the Bayesian procedures to several penalized likelihood approaches.
These approaches included the following. Ravikumar et al. (2009) introduced a penalized like-
lihood method called Sparse Additive Model (SpAM) for model selection for additive mod-
els. The penalty term of SpAM can be expressed as a weighted group LASSO penalty (Yuan
and Lin, 2006). Meier et al. (2009) proposed High-dimensional Generalized Additive Model
(HGAM) combining sparsity on the coefﬁcients of basis functions and regularization on the
smoothness of the marginal regression function. Huang et al. (2010) introduced a two-step
procedure using adaptive group LASSO (AdapGL) for variable selection in additive models,
which ﬁrst derives the weights of the group penalty, then applies it to the adaptive group
LASSO penalty. In the simulations, I used the R packages SAM, hgam and grpLasso to
implement SpAM, HGAM, AdapGL, respectively.
To compare the performances of the procedures independently of the choice of tuning
parameters, I used Precision-Recall (PR) curves. PR curves plot the precision=TP/(TP+FP)
versus the recall (or senstivity)=TP/(TP+FN), where TP, FP and FN respectively denote the
number of true positives, false positives and false negatives, as the tuning parameter varies
110

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
NLfP
g−prior
SpAM
HGAM
AdapGL
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Proportion of selecting true model
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Proportion of selecting true model
Figure 5.2: The ﬁrst column illustrates the PR curve of each method; the second column dis-
plays proportion of selecting the true model versus recall. The results of Scenario 1 and Sce-
nario 2 are represented in the ﬁrst row and the second row, respectively. “NLfP" denotes
the model selection procedure based on the product of inverse moment functional priors, and
“g-prior" is the model selection procedure deﬁned by the product of g-priors as in (5.21).
111

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Proportion of selecting true model
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Precision
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Proportion of selecting true model
Figure 5.3: The results of Scenario 3 and Scenario 4 are represented in the ﬁrst row and the
second row, respectively. The detailed description of this ﬁgure is given in the caption of Figure
5.2.
112

from a large value to a small value. Since precision is 1 - False Discovery Rate (FDR) and
recall is 1-Type II error rate, a PR curve illustrates the trade-off between the False Discovery
Rate (FDR) and the Type-I error rate. The performance of a procedure can be measured by the
area under the PR curve. The greater the area, the more accurate the method in model selec-
tion. In Davis and Goadrich (2006), PR curves were proposed as alternatives to the Receiver
Operating Characteristic (ROC) curves in high-dimensional settings. Shin et al. (2017) also
considered this measure in their simulation studies of model selection for high-dimensional
linear regression models. I plotted the proportion times each procedure selected the true model
versus recall as its tuning parameter varies. For each method, the curve is drawn by varying
different tuning parameters, so the comparison of the model selection procedures is free from
the choice of tuning parameters. I report the averaged results over 100 independent replicates
for each scenario.
Figure 5.2 and Figure 5.3 demonstrate the PR curves and plots the proportion times the
true model is selected versus recall. The performance of the model selection based on the
nonlocal functional prior was better than the penalized likelihood estimators in the sense that
they achieve a larger area under the PR curve. In addition, they more frequently selected the
true model at any recall level. This follows from the the plot of the proportion times the true
model selected that showed that the curve of the nonlocal procedure fully covered these curves
of the penalized likelihood methods on any recall value. Compared to the Bayesian procedure
based on the g-prior, the nonlocal procedure performed similiarly in all scenarios except the
third, where the nonlocal prior performed better.
In Table 5.2, I report the Mean Square Error (MSE) and the Mean Square Predictive Error
(MSPE) for each procedure. The tuning parameter used in Table 5.2 were chosen to minimize
the MSE, i.e.,
bλ = argmin
λ
E0(kF0 −eFλk2
n,2),
113

where eFλ is the estimated additive regression function with a tuning parameter λ. For the
Bayesian procedures, maximum a posteriori (MAP) estimators were used to evaluate the MSE
and the MSPE. These results suggest that the model selection procedure based on the nonlocal
functional prior shows promising performance compared to the penalized likelihoods proce-
dures, and its estimation and prediction performances are almost same as those based on the
product of g-priors.
5.5.5
Practical Selection of Hyperparameter Values
In the simulation studies, it was not necessary to choose a speciﬁc hyperparameter ⌧n be-
cause PR-curves were free from the choice of the hyperparameter. However, in practice one
must choose a value for the hyperparameter.
To choose an appropriate hyperparameter ⌧n, I used a procedure in which I compared the
null density of the maximum likelihood estimator (MLE) of the discrepancy measure F T(I −
Q0)F/σ2 to the prior density of βTΦT(I −Q0)Φβ/σ2 under the alternative hypothesis where
βTΦT(I −Q0)Φβ/σ2 > 0. Both densities were evaluated from randomly selected covariates.
This idea stems from Nikooienejad et al. (2016) who proposed a general idea for selecting a
hyperparameters in linear models. I extend their idea to nonparametric settings by using the
semi-norm F T(I −Q0)F. Under the additive regression model in (4.14), I ﬁrst evaluated bσ2 by
using a few variables chosen by INIS. I then deﬁned the null distribution of bF T(I −Q0) bF/bσ2,
where bF denotes the B-spline MLE of F under the null hypothesis where y = ✏for ✏⇠
N(0, bσ2I), so that the null distribution follows χ2
Kn−1. For a given ⌧n and estimated bσ2, I
randomly sampled a j 2 {1, . . . , p} and a βj from the prior density proportional to
exp
⇢
−βT
j βj
2bσ2⌧n
−
bσ2⌧n
βT
j ΦT
j (I −Q0)Φjβj
5
.
I evaluated βT
j ΦT
j (I −Q0)Φjβj/bσ2 by plugging the sampled βj in the discrepancy measure,
and repeated this procedure many times to approximate the prior density of the discrepancy
114

Figure 5.4: The black line and the blue line are the density functions of the null and the prior
distribution of F T(I −Q0)F/bσ2, respectively, for a given ⌧n and bσ2
measure.
Figure 5.4 illustrates the resulting null density and the prior density on the discrepancy
measure F T(I −Q0)F/bσ2 for some ⌧n and bσ2. I numerically determined the value of the
hyperparameter so that the overlap of the null density and the prior density of the discrepancy
(red-colored in Figure 5.4) falls below a certain threshold t. For example, I took t = p−1. By
choosing ⌧n to be large enough so that the intersection of these two densities is smaller than
the speciﬁed threshold, I was able to approximately bound the probability of false positives in
the model. As the threshold t decreases to zero, the prior density deviates more from the null
distribution. This results in a model selection procedure that strongly penalizes models with
covariates that have a smaller discrepancy measure than the intersection point between the null
distribution and the prior distribution on the discrepancy.
I applied this procedure to choose the hyperparameter for the nonlocal functional priors in
the following the section for real data analyses.
115

5.6
Applications to Real Data Sets
5.6.1
Bardet-Biedl Syndrome Gene Expression Data
I considered again the Bardet-Biedl syndrome data set that was used in Section 3.4.1. The
detailed description of this data set is provided in Section 3.4.1.
5.6.2
Near Infrared Spectroscopy Data
I also evaluated Near Infrared (NIR) Spectroscopy data set. This data set has previously
analysed in Liebmann et al. (2009) and Curtis et al. (2014), and is available from the R package
chemometrics. The NIR data includes glucose and ethanol concentration (in g/L) for 166
alcoholic fermentation mashes of different feedstock (rye, wheat and corn). The data set is
modeled by 235 NIR spectroscopy absorbance values acquired in the wavelength range of
115-2285 nanometer (nm) by a transﬂectance probe (Liebmann et al., 2009). I implement
my model selection procedure on the data values with response variables deﬁned by ethanol
concentrations (n = 166 and p = 235). I set the training and test set size to be 146 and 20,
respectively.
5.6.3
Technical Details and Results
For all data sets, the response variable was centered so that its sample mean was zero, and
each covariate was standardized so that its sample mean and standard deviation are zero and
one. Each model selection procedure was conducted on the training data set and the perfor-
mance of the procedures was examined on the test samples, and I repeated this process for
200 replicates. I report the out-of-sample prediction error (PE), which is the sum of square
prediction errors divided by the test sample size, and the average of model size (MS) in Table
5.3. I set Kn = 5, and assumed ⇡(σ2) / 1/σ2.
Following procedures described in Huang et al. (2008), I choose the tuning parameters
for the penalized likelihood approaches using Bayesian Information Criterion (BIC) (Schwarz,
116

Bardet-Biedl Data
NIR Data
Method
PE
MS
PE
MS
NLfP
1.667 (3.77)
6.00 (1.51)
1.220 (0.61)
5.80 (0.78)
g-prior(gn = n)
93.636 (459.82)
16.97 (0.31)
2.634 (2.01)
9.23 (1.26)
g-prior(gn = p)
82.186 (378.09)
16.92 (1.04)
2.302 (1.71)
8.70 (1.24)
g-prior(gn = p5/4)
32.829 (200.49)
5.24 (1.12)
1.718 (1.31)
6.86 (1.12)
g-prior(gn = p3/2)
3.705 (34.03)
2.05 (0.57)
1.428 (1.08)
5.83 (1.03)
g-prior(gn = p2)
1.804 (3.13)
1.01 (0.07)
1.395 (0.81)
4.14 (0.61)
HGAM(EBIC)
1.882 (1.76)
2.74 (3.66)
1.941 (0.97)
47.14 (3.41)
HGAM(BIC)
1.846 (1.72)
3.38 (4.70)
1.925 (0.96)
47.61 (3.17)
SpAM(EBIC)
2.904 (26.17)
5.86 (2.01)
54.52 (47.91)
9.05 (9.42)
SpAM(BIC)
2.931 (27.35)
5.89 (1.99)
17.101 (21.21)
16.00 (11.02)
AdapGL(EBIC)
2.301 (8.86)
6.80 (8.59)
23.299 (18.57)
5.11 (0.56)
AdapGL(BIC)
16.404 (92.11)
15.73 (7.58)
8.093 (7.19)
7.35 (1.57)
Table 5.3: Real data examples. PE and MS indicate the out-of-sample prediction error and
the average of model size; the PE for the Bardet-Biedl Data is scaled by 10−2. The standard
deviation of each quantity over 200 replicates is noted in parentheses.
1978) and Extended BIC (EBIC) (Chen and Chen, 2008) for the penalized likelihood methods.
These criteria can be expressed as
BIC(λ)
=
log(RSSλ) + |bkλ|Kn(log n)/n
EBIC(λ)
=
log(RSSλ) + |bkλ|Kn(log n)/n + ⌫|bkλ|Kn(log p)/n,
where 0 ⌫1 is a constant and λ is a tuning parameter. RSSλ and bkλ are the residual sum of
square and the selected model for a given λ, respectively. As in Huang et al. (2010), I use ⌫=
0.5 for the EBIC. Also, I have considered multiple hyperparameters (gn = {n, p, p5/4, p3/2, p2})
for the model selection procedure based on g-priors. The hyperparameter ⌧n for my approach
was chosen by the procedure described in Section 5.5.5. For Bayesian procedures, I used the
MAP estimators to summarize PE.
Table 4.2 summarizes the results from these studies. In both data sets, the model selection
117

procedure based on the nonlocal functional prior led to the smallest values of PE. In particular,
my procedure showed better prediction performance compared to the considered penalized
likelihood procedures. Even though multiple hyperparameters were considered for the g-priors,
these procedures had larger prediction errors than the nonlocal procedure. The g-prior results
were very sensitive to the choice of the hyperparameter gn.
5.7
Conclusion
This dissertation has proposed new classes of nonlocal functional prior densities that have
favorable asymptotic properties for nonparametric Bayesian testing problems. I have discussed
their advantages over local alternative priors with respect to the convergence rate of Bayes fac-
tors. I have focused on B-spline based nonparametric models. However, my methodology can
be applied to general classes of nonparametric functional models including Gaussian process
regression models. I suggested three natural examples for the usage of the proposed priors.
These included sparsity and linearity tests for the nonparametric simple regression models,
and a constant function test for the varying coefﬁcient model.
In Section 5.5, I applied one of the proposed priors (the inverse moment functional prior)
to additive model selection problems for high-dimensional settings. I showed that the result-
ing model selection procedure achieved consistency in the sense that the posterior true model
probability converged to one in probability under certain regularity conditions. I provided a
procedure to select an appropriate hyperparameter ⌧n. I also have examined its ﬁnite sample
performance in model selection using simulated data sets and real data sets. The model selec-
tion procedure based on my inverse moment functional priors performed better according to
several measures that several alternative procedures.
Finally, I have proposed a scalable computation algorithm that is a modiﬁed version of
S5. The computational functions for the additive model selection procedure described in this
dissertation are available in the R package BayesS5.
118

REFERENCES
Armagan, A., Dunson, D. B., and Lee, J. (2013). Generalized double Pareto shrinkage. Statis-
tica Sinica, 23(1):119–143.
Bae, K. and Mallick, B. K. (2004). Gene selection using a two-level hierarchical Bayesian
model. Bioinformatics, 20(18):3423–3430.
Bahadur, R. R. and Bickel, P. J. (1967). Asymptotic optimality of Bayes test statistics. Techni-
cal Report. University of Chicago.
Barber, R. F., Drton, M., and Tan, K. M. (2016). Laplace approximation in high-dimensional
Bayesian regression.
In Statistical Analysis for High-Dimensional Data, pages 15–36.
Springer.
Barbieri, M. M. and Berger, J. O. (2004). Optimal predictive model selection. Annals of
Statistics, 32(3):870–897.
Bernardo, J. M. and Smith, A. F. (1994). Bayesian theory. 1994. John Willey and Sons. Valencia
(España).
Bhattacharya, A., Chakraborty, A., and Mallick, B. K. (2016). Fast sampling with Gaussian
scale-mixture priors in high-dimensional regression. Biometrika, 103(4):985–991.
Bhattacharya, A., Pati, D., Pillai, N. S., and Dunson, D. B. (2015). Dirichlet-Laplace priors for
optimal shrinkage. Journal of the American Statistical Association, 110(512):1479–1490.
Birgé, L. (2001). An alternative point of view on Lepski’s method. Lecture Notes-Monograph
Series, 36:113–133.
119

Bondell, H. and Reich, B. (2012). Consistent high-dimensional Bayesian variable selection via
penalized credible regions. Journal of the American Statistical Association, 107(500):1610–
1624.
Bontemps, D. (2011). Bernstein–von Mises theorems for Gaussian regression with increasing
number of regressors. Annals of Statistics, 39(5):2557–2584.
Breiman, L. (1995). Better subset regression using the nonnegative garrote. Technometrics,
37(4):373–384.
Buja, A., Hastie, T., and Tibshirani, R. (1989). Linear smoothers and additive models. Annals
of Statistics, 17(2):453–510.
Carbonetto, P. and Stephens, M. (2012). Scalable variational inference for Bayesian variable
selection in regression, and its accuracy in genetic association studies. Bayesian Analysis,
7(1):73–108.
Caron, F. and Doucet, A. (2008). Sparse Bayesian nonparametric regression. In Proceedings
of the 25th international conference on Machine learning, pages 88–95. Association for
Computing Machinery.
Carvalho, C., Polson, N., and Scott, J. (2010). The horseshoe estimator for sparse signals.
Biometrika, 97(2):465–480.
Castillo, I., Schmidt-Hieber, J., and Van der Vaart, A. (2015). Bayesian linear regression with
sparse priors. Annals of Statistics, 43(5):1986–2018.
Castillo, I. and van der Vaart, A. (2012). Needles and straw in a haystack: Posterior concentra-
tion for possibly sparse sequences. Annals of Statistics, 40(4):2069–2101.
Chen, J. and Chen, Z. (2008). Extended Bayesian information criteria for model selection with
large model spaces. Biometrika, 95(3):759–771.
120

Choi, T., Lee, J., and Roy, A. (2009). A note on the Bayes factor in a semiparametric regression
model. Journal of Multivariate Analysis, 100(6):1316–1327.
Choi, T. and Rousseau, J. (2015). A note on Bayes factor consistency in partial linear models.
Journal of Statistical Planning and Inference, 166:158–170.
Claeskens, G., Krivobokova, T., and Opsomer, J. D. (2009). Asymptotic properties of penalized
spline estimators. Biometrika, 96(3):529–544.
Curtis, S. M., Banerjee, S., and Ghosal, S. (2014). Fast Bayesian model assessment for non-
parametric additive regression. Computational Statistics & Data Analysis, 71:347–358.
Davis, J. and Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves.
In Proceedings of the 23rd international conference on Machine learning, pages 233–240.
ACM.
De Boor, C. (1978). A practical guide to splines, volume 27. Springer-Verlag New York.
Efron, B. and Morris, C. (1973). Stein’s estimation rule and its competitors: an empirical
Bayes approach. Journal of the American Statistical Association, 68(341):117–130.
Fan, J., Feng, Y., and Song, R. (2011).
Nonparametric independence screening in sparse
ultra-high-dimensional additive models. Journal of the American Statistical Association,
106(494):544–557.
Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96(456):1348–1360.
Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space.
Royal Statistical Society: Series B, 70(5):849–911.
121

George, E. and Foster, D. P. (2000).
Calibration and empirical Bayes variable selection.
Biometrika, 87(4):731–747.
George, E. and McCulloch, R. (1993). Variable selection via Gibbs sampling. Journal of the
American Statistical Association, 88(423):881–889.
George, E. I. and McCulloch, R. E. (1997). Approaches for Bayesian variable selection. Sta-
tistica Sinica, 7(2):339–373.
Ghosal, S., Lember, J., and van der Vaart, A. (2008). Nonparametric Bayesian model selection
and averaging. Electronic Journal of Statistics, 2:63–89.
Ghosal, S. and van der Vaart, A. (2007). Convergence rates of posterior distributions for noniid
observations. Annals of Statistics, 35(1):192–223.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian
model determination. Biometrika, pages 711–732.
Grifﬁn, J. and Brown, P. (2010). Inference with normal-gamma prior distributions in regression
problems. Bayesian Analysis, 5(1):171–188.
Hans, C. (2009). Bayesian lasso regression. Biometrika, 96(4):835–845.
Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression.
Journal of the American Statistical Association, 102(478):507–516.
Härdle, W. K., Müller, M., Sperlich, S., and Werwatz, A. (2012). Nonparametric and semi-
parametric models. Springer Science & Business Media.
Hastie, T. and Tibshirani, R. (1986).
Generalized additive models.
Statistical Science,
1(3):297–318.
122

Hastie, T. and Tibshirani, R. (1993). Varying-coefﬁcient models. Royal Statistical Society:
Series B, 55(4):757–796.
Hoerl, A. E. and Kennard, R. W. (1970). Ridge regression: biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.
Huang, J., Horowitz, J. L., and Wei, F. (2010). Variable selection in nonparametric additive
models. Annals of Statistics, 38(4):2282–2313.
Huang, J., Ma, S., and Zhang, C.-H. (2008). Adaptive lasso for sparse high-dimensional re-
gression models. Statistica Sinica, 18:1603–1618.
Irizarry, R. A., Hobbs, B., Collin, F., Beazer-Barclay, Y. D., Antonellis, K. J., Scherf, U., and
Speed, T. P. (2003). Exploration, normalization, and summaries of high density oligonu-
cleotide array probe level data. Biostatistics, 4(2):249–264.
Ishwaran, H. and Rao, J. (2005). Spike and slab variable selection: frequentist and Bayesian
strategies. Annals of Statistics, 33(2):730–773.
James, W. and Stein, C. (1961). Estimation with quadratic loss. Proceedings of the fourth
Berkeley symposium on mathematical statistics and probability, 1(1961):361–379.
Jeffreys, H. (1961). Theory of Probability. Clarendon Press, Oxford.
Jiang, W. (2007). Bayesian variable selection for high dimensional generalized linear models:
convergence rates of the ﬁtted densities. Annals of Statistics, 35(4):1487–1511.
Johnson, V. E. and Rossell, D. (2010). On the use of non-local prior densities in Bayesian
hypothesis tests. Royal Statistical Society: Series B, 72(2):143–170.
Johnson, V. E. and Rossell, D. (2012). Bayesian model selection in high-dimensional settings.
Journal of the American Statistical Association, 107(498):649–660.
123

Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical
Association, 90(430):773–795.
Kim, Y., Choi, H., and Oh, H.-S. (2008). Smoothly clipped absolute deviation on high dimen-
sions. Journal of the American Statistical Association, 103(484):1665–1673.
Kim, Y., Kwon, S., and Choi, H. (2012). Consistent model selection criteria on high dimen-
sions. Journal of Machine Learning Research, 13:1037–1057.
Kirkpatrick, S. and Vecchi, M. (1983).
Optimization by simulated annealing.
Science,
220(4598):671–680.
Kooperberg, C. and Stone, C. J. (1991). A study of logspline density estimation. Computational
Statistics and Data Analysis, 12(3):327–347.
Lan, H., Chen, M., Flowers, J. B., Yandell, B. S., Stapleton, D. S., Mata, C. M., Mui, E. T.-
K., Flowers, M. T., Schueler, K. L., and Manly, K. F. (2006). Combined expression trait
correlations and expression quantitative trait locus mapping. PLoS Genetics, 2(1):51–61.
Liang, F., Paulo, R., Molina, G., Clyde, M. A., and Berger, J. O. (2008). Mixtures of g priors for
Bayesian variable selection. Journal of the American Statistical Association, 103(481):410–
423.
Liang, F., Song, Q., and Yu, K. (2013). Bayesian subset modeling for high-dimensional gener-
alized linear models. Journal of the American Statistical Association, 108(502):589–606.
Liebmann, B., Friedl, A., and Varmuza, K. (2009). Determination of glucose and ethanol in
bioethanol production by near infrared spectroscopy and chemometrics. Analytica Chimica
Acta, 642(1):171–178.
Lin, Y. and Zhang, H. H. (2006). Component selection and smoothing in multivariate nonpara-
metric regression. Annals of Statistics, 34(5):2272–2297.
124

Liu, Q. and Ihler, A. (2013). Variational algorithms for marginal MAP. Journal of Machine
Learning Research, 14(1):3165–3200.
Matthews, B. W. (1975). Comparison of the predicted and observed secondary structure of t4
phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451.
Meier, L., Van de Geer, S., and Bühlmann, P. (2009). High-dimensional additive modeling.
Annals of Statistics, 37(6B):3779–3821.
Narisetty, N. N. and He, X. (2014). Bayesian variable selection with shrinking and diffusing
priors. Annals of Statistics, 42(2):789–817.
Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31(3):705–767.
Nikooienejad, A., Wang, W., and Johnson, V. E. (2016). Bayesian variable selection for bi-
nary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics,
32:1338–1345.
Park, T. and Casella, G. (2008). The Bayesian lasso. Journal of the American Statistical
Association, 103(482):681–686.
Polson, N. and Scott, J. (2010a). Shrink globally, act locally: sparse Bayesian regularization
and prediction. In Proceedings of the 9th Valencia World Meeting on Bayesian Statistics,
volume 9, pages 501–538. Oxford University Press.
Polson, N. and Scott, J. (2010b). Shrink globally, act locally: sparse Bayesian regularization
and prediction. In Proceedings of the 9th Valencia World Meeting on Bayesian Statistics,
volume 9, pages 501–538. Oxford University Press.
Polson, N. G. and Scott, J. G. (2012). On the half-Cauchy prior for a global scale parameter.
Bayesian Analysis, 7(4):887–902.
125

Polson, N. G., Scott, J. G., and Windle, J. (2014). The Bayesian bridge. Royal Statistical
Society: Series B, 76(4):713–733.
Radchenko, P. and James, G. M. (2011). Improved variable selection with forward-lasso adap-
tive shrinkage. Annals of Applied Statistics, 5(1):427–448.
Raftery, A. E., Madigan, D., and Hoeting, J. A. (1997). Bayesian model averaging for linear
regression models. Journal of the American Statistical Association, 92(437):179–191.
Raskutti, G., Wainwright, M. J., and Yu, B. (2012). Minimax-optimal rates for sparse ad-
ditive models over kernel classes via convex programming. Journal of Machine Learning
Research, 13:389–427.
Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L. (2009). Sparse additive models. Royal
Statistical Society: Series B, 71(5):1009–1030.
Rockova, V. and George, E. I. (2014). EMVS: The EM approach to Bayesian variable selection.
Journal of the American Statistical Association, 109(506):828–846.
Rossell, D. and Telesca, D. (2017+). Non-local priors for high-dimensional estimation. Journal
of the American Statistical Association, (to appear).
Rossell, D., Telesca, D., and Johnson, V. E. (2013). High-dimensional Bayesian classiﬁers
using non-local priors. In Statistical Models for Data Analysis, pages 305–313. Springer.
Ruppert, D., Wand, M. P., and Carroll, R. J. (2003). Semiparametric regression. Cambridge
University Press.
Scheetz, T. E., Kim, K.-Y. A., Swiderski, R. E., Philp, A. R., Braun, T. A., Knudtson, K. L.,
Dorrance, A. M., DiBona, G. F., Huang, J., and Casavant, T. L. (2006).
Regulation of
gene expression in the mammalian eye and its relevance to eye disease. Proceedings of the
National Academy of Sciences, 103(39):14429–14434.
126

Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6(2):461–464.
Scott, J. and Berger, J. (2010).
Bayes and empirical-Bayes multiplicity adjustment in the
variable-selection problem. Annals of Statistics, 38(5):2587–2619.
Scott, J. G. and Walker, S. G. (2015).
Nonparametric Bayesian testing for monotonicity.
Biometrika, 102(3):617–630.
Shang, Z. and Li, P. (2014). High-dimensional Bayesian inference in nonparametric additive
models. Electronic Journal of Statistics, 8(2):2804–2847.
Shin, M., Bhattacharya, A., and Johnson, E. J. (2017+). Scalable Bayesian variable selec-
tion using nonlocal prior densities in ultrahigh-dimensional settings. Statistica Sinica, (to
appear).
Shin, M. and Tian, R. (2017). BayesS5: Bayesian Variable Selection Using Simpliﬁed Shotgun
Stochastic Search with Screening (S5). R package version 1.30.
Song, Q. and Liang, F. (2015).
High-dimensional variable selection with reciprocal l 1-
regularization. Journal of the American Statistical Association, 110(512):1607–1620.
Tibshirani, R. (1996).
Regression shrinkage and selection via the lasso.
Royal Statistical
Society: Series B, 58(1):267–288.
Tipping, M. (2001). Sparse Bayesian learning and the relevance vector machine. Journal of
Machine Learning Research, 1:211–244.
Wahba, G. (1990). Spline models for observational data. In CBMS-NSF regional conference
series in applied mathematics (59). Philadelphia: Society for Industrial and Applied Mathe-
matics.
127

Walker, A. M. (1969). On the asymptotic behaviour of posterior distributions. Royal Statistical
Society: Series B, 31(1):80–88.
Walker, S. G. (2004). Modern Bayesian asymptotics. Statistical Science, 19(1):111–117.
Xue, L. (2009). Consistent variable selection in additive models. Statistica Sinica, 19:1281–
1296.
Yang, Y., Wainwright, M. J., and Jordan, M. I. (2016). On the computational complexity of
high-dimensional bayesian variable selection. Annals of Statistics, 44(6):2497–2532.
Yuan, M. and Lin, Y. (2005). Efﬁcient empirical Bayes variable selection and estimation in
linear models. Journal of the American Statistical Association, 100(472):1215–1225.
Yuan, M. and Lin, Y. (2006).
Model selection and estimation in regression with grouped
variables. Royal Statistical Society: Series B, 68(1):49–67.
Yuan, M. and Zhou, D.-X. (2016). Minimax optimal rates of estimation in high dimensional
additive models. Annals of Statistics, 44(6):2564–2593.
Zellner, A. (1986). On assessing prior distributions and Bayesian regression analysis with
g-prior distributions. In Bayesian inference and decision techniques: Essays in Honor of
Bruno de Finetti, pages 233–243. North Holland, Amsterdam.
Zhang, C.-H. (2010).
Nearly unbiased variable selection under minimax concave penalty.
Annals of Statistics, 38(2):894–942.
Zhang, D., Lin, Y., and Zhang, M. (2009). Penalized orthogonal-components regression for
large p small n data. Electronic Journal of Statistics, 3:781–796.
128

Zhang, H. H., Cheng, G., and Liu, Y. (2011).
Linear or nonlinear? Automatic structure
discovery for partially linear models.
Journal of the American Statistical Association,
106(495):1099–1112.
Zhang, Y., Li, R., and Tsai, C.-L. (2010). Regularization parameter selections via generalized
information criterion. Journal of the American Statistical Association, 105(489):312–323.
Zhou, S., Shen, X., and Wolfe, D. (1998). Local asymptotics for regression splines and conﬁ-
dence regions. Annals of Statistics, 26(5):1760–1782.
Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical
Association, 101(476):1418–1429.
129

APPENDIX A
PROOFS OF THEORETICAL RESULTS
A.1
Nonlocal Prior Densities for High-dimensional Linear Model Selection
Preliminary Results
Lemma A.1.1. For Qk deﬁned in (2.6), Qk
j=1 QL
k,j Qk Qk
j=1 QU
k,j,
where
QL
k,j
=
c1(σ2)1/2(n⌫⇤
k + 1/⌧n,p)−1/2 exp{−⌧n,p/eβ⇤2
k,j},
QU
k,j
=
c2(σ2)1/2(n⌫k⇤+ 1/⌧n,p)−1/2 exp{−⌧n,p/(|eβk,j| + e✏n)2},
and e✏n ⇣(n⌫k⇤/⌧n,p)−1/4, with eβ⇤
k,j 2 [eβk,j −e✏n, eβk,j + e✏n] \ (−e✏n,e✏n)c for some positive
constants c1 and c2.
Proof. Recall e⌃k = (XT
k Xk + 1/⌧n,pIk)−1. From (2.8), all eigenvalues of (e⌃k)−1 are bounded
between n⌫k⇤+ 1/⌧n,p and n⌫⇤
k + 1/⌧n,p, which implies for all x 2 R|k|, (n⌫k⇤+ 1/⌧n,p)xTx 
xT(e⌃k)−1x (n⌫⇤
k + 1/⌧n,p)xTx. Let T1n = {(n⌫⇤
k + 1/⌧n,p)/σ2}1/2 and T2n = {(n⌫k⇤+
1/⌧n,p)/σ2}1/2. Substituting the above inequality in the expression for Qk, we have
|k|
Y
j=1
g1(eβk,j) Qk 
|k|
Y
j=1
g2(eβk,j),
(A.1)
where
gi(eβk,j) =
Z 1
−1
exp{−T 2
in(βk,j −eβk,j)2/2 −⌧n,p/β2
k,j}dβk,j,
(A.2)
for i = 1, 2. We establish the lower bound ﬁrst by showing that g1(eβk,j) ≥QL
k,j for all j =
130

1, . . . , |k|. Recall e✏n ⇣(n⌫k⇤/⌧n,p)−1/4 from the statement of the Lemma. We have
g1(eβk,j)
≥
Z
[eβk,j−e✏n,eβk,j+e✏n]\(−e✏n,e✏n)c exp{−T 2
1n(βk,j −eβk,j)2/2 −⌧n,p/β2
k,j}dβk,j
≥
exp{−⌧n,p/eβ⇤2
k,j}
Z
[eβk,j−e✏n,eβk,j+e✏n]\(−e✏n,e✏n)c exp{−T 2
1n(βk,j −eβk,j)2/2}dβk,j,
for some eβ⇤
k,j 2 [eβk,j −e✏n, eβk,j +e✏n]\(−e✏n,e✏n)c. Then, the integral in the last line of the above
display is equivalent to
Z
[−e✏n,e✏n]\(−eβk,j−e✏n,−eβk,j+e✏n)c e−T 2
1nt2/2dt ≥c1T −1
1n
Z T1ne✏n
0
e−z2/2dz ≥c2T −1
1n ,
where c1 and c2 are some positive constants and the last inequality in the above display fol-
lows since T1ne✏n ≥1 for large n. Substituting back in the previous display, g1(eβk,j) ≥
c1T −1
1n exp{−⌧n,p/eβ⇤2
k,j} for some constant c1 > 0, completing the proof of the lower bound.
We now establish the upper bound by showing that g2(eβk,j) QU
k,j for all j = 1, . . . , |k|. It
is straightforward to see that g2 is a symmetric function (i.e, g2(eβk,j) = g2(|eβk,j|)), so that it is
enough to establish the bound for eβk,j > 0; without loss of generality we assume that eβk,j > 0.
We have
Z 1
−1
exp{−T 2
2n(βk,j −eβk,j)2/2 −⌧n,pβ2
k,j}dβk,j
=
Z 0
−1
exp{−T 2
2n(βk,j −eβk,j)2/2 −⌧n,p/β2
k,j}dβk,j
+
Z eβk,j+e✏n
0
exp{−T 2
2n(βk,j −eβk,j)2/2 −⌧n,p/β2
k,j}dβk,j
+
Z 1
eβk,j+e✏n
exp{−T 2
2n(βk,j −eβk,j)2/2 −⌧n,p/β2
k,j}dβk,j.
Deﬁne the ﬁrst term of the above as W1, the second as W2, and the third term as W3. First, we
shall show that W1 cT −1
2n exp{−T2n(2⌧n,p)1/2} for some positive constant c. By transforming
131

the variable t = βk,j −eβk,j,
W1
=
Z 0
−1
exp{−T 2
2nt2/2 + T 2
2nteβk,j −T 2
2n eβ2
k,j/2 −⌧n,p/t2}dt

Z 0
−1
exp{−T 2
2nt2/2 −⌧n,p/t2}dt

c3T −1
2n exp{−T2n(2⌧n,p)1/2},
for some constant c3, since
R
exp{−µ/t2 −⇣t2}dt = (⇡/⇣)−1/2 exp{−2(µ⇣)1/2} for µ > 0
and ⇣> 0.
Second, by changing the variable z = t −e✏,
W2
=
Z eβk,j
−e✏n
exp{−T 2
2n(z −eβk,j + e✏n)2/2 −⌧n,p/(z + e✏n)2}dz

exp{−⌧n,p/(eβk,j + e✏n)2}
Z 1
−1
exp{−T 2
2n(z −eβk,j + e✏n)2/2}

c4T −1
2n exp{−⌧n,p/(eβk,j + e✏n)2},
for some positive constant c4.
Third, by changing the variable z = t−eβk,j, there exists some positive constant c such that
W3
=
Z 1
e✏n
exp{−T 2
2nz2/2 −⌧n,p/(z + eβk,j)2}dz

exp{−T 2
2ne✏2
n/4}
Z 1
−1
exp{−T 2
2nz2/4}dz

c5T −1
2n exp{−c6T2n⌧1/2
n,p },
for some constants c5and c6, since e✏n ⇣(n⌫k⇤/⌧n,p)−1/4. Then,
g2(eβk,j) c3T −1
2n exp{−T2n(2⌧n,p)1/2} + c4T −1
2n exp{−⌧n,p/(eβk,j + e✏n)2}
+c5T −1
2n exp{−c6T2n⌧1/2
n,p }.
132

Since e✏n ⇣(n⌫k⇤/⌧n,p)−1/4, when eβk,j < e✏n, ⌧n,p/(eβk,j + e✏n)2 < ⌧n,p/(4e✏2
n) ⇣T2n⌧1/2
n,p , and
when eβk,j ≥e✏n, ⌧n,p/(eβk,j + e✏n)2 ⌧n,p/(4eβ2
k,j) < T2n⌧1/2
n,p . In overall, the right-hand side
of the above display would be dominated by the second term, which shows that g2(eβk,j) 
cT −1
2n exp{−⌧n,p/(eβk,j + e✏n)2} for some constant c. When eβk,j < 0, we can show the same
result by following exactly the same steps explained above.
We now present some auxiliary results that are used to prove Theorems 1 and 2. We make
use of the following simple union bound multiple times: for non-negative random variables
V1, . . . , Vm and a > 0,
P(
m
X
l=1
Vl > a) 
m
X
l=1
P(Vl > a/m) m max
1lm P(Vl > a/m).
(A.3)
We deﬁne some notations that are used in the subsequent proofs. Let t denote the true
data generating model, and let β0
t denote the true regression coefﬁcient corresponding to t. Let
ct = t \ k, ck = k \ t, and u = k [ t. Also, we deﬁne the cardinality of a model k as k and
in the same spirit, denote ck = |ck|, ct = |ct|, and t = |t|. {x}j denotes the j-th element of the
vector x, and diag{A}j refers to the j-th diagonal element in the square matrix A. We denote
χ2
m(λ) a non-central chi-square distribution with the degrees of freedom m and non-centrality
parameter λ; a central chi-square distribution is simply denoted by χ2
m.
An important property that is used in the subsequent proofs concerns the distribution of the
marginal ridge estimator. Let eβk = (X T
kX + 1/⌧n,pIk)−1X T
ky and eβk,j = {eβk}j. Then,
eβk,j ⇠N(β⇤
k,j, σ2⇤
k,j),
(A.4)
where β⇤
k,j = {(X T
kX + 1/⌧n,pIk)−1X T
kXtβ⇤
t}j and σ2⇤
k,j = σ2diag{(X T
kXk + 1/⌧n,pIk)−1}j.
It is also evident that (eβk,j −β⇤
k,j)2/σ2⇤
k,j ⇠χ2
1.
133

A set of technical results follow that are used in the proof of the main results. Deﬁne
H1n =
X
k:t(k,
|k|qn
mk(y)⇡(k)
mt(y)⇡(t) =
X
k:t(k,
|k|qn
⇡(k | y)
⇡(t | y) ,
H2n =
X
k:t*k,
|k|qn
mk(y)⇡(k)
mt(y)⇡(t) =
X
k:t*k,
|k|qn
⇡(k | y)
⇡(t | y) .
(A.5)
Lemma A.1.2. Fix ✏> 0.
Let Γd = {k : |k| qn, t ( k, |k| −|t| = d} for d =
1, . . . , qn −|t|. Suppose there exist constants c, δ > 0 such that maxk2Γd P
?
⇡(k | y)/⇡(t |
y) > ✏p−d/qn
 
cp−d(1+δ) for d = 1, . . . , qn −|t|. Then, H1n converges to zero in probability
as n tends to 1, where H1n is as in (A.5).
Proof. Clearly, |Γd| =
-p−|t|
d
.
. Using (A.3), we bound
P
n X
k:t(k
⇡(k | y)
⇡(t | y) > ✏
o
=
P
n qn−|t|
X
d=1
X
k2Γd
⇡(k | y)
⇡(t | y) > ✏
o

qn−|t|
X
d=1
P
n X
k2Γd
⇡(k | y)
⇡(t | y) > ✏/qn
o

qn−|t|
X
d=1
✓p −|t|
d
◆
max
k2ΓdP
n⇡(k | y)
⇡(t | y) > ✏p−d/qn
o

qn−|t|
X
d=1
cp−dδ.
Finally, Pqn−|t|
d=1
cp−dδ cqnp−δ ! 0 as n ! 1.
Lemma A.1.3. Fix ✏> 0 and let t = |t|. Deﬁne Γk,ck,ct = {k : |k| qn, |k| = k, |k\t| =
ck, |t\k| = ct} for k = 0, . . . , qn; ck = 0, . . . , k; ct = 1, . . . , t. Suppose
max
k2Γk,ck,ct
P
h⇡(k | y)
⇡(t | y) > ✏n−3p−kn−ckt−ti
cp−k(1+δ),
with some postive constants c and δ. Then, H2n converges to zero as n tends to 1, where H2n
is as in (A.5).
134

Proof. Clearly, |Γk,ck,ct| =
-p
k
.- k
ck
.- t
ct
.
.
P
n X
k:t*k
⇡(k | y)
⇡(t | y) > ✏
o

P
n qn
X
k=1
k
X
ck=0
t
X
ct=1
X
k2Γk,ck,ct
⇡(k | y)
⇡(t | y) > ✏
o

P
n qn
X
k=1
k
X
ck=0
t
X
ct=1
X
k2Γk,ck,ct
⇡(k | y)
⇡(t | y) > ✏
o

qn
X
k=1
k
X
ck=0
t
X
ct=1
P
n
X
k2Γk,ck,ct
⇡(k | y)
⇡(t | y) > ✏n−3o

qn
X
k=1
k
X
ck=0
t
X
ct=1
pkncktt
max
k2Γk,ck,ct
P
n⇡(k | y)
⇡(t | y) > ✏n−3p−kn−ckt−to

qn
X
k=1
k
X
ck=0
t
X
ct=1
pknckttp−k(1+δ) ! 0,
as n ! 1.
Lemma A.1.4. Suppose W follows a non-central chi-square distribution with the degree of
freedom mn that is a positive integer and the non-central parameter λn ≥0, i.e, W ⇠
χ2
mn(λn). Also, consider wn and tn such that wn ! 0 and tn ! 1 as n tends to 1. Also,
assume that mn ≺tn. Then,
P(W λnwn) c1λ−1
n exp{−λn(1 −wn)2},
(A.6)
And
P(W > λn + tn) c2
✓tn
2mn
◆mn/2
exp {mn/2 −tn/2} + c3λ1/2
n t−1
n exp
⇢
−t2
n
32λn
5
, (A.7)
where c1, c2, and c3 are some positive constants.
Proof. W can be expressed as W = Pmn
i=1{Zi + (λn/mn)1/2}2, where Zi
i.i.d
⇠N(0, 1) for
i = 1, . . . , m. Then, by the fact that P(Z > a) (2⇡)−1/2a−1 exp{−a2/2} for any a > 0, we
135

can show that there exist some positive constants c1 such that
P (W λnwn)
=
P
? mn
X
i=1
Z2
i + 2(λn/mn)1/2
mn
X
i=1
Zi + λn λnwn
 

P
?
m−1/2
n
mn
X
i=1
Zi −λ1/2
n (1 −wn)/2
 
=
P
?
|Z1| ≥λ1/2
n (1 −wn)/2
 
/2

c1λ−1
n exp{−λn(1 −wn)2/2},
since Z1 follows a standard normal distribution.
Also, by using Chernoffs’s bound and the fact that P(Z > a) (2⇡)−1/2a−1 exp{−a2/2}
for any a > 0, one can show that
P(W > λn + tn) = P
( mn
X
i=1
Z2
i + 2(λn/mn)1/2
mn
X
i=1
Zi > tn
)

P
 mn
X
i=1
Z2
i > tn/2
!
+ P
(
m−1/2
n
mn
X
i=1
Zi > λ−1/2
n
tn/4
)

c2
✓tn
2mn
◆mn/2
exp {mn/2 −tn/2} + c3λ1/2
n t−1
n exp
⇢
−t2
n
32λn
5
,
where c2 and c3 are some positive constants.
Lemma A.1.5. Consider Qk deﬁned in (2.6) for an arbitrary model k. Fix any δ > 0. For any
k with t ( k,
P
⇥
Qk/Qt > exp
?
−|k \ t|⌧2/3
n,p (n⌫k⇤)1/3 + |t|⌧1−δ/8
n,p
(n⌫k⇤)δ/8 ⇤
p−|k\t|(1+δ),
(A.8)
and for k such that t * k,
P
h
Qk/Qt > exp
n11β0
t
112
2n⌫u⇤/{2 log(⌧n,p/ log p)}
oi
p−|k|(1+δ).
(A.9)
136

Proof. By Lemma A.1.1, it is sufﬁcient to show that
P
"Y
j2t
(QU
k,j/QL
t,j) > exp{|t|⌧1−δ/8
n,p
(n⌫k⇤)δ/8}
#
+P
2
4 Y
j2k\t
QU
k,j > exp{−|k \ t|⌧2/3
n,p (n⌫k⇤)1/3}
3
5

p−|k\t|(1+δ).
(A.10)
We ﬁrst shall show that the ﬁrst term in the left-hand side of (A.10) is bounded above by
exp{−cn⌫k⇤} for some constant c.
P
"Y
j2t
QU
k,j
QL
t,j
> exp
?
|t|⌧1−δ/8
n,p
(n⌫k⇤)δ/8 
#

X
j2t
P
"
QU
k,j
QL
t,j
> exp
?
⌧1−δ/8
n,p
(n⌫k⇤)δ/8 
#
=
X
j2t
P

c0
✓n⌫k⇤+ 1/⌧n,p
n⌫⇤
t + 1/⌧n,p
◆−1/2
exp
n
−⌧n,p
⇣
1/(|eβk,j| + e✏n)2 −1/eβ⇤2
k,j
⌘o
> exp
?
⌧1−δ/8
n,p
(n⌫k⇤)δ/8 >

X
j2t
P[|eβk,j −β⇤
k,j| > ✏0] +
X
j2t
P[|eβt,j −β⇤
t,j| > ✏0],
(A.11)
for some small enough ✏0 > 0 and some positive constant c0 and eβ⇤
k,j 2 [eβk,j −e✏n, eβk,j + e✏n] \
(−e✏n,e✏n)c as deﬁned in Lemma A.1.1, and eβk,j and β⇤
k,j deﬁned in (A.4). The last inequality
in the above display asymptotically holds, since
⌧1−δ/8
n,p
(n⌫k⇤)δ/8 ≻⌧n,p/(|β⇤
k,j| −✏0 −e✏n)2,
for any δ > 0.
Since (eβk,j −β⇤
k,j)2/σ⇤2
k,j ⇠χ2
1 and σ⇤2
k,j ≥(n⌫k⇤+ 1/⌧n,p)−1, by using Lemma A.1.4, one
can show that the ﬁrst term in (A.11) bounded above by exp{−c1✏02n⌫k⇤} for some constant
c1. Similarly, the second term in (A.11) is bounded above by exp{−c2✏02n} for some constant
137

c2, since Assumption 5 states that X T
t Xt/n is asymptically isotropic. Therefore, (A.11) is
asymptotically bounded by p−qn(1+δ) by Assumption 3.
Next, we shall show that the second term in the left-hand side of (A.10) is bounded above
by exp{−c⌧1/3
n,p (n⌫k⇤)2/3} for some positive constant c. Since when j 2 k \ t and t ( k,
β⇤
k,j ⇣n−1,
P
2
4 Y
j2k\t
QU
k,j > exp{−|k \ t|⌧2/3
n,p (n⌫k⇤)1/3}
3
5

X
j2k\t
P
"
c0(n⌫k,j + 1/⌧n,p)−1/2 exp
(
−
⌧n,p
(|eβk,j| + e✏n)2
)
> exp{−⌧2/3
n,p (n⌫k⇤)1/3}
#
=
X
j2k\t
P

eβ2
k,j >
n
⌧1/2
n,p
-
(n⌫k⇤)1/3⌧2/3
n.p −log(n⌫k⇤+ 1/⌧n,p)/2 + log c0.−1/2 −e✏n
o2>

X
j2k\t
P
"
(eβk,j −β⇤
k,j)2/σ⇤
k,j > c00
✓⌧n,p
n⌫k⇤
◆1/3
(n⌫k⇤+ 1/⌧n,p)/σ2
#
,
for some positive contant c0 and c00. Since (eβk,j −β⇤
k,j)2/σ⇤
k,j ⇠χ2
1, by Lemma A.1.4 the last
quantity in the above display can be bounded by exp{−c⌧1/3
n,p (n⌫k⇤)2/3} for some contant c. By
Assumption 3, exp{−c⌧1/3
n,p (n⌫k⇤)2/3} ≺p−qn(1+δ) p|k\t|(1+δ)|, which proves the statement
(A.10).
We now shall show that the equation (A.9) holds for any δ > 0. The left-hand side of (A.9)
138

can be bounded above by
P
"Y
j2k
QU
k,j
⇣Y
j2t
QL
t,j
⌘−1
> exp
n11β0
t
112
2n⌫u⇤/{2 log(⌧n,p/ log p)}
o#

X
j2k
P
"
c(n⌫k⇤+ 1/⌧n,p)−1/2 exp
n
−⌧n,p/(|eβk,j| + e✏n)2o
> exp
n11β0
t
112
2n⌫u⇤/{4|k| log(⌧n,p/ log p)}
o #
+
X
j2t
P
"
c0(n⌫t⇤+ 1/⌧n,p)1/2 exp
n
⌧n,p/(eβ⇤2
t,j)
o
> exp
n11β0
t
112
2n⌫u⇤/{4|t| log(⌧n,p/ log p)}
o #

X
j2k
P
"
−
⌧n,p
(|eβk,j| + e✏n)2 >
11β0
t
112
2n⌫u⇤/{4|k| log(⌧n,p/ log p)} + log c
#
(A.12)
+
X
j2t
P
h
|eβ⇤
t,j| < c0011β0
t
11−1
2 (n⌫u⇤)−1/2{4|t| log(⌧n,p/ log p)}1/2⌧1/2
n,p
i
,
(A.13)
where c, c0, and c00 are some positive constants.
(A.12) is always zero since the left-hand side in the probability is always negative and the
right-hand side in the probability operator is always positive. So, we focus on (A.13) as below:
Since eβt,j −e✏n eβ⇤
t,j eβt,j + e✏n implies |eβt,j| −e✏n |eβ⇤
t,j| |eβt,j| + e✏n, (A.13) can be
bounded above by
X
j2t
P
h
|eβ⇤
t,j| < c0011β0
t
11−1
2 (n⌫u⇤)−1/2{4|t| log(⌧n,p/ log p)}1/2⌧1/2
n,p
i

X
j2t
P
h
|eβt,j| < c0011β0
t
11−1
2 (n⌫u⇤)−1/2{4|t| log(⌧n,p/ log p)}1/2⌧1/2
n,p + e✏n
i
,
where β⇤
t,j is deﬁned in (A.4). Since eβ2
t,j/σ2
t,j ⇠χ2
1(β⇤2
t,j/σ2
t,j) and σ2
t,j ⇣σ2/n for j 2 t,
by using Lemma A.1.4 and Assumption 5, one can show that the probability is bounded by
exp{−cn} for some constant c, and it is evident that exp{−cn} ≺p−|k|(1+δ), which completes
139

the proof of the Lemma.
Proofs of Main Results
Proof of Theorem 1. We have ⇡(t | y) = mt(y)⇡(t)/{P
k:|k|qn mk(y)⇡(k)}, since ⇡(k) =
0 for any k with |k| > qn. Recall H1n and H2n from (A.5) and note that ⇡(t | y) = (1 + H1n +
H2n)−1. Hence to show that ⇡(t | y) converges to one in probability, it is sufﬁcient to establish
that H1n and H2n both converge in probability to zero as n tends to 1. We shall prove the
Theorem by showing:
For any δ 2 (0, 8/3) and any model k 2 Γd (deﬁned in Lemma A.1.2),
P
⇡(k | y)
⇡(t | y) > ✏p−dq−1
n
>
p−d(1+δ),
(A.14)
and for any model k 2 Γk,ck,ct (deﬁned in Lemma A.1.3),
P
⇡(k | y)
⇡(t | y) > ✏n−3p−kn−ckt−t
>
cp−k(1+δ).
(A.15)
Then, it is evident that H1n and H2n both converge to zero in probability by Lemma A.1.2 and
A.1.3 respectively.
First, we shall show that (A.14) holds. For any k 2 Γd, recall that
P
h⇡(k | y)
⇡(t | y) > ✏p−dq−1
n
i
P
h
C−d
n,p
Qk
Qt
exp
n
−
1
2σ2
- eRk −eRt
.o
> ✏p−d/qn
i
.
Since eRk > R⇤
k and eRt < R⇤
t + ⌘, where ⌘= d1bβT
t bβt/⌧n,p for some constant d1 and bβt is
the ordinary least square estimator of βt in the true model t, by using (A.3), the term in the last
140

display can be bounded above by
P
h
C−d
n,p
Qk
Qt
exp
?
−
-
R⇤
k −R⇤
t
.
/(2σ2) + ⌘/(2σ2)
 
> ✏p−d/qn
i

P
h
C−d
n,p
Qk
Qt
pd(1+δ)+δ > ✏p−d/qn
i
(A.16)
+P
⇥
R⇤
t −R⇤
k > 2σ2d(1 + δ) log p
⇤
(A.17)
+P
⇥
exp{⌘/(2σ2)} > ✏pδ⇤
.
(A.18)
By using Lemma A.1.5, (A.16) is less than p−d(1+δ) when δ < 8/3. Since (R⇤
t −R⇤
k)/σ2 ⇠
χ2
|k\t|, by using (A.6) in Lemma A.1.4, we can show that (A.17) is bounded by cp−d(1+δ) for
some positive constant c. Since ⌧n,pn⌫t⇤⌘/d1σ2 bβT
t XT
t Xtbβt/σ2 ⇠χ2
|t|
-
β0T
t XT
t Xtβ0
t
.
, by
using the inequality (A.7) in Lemma A.1.4, (A.18) can be expressed as
P
⇥
exp
?
⌘/2σ2 
> ✏pδ⇤

P
⇥
⌧n,pn⌫t⇤⌘/d1σ2 > 2⌧n,pn⌫t⇤(log ✏+ δ log p)/d1
⇤

P
⇥bβT
t XT
t Xtbβt/σ2 > 2⌧n,pn⌫t⇤(log ✏+ δ log p)/d1
⇤

(nδ log p)|t|/2 exp{−c1δ(n log p)} + n−1/2(δ log p)−1 exp{−c2(n log p)2/n}

c3p−|k|(1+δ),
(A.19)
for some positive constant c1, c2, and c3, which proves that (A.14) holds.
141

Next, we consider (A.15). Recall that u = k [ t. By using (A.3), it can be shown that
P
h⇡(k | y)
⇡(t | y) > ✏n−3p−|k|n−|k\t||t|−|t|i

P
h
C−(|k|−|t|)
n,p
Qk
Qt
exp
n
−( eRk −eRt)/(2σ2)
o
> ✏n−3p−|k|n−|k\t||t|−|t|i

P
h
C−|k|−|t|)
n,p
Qk
Qt
exp
?
−(R⇤
k −R⇤
u)/(2σ2)
 
> n−3−|k\t||t|−|t|p−|k|(2+δ)+δi
+P
⇥
exp
?-
R⇤
t −R⇤
u
.
/(2σ2)
 
≥✏p|k|(1+δ)⇤
+ P
⇥
exp
-
⌘/(2σ2)
.
> pδ⇤

P
⇥
exp
?-
R⇤
t −R⇤
u
.
/2σ2 
> ✏p|k|(1+δ)⇤
(A.20)
+P
⇥
exp
-
⌘/2σ2.
> pδ⇤
(A.21)
+P
h
R⇤
k −R⇤
u < 2σ211β0
t
112
2n⌫u⇤/ log(⌧n,p/ log p)
i
(A.22)
+P
h
Qk/Qt > exp
n11β0
t
112
2n⌫u⇤/{2 log(⌧n,p/ log p)}
oi
.
(A.23)
Since (R⇤
t −R⇤
u)/σ2 follows a χ2
|u\t| distribution, (A.20) is also bounded by c1p−|k|(1+δ) with
some constant c1. By following the same steps regarding (A.19), one can show that (A.21)
is bounded by c2p−|k|(1+δ) for some constant c2. We note that (R⇤
k −R⇤
u)/σ2 ⇠χ2
|u\k|(λn)
with λn = β0T
t XT
t (Pu −Pk)Xtβ0
t, where Pk is the projection matrix of Xk. As discussed
in Narisetty and He (2014), λn ≥n⌫u⇤
11β0
t
112
2.
Hence, by using Lemma A.1.4, one can
show that (A.22) is bounded by exp{−c3
11β0
t
112
2n⌫u⇤/ log(⌧n,p/ log p)} for some constant c3.
Lemma A.1.5 states that (A.23) is bounded by p−|k|(1+δ). In summary, since qn ≺⌧n,p/ log p
by Assumption 3, there exists some positive constant c4 such that P[⇡(k | y)/⇡(t | y) >
✏n−3p−|k|n−|k\t||t|−|t|] c4p−|k|(1+δ). which completes the proof of Theorem 1.
Proof of Corollary 2. Recall the penalty term of a model k, Q⇤
k, based on the piMoM priors
is
Q⇤
k =
Z
exp
?
−(βk −bβk)T⌃⇤−1
k
(βk −bβk)/(2σ2) −
|k|
X
j=1
⌧n,p/β2
k,j −r
|k|
X
j=1
log(β2
k,j)
 
dβk,
142

in (2.7). Since, for any ✏> 0, exp
⇥
−P|k|
j=1{✏⌧n,p/β2
k,j + r log(β2
k,j)}
⇤
is bounded above with
respect to βk,j, Q⇤
k C
R
exp{−(βk −bβk)T⌃⇤−1
k
(βk −bβk)/(2σ2)−P|k|
j=1(1−✏)⌧n,p/β2
k,j}dβk
for some constant C. Following the exactly same steps in Lemma A.1.1,
Q⇤
k C0(n⌫⇤
k)−1/2 Q|k|
j=1 exp{−(1 −✏)⌧n,p/(|bβk,j| + e✏n)2} for some constant C0 > 0.
We shall show that the model selection procedure based on piMoM priors as in (2.4) assures
consistency by proving that Q⇤
k and Qk are asymptotically equivalent.
Next, we shall show that Q⇤
k is bounded below by
C(n⌫⇤
k)−1/2 Q|k|
j=1 exp{−(1 −✏)⌧n,p/bβ⇤2
k,j} for some constant C > 0 and bβ⇤
k,j 2 [bβk,j −
e✏n, bβk,j +e✏n]. Since exp
?
−✏⌧n,p/β2
k,j +r log(β2
k,j)
 
can be minimized in [bβk,j −e✏n, bβk,j +e✏n],
by following the proof of Lemma A.1.1,
Z 1
−1
exp{−n⌫⇤
k(β −bβk,j)2/(2σ2) −⌧n,p/β2 −r log(β2)}dβ
≥
Z bβk,j+e✏n
bβk,j−e✏n
exp{−n⌫⇤
k(β −bβk,j)2/(2σ2) −(1 −✏)⌧n,p/β2}
⇥exp{−✏⌧n,p/β2 −r log(β2)}dβ
≥
C(n⌫⇤
k)−1/2 exp
n
−(1 −✏)⌧n,p/bβ⇤2
k,j
o
,
where C is some constant and bβ⇤
k,j 2 [bβk,j −e✏n, bβk,j + e✏n] \ (−e✏n,e✏n)c.
Therefore, due to the asymptotic similarity between the ridge estimator and the least square
estimator, the lower and upper bounds of Q⇤
k are asymptotically equivalent to those of Qk with
the penalty parameter (1 −✏)⌧n,p, which assures the strong consistency of the model selection
based on the piMoM priors.
143

Proof of Theorem 3. Under a situation where σ2 is unknown, it is clear that
mk(y)
=
⌧
−|k|
2
n,p
Z
(2⇡σ2)−n+|k|
2
Z
exp
(
|k|
✓2
σ2
◆1/2
−(βk −eβk)T e⌃−1
k (βk −eβk)
2σ2
)
⇥exp
8
<
:−
|k|
X
j=1
⌧n,p
β2
k,j
9
=
; ⇡(σ2)dβkdσ2,
where ⇡(σ2) is the prior for σ2 (Inverse-gamma density with hyperparameters a0 and b0).
First, we shall show that the ratio between marginal likelihoods of a model k and the true
model t can be bounded as
mk(y)
mt(y)

c
|k|−|t|
2
 eRk + 2b0
eRt + 2b0
!−n/2−a0
exp
8
<
:−
|k|
X
j=1
⌧n,p
(|eβk,j| + e✏n)2 +
|t|
X
j=1
⌧n,p
eβ⇤2
t,j
9
=
;
⇥(n⌫k⇤⌧n,p + 1)−|k|/2
(n⌫⇤
t⌧n,p + 1)−|t|/2 ,
(A.24)
where eβ⇤
t,j 2 [eβt,j −e✏n, eβt,j + e✏n] \ (−e✏n,e✏n)c for j 2 1, . . . , |t| and c is some constant. Next,
we shall show that {( eRk + 2b0)/( eRt + 2b0)}−n/2−a0 exp{−( eRk −eRt)/(2σ2
0(1 + un))},
where σ2
0 is the true regression variance that involves in the data-generating process, and un
is some random variable that is concentrated around a ﬁnite value with at least probability
1−exp{−cn} for some constant c. Then, by following the same steps in the proof of Theorem
1, the proof of Corollary 2 is completed.
144

By Lemma A.1.1, the marginal likelihood of a model k can be bounded by
mk(y)

{c1(n⌫k⇤⌧n,p + 1)}−|k|
2
Z
(σ2)−n+2a0
2
−1 exp
(
|k|
✓2
σ2
◆1/2
−
eRk + 2b0
2σ2
)
⇥exp
8
<
:−
|k|
X
j=1
⌧n,p
(|eβk,j| + e✏n)2
9
=
; dσ2

{c1(n⌫k⇤⌧n,p + 1)}−|k|
2 exp
8
<
:−
|k|
X
j=1
⌧n,p
(|eβk,j| + e✏n)2
9
=
;
⇥(1 + exp{2|k|})
⇣
eRk + 2b0
⌘−n+2a0
2
,
for some constant c1.
Also, by using Lemma A.1.1, one can show that
mk(y)
≥
{c2(n⌫k⇤⌧n,p + 1)}−|k|
2
Z
(σ2)−n+2a0
2
−1 exp
(
|k|
✓2
σ2
◆1/2
−
eRk + 2b0
2σ2
)
⇥exp
8
<
:−
|k|
X
j=1
⌧n,p
eβ⇤2
k,j
9
=
; dσ2
≥
{c2(n⌫k⇤⌧n,p + 1)}−|k|
2 exp
8
<
:−
|k|
X
j=1
⌧n,p
eβ⇤2
k,j
9
=
;
⇣
eRk + 2b0
⌘−n+2a0
2
,
where c2 is some constant and eβ⇤
k,j 2 [eβk,j −e✏n, eβk,j +e✏n]\(−e✏n,e✏n)c for j 2 1, . . . , |k|. These
results shows that (A.24) holds.
Next, we consider the asymptotic behavior of {( eRk + 2b0)/( eRt + 2b0)}−n/2−a0 in (A.24).
Deﬁne ⇢n as the follows:
⇢n = ( eRt + 2b0)/(nσ2
0) −1.
145

Since −log(1 −u) < u/(1 −u) for u 2 R,
−log{( eRk + 2b0)/( eRt + 2b0)}
=
−log[1 + ( eRk −eRt)/{n(1 + ⇢n)σ2
0}]

( eRt −eRk)/{nσ2
0(1 + un)},
where un = ⇢n + ( eRk −eRt)/(nσ2
0).
Since (R⇤
k −R⇤
u)/σ2
0 ⇠χ|u\k|(λn) with λn = β0T
t XT
t (Pu −Pk)Xtβ0
t/σ2
0, by using Lemma
A.1.4 one can show that
P (|un −λn/n| > ✏)

P (|⇢n| > ✏/4) + P
?
(R⇤
t −R⇤
u)/(nσ2
0) > ✏/4
 
+P
?==(R⇤
k −R⇤
u)/(nσ2
0) −λn/n
== > ✏/4
 
+ P
-
⌘/2nσ2
0 > ✏/4
.

exp{−c0n} + P
?==(R⇤
k −R⇤
u)/(nσ2
0) −λn/n
== > ✏/4
 

exp{−c00n},
for some constant c0 and c00, and ⌘is deﬁned in the proof of Theorem 1. Also, by Assumption
5, λn/n will be bounded below and above.
Proof of Corollary 4. Since we showed that the asymptotic equivalence between Qk and Q⇤
k
in the proof of Corollary 2, by following exactly same steps in the proof of Theorem 3 we can
prove the model selection consistency under piMoM prior densities.
Proof of Proposition 5. We shall show that for any ↵k = bβk + ✏n with ✏n = {✏n,j}j=1,...,|k|
and |✏n,j| ≻✏⇤
n for at least one j 2 {1, . . . , |k|}, P{g(↵k; k) < g(eβ⇤
k; k)} ! 0 as n tends to
1, where eβ⇤
k 2 B(bβk; ✏⇤
n) with ✏⇤
n ⇣(⌧n,p/n)1/3. More speciﬁcally, we set eβ⇤
k,j = bβk,j + ✏⇤
n for
j 2 t and eβ⇤
k,j = bβk,j for j 2 tc. Without loss of generality, we assume that X T
j Xj = n for
j = 1, . . . , p.
146

Note that
g(↵k; k)
=
||Xk↵k −Xkbβk||2
2 +
|k|
X
j=1
⌧n,p/|↵k,j| + Dn
=
|k|
X
j=1
{cjn✏2
n,j + ⌧n,p/|bβk,j + ✏n,j|} + Dn,
for some constants cj such that CL < cj < CU for j = 1, . . . , |k|, and some randome variable
Dn that are not relevant to ↵k. Then,
P{g(↵k; k) < g(eβ⇤
k; k)}

P
2
4
|k|
X
j=1
(
cjn✏2
n,j +
⌧n,p
|bβk,j + ✏n,j|
)
<
|k|
X
j=1
(
cjn✏⇤2
n + ⌧n,p
|eβ⇤
k,j|
)3
5

P
"
X
j2S⇤\Sk,n
(
cjn✏2
n,j +
⌧n,p
|bβk,j| + |✏n,j|
−tn,j
)
<
X
j2S⇤\Sk,n
(
cjn✏⇤2
n + ⌧n,p
|eβ⇤
k,j|
) #
(A.25)
+P
"
X
j2S⇤\Sc
k,n
(
cjn✏2
n,j +
⌧n,p
|bβk,j| + |✏n,j|
−tn,j
)
<
X
j2S⇤\Sc
k,n
(
cjn✏⇤2
n + ⌧n,p
|eβ⇤
k,j|
) #
(A.26)
+P
" X
j2S⇤c
(
cjn✏2
n,j +
⌧n,p
|bβk,j| + |✏n,j|
+
X
j2S⇤
tn,j
|S⇤c|
)
<
X
j2S⇤c
(
cjn✏⇤2
n + ⌧n,p
|eβ⇤
k,j|
) #
,
(A.27)
where tn is an arbitrary sequence such that tn,j = n2/3⌧1/3
n,p ✏n,j, and S⇤= {j 2 {1, . . . , p} :
|✏n,j| ≻✏⇤
n}, and Sk,n = {j 2 k : |bβk,j| < ✏⇤
n}. Then, to complete the proof, it is sufﬁcient to
147

show that each of (A.25), (A.26), and (A.27) converges to zero.
Since n(bβk,j −β0
t,j)2/σ2 ⇠χ2
1 for j = 1, . . . , |k|,
P(|bβt,j −β0
t,j| > ⇣n) (⇡n⇣2
n/2)−1/2 exp{−n⇣2
n/(2σ2)},
for any ⇣n > 0. This implies that Sk,n = t at least probability
1 −|tc|(⇡n✏⇤2
n /2)−1/2 exp{−n✏⇤2
n /(2σ2)}. Therefore, the equation (A.25) can be asymp-
totically bounded by
X
j2S⇤\t
P
"
cjn✏2
n,j +
⌧n,p
2|✏n,j| −tn,j < cjn✏⇤2
n +
⌧n,p
|bβk,j + ✏⇤
n|
#

X
j2S⇤\t
P
h
|bβk,j + ✏⇤
n| < c⌧n,p(n✏2
n,j −tn,j + ⌧n,p/|✏n,j|)−1i
,
for some positive constant c. Consider Lemma A.1.4 with λn = n✏⇤2
n /σ2 and
wn = c2⌧2
n,p/{✏⇤2
n (n✏2
n,j −tn,j + ⌧n,p/|✏n,j|)2} for j 2 S⇤\ t. Since n✏2
n,j ≻n1/3⌧2/3
n,p
for j 2 S⇤implies wn ! 0, Lemma A.1.4 guarantees that the last display is bounded by
c0|S⇤\ t|λ−1
n exp{−λn(1 −wn)2} for some constant c0, which means that (A.25) converges to
zero as n tends to 0. By following the same steps, one can show that (A.26) converges to zero.
Also, (A.27) can be asymptotically bounded by
X
j2S⇤c\t
P
"
cjn✏2
n,j +
⌧n,p
2|✏n,j| + c min
j2S⇤tn,j < cjn✏⇤2 +
⌧n,p
|bβk,j + ✏⇤
n|
#
+
X
j2S⇤c\tc
P
"
cjn✏2
n,j +
⌧n,p
2|bβk,j + ✏⇤
n|
+ c min
j2S⇤tn,j < cjn✏⇤2 +
⌧n,p
|bβk,j + ✏⇤
n|
#

X
j2S⇤c\t
P

|bβk,j + ✏⇤
n| < c0⌧n,p(n✏2
n,j −n✏⇤2
n + c min
j2S⇤tn,j + ⌧n,p/|✏n,j|)−1
>
+
X
j2S⇤c\tc
P

|bβk,j + ✏⇤
n| < c00⌧n,p(n✏2
n,j −n✏⇤2
n + c min
j2S⇤tn,j + ⌧n,p/|✏n,j|)−1/2
>
,
148

where c, c0, and c00 are some positive constants. For the ﬁrst term in the last line of the above dis-
play, by setting λn = n✏⇤2/σ2 and wn = c2⌧2
n,p/{✏⇤2
n (n✏2
n,j−n✏⇤
n+c minj2S⇤tn,j+⌧n,p/|✏n,j|)2},
we can apply Lemma A.1.4. Since wn ≺⌧2
n,p(✏⇤
n minj2S⇤tn,j)−2 implies wn ! 0, the ﬁrst term
in the above display converges to zero by Lemma A.1.4. Similarly, the second term also con-
verges to zero.
A.2
Functional Horseshoe Prior for Nonparametric Subspace Shrinkage
Proof of Lemma 1. As discussed in the paragraphs following Lemma 1 when L(Φ0) ( L(Φ),
we can generate a new basis eΦ = [Φ0, Φ1] such that ΦT
0Φ1 = 0 and L(Φ) = L(eΦ), which
implies QeΦ = QΦ. Then,
Φ
✓
ΦTΦ +
!
1 −!Φ
T(I −Q0)Φ
◆−1
Φ
T
=
eΦ
✓
eΦT eΦ +
!
1 −!
eΦ
T(I −Q0)eΦ
◆−1
eΦ
T
=
[Φ0, Φ1]
2
64
(ΦT
0Φ0)−1
0
0
(1 −!)(ΦT
1Φ1)−1
3
75
2
64
ΦT
0
ΦT
1
3
75
=
(1 −!)QeΦ + !Q0
=
(1 −!)QΦ + !Q0.
Proof of Lemma 2. From Polson and Scott (2012) it follows that
Z 1
0
!An−1(1 −!)Bn−1 exp{−Hn!}d! = Γ(An)Γ(Bn)
Γ(An + Bn) exp{−Hn}
1
X
m=0
(An)(m)
(An + Bn)(m)
Hn
m
m! ,
where (a)(m) = a(a+1) . . . (a+m−1). We shall show that P1
m=0
n
(Bn)(m)
(An+Bn)(m)
Hnm
m!
o
≥1+QL
n.
By using Lemma A.2.1 and Stirling’s approximation, i.e., m! ⇣mm+1/2 exp{−m}, it follows
149

that
1
X
m=0
⇢
(Bn)(m)
(An + Bn)(m)
Hn
m
m!
5
=
1 +
Bn
An + Bn
(
Hn +
1
X
m=1

(Bn + 1)(m)
(An + Bn + 1)(m)
Hn
m+1
(m + 1)!
>)
≥
1 +
Bn
An + Bn
(
Hn +
1
X
m=1

(Bn + m)!
(An + Bn + m)!
Hn
m+1
(m + 1)!
>)
≥
1 +
Bn
An + Bn
(
Hn + D
1
X
m=1
" ✓
Bn + m
An + Bn + m
◆An+Bn+m+1/2
(Bn + m)−An
eAn Hn
m+1
(m + 1)!
#)
≥
1 +
Bn
An + Bn
(
Hn + D
Tn
X
m=1
✓
Bn + 1
An + Bn + 1
◆1/2
(Bn + m)−An
⇥
✓
Bn + m
An + Bn + m
◆An+Bn+m
eAn Hn
m+1
(m + 1)!
>)
≥
1 +
Bn
An + Bn
⇢
Hn + D
✓
Bn + 1
An + Bn + 1
◆1/2
(Bn + Tn)−An
⇥exp
(
A2
n
2(An + Bn + Tn)
5 Tn+1
X
m=2
Hn
m
m!
)
,
(A.28)
where Tn = max{A2
n, 3 dHne]}, and D is some positive constant.
Since Hn < (Tn+2) exp{1}, by using the Stirling’s approximation, the term PTn+1
m=2 Hn/m!
in (A.28) can be expressed as follows:
Tn+1
X
m=2
Hm
n
m!
=
exp{Hn} −1 −Hn −
1
X
m=Tn+2
Hm
n
m!
⪯
exp{Hn} −1 −Hn −(Tn + 2)−1/2
1
X
m=Tn+2
✓exp{1}Hn
Tn + 2
◆m

exp{Hn} −1 −Hn −(Tn + 2)−1/2
150

Therefore, (A.28) can be bounded by
1 +
Bn
An + Bn
(
Hn + D
✓
Bn + 1
An + Bn + 1
◆1/2
(Bn + Tn)−An
⇥
-
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+
)
≥
1 +
BnHn
An + Bn
+
DBn
(An + Bn)3/2(Bn + Tn)−An -
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+ ,
where (·)+ denotes the positive hinge function (i.e., for any t 2 R, (t)+ = t, if t > 0, and
(t)+ = 0, otherwise).
Also, since (Bn + m)!/(An + Bn + m)! < 1 for any positive integer m, it follows that
Hn +
1
X
m=1

(Bn + m)!
(An + Bn + m)!
Hn
m+1
(m + 1)!
>
exp{Hn},
which completes the proof.
Lemma A.2.1. For arbitrary positive sequences un and wn,
✓
1 −
un
un + wn
◆un+wn
≥exp
⇢
−un +
u2
n
2(un + wn)
5
.
(A.29)
Proof. By Talyor’s theorem, there exists q⇤
n 2 (0, un/(un + wn)) such that
✓
1 −
un
un + wn
◆un+wn
=
exp
⇢
(un + wn) log
✓
1 −
un
un + wn
◆5
=
exp
⇢
(un + wn)
✓
−
un
un + wn
+
1
(1 −q⇤
n)2
u2
n
2(un + wn)2
◆5
≥
exp
⇢
−un +
u2
n
2(un + wn)
5
.
151

Lemma A.2.2.
n
11Q0Φβ −Q0Y
112
n,2/σ2 | Y, ! ⇠χ2
d0,
and
n
11Q1Φβ −(1 −!)Q1Y
112
n,2/{(1 −!)σ2} | Y, ! ⇠χ2
kn−d0.
Proof. Recall that
β | Y, ! ⇠N(eβ!, e⌃!),
where
eβ! =
✓
Φ
TΦ +
!
1 −!Φ
T(I −Q0)Φ
◆−1
Φ
TY,
e⌃! = σ2
✓
Φ
TΦ +
!
1 −!Φ
T(I −Q0)Φ
◆−1
.
As shown in the proof of Lemma 1, Φ
-
ΦTΦ +
!
1−!ΦT(I −Q0)Φ
.−1 ΦT = (1 −!)QΦ + !Q0,
so
E [Q0Φβ | Y, !] = Q0Y
Var [Q0Φβ | Y, !] = σ2Q0,
which shows that n
11Q0Φβ −Q0Y
112
n,2/σ2 | Y, ! ⇠χ2
d0.
Similarly,
E [Q1Φβ | Y, !] = (1 −!)Q1Y
Var [Q1Φβ | Y, !] = σ2(1 −!)Q1,
which proves that n
11Q1Φβ −(1 −!)Q1Y
112
n,2/{(1 −!)σ2} | Y, ! ⇠χ2
kn−d0.
152

Proof of Theorem 6. Let β⇤denote the projection of the true F0 on the basis {φj}1jkn, i.e.,
β⇤= argminβ2Rkn
11F0 −Φβ
11
2,n.
(A.30)
We shall treat β⇤as the pseudo-true parameter and study the posterior concentration of Φβ in
the posterior around Φβ⇤.
To prove Theorem 6, it is sufﬁcient to show that the posterior probability in the equation
(4.10) converges in probability to zero. The quantity in (4.10) can be decomposed as follows:
P
h11Φβ −F0
11
n,2 > M 1/2
n
| Y
i

P
h11Φβ −Φβ⇤11
n,2 > M 1/2
n /2 | Y
i
+ 1
h11Φβ⇤−F0
11
n,2 > M 1/2
n /2
i
,
where β⇤is deﬁned in (A.30) and 1(·) is the indicator function. The second term on the right-
hand side of this expression is always zero when F0 2 L(Φ0), since we assume that the column
space of Φ0 is contained in the column space of Φ, and its expectation with respect to the true
density is asymptotically zero when F T
0 (I −Q0)F0 ⇣n from (4.9). Therefore, we focus on the
ﬁrst term on the right-hand side. Since Φβ = Q1Φβ + Q0Φβ, by Lemma 1. the ﬁrst term can
be decomposed as
P
h11Φβ −Φβ⇤11
n,2 > M 1/2
n /2 | Y
i
= E!|Y
h
P
⇣11Φβ −Φβ⇤11
n,2 > M 1/2
n /2 | Y, !
⌘i

E!|Y
h
P
⇣11Φβ −Φeβ!
11
n,2 > M 1/2
n /4 | Y, !
⌘i
+E!|Y
h
P
⇣11Φeβ! −Φβ⇤11
n,2 > M 1/2
n /4 | Y, !
⌘i

E!|Y
h
P
⇣11Q1Φβ −(1 −!)Q1Y
11
n,2 > M 1/2
n /8 | Y, !
⌘i
+E!|Y
h
P
⇣11Q1Φβ⇤−(1 −!)Q1Y
11
n,2 > M 1/2
n /8 | Y, !
⌘i
+E!|Y
h
P
⇣11Q0Φβ −Q0Y
11
n,2 > M 1/2
n /8 | Y, !
⌘i
+1
h11Q0Φβ⇤−Q0Y
11
n,2 > M 1/2
n /8
i
,
153

where Φeβ! = (1 −!)QΦY + !Q0Y = (1 −!)Q1Y + Q0Y .
We denote
W1
=
P
⇣11Q1Φβ −(1 −!)Q1Y
11
n,2 > M 1/2
n /8 | Y, !
⌘
,
W2
=
P
⇣11Q1Φβ⇤−(1 −!)Q1Y
11
n,2 > M 1/2
n /8 | Y, !
⌘
,
W3
=
P
⇣11Q0Φβ −Q0Y
11
n,2 > M 1/2
n /8 | Y, !
⌘
.
The indicator function in the fourth term converges to zero in probability, since
11Q0Y −
Q0Φβ⇤112
2,n achieves the parametric optimal rate. To complete the proof we show that the ex-
pectations of W1, W2, and W3 with respect to the marginal posterior distribution of ! converge
to zero in probability.
First consider W3. Since n
11Q0Φβ −Q0Y
112
2,n/σ2 | Y, ! ⇠χ2
d0 by Lemma A.2.2, by using
Lemma A.1.4 it follows that
E!|Y [W3] = E!|Y
h
P
n11Q0Φβ −Q0Y
11
2,n > M 1/2
n /8 | Y, !
oi

C
✓nMn
64σd0
◆d0/2
exp{−nMn/(128σ2)},
for some constant C.
The last quantity converges to zero as n tends to 1, which implies that E!|Y [W3] = op(1).
Now we obtain the bounds on W1. By Lemma A.2.2 n
11Q1Φβ−(1−!)Q1Y
112
2,n/{(1−!)σ2} |
Y ⇠χ2
kn−d0. By using Lemma A.1.4, it follows that
W1


nMn
64σ2(kn −d0)(1 −!)−1
> kn−d0
2
exp
⇢kn −d0
2
−nMn
128σ2(1 −!)−1
5
⇥1
nMn
64σ2 (1 −!)−1 > kn −d0
>
+ 1
nMn
64σ2 (1 −!)−1 kn −d0
>
.
We denote the two terms in this expression as W1,1 and W1,2.
154

By using Lemma 2 and deﬁning b! = (kn −d0)/{nMn/(64σ2) + kn −d0}, it follows that
E!|Y [W1,1]
=
1
m(Y )
nMn exp{1}
64σ2(kn −d0)
> kn−d0
2
Z 1
mn
!a+ kn−d0
2
−1(1 −!)b−kn−d0
2
−1
⇥exp
⇢
−nMn
128σ2(1 −!)−1 −Hn!
5
d!

1
m(Y )
nMn exp{1}
64σ2(kn −d0)
> kn−d0
2
Z 1
mn
!a−1(1 −!)b−1 exp {−Hn!} d!
⇥b!
kn−d0
2
(1 −b!)−kn−d0
2
exp
⇢
−nMn
128σ2(1 −b!)−1
5
=
1
m(Y ) exp
⇢
−nMn
128σ2
5 Z 1
mn
!a−1(1 −!)b−1 exp {−Hn!} d!,
(A.31)
where mn = max[0, 1 −nMn/{16σ2(kn −d0)}].
Also,
E!|Y [W1,2] = P!|Y

! < 1 −
nMn
64σ2(kn −d0)
>
=
1
m(Y )
Z 1−
nMn
64σ2(kn−d0)
0
!a+(kn−d0)/2−1(1 −!)b−1 exp{−Hn!}d!

1
m(Y )
✓
nMn
64σ2(kn −d0)
◆b−1 Z 1
0
!a+(kn−d0)/2−1 exp{−Hn!}d!

✓
nMn
64σ2(kn −d0)
◆b−1 Γ(a + b + (kn −d0)/2)
Γ(a + (kn −d0)/2)Γ(b)H−1
n 1
✓
1 −
nMn
64σ2(kn −d0) ≥0
◆
⇥exp{Hn}
"
1 +
bHn
a + b + (kn −d0)/2 + D
b(b + Tn)−a−(kn−d0)/2
(a + b + (kn −d0)/2)3/2
⇥
-
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+
#−1
,
(A.32)
where Tn = max{(a + (kn −d0)/2)2, 3 dHne} and D is some constant.
155

We now consider two cases: (i) when F0 2 L(Φ0) and (ii) when F T
0 (I −Q0)F0 ⇣n.
Case (i) F0 2 L(Φ0):
Recall that in this case Mn = ⇣nn−1 for any arbitrary diverging sequence ⇣n. First, we
show that E!|Y [W1]
p! 0 by proving that E!|Y [W1,1]
p! 0 and E!|Y [W1,2]
p! 0.
Applying Lemma 2, it follows that (A.31) is bounded above by
E!|Y [W1,1] 
C exp {−nMn/(128σ2)}
-
1 +
b
a+b exp{Hn}
.
1 + δn + un
Db
a+b (exp{Hn} −1 −Hn −(Tn + 2)−1/2)+

C exp
⇢
−nMn
128σ2
5 ✓
1 +
b
a + b exp{Hn}
◆
,
(A.33)
where δn = bHn/(a + b + (kn −d0)/2) and un = (a + b)(b + Tn)−an−(kn−d0)/2/(a + b + (kn −
d0)/2)3/2 with Tn = max{(a + (kn −d0)/2)2, 3 dHne}, and C and D are some constants.
Since 2Hn ⇠χ2
kn−d0, by Lemma A.1.4 and deﬁning qn = k−1/2
n
(log kn)1/2(−log b)1/2, it
follows that
P [Hn > knqn/2] exp{−cknqn},
(A.34)
for some constant c. Hence, by the condition that kn log kn ≺−log b, it is clear that b exp{Hn} =
op(1), which shows that E!|Y [W1,1] = op(1).
Similarly, since Γ(b)−1 ⇣b, (A.32) is bounded by
C0b exp{Hn}
✓
nMn
64σ2(kn −d0)
◆b−1
,
for some constant C0. By (A.34), b exp{Hn} = op(1), which implies E!|Y [W1,2] = op(1).
We next show that E!|Y [W2] converges in probability to zero. Applying Lemma 2, it fol-
156

lows that
E!|Y [W2] = E!|Y
h
P
⇥11(1 −!)Q1Y −Q1Φβ⇤11
n,2 > M 1/2
n /8 | Y, !
⇤i
=
P!|Y
"
! < 1 −
✓nMn
64σ2Hn
◆1/2#
=
1
m(Y )
Z 1−
⇣
nMn
128σ2Hn
⌘1/2
0
!a+(kn−d0)/2−1(1 −!)b−1 exp{−Hn!}d!

1
(
1 −
✓
nMn
128σ2Hn
◆1/2
≥0
)
1
m(Y )
✓nMn
64σ2Hn
◆(b−1)/2
⇥
Z 1
0
!a+(kn−d0)/2−1 exp{−Hn!}d!

1
(
1 −
✓
nMn
128σ2Hn
◆1/2
≥0
)
Γ(a + b + (kn −d0)/2)
Γ(b)Γ(a + (kn −d0)/2)
✓
nMn
128σ2Hn
◆(b−1)/2
⇥exp{Hn}
⇢
1 + δn + un
Db
a + b
-
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+
5−1

Cb
✓nMn
128σ2
◆(b−1)/2
H1/2
n
exp{Hn},
where C is some constant, and δn and un are deﬁned following (A.33).
From (A.34), it follows that b{nMn/(128σ2)}(b−1)/2H1/2
n
exp{Hn} is bounded by
b{nMn/(128σ2)}(b−1)/2(knqn/2)1/2 exp{knqn/2} with probability greater than 1−exp{−cknqn}
from which it follows that E!|Y [W2] = op(1).
Case (ii) F T
0 (I −Q0)F0 ⇣n:
Recall that in this case Mn = ⇣nn−2↵/(1+2↵) log n for any arbitrary diverging sequence ⇣n,
157

and δn and un are deﬁned following (A.33). From (A.31) it follows that
E!|Y [W1,1] 
1
m(Y ) exp
⇢
−nMn
128σ2
5 Z 1
mn
!a−1(1 −!)b−1 exp {−Hn!} d!

C exp
⇢
−nMn
128σ2
5
1 +
b
a+b exp{Hn}
1 + δn + un
Db
a+b (exp{Hn} −1 −Hn −(Tn + 2)−1/2)+
,
for some constant C.
By Lemma A.1.4, for any sequence wn ! 0, Hn is larger than wnF T
0 Q1F0/σ2 with prob-
ability greater than 1 −exp{−cF T
0 Q1F0(1 −wn)2/σ2} for some constant c. Since F T
0 (I −
Q0)F0 ⇣n implies F T
0 Q1F0 ⇣n, the last line in the above display can be expressed as
C0 exp
⇢
−nMn
128σ2(kn −d0)3/2(b + Tn)(kn−d0)/2
5
+ op(1),
where Tn = max{(a + (kn −d0)/2)2, 3Hn} and C0 is some positive constant. Therefore, to
show E!|Y [W1,1]
p! 0, it is sufﬁcient to prove that T (kn−d0)/2
n
exp{−nMn/(128σ2)} = op(1).
For any ✏> 0,
P

T (kn−d0)/2
n
exp
⇢
−nMn
128σ2
5
> ✏
>

P

(3Hn)(kn−d0)/2 exp
⇢
−nMn
128σ2
5
> ✏
>
+ P
⇥
3Hn < (a + (kn −d0)/2)2⇤

P [log Hn > ⇣n log n] + P
⇥
3Hn < (a + (kn −d0)/2)2⇤
.
Since ⇣n ! 1 as n tends to 1, from (A.7) in Lemma A.1.4, it follows that the ﬁrst term in
the above display can be bounded above by exp{−c0(n⇣
n −F T
0 Q1F0/σ2)} for some constant c0.
Similarly, from (A.6) in Lemma A.1.4, the second term is bounded by exp{−c00F T
0 Q1F0/σ2}
with some constant c00, which proves that E!|Y [W1,1]
p! 0.
Since nMn ≻kn, the indicator function 1(1 −nMn/(64σ2(kn −d0)) ≥0) in (A.32) is
zero when n is large enough, which results in E!|Y [W1,2]
p! 0.
158

The marginal posterior mean of W2 can be decomposed as
E!|Y [W2]

P!|Y
11(1 −!)Q1Y −Q1Y
11
n,2 > 1
16M 1/2
n
>
+1
11Q1Y −Q1Φβ⇤11
n,2 > 1
16M 1/2
n
>
.
Results provided by Zhou et al. (1998) (see equation (4.9) on page 66) show that the second
term in the previous expression is op(1). The ﬁrst term can be expressed as
P!|Y
"
! >
✓
nMn
256σ2Hn
◆1/2#
=
1
m(Y )
Z 1
⇣
nMn
256σ2Hn
⌘1/2 !a+(kn−d0)/2−1(1 −!)b−1 exp{−Hn!}d!

1
m(Y ) exp
n
−H1/2
n
-
nMn/(256σ2)
.1/2o Z 1
0
!a+(kn−d0)/2−1(1 −!)b−1d!


un exp{−Hn} Db
a + b
-
exp{Hn} −1 −Hn −(Tn + 2)−1/2.
+
>−1
⇥exp
n
−H1/2
n
-
nMn/(256σ2)
.1/2o
,
for some positive constant D. Since Hn/n = Op(1) and −log b ≺n1/2k1/2
n , the above quantity
converges in probability to zero, which completes the proof.
159

A.3
Nonlocal Functional Priors for Nonparametric Hypothesis Testing and
High-dimensional Model Selection
A set of technical results follow that are used in the proof of the main results. For a given
model k,
eβk = (Φ
T
kΦk + 1/⌧nI)−1Φ
T
ky,
eFk = Φk eβk,
ePk = Φk(Φ
T
kΦk + 1/⌧nI)−1Φ
T
k,
Dk(y) = Eβk|y,k
"
exp
(
−
X
j2k
σ2⌧n
βT
j ΦT
j Φjβj
)#
,
(A.35)
where Φk is deﬁned in the second paragraph of Section 5.5.
For 1 j p, the subvector of eβk corresponding to the covariate xj is denoted by eβk,j,
and deﬁne
eβj = (Φ
T
j Φj + 1/⌧nI)−1Φ
T
j y,
eFj = Φj eβj,
eFk,j = Φj eβk,j,
and
ePj = Φj(Φ
T
j Φj + 1/⌧nI)−1Φ
T
j ,
(A.36)
where Φj is deﬁned in the second paragraph of Section 5.5. Similarly, we deﬁne bβk,j as the
subvector of bβk deﬁned in (5.14) corresponding the covariate xj for j 2 k, and bFk,j = Φj bβk,j.
Recall that P0 denotes the probability measure that generates data y.
For univariate settings, we simply denote the basis matrix by Φ and the corresponding
coefﬁcients by β 2 RKn. The ridge solution of β is deﬁned by eβ = (ΦTΦ + 1/⌧nI)−1ΦTy.
Lemma A.3.1. Suppose β⇤| y ⇠N
⇣
eβ, σ⇤2
n (ΦTΦ + 1/⌧nI)−1⌘
for some arbitrary eβ 2 RKn
160

and σ⇤2
n > 0. Let edn = eF T(I −Q0) eF, where eF = Φeβ. Suppose Kn ≺⌧nn−1/2/ log n. Then,
Eβ⇤|y
h
exp
n
−σ2⌧n (β⇤TΦ
T(I −Q0)Φβ⇤)−1oi

exp
(
−σ2
⇢c1σ⇤2
n
n1/2 + edn/⌧n + c2σ⇤
n(nedn)1/2⌧−1
n
5−1)
(A.37)
+ exp{−c3n−1/2⌧n} + exp{−c4n},
and
Eβ⇤|y
h
exp
n
−σ2⌧n (β⇤TΦ
T(I −Q0)Φβ⇤)−1oi
≥
exp
n
−⌧n ed−1
n log n
o ⇣
1 −c5 ed−1
n exp
n
−edn (1 −1/ log n)2o⌘
,
(A.38)
for some positive constants ci for i = 1, . . . , 5.
Proof. First, we shall show the upper bound (A.37). Since exp{−σ2⌧n (β⇤TΦT(I −Q0)Φβ⇤)−1} 
1 for any β⇤2 RKn and β⇤TΦT(I −Q0)Φβ⇤nkΦβ⇤−eFk2
n,2 + 2| eF T(I −Q0)(Φβ⇤−eF)| +
eF T(I −Q0) eF, it follows that
Eβ⇤|y
h
exp
n
−σ2⌧n (β⇤TΦ
T(I −Q0)Φβ⇤)−1oi

Eβ⇤|y

exp
⇢
−σ2⌧n
⇣
nkΦβ⇤−eFk2
n,2 + 2| eF
T(I −Q0)(Φβ⇤−eF)| + edn
⌘−15>

exp
(
−σ2
⇢c1σ⇤2
n
n1/2 + edn/⌧n + c2σ⇤
n(nedn)1/2⌧−1
n
5−1)
+Pβ⇤|y
h
nkΦβ⇤−eFk2
n,2 > c1σ⇤2
n ⌧nn−1/2i
+Pβ⇤|y
h
| eF
T(I −Q0)(Φβ⇤−eF)| > c2σ⇤
n(nedn)1/2/2
i
,
for some constants c1 and c2.
Since nkΦβ⇤−eFk2
n,2/σ⇤2
n = (β⇤−eβ)TΦTΦ(β⇤−eβ) (β⇤−eβ)T(ΦTΦ + 1/⌧nI)(β⇤−eβ),
(β⇤−eβ)T(ΦTΦ+1/⌧nI)(β⇤−eβ) | y ⇠χ2
Kn, and Kn ≺⌧nn−1/2/ log n, Lemma A.1.4 implies
161

that
Pβ⇤|y
h
nkΦβ⇤−eFk2
n,2 > c1σ⇤2
n ⌧nn−1/2i

exp{−c3n−1/2⌧n},
for some constant c3.
Since eF T(I −Q0)(Φβ⇤−eF) | y ⇠N(0, σ⇤2
n eF T(I −Q0) eQΦ(I −Q0) eF), where eQΦ =
Φ(ΦTΦ + 1/⌧nI)−1ΦT, and eF T(I −Q0) eQΦ(I −Q0) eF edn, it follows that
Pβ⇤|y
h
| eF
T(I −Q0)(Φβ⇤−eF)| > c2σ⇤
n(nedn)1/2/2
i

exp{−c4n},
for some constant c4, by the fact that for z , P(|Z| > z) (2⇡)−1/2z−1 exp{−z2/2}, where Z
follows a standard Gaussian distribution.
Second, we consider the lower bound (A.38). By Markov’s inequality, it follows that
Eβ⇤|y
h
exp
n
−σ2⌧n (β⇤TΦ
T(I −Q0)Φβ⇤)−1oi
≥
exp{−σ2⌧n ed−1
n log n}
⇥Pβ⇤|y
h
exp
n
−σ2⌧n (β⇤TΦ
T(I −Q0)Φβ⇤)−1o
> exp
n
−σ2⌧n ed−1
n log n
oi
≥
exp{−σ2⌧n ed−1
n log n}
⇣
1 −Pβ⇤|y
h
β⇤TΦ
T(I −Q0)Φβ⇤< edn/ log n
i⌘
.
Since β⇤TΦT(I−Q0)Φβ⇤/σ⇤2 | y ⇠χ2
Kn−d0( eF(I−Q0) eF/σ⇤2), by Lemma A.1.4, it follows
that
Pβ⇤|y
h
β⇤TΦ
T(I −Q0)Φβ⇤< edn/ log n
i
c5 ed−1
n exp
n
−edn(1 −1/ log n)2o
,
162

for some constant c5.
Lemma A.3.2. Deﬁne
tn,j =
8
>
>
<
>
>
:
qnF T
0 PjF0, if j 2 t,
un(log p + Kn + ⇣−1
n⇤), if j 62 t,
where un = q2
n(log n)2.
Then, for an arbitrary δ > 0,
P0

max
k:|k|qn max
j2k
eF
T
k,j eFk,j/σ2 > tn,j
>
p−|k|(1+δ),
(A.39)
for large enough n. Also,
P0

min
j2t
eF
T
t,j eFt,j/σ2 < F
T
0 PjF0/(σ2 log n)
>
|t| exp{−cn},
(A.40)
for some constant c.
Proof. We note that bF T
k,j bF T
k,j = ATΦjPk\{j}ΦT
j A + ATΦj(I −Pk\{j})ΦT
j A,
where A = (Φ⇤T
j Φ⇤
j)−1Φ⇤T
j y and Φ⇤
j is a full column-rank matrix of (I −Pk\{j})Φj created by
the Gram-Schmidt procedure. Since ATΦj(I−Pk\{j})ΦT
j A = yTP ⇤
j y, where P ⇤
j is a projection
matrix of Φ⇤
j, (A4) implies that
y
TP ⇤
j y bF
T
k,j bF
T
k,j 2y
TP ⇤
j y.
Since yTP ⇤
j y/σ2 ⇠χ2
Kn(F T
0 P ⇤
j F0/σ2), F T
0 PjF0 ≺log p for j 62 t by (A5), and F T
0 P ⇤
j F0 
163

F T
0 PjF0, Lemma A.1.4 and (A5) imply that
P0

max
k:|k|qn max
j2k
eF
T
k,j eFk,j/σ2 > tn,j
>

X
k:|k|qn
X
j2k
P0
h
bF
T
k,j bFk,j/σ2 > tn,j
i

X
k:|k|qn
X
j2k
" ✓tn,j
2Kn
◆Kn/2
exp{Kn/2 −c1tn,j} + c2(F
T
0 P ⇤
j F0/σ2)1/2t−1
n,j
⇥exp{−c3t2
n,j/F
T
0 P ⇤
j F0}
#

X
k:|k|qn
X
j2k\t
exp{−c4qnn} +
X
k:|k|qn
X
j2k\t
exp
⇢
−c5
u2
n(log p + Kn + ⇣−1
n⇤)2
qn log p
5

p−|k|(1+δ),
for some positive constants ci for i = 1, . . . , 5.
Similarly, by Lemma A.1.4, it follows that
P0

min
j2t
eF
T
t,j eFt,j/σ2 < F
T
0 PjF0/(σ2 log n)
>

X
j2t
P0
h
eF
T
t,j eFt,j/σ2 < F
T
0 PjF0/(σ2 log n)
i

|t| exp{−cn},
for some constant c.
Lemma A.3.3. Recall that Dk(y) is deﬁned in (A.35). Assume that (A2)–(A3) hold. Let
un = q2
n(log n)2. For k 6= t and a given δ > 0, there exist some positive constant c and c0 such
that
P0
"
Dk(y)
Dt(y) > exp
(
−
c|k|⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + +⇣−1
n⇤)1/2n1/2 + c0|t|⌧n log /n + zn
)#
p−|k|(1+δ),
where zn = Kn log(⇣⇤
n/⇣n⇤)/2.
164

Proof. We note that
Dk(y)
Dt(y) =
Eβk|y,k
h
exp
n
−P
j2k
σ2⌧n
βT
j ΦT
j Φjβj
oi
Eβt|y,t
h
exp
n
−P
j2t
σ2⌧n
βT
j ΦT
j Φjβj
oi

Y
j2k
⇣−Kn/2
⇤
Eβ⇤j|y,k

exp
⇢
−
σ2⌧n
βT
⇤jΦT
j Φjβ⇤j
5>
⇥
Y
j2t
⇣⇤Kn/2Eβ⇤
j |y,t

exp
⇢
−
σ2⌧n
β⇤T
j ΦT
j Φjβ⇤
j
5>−1
,
(A.41)
where β⇤
j | y, k ⇠N(eβk,j, σ2⇣⇤−1
n
(ΦT
j Φj + 1/⌧nI)−1) and β⇤j | y, k ⇠N(eβk,j, σ2⇣−1
n⇤(ΦT
j Φj +
1/⌧nI)−1) for j 2 k.
By using the upper bound and the lower bound provided in Lemma A.3.1, it follows that
Eβ⇤j|y,k

exp
⇢
−
σ2⌧n
βT
⇤jΦT
j Φjβ⇤j
5>

exp
(
−σ2
⇢c1σ2
⇣n⇤n1/2 + eF
T
k,j eFk,j/⌧n + c2σ⇣−1/2
n⇤
⇣
n eF
T
k,j eFk,j
⌘1/2
⌧−1
n
5−1)
+ exp{−c3n} + exp{−c4n−1/2⌧n},
for j 2 k and some constants ci for i = 1, . . . , 4. Also, by (A.38) in Lemma A.3.1, it follows
that
Eβ⇤
j |y,t

exp
⇢
−
σ2⌧n
β⇤T
j ΦT
j Φjβ⇤
j
5>
≥
exp
(
−σ2⌧n log n
eF T
k,j eFk,j
) ✓
1 −c5
⇣
eF
T
k,j eFk,j
⌘−1
exp
n
−eF
T
k,j eF,j(1 −1/ log n)2o◆
,
for j 2 t and some constant c5.
165

Plugging the bounds in (A.41), it follows that
Dk(y)
Dt(y)

Y
j2k
"
exp
(
−σ2
⇢c1σ2
⇣n⇤n1/2 + edk,j/⌧n + c2σ⇣−1/2
n⇤
⇣
nedk,j
⌘1/2
⌧−1
n
5−1)
+ exp{−c3n} + exp{−c4n−1/2⌧n}
#
⇥⇣−|k|Kn/2
n⇤
⇣⇤−|t|Kn/2
n
⇥
Y
j2t
"
exp
(
σ2⌧n log n
edk,j
) ⇣
1 −c5 ed−1
k,j exp
n
−edk,j(1 −1/ log n)2o⌘−1
#
,
(A.42)
where edk,j = eF T
k,j eFk,j.
Since F T
0 PjF0 ≺un(Kn + log p + ⇣−1
n⇤) ≺n for j 2 k \ t, by Lemma A.3.2, it follows that
Dk(y)/Dt(y)

exp
⇢
−c6|k|σ2⌧n
n
n−1/2⇣−1
n⇤+ un(Kn + log p + ⇣−1
n⇤)
+u1/2
n (Kn + log p + ⇣−1
n⇤)1/2n1/2⇣−1/2
n⇤
o−15
exp{c7|t|σ2⌧n log n/n + zn}

exp
(
−
c8|k|σ2⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + ⇣−1
n⇤)1/2n1/2 + c7|t|σ2⌧n log n/n + zn
)
,
with probability greater than 1 −p−|k|(1+δ) for some constants c6, c7, and c8.
Proof of Theorem 7. Since
log
⇢mL
1 (y)
m0(y)
5
= −Kn −d0
2
log(1 + gn) −
gn
2σ2(1 + gn)y
T(QΦ −Q0)y,
166

it is sufﬁcient to show that for any diverging sequence vn ! 1,
P0
⇣==y
T(QΦ −Q0)y/σ2 −
?
F
T
0 (QΦ −Q0)F0/σ2 + Kn −d0
 ==
> 2{2F
T
0 (QΦ −Q0)F0/σ2 + Kn −d0}1/2vn
⌘
=
o(1).
(A.43)
We note that Birgé (2001) showed the following statements:
When W ⇠χ2
mn(λn), for all t > 0,
P
h
W ≥mn + λn + 2 {(2λn + mn)t}1/2 + 2t
i

exp{−t}
P
h
W mn + λn −2 {(2λn + mn)t}1/2i

exp{−t}.
These results completes the proof, since yT(QΦ−Q0)y/σ2 ⇠χ2
Kn−d0(F T
0 (QΦ−Q0)F0/σ2).
Proof of Theorem 8. It is well-known that when Z ⇠N(m, V ), E(ZTWZ) = tr(WV ) +
mTWm and V ar(ZTWZ) = 2tr{(WV )2}+4mTWV Wm for some symmetric matrix W. So,
since the posterior distribution of β from the local prior follows N{gnbβ/(1+gn), σ2gn(ΦTΦ)−1/(1+
gn)}, where bβ = (ΦTΦ)−1ΦTy, it follows that
Dn(hr=1
M ; y)
=
Eβ|y [βTΦT(I −Q0)Φβ]
E⇡L [βTΦT(I −Q0)Φβ]
=
(1 + gn)−1 +
✓
gn
1 + gn
◆2
bF
T(QΦ −Q0) bF/
-
σ2gn(Kn −d0)
.
,
where bF = Φbβ = QΦy. Also,
Dn(hr=2
M ; y) = Eβ|y [{βTΦT(I −Q0)Φβ}2]
E⇡L [{βTΦT(I −Q0)Φβ}2] = U1 + U2
U3
,
167

where
U1
=
2(σ2gn)2
(1 + gn)2(Kn −d0)2 + 4σ2
✓
gn
1 + gn
◆3
bF
T(QΦ −Q0) bF
U2
=
σ2gn
1 + gn
(Kn −d0) +
✓
gn
1 + gn
◆
bF
T(QΦ −Q0) bF
>2
U3
=
2(σ2gn)2(Kn −d0) +
-
σ2gn(Kn −d0)
.2 .
Since bF T(QΦ −Q0) bF = yT(QΦ −Q0)y, (A.43) completes the proof.
Proof of Theorem 10. Since the local prior ⇡L on β follows N(0, σ2⌧n(ΦTΦ)−1), by using
Markov’s inequality, it follows that for any ﬁxed ✏> 0,
E⇡L

exp
⇢
−
σ2⌧n
βTΦT(I −Q0)Φβ
5>
≥
✏P⇡L

exp
⇢
−
σ2⌧n
βTΦT(I −Q0)Φβ
5>
≥
✏P⇡L
βTΦT(I −Q0)Φβ
σ2⌧n
> (−log ✏)−1
>
.
Since βTΦT(I −Q0)Φβ/(σ2⌧n) ⇠χ2
Kn−d0, the right-hand side of the last equation of the above
display is bounded below by ✏P [Z2 > (−log ✏)−1], where Z ⇠N(0, 1), which concludes that
E⇡L[exp{−σ2⌧n/{βTΦT(I −Q0)Φβ}}] is strictly bounded from zero. So, there exists a strictly
positive constant C such that E⇡L[exp{−σ2⌧{βTΦT(I −Q0)Φβ}−1}] > C > 0.
168

Therefore,
P0

Mn,I
−log Dn(hI; y) > vn
>
=
P0
"
log Eβ|y

exp
⇢
−
σ2⌧n
βTΦT(I −Q0)Φβ
5>
> log E⇡L

exp
⇢
−
σ2⌧n
βTΦT(I −Q0)Φβ
5>
−Mn,Iv−1
n
#

P0
⇢
log Eβ|y

exp
⇢
−
σ2⌧n
βTΦT(I −Q0)Φβ
5>
> C −Mn,Iv−1
n
5
\ An
>
+P0 [Ac
n] ,
(A.44)
where An = {y : yT(I −Q0)y < F T
0 (I −Q0)F0 + σ2(Kn −d0) + 2σ2{F T
0 (I −Q0)F0/σ2 +
(Kn −d0)}1/2 log n}.
Since P0[Ac
n] = o(1) by (A.43), it is sufﬁcient to show that the ﬁrst term in (A.44) is o(1).
By (A.37) in Lemma A.3.1, the ﬁrst term can be bounded above by
P0
"⇢
log
"
exp
(
−
σ2⌧n
c1σ⇤2
n n−1/2 + edn + c2σ⇤
n(nedn)1/2
)
+ exp
n
−c3
⌧n
n1/2
o#
+ exp{−c4n} > C −Mn,Iv−1
n
5
\ An
#

I

−
c5⌧n
dn + σ2(ndn)1/2 > C −Mn,Iv−1
n
>
,
(A.45)
where I(·) is the indicator function, σ⇤2
n = ⌧nσ2/(1 + ⌧n) and edn = ⌧2
n bF T(QΦ −Q0) bF/(1 +
⌧n)2 = ⌧2
nbyT(QΦ −Q0)y/(1 + ⌧n)2 for some constants ci for i = 1, . . . , 5. Since Mn,I =
⌧n{dn + σ2(ndn)1/2}−1 and v−1 ! 0, it is clear that (A.45) is o(1), which completes the
proof.
Proof of Proposition 12. The asymptotic property of the B-spline approximation De Boor
169

(1978) guarantees that if f0 2 C↵[0, 1], there exists some β1 2 RKn, kΦβ1 −F0k1 ⪯
K−↵
n kf0k↵. By using this asymptotic inequality, it follows that
F
T
0 (QΦ −Q0)F0
=
F
T
0 (I −Q0)F0 −F
T
0 (I −QΦ)F0
≥
F
T
0 (I −Q0)F0 −nkΦβ1 −F0k1
⌫
F
T
0 (I −Q0)F0 −nK−↵
n kf0k↵.
Also, it is clear that F T
0 (QΦ −Q0)F0 F T
0 (I −Q0)F0, which completes the proof.
Proof of Theorem 13. To show that ⇡NL(t | y) converges to one in probability, it is sufﬁcient
to show that H1n and H2n in (A.5) both converge zero in probability as n tends to 1. We shall
prove the Theorem by showing the follows:
For any ﬁxed δ > 0, ✏> 0 and any model k 2 Γd (deﬁned in Lemma A.1.2),
P0
mk(y)
mt(y) > ✏p−dq−1
n
>
p−d(1+δ),
(A.46)
and for any model k 2 Γk,ck,ct (deﬁned in Lemma A.1.3),
P0
mk(y)
mt(y) > ✏n−3p−kn−ckt−t
>
p−k(1+δ).
(A.47)
Then, it is clear that H1n and H2n both converge to zero in probability by Lemma A.1.2 and
Lemma A.1.3 respectively.
We ﬁrst show that the normalizing constant of the nonlocal functional prior densities is
asymptotically at rate of (2⇡⌧n)Kn/2, i.e.,
R
exp{−βTβ/(2⌧n) −σ2⌧n/βTΦT
j Φjβ}dβ ⇣(2⇡⌧n)Kn/2. Let Eβ[·] and Pβ[·] be the expecta-
tion and the probability induced by a random variable β ⇠N(0, σ2⌧nI). Then, by using the
170

Markov inequality and (A3), it follows that
Z
exp{−β
Tβ/(2σ2⌧n) −⌧n/β
TΦ
T
j Φjβ}dβ = (2⇡⌧n)Kn/2Eβ

exp
⇢
−
σ2⌧n
βTΦT
j Φjβ
5>
≥
(2⇡⌧n)Kn/2 exp{−σ2 log nKn/(λ⇤n)}
⇥Pβ
⇥
exp{−σ2⌧n/β
TΦ
T
j Φjβ} > exp{−σ2(log n)Kn/(λ⇤n)}
⇤
≥
(2⇡⌧n)Kn/2 exp{−σ2(log n)Kn/(λ⇤n)}Pβ
⇥
β
Tβ/(σ2⌧n) > 1/(σ2 log n)
⇤
≥
(2⇡⌧n)Kn/2(1 −o(1)),
where λ⇤is deﬁned in (A3). The last inequality is derived by using the fact that P(WKn <
x) (x/Kn)Kn/2 exp{Kn/2 −x/2} for WKn ⇠χ2
Kn and x < Kn. It is also clear that
R
exp{−βTβ/(2σ2⌧n)−σ2⌧n/βTΦT
j Φjβ}dβ (2⇡⌧n)Kn/2, since exp{−σ2⌧n/βTΦT
j Φjβ} 1
for any β 2 RKn.
Second, we shall show that (A.46) holds. Recall that Γd = {k : |k| qn t ( k, |k|−|t| =
d}. By Lemma A.3.3, (A2) and (A3), it follows that for any k 2 Γd, there exists δ > 0 such
171

that
P0
mk(y)
mt(y) > ✏p−dq−1
n
>

P0
"
(c1⌧n)−dKn/2
✓|ΦT
kΦk + 1/⌧nI|
|ΦT
t Φt + 1/⌧nI|
◆−1/2 ✓Dk(y)
Dt(y)
◆
⇥exp
(
yT( ePk −ePt)y
2σ2
)
> ✏p−dq−1
n
#

P0
"
(c1⌧n)−dKn/2Q|k|
n⇤Q⇤−|t|
n
exp
(
yT( ePk −ePt)y
2σ2
)
⇥exp
(
−
c2d⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + ⇣−1
n⇤)1/2n1/2
)
> ✏p−dq−1
n
#
+P0
"
Dk(y)
Dt(y) > exp
(
−
c2d⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + ⇣−1
n⇤)1/2n1/2
)#

P0
"
y
T( ePk −ePt)y/σ2 > −2d log p −2 log qn + dKn log ⌧n
⇥+
c2d⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + ⇣−1
n⇤)1/2n1/2 + Zn
#
+p−d(1+δ),
where Qn⇤= (⇣n⇤λ⇤n/Kn + 1/⌧n)−Kn/2, Q⇤
n = (⇣⇤
nλ⇤n/Kn + 1/⌧n)−Kn/2
and Zn = |k|Kn log(⇣n⇤λ⇤n/Kn) −|t|Kn log(⇣⇤
nλ⇤n/Kn), for some positive constants c1 and
c2.
Let
tn = −2d log p−2 log qn+dKn log(1+gn)+
c2d⌧nu−1/2
n
⇣1/2
n⇤
(Kn + log p + ⇣−1
n⇤)1/2n1/2 +Zn−F
T
0 (Pk−Pt)F0.
We note that yT( ePk −ePt)y yT(Pk −Pt)y +c0Kn(n⌧nλ⇤⇣n⇤)−1yTPty for some constant c0.
Therefore, since yT(Pk −Pt)y/σ2 ⇠χ2
dKn(F T
0 (Pk −Pt)F0), Lemma A.1.4 and (A5) implies
172

that
P0
h
y
T( ePk −ePt)y/σ2 > F
T
0 (Pk −Pt)F0/σ2 + tn
i

P0
⇥
y
T(Pk −Pt)y/σ2 > F
T
0 (Pk −Pt)F0/σ2 + tn/2
⇤
+ P0
c0KnyTPty
n⌧nλ⇤⇣n⇤
> tn/2
>

c3(tn/(2dKn))dKn/2 exp{dKn/2 −tn/4}
+c4{F
T
0 (Pk −Pt)F0}1/2t−1
n exp
⇢
−
σ2t2
n
128F T
0 (Pk −Pt)F0
5
+ exp{−c5n},
for some constant cl with l 2 {3, 4, 5}. Since ⌧n⇣1/2
n⇤u−1/2
n
(Kn + log p + ⇣−1
n⇤)−1/2n−1/2 ≻
max{F T
0 (Pk −Pt)F0, Zn, log p} by (A5), it follows that the last equation in the above display
is bounded above by p−d(1+δ) for any δ > 0, which proves (A.46).
Third, we shall show that (A.47) holds. Recall that Γk,ck,ct = {k : |k| qn, |k| =
k, |k\t| = ck, |t\k| = ct}. By following similar steps used in the previous proof for (A.46),
it follows that for any model k 2 Γk,ck,ct, there exists some δ > 0 such that
P0
mk(y)
mt(y) > ✏n−3p−kn−ckt−t
>

P0
"
(2⇡⌧n)−(ck−ct)Kn/2
✓|ΦT
kΦk + 1/⌧nI|
|ΦT
t Φt + 1/⌧nI|
◆−1/2 ✓Dk(y)
Dt(y)
◆
exp
n
y
T( ePk −ePt)y/(2σ2)
o
> ✏n−3p−kn−ckt−t
#

P0
"
(cn)−(ck−ct)KnQ|k|
n⇤Q⇤−|t|
n
exp
⇢yT( ePk −ePu)y
2σ2
−
c0ck⌧n⇣1/2
n⇤u−1/2
n
(Kn + log p + ⇣−1
n⇤)1/2n1/2
+qn log p + Zn
5
> p−k−δ
#
(A.48)
+P0
"
Dk(y)
Dt(y) > exp
(
−
c0ck⌧n⇣1/2
n⇤u−1/2
n
(Kn + log p + ⇣−1
n⇤)1/2n1/2 + Zn
)#
(A.49)
+P0
"
yT( ePu −ePt)y
2σ2
> qn log p
#
,
(A.50)
173

where u = k [ t and t = |t| for some constant c and c0.
We are going to show that the three terms (A.48), (A.49), and (A.50) all are bounded above
by p−|k|(1+δ) for some δ > 0. Then, the proof is competed by Lemma A.1.3.
By Lemma A.3.3, (A.49) is bounded above by p−|k|(1+δ). Let zn = qn log p −F T
0 (Pu −
Pt)F0/σ2. Then, since yT(Pu −Pt)y/σ2 ⇠χ2
ckKn(F T
0 (Pu −Pt)F0/σ2), by Lemma A.1.4,
(A.50) also can be shown as
P0
h
y
T( ePu −ePt)y/σ2 > 2qn log p
i

P0
⇥
y
T(Pu −Pt)y/σ2 > qn log p
⇤
+ P0
c0KnyTPty
n⌧nλ⇤⇣n⇤
> qn log p
>

c1 (zn/{ckKn})ckKn/2 exp {|k \ t|Kn/2 −zn/2}
+c2
-
F
T
0 (Pu −Pt)F0/σ2.1/2 z−1
n exp
?
−z2
n/{32F
T
0 (Pu −Pt)F0/σ2}
 
+P0
c0KnyTPty
n⌧nλ⇤⇣n⇤
> qn log p
>
,
for some constant c1 and c2. Since P [c0Kn/(n⌧nλ⇤⇣n⇤)yTPty/y > qn log p] p−|k|(1+δ) by
Lemma A.1.4, |k| qn and zn ⇣qn log p by (A5), (A.50) is bounded above by p−|k|(1+δ) for
any ﬁxed δ > 0.
Also, (A.48) can be bounded above by
P0
"
yT(Pu −Pk)y
σ2
< (2 + δ)qn log p −2 log
 
Q|k|
n⇤
Q⇤|t|
n
!
(A.51)
+
c0ck⌧n⇣1/2
n⇤u−1/2
n
(Kn + log p + ⇣−1
n⇤)1/2n1/2
#
(A.52)
+
P0
c0KnyTPuy
n⌧nλ⇤⇣n⇤
> σ2Kn log n
>
,
(A.53)
for some constant c3. We note that yT(Pu −Pk)y/σ2 ⇠χ2
ct(F T
0 (Pu −Pk)F0/σ2). Since
F T
0 (Pu −Pk)F0 ≻qn⌧n⇣1/2
n⇤u−1/2
n
/{(Kn + log p + ⇣−1
n⇤)1/2n1/2} by (A5), Lemma A.1.4 im-
plies that (A.51) is bounded above by p−|k|(1+δ) for some δ > 0. Also, since yTPuy/σ2 ⇠
174

χ2
|u|Kn(F T
0 PuF0/σ2), it follows that (A.53) is bounded above by p−|k|(1+δ) by Lemma A.1.4.
175

APPENDIX B
DETAILS OF COMPUTATION
B.1
Nonlocal Prior Densities for High-dimensional Linear Model Selection
In this section, we provide the Laplace approximation of the marginal likelihoods based on
the nonlocal priors. Because closed form expressions for posterior model probabilities based
on modiﬁed peMoM priors and modiﬁed piMoM priors are not available, we estimate the
posterior model probabilities using Laplace approximations. For posterior probabilities based
on the peMoM priors, an inverse-Gamma density with parameters (a0, b0) on σ2 the Laplace
approximation to the marginal density of the data for model k can be expressed as
⇡(k | y) / (2⇡)|k|/2 ==V (β⇤
k, σ2⇤)
==−1/2 exp{f(β⇤
k, σ2⇤)}p(k),
(B.1)
where
(β⇤
k, σ2⇤)
=
argmax
(βk,σ2)
f(βk, σ2)
f(βk, σ2)
=
−(n/2 + |k|/2 + a0 + 1) log σ2 −(y −Xkβk)T(y −Xkβk)/(2σ2)
−βT
k βk/(2σ2⌧n,p) −
|k|
X
j=1
⌧n,p/β2
k,j + |k|(2/σ2)1/2 −b0/σ2 + |k|(log ⌧n,p)/2,
176

and V (βk, σ2) is a (|k| + 1) ⇥(|k| + 1) matrix with the following blocks:
V11
=
XT
k Xk/σ2 + Ik/σ2⌧n,p + diag
?
6⌧n,p/β4
k,j
 
j=1,...,|k|
V12
=
XT
k (y −Xkβk)/σ4 −βk/{σ4⌧n,p}
V22
=
−(n/2 + |k|/2 + a0 + 1)/σ4 + (y −Xkβk)T(y −Xkβk)/σ6 −βT
k βk/⌧n,p
−3|k|21/2σ−5/4 + 2b0/σ6.
For the piMoM priors on βk, the Laplace approximation of the posterior model probability can
be expressed as in (B.3), but with
f(βk, σ2)
=
−(n/2 + a0 + 1) log σ2 −(y −Xkβk)T(y −Xkβk)/(2σ2) −b0/σ2
−
|k|
X
j=1
?
r log(β2
k,j) + ⌧n,p/β2
k,j
 
+ |k|
?
(r −1/2) log ⌧n,p −log Γ(r −1/2)
 
,
and V (βk, σ2) a (|k| + 1) ⇥(|k| + 1) matrix with the following blocks:
V11
=
XT
k Xk/σ2 + diag
?
6⌧n,p/β4
k,j −2r/β2
k,j
 
j=1,...,|k|
V12
=
XT
k (y −Xkβk)/σ4
V22
=
−(n/2 + a0 + 1)/σ4 + (y −Xkβk)T(y −Xkβk)/σ6 + 2b0/σ6.
B.2
Functional Horseshoe Prior for Nonparametric Subspace Shrinkage
In model (4.1), the conditional posterior distribution of ⌧based on the functional horseshoe
prior can be expressed as
⇡(⌧| Y, β) / (⌧2)−(kn−d0)/2+b−1/2(1 + ⌧2)−a−b exp{−β
TΦ
T(I −Q0)Φβ/(2σ2)}.
177

By reparameterizing ⌘= 1/⌧2, the resulting conditional posterior distribution of ⌘can be
expressed as
⇡(⌘| Y, β) / ⌘a+(kn−d0)/2−1 exp{−β
TΦ
T(I −Q0)Φβ/(2σ2)}
1
(1 + ⌘)a+b.
As in Polson et al. (2014), a slice sampling method (Neal, 2003) can be used to sample
⌘from its conditional posterior distribution. The resulting MCMC algorithm is described in
Algorithm 3.
Algorithm 3 MCMC algorithm for simple nonparametric regression models
Choose an initial value β(0) and ⌧(0).
For l in 0 : (L −1)
Sample β(l+1) from N(eβ!(l), σ2e⌃!(l)), where eβ! and e⌃! are deﬁned in (4.7).
(Slice sampling step) Set ⌘= 1/⌧2(l) and t = (⌘+ 1)−a−b.
Sample u ⇠Unif(0, t) and set t⇤= u−(a+b)−1 −1.
Sample ⌘⇤⇠truncated Gamma(a + (kn −d0)/2, β(l+1)TΦT(I −Q0)Φβ(l+1)/(2σ2))
on (0, t⇤),
Update ⌧(l+1) by ⌘⇤−1/2.
End.
In the additive model in (4.14) with a product of the functional horseshoe priors, the con-
ditional posterior distribution of βj given !j and the other coefﬁcients β(−j), for j = 1, . . . , p,
can be expressed as
βj | !j, β(−j), Y ⇠N
⇣
eβj,!, σ2e⌃j,!
⌘
,
178

where
eβj,! = e⌃j,!Φ
T
j rj,
e⌃j,! = (1 −!j)
-
Φ
T
j Φj
.−1 ,
rj = Y −
X
l6=j
Φlβl.
(B.2)
It follows that sampling Algorithm 3 can be extended to additive regression models to obtain
Algorithm 4 below.
Algorithm 4 MCMC algorithm for additive regression models
Choose an initial value β(0)
j
and ⌧(0)
j
for j = 1, · · · , p.
For l in 0 : (L −1)
For j in 1 : p
Sample β(l+1)
j
from N(eβj,!(l), σ2e⌃j,!(l)), where eβj,! and e⌃j,! are deﬁned in (B.2).
End.
For j in 1 : p
(Slice sampling step)
Set ⌘= 1/⌧2(l)
j
and t = (⌘+ 1)−a−b.
Sample u ⇠Unif(0, t) and set t⇤= u−(a+b)−1 −1.
Sample ⌘⇤⇠truncated Gamma(a + kn/2, β(l+1)T
j
ΦT
j Φjβ(l+1)
j
/(2σ2)) on (0, t⇤),
Update ⌧(l+1)
j
by ⌘⇤−1/2.
End.
End.
B.3
Nonlocal Functional Priors for Nonparametric Hypothesis Testing and
High-dimensional Model Selection
B.3.1
Modiﬁed Simpliﬁed Shotgun Stochastic Search with Screening (S5) for Additive
Models
We consider a sequence of L number of temperatures {tl}l=1,...,L such that t1 > t2 >
. . . > tL > 0. Also, we deﬁne a screened set by marginal correlations as SL
k(M) = {j 2
179

{1, . . . , p} : rank(|rT
kXj| M)}, where rk is the residual of a model k. Then, letting
nbd(k) = {Γ−, Γ+
scr}, where Γ−= {k \ {j} : j 2 k} and Γ+
scr = {k [ {j} : j 2 kc \ Sk(M)},
the modiﬁed S5 for additive model is illustrated in Algorithm 5.
Algorithm 5 Modiﬁed S5 for Additive Models
Set a temperature schedule t1 > t2 > . . . > tL > 0
Choose an initial model k(1,1) and a set of variables after screening Sk(1,1) based on k(1,1)
For l = 1 in l = L
For i in 1, . . . , J −1
Compute all ⇡(k | y) for all k 2 nbdscr(k(i,l))
Sample k+ and k−, from Γ+
scr and Γ−, with probabilities proportional to ⇡(k | y)1/tl
Sample k(i+1,l) from {k+, k−},
with probability proportional to {⇡(k+ | y)1/tl, ⇡(k−| y)1/tl}
Update the set of considered variables Sk(i+1,l) to be the union of variables in k(i+1,l) and
SINIS
k(i+1,l)(M) [ SL
k(i+1,l)(M).
B.3.2
Laplace Approximations of Marginal Likelihoods Based on Nonlocal Functional
Prior Densities
In this section, we provide the Laplace approximation of the marginal likelihoods based
on the inverse moment functional priors and ⇡(σ2) / 1/σ2. Because explicit expressions
for marginal likelihoods are not available, we estimate the marginal likelihoods using Laplace
approximations. Letting ⌘= 1/σ2, the Laplace approximation to the marginal density of the
data for model k can be expressed as
mk(y) ⇡(2⇡)|k|Kn/2 |V (β⇤
k, ⌘⇤)|−1/2 exp{f(β⇤
k, ⌘⇤)},
180

where
(β⇤
k, ⌘⇤)
=
argmax
(βk,⌘)
f(βk, ⌘)
f(βk, ⌘)
=
(n/2 + |k|/2) log ⌘−⌘(y −Φkβk)
T(y −Φkβk)/2 −⌘β
T
kβk/(2⌧n)
−
X
j2k
⌧n/
-
⌘β
T
j Φ
T
j (I −Q0)Φjβj
.
−
X
j2k
log Zj,
and V (βk, ⌘) is a (|k|Kn + 1) ⇥(|k|Kn + 1) matrix with the following blocks:
V11
=
diag
"⇢8⌧nΦT
j (I −Q0)ΦjβjβT
j ΦT
j (I −Q0)Φj
⌘(βT
j ΦT
j (I −Q0)Φjβj)3
−
2⌧nΦT
j Φj
⌘(βT
j ΦT
j (I −Q0)Φjβj)2
5
j2k
#
+⌘Φ
T
kΦk + ⌘
⌧n
I
V12
=
−Φ
T
k(y −Φkβk) + 1
⌧n
{βj}j2k +
⇢
2⌧nΦT
j Φjβj
⌘βT
j ΦT
j (I −Q0)Φjβj
5
j2k
V22
=
(n/2 + |k|Kn/2)/(⌘2) +
X
j2k
2⌧n
⌘2βT
j ΦT
j (I −Q0)Φjβj
.
The prior normalizing constant Zj for j = 1, . . . , p can be approximated by using important
sampling procedures, since Zj = E⇡L[exp{σ2⌧n/(βT
j Φj(I−Q0)Φjβj)}] and ⇡L ⇠N(0, σ2⌧nI).
181

